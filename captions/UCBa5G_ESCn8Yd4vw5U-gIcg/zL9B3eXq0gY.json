[
  {
    "text": " Today for our talk, we have\nProfessor Jake Williams",
    "start": "0",
    "end": "7990"
  },
  {
    "text": "from Drexel University. He is an associate professor\nat information science",
    "start": "7990",
    "end": "13389"
  },
  {
    "text": "at Drexel University's College\nof Computing and Informatics in Philadelphia, Pennsylvania.",
    "start": "13390",
    "end": "19120"
  },
  {
    "text": "Dr. Williams has a background\nin physics and math with degrees from the\nUniversity of Vermont,",
    "start": "19120",
    "end": "24760"
  },
  {
    "text": "and his research leverages\na quantitative linguistics perspective that applies math\nand statistical methodologies",
    "start": "24760",
    "end": "32559"
  },
  {
    "text": "to analyze and improve\nlinguistic learning systems. Following a one-year postdoc\nappointment at the University",
    "start": "32560",
    "end": "39340"
  },
  {
    "text": "of Berkeley, studying\nlarge language, large-scale machine\nlearning in 2015,",
    "start": "39340",
    "end": "45550"
  },
  {
    "text": "Dr. Williams became a data\nscience faculty at Drexel where he drove the foundation\nof a DSMS program and develops",
    "start": "45550",
    "end": "53170"
  },
  {
    "text": "and instruct data\nscience coursework, including natural language\nprocessing with deep learning.",
    "start": "53170",
    "end": "60460"
  },
  {
    "text": "So welcome, and thank you for\ncoming today for your talk. And you could do a quick\nintroduction of yourself",
    "start": "60460",
    "end": "67430"
  },
  {
    "text": "before you start. Great. Thanks so much. I got the mic here. Nice to see you all here.",
    "start": "67430",
    "end": "72630"
  },
  {
    "text": "Thanks for coming out, and\nalso for showing up online. It's a pleasure to be here.",
    "start": "72630",
    "end": "79670"
  },
  {
    "text": "As was mentioned,\nmy name is Jake, and my background is\nin math and physics. So the perspective that I'm\ncoming from towards this work",
    "start": "79670",
    "end": "87217"
  },
  {
    "text": "might be a little bit\ndifferent than the standard, and that will be a theme\nthroughout the discussion.",
    "start": "87217",
    "end": "92810"
  },
  {
    "text": "The purpose of\nthis discussion is to go through a relatively\nlong-term development, a project",
    "start": "92810",
    "end": "100969"
  },
  {
    "text": "that I've been working on. And as mentioned, my background\nis in quantitative linguistics,",
    "start": "100970",
    "end": "106880"
  },
  {
    "text": "which means my history\nof focus on language has primarily been to\ndevelop general theories",
    "start": "106880",
    "end": "116780"
  },
  {
    "text": "and descriptions of\nphenomena that you observe with regards to linguistic\nunits, whatever those might be.",
    "start": "116780",
    "end": "123830"
  },
  {
    "text": "It's a statistical\napproach which, based on theories of\nlanguage generation",
    "start": "123830",
    "end": "130580"
  },
  {
    "text": "that are statistical in basis. And over the course of\nmy time as a researcher,",
    "start": "130580",
    "end": "138470"
  },
  {
    "text": "I've explored and ventured\ninto language modeling itself, and ultimately, into neural\nnetworks as they approach",
    "start": "138470",
    "end": "145940"
  },
  {
    "text": "language modeling themselves. And that's what brought\nme here through quite",
    "start": "145940",
    "end": "153410"
  },
  {
    "text": "a bit of other work. So if you look into\nmy profile, you'll see a lot of different\nsubjects in either applied NLP,",
    "start": "153410",
    "end": "159390"
  },
  {
    "text": "like I said,\nquantitative linguistic. And neural networks is a\nnatural transition for me",
    "start": "159390",
    "end": "165170"
  },
  {
    "text": "into inferential work. So let's get started.",
    "start": "165170",
    "end": "170605"
  },
  {
    "text": " Well, this is how we'll\nstart the conversation today.",
    "start": "170605",
    "end": "176489"
  },
  {
    "text": "It's not exactly how\nwe got here in my lab. We came at this subject\nfrom a different approach,",
    "start": "176490",
    "end": "183800"
  },
  {
    "text": "trying to think about\nlayer initializations in neural networks.",
    "start": "183800",
    "end": "189700"
  },
  {
    "text": "And this subject\nthat we're discussing as a front for this\ntalk is specifically",
    "start": "189700",
    "end": "196990"
  },
  {
    "text": "focused on transformer\narchitecture components, the self-attention\ncomponent that's pivotal to the success of\nthe transformer architecture.",
    "start": "196990",
    "end": "205030"
  },
  {
    "text": "And it focuses on the fact\nthat self-attention requires a quadratic\ncomparison of vectors",
    "start": "205030",
    "end": "211030"
  },
  {
    "text": "in order to produce the feature\nweights of those vectors needed to model long-range\ndependencies in text.",
    "start": "211030",
    "end": "218140"
  },
  {
    "text": "Commonly, parameters\nfor self-attention are based on a\ntransformation matrix. Two, usually, queries\nand keys, that",
    "start": "218140",
    "end": "226690"
  },
  {
    "text": "are responsible for\ndimensionalizing input vectors. And I describe it this way\nbecause generally speaking,",
    "start": "226690",
    "end": "232450"
  },
  {
    "text": "when you're at the point\nof a self-attention layer, you already have low\ndimensional vectors. But the parameters in a\nstandard self-attention layer",
    "start": "232450",
    "end": "240580"
  },
  {
    "text": "are changing the dimensionality\nand the structure of that dimensional space. They are like an\nembedding layer,",
    "start": "240580",
    "end": "246930"
  },
  {
    "text": "which is factorizing the\nembedding dimensions. This re-dimensionalization\nis the primary means",
    "start": "246930",
    "end": "252770"
  },
  {
    "text": "by which self-attention\ncreates feature weights. It really just computes\nsimilarity in that shared space.",
    "start": "252770",
    "end": "260870"
  },
  {
    "text": "Large and similar inner\nproducts really just result in strongly\nweighted features. So it's up to that\ndimensionalization",
    "start": "260870",
    "end": "266509"
  },
  {
    "text": "to produce good similarities\nfor whatever purpose your prediction requires.",
    "start": "266510",
    "end": "273000"
  },
  {
    "text": "However, an alternative\nstrategy for future waits might ask given a basis. So in other words,\nyou're stuck with",
    "start": "273000",
    "end": "278940"
  },
  {
    "text": "your low dimensional vectors. What is the optimal way to\nconvert those comparisons",
    "start": "278940",
    "end": "284460"
  },
  {
    "text": "of the vectors you're looking\nat by a matrix transformation to modify the vector\nsimilarities that you",
    "start": "284460",
    "end": "291030"
  },
  {
    "text": "are stuck with that\ncorrespond to the best weights for features? In other words, treat this\nas a feedforward layer",
    "start": "291030",
    "end": "298320"
  },
  {
    "text": "to produce self-attention\nweights as opposed to try and transform to some basis that\nproduces good feature weights.",
    "start": "298320",
    "end": "306030"
  },
  {
    "text": "The use of this modified\nself-attention mechanism will be part and parcel\nthe substance of this talk.",
    "start": "306030",
    "end": "311405"
  },
  {
    "text": " It's worth noting that this\nalternative mechanism is",
    "start": "311405",
    "end": "318505"
  },
  {
    "text": "entirely compatible with the\ntraditional dimensionalizing version of self-attention. In other words, you could\nstill change the dimension",
    "start": "318505",
    "end": "327240"
  },
  {
    "text": "and compute\nsimilarities, and then convert that with a\nsecond feedforward layer",
    "start": "327240",
    "end": "332310"
  },
  {
    "text": "to produce optimal\nfeature weights. This is not\nexclusive in any way. This is exploring how useful\nthat alternative prediction",
    "start": "332310",
    "end": "340979"
  },
  {
    "text": "of feature weights can function. However, we'll avoid\nthe standard mechanism for two reasons.",
    "start": "340980",
    "end": "346960"
  },
  {
    "text": "First, we have no solution\nto the standard parameters for self-attention\nas an initialization.",
    "start": "346960",
    "end": "353699"
  },
  {
    "text": "And this will be discussed\nat length in slides to come. Likewise, it would create an\nadditional model complexity",
    "start": "353700",
    "end": "361830"
  },
  {
    "text": "that would muddle the\neffects of the modified form of self-attention\nthat we wish to study. So having that\ndimensionalization as a way",
    "start": "361830",
    "end": "369660"
  },
  {
    "text": "to produce good feature weights\nwould confuse whether or not the feedforward computation\nof feature weights",
    "start": "369660",
    "end": "376080"
  },
  {
    "text": "is functioning well. There's a catch to\nthis, however, which is that these\nvectors that we use",
    "start": "376080",
    "end": "382590"
  },
  {
    "text": "for such a self-attention\nlayer better be good. In other words,\ntheir comparisons",
    "start": "382590",
    "end": "388470"
  },
  {
    "text": "must be consistent and\nmeaningful in the first place.",
    "start": "388470",
    "end": "393710"
  },
  {
    "text": "So to get it out\nof the way, here is an architectural\ndiagram for the relatively simple near shallow architecture\npattern that we're using.",
    "start": "393710",
    "end": "403110"
  },
  {
    "text": "It doesn't seem like\nthere are many neurons in a network of this\ntype, and that's because all of the\nactivations are softmax.",
    "start": "403110",
    "end": "409370"
  },
  {
    "text": "Which means despite the fact\nthat the U-matrix, for example, is an entire layer,\nit's really just going",
    "start": "409370",
    "end": "416030"
  },
  {
    "text": "through a single\nprediction non-linearity, the softmax function. So you can think about\nthis as essentially",
    "start": "416030",
    "end": "421700"
  },
  {
    "text": "a three-layer\nnetwork that might be creating an encoder-decoder\nkind of design.",
    "start": "421700",
    "end": "427070"
  },
  {
    "text": "Likewise, the difference\nin presentation here over self-attention, which is\nparameterized by the matrix W",
    "start": "427070",
    "end": "433310"
  },
  {
    "text": "here, is intending\nto show how a--",
    "start": "433310",
    "end": "439970"
  },
  {
    "text": "whether you consider it\nthe query or the key-- one vector is the pivot\nfor the comparison that",
    "start": "439970",
    "end": "447440"
  },
  {
    "text": "will produce the\nfeature weights, which is then fed forward\nin this model through W.",
    "start": "447440",
    "end": "453480"
  },
  {
    "text": "This is the case for\nstandard self-attention, too. In other words,\nyou can reduce it to a biprediction\ndiagram in this way",
    "start": "453480",
    "end": "460710"
  },
  {
    "text": "where a gray vector such as is\ndepicted here is that pivot.",
    "start": "460710",
    "end": "467130"
  },
  {
    "text": "The attention\ndistribution coming out of the W matrix and\nthe softmax function is indicated by the\nvertical red bar",
    "start": "467130",
    "end": "474449"
  },
  {
    "text": "there, which weights the\nblock of vectors in black. That includes the\npivot vector in gray,",
    "start": "474450",
    "end": "481560"
  },
  {
    "text": "which is then passed through a\nfeedforward layer often called the values of a standard\nself-attention matrix--",
    "start": "481560",
    "end": "487140"
  },
  {
    "text": "U. Since we use U as\na way to reduce",
    "start": "487140",
    "end": "493200"
  },
  {
    "text": "the dimensionality\nof the prediction that we're trying\nto make, we then feed that forward\nthrough another layer,",
    "start": "493200",
    "end": "499800"
  },
  {
    "text": "and then to output. And that's essentially\nthe relative shallowness",
    "start": "499800",
    "end": "505680"
  },
  {
    "text": "that we're talking about here. W is a self-attention\nmatrix, which means there's really only\ntwo layers in effect here.",
    "start": "505680",
    "end": "513940"
  },
  {
    "text": "And the activation\nfunctions are strange. And you might\nwonder, for example, why we're using a\ndifferent activation",
    "start": "513940",
    "end": "519840"
  },
  {
    "text": "function to softmax instead\nof any of the dimensionally independent activation functions\nlike a logistic function",
    "start": "519840",
    "end": "526590"
  },
  {
    "text": "or anything else. And that's because we\nhave additional insight into the softmax function\nand the parameters that it",
    "start": "526590",
    "end": "533760"
  },
  {
    "text": "optimizes, which is very useful. So let's talk about those\nvectors first though,",
    "start": "533760",
    "end": "541050"
  },
  {
    "text": "before we get to\nlayer initialization. ",
    "start": "541050",
    "end": "547380"
  },
  {
    "text": "Optimizing the keys and queries\nof standard self-attention bears substantial similarity\nto token and word embedding.",
    "start": "547380",
    "end": "554220"
  },
  {
    "text": "This is because the\nkey and query matrices have a common dimension\nthat they project to,",
    "start": "554220",
    "end": "560430"
  },
  {
    "text": "much like you'd see with the\nfactorization of an embedding layer on its own. Think word word2vec,\nsomething like that.",
    "start": "560430",
    "end": "566285"
  },
  {
    "text": " Normally, there might be\nmultiple self-attention heads.",
    "start": "566285",
    "end": "573150"
  },
  {
    "text": "And because of the indeterminacy\nin creating a different dimensional space--\nin other words,",
    "start": "573150",
    "end": "578460"
  },
  {
    "text": "there are multiple equivalent\nreshufflings of those different dimensions which will\nproduce the same output--",
    "start": "578460",
    "end": "585030"
  },
  {
    "text": "that indeterminacy is\nsomething that we hypothesize has bearing on what is now\nreferred to as the lottery",
    "start": "585030",
    "end": "592680"
  },
  {
    "text": "ticket hypothesis. In other words, that multiple-- or this is the way\nthat I would state it--",
    "start": "592680",
    "end": "598230"
  },
  {
    "text": "but that multiple\ndifferent embeddings, which produce different vector spaces,\ncan be leveraged in parallel",
    "start": "598230",
    "end": "604440"
  },
  {
    "text": "to create further robustness\nfor the model or in the way that it's implemented that if a\nrandom initialization doesn't do",
    "start": "604440",
    "end": "613320"
  },
  {
    "text": "that well, you can eliminate\nit from the network, and that subnetwork will do\njust as well, even after it's",
    "start": "613320",
    "end": "618780"
  },
  {
    "text": "totally trained. In other words, having\nmultiple clones,",
    "start": "618780",
    "end": "624360"
  },
  {
    "text": "self-attention heads, which have\nno difference in the outputs that they're trying\nto predict, is at the root of the\nlottery ticket hypothesis.",
    "start": "624360",
    "end": "631920"
  },
  {
    "text": "And ultimately, that\ninvocation of the lottery ticket hypothesis is really a\njustification for eliminating",
    "start": "631920",
    "end": "637672"
  },
  {
    "text": "parameters through\nsubstantial cost of training are essentially wasted as a\nresult of random parameter",
    "start": "637672",
    "end": "643920"
  },
  {
    "text": "initialization. You might ask\nquestions like, well, what is a good initialization?",
    "start": "643920",
    "end": "649480"
  },
  {
    "text": "What is a good set of\nword embeddings to use? ",
    "start": "649480",
    "end": "655410"
  },
  {
    "text": "So how can a lottery\nticket hypothesis interact with effects of randomly\ninitialized embedding layers",
    "start": "655410",
    "end": "662340"
  },
  {
    "text": "be avoided when\nconstructing language models is another question that\nis embedded in this discussion.",
    "start": "662340",
    "end": "668860"
  },
  {
    "text": " But we shouldn't say that\ndimensionality reduction isn't",
    "start": "668860",
    "end": "675420"
  },
  {
    "text": "needed. It's incredibly necessary. For language modeling,\nyou absolutely",
    "start": "675420",
    "end": "681040"
  },
  {
    "text": "have to work with\nreduced dimension unless you're in a\nvery small vocabulary. For example, like 26 Latin\ncharacters or something",
    "start": "681040",
    "end": "689407"
  },
  {
    "text": "like that, like a wav2vec.  The inherent input dimension\nof a large vocabulary model",
    "start": "689407",
    "end": "696820"
  },
  {
    "text": "presents many computational\ninteract abilities when designing NLP systems,\nsomething that you're probably",
    "start": "696820",
    "end": "702220"
  },
  {
    "text": "all very aware of. Likewise, though, the\ndistance from embedding layers",
    "start": "702220",
    "end": "707650"
  },
  {
    "text": "to learning information,\nthe loss at outputs puts them in a challenging\nposition to train.",
    "start": "707650",
    "end": "713990"
  },
  {
    "text": "It's really hard to learn\nembedding layers because of the indeterminacy\nin the space",
    "start": "713990",
    "end": "720040"
  },
  {
    "text": "that you're trying to learn. You could swap dimensions\nand it's equivalent. ",
    "start": "720040",
    "end": "726699"
  },
  {
    "text": "But the distance means\nthat they receive learning information last.",
    "start": "726700",
    "end": "733490"
  },
  {
    "text": "This is a real\nchallenge, and it's present in the history\nof NLP and deep learning to vanishing\ngradient stuff.",
    "start": "733490",
    "end": "743959"
  },
  {
    "text": "And this is\nexacerbated in the way that we have to\nactually learn embedding layers in standard models where\nwe might modify learning rates",
    "start": "743960",
    "end": "751490"
  },
  {
    "text": "to be lower all the way back\nat the bottom of a network, to be gentle with\nthose embedding layers",
    "start": "751490",
    "end": "756889"
  },
  {
    "text": "and help them learn effectively. But this is really\ntrouble because if we",
    "start": "756890",
    "end": "762800"
  },
  {
    "text": "had a good embedding\nlayer at the start, those subsequent layers could\nbe much easier to learn.",
    "start": "762800",
    "end": "769209"
  },
  {
    "text": " So ultimately, in order to\napproach this challenge,",
    "start": "769210",
    "end": "780350"
  },
  {
    "text": "we came along with a\ndiscernibility hypothesis. In other words, this\nboiled down to the theory",
    "start": "780350",
    "end": "785810"
  },
  {
    "text": "that low dimensional\nvectors, more than anything, needed to be able\nto discern features.",
    "start": "785810",
    "end": "791720"
  },
  {
    "text": "And that doesn't sound like\na very strong assertion. And we started with a really,\nreally, really low bar",
    "start": "791720",
    "end": "801529"
  },
  {
    "text": "and assume that the most common\nfeatures needed to be the most discernible features.",
    "start": "801530",
    "end": "807480"
  },
  {
    "text": "So if we're stuck\nwith a lower dimension and we can't give\neverything a one hot vector to be told apart\nvery well, then we",
    "start": "807480",
    "end": "814460"
  },
  {
    "text": "might want to give the\nmore clear vectors which have more dimensional\nindependencies to those features",
    "start": "814460",
    "end": "822410"
  },
  {
    "text": "which appear most frequently\nand could stand to confuse models the most.",
    "start": "822410",
    "end": "828740"
  },
  {
    "text": "This hypothesis led us directly\nto develop the bit-cipher algorithm, which is really just\na scheme for assigning vectors",
    "start": "828740",
    "end": "837740"
  },
  {
    "text": "of 0's and 1's. Nothing too crazy in terms of\nwhat we're attempting to do. In the figure at right here,\nthe order of vector assignment",
    "start": "837740",
    "end": "846290"
  },
  {
    "text": "is by row from top to bottom. And this is on a 5 dimension,\n5 bit-vector system.",
    "start": "846290",
    "end": "853940"
  },
  {
    "text": "The first 5 from bottom\nare those one-hot vectors. Past that point, you'll\nsee two-hot vectors,",
    "start": "853940",
    "end": "860990"
  },
  {
    "text": "but they're a little\nbit less darkly shaded, indicating the way that we\nactually utilize the system.",
    "start": "860990",
    "end": "867240"
  },
  {
    "text": "In other words, we normalize\nthem to have unit sum. ",
    "start": "867240",
    "end": "873710"
  },
  {
    "text": "What I hope you\ncan see from this is that the bit-cipher algorithm\ngeneralizes one-hot vectors",
    "start": "873710",
    "end": "880520"
  },
  {
    "text": "to low dimensions. And as a result, we can work\nfrom a very sparse feature set",
    "start": "880520",
    "end": "888050"
  },
  {
    "text": "and explore dimensionalities\nas a controlled phenomenon.",
    "start": "888050",
    "end": "893540"
  },
  {
    "text": "And this assignment is\nincredibly naive, too. That's the other thing that\nI want you to see, as well.",
    "start": "893540",
    "end": "899930"
  },
  {
    "text": "That this discernibility\nhypothesis does not create any meaningful\ncorrelations between tokens that behave similarly.",
    "start": "899930",
    "end": "906060"
  },
  {
    "text": "So if you've got the upper\nand lower case of a word, their vectors aren't going\nto capture those similarities",
    "start": "906060",
    "end": "912550"
  },
  {
    "text": "according to the bit-cipher. It's really just going\nto try and make sure that those features\nare distinguishable",
    "start": "912550",
    "end": "918100"
  },
  {
    "text": "in a low dimensional\nspace and that the most distinguishable\nfeatures are those which appear most commonly.",
    "start": "918100",
    "end": "924700"
  },
  {
    "text": "This was enough to do a\nsurprising amount of work. ",
    "start": "924700",
    "end": "931600"
  },
  {
    "text": "So with some scheme\nfor a deterministic low",
    "start": "931600",
    "end": "937779"
  },
  {
    "text": "dimensionalization\nprocedure, we were then able to utilize this\nsolution that we had actually",
    "start": "937780",
    "end": "943420"
  },
  {
    "text": "developed previously. So this was actually the real\nmotivator for a lot of the work that you're seeing today, while\nit might seem like it's just",
    "start": "943420",
    "end": "951370"
  },
  {
    "text": "a checkpoint in the middle.  Provided bit-cipher\nproduces decent embedding.",
    "start": "951370",
    "end": "957980"
  },
  {
    "text": "We can ask, can other layers\nbe non-randomly initialized? In other words, without\ngradient descent",
    "start": "957980",
    "end": "963010"
  },
  {
    "text": "or back propagation or other\ngradient-based iterative algorithms. This equation came about\nfrom analysis of word2vec",
    "start": "963010",
    "end": "971950"
  },
  {
    "text": "with the original softmax\nactivation function. And much like\nother articulations",
    "start": "971950",
    "end": "981130"
  },
  {
    "text": "of the word2vec,\nfamily of embeddings came up with differential\nsolutions that",
    "start": "981130",
    "end": "988180"
  },
  {
    "text": "depended on\nco-occurrence matrices, we formalized this\nas a question. Is there a way to take a\nco-occurrence matrix, F,",
    "start": "988180",
    "end": "996460"
  },
  {
    "text": "in this equation here, and\nconvert it with some weights,",
    "start": "996460",
    "end": "1002160"
  },
  {
    "text": "some denominators by\nrho into something that warms up a single\nlayer feed forward",
    "start": "1002160",
    "end": "1010880"
  },
  {
    "text": "in a neural network. And ultimately, this k minus 1\nover k term here and this sum",
    "start": "1010880",
    "end": "1019160"
  },
  {
    "text": "is really just\nexpressing something like conditional probability. Like conditional probability,\nbecause k minus 1 over k",
    "start": "1019160",
    "end": "1027800"
  },
  {
    "text": "is a wrinkle that says that\nas the number of features",
    "start": "1027800",
    "end": "1033290"
  },
  {
    "text": "increases, in other\nwords, the context window increases in a\nblock transformer,",
    "start": "1033290",
    "end": "1038990"
  },
  {
    "text": "then the warm\nstart that we could apply to start off a neural\nnetwork without a randomness,",
    "start": "1038990",
    "end": "1045410"
  },
  {
    "text": "randomness entirely\ndetermined by the vectors underneath nearing whatever\ndirection it's going.",
    "start": "1045410",
    "end": "1054290"
  },
  {
    "text": "All we have to do is\ncompute some co-occurrences between inputs and outputs. And I don't mean necessarily\nstandard co-occurrences",
    "start": "1054290",
    "end": "1061557"
  },
  {
    "text": "that you might have learned\nabout a long time ago, which depend on a radius. I mean, whatever your inputs\nare, whatever your outputs are,",
    "start": "1061557",
    "end": "1069680"
  },
  {
    "text": "you take their sum\nof outer products and you get a co-occurrence\nmatrix of inputs and outputs.",
    "start": "1069680",
    "end": "1077210"
  },
  {
    "text": "And that can then be utilized\nto initialize your layer in that neural network to be\nvastly more performant than what",
    "start": "1077210",
    "end": "1086750"
  },
  {
    "text": "you'd get by a random\ninitialization. This was a strong\nmotivator for us.",
    "start": "1086750",
    "end": "1093470"
  },
  {
    "text": "This was just for a\nsingle layer model, but it depended on the softmax\nfunction for activation.",
    "start": "1093470",
    "end": "1101450"
  },
  {
    "text": "And the softmax function,\nas an activation function, we knew is also necessary\nfor self-attention features.",
    "start": "1101450",
    "end": "1109790"
  },
  {
    "text": "And this meant that if we\ncould put self-attention into some kind of a standard\nform with this equation,",
    "start": "1109790",
    "end": "1116310"
  },
  {
    "text": "just like a single layer, then\nwe could apply the same solution",
    "start": "1116310",
    "end": "1122070"
  },
  {
    "text": "with one catch. That catch is\nspecifically that we don't know what the targets\nare for self-attention.",
    "start": "1122070",
    "end": "1128790"
  },
  {
    "text": "There is no target vector y. The thing that you're\ntrying to predict,",
    "start": "1128790",
    "end": "1134370"
  },
  {
    "text": "which position is\nthe one that you want to weight most strongly. And so in order to\napply this solution",
    "start": "1134370",
    "end": "1141450"
  },
  {
    "text": "for a self-attention model, we\nhad to do some more analysis. And that's in the reference\nnumber 1, which is all the way",
    "start": "1141450",
    "end": "1148320"
  },
  {
    "text": "back up in the first slide\nif you want to see it. But that derives a\ndifferential criterion,",
    "start": "1148320",
    "end": "1154590"
  },
  {
    "text": "an analog for the\nsingle layer solution that tells us what the targets\nof that kind of self-attention",
    "start": "1154590",
    "end": "1161790"
  },
  {
    "text": "actually are. The hidden targets, the weights\nthat you're trying to create, which really are just\nabout making sure",
    "start": "1161790",
    "end": "1168659"
  },
  {
    "text": "that the layer\nabove self-attention has some unsurprising\nthings coming towards it.",
    "start": "1168660",
    "end": "1176260"
  },
  {
    "text": "The self-attention layer\nis really just trying to massage the vectors,\nso that way they look like something that the\nnext layer above expects.",
    "start": "1176260",
    "end": "1184560"
  },
  {
    "text": "Aside from that, though, it's a\nmuch more in-depth conversation. The point, though, is that for\nthe model in this picture here,",
    "start": "1184560",
    "end": "1197029"
  },
  {
    "text": "we can now start off with\nvectors x that are not random.",
    "start": "1197030",
    "end": "1204320"
  },
  {
    "text": "We can use those vectors x\nto initialize non-randomly",
    "start": "1204320",
    "end": "1210440"
  },
  {
    "text": "the parameters in w, the\nself-attention matrix, and then use that\ngoing up the network",
    "start": "1210440",
    "end": "1215990"
  },
  {
    "text": "to initialize the\nparameters in u since it's just a feed forward layer\nwith whatever self-attention",
    "start": "1215990",
    "end": "1222170"
  },
  {
    "text": "is giving it his weights. And then whatever that\nproduces, the hidden state h,",
    "start": "1222170",
    "end": "1227750"
  },
  {
    "text": "we can use that with the actual\ntargets after the output layer",
    "start": "1227750",
    "end": "1233780"
  },
  {
    "text": "to warm up the matrix O.\nAnd you might say OK, well,",
    "start": "1233780",
    "end": "1238850"
  },
  {
    "text": "how did you figure out what\nthose hidden targets are? Like you had to have an output\nfor the U matrix to try and hit.",
    "start": "1238850",
    "end": "1248539"
  },
  {
    "text": "That too is something\nthat the bit-cipher",
    "start": "1248540",
    "end": "1254300"
  },
  {
    "text": "can provide in the form\nof label embeddings. In other words, low\ndimensional targets of the thing that is downstream\nthat you're trying to hit",
    "start": "1254300",
    "end": "1262250"
  },
  {
    "text": "the language model's output. So similarly, we can\nwarm start the U matrix",
    "start": "1262250",
    "end": "1267650"
  },
  {
    "text": "in terms of those\nbit-cipher label embeddings. ",
    "start": "1267650",
    "end": "1275880"
  },
  {
    "text": "So in this view, the aim is\nto show how simple and general the single layer softmax\nactivated solution is to apply.",
    "start": "1275880",
    "end": "1282450"
  },
  {
    "text": "It's really just no more\nchallenging than computing conditional probability\ngiven inputs and outputs.",
    "start": "1282450",
    "end": "1289920"
  },
  {
    "text": "It's fast. It's something that\nyou can distribute in terms of processing.",
    "start": "1289920",
    "end": "1295350"
  },
  {
    "text": "And it's very, very general. So this is essentially\nthe process",
    "start": "1295350",
    "end": "1304390"
  },
  {
    "text": "that we're using in order to\nwarm up the W and U matrix.",
    "start": "1304390",
    "end": "1310300"
  },
  {
    "text": "There's the U matrix there. Starts out as zeros. So in other words, nothing.",
    "start": "1310300",
    "end": "1316539"
  },
  {
    "text": "No random values,\nno weights anywhere over the data, which\nis just borrowing",
    "start": "1316540",
    "end": "1323909"
  },
  {
    "text": "the dimension of this\ngigantic Y matrix that has all of the targets in\nit for the entire data set.",
    "start": "1323910",
    "end": "1331950"
  },
  {
    "text": "We simply just take\nthe outer products of whatever the hidden state\nthe input to that layer is,",
    "start": "1331950",
    "end": "1337350"
  },
  {
    "text": "assuming that the\nlower layers beneath it are also warmed up with whatever\nthe targets for that layer are.",
    "start": "1337350",
    "end": "1345120"
  },
  {
    "text": "Following that, it's really\njust about normalization and a logarithmic\ntransformation.",
    "start": "1345120",
    "end": "1351000"
  },
  {
    "text": "And that logarithm\nreally just emerges as a result of being an inverse\nto the exponential function,",
    "start": "1351000",
    "end": "1357360"
  },
  {
    "text": "which is a part of softmax,\npretty much all of softmax.",
    "start": "1357360",
    "end": "1363059"
  },
  {
    "text": "And that's really\nwhat brought us here. So what does warm\nstarting a network do?",
    "start": "1363060",
    "end": "1370410"
  },
  {
    "text": "This is going back to before\nwe had the bit-cipher algorithm",
    "start": "1370410",
    "end": "1376500"
  },
  {
    "text": "for dimensionality reduction. And we started out\nby just saying, OK, if we take a simple, simple\nlanguage model that only looks",
    "start": "1376500",
    "end": "1386280"
  },
  {
    "text": "at a radius of traditional\nco-occurrences as features, we can concatenate those\nvectors and feed them forward",
    "start": "1386280",
    "end": "1394470"
  },
  {
    "text": "for a language model's output. A completely random start, a\ncold start to a language model",
    "start": "1394470",
    "end": "1403530"
  },
  {
    "text": "is really just the size of\nthe vocabulary in perplexity. And those three lines here\nfor a few different radii",
    "start": "1403530",
    "end": "1412320"
  },
  {
    "text": "are demonstrating that point\nwith the point all the way at the top left-hand corner\nof this figure, cold starts.",
    "start": "1412320",
    "end": "1421020"
  },
  {
    "text": "In any of those cases when\nthe warm start is applied, the perplexity is immediately,\nautomatically lower.",
    "start": "1421020",
    "end": "1428590"
  },
  {
    "text": "And furthermore,\nthe trajectories that the updates follow continue\nin the same learning rate",
    "start": "1428590",
    "end": "1438070"
  },
  {
    "text": "and the same time\nto perform better than models that\nwere started cold.",
    "start": "1438070",
    "end": "1445690"
  },
  {
    "text": "If you have an early\nstopping criterion, similarly, early stopping\nwill more than just generally",
    "start": "1445690",
    "end": "1453309"
  },
  {
    "text": "engage first. And with a higher perplexity.",
    "start": "1453310",
    "end": "1459010"
  },
  {
    "text": "So this was the first\nindication that we had figured out something\nthat's very useful.",
    "start": "1459010",
    "end": "1464429"
  },
  {
    "text": " I saw some folks on Slido\nsay they're confused.",
    "start": "1464430",
    "end": "1471950"
  },
  {
    "text": "They're asking, are we talking\nabout an alternative approach to self-attention? We are. And that's all the\nway back at slide 1.",
    "start": "1471950",
    "end": "1478640"
  },
  {
    "text": " And it is the premise of\nthis whole conversation.",
    "start": "1478640",
    "end": "1485560"
  },
  {
    "text": "So here in this modified\nversion of self-attention, you might normally expect to\ndo a comparison of your inputs,",
    "start": "1485560",
    "end": "1493840"
  },
  {
    "text": "the matrix X. Whatever\nyour inputs are, they might be a whole\nblock of vectors or they might be--\nthis is self-attention.",
    "start": "1493840",
    "end": "1500540"
  },
  {
    "text": "It's not cross-attention where\nyou have different vectors that you're trying to attend.",
    "start": "1500540",
    "end": "1505960"
  },
  {
    "text": "And forgetting about\nthe values, which for us is the U matrix,\nthe keys and queries which",
    "start": "1505960",
    "end": "1515830"
  },
  {
    "text": "are the parameters\nfor self-attention are in the middle. They're in between the two\ncopies of the inputs X.",
    "start": "1515830",
    "end": "1525270"
  },
  {
    "text": "Each of those, you can view\nas some kind of a projection down to a dimension\nwhere they can interact.",
    "start": "1525270",
    "end": "1530561"
  },
  {
    "text": "And this is necessary\nfor something like cross-attention,\nwhere you might have different\ndimensionalities like X1 and X2",
    "start": "1530562",
    "end": "1537000"
  },
  {
    "text": "in two separate\ngroups of vectors if you're doing something\nlike machine translation. That's not necessary to\nthink about when you're just",
    "start": "1537000",
    "end": "1545340"
  },
  {
    "text": "looking to do a standard\nlanguage model that has to predict the next output\naccording to the inputs, which",
    "start": "1545340",
    "end": "1552660"
  },
  {
    "text": "are also outputs from\nprevious iterations. ",
    "start": "1552660",
    "end": "1558690"
  },
  {
    "text": "Two insights here. One that multiplying\nthe key and query",
    "start": "1558690",
    "end": "1564029"
  },
  {
    "text": "matrices Wk and Wq is just\nanother parameter matrix that's",
    "start": "1564030",
    "end": "1570660"
  },
  {
    "text": "implied. There aren't two\nparameter matrices there in the middle\nfor self-attention",
    "start": "1570660",
    "end": "1576480"
  },
  {
    "text": "in any effective way. There is a common\ndimension of comparison,",
    "start": "1576480",
    "end": "1581700"
  },
  {
    "text": "and that kind of just\nmoves stuff around. It creates degrees of\nfreedom so that optimization",
    "start": "1581700",
    "end": "1588059"
  },
  {
    "text": "can figure out what's the best\nweighting from comparisons. But the softmax\nfunction is strictly",
    "start": "1588060",
    "end": "1594030"
  },
  {
    "text": "operating on similarities\nof that comparison space.",
    "start": "1594030",
    "end": "1600060"
  },
  {
    "text": "It's not doing anything\nwith those similarities. It's just softmaxing them. It's just activating them.",
    "start": "1600060",
    "end": "1606059"
  },
  {
    "text": "So if it was a big similarity,\nit's a big attention value. In this equation,\nthere's no transformation",
    "start": "1606060",
    "end": "1614190"
  },
  {
    "text": "happening before those vectors\nare multiplied together. Inner products. So those vectors\nbetter be good vectors",
    "start": "1614190",
    "end": "1621220"
  },
  {
    "text": "that you're starting with, X\nand X transpose, the same thing. They better be vectors\nthat are comparable.",
    "start": "1621220",
    "end": "1628570"
  },
  {
    "text": "They can't be vectors\nfrom cross-attention where you're trying to translate\nfrom one language to another and they just don't\ninner product.",
    "start": "1628570",
    "end": "1634137"
  },
  {
    "text": "They're different dimensions. You could force it through\nif they were two differently trained embedding layers.",
    "start": "1634137",
    "end": "1639370"
  },
  {
    "text": "And they had the same\ndimension with this mechanism. And if you didn't, you could\nput those key and query matrices",
    "start": "1639370",
    "end": "1646480"
  },
  {
    "text": "back in between the 2x\nvectors, x blocks of vectors. ",
    "start": "1646480",
    "end": "1653179"
  },
  {
    "text": "But a lot of what's going\non here in this talk is trying to simplify and make\nmore efficient the architectures",
    "start": "1653180",
    "end": "1663550"
  },
  {
    "text": "that we need and the\nmechanisms that they utilize given what we know\nabout how language functions.",
    "start": "1663550",
    "end": "1672063"
  },
  {
    "text": "And that's a\ncritical piece there. We have assumptions\nthat we can make. If all we're doing\nis autoregression,",
    "start": "1672063",
    "end": "1677170"
  },
  {
    "text": "we don't need cross-attention\ndimensionalization in between.",
    "start": "1677170",
    "end": "1682920"
  },
  {
    "text": "That'll be the theme. In other words, that\ncan we use knowledge",
    "start": "1682920",
    "end": "1687990"
  },
  {
    "text": "that we have about\nthe way language functions to design better\nversions of architectures",
    "start": "1687990",
    "end": "1694080"
  },
  {
    "text": "that meet the needs of language\ninstead of being simply general? ",
    "start": "1694080",
    "end": "1700400"
  },
  {
    "text": "Just good. This is important. So if there are any questions\nhere, it's a good time.",
    "start": "1700400",
    "end": "1705698"
  },
  {
    "start": "1705698",
    "end": "1711120"
  },
  {
    "text": "We are there and there.",
    "start": "1711120",
    "end": "1716640"
  },
  {
    "text": "So we just talked briefly. This was for language. I'm thinking about\nlanguage models. It's a really simple\nlanguage model.",
    "start": "1716640",
    "end": "1723020"
  },
  {
    "text": "There's no\nself-attention here yet. This is really just\nevaluating that a warm start",
    "start": "1723020",
    "end": "1729630"
  },
  {
    "text": "in either the blue,\ngreen, or purple case does better than\nits partner, which",
    "start": "1729630",
    "end": "1735210"
  },
  {
    "text": "is a cold start of\nthe same architecture, same hyperparameters-- orange, reddish, and brown.",
    "start": "1735210",
    "end": "1744440"
  },
  {
    "text": "So three different models,\nregardless of how long your context is\nin each case here,",
    "start": "1744440",
    "end": "1749810"
  },
  {
    "text": "we see that a model which has\na non-random initialization by the equation presented\ntwo slides back from here",
    "start": "1749810",
    "end": "1758059"
  },
  {
    "text": "starts a network of with\na much lower perplexity.",
    "start": "1758060",
    "end": "1763560"
  },
  {
    "text": " The requirements to apply this\nsolution to a feedforward layer",
    "start": "1763560",
    "end": "1770470"
  },
  {
    "text": "of parameters is simply\nthat your inputs should not",
    "start": "1770470",
    "end": "1778480"
  },
  {
    "text": "have negative values. That's really all we\nhave to worry about.",
    "start": "1778480",
    "end": "1784060"
  },
  {
    "text": "So it becomes really easy to\nask questions like, well, what happens when you apply\nthis to other data",
    "start": "1784060",
    "end": "1790990"
  },
  {
    "text": "with non-negative values? Well, there's one\nlittle catch that we had to think about\nhere in this case.",
    "start": "1790990",
    "end": "1797380"
  },
  {
    "text": "And that is with the\nbit-cipher or one-hot vectors, we're controlling the\nnorms of the inputs",
    "start": "1797380",
    "end": "1804010"
  },
  {
    "text": "with standard embeddings,\nwith MNIST, for example, when you're trying to\npredict a handwritten digits",
    "start": "1804010",
    "end": "1812020"
  },
  {
    "text": "0 through 9 value. You don't get to assume\nnecessarily that all inputs have",
    "start": "1812020",
    "end": "1819160"
  },
  {
    "text": "the same norm. You can normalize the inputs,\nbut it doesn't necessarily",
    "start": "1819160",
    "end": "1824770"
  },
  {
    "text": "make sense to normalize\nthem to one when you're looking at images, for example. They're non-negative.",
    "start": "1824770",
    "end": "1830500"
  },
  {
    "text": "They have 0 through 255,\nfor example, in MNIST. And as a result, we\ncan put these data",
    "start": "1830500",
    "end": "1838190"
  },
  {
    "text": "through that same warm start. Now, one little caveat\nhere I've alluded",
    "start": "1838190",
    "end": "1845330"
  },
  {
    "text": "to about the norms of\nvectors is that we don't",
    "start": "1845330",
    "end": "1851090"
  },
  {
    "text": "know what that value of k is. In other words-- let me go back. ",
    "start": "1851090",
    "end": "1858230"
  },
  {
    "text": "You could look at\nit here or here. That's the number of\nfeatures per prediction,",
    "start": "1858230",
    "end": "1865809"
  },
  {
    "text": "which if you're looking at\nunit normed word vectors, is however big your context\nwindow is, because they all",
    "start": "1865810",
    "end": "1874570"
  },
  {
    "text": "have unit norm and\nthere's k of them. But if you're looking\nat just an image,",
    "start": "1874570",
    "end": "1880210"
  },
  {
    "text": "it's not clear if it's a\ncomposition of multiple vectors, if it's one vector, and how many\nit is, if it is a composition.",
    "start": "1880210",
    "end": "1888130"
  },
  {
    "text": "It just has a norm.  In application to\ndata like that,",
    "start": "1888130",
    "end": "1895860"
  },
  {
    "text": "that is what k becomes, the\naverage norm of an input.",
    "start": "1895860",
    "end": "1901410"
  },
  {
    "text": "And I'm regretting not\nputting a graph in this, but the paper that discusses\nthis shows that in the MNIST",
    "start": "1901410",
    "end": "1907080"
  },
  {
    "text": "data set, the exact\noptimal value of k is the average\nnorm of the inputs,",
    "start": "1907080",
    "end": "1914250"
  },
  {
    "text": "however you've\npreprocessed them. And that's how we generally\napply this rule when",
    "start": "1914250",
    "end": "1920270"
  },
  {
    "text": "we're warm starting systems and\nwe don't have unit norm vectors. And it was learned from studying\nthis model's application--",
    "start": "1920270",
    "end": "1928100"
  },
  {
    "text": "this solution's application\nto non-linguistic data. But as mentioned, the purpose\nwas always towards language.",
    "start": "1928100",
    "end": "1937510"
  },
  {
    "text": " So longer context\nwindows in principle",
    "start": "1937510",
    "end": "1944659"
  },
  {
    "text": "should provide models with more\ninformation than shorter context windows.",
    "start": "1944660",
    "end": "1950000"
  },
  {
    "text": "This means one should\nexpect that models perform better when context\nwindow length is longer,",
    "start": "1950000",
    "end": "1957590"
  },
  {
    "text": "theoretically. And this is\nessentially the reason for why self-attention\nwas initially developed.",
    "start": "1957590",
    "end": "1964620"
  },
  {
    "text": "Researchers wanted to improve\nlanguage models and context windows, providing more\ninformation were seen as the key",
    "start": "1964620",
    "end": "1970640"
  },
  {
    "text": "to that. In other words, the more\nfeatures, the more information, the more flexibility a model\ncan have and expressivity.",
    "start": "1970640",
    "end": "1981779"
  },
  {
    "text": "However, without\nfeature weights, models didn't simply get better\nwith long context windows, and feature weights\nand self-attention",
    "start": "1981780",
    "end": "1989009"
  },
  {
    "text": "were hypothesized to be needed. And this was proven back in\n2017 with the transformer",
    "start": "1989010",
    "end": "1994800"
  },
  {
    "text": "architecture.  In moving towards\nself-attention and transformer,",
    "start": "1994800",
    "end": "2001440"
  },
  {
    "text": "though, the primacy of the\ntransformer architecture's block context model casts a shadow\nover the use of other context",
    "start": "2001440",
    "end": "2010800"
  },
  {
    "text": "models. So, for example, if\nI were to ask here, is it clear to everyone that the\nstandard self-attention block",
    "start": "2010800",
    "end": "2021360"
  },
  {
    "text": "model of context is different\nthan the traditional notion of co-occurrences,\nwhich use a radius that",
    "start": "2021360",
    "end": "2027630"
  },
  {
    "text": "is not positionally anchored? It is the context model,\nthe positional anchoring",
    "start": "2027630",
    "end": "2034410"
  },
  {
    "text": "of the block context model\nthat gives it its information. It is not in all\nlikelihood anything else.",
    "start": "2034410",
    "end": "2046140"
  },
  {
    "text": "Now what you do with that\ncontext model matters. You can't just take\nthose vectors in a block,",
    "start": "2046140",
    "end": "2052440"
  },
  {
    "text": "add them together, and expect\na feed forward to do well. That's where self-attention\nis needed in order to figure out which vector needs\nthe best weight, most weight.",
    "start": "2052440",
    "end": "2063219"
  },
  {
    "text": "So what you'll also see\nin the architectures that are based on what\nI've already presented",
    "start": "2063219",
    "end": "2069129"
  },
  {
    "text": "is that we're\ninterested to explore how different models of\ncontext for language models",
    "start": "2069130",
    "end": "2074500"
  },
  {
    "text": "can be integrated in general,\nbecause they each provide",
    "start": "2074500",
    "end": "2079510"
  },
  {
    "text": "different information. And we all know that the\nstandard transformers block model of context requires a\nridiculous amount of information",
    "start": "2079510",
    "end": "2087489"
  },
  {
    "text": "and data in order to\nbecome effectively trained.",
    "start": "2087489",
    "end": "2092739"
  },
  {
    "text": "So the current spate of\ncontexts that we use,",
    "start": "2092739",
    "end": "2099870"
  },
  {
    "text": "top there might be the standard\ntransformer context that has a fixed positional block. And it takes the\nfirst 10 tokens,",
    "start": "2099870",
    "end": "2106230"
  },
  {
    "text": "for example, the second 10\ntokens, and the third 10 tokens each in different blocks.",
    "start": "2106230",
    "end": "2112800"
  },
  {
    "text": "And each of those is a group\nof contextualizing vectors.",
    "start": "2112800",
    "end": "2117870"
  },
  {
    "text": "The second one there that you\nsee with the r as a subscript is a radial model because\nthose do different things.",
    "start": "2117870",
    "end": "2124930"
  },
  {
    "text": "In other words,\nrather than assume you're looking at the first\n10 or the nth 10 features,",
    "start": "2124930",
    "end": "2130440"
  },
  {
    "text": "you pick a radius. And you say, what are the last\nr features, the last r vectors?",
    "start": "2130440",
    "end": "2136680"
  },
  {
    "text": "That can also have an\nattention distribution, a self-attention\ndistribution according to the exact same model\nthat's being presented.",
    "start": "2136680",
    "end": "2144810"
  },
  {
    "text": "It produces an entirely\nseparate context in the state, whatever\nyou want to call it,",
    "start": "2144810",
    "end": "2151530"
  },
  {
    "text": "which can be conjoined\nwith the block model to articulate features and be\ngiven to an output layer that",
    "start": "2151530",
    "end": "2160870"
  },
  {
    "text": "knows what to do with them\nwhen each has different values.",
    "start": "2160870",
    "end": "2166850"
  },
  {
    "text": "The concatenation of those\ndifferent context models keeps the information separate. So the output layer can decide\nwhich portion of the context",
    "start": "2166850",
    "end": "2174890"
  },
  {
    "text": "is useful for the prediction. This last one is getting really\ntraditional at the bottom.",
    "start": "2174890",
    "end": "2183079"
  },
  {
    "text": "It's what I refer to\nas a document model. If you've ever implemented\nsomething like a Naive Bayes",
    "start": "2183080",
    "end": "2191780"
  },
  {
    "text": "classifier or a term frequency\ninverse document frequency model, that's essentially\nwhat a document model is.",
    "start": "2191780",
    "end": "2198690"
  },
  {
    "text": "Add up your vectors,\nyou get something. Is it going to be the best\nfor predicting the next token?",
    "start": "2198690",
    "end": "2205680"
  },
  {
    "text": "Absolutely not. However, it's always different. And what that means\nis that even if you",
    "start": "2205680",
    "end": "2211460"
  },
  {
    "text": "wrap to the next block between\nthe radial and the document models, you have a unique\ncontext vector, even if you're",
    "start": "2211460",
    "end": "2219140"
  },
  {
    "text": "looking at the exact same block\nbecause the document has grown and the radius just says,\nwhat are the last three?",
    "start": "2219140",
    "end": "2225090"
  },
  {
    "text": "What are the last 10? As a result, when you\nincorporate different models",
    "start": "2225090",
    "end": "2230935"
  },
  {
    "text": "of context, you\ndon't really have to say that there's a\nfinite context window. It might not be very\ngood to make predictions",
    "start": "2230935",
    "end": "2237110"
  },
  {
    "text": "past the first block, but\nthat might be about how much data you've used,\nand it might be",
    "start": "2237110",
    "end": "2242610"
  },
  {
    "text": "about the hyperparameters\nfor each one of those models that you're applying. In other words, the radius\nand the block size like usual.",
    "start": "2242610",
    "end": "2248600"
  },
  {
    "text": " So far, the only embeddings\nthat I've suggested",
    "start": "2248600",
    "end": "2255420"
  },
  {
    "text": "are from this\nbit-cipher algorithm. And as I've\nexpressed there, they",
    "start": "2255420",
    "end": "2261090"
  },
  {
    "text": "don't capture any\nuseful similarities between similar tokens. The bit-cipher\nalgorithm, it doesn't",
    "start": "2261090",
    "end": "2267330"
  },
  {
    "text": "care if you're looking at the\nuppercase or lowercase version of a word. It doesn't see them as\nbearing any similarity,",
    "start": "2267330",
    "end": "2272527"
  },
  {
    "text": "even though there might\nbe used very similarly. So how can you\nutilize the bit-cipher",
    "start": "2272527",
    "end": "2281280"
  },
  {
    "text": "to create vectors\nfor tokens that have meaningful\nsimilarities between words",
    "start": "2281280",
    "end": "2288510"
  },
  {
    "text": "that are used similarly? And this is just backing off\nto the traditional methods",
    "start": "2288510",
    "end": "2296500"
  },
  {
    "text": "once again taking co-occurrences\nof bit-cipher vectors with whatever is there\nat the middle or center",
    "start": "2296500",
    "end": "2304859"
  },
  {
    "text": "of a co-occurrence model.  Normally, if you think\nabout one hot vectors,",
    "start": "2304860",
    "end": "2312140"
  },
  {
    "text": "a co-occurrence matrix is\nreally just the same thing, except now we just\nhave smaller vectors",
    "start": "2312140",
    "end": "2319190"
  },
  {
    "text": "with different dimensions\non, so to speak. And we normalize\nafter concatenating",
    "start": "2319190",
    "end": "2327440"
  },
  {
    "text": "these blocks of different\nradii from the bit-cipher to match the original input\nrequirements that we discovered",
    "start": "2327440",
    "end": "2337700"
  },
  {
    "text": "for the warm start solution. And that enables us to use\nthese just like we would",
    "start": "2337700",
    "end": "2343310"
  },
  {
    "text": "the original bit-cipher\nvectors, except now, just from the usual\nco-occurrence statistics,",
    "start": "2343310",
    "end": "2351260"
  },
  {
    "text": "you'll see that capital\nword and lowercase word have a lot of common usage.",
    "start": "2351260",
    "end": "2357160"
  },
  {
    "text": "And you know this works because\nyou've seen co-occurrences for a very long time.",
    "start": "2357160",
    "end": "2362520"
  },
  {
    "text": "And while they\nmight not normally be useful in our applications\nthese days with deep learning,",
    "start": "2362520",
    "end": "2369079"
  },
  {
    "text": "they can be imparted through\nthe bit-cipher algorithm to prescribe vectors as well.",
    "start": "2369080",
    "end": "2375160"
  },
  {
    "start": "2375160",
    "end": "2381619"
  },
  {
    "text": "So here's where things\nstart paying out in terms of speed and efficiency.",
    "start": "2381620",
    "end": "2387119"
  },
  {
    "text": " If you only have one\nlayer of self-attention,",
    "start": "2387120",
    "end": "2394500"
  },
  {
    "text": "then that means\nthat you don't need to worry about whatever weird\nexpressive stuff is happening",
    "start": "2394500",
    "end": "2401490"
  },
  {
    "text": "that similar inputs\nmight have slightly different hidden states.",
    "start": "2401490",
    "end": "2407110"
  },
  {
    "text": "Since that first layer is just\na set of static word embeddings,",
    "start": "2407110",
    "end": "2413050"
  },
  {
    "text": "the self-attention\nlayer is working off of static word embeddings.",
    "start": "2413050",
    "end": "2418150"
  },
  {
    "text": "And that means\neach pair of words have a fixed comparison\ngiven static word embeddings.",
    "start": "2418150",
    "end": "2425859"
  },
  {
    "text": "And that means if you want to\ncompute the quadratic features of self-attention, you can just\npre-compute them and pull them",
    "start": "2425860",
    "end": "2434080"
  },
  {
    "text": "from memory. This caching of\nvector comparisons is essentially reducing the\nself-attention layer's cost",
    "start": "2434080",
    "end": "2442960"
  },
  {
    "text": "from quadratic to\nlinear, since those values that we're\nusing to weight",
    "start": "2442960",
    "end": "2448090"
  },
  {
    "text": "the vectors for the\nfeedforward layer no longer require\ncomparison across the block.",
    "start": "2448090",
    "end": "2455569"
  },
  {
    "text": "They're already compared. So when our vectors are static,\nwhich is at inference time,",
    "start": "2455570",
    "end": "2461860"
  },
  {
    "text": "and if we're not learning\nthe embedding layers",
    "start": "2461860",
    "end": "2467180"
  },
  {
    "text": "parameters with iterative\ndifferential updates, then not only do we have\nto not track gradients",
    "start": "2467180",
    "end": "2476090"
  },
  {
    "text": "for the embedding\nlayer, but we don't even have to compute the\nvector comparisons. We can pre-compute them\nand just load them,",
    "start": "2476090",
    "end": "2482480"
  },
  {
    "text": "which is much, much faster. ",
    "start": "2482480",
    "end": "2491559"
  },
  {
    "text": "So we can reduce a lot of-- all the inference\nand training costs.",
    "start": "2491560",
    "end": "2497033"
  },
  {
    "text": "Not all of the training costs,\nsome of the training costs, because if we want to\nupdate those vectors, then we can't assume\ncached comparisons.",
    "start": "2497033",
    "end": "2505240"
  },
  {
    "text": "But it's a huge cost savings. This means that we can train\nthese self-attentive feedforward",
    "start": "2505240",
    "end": "2511539"
  },
  {
    "text": "unit models very quickly and\nwith good initializations.",
    "start": "2511540",
    "end": "2517060"
  },
  {
    "text": "But there are some other\nthings that we immediately observed while\ndeveloping these models, and that is the lack of\nrandomization produced models",
    "start": "2517060",
    "end": "2527620"
  },
  {
    "text": "which were quite effective\neven on small data. Now it doesn't mean that\ntraining on small data will let you generalize\nto everything else that's",
    "start": "2527620",
    "end": "2534622"
  },
  {
    "text": "out there in the world. In other words, training\non a small data set might produce a model which has\na surprisingly low perplexity",
    "start": "2534622",
    "end": "2540940"
  },
  {
    "text": "on its training set. But it doesn't mean\nthat you're going to be able to generalize and\nhave a language model that's",
    "start": "2540940",
    "end": "2546550"
  },
  {
    "text": "talking well from just hearing\na couple of thousand tokens. It does mean it will know\nthat couple of thousand tokens",
    "start": "2546550",
    "end": "2552470"
  },
  {
    "text": "very well, very quickly. ",
    "start": "2552470",
    "end": "2558309"
  },
  {
    "text": "But there is a challenge with\nusing self-attention still,",
    "start": "2558310",
    "end": "2563620"
  },
  {
    "text": "and that is the fact that\nthe block model of context often is not fully utilized\nsince many documents are",
    "start": "2563620",
    "end": "2573609"
  },
  {
    "text": "shorter than long context\nmodels or long context windows.",
    "start": "2573610",
    "end": "2578950"
  },
  {
    "text": "And these days, there are\nexceptionally long context windows. I'm not even\ntalking about those.",
    "start": "2578950",
    "end": "2585250"
  },
  {
    "text": "Many of the language modeling\nbenchmarks simply don't even go out to 1,000 words when it comes\nto contexts and you're looking",
    "start": "2585250",
    "end": "2590500"
  },
  {
    "text": "at a document to predict. So this has been a\nproblem for a while.",
    "start": "2590500",
    "end": "2599809"
  },
  {
    "text": "And it means that\nif you're going to pad your short\ndocuments, you're going to waste a lot of\nprediction on those paddings.",
    "start": "2599810",
    "end": "2608920"
  },
  {
    "text": "A lot of computation gets lost\njust for null information, essentially.",
    "start": "2608920",
    "end": "2614680"
  },
  {
    "text": "And the way that this is\noften relieved in some groups",
    "start": "2614680",
    "end": "2619809"
  },
  {
    "text": "and to great effect is\nby packing long contexts. So, for example, if you've got\na 100,000 token context window,",
    "start": "2619810",
    "end": "2628390"
  },
  {
    "text": "most documents will not\nbe 100,000 tokens long. What do you do with the rest of\nthat long context if you want",
    "start": "2628390",
    "end": "2634270"
  },
  {
    "text": "to use 1,000 tokens\nof good training data? You fill out the other 99,000\ntokens with a bunch of other",
    "start": "2634270",
    "end": "2641273"
  },
  {
    "text": "random documents that don't\nbelong anywhere near the first one. That's called packing.",
    "start": "2641273",
    "end": "2647050"
  },
  {
    "text": "Packing can be utilized without\nimpacting different documents",
    "start": "2647050",
    "end": "2654040"
  },
  {
    "text": "with each other, without\ncontaminating the information between documents. And that takes a lot of\nwork, but it can be done.",
    "start": "2654040",
    "end": "2661970"
  },
  {
    "text": "However, there are different\nstrategies that we could employ,",
    "start": "2661970",
    "end": "2667580"
  },
  {
    "text": "different engineering\ntricks that we could employ to make our\noperation of self-attention",
    "start": "2667580",
    "end": "2674360"
  },
  {
    "text": "more effective at any\nlength of document without having to deal\nwith this packing problem.",
    "start": "2674360",
    "end": "2680810"
  },
  {
    "text": "And that comes about by\ndynamically changing the context",
    "start": "2680810",
    "end": "2686930"
  },
  {
    "text": "length from some\nmaximum value, that's what you would normally set.",
    "start": "2686930",
    "end": "2692570"
  },
  {
    "text": "Just use the context\nthat you have. But you still have\nto create batches if you want to train\nmodels quickly.",
    "start": "2692570",
    "end": "2698640"
  },
  {
    "text": "And what that means is that\nthere's still some padding if you use this approach. But you can pad those short\ndocuments to set lengths.",
    "start": "2698640",
    "end": "2711230"
  },
  {
    "text": "Batch short documents together,\nbatch long documents together.",
    "start": "2711230",
    "end": "2716930"
  },
  {
    "text": "This means that we don't need\nto pack documents together",
    "start": "2716930",
    "end": "2722980"
  },
  {
    "text": "to make use of a\nlong context window. When a document is long, you\ncan let its context be long.",
    "start": "2722980",
    "end": "2728930"
  },
  {
    "text": "When a document\nis short, you can put it with other\nshort documents and just use a subset of those\nself-attention parameters.",
    "start": "2728930",
    "end": "2735970"
  },
  {
    "text": "And with traditional\nself-attention parameters, keys and queries, it would\nnever be a subset because it's a low dimensional\nthat, that matrix provides.",
    "start": "2735970",
    "end": "2744460"
  },
  {
    "text": "With this modified\nself-attention, though, there's a different\nshape to the weight matrix. And that's why it's a\nsubset of those parameters",
    "start": "2744460",
    "end": "2751210"
  },
  {
    "text": "that we have to utilize. And that might be something\nworth discussing afterwards. In other words, how\ndoes the difference",
    "start": "2751210",
    "end": "2757270"
  },
  {
    "text": "in shapes of\ndimensionalities between this and the standard self-attention\nweights shake out?",
    "start": "2757270",
    "end": "2764250"
  },
  {
    "text": " But we want to get to a\ndifferent point for the sake",
    "start": "2764250",
    "end": "2769970"
  },
  {
    "text": "of this conversation.  What is a model like\nthis useful for?",
    "start": "2769970",
    "end": "2777410"
  },
  {
    "text": "That should be a question\nthat you're asking. It's a question that\nwe've been asking. ",
    "start": "2777410",
    "end": "2785780"
  },
  {
    "text": "We're not entirely\ncertain yet how an extremely large\nmodel like this",
    "start": "2785780",
    "end": "2793550"
  },
  {
    "text": "will function on trillions\nof tokens, for example. In other words, can you\nexpect the same kinds",
    "start": "2793550",
    "end": "2799700"
  },
  {
    "text": "of outcomes like a\nChatGPT kind of thing from some of these models?",
    "start": "2799700",
    "end": "2806000"
  },
  {
    "text": "Human interaction and\nRLHF all the rest of that. Though it's something that\nwe're considering, but also",
    "start": "2806000",
    "end": "2815089"
  },
  {
    "text": "at different scales\ntoo, since those are performant on\ntheir own as well.",
    "start": "2815090",
    "end": "2821839"
  },
  {
    "text": "But for what?  So the point is, is that\nfrom what we've stress-tested",
    "start": "2821840",
    "end": "2830470"
  },
  {
    "text": "into the billions, models\ncan be trained very quickly on a relatively\nsmall GPU in ways",
    "start": "2830470",
    "end": "2836890"
  },
  {
    "text": "that we expect when we\ncache vector comparisons, we see really big\nspeed-ups. When",
    "start": "2836890",
    "end": "2842290"
  },
  {
    "text": "we don't cache\nthose comparisons, you see all of the growth\nin computation time",
    "start": "2842290",
    "end": "2849370"
  },
  {
    "text": "that you would expect from\nlonger context windows. ",
    "start": "2849370",
    "end": "2855750"
  },
  {
    "text": "This one here,\nthough, we're trying to make it really,\nreally, really small, the one called potato.",
    "start": "2855750",
    "end": "2861660"
  },
  {
    "text": "And that's because\nwe want to see if we can train a model\nfrom scratch since on very",
    "start": "2861660",
    "end": "2867150"
  },
  {
    "text": "little data, these models\ncan fit effectively with the initializations\nthat we've developed.",
    "start": "2867150",
    "end": "2875310"
  },
  {
    "text": "And with the purpose of\nstarting from scratch, starting with no data, we're\nthinking about edge computing",
    "start": "2875310",
    "end": "2882030"
  },
  {
    "text": "cases where we could deploy a\nlanguage model with a microphone so that a person can talk\nto it and just train it",
    "start": "2882030",
    "end": "2888960"
  },
  {
    "text": "from their own data, train\nit from their own speech to understand their speech. ",
    "start": "2888960",
    "end": "2899140"
  },
  {
    "text": "So between these,\nwe've explored a lot of different configurations,\ntrying to consider similarities",
    "start": "2899140",
    "end": "2905830"
  },
  {
    "text": "to what some standard\nconfigurations might look like. A couple thousand tokens\nin a context window, for example, to look something\nlike a GPT-2 style model.",
    "start": "2905830",
    "end": "2914230"
  },
  {
    "text": "Thinking about bit-cipher\nembeddings that are 500 dimensional or 1,000 dimensional\nto be something like a GPT-2.",
    "start": "2914230",
    "end": "2921230"
  },
  {
    "text": "That's again pointing towards\nthe big/large category of models",
    "start": "2921230",
    "end": "2926590"
  },
  {
    "text": "that we've experimented with. Beyond that, we haven't\nreally touched those scales",
    "start": "2926590",
    "end": "2932440"
  },
  {
    "text": "because our first objective is\nnot to make big, big language models and train chatbots.",
    "start": "2932440",
    "end": "2938900"
  },
  {
    "text": "We want to know, what can\nwe do with a small model since this is a relatively\nunique capability?",
    "start": "2938900",
    "end": "2944300"
  },
  {
    "start": "2944300",
    "end": "2949390"
  },
  {
    "text": "So what does training look like? To the best of our ability so\nfar, it's kind of hard to see,",
    "start": "2949390",
    "end": "2959050"
  },
  {
    "text": "but the first step is\nthat warm start where you train the bit-cipher and you\ntake a couple of splits of data",
    "start": "2959050",
    "end": "2967660"
  },
  {
    "text": "and you compute that warm start\nfor the self-attention layer and the feed forward layers.",
    "start": "2967660",
    "end": "2973810"
  },
  {
    "text": "In this case, which\nis really just using a 100 million token data\nset from the baby language",
    "start": "2973810",
    "end": "2980290"
  },
  {
    "text": "model challenge, which\nhas as an objective to see what language models\ncan do on a relatively",
    "start": "2980290",
    "end": "2989920"
  },
  {
    "text": "human scale of data. In other words,\n100 million tokens is something that a person might\nhear in 10 years of their life.",
    "start": "2989920",
    "end": "2998150"
  },
  {
    "text": "In 10 years of life, people\nbecome pretty proficient speakers. And can a language model\nbe trained at that scale?",
    "start": "2998150",
    "end": "3007620"
  },
  {
    "text": "The second stage after\nthe warm start happens is where the majority\nof training time",
    "start": "3007620",
    "end": "3013000"
  },
  {
    "text": "occurs, and yet is\nalso where training operates the most quickly.",
    "start": "3013000",
    "end": "3019213"
  },
  {
    "text": " At this stage, we find that\nfreezing vectors is important.",
    "start": "3019213",
    "end": "3026230"
  },
  {
    "text": "One, because it means that\nwe can train much quicker. So we can have the\nsubsequent layers optimized",
    "start": "3026230",
    "end": "3031450"
  },
  {
    "text": "beyond their warm\nstarts very, very fast using that vector caching,\nthe vector comparison caching",
    "start": "3031450",
    "end": "3039730"
  },
  {
    "text": "to avoid the quadratic\ncosts of self-attention. This articulates the\nparameters in the middle layers",
    "start": "3039730",
    "end": "3046089"
  },
  {
    "text": "of the model for taking\n100 million tokens and making five passes over\nthe data here a lot quicker",
    "start": "3046090",
    "end": "3055990"
  },
  {
    "text": "than any of the other stages. And the comparison\nthat you'd make to this is the training time once those\nembedding layers are unfrozen,",
    "start": "3055990",
    "end": "3064900"
  },
  {
    "text": "where everything slows down\nto the normal speeds, where you have to do all of your\nvector comparisons on the fly",
    "start": "3064900",
    "end": "3071180"
  },
  {
    "text": "since you can't assume that the\nsame comparisons will always result in the same numbers\nsince model parameters might",
    "start": "3071180",
    "end": "3077950"
  },
  {
    "text": "be updated.  This is the best procedure\nthat we've figured out so far.",
    "start": "3077950",
    "end": "3084990"
  },
  {
    "text": "And in order to make\nthose vectors update, we find that learning rates\nhave to be adjusted dynamically",
    "start": "3084990",
    "end": "3090860"
  },
  {
    "text": "inside of the\nnetwork like normal, and that the embedding\nlayers are really tough to make progress on.",
    "start": "3090860",
    "end": "3097210"
  },
  {
    "text": " And you'll notice\nhere in this picture",
    "start": "3097210",
    "end": "3102390"
  },
  {
    "text": "that the slowness and the lack\nof stability, for example, in learning the embedding layer,\nonce it had been prescribed",
    "start": "3102390",
    "end": "3109019"
  },
  {
    "text": "earlier, makes it really hard to\ntrain over the entire data set",
    "start": "3109020",
    "end": "3114540"
  },
  {
    "text": "compared to five passes, for\nexample, in the middle phase when the middle and upper\nparameters are being updated",
    "start": "3114540",
    "end": "3120599"
  },
  {
    "text": "still with backpropagation. And the other thing\nthat I would highlight",
    "start": "3120600",
    "end": "3126120"
  },
  {
    "text": "before leaving this slide is\nin phase 1, how the warm start",
    "start": "3126120",
    "end": "3132930"
  },
  {
    "text": "saturates pretty quickly? So if you have 100\nmillion tokens, you really only need to apply\nthe warm start to something",
    "start": "3132930",
    "end": "3139589"
  },
  {
    "text": "like maybe 10 million\ntokens, not that much more. You don't see that much gain\nfrom that much more data.",
    "start": "3139590",
    "end": "3147570"
  },
  {
    "text": "That's not a bad\nthing, because it means that we\ndon't have to apply that process for any longer.",
    "start": "3147570",
    "end": "3154050"
  },
  {
    "text": "I mean, it would be great if it\ngave us all of the optimization that we could hope for,\nbut it's not something",
    "start": "3154050",
    "end": "3159220"
  },
  {
    "text": "that we could necessarily\nexpect since it's just an approximation of where\nthe parameters are headed.",
    "start": "3159220",
    "end": "3164435"
  },
  {
    "text": " So on the back of an\nenvelope, thinking about",
    "start": "3164435",
    "end": "3171359"
  },
  {
    "text": "how the systems\nthat, that example was trained on as compared\nto other examples that",
    "start": "3171360",
    "end": "3177720"
  },
  {
    "text": "are out there. And thinking about models\nthat are kind of similar size.",
    "start": "3177720",
    "end": "3184710"
  },
  {
    "text": "We're talking about a 12\ngigabyte gpu, a relatively small single chip, specifically when\nreferring to these training",
    "start": "3184710",
    "end": "3193920"
  },
  {
    "text": "times. So that's 12 gigabyte gpu.",
    "start": "3193920",
    "end": "3199590"
  },
  {
    "text": "Just working off of eight chips,\neach having a roughly four times the scale and\ncomparing to this time",
    "start": "3199590",
    "end": "3207060"
  },
  {
    "text": "that it took to train something\nwith maybe an additional order of magnitude, although\nwe have trained models up",
    "start": "3207060",
    "end": "3213270"
  },
  {
    "text": "to around 50 million\nparameters to which is getting towards GPT-2 scale.",
    "start": "3213270",
    "end": "3219519"
  },
  {
    "text": "We see training times\nthat if we scaled up to the relatively large\nsystems that present us",
    "start": "3219520",
    "end": "3225580"
  },
  {
    "text": "with how much work we should\nexpect to have to do for a model that large, we can expect to\nbe able to train much faster.",
    "start": "3225580",
    "end": "3233130"
  },
  {
    "text": "But as mentioned,\nthe initial objective here is not to simply figure\nout how well we can do something",
    "start": "3233130",
    "end": "3240059"
  },
  {
    "text": "that's being done well already. It's to figure out what these\nalternative strategies are",
    "start": "3240060",
    "end": "3245640"
  },
  {
    "text": "useful for, since they give\nus access to different regimes of model scale as effective.",
    "start": "3245640",
    "end": "3251920"
  },
  {
    "text": " So as mentioned, we've gone\nto relatively large amounts",
    "start": "3251920",
    "end": "3261750"
  },
  {
    "text": "of data. I wouldn't really call\nthem big data at this time, even though just\na couple of years ago, a billion tokens would be a\nrelatively large amount of data.",
    "start": "3261750",
    "end": "3270420"
  },
  {
    "text": "It's really just a stress\ntest at this point, gives us something like, do\nwe continue to see",
    "start": "3270420",
    "end": "3276060"
  },
  {
    "text": "models getting better as we\ncontinue to give them more data? Do we continue to\nsee models getting better as we continue to give\nthem longer context windows?",
    "start": "3276060",
    "end": "3284170"
  },
  {
    "text": "And the answer to both of those\nquestions is absolutely Yes. So nothing is telling\nus that we can't",
    "start": "3284170",
    "end": "3289589"
  },
  {
    "text": "train bigger models with these. But will those\nbigger models be as good as a standard\nself-attention model?",
    "start": "3289590",
    "end": "3295260"
  },
  {
    "text": "I don't know. It's a different\nself-attention parameter matrix than what you see in a\nstandard self-attention model.",
    "start": "3295260",
    "end": "3301080"
  },
  {
    "text": "You could integrate the two. And in theory, that\nshould be overkill because you'd have more\nparameters and more power",
    "start": "3301080",
    "end": "3308640"
  },
  {
    "text": "through them. And we can see\nfrom this work that the alternative\nself-attention parameters",
    "start": "3308640",
    "end": "3314260"
  },
  {
    "text": "are reasonably effective. We're getting close to time.",
    "start": "3314260",
    "end": "3320800"
  },
  {
    "text": "So I'll go quick\nthrough these since this is the work that we're\napproaching right now.",
    "start": "3320800",
    "end": "3327190"
  },
  {
    "text": "And this is the idea\nthat we're seeing as a use case for such\na model like this.",
    "start": "3327190",
    "end": "3334910"
  },
  {
    "text": "In other words, no pre-training. Just training on the\ntarget data, whatever",
    "start": "3334910",
    "end": "3340540"
  },
  {
    "text": "the data of interaction are. And this example, you'll\nsee that this relatively",
    "start": "3340540",
    "end": "3345730"
  },
  {
    "text": "small or precision\nlanguage model just needs to predict whether or not\na light should go on or off.",
    "start": "3345730",
    "end": "3351789"
  },
  {
    "text": "A lamp that listens with\na microphone and a switch, and you can use that\nswitch to train the lamp.",
    "start": "3351790",
    "end": "3357615"
  },
  {
    "text": " So that's the goal here.",
    "start": "3357615",
    "end": "3364420"
  },
  {
    "text": "Can pre-training be eliminated? And we want to\nanticipate whether or not you're going to flip\nthe light on or off.",
    "start": "3364420",
    "end": "3372100"
  },
  {
    "text": "That's the task that we're\ngoing to try and approach, or that rather, we're\ncurrently approaching.",
    "start": "3372100",
    "end": "3379070"
  },
  {
    "text": "There's a few different\nprocesses that integrate into this approach. There has to be a\nmicrophone that's listening",
    "start": "3379070",
    "end": "3385820"
  },
  {
    "text": "to you, recording audio. There has to be a\ntranscription algorithm. And we use wav2vec at\nthis point because there's",
    "start": "3385820",
    "end": "3392000"
  },
  {
    "text": "a very small version of\nit that's character-based. And as a result doesn't even\nrequire you to use consistent",
    "start": "3392000",
    "end": "3397970"
  },
  {
    "text": "or it does require you to\nuse consistent language, but it doesn't even\nrequire you to use words since it's strictly phonetic.",
    "start": "3397970",
    "end": "3405470"
  },
  {
    "text": "There has to be-- and this is the bread\nand butter of what's going on here, a process which\nanticipates what you want.",
    "start": "3405470",
    "end": "3414290"
  },
  {
    "text": "And that process is responsible\nfor creating good training data. So this is a smart data\ncollection algorithm",
    "start": "3414290",
    "end": "3421970"
  },
  {
    "text": "that figures out when\nyou flip the switch, is that the target for\nsomething that you just said?",
    "start": "3421970",
    "end": "3428430"
  },
  {
    "text": "Is that the transfer\nlearning objectives from text that was transcribed\nthat it anticipates you want?",
    "start": "3428430",
    "end": "3439770"
  },
  {
    "text": "Following this, there's\nalso two other processes, one which operates on\na different time cycle,",
    "start": "3439770",
    "end": "3446760"
  },
  {
    "text": "and that's training. So always train a model,\nalways be training a model.",
    "start": "3446760",
    "end": "3452700"
  },
  {
    "text": "Whenever there is new\ndata is essentially what that fourth process says. And the last one is operation.",
    "start": "3452700",
    "end": "3459055"
  },
  {
    "text": "In other words, if\nyou flip the switch, there has to be a process\nwhich operates the light bulb. It always has to be a lamp\nin order to be useful.",
    "start": "3459055",
    "end": "3465910"
  },
  {
    "text": "It always has to be able\nto just be a switch.",
    "start": "3465910",
    "end": "3471359"
  },
  {
    "text": "However, that operation\nprocess likewise needs to see a directive\nfrom the anticipator.",
    "start": "3471360",
    "end": "3477550"
  },
  {
    "text": "If the language model\npredicts that you just said a thing, that means\nyou want there to be light,",
    "start": "3477550",
    "end": "3483210"
  },
  {
    "text": "that operator then needs\nto receive the signal from the anticipator and\nexecute the directive.",
    "start": "3483210",
    "end": "3491260"
  },
  {
    "text": "If the user then though\nwithin some time scale",
    "start": "3491260",
    "end": "3496800"
  },
  {
    "text": "changes the switch back\nafter the model created a prediction that\nwas bad, the operator",
    "start": "3496800",
    "end": "3502079"
  },
  {
    "text": "is also responsible for issuing\na correction to the anticipator to correct the training data.",
    "start": "3502080",
    "end": "3510109"
  },
  {
    "text": "What this looks like as a\nprocess is in this diagram here. And you can see the\nflow here from stage 1,",
    "start": "3510110",
    "end": "3519240"
  },
  {
    "text": "a verbal command maybe gets\nrecorded, transcribed, turned, in a text.",
    "start": "3519240",
    "end": "3524890"
  },
  {
    "text": "And if there's no model\nthat's yet trained, that text is just stored as\ndata along with any directives",
    "start": "3524890",
    "end": "3530810"
  },
  {
    "text": "given by the user in the form of\na light switch going on or off. ",
    "start": "3530810",
    "end": "3536750"
  },
  {
    "text": "Once there's any data,\nthe learning process says, OK, time to\ntrain a language model",
    "start": "3536750",
    "end": "3543290"
  },
  {
    "text": "and integrate it\nwith these targets. Once a model is\ndone training, it's",
    "start": "3543290",
    "end": "3549560"
  },
  {
    "text": "sent over to the anticipator\nwho is responsible for using the language model.",
    "start": "3549560",
    "end": "3555080"
  },
  {
    "text": "That small language\nmodel then is now empowered to make\npredictions every single time",
    "start": "3555080",
    "end": "3562100"
  },
  {
    "text": "it receives a text command. And those predictions are\nsent to the operator, which",
    "start": "3562100",
    "end": "3568910"
  },
  {
    "text": "then does whatever it's told. And the last thing\nthat can happen, step 6 is if the wrong\nprediction was made",
    "start": "3568910",
    "end": "3576300"
  },
  {
    "text": "and the user fixes it by turning\noff the light because they didn't want the light\non, that corrects",
    "start": "3576300",
    "end": "3583260"
  },
  {
    "text": "the data that was transcribed. And the next model\nwhich is trained will be able to\navoid that problem.",
    "start": "3583260",
    "end": "3588480"
  },
  {
    "text": "And there's some\ndialing this in, in terms of the time scales that\nyou want based on the way humans",
    "start": "3588480",
    "end": "3594660"
  },
  {
    "text": "interact with the light switch. So there's a lot\nof development that goes into figuring out the\nright way to set this up.",
    "start": "3594660",
    "end": "3601140"
  },
  {
    "text": "The data that you collect\nfrom a process like this, how do we organize it?",
    "start": "3601140",
    "end": "3606779"
  },
  {
    "text": "This actually is not\ntransfer learning. So I kind of lied\nthere a little bit. This is strictly\nlanguage modeling.",
    "start": "3606780",
    "end": "3613860"
  },
  {
    "text": "It's a conversation between\nthe human and the lamp. You say something, the lamp\nsays here's what you want.",
    "start": "3613860",
    "end": "3620105"
  },
  {
    "text": " And it's just extending\ncontext window",
    "start": "3620105",
    "end": "3626150"
  },
  {
    "text": "like you'd see with a decoder\nonly kind of architecture these days.",
    "start": "3626150",
    "end": "3631460"
  },
  {
    "text": "A chatbot kind of thing. A human personal assistant. Human assistant dialogue.",
    "start": "3631460",
    "end": "3638630"
  },
  {
    "text": "And you might also\nsuspect then that, well, couldn't you let the lamp talk? Yes.",
    "start": "3638630",
    "end": "3644090"
  },
  {
    "text": "You could absolutely\nlet it use other tokens. And that is something which\nis on the horizon for us. In other words, how to determine\nonce the model is learned enough",
    "start": "3644090",
    "end": "3653030"
  },
  {
    "text": "and knows when you\nwant to hear it talk and knows what you\nwant to hear it say,",
    "start": "3653030",
    "end": "3658370"
  },
  {
    "text": "which requires other smart\ndata collection currently in development.",
    "start": "3658370",
    "end": "3664280"
  },
  {
    "text": "And there's three tags\nhere if you don't see it, although what they\nreally are tokens since they're integrated within\nthe language models vocabulary.",
    "start": "3664280",
    "end": "3673369"
  },
  {
    "text": "I want the lamp lit. I want the lamp dark or\nnothing, if no switch is",
    "start": "3673370",
    "end": "3678500"
  },
  {
    "text": "applied during transcription.  So one of the models looked\nlike that go into a lamp.",
    "start": "3678500",
    "end": "3688307"
  },
  {
    "text": "They're a little bit smaller\nthan that micro model in terms of having a long\ncontext window be the block",
    "start": "3688307",
    "end": "3694019"
  },
  {
    "text": "size. They still use\nthese other features like a radius, which\nhelp them to do well",
    "start": "3694020",
    "end": "3700470"
  },
  {
    "text": "with only little data. Those other context models. And the embedding sizes around\n50 or 100 and something.",
    "start": "3700470",
    "end": "3711970"
  },
  {
    "text": "And this is small enough\nto fit on a microprocessor on a CPU of a microprocessor,\nincluding training, no GPU",
    "start": "3711970",
    "end": "3719849"
  },
  {
    "text": "whatsoever.  And the first time we ever\ngot the interaction right,",
    "start": "3719850",
    "end": "3727080"
  },
  {
    "text": "the right time scales\nfrom no data whatsoever,",
    "start": "3727080",
    "end": "3733530"
  },
  {
    "text": "creating this data. And 20 minutes of it was enough. And you can see there's\nloads of misspellings",
    "start": "3733530",
    "end": "3739380"
  },
  {
    "text": "here because the transcription\nis not required to produce known words, known tokens.",
    "start": "3739380",
    "end": "3744630"
  },
  {
    "text": "It's strictly character-based. So you can say whatever\nyou want to say. You can whistle. And as long as wav2vec\nthinks that's tokens,",
    "start": "3744630",
    "end": "3751650"
  },
  {
    "text": "it'll figure out\nwhat to transcribe. ",
    "start": "3751650",
    "end": "3759380"
  },
  {
    "text": "That's enough, 20\nminutes of talking to it to have it know pretty well\nwhen you want the light on.",
    "start": "3759380",
    "end": "3767388"
  },
  {
    "text": "This is what the numbers look\nlike for that prediction. And you see lots of zeros there. But that's because there's\nno positive instances yet",
    "start": "3767388",
    "end": "3774740"
  },
  {
    "text": "in the data until\nyou flip the switch, there's nothing to predict. Once there is enough\nto predict, we",
    "start": "3774740",
    "end": "3782390"
  },
  {
    "text": "see an immediate jump\nin the model's ability to figure out whether\nlamp should stay on or off",
    "start": "3782390",
    "end": "3788210"
  },
  {
    "text": "or nothing. And while we trained this\nfirst model, for example,",
    "start": "3788210",
    "end": "3796410"
  },
  {
    "text": "in 20 minutes on potato,\nwhich is a really, really, really small microprocessor,\nit's incredibly frustrating",
    "start": "3796410",
    "end": "3805020"
  },
  {
    "text": "to utilize because the\nprocessing time is a couple seconds and it feels like\nit's going somewhere,",
    "start": "3805020",
    "end": "3810908"
  },
  {
    "text": "even though the data is entirely\nlocalized, there's no Wi-Fi, there's no internet connection.",
    "start": "3810908",
    "end": "3815940"
  },
  {
    "text": "It just takes the model on\nthis tiny chip a minute-- not really a minute,\nlike a couple seconds",
    "start": "3815940",
    "end": "3821700"
  },
  {
    "text": "to flip the switch on because\nit has to transcribe it, interpret it, issue\nthe directive,",
    "start": "3821700",
    "end": "3826920"
  },
  {
    "text": "ask the operator to operate it. And so part of what we're\ndoing is figuring out, at what scale of microprocessing\ndo the models that we're",
    "start": "3826920",
    "end": "3833819"
  },
  {
    "text": "developing really make a good\nreal-time system that a user can make use of? Well, and as you can see,\nthe larger the model in terms",
    "start": "3833820",
    "end": "3844770"
  },
  {
    "text": "of hyperparameters and so forth,\nthe more performance it gets. ",
    "start": "3844770",
    "end": "3851500"
  },
  {
    "text": "So we see these as potentially\nuseful in edge scenarios, but not just for\noperation, for training.",
    "start": "3851500",
    "end": "3858760"
  },
  {
    "text": "So go to Home depot, buy\na light switch, install it in your house,\nstart talking to it.",
    "start": "3858760",
    "end": "3864320"
  },
  {
    "text": " But this isn't\nreally the stopping point that we want to get to.",
    "start": "3864320",
    "end": "3870438"
  },
  {
    "text": " We want to eventually get\nto the point of talk back.",
    "start": "3870438",
    "end": "3876880"
  },
  {
    "text": "We want to treat these as\nlanguage models that essentially have a bit of U inside of them\nthat you can converse with.",
    "start": "3876880",
    "end": "3883720"
  },
  {
    "text": "And that's important to know\nwhen the model is aware of what",
    "start": "3883720",
    "end": "3889599"
  },
  {
    "text": "you want to hear said. In other words, it needs to know\nwhat is a good thing to say back",
    "start": "3889600",
    "end": "3894700"
  },
  {
    "text": "to what you just said? And the lamp has never\nheard a lamp talk before. So there are challenges\nto figuring out the lamp's",
    "start": "3894700",
    "end": "3901030"
  },
  {
    "text": "role in conversation. And choosing a lamp,\nthough, is arbitrary.",
    "start": "3901030",
    "end": "3908252"
  },
  {
    "text": "We don't have to make it\nbe a light bulb, which goes on and off. This could be a\ncontroller for anything which is a binary switch.",
    "start": "3908252",
    "end": "3915039"
  },
  {
    "text": "And you could imagine, like\nothers are looking at right now, there's a lot of\nopportunities with predicting",
    "start": "3915040",
    "end": "3921430"
  },
  {
    "text": "the action on your phone\nthat you want to take, which thing you want to push. And with a system like this,\nmicro-sizing on to your cell",
    "start": "3921430",
    "end": "3930910"
  },
  {
    "text": "phone for example,\nassumes better hardware than what we're already using,\nbut would be entirely localized,",
    "start": "3930910",
    "end": "3936890"
  },
  {
    "text": "including training. ",
    "start": "3936890",
    "end": "3943809"
  },
  {
    "text": "But this is also\nreally just getting to the point of feasibility.",
    "start": "3943810",
    "end": "3949150"
  },
  {
    "text": "It's not getting to the point of\na well optimized system, which we're still developing. There are in principle\ndifferent modifications",
    "start": "3949150",
    "end": "3956980"
  },
  {
    "text": "that we could make to the\nself-attention layers, which include traditional\nself-attention parameters.",
    "start": "3956980",
    "end": "3962109"
  },
  {
    "text": "That's just one example. Then there are updates\nto the very naive scheme",
    "start": "3962110",
    "end": "3967390"
  },
  {
    "text": "that we have for\nbit-cipher, the vectors that we're using to\ninitialize our models.",
    "start": "3967390",
    "end": "3972700"
  },
  {
    "text": "And a lot of other minutiae\nthat need to be approached.",
    "start": "3972700",
    "end": "3979150"
  },
  {
    "text": "So this isn't really\nwork that's done. It's a work in progress. And in addition to\nwhat I just described,",
    "start": "3979150",
    "end": "3986200"
  },
  {
    "text": "we're moving towards larger\nmodels and evaluations that compare better to modern\nsystems, which will eventually",
    "start": "3986200",
    "end": "3992920"
  },
  {
    "text": "come online. We'll most likely participate in\nthis year's baby language model",
    "start": "3992920",
    "end": "3998170"
  },
  {
    "text": "challenge. Although that challenge\nassumes you're working with a standard\narchitecture, which is already",
    "start": "3998170",
    "end": "4004599"
  },
  {
    "text": "developed for all of\nthe evaluative needs. So there's a lot of work to do.",
    "start": "4004600",
    "end": "4010726"
  },
  {
    "text": "But that's really all I have\nprepared for you to discuss today in this conversation. I've gone over a lot of details.",
    "start": "4010727",
    "end": "4016640"
  },
  {
    "text": "And if you'd like to\ntalk about any of these, I'm certainly happy to. Questions that you\nmight have as well.",
    "start": "4016640",
    "end": "4021920"
  },
  {
    "text": "And if you have\naccess to the slides, there's some links to\nthe different papers I've referenced.",
    "start": "4021920",
    "end": "4029380"
  },
  {
    "text": "That's all for today. Thanks. [APPLAUSE]",
    "start": "4029380",
    "end": "4037040"
  },
  {
    "text": "OK, so thanks, Jake,\nfor the great talk. And now we'll have some\ntime for questions. So if anybody here\nhas any questions,",
    "start": "4037040",
    "end": "4043580"
  },
  {
    "text": "feel free to raise\nyour hand and ask. Otherwise, we'll go to\nsome questions on Slido. ",
    "start": "4043580",
    "end": "4055932"
  },
  {
    "text": "Now some folks are\nasking, so we'll be posting the slides later. But I've also pasted these\nreferences in the Zoom",
    "start": "4055932",
    "end": "4063480"
  },
  {
    "text": "chat as well as Discord in\ncase anybody wants to see them. ",
    "start": "4063480",
    "end": "4073420"
  },
  {
    "text": "I was wondering, in the\nplots that you showed for warm start\nversus cold start, does the cold start use\nthe modified self-attention",
    "start": "4073420",
    "end": "4081910"
  },
  {
    "text": "or the standard self-attention? Yeah, sure.",
    "start": "4081910",
    "end": "4088150"
  },
  {
    "text": "So the question was in this\npicture comparing warm starts to cold starts.",
    "start": "4088150",
    "end": "4093340"
  },
  {
    "text": "What self-attention\nwas used here? None. This is strictly a\nfeedforward experiment, where we take a\nsingle layer and all",
    "start": "4093340",
    "end": "4100420"
  },
  {
    "text": "we do is feedforward with one\nhot vectors from some context window.",
    "start": "4100420",
    "end": "4105970"
  },
  {
    "text": "And concatenate them together. And the general\nproperty that you'll see",
    "start": "4105970",
    "end": "4111520"
  },
  {
    "text": "is by concatenating\nvectors, there's very little for attention to do. Normally with a block, you're\nadding the vectors together.",
    "start": "4111520",
    "end": "4119140"
  },
  {
    "text": "And that superposition of\nthe dimensions smears them. And that's why self-attention\nis needed in order",
    "start": "4119140",
    "end": "4125950"
  },
  {
    "text": "to weight that superposition. So just the right one stick\nout and it's not muddled.",
    "start": "4125950",
    "end": "4131219"
  },
  {
    "text": "If those vectors are\ninstead concatenated, a weighting of those is\nreally just appealing to the sensibilities\nof the matrix above.",
    "start": "4131220",
    "end": "4140750"
  },
  {
    "text": "When they're\nsuperimposed, there's a lot to work on\nsince you're smearing separate information together.",
    "start": "4140750",
    "end": "4147199"
  },
  {
    "text": "When the information\nis already separated, there's not that much\nreweighting can do.",
    "start": "4147200",
    "end": "4153528"
  },
  {
    "text": "And in this case, there's\nabsolutely no reweighting going on. And what I've described\nto you is really",
    "start": "4153529",
    "end": "4160009"
  },
  {
    "text": "just something\nthat's become very clear from a lot of\nsmall scale experiments",
    "start": "4160010",
    "end": "4166339"
  },
  {
    "text": "in between the models\nthat we've developed. And moving towards\nself-attention took additional\ntime, and we didn't",
    "start": "4166340",
    "end": "4174290"
  },
  {
    "text": "have a solution for that layer\nyet when this work was done. ",
    "start": "4174290",
    "end": "4182420"
  },
  {
    "text": "I had a question in\nregards to, so you're doing this with on edge\ncontrollers, right?",
    "start": "4182420",
    "end": "4187889"
  },
  {
    "text": "With what? You're doing with on\nedge controllers, right? You're doing training\nfor on edge controllers.",
    "start": "4187889",
    "end": "4193089"
  },
  {
    "text": "So this could be for\nIoT devices, right? Could be. And you talked about how\nthis all sorts of work",
    "start": "4193090",
    "end": "4198300"
  },
  {
    "text": "for image data, right? Yeah. Have you conducted any\ntests with image data,",
    "start": "4198300",
    "end": "4204690"
  },
  {
    "text": "with these small\nmodels for IoT devices or do you plan to the future?",
    "start": "4204690",
    "end": "4210030"
  },
  {
    "text": "So image data work best on not\njust feedforward architectures.",
    "start": "4210030",
    "end": "4216070"
  },
  {
    "text": "They have, for example,\nconvolutional things, bits and pieces that\nare useful to them. And that means if we want to\napply some kind of a warm start,",
    "start": "4216070",
    "end": "4226073"
  },
  {
    "text": "for example, a\nconvolutional layer to create a perform\nan image classifier or something that's\nworking with images,",
    "start": "4226073",
    "end": "4231390"
  },
  {
    "text": "we'd want to develop an\ninitialization for that layer too. It has weirder\nactivation functions,",
    "start": "4231390",
    "end": "4237270"
  },
  {
    "text": "which means we need to\nbranch out from softmax as an activation function. But surprisingly\nsimilar convolution",
    "start": "4237270",
    "end": "4244380"
  },
  {
    "text": "is to a radial model. It's really just saying\nwhat's near where",
    "start": "4244380",
    "end": "4249389"
  },
  {
    "text": "I'm trying to create a feature. So I would say yes, it\nseems like it's something",
    "start": "4249390",
    "end": "4254429"
  },
  {
    "text": "that we could do. But currently it's in\nthe phase of future work",
    "start": "4254430",
    "end": "4259710"
  },
  {
    "text": "that it fits in one bullet here. ",
    "start": "4259710",
    "end": "4268070"
  },
  {
    "text": "At the bottom. different layer types\nneed formal derivation for warm starts.",
    "start": "4268070",
    "end": "4273300"
  },
  {
    "text": "So if we wanted to do\nthis kind of a thing with a performance\narchitecture, we would be probably\nuniformly or randomly",
    "start": "4273300",
    "end": "4280118"
  },
  {
    "text": "initializing some\nof those parameters that we don't have\nwarm starts for yet. And as a result, we would\nreceive a lot of noise",
    "start": "4280118",
    "end": "4287810"
  },
  {
    "text": "and where things are going. And if we started to utilize\nthe activation functions,",
    "start": "4287810",
    "end": "4292910"
  },
  {
    "text": "whether it's even just\nlogistic activation, a logistic activation is not\nreally fundamentally different",
    "start": "4292910",
    "end": "4298429"
  },
  {
    "text": "than a softmax activation. So you might say,\nfor example, well, why can't you just apply that to\na logistic function like a two",
    "start": "4298430",
    "end": "4303785"
  },
  {
    "text": "dimensional softmax? And the reason is, is\nbecause if we treat it like a standard logistic, then\neach dimension is independent.",
    "start": "4303785",
    "end": "4311540"
  },
  {
    "text": "Each dimension is trying\nto predict the same thing. And there's a lot more\nquestions about how you can get different\ninformation out",
    "start": "4311540",
    "end": "4318080"
  },
  {
    "text": "of different dimensions. So that's a question that's\nreally worth spending time on,",
    "start": "4318080",
    "end": "4323750"
  },
  {
    "text": "in my opinion, separately. And it's not the\nfirst question that makes a lot of what we've\ndeveloped practical.",
    "start": "4323750",
    "end": "4329970"
  },
  {
    "start": "4329970",
    "end": "4335020"
  },
  {
    "text": "One of the slides we had the\ndialogue between the user. And I'm wondering,\ndoes that imply",
    "start": "4335020",
    "end": "4340840"
  },
  {
    "text": "there is a speech to text system\ninside the microcontroller",
    "start": "4340840",
    "end": "4346270"
  },
  {
    "text": "housing? So audio goes in. And there's a process here,\nwhich accepts that audio.",
    "start": "4346270",
    "end": "4354940"
  },
  {
    "text": "And it utilizes a\npre-trained wav2vet. That's really just fitting a\nneed with a pre-trained model.",
    "start": "4354940",
    "end": "4361970"
  },
  {
    "text": "That's what we're\ndoing right now. Although transcription\nis something that we would like to move\ninto in our future work",
    "start": "4361970",
    "end": "4369130"
  },
  {
    "text": "for the purposes of training\nfrom scratch, because one of the real benefits\nof a system like this",
    "start": "4369130",
    "end": "4374199"
  },
  {
    "text": "is that it doesn't\ncome with any biases from other people's\ndata aside from the fact",
    "start": "4374200",
    "end": "4379690"
  },
  {
    "text": "that there's a pre-trained\ntranscription system, which means that it's pre-trained\ntowards whatever phonetics were",
    "start": "4379690",
    "end": "4385510"
  },
  {
    "text": "within the language\nthat was there for pre-training in\nthe wav2vec algorithm.",
    "start": "4385510",
    "end": "4391030"
  },
  {
    "text": "So there is external\nutility here coming from a pre-trained model.",
    "start": "4391030",
    "end": "4397340"
  },
  {
    "text": "But the text itself\nand the language model that we're presenting\nis only working from what gets transcribed.",
    "start": "4397340",
    "end": "4404730"
  },
  {
    "start": "4404730",
    "end": "4413390"
  },
  {
    "text": "I have a follow-up on\nmy previous question. You said that the\nfeedforward warm start",
    "start": "4413390",
    "end": "4418610"
  },
  {
    "text": "is independent of the\nchoice of self-attention. Does that mean that\nthe warm start strategy",
    "start": "4418610",
    "end": "4423860"
  },
  {
    "text": "can be used for any network that\nuses a feedforward layer, not",
    "start": "4423860",
    "end": "4430670"
  },
  {
    "text": "just PLNs but any LLM\nor any other network? Yeah. So that's going back to the\nwarm start solution here.",
    "start": "4430670",
    "end": "4441560"
  },
  {
    "text": "And what it says is that in\nterms of any layer beneath, if you assume that those layers\nparameters are what they are,",
    "start": "4441560",
    "end": "4449210"
  },
  {
    "text": "you're not going to update them. And assuming that you know\nwhat the targets for that layer",
    "start": "4449210",
    "end": "4455030"
  },
  {
    "text": "are, which for middle layers,\nthere are some questions to be answered, then\nthis initialization",
    "start": "4455030",
    "end": "4462080"
  },
  {
    "text": "will do better than\nrandom for softmax output. That's really\nimportant at this stage",
    "start": "4462080",
    "end": "4468909"
  },
  {
    "text": "if there's a softmax as\na part of the activation. If there's not, then\nmore math, basically.",
    "start": "4468910",
    "end": "4478530"
  },
  {
    "text": "But the point at\nwhich it becomes clear",
    "start": "4478530",
    "end": "4483690"
  },
  {
    "text": "should that whatever type\nof prediction scenario in, as long as you have\nnon-negative features",
    "start": "4483690",
    "end": "4492450"
  },
  {
    "text": "and a softmax for\nactivation, like in this case with a single layer\nor even two softmax layers,",
    "start": "4492450",
    "end": "4500100"
  },
  {
    "text": "whatever that's\ndoing on MNIST, you can get a really\ngood initialization.",
    "start": "4500100",
    "end": "4506739"
  },
  {
    "text": "Doesn't have to be\nlinguistic data. ",
    "start": "4506740",
    "end": "4512900"
  },
  {
    "text": "And this can be mixed data, too. You could do an image\ncaption generation system that has both\nfeatures from images and text",
    "start": "4512900",
    "end": "4520160"
  },
  {
    "text": "and warms them up with the\nsame solution with entirely different data in two places.",
    "start": "4520160",
    "end": "4525880"
  },
  {
    "text": " Could you point out\nwhich part of the process",
    "start": "4525880",
    "end": "4531480"
  },
  {
    "text": "requires the values\nto be non-negative? ",
    "start": "4531480",
    "end": "4539110"
  },
  {
    "text": "What happens when you put\na negative in a logarithm? I'm not saying you\ncan't, but it's not",
    "start": "4539110",
    "end": "4544938"
  },
  {
    "text": "going to start making\nprobabilities for you at the other end of the\nsoftmax anytime fast.",
    "start": "4544938",
    "end": "4550000"
  },
  {
    "text": "So you have to start with a\ndifferent premise, essentially. And that premise is something\nthat requires more derivation.",
    "start": "4550000",
    "end": "4560710"
  },
  {
    "text": "You'd want to assure\nif you're going to use a logarithm anywhere\nor assume that inverse, that you're able\nto probably modify",
    "start": "4560710",
    "end": "4571210"
  },
  {
    "text": "every parameter\nindependently instead of whole rows of parameters. ",
    "start": "4571210",
    "end": "4580690"
  },
  {
    "text": "To get to a couple of questions\non Slido that folks asked. The first is what's\nthe difference",
    "start": "4580690",
    "end": "4586270"
  },
  {
    "text": "in performance between\nnaive assignment and optimized or omniscient\nassignment for packing",
    "start": "4586270",
    "end": "4591910"
  },
  {
    "text": "tokens into bit vectors and\nany experimental results? What's the difference\nin performance between--",
    "start": "4591910",
    "end": "4601090"
  },
  {
    "text": "Naive assignment and optimized\nassignment for packing tokens into bit vectors?",
    "start": "4601090",
    "end": "4607380"
  },
  {
    "start": "4607380",
    "end": "4613650"
  },
  {
    "text": "The performance differences\nare going to be in speed. The systems which utilize\npacking for contexts",
    "start": "4613650",
    "end": "4619740"
  },
  {
    "text": "have at great length\ngone to make sure that information from different\nportions of the context that",
    "start": "4619740",
    "end": "4625740"
  },
  {
    "text": "have nothing to\ndo with each other don't bleed\ninformation if you're going to pack them together.",
    "start": "4625740",
    "end": "4631409"
  },
  {
    "text": "That creates a lot of\nlogistical challenges in terms of defining models. And it's still just doing the\nregular self-attention thing.",
    "start": "4631410",
    "end": "4639520"
  },
  {
    "text": "So it's quadratic. So if you have the same\nlength of context window, it's going to be the\nsame computational cost.",
    "start": "4639520",
    "end": "4645540"
  },
  {
    "text": "However, if you pack all of\nyour small documents together,",
    "start": "4645540",
    "end": "4651090"
  },
  {
    "text": "they don't need the\nwhole context window worth of quadratic comparisons.",
    "start": "4651090",
    "end": "4658380"
  },
  {
    "text": "And that's why you pack\nsomething into the empty end. I guess it should be over\nhere in your perspective.",
    "start": "4658380",
    "end": "4665650"
  },
  {
    "text": "But token or document\npacking isn't exactly-- even though it's well\nknown as a mechanism",
    "start": "4665650",
    "end": "4673210"
  },
  {
    "text": "to make training\nmuch more efficient. In other words, you\nonly need fewer batches if more documents\nare packed together,",
    "start": "4673210",
    "end": "4680500"
  },
  {
    "text": "it's not something which is,\nfor example, entirely accepted",
    "start": "4680500",
    "end": "4686770"
  },
  {
    "text": "as a published accepted\nform of pre-processing. So what I would say is just\ndocument packing is not",
    "start": "4686770",
    "end": "4694690"
  },
  {
    "text": "a correct model of context. It is an efficiency, but\nrequires the same level",
    "start": "4694690",
    "end": "4699880"
  },
  {
    "text": "of quadratic comparison. Whereas dynamically\nbatching and utilizing",
    "start": "4699880",
    "end": "4705489"
  },
  {
    "text": "a block size that is dynamic\npreserves the model of context. It does something that\nis true to the objective",
    "start": "4705490",
    "end": "4712449"
  },
  {
    "text": "and unwavering in that. And it reduces the complexity\nfor smaller documents.",
    "start": "4712450",
    "end": "4719040"
  },
  {
    "text": "But direct comparison\nof the two is something I have\nnot done because it would require having that Oracle\nand utilizing those algorithms.",
    "start": "4719040",
    "end": "4726449"
  },
  {
    "text": "And where they're used, they're\nused with insanely big models, which means we\nwould likewise have",
    "start": "4726450",
    "end": "4732330"
  },
  {
    "text": "to compare to insanely\nbig models to create the same level of\nexpectation that people have from packing in the future.",
    "start": "4732330",
    "end": "4740199"
  },
  {
    "text": "Great. Thanks for your\ndetailed response. We have a question\nquickly that's asking, are there any\nimplementations of sample",
    "start": "4740200",
    "end": "4747120"
  },
  {
    "text": "available that one\ncould experiment with? Well, once we publish,\nthere will be.",
    "start": "4747120",
    "end": "4754980"
  },
  {
    "text": "But that requires a lot of\nwork on developing systems for evaluation, since\nthe evaluation systems",
    "start": "4754980",
    "end": "4760890"
  },
  {
    "text": "rely upon standardized functions\nwithin the architectures that you're all very familiar\nwith, like GPT-2 that are easily",
    "start": "4760890",
    "end": "4768450"
  },
  {
    "text": "taken for granted. Even though you do lots\nof work and training them, you have to do a lot of work in\ncreating those functions that",
    "start": "4768450",
    "end": "4775080"
  },
  {
    "text": "meet the needs of the\nseparate prediction tasks and fine-tuning that\nevaluations perform.",
    "start": "4775080",
    "end": "4781522"
  },
  {
    "text": "All right, great. Makes sense. I think we're pretty\nmuch out of time. So T thanks, Jake,\nfor the great talk.",
    "start": "4781522",
    "end": "4787850"
  },
  {
    "text": "And thanks for coming to\nanother lecture, all of you. ",
    "start": "4787850",
    "end": "4796000"
  }
]