[
  {
    "start": "0",
    "end": "5390"
  },
  {
    "text": "So the plan for\ntoday, We're going to be talking about\nmeta-learning. And first I'm going to recap\na little bit of what we talked",
    "start": "5390",
    "end": "13040"
  },
  {
    "text": "about on Monday with\nregard to the problem formulation and\nthe general recipe of meta-learning\nalgorithms, and then we're",
    "start": "13040",
    "end": "18980"
  },
  {
    "text": "going to actually get into\napproaches for solving few-shot learning problems.",
    "start": "18980",
    "end": "24500"
  },
  {
    "text": "And so this will\ncover what you'll be implementing in Homework\n1, although actually,",
    "start": "24500",
    "end": "29869"
  },
  {
    "text": "I think I saw a couple of people\nalready turned in Homework 1, so maybe you've\nalready implemented it.",
    "start": "29870",
    "end": "35288"
  },
  {
    "text": "And by the end of\nthe lecture, you'll be able to implement\nfew-shot learning algorithms and allow neural networks to do\nthe kind of few-shot learning",
    "start": "35288",
    "end": "43760"
  },
  {
    "text": "problem that you did on the\nvery first day of class.",
    "start": "43760",
    "end": "49019"
  },
  {
    "text": "Cool. So let's get into a\nlittle bit of the recap. So on Monday, we talked about\nthe meta-learning problem,",
    "start": "49020",
    "end": "54450"
  },
  {
    "text": "where our goal was\nthat we were given some data from some set\nof tasks and our goal",
    "start": "54450",
    "end": "60150"
  },
  {
    "text": "was to solve a new task with\nless data at a higher level of accuracy or more stablly.",
    "start": "60150",
    "end": "66593"
  },
  {
    "text": "And in the context\nof this course, we'll mostly be\nconsidering trying to learn tasks more\nquickly, that is",
    "start": "66593",
    "end": "71700"
  },
  {
    "text": "with less data than if we\nwere training from scratch. One of the key assumptions\nthat I mentioned",
    "start": "71700",
    "end": "77990"
  },
  {
    "text": "is that the tasks should\nbe drawn from the same task distribution. This is an assumption\nthat's needed",
    "start": "77990",
    "end": "83690"
  },
  {
    "text": "to show that these\nalgorithms might generalize and be able to learn new tasks.",
    "start": "83690",
    "end": "90295"
  },
  {
    "text": "Although, of\ncourse, in practice, it's a little bit difficult\nto sometimes actually realize this assumption in practice.",
    "start": "90295",
    "end": "96043"
  },
  {
    "text": "This is something\nthat's basically analogous to the\nstandard IID assumption that you see in machine\nlearning where you assume",
    "start": "96043",
    "end": "101990"
  },
  {
    "text": "that your training data points\nand your test data points are drawn from the\nsame distribution.",
    "start": "101990",
    "end": "107689"
  },
  {
    "text": "And so you might have some\ndistribution over tasks. You're given some training\ntasks from that distribution and a test task from\nthat distribution.",
    "start": "107690",
    "end": "115560"
  },
  {
    "text": "Like before, we want the\ntask to share some structure. And the task could correspond\nto lots of different things. So in Homework 1, the\ntask will correspond",
    "start": "115560",
    "end": "122820"
  },
  {
    "text": "to recognizing\nhandwritten digits from different\nlanguages, shown here.",
    "start": "122820",
    "end": "128100"
  },
  {
    "text": "We'll also look at an\nexample where different tasks correspond to giving\nfeedback to students",
    "start": "128100",
    "end": "133170"
  },
  {
    "text": "on different exam problems\nor different assignments. We'll look at one\nexample where the goal",
    "start": "133170",
    "end": "139500"
  },
  {
    "text": "is different tasks correspond to\ndifferent regions of the world. And you might want\nto classify species in those different\nregions of the world.",
    "start": "139500",
    "end": "146512"
  },
  {
    "text": "And the task could also\ncorrespond to something like robots performing\ndifferent tasks.",
    "start": "146512",
    "end": "153235"
  },
  {
    "text": "Now a natural question\nto ask from here is, we need a set of tasks in\norder to learn a new task--",
    "start": "153235",
    "end": "159520"
  },
  {
    "text": "how many tasks do\nyou need in order to be able to quickly\nlearn a new task?",
    "start": "159520",
    "end": "165120"
  },
  {
    "text": "There isn't really\nany cut-and-dry answer here, but in general,\nthe more the better.",
    "start": "165120",
    "end": "170160"
  },
  {
    "text": "In machine learning,\nthe more data you have, the better off you'll be. In meta-learning, the\nmore tasks you have,",
    "start": "170160",
    "end": "175450"
  },
  {
    "text": "the better off you'll be. So essentially,\nin meta-learning, we're treating data points as--",
    "start": "175450",
    "end": "181650"
  },
  {
    "text": "we're essentially treating\ntasks as data points. ",
    "start": "181650",
    "end": "188000"
  },
  {
    "text": "Yeah. [INAUDIBLE]",
    "start": "188000",
    "end": "193590"
  },
  {
    "start": "193590",
    "end": "208420"
  },
  {
    "text": "Yeah. So I guess we can\ndraw an analogy to machine learning, where\nmaybe we have a lot of images",
    "start": "208420",
    "end": "216410"
  },
  {
    "text": "taken from the internet. But then we ultimately will\nwant to pass on new images into our machine learning model.",
    "start": "216410",
    "end": "222180"
  },
  {
    "text": "And so those images don't\nmatch any of the images that are in the training data set. Here maybe as one example,\nmaybe different tasks",
    "start": "222180",
    "end": "230069"
  },
  {
    "text": "correspond to different users. So you want to build a spam\nclassifier for different users",
    "start": "230070",
    "end": "235739"
  },
  {
    "text": "and then the test task\nwill be a new person. And it will be a person that\nwasn't in the training data",
    "start": "235740",
    "end": "241120"
  },
  {
    "text": "set. And it will be trying\nto classify spam versus real email for\nthat particular new user.",
    "start": "241120",
    "end": "247375"
  },
  {
    "text": "Does that make sense?  So it'll be something that\nis, it should be generally",
    "start": "247375",
    "end": "253682"
  },
  {
    "text": "similar to the kinds of\nthings that you've seen before because we want to-- that's going to\nallow us to, that's",
    "start": "253682",
    "end": "260260"
  },
  {
    "text": "going to let us generalize\nto that new task with a small amount of data. But it is going to be--",
    "start": "260260",
    "end": "265840"
  },
  {
    "text": "it can be something that's new. That's fundamentally not seen\nin the training data set. Yeah.",
    "start": "265840",
    "end": "271180"
  },
  {
    "text": "Can meta learning be a\npart of two tasks which are seemingly unrelated in\ncase but maybe like potentially",
    "start": "271180",
    "end": "278680"
  },
  {
    "text": "exploit some sort\nof chain structure to use one task to\nhelp you do the other?",
    "start": "278680",
    "end": "284990"
  },
  {
    "text": "Yeah. So the question was\ncan meta learning be used for two tasks that\nhave kind of seemingly very",
    "start": "284990",
    "end": "290800"
  },
  {
    "text": "little in common but perhaps\nmaybe there is something that it can discover\nthat they have in common? So first, if you only train on\ntwo tasks, then generalizing",
    "start": "290800",
    "end": "299020"
  },
  {
    "text": "to a third task will be very\ndifficult. But I'm not sure that's exactly\nwhat you're asking. If some of the tasks\nin your distribution",
    "start": "299020",
    "end": "305092"
  },
  {
    "text": "are very different\nfrom one another and you have more\nthan two tasks, then meta learning will try\nto find the common structure",
    "start": "305092",
    "end": "311890"
  },
  {
    "text": "between them in a way that\nallows it to very quickly solve those tasks. ",
    "start": "311890",
    "end": "318764"
  },
  {
    "text": "And if there is\nshared structure, then it's explicitly\ngoing to optimize for trying to find\nthe things that are",
    "start": "318765",
    "end": "323889"
  },
  {
    "text": "in common between those tasks. Yeah. So in the case if there really\nisn't much structure could it",
    "start": "323890",
    "end": "331830"
  },
  {
    "text": "be detrimental to actually\nuse meta learning? So if there isn't that\nmuch shared structure, would it be detrimental\nto use meta learning?",
    "start": "331830",
    "end": "341110"
  },
  {
    "text": "So I don't know if it\nnecessarily be detrimental in the sense that it may have\na hard time finding that shared",
    "start": "341110",
    "end": "348780"
  },
  {
    "text": "structure.  In the worst,\nwell, I guess we'll",
    "start": "348780",
    "end": "353903"
  },
  {
    "text": "get into this when we\nget into the algorithms, but there are some algorithms\nwhere they will essentially revert to learning from scratch.",
    "start": "353903",
    "end": "360020"
  },
  {
    "text": "And so you won't do worse\nthan learning from scratch whereas there are some\nalgorithms, including the ones that we'll talk about today\nwhere they could do worse",
    "start": "360020",
    "end": "366718"
  },
  {
    "text": "than learning from scratch. Yeah. Are the data sets for each\nof the tasks in the tree set",
    "start": "366718",
    "end": "373700"
  },
  {
    "text": "about as large as you\nwould need and like a normal [? dictionary ?] class\nor can you plus data per task?",
    "start": "373700",
    "end": "379949"
  },
  {
    "text": "Yeah. So one of the things that's\nreally cool about this is actually you can get away\nwith much less data per task than if you were to train\ncompletely from scratch.",
    "start": "379950",
    "end": "387150"
  },
  {
    "text": "And so we'll see this actually\nin one of the data sets, including actually the\ndata set that you'll use in your homework.",
    "start": "387150",
    "end": "392515"
  },
  {
    "text": " Cool so one of the things\nthat we talked about on Monday",
    "start": "392515",
    "end": "400180"
  },
  {
    "text": "is that there's a\ncouple of different ways that you can view meta\nlearning algorithms, one from a\nmechanistic standpoint",
    "start": "400180",
    "end": "406610"
  },
  {
    "text": "and one from a more\nprobabilistic standpoint. And from the\nmechanistic standpoint, you can think of meta\nlearning as basically trying",
    "start": "406610",
    "end": "413900"
  },
  {
    "text": "to train a neural network\nto read as input data and give you predictions\nfor new data points.",
    "start": "413900",
    "end": "419190"
  },
  {
    "text": "And so you can think of it\nas implementing a learning procedure whereas the more\nprobabilistic view is thinking",
    "start": "419190",
    "end": "425690"
  },
  {
    "text": "about how we may try\nto learn a prior-- extract prior knowledge from\nyour set of training tasks",
    "start": "425690",
    "end": "432260"
  },
  {
    "text": "and impose that\nprior test time when trying to learn so that you\ncan learn with less data.",
    "start": "432260",
    "end": "440330"
  },
  {
    "text": "In the next few lectures,\nwe're going to really be focusing on the mechanistic\nview which will make it easier",
    "start": "440330",
    "end": "445443"
  },
  {
    "text": "to think about how\nto actually implement these algorithms in\npractice, but we'll come back to the probabilistic\nview in a few weeks.",
    "start": "445443",
    "end": "452645"
  },
  {
    "text": " So and then lastly,\nthe last thing",
    "start": "452645",
    "end": "457993"
  },
  {
    "text": "that we talked\nabout on Monday was looking at an example, a few\nshort classification example, where ultimately\nwe want it to be",
    "start": "457993",
    "end": "465830"
  },
  {
    "text": "able to classify new examples\nshown on the right here, given a very small\ntraining data set.",
    "start": "465830",
    "end": "471139"
  },
  {
    "text": "So the train data set\nonly has five data points. And we want to be able to\nuse that training data set",
    "start": "471140",
    "end": "477080"
  },
  {
    "text": "and previous experience\nfrom other image classes in order to effectively\nsolve and make predictions",
    "start": "477080",
    "end": "484700"
  },
  {
    "text": "for new test examples. And so the way\nthat we can do this is construct tasks that look\na lot like this test task.",
    "start": "484700",
    "end": "493940"
  },
  {
    "text": "So construct training\nsets and test sets from other image classes,\nfrom other experience or data",
    "start": "493940",
    "end": "499178"
  },
  {
    "text": "that we might have.  Run kind of meta\ntraining on these tasks",
    "start": "499178",
    "end": "506060"
  },
  {
    "text": "such that when we see a\nnew task at meta test time, we can learn from this type of\ndata set and make predictions.",
    "start": "506060",
    "end": "515760"
  },
  {
    "text": "Now-- and this is kind of an\nimage classification example, we can consider this for other\nmachine learning problems",
    "start": "515760",
    "end": "522630"
  },
  {
    "text": "as well. Now what I'd like\nto talk about next is first getting a\nlittle bit into some of the terminology that's\nused in meta learning",
    "start": "522630",
    "end": "531420"
  },
  {
    "text": "in the context of this example. And then we'll dive a little\nbit more into the setup.",
    "start": "531420",
    "end": "537580"
  },
  {
    "text": "So we have the setup\nwhere we have a number of different training tasks.",
    "start": "537580",
    "end": "542850"
  },
  {
    "text": "For each of those tasks we\nhave a training data set and a test data set.",
    "start": "542850",
    "end": "549790"
  },
  {
    "text": "These are sometimes also\ncalled the support set and the query set,\nthe support set you can think of as kind\nof providing support",
    "start": "549790",
    "end": "557010"
  },
  {
    "text": "for the learning process\nand then the query data set will be used to query the\npredictions of the model",
    "start": "557010",
    "end": "562890"
  },
  {
    "text": "after learning on\nthe support set. Using these latter\nterminology can",
    "start": "562890",
    "end": "567930"
  },
  {
    "text": "be somewhat helpful\nto differentiate between meta training\nand meta testing. ",
    "start": "567930",
    "end": "576130"
  },
  {
    "text": "So we've talked about this\nnotion of few shot learning. And so you can think of this\nas k short learning, where",
    "start": "576130",
    "end": "582690"
  },
  {
    "text": "you have k examples per class. Or if you're in a\nregression scenario,",
    "start": "582690",
    "end": "588360"
  },
  {
    "text": "you have k examples total. So k short learning\nmeans you essentially have k examples to learn from.",
    "start": "588360",
    "end": "595860"
  },
  {
    "text": "And then we'll consider--\nwe'll use n typically to denote the number\nof classes that we're",
    "start": "595860",
    "end": "602850"
  },
  {
    "text": "trying to classify between,\nthe number of classes that we're choosing between.",
    "start": "602850",
    "end": "608000"
  },
  {
    "text": "So with that in mind I\nhave a question for you. So maybe I want to ask you what\nis k and n for this example.",
    "start": "608000",
    "end": "615980"
  },
  {
    "text": "And so first in\nterms of k, maybe you can kind of\nraise your fingers",
    "start": "615980",
    "end": "623350"
  },
  {
    "text": "and, kind of, say\nwhat you think. Actually, let's\nactually start with n.",
    "start": "623350",
    "end": "628690"
  },
  {
    "text": "So maybe raise your\nhand with fingers to denote what you think\nn is for this example. ",
    "start": "628690",
    "end": "639300"
  },
  {
    "text": "Cool. So I'm seeing a mixed\nmix of fives and twos. But mostly fives.",
    "start": "639300",
    "end": "645760"
  },
  {
    "text": "Cool. So in this example, we were\ndoing n way classification. I guess-- so yeah, the\nmix of fives and twos is--",
    "start": "645760",
    "end": "651802"
  },
  {
    "text": "I can see where\nyou're coming from.  So this is a five way\nexample because we",
    "start": "651802",
    "end": "658470"
  },
  {
    "text": "have five different\nimage classes shown in the left training data set.",
    "start": "658470",
    "end": "663750"
  },
  {
    "text": "I'm only showing two\ntest examples here just in terms of\nspace on the slides.",
    "start": "663750",
    "end": "669420"
  },
  {
    "text": "But there are kind of five\nclasses underlying these-- underlying these\ntraining data sets.",
    "start": "669420",
    "end": "675750"
  },
  {
    "text": "And then can you put up a\nshow of numbers for what k is. ",
    "start": "675750",
    "end": "683139"
  },
  {
    "text": "Cool. I'm seeing ones which is good. So yeah, we're giving\nit basically one example of five different classes.",
    "start": "683140",
    "end": "689020"
  },
  {
    "text": "So this is a one shot\nlearning problem where you have one example per class. And we're trying to solve a\nfive way classification problem.",
    "start": "689020",
    "end": "696150"
  },
  {
    "start": "696150",
    "end": "701380"
  },
  {
    "text": "Cool. So that's a bit of\nthe terminology. Now in terms of the\nkind of general recipe",
    "start": "701380",
    "end": "714680"
  },
  {
    "text": "or maybe another view\non the learning problem, is if you think about\nsupervised learning, our goal is to map\nfrom inputs to outputs",
    "start": "714680",
    "end": "720950"
  },
  {
    "text": "and to learn a function that\npredicts the input-- predicts the output given the input.",
    "start": "720950",
    "end": "726260"
  },
  {
    "text": "And we do this with a set of\ninput/output pairs, x, y pairs.",
    "start": "726260",
    "end": "733170"
  },
  {
    "text": "Now in meta learning, we\ncan also actually frame it as somewhat of a\nsupervised learning",
    "start": "733170",
    "end": "738350"
  },
  {
    "text": "problem where our\ninputs are going to be a little bit different. So our inputs are actually\ngoing to include the training",
    "start": "738350",
    "end": "746850"
  },
  {
    "text": "data set for a task as well\nas in that training data set, we'll have k examples\nor k examples per class.",
    "start": "746850",
    "end": "756370"
  },
  {
    "text": "And in addition to a training\ndata set for that task, we're also going\nto have a new test",
    "start": "756370",
    "end": "761820"
  },
  {
    "text": "input that we want to be\nable to make predictions for. So the training data\nset is essentially",
    "start": "761820",
    "end": "766950"
  },
  {
    "text": "what was in the green\nbox on the previous slide and the test examples\nor the test example is one of the things that was\nin the blue box on the right.",
    "start": "766950",
    "end": "774690"
  },
  {
    "text": "And then we want to be able to\npredict the label for this test example.",
    "start": "774690",
    "end": "781350"
  },
  {
    "text": "And so you can think\nof this green arrow here as the process of learning\nfrom that data set to make",
    "start": "781350",
    "end": "786560"
  },
  {
    "text": "a prediction for a new example. And from this standpoint,\nwe could actually, again,",
    "start": "786560",
    "end": "793340"
  },
  {
    "text": "view this as a supervised\nlearning problem where we want to\ntrain a neural network to take as input a data set,\nlearn from that data set,",
    "start": "793340",
    "end": "799470"
  },
  {
    "text": "and make a prediction\non a new test example. ",
    "start": "799470",
    "end": "804830"
  },
  {
    "text": "Now how do we go about\nlearning this kind of function,",
    "start": "804830",
    "end": "810470"
  },
  {
    "text": "well, instead of having a data\nset of input/output pairs, we're actually going to have\na data set of data sets.",
    "start": "810470",
    "end": "818130"
  },
  {
    "text": "And each of these\ndata sets has to have at least-- have some\nmore than k examples so that you can sample k examples\nto be used for the training",
    "start": "818130",
    "end": "825270"
  },
  {
    "text": "data set and at\nleast one example to be used for the test set.",
    "start": "825270",
    "end": "830550"
  },
  {
    "text": " Yeah. I have [INAUDIBLE] an\naudition that we're using.",
    "start": "830550",
    "end": "837230"
  },
  {
    "text": "So we're [INAUDIBLE]\nclassification, generally in the testing since we have\nan example of the classes",
    "start": "837230",
    "end": "843889"
  },
  {
    "text": "that we're going to test it on. So you have one let's say\nenvironment classifier",
    "start": "843890",
    "end": "850139"
  },
  {
    "text": "between cats and dogs and\nI have one example of cat and one example of dog. And that is what\nwill be testing on.",
    "start": "850140",
    "end": "857390"
  },
  {
    "text": "But what you are using,\nthat seems like zero shot learning, where are not using--",
    "start": "857390",
    "end": "863030"
  },
  {
    "text": "you have five classes in this\nset of five different classes and actually what you\nare actually classifying",
    "start": "863030",
    "end": "870080"
  },
  {
    "text": "for are two different classes.  Yeah. So going back to the slide here\nor this visualization here,",
    "start": "870080",
    "end": "883160"
  },
  {
    "text": "so the number of\nexamples in this red box here doesn't matter too much.",
    "start": "883160",
    "end": "890330"
  },
  {
    "text": "You need there to be\nat least one example and you need to be able to-- and in general your test is\ngoing to be larger than two.",
    "start": "890330",
    "end": "897620"
  },
  {
    "text": "You'll probably have\nat least five examples so that you're actually\nevaluating its ability",
    "start": "897620",
    "end": "905089"
  },
  {
    "text": "to make predictions\nfor all five classes in your training data set. Three to five\nsupport classes there",
    "start": "905090",
    "end": "913970"
  },
  {
    "text": "for the testing one, and\none for classification. [INAUDIBLE] Like the model\nitself and classes are there",
    "start": "913970",
    "end": "924068"
  },
  {
    "text": "when you classify them, but\nyou don't want to classify, [INAUDIBLE] why do we need\nother classes and functions.",
    "start": "924068",
    "end": "934820"
  },
  {
    "text": "Oh, so I guess what\nI'm visualizing here is a five shot-- or sorry a five way\nclassification problem.",
    "start": "934820",
    "end": "941070"
  },
  {
    "text": "And so you have one example\nfor five different classes. And so it's a one\nshot learning problem.",
    "start": "941070",
    "end": "946769"
  },
  {
    "text": "And then technically\nif all you cared about was to classify\nthese two images, then it's actually a two\nway classification problem.",
    "start": "946770",
    "end": "954050"
  },
  {
    "text": "I would have shown\nall five examples if I had a little bit\nmore space on the slides. But yeah, I'm trying to frame\nthis as a five way problem.",
    "start": "954050",
    "end": "962630"
  },
  {
    "text": "So you can sort of just\nthink of this as having-- like you could imagine if\nthere's like five examples here. Yeah. ",
    "start": "962630",
    "end": "972780"
  },
  {
    "text": "Cool and here I'm\nactually only showing one test example\nin terms of what",
    "start": "972780",
    "end": "980150"
  },
  {
    "text": "this function needs to produce. But then when you actually\ntrain this function, you'll actually sample more\nlike different test examples",
    "start": "980150",
    "end": "986300"
  },
  {
    "text": "to evaluate its\nability to generalize given a training example\nor given a training set.",
    "start": "986300",
    "end": "995450"
  },
  {
    "text": "OK and so we can learn\nthis function with a data set of data sets. So a set of data sets\nwhere each data set",
    "start": "995450",
    "end": "1003490"
  },
  {
    "text": "is for a particular task. And again this\ndata set, the data set for a given task i should\nhave more than k examples so",
    "start": "1003490",
    "end": "1011890"
  },
  {
    "text": "that you can use k to be\nused in the training data set and some other examples\nto be used in the test set",
    "start": "1011890",
    "end": "1019000"
  },
  {
    "text": "to measure generalization.  Yeah.",
    "start": "1019000",
    "end": "1024809"
  },
  {
    "text": "There are two different-- there are two different\nsubcategories that I",
    "start": "1024810",
    "end": "1030780"
  },
  {
    "text": "[INAUDIBLE] So I corresponds\nto the task j is the--",
    "start": "1030780",
    "end": "1037740"
  },
  {
    "text": "Number of examples. Yeah. So here are the i is indexing\nthe number of tasks and j",
    "start": "1037740",
    "end": "1043530"
  },
  {
    "text": "is indexing the examples that\nwe have within a task data set. ",
    "start": "1043530",
    "end": "1051590"
  },
  {
    "text": "Cool. And so one thing that's\nnice about this kind of view is it means that now in order\nto implement a meta learning",
    "start": "1051590",
    "end": "1059860"
  },
  {
    "text": "algorithm, all we\nhave to do is design this function f that can\nread as input a data set",
    "start": "1059860",
    "end": "1065950"
  },
  {
    "text": "and then optimize\nthat function f. And so essentially in this\nlecture in the next two lectures, we're going to be\ntalking about different ways",
    "start": "1065950",
    "end": "1073180"
  },
  {
    "text": "that you can design and\noptimize this function f. ",
    "start": "1073180",
    "end": "1081250"
  },
  {
    "text": "And so in particular,\nkind of the general recipe that you can think\nof for designing a meta learning\nalgorithm is choosing",
    "start": "1081250",
    "end": "1086730"
  },
  {
    "text": "a form of this function. In this lecture, we'll see\na form of this function",
    "start": "1086730",
    "end": "1091860"
  },
  {
    "text": "where it's just represented\nby a neural network. But we'll actually\nsee other forms of this function in\nthe later lectures",
    "start": "1091860",
    "end": "1097235"
  },
  {
    "text": "where f can\ncorrespond to running gradient descent or\nf could correspond to an algorithm like\nnearest neighbors.",
    "start": "1097235",
    "end": "1103919"
  },
  {
    "text": "And then once we\nhave that function that we're going to\noptimize the free parameters",
    "start": "1103920",
    "end": "1109169"
  },
  {
    "text": "of that function, which we'll\nrefer to as the meta parameters using the meta training data.",
    "start": "1109170",
    "end": "1117030"
  },
  {
    "text": "Yeah. The speed of the testers code\nis a completely [INAUDIBLE]",
    "start": "1117030",
    "end": "1123809"
  },
  {
    "text": "X test is the test\ndata for the task",
    "start": "1123810",
    "end": "1131550"
  },
  {
    "text": "that is specified by D train. So D train and x test are\nboth from the same task.",
    "start": "1131550",
    "end": "1139657"
  },
  {
    "text": "And so that could be if\nyou're doing meta training time, that's going to be one\nof the media training tasks. If you're at meta\ntest time, that's",
    "start": "1139657",
    "end": "1144830"
  },
  {
    "text": "going to be one of the new tasks\nthat you haven't seen before.  Yeah.",
    "start": "1144830",
    "end": "1152360"
  },
  {
    "text": "In previous slides,\nwhat was in the k? So k is the number of examples--",
    "start": "1152360",
    "end": "1159372"
  },
  {
    "text": "in k shot learning, it's\nthe number of examples that you're going\nto be learning from. [INAUDIBLE] or on our\nmeta supervised learning,",
    "start": "1159372",
    "end": "1167680"
  },
  {
    "text": "each iteration we send for\ntuple k from the training?",
    "start": "1167680",
    "end": "1174590"
  },
  {
    "text": " Yeah, actually. So there's a question\nof how do you choose k? Typically, if you have\na sense for how many--",
    "start": "1174590",
    "end": "1183963"
  },
  {
    "text": "if you have a sense\nfor want to be able to do one shot learning,\nthen you'll just set k to be--",
    "start": "1183963",
    "end": "1189370"
  },
  {
    "text": "essentially to be one per class. And then at test time,\nyou'll do one shot learning.",
    "start": "1189370",
    "end": "1195100"
  },
  {
    "text": "You could also have kb variable. So you could actually sample\ndifferent k for different tasks and have some task\nbe one shot learning",
    "start": "1195100",
    "end": "1201250"
  },
  {
    "text": "task some tasks be\nfive shot learning tasks, such that you're\npreparing your network to be able to do\none shot learning",
    "start": "1201250",
    "end": "1208120"
  },
  {
    "text": "and five shot learning\nat meta test time. Yeah. Shouldn't the index be nk\ngood instead of k [INAUDIBLE]",
    "start": "1208120",
    "end": "1216150"
  },
  {
    "text": "Yeah, so the question was\nshouldn't the index here be n times k rather than k? So if it is a\nclassification problem,",
    "start": "1216150",
    "end": "1223550"
  },
  {
    "text": "then it should be n times k. If it's a regression problem,\nthen the standard notation",
    "start": "1223550",
    "end": "1228710"
  },
  {
    "text": "is to just use k. And so I used k\nhere but it kind of",
    "start": "1228710",
    "end": "1233750"
  },
  {
    "text": "depends if it's a regression\nor classification problem. And the reason-- the convention\nfor that is one short learning,",
    "start": "1233750",
    "end": "1240050"
  },
  {
    "text": "you typically think is\nlearning from one example. And a regression-- in regression\nproblems, you can actually--",
    "start": "1240050",
    "end": "1247033"
  },
  {
    "text": "learning from one\nexample in regression, probably you won't get very far. But that one shot learning is\nlearning from one example there",
    "start": "1247033",
    "end": "1253797"
  },
  {
    "text": "whereas in classification,\nif you have only one example, it's fundamentally impossible\nto solve like a five way problem",
    "start": "1253797",
    "end": "1259970"
  },
  {
    "text": "with just one example. And so the kind of\nconvention is to use--",
    "start": "1259970",
    "end": "1265820"
  },
  {
    "text": "to have a b per class there. And so if this was\nclassification, the k would be n times k. Yeah.",
    "start": "1265820",
    "end": "1272540"
  },
  {
    "text": "[INAUDIBLE]  If you have data set\nfor these tasks which",
    "start": "1272540",
    "end": "1279590"
  },
  {
    "text": "are between the\ndifferent [INAUDIBLE]",
    "start": "1279590",
    "end": "1284870"
  },
  {
    "text": "So we'll get to the\nmethods in a second. But it basically\ncorresponds to how many--",
    "start": "1284870",
    "end": "1290275"
  },
  {
    "text": "like what is the\nsize of the data set you're passing into\nyour function and-- [INAUDIBLE]",
    "start": "1290275",
    "end": "1297790"
  },
  {
    "text": "Well. So basically, you can train\nthis function to take as input, variable size data sets. And that's useful\nbecause at test time.",
    "start": "1297790",
    "end": "1304203"
  },
  {
    "text": "If you're not sure what size\nyour data set is going to be, then your function should be\nable to handle different sizes.",
    "start": "1304203",
    "end": "1311288"
  },
  {
    "text": "Yeah. During the last class\nwhen we were talking about the probabilistic\nview of meta learning,",
    "start": "1311288",
    "end": "1316799"
  },
  {
    "text": "we kind of talked about\ndata between the parameters representing the shared\nstructure and then also i which",
    "start": "1316800",
    "end": "1322710"
  },
  {
    "text": "[? rates ?] the\ntask-specific parameters. So in this case, is f\nrepresenting both theta and phi",
    "start": "1322710",
    "end": "1329710"
  },
  {
    "text": "or is it just\nrepresenting the theta? Yeah so the parameters\nof f here are theta.",
    "start": "1329710",
    "end": "1335230"
  },
  {
    "text": "And that is kind of\nrepresenting the theta that we saw in the previous lecture.",
    "start": "1335230",
    "end": "1340649"
  },
  {
    "text": "Here, you don't\nsee phi appear here and that's because phi is-- we'll see in some algorithms\nphi will come out explicitly,",
    "start": "1340650",
    "end": "1348059"
  },
  {
    "text": "whereas in other\nalgorithms we won't be explicitly representing phi. And so it's useful to set up\nthis notation in a way that",
    "start": "1348060",
    "end": "1356190"
  },
  {
    "text": "can either have it or-- that doesn't actually\nexplicitly represent it because some of the algorithms\nwe'll look at next week",
    "start": "1356190",
    "end": "1361410"
  },
  {
    "text": "don't actually explicitly\nrepresent what phi is. If it's not explicitly\nrepresented,",
    "start": "1361410",
    "end": "1366630"
  },
  {
    "text": "does that just mean that\nit's a part of theta in some form or a-- Sort of.",
    "start": "1366630",
    "end": "1372900"
  },
  {
    "text": "Basically, next week we'll\ncover non-parametric meta learning methods and\nso those don't actually",
    "start": "1372900",
    "end": "1378150"
  },
  {
    "text": "have explicit parameters and\nso phi won't ever appear.",
    "start": "1378150",
    "end": "1385290"
  },
  {
    "text": "But we'll get into that\non Wednesday next week. Yeah. Just a quick clarification,\n[INAUDIBLE] should be x here so",
    "start": "1385290",
    "end": "1392130"
  },
  {
    "text": "that the task [INAUDIBLE] So x test is a new example,\na new input for that task.",
    "start": "1392130",
    "end": "1406299"
  },
  {
    "text": "So it's given a data set-- given a task data\nset here, it'll",
    "start": "1406300",
    "end": "1411600"
  },
  {
    "text": "basically just correspond to one\nof the xs in the task data set. ",
    "start": "1411600",
    "end": "1419070"
  },
  {
    "text": "Cool. So I think that we should start\nrunning into like approaches because I think that will also\nclarify the setup as well.",
    "start": "1419070",
    "end": "1427139"
  },
  {
    "text": "And so we're going to start\nwith a running example, which is going to use the\nOmniglot data set. And you'll be actually using\nthis data set in your homework.",
    "start": "1427140",
    "end": "1434420"
  },
  {
    "text": "It's actually a\npretty cool data set. It has a data set of 50\ndifferent alphabets written",
    "start": "1434420",
    "end": "1439970"
  },
  {
    "text": "alphabet. And it has a total\nof 1623 characters across all of those alphabets.",
    "start": "1439970",
    "end": "1446420"
  },
  {
    "text": "And they're all written\nby hand by people. And here are some examples\nof some of the alphabets",
    "start": "1446420",
    "end": "1454640"
  },
  {
    "text": "represented in that data set. And per character there's\nactually only 20 examples.",
    "start": "1454640",
    "end": "1461857"
  },
  {
    "text": "So this is getting into\none of the questions which is mentioned before, which is\nthat typically in a machine learning data set you wouldn't\nhave only 20 examples per class",
    "start": "1461857",
    "end": "1470000"
  },
  {
    "text": "in your data set. For example, in the\n[? m nest ?] data set,",
    "start": "1470000",
    "end": "1475340"
  },
  {
    "text": "I think it has on the order\nof thousands of examples, maybe tens of thousands of\nexamples, maybe like 6,000",
    "start": "1475340",
    "end": "1481580"
  },
  {
    "text": "examples per class I think. So you can think of this\nas the transpose of a data",
    "start": "1481580",
    "end": "1487200"
  },
  {
    "text": "set like m nest where\ninstead of having many examples in a\nsmall number of classes,",
    "start": "1487200",
    "end": "1492470"
  },
  {
    "text": "you actually have a\nlarge number of classes and a very small number\nof examples per class.",
    "start": "1492470",
    "end": "1498995"
  },
  {
    "text": "And I also think that the\nstatistics of this kind of data is that are a little bit more\nreflective of the real world because in the real\nworld, we don't",
    "start": "1498995",
    "end": "1505690"
  },
  {
    "text": "see like 1,000 forks\nand 1,000 notebooks and 1,000 pens for only\na small number of things,",
    "start": "1505690",
    "end": "1511784"
  },
  {
    "text": "we actually have a really\nmassive number of objects and we only kind of\nrun into those objects",
    "start": "1511785",
    "end": "1518200"
  },
  {
    "text": "a small number of\ntimes in general. ",
    "start": "1518200",
    "end": "1523502"
  },
  {
    "text": "So we're going to be\nusing this data set. And in this data\nset to start off, we can think of different\ntasks as different alphabets.",
    "start": "1523502",
    "end": "1530360"
  },
  {
    "text": " And let's think about\nhow to actually set up",
    "start": "1530360",
    "end": "1537040"
  },
  {
    "text": "the meta learning thing here. So we're to be looking at\na three way classification",
    "start": "1537040",
    "end": "1542700"
  },
  {
    "text": "problem just\nbecause then I don't have to draw as many images. And we're going to be looking\nat a one shot learning problem.",
    "start": "1542700",
    "end": "1551560"
  },
  {
    "text": "And it's worth\nmentioning that there are two different versions\nof black box meta learning",
    "start": "1551560",
    "end": "1557130"
  },
  {
    "text": "algorithms and so we'll\ncover both of them. ",
    "start": "1557130",
    "end": "1562620"
  },
  {
    "text": "And first we'll talk about\nthe meta training process. So the first thing to do during\ntraining is to sample a task.",
    "start": "1562620",
    "end": "1572120"
  },
  {
    "text": "And so this will correspond\nto sampling an alphabet and specifically sampling three\ncharacters from that alphabet.",
    "start": "1572120",
    "end": "1582285"
  },
  {
    "text": "And the reason why it's\nthree is because we're just going to be considering a\nthree-way classification problem.",
    "start": "1582285",
    "end": "1588480"
  },
  {
    "text": "And then once we've\nsampled a task and sampled the\ncharacters that we're going to be classifying\nbetween, we're",
    "start": "1588480",
    "end": "1594590"
  },
  {
    "text": "going to sample two\nimages per character.",
    "start": "1594590",
    "end": "1603549"
  },
  {
    "text": " And we're then going to break\nthese examples into a train",
    "start": "1603550",
    "end": "1612920"
  },
  {
    "text": "set and a test set. And so for example,\nmaybe the alphabet",
    "start": "1612920",
    "end": "1618770"
  },
  {
    "text": "that we sampled is\nthe alphabet that we use in English and\nthe three characters",
    "start": "1618770",
    "end": "1626750"
  },
  {
    "text": "that we sampled\nwere a, b, and c. And we're going to sample two\ndifferent images per character",
    "start": "1626750",
    "end": "1634100"
  },
  {
    "text": "so that we can use one for\na training data set and one",
    "start": "1634100",
    "end": "1640220"
  },
  {
    "text": "for a test data\nset for that task.  So these are maybe yeah, two.",
    "start": "1640220",
    "end": "1649370"
  },
  {
    "text": "This will be our d\ntrain for task i, maybe we sampled task i.",
    "start": "1649370",
    "end": "1656000"
  },
  {
    "text": "And this is going to\nbe d test for task i. ",
    "start": "1656000",
    "end": "1663108"
  },
  {
    "text": "Now one thing that\nwe're missing here is we're also missing labels. We need labels in our\ntraining set and test set.",
    "start": "1663108",
    "end": "1668168"
  },
  {
    "text": "And so what we're going\nto do is we're also going to assign labels\nto each one of these. So we can say labels\nzero, one, two.",
    "start": "1668168",
    "end": "1674340"
  },
  {
    "text": "It's important to\nhave consistent labels across our characters.",
    "start": "1674340",
    "end": "1679620"
  },
  {
    "text": "OK. And now we get to the fun part. So what we're going\nto do is we are",
    "start": "1679620",
    "end": "1684690"
  },
  {
    "text": "going to pass our\ntraining data set into a neural network that will\nimplement the learning process.",
    "start": "1684690",
    "end": "1693360"
  },
  {
    "text": "And so in terms of passing\nthis into a neural network,",
    "start": "1693360",
    "end": "1698660"
  },
  {
    "text": "these data sets may have a\nnumber of different examples. And so things like a\nrecurrent neural network",
    "start": "1698660",
    "end": "1704000"
  },
  {
    "text": "are typically a good\nchoice or a transformer, if you want to be a little\nmore into the times.",
    "start": "1704000",
    "end": "1711020"
  },
  {
    "text": "And so you'll pass this into-- your training data points into\na recurrent neural network.",
    "start": "1711020",
    "end": "1719918"
  },
  {
    "text": "And at this point,\nwe're going to get to the two different versions. So one version of\nblack box meta learning",
    "start": "1719918",
    "end": "1725870"
  },
  {
    "text": "will take as input these\ntraining data points and output a set of parameters.",
    "start": "1725870",
    "end": "1733150"
  },
  {
    "text": "And we'll then use\nthose set of parameters to classify new examples\nfrom our test set.",
    "start": "1733150",
    "end": "1739670"
  },
  {
    "text": "And so in particular, we will-- I want to work on\nmy space management.",
    "start": "1739670",
    "end": "1746059"
  },
  {
    "text": "So we'll take these\nset of parameters. We'll then pass as input\none of our test examples",
    "start": "1746060",
    "end": "1752539"
  },
  {
    "text": "into a neural network\nwith those parameters to get a prediction\nfor that test example.",
    "start": "1752540",
    "end": "1760360"
  },
  {
    "text": "So this will be x test i,\nthis will be y hat test i.",
    "start": "1760360",
    "end": "1766890"
  },
  {
    "text": "And then we'll compare this\nto the corresponding label for that example.",
    "start": "1766890",
    "end": "1774080"
  },
  {
    "text": "And then what we\ncan do is we can, once we compare our prediction\nfor this example to the label for that example, we\ncan back propagate",
    "start": "1774080",
    "end": "1780680"
  },
  {
    "text": "through this entire neural\nnetwork into the parameters to train this neural network\nso that it can learn how",
    "start": "1780680",
    "end": "1786860"
  },
  {
    "text": "to learn from this data set. ",
    "start": "1786860",
    "end": "1793130"
  },
  {
    "text": "Cool. And then once we\nthen kind of back propagate into the\nneural network, update the parameters\nof this network",
    "start": "1793130",
    "end": "1799480"
  },
  {
    "text": "here, which will be\nthe parameters theta, well then, this is\nstep three, then we'll go back to step\none sample another task",
    "start": "1799480",
    "end": "1807039"
  },
  {
    "text": "and iterate the process.  So this is the meta\ntraining process.",
    "start": "1807040",
    "end": "1814640"
  },
  {
    "start": "1814640",
    "end": "1823723"
  },
  {
    "text": "We can talk a little bit\nabout the meta test process before we move on to--",
    "start": "1823723",
    "end": "1829270"
  },
  {
    "text": "before we move on to\nthe second version. But, yeah. [INAUDIBLE] class named a.",
    "start": "1829270",
    "end": "1836680"
  },
  {
    "text": "[INAUDIBLE]  Does it go through\nthe neural network",
    "start": "1836680",
    "end": "1842950"
  },
  {
    "text": "or does it go through the neural\nnetwork and the classification? [INAUDIBLE]",
    "start": "1842950",
    "end": "1850450"
  },
  {
    "text": "So it's basically\ngoing to update all of the parameters of this\nnetwork and one actually-- one",
    "start": "1850450",
    "end": "1856846"
  },
  {
    "text": "really, really\nimportant thing to note is that it's not\ngoing to update phi. So phi, you can think\nof more as an activation",
    "start": "1856847",
    "end": "1863860"
  },
  {
    "text": "of this neural network. It is only going\nto update the meta parameters of this network.",
    "start": "1863860",
    "end": "1870550"
  },
  {
    "text": "So we're going to differentiate\nthrough phi into the parameters of this thing right here. I really should have brought\nsome more colors today,",
    "start": "1870550",
    "end": "1876750"
  },
  {
    "text": "but you can think\nof the parameters-- the parameters of this will\ninclude both the encoder as well as the kind of\nrecurrent parameters as well.",
    "start": "1876750",
    "end": "1888020"
  },
  {
    "text": "Yeah. [INAUDIBLE]  Yes the question is\nwhy does it make sense",
    "start": "1888020",
    "end": "1893270"
  },
  {
    "text": "to use a recurrent\nneural network? You can use other\nnetworks as well. And we'll talk about\nsome other choices.",
    "start": "1893270",
    "end": "1898910"
  },
  {
    "text": "You could use something\nlike a deep set architecture or a transformer. One thing that is\nconvenient about sequence",
    "start": "1898910",
    "end": "1904370"
  },
  {
    "text": "models in general is that they\ncan handle variable length sequences. And so you can pass in variable\nlength data sets for example.",
    "start": "1904370",
    "end": "1915470"
  },
  {
    "text": "And it should be\nable to handle that. In general, architectures for\nhandling sets and sequences are pretty good choices.",
    "start": "1915470",
    "end": "1922870"
  },
  {
    "text": "Yeah. I'm not sure if [INAUDIBLE] ",
    "start": "1922870",
    "end": "1940370"
  },
  {
    "text": "Yeah. The question is, if you do meta\ntraining with three classes and then evaluate this ability\nto learn with four classes,",
    "start": "1940370",
    "end": "1947560"
  },
  {
    "text": "then how well does that work? In general, probably\nnot very well.",
    "start": "1947560",
    "end": "1953380"
  },
  {
    "text": "It depends on the architecture\nchoice that you choose. But I mean, you're\nbasically training",
    "start": "1953380",
    "end": "1959185"
  },
  {
    "text": "a recurrent neural network. And if you train a\nrecurring neural network on a sequence like the\nthree and then test it",
    "start": "1959185",
    "end": "1964270"
  },
  {
    "text": "on a sequence like\nthe four, you're basically testing\nit on something that it hasn't\nbeen trained to do.",
    "start": "1964270",
    "end": "1969309"
  },
  {
    "text": "Yeah. Do you need [? task data ?]\nfor your training tasks,",
    "start": "1969310",
    "end": "1974330"
  },
  {
    "text": "or can you do this\nin an unsupervied way where you're just hoping to\nlearn how to cluster them?",
    "start": "1974330",
    "end": "1981360"
  },
  {
    "text": "Yeah. So the question is,\ndo you need test data? And can you do this in\nan unsupervised way?",
    "start": "1981360",
    "end": "1987950"
  },
  {
    "text": "There's lots of\ndifferent variations that you could consider here. You could consider\nsomething where the train data set is maybe\nunlabeled and it's trying to--",
    "start": "1987950",
    "end": "1995630"
  },
  {
    "text": "but the test set is\nlabeled, for example, and it's trying to\ncluster and so forth.",
    "start": "1995630",
    "end": "2001367"
  },
  {
    "text": "We'll get into some\nmore advanced kind of variations of this in\nsome of the coming lectures.",
    "start": "2001367",
    "end": "2006710"
  },
  {
    "text": "But in the well, yeah,\nin the basic setup, it's good to have--\nlabels are helpful.",
    "start": "2006710",
    "end": "2013403"
  },
  {
    "text": "The other thing that I'll say\nis it is important for these to be held out examples and not\njust the same examples as what",
    "start": "2013403",
    "end": "2018759"
  },
  {
    "text": "you pass in because\nif you pass it in exactly the same example-- this example here, instead\nof learning to learn,",
    "start": "2018760",
    "end": "2024040"
  },
  {
    "text": "it will learn to memorize. And it will learn\nto just exactly only be able to make predictions\nfor the examples",
    "start": "2024040",
    "end": "2030580"
  },
  {
    "text": "that you passed in. One other note that\nI should mention here is that here, note that we're\npassing in both x and y.",
    "start": "2030580",
    "end": "2038289"
  },
  {
    "text": "We're passing in both\nthe input in the label into the recurrent\nneural network.",
    "start": "2038290",
    "end": "2043480"
  },
  {
    "text": "Here, it's important\nnot to pass in the label because if you\npass on the label, then it will just\nnot look at the image",
    "start": "2043480",
    "end": "2050560"
  },
  {
    "text": "and predict the label. And this is\nsomething that you'll run into in your homework. It's a kind of common\nmistake to accidentally",
    "start": "2050560",
    "end": "2056320"
  },
  {
    "text": "pass in the label\ninto the network here. Yeah. So in the prediction,\n[? could each sample ?]",
    "start": "2056320",
    "end": "2063280"
  },
  {
    "text": "be independent\nirrespective of the orders? [INAUDIBLE] because if I were\nlike CNN for each image and you",
    "start": "2063280",
    "end": "2069514"
  },
  {
    "text": "learn predicting\nindependently of the sequence. Is that the two [INAUDIBLE]",
    "start": "2069515",
    "end": "2075980"
  },
  {
    "text": "Yeah, so one downside\nof using an RNN, is that the ordering of\nthese data points matters.",
    "start": "2075980",
    "end": "2081169"
  },
  {
    "text": "And in practice, this is a set. And so the order of\nexamples doesn't matter.",
    "start": "2081170",
    "end": "2086899"
  },
  {
    "text": "We'll get to this\nin a few slides. But the architecture is\nthat like deep sets that are permutation and\nvariant are actually,",
    "start": "2086900",
    "end": "2095319"
  },
  {
    "text": "can be a better choice\nthan an RNN for that reason because they'll encode the fact\nthat the ordering of the data",
    "start": "2095320",
    "end": "2100392"
  },
  {
    "text": "points doesn't matter. What exactly are we predicting\nthe parameters, which",
    "start": "2100392",
    "end": "2105420"
  },
  {
    "text": "is the [? point ?] to make? If so how many\nparameters do we predict or what are we predicting?",
    "start": "2105420",
    "end": "2110770"
  },
  {
    "text": "Yeah so here it\nis actually-- here it's predicting\nparameters that are being used to make predictions. And so this is, in general, a\npretty high dimensional vector.",
    "start": "2110770",
    "end": "2122380"
  },
  {
    "text": "And so it may correspond to the\nparameters of a neural network. I guess I'll get\ninto version two.",
    "start": "2122380",
    "end": "2127950"
  },
  {
    "text": "So that was like-- this was version one.",
    "start": "2127950",
    "end": "2133020"
  },
  {
    "text": "Version two is something\nthat is a little bit--",
    "start": "2133020",
    "end": "2138330"
  },
  {
    "text": "little bit less unwieldy. So predicting like\nmillions of parameters with the neural network\ncan be rather expensive.",
    "start": "2138330",
    "end": "2145180"
  },
  {
    "text": "And so what you\ncan do instead is",
    "start": "2145180",
    "end": "2152839"
  },
  {
    "text": "let me write these out again. You don't have the\nlabel for this. So what you can do\ninstead is you can simply",
    "start": "2152840",
    "end": "2161839"
  },
  {
    "text": "use something like a\nrecurrent neural network and actually just\ncontinue running recurrent neural network here.",
    "start": "2161840",
    "end": "2170670"
  },
  {
    "text": "And in this case, the\nphi i is more implicit. And here you'll\nagain be asking it",
    "start": "2170670",
    "end": "2178790"
  },
  {
    "text": "to predict y i test\nfor this example right here, which is x i test.",
    "start": "2178790",
    "end": "2186290"
  },
  {
    "text": "And this is something\nwhere you don't actually have to have your Rnn output\na huge parameter vector.",
    "start": "2186290",
    "end": "2191750"
  },
  {
    "text": "You can just have\nit output say, this is like the hidden state of your\nRNN after like three examples.",
    "start": "2191750",
    "end": "2203450"
  },
  {
    "text": "And you can think of phi\ni as like the set of--",
    "start": "2203450",
    "end": "2211890"
  },
  {
    "text": "the set of these hidden\nparameters as well as any parameters that are in\nthis network right here.",
    "start": "2211890",
    "end": "2218049"
  },
  {
    "text": "So we can maybe refer to this\nas like theta G in the sense that if this is\nlike a function G,",
    "start": "2218050",
    "end": "2224490"
  },
  {
    "text": "those are the parameters\nof that function. And so this is what phi\ni would be in that case.",
    "start": "2224490",
    "end": "2230270"
  },
  {
    "text": "In this case, this is\na lot nicer because-- and this is going to be\nused more in practice",
    "start": "2230270",
    "end": "2235530"
  },
  {
    "text": "than this version\nbecause then you don't have to\noutput like millions of parameters of a network. You can actually--",
    "start": "2235530",
    "end": "2242570"
  },
  {
    "text": "Yeah you can actually only\nrepresent a much smaller context here.",
    "start": "2242570",
    "end": "2248549"
  },
  {
    "text": "Yeah. So are the\ntask-specific parameters an output of the\nshared model data or are they the result of\nadapting data [INAUDIBLE]",
    "start": "2248550",
    "end": "2258140"
  },
  {
    "text": "Yeah so in this lecture,\nthe task-specific parameters are the output of this neural\nnetwork model here or here.",
    "start": "2258140",
    "end": "2268432"
  },
  {
    "text": "I guess we can call\nthis h i in the sense that this is kind\nof a context vector.",
    "start": "2268432",
    "end": "2274130"
  },
  {
    "text": "In the lecture on Monday,\nwe'll see examples where the task-specific\nparameters are the result of something else\nlike running an optimization",
    "start": "2274130",
    "end": "2281180"
  },
  {
    "text": "like gradient descent.  Yeah.",
    "start": "2281180",
    "end": "2286358"
  },
  {
    "text": "Why is it not important to\nupdate phi i in model one? Why is it important to--",
    "start": "2286358",
    "end": "2293559"
  },
  {
    "text": "[INAUDIBLE] ",
    "start": "2293560",
    "end": "2300282"
  },
  {
    "text": "Yeah, so the question\nis, why is it important to not update phi i? So I think that maybe--\nso in this example,",
    "start": "2300282",
    "end": "2311869"
  },
  {
    "text": "it may be a little bit\nmore clear in the sense that when we run gradient\ndescent on to kind of test",
    "start": "2311870",
    "end": "2318130"
  },
  {
    "text": "generalization, in\nthis case, we're going to back up into all of\nthe parameters of this RNN,",
    "start": "2318130",
    "end": "2323140"
  },
  {
    "text": "but we're not going to hi\nbecause hi is an activation and when you have activations\nof a neural network,",
    "start": "2323140",
    "end": "2328790"
  },
  {
    "text": "you don't update\nthe activations. You only update the weights. And so analogously in this\nexample, you can think of phi i",
    "start": "2328790",
    "end": "2334030"
  },
  {
    "text": "as the activations of\nyour neural network. And so you're not going\nto be updating that, you're only going to\nbe updating the weights",
    "start": "2334030",
    "end": "2339220"
  },
  {
    "text": "of this neural network.  Yeah.",
    "start": "2339220",
    "end": "2344300"
  },
  {
    "text": "Could you could\nsay one more time what the difference is\nbetween one and two? Yeah. So the difference between\nversion one and version two",
    "start": "2344300",
    "end": "2351020"
  },
  {
    "text": "is--  they are very similar. The main difference is that in\nthis case, this neural network",
    "start": "2351020",
    "end": "2358820"
  },
  {
    "text": "right here, the only\nparameters making predictions from this example to y\ntest are going to be phi i",
    "start": "2358820",
    "end": "2366200"
  },
  {
    "text": "and those are going to\nbe outputted by this. And so those are going to be\nthe activations of this RNN",
    "start": "2366200",
    "end": "2371330"
  },
  {
    "text": "whereas in this case\nsome of the parameters are actually going to be--",
    "start": "2371330",
    "end": "2376623"
  },
  {
    "text": "some of the parameters\nhere are actually going to be optimized as part\nof the meta optimization.",
    "start": "2376623",
    "end": "2383300"
  },
  {
    "text": "And so for example,\nthe encoder of this RNN",
    "start": "2383300",
    "end": "2388340"
  },
  {
    "text": "may actually be shared\nacross the timestamps. And so you basically have\nmore parameter sharing",
    "start": "2388340",
    "end": "2395480"
  },
  {
    "text": "in this version. Yeah. [INAUDIBLE] ",
    "start": "2395480",
    "end": "2410079"
  },
  {
    "text": "So you're asking--",
    "start": "2410080",
    "end": "2416490"
  },
  {
    "text": "[INAUDIBLE] ",
    "start": "2416490",
    "end": "2426160"
  },
  {
    "text": "Yeah. So in this case theta G\nwill be optimized with back prop as well.",
    "start": "2426160",
    "end": "2431470"
  },
  {
    "text": "But hi will not be-- [INAUDIBLE] training\nleg sequence.",
    "start": "2431470",
    "end": "2438100"
  },
  {
    "text": "It'll be optimized with\nrespect to the loss here. But are you using\nthe test scribbles",
    "start": "2438100",
    "end": "2444090"
  },
  {
    "text": "to optimize their\ntest settlements? You can basically think of all\nof this as the learning process",
    "start": "2444090",
    "end": "2452402"
  },
  {
    "text": "and we're optimizing all of\nthe parameters of this learning process with respect to how well\nit generalizes on a new test",
    "start": "2452402",
    "end": "2459000"
  },
  {
    "text": "example. The one thing that you\nmay be noticing here",
    "start": "2459000",
    "end": "2464130"
  },
  {
    "text": "is that it could be that the\nneural network could just ignore all of this and just\nlearn a classifier for it.",
    "start": "2464130",
    "end": "2470693"
  },
  {
    "text": "And that's actually\na problem that can come up in meta learning. And so one thing that I'll\nmention here is that the--",
    "start": "2470693",
    "end": "2480000"
  },
  {
    "text": "here we kind of assigned labels\nsomewhat arbitrarily like zero, one, two, like in the\norder of the alphabet.",
    "start": "2480000",
    "end": "2485340"
  },
  {
    "text": "But really that was\nsomewhat arbitrary and when you're\ngiven a new alphabet you don't know what\nis the first letter",
    "start": "2485340",
    "end": "2490590"
  },
  {
    "text": "of the alphabet versus the\nsecond and versus the third. And so what we'll do in practice\nis when we assign these labels,",
    "start": "2490590",
    "end": "2496710"
  },
  {
    "text": "we'll assign them randomly. And so when we sample\na task, in this case I assign this labeling.",
    "start": "2496710",
    "end": "2503400"
  },
  {
    "text": "But when you sample\nthis task the next time, you might sample a\ndifferent labeling where you have, for example\none, two, zero one, two, zero.",
    "start": "2503400",
    "end": "2511740"
  },
  {
    "text": "And as a result when you\nactually randomize that, that prevents the neural\nnetwork from just memorizing",
    "start": "2511740",
    "end": "2516930"
  },
  {
    "text": "a mapping from the input to the\nlabel and ignoring the context.",
    "start": "2516930",
    "end": "2522960"
  },
  {
    "text": "As a follow up are\nyou also allowed to optimize phi i on the\n[? latest assessment ?] [INAUDIBLE].",
    "start": "2522960",
    "end": "2528345"
  },
  {
    "text": "Are you allowed to do that? [INAUDIBLE]  Similar to hi here,\nphi i is an activation",
    "start": "2528345",
    "end": "2535230"
  },
  {
    "text": "of the neural network rather\nthan some of the weights. [INAUDIBLE]",
    "start": "2535230",
    "end": "2540900"
  },
  {
    "text": " So it's the output of\nthis it is the parameters",
    "start": "2540900",
    "end": "2547207"
  },
  {
    "text": "of a neural network. But it's the output of our\nrecurrent neural network. And so we're setting\nit to be the output",
    "start": "2547207",
    "end": "2553200"
  },
  {
    "text": "of this neural network. And so if you updated it one\nround, then the next round it would just be wiped\nby a forward pass",
    "start": "2553200",
    "end": "2560940"
  },
  {
    "text": "of the [? Anand. ?] Yeah. Just answering this question.",
    "start": "2560940",
    "end": "2566050"
  },
  {
    "text": "Could it be because in\nmy [? other ?] training, the regime of training\nis very different",
    "start": "2566050",
    "end": "2572400"
  },
  {
    "text": "from the normal training\nthat is in machine learning. So meta training and even in\ntraining, we have two subsets--",
    "start": "2572400",
    "end": "2580830"
  },
  {
    "text": "for each class there are\nsubsets of example [INAUDIBLE] like training and testing.",
    "start": "2580830",
    "end": "2586530"
  },
  {
    "text": "So although it is said to\nbe testing, [INAUDIBLE] testing data.",
    "start": "2586530",
    "end": "2591540"
  },
  {
    "text": "It's not actually testing we\nuse it to train [? on them. ?] Yeah. That's a great point.",
    "start": "2591540",
    "end": "2597240"
  },
  {
    "text": "So to kind of reiterate,\nwhen I'm talking about this, this is like our task test set.",
    "start": "2597240",
    "end": "2603060"
  },
  {
    "text": " for task i. And we are going to be training\nthe parameters of our RNN",
    "start": "2603060",
    "end": "2610640"
  },
  {
    "text": "with these test examples. And that may sound like\na really bad thing. And it, sort of, should\nsound like a really bad thing",
    "start": "2610640",
    "end": "2616478"
  },
  {
    "text": "because we're going to be\ntraining on these test sets. But meta learning actually,\nthe real test is new tasks.",
    "start": "2616478",
    "end": "2622770"
  },
  {
    "text": "So this is the test that\nof the training task or of one of the training tasks. And then we're going to\nbe given new test tasks",
    "start": "2622770",
    "end": "2631010"
  },
  {
    "text": "and those are going to be really\nthe real test for these models. ",
    "start": "2631010",
    "end": "2638380"
  },
  {
    "text": "Yeah. So is there of catastrophic\nforgetting in this sort of set up when we're assembling\ntasks and then feeding",
    "start": "2638380",
    "end": "2645520"
  },
  {
    "text": "the same parameters\nacross tasks? How would we prevent\nit from happening?",
    "start": "2645520",
    "end": "2651140"
  },
  {
    "text": "[INAUDIBLE] Yeah. So the question\nwas, is there a risk of catastrophic forgetting\nin this case of past tasks",
    "start": "2651140",
    "end": "2658810"
  },
  {
    "text": "that you've seen? So in practice when\nwe sample tasks, we'll sample the sample\nthem from a set of tasks.",
    "start": "2658810",
    "end": "2665450"
  },
  {
    "text": "And as long as you keep on\nsampling from that task IID, then you should be able\nto remember those tasks.",
    "start": "2665450",
    "end": "2674320"
  },
  {
    "text": "I should also mention here,\nI said that we're just going to sample one task. In practice you can\nsample a mini batch",
    "start": "2674320",
    "end": "2679900"
  },
  {
    "text": "of tasks, so multiple tasks. And that will give you a\nlower variance gradient. And so similar to like\nin machine learning,",
    "start": "2679900",
    "end": "2686688"
  },
  {
    "text": "if you are sampling IID\nfrom a training data set, you don't have to worry\ntoo much about forgetting. We also don't have to worry\nabout that too much here.",
    "start": "2686688",
    "end": "2693260"
  },
  {
    "text": "But if we did have a sequence--\nlike a non-stationary sequence of tasks, we might\nstart forgetting some of the older tasks if\nthey don't keep on reoccurring.",
    "start": "2693260",
    "end": "2702160"
  },
  {
    "text": "Yeah. [INAUDIBLE] neural\nnetwork rather than another kind of network.",
    "start": "2702160",
    "end": "2708339"
  },
  {
    "text": "Because these [INAUDIBLE]\ncannot be related, right? And there's not a\ndirect relationship",
    "start": "2708340",
    "end": "2714520"
  },
  {
    "text": "when you try to\ntranslate something. So why are we using\ndirect network instead of the [INAUDIBLE]?",
    "start": "2714520",
    "end": "2721420"
  },
  {
    "text": "So you want to\nhave some network-- the question is why\nare we using an RNN versus some other network? You do want this network\nto be able to take",
    "start": "2721420",
    "end": "2728680"
  },
  {
    "text": "as input a data set. And data sets, do you\nwant to be able to process",
    "start": "2728680",
    "end": "2733869"
  },
  {
    "text": "some set of examples. And so things like RNNs are\na good choice for modeling.",
    "start": "2733870",
    "end": "2739810"
  },
  {
    "text": "Well, they're actually not-- in practice, they're\nnot a great choice, but they're the simplest\nchoice for modeling sequences.",
    "start": "2739810",
    "end": "2745240"
  },
  {
    "text": "And things like transformers and\ndeep set architectures and 1D convolutions could\nalso be used as well.",
    "start": "2745240",
    "end": "2751390"
  },
  {
    "text": "You wouldn't want to\nuse a feedforward well. So one thing you\ncould do is you could use a feedforward\nfully connected network",
    "start": "2751390",
    "end": "2757300"
  },
  {
    "text": "and basically concatenate\nthe embeddings-- concatenate all these images together\nand pass it through that way. And that might not\nbe a great choice,",
    "start": "2757300",
    "end": "2764638"
  },
  {
    "text": "compared to something\nthat explicitly models these separate entities. ",
    "start": "2764638",
    "end": "2771650"
  },
  {
    "text": "Yeah? In practice, does\nversion 1 won't work with a very\nhigh dimensional phi? So yeah, in practice,\nthis version 1 work",
    "start": "2771650",
    "end": "2778160"
  },
  {
    "text": "with very high dimensional phi. So there are some papers\nthat have actually",
    "start": "2778160",
    "end": "2784010"
  },
  {
    "text": "gotten it to work pretty well. In practice, people\ntypically use some form of sharing across layers\nso that you're not",
    "start": "2784010",
    "end": "2791000"
  },
  {
    "text": "outputting the entire\nparameter vector all at once. You might be outputting\none layer at a time,",
    "start": "2791000",
    "end": "2797247"
  },
  {
    "text": "maybe telling the\nnetwork, which layer that it might be outputting. One paper or one thing\nthat this is often",
    "start": "2797247",
    "end": "2803390"
  },
  {
    "text": "referred to as a hyper network. And a hyper network is\nbasically any neural network",
    "start": "2803390",
    "end": "2809270"
  },
  {
    "text": "that's outputting the weights\nof another neural network. And it can be used\nfor meta learning,",
    "start": "2809270",
    "end": "2815462"
  },
  {
    "text": "but it can also be\nused for other things. And so there are some\nexamples of it working well. But also, in general for\nfuture learning problems,",
    "start": "2815462",
    "end": "2822829"
  },
  {
    "text": "this is the more\npractical solution. ",
    "start": "2822830",
    "end": "2831220"
  },
  {
    "text": "Cool. I have a couple of examples of\nother data sets on the slides if you're interested\nin exploring that for your project.",
    "start": "2831220",
    "end": "2838609"
  },
  {
    "text": "But for now, I\njust want to-- oh, actually, I want to recap some\nof the stuff on the whiteboard.",
    "start": "2838610",
    "end": "2843760"
  },
  {
    "text": "But also we've talked\nabout meta training. We haven't explicitly\ngone through what meta-test time looks like.",
    "start": "2843760",
    "end": "2850490"
  },
  {
    "text": "So let's also quickly\ndo that first. So at meta-test\ntime, you're actually",
    "start": "2850490",
    "end": "2859210"
  },
  {
    "text": "just given a test task. And you're also given a training\ndata set for that test task.",
    "start": "2859210",
    "end": "2872390"
  },
  {
    "text": "And so, for example,\nmaybe your data set is of the Greek alphabet. And so you have some\nexamples like this.",
    "start": "2872390",
    "end": "2881540"
  },
  {
    "text": "And then you're probably also\ngiven some test examples.",
    "start": "2881540",
    "end": "2886790"
  },
  {
    "text": "So maybe you're given an\nexample that looks like this. And you want to be able to\nclassify this example, given",
    "start": "2886790",
    "end": "2895760"
  },
  {
    "text": "your few training examples. And so does anyone\nwant to say what",
    "start": "2895760",
    "end": "2903180"
  },
  {
    "text": "we might do at meta-test time? ",
    "start": "2903180",
    "end": "2910960"
  },
  {
    "text": "Yeah. We put that training data\nset out of the images",
    "start": "2910960",
    "end": "2916180"
  },
  {
    "text": "that we have the information. And we use our [INAUDIBLE].",
    "start": "2916180",
    "end": "2921369"
  },
  {
    "text": "And also we've been\n[? good ?] the testing image, [? great ?] image. And then we see [? what ?]\n[? could ?] be the highest this",
    "start": "2921370",
    "end": "2927680"
  },
  {
    "text": "score so that we can classify. Yeah. So what we can do is we can\npass our training data set",
    "start": "2927680",
    "end": "2934150"
  },
  {
    "text": "into recurrent neural network. Also passes and put\nour test example and ask it what the\ncorresponding label is.",
    "start": "2934150",
    "end": "2940430"
  },
  {
    "text": "So we'll probably be\ngiven corresponding labels for this like, 0, 1, and 2.",
    "start": "2940430",
    "end": "2945790"
  },
  {
    "text": "And we will pass these\ninto our neural network.",
    "start": "2945790",
    "end": "2951100"
  },
  {
    "text": "Doesn't have to be an RNN. And then also, pass\nthis input this example.",
    "start": "2951100",
    "end": "2958000"
  },
  {
    "text": "And then, hopefully,\nthe neural network will output zero, insofar\nas that looks like an alpha.",
    "start": "2958000",
    "end": "2964780"
  },
  {
    "text": " Also for creating certain\n[? recipes, ?] we helped them",
    "start": "2964780",
    "end": "2971500"
  },
  {
    "text": "make multiple passes or did we\n[? do ?] [? you ?] [? know ?] [? when ?] [? the ?]\n[? form ?] [INAUDIBLE]?? This pass, I will probably\nbelieve will zero try",
    "start": "2971500",
    "end": "2977650"
  },
  {
    "text": "to predict out of the\n[? thousands ?] [? and ?] try on the second XPS thing?",
    "start": "2977650",
    "end": "2983435"
  },
  {
    "text": "So in Black-box\nmeta-learning, what you'll do is you'll always pass\non the whole training data set as input and then\npass in the test example.",
    "start": "2983435",
    "end": "2991130"
  },
  {
    "text": "One version, another method\nthat we'll see next week is something that actually\nmakes explicit comparisons between the test example\nand the training examples.",
    "start": "2991130",
    "end": "3000420"
  },
  {
    "text": "Does that answer your question? I want to know if this\nis purely happening because of the training set that\nwe're [? passing ?] in this?",
    "start": "3000420",
    "end": "3008880"
  },
  {
    "text": "Sorry, can you repeat that? Sorry. Is there any tutoring\n[INAUDIBLE] or any fine tuning that's happening because of the\n[? post ?] [? op ?] [? in ?]",
    "start": "3008880",
    "end": "3016120"
  },
  {
    "text": "[? the ?] gamma? Yeah. I understand now. So the question was, is\nthere any parameter updates or any tuning that's\nhappening when we do this?",
    "start": "3016120",
    "end": "3023135"
  },
  {
    "text": "And the answer is no. So we're just doing a forward\npass through this RNN. And so we're not actually--\nyou can think of this RNN",
    "start": "3023135",
    "end": "3030090"
  },
  {
    "text": "as kind of implementing\nthe learning process. And if you give it a big\nenough neural network, these neural networks\ncan learn in that way.",
    "start": "3030090",
    "end": "3038500"
  },
  {
    "text": "And so there isn't actually any\nparameter updating other than, perhaps, you can think\nof the hidden state",
    "start": "3038500",
    "end": "3045000"
  },
  {
    "text": "as being updated as you pass\nit through the recurrent neural network. Yeah?",
    "start": "3045000",
    "end": "3050040"
  },
  {
    "text": "Why don't we implement--\nor why don't we run any algorithm which\ndoesn't do the [? finding? ?]",
    "start": "3050040",
    "end": "3056170"
  },
  {
    "text": "So you're asking why not\nhave an algorithm that does do some amount of-- It'll just do\nsomething [INAUDIBLE]..",
    "start": "3056170",
    "end": "3062563"
  },
  {
    "text": "I know we are\nalready using alpha, beta, gamma to compare our\nresults, but more than that, is there any algorithm?",
    "start": "3062563",
    "end": "3068700"
  },
  {
    "text": "We'll see it on\nMonday, next week. Yeah? So here the RNN is predicting\nthe parameters on one",
    "start": "3068700",
    "end": "3074273"
  },
  {
    "text": "without actually\nknowing about it. What does the architecture-- is this some kind of\nmodel that is easier",
    "start": "3074273",
    "end": "3079490"
  },
  {
    "text": "to run with the parameters\nthat we're predicting? That would happen on the kind\nof model that we have, right? The parameters\n[? you're accompanying, ?] so.",
    "start": "3079490",
    "end": "3087920"
  },
  {
    "text": "Generally, for version\n1, the architecture of this function right here\nwill be always the same.",
    "start": "3087920",
    "end": "3093330"
  },
  {
    "text": "It will be fixed. Yeah. But is it some kinds\nof applications are [? easy for ?] phi i-- to learn phi i.",
    "start": "3093330",
    "end": "3098930"
  },
  {
    "text": "Some kinds architectures\nare easier to learn, predict phi i for or\nsomething like that? Yeah. If you're asking are\nthere some architectures",
    "start": "3098930",
    "end": "3104945"
  },
  {
    "text": "for which it would be easier\nto predict phi i than others? Generally, the smaller the\narchitecture, the easier.",
    "start": "3104945",
    "end": "3111560"
  },
  {
    "text": "Yeah. And if you only\nhave to predict some of the parameters\nof the architecture, which is similar to here.",
    "start": "3111560",
    "end": "3116840"
  },
  {
    "text": "For example, you could\nhave it only output-- each could correspond to the\nlast layer of this network.",
    "start": "3116840",
    "end": "3122580"
  },
  {
    "text": "And then you only have to\npredict some part of it, for example. ",
    "start": "3122580",
    "end": "3129190"
  },
  {
    "text": "Yeah? Actually, I have two questions. First question is, you're\nreferring to this as black-box.",
    "start": "3129190",
    "end": "3135260"
  },
  {
    "text": "What about this is black-box? Yeah. So I'm calling this\nblack-box meta-learning because the learning\nprocess itself is somewhat",
    "start": "3135260",
    "end": "3142450"
  },
  {
    "text": "of a black-box. It's just this big\nneural network. I guess, it's multiple\nboxes on the board. But you can think of\nthis as just one kind",
    "start": "3142450",
    "end": "3149349"
  },
  {
    "text": "of monolithic neural network. And we don't have a lot of-- we don't see a lot\nof what happens",
    "start": "3149350",
    "end": "3154750"
  },
  {
    "text": "inside this neural network\nto actually interpret how it's actually adapting or\nlearning from these examples.",
    "start": "3154750",
    "end": "3160541"
  },
  {
    "text": "And then my second question is,\nonce the hypernetwork produces phi i, does phi i get\nupdated in all phi?",
    "start": "3160541",
    "end": "3169900"
  },
  {
    "text": "Does this always happens over\nhere or is it just static? It's static. So it outputs it once. And then we make\npredictions with that.",
    "start": "3169900",
    "end": "3176270"
  },
  {
    "text": "And so we can\npredict this example and also make predictions\nfor the other test examples. And it will only be changed once\nwe pass in a new training data",
    "start": "3176270",
    "end": "3183380"
  },
  {
    "text": "set.  Yeah? One thing that I'm kind\nof confused about is that",
    "start": "3183380",
    "end": "3189100"
  },
  {
    "text": "the labels of your dash [? in ?]\n[? this, ?] they were just kind of random, right? Where can this [? meta ?]\n[? facing ?] setup,",
    "start": "3189100",
    "end": "3194500"
  },
  {
    "text": "when we're trying to\nmake these labels, then how is it [? therefore ?]\nI cannot start chucking and bouncing them to--",
    "start": "3194500",
    "end": "3199900"
  },
  {
    "text": "OK, both images are equal. So is it kind of it's just\nmarking those two while we will [? fall ?] [? apart? ?]",
    "start": "3199900",
    "end": "3206350"
  },
  {
    "text": "So you're asking what is\nthe neural network doing? [INAUDIBLE]",
    "start": "3206350",
    "end": "3212565"
  },
  {
    "text": "So I don't-- I guess,\nI don't have a-- because this is\nblack-box, I don't have a great sense for\nexactly how, in practice,",
    "start": "3212565",
    "end": "3217840"
  },
  {
    "text": "the neural network\nis implementing it, is choosing to implement this. It does have to\nstore information",
    "start": "3217840",
    "end": "3223120"
  },
  {
    "text": "about how you map images to\nlabels and what is zero--",
    "start": "3223120",
    "end": "3228940"
  },
  {
    "text": "what does label zero\nmean from an image. ",
    "start": "3228940",
    "end": "3234250"
  },
  {
    "text": "Or it could be, I mean-- there's a lot of things\nthat it could be doing. It could be-- actually, if it's\na really huge neural network,",
    "start": "3234250",
    "end": "3241297"
  },
  {
    "text": "it could be mimicking something\nkind of like gradient descent, for example. So it's a little bit hard\nto understand in general.",
    "start": "3241297",
    "end": "3249080"
  },
  {
    "text": "Yeah? I think my question comes\nfrom the multi-task learning.",
    "start": "3249080",
    "end": "3254990"
  },
  {
    "text": "So here, we know that\nwe use some classes to train our neural network. And the [? best ?] [? is ?]\n[? on ?] [? process. ?] Is",
    "start": "3254990",
    "end": "3262360"
  },
  {
    "text": "there any way, again, to know\nwhich classes would actually help during treatment?",
    "start": "3262360",
    "end": "3268015"
  },
  {
    "text": "Which classes\nwould actually help to use during training so\nyou get the better result?",
    "start": "3268015",
    "end": "3274250"
  },
  {
    "text": "Yeah. So the question\nis, is there a way to tell if certain character\nclasses or certain tasks",
    "start": "3274250",
    "end": "3279340"
  },
  {
    "text": "would be helpful for a\nparticular test task? Not in general.",
    "start": "3279340",
    "end": "3284540"
  },
  {
    "text": "So for languages, we do not. Languages, we have\ndifferences like a common set.",
    "start": "3284540",
    "end": "3290559"
  },
  {
    "text": "But when there's a huge bunch\nof data in different variation of classes, how would we know?",
    "start": "3290560",
    "end": "3296840"
  },
  {
    "text": "Yeah. So if you have\ntons of tasks then, there's going to be more\nheterogeneity in terms",
    "start": "3296840",
    "end": "3303430"
  },
  {
    "text": "of when one task might be\nhelpful for the test task. But yeah. In general, we\ndon't really know.",
    "start": "3303430",
    "end": "3310840"
  },
  {
    "text": "It's kind of similar to fine\ntuning in multi-tasking. And maybe you should figure\nit out for your project and then tell us\nall how to do it.",
    "start": "3310840",
    "end": "3317140"
  },
  {
    "text": " Cool. So I went through a lot\nof that on the whiteboard",
    "start": "3317140",
    "end": "3325040"
  },
  {
    "text": "because I felt like that would\nbe easier and kind of more intuitive to walk through. The slides have basically\neverything that we went through",
    "start": "3325040",
    "end": "3332220"
  },
  {
    "text": "on the whiteboard. So version 1 we're going to\nbe training a neural network to output these parameters.",
    "start": "3332220",
    "end": "3337752"
  },
  {
    "text": "And then we're going to\nbe predicting test data points using a neural\nnetwork parameterized",
    "start": "3337752",
    "end": "3342890"
  },
  {
    "text": "by those parameters by phi i. And you can think of the first\nnetwork as kind of the learner, it's learning from these\ntraining data points.",
    "start": "3342890",
    "end": "3350540"
  },
  {
    "text": "And the second\nnetwork as something that's actually\nmaking predictions using those parameters.",
    "start": "3350540",
    "end": "3356790"
  },
  {
    "text": "Here's the RNN that\nwe drew on the board. This is representing f theta.",
    "start": "3356790",
    "end": "3362090"
  },
  {
    "text": "And then here's the\nsecond part of the network that we draw on the board as\nwell from the test examples.",
    "start": "3362090",
    "end": "3368810"
  },
  {
    "text": "And then we can train\nthe parameters of theta with standard\nsupervised learning.",
    "start": "3368810",
    "end": "3375260"
  },
  {
    "text": "In terms of what the\nloss function looks like, negative log likelihood,\nwhich is cross entropy loss",
    "start": "3375260",
    "end": "3382160"
  },
  {
    "text": "or mean squared error,\nis the loss function that's typical to use\nfor supervised learning.",
    "start": "3382160",
    "end": "3388290"
  },
  {
    "text": "And so that's basically\nthe loss function that will be applied right\nhere to back propagate through.",
    "start": "3388290",
    "end": "3394820"
  },
  {
    "text": " And you can-- if\nyou think about this as the loss function\nfor task I, then",
    "start": "3394820",
    "end": "3403640"
  },
  {
    "text": "you can basically view\nthis overall loss function as making a\nprediction for task I,",
    "start": "3403640",
    "end": "3411830"
  },
  {
    "text": "and then evaluating that on the\ntest data points for task I, and then averaging\nthat across the tasks.",
    "start": "3411830",
    "end": "3418655"
  },
  {
    "text": " I want to go through\nthis somewhat quickly because this is basically\nstuff that we all",
    "start": "3418655",
    "end": "3425470"
  },
  {
    "text": "went through on the board. We sample a task. We can also sample a\nmini batch of tasks",
    "start": "3425470",
    "end": "3431020"
  },
  {
    "text": "rather than just one task. We can then sample\ndisjoint data sets.",
    "start": "3431020",
    "end": "3436360"
  },
  {
    "text": "So this would be like our\ntraining set and our test data set from all\nof the examples that we have for\na particular task.",
    "start": "3436360",
    "end": "3443160"
  },
  {
    "text": "I should mention maybe here that\nit's worthwhile to kind of mix",
    "start": "3443160",
    "end": "3448829"
  },
  {
    "text": "and match these. So you shouldn't always use\nthis as your test examples and always use this as\nyour training examples.",
    "start": "3448830",
    "end": "3454410"
  },
  {
    "text": "You can get a little bit\nmore data for meta-training by randomly sampling what\nyou use as a training example",
    "start": "3454410",
    "end": "3459810"
  },
  {
    "text": "and what you use\nas a test example. And so kind of visually\nwhat this looks like is, maybe these are all the examples\nyou have for one of your tasks.",
    "start": "3459810",
    "end": "3468235"
  },
  {
    "text": "You want to allocate this into\ntraining examples and test examples. And so you'll kind\nof just randomly",
    "start": "3468235",
    "end": "3473250"
  },
  {
    "text": "assign them to a training\nset and do a test set. ",
    "start": "3473250",
    "end": "3479430"
  },
  {
    "text": "Cool. And then once you have your\ntraining set and test set, you can compute the\nparameters and then update--",
    "start": "3479430",
    "end": "3487619"
  },
  {
    "text": "compute the parameters\nwith the forward pass and then update\nthe meta parameters",
    "start": "3487620",
    "end": "3493049"
  },
  {
    "text": "with the backward pass.  And then repeat. ",
    "start": "3493050",
    "end": "3501540"
  },
  {
    "text": "Cool. We talked about how outputting\nall the neural network parameters isn't very scalable. And so instead of\ndoing that, you",
    "start": "3501540",
    "end": "3507960"
  },
  {
    "text": "can just only output the\nsufficient statistics and use another basically\nreplaced phi i with h i",
    "start": "3507960",
    "end": "3515220"
  },
  {
    "text": "and have this be a lower\ndimensional vector. And how that can be combined\nwith some other meta",
    "start": "3515220",
    "end": "3520860"
  },
  {
    "text": "parameters in order\nto make a prediction. One thing that's\nkind of intuitively",
    "start": "3520860",
    "end": "3526880"
  },
  {
    "text": "nice about this low\ndimensional vector is you can think of it as\nrepresenting contextual task information.",
    "start": "3526880",
    "end": "3532859"
  },
  {
    "text": "And so remember, back to\nmulti-task learning where we had this task descriptor z i.",
    "start": "3532860",
    "end": "3539320"
  },
  {
    "text": "You could sort of think of h\ni as a form of task descriptor that we're inferring from data.",
    "start": "3539320",
    "end": "3544848"
  },
  {
    "text": "And then we're just going\nto be passing that h i into a neural network in\norder to make predictions for that task. ",
    "start": "3544848",
    "end": "3554890"
  },
  {
    "text": "Yeah? How many tasks in\nthe test set do you need to test on to make sure\nit's effective at working on",
    "start": "3554890",
    "end": "3564560"
  },
  {
    "text": "[? top? ?] Because they\nmight be quite different. So how do you know that your\ntest [? task ?] is a good",
    "start": "3564560",
    "end": "3571088"
  },
  {
    "text": "representation of how well\n[? it ?] [? will ?] [? do ?] on another random task? Yeah. So the question is\nhow many test tasks",
    "start": "3571088",
    "end": "3577592"
  },
  {
    "text": "should we use when\nwe're evaluating a meta-learning algorithm? And so yeah. In general, you don't want\nto just have one target task.",
    "start": "3577592",
    "end": "3585280"
  },
  {
    "text": "When you're evaluating, you\nwant to have at least a few. And one thing that you can do\nto get a sense for how many you",
    "start": "3585280",
    "end": "3593420"
  },
  {
    "text": "need is you can\nlook at the variance of the accuracy across tasks. And so if one task you're\nseeing has 80% accuracy,",
    "start": "3593420",
    "end": "3601880"
  },
  {
    "text": "another task has 100%\naccuracy, and another task is like 0% accuracy, then\nyour variance is pretty large.",
    "start": "3601880",
    "end": "3607670"
  },
  {
    "text": "Whereas if they seem\nto be more or less consistent across\ntasks, so that maybe is a good indication that you\nmay have enough tasks to get",
    "start": "3607670",
    "end": "3615500"
  },
  {
    "text": "a good reading on it. Yeah? How do we learn the\nbig theta g for phi i?",
    "start": "3615500",
    "end": "3621849"
  },
  {
    "text": "Yeah. So the question was, how\ndo we learn the big theta G for phi i. So you can think of theta g as\npart of the meta parameters.",
    "start": "3621850",
    "end": "3630630"
  },
  {
    "text": "And so that's actually\nwhy I'm using theta here. So we're going to\noptimize theta g alongside all of the rest\nof the parameters of theta.",
    "start": "3630630",
    "end": "3639700"
  },
  {
    "text": "And so that's how we\nallow-- that's how we make it possible\nto have h i be only a low dimensional vector.",
    "start": "3639700",
    "end": "3645060"
  },
  {
    "text": "Likewise, theta g may\nhave some parameters that are shared\nwith the other part",
    "start": "3645060",
    "end": "3651450"
  },
  {
    "text": "of the recurrent neural network. Because, for example, you may\nhave the encoder of the image",
    "start": "3651450",
    "end": "3656940"
  },
  {
    "text": "be the same for these training\nexamples and test examples.",
    "start": "3656940",
    "end": "3662109"
  },
  {
    "text": "Yeah? Is there a ballpark\nfor how much less data do you need for each task with\nmeta-learning versus training",
    "start": "3662110",
    "end": "3669079"
  },
  {
    "text": "an individual mental? Yeah. So is there a ballpark for\nhow much data you need.",
    "start": "3669080",
    "end": "3676150"
  },
  {
    "text": "So with meta-training and as\nyou'll see in the assignment, you can go down to as few\nas one example per class",
    "start": "3676150",
    "end": "3682500"
  },
  {
    "text": "if you have a lot of tasks. And so, per task,\nyou can go down",
    "start": "3682500",
    "end": "3688848"
  },
  {
    "text": "to something very, very small\nif you have a lot of task and if they have\nshared structure.",
    "start": "3688848",
    "end": "3694019"
  },
  {
    "text": "That said, if you only\nhave one example per class, you're still going\nto-- your total amount of data may still be somewhat\nsimilar to kind of single task",
    "start": "3694020",
    "end": "3703410"
  },
  {
    "text": "learning in that you\nmay have a lot of tasks. And so you may have a small\namount of examples per task",
    "start": "3703410",
    "end": "3709560"
  },
  {
    "text": "but a lot of different tasks. So your total data may\nbe somewhat similar. And then, of course, the\namount of data you need",
    "start": "3709560",
    "end": "3715740"
  },
  {
    "text": "will depend a little\nbit on the complexity of the problem you're solving. ",
    "start": "3715740",
    "end": "3725490"
  },
  {
    "text": "Cool.  And then, I thought I\nwould talk a little bit",
    "start": "3725490",
    "end": "3730920"
  },
  {
    "text": "about architectures. So we talked a little\nbit about RNNs before.",
    "start": "3730920",
    "end": "3736680"
  },
  {
    "text": "One of the first papers that\nintroduced this, at least, more modern notion\nof meta-learning",
    "start": "3736680",
    "end": "3742050"
  },
  {
    "text": "with Black-box neural networks\nis this paper right here. And this is from 2016.",
    "start": "3742050",
    "end": "3748320"
  },
  {
    "text": "This is actually\nsort of in response to a challenge-- basically,\nthe Omniglot challenge being released. And some cognitive scientists\nsaying, neural networks",
    "start": "3748320",
    "end": "3755244"
  },
  {
    "text": "can't do few-shot learning. And this paper showed that, oh,\nneural networks can actually do few-shot learning.",
    "start": "3755245",
    "end": "3760350"
  },
  {
    "text": "And it was able to do pretty\nwell on the Omniglot data set. They used LSTMs and\nneural turing machines.",
    "start": "3760350",
    "end": "3767857"
  },
  {
    "text": "I wouldn't necessarily recommend\nusing neural turing machines. They're, I think, a little\nbit of a relic of the past.",
    "start": "3767857",
    "end": "3773710"
  },
  {
    "text": "But yeah. It's worth mentioning this\nis one of the first papers. And in your homework,\nI believe, we",
    "start": "3773710",
    "end": "3780270"
  },
  {
    "text": "have using LSTMs\nin the homework. But if you want to, you\ncan also play around",
    "start": "3780270",
    "end": "3786240"
  },
  {
    "text": "a little bit with transformers. Another architecture, which\nis called the deep set",
    "start": "3786240",
    "end": "3791790"
  },
  {
    "text": "architecture, is to-- instead of having a\nrecurrent neural network,",
    "start": "3791790",
    "end": "3797880"
  },
  {
    "text": "instead pass all\nof your examples through a feedforward\nneural network to get an embedding\nof each of your images",
    "start": "3797880",
    "end": "3804359"
  },
  {
    "text": "and then average\nthose embeddings to get a summary vector.",
    "start": "3804360",
    "end": "3812550"
  },
  {
    "text": "I already answered\nthis earlier, but I guess I can see if you're\npaying attention which is, why would something like\nfeedforward and then average",
    "start": "3812550",
    "end": "3820019"
  },
  {
    "text": "be better than a\nrecurrent neural network? [? Because it is ?]\n[? permutation ?] [? invariant. ?]",
    "start": "3820020",
    "end": "3825330"
  },
  {
    "text": "Yeah. So it's permutation invariant. So yeah. So averaging is agnostic\nto the order of the things",
    "start": "3825330",
    "end": "3833020"
  },
  {
    "text": "that you're averaging. And so this sort of architecture\nis permutation invariant. This is an architecture\ncalled deep sets.",
    "start": "3833020",
    "end": "3839620"
  },
  {
    "text": "One thing that's actually\npretty cool about deep sets is, there's a result\nthat shows that--",
    "start": "3839620",
    "end": "3845019"
  },
  {
    "text": "for some conditions on the\nwidth and depth of the network, these kinds of\narchitectures can represent any permutation\ninvariant function.",
    "start": "3845020",
    "end": "3852230"
  },
  {
    "text": "And so while it may seem\nactually somewhat limiting, these architectures are\nactually very expressive as long",
    "start": "3852230",
    "end": "3858220"
  },
  {
    "text": "as you only care about\npermutation invariant functions. ",
    "start": "3858220",
    "end": "3865540"
  },
  {
    "text": "There's also other\nexternal memory mechanisms that have been proposed. And then this--\nthere's also papers",
    "start": "3865540",
    "end": "3873390"
  },
  {
    "text": "that have proposed things that\nuse attention and convolution. This paper came out before,\nI think, transformers.",
    "start": "3873390",
    "end": "3881640"
  },
  {
    "text": "Or certainly before\ntransformers were popular. And so they probably\nwould have called it a transformer or something.",
    "start": "3881640",
    "end": "3887040"
  },
  {
    "text": "And then there's probably\nmore recent papers that actually use things\nlike transformers as well.",
    "start": "3887040",
    "end": "3892440"
  },
  {
    "text": " And then these methods\ncan do pretty well.",
    "start": "3892440",
    "end": "3897880"
  },
  {
    "text": "So on the Omniglot data set, if\nyou use this architecture which uses attention and\nconvolutions interleaved,",
    "start": "3897880",
    "end": "3904950"
  },
  {
    "text": "on Omniglot, on five-way\none-shot Omniglot, you can get 99% accuracy.",
    "start": "3904950",
    "end": "3910230"
  },
  {
    "text": "If you have five examples,\nyou can get even higher 99.78% accuracy. If you make it a\nlittle bit harder",
    "start": "3910230",
    "end": "3916530"
  },
  {
    "text": "and you're now doing a 20-way\nclassification problem, then you can still get\naccuracies in the high 90s.",
    "start": "3916530",
    "end": "3922170"
  },
  {
    "text": "So Omniglot is\nnot too difficult.",
    "start": "3922170",
    "end": "3927513"
  },
  {
    "text": "And then if you look at\nsomething like Mini-ImageNet which is a smaller version of\nImageNet with smaller images,",
    "start": "3927513",
    "end": "3933690"
  },
  {
    "text": "you can start to get\nsomewhat reasonable one-shot and 5-shot accuracy but\nthese numbers are a lot lower",
    "start": "3933690",
    "end": "3940920"
  },
  {
    "text": "than Omniglot numbers. And they're also certainly\nbelow state of the art.",
    "start": "3940920",
    "end": "3950510"
  },
  {
    "text": "Yeah. But we'll see some\nthings that are closer to state of the art\nin the next two lectures.",
    "start": "3950510",
    "end": "3956860"
  },
  {
    "text": "Yeah? Are there [? any ?] [? that ?]\n[? would ?] make viable external memory systems are\nbeing used for meta-learning?",
    "start": "3956860",
    "end": "3962190"
  },
  {
    "text": "How will they help\nin meta-learning? Yeah. I certainly don't think\nthat they are necessarily--",
    "start": "3962190",
    "end": "3969040"
  },
  {
    "text": "things like neural\ntraining machines and these other external\nmemory mechanisms, I guess, I wouldn't necessarily\nrecommend them.",
    "start": "3969040",
    "end": "3975130"
  },
  {
    "text": "I think that they're possibly\ninteresting from a neuroscience standpoint because--",
    "start": "3975130",
    "end": "3981735"
  },
  {
    "text": "I guess, I'm not\na neuroscientist so maybe I shouldn't\ntalk about this. But I think that\nthere are aspects of the brain that kind\nof store memories.",
    "start": "3981735",
    "end": "3988360"
  },
  {
    "text": "And so if you want to\nthink about key value storage and kind of\nstoring things in that way,",
    "start": "3988360",
    "end": "3993910"
  },
  {
    "text": "these things may be interesting. They also, perhaps,\nhave some benefits from the standpoint of--",
    "start": "3993910",
    "end": "3999640"
  },
  {
    "text": " from the standpoint of maybe\nkind of memory footprint.",
    "start": "3999640",
    "end": "4008871"
  },
  {
    "text": "Because if you can\nvery efficiently store that\ninformation, then maybe you don't need this\nreally big transformer.",
    "start": "4008872",
    "end": "4015760"
  },
  {
    "text": "But we haven't seen-- that's somewhat\nspeculative and we really haven't seen a lot of benefits\nempirically from using them.",
    "start": "4015760",
    "end": "4023980"
  },
  {
    "text": " Cool.",
    "start": "4023980",
    "end": "4029400"
  },
  {
    "text": "So in homework 1, there's going\nto be some key things that you're-- the two key things\nthat you'll do is, implement the\ndata processing.",
    "start": "4029400",
    "end": "4035280"
  },
  {
    "text": "So implement, how do you\nactually like split things up into tasks and\nexamples per tasks. And then also implement a pretty\nsimple Black-box meta-learner",
    "start": "4035280",
    "end": "4043200"
  },
  {
    "text": "and train that on Omniglot\nand get some results. ",
    "start": "4043200",
    "end": "4049390"
  },
  {
    "text": "Cool. So in terms of, just to also sum\nthings up and talk about some",
    "start": "4049390",
    "end": "4055930"
  },
  {
    "text": "of the pros and cons\nof this approach. One thing that's really\nnice about these this kind",
    "start": "4055930",
    "end": "4061840"
  },
  {
    "text": "of approach to meta-learning is\nthat these big neural networks are quite expressive\nand they can represent lots of different\nlearning procedures.",
    "start": "4061840",
    "end": "4069069"
  },
  {
    "text": "RNNs can represent-- if\nthey're large enough, they can represent\nreally any function.",
    "start": "4069070",
    "end": "4076000"
  },
  {
    "text": "They're also really\neasy to combine with a variety of problems. Today we talked about\nsupervised learning",
    "start": "4076000",
    "end": "4081280"
  },
  {
    "text": "where you back\npropagate the loss with a supervised objective. But it's also very easy to\nplug this into reinforcement",
    "start": "4081280",
    "end": "4086533"
  },
  {
    "text": "learning objectives as well. Because maybe it's outputting\nnot the label, but an action or a [? q ?] value.",
    "start": "4086533",
    "end": "4091720"
  },
  {
    "text": "And you can train\nthis just like you would train a recurrent\nneural network with kind of reinforcement\nlearning objectives.",
    "start": "4091720",
    "end": "4098649"
  },
  {
    "text": " In terms of the downsides,\nthese big networks",
    "start": "4098649",
    "end": "4106089"
  },
  {
    "text": "are somewhat complex\nand they have to learn something\npretty difficult, which is learning from data points.",
    "start": "4106090",
    "end": "4113441"
  },
  {
    "text": "And this can lead to somewhat\nof a challenging optimization problem. And as we'll start\nto see in some of the next lectures,\ntrying to learn",
    "start": "4113441",
    "end": "4122500"
  },
  {
    "text": "how to learn from\nscratch, from kind of randomly\ninitialized RNN weights can be pretty\ndifficult in comparison",
    "start": "4122500",
    "end": "4128920"
  },
  {
    "text": "to giving it some of the\nstructure of learning algorithms that we already know. ",
    "start": "4128920",
    "end": "4135089"
  },
  {
    "text": "And so, as a result, because\nit's a hard optimization, these can be less\ndata efficient than",
    "start": "4135090",
    "end": "4141470"
  },
  {
    "text": "other meta-learning approaches.  Cool.",
    "start": "4141470",
    "end": "4147370"
  },
  {
    "text": "So there are other ways that\nwe can represent f theta. And so next time\non Monday, we'll",
    "start": "4147370",
    "end": "4154270"
  },
  {
    "text": "talk about what if we\ntreat f an optimization problem and actually\nembed optimization into f",
    "start": "4154270",
    "end": "4159969"
  },
  {
    "text": "rather than having this big RNN. ",
    "start": "4159970",
    "end": "4164989"
  },
  {
    "text": "Cool. So now I'd like to talk a little\nbit about large language models",
    "start": "4164990",
    "end": "4170599"
  },
  {
    "text": "and things like\nGPT-3, Chinchilla and the latest Gopher, PaLM,\nthe biggest language models",
    "start": "4170600",
    "end": "4176509"
  },
  {
    "text": "these days. And we're going to focus\non GPT-3 just because it's",
    "start": "4176510",
    "end": "4182490"
  },
  {
    "text": "a fairly canonical example. And they also have lots\nof really cool qualitative",
    "start": "4182490",
    "end": "4188430"
  },
  {
    "text": "examples in the paper\nthat we can draw from. And things like\nGPT-3 are a lot like",
    "start": "4188430",
    "end": "4194820"
  },
  {
    "text": "a Black-box neural network. But one thing that's a little\nbit different from here that we'll talk about\nis that they have--",
    "start": "4194820",
    "end": "4203040"
  },
  {
    "text": "few-shot learning kind\nof emerges in a way that wasn't explicitly-- ",
    "start": "4203040",
    "end": "4210769"
  },
  {
    "text": "I think that when you train a\nlanguage model with a language modeling objective, you're\nnot kind of setting out for a few-shot learning.",
    "start": "4210770",
    "end": "4216250"
  },
  {
    "text": "You're not setting\nup the data to train it to do few-shot learning. It's something that\nkind of emerges",
    "start": "4216250",
    "end": "4221880"
  },
  {
    "text": "from the data in a somewhat\nmore surprising way. And so we'll talk a\nlittle bit about emergent versus the kind of few-shot\nlearning we see here.",
    "start": "4221880",
    "end": "4230950"
  },
  {
    "text": "So GPT-3 is a language model. You can think of it as a\nBlack-box meta-learner,",
    "start": "4230950",
    "end": "4236520"
  },
  {
    "text": "or a kind of a big\nRNN, well, transformer neural network that's trained\non language generation tasks.",
    "start": "4236520",
    "end": "4245260"
  },
  {
    "text": "And you could think of it\nas where D train corresponds",
    "start": "4245260",
    "end": "4250329"
  },
  {
    "text": "to a sequence of characters. And D test corresponds\nto the following sequence",
    "start": "4250330",
    "end": "4256329"
  },
  {
    "text": "of characters. And so D train is what\nthe language model is being conditioned\non and D test",
    "start": "4256330",
    "end": "4261360"
  },
  {
    "text": "is what it's being\ntrained to generate. The data set corresponds\nto crawled data",
    "start": "4261360",
    "end": "4268030"
  },
  {
    "text": "from the internet, English\nlanguage Wikipedia, and to book corporas.",
    "start": "4268030",
    "end": "4273400"
  },
  {
    "text": "And the architecture\nis a very large-- well, I guess, by\nthese standards-- by these days it might be\nconsidered somewhat small.",
    "start": "4273400",
    "end": "4280510"
  },
  {
    "text": "But in my opinion, a\nsomewhat large transformer neural network. And So it has 175 billion\nparameters, 96 layers,",
    "start": "4280510",
    "end": "4289140"
  },
  {
    "text": "and a pretty large batch size. What do the different\ntasks correspond to?",
    "start": "4289140",
    "end": "4294760"
  },
  {
    "text": "So this is something that-- this is where kind of\nemergent few-shot learning comes into play. So you can see it do\ntasks like spelling",
    "start": "4294760",
    "end": "4302970"
  },
  {
    "text": "correction, simple\nmath problems, translating between languages. And these tasks are, in some\nways, somewhat emergent.",
    "start": "4302970",
    "end": "4309750"
  },
  {
    "text": "But also, in some ways\nreflective of the kind of data that you see on the internet. ",
    "start": "4309750",
    "end": "4317800"
  },
  {
    "text": "And the cool thing about\nthings like language models is that these seem like-- on the surface these seem\nlike very different tasks,",
    "start": "4317800",
    "end": "4326489"
  },
  {
    "text": "like math, versus machine\ntranslation, versus spelling. And we could actually have\nall these tasks solved",
    "start": "4326490",
    "end": "4331830"
  },
  {
    "text": "by a single architecture. So the way that you can\ndo that is to put them",
    "start": "4331830",
    "end": "4337369"
  },
  {
    "text": "all in the form of text. Put them all in kind of a\ncommon language for the network.",
    "start": "4337370",
    "end": "4343830"
  },
  {
    "text": "And that's a good\nidea because it's also very easy to get a lot of\ndata of text on the internet. ",
    "start": "4343830",
    "end": "4351460"
  },
  {
    "text": "And so what this\nlooks like is you have an inner loop\nand an outer loop. So in here, kind of a\nforward pass of our RNN",
    "start": "4351460",
    "end": "4358619"
  },
  {
    "text": "was an inner loop. And training that RNN to\nlearn was our outer loop.",
    "start": "4358620",
    "end": "4363969"
  },
  {
    "text": "And so, likewise, here\nin context learning, kind of running the RNN on\ntext is your inner loop.",
    "start": "4363970",
    "end": "4372030"
  },
  {
    "text": "And then across. When you see-- across tasks and\noptimizing the neural network",
    "start": "4372030",
    "end": "4380490"
  },
  {
    "text": "across to these different\ntasks is the outer loop. And that's kind of\nthe learning process",
    "start": "4380490",
    "end": "4385679"
  },
  {
    "text": "of optimizing the parameters\nof your big transformer model. ",
    "start": "4385680",
    "end": "4392070"
  },
  {
    "text": "And so these are kind\nof the example tasks that we saw before\nall in text form. ",
    "start": "4392070",
    "end": "4400580"
  },
  {
    "text": "Cool. And so they trained this\non data from the internet, and Wikipedia, and so forth.",
    "start": "4400580",
    "end": "4406332"
  },
  {
    "text": "And you can get some\npretty cool results. So you can see something\nlike one-shot learning where you give it a\ndictionary definition saying,",
    "start": "4406332",
    "end": "4412660"
  },
  {
    "text": "to screech something as\nto swing a sword at it. An example of a sentence\nthat use the word screech is, and then it\ngives you a sentence.",
    "start": "4412660",
    "end": "4419413"
  },
  {
    "text": "We screeched at each\nother for several minutes and then we went outside\nand ate ice cream.",
    "start": "4419413",
    "end": "4425240"
  },
  {
    "text": "So this is one example\nof one-shot learning. You could also do\nfew-shot language editing. Where here you give it 5-ish\nexamples of poor English",
    "start": "4425240",
    "end": "4436390"
  },
  {
    "text": "versus good English. And once you give\nit those examples, and then give an example\nof poor English input, I'd be more than happy to work\nwith you in another project.",
    "start": "4436390",
    "end": "4444190"
  },
  {
    "text": "The good English output\nthat corresponds to that, that it gives you is, I'd\nbe more than happy to work with you on another project.",
    "start": "4444190",
    "end": "4452110"
  },
  {
    "text": "And likewise, you see another\nexample for the same prompt at the bottom.",
    "start": "4452110",
    "end": "4458710"
  },
  {
    "text": "And then, it can also do things\nthat aren't really considered few-shot learning tasks. So you can ask it to\nwrite an article for you",
    "start": "4458710",
    "end": "4466450"
  },
  {
    "text": "given a title and a subtitle. ",
    "start": "4466450",
    "end": "4472480"
  },
  {
    "text": "Cool. So some things about\nGPT-3 is that the results are really impressive.",
    "start": "4472480",
    "end": "4478420"
  },
  {
    "text": "And also, since\nGPT-3 has come out, there's actually some\nmodels that do even better",
    "start": "4478420",
    "end": "4483489"
  },
  {
    "text": "by a pretty significant margin. GPT-3 and other\nmore recent models",
    "start": "4483490",
    "end": "4489309"
  },
  {
    "text": "are also far from perfect. They make mistakes. So GPT-3 itself, even\nthe largest model",
    "start": "4489310",
    "end": "4495730"
  },
  {
    "text": "has an accuracy around 50% to\n60% on few-shot learning tasks.",
    "start": "4495730",
    "end": "4501100"
  },
  {
    "text": "I think that some of-- I think that things\nlike Chinchilla are maybe closer to high 70s.",
    "start": "4501100",
    "end": "4507310"
  },
  {
    "text": "But still, have a lot\nof room for improvement.",
    "start": "4507310",
    "end": "4512390"
  },
  {
    "text": "They also fail in\nsomewhat unintuitive ways. So if you ask how many\neyes does a giraffe have?",
    "start": "4512390",
    "end": "4519065"
  },
  {
    "text": "Giraffe has two eyes. How many eyes does my foot have? It starts telling you things\nlike your foot has two eyes,",
    "start": "4519065",
    "end": "4525520"
  },
  {
    "text": "and a spider has eight\neyes, and so forth. So yeah.",
    "start": "4525520",
    "end": "4531278"
  },
  {
    "text": "It doesn't fail in the\nsame way that humans fail. ",
    "start": "4531278",
    "end": "4538000"
  },
  {
    "text": "And the last note here\nis that the choice of D train, basically\nhow you prompt the model, how you\ngive it these examples",
    "start": "4538000",
    "end": "4544380"
  },
  {
    "text": "is also quite important. And we'll see an\nexample of this. You'll get to play around\nwith this a little bit",
    "start": "4544380",
    "end": "4549390"
  },
  {
    "text": "in homework 3.  And then the last thing I wanted\nto mention about these models",
    "start": "4549390",
    "end": "4555730"
  },
  {
    "text": "is that there's some recent\nresearch on actually trying to understand when will few-shot\nlearning emerge from these",
    "start": "4555730",
    "end": "4562630"
  },
  {
    "text": "data-- from these kinds of models\nthat are trained on text data.",
    "start": "4562630",
    "end": "4567880"
  },
  {
    "text": "And the research has shown\nthat both aspects of the data and the model is important.",
    "start": "4567880",
    "end": "4573260"
  },
  {
    "text": "So it has shown that\nhaving temporal correlation in the data is important.",
    "start": "4573260",
    "end": "4579230"
  },
  {
    "text": "And for example, if\nD train and D test are completely independent\nfrom one another, they won't actually learn\nto use that context.",
    "start": "4579230",
    "end": "4585250"
  },
  {
    "text": "And it will just\nignore that context. And so they think\nof this as kind of having a kind of bursty data\nin time versus non-bursty data.",
    "start": "4585250",
    "end": "4596630"
  },
  {
    "text": "They also find that having\na dynamic meaning of words is helpful. If there's a word\nthat kind of means",
    "start": "4596630",
    "end": "4602590"
  },
  {
    "text": "different things in\ndifferent contexts, like the word wicked might\nmean something like evil in one",
    "start": "4602590",
    "end": "4608260"
  },
  {
    "text": "context and something like\nreally cool in another context. And that can help\nwith few-shot learning because that\nencourages the model",
    "start": "4608260",
    "end": "4614650"
  },
  {
    "text": "to actually pay\nattention to the context when making future predictions.",
    "start": "4614650",
    "end": "4621520"
  },
  {
    "text": "And then, also there's\nsome work suggesting that different aspects of\nthe model are important. The biggest number one\nthing is that the model",
    "start": "4621520",
    "end": "4627760"
  },
  {
    "text": "has a high capacity. People have found\nthat transformers are much better at giving\nyou capacity than things",
    "start": "4627760",
    "end": "4634150"
  },
  {
    "text": "like LSTMs and RNNs. And so here's a plot of\nfew-shot learning accuracy",
    "start": "4634150",
    "end": "4640570"
  },
  {
    "text": "for these different\narchitectures. And then also\nlarger transformers are better than\nsmaller transformers.",
    "start": "4640570",
    "end": "4646690"
  },
  {
    "text": "In the GPT-3 paper, they\nshowed that a 175 billion parameter model was much\nbetter at few-shot learning",
    "start": "4646690",
    "end": "4652938"
  },
  {
    "text": "than some of these\nsmaller models.  Cool.",
    "start": "4652938",
    "end": "4658390"
  },
  {
    "text": "So yeah. That was it for today. We went over Black-box\nmeta-learning and saw how we can\ntrain neural networks",
    "start": "4658390",
    "end": "4665110"
  },
  {
    "text": "to do few-shot learning. As a couple reminders,\nyour project group form is due on Monday and homework 1\nis due on Wednesday next week.",
    "start": "4665110",
    "end": "4673740"
  },
  {
    "start": "4673740",
    "end": "4678000"
  }
]