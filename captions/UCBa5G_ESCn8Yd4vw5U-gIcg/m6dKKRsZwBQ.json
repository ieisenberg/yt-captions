[
  {
    "start": "0",
    "end": "5290"
  },
  {
    "text": "So the plan for\ntoday is to finish up the VAE slides that we\ndidn't cover on Monday.",
    "start": "5290",
    "end": "12730"
  },
  {
    "text": "And then we'll start talking\nabout flow models, which are going to be yet another\nclass of generative models",
    "start": "12730",
    "end": "18610"
  },
  {
    "text": "with different\nsort of trade offs. So the thing that I really\nwanted to talk about",
    "start": "18610",
    "end": "25329"
  },
  {
    "text": "is this interpretation of\na variational autoencoder or a VA as an autoencoder.",
    "start": "25330",
    "end": "32570"
  },
  {
    "text": "So we've derived it just from\nthe perspective of, OK, there is a latent variable model.",
    "start": "32570",
    "end": "38690"
  },
  {
    "text": "And then there is this\nvariational inference technique for training the model, where\nyou have the decoder, which",
    "start": "38690",
    "end": "47410"
  },
  {
    "text": "defines the\ngenerative process, P. And then you have\nthis encoder network Q that is used to essentially\noutput variational",
    "start": "47410",
    "end": "58000"
  },
  {
    "text": "parameters that are\nsupposed to give you a decent approximation\nof the posterior",
    "start": "58000",
    "end": "63310"
  },
  {
    "text": "under the true generative model. And we've come up with\nthis kind of training",
    "start": "63310",
    "end": "70340"
  },
  {
    "text": "objective, where you-- whatever data point, you\nkind of have a function",
    "start": "70340",
    "end": "76490"
  },
  {
    "text": "that depends on the\nparameters of the decoder, the real generative model,\ntheta, and the encoder phi.",
    "start": "76490",
    "end": "83600"
  },
  {
    "text": "And we've seen that\nthis objective function is a lower bound to the true\nmarginal probability of a data",
    "start": "83600",
    "end": "90230"
  },
  {
    "text": "point. And it kind of\nmakes sense to try to jointly optimize and\njointly maximize this",
    "start": "90230",
    "end": "95990"
  },
  {
    "text": "as a function of\nboth theta and phi. And you can kind\nof see intuitively what's going on here.",
    "start": "95990",
    "end": "101640"
  },
  {
    "text": "We're saying that for\nevery data point x, we're going to use\nq to try to guess possible completions, possible\nvalues for the latent variable",
    "start": "101640",
    "end": "109440"
  },
  {
    "text": "z. So that's why there's an\nexpectation with respect to this distribution. And then we basically look at\nthe log likelihood of the data",
    "start": "109440",
    "end": "118220"
  },
  {
    "text": "point after we've\nguessed what we don't using this inference\ndistribution, this encoder,",
    "start": "118220",
    "end": "124760"
  },
  {
    "text": "this q distribution. And if you were to just\noptimize these first two pieces,",
    "start": "124760",
    "end": "134200"
  },
  {
    "text": "essentially, q would\nbe incentivized to try to find\ncompletions that are",
    "start": "134200",
    "end": "139390"
  },
  {
    "text": "most likely under the\noriginal generative model. And instead, there is also\nkind of like this regularizer,",
    "start": "139390",
    "end": "147010"
  },
  {
    "text": "this other term\nhere, where we also look at the probability of\nthe completions under q.",
    "start": "147010",
    "end": "153370"
  },
  {
    "text": "And this is basically\ncorresponds to that entropy of the variational\ndistribution q",
    "start": "153370",
    "end": "159159"
  },
  {
    "text": "term that is kind of\nencouraging the distribution q,",
    "start": "159160",
    "end": "164680"
  },
  {
    "text": "that the inference distribution\nto spread out the probability mass. So not just try to\nfind the most likely z,",
    "start": "164680",
    "end": "172810"
  },
  {
    "text": "but also try to find\nall possible z's that are consistent with the x\nthat you have access to.",
    "start": "172810",
    "end": "180740"
  },
  {
    "text": "And we have seen\nthat, to some extent, if your q is\nsufficiently flexible,",
    "start": "180740",
    "end": "186829"
  },
  {
    "text": "then you might be\nable to actually, and it's actually able to be\nequal to the true conditional",
    "start": "186830",
    "end": "194150"
  },
  {
    "text": "distribution p of z given x. Then this objective\nfunction actually becomes",
    "start": "194150",
    "end": "200330"
  },
  {
    "text": "exactly the log\nmarginal probability over x, which is the traditional\nmaximum likelihood objective.",
    "start": "200330",
    "end": "207170"
  },
  {
    "text": "And so we've motivated it from\nthat perspective and everything made sense.",
    "start": "207170",
    "end": "212480"
  },
  {
    "text": "We haven't really\ndiscussed why it's called the variational\nautoencoder like, what's the autoencoding flavor here.",
    "start": "212480",
    "end": "219709"
  },
  {
    "text": "And we can see it\nif you kind of like unpack this loss a little bit.",
    "start": "219710",
    "end": "225560"
  },
  {
    "text": "In particular, what you can\ndo is you can add and subtract the prior distribution\nover the latent",
    "start": "225560",
    "end": "234140"
  },
  {
    "text": "variables that you used in\nyour generative model, which recall usually is just a\nGaussian distribution over z.",
    "start": "234140",
    "end": "240590"
  },
  {
    "text": "So and your sample in your\nvariational autoencoder, your sample, a latent variable\naccording to some prior p of z,",
    "start": "240590",
    "end": "248330"
  },
  {
    "text": "then you feed the z into the\ndecoder that produces parameters for p of x, given z, and then\nyou sample from p of x given z.",
    "start": "248330",
    "end": "256458"
  },
  {
    "text": "So if you add and subtract\nthis quantity in here, then you end up, and then\nyou look at the joint over x",
    "start": "256459",
    "end": "268169"
  },
  {
    "text": "and z, divided by\nthe marginal over z is just the conditional\ndistribution of x given z, which is just the decoder.",
    "start": "268170",
    "end": "274050"
  },
  {
    "text": "And then you can see that\nyou end up with another term here, which is the KL\ndivergence between the inference",
    "start": "274050",
    "end": "281220"
  },
  {
    "text": "distribution and the prior.  And so what does this\nobjective look like",
    "start": "281220",
    "end": "291040"
  },
  {
    "text": "if you were to actually evaluate\nit and do some kind of Monte Carlo approximation?",
    "start": "291040",
    "end": "296740"
  },
  {
    "text": "What you would do is you would\nhave some data point, which gives you the x component.",
    "start": "296740",
    "end": "302139"
  },
  {
    "text": "So it could be an image like\nthe one you see on the left. That's the input. That's the i-th data point.",
    "start": "302140",
    "end": "308560"
  },
  {
    "text": "Then when you want to compute\nthis expectation with respect to q, what you would do is\nyou can approximate that",
    "start": "308560",
    "end": "316300"
  },
  {
    "text": "by Monte Carlo. So what you would\ndo is you would draw a sample from q of z, given x.",
    "start": "316300",
    "end": "322510"
  },
  {
    "text": "And recall that q of\nz, given x is just some other neural network that\nbasically takes x as an input,",
    "start": "322510",
    "end": "330730"
  },
  {
    "text": "you feed it in. And then as an output,\nsome variational parameters",
    "start": "330730",
    "end": "339160"
  },
  {
    "text": "over the distribution,\nover the latent variables. And so if q of z,\ngiven x describes",
    "start": "339160",
    "end": "346430"
  },
  {
    "text": "Gaussian distributions,\nthe output of this first neural network,\nwhich is the encoder, might be a mu and sigma,\nwhich basically defines",
    "start": "346430",
    "end": "355285"
  },
  {
    "text": "the kind of Gaussian\nyou're going to use to guess what\nare likely, what",
    "start": "355285",
    "end": "360980"
  },
  {
    "text": "are reasonable values\nof the latent variables, given what you know, given xi. And then what you\ncould do is you could",
    "start": "360980",
    "end": "367580"
  },
  {
    "text": "sample from this distribution. So you sample with a Gaussian,\nwith mean, and variance,",
    "start": "367580",
    "end": "373160"
  },
  {
    "text": "defined by what you get\nby fitting the image through an encoder.",
    "start": "373160",
    "end": "378770"
  },
  {
    "text": "Then we can look at-- so yeah, this is\nwhat I just said. So there is this encoder,\none neural network that",
    "start": "378770",
    "end": "384500"
  },
  {
    "text": "would give you\nparameters, and then you sample from that\nGaussian distribution. Then we can essentially\nlook at the first term here",
    "start": "384500",
    "end": "392420"
  },
  {
    "text": "of the loss, which you can think\nof it as a reconstruction loss. So essentially, what we're doing\nis we're evaluating p of xi,",
    "start": "392420",
    "end": "404000"
  },
  {
    "text": "given this latent variable\nz that we've sampled. And essentially, what\nwe're saying is we are--",
    "start": "404000",
    "end": "413389"
  },
  {
    "text": "if you were to sample\nfrom this distribution, you would sample a data\npoint from a Gaussian",
    "start": "413390",
    "end": "420400"
  },
  {
    "text": "with parameters given by what\nyou get from the decoder. And that would\nessentially kind of",
    "start": "420400",
    "end": "426100"
  },
  {
    "text": "like produce another image out. And if you actually look at\nthis likelihood term here,",
    "start": "426100",
    "end": "434170"
  },
  {
    "text": "it would essentially tell you\nhow likely was the original data point according to this scheme.",
    "start": "434170",
    "end": "441500"
  },
  {
    "text": "And so it's kind of like if p\nof x, given z is a Gaussian, it's some kind of\nreconstruction loss",
    "start": "441500",
    "end": "446890"
  },
  {
    "text": "that tells you how well can you\nreconstruct the original image given this latent variable z.",
    "start": "446890",
    "end": "454300"
  },
  {
    "text": "And so the first term has some\nkind of autoencoding flavor. And if you didn't\nhave the second term,",
    "start": "454300",
    "end": "461740"
  },
  {
    "text": "it would essentially correspond\nto an autoencoder that is a little bit stochastic.",
    "start": "461740",
    "end": "466830"
  },
  {
    "text": "So in a typical autoencoder,\nyou would take an input, you would map it to a vector\nin a deterministic way.",
    "start": "466830",
    "end": "472800"
  },
  {
    "text": "Then you would try to go from\nthe vector back to the input. This is kind of like a\nstochastic autoencoder, where",
    "start": "472800",
    "end": "478330"
  },
  {
    "text": "you take an input, you map it\nto a distribution over latent variables, and then these\nlatent variables that you sample",
    "start": "478330",
    "end": "485780"
  },
  {
    "text": "from the distribution\nshould be useful, should be good at reconstructing\nthe original input.",
    "start": "485780",
    "end": "494280"
  },
  {
    "text": "And so yeah, the first\nterm, essentially, encourages that what you get by\nfeeding these latent variables,",
    "start": "494280",
    "end": "505380"
  },
  {
    "text": "like you kind of like these\nautoencoding objective. So like the output that you\nget is similar to the input",
    "start": "505380",
    "end": "510990"
  },
  {
    "text": "that you feed in. The reconstruction\npart, I'm curious, like what in the\ntraining objective causes",
    "start": "510990",
    "end": "517590"
  },
  {
    "text": "are hidden representation\nto resemble a Gaussian? Yeah.",
    "start": "517590",
    "end": "522688"
  },
  {
    "text": "So this is just the first term. So if you were to\njust do that, that's a fine way of training a model.",
    "start": "522688",
    "end": "528029"
  },
  {
    "text": "And you would get some\nkind of autoencoder. Now, there is a\nsecond term here. That is this KL\ndivergence term between q",
    "start": "528030",
    "end": "536730"
  },
  {
    "text": "and the prior distribution\nthat we used to define the VA.",
    "start": "536730",
    "end": "543269"
  },
  {
    "text": "That term, so that's\nthe autoencoding loss. The second term is basically\nencouraging this latent",
    "start": "543270",
    "end": "550199"
  },
  {
    "text": "variables that you generate\nthrough the encoder to be distributed similar\nas measured by KL divergence",
    "start": "550200",
    "end": "559890"
  },
  {
    "text": "to this Gaussian\ndistribution that we use in the generative process.",
    "start": "559890",
    "end": "566820"
  },
  {
    "text": "And so this is kind of like\nsaying that not only you should be able to\nreconstruct well,",
    "start": "566820",
    "end": "573149"
  },
  {
    "text": "but the kind of latent variables\nthat you use to reconstruct should be distributed as a\nGaussian random variable.",
    "start": "573150",
    "end": "581070"
  },
  {
    "text": "And if that's the\ncase, then you kind of see why we would get a\ngenerative model this way.",
    "start": "581070",
    "end": "587170"
  },
  {
    "text": "Because if you just\nhave the first piece, where you have an\nautoencoder, that's great. But you don't know how to\ngenerate new data points.",
    "start": "587170",
    "end": "594150"
  },
  {
    "text": "But if you somehow have\na way of generating z's",
    "start": "594150",
    "end": "599820"
  },
  {
    "text": "just by sampling from a\nGaussian or by sampling from a simple distribution,\nthen you can kind of trick the decoder to\ngenerate reasonable samples.",
    "start": "599820",
    "end": "608490"
  },
  {
    "text": "Because it has been trained\nto reconstruct images when the z's came from the--",
    "start": "608490",
    "end": "614620"
  },
  {
    "text": "were produced by the encoder. And now if these z's have\nsome simple distribution, and so you have some way of\ngenerating the z's yourself just",
    "start": "614620",
    "end": "622180"
  },
  {
    "text": "by sampling from a Gaussian,\nthen you essentially have a generative model.",
    "start": "622180",
    "end": "627212"
  },
  {
    "text": "And that's why it's called\na variational autoencoder because you can think of\nit as an autoencoder that",
    "start": "627213",
    "end": "632590"
  },
  {
    "text": "is regularized so that\nthe latent variables have a specific shape,\nhave a particular kind",
    "start": "632590",
    "end": "638290"
  },
  {
    "text": "of distribution, which is\njust the prior of your VA. So that you can\nalso generate-- you",
    "start": "638290",
    "end": "644720"
  },
  {
    "text": "can use it as a generative\nmodel, essentially. So the classic setting\nwith the autoencoder.",
    "start": "644720",
    "end": "650630"
  },
  {
    "text": "It can only reconstruct\nimages or inputs that it has seen before.",
    "start": "650630",
    "end": "656100"
  },
  {
    "text": "Is that correct. Well, if you train\nan autoencoder, you train it on a\ntraining set, and then you",
    "start": "656100",
    "end": "661700"
  },
  {
    "text": "hope that it generalizes. So you would hope\nthat it might still be able to reconstruct\nimages that",
    "start": "661700",
    "end": "667490"
  },
  {
    "text": "are similar to the ones\nyou've seen during training. And that would still be\nachieved by this first term",
    "start": "667490",
    "end": "675019"
  },
  {
    "text": "to the extent that\nthe model generalizes, which is always a bit tricky\nto quantify, but to the extent",
    "start": "675020",
    "end": "680600"
  },
  {
    "text": "that the autoencoder\ngeneralizes, it's fine. But you still don't have a\nway of generating fresh data",
    "start": "680600",
    "end": "685880"
  },
  {
    "text": "points because you don't have\na way to start the process. The process always starts from\ndata and produces data out.",
    "start": "685880",
    "end": "692720"
  },
  {
    "text": "But somehow, you have\nto hijack this process and fit in latent\nvariables by sampling",
    "start": "692720",
    "end": "699079"
  },
  {
    "text": "from this prior distribution. And this term here, this\nKL divergence term here,",
    "start": "699080",
    "end": "705930"
  },
  {
    "text": "encourages the fact\nthat that's not going to cause a lot of trouble\nbecause the z's that you get",
    "start": "705930",
    "end": "711990"
  },
  {
    "text": "by sampling from the prior\nare similar to the ones that you've seen when you\ntrain the autoencoder.",
    "start": "711990",
    "end": "717870"
  },
  {
    "text": "Yeah. Kind of comment on that. So like for autoencoder,\nessentially, when we train,",
    "start": "717870",
    "end": "725160"
  },
  {
    "text": "once it's trained and we just\nhave a deterministic model,",
    "start": "725160",
    "end": "730199"
  },
  {
    "text": "whenever we give our images\nand then sort of just compress the information\nto this lower dimension.",
    "start": "730200",
    "end": "735900"
  },
  {
    "text": "No, it's not. So the question is whether\nthe after training-- in a regular autoencoder or\na variational autoencoder?",
    "start": "735900",
    "end": "741822"
  },
  {
    "text": "Like the regular autoencoder. Regular autoencoder,\nyes, it's deterministic. Yes. But this [INAUDIBLE] sample\nfrom this distribution.",
    "start": "741822",
    "end": "749019"
  },
  {
    "text": "Yeah. So this is a\nstochastic autoencoder in the sense that\nthe mapping here. Q is stochastic.",
    "start": "749020",
    "end": "755520"
  },
  {
    "text": "I guess, technically,\nyou could make it very almost deterministic. Like you're allowed to\nchoose any distribution",
    "start": "755520",
    "end": "761430"
  },
  {
    "text": "you want, but that might not be\nthe optimal way because there could be uncertainty over.",
    "start": "761430",
    "end": "768390"
  },
  {
    "text": "Recall that this q should be\nclose to the true conditional distribution of z,\ngiven x, under p.",
    "start": "768390",
    "end": "773610"
  },
  {
    "text": "And so to the extent that you\nbelieve that conditional is not very concentrated,\nthen you might",
    "start": "773610",
    "end": "780430"
  },
  {
    "text": "want to use a q that is\nalso somehow capturing that uncertainty. Yeah.",
    "start": "780430",
    "end": "785690"
  },
  {
    "text": "It seems like this [INAUDIBLE]\nencouraging good samples",
    "start": "785690",
    "end": "792490"
  },
  {
    "text": "and your second term is\ndiscouraging it from going away from the existing distribution.",
    "start": "792490",
    "end": "798860"
  },
  {
    "text": "And I know you\nmentioned previously that the reinforcing might be\nusing a reinforce argument.",
    "start": "798860",
    "end": "803920"
  },
  {
    "text": "Does that also\nlead to this thing? So the reinforce\nalgorithm is just a way to a different\noptimization",
    "start": "803920",
    "end": "810880"
  },
  {
    "text": "algorithm for this loss. That works more generally,\nlike for an arbitrary q.",
    "start": "810880",
    "end": "816370"
  },
  {
    "text": "And it works for cases when the\nlatent variable z, for example, are discrete.",
    "start": "816370",
    "end": "821620"
  },
  {
    "text": "There is some similarity to the\n[INAUDIBLE] thing in the sense that one also has this flavor of\nkind of like optimizing a reward",
    "start": "821620",
    "end": "828910"
  },
  {
    "text": "subject to some KL constraint. So it has that flavor of\nregularizing something.",
    "start": "828910",
    "end": "837340"
  },
  {
    "text": "And so if you were to just\noptimize the first piece, it would not be useful as\na generative model or not",
    "start": "837340",
    "end": "843370"
  },
  {
    "text": "necessarily. And then you have to add\nthis regularization term to allow you to do something.",
    "start": "843370",
    "end": "849230"
  },
  {
    "text": "But it's not the\n[INAUDIBLE] case, where both p and q\nare generative models.",
    "start": "849230",
    "end": "854810"
  },
  {
    "text": "This is slightly\ndifferent in the sense that we're just regularizing\nthe latent space, essentially, of an autoencoder.",
    "start": "854810",
    "end": "861817"
  },
  {
    "text": "I'm just trying to understand,\nlike for the second term, the KL divergence\nterm, and there's--",
    "start": "861817",
    "end": "867300"
  },
  {
    "text": "what's the rationale\nbetween forcing the q. Because in this\nexample, maybe the x",
    "start": "867300",
    "end": "874175"
  },
  {
    "text": "is the observable\npart of the digit and the z is the\nunobservable part. And knowing x will give\nstrong indication of what z is",
    "start": "874176",
    "end": "882960"
  },
  {
    "text": "or what is the rationale behind\nforcing the q of z, given x, to be very close to\nunconditional p of z.",
    "start": "882960",
    "end": "890880"
  },
  {
    "text": "Yeah. So the reason we're doing\nthis is to basically be allowed to then essentially\ngenerate fresh latent variables",
    "start": "890880",
    "end": "900630"
  },
  {
    "text": "by sampling from the prior\nwithout actually needing an x and feed it into the.",
    "start": "900630",
    "end": "906629"
  },
  {
    "text": "So that's what allows\nus to basically use this generative model. I think what you\nare alluding to is",
    "start": "906630",
    "end": "911670"
  },
  {
    "text": "that it would seem like\nmaybe it would make sense to compare the marginal\ndistribution of z",
    "start": "911670",
    "end": "917130"
  },
  {
    "text": "under q to the marginal\ndistribution of z under p. That would be a very\nreasonable objective too.",
    "start": "917130",
    "end": "923589"
  },
  {
    "text": "It's just not tractable. And so meaning that again, you\nend up with some kind of very",
    "start": "923590",
    "end": "933250"
  },
  {
    "text": "hard sort of like integral that\nyou cannot necessarily evaluate. But there are other\nways to enforce this.",
    "start": "933250",
    "end": "941089"
  },
  {
    "text": "You can use discriminators to-- there are different flavors.",
    "start": "941090",
    "end": "947540"
  },
  {
    "text": "The VAE uses this particular\nkind of regularization. It's not the only way to\nachieve this kind of behavior.",
    "start": "947540",
    "end": "954045"
  },
  {
    "text": " So in the inference like when\ngenerating, how do we get the--",
    "start": "954045",
    "end": "962959"
  },
  {
    "text": "like how do we sample from it. Because we don't have the x. Exactly. Exactly. So for sampling, we\ndon't have the x,",
    "start": "962960",
    "end": "969042"
  },
  {
    "text": "so you cannot just use both\nthe encoder and the decoder. So to sample, recall, we\nonly have the decoder.",
    "start": "969042",
    "end": "975450"
  },
  {
    "text": "So to generate samples, you\ndon't need the encoder anymore. And the difference\nis that the z's--",
    "start": "975450",
    "end": "981740"
  },
  {
    "text": "during training,\nthe z's are produced by encoding real data points. During sampling,\nduring inference time,",
    "start": "981740",
    "end": "987990"
  },
  {
    "text": "the Z's are produced\njust by sampling from this prior\ndistribution p of z. [INAUDIBLE] p of z also have\na neural network like you have",
    "start": "987990",
    "end": "997580"
  },
  {
    "text": "to-- P of z, no. P of z is something\nsuper simple. Now VAE could be just a Gaussian\ndistribution with 0 mean",
    "start": "997580",
    "end": "1005770"
  },
  {
    "text": "and identity covariance. That's kind of like that simple\nprior that we always use.",
    "start": "1005770",
    "end": "1011680"
  },
  {
    "text": "So recall the sampling\nprocedure is you sample z from a\nsimple prior, then you feed it through\nthis neural network,",
    "start": "1011680",
    "end": "1019220"
  },
  {
    "text": "the decoder to get parameters. OK. OK. Just-- OK. Got you. Yeah.",
    "start": "1019220",
    "end": "1024290"
  },
  {
    "text": "How close does p of z actually\nhave to be for a Gaussian? I know Monday, you\nmentioned that we",
    "start": "1024290",
    "end": "1030410"
  },
  {
    "text": "use-- that it's a Gaussian\nto do the parametrization. But let's say, it's\nlike financial data where the tails are bigger.",
    "start": "1030410",
    "end": "1036650"
  },
  {
    "text": "Would this still approach or you\nhave to use a different approach to model? Yeah. So the extent that\nthis works depends.",
    "start": "1036650",
    "end": "1043619"
  },
  {
    "text": "Again, it's kind of related\nto the KL divergence between the true posterior\nand the approximate posterior. Like if you believe that the\napproximate, the true posterior",
    "start": "1043619",
    "end": "1051050"
  },
  {
    "text": "is not Gaussian, it's\nsomething complicated, then you might want to use a\nmore flexible distribution for q",
    "start": "1051050",
    "end": "1057260"
  },
  {
    "text": "or something with heavy tails. So there is a lot of degrees of\nfreedom in designing the model.",
    "start": "1057260",
    "end": "1063590"
  },
  {
    "text": "I think that understanding\nhow the ELBO is derived tells you what should\nwork or shouldn't work.",
    "start": "1063590",
    "end": "1069792"
  },
  {
    "text": "But yeah, it doesn't\nhave to be Gaussian. That's just like the\nsimplest instantiation. But there's a lot of\nfreedom in terms of choosing",
    "start": "1069792",
    "end": "1075920"
  },
  {
    "text": "the different pieces. Who was first?",
    "start": "1075920",
    "end": "1080990"
  },
  {
    "text": "Yeah, go ahead. So why is the first term\ncalled autoencoding loss? I thought the [INAUDIBLE]\nis the log likelihood",
    "start": "1080990",
    "end": "1087200"
  },
  {
    "text": "not the loss function. The first term is basically\nan autoencoding loss",
    "start": "1087200",
    "end": "1093530"
  },
  {
    "text": "because it's saying that if you\nthink about it, you are saying, you fit in an x, and then\nyou check-- you produce a z,",
    "start": "1093530",
    "end": "1100850"
  },
  {
    "text": "and then you check how likely\nis the original x, given that z, which if p of x, given\nz, is a Gaussian,",
    "start": "1100850",
    "end": "1107660"
  },
  {
    "text": "it's basically some\nkind of L2 loss",
    "start": "1107660",
    "end": "1113570"
  },
  {
    "text": "basically between what you\nfeed in and what you get out, essentially.",
    "start": "1113570",
    "end": "1120830"
  },
  {
    "text": "So in that sense, it's\nan autoencoding loss. But the true loss that we\noptimize is not just that.",
    "start": "1120830",
    "end": "1126590"
  },
  {
    "text": "It's this ELBO L, which\nis the auto encoding loss plus regularization.",
    "start": "1126590",
    "end": "1131690"
  },
  {
    "text": "Because we want to use\nit as a generative model. So since we're using\nthe KL divergence",
    "start": "1131690",
    "end": "1138440"
  },
  {
    "text": "between the conditional\nprobability of q with the unconditional\nprobability of p, wouldn't that just\nencourage the encoder model",
    "start": "1138440",
    "end": "1146240"
  },
  {
    "text": "to generate the same\ndistribution like for every--",
    "start": "1146240",
    "end": "1151760"
  },
  {
    "text": "Yes. So there is that. Yeah, that's a valid point. Like it's a pretty strong\nkind of regularization.",
    "start": "1151760",
    "end": "1158510"
  },
  {
    "text": "And that is kind\nof like forcing it to try to do as well as it\ncan to generate the same.",
    "start": "1158510",
    "end": "1165420"
  },
  {
    "text": "Then there is also\nthis other term that is sort of like\nforcing you to try to find different representation\nfor different kinds of inputs.",
    "start": "1165420",
    "end": "1171809"
  },
  {
    "text": "So you can do a good job\nat reconstructing them. So these two terms are kind of\nlike fighting with each other.",
    "start": "1171810",
    "end": "1177410"
  },
  {
    "text": "And you try to find the\nbest solution you can. ",
    "start": "1177410",
    "end": "1183230"
  },
  {
    "text": "So if we just heard about\nthe generative aspect, because it seems like if we just\nchose the mean instead of just 0",
    "start": "1183230",
    "end": "1191960"
  },
  {
    "text": "and 1. When we start\nsampling from this, we're just going\nto reconstruct it. So my question is, should we\njust not start with random noise",
    "start": "1191960",
    "end": "1200330"
  },
  {
    "text": "and then reconstruct\nit and just forget about all the encoding of it?",
    "start": "1200330",
    "end": "1205485"
  },
  {
    "text": "So they are suggesting\na different kind of training objective,\nwhere we would sample fresh from the path?",
    "start": "1205485",
    "end": "1211070"
  },
  {
    "text": "So if we're going to\nessentially take the latence and make it go between\nstandard deviation of 0 and 1,",
    "start": "1211070",
    "end": "1219919"
  },
  {
    "text": "then why don't we just start\nwith that to begin with? You could. Basically, I think that\nwould end up being something",
    "start": "1219920",
    "end": "1224997"
  },
  {
    "text": "very similar as one of the\noriginal kind of like Monte Carlo approximation to the\nmarginal likelihood, where you would just guess\nthe z's and try",
    "start": "1224997",
    "end": "1231980"
  },
  {
    "text": "to see what is the likelihood\nthat you get as a result. And the problem is that most\nz's wouldn't make sense.",
    "start": "1231980",
    "end": "1238640"
  },
  {
    "text": "And so that yeah, it\nwould be potentially-- if I'm understanding correctly,\nyou could probably cook up",
    "start": "1238640",
    "end": "1244070"
  },
  {
    "text": "something that would be-- that might work if the z's are\nsufficiently low dimensional.",
    "start": "1244070",
    "end": "1250258"
  },
  {
    "text": "But I think the problem\nis that if you just sample the z's just\nfrom the prior,",
    "start": "1250258",
    "end": "1255990"
  },
  {
    "text": "they might not be consistent. So when we first\nstarted learning models,",
    "start": "1255990",
    "end": "1263390"
  },
  {
    "text": "it is motivated that z's\nwill refer to some features.",
    "start": "1263390",
    "end": "1268530"
  },
  {
    "text": "So in real life,\nif we train a model and then I figured\nout what z or what",
    "start": "1268530",
    "end": "1276240"
  },
  {
    "text": "the z is some image\nlives to, do we just look at it qualitatively? If I change this,\nwhat is happening",
    "start": "1276240",
    "end": "1281399"
  },
  {
    "text": "in the image, and maybe then,\nI can get some social features. Yeah. So if you want to interpret\nthe meaning of the z's, what",
    "start": "1281400",
    "end": "1287813"
  },
  {
    "text": "you could do is you could,\nlet's say, start with an image or even start from\na random z, and then",
    "start": "1287813",
    "end": "1292830"
  },
  {
    "text": "see what you get as an output. And then can try to change\none axis, one of the latent variables, which\nrecall z is a vector.",
    "start": "1292830",
    "end": "1298658"
  },
  {
    "text": "So there's multiple ones. That you try to see if I\nchange one or do I get maybe thicker digits or maybe I change\nthe position of the digit,",
    "start": "1298658",
    "end": "1307530"
  },
  {
    "text": "that was one of the factors\nof variation in the data. And nothing guarantees\nthat that happens.",
    "start": "1307530",
    "end": "1312610"
  },
  {
    "text": "But we'll see in the next slide\nthat it kind of has the right-- it's encouraging\nsomething similar.",
    "start": "1312610",
    "end": "1319240"
  },
  {
    "text": "So once we're done training and\nnow we want to generate stuff,",
    "start": "1319240",
    "end": "1325120"
  },
  {
    "text": "can you repeat again\nwhat you exactly said? Like you sample from q of\nz again or what you do?",
    "start": "1325120",
    "end": "1332000"
  },
  {
    "text": "Yeah. So at generation time,\nthe q can be ignored. You can throw away the q. And what you do is instead\nof generating the z's",
    "start": "1332000",
    "end": "1341148"
  },
  {
    "text": "by sampling from q,\nwhich is what you would do during training,\nyou generate the z's by sampling from p,\nwhich is the prior VA.",
    "start": "1341148",
    "end": "1349330"
  },
  {
    "text": "So instead of going\nfrom kind of like left to right in this computational\ngraph, you start in the middle",
    "start": "1349330",
    "end": "1356590"
  },
  {
    "text": "and you generate the z's\nby sampling from the prior. And we do have the\nprior as part--",
    "start": "1356590",
    "end": "1363645"
  },
  {
    "text": "That's part of the\ngenerative model. And this term here\nencourages the fact",
    "start": "1363645",
    "end": "1369970"
  },
  {
    "text": "that what the z's that you get\nby going from left to right versus just injecting them\nby sampling from the prior",
    "start": "1369970",
    "end": "1376720"
  },
  {
    "text": "are similar. So you might expect\nsimilar behavior. So if the approximate\nposterior goes--",
    "start": "1376720",
    "end": "1384230"
  },
  {
    "text": "there's a problem. If the approximate posterior\ngoes too close to the prior, I recall that there's\na phenomenon called posterior collapse.",
    "start": "1384230",
    "end": "1390640"
  },
  {
    "text": "And I remember that a few\nyears ago that people tried to have a minimum distance. So let's say, we wanted the\napproximate posterior should not",
    "start": "1390640",
    "end": "1397510"
  },
  {
    "text": "get too close to the prior as\nin any sort of progress on this.",
    "start": "1397510",
    "end": "1404630"
  },
  {
    "text": "Yeah. So that's like if the posterior\nhere is too close to the prior,",
    "start": "1404630",
    "end": "1409780"
  },
  {
    "text": "then you're kind of ignoring\nthe x, which might not be what you want because\nrecall that we're",
    "start": "1409780",
    "end": "1415930"
  },
  {
    "text": "trying to find good latent\nrepresentations of the data. And so if there is\nzero mutual information",
    "start": "1415930",
    "end": "1421539"
  },
  {
    "text": "between the z and the x, maybe\nthat's not what you want. On the other hand, you can\nonly achieve that if somehow,",
    "start": "1421540",
    "end": "1428169"
  },
  {
    "text": "you're not really\nleveraging the mixture, all the kind of mixtures\nthat you have access",
    "start": "1428170",
    "end": "1434470"
  },
  {
    "text": "to when modeling the data. And so you can encourage, you\ncan avoid that kind of behavior",
    "start": "1434470",
    "end": "1440770"
  },
  {
    "text": "by choosing simple\np of x, given z. Because then you're\nforced to use the z's",
    "start": "1440770",
    "end": "1447250"
  },
  {
    "text": "to model different data points. Like if p of x,\ngiven z, is already a very powerful\nautoregressive model,",
    "start": "1447250",
    "end": "1454180"
  },
  {
    "text": "then you don't need a mixture\nof complicated autoregressive models. You can use the same z's to\nmodel the entire data set,",
    "start": "1454180",
    "end": "1460630"
  },
  {
    "text": "and then you're not going\nto use the latent variables. And you're going to have exactly\nthat problem, where you can just",
    "start": "1460630",
    "end": "1467380"
  },
  {
    "text": "choose this q to be just the\nprior, ignore the x completely. And everything would work\nbecause you're ignoring the z,",
    "start": "1467380",
    "end": "1475060"
  },
  {
    "text": "you're not using it at all. And there are ways\nto try to encourage",
    "start": "1475060",
    "end": "1480785"
  },
  {
    "text": "the VAE have more or less\nmutual information with respect between the x and the z.",
    "start": "1480785",
    "end": "1486590"
  },
  {
    "text": "Sometimes, you want\nmore mutual information. You want the latent\nvariables to be highly informative about the inputs.",
    "start": "1486590",
    "end": "1493340"
  },
  {
    "text": "Sometimes you want to\ndiscard information. Maybe you have\nsensitive attributes and you don't want the\nlatent representations",
    "start": "1493340",
    "end": "1498980"
  },
  {
    "text": "to capture sensitive attributes\nthat you have in the data. And so maybe you want to\nreduce the mutual information.",
    "start": "1498980",
    "end": "1505640"
  },
  {
    "text": "So there are flavors of this\ntraining objective, where you can encourage more or\nless mutual information",
    "start": "1505640",
    "end": "1512140"
  },
  {
    "text": "between the latent variables\nand the observed ones.",
    "start": "1512140",
    "end": "1517280"
  },
  {
    "text": "We are not really training\np of z [INAUDIBLE],, right? P of z is not trained here. P of z is fixed. You could, but here, it's not.",
    "start": "1517280",
    "end": "1523930"
  },
  {
    "text": " Right. So then maybe that's\nanother way of thinking",
    "start": "1523930",
    "end": "1530980"
  },
  {
    "text": "about what a variational\nautoencoder is doing that kind of gets at the\ncompression kind of behavior",
    "start": "1530980",
    "end": "1537100"
  },
  {
    "text": "and why we're sort of like\ndiscovering a latent structure that might be meaningful.",
    "start": "1537100",
    "end": "1542180"
  },
  {
    "text": "Like, you can imagine\nthis kind of setup where Alice is an astronaut\nand she goes on a space mission",
    "start": "1542180",
    "end": "1551049"
  },
  {
    "text": "and she needs to send images\nback to earth, back to Bob. And the images are too big.",
    "start": "1551050",
    "end": "1557059"
  },
  {
    "text": "And so maybe the only\nthing that she can do is she can only send\none bit of information or just a single real number\nor something like that.",
    "start": "1557060",
    "end": "1565310"
  },
  {
    "text": "And so the way she does it\nis by using this encoder, q.",
    "start": "1565310",
    "end": "1570320"
  },
  {
    "text": "And given an image, she\nbasically compresses it by obtaining a compact\nrepresentation, z.",
    "start": "1570320",
    "end": "1580450"
  },
  {
    "text": "And so if you imagine that z is\njust a single binary variable,",
    "start": "1580450",
    "end": "1586370"
  },
  {
    "text": "then you can either map\nan image to a 0 or a 1. So you can only\nsend one bit, that's",
    "start": "1586370",
    "end": "1591649"
  },
  {
    "text": "the only thing you can do. If z is a real number, then\nyou can map different images",
    "start": "1591650",
    "end": "1597590"
  },
  {
    "text": "to different real numbers. But the only thing you can send\nto Bob is a single real number.",
    "start": "1597590",
    "end": "1603170"
  },
  {
    "text": "And then what Bob does is\nBob tries to reconstruct the original image.",
    "start": "1603170",
    "end": "1608190"
  },
  {
    "text": "And you do that through this\ndecoder, this decompressor, which tries to infer x, given\nthe message that he receives.",
    "start": "1608190",
    "end": "1618970"
  },
  {
    "text": "And if you think about this\nkind of scheme will work. Well, if this\nautoencoding loss--",
    "start": "1618970",
    "end": "1628900"
  },
  {
    "text": "well, if the loss is low. If this term is\nlarge, then it means that Bob is actually pretty--",
    "start": "1628900",
    "end": "1635530"
  },
  {
    "text": "is doing a pretty good job\nat assigning high probability to the image that Alice\nwas sending, given",
    "start": "1635530",
    "end": "1642850"
  },
  {
    "text": "the message that he receives. There's not a lot\nof information lost by sending the messages\nthrough by compressing down",
    "start": "1642850",
    "end": "1650650"
  },
  {
    "text": "to a single z variable. And you can kind\nof imagine that,",
    "start": "1650650",
    "end": "1658220"
  },
  {
    "text": "if you can only send maybe\none bit of information, then there's going to be\nsome loss of information.",
    "start": "1658220",
    "end": "1666240"
  },
  {
    "text": "But you can what\nyou're going to try to do is you're going to\ntry to cluster together images that look similar.",
    "start": "1666240",
    "end": "1671600"
  },
  {
    "text": "And you only have two groups of\nimages, and you take one group and you say, OK, these\nare the zero bit.",
    "start": "1671600",
    "end": "1677130"
  },
  {
    "text": "The other group is\ngoing to be the one bit. And that's the best you can\ndo with that kind of setup.",
    "start": "1677130",
    "end": "1682769"
  },
  {
    "text": "And so the fact\nthat this is small, it's kind of like forcing you\nto maybe discover features.",
    "start": "1682770",
    "end": "1691470"
  },
  {
    "text": "You might say, OK,\nthere is a dog, it's running with a Frisbee. There's grass. That's a more compact\nkind of representation",
    "start": "1691470",
    "end": "1698870"
  },
  {
    "text": "of the input that comes in. And that's the z variable. And the term, this\nKL divergence term",
    "start": "1698870",
    "end": "1706520"
  },
  {
    "text": "is basically forcing the\ndistribution of our messages to have a specific distribution.",
    "start": "1706520",
    "end": "1712060"
  },
  {
    "text": "And if this term\nis small, then it means that basically Bob can\nkind of like generate messages",
    "start": "1712060",
    "end": "1719650"
  },
  {
    "text": "by himself without actually\nreceiving them from Alice. He can just sample\nfrom the prior,",
    "start": "1719650",
    "end": "1726010"
  },
  {
    "text": "generate a message\nthat looks realistic because it's very close in\ndistribution to what Alice",
    "start": "1726010",
    "end": "1731049"
  },
  {
    "text": "could have sent him. And then by just decoding\nthat, he generates images.",
    "start": "1731050",
    "end": "1736090"
  },
  {
    "text": "So instead of receiving the\nmessages, the descriptions from Alice, it just generates\nthe description himself",
    "start": "1736090",
    "end": "1742300"
  },
  {
    "text": "by sampling from the prior. And that's how you\ngenerate images. And that's really what\nthe objective is doing.",
    "start": "1742300",
    "end": "1749040"
  },
  {
    "text": " When you're training,\nhow do you know--",
    "start": "1749040",
    "end": "1756070"
  },
  {
    "text": "like, how do you\ncompute this divergence? I feel like it assumes p of z.",
    "start": "1756070",
    "end": "1762830"
  },
  {
    "text": "Yeah. How do you compute\nthe divergence? So recall that this\nis just the ELBO. So I'm just rewriting the ELBO\nin a slightly different way.",
    "start": "1762830",
    "end": "1769910"
  },
  {
    "text": "But if you look at the first\nline, everything is computable, everything is tractable.",
    "start": "1769910",
    "end": "1774990"
  },
  {
    "text": "Everything is the same\nthing we derived before. ",
    "start": "1774990",
    "end": "1784070"
  },
  {
    "text": "OK. Questions on this? Yeah.",
    "start": "1784070",
    "end": "1789302"
  },
  {
    "text": "So like, let's say,\nyou have a data set with 1,000\ndogs and five cats, would your latent representation\nstart allocating more bits to,",
    "start": "1789302",
    "end": "1799160"
  },
  {
    "text": "I guess, the class\nyou're seeing more of? So you're better\nable to reconstruct that or you have to manually--",
    "start": "1799160",
    "end": "1805040"
  },
  {
    "text": "In that case, if you have a\nlot more data points belonging to some class, it would\npay more attention to those because you're going to incur\nyou're going to see them often",
    "start": "1805040",
    "end": "1812960"
  },
  {
    "text": "and so you want to be\nable to encode them well. So if something is very\nrare, you never see it. You don't care about encoding\nit particularly well because you",
    "start": "1812960",
    "end": "1820460"
  },
  {
    "text": "just care about the average\nperformance across the data set. Got you. And I guess, the flip side\nto that question is like,",
    "start": "1820460",
    "end": "1826437"
  },
  {
    "text": "let's say, there is a feature\nfor some semantic attribute that you want to pay\nmore attention to. I'm just wondering\nif there's work",
    "start": "1826437",
    "end": "1832237"
  },
  {
    "text": "done so that the\nmodel specifically focuses on those portions of the\nimage more than other portions.",
    "start": "1832237",
    "end": "1838080"
  },
  {
    "text": "Yeah. So there's different\nways to do it. Like one is if you know\nwhat you care about, you could try to change this\nreconstruction laws to pay more",
    "start": "1838080",
    "end": "1844430"
  },
  {
    "text": "attention to things that\nmatters, because right now the reconstruction\nloss is just L2,",
    "start": "1844430",
    "end": "1849920"
  },
  {
    "text": "which might not\nbe what you want. Maybe you have-- there are\nsome features you care more. So you can change the\nreconstruction loss",
    "start": "1849920",
    "end": "1856687"
  },
  {
    "text": "to pay more attention\nto those things. And that's the same\nthing as changing the shape of this\ndistribution, essentially.",
    "start": "1856687",
    "end": "1863720"
  },
  {
    "text": "To say, OK, I care more about\ncertain things versus others. The other thing you can\ndo is if you have labels",
    "start": "1863720",
    "end": "1870679"
  },
  {
    "text": "and you know-- because right\nnow, this is kind of made up. There is no-- it's discovered\nwhatever it discovers.",
    "start": "1870680",
    "end": "1877580"
  },
  {
    "text": "There is no guarantee\nthat it finds anything semantically meaningful. So the only way to force that is\nif you have somehow labeled data",
    "start": "1877580",
    "end": "1885860"
  },
  {
    "text": "and you know somebody is\ncaptioning the images for you or something, then you can try\nto change the training objective",
    "start": "1885860",
    "end": "1892730"
  },
  {
    "text": "and make sure that\nwhen you know what the values of the z variables\nis, then your life is easier.",
    "start": "1892730",
    "end": "1899510"
  },
  {
    "text": "You can just do maximum\nlikelihood on those. That's going to force the model\nto use them in a certain way.",
    "start": "1899510",
    "end": "1906580"
  },
  {
    "text": "So say, when training,\nshould we always sample the most likely z\nor the sampling just sort",
    "start": "1906580",
    "end": "1913900"
  },
  {
    "text": "of randomly help [INAUDIBLE]. Yeah. So the question is\nwhether we should always choose the most likely z.",
    "start": "1913900",
    "end": "1920110"
  },
  {
    "text": "And if you think about the ELBO\nderivation, the answer is no. You always want to\nsample according",
    "start": "1920110",
    "end": "1925840"
  },
  {
    "text": "to the p of z, given x. So you would like to really\ninvert the generative process",
    "start": "1925840",
    "end": "1932320"
  },
  {
    "text": "and try to find z's that are\nlikely under that posterior, which is intractable to compute.",
    "start": "1932320",
    "end": "1938020"
  },
  {
    "text": "But we know that that will\nbe the optimal choice for q. I was saying like should you\nalways sample the most likely seed based on the encoder.",
    "start": "1938020",
    "end": "1945669"
  },
  {
    "text": "We shouldn't. The objective is just\nto sample from it. Because there could be many.",
    "start": "1945670",
    "end": "1950740"
  },
  {
    "text": "And there might be many\nother possible explanations",
    "start": "1950740",
    "end": "1956300"
  },
  {
    "text": "or possible completions. And you will really want\nto cover all of them. So for each input x,\nis it better practice",
    "start": "1956300",
    "end": "1962240"
  },
  {
    "text": "to sample multiple and\ndo multiple generations? Yeah. So the question is, should\nyou get more than one?",
    "start": "1962240",
    "end": "1968640"
  },
  {
    "text": "Yes, in the sense that\njust like it's Monte Carlo. So the more z's you get,\nthe more samples you get.",
    "start": "1968640",
    "end": "1975330"
  },
  {
    "text": "Recall, you really want\nan expectation here. We cannot do the expectation. You can only approximate\nit with a sample average.",
    "start": "1975330",
    "end": "1983100"
  },
  {
    "text": "The more samples you\nhave in your average, the closer it is to the\ntrue expected value. So the better, more\naccurate of an estimate",
    "start": "1983100",
    "end": "1990980"
  },
  {
    "text": "you have of the loss\nand the gradient. But it's going to\nbe more expensive. So in practice, you might\nwant to just choose one.",
    "start": "1990980",
    "end": "1996730"
  },
  {
    "text": " Yeah. I'm just curious about\nif I have that much data,",
    "start": "1996730",
    "end": "2005200"
  },
  {
    "text": "can I use like reconstruction\nto the training data to start out at all?",
    "start": "2005200",
    "end": "2012149"
  },
  {
    "text": "So you would augment\nthe training data with samples from the\nmodel, essentially? That's something that\npeople are starting",
    "start": "2012150",
    "end": "2018180"
  },
  {
    "text": "to explore using synthetic data\nto train generative models. And there is some\ntheoretical studies showing",
    "start": "2018180",
    "end": "2023850"
  },
  {
    "text": "what happens if you start\nusing synthetic data",
    "start": "2023850",
    "end": "2028950"
  },
  {
    "text": "and put it in the training set. And there are some\ntheoretical results showing that under\nsome assumptions,",
    "start": "2028950",
    "end": "2035910"
  },
  {
    "text": "this procedure diverges. And this, I think is\ncalled generative models going mad or something.",
    "start": "2035910",
    "end": "2041310"
  },
  {
    "text": "And meaning that bad\nthings happen if you start doing that kind of thing.",
    "start": "2041310",
    "end": "2046860"
  },
  {
    "text": "But it's under some\nassumptions that are not really very realistic in practice. So it's not clear.",
    "start": "2046860",
    "end": "2053129"
  },
  {
    "text": "I guess, a question to\nthe previous question that was asked, you said, we don't\ndo an important sampling",
    "start": "2053130",
    "end": "2059040"
  },
  {
    "text": "during inference time.  We don't sample from the\nmore likely outcomes.",
    "start": "2059040",
    "end": "2068369"
  },
  {
    "text": "We sample from the conditional. We don't pick the most likely z. I see. So we're still more likely\nto pick the more likely.",
    "start": "2068370",
    "end": "2075280"
  },
  {
    "text": "Yeah. The more likely they are,\nthe more likely we pick them. But we don't just pick\nthe most likely one. We just sample the distribution.",
    "start": "2075280",
    "end": "2081855"
  },
  {
    "text": " OK. So now the other thing I\nwanted to talk about today",
    "start": "2081855",
    "end": "2089010"
  },
  {
    "text": "is start talking about\nflow models, which is another variant,\nanother way of going around",
    "start": "2089010",
    "end": "2099890"
  },
  {
    "text": "the intractable, kind of like\nmarginal probability in a latent variable model.",
    "start": "2099890",
    "end": "2104945"
  },
  {
    "text": "So far, we've seen\nautoregressive models, we've seen variational\nautoencoders, where the marginal probability,\nmarginal likelihood",
    "start": "2104945",
    "end": "2112760"
  },
  {
    "text": "is given by this mixture model,\nthe integral over the latent variables. And we've seen that\nautoregressive models",
    "start": "2112760",
    "end": "2121010"
  },
  {
    "text": "are nice because you don't have\nto use variational inference. You directly have access to\nthe probability of the data.",
    "start": "2121010",
    "end": "2126410"
  },
  {
    "text": "And you don't have to deal with\nthis encoders and decoders. And VAEs are nice because, well,\nyou get the representation.",
    "start": "2126410",
    "end": "2136020"
  },
  {
    "text": "Z, and you can actually\ndefine pretty flexible",
    "start": "2136020",
    "end": "2141290"
  },
  {
    "text": "marginal distributions. You can generate in one shot. So they have some advantages\nthat autoregressive models don't",
    "start": "2141290",
    "end": "2147230"
  },
  {
    "text": "have. But the challenge with the\nlatent variable model was that,",
    "start": "2147230",
    "end": "2152780"
  },
  {
    "text": "well, we cannot evaluate\nthis marginal probability. And so training\nwas a pain and we",
    "start": "2152780",
    "end": "2158300"
  },
  {
    "text": "have to come up with the ELBO. And so what flow models do is\nit's a type of latent variable",
    "start": "2158300",
    "end": "2164010"
  },
  {
    "text": "model, kind of like a VAE\nthat has spatial structure so that you don't need to\ndo variational inference.",
    "start": "2164010",
    "end": "2170070"
  },
  {
    "text": "And you can train them\nin a more direct way. So it's actually very\nefficient to evaluate",
    "start": "2170070",
    "end": "2175950"
  },
  {
    "text": "the probability of\na observed data x even though you have\nlatent variables. And so which means\nthat you can train them",
    "start": "2175950",
    "end": "2182760"
  },
  {
    "text": "by maximum likelihood. And so the kind\nof idea is that we",
    "start": "2182760",
    "end": "2192730"
  },
  {
    "text": "would like to have\na model distribution over the visible data,\nover the observed data that it's easy to evaluate\nand easy to sample from.",
    "start": "2192730",
    "end": "2201950"
  },
  {
    "text": "Because then, we can generate\nefficiently at inference time. And we know that there is\nmany simple distributions that",
    "start": "2201950",
    "end": "2209470"
  },
  {
    "text": "would satisfy these properties\nlike a Gaussian distribution or a uniform distribution.",
    "start": "2209470",
    "end": "2215020"
  },
  {
    "text": "But what we want is some\nkind of distribution that has a complicated\nshape, kind",
    "start": "2215020",
    "end": "2220570"
  },
  {
    "text": "of like the one you see here. So here, the colors represent\nprobability, density, mass.",
    "start": "2220570",
    "end": "2226790"
  },
  {
    "text": "And so if you have\na Gaussian and it has this relatively simple\nshape, where all the probability mass is sort of like\ncentered around the mean.",
    "start": "2226790",
    "end": "2234079"
  },
  {
    "text": "And so if you think\nabout modeling images with something like\na Gaussian, it's not going to work very\nwell because there's only going to be a single point\nand all the probability",
    "start": "2234080",
    "end": "2241150"
  },
  {
    "text": "mass is shaped around it. Think about a\nuniform distribution. Again, it's not going to\nbe very, very practical",
    "start": "2241150",
    "end": "2248270"
  },
  {
    "text": "to model real data. And you want something\nmuch more multimodal, something that looks like this,\nwhere you can have a probability",
    "start": "2248270",
    "end": "2254810"
  },
  {
    "text": "mass somewhere and then\nhave no [INAUDIBLE],, the probability goes, decreases\na lot and then goes up",
    "start": "2254810",
    "end": "2261290"
  },
  {
    "text": "and then decreases like you\nwant complicated kind of shapes for this p theta of x, which is\nkind of like the same reason we",
    "start": "2261290",
    "end": "2269120"
  },
  {
    "text": "were using mixtures as\none way of achieving this kind of complicated\nshapes by taking",
    "start": "2269120",
    "end": "2274520"
  },
  {
    "text": "a mixture of many\nsimple distributions. The way flow models\ndo is they instead",
    "start": "2274520",
    "end": "2281540"
  },
  {
    "text": "try to transform essentially\nsimple distribution into more",
    "start": "2281540",
    "end": "2288320"
  },
  {
    "text": "complicated ones by applying\ninvertible transformations.",
    "start": "2288320",
    "end": "2293330"
  },
  {
    "text": "And that's essentially of like\na variational autoencoder.",
    "start": "2293330",
    "end": "2299050"
  },
  {
    "text": "So it's going to be the same\nkind of generative model, where you have a latent\nvariable z that you sample from.",
    "start": "2299050",
    "end": "2307619"
  },
  {
    "text": "And that's again, sampled from\na simple prior distribution like a unit Gaussian,\nfixed Gaussian",
    "start": "2307620",
    "end": "2315020"
  },
  {
    "text": "with fixed mean\nand some kind of, let's say identity covariance.",
    "start": "2315020",
    "end": "2320039"
  },
  {
    "text": "And then you transform it. In a VAE, what you\nwould do is you",
    "start": "2320040",
    "end": "2325070"
  },
  {
    "text": "would compute the\nconditional distribution of x given z by passing\nz through some two",
    "start": "2325070",
    "end": "2333110"
  },
  {
    "text": "neural networks. And what we've seen\nis that this is one way of getting a\npotentially complicated",
    "start": "2333110",
    "end": "2340550"
  },
  {
    "text": "marginal distribution because\nof this mixture in behavior. But you have this\nissue that when",
    "start": "2340550",
    "end": "2346369"
  },
  {
    "text": "you want to evaluate the\nprobability of one x, you have to go through all\npossible z's to figure out",
    "start": "2346370",
    "end": "2353036"
  },
  {
    "text": "which ones are likely to\nproduce that particular image, let's say, x that\nyou have access to.",
    "start": "2353036",
    "end": "2360690"
  },
  {
    "text": "And this enumeration,\nover all possible z's, is the tricky part,\nis the hard part.",
    "start": "2360690",
    "end": "2367150"
  },
  {
    "text": "And there could be multiple z's\nthat produce the image or even just finding which\nz is producing",
    "start": "2367150",
    "end": "2374580"
  },
  {
    "text": "that is likely to have produced\nthe image x that you have access to is not easy. And the way the VAE\nis get around this",
    "start": "2374580",
    "end": "2381359"
  },
  {
    "text": "is by using the encoder\nthat is essentially trying to guess\ngiven the x, which z's are likely to have produced\nthe image that you have access",
    "start": "2381360",
    "end": "2390180"
  },
  {
    "text": "to. And one way to try to get\naround the problem by design",
    "start": "2390180",
    "end": "2397650"
  },
  {
    "text": "is to construct conditionals,\nsuch that inverting them is easy.",
    "start": "2397650",
    "end": "2404609"
  },
  {
    "text": "And one way to do that is to\napply a deterministic invertible",
    "start": "2404610",
    "end": "2411510"
  },
  {
    "text": "function to the\nlatent variable z. So instead of passing\nthe z through these two",
    "start": "2411510",
    "end": "2417450"
  },
  {
    "text": "neural network, and then\nsampling from a Gaussian, defined by the neural networks,\nwe directly transform the latent",
    "start": "2417450",
    "end": "2424950"
  },
  {
    "text": "variable by applying a single\ndeterministic invertible",
    "start": "2424950",
    "end": "2429960"
  },
  {
    "text": "function. And the reason this is good\nis that if we do this, then",
    "start": "2429960",
    "end": "2435060"
  },
  {
    "text": "it's trivial to figure out what\nwas the z to produce that x, because we know\nthere is only one.",
    "start": "2435060",
    "end": "2441365"
  },
  {
    "text": "And the only thing\nyou have to do is you have to be able\nto invert the mapping. So to the extent, that\nby design, this functions",
    "start": "2441365",
    "end": "2449339"
  },
  {
    "text": "that we use are invertible. And they're easy to invert,\nhopefully, and deterministic.",
    "start": "2449340",
    "end": "2456030"
  },
  {
    "text": "So that there's always\nonly one z that could have produced any particular x. Then that kind of solves\nthe problem at the root",
    "start": "2456030",
    "end": "2464880"
  },
  {
    "text": "that we had when we were dealing\nwith variational autoencoders. And that's really the\nwhole idea of this class",
    "start": "2464880",
    "end": "2472980"
  },
  {
    "text": "of generative models called\nflow models, which you can think of it as a VAE, where\nthe mapping from z to x",
    "start": "2472980",
    "end": "2479700"
  },
  {
    "text": "is deterministic and invertible,\nwhich makes, as we'll see,",
    "start": "2479700",
    "end": "2486740"
  },
  {
    "text": "learning much easier. [INAUDIBLE] the z, the latent,\nand priors that we choose",
    "start": "2486740",
    "end": "2495589"
  },
  {
    "text": "has the same dimensions with x? Yeah. So that's going to be\none that will come up.",
    "start": "2495590",
    "end": "2501359"
  },
  {
    "text": "But that's a great\nobservation that if we want this mapping to\nbe invertible, then",
    "start": "2501360",
    "end": "2506450"
  },
  {
    "text": "we're sort of requiring z and x\nto have the same dimensionality.",
    "start": "2506450",
    "end": "2511650"
  },
  {
    "text": "And so one of the\nthings you lose if you do this is\nthat there is no longer this idea of a compressed\nkind of representation",
    "start": "2511650",
    "end": "2519350"
  },
  {
    "text": "because now z and\nx end up having the same number of dimensions. ",
    "start": "2519350",
    "end": "2528315"
  },
  {
    "text": "OK. So that's the high level\nmotivation, the high level idea. Now let's see how it's\nactually done in practice.",
    "start": "2528315",
    "end": "2536010"
  },
  {
    "text": "So just as and let's start\nwith a simple refresher on what happens if you\ntake random variables",
    "start": "2536010",
    "end": "2542120"
  },
  {
    "text": "and you transform them through,\nlet's say, invertible functions.",
    "start": "2542120",
    "end": "2547450"
  },
  {
    "text": "And so just to start,\nlet's say that we have a single continuous\nrandom variable x.",
    "start": "2547450",
    "end": "2554270"
  },
  {
    "text": "And you might\nrecall that one way to describe the\nrandom variable is",
    "start": "2554270",
    "end": "2559280"
  },
  {
    "text": "through the CDF, the cumulative\ndensity function, which basically tells you for\nevery scalar a, what",
    "start": "2559280",
    "end": "2565970"
  },
  {
    "text": "is the probability that the\nrandom variable is less than or equal to a? And then the other way to\ndescribe the random variable",
    "start": "2565970",
    "end": "2572540"
  },
  {
    "text": "is through the PDF, which is\njust the derivative of the CDF. ",
    "start": "2572540",
    "end": "2579440"
  },
  {
    "text": "And typically, we\ndescribe random variables by specifying a kind of\nfunctional form for this PDF",
    "start": "2579440",
    "end": "2589040"
  },
  {
    "text": "or CDF. In the case of a Gaussian, it\nmight look something like this. You have two parameters, the\nmean and the standard deviation.",
    "start": "2589040",
    "end": "2596520"
  },
  {
    "text": "And then you get\nthe shape of the PDF by applying the function.",
    "start": "2596520",
    "end": "2604150"
  },
  {
    "text": "And it could be uniform,\nin which case, the PDF would have this kind\nof functional form,",
    "start": "2604150",
    "end": "2610380"
  },
  {
    "text": "where if it's uniform\nbetween and b, then the PDF is 0\noutside that interval.",
    "start": "2610380",
    "end": "2619590"
  },
  {
    "text": "And it's 1 over the length of\nthe interval when x is between a and b.",
    "start": "2619590",
    "end": "2625980"
  },
  {
    "text": "And same thing holds when\nyou have random vectors.",
    "start": "2625980",
    "end": "2631300"
  },
  {
    "text": "So if you have a collection\nof random variables, we can describe this\ncollection of random variables",
    "start": "2631300",
    "end": "2638130"
  },
  {
    "text": "through the joint\nprobability density function. And again, an example would be\nif these random variables are",
    "start": "2638130",
    "end": "2645300"
  },
  {
    "text": "jointly distributed as\na Gaussian distribution, then the PDF would have that\nkind of functional form.",
    "start": "2645300",
    "end": "2652220"
  },
  {
    "text": "So now, x is a vector. So it's a sequence of numbers. And you can get the probability\ndensity at a particular point",
    "start": "2652220",
    "end": "2659750"
  },
  {
    "text": "by evaluating this function. And again, the problem here is\nthat this kind of simple PDFs,",
    "start": "2659750",
    "end": "2667610"
  },
  {
    "text": "they are easy to evaluate,\nthey're easy to sample from, but they are not\nvery complicated. Like the shape is pretty\nsimple, kind of the probability",
    "start": "2667610",
    "end": "2675770"
  },
  {
    "text": "only depends on how far x\nis from this mean vector mu.",
    "start": "2675770",
    "end": "2680990"
  },
  {
    "text": "And kind of that\ndetermines the shape. And you don't have many\nparameters to change the shape of this function.",
    "start": "2680990",
    "end": "2690240"
  },
  {
    "text": "OK. Now let's see what happens when\nwe transform random variables",
    "start": "2690240",
    "end": "2695840"
  },
  {
    "text": "by applying functions to them. So let's say that z is a uniform\nrandom variable between 0 and 2.",
    "start": "2695840",
    "end": "2704030"
  },
  {
    "text": "And pz is the density\nof this random variable. Now, what is the density\nPDF evaluated at 1?",
    "start": "2704030",
    "end": "2713440"
  },
  {
    "text": " 0.5.",
    "start": "2713440",
    "end": "2720089"
  },
  {
    "text": "0.5, yeah. So 1/2. And just a sanity check,\nif you integrate over the PDF over the interval,\n0 to 2, you get 1.",
    "start": "2720090",
    "end": "2727545"
  },
  {
    "text": "This is what you want. Now, let's say that we define\na new random variable x",
    "start": "2727545",
    "end": "2734070"
  },
  {
    "text": "by multiplying it by 4. And so now, we have two\nrandom variables, x and z.",
    "start": "2734070",
    "end": "2739230"
  },
  {
    "text": "And x is just 4x. And now let's say that we\nwant to evaluate the-- we want",
    "start": "2739230",
    "end": "2745410"
  },
  {
    "text": "to figure out what is the PDF\nof this new random variable that we've constructed\njust by multiplying by 4.",
    "start": "2745410",
    "end": "2751680"
  },
  {
    "text": "And one thing you\nmight be tempted to do is to do something like this. And this is going to be wrong.",
    "start": "2751680",
    "end": "2757390"
  },
  {
    "text": "So the probability,\nlet's say, if you want to evaluate it at 4,\nthis kind of the probability",
    "start": "2757390",
    "end": "2763440"
  },
  {
    "text": "that x takes value 4. And we know that x is 4z. And so this is kind\nof the probability",
    "start": "2763440",
    "end": "2769440"
  },
  {
    "text": "that z takes value 1, which\nis what we had before, which is 1/2.",
    "start": "2769440",
    "end": "2775530"
  },
  {
    "text": "And this is wrong. This is not the right answer. It's pretty clear\nthat 4z is just",
    "start": "2775530",
    "end": "2781500"
  },
  {
    "text": "going to be a uniform random\nvariable between 0 and 8. And so the density\nis actually 1/8",
    "start": "2781500",
    "end": "2787920"
  },
  {
    "text": "because it has to integrate\nto 1 over the interval.",
    "start": "2787920",
    "end": "2792960"
  },
  {
    "text": "So this kind of\nreplacing a change of variables inside the PDF\ncalculation is not correct.",
    "start": "2792960",
    "end": "2801700"
  },
  {
    "text": "And what you have\nto do is you have to use the change of variable\nformula, which you might have seen in previous classes.",
    "start": "2801700",
    "end": "2809319"
  },
  {
    "text": "The idea is that when you apply\nsome invertible transformation.",
    "start": "2809320",
    "end": "2815740"
  },
  {
    "text": "And so you define z-- x as f of z.",
    "start": "2815740",
    "end": "2823299"
  },
  {
    "text": "f is invertible. And so equivalently,\nyou can get z by applying the\ninverse of f to x,",
    "start": "2823300",
    "end": "2830270"
  },
  {
    "text": "which we're going to denote h. So h is the inverse of f.",
    "start": "2830270",
    "end": "2835520"
  },
  {
    "text": "So h applied to f is\nthe identity function.",
    "start": "2835520",
    "end": "2840710"
  },
  {
    "text": "And if you want to get the\ndensity of this random variable that we get by transforming z\nthrough this invertible mapping,",
    "start": "2840710",
    "end": "2849950"
  },
  {
    "text": "it is a kind of a p of\nz evaluated at h of x. So that's the kind of\nthing we're doing before.",
    "start": "2849950",
    "end": "2855440"
  },
  {
    "text": "But you have to rescale by the\nabsolute value of the derivative of this function h.",
    "start": "2855440",
    "end": "2862670"
  },
  {
    "text": "And so in the previous\nexample, let's say, the function is just\nmultiplying by 4.",
    "start": "2862670",
    "end": "2870480"
  },
  {
    "text": "And so if you were to apply\nthe formula in this case, the inverse function\nis just dividing by 4.",
    "start": "2870480",
    "end": "2878250"
  },
  {
    "text": "And then the derivative\nof h is just one quarter. It's just a constant.",
    "start": "2878250",
    "end": "2884440"
  },
  {
    "text": "And so if we want to\nevaluate the probability of this transformed random\nvariable evaluated at 4,",
    "start": "2884440",
    "end": "2889770"
  },
  {
    "text": "what you do is you evaluate the\nprobability of z at 4 over 4,",
    "start": "2889770",
    "end": "2895590"
  },
  {
    "text": "which is 1. But then you have to multiply\nby this scaling factor, which is the derivative,\nevaluated at 4,",
    "start": "2895590",
    "end": "2903690"
  },
  {
    "text": "or the absolute value\nof the derivative. And this is giving us\nthe right answer, 1/8.",
    "start": "2903690",
    "end": "2910580"
  },
  {
    "text": "So this part here of 1 is\nkind of like the naive thing",
    "start": "2910580",
    "end": "2916700"
  },
  {
    "text": "we try to do that was wrong. And it becomes\nright if you account for this derivative of h term.",
    "start": "2916700",
    "end": "2925460"
  },
  {
    "text": "That is rescaling things. So then we get 1/8. And more interesting\nexample could",
    "start": "2925460",
    "end": "2932600"
  },
  {
    "text": "be something like if\ninstead of multiplying by 4, we apply an\nexponential function.",
    "start": "2932600",
    "end": "2939680"
  },
  {
    "text": "So we have z, which again\nis something simple, a uniform random\nvariable between 0 and 2.",
    "start": "2939680",
    "end": "2945120"
  },
  {
    "text": "But now we define x as\nthe exponential of z. Now we can work out\nwhat is the density",
    "start": "2945120",
    "end": "2951080"
  },
  {
    "text": "of this new random\nvariable that we get through this transformation.",
    "start": "2951080",
    "end": "2956450"
  },
  {
    "text": "What is the inverse of\nthe exponential function?",
    "start": "2956450",
    "end": "2961619"
  },
  {
    "text": "The log. So h of x is the log. And then we can\napply-- if you want",
    "start": "2961620",
    "end": "2967650"
  },
  {
    "text": "to evaluate the density\nof this random variable x at a particular\npoint, what we do",
    "start": "2967650",
    "end": "2973200"
  },
  {
    "text": "is we evaluate the density\nof z at the inverse, and then we scale\nby the derivative.",
    "start": "2973200",
    "end": "2980860"
  },
  {
    "text": "So we take x, we invert it\nto get the corresponding z. And there is only one\nz that maps to that x.",
    "start": "2980860",
    "end": "2988380"
  },
  {
    "text": "We evaluate the density of that\nz under the prior distribution p of z, and then we always\nrescale by this derivative.",
    "start": "2988380",
    "end": "2998760"
  },
  {
    "text": "And so in this case,\np of z is uniform. So it's just the 1/2\neverywhere because it's",
    "start": "2998760",
    "end": "3008660"
  },
  {
    "text": "uniform between 0 and 2. And then the derivative of\nthe logarithm is 1 over x.",
    "start": "3008660",
    "end": "3014990"
  },
  {
    "text": "And so now we see that the\nPDF of this random variable x",
    "start": "3014990",
    "end": "3020119"
  },
  {
    "text": "has this more interesting shape. It's kind of 1 over 2x. So we started out with\nsomething pretty simple,",
    "start": "3020120",
    "end": "3026750"
  },
  {
    "text": "just a constant basically. And by applying an\ninvertible transformation, we got a new random\nvariable, which",
    "start": "3026750",
    "end": "3032690"
  },
  {
    "text": "is a more interesting\nkind of shape. ",
    "start": "3032690",
    "end": "3038400"
  },
  {
    "text": "Again, hopefully, this is\njust a recap of formulas that you've seen\nbefore, but this",
    "start": "3038400",
    "end": "3044300"
  },
  {
    "text": "is kind of like a change of\nvariable that we're doing here. And you have to account\nfor these derivatives",
    "start": "3044300",
    "end": "3053360"
  },
  {
    "text": "when you apply a change\nof variables here. ",
    "start": "3053360",
    "end": "3061789"
  },
  {
    "text": "OK. Questions on this?",
    "start": "3061790",
    "end": "3066918"
  },
  {
    "text": "Does this sound familiar? ",
    "start": "3066918",
    "end": "3074880"
  },
  {
    "text": "OK. Now let's see. This is the formula\nfor the 1D case.",
    "start": "3074880",
    "end": "3081540"
  },
  {
    "text": "And you can see our\nproof, actually. It's actually pretty simple.",
    "start": "3081540",
    "end": "3087990"
  },
  {
    "text": "We can work out the\nlevel of the CDFs, so the probability that this\nnew transformed random variable is less than a\nparticular number is just",
    "start": "3087990",
    "end": "3095010"
  },
  {
    "text": "the CDF evaluated at one point. Now we know that\nx is just f of z..",
    "start": "3095010",
    "end": "3101550"
  },
  {
    "text": "So the probability\nthat x is less than or equal to some number. It's the probability\nthat f of z is less than",
    "start": "3101550",
    "end": "3107220"
  },
  {
    "text": "or equal to that number. Then if you apply\nthe inverse of f on both sides of this\ninequality, which",
    "start": "3107220",
    "end": "3114300"
  },
  {
    "text": "you can because it's\na monotonic function, then you get that expression,\nwhich is just the CDF of z,",
    "start": "3114300",
    "end": "3122880"
  },
  {
    "text": "evaluated at h of x. And now just we\nknow that the PDF is",
    "start": "3122880",
    "end": "3129420"
  },
  {
    "text": "just the derivative of the CDF. So if you want to get the\ndensity of this random variable,",
    "start": "3129420",
    "end": "3137830"
  },
  {
    "text": "you just take the derivative\nof the left hand side. Or equivalently, you\ncan take the derivative of this expression\nthat we have here,",
    "start": "3137830",
    "end": "3144640"
  },
  {
    "text": "and then you just\nuse chain rule. So you get exactly\nwhat we had before, where you need to evaluate the\noriginal variable at h of x.",
    "start": "3144640",
    "end": "3154730"
  },
  {
    "text": "You take x, you invert it,\nyou evaluate the density at the corresponding z point.",
    "start": "3154730",
    "end": "3159800"
  },
  {
    "text": "But then because\nof the chain rule, you have to multiply by h prime.",
    "start": "3159800",
    "end": "3165165"
  },
  {
    "text": " That's where the\nformula comes from.",
    "start": "3165165",
    "end": "3171859"
  },
  {
    "text": "You can see you need an\nabsolute value because I guess, it could be decreasing.",
    "start": "3171860",
    "end": "3179339"
  },
  {
    "text": "And now, there's\nan equivalent way",
    "start": "3179340",
    "end": "3185610"
  },
  {
    "text": "of writing the same\nexpression, which will turn out to be somewhat useful.",
    "start": "3185610",
    "end": "3191340"
  },
  {
    "text": "If you want to compute the\nderivative of the inverse of a function, you\ncan actually get it",
    "start": "3191340",
    "end": "3197820"
  },
  {
    "text": "in terms of the derivative\nof the original function. There is this simple\ncalculus kind of formula",
    "start": "3197820",
    "end": "3206737"
  },
  {
    "text": "that you might have seen before. So if you want to compute\nthe derivative of the inverse of a function, which is h prime,\nwhich is what we have here,",
    "start": "3206737",
    "end": "3213060"
  },
  {
    "text": "you can get it in terms\nof the derivative of f, which is the original function\nevaluated at the inverse point.",
    "start": "3213060",
    "end": "3220470"
  },
  {
    "text": "So an equivalent way of\nwriting what we have here is that you can just\nevaluate the original PDF",
    "start": "3220470",
    "end": "3230010"
  },
  {
    "text": "at the inverse\npoint, and then you can multiply by 1\nover f prime of z, where f is the kind\nof forward map.",
    "start": "3230010",
    "end": "3238020"
  },
  {
    "text": "So you can basically\neither write it in terms of the\nderivative of the inverse, or you can write it in\nterms of the derivative",
    "start": "3238020",
    "end": "3243790"
  },
  {
    "text": "of the forward map and you\njust do 1 over instead of-- these two things are the same.",
    "start": "3243790",
    "end": "3251840"
  },
  {
    "text": "Yeah. We still need absolute values. ",
    "start": "3251840",
    "end": "3258960"
  },
  {
    "text": "Yeah. Why it'd be problematic\nif [INAUDIBLE] approach",
    "start": "3258960",
    "end": "3265020"
  },
  {
    "text": "to zero in the denominator. Yeah. So it has to be invertible.",
    "start": "3265020",
    "end": "3270060"
  },
  {
    "text": "So that kind of happen. But yeah, I mean, you need\nto be able to somehow compute these derivatives. And that's not going\nto be necessarily easy.",
    "start": "3270060",
    "end": "3278770"
  },
  {
    "text": "Yeah. Because if prime is zero,\nthen it means it's a constant, and then it's no\nlonger invertible.",
    "start": "3278770",
    "end": "3285390"
  },
  {
    "text": "OK. So that's the easy thing. Now let's see what happens when\nwe transform random vectors.",
    "start": "3285390",
    "end": "3292930"
  },
  {
    "text": "Because if you\nthink about a VAE, we want to transform random\nvectors into random vectors.",
    "start": "3292930",
    "end": "3298900"
  },
  {
    "text": "So we kind of need\nto understand what happens if we apply an\ninvertible transformation to a random variable that\nhas a simple distribution.",
    "start": "3298900",
    "end": "3307910"
  },
  {
    "text": "So let's say our\nrandom variable is a random vector z is now uniform\nover this unit, hypercube.",
    "start": "3307910",
    "end": "3318269"
  },
  {
    "text": "So we have n kind of dimensions. And each one of them is uniform.",
    "start": "3318270",
    "end": "3324730"
  },
  {
    "text": "And we want to understand\nwhat happens if we transform that random variable.",
    "start": "3324730",
    "end": "3330100"
  },
  {
    "text": "And as the just to\nget some intuition, we can start with\nlinear transformations. Just like before, we kind of\nstarted by saying multiply by 4",
    "start": "3330100",
    "end": "3338770"
  },
  {
    "text": "and see what happens. We can do the same\nthing and instead look at what happens\nif we linearly",
    "start": "3338770",
    "end": "3344170"
  },
  {
    "text": "transform a random vector,\nwhich means that we basically just multiply it by a matrix A.\nAnd we want this transformation",
    "start": "3344170",
    "end": "3353109"
  },
  {
    "text": "to be invertible,\nwhich in this case just means that the matrix\nitself has to be invertible.",
    "start": "3353110",
    "end": "3358960"
  },
  {
    "text": "So that you can go-- there is a unique\ncorrespondence between x and z.",
    "start": "3358960",
    "end": "3365020"
  },
  {
    "text": "And we're going to denote\nthe inverse of h with W.",
    "start": "3365020",
    "end": "3372380"
  },
  {
    "text": "And the question is\nhow is x distributed? ",
    "start": "3372380",
    "end": "3379200"
  },
  {
    "text": "Or what happens if you\nstart with basically uniform",
    "start": "3379200",
    "end": "3384300"
  },
  {
    "text": "and then you pass\nit through a matrix and multiply it by a matrix,\nyou get another random variable,",
    "start": "3384300",
    "end": "3390000"
  },
  {
    "text": "how is that distributed? And you kind see\nthat A is linear. It's going to stretch\nthings somehow.",
    "start": "3390000",
    "end": "3397980"
  },
  {
    "text": "And essentially, what\nhappens is that it's mapping the hypercube to a\nparallelotope, which is just",
    "start": "3397980",
    "end": "3408150"
  },
  {
    "text": "kind of like a parallelogram. And so in 2D, it would\nlook something like this. So you have a uniform.",
    "start": "3408150",
    "end": "3414580"
  },
  {
    "text": "So if n is 2, then\nyou have a z is distributed uniformly between\n0 and 1 in both directions.",
    "start": "3414580",
    "end": "3421720"
  },
  {
    "text": "So it's less uniform\nin this square. And then what happens if you\nmultiply it by this matrix, A,",
    "start": "3421720",
    "end": "3427380"
  },
  {
    "text": "which is just a, b,\nc, d, you're going to get a uniform distribution\nover that parallelogram.",
    "start": "3427380",
    "end": "3434549"
  },
  {
    "text": "You can kind of see that\nthe vertices correspond to what you would\nget if you were to multiply the matrix by 0,\n0, You're going to get 0, 0.",
    "start": "3434550",
    "end": "3443130"
  },
  {
    "text": "If you multiply this matrix\nby 1, 0, this corner, you're going to get\nthis corner, a, b.",
    "start": "3443130",
    "end": "3448170"
  },
  {
    "text": "And if you multiply by 0, 1,\nyou get this corner, c, d. And if you multiply by 1, 1,\nyou get this corner up here.",
    "start": "3448170",
    "end": "3454470"
  },
  {
    "text": "And then it's all\nthe other points. [INAUDIBLE] which was this kind\nof transformation from the unit",
    "start": "3454470",
    "end": "3463300"
  },
  {
    "text": "square to this. And such a process\nis linear, right?",
    "start": "3463300",
    "end": "3469290"
  },
  {
    "text": "But the one you showed\nus, when we apply the A, essentially, I think\nthat's a nonlinear process.",
    "start": "3469290",
    "end": "3477700"
  },
  {
    "text": "I was trying to imagine what\nis this transformation matrix. Is it like a linear operations?",
    "start": "3477700",
    "end": "3484310"
  },
  {
    "text": "So here, it's linear. Everything is linear so far. Then we'll go into\nnon-linear because we want to use neural networks.",
    "start": "3484310",
    "end": "3489670"
  },
  {
    "text": "But it turns out\nthat it's better to understand the\nlinear case first. But so far, it's all linear.",
    "start": "3489670",
    "end": "3495410"
  },
  {
    "text": "We're just multiplying by A, and\nthat's a linear transformation. OK. Even like the A matrix itself\nis also a linear systems.",
    "start": "3495410",
    "end": "3504250"
  },
  {
    "text": "It's fixed. So it's just a linear mapping. And so it's a\nlinear information.",
    "start": "3504250",
    "end": "3509450"
  },
  {
    "text": " OK. So now, we have some intuition\nfor what happens here.",
    "start": "3509450",
    "end": "3517839"
  },
  {
    "text": "z-- x, which is what we\ngot on multiplying A by z.",
    "start": "3517840",
    "end": "3524060"
  },
  {
    "text": "It should be a uniform random\nvariable over this red area, essentially.",
    "start": "3524060",
    "end": "3530020"
  },
  {
    "text": "And so what is the density? Well, we need to figure\nout what is the area",
    "start": "3530020",
    "end": "3535390"
  },
  {
    "text": "or the volume of that object. Because if it's\nuniform, then it's just going to be 1\nover the total area",
    "start": "3535390",
    "end": "3544156"
  },
  {
    "text": "of that parallelogram,\nof this red thing here. And I don't know if you\nmight have seen this,",
    "start": "3544156",
    "end": "3550840"
  },
  {
    "text": "but you can get the area\nof the parallelogram by basically computing the\ndeterminant of the matrix.",
    "start": "3550840",
    "end": "3558550"
  },
  {
    "text": "And here there is a geometric\nproof showing that indeed, if you can get the area\nof the parallelogram",
    "start": "3558550",
    "end": "3566650"
  },
  {
    "text": "by taking the area\nof this rectangle and subtracting off a bunch of\nparts, you get that expression.",
    "start": "3566650",
    "end": "3574760"
  },
  {
    "text": "So this is the\ndeterminant, ad minus cb. And that's the area\nof the parallelogram.",
    "start": "3574760",
    "end": "3582700"
  },
  {
    "text": "And so what this\nmeans is that x is going to be\nuniformly distributed",
    "start": "3582700",
    "end": "3589359"
  },
  {
    "text": "over this parallelotope\nof area, absolute value of the determinant of a, which\nmeans that the density of x",
    "start": "3589360",
    "end": "3598690"
  },
  {
    "text": "is going to be the density of\nthe original variable evaluated at the inverse.",
    "start": "3598690",
    "end": "3604330"
  },
  {
    "text": "And then again, we have\nto basically divide by the total area,\nwhich is the determinant",
    "start": "3604330",
    "end": "3609849"
  },
  {
    "text": "of the absolute value, of the\ndeterminant of this matrix. ",
    "start": "3609850",
    "end": "3617550"
  },
  {
    "text": "Or equivalently, because,\nif W is the inverse of A,",
    "start": "3617550",
    "end": "3625140"
  },
  {
    "text": "then the determinant\nof W is going to be 1 over the\ndeterminant of A. And so you can equivalently,\njust like before,",
    "start": "3625140",
    "end": "3632309"
  },
  {
    "text": "write it in terms of the\nderivative of the inverse of the mapping\ndefined by A, which is",
    "start": "3632310",
    "end": "3639660"
  },
  {
    "text": "just the determinant of W here. So you take x, you\ntake a point in here,",
    "start": "3639660",
    "end": "3647170"
  },
  {
    "text": "you map it back to the unit cube\nby multiplying it by W, which",
    "start": "3647170",
    "end": "3652690"
  },
  {
    "text": "gives you the corresponding x-- the corresponding z. You evaluate the density.",
    "start": "3652690",
    "end": "3658340"
  },
  {
    "text": "And then you have\nto take into account the fact that basically the\nvolume is stretched by applying",
    "start": "3658340",
    "end": "3663910"
  },
  {
    "text": "this linear transformation. And so things have\nto be normalized. And so you have to\ndivide by the total area",
    "start": "3663910",
    "end": "3670460"
  },
  {
    "text": "to get a uniform,\nrandom variable. And just like before, you\nhave to account basically",
    "start": "3670460",
    "end": "3677630"
  },
  {
    "text": "by how much the volume is\nshrinked or stretched when you apply a linear transformation.",
    "start": "3677630",
    "end": "3684090"
  },
  {
    "text": " Yeah. [INAUDIBLE] uniform\nhypercube with--",
    "start": "3684090",
    "end": "3693507"
  },
  {
    "text": "The question is, does it only\napply to a unit hypercube? No. It applies for-- this\nformula here is general.",
    "start": "3693507",
    "end": "3699800"
  },
  {
    "text": "Whatever is the\ndensity you begin with, as long as you apply an\ninvertible transformation,",
    "start": "3699800",
    "end": "3706010"
  },
  {
    "text": "you kind of get the\ndensity of the Wx-- or Wz, sorry, I\nthink, I have here.",
    "start": "3706010",
    "end": "3712340"
  },
  {
    "text": "Yeah. Or AZ. So if Z has an\narbitrary density, pz, you can get the density\nof x through this formula.",
    "start": "3712340",
    "end": "3721460"
  },
  {
    "text": "And in which case pz\nmight not be uniform. It could be a\nGaussian or something. This can still be used.",
    "start": "3721460",
    "end": "3727735"
  },
  {
    "text": " So again, we are getting\ntowards this idea",
    "start": "3727735",
    "end": "3734020"
  },
  {
    "text": "of starting from something\nsimple, the z, transforming it, and then being able\nto somehow evaluate",
    "start": "3734020",
    "end": "3741220"
  },
  {
    "text": "the density of the\ntransformed random variable. So recall this is\nkind of like a VAE.",
    "start": "3741220",
    "end": "3747700"
  },
  {
    "text": "We have a latent variable z. We have the observed variable x,\nbut now through these formulas,",
    "start": "3747700",
    "end": "3752860"
  },
  {
    "text": "we are able to evaluate the\nmarginal probability of a data point without having\nto compute integrals,",
    "start": "3752860",
    "end": "3758800"
  },
  {
    "text": "without having to do\nvariational inference. We get it through this\ncalculus formula--",
    "start": "3758800",
    "end": "3764619"
  },
  {
    "text": "by these formulas, basically. And the key idea\nis that given an x,",
    "start": "3764620",
    "end": "3770990"
  },
  {
    "text": "there is only one\ncorresponding z. So it's just a matter of finding\nit by multiplying it by x, by W",
    "start": "3770990",
    "end": "3777050"
  },
  {
    "text": "in this case, and then\ntaking into account the fact that the geometry\nchanges a little bit.",
    "start": "3777050",
    "end": "3782965"
  },
  {
    "start": "3782965",
    "end": "3790520"
  },
  {
    "text": "And notice, yeah, this is the\nsame kind of, not surprisingly, this is strictly more\ngeneral than what",
    "start": "3790520",
    "end": "3796460"
  },
  {
    "text": "we had before in the\n1D case, but it's kind of like the same thing.",
    "start": "3796460",
    "end": "3801670"
  },
  {
    "text": "Yeah. Once we do this\nlinear transformation, will it guarantee\nus a distribution?",
    "start": "3801670",
    "end": "3809480"
  },
  {
    "text": "Like because it starts\nfrom a distribution, which some area, some to\nbe one, and then we",
    "start": "3809480",
    "end": "3816220"
  },
  {
    "text": "do the linear transformation,\nit might not necessarily be a probability. Yeah. So the question is, is\nthis p of x, is this a PDF?",
    "start": "3816220",
    "end": "3825493"
  },
  {
    "text": "Like if you integrate\nover all possible values of x, do you get 1? And you have--\nbasically, the reason",
    "start": "3825493",
    "end": "3831755"
  },
  {
    "text": "you have to apply-- you have\nto divide by the determinant is to make sure that\nit is indeed uniform.",
    "start": "3831755",
    "end": "3837890"
  },
  {
    "text": "Because if you were not to\ndo that, then you would-- it's kind of like\nthe wrong calculation",
    "start": "3837890",
    "end": "3844007"
  },
  {
    "text": "that we did at the beginning,\nwhere you just map it back and you evaluate. But to make sure that\nthings are normalized,",
    "start": "3844007",
    "end": "3849670"
  },
  {
    "text": "you have to take\ninto account the fact that the area of\nthat parallelogram might grow a lot by applying\ncertain kinds of aids.",
    "start": "3849670",
    "end": "3857470"
  },
  {
    "text": "Or it might shrink a lot if\nyou apply very small kind of coefficients. So you have to\nrenormalize everything",
    "start": "3857470",
    "end": "3864620"
  },
  {
    "text": "through this determinant. That's why they are\ncalled normalizing flows, because this change\nof variable formula",
    "start": "3864620",
    "end": "3870589"
  },
  {
    "text": "takes care of the\nnormalization for you and guarantees that what you\nget is indeed a valid PDF.",
    "start": "3870590",
    "end": "3876220"
  },
  {
    "start": "3876220",
    "end": "3882090"
  },
  {
    "text": "Cool. Now we know how\nto do these things",
    "start": "3882090",
    "end": "3887420"
  },
  {
    "text": "for linear transformations. What we want to do is we want\nto use deep neural networks.",
    "start": "3887420",
    "end": "3893130"
  },
  {
    "text": "So we need to\nunderstand what happens if we apply non-linear\ntransformations,",
    "start": "3893130",
    "end": "3901700"
  },
  {
    "text": "invertible, nonlinear\ntransformations. So now, instead of just\nmultiplying by a matrix,",
    "start": "3901700",
    "end": "3909650"
  },
  {
    "text": "we want to feed x into\nsome kind of neural network and get an output z. And assuming that somehow\nwe are able to construct",
    "start": "3909650",
    "end": "3917930"
  },
  {
    "text": "a neural network\nthat is invertible, we still need to understand how\nthat changes the distribution of the variables.",
    "start": "3917930",
    "end": "3923650"
  },
  {
    "text": "So if you have a simple\nrandom variable z, you feed it through\na neural network. f.",
    "start": "3923650",
    "end": "3929030"
  },
  {
    "text": "The output is some\nother random variable x. And we need to\nsomehow understand",
    "start": "3929030",
    "end": "3934460"
  },
  {
    "text": "what is the PDF of that object. And it turns out that it's\nbasically the same thing,",
    "start": "3934460",
    "end": "3941910"
  },
  {
    "text": "that if you understand what\nhappens in the linear case, all you have to do is\nto basically linearize",
    "start": "3941910",
    "end": "3947250"
  },
  {
    "text": "the function by\ndoing essentially a Taylor approximation. So you compute the\nJacobian of the function,",
    "start": "3947250",
    "end": "3953160"
  },
  {
    "text": "and then it's the same formula. It's the determinant\nof the Jacobian, which is a linearized\napproximation to the function.",
    "start": "3953160",
    "end": "3961260"
  },
  {
    "text": "And so again, this\nis probably something you might have seen in\nsome calculus class, but it's essentially\nthe same formula.",
    "start": "3961260",
    "end": "3969960"
  },
  {
    "text": "So if you have, again,\na random variable x that is obtained by feeding\na simple random variable",
    "start": "3969960",
    "end": "3977520"
  },
  {
    "text": "through some kind of\ninvertible neural network, f, you can work out the\ndensity of the output",
    "start": "3977520",
    "end": "3983700"
  },
  {
    "text": "of the neural network, which\nis x, by basically computing, by inverting it and computing\nthe density of the input that",
    "start": "3983700",
    "end": "3991920"
  },
  {
    "text": "generated that\nparticular output. And as usual, you\nhave to account for how much the volume\nis stretched locally,",
    "start": "3991920",
    "end": "4002720"
  },
  {
    "text": "which is just the\ndeterminant of the Jacobian of the inverse mapping.",
    "start": "4002720",
    "end": "4010410"
  },
  {
    "text": "Just like before, we were\nalways looking at the derivative of the inverse mapping.",
    "start": "4010410",
    "end": "4015750"
  },
  {
    "text": "In the 1D case, the\nmultivariate extension is the determinant\nof the Jacobian.",
    "start": "4015750",
    "end": "4022955"
  },
  {
    "text": " And so just again,\nas a sanity check,",
    "start": "4022955",
    "end": "4030089"
  },
  {
    "text": "recall the simple\nformula that we proved was something\nlike this, which is exactly what you would get\nif f is just a scalar function.",
    "start": "4030090",
    "end": "4041839"
  },
  {
    "text": "So instead of having\ndeterminant of Jacobian, you just have or absolute\nvalue of the determinant",
    "start": "4041840",
    "end": "4048140"
  },
  {
    "text": "of the Jacobian, you just have\nabsolute value of the derivative of the inverse of the function. ",
    "start": "4048140",
    "end": "4057789"
  },
  {
    "text": "And let's see. And just like before, if you\nhave an invertible matrix,",
    "start": "4057790",
    "end": "4065470"
  },
  {
    "text": "the determinant of\nthe inverse, it's 1 over the determinant\nof the original matrix.",
    "start": "4065470",
    "end": "4074289"
  },
  {
    "text": "And so you can\nequivalently write things just like before in terms of the\nJacobian of the forward mapping.",
    "start": "4074290",
    "end": "4081610"
  },
  {
    "text": "So here, things are\nif you go from z to x,",
    "start": "4081610",
    "end": "4086620"
  },
  {
    "text": "then the formula basically\ninvolves the Jacobian of the mapping from x to z.",
    "start": "4086620",
    "end": "4095070"
  },
  {
    "text": "You can also write\nthings in terms of the Jacobian of the mapping.",
    "start": "4095070",
    "end": "4101189"
  },
  {
    "text": "And just like before, you\njust do 1 over instead.",
    "start": "4101189",
    "end": "4107040"
  },
  {
    "text": "You can also compute directly\nthe Jacobian of f, annd then",
    "start": "4107040",
    "end": "4112229"
  },
  {
    "text": "you compute the determinant of\nthat, and then you do 1 over. And that's the same\nthing, just like before.",
    "start": "4112229",
    "end": "4117440"
  },
  {
    "text": " Remember, before, we\nhad the formula where you could write\nthings in terms of h",
    "start": "4117440",
    "end": "4123750"
  },
  {
    "text": "or you could write\nthings in terms of f, and this is the same thing.",
    "start": "4123750",
    "end": "4129790"
  },
  {
    "text": "But this might be\ncomputationally, as we'll see, sometimes,\nit's easier depending",
    "start": "4129790",
    "end": "4135250"
  },
  {
    "text": "on whether you model the-- when you start using\nneural networks to model f,",
    "start": "4135250",
    "end": "4141310"
  },
  {
    "text": "it might be more convenient\nto use one or the other. And that's the reason\nthese formulas are handy.",
    "start": "4141310",
    "end": "4147734"
  },
  {
    "start": "4147735",
    "end": "4152790"
  },
  {
    "text": "All right. OK. Questions on this?  Yeah.",
    "start": "4152790",
    "end": "4158449"
  },
  {
    "text": "So the z's that we have\nbeen talking about, they are the latent variables\nin the previous variational",
    "start": "4158450",
    "end": "4163649"
  },
  {
    "text": "autoencoder? Yeah. So you should still think of-- I mean, this is\njust math so far. We haven't really built\na generative model.",
    "start": "4163649",
    "end": "4170350"
  },
  {
    "text": "But yeah, you should\nthink of the z as having some\nsimple distribution. And then you pass\nthem through a decoder",
    "start": "4170350",
    "end": "4177810"
  },
  {
    "text": "f, which is now an\ninvertible transformation, and then you get your\nsamples, x images out.",
    "start": "4177810",
    "end": "4185104"
  },
  {
    "text": " And now you can\nevaluate the density",
    "start": "4185104",
    "end": "4191330"
  },
  {
    "text": "over the images,\nwhich is what you need if you want to do maximum\nlikelihood through this formula.",
    "start": "4191330",
    "end": "4198410"
  },
  {
    "text": "So you don't have to do\nvariational inference. You don't have to compute ELBOs. You don't have to use\nan encoder to the extent",
    "start": "4198410",
    "end": "4204920"
  },
  {
    "text": "that you can invert the\nmapping by construction, then you're done.",
    "start": "4204920",
    "end": "4210410"
  },
  {
    "text": "You just need to invert\nand take into account this change in\nvolume, essentially,",
    "start": "4210410",
    "end": "4216830"
  },
  {
    "text": "given by the linearized\ntransformation. That is like [INAUDIBLE] that\nneed to be continuous and have",
    "start": "4216830",
    "end": "4225490"
  },
  {
    "text": "the same dimension. So how does this fit into\nthe image generations where x are discrete values?",
    "start": "4225490",
    "end": "4232440"
  },
  {
    "text": "So yeah, the\nquestion is, do they have to have the same dimension? Do they need to be continuous?",
    "start": "4232440",
    "end": "4237543"
  },
  {
    "text": "So yeah, they need to have\nthe same dimension, which is kind of like what we were\ndiscussing before if you want things to be invertible.",
    "start": "4237543",
    "end": "4243190"
  },
  {
    "text": "How does it apply to images? Well, you can think of\nimages as being continuous. I guess, the kind of\nmeasurements that you get",
    "start": "4243190",
    "end": "4250930"
  },
  {
    "text": "are often discrete because, you\nhave maybe some kind of fixed resolution.",
    "start": "4250930",
    "end": "4256180"
  },
  {
    "text": "But you can pretend that\nthings are continuous or you can add a little bit\nof noise to the training data",
    "start": "4256180",
    "end": "4264310"
  },
  {
    "text": "if you want. It's basically not a problem. You can train these\nmodels on images.",
    "start": "4264310",
    "end": "4271659"
  },
  {
    "text": "In the case when f\nis a neural network, we will apply a chain rule to\nmake the [INAUDIBLE] actually",
    "start": "4271660",
    "end": "4278730"
  },
  {
    "text": "computable space. Yeah. And what if we\ncheat a little bit",
    "start": "4278730",
    "end": "4284170"
  },
  {
    "text": "and make have [INAUDIBLE]\nactually invertible? For example, if the\nneural network capacity",
    "start": "4284170",
    "end": "4289480"
  },
  {
    "text": "first, and then like get 2x. And in the second one, we don't\nreally need the input of f.",
    "start": "4289480",
    "end": "4299150"
  },
  {
    "text": "So we have the f, but\nit's not invertible. We can still compute the\nprobability of px, of this--",
    "start": "4299150",
    "end": "4308290"
  },
  {
    "text": "So yeah. So if f is not\nreally invertible, then the formula\ndoesn't quite work.",
    "start": "4308290",
    "end": "4313690"
  },
  {
    "text": "And you're back in\nkiind of VAE land. This basically means that\nthere could be multiple z's that map to the same x.",
    "start": "4313690",
    "end": "4320878"
  },
  {
    "text": "And so if you want to compute\nthe probability of having generated this\nparticular x, you're",
    "start": "4320878",
    "end": "4326383"
  },
  {
    "text": "no longer guaranteed\nthat there is only one z. You just have to compute\nit and apply the formula. You would still have\nto work through all",
    "start": "4326383",
    "end": "4333110"
  },
  {
    "text": "the possible z's\nthat produced that x. And people have\nlooked at extensions",
    "start": "4333110",
    "end": "4338810"
  },
  {
    "text": "of these models,\nwhere maybe you're guaranteed that there is\nonly a up to k kind of it's almost invertible.",
    "start": "4338810",
    "end": "4344760"
  },
  {
    "text": "Like if you know all the\npossible-- all you have to know is basically all the\npossible x's that",
    "start": "4344760",
    "end": "4350402"
  },
  {
    "text": "could have produced-- all\nthe possible z's that could have produced any particular x. As long as you\nconstruct things such",
    "start": "4350402",
    "end": "4357260"
  },
  {
    "text": "that that's always\nthe case, then you can still apply\nsimilar tricks. But in general, if\nthere could be a lot,",
    "start": "4357260",
    "end": "4363860"
  },
  {
    "text": "if it's very-- it's\nhighly non-invertible, then you're back in\nVAE land, and then you",
    "start": "4363860",
    "end": "4368960"
  },
  {
    "text": "need some kind of encoder\nthat guesses that kind of like inverts the function for you. And you have to train them\njointly so that the encoder is",
    "start": "4368960",
    "end": "4375860"
  },
  {
    "text": "doing a good job, but\ninverting the decoder. And so then, you might\nas well use the ELBO.",
    "start": "4375860",
    "end": "4382320"
  },
  {
    "start": "4382320",
    "end": "4387719"
  },
  {
    "text": "Cool. And just let's see,\none example just worked out what that\nactually means just",
    "start": "4387720",
    "end": "4393659"
  },
  {
    "text": "to be a little\nbit more concrete. You might imagine that\nyou have the prior, that's",
    "start": "4393660",
    "end": "4399630"
  },
  {
    "text": "just two random\nvariables, z1 and z2 with some kind of joint density,\nmaybe it could be Gaussian.",
    "start": "4399630",
    "end": "4406775"
  },
  {
    "text": " And then we have this\ninvertible transformation.",
    "start": "4406775",
    "end": "4411810"
  },
  {
    "text": "And this is a multivariate\nfunction, two inputs to outputs.",
    "start": "4411810",
    "end": "4417960"
  },
  {
    "text": "So you can, for\nexample, denote it in terms of two scalar\nfunction, u1 and u2.",
    "start": "4417960",
    "end": "4423869"
  },
  {
    "text": "So each one of them takes two\ninputs and map it to one scalar. So it's multivariate,\ntwo inputs, two outputs.",
    "start": "4423870",
    "end": "4432840"
  },
  {
    "text": "And we're assuming that\nthese things are invertible. So there is an inverse mapping\nv, which always maps you back.",
    "start": "4432840",
    "end": "4440710"
  },
  {
    "text": "And again, it's two inputs,\ntwo outputs in this case.",
    "start": "4440710",
    "end": "4446060"
  },
  {
    "text": "And then we can\ndefine the outputs. So if you take this simple\nrandom variables z1 and z2,",
    "start": "4446060",
    "end": "4452660"
  },
  {
    "text": "and you feed them through\nthis neural network, which takes two inputs and\nproduces two outputs,",
    "start": "4452660",
    "end": "4457970"
  },
  {
    "text": "you're going to get two\nrandom variables, x1 for the first output\nof the network,",
    "start": "4457970",
    "end": "4463220"
  },
  {
    "text": "and x2 for the second\noutput of the network, just by applying u1 and u2.",
    "start": "4463220",
    "end": "4470180"
  },
  {
    "text": "And similarly, you can go\nback, given the outputs,",
    "start": "4470180",
    "end": "4475460"
  },
  {
    "text": "you can get the inputs by\nusing these v functions,",
    "start": "4475460",
    "end": "4480560"
  },
  {
    "text": "two inputs, two outputs again. And what you can\ntry to do is you can try to get the\ndensity over the outputs",
    "start": "4480560",
    "end": "4488480"
  },
  {
    "text": "of this neural network u. So how do you figure out what\nis the density at x1 and x2?",
    "start": "4488480",
    "end": "4497420"
  },
  {
    "text": "When these random variables\nare obtained by transforming through some neural network u.",
    "start": "4497420",
    "end": "4504170"
  },
  {
    "text": "And it's always the\nsame thing where what you do is you take the\noutputs, which are x1 and x2,",
    "start": "4504170",
    "end": "4511440"
  },
  {
    "text": "you invert the network. So you figure out which were\nthe two inputs that produced the outputs that we have.",
    "start": "4511440",
    "end": "4518330"
  },
  {
    "text": "And then you evaluate the\ndensity, the original density, the input density\nat those points.",
    "start": "4518330",
    "end": "4524510"
  },
  {
    "text": "This is kind of like the\nsame calculation that we did, the wrong calculation that\nwe did at the beginning.",
    "start": "4524510",
    "end": "4530600"
  },
  {
    "text": "Just invert and evaluate\nthe original density at the inverted points. And then as usual,\nyou have to fix things",
    "start": "4530600",
    "end": "4538190"
  },
  {
    "text": "by looking at how the volume is\nstretched, essentially, locally.",
    "start": "4538190",
    "end": "4545060"
  },
  {
    "text": "And what you would\ndo in this case is you would get\nthe absolute value of the determinant,\nof the Jacobian,",
    "start": "4545060",
    "end": "4551510"
  },
  {
    "text": "of the inverse mapping. The inverse mapping is V.\nThe Jacobian is this matrix",
    "start": "4551510",
    "end": "4558739"
  },
  {
    "text": "of partial derivatives. So we have two\noutputs, two inputs. So you have four kind of partial\nderivatives that you can get.",
    "start": "4558740",
    "end": "4569060"
  },
  {
    "text": "First output with respect to\nthe first input, first output with respect to\nthe second input,",
    "start": "4569060",
    "end": "4574620"
  },
  {
    "text": "second output with respect to\nthe first input, and so forth.",
    "start": "4574620",
    "end": "4579900"
  },
  {
    "text": "That's a matrix. You get the determinant\nof that matrix, you get the absolute value. And that gives you the\ndensity that you want.",
    "start": "4579900",
    "end": "4586695"
  },
  {
    "text": " To the extent you\ncan compute that, you",
    "start": "4586695",
    "end": "4592570"
  },
  {
    "text": "have a way of evaluating\ndensities for the outputs. Or equivalently, you\ncan do it in terms",
    "start": "4592570",
    "end": "4600160"
  },
  {
    "text": "of the Jacobian of u, which\nis the network that you use to transform\nthe simple variable.",
    "start": "4600160",
    "end": "4605870"
  },
  {
    "text": "And so again, you can\nevaluate it directly at the z's, the\ncorresponding inputs.",
    "start": "4605870",
    "end": "4612469"
  },
  {
    "text": "And then you apply this kind\nof the Jacobian of the mapping",
    "start": "4612470",
    "end": "4618640"
  },
  {
    "text": "in the other direction.  And we'll see that sometimes\none versus the other",
    "start": "4618640",
    "end": "4626250"
  },
  {
    "text": "could be more convenient,\ncomputationally. ",
    "start": "4626250",
    "end": "4633150"
  },
  {
    "text": "Yeah. This is more a broader question. But are there domains, which\nthese flow models are better",
    "start": "4633150",
    "end": "4639929"
  },
  {
    "text": "suited than VAEs Like\nthe autoregressive ones? What are the applications\nwhere this approach of modeling",
    "start": "4639930",
    "end": "4648300"
  },
  {
    "text": "the problem [INAUDIBLE]? Yeah. So flow models are\nactually-- the question is, when are flow\nmodels suitable?",
    "start": "4648300",
    "end": "4654070"
  },
  {
    "text": "And flow models are\npretty successful. Like in fact, you can even\nthink of a diffusion models as a certain kind of flow model.",
    "start": "4654070",
    "end": "4661540"
  },
  {
    "text": "And if you want to evaluate\nwhich diffusion models are kind of state of the art right\nnow for images, video speech,",
    "start": "4661540",
    "end": "4667860"
  },
  {
    "text": "there is an interpretation\nof diffusion models as flow models kind of\ninfinitely deep flow models.",
    "start": "4667860",
    "end": "4674070"
  },
  {
    "text": "And these formulas here\nare what you use to--",
    "start": "4674070",
    "end": "4679170"
  },
  {
    "text": "or an extension of the\nformula is basically what you use to evaluate\nlikelihoods in diffusion models.",
    "start": "4679170",
    "end": "4686540"
  },
  {
    "text": "I think that's like you\nsaid that you can also think of diffusion models\nas like stacked VAEs, right?",
    "start": "4686540",
    "end": "4691840"
  },
  {
    "text": "Yeah. So there is going to be two\ninterpretations of them. And flow models help you if you\nwant to evaluate likelihoods",
    "start": "4691840",
    "end": "4699840"
  },
  {
    "text": "because if you think\nof it as a stack VAE, you don't have likelihoods. If you want\nlikelihoods, then you",
    "start": "4699840",
    "end": "4705330"
  },
  {
    "text": "can think of them\nas flow models. And then you can get likelihoods\nthrough exactly this formula. So what are you saying is\nbasically the diffusion model",
    "start": "4705330",
    "end": "4711562"
  },
  {
    "text": "approach basically unifies this\nflow modeling approach and then VAE. Yeah. Yeah.",
    "start": "4711562",
    "end": "4716740"
  },
  {
    "text": " I think you might\nhave covered this, but if we're dealing with\na higher dimension problem,",
    "start": "4716740",
    "end": "4725639"
  },
  {
    "text": "then for the linear system that\nwe're trying to solve here, I guess, will become intractable.",
    "start": "4725640",
    "end": "4732039"
  },
  {
    "text": "But like theory\nguarantees that it's invertible functions,\nbut practicality wise,",
    "start": "4732040",
    "end": "4737730"
  },
  {
    "text": "it's hard to solve\na linear system. Yeah. So the question is,\nyeah, like how do you--",
    "start": "4737730",
    "end": "4742830"
  },
  {
    "text": "I guess, it seems like we\nneed to do a bunch of things. You need to be able to\ninvert the function. You need to be able to\ncompute the Jacobian",
    "start": "4742830",
    "end": "4751263"
  },
  {
    "text": "and you need to get the\ndeterminant of the Jacobian, which in general, the\ndeterminant of a matrix is kind of like an n\ncube kind of operation.",
    "start": "4751263",
    "end": "4758660"
  },
  {
    "text": "So what's going to come next is\nhow you parameterize functions",
    "start": "4758660",
    "end": "4764590"
  },
  {
    "text": "with neural networks that\nhave the properties we want. So they're easy to invert. And they give-- it's\neasy to compute Jacobians",
    "start": "4764590",
    "end": "4773020"
  },
  {
    "text": "and it's easy to compute\ndeterminant of Jacobians. Now for the infusions, we don't\neven do the whole [INAUDIBLE]..",
    "start": "4773020",
    "end": "4782960"
  },
  {
    "text": "So it's kind of like a\nmixture of [INAUDIBLE].. Like the pure\nversion of diffusion",
    "start": "4782960",
    "end": "4790280"
  },
  {
    "text": "model would be\ndefining pixel space. And the latent variables\nhave the same dimension",
    "start": "4790280",
    "end": "4795830"
  },
  {
    "text": "as the inputs. And that's why you can\nthink of it as a flow model. ",
    "start": "4795830",
    "end": "4802120"
  },
  {
    "text": "So can we reduce the\ndimensionality at all? They don't. Yeah. Yeah.",
    "start": "4802120",
    "end": "4807590"
  },
  {
    "text": "Is the interpretation of\nthe latent state space still being some sort of\nintrinsic hidden variables",
    "start": "4807590",
    "end": "4814330"
  },
  {
    "text": "still applicable when\nthey're the same size? Or is it-- it seems\nlike you haven't reduced the problem at all.",
    "start": "4814330",
    "end": "4821409"
  },
  {
    "text": "Yeah. So the question is, can\nyou still think of them as latent variables? I mean, it is a latent\nvariable to some extent,",
    "start": "4821410",
    "end": "4829120"
  },
  {
    "text": "but then it has the\nsame dimensionality. So it doesn't really\ncompress in any way.",
    "start": "4829120",
    "end": "4834310"
  },
  {
    "text": "It has a simple distribution. It is distributed in\na simple form, which",
    "start": "4834310",
    "end": "4839679"
  },
  {
    "text": "is desirable to some extent. But it's really more like\na change of variables. It's kind of like you're\nmeasuring things in pixels",
    "start": "4839680",
    "end": "4848230"
  },
  {
    "text": "or meters, and then\nyou change and you start measuring things in feet.",
    "start": "4848230",
    "end": "4853330"
  },
  {
    "text": "But it's not really\nchanging anything. You're just really changing the\nunits of measure in some sense.",
    "start": "4853330",
    "end": "4858590"
  },
  {
    "text": "And at least if you were\nto do just linear scaling, that would just be changing\nthe units of measure.",
    "start": "4858590",
    "end": "4863700"
  },
  {
    "text": "Here, you are doing\nnonlinear, so you're kind of changing the coordinate\nsystem in more complicated ways.",
    "start": "4863700",
    "end": "4870320"
  },
  {
    "text": "There is no loss of information. Everything is invertible. And so it's really just\nlooking at the data from a different\nangle that makes",
    "start": "4870320",
    "end": "4878570"
  },
  {
    "text": "things more simpler to model. Because if you start\nlooking at things",
    "start": "4878570",
    "end": "4885170"
  },
  {
    "text": "through the lens of f inverse,\nthen things become Gaussian. So somehow, by using\nthe right units",
    "start": "4885170",
    "end": "4893810"
  },
  {
    "text": "of measure or by looking\nat the-- by changing the axis in the right\nway and stretching them in the right way, things\nbecome much easier to model.",
    "start": "4893810",
    "end": "4902130"
  },
  {
    "text": "So that's a better way\nto probably think what flow models are really doing.",
    "start": "4902130",
    "end": "4907670"
  },
  {
    "text": "So for functions that some that\nare not generally continuous,",
    "start": "4907670",
    "end": "4912830"
  },
  {
    "text": "for example, like discretizing\nthe latent space and so on, what are some tricks that we can\nuse to approximate continuity",
    "start": "4912830",
    "end": "4920630"
  },
  {
    "text": "for such functions? Yeah. So the question, I guess, is\nwhether this can be applied",
    "start": "4920630",
    "end": "4926840"
  },
  {
    "text": "to discrete or whether-- yeah. So there are extensions of\nthis sort of ideas to discrete,",
    "start": "4926840",
    "end": "4934375"
  },
  {
    "text": "but then you lose a lot of\nthe mathematical-- a lot of the mathematical and\ncomputational advantages",
    "start": "4934375",
    "end": "4939410"
  },
  {
    "text": "really rely on continuity. So people have looked at-- I mean, the equivalent\nof an invertible mapping",
    "start": "4939410",
    "end": "4946010"
  },
  {
    "text": "in a discrete space\nwould be some kind of permutation--\nsome kind of yeah,",
    "start": "4946010",
    "end": "4951469"
  },
  {
    "text": "kind of like a\npermutation, essentially. And so people have tried to\ndiscover ways to permute things",
    "start": "4951470",
    "end": "4959660"
  },
  {
    "text": "in a way that makes\nthem easier to model, but you lose a lot of the\nmathematical structure. And so it's not\neasy to actually--",
    "start": "4959660",
    "end": "4965690"
  },
  {
    "text": "[INAUDIBLE] After you've trained\nthe model, then you can certainly discretize.",
    "start": "4965690",
    "end": "4971570"
  },
  {
    "text": "But for training,\nyou really want to think of things\nas being continuous.",
    "start": "4971570",
    "end": "4976989"
  },
  {
    "start": "4976990",
    "end": "4981000"
  }
]