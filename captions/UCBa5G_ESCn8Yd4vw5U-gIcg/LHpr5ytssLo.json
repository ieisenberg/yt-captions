[
  {
    "text": "this is week two of the systems lecture where we try to leverage the most out of the hardware we have to make uh models",
    "start": "5120",
    "end": "12240"
  },
  {
    "text": "train faster and last week we talked about parallelism within a single GPU and this",
    "start": "12240",
    "end": "19760"
  },
  {
    "text": "week we're talking about parallelism across multiple GPUs so this is a picture you should have in your head so",
    "start": "19760",
    "end": "26480"
  },
  {
    "text": "we have a bunch of nodes these are basically you know computers that have each have a number of GPUs usually eight",
    "start": "26480",
    "end": "34000"
  },
  {
    "text": "and within each GPU there's a bunch of uh streaming multipprocessors or SMS and",
    "start": "34000",
    "end": "40079"
  },
  {
    "text": "which actually do the work and you see that in green here are essentially the",
    "start": "40079",
    "end": "45840"
  },
  {
    "text": "memory and the communication so within each SM you have an very small L1 cache",
    "start": "45840",
    "end": "51440"
  },
  {
    "text": "um with on a GPU you have u high bandwidth memory HPM which is bigger and",
    "start": "51440",
    "end": "57280"
  },
  {
    "text": "then you have these u links that connect the different GPUs so the way to think about it is",
    "start": "57280",
    "end": "65198"
  },
  {
    "text": "that compute has to happen within the SM on these ALUS right and compute needs",
    "start": "65199",
    "end": "71760"
  },
  {
    "text": "inputs and uh needs to write outputs and generally the the inputs and outputs are",
    "start": "71760",
    "end": "77840"
  },
  {
    "text": "can be relatively far if you're lucky they're on the L1 cache if you're say less unlucky they're in HPM and now this",
    "start": "77840",
    "end": "85119"
  },
  {
    "text": "week we're talking about multi- uh GPU and multi- node training where the data that you might need might be across on",
    "start": "85119",
    "end": "91119"
  },
  {
    "text": "another GPU you right so the name of the game is how do you structure all your computation to avoid data transfer",
    "start": "91119",
    "end": "98920"
  },
  {
    "text": "bottlenecks because we want to remember keep the arithmetic intensity high we",
    "start": "98920",
    "end": "104560"
  },
  {
    "text": "want to saturate our GPUs make them go um humalong and generally data transfer",
    "start": "104560",
    "end": "110399"
  },
  {
    "text": "is going to be a lot slower so we have to that's going to be the bottleneck so last week we saw a bunch of different",
    "start": "110399",
    "end": "116960"
  },
  {
    "text": "techniques to try to do that within a GPU including fusion and tiling so the",
    "start": "116960",
    "end": "122159"
  },
  {
    "text": "idea basically is that instead of reading and writing from HPM you can load into L1 cache or or I guess uh you",
    "start": "122159",
    "end": "129759"
  },
  {
    "text": "know shared memory which is using the same type of um you know uh you know has",
    "start": "129759",
    "end": "134879"
  },
  {
    "text": "the same speed um and just work there on your local scratchpad and then write out",
    "start": "134879",
    "end": "140480"
  },
  {
    "text": "to HPM um only judiciously um and this week we started looking at communication",
    "start": "140480",
    "end": "146959"
  },
  {
    "text": "across GPUs and nodes where we have to replicate and shard our models and",
    "start": "146959",
    "end": "152080"
  },
  {
    "text": "parameters and optimize our states and there it's uh the the way we do that",
    "start": "152080",
    "end": "157680"
  },
  {
    "text": "will determine the the cost so here's a kind of a I'm taking a",
    "start": "157680",
    "end": "163120"
  },
  {
    "text": "little bit of liberty to put everything in kind of one hierarchy you can think from small fast to big slow so the",
    "start": "163120",
    "end": "170319"
  },
  {
    "text": "smallest and fastest is on a single node single GPU you have L1 cache that's",
    "start": "170319",
    "end": "175400"
  },
  {
    "text": "extremely fast but very small and then you have uh HPM on a single GPU and then",
    "start": "175400",
    "end": "182400"
  },
  {
    "text": "between um uh GPUs on the same same node we have",
    "start": "182400",
    "end": "188239"
  },
  {
    "text": "MVL link and then finally we have you know MV switch and of course this is all in the Nvidia",
    "start": "188239",
    "end": "194120"
  },
  {
    "text": "ecosystem um so so the idea is that many of the core concepts of minimizing data",
    "start": "194120",
    "end": "202400"
  },
  {
    "text": "transfer are really the same but now uh the mechanics are a bit different",
    "start": "202400",
    "end": "207519"
  },
  {
    "text": "because um L1 is behaves differently than these kind of envy um",
    "start": "207519",
    "end": "212680"
  },
  {
    "text": "switches um so this lecture is going to be mostly about concretizing the",
    "start": "212680",
    "end": "217920"
  },
  {
    "text": "concepts from the last lecture in in code there's going to be a few new things but um Tatsu did an excellent job",
    "start": "217920",
    "end": "224480"
  },
  {
    "text": "of giving you an overview of all the different types of parallelism i'm going to try to anchor it in the code so we",
    "start": "224480",
    "end": "230239"
  },
  {
    "text": "can more deeply understand what's going on um and then u we're going to have uh",
    "start": "230239",
    "end": "236959"
  },
  {
    "text": "I'm going to refer to this standard out file here which is the output of running this this lecture um there were some",
    "start": "236959",
    "end": "244159"
  },
  {
    "text": "minor issues I'll spare you of where if you have multi- um processing then this",
    "start": "244159",
    "end": "249519"
  },
  {
    "text": "uh this framework doesn't quite work um okay so this lecture has uh two parts",
    "start": "249519",
    "end": "254799"
  },
  {
    "text": "one in part one we're going to look at the building blocks um collective operations which we discussed last time",
    "start": "254799",
    "end": "261840"
  },
  {
    "text": "how this is implemented in nickel and pietorrch and then we're going to do some benchmarking and then in part two",
    "start": "261840",
    "end": "268479"
  },
  {
    "text": "we're going to look at actually distributed training data tensor and par pipeline",
    "start": "268479",
    "end": "274440"
  },
  {
    "text": "parallelism okay so let's start with collective operations so collective",
    "start": "274440",
    "end": "280000"
  },
  {
    "text": "operations are these primitives that are used generally for distributed programming and collective means that",
    "start": "280000",
    "end": "286080"
  },
  {
    "text": "you have um you know many nodes these are actually quite old from the at least the the 80s in the parallel programming",
    "start": "286080",
    "end": "293360"
  },
  {
    "text": "uh literature um and generally they provide a better abstraction than trying",
    "start": "293360",
    "end": "299440"
  },
  {
    "text": "to manage the pointto-point communication um yourself so these are really tried and true um um you know",
    "start": "299440",
    "end": "307360"
  },
  {
    "text": "primitives that have stood the test of time so a bit of uh terminology so world",
    "start": "307360",
    "end": "313120"
  },
  {
    "text": "size refers essentially the number of devices for example four and the rank um",
    "start": "313120",
    "end": "318880"
  },
  {
    "text": "sort of confusingly if you're used to kind of linear algebra is is actually just refers to device so we have rank",
    "start": "318880",
    "end": "324479"
  },
  {
    "text": "zero rank one rank two and rank three if you have four devices okay so um the",
    "start": "324479",
    "end": "330639"
  },
  {
    "text": "collective operations are as as follows um so um starting from you know",
    "start": "330639",
    "end": "336960"
  },
  {
    "text": "broadcast the idea is you have t0 on one of the ranks and you just want to put it",
    "start": "336960",
    "end": "343759"
  },
  {
    "text": "on all the other ranks or all ranks okay so that's very straightforward scatter",
    "start": "343759",
    "end": "349360"
  },
  {
    "text": "is similar but you have four values and you want to put each of the values on",
    "start": "349360",
    "end": "356080"
  },
  {
    "text": "different ranks so each of the ranks get different values not the same um value",
    "start": "356080",
    "end": "361680"
  },
  {
    "text": "um gather is the sort of the inverse of scatter where you have each rank having",
    "start": "361680",
    "end": "367120"
  },
  {
    "text": "a different value and then you bring them all together onto one rank um you",
    "start": "367120",
    "end": "372639"
  },
  {
    "text": "know reduce is the same as gather except for instead of concatenating you add them um all gather is the same as gather",
    "start": "372639",
    "end": "382319"
  },
  {
    "text": "except for you just do it for all the destinations um gather was just uh rank",
    "start": "382319",
    "end": "387919"
  },
  {
    "text": "zero or you know rank one or rank two or any individual rank all gather as you do",
    "start": "387919",
    "end": "392960"
  },
  {
    "text": "it for all of them and then finally reduce scatter I couldn't um you know find a a good",
    "start": "392960",
    "end": "399120"
  },
  {
    "text": "picture of this so I'm reusing the one from um from last time is um like reduce",
    "start": "399120",
    "end": "407759"
  },
  {
    "text": "um where you take a bunch of different values and you um you know add them or",
    "start": "407759",
    "end": "414000"
  },
  {
    "text": "perform other commutive operation on them and uh put it on one rank but like",
    "start": "414000",
    "end": "420240"
  },
  {
    "text": "scatter um you're going to be um putting different pieces of uh the vector or",
    "start": "420240",
    "end": "427520"
  },
  {
    "text": "tensor on different ranks okay and remember that all reduce",
    "start": "427520",
    "end": "434160"
  },
  {
    "text": "is equivalent to reduce uh plus all gather um so the way to remember this",
    "start": "434160",
    "end": "442240"
  },
  {
    "text": "terminology is as follows um because it can get kind of confusing like which",
    "start": "442240",
    "end": "447599"
  },
  {
    "text": "one's all gather which one's reduce scatter is that reduce just means you're performing some um you know associative",
    "start": "447599",
    "end": "454880"
  },
  {
    "text": "and commutive operation like sum or min and max or average um broadcast scatter",
    "start": "454880",
    "end": "460319"
  },
  {
    "text": "is the inverse of gather and all just means all destination is all you know",
    "start": "460319",
    "end": "467479"
  },
  {
    "text": "devices okay so hopefully this is a review from from last time",
    "start": "467479",
    "end": "474360"
  },
  {
    "text": "um so actually any questions before I move",
    "start": "474360",
    "end": "480280"
  },
  {
    "text": "on since we're going to build on these primitives so it's it's useful if everyone",
    "start": "480280",
    "end": "486800"
  },
  {
    "text": "understands okay so now let's see how this is actually implemented in um",
    "start": "492280",
    "end": "499199"
  },
  {
    "text": "starting with the hardware okay so um here's a classically what uh you know",
    "start": "499199",
    "end": "506000"
  },
  {
    "text": "hardware for GPUs looks like so um this is kind of in the home you have a",
    "start": "506000",
    "end": "511599"
  },
  {
    "text": "computer I guess um and you have your CPUs and generally you have your GPUs um",
    "start": "511599",
    "end": "519839"
  },
  {
    "text": "on one node that communicate via a PCIe bus um and if you have to go connect uh",
    "start": "519839",
    "end": "529440"
  },
  {
    "text": "communicate between different um nodes then this is all connected to Ethernet",
    "start": "529440",
    "end": "534880"
  },
  {
    "text": "so this is kind of typically how you know machines were built if you buy a GPU and you're uh for you know gaming or",
    "start": "534880",
    "end": "541120"
  },
  {
    "text": "something um this is kind of probably what your your setup looks like um as we'll see this is uh kind of suboptimal",
    "start": "541120",
    "end": "548800"
  },
  {
    "text": "because um there's a lot of overhead when the data gets needs to get shipped",
    "start": "548800",
    "end": "553839"
  },
  {
    "text": "from GPU to GPU it has to go through you know the kernel get copied into buffers and then go through this kind of a you",
    "start": "553839",
    "end": "560880"
  },
  {
    "text": "know transport over Ethernet and that introduces a lot of overhead so what has happened in modern",
    "start": "560880",
    "end": "568480"
  },
  {
    "text": "times uh with scientific computing and you know deep learning is that u if you",
    "start": "568480",
    "end": "573839"
  },
  {
    "text": "know that you're going to just string a bunch of GPUs together and do something uh together then we're just going to",
    "start": "573839",
    "end": "581519"
  },
  {
    "text": "hook the GPUs up you know directly basically um so in the Nvidia ecosystem",
    "start": "581519",
    "end": "587519"
  },
  {
    "text": "um we have MVLink that directly connects the the GPUs um therefore bypassing the",
    "start": "587519",
    "end": "592880"
  },
  {
    "text": "CPU you don't need to go through kind of the you know um the kernel of the of the of the host machine um and across even",
    "start": "592880",
    "end": "600880"
  },
  {
    "text": "across nodes uh we can connect the GPUs directly via MV switch so therefore",
    "start": "600880",
    "end": "606720"
  },
  {
    "text": "we're bypassing Ethernet and because Ethernet was developed a long time ago uh clearly not for for for uh these type",
    "start": "606720",
    "end": "614240"
  },
  {
    "text": "of applications so MV switch just and MVLink kind of skip all that and just",
    "start": "614240",
    "end": "619360"
  },
  {
    "text": "optimize directly for um the type of workloads that we're interested in",
    "start": "619360",
    "end": "624880"
  },
  {
    "text": "so if you look at H100's um each um node",
    "start": "624880",
    "end": "630000"
  },
  {
    "text": "has or sorry each GPU has 18 MV you know links um generation 4 coming out so",
    "start": "630000",
    "end": "637600"
  },
  {
    "text": "that's uh gives you a total bandwidth of 900 gigabytes um if you compare to um",
    "start": "637600",
    "end": "644720"
  },
  {
    "text": "these um it's certainly a lot faster than PCIe and it's certainly way faster than you know Ethernet um and um in",
    "start": "644720",
    "end": "653800"
  },
  {
    "text": "comparison um if you think about the cost of just going from the SM to you",
    "start": "653800",
    "end": "660640"
  },
  {
    "text": "know reading from high bandwidth memory um that's still quite a bit faster uh by",
    "start": "660640",
    "end": "665760"
  },
  {
    "text": "a factor of you know four or so um and of course these numbers are constantly",
    "start": "665760",
    "end": "671279"
  },
  {
    "text": "changing with a new black wells uh this number is like you know a like two or three times um more I believe um okay",
    "start": "671279",
    "end": "680800"
  },
  {
    "text": "yeah for the PC like does it go to the uh CPU and then like with another GPU or",
    "start": "680800",
    "end": "686560"
  },
  {
    "text": "it's directly used with the GPU so the question is for the PCIe where how does",
    "start": "686560",
    "end": "692640"
  },
  {
    "text": "the data get uh you know transferred i think it has to still go through the",
    "start": "692640",
    "end": "700640"
  },
  {
    "text": "CPU was there another",
    "start": "701480",
    "end": "705279"
  },
  {
    "text": "question and the PI was I mean it's developed for you know things like other",
    "start": "708200",
    "end": "713680"
  },
  {
    "text": "things are connected to it as well like your sound card or your SSD hard drive so it's not really it's sort of like a",
    "start": "713680",
    "end": "719839"
  },
  {
    "text": "general purpose uh you know uh bus for communication of devices yeah mv link",
    "start": "719839",
    "end": "726079"
  },
  {
    "text": "also has connection with CPU yeah so the question is MVL link also",
    "start": "726079",
    "end": "731839"
  },
  {
    "text": "connects to the uh CPU we're going to see a bit later how um I think maybe",
    "start": "731839",
    "end": "737760"
  },
  {
    "text": "just in the slide how things are connected yeah so you still need to talk to your CPU of course",
    "start": "737760",
    "end": "744760"
  },
  {
    "text": "yeah okay so there's this command uh that you can run um and this produces",
    "start": "744760",
    "end": "750079"
  },
  {
    "text": "some you know output which allows you to see um how the GPUs are actually",
    "start": "750079",
    "end": "756639"
  },
  {
    "text": "connected so I ran this on you know our our cluster there's um eight GPUs i",
    "start": "756639",
    "end": "763120"
  },
  {
    "text": "guess you won't be able to get eight GPUs but um but uh I guess if you could this is what it would look like and you",
    "start": "763120",
    "end": "769040"
  },
  {
    "text": "see that between every pair of GPUs there's MV18 um you know connecting um there's also",
    "start": "769040",
    "end": "776079"
  },
  {
    "text": "um these kind of network u you know cards and and other",
    "start": "776079",
    "end": "782480"
  },
  {
    "text": "things okay um oh yeah so then network cards are",
    "start": "783720",
    "end": "789120"
  },
  {
    "text": "basically what gives you the PCIe connection and and the the",
    "start": "789120",
    "end": "794680"
  },
  {
    "text": "CPUs so um okay so that's the hardware",
    "start": "794680",
    "end": "799760"
  },
  {
    "text": "so how do you use the hardware so Nvidia has spent a lot of you know time",
    "start": "799760",
    "end": "804839"
  },
  {
    "text": "developing really good software on top of uh their I guess really good hardware",
    "start": "804839",
    "end": "810480"
  },
  {
    "text": "um and there's a collective communication library by Nvidia called Nickel um and this essentially",
    "start": "810480",
    "end": "817680"
  },
  {
    "text": "translates the collective operations which we looked at before like all reduce um into low-level packets that",
    "start": "817680",
    "end": "823760"
  },
  {
    "text": "need to be sent between GPUs so this library actually does a lot of work because it allows the programmer just to",
    "start": "823760",
    "end": "830639"
  },
  {
    "text": "operate the level of I need this tensor to appear on all the machines and it just happens okay um so you know just a",
    "start": "830639",
    "end": "840639"
  },
  {
    "text": "little bit of what uh what happens is when you configure um setup nickel you",
    "start": "840639",
    "end": "847040"
  },
  {
    "text": "bring up a bunch of um you know devices and there's some communication that",
    "start": "847040",
    "end": "852560"
  },
  {
    "text": "happens to figure out the topology of the hardware it optimizes the path between the GPUs and then uh when you",
    "start": "852560",
    "end": "859120"
  },
  {
    "text": "actually call these collective communication operations and then launch CUDA kernels to send and receive",
    "start": "859120",
    "end": "865399"
  },
  {
    "text": "data okay so that's nickel it's provided as a as a library but nickel is still a",
    "start": "865399",
    "end": "871120"
  },
  {
    "text": "bit too um low level to us because most of what we're doing is um you know in",
    "start": "871120",
    "end": "877040"
  },
  {
    "text": "Python so there's a pietorch has this torch distributed library which",
    "start": "877040",
    "end": "882079"
  },
  {
    "text": "essentially provides a clean interface for these collective operations now from the comfort of your PyTorch pro pytorch",
    "start": "882079",
    "end": "889839"
  },
  {
    "text": "program you can just write all gather into tensor on a tensor and it will appear on all the uh on different",
    "start": "889839",
    "end": "897880"
  },
  {
    "text": "ranks um it also has this uh nice uh useful feature that it supports multiple",
    "start": "897880",
    "end": "904560"
  },
  {
    "text": "backends for different hardware so in particular nickel remember was for GPU but you can also run collective",
    "start": "904560",
    "end": "911360"
  },
  {
    "text": "operations remember this is not GPU specific it's just for any set of um you know devices so you can also do it for",
    "start": "911360",
    "end": "918160"
  },
  {
    "text": "CPU using this back end called uh glue um so if you're debugging stuff on your",
    "start": "918160",
    "end": "923680"
  },
  {
    "text": "laptop for your assignment for example you can use glue and still be able to run things without even a a GPU",
    "start": "923680",
    "end": "932079"
  },
  {
    "text": "um so anyway that that's another advantage of having these highle primitives is that they're much more portable than having to uh you know only",
    "start": "932079",
    "end": "941120"
  },
  {
    "text": "having something that's very GPU uh specific um of course the performance is",
    "start": "941120",
    "end": "947600"
  },
  {
    "text": "going to really depend on the hardware but at least logically you can make sure your code um runs um Pyro distributed also supports",
    "start": "947600",
    "end": "955920"
  },
  {
    "text": "other highle things like uh FSTP which Tatsu talked about last lecture but",
    "start": "955920",
    "end": "961120"
  },
  {
    "text": "we're not going to use this in this class because in the spirit of developing things from scratch um that's",
    "start": "961120",
    "end": "966800"
  },
  {
    "text": "just what we're going to do okay so uh let's look at some",
    "start": "966800",
    "end": "972000"
  },
  {
    "text": "examples of how to distributed collective operations work okay so there's this uh utility function I wrote",
    "start": "972000",
    "end": "978959"
  },
  {
    "text": "um which you can take a look at it in the code if you want which takes a function and just um runs this uh",
    "start": "978959",
    "end": "986800"
  },
  {
    "text": "basically it's a wrapper around um Python multipprocessing where it it just",
    "start": "986800",
    "end": "991920"
  },
  {
    "text": "runs four processes that execute this function so when you're in this function",
    "start": "991920",
    "end": "998000"
  },
  {
    "text": "you should think about it as there's actually worldsiz number of uh processes",
    "start": "998000",
    "end": "1004000"
  },
  {
    "text": "running this identical function where the rank indexes from zero one all the way to world size minus one okay so",
    "start": "1004000",
    "end": "1011759"
  },
  {
    "text": "right now I'm stepping through uh just one of the ranks um uh because lectures",
    "start": "1011759",
    "end": "1017199"
  },
  {
    "text": "are not parallel um and so generally what you do is you the first thing the",
    "start": "1017199",
    "end": "1024720"
  },
  {
    "text": "process uh needs to initialize itself and you essentially the they need to",
    "start": "1024720",
    "end": "1031600"
  },
  {
    "text": "kind of find each other right because you're multi-processor running a lot of processes they need to connect to a uh a",
    "start": "1031600",
    "end": "1039678"
  },
  {
    "text": "single host so that they can you know figure know that each other exist um so",
    "start": "1039679",
    "end": "1045918"
  },
  {
    "text": "note that this is not where all of the data goes the data goes through nickel but this is just for kind of",
    "start": "1045919",
    "end": "1051640"
  },
  {
    "text": "coordination um and since we have a a GPU we can use nickel otherwise you",
    "start": "1051640",
    "end": "1057840"
  },
  {
    "text": "would use uh glue um okay so after you set up um so now we're going to do some",
    "start": "1057840",
    "end": "1064559"
  },
  {
    "text": "stuff there's this useful um function called barrier which um basically waits",
    "start": "1064559",
    "end": "1070799"
  },
  {
    "text": "for all the processes in your process group to get to this uh point right",
    "start": "1070799",
    "end": "1075840"
  },
  {
    "text": "remember everything's running asynchronously um and in some cases you just want to have a synchronization point so barrier",
    "start": "1075840",
    "end": "1082480"
  },
  {
    "text": "does that um the reason I put it here is actually sort of for trivial reasons",
    "start": "1082480",
    "end": "1088080"
  },
  {
    "text": "because I want all these print statements to kind of be grouped together um but there's other reasons",
    "start": "1088080",
    "end": "1094000"
  },
  {
    "text": "why you might unused barrier as we'll get to later so I'm going to for each of",
    "start": "1094000",
    "end": "1099679"
  },
  {
    "text": "these um groups construct a tensor so the tensor is 01 23 U plus you know the",
    "start": "1099679",
    "end": "1106360"
  },
  {
    "text": "rank um so I'm going to print out um for each rank before the all reduce what",
    "start": "1106360",
    "end": "1113120"
  },
  {
    "text": "does it look like okay so here's what it looks like um can people read that in",
    "start": "1113120",
    "end": "1119679"
  },
  {
    "text": "the back yes okay good all right so on",
    "start": "1119679",
    "end": "1124720"
  },
  {
    "text": "rank zero it's 0 1 2 3 rank one 1 2 3 4 and so on and notice that because it's",
    "start": "1124720",
    "end": "1129840"
  },
  {
    "text": "async the orders uh it's just out of order in whatever order it happens to print okay so each",
    "start": "1129840",
    "end": "1138280"
  },
  {
    "text": "um um rank has a different tensor and then then you all reduce so all reduce",
    "start": "1138280",
    "end": "1145679"
  },
  {
    "text": "you pass in that tensor you say I want to sum it um in this case I'm not going to do async but you can do async um to",
    "start": "1145679",
    "end": "1153840"
  },
  {
    "text": "uh which is useful for overlapping communication and um in computation um",
    "start": "1153840",
    "end": "1159679"
  },
  {
    "text": "and then you know afterwards what happens after all reduce as advertised",
    "start": "1159679",
    "end": "1165600"
  },
  {
    "text": "basically for the first component you you add them up you get six this you get 10 14 and 18 okay so after all reduce",
    "start": "1165600",
    "end": "1175360"
  },
  {
    "text": "the um basically this tensor gets overwritten with um the the",
    "start": "1175360",
    "end": "1180480"
  },
  {
    "text": "corresponding um sum so it's very very kind of you know",
    "start": "1180480",
    "end": "1186480"
  },
  {
    "text": "nice and simple to use okay um so so let's do reduce",
    "start": "1186480",
    "end": "1193600"
  },
  {
    "text": "scatter so reduce scatter um I'm going to create an an input um",
    "start": "1193600",
    "end": "1202160"
  },
  {
    "text": "which is uh has dimension you know world size in which case this is four um and",
    "start": "1202160",
    "end": "1209440"
  },
  {
    "text": "um I'm going to allocate an output because reduce scatter is not going to operate in in place this is just going",
    "start": "1209440",
    "end": "1215360"
  },
  {
    "text": "to be a scalar um so before the reduce scatter um this is you know what it",
    "start": "1215360",
    "end": "1221600"
  },
  {
    "text": "looks like um I have my you know input as before output um you know happens to",
    "start": "1221600",
    "end": "1228159"
  },
  {
    "text": "be zeros but it could be any value um since I didn't initialize it and then",
    "start": "1228159",
    "end": "1233280"
  },
  {
    "text": "after the reduce scatter I pass in the input and the output and I'm going to",
    "start": "1233280",
    "end": "1238960"
  },
  {
    "text": "sum um then I get um essentially what",
    "start": "1238960",
    "end": "1245039"
  },
  {
    "text": "happens is that for the first component I sum and that goes on rank zero for the",
    "start": "1245039",
    "end": "1251760"
  },
  {
    "text": "se uh second component I sum and it goes on rank one and so on okay so as you",
    "start": "1251760",
    "end": "1257360"
  },
  {
    "text": "notice it is producing the same operation as all reduce except for the the output is sort of scattered across",
    "start": "1257360",
    "end": "1264480"
  },
  {
    "text": "all the different um",
    "start": "1264480",
    "end": "1268400"
  },
  {
    "text": "ranks okay so now let's do all gather so",
    "start": "1270200",
    "end": "1275919"
  },
  {
    "text": "um I'm going to just directly use the output of reduce scatter which is um",
    "start": "1275919",
    "end": "1282440"
  },
  {
    "text": "this um as the input and then I'm going to allocate an empty array for the the",
    "start": "1282440",
    "end": "1289280"
  },
  {
    "text": "output um and then so so before um the all",
    "start": "1289280",
    "end": "1294960"
  },
  {
    "text": "gather um the input is this and the output um I guess are just arbitrary",
    "start": "1294960",
    "end": "1301039"
  },
  {
    "text": "values um and uh after I do the all",
    "start": "1301039",
    "end": "1306480"
  },
  {
    "text": "gather um you know what happens is I get the all these uh",
    "start": "1306480",
    "end": "1314799"
  },
  {
    "text": "tensors to to show up in on all the",
    "start": "1314799",
    "end": "1319880"
  },
  {
    "text": "devices okay so this is just a kind of also an example uh hopefully now you're",
    "start": "1319880",
    "end": "1325760"
  },
  {
    "text": "very convinced that reduce scatter plus all gather is just all reduce because I computed exactly the same quantity as I",
    "start": "1325760",
    "end": "1332880"
  },
  {
    "text": "did for all",
    "start": "1332880",
    "end": "1335520"
  },
  {
    "text": "reduce okay questions this clear",
    "start": "1339000",
    "end": "1346480"
  },
  {
    "text": "in reduce scatter are we keeping track of which GPU so the question is uh in",
    "start": "1346480",
    "end": "1354400"
  },
  {
    "text": "reduced scatter do you keep track of which index goes to which GPU so by",
    "start": "1354400",
    "end": "1360080"
  },
  {
    "text": "convention um the the dimensionality has to be the",
    "start": "1360080",
    "end": "1365840"
  },
  {
    "text": "basically the world's I mean it could be a general tensor but one of the dimensions is the world size and it just",
    "start": "1365840",
    "end": "1372000"
  },
  {
    "text": "you know infers that um basically what you want to do is um the output is the",
    "start": "1372000",
    "end": "1377919"
  },
  {
    "text": "let's say the um sorry um the input has to be",
    "start": "1377919",
    "end": "1383280"
  },
  {
    "text": "basically world size and then it knows that uh basically the corresponding",
    "start": "1383280",
    "end": "1389039"
  },
  {
    "text": "you know computations go to each uh of the outputs yeah you have to be a bit",
    "start": "1389039",
    "end": "1395919"
  },
  {
    "text": "careful with the making sure the dimensionality uh align so you know going through this uh you know smaller",
    "start": "1395919",
    "end": "1402159"
  },
  {
    "text": "examples can be helpful is there another",
    "start": "1402159",
    "end": "1407360"
  },
  {
    "text": "question okay so finally we're now in this um process that's uh running and",
    "start": "1410600",
    "end": "1419039"
  },
  {
    "text": "when you're done you just you know clean up okay so um so so far we've talked",
    "start": "1419039",
    "end": "1426559"
  },
  {
    "text": "about these collective operations um bit about how they're implemented in you",
    "start": "1426559",
    "end": "1431760"
  },
  {
    "text": "know pietorch and it's uh nickel and then pietorch um let's do a bit of um",
    "start": "1431760",
    "end": "1437200"
  },
  {
    "text": "you know benchmarking um in the spirit of what we did in um",
    "start": "1437200",
    "end": "1442640"
  },
  {
    "text": "assignment uh or the first lecture or rather the second lecture um we're going",
    "start": "1442640",
    "end": "1449039"
  },
  {
    "text": "to focus on one node uh you know for now so let's do all reduce so I'm going to",
    "start": "1449039",
    "end": "1456320"
  },
  {
    "text": "have this uh tensor of 100 million elements um and a world size of four okay so",
    "start": "1456320",
    "end": "1464600"
  },
  {
    "text": "um I'm going to just allocate a tensor um and generally as I I think um as you",
    "start": "1464600",
    "end": "1472799"
  },
  {
    "text": "hopefully are uh can appreciate now that when you benchmark you have to really be",
    "start": "1472799",
    "end": "1478400"
  },
  {
    "text": "careful to kind of clean your pallet in some sense like you um in this case I'm",
    "start": "1478400",
    "end": "1483679"
  },
  {
    "text": "going to warm up basically com run the operation um once um and then",
    "start": "1483679",
    "end": "1490400"
  },
  {
    "text": "synchronize and do barrier some of this is I think probably a bit defensive but but just to be but just to be safe so",
    "start": "1490400",
    "end": "1497039"
  },
  {
    "text": "that all the kernels get you know loaded and whatever needs to be kind of computed gets computed and then I'm",
    "start": "1497039",
    "end": "1504320"
  },
  {
    "text": "going to start the clock all reduce and then synchronize again and and and stop",
    "start": "1504320",
    "end": "1510559"
  },
  {
    "text": "the clock okay so um now I can look at the how",
    "start": "1510559",
    "end": "1518080"
  },
  {
    "text": "long that took okay so if I scroll down",
    "start": "1518080",
    "end": "1523120"
  },
  {
    "text": "to here um I guess this is not that informal i should have printed in microsconds probably um it was I guess",
    "start": "1523120",
    "end": "1530159"
  },
  {
    "text": "very quick um some number of uh seconds um and now let's measure the the",
    "start": "1530159",
    "end": "1536799"
  },
  {
    "text": "bandwidth which is the number of gigabytes that were actually transferred in aggregate per second okay so the way",
    "start": "1536799",
    "end": "1544240"
  },
  {
    "text": "we do that is we have to think about what actually gets transferred here so",
    "start": "1544240",
    "end": "1550720"
  },
  {
    "text": "there's a tensor with that element size and the size of each element is um I",
    "start": "1550720",
    "end": "1556640"
  },
  {
    "text": "guess this I think this is float um 32 so that would be uh you know uh",
    "start": "1556640",
    "end": "1562919"
  },
  {
    "text": "two or sorry four four bytes um and and",
    "start": "1562919",
    "end": "1570200"
  },
  {
    "text": "um and so that's the size in bytes okay so now this is a little bit uh you know",
    "start": "1570200",
    "end": "1577600"
  },
  {
    "text": "subtle so um how many bytes are actually sent or",
    "start": "1577600",
    "end": "1583679"
  },
  {
    "text": "or transferred sent received um so each",
    "start": "1583679",
    "end": "1589200"
  },
  {
    "text": "tensor sitting on a rank has size bytes okay and it needs to send it to world",
    "start": "1589200",
    "end": "1596159"
  },
  {
    "text": "size minus one you know other you know machines or or not or ranks rather so",
    "start": "1596159",
    "end": "1603039"
  },
  {
    "text": "there but there's a factor of two so why is there a factor of",
    "start": "1603039",
    "end": "1608360"
  },
  {
    "text": "too because you're doing an all reduce remember so you need to send all the distinct um you know elements into",
    "start": "1608360",
    "end": "1617039"
  },
  {
    "text": "basically one place it needs to get uh summed up and then that needs to go back",
    "start": "1617039",
    "end": "1622400"
  },
  {
    "text": "to everyone okay so a rank needs to kind of",
    "start": "1622400",
    "end": "1629360"
  },
  {
    "text": "send the input out and then receive the output so that's why there's a there's a factor of two there and so the total",
    "start": "1629360",
    "end": "1636799"
  },
  {
    "text": "duration is um the world size times the",
    "start": "1636799",
    "end": "1642240"
  },
  {
    "text": "the actual duration that passed so I guess we're just kind of assuming that every we're we're you know if there's",
    "start": "1642240",
    "end": "1648720"
  },
  {
    "text": "four processors that's sort of like four times as much wall clock time that happened and the bandwidth is just the",
    "start": "1648720",
    "end": "1655120"
  },
  {
    "text": "bytes over the duration okay so what do we get here um",
    "start": "1655120",
    "end": "1662240"
  },
  {
    "text": "is about 277 gigabytes you know per second okay so you know I think um for",
    "start": "1662240",
    "end": "1671440"
  },
  {
    "text": "H100 um be above I think I claimed that there",
    "start": "1671440",
    "end": "1676480"
  },
  {
    "text": "was something like 900 gigabytes uh per second now of course as we know your",
    "start": "1676480",
    "end": "1682080"
  },
  {
    "text": "mileage varies depending on the size of the tensors and the exact number of uh devices and the weather and what no not",
    "start": "1682080",
    "end": "1689679"
  },
  {
    "text": "the weather but uh but you know various factors so your um your mileage might",
    "start": "1689679",
    "end": "1695120"
  },
  {
    "text": "vary so it's always good to benchmark to see what is actually the number of uh",
    "start": "1695120",
    "end": "1700480"
  },
  {
    "text": "gigabytes per second you're you're getting okay so so reduce scatter is",
    "start": "1700480",
    "end": "1708240"
  },
  {
    "text": "going to be very very similar so let's just go through this very quickly so we create a input um which is world size",
    "start": "1708240",
    "end": "1715919"
  },
  {
    "text": "times number of elements so each rank is going to have this um this the matrix um",
    "start": "1715919",
    "end": "1724520"
  },
  {
    "text": "and and so we're going to warm up and then um start the clock reduce uh",
    "start": "1724520",
    "end": "1731440"
  },
  {
    "text": "scatter um stop the clock and then see how long it took well okay that's not helpful um",
    "start": "1731440",
    "end": "1739440"
  },
  {
    "text": "and then let's look at the uh the bandwidth so the number of scent bytes",
    "start": "1739440",
    "end": "1745600"
  },
  {
    "text": "is no factor of two here because in reduce scatter remember all you're doing",
    "start": "1745600",
    "end": "1751360"
  },
  {
    "text": "is you're you're sending um you know your inputs into you know one place if",
    "start": "1751360",
    "end": "1757760"
  },
  {
    "text": "you just think about uh reduce right all the elements just go into one place and that's it and scatter just means that",
    "start": "1757760",
    "end": "1764799"
  },
  {
    "text": "different components of your tensor are going to different places but it's effectively it's it's like a you know",
    "start": "1764799",
    "end": "1771720"
  },
  {
    "text": "reduce okay so if you do the same calculation um you'll see that it's I guess I get 70",
    "start": "1771720",
    "end": "1779520"
  },
  {
    "text": "in this case um so I don't exactly know why it's exactly 70 as opposed to some",
    "start": "1779520",
    "end": "1786080"
  },
  {
    "text": "other number um I guess one could speculate that all reduce generally there's more traffic that you know",
    "start": "1786080",
    "end": "1793200"
  },
  {
    "text": "happens and um all reduces are you know potentially more optimized i think that",
    "start": "1793200",
    "end": "1799279"
  },
  {
    "text": "Nvidia hardware has this kind of sharp acceleration that actually does sort of um some of these um computations in you",
    "start": "1799279",
    "end": "1807520"
  },
  {
    "text": "know in the actual network um which shaves off a factor of two but I don't",
    "start": "1807520",
    "end": "1812559"
  },
  {
    "text": "know if that completely accounts for a difference here um there's a lot of stuff that happens in nickel that it's a",
    "start": "1812559",
    "end": "1819760"
  },
  {
    "text": "little bit hard to kind of reason about the performance exactly hence um benchmarking",
    "start": "1819760",
    "end": "1827640"
  },
  {
    "text": "another question about the set bytes and or the data bytes and how that was calculated specifically it looks like it",
    "start": "1828960",
    "end": "1836159"
  },
  {
    "text": "calculates just like the uh data that's being sent to the output but what about",
    "start": "1836159",
    "end": "1841440"
  },
  {
    "text": "like the input so the reduction step i was wondering how it gets the inputs to do the reduction so the the question is",
    "start": "1841440",
    "end": "1847600"
  },
  {
    "text": "it seems like this is just the uh the bytes for the output and what about the",
    "start": "1847600",
    "end": "1853520"
  },
  {
    "text": "input so to be clear I am assuming that the inputs just are already on the",
    "start": "1853520",
    "end": "1858799"
  },
  {
    "text": "device so I'm not counting that time and just uh I'm just counting what needs to",
    "start": "1858799",
    "end": "1864240"
  },
  {
    "text": "happen to do the reduce scatter is this just a scatter operation",
    "start": "1864240",
    "end": "1872080"
  },
  {
    "text": "this is a reduce scatter operation so so you need reduction step um so this",
    "start": "1872080",
    "end": "1880480"
  },
  {
    "text": "function does reduce scatter so it's one operation",
    "start": "1880480",
    "end": "1886679"
  },
  {
    "text": "okay i mean like we count it twice in the previous uh because we were doing",
    "start": "1886679",
    "end": "1894039"
  },
  {
    "text": "reduction for half two by half",
    "start": "1894039",
    "end": "1899440"
  },
  {
    "text": "so you're saying that for all reduce um there were there was a 2x because you",
    "start": "1899440",
    "end": "1905200"
  },
  {
    "text": "needed to reduce and then you needed to you know uh spread out again um for",
    "start": "1905200",
    "end": "1910640"
  },
  {
    "text": "reduce scatter I mean it's just a name it's called reduce scatter but it's really just um a",
    "start": "1910640",
    "end": "1919919"
  },
  {
    "text": "reduction okay and you can also see uh based on this that um if you do reduce",
    "start": "1923720",
    "end": "1930159"
  },
  {
    "text": "scatter and you do all gather each of those is doesn't have the factor of two so when you add them up you get a factor",
    "start": "1930159",
    "end": "1935519"
  },
  {
    "text": "of two which is another way to see that all reduces um twice okay",
    "start": "1935519",
    "end": "1942760"
  },
  {
    "text": "um and there's some references you can go read about how to benchmark and um",
    "start": "1942760",
    "end": "1947840"
  },
  {
    "text": "these collective operations okay so let's now talk about",
    "start": "1947840",
    "end": "1953039"
  },
  {
    "text": "the distributed um training uh piece so our general approach here is going to be",
    "start": "1953039",
    "end": "1959840"
  },
  {
    "text": "I'm going to walk through a barebones implementation of each strategy on deep um MLPS essentially so recall that you",
    "start": "1959840",
    "end": "1969039"
  },
  {
    "text": "generally are in the regime where MLPS are the compute bottleneck and transformers um not the attention so in",
    "start": "1969039",
    "end": "1975279"
  },
  {
    "text": "some ways even though this is a very simple architecture it's fairly representative of the type of um you",
    "start": "1975279",
    "end": "1982000"
  },
  {
    "text": "know workloads that you'll see um okay so let's start with data parallelism",
    "start": "1982000",
    "end": "1991039"
  },
  {
    "text": "um actually just one note is that data tensor and pipeline parism are you can",
    "start": "1991039",
    "end": "1996720"
  },
  {
    "text": "just think about them as different ways of cutting up uh your um your either",
    "start": "1996720",
    "end": "2001760"
  },
  {
    "text": "your model or your your data which hopefully I'll uh depict visually",
    "start": "2001760",
    "end": "2006840"
  },
  {
    "text": "here okay so in data parallelism um here's your model assume it has four",
    "start": "2006840",
    "end": "2013120"
  },
  {
    "text": "layers each layer of the MLP is just a matrix multiply where this is the dimen",
    "start": "2013120",
    "end": "2018640"
  },
  {
    "text": "hidden dimension um and so the data is also a matrix which is um there's the",
    "start": "2018640",
    "end": "2024880"
  },
  {
    "text": "batch dimension and then the hidden dimension and data parallel just cuts along the batch dimension into um you",
    "start": "2024880",
    "end": "2032880"
  },
  {
    "text": "know essentially smaller um you know pieces okay so now each rank is going to",
    "start": "2032880",
    "end": "2038640"
  },
  {
    "text": "get a different slice of the data so let's uh do an example here um so I'm",
    "start": "2038640",
    "end": "2046000"
  },
  {
    "text": "going to generate some sample data so let's say I have batch size of 128 hidden dimension of",
    "start": "2046000",
    "end": "2052280"
  },
  {
    "text": "1,024 um and then just generate some random data okay so I have batch size by number",
    "start": "2052280",
    "end": "2059280"
  },
  {
    "text": "of dimension and I'm going to run this data parallel algorithm um or DDP",
    "start": "2059280",
    "end": "2067358"
  },
  {
    "text": "so um so here um I'm going to so I got past",
    "start": "2067359",
    "end": "2074398"
  },
  {
    "text": "this this data there's a batch size and the dimension as as uh claimed from",
    "start": "2074399",
    "end": "2080118"
  },
  {
    "text": "before now I uh divide the batch size by the world size so I get the local batch",
    "start": "2080119",
    "end": "2085280"
  },
  {
    "text": "size that's how many um uh you know how big the batch size is on a a given rank",
    "start": "2085280",
    "end": "2092638"
  },
  {
    "text": "um and then I'm going to based on the rank just um figure out which um",
    "start": "2092639",
    "end": "2098960"
  },
  {
    "text": "starting and ending indices um of size local batch size I need to um access and",
    "start": "2098960",
    "end": "2104480"
  },
  {
    "text": "then get the corresponding data from that so basically I'm just like reaching in and grabbing some subset of the rows",
    "start": "2104480",
    "end": "2111359"
  },
  {
    "text": "based on the on the rank okay so now I'm setting up the the MLP here um and this",
    "start": "2111359",
    "end": "2118880"
  },
  {
    "text": "is done very um sort of bare bones you could say so here I am creating the MLP",
    "start": "2118880",
    "end": "2127760"
  },
  {
    "text": "parameters so each layer has essentially a matrix which is num demand mentioned",
    "start": "2127760",
    "end": "2133839"
  },
  {
    "text": "by num dimension and remember num dimension is 1024 um and I'm going to create the optimizer",
    "start": "2133839",
    "end": "2141599"
  },
  {
    "text": "so remember this um uh function is running asynchronously",
    "start": "2141599",
    "end": "2147920"
  },
  {
    "text": "on all the different on each rank so each of the four ranks is going to be running this with rank equals",
    "start": "2147920",
    "end": "2155320"
  },
  {
    "text": "0123 um and now I'm going to start training so for a number of steps I'm going to do a forward pass um through",
    "start": "2155320",
    "end": "2162400"
  },
  {
    "text": "the layers um matrix multiply nonlinearity matrix multiply nonlinearity there's four layers here",
    "start": "2162400",
    "end": "2168880"
  },
  {
    "text": "going to compute some loss i don't really care what the loss is it's just made up something made up and I'm going",
    "start": "2168880",
    "end": "2174240"
  },
  {
    "text": "to do the backward pass so so far this just looks like I'm implementing uh you",
    "start": "2174240",
    "end": "2179680"
  },
  {
    "text": "SGD right um and that's the kind of the point the only difference is now to",
    "start": "2179680",
    "end": "2185359"
  },
  {
    "text": "implement DDP is that you just like inject this line here which syncs",
    "start": "2185359",
    "end": "2191040"
  },
  {
    "text": "synchronizes the gradients across workers so what you do is for each of the layers you call it all reduce",
    "start": "2191040",
    "end": "2199359"
  },
  {
    "text": "um where you're averaging and the thing you're averaging is",
    "start": "2199359",
    "end": "2205440"
  },
  {
    "text": "pram.grad okay so it's just like you've kind of hijacked this um someone's sd",
    "start": "2205800",
    "end": "2211359"
  },
  {
    "text": "code and you're saying wait um I'm actually going to just mix all the gradients after um the backward pass and",
    "start": "2211359",
    "end": "2218160"
  },
  {
    "text": "then after you do that you just um update the parameters as uh",
    "start": "2218160",
    "end": "2224119"
  },
  {
    "text": "usual so from the SGD perspective it seems like nothing is happening i'm just running SGD but you know someone has",
    "start": "2224119",
    "end": "2230640"
  },
  {
    "text": "just uh um you know mixed my um",
    "start": "2230640",
    "end": "2235920"
  },
  {
    "text": "gradients okay so um so I guess just print out",
    "start": "2237160",
    "end": "2244000"
  },
  {
    "text": "some some things um so data parallel um I'm printing out",
    "start": "2244000",
    "end": "2250880"
  },
  {
    "text": "the loss so one thing to note is that the the losses are different between all",
    "start": "2250880",
    "end": "2256320"
  },
  {
    "text": "the different ranks because they have different datas um but after the all reduce all the parameters are you know",
    "start": "2256320",
    "end": "2263599"
  },
  {
    "text": "the same okay so this is a kind of your textbook uh application of all reduce in",
    "start": "2263599",
    "end": "2271839"
  },
  {
    "text": "ML setup um yeah when each rank runs this all how do they",
    "start": "2271839",
    "end": "2278800"
  },
  {
    "text": "ensure that they're all sort of at the same",
    "start": "2278800",
    "end": "2283640"
  },
  {
    "text": "so the question is how do you ensure if all of these uh processes are",
    "start": "2285680",
    "end": "2291440"
  },
  {
    "text": "just running asynchronously how do you make sure that each of them is actually",
    "start": "2291440",
    "end": "2296640"
  },
  {
    "text": "for example on the same step um this is because um all reduce is a is a",
    "start": "2296640",
    "end": "2302560"
  },
  {
    "text": "synchronization point it'll pop everyone and um and do the all reduce so you have",
    "start": "2302560",
    "end": "2310720"
  },
  {
    "text": "to be careful because if one of your um you know ranks has a missing all reduce",
    "start": "2310720",
    "end": "2317599"
  },
  {
    "text": "then it'll just you know hang",
    "start": "2317599",
    "end": "2321960"
  },
  {
    "text": "yeah yeah oh why does getting the initial",
    "start": "2323040",
    "end": "2328640"
  },
  {
    "text": "parameters depend on the rank the question is why does getting initial parameters depend on the rank the same",
    "start": "2328640",
    "end": "2335680"
  },
  {
    "text": "they should be they're the same the reason is just because I guess I don't um the code for this basically puts it",
    "start": "2335680",
    "end": "2342720"
  },
  {
    "text": "on the uh the appropriate GPU",
    "start": "2342720",
    "end": "2347320"
  },
  {
    "text": "okay any other questions",
    "start": "2349760",
    "end": "2353720"
  },
  {
    "text": "so DDP is something you implement in assignment 2 which maybe some of you have uh um you know look at or maybe not",
    "start": "2358480",
    "end": "2366160"
  },
  {
    "text": "um it will be done in the context of a transformer but this is sort of the sort of the most bare bones version so you",
    "start": "2366160",
    "end": "2372400"
  },
  {
    "text": "can see um very clearly what's",
    "start": "2372400",
    "end": "2376720"
  },
  {
    "text": "happening okay so that's DDP um",
    "start": "2377720",
    "end": "2383680"
  },
  {
    "text": "um losses are different across ranks um but the gradients are reduced to be all",
    "start": "2383680",
    "end": "2389440"
  },
  {
    "text": "the same so therefore the parameters um are of all the ranks are the same right",
    "start": "2389440",
    "end": "2396720"
  },
  {
    "text": "so actually you're doing worldsiz number of SGD runs but because they're",
    "start": "2396720",
    "end": "2404160"
  },
  {
    "text": "synchronized they're doing the same thing so you can think about this as sort of an instantiation of you know",
    "start": "2404160",
    "end": "2410480"
  },
  {
    "text": "analog of activation you know checkpointing where sometimes you just do extra compute because you don't want",
    "start": "2410480",
    "end": "2416240"
  },
  {
    "text": "to store things in this case you know we could have for example ship the optimizer state around but that would be",
    "start": "2416240",
    "end": "2421280"
  },
  {
    "text": "a bad idea because you know it's much faster just to run the to update the",
    "start": "2421280",
    "end": "2426320"
  },
  {
    "text": "optimizer um state than to actually move the the optimizer parameters",
    "start": "2426320",
    "end": "2433119"
  },
  {
    "text": "around okay so last year I did try to do FSDP that but that was a sort of a",
    "start": "2433480",
    "end": "2440079"
  },
  {
    "text": "hairball so I'm gonna skip skip that um and do a tensor parallel so here the the",
    "start": "2440079",
    "end": "2449040"
  },
  {
    "text": "picture is we leave the data the same and now what we're going to do is we're",
    "start": "2449040",
    "end": "2454240"
  },
  {
    "text": "going to cut um the model along the hidden",
    "start": "2454240",
    "end": "2460119"
  },
  {
    "text": "dimension okay so um so each rank is going to get every layer but it's going",
    "start": "2460119",
    "end": "2466800"
  },
  {
    "text": "to get only part of each layer and um what we're going to end up doing is transfer all the data and the",
    "start": "2466800",
    "end": "2474079"
  },
  {
    "text": "activations around um okay so so we're generating the same sample data um and",
    "start": "2474079",
    "end": "2482000"
  },
  {
    "text": "let's look at tensor parallel um okay",
    "start": "2482000",
    "end": "2487160"
  },
  {
    "text": "so so I have um the batch size and number of dimension as before and now",
    "start": "2487160",
    "end": "2494079"
  },
  {
    "text": "I'm going to not before I was cutting um batch size but now I'm cutting num dim",
    "start": "2494079",
    "end": "2499200"
  },
  {
    "text": "so I have local num dim equals 124 uh 1",
    "start": "2499200",
    "end": "2504480"
  },
  {
    "text": "024 4 um divided by world size and that's 256 so each model essentially",
    "start": "2504480",
    "end": "2513400"
  },
  {
    "text": "uh sorry each uh rank gets a part of the model which is one over the world size",
    "start": "2513400",
    "end": "2520000"
  },
  {
    "text": "fraction of the parameters okay and remember the whole why we're doing parallelism at all is",
    "start": "2520000",
    "end": "2525839"
  },
  {
    "text": "because the model won't be able to fit into a single GPU so we're going to shard it across multiple um GPUs um so",
    "start": "2525839",
    "end": "2535599"
  },
  {
    "text": "so the parameter matrices are now um num dim by local num",
    "start": "2535599",
    "end": "2542680"
  },
  {
    "text": "dim and now each rank is going to I'm only going to implement the for forward",
    "start": "2542680",
    "end": "2548079"
  },
  {
    "text": "pass uh here um not the whole training loop um so I'm going to start going",
    "start": "2548079",
    "end": "2554480"
  },
  {
    "text": "through all the layers so um I'm going to compute the",
    "start": "2554480",
    "end": "2560200"
  },
  {
    "text": "activations first so this looks pretty normal except for remember the activations are actually batch sized by",
    "start": "2560200",
    "end": "2567839"
  },
  {
    "text": "local num dim rather than num dim because I only each rank only has a fraction of the activations now",
    "start": "2567839",
    "end": "2576240"
  },
  {
    "text": "but now once I get the the activations um I need to you know communicate",
    "start": "2576240",
    "end": "2582880"
  },
  {
    "text": "um and here I what I have to do is um I'm going to",
    "start": "2582880",
    "end": "2588880"
  },
  {
    "text": "allocate memory for all the activations so at this point every one has a um as a",
    "start": "2588880",
    "end": "2596640"
  },
  {
    "text": "X but that X um represents a different um part of the activations okay so now",
    "start": "2596640",
    "end": "2604079"
  },
  {
    "text": "I'm going to just allocate um batch size I'm local num dim but world size number",
    "start": "2604079",
    "end": "2611280"
  },
  {
    "text": "so basically each rank is going to basically have enough uh I'm going to",
    "start": "2611280",
    "end": "2616800"
  },
  {
    "text": "just get the um basically have worldsized number of",
    "start": "2616800",
    "end": "2622640"
  },
  {
    "text": "batch size by local num um you know matrices and",
    "start": "2622640",
    "end": "2628280"
  },
  {
    "text": "then I'm going to do an all gather okay so um I'm going to send all the",
    "start": "2628280",
    "end": "2638040"
  },
  {
    "text": "activations um and this I mean it's you know fairly uh simple so X remember is",
    "start": "2638040",
    "end": "2644640"
  },
  {
    "text": "um batch size times local num dim but x is different for every rank so when I do",
    "start": "2644640",
    "end": "2650240"
  },
  {
    "text": "that all together I'm going to put it in activations which has essentially a",
    "start": "2650240",
    "end": "2656880"
  },
  {
    "text": "world-sized number of uh you know the same shape as X okay so now every um",
    "start": "2656880",
    "end": "2665200"
  },
  {
    "text": "rank has the same activations now has activations of all the models of the of",
    "start": "2665200",
    "end": "2671680"
  },
  {
    "text": "the whole model okay and then just like just to concatenate them together to get",
    "start": "2671680",
    "end": "2677599"
  },
  {
    "text": "you know X Okay so now X is um now again batch size",
    "start": "2677599",
    "end": "2684079"
  },
  {
    "text": "by um num dim okay and I you know",
    "start": "2684079",
    "end": "2691839"
  },
  {
    "text": "repeat so as you can see this is um you know there's a quite a bit of communication that happens which is why",
    "start": "2692520",
    "end": "2699760"
  },
  {
    "text": "you know remember Tatsu said that for tensor parallel you need pretty high um interconnects otherwise you'll be",
    "start": "2699760",
    "end": "2705760"
  },
  {
    "text": "passing a lot of these activations you know around okay and then you do it for the",
    "start": "2705760",
    "end": "2712640"
  },
  {
    "text": "next layer and the next layer and you get the idea",
    "start": "2712640",
    "end": "2718079"
  },
  {
    "text": "um and just to print out some um output uh so tensor parallel",
    "start": "2718079",
    "end": "2724440"
  },
  {
    "text": "um let's see here um forward pass produces activations of basically the",
    "start": "2724440",
    "end": "2731520"
  },
  {
    "text": "you know the full size and everyone has the same um activations at the",
    "start": "2731520",
    "end": "2738799"
  },
  {
    "text": "end okay so backward pass I'm going to skip because that's kind of a annoying",
    "start": "2742040",
    "end": "2748319"
  },
  {
    "text": "uh to do um all right any questions about that",
    "start": "2748319",
    "end": "2757720"
  },
  {
    "text": "i was just wondering why it's hard to do um so why is it hard to do the backward pass i I don't think it's necessarily",
    "start": "2761119",
    "end": "2768079"
  },
  {
    "text": "hard but in I guess in in the constrained you know time and space it's",
    "start": "2768079",
    "end": "2774079"
  },
  {
    "text": "it's um it's not hard it's just uh you know requires a bit more",
    "start": "2774079",
    "end": "2780400"
  },
  {
    "text": "work okay so now let's go to pipeline",
    "start": "2782920",
    "end": "2789160"
  },
  {
    "text": "parallelism so in this case we're cutting um the model by layers so all",
    "start": "2789160",
    "end": "2796640"
  },
  {
    "text": "the ranks get all the data um and um all the",
    "start": "2796640",
    "end": "2802520"
  },
  {
    "text": "ranks each rank gets all of one layer but they get different",
    "start": "2802520",
    "end": "2808119"
  },
  {
    "text": "layers okay so sample the data and run this program of this function for all",
    "start": "2808119",
    "end": "2814240"
  },
  {
    "text": "the the ranks um okay so here I'm going to",
    "start": "2814240",
    "end": "2822960"
  },
  {
    "text": "uh figure out how many layers go in each um you know rank",
    "start": "2822960",
    "end": "2830240"
  },
  {
    "text": "um which is two here so I have a four layer network i have two um you know two",
    "start": "2830240",
    "end": "2837960"
  },
  {
    "text": "ranks so each rank gets two of the layers um just like this picture actually",
    "start": "2837960",
    "end": "2845880"
  },
  {
    "text": "um and here I'm going to um just allocate the the parameters just for the",
    "start": "2845880",
    "end": "2852720"
  },
  {
    "text": "layers um that I need",
    "start": "2852720",
    "end": "2858240"
  },
  {
    "text": "okay so I'm going to do the forward pass um remember there's a further",
    "start": "2858359",
    "end": "2865440"
  },
  {
    "text": "optimization that you can you do which is um you know if you just you know do",
    "start": "2865440",
    "end": "2873839"
  },
  {
    "text": "it naively you get these pipeline bubbles that Tatsu talked about um before um one way to sort of mitigate",
    "start": "2873839",
    "end": "2880480"
  },
  {
    "text": "that is to break up the batch into microbatches so here I'm going to divide",
    "start": "2880480",
    "end": "2887319"
  },
  {
    "text": "um this this batch into um you know uh",
    "start": "2887319",
    "end": "2892720"
  },
  {
    "text": "batches of size 32 so four batches of size 32 um and then now the idea is that",
    "start": "2892720",
    "end": "2900680"
  },
  {
    "text": "every rank is going to essentially wait for the previous rank to pass it the",
    "start": "2900680",
    "end": "2907839"
  },
  {
    "text": "activations it's going to apply those layers and then it's going to forward it to the next rank so starting at the base",
    "start": "2907839",
    "end": "2914800"
  },
  {
    "text": "case we have rank equals zero that's uh just the data um so I'm just chunking",
    "start": "2914800",
    "end": "2921119"
  },
  {
    "text": "the the data into a bunch of microbatches um and going through each of micro",
    "start": "2921119",
    "end": "2928319"
  },
  {
    "text": "batches um I um first I receive the tensor so I'm",
    "start": "2928319",
    "end": "2934640"
  },
  {
    "text": "using these um pointto-point primitives now um instead of the collective",
    "start": "2934640",
    "end": "2939920"
  },
  {
    "text": "primitives um and I uh",
    "start": "2939920",
    "end": "2945559"
  },
  {
    "text": "essentially you know ba basically receive the tensor X um and then I'm going to compute the layers that are",
    "start": "2945559",
    "end": "2951359"
  },
  {
    "text": "assigned to this rank so in this case there's only two of them and then um I'm",
    "start": "2951359",
    "end": "2957440"
  },
  {
    "text": "going to send it to the the next rank um and then again send is a",
    "start": "2957440",
    "end": "2963839"
  },
  {
    "text": "pointto-point um you know operation and then the next",
    "start": "2963839",
    "end": "2969800"
  },
  {
    "text": "uh batch I'm going to do the same thing so okay so I'm going to skip",
    "start": "2969800",
    "end": "2975960"
  },
  {
    "text": "that okay so that's basically it so pipeline parallel at least the very naive version of it is relatively",
    "start": "2975960",
    "end": "2983839"
  },
  {
    "text": "conceptually simple as Satu mentioned last time um there's many things that are missing from this basic",
    "start": "2983839",
    "end": "2990680"
  },
  {
    "text": "implementation um overlapping the communication and computation is something we're not um you know doing at",
    "start": "2990680",
    "end": "2997680"
  },
  {
    "text": "all here um for example receive and send are synchronous but you should really make them async and also the the order",
    "start": "2997680",
    "end": "3005520"
  },
  {
    "text": "in which you do the forward um actually this is just the forward even the not the backward but once you have the",
    "start": "3005520",
    "end": "3011359"
  },
  {
    "text": "backward then um you have to figure out how to interle the forward and the backward uh steps",
    "start": "3011359",
    "end": "3019480"
  },
  {
    "text": "yeah wonder I guess like maybe what you just mentioned about like the async not being shown here it's I guess in",
    "start": "3022079",
    "end": "3028000"
  },
  {
    "text": "actuality like the GP will be sort of listening like whether another one passes something to it and it's kind of",
    "start": "3028000",
    "end": "3034960"
  },
  {
    "text": "this kind of kind of in like a event driven sort of like it only starts",
    "start": "3034960",
    "end": "3041520"
  },
  {
    "text": "processing once the like layer before it passes through it and then it starts",
    "start": "3041520",
    "end": "3046880"
  },
  {
    "text": "processing so the question is is this kind of like event driven programming where you're just waiting for things to",
    "start": "3046880",
    "end": "3054160"
  },
  {
    "text": "happen um in I think in event- driven programming you basically write these",
    "start": "3054160",
    "end": "3060400"
  },
  {
    "text": "handlers and then whenever stuff happens maybe you get a mouse click maybe you get you know a file ready event then a",
    "start": "3060400",
    "end": "3067920"
  },
  {
    "text": "piece of code runs that's quite different I think from this style of coding where um everything has to work",
    "start": "3067920",
    "end": "3074480"
  },
  {
    "text": "in lock step um it is true that you're",
    "start": "3074480",
    "end": "3079680"
  },
  {
    "text": "sort of waiting for the previous um um your rank to send you the information",
    "start": "3079680",
    "end": "3085920"
  },
  {
    "text": "but at least in this implementation there's no flexibility of where it's getting from it's not like it's waiting",
    "start": "3085920",
    "end": "3091920"
  },
  {
    "text": "for arbitrary data come from anywhere um I think there are ways to do",
    "start": "3091920",
    "end": "3098960"
  },
  {
    "text": "asynchronous training which was you know uh I think quite popular you know 10",
    "start": "3098960",
    "end": "3104960"
  },
  {
    "text": "more than 10 years ago where um there is more event driven where um you have a",
    "start": "3104960",
    "end": "3110720"
  },
  {
    "text": "you know server that sends data and whenever the gradients are ready it just like uploads and then the gradients get",
    "start": "3110720",
    "end": "3116800"
  },
  {
    "text": "accumulated and if workers die then you know that's um then you know that's sort",
    "start": "3116800",
    "end": "3122000"
  },
  {
    "text": "of handled more robustly but in modern training um despite",
    "start": "3122000",
    "end": "3127680"
  },
  {
    "text": "scaling up quite a bit um you know everything seems to be kind of in a",
    "start": "3127680",
    "end": "3132720"
  },
  {
    "text": "synchronous paradigm yeah so it is true",
    "start": "3132720",
    "end": "3139000"
  },
  {
    "text": "that when I say the the workers are and the ranks are operating asynchronous",
    "start": "3139000",
    "end": "3145200"
  },
  {
    "text": "that that's just because it's different processes but you're still putting quite rigid synchronization on how everything",
    "start": "3145200",
    "end": "3152559"
  },
  {
    "text": "is uh working in lock step",
    "start": "3152559",
    "end": "3157000"
  },
  {
    "text": "how would you change this program to handle to overap the configuration",
    "start": "3157680",
    "end": "3165640"
  },
  {
    "text": "um so the question is how would you change this to overlap communication and computation so um for example when you",
    "start": "3166000",
    "end": "3172640"
  },
  {
    "text": "send this there's no reason to just wait for the data to be sent you just",
    "start": "3172640",
    "end": "3178480"
  },
  {
    "text": "basically fire off the send um remember the the send actually gets hap happens on the GPU via some kernel launch so",
    "start": "3178480",
    "end": "3185839"
  },
  {
    "text": "that's sort of independent um and it can just go and process another microbatch",
    "start": "3185839",
    "end": "3191680"
  },
  {
    "text": "you know right away so um the way I think you would do this is there's another uh function called send which is",
    "start": "3191680",
    "end": "3199920"
  },
  {
    "text": "um uh asynchronous um actually this should be as synchronous um asynchronous",
    "start": "3199920",
    "end": "3207359"
  },
  {
    "text": "which returns a handle and so you basically do all the the send and then at the end you basically wait for all",
    "start": "3207359",
    "end": "3214079"
  },
  {
    "text": "the um the sends to complete and then for overlapping the um",
    "start": "3214079",
    "end": "3222160"
  },
  {
    "text": "when you actually have the backwards step then you basically have to you know schedule that um in here",
    "start": "3222160",
    "end": "3231559"
  },
  {
    "text": "send and receive the same if you have multiple sends multiple receives how does it know which one is which so the",
    "start": "3235359",
    "end": "3243359"
  },
  {
    "text": "question is if you have multiple sends and multiple receives how do you know",
    "start": "3243359",
    "end": "3248880"
  },
  {
    "text": "which is which so here you're spec the the tensor name doesn't matter it's just",
    "start": "3248880",
    "end": "3254640"
  },
  {
    "text": "uh whatever variable is there and what you're specifying is the the source so",
    "start": "3254640",
    "end": "3262240"
  },
  {
    "text": "if I'm at a node and I'm receiving then whatever the next message coming from",
    "start": "3262240",
    "end": "3268720"
  },
  {
    "text": "that rank I'm just going to you know put in this uh x and move continue executing",
    "start": "3268720",
    "end": "3277040"
  },
  {
    "text": "what if I want to do two sends from the same rank if you want to do two cents from the",
    "start": "3277040",
    "end": "3284960"
  },
  {
    "text": "same rank uh to the same destination",
    "start": "3284960",
    "end": "3291720"
  },
  {
    "text": "so so I'm not quite sure about this but I think if you have two sends it's sort of put in a stream so the order of the",
    "start": "3297880",
    "end": "3304960"
  },
  {
    "text": "sends still is preserved it's just that other stuff can happen at the same time",
    "start": "3304960",
    "end": "3310880"
  },
  {
    "text": "like you know you can send to um like I think if you have a a pair you do two",
    "start": "3310880",
    "end": "3318000"
  },
  {
    "text": "sends then that order is preserved but um the order in which um you know you",
    "start": "3318000",
    "end": "3323599"
  },
  {
    "text": "send some other rank is sending to another rank it can happen at any time",
    "start": "3323599",
    "end": "3330599"
  },
  {
    "text": "yeah what would happen if you just did like this send but then no one's receiving it would just get stopped",
    "start": "3332880",
    "end": "3339599"
  },
  {
    "text": "there or like so what happens if you send and no one's uh receives it I think it would just stop it just wait because",
    "start": "3339599",
    "end": "3347280"
  },
  {
    "text": "there's no yeah I mean because I mean the the process could",
    "start": "3347280",
    "end": "3354640"
  },
  {
    "text": "just be running and you don't know whether it will it's just I mean it's just code executing so you don't know if",
    "start": "3354640",
    "end": "3360880"
  },
  {
    "text": "it's never going to get there or if it's just gonna be a matter of time yeah",
    "start": "3360880",
    "end": "3368839"
  },
  {
    "text": "so the question is what happens to the last rank so at the end the last rank",
    "start": "3369839",
    "end": "3376000"
  },
  {
    "text": "has all the activation so that has basically the results of a full uh forward pass and then you know if you",
    "start": "3376000",
    "end": "3383040"
  },
  {
    "text": "implement the backward pass then you would be actually now computing the gradient with respect to loss and then",
    "start": "3383040",
    "end": "3389440"
  },
  {
    "text": "you would uh go back down and send um to from rank to rank minus one and so",
    "start": "3389440",
    "end": "3397280"
  },
  {
    "text": "on okay i guess maybe um I was afraid I was going to run out of time but it",
    "start": "3401640",
    "end": "3406880"
  },
  {
    "text": "looks like I had actually have time maybe next year I should do the backward pass",
    "start": "3406880",
    "end": "3412839"
  },
  {
    "text": "um okay so actually I'm going to finish quite early today but um so if you have",
    "start": "3412839",
    "end": "3418240"
  },
  {
    "text": "any other questions you should ask um so so far we've gone through three simple",
    "start": "3418240",
    "end": "3424720"
  },
  {
    "text": "examples of data tensor pipeline parallel um of course this is for simple",
    "start": "3424720",
    "end": "3431040"
  },
  {
    "text": "MLPS um you would actually want to do this with your own um you know fancier",
    "start": "3431040",
    "end": "3436480"
  },
  {
    "text": "model um like a transformer um I did argue that at least at the the core",
    "start": "3436480",
    "end": "3444319"
  },
  {
    "text": "ideas you can sort of understand through the MLP um I think",
    "start": "3444319",
    "end": "3450119"
  },
  {
    "text": "the but of course when you want to train you want to train transformer not a deep MLP um so you still have to implement",
    "start": "3450119",
    "end": "3457359"
  },
  {
    "text": "the the full complexity um what's also missing is the communication and uh",
    "start": "3457359",
    "end": "3464079"
  },
  {
    "text": "computation overlap which is not really handled very uh carefully here um and",
    "start": "3464079",
    "end": "3470160"
  },
  {
    "text": "there is generally a more complex code with bookkeeping i you know encourage you to check out like Megatron LM or um",
    "start": "3470160",
    "end": "3477359"
  },
  {
    "text": "PyTorch's FSTP it gets uh you know fairly um hairy and one of the things",
    "start": "3477359",
    "end": "3484400"
  },
  {
    "text": "that I think makes some of the bookkeeping at least for let's say FSTP and you'll be exposed to this in uh a A2",
    "start": "3484400",
    "end": "3491920"
  },
  {
    "text": "a bit is that um if you want something that handles arbitrary architectures",
    "start": "3491920",
    "end": "3497760"
  },
  {
    "text": "then you have to you know figure out the parameters and do a book a bunch of bookkeeping to and you know figure out",
    "start": "3497760",
    "end": "3503680"
  },
  {
    "text": "what their layers are and and so on whereas in the MLP case it's just I've sort of made the decision that I'm going",
    "start": "3503680",
    "end": "3510240"
  },
  {
    "text": "to split the model in this you know particularly simple way um one other",
    "start": "3510240",
    "end": "3516160"
  },
  {
    "text": "thing I'll just mention as an aside is that all of what we're doing in this course is is PyTorch but it is useful um",
    "start": "3516160",
    "end": "3524240"
  },
  {
    "text": "to be aware of this whole other ecosystem around jacks and TPUs um which",
    "start": "3524240",
    "end": "3530160"
  },
  {
    "text": "is actually kind of uh nice in some way um and the idea here is um Jax has",
    "start": "3530160",
    "end": "3538480"
  },
  {
    "text": "allows you to just define the model it define the sharding strategy and then",
    "start": "3538480",
    "end": "3543839"
  },
  {
    "text": "the Jax uh compiler handles the rest so there's this um toolkit that we uh",
    "start": "3543839",
    "end": "3549839"
  },
  {
    "text": "developed called Lavanter based on Jax um and I'll just show you a snippet of",
    "start": "3549839",
    "end": "3556000"
  },
  {
    "text": "um what it happened so this is FSTP and 10 lines of code and uh basically you",
    "start": "3556000",
    "end": "3561680"
  },
  {
    "text": "have a um you know uh model and then you just",
    "start": "3561680",
    "end": "3568480"
  },
  {
    "text": "say shard with this partic I mean I don't expect you to kind of read this",
    "start": "3568480",
    "end": "3573520"
  },
  {
    "text": "exactly but um basically you define which dimension you're going to shard by",
    "start": "3573520",
    "end": "3578799"
  },
  {
    "text": "um and then you know that's it and similarly for tensor parallel um you're",
    "start": "3578799",
    "end": "3585839"
  },
  {
    "text": "just saying I'm going to um shard um the model along the you know you can shard",
    "start": "3585839",
    "end": "3593200"
  },
  {
    "text": "by the on the head dimension um for uh for attention and also you can",
    "start": "3593200",
    "end": "3598480"
  },
  {
    "text": "shard based on um the the model dimension so in some sense you know this",
    "start": "3598480",
    "end": "3605520"
  },
  {
    "text": "gives you a sort of com you know comp a conceptual simplicity of what you're",
    "start": "3605520",
    "end": "3611599"
  },
  {
    "text": "trying to do is you have this basically um um computation graph",
    "start": "3611599",
    "end": "3617599"
  },
  {
    "text": "but it has these kind of dimensions you know the the model dimensions the embedding dimension the attention",
    "start": "3617599",
    "end": "3624480"
  },
  {
    "text": "sequence dimension and Jax allows you to basically uh just specify which",
    "start": "3624480",
    "end": "3631520"
  },
  {
    "text": "dimensions you want to cut by and also define a mapping from that onto the actual uh TPUs and then the JAX compiler",
    "start": "3631520",
    "end": "3639520"
  },
  {
    "text": "magically just you know figures out how to compile that down into the the primitives that you know shuffle things",
    "start": "3639520",
    "end": "3646359"
  },
  {
    "text": "around so this is much more higher level than um you know doing the the operating",
    "start": "3646359",
    "end": "3653119"
  },
  {
    "text": "with the collective communication um but you know we're sticking with PyTorch um",
    "start": "3653119",
    "end": "3659319"
  },
  {
    "text": "because it's it allows you to see kind of underneath the hood what's actually",
    "start": "3659319",
    "end": "3664880"
  },
  {
    "text": "um happening but if you're actually doing this in the in the real world um obviously you don't need a you and you",
    "start": "3664880",
    "end": "3670559"
  },
  {
    "text": "probably shouldn't implement all of this from scratch okay so that's the end of the",
    "start": "3670559",
    "end": "3676480"
  },
  {
    "text": "Jack's digression um so just just summarize we've um seen many ways to",
    "start": "3676480",
    "end": "3683920"
  },
  {
    "text": "parallelize so far um and each of these ways of parallelizing is you can think",
    "start": "3683920",
    "end": "3689119"
  },
  {
    "text": "about just like splitting either the model or the um data along some",
    "start": "3689119",
    "end": "3694160"
  },
  {
    "text": "dimension either the data the batch dimension the width dimension or depth dimension or the um the context length",
    "start": "3694160",
    "end": "3700319"
  },
  {
    "text": "dimension um we also see these this kind of recurring theme of you know",
    "start": "3700319",
    "end": "3707880"
  },
  {
    "text": "recomputation you can um you can kind of recomputee something",
    "start": "3707880",
    "end": "3714720"
  },
  {
    "text": "um from scratch or you can store in memory and suffer the the data transfer",
    "start": "3714720",
    "end": "3720599"
  },
  {
    "text": "cost or in now in the multi-GPU multi-node setting you can actually",
    "start": "3720599",
    "end": "3725680"
  },
  {
    "text": "store on another GPU's memory and then you know communicate which is you know even slower um so there's kind of these",
    "start": "3725680",
    "end": "3734799"
  },
  {
    "text": "these tradeoffs um you know here And you know often recomputation is",
    "start": "3734799",
    "end": "3741760"
  },
  {
    "text": "actually um you know can be you know better but obviously you can't you know",
    "start": "3741760",
    "end": "3747839"
  },
  {
    "text": "you can't repro compute the whole thing and often you're either communication or memory",
    "start": "3747839",
    "end": "3753160"
  },
  {
    "text": "limited um a final word is that um it is the case that hardware is getting better",
    "start": "3753160",
    "end": "3760640"
  },
  {
    "text": "so you might think that well maybe none of this is really necessary because in five years everything will fit in you",
    "start": "3760640",
    "end": "3767119"
  },
  {
    "text": "know L1 HPM so this is not going to be the case because um those might uh grow",
    "start": "3767119",
    "end": "3774880"
  },
  {
    "text": "quite a bit um although there are still physical limits um we'll always be",
    "start": "3774880",
    "end": "3780000"
  },
  {
    "text": "ending up with bigger models that sort of are at the limit of what the hardware can do so this hierarchical structure um",
    "start": "3780000",
    "end": "3789599"
  },
  {
    "text": "ever since system computer systems was a a thing has always been um with us and",
    "start": "3789599",
    "end": "3795039"
  },
  {
    "text": "it will always uh be there okay that's all I have for you uh",
    "start": "3795039",
    "end": "3800920"
  },
  {
    "text": "today so I'm can take any questions",
    "start": "3800920",
    "end": "3806920"
  },
  {
    "text": "yeah increase with the same set of parameters",
    "start": "3806920",
    "end": "3812400"
  },
  {
    "text": "the cost might be different because like your normalization might be a function",
    "start": "3812400",
    "end": "3817440"
  },
  {
    "text": "of the whole data set uh for example in",
    "start": "3817440",
    "end": "3823640"
  },
  {
    "text": "so the question is in data parallel um you're saying that even though the",
    "start": "3830880",
    "end": "3835920"
  },
  {
    "text": "parameters are all kind of synchronized there could be other things that depend on the the data like in batchorm um so I",
    "start": "3835920",
    "end": "3844559"
  },
  {
    "text": "don't actually know how you batch norm is always kind of annoying Um so I don't",
    "start": "3844559",
    "end": "3849839"
  },
  {
    "text": "know exactly how you would do that off the top of my head um and I guess at",
    "start": "3849839",
    "end": "3855680"
  },
  {
    "text": "least in the LM world that doesn't really show up uh because layer norm is",
    "start": "3855680",
    "end": "3861039"
  },
  {
    "text": "is is used um and as long as you initialize all the parameters and",
    "start": "3861039",
    "end": "3867520"
  },
  {
    "text": "they're using the same random seed you'll be fine i mean there could be like non-determinism issues um on on the",
    "start": "3867520",
    "end": "3874559"
  },
  {
    "text": "GPU but hopefully those are you know minor uh yeah",
    "start": "3874559",
    "end": "3882599"
  },
  {
    "text": "so the question is uh is does PyTorch have um some nicities as well kind of",
    "start": "3890559",
    "end": "3897280"
  },
  {
    "text": "like what Jax offers is Yeah so I mean PyTorch does have the",
    "start": "3897280",
    "end": "3903920"
  },
  {
    "text": "FSTP library which you should absolutely use if you're not taking this class um",
    "start": "3903920",
    "end": "3910799"
  },
  {
    "text": "which basically is a wrapper you define any model and it just does FFTDP on it",
    "start": "3910799",
    "end": "3916079"
  },
  {
    "text": "um I think that now if you're asking how well it can more custom allow you to",
    "start": "3916079",
    "end": "3924000"
  },
  {
    "text": "more do custom charting I I think there are some things that are coming but it's",
    "start": "3924000",
    "end": "3929599"
  },
  {
    "text": "not as I think as developed i mean I think there's sort of",
    "start": "3929599",
    "end": "3934880"
  },
  {
    "text": "this I think spectrum between the Jax world where you sort of declarity define",
    "start": "3934880",
    "end": "3940079"
  },
  {
    "text": "things and I think the Google infrastructure if you stay within the Jack's TPU system is pretty well",
    "start": "3940079",
    "end": "3947280"
  },
  {
    "text": "developed and but then if you look at kind of deepseek which is a kind of opposite end where you have um these uh",
    "start": "3947280",
    "end": "3956400"
  },
  {
    "text": "GPUs with actually really bad inter you know connect which means that they have",
    "start": "3956400",
    "end": "3961680"
  },
  {
    "text": "to go in and hack you know they actually go to the kind of nickel level and actually do a bunch of things which I",
    "start": "3961680",
    "end": "3968240"
  },
  {
    "text": "don't quite understand to ek out the performance whereas if you're writing a jack you just kind of from on high",
    "start": "3968240",
    "end": "3975039"
  },
  {
    "text": "declare your model and then you know stuff stuff happens so it it's kind of uh the ways that you leverage hardware I",
    "start": "3975039",
    "end": "3983920"
  },
  {
    "text": "think really depends on what what ecosystem you're operating in",
    "start": "3983920",
    "end": "3990200"
  },
  {
    "text": "yeah the the amount of recreation of the applications they can rec some of the",
    "start": "3990599",
    "end": "3998839"
  },
  {
    "text": "activations is there API which may yeah so the question is activation uh",
    "start": "3998839",
    "end": "4005760"
  },
  {
    "text": "checkpointing what's there is an API that's basically allows you to uh in I",
    "start": "4005760",
    "end": "4011599"
  },
  {
    "text": "mean I guess in PyTorch Njax to uh specify which parts you want to uh",
    "start": "4011599",
    "end": "4017039"
  },
  {
    "text": "recomputee because clearly you don't want to recomputee you know everything and or nothing um probably every few",
    "start": "4017039",
    "end": "4025359"
  },
  {
    "text": "layers probably right after like big Matt malls where um for example if you have let's say um mammal and then",
    "start": "4025359",
    "end": "4032319"
  },
  {
    "text": "pointwise um linearity I don't think you need to store like two copies of um",
    "start": "4032319",
    "end": "4039359"
  },
  {
    "text": "basically if you have two things where it's sort of trivial to uh comp um get",
    "start": "4039359",
    "end": "4045520"
  },
  {
    "text": "to then you might as well just store you know one version",
    "start": "4045520",
    "end": "4051000"
  },
  {
    "text": "um yeah over Okay gpus are replaced by",
    "start": "4051039",
    "end": "4056160"
  },
  {
    "text": "specific hardware or like more specialized so the question is are GPUs going to",
    "start": "4056160",
    "end": "4061440"
  },
  {
    "text": "ever be replaced by transformer specific hardware um so you're seeing this in the",
    "start": "4061440",
    "end": "4067440"
  },
  {
    "text": "inference space uh uh quite a bit already with um um like Grock and",
    "start": "4067440",
    "end": "4074160"
  },
  {
    "text": "Cerebras have specialized hardware that can do um inference and also I guess training severe resist training um so",
    "start": "4074160",
    "end": "4082480"
  },
  {
    "text": "basically those hardwares um essentially give you just a lot more kind of onchip",
    "start": "4082480",
    "end": "4090400"
  },
  {
    "text": "memory i mean that's basically the the name of the game i think cerebrus has",
    "start": "4090400",
    "end": "4095520"
  },
  {
    "text": "like a huge um you know essentially effectively a L1 cache so you don't have",
    "start": "4095520",
    "end": "4100880"
  },
  {
    "text": "to move things off and I think a lot of simplifications can happen because GPUs",
    "start": "4100880",
    "end": "4106480"
  },
  {
    "text": "were there's a lot of baggage actually if you think about because um they were designed in an era where you had to do a",
    "start": "4106480",
    "end": "4113440"
  },
  {
    "text": "lot of branching and like you know various types of ad hoc computations which are not really needed in the deep",
    "start": "4113440",
    "end": "4120560"
  },
  {
    "text": "learning regime so I think there are quite a few opportunities to um improve the hardware as well um I think there",
    "start": "4120560",
    "end": "4127920"
  },
  {
    "text": "was a hand back there and I'll um I don't know if I this is like the right question that I'm thinking about",
    "start": "4127920",
    "end": "4134480"
  },
  {
    "text": "but um in the context of the lecture it's basically a model um that's being",
    "start": "4134480",
    "end": "4140640"
  },
  {
    "text": "trained in one go that's been optimizing but I'm wondering if any of the techniques that we're talking about can",
    "start": "4140640",
    "end": "4147278"
  },
  {
    "text": "be used to incrementally train a model for example as you get new training data",
    "start": "4147279",
    "end": "4152880"
  },
  {
    "text": "um not just to like fine tune but actually to kind of recapulate everything without having to recalculate",
    "start": "4152880",
    "end": "4159120"
  },
  {
    "text": "them yeah so the question is um can these techniques be used to essentially",
    "start": "4159120",
    "end": "4164480"
  },
  {
    "text": "do continued um training um yeah absolutely so if you think about the the",
    "start": "4164480",
    "end": "4170238"
  },
  {
    "text": "unit of what we're working with is just doing gradient uh steps right so if you",
    "start": "4170239",
    "end": "4175359"
  },
  {
    "text": "take a halftra you know checkpoint you can just like continue doing what this",
    "start": "4175359",
    "end": "4180480"
  },
  {
    "text": "is there's nothing specific about starting from scratch um here",
    "start": "4180480",
    "end": "4187600"
  },
  {
    "text": "um I think there was a question there yeah so on the like the model specific",
    "start": "4187600",
    "end": "4192640"
  },
  {
    "text": "hardware end you know the previous question um like presumably there's like",
    "start": "4192640",
    "end": "4199120"
  },
  {
    "text": "a physical technical reason you can't make nodes much larger than they are",
    "start": "4199120",
    "end": "4205560"
  },
  {
    "text": "currently like what's the change that you're talking about",
    "start": "4205560",
    "end": "4213040"
  },
  {
    "text": "like so if you could just make GPU nodes like infinitely like as big as you",
    "start": "4213040",
    "end": "4219679"
  },
  {
    "text": "wanted people would do that so presumably there's a tech like a hardware reason that's not possible so",
    "start": "4219679",
    "end": "4225600"
  },
  {
    "text": "what's the actual advancement being done for like the rock specific hardware you",
    "start": "4225600",
    "end": "4230719"
  },
  {
    "text": "mentioned yeah so the question is there are physical limits for sure uh for a um",
    "start": "4230719",
    "end": "4237600"
  },
  {
    "text": "you know for a GPU let me just go so the um so you can't make GPUs obviously",
    "start": "4237600",
    "end": "4244400"
  },
  {
    "text": "infinitely large um or infinitely dense i mean there's also like you know power",
    "start": "4244400",
    "end": "4249920"
  },
  {
    "text": "um issues uh you know you need to get rid of all the the the heat and um you",
    "start": "4249920",
    "end": "4256560"
  },
  {
    "text": "know there's only so much kind of bandwidth that can um you know fit um so",
    "start": "4256560",
    "end": "4262000"
  },
  {
    "text": "I don't know the the exact um you know details but at least in some of the cerebrous case I mean they um they sort",
    "start": "4262000",
    "end": "4270560"
  },
  {
    "text": "of have this way of you know manufacturing you know basically the the chips so that the um the memory is kind",
    "start": "4270560",
    "end": "4279040"
  },
  {
    "text": "of on on on the chip so So I guess it's just a way of putting it on there and I",
    "start": "4279040",
    "end": "4285440"
  },
  {
    "text": "think that there are obviously um you know trade-offs because it comes at a um",
    "start": "4285440",
    "end": "4291520"
  },
  {
    "text": "cost of not having as much you know flexibility um but in but in general I",
    "start": "4291520",
    "end": "4297920"
  },
  {
    "text": "think the way to maybe think about this more broadly is that you know GPUs were",
    "start": "4297920",
    "end": "4303920"
  },
  {
    "text": "still developed in kind of the CPU era where it's much more control focused",
    "start": "4303920",
    "end": "4309040"
  },
  {
    "text": "like I have code that I'm executing that's the sort of first class citizen and then data needs to be moved to you",
    "start": "4309040",
    "end": "4315679"
  },
  {
    "text": "know um execute the to to handle the code but um the big difference with deep",
    "start": "4315679",
    "end": "4322159"
  },
  {
    "text": "learning workloads is that it's all sort of data flow like the the computation graph if you look at these sum is like",
    "start": "4322159",
    "end": "4328560"
  },
  {
    "text": "static you know from the beginning exactly all the computations that are going to be done until essentially the",
    "start": "4328560",
    "end": "4335040"
  },
  {
    "text": "end of training right so using that knowledge you should be able to kind of lay out your computation in a much",
    "start": "4335040",
    "end": "4342080"
  },
  {
    "text": "smarter way than having to deal with the the flexibility uncertainty over ad hoc",
    "start": "4342080",
    "end": "4348000"
  },
  {
    "text": "uh computation okay maybe a few more questions and I'll",
    "start": "4348000",
    "end": "4354880"
  },
  {
    "text": "end there yeah is the computational graph usually stored in the CPU or in the GPU",
    "start": "4354880",
    "end": "4360800"
  },
  {
    "text": "so the question is where is the computation graph uh stored well the the",
    "start": "4360800",
    "end": "4365920"
  },
  {
    "text": "code is um all I mean all this code is running on on on this you know CPU",
    "start": "4365920",
    "end": "4373159"
  },
  {
    "text": "um but but when you call something like um the PyTorch function it it that's",
    "start": "4373159",
    "end": "4382640"
  },
  {
    "text": "needs to run on GPU then it launches kernels under the hood and the kernels",
    "start": "4382640",
    "end": "4387920"
  },
  {
    "text": "are code that runs on the GPU um yeah I'm not sure if that so so I",
    "start": "4387920",
    "end": "4395840"
  },
  {
    "text": "guess maybe another answer is that the computation of graph is more of a I guess a conceptual um you know it's not",
    "start": "4395840",
    "end": "4404080"
  },
  {
    "text": "like there's a graph literally that's being you know uh I mean I guess there",
    "start": "4404080",
    "end": "4409199"
  },
  {
    "text": "sort of is but it's uh it's uh it's not like the graph gets put on the GPU if",
    "start": "4409199",
    "end": "4415920"
  },
  {
    "text": "that makes sense",
    "start": "4415920",
    "end": "4419560"
  },
  {
    "text": "okay so so these communication primitives that we have",
    "start": "4421120",
    "end": "4426639"
  },
  {
    "text": "like they are actually CPU instructions or like are these programs using the GPU",
    "start": "4427000",
    "end": "4437360"
  },
  {
    "text": "so the question is the communication primitives are they CPU or GPU so these collective operations are in some sense",
    "start": "4437360",
    "end": "4445440"
  },
  {
    "text": "um abstract specification of what types of operations need to happen um which",
    "start": "4445440",
    "end": "4452640"
  },
  {
    "text": "can happen if you remember um this PyTorch distributed um has different",
    "start": "4452640",
    "end": "4458800"
  },
  {
    "text": "backends so it could happen on GPU or happen on um CPU",
    "start": "4458800",
    "end": "4465600"
  },
  {
    "text": "but but when they're happening on CPU is it like is the CPU sort of scheduling them or or is it like kernels which are",
    "start": "4465600",
    "end": "4474239"
  },
  {
    "text": "independently yeah so well the CPU sort of drives basically is the sort of the",
    "start": "4474239",
    "end": "4480719"
  },
  {
    "text": "master still and then when you do a collective operation it calls the nickel",
    "start": "4480719",
    "end": "4485920"
  },
  {
    "text": "library which uh launches which is you know it's still CPU and then it launches some kernels that move data around yeah",
    "start": "4485920",
    "end": "4495760"
  },
  {
    "text": "okay maybe this is a good place to end all right i will see you uh next Monday",
    "start": "4495760",
    "end": "4500880"
  },
  {
    "text": "or Tuesday [Applause]",
    "start": "4500880",
    "end": "4506920"
  }
]