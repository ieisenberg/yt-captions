[
  {
    "start": "0",
    "end": "190000"
  },
  {
    "text": "if it happens\nthat I write too small font, please feel free to\nstop me and let me know.",
    "start": "5080",
    "end": "11380"
  },
  {
    "text": "It's just, as I said,\nafter a few lectures,",
    "start": "11380",
    "end": "16990"
  },
  {
    "text": "I start to forget about it. So please remind me. OK.",
    "start": "16990",
    "end": "23609"
  },
  {
    "text": "So I guess first let me briefly\nreview the last lecture just very quick.",
    "start": "23609",
    "end": "29539"
  },
  {
    "text": "So last lecture, we talk about\nthese two important concepts underfitting and overfitting.",
    "start": "29539",
    "end": "37110"
  },
  {
    "text": "So here, our goal is to make\nthe transition work, right? So we want to generalize\nto unseen examples.",
    "start": "37110",
    "end": "44780"
  },
  {
    "text": "And last time, we talked\nabout two possible reasons for why your test error\nis not good enough, right?",
    "start": "44780",
    "end": "52420"
  },
  {
    "text": "So one possible\nreason is overfitting. So overfitting means that your\ntraining error is actually",
    "start": "52420",
    "end": "57628"
  },
  {
    "text": "pretty good, your training\nloss is pretty small, but your test loss\nis pretty high.",
    "start": "57629",
    "end": "63500"
  },
  {
    "text": "And we have discussed the\npossible reasons for why",
    "start": "63500",
    "end": "68890"
  },
  {
    "text": "you can have overfitting. And two possible\nreasons are maybe you have too complex of model.",
    "start": "68890",
    "end": "75310"
  },
  {
    "text": "For example, last\ntime we discussed that if you use a 5th degree\npolynomial for this very, very",
    "start": "75310",
    "end": "80470"
  },
  {
    "text": "small data set where you\nonly have four examples, then you may overfit. Or maybe you don't\nhave enough data.",
    "start": "80470",
    "end": "86710"
  },
  {
    "text": "If you have more and more data-- if you have a million data,\nthen a 5th degree polynomial",
    "start": "86710",
    "end": "92440"
  },
  {
    "text": "wouldn't be a problem. And also, we discussed\nanother reason, underfitting.",
    "start": "92440",
    "end": "99450"
  },
  {
    "text": "So underfitting is much easier. Underfitting, in some\nsense, basically just means that you don't have\nsmall enough training",
    "start": "99450",
    "end": "106420"
  },
  {
    "text": "loss or training error. So your model is just not\npowerful enough so that you cannot even fit to the\ntraining data you have.",
    "start": "106420",
    "end": "116619"
  },
  {
    "text": "So in some sense,\nthese are kind of like two complementary\nsituations.",
    "start": "116619",
    "end": "121640"
  },
  {
    "text": "So in this case,\nyou probably want to make your model more\nexpressive, and in this case,",
    "start": "121640",
    "end": "126649"
  },
  {
    "text": "you may want to make your\nmodel less expressive or less complex. So we use these words\ncomplex expressive a lot",
    "start": "126649",
    "end": "134030"
  },
  {
    "text": "without a formal\ndefinition, right? So we say, some models\nare more complex, some models are less complex.",
    "start": "134030",
    "end": "140230"
  },
  {
    "text": "Typically, you can\nsomehow feel it. So a 5th degree\npolynomial probably is more complex\nthan linear model.",
    "start": "140230",
    "end": "145870"
  },
  {
    "text": "But actually, if you really want\nto have a concrete definition, it becomes a little bit tricky. So what is the right complexity\nmeasure of the model?",
    "start": "145870",
    "end": "154110"
  },
  {
    "text": "Someone ask about that as\nwell in the last lecture. And the answer is that there is\nno universal measure for what's",
    "start": "154110",
    "end": "160810"
  },
  {
    "text": "the right complexity measure. And there are a few kind\nof complexity measures",
    "start": "160810",
    "end": "169730"
  },
  {
    "text": "people often use. They all have their kind of\nlike particular strengths",
    "start": "169730",
    "end": "175430"
  },
  {
    "text": "and also, there is no\nreal kind of formal theory to say which one is better.",
    "start": "175430",
    "end": "181530"
  },
  {
    "text": "So these are kind of\ncomplexity measures that can be theoretically kind\nof justified in certain cases,",
    "start": "181530",
    "end": "188090"
  },
  {
    "text": "but they are not universal. So what are the\ncomplexity measures? So I'm just listing\na few just for kind",
    "start": "188090",
    "end": "196010"
  },
  {
    "start": "190000",
    "end": "234000"
  },
  {
    "text": "of knowledge in some sense. So I guess the\nmost obvious one is",
    "start": "196010",
    "end": "201860"
  },
  {
    "text": "how many parameters they are. So if you have more\nparameters, then your model might be more complex. And this is very intuitive.",
    "start": "201860",
    "end": "209090"
  },
  {
    "text": "However, the limitation\nhere is that maybe you have a lot of\nparameters, but actually,",
    "start": "209090",
    "end": "216020"
  },
  {
    "text": "the effective complexity\nof the model is very low, maybe all the parameters\nare very, very small. Then maybe you can\nsay, in this case,",
    "start": "216020",
    "end": "222000"
  },
  {
    "text": "maybe the complexity is actually\nnot as big as you thought. So to kind of deal with\nthis kind of scaling thing,",
    "start": "222000",
    "end": "228920"
  },
  {
    "text": "so what if all your\nparameters are basically zero even though you have\na million parameters.",
    "start": "228920",
    "end": "235250"
  },
  {
    "start": "234000",
    "end": "514000"
  },
  {
    "text": "Then people consider norms\nof ",
    "start": "235250",
    "end": "244579"
  },
  {
    "text": "the parameters, right? But this may not be-- no, this is actually typical\nin the norms of parameters. It's actually very good--",
    "start": "244579",
    "end": "250849"
  },
  {
    "text": "they are very good complexity\nmeasures for linear models. Basically before\ndeepening kind of arised,",
    "start": "250849",
    "end": "257979"
  },
  {
    "text": "before, I think\nwe are using norms as complexity measures a lot.",
    "start": "257980",
    "end": "264660"
  },
  {
    "text": "And still, we use\nthem in some cases. But this also has limitations. For example, sometimes you may,\nfor example, one certain case",
    "start": "264660",
    "end": "273060"
  },
  {
    "text": "would be that you have\na low norm solution and you add some random\nnoise to the model.",
    "start": "273060",
    "end": "280970"
  },
  {
    "text": "And when you add the noise,\nyou make the norm bigger. But actually, the noise doesn't\nreally change the complexity because you add\nsome noise, and when",
    "start": "280970",
    "end": "287120"
  },
  {
    "text": "you take the matrix\nmodification, you average out the\nnoise to some extent. So there are also these issues.",
    "start": "287120",
    "end": "294250"
  },
  {
    "text": "So some of the other kind of\nmore than complex measures people have considered\nare, for example, something",
    "start": "294250",
    "end": "299710"
  },
  {
    "text": "like Lipschitzness, whether\nyour model is Lipschitz or maybe",
    "start": "299710",
    "end": "305830"
  },
  {
    "text": "your model is smooth enough. And here, I'm using the\nword smooth relatively",
    "start": "305830",
    "end": "311050"
  },
  {
    "text": "kind of like informal way. This could mean the bound on\nthe second order derivative, it could mean the bound on\nthird derivative, something",
    "start": "311050",
    "end": "318150"
  },
  {
    "text": "like that. If your model is\nkind of like now it's oscillating or kind\nof fluctuating a lot,",
    "start": "318150",
    "end": "323169"
  },
  {
    "text": "maybe that means it's\nnot very complex. And there are other\ncomplexity measures for how invariant your model is\nwith respect to, for example,",
    "start": "323169",
    "end": "332160"
  },
  {
    "text": "certain translations, certain\ninvariances that you should have in a data set, for\nexample, whether your model is invariant to data augmentation.",
    "start": "332160",
    "end": "340228"
  },
  {
    "text": "But in general, there is\nno very established theory on what is exactly the\nright complexity measure.",
    "start": "340229",
    "end": "346380"
  },
  {
    "text": "And sometimes, it also\ndepends on the data as we will see today. So sometimes, for example,\nsuppose your data--",
    "start": "346380",
    "end": "358040"
  },
  {
    "text": "for example, let's\ntalk about norms. So different norms--\nwhat are the norms?",
    "start": "358040",
    "end": "363870"
  },
  {
    "text": "Are you talking about\nL1 norm, L2 norm? Sometimes, L2 norm is the\nright complexity measure for certain type of data\nand sometimes L1 norm",
    "start": "363870",
    "end": "371349"
  },
  {
    "text": "is the right complexity measure\nfor certain type of data. So basically, I don't think\nthis is kind of like there's anything super concrete.",
    "start": "371349",
    "end": "378770"
  },
  {
    "text": "It's now have just a fixed\nsuggestion for you to consider.",
    "start": "378770",
    "end": "386888"
  },
  {
    "text": "So so in some sense, you\nshould just keep this in mind and consider them when\nyou do your own data set.",
    "start": "386889",
    "end": "399870"
  },
  {
    "text": "So now we have to discuss\nthe complexity measures. And now in the rest\nof the lecture,",
    "start": "399870",
    "end": "407400"
  },
  {
    "text": "I think I'm going\nto cover two things. So one thing is that-- so once you have\nsome kind of guess",
    "start": "407400",
    "end": "413020"
  },
  {
    "text": "on what's the right complexity\nmeasure you are looking for, how do you make the\ncomplexity measure small. So how do you encourage the\nmodel to have small complexity.",
    "start": "413020",
    "end": "422180"
  },
  {
    "text": "So it's easier to do\nthis because you just change how many neurons or\nhow many hidden variables",
    "start": "422180",
    "end": "428150"
  },
  {
    "text": "on deep networks,\nwhether you can change number of parameters. But if you only change\nthe norm, what do you do?",
    "start": "428150",
    "end": "434220"
  },
  {
    "text": "So that's called regularization. I'm going to discuss that in\nthe first half of the lecture. And then in the second\nhalf of lecture,",
    "start": "434220",
    "end": "440740"
  },
  {
    "text": "I'm going to talk about\nsome more general advice,",
    "start": "440740",
    "end": "446460"
  },
  {
    "text": "for example, how do you\ntune your hyperparameters. When you do regularization\nor when you choose your model",
    "start": "446460",
    "end": "454580"
  },
  {
    "text": "complexity, right,\nyou're going to use a lot of\nhyperparameters, meaning you're going to choose how\nmany parameters you have,",
    "start": "454580",
    "end": "459940"
  },
  {
    "text": "you're going to choose how\nstrong your regularization is. So how do you tune\nyour hyperparameters and on what it is you should\ntune your hyperparameters.",
    "start": "459940",
    "end": "468240"
  },
  {
    "text": "And at the end, I'm\ngoing to probably spend and go about some ML advice.",
    "start": "468240",
    "end": "475580"
  },
  {
    "text": "So for example, how do you\ndesign ML system from scratch. There are a lot more\nthings in reality, more",
    "start": "475580",
    "end": "483129"
  },
  {
    "text": "than what you do in research. So that part, I\nwill use some slides to talk about some\ngeneral ideas on how",
    "start": "483130",
    "end": "489550"
  },
  {
    "text": "to design ML system in reality. So that's the general kind of\nintroduction of this course,",
    "start": "489550",
    "end": "497180"
  },
  {
    "text": "of this lecture. I'm going to start\nwith regularization. Any questions so far?",
    "start": "497180",
    "end": "504430"
  },
  {
    "text": "So regularization--",
    "start": "504430",
    "end": "516320"
  },
  {
    "start": "514000",
    "end": "583000"
  },
  {
    "text": "I think we have probably\nmentioned this thing sometimes in the previous kind of\nlectures just because ",
    "start": "516320",
    "end": "528370"
  },
  {
    "text": "we have mentioned this informally. So by regularization,\nmostly we just mean that you add\nsome additional term",
    "start": "528370",
    "end": "534690"
  },
  {
    "text": "in your training loss to\nencourage low complexity models. So for example, we use J of\ntheta as our training laws",
    "start": "534690",
    "end": "544380"
  },
  {
    "text": "and then you consider this\nso-called regularized loss where you add a term\nlambda times R theta.",
    "start": "544380",
    "end": "551889"
  },
  {
    "text": "So here, this R\ntheta is often called regularizer and lambda is--",
    "start": "551890",
    "end": "559120"
  },
  {
    "text": "I think there are\ndifferent names for this, but you could call it\nregularization stress or regularization coefficient,\nregularization parameter,",
    "start": "559120",
    "end": "564820"
  },
  {
    "text": "regularization stress,\nwhatever you call it. Maybe let's call it\nregularization stress.",
    "start": "564820",
    "end": "570980"
  },
  {
    "text": "So this lambda is a scalar and R\nof theta as a function of theta",
    "start": "570980",
    "end": "578949"
  },
  {
    "text": "which will change\nas theta changes. And the goal of\nthis R of theta is",
    "start": "578950",
    "end": "586649"
  },
  {
    "start": "583000",
    "end": "747000"
  },
  {
    "text": "to add additional encouragement\nto find model theta such that R of theta is small.",
    "start": "586649",
    "end": "593190"
  },
  {
    "text": "So for example,\ntypical R of theta could be something like\nL2 t regularization.",
    "start": "593190",
    "end": "600019"
  },
  {
    "text": "So you say R of theta is that you take-- this is probably the\nmost common choice.",
    "start": "600019",
    "end": "605690"
  },
  {
    "text": "You take the L2 norm square\nand you multiply by 1/2.",
    "start": "605690",
    "end": "613160"
  },
  {
    "text": "the 1/2 doesn't really matter. This is just some kind of\nconvention because either way,",
    "start": "613160",
    "end": "618180"
  },
  {
    "text": "you're going to multiply\nlambda in front of it. So whether you have we just change our\nchoice of lambda.",
    "start": "618180",
    "end": "624850"
  },
  {
    "text": "But this is just a convention. And so this is called\nL2 regularization.",
    "start": "624850",
    "end": "631750"
  },
  {
    "text": "Also in deep learning,\npeople call it weight decay. There's reason why people\ncall it weight decay. I guess, probably, I wouldn't\nhave time to discuss it today.",
    "start": "631750",
    "end": "639519"
  },
  {
    "text": "So in the lecture notes, there\nis a very short paragraph you can see. Actually, if you use this\nregularization in the update",
    "start": "639520",
    "end": "645829"
  },
  {
    "text": "rule, it will look\nlike a weight decay. So there's one step\nin the update rule where you decay your\nparameter-- you'll shrink",
    "start": "645830",
    "end": "651480"
  },
  {
    "text": "your parameter by scalar. But anyway, so\nit's just the name. Either it's called\nweight decay or you",
    "start": "651480",
    "end": "657769"
  },
  {
    "text": "call it L2 regularization. So that's one of the\npretty common one.",
    "start": "657770",
    "end": "663730"
  },
  {
    "text": "And you can see that if you\nadd this thing to your loss function, you're minimizing\nyour loss function. So then you are trying to\nmake both the loss small",
    "start": "663730",
    "end": "672680"
  },
  {
    "text": "and also make the L2 norm\nof our parameter small. And the lambda in\nsome sense is kind",
    "start": "672680",
    "end": "679200"
  },
  {
    "text": "of controlling the trade-off\nbetween these two terms. If you take lambda",
    "start": "679200",
    "end": "684529"
  },
  {
    "text": "focus on the regularization,\nyou just only focus on low norm\nsolution, but maybe you don't fit your data very well.",
    "start": "684529",
    "end": "691040"
  },
  {
    "text": "If you take lambda to be,\nfor example, 0, literally 0, then you are not using\nyour regularization, you are just only\nfitting your data.",
    "start": "691040",
    "end": "698580"
  },
  {
    "text": "And actually, when you make\nthe lambda very, very small, this can still do something. So even say the lambda\nis 0.00001, very small,",
    "start": "698580",
    "end": "707990"
  },
  {
    "text": "still this might do something\nbecause maybe there are multiple theta such that\nJ of theta is really,",
    "start": "707990",
    "end": "713829"
  },
  {
    "text": "really close to 0 or\nmaybe even literally 0. So if you don't\nhave this, then you",
    "start": "713830",
    "end": "719320"
  },
  {
    "text": "are not doing any type working. If you literally make\nlambda 0, then you",
    "start": "719320",
    "end": "725230"
  },
  {
    "text": "are just picking one of the\nsolution where J theta is 0. But you don't know which\none you pick, but as long as you add a little bit\nregularization that you",
    "start": "725230",
    "end": "732450"
  },
  {
    "text": "are using this as a\ntiebreaker in some sense. So you are finding\nsome solutions such",
    "start": "732450",
    "end": "737700"
  },
  {
    "text": "that J theta is\nvery, very small, but you use the\nnorm as a tiebreaker",
    "start": "737700",
    "end": "742959"
  },
  {
    "text": "among all of these solutions\nthat have very small training loss.",
    "start": "742959",
    "end": "748800"
  },
  {
    "text": "So this is probably the most\ntypical regularization people use. And another one\nis the following--",
    "start": "748800",
    "end": "754470"
  },
  {
    "text": "so you can take R theta\nto be the so-called 0",
    "start": "754470",
    "end": "760939"
  },
  {
    "text": "norm of the parameter. But actually, this\nis not really a norm, this is just a notation. So this is really just defined\nto be the number of non",
    "start": "760940",
    "end": "768709"
  },
  {
    "text": "zeros in the model in theta.",
    "start": "768709",
    "end": "775709"
  },
  {
    "text": "So you count how many non-zero\nentries in theta, and that's what this notation is for.",
    "start": "775710",
    "end": "780860"
  },
  {
    "text": "So sometimes people\ncall it zero norm, but it's actually\nnot a norm literally. It's just the number of\nnon zeros in a parameter.",
    "start": "780860",
    "end": "788420"
  },
  {
    "text": "And sometimes, people\ncall this sparsity because if you have very\nfew non-zero entries, then",
    "start": "788420",
    "end": "796540"
  },
  {
    "text": "it's sparse,\notherwise it's dense. So if you add this\nto this thing, then you're going to\nhave a different effect.",
    "start": "796540",
    "end": "802940"
  },
  {
    "text": "You are trying to say that\nI'm going to find a model such that the number of non\nzeros in it is small.",
    "start": "802940",
    "end": "809350"
  },
  {
    "text": "And this is particularly\nmeaningful for linear models in the following\nsense because if you think of this theta\nas a linear model,",
    "start": "809350",
    "end": "814899"
  },
  {
    "text": "then-- say you have theta\ntranspose x, suppose you have a linear model. And then what is this? This is really just sum of\ntheta i x i, i from 1 to d.",
    "start": "814900",
    "end": "826060"
  },
  {
    "text": "And then you can see that\nthe number of non zeros is really how many-- if suppose you have as ith\nnon-zero entries in theta,",
    "start": "826060",
    "end": "834019"
  },
  {
    "text": "that means that you are using\nonly ith of the coordinates of x i's. So basically, the number\nof non zeros in theta",
    "start": "834019",
    "end": "842430"
  },
  {
    "text": "is the number of\ncoordinates or number of features you\nare using from xi.",
    "start": "842430",
    "end": "848060"
  },
  {
    "text": "So you can imagine that\nmaybe, for example, for some applications, you\nhave a lot of coordinates",
    "start": "848060",
    "end": "855399"
  },
  {
    "text": "in your input features. So you have so many\ndifferent informations, but you don't know which one\nyou should use to predict.",
    "start": "855399",
    "end": "862579"
  },
  {
    "text": "For example, suppose you want\nto predict the price of a house. Then you have so many\ndifferent features.",
    "start": "862580",
    "end": "867990"
  },
  {
    "text": "But some features may\nnot be that useful. So then you can imagine that\ncould be a situation where you should use this\nas a regularizer",
    "start": "867990",
    "end": "873690"
  },
  {
    "text": "because you want to\nsay, I want to use as few features as\npossible, but also I want to make sure my\ntraining loss is good.",
    "start": "873690",
    "end": "880750"
  },
  {
    "text": "So I find the\nsimplest explanations of the existing\ndata or simplest,",
    "start": "880750",
    "end": "887310"
  },
  {
    "text": "meaning that you want to use\nas few features as possible. And once you find this theta\nsuch that theta is sparse,",
    "start": "887310",
    "end": "894800"
  },
  {
    "text": "so suppose you have a\ntheta such that you only have a few non zeros\nin theta, then you are",
    "start": "894800",
    "end": "899860"
  },
  {
    "text": "selecting the right feature. So in some sense, you\nhave a sparse model,",
    "start": "899860",
    "end": "906250"
  },
  {
    "text": "means you are\nselecting the features",
    "start": "906250",
    "end": "913560"
  },
  {
    "text": "because those\nnon-zero corresponds to the features that are\nselected by the model.",
    "start": "913560",
    "end": "918660"
  },
  {
    "text": "So people often call\nthis feature selections in certain kind of contexts.",
    "start": "918660",
    "end": "927170"
  },
  {
    "text": "However, you may have realized\nthat this regularizer, as a function of theta,\nthis sparsity one is not",
    "start": "927170",
    "end": "935620"
  },
  {
    "text": "differentiable. So you're just counting\nhow many zeros there are.",
    "start": "935620",
    "end": "941079"
  },
  {
    "text": "So suppose you have one entry\nthat is maybe it say that",
    "start": "941079",
    "end": "946600"
  },
  {
    "text": "it's just 000. You do an infinitesimal change\nin any of the coordinates.",
    "start": "946600",
    "end": "951930"
  },
  {
    "text": "You are going to change\nthe value of this function",
    "start": "951930",
    "end": "958100"
  },
  {
    "text": "by a lot. So that's why it's\nnot differentiable. A differentiable\nfunction should satisfy that if you change theta\ninfinitesimally small,",
    "start": "958100",
    "end": "965670"
  },
  {
    "text": "then you should change\nthe function output by a small amount. But actually here, currently\nthe sparsity is zero.",
    "start": "965670",
    "end": "974310"
  },
  {
    "text": "But you've changed\ntheta a little bit, your sparsely becomes 1. So you can have\ninfinitesimally small changes",
    "start": "974310",
    "end": "979880"
  },
  {
    "text": "to make the regularizer value\nchange by a large amount. So that's why it's\nnot differentiable.",
    "start": "979880",
    "end": "985980"
  },
  {
    "text": "So because it's\nnot differentiable, then you don't have gradients,\nyou don't have derivatives.",
    "start": "985980",
    "end": "992610"
  },
  {
    "text": "So that cause a\nproblem in using this. So basically, even though I\ntold you this is a regularizer,",
    "start": "992610",
    "end": "999020"
  },
  {
    "text": "but literally in the reality,\nnobody use this exactly in their algorithm because if\nyou use it, you put it here,",
    "start": "999020",
    "end": "1007180"
  },
  {
    "text": "but this term has no gradient. How do you optimize it? So because it's\nnon differentiable,",
    "start": "1007180",
    "end": "1013459"
  },
  {
    "text": "so then what we will do is\nthat you have a surrogate.",
    "start": "1013460",
    "end": "1020399"
  },
  {
    "text": "So this is a typical surrogate. The reason why this is surrogate\nis a little bit kind of tricky.",
    "start": "1020399",
    "end": "1028209"
  },
  {
    "text": "But this has been a\nsurrogate for the sparsity. So this is a\ndifferentiable surrogate",
    "start": "1028210",
    "end": "1033550"
  },
  {
    "text": "for the sparsity of the model.",
    "start": "1033550",
    "end": "1044010"
  },
  {
    "text": "So you use the 1 norm. So here, 1 norm just means\nthe sum of the absolute value,",
    "start": "1044010",
    "end": "1050929"
  },
  {
    "text": "is the sum of the absolute\nvalue of each coordinate. And you can see,\nI wouldn't attempt",
    "start": "1050929",
    "end": "1058370"
  },
  {
    "text": "to give you a very formal\njustification for why this is so good, why this is\na good surrogate for the zero.",
    "start": "1058370",
    "end": "1066669"
  },
  {
    "text": "One reason could be that you\ncan see at least the 1 norm is closer than 2 norm to 0 norm.",
    "start": "1066669",
    "end": "1076610"
  },
  {
    "text": "And another reason is probably-- this is not really very\nsolid mathematical reason,",
    "start": "1076610",
    "end": "1082060"
  },
  {
    "text": "but it's just to give\nyou some intuition. So suppose you think of\ntheta as a vector in 01,",
    "start": "1082060",
    "end": "1088950"
  },
  {
    "text": "suppose you really just\nhave a binary vector, then indeed this is\nequal to this, right?",
    "start": "1088950",
    "end": "1098230"
  },
  {
    "text": "So that's probably another\nkind of intuitive reason why they are somewhat related.",
    "start": "1098230",
    "end": "1103580"
  },
  {
    "text": "But you can see a lot of\nproblems with this argument. So why I'm assuming theta is\nonly taking value from 0 and 1,",
    "start": "1103580",
    "end": "1110770"
  },
  {
    "text": "right? So if they are from 0\nand 2, then these two are no longer that\nrelated anymore.",
    "start": "1110770",
    "end": "1116500"
  },
  {
    "text": "So I'm not saying that this is\nreally a good argument for why they are related. So if you really want\nto say they are related",
    "start": "1116500",
    "end": "1122620"
  },
  {
    "text": "or this is a good\nsurrogate, I think you have to go through\nmuch more math.",
    "start": "1122620",
    "end": "1127850"
  },
  {
    "text": "Any questions so far? [INAUDIBLE] regularizer with\nthe second norm regularizer?",
    "start": "1127850",
    "end": "1138490"
  },
  {
    "text": "You mean the 1 norm and 2 norm? Yes. Yes, that's what I'm\ngoing to do next.",
    "start": "1138490",
    "end": "1145740"
  },
  {
    "text": "All right, so that's\na great question. So why you want to\nencourage sparsity sometimes",
    "start": "1145740",
    "end": "1153549"
  },
  {
    "text": "and sometimes you\nwant to encourage the 2 norm to be small? So let's answer that question. Let's think of this as a\nsurrogate for the sparsity.",
    "start": "1153549",
    "end": "1160370"
  },
  {
    "text": "So the question I'm\ntrying to answer is why sometimes this is better,\nsometimes this is better. So in some sense, the\nfundamental reason",
    "start": "1160370",
    "end": "1167380"
  },
  {
    "text": "is just that in some\nsense, another way to think about\nregularization, instead",
    "start": "1167380",
    "end": "1173640"
  },
  {
    "text": "of just encouraging\nlow complexity, is that regularization\ncan also impose structures",
    "start": "1173640",
    "end": "1180789"
  },
  {
    "text": "kind of like prior\nbeliefs about the theta. So this is probably another--",
    "start": "1180789",
    "end": "1186650"
  },
  {
    "text": "at least one of the other ways\nto think of regularization. This also imposes structures.",
    "start": "1186650",
    "end": "1200500"
  },
  {
    "text": "And what are the structures? The structures probably\nsometimes, students, you have a prior belief.",
    "start": "1200500",
    "end": "1207740"
  },
  {
    "text": "So for example,\nsuppose you believe, you have a prior belief because\nof the common knowledge.",
    "start": "1207740",
    "end": "1213370"
  },
  {
    "text": "So such that you somehow\nbelieve that faith is sparse.",
    "start": "1213370",
    "end": "1224179"
  },
  {
    "text": "So in sparse theta. Then in this case, you\nprobably should just",
    "start": "1224179",
    "end": "1231049"
  },
  {
    "text": "use R theta is the 1\nnorm or the zero norm.",
    "start": "1231049",
    "end": "1236908"
  },
  {
    "text": "Just because you believe\nthat your model is sparse, why not just encourage that? So when you have\nthis belief, then you",
    "start": "1236909",
    "end": "1244020"
  },
  {
    "text": "should say, OK, if you encourage\nthe 1 norm, then in some sense,",
    "start": "1244020",
    "end": "1249290"
  },
  {
    "text": "you limit your search space. You limit your search space. Before, you are searching\nover all possible parameters.",
    "start": "1249290",
    "end": "1255530"
  },
  {
    "text": "And now you're only searching\nover low 1 norm parameters. And because you believe\nthat your true model is",
    "start": "1255530",
    "end": "1262779"
  },
  {
    "text": "having low norm, then even\nnarrowing the search space is always-- narrowing your search\nspace is always",
    "start": "1262780",
    "end": "1268640"
  },
  {
    "text": "helping you because you\ndidn't lose anything, because you know that every\nmodel that you excluded",
    "start": "1268640",
    "end": "1275850"
  },
  {
    "text": "are not going to be\nthe right solution. So you narrow search\nspace, the new search space",
    "start": "1275850",
    "end": "1281320"
  },
  {
    "text": "still has the right\nmodel, then why not do it. So that's another interpretation\nof the regularizer.",
    "start": "1281320",
    "end": "1291250"
  },
  {
    "text": "So it can impose\nadditional prior belief",
    "start": "1291250",
    "end": "1296370"
  },
  {
    "text": "in the structure of the model. So if you believe\nin 1 norm, then you should encourage 1 norm.",
    "start": "1296370",
    "end": "1301940"
  },
  {
    "text": "If you believe that your true\nmodel has a small L2 norm, then you should\nencourage small L2 norm.",
    "start": "1301940",
    "end": "1309890"
  },
  {
    "text": "And if you go into more\nmathematical theory, I think L2 norm typical\ncorresponds to situations",
    "start": "1309890",
    "end": "1315890"
  },
  {
    "text": "where you believe that all\nthe features are useful, but you have to use\nthem in a combination.",
    "start": "1315890",
    "end": "1321350"
  },
  {
    "text": "So you have to use\neach of the features a little bit and L1 norm or\nL0 norm typically corresponds",
    "start": "1321350",
    "end": "1327730"
  },
  {
    "text": "to situations where you believe\nonly a subset of the features are meaningful and\nyou should discard other ones because other\nones are just kind of there",
    "start": "1327730",
    "end": "1335309"
  },
  {
    "text": "to confuse you in some sense. So if you believe\nyour model is sparse, then you should use\nL1 norm, and if you",
    "start": "1335309",
    "end": "1341001"
  },
  {
    "text": "believe your model shouldn't\nbe sparse, then typically, people use L2 norm.",
    "start": "1341001",
    "end": "1348230"
  },
  {
    "text": "And if you have a\nlinear model, so suppose you have a linear\nmodel, and then this loss--",
    "start": "1348230",
    "end": "1361430"
  },
  {
    "text": "if you use one regularizer\nthis is called LASSO.",
    "start": "1361430",
    "end": "1366510"
  },
  {
    "text": "I guess here, I'm\njust defining the name because I think it's probably\nuseful for to at least heard of this acronym.",
    "start": "1366510",
    "end": "1373659"
  },
  {
    "text": "Actually, I don't know what\nit originally stands for. But this has been like\nthere for 20 or 30",
    "start": "1373659",
    "end": "1381950"
  },
  {
    "text": "years, which is a very,\nvery important algorithm. For linear model, you\napply L1 normalization and it's called LASSO.",
    "start": "1381950",
    "end": "1388919"
  },
  {
    "text": "Everyone in machine learning\nshould know the acronym.",
    "start": "1388919",
    "end": "1399120"
  },
  {
    "start": "1397000",
    "end": "1637000"
  },
  {
    "text": "I'm taking a little bit\nmore broader perspective. So if you think about nonlinear\nmodels, deep learning models,",
    "start": "1399120",
    "end": "1407140"
  },
  {
    "text": "so what are the most popular\nregularizers these days? I think L1 norm is\nnot used very often.",
    "start": "1407140",
    "end": "1412720"
  },
  {
    "text": "And actually, pretty\nmuch, it's never used. I don't know exactly\nhow frequent it is, but I think probably\nless than 10% of models",
    "start": "1412720",
    "end": "1420289"
  },
  {
    "text": "use L1 regularizer, maybe\neven less than that. much overestimated. Maybe 1%.",
    "start": "1420289",
    "end": "1425559"
  },
  {
    "text": "But the L2 regularization\nis almost always used,",
    "start": "1425559",
    "end": "1431440"
  },
  {
    "text": "even though sometimes you\nonly use a very weak L2 regularization. I'm talking about\ndeep learning model. Sorry, maybe let\nme just clarify.",
    "start": "1431440",
    "end": "1439170"
  },
  {
    "text": "For linear models, you\ncan try almost anything. Anything would be reasonable.",
    "start": "1439170",
    "end": "1445240"
  },
  {
    "text": "And you probably\nshould try all of them. You could try 1 norm, 2\nnorm, and sometimes you can try different norms,\nwhich I didn't write down.",
    "start": "1445240",
    "end": "1452039"
  },
  {
    "text": "But you can try 1.5 norm,\nsomething like that. So for nonlinear model,\nfor deep learning models,",
    "start": "1452039",
    "end": "1457480"
  },
  {
    "text": "I think basically,\nL2 norm is something that you almost always\nuse, but you only use it",
    "start": "1457480",
    "end": "1464250"
  },
  {
    "text": "with relatively small lambda. People generally don't\nuse very large lambda.",
    "start": "1464250",
    "end": "1469429"
  },
  {
    "text": "I don't know exactly\nwhat's the reason. Researchers don't really\nknow that much either.",
    "start": "1469429",
    "end": "1474610"
  },
  {
    "text": "But a small L2\nregularization is typically useful for deep learning. And in deep learning, I think\nsome of the other regulations",
    "start": "1474610",
    "end": "1482630"
  },
  {
    "text": "could be useful--\nfor example, you can try to regularize the\nlipschitzness of the model, and you can try to use data\naugmentation, which we probably",
    "start": "1482630",
    "end": "1490370"
  },
  {
    "text": "haven't discussed. I'm going to discuss\nthat later lecture. But you can use\ndata augmentation",
    "start": "1490370",
    "end": "1496409"
  },
  {
    "text": "which tries to\nencourage your model to be invariant with respect\nto kind of translation,",
    "start": "1496409",
    "end": "1501790"
  },
  {
    "text": "cropping, these kind\nof things for images. I think those are pretty\nmuch the only regularization",
    "start": "1501790",
    "end": "1508470"
  },
  {
    "text": "techniques in deep learning. Question?",
    "start": "1508470",
    "end": "1513658"
  },
  {
    "text": "So this, is just kind of a\nselfish question, just kind",
    "start": "1513659",
    "end": "1521409"
  },
  {
    "text": "of for the [INAUDIBLE]. Would you suggest\ninitially using the L1 norm to eliminate the features and\nthen went on to use the normal?",
    "start": "1521409",
    "end": "1527919"
  },
  {
    "text": "That's a very good question. So I think this\nkind of algorithm",
    "start": "1527919",
    "end": "1535148"
  },
  {
    "text": "was pretty popular in bulk\nbefore deep learning era.",
    "start": "1535149",
    "end": "1540960"
  },
  {
    "text": "So when you use\nlinear models, I think using L1 to do a selection,\nand then you use L2. I don't know how exactly\nhow popular they are,",
    "start": "1540960",
    "end": "1548419"
  },
  {
    "text": "but this is definitely one\nalgorithm you could try to use.",
    "start": "1548419",
    "end": "1553840"
  },
  {
    "text": "In deep learning, I\nthink it's probably less likely to be useful, but\nalso depends on the situation.",
    "start": "1553840",
    "end": "1560460"
  },
  {
    "text": "For example, if you\nhave enough data, maybe you are more or\nless linear model case. But you just need a nonlinearity\nto help you a little bit.",
    "start": "1560460",
    "end": "1567059"
  },
  {
    "text": "Maybe then, in that\ncase, you should still mostly use some more linear\nmodel type of approach.",
    "start": "1567059",
    "end": "1572120"
  },
  {
    "text": "If you are in the typical deep\nlearning setting-- for example, you do for a vision project,\nyou have images as your inputs--",
    "start": "1572120",
    "end": "1580870"
  },
  {
    "text": "I think in those\ncases, you probably don't want to select\nyour features first. I think all the inputs\nare useful and use them",
    "start": "1580870",
    "end": "1588919"
  },
  {
    "text": "as much as possible,\nand you just want to let the\nnetworks to figure out what's the best way\nto use those inputs.",
    "start": "1588919",
    "end": "1596090"
  },
  {
    "text": "Thank you. Any other questions?",
    "start": "1596090",
    "end": "1601230"
  },
  {
    "text": "By the way, this\nlecture will be pretty-- we don't have a lot of math. Most of the things\nare about just the--",
    "start": "1601230",
    "end": "1606590"
  },
  {
    "text": "I don't think there\nis even theory here. Sometimes, they are just\nexperiences because especially if you talk about the modern\nmachine learning in the last",
    "start": "1606590",
    "end": "1614830"
  },
  {
    "text": "five years where everything\nseems to change a little bit-- so I cannot say anything\nwith 100% guarantee.",
    "start": "1614830",
    "end": "1620809"
  },
  {
    "text": "I can only say, OK,\nit sounds like people are doing this a lot. That's the best thing I\ncan tell you in some sense.",
    "start": "1620809",
    "end": "1630410"
  },
  {
    "text": "So feel free to\nask any questions. Any other questions?",
    "start": "1630410",
    "end": "1639470"
  },
  {
    "start": "1637000",
    "end": "2037000"
  },
  {
    "text": "And the next thing\nI'm going to discuss is the so-called implicit\nregularization effect.",
    "start": "1639470",
    "end": "1653408"
  },
  {
    "text": "This release more to\nthe deep learning.",
    "start": "1653409",
    "end": "1659220"
  },
  {
    "text": "And so one reason that people\nstarted to think about this is that-- I haven't told you\nwhat exactly it means.",
    "start": "1659220",
    "end": "1664679"
  },
  {
    "text": "So one motivation that people\nstudy this kind of research is that people realize\nthat in deep learning, you don't use a lot of\nregularization technique.",
    "start": "1664680",
    "end": "1671390"
  },
  {
    "text": "So you use L2 as\nI said, you only use a weak L2 regularization. And often, some of these\nlipschitzness ones,",
    "start": "1671390",
    "end": "1677250"
  },
  {
    "text": "but they only help a little bit. They can be useful, but\npeople don't necessarily use them very often. So why, in deep\nlearning, you don't have",
    "start": "1677250",
    "end": "1684330"
  },
  {
    "text": "to use strong regularization? At least you can feel\nthat the regularization",
    "start": "1684330",
    "end": "1689640"
  },
  {
    "text": "stop mattering that much. It still matters\nwhen you really care about the final performance,\nyou care about 95% versus 97%.",
    "start": "1689640",
    "end": "1696360"
  },
  {
    "text": "But even you don't\nuse regularization, sometimes you get\nreasonably good performance. So that's why people, especially\ntheoretical researchers people,",
    "start": "1696360",
    "end": "1704339"
  },
  {
    "text": "are wondering why you don't need\nto use strong regularization in deep learning.",
    "start": "1704339",
    "end": "1709669"
  },
  {
    "text": "And this is\nparticularly mysterious because in deep\nlearning, people are using over parameterization.",
    "start": "1709669",
    "end": "1715158"
  },
  {
    "text": "We are in this regime where\nyou have more parameters than the number of samples. Recall that in the\nlast lecture, we",
    "start": "1715159",
    "end": "1721640"
  },
  {
    "text": "have drawn this\ndouble descent where you have these kind\nof things-- here is the number of parameters\nand this is the test error.",
    "start": "1721640",
    "end": "1733970"
  },
  {
    "text": "And we have kind of\ndiscussed that its peak might be just something about\nthe sub optimality of the algorithm which let's say\nyou don't care for the moment.",
    "start": "1733970",
    "end": "1741549"
  },
  {
    "text": "But at least you have\nto care about why it's going down still here.",
    "start": "1741549",
    "end": "1746789"
  },
  {
    "text": "So why will you have\nso many parameters, a lot more parameters,\nyou can still make your model generalize?",
    "start": "1746789",
    "end": "1752540"
  },
  {
    "text": "And it seems that more\nand more parameters makes it looks better. So the over parameterized\nregime is kind of mysterious",
    "start": "1752540",
    "end": "1760289"
  },
  {
    "text": "because you don't use\nstrong regularization, but you can still generalize. So that was kind\nof the motivation",
    "start": "1760289",
    "end": "1766460"
  },
  {
    "text": "for people to study this. And people realized that\neven though in this regime,",
    "start": "1766460",
    "end": "1771629"
  },
  {
    "text": "suppose you don't use\nany explicit regularizer, you don't mimic the lambda 0,\nliterally 0 in this regime,",
    "start": "1771630",
    "end": "1777289"
  },
  {
    "text": "still it can generalize. And the reason it can\ngeneralize in many cases is because you can still have\nsome implicit regularization",
    "start": "1777289",
    "end": "1784260"
  },
  {
    "text": "in fact, even without\nexplicit regularizer. And where that\neffect comes from, what kind of make that happen?",
    "start": "1784260",
    "end": "1790160"
  },
  {
    "text": "The reason is that the\noptimization process, the optimization\nalgorithm, the optimizers",
    "start": "1790160",
    "end": "1797260"
  },
  {
    "text": "can implicitly regularize.",
    "start": "1797260",
    "end": "1807400"
  },
  {
    "text": "So why this can happen? I think the reason is that-- let me draw a kind of\nillustrative figure",
    "start": "1807400",
    "end": "1812440"
  },
  {
    "text": "which I kind of\nuse pretty often. So suppose let's say\nthis is the parameter--",
    "start": "1812440",
    "end": "1819870"
  },
  {
    "text": "suppose this is the loss\nlandscape, the loss surface. So meaning that here\nis theta, let's say",
    "start": "1819870",
    "end": "1825710"
  },
  {
    "text": "theta is one dimensional. And because we are\nin this deep learning setting where we have a\nnonlinear models and non convex",
    "start": "1825710",
    "end": "1832980"
  },
  {
    "text": "loss function, so maybe a loss\nfunction, it looks like this.",
    "start": "1832980",
    "end": "1838370"
  },
  {
    "text": "So this is the loss function.",
    "start": "1838370",
    "end": "1843580"
  },
  {
    "text": "And you have 2, maybe you\nhave multiple global minima of your loss function. So this is a global minimum,\nthis is a global minimum.",
    "start": "1843580",
    "end": "1853590"
  },
  {
    "text": "But you have multiple global\nminima in your loss function. However, here, I'm talking\nabout training loss.",
    "start": "1853590",
    "end": "1859110"
  },
  {
    "text": "If you really look\nat the test loss, they will look a\nlittle bit different. The test loss would be different\nfrom the training loss.",
    "start": "1859110",
    "end": "1864648"
  },
  {
    "text": "So test loss maybe look\nlike something like this. Maybe let me draw something\naccording to my figure so",
    "start": "1864649",
    "end": "1872610"
  },
  {
    "text": "that--",
    "start": "1872610",
    "end": "1891010"
  },
  {
    "text": "So this is the training loss. And test loss probably\nlook like this.",
    "start": "1891010",
    "end": "1900909"
  },
  {
    "text": "So that means that even though\nboth of these two global minima are a good solutions from the\ntraining loss perspective,",
    "start": "1900910",
    "end": "1909720"
  },
  {
    "text": "one of them is better from a\ntest performance perspective. This global minimum is better\nthan this global minimum",
    "start": "1909720",
    "end": "1917440"
  },
  {
    "text": "because the test\nperformance is better. And in some sense, the\nregularization, in fact,",
    "start": "1917440",
    "end": "1923679"
  },
  {
    "text": "is trying to choose the\nright global minimum. You want the\nregularization, in fact, to choose the right\nglobal minimum so that you",
    "start": "1923680",
    "end": "1930779"
  },
  {
    "text": "can do some type breaking\nor you can encourage certain kind of models. Maybe this model is more\nkind of like lipschitz or this model has more\nnorm than this model.",
    "start": "1930780",
    "end": "1937860"
  },
  {
    "text": "So that's why you\nprefer this one. So if you use explicit\nregularization, what you do is that you're going to say,\nI'm going to change the training",
    "start": "1937860",
    "end": "1944280"
  },
  {
    "text": "loss, I'm going to add\nsomething to prefer this one than this\none, I'm going to reshape the training loss.",
    "start": "1944280",
    "end": "1949880"
  },
  {
    "text": "That's what the explicit\nregularization would do. But in place, the regularization\nwill do is the following-- so if you consider\nan algorithm that",
    "start": "1949880",
    "end": "1957350"
  },
  {
    "text": "optimize this-- for example,\nsuppose you run algorithm. This algorithm just\nalways initialize-- this is initialization.",
    "start": "1957350",
    "end": "1965549"
  },
  {
    "text": "And you do gradient descent. So you're going to do\nsomething like this, and you converge to this one.",
    "start": "1965549",
    "end": "1973600"
  },
  {
    "text": "So this algorithm\nwill only converge to this one but not this\none just because you initialize at this far right.",
    "start": "1973600",
    "end": "1980900"
  },
  {
    "text": "So that is, in some\nsense, a preference to converge to\nthis global minimum",
    "start": "1980900",
    "end": "1986080"
  },
  {
    "text": "over this global minimum\nbecause your algorithm somehow prefer one global\nminimum than the other",
    "start": "1986080",
    "end": "1991350"
  },
  {
    "text": "just because your algorithm has\nsome certain specifics, right? So the initialization make it to\nprefer to converge to this one.",
    "start": "1991350",
    "end": "1998580"
  },
  {
    "text": "And there could be\nother kind of effects. For example, if you\nuse bigger step size,",
    "start": "1998580",
    "end": "2003980"
  },
  {
    "text": "maybe you are more likely\nto converge to this one maybe or maybe vice versa\ndepending on the situations.",
    "start": "2003980",
    "end": "2009360"
  },
  {
    "text": "So this is a very illustrative\nthing with one dimension that you don't really have\na lot of flexibility here.",
    "start": "2009360",
    "end": "2016639"
  },
  {
    "text": "But if you have a very,\nvery complex thing and if you run a\ndifferent algorithm, different algorithm\nwill converge to different global minimum.",
    "start": "2016639",
    "end": "2023210"
  },
  {
    "text": "And that preference to\ncertain type of global minimum is, in some sense, is\na regularization effect",
    "start": "2023210",
    "end": "2029970"
  },
  {
    "text": "so that you don't converge to\nan arbitrary global minimum. Does it make some sense?",
    "start": "2029970",
    "end": "2038460"
  },
  {
    "start": "2037000",
    "end": "2232000"
  },
  {
    "text": "Can you just repeat\nwhere you said-- so how does having a large\nnumber of parameters",
    "start": "2038460",
    "end": "2045980"
  },
  {
    "text": "ensure that it\ninitializes at that point? Yeah, I was selling\non that in some sense.",
    "start": "2045980",
    "end": "2051240"
  },
  {
    "text": "I didn't really say why the\ninitialization has to be here. This is an active\narea of research.",
    "start": "2051240",
    "end": "2057310"
  },
  {
    "text": "So what we are sure about is\nthat the algorithm could have this effect, the\nalgorithm could possibly",
    "start": "2057310",
    "end": "2064940"
  },
  {
    "text": "prefer certain kind of global\nminimum than the others. But why it would prefer\nwhich kind of global minimum,",
    "start": "2064940",
    "end": "2070588"
  },
  {
    "text": "we don't exactly know. For certain kind of\ntoy cases, we know. But for the general\ncases, we don't.",
    "start": "2070589",
    "end": "2076230"
  },
  {
    "text": "I'm going to show you one\ncases where we actually can say what does the\nalgorithm prefer to do.",
    "start": "2076230",
    "end": "2082608"
  },
  {
    "text": "But that's very,\nvery simple case. For general case, I think this\nis still a very open research.",
    "start": "2082609",
    "end": "2088490"
  },
  {
    "text": "Question. I saw two other questions here. [INAUDIBLE] No, no. Here-- no, no.",
    "start": "2088490",
    "end": "2094658"
  },
  {
    "text": "What do you mean\nby the optimizers? What is on the access?",
    "start": "2094659",
    "end": "2101250"
  },
  {
    "text": "The access is the\nvalue of the parameter. They only have one parameter.",
    "start": "2101250",
    "end": "2107869"
  },
  {
    "text": "I'm drawing the landscape\nof the parameter. And I can only draw\nsomething in one dimension.",
    "start": "2107869",
    "end": "2114050"
  },
  {
    "text": "So this is the value\nof the parameter. You are just tuning\nthis parameter, you are doing good instant. And this is the loss surface.",
    "start": "2114050",
    "end": "2119559"
  },
  {
    "text": "So it does depend on\nwhere you initialize. So if you initialize\nat different places, you're going to converge\ndifferent global minimum,",
    "start": "2119560",
    "end": "2126130"
  },
  {
    "text": "and they may have different\ngeneralization effect. So in practice, we can use\nmultiple different algorithms",
    "start": "2126130",
    "end": "2132369"
  },
  {
    "text": "and then just choose the one\nthat has the best performance? That's pretty much\nthe right thing to do. Of course, there are some--",
    "start": "2132369",
    "end": "2137550"
  },
  {
    "text": "I'm going to discuss this\nin a bit more detail later. But basically, you can\nhave some intuition.",
    "start": "2137550",
    "end": "2147510"
  },
  {
    "text": "The theoreticians have\ntried to understand what kind of like algorithms\ncan help generalization.",
    "start": "2147510",
    "end": "2153840"
  },
  {
    "text": "But I think the conclusion,\nat least so far, is very far from conclusive. They can give you\nsome intuition,",
    "start": "2153840",
    "end": "2160220"
  },
  {
    "text": "but they are not going\nto be predictive. They are not going to\njust tell you what to do.",
    "start": "2160220",
    "end": "2165369"
  },
  {
    "text": "So you still have to try a lot. Yeah, going back to this. This is just one dimension.",
    "start": "2165369",
    "end": "2171500"
  },
  {
    "text": "Another way to think\nabout is that you can think of like a two\ndimensional question. For example, you are\nskiing in a ski resort.",
    "start": "2171500",
    "end": "2178170"
  },
  {
    "text": "So your objective is\nbasically minimizing your-- you're trying to go downhill.",
    "start": "2178170",
    "end": "2184190"
  },
  {
    "text": "That's the objective. And the ski resort probably\nhave a lot of villages that you can eventually go home.",
    "start": "2184190",
    "end": "2190640"
  },
  {
    "text": "There are multiple parking lots. So in some sense, you are saying\nthat one of this parking lot",
    "start": "2190640",
    "end": "2196010"
  },
  {
    "text": "is great. So one of this parking\nlot is really [AUDIO OUT].. So you want to go to that one.",
    "start": "2196010",
    "end": "2202750"
  },
  {
    "text": "But different\nalgorithm would lead you to converge to\ndifferent parking lots.",
    "start": "2202750",
    "end": "2208720"
  },
  {
    "text": "So for example, someone is doing\nvery faster skiing then when you do that you cannot go to\nthose kind of small trails.",
    "start": "2208720",
    "end": "2215260"
  },
  {
    "text": "So then you go to one\nof the parking lot. And some other one\nprefers a wider trails",
    "start": "2215260",
    "end": "2222190"
  },
  {
    "text": "and then you go to\nthe other parking lot. So different algorithm\nwill need to lead you",
    "start": "2222190",
    "end": "2227220"
  },
  {
    "text": "to different parking lot\nand different parking lots have different\ngeneralization performance eventually.",
    "start": "2227220",
    "end": "2233900"
  },
  {
    "text": "So this is the high\nlevel intuition. So let's see.",
    "start": "2233900",
    "end": "2242599"
  },
  {
    "text": "I'm going to discuss a\nconcrete case which will also be part of a homework question.",
    "start": "2242599",
    "end": "2248849"
  },
  {
    "text": "So this concrete\ncase, just to give you a concrete sense on how this\ncould even be possible--",
    "start": "2248849",
    "end": "2254710"
  },
  {
    "text": "so I'm going to show you the\nhigh level thing and there is some mathematical part\nwhich will be in the homework.",
    "start": "2254710",
    "end": "2261040"
  },
  {
    "text": "So this is in the linear model. So interestingly, even though\nthis implicit regularization",
    "start": "2261040",
    "end": "2268420"
  },
  {
    "text": "effect was mostly discovered\nafter deep learning start to be powerful,\nbut actually, you",
    "start": "2268420",
    "end": "2275019"
  },
  {
    "text": "can still see it\nin linear models. And that's how researchers\nstart to do research.",
    "start": "2275020",
    "end": "2281390"
  },
  {
    "text": "So let's say,\nsuppose we are just in the most vanilla linear model\nsetting where you have some under the exam data points.",
    "start": "2281390",
    "end": "2291650"
  },
  {
    "text": "This is just the trivial\nlinear regression. And your last\nfunction is something like just the L2 loss.",
    "start": "2291650",
    "end": "2298260"
  },
  {
    "text": "This means squared error,\nsomething like this.",
    "start": "2298260",
    "end": "2306890"
  },
  {
    "text": "You have a linear model. But let's say one different\nthing is that we assume",
    "start": "2306890",
    "end": "2313530"
  },
  {
    "text": "n is much smaller than d. So you have very few examples\nand a very high dimension.",
    "start": "2313530",
    "end": "2319500"
  },
  {
    "text": "So what is d? d is the\ndimension of the data and n is the number of examples.",
    "start": "2319500",
    "end": "2325040"
  },
  {
    "text": "I'm going to assume n\nis much smaller than d. So this is\noverparameterized, you",
    "start": "2325040",
    "end": "2330240"
  },
  {
    "text": "have multiple global minimum. Why you have multiple? So first of all, you have\nmultiple global minimum.",
    "start": "2330240",
    "end": "2340130"
  },
  {
    "text": "Why? Because I'm claiming\nthat they are minus theta such that minus theta satisfies\nyi is equal to theta transpose",
    "start": "2340130",
    "end": "2352720"
  },
  {
    "text": "xi for all i. Why?",
    "start": "2352720",
    "end": "2358029"
  },
  {
    "text": "Because how many\nequations here you have. So this is the equation\nto make 20 plus 0,",
    "start": "2358030",
    "end": "2364280"
  },
  {
    "text": "which means global minimum. So if you have all\nof this equality, then it means you are at a\nglobal minimum of this training",
    "start": "2364280",
    "end": "2371190"
  },
  {
    "text": "loss. And why there are multiple theta\nsuch that you can satisfy this? That's because you can count\nhow many equations they are.",
    "start": "2371190",
    "end": "2378990"
  },
  {
    "text": "So they are n equations\nand d variables.",
    "start": "2378990",
    "end": "2390200"
  },
  {
    "text": "And these are linear equations. So I guess the linear\nalgebra tells us that if you have n\nequations, d variables,",
    "start": "2390200",
    "end": "2395760"
  },
  {
    "text": "and if n is less than-- I think if n is less than d or\nd minus 1, n is less than d,",
    "start": "2395760",
    "end": "2401530"
  },
  {
    "text": "then you can have at\nleast one solution. And if n is much, much\nsmaller than d, then you will have a\nsubspace of solutions.",
    "start": "2401530",
    "end": "2408220"
  },
  {
    "text": "And that's called the-- what's the kernels of the-- anyway, you have a subspace\nof solutions for this kind",
    "start": "2408220",
    "end": "2417150"
  },
  {
    "text": "of linear system equations.",
    "start": "2417150",
    "end": "2422260"
  },
  {
    "text": "And that's why you're going to\nhave multiple global minimum of the training loss because\nthe entire subspace of solutions",
    "start": "2422260",
    "end": "2427790"
  },
  {
    "text": "are global minimum\nof the training loss. So the question is, which one\nyou're going to converge to.",
    "start": "2427790",
    "end": "2433099"
  },
  {
    "text": "So which one your\noptimizer will choose. So it turns out that if you\nuse gradient descent with zero",
    "start": "2433099",
    "end": "2441800"
  },
  {
    "text": "initialization,\nthen you are going to choose the one with\nthe minimum L2 norm. So here is the claim.",
    "start": "2441800",
    "end": "2448650"
  },
  {
    "text": "So the claim is that if\nyou do gradient descent with initialization\ntheta is 0, this",
    "start": "2448650",
    "end": "2460500"
  },
  {
    "text": "will converge to the\nminimum norm solution.",
    "start": "2460500",
    "end": "2475160"
  },
  {
    "text": "So what does the minimum\nnorm solution mean? Formally, it means\nthat you converge to the solution\nwith the smallest L2",
    "start": "2475160",
    "end": "2482630"
  },
  {
    "text": "norm among those solutions\nsuch that those global minimum",
    "start": "2482630",
    "end": "2489250"
  },
  {
    "text": "of the loss function. So when you use\ngradient descent, you are not only just\nfinding a theta such",
    "start": "2489250",
    "end": "2495390"
  },
  {
    "text": "that the loss function zero. So typically, when you\nthink about optimization, the optimization is trying\nto find a solution such",
    "start": "2495390",
    "end": "2502050"
  },
  {
    "text": "that the loss\nfunction is minimized. That's true. You definitely find\na solution such that the loss\nfunction is minimized.",
    "start": "2502050",
    "end": "2508740"
  },
  {
    "text": "But you actually\nhave a tie breaking effect among the solutions\nsuch that the loss function",
    "start": "2508740",
    "end": "2515220"
  },
  {
    "text": "is minimized, you\nactually choose the one with the smallest L2 norm.",
    "start": "2515220",
    "end": "2530270"
  },
  {
    "text": "So I guess in some sense,\nthe kind of intuition is the following. So I'm going to\ntry to draw this.",
    "start": "2530270",
    "end": "2535490"
  },
  {
    "text": "This is a little bit-- I need to try to draw this well.",
    "start": "2535490",
    "end": "2546950"
  },
  {
    "text": "So suppose let's say the\nintuition is supposed let's say you have n is 1 and d is 3.",
    "start": "2546950",
    "end": "2561869"
  },
  {
    "text": "So you just have one\nequation, one linear equation, and you have three variables.",
    "start": "2561869",
    "end": "2570059"
  },
  {
    "text": "So that means that the final\nsolution is a two dimensional subspace.",
    "start": "2570060",
    "end": "2575119"
  },
  {
    "text": "So let me try to draw this.",
    "start": "2575119",
    "end": "2609700"
  },
  {
    "text": "So here, the subspace\nI'm drawing here is-- this is the family of theta\nsuch that you satisfies",
    "start": "2609700",
    "end": "2617580"
  },
  {
    "text": "that the loss is zero. This is the subspace. So you have a\nsubspace of solutions.",
    "start": "2617580",
    "end": "2623680"
  },
  {
    "text": "But which solution you converge\nto-- that's the question. It turns out that\nif you start with--",
    "start": "2623680",
    "end": "2632839"
  },
  {
    "text": "let's see. Maybe I will write here. It turns out that you're going\nto find the solution such",
    "start": "2632839",
    "end": "2638690"
  },
  {
    "text": "that this is the solution\nI'm going to find. This is the solution-- how do I draw this?",
    "start": "2638690",
    "end": "2647410"
  },
  {
    "text": "drawing this is a little\nbit challenging, I guess.",
    "start": "2647410",
    "end": "2654619"
  },
  {
    "text": "How did I do this? I think did this.",
    "start": "2654619",
    "end": "2665140"
  },
  {
    "text": "So you consider that you\nprojected 0 to this subspace so that you find this point. This point is the solution\nwith the minimum norm",
    "start": "2665140",
    "end": "2673039"
  },
  {
    "text": "that is closest to\nzero on the subspace. And this is the solution\nthat you will find.",
    "start": "2673040",
    "end": "2678570"
  },
  {
    "text": "You are not going to find other\nsolutions with gradient descent",
    "start": "2678570",
    "end": "2683720"
  },
  {
    "text": "with initialization zero. So basically that's the claim. The claim is that you can\nfind this particular solution but not the other solutions.",
    "start": "2683720",
    "end": "2691000"
  },
  {
    "text": "And the reason is actually,\nfundamental reason is pretty simple, especially\nif I draw it in this way.",
    "start": "2691000",
    "end": "2696130"
  },
  {
    "text": "Of course, if you want to proof\nit, it's a little bit more complicated. So the reason is really\njust that you start with 0.",
    "start": "2696130",
    "end": "2705800"
  },
  {
    "text": "This is where you start\nwith, gradient descent. And you have a property\nsuch that when you--",
    "start": "2705800",
    "end": "2712470"
  },
  {
    "text": "how do I-- maybe\nlet's erase this.",
    "start": "2712470",
    "end": "2719400"
  },
  {
    "text": "So you have a property such\nthat if you start with--",
    "start": "2719400",
    "end": "2725078"
  },
  {
    "text": "initially it's 0. And then at any time--",
    "start": "2725079",
    "end": "2732109"
  },
  {
    "text": "so your theta is always in the\nspan of all the data points.",
    "start": "2732109",
    "end": "2737799"
  },
  {
    "text": "Here, I'm going to have\nactually one data point.",
    "start": "2737800",
    "end": "2743630"
  },
  {
    "text": "So basically, your theta cannot\nmove arbitrarily in any places. So you have a restriction\non where the theta can go.",
    "start": "2743630",
    "end": "2751550"
  },
  {
    "text": "So actually, for this\nparticular case, what happens is really just that you are just\nmoving along this direction.",
    "start": "2751550",
    "end": "2759070"
  },
  {
    "text": "And here, you find this\npoint that has the substance. And that's what the\ngradient descent is doing. So gradient descent\nwill not do something",
    "start": "2759070",
    "end": "2765080"
  },
  {
    "text": "like this, will not\nconverge to here, it will not converge to here. It will just directly go to this\nclosest point, the point that",
    "start": "2765080",
    "end": "2773011"
  },
  {
    "text": "is closest to 0 on a subspace. So this is clearly a\nproperty of the optimizers.",
    "start": "2773011",
    "end": "2780630"
  },
  {
    "text": "You can imagine you may have\nother optimizer-- suppose you designed some\ncrazy optimizer which does this or does this.",
    "start": "2780630",
    "end": "2786220"
  },
  {
    "text": "Then you will convert\nto a different point. But if you use gradient descent,\nyou're going to do this.",
    "start": "2786220",
    "end": "2798420"
  },
  {
    "text": "And the main property you show\nthat gradient descent is doing this is by saying that the\ngradient descent is always",
    "start": "2798420",
    "end": "2804619"
  },
  {
    "text": "in the span of the data. I think this is\nactually something we have approved\nfor in the kernel",
    "start": "2804619",
    "end": "2813160"
  },
  {
    "text": "lecture for a different\npurpose, not for this purpose. Remember that in\nthe kernel lecture,",
    "start": "2813160",
    "end": "2818450"
  },
  {
    "text": "we try to show that\nyour parameter is always in a linear combination\nof the data.",
    "start": "2818450",
    "end": "2824049"
  },
  {
    "text": "And then there, the purpose was\nthat you want to represent it by the betas in that lecture.",
    "start": "2824050",
    "end": "2829630"
  },
  {
    "text": "So it's a different\nreason, it's different goal but it's the same fact. Your theta is always\nin a span of the data.",
    "start": "2829630",
    "end": "2838150"
  },
  {
    "text": "So are you saying that the\noptimal theta is in this span,",
    "start": "2838150",
    "end": "2850640"
  },
  {
    "text": "it's always in this\nspan of the data? The optimal-- no,\nthis span is defined to be all the solutions\nthat have zero loss. So these are all-- the span is--",
    "start": "2850640",
    "end": "2857599"
  },
  {
    "text": "that's my definition\nof the sub space. This is the family of solutions\nthat have zero training loss.",
    "start": "2857599",
    "end": "2864599"
  },
  {
    "text": "So the question is, which\none I'm going to converge to. I was arguing that there\nare multiple global minimum.",
    "start": "2864599",
    "end": "2871599"
  },
  {
    "text": "So this whole span is\nour global minimum. All of them are global minimum. And which one you\nwant to converge to. So different algorithm\nprobably would",
    "start": "2871599",
    "end": "2878430"
  },
  {
    "text": "converge to different points. So if you run\ngradient descent, you are going to converge to one\nparticular one in this span.",
    "start": "2878430",
    "end": "2888350"
  },
  {
    "text": "But this phenomenon also\nshows up in other cases,",
    "start": "2888350",
    "end": "2894720"
  },
  {
    "text": "but they're going to be\nmuch more complicated. I think there are only a very\nlimited number of situations",
    "start": "2894720",
    "end": "2900690"
  },
  {
    "text": "where we can theoretically\nprove where you converge to. But it's almost always the\ncase that the optimizer",
    "start": "2900690",
    "end": "2907040"
  },
  {
    "text": "has some preferences. The optimizer will not converge\nto an arbitrary zero training",
    "start": "2907040",
    "end": "2912240"
  },
  {
    "text": "loss solution. It will converge to one\nparticular zero training loss solution. And sometimes that solution\njust generalize much better",
    "start": "2912240",
    "end": "2918300"
  },
  {
    "text": "than the other ones.",
    "start": "2918300",
    "end": "2926790"
  },
  {
    "text": "[INAUDIBLE]",
    "start": "2926790",
    "end": "2934730"
  },
  {
    "text": "So only for linear models,\nthe family of 0 loss solution",
    "start": "2934730",
    "end": "2940030"
  },
  {
    "text": "is a span, right? So if you have\nnon-linear models, then the family of\nsolutions satisfying this",
    "start": "2940030",
    "end": "2946049"
  },
  {
    "text": "wouldn't be a span. Maybe it's a manifold, some\nother weird structure, right? So in that sense,\nthis is very special",
    "start": "2946049",
    "end": "2952568"
  },
  {
    "text": "I think it was like,\nOK, I understand",
    "start": "2952569",
    "end": "2960559"
  },
  {
    "text": "that it's like this\n[INAUDIBLE] how",
    "start": "2960559",
    "end": "2965609"
  },
  {
    "text": "are we sure it's going to be\nthat constrained optimization",
    "start": "2965609",
    "end": "2972940"
  },
  {
    "text": "[INAUDIBLE] Right. So I didn't show\nyou the full proof. So this point turns\nout to be-- the point that you converge to turns\nout to be the minimum norm",
    "start": "2972940",
    "end": "2980260"
  },
  {
    "text": "solution. And it turns out that actually,\nyou're just going straight-- at least for this one case.",
    "start": "2980260",
    "end": "2986349"
  },
  {
    "text": "So actually, it's\nnot even always true that you are going\nin a straight line.",
    "start": "2986349",
    "end": "2992309"
  },
  {
    "text": "But you always go\nin this subspace. So am I answering the question?",
    "start": "2992310",
    "end": "2997460"
  },
  {
    "text": "Maybe I didn't. Can you prove that-- You can prove it.",
    "start": "2997460",
    "end": "3003510"
  },
  {
    "text": "I think the homework\nquestion actually asks if that's your converge. This point is exactly\nthe minimum norm solution",
    "start": "3003510",
    "end": "3008839"
  },
  {
    "text": "and also you're going\nto converge to that. OK. Actually, you can have a\npretty concrete representation",
    "start": "3008839",
    "end": "3016420"
  },
  {
    "text": "of this point. It's really just some inverse\nsome of the matrix times something.",
    "start": "3016420",
    "end": "3022190"
  },
  {
    "text": "You can compute what exactly\nit is and you can show your converse to the point.",
    "start": "3022190",
    "end": "3027369"
  },
  {
    "text": "I'm not sure whether the\nhomework ask you to show-- I think the homework\nask you to show both.",
    "start": "3027369",
    "end": "3040010"
  },
  {
    "text": "But we have a lot of\nhints along the way. It's not going to be just\nshow this, that's it.",
    "start": "3040010",
    "end": "3047510"
  },
  {
    "text": "And maybe for example, another--",
    "start": "3047510",
    "end": "3053900"
  },
  {
    "text": "just to give you a sense\nof what this kind of things can change where suppose\nyou initialize here, then you wouldn't converge here.",
    "start": "3053900",
    "end": "3060599"
  },
  {
    "text": "So you probably would\nconverge to somewhere here. And if you use stochastic\ngradient descent,",
    "start": "3060599",
    "end": "3068740"
  },
  {
    "text": "you probably wouldn't\nconverge exactly here either. You'll probably converge\nsomewhere differently.",
    "start": "3068740",
    "end": "3074320"
  },
  {
    "text": "So where do you exactly converge\nto is a very hard question. We don't really know.",
    "start": "3074320",
    "end": "3080060"
  },
  {
    "text": "The only thing we know\nright now, I think, firmly is that this matters. If you use different\nalgorithm, you",
    "start": "3080060",
    "end": "3085980"
  },
  {
    "text": "converge different solutions,\nand different solutions generalize differently. So you have to consider the\neffect of the optimizers.",
    "start": "3085980",
    "end": "3094309"
  },
  {
    "start": "3092000",
    "end": "3167000"
  },
  {
    "text": "And going back to this,\nthe reason here is really-- so I guess this, in\nsome sense, this kind of",
    "start": "3094309",
    "end": "3102760"
  },
  {
    "text": "is trying to explain why\nyou can generalize here. That's because of this\nimplicit regulation, in fact,",
    "start": "3102760",
    "end": "3109030"
  },
  {
    "text": "even though you don't\nhave regularizers, you still implicit\nregularized the L2 norm.",
    "start": "3109030",
    "end": "3114490"
  },
  {
    "text": "And that's why in this\nregime, even though you have a lot of\nparameters, but actually, you are still implicitly\nregularizing the L2 norm.",
    "start": "3114490",
    "end": "3121059"
  },
  {
    "text": "And if you look at the norm,\nthe norm will look like this. So this is the norm as\nyou change the parameter.",
    "start": "3121060",
    "end": "3130020"
  },
  {
    "text": "So basically, this\nis saying that when you have a lot of\nparameters, actually your real norm is\nactually relatively small.",
    "start": "3130020",
    "end": "3135600"
  },
  {
    "text": "And that's why you\ncan generalize. So the reason why you don't\ngeneralize in the middle",
    "start": "3135600",
    "end": "3143578"
  },
  {
    "text": "is because this minimum norm\nsolution is not actually doing well in the middle\nfor some other reason.",
    "start": "3143579",
    "end": "3150230"
  },
  {
    "text": "So the norm actually\nturns out to be big. But actually, the norm is very\nsmall in over parameters regime even though you use\na lot of parameters.",
    "start": "3150230",
    "end": "3169740"
  },
  {
    "start": "3167000",
    "end": "3599000"
  },
  {
    "text": "So now let's talk about how\ndo you really find out what--",
    "start": "3169740",
    "end": "3175760"
  },
  {
    "text": "I've told you that we don't\nknow too much about how does the optimizer change things.",
    "start": "3175760",
    "end": "3180920"
  },
  {
    "text": "So we also don't know\nexactly how does the model complexity change things. So you only know\nsome intuitions.",
    "start": "3180920",
    "end": "3187160"
  },
  {
    "text": "So you know that if you\nhave more complexity, it turns out to be\nmore likely to overfit.",
    "start": "3187160",
    "end": "3193359"
  },
  {
    "text": "But you don't know exactly\nwhat is the right complexity. So how do you find out the right\nmodel, the right optimization",
    "start": "3193359",
    "end": "3198750"
  },
  {
    "text": "algorithm, the right\nregularizer, all of this? You have so many decisions. You probably have",
    "start": "3198750",
    "end": "3204109"
  },
  {
    "text": "have to make in this\nmachine learning algorithm. So how do you find out\nwhat's the best thing?",
    "start": "3204109",
    "end": "3210369"
  },
  {
    "text": "So I think the\ntechnical way is just that you use a validation\nset to figure out",
    "start": "3210369",
    "end": "3219589"
  },
  {
    "text": "what's the best decision. So maybe just to motivate\nthat just briefly--",
    "start": "3219589",
    "end": "3225200"
  },
  {
    "text": "so the easiest way to do is\nthat you just use a test set. So you have some\ntest set and you just",
    "start": "3225200",
    "end": "3232329"
  },
  {
    "text": "try all kind of algorithms,\nall kind of models, all kind of\nregularization strength. And you see which one has the\nbest performance test set.",
    "start": "3232329",
    "end": "3241050"
  },
  {
    "text": "So that's OK as long as you only\nuse the test set at the end.",
    "start": "3241050",
    "end": "3247200"
  },
  {
    "text": "So you try all of this\nalgorithm in advance, and then you collect\nsome test set, or maybe you have to test set\nbefore but you never touch it.",
    "start": "3247200",
    "end": "3255390"
  },
  {
    "text": "So that's OK. So if you only use\nthe test set once, then you can use the test set to\nevaluate the performance of all",
    "start": "3255390",
    "end": "3263230"
  },
  {
    "text": "possible algorithms\nor all possible models you want to use.",
    "start": "3263230",
    "end": "3269029"
  },
  {
    "text": "So that's a good thing. So however, the problem\nis that sometimes, you",
    "start": "3269030",
    "end": "3278039"
  },
  {
    "text": "want to do this iteratively. You want to look at\na test set and see what the performance is. And then you go back\nto say, OK, maybe I'll change my model size, maybe\nI'll change my optimizer.",
    "start": "3278040",
    "end": "3289450"
  },
  {
    "text": "So maybe I'll change\nfrom gradient descent to stochastic gradient\ndescent, maybe I want to add some\nregularization effect,",
    "start": "3289450",
    "end": "3296900"
  },
  {
    "text": "add some regularization\nfunction. So if you want to\ndo it iteratively,",
    "start": "3296900",
    "end": "3302580"
  },
  {
    "text": "then what I said before\nwas not going to work. That's because typically,\nif you have a test set,",
    "start": "3302580",
    "end": "3309930"
  },
  {
    "text": "you can only use it once because\nif you use it multiple times,",
    "start": "3309930",
    "end": "3315829"
  },
  {
    "text": "what happens is that you\ncould overfit the test set.",
    "start": "3315830",
    "end": "3321470"
  },
  {
    "text": "So basically, your later\ndecision becomes overfit--",
    "start": "3321470",
    "end": "3327190"
  },
  {
    "text": "our decisions overfitting to\nthe test you have seen before. So the validity of\nthe test set is only",
    "start": "3327190",
    "end": "3333789"
  },
  {
    "text": "insured when you\nonly see the test set after you do the training.",
    "start": "3333789",
    "end": "3339290"
  },
  {
    "text": "And if you see the test set\nand then you do the training, and then you test it again,\nthen the second time it",
    "start": "3339290",
    "end": "3346700"
  },
  {
    "text": "has on test set, it will\nnot guaranteed to be valid. So you may overfit\nto the test set.",
    "start": "3346700",
    "end": "3353369"
  },
  {
    "text": "Does it makes sense? I'm trying to be not\nover complicate this.",
    "start": "3353369",
    "end": "3359349"
  },
  {
    "text": "So that's why I'm trying to\nuse informal words for it. But if there's any questions--",
    "start": "3359349",
    "end": "3366180"
  },
  {
    "text": "So how do we deal with this. So the test that, we\ncan only use once. Or at least we can only use it--\nwe cannot use it interactively.",
    "start": "3366180",
    "end": "3372619"
  },
  {
    "text": "You can't see the test\nset, train, and then see the test set again. So one way to deal with this\nis that you have a holdout,",
    "start": "3372619",
    "end": "3385490"
  },
  {
    "text": "or you have a validation set. So basically, you split\nthe data into three parts.",
    "start": "3385490",
    "end": "3394210"
  },
  {
    "text": "So one part is called\ntraining set and one part is called validation\nset and also test set.",
    "start": "3394210",
    "end": "3407789"
  },
  {
    "text": "And for test set, you have\nto be very careful about it.",
    "start": "3407789",
    "end": "3414150"
  },
  {
    "text": "You shouldn't touch it. This test set is only\nat the very, very end, you are using a test set to\nevaluate your performance.",
    "start": "3414150",
    "end": "3423170"
  },
  {
    "text": "But the validation set, you use\nthis to tune hyperparameters. And the parameters here--",
    "start": "3423170",
    "end": "3430859"
  },
  {
    "text": "I mean, all the type of\nparameters that you are choosing-- for example,\nthe batch sets,",
    "start": "3430859",
    "end": "3439828"
  },
  {
    "text": "the Lambda in the\nregularization, maybe the choice\nof the optimizer, the number of neurons you're\ngoing to use in your deep",
    "start": "3439829",
    "end": "3447170"
  },
  {
    "text": "learning model, how long\nyou are going to train-- all of this decisions\nthat you are",
    "start": "3447170",
    "end": "3453780"
  },
  {
    "text": "going to decide in\nthis process, they are called hyperparameters. And so you're using\nthe validation set",
    "start": "3453780",
    "end": "3460250"
  },
  {
    "text": "to tune the\nhyperparameters and you are using a training set to tune\nthe real parameters to optimize",
    "start": "3460250",
    "end": "3465480"
  },
  {
    "text": "the parameters. So I guess typically, we don't\nknow to tune the parameters.",
    "start": "3465480",
    "end": "3472109"
  },
  {
    "text": "These parameters are just a\nnumerical numbers in the model which, either way, you don't\nknow where the means are.",
    "start": "3472109",
    "end": "3479049"
  },
  {
    "text": "But hyperparameters\nare those kind of things that you know\ntheir meanings-- batch size, learning rate, step size.",
    "start": "3479049",
    "end": "3485539"
  },
  {
    "text": "So they all have some meanings. And you want to use\nthis validation set to tune the hyperparameters.",
    "start": "3485539",
    "end": "3492480"
  },
  {
    "text": "So basically, the processes that\nyou start with the training, and then you start with training\nwith some hyperparameters,",
    "start": "3492480",
    "end": "3497890"
  },
  {
    "text": "and then you validate\non your performance. And then you go back\nto tune again maybe",
    "start": "3497890",
    "end": "3503260"
  },
  {
    "text": "using some other\nparameters, and then you do this iteration\nfor many times. And after you are\ndone with everything",
    "start": "3503260",
    "end": "3510339"
  },
  {
    "text": "and you find out a model that\nyou are happy with, which by you are happy with,\nI mean that you find out",
    "start": "3510340",
    "end": "3515790"
  },
  {
    "text": "a model that is very\ngood on a validation set, then you finally test\nthe model on a test set.",
    "start": "3515790",
    "end": "3522180"
  },
  {
    "text": "And that can be only done once. So in some sense, I'm not\nsure how many you have seen this Kaggle competition.",
    "start": "3522180",
    "end": "3528160"
  },
  {
    "text": "It's kind of structured\nexactly like this. So there is this\nonline platform where people release their\ndata set and set up",
    "start": "3528160",
    "end": "3535150"
  },
  {
    "text": "some kind of\nchallenge for people to submit their machine learning\nmodel to solve their tasks.",
    "start": "3535150",
    "end": "3540720"
  },
  {
    "text": "So basically, in a\nKaggle computation, the organizer have a test set\nwhich nobody can touch at all.",
    "start": "3540720",
    "end": "3548369"
  },
  {
    "text": "This test set is only\nused once at the very end when you decide\nwho is the winner.",
    "start": "3548369",
    "end": "3555890"
  },
  {
    "text": "And then the organizer\nreleased this to-- actually, I'm not sure.",
    "start": "3555890",
    "end": "3561859"
  },
  {
    "text": "Sometimes they give\nyou a division. So they say, this is\nthe validation set, this is the training set. Sometimes they just\nrelease all of them to you",
    "start": "3561859",
    "end": "3569588"
  },
  {
    "text": "and then you can\ndivide it yourself. Even they release\nin this format, you can redivide them,\nwhatever you want to do.",
    "start": "3569589",
    "end": "3576960"
  },
  {
    "text": "So let's say suppose you\nhave divided all the training examples into these two sets. You can do whatever kind\nof optimization you want.",
    "start": "3576960",
    "end": "3585890"
  },
  {
    "text": "And I think typically, they do\nhave a designated validation set, which is used for computing\nthe scores on the leaderboard.",
    "start": "3585890",
    "end": "3593630"
  },
  {
    "text": "There's a leaderboard\nwhich tells you how well you are doing against\nothers, at least temporarily.",
    "start": "3593630",
    "end": "3600650"
  },
  {
    "text": "So that's the validation set. That's evaluated on\nthe validation set. But this leaderboard may\nnot be exactly the same",
    "start": "3600650",
    "end": "3607490"
  },
  {
    "text": "as the final rank. It's possible that\nfinally, you found out that somebody is succeeding\nin the leaderboard.",
    "start": "3607490",
    "end": "3613380"
  },
  {
    "text": "But eventually, in\nthe very final test, the performance is not as good\nas the validation set suggest.",
    "start": "3613380",
    "end": "3622289"
  },
  {
    "text": "But this is the general\nsetup that people are doing. Does it make some sense?",
    "start": "3622290",
    "end": "3628780"
  },
  {
    "text": "So one common question that\npeople kind of generally ask, which I ask myself\nas well, is how reliable",
    "start": "3628780",
    "end": "3636220"
  },
  {
    "text": "is validation set is, right? So if you have very high\nperformance on the validation",
    "start": "3636220",
    "end": "3641640"
  },
  {
    "text": "set, should you trust yourself? So on one side, you\nshouldn't trust yourself",
    "start": "3641640",
    "end": "3647588"
  },
  {
    "text": "trust your validation set performance, why\nyou need a test set? The test set is supposed to\ngive you the final verdict",
    "start": "3647589",
    "end": "3653990"
  },
  {
    "text": "in some sense. It's something that guarantees\nto give you the right answer.",
    "start": "3653990",
    "end": "3661588"
  },
  {
    "text": "So the validation\nset is never 100%. You probably shouldn't",
    "start": "3661589",
    "end": "3666780"
  },
  {
    "text": "So on the other hand,\nempirically, so people realize in the last five years that--",
    "start": "3666780",
    "end": "3672079"
  },
  {
    "text": "I think there is a\nsequence of paper on this. People realized that actually,\nthe validation set performance is actually well carried\nwith the test set.",
    "start": "3672079",
    "end": "3679609"
  },
  {
    "text": "So it's a reasonable\nindicator about how good your performance is on test set. It's just there is no\ntheoretical guarantee",
    "start": "3679609",
    "end": "3685940"
  },
  {
    "text": "that these two are\nexactly the same. But in most of cases, if\nyou don't do anything crazy, you don't somehow just memorize\nthe entire validation set",
    "start": "3685940",
    "end": "3695260"
  },
  {
    "text": "by creating some kind of some\nlookup table kind of things,",
    "start": "3695260",
    "end": "3700750"
  },
  {
    "text": "then typically, the performance\non the validation set is very close to test set.",
    "start": "3700750",
    "end": "3706720"
  },
  {
    "text": "And there is a very important\npaper probably three or four years ago by Berkeley people.",
    "start": "3706720",
    "end": "3713609"
  },
  {
    "text": "So they actually look at\nmaybe 300 Kaggle competitions,",
    "start": "3713609",
    "end": "3719430"
  },
  {
    "text": "and they look at the\nbest performance, the rank of the performance\non the validation set",
    "start": "3719430",
    "end": "3725809"
  },
  {
    "text": "on the leaderboard. And they look at\nhow they correlated with the final winner,\nthe final performance.",
    "start": "3725810",
    "end": "3731059"
  },
  {
    "text": "And they found that they\nare very correlated which suggests that the\nvalidation set is actually a pretty good indicator for the\ntest set even though it's not",
    "start": "3731059",
    "end": "3737170"
  },
  {
    "text": "guaranteed. And this states the\ntypical machine learning",
    "start": "3737170",
    "end": "3744760"
  },
  {
    "text": "practice is that if look at\nwhen people publish papers,",
    "start": "3744760",
    "end": "3754290"
  },
  {
    "text": "so in some sense,\npeople publish results based on validation sets. So for example, if you look\nat ImageNet performance,",
    "start": "3754290",
    "end": "3761599"
  },
  {
    "text": "in some sense, the\nso-called test performance that people report is actually\na performance on the validation",
    "start": "3761599",
    "end": "3769539"
  },
  {
    "text": "set because that so-called\ntest set has been seen so many so many times. I think actually--",
    "start": "3769539",
    "end": "3775460"
  },
  {
    "text": "I don't know exactly\nwhether there is a label, name for it in the\nImageNet the official data set.",
    "start": "3775460",
    "end": "3781539"
  },
  {
    "text": "But at least that set that you\nreport your performance with, that set shouldn't be considered\nas a test set because test set,",
    "start": "3781539",
    "end": "3789510"
  },
  {
    "text": "you should only use it once. But actually, people have\nused it so many times, maybe a million times. So basically, abstractly\nspeaking, I think these days,",
    "start": "3789510",
    "end": "3796089"
  },
  {
    "text": "when you publish paper,\nyou use the validation set. Only when you have the\nKaggle computation,",
    "start": "3796089",
    "end": "3801369"
  },
  {
    "text": "you use the test set to\nreally decide the winner. But empirically it sounds\nlike they are very close.",
    "start": "3801369",
    "end": "3806920"
  },
  {
    "text": "So actually, that's why we are\nnot worried too much about it.",
    "start": "3806920",
    "end": "3815260"
  },
  {
    "text": "Any questions? What's the name of\nthe competition?",
    "start": "3815260",
    "end": "3821260"
  },
  {
    "text": "I think this is-- I think it's called\nKaggle or Kaggle. I don't know how to pronounce.",
    "start": "3821260",
    "end": "3826390"
  },
  {
    "text": "This is a platform. So the platform hosts\na lot of competitions, maybe 100 every year\nor something like that.",
    "start": "3826390",
    "end": "3834220"
  },
  {
    "text": "You can submit your model. And sometimes, there is a prize\nfor winning the competition.",
    "start": "3834220",
    "end": "3840309"
  },
  {
    "text": "And by the way, I think\nthis validation set--",
    "start": "3840309",
    "end": "3846030"
  },
  {
    "text": "sometimes, now people call\nit development set as well.",
    "start": "3846030",
    "end": "3853980"
  },
  {
    "text": "I don't know how\npopular this name is. But at least if you\nsay validation set, I think everyone would know\nwhat you're talking about.",
    "start": "3853980",
    "end": "3860859"
  },
  {
    "text": "Development set, I think most\npeople would know as well. But it's a relatively new\nterm in the last five years.",
    "start": "3860860",
    "end": "3868279"
  },
  {
    "text": "[INAUDIBLE] training sets\nand validation sets as part of a bigger-- so that once essentially decided\non what type of parameters",
    "start": "3868279",
    "end": "3877160"
  },
  {
    "text": "you want to use, you kind\nof have a good training and validation set and\njust choose those randomly?",
    "start": "3877160",
    "end": "3884790"
  },
  {
    "text": "Right. So how do you do the split? So the most typical way is\nthat you just split randomly.",
    "start": "3884790",
    "end": "3892578"
  },
  {
    "text": "You reserve probably a tenth\nof the dataset as validation set, maybe 20% depending\non how many data you have.",
    "start": "3892579",
    "end": "3901010"
  },
  {
    "text": "And I think what\nyou are probably thinking is this so-called cross\nvalidation which does something much more complicated.",
    "start": "3901010",
    "end": "3906400"
  },
  {
    "text": "You can kind of split\nyour dataset into-- you can do multiple splits\nand try multiple experiments",
    "start": "3906400",
    "end": "3911750"
  },
  {
    "text": "on different splits. So I think I'm not going to\ncover it for this lecture",
    "start": "3911750",
    "end": "3920069"
  },
  {
    "text": "mostly because I\nthink these days, if you have a large\nenough data set, typically you just do this\nstatic split just because it's",
    "start": "3920069",
    "end": "3927200"
  },
  {
    "text": "much easier, you don't\nhave to run your algorithm multiple times, and this\nis just almost like in most",
    "start": "3927200",
    "end": "3934190"
  },
  {
    "text": "of the larger scale machine\nlearning situations, you just use this.",
    "start": "3934190",
    "end": "3939779"
  },
  {
    "text": "But if you just\nhave 100 examples, then indeed as you said, if you\nfixed 20 examples as validation",
    "start": "3939780",
    "end": "3945670"
  },
  {
    "text": "set, it's a little wasteful. So then you have to do\nsome cross validation. So we have a section\nin the lecture",
    "start": "3945670",
    "end": "3951980"
  },
  {
    "text": "notes about cross validation. There's a description\nof the practical.",
    "start": "3951980",
    "end": "3958309"
  },
  {
    "text": "I think if you're\ninterested, you can read it. It's nothing very\ncomplicated either.",
    "start": "3958310",
    "end": "3965369"
  },
  {
    "text": "So I guess-- yeah, so I'm going\nto use the last 20 minutes talk about some more\napplied perspective.",
    "start": "3965369",
    "end": "3971430"
  },
  {
    "text": "I'm going to use the slides. So I guess I'll--",
    "start": "3971430",
    "end": "3994799"
  },
  {
    "text": "See if this can work.",
    "start": "3994799",
    "end": "4027029"
  },
  {
    "text": "OK, great.",
    "start": "4027029",
    "end": "4039920"
  },
  {
    "text": "So it's not centered, is it?",
    "start": "4039920",
    "end": "4046020"
  },
  {
    "text": "OK, sounds good. OK, good.",
    "start": "4046020",
    "end": "4052019"
  },
  {
    "text": "So I think this lecture-- so we are talking\nabout some ML advice. So, here, these slides are made\nby our other instructor, Chris",
    "start": "4052020",
    "end": "4062630"
  },
  {
    "text": "Re with the help of Alex Ratner. I'm pretty much just\nrepeating whatever",
    "start": "4062630",
    "end": "4069640"
  },
  {
    "text": "he's saying in the slides. So I think the slides used to be\na little bit longer than this.",
    "start": "4069640",
    "end": "4076420"
  },
  {
    "text": "I'm going to release a\nlonger version as well. So I shorten it to only",
    "start": "4076420",
    "end": "4081530"
  },
  {
    "text": "And part of the reason is\nthat I think the slides also contain something that have\nbeen covered on the whiteboard. And also, part of the\nreason is that there",
    "start": "4081530",
    "end": "4088460"
  },
  {
    "text": "are some applied\nparts, which I think we don't have a lot of time\nto discuss in this quarter.",
    "start": "4088460",
    "end": "4095960"
  },
  {
    "text": "But I'm going to release\nthe longer slides as well for reference.",
    "start": "4095960",
    "end": "4101079"
  },
  {
    "text": "So these set of\nslides are mostly for a little more\napplied situations. For example, you are thinking\nthat you are, for example, your",
    "start": "4101080",
    "end": "4108940"
  },
  {
    "text": "start up and you\nare doing machine learning to solve some\nconcrete problems. So it's a little\nless like a research",
    "start": "4108940",
    "end": "4114949"
  },
  {
    "text": "because you're going\nto see that we're going to have much more issues\nthan a concrete research",
    "start": "4114949",
    "end": "4120230"
  },
  {
    "text": "setting. In research, actually you\nsometimes also have this, right? In research, the\nmost typical setting",
    "start": "4120230",
    "end": "4125250"
  },
  {
    "text": "is that you probably have\na very concrete data set. You know the input, output,\nyou know everything.",
    "start": "4125250",
    "end": "4130520"
  },
  {
    "text": "There is no any\nroom in flexibility. You cannot redefine the problem. And you just want to\nget the best number. That's one type of research.",
    "start": "4130520",
    "end": "4136370"
  },
  {
    "text": "I don't think this is the\nmost typical one either. But this is one\ntype of research. And then from there, you can\nhave more and more flexibility.",
    "start": "4136370",
    "end": "4143068"
  },
  {
    "text": "You can change your data, you\ncan rephrase your problem, you can find out what's\nthe right problem. And once you really\ndo it in industry,",
    "start": "4143069",
    "end": "4149770"
  },
  {
    "text": "then it's going to be\nmuch more complicated. So some disclaimers\nto start with--",
    "start": "4149770",
    "end": "4156238"
  },
  {
    "text": "I think this is Chris\ndisclaimer, which is also one. So these are like there\nis no universal what",
    "start": "4156239",
    "end": "4162318"
  },
  {
    "text": "is ground truth here. So there is no\nground truth, it's really just some experiences\nfrom people doing it",
    "start": "4162319",
    "end": "4170330"
  },
  {
    "text": "in real life. And things change over time.",
    "start": "4170330",
    "end": "4176210"
  },
  {
    "text": "Sometimes people thought\nthat was the right thing to do in five years ago,\nand now things changed. So I'm going to go through\nthis a little quickly.",
    "start": "4176210",
    "end": "4187330"
  },
  {
    "text": "I'm going to omit some\nof details as well. But feel free to stop me.",
    "start": "4187330",
    "end": "4197150"
  },
  {
    "text": "So in some sense, there are\nmany phases of ML project if you really do it in industry. So for example, one\nthing you discuss",
    "start": "4197150",
    "end": "4203050"
  },
  {
    "text": "is that do you really need a\nML system even to start with. Some of the questions\nare not really",
    "start": "4203050",
    "end": "4208210"
  },
  {
    "text": "necessarily suitable for ML. I think at least I knew I\ndon't do as much industry work.",
    "start": "4208210",
    "end": "4216190"
  },
  {
    "text": "Chris is also an entrepreneur\nbesides a professor.",
    "start": "4216190",
    "end": "4221659"
  },
  {
    "text": "So he knows a lot about this. But even I know that\nsometimes, people actually-- when they really sell\ntheir product as ML system,",
    "start": "4221659",
    "end": "4229350"
  },
  {
    "text": "but actually, the\nunderlying system is not really using much ML. So sometimes, you\ndon't really need ML.",
    "start": "4229350",
    "end": "4237550"
  },
  {
    "text": "And when you use the ML, if it\ndoesn't work, what do we do? And also, how do you deal\nwith all the ecosystem?",
    "start": "4237550",
    "end": "4244410"
  },
  {
    "text": "And we'll use the\nrunning example. We're going to have\na spam detector and the question is,\nhow do you detect spams.",
    "start": "4244410",
    "end": "4253190"
  },
  {
    "text": "We use this example\na lot in this course. So this is a seven\nsteps for ML system.",
    "start": "4253190",
    "end": "4261890"
  },
  {
    "text": "So here, again, it's\na little broader than just the ML research. So you are thinking about\ndesigning a system that",
    "start": "4261890",
    "end": "4267949"
  },
  {
    "text": "can actually work in practice. So acquire data, and you\nwant to look at the data.",
    "start": "4267950",
    "end": "4276469"
  },
  {
    "text": "And maybe want to create\nsome of tune development set that as we discussed. You want to define or\nrefine specification,",
    "start": "4276469",
    "end": "4284150"
  },
  {
    "text": "which I'm going to discuss. In some sense, this\nis saying that you have to have an evaluation\nmetric for your model.",
    "start": "4284150",
    "end": "4290530"
  },
  {
    "text": "In one sense, you want\nyour model to succeed. And then you want\nto build your model and try a bunch of models.",
    "start": "4290530",
    "end": "4296020"
  },
  {
    "text": "Maybe you're going to spend\na lot of time in step 5. And then you\neventually are going to measure a\nmodel's performance,",
    "start": "4296020",
    "end": "4302599"
  },
  {
    "text": "not necessarily only according\nto the specification you have defined in step 4,\nbut maybe you're going to have\nother measurements.",
    "start": "4302599",
    "end": "4308860"
  },
  {
    "text": "For example-- speed, training\ntime, so and so forth. And then eventually,\nyou have to repeat and maybe you have to\nrepeat a lot of times.",
    "start": "4308860",
    "end": "4316219"
  },
  {
    "text": "So I'm going to go through\nthese steps relatively quickly. I only have 15 minutes. But if you're interested, you\ncan look at the longer slides",
    "start": "4316220",
    "end": "4325100"
  },
  {
    "text": "as well. So suppose let's say you want\nto decide what is spam or not.",
    "start": "4325100",
    "end": "4332240"
  },
  {
    "text": "So ideally, you want a\ndata sample from the data that your spam product\nwill be run on.",
    "start": "4332240",
    "end": "4337780"
  },
  {
    "text": "So you want to have\nyour data to be somewhat closer to the final test data. So you don't want to just\ncollect some spam data",
    "start": "4337780",
    "end": "4345880"
  },
  {
    "text": "from 30 years ago and then use\nthis data to choose something that can work not these days.",
    "start": "4345880",
    "end": "4352520"
  },
  {
    "text": "But sometimes, this is\nnot always available because you never\nknow what the spam emails will be 10 years after.",
    "start": "4352520",
    "end": "4358739"
  },
  {
    "text": "So you have to make\nsome sacrifice. Sometimes you don't\neven have the features.",
    "start": "4358740",
    "end": "4364670"
  },
  {
    "text": "Maybe your existing record\ndidn't save everything. Maybe you just save\nthe title of the email, didn't save the entire content.",
    "start": "4364670",
    "end": "4370980"
  },
  {
    "text": "And that will limit your\ncapability of detecting spam. And there are many legal issues\nto look at the data and this",
    "start": "4370980",
    "end": "4378690"
  },
  {
    "text": "according to Chris. I think this is true as well. So you get it wrong\non the first try.",
    "start": "4378690",
    "end": "4383870"
  },
  {
    "text": "So sometimes, you'll find\nout that the data you collect are not the right one. You have to repeat.",
    "start": "4383870",
    "end": "4390138"
  },
  {
    "text": "And then after you\ncollect some data, you have to look at them, right? And this is something that we\nactually don't really teach",
    "start": "4390139",
    "end": "4396730"
  },
  {
    "text": "a lot in this machine learning\ncourse-- looking at our data-- because we are mostly\nassuming that you already",
    "start": "4396730",
    "end": "4401930"
  },
  {
    "text": "have your data, you make the\nright assumption already, you already know your\ndata is Gaussian. And then you are in Gaussian\ndiscriminant analysis.",
    "start": "4401930",
    "end": "4409430"
  },
  {
    "text": "But we never say, how do you\ndecide whether you really make the assumption about\nthe Gaussian assumption.",
    "start": "4409430",
    "end": "4414559"
  },
  {
    "text": "But in practice, you have to\ndo that because you have to see",
    "start": "4414560",
    "end": "4420470"
  },
  {
    "text": "whether the data makes sense. There are many nuances there. For example, sometimes your data\nare not as good as you think.",
    "start": "4420470",
    "end": "4428260"
  },
  {
    "text": "Maybe the format\nis not right, maybe there are some kind of like\noutliers, so and so forth.",
    "start": "4428260",
    "end": "4433698"
  },
  {
    "text": "And only if you\nlook at the data, you cannot see what\nwas going on there. So actually, even in research,\nsometimes I experience this.",
    "start": "4433699",
    "end": "4441349"
  },
  {
    "text": "So I think in one\nof my project, I think we just use the wrong\ndata from day one in some sense.",
    "start": "4441350",
    "end": "4448540"
  },
  {
    "text": "I think some of\nthe data were just corrupted just by accident. And we are training on them.",
    "start": "4448540",
    "end": "4454179"
  },
  {
    "text": "And only until one month,\nI think we realized that. So of course, in research, it's\nprobably easier to detect that.",
    "start": "4454180",
    "end": "4460928"
  },
  {
    "text": "One month is a long time for\nus to detect it, I think. But actually, you can\neasily detect them.",
    "start": "4460929",
    "end": "4466239"
  },
  {
    "text": "But for real life cases,\nsometimes it's even harder. For example, you\ndon't even necessarily have the tools to\nlook at your data.",
    "start": "4466239",
    "end": "4473408"
  },
  {
    "text": "You maybe have to build\nsome tools to look at data. And you need to think about\ndifferent subpopulations,",
    "start": "4473409",
    "end": "4481530"
  },
  {
    "text": "maybe spams from edu emails or\nspams from .com emails and see",
    "start": "4481530",
    "end": "4487039"
  },
  {
    "text": "what are the differences. So this will give you a lot\nof intuitions on what data you should use and what kind\nof models you should use.",
    "start": "4487040",
    "end": "4494820"
  },
  {
    "text": "And do this at every\nstage because for example, when you really do the--",
    "start": "4494820",
    "end": "4501280"
  },
  {
    "text": "and this is also\nthe reason why you want to build some tools to\nlook at data conveniently.",
    "start": "4501280",
    "end": "4507210"
  },
  {
    "text": "So sometimes, if you\njust look at it once, sure, then you can just\nmaybe print out something.",
    "start": "4507210",
    "end": "4513280"
  },
  {
    "text": "So but if you want to look\nat it many times, then you should have some convenient\ntools which actually eventually",
    "start": "4513280",
    "end": "4519030"
  },
  {
    "text": "will reinforce and let it\nmore likely to look at data. So I think at least in\nresearch, I also realize this.",
    "start": "4519030",
    "end": "4525920"
  },
  {
    "text": "So if the data is very\nhard to visualize, then people are less likely\nto visualize the data.",
    "start": "4525920",
    "end": "4533030"
  },
  {
    "text": "So sometimes, it requires an\ninvestment so that you can-- you can have this tool\nso that in the future,",
    "start": "4533030",
    "end": "4539030"
  },
  {
    "text": "you have less of kind of\ncost to look at your data.",
    "start": "4539030",
    "end": "4544678"
  },
  {
    "text": "And you should do this at\nevery stage in many cases.",
    "start": "4544679",
    "end": "4551920"
  },
  {
    "text": "I guess this is about\ndomain knowledge where sometimes, some of\nthe data requires expertise.",
    "start": "4551920",
    "end": "4559760"
  },
  {
    "text": "I think there are some\nexamples in the slides which I removed just to save some time.",
    "start": "4559760",
    "end": "4565470"
  },
  {
    "text": "But in short,\nsometimes, for example, if your data is corrupted,\nonly experts can know.",
    "start": "4565470",
    "end": "4572329"
  },
  {
    "text": "For example, you\nhave multiple data. Only experts can know that\nyour data are corrupted. But from a machine\nlearner perspective,",
    "start": "4572330",
    "end": "4579230"
  },
  {
    "text": "the data looks fine. So I would talk about\ntraining dev test split.",
    "start": "4579230",
    "end": "4586270"
  },
  {
    "text": "So this of course is something\nimportant for you to do. And in practice, it's a little\nbit less clear than in research",
    "start": "4586270",
    "end": "4596690"
  },
  {
    "text": "because in research, sometimes\nyou already got a split even at the first place. So you gather data, the\ndata already has a split.",
    "start": "4596690",
    "end": "4604120"
  },
  {
    "text": "But in real life, sometimes you\nhave to avoid certain leakage.",
    "start": "4604120",
    "end": "4610130"
  },
  {
    "text": "So for example, let me\ntake an extreme case. So suppose your data\nhas repetitions.",
    "start": "4610130",
    "end": "4617210"
  },
  {
    "text": "So you have a million data. But actually, every data\npoint is repeated twice.",
    "start": "4617210",
    "end": "4623679"
  },
  {
    "text": "So essentially, you just\nhave 500k but repeated twice. If that's the case,\nthen you split the data,",
    "start": "4623680",
    "end": "4629179"
  },
  {
    "text": "then you're going to\nsee some repetitions between-- some examples\nand test will also show up in the training\nexactly the same.",
    "start": "4629180",
    "end": "4636200"
  },
  {
    "text": "So that would be disastrous. So you have to kind of\navoid these situations. And this actually happens\nin the Kaggle contests.",
    "start": "4636200",
    "end": "4643410"
  },
  {
    "text": "So actually, in many, many-- actually, I try to do some\nof these Kaggle contests",
    "start": "4643410",
    "end": "4650719"
  },
  {
    "text": "at some point. At least at that\ntime, that's probably three or four years ago, maybe\nmore than four years ago,",
    "start": "4650719",
    "end": "4657040"
  },
  {
    "text": "maybe six years ago. At that time, many\nof the contest-- So if you look at the-- there's always some kind\nof forum for discussions,",
    "start": "4657040",
    "end": "4668930"
  },
  {
    "text": "discussing-- [SIDE CONVERSATIONS]",
    "start": "4668930",
    "end": "4681300"
  },
  {
    "text": "So in this Kaggle, I'm\nsure this happens more",
    "start": "4681300",
    "end": "4690570"
  },
  {
    "text": "in the industry, which\nI'm not familiar with. But in even the Kaggle context-- so in many of the\nKaggle contest,",
    "start": "4690570",
    "end": "4697800"
  },
  {
    "text": "if you look at the forum,\nalways after half a month, after a few weeks, someone\nwill figure out some leakage",
    "start": "4697800",
    "end": "4705590"
  },
  {
    "text": "just because some examples\nare very, very close to text example so that they\njust use this leakage to hack",
    "start": "4705590",
    "end": "4713869"
  },
  {
    "text": "the number. So it's kind of like\nsome kind of weird rule so that you can make the\nvalidation performance much better than you thought.",
    "start": "4713870",
    "end": "4720380"
  },
  {
    "text": "And everyone has to use that. And it's kind of interesting. I don't know why. Everyone who found this kind\nof leakage-- they always",
    "start": "4720380",
    "end": "4725630"
  },
  {
    "text": "post it in the forum somehow. And I don't know whether\nthis is always true. But for a few cases,\nI've seen they do this.",
    "start": "4725630",
    "end": "4733370"
  },
  {
    "text": "And then everyone\nelse, well, you have to use this small gadget to\nimprove their model performance because if you don't use it,\nyour model performance is just",
    "start": "4733370",
    "end": "4740020"
  },
  {
    "text": "not as good as others. So yeah, I don't know whether\nnow they have-- maybe they",
    "start": "4740020",
    "end": "4745469"
  },
  {
    "text": "have some better ways to\ndetect this leakage to design the competition much better.",
    "start": "4745469",
    "end": "4750540"
  },
  {
    "text": "I don't know. but this is something you have\nto pay attention in practice. And also, another\nkind of tricky thing",
    "start": "4750540",
    "end": "4756679"
  },
  {
    "text": "is that what is a good split. So we have discussed whether\nyou should do random split. So in research, as\nI said, random split",
    "start": "4756679",
    "end": "4763600"
  },
  {
    "text": "is pretty much the\nbest way you can do because you really literally\ncare about the validation performance.",
    "start": "4763600",
    "end": "4769610"
  },
  {
    "text": "But the problem\nis ",
    "start": "4769610",
    "end": "4776619"
  },
  {
    "text": "that sometimes, in the real world, the test\nset is actually not really",
    "start": "4776620",
    "end": "4782080"
  },
  {
    "text": "what you care about. So that's why when\nyou split it, you also want to split the training\nvalidation set in a way such",
    "start": "4782080",
    "end": "4789300"
  },
  {
    "text": "that the validation\nset is something closer to the real test set.",
    "start": "4789300",
    "end": "4794600"
  },
  {
    "text": "So I guess this\nis the situation. So suppose you are thinking\nabout stock price prediction,",
    "start": "4794600",
    "end": "4799780"
  },
  {
    "text": "right? So your final goal is to\npredict a price in the future.",
    "start": "4799780",
    "end": "4804908"
  },
  {
    "text": "So that's something that you\njust don't have at all, right. And so now, suppose you have\ndata between 2000 and 2020.",
    "start": "4804909",
    "end": "4815400"
  },
  {
    "text": "So you have these So how do you do the\ntwin validation split?",
    "start": "4815400",
    "end": "4821780"
  },
  {
    "text": "So you just do a\nrandom split or you should do some other things? So one possible option is\nthat you should probably",
    "start": "4821780",
    "end": "4829289"
  },
  {
    "text": "split into, for\nexample, 2000 to 2015. That's the training. And 2015 to 2020,\nthat's the validation.",
    "start": "4829290",
    "end": "4837090"
  },
  {
    "text": "Why you argue that, why\nthat's a possible option? Possibly just because\nthe last five years",
    "start": "4837090",
    "end": "4842280"
  },
  {
    "text": "is more predictive of the\nfuture than the earlier years.",
    "start": "4842280",
    "end": "4847760"
  },
  {
    "text": "So I'm not necessarily saying\nthat this is the only option or the best option.",
    "start": "4847760",
    "end": "4853330"
  },
  {
    "text": "But this is at least\nsomething to consider. So this is not what\nwe do in research.",
    "start": "4853330",
    "end": "4860380"
  },
  {
    "text": "And the reason is\njust because you care about the performance in\nthe future, which is something",
    "start": "4860380",
    "end": "4866369"
  },
  {
    "text": "you don't have access to. So I guess this is-- so the better split is\nthey use the first 50 days",
    "start": "4866370",
    "end": "4877560"
  },
  {
    "text": "to predict the last 50 days. And create a specification--",
    "start": "4877560",
    "end": "4884449"
  },
  {
    "text": "so I think this\nis mostly related to how do you define\nwhat you want to predict",
    "start": "4884449",
    "end": "4890270"
  },
  {
    "text": "and what's kind of the goal. So in many cases, you\ncan use machine model",
    "start": "4890270",
    "end": "4896050"
  },
  {
    "text": "in many different ways from\nwhat you wanted to use it.",
    "start": "4896050",
    "end": "4903110"
  },
  {
    "text": "And also, you care about\ndifferent perspectives. For example, what is the spam?",
    "start": "4903110",
    "end": "4909840"
  },
  {
    "text": "So the definition\nof spam sometimes are different between\ndifferent people.",
    "start": "4909840",
    "end": "4915679"
  },
  {
    "text": "So maybe do you think an\nad from, say, Google-- do you think that as a spam?",
    "start": "4915679",
    "end": "4924440"
  },
  {
    "text": "Maybe I will think like that,\nbut somebody else probably prefer to receive\nsome emails, some ad",
    "start": "4924440",
    "end": "4929620"
  },
  {
    "text": "emails with very low risk. So you have to specify exactly\nwhat you want to predict.",
    "start": "4929620",
    "end": "4934770"
  },
  {
    "text": "So what is really the\ndefinition of spam? And you don't want\nto have ambiguities at least from these\ndays perspective.",
    "start": "4934770",
    "end": "4942400"
  },
  {
    "text": "So machine learning models\ndon't like ambiguity. So you really want to\nhave a clear cut what",
    "start": "4942400",
    "end": "4947679"
  },
  {
    "text": "is a spam, what is not. And also, what\nlevel of expertise is required to understand\nit because if you",
    "start": "4947679",
    "end": "4954219"
  },
  {
    "text": "specify the spam, you\ncan have a definition. But if your labeler are not\nable to label the spam account,",
    "start": "4954219",
    "end": "4963120"
  },
  {
    "text": "your definition is not\ngoing to be useful. Suppose you have a very, very\ncomplex definition of spam. And then you say,\nI have this data",
    "start": "4963120",
    "end": "4969910"
  },
  {
    "text": "and I ask the labelers\nto label them. But the labelers cannot execute\nmy definition of spam easily.",
    "start": "4969910",
    "end": "4975780"
  },
  {
    "text": "That's going to\nbe another issue. And also, the specifications--",
    "start": "4975780",
    "end": "4982640"
  },
  {
    "text": "so you use the specification\nyou want because you want to-- you use the specification\nto define a set of examples",
    "start": "4982640",
    "end": "4991840"
  },
  {
    "text": "because eventually, if you just\nhave some kind of description, text description about what\nis a spam, that's probably not",
    "start": "4991840",
    "end": "4997520"
  },
  {
    "text": "useful. You have to really have\na set of test examples, and the test\nexamples have labels, and that is really your\ndefinition of spam.",
    "start": "4997520",
    "end": "5006969"
  },
  {
    "text": "And for example, one of\nthe quick and dirty test here is that whether your\ndefinition of the spam",
    "start": "5006969",
    "end": "5014120"
  },
  {
    "text": "can pass this so-called\ninner annotator agreement. So basically, what you\nsay is that you write down",
    "start": "5014120",
    "end": "5020449"
  },
  {
    "text": "some definition,\nand then you select some N randomly selected. You get some examples of emails.",
    "start": "5020449",
    "end": "5027850"
  },
  {
    "text": "And then you ask\nthree annotators to see whether they can agree\non which email is a spam",
    "start": "5027850",
    "end": "5034470"
  },
  {
    "text": "or not according to the\ndefinition you give them. And often, you don't really\nget that high of agreement.",
    "start": "5034470",
    "end": "5041140"
  },
  {
    "text": "You don't get 100% agreement. In many cases,\npeople's interpretation of the same definition\nwould be different.",
    "start": "5041140",
    "end": "5046840"
  },
  {
    "text": "And let's say you\nhave 95% agreement. I think that's already\nconsidered to be great.",
    "start": "5046840",
    "end": "5053349"
  },
  {
    "text": "And then you have the question\nbecomes whether it's",
    "start": "5053350",
    "end": "5060060"
  },
  {
    "text": "meaningful to shoot for\nsome accuracy more than 85%. If the annotators don't\nagree with each other,",
    "start": "5060060",
    "end": "5065448"
  },
  {
    "text": "only agree with each\nother 95% of time,",
    "start": "5065449",
    "end": "5072389"
  },
  {
    "text": "should your machine learning\nmodel do better than that? Actually, sometimes\nyou can do better than that just because\nhumans are sometimes",
    "start": "5072389",
    "end": "5078020"
  },
  {
    "text": "have less accurate\ninterpretation. Sometimes, machine learning\nmodels can do better.",
    "start": "5078020",
    "end": "5085719"
  },
  {
    "text": "But typically, you\nprobably shouldn't shoot for much higher\nthan 95% in many cases.",
    "start": "5085719",
    "end": "5094989"
  },
  {
    "text": "And then you're going\nto do this iteratively. For example, you have to kind\nof examine the specifications,",
    "start": "5094989",
    "end": "5099990"
  },
  {
    "text": "you're going to look at what's\nthe disagreement, why you have disagreement, maybe that\nmeans you have changed",
    "start": "5099990",
    "end": "5105760"
  },
  {
    "text": "your specification. And the last question\nis kind of interesting--",
    "start": "5105760",
    "end": "5110790"
  },
  {
    "text": "do you train the\npeople or the machine. Eventually, at sometime, if\nyou have a lot of different-- sometimes, you have to train the\npeople to label them correctly,",
    "start": "5110790",
    "end": "5118460"
  },
  {
    "text": "right? So for example, even the\nimage classification problem-- you have these clearly\ndefined labels.",
    "start": "5118460",
    "end": "5124699"
  },
  {
    "text": "So dog, cat-- but once you\ngo to the breeds of dogs,",
    "start": "5124699",
    "end": "5131450"
  },
  {
    "text": "some of the labelers\ndon't really recognize different\nbreeds of dogs. So you have to train the\nlabelers in some way.",
    "start": "5131450",
    "end": "5137210"
  },
  {
    "text": "So I have a friend\nwho did this in PhD. And basically, he has a\nlot of training documents",
    "start": "5137210",
    "end": "5142869"
  },
  {
    "text": "for the labelers, the\nAmazon Mechanical Turkers. And they have to actually ask\nthe Turkers to pass some exams",
    "start": "5142869",
    "end": "5150659"
  },
  {
    "text": "to be a labeler for them. So it's actually\nkind of complicated.",
    "start": "5150659",
    "end": "5157470"
  },
  {
    "text": "So I guess I'll be quick given\nthat we are almost running out of time. And then we'll do the model.",
    "start": "5157470",
    "end": "5162480"
  },
  {
    "text": "This is the more\nmachine learning part. So you want to implement\nthe simplest possible model",
    "start": "5162480",
    "end": "5169020"
  },
  {
    "text": "and you keep it simple. And sometimes, I\nthink this thing",
    "start": "5169020",
    "end": "5174050"
  },
  {
    "text": "don't get bogged\ndown in new models use them to understand data. So sometimes, the problem is\nthat the models are not only",
    "start": "5174050",
    "end": "5184100"
  },
  {
    "text": "the end goal. Sometimes you want\nto use the model to understand what are the\nproblems with the data.",
    "start": "5184100",
    "end": "5190350"
  },
  {
    "text": "And sometimes you\ncan fix the data, and then the performance\nbecomes much better.",
    "start": "5190350",
    "end": "5197158"
  },
  {
    "text": "So this is a whole loop. Your bottleneck may not just\nbe only about the model.",
    "start": "5197159",
    "end": "5203350"
  },
  {
    "text": "Sometimes it could come\nfrom some other places. Maybe a data, maybe\nthe specification, maybe the train set\nsplit, so on and so forth.",
    "start": "5203350",
    "end": "5211580"
  },
  {
    "text": "And you have some\nbaseline so that you know why you are doing. And you need to do some ablation\nstudies so and so forth.",
    "start": "5211580",
    "end": "5219969"
  },
  {
    "text": "And then step six, you\nneed to measure the output. And you have to measure\nthe output so that you",
    "start": "5219970",
    "end": "5227239"
  },
  {
    "text": "don't make mistakes twice. And you want to catch new\nmistakes as soon as possible.",
    "start": "5227239",
    "end": "5232580"
  },
  {
    "text": "And you want to measure\ndifferent things and simple things. For example, a\nbunch of quantities",
    "start": "5232580",
    "end": "5240929"
  },
  {
    "text": "you carry about here\nand so on and so forth.",
    "start": "5240929",
    "end": "5247159"
  },
  {
    "text": "And this is probably\none thing that-- is one challenge that we\nare really facing these days",
    "start": "5247159",
    "end": "5252780"
  },
  {
    "text": "about machine learning model. Maybe I would say probably\none of the most important challenges. So the reason is that you\nhave a distribution shift.",
    "start": "5252780",
    "end": "5259780"
  },
  {
    "text": "So your training and\nvalidation distribution are very different\nfrom the distribution that you will test eventually.",
    "start": "5259780",
    "end": "5266780"
  },
  {
    "text": "Or maybe they are\nsimilar, but there is some kind of\nspecial subpopulations that make them different.",
    "start": "5266780",
    "end": "5271929"
  },
  {
    "text": "So for example, you were\ntraining on San Francisco street views, and then you\ntest on Arizona street views.",
    "start": "5271929",
    "end": "5277490"
  },
  {
    "text": "For example, when you build\nan autonomous driving car, then you train on\nsome street views,",
    "start": "5277490",
    "end": "5282800"
  },
  {
    "text": "and you have to test\non some other places. And that creates a\ndistribution shift. And then you can have surprises.",
    "start": "5282800",
    "end": "5291060"
  },
  {
    "text": "And there are not so many\ngood ways to do this, except that you have\nto be careful about it",
    "start": "5291060",
    "end": "5297408"
  },
  {
    "text": "and design new algorithms. And this is incredibly hard\nwhere there's no real solutions",
    "start": "5297409",
    "end": "5304219"
  },
  {
    "text": "industry. I don't think there's real\nsolutions in research either.",
    "start": "5304219",
    "end": "5309239"
  },
  {
    "text": "So I think we're going\nto have a one guest lecture by James Zou about\nthe robustness of machine learning models.",
    "start": "5309239",
    "end": "5314949"
  },
  {
    "text": "There are a lot of\nrecent work on it. But so far, we don't have-- I think we have\ndefinitely better",
    "start": "5314949",
    "end": "5321130"
  },
  {
    "text": "algorithms to be more robust. But I think the performance\nis probably-- the robustness is still not ideal enough.",
    "start": "5321130",
    "end": "5330219"
  },
  {
    "text": "So I think I would just\njump to step 7 so you have",
    "start": "5330219",
    "end": "5337260"
  },
  {
    "text": "to repeat and look your data. I released the longer\nversion of the slides if you are interested\nin some of these things.",
    "start": "5337260",
    "end": "5343370"
  }
]