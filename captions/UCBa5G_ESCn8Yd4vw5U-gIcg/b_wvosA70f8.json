[
  {
    "start": "0",
    "end": "5910"
  },
  {
    "text": "All right. Come back. We're going to start lecture\n4 in reinforcement learning. So we're going to be\ncovering today Q-learning,",
    "start": "5910",
    "end": "13160"
  },
  {
    "text": "and we're going to\ncover Deep Q-learning. This result came\nout in roughly 2014. And I remember it being\na really big deal,",
    "start": "13160",
    "end": "19420"
  },
  {
    "text": "because one of the big\nconferences, Neural Information Processing Systems,\nDeepMind came and had this amazing\ndemonstration that they were",
    "start": "19420",
    "end": "26890"
  },
  {
    "text": "able to now have an agent\nthat could learn to play video games really well. And an important\nthing to note here",
    "start": "26890",
    "end": "33070"
  },
  {
    "text": "is like they are doing video\ngames from pixel input. So like they're just getting\nthe same input as what we do.",
    "start": "33070",
    "end": "38840"
  },
  {
    "text": "And what the agent\nwas learning to do is to control the game through\nthis and through reinforcement",
    "start": "38840",
    "end": "44470"
  },
  {
    "text": "learning. And so we'll talk today\nabout the algorithm that they did to do that, and\nwe'll build up to that point.",
    "start": "44470",
    "end": "50570"
  },
  {
    "text": "And this is a short video\nthey show to just illustrate how the agent is learning\nthrough direct experience",
    "start": "50570",
    "end": "58120"
  },
  {
    "text": "to try to optimize the score. And so what it\nlearns in this case is it starts to learn\nparticular strategies that",
    "start": "58120",
    "end": "63760"
  },
  {
    "text": "allow it to do really\nwell, which may or may not be the same ones as\nwhat humans would use.",
    "start": "63760",
    "end": "70460"
  },
  {
    "text": "And so it was pretty incredible. This was one of the sort of\nmost impressive successes of reinforcement\nlearning at this point,",
    "start": "70460",
    "end": "77960"
  },
  {
    "text": "particularly at trying to\ndo tasks that humans can do as well and from pixel inputs.",
    "start": "77960",
    "end": "83149"
  },
  {
    "text": "So we're going to see today\nhow that algorithm works. ",
    "start": "83150",
    "end": "88750"
  },
  {
    "text": "All right. But before we do that,\nlet's start with a quick, check your understanding. These are posted inside of Ed.",
    "start": "88750",
    "end": "94680"
  },
  {
    "text": "And this asks you to think about\nthe policy improvement stage. So we're going to\nbe talking today",
    "start": "94680",
    "end": "99830"
  },
  {
    "text": "a lot about learning through\ndirect experience and scaling up towards function\napproximation with doing that.",
    "start": "99830",
    "end": "105870"
  },
  {
    "text": "But first, let's\nthink about when we're doing this, what sort\nof form the policy has?",
    "start": "105870",
    "end": "112909"
  },
  {
    "text": "And then as we do\nthis evaluation-- we do this sort of repeated\nevaluation and policy improvement, what\nhappens in this case?",
    "start": "112910",
    "end": "120024"
  },
  {
    "start": "120025",
    "end": "127181"
  },
  {
    "text": "These are the first two\nquestions on the polls. ",
    "start": "127181",
    "end": "155000"
  },
  {
    "text": "Sorry, I just joined the class. The poll? It's on Ed. Yeah.",
    "start": "155000",
    "end": "160370"
  },
  {
    "text": "What's your name? Thanks, yeah. So if anybody is not-- is new\nto class, you can go to Ed.",
    "start": "160370",
    "end": "166058"
  },
  {
    "text": "You should be able to get\nto that through Canvas. ",
    "start": "166058",
    "end": "209550"
  },
  {
    "text": "All right. We have good agreement\non the first one. This policy is stochastic\nunder the assumption that for each state,\nthere's a unique max.",
    "start": "209550",
    "end": "217070"
  },
  {
    "text": "And it means that the new\npolicy will be deterministic. So almost I think, everybody\nsaid that correctly, which is great.",
    "start": "217070",
    "end": "223570"
  },
  {
    "text": "So now-- so this is the-- it's the answer for this.",
    "start": "223570",
    "end": "228890"
  },
  {
    "text": "But there's some disagreement\nabout the second one. So why don't you\nturn to a neighbor and compare what you\ngot for whether you",
    "start": "228890",
    "end": "235030"
  },
  {
    "text": "can compute Q pi i\nplus 1 by using this to generate new trajectories?",
    "start": "235030",
    "end": "241069"
  },
  {
    "start": "241070",
    "end": "265170"
  },
  {
    "text": "And remember, what\nI mean by this is I want to know whether or\nnot you can get the state action value for every state and action\npair under this new policy.",
    "start": "265170",
    "end": "273830"
  },
  {
    "text": "So I want to know if you\ncan compute Q of s, a under this new policy. ",
    "start": "273830",
    "end": "309139"
  },
  {
    "text": "So I'll give you a hint. If a policy is deterministic,\nhow many actions",
    "start": "309140",
    "end": "316323"
  },
  {
    "text": "does it take in the same state? ",
    "start": "316323",
    "end": "322319"
  },
  {
    "text": "What? Right. So are you going to get any\ndata about any other actions in that state?",
    "start": "322320",
    "end": "328350"
  },
  {
    "text": "So can we compute the Q value\nof all actions in that state?",
    "start": "328350",
    "end": "334160"
  },
  {
    "text": "No. That's right. Yeah. So this is false.",
    "start": "334160",
    "end": "339940"
  },
  {
    "text": "We can't compute it, because if\nwe have a deterministic policy, then we only ever take pi of s.",
    "start": "339940",
    "end": "347389"
  },
  {
    "text": "So we would only take\npi of i plus 1 of s. That would be the only action\nwe'd ever take in that state.",
    "start": "347390",
    "end": "355590"
  },
  {
    "text": "Because the policy\nis deterministic, it only takes that\none-- that one action. And so that means\nyou're just not going",
    "start": "355590",
    "end": "360810"
  },
  {
    "text": "to get any data about what\nit would be like to take other actions in that state.",
    "start": "360810",
    "end": "365880"
  },
  {
    "text": "And so it's useful\nto know, because it means that if we had\nmodels of the dynamics or if we had models\nof the reward",
    "start": "365880",
    "end": "372363"
  },
  {
    "text": "and we could do\nsome other things, then we might be able to\ncompute these Q values. But here, if we're going to\nstart thinking about just",
    "start": "372363",
    "end": "377940"
  },
  {
    "text": "learning this from data\nand from direct experience, that if we have a\ndeterministic policy, it's not going to\ngive us any data",
    "start": "377940",
    "end": "384090"
  },
  {
    "text": "about trying different\nactions in the same state. And so that's going to introduce\nsome important challenges",
    "start": "384090",
    "end": "390270"
  },
  {
    "text": "that we have to\ntackle when we're trying to get data\nabout the world in order to learn an\noptimal Q-function.",
    "start": "390270",
    "end": "396280"
  },
  {
    "start": "396280",
    "end": "402320"
  },
  {
    "text": "Great. So what we're going\nto be doing today then is try to think about\nbuilding on what we learned last time about\npolicy evaluation,",
    "start": "402320",
    "end": "408880"
  },
  {
    "text": "where we're trying to learn\ndirectly from experience, to be able to evaluate how good\na particular decision policy is.",
    "start": "408880",
    "end": "414400"
  },
  {
    "text": "How do we leverage\nthat information to then actually learn\nan optimal policy, to actually learn a\ngood decision, you know,",
    "start": "414400",
    "end": "420180"
  },
  {
    "text": "a good policy without having to\nmodel of how the world works? So we don't have access\nto an explicit parametric",
    "start": "420180",
    "end": "425850"
  },
  {
    "text": "representation of the dynamics\nmodel or the reward model. And then we're also going\nto talk about value function",
    "start": "425850",
    "end": "432340"
  },
  {
    "text": "approximation. And in particular, we're\ngoing to talk about Q-learning with deep neural\nnetworks, a.k.a.",
    "start": "432340",
    "end": "440729"
  },
  {
    "text": "DQN, which led to this\nreally seminal result in having machines that can\njust play directly from vision",
    "start": "440730",
    "end": "448110"
  },
  {
    "text": "to learn how to play\ngames like Atari. But I'll just pause\nhere in case anybody has any questions or\nlogistic questions",
    "start": "448110",
    "end": "453875"
  },
  {
    "text": "before we dive into this. ",
    "start": "453875",
    "end": "459180"
  },
  {
    "text": "All right. And we're going to cover a\nlot today, because next week, we're going to start\npolicy gradient methods.",
    "start": "459180",
    "end": "464707"
  },
  {
    "text": "And we're doing that,\nbecause we think that that's a really\nimportant thing to focus on. So-- but there will\nbe quite a lot today.",
    "start": "464707",
    "end": "472200"
  },
  {
    "text": "And you're welcome to reach out. I put a bunch of work\nexamples at the end, in case people want to\nstep through some of those",
    "start": "472200",
    "end": "477392"
  },
  {
    "text": "with Mars Rover and others. All right. So these are we're going to\nassess a bunch of things.",
    "start": "477392",
    "end": "482780"
  },
  {
    "text": "And we're going to\nstart by thinking about staying in\nthe tabular land, so staying where we can\nwrite down the value",
    "start": "482780",
    "end": "487940"
  },
  {
    "text": "function as a vector,\nand then trying to learn how to make optimal\ndecisions in that case.",
    "start": "487940",
    "end": "494169"
  },
  {
    "text": "So let's first just talk about\nthe idea of generalized policy improvement. So we've seen before\nthis idea of alternating",
    "start": "494170",
    "end": "501310"
  },
  {
    "text": "between policy evaluation\nand policy improvement. And now we're going to think\nabout that for slightly more",
    "start": "501310",
    "end": "506560"
  },
  {
    "text": "general cases of policies.  So what we just said here\nis that if the policy is",
    "start": "506560",
    "end": "513760"
  },
  {
    "text": "deterministic, we can't\ncompute the state action value for any action\nthat's not the policy. And so what we'd like\nto be able to do now",
    "start": "513760",
    "end": "521799"
  },
  {
    "text": "is to have kind\nof more coverage. And to do that, we're going\nto have stochastic policies. Because if the\npolicy is stochastic,",
    "start": "521799",
    "end": "528680"
  },
  {
    "text": "then we'll try multiple\nactions in the same state. And we can use that data\nto estimate the Q-function.",
    "start": "528680",
    "end": "534610"
  },
  {
    "text": "So we're staying in what we're\ncalling model-free policy iteration, meaning we're not\ntrying to explicitly build a dynamics or reward model.",
    "start": "534610",
    "end": "541300"
  },
  {
    "text": "We're just trying to directly\nestimate a Q-function. And once we have a\nQ-function, then we can extract from it an argmax\npolicy or something else.",
    "start": "541300",
    "end": "550230"
  },
  {
    "text": "Yeah. And we're now going to be using\nan estimated Q, because we",
    "start": "550230",
    "end": "557920"
  },
  {
    "text": "will be estimating Q from\ndata directly from experience.",
    "start": "557920",
    "end": "567440"
  },
  {
    "text": " All right. So this is going to introduce\nthis general challenge",
    "start": "567440",
    "end": "572567"
  },
  {
    "text": "of exploration, which is we can\nonly learn about the things we try in the world. So this is just like the-- can't\nknow how much better or worse",
    "start": "572567",
    "end": "579850"
  },
  {
    "text": "your life would be right now if\nyou're drinking coffee at Coupa. Same thing. Like we can only learn about\nthe actions that we take.",
    "start": "579850",
    "end": "586500"
  },
  {
    "text": "And so we need to learn\nabout actions by trying them. So we need to explore.",
    "start": "586500",
    "end": "592470"
  },
  {
    "text": "But the downside in general\nis if we try new actions, we are spending less\ntime using our knowledge",
    "start": "592470",
    "end": "597870"
  },
  {
    "text": "to make good decisions. So you might imagine that\nyou can act randomly always.",
    "start": "597870",
    "end": "604240"
  },
  {
    "text": "And that would work for like\nlearning a lot about the world and learning a lot\nabout Q-functions. But you wouldn't be finding--",
    "start": "604240",
    "end": "610717"
  },
  {
    "text": "you wouldn't be acting\nusing that knowledge to try to gain high reward. So this is known as the general\nchallenge between exploration",
    "start": "610718",
    "end": "617430"
  },
  {
    "text": "and exploitation. How much time do we spend\nexploring and getting",
    "start": "617430",
    "end": "622590"
  },
  {
    "text": "new data about things\nthat might be good? Versus how much of the time do\nwe exploit our knowledge of how",
    "start": "622590",
    "end": "628320"
  },
  {
    "text": "the world works, according\nto the data we have so far, to try to make good decisions? And this will come up a lot.",
    "start": "628320",
    "end": "634850"
  },
  {
    "text": "There's really deep questions\naround here about thinking of, how do we quantify our\nuncertainty in our knowledge?",
    "start": "634850",
    "end": "641480"
  },
  {
    "text": "And then how do we\npropagate that uncertainty into the value of\nthat uncertainty for downstream decision making?",
    "start": "641480",
    "end": "646838"
  },
  {
    "text": "So we'll see a lot more about\nthat later in the course. And this continues to be a\nreally active area of research.",
    "start": "646838",
    "end": "652340"
  },
  {
    "text": "This is not at all solved. But here, we're\njust going to start to see some simple\nmethods to try to tackle this challenge of\nbalancing between these two",
    "start": "652340",
    "end": "660640"
  },
  {
    "text": "things. So one of the simplest things\nyou could imagine doing is what's called epsilon greedy.",
    "start": "660640",
    "end": "668130"
  },
  {
    "text": "And the idea with\nepsilon greedy is you're going to just spend\nsome of the time doing things randomly and some of the\ntimes, doing things the best",
    "start": "668130",
    "end": "674250"
  },
  {
    "text": "way you know how,\nbecause you're kind of exploiting that knowledge. So if we just have\na finite number",
    "start": "674250",
    "end": "680160"
  },
  {
    "text": "of actions, because right now,\nwe're still in the tabular case. So we just have a\nfinite number of states and a finite number of actions.",
    "start": "680160",
    "end": "686170"
  },
  {
    "text": "Then epsilon greedy policy\nsays, with high probability--",
    "start": "686170",
    "end": "691639"
  },
  {
    "text": "so we have some epsilon here. Epsilon is going\nto be less than 1. It could be like probability--\nit could be 0.1, for example.",
    "start": "691640",
    "end": "697680"
  },
  {
    "text": "So with high\nprobability, you're going to take whatever\naction maximizes your Q value in your current state.",
    "start": "697680",
    "end": "704652"
  },
  {
    "text": "So you're going\nto kind of exploit your knowledge for whatever\nyour state action value says. And you're going to do that with\nprobability 1 minus epsilon.",
    "start": "704653",
    "end": "711980"
  },
  {
    "text": "And then otherwise, you're going\nto take an action at random. And so when you pick an\naction uniformly at random,",
    "start": "711980",
    "end": "718839"
  },
  {
    "text": "it might be one of-- the\nsame one as the argmax, or it might be a different one. But either way, the main\nidea is that essentially, you",
    "start": "718840",
    "end": "724890"
  },
  {
    "text": "spend 1 minus epsilon\npercentage of the time being greedy with\nrespect to your knowledge",
    "start": "724890",
    "end": "731130"
  },
  {
    "text": "and epsilon percent spend\ntime acting randomly. ",
    "start": "731130",
    "end": "736710"
  },
  {
    "text": "So it's like maybe\nyou say, OK, I'm committed to trying out new\nthings at my restaurant. So once a week, I will\ntry a random dish.",
    "start": "736710",
    "end": "742740"
  },
  {
    "text": "And the other six days,\nI'll pick whatever I like-- like whatever I've liked in the\npast, and it's always been good.",
    "start": "742740",
    "end": "749509"
  },
  {
    "text": "So this is a pretty\nsimple strategy. This is not trying to have\na deep notion of uncertainty or trying to quantify that.",
    "start": "749510",
    "end": "755730"
  },
  {
    "text": "But nevertheless, this\ncan be pretty effective. So in particular, we can prove\nthings about policy improvement",
    "start": "755730",
    "end": "763970"
  },
  {
    "text": "with epsilon greedy policies. So what we proved in\nthe past is that if you do policy iteration, when you\nknow the dynamics and reward",
    "start": "763970",
    "end": "771500"
  },
  {
    "text": "models, you are guaranteed\nto monotonically improve. So each round of\npolicy iteration,",
    "start": "771500",
    "end": "776940"
  },
  {
    "text": "either you would stay the\nsame, in which case, you'd found the optimal policy. Or you wouldn't change\nit, and in that case--",
    "start": "776940",
    "end": "784340"
  },
  {
    "text": "or you would improve. But when we did that proof,\nwe assumed policy improvement",
    "start": "784340",
    "end": "789410"
  },
  {
    "text": "using a deterministic policy. And it turns out the\nsame property holds with epsilon greedy policies.",
    "start": "789410",
    "end": "796860"
  },
  {
    "text": "So if your policy is always\nlike an epsilon greedy policy, you can also get this kind\nof monotonic improvement.",
    "start": "796860",
    "end": "802750"
  },
  {
    "text": " So in particular,\nand I'm not going to do the full proof today, but\nI'll leave it in just for time.",
    "start": "802750",
    "end": "811030"
  },
  {
    "text": "But what this shows\nhere is imagine that you have a Q-function,\nyou have some policy pi i.",
    "start": "811030",
    "end": "816241"
  },
  {
    "text": "And you have a Q-function,\nwhich tells you the state action value for that policy pi i. And pi is e-greedy, which\nmeans some of the time,",
    "start": "816242",
    "end": "825540"
  },
  {
    "text": "it acts greedily with\nrespect to that Q-function. And some of the time, it\nselects an action at random.",
    "start": "825540",
    "end": "831210"
  },
  {
    "text": "So that's what it means to be\nan e-greedy policy with respect to that Q. It's making\nthose decisions when",
    "start": "831210",
    "end": "837690"
  },
  {
    "text": "it's being greedy with\nrespect to that Q-function. So what this says\nis that pi i plus 1",
    "start": "837690",
    "end": "843090"
  },
  {
    "text": "is a monotonic improvement,\nso that V pi i plus 1 is greater than V pi i.",
    "start": "843090",
    "end": "850250"
  },
  {
    "text": "And we can prove this here. So essentially, we're\ntrying to prove in this case that the new policy that you\nextract through doing policy",
    "start": "850250",
    "end": "857120"
  },
  {
    "text": "improvement, which is\nstill an e-greedy policy, is going to be better than\nyour old e-greedy policy.",
    "start": "857120",
    "end": "864830"
  },
  {
    "text": "And the main idea is just\nto say you can kind of also do policy improvement\nwhen you don't have deterministic policies,\nbut you have these kind",
    "start": "864830",
    "end": "871250"
  },
  {
    "text": "of e-greedy policies. And you could still get\nmonotonic improvement.",
    "start": "871250",
    "end": "877550"
  },
  {
    "text": "And I'll leave that-- I'll put that at the end\nfor later post proof.",
    "start": "877550",
    "end": "883740"
  },
  {
    "text": "So this is just to\nhighlight like, here's one thing we could do. And we're going to see\nthat this is actually going to be a pretty\nhelpful thing to do.",
    "start": "883740",
    "end": "889618"
  },
  {
    "text": "This is one thing we could\ndo to try to get data about other actions. So we're not just taking a\nsingle action in a single state,",
    "start": "889618",
    "end": "896813"
  },
  {
    "text": "but we actually have some\nprobability of drawing out multiple actions. And just to make that\nconcrete, if you think back",
    "start": "896813",
    "end": "902820"
  },
  {
    "text": "to our Mars Rover example,\nthere are only seven states. So if you act in\nit for a long time, you'd repeatedly\nreach the same states.",
    "start": "902820",
    "end": "909550"
  },
  {
    "text": "What this e-greedy\npolicy is doing is saying like, even when\nyou get to the same state, you might take\ndifferent actions.",
    "start": "909550",
    "end": "914649"
  },
  {
    "text": "So over time,\nyou're going to get data that allows you\nto estimate the Q value of that whole policy.",
    "start": "914650",
    "end": "920871"
  },
  {
    "text": "So now we're going\nto see is how we can use these ideas of e-greedy\npolicies to actually do control.",
    "start": "920872",
    "end": "927020"
  },
  {
    "text": "So what I mean by\nthat is that we're going to try to learn\noptimal ways of acting in the environment. And we're going to start--\nwe're going to have",
    "start": "927020",
    "end": "933826"
  },
  {
    "text": "the same scenario as last time. So we're going to either have\nMonte Carlo approaches, where we simulate in the\nworld, and then we",
    "start": "933827",
    "end": "939370"
  },
  {
    "text": "use that to try to improve, or\ntemporal difference approaches, which more directly try to\nuse the Bellman and Markov",
    "start": "939370",
    "end": "945280"
  },
  {
    "text": "structure. OK, so let's start\nwith Monte Carlo.",
    "start": "945280",
    "end": "951710"
  },
  {
    "text": "So remember, what we had before. We used to have this Monte\nCarlo policy evaluation algorithm, where we\nwould repeatedly loop,",
    "start": "951710",
    "end": "959580"
  },
  {
    "text": "we would sample the kth episode. So we'd just like sample a\nseries of states and actions",
    "start": "959580",
    "end": "965180"
  },
  {
    "text": "under a particular policy. OK. And then you could compute\nthe return from each step",
    "start": "965180",
    "end": "972800"
  },
  {
    "text": "till the end of the episode. And then what you would do\nis you would-- bless you-- you would update--",
    "start": "972800",
    "end": "977899"
  },
  {
    "text": "for the first time you visited\na particular state action tuple, you would update the Q\nvalue by a weighted average",
    "start": "977900",
    "end": "984110"
  },
  {
    "text": "between your old estimate\nand then your new target,",
    "start": "984110",
    "end": "989149"
  },
  {
    "text": "which was just\nthe sum of rewards you got starting in\nthat state and action till the end of the episode.",
    "start": "989150",
    "end": "995120"
  },
  {
    "text": "OK. So this is where we often\ncall it like our target. ",
    "start": "995120",
    "end": "1001360"
  },
  {
    "text": "And we were using that, because\nwe knew from Monte Carlo that what we want\nto do is really",
    "start": "1001360",
    "end": "1007900"
  },
  {
    "text": "estimate the value of\nstarting in this state, taking this action,\nand following this policy to the end--",
    "start": "1007900",
    "end": "1013420"
  },
  {
    "text": "to the end of the episode, that\nwe can get a sample of that by doing this. And this sample is an\nunbiased approximation",
    "start": "1013420",
    "end": "1022600"
  },
  {
    "text": "to the true expected\nsum of rewards you would get starting\nin this state and action and going till the\nend of the episode.",
    "start": "1022600",
    "end": "1029199"
  },
  {
    "text": "Yeah? We can apply\nepsilon-greedy [INAUDIBLE]? We're going to see that. Yes, exactly.",
    "start": "1029200",
    "end": "1034670"
  },
  {
    "text": "Yeah. So when we thought\nabout this before, we thought the policy was\nlike a deterministic policy, or that was the easiest\nway to think about it.",
    "start": "1034670",
    "end": "1040849"
  },
  {
    "text": "But now the policy\ncould be stochastic, and so it could be e-greedy. Yeah, great question.",
    "start": "1040849",
    "end": "1046010"
  },
  {
    "text": "OK, so now, this policy-- good. Well, here, we'll go\non to the next one.",
    "start": "1046010",
    "end": "1053590"
  },
  {
    "text": "OK, so this was Monte\nCarlo policy evaluation.",
    "start": "1053590",
    "end": "1059289"
  },
  {
    "text": "Now, what we could try to do\nis Monte Carlo online control. So what I'm going\nto do here is I'm",
    "start": "1059290",
    "end": "1065320"
  },
  {
    "text": "going to introduce a different--\nan additional line here at the bottom, which says\nafter I do an episode,",
    "start": "1065320",
    "end": "1071510"
  },
  {
    "text": "I'm going to potentially\nchange my policy. So you can think of this as\nlike my policy evaluation part.",
    "start": "1071510",
    "end": "1077090"
  },
  {
    "text": "And this is my\npolicy improvement. And again, I'll just\nwrite out what this means. So what this means is that\nfor each state, for each s,",
    "start": "1077090",
    "end": "1089110"
  },
  {
    "text": "the policy for s is going to\nbe equal to argmax Q of s,",
    "start": "1089110",
    "end": "1096934"
  },
  {
    "text": "a with probability 1 minus\nepsilon, else random.",
    "start": "1096935",
    "end": "1109340"
  },
  {
    "text": "So that's what I\nmean by say, we're doing the policy improvement\nstep is we take our Q-function. We say either you would\ntake the argmax action,",
    "start": "1109340",
    "end": "1117230"
  },
  {
    "text": "or you would act randomly. Sorry. What are we looping over\nin the outermost loop?",
    "start": "1117230",
    "end": "1124220"
  },
  {
    "text": "Is it a k or? Yeah, this would be--\nyes, this would be k. Yeah. So this is just-- you can\nthink of the loop here.",
    "start": "1124220",
    "end": "1130912"
  },
  {
    "text": "And I'll write that down. Loop over the episodes.",
    "start": "1130912",
    "end": "1135950"
  },
  {
    "text": "So it's like I play\none game of Atari. And then I update my\npolicy evaluation. And maybe I change my policy.",
    "start": "1135950",
    "end": "1141440"
  },
  {
    "text": "And then I do another\nround of Atari. So I like play Breakout a\nmillion times, sometimes more",
    "start": "1141440",
    "end": "1148010"
  },
  {
    "text": "than that in some\nof these cases. Yeah. Ana, right? Is it-- Yeah. Yeah.",
    "start": "1148010",
    "end": "1154130"
  },
  {
    "text": "I'm getting confused\nby this last line that you have out there. So, I mean, isn't it implicit\nthat you are using the new--",
    "start": "1154130",
    "end": "1163460"
  },
  {
    "text": "you're using a new Q in the-- let's say, you're done\nwith iteration number k.",
    "start": "1163460",
    "end": "1168690"
  },
  {
    "text": "You moved on to iteration\nnumber k plus 1. When you're sampling\nthe next episode, you're using the\nupdated Q, right?",
    "start": "1168690",
    "end": "1177095"
  },
  {
    "text": "Great question. OK. So maybe I should--\nso what this says here is that initially, you\nconstruct-- so your Q initially",
    "start": "1177095",
    "end": "1184190"
  },
  {
    "text": "is 0 everywhere. You could initialize\nin some ways, but your queue is 0 everywhere. And you're going to\nselect something that's e-greedy with respect to that.",
    "start": "1184190",
    "end": "1190169"
  },
  {
    "text": "Now, if your Q value\nis 0 everywhere, it means that all of\nyour actions are tied. You have no information.",
    "start": "1190170",
    "end": "1195340"
  },
  {
    "text": "You basically are\njust acting randomly. What this says is\nthat the way we act is always with respect\nto our current policy.",
    "start": "1195340",
    "end": "1202419"
  },
  {
    "text": "So the first time-- or you can\nthink of as like motor babbling. Like your agent will just\nrandomly press buttons.",
    "start": "1202420",
    "end": "1207760"
  },
  {
    "text": "It'll move over the screen. It'll do that till it\nwins or loses the game. And then it will\nupdate its Q value.",
    "start": "1207760",
    "end": "1215300"
  },
  {
    "text": "And what this is saying is\nthat the next time, you're going to change what that policy\nis that you're using to act.",
    "start": "1215300",
    "end": "1222497"
  },
  {
    "text": "So hopefully, it won't\nbabble quite as much. It's just like, oh, well,\nsometimes I hit something and then I got an\nincrease in the points. So maybe I'll try to\ndo that action again.",
    "start": "1222497",
    "end": "1229380"
  },
  {
    "text": " Is the policy guarantee to\none of the new [INAUDIBLE]?",
    "start": "1229380",
    "end": "1236970"
  },
  {
    "text": "Great question. OK. So I've not said\nanything about that yet. I haven't said anything about\nwhat the properties are of this. Guy in the back.",
    "start": "1236970",
    "end": "1242467"
  },
  {
    "text": "I can't remember your name. Is it required to\ndo this on-policy? Or could you do\nthis off-policy that",
    "start": "1242467",
    "end": "1247470"
  },
  {
    "text": "collects a number of\ndemonstrations in an update later? Great question. Yes, you can definitely\ndo off-policy. And we'll see that in\na couple of slides.",
    "start": "1247470",
    "end": "1253678"
  },
  {
    "text": "Yeah. OK. Any questions? These are all great. OK, so you should be skeptical\nthat this is necessarily going",
    "start": "1253678",
    "end": "1260190"
  },
  {
    "text": "to do anything reasonable. But it's certainly\nsomething you could run, like something that you\ncould write down in a computer.",
    "start": "1260190",
    "end": "1265559"
  },
  {
    "text": "So this is a process. So then, a question would be-- and I put some-- this is\nan optional worked example.",
    "start": "1265560",
    "end": "1273045"
  },
  {
    "text": "You can go through it\njust to think about how it would actually update these. So some important properties\nare, how expensive is this?",
    "start": "1273045",
    "end": "1281140"
  },
  {
    "text": "Does it converge to\nthe optimal Q star as well as what is its\nempirical performance?",
    "start": "1281140",
    "end": "1288750"
  },
  {
    "text": "Let's think first whether or\nnot we think this is a good idea and whether or not we think\nthat this procedure here",
    "start": "1288750",
    "end": "1295350"
  },
  {
    "text": "is guaranteed to become a good\nestimate of the optimal Q star.",
    "start": "1295350",
    "end": "1302630"
  },
  {
    "text": "So this is another check\nyour understanding. It's on Ed. But what I would like\nyou to think about here",
    "start": "1302630",
    "end": "1309630"
  },
  {
    "text": "is that given the process\nI've just shown you here, do you think that the\nQ value we're computing",
    "start": "1309630",
    "end": "1315779"
  },
  {
    "text": "is an estimate of\nthe current policy? And do you think it will\nultimately become Q star?",
    "start": "1315780",
    "end": "1323818"
  },
  {
    "text": "And if you think it might or\nmight not under some conditions, that's fine too. You can put that in there.",
    "start": "1323818",
    "end": "1329080"
  },
  {
    "text": "Yeah. k-- like k changes\nwith these two points?",
    "start": "1329080",
    "end": "1334360"
  },
  {
    "text": "That's right. ",
    "start": "1334360",
    "end": "1354313"
  },
  {
    "text": "I was a bit confused\nabout [? innovation. ?] What is Q pi k again? Q pi k is the true\nstate action value",
    "start": "1354313",
    "end": "1362230"
  },
  {
    "text": "function for the pi k policy. So what is the expected\ndiscounted sum of rewards?",
    "start": "1362230",
    "end": "1368160"
  },
  {
    "text": "If you start in state s, take\naction a, and then follow pi k. Yeah, thanks for\nthe clarification.",
    "start": "1368160",
    "end": "1374473"
  },
  {
    "start": "1374473",
    "end": "1390250"
  },
  {
    "text": "Why do you vary the-- I don't remember your name. Why do you vary the epsilon?",
    "start": "1390250",
    "end": "1396915"
  },
  {
    "text": "I have not said anything about\nwhether I am varying the-- [INAUDIBLE]",
    "start": "1396915",
    "end": "1402140"
  },
  {
    "text": "You're setting a different-- Oh, yeah, yeah. Here I am. Yes. I know what you're\ntalking about. Why are we doing that? We'll talk about that.",
    "start": "1402140",
    "end": "1407377"
  },
  {
    "text": " I forgot that I had\nalready put that in there. Yeah, we can talk about it.",
    "start": "1407377",
    "end": "1413030"
  },
  {
    "start": "1413030",
    "end": "1422320"
  },
  {
    "text": "OK. And one thing just\nto note here-- and I think this\nis a question 2. So just to be clear here, as\nyou're thinking about this,",
    "start": "1422320",
    "end": "1430140"
  },
  {
    "text": "so this is like an approximation\nof policy iteration. So we're kind of doing\npolicy evaluation and then",
    "start": "1430140",
    "end": "1435480"
  },
  {
    "text": "policy improvement. But it's helpful to think about\nkind of how much time we're",
    "start": "1435480",
    "end": "1441360"
  },
  {
    "text": "spending doing policy\nimprovement versus policy evaluation. So what this is saying\nhere is that, you're",
    "start": "1441360",
    "end": "1447390"
  },
  {
    "text": "going to sample one episode. And then you're going\nto do policy evaluation.",
    "start": "1447390",
    "end": "1453695"
  },
  {
    "text": "OK. This is all just one episode. So it's like I'm\ngoing to play one--",
    "start": "1453695",
    "end": "1458980"
  },
  {
    "text": "I'm going to play until I\nwin or lose at Breakout once under a particular policy.",
    "start": "1458980",
    "end": "1465960"
  },
  {
    "text": "And then I'm going\nto change my policy. And then I'm going to play\nwith my new policy once.",
    "start": "1465960",
    "end": "1471539"
  },
  {
    "text": "And then I'm going\nto change my policy. And some of those games might\nbe really long, or some of them",
    "start": "1471540",
    "end": "1478050"
  },
  {
    "text": "might be really short. Yeah. So this is-- just\nto clarify, this is like representing\nwhat you do after playing",
    "start": "1478050",
    "end": "1484990"
  },
  {
    "text": "one game in a breakout? So then I think I might just\nbe confused, like, [INAUDIBLE].",
    "start": "1484990",
    "end": "1492920"
  },
  {
    "text": "When they're just like--\nif I'm playing a game, the episodes just follow\none after the other.",
    "start": "1492920",
    "end": "1498110"
  },
  {
    "text": "So isn't there just--\nthere's just one kth episode? There's one kth episode.",
    "start": "1498110",
    "end": "1503120"
  },
  {
    "text": "Yeah so like k is like\nI play-- so if k is 1, I'm going to play my first game. And I'm going to play\nit until I win or lose",
    "start": "1503120",
    "end": "1509893"
  },
  {
    "text": "or until the game ends. So maybe Breakout finishes. Or maybe I'm playing Tetris,\nand like I fail, and I die.",
    "start": "1509893",
    "end": "1515650"
  },
  {
    "text": "And that is one episode. And I'm going to use that to\nthen update my Q-function. Then I'm going to change it and\nsay, OK, well, my next round,",
    "start": "1515650",
    "end": "1521800"
  },
  {
    "text": "I'm going to play differently. And then I play Tetris\nagain until I fail. And then I see what\nthe total points are.",
    "start": "1521800",
    "end": "1528860"
  },
  {
    "text": "I update my Q-function,\nand I repeat. And some of those episodes\nmight be really short. So maybe the first\ntime-- particularly",
    "start": "1528860",
    "end": "1534645"
  },
  {
    "text": "for these agents, the first\ntime they play Tetris, maybe they lose in like 10\nsteps, might be a really short",
    "start": "1534645",
    "end": "1539890"
  },
  {
    "text": "10 step. Later, maybe they\nplay for a long time. But in general, I've\nnot told you anything about how long\nthese episodes are.",
    "start": "1539890",
    "end": "1546470"
  },
  {
    "text": "They might be really short. Or they might be really long. ",
    "start": "1546470",
    "end": "1552620"
  },
  {
    "text": "OK. And I think one\nuseful way I find to think about this\nis that think about",
    "start": "1552620",
    "end": "1558170"
  },
  {
    "text": "if they're really short,\nlike really, really short. Like I take two steps,\nand I just fail. I did something really dumb.",
    "start": "1558170",
    "end": "1563440"
  },
  {
    "text": "So in that case, think\nabout whether your Q would be a good estimate of Q pi k.",
    "start": "1563440",
    "end": "1571750"
  },
  {
    "text": "Like, would it be good if\nyou've only seen two states? Or would it be pretty bad? ",
    "start": "1571750",
    "end": "1580950"
  },
  {
    "text": "So why don't you turn\nto someone near you? I think most people\nhave voted if you have written something you have. But why don't you check\nand see what you think.",
    "start": "1580950",
    "end": "1587218"
  },
  {
    "start": "1587218",
    "end": "1683672"
  },
  {
    "text": "[INTERPOSING VOICES] ",
    "start": "1683672",
    "end": "1809769"
  },
  {
    "text": "OK. Awesome. I'm hearing a lot of\nreally good discussion. But I'm going to interrupt\nyou, because I want to make sure we get to DQN.",
    "start": "1809770",
    "end": "1816049"
  },
  {
    "text": "So this is where-- so one of the reasons that I\nbring up this particular example is that here, it's tabular.",
    "start": "1816050",
    "end": "1822205"
  },
  {
    "text": "I mean, these are a\nlittle bit smaller, so it's a bit easier to see. But essentially, what I kind\nof want you guys to get out",
    "start": "1822205",
    "end": "1827363"
  },
  {
    "text": "of today is that it\nshould be sort of shocking that reinforcement\nlearning works. [CHUCKLES] And we're not going\nto have time to go",
    "start": "1827363",
    "end": "1833929"
  },
  {
    "text": "through all the deep\nmathematical reasons for why it does work sometimes\nin this class. But I'm happy to\ngive people pointers.",
    "start": "1833930",
    "end": "1839360"
  },
  {
    "text": "But so there's several things\nthat are really kind of odd, if you start to think about\nthis, when you go through this.",
    "start": "1839360",
    "end": "1844740"
  },
  {
    "text": "So first of all, Q is not\nan estimate of Q pi k. It is not, because\nit is averaging",
    "start": "1844740",
    "end": "1850520"
  },
  {
    "text": "over policies that are\nchanging every episode or potentially changing\nevery episode, right?",
    "start": "1850520",
    "end": "1856230"
  },
  {
    "text": "Because in fact, in\ngeneral, it will be, right, because we're decaying epsilon.",
    "start": "1856230",
    "end": "1861620"
  },
  {
    "text": "So we're changing\nepsilon each round, which means we're making things\nmore and more deterministic. But in addition to that,\nour Q might be changing.",
    "start": "1861620",
    "end": "1868539"
  },
  {
    "text": "So essentially, I'm just\ntrying a policy one round. And then I update my Q, and\nthen I try something again.",
    "start": "1868540",
    "end": "1876780"
  },
  {
    "text": "And sort of extreme\nexample of this would be like flipping a coin\nonce and deciding whether-- what its bias is or\nsomething like that.",
    "start": "1876780",
    "end": "1883630"
  },
  {
    "text": "That's just not very much\ndata to do this evaluation. And also, you're\naveraging this over many, many different policies.",
    "start": "1883630",
    "end": "1890580"
  },
  {
    "text": "So Q is not an\nestimate of Q pi k. It's this weird weighted average\nof all the previous data and all",
    "start": "1890580",
    "end": "1896549"
  },
  {
    "text": "the policies you've done before. Q should not be an estimate\nof like the [? big ?] changes",
    "start": "1896550",
    "end": "1902995"
  },
  {
    "text": "in the latest k, k minus 1? Well, but not really. Because I mean you've\naveraged in that part.",
    "start": "1902995",
    "end": "1910070"
  },
  {
    "text": "That part's from pi k\nplus 1, but this old thing was over all of your-- is like\nthis weird weighted average of all the other\npolicies you've tried.",
    "start": "1910070",
    "end": "1917030"
  },
  {
    "text": "So yes, it is that,\nbut also like that plus all the other policies. So it's this weird thing, right?",
    "start": "1917030",
    "end": "1924890"
  },
  {
    "text": "The second thing is\nthat we're only doing-- and I was talking to\nsome people about this. We're only doing one rollout\nto try to evaluate a policy.",
    "start": "1924890",
    "end": "1933013"
  },
  {
    "text": "And you might imagine there's\na lot of stochasticity like even in something\nlike some games, there's like random rolls of the\ndice and stuff like that, which",
    "start": "1933013",
    "end": "1940040"
  },
  {
    "text": "means even with\nthe same strategy, you might get different\noutcomes each time. So it'd be like if you drove\nto SF, and you did it once.",
    "start": "1940040",
    "end": "1947090"
  },
  {
    "text": "And there was no traffic. And so you're like,\nI can always get to SF in like, I don't know,\n20 minutes on the highway.",
    "start": "1947090",
    "end": "1953160"
  },
  {
    "text": "But for those of you\nthat drive to SF, you would know that often,\nthere's lots of traffic. And so you would\nneed to average over",
    "start": "1953160",
    "end": "1958309"
  },
  {
    "text": "many rounds of doing this to see\nhow good a particular route is. So the weird thing\nhere is that we're just",
    "start": "1958310",
    "end": "1964950"
  },
  {
    "text": "doing kind of one rollout. We're averaging into this\nweird Q thing, which is now going to be this\nweighted average of all",
    "start": "1964950",
    "end": "1971460"
  },
  {
    "text": "the policies we've done. And we have this\nweird epsilon thing. And it should not be clear\nyet that we will necessarily",
    "start": "1971460",
    "end": "1979620"
  },
  {
    "text": "converge to Q star. Like we are getting more\nand more deterministic over time, because\nwe're reducing epsilon.",
    "start": "1979620",
    "end": "1986370"
  },
  {
    "text": "So we're reducing epsilon\nhere towards zero. Eventually, we're going to\nconverge towards something deterministic.",
    "start": "1986370",
    "end": "1992010"
  },
  {
    "text": "But you may or may\nnot be convinced yet that the thing we're going to\nconverge to is actually Q star. ",
    "start": "1992010",
    "end": "1999630"
  },
  {
    "text": "So fortunately, there are\nsome sufficient conditions under which we can guarantee\nthat this sort of thing",
    "start": "1999630",
    "end": "2006260"
  },
  {
    "text": "will converge to Q star. And really, it's quite\nbeautiful that this works.",
    "start": "2006260",
    "end": "2011340"
  },
  {
    "text": "So one is what's called\ngreedy in the limit of infinite exploration or GLIE.",
    "start": "2011340",
    "end": "2017480"
  },
  {
    "text": "So the idea in this case is that\nif you can ensure that all state action pairs are visited an\ninfinite number of times,",
    "start": "2017480",
    "end": "2024230"
  },
  {
    "text": "meaning the number\nof counts that you have for a particular\nstate and action pair goes to infinity for\nall states and actions--",
    "start": "2024230",
    "end": "2033440"
  },
  {
    "text": "this is for all. ",
    "start": "2033440",
    "end": "2039080"
  },
  {
    "text": "And the behavior policy. And what I mean by the\nbehavior policy is, this is the policy\nyou're actually using",
    "start": "2039080",
    "end": "2044417"
  },
  {
    "text": "to make decisions in the world. And it will be important. There'll be distinctions\nbetween this and other policies",
    "start": "2044417",
    "end": "2050113"
  },
  {
    "text": "soon, which is why we\ncall this behavior policy. If the behavior-- so if\nyou sample state action",
    "start": "2050113",
    "end": "2055760"
  },
  {
    "text": "pairs an infinite number of\ntimes and your behavior policy converges to the greedy policy,\nwhich means that asymptotically,",
    "start": "2055760",
    "end": "2063110"
  },
  {
    "text": "the action you select\nin a state is exactly equal to the argmax of your\nQ-function with probability 1.",
    "start": "2063110",
    "end": "2070158"
  },
  {
    "text": "So you're just getting more\nand more deterministic. So then you were being\ngreedy in the limit of infinite\nexploration that says",
    "start": "2070159",
    "end": "2077260"
  },
  {
    "text": "that you're exploring everything\nan infinite number of times. You're always continuing to\ntry all actions in all states.",
    "start": "2077260",
    "end": "2084530"
  },
  {
    "text": "But you're getting more\nand more deterministic. So this is what it\nmeans to be GLIE.",
    "start": "2084530",
    "end": "2089850"
  },
  {
    "text": "If you have a GLIE algorithm-- and I'll just note\nhere, like a simple way",
    "start": "2089850",
    "end": "2096638"
  },
  {
    "text": "to do this is to do e-greedy,\nwhere epsilon is reduced to 0 at the following rate.",
    "start": "2096639",
    "end": "2103170"
  },
  {
    "text": " Yeah, so we'd have this--",
    "start": "2103170",
    "end": "2108440"
  },
  {
    "text": "so that's a simple one. And visit all states.",
    "start": "2108440",
    "end": "2116665"
  },
  {
    "text": " And that should\nhold-- as long as you",
    "start": "2116665",
    "end": "2123069"
  },
  {
    "text": "have an e-greedy\nstrategy, then you will be able to visit\nall states and actions. So you're going to be visiting\nall states and actions",
    "start": "2123070",
    "end": "2129580"
  },
  {
    "text": "under this GLIE strategy. Then under that, the\nMonte Carlo algorithm",
    "start": "2129580",
    "end": "2135100"
  },
  {
    "text": "I just showed you for\ntabular representations will converge to Q\nstar, which means",
    "start": "2135100",
    "end": "2142150"
  },
  {
    "text": "as long as you decay\nepsilon at this rate, you are actually\nconverging to Q star. You're getting more\nand more deterministic.",
    "start": "2142150",
    "end": "2147710"
  },
  {
    "text": "You're still visiting\nall states and actions an infinite number of times. And this procedure is guaranteed\nto asymptotically get you",
    "start": "2147710",
    "end": "2153820"
  },
  {
    "text": "to the optimal Q-function,\nwhich is pretty cool. And it should be\nsomewhat surprising.",
    "start": "2153820",
    "end": "2159950"
  },
  {
    "text": "All right. So that is GLIE. And that is of\nthe reasons why we like to think about\ne-greedy algorithms,",
    "start": "2159950",
    "end": "2166509"
  },
  {
    "text": "because they have\nthis nice property that we can prove that we are\ngoing to get an optimal policy, even though all we're doing\nis we're acting in the world.",
    "start": "2166510",
    "end": "2173235"
  },
  {
    "text": "And we're getting this data. Now, what you should be\nthinking about at this point is that, all right. Here's the Monte Carlo\napproach to doing this.",
    "start": "2173235",
    "end": "2179680"
  },
  {
    "text": "There's probably going to\nbe a temporal difference approach to doing this. And that's what we're\ngoing to see now. So now we're going to look\ninto temporal difference",
    "start": "2179680",
    "end": "2186840"
  },
  {
    "text": "methods for control. OK. So one of the\ninteresting things is",
    "start": "2186840",
    "end": "2193230"
  },
  {
    "text": "that there's going to be two\ndifferent types of algorithms that we're going to focus on for\ntemporal difference for control.",
    "start": "2193230",
    "end": "2201240"
  },
  {
    "text": "And the idea in\nthese settings is that we're going to\nalternate between two steps, again, this policy evaluation\nversus policy improvement.",
    "start": "2201240",
    "end": "2208932"
  },
  {
    "text": "And one of the key things\nto think about in this case is how much time are\nyou spending doing evaluation versus improvement.",
    "start": "2208932",
    "end": "2214361"
  },
  {
    "text": "And what are we\ntrying to evaluate, and what are we improving\nwith respect to? So the idea now is\nthat we're going",
    "start": "2214362",
    "end": "2219960"
  },
  {
    "text": "to compute Q pi using\ntemporal difference updating with an e-greedy policy.",
    "start": "2219960",
    "end": "2225405"
  },
  {
    "text": "And then we're going to\ndo policy improvement in the same way that we saw\nbefore for Monte Carlo methods.",
    "start": "2225405",
    "end": "2230460"
  },
  {
    "text": "So we can do this\ne-greedy thing, where we are greedy with\nrespect to our current Q value.",
    "start": "2230460",
    "end": "2236160"
  },
  {
    "text": "And the first algorithm we're\ngoing to see is called SARSA. And the reason it is\ncalled SARSA is it",
    "start": "2236160",
    "end": "2242100"
  },
  {
    "text": "is state action reward\nnext state next action.",
    "start": "2242100",
    "end": "2251750"
  },
  {
    "text": "It is short for that. S-A-R-S-A, SARSA.",
    "start": "2251750",
    "end": "2257710"
  },
  {
    "text": "That's an easy way to think-- to\nremember why this method would be called SARSA, because those\nare the tuples we need in order",
    "start": "2257710",
    "end": "2264790"
  },
  {
    "text": "to do updates. We need s, a, r, s prime,\na prime to do an update.",
    "start": "2264790",
    "end": "2270099"
  },
  {
    "text": "And this is going to be\nan on-policy algorithm. And this is related to what\nwas suggested in the back.",
    "start": "2270100",
    "end": "2275500"
  },
  {
    "text": "Remind me your name. Yeah, exactly what\n[INAUDIBLE] said. So can we also use\noff-policy data?",
    "start": "2275500",
    "end": "2281960"
  },
  {
    "text": "And we'll see that\nreally shortly. But SARSA is going\nto be on-policy. And what we mean by\nthat is that it's",
    "start": "2281960",
    "end": "2287570"
  },
  {
    "text": "going to be computing\nan estimate of the Q value of the policy we're\nusing to act or what policy",
    "start": "2287570",
    "end": "2293720"
  },
  {
    "text": "we're using to make\ndecisions in the world. So let's see how it works. So in general, the form\nof SARSA is the following.",
    "start": "2293720",
    "end": "2302720"
  },
  {
    "text": "We are going to\niterate, our loop is going to be such\nthat we start off.",
    "start": "2302720",
    "end": "2309120"
  },
  {
    "text": "So this is the-- we start in some state. This is the s. We take an action a.",
    "start": "2309120",
    "end": "2314390"
  },
  {
    "text": "We observe reward in the\nnext state, and then we loop. And we take the\nnext action still",
    "start": "2314390",
    "end": "2320510"
  },
  {
    "text": "according to the same policy. And then what we're\ngoing to do is we're going to update our\nQ-function, given this tuple",
    "start": "2320510",
    "end": "2329330"
  },
  {
    "text": "of SARSA, essentially. And what we're going\nto do in this case is going to look similar\nto what we saw before.",
    "start": "2329330",
    "end": "2336500"
  },
  {
    "text": "So we're going to\nhave our updated one is our old value plus alpha.",
    "start": "2336500",
    "end": "2346500"
  },
  {
    "text": "So this is like our\nlearning rate, our target.  s t plus 1, a t plus\n1, minus Q of s t, a t.",
    "start": "2346500",
    "end": "2357450"
  },
  {
    "text": " So this is the target.",
    "start": "2357450",
    "end": "2363105"
  },
  {
    "text": " And it's going to look similar\nto what we saw for TD 0,",
    "start": "2363105",
    "end": "2370060"
  },
  {
    "text": "where we plug-in our immediate\nreward, plus our estimate of the expected\ndiscounted sum of rewards",
    "start": "2370060",
    "end": "2375360"
  },
  {
    "text": "starting in that next state. And one of the important\nthings to notice in this case is we are plugging in\nthe actual action we",
    "start": "2375360",
    "end": "2383760"
  },
  {
    "text": "took in the next state. So we're saying what is\nthe expected discounted",
    "start": "2383760",
    "end": "2389018"
  },
  {
    "text": "sum of rewards starting in this\nstate and taking this action? Well, when-- estimate of\nit is the immediate reward",
    "start": "2389018",
    "end": "2395330"
  },
  {
    "text": "I got plus gamma, times the Q\nvalue for the state I reached, plus the action I would\ntake under this policy next.",
    "start": "2395330",
    "end": "2402414"
  },
  {
    "text": "And that's one of\nthe reasons why it's called on policy, because\nit's been specific to the action you would actually\ntake under this policy.",
    "start": "2402415",
    "end": "2411120"
  },
  {
    "text": "All right. And then after--\nthen the next thing we do is we do\npolicy improvement. And what we would do in this\ncase is, again, similar to what",
    "start": "2411120",
    "end": "2418580"
  },
  {
    "text": "we saw, [? new one. ?]\nSo for all s, this just means for\nall, for anybody who hasn't seen this notation.",
    "start": "2418580",
    "end": "2424880"
  },
  {
    "text": "Pi of s is equal to\nargmax over a, Q of s,",
    "start": "2424880",
    "end": "2430970"
  },
  {
    "text": "a with probability epsilon. ",
    "start": "2430970",
    "end": "2445834"
  },
  {
    "text": "And then what we do is\nwe update our timestep. We update our epsilon.",
    "start": "2445834",
    "end": "2451720"
  },
  {
    "text": "And then what we're going\nto do is just repeat. So then we're going to go-- then we're going\nto go to the next--",
    "start": "2451720",
    "end": "2458535"
  },
  {
    "text": "we're going to take our\nnext state, take an action, and repeat this updating.",
    "start": "2458535",
    "end": "2463740"
  },
  {
    "text": "So this is called-- yeah. Quick question. Like do we-- I have a bit\nconfused about setting pi of s.",
    "start": "2463740",
    "end": "2472210"
  },
  {
    "text": "Do we say pi is a\ndeterministic policy that is one of this with\nthis probability",
    "start": "2472210",
    "end": "2477745"
  },
  {
    "text": "and the other one with\nthe other probability? Or are we saying it's a\nstochastic policy that can--",
    "start": "2477745",
    "end": "2483150"
  },
  {
    "text": "It's a stochastic policy. Yeah, so it's a\nstochastic policy. At the very beginning,\nit's totally random.",
    "start": "2483150",
    "end": "2489397"
  },
  {
    "text": "You just take any\naction in any state. Later, you're defining it with\nrespect to your current Q value. And you're either being greedy\nwith respect to that Q value",
    "start": "2489397",
    "end": "2496350"
  },
  {
    "text": "or selecting action at random. Yeah. So one concern that\nI had was that what",
    "start": "2496350",
    "end": "2501870"
  },
  {
    "text": "if we reach a terminal\nstate and then just end? Good question. And this actually came up\nin another conversation",
    "start": "2501870",
    "end": "2508740"
  },
  {
    "text": "earlier this morning. Yes, so if you reach a terminal\nstate, then you just reset. So if s t plus 2 is terminal,\nreset the episode and sample s.",
    "start": "2508740",
    "end": "2529180"
  },
  {
    "text": "So if you ever\nreach a state where it's terminal, what\nwould happen next is then your whole\nepisode just resets.",
    "start": "2529180",
    "end": "2534984"
  },
  {
    "text": "You sample this initial\nstate from the world, and then you repeat. So just like if I like finished\nmy game, I failed at Tesla,",
    "start": "2534985",
    "end": "2540290"
  },
  {
    "text": "it reinitializes the world. So these are still\nsort of assumed to be continuing processes. Yeah?",
    "start": "2540290",
    "end": "2547150"
  },
  {
    "text": "I'm wondering-- what will\n[INAUDIBLE] to athletes?",
    "start": "2547150",
    "end": "2552895"
  },
  {
    "text": "Great question. So what best and what\nwe're going to see in just like a slide or two-- and\nyou guys are-- probably half of you at least have\nprobably seen this before.",
    "start": "2552895",
    "end": "2559610"
  },
  {
    "text": "We're going to see\nQ-learning, and that's where it's going to be off-policy. OK. Really quick question.",
    "start": "2559610",
    "end": "2565559"
  },
  {
    "text": "Like when it says t plus\n2, do we do the step seven?",
    "start": "2565560",
    "end": "2570660"
  },
  {
    "text": "Or do we skip it for one step\nand do it in the next one, because-- If it's terminal or in general?",
    "start": "2570660",
    "end": "2575885"
  },
  {
    "text": "Like if it's terminal. If it's terminal,\nyou would halt here. And then you would\nreset the whole thing.",
    "start": "2575885",
    "end": "2582267"
  },
  {
    "text": "Then you would need to take an\naction, observe, or next state, and then jump into five. So you have to reset to two.",
    "start": "2582267",
    "end": "2588010"
  },
  {
    "text": "Yeah, great question. All right. So let's see. Well, first, let's\ntalk about whether this",
    "start": "2588010",
    "end": "2593490"
  },
  {
    "text": "is guaranteed to do\nanything reasonable, and then we'll get going. So I've written\nthis up neatly here.",
    "start": "2593490",
    "end": "2599920"
  },
  {
    "text": "And then there's a worked\nexample for the Mars Rover at the end of the slides. OK. So one thing to\nnote here, too, is",
    "start": "2599920",
    "end": "2607320"
  },
  {
    "text": "that now we've defined\na general learning rate, so that we have a general\nlearning rate here. OK.",
    "start": "2607320",
    "end": "2612690"
  },
  {
    "text": "And we also have-- let me make sure I\nkeep this in here. ",
    "start": "2612690",
    "end": "2618810"
  },
  {
    "text": "We're going to keep\nupdating our epsilon. OK, so is this a good approach?",
    "start": "2618810",
    "end": "2625611"
  },
  {
    "text": "So we can think of a couple\nof different things here. We can think of the\ncomputational complexity. So here after each tuple,\nwe're doing an update.",
    "start": "2625612",
    "end": "2634560"
  },
  {
    "text": "And in fact, we know that\nthat's in general only going to change the Q value for\nthe states and the actions",
    "start": "2634560",
    "end": "2640307"
  },
  {
    "text": "that we're updating. So we just are doing that\nsmall update each time. We don't have to sum\nover all the states.",
    "start": "2640307",
    "end": "2646750"
  },
  {
    "text": "So there's nothing that\ndepends on the state space size per update. But of course, we're doing\nthis many, many, many times.",
    "start": "2646750",
    "end": "2653010"
  },
  {
    "text": "Does this converge to\nthe optimal Q-function? So what we have\nhere in this case is we have this\nweighted combination",
    "start": "2653010",
    "end": "2659789"
  },
  {
    "text": "between our last Q-function\nand this new target. And again, Q is an\nestimate of the performance",
    "start": "2659790",
    "end": "2667559"
  },
  {
    "text": "of a policy that might be\nchanging at each time point. So it's similar to Monte Carlo. Like we're just like\nwe're constantly",
    "start": "2667560",
    "end": "2673900"
  },
  {
    "text": "changing the policy\nin this case. And so that should feel\na little bit concerning. And empirically, it\noften does quite well.",
    "start": "2673900",
    "end": "2680300"
  },
  {
    "text": "But Q-learning is more popular. OK, so what are the\nconvergence properties? So it turns out that\nin terms of some",
    "start": "2680300",
    "end": "2686840"
  },
  {
    "text": "of the mathematical\nformulations, this relates really strongly\nto stochastic approximation. And this is a deep\nliterature with lots",
    "start": "2686840",
    "end": "2693980"
  },
  {
    "text": "of really amazing results. In the finite state\nand finite action case,",
    "start": "2693980",
    "end": "2699180"
  },
  {
    "text": "it's going to converge to\nthe optimal Q star for SARSA, if your policy sequence\nsatisfies the condition of GLIE.",
    "start": "2699180",
    "end": "2706553"
  },
  {
    "text": "So we're going to visit\nall states and actions an infinite number of times. And we're getting greedy\nand greedier over time.",
    "start": "2706553",
    "end": "2712640"
  },
  {
    "text": "And we have to put in a\ncondition about the learning rates, the step sizes. So in particular,\nthey have to satisfy",
    "start": "2712640",
    "end": "2719390"
  },
  {
    "text": "the Robbins-Munro sequence. So they have to satisfy\nthese two things, which",
    "start": "2719390",
    "end": "2724650"
  },
  {
    "text": "is their sum goes to\ninfinity, and their square is less than infinity. And we've seen this before.",
    "start": "2724650",
    "end": "2730800"
  },
  {
    "text": "And an example of this\nwould be a t equals 1 over t satisfies these conditions. ",
    "start": "2730800",
    "end": "2738569"
  },
  {
    "text": "So these results\nreally sort of rely on these really nice results\nfrom stochastic approximation,",
    "start": "2738570",
    "end": "2745875"
  },
  {
    "text": "because it should be a\nlittle bit surprising. You can think of this\nas kind of there's these different-- there's\nthese mixing processes that",
    "start": "2745875",
    "end": "2752270"
  },
  {
    "text": "are going on, because\nour policy is changing, our estimates are changing. How can we be sure\nthat it's essentially",
    "start": "2752270",
    "end": "2757400"
  },
  {
    "text": "going to be stable enough\nthat over time, we're actually going to converge to\nsomething that's both fixed--",
    "start": "2757400",
    "end": "2763140"
  },
  {
    "text": "like we're not just going\nto oscillate forever-- and that it is optimal? So it should not be at all clear\nwhy this would necessarily work.",
    "start": "2763140",
    "end": "2772339"
  },
  {
    "text": "And this is where we rely on\nthose results from stochastic approximation, that also had to\nbe extended to think about these",
    "start": "2772340",
    "end": "2779990"
  },
  {
    "text": "particular cases during a number\nof really beautiful papers from the 1990s.",
    "start": "2779990",
    "end": "2785300"
  },
  {
    "text": "So those are the 1992 and\n1994 papers that show this.",
    "start": "2785300",
    "end": "2794690"
  },
  {
    "text": "OK, so there's some\nreally cool results that illustrate why\nthis is possible. OK, SARSA for tabular settings\nunder some mild conditions",
    "start": "2794690",
    "end": "2803890"
  },
  {
    "text": "is guaranteed to\nconverge to Q star. So now let's see if we can\ndo off-policy learning.",
    "start": "2803890",
    "end": "2809410"
  },
  {
    "text": "So off-policy\nlearning is the idea that now we're going to be\ntrying to estimate and evaluate a policy using experience\ngathered from following",
    "start": "2809410",
    "end": "2816010"
  },
  {
    "text": "a different policy. So, so far, we've been thinking\nabout Monte Carlo methods and SARSA, where\nwe're at least sort",
    "start": "2816010",
    "end": "2822630"
  },
  {
    "text": "of trying to always approximate\nthe value of the most recent policy or averaged\nover all those policies.",
    "start": "2822630",
    "end": "2828460"
  },
  {
    "text": "But now we're\ngoing to explicitly be trying to estimate Q\nstar at all time points.",
    "start": "2828460",
    "end": "2834390"
  },
  {
    "text": "OK. So in Q-learning, we are going\nto try to directly estimate",
    "start": "2834390",
    "end": "2840710"
  },
  {
    "text": "the value of pi star-- which remember, we don't\nknow, because if we knew what pi star\nwas, then we wouldn't have to do any of\nthis learning--",
    "start": "2840710",
    "end": "2846890"
  },
  {
    "text": "with another behavior policy. Pi b. So we're going to be\nacting in one way. And we're going to\nbe trying to use",
    "start": "2846890",
    "end": "2852428"
  },
  {
    "text": "that data to estimate the\nvalue of an alternative policy. And that's what Q-learning does.",
    "start": "2852428",
    "end": "2859540"
  },
  {
    "text": "So in Q-learning,\nthe key difference is that instead of trying\nto think about what is the action we actually\ntook on the next time step,",
    "start": "2859540",
    "end": "2867350"
  },
  {
    "text": "we're just going to figure out\nwhat is the best action I could have taken, because we\nknow for the Q star value,",
    "start": "2867350",
    "end": "2874670"
  },
  {
    "text": "it is the estimate of the\noptimal expected reward you could get if you\ntake the current action",
    "start": "2874670",
    "end": "2880510"
  },
  {
    "text": "and then act\noptimally from now on. So really, you\nwould normally like to have something like this.",
    "start": "2880510",
    "end": "2886150"
  },
  {
    "text": "So sum over s prime probability\nof s prime given s, a, times V star of s prime.",
    "start": "2886150",
    "end": "2893339"
  },
  {
    "text": "So you would have in\nthe Bellman equation. And what we're going to do\nhere, what Q-learning does",
    "start": "2893340",
    "end": "2898470"
  },
  {
    "text": "is it approximates\nthat by this max.",
    "start": "2898470",
    "end": "2904200"
  },
  {
    "text": "And that is different\nthan what SARSA does, because SARSA used\nthe actual action.",
    "start": "2904200",
    "end": "2910862"
  },
  {
    "text": "And Q-learning\nsays, I don't really care what actual\naction you took. I care about what is the best\nthing you could have done there,",
    "start": "2910862",
    "end": "2915930"
  },
  {
    "text": "because that's\ngiving me a better estimate of the maximum expected\ndiscounted sum of rewards I'd get from that state\ntill the end of time.",
    "start": "2915930",
    "end": "2924720"
  },
  {
    "text": "So that is what\nQ-learning is doing. So it looks really similar\nto the SARSA update. But our target is going\nto be the reward I got,",
    "start": "2924720",
    "end": "2931750"
  },
  {
    "text": "plus the best reward that I\nthink I could have achieved from that next state. ",
    "start": "2931750",
    "end": "2938980"
  },
  {
    "text": "All right. So then we get an algorithm\nthat looks extremely similar to what we saw before. But we have this max\nover the next action.",
    "start": "2938980",
    "end": "2947790"
  },
  {
    "text": "And then I'll just make sure-- I think I forgot\nto write down here. ",
    "start": "2947790",
    "end": "2954550"
  },
  {
    "text": "So whether we're doing Monte\nCarlo or SARSA or Q-learning, in all of these cases, we're\ninterleaving gathering some data",
    "start": "2954550",
    "end": "2960789"
  },
  {
    "text": "under our current\nepsilon greedy policy, and then using it\nto update a Q value. And because we don't know\nwhat the actual Q-function is,",
    "start": "2960790",
    "end": "2968660"
  },
  {
    "text": "we're sort of doing this\nweighted approximation between our current estimate of\nthe Q-function and the target",
    "start": "2968660",
    "end": "2975580"
  },
  {
    "text": "that we just put in. And we do this over and\nover and over again. ",
    "start": "2975580",
    "end": "2983210"
  },
  {
    "text": "So similar to SARSA,\nthe conditions to make sure that Q-learning\nin the tabular case-- so things get a lot\nmore complicated",
    "start": "2983210",
    "end": "2990140"
  },
  {
    "text": "once we go into the\nfunction approximation case.",
    "start": "2990140",
    "end": "2995380"
  },
  {
    "text": "But in order for\ntabular Q-learning with e-greedy exploration to\nconverge the optimal Q star,",
    "start": "2995380",
    "end": "3001153"
  },
  {
    "text": "you again need to visit\neverything infinitely often. Your step sizes has to satisfy\nthe Robbins-Munro sequence.",
    "start": "3001153",
    "end": "3007730"
  },
  {
    "text": "And one important\nthing to notice here is that you can estimate Q\nstar without being GLIE, which",
    "start": "3007730",
    "end": "3020369"
  },
  {
    "text": "is different than SARSA, because\nyou're always doing this max.",
    "start": "3020370",
    "end": "3026240"
  },
  {
    "text": "So even if you act\ncompletely randomly, so just like infinite\nexploration, not being greedy,",
    "start": "3026240",
    "end": "3032609"
  },
  {
    "text": "you could learn Q star, because\nin your Q star estimate here,",
    "start": "3032610",
    "end": "3038390"
  },
  {
    "text": "you're always doing this max a. So that's an important\ndifference compared to SARSA.",
    "start": "3038390",
    "end": "3046320"
  },
  {
    "text": "But if you actually want\nto use that information to make good decisions\nin the world, you need to become\ngreedy over time and be using that information to\nactually select the best action",
    "start": "3046320",
    "end": "3054470"
  },
  {
    "text": "according to your Q-function. And for e-greedy\nalgorithms with Q-learning,",
    "start": "3054470",
    "end": "3059769"
  },
  {
    "text": "you normally do k over\nepsilon over time. So you're getting more\nand more deterministic. And you're taking your\nestimate of what Q star is",
    "start": "3059770",
    "end": "3066070"
  },
  {
    "text": "and using it to make decisions. ",
    "start": "3066070",
    "end": "3071667"
  },
  {
    "text": "Yeah, we're now going to go\ninto function approximation. I'm just going to pause there in\ncase people had any questions. ",
    "start": "3071667",
    "end": "3081600"
  },
  {
    "text": "Yeah. So for either using\nSARSA or Q-learning,",
    "start": "3081600",
    "end": "3086829"
  },
  {
    "text": "will it converge--\ndoes it converge to a stochastic policy or\na deterministic policy?",
    "start": "3086830",
    "end": "3092720"
  },
  {
    "text": "Great, great question. So if there are no ties\nin your Q-function, as in like for any action\nthere or any state,",
    "start": "3092720",
    "end": "3100319"
  },
  {
    "text": "there is a uniquely\nbest action, it'll converge to a\ndeterministic policy. If there are ties,\nit'll generally",
    "start": "3100320",
    "end": "3107120"
  },
  {
    "text": "pick between those arbitrarily. There'll be an infinite\nnumber of optimal policies if there are ties\nin your Q-function.",
    "start": "3107120",
    "end": "3113175"
  },
  {
    "text": "Great question. ",
    "start": "3113175",
    "end": "3118310"
  },
  {
    "text": "All right. So now what we're\ngoing to do is we're going to layer on function\napproximation on top. So this was all\nassuming that we just",
    "start": "3118310",
    "end": "3123530"
  },
  {
    "text": "had this table where\nyou could write down the value for every state\nand action separately.",
    "start": "3123530",
    "end": "3128609"
  },
  {
    "text": "And now we want to use\nfunction approximation. So we can start to do\nproblems like Atari.",
    "start": "3128610",
    "end": "3134690"
  },
  {
    "text": "So the motivation for\ndoing this-- and I know for those of you who've\ntaken machine learning, this is probably clear,\nbut it's nice to think",
    "start": "3134690",
    "end": "3140930"
  },
  {
    "text": "about what this means in\nthe context of reinforcement learning. So what are the things\nthat we might be storing or trying to manipulate?",
    "start": "3140930",
    "end": "3147410"
  },
  {
    "text": "That might be the dynamics\nor reward model, the value function, the state action\nvalue function, or the policy.",
    "start": "3147410",
    "end": "3153260"
  },
  {
    "text": "And if you were thinking\nabout pixel space, you do not want to write that\ndown as like one different value",
    "start": "3153260",
    "end": "3159530"
  },
  {
    "text": "for every-- bless you-- for\nevery different possible image in the world. So we're going to want compact\nrepresentations like what",
    "start": "3159530",
    "end": "3166220"
  },
  {
    "text": "we can do with neural networks,\nso that we reduce the memory we need to write down those\ndynamics models, the value",
    "start": "3166220",
    "end": "3172190"
  },
  {
    "text": "function, or Q or the policy. We reduce the computation. And ideally, we might even be\nable to reduce the experience.",
    "start": "3172190",
    "end": "3179875"
  },
  {
    "text": "And I think this\nlast point maybe is a particularly interesting\none to think about. So you can imagine, if\nyour agent is learning",
    "start": "3179875",
    "end": "3185480"
  },
  {
    "text": "to play an Atari game\nor play Breakout, it might want to know that,\noh, well, if these pixels are",
    "start": "3185480",
    "end": "3191829"
  },
  {
    "text": "slightly different\nhere, most of the time, you might still take\nthe same decision. And so then instead of\nhaving to learn from scratch",
    "start": "3191830",
    "end": "3198230"
  },
  {
    "text": "what to do in each\nstate, you can get this sort of generalization. And that could be really\nimportant in terms",
    "start": "3198230",
    "end": "3203480"
  },
  {
    "text": "of reducing the\namount of data we need to learn to\nmake good decisions.",
    "start": "3203480",
    "end": "3209119"
  },
  {
    "text": "All right. So how do we do this? What we're going to\ntry to do is we're going to essentially do the same\nthing as what we did before.",
    "start": "3209120",
    "end": "3215790"
  },
  {
    "text": "But we're also going to have\nto incorporate a function approximation step. So let's just think about\nhow we would do this",
    "start": "3215790",
    "end": "3220940"
  },
  {
    "text": "if we had an Oracle. So what I mean by\nthis is we're not thinking yet right now\nabout all the learning",
    "start": "3220940",
    "end": "3227020"
  },
  {
    "text": "and like gathering data. We're just assuming how do we\nfit a function to represent our Q-function.",
    "start": "3227020",
    "end": "3232220"
  },
  {
    "text": "So let's imagine that you had\nan Oracle that for any state and action, it would\ngive you the true value",
    "start": "3232220",
    "end": "3237260"
  },
  {
    "text": "for a particular policy\nand that state and action. So it would tell you like\nthat's three or that's seven.",
    "start": "3237260",
    "end": "3243710"
  },
  {
    "text": "So then you could\nsay, OK, now I've just got a supervised\nlearning problem, I've got input tuples\nof states and actions.",
    "start": "3243710",
    "end": "3249680"
  },
  {
    "text": "And I have output\nvalues of my Q-function. And what I want to do now is\njust learn a function to--",
    "start": "3249680",
    "end": "3256160"
  },
  {
    "text": "a regression function to say,\ngiven the state and action, what is the output? So imagine that\nyou're in a case where",
    "start": "3256160",
    "end": "3263330"
  },
  {
    "text": "we have a continuous\nset of states, and we only have one action. Then you might just have\nall these different points.",
    "start": "3263330",
    "end": "3272660"
  },
  {
    "text": "And maybe you just want to\nlearn a function that predicts the Q for every single state.",
    "start": "3272660",
    "end": "3279580"
  },
  {
    "text": "And you just learn like\na parametric function. Or it could be a\ndeep neural network. And in general, just like\nin supervised learning,",
    "start": "3279580",
    "end": "3286100"
  },
  {
    "text": "the objective is going\nto be to find the best approximate representation\nof Q, given some weights or given some neural\nnetwork architecture.",
    "start": "3286100",
    "end": "3293950"
  },
  {
    "text": "So we've got some neural net.  And we're just going\nto fit this to try",
    "start": "3293950",
    "end": "3301310"
  },
  {
    "text": "to-- if we had these points. But of course, we don't\nhave these points. And we're going to see how\nwe're going to handle it.",
    "start": "3301310",
    "end": "3307619"
  },
  {
    "text": "We don't have these points. But this is the intuition is\nthat if you had these, then you could do the function\napproximation step by saying,",
    "start": "3307620",
    "end": "3314339"
  },
  {
    "text": "OK, well, how do I-- I'm going to handle\ngeneralization by using a linear function or\na deep neural network to say,",
    "start": "3314340",
    "end": "3319800"
  },
  {
    "text": "for each of these states and\nactions, what is the output? ",
    "start": "3319800",
    "end": "3325039"
  },
  {
    "text": "So just to highlight\nhere in this class, generally, we will be\nfocusing on methods that use stochastic\ngradient descent to try",
    "start": "3325040",
    "end": "3331280"
  },
  {
    "text": "to fit these functions. And again, I expect most of\nthis is familiar for you guys if you've done machine learning.",
    "start": "3331280",
    "end": "3336480"
  },
  {
    "text": "If you haven't, you can come\ntalk to me or any of the TAs. Generally, we're going to\njust use mean squared error.",
    "start": "3336480",
    "end": "3343670"
  },
  {
    "text": "And we're going to try to\nfit a function that minimizes the mean squared error. We're going to do gradient\ndescent to find a local minimum.",
    "start": "3343670",
    "end": "3350720"
  },
  {
    "text": "And we're going to do\nstochastic gradient descent just to compute an\napproximate gradient.",
    "start": "3350720",
    "end": "3358930"
  },
  {
    "text": "Right. So in this case--",
    "start": "3358930",
    "end": "3364110"
  },
  {
    "text": "and I have here is just to\nwrite that out really quickly. You would have\nsomething like this. You'd have w, J, and--",
    "start": "3364110",
    "end": "3369810"
  },
  {
    "text": " it's just the\nderivative of this. ",
    "start": "3369810",
    "end": "3377319"
  },
  {
    "text": "I'm sorry. Like that.  So I'm going to take\nthis equation star.",
    "start": "3377320",
    "end": "3383440"
  },
  {
    "text": "And I'm just going to take\nthe derivative of it, which is going to be two.",
    "start": "3383440",
    "end": "3389099"
  },
  {
    "start": "3389100",
    "end": "3408125"
  },
  {
    "text": "So we're just going to take\nthe derivative of this. And essentially,\nthat just means we're going to have to\ntake the derivative through our Q-function\nrepresentation,",
    "start": "3408125",
    "end": "3414930"
  },
  {
    "text": "like using autodiff for\ndeep neural networks. And then we can use this\nto update our weights.",
    "start": "3414930",
    "end": "3422367"
  },
  {
    "text": "All right, so we'll do\nstochastic gradient descent to do this. And the main thing\nis that that's",
    "start": "3422367",
    "end": "3429350"
  },
  {
    "text": "what we're going to be doing to\nplug-in in order to do policy evaluation or to do control.",
    "start": "3429350",
    "end": "3436020"
  },
  {
    "text": "So of course, in general,\nwe don't have those. We don't have for each state and\naction, what the Q value was.",
    "start": "3436020",
    "end": "3441190"
  },
  {
    "text": "If it was, we wouldn't\nneed to do any learning. We need to learn that from data. And so the idea is\nthat we're going",
    "start": "3441190",
    "end": "3447810"
  },
  {
    "text": "to do model-free state action,\nvalue function approximation. So just like what we've\nbeen seeing before,",
    "start": "3447810",
    "end": "3453579"
  },
  {
    "text": "we're doing model-free\nstate action value function. Now we're going to\nactually do that, but just",
    "start": "3453580",
    "end": "3459240"
  },
  {
    "text": "do an approximation, where\ninstead of writing it down as a table, we're going to write\nit down with these parameters function--\nparametrized functions.",
    "start": "3459240",
    "end": "3466770"
  },
  {
    "text": "OK, so the idea now\nis like similarly,",
    "start": "3466770",
    "end": "3472158"
  },
  {
    "text": "we just saw before\nall these methods, either where we use Monte Carlo\nmethods or temporal difference methods to try to do\nthese approximations.",
    "start": "3472158",
    "end": "3479700"
  },
  {
    "text": "Now what we're going to do is\nthat when we do the estimate update step, we're also going to\nfit the function approximator.",
    "start": "3479700",
    "end": "3486790"
  },
  {
    "text": "So just like in\nthe algorithms we saw before where we do like\npolicy evaluation and policy improvement, now when we\ndo the policy evaluation,",
    "start": "3486790",
    "end": "3493250"
  },
  {
    "text": "we're also going to just refit\nlike our whole Q-function, for example.",
    "start": "3493250",
    "end": "3498890"
  },
  {
    "text": "OK, so let's see\nhow that could work. So for Monte Carlo value\nfunction approximation,",
    "start": "3498890",
    "end": "3504450"
  },
  {
    "text": "we're going to remember\nthat our return G is an unbiased but noisy\nsample of the expected return.",
    "start": "3504450",
    "end": "3510260"
  },
  {
    "text": "So we can think of us having\nthis state, action return, state, action,\nreturn, et cetera.",
    "start": "3510260",
    "end": "3516590"
  },
  {
    "text": "And so you can substitute in\nthose Gs for the true Q pi when you're doing your fitting.",
    "start": "3516590",
    "end": "3524450"
  },
  {
    "text": "So let's see what\nthat would look like. So in this case, remember\nwhat you would like here",
    "start": "3524450",
    "end": "3531540"
  },
  {
    "text": "when we're doing our\nfunction approximation is that this is the\nreal Q of the policy,",
    "start": "3531540",
    "end": "3537369"
  },
  {
    "text": "but we don't know what the\nreal policy, real Q value is. So we're going to plug-in\nr, observed return.",
    "start": "3537370",
    "end": "3544270"
  },
  {
    "text": "So, we want-- would\nlike Q of s, a.",
    "start": "3544270",
    "end": "3554290"
  },
  {
    "text": "But we don't have that. So we're going to plug-in the\nreturn that we just observed. And then we'll just\ndo the derivative--",
    "start": "3554290",
    "end": "3560540"
  },
  {
    "text": "we'll be plugging that\nin for our derivative. And then update\nour weights using that derivative with\nrespect to minimizing",
    "start": "3560540",
    "end": "3566530"
  },
  {
    "text": "the mean squared error. OK, so this would just\nbe for policy evaluation.",
    "start": "3566530",
    "end": "3572020"
  },
  {
    "text": "If you have a fixed\npolicy, you would just do this at each time point. So after you see--",
    "start": "3572020",
    "end": "3577500"
  },
  {
    "text": "after you get a return, then you\nwould update your Q-function. And you would do this\nmany, many times.",
    "start": "3577500",
    "end": "3583147"
  },
  {
    "text": "And for some of you, this\nmight start to look redundant. But I think it's\njust useful to see that essentially the structure\nof all of these algorithms,",
    "start": "3583147",
    "end": "3588997"
  },
  {
    "text": "whether it is policy evaluation\nor tabular or function approximation, is\nextremely similar.",
    "start": "3588997",
    "end": "3594900"
  },
  {
    "text": "We are just either sampling an\nepisode or sampling a tuple. We are going to\ndo one step, which",
    "start": "3594900",
    "end": "3600440"
  },
  {
    "text": "is like policy evaluation,\nwhere we update our estimate of the\nQ-function, maybe optionally do function approximation fitting.",
    "start": "3600440",
    "end": "3607079"
  },
  {
    "text": "And then we're going to use that\nto figure out how to act next if we are doing control.",
    "start": "3607080",
    "end": "3612580"
  },
  {
    "text": "We'll see an example\nof that shortly. OK, so that is Monte Carlo.",
    "start": "3612580",
    "end": "3618490"
  },
  {
    "text": "OK. Oops. ",
    "start": "3618490",
    "end": "3624080"
  },
  {
    "text": "OK. All right. For temporal difference\nlearning, it's very similar. But now we are going to have\nthis weighted sum where we",
    "start": "3624080",
    "end": "3631369"
  },
  {
    "text": "plug-in-- we bootstrap. So we plug-in our current\nestimate of the value of s",
    "start": "3631370",
    "end": "3636500"
  },
  {
    "text": "prime. So this is the same\nupdate we saw before. This was for tabular\ncases and now we're",
    "start": "3636500",
    "end": "3642410"
  },
  {
    "text": "going to do it for\nfunction approximation. OK. So let's first just\nsee how we do it",
    "start": "3642410",
    "end": "3648875"
  },
  {
    "text": "for function approximation. It's just useful, I think,\nwhen we look at this, to think about all the different\nways we're doing approximations.",
    "start": "3648875",
    "end": "3654740"
  },
  {
    "text": "We are sampling to\napproximate the expected value over the next state. We are bootstrapping to plug-in\nwhat the value of those states",
    "start": "3654740",
    "end": "3661940"
  },
  {
    "text": "are. And now we're also going to\ndo function approximation, because we're going to represent\nthe value of a function",
    "start": "3661940",
    "end": "3667100"
  },
  {
    "text": "with some weights. OK. So we're going to\nhave these weights. All right.",
    "start": "3667100",
    "end": "3673920"
  },
  {
    "text": "And again, we can just do\nstochastic gradient descent to fit our weight function to\nrepresent that value function.",
    "start": "3673920",
    "end": "3679865"
  },
  {
    "text": " OK. ",
    "start": "3679865",
    "end": "3687303"
  },
  {
    "text": "So you'll get\nsomething like this, where as long as if you're\nin a terminal state, you'll restart the episode.",
    "start": "3687303",
    "end": "3692950"
  },
  {
    "text": "Otherwise, you'll just be doing\nthese computing the gradient with respect to your minimizing\nyour mean squared error",
    "start": "3692950",
    "end": "3698682"
  },
  {
    "text": "and updating your weights. ",
    "start": "3698682",
    "end": "3704660"
  },
  {
    "text": "And then we'll see how\nwe do this for control.",
    "start": "3704660",
    "end": "3709839"
  },
  {
    "text": "So for control, it's\ngoing to be very similar. Now, what we'll\nmake sure to do is",
    "start": "3709840",
    "end": "3715380"
  },
  {
    "text": "we're always going to be\nusing the Q-function instead of the value function. And we're now often going to be\ndoing off-policy learning again,",
    "start": "3715380",
    "end": "3723190"
  },
  {
    "text": "like Q-learning. So we'll again do\nstochastic gradient descent. With respect to our\nQ-function, we're",
    "start": "3723190",
    "end": "3729670"
  },
  {
    "text": "going to sample the gradient. And we'll have very-- an algorithm that's very similar\nto the one we've seen before.",
    "start": "3729670",
    "end": "3736030"
  },
  {
    "text": "So we can either use SARSA where\nwe have our Q-function, where",
    "start": "3736030",
    "end": "3742240"
  },
  {
    "text": "we always plug-in what is the\nactual action we took next. Or we can have Q-learning\nwhere we plug-in",
    "start": "3742240",
    "end": "3748780"
  },
  {
    "text": "a max over the next Q-function. And raise your hand if you've\nimplemented deep Q-learning",
    "start": "3748780",
    "end": "3755960"
  },
  {
    "text": "before. OK. So one person, but\nmost people not. Yeah. OK.",
    "start": "3755960",
    "end": "3760970"
  },
  {
    "text": "So you can imagine in general\nthis is any form of function approximator. But often this is going to be\nlike a deep neural network.",
    "start": "3760970",
    "end": "3768670"
  },
  {
    "text": "OK. Now, one thing I just want to\nhighlight here is that, again, just being in terms of being\nconcerned whether all of this",
    "start": "3768670",
    "end": "3776109"
  },
  {
    "text": "is going to work, there's\na lot of approximations that are happening here, so\nparticularly for Q-learning.",
    "start": "3776110",
    "end": "3781599"
  },
  {
    "text": "And it's led to what Sutton\nand Barto, the authors of the book that is the\noptional textbook for the class",
    "start": "3781600",
    "end": "3787690"
  },
  {
    "text": "called The Deadly Triad. And what they say is that if you\nare doing bootstrapping, meaning",
    "start": "3787690",
    "end": "3793150"
  },
  {
    "text": "that you're plugging\nin an estimate of what is the value of the next state\nand you're doing function approximation, like you're\nusing a deep neural network",
    "start": "3793150",
    "end": "3800529"
  },
  {
    "text": "or a linear function, and you're\ndoing off-policy learning where you are acting in a\ndifferent way than the data",
    "start": "3800530",
    "end": "3806020"
  },
  {
    "text": "you're getting,\nunder those cases, you may not converge at all.",
    "start": "3806020",
    "end": "3811320"
  },
  {
    "text": "Like just your\nQ-function may oscillate. You may not converge\nto anything.",
    "start": "3811320",
    "end": "3816330"
  },
  {
    "text": "And you are certainly\nnot guaranteed to converge to Q star.",
    "start": "3816330",
    "end": "3821589"
  },
  {
    "text": "So it's just good to keep in\nmind that that could occur. I think for some intuition\nfor why this can occur--",
    "start": "3821590",
    "end": "3829220"
  },
  {
    "text": "the Bellman operator,\nif you think back a couple of lectures ago,\nwe proved as a contraction, meaning that as we\napply it repeatedly,",
    "start": "3829220",
    "end": "3835550"
  },
  {
    "text": "we went to this fixed point\nin the tabular setting. But the problem is that when\nyou do a Bellman backup,",
    "start": "3835550",
    "end": "3841760"
  },
  {
    "text": "that operator is a\ncontraction, meaning that if you apply the Bellman\noperator to different things, their distance gets\nsmaller afterwards.",
    "start": "3841760",
    "end": "3848290"
  },
  {
    "text": "Value function\napproximation fitting can be an expansion, which\nmeans if you take two things and then you try to do value\nfunction approximation,",
    "start": "3848290",
    "end": "3856040"
  },
  {
    "text": "like you align to this\none and align to this one, the distance between\ntwo points afterwards can actually get bigger than\nbefore you did the value",
    "start": "3856040",
    "end": "3862810"
  },
  {
    "text": "function approximation. So there's a really\nbeautiful example of this in a paper by\nJeff Gordon from 1995.",
    "start": "3862810",
    "end": "3869619"
  },
  {
    "text": "I will just-- Jeff Gordon\n1995 has a really nice example",
    "start": "3869620",
    "end": "3876110"
  },
  {
    "text": "of this where you just\ncan kind of visually see, when you have these two\nfunctions and these points, that after you do this value\nfunction approximation,",
    "start": "3876110",
    "end": "3882990"
  },
  {
    "text": "you've actually made the\ndistance between them bigger. And so that means that you\nhave this thing, where you're",
    "start": "3882990",
    "end": "3888560"
  },
  {
    "text": "kind of alternating between\nsomething which you know is a contraction and driving\nyou towards a fixed point,",
    "start": "3888560",
    "end": "3894500"
  },
  {
    "text": "and something which might\nactually amplify differences. And so because of that,\nit's not always the case",
    "start": "3894500",
    "end": "3899840"
  },
  {
    "text": "that you're guaranteed to\nconverge to a fixed point. So this is something\nimportant to know. However, I think\nit's also kind of a--",
    "start": "3899840",
    "end": "3907700"
  },
  {
    "text": "it's an important part of the\nhistory of the field in that, in the 1990s, there was a bunch\nof work showing that this could",
    "start": "3907700",
    "end": "3913849"
  },
  {
    "text": "occur that even with some really\nsimple settings like linear value function approximators,\nwe just approximate things with",
    "start": "3913850",
    "end": "3919880"
  },
  {
    "text": "a line, and that sometimes\nyou could get these kind of oscillations or\nlack of convergence. And so people were really\nconcerned about using",
    "start": "3919880",
    "end": "3926450"
  },
  {
    "text": "function approximators with\nreinforcement learning. But then what happened is that\nDeepMind showed well, actually",
    "start": "3926450",
    "end": "3933170"
  },
  {
    "text": "there are some ways\nto tackle this. And we can do really\namazing things with it. And so I think it's a useful--",
    "start": "3933170",
    "end": "3938370"
  },
  {
    "text": "like a useful\nlesson from history over the difference between,\nwell, what can occur and maybe some sort of not\nideal cases versus what actually",
    "start": "3938370",
    "end": "3947330"
  },
  {
    "text": "occurs in practice. And so we shouldn't let some\nof the negative examples limit us from\nconsidering what might",
    "start": "3947330",
    "end": "3954109"
  },
  {
    "text": "work in some other scenarios. So let's see that now. Let's see DQN.",
    "start": "3954110",
    "end": "3960170"
  },
  {
    "text": "OK. So the idea with\nDQN is we're going to use these ideas to\nactually play Atari.",
    "start": "3960170",
    "end": "3965930"
  },
  {
    "text": "So we're going to take\nin images of the game. We're going to use\nconvolutional neural networks. And we're going to have a\nreally big, deep neural network",
    "start": "3965930",
    "end": "3973180"
  },
  {
    "text": "to represent the Q-function\nand do Q-learning with it. So the idea was, well,\nwe knew that sometimes,",
    "start": "3973180",
    "end": "3981700"
  },
  {
    "text": "like Q-learning with value\nfunction approximation can diverge. And there's a number\nof different issues,",
    "start": "3981700",
    "end": "3987550"
  },
  {
    "text": "but one of them is kind\nof this stability thing. So we know that there's\ncorrelations between samples. Your data is not IID, which\nis what you would normally",
    "start": "3987550",
    "end": "3994740"
  },
  {
    "text": "want for when you're doing\nfunction approximation. And the other is that you have\nthis kind of nonstationary target thing, which is like when\nyou plug-in, say with learning,",
    "start": "3994740",
    "end": "4003809"
  },
  {
    "text": "you're plugging in gamma plus-- sorry, r plus gamma times\nthe value of your next state.",
    "start": "4003810",
    "end": "4009053"
  },
  {
    "text": "And that value of the\nnext state is constantly changing as you get more data. So what DQN did is\nthey said, well,",
    "start": "4009053",
    "end": "4015517"
  },
  {
    "text": "look, what we're\ngoing to do is we're going to use experience replay. In particular, we're going\nto reuse tuples over time.",
    "start": "4015518",
    "end": "4022069"
  },
  {
    "text": "And we're also going\nto get fixed Q-targets. And both of those\nthings ended up making a really big difference,\nparticularly one of them.",
    "start": "4022070",
    "end": "4029323"
  },
  {
    "text": "We'll see in a second. So the idea of\nexperience replay is to say in general, if I think\nabout states that are nearby,",
    "start": "4029323",
    "end": "4037340"
  },
  {
    "text": "their Q-function might\nbe pretty similar. And if I'm doing\nlots of updates, that's breaking my IID stuff\nthat I want for my function",
    "start": "4037340",
    "end": "4044490"
  },
  {
    "text": "approximation. So another thing\nyou could do is just have a replay buffer of lots of\nall the different tuples you've",
    "start": "4044490",
    "end": "4050490"
  },
  {
    "text": "seen in the past. And you could just\nsample from one of those and then compute a\ntarget value and then",
    "start": "4050490",
    "end": "4056609"
  },
  {
    "text": "do stochastic gradient descent. And this might be really\nhelpful anyway just in terms of data\nefficiency, because it",
    "start": "4056610",
    "end": "4063240"
  },
  {
    "text": "means that instead of taking\nyour data and using it once and then throwing\nit away, you keep it and then you can replay it.",
    "start": "4063240",
    "end": "4069940"
  },
  {
    "text": "Just like how we talked about\nbatch learning last time. So an experience\nreplay can be useful,",
    "start": "4069940",
    "end": "4075390"
  },
  {
    "text": "because we're both\nreplaying our data. So we can squeeze more\ninformation out of it. And also we can select\nfrom very different parts",
    "start": "4075390",
    "end": "4081750"
  },
  {
    "text": "of the past history, which makes\nthose updates more independent. OK. So this is-- and\nin general, we're",
    "start": "4081750",
    "end": "4088530"
  },
  {
    "text": "not going to keep the\nbuffer for all time. We might keep the last million\nepisodes or things like that.",
    "start": "4088530",
    "end": "4094470"
  },
  {
    "text": "OK, So that's one\nthing we could do. Now, the other\nthing is that if we",
    "start": "4094470",
    "end": "4101049"
  },
  {
    "text": "think about what's happening\nin this case, the way we change the weights\nis going to be--",
    "start": "4101050",
    "end": "4108278"
  },
  {
    "text": "in general, the weights\nappear here and here and here. So this target value is a\nfunction of the weights itself,",
    "start": "4108279",
    "end": "4116705"
  },
  {
    "text": "because you're using\nthe value function approximation to represent\nthe value of your next state. And so the problem\nis that in general,",
    "start": "4116705",
    "end": "4123410"
  },
  {
    "text": "this is going to change\non your next update, because you've just\nchanged your weights.",
    "start": "4123410",
    "end": "4128950"
  },
  {
    "text": "And this can also\nlead to instabilities, because if you think of\nsupervised learning, your x, y",
    "start": "4128950",
    "end": "4134649"
  },
  {
    "text": "pairs, your y is changing\neven for the same x over time, because you're changing\nyour Q-function.",
    "start": "4134649",
    "end": "4141370"
  },
  {
    "text": "OK, because this is a\nfunction of the weights. And so as the weights\nchange, this target value is going to change even\nfor the same input.",
    "start": "4141370",
    "end": "4149140"
  },
  {
    "text": "So the second idea is\nto have fixed Q-updates. And what the idea here\nis-- and so remember,",
    "start": "4149140",
    "end": "4155210"
  },
  {
    "text": "this is like when we\nsay the target weights-- this is going to be what we're\nusing for the target weights--",
    "start": "4155210",
    "end": "4161259"
  },
  {
    "text": "is that the weight-- the weights or the\nparameters we're using to estimate the value\nof the next state we reach,",
    "start": "4161260",
    "end": "4168170"
  },
  {
    "text": "we are going to not\nupdate those as much. So we're going to have\nour target network",
    "start": "4168170",
    "end": "4173228"
  },
  {
    "text": "using a different set of\nweights and the weights that are being updated. So you can see here\nthat we have a w minus,",
    "start": "4173228",
    "end": "4179299"
  },
  {
    "text": "meaning that we're\ntrying to make this more like supervised learning\nwhere we have a fixed output y that is not changing, while\nwe're trying to update our w.",
    "start": "4179300",
    "end": "4188739"
  },
  {
    "text": "And so if you think\nabout the example, we want to just\ndraw it like this. Here's our states,\nhere's our Q-function.",
    "start": "4188740",
    "end": "4196540"
  },
  {
    "text": "Right now, we'd\nlike to make sure that these points,\nwhen we're trying to fit a line that those y's\nare not changing a lot where",
    "start": "4196540",
    "end": "4203240"
  },
  {
    "text": "we're trying to fit the line. And in general, because\nthey're a function of the weights themselves, they\nmight be moving and perturbing.",
    "start": "4203240",
    "end": "4209730"
  },
  {
    "text": "And so what we're saying is,\nno, we're going to fix these. So you can think of this as just\nbeing a fixed number for a while",
    "start": "4209730",
    "end": "4215600"
  },
  {
    "text": "and then do multiple\nupdates on this w to try to fit that function. ",
    "start": "4215600",
    "end": "4222450"
  },
  {
    "text": "And so what that means is we\njust have to we keep around these target weights, and we\nkeep around the other weights.",
    "start": "4222450",
    "end": "4231150"
  },
  {
    "text": "And this allows us\nto do-- this is what is called the fixed Q-updating. So if you think about\nwhat this pseudocode would",
    "start": "4231150",
    "end": "4237840"
  },
  {
    "text": "look like in this case-- let's see-- is\nit's going to look",
    "start": "4237840",
    "end": "4248583"
  },
  {
    "text": "pretty similar to the\nthings we've seen. You're going to\nsample an action. You're going to observe\na word in the next state. You're going to store the\ntransition in a replay buffer.",
    "start": "4248583",
    "end": "4255350"
  },
  {
    "text": "So you're going to\nkeep track of it. Then you're going to sample\na random mini batch of tuples from the past.",
    "start": "4255350",
    "end": "4260783"
  },
  {
    "text": "You're going to do\nsomething, keep track of the episodes terminated. Otherwise, you're\ngoing to say my target",
    "start": "4260783",
    "end": "4265969"
  },
  {
    "text": "y that I'm going to try to fit\nin my function approximator is my immediate reward,\nplus the maximum",
    "start": "4265970",
    "end": "4271120"
  },
  {
    "text": "of my actions of my Q-function\nwith my target weights. So I use my deep neural\nnetwork to predict the value",
    "start": "4271120",
    "end": "4277480"
  },
  {
    "text": "of that state action pair. And then I'm going to\ndo gradient descent on the difference between\nthese predicted y's",
    "start": "4277480",
    "end": "4283780"
  },
  {
    "text": "and my current estimate\nwith my current weights. So this is just the\nfunction fitting part.",
    "start": "4283780",
    "end": "4289270"
  },
  {
    "text": "And then you repeat this. And then periodically, you\nupdate your target weights.",
    "start": "4289270",
    "end": "4294950"
  },
  {
    "text": "OK? And I just want\nto highlight here, there's a bunch of different\nchoices to be made. You have to decide, what\nfunction approximator are you",
    "start": "4294950",
    "end": "4301620"
  },
  {
    "text": "using? Are you using a\ndeep neural network? What's your learning rate? How often to update\nthe target weight?",
    "start": "4301620",
    "end": "4307619"
  },
  {
    "text": "How big should your\nreplay buffer be? There's a lot of different\nchoices that you have to make. ",
    "start": "4307620",
    "end": "4315070"
  },
  {
    "text": "All right. Let's just take a quick second\nhere to see if this part's-- like a chance to\nthink about this part.",
    "start": "4315070",
    "end": "4322730"
  },
  {
    "text": "You may have just seen\nthe answer, but that's OK. Which is-- OK, in\nDQN, we're going to compute the target value\nfor the sampled state action",
    "start": "4322730",
    "end": "4329390"
  },
  {
    "text": "reward next states using a\nseparate set of target weights. So does that change\nthe computation time?",
    "start": "4329390",
    "end": "4335580"
  },
  {
    "text": "Does it change the\nmemory requirements? Are you not sure?",
    "start": "4335580",
    "end": "4341400"
  },
  {
    "text": "Put that in here. ",
    "start": "4341400",
    "end": "4346800"
  },
  {
    "text": "We're now going to maintain\ntwo different sets of weights to do our function\napproximation. ",
    "start": "4346800",
    "end": "4410100"
  },
  {
    "text": "All right. Yep. I see almost everyone\nconverged to the right answer",
    "start": "4410100",
    "end": "4415210"
  },
  {
    "text": "very quickly. It is doubling the\nmemory requirements. So you have to keep track of\na second set of parameters.",
    "start": "4415210",
    "end": "4421860"
  },
  {
    "text": "It does not change\nthe computation time. It just changes the\nmemory requirements. So we just keep\naround two copies",
    "start": "4421860",
    "end": "4427825"
  },
  {
    "text": "of your deep neural network,\none with the old weights, one with the new ones. And then the Q-updating with\nrespect to that is the same.",
    "start": "4427825",
    "end": "4435130"
  },
  {
    "text": "All right, let's see\nwhat that actually does. So the kind of the key\ninnovations for DQN where we are going to use deep\nneural networks that had been",
    "start": "4435130",
    "end": "4442270"
  },
  {
    "text": "done before, but not with-- I think this is the\nfirst really big example with convolutional\nneural networks.",
    "start": "4442270",
    "end": "4448750"
  },
  {
    "text": "It's going to maintain these\nreally large episodic replays. And then it is also going\nto have these fixed targets.",
    "start": "4448750",
    "end": "4456890"
  },
  {
    "text": "All right. So what they have\nhere is they're going to do these series\nof convolutions, output.",
    "start": "4456890",
    "end": "4462170"
  },
  {
    "text": "And they're going to output\na Q value for each action. And they're going to use\nthat to make decisions.",
    "start": "4462170",
    "end": "4467510"
  },
  {
    "text": "And I think one of\nthe things-- well, there's multiple really\nremarkable things about this paper. One is that they got\nextremely good performance",
    "start": "4467510",
    "end": "4474289"
  },
  {
    "text": "across a really\nwide set of games. So instead of only having\na few benchmark tasks, they looked at the whole\nsuite of performance.",
    "start": "4474290",
    "end": "4480320"
  },
  {
    "text": "They are learning a different\npolicy per video game, but it is the same neural\nnetwork architecture,",
    "start": "4480320",
    "end": "4485750"
  },
  {
    "text": "and I believe, all the\nsame hyperparameters too. So the idea with\nthat is to say like, could we actually\nhave the same type",
    "start": "4485750",
    "end": "4491570"
  },
  {
    "text": "of architecture in the same\nway that we don't swap brains when we do different tasks,\nbut have the same learning",
    "start": "4491570",
    "end": "4497900"
  },
  {
    "text": "algorithm, learn\nto be able to do many different types of tasks? And so I think that was\npretty impressive that they",
    "start": "4497900",
    "end": "4503180"
  },
  {
    "text": "showed that that was possible. So you have the same algorithm,\nsame hyperparameters, but it could learn to do\nwell in many different tasks.",
    "start": "4503180",
    "end": "4510812"
  },
  {
    "text": "I think one of the interesting\nthings about the paper is to consider what\nwere the aspects that were important for success.",
    "start": "4510812",
    "end": "4516990"
  },
  {
    "text": "So here's just a\nsubset of algorithms-- or sorry, a subset\nof the domains. This is a few of the games.",
    "start": "4516990",
    "end": "4523500"
  },
  {
    "text": "And they also compared to using\na much more simple function approximator. And what you can see here is\nthat the deep neural network",
    "start": "4523500",
    "end": "4531780"
  },
  {
    "text": "is not actually better, right? Like the deep neural\nnetwork does not look better than the linear case.",
    "start": "4531780",
    "end": "4538980"
  },
  {
    "text": "So it's not clear\nthat just using a more function approximate--\nlike it wasn't just that they used a much more\ncareful function approximator.",
    "start": "4538980",
    "end": "4548520"
  },
  {
    "text": "And the second thing was\nwhether they use this fixed Q, and that helped.",
    "start": "4548520",
    "end": "4553739"
  },
  {
    "text": "So you can see now\nthat they are exceeding the performance of using a more\nsimple function approximator.",
    "start": "4553740",
    "end": "4559060"
  },
  {
    "text": "So this idea of\nkeeping things stable is helpful in terms\nof oscillations.",
    "start": "4559060",
    "end": "4565020"
  },
  {
    "text": "But using the replay\nwas incredibly helpful. So they went from 3 or 10\nup to 241, or in some--",
    "start": "4565020",
    "end": "4575699"
  },
  {
    "text": "something from either\nroughly three times as good and sometimes even\nmore like a couple orders of magnitude.",
    "start": "4575700",
    "end": "4581520"
  },
  {
    "text": "So it was incredibly helpful to\nuse an experience replay buffer. And maybe this\nisn't so surprising,",
    "start": "4581520",
    "end": "4587760"
  },
  {
    "text": "because it means that they are\njust reusing their data lot. But it was really,\nincredibly important.",
    "start": "4587760",
    "end": "4594395"
  },
  {
    "text": "And I think that's\nreally helpful to motivate why thinking\nabout sample efficiency and reusing your\ndata is helpful.",
    "start": "4594395",
    "end": "4601030"
  },
  {
    "text": "And then combining these ideas\nled to even bigger benefits. So it was helpful to have both\nthe fixed targets and the replay",
    "start": "4601030",
    "end": "4607199"
  },
  {
    "text": "buffer. But if you could only pick\none, the replay buffer was just enormously helpful.",
    "start": "4607200",
    "end": "4613310"
  },
  {
    "text": "All right. So as you guys know, there's\nbeen an enormous amount of interest in\nreinforcement learning",
    "start": "4613310",
    "end": "4618710"
  },
  {
    "text": "and deep reinforcement\nlearning since. There was some immediate\nimprovements kind of within the next year or two.",
    "start": "4618710",
    "end": "4624470"
  },
  {
    "text": "One is called Double DQN. And that also is a\nvery simple change.",
    "start": "4624470",
    "end": "4630750"
  },
  {
    "text": "It's maybe one or two lines. And it does increase\nsome of the requirements. But for memory, but it is\na really helpful approach.",
    "start": "4630750",
    "end": "4641100"
  },
  {
    "text": "So it tries to\ndeal with the fact that you can get some\ninteresting maximization bias issues. And happy to talk\nabout that offline.",
    "start": "4641100",
    "end": "4647630"
  },
  {
    "text": "So there is a few different\nimmediate next algorithms. But then there's been an\nenormous amount of work since. And I think it really\nled to a huge excitement",
    "start": "4647630",
    "end": "4654200"
  },
  {
    "text": "in how we could couple these\nwith really impressive function approximators.",
    "start": "4654200",
    "end": "4659719"
  },
  {
    "text": "So just to summarize, the things\nthat you should understand is to be able to implement\nTD 0 of Monte Carlo",
    "start": "4659720",
    "end": "4664970"
  },
  {
    "text": "on-policy evaluation, so things\nlike we talked about last time. You should be able to implement\nQ-learning, SARSA, and MC",
    "start": "4664970",
    "end": "4670580"
  },
  {
    "text": "control algorithms, again\nin tabular settings. You should understand,\nwhat are the issues that",
    "start": "4670580",
    "end": "4676040"
  },
  {
    "text": "can cause instability-- so things like function\napproximation, bootstrapping and off-policy learning, and\nhave an intuitive sense for why",
    "start": "4676040",
    "end": "4684050"
  },
  {
    "text": "that might be concerning. And then also you should know\nsome of the key features in DQN that were critical.",
    "start": "4684050",
    "end": "4689864"
  },
  {
    "text": "And then next week\nwe're going to start to talk about a\nvery different way to do things, which is just\npolicy gradient methods.",
    "start": "4689865",
    "end": "4695520"
  },
  {
    "text": "It is similar again to this. You can see how important\npolicy iteration is. It's going to be similar\nto policy iteration",
    "start": "4695520",
    "end": "4701900"
  },
  {
    "text": "and kind of similar to\npolicy iteration of Monte Carlo in certain\nways and directly trying to work with the policy.",
    "start": "4701900",
    "end": "4707840"
  },
  {
    "text": "I'll see you then. ",
    "start": "4707840",
    "end": "4714000"
  }
]