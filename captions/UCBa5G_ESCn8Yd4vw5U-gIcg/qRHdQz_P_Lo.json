[
  {
    "start": "0",
    "end": "0"
  },
  {
    "text": "The last topic we're\ngoing to discuss",
    "start": "0",
    "end": "1790"
  },
  {
    "text": "is a pretty hot\ntopic today, and it's",
    "start": "1790",
    "end": "5240"
  },
  {
    "text": "arisen in the last\nfew years and it goes",
    "start": "5240",
    "end": "8660"
  },
  {
    "text": "by the name of double descent.",
    "start": "8660",
    "end": "11430"
  },
  {
    "text": "And just to set the stage,\nso with neural networks,",
    "start": "11430",
    "end": "15900"
  },
  {
    "text": "it seems that it's better to\nhave too many hidden units",
    "start": "15900",
    "end": "19140"
  },
  {
    "text": "than too few.",
    "start": "19140",
    "end": "20699"
  },
  {
    "text": "Likewise, more hidden\nlayers seem better than few.",
    "start": "20700",
    "end": "25020"
  },
  {
    "text": "Running stochastic gradient\ndescent until 0 training error",
    "start": "25020",
    "end": "28710"
  },
  {
    "text": "often gives good\nout-of-sample error.",
    "start": "28710",
    "end": "31109"
  },
  {
    "text": "In fact, in many\nof these networks",
    "start": "31110",
    "end": "32700"
  },
  {
    "text": "like image with signal\nto noise ratio is high,",
    "start": "32700",
    "end": "35350"
  },
  {
    "text": "that's what's done.",
    "start": "35350",
    "end": "36280"
  },
  {
    "text": "They just run all the way\ntill you get 0 training error.",
    "start": "36280",
    "end": "39059"
  },
  {
    "text": "And number of parameters in\nthese networks are enormous.",
    "start": "39060",
    "end": "43140"
  },
  {
    "text": "Increase in the number of units\nor layers and again, training",
    "start": "43140",
    "end": "46200"
  },
  {
    "text": "until 0 error sometimes give\neven better out-of-sample error.",
    "start": "46200",
    "end": "49670"
  },
  {
    "text": "So all this sort of\nlays out the fact",
    "start": "49670",
    "end": "52649"
  },
  {
    "text": "that it seems like\nneural networks are",
    "start": "52650",
    "end": "55410"
  },
  {
    "text": "reluctant to overfit and you\ncould put as many parameters",
    "start": "55410",
    "end": "58110"
  },
  {
    "text": "as you like in the model.",
    "start": "58110",
    "end": "59610"
  },
  {
    "text": "So what happened to overfitting\nin the usual bias-variance",
    "start": "59610",
    "end": "63840"
  },
  {
    "text": "trade-off?",
    "start": "63840",
    "end": "65128"
  },
  {
    "text": "And so this is set\noff a firestorm",
    "start": "65129",
    "end": "68430"
  },
  {
    "text": "of papers and research.",
    "start": "68430",
    "end": "70110"
  },
  {
    "text": "And one of the original\npapers is referenced here",
    "start": "70110",
    "end": "73450"
  },
  {
    "text": "and the title is Reconciling\nModern Machine Learning",
    "start": "73450",
    "end": "76289"
  },
  {
    "text": "and the Bias-Variance Trade-Off.",
    "start": "76290",
    "end": "78180"
  },
  {
    "text": "Basically, the message from\nthese papers in this research",
    "start": "78180",
    "end": "82290"
  },
  {
    "text": "is that the bias-variance\ntrade-off is all",
    "start": "82290",
    "end": "84060"
  },
  {
    "text": "wrong in modern settings.",
    "start": "84060",
    "end": "86110"
  },
  {
    "text": "We've had people talk\nabout both of our books",
    "start": "86110",
    "end": "88090"
  },
  {
    "text": "and the u-shaped error\ncurve for test error",
    "start": "88090",
    "end": "91340"
  },
  {
    "text": "and saying it's\nwrong, that doesn't",
    "start": "91340",
    "end": "93729"
  },
  {
    "text": "apply to neural networks.",
    "start": "93730",
    "end": "96160"
  },
  {
    "text": "Let's see.",
    "start": "96160",
    "end": "96910"
  },
  {
    "text": "So Rob and I and others\nhave worked on this,",
    "start": "96910",
    "end": "102760"
  },
  {
    "text": "and that's why we've added\na section in the book",
    "start": "102760",
    "end": "105760"
  },
  {
    "text": "on double descent, because many\npeople have now heard of it",
    "start": "105760",
    "end": "109880"
  },
  {
    "start": "108000",
    "end": "108000"
  },
  {
    "text": "and we want to give\nour point of view.",
    "start": "109880",
    "end": "112509"
  },
  {
    "text": "And to do that, we're going\nto do a little simulation.",
    "start": "112510",
    "end": "116170"
  },
  {
    "text": "Often, simulations are helpful\nfor understanding phenomenon.",
    "start": "116170",
    "end": "119360"
  },
  {
    "text": "So this is very simple.",
    "start": "119360",
    "end": "120760"
  },
  {
    "text": "We're going to generate\ndata from a sine curve.",
    "start": "120760",
    "end": "124370"
  },
  {
    "text": "And the feature x is going\nto be uniformly distributed",
    "start": "124370",
    "end": "127930"
  },
  {
    "text": "on minus 5 to 5,\nand the error is",
    "start": "127930",
    "end": "131240"
  },
  {
    "text": "going to be Gaussian with\nstandard deviation 0.3.",
    "start": "131240",
    "end": "134370"
  },
  {
    "text": "So you can do the\nsimulation yourself.",
    "start": "134370",
    "end": "137060"
  },
  {
    "text": "We have a small\ntraining set of size 20,",
    "start": "137060",
    "end": "140069"
  },
  {
    "text": "and we have a very\nlarge test set.",
    "start": "140070",
    "end": "142140"
  },
  {
    "text": "And by the way, if\nyou do a simulation,",
    "start": "142140",
    "end": "144260"
  },
  {
    "text": "there is no value in\nhaving a small test set.",
    "start": "144260",
    "end": "147120"
  },
  {
    "text": "Make it really large.",
    "start": "147120",
    "end": "148409"
  },
  {
    "text": "You want to find out exactly\nwhat the test error is.",
    "start": "148410",
    "end": "151460"
  },
  {
    "text": "So we're going to fit a\nnatural spline to the data.",
    "start": "151460",
    "end": "154350"
  },
  {
    "text": "And if you've forgotten\nabout natural splines,",
    "start": "154350",
    "end": "157850"
  },
  {
    "text": "these are in section\n7.4 of the book,",
    "start": "157850",
    "end": "161420"
  },
  {
    "text": "and it's a way of fitting\na flexible function.",
    "start": "161420",
    "end": "164120"
  },
  {
    "text": "And there's lectures as well on\nnatural splines in the series.",
    "start": "164120",
    "end": "168200"
  },
  {
    "text": "And we're going to fit\nwith degrees of freedom.",
    "start": "168200",
    "end": "171140"
  },
  {
    "text": "So it's going to be--",
    "start": "171140",
    "end": "172730"
  },
  {
    "text": "what that means is\nyou're going to be",
    "start": "172730",
    "end": "174590"
  },
  {
    "text": "fitting a linear regression\nonto d basis functions.",
    "start": "174590",
    "end": "178290"
  },
  {
    "text": "So your prediction is\ngoing to be of the form",
    "start": "178290",
    "end": "181129"
  },
  {
    "text": "a linear combination\nof these basis",
    "start": "181130",
    "end": "183890"
  },
  {
    "text": "functions with d parameters.",
    "start": "183890",
    "end": "186860"
  },
  {
    "text": "And what we're going\nto change is d.",
    "start": "186860",
    "end": "188990"
  },
  {
    "text": "So we're going to enrich\nthe spaces by increasing d",
    "start": "188990",
    "end": "192830"
  },
  {
    "text": "and see what happens.",
    "start": "192830",
    "end": "194250"
  },
  {
    "text": "So when d is 20, you're going\nto have exactly the same number",
    "start": "194250",
    "end": "197570"
  },
  {
    "text": "of features as you've got\ntraining observations, right?",
    "start": "197570",
    "end": "201210"
  },
  {
    "text": "And because these basis\nfunctions are all different,",
    "start": "201210",
    "end": "204830"
  },
  {
    "text": "you're going to fit the\ntraining data exactly",
    "start": "204830",
    "end": "207500"
  },
  {
    "text": "and all the residuals\nare going to equal to 0.",
    "start": "207500",
    "end": "210830"
  },
  {
    "text": "But now we're going\nto even go further.",
    "start": "210830",
    "end": "212640"
  },
  {
    "text": "We're going to let d\nget bigger than 20.",
    "start": "212640",
    "end": "214770"
  },
  {
    "text": "So we need to say what\nwe're going to do there.",
    "start": "214770",
    "end": "217290"
  },
  {
    "text": "Well, what we do is when\nd is bigger than 20,",
    "start": "217290",
    "end": "221719"
  },
  {
    "text": "you can still get 0\nresidual solutions,",
    "start": "221720",
    "end": "224540"
  },
  {
    "text": "but they're not\ngoing to be unique.",
    "start": "224540",
    "end": "226370"
  },
  {
    "text": "At exactly 20, you'll\nget a single solution",
    "start": "226370",
    "end": "229080"
  },
  {
    "text": "but when d is bigger than\n20, there's infinitely",
    "start": "229080",
    "end": "231500"
  },
  {
    "text": "many solutions with 0 residual.",
    "start": "231500",
    "end": "233640"
  },
  {
    "text": "So you've got to\npick amongst them.",
    "start": "233640",
    "end": "235580"
  },
  {
    "text": "So among the 0\nresidual solutions,",
    "start": "235580",
    "end": "238400"
  },
  {
    "text": "we'll pick the one with\nwhat's known as minimum norm.",
    "start": "238400",
    "end": "242920"
  },
  {
    "text": "I.e.",
    "start": "242920",
    "end": "243520"
  },
  {
    "text": "the 0 residual solution with\nthe smallest value of sum",
    "start": "243520",
    "end": "246700"
  },
  {
    "text": "of squares of beta j squared.",
    "start": "246700",
    "end": "249069"
  },
  {
    "text": "And we'll see why that--",
    "start": "249070",
    "end": "251150"
  },
  {
    "text": "how that plays a role.",
    "start": "251150",
    "end": "254000"
  },
  {
    "text": "And we'll also explain\nin a little while",
    "start": "254000",
    "end": "256370"
  },
  {
    "text": "why we set up this particular\nsimulation in this kind.",
    "start": "256370",
    "end": "261500"
  },
  {
    "start": "259000",
    "end": "259000"
  },
  {
    "text": "So here's a result, and this\nshows the double descent curve.",
    "start": "261500",
    "end": "265820"
  },
  {
    "text": "So in this picture, we show\non the horizontal axis,",
    "start": "265820",
    "end": "270140"
  },
  {
    "text": "the degrees of freedom or d\nas we add basis functions.",
    "start": "270140",
    "end": "276520"
  },
  {
    "text": "The vertical axis,\nwe show the error.",
    "start": "276520",
    "end": "279550"
  },
  {
    "text": "The test error is in blue and\nthe training error is in orange.",
    "start": "279550",
    "end": "283270"
  },
  {
    "text": "So sure enough, you can see\nthe training error that's",
    "start": "283270",
    "end": "285550"
  },
  {
    "text": "over the 20 observations.",
    "start": "285550",
    "end": "287620"
  },
  {
    "text": "As d increases, it drops\nand drops and drops",
    "start": "287620",
    "end": "290440"
  },
  {
    "text": "and at D equals 20, it\nhits 0 and then of course,",
    "start": "290440",
    "end": "294040"
  },
  {
    "text": "thereafter it's 0.",
    "start": "294040",
    "end": "296240"
  },
  {
    "text": "And the degrees of freedom\nwe're shown on the log scale,",
    "start": "296240",
    "end": "299520"
  },
  {
    "text": "and so you can see things\nget compressed up here.",
    "start": "299520",
    "end": "303080"
  },
  {
    "text": "The test error is what\nis interesting here.",
    "start": "303080",
    "end": "305550"
  },
  {
    "text": "The test error\ninitially drops and then",
    "start": "305550",
    "end": "309349"
  },
  {
    "text": "starts increasing as we start\noverfitting the training data.",
    "start": "309350",
    "end": "313640"
  },
  {
    "text": "And this part shows a usual\nbias variance trade off.",
    "start": "313640",
    "end": "319480"
  },
  {
    "text": "Initially, error is high\nbecause of bias, drops down,",
    "start": "319480",
    "end": "322280"
  },
  {
    "text": "and then error starts\nincreasing because of variance.",
    "start": "322280",
    "end": "324980"
  },
  {
    "text": "And it literally shoots\nthrough the ceiling here.",
    "start": "324980",
    "end": "327830"
  },
  {
    "text": "But then something\ninteresting happens.",
    "start": "327830",
    "end": "330479"
  },
  {
    "text": "The error starts\ndecreasing again.",
    "start": "330480",
    "end": "332520"
  },
  {
    "text": "And so this is the\ndouble descent.",
    "start": "332520",
    "end": "335419"
  },
  {
    "text": "And it starts decreasing,\nreaches a minimum,",
    "start": "335420",
    "end": "338120"
  },
  {
    "text": "and then it seems like it\nstarts increasing again.",
    "start": "338120",
    "end": "341120"
  },
  {
    "text": "So what happens as\nd increases above 20",
    "start": "341120",
    "end": "346419"
  },
  {
    "text": "is the sum of squares\nof the coefficients,",
    "start": "346420",
    "end": "350770"
  },
  {
    "text": "even though there are more\ncoefficients, the sum of squares",
    "start": "350770",
    "end": "353910"
  },
  {
    "text": "decreases.",
    "start": "353910",
    "end": "356460"
  },
  {
    "text": "Because we have\nmore opportunities",
    "start": "356460",
    "end": "358770"
  },
  {
    "text": "to fit the data\nexactly, and so we",
    "start": "358770",
    "end": "361470"
  },
  {
    "text": "can find a\nconfiguration of betas",
    "start": "361470",
    "end": "363990"
  },
  {
    "text": "that have a smaller\nsum of squares.",
    "start": "363990",
    "end": "367500"
  },
  {
    "text": "So let's say we go from d\nis 25 to d is 40, right?",
    "start": "367500",
    "end": "372840"
  },
  {
    "text": "One candidate is the solution\nwe got at d equals 25.",
    "start": "372840",
    "end": "378650"
  },
  {
    "text": "But we've got lots of other\ncandidates because we've got 40.",
    "start": "378650",
    "end": "381740"
  },
  {
    "text": "And we could set the\nremaining ones to 0.",
    "start": "381740",
    "end": "383699"
  },
  {
    "text": "So that's a candidate.",
    "start": "383700",
    "end": "385020"
  },
  {
    "text": "So we can only do\nbetter if we're",
    "start": "385020",
    "end": "387229"
  },
  {
    "text": "trying to minimize the sum of\nsquares of the betas having",
    "start": "387230",
    "end": "389960"
  },
  {
    "text": "more betas to do it.",
    "start": "389960",
    "end": "392069"
  },
  {
    "text": "And it turns out that\nwhat that does is--",
    "start": "392070",
    "end": "394880"
  },
  {
    "text": "by making the sum of\nsquares of the betas small,",
    "start": "394880",
    "end": "397280"
  },
  {
    "text": "that means all the betas\nare getting smaller.",
    "start": "397280",
    "end": "400040"
  },
  {
    "text": "We're going to have\nless wiggly solutions.",
    "start": "400040",
    "end": "402740"
  },
  {
    "text": "We're getting smaller.",
    "start": "402740",
    "end": "403656"
  },
  {
    "text": "I guess, they're being spread\nout over more functions.",
    "start": "403657",
    "end": "405907"
  },
  {
    "text": "Right.",
    "start": "405907",
    "end": "406430"
  },
  {
    "start": "406000",
    "end": "406000"
  },
  {
    "text": "Yeah.",
    "start": "406430",
    "end": "406940"
  },
  {
    "text": "Spread out over more functions.",
    "start": "406940",
    "end": "408840"
  },
  {
    "text": "And so this picture\nshows what's happening.",
    "start": "408840",
    "end": "412070"
  },
  {
    "text": "So here's 8 degrees\nof freedom, which--",
    "start": "412070",
    "end": "415280"
  },
  {
    "text": "let's see, 8 degrees of\nfreedom, that's around here.",
    "start": "415280",
    "end": "417820"
  },
  {
    "text": "That's a pretty good solution.",
    "start": "417820",
    "end": "419070"
  },
  {
    "text": "That's about as good\nas we get, right?",
    "start": "419070",
    "end": "423000"
  },
  {
    "text": "And you can see it's getting\na nice approximating solution.",
    "start": "423000",
    "end": "426670"
  },
  {
    "text": "There's 20 degrees of freedom.",
    "start": "426670",
    "end": "428580"
  },
  {
    "text": "This is the one.",
    "start": "428580",
    "end": "430889"
  },
  {
    "text": "So what we're showing here\nis the true sine wave,",
    "start": "430890",
    "end": "433780"
  },
  {
    "text": "the actual data\npoints, 20 data points,",
    "start": "433780",
    "end": "438120"
  },
  {
    "text": "and we've shown\nthe fitted curve.",
    "start": "438120",
    "end": "440669"
  },
  {
    "text": "And we're showing the\nfitted curve everywhere.",
    "start": "440670",
    "end": "443690"
  },
  {
    "text": "So 20, of course,\nthe fitted curve",
    "start": "443690",
    "end": "446000"
  },
  {
    "text": "has to go through every\ndata point, which it does.",
    "start": "446000",
    "end": "448830"
  },
  {
    "text": "The observed data points.",
    "start": "448830",
    "end": "450470"
  },
  {
    "text": "But what you see is\nin order to do that",
    "start": "450470",
    "end": "453830"
  },
  {
    "text": "with exactly 20\ndegrees of freedom,",
    "start": "453830",
    "end": "455780"
  },
  {
    "text": "it has to really stretch itself.",
    "start": "455780",
    "end": "457730"
  },
  {
    "text": "And the function goes shooting\noff in all different places",
    "start": "457730",
    "end": "460730"
  },
  {
    "text": "elsewhere because the loss is\nonly concerned with the training",
    "start": "460730",
    "end": "464000"
  },
  {
    "text": "data.",
    "start": "464000",
    "end": "466070"
  },
  {
    "text": "So it really has to stretch\nitself to fit the data exactly.",
    "start": "466070",
    "end": "470530"
  },
  {
    "text": "But when you go to 42\ndegrees of freedom,",
    "start": "470530",
    "end": "472810"
  },
  {
    "text": "for example, which\nis where we achieved",
    "start": "472810",
    "end": "475930"
  },
  {
    "text": "the minimum to the\nright of the 20 point,",
    "start": "475930",
    "end": "479889"
  },
  {
    "text": "you can see the function\nis much better behaved.",
    "start": "479890",
    "end": "482230"
  },
  {
    "text": "These departures, it's a\nmuch smoother function.",
    "start": "482230",
    "end": "484310"
  },
  {
    "text": "It's still making little jumps\nin that, but nothing nearly",
    "start": "484310",
    "end": "486727"
  },
  {
    "text": "as severe as that.",
    "start": "486727",
    "end": "488310"
  },
  {
    "text": "And that's because\nall these are smaller.",
    "start": "488310",
    "end": "490919"
  },
  {
    "text": "And likewise, as you go\nup to 80, not too much",
    "start": "490920",
    "end": "493500"
  },
  {
    "text": "has changed over there.",
    "start": "493500",
    "end": "496281"
  },
  {
    "text": "So that's an explanation\nfor what's going on",
    "start": "496281",
    "end": "499080"
  },
  {
    "text": "in this little example.",
    "start": "499080",
    "end": "501150"
  },
  {
    "start": "500000",
    "end": "500000"
  },
  {
    "text": "So here's some facts.",
    "start": "501150",
    "end": "502919"
  },
  {
    "text": "So in a wide linear\nmodel, in other words,",
    "start": "502920",
    "end": "505710"
  },
  {
    "text": "p is much bigger\nthan n, and you fit",
    "start": "505710",
    "end": "509160"
  },
  {
    "text": "by least squares using\nstochastic gradient descent",
    "start": "509160",
    "end": "512280"
  },
  {
    "text": "with a small step\nsize and you keep",
    "start": "512280",
    "end": "515130"
  },
  {
    "text": "on going, that leads to a\nminimum norm residual solution.",
    "start": "515130",
    "end": "519840"
  },
  {
    "text": "So this ties stochastic\ngradient descent",
    "start": "519840",
    "end": "523380"
  },
  {
    "text": "and going all the\nway to the solution,",
    "start": "523380",
    "end": "525870"
  },
  {
    "text": "to the minimum 0\nresidual solution,",
    "start": "525870",
    "end": "528680"
  },
  {
    "text": "it actually gets the\nminimum norm solution",
    "start": "528680",
    "end": "534870"
  },
  {
    "text": "with a minimum sum of\nsquares of the betas.",
    "start": "534870",
    "end": "538710"
  },
  {
    "text": "And stochastic gradient descent\nis used in the neural network.",
    "start": "538710",
    "end": "541900"
  },
  {
    "text": "So by analogy, by training\nslowly to 0 residual,",
    "start": "541900",
    "end": "547050"
  },
  {
    "text": "you get in a more\nregularized solution.",
    "start": "547050",
    "end": "550310"
  },
  {
    "text": "Another way of\ntalking about this",
    "start": "550310",
    "end": "552260"
  },
  {
    "text": "is stochastic\ngradient flow, i.e.",
    "start": "552260",
    "end": "555020"
  },
  {
    "text": "the entire path of stochastic\ngradient descent solutions",
    "start": "555020",
    "end": "558740"
  },
  {
    "text": "is somewhat similar\nto the ridge path.",
    "start": "558740",
    "end": "561520"
  },
  {
    "text": "So the rich path is the\nsequence of solutions",
    "start": "561520",
    "end": "563950"
  },
  {
    "text": "you get when you slowly vary\nthe ridge regularization",
    "start": "563950",
    "end": "568060"
  },
  {
    "text": "parameter that's called the\nridge path of solutions.",
    "start": "568060",
    "end": "570890"
  },
  {
    "text": "And the stochastic\ngradient flow is",
    "start": "570890",
    "end": "574690"
  },
  {
    "text": "a sequence of solutions you get\nas you go down gradient descent.",
    "start": "574690",
    "end": "577910"
  },
  {
    "text": "They are quite similar.",
    "start": "577910",
    "end": "579350"
  },
  {
    "text": "Not exactly, but quite.",
    "start": "579350",
    "end": "581139"
  },
  {
    "text": "So by analogy, deep and\nwide neural networks",
    "start": "581140",
    "end": "583630"
  },
  {
    "text": "fit by stochastic\ngradient descent",
    "start": "583630",
    "end": "585280"
  },
  {
    "text": "down to 0 training error\noften give good solutions that",
    "start": "585280",
    "end": "588610"
  },
  {
    "text": "generalize well.",
    "start": "588610",
    "end": "590910"
  },
  {
    "text": "And in particular cases with\nhigh signal to noise ratio, e.g.",
    "start": "590910",
    "end": "595019"
  },
  {
    "text": "image recognition are less\nprone to overfitting the 0 error",
    "start": "595020",
    "end": "599100"
  },
  {
    "text": "solutions.",
    "start": "599100",
    "end": "600959"
  },
  {
    "text": "It's mostly a signal.",
    "start": "600960",
    "end": "602200"
  },
  {
    "text": "The 0 error solution\nis mostly a signal.",
    "start": "602200",
    "end": "604310"
  },
  {
    "start": "604310",
    "end": "608130"
  },
  {
    "text": "That's a little discussion\non double descent.",
    "start": "608130",
    "end": "613260"
  },
  {
    "start": "610000",
    "end": "610000"
  },
  {
    "text": "So we'll end up this just\ntalking a bit about software.",
    "start": "613260",
    "end": "616630"
  },
  {
    "text": "There's wonderful software\navailable for neural networks",
    "start": "616630",
    "end": "619260"
  },
  {
    "text": "and deep learning.",
    "start": "619260",
    "end": "620760"
  },
  {
    "text": "So TensorFlow from Google and\nPyTorch from Facebook and both",
    "start": "620760",
    "end": "625020"
  },
  {
    "text": "are Python packages.",
    "start": "625020",
    "end": "627680"
  },
  {
    "text": "So in the chapter 10 lab, we\ndemonstrate TensorFlow and Keras",
    "start": "627680",
    "end": "632390"
  },
  {
    "text": "package in R which interface\nto the Python versions",
    "start": "632390",
    "end": "636320"
  },
  {
    "text": "of these packages.",
    "start": "636320",
    "end": "637850"
  },
  {
    "text": "And so you can see the\ntextbook and online resources",
    "start": "637850",
    "end": "641870"
  },
  {
    "text": "for R Markdown and\nJupyter Notebooks",
    "start": "641870",
    "end": "644450"
  },
  {
    "text": "for these and all the labs in\nthe second edition of the book.",
    "start": "644450",
    "end": "647780"
  },
  {
    "text": "And there's a Torch package in\nR which is available as well,",
    "start": "647780",
    "end": "653460"
  },
  {
    "text": "and it implements the\nPyTorch version of a dialect",
    "start": "653460",
    "end": "658520"
  },
  {
    "text": "for fitting neural networks.",
    "start": "658520",
    "end": "660650"
  },
  {
    "text": "And the chapter 10 lab will\nbe available in this dialect",
    "start": "660650",
    "end": "664640"
  },
  {
    "text": "as well.",
    "start": "664640",
    "end": "665250"
  },
  {
    "text": "So if you watch the Resource\npage at www.statlearning.com.",
    "start": "665250",
    "end": "670500"
  },
  {
    "start": "670500",
    "end": "671000"
  }
]