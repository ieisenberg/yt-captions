[
  {
    "start": "0",
    "end": "101000"
  },
  {
    "start": "0",
    "end": "4400"
  },
  {
    "text": "CHRISTOPHER POTTS:\nWelcome back, everyone.",
    "start": "4400",
    "end": "6150"
  },
  {
    "text": "This is part 2 in our series\non methods and metrics.",
    "start": "6150",
    "end": "8470"
  },
  {
    "text": "We're going to be talking\nabout classifier metrics.",
    "start": "8470",
    "end": "11050"
  },
  {
    "text": "I'm sort of assuming that the\nmetrics I'll be discussing",
    "start": "11050",
    "end": "13480"
  },
  {
    "text": "are broadly familiar to us.",
    "start": "13480",
    "end": "15670"
  },
  {
    "text": "I think that's a chance\nfor us to step back and be",
    "start": "15670",
    "end": "18550"
  },
  {
    "text": "reflective about what values\nthese familiar metrics actually",
    "start": "18550",
    "end": "21849"
  },
  {
    "text": "encode.",
    "start": "21850",
    "end": "23020"
  },
  {
    "text": "Because that really is\nthe name of the game here.",
    "start": "23020",
    "end": "25330"
  },
  {
    "text": "No matter what kind of\ntask you're working on",
    "start": "25330",
    "end": "27250"
  },
  {
    "text": "or what the structure\nof your model",
    "start": "27250",
    "end": "28708"
  },
  {
    "text": "is like, it's just fundamentally\ntrue that different evaluation",
    "start": "28708",
    "end": "32169"
  },
  {
    "text": "metrics will encode different\nvalues, different goals",
    "start": "32170",
    "end": "35410"
  },
  {
    "text": "you have for your system, and\ndifferent kinds of hypotheses",
    "start": "35410",
    "end": "38440"
  },
  {
    "text": "that you might be pursuing.",
    "start": "38440",
    "end": "39910"
  },
  {
    "text": "You can hear in that, that\nreally fundamentally choosing",
    "start": "39910",
    "end": "43060"
  },
  {
    "text": "a metric is a crucial aspect to\nany kind of experimental work.",
    "start": "43060",
    "end": "46450"
  },
  {
    "text": "It's a fundamental step\nin how we operationalize",
    "start": "46450",
    "end": "49300"
  },
  {
    "text": "hypotheses in terms of data, and\nmodels, and model comparisons.",
    "start": "49300",
    "end": "54640"
  },
  {
    "text": "As a result, you should\nfeel free, for whatever task",
    "start": "54640",
    "end": "57550"
  },
  {
    "text": "you're working on, to motivate\nnew metrics or specific uses",
    "start": "57550",
    "end": "61090"
  },
  {
    "text": "of existing metrics,\ndepending on what",
    "start": "61090",
    "end": "63460"
  },
  {
    "text": "your actual goals for your\nexperiments actually are.",
    "start": "63460",
    "end": "67180"
  },
  {
    "text": "Relatedly, for\nestablished tasks,",
    "start": "67180",
    "end": "69130"
  },
  {
    "text": "you'll probably\nfeel some pressure",
    "start": "69130",
    "end": "71140"
  },
  {
    "text": "to use specific\nwell-established metrics.",
    "start": "71140",
    "end": "74290"
  },
  {
    "text": "But you should always,\nas a scientist,",
    "start": "74290",
    "end": "76420"
  },
  {
    "text": "feel empowered to push back\nif you feel that the accepted",
    "start": "76420",
    "end": "79810"
  },
  {
    "text": "metrics are not reflective\nof your hypothesis",
    "start": "79810",
    "end": "83140"
  },
  {
    "text": "or are distorting our\nnotions of progress somehow.",
    "start": "83140",
    "end": "86140"
  },
  {
    "text": "Because remember,\nareas of research",
    "start": "86140",
    "end": "88720"
  },
  {
    "text": "can stagnate due\nto poor metrics.",
    "start": "88720",
    "end": "90700"
  },
  {
    "text": "And so we have to be vigilant.",
    "start": "90700",
    "end": "91969"
  },
  {
    "text": "We have to be on the\nlookout for cases",
    "start": "91970",
    "end": "93700"
  },
  {
    "text": "in which the metrics\nwe've accepted",
    "start": "93700",
    "end": "95920"
  },
  {
    "text": "might be at odds with\nthe actual goals we have",
    "start": "95920",
    "end": "98492"
  },
  {
    "text": "for the research we're doing.",
    "start": "98492",
    "end": "99700"
  },
  {
    "start": "99700",
    "end": "102710"
  },
  {
    "start": "101000",
    "end": "210000"
  },
  {
    "text": "Let's begin our discussion\nof classifier metrics",
    "start": "102710",
    "end": "105020"
  },
  {
    "text": "by talking about confusion\nmatrices, a pretty fundamental",
    "start": "105020",
    "end": "108200"
  },
  {
    "text": "data structure for a\nlot of the calculations",
    "start": "108200",
    "end": "110810"
  },
  {
    "text": "that we'll perform.",
    "start": "110810",
    "end": "112280"
  },
  {
    "text": "So by convention, for\nmy confusion matrices,",
    "start": "112280",
    "end": "114440"
  },
  {
    "text": "I'll have the actual labels\ngoing across the rows here.",
    "start": "114440",
    "end": "118130"
  },
  {
    "text": "And across the columns,\nI'll have the predictions",
    "start": "118130",
    "end": "120469"
  },
  {
    "text": "from some classifier model.",
    "start": "120470",
    "end": "122280"
  },
  {
    "text": "So you can see in\nthis confusion matrix,",
    "start": "122280",
    "end": "124040"
  },
  {
    "text": "that there were 15 cases in\nwhich the model predicted",
    "start": "124040",
    "end": "127250"
  },
  {
    "text": "positive.",
    "start": "127250",
    "end": "128149"
  },
  {
    "text": "And the actual\nlabel was positive.",
    "start": "128150",
    "end": "130310"
  },
  {
    "text": "Where there-- whereas there are\n10 cases where the actual label",
    "start": "130310",
    "end": "133370"
  },
  {
    "text": "was positive and\nthe model predicted",
    "start": "133370",
    "end": "135170"
  },
  {
    "text": "negative, and so forth for the\nother values in this table.",
    "start": "135170",
    "end": "139400"
  },
  {
    "text": "I think that seems familiar.",
    "start": "139400",
    "end": "140655"
  },
  {
    "text": "It's something we\ncan take for granted.",
    "start": "140655",
    "end": "142280"
  },
  {
    "text": "But we should remember that\nbehind the scenes here,",
    "start": "142280",
    "end": "145550"
  },
  {
    "text": "a threshold was imposed\nin order to create",
    "start": "145550",
    "end": "148070"
  },
  {
    "text": "these categorical predictions.",
    "start": "148070",
    "end": "150050"
  },
  {
    "text": "By and large, classifier\nmodels that we use today",
    "start": "150050",
    "end": "153320"
  },
  {
    "text": "predict probability\ndistributions over the labels.",
    "start": "153320",
    "end": "156620"
  },
  {
    "text": "And so in order to create an\nactual categorical prediction,",
    "start": "156620",
    "end": "159650"
  },
  {
    "text": "we decided, for\nexample, that the label",
    "start": "159650",
    "end": "162200"
  },
  {
    "text": "with the maximum probability\nwould be the true one.",
    "start": "162200",
    "end": "165500"
  },
  {
    "text": "And that was-- the result\nof that decision was",
    "start": "165500",
    "end": "167810"
  },
  {
    "text": "used to aggregate this table.",
    "start": "167810",
    "end": "169620"
  },
  {
    "text": "But, of course, different\nchoices of that threshold",
    "start": "169620",
    "end": "172040"
  },
  {
    "text": "might give very\ndifferent results.",
    "start": "172040",
    "end": "174000"
  },
  {
    "text": "And there might be\ncontexts in which",
    "start": "174000",
    "end": "175640"
  },
  {
    "text": "we want to explore\nthe full range",
    "start": "175640",
    "end": "177470"
  },
  {
    "text": "of probabilistic predictions.",
    "start": "177470",
    "end": "179360"
  },
  {
    "text": "That's something I'll return to\nat the end of the screencast.",
    "start": "179360",
    "end": "183050"
  },
  {
    "text": "Final note about this.",
    "start": "183050",
    "end": "183980"
  },
  {
    "text": "It can be helpful in the\ncontext of confusion matrices",
    "start": "183980",
    "end": "186620"
  },
  {
    "text": "to add a column for what's\ncalled support, which is simply",
    "start": "186620",
    "end": "189709"
  },
  {
    "text": "the number of actual\ntrue instances",
    "start": "189710",
    "end": "192380"
  },
  {
    "text": "that fall into each class.",
    "start": "192380",
    "end": "193720"
  },
  {
    "text": "So there are 125\npositive instances",
    "start": "193720",
    "end": "196340"
  },
  {
    "text": "in this corpus, 35 negative,\nand over 1,000 that's",
    "start": "196340",
    "end": "200090"
  },
  {
    "text": "all in the neutral category.",
    "start": "200090",
    "end": "201319"
  },
  {
    "text": "And that's already illuminating\nabout how specific metrics",
    "start": "201320",
    "end": "204770"
  },
  {
    "text": "might deal with that extremely\nimbalanced vector of support",
    "start": "204770",
    "end": "208650"
  },
  {
    "text": "values.",
    "start": "208650",
    "end": "209150"
  },
  {
    "start": "209150",
    "end": "212000"
  },
  {
    "start": "210000",
    "end": "291000"
  },
  {
    "text": "So let's start with\naccuracy-- by far,",
    "start": "212000",
    "end": "213730"
  },
  {
    "text": "the most famous and familiar\nof all the classifier metrics.",
    "start": "213730",
    "end": "216970"
  },
  {
    "text": "Accuracy is simply the number\nof correct predictions divided",
    "start": "216970",
    "end": "220120"
  },
  {
    "text": "by the total number of examples.",
    "start": "220120",
    "end": "222310"
  },
  {
    "text": "In terms of our\nconfusion matrices,",
    "start": "222310",
    "end": "224200"
  },
  {
    "text": "that is just the sum\nof all the values",
    "start": "224200",
    "end": "226030"
  },
  {
    "text": "along the diagonal divided\nby the sum of all the values",
    "start": "226030",
    "end": "229240"
  },
  {
    "text": "that are in this table.",
    "start": "229240",
    "end": "231740"
  },
  {
    "text": "The bounds are 0 and 1, of\ncourse, with 0 the worst",
    "start": "231740",
    "end": "234470"
  },
  {
    "text": "and 1 the best.",
    "start": "234470",
    "end": "236150"
  },
  {
    "text": "In terms of the value\nencoded by accuracy,",
    "start": "236150",
    "end": "238760"
  },
  {
    "text": "I would say it's an attempt\nto answer the question,",
    "start": "238760",
    "end": "241349"
  },
  {
    "text": "how often is the system correct?",
    "start": "241350",
    "end": "244250"
  },
  {
    "text": "And that kind of feeds\ninto the weaknesses here.",
    "start": "244250",
    "end": "246600"
  },
  {
    "text": "So the weaknesses are,\nfirst, there's no per class",
    "start": "246600",
    "end": "248990"
  },
  {
    "text": "notions of accuracy.",
    "start": "248990",
    "end": "250490"
  },
  {
    "text": "Not directly.",
    "start": "250490",
    "end": "251510"
  },
  {
    "text": "We just get a single\nholistic number.",
    "start": "251510",
    "end": "254180"
  },
  {
    "text": "And relatedly, there is\njust a complete failure",
    "start": "254180",
    "end": "256669"
  },
  {
    "text": "to control for class size.",
    "start": "256670",
    "end": "259290"
  },
  {
    "text": "So you can see, for example,\nin this confusion matrix,",
    "start": "259290",
    "end": "261859"
  },
  {
    "text": "that performance on\nthe neutral class",
    "start": "261860",
    "end": "264319"
  },
  {
    "text": "will completely dominate\nthe accuracy values.",
    "start": "264320",
    "end": "268130"
  },
  {
    "text": "And it's to the\npoint, in this table,",
    "start": "268130",
    "end": "270020"
  },
  {
    "text": "where no matter\nhow much progress",
    "start": "270020",
    "end": "272240"
  },
  {
    "text": "we make on the positive\nand negative classes",
    "start": "272240",
    "end": "274400"
  },
  {
    "text": "because they are so much smaller\nin terms of their support",
    "start": "274400",
    "end": "277040"
  },
  {
    "text": "for neutral, that progress\nis unlikely to be reflected",
    "start": "277040",
    "end": "280940"
  },
  {
    "text": "in our accuracy values.",
    "start": "280940",
    "end": "282800"
  },
  {
    "text": "And that's why if you\nreturn to the value encoded,",
    "start": "282800",
    "end": "284970"
  },
  {
    "text": "you can see that just at\na raw fundamental level,",
    "start": "284970",
    "end": "287210"
  },
  {
    "text": "it is simply answering how\noften is the system correct?",
    "start": "287210",
    "end": "292300"
  },
  {
    "start": "291000",
    "end": "337000"
  },
  {
    "text": "Another thing to keep in mind\nis that for many classifier",
    "start": "292300",
    "end": "295830"
  },
  {
    "text": "models, the loss\nfor those models",
    "start": "295830",
    "end": "298860"
  },
  {
    "text": "is what's called the\ncross-entropy loss.",
    "start": "298860",
    "end": "300900"
  },
  {
    "text": "It's also called the negative\nlog-loss in Scikit-learn.",
    "start": "300900",
    "end": "305669"
  },
  {
    "text": "And that value is inversely\nproportional to accuracy.",
    "start": "305670",
    "end": "310680"
  },
  {
    "text": "The takeaway there\nis that even as we",
    "start": "310680",
    "end": "312870"
  },
  {
    "text": "might choose other metrics to\ncompare models and evaluate",
    "start": "312870",
    "end": "315960"
  },
  {
    "text": "models, we should keep in mind\nthat our classifiers themselves",
    "start": "315960",
    "end": "319770"
  },
  {
    "text": "are kind of engines for\ntrying to maximize accuracy.",
    "start": "319770",
    "end": "323460"
  },
  {
    "text": "And so they are likely to\ninherit whatever properties,",
    "start": "323460",
    "end": "326789"
  },
  {
    "text": "and values, and\nstrengths, and weaknesses",
    "start": "326790",
    "end": "328890"
  },
  {
    "text": "are inherent in the\naccuracy calculation, which",
    "start": "328890",
    "end": "331710"
  },
  {
    "text": "as we'll see, could be at\nodds with our actual values",
    "start": "331710",
    "end": "334840"
  },
  {
    "text": "for the system that\nwe're developing.",
    "start": "334840",
    "end": "338480"
  },
  {
    "start": "337000",
    "end": "436000"
  },
  {
    "text": "And that kind of feeds nicely\ninto precision, recall,",
    "start": "338480",
    "end": "341440"
  },
  {
    "text": "and F scores which are\nattempts to make up",
    "start": "341440",
    "end": "344590"
  },
  {
    "text": "for some of the weaknesses\nthat you see in accuracy.",
    "start": "344590",
    "end": "347380"
  },
  {
    "text": "We'll start with precision.",
    "start": "347380",
    "end": "348730"
  },
  {
    "text": "This is a per class notion.",
    "start": "348730",
    "end": "350680"
  },
  {
    "text": "For a class k, it's\nthe correct predictions",
    "start": "350680",
    "end": "352960"
  },
  {
    "text": "for k divided by the sum\nof all the guesses for k",
    "start": "352960",
    "end": "356319"
  },
  {
    "text": "that were made by your model.",
    "start": "356320",
    "end": "358900"
  },
  {
    "text": "So in terms of this\nconfusion matrix,",
    "start": "358900",
    "end": "360630"
  },
  {
    "text": "if we focus on the\npositive class here,",
    "start": "360630",
    "end": "362650"
  },
  {
    "text": "the numerator is the number\nof correct predictions",
    "start": "362650",
    "end": "364949"
  },
  {
    "text": "for that class divided by\nthe sum of all the values",
    "start": "364950",
    "end": "368370"
  },
  {
    "text": "that are in this column.",
    "start": "368370",
    "end": "369900"
  },
  {
    "text": "And for the negative class,\nwe would repeat that.",
    "start": "369900",
    "end": "372300"
  },
  {
    "text": "The numerator would be 15, and\nwe would sum over the column.",
    "start": "372300",
    "end": "375370"
  },
  {
    "text": "And finally, for neutral,\nthe numerator would be 1,000.",
    "start": "375370",
    "end": "377790"
  },
  {
    "text": "And we would again\nsum over this column.",
    "start": "377790",
    "end": "379890"
  },
  {
    "text": "And that leads to this\nvector of precision values",
    "start": "379890",
    "end": "382560"
  },
  {
    "text": "that you see along\nthe bottom here.",
    "start": "382560",
    "end": "386110"
  },
  {
    "text": "The bounds of precision\nare 0 and 1 approximately.",
    "start": "386110",
    "end": "389050"
  },
  {
    "text": "With 0 the worst and 1 the best.",
    "start": "389050",
    "end": "390550"
  },
  {
    "text": "There is an important\ncaveat here though.",
    "start": "390550",
    "end": "392919"
  },
  {
    "text": "Precision is technically\nundefined in situations",
    "start": "392920",
    "end": "396370"
  },
  {
    "text": "where a model makes no\npredictions about a given",
    "start": "396370",
    "end": "398510"
  },
  {
    "text": "class.",
    "start": "398510",
    "end": "399010"
  },
  {
    "text": "Because in that situation,\nyou're dividing by 0.",
    "start": "399010",
    "end": "402160"
  },
  {
    "text": "And that's\ntechnically undefined.",
    "start": "402160",
    "end": "404110"
  },
  {
    "text": "It is common practice\nto map those to 0,",
    "start": "404110",
    "end": "406802"
  },
  {
    "text": "but we should keep\nin mind that we",
    "start": "406803",
    "end": "408220"
  },
  {
    "text": "are making that extra decision.",
    "start": "408220",
    "end": "410440"
  },
  {
    "text": "The value encoded is a\nkind of conservative one.",
    "start": "410440",
    "end": "413590"
  },
  {
    "text": "We're going to penalize\nincorrect guesses",
    "start": "413590",
    "end": "416020"
  },
  {
    "text": "for a certain class.",
    "start": "416020",
    "end": "418509"
  },
  {
    "text": "So you can imagine that a\nfailure mode there is to just",
    "start": "418510",
    "end": "421750"
  },
  {
    "text": "rarely guess a certain class.",
    "start": "421750",
    "end": "423340"
  },
  {
    "text": "That is the core weakness.",
    "start": "423340",
    "end": "424750"
  },
  {
    "text": "You can achieve high\nprecision for a class k",
    "start": "424750",
    "end": "427890"
  },
  {
    "text": "simply by rarely guessing k.",
    "start": "427890",
    "end": "430510"
  },
  {
    "text": "So we'll obviously\nneed to offset",
    "start": "430510",
    "end": "432010"
  },
  {
    "text": "that with some other pressure.",
    "start": "432010",
    "end": "433690"
  },
  {
    "text": "And by and large, the\noffset pressure is recall.",
    "start": "433690",
    "end": "437480"
  },
  {
    "text": "Recall is, again,\na per class notion.",
    "start": "437480",
    "end": "439450"
  },
  {
    "text": "For a class k, it's going to be\nthe correct predictions for k",
    "start": "439450",
    "end": "442540"
  },
  {
    "text": "divided by the sum of all\nthe true members of k.",
    "start": "442540",
    "end": "446110"
  },
  {
    "text": "So now we're going\nto operate row--",
    "start": "446110",
    "end": "448090"
  },
  {
    "text": "rowwise.",
    "start": "448090",
    "end": "449169"
  },
  {
    "text": "We focus on the positive class.",
    "start": "449170",
    "end": "450880"
  },
  {
    "text": "Our numerator is 15, the\nnumber of true predictions",
    "start": "450880",
    "end": "453670"
  },
  {
    "text": "for the positive class, divided\nby the sum of all the values",
    "start": "453670",
    "end": "457000"
  },
  {
    "text": "along the rows.",
    "start": "457000",
    "end": "458200"
  },
  {
    "text": "That is all the true members\nof that class for positive.",
    "start": "458200",
    "end": "461410"
  },
  {
    "text": "That gives us a\nrecall value of 0.12.",
    "start": "461410",
    "end": "463990"
  },
  {
    "text": "And we can repeat that\nfor the other two rows.",
    "start": "463990",
    "end": "467280"
  },
  {
    "text": "The bounds of 0 and 1.",
    "start": "467280",
    "end": "468710"
  },
  {
    "text": "0 the worst and 1 the best.",
    "start": "468710",
    "end": "470910"
  },
  {
    "text": "The value encoded\nis a permissive one.",
    "start": "470910",
    "end": "473250"
  },
  {
    "text": "We want to penalize\nmissed true cases.",
    "start": "473250",
    "end": "476820"
  },
  {
    "text": "We would like to make a lot\nof predictions about a class",
    "start": "476820",
    "end": "479340"
  },
  {
    "text": "in order to avoid leaving\nany out, so to speak.",
    "start": "479340",
    "end": "482400"
  },
  {
    "text": "And that leads into\nthe core weakness.",
    "start": "482400",
    "end": "484290"
  },
  {
    "text": "We can achieve higher\nrecall for a class k",
    "start": "484290",
    "end": "487840"
  },
  {
    "text": "simply by always guessing k.",
    "start": "487840",
    "end": "489840"
  },
  {
    "text": "Never mind the mistakes.",
    "start": "489840",
    "end": "491190"
  },
  {
    "text": "As long as we get all the actual\ncases into our predictions,",
    "start": "491190",
    "end": "494280"
  },
  {
    "text": "we're doing well by recall.",
    "start": "494280",
    "end": "495893"
  },
  {
    "text": "And you can hear\nin that that it's",
    "start": "495893",
    "end": "497310"
  },
  {
    "text": "important to offset this\npressure by something else.",
    "start": "497310",
    "end": "500490"
  },
  {
    "text": "And that's standard precision.",
    "start": "500490",
    "end": "503500"
  },
  {
    "text": "And the way we offset\nthese two pressures",
    "start": "503500",
    "end": "505450"
  },
  {
    "text": "is typically with F scores.",
    "start": "505450",
    "end": "507550"
  },
  {
    "start": "507000",
    "end": "609000"
  },
  {
    "text": "So F scores are a harmonic mean\nof the precision and recall",
    "start": "507550",
    "end": "511240"
  },
  {
    "text": "scores.",
    "start": "511240",
    "end": "511759"
  },
  {
    "text": "It's again a per class notion.",
    "start": "511760",
    "end": "513580"
  },
  {
    "text": "And it has this\nweighting value, beta.",
    "start": "513580",
    "end": "515620"
  },
  {
    "text": "If we want to evenly balance\nprecision and recall,",
    "start": "515620",
    "end": "518500"
  },
  {
    "text": "then we set beta to 1.",
    "start": "518500",
    "end": "521400"
  },
  {
    "text": "So here's that\nconfusion matrix again.",
    "start": "521400",
    "end": "523349"
  },
  {
    "text": "And along this column here,\nI've given the per class",
    "start": "523350",
    "end": "525690"
  },
  {
    "text": "F1 values here.",
    "start": "525690",
    "end": "528320"
  },
  {
    "text": "The bounds are 0 and\n1 as before with 0",
    "start": "528320",
    "end": "531200"
  },
  {
    "text": "the worst and 1 the best.",
    "start": "531200",
    "end": "532700"
  },
  {
    "text": "And you can count on the fact\nthat the F1 score for a class",
    "start": "532700",
    "end": "535580"
  },
  {
    "text": "will always fall between the\nprecision and recall classes,",
    "start": "535580",
    "end": "538580"
  },
  {
    "text": "because it's a,\nkind of, an average.",
    "start": "538580",
    "end": "540320"
  },
  {
    "text": "It's the harmonic mean.",
    "start": "540320",
    "end": "542920"
  },
  {
    "text": "What's the value encoded?",
    "start": "542920",
    "end": "544269"
  },
  {
    "text": "The best way I can say this\nis that we're essentially",
    "start": "544270",
    "end": "546830"
  },
  {
    "text": "trying to answer the\nquestion, for a given class",
    "start": "546830",
    "end": "548830"
  },
  {
    "text": "k, how much do\npredictions for k align",
    "start": "548830",
    "end": "551830"
  },
  {
    "text": "with a true instance of k?",
    "start": "551830",
    "end": "553780"
  },
  {
    "text": "That is aligning with\nboth precision and recall",
    "start": "553780",
    "end": "556780"
  },
  {
    "text": "as pressures.",
    "start": "556780",
    "end": "557930"
  },
  {
    "text": "And then we can\nuse the beta value",
    "start": "557930",
    "end": "559480"
  },
  {
    "text": "to control how much\nweight we place",
    "start": "559480",
    "end": "561279"
  },
  {
    "text": "on precision versus recall.",
    "start": "561280",
    "end": "564220"
  },
  {
    "text": "What are the\nweaknesses of F scores?",
    "start": "564220",
    "end": "565850"
  },
  {
    "text": "Well, I can really think of two.",
    "start": "565850",
    "end": "567579"
  },
  {
    "text": "The first is that\nthere's no normalization",
    "start": "567580",
    "end": "569560"
  },
  {
    "text": "for the size of\nthe dataset because",
    "start": "569560",
    "end": "571510"
  },
  {
    "text": "of the way we use the\ndenominators for the row",
    "start": "571510",
    "end": "573850"
  },
  {
    "text": "and column sums.",
    "start": "573850",
    "end": "575680"
  },
  {
    "text": "And relatedly, for a given class\nthat we decide to focus on,",
    "start": "575680",
    "end": "579160"
  },
  {
    "text": "we actually ignore most of\nthe data that's in the table.",
    "start": "579160",
    "end": "582160"
  },
  {
    "text": "Consider the fact\nthat if we decided",
    "start": "582160",
    "end": "584230"
  },
  {
    "text": "to calculate the F1 score\nfor the positive class,",
    "start": "584230",
    "end": "586779"
  },
  {
    "text": "we pay attention to\nthese column values",
    "start": "586780",
    "end": "589220"
  },
  {
    "text": "and these are row\nvalues, but we completely",
    "start": "589220",
    "end": "591129"
  },
  {
    "text": "ignore these four values here.",
    "start": "591130",
    "end": "593080"
  },
  {
    "text": "They're just not involved\nin the calculation at all.",
    "start": "593080",
    "end": "595670"
  },
  {
    "text": "And as a result, the\npositive class F1 score",
    "start": "595670",
    "end": "599680"
  },
  {
    "text": "might give a distorted picture\nof what the model's predictions",
    "start": "599680",
    "end": "602770"
  },
  {
    "text": "are actually like in\nvirtue of the fact",
    "start": "602770",
    "end": "604780"
  },
  {
    "text": "that they leave out so much of\nthe data here as you can see.",
    "start": "604780",
    "end": "610570"
  },
  {
    "start": "609000",
    "end": "641000"
  },
  {
    "text": "Now because F scores\nare a per class notion,",
    "start": "610570",
    "end": "614032"
  },
  {
    "text": "I think, that's\nuseful in the sense",
    "start": "614032",
    "end": "615490"
  },
  {
    "text": "that it gives us a perspective\non each one of the classes",
    "start": "615490",
    "end": "617865"
  },
  {
    "text": "separately.",
    "start": "617865",
    "end": "618640"
  },
  {
    "text": "But for many kinds\nof model evaluations,",
    "start": "618640",
    "end": "620620"
  },
  {
    "text": "we need a summary\nnumber, a single number",
    "start": "620620",
    "end": "622570"
  },
  {
    "text": "that we can use to\ncompare models and assess",
    "start": "622570",
    "end": "625030"
  },
  {
    "text": "overall progress.",
    "start": "625030",
    "end": "626437"
  },
  {
    "text": "So we're going to do\nsome kind of averaging.",
    "start": "626437",
    "end": "628270"
  },
  {
    "text": "And I'd like to\noffer you three ways",
    "start": "628270",
    "end": "629950"
  },
  {
    "text": "that we might average\nthese F scores.",
    "start": "629950",
    "end": "632050"
  },
  {
    "text": "Macro-averaging, weighted\naveraging, and micro-averaging.",
    "start": "632050",
    "end": "635450"
  },
  {
    "text": "And as you'll see, these\nencode quite different",
    "start": "635450",
    "end": "637750"
  },
  {
    "text": "values about how we want to\nthink about the F scores.",
    "start": "637750",
    "end": "642510"
  },
  {
    "start": "641000",
    "end": "724000"
  },
  {
    "text": "Macro-averaging is a\naveraging that we've",
    "start": "642510",
    "end": "644760"
  },
  {
    "text": "done at various points\nthroughout the quarter.",
    "start": "644760",
    "end": "647730"
  },
  {
    "text": "It is simply the\narithmetic mean of all the",
    "start": "647730",
    "end": "651209"
  },
  {
    "text": "per category F1 scores.",
    "start": "651210",
    "end": "653200"
  },
  {
    "text": "So it's just the mean of the\nvalues along this column.",
    "start": "653200",
    "end": "656850"
  },
  {
    "text": "Its bounds are 0 and 1 with\n0 the worst and 1 the best.",
    "start": "656850",
    "end": "660930"
  },
  {
    "text": "What value does encode?",
    "start": "660930",
    "end": "662279"
  },
  {
    "text": "Well, it's the same values\nthat we get from F scores,",
    "start": "662280",
    "end": "665550"
  },
  {
    "text": "plus the additional and\nnon-trivial assumption",
    "start": "665550",
    "end": "668100"
  },
  {
    "text": "that all of the\nclasses are equal",
    "start": "668100",
    "end": "670170"
  },
  {
    "text": "regardless of size differences\nbetween them, right?",
    "start": "670170",
    "end": "673889"
  },
  {
    "text": "And that kind of feeds\ninto the weaknesses here.",
    "start": "673890",
    "end": "676050"
  },
  {
    "text": "A classifier that does well\nonly on the small classes",
    "start": "676050",
    "end": "679470"
  },
  {
    "text": "might not actually do\nwell in the real world.",
    "start": "679470",
    "end": "682019"
  },
  {
    "text": "If you imagine counterfactually\nthat for our given model here,",
    "start": "682020",
    "end": "685440"
  },
  {
    "text": "we had really\noutstanding F1 scores",
    "start": "685440",
    "end": "687660"
  },
  {
    "text": "for positive and negative\nand really low for neutral.",
    "start": "687660",
    "end": "690959"
  },
  {
    "text": "That might really be at odds\nwith how this classifier would",
    "start": "690960",
    "end": "693780"
  },
  {
    "text": "behave in the world,\nassuming that most",
    "start": "693780",
    "end": "696390"
  },
  {
    "text": "of the examples that\nare streaming in",
    "start": "696390",
    "end": "698370"
  },
  {
    "text": "are in the neutral category.",
    "start": "698370",
    "end": "700980"
  },
  {
    "text": "Relatedly, a classifier that\ndoes well only on large classes",
    "start": "700980",
    "end": "704970"
  },
  {
    "text": "might do poorly on the\nsmall, but nonetheless vital",
    "start": "704970",
    "end": "708209"
  },
  {
    "text": "classes that are in our data.",
    "start": "708210",
    "end": "710170"
  },
  {
    "text": "And that just reflects the fact\nthat very often in NLP, it's",
    "start": "710170",
    "end": "713430"
  },
  {
    "text": "the small classes that are the\nmost precious, the ones that we",
    "start": "713430",
    "end": "716250"
  },
  {
    "text": "care about the most.",
    "start": "716250",
    "end": "717570"
  },
  {
    "text": "And we're not reflecting that\nkind of asymmetry in our values",
    "start": "717570",
    "end": "721020"
  },
  {
    "text": "by simply taking the average\nof all these F scores.",
    "start": "721020",
    "end": "725570"
  },
  {
    "text": "Weighted average\nF scores will give",
    "start": "725570",
    "end": "727520"
  },
  {
    "text": "a very different perspective\non model performance.",
    "start": "727520",
    "end": "730550"
  },
  {
    "text": "In this case, we\nare again just going",
    "start": "730550",
    "end": "732230"
  },
  {
    "text": "to take an average\nof the F1 scores,",
    "start": "732230",
    "end": "734060"
  },
  {
    "text": "but now weighted by\nthe amount of support",
    "start": "734060",
    "end": "736190"
  },
  {
    "text": "for each one of the classes.",
    "start": "736190",
    "end": "738500"
  },
  {
    "text": "That, again, has bound\n0 to 1 with 0 the worst",
    "start": "738500",
    "end": "741140"
  },
  {
    "text": "and 1 the best.",
    "start": "741140",
    "end": "742415"
  },
  {
    "text": "The value encoded is\nthe same as the values",
    "start": "742415",
    "end": "745009"
  },
  {
    "text": "that we get for the F scores,\nbut now with the added",
    "start": "745010",
    "end": "747980"
  },
  {
    "text": "assumption that the size of the\nclass as the amount of support",
    "start": "747980",
    "end": "751010"
  },
  {
    "text": "really does matter.",
    "start": "751010",
    "end": "752645"
  },
  {
    "text": "And that's going to feed\ninto the weaknesses.",
    "start": "752645",
    "end": "754520"
  },
  {
    "text": "And the fundamental thing\nhere is that large classes",
    "start": "754520",
    "end": "757460"
  },
  {
    "text": "will dominate.",
    "start": "757460",
    "end": "758420"
  },
  {
    "text": "Just as with accuracy,\nthe larger our classes,",
    "start": "758420",
    "end": "761334"
  },
  {
    "text": "the more it's\ngoing to contribute",
    "start": "761335",
    "end": "762710"
  },
  {
    "text": "to our overall summary number.",
    "start": "762710",
    "end": "764510"
  },
  {
    "text": "And that can lead to the kind\nof problematic situation,",
    "start": "764510",
    "end": "767930"
  },
  {
    "text": "where the small classes are just\nnot relevant for the evaluation",
    "start": "767930",
    "end": "771529"
  },
  {
    "text": "metric.",
    "start": "771530",
    "end": "772430"
  },
  {
    "text": "That could reflect\nyour values because.",
    "start": "772430",
    "end": "774470"
  },
  {
    "text": "If what you really\ncare about is raw rate",
    "start": "774470",
    "end": "776540"
  },
  {
    "text": "of correct\npredictions, you might",
    "start": "776540",
    "end": "778399"
  },
  {
    "text": "want to weight the larger\nclasses more heavily.",
    "start": "778400",
    "end": "781070"
  },
  {
    "text": "But again, for many\ncontexts in NLP",
    "start": "781070",
    "end": "783130"
  },
  {
    "text": "we really care about\nhow much progress",
    "start": "783130",
    "end": "785030"
  },
  {
    "text": "we can make on the small, but\nnonetheless important classes.",
    "start": "785030",
    "end": "788780"
  },
  {
    "text": "And so in those contexts,\nweighted averaging",
    "start": "788780",
    "end": "791030"
  },
  {
    "text": "is probably not\nthe right choice.",
    "start": "791030",
    "end": "794567"
  },
  {
    "start": "793000",
    "end": "853000"
  },
  {
    "text": "The final averaging scheme\nthat I would like to consider",
    "start": "794567",
    "end": "796900"
  },
  {
    "text": "is micro-average F scores.",
    "start": "796900",
    "end": "799010"
  },
  {
    "text": "This will be very similar to\nweighted averaging of F1 scores",
    "start": "799010",
    "end": "802660"
  },
  {
    "text": "and is directly\nconnected to accuracy.",
    "start": "802660",
    "end": "805839"
  },
  {
    "text": "The way this works is\na little bit involved.",
    "start": "805840",
    "end": "807700"
  },
  {
    "text": "We start with this\ncore confusion matrix.",
    "start": "807700",
    "end": "810790"
  },
  {
    "text": "And we're going to break it down\ninto three smaller confusion",
    "start": "810790",
    "end": "814630"
  },
  {
    "text": "matrices, one per class.",
    "start": "814630",
    "end": "816652"
  },
  {
    "text": "So you can see this\none on the left",
    "start": "816652",
    "end": "818110"
  },
  {
    "text": "here is for the positive class.",
    "start": "818110",
    "end": "820120"
  },
  {
    "text": "The yes'es are 15 and the no's\nare the sum of these two values",
    "start": "820120",
    "end": "824200"
  },
  {
    "text": "here along this row.",
    "start": "824200",
    "end": "826120"
  },
  {
    "text": "The no's are the 20, which is\nthe sum of these two values.",
    "start": "826120",
    "end": "829390"
  },
  {
    "text": "And then there's\nno no categories,",
    "start": "829390",
    "end": "831250"
  },
  {
    "text": "for all the remaining data\nin this quadrant here.",
    "start": "831250",
    "end": "834400"
  },
  {
    "text": "We repeat that same procedure\nfor the negative class",
    "start": "834400",
    "end": "837520"
  },
  {
    "text": "and for the neutral class.",
    "start": "837520",
    "end": "839320"
  },
  {
    "text": "And then we simply sum\nup those three smaller",
    "start": "839320",
    "end": "841660"
  },
  {
    "text": "tables into one big\nyes-no confusion matrix",
    "start": "841660",
    "end": "845920"
  },
  {
    "text": "and calculate the F1\nscores per category.",
    "start": "845920",
    "end": "849490"
  },
  {
    "text": "That gives us two scores here.",
    "start": "849490",
    "end": "850930"
  },
  {
    "text": "One for yes and one for no.",
    "start": "850930",
    "end": "854130"
  },
  {
    "start": "853000",
    "end": "913000"
  },
  {
    "text": "The bounds on this are 0 and 1.",
    "start": "854130",
    "end": "855810"
  },
  {
    "text": "With 0 the worst and 1 the best.",
    "start": "855810",
    "end": "858210"
  },
  {
    "text": "The value encoded is\nreally easy to state.",
    "start": "858210",
    "end": "861690"
  },
  {
    "text": "Macro-averaged F1 scores\nfor the yes category",
    "start": "861690",
    "end": "864870"
  },
  {
    "text": "are equivalent to accuracy\nscores numerically.",
    "start": "864870",
    "end": "867555"
  },
  {
    "text": "So this is identical in\nterms of that metric.",
    "start": "867555",
    "end": "870420"
  },
  {
    "text": "And we have this additional\nproblem now that--",
    "start": "870420",
    "end": "872910"
  },
  {
    "text": "well, we have the same\nkind of value reflected",
    "start": "872910",
    "end": "875550"
  },
  {
    "text": "as we have for the weighted\nF scores or for accuracy,",
    "start": "875550",
    "end": "878482"
  },
  {
    "text": "but now we have brought\nin an additional source",
    "start": "878482",
    "end": "880440"
  },
  {
    "text": "of uncertainty, which is we\nhave a number for the yes",
    "start": "880440",
    "end": "883320"
  },
  {
    "text": "category and the no category.",
    "start": "883320",
    "end": "885360"
  },
  {
    "text": "And hence, no single\nsummary number.",
    "start": "885360",
    "end": "887890"
  },
  {
    "text": "The convention in\nthe literature is",
    "start": "887890",
    "end": "889890"
  },
  {
    "text": "to focus on the yes category,\nbut that simply brings us back",
    "start": "889890",
    "end": "893190"
  },
  {
    "text": "to accuracy with a more\ninvolved calculation.",
    "start": "893190",
    "end": "896200"
  },
  {
    "text": "So that's obviously\nnot very productive.",
    "start": "896200",
    "end": "897940"
  },
  {
    "text": "And I would say\nas a result here,",
    "start": "897940",
    "end": "899550"
  },
  {
    "text": "the two real choices\nthat you want to make",
    "start": "899550",
    "end": "901680"
  },
  {
    "text": "are between macro-averaging\nand weighted averaging",
    "start": "901680",
    "end": "905190"
  },
  {
    "text": "of your F1 scores.",
    "start": "905190",
    "end": "906570"
  },
  {
    "text": "And again, that will\ncome down to what",
    "start": "906570",
    "end": "908550"
  },
  {
    "text": "your fundamental\nvalues are and what",
    "start": "908550",
    "end": "910709"
  },
  {
    "text": "hypotheses you're pursuing.",
    "start": "910710",
    "end": "913920"
  },
  {
    "start": "913000",
    "end": "997000"
  },
  {
    "text": "The final point I want to\nmake is that thus far, we",
    "start": "913920",
    "end": "916260"
  },
  {
    "text": "have operated in terms of\nthe confusion matrix, which",
    "start": "916260",
    "end": "919170"
  },
  {
    "text": "involved imposing a threshold\non probabilistic predictions",
    "start": "919170",
    "end": "922709"
  },
  {
    "text": "in order to create categorical\nvalues that we could then",
    "start": "922710",
    "end": "926040"
  },
  {
    "text": "compare with precision,\nand recall, and so forth.",
    "start": "926040",
    "end": "928800"
  },
  {
    "text": "Precision and recall curves\noffer a fundamentally different",
    "start": "928800",
    "end": "932430"
  },
  {
    "text": "perspective.",
    "start": "932430",
    "end": "933570"
  },
  {
    "text": "In this case, instead of\nimposing one threshold,",
    "start": "933570",
    "end": "936630"
  },
  {
    "text": "we'll take every\npossible value that's",
    "start": "936630",
    "end": "939360"
  },
  {
    "text": "predicted by our classifier\nto be a potential threshold,",
    "start": "939360",
    "end": "943019"
  },
  {
    "text": "and essentially create a\nbunch of confusion matrices",
    "start": "943020",
    "end": "945600"
  },
  {
    "text": "based on that successive\nseries of thresholds.",
    "start": "945600",
    "end": "949529"
  },
  {
    "text": "And then we can plot the\ntrade-offs between precision",
    "start": "949530",
    "end": "952350"
  },
  {
    "text": "here along the y-axis\nand recall along",
    "start": "952350",
    "end": "955079"
  },
  {
    "text": "the x-axis for all those\ndifferent notions of threshold.",
    "start": "955080",
    "end": "958380"
  },
  {
    "text": "And that can be really\nilluminating in terms",
    "start": "958380",
    "end": "960570"
  },
  {
    "text": "of helping us see how our\nsystem trades precision",
    "start": "960570",
    "end": "962925"
  },
  {
    "text": "and recall against each other.",
    "start": "962925",
    "end": "965010"
  },
  {
    "text": "And help us find based\non values that we",
    "start": "965010",
    "end": "967710"
  },
  {
    "text": "have about our\nproblem and our goals,",
    "start": "967710",
    "end": "970080"
  },
  {
    "text": "what the optimal balance\nbetween precision and recall",
    "start": "970080",
    "end": "972810"
  },
  {
    "text": "actually is.",
    "start": "972810",
    "end": "974253"
  },
  {
    "text": "And then if you\ndo need to summary",
    "start": "974253",
    "end": "975670"
  },
  {
    "text": "a number of this entire\ntable, average precision,",
    "start": "975670",
    "end": "978700"
  },
  {
    "text": "which is implemented\nin Scikit-learn",
    "start": "978700",
    "end": "980610"
  },
  {
    "text": "is a standard way of\nsummarizing the entire curve",
    "start": "980610",
    "end": "983850"
  },
  {
    "text": "with a single number\nwithout, though, imposing",
    "start": "983850",
    "end": "986819"
  },
  {
    "text": "that single threshold\nthat was so much shaping",
    "start": "986820",
    "end": "990120"
  },
  {
    "text": "all of the previous\nmetrics that we discussed.",
    "start": "990120",
    "end": "993740"
  },
  {
    "start": "993740",
    "end": "998000"
  }
]