[
  {
    "start": "0",
    "end": "6000"
  },
  {
    "start": "0",
    "end": "5500"
  },
  {
    "text": "Hi.",
    "start": "5500",
    "end": "6000"
  },
  {
    "start": "6000",
    "end": "14000"
  },
  {
    "text": "This module is about\nlinear classification.",
    "start": "6000",
    "end": "8370"
  },
  {
    "text": "We're going to go through\na linear classification",
    "start": "8370",
    "end": "10469"
  },
  {
    "text": "via a simple example, just like\nwe did for linear regression.",
    "start": "10470",
    "end": "14900"
  },
  {
    "start": "14000",
    "end": "163000"
  },
  {
    "text": "So as before, we have\ntraining data, which",
    "start": "14900",
    "end": "18070"
  },
  {
    "text": "consists of a set of examples.",
    "start": "18070",
    "end": "20350"
  },
  {
    "text": "And each example is now going to\nbe an input x1 and x2 followed",
    "start": "20350",
    "end": "26620"
  },
  {
    "text": "by a label y.",
    "start": "26620",
    "end": "28720"
  },
  {
    "text": "So we have three examples here.",
    "start": "28720",
    "end": "30189"
  },
  {
    "text": "The input 0, 2 has output 1.",
    "start": "30190",
    "end": "34660"
  },
  {
    "text": "Minus 2, 0 has output 1.",
    "start": "34660",
    "end": "36940"
  },
  {
    "text": "And minus 1, minus 1-- sorry,\n1 minus 1 has output minus 1.",
    "start": "36940",
    "end": "43120"
  },
  {
    "text": "So we can visualize these\npoints, just the input part,",
    "start": "43120",
    "end": "46149"
  },
  {
    "text": "on a 2D diagram, so where\nI'm plotting x1 by x2.",
    "start": "46150",
    "end": "53320"
  },
  {
    "text": "So here is 0, 2.",
    "start": "53320",
    "end": "55750"
  },
  {
    "text": "And I'm coloring\nit orange to denote",
    "start": "55750",
    "end": "58840"
  },
  {
    "text": "that as a positive point.",
    "start": "58840",
    "end": "60310"
  },
  {
    "text": "This is minus 2, 0.",
    "start": "60310",
    "end": "62710"
  },
  {
    "text": "That's also orange\nbecause it's positive.",
    "start": "62710",
    "end": "64599"
  },
  {
    "text": "And here is 1, minus 1,\nwhich is blue because it's",
    "start": "64599",
    "end": "70450"
  },
  {
    "text": "labeled as negative.",
    "start": "70450",
    "end": "73210"
  },
  {
    "text": "So given these points,\nwe want to design",
    "start": "73210",
    "end": "75400"
  },
  {
    "text": "a learning algorithm\nthat can output",
    "start": "75400",
    "end": "78040"
  },
  {
    "text": "a predictor in classification.",
    "start": "78040",
    "end": "79780"
  },
  {
    "text": "It's known as a classifier.",
    "start": "79780",
    "end": "81580"
  },
  {
    "text": "And this classifier\ncan take new inputs,",
    "start": "81580",
    "end": "84520"
  },
  {
    "text": "and crank them through\nthe classifier,",
    "start": "84520",
    "end": "86649"
  },
  {
    "text": "and produce an output label.",
    "start": "86650",
    "end": "90490"
  },
  {
    "text": "And so this is demonstrated as\nfollows on the plot as follows.",
    "start": "90490",
    "end": "96080"
  },
  {
    "text": "So the classifier\nin classification",
    "start": "96080",
    "end": "99130"
  },
  {
    "text": "is going to be represented\nby a decision boundary.",
    "start": "99130",
    "end": "102640"
  },
  {
    "text": "So the decision boundary\ncarves up the space",
    "start": "102640",
    "end": "106060"
  },
  {
    "text": "into a region where the\npoints are labeled positive",
    "start": "106060",
    "end": "112329"
  },
  {
    "text": "and the region where the\npoints are labeled minus.",
    "start": "112330",
    "end": "116320"
  },
  {
    "text": "So 2, 0 is going to be predicted\nas a minus 1 in this case.",
    "start": "116320",
    "end": "124070"
  },
  {
    "text": "OK, as before, we have\nthree design decisions",
    "start": "124070",
    "end": "126170"
  },
  {
    "text": "we need to settle.",
    "start": "126170",
    "end": "127490"
  },
  {
    "text": "First, which classifiers\nare possible?",
    "start": "127490",
    "end": "131390"
  },
  {
    "text": "And this is a question\nof the hypothesis class",
    "start": "131390",
    "end": "134150"
  },
  {
    "text": "we're going to consider.",
    "start": "134150",
    "end": "135834"
  },
  {
    "text": "Are the decision\nboundaries going",
    "start": "135835",
    "end": "137210"
  },
  {
    "text": "to be straight or\ncan they be curved?",
    "start": "137210",
    "end": "140280"
  },
  {
    "text": "Second, how good\nis a classifier?",
    "start": "140280",
    "end": "142350"
  },
  {
    "text": "This is a question\nof a loss function.",
    "start": "142350",
    "end": "144540"
  },
  {
    "text": "And third, how do we compute\nthe best classifier, a.k.a.",
    "start": "144540",
    "end": "149189"
  },
  {
    "text": "The classifier with\nthe lowest loss?",
    "start": "149190",
    "end": "152900"
  },
  {
    "text": "And that's going to be a\nquestion of the optimization",
    "start": "152900",
    "end": "155260"
  },
  {
    "text": "algorithm.",
    "start": "155260",
    "end": "157970"
  },
  {
    "text": "So before we begin\ntalking about the design",
    "start": "157970",
    "end": "162800"
  },
  {
    "text": "space of the hypothesis class.",
    "start": "162800",
    "end": "165740"
  },
  {
    "start": "163000",
    "end": "386000"
  },
  {
    "text": "I want to focus on an example\nof a linear classifier here.",
    "start": "165740",
    "end": "169590"
  },
  {
    "text": "So we have f of x equals.",
    "start": "169590",
    "end": "173060"
  },
  {
    "text": "And then, we have--",
    "start": "173060",
    "end": "174530"
  },
  {
    "text": "I'm going to define this weight\nvector, w, to be minus 0.6,",
    "start": "174530",
    "end": "182480"
  },
  {
    "text": "0.6, OK?",
    "start": "182480",
    "end": "185769"
  },
  {
    "text": "And I'm going to take the\ndot product with a feature",
    "start": "185770",
    "end": "189940"
  },
  {
    "text": "vector, which is going to\nbe just the identity feature",
    "start": "189940",
    "end": "195060"
  },
  {
    "text": "vector mapping to x1, x2.",
    "start": "195060",
    "end": "197250"
  },
  {
    "text": "Remember, x is now a\ntwo-dimensional list",
    "start": "197250",
    "end": "201090"
  },
  {
    "text": "of two numbers.",
    "start": "201090",
    "end": "203430"
  },
  {
    "text": "And then, I'm going to\ntake this dot product.",
    "start": "203430",
    "end": "205349"
  },
  {
    "text": "I'm going to take the sign.",
    "start": "205350",
    "end": "208400"
  },
  {
    "text": "And remember, the\nsign of a scalar",
    "start": "208400",
    "end": "212120"
  },
  {
    "text": "is equal to plus 1 if\nthat scalar is positive,",
    "start": "212120",
    "end": "219200"
  },
  {
    "text": "minus 1 if it's negative,\nand 0 if it is 0, OK?",
    "start": "219200",
    "end": "225245"
  },
  {
    "start": "225245",
    "end": "228230"
  },
  {
    "text": "So let's see what this\nclassifier does on some points.",
    "start": "228230",
    "end": "234580"
  },
  {
    "text": "So each point is x1, x2.",
    "start": "234580",
    "end": "237060"
  },
  {
    "text": "So let's look at 0, 2, OK?",
    "start": "237060",
    "end": "240209"
  },
  {
    "text": "So let's look at where\n0, 2 is on the plot.",
    "start": "240210",
    "end": "243600"
  },
  {
    "text": "0, 2 is right here.",
    "start": "243600",
    "end": "245920"
  },
  {
    "text": "And I'm going to represent\nit by this vector here.",
    "start": "245920",
    "end": "249060"
  },
  {
    "text": "And now, this\nvector is phi of x.",
    "start": "249060",
    "end": "252950"
  },
  {
    "text": "w is going to be\nthis vector here.",
    "start": "252950",
    "end": "255930"
  },
  {
    "text": "That's the weight vector.",
    "start": "255930",
    "end": "258070"
  },
  {
    "text": "And the dot product,\nremembering from linear algebra,",
    "start": "258070",
    "end": "261880"
  },
  {
    "text": "is the cosine of this angle.",
    "start": "261880",
    "end": "265240"
  },
  {
    "text": "And in particular, the product\nis positive if and only",
    "start": "265240",
    "end": "268360"
  },
  {
    "text": "if this angle is acute.",
    "start": "268360",
    "end": "270639"
  },
  {
    "text": "And it's negative if\nthe angle is obtuse.",
    "start": "270640",
    "end": "274670"
  },
  {
    "text": "So in this case, it is acute.",
    "start": "274670",
    "end": "277510"
  },
  {
    "text": "So therefore, this\npoint is going",
    "start": "277510",
    "end": "279790"
  },
  {
    "text": "to be classified as positive.",
    "start": "279790",
    "end": "283420"
  },
  {
    "text": "So let's take another\npoint, so minus 2, 0.",
    "start": "283420",
    "end": "288010"
  },
  {
    "text": "Minus 2, 0 is here.",
    "start": "288010",
    "end": "290590"
  },
  {
    "text": "And this angle is also acute.",
    "start": "290590",
    "end": "292750"
  },
  {
    "text": "So therefore, this point is\nalso labeled as positive.",
    "start": "292750",
    "end": "298650"
  },
  {
    "text": "And the third point\nis 1, minus 1.",
    "start": "298650",
    "end": "301169"
  },
  {
    "text": "So 1, minus 1 is over here.",
    "start": "301170",
    "end": "303900"
  },
  {
    "text": "And now, this angle between\nthe red and the blue is obtuse.",
    "start": "303900",
    "end": "308759"
  },
  {
    "text": "Therefore, the sign is negative.",
    "start": "308760",
    "end": "314000"
  },
  {
    "text": "So you can kind of understand\nhow a classifier behaves",
    "start": "314000",
    "end": "317210"
  },
  {
    "text": "geometrically.",
    "start": "317210",
    "end": "319160"
  },
  {
    "text": "But you can also do\nthis symbolically",
    "start": "319160",
    "end": "321980"
  },
  {
    "text": "by following the math.",
    "start": "321980",
    "end": "323030"
  },
  {
    "text": "So if you plug in for\nour first point, 0, 2--",
    "start": "323030",
    "end": "325220"
  },
  {
    "start": "325220",
    "end": "328880"
  },
  {
    "text": "the dot product is 1.2.",
    "start": "328880",
    "end": "330440"
  },
  {
    "text": "You take the sign and you get 1.",
    "start": "330440",
    "end": "333050"
  },
  {
    "text": "If you take the\nsecond point, the sign",
    "start": "333050",
    "end": "335900"
  },
  {
    "text": "is also 1.2 on either one.",
    "start": "335900",
    "end": "337580"
  },
  {
    "text": "And you take the third\npoint, the sign is minus 1.2.",
    "start": "337580",
    "end": "341180"
  },
  {
    "text": "And the sign of\nminus 1.2 is minus 1.",
    "start": "341180",
    "end": "345539"
  },
  {
    "text": "OK, so you can kind of\nsee the pattern now.",
    "start": "345540",
    "end": "348120"
  },
  {
    "text": "So we have any point over\nhere that forms an acute angle",
    "start": "348120",
    "end": "353430"
  },
  {
    "text": "with this weight vector--",
    "start": "353430",
    "end": "355740"
  },
  {
    "text": "minus 0.6, 0.6-- is going\nto be labeled as positive.",
    "start": "355740",
    "end": "360930"
  },
  {
    "text": "And anything that forms an\nobtuse angle with this weight",
    "start": "360930",
    "end": "364020"
  },
  {
    "text": "vector is going to be\nlabeled as negative.",
    "start": "364020",
    "end": "367050"
  },
  {
    "text": "And the decision boundary\nis exactly those points",
    "start": "367050",
    "end": "370889"
  },
  {
    "text": "that are perpendicular.",
    "start": "370890",
    "end": "372780"
  },
  {
    "text": "And indeed, you can see that\nthis is a right angle here.",
    "start": "372780",
    "end": "375600"
  },
  {
    "text": "These are the points which the\nclassifer just doesn't know",
    "start": "375600",
    "end": "378450"
  },
  {
    "text": "if it's positive or negative.",
    "start": "378450",
    "end": "380203"
  },
  {
    "start": "380203",
    "end": "383180"
  },
  {
    "text": "OK, so that was one\nparticular classifier.",
    "start": "383180",
    "end": "387560"
  },
  {
    "start": "386000",
    "end": "454000"
  },
  {
    "text": "That was this one.",
    "start": "387560",
    "end": "389960"
  },
  {
    "text": "But we can imagine other ones.",
    "start": "389960",
    "end": "392270"
  },
  {
    "text": "We can imagine this\npurple classifier,",
    "start": "392270",
    "end": "394400"
  },
  {
    "text": "which has weights 0.5 and 1.",
    "start": "394400",
    "end": "397610"
  },
  {
    "text": "And that corresponds\nto this point here.",
    "start": "397610",
    "end": "402379"
  },
  {
    "text": "So that is 0.51.",
    "start": "402380",
    "end": "404360"
  },
  {
    "text": "And remember, the\ndecision boundary",
    "start": "404360",
    "end": "406210"
  },
  {
    "text": "is the thing that is\nperpendicular or normal",
    "start": "406210",
    "end": "408850"
  },
  {
    "text": "to the weight vector.",
    "start": "408850",
    "end": "410440"
  },
  {
    "text": "And in 2D, it's\ngiven by this line.",
    "start": "410440",
    "end": "413060"
  },
  {
    "text": "So this purple\nclassifier will classify",
    "start": "413060",
    "end": "414880"
  },
  {
    "text": "all of these points plus and\nall of these points minus.",
    "start": "414880",
    "end": "420070"
  },
  {
    "text": "In general, the binary\nclassifier f sub w,",
    "start": "420070",
    "end": "426144"
  },
  {
    "text": "where fw is a weight\nof a particular input x",
    "start": "426144",
    "end": "429670"
  },
  {
    "text": "is equal to--",
    "start": "429670",
    "end": "431350"
  },
  {
    "text": "you take the dot product.",
    "start": "431350",
    "end": "433510"
  },
  {
    "text": "And then, you take the\nsign of that dot product.",
    "start": "433510",
    "end": "436180"
  },
  {
    "start": "436180",
    "end": "439580"
  },
  {
    "text": "And the hypothesis\nclass, as before,",
    "start": "439580",
    "end": "442310"
  },
  {
    "text": "is just simply the set of\nall possible classifiers",
    "start": "442310",
    "end": "445669"
  },
  {
    "text": "by ranging the weights\nover any two real numbers.",
    "start": "445670",
    "end": "449360"
  },
  {
    "start": "449360",
    "end": "453050"
  },
  {
    "text": "So that's the hypothesis class.",
    "start": "453050",
    "end": "454520"
  },
  {
    "start": "454000",
    "end": "607000"
  },
  {
    "text": "Now, let's go on to the\nsecond design decision.",
    "start": "454520",
    "end": "457060"
  },
  {
    "text": "What is a good\nloss function, OK?",
    "start": "457060",
    "end": "459700"
  },
  {
    "text": "So let's take our purple\nclassifier and some training",
    "start": "459700",
    "end": "464470"
  },
  {
    "text": "data.",
    "start": "464470",
    "end": "466540"
  },
  {
    "text": "And we're going to evaluate\nhow good this classifier is",
    "start": "466540",
    "end": "470500"
  },
  {
    "text": "on this training data, OK?",
    "start": "470500",
    "end": "472400"
  },
  {
    "text": "So the training data,\nlet's go through this.",
    "start": "472400",
    "end": "476389"
  },
  {
    "text": "So here's the classifier.",
    "start": "476390",
    "end": "477850"
  },
  {
    "text": "And the first point is 0, 2.",
    "start": "477850",
    "end": "481870"
  },
  {
    "text": "And this was labeled\nas plus 1, OK?",
    "start": "481870",
    "end": "485410"
  },
  {
    "text": "So that is this point over here.",
    "start": "485410",
    "end": "490130"
  },
  {
    "text": "And this classifier\nis predicted correctly",
    "start": "490130",
    "end": "493570"
  },
  {
    "text": "because it's on this side.",
    "start": "493570",
    "end": "495070"
  },
  {
    "text": "It's a positive label,\nand the classifier also",
    "start": "495070",
    "end": "498370"
  },
  {
    "text": "thinks that it's positive.",
    "start": "498370",
    "end": "499970"
  },
  {
    "text": "So therefore, we\nexpect low loss.",
    "start": "499970",
    "end": "502930"
  },
  {
    "text": "Whereas this point over here--",
    "start": "502930",
    "end": "505479"
  },
  {
    "text": "minus 2, 0-- is\nlabeled as positive,",
    "start": "505480",
    "end": "508570"
  },
  {
    "text": "but it's on the other side\nof the decision boundary.",
    "start": "508570",
    "end": "510790"
  },
  {
    "text": "And therefore, it's\nclassified incorrectly.",
    "start": "510790",
    "end": "515140"
  },
  {
    "text": "On this point--",
    "start": "515140",
    "end": "517453"
  },
  {
    "text": "1, minus 1, minus\n1 is over here.",
    "start": "517454",
    "end": "521870"
  },
  {
    "text": "And it's labeled in the\ntraining data as a minus",
    "start": "521870",
    "end": "526055"
  },
  {
    "text": "and is on this side of\nthe decision boundary.",
    "start": "526055",
    "end": "527930"
  },
  {
    "text": "So it's predicted as minus.",
    "start": "527930",
    "end": "529460"
  },
  {
    "text": "Therefore, it is labeled\nincorrectly as well.",
    "start": "529460",
    "end": "533580"
  },
  {
    "text": "So to formalize\nthis, we're going",
    "start": "533580",
    "end": "535250"
  },
  {
    "text": "to find something called\nthe zero-one loss.",
    "start": "535250",
    "end": "538910"
  },
  {
    "text": "And just like any\nloss function, it",
    "start": "538910",
    "end": "540769"
  },
  {
    "text": "takes in a particular\nexample and a weight vector.",
    "start": "540770",
    "end": "543980"
  },
  {
    "text": "And it looks at the prediction\nand the target and says,",
    "start": "543980",
    "end": "549889"
  },
  {
    "text": "do they disagree?",
    "start": "549890",
    "end": "551450"
  },
  {
    "text": "And if they disagree, then\nthis indicator function",
    "start": "551450",
    "end": "555350"
  },
  {
    "text": "will return 1.",
    "start": "555350",
    "end": "556819"
  },
  {
    "text": "And if they agree, then the\nindicator function returns 0.",
    "start": "556820",
    "end": "560100"
  },
  {
    "text": "So this is a zero-one loss.",
    "start": "560100",
    "end": "562630"
  },
  {
    "text": "So mathematically, you can walk\nthrough these calculations.",
    "start": "562630",
    "end": "566140"
  },
  {
    "text": "You plug in the first point\nand you look at the sign.",
    "start": "566140",
    "end": "571610"
  },
  {
    "text": "The sign here is going to be 2.",
    "start": "571610",
    "end": "576500"
  },
  {
    "text": "The dot product of 2 is 1.",
    "start": "576500",
    "end": "578930"
  },
  {
    "text": "They don't disagree,\nso that's 0.",
    "start": "578930",
    "end": "581210"
  },
  {
    "text": "The second point, they do\ndisagree, so the loss is 1.",
    "start": "581210",
    "end": "585230"
  },
  {
    "text": "And the third point,\nthey also don't disagree.",
    "start": "585230",
    "end": "588410"
  },
  {
    "text": "And the loss is 0.",
    "start": "588410",
    "end": "591250"
  },
  {
    "text": "And as before, the training loss\nover the entire training set",
    "start": "591250",
    "end": "595510"
  },
  {
    "text": "of examples is just\nsimply the average",
    "start": "595510",
    "end": "599290"
  },
  {
    "text": "over the per-example losses.",
    "start": "599290",
    "end": "601029"
  },
  {
    "text": "And in this case, it's 1/3.",
    "start": "601030",
    "end": "602310"
  },
  {
    "start": "602310",
    "end": "607520"
  },
  {
    "start": "607000",
    "end": "715000"
  },
  {
    "text": "So before we move on to\nthe design decision of how",
    "start": "607520",
    "end": "610790"
  },
  {
    "text": "to optimize the loss function,\nlet's spend some time",
    "start": "610790",
    "end": "613339"
  },
  {
    "text": "understanding two\nimportant concepts",
    "start": "613340",
    "end": "615900"
  },
  {
    "text": "so we can rewrite\nthe zero-one loss",
    "start": "615900",
    "end": "617540"
  },
  {
    "text": "in a slightly different way.",
    "start": "617540",
    "end": "620089"
  },
  {
    "text": "So recall that the predicted\nlabel on a particular input",
    "start": "620090",
    "end": "623050"
  },
  {
    "text": "is the sign of the dot product.",
    "start": "623050",
    "end": "625510"
  },
  {
    "text": "And the target label is y, OK?",
    "start": "625510",
    "end": "628330"
  },
  {
    "start": "628330",
    "end": "630870"
  },
  {
    "text": "So the score is something\nthat we've seen before.",
    "start": "630870",
    "end": "636460"
  },
  {
    "text": "The score on an\nexample is simply",
    "start": "636460",
    "end": "638710"
  },
  {
    "text": "this expression, which is a\ndot product inside the sign.",
    "start": "638710",
    "end": "643730"
  },
  {
    "text": "And while the sign\nis just 1 or minus 1,",
    "start": "643730",
    "end": "647959"
  },
  {
    "text": "the score is a\nreal number, which",
    "start": "647960",
    "end": "651170"
  },
  {
    "text": "intuitively represents\nhow confident we",
    "start": "651170",
    "end": "653480"
  },
  {
    "text": "are in predicting plus 1.",
    "start": "653480",
    "end": "656760"
  },
  {
    "text": "So points over here\nhave large dot products",
    "start": "656760",
    "end": "659580"
  },
  {
    "text": "with its purple weight\nvector and have a high score.",
    "start": "659580",
    "end": "663510"
  },
  {
    "text": "Ones on the decision\nboundary have 0 score.",
    "start": "663510",
    "end": "665820"
  },
  {
    "text": "Ones over here have\nvery negative scores.",
    "start": "665820",
    "end": "671160"
  },
  {
    "text": "The second concept is that\nof a margin, which takes",
    "start": "671160",
    "end": "674430"
  },
  {
    "text": "into account the target label.",
    "start": "674430",
    "end": "676870"
  },
  {
    "text": "So the margin on example\nis simply the score",
    "start": "676870",
    "end": "680170"
  },
  {
    "text": "times the correct target label.",
    "start": "680170",
    "end": "684010"
  },
  {
    "text": "And this measures\nhow correct we are.",
    "start": "684010",
    "end": "686540"
  },
  {
    "text": "Notice that you can be\nconfident but not correct,",
    "start": "686540",
    "end": "689860"
  },
  {
    "text": "important life lesson.",
    "start": "689860",
    "end": "690850"
  },
  {
    "start": "690850",
    "end": "694620"
  },
  {
    "text": "So if y is positive,\nthen the margin",
    "start": "694620",
    "end": "698610"
  },
  {
    "text": "is going to be high when this\nnumber is hugely positive.",
    "start": "698610",
    "end": "702870"
  },
  {
    "text": "And if y is minus\n1, then the margin",
    "start": "702870",
    "end": "706470"
  },
  {
    "text": "is going to be high when this\nscore is hugely negative.",
    "start": "706470",
    "end": "713399"
  },
  {
    "text": "OK, so with these two\ndefinitions in mind,",
    "start": "713400",
    "end": "715870"
  },
  {
    "start": "715000",
    "end": "763000"
  },
  {
    "text": "we can now look at the\nzero-one loss again.",
    "start": "715870",
    "end": "718920"
  },
  {
    "text": "Remember that the\nzero-one loss is",
    "start": "718920",
    "end": "720839"
  },
  {
    "text": "the indicator of whether the\nprediction and target disagree.",
    "start": "720840",
    "end": "724500"
  },
  {
    "text": "But now we can represent\nit in terms of the margin.",
    "start": "724500",
    "end": "728050"
  },
  {
    "text": "So this is the expression.",
    "start": "728050",
    "end": "729149"
  },
  {
    "text": "It's basically when the\nmargin is less or equal to 0.",
    "start": "729150",
    "end": "732960"
  },
  {
    "text": "So remember, a more\npositive margin",
    "start": "732960",
    "end": "734850"
  },
  {
    "text": "means that we're\nclassifying correctly.",
    "start": "734850",
    "end": "736800"
  },
  {
    "text": "A negative margin means that\nwe're classifying incorrectly.",
    "start": "736800",
    "end": "740370"
  },
  {
    "text": "And we can visualize\nthis as follows.",
    "start": "740370",
    "end": "742839"
  },
  {
    "text": "So here I'm plotting the\nmargin against the loss.",
    "start": "742840",
    "end": "747220"
  },
  {
    "text": "And if the margin is positive,\ngreater than 0, the loss is 0.",
    "start": "747220",
    "end": "752639"
  },
  {
    "text": "And if the margin is less or\nequal to 0, then the loss is 1.",
    "start": "752640",
    "end": "758010"
  },
  {
    "start": "758010",
    "end": "761620"
  },
  {
    "text": "OK, so that is zero-one loss\nexpressed in the margin.",
    "start": "761620",
    "end": "766060"
  },
  {
    "start": "763000",
    "end": "988000"
  },
  {
    "text": "OK, so now, let's optimize\nthe third design decision.",
    "start": "766060",
    "end": "768340"
  },
  {
    "text": "Let's optimize\nthe training loss.",
    "start": "768340",
    "end": "773140"
  },
  {
    "text": "We want to find the\nminimum weight vector that",
    "start": "773140",
    "end": "775090"
  },
  {
    "text": "minimizes this\nexpression, which is",
    "start": "775090",
    "end": "776800"
  },
  {
    "text": "the average of the\nindividual losses.",
    "start": "776800",
    "end": "779019"
  },
  {
    "text": "And let's just use gradient\ndescent as we did before.",
    "start": "779020",
    "end": "782530"
  },
  {
    "text": "And to do it, we have to\ncompute the gradients.",
    "start": "782530",
    "end": "784640"
  },
  {
    "text": "So the gradient\nof a training loss",
    "start": "784640",
    "end": "786190"
  },
  {
    "text": "is equal to the sum\nover the gradient",
    "start": "786190",
    "end": "788710"
  },
  {
    "text": "of the individual losses.",
    "start": "788710",
    "end": "790300"
  },
  {
    "text": "You look at the individual\nlosses, take the gradient.",
    "start": "790300",
    "end": "793149"
  },
  {
    "text": "And now, you have to take\nthe gradient with respect",
    "start": "793150",
    "end": "795550"
  },
  {
    "text": "to this indicator function.",
    "start": "795550",
    "end": "799190"
  },
  {
    "text": "And now, that's where\nthings go wrong.",
    "start": "799190",
    "end": "802290"
  },
  {
    "text": "So if you remember what\nthe loss looks like,",
    "start": "802290",
    "end": "807410"
  },
  {
    "text": "it looks like this\nstep function.",
    "start": "807410",
    "end": "810379"
  },
  {
    "text": "And what's the gradient\nof this function?",
    "start": "810380",
    "end": "812910"
  },
  {
    "text": "Well, it's 0 almost everywhere.",
    "start": "812910",
    "end": "815629"
  },
  {
    "text": "It's 0, 0, 0, 0, 0.",
    "start": "815630",
    "end": "817328"
  },
  {
    "text": "And then, there is\nthis discontinuity",
    "start": "817328",
    "end": "818870"
  },
  {
    "text": "where it's undefined.",
    "start": "818870",
    "end": "819779"
  },
  {
    "text": "And then, 0, 0, 0, 0.",
    "start": "819780",
    "end": "822187"
  },
  {
    "text": "So remember what gradient\ndescent is trying to do.",
    "start": "822187",
    "end": "824270"
  },
  {
    "text": "It computes a gradient.",
    "start": "824270",
    "end": "825800"
  },
  {
    "text": "And then, it moves\nin that direction.",
    "start": "825800",
    "end": "827360"
  },
  {
    "text": "And if the the gradient is\n0, the gradient descent just",
    "start": "827360",
    "end": "829760"
  },
  {
    "text": "gets stuck and it\ncan't go anywhere.",
    "start": "829760",
    "end": "832130"
  },
  {
    "text": "So gradient descent will not\nwork on the zero-one loss.",
    "start": "832130",
    "end": "836920"
  },
  {
    "text": "So one kind of technical note\nis that if someone asks you,",
    "start": "836920",
    "end": "840040"
  },
  {
    "text": "why can't you do gradient\ndescent on the zero-one loss?",
    "start": "840040",
    "end": "843279"
  },
  {
    "text": "Initial reaction might\nbe because it's not",
    "start": "843280",
    "end": "845110"
  },
  {
    "text": "differentiable.",
    "start": "845110",
    "end": "846209"
  },
  {
    "text": "And that is true, it's\nnot differentiable.",
    "start": "846210",
    "end": "847960"
  },
  {
    "text": "But it's only not\ndifferentiable at one point.",
    "start": "847960",
    "end": "851530"
  },
  {
    "text": "The real reason is that the\ngradient is 0 everywhere.",
    "start": "851530",
    "end": "854185"
  },
  {
    "text": "And with a 0 gradient, you\njust can't make any progress.",
    "start": "854185",
    "end": "857500"
  },
  {
    "start": "857500",
    "end": "860040"
  },
  {
    "text": "So how do you fix this?",
    "start": "860040",
    "end": "861420"
  },
  {
    "text": "There's a few things you can do.",
    "start": "861420",
    "end": "863010"
  },
  {
    "text": "But one example is what\nis called the hinge loss.",
    "start": "863010",
    "end": "866670"
  },
  {
    "text": "So pictorially, the hinge loss\nis just another loss function,",
    "start": "866670",
    "end": "871950"
  },
  {
    "text": "the one in green here.",
    "start": "871950",
    "end": "873960"
  },
  {
    "text": "And I'm plotting it on this\nmargin-versus-loss plot.",
    "start": "873960",
    "end": "876960"
  },
  {
    "text": "The zero-one loss looks\nlike this and the hinge loss",
    "start": "876960",
    "end": "879840"
  },
  {
    "text": "looks like that.",
    "start": "879840",
    "end": "881050"
  },
  {
    "text": "So it's the maximum\nof two lines.",
    "start": "881050",
    "end": "883870"
  },
  {
    "text": "One is this descending line,\nand one is this flat line at 0.",
    "start": "883870",
    "end": "889980"
  },
  {
    "text": "OK, so formally, what is this?",
    "start": "889980",
    "end": "891360"
  },
  {
    "text": "So the hinge loss is equal\nto the max over two things.",
    "start": "891360",
    "end": "898209"
  },
  {
    "text": "The first is 1\nminus the margin--",
    "start": "898210",
    "end": "903250"
  },
  {
    "text": "so this complicated expression\nis just the margin--",
    "start": "903250",
    "end": "906060"
  },
  {
    "text": "and the 0 function,\ncorresponding these two",
    "start": "906060",
    "end": "910460"
  },
  {
    "text": "arguments to the max,\ncorresponding to these two",
    "start": "910460",
    "end": "913370"
  },
  {
    "text": "regions of the hinge loss.",
    "start": "913370",
    "end": "917932"
  },
  {
    "text": "OK, so let's interpret\nthis a little bit.",
    "start": "917932",
    "end": "919640"
  },
  {
    "text": "So if the margin is\ngreater than or equal to 1,",
    "start": "919640",
    "end": "924820"
  },
  {
    "text": "then the hinge loss is 0.",
    "start": "924820",
    "end": "927780"
  },
  {
    "text": "But once the margin\nstarts dipping below 1,",
    "start": "927780",
    "end": "930540"
  },
  {
    "text": "then the hinge loss\nstarts growing linearly",
    "start": "930540",
    "end": "934019"
  },
  {
    "text": "with the margin violation.",
    "start": "934020",
    "end": "938230"
  },
  {
    "text": "Now why is there a\n1 here and not at 0?",
    "start": "938230",
    "end": "940800"
  },
  {
    "text": "Well, this is because\nwe asked the classifier",
    "start": "940800",
    "end": "942690"
  },
  {
    "text": "to predict not only correctly,\nbut by a positive margin",
    "start": "942690",
    "end": "946650"
  },
  {
    "text": "of safety.",
    "start": "946650",
    "end": "948840"
  },
  {
    "text": "And just an aside,\nthis 1 could really",
    "start": "948840",
    "end": "950970"
  },
  {
    "text": "be 2, or 3, or any number\nas long as it's positive",
    "start": "950970",
    "end": "953910"
  },
  {
    "text": "and its magnitude effectively\ndetermines the regularization",
    "start": "953910",
    "end": "958379"
  },
  {
    "text": "strength if you're\nusing regularizers.",
    "start": "958380",
    "end": "961110"
  },
  {
    "text": "Don't worry if you\ndidn't get that.",
    "start": "961110",
    "end": "964930"
  },
  {
    "text": "OK, so also notice that the\nhinge loss is an upper bound",
    "start": "964930",
    "end": "967640"
  },
  {
    "text": "on the zero-one loss.",
    "start": "967640",
    "end": "968720"
  },
  {
    "text": "So this is cool because suppose\nyou optimize a hinge loss",
    "start": "968720",
    "end": "971878"
  },
  {
    "text": "and you drive it\ndown, drive it down.",
    "start": "971878",
    "end": "973420"
  },
  {
    "text": "This thing is going\nto start pushing",
    "start": "973420",
    "end": "974920"
  },
  {
    "text": "on the zero-one more or less.",
    "start": "974920",
    "end": "977500"
  },
  {
    "text": "And in particular, if you\nget this hinge loss of 0,",
    "start": "977500",
    "end": "980260"
  },
  {
    "text": "then what is the zero-one loss?",
    "start": "980260",
    "end": "982510"
  },
  {
    "text": "Well, it's also going to be 0.",
    "start": "982510",
    "end": "984400"
  },
  {
    "text": "So that's a nice fact.",
    "start": "984400",
    "end": "988020"
  },
  {
    "start": "988000",
    "end": "1048000"
  },
  {
    "text": "So here's a minor digression.",
    "start": "988020",
    "end": "989940"
  },
  {
    "text": "There's a lot of\nother loss functions.",
    "start": "989940",
    "end": "991710"
  },
  {
    "text": "Here is the logistic loss.",
    "start": "991710",
    "end": "993250"
  },
  {
    "text": "And we can just plot\nit on this diagram.",
    "start": "993250",
    "end": "995220"
  },
  {
    "text": "And you see that\nthe logistic loss",
    "start": "995220",
    "end": "996930"
  },
  {
    "text": "doesn't have this kink in it.",
    "start": "996930",
    "end": "998649"
  },
  {
    "text": "It has a smooth transition\nbetween something",
    "start": "998650",
    "end": "1001520"
  },
  {
    "text": "that's growing linearly to\nsomething that fades away to 0.",
    "start": "1001520",
    "end": "1006320"
  },
  {
    "text": "And the key property\nof the logistic loss",
    "start": "1006320",
    "end": "1010040"
  },
  {
    "text": "is that even if you\nwere out here, so",
    "start": "1010040",
    "end": "1013430"
  },
  {
    "text": "if you have a margin of 2, then\nyou're classifying correctly.",
    "start": "1013430",
    "end": "1016850"
  },
  {
    "text": "And the hinge loss\nwould say you get 0 loss",
    "start": "1016850",
    "end": "1018673"
  },
  {
    "text": "and you don't need\nto do anything.",
    "start": "1018673",
    "end": "1020089"
  },
  {
    "text": "But the logistic loss is greedy.",
    "start": "1020090",
    "end": "1021900"
  },
  {
    "text": "It says, well, you still\nhave a little bit of a loss.",
    "start": "1021900",
    "end": "1025800"
  },
  {
    "text": "And if you try to\nminimize a logistic loss,",
    "start": "1025800",
    "end": "1028280"
  },
  {
    "text": "you're going to try to keep\non pushing this margin as far",
    "start": "1028280",
    "end": "1032329"
  },
  {
    "text": "out as possible.",
    "start": "1032329",
    "end": "1035470"
  },
  {
    "text": "So the logistic loss is\ndifferentiable everywhere,",
    "start": "1035470",
    "end": "1040000"
  },
  {
    "text": "and smooth, and it's nice.",
    "start": "1040000",
    "end": "1041530"
  },
  {
    "text": "And it's typically known\nas logistic regression",
    "start": "1041530",
    "end": "1043569"
  },
  {
    "text": "because it has connections\nto probability.",
    "start": "1043569",
    "end": "1047230"
  },
  {
    "text": "OK, so let's now go\nback to the hinge loss.",
    "start": "1047230",
    "end": "1050080"
  },
  {
    "start": "1048000",
    "end": "1174000"
  },
  {
    "text": "Here is our friend,\nthe hinge loss.",
    "start": "1050080",
    "end": "1052570"
  },
  {
    "text": "And here is the expression\nfor the hinge loss.",
    "start": "1052570",
    "end": "1055539"
  },
  {
    "text": "And remember, it's the\nmaximum of two expressions--",
    "start": "1055540",
    "end": "1059410"
  },
  {
    "text": "this decreasing line\npart and then the 0 part,",
    "start": "1059410",
    "end": "1063880"
  },
  {
    "text": "in orange and\nblue, respectively.",
    "start": "1063880",
    "end": "1066910"
  },
  {
    "text": "OK, so now if we want to\napply gradient descent",
    "start": "1066910",
    "end": "1069190"
  },
  {
    "text": "to the hinge loss, we\nhave to take the gradient.",
    "start": "1069190",
    "end": "1072259"
  },
  {
    "text": "So how do we take the gradient?",
    "start": "1072260",
    "end": "1073730"
  },
  {
    "text": "So the gradient of the\nloss hinge is equal to.",
    "start": "1073730",
    "end": "1079270"
  },
  {
    "text": "And now, we have\nthis max thing, OK,",
    "start": "1079270",
    "end": "1081573"
  },
  {
    "text": "which might be a\nlittle bit scary.",
    "start": "1081573",
    "end": "1082990"
  },
  {
    "text": "But if you look up here, we can\njust do this kind of visually.",
    "start": "1082990",
    "end": "1087160"
  },
  {
    "text": "So what is the slope here?",
    "start": "1087160",
    "end": "1089290"
  },
  {
    "text": "Well, the slope here is whatever\nthe slope of the orange part",
    "start": "1089290",
    "end": "1093140"
  },
  {
    "text": "is.",
    "start": "1093140",
    "end": "1093640"
  },
  {
    "text": "And what is a slope here?",
    "start": "1093640",
    "end": "1095110"
  },
  {
    "text": "It's the slope of\nthis blue part.",
    "start": "1095110",
    "end": "1098440"
  },
  {
    "text": "And so now, we just have to\nswitch between the two cases.",
    "start": "1098440",
    "end": "1102139"
  },
  {
    "text": "So in particular,\nif the margin--",
    "start": "1102140",
    "end": "1106670"
  },
  {
    "text": "this orange part-- is\ngreater than 0, that",
    "start": "1106670",
    "end": "1112220"
  },
  {
    "text": "means we're in this region.",
    "start": "1112220",
    "end": "1114850"
  },
  {
    "text": "And then, the\ngradient is just going",
    "start": "1114850",
    "end": "1116770"
  },
  {
    "text": "to be the gradient\nof this expression.",
    "start": "1116770",
    "end": "1119560"
  },
  {
    "text": "1 is a constant.",
    "start": "1119560",
    "end": "1121360"
  },
  {
    "text": "So it's going to be minus.",
    "start": "1121360",
    "end": "1123760"
  },
  {
    "text": "We're differentiating\nwith respect to w.",
    "start": "1123760",
    "end": "1126370"
  },
  {
    "text": "So and phi of xy is a constant.",
    "start": "1126370",
    "end": "1129080"
  },
  {
    "text": "So it's just going\nto be phi of xy.",
    "start": "1129080",
    "end": "1132669"
  },
  {
    "text": "And then if this condition\ndoesn't hold otherwise,",
    "start": "1132670",
    "end": "1136330"
  },
  {
    "text": "that means we're in this region.",
    "start": "1136330",
    "end": "1138279"
  },
  {
    "text": "And what is the gradient of 0?",
    "start": "1138280",
    "end": "1140280"
  },
  {
    "text": "Well, that's the world's easiest\ndifferential calculus problem.",
    "start": "1140280",
    "end": "1144880"
  },
  {
    "text": "And it's 0.",
    "start": "1144880",
    "end": "1148530"
  },
  {
    "text": "OK, so this is the\ngradient of the hinge loss.",
    "start": "1148530",
    "end": "1151290"
  },
  {
    "text": "And just to kind of\nsanity check things--",
    "start": "1151290",
    "end": "1153970"
  },
  {
    "text": "so you have to\npick up an example",
    "start": "1153970",
    "end": "1155909"
  },
  {
    "text": "and it's on this\nside over here, then",
    "start": "1155910",
    "end": "1159630"
  },
  {
    "text": "the gradients are going to be 0.",
    "start": "1159630",
    "end": "1161070"
  },
  {
    "text": "And you're not going\nto update your weights.",
    "start": "1161070",
    "end": "1163799"
  },
  {
    "text": "On the other hand,\nif you are over here,",
    "start": "1163800",
    "end": "1166320"
  },
  {
    "text": "then the gradient\nwill be non-zero.",
    "start": "1166320",
    "end": "1168029"
  },
  {
    "text": "In particular, it's going\nto be minus phi of xy.",
    "start": "1168030",
    "end": "1170640"
  },
  {
    "start": "1170640",
    "end": "1174920"
  },
  {
    "start": "1174000",
    "end": "1354000"
  },
  {
    "text": "OK, so now, let's\nput things together",
    "start": "1174920",
    "end": "1176540"
  },
  {
    "text": "and revisit our example.",
    "start": "1176540",
    "end": "1179310"
  },
  {
    "text": "So here's the purple\nclassifier over here.",
    "start": "1179310",
    "end": "1183140"
  },
  {
    "text": "And here, we have\nsome training data.",
    "start": "1183140",
    "end": "1186260"
  },
  {
    "text": "And we're going\nto try to compute",
    "start": "1186260",
    "end": "1188143"
  },
  {
    "text": "the hinge loss on this training\ndata along with its gradient,",
    "start": "1188143",
    "end": "1190684"
  },
  {
    "text": "OK?",
    "start": "1190685",
    "end": "1192050"
  },
  {
    "text": "So remember, the hinge\nloss is the expression.",
    "start": "1192050",
    "end": "1196320"
  },
  {
    "text": "So let's look at the\nfirst point, 0, 2.",
    "start": "1196320",
    "end": "1199595"
  },
  {
    "text": "0, 2 is here, and it's\nlabeled as a positive.",
    "start": "1199595",
    "end": "1202620"
  },
  {
    "text": "So if you go and you plug that\npoint into the hinge loss,",
    "start": "1202620",
    "end": "1208340"
  },
  {
    "text": "then you get a max over\n1 minus the margin and 0.",
    "start": "1208340",
    "end": "1213289"
  },
  {
    "text": "So what is the margin here?",
    "start": "1213290",
    "end": "1215240"
  },
  {
    "text": "Well, it's this dot product,\nwhich happens to be 2.",
    "start": "1215240",
    "end": "1219290"
  },
  {
    "text": "So we have 1, minus 2, minus 1.",
    "start": "1219290",
    "end": "1222650"
  },
  {
    "text": "And my max of\nminus 1 and 0 is 0.",
    "start": "1222650",
    "end": "1225920"
  },
  {
    "text": "And that agrees\nwith our intuition",
    "start": "1225920",
    "end": "1228350"
  },
  {
    "text": "that the loss here\nshould be 0 because it's",
    "start": "1228350",
    "end": "1231260"
  },
  {
    "text": "correctly classified\nand correctly classified",
    "start": "1231260",
    "end": "1234020"
  },
  {
    "text": "by a margin of 2.",
    "start": "1234020",
    "end": "1236800"
  },
  {
    "text": "And now, let's look at the\nsecond point-- minus 2, 0.",
    "start": "1236800",
    "end": "1242020"
  },
  {
    "text": "So if we compute\nthe loss here, we",
    "start": "1242020",
    "end": "1244510"
  },
  {
    "text": "see that the loss is actually 2.",
    "start": "1244510",
    "end": "1247560"
  },
  {
    "text": "So notice that even though\nwe are getting this--",
    "start": "1247560",
    "end": "1253320"
  },
  {
    "text": "sorry, this loss is actually 2.",
    "start": "1253320",
    "end": "1256019"
  },
  {
    "text": "And that makes sense because\nwe misclassified this point.",
    "start": "1256020",
    "end": "1260210"
  },
  {
    "text": "So now if we look at\nthe third point here--",
    "start": "1260210",
    "end": "1263529"
  },
  {
    "start": "1263530",
    "end": "1266290"
  },
  {
    "text": "1, minus 1.",
    "start": "1266290",
    "end": "1268420"
  },
  {
    "text": "And the loss on\nthis example is 1.5.",
    "start": "1268420",
    "end": "1271450"
  },
  {
    "text": "So notice that even though\nwe're classifying this point",
    "start": "1271450",
    "end": "1274179"
  },
  {
    "text": "correctly, we're\nstill incurring a loss",
    "start": "1274180",
    "end": "1276790"
  },
  {
    "text": "because the margin was only 0.5\nand didn't meet the threshold.",
    "start": "1276790",
    "end": "1281830"
  },
  {
    "start": "1281830",
    "end": "1285630"
  },
  {
    "text": "Or I guess the\nloss is maybe 1.5.",
    "start": "1285630",
    "end": "1289320"
  },
  {
    "text": "Sorry, the margin is\n0.5, but the loss is 1.5.",
    "start": "1289320",
    "end": "1293929"
  },
  {
    "text": "So now we can also compute\nthe gradients here.",
    "start": "1293930",
    "end": "1297980"
  },
  {
    "text": "So the loss on the first point\nis 0 because the loss is 0.",
    "start": "1297980",
    "end": "1301780"
  },
  {
    "text": "And generally, not\nalways, if the loss is 0,",
    "start": "1301780",
    "end": "1305140"
  },
  {
    "text": "then the gradient\nwill be 0 as well.",
    "start": "1305140",
    "end": "1308200"
  },
  {
    "text": "And on the second one,\nthe loss is not 0,",
    "start": "1308200",
    "end": "1311710"
  },
  {
    "text": "so we have a non-zero gradient,\nwhich is minus phi of xy.",
    "start": "1311710",
    "end": "1319390"
  },
  {
    "text": "So it's this part\ntimes this minus sign.",
    "start": "1319390",
    "end": "1325640"
  },
  {
    "text": "And the third point\nalso has positive loss,",
    "start": "1325640",
    "end": "1329710"
  },
  {
    "text": "so it has a positive\ngradient, 1, minus 1.",
    "start": "1329710",
    "end": "1334450"
  },
  {
    "text": "Now, we can compute the\ntraining loss, which",
    "start": "1334450",
    "end": "1337929"
  },
  {
    "text": "is the average over the losses.",
    "start": "1337930",
    "end": "1339700"
  },
  {
    "text": "That gives us 1.17.",
    "start": "1339700",
    "end": "1341169"
  },
  {
    "text": "And the gradient of\nthe training loss",
    "start": "1341170",
    "end": "1343390"
  },
  {
    "text": "is just an average\nof the gradients.",
    "start": "1343390",
    "end": "1345370"
  },
  {
    "text": "And that gives us 1, minus 0.33.",
    "start": "1345370",
    "end": "1352890"
  },
  {
    "text": "OK, so let us now move on.",
    "start": "1352890",
    "end": "1355260"
  },
  {
    "start": "1354000",
    "end": "1576000"
  },
  {
    "text": "Let's concretize this in Python.",
    "start": "1355260",
    "end": "1359590"
  },
  {
    "text": "OK, so let's remember.",
    "start": "1359590",
    "end": "1361659"
  },
  {
    "text": "Last time, we coded up gradient\ndescent for linear regression.",
    "start": "1361660",
    "end": "1369320"
  },
  {
    "text": "So now, I'm going to\njust copy this, launch",
    "start": "1369320",
    "end": "1373179"
  },
  {
    "text": "gradientDescentHinge and\ndo it for the hinge loss.",
    "start": "1373180",
    "end": "1378010"
  },
  {
    "text": "OK, so I'm going to use\nthis as a starting point.",
    "start": "1378010",
    "end": "1380270"
  },
  {
    "text": "And I'm going to just\nchange a few things.",
    "start": "1380270",
    "end": "1386210"
  },
  {
    "text": "Let's change the\ntraining examples",
    "start": "1386210",
    "end": "1388990"
  },
  {
    "text": "because now we're working\nwith this training data.",
    "start": "1388990",
    "end": "1392360"
  },
  {
    "text": "So we have, just to\nkeep track of things,",
    "start": "1392360",
    "end": "1395420"
  },
  {
    "text": "so this is x, y pairs.",
    "start": "1395420",
    "end": "1397480"
  },
  {
    "text": "So x, now, is 0, 2, 1.",
    "start": "1397480",
    "end": "1404140"
  },
  {
    "text": "And then, the second\npoint is minus 2, 0.",
    "start": "1404140",
    "end": "1407410"
  },
  {
    "text": "And the third point\nis 1, minus 1.",
    "start": "1407410",
    "end": "1410470"
  },
  {
    "text": "And so we have three points--",
    "start": "1410470",
    "end": "1412240"
  },
  {
    "text": "xy, where x is a triple.",
    "start": "1412240",
    "end": "1416910"
  },
  {
    "text": "OK, so phi is just\ngoing to be x.",
    "start": "1416910",
    "end": "1423025"
  },
  {
    "start": "1423025",
    "end": "1425530"
  },
  {
    "text": "And the dimension of the\nweight vector is still 2.",
    "start": "1425530",
    "end": "1430630"
  },
  {
    "text": "And now, the key\nthing we have to do",
    "start": "1430630",
    "end": "1432220"
  },
  {
    "text": "is change the\ndefinition of the loss.",
    "start": "1432220",
    "end": "1437600"
  },
  {
    "text": "So now, let us see.",
    "start": "1437600",
    "end": "1440350"
  },
  {
    "text": "So before, we have\naverage over a sum here.",
    "start": "1440350",
    "end": "1444130"
  },
  {
    "text": "Instead of the square\nloss, I'm going",
    "start": "1444130",
    "end": "1447190"
  },
  {
    "text": "to make this the hinge loss.",
    "start": "1447190",
    "end": "1448669"
  },
  {
    "text": "So the hinge loss is max over\n1 minus the margin, OK, and 0.",
    "start": "1448670",
    "end": "1459400"
  },
  {
    "text": "So this is max over 1\nminus the margin and 0.",
    "start": "1459400",
    "end": "1464420"
  },
  {
    "text": "And the gradient of\nthat is going to be--",
    "start": "1464420",
    "end": "1469340"
  },
  {
    "text": "so let's actually\njust copy this down.",
    "start": "1469340",
    "end": "1474929"
  },
  {
    "text": "Let's delete this so we\ndo not confuse ourselves.",
    "start": "1474930",
    "end": "1479250"
  },
  {
    "text": "So remember, if this first\nexpression is greater than 0,",
    "start": "1479250",
    "end": "1486680"
  },
  {
    "text": "then the gradient is\nminus phi of x times y",
    "start": "1486680",
    "end": "1493070"
  },
  {
    "text": "if we're on that\nside of the curve.",
    "start": "1493070",
    "end": "1496880"
  },
  {
    "text": "And otherwise, it's\njust going to be 0.",
    "start": "1496880",
    "end": "1500180"
  },
  {
    "start": "1500180",
    "end": "1503230"
  },
  {
    "text": "OK, so that's it.",
    "start": "1503230",
    "end": "1504429"
  },
  {
    "text": "We just changed the\ntraining examples",
    "start": "1504430",
    "end": "1506350"
  },
  {
    "text": "and changed the definition\nof the loss function.",
    "start": "1506350",
    "end": "1511059"
  },
  {
    "text": "And the optimization\nalgorithm, we don't actually",
    "start": "1511060",
    "end": "1515050"
  },
  {
    "text": "have to change at all, OK?",
    "start": "1515050",
    "end": "1517780"
  },
  {
    "text": "So let's run this\nand see what we get.",
    "start": "1517780",
    "end": "1521485"
  },
  {
    "start": "1521485",
    "end": "1524640"
  },
  {
    "text": "So here's gradient descent.",
    "start": "1524640",
    "end": "1526130"
  },
  {
    "text": "It starts out with w equals 0.",
    "start": "1526130",
    "end": "1529040"
  },
  {
    "text": "And then, it starts\nmoving w to minus 0.5, 5.",
    "start": "1529040",
    "end": "1535070"
  },
  {
    "text": "You see that the train\nloss is decreasing nicely.",
    "start": "1535070",
    "end": "1538682"
  },
  {
    "text": "And actually, in\nthis case, it gets",
    "start": "1538682",
    "end": "1540140"
  },
  {
    "text": "to 0, which means that we--",
    "start": "1540140",
    "end": "1544198"
  },
  {
    "text": "remember, the hinge\nloss is the upper bound",
    "start": "1544198",
    "end": "1545990"
  },
  {
    "text": "of the zero-one loss.",
    "start": "1545990",
    "end": "1546865"
  },
  {
    "text": "So that means the\nzero-one loss is also 0.",
    "start": "1546865",
    "end": "1549260"
  },
  {
    "text": "And the gradient also\nvanishes and becomes 0,",
    "start": "1549260",
    "end": "1552980"
  },
  {
    "text": "meaning that we converged.",
    "start": "1552980",
    "end": "1554330"
  },
  {
    "start": "1554330",
    "end": "1557090"
  },
  {
    "text": "OK, so just to recap,\nall we did here",
    "start": "1557090",
    "end": "1559460"
  },
  {
    "text": "was changed the training\nexamples, the featurizer,",
    "start": "1559460",
    "end": "1562880"
  },
  {
    "text": "and redefined the loss.",
    "start": "1562880",
    "end": "1565120"
  },
  {
    "text": "And it's great that we didn't\nhave to touch the optimization",
    "start": "1565120",
    "end": "1567620"
  },
  {
    "text": "algorithm because this was meant\nto be a generic piece of code.",
    "start": "1567620",
    "end": "1571490"
  },
  {
    "start": "1571490",
    "end": "1574530"
  },
  {
    "text": "All right, so let us summarize.",
    "start": "1574530",
    "end": "1578470"
  },
  {
    "start": "1576000",
    "end": "1682000"
  },
  {
    "text": "And in particular, I'm\ngoing to contrast regression",
    "start": "1578470",
    "end": "1581470"
  },
  {
    "text": "with classification, since\nwe've seen two of them so far.",
    "start": "1581470",
    "end": "1585909"
  },
  {
    "text": "So the key quantity that drives\nthe prediction in both cases",
    "start": "1585910",
    "end": "1591730"
  },
  {
    "text": "is the score, the dot product\nbetween the weight vector",
    "start": "1591730",
    "end": "1594309"
  },
  {
    "text": "and the feature vector.",
    "start": "1594310",
    "end": "1596540"
  },
  {
    "text": "And in regression,\nthe prediction",
    "start": "1596540",
    "end": "1598760"
  },
  {
    "text": "is exactly just a raw score.",
    "start": "1598760",
    "end": "1600650"
  },
  {
    "text": "While in classification,\nyou stick it",
    "start": "1600650",
    "end": "1602840"
  },
  {
    "text": "through the sign function\nso you get 1 or minus 1.",
    "start": "1602840",
    "end": "1608809"
  },
  {
    "text": "How the prediction is\nrelated to the target--",
    "start": "1608810",
    "end": "1613340"
  },
  {
    "text": "well, in regression, we\nlooked at the residual,",
    "start": "1613340",
    "end": "1616799"
  },
  {
    "text": "which was the score minus y.",
    "start": "1616800",
    "end": "1619010"
  },
  {
    "text": "And in classification,\nwe're looking at the margin.",
    "start": "1619010",
    "end": "1621870"
  },
  {
    "text": "So in regression, low\nresidual was good.",
    "start": "1621870",
    "end": "1624515"
  },
  {
    "text": "And in classification,\nhigh margin",
    "start": "1624515",
    "end": "1626840"
  },
  {
    "text": "is good because we want score\nand y to have the same sign.",
    "start": "1626840",
    "end": "1629585"
  },
  {
    "start": "1629585",
    "end": "1632130"
  },
  {
    "text": "Using those quantities, we\ncan define loss functions.",
    "start": "1632130",
    "end": "1636010"
  },
  {
    "text": "So in regression, we\nlooked at the square loss.",
    "start": "1636010",
    "end": "1638310"
  },
  {
    "text": "But as I mentioned\nbriefly, you can also",
    "start": "1638310",
    "end": "1640170"
  },
  {
    "text": "do the absolute deviation loss.",
    "start": "1640170",
    "end": "1642030"
  },
  {
    "text": "In classification, the story\nbecomes a little bit stranger",
    "start": "1642030",
    "end": "1646320"
  },
  {
    "text": "because we generally care\nabout the zero-one loss, that's",
    "start": "1646320",
    "end": "1649289"
  },
  {
    "text": "our misclassification rate.",
    "start": "1649290",
    "end": "1651060"
  },
  {
    "text": "But we can't optimize\nit, so we have",
    "start": "1651060",
    "end": "1652890"
  },
  {
    "text": "to come up with a\nsurrogate loss function,",
    "start": "1652890",
    "end": "1655050"
  },
  {
    "text": "like the hinge\nloss, which we went",
    "start": "1655050",
    "end": "1657120"
  },
  {
    "text": "into depth and\nthe logistic loss,",
    "start": "1657120",
    "end": "1659050"
  },
  {
    "text": "which we briefly mentioned.",
    "start": "1659050",
    "end": "1661650"
  },
  {
    "text": "And given the loss\nfunctions in both cases,",
    "start": "1661650",
    "end": "1664690"
  },
  {
    "text": "we use the gradient\ndescent algorithm",
    "start": "1664690",
    "end": "1666840"
  },
  {
    "text": "to optimize the loss function.",
    "start": "1666840",
    "end": "1670116"
  },
  {
    "text": "And that's it.",
    "start": "1670116",
    "end": "1670940"
  },
  {
    "text": "That concludes the unit\non linear classification.",
    "start": "1670940",
    "end": "1675559"
  },
  {
    "text": "Thanks for listening.",
    "start": "1675560",
    "end": "1677710"
  },
  {
    "start": "1677710",
    "end": "1682000"
  }
]