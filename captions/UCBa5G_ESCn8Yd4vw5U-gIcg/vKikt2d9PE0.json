[
  {
    "start": "0",
    "end": "120000"
  },
  {
    "start": "0",
    "end": "9805"
  },
  {
    "text": "PAX HEHMEYER: It's my pleasure,\nagain, to be hosting Dan Boneh. So Dan heads the Applied\nCryptography Group,",
    "start": "9805",
    "end": "15310"
  },
  {
    "text": "and co-directs the Computer\nSecurity Lab here at Stanford. His research focuses on\napplications of cryptography",
    "start": "15310",
    "end": "21400"
  },
  {
    "text": "to computer security. In addition to\nbeing a researcher, he is also an\ninnovative educator.",
    "start": "21400",
    "end": "27760"
  },
  {
    "text": "Even before COVID kind of\npushed all of this online, he had been doing a lot\nonline and putting a lot of--",
    "start": "27760",
    "end": "33130"
  },
  {
    "text": "making a lot of\nmaterials free out there. So one thing he does,\nhe is co-director of Stanford's Advanced\nCyber Security Professional",
    "start": "33130",
    "end": "39629"
  },
  {
    "text": "Program, which I'll\ntalk a little bit more about at the end of\nthe webinar, as well as our online cybersecurity\ngraduate program.",
    "start": "39630",
    "end": "46330"
  },
  {
    "text": "He also has a MOOC on\nCoursera on cryptography, and he has co-written\na free online textbook",
    "start": "46330",
    "end": "51940"
  },
  {
    "text": "on applied cryptography that\nyou can find on his website. I think the latest edition\nis from January 2020,",
    "start": "51940",
    "end": "57309"
  },
  {
    "text": "but it's been there since 2008. So again, it's my pleasure\nto host Dan today, who's",
    "start": "57310",
    "end": "62469"
  },
  {
    "text": "going to be talking about the\nprivacy and security machine learning models. And with that, I'm going\nto hand it over to him.",
    "start": "62470",
    "end": "69697"
  },
  {
    "text": "DAN BONEH: So let's get started. So welcome everyone. I'm excited to be talking\nabout this machine learning",
    "start": "69697",
    "end": "75863"
  },
  {
    "text": "and security and privacy,\nof in particular, of machine learning. So this will be\nkind of an overview talk about some of the\nlatest research in this area.",
    "start": "75863",
    "end": "84950"
  },
  {
    "text": "But it's mostly meant to be kind\nof a high-level introduction. And I'll give some\nmore materials",
    "start": "84950",
    "end": "90909"
  },
  {
    "text": "for people who\nwant to learn more about this topic at\nthe end of the webinar.",
    "start": "90910",
    "end": "96010"
  },
  {
    "text": "So with that, let's get started. And I'm actually really looking\nforward to your questions.",
    "start": "96010",
    "end": "101750"
  },
  {
    "text": "So please type your questions,\nand I'll come back to them at the end of the talk. And feel free to ask questions\nabout anything in security.",
    "start": "101750",
    "end": "109240"
  },
  {
    "text": "I know this topic,\nwe're primarily going to be talking\nabout security of machine learning in this webinar.",
    "start": "109240",
    "end": "114640"
  },
  {
    "text": "But feel free to ask anything\nin computer security. So let's get started.",
    "start": "114640",
    "end": "121070"
  },
  {
    "start": "120000",
    "end": "460000"
  },
  {
    "text": "So machine learning is,\nas you know, is amazing, in that it's able to\nachieve superhuman",
    "start": "121070",
    "end": "126610"
  },
  {
    "text": "performance on many,\nmany different tasks now. However, what I want to show you\nin the next couple of minutes",
    "start": "126610",
    "end": "134230"
  },
  {
    "text": "is basically that even though\nmachine learning works really well on sort of random examples\nout there in the real world,",
    "start": "134230",
    "end": "140469"
  },
  {
    "text": "it turns out under adversarial\nexamples, specifically in adversarial environments,\nit tends to have some problems.",
    "start": "140470",
    "end": "146980"
  },
  {
    "text": "And that's what I\nwant to talk about. But before we get\nto that, I just want to quickly make sure\nwe're all on the same page.",
    "start": "146980",
    "end": "152142"
  },
  {
    "text": "So let me talk a little bit\nabout what the machine learning pipeline looks like. I'll specifically\ntalk about what's",
    "start": "152142",
    "end": "157780"
  },
  {
    "text": "called supervised learning. So imagine we want to train\nsome cars to self drive. Yeah?",
    "start": "157780",
    "end": "163180"
  },
  {
    "text": "So what we do is basically we\ngo through a data collection phase, where we collect data\nfrom all the cars out there.",
    "start": "163180",
    "end": "171250"
  },
  {
    "text": "So everybody sends data. Let's say we want to train\nthem to recognize road signs. So everybody collects\nlabelled data.",
    "start": "171250",
    "end": "178239"
  },
  {
    "text": "You can see that this car is\nsaying that this is a yield sign, and labeled as a yield. This car said it's a stop sign.",
    "start": "178240",
    "end": "183880"
  },
  {
    "text": "Labeled as a stop. Generally, these\nthings are denoted as x is the example\nand y is the label.",
    "start": "183880",
    "end": "189800"
  },
  {
    "text": "So we get many examples and\nmany corresponding labels. And we're trying to learn\nto build the classifier that",
    "start": "189800",
    "end": "195760"
  },
  {
    "text": "classifies road signs. So we take all the examples that\nwe collected, all the labeled",
    "start": "195760",
    "end": "201549"
  },
  {
    "text": "examples we collected,\nand we pushed them through a machine\nlearning model, a machine learning algorithm.",
    "start": "201550",
    "end": "207790"
  },
  {
    "text": "And that's called\nthe training phase. So what the machine\nlearning algorithm does is it produces what we call\nmodel parameters, which",
    "start": "207790",
    "end": "214209"
  },
  {
    "text": "are typically denoted by theta. And those model\nparameters basically are the parameters that\ndrive the neural net.",
    "start": "214210",
    "end": "221810"
  },
  {
    "text": "So what is a neural net? A neural net\nbasically is a graph, as you can see in\nthe picture here.",
    "start": "221810",
    "end": "227080"
  },
  {
    "text": "And the model\nparameters basically are the weights on the\nedges in this graph.",
    "start": "227080",
    "end": "232720"
  },
  {
    "text": "So we have these model\nparameters that the training algorithm produced. Then the next thing we do is\nwe send the model parameters",
    "start": "232720",
    "end": "240280"
  },
  {
    "text": "over back to all the cars. And now, the cars can\nbasically use this new model",
    "start": "240280",
    "end": "246040"
  },
  {
    "text": "that we just trained to\nrecognize road signs. What's really kind of\namazing about this is it's very different from\nhow humans learn.",
    "start": "246040",
    "end": "252939"
  },
  {
    "text": "So humans basically,\neach one of us has to run, has to collect\ndata, set examples on our own,",
    "start": "252940",
    "end": "258609"
  },
  {
    "text": "and then we run the training\nalgorithm in our own minds. And then we use the results\nof the training to kind of do",
    "start": "258610",
    "end": "263770"
  },
  {
    "text": "the tasks we need to do. Whereas in machine\nlearning, what's interesting is if one car\nfinds an example that's",
    "start": "263770",
    "end": "269800"
  },
  {
    "text": "not classified correctly,\nyou know, a road sign that's not classified correctly, it\nsends it back to the home base.",
    "start": "269800",
    "end": "275680"
  },
  {
    "text": "In the home base, they rerun\nthe training algorithm, and then all the cars\nget better as a result.",
    "start": "275680",
    "end": "281470"
  },
  {
    "text": "So it's kind of a\nvery different process from what humans go through. So once we send the\nmodel to all the cars,",
    "start": "281470",
    "end": "287320"
  },
  {
    "text": "we have the inference step,\nwhere basically now a car sees a particular road sign.",
    "start": "287320",
    "end": "293500"
  },
  {
    "text": "It uses the model theta\nthat was sent to it. And then it\nclassifies, basically saying this is a yield sign\nwith very high probability,",
    "start": "293500",
    "end": "300520"
  },
  {
    "text": "and it's a stop sign with\nvery low probability. So then the car says, oh, I'm\nprobably seeing the yield sign.",
    "start": "300520",
    "end": "305710"
  },
  {
    "text": "So this is how basically machine\nlearning is supposed to work. However, as we'll\nsee in just a minute,",
    "start": "305710",
    "end": "312240"
  },
  {
    "text": "there are some\npossibilities for attacks. In all of this pipeline,\nthere are these three steps.",
    "start": "312240",
    "end": "318080"
  },
  {
    "text": "Data collection,\ntraining, and inference. It turns out in all three\nsteps of this pipeline,",
    "start": "318080",
    "end": "323390"
  },
  {
    "text": "there are opportunities\nfor attack. So let's see how\nthis would work.",
    "start": "323390",
    "end": "328580"
  },
  {
    "text": "So the first example I want\nto go through very quickly is a fantastic example of\nan attack during training.",
    "start": "328580",
    "end": "337199"
  },
  {
    "text": "This is an example due to one\nof my colleagues, Priscilla Yang and the students. And it really kind of\ndrives the point on",
    "start": "337200",
    "end": "343370"
  },
  {
    "text": "how reliable we are on\nvalid training data. So what they did\nis they basically,",
    "start": "343370",
    "end": "349970"
  },
  {
    "text": "they looked at--\nthey were trying to build a classifier\nto classify images of dogs versus fish.",
    "start": "349970",
    "end": "357050"
  },
  {
    "text": "So the two classes\nwere dogs and fish. So they had like 6,000\nimages in their data set.",
    "start": "357050",
    "end": "362750"
  },
  {
    "text": "And what they did is\nthey took just one image, one of the 6,000 labeled images.",
    "start": "362750",
    "end": "368270"
  },
  {
    "text": "And they added very\ncarefully chosen noise to it. And you see the noise\nwas scaled down.",
    "start": "368270",
    "end": "374000"
  },
  {
    "text": "So really, the resulting image\nlooks like the original image. Except a little bit of low\nvolume noise was added to it.",
    "start": "374000",
    "end": "381200"
  },
  {
    "text": "And you notice this picture\nhas a dog and a fish in it. What they did is they\nlabeled this image as a fish.",
    "start": "381200",
    "end": "388370"
  },
  {
    "text": "So they really\npoisoned just one entry in the entire training set.",
    "start": "388370",
    "end": "393470"
  },
  {
    "text": "And as a result, when you\ntrain an algorithm now using this poisoned data sets, so\n6,000 images, only one of them",
    "start": "393470",
    "end": "400220"
  },
  {
    "text": "was poisoned. It turns out the model\nactually produces an incorrect-- the\ntraining algorithm produces",
    "start": "400220",
    "end": "406639"
  },
  {
    "text": "an incorrect model. So for example, if you show the\nnew model, these pictures that are clearly pictures\nof dogs, the old model,",
    "start": "406640",
    "end": "413990"
  },
  {
    "text": "the original model without\nthe poisoned data set, you can see says this is a dog\nwith 97%, 98%, 98% probability.",
    "start": "413990",
    "end": "420710"
  },
  {
    "text": "However, once you poison one\nelement in the training set, all of a sudden now\nthe model becomes",
    "start": "420710",
    "end": "426440"
  },
  {
    "text": "confident this is a fish with\n97%, 93%, and 87% probability.",
    "start": "426440",
    "end": "432050"
  },
  {
    "text": "So I think this is a\npretty strong statement about the training data,\nthat if an attacker can",
    "start": "432050",
    "end": "437150"
  },
  {
    "text": "look at your training data\nand basically corrupt just one entry, say just one car,\nsends one corrupt entry, that",
    "start": "437150",
    "end": "444830"
  },
  {
    "text": "actually is enough to\ncompletely lead the training algorithm awry. And as a result,\nthe resulting model",
    "start": "444830",
    "end": "452330"
  },
  {
    "text": "just makes incorrect results,\nincorrect conclusions. So again, keep that in mind\nwhen you run-- when you collect",
    "start": "452330",
    "end": "457550"
  },
  {
    "text": "training data, you have to make\nsure the training data comes from trusted sources. The more interesting\nattacks I want to talk about",
    "start": "457550",
    "end": "463940"
  },
  {
    "start": "460000",
    "end": "964000"
  },
  {
    "text": "are attacks during inference. So after we've\ntrained the model, and now we show it\na bunch of examples,",
    "start": "463940",
    "end": "470062"
  },
  {
    "text": "the question is how\nwell does it classify the examples we show it. And so here there's\na very large field",
    "start": "470062",
    "end": "476510"
  },
  {
    "text": "of what's called adversarial\nmachine learning, or particularly, it's\ncalled adversarial examples. This was created\nback in 2013, 2014.",
    "start": "476510",
    "end": "484940"
  },
  {
    "text": "And it's an amazing set\nof results that basically says the following thing.",
    "start": "484940",
    "end": "490280"
  },
  {
    "text": "You can train the model\nto recognize cats. You can give it a\npicture of a cat,",
    "start": "490280",
    "end": "495980"
  },
  {
    "text": "and the model says very high\nconfidence, this is a cat. The amazing thing is you can\nnow take this image of a cat",
    "start": "495980",
    "end": "502160"
  },
  {
    "text": "and add a little\nbit of noise to it. So again, this is\ncarefully chosen noise. I'll explain how we choose\nit in just a minute.",
    "start": "502160",
    "end": "508830"
  },
  {
    "text": "And then you tamper it down. You multiply it by\na very small number. So the actual\nperturbation to the image",
    "start": "508830",
    "end": "514669"
  },
  {
    "text": "is a very, very\nsmall perturbation. We just changed the pixel\nvalues by a very small amount.",
    "start": "514669",
    "end": "521030"
  },
  {
    "text": "You can see that when you look\nat the final image, to a human, there's no difference. This image looks exactly\nlike the original image,",
    "start": "521030",
    "end": "527960"
  },
  {
    "text": "but it's been\nperturbed a little bit. The amazing thing is\nwhen you give the model this perturbed image,\nall of a sudden",
    "start": "527960",
    "end": "533930"
  },
  {
    "text": "the model says I am\ncompletely confident that this is guacamole. Yeah, not a cat. It's something\ncompletely different.",
    "start": "533930",
    "end": "540300"
  },
  {
    "text": "So how does this work? How do how do we generate\nthese adversarial examples? I should say, this is what's\ncalled an adversarial example.",
    "start": "540300",
    "end": "546829"
  },
  {
    "text": "You give me a model. I construct an example that the\nmodel gets completely wrong.",
    "start": "546830",
    "end": "553230"
  },
  {
    "text": "So how do we do this? Well, here you have like a\nthree-dimensional terrain of different images. So every pixel here\ncorresponds to one image.",
    "start": "553230",
    "end": "561710"
  },
  {
    "text": "The height of the\nterrain corresponds to how confident the model is\nthat the given image is a cat.",
    "start": "561710",
    "end": "567710"
  },
  {
    "text": "So you can see the\ngray area corresponds to the great cat zone. Everything in the gray area\nwould be considered a cat.",
    "start": "567710",
    "end": "574970"
  },
  {
    "text": "Everything in the,\nI don't know what this is, this blue area would\nbe considered guacamole.",
    "start": "574970",
    "end": "581540"
  },
  {
    "text": "So these two\nconcept classes, you notice they have a close\nboundary between them.",
    "start": "581540",
    "end": "589160"
  },
  {
    "text": "And it turns out\nthere's one direction, one specific direction\nthat if you go down, then it turns out\nwith very few changes,",
    "start": "589160",
    "end": "596390"
  },
  {
    "text": "you have to make a very small\nstep, very small perturbation that takes you into\nthe guacamole step.",
    "start": "596390",
    "end": "602600"
  },
  {
    "text": "You notice that if you\nwalk in a random direction, so you go right or left, you're\ngoing to stay in the cat area",
    "start": "602600",
    "end": "608149"
  },
  {
    "text": "direction. So actually, just adding\na random perturbation is not going to confuse\nthe model at all.",
    "start": "608150",
    "end": "614699"
  },
  {
    "text": "But if you take a\nperturbation that's just in the right direction,\nthat pushes the model quickly into the guacamole area,\nthen the model gets confused.",
    "start": "614700",
    "end": "623000"
  },
  {
    "text": "Then it thinks that this\ncat, this perturbed cat is guacamole. So this is how this works.",
    "start": "623000",
    "end": "628040"
  },
  {
    "text": "And the algorithm for doing this\nis called FGSM, fast gradient sign method, which basically\nlooks for the steepest descent",
    "start": "628040",
    "end": "636410"
  },
  {
    "text": "in this terrain. And then it just\nmakes a few steps in the gradient\ndirection, and that",
    "start": "636410",
    "end": "642110"
  },
  {
    "text": "ensures that the category\nis going to change with very small perturbations.",
    "start": "642110",
    "end": "647890"
  },
  {
    "text": "That's the idea of\nadversarial examples. One thing that I\nwanted to point out is that, in fact, it turns\nout there are many directions",
    "start": "647890",
    "end": "655730"
  },
  {
    "text": "that you can go in that would\nintroduce adversarial examples. Here in the previous\nslide, I said",
    "start": "655730",
    "end": "661100"
  },
  {
    "text": "you walk in the\ngradient direction. That's one direction that would\ncause an adversarial example to appear.",
    "start": "661100",
    "end": "666590"
  },
  {
    "text": "But it turns out we did an\nexperiment a few years ago to show that, for example,\nfor these models, these three",
    "start": "666590",
    "end": "671930"
  },
  {
    "text": "models, V3, V3 adv and\nanother variant of V3, these are models for\nidentifying images,",
    "start": "671930",
    "end": "678899"
  },
  {
    "text": "it turns out something like 60%\nof the images in the data set have like 20\nadversarial directions.",
    "start": "678900",
    "end": "685980"
  },
  {
    "text": "So if you think about\nan image, an image is a point in a very\nhigh dimensional space.",
    "start": "685980",
    "end": "693350"
  },
  {
    "text": "An image has thousands, if\nnot millions of pixels in it. So we're in the\ndimension that we're",
    "start": "693350",
    "end": "698790"
  },
  {
    "text": "in could be in the\nhundreds of thousands. Every pixel corresponds\nto one dimension.",
    "start": "698790",
    "end": "704240"
  },
  {
    "text": "And yet, the gradient\ngives you one direction to get adversarial examples.",
    "start": "704240",
    "end": "710168"
  },
  {
    "text": "But it turns out\nmany images actually have many directions that will\ngive you adversarial examples. All right? So FGSM is one way to do it.",
    "start": "710168",
    "end": "717150"
  },
  {
    "text": "But in fact, there are\nother ways to do it as well. It turns out now these\nadversarial examples basically",
    "start": "717150",
    "end": "722810"
  },
  {
    "text": "affect everything. Yeah. Literally every model\nthat has been trained is vulnerable to\nadversarial example.",
    "start": "722810",
    "end": "728850"
  },
  {
    "text": "So let me just give\nyou a few examples. So one classic one\nis a model that's trained to recognize faces.",
    "start": "728850",
    "end": "735199"
  },
  {
    "text": "So you give it,\nlet's say, 50 faces. You train it to\nrecognize 1 of 50 faces.",
    "start": "735200",
    "end": "740540"
  },
  {
    "text": "And then it turns out if\nyou want to confuse it, there was this really\nbeautiful experiment in 2016",
    "start": "740540",
    "end": "746839"
  },
  {
    "text": "this showed that you\ncan print 3D glasses. These are physical 3D glasses\nthat you can put on your face.",
    "start": "746840",
    "end": "752700"
  },
  {
    "text": "And when you put those\nglasses on your face, the model all of a\nsudden thinks that you're Brad Pitt, if it's been\ntrained to recognize Brad Pitt.",
    "start": "752700",
    "end": "760000"
  },
  {
    "text": "So just by printing 3D glasses\nand putting them on your face, you can confuse the\nmodel into thinking",
    "start": "760000",
    "end": "766280"
  },
  {
    "text": "that you are in a class\nthat you're not actually in. And of course, the\nreason that works",
    "start": "766280",
    "end": "771560"
  },
  {
    "text": "is because the model picked\nup on some categories around the eyes, and some\nfeatures around the eyes.",
    "start": "771560",
    "end": "777380"
  },
  {
    "text": "And these glasses basically\nmirror the features that the model is\nactually looking for. Another example, famous\nexample is road signs.",
    "start": "777380",
    "end": "786770"
  },
  {
    "text": "So it turns out you\ncan take a stop sign, train the model to\nrecognize stop signs, recognize speed limit signs.",
    "start": "786770",
    "end": "793399"
  },
  {
    "text": "You can put a few\nstickers on the stop sign. And all of a sudden,\nagain, to us this still looks like a stop sign.",
    "start": "793400",
    "end": "798545"
  },
  {
    "text": "But all of a sudden,\nthe model says, oh, this is a 40 miles per\nhour speed limit sign. It's not a stop sign.",
    "start": "798545",
    "end": "804380"
  },
  {
    "text": "So if a car was\nusing this, the car would not stop, even\nthough it's supposed to. And the third example I'll\ngive is in the audio domain.",
    "start": "804380",
    "end": "811400"
  },
  {
    "text": "Again, a very famous example. We all have these phones\nwhere you say, hey, Siri, and the phone wakes up\nand executes our commands.",
    "start": "811400",
    "end": "818201"
  },
  {
    "text": "Well, what they showed\nis that you can actually create white noise. This literally just sounds like\nwhite noise to the human ear.",
    "start": "818202",
    "end": "825210"
  },
  {
    "text": "But it's very carefully\ncrafted white noise that the model interprets\nthis as the user saying, hey",
    "start": "825210",
    "end": "831350"
  },
  {
    "text": "Siri, open evil.com. You might be holding your\nphone in some environment. All of a sudden, this white\nnoise plays in the background.",
    "start": "831350",
    "end": "838700"
  },
  {
    "text": "And your phone ends up going\nto evil.com as a result. To you, it just sounds\nlike white noise.",
    "start": "838700",
    "end": "844050"
  },
  {
    "text": "So you have no idea that\nyour phone is under attack. So these are all\nexamples of-- these are all adversarial examples\nwhere the model is trying",
    "start": "844050",
    "end": "851432"
  },
  {
    "text": "to do one thing,\nbut we can give it a carefully crafted malicious\nexample, and the model all",
    "start": "851432",
    "end": "857330"
  },
  {
    "text": "of a sudden it gets confused. So why is this happening? Well here, I'll just\nsay this very briefly.",
    "start": "857330",
    "end": "862820"
  },
  {
    "text": "I'll just tell you that\nthe solid line here represents the true concept\nwe're trying to learn,",
    "start": "862820",
    "end": "868910"
  },
  {
    "text": "like stop signs. And the dashed line represents\nwhat the model actually learns.",
    "start": "868910",
    "end": "874210"
  },
  {
    "text": "And those two things\nare different. The model learns sort\nof an approximation of the true concept.",
    "start": "874210",
    "end": "880760"
  },
  {
    "text": "And if you look to\nthe far left, you'll see that there is an x\nthat if you perturb it just by a little bit, you\nmove it to a point",
    "start": "880760",
    "end": "887839"
  },
  {
    "text": "where the model, actually it's\noutside of the dashed lines. So the model will\nclassify it differently.",
    "start": "887840",
    "end": "893240"
  },
  {
    "text": "But in reality, it\nstill is a stop sign. Because it's still\ninside of the solid line.",
    "start": "893240",
    "end": "898350"
  },
  {
    "text": "So you can see the\nx on the far left. I don't have a pointer, so I\ncan't quite show this to you. But if you just look at\nthe x and the far left,",
    "start": "898350",
    "end": "904778"
  },
  {
    "text": "you can see a very\nsmall perturbation moves it out of the model\nboundary, but leaves it inside",
    "start": "904778",
    "end": "910680"
  },
  {
    "text": "of the true boundary. And that's why these\nperturbation examples, adversarial examples exist.",
    "start": "910680",
    "end": "916110"
  },
  {
    "text": "Basically, in high\ndimensional space, there is always a\nsmall perturbation that moves you from one concept\nclass to another concept class.",
    "start": "916110",
    "end": "925150"
  },
  {
    "text": "So why do these adversarial\nexamples matter? Well, they matter\nfor a lot of reasons. First of all, it's kind\nof interesting to learn,",
    "start": "925150",
    "end": "931830"
  },
  {
    "text": "to understand basically how\ndoes the model actually work. They help us understand\nwhat was actually learned.",
    "start": "931830",
    "end": "937450"
  },
  {
    "text": "And they also help us understand\nwhat did these models, like we said, what do they do?",
    "start": "937450",
    "end": "943320"
  },
  {
    "text": "Like why is it that\nthey generalize? For security, these\nare really important. So for security, it's\nreally kind of important",
    "start": "943320",
    "end": "949230"
  },
  {
    "text": "to understand how likely\nis a machine learning model that I just trained. How likely is it to\nfail under attack?",
    "start": "949230",
    "end": "955980"
  },
  {
    "text": "If somebody gives\nit a bad example, an adversarial\nexample, how likely is it to actually\nmiscategorize the results?",
    "start": "955980",
    "end": "962610"
  },
  {
    "text": "That's what we'd\nlike to understand. Now, this area,\nlike I said, this was, first, these kind of\nattacks first came out in 2014.",
    "start": "962610",
    "end": "971550"
  },
  {
    "start": "964000",
    "end": "1109000"
  },
  {
    "text": "So what, this area is\nlike seven years old now. And since then there\nhave been thousands",
    "start": "971550",
    "end": "976590"
  },
  {
    "text": "of papers written on more\nattacks and many, many attempts at defending against\nthese attacks.",
    "start": "976590",
    "end": "982960"
  },
  {
    "text": "So many, many defensive\ntechniques have been tried. Unfortunately, many of\nthese proposed defenses",
    "start": "982960",
    "end": "989037"
  },
  {
    "text": "have actually been broken. And there's actually sort\nof a cat and mouse game where somebody writes a\ndefense, and somebody then",
    "start": "989037",
    "end": "994350"
  },
  {
    "text": "implements an attack. So that it's been quite\nchallenging to come up",
    "start": "994350",
    "end": "999899"
  },
  {
    "text": "with robust defenses. It turns out some of the\ndefenses that we have work. And these are some examples of\ndefenses that do a decent job.",
    "start": "999900",
    "end": "1009530"
  },
  {
    "text": "However, it's\nimportant to understand that these defenses basically\nonly defend against one",
    "start": "1009530",
    "end": "1014990"
  },
  {
    "text": "type of perturbation. For example, you might\nonly want to defend against perturbations that\nhave what's called low L2 norm.",
    "start": "1014990",
    "end": "1022970"
  },
  {
    "text": "So that you can modify\nall the pixels but only by a small amount. However, there\ncould be other types",
    "start": "1022970",
    "end": "1029270"
  },
  {
    "text": "of perturbations where\nmaybe you can modify a small number of pixels. But those small pixels,\nyou can modify quite a lot.",
    "start": "1029270",
    "end": "1036140"
  },
  {
    "text": "That corresponds to putting a\nsticker on a stop sign, where you're modifying a\nsmall number of pixels,",
    "start": "1036140",
    "end": "1042230"
  },
  {
    "text": "but those you're\nmodifying quite a bit. That's sometimes called\na hamming perturbation.",
    "start": "1042230",
    "end": "1047359"
  },
  {
    "text": "Maybe an L0 perturbation. And so it turns out that\nthese methods that do work,",
    "start": "1047359",
    "end": "1052640"
  },
  {
    "text": "they work very well for the\nset of perturbations they were trained to defend against. But they don't work very\nwell once you give them",
    "start": "1052640",
    "end": "1059960"
  },
  {
    "text": "a new type of perturbation. They, again, start\nto be vulnerable to adversarial examples. That's what happens.",
    "start": "1059960",
    "end": "1066440"
  },
  {
    "text": "So that's kind of\na result that says that they're very good at\ndefending against the chosen",
    "start": "1066440",
    "end": "1071450"
  },
  {
    "text": "sets. But that's the\nresult on the left. But they're not very good at\ndefending against other types",
    "start": "1071450",
    "end": "1076850"
  },
  {
    "text": "of perturbations. What's weird is something\nwe showed, again, in 2019, that if you try to train a model\nto be robust to work multiple",
    "start": "1076850",
    "end": "1084770"
  },
  {
    "text": "types of perturbations-- let's\nsay both low L2 norm and low hamming weight norm--",
    "start": "1084770",
    "end": "1090440"
  },
  {
    "text": "it turns out the models you\nend up with actually are not as robust as you would like. So it's kind of\ninteresting that training",
    "start": "1090440",
    "end": "1097880"
  },
  {
    "text": "to be robust to\nmultiple perturbations is not as good as training\nagainst a narrower",
    "start": "1097880",
    "end": "1104750"
  },
  {
    "text": "set of perturbations. So that's kind of\nthe state of the art. Unfortunately, what this\nmeans is as of today,",
    "start": "1104750",
    "end": "1111620"
  },
  {
    "start": "1109000",
    "end": "1315000"
  },
  {
    "text": "we still don't have\na complete defense against these\nadversarial examples. So this is a pretty\ninteresting thing to remember.",
    "start": "1111620",
    "end": "1119000"
  },
  {
    "text": "So any of you who actually\nuse machine learning model and the machine learning\nmodels in the real world, basically whenever\nsomeone comes to you",
    "start": "1119000",
    "end": "1126169"
  },
  {
    "text": "and say that you're using ML\nfor certain tasks, you should always ask them, what\nhappens if somebody presents",
    "start": "1126170",
    "end": "1132799"
  },
  {
    "text": "an adversarial example to you? So for example, if you use an\nML model to classify spam email",
    "start": "1132800",
    "end": "1138680"
  },
  {
    "text": "or to classify security events. Or maybe even in\nmedical conditions, or maybe there's a reason\nto behave adverserially,",
    "start": "1138680",
    "end": "1146720"
  },
  {
    "text": "you should always keep\nin the back of your mind this question of what\nhappens if somebody presents an adversarial example and the\nmodel actually misclassifies.",
    "start": "1146720",
    "end": "1155309"
  },
  {
    "text": "So if you're training a car\nto recognize road signs, you have to ask\nyourself, well, what",
    "start": "1155310",
    "end": "1160549"
  },
  {
    "text": "happens if my model\nactually encounters an adversarial example and\nmisclassifies the road sign.",
    "start": "1160550",
    "end": "1166850"
  },
  {
    "text": "So that's kind of a\nvery important lesson to take away from this line\nof research, that you always,",
    "start": "1166850",
    "end": "1172700"
  },
  {
    "text": "always, always have to\nhave adversarial examples in your mind when you try\nto deploy machine learning algorithms in the real world.",
    "start": "1172700",
    "end": "1179110"
  },
  {
    "text": "It turns out-- so you\nshould ask yourself, is it possible to have\ncompletely robust adversarial",
    "start": "1179110",
    "end": "1185870"
  },
  {
    "text": "models-- sorry, completely robust\nmachine learning models? Well, humans are pretty good. We're pretty robust.",
    "start": "1185870",
    "end": "1191183"
  },
  {
    "text": "You can kind of show us\nall sorts of example-- all sorts of perturbed\nimages, and we would just",
    "start": "1191183",
    "end": "1196429"
  },
  {
    "text": "ignore the perturbations. But it turns out that\neven we are not perfect. There are a lot of\noptical illusions.",
    "start": "1196430",
    "end": "1203600"
  },
  {
    "text": "Here's a famous\noptical illusion, where if I show you\nthese two shapes, and I ask you which\none is bigger,",
    "start": "1203600",
    "end": "1209990"
  },
  {
    "text": "you would say, oh,\nclearly shape B is bigger. At least, most of us will\nsee that shape B is bigger.",
    "start": "1209990",
    "end": "1215333"
  },
  {
    "text": "But it turns out if\nyou just look at it, stare at it a little\nbit longer you, realize these two\nshapes are identical. In fact, if you look at\nwhere these edges are,",
    "start": "1215333",
    "end": "1222309"
  },
  {
    "text": "the right side edges, you'll\nsee that they're exactly one underneath the other. So these two shapes\nare actually identical,",
    "start": "1222310",
    "end": "1228160"
  },
  {
    "text": "but somehow our optical-- but somehow our visual\nsystem makes this mistake. So it's quite possible\nthat maybe even we",
    "start": "1228160",
    "end": "1236530"
  },
  {
    "text": "think of ourselves as robust,\nbut clearly we're not robust either. So maybe there is no\nperfectly robust ML model.",
    "start": "1236530",
    "end": "1243178"
  },
  {
    "text": "And we'll never have\nsomething that's not going to be vulnerable\nto adversarial example. By the way, what's\ninteresting about",
    "start": "1243178",
    "end": "1249550"
  },
  {
    "text": "this is also that all of us were\ntrained on different examples.",
    "start": "1249550",
    "end": "1255160"
  },
  {
    "text": "And all of us used our own\nbuilt in training algorithm. So there are however many\npeople on the planet.",
    "start": "1255160",
    "end": "1262090"
  },
  {
    "text": "All of us trained our\nvisual system using examples that we saw when\nwe were growing up.",
    "start": "1262090",
    "end": "1267160"
  },
  {
    "text": "And yet this example\nfools all of us. Or a large fraction of us.",
    "start": "1267160",
    "end": "1272260"
  },
  {
    "text": "And so what this illustrates\nis a really interesting example of a phenomenon called\ntransferability.",
    "start": "1272260",
    "end": "1278530"
  },
  {
    "text": "Where in fact, if you train\none model in one way on one data set, and you build\nadversarial examples",
    "start": "1278530",
    "end": "1284350"
  },
  {
    "text": "against that model, it turns out\nthose adversarial examples will also for other models that were\ntrained on independent data",
    "start": "1284350",
    "end": "1290830"
  },
  {
    "text": "sets in different ways. So even in machine learning,\nthere is transferability, where one adversarial\nexample against one model",
    "start": "1290830",
    "end": "1298960"
  },
  {
    "text": "can confuse a model\nthat was trained completely independently. Just like this confuses me.",
    "start": "1298960",
    "end": "1304150"
  },
  {
    "text": "This image confuses me. But it also confuses you. Even though both of us\nwere trained on completely",
    "start": "1304150",
    "end": "1310870"
  },
  {
    "text": "independent labeled examples. ",
    "start": "1310870",
    "end": "1316300"
  },
  {
    "start": "1315000",
    "end": "1511000"
  },
  {
    "text": "The last example\nI want to show you in terms of inference attacks is\na kind of work we did last year",
    "start": "1316300",
    "end": "1324190"
  },
  {
    "text": "or so on an area where you\nwould think ML would be useful.",
    "start": "1324190",
    "end": "1329230"
  },
  {
    "text": "But it turns out\nit's actually quite vulnerable to\nadversarial examples. So let me tell you the\nstory, because it's",
    "start": "1329230",
    "end": "1335080"
  },
  {
    "text": "kind of a cute story. So this is basically\nan example where",
    "start": "1335080",
    "end": "1340420"
  },
  {
    "text": "you'd like to use machine\nlearning to block ads. So you have a web page.",
    "start": "1340420",
    "end": "1345700"
  },
  {
    "text": "The web page displays ads. Like you go to the\nNew York Times. And the New York Times\ndisplays ads to you.",
    "start": "1345700",
    "end": "1351850"
  },
  {
    "text": "And the user, some\nusers would like to actually block those ads so\nthey can just read the content without seeing the ads.",
    "start": "1351850",
    "end": "1357669"
  },
  {
    "text": "So there's this perpetual\nwar between ad blockers like Adblock Plus, which you\ncan install on your browser",
    "start": "1357670",
    "end": "1364450"
  },
  {
    "text": "to block ads, and\nthe publishers, like the New York Times. The New York Times wants the\nads to be shown on the screen.",
    "start": "1364450",
    "end": "1370270"
  },
  {
    "text": "And the ad blocker wants the ads\nto be taken off the screen, so that the user won't see them.",
    "start": "1370270",
    "end": "1375370"
  },
  {
    "text": "So there's this war between the\nad blockers and the publishers. And so what the\npublishers do is they make",
    "start": "1375370",
    "end": "1382180"
  },
  {
    "text": "their page very complicated. So the HTML on the page is\nvery complicated to parse, so it becomes very difficult for\nthe ad blocker to tell what's--",
    "start": "1382180",
    "end": "1390430"
  },
  {
    "text": "just by looking at the\ncode, it's very difficult to tell it to tell what's\nan ad and what's not an ad. So a few years ago,\nthere was this idea",
    "start": "1390430",
    "end": "1397029"
  },
  {
    "text": "called perceptual\nad blocking, where it was supposed to be kind of\nthe end all and ultimate ad",
    "start": "1397030",
    "end": "1402820"
  },
  {
    "text": "blocker. So what it would do\nis it would actually render the page as the page\nis supposed to be rendered.",
    "start": "1402820",
    "end": "1408945"
  },
  {
    "text": "And then it would actually\njust look at the pixels. And the idea was to train\na machine learning model",
    "start": "1408945",
    "end": "1413980"
  },
  {
    "text": "to recognize ads on the page. Humans can recognize ads\non the page pretty well. So let's train a\nmachine learning model",
    "start": "1413980",
    "end": "1420580"
  },
  {
    "text": "to recognize ads on the page. And then once, we\nfrom the pixels, once we recognize which\npixels corresponds to ads,",
    "start": "1420580",
    "end": "1428440"
  },
  {
    "text": "we can just block those pixels. Yeah. So perceptual ad\nblocking is supposed to be this ultimate way to\nprevent ads from showing,",
    "start": "1428440",
    "end": "1436000"
  },
  {
    "text": "because the machine learning\nmodel will recognize the ads, and then just block them. So this is something,\nAd Block Plus",
    "start": "1436000",
    "end": "1442510"
  },
  {
    "text": "was actually considering\ndeploying this in the system called Sentinel. However, it turns out\nthis is the hardest",
    "start": "1442510",
    "end": "1448720"
  },
  {
    "text": "setting for machine learning. First of all, the model\nis completely public. Every browser contains\na copy of the model,",
    "start": "1448720",
    "end": "1456070"
  },
  {
    "text": "because well, it needs\nto decide what's an ad and what's not an ad. The classification task\nhas to be really fast.",
    "start": "1456070",
    "end": "1462070"
  },
  {
    "text": "The page loads, and then\nimmediately the model has to say what's an ad\nand what's not an ad. You can't think for 30 minutes\nbefore you decide what to block",
    "start": "1462070",
    "end": "1469210"
  },
  {
    "text": "and what not to block. And then the way the\ntraining data was collected was basically by\nhaving Adblock Plus,",
    "start": "1469210",
    "end": "1476290"
  },
  {
    "text": "or rather, ad blockers\nout there in the wild send examples of pages with ads. And then you would train\nbased on those examples.",
    "start": "1476290",
    "end": "1485299"
  },
  {
    "text": "So in some sense, this\nis the hardest model. A model that's public. Classification has to be fast. And the training\ndata can be polluted.",
    "start": "1485300",
    "end": "1491600"
  },
  {
    "text": "It's a very hard setting\nfor machine learning. And as a result, it's\nnot too surprising that there are attacks on this.",
    "start": "1491600",
    "end": "1498190"
  },
  {
    "text": "So this is joint work with my\nstudent Florian Tramer, Dupre,",
    "start": "1498190",
    "end": "1503320"
  },
  {
    "text": "Rusak, and Pellegrino. And what's this project\ndid is basically",
    "start": "1503320",
    "end": "1511549"
  },
  {
    "start": "1511000",
    "end": "1718000"
  },
  {
    "text": "we kind of trained a\nmachine learning model to recognize ads. So you can see here\non the left, you",
    "start": "1511550",
    "end": "1517070"
  },
  {
    "text": "can see a page with\na bunch of ads in it. And you can see\nthe model actually does a pretty good job\nof actually picking out",
    "start": "1517070",
    "end": "1522710"
  },
  {
    "text": "what the ads are. As the page scrolls,\nyou can see it actually recognizes what's an ad,\nand what's not an ad.",
    "start": "1522710",
    "end": "1528830"
  },
  {
    "text": "However, if you want to defeat\nthis, all you have to do is add a little bit\nof adversarial noise.",
    "start": "1528830",
    "end": "1535080"
  },
  {
    "text": "What's the adversarial noise? It's basically going to be a\ntransparent background that overlays on top of the web page.",
    "start": "1535080",
    "end": "1542150"
  },
  {
    "text": "So all the New York\nTimes has to do is add this sort of\ntransparent background",
    "start": "1542150",
    "end": "1547940"
  },
  {
    "text": "that modifies the page\njust a little bit, and so you can see this\nvideo on the right.",
    "start": "1547940",
    "end": "1554960"
  },
  {
    "text": "Here there's this\ntransparent background. To the user, this is\nbasically invisible. But it just adds a\nfew-- it just modifies",
    "start": "1554960",
    "end": "1562070"
  },
  {
    "text": "the pixel and the pixels on\nthe page just a little bit. And as a result, the model\nno longer recognizes the ads.",
    "start": "1562070",
    "end": "1567980"
  },
  {
    "text": "You see, none of the\nads are now flagged. And basically, all the\nads will be displayed,",
    "start": "1567980",
    "end": "1573650"
  },
  {
    "text": "and the model doesn't\nactually see them. So again, it's an\nexample showing that adversarial examples can\ndefeat, very easily defeat",
    "start": "1573650",
    "end": "1581060"
  },
  {
    "text": "Ad Block. Could defeat a model that\nis trained to recognize ads. So you can say,\nwell, wait a minute.",
    "start": "1581060",
    "end": "1586460"
  },
  {
    "text": "Recognizing ads is pretty easy. Every ad is required to be\nlabeled by a certain logo",
    "start": "1586460",
    "end": "1593180"
  },
  {
    "text": "saying that it's an ad. Like something called\nthe Ad Choices logo. So the ad is supposed\nto be marked as an ad.",
    "start": "1593180",
    "end": "1599450"
  },
  {
    "text": "So why can't we train the\nmodel to just recognize the Ad Choices logo. And then from that we can\ndeduce that it's an ad.",
    "start": "1599450",
    "end": "1606230"
  },
  {
    "text": "And then the ad blocker\nwould block that ad. Well, it turns out you can\nactually even fool that. So you can kind of build\nversions of the Ad Choices logo",
    "start": "1606230",
    "end": "1614540"
  },
  {
    "text": "that look like the Ad\nChoices logo to a human. But the model would not\nrecognize it as the Ad Choices.",
    "start": "1614540",
    "end": "1621799"
  },
  {
    "text": "So again, it would\nmiss a lot of ads because it doesn't\nrecognize this logo that",
    "start": "1621800",
    "end": "1627170"
  },
  {
    "text": "was built adversarially. Again, that's a way to basically\nevade this ad blocking machine",
    "start": "1627170",
    "end": "1633980"
  },
  {
    "text": "learning model. And finally, the most\ninteresting thing is not only can you get\nads to show, you",
    "start": "1633980",
    "end": "1639320"
  },
  {
    "text": "can actually confuse the model\ninto blocking the wrong thing. So this is a pretty\ncute example.",
    "start": "1639320",
    "end": "1644550"
  },
  {
    "text": "So imagine we have like\na social network where multiple people can load\nimages onto the same page.",
    "start": "1644550",
    "end": "1651150"
  },
  {
    "text": "It turns out, so\nwe have Tom here at the top loaded an image. It turns out Jerry on the\nbottom can actually upload here.",
    "start": "1651150",
    "end": "1659643"
  },
  {
    "text": "Let me show you again. He can upload a malicious image. And what that image\ndoes is it basically",
    "start": "1659643",
    "end": "1665120"
  },
  {
    "text": "confuses the segmentation\nmechanism of the ad recognition model.",
    "start": "1665120",
    "end": "1670550"
  },
  {
    "text": "So the model, as a\nresult, now thinks that not only is\nJerry not an ad,",
    "start": "1670550",
    "end": "1676190"
  },
  {
    "text": "the model gets confused into\nthinking that Tom is an ad. And as a result, it\ngoes ahead and blocks",
    "start": "1676190",
    "end": "1681890"
  },
  {
    "text": "somebody else's content. So just to make sure this is\nclear, the model, what happened is the adversarial example sort\nof confused the fragmentation",
    "start": "1681890",
    "end": "1694190"
  },
  {
    "text": "mechanism in the\nmachine learning model. So it doesn't recognize\nthe borders of the ads.",
    "start": "1694190",
    "end": "1699860"
  },
  {
    "text": "And as a result, it thinks\nthat somebody else's content actually is part of the ad\nand goes ahead and blocks it.",
    "start": "1699860",
    "end": "1706880"
  },
  {
    "text": "So not only can you get\nyour own ads to show, you can actually block\nother content on the page.",
    "start": "1706880",
    "end": "1713039"
  },
  {
    "text": "So that was kind of a problem. And as a result, these methods\nactually have not actually been deployed.",
    "start": "1713040",
    "end": "1718730"
  },
  {
    "start": "1718000",
    "end": "2168000"
  },
  {
    "text": "All right. So I think I told you enough\nabout adversarial examples. Let's move on to another topic.",
    "start": "1718730",
    "end": "1723770"
  },
  {
    "text": "I just want to make\nsure you understand that adversarial examples\nare kind of a risk to machine learning models, and you\nalways, always, always",
    "start": "1723770",
    "end": "1730700"
  },
  {
    "text": "have to think about them\nwhen you try to deploy a model in the real world. So the next thing I\nwant to talk about",
    "start": "1730700",
    "end": "1736910"
  },
  {
    "text": "is actually how do we\nprotect data that's being fed into the model.",
    "start": "1736910",
    "end": "1742580"
  },
  {
    "text": "So here's a situation\nthat I want to discuss, which is basically a\nsettings where you the user",
    "start": "1742580",
    "end": "1749840"
  },
  {
    "text": "have a lot of data that\nyou'd like to classify. Maybe you have tens\nof thousands of images",
    "start": "1749840",
    "end": "1755090"
  },
  {
    "text": "that you want to classify\nusing a machine learning model. So typically, what\nyou would do is you would move\nthe model to the--",
    "start": "1755090",
    "end": "1761150"
  },
  {
    "text": "you would store the\nmodel in the cloud. The cloud, as you can see,\nhas a racks and racks of GPUs.",
    "start": "1761150",
    "end": "1767870"
  },
  {
    "text": "These things here\non the right are GPUs that are really\nfast and good at running",
    "start": "1767870",
    "end": "1774080"
  },
  {
    "text": "machine learning models. And so you can send your\n100,000 images to the cloud.",
    "start": "1774080",
    "end": "1779730"
  },
  {
    "text": "The cloud will use its\nspecialized hardware to classify those images, and\nsend the results back to you.",
    "start": "1779730",
    "end": "1786059"
  },
  {
    "text": "Well, what if you\ndon't trust the cloud? Suppose the cloud\nis adversarial, and you actually don't want\nto send your private pictures",
    "start": "1786060",
    "end": "1792060"
  },
  {
    "text": "to the cloud. What do you do then? Well, so there are a couple\nof things we might ask for.",
    "start": "1792060",
    "end": "1798233"
  },
  {
    "text": "First of all, we\nwant to make sure that the cloud has integrity. In other words, the\ncloud correctly runs",
    "start": "1798233",
    "end": "1805880"
  },
  {
    "text": "the machine learning\nmodel and provides us with the right results. Another thing we might want is\nprivacy where the cloud can't",
    "start": "1805880",
    "end": "1812160"
  },
  {
    "text": "actually look at the data. And the third\nthing we might want is to keep the model private,\nso that the cloud doesn't even",
    "start": "1812160",
    "end": "1819060"
  },
  {
    "text": "learn how our model works. So what kind of natural things\nwe might ask the cloud to do.",
    "start": "1819060",
    "end": "1825180"
  },
  {
    "text": "So here's a very\nnatural way to do it. And that's using basically what\nare called hardware enclaves. So hardware enclaves are,\nagain, things that all of you",
    "start": "1825180",
    "end": "1832800"
  },
  {
    "text": "should know about. So this is basically a\nway to dedicate a process,",
    "start": "1832800",
    "end": "1838320"
  },
  {
    "text": "actually dedicate a processor,\nso some specialized hardware,",
    "start": "1838320",
    "end": "1843840"
  },
  {
    "text": "or even there are\nenclaves available on certain standard processors.",
    "start": "1843840",
    "end": "1849870"
  },
  {
    "text": "So examples of enclaves are\nthings called Intel SGX, RISC-V has an enclave\ndesign called Keystone.",
    "start": "1849870",
    "end": "1856890"
  },
  {
    "text": "Arm has an enclave design. So this is basically a\npart of the processor that",
    "start": "1856890",
    "end": "1862860"
  },
  {
    "text": "can hold secret data,\nand even an administrator of that processor\ncan't get the data out.",
    "start": "1862860",
    "end": "1868830"
  },
  {
    "text": "So the way to think\nabout enclaves is basically you hold a\nprocessor in your hands.",
    "start": "1868830",
    "end": "1874169"
  },
  {
    "text": "The processor is working. Think of a motherboard. Like a server, a motherboard\nthat you hold in your hands.",
    "start": "1874170",
    "end": "1881070"
  },
  {
    "text": "It's executing a certain\ntask, and yet there is nothing you can do to extract\nsecret data from the enclave.",
    "start": "1881070",
    "end": "1887850"
  },
  {
    "text": "So the enclave in some sense\nis protecting data inside of it from the owner of the machine\nthat owns that server.",
    "start": "1887850",
    "end": "1897510"
  },
  {
    "text": "So that's what a\nhardware enclave does. And so the idea would be that\nwe will encrypt all of our data",
    "start": "1897510",
    "end": "1904086"
  },
  {
    "text": "that we're going to\nsend to the cloud, so all this x input\nwill get encrypted under a public key, where\nthe secret key lives",
    "start": "1904087",
    "end": "1912210"
  },
  {
    "text": "inside of a cloud enclave. We would then send all this\nencrypted data to the cloud.",
    "start": "1912210",
    "end": "1918010"
  },
  {
    "text": "So now the cloud administrator\ndoesn't see any of it. It's all encrypted. But when we load it\ninto the enclave,",
    "start": "1918010",
    "end": "1923669"
  },
  {
    "text": "the enclave has\na decryption key. It can go ahead and\ndecrypt the data, run the machine learning\nmodel, get the result,",
    "start": "1923670",
    "end": "1930480"
  },
  {
    "text": "and send the result back\nto us, while deleting the decrypted data. So the point is that the\nmodel and the decrypted data",
    "start": "1930480",
    "end": "1938700"
  },
  {
    "text": "only live inside of\nthe hardware enclave. Nobody else can see that data,\nbecause the hardware enclave",
    "start": "1938700",
    "end": "1945150"
  },
  {
    "text": "protects it. And as a result, the\ncloud administrator, even if it's malicious,\nit can't extract the data",
    "start": "1945150",
    "end": "1951630"
  },
  {
    "text": "from the enclave. Of course, this depends on\nthe security of the enclave. If somehow the\ncloud administrator",
    "start": "1951630",
    "end": "1957660"
  },
  {
    "text": "is able to tamper\nwith the enclave and extract values from\nit, then all bets are off. But if we have an\nenclave and we assume",
    "start": "1957660",
    "end": "1964260"
  },
  {
    "text": "that it's too expensive\nto actually break into the enclave,\nthen this is actually a pretty interesting\nway to protect data",
    "start": "1964260",
    "end": "1970350"
  },
  {
    "text": "that you send to the cloud. There are actually a\ncouple of companies that are deploying this\napproach to protecting data.",
    "start": "1970350",
    "end": "1976799"
  },
  {
    "text": "So this is one way to run\nprivately in the cloud, so that the cloud learns\nnothing about our data.",
    "start": "1976800",
    "end": "1983250"
  },
  {
    "text": "However, there is a problem. The problem is that the\ncloud invested a lot of money in building and\nbuying all these GPUs.",
    "start": "1983250",
    "end": "1990149"
  },
  {
    "text": "The GPUs themselves don't\nhave enclaves on them. GPUs are general\npurpose processors.",
    "start": "1990150",
    "end": "1996360"
  },
  {
    "text": "And so the GPUs are\nactually just designed to do some things very fast.",
    "start": "1996360",
    "end": "2001550"
  },
  {
    "text": "They don't have\nenclaves on them. As a result, with\nthis architecture, your GPUs are basically\ncompletely idle.",
    "start": "2001550",
    "end": "2007340"
  },
  {
    "text": "Everything has to be done on\nthe general-purpose CPU, which is much, much, much slower.",
    "start": "2007340",
    "end": "2014160"
  },
  {
    "text": "So the question is what to do. And so we designed\na system called Slalom that actually allows the\nhardware enclave to interact",
    "start": "2014160",
    "end": "2021260"
  },
  {
    "text": "with the GPUs. So it's a multi-round protocol. Effectively, it's like\none round for every layer",
    "start": "2021260",
    "end": "2027320"
  },
  {
    "text": "of the neural net. Yeah, so you interact between\nthe general-purpose CPU, which has the enclave, and the\nGPUs, which has no enclave.",
    "start": "2027320",
    "end": "2035809"
  },
  {
    "text": "Effectively, everything\nthat gets sent to the GPU is encrypted, and everything\nthat comes back from the GPU",
    "start": "2035810",
    "end": "2041600"
  },
  {
    "text": "is verified to be correct\nby the hardware enclave. So this ping pong\nhappens back and forth.",
    "start": "2041600",
    "end": "2047750"
  },
  {
    "text": "Even if you break into the GPUs\nand look at everything that's on them, you learn nothing.",
    "start": "2047750",
    "end": "2053090"
  },
  {
    "text": "And you can't\ncorrupt the results, because it's all being checked\nby the general purpose CPU.",
    "start": "2053090",
    "end": "2058158"
  },
  {
    "text": "So that comes back to\nthe hardware enclave. And then the hardware\nenclave does its job",
    "start": "2058159",
    "end": "2063169"
  },
  {
    "text": "and sends the results\nback to the client. So that's how this works. That's how Slalom works.",
    "start": "2063170",
    "end": "2069169"
  },
  {
    "text": "It's basically a way to\nuse GPUs in the cloud, even if you want to protect your\ndata using a hardware enclave.",
    "start": "2069170",
    "end": "2075830"
  },
  {
    "text": "The amazing thing\nis this actually does achieve quite\nsignificant performance gains.",
    "start": "2075830",
    "end": "2081060"
  },
  {
    "text": "So for example,\nhere are a couple of models for recognizing\nimages that we measured.",
    "start": "2081060",
    "end": "2087500"
  },
  {
    "text": "And you can see that if\nI'll look at the, let's see, the left bars.",
    "start": "2087500",
    "end": "2093350"
  },
  {
    "text": "Yeah, these three left bars-- sorry, the bars on the right. The three bars on the right\nrepresent both integrity",
    "start": "2093350",
    "end": "2100579"
  },
  {
    "text": "and privacy. So the model can't-- the GPU can't see the\ndata in the clear. And the GPU can't lie\nabout the results.",
    "start": "2100580",
    "end": "2107780"
  },
  {
    "text": "And you can see, we're\ngetting like factors of 10, or factor of 4, or factor of\n4.6 performance improvements",
    "start": "2107780",
    "end": "2113840"
  },
  {
    "text": "over running things on the CPU. This is a factor of 10\nfaster than running the job",
    "start": "2113840",
    "end": "2119810"
  },
  {
    "text": "on the CPU. So this actually gives quite\nsignificant performance improvements. Of course, even with\nthis, it's still slower",
    "start": "2119810",
    "end": "2128090"
  },
  {
    "text": "than running things natively\nwith no privacy on the GPU. Security always comes at a cost.",
    "start": "2128090",
    "end": "2133370"
  },
  {
    "text": "Nothing you can do about that. But at least it's not as bad\nas running natively on the CPU.",
    "start": "2133370",
    "end": "2139940"
  },
  {
    "text": "You're still able to use\na GPU that's in the cloud. So what I want you to\ntake away from this is if you do want to run\nyour jobs in an enclave",
    "start": "2139940",
    "end": "2148369"
  },
  {
    "text": "in the cloud,\nthere's still a way to use GPUs and gain\nperformance improvements",
    "start": "2148370",
    "end": "2154550"
  },
  {
    "text": "over running everything\ninstead of the enclave. So that's kind of what I wanted\nyou to take away from this.",
    "start": "2154550",
    "end": "2160650"
  },
  {
    "text": "So this system is called Slalom. You're welcome to look at it. And if you're interested in\nusing it, please let us know.",
    "start": "2160650",
    "end": "2168029"
  },
  {
    "start": "2168000",
    "end": "2392000"
  },
  {
    "text": "All right. The last thing I\nwant to talk about-- I guess I only have-- I want to stop it at 45, so\nI only have eight minutes.",
    "start": "2168030",
    "end": "2174410"
  },
  {
    "text": "And then we'll take questions. I'm really looking\nforward to your questions,",
    "start": "2174410",
    "end": "2180110"
  },
  {
    "text": "since this will be a\nfun topic to discuss. So the last thing I\nwant to talk about is basically how do we\nkeep the training-- how do",
    "start": "2180110",
    "end": "2187280"
  },
  {
    "text": "we keep training data private? So here's the problem. The problem is that when\nwe train a neural net using",
    "start": "2187280",
    "end": "2196190"
  },
  {
    "text": "collected data samples, the data\nsamples can often be private.",
    "start": "2196190",
    "end": "2201470"
  },
  {
    "text": "So in fact, what might happen\nis that we train and neural net, and after the neural\nnet gets trained,",
    "start": "2201470",
    "end": "2207200"
  },
  {
    "text": "the attacker might try to\ninteract with the neural net and extract the training\ndata from the neural net.",
    "start": "2207200",
    "end": "2212750"
  },
  {
    "text": "Think of like a\nneural net that's trained to recognize certain,\nI don't know, certain health",
    "start": "2212750",
    "end": "2220730"
  },
  {
    "text": "conditions based on various\nimages, based on X-ray images that it's trained on.",
    "start": "2220730",
    "end": "2226010"
  },
  {
    "text": "Well, once the\nmodel is available and is sent to\nall the hospitals, we want to make sure that\nan adversary can't actually",
    "start": "2226010",
    "end": "2232760"
  },
  {
    "text": "look at the model and somehow\nextract the images from which the model was trained.",
    "start": "2232760",
    "end": "2238930"
  },
  {
    "text": "That's kind of the goal. We want to train a\nmodel, but we want to make sure that the\nunderlying data set actually",
    "start": "2238930",
    "end": "2244520"
  },
  {
    "text": "remains private. So let me show you\nhow that's done. By the way, I should say\nthese are really quite",
    "start": "2244520",
    "end": "2251270"
  },
  {
    "text": "beautiful slides, and they were\nmade by my students Florian Tramer, so he deserves credit\nfor these beautiful, beautiful",
    "start": "2251270",
    "end": "2257720"
  },
  {
    "text": "animations. So traditionally\nthe way we achieve privacy for the\nunderlying training data,",
    "start": "2257720",
    "end": "2264392"
  },
  {
    "text": "yes, we train the\nmodel and we want to make sure that it's\nimpossible to extract the training data from the\nmodel that we just trained.",
    "start": "2264392",
    "end": "2271400"
  },
  {
    "text": "The way we do that is using\na wonderful, wonderful idea called differential privacy. This dates back to 2006.",
    "start": "2271400",
    "end": "2277970"
  },
  {
    "text": "It's due to Dwork et al. And the idea behind\ndifferential privacy is basically to add noise\nto the training process",
    "start": "2277970",
    "end": "2285300"
  },
  {
    "text": "so that no single training\ndata, training element would be extractable from the model.",
    "start": "2285300",
    "end": "2292070"
  },
  {
    "text": "So technically, what\ndifferential privacy means is if I train on the top three\nimages, versus if I train",
    "start": "2292070",
    "end": "2298640"
  },
  {
    "text": "on the bottom three images,\nyou notice the only difference between the top and the bottom\nis that the images on the left",
    "start": "2298640",
    "end": "2305286"
  },
  {
    "text": "are different. I guess in one case, it's a cat. In the other case, it's\na cat wearing a mask.",
    "start": "2305287",
    "end": "2311539"
  },
  {
    "text": "The point is that those two\nmodels basically look the same. We get the same result up to\nsome e to the epsilon value.",
    "start": "2311540",
    "end": "2319368"
  },
  {
    "text": "Yeah, that's kind of the point. So whether, if I change only\none element in my training data,",
    "start": "2319368",
    "end": "2324410"
  },
  {
    "text": "the resulting model is\ngoing to be the same. That's kind of the requirement\nfor differential privacy. And so you should ask\nme, what is this epsilon?",
    "start": "2324410",
    "end": "2331070"
  },
  {
    "text": "And it turns out\nthe smaller epsilon is, the more privacy we have.",
    "start": "2331070",
    "end": "2336200"
  },
  {
    "text": "So the goal is to come up\nwith training algorithms that achieve the smallest\npossible epsilon.",
    "start": "2336200",
    "end": "2342829"
  },
  {
    "text": "So the way that's\ndone typically is you might know that\nwhen you train a model, you use gradient descents.",
    "start": "2342830",
    "end": "2348960"
  },
  {
    "text": "So I'm not going to explain\nhow gradient descent works. But that's how the model works. And the idea behind\nthe private training",
    "start": "2348960",
    "end": "2356270"
  },
  {
    "text": "is to basically add noise at\nevery gradient descent step. You can see these red arrows\nbasically kind of add noise.",
    "start": "2356270",
    "end": "2363800"
  },
  {
    "text": "And the final model that we\nend up with, it turns out, can actually be proven to\nsatisfy a certain differential",
    "start": "2363800",
    "end": "2369710"
  },
  {
    "text": "privacy guarantee. All right? So that's kind of the idea\nbehind private training.",
    "start": "2369710",
    "end": "2374720"
  },
  {
    "text": "And again, the differential\nprivacy guarantee means that nobody's data\ninfluenced the final model",
    "start": "2374720",
    "end": "2382020"
  },
  {
    "text": "by much. So that means that you cannot\nextract an individual's data just from the trained model.",
    "start": "2382020",
    "end": "2389080"
  },
  {
    "text": "So that's what private\ntraining means. The problem is that\nprivate training actually",
    "start": "2389080",
    "end": "2394560"
  },
  {
    "start": "2392000",
    "end": "2442000"
  },
  {
    "text": "affects the accuracy\nof the resulting model. In particular, if we\ntrain without privacy,",
    "start": "2394560",
    "end": "2399803"
  },
  {
    "text": "it's really quite\namazing when you train a model to\nrecognize images, we achieve incredible accuracy.",
    "start": "2399803",
    "end": "2404860"
  },
  {
    "text": "Yeah, this is very close to\n100% in terms of accuracy. However, the minute you\ndo private training--",
    "start": "2404860",
    "end": "2412110"
  },
  {
    "text": "by the way, this accuracy, this\nis all things to deep learning. This is actually worth\nsaying, that the deep learning",
    "start": "2412110",
    "end": "2419850"
  },
  {
    "text": "era where we build\nmodels that are many, many, many layers,\nthat's what enables us to achieve such high accuracy.",
    "start": "2419850",
    "end": "2428430"
  },
  {
    "text": "In the old days,\npre-2011, everybody was using shallow models,\nvery, very few layers.",
    "start": "2428430",
    "end": "2434530"
  },
  {
    "text": "And as a result, you can see\nthe accuracy back in 2009, 2010 was dramatically lower.",
    "start": "2434530",
    "end": "2441059"
  },
  {
    "text": "So the interesting thing is that\nonce you train with privacy, even if you train\na deep neural net,",
    "start": "2441060",
    "end": "2446610"
  },
  {
    "start": "2442000",
    "end": "2495000"
  },
  {
    "text": "it turns out that there's a\nsignificant impact to accuracy.",
    "start": "2446610",
    "end": "2452610"
  },
  {
    "text": "You see there's a 40%\ndrop in inaccuracy as a result of private training.",
    "start": "2452610",
    "end": "2459359"
  },
  {
    "text": "So that's kind of\nbased on the latest work on private training.",
    "start": "2459360",
    "end": "2464973"
  },
  {
    "text": "So that's interesting. So there's a trade between\nthe level of privacy you want and the accuracy of the model.",
    "start": "2464973",
    "end": "2471760"
  },
  {
    "text": "What's interesting is because\nthe drop is so significant, it turns out if you go\nback to the old days of how",
    "start": "2471760",
    "end": "2478470"
  },
  {
    "text": "image recognition works, using\nbasically pre-trained features, then you can actually just\nuse very simple classifiers.",
    "start": "2478470",
    "end": "2487320"
  },
  {
    "text": "This is the star here. That actually achieves privacy,\nand actually, interestingly, beats the deep\nneural net methods.",
    "start": "2487320",
    "end": "2494380"
  },
  {
    "text": "So let me explain\nwhat happened here. So the idea is-- maybe in some\nsense, I should say",
    "start": "2494380",
    "end": "2500040"
  },
  {
    "text": "the reason the private training\nprivate gradient descent, it has trouble, the\naccuracy is hurt somewhat,",
    "start": "2500040",
    "end": "2508710"
  },
  {
    "text": "is that you have to touch\nthe data many, many times. One time, one time for every\nstep in the gradual descent,",
    "start": "2508710",
    "end": "2515520"
  },
  {
    "text": "as a gradient descent. And as a result, you\nhave to kind of cut off the algorithm before it\nconverges to the best",
    "start": "2515520",
    "end": "2521910"
  },
  {
    "text": "possible result.\nWhereas it turns out, if you just use handcrafted\nfeatures-- so you don't use",
    "start": "2521910",
    "end": "2527430"
  },
  {
    "text": "learning to learn the features. You just use sort of\nstandard SIFT-like features",
    "start": "2527430",
    "end": "2532920"
  },
  {
    "text": "that were already used in\nthe early days of image recognition, and\nthen you just train",
    "start": "2532920",
    "end": "2539010"
  },
  {
    "text": "a simple linear classifier using\nthose features-- it turns out you're already able to get\nbetter results than what",
    "start": "2539010",
    "end": "2545440"
  },
  {
    "text": "the deep neural\nnets will give you. That's kind of surprising,\nthat pre-chosen features using",
    "start": "2545440",
    "end": "2555270"
  },
  {
    "text": "a linear classifier will\nactually perform better than a neural net, a\ndeep neural net that's",
    "start": "2555270",
    "end": "2560589"
  },
  {
    "text": "supposed to learn the\nfeatures on its own. Again, without\nprivacy, the neural net will beat this hands down.",
    "start": "2560590",
    "end": "2566460"
  },
  {
    "text": "The minute you introduce\nprivacy, somehow learning the features exhausts a\nlot of the privacy budgets.",
    "start": "2566460",
    "end": "2574619"
  },
  {
    "text": "So kind of using\npre-trained features helps us learn things\nmore accurately.",
    "start": "2574620",
    "end": "2580460"
  },
  {
    "start": "2580000",
    "end": "2687000"
  },
  {
    "text": "So you can see that,\nagain, this is basically the access that accesses the\ndifferential privacy levels.",
    "start": "2580460",
    "end": "2587609"
  },
  {
    "text": "So remember, the smaller\nepsilon is, the better. So as we move to the\nright on the x-axis,",
    "start": "2587610",
    "end": "2593800"
  },
  {
    "text": "we get better\ndifferential privacy. So stronger\ndifferential privacy. So accuracy goes\ndown as a result.",
    "start": "2593800",
    "end": "2599670"
  },
  {
    "text": "But you can see, the green\nline uses handcrafted features and simple linear models. Whereas the red line\nuses deep neural net.",
    "start": "2599670",
    "end": "2606900"
  },
  {
    "text": "And amazingly, the green line\ndoes better than the red line. So simple handcrafted\nfeatures using linear models",
    "start": "2606900",
    "end": "2614160"
  },
  {
    "text": "can beat the deep neural nets\nwhen privacy is required.",
    "start": "2614160",
    "end": "2619539"
  },
  {
    "text": "So this story actually\ngets even more interesting in that it turns out if\nyou have enough data--",
    "start": "2619540",
    "end": "2626069"
  },
  {
    "text": "if you have enough\ndata, eventually the deep neural\nnets, with privacy, will actually beat the\nhandcrafted features.",
    "start": "2626070",
    "end": "2633869"
  },
  {
    "text": "So you can see\nthis red line here corresponds to deep learning,\nand the green line corresponds to handcrafted features\nusing linear classifiers,",
    "start": "2633870",
    "end": "2642120"
  },
  {
    "text": "simple linear classifiers. And you can see\nthat when you get to a certain point on the right,\nwith a huge number of samples",
    "start": "2642120",
    "end": "2648660"
  },
  {
    "text": "the neural nets\nactually do get better. So what it says is that private\ndeep learning eventually",
    "start": "2648660",
    "end": "2654359"
  },
  {
    "text": "gets really good. But you have to\nhave a lot of data in order to get to that point.",
    "start": "2654360",
    "end": "2660130"
  },
  {
    "text": "So this is kind of\na bizarre conclusion to take away from this, where\nbasically someone like Google",
    "start": "2660130",
    "end": "2666730"
  },
  {
    "text": "would say, yes, we need\nto collect much more data in order to improve\nuser privacy. It's kind of a\ncontradiction, that you",
    "start": "2666730",
    "end": "2673920"
  },
  {
    "text": "need to collect more data\nin order to improve privacy. But it turns out that's\nactually how deep neural nets",
    "start": "2673920",
    "end": "2681750"
  },
  {
    "text": "with privacy would work. However, there's\nanother approach. There's another\napproach, which is",
    "start": "2681750",
    "end": "2687420"
  },
  {
    "start": "2687000",
    "end": "2775000"
  },
  {
    "text": "to say that we can use\npublicly trained features. So basically, we can\ntrain on public data.",
    "start": "2687420",
    "end": "2694960"
  },
  {
    "text": "So no secrecy, no\nprivacy involved. We train features\non the public data.",
    "start": "2694960",
    "end": "2700470"
  },
  {
    "text": "And once we have, we've\ntrained those features on the public data, that's when\nwe move to the private data, and use the features\nthat we trained--",
    "start": "2700470",
    "end": "2707109"
  },
  {
    "text": "this is sometimes called\ntransfer learning, where you learn features\non one data set, and then you use those\nfeatures on another data set.",
    "start": "2707110",
    "end": "2714450"
  },
  {
    "text": "And then we use the features\nthat we just learned to train a linear classifier.",
    "start": "2714450",
    "end": "2720390"
  },
  {
    "text": "And that actually\nstill remains private. And then it turns out\naccuracy goes up quite a bit.",
    "start": "2720390",
    "end": "2726310"
  },
  {
    "text": "So you can see that now\nthere's only a 5% gap between training with privacy\nand training without privacy.",
    "start": "2726310",
    "end": "2732910"
  },
  {
    "text": "So the lesson here\nis when you train, when you learn the features\nthat you want to train on,",
    "start": "2732910",
    "end": "2739230"
  },
  {
    "text": "those are good to\ndo on public data. So you don't exhaust\nyour privacy budget on learning features.",
    "start": "2739230",
    "end": "2745050"
  },
  {
    "text": "And then once you've used-- once you've learned\nthose features, that's when you apply\nthem to your private data.",
    "start": "2745050",
    "end": "2750660"
  },
  {
    "text": "And now you don't need\nto make as many steps in the gradient descent process. And you quickly converge\nto a good model.",
    "start": "2750660",
    "end": "2758710"
  },
  {
    "text": "All right. So that's kind of\nthe lesson here, in fact, that it is possible\nto train good private models.",
    "start": "2758710",
    "end": "2765120"
  },
  {
    "text": "You just have to do it\ndifferently from how you would train a non-private model. All right?",
    "start": "2765120",
    "end": "2770460"
  },
  {
    "text": "So I think it's kind\nof an important lesson. And I hope that comes across.",
    "start": "2770460",
    "end": "2776220"
  },
  {
    "start": "2775000",
    "end": "2865000"
  },
  {
    "text": "So I think I'm\ngoing to stop here. The last thing I'll\nsay is this is really quite a fascinating area.",
    "start": "2776220",
    "end": "2781830"
  },
  {
    "text": "There is a tremendous amount\nof work on this topic. There's lots and lots\nof work happening on adversarial machine learning.",
    "start": "2781830",
    "end": "2787650"
  },
  {
    "text": "If you're interested in learning\nmore, we're part of a center. It's an NSF center on\ntrustworthy machine learning.",
    "start": "2787650",
    "end": "2793710"
  },
  {
    "text": "It's called CTML. I put up the website\nfor the center here. There are a lot of publications\nthat you can look at",
    "start": "2793710",
    "end": "2801180"
  },
  {
    "text": "to learn more about this. There's also a number\nof Systematization of Knowledge papers, SoKs, sort\nof survey papers of the area.",
    "start": "2801180",
    "end": "2808840"
  },
  {
    "text": "So you can search\naround for some survey papers of machine learning, of\nadversarial machine learning.",
    "start": "2808840",
    "end": "2814319"
  },
  {
    "text": "I kind of put a reference\nto one in EuroS&P-2018. But there are many others.",
    "start": "2814320",
    "end": "2819670"
  },
  {
    "text": "And then there are also\nwonderful blog posts on adversarial machine learning. So if you just search for\nadversarial machine learning,",
    "start": "2819670",
    "end": "2825120"
  },
  {
    "text": "you'll find a really good blog\nposts that sort of explain the problem and some attempts\nat what can be done about this.",
    "start": "2825120",
    "end": "2831370"
  },
  {
    "text": "So this is really-- it's\nreally quite an exciting area. It's really amazing that\nall this is happening.",
    "start": "2831370",
    "end": "2839400"
  },
  {
    "text": "Both very clever attacks, very\nclever defenses, and sort of",
    "start": "2839400",
    "end": "2844500"
  },
  {
    "text": "trying to kind of make machine\nlearning as robust as possible. It's really quite an\nactive and exciting area.",
    "start": "2844500",
    "end": "2851670"
  },
  {
    "text": "So I encourage you to get into\nit and learn a little bit more about it by yourself. All I wanted to do\nhere was give you",
    "start": "2851670",
    "end": "2856980"
  },
  {
    "text": "sort of a high level overview\nand kind of a taste of what's happening in the space. All right? So I'll stop here.",
    "start": "2856980",
    "end": "2862450"
  },
  {
    "text": "I think I'll hand\nit over back to Pax. PAX HEHMEYER: PAX HEHMEYER: Dan, we have a\nton of questions that came in.",
    "start": "2862450",
    "end": "2868329"
  },
  {
    "text": "So across the kind of range. And just looking at\nthem, maybe we'll",
    "start": "2868330",
    "end": "2873690"
  },
  {
    "text": "jump through a couple of\nthese, a couple of the topics. So maybe the first\none is just, can you talk a little bit about--",
    "start": "2873690",
    "end": "2879329"
  },
  {
    "text": "we learned about the\nattacks on machine learning. How is machine learning\nbeing used for things like malware analysis?",
    "start": "2879330",
    "end": "2886019"
  },
  {
    "text": "DAN BONEH: Oh yeah, for sure. There have been a\ncouple of attempts at using machine learning\nto identify malware.",
    "start": "2886020",
    "end": "2892980"
  },
  {
    "text": "It turns out that that\ndoesn't work too well. Yeah? There are-- again,\nthis is an example",
    "start": "2892980",
    "end": "2899010"
  },
  {
    "text": "where adversaries really do try\nto confuse the machine learning algorithm. So it turns out,\nif you look at--",
    "start": "2899010",
    "end": "2906053"
  },
  {
    "text": "so you can train\nyou, you can take a bunch of non-malware pieces\nof software and malware pieces of software.",
    "start": "2906053",
    "end": "2912090"
  },
  {
    "text": "You can label the\nmalware and non-malware and then have a machine\nlearning model algorithm try to learn to distinguish\none from the other.",
    "start": "2912090",
    "end": "2919329"
  },
  {
    "text": "So it does do-- it\ndoes produce something. But it turns out it's actually\nquite easy to confuse it.",
    "start": "2919330",
    "end": "2924630"
  },
  {
    "text": "It turns out that the kind of\nfeatures that it's homing in on are not exactly\nthe right features",
    "start": "2924630",
    "end": "2930720"
  },
  {
    "text": "that you'd like to use. And it turns out\nit's much easier-- actually, maybe I can say, one\nfeature it turns out that--",
    "start": "2930720",
    "end": "2937570"
  },
  {
    "text": "turns out the malware\nPDFs tend to be longer than non-malware PDFs. So it uses the length\nof the documents",
    "start": "2937570",
    "end": "2944550"
  },
  {
    "text": "to identify whether\nit's malware or not. So you can see\nhow to confuse it, you just try to build malware\nthat happens to be short.",
    "start": "2944550",
    "end": "2950550"
  },
  {
    "text": "And then it wouldn't\ndetect that as malware. And so those are-- that's a really good question. But it's an example\nwhere, again, adversaries",
    "start": "2950550",
    "end": "2957540"
  },
  {
    "text": "will try to build\nadversarial examples that are malware but would be\nclassified as non-malware.",
    "start": "2957540",
    "end": "2963360"
  },
  {
    "text": "And as far as we can\ntell, that's usually not that difficult to do. So I would not recommend using\nmachine learning algorithm",
    "start": "2963360",
    "end": "2969690"
  },
  {
    "text": "to identify malware. Or at least not purely\nrely on it for that.",
    "start": "2969690",
    "end": "2974740"
  },
  {
    "text": "PAX HEHMEYER: So\nmaybe related to that, a lot of the examples\nyou gave were around image classification.",
    "start": "2974740",
    "end": "2980050"
  },
  {
    "text": "Have we seen similar kinds\nof adversarial attacks in natural language processing? And what do those look like?",
    "start": "2980050",
    "end": "2986973"
  },
  {
    "text": "DAN BONEH: Yeah. That's a really good question. So there have been some\nworks on that as well.",
    "start": "2986973",
    "end": "2992040"
  },
  {
    "text": "So adversarial\nexamples against NLP. Effectively, what happens is\nit's exactly the same problem.",
    "start": "2992040",
    "end": "2998020"
  },
  {
    "text": "Yeah, you can kind of change-- you can train a model to\nanswer questions, like",
    "start": "2998020",
    "end": "3003600"
  },
  {
    "text": "read an article, and then\nanswer some questions about that article. And then it turns out you\ncan add a few sentences",
    "start": "3003600",
    "end": "3011430"
  },
  {
    "text": "to the article or add a\nfew words to the article, and then when you ask\nit the same questions,",
    "start": "3011430",
    "end": "3017100"
  },
  {
    "text": "it just gives you\ncompletely wrong results I think a nice example\nof that, scientists",
    "start": "3017100",
    "end": "3022710"
  },
  {
    "text": "gave a nice example of that,\nwhere you can train a model to answer a question-- You give the model\nas an example,",
    "start": "3022710",
    "end": "3030540"
  },
  {
    "text": "you give the model biographical\npages of various people. And then those\nbiographical pages",
    "start": "3030540",
    "end": "3037260"
  },
  {
    "text": "typically would\ninclude information like when that person was born.",
    "start": "3037260",
    "end": "3042450"
  },
  {
    "text": "So you train the model\nto answer a question like when was so-and-so born. Yeah? And so what the model\nlearns is, oh, I'm",
    "start": "3042450",
    "end": "3048540"
  },
  {
    "text": "going to look for dates in the\ndocuments that you gave me. And if I can find a date, then\nI answer that as the result.",
    "start": "3048540",
    "end": "3055183"
  },
  {
    "text": "But then it turns out\nyou can confuse it by basically adding a few\nmore dates to the documents. And now the model doesn't\nknow which date it's",
    "start": "3055183",
    "end": "3061650"
  },
  {
    "text": "supposed to actually produce. So again, it's\nlearning how to answer when was this person\nborn by learning",
    "start": "3061650",
    "end": "3069120"
  },
  {
    "text": "that it needs to look for a\ndate in the given document. And that's pretty easy\nto confuse it then",
    "start": "3069120",
    "end": "3075030"
  },
  {
    "text": "by adding some more words\nto the given document. So it's kind of\ninteresting to come up--",
    "start": "3075030",
    "end": "3083340"
  },
  {
    "text": "maybe the way you would come\nup with adversarial examples against NLP, they're\ndifferent from what",
    "start": "3083340",
    "end": "3088619"
  },
  {
    "text": "you would do for images. But it's doable nonetheless. It's exactly the same idea.",
    "start": "3088620",
    "end": "3094059"
  },
  {
    "text": "So that's a great question. But this translates\nto other domains. PAX HEHMEYER: And related\nto that, how hard is it--",
    "start": "3094060",
    "end": "3101190"
  },
  {
    "start": "3098000",
    "end": "3179000"
  },
  {
    "text": "why can't we identify\nwhat the poisoned set-- the poisoned example is. So if you have 1,000\npictures of dogs,",
    "start": "3101190",
    "end": "3108300"
  },
  {
    "text": "and then one flips the model\nto identify all dogs as fish, what makes it difficult to\ngo back through and identify",
    "start": "3108300",
    "end": "3114880"
  },
  {
    "text": "that outlier? DAN BONEH: Maybe\nI'll answer that in the context of\nadversarial examples. So during classification time.",
    "start": "3114880",
    "end": "3120270"
  },
  {
    "text": "So suppose you want to\nrecognize road signs. So you build a model to\nrecognize stop signs.",
    "start": "3120270",
    "end": "3125810"
  },
  {
    "text": "And now the adversary\nis going to give you some sort of perturbed\nimage of a stop sign.",
    "start": "3125810",
    "end": "3131315"
  },
  {
    "text": "And the question\nis why can't you recognize that this\nis a perturbed image? That's a great question. And then the model just says,\noh, this is an attacked image.",
    "start": "3131315",
    "end": "3138660"
  },
  {
    "text": "I'm not going to answer it. It turns out that's\nthe actual model that",
    "start": "3138660",
    "end": "3143940"
  },
  {
    "text": "recognizes that this is a\nperturbed image can itself be attacked. So that I can prevent the model\nfrom recognizing that something",
    "start": "3143940",
    "end": "3152850"
  },
  {
    "text": "is actually an attack image. And so just like I can\nbuild adversarial examples",
    "start": "3152850",
    "end": "3158400"
  },
  {
    "text": "against the core\nrecognition model, I can build adversarial examples\nagainst the attack recognition",
    "start": "3158400",
    "end": "3165150"
  },
  {
    "text": "model, and make it think that\nsomething that's an attack is actually not an attack. So it's a great question.",
    "start": "3165150",
    "end": "3170740"
  },
  {
    "text": "It's a very good\nattempt at defending. But in fact, that\ncan also be bypassed.",
    "start": "3170740",
    "end": "3175769"
  },
  {
    "text": "It's not too hard. It's a very good question. Thanks. PAX HEHMEYER: And in terms of\ndefense, are there any agreed",
    "start": "3175770",
    "end": "3182339"
  },
  {
    "start": "3179000",
    "end": "3238000"
  },
  {
    "text": "upon measurements or methods\nto measure resistance to adversarial attacks? Do we have any\nstandards out there yet?",
    "start": "3182340",
    "end": "3189750"
  },
  {
    "text": "DAN BONEH: Oh, to defend,\nto measure resistance. That's an excellent question.",
    "start": "3189750",
    "end": "3194790"
  },
  {
    "text": "Yeah, that actually is a\ngood research question, I have to say. I don't know of any way\nto actually measure that.",
    "start": "3194790",
    "end": "3201720"
  },
  {
    "text": "Mostly because attacks\nare sort of binary. I mean, either there are\nattacks or there aren't attacks.",
    "start": "3201720",
    "end": "3208560"
  },
  {
    "text": "Maybe what you're kind of asking\nis how hard is it to come up with adversarial examples.",
    "start": "3208560",
    "end": "3213750"
  },
  {
    "text": "Or maybe what kind\nof perturbations you're supposed to use to come\nup with adversarial examples.",
    "start": "3213750",
    "end": "3219420"
  },
  {
    "text": "Those are things that are hard\nto kind of capture in numbers. So I would say, again,\nthat's a fantastic question.",
    "start": "3219420",
    "end": "3225400"
  },
  {
    "text": "Thank you for asking that. But I would say that's kind\nof a good research question. How do we classify-- how\ndo we quantify robustness?",
    "start": "3225400",
    "end": "3233285"
  },
  {
    "text": "That's something I would\nhave to think about. It's a good research question. Thanks. PAX HEHMEYER: So maybe\nmoving a little bit,",
    "start": "3233285",
    "end": "3239730"
  },
  {
    "start": "3238000",
    "end": "3495000"
  },
  {
    "text": "we can come back if we have\ntime for some of the machine learning, but we did get\na fair number of questions kind of more broadly\nabout cybersecurity",
    "start": "3239730",
    "end": "3245580"
  },
  {
    "text": "you would ask people to submit. So one that comes\nup a lot, and I think is worth addressing again\nis the question of quantum",
    "start": "3245580",
    "end": "3252330"
  },
  {
    "text": "computing. So how will quantum computing\naffect computer security going forward?",
    "start": "3252330",
    "end": "3258000"
  },
  {
    "text": "And in particular,\nwill it impact what we're seeing right now\nwith machine learning models and, say, safe\nsecurity algorithms?",
    "start": "3258000",
    "end": "3264635"
  },
  {
    "text": "DAN BONEH: Wow. OK, yeah. So it seems like in\nevery one of these talks, people are interested in\nhearing about quantum computing.",
    "start": "3264635",
    "end": "3270300"
  },
  {
    "text": "That's great. I mean, I'm happy\nto discuss that. So quantum computing\nis something",
    "start": "3270300",
    "end": "3276750"
  },
  {
    "text": "that's probably going to\nrevolutionize computer science. And I would say, maybe\nour grandkids, maybe",
    "start": "3276750",
    "end": "3282510"
  },
  {
    "text": "our great grandkids\nwill be learning to program in a quantum\nC rather than C.",
    "start": "3282510",
    "end": "3288329"
  },
  {
    "text": "So computers are\njust more powerful when they can use\nquantum physics, instead of the classical computers\nthat we have today.",
    "start": "3288330",
    "end": "3295890"
  },
  {
    "text": "As you know, one\nof the applications for quantum\ncomputers is to break existing public\nkey crypto systems.",
    "start": "3295890",
    "end": "3301470"
  },
  {
    "text": "Like signature algorithms, key\nexchange, things like that. There are other applications\nfor quantum computers,",
    "start": "3301470",
    "end": "3308070"
  },
  {
    "text": "like computational chemistry. Maybe we can do chemistry\ninstead of doing it in the lab.",
    "start": "3308070",
    "end": "3313230"
  },
  {
    "text": "Maybe we can do it better\non a quantum computer. And so those have\nto be the ones that",
    "start": "3313230",
    "end": "3319680"
  },
  {
    "text": "drive the development of\nquantum computers, right? So those applications to\nchemistry, for example,",
    "start": "3319680",
    "end": "3324900"
  },
  {
    "text": "would drive the development\nof those computers. Breaking cryptography\nis not a reason",
    "start": "3324900",
    "end": "3330030"
  },
  {
    "text": "to develop a quantum computer. And that's really\nimportant to understand, because we already have\ncryptographic systems that",
    "start": "3330030",
    "end": "3336600"
  },
  {
    "text": "are secure against\nquantum computation. Yeah, so just to rattle\noff a couple of names, there are things like\nlattice-based systems",
    "start": "3336600",
    "end": "3344190"
  },
  {
    "text": "that can replace both key\nexchange and signature schemes. There are isogeny-based systems.",
    "start": "3344190",
    "end": "3349412"
  },
  {
    "text": "There are code-based systems. There are hash-based systems. There are many new\ncryptographic schemes",
    "start": "3349412",
    "end": "3355290"
  },
  {
    "text": "that are secure, even if\nthe adversary has a quantum computer. So what that means\nis if you're worried",
    "start": "3355290",
    "end": "3362025"
  },
  {
    "text": "about Bitcoin, for example. Bitcoin is based on signatures. And the signature algorithm\nthat Bitcoin uses today, it's",
    "start": "3362025",
    "end": "3368579"
  },
  {
    "text": "called ECDSA, it's vulnerable\nto a quantum computing attack. So what will happen is if\nwe see that in 30 years,",
    "start": "3368580",
    "end": "3376380"
  },
  {
    "text": "someone is getting close to\nbuilding a quantum computer, all that will happen is\nthat Bitcoin will simply shift to using a quantum\nresistance signature scheme.",
    "start": "3376380",
    "end": "3384570"
  },
  {
    "text": "What's called\npost-quantum signatures. And there are many of those. As you probably know, NIST,\nthe National Institute",
    "start": "3384570",
    "end": "3390150"
  },
  {
    "text": "of Standards, is even\ngoing through a process of standardizing\npost-quantum cryptography, both for key change and\nfor digital signatures.",
    "start": "3390150",
    "end": "3397620"
  },
  {
    "text": "And so we already\nknow what to use. So what will happen is as\nquantum computers become closer",
    "start": "3397620",
    "end": "3403890"
  },
  {
    "text": "to reality, we'll simply shift\nto using digital signatures and key exchange that are secure\nagainst quantum computing,",
    "start": "3403890",
    "end": "3412079"
  },
  {
    "text": "and basically the\ncrypto will continue to work as it does today. So breaking crypto cannot be\nthe reason to develop a quantum",
    "start": "3412080",
    "end": "3420100"
  },
  {
    "text": "computer. There has to be other\ncommercial motivations for it. And I really hope that\nchemistry is enough of a reason,",
    "start": "3420100",
    "end": "3427890"
  },
  {
    "text": "because the investments to\ndevelop a quantum computer is going to be enormous. It's going to take 30 years\nof sustained investment",
    "start": "3427890",
    "end": "3434819"
  },
  {
    "text": "to develop one. And we better see a\nreturn on investment in less than 30 years.",
    "start": "3434820",
    "end": "3440550"
  },
  {
    "text": "Otherwise no company will do it. Only governments can afford\nto such a long term sustained",
    "start": "3440550",
    "end": "3446640"
  },
  {
    "text": "investment. So I hope it will happen. But it can't be\njust cryptography.",
    "start": "3446640",
    "end": "3453960"
  },
  {
    "text": "And the timeline, by\nthe way, is around, my guess is around\n30 to 50 years. So hopefully that's when\nwe'll shift to having quantum",
    "start": "3453960",
    "end": "3463680"
  },
  {
    "text": "computers all around this. PAX HEHMEYER: Perfect. Well, Dan, thank you so much. We're at a time.",
    "start": "3463680",
    "end": "3469109"
  },
  {
    "text": "We have a lot of\nquestions that came in. We'll share those with\nDan so he can take a look. And maybe there's the seed\nof the next webinar in there.",
    "start": "3469110",
    "end": "3477220"
  },
  {
    "text": "So thank you again\neveryone who joined us. And thank you, Dan, for\nthis wonderful talk. DAN BONEH: Thank you, all.",
    "start": "3477220",
    "end": "3482550"
  },
  {
    "text": "This was fun. Hope to see you again\nin a future talk. ",
    "start": "3482550",
    "end": "3495000"
  }
]