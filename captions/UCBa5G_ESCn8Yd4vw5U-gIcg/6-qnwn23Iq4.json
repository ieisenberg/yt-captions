[
  {
    "start": "0",
    "end": "5420"
  },
  {
    "text": "All right. We'll get started today. So welcome back, everyone. And then yeah.",
    "start": "5420",
    "end": "11030"
  },
  {
    "text": "Let's jump into lecture 2. So this is a lecture on system\nmodeling, which corresponds",
    "start": "11030",
    "end": "16850"
  },
  {
    "text": "to chapter 2 of the book. So I'll just note,\nsome of today might",
    "start": "16850",
    "end": "22220"
  },
  {
    "text": "be a review for some of you. Some of it might be new. But I just really want\nto note that we're kind of cramming a lot of topics\ninto one lecture here today.",
    "start": "22220",
    "end": "30210"
  },
  {
    "text": "So this lecture and the\nlecture next Tuesday are going to be a\nlittle different,",
    "start": "30210",
    "end": "35982"
  },
  {
    "text": "I guess, than the rest of\nthe lectures because we're going to talk about\nthose input components to the validation problem.",
    "start": "35983",
    "end": "41610"
  },
  {
    "text": "So if you remember\nfrom last lecture, we had the inputs as a system,\nthe system that we want to test,",
    "start": "41610",
    "end": "47239"
  },
  {
    "text": "and then the property that we\nwant that system to satisfy. So we're going to talk\nabout, in this lecture,",
    "start": "47240",
    "end": "53120"
  },
  {
    "text": "how to set up the system,\nand in the next lecture, how to set up that property. And the rest of the course will\nbe about validation algorithms.",
    "start": "53120",
    "end": "60022"
  },
  {
    "text": "So I just kind of want\nto give a warning. Today might be a lot. We might not actually\nget through all of it. We'll finish up what we don't\nget through next Tuesday.",
    "start": "60022",
    "end": "67900"
  },
  {
    "text": "But if I am going too fast\nor I brush over something, definitely stop me and ask\nany questions that you have.",
    "start": "67900",
    "end": "74033"
  },
  {
    "text": "The other good news\nis that this lecture is kind of self-contained. So everything we\nlearn here, we're going to talk about how we can\ngo about modeling the system",
    "start": "74033",
    "end": "82023"
  },
  {
    "text": "because we want you to\nunderstand various ways to do that when you need to set\nit up in the real world.",
    "start": "82023",
    "end": "87370"
  },
  {
    "text": "But for the rest of\nthe course after this, we'll actually just\ngive you the system that we want you to validate. So everything-- this lecture\nis kind of self-contained.",
    "start": "87370",
    "end": "95590"
  },
  {
    "text": "But let's jump into\nsystem modeling. And I want to start again\nwith just remembering the context we have here.",
    "start": "95590",
    "end": "101540"
  },
  {
    "text": "So in this class,\nwe're essentially finding out, as we said. And we said in the last lecture\nthat if we're here to find out,",
    "start": "101540",
    "end": "108470"
  },
  {
    "text": "we don't really want to\nfind out when it's too late. We don't want to find out\nwhen we've already deployed the system in the real world.",
    "start": "108470",
    "end": "114140"
  },
  {
    "text": "What we'd rather do is find\nout in this controlled, safe environment,\nunlike these people did.",
    "start": "114140",
    "end": "120520"
  },
  {
    "text": "OK. So like I said, we don't want\nto find out in the real world. What we do want to do is\nfind out in simulation.",
    "start": "120520",
    "end": "127219"
  },
  {
    "text": "Because if things go wrong in\nsimulation, it's not so bad. Versus if things go\nwrong in the real world,",
    "start": "127220",
    "end": "132319"
  },
  {
    "text": "we can get ourselves\nin a lot of trouble. So today, we're going to\ndiscuss ways to model systems,",
    "start": "132320",
    "end": "139060"
  },
  {
    "text": "so that we can simulate\nthem for offline validation. So by offline validation,\nI mean essentially",
    "start": "139060",
    "end": "144850"
  },
  {
    "text": "not in the environment\nwe're intending to deploy the\nsystem in some kind of simulated version of that.",
    "start": "144850",
    "end": "151870"
  },
  {
    "text": "So plan for today. We'll start out with just\nsome general thoughts I have on models that\nI want you all to know.",
    "start": "151870",
    "end": "157998"
  },
  {
    "text": "And then we're just going to go\nthrough essentially the steps required for building models. So the first step is to\nselect a model class.",
    "start": "157998",
    "end": "165680"
  },
  {
    "text": "We'll talk about\nwhat that means. Once we have selected\na model class, we typically have some set of\nparameters for that model class,",
    "start": "165680",
    "end": "172383"
  },
  {
    "text": "and we need to figure\nout what they are. So that's the next step. And the final step is\nto validate the model.",
    "start": "172383",
    "end": "178493"
  },
  {
    "text": "I'm not sure if we'll\nget to that today, but if we don't, we'll\nget to it on Tuesday. ",
    "start": "178493",
    "end": "184440"
  },
  {
    "text": "So let's start with some\ngeneral thoughts on models. So there's many different\ntypes of models and simulators.",
    "start": "184440",
    "end": "190150"
  },
  {
    "text": "I'm sure you've heard\nof many of them. And what's kind of interesting\nis they don't even necessarily have to be on the computer.",
    "start": "190150",
    "end": "195645"
  },
  {
    "text": "So it's essentially just\nsome kind of safe environment where we can find out\nwithout having to deploy it",
    "start": "195645",
    "end": "201750"
  },
  {
    "text": "in its real environment. So an example of a model\nthat's not on the computer would be like the\nNeutral Buoyancy Lab",
    "start": "201750",
    "end": "208410"
  },
  {
    "text": "at the Johnson Space Center. Maybe some of you\nhave seen this before. Essentially, it's this giant\nswimming pool that astronauts",
    "start": "208410",
    "end": "214470"
  },
  {
    "text": "can do their training\nin, which simulates what it would be\nlike when they're doing a spacewalk on\nthe Space Station.",
    "start": "214470",
    "end": "221170"
  },
  {
    "text": "So this isn't actually\na computational model, but this is a way\nfor them to find out",
    "start": "221170",
    "end": "226560"
  },
  {
    "text": "what might go wrong in a safe\nenvironment, such as a swimming pool on Earth, not\nwhen they're out",
    "start": "226560",
    "end": "232079"
  },
  {
    "text": "in the middle of outer space. So giant swimming\npools are super cool. But in this class,\nwe're going to focus",
    "start": "232080",
    "end": "238185"
  },
  {
    "text": "on computational models. So mathematical models that we\ncan simulate on the computer.",
    "start": "238185",
    "end": "243665"
  },
  {
    "text": " So models vary in complexity.",
    "start": "243665",
    "end": "249040"
  },
  {
    "text": "So for example, let's\nimagine that we're modeling the dynamics of an aircraft. So how an aircraft's\nstate evolves over time.",
    "start": "249040",
    "end": "256588"
  },
  {
    "text": "And for example, one\noption here would be to just have a couple of\nequations that model its state.",
    "start": "256589",
    "end": "262180"
  },
  {
    "text": "So this is just saying that our\naltitude at the next time step is our current altitude\nplus our vertical rate",
    "start": "262180",
    "end": "269160"
  },
  {
    "text": "times some delta t. And similarly for\nthe vertical rate. So this is just two\nsimple linear equations.",
    "start": "269160",
    "end": "276690"
  },
  {
    "text": "Or what we could do\nif we want to model the dynamics of an aircraft\nis use this very high fidelity flight simulator.",
    "start": "276690",
    "end": "282970"
  },
  {
    "text": "So one flight simulator\nwe use a lot in our lab is called X-Plane. And it has all these very\ncomplicated dynamics.",
    "start": "282970",
    "end": "288789"
  },
  {
    "text": "So it's paying attention to\nthe angles of the control surfaces on the aircraft. It's paying attention\nto the weight of the aircraft, the\ntype of aircraft,",
    "start": "288790",
    "end": "296240"
  },
  {
    "text": "the wind conditions, et cetera. So this is a much more\ncomplicated model. And there's two ends\nof the spectrum.",
    "start": "296240",
    "end": "302610"
  },
  {
    "text": "So we could have just\na very simple model, like these equations here,\nor we could go all the way up to this very high\nfidelity flight simulator.",
    "start": "302610",
    "end": "310970"
  },
  {
    "text": "And essentially,\nthe goal here is that we want to pick the model\nthat is just complex enough",
    "start": "310970",
    "end": "316010"
  },
  {
    "text": "to capture what matters. So if we don't have\nto, maybe some of you who've worked with X-Plane\nbefore, could agree with me.",
    "start": "316010",
    "end": "322570"
  },
  {
    "text": "You might want to avoid using\nthis very high fidelity flight simulator in cases where\nit's not necessary,",
    "start": "322570",
    "end": "327750"
  },
  {
    "text": "and we could just get away\nwith these simple equations. And knowing when\nwe need something",
    "start": "327750",
    "end": "333500"
  },
  {
    "text": "more complex versus less complex\nis typically problem-dependent. But it's important\nwhen you're thinking",
    "start": "333500",
    "end": "338600"
  },
  {
    "text": "about what model to use to\ntry to understand the problem domain that you're\nin and pick something that's just complex enough.",
    "start": "338600",
    "end": "345380"
  },
  {
    "text": "So a good way to summarize this\nquote from Albert Einstein that says everything should be\nsimple, as simple as possible,",
    "start": "345380",
    "end": "353520"
  },
  {
    "text": "but not simpler. Maybe. We're not really sure if\nAlbert Einstein said this.",
    "start": "353520",
    "end": "358950"
  },
  {
    "text": "We wanted to put it in the book. And Michael's requirement\nwas if we put it in the book, we need to have a\ncitation for it.",
    "start": "358950",
    "end": "365910"
  },
  {
    "text": "And so I started searching\nall over the place, and everyone's\ngot like all this, all the sources online are\nlike, he probably said it,",
    "start": "365910",
    "end": "371910"
  },
  {
    "text": "but they're not really sure. He said something similar to it. And so then everyone's like, oh,\nthat's probably what he meant.",
    "start": "371910",
    "end": "378440"
  },
  {
    "text": "So either way, it's a\ngood motto to live by when you're selecting your models.",
    "start": "378440",
    "end": "386210"
  },
  {
    "text": "And then we didn't put it\nin the book for that reason. Another kind thing that\ncan vary with models",
    "start": "386210",
    "end": "392330"
  },
  {
    "text": "is whether they're\nwhite box or black box. So a white box model means\nthat we give it some inputs.",
    "start": "392330",
    "end": "398370"
  },
  {
    "text": "So in this case, we\ngive it, for example, the current altitude\nof the aircraft and its current vertical rate. And then we can see what\nequations are involved",
    "start": "398370",
    "end": "405800"
  },
  {
    "text": "in determining the outputs. So the next altitude and\nthe next vertical rate.",
    "start": "405800",
    "end": "410960"
  },
  {
    "text": "A black box model, on the\nother hand, essentially what we're doing here is\nwe give it some inputs.",
    "start": "410960",
    "end": "416270"
  },
  {
    "text": "And then we don't\nreally get to see what happens when it's doing\nthings to these inputs. We just get to see\nwhat the outputs are.",
    "start": "416270",
    "end": "424422"
  },
  {
    "text": "And then of course,\nwe can have something in the middle, where maybe\nwe know what's happening to some of the inputs. So in X-Plane for example,\nwe know some of the dynamics",
    "start": "424422",
    "end": "432515"
  },
  {
    "text": "equations that it's\nusing, but maybe we don't know exactly\nwhat's going on inside. ",
    "start": "432515",
    "end": "440110"
  },
  {
    "text": "So with that, there's a few\nkey challenges that we have when we create models.",
    "start": "440110",
    "end": "446590"
  },
  {
    "text": "The first one is that, kind\nof like we were saying, the model needs to\nbe expressive enough to capture all possible\nscenarios the system may",
    "start": "446590",
    "end": "454689"
  },
  {
    "text": "encounter, but you don't\nwant it to be too expressive. So I want to actually dwell\non this point, this challenge,",
    "start": "454690",
    "end": "463490"
  },
  {
    "text": "for just a little bit longer\nbecause I think it's really key, and it's really important.",
    "start": "463490",
    "end": "468880"
  },
  {
    "text": "Because when you think\nabout really what this means to capture\nall possible scenarios, that's like a pretty tall task.",
    "start": "468880",
    "end": "475310"
  },
  {
    "text": "So imagine, for\nexample, that our system is a self-driving car. So now, all possible scenarios\nis like all the pedestrians",
    "start": "475310",
    "end": "482389"
  },
  {
    "text": "it might see, all the other cars\non the road, what they might do. Bikers, construction.",
    "start": "482390",
    "end": "487730"
  },
  {
    "text": "It's so many things. And so this is definitely\nlike no easy feat here,",
    "start": "487730",
    "end": "493460"
  },
  {
    "text": "and it's worth putting\na lot of effort into making sure that you\ncapture all of these scenarios.",
    "start": "493460",
    "end": "499360"
  },
  {
    "text": "Another thing I'll say is\nI've given a number of talks on validation at this point.",
    "start": "499360",
    "end": "504530"
  },
  {
    "text": "And at almost every\nsingle talk, I get one question\nthat is this, which",
    "start": "504530",
    "end": "510160"
  },
  {
    "text": "is how can I capture all\npossible scenarios my, insert crazy complex system\nhere, will end up in?",
    "start": "510160",
    "end": "516289"
  },
  {
    "text": "And they'll say, like I don't\neven know what they are. So like we were just saying\nfor the self-driving car, like how am I supposed to figure\nout all possible scenarios",
    "start": "516289",
    "end": "524770"
  },
  {
    "text": "that this car is\ngoing to end up in? And my answer to them is\nalways, yeah, it's a problem.",
    "start": "524770",
    "end": "531490"
  },
  {
    "text": "This is extremely difficult\nto do, as I was saying. But I think there\nis some hope here,",
    "start": "531490",
    "end": "537190"
  },
  {
    "text": "a kind of a silver lining,\nwhich is stay tuned for the runtime\nmonitoring chapter",
    "start": "537190",
    "end": "543220"
  },
  {
    "text": "because I think what we could\ndo is we can make the best effort to capture all possible\nscenarios or things that might",
    "start": "543220",
    "end": "549480"
  },
  {
    "text": "occur in our model, but\nwe also could realize that we are probably going\nto miss stuff, especially",
    "start": "549480",
    "end": "555090"
  },
  {
    "text": "for very complicated\nsystems that operate in very complicated environments. So what the next best\nthing we could do",
    "start": "555090",
    "end": "562620"
  },
  {
    "text": "is deploy these systems\nwith runtime monitors, so that when we're\noperating at runtime,",
    "start": "562620",
    "end": "569139"
  },
  {
    "text": "we can try to understand\nif we're in a scenario that we didn't\nmodel or something that we are unfamiliar with.",
    "start": "569140",
    "end": "575530"
  },
  {
    "text": "So that would be my answer,\nbut it is really a challenge. Second challenge is we're going\nto talk a little bit about",
    "start": "575530",
    "end": "582450"
  },
  {
    "text": "this today. We have to base the\nmodel on something. We need to know something\nabout the real world,",
    "start": "582450",
    "end": "587600"
  },
  {
    "text": "and we could either get that\nfrom data or expert knowledge. But both of those can be\nkind of difficult to obtain.",
    "start": "587600",
    "end": "594190"
  },
  {
    "text": "For data, you might need to\nthen go into the real world and collect this data or\nyou need to find an expert",
    "start": "594190",
    "end": "600160"
  },
  {
    "text": "or be an expert yourself. And then finally, once we\nhave this data or knowledge,",
    "start": "600160",
    "end": "606830"
  },
  {
    "text": "we need to actually\ncreate the model. And one way we often do\nthis is by optimizing for some objective, which\nwe'll talk about today.",
    "start": "606830",
    "end": "616820"
  },
  {
    "text": "OK. Another quote. All models are wrong,\nbut some are useful. Maybe you guys have\nseen this before.",
    "start": "616820",
    "end": "623649"
  },
  {
    "text": "The idea here is just that all\nof the models that we make, they're never going to\nperfectly model the real world,",
    "start": "623650",
    "end": "629120"
  },
  {
    "text": "but if they model it well\nenough, they'll be useful to us, and our validation results\nthat we use the models to get",
    "start": "629120",
    "end": "634630"
  },
  {
    "text": "will be useful to us\nto make decisions. So that was said by British\nstatistician George Box,",
    "start": "634630",
    "end": "641230"
  },
  {
    "text": "for real. We actually found a citation\nfor the book for that one. So we put it in there.",
    "start": "641230",
    "end": "648170"
  },
  {
    "text": "OK. Let's do some math then. So the first thing we're\ngoing to talk about,",
    "start": "648170",
    "end": "653880"
  },
  {
    "text": "the first step in\ncreating a model is to select a model class. So in this class, we're going to\nfocus on probabilistic models,",
    "start": "653880",
    "end": "662060"
  },
  {
    "text": "and we'll talk\nexactly what it means to have a model class that's a\nprobabilistic model in a second.",
    "start": "662060",
    "end": "667340"
  },
  {
    "text": "But first, I want to think about\nwhat exactly probability is. So there's lots\nof different ideas out there of how to\ndescribe probability.",
    "start": "667340",
    "end": "675320"
  },
  {
    "text": "Here's one of the\nmost interesting ones that I found recently that\nbasically says why probability",
    "start": "675320",
    "end": "680450"
  },
  {
    "text": "probably doesn't exist, but\nit's useful to act like it does. And just in case you\ncan't read this kind",
    "start": "680450",
    "end": "686240"
  },
  {
    "text": "of smaller text below it,\nit's quite interesting. It says all of statistics\nand much of science depends on probability,\nan astonishing achievement",
    "start": "686240",
    "end": "694069"
  },
  {
    "text": "considering no one's\nreally sure what it is. So I found this\nkind of interesting. If anyone's interested, you\ncan check out this article.",
    "start": "694070",
    "end": "701420"
  },
  {
    "text": "It's quite philosophical\nabout the world and randomness and outcomes.",
    "start": "701420",
    "end": "706640"
  },
  {
    "text": "We're not going to be so\nphilosophical in this class. We're going to give it\nan actual definition. But we're just going\nto say the probability",
    "start": "706640",
    "end": "713240"
  },
  {
    "text": "of a particular outcome is\na number between 0 and 1 that quantifies the likelihood\nof that outcome occurring",
    "start": "713240",
    "end": "719270"
  },
  {
    "text": "relative to all other\npossible outcomes. ",
    "start": "719270",
    "end": "725180"
  },
  {
    "text": "And so the way that we\noften describe probability is using probability\ndistributions.",
    "start": "725180",
    "end": "731640"
  },
  {
    "text": "And so this is typically what\nwe mean by a model class, is we'll say that different\ntypes of probability",
    "start": "731640",
    "end": "738020"
  },
  {
    "text": "distributions are essentially\ndifferent model classes. And a probability\ndistribution is",
    "start": "738020",
    "end": "743060"
  },
  {
    "text": "a function that\nassigns probabilities to different outcomes. ",
    "start": "743060",
    "end": "748850"
  },
  {
    "text": "So in this class, we'll talk\nabout two different types of probability distributions. We have discrete\ndistributions, which",
    "start": "748850",
    "end": "754910"
  },
  {
    "text": "operate over variables\nor events that have a discrete number of outcomes. So for example,\nhere's an example",
    "start": "754910",
    "end": "761480"
  },
  {
    "text": "of a discrete distribution\nwith six possible outcomes. So you could think of this\nas like rolling a dice.",
    "start": "761480",
    "end": "767649"
  },
  {
    "text": "Then we have continuous\ndistributions, which are defined\nover variables that have a continuous set\nof possible outcomes.",
    "start": "767650",
    "end": "774470"
  },
  {
    "text": "So you could think of this as\nmaybe a variable that represents the height of a student if we\njust randomly selected them",
    "start": "774470",
    "end": "780850"
  },
  {
    "text": "from this class. We typically define\nthese distributions",
    "start": "780850",
    "end": "786130"
  },
  {
    "text": "using something called\na probability mass function for a discrete\ndistribution and a probability",
    "start": "786130",
    "end": "792070"
  },
  {
    "text": "density function for a\ncontinuous distribution. In this class, we're going\nto represent probability mass",
    "start": "792070",
    "end": "797440"
  },
  {
    "text": "functions with a capital P and\na probability density function with a lowercase p.",
    "start": "797440",
    "end": "805065"
  },
  {
    "text": "And the way that\nthese work, they have slightly\ndifferent properties. A probability mass\nfunction, the values",
    "start": "805065",
    "end": "810280"
  },
  {
    "text": "need to be between 0 and 1. For a probability\ndensity function,",
    "start": "810280",
    "end": "815440"
  },
  {
    "text": "the values need to be just\ngreater than or equal to 0. It just needs to be positive.",
    "start": "815440",
    "end": "820990"
  },
  {
    "text": "For a probability\nmass function, the sum of all possible\nevents, the probability of all possible events\noccurring must add up to 1.",
    "start": "820990",
    "end": "829020"
  },
  {
    "text": "And then kind of the equivalent\nfor continuous distributions is that the integral of all\npossible values of the variable,",
    "start": "829020",
    "end": "835930"
  },
  {
    "text": "so from minus\ninfinity to infinity, needs to integrate to 1.",
    "start": "835930",
    "end": "841470"
  },
  {
    "text": "And the idea here is that for\na probability mass function, it outputs actual probabilities.",
    "start": "841470",
    "end": "847570"
  },
  {
    "text": "So if we input a value\nsaying like x equals 4, so we're saying, what is the\nprobability that x equals 4?",
    "start": "847570",
    "end": "853780"
  },
  {
    "text": "The output of this p of x\nwill be that probability. So that's what this\nplot is showing here.",
    "start": "853780",
    "end": "858820"
  },
  {
    "text": "So you can see it's\naround like 0.25.",
    "start": "858820",
    "end": "864060"
  },
  {
    "text": "Over here, when we look at\nprobability density functions, the value that you get out\nis not an actual probability.",
    "start": "864060",
    "end": "872080"
  },
  {
    "text": "So if we input this\nvalue into p of x, we don't get the\nprobability that a occurs.",
    "start": "872080",
    "end": "877180"
  },
  {
    "text": "In fact, the probability\nof one single value is actually 0, which is kind\nof strange to think about. And typically, for these\ntypes of distributions,",
    "start": "877180",
    "end": "884300"
  },
  {
    "text": "we think about what is the\nprobability of the variable lying in some continuous range? So what is the probability that\nx is between the value a and b?",
    "start": "884300",
    "end": "893899"
  },
  {
    "text": "And in order to\nfind that, we just integrate the probability\ndensity function in that region.",
    "start": "893900",
    "end": "899150"
  },
  {
    "text": "So it corresponds to the\narea under the curve. ",
    "start": "899150",
    "end": "904990"
  },
  {
    "text": "Hopefully this might\nbe review somewhat. So we often represent these\nprobability distributions",
    "start": "904990",
    "end": "910930"
  },
  {
    "text": "using a set of parameters,\nwhich we'll call theta. So for discrete\ndistributions, the parameters",
    "start": "910930",
    "end": "918310"
  },
  {
    "text": "look like this, where this\nis essentially one way to represent the parameters.",
    "start": "918310",
    "end": "923920"
  },
  {
    "text": "Where we have, for\nexample, for x equal 1, the probability that\nx equals 1 is theta 1.",
    "start": "923920",
    "end": "930230"
  },
  {
    "text": "The probability that x\nequals 2 is theta 2, 3 is theta 3, and so on. And so we just\nhave one parameter",
    "start": "930230",
    "end": "936220"
  },
  {
    "text": "for each possible outcome\nsaying the probability of these outcomes.",
    "start": "936220",
    "end": "942050"
  },
  {
    "text": "For a continuous\ndistribution, this is an example of a\ncontinuous distribution called a normal distribution\nor a Gaussian distribution.",
    "start": "942050",
    "end": "948990"
  },
  {
    "text": "And they often have\nparameters as well. For a Gaussian distribution,\nthe parameters we often use",
    "start": "948990",
    "end": "954199"
  },
  {
    "text": "are its mean, which\nwill represent using mu, or its standard deviation,\nwhich we'll represent",
    "start": "954200",
    "end": "959300"
  },
  {
    "text": "using the letter sigma. So the theta, the\nset of parameters for a Gaussian\ndistribution is that mean",
    "start": "959300",
    "end": "965690"
  },
  {
    "text": "and standard deviation.  So now, we need to\ndecide how we decide",
    "start": "965690",
    "end": "972980"
  },
  {
    "text": "on a model class for\nour whatever problem we're trying to solve\nor whatever system we're",
    "start": "972980",
    "end": "980060"
  },
  {
    "text": "trying to model. And the key\nconsideration here is that we need a model class\nthat's expressive enough.",
    "start": "980060",
    "end": "986880"
  },
  {
    "text": "So what do I mean by\nexpressive enough? There needs to be some setting\nof the model parameters",
    "start": "986880",
    "end": "992569"
  },
  {
    "text": "for which the model\nwill fit the data well. So I'm going to go\nthrough a Pluto Notebook",
    "start": "992570",
    "end": "997800"
  },
  {
    "text": "now to describe this idea. So we're going to start here\nwith a Gaussian distribution.",
    "start": "997800",
    "end": "1004250"
  },
  {
    "text": "And we're going to say\nthat the blue points there, so everything, that\nkind of blue histogram is plotting a bunch\nof samples we have.",
    "start": "1004250",
    "end": "1011180"
  },
  {
    "text": "So maybe this is a bunch\nof sensor measurements we got from our system. Essentially, this is the data\nthat we're trying to match.",
    "start": "1011180",
    "end": "1017529"
  },
  {
    "text": "And then the\nmagenta or pink line there is showing the\ndistribution that we currently",
    "start": "1017530",
    "end": "1024449"
  },
  {
    "text": "have. So we basically want to line up\nthat pink line with our data, and then we can say that we've\ndone a good job modeling it.",
    "start": "1024450",
    "end": "1031809"
  },
  {
    "text": "And we have these kind of\nparameters to play around with for our distribution. For a Gaussian\ndistribution, the parameters",
    "start": "1031810",
    "end": "1037449"
  },
  {
    "text": "we can play around with are the\nmean and the standard deviation. So I'll mess with the\nmean here and do my best",
    "start": "1037450",
    "end": "1044140"
  },
  {
    "text": "to get this\ndistribution lined up. And then we can\ncontrol its width",
    "start": "1044140",
    "end": "1049870"
  },
  {
    "text": "using the standard deviation.  And yeah.",
    "start": "1049870",
    "end": "1056830"
  },
  {
    "text": "Given my fine finger\nmovements, this is about as good as I can do. So you can see though that\nthis matches up pretty well.",
    "start": "1056830",
    "end": "1063518"
  },
  {
    "text": "We're able to get a\ndistribution that lines up pretty well with the blue.",
    "start": "1063518",
    "end": "1068550"
  },
  {
    "text": "And so this is a good fit. So I would say in this case\na Gaussian distribution is",
    "start": "1068550",
    "end": "1074429"
  },
  {
    "text": "expressive enough\nto fit our system and to accurately model it.",
    "start": "1074430",
    "end": "1080700"
  },
  {
    "text": "But now, let's imagine that\nwe sample a different system, and we get a data set\nthat looks like this.",
    "start": "1080700",
    "end": "1087120"
  },
  {
    "text": "So now, in this case,\nwe can mess around with the mean a little bit,\nsee how well we can do. But no matter what\nwe do here, we're",
    "start": "1087120",
    "end": "1094980"
  },
  {
    "text": "not really ever going\nto get something that fits this distribution well. And the reason for that is\nbecause this distribution here",
    "start": "1094980",
    "end": "1102660"
  },
  {
    "text": "is multimodal. So it has two peaks in it there. You can see there's one on\nthe right and one on the left.",
    "start": "1102660",
    "end": "1107920"
  },
  {
    "text": "And a Gaussian\ndistribution is unimodal. So no matter what we\ndo to mu and sigma,",
    "start": "1107920",
    "end": "1114360"
  },
  {
    "text": "we're never going to\nget a distribution that has that shape. And essentially\nwhat that means is",
    "start": "1114360",
    "end": "1120690"
  },
  {
    "text": "that a Gaussian distribution\nis not expressive enough to describe this data set.",
    "start": "1120690",
    "end": "1126600"
  },
  {
    "text": "So there's a few\nthings that we can do if we find that we have a\ndata set that's not modeled well",
    "start": "1126600",
    "end": "1133860"
  },
  {
    "text": "by these simple distributions. There's a few\ndifferent ways that we can increase the complexity\nof our model class.",
    "start": "1133860",
    "end": "1139920"
  },
  {
    "text": "One option is to\ncreate a mixture of simpler distributions.",
    "start": "1139920",
    "end": "1145440"
  },
  {
    "text": "So for example, right\nhere, I'm showing what we call a Gaussian mixture,\nwhere we take two simpler",
    "start": "1145440",
    "end": "1151680"
  },
  {
    "text": "distributions. So in this case, we take\ntwo different Gaussian distributions. We can take more than\ntwo, but for this example,",
    "start": "1151680",
    "end": "1157600"
  },
  {
    "text": "we're just showing two. And then we apply a\nset of weights to them to balance between\nthe relative weighting",
    "start": "1157600",
    "end": "1163559"
  },
  {
    "text": "of each of those distributions. So now, we have five\ndifferent parameters",
    "start": "1163560",
    "end": "1170130"
  },
  {
    "text": "that we can try to tune. And we'll see if we\ncan find basically",
    "start": "1170130",
    "end": "1175669"
  },
  {
    "text": "a setting of these\nparameters that allows us to fit this data set well. So I'll try that real quick.",
    "start": "1175670",
    "end": "1182035"
  },
  {
    "text": " I'll be honest, I\nhave the answer. So I'm just going for it\nhere right to the answer.",
    "start": "1182035",
    "end": "1188752"
  },
  {
    "text": "But it might be kind\nof difficult to pick these parameters at some point. ",
    "start": "1188752",
    "end": "1205909"
  },
  {
    "text": "So essentially, I'm\nmessing with kind of the mean and standard\ndeviation of each of them. And then we can mess with\nthe relative weighting",
    "start": "1205910",
    "end": "1212900"
  },
  {
    "text": "of each of them. So this is getting\nfurther from what we want. But you can see how the\nshape of the distribution",
    "start": "1212900",
    "end": "1219350"
  },
  {
    "text": "changes as we mess with\nthis relative waiting until we get something\nthat fits pretty well.",
    "start": "1219350",
    "end": "1227600"
  },
  {
    "text": "What questions do\nyou have on this?  Yeah.",
    "start": "1227600",
    "end": "1234711"
  },
  {
    "text": "I'm just realizing there's\na formula there now. Is it just to bounce between\nthe two Gaussian terms?",
    "start": "1234712",
    "end": "1241150"
  },
  {
    "text": "Yeah. That's exactly right. And the weights\nneed to sum up to 1. And the reason for\nthat is we still",
    "start": "1241150",
    "end": "1246770"
  },
  {
    "text": "need to have a valid\nprobability distribution. So it still needs\nto integrate to 1. Good question.",
    "start": "1246770",
    "end": "1252570"
  },
  {
    "text": "Yeah. So what I'm understanding is\nthat we have this like data, and then we are trying to fit\na probability distribution.",
    "start": "1252570",
    "end": "1259320"
  },
  {
    "text": "So are we like seeing that\nthe model is a probability distribution?",
    "start": "1259320",
    "end": "1264740"
  },
  {
    "text": "Yeah, exactly. The model is a\nprobability distribution. So in this class, all of\nour models for the most part",
    "start": "1264740",
    "end": "1270320"
  },
  {
    "text": "are going to be\nprobability distributions. And I was just\nremembering I'm supposed to repeat the questions for\npeople that are watching online.",
    "start": "1270320",
    "end": "1276360"
  },
  {
    "text": "The question was what is\nthis kind of pink model? And what is this pink\nprobability distribution?",
    "start": "1276360",
    "end": "1282060"
  },
  {
    "text": "Is that the model the thing that\nwe're trying to come up with? Yes. That is what we're trying\nto do in this lecture.",
    "start": "1282060",
    "end": "1288470"
  },
  {
    "text": "And the other question\nwas what are the weights mean between the\ntwo distributions",
    "start": "1288470",
    "end": "1293920"
  },
  {
    "text": "that make up the mixture? Yeah. [INAUDIBLE] we're mixing. Obviously, it's simple.",
    "start": "1293920",
    "end": "1299780"
  },
  {
    "text": "In this case, the\nsame distribution. Like is it possible to mix\nmultiple different types of distributions in one mixture?",
    "start": "1299780",
    "end": "1306040"
  },
  {
    "text": "I don't see any reason why\nyou couldn't as long as you make sure that it\nintegrates to 1.",
    "start": "1306040",
    "end": "1311560"
  },
  {
    "text": "But yeah. You can do that. ",
    "start": "1311560",
    "end": "1317110"
  },
  {
    "text": "Any other questions? Oh, sorry. The question was could we mix\ndifferent types of distributions",
    "start": "1317110",
    "end": "1322870"
  },
  {
    "text": "in the mixture model? ",
    "start": "1322870",
    "end": "1328509"
  },
  {
    "text": "OK. So that's one way that we could\nmake more complex distributions. And you could imagine, as we\nadd more and more Gaussians",
    "start": "1328510",
    "end": "1335200"
  },
  {
    "text": "or use different types\nof distributions, that increases\nthe amount of data sets we would be able to fit.",
    "start": "1335200",
    "end": "1342070"
  },
  {
    "text": "Another way to make more\ncomplex distributions is to transform a\nsimple distribution.",
    "start": "1342070",
    "end": "1348139"
  },
  {
    "text": "So let's imagine that we have\nsome simple distribution. We'll say just kind of\na Gaussian distribution",
    "start": "1348140",
    "end": "1353679"
  },
  {
    "text": "with a mean of 0\nand a variance of 1.",
    "start": "1353680",
    "end": "1359260"
  },
  {
    "text": "And then let's imagine we're\nnow going to draw 10,000 samples from this distribution. So what that line of code at the\nbottom there is doing is just",
    "start": "1359260",
    "end": "1367000"
  },
  {
    "text": "saying we're going to take\n10,000 samples from this normal distribution. We're going to call those\nsamples z and store them",
    "start": "1367000",
    "end": "1374770"
  },
  {
    "text": "in a vector called z's. And now, OK, so here's\nwhat they look like.",
    "start": "1374770",
    "end": "1380600"
  },
  {
    "text": "The normal distribution\nis that pink line there. And you can see that since\nwe just drew samples directly from it, the samples fit\nthat normal distribution.",
    "start": "1380600",
    "end": "1389530"
  },
  {
    "text": "Now, let's imagine that we\ntake all of these samples and we apply some\nfunction to them",
    "start": "1389530",
    "end": "1395770"
  },
  {
    "text": "to get some new variable\nthat we're going to call x. So in this case, we're going to\napply the cube root function.",
    "start": "1395770",
    "end": "1402230"
  },
  {
    "text": "So we'll define this\nfunction as f of z and say that that's equal\nto the cube root of z.",
    "start": "1402230",
    "end": "1408570"
  },
  {
    "text": "And then we'll just apply that\nfunction to all of the samples that we just took. So now we took all those samples\nfrom the normal distribution,",
    "start": "1408570",
    "end": "1414940"
  },
  {
    "text": "took their cubed root, and\nwe stored them in this vector called x's. So then we can plot a histogram\nover the results here.",
    "start": "1414940",
    "end": "1422822"
  },
  {
    "text": "And this might look kind\nof interesting to you. So what just happened is we\ntook all of the samples that came from that blue histogram.",
    "start": "1422822",
    "end": "1430470"
  },
  {
    "text": "We just applied a\nfunction to them, and then we got this purple\nhistogram, which is much more interesting looking.",
    "start": "1430470",
    "end": "1435520"
  },
  {
    "text": "It's now bimodal. So we've kind of done\nwhat we wanted to in creating a more complex\nprobability distribution.",
    "start": "1435520",
    "end": "1443730"
  },
  {
    "text": "But what we need to know or\nwhat we might want to know is what is the\nprobability density then of this new distribution?",
    "start": "1443730",
    "end": "1449860"
  },
  {
    "text": "So how do we get this\npink line that we had for the other distribution? So let's just make sure we\ndefine some notation here.",
    "start": "1449860",
    "end": "1459143"
  },
  {
    "text": "So we're going to\nsay the variable we took from the simple\ndistribution, we call it z. It comes from the\ndistribution that we're",
    "start": "1459143",
    "end": "1465120"
  },
  {
    "text": "going to call p sub z. And then we transformed it\nto this variable we called x.",
    "start": "1465120",
    "end": "1471240"
  },
  {
    "text": "And we want to know\nwhat is p sub x. So what is the\nformula for p sub x?",
    "start": "1471240",
    "end": "1476690"
  },
  {
    "text": "And x is equal to f of z. So it turns out that if\nf is both invertible and",
    "start": "1476690",
    "end": "1482930"
  },
  {
    "text": "differentiable,\nthen we can actually calculate the density of the\nresulting random variable.",
    "start": "1482930",
    "end": "1490340"
  },
  {
    "text": "And it follows\nthat formula there, which essentially\ninvolves the inverse of f.",
    "start": "1490340",
    "end": "1496490"
  },
  {
    "text": "So that g you see in the\nformula is the inverse of f. So again, we need\nthat function to be",
    "start": "1496490",
    "end": "1502430"
  },
  {
    "text": "both invertible and\ndifferentiable to do this. Luckily, cube root is both\ninvertible and differentiable.",
    "start": "1502430",
    "end": "1509820"
  },
  {
    "text": "So we can test this out. So we can say g of x\nis equal to x cubed.",
    "start": "1509820",
    "end": "1515279"
  },
  {
    "text": "So inverse of cube root\nis equal to x cubed. The derivative of the inverse\ng prime of x is equal to 3x",
    "start": "1515280",
    "end": "1523400"
  },
  {
    "text": "squared. So we've got that implemented. And then we can just implement\nwhat's shown in that equation",
    "start": "1523400",
    "end": "1529580"
  },
  {
    "text": "there. So we say that our new\ndensity p sub x, yeah. Can you zoom in?",
    "start": "1529580",
    "end": "1534650"
  },
  {
    "text": "Oh, yeah. Thank you so much. Is that better? Yes. OK.",
    "start": "1534650",
    "end": "1539750"
  },
  {
    "text": "So our new density now is p sub\nx, which is the pdf, so p sub z,",
    "start": "1539750",
    "end": "1547190"
  },
  {
    "text": "of g of x. So that's the pdf of\na normal distribution because we remember z\nwas distributed according",
    "start": "1547190",
    "end": "1552680"
  },
  {
    "text": "to a normal distribution. Then we input g of\nx, and we multiply it by the absolute value of the\nderivative of the inverse.",
    "start": "1552680",
    "end": "1561163"
  },
  {
    "text": "So it kind of seems\na lot going on here. But if you just kind of break\ndown what each of these things are representing and\nlook at this formula,",
    "start": "1561163",
    "end": "1567299"
  },
  {
    "text": "you can calculate\nthis new density. So once we use\nthat equation, you",
    "start": "1567300",
    "end": "1573410"
  },
  {
    "text": "can see we get the density of\nthe transformed distribution, which lines up well\nwith our histogram",
    "start": "1573410",
    "end": "1578570"
  },
  {
    "text": "of those samples from it.  What questions do\nyou have on that?",
    "start": "1578570",
    "end": "1584210"
  },
  {
    "text": "Yeah. Why did you sample the\noriginal distribution?",
    "start": "1584210",
    "end": "1590860"
  },
  {
    "text": "Why did I take samples\nfrom this distribution? Yeah.",
    "start": "1590860",
    "end": "1596529"
  },
  {
    "text": "I'm guessing that that\ndistribution [INAUDIBLE]. ",
    "start": "1596530",
    "end": "1606590"
  },
  {
    "text": "OK. Great question. So let me try to make the\nconnection between what we're",
    "start": "1606590",
    "end": "1613025"
  },
  {
    "text": "doing here and what we would\ndo if we were trying to make a model in the real world. So in the real world, let's\nsay that we sampled something,",
    "start": "1613025",
    "end": "1620200"
  },
  {
    "text": "and we got this purple data. And we're like, OK, we need\nto fit a distribution to this. We need to pick\nsome of model class.",
    "start": "1620200",
    "end": "1626659"
  },
  {
    "text": "Well, how are we\ngoing to do that? We need something\nthat's bimodal. And so one thing we\ncould do is maybe",
    "start": "1626660",
    "end": "1632290"
  },
  {
    "text": "we could try a Gaussian mixture. Now I'm showing you one\nother thing we could do. We could say, OK, how\ndo we define this?",
    "start": "1632290",
    "end": "1639200"
  },
  {
    "text": "Maybe let's say let's\nstart with a distribution we know a lot of things about.",
    "start": "1639200",
    "end": "1644900"
  },
  {
    "text": "So that's where we get this\nnormal distribution from. And then let's\ntransform it in some way to try to make it fit this data.",
    "start": "1644900",
    "end": "1653169"
  },
  {
    "text": "Does that answer your question? So it's not that you\nalready have the data.",
    "start": "1653170",
    "end": "1660070"
  },
  {
    "text": "It's not that you\nalready have data, like you essentially\ntry to manipulate",
    "start": "1660070",
    "end": "1665080"
  },
  {
    "text": "like the Gaussian\nin this case based on that you know that you can\nmake it become a multimodal.",
    "start": "1665080",
    "end": "1673280"
  },
  {
    "text": "Yeah. Yeah. That's exactly right. So this would be the\ndata that we're given. We want to fit this data. But we don't have any\nlike nice distributions",
    "start": "1673280",
    "end": "1680800"
  },
  {
    "text": "we can use to fit this data. So one thing we\ncould do is we could try to create a mixture\nof distributions, or we could say we know how\nto define this distribution.",
    "start": "1680800",
    "end": "1688580"
  },
  {
    "text": "So let's define our model as a\ntransformation of that simpler distribution.",
    "start": "1688580",
    "end": "1694280"
  },
  {
    "text": "Yeah. Yeah. So really quick-- So it that just\nthe transformation",
    "start": "1694280",
    "end": "1700830"
  },
  {
    "text": "that we're making on this,\nand how did we find that? Oh, great question.",
    "start": "1700830",
    "end": "1705899"
  },
  {
    "text": "I just picked it because\nI knew that this one makes it look cool. It won't always look this cool\ndepending on the f you pick.",
    "start": "1705900",
    "end": "1712260"
  },
  {
    "text": "And again, we needed it\nto be both invertible and differentiable. But this actually leads me\nvery well to my next point.",
    "start": "1712260",
    "end": "1718510"
  },
  {
    "text": "So the question was\nhow do you pick f? So what's really cool is I\njust picked this nice f that",
    "start": "1718510",
    "end": "1724740"
  },
  {
    "text": "allowed us to transform\nthis unimodal distribution into a multimodal one.",
    "start": "1724740",
    "end": "1729780"
  },
  {
    "text": "And this type of transformation\nactually can be quite powerful. And if you've heard the term\nnormalizing flows before,",
    "start": "1729780",
    "end": "1736390"
  },
  {
    "text": "this is the key idea\nbehind normalizing flows. So normalizing\nflows use a series",
    "start": "1736390",
    "end": "1742680"
  },
  {
    "text": "of parametrized, differentiable,\nand invertible functions. So in normalizing flows,\nessentially, what they're doing is trying to learn\nthat transformation.",
    "start": "1742680",
    "end": "1750040"
  },
  {
    "text": "So I just picked cube root\nbecause it works well. But we don't always\nknow what works well if we're trying to\ntransform to some very",
    "start": "1750040",
    "end": "1755640"
  },
  {
    "text": "complicated distribution. And so what\nnormalizing flows do is kind of let there be\na set of parameters",
    "start": "1755640",
    "end": "1760980"
  },
  {
    "text": "for these invertible\ntransformations, and they learn\nthem, so that we can transform simple\ndistributions to complex ones.",
    "start": "1760980",
    "end": "1767670"
  },
  {
    "text": "So there's a nice tutorial\nI linked this notebook. I can also link it on Ed\nif people are interested.",
    "start": "1767670",
    "end": "1772809"
  },
  {
    "text": "You can learn more about it. The normalizing flows are\noutside the scope of this class, unfortunately.",
    "start": "1772810",
    "end": "1778930"
  },
  {
    "text": "But what I really\njust want you to know is this simple change\nof variables formula, assuming that I gave\nyou a transformation.",
    "start": "1778930",
    "end": "1786870"
  },
  {
    "text": "But just to get you\nexcited about them, here's a super cool animation\nof transforming a very simple",
    "start": "1786870",
    "end": "1793320"
  },
  {
    "text": "distribution into\nsomething more complex using this idea of\na normalizing flow.",
    "start": "1793320",
    "end": "1798470"
  },
  {
    "text": " What other questions\ndo you have on this?",
    "start": "1798470",
    "end": "1803705"
  },
  {
    "start": "1803705",
    "end": "1808830"
  },
  {
    "text": "Yeah. I'm just curious. So is it called a flow because\nof the optimization process that's going on to find\nwhich functions to use?",
    "start": "1808830",
    "end": "1818240"
  },
  {
    "text": "[INAUDIBLE] but\nwhat is that over? The question is why\nis it called a flow? To be honest, I\ndon't know for sure.",
    "start": "1818240",
    "end": "1824538"
  },
  {
    "text": "I think it's because you\nhave this kind of series of transformations. So it's flowing through\nthat, I don't know.",
    "start": "1824538",
    "end": "1831090"
  },
  {
    "text": "Does anyone have any\nother-- yeah, OK.  So yeah.",
    "start": "1831090",
    "end": "1836970"
  },
  {
    "text": "Like I said, we needed\nthis kind of transformation we applied to be both\ndifferentiable and invertible.",
    "start": "1836970",
    "end": "1843410"
  },
  {
    "text": "We could still do\nwhat we just did without having a differentiable\nand invertible transformation.",
    "start": "1843410",
    "end": "1849600"
  },
  {
    "text": "We just won't be able to\ncalculate the density. So we could still\ndraw samples though. So for example, we could\ntake a bunch of samples",
    "start": "1849600",
    "end": "1856970"
  },
  {
    "text": "from a uniform distribution,\nwhich is what's shown on the left in blue. Draw samples and then maybe\npass them through some crazier",
    "start": "1856970",
    "end": "1865850"
  },
  {
    "text": "function like x of\nz times sine of 8z. And then we get the samples\nthat are shown on the right.",
    "start": "1865850",
    "end": "1871862"
  },
  {
    "text": "So we have this very\ninteresting-looking distribution. Unfortunately, we can't\ncalculate the density of it,",
    "start": "1871863",
    "end": "1876908"
  },
  {
    "text": "but sometimes, that's OK. Sometimes, we only need\nsamples from our distribution, so we could just\nget the samples.",
    "start": "1876908",
    "end": "1884299"
  },
  {
    "text": "So let's go back here and review\nwhat we just talked about. So what we showed was basically\na very simple distribution",
    "start": "1884300",
    "end": "1892130"
  },
  {
    "text": "doesn't fit the data that we\ngathered from the real world well, then we need something\nmore complex to model our data.",
    "start": "1892130",
    "end": "1900080"
  },
  {
    "text": "One way to create a\nmore complex model was to create a weighted\nmixture of simple distributions.",
    "start": "1900080",
    "end": "1907250"
  },
  {
    "text": "Another way to make\nthat more complex model was to apply a\nfunctional transformation to a simple distribution.",
    "start": "1907250",
    "end": "1913670"
  },
  {
    "text": "So in this case, we took\nour distribution, p of z, which was a Gaussian\ndistribution.",
    "start": "1913670",
    "end": "1919620"
  },
  {
    "text": "We applied the\ncube root function, and we got a bimodal\ndistribution.",
    "start": "1919620",
    "end": "1925340"
  },
  {
    "text": "This is the formula\nthat we used for that. And as a note that I\nsaid a few times now,",
    "start": "1925340",
    "end": "1931760"
  },
  {
    "text": "this formula requires that\nf is both differentiable and invertible. But even if it's\nnot, we can still",
    "start": "1931760",
    "end": "1937419"
  },
  {
    "text": "apply some of functional\ntransformation. We're just not going to be able\nto calculate the probability density function.",
    "start": "1937420",
    "end": "1944320"
  },
  {
    "text": "But we could still\ndraw samples from it. So these, for example,\nwould be samples",
    "start": "1944320",
    "end": "1950080"
  },
  {
    "text": "from a pseudorandom number\ngenerator, something that we can sample using\nany programming language.",
    "start": "1950080",
    "end": "1956023"
  },
  {
    "text": "For example, in Julia,\nyou can just call rand, and you'll get these\ntypes of samples. And then we can\ntransform them using",
    "start": "1956023",
    "end": "1961510"
  },
  {
    "text": "some more complicated function.  So the risk of using\nlots of buzzwords,",
    "start": "1961510",
    "end": "1970330"
  },
  {
    "text": "we typically call\nmodels represented in this way generative models.",
    "start": "1970330",
    "end": "1975970"
  },
  {
    "text": "I'll just use some\nmore buzzwords anyway. So you've probably heard of\ngenerative adversarial networks,",
    "start": "1975970",
    "end": "1982150"
  },
  {
    "text": "variational autoencoders. Maybe you've heard\nof diffusion models. These models all kind\nof work in this way.",
    "start": "1982150",
    "end": "1988102"
  },
  {
    "text": "So they take samples\nfrom a distribution we know how to sample from,\nand they transform them in some way to some more\ncomplicated distribution",
    "start": "1988103",
    "end": "1994750"
  },
  {
    "text": "that models something\nthat we care about. So for example, maybe\nthey take a bunch of samples from a multivariate\nnormal distribution.",
    "start": "1994750",
    "end": "2002170"
  },
  {
    "text": "They transform them in some way. Typically, the transformation\ninvolves a neural network. And then you get some\ninteresting set of samples",
    "start": "2002170",
    "end": "2008850"
  },
  {
    "text": "like images of cat faces. ",
    "start": "2008850",
    "end": "2014520"
  },
  {
    "text": "All right. So everything we just\ntalked about for now was just talking about if we\nhave one variable, so only",
    "start": "2014520",
    "end": "2021810"
  },
  {
    "text": "one event that we care\nabout the outcome of. So maybe this is like a\nsensor that's observing",
    "start": "2021810",
    "end": "2027029"
  },
  {
    "text": "the altitude of the aircraft. And we just have\none observation, which is the observation\nof the predicted altitude",
    "start": "2027030",
    "end": "2034650"
  },
  {
    "text": "of the aircraft. And we're trying to\nmodel that sensor. We also might have multiple\nvariables involved.",
    "start": "2034650",
    "end": "2040750"
  },
  {
    "text": "So maybe we have a sensor\nthat's predicting both the altitude and the airspeed. And in this case,\nwe're typically",
    "start": "2040750",
    "end": "2046860"
  },
  {
    "text": "interested in joint or\nconditional distributions. So a joint distribution is\njust a probability distribution",
    "start": "2046860",
    "end": "2054020"
  },
  {
    "text": "over multiple variables\nor multiple outcomes. Sometimes, we also call this\na multivariate distribution.",
    "start": "2054020",
    "end": "2060649"
  },
  {
    "text": "And it represents the likelihood\nof multiple different outcomes occurring simultaneously.",
    "start": "2060650",
    "end": "2065780"
  },
  {
    "text": "So for discrete\ndistributions, we still represent them using\nprobability mass functions. But now, we have two\ninputs to them or more.",
    "start": "2065780",
    "end": "2072719"
  },
  {
    "text": "So we have, for example,\nthe probability of x and y occurring. We can represent\nthem using tables.",
    "start": "2072719",
    "end": "2079190"
  },
  {
    "text": "This is a very\ncommon representation that we'll use in the class. And essentially, the\nway to read this table is by saying, assume here that\nx and y are both binaries.",
    "start": "2079190",
    "end": "2087449"
  },
  {
    "text": "So they have two\npossible outcomes. X could either be 0 or 1,\nand y could either be 0 or 1.",
    "start": "2087449",
    "end": "2092520"
  },
  {
    "text": "And what this\ntable is showing is if x is the probability\nthat both x is 0 and y is 0 is equal to 0.1.",
    "start": "2092520",
    "end": "2100550"
  },
  {
    "text": "The probability that x is 0\nand y is 1 is 0.2 and so on.",
    "start": "2100550",
    "end": "2106530"
  },
  {
    "text": "And this is a valid\nprobability distribution. So that means that the\nentries in this table need to sum up to 1.",
    "start": "2106530",
    "end": "2112550"
  },
  {
    "text": " We also have continuous\njoint distributions.",
    "start": "2112550",
    "end": "2118450"
  },
  {
    "text": "So in this case, we now have\na probability density function over these variables. One very common continuous\nprobability distribution",
    "start": "2118450",
    "end": "2126869"
  },
  {
    "text": "for multiple variables is\na multivariate Gaussian distribution. So we're extending the\nGaussian distribution",
    "start": "2126870",
    "end": "2132630"
  },
  {
    "text": "to multiple dimensions. It's defined by a mean vector,\nwhich kind of specifies the center of the distribution.",
    "start": "2132630",
    "end": "2139810"
  },
  {
    "text": "And a covariance matrix. And I don't know\nif it's just me, but like covariance\nmatrices like scared me",
    "start": "2139810",
    "end": "2146790"
  },
  {
    "text": "for the longest time. So I just want to go\nthrough it real quick. What exactly the entries mean?",
    "start": "2146790",
    "end": "2152050"
  },
  {
    "text": "Because there's really nothing\nsuper crazy going on here. So here's a multivariate\nGaussian distribution.",
    "start": "2152050",
    "end": "2158170"
  },
  {
    "text": "So its mean is kind of in the\ncenter of that circle there. This is a plot of the contours\nof the probability density.",
    "start": "2158170",
    "end": "2165369"
  },
  {
    "text": "So brighter blue colors will\nindicate higher density. And so here's the current\ncovariance matrix we have.",
    "start": "2165370",
    "end": "2171830"
  },
  {
    "text": "So let me go through each entry. So the upper left entry\nthere is kind of showing you",
    "start": "2171830",
    "end": "2179260"
  },
  {
    "text": "how much does x\nvary with itself? So how much does the variable\non the x-axis vary with itself?",
    "start": "2179260",
    "end": "2186530"
  },
  {
    "text": "So you can see, if I\nincrease this value, then we get kind of a wider\nspread along the x direction.",
    "start": "2186530",
    "end": "2194030"
  },
  {
    "text": "And if I decrease this value, we\nget kind of a narrower spread, OK? But nothing really\nhappened to y at all.",
    "start": "2194030",
    "end": "2202060"
  },
  {
    "text": "Similarly, the value\non the bottom right of this covariance\nmatrix tells us how much",
    "start": "2202060",
    "end": "2207339"
  },
  {
    "text": "does y vary with itself? So it's a very similar story. If we increase this value,\nwe increase the spread of y.",
    "start": "2207340",
    "end": "2213559"
  },
  {
    "text": "If we decrease this value,\nwe decrease the spread of y.",
    "start": "2213560",
    "end": "2218860"
  },
  {
    "text": "Let's put that back. OK. And then the off\ndiagonal ones, which I think is what tends to\nconfuse people sometimes,",
    "start": "2218860",
    "end": "2225470"
  },
  {
    "text": "is just how much do the\nvariables vary with each other. So right now, it's set to 0. So you can see\nthere's not really",
    "start": "2225470",
    "end": "2231750"
  },
  {
    "text": "any correlation between the\nvalues of the variables. We just get circles there. But now, let's\nsay I increase it.",
    "start": "2231750",
    "end": "2237700"
  },
  {
    "text": "So that means that x and y vary\npositively with each other. So now that we can see that if\nwe have a positive value of x,",
    "start": "2237700",
    "end": "2245647"
  },
  {
    "text": "it's also more likely\nthat we're going to have a positive value of y. And similarly, if we have\na negative value of x,",
    "start": "2245647",
    "end": "2251862"
  },
  {
    "text": "it's more likely we're going\nto have a negative value of y. So it's these variables\nhave some correlation",
    "start": "2251862",
    "end": "2257040"
  },
  {
    "text": "between each other. And the higher this value is,\nthe more correlation we have.",
    "start": "2257040",
    "end": "2262450"
  },
  {
    "text": "So the more strongly\nwe see that trend. And then similarly,\nif we go the other way it's just saying they have\nthe opposite correlation.",
    "start": "2262450",
    "end": "2269320"
  },
  {
    "text": "So if we have a\npositive value of x, we're now more likely to\nhave a negative value of y.",
    "start": "2269320",
    "end": "2275040"
  },
  {
    "text": "So that's a multivariate\nGaussian distribution. Hopefully there's no\nmore fears of covariance.",
    "start": "2275040",
    "end": "2282730"
  },
  {
    "text": "All right. So one more note here. If two variables are what\nwe consider independent,",
    "start": "2282730",
    "end": "2288100"
  },
  {
    "text": "then their joint\ndistribution is just the product of their\nindividual probabilities.",
    "start": "2288100",
    "end": "2294360"
  },
  {
    "text": "Otherwise, this is\nnot necessarily true. So I just thought I would\ndo a fun little aside here.",
    "start": "2294360",
    "end": "2300480"
  },
  {
    "text": "So we've been talking\nabout wisdom teeth. And it turns out we have\nthis very weird combination of wisdom teeth among the\nfour textbook authors.",
    "start": "2300480",
    "end": "2308440"
  },
  {
    "text": "So like I said, I never had any. Anthony, who's our other\nauthor, he also never had any.",
    "start": "2308440",
    "end": "2315360"
  },
  {
    "text": "Robert has all four,\nand Michael had five. So we thought maybe it would\nbe fun to just calculate",
    "start": "2315360",
    "end": "2321450"
  },
  {
    "text": "the probability\nthat this happens. So let's just do\nthat real quick.",
    "start": "2321450",
    "end": "2327930"
  },
  {
    "text": "All right. Let me see here. So essentially, what we\nwant to calculate here",
    "start": "2327930",
    "end": "2334740"
  },
  {
    "text": "is the probability that\nI have 0 wisdom teeth. So that's S0 for Sidney.",
    "start": "2334740",
    "end": "2340260"
  },
  {
    "text": "Anthony also has 0 wisdom teeth\nRobert has 4 wisdom teeth, and Michael has 5.",
    "start": "2340260",
    "end": "2346902"
  },
  {
    "text": "And because we're going\nto assume these are all independent events, so the\nnumber of wisdom teeth I had I",
    "start": "2346902",
    "end": "2352010"
  },
  {
    "text": "don't think really affected\nthe number of wisdom teeth Anthony had. And so we're just\ngoing to say then that's the probability\nthat I have zero wisdom",
    "start": "2352010",
    "end": "2359119"
  },
  {
    "text": "teeth times the probability\nthat Anthony has zero wisdom teeth times the\nprobability that Robert",
    "start": "2359120",
    "end": "2364910"
  },
  {
    "text": "has four times the probability\nthat Michael has five.",
    "start": "2364910",
    "end": "2370520"
  },
  {
    "text": "And then if you assign numbers\nto all of these probabilities, you get something like 0.00017.",
    "start": "2370520",
    "end": "2381560"
  },
  {
    "text": "But I'll point out these are\nsketchy numbers from googling",
    "start": "2381560",
    "end": "2391735"
  },
  {
    "text": "ChatGPT.  So take that as you will.",
    "start": "2391735",
    "end": "2397520"
  },
  {
    "text": "But there you go. So very unlikely\ncombination of wisdom teeth",
    "start": "2397520",
    "end": "2402530"
  },
  {
    "text": "for the textbook authors. And then just in\ncase you really want to explore this\ntopic further, I'm",
    "start": "2402530",
    "end": "2408580"
  },
  {
    "text": "not going to go through\nthis in class right now, but I actually took\nthe time to figure out what is the probability that\nthe sum of the number of wisdom",
    "start": "2408580",
    "end": "2415190"
  },
  {
    "text": "teeth of the four textbook\nauthors is equal to nine because that feels like\na really weird number. So if you want to\nknow, that probability,",
    "start": "2415190",
    "end": "2422130"
  },
  {
    "text": "I got to be around 3%. And I actually plotted\nthe full distribution",
    "start": "2422130",
    "end": "2427280"
  },
  {
    "text": "over sums of wisdom teeth\nfor four book authors, all based on the\nsketchiest assumptions",
    "start": "2427280",
    "end": "2432710"
  },
  {
    "text": "from Google and ChatGPT. So take that as you will. But it is kind of interesting.",
    "start": "2432710",
    "end": "2438960"
  },
  {
    "text": "You can see there's\nlike peaks at 16 and 12. So most likely, three had\nfour, and one had zero,",
    "start": "2438960",
    "end": "2445140"
  },
  {
    "text": "or everyone had four. Yeah. Kind of interesting.",
    "start": "2445140",
    "end": "2450260"
  },
  {
    "text": "Now, we'll move on to\nconditional distributions. So this is a distribution\nover a single variable",
    "start": "2450260",
    "end": "2457430"
  },
  {
    "text": "or a set of variables\ngiven the values of one or more other variables. So this is saying,\nhere in this case,",
    "start": "2457430",
    "end": "2463700"
  },
  {
    "text": "we're saying we\nknow the value of x. What does that tell us about the\nprobability of the outcome y?",
    "start": "2463700",
    "end": "2470170"
  },
  {
    "text": "And we can define that in\nterms of the joint probability and the univariate\nprobability, saying",
    "start": "2470170",
    "end": "2475870"
  },
  {
    "text": "that this is equal to\nthe probability of y and x occurring, divided by\nthe probability of x occurring.",
    "start": "2475870",
    "end": "2484750"
  },
  {
    "text": "So in the discrete\ncase, again, we have this kind of\nprobability mass. We can use a table\nto represent it.",
    "start": "2484750",
    "end": "2490430"
  },
  {
    "text": "So what this is showing is\nessentially the probability that if x is equal to 0.",
    "start": "2490430",
    "end": "2495730"
  },
  {
    "text": "What is the probability\nthat y is equal to 0? And for this row,\nit says given that x",
    "start": "2495730",
    "end": "2502359"
  },
  {
    "text": "is equal to 1, what is\nthe probability that y is equal to 0? There's two entries\nin this table missing.",
    "start": "2502360",
    "end": "2509900"
  },
  {
    "text": "Can anyone tell me what\nthis first entry should be? ",
    "start": "2509900",
    "end": "2516760"
  },
  {
    "text": "0.9? 0.9, yeah. So if x is 0, the\nprobability that y is 0--",
    "start": "2516760",
    "end": "2523430"
  },
  {
    "text": "oh, I guess I should say\nthis was a binary variable. I wasn't trying\nto trick you here. So if x is 0, and we know the\nprobability of y being 0 is 0.1,",
    "start": "2523430",
    "end": "2533240"
  },
  {
    "text": "the only other\noption is y being 1. And so it must be the case,\nthen that has a 0.9 probability",
    "start": "2533240",
    "end": "2539320"
  },
  {
    "text": "of occurring because we need our\nprobabilities to sum up to 1. Similarly here, if\nx is equal to 1,",
    "start": "2539320",
    "end": "2545240"
  },
  {
    "text": "we know the probability\nthat y of 0 is 0.5. The other option is\nthat y is equal to 1.",
    "start": "2545240",
    "end": "2550850"
  },
  {
    "text": "And so the probability\nmissing here is 0.5.",
    "start": "2550850",
    "end": "2557410"
  },
  {
    "text": "For continuous\ndistributions, we can also have conditional probabilities. Here's a really\ncommon one that we're",
    "start": "2557410",
    "end": "2563080"
  },
  {
    "text": "going to use in\nthis class, where we have a conditional\nGaussian distribution.",
    "start": "2563080",
    "end": "2568600"
  },
  {
    "text": "So what's going on here is we\nhave a normal distribution. So y is distributed according\nto a normal distribution.",
    "start": "2568600",
    "end": "2575480"
  },
  {
    "text": "But that distribution,\nthe parameters of that normal\ndistribution depend on x. So it's conditioned\non the value of x.",
    "start": "2575480",
    "end": "2582910"
  },
  {
    "text": "In this case, we're saying it's\njust the mean that depends on x. The mean is actually\njust equal to x. So you get something like this,\nwhere over here on the x-axis,",
    "start": "2582910",
    "end": "2591990"
  },
  {
    "text": "we say the mean is equal to x. So for this value here,\nwe have a distribution that looks like that.",
    "start": "2591990",
    "end": "2597940"
  },
  {
    "text": "For this value here,\nit has a mean here. So we have a distribution\nthat looks like that. And if you plot all\nof those together",
    "start": "2597940",
    "end": "2603600"
  },
  {
    "text": "with the brighter colors\nindicating higher likelihood, this is what that\ndistribution looks like.",
    "start": "2603600",
    "end": "2608705"
  },
  {
    "start": "2608705",
    "end": "2617910"
  },
  {
    "text": "So yeah. One note here we're\ngoing to be talking about conditional\ndistributions quite a bit. Because if you remember\nfrom the last lecture,",
    "start": "2617910",
    "end": "2624270"
  },
  {
    "text": "the distributions that we\nwant to model for our system are all conditional\ndistributions. So for the agent,\nfor example, we",
    "start": "2624270",
    "end": "2631980"
  },
  {
    "text": "want to know the probability of\nan action given an observation. The transition, we\nwant the probability",
    "start": "2631980",
    "end": "2637020"
  },
  {
    "text": "of the next state given the\ncurrent state in action. And for the sensor, we\nwant the probability of an observation given\nthe current state.",
    "start": "2637020",
    "end": "2646050"
  },
  {
    "text": "OK. What questions do you have on\nmodel classes before we move on?",
    "start": "2646050",
    "end": "2651215"
  },
  {
    "text": " [INAUDIBLE] how to read the\ncontinuous distribution graph?",
    "start": "2651215",
    "end": "2657670"
  },
  {
    "text": "Yeah, absolutely. Let's go back here.",
    "start": "2657670",
    "end": "2662857"
  },
  {
    "text": "So the question was\nif I could explain this continuous\ndistribution graph again? So the brighter colors\nindicate higher probability.",
    "start": "2662857",
    "end": "2670240"
  },
  {
    "text": "And we're saying that\nthe distribution over y is dependent on x. So when x is at this\nbottom corner here,",
    "start": "2670240",
    "end": "2678130"
  },
  {
    "text": "the mean of the distribution\nover y is just equal to x. So mu is equal to x.",
    "start": "2678130",
    "end": "2684010"
  },
  {
    "text": "So the distribution\nover y is going to look something like that. And so that's why we see\nbright colors here centered",
    "start": "2684010",
    "end": "2691080"
  },
  {
    "text": "around the mean. And then these darker\ncolors out here. Now if x is over here, the mean\nis still going to be equal to x.",
    "start": "2691080",
    "end": "2699240"
  },
  {
    "text": "So the mean of y is\ngoing to be right here. And then we're going to\nhave a distribution that looks something like this.",
    "start": "2699240",
    "end": "2704610"
  },
  {
    "text": "And you can see that\nkind of smoothly changing as we change x. So it's kind of like on\na diagonal kind of view.",
    "start": "2704610",
    "end": "2713720"
  },
  {
    "text": "Yeah, exactly. And that's only because\nwe said that the mean is going to be equal to x. So kind of the highest\nlikelihood thing",
    "start": "2713720",
    "end": "2720650"
  },
  {
    "text": "is going to be y equals x. And then it's going to\nspread out from there. Yeah, no problem.",
    "start": "2720650",
    "end": "2728210"
  },
  {
    "text": "Let's go on to parameter. So now we've picked\na model class. And you saw we did a\nlot of messing around",
    "start": "2728210",
    "end": "2733867"
  },
  {
    "text": "with parameters here. How do we actually\ndecide what parameters we want for our model class?",
    "start": "2733867",
    "end": "2740180"
  },
  {
    "text": "So what we want to do\nhere is we basically want to automatically do what\nwe were doing in that notebook. So dragging around those\nsliders was not so easy.",
    "start": "2740180",
    "end": "2748170"
  },
  {
    "text": "And you can imagine, what if\nwe have millions of parameters? Like neural networks have\nmillions of parameters?",
    "start": "2748170",
    "end": "2754609"
  },
  {
    "text": "We're not going to sit there\ndragging around sliders all the time. So to formalize\nwhat's going on here",
    "start": "2754610",
    "end": "2760920"
  },
  {
    "text": "is we had some set\nof parameters, theta, and we had some data set that\nwe observed from the real world",
    "start": "2760920",
    "end": "2766740"
  },
  {
    "text": "that we want to model. And we want to find\nthe parameters theta that best fit this data set.",
    "start": "2766740",
    "end": "2774060"
  },
  {
    "text": "So how do we know which of\nthese parameters are best. One option is to look\nfor the ones that",
    "start": "2774060",
    "end": "2780390"
  },
  {
    "text": "make the data most likely. So given a set of\nparameters, we can calculate the probability of\nobserving the data set we did.",
    "start": "2780390",
    "end": "2787720"
  },
  {
    "text": "So we could say let's assume\nthe parameters were this. What is the probability then\nthat we got that data set?",
    "start": "2787720",
    "end": "2794010"
  },
  {
    "text": "And the higher that probability\nis, the better it is. And basically,\nwhat we want to do",
    "start": "2794010",
    "end": "2800490"
  },
  {
    "text": "is find the parameters that make\nthe data seem the most likely to have occurred.",
    "start": "2800490",
    "end": "2806850"
  },
  {
    "text": "So the way that we write\nthis is saying theta hat. So our estimate for\nthose parameters is equal to the theta that\nmaximizes this probability.",
    "start": "2806850",
    "end": "2815750"
  },
  {
    "text": "And what this says this is\nthe probability of the data. So D is the data set. So all those data\npoints that we observed.",
    "start": "2815750",
    "end": "2821630"
  },
  {
    "text": "All those blue data points that\nI was showing given theta, given the parameters.",
    "start": "2821630",
    "end": "2828772"
  },
  {
    "text": "We're going to\nassume that our data set is some set of\nobservations from the world. So maybe this was a bunch of\nsensor measurements we got.",
    "start": "2828772",
    "end": "2836140"
  },
  {
    "text": "And then another way that\nwe're going to write this. So we use this notation\nsometimes in the book.",
    "start": "2836140",
    "end": "2841150"
  },
  {
    "text": "It's the colon notation. So whenever you see this 1 colon\nm, it just means observation 1",
    "start": "2841150",
    "end": "2846760"
  },
  {
    "text": "through observation m.  So we're going to make\nan assumption here,",
    "start": "2846760",
    "end": "2853580"
  },
  {
    "text": "which is that all\nthe data points that we gathered\nwere independent and identically distributed.",
    "start": "2853580",
    "end": "2858610"
  },
  {
    "text": "And what that allows\nus to do is use what I was showing\nyou before, where we could take those\nindependent events and just multiply\ntheir probabilities.",
    "start": "2858610",
    "end": "2865670"
  },
  {
    "text": "So then the probability that\nwe got this whole data set, given our parameters,\nis just the probability",
    "start": "2865670",
    "end": "2871150"
  },
  {
    "text": "that we got the first\nobservation given our parameters times the probability that we\ngot the second observation, given our parameters, and so on,\nall the way to the probability",
    "start": "2871150",
    "end": "2878720"
  },
  {
    "text": "that we got the nth observation\ngiven our parameters.",
    "start": "2878720",
    "end": "2884000"
  },
  {
    "text": "So we can concisely write\nthat as this product from i equals 1 to m of all\nof our probabilities",
    "start": "2884000",
    "end": "2889700"
  },
  {
    "text": "of the observations, given\nsome choice for the parameters.",
    "start": "2889700",
    "end": "2895947"
  },
  {
    "text": "One thing we're going\nto do that might seem a little weird at first\nis take the logarithm of this. The point of that\nis just it typically",
    "start": "2895947",
    "end": "2902210"
  },
  {
    "text": "results in better\nnumerical stability. Nothing crazy going on. But when we take\nthe log, we end up",
    "start": "2902210",
    "end": "2908480"
  },
  {
    "text": "changing this product\ninto a sum, which makes things a little bit nicer.",
    "start": "2908480",
    "end": "2913790"
  },
  {
    "text": "And then it turns out\nthat maximizing the log will have the exact same\nsolution as maximizing it",
    "start": "2913790",
    "end": "2920150"
  },
  {
    "text": "without the log. And so we can rewrite\nour maximization problem that we're trying\nto solve like this.",
    "start": "2920150",
    "end": "2927300"
  },
  {
    "text": "So we're saying\nthe theta we choose is the theta that maximizes\nthe sum of the log",
    "start": "2927300",
    "end": "2932370"
  },
  {
    "text": "probabilities of all of\nthe observations given our choice for theta.",
    "start": "2932370",
    "end": "2939329"
  },
  {
    "text": "So to summarize what\nwe just did here, this is something called maximum\nlikelihood parameter estimation.",
    "start": "2939330",
    "end": "2945369"
  },
  {
    "text": "So we started by\nsaying we just want to take the parameter that\nmakes the data we observed most likely.",
    "start": "2945370",
    "end": "2950640"
  },
  {
    "text": "We assumed independent\nobservations, which changed it into this product. And then we took the log just\nto make the optimization easier.",
    "start": "2950640",
    "end": "2958329"
  },
  {
    "text": "And we got this summation\nof log probabilities. ",
    "start": "2958330",
    "end": "2963690"
  },
  {
    "text": "So let me just show you really\nquick a very silly way to solve this optimization problem.",
    "start": "2963690",
    "end": "2968760"
  },
  {
    "text": "So one thing that\nwe could do here. Let's say, for example, that\nwe have this data set here",
    "start": "2968760",
    "end": "2975540"
  },
  {
    "text": "of x and y values. So I didn't label the\naxes here, but the x-axis is like some input x.",
    "start": "2975540",
    "end": "2982270"
  },
  {
    "text": "And then maybe this is,\nfor example, the state that our aircraft\nwas actually in. And then the y value\nis some observation.",
    "start": "2982270",
    "end": "2990170"
  },
  {
    "text": "And so maybe this is like\nthe observation, for example. And what we want to do is fit\na conditional probability model",
    "start": "2990170",
    "end": "2998869"
  },
  {
    "text": "to this data. And we're going to\nsay, probably should have written what\nit is, but we're",
    "start": "2998870",
    "end": "3004570"
  },
  {
    "text": "going to say the model looks\nsomething like this, where we have these kind of\nparameters here, where we're going to say it's\na normal distribution centered",
    "start": "3004570",
    "end": "3011740"
  },
  {
    "text": "with a mean. That's some linear\nfunction of our input x and some standard deviation.",
    "start": "3011740",
    "end": "3017990"
  },
  {
    "text": "So this we're going\nto call theta 1. It's already filled in\nhere, but we'll just call it theta 1, which is\nwhat we're trying to find.",
    "start": "3017990",
    "end": "3024160"
  },
  {
    "text": "This is theta 2,\nwhat we're adding on. And then this is the\nstandard deviation.",
    "start": "3024160",
    "end": "3029620"
  },
  {
    "text": "So one kind of silly way\nto do this optimization is to just mess with\nthese parameters",
    "start": "3029620",
    "end": "3034930"
  },
  {
    "text": "until we get the lowest\npossible likelihood. So for example, I can\nmess with theta 1, which",
    "start": "3034930",
    "end": "3039970"
  },
  {
    "text": "is the slope of that\nlinear function that determines the mean. So that's why it's\nkind of changing",
    "start": "3039970",
    "end": "3045308"
  },
  {
    "text": "the slope of this\nprobability density here. So I could just mess\nwith that for a while",
    "start": "3045308",
    "end": "3050920"
  },
  {
    "text": "and watch the log\nlikelihood that comes out and try to find some setting of\nthese parameters that gives us",
    "start": "3050920",
    "end": "3057430"
  },
  {
    "text": "the best log likelihood. So for example, I'm noticing\nthat log likelihood gets higher",
    "start": "3057430",
    "end": "3063609"
  },
  {
    "text": "as I decrease this variance. So maybe I'll keep decreasing it\nuntil I get some log likelihood",
    "start": "3063610",
    "end": "3070120"
  },
  {
    "text": "that's no longer increasing. So it decreased.",
    "start": "3070120",
    "end": "3075140"
  },
  {
    "text": "So now, I'll go back up. And we could just\ndo this for a while.",
    "start": "3075140",
    "end": "3080640"
  },
  {
    "text": "OK. So we had our\nobjective to optimize. But again, we're still\nusing these sliders. So that's why it's a very silly\nway to do this optimization.",
    "start": "3080640",
    "end": "3088337"
  },
  {
    "text": "It turns out there's\na much better way to do this optimization, which\nis to actually just give it",
    "start": "3088337",
    "end": "3093760"
  },
  {
    "text": "to an optimization algorithm\nthat can do this for you. So we're not really\ngoing to talk about how these optimization\nalgorithms work.",
    "start": "3093760",
    "end": "3100789"
  },
  {
    "text": "We'll talk a little bit more\nabout them later in the quarter. But for now, I just want\nyou to think of them as these black boxes that\ntake in some objective",
    "start": "3100790",
    "end": "3108720"
  },
  {
    "text": "and the parameters that\nthey're allowed to change, and then they give you back\nthe optimal parameters.",
    "start": "3108720",
    "end": "3116589"
  },
  {
    "text": "OK. More buzzwords. There's gradient descent. You may have heard of ADAM,\ngenetic algorithms, momentum.",
    "start": "3116590",
    "end": "3122560"
  },
  {
    "text": "There's lots of different types\nof optimization algorithms. And we'll talk a little bit\nmore about them in chapter 4.",
    "start": "3122560",
    "end": "3129809"
  },
  {
    "text": "But if you really want\nthe full story on these, you should take\nAA222 next quarter if you haven't taken it already.",
    "start": "3129810",
    "end": "3137040"
  },
  {
    "text": "Questions on this? Yep. This might be stepping\noutside the scope of the class a little bit.",
    "start": "3137040",
    "end": "3142600"
  },
  {
    "text": "But if we have, say, a data\nset, and we have a probability",
    "start": "3142600",
    "end": "3148102"
  },
  {
    "text": "distribution that we want\nto transform in order to represent the\ndata points, would it be possible to use a functional\nto find a function with which we",
    "start": "3148102",
    "end": "3157470"
  },
  {
    "text": "can transform the\nprobability distribution? Yeah. So the question was\nif we have a data set",
    "start": "3157470",
    "end": "3163770"
  },
  {
    "text": "and we have some\nsimple distribution we want to transform in order\nto get that data set, can we",
    "start": "3163770",
    "end": "3169320"
  },
  {
    "text": "try to learn that function. Is that correct? Can we try to learn\nthat function? Yes, exactly.",
    "start": "3169320",
    "end": "3174550"
  },
  {
    "text": "So that's actually exactly\nwhat normalizing flows do. That's kind of what generative\nadversarial networks do.",
    "start": "3174550",
    "end": "3180543"
  },
  {
    "text": "They're basically learning\nthis transformation of a simple distribution to a\nmuch more complex distribution.",
    "start": "3180543",
    "end": "3186300"
  },
  {
    "text": "But yeah. I think like models of that\nsize are outside the scope of this class. ",
    "start": "3186300",
    "end": "3196340"
  },
  {
    "text": "All right. So here's another code snippet. So this is one of the\nalgorithms from the book. We call it maximum likelihood\nparameter estimation.",
    "start": "3196340",
    "end": "3204180"
  },
  {
    "text": "It takes in this\nlikelihood model, which is a function that given\nsome input x and some set",
    "start": "3204180",
    "end": "3210299"
  },
  {
    "text": "of parameters, it gives us the\nprobability distribution over y. And then we have some\noptimization algorithm",
    "start": "3210300",
    "end": "3218040"
  },
  {
    "text": "or optimizer. There's various\ntools in Julia you can use that kind of\nhave off the shelf",
    "start": "3218040",
    "end": "3224540"
  },
  {
    "text": "optimization algorithms. You don't need to worry too much\nabout how to implement those. And then we have\nthis function that we",
    "start": "3224540",
    "end": "3230690"
  },
  {
    "text": "call fit that takes in our\nalgorithm and some set of data. We set our objective to be\nthe sum of the negative log",
    "start": "3230690",
    "end": "3240170"
  },
  {
    "text": "likelihood of all the data. The reason that it's negative\nis because oftentimes, our optimizers minimize.",
    "start": "3240170",
    "end": "3245510"
  },
  {
    "text": "They don't maximize. So we want to maximize\nthe log likelihood, which means we want to minimize\nthe negative log likelihood.",
    "start": "3245510",
    "end": "3252420"
  },
  {
    "text": "But all this is doing is\ncalculating the likelihood of each of our data points\ngiven some value of theta,",
    "start": "3252420",
    "end": "3257880"
  },
  {
    "text": "the value of theta that\ngets input to our objective. Summing all of these\nlog probabilities,",
    "start": "3257880",
    "end": "3263610"
  },
  {
    "text": "we give this as the objective\nfunction to the optimizer, and we say find\nus the best theta for this particular objective.",
    "start": "3263610",
    "end": "3269930"
  },
  {
    "text": "And that allows us to\npick these parameters. So let me show you\nwhat this looks like.",
    "start": "3269930",
    "end": "3278300"
  },
  {
    "text": "So here is essentially\nimplementing this for the problem\nwe were looking",
    "start": "3278300",
    "end": "3283520"
  },
  {
    "text": "at before to find that theta\n1, theta 2 and theta 3. So we're saying here\nthat the likelihood",
    "start": "3283520",
    "end": "3291589"
  },
  {
    "text": "is a normal distribution,\nwhere we take our theta 1. We multiply it by the input x.",
    "start": "3291590",
    "end": "3297800"
  },
  {
    "text": "We add on theta 2. So this is the\nlinear function of x. And then theta 3 describes\nthe standard deviation.",
    "start": "3297800",
    "end": "3305017"
  },
  {
    "text": "You don't need to worry too much\nabout this exponential here. It's just to make\nsure that we always get positive values for\nthe standard deviation.",
    "start": "3305018",
    "end": "3311430"
  },
  {
    "text": "But you can ignore that\njust for your understanding. Then we define this optimizer.",
    "start": "3311430",
    "end": "3317400"
  },
  {
    "text": "We're using a Julia package\ncalled Optim, which implements lots of different optimizers. The specific optimizer we'll\nuse is gradient descent.",
    "start": "3317400",
    "end": "3325309"
  },
  {
    "text": "Again, you don't need to worry\ntoo much about the syntax here. But essentially,\nwhat's going on is we're just defining\nthis likelihood",
    "start": "3325310",
    "end": "3331478"
  },
  {
    "text": "function for our model\nclass, and then we're picking an optimizer. We then feed this\nto our algorithm.",
    "start": "3331478",
    "end": "3338108"
  },
  {
    "text": "So we create an algorithm\nwith that likelihood function and optimizer. We call fit, and then I'm\nprinting out the results.",
    "start": "3338108",
    "end": "3345730"
  },
  {
    "text": "So if you see here what's going\non is what we ended up printing out is we said the best model\nwas 0.999x plus 0 and a variance",
    "start": "3345730",
    "end": "3354099"
  },
  {
    "text": "of 0.05 squared. And if we go back and\nlook at these values that we found by just kind of\nhandtuning everything ourselves,",
    "start": "3354100",
    "end": "3362360"
  },
  {
    "text": "we got something very similar. So we got 1 for theta 1, 0 for\ntheta 2, and 0.05 for theta 3.",
    "start": "3362360",
    "end": "3369002"
  },
  {
    "text": "So what this did was just\nkind of automatically do that process for us. And you can see it gets\nthat log likelihood.",
    "start": "3369002",
    "end": "3375140"
  },
  {
    "text": "And that's what the plot looks\nlike with respect to our data.",
    "start": "3375140",
    "end": "3380680"
  },
  {
    "text": "OK. So this optimization problem\nleads to many common loss functions actually.",
    "start": "3380680",
    "end": "3387310"
  },
  {
    "text": "So buckle up because\nI'm about to show you my favorite derivation ever.",
    "start": "3387310",
    "end": "3393700"
  },
  {
    "text": "OK. So here's how it works.",
    "start": "3393700",
    "end": "3399760"
  },
  {
    "text": "So let's say that\nwe want to find. ",
    "start": "3399760",
    "end": "3406270"
  },
  {
    "text": "The parameters theta for\na conditional Gaussian",
    "start": "3406270",
    "end": "3419440"
  },
  {
    "text": "with constant variance. ",
    "start": "3419440",
    "end": "3426520"
  },
  {
    "text": "And that's it. OK. With constant variance. ",
    "start": "3426520",
    "end": "3434380"
  },
  {
    "text": "So we're going to say\nwe have this model that is going to be Pui I given Xi.",
    "start": "3434380",
    "end": "3442100"
  },
  {
    "text": "So this is our data point, is\nequal to a normal distribution",
    "start": "3442100",
    "end": "3447880"
  },
  {
    "text": "with over Yi, where our\nmean is some function of Xi",
    "start": "3447880",
    "end": "3455589"
  },
  {
    "text": "and our variance is\nsome constant value. And we're going to assume\nwe have this data set",
    "start": "3455590",
    "end": "3463980"
  },
  {
    "text": "of a bunch of XY pairs. So we have X1 and Y1, X2-Y2,\nand so on, all the way to Xm-Ym.",
    "start": "3463980",
    "end": "3473790"
  },
  {
    "text": " And then we want to find the\nmaximum likelihood estimate",
    "start": "3473790",
    "end": "3481049"
  },
  {
    "text": "for this type of model class. So we'll say theta hat is\nequal to the argmax of theta",
    "start": "3481050",
    "end": "3489930"
  },
  {
    "text": "of the sum over all\nof our different data points of the log\nprobability of them.",
    "start": "3489930",
    "end": "3498390"
  },
  {
    "text": "So that's just coming from\nthe equation that's up here. And then we'll say\nwe just defined",
    "start": "3498390",
    "end": "3507030"
  },
  {
    "text": "that probability to be\na normal distribution, a conditional Gaussian.",
    "start": "3507030",
    "end": "3513000"
  },
  {
    "text": "So that's going to be the log. We're just going to\nplug in for this now. So it'll be the log of this\nnormal distribution, which",
    "start": "3513000",
    "end": "3520380"
  },
  {
    "text": "has mean as some function of x\nand a variance sigma squared.",
    "start": "3520380",
    "end": "3525845"
  },
  {
    "text": " So now, I'm just going\nto plug-in the equation",
    "start": "3525845",
    "end": "3531660"
  },
  {
    "text": "for a normal distribution. So that allows us to say\nwe have argmax theta log.",
    "start": "3531660",
    "end": "3542190"
  },
  {
    "text": "So this is just going\nto be the equation for a normal distribution. So it's 1 over root\n2 pi sigma squared",
    "start": "3542190",
    "end": "3549300"
  },
  {
    "text": "times the exponential of\nnegative Yi minus f theta Xi,",
    "start": "3549300",
    "end": "3558720"
  },
  {
    "text": "squared over 2 sigma squared. ",
    "start": "3558720",
    "end": "3564690"
  },
  {
    "text": "OK. I'm going to come\nup over here now. So then I'm just going to now\ntake the log of this formula",
    "start": "3564690",
    "end": "3572580"
  },
  {
    "text": "to simplify a few things. So that's going to be the log\nof this times the log of this.",
    "start": "3572580",
    "end": "3577950"
  },
  {
    "text": "So we're going to\nget argmax theta. ",
    "start": "3577950",
    "end": "3587180"
  },
  {
    "text": "Log 1 over root 2\npi sigma squared. Minus so when I take\nthe log of the exponent,",
    "start": "3587180",
    "end": "3597470"
  },
  {
    "text": "it just cancels those two out. So I just get minus Yi\nminus f theta Xi squared,",
    "start": "3597470",
    "end": "3606230"
  },
  {
    "text": "divided by 2 sigma squared. And now, we'll think about\nwhat we're doing here.",
    "start": "3606230",
    "end": "3611820"
  },
  {
    "text": "So we're solving this\nmaximization problem over theta. So anything that doesn't\nhave a theta in it",
    "start": "3611820",
    "end": "3617732"
  },
  {
    "text": "we don't actually\nreally care about because it's not going to affect\nthe value of this objective. So we can basically just\ncross out this whole term",
    "start": "3617732",
    "end": "3624380"
  },
  {
    "text": "because it has no theta in it. And we also don't really\ncare about this denominator because it also\ndoesn't involve theta.",
    "start": "3624380",
    "end": "3631040"
  },
  {
    "text": "So then we can rewrite this\nwith those things gone,",
    "start": "3631040",
    "end": "3637330"
  },
  {
    "text": "and we'll get sum\ni equals 1 to m",
    "start": "3637330",
    "end": "3643900"
  },
  {
    "text": "of negative Yi minus\nf theta Xi squared.",
    "start": "3643900",
    "end": "3650079"
  },
  {
    "text": "And it turns out the argmax of\nthe negative of this function is going to be the same as\nthe argmin of the inverse",
    "start": "3650080",
    "end": "3657039"
  },
  {
    "text": "or the converse of that. So we'll say this is\nactually the argmin of theta",
    "start": "3657040",
    "end": "3662260"
  },
  {
    "text": "of the sum from i equals 1 to\nm, Yi minus f theta Xi squared.",
    "start": "3662260",
    "end": "3669540"
  },
  {
    "text": "OK. So maybe this looks\nfamiliar to some of you. If it doesn't, I'll just tell\nyou what it is right now.",
    "start": "3669540",
    "end": "3676450"
  },
  {
    "text": "This is just the least\nsquares objective. ",
    "start": "3676450",
    "end": "3684609"
  },
  {
    "text": "Pretty cool. So I just naively was applying\nleast squares my whole life",
    "start": "3684610",
    "end": "3689680"
  },
  {
    "text": "until I saw this. And it turns out when you're\napplying these squares, what you're doing is you're actually\nassuming that the data comes",
    "start": "3689680",
    "end": "3695750"
  },
  {
    "text": "from a conditional Gaussian\nmodel where your inputs, so",
    "start": "3695750",
    "end": "3701050"
  },
  {
    "text": "your Xs, essentially determine\nyour Ys by passing them through some function and\nadding some Gaussian noise",
    "start": "3701050",
    "end": "3707110"
  },
  {
    "text": "with constant variance to it. Kind of blew my mind\nwhen I saw this.",
    "start": "3707110",
    "end": "3714040"
  },
  {
    "text": "Any questions on this? Yeah.",
    "start": "3714040",
    "end": "3720490"
  },
  {
    "text": "Actually, you know what? I anticipated this\ngetting messy. So I copied the\none from the book.",
    "start": "3720490",
    "end": "3727100"
  },
  {
    "text": "So here's a much\ncleaner version of this. But I will post this with\nmy annotations later.",
    "start": "3727100",
    "end": "3735160"
  },
  {
    "text": "Any other questions? Yeah. ",
    "start": "3735160",
    "end": "3742180"
  },
  {
    "text": "Like the part about what\nobviously [INAUDIBLE]",
    "start": "3742180",
    "end": "3748150"
  },
  {
    "text": "assumptions that we're making. I want to make sure. Yeah, sure. So let's say that we have a\nwhole bunch of data points,",
    "start": "3748150",
    "end": "3753450"
  },
  {
    "text": "and they're XY pairs. So we have some input-output\npairs basically. And what we want\nto do is come up",
    "start": "3753450",
    "end": "3759390"
  },
  {
    "text": "with a model that predicts\nY given our input X. And so",
    "start": "3759390",
    "end": "3765269"
  },
  {
    "text": "what we're going to\nassume is that to generate that data-- so we\ndon't actually know how that data was generated. It was just given to us.",
    "start": "3765270",
    "end": "3771580"
  },
  {
    "text": "But we're going to assume\nthat that data was generated by taking X, applying some\nfunction to it, parameterized",
    "start": "3771580",
    "end": "3778319"
  },
  {
    "text": "by some values theta, and\nthen adding some noise to it because the world is kind\nof just noisy or something, or we're missing\nsomething in our function.",
    "start": "3778320",
    "end": "3785610"
  },
  {
    "text": "And that's how we\ngot the Y value. And then what we're\ntrying to do is",
    "start": "3785610",
    "end": "3790680"
  },
  {
    "text": "it turns out that if we do\nthis least squares objective, that's the assumption\nthat we're making. We're assuming that the data\nwas generated in that way.",
    "start": "3790680",
    "end": "3798970"
  },
  {
    "text": "Yeah. And so is the dual also true\nthat if we don't know something about the structure of the\ndata, we should not be using",
    "start": "3798970",
    "end": "3807480"
  },
  {
    "text": "[INAUDIBLE] because it's like\nbad to assume that Gaussian quotient property when it isn't?",
    "start": "3807480",
    "end": "3812589"
  },
  {
    "text": "Yeah. If you know that\nit's not Gaussian, you might do better with\na different objective. And you actually\ncan find, there's",
    "start": "3812590",
    "end": "3819312"
  },
  {
    "text": "lots of distributions\nwhere you can go through this similar\nprocess, and you'll get different objective\nfunctions, which is pretty cool.",
    "start": "3819312",
    "end": "3825023"
  },
  {
    "start": "3825023",
    "end": "3830970"
  },
  {
    "text": "Any other questions?  [INAUDIBLE]",
    "start": "3830970",
    "end": "3839099"
  },
  {
    "text": "There's this thing called\nthe exponential family. I recommend looking that up.",
    "start": "3839100",
    "end": "3844740"
  },
  {
    "text": "So sometimes, it turns out, so\nwe drive to this least squares",
    "start": "3844740",
    "end": "3850320"
  },
  {
    "text": "objective. It turns out sometimes, we can\nkeep going-- because we still need to optimize this. So a lot of people will optimize\nit with gradient descent.",
    "start": "3850320",
    "end": "3856960"
  },
  {
    "text": "A lot of neural networks use\nthis as a function to optimize.",
    "start": "3856960",
    "end": "3862238"
  },
  {
    "text": "But sometimes, we could\nactually keep going and get an analytical solution. I'm not going to show you\nhow to do that right now,",
    "start": "3862238",
    "end": "3867880"
  },
  {
    "text": "but if you want extra\npractice, you can check out example 2.9 in the textbook. It turns out it just requires\nthat this f theta of x",
    "start": "3867880",
    "end": "3875930"
  },
  {
    "text": "is a linear function. So you can check that\nout in the textbook.",
    "start": "3875930",
    "end": "3882170"
  },
  {
    "text": "It turns out there's\nalso ways to derive analytical solutions for maybe\nnonconditional distributions.",
    "start": "3882170",
    "end": "3888720"
  },
  {
    "text": "So standard distributions\nlike a Gaussian distribution, which you can see in example 2.8\nof the textbook and a Bernoulli",
    "start": "3888720",
    "end": "3895760"
  },
  {
    "text": "distribution, which you\ncan see in example 2.7. So if you want extra practice\non all of these concepts, I would definitely recommend\nchecking out those examples.",
    "start": "3895760",
    "end": "3903030"
  },
  {
    "text": "Yeah. What did we just do? Because what we did was\na [INAUDIBLE] Gaussian. Yeah. Good question.",
    "start": "3903030",
    "end": "3908339"
  },
  {
    "text": "We assumed we had XY pairs. So we had an X and a Y output. We were modeling things\nas a conditional Gaussian",
    "start": "3908340",
    "end": "3915619"
  },
  {
    "text": "distribution. Example 2.8 just shows like\nif you have a bunch of Xs, there's no labels.",
    "start": "3915620",
    "end": "3920839"
  },
  {
    "text": "How do you fit a\nGaussian distribution? Yeah. There's nothing super blowing. You basically find out that\nthe mean of the Gaussian",
    "start": "3920840",
    "end": "3926630"
  },
  {
    "text": "distribution is just the\nmean of your samples, and the standard deviation\nis calculated similarly.",
    "start": "3926630",
    "end": "3933650"
  },
  {
    "text": "All right. So that's maximum likelihood\nparameter learning. So there is another\noption, and that's",
    "start": "3933650",
    "end": "3941780"
  },
  {
    "text": "Bayesian parameter learning. So I really like Bayesian\nparameter learning. And let me explain why.",
    "start": "3941780",
    "end": "3947850"
  },
  {
    "text": "So people who know\nme know that I'm like an insanely\nindecisive person.",
    "start": "3947850",
    "end": "3953450"
  },
  {
    "text": "Even though I work in a lab\nthat focuses on decision making under uncertainty, if I have\nto make a decision for myself,",
    "start": "3953450",
    "end": "3959000"
  },
  {
    "text": "it just doesn't work. Bayesian parameter\nlearning is just perfect for indecisive\npeople like me",
    "start": "3959000",
    "end": "3964830"
  },
  {
    "text": "because why pick just\none set of parameters when you could just\nmaintain a distribution over all possible parameters?",
    "start": "3964830",
    "end": "3971549"
  },
  {
    "text": "So that's what we do in\nBayesian parameter learning. So we have our\nparameter theta, and we",
    "start": "3971550",
    "end": "3977180"
  },
  {
    "text": "want to maintain a distribution\ngiven our data over what we think those likely values\nfor the parameter theta is.",
    "start": "3977180",
    "end": "3986090"
  },
  {
    "text": "So what we want to calculate\nis this P theta given D. So we're saying we\nhave this data set.",
    "start": "3986090",
    "end": "3992990"
  },
  {
    "text": "What is the probability of\nsome particular value of theta? And we don't know how to\ndirectly compute this.",
    "start": "3992990",
    "end": "4000970"
  },
  {
    "text": "So just kind of\nby looking at it, we can't actually just\nderive an equation for this.",
    "start": "4000970",
    "end": "4006329"
  },
  {
    "text": "But what we do know is how\nto compute PD given theta. And so we actually\ndid this earlier",
    "start": "4006330",
    "end": "4014370"
  },
  {
    "text": "when we were doing the maximum\nlikelihood parameter learning. So if you remember,\nwe assumed we had this data point or\nthis data set of a bunch",
    "start": "4014370",
    "end": "4021540"
  },
  {
    "text": "of different observations. And then we show that if\nwe assume that they're all independent, we could just\ncompute PD given theta",
    "start": "4021540",
    "end": "4027420"
  },
  {
    "text": "as the product of all of\ntheir probabilities given some set of parameters\nor some instantiation",
    "start": "4027420",
    "end": "4033180"
  },
  {
    "text": "of our parameters theta. So it turns out using Bayes'\nrule, we could actually",
    "start": "4033180",
    "end": "4039240"
  },
  {
    "text": "then derive a formula for a\ndistribution of P theta given D.",
    "start": "4039240",
    "end": "4044910"
  },
  {
    "text": "But first, let me just quickly\nderive Bayes' rule for you. So hopefully, many of\nyou have seen it before.",
    "start": "4044910",
    "end": "4051060"
  },
  {
    "text": "But essentially, what\nwe're trying to figure out is this P of y given x. And we have this definition\nof conditional probability",
    "start": "4051060",
    "end": "4059190"
  },
  {
    "text": "that I already\ntalked about earlier. So that's one definition. And then all I'm going to\ndo is just rewrite this,",
    "start": "4059190",
    "end": "4065339"
  },
  {
    "text": "switching x and y. So this is still true. Px given y is Px\nand y divided by Py.",
    "start": "4065340",
    "end": "4071430"
  },
  {
    "text": "And then all I'm going\nto do is multiply, so rearrange these a little bit. So I'm just going to\nmultiply the Px over here",
    "start": "4071430",
    "end": "4076837"
  },
  {
    "text": "and the Py over here. Then you can see we have the\nsame thing on the right side of these equations.",
    "start": "4076837",
    "end": "4082465"
  },
  {
    "text": "So that means we can\nset the two things on the left side of these\nequations equal to each other. So I'll do that.",
    "start": "4082465",
    "end": "4088230"
  },
  {
    "text": "And then all I'm going to do\nis divide both sides by P of x.",
    "start": "4088230",
    "end": "4094325"
  },
  {
    "text": " There we go. And there's Bayes' rule. So it's not super crazy.",
    "start": "4094325",
    "end": "4100928"
  },
  {
    "text": "It literally just comes\nfrom the definition of conditional probability. And a lot of times\nthough, we tend",
    "start": "4100928",
    "end": "4107434"
  },
  {
    "text": "to go one step\nfurther because it can be quite\ndifficult to calculate this just plain P of x.",
    "start": "4107435",
    "end": "4113339"
  },
  {
    "text": "And so we often come over\nhere and apply something called the law of\ntotal probability to rewrite this P of x in\nterms of P of x given y and Py.",
    "start": "4113340",
    "end": "4123778"
  },
  {
    "text": "So that's Bayes' rule. And the way that this\nhelps us in our scenario is you remember we have\nthis P theta given D.",
    "start": "4123779",
    "end": "4131370"
  },
  {
    "text": "And so now, we can\nrewrite it in terms of PD given theta, which\nwe said we actually did know how to calculate.",
    "start": "4131370",
    "end": "4137109"
  },
  {
    "text": "So we have this PD\ngiven theta times P theta, divided\nby PD given theta,",
    "start": "4137109",
    "end": "4143640"
  },
  {
    "text": "the sum over all possible\nvalues of theta of PD given theta times P theta.",
    "start": "4143640",
    "end": "4149790"
  },
  {
    "text": "So we call this Bayesian\nparameter estimation. And let's think about this\nformula a little bit more.",
    "start": "4149790",
    "end": "4155409"
  },
  {
    "text": "So the PD given\ntheta part, that's what we call the\nlikelihood model. So that's what we\nwere optimizing",
    "start": "4155410",
    "end": "4161130"
  },
  {
    "text": "in our maximum likelihood\nparameter learning. Yeah. When you took this out, you\ndid the law of probability.",
    "start": "4161130",
    "end": "4166170"
  },
  {
    "text": "I think maybe the sum\nis the other variable, but you might also just\nbe misremembering here.",
    "start": "4166170",
    "end": "4172939"
  },
  {
    "text": "So we have this\nprobability of x is equal to the summation over x. Oh, yeah. That should be y.",
    "start": "4172939",
    "end": "4177950"
  },
  {
    "text": "Yeah. I'll make that correction\nbefore I post these slides.",
    "start": "4177950",
    "end": "4183359"
  },
  {
    "text": "So yeah. We have this likelihood model. And then we have this thing we\nhaven't really talked about,",
    "start": "4183359",
    "end": "4192778"
  },
  {
    "text": "which is P of theta. So this is in\naddition to the things we use for maximum likelihood\nparameter learning.",
    "start": "4192779",
    "end": "4199080"
  },
  {
    "text": "Now we have this\nP of theta, which is a prior distribution\nover theta. So this gives us what is\nour prior belief over theta",
    "start": "4199080",
    "end": "4206690"
  },
  {
    "text": "before we observe\nany of the data? So for example, if I was\nflipping a coin and someone just handed me a quarter,\nI have a prior belief",
    "start": "4206690",
    "end": "4213800"
  },
  {
    "text": "over what the probability of\nthat quarter coming up heads is. I think it's\nprobably around 50%.",
    "start": "4213800",
    "end": "4219380"
  },
  {
    "text": "So I'd have some prior\ndistribution that assigns high likelihood to 50%.",
    "start": "4219380",
    "end": "4224420"
  },
  {
    "text": "And then I would observe\nsome flip of that coin. I would see how often\nit comes up heads. And then that gives me what we\ncall our posterior distribution",
    "start": "4224420",
    "end": "4231160"
  },
  {
    "text": "over theta, which is\nafter we observe the data, what is our distribution\nover the likelihood of different values of theta?",
    "start": "4231160",
    "end": "4237210"
  },
  {
    "text": " So that's for the discrete case.",
    "start": "4237210",
    "end": "4242360"
  },
  {
    "text": "For the continuous\ncase, essentially, the summation at\nthe bottom here just gets replaced by an integral.",
    "start": "4242360",
    "end": "4250462"
  },
  {
    "text": "But there's still\none problem here, is that we've now\ntransformed it, so that we have everything\nin terms of PD given theta",
    "start": "4250462",
    "end": "4255850"
  },
  {
    "text": "and P theta, which we\nknow how to calculate. But it still can be quite\ndifficult to actually calculate what comes out here.",
    "start": "4255850",
    "end": "4262160"
  },
  {
    "text": "And that's because\nthis summation grows exponentially with\nthe number of parameters",
    "start": "4262160",
    "end": "4267280"
  },
  {
    "text": "that we have. And this integral is\noften quite difficult to compute analytically. Sometimes, it's impossible\nto compute analytically.",
    "start": "4267280",
    "end": "4273455"
  },
  {
    "text": "And so a lot of times,\nwe actually just can't even do this computation. But what's really nice is we\ncan compute this numerator.",
    "start": "4273455",
    "end": "4281510"
  },
  {
    "text": "So it turns out that\nthere's various algorithms that if you can just\ncompute the numerator,",
    "start": "4281510",
    "end": "4286849"
  },
  {
    "text": "you can actually get\nsamples from the posterior distribution that comes\nout using something called",
    "start": "4286850",
    "end": "4293980"
  },
  {
    "text": "probabilistic programming. I think this is probably a\ngood place to stop for today.",
    "start": "4293980",
    "end": "4300530"
  },
  {
    "text": "So I guess I am going to\nhave to leave you in suspense because I had all\nthese frisbees, and you're going to do\nthis really fun thing,",
    "start": "4300530",
    "end": "4305725"
  },
  {
    "text": "but now you're going to have\nto wait till Tuesday for that. So what questions\ndo you guys have",
    "start": "4305725",
    "end": "4310930"
  },
  {
    "text": "for the remaining few minutes?  Just wanna know a\nlittle bit about how",
    "start": "4310930",
    "end": "4315940"
  },
  {
    "text": "we can pull from the continuous\ncase when we don't [INAUDIBLE]. Yeah. That's what I'm\ngoing to leave you",
    "start": "4315940",
    "end": "4321917"
  },
  {
    "text": "in suspense for because that'll\nbe what comes up on Tuesday. The question was if I could\nsay more about how we actually",
    "start": "4321917",
    "end": "4330610"
  },
  {
    "text": "go about drawing samples, if we\ncan only compute the numerator. ",
    "start": "4330610",
    "end": "4339000"
  }
]