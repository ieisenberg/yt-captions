[
  {
    "start": "0",
    "end": "5300"
  },
  {
    "text": "As you may have noticed, I'm\na little bit less innovative in my lecturing them,\nPercy, so you're going to get PowerPoint\nslides rather",
    "start": "5300",
    "end": "12290"
  },
  {
    "text": "than executable\nPython ones, but you should be able to find the\nPDFs on the website as well.",
    "start": "12290",
    "end": "18570"
  },
  {
    "text": "So I've titled this lecture,\n\"Everything You Didn't Want To Know About LM\nArchitecture And Training,\" because we're going to get into\nsome of the nitty-gritty details",
    "start": "18570",
    "end": "26007"
  },
  {
    "text": "that I think most other classes\nwould spare you the details of. Like what should my\nhyperparameters be?",
    "start": "26007",
    "end": "31740"
  },
  {
    "text": "And those kinds of questions. Some minor logistics. Also, if you're doing\nthe assignments,",
    "start": "31740",
    "end": "37590"
  },
  {
    "text": "we are updating assignments\nas we find mostly minor bugs. Make sure you pull updates to\nthe assignments as you go along.",
    "start": "37590",
    "end": "47510"
  },
  {
    "text": "So what we're going\nto do, we're going to start with a quick\nrecap of a transformer.",
    "start": "47510",
    "end": "52559"
  },
  {
    "text": "And I'll give you two variants\nof a standard transformer, one that's probably coming\nfrom the standard transformer",
    "start": "52560",
    "end": "59890"
  },
  {
    "text": "lectures that you\nmight see in 224N. And then I'll talk\nabout what you implement",
    "start": "59890",
    "end": "65049"
  },
  {
    "text": "and the modern consensus\nvariant of a transformer. And then we're going to\ntake a much more data-driven",
    "start": "65050",
    "end": "70750"
  },
  {
    "text": "perspective to understanding\ntransformer architectures. So the question that\nwe're going to ask is people have trained\nlots of LLMs at this point,",
    "start": "70750",
    "end": "78890"
  },
  {
    "text": "and you can go and read\nall of those papers and try to understand\nwhat has changed, what has been in common.",
    "start": "78890",
    "end": "84470"
  },
  {
    "text": "And from that, almost an\nevolutionary analysis, try to understand what are\nthe things that are really",
    "start": "84470",
    "end": "89800"
  },
  {
    "text": "important to make\ntransformers work. So today's theme is--\nthe theme of the class is the best way to learn\nis hands-on experience.",
    "start": "89800",
    "end": "96270"
  },
  {
    "text": "But the theme of this\nlecture, because we can't train all\nthese transformers, is to learn from the\nexperience of others.",
    "start": "96270",
    "end": "103299"
  },
  {
    "text": "So the starting point is\nthe original transformer. So just as a review, hopefully,\nyou all remember this from 224N",
    "start": "103300",
    "end": "111070"
  },
  {
    "text": "or your other NLP classes. You've got some simple position\nembeddings at the bottom.",
    "start": "111070",
    "end": "116450"
  },
  {
    "text": "You've got multi-head attention. You've got LayerNorms\nafterwards. You've got a residual\nstream going upwards.",
    "start": "116450",
    "end": "122409"
  },
  {
    "text": "You've got an MLP, and then\na softmax at the very end. And we're going to see variants\nto all of these different pieces",
    "start": "122410",
    "end": "128550"
  },
  {
    "text": "until we get to basically\nthe most modern variants of the transformer. And the latest one\nI'll talk about",
    "start": "128550",
    "end": "134610"
  },
  {
    "text": "will be just a\nfew months before. So what you implemented is not\nthe vanilla transformer variant",
    "start": "134610",
    "end": "142950"
  },
  {
    "text": "from the original paper. We've modified a few things. We've put the LayerNorm\nin front of the block.",
    "start": "142950",
    "end": "148750"
  },
  {
    "text": "So you can see on\nthis slide over here that there's the\nnorm is over here right before each of these\nblocks in the residual stream.",
    "start": "148750",
    "end": "156570"
  },
  {
    "text": "We've asked you to implement\nrotary position embeddings. The feed-forward layers use\nsomething called a SwiGLU.",
    "start": "156570",
    "end": "163079"
  },
  {
    "text": "And then linear layers\nnow emit these bias terms. And you might ask,\nwhy have you forced",
    "start": "163080",
    "end": "170100"
  },
  {
    "text": "us to implement this weird\nvariant of a transformer instead of the original transformer\nis all you need transformer?",
    "start": "170100",
    "end": "177140"
  },
  {
    "text": "And so we're going to go\nthrough some of those questions. And then yesterday,\nI was thinking, OK, I should catch up on\nall the developments",
    "start": "177140",
    "end": "183140"
  },
  {
    "text": "that have happened\nin architectures over the last year. And Percy warned me about\nthis because he said, you're going to have to\nredo the lecture every year.",
    "start": "183140",
    "end": "189810"
  },
  {
    "text": "And so I started looking. And I was like, all right, yeah,\nthere's a couple of good papers recently. There's Command A,\nthere's 2 OLMo 2 Furious.",
    "start": "189810",
    "end": "195989"
  },
  {
    "text": "There's SmolLM and Phi-4. And then you go looking,\nand you're like, wow, yeah, there's Gemma 3 and\nQwen 2.5 and InternLM.",
    "start": "195990",
    "end": "202910"
  },
  {
    "text": "And then we got more. I can't even cover the\nscreen with these guys. There's a lot of models.",
    "start": "202910",
    "end": "209490"
  },
  {
    "text": "There were about 19 new\ndense model releases in the last year, many of them\nwith minor architecture tweaks.",
    "start": "209490",
    "end": "216205"
  },
  {
    "text": "And on the one\nhand, it's kind of annoying to go through\nall these papers and say, like, what is\nhappening in all of these?",
    "start": "216205",
    "end": "222920"
  },
  {
    "text": "But also, it's like actually\na wealth of information because not all of\nthem do the same thing. And you can see--",
    "start": "222920",
    "end": "228224"
  },
  {
    "text": "not all of you,\nespecially in the back, can see the details\nof this slide. But I put together\na little spreadsheet",
    "start": "228225",
    "end": "234430"
  },
  {
    "text": "of what all these\nmodels are doing. And starting with all\nthe way from 2017, the original transformer,\nall the way to 2025,",
    "start": "234430",
    "end": "241280"
  },
  {
    "text": "what the newest\nmodels are doing. And we'll talk\nabout this as we go. But you see certain kinds\nof architecture changes",
    "start": "241280",
    "end": "248140"
  },
  {
    "text": "being explored. So here, on this column,\nis position embeddings. People used to do\nall sorts of stuff, like absolute,\nrelative, RoPE There was",
    "start": "248140",
    "end": "255459"
  },
  {
    "text": "a alibi phase for some people. But then, now, starting around\n2023, everyone just does RoPE.",
    "start": "255460",
    "end": "261640"
  },
  {
    "text": "So you can see this\nconvergent evolution almost of neural architectures. And we're going\nto talk about all",
    "start": "261640",
    "end": "267460"
  },
  {
    "text": "of these different\nkinds of things. So the parts that\nI'll cover-- so this",
    "start": "267460",
    "end": "273190"
  },
  {
    "text": "is a preview of the three\nmajor sections of this lecture. And if I have time,\nI'm also going to talk about different\nattention variants at the end.",
    "start": "273190",
    "end": "280520"
  },
  {
    "text": "The first thing is going to\nbe architecture variations. That's what I'm\ngoing to talk about. So activations, feed-forwards,\nattention variants,",
    "start": "280520",
    "end": "286910"
  },
  {
    "text": "position embeddings,\nall of those things. And then having nailed down\nthe architecture, what do we",
    "start": "286910",
    "end": "291919"
  },
  {
    "text": "have to do? Well, we have to\npick hyperparameters. How big do we make\nthe hidden dimension?",
    "start": "291920",
    "end": "297460"
  },
  {
    "text": "How big do we make the inner\nprojection layer inside of MLP? What do we do about the\nnumber of dimensions?",
    "start": "297460",
    "end": "303250"
  },
  {
    "text": "How many vocab elements? Those are all important\nthings that you have to choose when you're\nactually training your language",
    "start": "303250",
    "end": "308610"
  },
  {
    "text": "model. And you don't want to just\npick these out of a hat. You want to select them in\nsome fairly intelligent way.",
    "start": "308610",
    "end": "315040"
  },
  {
    "text": "So we're going to start with\narchitecture variations.",
    "start": "315040",
    "end": "320250"
  },
  {
    "text": "And the two things that\nI'll mention right here-- and I'll go back\nto them as I talk--",
    "start": "320250",
    "end": "326320"
  },
  {
    "text": "the first one is, there's\nnot that much consensus in a lot of the choices. There's been convergent\nevolution in the last few years,",
    "start": "326320",
    "end": "334630"
  },
  {
    "text": "what I'll call LLaMA-like\narchitectures at the very bottom here. But people do all\nsorts of things. They swap between\nLayerNorm and RMS Norm.",
    "start": "334630",
    "end": "340930"
  },
  {
    "text": "They do serial versus\nparallel layers. There's one choice\nthat basically everyone does since the very first GPT.",
    "start": "340930",
    "end": "347880"
  },
  {
    "text": "And I'll talk about\nthat in a bit. But there's lots of\ndifferent variations that we can learn from here.",
    "start": "347880",
    "end": "354770"
  },
  {
    "text": "The big one, I've already\ntalked about this guy in 224N. So if you remember that lecture,\nthis will be review for you",
    "start": "354770",
    "end": "361040"
  },
  {
    "text": "rather than being totally new. I think the one thing basically\neveryone agrees on and agreed",
    "start": "361040",
    "end": "366350"
  },
  {
    "text": "on almost from the\nvery start, is the use of pre-norm versus post-norm.",
    "start": "366350",
    "end": "372170"
  },
  {
    "text": "That terminology will get a\nlittle bit more confusing. But the original\ntransformer paper did this thing on\nthe left over here",
    "start": "372170",
    "end": "379460"
  },
  {
    "text": "where you had your residual\nstream in the gray. And in addition to\nthe residual stream,",
    "start": "379460",
    "end": "384889"
  },
  {
    "text": "you had these LayerNorms\nafter every subcomponent. So you would do your\nmulti-head attention. You would add back to\nthe residual stream.",
    "start": "384890",
    "end": "391319"
  },
  {
    "text": "And then you would\nLayerNorm that. And then you would do the same\nthing with your fully connected layer. And then you would\nLayerNorm that.",
    "start": "391320",
    "end": "396966"
  },
  {
    "text": "And very, very early\non, people realized that moving this\nLayerNorm to the front",
    "start": "396967",
    "end": "403280"
  },
  {
    "text": "of this non-residual part,\nso this block on the right did much better in\nmany different ways.",
    "start": "403280",
    "end": "409510"
  },
  {
    "text": "And basically, almost\nall modern LMs that I use this kind of pre-norm.",
    "start": "409510",
    "end": "415590"
  },
  {
    "text": "There have been\nsome new innovations recently that I'll\ntouch on in two slides. But lots of models\nhave moved to this.",
    "start": "415590",
    "end": "423580"
  },
  {
    "text": "The one exception is\nOPT350M, which I'm guessing, they messed that one up.",
    "start": "423580",
    "end": "429160"
  },
  {
    "text": "And that was orphaned\nwhen they were training. That was a fun find in my\nsurvey of architectures.",
    "start": "429160",
    "end": "435960"
  },
  {
    "text": "So this pre versus\npost-norm thing, if you look into why it\nwas originally developed, the arguments were that if you\nwanted to use this post norm",
    "start": "435960",
    "end": "444840"
  },
  {
    "text": "stuff, it was much less stable. And so you would have to do some\ncareful learning rate warm up style things to make it\ntrain in a stable way.",
    "start": "444840",
    "end": "452759"
  },
  {
    "text": "And so if you look at some\nof the earlier papers arguing for this pre-norm approach,\nSalazar and Ngyuen and also",
    "start": "452760",
    "end": "459990"
  },
  {
    "text": "this Xiong 2020 paper,\nyou almost always see this comparison of,\nhey, if we use pre-norm",
    "start": "459990",
    "end": "466400"
  },
  {
    "text": "and we do some other\nstability-inducing tricks, then we can remove\nwarmup and these systems work just as well,\nif not better,",
    "start": "466400",
    "end": "473390"
  },
  {
    "text": "than the post-normal\nLayerNorm with careful warmup-type approaches.",
    "start": "473390",
    "end": "478980"
  },
  {
    "text": "And you see this in a machine\ntranslation setting here. You see this as well on the\nright on various other tasks,",
    "start": "478980",
    "end": "487110"
  },
  {
    "text": "especially using BERT which\nwas trained with post-norm. So there were many arguments\nabout why this was helpful.",
    "start": "487110",
    "end": "495462"
  },
  {
    "text": "There were arguments\nabout gradient attenuation across layers. Like if you do pre-norm,\nthen the gradient sizes",
    "start": "495462",
    "end": "501110"
  },
  {
    "text": "will remain constant. Whereas if you did\npost-norm without warm-up, then it would blow up\nin this orange way.",
    "start": "501110",
    "end": "507900"
  },
  {
    "text": "It's a reasonable argument. But I think a maybe more\ncloser to modern intuition would be this argument that\npre-norm is just a more",
    "start": "507900",
    "end": "515299"
  },
  {
    "text": "stable architecture to train. And so some of the earlier\nwork by Salazar and Ngyuen identified all these loss\nspikes that if you were training",
    "start": "515299",
    "end": "523719"
  },
  {
    "text": "with pre-norm, in blue here, you\nwould see a lot more loss spikes and the training would be\nkind of unstable as you",
    "start": "523720",
    "end": "531067"
  },
  {
    "text": "were training. So you see the\ngradient norm here is spiking and generally higher\nthan the one with pre-norm.",
    "start": "531067",
    "end": "537639"
  },
  {
    "text": "And so, today, you see pre-norm\nand other LayerNorm tricks being used essentially as\na stability-inducing aids",
    "start": "537640",
    "end": "547510"
  },
  {
    "text": "for using training\nlarge neural networks. And so this brings us to one\nnew fairly recent innovation.",
    "start": "547510",
    "end": "556467"
  },
  {
    "text": "I think this didn't\nexist when I gave this lecture last year, which is\nthis variant that I don't think",
    "start": "556468",
    "end": "562450"
  },
  {
    "text": "really has a great\nname, but I'm just going to call it the double\nnorm for the moment here. So this is the original\nfigure that I showed you",
    "start": "562450",
    "end": "569260"
  },
  {
    "text": "at the very beginning. And we know that\nputting LayerNorms in the residual stream is bad. But actually, someone in two\n224N this year asked, well,",
    "start": "569260",
    "end": "577400"
  },
  {
    "text": "but why do you have to put\nthe LayerNorm in the front? Why can't you put it after\nthe feed-forward network?",
    "start": "577400",
    "end": "582460"
  },
  {
    "text": "And of course, you can. And not only that, recent\npeople have gone around and just",
    "start": "582460",
    "end": "588000"
  },
  {
    "text": "add the LayerNorm after\nthe blocks as well. And so Grok and Gemma 2 both\ntake this approach of LayerNorms",
    "start": "588000",
    "end": "595410"
  },
  {
    "text": "both in front and after. Olmo 2 two does\nonly the LayerNorm",
    "start": "595410",
    "end": "600660"
  },
  {
    "text": "after the feed-forward in\nthe multi-head attention. And so this is actually\nan interesting change.",
    "start": "600660",
    "end": "605910"
  },
  {
    "text": "Pre-norm has just been dominant\nand the only thing for a while. But things have been\nchanged up a little bit.",
    "start": "605910",
    "end": "611830"
  },
  {
    "text": "So now, there's a new variant. And there's been\nsome evaluations of this kind of approach.",
    "start": "611830",
    "end": "617649"
  },
  {
    "text": "People have argued it's a\nlittle bit more stable and nicer to train on these larger models.",
    "start": "617650",
    "end": "623399"
  },
  {
    "text": "By the way, feel free to stop\nme and ask me questions as well. I have a tendency to keep\ngoing if no one stops me.",
    "start": "623400",
    "end": "630160"
  },
  {
    "text": "So, yes? Why is LayerNorm in\nthe residual bad? Why is LayerNorm in\nthe residual bad?",
    "start": "630160",
    "end": "635770"
  },
  {
    "text": "That's a good question. I don't think I\ncan give you a this is the proof of why it's bad.",
    "start": "635770",
    "end": "641340"
  },
  {
    "text": "I think one intuitive argument\nfor why this might be bad is that the residual gives you\nthis identity connection all",
    "start": "641340",
    "end": "647389"
  },
  {
    "text": "the way from almost\nthe top of the network, all the way to the bottom. And so if you're trying to\ntrain really deep networks,",
    "start": "647390",
    "end": "652800"
  },
  {
    "text": "this makes gradient\npropagation very easy. So there's lots of\narguments about how LSTMs and these other\nkinds of state space models",
    "start": "652800",
    "end": "660260"
  },
  {
    "text": "have difficulty propagating\ngradients backwards. An identity connection does\nnot have any such problems. And so putting\nLayerNorms in the middle",
    "start": "660260",
    "end": "666950"
  },
  {
    "text": "might mess with that kind\nof gradient behavior. And that of course,\nyou see back here. This is exactly the\nkind of plot you expect",
    "start": "666950",
    "end": "673340"
  },
  {
    "text": "to see if that's happening. Cool.",
    "start": "673340",
    "end": "679550"
  },
  {
    "text": "The other thing\nthat people now do is in the original transformer,\npeople did LayerNorm.",
    "start": "679550",
    "end": "687030"
  },
  {
    "text": "And so LayerNorm is\nthis equation over here. What you do is you have the\nactivations x coming in.",
    "start": "687030",
    "end": "693300"
  },
  {
    "text": "You subtract the empirical mean. So that's the average\nof the x's up top. And then you divide by the\nstandard or the variance",
    "start": "693300",
    "end": "700870"
  },
  {
    "text": "plus a little fudge\nfactor epsilon. And then you square root that. So that you can roughly think\nof it as a standard deviation.",
    "start": "700870",
    "end": "706520"
  },
  {
    "text": "So that's going to standardize\nyour activations x. You're going to scale\nit up by a gamma.",
    "start": "706520",
    "end": "711910"
  },
  {
    "text": "That's a learnable parameter. And then shift it by a beta. So this makes sense. You're going to normalize\nyour activations.",
    "start": "711910",
    "end": "717852"
  },
  {
    "text": "And then you're going\nto shift them around to whatever point you want. And many models use\nthis LayerNorm thing.",
    "start": "717852",
    "end": "723350"
  },
  {
    "text": "And it worked quite well. But many models have now\nmoved on to RMS Norm. And this is one of\nthe consensus changes.",
    "start": "723350",
    "end": "729800"
  },
  {
    "text": "Basically, all the models have\nswitched to using RMS Norm. And now, what do you do, You? Just drop the mean adjustment.",
    "start": "729800",
    "end": "736727"
  },
  {
    "text": "So you don't subtract the mean. You don't add a bias term. And many notable models do this. The LLaMA-family PaLM,\nChinchilla, T5's,",
    "start": "736728",
    "end": "743149"
  },
  {
    "text": "they've all moved to RMS Norm. And what's the reason for this? One reason is that it doesn't\nreally make a difference.",
    "start": "743150",
    "end": "750830"
  },
  {
    "text": "It turns out if you train\nmodels with RMS Norm does just as well as\ntraining with LayerNorm.",
    "start": "750830",
    "end": "755950"
  },
  {
    "text": "And so there's a\nsimplification argument. But really, I think\nthe argument that's often given in\nthese papers, and I",
    "start": "755950",
    "end": "763470"
  },
  {
    "text": "think it's good to appreciate\nthe details of this argument, is that going to RMS Norm is--\nit's faster and just as good.",
    "start": "763470",
    "end": "771880"
  },
  {
    "text": "So in what way is it faster? Well, if I don't subtract the\nmean, it's fewer operations. If I don't have to add\nthat bias term beta back,",
    "start": "771880",
    "end": "779500"
  },
  {
    "text": "it's fewer parameters that I\nhave to load from memory back into my compute units. So I don't have to\nretrieve this state.",
    "start": "779500",
    "end": "787889"
  },
  {
    "text": "And some of you might\nbe thinking, but wait, you told me in 224N that nothing\nbut matrix multiplies matter",
    "start": "787890",
    "end": "793530"
  },
  {
    "text": "for the purpose of runtime. And this is not a\nmatrix multiply. And so I shouldn't\ncare about any of this.",
    "start": "793530",
    "end": "799120"
  },
  {
    "text": "And that's a reasonable\nperspective to take. If you think about the number\nof the percentage of flops that",
    "start": "799120",
    "end": "805500"
  },
  {
    "text": "is taken up by\ndifferent operations in a transformer,\nthis table, there's a nice paper by\nIvanov et al 2023.",
    "start": "805500",
    "end": "814400"
  },
  {
    "text": "I think the title is Memory\nMovement Is All You Need or something that does profiling\nof all the different components",
    "start": "814400",
    "end": "820490"
  },
  {
    "text": "of a transformer. And you see that\ntensor contractions, which are like matrix\nmultiplies, that's",
    "start": "820490",
    "end": "825527"
  },
  {
    "text": "like 99.8% of the flops that\nhappen in a transformer. And so saving\n0.17% of your flops",
    "start": "825527",
    "end": "832430"
  },
  {
    "text": "doesn't seem like a huge win. But I think one of the\nthings that's important",
    "start": "832430",
    "end": "837950"
  },
  {
    "text": "for architecture design now is\nto not just think about flops. Because flops are important,\nbut that's not the only resource",
    "start": "837950",
    "end": "845720"
  },
  {
    "text": "that you have to think about. It's also that you have to think\ncarefully about memory movement.",
    "start": "845720",
    "end": "852470"
  },
  {
    "text": "And so even though\ntensor contraction, so this is things like\nmatrix multiplies. That's like 99.8% of the flops.",
    "start": "852470",
    "end": "859980"
  },
  {
    "text": "If you have things like the\nsoftmax operation or LayerNorms, all these normalization\noperations that",
    "start": "859980",
    "end": "865850"
  },
  {
    "text": "happen in a transformer,\nthere are 0.17% of the flops-- actually, they're\n25% of the runtime.",
    "start": "865850",
    "end": "873500"
  },
  {
    "text": "And a big reason for that is\nbecause these normalization operations still incur a lot\nof memory movement overhead.",
    "start": "873500",
    "end": "880430"
  },
  {
    "text": "And so it does\nactually matter to try to optimize some of\nthese lower level things",
    "start": "880430",
    "end": "886040"
  },
  {
    "text": "because it's not\njust about flops, it's also about memory movement. I'm going to emphasize\nthis quite a bit more as I",
    "start": "886040",
    "end": "891790"
  },
  {
    "text": "get into the systems lecture. When we talk about\nGPU architectures, it's going to become\nvery, very, very important to think about\nmemory, not just about flops.",
    "start": "891790",
    "end": "899990"
  },
  {
    "text": "And so this is one of the\nreasons why RMS Norm has now become much more popular.",
    "start": "899990",
    "end": "907430"
  },
  {
    "text": "And so I went back and looked\nat some of the earlier RMS Norm papers. I think the sad thing is\nthat there aren't quite",
    "start": "907430",
    "end": "915130"
  },
  {
    "text": "as many papers published\nby industry labs with big, nice ablations. And so many of the\nablations that I'll show you",
    "start": "915130",
    "end": "921550"
  },
  {
    "text": "are going to be from a\ncouple of years back. But Narang et al in 2020\nhad this very nice ablation",
    "start": "921550",
    "end": "927340"
  },
  {
    "text": "showing-- here's the\nvanilla transformer, here's the RMS Norm version. And you see the exact\nthing I told you.",
    "start": "927340",
    "end": "933149"
  },
  {
    "text": "The number of steps per second\nthat you can do in a vanilla transformer, 3.5 per second. With RMS Norm, you get 3.68.",
    "start": "933150",
    "end": "939660"
  },
  {
    "text": "Not a huge gain, but that's,\nin some sense, for free. And you get a final loss\nthat's lower than the vanilla",
    "start": "939660",
    "end": "947340"
  },
  {
    "text": "transformer. So that's great. In some sense, we've\ngotten runtime improvements and we've gotten\nin fact, at least",
    "start": "947340",
    "end": "953580"
  },
  {
    "text": "in this case, loss improvements. And so that's a win-win for us.",
    "start": "953580",
    "end": "959670"
  },
  {
    "text": "The final thing that I'll say,\nwhich is very much in line with this RMS Norm\nthing in terms of theme,",
    "start": "959670",
    "end": "966250"
  },
  {
    "text": "is that most modern transformers\ndo not have bias terms. So the original transformer,\nif you look at the FFN,",
    "start": "966250",
    "end": "974076"
  },
  {
    "text": "will look something like this. You have your inputs x. You're going to do a linear\nlayer with a bias term. And then you ReLU it.",
    "start": "974076",
    "end": "979600"
  },
  {
    "text": "And then you'll have a second\nlinear layer wrapping around it. But most implementations--\nif they're not gated units,",
    "start": "979600",
    "end": "986852"
  },
  {
    "text": "which I'll talk\nabout in a moment-- look actually\nsomething like this. They just drop the bias terms. And you can just\nmake this argument",
    "start": "986852",
    "end": "992980"
  },
  {
    "text": "from basically the same kinds\nof underlying principles. They perform just as well. Matrix multiplies are\napparently all that you",
    "start": "992980",
    "end": "1000540"
  },
  {
    "text": "need to get these guys to work. And the other thing,\nwhich is maybe more subtle is actually\noptimization stability.",
    "start": "1000540",
    "end": "1008459"
  },
  {
    "text": "I don't quite have the deepest\nunderstanding of why the bias terms are particularly\nbad for stability,",
    "start": "1008460",
    "end": "1014190"
  },
  {
    "text": "but there's been really\nclear empirical observations that people have made that\nbasically dropping these bias",
    "start": "1014190",
    "end": "1019800"
  },
  {
    "text": "terms often stabilizes the\ntraining of these largest neural networks. And so now, a lot of\nthe implementations",
    "start": "1019800",
    "end": "1025839"
  },
  {
    "text": "now omit bias terms entirely and\ntrain only on these pure matrix multiply kind of settings.",
    "start": "1025839",
    "end": "1032250"
  },
  {
    "text": "So that's the LayerNorm bit. And so there is two things\nthat you should think of.",
    "start": "1032250",
    "end": "1039880"
  },
  {
    "text": "This is nice because the\nstory is pretty clear. Everyone does something, and\nso you should just know this.",
    "start": "1039880",
    "end": "1045130"
  },
  {
    "text": "Basically, everyone\ndoes pre-norm. Or at least, they do\nthe LayerNorms outside of the residual stream. That's kind of the iron rule.",
    "start": "1045130",
    "end": "1052640"
  },
  {
    "text": "You get nicer\ngradient propagation. You get much more\nstable training. It just doesn't make sense\nto do it the other way.",
    "start": "1052640",
    "end": "1058970"
  },
  {
    "text": "Most people or almost\neverybody does RMS Norm.",
    "start": "1058970",
    "end": "1064549"
  },
  {
    "text": "In practice, it works almost\nas well, has fewer parameters to move around. And this idea of dropping bias\nterms just broadly applies.",
    "start": "1064550",
    "end": "1071460"
  },
  {
    "text": "A lot of these models just don't\nhave bias terms in most places. I think the one exception\nto this RMS Norm one,",
    "start": "1071460",
    "end": "1077550"
  },
  {
    "text": "as I was reading\nyesterday, is, I think, Cohere both Command A\nand R+ use LayerNorm. Not quite sure why.",
    "start": "1077550",
    "end": "1085460"
  },
  {
    "text": "Any questions on the LayerNorm,\nRMS Norm and bias terms",
    "start": "1085460",
    "end": "1091490"
  },
  {
    "text": "stuff before I move on? Yes, question? Do you think there are some long\nterm lessons you can take away",
    "start": "1091490",
    "end": "1097130"
  },
  {
    "text": "from these details that are\nmore future-proofed potentially, or do you think these are-- yeah.",
    "start": "1097130",
    "end": "1102410"
  },
  {
    "text": "So the question was, is there\nsomething more future-proof? And I think it's hard to\nhave the biggest picture.",
    "start": "1102410",
    "end": "1108679"
  },
  {
    "text": "In many ways, deep learning has\nbeen very empirical and bottom up rather than top down. But I do think there's some\ngeneralizable lessons that you",
    "start": "1108680",
    "end": "1116320"
  },
  {
    "text": "could draw from here. I think the lesson of\nhave very direct identity map residual connections is\na story and a lesson that",
    "start": "1116320",
    "end": "1124150"
  },
  {
    "text": "has played out in many,\nmany different kinds of architectures, not just\nkinds of architectures.",
    "start": "1124150",
    "end": "1129880"
  },
  {
    "text": "The effectiveness\nof LayerNorm we'll see, once again, later\non in this lecture has been very effective.",
    "start": "1129880",
    "end": "1135650"
  },
  {
    "text": "And so not letting your\nactivations drift in scale is another thing that\nI think generally has been very effective\nfor training stability.",
    "start": "1135650",
    "end": "1143470"
  },
  {
    "text": "Those two seem like fairly\ngeneralizable lessons. We will also see the systems\nconcerns come into play again.",
    "start": "1143470",
    "end": "1151790"
  },
  {
    "text": "So this is another\ngeneralizable lesson of thinking really\ncarefully about the impact of your architecture\non the systems",
    "start": "1151790",
    "end": "1157450"
  },
  {
    "text": "components of your design. So now, there's this\nother component,",
    "start": "1157450",
    "end": "1165250"
  },
  {
    "text": "which is the activations. And there is a whole\nbig zoo of activations-- ReLU, GeLU, Swiss, ELU, GeGLU.",
    "start": "1165250",
    "end": "1172840"
  },
  {
    "text": "And then, I mean, these\naren't activations. There are different\nkinds of MLPs-- GeGLU, ReGLU, SeLU,\nSwiGLU, and LiGLU.",
    "start": "1172840",
    "end": "1181409"
  },
  {
    "text": "And yeah, I think this\nis exactly the kind thing that I didn't originally\nwant to learn.",
    "start": "1181410",
    "end": "1187840"
  },
  {
    "text": "When I got into doing\ndeep learning, I was like, I don't care about activations. It's going to train anyway. But it really does matter.",
    "start": "1187840",
    "end": "1194260"
  },
  {
    "text": "Unfortunately, for\nboth you and me, that SwiGLU and\nother GLU variants",
    "start": "1194260",
    "end": "1199530"
  },
  {
    "text": "just consistently work well. And so I will\nexplain those to you. And you should think about\nthem carefully because they",
    "start": "1199530",
    "end": "1204750"
  },
  {
    "text": "do work and internalize that. So I think the ReLU,\nand maybe the GeLU,",
    "start": "1204750",
    "end": "1210910"
  },
  {
    "text": "you all should already know. The ReLU, you learn in some of\nthe most basic deep learning classes.",
    "start": "1210910",
    "end": "1216450"
  },
  {
    "text": "You just take the max of 0. And in the case of the\nMLP, you've got your-- I've dropped the bias\nterms here. x dot W1,",
    "start": "1216450",
    "end": "1222970"
  },
  {
    "text": "you take the ReLU,\nand then you do W2. Fairly easy. A GeLU is a Gaussian\nerror linear unit.",
    "start": "1222970",
    "end": "1231120"
  },
  {
    "text": "This one multiplies the linear\nwith a CDF of a Gaussian. And so it's basically\ngoing to be like the ReLU,",
    "start": "1231120",
    "end": "1239100"
  },
  {
    "text": "but with a little\nbit of a bump here. Hopefully, you can\nsee that over here. This is not just flat\nat the very bottom.",
    "start": "1239100",
    "end": "1245880"
  },
  {
    "text": "This makes things a little\nbit more differentiable, which may or may not help. And the GPT family of models--",
    "start": "1245880",
    "end": "1253240"
  },
  {
    "text": "1, 2, 3, and GPTJ and so on-- all use the GeLU.",
    "start": "1253240",
    "end": "1258540"
  },
  {
    "text": "And the original transformer\nand some of the older models used the ReLU. And really, almost\nall the modern models",
    "start": "1258540",
    "end": "1265230"
  },
  {
    "text": "have switched to the gated\nlinear units like SwiGLU and the GeGLU and others. And really, I think the Google\nfolks really pushed for this,",
    "start": "1265230",
    "end": "1275190"
  },
  {
    "text": "like PaLM, and T5 and others. But since it's been tried\nand true, basically,",
    "start": "1275190",
    "end": "1280480"
  },
  {
    "text": "almost all the models post\n2023 use a gated linear unit.",
    "start": "1280480",
    "end": "1287140"
  },
  {
    "text": "And so going back to that\nearlier question of what generalizable\narchitecture things can",
    "start": "1287140",
    "end": "1292630"
  },
  {
    "text": "we learn from this lecture? There are some things\nthat have really consistently been very\nuseful-- residual connections,",
    "start": "1292630",
    "end": "1298870"
  },
  {
    "text": "LayerNorms. Gating is yet another one. And so this is another\nplace where gating just",
    "start": "1298870",
    "end": "1304240"
  },
  {
    "text": "appears and is a very\ngood way of doing things. So, originally, this is\nour fully connected layer,",
    "start": "1304240",
    "end": "1311010"
  },
  {
    "text": "right here. This is with a ReLU. Now, instead of doing\njust linear and a ReLU,",
    "start": "1311010",
    "end": "1316130"
  },
  {
    "text": "what I'm going to do, is I'm\ngoing to gate the output here with entrywise linear term.",
    "start": "1316130",
    "end": "1322490"
  },
  {
    "text": "So x dot V is going\nto give me a vector. And I'm going to\nmultiply that entrywise",
    "start": "1322490",
    "end": "1327700"
  },
  {
    "text": "with my original\ninside term of the MLP. And then I'm going to multiply\nthe whole thing with W2.",
    "start": "1327700",
    "end": "1334760"
  },
  {
    "text": "So the way to think\nabout this, is I've gated the hidden\npart of the MLP.",
    "start": "1334760",
    "end": "1340200"
  },
  {
    "text": "So I've got my original\nactivation that takes my inputs and puts it into\nthe hidden space. And then I'm going to\ngate that with x dot V.",
    "start": "1340200",
    "end": "1347280"
  },
  {
    "text": "And then I'm going\nto project that back into the hidden\ndimensionality using W2.",
    "start": "1347280",
    "end": "1353259"
  },
  {
    "text": "So there's this gating operation\nthat happens entrywise. And that's really the basic\nthing that's happening here.",
    "start": "1353260",
    "end": "1358990"
  },
  {
    "text": "And this is the GLU plus\nthe ReLUs, the ReGLU. And then we have\nan extra parameter",
    "start": "1358990",
    "end": "1364590"
  },
  {
    "text": "that we've added\nhere for the gating. This is V. And so when someone\nsays something like,",
    "start": "1364590",
    "end": "1370200"
  },
  {
    "text": "oh, it's a GeGLU fully-- there's nothing to\nlaugh about that.",
    "start": "1370200",
    "end": "1375450"
  },
  {
    "text": "There's the GeGLU\nfully-connected layer, what I've got here is, I've got\nthe GeLU for the non-linearity.",
    "start": "1375450",
    "end": "1383679"
  },
  {
    "text": "And I've still got the exact\nsame gating here of x dot V. And this is the architecture\nthat was used by many of the Google models like\nT5v1.1, Gemma 2, Gemma 3.",
    "start": "1383680",
    "end": "1395090"
  },
  {
    "text": "And then another variants,\nthere's a SwiGLU, and this has been\nvery, very popular.",
    "start": "1395090",
    "end": "1400169"
  },
  {
    "text": "Swish is x times the sigmoid. And this is the non-linearity. And sigmoid is like this.",
    "start": "1400170",
    "end": "1405888"
  },
  {
    "text": "And x is like this. So it will look just like\nthe Gaussian error unit. And then you do the\nsame thing here.",
    "start": "1405888",
    "end": "1412039"
  },
  {
    "text": "You have a gating\nover the swish. And then you get a fully\nconnected layer here. Yes? I have a question.",
    "start": "1412040",
    "end": "1417650"
  },
  {
    "text": "Below a certain negative value,\nthe swish function and also the GeLU function, it's not\nmonotonically increasing.",
    "start": "1417650",
    "end": "1425902"
  },
  {
    "text": "And in fact, it's decreasing. And a lot of the argument about\nhow gradient descent works in introductory machine\nlearning is that, OK,",
    "start": "1425902",
    "end": "1431820"
  },
  {
    "text": "you want to do gradient descent. But here, it seems\nlike you would go in the opposite direction\nif you use GeLU or swish",
    "start": "1431820",
    "end": "1439519"
  },
  {
    "text": "or their gated versions. So, I'm not-- So the question was, this\nisn't monotonically decreasing.",
    "start": "1439520",
    "end": "1446370"
  },
  {
    "text": "There's a bit on the\nvery left of this 0 here that's flipping\nin the derivative.",
    "start": "1446370",
    "end": "1451930"
  },
  {
    "text": "And isn't that going\nto be a problem? I think intuitively,\nyou could have argued that this would be a problem.",
    "start": "1451930",
    "end": "1457940"
  },
  {
    "text": "You might trap a bunch\nof activations at zeros. I think, in practice, if\nyou look at neural network",
    "start": "1457940",
    "end": "1464620"
  },
  {
    "text": "optimization dynamics,\nwhat's actually happening is often you're\nthrowing very high learning",
    "start": "1464620",
    "end": "1469809"
  },
  {
    "text": "rates with momentum\ninto the optimizer. And so you're not really going\nto converge to the 0 point.",
    "start": "1469810",
    "end": "1476000"
  },
  {
    "text": "These activations are going\nto be all over the place. And so in practice,\nI don't think this little tiny\nnegative piece is really",
    "start": "1476000",
    "end": "1483700"
  },
  {
    "text": "an effect that's going\nto be huge for the model, if that makes sense. ",
    "start": "1483700",
    "end": "1489940"
  },
  {
    "text": "And then going back\nto this, the SwiGLU is basically most models today.",
    "start": "1489940",
    "end": "1495279"
  },
  {
    "text": "Like the LLaMA-family\nPaLM, OlMo-- and I'll show you\nthe big table later. But you'll see that the\nSwiGLU is very, very popular.",
    "start": "1495280",
    "end": "1502700"
  },
  {
    "text": "And one thing to note-- I'll talk about this again\nin the hyperparameters part-- is now, remember, I've\nadded this V term,",
    "start": "1502700",
    "end": "1510179"
  },
  {
    "text": "this extra parameter. And so I want to think\nabout how to size. This extra parameter.",
    "start": "1510180",
    "end": "1516420"
  },
  {
    "text": "And what people do is\ngated models usually make this hidden size. The basically output\ndimensionality of W slightly",
    "start": "1516420",
    "end": "1523820"
  },
  {
    "text": "smaller by a factor of 2/3\nin order to make sure that the total number of parameters\nof this whole thing remains",
    "start": "1523820",
    "end": "1531590"
  },
  {
    "text": "the same as the\nnon-gated counterparts. And that's a convention\nthing that most people do. If you don't quite\nunderstand what that is,",
    "start": "1531590",
    "end": "1538160"
  },
  {
    "text": "I'll go back over\nthat again later. But you can just keep\nin mind that basically for the gated linear\nunits, you just",
    "start": "1538160",
    "end": "1543860"
  },
  {
    "text": "make everything a\nlittle bit smaller to make sure things\nremain parameter matched.",
    "start": "1543860",
    "end": "1550130"
  },
  {
    "text": "So, yes, question. This may be obvious or\nhave gone over in the past.",
    "start": "1550130",
    "end": "1556130"
  },
  {
    "text": "One of the benefits\nof ReLU is like, it's very easily differentiable\nby the input,",
    "start": "1556130",
    "end": "1562140"
  },
  {
    "text": "but if you have the derivative\nof the CDF of the Gaussian, you have a squared with x.",
    "start": "1562140",
    "end": "1567980"
  },
  {
    "text": "Does that not really\nslow things down? That's a very good question. I'm not 100% sure what the\ninternal CUDA implementation",
    "start": "1567980",
    "end": "1576520"
  },
  {
    "text": "of the SwiGLU or the GeGLU is. I think it's entirely possible\nthat internally, they might be",
    "start": "1576520",
    "end": "1582670"
  },
  {
    "text": "implemented with lookup tables. Go ahead. I mean, what really matters\nis the memory pressure here.",
    "start": "1582670",
    "end": "1588140"
  },
  {
    "text": "And it will be the exact\nsame because you're reading the same amount of elements. So the extra compute is\nnegligible on [INAUDIBLE].",
    "start": "1588140",
    "end": "1594139"
  },
  {
    "text": "That's probably\na better argument that basically flops why\nthis is negligible anyway.",
    "start": "1594140",
    "end": "1599540"
  },
  {
    "text": "And actually, the memory\ncalculus is the same, so. Cool.",
    "start": "1599540",
    "end": "1604805"
  },
  {
    "text": " So, do gated linear units work?",
    "start": "1604805",
    "end": "1609980"
  },
  {
    "text": "I will have more modern\nevidence for this as well. But I thought I should\ntake you straight to the horse's mouth, Noam\nShazeer's original paper, where",
    "start": "1609980",
    "end": "1618370"
  },
  {
    "text": "he evaluates all\nthese GLU variants. And this is somewhat\nolder stuff.",
    "start": "1618370",
    "end": "1624020"
  },
  {
    "text": "So you're seeing CoLA\nand SST-2 performance. But you do see basically that\nthe GLU variants consistently",
    "start": "1624020",
    "end": "1631050"
  },
  {
    "text": "perform better. GLU is 84.20,\n84.12, 84.36, 84.67.",
    "start": "1631050",
    "end": "1637590"
  },
  {
    "text": "And wow, it's 2020s. They even give you the\nstandard deviations so you can figure out how\nsignificant those results are.",
    "start": "1637590",
    "end": "1644530"
  },
  {
    "text": "And they, in fact,\nare significant. And so this is some nice\nevidence to see here.",
    "start": "1644530",
    "end": "1651090"
  },
  {
    "text": "There was also the Narang\net al in 2020 paper, which is a very nice paper, studying\nall sorts of architecture",
    "start": "1651090",
    "end": "1656910"
  },
  {
    "text": "variants, I think in the\ncontext of T5 style models. And once again, you see\nthat the gated linear unit",
    "start": "1656910",
    "end": "1663840"
  },
  {
    "text": "variants consistently\nachieve lower losses than their counterparts.",
    "start": "1663840",
    "end": "1668980"
  },
  {
    "text": "You see that the bolded lines\nare exactly at the GLU variants. And this pattern has\nbasically held up.",
    "start": "1668980",
    "end": "1677640"
  },
  {
    "text": "So for gating and\nactivations, there are lots of variants\nacross different models.",
    "start": "1677640",
    "end": "1683960"
  },
  {
    "text": "But the gated linear\nunit has become basically widespread\nand dominant, and I think for good reason.",
    "start": "1683960",
    "end": "1689720"
  },
  {
    "text": "Of course, the GLU isn't\nnecessary for a good model. It's important to\nseparate the two.",
    "start": "1689720",
    "end": "1695600"
  },
  {
    "text": "Just because it's probably\nthe slightly better and everyone does it\ndoesn't mean it's necessary.",
    "start": "1695600",
    "end": "1701240"
  },
  {
    "text": "And you do see examples of\nvery high performance models. not using a GLU. Like GPT-3 is one example.",
    "start": "1701240",
    "end": "1707490"
  },
  {
    "text": "A more recent one, Nemotron\n340B uses a squared ReLU, which I had not seen before.",
    "start": "1707490",
    "end": "1713700"
  },
  {
    "text": "And Falcon 211b uses a ReLU. Both of those are relatively\nhigh performance models. So you can see that it's\nnot really necessary.",
    "start": "1713700",
    "end": "1721230"
  },
  {
    "text": "And so evidence does point\ntowards consistent gains from SwiGLU and GeGLU. And that's why we ask you to\nimplement exactly that variant.",
    "start": "1721230",
    "end": "1730455"
  },
  {
    "text": "Cool.  The final thing that I want to\ntalk about for architectures,",
    "start": "1730455",
    "end": "1737580"
  },
  {
    "text": "and this is one final-- major, I want to say,\nvariation that we've seen. Normally, the transformer\nblock is serial.",
    "start": "1737580",
    "end": "1745750"
  },
  {
    "text": "In the sense that\nfor each block, the outputs come\nin from the bottom.",
    "start": "1745750",
    "end": "1752360"
  },
  {
    "text": "And then you do your\nattention, and then you pass the result of that\ncomputation forward. And then you do your\nMLP, and then you",
    "start": "1752360",
    "end": "1758680"
  },
  {
    "text": "pass that computation forward. And so this is\ninherently serial. You do attention, and then MLP.",
    "start": "1758680",
    "end": "1764350"
  },
  {
    "text": "But of course, this might have\ncertain parallelism constraints. If you want to parallelize this\nover gigantic sets of GPUs,",
    "start": "1764350",
    "end": "1771820"
  },
  {
    "text": "it might be harder to do so if\nyou have the serial connection. The systems concerns might\nalso be more difficult.",
    "start": "1771820",
    "end": "1778330"
  },
  {
    "text": "You might get lower\nutilization from your GPUs. And so a few models\nhave done this thing",
    "start": "1778330",
    "end": "1784090"
  },
  {
    "text": "that I'll call parallel layers. Where, basically, instead\nof having serial computation",
    "start": "1784090",
    "end": "1790450"
  },
  {
    "text": "of attention and then\nMLP, they will do them both at the same time. So you will get your x\nfrom your previous layer.",
    "start": "1790450",
    "end": "1797150"
  },
  {
    "text": "You will compute both the MLP\nand the attention side by side, and then you will\nadd them together into the residual stream.",
    "start": "1797150",
    "end": "1803013"
  },
  {
    "text": "And then that will\nbe your output. And this was pioneered by GPT-J,\nwhich was this open source",
    "start": "1803013",
    "end": "1809220"
  },
  {
    "text": "replication effort. And the folks at\nGoogle doing PaLM were bold enough to do this\nat the really big scale.",
    "start": "1809220",
    "end": "1816840"
  },
  {
    "text": "And many others\nhave followed since. So if you're\nimplementing this right, you can share a lot of\nstuff like the Layer Norms.",
    "start": "1816840",
    "end": "1823470"
  },
  {
    "text": "And the matrix multiplies\ncan get fused together. And you can get some systems\nefficiencies out of that.",
    "start": "1823470",
    "end": "1829500"
  },
  {
    "text": "It hasn't been quite as\npopular since then, at least in the last year. I think most of the\nmodels that we've seen have been serial layers\nrather than parallel ones.",
    "start": "1829500",
    "end": "1836582"
  },
  {
    "text": "I think the only exceptions to\nthis are like Cohere Command A, Command R+, and\na Falcon 2 11B.",
    "start": "1836582",
    "end": "1844020"
  },
  {
    "text": "So now, I think we have the\nability to go back to this big, hard to see chart, and\nthen see what I was",
    "start": "1844020",
    "end": "1851280"
  },
  {
    "text": "pointing at the very beginning. So this column here,\nyou don't really need to be able to\nread any of the text.",
    "start": "1851280",
    "end": "1857010"
  },
  {
    "text": "Because I think the colors\nwill tell you everything you need to see. This check mark here, this is\nbasically pre versus post-norm.",
    "start": "1857010",
    "end": "1863200"
  },
  {
    "text": "The only two models I really\nknow of in the early days that did post norm, this is\nthe original transformer",
    "start": "1863200",
    "end": "1870090"
  },
  {
    "text": "and GPT and BERT, if you want\nto include that into this table. And then almost everybody else-- I think, basically, everyone\nelse has done pre-norm.",
    "start": "1870090",
    "end": "1878080"
  },
  {
    "text": "The only other non\nchecked boxes here are models that are proprietary,\nand I don't have details for.",
    "start": "1878080",
    "end": "1883380"
  },
  {
    "text": "This column here, on\nthe leftmost thing, this is RMS Norm\nversus LayerNorm. The gray boxes\nare the LayerNorm.",
    "start": "1883380",
    "end": "1889840"
  },
  {
    "text": "The blue ones are RMS Norm. Basically, most people have\nconverged to RMS Norm as I said.",
    "start": "1889840",
    "end": "1894960"
  },
  {
    "text": "This column next to it is\nserial and parallel layers. Once again, most\npeople do serial, but you see other variants.",
    "start": "1894960",
    "end": "1901355"
  },
  {
    "text": "What I'm going to\ntalk about next is going to be\nposition embeddings, and that will be more\ninteresting in a moment here. Any questions about any\nof this architecture",
    "start": "1901355",
    "end": "1908160"
  },
  {
    "text": "stuff before I move on? Hopefully, that gives\nyou a bit of an overview of at least the major variations\nin architectures that we see.",
    "start": "1908160",
    "end": "1916720"
  },
  {
    "text": "Is the serial layer more\ncomputationally efficient than parallel layer? So the question was\nwhether serial is",
    "start": "1916720",
    "end": "1923620"
  },
  {
    "text": "more efficient than parallel. It should be\nactually the reverse. That parallel is more\nefficient than serial.",
    "start": "1923620",
    "end": "1929300"
  },
  {
    "text": "And that's why you're\nwilling to do this. So in some sense, you\nmight expect serial to be more expressive\nbecause you're",
    "start": "1929300",
    "end": "1935247"
  },
  {
    "text": "composing two computations\nrather than just adding them together. But the benefit of\nparallel, in theory,",
    "start": "1935248",
    "end": "1940450"
  },
  {
    "text": "is that if you write the\nright kinds of fused kernels, a lot of these operations\ncan be done in parallel,",
    "start": "1940450",
    "end": "1945950"
  },
  {
    "text": "or the computation\nis shared across the different parallel parts.",
    "start": "1945950",
    "end": "1951490"
  },
  {
    "text": "So, cool. So the last thing I want to talk\nabout in architecture land--",
    "start": "1951490",
    "end": "1958330"
  },
  {
    "text": "I think this is the last\nthing-- is variations in position embeddings. And I think this\none's interesting",
    "start": "1958330",
    "end": "1963640"
  },
  {
    "text": "because in the first\nfew years of LMs land, there were a lot of different\nthings that people were trying.",
    "start": "1963640",
    "end": "1971559"
  },
  {
    "text": "Sign embeddings were from\nthe original transformer. You should have\nlearned this in 224N. There's sine and\ncosine positions.",
    "start": "1971560",
    "end": "1978659"
  },
  {
    "text": "Many others did\nabsolute embeddings, like the GPTs and OPT,\nall basically just added a position\nlearned position vector",
    "start": "1978660",
    "end": "1985470"
  },
  {
    "text": "to the embedding. Some others, like T5 and\nGopher, did various kinds",
    "start": "1985470",
    "end": "1991530"
  },
  {
    "text": "of relative embeddings\nthat add vectors to the attention computation. And then, I think\nmost models have",
    "start": "1991530",
    "end": "1998039"
  },
  {
    "text": "converged to rope, which is\nrelative position embeddings. And this, I think,\nactually started in GPTJ,",
    "start": "1998040",
    "end": "2004770"
  },
  {
    "text": "once again, another open\nsource contribution. It has really rapidly been\npicked up by most of the models.",
    "start": "2004770",
    "end": "2011750"
  },
  {
    "text": "And so the high level\nthought process behind RoPE is that the thing that\nmatters is relative positions",
    "start": "2011750",
    "end": "2018380"
  },
  {
    "text": "of these vectors. And so if I have an embedding f\nof x of i, where x is the word",
    "start": "2018380",
    "end": "2025789"
  },
  {
    "text": "I'm trying to embed,\nand i is my position, then I should be able to\nwrite things down in this way.",
    "start": "2025790",
    "end": "2031140"
  },
  {
    "text": "So there should exist the f,\nsuch that f of x, i, and f of y, j, if I take the inner\nproduct of these embeddings,",
    "start": "2031140",
    "end": "2037820"
  },
  {
    "text": "then, I can write this down\nas some different function g, which is a function\nof the two words",
    "start": "2037820",
    "end": "2043120"
  },
  {
    "text": "and the difference\nin their positions. So this is a definition\nthat enforces basically",
    "start": "2043120",
    "end": "2051280"
  },
  {
    "text": "position invariance or\nabsolute position invariance. So you only pay attention to how\nfar apart these two words are.",
    "start": "2051280",
    "end": "2057969"
  },
  {
    "text": "And so you can do a\nbrief check and see, OK, what happens with sines while\nyou get these cross terms that",
    "start": "2057969",
    "end": "2063399"
  },
  {
    "text": "are not relative? So you do still leak absolute\nposition information. Absolute positions\nlike it's in the name.",
    "start": "2063400",
    "end": "2069730"
  },
  {
    "text": "It's not a relative\nposition embedding. And relative embeddings,\nwell, it is relative,",
    "start": "2069730",
    "end": "2076059"
  },
  {
    "text": "but it's not an inner product. So it violates this constraint. And so RoPE is this kind\nof clever observation",
    "start": "2076060",
    "end": "2083800"
  },
  {
    "text": "that we do know\none thing, that is invariant to absolute\nthings, which is rotations.",
    "start": "2083800",
    "end": "2090968"
  },
  {
    "text": "And so we're going to exploit\nthat structure to come up with our position embeddings. We know that inner products are\ninvariant to arbitrary rotation.",
    "start": "2090968",
    "end": "2099437"
  },
  {
    "text": "So we're going to leverage that. So on the left, this\nis the starting point. Let's say my embedding\nfor the word \"we\"",
    "start": "2099437",
    "end": "2105230"
  },
  {
    "text": "is this arrow over here. And my embedding\nfor the word \"no\" is this other arrow over here.",
    "start": "2105230",
    "end": "2111200"
  },
  {
    "text": "Now, I want to embed this\nsequence, \"we know that.\" And I look at the\nword \"we\" and \"no.\"",
    "start": "2111200",
    "end": "2116760"
  },
  {
    "text": "So how do I do that? Well, we assume position 0. So I'm not going to\nrotate that guy at all.",
    "start": "2116760",
    "end": "2121970"
  },
  {
    "text": "\"No\" is in position one, so I'm\ngoing to rotate him by one unit",
    "start": "2121970",
    "end": "2127400"
  },
  {
    "text": "of rotation. And so now, I have this\nembedding for \"we know.\" And now, let's say I want to\nembed this sequence, \"Of course,",
    "start": "2127400",
    "end": "2134700"
  },
  {
    "text": "we know.\" Now, \"we\" and \"know\" have\nthe same relative positioning to each other. And so let's look\nat what happens.",
    "start": "2134700",
    "end": "2140640"
  },
  {
    "text": "We get shifted by two positions. I rotate \"we\" by-- I start in this\nvertical position,",
    "start": "2140640",
    "end": "2146300"
  },
  {
    "text": "and I rotate them\ntwice, 1 and 2. And then I rotate \"no\" by three\npositions because it's 1, 2, 3--",
    "start": "2146300",
    "end": "2153190"
  },
  {
    "text": "sorry, 0, 1, 2, 3rd position. And so, now, if you look\nat these two arrows,",
    "start": "2153190",
    "end": "2158500"
  },
  {
    "text": "they have the same\nrelative angle right. So their inner\nproducts are preserved. And so this is the nice\nfun idea about RoPE.",
    "start": "2158500",
    "end": "2164930"
  },
  {
    "text": "You just rotate the vectors. And the rotation angle is\ndetermined by the position",
    "start": "2164930",
    "end": "2170470"
  },
  {
    "text": "of each word and rotations-- the inner products don't care\nabout relative rotations.",
    "start": "2170470",
    "end": "2176359"
  },
  {
    "text": "And so these inner\nproducts are only going to look at the\ndifference in distance.",
    "start": "2176360",
    "end": "2181960"
  },
  {
    "text": "Now, it's easy to think about\nin 2D because rotations are kind of obvious in 2D. There's only one way\nto rotate a vector.",
    "start": "2181960",
    "end": "2189339"
  },
  {
    "text": "But in high dimensional\nspaces where we operate, it's not obvious at all how we\nare going to do this rotation.",
    "start": "2189340",
    "end": "2195620"
  },
  {
    "text": "So the RoPE folks came\nup with, in some ways, the simplest but also\neffective way of doing this.",
    "start": "2195620",
    "end": "2201410"
  },
  {
    "text": "And the way to do it is you take\nyour high dimensional vector. In this case, D. And I'm\njust going to cut it up",
    "start": "2201410",
    "end": "2207010"
  },
  {
    "text": "into blocks of two dimensions. And every two dimension is going\nto be rotated by some theta.",
    "start": "2207010",
    "end": "2212750"
  },
  {
    "text": "So there's going to\nbe a rotation speed. And I'm going to rotate\nthe pairs of dimensions.",
    "start": "2212750",
    "end": "2219040"
  },
  {
    "text": "And so now, every\npair of dimensions is encoding all of these\nrelative positions. And much like in sine\nand cosine embeddings,",
    "start": "2219040",
    "end": "2226310"
  },
  {
    "text": "I'm going to pick\nsome set of thetas such that some embeddings\nare rotated quickly, and others are rotated\nmuch more slowly.",
    "start": "2226310",
    "end": "2233570"
  },
  {
    "text": "So they can capture both\nhigh frequency information or close by information\nand very far away,",
    "start": "2233570",
    "end": "2239780"
  },
  {
    "text": "lower frequency\npositioning information. And the actual\nRoPE math here is,",
    "start": "2239780",
    "end": "2247222"
  },
  {
    "text": "if you're going to\nthink about rotations, it's just going to be\nmultiplying with various sine and cosine rotation matrices.",
    "start": "2247222",
    "end": "2252740"
  },
  {
    "text": "Hopefully, you remember this\nfrom linear algebra and trig. And so you can think\nabout this as an operation",
    "start": "2252740",
    "end": "2258700"
  },
  {
    "text": "where you multiply\nyour embedding vectors with these block 2\nby 2 block matrices.",
    "start": "2258700",
    "end": "2265610"
  },
  {
    "text": "And there's no additive or\ncross terms that appear here. This is all purely relative.",
    "start": "2265610",
    "end": "2273950"
  },
  {
    "text": "One thing that is different, if\nyou're used to absolute position embeddings or sine\nand cosine embeddings",
    "start": "2273950",
    "end": "2280309"
  },
  {
    "text": "here is that the RoPE\nis going to operate at the actual attention layer. You're not going to add position\nembeddings at the bottom.",
    "start": "2280310",
    "end": "2287400"
  },
  {
    "text": "Whenever these\nattention computations are going to be\ndone, you're going to intervene on that layer. And then that's going to give\nyou your position information.",
    "start": "2287400",
    "end": "2295079"
  },
  {
    "text": "And so I pulled this from I\nthink the LLaMA implementation of RoPE. You've got the initial\nnormal attention stuff",
    "start": "2295080",
    "end": "2300770"
  },
  {
    "text": "at the very top like\nquery keys and values. These are your normal\nlinear projections.",
    "start": "2300770",
    "end": "2305930"
  },
  {
    "text": "And then, you're going to come\nup with cosine and sine angles. These are rotation\nangles telling you",
    "start": "2305930",
    "end": "2311990"
  },
  {
    "text": "how much to rotate different\nblocks of the query and key.",
    "start": "2311990",
    "end": "2317010"
  },
  {
    "text": "And so you take your\nquery and your key, and you're going to rotate\nthem by the cosines and sines.",
    "start": "2317010",
    "end": "2322220"
  },
  {
    "text": "And now, you've gotten\nrotated query and rotated key. And that's going\nto be what's going to go into the rest of\nyour attention computation.",
    "start": "2322220",
    "end": "2328400"
  },
  {
    "text": "So you don't do\nthis at the bottom. You do it whenever you\ngenerate your queries and keys. Hopefully that's clear.",
    "start": "2328400",
    "end": "2334210"
  },
  {
    "text": "That's really\ncritical to enforcing this relative positioning\nonly information.",
    "start": "2334210",
    "end": "2340900"
  },
  {
    "text": "OK? Good. So one of the things\nI want to highlight",
    "start": "2340900",
    "end": "2346990"
  },
  {
    "text": "is that RoPE is actually\none of the things that it seems like\neveryone has converged on. I went through all 19 of\nthose papers over the weekend.",
    "start": "2346990",
    "end": "2355609"
  },
  {
    "text": "And basically, all\nof them now use RoPE for various different reasons. The reason that RoPE has now\nmany different algorithms",
    "start": "2355610",
    "end": "2363340"
  },
  {
    "text": "for extrapolating\ncontext length, and that's an important part\nof the modern productionized language model.",
    "start": "2363340",
    "end": "2369400"
  },
  {
    "text": "But also, it seems to be\nempirically quite effective, even at fairly small scales\nand small context lengths.",
    "start": "2369400",
    "end": "2374480"
  },
  {
    "text": "So it's kind of\nworn out on this-- what's it called--\nposition embedding battle.",
    "start": "2374480",
    "end": "2380790"
  },
  {
    "text": "Any questions before I move on\nto some of the hyperparameter stuff? Yes. Is the rate of rotation\nis consistent across all",
    "start": "2380790",
    "end": "2386369"
  },
  {
    "text": "of these models? I don't think\nthey're all the same. There's some variation\nin the thetas. ",
    "start": "2386370",
    "end": "2393650"
  },
  {
    "text": "Yes. Are the thetas for each pair,\nare those hyperparameters",
    "start": "2393650",
    "end": "2399690"
  },
  {
    "text": "or are they trained? The thetas that determine\nthe rotation angles, they're not hyperparameters.",
    "start": "2399690",
    "end": "2405300"
  },
  {
    "text": "Much like in the sines\nand cosines here, there's a schedule to\nthe rotation angles",
    "start": "2405300",
    "end": "2412320"
  },
  {
    "text": "that are determined. And it's in the same intuition\nas the sines and cosines. You want to cover different\nfrequency ranges in order",
    "start": "2412320",
    "end": "2418650"
  },
  {
    "text": "to get higher or lower\nfrequency information.",
    "start": "2418650",
    "end": "2424020"
  },
  {
    "text": "Yes. The rotations create any\ndifficulty with training, I wonder like this,\nangular rotations.",
    "start": "2424020",
    "end": "2431430"
  },
  {
    "text": "The rotations themselves don't\nreally create any issues. Because one way of\nthinking about a rotation is that it's just\na matrix multiply.",
    "start": "2431430",
    "end": "2437040"
  },
  {
    "text": "Since thetas are fixed and\nthe m's here are fixed, this is really\njust a fixed matrix",
    "start": "2437040",
    "end": "2442790"
  },
  {
    "text": "that multiplies your vector. And so in that sense,\nit's not really an issue. If you were learning\nthe thetas, then maybe you have issues because\nyou're maybe differentiating",
    "start": "2442790",
    "end": "2449787"
  },
  {
    "text": "through trig functions. But you're not\ndoing that here, so. OK, cool.",
    "start": "2449787",
    "end": "2456410"
  },
  {
    "text": "So now, I think we go even one\nmore level into the details here. And we're going to talk\nabout hyperparameters.",
    "start": "2456410",
    "end": "2463609"
  },
  {
    "text": "I feel like when you have\nto-- you're dropped in, and you're asked to train\na new language model.",
    "start": "2463610",
    "end": "2469363"
  },
  {
    "text": "There's a lot of questions you\nhave about hyperparameters, because there's\nquite a few of them. And one of the things\nthat I've realized",
    "start": "2469363",
    "end": "2475010"
  },
  {
    "text": "is that actually,\nonly a few of these really get changed across\ndifferent successful models. There's actually like\nfairly clear rules of thumb",
    "start": "2475010",
    "end": "2481910"
  },
  {
    "text": "and fairly clear guidelines that\npeople seem to be following. So there are some things\nlike how much bigger should",
    "start": "2481910",
    "end": "2488329"
  },
  {
    "text": "the feed-forward size be? Or how many heads should I have? Or what should my vocab size be?",
    "start": "2488330",
    "end": "2493820"
  },
  {
    "text": "And so we'll talk about\neach of those things, and we'll try to constrain\nthe space of hyperparameters",
    "start": "2493820",
    "end": "2498880"
  },
  {
    "text": "that people have. So the starting\npoint, we're going",
    "start": "2498880",
    "end": "2503920"
  },
  {
    "text": "to look at a simple\nfeed-forward layer. Just with the bias, let's say. This is a ReLU version of it.",
    "start": "2503920",
    "end": "2510470"
  },
  {
    "text": "And so there's two\nhyperparameters here. There's d model, which is\nthe dimensionality of x.",
    "start": "2510470",
    "end": "2515480"
  },
  {
    "text": "That's the input\ncoming into your MLP. And then you've got dff. So this is the\nfeed-forward dimension.",
    "start": "2515480",
    "end": "2521390"
  },
  {
    "text": "This is the output hidden\ndimension of your MLP. And from there, you're going\nto project back onto d model.",
    "start": "2521390",
    "end": "2527900"
  },
  {
    "text": "So what should dff be? In general. These things are going\nto be up projections.",
    "start": "2527900",
    "end": "2535160"
  },
  {
    "text": "You're going to have more hidden\nunits than there were inputs. But how much bigger? Well, there is actually\njust like a consensus.",
    "start": "2535160",
    "end": "2542619"
  },
  {
    "text": "Almost everybody that uses ReLU\nstyle MLPs are going to pick dff",
    "start": "2542620",
    "end": "2548560"
  },
  {
    "text": "is equal to 4 times d model. I will show you some\nempirical evidence for why",
    "start": "2548560",
    "end": "2555310"
  },
  {
    "text": "this is a sane number later. But as far as I\ncan tell, there's no law of nature that\nsays you have to pick 4.",
    "start": "2555310",
    "end": "2563020"
  },
  {
    "text": "This is a convention\nthat has really held up. Now, there are a few\nexceptions to this rule.",
    "start": "2563020",
    "end": "2569500"
  },
  {
    "text": "Remember that the GLU variants\nare going to scale this down by a factor of 2/3.",
    "start": "2569500",
    "end": "2574700"
  },
  {
    "text": "And if you scale it\ndown by a factor of 2/3, you're going to have roughly\nthe same number of parameters.",
    "start": "2574700",
    "end": "2582220"
  },
  {
    "text": "You can do a little bit of math. And if you scale the GLU\nvariants down by a factor of 2/3, you'll come to the\nconclusion that the way to do",
    "start": "2582220",
    "end": "2589270"
  },
  {
    "text": "that is to set dff equal\nto 8 over 3d model. That's going to be the\nnumber that you end up at.",
    "start": "2589270",
    "end": "2594830"
  },
  {
    "text": "And you guys convince yourself\nthat that will give you the same number of parameters. And that's the ratio\nthat you would get if you",
    "start": "2594830",
    "end": "2600107"
  },
  {
    "text": "started with a ratio of 4. So if you look at\nmany of the models, they actually do follow\nthis rule of thumb.",
    "start": "2600107",
    "end": "2606849"
  },
  {
    "text": "PaLM, for example-- PaLM, Mistral, and LLaMA\nare slightly larger.",
    "start": "2606850",
    "end": "2611910"
  },
  {
    "text": "These are GLU models, but they\ndon't follow this 2.6 rule. But if you look at, for example,\nLLaMA-1, Qwen, DeepSeek,",
    "start": "2611910",
    "end": "2618109"
  },
  {
    "text": "Yi, and T5, they all roughly\nfollow this 2.6 ish rule.",
    "start": "2618110",
    "end": "2623810"
  },
  {
    "text": "And I can put up\nthe big table of LMs that I made later\nwith hyperparameters. Many, many, many of them fall\ninto this roughly 2.6 range.",
    "start": "2623810",
    "end": "2632730"
  },
  {
    "text": "And that's the standard\nparameterization of a GLU unit.",
    "start": "2632730",
    "end": "2638240"
  },
  {
    "text": "I'll go through one\nother exception. I really like this exception\nbecause I think, in many ways,",
    "start": "2638240",
    "end": "2644540"
  },
  {
    "text": "big large language\nmodel training is a game of copying\nhyperparameters from other people.",
    "start": "2644540",
    "end": "2649817"
  },
  {
    "text": "And so we don't learn very much. It's very conservative. But T5, I really like.",
    "start": "2649817",
    "end": "2655520"
  },
  {
    "text": "Because in some sense,\nit's really bold. And I think Google\npeople actually do some pretty bold stuff.",
    "start": "2655520",
    "end": "2660829"
  },
  {
    "text": "And so if you look at the 11\nbillion parameter T5 model, they have a pretty\nincredible setting.",
    "start": "2660830",
    "end": "2667860"
  },
  {
    "text": "Their hidden dim is 1,024. But their dff, their\nup-projected dimension is",
    "start": "2667860",
    "end": "2673770"
  },
  {
    "text": "65,000. And so that's going\nto give you a 64 times multiplier on the ratio\nof dff to d model.",
    "start": "2673770",
    "end": "2682540"
  },
  {
    "text": "And of course, you\ncompare to this, where PaLM is like a factor of 4. And everyone else\nis much smaller.",
    "start": "2682540",
    "end": "2687550"
  },
  {
    "text": "This is a very large difference. And there's some\nother recent examples of using much\nbigger multipliers.",
    "start": "2687550",
    "end": "2695430"
  },
  {
    "text": "Gemma 2 follows\nin these footsteps and does a factor of 8. And I'll talk a little bit\nabout this exception later.",
    "start": "2695430",
    "end": "2701950"
  },
  {
    "text": "Of course, T5 was a\ntotally fine model. So this should tell you\nit is possible to train a model with such a\nmuch larger ratio.",
    "start": "2701950",
    "end": "2711090"
  },
  {
    "text": "So one of the things that I\nthink is quantitative evidence, I saw that 4x multiplier. And I thought, is that\nreally the right thing to do,",
    "start": "2711090",
    "end": "2719020"
  },
  {
    "text": "or is there some more\nquantitative experiment someone's done to convince\nme that that is a good idea?",
    "start": "2719020",
    "end": "2724470"
  },
  {
    "text": "So one of the figures from\nJared Kaplan's scaling law paper-- and most\npeople know this paper",
    "start": "2724470",
    "end": "2730220"
  },
  {
    "text": "for the scaling law component. But actually, there's also some\nreally useful hyperparameter components to this paper.",
    "start": "2730220",
    "end": "2736339"
  },
  {
    "text": "You'll actually see that\nthey do exactly this thing that I'm talking about,\nthe dff to d model ratio.",
    "start": "2736340",
    "end": "2742010"
  },
  {
    "text": "And they plot essentially\nhow much the loss increases as you vary this. And you see that\nthere's a sweet spot.",
    "start": "2742010",
    "end": "2750360"
  },
  {
    "text": "This is a ratio of 1, 2, 3, 4\nand then up to 10 or so here.",
    "start": "2750360",
    "end": "2755670"
  },
  {
    "text": "And so there's a\npretty wide basin here, anywhere between 1\nto maybe up to 10,",
    "start": "2755670",
    "end": "2761730"
  },
  {
    "text": "where you can pick whatever\nfeed-forward ratio you want. And it'll be roughly optimal.",
    "start": "2761730",
    "end": "2768020"
  },
  {
    "text": "And 4 is not too far off from\nyour optimal choices over here. It's 1, 2, 3, 4.",
    "start": "2768020",
    "end": "2774661"
  },
  {
    "text": "It's like right here\nor maybe right here. So that's a pretty\nreasonable choice. ",
    "start": "2774662",
    "end": "2781400"
  },
  {
    "text": "So what can we learn from all\nof this hyperparameter stuff? I think a lot of the\nevidence points towards--",
    "start": "2781400",
    "end": "2786940"
  },
  {
    "text": "you can pick the same defaults\nof, if you're not using a glue, you can multiply by 4. If you're using a glue,\nyou can use roughly 2.66.",
    "start": "2786940",
    "end": "2795700"
  },
  {
    "text": "And they can work pretty well\nfor mostly all the modern LMs. T5, once again, does\nshow that you don't",
    "start": "2795700",
    "end": "2802630"
  },
  {
    "text": "have to follow these rules. You can be a rule breaker\nand do whatever you'd like. There's no hyperparameter\nchoice written in stone.",
    "start": "2802630",
    "end": "2808880"
  },
  {
    "text": "You can get reasonable LMs at\nmany other hyperparameters. That said, I think the really\nfunny epilogue to this story is",
    "start": "2808880",
    "end": "2815890"
  },
  {
    "text": "that T5 has a follow up model\ncalled T5v1.1 that's improved.",
    "start": "2815890",
    "end": "2821150"
  },
  {
    "text": "And it uses a much more standard\n2.5 multiplier on GeGLU. So you can read\nbetween the lines",
    "start": "2821150",
    "end": "2826420"
  },
  {
    "text": "and say maybe they looked\nat the original T5 and said, actually, maybe we want to walk\nback that 64 times multiplier",
    "start": "2826420",
    "end": "2833020"
  },
  {
    "text": "and pick a more standard one. And they did end up\nwith a better model. So, cool. What is the relationship\nbetween the ratio",
    "start": "2833020",
    "end": "2840280"
  },
  {
    "text": "and the model efficiency\non [INAUDIBLE]?",
    "start": "2840280",
    "end": "2846190"
  },
  {
    "text": "So I think that's\na good question. So the question was,\nwhat's the ratio-- or sorry, what's\nthe relationship",
    "start": "2846190",
    "end": "2851880"
  },
  {
    "text": "between this ratio that I'm\ntalking about here and generally the impact on the model? And so if we go all\nthe way back here,",
    "start": "2851880",
    "end": "2863520"
  },
  {
    "text": "the ratio is controlling\nessentially how wide the hidden part of this MLP is.",
    "start": "2863520",
    "end": "2869830"
  },
  {
    "text": "And so the original\njustification in the T5 paper for\npicking 64 was to say,",
    "start": "2869830",
    "end": "2874980"
  },
  {
    "text": "actually, we can get bigger\nand fatter matrix multiplies if we make that dimension\nreally, really large.",
    "start": "2874980",
    "end": "2880540"
  },
  {
    "text": "And while that is kind of a\ntrue statement, the wider it is, you're getting more parallel\ncomputation, so to speak,",
    "start": "2880540",
    "end": "2887560"
  },
  {
    "text": "rather than serial computation. So you're spending your\nflops and your parameters in a slightly different way than\nif you made your hidden units",
    "start": "2887560",
    "end": "2893313"
  },
  {
    "text": "bigger, which would let\nyou pass more information, or using more units,\nwhich would give you more serial computation.",
    "start": "2893313",
    "end": "2899349"
  },
  {
    "text": "So you're spending your\nparameters and your flops in a slightly suboptimal\nway from expressive power,",
    "start": "2899350",
    "end": "2904750"
  },
  {
    "text": "but you might get systems\ngains if your matrices are wide enough. ",
    "start": "2904750",
    "end": "2913260"
  },
  {
    "text": "Excellent. So another thing that is\na surprising-- or maybe",
    "start": "2913260",
    "end": "2918960"
  },
  {
    "text": "not surprising--\nconsensus hyperparameter is the ratio between\nthe model dimension",
    "start": "2918960",
    "end": "2925620"
  },
  {
    "text": "and the head dimension\ntimes the number of heads. So I clipped this from 224N.",
    "start": "2925620",
    "end": "2931200"
  },
  {
    "text": "But really, the basically\ncanonical choice is to pick things so\nthat the dimension d--",
    "start": "2931200",
    "end": "2937890"
  },
  {
    "text": "that's a hidden dimension--\nand if you have multiple heads, you're just going to split\nup the number of dimensions",
    "start": "2937890",
    "end": "2943869"
  },
  {
    "text": "each head gets. So you're going to keep\nthe dimensions fixed as you add more heads. And you don't have to do that.",
    "start": "2943870",
    "end": "2950088"
  },
  {
    "text": "As you add more\nheads, you could just keep the same number\nof dimensions per head, and you could just\nlet the attention part",
    "start": "2950088",
    "end": "2955320"
  },
  {
    "text": "take more and more parameters. You could do that. That's an option that you have. But most models, once again,\ndo follow this guideline.",
    "start": "2955320",
    "end": "2963190"
  },
  {
    "text": "We see GPT-3, T5,\nLaMDA, PaLM, and LLaMA2, they all have a ratio of\none or almost exactly one.",
    "start": "2963190",
    "end": "2970570"
  },
  {
    "text": "T5 is the one exception\nthat breaks this rule. They tried the big ratio of 16.",
    "start": "2970570",
    "end": "2976660"
  },
  {
    "text": "But otherwise, it is all fairly\nfollowing this consensus. There's been a couple of\npapers that have argued",
    "start": "2976660",
    "end": "2983260"
  },
  {
    "text": "against this 1 to 1 ratio. There's a notable one by-- I don't know how\nto pronounce this--",
    "start": "2983260",
    "end": "2988790"
  },
  {
    "text": "Bhojanapalli et\nal 2020, who have argued that if you have\nmore and more heads,",
    "start": "2988790",
    "end": "2994849"
  },
  {
    "text": "they're going to have\nlower and lower rank. And if you have very\nfew dimensions per head, that's going to start\naffecting the expressiveness",
    "start": "2994850",
    "end": "3001710"
  },
  {
    "text": "of the attention operation. But in practice,\nit doesn't really seem like we see too\nmany significant low rank",
    "start": "3001710",
    "end": "3008010"
  },
  {
    "text": "bottlenecks in practice. And most of the models with this\nratio of 1 seem to do just fine.",
    "start": "3008010",
    "end": "3013450"
  },
  {
    "text": "This is really a\nparameter that's generally been held constant\nby most of the models that we've seen.",
    "start": "3013450",
    "end": "3019137"
  },
  {
    "text": "If I have time, I'll\ntalk a little bit about different optimizations\nthat people have made on this multi-head component.",
    "start": "3019137",
    "end": "3025520"
  },
  {
    "text": "But hyperparameter wise, things\nhave stayed fairly similar.",
    "start": "3025520",
    "end": "3030710"
  },
  {
    "text": "I think one of the big ones\nin terms of hyperparameters is the aspect ratio.",
    "start": "3030710",
    "end": "3037100"
  },
  {
    "text": "So we can think\nabout deep networks. We can have more\nand more layers, or we can have wide networks.",
    "start": "3037100",
    "end": "3043200"
  },
  {
    "text": "And generally, if you want\none knob to control the width, that would be the\nhidden dimension",
    "start": "3043200",
    "end": "3048260"
  },
  {
    "text": "of the residual stream. That would control\nessentially the width of almost all the\noperations at once.",
    "start": "3048260",
    "end": "3053340"
  },
  {
    "text": "And so this seems like a\npretty critical thing to tune. You might think that\ndeeper networks are",
    "start": "3053340",
    "end": "3058700"
  },
  {
    "text": "smarter and more expressive,\nor wider networks are more efficient. There is generally a\nsweet spot of ratios",
    "start": "3058700",
    "end": "3065360"
  },
  {
    "text": "that people have picked. There have been outliers. Some of the early models used\nmuch smaller ratios here.",
    "start": "3065360",
    "end": "3072690"
  },
  {
    "text": "So what that means\nis that they were much wider than they were deep.",
    "start": "3072690",
    "end": "3078980"
  },
  {
    "text": "And then some models\nhave gone really deep where they had way more-- sorry,\nthe other way around-- really",
    "start": "3078980",
    "end": "3085299"
  },
  {
    "text": "wide, where they had way\nmore d model than n layer. And there's been generally\na sweet spot of saying,",
    "start": "3085300",
    "end": "3091250"
  },
  {
    "text": "we want about 128 hidden\ndimensions per layer. And that has been\ngenerally stuck to",
    "start": "3091250",
    "end": "3098380"
  },
  {
    "text": "by a lot of the GPT-3\nand LLaMA variant models. And I'll talk a little bit about\nevidence for that in a second.",
    "start": "3098380",
    "end": "3106720"
  },
  {
    "text": "There's considerations\nabout aspect ratio that are quite important. They will control the amount\nof parallelism that we can do.",
    "start": "3106720",
    "end": "3115339"
  },
  {
    "text": "So if you're doing something\ncalled pipeline parallel, what you're often\ngoing to do is you're",
    "start": "3115340",
    "end": "3120640"
  },
  {
    "text": "going to take your\ndifferent layers and you're going to\ncut them up and you're going to put them on different\ndevices or different blocks",
    "start": "3120640",
    "end": "3125950"
  },
  {
    "text": "of devices because you'll\nparallelize within each layer as well. And so there's going to be\ncertain kinds of constraints",
    "start": "3125950",
    "end": "3133950"
  },
  {
    "text": "that you're going to\nput on your model. And also, if you have\nreally wide models, then you can do something\ncalled tensor parallel where",
    "start": "3133950",
    "end": "3140310"
  },
  {
    "text": "you slice up the\nmatrices, and then you distribute those on GPUs. And one thing that we'll learn\nin, I think, 1, 2, 3, 4 or 5",
    "start": "3140310",
    "end": "3147870"
  },
  {
    "text": "lectures is that these different\nparallelism paradigms are going to have different constraints.",
    "start": "3147870",
    "end": "3153030"
  },
  {
    "text": "You need really fast\nnetworking for tensor parallel. And you can maybe get away with\nslower networking or higher",
    "start": "3153030",
    "end": "3160140"
  },
  {
    "text": "latency networking\nfor pipeline parallel. And so your\nnetworking constraints might, in turn, drive\nsome of these width-depth",
    "start": "3160140",
    "end": "3167580"
  },
  {
    "text": "considerations. But setting that aside,\nyou might abstractly ask, what is the impact\nof aspect ratio on model",
    "start": "3167580",
    "end": "3174619"
  },
  {
    "text": "performance? And once again, Kaplan et al\nhave a really nice visual aid",
    "start": "3174620",
    "end": "3180570"
  },
  {
    "text": "showing how aspect ratio\nimpacts performance. And so this is three\ndifferent scales-- 50 million, 274 million,\nand 1.5 billion parameters.",
    "start": "3180570",
    "end": "3189580"
  },
  {
    "text": "And the x-axis is aspect ratio. Y-axis is loss difference\nin percentage change.",
    "start": "3189580",
    "end": "3196490"
  },
  {
    "text": "And you see, that around\n100, which is, once again, I told you was around\nthe consensus",
    "start": "3196490",
    "end": "3202200"
  },
  {
    "text": "choice of hyperparameters,\nis the minimum across different scales. So this is backed by some of\nthis large scale hyperparameter",
    "start": "3202200",
    "end": "3210470"
  },
  {
    "text": "data that's been\npublished by Kaplan et al and roughly matches\nthat intuition. And a really nice\nthing here, is it",
    "start": "3210470",
    "end": "3216049"
  },
  {
    "text": "seems to be the case that\naspect ratio optima does not shift too much across several\norders of magnitude here.",
    "start": "3216050",
    "end": "3223560"
  },
  {
    "text": "So if this holds up even\nmore, that's very good news. You can keep training on\none fixed aspect ratio.",
    "start": "3223560",
    "end": "3230690"
  },
  {
    "text": "One thing I will know that is\nquite an interesting result is E Tay and others at Google had\nthis very interesting paper",
    "start": "3230690",
    "end": "3239600"
  },
  {
    "text": "studying the impact\nof depth versus width both upstream and downstream.",
    "start": "3239600",
    "end": "3244650"
  },
  {
    "text": "And one of the things\nthat they found was that if you're\nlooking at losses, then,",
    "start": "3244650",
    "end": "3250230"
  },
  {
    "text": "it doesn't really matter. Parameter is the only\nthing that matters. Deeper models don't help you. But the story is\nless clear if you're",
    "start": "3250230",
    "end": "3257460"
  },
  {
    "text": "looking at downstream accuracy. At the time, they were\nlooking at fine tuned superglue accuracy. They were arguing that for\nthe same amount of flops,",
    "start": "3257460",
    "end": "3264310"
  },
  {
    "text": "deeper models might be better. So I'll just leave it at that. There's not quite as much\nfollow up to this work,",
    "start": "3264310",
    "end": "3269818"
  },
  {
    "text": "at least in the\nopen, that I've seen. But downstream\nperformance may actually be slightly different in\nterms of the aspect ratio",
    "start": "3269818",
    "end": "3275880"
  },
  {
    "text": "considerations here. ",
    "start": "3275880",
    "end": "3281849"
  },
  {
    "text": "Cool. The final thing that\nI want to talk about in this very low level\nhyperparameter world",
    "start": "3281850",
    "end": "3288870"
  },
  {
    "text": "is what are the vocabulary sizes\nthat you might want to pick. And in general, vocabulary sizes\nhave been trending upwards.",
    "start": "3288870",
    "end": "3297510"
  },
  {
    "text": "And I think a big part of\nwhy is because LLMs are being deployed out in the wild.",
    "start": "3297510",
    "end": "3302830"
  },
  {
    "text": "They're becoming\nmore useful services. And when that\nhappens, you're going to interact with people speaking\ndifferent languages, people",
    "start": "3302830",
    "end": "3308690"
  },
  {
    "text": "using emojis, all sorts of\nother kinds of almost modalities or languages than\nwhat you might expect.",
    "start": "3308690",
    "end": "3315860"
  },
  {
    "text": "And so I think some\nof the earlier models, and especially\nmonolingual models, ranged around in the 30,000 to\n50,000 token vocabulary range.",
    "start": "3315860",
    "end": "3325950"
  },
  {
    "text": "And you can see this in\nGPTs, the early LLaMAs. But if you look at\nthe multilingual,",
    "start": "3325950",
    "end": "3331400"
  },
  {
    "text": "or I would call production\nsystems that have come out, they've all been shifting\ntowards the 100,000 to 250,000",
    "start": "3331400",
    "end": "3339109"
  },
  {
    "text": "range for their\nvocabulary sizes. And I looked at Command A,\nwhich is one of Cahere's models.",
    "start": "3339110",
    "end": "3345625"
  },
  {
    "text": "They're a company\nthat emphasizes a lot of multilingual stuff. You see very large\nvocab sizes from them.",
    "start": "3345625",
    "end": "3352220"
  },
  {
    "text": "Even with GPT-4 and many others\nthat have copied the GPT-4 tokenizer are going to\nbe around 100k tokens.",
    "start": "3352220",
    "end": "3359150"
  },
  {
    "text": "And so that's the standard that\na lot of people are operating at, roughly at 100k\nto 200k token size.",
    "start": "3359150",
    "end": "3367640"
  },
  {
    "text": "And I think there's\nbeen work showing that as models get bigger,\nthese models can, in some sense, handle more and more, or make\ngood use of more and more vocab",
    "start": "3367640",
    "end": "3377020"
  },
  {
    "text": "elements. And so you might see increasing\ntrends to token counts as models get scaled up\nor more and more data",
    "start": "3377020",
    "end": "3383380"
  },
  {
    "text": "is used to train them. Cool.",
    "start": "3383380",
    "end": "3388390"
  },
  {
    "text": "So the last thing, this is no\nlonger specific hyperparameters,",
    "start": "3388390",
    "end": "3394940"
  },
  {
    "text": "but two other things\nthat you might need to do before you\nset your model to run,",
    "start": "3394940",
    "end": "3400000"
  },
  {
    "text": "which is dropout and other\nkinds of regularization. And I think this one was\nreally interesting to me",
    "start": "3400000",
    "end": "3405670"
  },
  {
    "text": "when I was originally doing\nthe research for putting this lecture together. If you think about\npre-training, pre-training",
    "start": "3405670",
    "end": "3412230"
  },
  {
    "text": "is about the furthest\nplace that you might think of from regularization. Because pre-training,\nyou do usually one epoch.",
    "start": "3412230",
    "end": "3419819"
  },
  {
    "text": "You can't even go\nthrough all of your data because you have too much of it. So you're going to do\none epoch training,",
    "start": "3419820",
    "end": "3425130"
  },
  {
    "text": "and you're almost\ncertainly not overfitting the data in that one\npass that you're doing. And so you might think, we\ndon't need regularization",
    "start": "3425130",
    "end": "3432390"
  },
  {
    "text": "for pre-training. Let's just set your\noptimizer loose. It's all about minimizing loss.",
    "start": "3432390",
    "end": "3438030"
  },
  {
    "text": "And this is really\ngood arguments for why you shouldn't\nneed to regularize.",
    "start": "3438030",
    "end": "3444280"
  },
  {
    "text": "But then, if you look\nat what people do, the story is actually\nkind of mixed.",
    "start": "3444280",
    "end": "3451110"
  },
  {
    "text": "And this story actually\nis maybe even more mixed than what has turned out to be.",
    "start": "3451110",
    "end": "3456520"
  },
  {
    "text": "But early days, people\ndid a lot of dropout. And then there's a\nlot of weight decay",
    "start": "3456520",
    "end": "3462300"
  },
  {
    "text": "that also seems to be happening. And these days, I think,\na lot of the people have stopped publishing\ndetails on precisely they're",
    "start": "3462300",
    "end": "3469560"
  },
  {
    "text": "training hyperparameters. But dropout has\ngone out of fashion. But weight decay has really been\nsomething that a lot of people",
    "start": "3469560",
    "end": "3477270"
  },
  {
    "text": "continue to do. And why is that? That's a really odd\nthing to be doing.",
    "start": "3477270",
    "end": "3483015"
  },
  {
    "text": "So I'll give you a\nmoment to just think about this state of affairs. If you're training a\nreally large neural network",
    "start": "3483015",
    "end": "3489470"
  },
  {
    "text": "for one pass on SGD, on\nvast amounts of data, why would You use weight\ndecay when you're doing that?",
    "start": "3489470",
    "end": "3496230"
  },
  {
    "text": "So maybe some of the answer. But I think that's a\nkind of interesting thing to think about. It's very intuition\nviolating, at least for me.",
    "start": "3496230",
    "end": "3507030"
  },
  {
    "text": "So the reason is because it's\nnot to control overfitting.",
    "start": "3507030",
    "end": "3513650"
  },
  {
    "text": "In the sense that if you\nlook at weight decay, different amounts\nof weight decay don't really seem to change\nthe ratio of training loss",
    "start": "3513650",
    "end": "3521930"
  },
  {
    "text": "to validation loss. So you can train with different\namounts of weight decay if you train for long enough or\nyou control your hyperparameters",
    "start": "3521930",
    "end": "3528380"
  },
  {
    "text": "appropriately, you'll end\nup with the same train to val loss gap. So overfitting, nothing's\nhappening here, even",
    "start": "3528380",
    "end": "3534619"
  },
  {
    "text": "with zero weight decay. But what is interesting\nis that the weight decay seems to be interacting\nsomewhat in a strange way",
    "start": "3534620",
    "end": "3543130"
  },
  {
    "text": "with the learning rate\nschedules of the optimizers. And so what's happening\nis that if you",
    "start": "3543130",
    "end": "3549550"
  },
  {
    "text": "look at a constant\nlearning rate, so this is a model trained\non constant learning rate.",
    "start": "3549550",
    "end": "3555920"
  },
  {
    "text": "And then you suddenly decrease\nthe learning rate to near zero. So you see this drop off as\nyou decrease the learning rate.",
    "start": "3555920",
    "end": "3563440"
  },
  {
    "text": "And then let's look at\ndifferent kinds of weight decay that you could do. And what happens is\nwith weight decay,",
    "start": "3563440",
    "end": "3570250"
  },
  {
    "text": "the model is not training very\nwell at this high learning rate. And then when you decrease\nthe learning rate, it'll very rapidly drop off.",
    "start": "3570250",
    "end": "3577309"
  },
  {
    "text": "And when you look at cosine\nlearning rate decay, what happens is that the models\nwith high weight decay",
    "start": "3577310",
    "end": "3584200"
  },
  {
    "text": "start out very slow. But then as they cool down,\nthat is, their learning rate decreases, they very\nrapidly optimize.",
    "start": "3584200",
    "end": "3591460"
  },
  {
    "text": "And so there's some very complex\ninteraction happening here between the optimizer\nand the weight decay",
    "start": "3591460",
    "end": "3599130"
  },
  {
    "text": "and some of implicit\nacceleration that happens near the tail\nend of training that ends up",
    "start": "3599130",
    "end": "3604319"
  },
  {
    "text": "giving you better models. And so the answer\nto the question I posed you is you\ndon't weight decay",
    "start": "3604320",
    "end": "3610218"
  },
  {
    "text": "because you want to regularize\nthe model, which is kind of what it was designed for. Your weight decaying in order\nto get actually better training",
    "start": "3610218",
    "end": "3617220"
  },
  {
    "text": "losses. And you end up doing that\nbecause of the various learning dynamics at the tail end of\ntraining, as you decrease",
    "start": "3617220",
    "end": "3623910"
  },
  {
    "text": "your learning rates to 0. It's a very interesting\nand complex. And in some ways,\ntroubling thing",
    "start": "3623910",
    "end": "3631770"
  },
  {
    "text": "to be doing with\nlanguage models. But now, you see why, if you\nlook at a lot of the reports,",
    "start": "3631770",
    "end": "3637747"
  },
  {
    "text": "you'll see we use weight decay. This is why that\nends up happening. Cool. ",
    "start": "3637748",
    "end": "3645790"
  },
  {
    "text": "So putting all that together, so\nthere are certain things that I think are just no-brainer.",
    "start": "3645790",
    "end": "3652300"
  },
  {
    "text": "So if you're picking various\nhyperparameters for your model. You don't really need to\nthink too deeply about them",
    "start": "3652300",
    "end": "3657590"
  },
  {
    "text": "in the sense that\nthey've been validated and basically everyone\nelse does them. So this is things like\nthe hidden size of MLP,",
    "start": "3657590",
    "end": "3664250"
  },
  {
    "text": "the head dimensions of\nyour multi-head attention, your aspect ratio, and your\nchoice of regularization",
    "start": "3664250",
    "end": "3671480"
  },
  {
    "text": "through weight decay. All of those,\nthere's fairly good, I think, consensus\nevidence of how",
    "start": "3671480",
    "end": "3677000"
  },
  {
    "text": "to pick most of these\nhyperparameters. And those defaults roughly\ngive you the kinds of things that we suggest\nin the assignment.",
    "start": "3677000",
    "end": "3683100"
  },
  {
    "text": "So you can follow along,\nand they'll roughly give you something similar to this. So any questions about\nthe hyperparameter piece?",
    "start": "3683100",
    "end": "3693829"
  },
  {
    "text": "Yes. Is there a reason why\ndropouts gone out of fashion? That's a good question.",
    "start": "3693830",
    "end": "3699620"
  },
  {
    "text": "I don't think I've seen-- the question was, why did\ndropout go out of fashion? I haven't quite seen\na deep analysis of why",
    "start": "3699620",
    "end": "3707150"
  },
  {
    "text": "dropout is or isn't helpful. I haven't seen any result that,\nfor example, shows that it helps for training loss.",
    "start": "3707150",
    "end": "3713120"
  },
  {
    "text": "And both this paper argues\nand logic would dictate, there's not really a\ntraining overfitting issue",
    "start": "3713120",
    "end": "3720340"
  },
  {
    "text": "with these models\nthat can't even do one epoch over\ntheir training data. Yes.",
    "start": "3720340",
    "end": "3725500"
  },
  {
    "text": " Multilingual\nvocabularies actually",
    "start": "3725500",
    "end": "3730690"
  },
  {
    "text": "contribute to improve\nperformance in one language. So, [INAUDIBLE].",
    "start": "3730690",
    "end": "3737329"
  },
  {
    "text": "So the question was, do\nmultilingual vocabularies contribute to improving\nperformance in one language?",
    "start": "3737330",
    "end": "3742400"
  },
  {
    "text": "When you say one\nlanguage, you mean do multilingual or larger\nvocabularies help performance in English?",
    "start": "3742400",
    "end": "3747800"
  },
  {
    "text": "Is that the right question? Yeah. So I think in your high resource\nlanguage, the impact is less.",
    "start": "3747800",
    "end": "3754310"
  },
  {
    "text": "So if you're only thinking\nabout English language language modeling, you can get away\nwith smaller vocabularies.",
    "start": "3754310",
    "end": "3761360"
  },
  {
    "text": "This much is kind of true. But the place where larger\nvocabularies is really helpful",
    "start": "3761360",
    "end": "3767140"
  },
  {
    "text": "is when you're\nstarting to get at, I wouldn't say the tail\nof your distribution, but when you get to languages\nthat are more minority.",
    "start": "3767140",
    "end": "3773650"
  },
  {
    "text": "And one great example\nof this, if you look at any of the\nCohere announcements about their models\nor their tokenizers,",
    "start": "3773650",
    "end": "3780300"
  },
  {
    "text": "they basically always argue\nthat because of the way they have larger\nvocabularies and the way they train their tokenizer,\nnon-English and low resource",
    "start": "3780300",
    "end": "3788550"
  },
  {
    "text": "languages, they are packed\ninto much fewer tokens. And so people using\nthose pay much lower cost",
    "start": "3788550",
    "end": "3797190"
  },
  {
    "text": "at inference time, which\nis a great benefit. Yes, question? [INAUDIBLE] if\nweight decay doesn't",
    "start": "3797190",
    "end": "3805170"
  },
  {
    "text": "have a significant impact\non the vali loss, why do we care about the training dynamics\nor the favorable [INAUDIBLE]",
    "start": "3805170",
    "end": "3810960"
  },
  {
    "text": "dynamics? So the question\nwas, if it doesn't have an impact on\nvali loss, why do we care about training dynamics?",
    "start": "3810960",
    "end": "3816630"
  },
  {
    "text": "The goal is still, I want\nto get good training loss. This is the game\nthat we're playing.",
    "start": "3816630",
    "end": "3823000"
  },
  {
    "text": "And the surprising\nthing about weight decay is that somehow, it gets\nus better training losses. I think the intuitive\nthing that makes",
    "start": "3823000",
    "end": "3828900"
  },
  {
    "text": "sense is, you do weight decay. It gives you better vali losses. But that's not what happens. What it's getting you is better\ntraining losses, which are also",
    "start": "3828900",
    "end": "3835990"
  },
  {
    "text": "the same as vali losses.  Yes.",
    "start": "3835990",
    "end": "3842230"
  },
  {
    "text": "Are there differences in the\narchitecture hyperparameter choices people make\nas they move towards",
    "start": "3842230",
    "end": "3847930"
  },
  {
    "text": "multimodal\narchitectures, if they're doing images as well as text? So the question was\nabout multimodal models.",
    "start": "3847930",
    "end": "3854990"
  },
  {
    "text": "That is a great question. My survey of multimodal\nmodels is very incomplete.",
    "start": "3854990",
    "end": "3860470"
  },
  {
    "text": "What I can say is, a lot of the\nacademic and open work that I've seen, they do what\nyou might call",
    "start": "3860470",
    "end": "3866710"
  },
  {
    "text": "like shallow or later\nfusion or earlier fusion of the modalities.",
    "start": "3866710",
    "end": "3871760"
  },
  {
    "text": "And the way that works is\nyou bolt the vision modality onto an existing language model. In those cases, the\nhyperparameter and architecture",
    "start": "3871760",
    "end": "3878110"
  },
  {
    "text": "choices are fixed. One thing I will\nnote, and I will talk about this in\njust a few slides,",
    "start": "3878110",
    "end": "3883960"
  },
  {
    "text": "is that the multimodal\nmodels pioneered some pretty interesting\ntechniques in stabilizing",
    "start": "3883960",
    "end": "3889740"
  },
  {
    "text": "language model training. And that's been a\nreally big theme, and I'll talk a little\nbit about those. So what is different is often\nwhen you bolt on this new vision",
    "start": "3889740",
    "end": "3897690"
  },
  {
    "text": "piece and you retrain with that,\nthat's a big shock to the model. And so you have\nto think carefully about how to stabilize\nthat training process.",
    "start": "3897690",
    "end": "3904150"
  },
  {
    "text": "And those innovations\nhave actually seeped back into pure text language\nmodel training. ",
    "start": "3904150",
    "end": "3911613"
  },
  {
    "text": "Cool.  So I went back\nthrough and I looked",
    "start": "3911613",
    "end": "3919200"
  },
  {
    "text": "through all these new papers. And as I was trying\nto think about, OK, what's been new\nin the last year? And what new architecture and\nrelated things have happened?",
    "start": "3919200",
    "end": "3927039"
  },
  {
    "text": "Actually, the core architecture\nhasn't changed much. But I think the one\nthing that stood out as being very emphasized\nin a lot of the releases",
    "start": "3927040",
    "end": "3935640"
  },
  {
    "text": "has been what I would\ncall stability tricks. And so these are\nthings where you",
    "start": "3935640",
    "end": "3940829"
  },
  {
    "text": "would like to train your model\nin much more stable ways. And as you make bigger\nand bigger models,",
    "start": "3940830",
    "end": "3946830"
  },
  {
    "text": "you train for longer and longer. These kinds of issues start\nto appear more and more. So I've taken this\nfrom the OLMo 2 paper.",
    "start": "3946830",
    "end": "3955400"
  },
  {
    "text": "And actually, that paper is a\ngreat set of academic results on LLM training stability.",
    "start": "3955400",
    "end": "3962660"
  },
  {
    "text": "And one thing they start\nwith is this figure. And you look at this\nblue curve over here.",
    "start": "3962660",
    "end": "3968520"
  },
  {
    "text": "And you look at this, L2\nnorm of the gradient graph. And this is terrifying\ngraph to look at.",
    "start": "3968520",
    "end": "3974730"
  },
  {
    "text": "Your loss curve kind of\nseems to be behaving OK. But you've got some bad\nspikes every now and then.",
    "start": "3974730",
    "end": "3981269"
  },
  {
    "text": "And you open up\nyour gradient norm, and it's this\nhorrible plot where you've got spikes everywhere,\nwhere your norms are completely",
    "start": "3981270",
    "end": "3987500"
  },
  {
    "text": "blowing up. And if you're training\nmodels like this, you're going to have a\nreally tough time getting",
    "start": "3987500",
    "end": "3993500"
  },
  {
    "text": "it to converge reasonably. At some point,\nit's going to hit-- gradient norm explodes,\nand you can't do anything.",
    "start": "3993500",
    "end": "3999680"
  },
  {
    "text": "And your training is done so\nyou can't train any further. And so there's been a lot\nof emphasis basically trying",
    "start": "3999680",
    "end": "4005760"
  },
  {
    "text": "to turn this blue curve\ninto something that looks a lot like the orange curve. And of course, this loss is\nhigher, but ignore that fact",
    "start": "4005760",
    "end": "4012210"
  },
  {
    "text": "because I think\nthey just switched data sets in between\nthese two training runs. But this orange curve has nice\nlow gradient norms throughout.",
    "start": "4012210",
    "end": "4019943"
  },
  {
    "text": "And that's really\nthe kind of plot that you would much rather see. And so you might ask, where\ndo stability issues arise",
    "start": "4019943",
    "end": "4027450"
  },
  {
    "text": "in transformers? And of course, they can\narise basically everywhere. But if you look at the\nkind of interventions",
    "start": "4027450",
    "end": "4033960"
  },
  {
    "text": "that people are\nmaking, there's really one place that really stands out\nas the kind of problem child.",
    "start": "4033960",
    "end": "4040180"
  },
  {
    "text": "And that's the softmaxes. And it can be a problem\nbecause you're going to be taking exponentials.",
    "start": "4040180",
    "end": "4045700"
  },
  {
    "text": "And those can be\nnumerically badly behaved. You're also dividing\ntwo numbers.",
    "start": "4045700",
    "end": "4050837"
  },
  {
    "text": "And so you might\nhave a division by 0. So for many different\nreasons, this softmax piece is a part that you might\nhave lots of issues with.",
    "start": "4050837",
    "end": "4060329"
  },
  {
    "text": "And so, actually, one more\nthing I want to talk about. So where are the softmaxes\nin a transformer?",
    "start": "4060330",
    "end": "4066089"
  },
  {
    "text": "Well, there's one\nat the very end. So you've got to be careful\nabout that output softmax. And also, there's softmaxes\nin your self-attention.",
    "start": "4066090",
    "end": "4073740"
  },
  {
    "text": "So there's two softmaxes\nthat we're going to think a little bit about. And for each one,\nI'm going to mention",
    "start": "4073740",
    "end": "4079069"
  },
  {
    "text": "stability intervention\nthat has generally seemed to be effective.",
    "start": "4079070",
    "end": "4085290"
  },
  {
    "text": "So the first one is\ncalled the z-loss. And it might desire to\ncite a paper that's older.",
    "start": "4085290",
    "end": "4091440"
  },
  {
    "text": "I've gone back to\nDevlin in 2014, where in a machine\ntranslation paper,",
    "start": "4091440",
    "end": "4098120"
  },
  {
    "text": "their goal was to try to make\nsure that this normalizer was near 1.",
    "start": "4098120",
    "end": "4103710"
  },
  {
    "text": "So if you look at p of x, that's\nthe output softmax over here. The output softmax is two terms.",
    "start": "4103710",
    "end": "4109649"
  },
  {
    "text": "You exponentiate your\nlogits, and then you divide by the normalizer z. And the z is just summing up\nthe values across all the vocab.",
    "start": "4109649",
    "end": "4117540"
  },
  {
    "text": "And so if you want\nthis z of x, you want to train the network to\nhave a z of x close to one.",
    "start": "4117540",
    "end": "4123620"
  },
  {
    "text": "Well, then you can\nrewrite your loss, and you can add a\nlittle second term here to try to force log of z\nof xi to be close to 0.",
    "start": "4123620",
    "end": "4132109"
  },
  {
    "text": "So you're going to end\nup with an auxiliary loss term that's alpha log\nsquared z of xi right.",
    "start": "4132109",
    "end": "4137479"
  },
  {
    "text": "You can see that derivation\non the right here. And this is in some sense what\npeople often call the z-loss.",
    "start": "4137479",
    "end": "4144670"
  },
  {
    "text": "I think Jacob Devlin\nand others did this for machine translation\nfor totally different reasons than what it's used for today.",
    "start": "4144670",
    "end": "4151359"
  },
  {
    "text": "But this was, I think, the first\ninstance of this in language modeling land was PaLM, who\nused this, as they called it,",
    "start": "4151359",
    "end": "4158239"
  },
  {
    "text": "auxiliary loss of z-loss\n10 to the negative 4 log squared z to basically\nencourage the softmax normalizer",
    "start": "4158240",
    "end": "4164680"
  },
  {
    "text": "to behave nicely. And you can reason through the\nbehavior of this regularizer.",
    "start": "4164680",
    "end": "4169960"
  },
  {
    "text": "If it succeeds and it forces\nlog of z of x to always be 0, then the log and the\nexponential cancels,",
    "start": "4169960",
    "end": "4177989"
  },
  {
    "text": "and you've basically\njust got U of r of x. And that's a good place to be. That's a nice numerically\nstable operation.",
    "start": "4177990",
    "end": "4183790"
  },
  {
    "text": "So all of these problematic\noperations go away. And so you can\nthink of the softmax as being well-behaved\nwhen z of x is close to 1.",
    "start": "4183790",
    "end": "4192660"
  },
  {
    "text": "Or log of z is close to 0. And PaLM, in some sense,\nis very much a pioneer.",
    "start": "4192660",
    "end": "4198960"
  },
  {
    "text": "Because they did this z\nloss trick, and many others didn't really do it for a long\ntime, or at least the ones",
    "start": "4198960",
    "end": "4204929"
  },
  {
    "text": "that had open papers. But then there was\na kind of sequence of papers that have done this. Baichuan 2 is\nactually the earliest",
    "start": "4204930",
    "end": "4212010"
  },
  {
    "text": "follow up that I know of. And then DCLM and OLMo\n2 and now several others have basically\npicked up on z-loss.",
    "start": "4212010",
    "end": "4217949"
  },
  {
    "text": "So a very nice, convenient\nintervention for improving stability.",
    "start": "4217950",
    "end": "4223500"
  },
  {
    "text": "And then, the other\ntrick that we see, so that was how to stabilize\nthe output softmax.",
    "start": "4223500",
    "end": "4231250"
  },
  {
    "text": "But we've got another softmax\nwe've got to deal with. The other softmax\nwe have to deal with is in the attention operation.",
    "start": "4231250",
    "end": "4238440"
  },
  {
    "text": "And so this is from\nan NVIDIA paper. I forgot to put the\ncitation marker.",
    "start": "4238440",
    "end": "4244050"
  },
  {
    "text": "But here, this is a block\ndiagram of how attention works. You got your layer\nnorm at the beginning.",
    "start": "4244050",
    "end": "4250810"
  },
  {
    "text": "You got your QKVs. Ignore this for the moment. You might multiply\nyour Qs and your Ks.",
    "start": "4250810",
    "end": "4256870"
  },
  {
    "text": "You'll softmax it. You'll multiply the V, and\nthen you'll project it. And then that's going to\ngive you your fully connected",
    "start": "4256870",
    "end": "4262890"
  },
  {
    "text": "and your output. So if you ignore this\nlittle piece over here, this looks just like your normal\nmulti-head attention operation.",
    "start": "4262890",
    "end": "4271140"
  },
  {
    "text": "So what's the difference here? So several folks came up with\nthis idea or this approach",
    "start": "4271140",
    "end": "4278460"
  },
  {
    "text": "called the QK norm, where you\ntake the queries and the keys, and you pass them\nthrough a LayerNorm layer",
    "start": "4278460",
    "end": "4284640"
  },
  {
    "text": "before you take their inner\nproduct for the softmax operation. And this is a very\ndifferent kind",
    "start": "4284640",
    "end": "4290670"
  },
  {
    "text": "of approach to controlling\nthe behavior of the softmax. Here, you're not controlling\nthe normalizer z.",
    "start": "4290670",
    "end": "4297260"
  },
  {
    "text": "Instead, you're controlling\nthe inputs to the softmax to be bounded in size. And that's going to naturally\ncontrol the bad behaviors",
    "start": "4297260",
    "end": "4305440"
  },
  {
    "text": "of the softmax. And as I said before,\nthis is originally an innovation from the vision\nand multimodal model community.",
    "start": "4305440",
    "end": "4313510"
  },
  {
    "text": "Dehgani in 2023, this\nwas a paper on training, very large vision transformers.",
    "start": "4313510",
    "end": "4319180"
  },
  {
    "text": "And then Chameleon and\nIdefcs from Hugging Face use these tricks for their\nmulti-modal training components.",
    "start": "4319180",
    "end": "4327550"
  },
  {
    "text": "And then it got picked\nup by several others like Gemma 2, DCLM,\nOLMo 2, all basically",
    "start": "4327550",
    "end": "4334390"
  },
  {
    "text": "uses this kind of\ntechniques in order to stabilize their training.",
    "start": "4334390",
    "end": "4341290"
  },
  {
    "text": "And I think I'm allowed to\nadd one joke per lecture. And so this is the one\nI'm going to go with here.",
    "start": "4341290",
    "end": "4347470"
  },
  {
    "text": "I think one of the things\nthat really has stood out in terms of stability\ninterventions has been just how strikingly\neffective LayerNorms are.",
    "start": "4347470",
    "end": "4355480"
  },
  {
    "text": "So we've seen going from\nLayerNorms just in the pre part of the block to\nboth the beginning",
    "start": "4355480",
    "end": "4363240"
  },
  {
    "text": "and the end of the non\nresidual component. And now, we've also thrown it\ninto the Q and the K component.",
    "start": "4363240",
    "end": "4369510"
  },
  {
    "text": "At least in terms of\nimproving stability, LayerNorms have been shockingly\neffective without affecting",
    "start": "4369510",
    "end": "4375300"
  },
  {
    "text": "performance too much. The last trick that\nI'll note, I think this one has been not\nquite as frequently used,",
    "start": "4375300",
    "end": "4384750"
  },
  {
    "text": "which is to soft-cap the logits\nthat go into the softmax.",
    "start": "4384750",
    "end": "4390610"
  },
  {
    "text": "So the other approach that you\ncan take, so QK norm is in some sense a very heavy\nhanded intervention",
    "start": "4390610",
    "end": "4395910"
  },
  {
    "text": "because we're going to operate\nover the entire vector. But one thing you\ncould do is after you take the inner products\nfor self-attention,",
    "start": "4395910",
    "end": "4403000"
  },
  {
    "text": "you could pass them through\na soft maximum operation. So you can pass them through\nthis equation over here.",
    "start": "4403000",
    "end": "4409210"
  },
  {
    "text": "So you have your\nlogits as your input. Divide it by the soft cap. Multiply by the soft cap.",
    "start": "4409210",
    "end": "4414650"
  },
  {
    "text": "What does that do? Well, if your logits start\nexceeding the soft cap by a lot, the tanh is going to\nclip them off to one.",
    "start": "4414650",
    "end": "4421310"
  },
  {
    "text": "And so you're going to\nhave a maximum value of soft cap over here. So this is going to control,\nin some sense, soft clipping",
    "start": "4421310",
    "end": "4428830"
  },
  {
    "text": "of the logits and Gemma 2, and\nI think OLMo 2 also do this.",
    "start": "4428830",
    "end": "4434290"
  },
  {
    "text": "It hasn't been I think\nquite as popular otherwise. And I think the other evidence\nagainst this, the NVIDIA",
    "start": "4434290",
    "end": "4442690"
  },
  {
    "text": "folks that I mentioned\nearlier, did actually quite a few different stability\nimproving interventions.",
    "start": "4442690",
    "end": "4448670"
  },
  {
    "text": "And what they find is you have\nyour baseline model over here. This is the perplexity of\nthe baseline model, 11.19.",
    "start": "4448670",
    "end": "4455450"
  },
  {
    "text": "Soft capping makes it worse. QK norm actually makes\nit better because you can use more aggressive\nlearning rates",
    "start": "4455450",
    "end": "4460630"
  },
  {
    "text": "and push the optimizer further.  Cool.",
    "start": "4460630",
    "end": "4466370"
  },
  {
    "text": "So that's the end of the\nstability improving intervention stuff.",
    "start": "4466370",
    "end": "4471450"
  },
  {
    "text": "Does anyone have any questions? I think that's been kind\nof the new development over the last year.",
    "start": "4471450",
    "end": "4477040"
  },
  {
    "text": "Yes. So for the QKV\nnorm, I understand that during training,\nyou will have the LayerNorm being applied.",
    "start": "4477040",
    "end": "4482910"
  },
  {
    "text": "At inference time, it's the\nlayer norm still being kept? Yes. So the question was,\nat inference time,",
    "start": "4482910",
    "end": "4488200"
  },
  {
    "text": "do you still use the norm? And the answer is yes. Because the layer norm\nhas learned parameters.",
    "start": "4488200",
    "end": "4493470"
  },
  {
    "text": "Like the whole\naction of the layer norm is it takes an activation,\nnormalizes it to a unit, and then scales\nthem to some size.",
    "start": "4493470",
    "end": "4499900"
  },
  {
    "text": "If you take that out, that's\na huge change to the model. It will have no\nidea what to do with those unnormalized activations.",
    "start": "4499900",
    "end": "4505266"
  },
  {
    "text": " Cool. ",
    "start": "4505266",
    "end": "4512440"
  },
  {
    "text": "So I have this last\nbit, last few slides that I want to end with.",
    "start": "4512440",
    "end": "4517960"
  },
  {
    "text": "If we go over\nthen, we can always push this into the MOE lecture. But I think we also have a lot\nof content next time because I",
    "start": "4517960",
    "end": "4525030"
  },
  {
    "text": "have to cover deep seq V3. So the last thing\nI want to cover is variations on\nthe attention heads.",
    "start": "4525030",
    "end": "4532489"
  },
  {
    "text": "So attention heads,\nI think, haven't had as much work done to them.",
    "start": "4532490",
    "end": "4537980"
  },
  {
    "text": "But there have been a few,\nI think, important changes that you need to\nknow about in order to understand the models\nthat are being trained.",
    "start": "4537980",
    "end": "4544950"
  },
  {
    "text": "So the one thing\nI'll talk about-- the first thing I'll talk\nabout is GQA and MQA. And these aren't really\ncritical to the training time",
    "start": "4544950",
    "end": "4552512"
  },
  {
    "text": "behavior of the\nmodels, but they're very important in understanding\nthe inference costs and inference behavior\nof the models.",
    "start": "4552512",
    "end": "4558840"
  },
  {
    "text": "And because this is an\nimportant architecture change, I'll mention them\nhere in addition to probably being mentioned\nby Percy in some",
    "start": "4558840",
    "end": "4565070"
  },
  {
    "text": "of the inference lectures. The other thing that's a\nnew development I'll mention is how the most recent models,\nlike LLaMA 4, if you've",
    "start": "4565070",
    "end": "4572930"
  },
  {
    "text": "heard of it, supports supposedly\n10 million tokens of context. How does it do that? Well, it does so by messing\nwith the attention pattern",
    "start": "4572930",
    "end": "4580340"
  },
  {
    "text": "in very structured ways. So I'll talk about that as well.",
    "start": "4580340",
    "end": "4586570"
  },
  {
    "text": "So GQA and MQA, if you looked\nat some of the larger models,",
    "start": "4586570",
    "end": "4591650"
  },
  {
    "text": "like the big LLaMA\nmodels or others, you'll have heard or seen\nthis term GQA or MQA.",
    "start": "4591650",
    "end": "4597250"
  },
  {
    "text": "And I'll talk through\nwhat that means. So to set the stage, let's\nthink about the compute",
    "start": "4597250",
    "end": "4603309"
  },
  {
    "text": "that you need to do attention. So this is, once again,\n224N slides here.",
    "start": "4603310",
    "end": "4608560"
  },
  {
    "text": "You're going to take your\nXQ, your query, and your XK. And then you're going to form\nyour big quadratic attention",
    "start": "4608560",
    "end": "4615370"
  },
  {
    "text": "matrix. And you can walk through each\nof these matrix multiplies. And you can convince yourself\nthat the total number",
    "start": "4615370",
    "end": "4621400"
  },
  {
    "text": "of arithmetic operations is\ngoing to be b times n times d squared.",
    "start": "4621400",
    "end": "4626449"
  },
  {
    "text": "So that's going to be b\nis the batch dimension. n is the sequence length.",
    "start": "4626450",
    "end": "4631760"
  },
  {
    "text": "And d squared is going to be\nthe hidden dimension squared.",
    "start": "4631760",
    "end": "4637119"
  },
  {
    "text": "And you can ask about the\ntotal memory accesses. And this is going to\nbe b times n times d.",
    "start": "4637120",
    "end": "4642310"
  },
  {
    "text": "And this is going to be,\nfor example accessing just this matrix here. This XQ is going\nto be that size.",
    "start": "4642310",
    "end": "4648370"
  },
  {
    "text": "And then the softmax is going\nto be b times h times n squared. And you can convince\nyourself of that by just thinking about the size\nof the softmax matrix, which",
    "start": "4648370",
    "end": "4656370"
  },
  {
    "text": "is going to be batch\ntimes number of heads times all of the\ndifferent softmax activations that you have.",
    "start": "4656370",
    "end": "4661929"
  },
  {
    "text": "So that's n squared of them. And you've got a\nprojection, and you've got d squared projection\noperations at the very end",
    "start": "4661930",
    "end": "4668699"
  },
  {
    "text": "over here. And so we can take the ratio\nof total memory accesses",
    "start": "4668700",
    "end": "4674490"
  },
  {
    "text": "and arithmetic operations. And this is going\nto be something that will be very important\nin a couple of lectures,",
    "start": "4674490",
    "end": "4681010"
  },
  {
    "text": "this idea called\narithmetic intensity. So we want our arithmetic\nintensity to be high.",
    "start": "4681010",
    "end": "4686530"
  },
  {
    "text": "What that means\nis, we want to be doing a lot of compute for every\nsingle memory access that we do.",
    "start": "4686530",
    "end": "4692440"
  },
  {
    "text": "And this is going to be\nbecause memory accesses are very expensive on a GPU,\nrelatively speaking. And compute is relatively cheap.",
    "start": "4692440",
    "end": "4699710"
  },
  {
    "text": "And so in this batch computation\nthat I'm showing you here, the arithmetic intensity, if\nyou take the ratio of those two",
    "start": "4699710",
    "end": "4706220"
  },
  {
    "text": "things, is going to be 1 over\nk plus 1 over bn inverse.",
    "start": "4706220",
    "end": "4711450"
  },
  {
    "text": "And so this is going\nto mean that we can keep our GPUs running. ",
    "start": "4711450",
    "end": "4718100"
  },
  {
    "text": "Because if we have of\nlarge number of heads and we have large batch size\nand large sequence length,",
    "start": "4718100",
    "end": "4724040"
  },
  {
    "text": "those are all going to\nbe good large numbers. Of course, this is what\nhappens at training time.",
    "start": "4724040",
    "end": "4731000"
  },
  {
    "text": "So the issue is\nthat inference time, we do not have these big chunky\nmatrices to multiply together.",
    "start": "4731000",
    "end": "4736380"
  },
  {
    "text": "And so that's going\nto really change the nature of the behavior\nof our algorithms. So when we're generating\ntext, remember that we",
    "start": "4736380",
    "end": "4744710"
  },
  {
    "text": "have to generate a token. And then the transformer\nhas to read that token. And then it has to process it.",
    "start": "4744710",
    "end": "4749880"
  },
  {
    "text": "And now, we can get the\nnext token distribution. And then we do the things\nautoregressively one token at a time.",
    "start": "4749880",
    "end": "4754920"
  },
  {
    "text": "And by doing this, we can't\nparallelize this generation process. We need to go step by step\nfor every single new token.",
    "start": "4754920",
    "end": "4761980"
  },
  {
    "text": "And when we do this, we're going\nto need to incrementally compute attention, an idea that\npeople call the KV cache.",
    "start": "4761980",
    "end": "4769150"
  },
  {
    "text": "And so what do you do? This is a lovely animation of a\nKV cache that's been explained.",
    "start": "4769150",
    "end": "4775070"
  },
  {
    "text": "So if you can look at this\nfigure, what you're doing is you've got a query token.",
    "start": "4775070",
    "end": "4782800"
  },
  {
    "text": "A query token here is you've\ngenerated a new token. You're conditioning on it. And now, you want to ask,\nwhat information should I",
    "start": "4782800",
    "end": "4789430"
  },
  {
    "text": "look up in the past,\npast that query token? And your query tokens\nare shifting from 1 through n because you're\ngenerating new tokens",
    "start": "4789430",
    "end": "4796090"
  },
  {
    "text": "one at a time. You're building up this\nkey cache over here, where, basically, I'm building\nup all of the past tokens keys.",
    "start": "4796090",
    "end": "4804590"
  },
  {
    "text": "And the past tokens keys\ndon't change because they only depend on things in the past. And so, incrementally, as I\ngenerate tokens, building up",
    "start": "4804590",
    "end": "4812590"
  },
  {
    "text": "all of these past keys. And each time, I can compute\none new element of Q dot K.",
    "start": "4812590",
    "end": "4818490"
  },
  {
    "text": "So the big attention\nmatrix is going to be this lower\ntriangular matrix. I'm computing one row at a time.",
    "start": "4818490",
    "end": "4824590"
  },
  {
    "text": "And that row is exactly\nwhat's necessary to generate the next token. So this KV cache idea, if\nyou've not seen this before,",
    "start": "4824590",
    "end": "4831995"
  },
  {
    "text": "is this idea of\nsaying, I'm going to generate the K's and the\nV's incrementally as I go,",
    "start": "4831995",
    "end": "4837310"
  },
  {
    "text": "as I generate each token. And I'm only going to\ncompute QK that's absolutely",
    "start": "4837310",
    "end": "4842700"
  },
  {
    "text": "necessary to do my operations. And so once again,\nyou can go through",
    "start": "4842700",
    "end": "4847860"
  },
  {
    "text": "and do the various\narithmetic components of how many flops do we do?",
    "start": "4847860",
    "end": "4854650"
  },
  {
    "text": "What's the total number\nof memory accesses? And if you think\nabout the KV cache, I'm only multiplying the\nabsolute necessary keys",
    "start": "4854650",
    "end": "4862200"
  },
  {
    "text": "and values. Since I'm saving all of the\nintermediate computations, I'm not wasting any matrix\nor vector vector multiplies.",
    "start": "4862200",
    "end": "4870480"
  },
  {
    "text": "The total number of\narithmetic operations remains exactly the\nsame, bnd squared.",
    "start": "4870480",
    "end": "4875640"
  },
  {
    "text": "But the memory access\npatterns are now different. Why is that? Because when I do\nthis KV caching thing,",
    "start": "4875640",
    "end": "4883170"
  },
  {
    "text": "I'm going to have to move\nvarious kinds of parameters in and out of memory repeatedly. Whenever I multiply\nwith a key matrix,",
    "start": "4883170",
    "end": "4891510"
  },
  {
    "text": "I'm going to have to\nput that into memory, and then multiply by K. And\nthen I need to put that away, and I need to compute\nsome activations.",
    "start": "4891510",
    "end": "4897690"
  },
  {
    "text": "And so I'm repeatedly loading\nin different matrices. And that's going to give me a\nmuch higher total memory access",
    "start": "4897690",
    "end": "4902960"
  },
  {
    "text": "of b squared d plus nd squared. And so when you take this ratio,\nnow, the arithmetic intensity",
    "start": "4902960",
    "end": "4909500"
  },
  {
    "text": "is not so good. You're going to get n over\nd plus 1 over b inverse.",
    "start": "4909500",
    "end": "4914840"
  },
  {
    "text": "And so if we reason\nthrough this-- so if I want arithmetic\nintensity to be high,",
    "start": "4914840",
    "end": "4920300"
  },
  {
    "text": "I want this thing\ninside to be very small. So I need really large batches. And I need n over d to be small.",
    "start": "4920300",
    "end": "4927337"
  },
  {
    "text": "What does that mean? I need really short sequence\nlengths or really big model dimensions.",
    "start": "4927337",
    "end": "4932389"
  },
  {
    "text": "And this n over d is\nreally unfavorable because I don't\nwant a bigger model, and I don't want a\nshorter sequence length.",
    "start": "4932390",
    "end": "4939349"
  },
  {
    "text": "And so this is the\ncore, in some sense, inference cost trade\noff that people face.",
    "start": "4939350",
    "end": "4944360"
  },
  {
    "text": "You have this very bad\nmemory access pattern where you have this\none term, n over d,",
    "start": "4944360",
    "end": "4949760"
  },
  {
    "text": "that's really\nkilling you in terms of the throughput\nof your system.",
    "start": "4949760",
    "end": "4955060"
  },
  {
    "text": "And so this motivates\nthis thing called MQA. And the key idea\nhere, hopefully, you",
    "start": "4955060",
    "end": "4961840"
  },
  {
    "text": "see from this figure\nback here that, really, the part that's really bad\nis the keys and the values.",
    "start": "4961840",
    "end": "4969020"
  },
  {
    "text": "They have this KV cache\nthing being built up, and there's memory\nmoving in and out. So what you do is you can have\nmultiple heads for the query,",
    "start": "4969020",
    "end": "4976940"
  },
  {
    "text": "multiple query heads, but\nonly one dimension or one head for the keys and values.",
    "start": "4976940",
    "end": "4982430"
  },
  {
    "text": "This immensely\nsimplifies things. Once you do this, now, you're\nmoving much less information",
    "start": "4982430",
    "end": "4987580"
  },
  {
    "text": "for the K's and the V's. And so K and V is shared. But query has many heads.",
    "start": "4987580",
    "end": "4993760"
  },
  {
    "text": "And so you still have multi-head\nattention or multiple queries, but only single K's and V's.",
    "start": "4993760",
    "end": "5000119"
  },
  {
    "text": "So that's why it's called\nmulti-query attention. And now, when you do the same\nkind of arithmetic we have fewer",
    "start": "5000120",
    "end": "5005540"
  },
  {
    "text": "memory accesses because we've\nshared the K's and the V's. And the arithmetic intensity\nis much, much better behaved.",
    "start": "5005540",
    "end": "5011400"
  },
  {
    "text": "And so we can\nincrease things like-- we've decreased the first\nterm by a factor of n.",
    "start": "5011400",
    "end": "5016830"
  },
  {
    "text": "So longer sequence\nlengths are now viable. And the second term is now\ndivided by the number of heads.",
    "start": "5016830",
    "end": "5021900"
  },
  {
    "text": "So this term is also\nnot so terrible. So all the different\nterms are controlled now. And MQA can give you\nmuch better behaviors.",
    "start": "5021900",
    "end": "5030469"
  },
  {
    "text": "GQA or group query attention\nbasically changes this slightly. Instead of having single query--",
    "start": "5030470",
    "end": "5038570"
  },
  {
    "text": "or sorry, multiple\nquery and single key, you can reduce the number\nof keys by some multiple.",
    "start": "5038570",
    "end": "5044670"
  },
  {
    "text": "And so this will\nlet you trade off between the inference time\nbehaviors and the expressiveness",
    "start": "5044670",
    "end": "5049748"
  },
  {
    "text": "of the model. Because maybe going\nfrom multi-head all the way to multi-query is\na little bit too aggressive.",
    "start": "5049748",
    "end": "5057480"
  },
  {
    "text": "Some works show that\nGQA doesn't hurt, but multi-head attention hurts.",
    "start": "5057480",
    "end": "5063460"
  },
  {
    "text": "I'm not going to get into that. I'm just going to close off\nwith this very last thing, which I think is a really\ninteresting development",
    "start": "5063460",
    "end": "5070230"
  },
  {
    "text": "in the last few months. So back in 2019, OpenAI had this\ncool paper basically arguing",
    "start": "5070230",
    "end": "5078210"
  },
  {
    "text": "how to build longer\nattention models. And they were basically\narguing, well, one way to do that is to come up\nwith sparse attention patterns.",
    "start": "5078210",
    "end": "5087150"
  },
  {
    "text": "So instead of paying\nattention to all the sequence, I'm going to pay\nattention to, let's say, a local window at each chunk.",
    "start": "5087150",
    "end": "5093040"
  },
  {
    "text": "And then I can have\nother attention patterns that are like\ndiagonals that help",
    "start": "5093040",
    "end": "5098610"
  },
  {
    "text": "propagate information across. So you can build sparse\nor structured attention that trades off various kinds of\nexpressiveness versus runtime.",
    "start": "5098610",
    "end": "5105820"
  },
  {
    "text": "GPT-3 three uses\nexactly these kinds of tricks when they\noriginally released it to get",
    "start": "5105820",
    "end": "5110840"
  },
  {
    "text": "larger attention windows. Sliding window attention\nis another variant of this idea, where\nat each layer,",
    "start": "5110840",
    "end": "5118650"
  },
  {
    "text": "you only pay attention\nto a small region around your current position. And this also is going to\ncontrol the total amount",
    "start": "5118650",
    "end": "5126680"
  },
  {
    "text": "of resources that you need-- total amount of resources\nyou need in order",
    "start": "5126680",
    "end": "5131900"
  },
  {
    "text": "to do longer contacts. So your effective receptive\nfield is now the local 1 times the layers.",
    "start": "5131900",
    "end": "5139730"
  },
  {
    "text": "The final trick-- so those\nwere the older ideas. But the way that this has kind\nof been modern instantiation",
    "start": "5139730",
    "end": "5146420"
  },
  {
    "text": "is some of the recent\npapers like LLaMA 4 and Gemma and Cohere\ncommand A have now",
    "start": "5146420",
    "end": "5153469"
  },
  {
    "text": "come up with this very clever\ntrick of basically having transformer blocks,\nwhere in this case,",
    "start": "5153470",
    "end": "5160410"
  },
  {
    "text": "you have a set of four\ntransformer blocks. The very bottom one\nuses full self-attention",
    "start": "5160410",
    "end": "5166270"
  },
  {
    "text": "with no position embedding. So there's no RoPE, no nothing. It doesn't know about\nposition at all. But it's full self-attention.",
    "start": "5166270",
    "end": "5171778"
  },
  {
    "text": "And it only happens\nonce every four blocks. And then the three\nblocks above it use sliding window\nattention with RoPE.",
    "start": "5171778",
    "end": "5179000"
  },
  {
    "text": "And so this is actually\na really clever trick to both control the\nsystems aspect of things",
    "start": "5179000",
    "end": "5184160"
  },
  {
    "text": "because the full attention only\nhappens every now and then. And also, the length\nextrapolation aspect",
    "start": "5184160",
    "end": "5190520"
  },
  {
    "text": "because RoPE only deals\nwith local context windows. And anything that's\nreally, really long range has no position\nembeddings at all",
    "start": "5190520",
    "end": "5197900"
  },
  {
    "text": "so it could extrapolate\nvery, very aggressively. Because you don't have\nto do this position extrapolation that you do\nwith something like RoPE.",
    "start": "5197900",
    "end": "5205252"
  },
  {
    "text": "So that's a really\ncool development that we've seen in the\nlast couple of months. So I think we're\ncoming up on time.",
    "start": "5205252",
    "end": "5212380"
  },
  {
    "text": "Feel free to ask me\nquestions about architecture or hyperparameters. I'll be happy to\nanswer questions after.",
    "start": "5212380",
    "end": "5218400"
  },
  {
    "start": "5218400",
    "end": "5223000"
  }
]