[
  {
    "start": "0",
    "end": "67000"
  },
  {
    "text": "and I'm 3 to be invited to give a lecture here so um today I'm going to give a talk on deep neuron Network the",
    "start": "11160",
    "end": "17760"
  },
  {
    "text": "model compression and a hardware accelerator engine to accelerate the",
    "start": "17760",
    "end": "22800"
  },
  {
    "text": "compressed neuron Network a brief introduction of myself and my adviser So",
    "start": "22800",
    "end": "28359"
  },
  {
    "text": "currently I'm a fourth year PhD student at Stanford and my research interest is",
    "start": "28359",
    "end": "33440"
  },
  {
    "text": "in um computer architecture for deep learning to improve the energy energy",
    "start": "33440",
    "end": "38600"
  },
  {
    "text": "efficiency for neural networks running on the mobile and embedded systems so this recent work on deep compression and",
    "start": "38600",
    "end": "45600"
  },
  {
    "text": "energy efficient inference engine has been published in nips and covered by the next platform and this is my advisor",
    "start": "45600",
    "end": "53800"
  },
  {
    "text": "bill so bill is the professor at Stanford and former chairman of Cs department and he leades us our CVA",
    "start": "53800",
    "end": "60920"
  },
  {
    "text": "group and also the chief scientist of Nvidia and host numerous titles and the",
    "start": "60920",
    "end": "66040"
  },
  {
    "text": "walls uh so this talk I'm going to talk about two two things the first one is",
    "start": "66040",
    "end": "71640"
  },
  {
    "start": "67000",
    "end": "93000"
  },
  {
    "text": "deep compression so how to compress the large deep neural models to make it",
    "start": "71640",
    "end": "77119"
  },
  {
    "text": "small to run efficiently on the mobile system and the second part is eie",
    "start": "77119",
    "end": "82640"
  },
  {
    "text": "accelerator so it is a efficient inference engine that runs on the",
    "start": "82640",
    "end": "88439"
  },
  {
    "text": "compressed neuron Network model model so let's get started with the uh",
    "start": "88439",
    "end": "94560"
  },
  {
    "start": "93000",
    "end": "146000"
  },
  {
    "text": "introduction of deep learning the next wave of AI so on the left hand side is",
    "start": "94560",
    "end": "100799"
  },
  {
    "text": "the human brain and on the right hand side is the artificial neuron Network so",
    "start": "100799",
    "end": "105880"
  },
  {
    "text": "the artificial neuron network is inspired by our human brain that the",
    "start": "105880",
    "end": "110920"
  },
  {
    "text": "signal flows from our eyes and goes all the way to the back of our brain and",
    "start": "110920",
    "end": "116680"
  },
  {
    "text": "after the stages of processing it can recognize from from the picture to tell you that this is a",
    "start": "116680",
    "end": "122360"
  },
  {
    "text": "cat so for the artificial neuron Network it has has a input layer and many hidden",
    "start": "122360",
    "end": "128679"
  },
  {
    "text": "layers and finally an output layer it tries to construct a highly nonlinear",
    "start": "128679",
    "end": "134599"
  },
  {
    "text": "function um so that it can be widely applied to uh applications ring from",
    "start": "134599",
    "end": "140440"
  },
  {
    "text": "image recognition speech recognition natural and natural language processing",
    "start": "140440",
    "end": "145879"
  },
  {
    "text": "with applications such as speech recognition and image recognition machine translation robotics",
    "start": "145879",
    "end": "152800"
  },
  {
    "start": "146000",
    "end": "411000"
  },
  {
    "text": "surveillance self-driving and assistant driving but with all these interesting",
    "start": "152800",
    "end": "160440"
  },
  {
    "text": "applications what is the problem of really deploying such deep neuron",
    "start": "160440",
    "end": "166319"
  },
  {
    "text": "networks onto the mobile platform say if you want to have an app that runs deep",
    "start": "166319",
    "end": "173440"
  },
  {
    "text": "neuron networks on your phone um current the app developers suffers from the",
    "start": "173440",
    "end": "179519"
  },
  {
    "text": "large model size for example the current um the previous alexnet which is you met",
    "start": "179519",
    "end": "186360"
  },
  {
    "text": "in 2012 has a model size of around 240 megabytes and the vget V6 network has a",
    "start": "186360",
    "end": "194640"
  },
  {
    "text": "model size about over 500 megabytes but App Store have the Restriction that an",
    "start": "194640",
    "end": "202159"
  },
  {
    "text": "item if it's over 100 megabytes then you cannot download until you connect to the",
    "start": "202159",
    "end": "207680"
  },
  {
    "text": "to the Wi-Fi so so um this is a comment that talking about model the importance",
    "start": "207680",
    "end": "215239"
  },
  {
    "text": "of model compression commented by Andrew uh saying that at by the number one",
    "start": "215239",
    "end": "221720"
  },
  {
    "text": "motivation of compressing the networks is to bring down the size of the binary file so as the mobile first company and",
    "start": "221720",
    "end": "228360"
  },
  {
    "text": "some companies are mobile only companies we frequently update various apps with different app stores so it's very",
    "start": "228360",
    "end": "234879"
  },
  {
    "text": "sensitive to the size of the binary files so if a new feature increase uh the size by 100 megabytes it will",
    "start": "234879",
    "end": "241959"
  },
  {
    "text": "receive much more scrutiny than one that increases it by only 10 megabytes so",
    "start": "241959",
    "end": "247239"
  },
  {
    "text": "it's very crucial uh to reduce the size of the uh neural network model to make",
    "start": "247239",
    "end": "253319"
  },
  {
    "text": "software engineers and app developers happy to make it to put it small enough to put it into the mobile",
    "start": "253319",
    "end": "259440"
  },
  {
    "text": "phone and secondly uh our Hardware Engineers suffers from the large model",
    "start": "259440",
    "end": "265880"
  },
  {
    "text": "size so especially given the uh limited resource constrainted of the embedded",
    "start": "265880",
    "end": "272479"
  },
  {
    "text": "system so what does large model size mean so large model size means we have",
    "start": "272479",
    "end": "278320"
  },
  {
    "text": "to pay a lot of memory bandwidth to run such uh neuron Network models but",
    "start": "278320",
    "end": "284960"
  },
  {
    "text": "comparing the uh energy consumption of different operations we can see that the",
    "start": "284960",
    "end": "291800"
  },
  {
    "text": "um memory reference is two or three more or order of more magnitude than those",
    "start": "291800",
    "end": "300039"
  },
  {
    "text": "aru app operations such as ADD multiply and IAM cach access so a dam memory",
    "start": "300039",
    "end": "308240"
  },
  {
    "text": "access takes as much as 640 PJs comparing with the only five PJs if we",
    "start": "308240",
    "end": "315320"
  },
  {
    "text": "fetch a parameter from isram mam cache so reducing the model size is very",
    "start": "315320",
    "end": "321800"
  },
  {
    "text": "crucial if we want to deploy it on inance system giving the limited Hardware resource on the other hand what about",
    "start": "321800",
    "end": "329440"
  },
  {
    "text": "the deploying the neur network on the cloud So It suffers from these problems so first we have a network delay say we",
    "start": "329440",
    "end": "338560"
  },
  {
    "text": "uh upload our image to the cloud and then get the result back given say 0.1",
    "start": "338560",
    "end": "344600"
  },
  {
    "text": "second delay if you have a self-driving car application running on your on your car 0.1 second if you're driving at 70 m",
    "start": "344600",
    "end": "353319"
  },
  {
    "text": "per hour that is already 10 ft away and also for power budget so the",
    "start": "353319",
    "end": "359680"
  },
  {
    "text": "the uh power consumption is a um major part of the total cost of ownership of a",
    "start": "359680",
    "end": "365440"
  },
  {
    "text": "data center so the power budget is a crucial Factor as well and finally the",
    "start": "365440",
    "end": "370720"
  },
  {
    "text": "user privacy so companies like Snapchat uh they uh want to preserve the users",
    "start": "370720",
    "end": "377520"
  },
  {
    "text": "privacy and don't want users users don't want upload their images to the cloud and then get the result so it is if you",
    "start": "377520",
    "end": "384880"
  },
  {
    "text": "have some app that say scans your credit card and to know those numbers you don't",
    "start": "384880",
    "end": "390759"
  },
  {
    "text": "want your credit card to be uploaded to the cloud and then get back so essentially you want everything to be",
    "start": "390759",
    "end": "396680"
  },
  {
    "text": "done locally on your phone so that motivates us to have an engine that can",
    "start": "396680",
    "end": "402599"
  },
  {
    "text": "run neuron Network and deep learning applications in real time on your uh mobile",
    "start": "402599",
    "end": "409720"
  },
  {
    "text": "devices so given those constraints um we propose two two solvers the first one is",
    "start": "409720",
    "end": "418240"
  },
  {
    "start": "411000",
    "end": "465000"
  },
  {
    "text": "deep compression that compress deep new network model size down to a very small",
    "start": "418240",
    "end": "424720"
  },
  {
    "text": "size so that it can run efficiently on mobile so it has three main features",
    "start": "424720",
    "end": "431440"
  },
  {
    "text": "first one is it has a smaller size it can compress the app the mo mobile app",
    "start": "431440",
    "end": "436599"
  },
  {
    "text": "size by uh 35 times to 50 times and the second one is accuracy so although the",
    "start": "436599",
    "end": "443639"
  },
  {
    "text": "neuronal model is compressed it does not suffer from accuracy drop it still has",
    "start": "443639",
    "end": "449599"
  },
  {
    "text": "have the same accuracy as the original Network and finally speed up so we hope",
    "start": "449599",
    "end": "456440"
  },
  {
    "text": "to have the uh inference running faster and have lower latency running the uh",
    "start": "456440",
    "end": "462440"
  },
  {
    "text": "compressed model so the second solware is uh",
    "start": "462440",
    "end": "467879"
  },
  {
    "start": "465000",
    "end": "548000"
  },
  {
    "text": "efficient inference engine which is the as6 accelerator that accelerates and",
    "start": "467879",
    "end": "474680"
  },
  {
    "text": "runs on the uh compressed model so it also has three features first one is",
    "start": "474680",
    "end": "480360"
  },
  {
    "text": "offline so everything we can run uh locally on the mobile so no dependency",
    "start": "480360",
    "end": "487000"
  },
  {
    "text": "on the nwork connection a second feature is real time there's no uh Network delay",
    "start": "487000",
    "end": "492720"
  },
  {
    "text": "and have a high frame rate and finally low power so that we can have a uh High",
    "start": "492720",
    "end": "498919"
  },
  {
    "text": "Energy Efficiency and have our battery runs",
    "start": "498919",
    "end": "504360"
  },
  {
    "text": "longer so let's look at the first the uh deep compression so so as a highlight it",
    "start": "504360",
    "end": "511240"
  },
  {
    "text": "can compress alxn by 35 times Bringing Down the model size from 240 megabytes",
    "start": "511240",
    "end": "517240"
  },
  {
    "text": "down to only around 6.6 megabytes and for VG Network it comprise by 40 49",
    "start": "517240",
    "end": "524360"
  },
  {
    "text": "times compress from the original model size 5 over 500 megabytes down to only",
    "start": "524360",
    "end": "529560"
  },
  {
    "text": "11 megabytes so both cases has no accuracy loss so now the weights can fit",
    "start": "529560",
    "end": "536519"
  },
  {
    "text": "on the onchip as RAM which takes 120 times less energy than if you have to go",
    "start": "536519",
    "end": "544560"
  },
  {
    "text": "to the dam memory so how is the uh deep compression",
    "start": "544560",
    "end": "550959"
  },
  {
    "start": "548000",
    "end": "604000"
  },
  {
    "text": "implemented so it is a pipeline consisting of first we prun the network",
    "start": "550959",
    "end": "556800"
  },
  {
    "text": "and then we enforce we sharing on the network so pruning says we remove the",
    "start": "556800",
    "end": "562760"
  },
  {
    "text": "Redundant synapsis from the network so this is the original Network that is",
    "start": "562760",
    "end": "567800"
  },
  {
    "text": "densely connected and this is the prune Network that redundant weights are",
    "start": "567800",
    "end": "573399"
  },
  {
    "text": "removed so next I'll talk about how do we eliminate the Redundant connections",
    "start": "573399",
    "end": "580480"
  },
  {
    "text": "and the next step is apply weight sharing say this weight and this weight",
    "start": "580480",
    "end": "585800"
  },
  {
    "text": "are pointing to the same memory location so both during experience and doing",
    "start": "585800",
    "end": "590920"
  },
  {
    "text": "training they are updated and used at the same time with the same",
    "start": "590920",
    "end": "596160"
  },
  {
    "text": "value and the final step is halfman code it's not on this picture but I will talk",
    "start": "596160",
    "end": "602120"
  },
  {
    "text": "about it later so let's go through the first one which is pruning pruning is implemented",
    "start": "602120",
    "end": "609160"
  },
  {
    "start": "604000",
    "end": "633000"
  },
  {
    "text": "by we T the network like before and then we prune the connections by finding the",
    "start": "609160",
    "end": "615399"
  },
  {
    "text": "uh absolute value of the weights if they are small then we remove them away and finally we retra the weights and we do",
    "start": "615399",
    "end": "622360"
  },
  {
    "text": "it iteratively to recover the accuracy so after this pipeline the",
    "start": "622360",
    "end": "628399"
  },
  {
    "text": "network can be reduced by 10 times without losing accuracy so this is a little bit of uh",
    "start": "628399",
    "end": "636760"
  },
  {
    "start": "633000",
    "end": "692000"
  },
  {
    "text": "motivation it's actually motivated by our human brain so um when we are born",
    "start": "636760",
    "end": "643320"
  },
  {
    "text": "trillions of snap synapses are generated during the first few months of birth and then at one year old it peaked",
    "start": "643320",
    "end": "652279"
  },
  {
    "text": "at a thousand trillion and at 10-year-old a child has",
    "start": "652279",
    "end": "657800"
  },
  {
    "text": "only around 500 trating connections so this result has been published by Nature",
    "start": "657800",
    "end": "663760"
  },
  {
    "text": "saying that this pruning mechanism removes the Redundant Connections in the brain so we first develop a lot of",
    "start": "663760",
    "end": "670279"
  },
  {
    "text": "connections uh and then during we as we grow up as we learn new stuff a lot of",
    "start": "670279",
    "end": "676519"
  },
  {
    "text": "redundant connections are removed so only those useful connections synapses are",
    "start": "676519",
    "end": "682560"
  },
  {
    "text": "preserved so does this mechanism also work on artificial neuron Network so the",
    "start": "682560",
    "end": "690040"
  },
  {
    "text": "answer is yes if we see the results across four",
    "start": "690040",
    "end": "695959"
  },
  {
    "start": "692000",
    "end": "730000"
  },
  {
    "text": "different neuro artificial neuron networks across two different Benchmark uh across two different data sets this",
    "start": "695959",
    "end": "702160"
  },
  {
    "text": "is amist data set uh classifying 0 to n uh digits and this is um image net that",
    "start": "702160",
    "end": "711160"
  },
  {
    "text": "has a thousand classes the largest data set of image recognition so the network can be",
    "start": "711160",
    "end": "717639"
  },
  {
    "text": "comprised ranging from NX X to uh 13x without losing the accuracy some of them",
    "start": "717639",
    "end": "725680"
  },
  {
    "text": "even have a less error rate and this is a a detailed result on",
    "start": "725680",
    "end": "733760"
  },
  {
    "start": "730000",
    "end": "844000"
  },
  {
    "text": "alexnet through pruning this uh horizontal is telling us how many",
    "start": "733760",
    "end": "739560"
  },
  {
    "text": "parameters are we removing away so at this point we are removing uh 90% of the parameters in the",
    "start": "739560",
    "end": "748440"
  },
  {
    "text": "original Network and the Y AIS is the accuracy loss telling us how many accuracy do we lose",
    "start": "748440",
    "end": "755839"
  },
  {
    "text": "by pruning the network so of course the more to the right left uh right up side the better",
    "start": "755839",
    "end": "763320"
  },
  {
    "text": "the result so this is the best result telling us that removing 90% of the",
    "start": "763320",
    "end": "769440"
  },
  {
    "text": "parameters resulted in no accuracy loss",
    "start": "769440",
    "end": "775320"
  },
  {
    "text": "so these four lines are uh designed based exploration of different uh",
    "start": "775320",
    "end": "781920"
  },
  {
    "text": "pruning methods including using R1 regularization versus R2 regularization",
    "start": "781920",
    "end": "788360"
  },
  {
    "text": "and the dash line are pring the network without retraining the network so we can see that even without",
    "start": "788360",
    "end": "795920"
  },
  {
    "text": "retraining the network say if you don't have the data set to retre you can still get around 50% free lunch by pruning the",
    "start": "795920",
    "end": "805480"
  },
  {
    "text": "network and then an interesting point like in this line this green line on the",
    "start": "805480",
    "end": "811880"
  },
  {
    "text": "top we can see that sometimes the accuracy even got improved by finding the right number of",
    "start": "811880",
    "end": "819399"
  },
  {
    "text": "model capacity of the prun network but if we prune too much the accuracy will",
    "start": "819399",
    "end": "825600"
  },
  {
    "text": "begin to drop so this technique has recently been used by Hong Kong University they are",
    "start": "825600",
    "end": "832800"
  },
  {
    "text": "inps they used pruning deliberately to increase the accuracy of classifying uh",
    "start": "832800",
    "end": "839120"
  },
  {
    "text": "detecting phases and classifying phases so this figure shows how many",
    "start": "839120",
    "end": "847079"
  },
  {
    "start": "844000",
    "end": "911000"
  },
  {
    "text": "parameters in each layer of Alx net and VG net are redundant and can be safely",
    "start": "847079",
    "end": "852519"
  },
  {
    "text": "prune away the green part is like the water in that you can safely remove in a",
    "start": "852519",
    "end": "860560"
  },
  {
    "text": "sponge we can see that this is the total uh uh parameters and all of these green",
    "start": "860560",
    "end": "868519"
  },
  {
    "text": "parts can be safely removed and so it is for the VG net around",
    "start": "868519",
    "end": "874360"
  },
  {
    "text": "93% of the connections can be safely removed without impacting the accuracy",
    "start": "874360",
    "end": "880560"
  },
  {
    "text": "and if you see the breakdown of the con lotion layer versus the fully connected layer so both C layer and fully connect",
    "start": "880560",
    "end": "888519"
  },
  {
    "text": "layer can be pruned but C layer can be pruned less around three times and FC",
    "start": "888519",
    "end": "895440"
  },
  {
    "text": "layers can be pruned more about 10 times and usally the first layer that directly",
    "start": "895440",
    "end": "902720"
  },
  {
    "text": "interacts with the input image have less redundancy generally for both",
    "start": "902720",
    "end": "911000"
  },
  {
    "start": "911000",
    "end": "1247000"
  },
  {
    "text": "networks so let's see what exactly does pruning do and what is the pattern of",
    "start": "911600",
    "end": "919120"
  },
  {
    "text": "the prun network so this is a very um basic illustration of on the amist data",
    "start": "919120",
    "end": "927360"
  },
  {
    "text": "set that has did is 0 1 2 3 out way to 9 so this is the Lanette 300 100 so the",
    "start": "927360",
    "end": "937399"
  },
  {
    "text": "first and this is the sparcity pattern of the first fully connected layer of L",
    "start": "937399",
    "end": "944600"
  },
  {
    "text": "not 300 and 100 which is a fully connected neural network classifying the",
    "start": "944600",
    "end": "949720"
  },
  {
    "text": "digits so the input is has image has 28 by 28 pixels so all together 764",
    "start": "949720",
    "end": "959839"
  },
  {
    "text": "that's the input and the hidden layer first the hidden layer have 300 elements",
    "start": "959839",
    "end": "966120"
  },
  {
    "text": "so the first Matrix of the network is basically a",
    "start": "966120",
    "end": "972560"
  },
  {
    "text": "764 by 300 Matrix so this is the sparcity pattern",
    "start": "972560",
    "end": "978360"
  },
  {
    "text": "of the Matrix of the pruning we can see that it clearly has a",
    "start": "978360",
    "end": "984319"
  },
  {
    "text": "banded structure so if we count uh there are exactly 28 bands and each band have",
    "start": "984319",
    "end": "992480"
  },
  {
    "text": "exactly 28 pixels so why is it like that and also the it is white here and also",
    "start": "992480",
    "end": "998519"
  },
  {
    "text": "white here so why is it like that because if we see the data set all the",
    "start": "998519",
    "end": "1003759"
  },
  {
    "text": "digits are mostly written in the center of the image so on the peripheral say here and here here and here there's",
    "start": "1003759",
    "end": "1011560"
  },
  {
    "text": "nothing in the image so the connections so pruny automatically find finds that",
    "start": "1011560",
    "end": "1017240"
  },
  {
    "text": "those connections that connect with these empty parts are useless so that it",
    "start": "1017240",
    "end": "1023839"
  },
  {
    "text": "can be safely pruned so this part corresponds to uh the first three um",
    "start": "1023839",
    "end": "1031558"
  },
  {
    "text": "horizontal pixels first line Second Line third line they are mostly white and",
    "start": "1031559",
    "end": "1037000"
  },
  {
    "text": "then these um amp entries corresponds to",
    "start": "1037000",
    "end": "1042120"
  },
  {
    "text": "after after at the end beginning and the end of um when we scan the image these",
    "start": "1042120",
    "end": "1049320"
  },
  {
    "text": "part are empty and finally these empty Parts corresponds to the bottom of the image that are mostly empty so that this",
    "start": "1049320",
    "end": "1058039"
  },
  {
    "text": "figure uh this visualization can show that the pruning mechanism automatically",
    "start": "1058039",
    "end": "1064400"
  },
  {
    "text": "finds the important connections that we need to preserve in the",
    "start": "1064400",
    "end": "1071759"
  },
  {
    "text": "image so it not only learns not only learns the weight but also learns the",
    "start": "1071880",
    "end": "1077919"
  },
  {
    "text": "connections not only learns the weight of the connections but also learns which connections are right to preserve in the",
    "start": "1077919",
    "end": "1084320"
  },
  {
    "text": "neural network so above we talk about the applications for uh image recognition so",
    "start": "1084320",
    "end": "1091960"
  },
  {
    "text": "how about uh those are convolution neuron networks how about uh recurrent and recursive neural networks and rstm",
    "start": "1091960",
    "end": "1099320"
  },
  {
    "text": "used in natural language processing um so this ex experiment works on the uh",
    "start": "1099320",
    "end": "1107880"
  },
  {
    "text": "neurot talk which is done by Audrey karti upstairs and then it's application",
    "start": "1107880",
    "end": "1114559"
  },
  {
    "text": "of the application is doing the image captioning so I gave you an image and let the neuron Network generate the",
    "start": "1114559",
    "end": "1121280"
  },
  {
    "text": "caption of the image so we can see that with with no retraining the uh blue",
    "start": "1121280",
    "end": "1126840"
  },
  {
    "text": "score begins to drop and with proper retraining the blue score can be uh",
    "start": "1126840",
    "end": "1132120"
  },
  {
    "text": "preserved until we remove 90% of the connections so it's um really",
    "start": "1132120",
    "end": "1140320"
  },
  {
    "text": "interesting to see that about 90% of the connections in neurot talk are redundant and can be safely removed and also",
    "start": "1140320",
    "end": "1147200"
  },
  {
    "text": "preserve a very good performance so let's see these examples",
    "start": "1147200",
    "end": "1152919"
  },
  {
    "text": "so for the first picture the original Network predicts say uh predicts the",
    "start": "1152919",
    "end": "1158120"
  },
  {
    "text": "caption to be a basketball player in the white uniform is playing with the ball",
    "start": "1158120",
    "end": "1163640"
  },
  {
    "text": "and if we PR away 90% of the connections the network produces is the result",
    "start": "1163640",
    "end": "1169280"
  },
  {
    "text": "saying that a basketball player in a white uniform is playing with a big basketball so it's even more accurate",
    "start": "1169280",
    "end": "1174880"
  },
  {
    "text": "than ball and how about the second one is the original Network says a brown dog",
    "start": "1174880",
    "end": "1180080"
  },
  {
    "text": "is running through a grassy field and the the prun network say that a brown",
    "start": "1180080",
    "end": "1185159"
  },
  {
    "text": "dog is running through aggressive area they're almost the same and for this one the original Network says the a man is",
    "start": "1185159",
    "end": "1192320"
  },
  {
    "text": "Runing a surfboard on a wave and if you prune away 90% of the parameters it says",
    "start": "1192320",
    "end": "1198840"
  },
  {
    "text": "a man in the white suit is riding a wave on a beach so this seems like uh",
    "start": "1198840",
    "end": "1205039"
  },
  {
    "text": "something like a beach although not a beach I think something yellow but if we",
    "start": "1205039",
    "end": "1211280"
  },
  {
    "text": "are too gritty and P away too much parameters the network becomes drunk",
    "start": "1211280",
    "end": "1216520"
  },
  {
    "text": "totally drunk not totally drunk but a little bit the original Network says a soccer player in red is running the",
    "start": "1216520",
    "end": "1222679"
  },
  {
    "text": "field and the pr Network says a man in a red shirt and black and white black",
    "start": "1222679",
    "end": "1228280"
  },
  {
    "text": "shirt is running through a field is not logically correct but we can see there's still white black and",
    "start": "1228280",
    "end": "1236000"
  },
  {
    "text": "red shirts so this shows that um the pruning",
    "start": "1236000",
    "end": "1241120"
  },
  {
    "text": "also applies to not only cnns but rnns and",
    "start": "1241120",
    "end": "1246360"
  },
  {
    "text": "RMS so just now we see the uh uh function and performance what about the",
    "start": "1246880",
    "end": "1252200"
  },
  {
    "start": "1247000",
    "end": "1383000"
  },
  {
    "text": "speed up so I experimented with the FC layers of alexnet VG net and neurot talk",
    "start": "1252200",
    "end": "1259400"
  },
  {
    "text": "and on three different platforms the Intel i7 CPU the NVIDIA GTX titanx GPU",
    "start": "1259400",
    "end": "1267120"
  },
  {
    "text": "and also the Nvidia tk1 mobile GPU so I used I I experimented with both",
    "start": "1267120",
    "end": "1275520"
  },
  {
    "text": "the dance Network and the prune Network and compare compare their spe on 1 2 3 4",
    "start": "1275520",
    "end": "1283799"
  },
  {
    "text": "five one nine benchmarks and this is the GE mean",
    "start": "1283799",
    "end": "1289159"
  },
  {
    "text": "so if if you see the average so the on CPU the prun network is roughly 3x",
    "start": "1289159",
    "end": "1296279"
  },
  {
    "text": "faster than the unun network and on the GPU it's also roughly 3x faster than",
    "start": "1296279",
    "end": "1303320"
  },
  {
    "text": "unun Network and a mobile GPU is roughly uh five times 3 divided by 0.6 faster",
    "start": "1303320",
    "end": "1311400"
  },
  {
    "text": "than running on the uh unpr Network so the um there are some cases",
    "start": "1311400",
    "end": "1319799"
  },
  {
    "text": "where there's no speed up for example this one on CPU vg8 so this layer is",
    "start": "1319799",
    "end": "1325039"
  },
  {
    "text": "already small enough that even the it has a size of 4K by 4K so it's already",
    "start": "1325039",
    "end": "1330799"
  },
  {
    "text": "uh 4K by a thousand sorry 4K by a th000 so the unpr network is already small enough to fit in Cache so the prune",
    "start": "1330799",
    "end": "1338120"
  },
  {
    "text": "Network also even got prune is still fit in the last level cache so the speed up",
    "start": "1338120",
    "end": "1343880"
  },
  {
    "text": "is not very um uh significant but if you see those large layers years say V6 it",
    "start": "1343880",
    "end": "1350880"
  },
  {
    "text": "has as much as nine times speed up on the CPU because the prun network can fully Feit on the AR three cache but the",
    "start": "1350880",
    "end": "1358440"
  },
  {
    "text": "unproved network cannot some details the uh unproved",
    "start": "1358440",
    "end": "1363799"
  },
  {
    "text": "network is impl on CPU is Implement with the MK C plus g m and the pr network is",
    "start": "1363799",
    "end": "1370559"
  },
  {
    "text": "Implement with the CSR M and similarly uh on GP the pr network is implemented",
    "start": "1370559",
    "end": "1376240"
  },
  {
    "text": "the C uh C sparse Library Co bars and unpr networks implement this cool",
    "start": "1376240",
    "end": "1382279"
  },
  {
    "text": "blast so what about the Energy Efficiency running those layers so on",
    "start": "1382279",
    "end": "1388200"
  },
  {
    "start": "1383000",
    "end": "1406000"
  },
  {
    "text": "average this if you see here on the CPU it's roughly takes six times less energy",
    "start": "1388200",
    "end": "1396320"
  },
  {
    "text": "and on GPU takes three times less and a mobile GPU takes roughly four times less",
    "start": "1396320",
    "end": "1401960"
  },
  {
    "text": "than the uh unpruned Network okay so having talking about",
    "start": "1401960",
    "end": "1408760"
  },
  {
    "start": "1406000",
    "end": "1443000"
  },
  {
    "text": "prun pruning uh we switch gear to quantization and we sharing so",
    "start": "1408760",
    "end": "1414360"
  },
  {
    "text": "quantization and WD sharing are implemented in with this method so we",
    "start": "1414360",
    "end": "1420880"
  },
  {
    "text": "first cluster the weights by say King clustering and then we generate the code book and then we quantize the weights",
    "start": "1420880",
    "end": "1427640"
  },
  {
    "text": "according to the code book and finally we retrain the code book and eally do this process and by reducing the network",
    "start": "1427640",
    "end": "1435480"
  },
  {
    "text": "by another 27x to 31x we can and get preserve the same accuracy by proper",
    "start": "1435480",
    "end": "1442520"
  },
  {
    "text": "retraining so what is the retraining like if we see this picture it is a 4x4",
    "start": "1442520",
    "end": "1450120"
  },
  {
    "start": "1443000",
    "end": "1544000"
  },
  {
    "text": "Matrix um and the weights with the same color are quantized with the same value",
    "start": "1450120",
    "end": "1457840"
  },
  {
    "text": "say if this weight is 2.09 and that weight is 1.92 then they are both quantized to two",
    "start": "1457840",
    "end": "1466960"
  },
  {
    "text": "and then having an index X of three pointing to this location so when doing",
    "start": "1466960",
    "end": "1472480"
  },
  {
    "text": "updates these are the gradient generated by uh say Cafe and then this gradient is",
    "start": "1472480",
    "end": "1479960"
  },
  {
    "text": "grouped by the index say all the weights with the same color are grouped with the",
    "start": "1479960",
    "end": "1485480"
  },
  {
    "text": "same same row here and then they are reduced by summing these gradients",
    "start": "1485480",
    "end": "1490559"
  },
  {
    "text": "together and then finally these reduced weights are multiplied with the learning",
    "start": "1490559",
    "end": "1495679"
  },
  {
    "text": "rate and minused by the original Sid weights and this concludes uh one",
    "start": "1495679",
    "end": "1503520"
  },
  {
    "text": "iteration of the updates so what is the benefit of this",
    "start": "1503520",
    "end": "1509240"
  },
  {
    "text": "so let's see what exactly do we need to store previously and now previously we need to store all these 16 weights each",
    "start": "1509240",
    "end": "1516159"
  },
  {
    "text": "weight if it's a floating Point number that's 32 bits or half bit or 16 bit now",
    "start": "1516159",
    "end": "1522840"
  },
  {
    "text": "we only need to store the four four numbers the sent choice plus the the index so in this case the index can be",
    "start": "1522840",
    "end": "1530159"
  },
  {
    "text": "represented with only two bits instead of previously 32bit and the code book is",
    "start": "1530159",
    "end": "1536840"
  },
  {
    "text": "only four has only four entries but this is still using the 32bit",
    "start": "1536840",
    "end": "1542880"
  },
  {
    "text": "F so if you see a real uh distribution of the weights on alexnet so these are",
    "start": "1543559",
    "end": "1551399"
  },
  {
    "start": "1544000",
    "end": "1637000"
  },
  {
    "text": "the weights after pruning and retraining we can see that everything that's close to zero has been eliminated",
    "start": "1551399",
    "end": "1558760"
  },
  {
    "text": "after retraining so it's roughly formed by model distribution so the Green Dots here",
    "start": "1558760",
    "end": "1567360"
  },
  {
    "text": "green crosses here are the initialization of the Sano but after clustering and fine-tuning the weights",
    "start": "1567360",
    "end": "1575520"
  },
  {
    "text": "the code book are those red dots so those uh millions of connections here",
    "start": "1575520",
    "end": "1582960"
  },
  {
    "text": "can be represented in only these 16 dots only these 16 red dots to represent the",
    "start": "1582960",
    "end": "1590480"
  },
  {
    "text": "original Network so for example the fc7 layer of alet has 60 million parameters",
    "start": "1590480",
    "end": "1597200"
  },
  {
    "text": "but now you have only you have to only store 16 parameters and their indexes so",
    "start": "1597200",
    "end": "1602520"
  },
  {
    "text": "that's a huge saving so in this here's nips there's a",
    "start": "1602520",
    "end": "1608360"
  },
  {
    "text": "another work uh doing even more aggressive quantization by uh uh making",
    "start": "1608360",
    "end": "1615840"
  },
  {
    "text": "a only one bit so plus one and minus one uh they show their result on um amn data",
    "start": "1615840",
    "end": "1622520"
  },
  {
    "text": "set and ccii data set but I'm really looking forward to the result on the image net dat set as well but this",
    "start": "1622520",
    "end": "1629000"
  },
  {
    "text": "figure roughly shows the the shape of uh say this is plus one and this is minus",
    "start": "1629000",
    "end": "1634120"
  },
  {
    "text": "one the shape is very uh similar so the result so 60 million",
    "start": "1634120",
    "end": "1640760"
  },
  {
    "start": "1637000",
    "end": "1732000"
  },
  {
    "text": "parameters now you can have only 16 parameters so if we have 85 bit conation",
    "start": "1640760",
    "end": "1647919"
  },
  {
    "text": "means Comm layers represent by eight bits FC layers represented by five bits",
    "start": "1647919",
    "end": "1653200"
  },
  {
    "text": "there's no accuracy loss on image net data set an 84bit quantization which is",
    "start": "1653200",
    "end": "1658760"
  },
  {
    "text": "a little bit more Hardware friendly two 4bits or 8 bit which is a bite uh",
    "start": "1658760",
    "end": "1664440"
  },
  {
    "text": "there's only 0.1 top one AR L loss which is kind of negligible and then if you",
    "start": "1664440",
    "end": "1670919"
  },
  {
    "text": "want to be more aggressive so if you are really limited by a hardware resource and you don't care a little bit of",
    "start": "1670919",
    "end": "1677120"
  },
  {
    "text": "accurac law say 2% or 2.6% you can more aggressively comprise it to only 4 bit",
    "start": "1677120",
    "end": "1684159"
  },
  {
    "text": "or 2 bit for the network so those results are astonishing",
    "start": "1684159",
    "end": "1691399"
  },
  {
    "text": "for let's see the detailed breakdown of each layer of the RX net so the X AIS is",
    "start": "1691399",
    "end": "1698279"
  },
  {
    "text": "the number of bits we preserve say this is 8 bit 7 6 5 4 3 2 all the way to two",
    "start": "1698279",
    "end": "1705279"
  },
  {
    "text": "bits and the Y AIS is the accuracy the inference accuracy so if you see the",
    "start": "1705279",
    "end": "1710840"
  },
  {
    "text": "comp layers not until the three bits three bits or four bits does the",
    "start": "1710840",
    "end": "1716240"
  },
  {
    "text": "accuracy begin to drop and for the FC layers not until two",
    "start": "1716240",
    "end": "1722039"
  },
  {
    "text": "bits does the accuracy begin to drop so this shows that there's a lot of",
    "start": "1722039",
    "end": "1728640"
  },
  {
    "text": "redundancy in the network and just now we have seen both",
    "start": "1728640",
    "end": "1735279"
  },
  {
    "start": "1732000",
    "end": "1849000"
  },
  {
    "text": "pruning and quantization so these methods both reduce the uh model",
    "start": "1735279",
    "end": "1741039"
  },
  {
    "text": "capacity and the size of the neural network so one not one one question that",
    "start": "1741039",
    "end": "1746120"
  },
  {
    "text": "we may have is can they work together to further bring down the size of the uh",
    "start": "1746120",
    "end": "1752799"
  },
  {
    "text": "neuron Network the answer is yes so this is let's see these three",
    "start": "1752799",
    "end": "1759080"
  },
  {
    "text": "figures on IC layers and C layers and other",
    "start": "1759080",
    "end": "1764840"
  },
  {
    "text": "layers and uh these this is is the top five accuracy and this is the top one",
    "start": "1764840",
    "end": "1770559"
  },
  {
    "text": "accuracy so there is a dash line and a solid line uh meaning both prune and",
    "start": "1770559",
    "end": "1777720"
  },
  {
    "text": "quantied network and the quantized only Network showing in let's see this one",
    "start": "1777720",
    "end": "1783880"
  },
  {
    "text": "showing in dash we can see that this solid line works even better than this",
    "start": "1783880",
    "end": "1789440"
  },
  {
    "text": "dash line meaning that the quantized and prun network works even better than if",
    "start": "1789440",
    "end": "1796080"
  },
  {
    "text": "you quantize the network only so why is it like that if we see the distribution",
    "start": "1796080",
    "end": "1803240"
  },
  {
    "text": "here pruning which is make the distribution in in blue like this",
    "start": "1803240",
    "end": "1808880"
  },
  {
    "text": "already roughly quantize the network into binarize the network say this is",
    "start": "1808880",
    "end": "1814279"
  },
  {
    "text": "one group this this it's low power this is one group this is another group so it",
    "start": "1814279",
    "end": "1820960"
  },
  {
    "text": "already helped the uh quantization a little bit and furthermore prunin reduced the number of connections that",
    "start": "1820960",
    "end": "1827720"
  },
  {
    "text": "you have to quantize say previously you have um 10 million uh you have 16",
    "start": "1827720",
    "end": "1832919"
  },
  {
    "text": "million parameters to Pro to to quantize now you only have one one million um um",
    "start": "1832919",
    "end": "1840360"
  },
  {
    "text": "parameters to quantize that makes the quantize even easier so that's why pruning and quantization can work very",
    "start": "1840360",
    "end": "1847000"
  },
  {
    "text": "well together so lastly so the final step of",
    "start": "1847000",
    "end": "1852440"
  },
  {
    "start": "1849000",
    "end": "1905000"
  },
  {
    "text": "the compression pipeline is halfman coding so this is a lossless compression",
    "start": "1852440",
    "end": "1857600"
  },
  {
    "text": "Tech technique working on both the weight weight and the index and the motivation is that both the weight and",
    "start": "1857600",
    "end": "1865080"
  },
  {
    "text": "the index are biasly distributed so if we can represent the weight that occurs",
    "start": "1865080",
    "end": "1872720"
  },
  {
    "text": "more frequently say this way and use less uh use more number of",
    "start": "1872720",
    "end": "1878360"
  },
  {
    "text": "bits to represent the uh weight that doesn't occur very frequently use very",
    "start": "1878360",
    "end": "1883799"
  },
  {
    "text": "little bits here and use a variable length of bits of of representation it",
    "start": "1883799",
    "end": "1889519"
  },
  {
    "text": "can further bring down the model size although this method in use half my",
    "start": "1889519",
    "end": "1895960"
  },
  {
    "text": "encoding and decod halfman decoding in the inferior stage so I did did not implement this one in Hardware but",
    "start": "1895960",
    "end": "1902919"
  },
  {
    "text": "implemented in the compression software so if we",
    "start": "1902919",
    "end": "1908880"
  },
  {
    "start": "1905000",
    "end": "1999000"
  },
  {
    "text": "uh apply the whole compression pipeline the pruning quantization and halfman",
    "start": "1908880",
    "end": "1914000"
  },
  {
    "text": "coding we can comprise the network uh these for networks ranging from 35 times",
    "start": "1914000",
    "end": "1920919"
  },
  {
    "text": "to 49 times without increasing the uh test error for all these",
    "start": "1920919",
    "end": "1928559"
  },
  {
    "text": "cases so here is a uh comparison of different methods so the x axis is the",
    "start": "1930480",
    "end": "1937760"
  },
  {
    "text": "uh compression rate say here we compress the network by 5% of its original size",
    "start": "1937760",
    "end": "1945639"
  },
  {
    "text": "and this one shows the accuracy loss so if we apply both pruning and",
    "start": "1945639",
    "end": "1951840"
  },
  {
    "text": "quantization the network can be compressed all the way down to around say 3% of the original size without",
    "start": "1951840",
    "end": "1960120"
  },
  {
    "text": "losing accuracy but if we apply pruning only it begins to and also quantization",
    "start": "1960120",
    "end": "1966080"
  },
  {
    "text": "only the accuracy begins to drop when it's comprised about 10% of its original",
    "start": "1966080",
    "end": "1972840"
  },
  {
    "text": "SES furthermore if we apply the relatively cheap ivd the singular",
    "start": "1972840",
    "end": "1978679"
  },
  {
    "text": "value decomposition technique to approximate The Matrix we can compress Network by only",
    "start": "1978679",
    "end": "1985360"
  },
  {
    "text": "around um 20% Then the accuracy begins begins to drop",
    "start": "1985360",
    "end": "1991039"
  },
  {
    "text": "here so the the thing that work best is the pruning plus quantization the right",
    "start": "1991039",
    "end": "1996279"
  },
  {
    "text": "line here uh so here's a detailed breakdown",
    "start": "1996279",
    "end": "2002399"
  },
  {
    "start": "1999000",
    "end": "2030000"
  },
  {
    "text": "of uh different layers on the left hand side of the Alx net",
    "start": "2002399",
    "end": "2008480"
  },
  {
    "text": "so with the whole pipeline is comprised to 2.88 of its original size but without",
    "start": "2008480",
    "end": "2015360"
  },
  {
    "text": "halfman coding is still compressed to about 3.7 uh% of its original",
    "start": "2015360",
    "end": "2021480"
  },
  {
    "text": "size and the layers are presented with 8 Bits for C layer and five bits for thefc",
    "start": "2021480",
    "end": "2029638"
  },
  {
    "text": "layers so here's a comparison with the other published compression methods so",
    "start": "2029720",
    "end": "2036840"
  },
  {
    "start": "2030000",
    "end": "2046000"
  },
  {
    "text": "the pruning Plus ization process half monod worked best compared with the",
    "start": "2036840",
    "end": "2042159"
  },
  {
    "text": "other comprehension methods find on the literature so here is a conclusion of",
    "start": "2042159",
    "end": "2049520"
  },
  {
    "start": "2046000",
    "end": "2100000"
  },
  {
    "text": "network of deep compression so we have presented method to comprise deep neuron",
    "start": "2049520",
    "end": "2054720"
  },
  {
    "text": "networks without losing their accuracy by first pruning away those important",
    "start": "2054720",
    "end": "2060878"
  },
  {
    "text": "connections and then Quant the network can reinforce wi sharing and finally apply halfman coding the network can be",
    "start": "2060879",
    "end": "2067200"
  },
  {
    "text": "comprised by 30 time x ranging from 35x to 49x without losing the accuracy so",
    "start": "2067200",
    "end": "2074158"
  },
  {
    "text": "now all the wids can fit in cach and can um motivate us to build a specialized",
    "start": "2074159",
    "end": "2081280"
  },
  {
    "text": "Hardware accelerator that works on this compressed model if we think of the compressed model it is a sparse model it",
    "start": "2081280",
    "end": "2088480"
  },
  {
    "text": "has indirect lookups which is not that uh Hardware friendly on those customized",
    "start": "2088480",
    "end": "2094118"
  },
  {
    "text": "Hardware that's why later I build a hardware accelerator that works on the compressive model and by the way as a",
    "start": "2094119",
    "end": "2101520"
  },
  {
    "start": "2100000",
    "end": "2168000"
  },
  {
    "text": "product we are going to build a model compression tool for developers if you're interested so we're going to",
    "start": "2101520",
    "end": "2107760"
  },
  {
    "text": "produce two versions an easy version so no retraining is needed it's very fast",
    "start": "2107760",
    "end": "2113400"
  },
  {
    "text": "can have around five times the 10x compression rate and giving given about",
    "start": "2113400",
    "end": "2119160"
  },
  {
    "text": "1% accuracy loss and we're going to produce Advanced version which have uh",
    "start": "2119160",
    "end": "2124960"
  },
  {
    "text": "35 times to 4 50 times compression rate with no loss of accuracy but training is",
    "start": "2124960",
    "end": "2130480"
  },
  {
    "text": "needed it requires some training time and this is a demo of uh alexnet pocket",
    "start": "2130480",
    "end": "2136400"
  },
  {
    "text": "alexnet that's very small that you can put it in pocket that's why I call it pocket alexnet roughly only 8",
    "start": "2136400",
    "end": "2145280"
  },
  {
    "text": "megabytes uh so on the with the compressed model um The Next Step of my",
    "start": "2145960",
    "end": "2151079"
  },
  {
    "text": "work is to build a uh infer efficient inference engine that works on the",
    "start": "2151079",
    "end": "2156720"
  },
  {
    "text": "compressed model to give even larger speed up and even more make it even more",
    "start": "2156720",
    "end": "2162079"
  },
  {
    "text": "energy efficient by Building A specialized Hardware accelerator so this is the hardware",
    "start": "2162079",
    "end": "2168480"
  },
  {
    "start": "2168000",
    "end": "2338000"
  },
  {
    "text": "accelerator we're going to build so it has three features first of all is running offline you don't have to",
    "start": "2168480",
    "end": "2174920"
  },
  {
    "text": "upload your feed uh image to the cloud and then real time no network delay and",
    "start": "2174920",
    "end": "2181319"
  },
  {
    "text": "have high frame frame rate and finally low power has High Energy Efficiency that pre preserves your battery",
    "start": "2181319",
    "end": "2189559"
  },
  {
    "text": "so how do we um achieve those three goals so the main motivation and the",
    "start": "2190640",
    "end": "2197000"
  },
  {
    "text": "main solution is put everything on chip so previously we have seen that picture",
    "start": "2197000",
    "end": "2202920"
  },
  {
    "text": "that memory reference is really expensive compared with other operations",
    "start": "2202920",
    "end": "2208160"
  },
  {
    "text": "like aru operations and cash access and everything else is two or three or more",
    "start": "2208160",
    "end": "2214319"
  },
  {
    "text": "order of magnitude than those operations so after impression everything can completely fit on chip so that you don't",
    "start": "2214319",
    "end": "2221000"
  },
  {
    "text": "have to go to off chip to fetch those parameters from Dam so that gives you both speed up and Energy",
    "start": "2221000",
    "end": "2227640"
  },
  {
    "text": "Efficiency so we present a spar indirectly index which shared Matrix",
    "start": "2227640",
    "end": "2233160"
  },
  {
    "text": "Vector accelerator that accelerates the compressed model the halfers of the compressed model now the large DN models",
    "start": "2233160",
    "end": "2241400"
  },
  {
    "text": "can fit on chip on chip as Ram with which is 120x um energy saving B going to Dam and",
    "start": "2241400",
    "end": "2250280"
  },
  {
    "text": "then the E efficient inference engine exploit disparity of the activation as",
    "start": "2250280",
    "end": "2256119"
  },
  {
    "text": "well since after R operation uh the activation also becomes sparse lots of them are zero so if we don't do",
    "start": "2256119",
    "end": "2263280"
  },
  {
    "text": "computation on those zero activations we can save another three times computation",
    "start": "2263280",
    "end": "2268480"
  },
  {
    "text": "time and also e works on the compressed model which uh without half encoding is",
    "start": "2268480",
    "end": "2274880"
  },
  {
    "text": "30 times model reduction compared with the original model so you you do less",
    "start": "2274880",
    "end": "2280920"
  },
  {
    "text": "work so you do 30 times less F 30 times less parameters and for each parameter",
    "start": "2280920",
    "end": "2286720"
  },
  {
    "text": "you save 12x energy so for the accelerator I",
    "start": "2286720",
    "end": "2292240"
  },
  {
    "text": "distributed both the storage and computation across multiple P so that you can scale your system with your",
    "start": "2292240",
    "end": "2299040"
  },
  {
    "text": "application if your application need a large Network then I have lot of such PS",
    "start": "2299040",
    "end": "2304640"
  },
  {
    "text": "but if you have a small see from this picture if you have a large Network I",
    "start": "2304640",
    "end": "2309760"
  },
  {
    "text": "have more PS here but you have a small application with a small model I can only say have say four Ps so that you",
    "start": "2309760",
    "end": "2316200"
  },
  {
    "text": "can scale your problem scale your system with your problem and finally we",
    "start": "2316200",
    "end": "2322040"
  },
  {
    "text": "evaluated eie on a wide range of deep learning models uh ranging from CN for",
    "start": "2322040",
    "end": "2327880"
  },
  {
    "text": "object detection rstm for natural language processing and image captioning and also compared with the gpus CPU gpus",
    "start": "2327880",
    "end": "2335560"
  },
  {
    "text": "and other Asic accelerators so uh this figure shows how how do we",
    "start": "2335560",
    "end": "2342640"
  },
  {
    "start": "2338000",
    "end": "2392000"
  },
  {
    "text": "distribute the storage and processing across different pees so showing this figure this is a matrix in a certain",
    "start": "2342640",
    "end": "2349720"
  },
  {
    "text": "hidden layer of a uh deep neuron Network and the weights you can see",
    "start": "2349720",
    "end": "2356200"
  },
  {
    "text": "there are a lot of zeros corresponding to we have p network so that a lot of them are zeros and a lot of them um so",
    "start": "2356200",
    "end": "2365160"
  },
  {
    "text": "the the weights with the same color are stored in the same PE so that it's",
    "start": "2365160",
    "end": "2371200"
  },
  {
    "text": "distributed and partitioned in the r in the r manner so this um each PE can",
    "start": "2371200",
    "end": "2378079"
  },
  {
    "text": "process its own part it does a Lo local reduction without too much communication",
    "start": "2378079",
    "end": "2384680"
  },
  {
    "text": "with the other PES while you process a single",
    "start": "2384680",
    "end": "2390240"
  },
  {
    "text": "element and then inside HP it goes through these three steps so first of",
    "start": "2390520",
    "end": "2397119"
  },
  {
    "start": "2392000",
    "end": "2456000"
  },
  {
    "text": "all the input of the engine is the compress the Deep Neuron nwor model and the input",
    "start": "2397119",
    "end": "2403400"
  },
  {
    "text": "image and so that we store the encoded weight in SRAM relative index in SRAM",
    "start": "2403400",
    "end": "2410079"
  },
  {
    "text": "with a sparse format and and then we fetch the 4bit",
    "start": "2410079",
    "end": "2416720"
  },
  {
    "text": "virtual weight and the 4bit relative index remember this is only 4bit compared with the original 32bit uh",
    "start": "2416720",
    "end": "2423520"
  },
  {
    "text": "floating point or 16bit half floating uh half Precision floating point and then",
    "start": "2423520",
    "end": "2429560"
  },
  {
    "text": "for the weight we do a weight look up and for the index we do a index accumulation since it's a relative index",
    "start": "2429560",
    "end": "2435880"
  },
  {
    "text": "and then we get the real weight and the and the absolute index so that we can uh",
    "start": "2435880",
    "end": "2441319"
  },
  {
    "text": "using use the real weight to do aru operations and then use the absolute index to fetch the code uh for fetch the",
    "start": "2441319",
    "end": "2449960"
  },
  {
    "text": "parameter from the code book finally we do the prediction and get the",
    "start": "2449960",
    "end": "2455318"
  },
  {
    "text": "result so this Hardware accelerator is evaluated with the uh standard I6 design",
    "start": "2455359",
    "end": "2461520"
  },
  {
    "start": "2456000",
    "end": "2573000"
  },
  {
    "text": "flow so first of all we build a c accurate C++ simulator that has two",
    "start": "2461520",
    "end": "2469040"
  },
  {
    "text": "abstract methods propagate and update so this simulator is used for design space",
    "start": "2469040",
    "end": "2475160"
  },
  {
    "text": "exploration and later served as a golden model for the verification of the ver",
    "start": "2475160",
    "end": "2480359"
  },
  {
    "text": "log and the next step with the Accel uh with the",
    "start": "2480359",
    "end": "2486000"
  },
  {
    "text": "um with the simulator we have a RTL written in very log that we have",
    "start": "2486000",
    "end": "2492960"
  },
  {
    "text": "verified its output with the golden model in the in model Sim and next upep we synthesize the E",
    "start": "2492960",
    "end": "2501240"
  },
  {
    "text": "using the synopsis design compiler under the tsmc 45 GP uh Library which we used",
    "start": "2501240",
    "end": "2508240"
  },
  {
    "text": "standard VT library with the worst case pvt corner to make it sure everything works",
    "start": "2508240",
    "end": "2513720"
  },
  {
    "text": "correctly and finally we uh place and rotted the the PE using the synopsis IC",
    "start": "2513720",
    "end": "2520000"
  },
  {
    "text": "compiler the ICC and we use the kti to get the asram area and energy numbers",
    "start": "2520000",
    "end": "2526880"
  },
  {
    "text": "and in order to get the power consumption model the power consumption we have annotated the toal rate from the",
    "start": "2526880",
    "end": "2532640"
  },
  {
    "text": "ITR simulation to the gate level at least remember the power consumption is",
    "start": "2532640",
    "end": "2537680"
  },
  {
    "text": "the equation is CV s and times frequency so we need uh we can get see the",
    "start": "2537680",
    "end": "2543720"
  },
  {
    "text": "capacitance from the uh post uh post layout uh libraries Li um the libraries",
    "start": "2543720",
    "end": "2553359"
  },
  {
    "text": "from the post layout circuit and V we can get it from the library as well and",
    "start": "2553359",
    "end": "2558599"
  },
  {
    "text": "F the frequency we have to get it from the simulation result which is annotated",
    "start": "2558599",
    "end": "2563960"
  },
  {
    "text": "with the dumped into a switch activity interchange format the save format and",
    "start": "2563960",
    "end": "2569079"
  },
  {
    "text": "estimate the power using the prime time TPX tool and then we baselined",
    "start": "2569079",
    "end": "2578040"
  },
  {
    "start": "2573000",
    "end": "2597000"
  },
  {
    "text": "the uh uh Hardware uh efficient uh efficient inference engine on these",
    "start": "2578040",
    "end": "2585640"
  },
  {
    "text": "benchmarks which is Alex ring from alexnet vet and neurot talk and run it",
    "start": "2585640",
    "end": "2592280"
  },
  {
    "text": "on compared with uh GP CPU GPU and mobile gpus and this is the layout of a",
    "start": "2592280",
    "end": "2598160"
  },
  {
    "start": "2597000",
    "end": "2666000"
  },
  {
    "text": "ro chip so this is this is the ice Ram that stores The Spar Matrix sparse",
    "start": "2598160",
    "end": "2604599"
  },
  {
    "text": "Matrix and this is the ice Ram that stores the pointers and the inside here",
    "start": "2604599",
    "end": "2609880"
  },
  {
    "text": "is the is the meat which is the arithmetic unit so as we can see still most of the",
    "start": "2609880",
    "end": "2616480"
  },
  {
    "text": "chip area is consumed by the ice Rams so the under tsmc 45 nanometer it consumes",
    "start": "2616480",
    "end": "2625200"
  },
  {
    "text": "9 M for HP and taking an area of6 mm",
    "start": "2625200",
    "end": "2631880"
  },
  {
    "text": "square and the critical puff of the engine is 1.15",
    "start": "2631880",
    "end": "2637920"
  },
  {
    "text": "ners so 64 uh so 48 such pees is large enough to hold Alex net so you can scale",
    "start": "2637920",
    "end": "2646000"
  },
  {
    "text": "the number of PS with your problem sets so without model compression uh we",
    "start": "2646000",
    "end": "2653559"
  },
  {
    "text": "cannot fit all the parameters on those srams but only with model compression",
    "start": "2653559",
    "end": "2659599"
  },
  {
    "text": "can we fit everything on chip that's the mer uh Merit of uh",
    "start": "2659599",
    "end": "2665240"
  },
  {
    "text": "eie so let's see the speed up of and the Energy Efficiency of eiee so this is the",
    "start": "2665240",
    "end": "2672839"
  },
  {
    "start": "2666000",
    "end": "2850000"
  },
  {
    "text": "speed up and this is the Energy Efficiency so across different benchmarks the right hand side is the",
    "start": "2672839",
    "end": "2680200"
  },
  {
    "text": "ABS is the average value so previously we have seen these bars now we have a",
    "start": "2680200",
    "end": "2686960"
  },
  {
    "text": "new bar here so we can see that the E engine which is in red is 189 times",
    "start": "2686960",
    "end": "2694720"
  },
  {
    "text": "faster than CPU running the uncompress PR model and it's roughly 18 times",
    "start": "2694720",
    "end": "2701839"
  },
  {
    "text": "faster compared with the GPU with running the uncompressed model and with",
    "start": "2701839",
    "end": "2707280"
  },
  {
    "text": "respect to the Energy Efficiency uh the righted bar which is eie is",
    "start": "2707280",
    "end": "2713040"
  },
  {
    "text": "24,000 times compared with the CPU running on the uncompressed model and is",
    "start": "2713040",
    "end": "2718119"
  },
  {
    "text": "roughly 3,000 times more energy efficient than the gpus running the",
    "start": "2718119",
    "end": "2724640"
  },
  {
    "text": "uncompressed model so where are these numbers where does",
    "start": "2724640",
    "end": "2729920"
  },
  {
    "text": "this number come from the 3,000 times more energy efficient than uh CPU than",
    "start": "2729920",
    "end": "2735760"
  },
  {
    "text": "the GPU so if we do a hand calculation so by compressing the network we can",
    "start": "2735760",
    "end": "2741960"
  },
  {
    "text": "save around 30 times of uh we can have three 30 times less",
    "start": "2741960",
    "end": "2749319"
  },
  {
    "text": "model size and less parameters to fetch from from either SRAM or Dam and then",
    "start": "2749319",
    "end": "2755520"
  },
  {
    "text": "since we compress the model and everything fits in ice Ram the ice Ram um is another 120 times more energy",
    "start": "2755520",
    "end": "2763000"
  },
  {
    "text": "efficient than fting fetching from Dam and remember we can utilize the uh",
    "start": "2763000",
    "end": "2768760"
  },
  {
    "text": "sparcity of the activation which is roughly three times sp sp sparity so all",
    "start": "2768760",
    "end": "2775240"
  },
  {
    "text": "the way there is roughly if you do the calculation is roughly",
    "start": "2775240",
    "end": "2780480"
  },
  {
    "text": "10,000 times more energy uh more energy efficient but this is using 45 5",
    "start": "2780480",
    "end": "2787160"
  },
  {
    "text": "nanometer technology and the gpus are done GTX Titan X is done in 28 n",
    "start": "2787160",
    "end": "2794040"
  },
  {
    "text": "nanometer so divided this number by two we get roughly 5,000",
    "start": "2794040",
    "end": "2800240"
  },
  {
    "text": "times theoretically 5,000 times more energy efficient but probably we didn't",
    "start": "2800240",
    "end": "2805680"
  },
  {
    "text": "do it good enough we're going to get 3,000 times",
    "start": "2805680",
    "end": "2812240"
  },
  {
    "text": "so let's see the speed up by benchmarking with the CPU GPU and mobile",
    "start": "2812240",
    "end": "2818520"
  },
  {
    "text": "GPU so these are the uh those are really small numbers but um different",
    "start": "2818520",
    "end": "2825319"
  },
  {
    "text": "benchmarks with the different batch size so this this engine is targeting real time process in which the numbers are",
    "start": "2825319",
    "end": "2832559"
  },
  {
    "text": "gener the previous figure are generated using batch size of one so during say",
    "start": "2832559",
    "end": "2837880"
  },
  {
    "text": "self-driving car you don't have the time to batch 64 images and wait until 64",
    "start": "2837880",
    "end": "2842920"
  },
  {
    "text": "images to come and process them together that's why we use batch size of one to do the um",
    "start": "2842920",
    "end": "2849920"
  },
  {
    "text": "Benchmark so this figure shows the scalability of e ranging from 1 PE all",
    "start": "2849920",
    "end": "2856040"
  },
  {
    "start": "2850000",
    "end": "2891000"
  },
  {
    "text": "the way to 256 PES so the scalability log scale is",
    "start": "2856040",
    "end": "2861800"
  },
  {
    "text": "roughly linear speed up except for this one uh so the neurot",
    "start": "2861800",
    "end": "2868400"
  },
  {
    "text": "talk we this uh Matrix is really small",
    "start": "2868400",
    "end": "2873599"
  },
  {
    "text": "so in with 256 p is considering the sparcity each P has",
    "start": "2873599",
    "end": "2880040"
  },
  {
    "text": "around just one it to process so that when there are a lot of PS it does not",
    "start": "2880040",
    "end": "2885319"
  },
  {
    "text": "grow that much for this Benchmark but for the rest the scalability is really",
    "start": "2885319",
    "end": "2891200"
  },
  {
    "start": "2891000",
    "end": "2993000"
  },
  {
    "text": "good and this figure shows the uh number of useful computation on the top across",
    "start": "2891200",
    "end": "2897920"
  },
  {
    "text": "different PES and also the load balance for with the the different number of PES",
    "start": "2897920",
    "end": "2904000"
  },
  {
    "text": "the useful computation some of them is not uh 100% because with the relative",
    "start": "2904000",
    "end": "2909800"
  },
  {
    "text": "index we have to pad zeros if we cannot use four bits to represent the jump we",
    "start": "2909800",
    "end": "2915480"
  },
  {
    "text": "have to pad a dami zero over there uh so that the useful computation is less than",
    "start": "2915480",
    "end": "2921280"
  },
  {
    "text": "100% but with more PES the more PE we have the less paddings eror we need so",
    "start": "2921280",
    "end": "2926960"
  },
  {
    "text": "that the useful pend computations becomes more and this is analysis of the",
    "start": "2926960",
    "end": "2932280"
  },
  {
    "text": "load balance with more PES so the more PES we have each PE will have less number of parameters to process thus",
    "start": "2932280",
    "end": "2940040"
  },
  {
    "text": "it's highly possible that some P's have three parameters some of them have eight parameters process leading to Temporary",
    "start": "2940040",
    "end": "2947079"
  },
  {
    "text": "load imbalance so how do we deal with this so we introduced the F FAL q that",
    "start": "2947079",
    "end": "2954960"
  },
  {
    "text": "decouples the computation between the different PES so we have a couple of",
    "start": "2954960",
    "end": "2960000"
  },
  {
    "text": "different sizes 1 2 4 8 16 we experimented with different F size",
    "start": "2960000",
    "end": "2967079"
  },
  {
    "text": "to see the load balance feature of the E engine we can see that the the load",
    "start": "2967079",
    "end": "2973000"
  },
  {
    "text": "balance becomes pretty well when the five4 size is roughly of size one two",
    "start": "2973000",
    "end": "2978640"
  },
  {
    "text": "three 1 two 4 eight here and after size of eight the game became the benefit",
    "start": "2978640",
    "end": "2985559"
  },
  {
    "text": "becomes marginal so we choose in the design we choose the five4 size to be 16",
    "start": "2985559",
    "end": "2991720"
  },
  {
    "text": "here and here's More Design space exploration of the eiee engine",
    "start": "2991720",
    "end": "2997599"
  },
  {
    "start": "2993000",
    "end": "3119000"
  },
  {
    "text": "so the on the top this is the design space exploration of the Asam width so",
    "start": "2997599",
    "end": "3004440"
  },
  {
    "text": "the wider ice Ram the less number of readed sh shown in green we we need but",
    "start": "3004440",
    "end": "3012880"
  },
  {
    "text": "the more energy it takes reading reading each parameter but if we multiply them",
    "start": "3012880",
    "end": "3018440"
  },
  {
    "text": "together we will see the total energy we can see at 64 bit the width of the SRAM",
    "start": "3018440",
    "end": "3023520"
  },
  {
    "text": "gives the uh most efficient usage so we choose the design of the ism width to be",
    "start": "3023520",
    "end": "3029400"
  },
  {
    "text": "64bit and also we have analyzed arithmetic Precision after the quantized",
    "start": "3029400",
    "end": "3036559"
  },
  {
    "text": "weights so we uh so in the code book in previously remember we quantize all the",
    "start": "3036559",
    "end": "3043400"
  },
  {
    "text": "weights with the code book so that 16 million parameters became only 16 parameters and those 16 parameters how",
    "start": "3043400",
    "end": "3050160"
  },
  {
    "text": "many bits do we use to represent the weight play a larger role in the Energy",
    "start": "3050160",
    "end": "3056280"
  },
  {
    "text": "Efficiency so we can see that the 30 32bit floating Point multiply takes roughly four PCO",
    "start": "3056280",
    "end": "3064640"
  },
  {
    "text": "Jews but a 16bit integer operation takes roughly only half a p jeel which is a",
    "start": "3064640",
    "end": "3070799"
  },
  {
    "text": "lot more efficient and then we experimented with the accuracy from 32bit float to 32bit",
    "start": "3070799",
    "end": "3078920"
  },
  {
    "text": "int to 16bit int and we can see not until using 32 uh 16bit integer does the",
    "start": "3078920",
    "end": "3087359"
  },
  {
    "text": "accuracy begins to drop with the 8bit integer the accuracy becomes un um",
    "start": "3087359",
    "end": "3094319"
  },
  {
    "text": "unusable so that we choose the arithmetic Precision of the code book to",
    "start": "3094319",
    "end": "3099960"
  },
  {
    "text": "be 16bit integer operation so the E engine has recently",
    "start": "3099960",
    "end": "3108680"
  },
  {
    "text": "been covered by the next platform uh with titled the emerging chip V",
    "start": "3108680",
    "end": "3115119"
  },
  {
    "text": "accelerates deep neural networks and we see if we see the big",
    "start": "3115119",
    "end": "3120480"
  },
  {
    "start": "3119000",
    "end": "3602000"
  },
  {
    "text": "picture um if you say the Intel defined the chips that does computation in the",
    "start": "3120480",
    "end": "3127359"
  },
  {
    "text": "PC era and then Qualcomm and Samsung and huawe and those components defined the",
    "start": "3127359",
    "end": "3135599"
  },
  {
    "text": "mobile computation in the mobile era then what is the next generation of",
    "start": "3135599",
    "end": "3140720"
  },
  {
    "text": "chips that defines the era of the intelligent mobile computation and and I",
    "start": "3140720",
    "end": "3146760"
  },
  {
    "text": "hope that deep learning hardware for deep learning such as eiee can uh shed",
    "start": "3146760",
    "end": "3152559"
  },
  {
    "text": "some lights and give some um interesting motivation",
    "start": "3152559",
    "end": "3157720"
  },
  {
    "text": "and um and and uh paved the way for future chips that targets intelligent",
    "start": "3157720",
    "end": "3164440"
  },
  {
    "text": "mobile computation so here is a conclusion of",
    "start": "3164440",
    "end": "3170079"
  },
  {
    "text": "my uh talk or we presented eie and efficient energy efficient engine",
    "start": "3170079",
    "end": "3177040"
  },
  {
    "text": "optimized to operate on the compressed deep neuron Network which is done by pruning weight sharing and halfman",
    "start": "3177040",
    "end": "3184559"
  },
  {
    "text": "coding by leveraging the sparcity in both the activations since we have R",
    "start": "3184559",
    "end": "3190240"
  },
  {
    "text": "activation function and the weights yeah reduced energy consumption of uh typical",
    "start": "3190240",
    "end": "3195880"
  },
  {
    "text": "filler by 3,000 times and this is the breakdown of how does where does this",
    "start": "3195880",
    "end": "3202440"
  },
  {
    "text": "3,000 times come from so first compression and then d to SRAM and finally spse",
    "start": "3202440",
    "end": "3210040"
  },
  {
    "text": "activation uh that's my talk thank you for your",
    "start": "3210960",
    "end": "3215720"
  },
  {
    "text": "attentions and um I'm glad to take questions yeah yes did you consider um",
    "start": "3218599",
    "end": "3223960"
  },
  {
    "text": "generalizing your Matrix Vector representation into a tensor into a tensor so Curr deals with",
    "start": "3223960",
    "end": "3231319"
  },
  {
    "text": "the single Matrix Vector modifcation with the fch size of one currently does not support tensor operation at this",
    "start": "3231319",
    "end": "3239200"
  },
  {
    "text": "moment so once you've done the pruning of course you can't train on a different",
    "start": "3239200",
    "end": "3245000"
  },
  {
    "text": "um training set because it might have outliers that would have put substantive weights on the links that you've removed",
    "start": "3245000",
    "end": "3251839"
  },
  {
    "text": "right sorry what's the question so if I retrain if I wanted to train on a different training set different",
    "start": "3251839",
    "end": "3257400"
  },
  {
    "text": "training set and it had it would have not made zero weights on some of the links but once you've pruned them it's",
    "start": "3257400",
    "end": "3264079"
  },
  {
    "text": "all it's all gone you can never put a there because there's no link right right so as we see from the digits I'm",
    "start": "3264079",
    "end": "3271440"
  },
  {
    "text": "this the data set uh if the digits are written on let's say the L hand side of",
    "start": "3271440",
    "end": "3277079"
  },
  {
    "text": "EXA have to R the network and have a different pattern but from but for example in speech recognition um they're",
    "start": "3277079",
    "end": "3283280"
  },
  {
    "text": "always improving because they're continually retraining based on continual input might that not lead to",
    "start": "3283280",
    "end": "3290680"
  },
  {
    "text": "positive weights where you've pruned the link uh if prune a link with the as the",
    "start": "3290680",
    "end": "3295880"
  },
  {
    "text": "new that oh it should be a new problem with the the uh say online learning when you constantly have different data sets",
    "start": "3295880",
    "end": "3303480"
  },
  {
    "text": "yeah then you cannot fix the uh pattern of the Mask yeah how many systems",
    "start": "3303480",
    "end": "3311000"
  },
  {
    "text": "actually train how many the how many Learning",
    "start": "3311000",
    "end": "3317760"
  },
  {
    "text": "Systems deep learning systems do iterative training anyway so the speech recognition systems even dragon is quite",
    "start": "3317760",
    "end": "3324960"
  },
  {
    "text": "good when you start but it gets better over time but do they it do",
    "start": "3324960",
    "end": "3330359"
  },
  {
    "text": "they or they rerun I don't know I'm just saying but it seemed like you know his example if another training set had the",
    "start": "3330359",
    "end": "3337720"
  },
  {
    "text": "figures left Justified instead of Center then you you wouldn't get the right weights yeah so this is targeting say",
    "start": "3337720",
    "end": "3345079"
  },
  {
    "text": "you have a very well developed model then you want to actually deploy it",
    "start": "3345079",
    "end": "3350400"
  },
  {
    "text": "that's process yeah so what I the key feature",
    "start": "3350400",
    "end": "3355880"
  },
  {
    "text": "here that compressing the network and make it fit inside the cach right so which Tres two points what if if I after",
    "start": "3355880",
    "end": "3362480"
  },
  {
    "text": "compression it doesn't still fit in the cach OR there exist or there's a new technology that makes the Ram or let's",
    "start": "3362480",
    "end": "3369480"
  },
  {
    "text": "say the memory has efficient at s would I still need to do compression or not so",
    "start": "3369480",
    "end": "3375799"
  },
  {
    "text": "um if you see the equation here there are three terms uh if you are d r is is",
    "start": "3375799",
    "end": "3381960"
  },
  {
    "text": "rme is large enough that can hold a lot of parameters you can lose one of the terms here here but you still have the",
    "start": "3381960",
    "end": "3388000"
  },
  {
    "text": "rest of the terms I think and then generally um if you have a app if you",
    "start": "3388000",
    "end": "3393119"
  },
  {
    "text": "build a app app writer and you want to have your app on the Apple Store you",
    "start": "3393119",
    "end": "3398280"
  },
  {
    "text": "want a user can download your app very fast then you still need to have a relatively small app just depends on the",
    "start": "3398280",
    "end": "3404960"
  },
  {
    "text": "game if the hardware continues to uh say in one day you can have a really really",
    "start": "3404960",
    "end": "3411640"
  },
  {
    "text": "large D um D Ram or I Ram then probably will",
    "start": "3411640",
    "end": "3417119"
  },
  {
    "text": "um do doesn't need those really aggressive compression but compression can still gain you some something maybe",
    "start": "3417119",
    "end": "3423720"
  },
  {
    "text": "not that much 3,000 times but maybe say 300 times",
    "start": "3423720",
    "end": "3429720"
  },
  {
    "text": "yeah so you saw the comparison the comparison with SVD approximation to the",
    "start": "3429839",
    "end": "3435839"
  },
  {
    "text": "wave Matrix right you can you know to compress and what you did SVD did you",
    "start": "3435839",
    "end": "3441640"
  },
  {
    "text": "retrain on the on the the SVD uh for you can you can retrain those singular",
    "start": "3441640",
    "end": "3449000"
  },
  {
    "text": "vectors to move them around oh that one I that's a simple activ I didn't retrain",
    "start": "3449000",
    "end": "3455200"
  },
  {
    "text": "the uh singular vectors okay so do you know how well that would do",
    "start": "3455200",
    "end": "3460599"
  },
  {
    "text": "or uh given the uh it's not I also tested with the not retrain the prun",
    "start": "3460599",
    "end": "3467039"
  },
  {
    "text": "model and not retrain the uh quantied model and seems like even without",
    "start": "3467039",
    "end": "3473280"
  },
  {
    "text": "retraining it's less um um optimistic if you just do the ivds",
    "start": "3473280",
    "end": "3480599"
  },
  {
    "text": "already begin to fall pretty sharply when you decrease the model size and there's a detailed paper about",
    "start": "3480599",
    "end": "3488079"
  },
  {
    "text": "um SVD published in I believe in nips last year by Facebook and so Advanced",
    "start": "3488079",
    "end": "3494559"
  },
  {
    "text": "version of SVD yeah thanks for a very nice talk very",
    "start": "3494559",
    "end": "3500760"
  },
  {
    "text": "good work um I'm wondering so like when you're pruning uh you're typically the biggest savings coming from the fully",
    "start": "3500760",
    "end": "3506880"
  },
  {
    "text": "connected layers right because that's where all the weights are um but you know a lot of these comets don't",
    "start": "3506880",
    "end": "3511920"
  },
  {
    "text": "actually use fully connected layers now they just have composition layers all the way through so it's almost like they",
    "start": "3511920",
    "end": "3518640"
  },
  {
    "text": "pruned up front structure to begin with and then they train and already the Network's 30 times smaller and we still",
    "start": "3518640",
    "end": "3525480"
  },
  {
    "text": "get very good results so do you think it's the prunings important or just removing those connections up front I",
    "start": "3525480",
    "end": "3531240"
  },
  {
    "text": "think the uh structure compression is a really good thing for example the uh Google anal",
    "start": "3531240",
    "end": "3536599"
  },
  {
    "text": "which is really really small and then I exactly have the same um thinking when I",
    "start": "3536599",
    "end": "3543160"
  },
  {
    "text": "visited by so they have a uh uh face recognition network that almost all the",
    "start": "3543160",
    "end": "3549280"
  },
  {
    "text": "layers are comp layers and only have one FC ler and they still using my technique",
    "start": "3549280",
    "end": "3555839"
  },
  {
    "text": "to compress their model by 10 times to make it even smaller to put in by do face recognition app so I think uh the",
    "start": "3555839",
    "end": "3564359"
  },
  {
    "text": "final uh solution I appre most is applying both methods so first the structure structural compression and",
    "start": "3564359",
    "end": "3571520"
  },
  {
    "text": "then those deep compression techniques I described to make it smaller to have the",
    "start": "3571520",
    "end": "3577680"
  },
  {
    "text": "to the Limit any more",
    "start": "3577680",
    "end": "3586078"
  },
  {
    "text": "questions cool",
    "start": "3586240",
    "end": "3590240"
  }
]