[
  {
    "start": "0",
    "end": "5017"
  },
  {
    "text": "SPEAKER: Welcome back, everyone.",
    "start": "5017",
    "end": "6350"
  },
  {
    "text": "This is the sixth\nand final screencast",
    "start": "6350",
    "end": "8770"
  },
  {
    "text": "in our series on\nmethods and metrics.",
    "start": "8770",
    "end": "10630"
  },
  {
    "text": "We're going to talk\nabout model evaluation.",
    "start": "10630",
    "end": "12950"
  },
  {
    "text": "This is a high-level\ndiscussion that",
    "start": "12950",
    "end": "14530"
  },
  {
    "text": "is directly oriented\ntoward helping you",
    "start": "14530",
    "end": "17050"
  },
  {
    "text": "with your final project work.",
    "start": "17050",
    "end": "19039"
  },
  {
    "text": "Here's an overview.",
    "start": "19040",
    "end": "20150"
  },
  {
    "text": "We're going to talk\nabout baselines.",
    "start": "20150",
    "end": "21949"
  },
  {
    "text": "What are they?",
    "start": "21950",
    "end": "22810"
  },
  {
    "text": "Why are they important?",
    "start": "22810",
    "end": "24380"
  },
  {
    "text": "We'll talk about the trials and\ntribulations of hyperparameter",
    "start": "24380",
    "end": "27880"
  },
  {
    "text": "optimization, and\nwhy it's important.",
    "start": "27880",
    "end": "30160"
  },
  {
    "text": "We'll think about classifier\ncomparison, a common mode",
    "start": "30160",
    "end": "33489"
  },
  {
    "text": "to be in as you're evaluating\nsystems and hypotheses.",
    "start": "33490",
    "end": "36650"
  },
  {
    "text": "And then we'll talk\nabout two things that",
    "start": "36650",
    "end": "38410"
  },
  {
    "text": "are really particular to\nthe deep learning era, how",
    "start": "38410",
    "end": "41350"
  },
  {
    "text": "to assess models that don't\nconverge in any strict sense,",
    "start": "41350",
    "end": "45100"
  },
  {
    "text": "and also the role\nof random parameter",
    "start": "45100",
    "end": "47469"
  },
  {
    "text": "initialization in the\nperformance of our biggest",
    "start": "47470",
    "end": "50620"
  },
  {
    "text": "models.",
    "start": "50620",
    "end": "52239"
  },
  {
    "text": "Let's start with baselines.",
    "start": "52240",
    "end": "54100"
  },
  {
    "text": "We take this for granted,\nbut this is actually",
    "start": "54100",
    "end": "56230"
  },
  {
    "text": "pretty important conceptually.",
    "start": "56230",
    "end": "58450"
  },
  {
    "text": "Here's a fundamental\nobservation about baselines.",
    "start": "58450",
    "end": "61570"
  },
  {
    "text": "Evaluation numbers in\nour field can never",
    "start": "61570",
    "end": "64300"
  },
  {
    "text": "be understood\nproperly in isolation.",
    "start": "64300",
    "end": "66790"
  },
  {
    "text": "Suppose your system gets\n0.95 F1, you feel overjoyed.",
    "start": "66790",
    "end": "71600"
  },
  {
    "text": "But the first question\nreviewers will ask",
    "start": "71600",
    "end": "73990"
  },
  {
    "text": "you is, is the task too easy?",
    "start": "73990",
    "end": "76240"
  },
  {
    "text": "How do simple baselines\ndo on the problem?",
    "start": "76240",
    "end": "79960"
  },
  {
    "text": "Conversely, suppose\nyour system gets 0.6",
    "start": "79960",
    "end": "83409"
  },
  {
    "text": "and you feel in\ndespair because you",
    "start": "83410",
    "end": "86470"
  },
  {
    "text": "feel like you haven't\nhad a success here,",
    "start": "86470",
    "end": "88250"
  },
  {
    "text": "but the next question\nshould be, how do humans do?",
    "start": "88250",
    "end": "91150"
  },
  {
    "text": "They're presumably a\nkind of upper bound.",
    "start": "91150",
    "end": "93580"
  },
  {
    "text": "And if it's a hard\ntask or a noisy task,",
    "start": "93580",
    "end": "96100"
  },
  {
    "text": "human performance\nmight be close to 0.61,",
    "start": "96100",
    "end": "98380"
  },
  {
    "text": "and you might really have\nachieved something meaningful",
    "start": "98380",
    "end": "100719"
  },
  {
    "text": "there.",
    "start": "100720",
    "end": "101260"
  },
  {
    "text": "So it's baseline models, and\nin that case, Oracle models",
    "start": "101260",
    "end": "104260"
  },
  {
    "text": "that are helping\nus to understand.",
    "start": "104260",
    "end": "107090"
  },
  {
    "text": "Baselines are also crucial for\nstrong experimental design.",
    "start": "107090",
    "end": "111270"
  },
  {
    "text": "So defining your\nbaseline should not",
    "start": "111270",
    "end": "113479"
  },
  {
    "text": "be some afterthought,\nbut rather central",
    "start": "113480",
    "end": "116180"
  },
  {
    "text": "to how you define your\noverall hypothesis.",
    "start": "116180",
    "end": "118730"
  },
  {
    "text": "Think about simple systems.",
    "start": "118730",
    "end": "121100"
  },
  {
    "text": "Think about ablations\nof your target system",
    "start": "121100",
    "end": "123680"
  },
  {
    "text": "and incorporate those into your\nthinking about the comparisons",
    "start": "123680",
    "end": "126782"
  },
  {
    "text": "that you'll make.",
    "start": "126782",
    "end": "127490"
  },
  {
    "text": "And baselines are really just\none aspect of the comparisons",
    "start": "127490",
    "end": "131180"
  },
  {
    "text": "we want to offer.",
    "start": "131180",
    "end": "133280"
  },
  {
    "text": "Baselines are essential for\nbuilding a persuasive case.",
    "start": "133280",
    "end": "136709"
  },
  {
    "text": "We saw that in my\ntwo examples there.",
    "start": "136710",
    "end": "138650"
  },
  {
    "text": "To really understand and\ncalibrate on what you achieved,",
    "start": "138650",
    "end": "141980"
  },
  {
    "text": "we need some baselines to\ncalibrate all of that stuff.",
    "start": "141980",
    "end": "146459"
  },
  {
    "text": "They can also be\nused to illuminate",
    "start": "146460",
    "end": "148610"
  },
  {
    "text": "specific aspects of the\nproblem and specific virtues",
    "start": "148610",
    "end": "152300"
  },
  {
    "text": "of your proposed system.",
    "start": "152300",
    "end": "153620"
  },
  {
    "text": "That often falls\nunder the heading",
    "start": "153620",
    "end": "155180"
  },
  {
    "text": "of ablations of your system.",
    "start": "155180",
    "end": "156680"
  },
  {
    "text": "Those are baselines that remove\ncrucial features or components",
    "start": "156680",
    "end": "161150"
  },
  {
    "text": "and test the model\nwith the same protocol.",
    "start": "161150",
    "end": "163280"
  },
  {
    "text": "And then the distance\nbetween your chosen model",
    "start": "163280",
    "end": "166770"
  },
  {
    "text": "and the ablated model\nis a kind of estimate",
    "start": "166770",
    "end": "169380"
  },
  {
    "text": "of the importance of the ablated\ncomponent to the overall system",
    "start": "169380",
    "end": "173760"
  },
  {
    "text": "performance.",
    "start": "173760",
    "end": "174640"
  },
  {
    "text": "Crucial aspect of\narguing and supporting",
    "start": "174640",
    "end": "177450"
  },
  {
    "text": "hypotheses and everything else.",
    "start": "177450",
    "end": "181550"
  },
  {
    "text": "Random baselines are really\nuseful for many purposes.",
    "start": "181550",
    "end": "185370"
  },
  {
    "text": "First, they can provide\na really true lower bound",
    "start": "185370",
    "end": "188180"
  },
  {
    "text": "on how systems can\ndo on your problem.",
    "start": "188180",
    "end": "190430"
  },
  {
    "text": "Sometimes they are\nsurprisingly robust,",
    "start": "190430",
    "end": "192709"
  },
  {
    "text": "and so it's worth\nrunning these early.",
    "start": "192710",
    "end": "194660"
  },
  {
    "text": "And I think also they can help\nyou fully debug your system.",
    "start": "194660",
    "end": "198290"
  },
  {
    "text": "These are probably\nlightweight models",
    "start": "198290",
    "end": "199909"
  },
  {
    "text": "that do relatively\nlittle processing",
    "start": "199910",
    "end": "201980"
  },
  {
    "text": "and can make sure\nthat everything",
    "start": "201980",
    "end": "203569"
  },
  {
    "text": "is functioning and makes sense\nand all that other stuff.",
    "start": "203570",
    "end": "206690"
  },
  {
    "text": "Scikit-Learn, again,\nhas you covered.",
    "start": "206690",
    "end": "208760"
  },
  {
    "text": "They have dummy classifier\nand dummy regressor.",
    "start": "208760",
    "end": "211580"
  },
  {
    "text": "They have different ways\nof acting as random models,",
    "start": "211580",
    "end": "215190"
  },
  {
    "text": "and I think this is\nreally useful to set",
    "start": "215190",
    "end": "216950"
  },
  {
    "text": "up early in your process.",
    "start": "216950",
    "end": "220330"
  },
  {
    "text": "You could also think about\ntask-specific baselines.",
    "start": "220330",
    "end": "223570"
  },
  {
    "text": "This might require real\nthought and real study",
    "start": "223570",
    "end": "226090"
  },
  {
    "text": "in the literature.",
    "start": "226090",
    "end": "227379"
  },
  {
    "text": "Does your problem\nsuggest a baseline",
    "start": "227380",
    "end": "229690"
  },
  {
    "text": "that will reveal something\nabout the problem or the way",
    "start": "229690",
    "end": "232840"
  },
  {
    "text": "it's modeled?",
    "start": "232840",
    "end": "233470"
  },
  {
    "text": "If so, you should have one of\nthese task-specific baselines.",
    "start": "233470",
    "end": "237070"
  },
  {
    "text": "Here are two recent\nexamples from NLU.",
    "start": "237070",
    "end": "239800"
  },
  {
    "text": "The first one is natural\nlanguage inference.",
    "start": "239800",
    "end": "242140"
  },
  {
    "text": "People discovered that so-called\nhypothesis-only baselines",
    "start": "242140",
    "end": "246370"
  },
  {
    "text": "can be very strong.",
    "start": "246370",
    "end": "248140"
  },
  {
    "text": "The reason this happens is that\nin the underlying crowdsourcing",
    "start": "248140",
    "end": "251410"
  },
  {
    "text": "effort, crowdworkers were\ngiven premise sentences",
    "start": "251410",
    "end": "254470"
  },
  {
    "text": "and asked to construct\nthree hypotheses.",
    "start": "254470",
    "end": "256630"
  },
  {
    "text": "One for neutral, one\nfor contradiction,",
    "start": "256630",
    "end": "259209"
  },
  {
    "text": "and one for entailment.",
    "start": "259209",
    "end": "260769"
  },
  {
    "text": "In that process of construction,\nthey did some systematic things",
    "start": "260769",
    "end": "264700"
  },
  {
    "text": "that kind of convey\ninformation about the label",
    "start": "264700",
    "end": "267880"
  },
  {
    "text": "inadvertently through\nthe hypothesis.",
    "start": "267880",
    "end": "270650"
  },
  {
    "text": "For example, many\ncontradictions involve negation,",
    "start": "270650",
    "end": "274300"
  },
  {
    "text": "and many entailment pairs\ninvolve very general terms",
    "start": "274300",
    "end": "277659"
  },
  {
    "text": "as part of the hypothesis.",
    "start": "277660",
    "end": "279830"
  },
  {
    "text": "What that means is that\nthe hypothesis actually",
    "start": "279830",
    "end": "282050"
  },
  {
    "text": "carries information\nabout the label,",
    "start": "282050",
    "end": "283940"
  },
  {
    "text": "and a hypothesis-only\nbaseline quantifies that.",
    "start": "283940",
    "end": "286760"
  },
  {
    "text": "You simply fit a model without\nany premise information",
    "start": "286760",
    "end": "290180"
  },
  {
    "text": "and see how you do.",
    "start": "290180",
    "end": "291600"
  },
  {
    "text": "And the finding\nof the literature",
    "start": "291600",
    "end": "293210"
  },
  {
    "text": "is that very often\nfor our benchmarks,",
    "start": "293210",
    "end": "295880"
  },
  {
    "text": "the hypothesis-only baseline\nis way above chance.",
    "start": "295880",
    "end": "299120"
  },
  {
    "text": "And what that shows you is\nthat the random baseline is not",
    "start": "299120",
    "end": "302720"
  },
  {
    "text": "so informative anymore.",
    "start": "302720",
    "end": "304700"
  },
  {
    "text": "There's a similar story\nfor the Story Cloze task.",
    "start": "304700",
    "end": "307910"
  },
  {
    "text": "This is to distinguish between\na coherent and incoherent ending",
    "start": "307910",
    "end": "311510"
  },
  {
    "text": "for a story.",
    "start": "311510",
    "end": "312500"
  },
  {
    "text": "And, again, systems that\nlook only at the ending",
    "start": "312500",
    "end": "315260"
  },
  {
    "text": "often do really well.",
    "start": "315260",
    "end": "316500"
  },
  {
    "text": "I think for the same\nreason the coherent versus",
    "start": "316500",
    "end": "319190"
  },
  {
    "text": "incoherent thing is\noften actually inferable",
    "start": "319190",
    "end": "322550"
  },
  {
    "text": "just from the ending,\nneglecting the story.",
    "start": "322550",
    "end": "325500"
  },
  {
    "text": "So it's not that the task\nis broken here necessarily,",
    "start": "325500",
    "end": "328543"
  },
  {
    "text": "but rather, again,\nthat you should",
    "start": "328543",
    "end": "329960"
  },
  {
    "text": "think about this as a\nbaseline to compare against.",
    "start": "329960",
    "end": "333110"
  },
  {
    "text": "And progress is really progress\nfrom this very specialized",
    "start": "333110",
    "end": "337159"
  },
  {
    "text": "baseline.",
    "start": "337160",
    "end": "340150"
  },
  {
    "text": "Next topic, hyperparameter\noptimization.",
    "start": "340150",
    "end": "342780"
  },
  {
    "text": "This is discussed extensively\nin one of our background units",
    "start": "342780",
    "end": "345930"
  },
  {
    "text": "on sentiment analysis.",
    "start": "345930",
    "end": "347370"
  },
  {
    "text": "You might go there\nfor a refresher.",
    "start": "347370",
    "end": "349500"
  },
  {
    "text": "Here I'll just briefly\nreview the rationale.",
    "start": "349500",
    "end": "352230"
  },
  {
    "text": "You want, maybe, to obtain the\nbest version of your model,",
    "start": "352230",
    "end": "355800"
  },
  {
    "text": "and that might mean exploring\nover different hyperparameters",
    "start": "355800",
    "end": "358800"
  },
  {
    "text": "to find a optimal\nsetting for it.",
    "start": "358800",
    "end": "361889"
  },
  {
    "text": "Another motivation is about\ncomparison between models.",
    "start": "361890",
    "end": "365250"
  },
  {
    "text": "Suppose you do have a results\ntable full of different systems",
    "start": "365250",
    "end": "368500"
  },
  {
    "text": "you're comparing,\nit makes no sense",
    "start": "368500",
    "end": "370980"
  },
  {
    "text": "to compare them against randomly\nchosen parameter settings",
    "start": "370980",
    "end": "374400"
  },
  {
    "text": "because you really want to\ngive every model the best",
    "start": "374400",
    "end": "377400"
  },
  {
    "text": "chance to shine.",
    "start": "377400",
    "end": "378810"
  },
  {
    "text": "Otherwise, there's\nan arbitrariness",
    "start": "378810",
    "end": "380610"
  },
  {
    "text": "to the evaluation that might not\ntranslate into robust results.",
    "start": "380610",
    "end": "384879"
  },
  {
    "text": "So what you really do is\ngive every system a chance",
    "start": "384880",
    "end": "387540"
  },
  {
    "text": "by exploring a wide\nrange of hyperparameters",
    "start": "387540",
    "end": "390690"
  },
  {
    "text": "and reporting the\noptimal results according",
    "start": "390690",
    "end": "393510"
  },
  {
    "text": "to that exploration.",
    "start": "393510",
    "end": "394920"
  },
  {
    "text": "That's a fair comparison,\nand it implies a lot",
    "start": "394920",
    "end": "397650"
  },
  {
    "text": "of search over hyperparameters.",
    "start": "397650",
    "end": "400259"
  },
  {
    "text": "You might want to understand the\nstability of your architecture.",
    "start": "400260",
    "end": "403007"
  },
  {
    "text": "This is interestingly different.",
    "start": "403007",
    "end": "404340"
  },
  {
    "text": "This is where you're not\ninterested in the best",
    "start": "404340",
    "end": "406620"
  },
  {
    "text": "parameters, but rather how\nstable system performance",
    "start": "406620",
    "end": "409710"
  },
  {
    "text": "is under various choices\npeople might make in order",
    "start": "409710",
    "end": "413520"
  },
  {
    "text": "to get a sense for how robustly\nit will perform if people are,",
    "start": "413520",
    "end": "417030"
  },
  {
    "text": "say, not attentive to\nthese hyperparameters,",
    "start": "417030",
    "end": "419520"
  },
  {
    "text": "or set them incorrectly in a\nkind of inadvertent accident",
    "start": "419520",
    "end": "423419"
  },
  {
    "text": "or an adversarial setting.",
    "start": "423420",
    "end": "426920"
  },
  {
    "text": "Crucial to all of\nthis, no matter",
    "start": "426920",
    "end": "428720"
  },
  {
    "text": "what your goals, hyperparameter\ntuning must be done only",
    "start": "428720",
    "end": "432530"
  },
  {
    "text": "on train and development data.",
    "start": "432530",
    "end": "434300"
  },
  {
    "text": "You never do model\nselection of any kind",
    "start": "434300",
    "end": "437509"
  },
  {
    "text": "based on the test data.",
    "start": "437510",
    "end": "439140"
  },
  {
    "text": "This is a special\ncase of the rule",
    "start": "439140",
    "end": "440932"
  },
  {
    "text": "that I've been repeating\nthroughout the course.",
    "start": "440932",
    "end": "442890"
  },
  {
    "text": "This is really\nfundamental to how",
    "start": "442890",
    "end": "444380"
  },
  {
    "text": "we think about testing\nand generalization,",
    "start": "444380",
    "end": "446870"
  },
  {
    "text": "and it applies with real\nforce in the context",
    "start": "446870",
    "end": "449690"
  },
  {
    "text": "of the kind of model\nselection we're doing here.",
    "start": "449690",
    "end": "453830"
  },
  {
    "text": "Now, hyperparameter optimization\nhas gotten really challenging",
    "start": "453830",
    "end": "458449"
  },
  {
    "text": "in the era of long-running,\nexpensive training regimes,",
    "start": "458450",
    "end": "461990"
  },
  {
    "text": "and let me give you a sense\nfor what the problem is",
    "start": "461990",
    "end": "464240"
  },
  {
    "text": "by way of an example.",
    "start": "464240",
    "end": "466289"
  },
  {
    "text": "For each hyperparameter,\nyou identify a large set",
    "start": "466290",
    "end": "469320"
  },
  {
    "text": "of values for it in some range.",
    "start": "469320",
    "end": "472030"
  },
  {
    "text": "Then you create a list\nof all the combinations",
    "start": "472030",
    "end": "474690"
  },
  {
    "text": "of all the hyperparameters.",
    "start": "474690",
    "end": "476520"
  },
  {
    "text": "This is the cross product of all\nthe values for all the features",
    "start": "476520",
    "end": "480120"
  },
  {
    "text": "you identified in step\n1, which you can hear in",
    "start": "480120",
    "end": "482880"
  },
  {
    "text": "that description is\nan exponential growth",
    "start": "482880",
    "end": "485310"
  },
  {
    "text": "in the number of settings.",
    "start": "485310",
    "end": "487139"
  },
  {
    "text": "For each setting,\nyou cross-validate it",
    "start": "487140",
    "end": "489690"
  },
  {
    "text": "on the available training\ndata, which might imply",
    "start": "489690",
    "end": "492330"
  },
  {
    "text": "5, or 10, or 20 experiments.",
    "start": "492330",
    "end": "495599"
  },
  {
    "text": "Then you choose the settings\nthat did best in step 3,",
    "start": "495600",
    "end": "498690"
  },
  {
    "text": "and you train on all the\ntrain data using that setting,",
    "start": "498690",
    "end": "501720"
  },
  {
    "text": "and then you evaluate that\nmodel on the test set.",
    "start": "501720",
    "end": "504120"
  },
  {
    "text": "That is a pristine\nversion of the protocol",
    "start": "504120",
    "end": "507449"
  },
  {
    "text": "that we might be implementing.",
    "start": "507450",
    "end": "509900"
  },
  {
    "text": "But here's the problem.",
    "start": "509900",
    "end": "511500"
  },
  {
    "text": "Suppose parameter\nh1 has five values,",
    "start": "511500",
    "end": "514039"
  },
  {
    "text": "and parameter h2 has 10, then\nthe total number of settings",
    "start": "514039",
    "end": "517700"
  },
  {
    "text": "is now 50.",
    "start": "517700",
    "end": "519140"
  },
  {
    "text": "Suppose we add 3,\nthen it goes to 100.",
    "start": "519140",
    "end": "523250"
  },
  {
    "text": "Now, suppose we're going to\ndo five-fold cross-validation",
    "start": "523250",
    "end": "526130"
  },
  {
    "text": "to select optimal parameters,\nnow we are at 500 runs.",
    "start": "526130",
    "end": "529940"
  },
  {
    "text": "Very quickly, the number\nof experiments exploded.",
    "start": "529940",
    "end": "533930"
  },
  {
    "text": "And if each one of\nthese runs takes a day,",
    "start": "533930",
    "end": "536240"
  },
  {
    "text": "you're pretty much out of\ncontention in terms of actually",
    "start": "536240",
    "end": "538970"
  },
  {
    "text": "implementing this\nprotocol completely.",
    "start": "538970",
    "end": "541639"
  },
  {
    "text": "Something has to change, right?",
    "start": "541640",
    "end": "544310"
  },
  {
    "text": "The above is untenable\nas a set of laws",
    "start": "544310",
    "end": "547190"
  },
  {
    "text": "for the scientific community.",
    "start": "547190",
    "end": "548480"
  },
  {
    "text": "We cannot insist on this level\nof hyperparameter optimization.",
    "start": "548480",
    "end": "552620"
  },
  {
    "text": "If we adopted it, complex models\ntrained on large data sets",
    "start": "552620",
    "end": "556400"
  },
  {
    "text": "would end up disfavored\nand only the very wealthy",
    "start": "556400",
    "end": "559520"
  },
  {
    "text": "would be able to participate.",
    "start": "559520",
    "end": "560840"
  },
  {
    "text": "And to give you a\nglimpse of this,",
    "start": "560840",
    "end": "562257"
  },
  {
    "text": "here's a quote from a\npaper from a team at Google",
    "start": "562257",
    "end": "564620"
  },
  {
    "text": "doing NLP for health care.",
    "start": "564620",
    "end": "567020"
  },
  {
    "text": "\"The performance of all\nabove neural networks",
    "start": "567020",
    "end": "569610"
  },
  {
    "text": "were tuned automatically using\nGoogle Vizier with a total",
    "start": "569610",
    "end": "573209"
  },
  {
    "text": "of over 200,000 GPU hours.\"",
    "start": "573210",
    "end": "576750"
  },
  {
    "text": "That is a lot of money\nspent on a lot of compute.",
    "start": "576750",
    "end": "580620"
  },
  {
    "text": "Obviously, we cannot insist on\na similar level of investment",
    "start": "580620",
    "end": "584640"
  },
  {
    "text": "for experiments, say, for\nthis course, but frankly,",
    "start": "584640",
    "end": "587640"
  },
  {
    "text": "for any contribution\nin the field.",
    "start": "587640",
    "end": "589470"
  },
  {
    "text": "We have to have compromises.",
    "start": "589470",
    "end": "591389"
  },
  {
    "text": "So here are some\nreasonable compromises.",
    "start": "591390",
    "end": "593670"
  },
  {
    "text": "These are pragmatic steps\nyou can take to alleviate",
    "start": "593670",
    "end": "596850"
  },
  {
    "text": "this resource problem.",
    "start": "596850",
    "end": "598620"
  },
  {
    "text": "I've given them in descending\norder of attractiveness,",
    "start": "598620",
    "end": "601500"
  },
  {
    "text": "and I find that\nas the days go by,",
    "start": "601500",
    "end": "604350"
  },
  {
    "text": "we need to go lower\nand lower on this list.",
    "start": "604350",
    "end": "608649"
  },
  {
    "text": "You could do random\nsampling and guided",
    "start": "608650",
    "end": "611010"
  },
  {
    "text": "sampling to explore a large\nspace on a fixed budget.",
    "start": "611010",
    "end": "613980"
  },
  {
    "text": "This is kind of nice\nbecause you have",
    "start": "613980",
    "end": "615570"
  },
  {
    "text": "the cross product of all of\nthe settings that's too large.",
    "start": "615570",
    "end": "619110"
  },
  {
    "text": "You simply randomly\nsample in the space",
    "start": "619110",
    "end": "621540"
  },
  {
    "text": "maybe with some\nguidance from a model,",
    "start": "621540",
    "end": "623730"
  },
  {
    "text": "and you can then, on a fixed\nbudget of, say, 5, or 10,",
    "start": "623730",
    "end": "626870"
  },
  {
    "text": "or 100 runs, do a version\nof the full grid search.",
    "start": "626870",
    "end": "631470"
  },
  {
    "text": "You could also search based\non a few epochs of training.",
    "start": "631470",
    "end": "634649"
  },
  {
    "text": "The expense comes\nfrom multiple epochs,",
    "start": "634650",
    "end": "636870"
  },
  {
    "text": "maybe you do one or\ntwo, and then you",
    "start": "636870",
    "end": "639150"
  },
  {
    "text": "pick the hyperparameters\nthat were best at that point.",
    "start": "639150",
    "end": "642340"
  },
  {
    "text": "And if the learning curves\nare familiar and consistent,",
    "start": "642340",
    "end": "647220"
  },
  {
    "text": "then this will be a pretty\nstrong approach here.",
    "start": "647220",
    "end": "650759"
  },
  {
    "text": "You could also search based\non subsets of the data.",
    "start": "650760",
    "end": "653830"
  },
  {
    "text": "This is fine, but\nit could be risky",
    "start": "653830",
    "end": "656070"
  },
  {
    "text": "because we know some\nparameters are very",
    "start": "656070",
    "end": "658020"
  },
  {
    "text": "dependent on data set size.",
    "start": "658020",
    "end": "660000"
  },
  {
    "text": "And so you're selecting\nbased on small data",
    "start": "660000",
    "end": "662250"
  },
  {
    "text": "and applying it\nto large data even",
    "start": "662250",
    "end": "663960"
  },
  {
    "text": "though you know that's\nprobably a risky assumption.",
    "start": "663960",
    "end": "667920"
  },
  {
    "text": "You could do heuristic\nsearch and define",
    "start": "667920",
    "end": "670470"
  },
  {
    "text": "which hyperparameters matter\nless, and then set them by hand",
    "start": "670470",
    "end": "673980"
  },
  {
    "text": "and justify that in the paper.",
    "start": "673980",
    "end": "675899"
  },
  {
    "text": "That's increasingly common.",
    "start": "675900",
    "end": "677520"
  },
  {
    "text": "People describe things\nlike, we determined",
    "start": "677520",
    "end": "679680"
  },
  {
    "text": "in our initial experiments\nthat these hyperparameters",
    "start": "679680",
    "end": "682830"
  },
  {
    "text": "had this optimal value or\ndidn't matter that much,",
    "start": "682830",
    "end": "685990"
  },
  {
    "text": "and so we chose these\nreasonable values.",
    "start": "685990",
    "end": "688890"
  },
  {
    "text": "And then the actual\nsearch happens only",
    "start": "688890",
    "end": "691180"
  },
  {
    "text": "over the ones that you\ncan tell are important.",
    "start": "691180",
    "end": "694585"
  },
  {
    "text": "We have to take your word for it\nthat you've done the heuristic",
    "start": "694585",
    "end": "697210"
  },
  {
    "text": "search responsibly,\nbut this is, obviously,",
    "start": "697210",
    "end": "699700"
  },
  {
    "text": "a really good way to\nbalance exploration",
    "start": "699700",
    "end": "702550"
  },
  {
    "text": "with constrained resources.",
    "start": "702550",
    "end": "705235"
  },
  {
    "text": "You could find the\noptimal hyperparameters",
    "start": "705235",
    "end": "706985"
  },
  {
    "text": "via a single split and use\nthem for all subsequent splits,",
    "start": "706985",
    "end": "710600"
  },
  {
    "text": "and then justify that\nbased on the fact",
    "start": "710600",
    "end": "712823"
  },
  {
    "text": "that the splits are similar.",
    "start": "712823",
    "end": "713990"
  },
  {
    "text": "That would automatically\ncut down, substantially,",
    "start": "713990",
    "end": "716839"
  },
  {
    "text": "on the number of\nruns you need to do",
    "start": "716840",
    "end": "718460"
  },
  {
    "text": "because you don't\nneed to do so much",
    "start": "718460",
    "end": "720170"
  },
  {
    "text": "cross-validation in this mode.",
    "start": "720170",
    "end": "723389"
  },
  {
    "text": "And then finally, you could\nadopt others choices, right?",
    "start": "723390",
    "end": "725917"
  },
  {
    "text": "And the skeptics will complain\nthat these findings don't",
    "start": "725917",
    "end": "728250"
  },
  {
    "text": "translate to new data sets, but\nit could be the only option.",
    "start": "728250",
    "end": "731740"
  },
  {
    "text": "And as I say, a few years ago,\nthis was kind of frowned upon.",
    "start": "731740",
    "end": "735779"
  },
  {
    "text": "But now in the modern era\nwith these massive models,",
    "start": "735780",
    "end": "738660"
  },
  {
    "text": "it's basically the only option.",
    "start": "738660",
    "end": "740379"
  },
  {
    "text": "And so I think,\nincreasingly, people",
    "start": "740380",
    "end": "742350"
  },
  {
    "text": "are simply carrying forward\nother hyperparameters.",
    "start": "742350",
    "end": "745259"
  },
  {
    "text": "It means less exploration.",
    "start": "745260",
    "end": "747180"
  },
  {
    "text": "We might not be seeing the\nbest versions of these models,",
    "start": "747180",
    "end": "749790"
  },
  {
    "text": "but it might then, again,\nbe the only option.",
    "start": "749790",
    "end": "753990"
  },
  {
    "text": "In terms of tools for\nhyperparameter search,",
    "start": "753990",
    "end": "756270"
  },
  {
    "text": "as always, Scikit is really\nrich with these things.",
    "start": "756270",
    "end": "758610"
  },
  {
    "text": "They've got a lot\nof these toolings.",
    "start": "758610",
    "end": "760800"
  },
  {
    "text": "And in addition,\nScikit-Optimize is one level up",
    "start": "760800",
    "end": "764459"
  },
  {
    "text": "in terms of sophistication.",
    "start": "764460",
    "end": "766110"
  },
  {
    "text": "That's where you could\ndo model guided search",
    "start": "766110",
    "end": "768750"
  },
  {
    "text": "through a hyperparameter\ngrid in order",
    "start": "768750",
    "end": "771060"
  },
  {
    "text": "to intelligently select\ngood settings to lead",
    "start": "771060",
    "end": "774660"
  },
  {
    "text": "to a good model\non a fixed budget.",
    "start": "774660",
    "end": "778459"
  },
  {
    "text": "Next topic,\nclassifier comparison.",
    "start": "778460",
    "end": "780762"
  },
  {
    "text": "This is a short one, but\nthis can be important.",
    "start": "780762",
    "end": "782720"
  },
  {
    "text": "Suppose you've assessed\ntwo classifier models.",
    "start": "782720",
    "end": "785600"
  },
  {
    "text": "Their performance is probably\ndifferent in some way.",
    "start": "785600",
    "end": "788959"
  },
  {
    "text": "What can you do to establish\nwhether these models are",
    "start": "788960",
    "end": "791810"
  },
  {
    "text": "different in any\nmeaningful sense?",
    "start": "791810",
    "end": "793937"
  },
  {
    "text": "I think there are a few options.",
    "start": "793937",
    "end": "795270"
  },
  {
    "text": "The first would be\npractical differences.",
    "start": "795270",
    "end": "797630"
  },
  {
    "text": "If they, obviously,\nmake a large number",
    "start": "797630",
    "end": "799880"
  },
  {
    "text": "of different\npredictions, you might",
    "start": "799880",
    "end": "801410"
  },
  {
    "text": "be able to quantify\nthat difference",
    "start": "801410",
    "end": "803329"
  },
  {
    "text": "in terms of some actual\nexternal outcome.",
    "start": "803330",
    "end": "806360"
  },
  {
    "text": "That's a really good\nscenario to be in.",
    "start": "806360",
    "end": "809730"
  },
  {
    "text": "You could also think\nabout confidence intervals",
    "start": "809730",
    "end": "812060"
  },
  {
    "text": "to further bolster the\nargument that you're making.",
    "start": "812060",
    "end": "814880"
  },
  {
    "text": "This will give us a picture\nof how consistently different",
    "start": "814880",
    "end": "818100"
  },
  {
    "text": "the two systems are.",
    "start": "818100",
    "end": "819589"
  },
  {
    "text": "And if they are\nconsistently different,",
    "start": "819590",
    "end": "821600"
  },
  {
    "text": "then you have a very clear\nargument in favor of one",
    "start": "821600",
    "end": "824209"
  },
  {
    "text": "over the other.",
    "start": "824210",
    "end": "825880"
  },
  {
    "text": "The Wilcoxon signed-rank\ntest is a kind",
    "start": "825880",
    "end": "828120"
  },
  {
    "text": "of accepted method in the field\nfor assessing classifiers using",
    "start": "828120",
    "end": "832350"
  },
  {
    "text": "methodologies that are\nsimilar to standard t-tests,",
    "start": "832350",
    "end": "835290"
  },
  {
    "text": "and I guess the\nconsensus is just",
    "start": "835290",
    "end": "836970"
  },
  {
    "text": "that the assumptions\nbehind the Wilcoxon test",
    "start": "836970",
    "end": "839579"
  },
  {
    "text": "are somewhat more aligned\nwith classifier comparison.",
    "start": "839580",
    "end": "842910"
  },
  {
    "text": "To do that as well as\nconfidence intervals,",
    "start": "842910",
    "end": "845339"
  },
  {
    "text": "you will have had to\nrun your model on lots",
    "start": "845340",
    "end": "848040"
  },
  {
    "text": "of different settings to get a\nlong vector of 10 to 20 scores",
    "start": "848040",
    "end": "852449"
  },
  {
    "text": "to use as the basis\nfor the stats testing.",
    "start": "852450",
    "end": "855600"
  },
  {
    "text": "If that is too expensive, you\ncould opt for McNemar's test.",
    "start": "855600",
    "end": "859769"
  },
  {
    "text": "This is a comparison that you\ndo over two single trained",
    "start": "859770",
    "end": "864960"
  },
  {
    "text": "classifiers based on\ntheir confusion matrices.",
    "start": "864960",
    "end": "867970"
  },
  {
    "text": "So you only need one run.",
    "start": "867970",
    "end": "869459"
  },
  {
    "text": "It will be unstable if\nthe models are unstable,",
    "start": "869460",
    "end": "872340"
  },
  {
    "text": "but it is a way of\ndoing a stats test",
    "start": "872340",
    "end": "874980"
  },
  {
    "text": "in the mode of the\nchi-squared test",
    "start": "874980",
    "end": "877829"
  },
  {
    "text": "to give you some information\nabout how two fixed artifacts",
    "start": "877830",
    "end": "881040"
  },
  {
    "text": "compare to each other.",
    "start": "881040",
    "end": "882389"
  },
  {
    "text": "Not as strong as the\nprevious methods,",
    "start": "882390",
    "end": "884860"
  },
  {
    "text": "but nonetheless useful and\narguably better than nothing.",
    "start": "884860",
    "end": "890480"
  },
  {
    "text": "OK.",
    "start": "890480",
    "end": "891199"
  },
  {
    "text": "A special topic\nfor deep learning,",
    "start": "891200",
    "end": "894170"
  },
  {
    "text": "how do you assess models\nwithout convergence?",
    "start": "894170",
    "end": "896660"
  },
  {
    "text": "This never used to arise.",
    "start": "896660",
    "end": "898279"
  },
  {
    "text": "Back in the days of\nall linear models,",
    "start": "898280",
    "end": "900830"
  },
  {
    "text": "all these models would\nconverge more or less instantly",
    "start": "900830",
    "end": "903590"
  },
  {
    "text": "to epsilon loss,\nand then you could",
    "start": "903590",
    "end": "905840"
  },
  {
    "text": "feel like that was how\nyou would move forward",
    "start": "905840",
    "end": "907850"
  },
  {
    "text": "with assessing them, right?",
    "start": "907850",
    "end": "910160"
  },
  {
    "text": "But now with neural networks,\nconvergence has really",
    "start": "910160",
    "end": "913310"
  },
  {
    "text": "taken center stage, and\nit's in a complicated way",
    "start": "913310",
    "end": "915830"
  },
  {
    "text": "that it takes center stage.",
    "start": "915830",
    "end": "917090"
  },
  {
    "text": "First, these models rarely\nconverge to epsilon loss.",
    "start": "917090",
    "end": "920960"
  },
  {
    "text": "And therefore, it's\na kind of nonissue",
    "start": "920960",
    "end": "923390"
  },
  {
    "text": "whether or not that would\nbe your stopping criterion.",
    "start": "923390",
    "end": "925850"
  },
  {
    "text": "In addition, they might\nconverge at different rates",
    "start": "925850",
    "end": "928310"
  },
  {
    "text": "between runs, and their\nperformance on the test set",
    "start": "928310",
    "end": "931730"
  },
  {
    "text": "might not even be\nespecially related",
    "start": "931730",
    "end": "933980"
  },
  {
    "text": "to how small the loss got.",
    "start": "933980",
    "end": "936709"
  },
  {
    "text": "So you need to be thoughtful\nabout exactly what",
    "start": "936710",
    "end": "939860"
  },
  {
    "text": "your stopping criteria will be.",
    "start": "939860",
    "end": "941750"
  },
  {
    "text": "Yes, sometimes a model\nwith low final error",
    "start": "941750",
    "end": "944270"
  },
  {
    "text": "turns out to be great,\nand sometimes it's",
    "start": "944270",
    "end": "946400"
  },
  {
    "text": "worse than the one that\nfinished with a higher error.",
    "start": "946400",
    "end": "948740"
  },
  {
    "text": "This might have something\nto do with overfitting",
    "start": "948740",
    "end": "951120"
  },
  {
    "text": "and regularization, but\nthe bottom line here",
    "start": "951120",
    "end": "953550"
  },
  {
    "text": "is we don't know a\npriori what's going on.",
    "start": "953550",
    "end": "956130"
  },
  {
    "text": "This is very experiment-driven.",
    "start": "956130",
    "end": "959850"
  },
  {
    "text": "So one thing to think about for\nstopping criteria in general",
    "start": "959850",
    "end": "963540"
  },
  {
    "text": "is what we call incremental\ndev-set testing.",
    "start": "963540",
    "end": "966750"
  },
  {
    "text": "To address the\nkind of uncertainty",
    "start": "966750",
    "end": "968640"
  },
  {
    "text": "that I just reviewed,\nyou regularly",
    "start": "968640",
    "end": "970530"
  },
  {
    "text": "collect information\nabout dev-set performance",
    "start": "970530",
    "end": "973530"
  },
  {
    "text": "as part of the training\nthat you're doing.",
    "start": "973530",
    "end": "975780"
  },
  {
    "text": "For example, at every\n100th iteration,",
    "start": "975780",
    "end": "978270"
  },
  {
    "text": "you could make\npredictions on the dev set",
    "start": "978270",
    "end": "980520"
  },
  {
    "text": "and store the resulting\nvector of predictions.",
    "start": "980520",
    "end": "984210"
  },
  {
    "text": "All the PyTorch\nmodels for this course",
    "start": "984210",
    "end": "986680"
  },
  {
    "text": "have an early stopping\nparameter and a bunch",
    "start": "986680",
    "end": "988890"
  },
  {
    "text": "of related parameters that\nwill help you set this up",
    "start": "988890",
    "end": "991800"
  },
  {
    "text": "in a way that will allow you\nto do this incremental testing",
    "start": "991800",
    "end": "994980"
  },
  {
    "text": "and with luck get\nthe best model.",
    "start": "994980",
    "end": "997709"
  },
  {
    "text": "And if you need a little\nbit of motivation for this,",
    "start": "997710",
    "end": "1000120"
  },
  {
    "text": "here are some plots\nfrom an actual model.",
    "start": "1000120",
    "end": "1002390"
  },
  {
    "text": "You can see the loss\ngoing down very steadily",
    "start": "1002390",
    "end": "1005390"
  },
  {
    "text": "across different\niterations of training,",
    "start": "1005390",
    "end": "1007760"
  },
  {
    "text": "but the performance\non the dev set",
    "start": "1007760",
    "end": "1009860"
  },
  {
    "text": "tells a very different story.",
    "start": "1009860",
    "end": "1011940"
  },
  {
    "text": "You can see, based\non this performance,",
    "start": "1011940",
    "end": "1013880"
  },
  {
    "text": "that at a certain very\nearly point in this process,",
    "start": "1013880",
    "end": "1016880"
  },
  {
    "text": "around iteration\n10, our performance",
    "start": "1016880",
    "end": "1019400"
  },
  {
    "text": "was actually getting worse even\nthough the loss was going down.",
    "start": "1019400",
    "end": "1022640"
  },
  {
    "text": "And that just shows you that\nsometimes the steady loss curve",
    "start": "1022640",
    "end": "1026030"
  },
  {
    "text": "is a picture of overfitting\nand not of your model actually",
    "start": "1026030",
    "end": "1030199"
  },
  {
    "text": "getting better at the\nthing that you care about.",
    "start": "1030200",
    "end": "1033289"
  },
  {
    "text": "So think carefully about\nyour stopping criteria.",
    "start": "1033290",
    "end": "1036709"
  },
  {
    "text": "And in general,\nthough, I think we",
    "start": "1036710",
    "end": "1039199"
  },
  {
    "text": "might want to take a more\nexpansive view of how",
    "start": "1039200",
    "end": "1042380"
  },
  {
    "text": "we do evaluation in this mode.",
    "start": "1042380",
    "end": "1044449"
  },
  {
    "text": "Here what I'm\npitching is that we",
    "start": "1044450",
    "end": "1046279"
  },
  {
    "text": "look at the entire\nperformance curve,",
    "start": "1046280",
    "end": "1049070"
  },
  {
    "text": "maybe with confidence\nintervals, so we can make",
    "start": "1049070",
    "end": "1051559"
  },
  {
    "text": "some confident distinctions.",
    "start": "1051560",
    "end": "1053270"
  },
  {
    "text": "All these plots for\ndifferent conditions",
    "start": "1053270",
    "end": "1055370"
  },
  {
    "text": "across models we were comparing\nhave epochs along the x-axis",
    "start": "1055370",
    "end": "1059630"
  },
  {
    "text": "and F1 along the y-axis.",
    "start": "1059630",
    "end": "1062100"
  },
  {
    "text": "And if you step back,\nwhat I think you see",
    "start": "1062100",
    "end": "1064580"
  },
  {
    "text": "is that our Mittens model, the\none that we were advocating",
    "start": "1064580",
    "end": "1067760"
  },
  {
    "text": "for, is the best model\non average, but largely",
    "start": "1067760",
    "end": "1071450"
  },
  {
    "text": "in early parts of training.",
    "start": "1071450",
    "end": "1073279"
  },
  {
    "text": "If you train for long enough,\na lot of the distinctions",
    "start": "1073280",
    "end": "1076340"
  },
  {
    "text": "disappear.",
    "start": "1076340",
    "end": "1078169"
  },
  {
    "text": "So if you have a fixed\nbudget of epochs,",
    "start": "1078170",
    "end": "1080440"
  },
  {
    "text": "Mittens is a good choice.",
    "start": "1080440",
    "end": "1081669"
  },
  {
    "text": "If you don't care\nabout the resources,",
    "start": "1081670",
    "end": "1083680"
  },
  {
    "text": "it might not be so clear\nwhich one you should choose,",
    "start": "1083680",
    "end": "1086260"
  },
  {
    "text": "maybe it doesn't matter.",
    "start": "1086260",
    "end": "1087490"
  },
  {
    "text": "That's a nuanced\nlesson that I think",
    "start": "1087490",
    "end": "1089800"
  },
  {
    "text": "is really powerful to\nteach, and we can't really",
    "start": "1089800",
    "end": "1092770"
  },
  {
    "text": "teach it if all we\ndo is offer point",
    "start": "1092770",
    "end": "1094810"
  },
  {
    "text": "estimates of model performance.",
    "start": "1094810",
    "end": "1096340"
  },
  {
    "text": "We really need to\nsee the full curve",
    "start": "1096340",
    "end": "1098289"
  },
  {
    "text": "to see that level of nuance.",
    "start": "1098290",
    "end": "1100000"
  },
  {
    "text": "And so I know that NLPers\nlove their results tables,",
    "start": "1100000",
    "end": "1103900"
  },
  {
    "text": "you should have results\ntables, but maybe you",
    "start": "1103900",
    "end": "1106510"
  },
  {
    "text": "could supplement them\nwith some figures that",
    "start": "1106510",
    "end": "1108490"
  },
  {
    "text": "would give us a fuller\npicture of what was going on.",
    "start": "1108490",
    "end": "1113320"
  },
  {
    "text": "Final topic, the role of random\nparameter initialization.",
    "start": "1113320",
    "end": "1117130"
  },
  {
    "text": "Most deep learning\nmodels have parameters",
    "start": "1117130",
    "end": "1119320"
  },
  {
    "text": "that are random at the start.",
    "start": "1119320",
    "end": "1120674"
  },
  {
    "text": "Even if they're\npretrained, there",
    "start": "1120675",
    "end": "1122050"
  },
  {
    "text": "are usually some random\nparameters in the mix there.",
    "start": "1122050",
    "end": "1125600"
  },
  {
    "text": "This is clearly meaningful\nfor the nonconvex problems",
    "start": "1125600",
    "end": "1129190"
  },
  {
    "text": "that we're posing.",
    "start": "1129190",
    "end": "1130419"
  },
  {
    "text": "Simpler models can\nbe impacted as well,",
    "start": "1130420",
    "end": "1132490"
  },
  {
    "text": "but it's especially pressing\nin the deep learning era.",
    "start": "1132490",
    "end": "1135010"
  },
  {
    "text": "And here is a relatively\nrecent paper showing, actually,",
    "start": "1135010",
    "end": "1137840"
  },
  {
    "text": "that different initializations\nfor neural sequence models",
    "start": "1137840",
    "end": "1140870"
  },
  {
    "text": "led to statistically significant\ndifferences in performance.",
    "start": "1140870",
    "end": "1145260"
  },
  {
    "text": "And so a number\nof recent systems",
    "start": "1145260",
    "end": "1147290"
  },
  {
    "text": "were actually\nindistinguishable in terms",
    "start": "1147290",
    "end": "1149270"
  },
  {
    "text": "of their raw performance once\nwe took the source of variation",
    "start": "1149270",
    "end": "1152660"
  },
  {
    "text": "into account.",
    "start": "1152660",
    "end": "1154040"
  },
  {
    "text": "There's a related issue\nhere of catastrophic failure",
    "start": "1154040",
    "end": "1157130"
  },
  {
    "text": "from unlucky initializations.",
    "start": "1157130",
    "end": "1159110"
  },
  {
    "text": "Sometimes that happens.",
    "start": "1159110",
    "end": "1160790"
  },
  {
    "text": "Sometimes you see it.",
    "start": "1160790",
    "end": "1161930"
  },
  {
    "text": "Sometimes it's hard to notice.",
    "start": "1161930",
    "end": "1164120"
  },
  {
    "text": "There's a question of how\nto report that as part",
    "start": "1164120",
    "end": "1166580"
  },
  {
    "text": "of overall system performance.",
    "start": "1166580",
    "end": "1168350"
  },
  {
    "text": "We need to be\nreflective about this.",
    "start": "1168350",
    "end": "1170570"
  },
  {
    "text": "And maybe the bottom\nline here is just",
    "start": "1170570",
    "end": "1172730"
  },
  {
    "text": "for the associated notebook for\nthis unit, evaluation methods,",
    "start": "1172730",
    "end": "1176240"
  },
  {
    "text": "I just showed you with\nthe classic XOR problem,",
    "start": "1176240",
    "end": "1179900"
  },
  {
    "text": "which has always been used to\nmotivate the powerful models",
    "start": "1179900",
    "end": "1182900"
  },
  {
    "text": "that we work with now, that\nyou don't actually get success",
    "start": "1182900",
    "end": "1186440"
  },
  {
    "text": "for a simple feedforward\nnetwork for that problem.",
    "start": "1186440",
    "end": "1189559"
  },
  {
    "text": "Eight out of 10\ntimes it succeeds,",
    "start": "1189560",
    "end": "1191660"
  },
  {
    "text": "and two out of the 10 times\nit's a colossal failure.",
    "start": "1191660",
    "end": "1194750"
  },
  {
    "text": "That is a glimpse of just\nhow important initialization",
    "start": "1194750",
    "end": "1198540"
  },
  {
    "text": "can be.",
    "start": "1198540",
    "end": "1199040"
  },
  {
    "text": "And since we don't\nanalytically understand",
    "start": "1199040",
    "end": "1201380"
  },
  {
    "text": "why we're seeing this\nvariation, the best response,",
    "start": "1201380",
    "end": "1204320"
  },
  {
    "text": "if you can afford it, is\na bunch more experiments.",
    "start": "1204320",
    "end": "1209389"
  },
  {
    "text": "All right.",
    "start": "1209390",
    "end": "1209890"
  },
  {
    "text": "Let's wrap up.",
    "start": "1209890",
    "end": "1211000"
  },
  {
    "text": "A lot of this in\nthe back of my mind",
    "start": "1211000",
    "end": "1213250"
  },
  {
    "text": "is oriented toward helping\nyou with the protocols, which",
    "start": "1213250",
    "end": "1216010"
  },
  {
    "text": "is a document associated\nwith your final project",
    "start": "1216010",
    "end": "1219100"
  },
  {
    "text": "where you give us the nuts\nand bolts of the project",
    "start": "1219100",
    "end": "1221770"
  },
  {
    "text": "and try to identify any\nobstacles to success.",
    "start": "1221770",
    "end": "1225080"
  },
  {
    "text": "So all the lessons we've been\nteaching throughout this series",
    "start": "1225080",
    "end": "1228100"
  },
  {
    "text": "are oriented toward helping\nyou think critically",
    "start": "1228100",
    "end": "1230500"
  },
  {
    "text": "about this protocol, and\nultimately set up a firm",
    "start": "1230500",
    "end": "1233350"
  },
  {
    "text": "foundation for your project.",
    "start": "1233350",
    "end": "1236020"
  },
  {
    "text": "With that out of\nthe way, I thought",
    "start": "1236020",
    "end": "1237520"
  },
  {
    "text": "I would look ahead to this\nmoment and the future.",
    "start": "1237520",
    "end": "1241300"
  },
  {
    "text": "I think this is an ideal\nmoment for innovation",
    "start": "1241300",
    "end": "1244060"
  },
  {
    "text": "in surprising new places.",
    "start": "1244060",
    "end": "1246220"
  },
  {
    "text": "Architecture innovation,\nway overrated at this point.",
    "start": "1246220",
    "end": "1250580"
  },
  {
    "text": "I mean, it's still\nimportant, but it",
    "start": "1250580",
    "end": "1252159"
  },
  {
    "text": "is overrated relative to the\namount of things people do.",
    "start": "1252160",
    "end": "1255460"
  },
  {
    "text": "Metric innovation,\nway underrated.",
    "start": "1255460",
    "end": "1258730"
  },
  {
    "text": "It's been a theme\nof these lectures",
    "start": "1258730",
    "end": "1260470"
  },
  {
    "text": "that we need to think very\ncarefully about our metrics",
    "start": "1260470",
    "end": "1263539"
  },
  {
    "text": "because they are our guidepost\ntoward whether we're succeeding",
    "start": "1263540",
    "end": "1267340"
  },
  {
    "text": "or not.",
    "start": "1267340",
    "end": "1268029"
  },
  {
    "text": "Relatedly, evaluations\nin general.",
    "start": "1268030",
    "end": "1270540"
  },
  {
    "text": "We need innovation there.",
    "start": "1270540",
    "end": "1271700"
  },
  {
    "text": "This is way underrated by\nthe community at this point.",
    "start": "1271700",
    "end": "1274940"
  },
  {
    "text": "Task innovation, underrated.",
    "start": "1274940",
    "end": "1276950"
  },
  {
    "text": "We are seeing some\nthings, so it's not",
    "start": "1276950",
    "end": "1278630"
  },
  {
    "text": "as bad as two and three,\nbut still, we should all",
    "start": "1278630",
    "end": "1281240"
  },
  {
    "text": "be participating in this area.",
    "start": "1281240",
    "end": "1283340"
  },
  {
    "text": "And then finally, exhaustive\nhyperparameter search.",
    "start": "1283340",
    "end": "1286610"
  },
  {
    "text": "You need to weigh this\nagainst other factors.",
    "start": "1286610",
    "end": "1289160"
  },
  {
    "text": "There is more at\nplay here than just",
    "start": "1289160",
    "end": "1291110"
  },
  {
    "text": "that pristine\nscientific paradigm.",
    "start": "1291110",
    "end": "1293510"
  },
  {
    "text": "We need to think about\ncosts in every sense",
    "start": "1293510",
    "end": "1296330"
  },
  {
    "text": "and how it relates to\nthe kind of innovations",
    "start": "1296330",
    "end": "1298700"
  },
  {
    "text": "that we're likely to see.",
    "start": "1298700",
    "end": "1299960"
  },
  {
    "text": "So I'm pitching a\npragmatic approach,",
    "start": "1299960",
    "end": "1302029"
  },
  {
    "text": "but I'm also exhorting\nyou to think expansively",
    "start": "1302030",
    "end": "1305600"
  },
  {
    "text": "about how you might participate\nin pushing the field forward.",
    "start": "1305600",
    "end": "1309820"
  },
  {
    "start": "1309820",
    "end": "1314000"
  }
]