[
  {
    "text": "All right. We're gonna go ahead and get started, and in consistent with this, uh, theme of this section, we're gonna be optimistic under",
    "start": "4280",
    "end": "10800"
  },
  {
    "text": "uncertainty and hope that this will work but we will see. Um, before we get into the content, um,",
    "start": "10800",
    "end": "16230"
  },
  {
    "text": "I wanted to ask if anybody has any questions about logistics or, um, any other aspects of the general course. Yeah.",
    "start": "16230",
    "end": "22035"
  },
  {
    "text": "I just wanna double check with people who are doing the default project are okay, either not submitting or just submitting",
    "start": "22035",
    "end": "27960"
  },
  {
    "text": "another thing for the milestone instead of doing the default. Yes. Yeah, so, the question was asking about, um, to repeat.",
    "start": "27960",
    "end": "36240"
  },
  {
    "text": "The question was asking about whether or not people who were doing the default project if they need to do anything in particular for the milestone.",
    "start": "36240",
    "end": "41480"
  },
  {
    "text": "Um, no, you don't. Any other questions? Okay. So, just as a reminder,",
    "start": "41480",
    "end": "48920"
  },
  {
    "text": "where we are sort of in the class right now is, um, last time we started talking about bandits and regret and we'll do,",
    "start": "48920",
    "end": "55085"
  },
  {
    "text": "um, a brief, uh, recap of that today and we're gonna continue to talk about fast learning. Um, and we're gonna go from Bayesian bandits towards Markov decision processes today,",
    "start": "55085",
    "end": "64159"
  },
  {
    "text": "and then on Wednesday we're gonna talk some more about fast learning and then we're gonna try to talk about sort of fast learning and exploration.",
    "start": "64160",
    "end": "71520"
  },
  {
    "text": "And just to remind us all about, like, why we're doing this, um, the idea was that if we wanna move reinforcement learning into real-world applications,",
    "start": "76400",
    "end": "84320"
  },
  {
    "text": "we need to think about carefully taking the data that we have, um, and how do we gather it and how do we best use it so that we don't need to",
    "start": "84320",
    "end": "90950"
  },
  {
    "text": "collect a lot of data in order for our agents to learn to make good decisions. One of my original interests in",
    "start": "90950",
    "end": "96950"
  },
  {
    "text": "this whole topic was to think sort of formally about what does it mean for an agent to learn to make good decisions and what are the sort of,",
    "start": "96950",
    "end": "103159"
  },
  {
    "text": "information theoretic limits of how much information would an agent need in order to be able to make a provably optimal decision.",
    "start": "103160",
    "end": "110250"
  },
  {
    "text": "So, we've been thinking about sort of a couple of different main things here. We're, we're talking about a couple of settings.",
    "start": "110630",
    "end": "116030"
  },
  {
    "text": "Last time we talked about bandits, today we'll also talk about bandits and Markov decision processes. We're talking about frameworks,",
    "start": "116030",
    "end": "122270"
  },
  {
    "text": "which are ways for us to formally assess how good an algorithm is. Um, you know, these could be the framework of empirical success.",
    "start": "122270",
    "end": "128690"
  },
  {
    "text": "We, we talked last time about the mathematical framework of regret and we'll talk today about some other frameworks for evaluating in",
    "start": "128690",
    "end": "134840"
  },
  {
    "text": "general how good a reinforcement learning algorithm is. And then, we're also talking about styles of approaches that",
    "start": "134840",
    "end": "140240"
  },
  {
    "text": "tend to allow us to achieve these different frameworks. Um, and last time what we started to do is to talk about optimism under uncertainty.",
    "start": "140240",
    "end": "148360"
  },
  {
    "text": "So, just a quick recap from bandits. Um, so, bandit was basically a simplified version of a",
    "start": "148360",
    "end": "156290"
  },
  {
    "text": "Markov decision process where in the most simple setting there's no state, there's a set of actions, um,",
    "start": "156290",
    "end": "162880"
  },
  {
    "text": "and now we're going to think specifically about the fact that the reward is through some stochastic distribution.",
    "start": "162880",
    "end": "168670"
  },
  {
    "text": "Um, there's some unknown probability distribution over rewards. Um, at each timestep you get to select an action,",
    "start": "168670",
    "end": "175505"
  },
  {
    "text": "um, and see, you know, a reward. And then your goal is to select actions in a way that is gonna optimize your rewards over time.",
    "start": "175505",
    "end": "182254"
  },
  {
    "text": "And the reason this was different than a supervised learning problem is that you only get to observe the reward for the action that you sample.",
    "start": "182255",
    "end": "189625"
  },
  {
    "text": "So, it's what's known as censored data. Um, you don't get to see what would have happened if you'd went to Harvard.",
    "start": "189625",
    "end": "194989"
  },
  {
    "text": "So, um, we get to see censored data and we have to use that censored data to make good decisions. Um, and what we discussed here talking about regret, what we mean that in a,",
    "start": "194990",
    "end": "205410"
  },
  {
    "text": "in a formal mathematical sense is that we were comparing, um, the expected reward from the action we took,",
    "start": "205410",
    "end": "212495"
  },
  {
    "text": "um, to the expected reward of the optimal action. Now, notice that all of these things are stochastic.",
    "start": "212495",
    "end": "219709"
  },
  {
    "text": "Um, so, it doesn't have to have a particular parametric distribution but imagine that we're thinking about, um, Gaussians.",
    "start": "219710",
    "end": "228209"
  },
  {
    "text": "Let's say, we had two Gaussians. So, this is action 2 and this is action 1.",
    "start": "228590",
    "end": "235420"
  },
  {
    "text": "Okay. So, here's the mean of action 1. So, Q of a1 is greater than Q of a2.",
    "start": "235520",
    "end": "243540"
  },
  {
    "text": "So, action 1 has the better expected reward. But notice that on any particular trial,",
    "start": "243540",
    "end": "248960"
  },
  {
    "text": "you might sometimes get a, um, uh, you could, ima- imagine getting a result where, um,",
    "start": "248960",
    "end": "254120"
  },
  {
    "text": "the actual reward you get from a sub-optimal arm is better than the expected reward of the optimal arm.",
    "start": "254120",
    "end": "259480"
  },
  {
    "text": "I just wanna highlight that. So, imagine that you've sampled from action A2 and you got here.",
    "start": "259480",
    "end": "264995"
  },
  {
    "text": "It's a particular sample.",
    "start": "264995",
    "end": "267900"
  },
  {
    "text": "So, you could have a particular sample be better than the expected reward of the optimal arm.",
    "start": "270220",
    "end": "276460"
  },
  {
    "text": "But because we're imagining doing this many, many times we're again just looking at expectations. So, we're saying, \"On average,",
    "start": "276460",
    "end": "282385"
  },
  {
    "text": "which is the best arm, and on average how much do we lose from selecting a sub-optimal arm?\"",
    "start": "282385",
    "end": "288410"
  },
  {
    "text": "And the goal was to minimize our total regret which is equivalent to maximizing our cumulative reward over time.",
    "start": "288490",
    "end": "296810"
  },
  {
    "text": "So, we then introduced this idea of optimism under, of, under uncertainty.",
    "start": "297320",
    "end": "302615"
  },
  {
    "text": "Um, and the idea was to, um, uh, uh, estimate an upper confidence bound on the potential expected reward of each of the arms.",
    "start": "302615",
    "end": "311690"
  },
  {
    "text": "So, this was to say we wanna be able to say for each of the arms,",
    "start": "311690",
    "end": "317325"
  },
  {
    "text": "what do we think is an upper bound of their expected value, and then when we act, we're gonna pick whichever arm has the highest upper confidence bound.",
    "start": "317325",
    "end": "325870"
  },
  {
    "text": "And this was gonna lead to one of two outcomes. So, this could, um, two things could happen.",
    "start": "325870",
    "end": "332630"
  },
  {
    "text": "So, either, either at is equal to A star,",
    "start": "335190",
    "end": "343470"
  },
  {
    "text": "and in that case, what's our regret? 0. So, if we select",
    "start": "343470",
    "end": "349639"
  },
  {
    "text": "the optimal action and we have our regret at 0. So, that's good. And we have our regret at 0 like at per timestep or it's not.",
    "start": "349640",
    "end": "359800"
  },
  {
    "text": "And if it's not on average, what happens to that upper confidence bound? Yeah.",
    "start": "362420",
    "end": "370600"
  },
  {
    "text": "Yes. Yeah, [NOISE] that answer is correct. So, we lower it. So, if we get, if we select an arm which is, um, not optimal,",
    "start": "370600",
    "end": "379230"
  },
  {
    "text": "then it means its real mean is lower, um, uh, than the upper confidence bound we're averaging,",
    "start": "379230",
    "end": "384650"
  },
  {
    "text": "um, at least with high probability. And so, then in general our UT of AT will decrease.",
    "start": "384650",
    "end": "391560"
  },
  {
    "text": "So, we're gonna gain information about what the real mean is of that arm and we'll reduce it.",
    "start": "392780",
    "end": "399080"
  },
  {
    "text": "And if we reduce it enough, over time we should find that the optimal arm's upper confidence bound is higher.",
    "start": "399080",
    "end": "405060"
  },
  {
    "text": "So, I'll ask you to play about what might happen if we do lower bounds. Um, but that's one of the reasons why upper bounds is really good, and,",
    "start": "405410",
    "end": "412100"
  },
  {
    "text": "and I mentioned that these ideas have really been around for a long time, um, at least around like 20, almos- 25-30 years.",
    "start": "412100",
    "end": "418635"
  },
  {
    "text": "So, I think the first one was 1993, Kaelbling, by Leslie Kaelbling. She's at MIT.",
    "start": "418635",
    "end": "425069"
  },
  {
    "text": "Um, don't remember if it was her PhD thesis or if it was just after that. Um, uh, but she talked about this idea of interval estimation of estimating,",
    "start": "425070",
    "end": "433800"
  },
  {
    "text": "um, the potential rewards. She didn't do formal proofs of this being a good idea, but she did it for Markov decision processes,",
    "start": "433800",
    "end": "440000"
  },
  {
    "text": "and they found that it was, uh, a very good idea in terms of empirical performance, and then a lot of people went around and did",
    "start": "440000",
    "end": "446420"
  },
  {
    "text": "the theoretical analysis and showed that provably this is a good thing. And I think it's interesting often about which area ends up",
    "start": "446420",
    "end": "451820"
  },
  {
    "text": "being more advanced whether it's the empirical side or the theoretical side. Okay. So, optimism under uncertainty in",
    "start": "451820",
    "end": "457580"
  },
  {
    "text": "the bandit case just involves keeping track of the rewards we've seen and we showed, saw that we could use things like Hoeffding inequality to compute these upper confidence bounds.",
    "start": "457580",
    "end": "467940"
  },
  {
    "text": "Because remember each of our samples from the arms is iid, because they're all coming from the same parametric distribution,",
    "start": "469660",
    "end": "476690"
  },
  {
    "text": "which is unknown and we're given these samples. So, what we found last time is that, uh,",
    "start": "476690",
    "end": "483400"
  },
  {
    "text": "if we use the upper confidence bound algorithm I did a proof on the board to show that with high probability we had logarithmic regret.",
    "start": "483400",
    "end": "490570"
  },
  {
    "text": "Why was this important? Because we looked at greedy algorithms and showed that they could have linear regret.",
    "start": "490570",
    "end": "496205"
  },
  {
    "text": "And what is it linear in? So, um, this is the number of timesteps, number of timesteps we act.",
    "start": "496205",
    "end": "504150"
  },
  {
    "text": "So why is linear regret bad? Well, if it's linear, it means that essentially you could be making the worst decision on",
    "start": "505920",
    "end": "512260"
  },
  {
    "text": "every single time point, and that's pretty bad. So we'd like to have things that are going slower which means",
    "start": "512260",
    "end": "517419"
  },
  {
    "text": "that our algorithm is essentially learning to make good decisions. Now, notice that in this case,",
    "start": "517420",
    "end": "522580"
  },
  {
    "text": "this is a little bit different, um, statement of our result than what we saw before. Um, it's related, this involves the gaps.",
    "start": "522580",
    "end": "529855"
  },
  {
    "text": "This is the gaps, delta a is equal to Q of a star, - q of a.",
    "start": "529855",
    "end": "538555"
  },
  {
    "text": "It's how much worse, it is to take a particular action than to take the optimal action.",
    "start": "538555",
    "end": "543835"
  },
  {
    "text": "Um, and this is related to a bit different,",
    "start": "543835",
    "end": "550855"
  },
  {
    "text": "to the bound from last time. Last time, we proved a bound that was independent of the gaps,",
    "start": "550855",
    "end": "560200"
  },
  {
    "text": "so it didn't matter what the gaps are in the problem. Um, this is the bound that depends on the gaps.",
    "start": "560200",
    "end": "565555"
  },
  {
    "text": "Now, of course, you don't know what the gaps are in practice. If you didn't know the gaps, then you would know the optimal arm to prove, um,",
    "start": "565555",
    "end": "571075"
  },
  {
    "text": "but the nice thing about this is that i- it's always important to know whether or not this sort of knowledge appears in the analysis or in the algorithm.",
    "start": "571075",
    "end": "580470"
  },
  {
    "text": "This is saying, you don't need to know what the gaps are, but if you use this algorithm, [NOISE] how your, how your regret grows depends on a property of the domain.",
    "start": "580470",
    "end": "589345"
  },
  {
    "text": "You don't have to know the property of that domain, but that's what your regret will depend on. And so this is saying that for upper confidence bounds,",
    "start": "589345",
    "end": "597295"
  },
  {
    "text": "if you have different sizes of gaps, you're gonna get different regrets. Um, your algorithm is just going to proceed by following upper confidence bounds,",
    "start": "597295",
    "end": "605695"
  },
  {
    "text": "it doesn't need to know about these gaps, you're gonna get better or worse performance depending on what the gaps are. And in general, we'd really like these.",
    "start": "605695",
    "end": "612550"
  },
  {
    "text": "We'd like our algorithms to be able to adapt to the problem so that thi- this is known as a problem-dependent bound,",
    "start": "612550",
    "end": "619100"
  },
  {
    "text": "right here, and you would like those. You'd like to have algorithms that are agnostic,",
    "start": "619440",
    "end": "625089"
  },
  {
    "text": "that are sort of saying, provide problem-dependent balance, that they work better if the problem is easier. All right.",
    "start": "625090",
    "end": "632589"
  },
  {
    "text": "So let's go to our toy example, which we were talking about as a way to sort of see how these different algorithms work, and we're looking at fake, um, ah,",
    "start": "632590",
    "end": "639699"
  },
  {
    "text": "ways to treat broken toes, um, where we are looking at surgery, buddy taping, or doing nothing,",
    "start": "639700",
    "end": "644875"
  },
  {
    "text": "and we're imagining that we had a, a Bernoulli variable that determined whether or not these treatments worked.",
    "start": "644875",
    "end": "650620"
  },
  {
    "text": "So surgery on in expectation was the most expect- uh, most effective thing, with 0.95 success,",
    "start": "650620",
    "end": "657040"
  },
  {
    "text": "ah, buddy taping was 0.9, and doing nothing was 0.1. So how would something like upper confidence bound algorithms work?",
    "start": "657040",
    "end": "663775"
  },
  {
    "text": "Well, in the beginning we don't have any data, so let's sample all the arms once. That's going to involve sampling from a Bernoulli distribution.",
    "start": "663775",
    "end": "671439"
  },
  {
    "text": "Um, so in this case, let's imagine that we've got, um, a1, a2, and a3 here.",
    "start": "671440",
    "end": "677185"
  },
  {
    "text": "And note that this is, [NOISE], yeah, all right, this is our, their empirical estimate where we just average,",
    "start": "677185",
    "end": "687085"
  },
  {
    "text": "over all the, all the rewards we've seen from a particular arm. Okay? So we pulled each arm once,",
    "start": "687085",
    "end": "695590"
  },
  {
    "text": "which is the same as taking each action once, we got 1-1-0. And now what we have to do is compute the upper confidence bounds",
    "start": "695590",
    "end": "701680"
  },
  {
    "text": "for each of those arms before we know what to do next. Okay. So in this case,",
    "start": "701680",
    "end": "708265"
  },
  {
    "text": "we're gonna define our upper confidence bounds by being the empirical average, plus the square root of 2 log t,",
    "start": "708265",
    "end": "715555"
  },
  {
    "text": "t is the number of times we pulled arms, and N t of a is the number of times we've sampled a particular arm.",
    "start": "715555",
    "end": "721780"
  },
  {
    "text": "So this is total arm pulls.",
    "start": "721780",
    "end": "725000"
  },
  {
    "text": "This is a particular arm.",
    "start": "727290",
    "end": "731630"
  },
  {
    "text": "Okay. So what does that gonna be in this case? Let's just define it for each of them. So UCB of a1 is gonna be equal to 1,",
    "start": "733200",
    "end": "741714"
  },
  {
    "text": "because that's what we've got so far, plus square root 2 log of 3,",
    "start": "741715",
    "end": "747970"
  },
  {
    "text": "because we pulled three arms so far, divided by 1. UCB of a2 is gonna be the same,",
    "start": "747970",
    "end": "757045"
  },
  {
    "text": "because we've also pulled that arm once, and it got the same outcome. And then UCB of a3 is gonna be different,",
    "start": "757045",
    "end": "765115"
  },
  {
    "text": "because its current reward is- or current expected value is 0. So it's just gonna be equal to 2 log 3 divided by 1.",
    "start": "765115",
    "end": "775660"
  },
  {
    "text": "That's how we could instantiate each of the bounds, and now we've defined the upper confidence bounds for each of the,",
    "start": "775820",
    "end": "781080"
  },
  {
    "text": "ah, um, each of the arms. So in this case, after we've done that, we're going to pull one of these arms.",
    "start": "781080",
    "end": "787405"
  },
  {
    "text": "Um, let's say that we break ties randomly, so the upper confidence bound of a1 and a2 is identical.",
    "start": "787405",
    "end": "792685"
  },
  {
    "text": "So with 50% probability, we select one. With 50% probability we can select the other.",
    "start": "792685",
    "end": "798500"
  },
  {
    "text": "Okay. Um, and let's just compare that for a second.",
    "start": "798500",
    "end": "803590"
  },
  {
    "text": "So, um, if we're using UCB, I said that, so I'll just redefine this here so people can remember,",
    "start": "803590",
    "end": "812060"
  },
  {
    "text": "is equal to UCB of a2. This is- UCB stands for upper confidence bound is equal to 1,",
    "start": "812060",
    "end": "819850"
  },
  {
    "text": "plus square root 2 log 3 divided by 1. And UCB of a3 is equal to square root 2 log 3 divided by 1.",
    "start": "819850",
    "end": "832464"
  },
  {
    "text": "Okay. So why don't we just take a second, um, and define what would be the probability of selecting",
    "start": "832465",
    "end": "837970"
  },
  {
    "text": "each arm if you're using e-greedy with epsilon = 0.1. And what about if you're using UCB?",
    "start": "837970",
    "end": "843890"
  },
  {
    "text": "As always, feel free to talk to anybody nearby. [NOISE]",
    "start": "844530",
    "end": "886779"
  },
  {
    "text": "All right, so let's vote [NOISE]. I'm gonna ask you to vote if, um,",
    "start": "886780",
    "end": "892810"
  },
  {
    "text": "if two arms have, um, non-zero probability or three arms have non-zero probability.",
    "start": "892810",
    "end": "897850"
  },
  {
    "text": "So if you're using UCB [NOISE], do two arms have a non-zero probability?",
    "start": "897850",
    "end": "903910"
  },
  {
    "text": "Do three arms have a non-zero probability? Somebody who know- who thought with UCB,",
    "start": "903910",
    "end": "909400"
  },
  {
    "text": "you only have two arms with non-zero probability want to explain why? Yeah. Because since you're picking the maximum action you're only going to pick a1 or a2.",
    "start": "909400",
    "end": "920770"
  },
  {
    "text": "That's right, yeah. So, um, if we're picking the maximum action here, um, we're only gonna pick, um,",
    "start": "920770",
    "end": "926709"
  },
  {
    "text": "action a1 or a2, we have zero probability of my third arm. Okay. Let's do a quick vote for, um, e-greedy.",
    "start": "926710",
    "end": "933145"
  },
  {
    "text": "For e-greedy, do we have non-zero probability on two arms? On three arms? That's right.",
    "start": "933145",
    "end": "938860"
  },
  {
    "text": "What's the probability of selecting a3? [NOISE]. Yeah, [inaudible]",
    "start": "938860",
    "end": "944560"
  },
  {
    "text": "[NOISE] Um, 0.1 [NOISE].",
    "start": "944560",
    "end": "951400"
  },
  {
    "text": "Anyone else wanna add? Yeah? It's 0.03. Yes. Or 0323, yes, exactly.",
    "start": "951400",
    "end": "957820"
  },
  {
    "text": "So in the 0.1 in this case, just, um, remember we normally define that by uniformly splitting. Yeah. So we're gonna just have 0.1 divided by the number of arms.",
    "start": "957820",
    "end": "966355"
  },
  {
    "text": "Okay. So here, um, wh- why do I bring this up? So I bring this up to indicate that while UCB is still, um,",
    "start": "966355",
    "end": "973480"
  },
  {
    "text": "splitting its attention among all the arms that look good, it's not putting any weight right now on the arms that it doesn't think could be good,",
    "start": "973480",
    "end": "978925"
  },
  {
    "text": "which in this case is arm 3. Whereas an e-greedy, um, approach is gonna be uniform probability across anything it doesn't think it's best.",
    "start": "978925",
    "end": "986845"
  },
  {
    "text": "So this is one of the insights for why these arguments might be better, is they're being more strategic in how they're weighing, um,",
    "start": "986845",
    "end": "993430"
  },
  {
    "text": "what arms to pull, um, compared to what a- what an epsilon greedy, which is just doing uniform.",
    "start": "993430",
    "end": "999589"
  },
  {
    "text": "Okay. So let's look at, um, sort of, uh, what the regret would be in this case.",
    "start": "999750",
    "end": "1005010"
  },
  {
    "text": "So, um, the actions we pulled is a1, a2, a3, a1, a2. Um, so why would,",
    "start": "1005010",
    "end": "1012675"
  },
  {
    "text": "why would we pull a2 again, let's just go through that briefly. So let's say, um, so first, let's say we're, we're,",
    "start": "1012675",
    "end": "1018475"
  },
  {
    "text": "we're gonna pull a1. Let's imagine that's which one we picked. Okay, so we pulled a1. So let me just go through one more step of what",
    "start": "1018475",
    "end": "1024610"
  },
  {
    "text": "the upper confidence bounds would be in this case. So let's say we pull a1, okay.",
    "start": "1024610",
    "end": "1030280"
  },
  {
    "text": "So now, we need to redefine the upper confidence bounds, we actually need to redefine them for all the arms,",
    "start": "1030280",
    "end": "1035775"
  },
  {
    "text": "because if the denominator of that upper confidence bound, it depends on t, which is the total number of pulls",
    "start": "1035775",
    "end": "1042235"
  },
  {
    "text": "or the total number of pulls so far, if we're using that form. So now, we're gonna have that UCB.",
    "start": "1042235",
    "end": "1047305"
  },
  {
    "text": "Let's say we pulled action a1, and let's say I got a 1. Okay? So UCB of a1 is gonna have the same mean which is 1,",
    "start": "1047305",
    "end": "1057724"
  },
  {
    "text": "plus square root 2 log 4, because we've now pulled things four times, but now, we pulled this arm twice.",
    "start": "1057724",
    "end": "1063745"
  },
  {
    "text": "So it's- we're gonna divide this by 2. UCB of a2 is gonna have the same mean as before because we didn't pull it.",
    "start": "1063745",
    "end": "1071435"
  },
  {
    "text": "And then it's gonna also have the 2 log 4, but we've only pulled it once,",
    "start": "1071435",
    "end": "1077405"
  },
  {
    "text": "and UCB of a3 still has a mean of 0,",
    "start": "1077405",
    "end": "1083130"
  },
  {
    "text": "and it's also gonna have 2 log 4 divided by 1 [NOISE].",
    "start": "1083130",
    "end": "1088630"
  },
  {
    "text": "Okay. So in this case, what we're gonna find is that we sort of are gonna get this trading off, we still have the same empirical mean for a1 and a2.",
    "start": "1088630",
    "end": "1096400"
  },
  {
    "text": "But now, we haven't pulled a2 as much as a1, so we're gonna flip, and we're gonna pick a2 now.",
    "start": "1096400",
    "end": "1101470"
  },
  {
    "text": "So that's why if we imagine that we got, well actually, as a quick check your understanding, um,",
    "start": "1101470",
    "end": "1108400"
  },
  {
    "text": "ah, this result would happen whether we picked 1, whether we got a 1 or 0,",
    "start": "1108400",
    "end": "1113620"
  },
  {
    "text": "for a1 when we last pulled it. Um, so it's a good thing to check, is all I'm saying. But even if we got a 1 for a1,",
    "start": "1113620",
    "end": "1120039"
  },
  {
    "text": "we'd still select a2 on the next round, because the upper confidence bound of it would drop,",
    "start": "1120040",
    "end": "1125965"
  },
  {
    "text": "even if its mean was the same. So if we, if we look at this then, so we're gonna compare it to what would be",
    "start": "1125965",
    "end": "1132010"
  },
  {
    "text": "the optimal action if we take him out the whole time, which is a1. So what is our regret? Our regret is gonna be 0,",
    "start": "1132010",
    "end": "1138245"
  },
  {
    "text": "and then here it's gonna be 0.05, which is just equal to 0.95 - 0.9.",
    "start": "1138245",
    "end": "1143445"
  },
  {
    "text": "Here it's gonna be 0.85, because it's 0.95 - 0.1.",
    "start": "1143445",
    "end": "1149414"
  },
  {
    "text": "Here it's gonna be 0, and here it is going to be 0.05 again. So that's how we'd sum up regret, the regret.",
    "start": "1149415",
    "end": "1157080"
  },
  {
    "text": "Of course, we don't actually know this, and we can know this if we're doing this in the simulated world, but we can't knew- know this in reality because, again,",
    "start": "1157080",
    "end": "1162909"
  },
  {
    "text": "if we actually knew it in reality, then we wouldn't have to be doing this. You would know the optimal arm. May I have any other questions",
    "start": "1162910",
    "end": "1170890"
  },
  {
    "text": "about how UCB is working? Okay.",
    "start": "1170890",
    "end": "1176405"
  },
  {
    "text": "Now, just I guess, one other quick note here which is, you know, these upper confidence bounds are high probability bounds, so they can fail.",
    "start": "1176405",
    "end": "1183450"
  },
  {
    "text": "That is possible that sometimes the upper confidence bound is lower than, um, the true mean.",
    "start": "1183450",
    "end": "1188970"
  },
  {
    "text": "And so that's why when we did the proof last time we had to talk about sort of these different, what happens if the upper confidence bounds hold?",
    "start": "1188970",
    "end": "1195980"
  },
  {
    "text": "Um, uh, and there we did sort of a high probability bound.",
    "start": "1195980",
    "end": "1200130"
  },
  {
    "text": "Okay, so an alternative would be to always select the arm with the highest lower bound.",
    "start": "1201460",
    "end": "1207580"
  },
  {
    "text": "So what we're doing right now is just selecting the arm with the highest upper bound, but you could select the arm with the best lower bound.",
    "start": "1207580",
    "end": "1213585"
  },
  {
    "text": "Um, so what might that look like, let's imagine that we have two arms, a1 and a2, and this is our estimate of the Q of a.",
    "start": "1213585",
    "end": "1222730"
  },
  {
    "text": "Okay. So let's imagine that these are uncertainty bounds. So in this case a1 has a higher upper bound,",
    "start": "1228350",
    "end": "1235169"
  },
  {
    "text": "but a2 has a higher lower bound. So why don't we take a minute or two and think about,",
    "start": "1235170",
    "end": "1240990"
  },
  {
    "text": "why this could lead to linear regret?",
    "start": "1240990",
    "end": "1244150"
  },
  {
    "text": "I think two-arm case is the easiest one to think about for this. Feel free to talk to anybody around you, if you wanna brainstorm.",
    "start": "1246320",
    "end": "1255130"
  },
  {
    "text": "And this is actually an important reason why it's good to be optimistic, at least in reinforcement learning.",
    "start": "1261710",
    "end": "1268710"
  },
  {
    "text": "[NOISE]",
    "start": "1268710",
    "end": "1303929"
  },
  {
    "text": "What do you guys think? Does it lead to linear regret?",
    "start": "1303930",
    "end": "1317070"
  },
  {
    "text": "[NOISE] Right, so we're going to get, sort of like, confirmation bill, so like, a2 is giving out smaller and smaller [inaudible]. Okay, so I talked to at least one person in the audience that gave the right answer.",
    "start": "1317070",
    "end": "1325515"
  },
  {
    "text": "Which is, um, in this case if you select a2, you're gonna continue to get towards the mean.",
    "start": "1325515",
    "end": "1332670"
  },
  {
    "text": "The real mean is above the lower bound of a1. And so you're gonna,",
    "start": "1332670",
    "end": "1338880"
  },
  {
    "text": "kind of, get confirmation bias, like, a2 is gonna continue to look good, and you're never gonna select a1. So we're never gonna get information that allows us to disprove our,",
    "start": "1338880",
    "end": "1346620"
  },
  {
    "text": "our hypothesis, and we're never gonna learn what the true optimal mean is. So that's why we can get linear regret.",
    "start": "1346620",
    "end": "1352810"
  },
  {
    "text": "So one thing I just wanna highlight is that, um, upper confidence bounds are one nice way to do optimism, er,",
    "start": "1354350",
    "end": "1360510"
  },
  {
    "text": "and they can change over time in terms of upper bound, but the simpler thing you might imagine doing is just to initialize things really to a high value.",
    "start": "1360510",
    "end": "1367875"
  },
  {
    "text": "Um, so pretend, for example, that you already observed one pool of each of the arms and that it was really, really good.",
    "start": "1367875",
    "end": "1372945"
  },
  {
    "text": "So just initialize all your values at like a million or something like that. Um, and then you just kind of average in",
    "start": "1372945",
    "end": "1378870"
  },
  {
    "text": "that weird fake pool when you're doing your empirical average. So you can imagine you pretend you pull each of your arms once you've got a million,",
    "start": "1378870",
    "end": "1386550"
  },
  {
    "text": "you've got a trillion, and then after that you just average in all your empirical rewards.",
    "start": "1386550",
    "end": "1392340"
  },
  {
    "text": "So this actually can work fairly well in a lot of cases, the challenge is to figure out how optimistic you need to be for that fake pool.",
    "start": "1392340",
    "end": "1399420"
  },
  {
    "text": "So just in terms of sort of comparing that approach to other approaches, recall that greedy gives you linear total regret.",
    "start": "1399420",
    "end": "1405615"
  },
  {
    "text": "Constant e-greedy can also give you a linear total regret. If you decay e-greedy,",
    "start": "1405615",
    "end": "1410685"
  },
  {
    "text": "you can get actually sub-linear regret, um, if you use the right schedule for decay in epsilon,",
    "start": "1410685",
    "end": "1416490"
  },
  {
    "text": "but that generally requires knowledge of the gaps which are unknown. So this is sort of uh,",
    "start": "1416490",
    "end": "1424500"
  },
  {
    "text": "it's generally impossible to actually achieve. But if, in principle, you know, if you sort of, you know, had an oracle, you could,",
    "start": "1424500",
    "end": "1430515"
  },
  {
    "text": "uh, you could figure out how to decay things. If you're too optimistic in your initialization, um, if you initialize the value sufficiently optimistically,",
    "start": "1430515",
    "end": "1438210"
  },
  {
    "text": "then you can achieve sub-linear regret, but again, it can be pretty subtle to figure out exactly how optimistic those need to be.",
    "start": "1438210",
    "end": "1444645"
  },
  {
    "text": "I, and I'll talk later about an example where in that Markov decision process case they have to be much more optimistic than you might think they",
    "start": "1444645",
    "end": "1451380"
  },
  {
    "text": "would need to be in order for this to work well. All right, so we're gonna now start to talk about Bayesian bandits and Bayesian regret.",
    "start": "1451380",
    "end": "1461490"
  },
  {
    "text": "And so far we've sort of made very little assumptions about the reward distribution. We've assumed that the rewards might be bounded,",
    "start": "1461490",
    "end": "1468585"
  },
  {
    "text": "so we typically assume that the rewards are gonna lie in sort of 0,",
    "start": "1468585",
    "end": "1473745"
  },
  {
    "text": "0 to 1, or 0 to some, you know,",
    "start": "1473745",
    "end": "1478934"
  },
  {
    "text": "0 to R max or something like that, that we have bounded rewards that you can't have infinite rewards as nice as it would be.",
    "start": "1478935",
    "end": "1484125"
  },
  {
    "text": "Um, [NOISE] but we haven't been making strong parametric assumptions over the distribution of rewards.",
    "start": "1484125",
    "end": "1489180"
  },
  {
    "text": "[NOISE] Um, Hoeffding doesn't, uh, Hoeffding requires the rewards to be bounded but it doesn't assume, for example, that they're Gaussian or Bernoulli or things like that.",
    "start": "1489180",
    "end": "1497264"
  },
  {
    "text": "So an alternative approach is to assume that we actually do have information on the parametric distribution of the rewards, and it'll exploit that.",
    "start": "1497265",
    "end": "1504659"
  },
  {
    "text": "Um, so we're gonna talk about Bayesian bandits now, where we sort of explicitly compute a posterior over the rewards given the history.",
    "start": "1504660",
    "end": "1512505"
  },
  {
    "text": "So given the previous actions, we, the arms we've pulled and the rewards we've observed.",
    "start": "1512505",
    "end": "1517710"
  },
  {
    "text": "Uh, uh, and given that posterior we can use that posterior to guide exploration.",
    "start": "1517710",
    "end": "1522909"
  },
  {
    "text": "And of course, if your prior knowledge is accurate, that might help you. Um, there's sort of somewhat of a debate between Frequentist and Bayesian,",
    "start": "1523550",
    "end": "1532110"
  },
  {
    "text": "um, views of the world. We're not kind of get really too much into that in this class, but the idea is that it's also gonna be a nice way to put in prior knowledge.",
    "start": "1532110",
    "end": "1538950"
  },
  {
    "text": "If you have prior knowledge about the particular reward structure of your environment you can put those in and it can help in terms of exploration.",
    "start": "1538950",
    "end": "1545850"
  },
  {
    "text": "Okay, so in the Bayesian view now we're just gonna do sort of a quick review of Bayesian inference.",
    "start": "1545850",
    "end": "1552195"
  },
  {
    "text": "Um, a number of you guys it's probably gonna be a- a refresher for, for some people it might be new. We're gonna assume that we have a prior over the unknown parameters.",
    "start": "1552195",
    "end": "1559635"
  },
  {
    "text": "So in our case the unknown parameters are gonna be the parameters that determine the reward probability distribution for each of the arms.",
    "start": "1559635",
    "end": "1569010"
  },
  {
    "text": "And the idea is that given sort of observations or data about that parameter like observing rewards when you pull an arm,",
    "start": "1569010",
    "end": "1576135"
  },
  {
    "text": "we're gonna update our uncertainty over the unknown parameters using Bayes' rule. So let's look at that as a specific example.",
    "start": "1576135",
    "end": "1584070"
  },
  {
    "text": "So for example imagine that the reward of an arm i is a probability distribution that depends on some unknown parameter phi i.",
    "start": "1584070",
    "end": "1591960"
  },
  {
    "text": "So note that this is unknown. And we're gonna have some initial prior over phi i,",
    "start": "1591960",
    "end": "1599880"
  },
  {
    "text": "which is the probability of phi i. So this was before we pulled that arm at all. This is sort of our uncertainty over that parameter.",
    "start": "1599880",
    "end": "1606390"
  },
  {
    "text": "[NOISE] And then we pull arm i and we observe a particular reward, ri1. And then we can use this to update our estimate of the distribution over",
    "start": "1606390",
    "end": "1615659"
  },
  {
    "text": "the parameters that determine our reward probability distribution for this arm. So, we do that using Bayes' rule.",
    "start": "1615660",
    "end": "1623200"
  },
  {
    "text": "And we say that the posterior probability of our parameters phi i given that we've observed that reward is equal to",
    "start": "1624110",
    "end": "1631649"
  },
  {
    "text": "our prior probability over those times provide data evidence or likelihood,",
    "start": "1631650",
    "end": "1639940"
  },
  {
    "text": "divided by the probability of observing that reward regardless of what your parameters were.",
    "start": "1641390",
    "end": "1648070"
  },
  {
    "text": "And so this is Bayes' rule, um, and the challenge or the important thing here is how do we compute all of those things.",
    "start": "1650990",
    "end": "1659190"
  },
  {
    "text": "So, that tells us how to update our posterior over the parameters, and the question is how do we do this?",
    "start": "1659190",
    "end": "1664379"
  },
  {
    "text": "[NOISE] Okay. So, in general, doing this sort of updating can be very tricky to do.",
    "start": "1664380",
    "end": "1672575"
  },
  {
    "text": "Because if you don't have any structure on the sort of parametric form of the prior and the data likelihood.",
    "start": "1672575",
    "end": "1678050"
  },
  {
    "text": "So, this again is the prior, and this is the data likelihood.",
    "start": "1678050",
    "end": "1683429"
  },
  {
    "text": "If you have no structure on this, um, one of them is a deep neural network and another one of them is some random other, um,",
    "start": "1686870",
    "end": "1695325"
  },
  {
    "text": "parametric distribution, then it may be impossible to have a closed-form representation for what the posterior is.",
    "start": "1695325",
    "end": "1702210"
  },
  {
    "text": "So in general, this can be really hard. Um, [NOISE] but it turns out that there's particular forms of the prior and the data likelihood that mean that we can do this analytically.",
    "start": "1702210",
    "end": "1709830"
  },
  {
    "text": "[NOISE] So, who here is familiar with conjugates? Okay, some people but not everybody.",
    "start": "1709830",
    "end": "1717195"
  },
  {
    "text": "So, these are really cool conjugates. Um, exponential families, for example, are conjugate distributions.",
    "start": "1717195",
    "end": "1722850"
  },
  {
    "text": "Um, [NOISE] the idea is that if the parametric representation of the prior and the posterior is the same,",
    "start": "1722850",
    "end": "1728565"
  },
  {
    "text": "we call the prior and the model conjugate. So, what would that mean so, for example, what if this is like a Gaussian?",
    "start": "1728565",
    "end": "1735210"
  },
  {
    "text": "If this is a Gaussian and this is a Gaussian,",
    "start": "1735210",
    "end": "1739659"
  },
  {
    "text": "then we would say that this and this are conjugate. Whatever thing we're using for the data likelihood.",
    "start": "1740540",
    "end": "1747970"
  },
  {
    "text": "It essentially means that we can do this posterior update- updating analytically or in closed form, which is really nice.",
    "start": "1748310",
    "end": "1756345"
  },
  {
    "text": "So, we call it, this means that we sort of keep things in the same parametric family as we're getting more evidence about these hidden parameters.",
    "start": "1756345",
    "end": "1764400"
  },
  {
    "text": "[NOISE] I'll give an example of this in a second. Um, but there are a number of different parametric families which have conjugate priors.",
    "start": "1764400",
    "end": "1771990"
  },
  {
    "text": "Which means that, um, if you have an initial uncertainty over the parameter in that distribution,",
    "start": "1771990",
    "end": "1777165"
  },
  {
    "text": "then if you observe some data you can update it and you are still in the same parametric family. So, they're super elegant, and come up in statistics a lot.",
    "start": "1777165",
    "end": "1784830"
  },
  {
    "text": "Um, [NOISE] all right. So, here's a particular example that's relevant to us which is Bernoulli's. Um, so let's think about a bandit problem",
    "start": "1784830",
    "end": "1791549"
  },
  {
    "text": "where the reward of an arm is just a binary outcome. Um, and that this is sampled from a Bernoulli with parameter theta. So this comes up a lot.",
    "start": "1791550",
    "end": "1799350"
  },
  {
    "text": "This is things like advertisement click-through rates, patient treatment succeeds or fails et cetera.",
    "start": "1799350",
    "end": "1804705"
  },
  {
    "text": "So, many, many cases we- when we pull an arm, or when we take an action we're gonna get a binary reward, either 0 or 1.",
    "start": "1804705",
    "end": "1810960"
  },
  {
    "text": "[NOISE] So it turns out that, um, the beta distribution, beta alpha beta is conjugate for the Bernoulli distribution.",
    "start": "1810960",
    "end": "1819370"
  },
  {
    "text": "So that means we can write down our prior over the Bernoulli parameter,",
    "start": "1819590",
    "end": "1827625"
  },
  {
    "text": "um, given alpha and beta as follows. It's theta to the alpha - 1,",
    "start": "1827625",
    "end": "1833340"
  },
  {
    "text": "1 - theta to the beta - 1, times a ratio of the gammas.",
    "start": "1833340",
    "end": "1839170"
  },
  {
    "text": "Gammas are related to the factorial, factorial distribution. So, all of this can be computed analytically.",
    "start": "1839270",
    "end": "1846435"
  },
  {
    "text": "And one nice way I like to think about this is that we can think of alpha and beta as essentially being the result of prior pulls of the arm.",
    "start": "1846435",
    "end": "1854130"
  },
  {
    "text": "So, we can use them also to encode sort of prior information about this. And I'll show you shor- shortly an example of how these get updated.",
    "start": "1854130",
    "end": "1861270"
  },
  {
    "text": "[NOISE] But what happens is that if you assume that the prior over Theta is a Beta.",
    "start": "1861270",
    "end": "1867045"
  },
  {
    "text": "Um, so, if it looks like this, this is the prior. Then if you observe our reward that's in 0, 1,",
    "start": "1867045",
    "end": "1874169"
  },
  {
    "text": "because that's what our reward, distribute rewards are whenever we sample one, then the updated posterior over theta is a really nice form.",
    "start": "1874170",
    "end": "1880370"
  },
  {
    "text": "It's just the same beta distribution with either 1 added to the alpha or 1 added to the beta.",
    "start": "1880370",
    "end": "1886625"
  },
  {
    "text": "So, essentially if you observe r = 1 then you get beta of alpha + 1 and beta.",
    "start": "1886625",
    "end": "1897120"
  },
  {
    "text": "If you observe r = 0 you instead add it to the Beta term.",
    "start": "1897120",
    "end": "1902910"
  },
  {
    "text": "So, you can think of alpha as being all the number of times that you saw a reward of 1,",
    "start": "1902910",
    "end": "1908070"
  },
  {
    "text": "and beta as all the number of times you saw a reward of 0. Can you explaining how the fact that theta was Bernoulli factored into",
    "start": "1908070",
    "end": "1915660"
  },
  {
    "text": "this description and why this isn't just a description of a beta distribution, the main equation?",
    "start": "1915660",
    "end": "1920670"
  },
  {
    "text": "[NOISE] Like why- why it is important to first say that theta is Bernoulli? I'm bringing up that, uh, great question.",
    "start": "1920670",
    "end": "1927240"
  },
  {
    "text": "So, the reason I bring this up is that beta is a conjugate prior for the Bernoulli.",
    "start": "1927240",
    "end": "1932595"
  },
  {
    "text": "So, the idea is that in this case if we're thinking about an arm which has binary outcomes,",
    "start": "1932595",
    "end": "1939164"
  },
  {
    "text": "then we can think of the average of that arm as being represented by a Bernoulli parameter. So, let's say like 0.7%, you know,",
    "start": "1939165",
    "end": "1946335"
  },
  {
    "text": "on average 0.7 times, we get a reward of 1, and 0.3 we get a reward of 0.",
    "start": "1946335",
    "end": "1952545"
  },
  {
    "text": "So, [NOISE], um, the mean of that arm is 0.7. Okay. So, we're thinking about an arm",
    "start": "1952545",
    "end": "1959730"
  },
  {
    "text": "which has a Bernoulli parameter that describes its mean. So, we're thinking of like you know an arm with mean equal to theta.",
    "start": "1959730",
    "end": "1970860"
  },
  {
    "text": "So, that's what the mean is for a Bernoulli distribution. Um, and what I'm saying is that we want to now be Bayesian about that,",
    "start": "1970860",
    "end": "1978060"
  },
  {
    "text": "and we want to think about what is the probability of that parameter given the data we've seen so far?",
    "start": "1978060",
    "end": "1984375"
  },
  {
    "text": "And if we wanna be able to update our estimate over theta, not over rewards, over theta,",
    "start": "1984375",
    "end": "1991590"
  },
  {
    "text": "then we're gonna write down our distribution over what theta's might be possible,",
    "start": "1991590",
    "end": "1997018"
  },
  {
    "text": "um, [NOISE] as a beta distribution. And we're going to update that as we see evidence. And I'll show you shortly like what these betas look like, so we can think of so,",
    "start": "1997019",
    "end": "2005600"
  },
  {
    "text": "what is the probability distribution over thetas as we get more evidence. So, for example, you might imagine if you see a reward which is 1, 1, 1, 1, 1, 1, 1,",
    "start": "2005600",
    "end": "2014225"
  },
  {
    "text": "um, then your beta distribution is going to indicate that a theta which is really high is more likely.",
    "start": "2014225",
    "end": "2019865"
  },
  {
    "text": "If you get 0, 0, 0, 0, 0, your beta is gonna have a different shifted posterior,",
    "start": "2019865",
    "end": "2025895"
  },
  {
    "text": "which is gonna say probably your theta's really low, close to 0.",
    "start": "2025895",
    "end": "2030450"
  },
  {
    "text": "Cool. So, we'll, we'll see an example of this in just a second. Um, so, the nice thing in this case is that for",
    "start": "2031090",
    "end": "2037159"
  },
  {
    "text": "Bernoulli's which is a really common distribution that we often want to think about, we can write down, um,",
    "start": "2037160",
    "end": "2042215"
  },
  {
    "text": "a prior over that parameter and we can update it analytically just using the counts. So, we just keep track of how many times we've seen",
    "start": "2042215",
    "end": "2048770"
  },
  {
    "text": "a 1 or how many times we have a 0, and we use that to update our posterior.",
    "start": "2048770",
    "end": "2053190"
  },
  {
    "text": "Okay so how do we evaluate performance now we're in this Bayesian setting? So in the frequentist regret,",
    "start": "2055090",
    "end": "2061760"
  },
  {
    "text": "we didn't think about having distributions over parameters. We just thought of there being some parameter like,",
    "start": "2061760",
    "end": "2066875"
  },
  {
    "text": "you know what's the mean of that arm. Um, and then we defined our regret with respect to the best arm.",
    "start": "2066875",
    "end": "2072935"
  },
  {
    "text": "Bayesian regret assumes there's this prior over parameters. And so Bayesian regret says,",
    "start": "2072935",
    "end": "2078125"
  },
  {
    "text": "what is my expected regret by thinking about what are the possible parameters given my prior.",
    "start": "2078125",
    "end": "2083194"
  },
  {
    "text": "Um, and then looking at the expected performance if I got a particular theta.",
    "start": "2083195",
    "end": "2088220"
  },
  {
    "text": "So it's a little bit different way of looking at the world. Um, again, we're not gonna really get into the philosophical aspects of this.",
    "start": "2088220",
    "end": "2094369"
  },
  {
    "text": "But Ba- Bayesian regret is saying like well, we're not sure, you know what the distributions are of these arms.",
    "start": "2094370",
    "end": "2099905"
  },
  {
    "text": "Um, and there'll be different worlds in which they'll take on different values and how well do you do in those different worlds on average. All right.",
    "start": "2099905",
    "end": "2109430"
  },
  {
    "text": "So how do we try to make good decisions for Bayesian bandits? So one thing you might imagine is let's say we have",
    "start": "2109430",
    "end": "2116105"
  },
  {
    "text": "a parametric distribution over the rewards for each of the arms. Um, we, we could have,",
    "start": "2116105",
    "end": "2121460"
  },
  {
    "text": "we could certainly have that in the Bayesian case. The idea of probability matching which I think has been around since around 1929.",
    "start": "2121460",
    "end": "2129725"
  },
  {
    "text": "Its been around a long time, like almost 100 years. Um, ah, is that we wanna select an action a according to the probability that it's optimal.",
    "start": "2129725",
    "end": "2138930"
  },
  {
    "text": "So it seems quite intuitively appealing like we want to select arms that might be optimal more.",
    "start": "2139030",
    "end": "2144454"
  },
  {
    "text": "Um, we want to select arms that probably aren't likely to be optimal less. And it is optimistic in the face of uncertainty because [NOISE] in",
    "start": "2144455",
    "end": "2152510"
  },
  {
    "text": "general uncertain actions have a higher probability of being the best. So uncertain actions mean we don't know very much about what their rewards are.",
    "start": "2152510",
    "end": "2161089"
  },
  {
    "text": "Um, the problem is this sounds really nice, we'd like to sort of select arms according to the probability that they're",
    "start": "2161090",
    "end": "2167270"
  },
  {
    "text": "optimal but it's completely unclear how to compute that. So this expression here is saying we wanna sample an arm given a history.",
    "start": "2167270",
    "end": "2176599"
  },
  {
    "text": "So the history here, here is prior pulls and reward outcomes.",
    "start": "2176600",
    "end": "2182735"
  },
  {
    "text": "So this is the history of the arms we pulled and whether we've got what sort of rewards we've gotten.",
    "start": "2182735",
    "end": "2188330"
  },
  {
    "text": "And then we wanna pull an arm according to the probability that that arm is better than all the other arms given that history.",
    "start": "2188330",
    "end": "2196190"
  },
  {
    "text": "So that's quite intellectually, um, appealing but it's not at all clear how we would compute that quantity.",
    "start": "2196190",
    "end": "2203194"
  },
  {
    "text": "And so it's sort of somewhat magical that, um, a very simple approach turns out to implement probability matching.",
    "start": "2203195",
    "end": "2210905"
  },
  {
    "text": "And the idea is called Thompson sampling, and again this came out, you know, roughly in the 1920s.",
    "start": "2210905",
    "end": "2216335"
  },
  {
    "text": "And one of the really interesting aspects of that is it sort of disappeared for a long time in terms of bandits.",
    "start": "2216335",
    "end": "2222050"
  },
  {
    "text": "Certainly in the AI community and CS community. And then around eight years ago, eight to nine years ago,",
    "start": "2222050",
    "end": "2227705"
  },
  {
    "text": "people sort of got re-interested in understanding these, um, in part due to a paper that [NOISE] a colleague of mine published which you'll see",
    "start": "2227705",
    "end": "2235070"
  },
  {
    "text": "results have shortly which indicated that empirically it can be really good. Okay. So how does Thompson sampling work?",
    "start": "2235070",
    "end": "2242255"
  },
  {
    "text": "We're gonna initialize a prior over each of the arms. Often we'd like this to be conjugate, doesn't have to be.",
    "start": "2242255",
    "end": "2248015"
  },
  {
    "text": "It's nice if it's conjugate. But we gonna have a probability over each of the arms. Um, now remember that this is",
    "start": "2248015",
    "end": "2256730"
  },
  {
    "text": "sort of a probability over the parameters determining the distribution. So, um, it could be if we have Bernoulli arms,",
    "start": "2256730",
    "end": "2265535"
  },
  {
    "text": "it could be the probability of theta. I for i equals 1 to the number of arms.",
    "start": "2265535",
    "end": "2274370"
  },
  {
    "text": "So, for example, this could be a beta distribution of 1, 1.",
    "start": "2274370",
    "end": "2279410"
  },
  {
    "text": "We could say the probability that my ith arm has a Bernoulli parameter of theta is equal to,",
    "start": "2279410",
    "end": "2289940"
  },
  {
    "text": "um, uh, sampling from a beta 1, 1. Okay. So we're gonna pick a particular parametric family",
    "start": "2289940",
    "end": "2295175"
  },
  {
    "text": "to represent our prior distribution over the, um, reward distributions for each of the arms.",
    "start": "2295175",
    "end": "2301130"
  },
  {
    "text": "And then what we do is for each round, we first sample a rewards distribution from that posterior.",
    "start": "2301130",
    "end": "2308180"
  },
  {
    "text": "Again we'll go through a concrete example of this in a second. But this is like picking a particular theta.",
    "start": "2308180",
    "end": "2314360"
  },
  {
    "text": "So it's like saying I'm assuming that my mean for this arm e- or my Bernoulli parameter for this arm is 0.7,",
    "start": "2314360",
    "end": "2320585"
  },
  {
    "text": "picking a particular value for that parameter. And then once you have that you can compute the action value function by just taking the mean of whatever that is.",
    "start": "2320585",
    "end": "2329765"
  },
  {
    "text": "So notice that if theta, let's say we sample for arm 1,",
    "start": "2329765",
    "end": "2335660"
  },
  {
    "text": "let's say we sample 0.9. We just happen to sample that arm 1 has,",
    "start": "2335660",
    "end": "2343820"
  },
  {
    "text": "um, uh, a theta parameter 0.9. Well, the Q for arm 1 is then",
    "start": "2343820",
    "end": "2350839"
  },
  {
    "text": "gonna be the expected value of a Bernoulli parameter theta one which is just 0.9.",
    "start": "2350840",
    "end": "2356450"
  },
  {
    "text": "Because the expected value for a Bernoulli variable is just p,",
    "start": "2356450",
    "end": "2362974"
  },
  {
    "text": "just the, just the theta. So we're gonna compute the action value function for each of these.",
    "start": "2362975",
    "end": "2368225"
  },
  {
    "text": "Um, in the case of a Gaussian, it would just be its mean, for example. So you just compute what the mean expected reward is for each of the arms under",
    "start": "2368225",
    "end": "2376280"
  },
  {
    "text": "the particular reward distribution we sampled and then you take whichever action looks best for those sampled parameters.",
    "start": "2376280",
    "end": "2383175"
  },
  {
    "text": "So that's this. And then we take that action and we observe a reward and then we update our posterior using Bayes' Law.",
    "start": "2383175",
    "end": "2391970"
  },
  {
    "text": "So we're just gonna take our priors over the parameters. We're gonna sample a particular set of parameters.",
    "start": "2391970",
    "end": "2398105"
  },
  {
    "text": "These are probably totally wrong. This is just us making up what the actual, you know, parameters are for each of the arms.",
    "start": "2398105",
    "end": "2404045"
  },
  {
    "text": "Then we act as if that world is optimal, we get some data, and we repeat.",
    "start": "2404045",
    "end": "2408780"
  },
  {
    "text": "So it's a, it's a fairly simple thing to do. Um, we of course have to see how we can do this sampling.",
    "start": "2410890",
    "end": "2416540"
  },
  {
    "text": "Um, and I'll show you an example with Bernoulli's in a second. Um, but nowhere here are we trying to explicitly",
    "start": "2416540",
    "end": "2423230"
  },
  {
    "text": "compute like what is the posterior probability that this arm is optimal. We're just sampling some and then we're going to",
    "start": "2423230",
    "end": "2429934"
  },
  {
    "text": "be sort of greedy with respect to those samples. Okay. So Thompson sampling",
    "start": "2429935",
    "end": "2437270"
  },
  {
    "text": "turns out to implement probability matching which is super cool. Um, and for some of the intuition of this,",
    "start": "2437270",
    "end": "2442280"
  },
  {
    "text": "so this is what probability matching is. Probability matching is that we wanna select an action given the,",
    "start": "2442280",
    "end": "2449480"
  },
  {
    "text": "um, history according to the probability that it's optimal. According to the probability that arm really is the best arm.",
    "start": "2449480",
    "end": "2456140"
  },
  {
    "text": "And we can think of that as being equivalent to the expected value of picking a reward given the H and that,",
    "start": "2456140",
    "end": "2462770"
  },
  {
    "text": "ah, the arm is equal to the arg max of QA given that history. And that's what Thompson sampling is computing.",
    "start": "2462770",
    "end": "2470640"
  },
  {
    "text": "Okay, so let's see how this actually looks for the broken toe example, because I think that'll make it a lot more concrete.",
    "start": "2472060",
    "end": "2477515"
  },
  {
    "text": "So again, in this case, remember that we have, um, three different arms and they each are looking at the success or failure of,",
    "start": "2477515",
    "end": "2486050"
  },
  {
    "text": "um, doing this treatment to try to make people's broken toes better. Um, and surgery is a Bernoulli parameter with 0.95.",
    "start": "2486050",
    "end": "2492965"
  },
  {
    "text": "Taping is, uh, is, uh, 0.9 and nothing is 0.1. So if we wanted to then run Thompson sampling in this environment,",
    "start": "2492965",
    "end": "2501365"
  },
  {
    "text": "remember it doesn't know what those actual parameters are. We're gonna choose a beta 1, 1 prior over the parameters.",
    "start": "2501365",
    "end": "2508234"
  },
  {
    "text": "So that means that we're gonna say the probability of theta 1 is equal to a beta.",
    "start": "2508235",
    "end": "2514320"
  },
  {
    "text": "Okay. So what does a beta 1, 1 look like? It looks like a uniform distribution.",
    "start": "2515000",
    "end": "2520740"
  },
  {
    "text": "So this is 0 to 1. This is theta. This is probability of theta.",
    "start": "2520740",
    "end": "2527920"
  },
  {
    "text": "So what this says is that if someone gives you a beta 1, 1 distribution, it says you're going to select a Bernoulli parameter from that.",
    "start": "2527930",
    "end": "2536265"
  },
  {
    "text": "I have no idea what its value is. Could be 0, it could be 1, it could be 0.5. It- it's sort of an uninformative prior.",
    "start": "2536265",
    "end": "2543000"
  },
  {
    "text": "Okay. So it says that initially I have no idea what, um, these theta parameters might be for each of the arms.",
    "start": "2543000",
    "end": "2549915"
  },
  {
    "text": "Um, so I'm just gonna pretend it's- I'm gonna start off and assume it's flat. I have no information.",
    "start": "2549915",
    "end": "2555420"
  },
  {
    "text": "Okay. So this is what, um, a beta 1, 1 looks like.",
    "start": "2555420",
    "end": "2561150"
  },
  {
    "text": "But what's gonna happen in Thompson sampling? So this is our distribution over thetas.",
    "start": "2561150",
    "end": "2567510"
  },
  {
    "text": "And what Thompson sampling is gonna do is it's gonna sample a parameter from that distribution. So in this case it's just a uniform distribution between 0 and 1.",
    "start": "2567510",
    "end": "2576270"
  },
  {
    "text": "So we're just gonna select some value between 0 and 1.",
    "start": "2576270",
    "end": "2580630"
  },
  {
    "text": "So in this case imagine what we got is, I'll just leave this up for a second so people can see 0.3, 0.5. and 0.6.",
    "start": "2581690",
    "end": "2592470"
  },
  {
    "text": "There's no reason that arm three would be higher or lower than arm one or arm two or arm three.",
    "start": "2592470",
    "end": "2598080"
  },
  {
    "text": "In reality arm one is best, but we have no information about that so far. We have no rewards so far.",
    "start": "2598080",
    "end": "2603660"
  },
  {
    "text": "All we've done is we've just said, I have a uniform distribution over what my theta parameter might be, I'm gonna sample from it.",
    "start": "2603660",
    "end": "2610994"
  },
  {
    "text": "And so in this case, it's like sampling and you've got this value once, you got this value once,",
    "start": "2610995",
    "end": "2616994"
  },
  {
    "text": "and you got this value once. And we just sampled from that distribute-",
    "start": "2616995",
    "end": "2622950"
  },
  {
    "text": "that uniform distribution and these are the parameters we pegged. And now we're going to pretend that's real. So we're gonna say I'm gonna pretend that my theta for surgery is 0.3.",
    "start": "2622950",
    "end": "2632865"
  },
  {
    "text": "My theta for taping is 0.5 and my theta for nothing is 0.6.",
    "start": "2632865",
    "end": "2637600"
  },
  {
    "text": "So if that was the real world we lived in, what arm would we select?",
    "start": "2638420",
    "end": "2644230"
  },
  {
    "text": "Third arm. Third arm. Exactly. So in this world, the third arm really is best because that's a theta of 0.6.",
    "start": "2645350",
    "end": "2653490"
  },
  {
    "text": "So we're going to select theta as 0.6. Yeah. Using uninformative priors because I could have many values of beta 2, 2 or 5, 5.",
    "start": "2653490",
    "end": "2666080"
  },
  {
    "text": "So is that a significant advantage of using this for Thompson or something?",
    "start": "2666080",
    "end": "2672180"
  },
  {
    "text": "Yeah. It makes a really good point. She said we currently use an uninformative prior,",
    "start": "2672180",
    "end": "2678630"
  },
  {
    "text": "is it better or worse to do like that compared to using an uninformative prior. So you could have had a beta of like 3, 4 et cetera.",
    "start": "2678630",
    "end": "2684765"
  },
  {
    "text": "Um, a beta of 3, 4 or anything that's not 1, 1 is gonna give you, um, is gonna bias your distribution,",
    "start": "2684765",
    "end": "2691349"
  },
  {
    "text": "it's gonna change the shape of how you sample things. If you have actually good information that can be really useful,",
    "start": "2691350",
    "end": "2697589"
  },
  {
    "text": "um, because it's essentially like having fake pools. Um, and it can- it can guide sort of your initial samples.",
    "start": "2697590",
    "end": "2703710"
  },
  {
    "text": "The downside is that if that isn't correct, you can be misled for a while. So we often talk about like how robust are we",
    "start": "2703710",
    "end": "2710819"
  },
  {
    "text": "to misspecified priors or to wrong priors. Um, and so using the uninformative prior means that,",
    "start": "2710820",
    "end": "2716175"
  },
  {
    "text": "ah, you're not getting a lot of benefit from prior knowledge but you're also not gonna get a disadvantage.",
    "start": "2716175",
    "end": "2721810"
  },
  {
    "text": "Okay. So in this case we're gonna select the arm- arm three because that's just the arm that has the best expected mean,",
    "start": "2722900",
    "end": "2731100"
  },
  {
    "text": "um, under the samples that we did. Okay, but arm three is actually not very good. And we know that because arm three actually only has a reward of 0.1.",
    "start": "2731100",
    "end": "2739020"
  },
  {
    "text": "And so when we sample it and we get the patient's outcome, we're gonna get a 0 in this particular case. Because the real arm three is 0.1.",
    "start": "2739020",
    "end": "2747465"
  },
  {
    "text": "So if we sample from a Bernoulli with 0.1, most of the time we're going to get a 0. So now we have to do is we have to update our posterior over arm three.",
    "start": "2747465",
    "end": "2757650"
  },
  {
    "text": "Okay, we have to update what sort of what are our probability of theta of arm three is, given that the reward was equal to 0.",
    "start": "2757650",
    "end": "2766750"
  },
  {
    "text": "Okay. [NOISE] So what we talked about is that the beta is a conjugate prior for the Bernoulli.",
    "start": "2768230",
    "end": "2773370"
  },
  {
    "text": "And if we observe a 1, we're going to update the first parameter, also we're gonna update the second parameter, so we just saw a 0.",
    "start": "2773370",
    "end": "2779910"
  },
  {
    "text": "So our new beta is 1, 2. Because we just saw a 0 and so we update.",
    "start": "2779910",
    "end": "2785280"
  },
  {
    "text": "So this is our new parameter. That's our new posterior over arm- the arm three and it looks like this.",
    "start": "2785280",
    "end": "2794715"
  },
  {
    "text": "Okay. So this is still theta, always has to be between 0 and 1 because this is a Bernoulli parameter.",
    "start": "2794715",
    "end": "2801615"
  },
  {
    "text": "And this is what the probability looks like now. So notice it shifted,",
    "start": "2801615",
    "end": "2807463"
  },
  {
    "text": "so it used to be flat, and now it says well no, I just observed that we've got a reward of 0.",
    "start": "2807464",
    "end": "2813285"
  },
  {
    "text": "So now I have a higher probability that theta is small. So if I was going to sample from this,",
    "start": "2813285",
    "end": "2818670"
  },
  {
    "text": "it is more likely I would get a lower value compared to a higher value unlike before.",
    "start": "2818670",
    "end": "2823859"
  },
  {
    "text": "Okay. So this is our new posterior. And what does the posterior look like for the other arm?",
    "start": "2823860",
    "end": "2830130"
  },
  {
    "text": "So this is for, um, this is for theta three. And for the other two,",
    "start": "2830130",
    "end": "2836980"
  },
  {
    "text": "they still look uniform because they're still a beta 1, 1. So for beta 1, 1.",
    "start": "2837530",
    "end": "2844060"
  },
  {
    "text": "This is for the other arms, theta 1, theta 2, and this is probability of theta.",
    "start": "2845810",
    "end": "2851445"
  },
  {
    "text": "The other two are still uniform because we- we haven't pulled them yet. We don't have any outcomes. So they still look like uniform distributions.",
    "start": "2851445",
    "end": "2857745"
  },
  {
    "text": "But the probability over theta 3 looks skewed towards 0. So now in Thompson sampling,",
    "start": "2857745",
    "end": "2863070"
  },
  {
    "text": "we again are just going to sample a value from each of these different ones. Each of those three distributions.",
    "start": "2863070",
    "end": "2868845"
  },
  {
    "text": "And now imagine we get 0.7, 0.5 and 0.3, yeah?",
    "start": "2868845",
    "end": "2872859"
  },
  {
    "text": "Turn back a slide, should that say p of Q a3, not Q of a1 because didn't you say a3- Thank you. Yeah, hold on, there's a couple of errors there, yeah.",
    "start": "2874070",
    "end": "2885550"
  },
  {
    "text": "Thanks for catching that. Any other questions? Okay. So we updated our posterior over arm three.",
    "start": "2889370",
    "end": "2897285"
  },
  {
    "text": "Our posterior over arm one and arm two is the same as the prior because we didn't- we didn't pull them.",
    "start": "2897285",
    "end": "2902655"
  },
  {
    "text": "Okay. So now we're gonna sample from those three distributions. What Thompson sampling would say is now given our posterior over all of the arms,",
    "start": "2902655",
    "end": "2910380"
  },
  {
    "text": "let's select an actual parameter for each of the arm. And this time we're going to get 0.7, 0.5 and 0.3.",
    "start": "2910380",
    "end": "2916560"
  },
  {
    "text": "So which arm where are we going to select this time? Arm one.",
    "start": "2916560",
    "end": "2922470"
  },
  {
    "text": "Arm one. Right? So now the max is gonna be arm one.",
    "start": "2922470",
    "end": "2927525"
  },
  {
    "text": "Okay. So now we're gonna have a posterior that looks like beta 2,1 because we update our pe- beta.",
    "start": "2927525",
    "end": "2936914"
  },
  {
    "text": "And remember we can just think of this as being the number of R equals 1s + 1 because we started with a beta",
    "start": "2936915",
    "end": "2945630"
  },
  {
    "text": "1, 1 and this is the number of r = 0s plus 1.",
    "start": "2945630",
    "end": "2950819"
  },
  {
    "text": "So now our new posterior for this one makes it look like this. So this is theta 1,",
    "start": "2950820",
    "end": "2956640"
  },
  {
    "text": "this is 1, 0 probability of theta 1. Okay. So now as we would expect,",
    "start": "2956640",
    "end": "2962730"
  },
  {
    "text": "we saw that arm- arm one had a good outcome and so now our probability that that Bernoulli parameter is higher than 0.5 is going up because we saw some positive results.",
    "start": "2962730",
    "end": "2973020"
  },
  {
    "text": "Okay. So now we have a- so what does our new distributions look like? We have a beta 2, 1,",
    "start": "2973020",
    "end": "2980670"
  },
  {
    "text": "we have a beta 1, 1 because we haven't selected arm two yet, and then we have a beta 1, 2.",
    "start": "2980670",
    "end": "2987460"
  },
  {
    "text": "All right. So what's going to happen next, um, let me again are gonna sample a Bernoulli parameter.",
    "start": "2987710",
    "end": "2993960"
  },
  {
    "text": "So let's imagine that we got 0.71, 0.65 and 0.1. And so that means we're again gonna select arm one.",
    "start": "2993960",
    "end": "3002375"
  },
  {
    "text": "And we again observe a one, surgery is pretty effective. And now our posterior is 3, 1.",
    "start": "3002375",
    "end": "3009440"
  },
  {
    "text": "So now this again is 0 to 1. This is our probability of theta 1.",
    "start": "3009440",
    "end": "3015395"
  },
  {
    "text": "Okay. So now it's looking even more peaked. So what's your guess of what's the next arm we're likely to sample?",
    "start": "3015395",
    "end": "3023690"
  },
  {
    "text": "So remember the three distributions that we have right now is for arm two,",
    "start": "3023690",
    "end": "3029060"
  },
  {
    "text": "looks like this, for arm three, looks like this. So this is the probability of that arm.",
    "start": "3029060",
    "end": "3037290"
  },
  {
    "text": "Since theta a2, theta a3, 0, 1, 0, 1.",
    "start": "3039940",
    "end": "3045680"
  },
  {
    "text": "So who thinks that, um, theta 1 is again gonna be sampled and look better than everything else?",
    "start": "3045680",
    "end": "3052130"
  },
  {
    "text": "That's right because it's going to have- has a posterior over its Bernoulli parameter that is getting closer and more and more steep towards 1.",
    "start": "3052130",
    "end": "3060305"
  },
  {
    "text": "Theta 2 we still never- we've still never taken action a2, but it just has a uniform probability.",
    "start": "3060305",
    "end": "3065615"
  },
  {
    "text": "So it's very unlikely that we're gonna sample a value for it that is better than the value we sampled for arm one.",
    "start": "3065615",
    "end": "3071870"
  },
  {
    "text": "So again in this case, we can imagine sampling again, we get 0.75, 0.45, 0.4.",
    "start": "3071870",
    "end": "3077525"
  },
  {
    "text": "We select action a1 again and now we have a beta 4,1 and it's looking even more sharp up.",
    "start": "3077525",
    "end": "3083160"
  },
  {
    "text": "So notice this is quite different than what UCB was doing. UCB was splitting its time between a1 and 2 at the beginning because,",
    "start": "3084300",
    "end": "3093040"
  },
  {
    "text": "um, they were both reliant on their empirical means, um, but then a2 had been taken less times.",
    "start": "3093040",
    "end": "3098560"
  },
  {
    "text": "In this case, we still haven't taken action a2 yet. And it may be hard for us to pull it for a while.",
    "start": "3098560",
    "end": "3104710"
  },
  {
    "text": "Now, that's not actually bad in this case because theta 1 is actually the best arm. But there can be sor- some trade-offs. Yes.",
    "start": "3104710",
    "end": "3111700"
  },
  {
    "text": "Is this the only way we can update the Beta distributions? Uh, is, is there a rule that we should increment it",
    "start": "3111700",
    "end": "3119290"
  },
  {
    "text": "it by one or [NOISE] of course we have different kinds of rewards. Here rewards are 0 and 1, right?",
    "start": "3119290",
    "end": "3124345"
  },
  {
    "text": "So do you have some kinds of rewards probably of beta, beta distribution. Great question. Category is like, okay,",
    "start": "3124345",
    "end": "3130360"
  },
  {
    "text": "so here we've got, um, binary rewards. How would we do this if the things were not binary? In that case we wouldn't use a beta in a Bernoulli.",
    "start": "3130360",
    "end": "3137305"
  },
  {
    "text": "So if you didn't have, uh, for binary rewards, Bernoulli's a really nice choice and betas conjugate. If you have real-valued rewards you might use a Gaussian.",
    "start": "3137305",
    "end": "3144925"
  },
  {
    "text": "Um, and then you'd have a, a, a sort of Gaussian prior depending on whether you know your theta or not. And in general, uh, like for multinomials you can use Dirichlet distributions.",
    "start": "3144925",
    "end": "3153580"
  },
  {
    "text": "Depends on what your reward distribution looks like and then you wanna find a conjugate prior for that distribution.",
    "start": "3153580",
    "end": "3158650"
  },
  {
    "text": "So there's a lot of different families of parametric distributions for which you can do this sort of updating. Yeah.",
    "start": "3158650",
    "end": "3165130"
  },
  {
    "text": "Then the other things we're talking about. Remind me your name. About being optimistic.",
    "start": "3165130",
    "end": "3170560"
  },
  {
    "text": "Yes. And here is like a uniform distribution for initialization. Is it better to use something more optimistic?",
    "start": "3170560",
    "end": "3177010"
  },
  {
    "text": "That is a great question. His question was, uh, so we talked before about the benefits of optimism.",
    "start": "3177010",
    "end": "3182380"
  },
  {
    "text": "Here we just used a uniform prior. Um, and wouldn't be better to use one that is optimistic.",
    "start": "3182380",
    "end": "3187825"
  },
  {
    "text": "It depends, um, I, the empirically what, so what is this doing?",
    "start": "3187825",
    "end": "3193224"
  },
  {
    "text": "I- I'm just gonna, let me hold on that question for a second. So we can look at sort of what these look like. So if we did optimism,",
    "start": "3193225",
    "end": "3199510"
  },
  {
    "text": "we sampled all the actions first and then we sort of got this interleaving of a1 or, and a2. In Thompson sampling, we took a3 and we took a1,",
    "start": "3199510",
    "end": "3208330"
  },
  {
    "text": "a1, a1, a1, and a1, a1, a1, a1. a1 is optimal in this case.",
    "start": "3208330",
    "end": "3214060"
  },
  {
    "text": "So by using a uniform prior here, essentially, um, as soon as you see something that looks pretty good like better than 0.5,",
    "start": "3214060",
    "end": "3221005"
  },
  {
    "text": "um, you're gonna tend to often sample it a lot more. Um, so you're sort of exploiting faster to some extent.",
    "start": "3221005",
    "end": "3226715"
  },
  {
    "text": "Um, you can put priors in there. The question is often like, how much to put that in there and if it actually helps.",
    "start": "3226715",
    "end": "3233950"
  },
  {
    "text": "So one of the cool things with Thompson sampling is it turns out in terms of sort of the Bayesian regret bounds, [NOISE] they're as good as the, as the upper confidence bounds,",
    "start": "3233950",
    "end": "3240849"
  },
  {
    "text": "but empirically often exploring faster is helpful. So you could put optimism in there,",
    "start": "3240850",
    "end": "3246190"
  },
  {
    "text": "but it might actually hurt performance because it's gonna force you to take, like in this case, r1 actually is optimal.",
    "start": "3246190",
    "end": "3251605"
  },
  {
    "text": "Um, now, you could have imagined maybe we were just lucky there and instead we got an a2. In that case, you'd want something to help you eventually take a1.",
    "start": "3251605",
    "end": "3259180"
  },
  {
    "text": "Now note, we, we will still take a2 likely at some point",
    "start": "3259180",
    "end": "3264325"
  },
  {
    "text": "because there still will be a probability under that uniform prior that you'll sample like 0.999 and then you'll take a2.",
    "start": "3264325",
    "end": "3270160"
  },
  {
    "text": "So you can still be sure to start taking other actions even using these uniform priors.",
    "start": "3270160",
    "end": "3276430"
  },
  {
    "text": "But it's a really good question about sort of, you know, weird, what information to put in there. Yeah.",
    "start": "3276430",
    "end": "3281530"
  },
  {
    "text": "Um. Remind me of your name.",
    "start": "3281530",
    "end": "3288550"
  },
  {
    "text": "[inaudible].it's stuck, it becomes very hard for a different action to catch up. So the, uh, so in that sense that chance that is suck is more that important? And then [inaudible].",
    "start": "3288550",
    "end": "3299260"
  },
  {
    "text": "That is a really good one which is, okay, so maybe if one arm is really good, then, um, it becomes really hard for the other arms to, to catch up.",
    "start": "3299260",
    "end": "3304915"
  },
  {
    "text": "So in this case, that's true because theta 1, um, really does have 0.95. Let's imagine a slightly [NOISE] different case where this was",
    "start": "3304915",
    "end": "3311800"
  },
  {
    "text": "like 0.7 or something like that. Then in that case over time it would be likely that your Beta distribution is going to",
    "start": "3311800",
    "end": "3318520"
  },
  {
    "text": "converge to around the real distribution of the parameter. So if you keep sampling theta 1 forever, uh, eventually,",
    "start": "3318520",
    "end": "3325360"
  },
  {
    "text": "[NOISE] you know, it's gonna sort of collapse towards what the true value is. Um, and so if the true value isn't very close to one,",
    "start": "3325360",
    "end": "3333070"
  },
  {
    "text": "there will be some probability you'll sample. Like so imagine this versus a uniform.",
    "start": "3333070",
    "end": "3338320"
  },
  {
    "text": "That's not very close to 1. At some point, there is a non-zero probability that you'd sample something that's higher. [NOISE] So the beginning matters,",
    "start": "3338320",
    "end": "3346720"
  },
  {
    "text": "um, but you can outweigh it over time. Just like what we can with the empirical distributions.",
    "start": "3346720",
    "end": "3353000"
  },
  {
    "text": "Okay. So if we look at this and we sort of look at the incurred frequentist regret [NOISE] which is not the same as Bayesian regret because in",
    "start": "3353460",
    "end": "3359920"
  },
  {
    "text": "that case we'd have to average over the parameters. Um, in this case Thompson sampling is doing, uh, a lot better.",
    "start": "3359920",
    "end": "3365424"
  },
  {
    "text": "So [NOISE] in this case, here, this would be 0.85 and that'll be 0, 0, 0, 0.",
    "start": "3365425",
    "end": "3371410"
  },
  {
    "text": "[NOISE] Okay. So in this case, Thompson sampling would be doing a much better job. [NOISE] Now, um, Thompson sampling,",
    "start": "3371410",
    "end": "3381205"
  },
  {
    "text": "uh, actually does achieve the Lai and Robbins lower bound for the performance of a- an algorithm.",
    "start": "3381205",
    "end": "3387714"
  },
  {
    "text": "So, um, in terms of its lower bound is similar. So that's one indication this might be a good algorithm.",
    "start": "3387715",
    "end": "3394135"
  },
  {
    "text": "But we have, there's a lot of bounds for optimism. In general, um, the, the bounds for optimism are better than the bounds for Thompson sampling.",
    "start": "3394135",
    "end": "3401500"
  },
  {
    "text": "A lot of the Thompson sampling, uh, bounds end up converting to sort of upper confidence bounds.",
    "start": "3401500",
    "end": "3407650"
  },
  {
    "text": "A little bit like what  was asking. So if we, um, if you want to make Thompson sampling have frequentist-like bounds, um,",
    "start": "3407650",
    "end": "3415065"
  },
  {
    "text": "often we end up sort of making our comfort, our sort of being more optimistic in terms of Thompson sampling.",
    "start": "3415065",
    "end": "3421925"
  },
  {
    "text": "Okay, to put those. Um, but empirically Thompson sampling is often great. Yeah. Can you mix them?",
    "start": "3421925",
    "end": "3427960"
  },
  {
    "text": "Can you mix Thompson sampling and upper confidence bounds? Uh, maybe start with some form of upper confidence bound and",
    "start": "3427960",
    "end": "3433840"
  },
  {
    "text": "use that information to update like your priors on the Thompson sampling? [NOISE] Um, I, I shouldn't just say,",
    "start": "3433840",
    "end": "3440559"
  },
  {
    "text": "can you, could you mix them? Y- You probably could. I, you could probably do it. I don't know. So I guess to me one of the,",
    "start": "3440560",
    "end": "3446964"
  },
  {
    "text": "so maybe you could start with upper confidence bounds and then use Thompson sampling. Um, to me one of the big benefits of Thompson sampling",
    "start": "3446965",
    "end": "3452230"
  },
  {
    "text": "empirically is that it is less optimistic than upper confidence bounds. Upper confidence bounds tend to be too optimistic for too long.",
    "start": "3452230",
    "end": "3458365"
  },
  {
    "text": "And it's like, \"Oh, you know, I ran into the door 30,000 times, but maybe that 30,001th time I won't.\" You know, like for your robot or something like that.",
    "start": "3458365",
    "end": "3465190"
  },
  {
    "text": "So, um, it often tends to sort of think about the extreme events. Whereas, if you know that say the world is really Gaussian like the probability that",
    "start": "3465190",
    "end": "3473110"
  },
  {
    "text": "your robot is going to run into a wall again is still really high even if it's only run into the wall 10 times. So maybe you should pick a different path.",
    "start": "3473110",
    "end": "3479305"
  },
  {
    "text": "So I think often you probably would want to start with Thompson sampling,",
    "start": "3479305",
    "end": "3485200"
  },
  {
    "text": "but you would like to be robust to your prior. And so some work that tries to combine these ideas is known as PAC-Bayesian,",
    "start": "3485200",
    "end": "3491425"
  },
  {
    "text": "where you try to get, I'll define what PAC is in a second. But, um, you'll try to get bounds that are kinda frequentist-like,",
    "start": "3491425",
    "end": "3497140"
  },
  {
    "text": "but also get the best of both worlds. So you'd like to be like Bayesian if your prior is really good and, um, and PAC if you like sort of frequentist, if it turns out that your prior's wrong.",
    "start": "3497140",
    "end": "3506515"
  },
  {
    "text": "Okay. So, um, one of the papers that I think sort of changed a lot of people's minds about Bayesian and, uh,",
    "start": "3506515",
    "end": "3514105"
  },
  {
    "text": "bandits and also Thompson sampling being a good idea was this paper by my colleague Lihong Li and also, uh, Chapelle where they looked at",
    "start": "3514105",
    "end": "3520570"
  },
  {
    "text": "contextual bandits and we will hopefully get to this for a little bit on Wednesday. But the idea in contextual bandits is that you have a state and an action.",
    "start": "3520570",
    "end": "3528954"
  },
  {
    "text": "So it's a little bit different than the bandits we've seen so far. Unlike in MDPs your action does not affect the next state.",
    "start": "3528955",
    "end": "3535180"
  },
  {
    "text": "So for those of you doing the default project, you're seeing examples of this where how you treat the current patient doesn't impact the next patient that comes along,",
    "start": "3535180",
    "end": "3542530"
  },
  {
    "text": "but the patient characteristics can affect which arm is best. So, so contextual bandits is a very popular and powerful framework.",
    "start": "3542530",
    "end": "3551080"
  },
  {
    "text": "Um, and so in this case they were looking at news article recommendations and they were [NOISE] finding Thompson sampling did much better",
    "start": "3551080",
    "end": "3558160"
  },
  {
    "text": "than upper confidence bounds in a number of other algorithms. It also can be more robust, uh, when your outcomes are delayed.",
    "start": "3558160",
    "end": "3565119"
  },
  {
    "text": "So this happens a lot often in real cases. You can imagine here you treat a patient, you're not gonna find out whether or not that toe procedure helps for another six weeks.",
    "start": "3565120",
    "end": "3573655"
  },
  {
    "text": "But in the meantime other people come in whose toes needs to be treated. Um, and if you use upper confidence bound algorithms [NOISE] they tend to be deterministic. Bless you.",
    "start": "3573655",
    "end": "3580705"
  },
  {
    "text": "And, um, and so you just keep treating everybody with the same thing until you get the outcome from the first, whereas Thompson sampling is stochastic.",
    "start": "3580705",
    "end": "3587755"
  },
  {
    "text": "So you'll be sort of trying out a lot of things. That's another good reason in practice why Thompson sampling can be helpful.",
    "start": "3587755",
    "end": "3594835"
  },
  {
    "text": "Okay. So I'm not gonna go through the proof today, but I'll put some pointers so that, um, the, the nice thing is that if you look at the Bayesian regret of Thompson sampling,",
    "start": "3594835",
    "end": "3603160"
  },
  {
    "text": "uh, it's going to have a similar result to what upper confidence bounds has. So it essentially has,",
    "start": "3603160",
    "end": "3609680"
  },
  {
    "text": "has the same regar- regrets bounds as UCB, essentially.",
    "start": "3609720",
    "end": "3616175"
  },
  {
    "text": "I'm being slightly hand-wavey for that, there's some important subtle details, but roughly you can show that these also have",
    "start": "3616175",
    "end": "3622240"
  },
  {
    "text": "good Bayesian regret bounds if your prior's correct. Um, and so that's sort of again a nice sanity check,",
    "start": "3622240",
    "end": "3627609"
  },
  {
    "text": "that you kind of get this logarithmic regret growth. All right. Another framework that I just mentioned sort",
    "start": "3627610",
    "end": "3635530"
  },
  {
    "text": "of in passing just now is probably approximately correct. So, these theoretical regret bounds specify how your regret grows over time.",
    "start": "3635530",
    "end": "3643555"
  },
  {
    "text": "Um, and one thing that's hard to know is whether you're making a lot of small mistakes or a few big mistakes.",
    "start": "3643555",
    "end": "3648925"
  },
  {
    "text": "Your regret bounds are cumulative, so it doesn't allow you to distinguish between those two. So, you can imagine in the case of patient treatments,",
    "start": "3648925",
    "end": "3655150"
  },
  {
    "text": "this could be pretty important. Like are you giving everybody a headache, um, or are a few patients really,",
    "start": "3655150",
    "end": "3660295"
  },
  {
    "text": "you know, having really really bad side effects. So, so regret- cumulative regret,",
    "start": "3660295",
    "end": "3666130"
  },
  {
    "text": "um, doesn't distinguish between those two, because if a couple of people have really bad side effects, that's the same as a lot of people having headaches when you average over those.",
    "start": "3666130",
    "end": "3674140"
  },
  {
    "text": "Um, and so one idea is to say well, maybe we just wanna kinda bound the number of non-small errors.",
    "start": "3674140",
    "end": "3681895"
  },
  {
    "text": "So, we wanna bound the number of people that experience really bad side effects, for example. [NOISE] So, Probably Approximately Correct comes up in supervised learning.",
    "start": "3681895",
    "end": "3690970"
  },
  {
    "text": "In the context of decision-making, we often define it as follows, a Probably Approximately Correct algorithm or a PAC state that",
    "start": "3690970",
    "end": "3698079"
  },
  {
    "text": "the algorithm will choose an action who is- which is epsilon close to optimal, with probabilities 1 - delta,",
    "start": "3698080",
    "end": "3704710"
  },
  {
    "text": "on all but a polynomial number of steps. So, the probability part comes from here,",
    "start": "3704710",
    "end": "3711940"
  },
  {
    "text": "so it's not guaranteeing that you will do this, but with high confidence or probably it will do this.",
    "start": "3711940",
    "end": "3718150"
  },
  {
    "text": "It's approximately correct because we're only guaranteeing epsilon-optimality.",
    "start": "3718150",
    "end": "3723474"
  },
  {
    "text": "And the important aspect is it's- only it does- makes these sort of, um,",
    "start": "3723475",
    "end": "3729130"
  },
  {
    "text": "makes mistakes that might be bigger than epsilon, so the number of, you know, patients we might treat that have really, really bad side effects is gonna be no more than a polynomial function.",
    "start": "3729130",
    "end": "3738369"
  },
  {
    "text": "Where the polynomial function is a function of the parameters of your domain.",
    "start": "3738370",
    "end": "3743650"
  },
  {
    "text": "So, things like the number of actions you have, epsilon and delta.",
    "start": "3743650",
    "end": "3748849"
  },
  {
    "text": "And you should be able to compute this in advance too. So, you should be able to compute how many mistakes you might make.",
    "start": "3749130",
    "end": "3755485"
  },
  {
    "text": "Um, and one of the cool things is that you g- a lot of the PAC algorithms, um, algorithms that are PAC are based on optimism or Thompson sampling.",
    "start": "3755485",
    "end": "3764275"
  },
  {
    "text": "Now, PAC for bandits is a much less common, uh, approach than when we go to MDPs.",
    "start": "3764275",
    "end": "3770575"
  },
  {
    "text": "In bandits, most of the time we look at regret. But for when we look at Markov Decision Processes,",
    "start": "3770575",
    "end": "3776830"
  },
  {
    "text": "PAC is more popular, and, and we'll see one of the reasons for that probably later,",
    "start": "3776830",
    "end": "3782035"
  },
  {
    "text": "or feel free to ask me about it if we don't get to it today. Okay. So, what would PAC look like in our little example we had here before?",
    "start": "3782035",
    "end": "3789809"
  },
  {
    "text": "So, let's use O to denote optimism, TS to denote Thompson sampling, and within epsilon, um,",
    "start": "3789810",
    "end": "3795990"
  },
  {
    "text": "means that the action that we select is within epsilon of the optimal action. So, its value is epsilon close to the optimal action.",
    "start": "3795990",
    "end": "3803815"
  },
  {
    "text": "So, I've written down the regret in this case. Um, here what we'd have is that the- for, um,",
    "start": "3803815",
    "end": "3810925"
  },
  {
    "text": "optimism, the first action that we pull is a1, so, um, it's within epsilon, yes,",
    "start": "3810925",
    "end": "3817300"
  },
  {
    "text": "because we're close to the optimal action, a2, um, has a mean of 0.9,",
    "start": "3817300",
    "end": "3824050"
  },
  {
    "text": "so that's within 0.05 of 0.95, so this is yes. Action a3 is 0.1,",
    "start": "3824050",
    "end": "3830830"
  },
  {
    "text": "so it's not within epsilon of the optimal action, so this is no, and so forth.",
    "start": "3830830",
    "end": "3836870"
  },
  {
    "text": "So, this essentially allows the algorithm to be taking either a1 or action a2 under this definition of epsilon.",
    "start": "3837150",
    "end": "3844150"
  },
  {
    "text": "Because I don't care whether or not you're taking action a1 or a2, both of them are really pretty good; both of them are within 0.05 of each other, I mean, that's fine.",
    "start": "3844150",
    "end": "3852415"
  },
  {
    "text": "Um, but you- we don't want you to take action 3 very much because it's much worse.",
    "start": "3852415",
    "end": "3857575"
  },
  {
    "text": "Um, and then in this case, uh, this one would say, this is not within epsilon because the first action we",
    "start": "3857575",
    "end": "3864099"
  },
  {
    "text": "take is bad but then all the rest are good. And what a PAC approach would do would they'd be counting",
    "start": "3864100",
    "end": "3870820"
  },
  {
    "text": "all these- counting the, the mistakes.",
    "start": "3870820",
    "end": "3874010"
  },
  {
    "text": "Okay. So, we just talked about for bandits, um, different sorts of frameworks and criterias.",
    "start": "3881790",
    "end": "3888040"
  },
  {
    "text": "We talked about regret, Bayesian regret and PAC, um, [NOISE] and we talked about two styles of approaches,",
    "start": "3888040",
    "end": "3893530"
  },
  {
    "text": "either optimism or Thompson sampling. And what we can see now is that Markov decision processes",
    "start": "3893530",
    "end": "3898900"
  },
  {
    "text": "have many of the same sorts of ideas being applicable, but it also does get a lot more challenging.",
    "start": "3898900",
    "end": "3905180"
  },
  {
    "text": "So, in particular what we're gonna talk about right now is we're gonna talk about tabular MDPs.",
    "start": "3905250",
    "end": "3912430"
  },
  {
    "text": "And it turns out that even from with tabular MDPs that things are a lot more subtle. Um, so, how does this work?",
    "start": "3912770",
    "end": "3920260"
  },
  {
    "text": "The, the regret- the Bayesian regret in PAC is all gonna be applicable, so is optimism, and so is probability matching.",
    "start": "3920260",
    "end": "3927109"
  },
  {
    "text": "So, let's start with thinking about optimism under uncertainty. First, let's think about just doing optimistic initialization.",
    "start": "3927570",
    "end": "3936040"
  },
  {
    "text": "So, in this case, imagine that we just initialize all of our queue state actions, um, to some value.",
    "start": "3936040",
    "end": "3942610"
  },
  {
    "text": "So, let's imagine that we initialize them to rmax divided by 1 - gamma, where rmax is the highest reward you could see in any state-action pair.",
    "start": "3942610",
    "end": "3952190"
  },
  {
    "text": "Let's just take one minute, why is that value guaranteed to be optimistic?",
    "start": "3952740",
    "end": "3957680"
  },
  {
    "text": "Anybody wanna answer why that's guaranteed to be optimistic?",
    "start": "3969600",
    "end": "3974030"
  },
  {
    "text": "Right. Yeah. It's higher than like, the possible, um, total value,",
    "start": "3977070",
    "end": "3984280"
  },
  {
    "text": "because like we've shown a couple of times that rmax one line of scandal would be the highest value, but it goes on a bit [inaudible].",
    "start": "3984280",
    "end": "3990250"
  },
  {
    "text": "That's right. Yeah, so what  said is correct. Um, we've shown that for a discounted Markov decision process that the highest value you could get is rmax divided by 1 - gamma.",
    "start": "3990250",
    "end": "3998305"
  },
  {
    "text": "At best all of your states have that, or else some of them might not, so this is guaranteed to be an optimistic value.",
    "start": "3998305",
    "end": "4004305"
  },
  {
    "text": "So, you could start off and if you've, uh, initialized all of your state action values to be rmax divided by 1 - gamma.",
    "start": "4004305",
    "end": "4010725"
  },
  {
    "text": "And then you can do Monte-Carlo, you can do Q-learning, you can do Sarsa. Um, and you could incrementally update using that.",
    "start": "4010725",
    "end": "4018420"
  },
  {
    "text": "And this can be very helpful, it can sort of encourage systematic exploration of states and actions,",
    "start": "4018420",
    "end": "4023700"
  },
  {
    "text": "because essentially you're pretending that everything in the world is really awesome, um, until proven otherwise.",
    "start": "4023700",
    "end": "4029470"
  },
  {
    "text": "So, on the downside, unfortunately if you do this in general there's no guarantees on performance,",
    "start": "4030650",
    "end": "4035880"
  },
  {
    "text": "um, even though it's often empirically better. So, even though this really is,",
    "start": "4035880",
    "end": "4042165"
  },
  {
    "text": "um, you- you know, an upper bound, this is optimistic. Um, a key issue is how quickly you're updating from those optimistic values.",
    "start": "4042165",
    "end": "4051135"
  },
  {
    "text": "So, as an early result in this case, Even-Dar and Mansour in 2002 proved that,",
    "start": "4051135",
    "end": "4056760"
  },
  {
    "text": "if you run Q-learning with learning rates- this should say alpha-i.",
    "start": "4056760",
    "end": "4061690"
  },
  {
    "text": "So if you, uh, run Q-learning with particular alpha rates, um, alpha-i on each time step i,",
    "start": "4061970",
    "end": "4069030"
  },
  {
    "text": "and you initialize the value of a state, so this is the very beginning, um, to be rmax divide by 1 - gamma times the product of those learning rates,",
    "start": "4069030",
    "end": "4078705"
  },
  {
    "text": "and t is the number of samples you need to learn optimal Q, then greedy-only Q-learning is PAC with that initialization.",
    "start": "4078705",
    "end": "4085980"
  },
  {
    "text": "So, I just wanna highlight something here which is this part. So, notice this is way, way,",
    "start": "4085980",
    "end": "4092910"
  },
  {
    "text": "way larger than just rmax over 1 - gamma, because this is a product of all your learning rates.",
    "start": "4092910",
    "end": "4099270"
  },
  {
    "text": "Okay, so, this could be really enormous, like you'd imagine that, um,",
    "start": "4099270",
    "end": "4105674"
  },
  {
    "text": "imagine that alpha = 0.1 for all time steps,",
    "start": "4105675",
    "end": "4111270"
  },
  {
    "text": "then what you have here is you have 1 over 0.1 to the t, which is approximately- it would just equal to 10 to the t [NOISE].",
    "start": "4111270",
    "end": "4121830"
  },
  {
    "text": "So, this is like exponential in the number of time steps you're gonna make decisions. It's incredibly optimistic.",
    "start": "4121830",
    "end": "4128895"
  },
  {
    "text": "Um, it turns out this is sufficient to be PAC, but it's also not very good. Um, uh, it's, it's very, very extremely large.",
    "start": "4128895",
    "end": "4137370"
  },
  {
    "text": "Okay? Um, now, there's been some really cool work by Chi Jin and some others over at Berkeley that showed that,",
    "start": "4137370",
    "end": "4143744"
  },
  {
    "text": "um, if you use a less optimistic initialization, um, that's strongly related to upper confidence bounds,",
    "start": "4143745",
    "end": "4150375"
  },
  {
    "text": "um, and you were careful about your learning rates, so you have to change your learning rates, but if you're careful about your learning rates,",
    "start": "4150375",
    "end": "4156359"
  },
  {
    "text": "they proved that, um, model-free Q-learning could also be PAC. And this was a pretty big deal recently because almost",
    "start": "4156360",
    "end": "4162930"
  },
  {
    "text": "all of the work that's been going on has been in the model-based setting. So this just came out in NeurIPS, uh,",
    "start": "4162930",
    "end": "4167940"
  },
  {
    "text": "about two months ago, um, and so they- oh sorry, not PAC. They, they showed the regret bounds. Um, they're not optimal regret bounds, but they're good.",
    "start": "4167940",
    "end": "4174825"
  },
  {
    "text": "So, um, they're, they're not tight yet but, uh, it shows that model-free algorithms can do pretty well.",
    "start": "4174825",
    "end": "4180029"
  },
  {
    "text": "[NOISE] Okay. So what about model-based approaches?",
    "start": "4180030",
    "end": "4186210"
  },
  {
    "text": "And the model-based approaches for MDPs are the ones where we really have the best bounds right now. So there's a couple of main ideas or a couple different procedures we could go with.",
    "start": "4186210",
    "end": "4194610"
  },
  {
    "text": "One is that, you can be really, really optimistic in all your estimates, until you're confident that your empirical estimates of",
    "start": "4194610",
    "end": "4202079"
  },
  {
    "text": "your dynamics and reward model are close to the true dynamics in reward model parameters.",
    "start": "4202080",
    "end": "4207880"
  },
  {
    "text": "So these sort of algorithms proceed as if they say, the reward for all state action pairs is amazing,",
    "start": "4208370",
    "end": "4214800"
  },
  {
    "text": "it's rmax divided by 1 - gamma. And I'm gonna continue to pretend that's true, until I think I have enough data for",
    "start": "4214800",
    "end": "4220920"
  },
  {
    "text": "that state action pair that I think that if I did a MLE, maximum likelihood estimate of those parameters,",
    "start": "4220920",
    "end": "4226829"
  },
  {
    "text": "they will be close to the true parameters. So you could say, I'm just going to be incredibly optimistic until I've got enough data.",
    "start": "4226830",
    "end": "4233909"
  },
  {
    "text": "And then when I've got enough data then I, um, think I can get a good empirical estimate that is close to the true estimate,",
    "start": "4233910",
    "end": "4239250"
  },
  {
    "text": "and then I'll use those instead. So it's almost kinda like a switching point. You sort of keep, um, you pretend everything's really,",
    "start": "4239250",
    "end": "4244860"
  },
  {
    "text": "really great until you get enough data, and then you switch over to the empirical estimate. So these were some of the earliest ones, um,",
    "start": "4244860",
    "end": "4251835"
  },
  {
    "text": "that showed that MDPs could be pa- oh, algorithms for MDPs could be PAC. This is from 2002.",
    "start": "4251835",
    "end": "4257594"
  },
  {
    "text": "Uh, but they're also empirically not normally so good because, um,",
    "start": "4257595",
    "end": "4263070"
  },
  {
    "text": "you're pretending things are really, really awesome, even though you might have quite a lot of evidence for that state-action pair that it's not awesome.",
    "start": "4263070",
    "end": "4268905"
  },
  {
    "text": "So another approach is to be optimistic given the information you have. So what do I mean by that?",
    "start": "4268905",
    "end": "4274350"
  },
  {
    "text": "I mean that as your agent walks around and gathers observations of the actions and rewards it gets,",
    "start": "4274350",
    "end": "4280215"
  },
  {
    "text": "it uses that to try to estimate, um, how good the world could be given that data.",
    "start": "4280215",
    "end": "4285554"
  },
  {
    "text": "And so one approach to this is to compute confidence sets on dynamics and rewards models.",
    "start": "4285555",
    "end": "4290805"
  },
  {
    "text": "So we already saw this for bandits, where we computed upper and lower confidence, or we could compute upper and lower confidence bounds for the rewards.",
    "start": "4290805",
    "end": "4297735"
  },
  {
    "text": "Turns out we can also compute confidence sets over the dynamics model. Or we could just add reward bonuses that depend on the experience or data.",
    "start": "4297735",
    "end": "4307440"
  },
  {
    "text": "And I'm gonna talk, um, at least a little bit today before we finish, about the second thing.",
    "start": "4307440",
    "end": "4313659"
  },
  {
    "text": "And the reason I'm gonna talk about this particular approach is because when we start to think about doing this in the function approximation setting,",
    "start": "4314020",
    "end": "4321520"
  },
  {
    "text": "if the way that your dynamics model is represented is by a deep neural network,",
    "start": "4321520",
    "end": "4326685"
  },
  {
    "text": "um, then writing down, ah, uncertainties over that can be really tricky. And also a lot of the progress in",
    "start": "4326685",
    "end": "4332790"
  },
  {
    "text": "deep neural networks for RL focus on model-free approaches. And if we have reward bonuses,",
    "start": "4332790",
    "end": "4338070"
  },
  {
    "text": "then we can easily extend that to the model-free case. Um, and empirically these ones generally do",
    "start": "4338070",
    "end": "4343755"
  },
  {
    "text": "pretty much as well as if we use explicit confidence sets. So I'm just gonna explain how the model-based confidence,",
    "start": "4343755",
    "end": "4351824"
  },
  {
    "text": "model-based interval estimation with exploration bonus works. Um, so it's gonna assume that we're given an epsilon delta and some constant m. Okay.",
    "start": "4351825",
    "end": "4363750"
  },
  {
    "text": "And then what we're gonna do is we're gonna initialize some counts. [NOISE] So this is just",
    "start": "4363750",
    "end": "4371160"
  },
  {
    "text": "gonna keep track of the number of times we've seen a state-action pair. So we're gonna do this for all s and for all a.",
    "start": "4371160",
    "end": "4377250"
  },
  {
    "text": "We're also gonna keep track of the number of times we've seen an actio- state-action, next state pair.",
    "start": "4377250",
    "end": "4383085"
  },
  {
    "text": "0 for all s, for all a, for all s prime.",
    "start": "4383085",
    "end": "4388275"
  },
  {
    "text": "And we're also going to keep track of the total sum of rewards we've gotten from any state and action pair.",
    "start": "4388275",
    "end": "4393795"
  },
  {
    "text": "So we're gonna say rc of s, a = 0 for all s. So essentially we are gonna keep track of the times that we've been in any state,",
    "start": "4393795",
    "end": "4407640"
  },
  {
    "text": "taking any action and went to any next state, and what the sum of rewards are for when we've done that.",
    "start": "4407640",
    "end": "4414159"
  },
  {
    "text": "And then we're going to define a beta parameter.",
    "start": "4415490",
    "end": "4420190"
  },
  {
    "text": "Okay, I'm going to double-check, I get the-",
    "start": "4421640",
    "end": "4424720"
  },
  {
    "text": "Yeah.",
    "start": "4429590",
    "end": "4441869"
  },
  {
    "text": "Okay. All right. So beta is gonna be a parameter that we're gonna use to define our reward bonuses.",
    "start": "4441870",
    "end": "4448270"
  },
  {
    "text": "Okay, it's 1 over 1 - gamma, 2 log the number of states, number of actions, 2 times m, m is an input parameter divided by delta.",
    "start": "4448310",
    "end": "4457170"
  },
  {
    "text": "Okay. And then- yeah,",
    "start": "4457170",
    "end": "4465960"
  },
  {
    "text": "I think that's all I need here. Now I'm gonna say t = 0. We're going to initialize our state.",
    "start": "4465960",
    "end": "4471880"
  },
  {
    "text": "And to start, we can just say Qt of s, a = 1 divided by 1 - gamma.",
    "start": "4478250",
    "end": "4489970"
  },
  {
    "text": "And this assumes that all of our rewards are bounded between 0 and 1.",
    "start": "4491440",
    "end": "4496460"
  },
  {
    "text": "So they're bounded rewards. Okay, so we start off when we initialize all our accounts to 0,",
    "start": "4496460",
    "end": "4501540"
  },
  {
    "text": "we said we haven't observe- observed any rewards yet, and we pretend that the world is awesome, and that our Q value is the highest it could possibly be in every state-action pair.",
    "start": "4501540",
    "end": "4509910"
  },
  {
    "text": "Um, here r-max = 1. So r-max is going to be equal to 1",
    "start": "4509910",
    "end": "4515190"
  },
  {
    "text": "because our rewards are bounded between 0 and 1. So what we do then is we take an action in the current state,",
    "start": "4515190",
    "end": "4522389"
  },
  {
    "text": "given our- let's do tildes given our Q function.",
    "start": "4522390",
    "end": "4528465"
  },
  {
    "text": "So getting, which is going to break ties randomly. And then we're going to observe the reward and observe the next state.",
    "start": "4528465",
    "end": "4534150"
  },
  {
    "text": "And then we just update our counts. So we update our counts for that particular state action pair.",
    "start": "4534150",
    "end": "4539970"
  },
  {
    "text": "We update our counts for s, a, s prime, s, a, s prime, for the number of",
    "start": "4539970",
    "end": "4547740"
  },
  {
    "text": "times we've been in that state taking that action and went to that particular next state. And then we update our rewards for that state-action pair.",
    "start": "4547740",
    "end": "4555495"
  },
  {
    "text": "It is equal to the previous rewards for that state action pair plus r_t.",
    "start": "4555495",
    "end": "4560740"
  },
  {
    "text": "And then what we're gonna do is we're gonna use, um, those empirical counts to define",
    "start": "4561710",
    "end": "4567389"
  },
  {
    "text": "an empirical transition model and empirical reward model. So our reward model is going to just be the MLE reward model,",
    "start": "4567390",
    "end": "4575969"
  },
  {
    "text": "which is just gonna be rc for s, a divide-",
    "start": "4575970",
    "end": "4581085"
  },
  {
    "text": "times- divided by the number of times we've been in that state-action pair.",
    "start": "4581085",
    "end": "4588219"
  },
  {
    "text": "That's just the average reward for that state-action pair.",
    "start": "4588350",
    "end": "4593260"
  },
  {
    "text": "And then our transition model is also just going to be the number of times a,",
    "start": "4594860",
    "end": "4605670"
  },
  {
    "text": "s prime divided by the number of times you've been in that state-action pair.",
    "start": "4605670",
    "end": "4611699"
  },
  {
    "text": "We're just gonna define our empirical transition model and our empirical reward model. And it doesn't matter how we initialize things that we haven't seen at all.",
    "start": "4611700",
    "end": "4621510"
  },
  {
    "text": "But you can treat them as uniform. Okay. So we're gonna do this for all s, a.",
    "start": "4621510",
    "end": "4627850"
  },
  {
    "text": "And then we're gonna compute some new Q functions. Okay. And we're gonna compute some new Q functions where we do this.",
    "start": "4628040",
    "end": "4637659"
  },
  {
    "text": "Where we take our empirical models and we also add in",
    "start": "4640040",
    "end": "4645240"
  },
  {
    "text": "a reward bonus term that depends on beta and the number of times we've tried that state action pair.",
    "start": "4645240",
    "end": "4651210"
  },
  {
    "text": "And we can do value iteration. That's what I'm doing here. But you could solve it however you'd like.",
    "start": "4651210",
    "end": "4656550"
  },
  {
    "text": "But the main idea here is that we're gonna use our empirical estimates of the reward model and the transition model by just averaging our counts,",
    "start": "4656550",
    "end": "4664170"
  },
  {
    "text": "or averaging the rewards we've gotten for that state-action pair. And then we're gonna add in this as a reward bonus.",
    "start": "4664170",
    "end": "4670510"
  },
  {
    "text": "And note at the beginning of this reward bonus can be, like, it can be infinity, so you can- because if we have no counts for that,",
    "start": "4674510",
    "end": "4681389"
  },
  {
    "text": "so then we can just initialize for, for any Q s,a. So for all s, a such that nsa of s,a = 0.",
    "start": "4681390",
    "end": "4692820"
  },
  {
    "text": "You can just set this to be Q-max. So to deal with if you haven't sampled that state-action pair yet.",
    "start": "4692820",
    "end": "4702510"
  },
  {
    "text": "So that means anything for which you haven't sampled it yet is gonna look maximally awesome.",
    "start": "4702510",
    "end": "4707940"
  },
  {
    "text": "And anything else is going to be a combination of its empirical average parameters,",
    "start": "4707940",
    "end": "4714455"
  },
  {
    "text": "plus a reward bonus. And that reward bonus is gonna get smaller as we have more data.",
    "start": "4714455",
    "end": "4720530"
  },
  {
    "text": "So I'll put this on here where it will be neater. Um, so this is the reward bonus.",
    "start": "4720530",
    "end": "4727500"
  },
  {
    "text": "And what you can see here is that over time that's going to shrink. Over time, um, you're going to get closer and closer to",
    "start": "4730400",
    "end": "4737820"
  },
  {
    "text": "using the empirical estimates for a particular state action pair. But for state action pairs you haven't tried very much,",
    "start": "4737820",
    "end": "4743460"
  },
  {
    "text": "there's going to be a large reward bonus.",
    "start": "4743460",
    "end": "4746140"
  },
  {
    "text": "So the- the cool thing about this is that, um, we can think about whether it's PAC.",
    "start": "4748670",
    "end": "4753735"
  },
  {
    "text": "So I'll just take one more minute, which is in an RL case, ah, an algorithm is PAC if on all but N time steps,",
    "start": "4753735",
    "end": "4760380"
  },
  {
    "text": "the action selected is epsilon-close to the optimal action, where N is a polynomial function of these things.",
    "start": "4760380",
    "end": "4765960"
  },
  {
    "text": "The number of states, number of actions, gamma, epsilon, and delta, this is not true for all algorithms.",
    "start": "4765960",
    "end": "4772474"
  },
  {
    "text": "Greedy is not PAC. Greedy can be exponential. Um, we might talk about that on Wednesday. So no.",
    "start": "4772475",
    "end": "4781640"
  },
  {
    "text": "And the nice thing is that the MBIE-EB algorithm I just showed you is PAC.",
    "start": "4781640",
    "end": "4787010"
  },
  {
    "text": "So what does it PAC in? It means that on all, but this number of time-steps,",
    "start": "4787010",
    "end": "4792485"
  },
  {
    "text": "well I'll just circle it. So this is sort of a large ugly expression,",
    "start": "4792485",
    "end": "4800520"
  },
  {
    "text": "but it is polynomial in the number of states and actions. It's also a function of the discount factor and the epsilon.",
    "start": "4800520",
    "end": "4806685"
  },
  {
    "text": "In general, if you want to be closer to optimal, it's gonna take you more data to ensure that you're close to optimal. So it's inversely dependent on epsilon,",
    "start": "4806685",
    "end": "4813840"
  },
  {
    "text": "ah, it's polynomially dependent on, ah, state and actions. And this says on all but,",
    "start": "4813840",
    "end": "4820050"
  },
  {
    "text": "ah, this many time steps, your algorithm is gonna be taking actions that are close to optimal. So this is pretty cool.",
    "start": "4820050",
    "end": "4826770"
  },
  {
    "text": "It says like by just using these average estimates plus tacking on a bonus term, and then computing the Q functions, um,",
    "start": "4826770",
    "end": "4832950"
  },
  {
    "text": "then you can actually act really well on all time-steps except for a polynomial number.",
    "start": "4832950",
    "end": "4838455"
  },
  {
    "text": "Okay. And then, um, I put in here this are theoretical of that,",
    "start": "4838455",
    "end": "4843570"
  },
  {
    "text": "and I'll just say briefly that on some sort of hard to construct, sort of simple toy domains.",
    "start": "4843570",
    "end": "4848730"
  },
  {
    "text": "These type of algorithms do much better even than some other ones that are provably efficient. And they can do much, much better than things like greedy.",
    "start": "4848730",
    "end": "4855960"
  },
  {
    "text": "So algorithms like MBIE-EB, MBIE is a related one that uses confidence sets, um,",
    "start": "4855960",
    "end": "4862275"
  },
  {
    "text": "it can do much, much better than this is sort of be, be optimistic until confident.",
    "start": "4862275",
    "end": "4870790"
  },
  {
    "text": "And these ones are generally much better than greedy. So these types of optimistic algorithms can empirically be much better,",
    "start": "4872420",
    "end": "4879570"
  },
  {
    "text": "as well being provably better. And on Wednesday we'll start to talk about how to combine them with generalization. Thanks.",
    "start": "4879570",
    "end": "4885909"
  }
]