[
  {
    "start": "0",
    "end": "254000"
  },
  {
    "start": "0",
    "end": "5830"
  },
  {
    "text": "Great. So let's get started for today. OK, so let's get\nstarted for today.",
    "start": "5830",
    "end": "12210"
  },
  {
    "text": "Today we're going to be talking\nabout Bayesian meta-learning, and specifically\nwe'll talk about what",
    "start": "12210",
    "end": "17530"
  },
  {
    "text": "are the benefits of this\nclass of approaches. We'll describe different\nways to integrate",
    "start": "17530",
    "end": "27825"
  },
  {
    "text": "Bayesian kind of\ngraphical models into meta-learning approaches,\nboth for black-box approaches and optimization-based\napproaches.",
    "start": "27825",
    "end": "33580"
  },
  {
    "text": "And then finally, we'll\ntalk about evaluation and the different\nkinds of things that you can do when you\nhave a Bayesian meta-learning",
    "start": "33580",
    "end": "39580"
  },
  {
    "text": "algorithm. By the end of the\nlecture, the goal is for you to be\nable to understand",
    "start": "39580",
    "end": "47300"
  },
  {
    "text": "the interpretation\nof meta-learning within a Bayesian\ninference framework. And also understand\nthe techniques",
    "start": "47300",
    "end": "52760"
  },
  {
    "text": "for representing\nuncertainty over parameters and over predictions.",
    "start": "52760",
    "end": "58720"
  },
  {
    "text": "Disclaimers, similar to\nthe lecture on Monday, this is a very active\narea of research,",
    "start": "58720",
    "end": "65370"
  },
  {
    "text": "and also, like most\nof the class content. So there's going to be more\nquestions than answers. And the lecture is also going to\ncover some of the most advanced",
    "start": "65370",
    "end": "74340"
  },
  {
    "text": "and some of the mathiest\ntopics of this course. I'll do my best to go\nslowly and hope to still be",
    "start": "74340",
    "end": "83700"
  },
  {
    "text": "able to convey the intuition. So please ask questions\nthroughout based on what is-- ",
    "start": "83700",
    "end": "91420"
  },
  {
    "text": "based on, yeah, the content. I also see a message\nin the chat about being",
    "start": "91420",
    "end": "96460"
  },
  {
    "text": "able to download the\nslides on the website. I reached out to\na TA to do that, and so I hope the TA will\nupload the slides very soon.",
    "start": "96460",
    "end": "106329"
  },
  {
    "text": "So they'll be available\nwhenever the TA gets to it. ",
    "start": "106330",
    "end": "111850"
  },
  {
    "text": "OK, cool. So a recap from last week.",
    "start": "111850",
    "end": "118107"
  },
  {
    "text": "We talked about different\nmeta-learning approaches and we specifically\nlooked-- we specifically overviewed two\ndifferent perspectives",
    "start": "118107",
    "end": "124900"
  },
  {
    "text": "on meta-learning algorithms. The first perspective was the\nperspective of a computation",
    "start": "124900",
    "end": "130449"
  },
  {
    "text": "graph, and we saw\nhow we could view different kinds of approaches\nwithin the same perspective",
    "start": "130449",
    "end": "136120"
  },
  {
    "text": "of past-- a computation graph\nthat takes as input training data and a test input,\nand processes it",
    "start": "136120",
    "end": "141430"
  },
  {
    "text": "either with a black-box\nfunction, with an optimization, or with a non-parametric\ncomputation.",
    "start": "141430",
    "end": "150217"
  },
  {
    "text": "So this is the first\nperspective that we covered. And then we also thought\nabout the algorithms from their algorithmic\nproperties,",
    "start": "150218",
    "end": "157360"
  },
  {
    "text": "where one property was\nthinking about the expressive power of the meta-learner. And another property was\nthinking about the consistency",
    "start": "157360",
    "end": "164290"
  },
  {
    "text": "of the meta-learner. And these two properties\nare both quite important for a\nlot of applications",
    "start": "164290",
    "end": "172180"
  },
  {
    "text": "and there was a third property\nthat we mentioned on Wednesday, last week, where we were talking\nabout uncertainty awareness",
    "start": "172180",
    "end": "178217"
  },
  {
    "text": "where we want to build\na reason about ambiguity during the learning\nprocess which",
    "start": "178217",
    "end": "183640"
  },
  {
    "text": "may help in a\nvariety of situations such as active learning\nand reinforcement learning, and also bring in some\nprincipled probabilistic",
    "start": "183640",
    "end": "191590"
  },
  {
    "text": "interpretations to\nthese algorithms.",
    "start": "191590",
    "end": "197090"
  },
  {
    "text": "And this kind of\nlast property is exactly what we're going\nto be talking about today. ",
    "start": "197090",
    "end": "204459"
  },
  {
    "text": "So first, let's talk about some\nmotivation and the benefits that this\ninterpretation gives us.",
    "start": "204460",
    "end": "212540"
  },
  {
    "text": "So earlier in the\ncourse, we talked about how at training and test\ntimes, we want things to match,",
    "start": "212540",
    "end": "221050"
  },
  {
    "text": "and we also talked about\nhow the tasks have to share some amount of structure.",
    "start": "221050",
    "end": "226500"
  },
  {
    "text": "But we never really went\ninto what structure means. What does it mean for a\ntask to share structure?",
    "start": "226500",
    "end": "233460"
  },
  {
    "text": "What is that structure? And one way to kind of get\na handle on this question",
    "start": "233460",
    "end": "239610"
  },
  {
    "text": "is to think about that structure\nas a statistical dependence on some shared information\nwhere that kind of shared",
    "start": "239610",
    "end": "248398"
  },
  {
    "text": "kind of information is\nlatent, it's not something that we can directly observe.",
    "start": "248398",
    "end": "254010"
  },
  {
    "start": "254000",
    "end": "732000"
  },
  {
    "text": "And in particular, we can\nconsider this graphical model here, which uses\nplate notation, where",
    "start": "254010",
    "end": "261239"
  },
  {
    "text": "you have some kind of shared\nlatent information theta. ",
    "start": "261240",
    "end": "267615"
  },
  {
    "text": "This has, this kind of-- There's a dependence from this\nshared information to each of the task parameters phi i.",
    "start": "267615",
    "end": "273990"
  },
  {
    "text": "And I'm using the plate here to\ndenote that there are kind of-- i is indexing the different\ntask-specific parameters",
    "start": "273990",
    "end": "280680"
  },
  {
    "text": "for each of our tasks. And then there's\nalso a dependence from the task-specific\nparameters",
    "start": "280680",
    "end": "285949"
  },
  {
    "text": "to the labels conditioned\non the kind of inputs.",
    "start": "285950",
    "end": "292310"
  },
  {
    "text": "So this part right here is\nessentially D train for task i",
    "start": "292310",
    "end": "298220"
  },
  {
    "text": "and these two variables over\nhere correspond to D test. For task i and j is\nindexing the data points",
    "start": "298220",
    "end": "307340"
  },
  {
    "text": "within those data sets. And everything that is shaded\nmeans that it's observed.",
    "start": "307340",
    "end": "314599"
  },
  {
    "text": "So we can always observe D\ntrain, but at meta-test time, we can only observe x test. We can't observe y test.",
    "start": "314600",
    "end": "320690"
  },
  {
    "text": "That's what we're\ntrying to predict. It's kind of partially\nshaded because we do get to observe y test during\nthe meta-training process.",
    "start": "320690",
    "end": "327379"
  },
  {
    "text": " So this is, in many\nways, a graphical model that explains the kind of\ndata generation process",
    "start": "327380",
    "end": "337530"
  },
  {
    "text": "in a meta-learning setting. And there's a couple\nof things that you",
    "start": "337530",
    "end": "343240"
  },
  {
    "text": "can do that this graphical\nmodel can potentially tell you about the\nmeta-learning process. So first, if you condition\non the the latent task",
    "start": "343240",
    "end": "352098"
  },
  {
    "text": "information-- the latent\nshared information theta, then we see that the task\nparameters become independent.",
    "start": "352098",
    "end": "358550"
  },
  {
    "text": "So we know that the\ntask-specific parameters for one task and\nother tasks become independent once you condition\non all of the information",
    "start": "358550",
    "end": "366400"
  },
  {
    "text": "that they share. And second, if you don't\ncondition on this information, then the tasks are not\nstatistically independent.",
    "start": "366400",
    "end": "374825"
  },
  {
    "text": " And hence, if there\nis shared information,",
    "start": "374825",
    "end": "381930"
  },
  {
    "text": "then you actually\nhave lower entropy.  So essentially, when\nyou condition on theta,",
    "start": "381930",
    "end": "388770"
  },
  {
    "text": "that gives you more\ninformation, or hence, lowers your entropy\nover the distribution",
    "start": "388770",
    "end": "393960"
  },
  {
    "text": "for the task-specific\nparameters, phi i. ",
    "start": "393960",
    "end": "399160"
  },
  {
    "text": "So basically, this\nequation is telling you that as you kind of condition\non this shared information,",
    "start": "399160",
    "end": "407640"
  },
  {
    "text": "you now have a\nbetter idea for what the task-specific\nparameters might be. They're telling you\ninformation about",
    "start": "407640",
    "end": "413120"
  },
  {
    "text": "these task-specific parameters.  OK, so this is one of the\nthings that this graphical model",
    "start": "413120",
    "end": "422180"
  },
  {
    "text": "can tell us a bit about or\nhelp us think about what's going on in this\nsort of meta-learning",
    "start": "422180",
    "end": "428780"
  },
  {
    "text": "or multi-task learning setting. Now, one thought exercise,\nif you can identify",
    "start": "428780",
    "end": "437180"
  },
  {
    "text": "the task-specific\nparameter-- sorry, the shared parameters theta-- the shared information theta.",
    "start": "437180",
    "end": "442820"
  },
  {
    "text": "For example, with meta-learning,\nin what situations should learning the\ntask-specific parameters",
    "start": "442820",
    "end": "448969"
  },
  {
    "text": "be faster than\nlearning from scratch?  Does anyone want to share\nthoughts either in chat",
    "start": "448970",
    "end": "456560"
  },
  {
    "text": "or by raising your hand? ",
    "start": "456560",
    "end": "463550"
  },
  {
    "text": "Or any questions about what\nI was talking about here? ",
    "start": "463550",
    "end": "472400"
  },
  {
    "text": "Yeah, I have one question. So knowing phi i, does\nit mean like [INAUDIBLE]",
    "start": "472400",
    "end": "480070"
  },
  {
    "text": "xi test yi test? Yeah, so once you know the\ntask-specific parameters,",
    "start": "480070",
    "end": "485810"
  },
  {
    "text": "then the training set and\nthe test set are independent. And this is because we\nassume that our training set",
    "start": "485810",
    "end": "492290"
  },
  {
    "text": "and our test set\nare sampled IID, they're sampled kind\nof independently from the task distribution.",
    "start": "492290",
    "end": "497910"
  },
  {
    "text": "So once you have the task,\nthen these can be independent. Is it during training, if\nphi i from x to y [INAUDIBLE]",
    "start": "497910",
    "end": "505419"
  },
  {
    "text": "it's not independent, right? And you aren't\ntraining them anymore. Yeah, so at test\ntime, kind of our goal",
    "start": "505420",
    "end": "511100"
  },
  {
    "text": "is to figure out\nwhat is basically-- is to figure out\nthe distribution",
    "start": "511100",
    "end": "517210"
  },
  {
    "text": "of our task-specific parameters\nonce we have D train and theta.",
    "start": "517210",
    "end": "523909"
  },
  {
    "text": "And once you're trying to kind\nof figure this out, it's not--",
    "start": "523909",
    "end": "532470"
  },
  {
    "text": "well, I guess they're not\nindependent, but still D train and D test are independent.",
    "start": "532470",
    "end": "537780"
  },
  {
    "text": "If you don't know what phi i is,\nthen they're not independent. Thank you. ",
    "start": "537780",
    "end": "546460"
  },
  {
    "text": "So getting back to\nthe thought exercise, one thought that\n[MUTED] had in the chat",
    "start": "546460",
    "end": "552360"
  },
  {
    "text": "is if the mutual information\nbetween theta and phi i is high, then this\nshould be faster",
    "start": "552360",
    "end": "560820"
  },
  {
    "text": "than learning from scratch. And so basically, I think\nwhat [MUTED] is saying is that basically if there's\na lot of shared information",
    "start": "560820",
    "end": "566700"
  },
  {
    "text": "or a kind of pretty heavy\ndependence from theta to phi i, then you would\nexpect it to be faster",
    "start": "566700",
    "end": "573780"
  },
  {
    "text": "than learning from scratch. And that's largely true.",
    "start": "573780",
    "end": "579690"
  },
  {
    "text": "Any other thoughts? ",
    "start": "579690",
    "end": "593250"
  },
  {
    "text": "OK, so one other\nthought here also is if you're learning\nfrom scratch,",
    "start": "593250",
    "end": "598260"
  },
  {
    "text": "then you're basically\nlearning from D train i without knowing the\nshared information theta.",
    "start": "598260",
    "end": "606460"
  },
  {
    "text": "And so if all of the\nshared information theta, if that shared latent\ninformation tells you something",
    "start": "606460",
    "end": "613500"
  },
  {
    "text": "about phi i that D\ntrain doesn't tell you, then that's a\nsituation where we'd",
    "start": "613500",
    "end": "619140"
  },
  {
    "text": "expect to be faster than\nlearning from scratch. Because when you're\nlearning from scratch, you only have the training data.",
    "start": "619140",
    "end": "624270"
  },
  {
    "text": "You don't have the\nshared information. So basically, it's when you\nhave high mutual information,",
    "start": "624270",
    "end": "630960"
  },
  {
    "text": "but also when that information\nisn't available in the training data already. ",
    "start": "630960",
    "end": "638130"
  },
  {
    "text": "OK, and then a second\nthought exercise",
    "start": "638130",
    "end": "643500"
  },
  {
    "text": "is what happens when the entropy\nof this distribution right here is 0?",
    "start": "643500",
    "end": "650250"
  },
  {
    "text": "And specifically,\nwhat happens when-- if an entropy\ndistribution is 0, it",
    "start": "650250",
    "end": "655740"
  },
  {
    "text": "means that you can basically\ndeterministically figure out what phi i should be if\nyou know what theta is.",
    "start": "655740",
    "end": "663839"
  },
  {
    "text": "And for all i would\nsuggest that basically, you can determine so\nyou can figure out what phi i is for all i given theta.",
    "start": "663840",
    "end": "672490"
  },
  {
    "text": "Yeah, and then [MUTED]\nsaid in the chat, if this happens, then that means\nthat the data vector contains",
    "start": "672490",
    "end": "678280"
  },
  {
    "text": "all of the information\nneeded to solve the tasks. And kind of the\ncorollary to that",
    "start": "678280",
    "end": "683560"
  },
  {
    "text": "is if it has all\nthe information, then you don't actually need\nto pay attention to D train. And it kind of looks a bit\nlike the memorization problems",
    "start": "683560",
    "end": "690680"
  },
  {
    "text": "that we talked about before. And you can get\nbasically perfect-- yeah, you can get a\nperfect score even",
    "start": "690680",
    "end": "696310"
  },
  {
    "text": "in the zero-shot\nsetting, for example.  OK, so these are a\ncouple of different kind",
    "start": "696310",
    "end": "702890"
  },
  {
    "text": "of ways, exercises\nor things that are nice to look at in\nbasically kind of things",
    "start": "702890",
    "end": "712310"
  },
  {
    "text": "that the Bayesian probabilistic\nmodeling framework can tell you and allows you to reason about\nin a very formal way, which",
    "start": "712310",
    "end": "719090"
  },
  {
    "text": "is pretty nice, and gives some\nintuition for why you would expect meta-learning to\nprovide some benefits compared",
    "start": "719090",
    "end": "725630"
  },
  {
    "text": "to learning from scratch in the\nsituations in which you would expect that to be true. ",
    "start": "725630",
    "end": "733010"
  },
  {
    "start": "732000",
    "end": "1132000"
  },
  {
    "text": "So that's one motivation. And then more concretely,\nwhat information",
    "start": "733010",
    "end": "739150"
  },
  {
    "text": "might we expect theta to\ncontain in different situations? So say, for example,\nour task distribution",
    "start": "739150",
    "end": "745990"
  },
  {
    "text": "is a family of\nsinusoid functions that vary in terms of the\namplitude and the phase.",
    "start": "745990",
    "end": "752740"
  },
  {
    "text": "Any thoughts on what\ninformation would be contained within theta\nfor that family of tasks?",
    "start": "752740",
    "end": "758865"
  },
  {
    "start": "758865",
    "end": "771240"
  },
  {
    "text": "Yeah, so [MUTED]\nsaid that [INAUDIBLE]",
    "start": "771240",
    "end": "776640"
  },
  {
    "text": "what theta would contain the\nunit sinusoid, that it would contain sinusoid functions.",
    "start": "776640",
    "end": "782700"
  },
  {
    "text": "Do you want to give an opinion? Yeah, I guess on that answer,\njust the function itself,",
    "start": "782700",
    "end": "788740"
  },
  {
    "text": "the [INAUDIBLE]\nvector and frequency. Those kind of depend on phi i. Yeah, exactly.",
    "start": "788740",
    "end": "794440"
  },
  {
    "text": "So if different functions\nvary based on their amplitude and phase, then\nyou'd expect theta",
    "start": "794440",
    "end": "799870"
  },
  {
    "text": "to contain everything but that\ntask-specific information. So essentially,\ntheta will contain--",
    "start": "799870",
    "end": "805420"
  },
  {
    "text": "correspond to that family\nof sinusoid functions in a way that is agnostic to\nthe phase and the amplitude.",
    "start": "805420",
    "end": "815810"
  },
  {
    "text": "And then when you go\nkind of from theta to phi i, and also kind\nof infer phi i from data,",
    "start": "815810",
    "end": "824500"
  },
  {
    "text": "it's going to be\nlooking to that data to figure out what the\namplitude and the phase is.",
    "start": "824500",
    "end": "829752"
  },
  {
    "text": "Yeah, so it\nessentially corresponds to the general structure\nof sinusoid functions. ",
    "start": "829752",
    "end": "836480"
  },
  {
    "text": "Cool.  OK, now, what about\none more example?",
    "start": "836480",
    "end": "844269"
  },
  {
    "text": "So say that we were\ntrying to translate between different languages, and\nyou wanted to quickly learn it",
    "start": "844270",
    "end": "850953"
  },
  {
    "text": "to a model that can translate\nbetween a new language with a small amount of data. In this case, what\nis the information",
    "start": "850953",
    "end": "856630"
  },
  {
    "text": "that theta might contain? Feel free to enter in\nchat or raise your hand.",
    "start": "856630",
    "end": "862170"
  },
  {
    "start": "862170",
    "end": "867350"
  },
  {
    "text": "Yeah, so [MUTED] said the\nfamily of language pairs, some of the grammatical\nstructures, and this is right.",
    "start": "867350",
    "end": "874530"
  },
  {
    "text": "So basically, theta\nwill correspond to this family of\nlanguage pairs,",
    "start": "874530",
    "end": "880380"
  },
  {
    "text": "it will correspond to the shared\nstructure among languages. It won't tell you things about\nlike vocabulary, for example,",
    "start": "880380",
    "end": "887987"
  },
  {
    "text": "or things that are specific\nto a certain language. ",
    "start": "887987",
    "end": "893520"
  },
  {
    "text": "OK, and then it's also\nworth mentioning that theta, the kind of information\nstored in theta",
    "start": "893520",
    "end": "899130"
  },
  {
    "text": "is going to be\nnarrower than the space of all possible functions.",
    "start": "899130",
    "end": "904891"
  },
  {
    "text": "If theta was basically the\nspace of all possible functions, then we wouldn't\nhave any benefit",
    "start": "904892",
    "end": "909957"
  },
  {
    "text": "from actually leveraging\nthe information in theta to solve a new task. ",
    "start": "909958",
    "end": "916720"
  },
  {
    "text": "And now one last\nthought exercise is what happens\nif you meta-learn",
    "start": "916720",
    "end": "922449"
  },
  {
    "text": "without a lot of tasks? If you only had a\nsmall number of tasks",
    "start": "922450",
    "end": "927550"
  },
  {
    "text": "and you ran\nmeta-learning on that, how would that affect\nthe information in theta?",
    "start": "927550",
    "end": "933670"
  },
  {
    "text": "So [MUTED] said\noverfitting, which is on the right track,\nwhich is correct. ",
    "start": "933670",
    "end": "940980"
  },
  {
    "text": "Any kind of more\nspecific thoughts on how this affects the\ninformation in theta? ",
    "start": "940980",
    "end": "951860"
  },
  {
    "text": "Yeah, so [MUTED] said that\nif you're overfitting, then theta will be too narrow.",
    "start": "951860",
    "end": "959040"
  },
  {
    "text": "And basically, kind of what\ncan happen in this case is if you have a set of--",
    "start": "959040",
    "end": "967150"
  },
  {
    "text": "a small set of training\ntasks or training functions, then you'll get this\nform of meta-overfitting where it considers\nnot only, for example,",
    "start": "967150",
    "end": "976973"
  },
  {
    "text": "a family of sinusoid\nfunctions, but only the set of sinusoid\nfunctions that are in your meta-training set. It's going to basically\nbe a narrower set of tasks",
    "start": "976973",
    "end": "985628"
  },
  {
    "text": "because it will kind of\noverfit to the tasks that are in the training set rather\nthan considering the underlying",
    "start": "985628",
    "end": "992260"
  },
  {
    "text": "distribution of functions.  OK, any questions on this\nbefore we move on to methods?",
    "start": "992260",
    "end": "1003459"
  },
  {
    "text": "Yeah, regarding the\nmeta overfitting, I guess maybe at this\npoint, or should I",
    "start": "1003460",
    "end": "1008954"
  },
  {
    "text": "say my question\nis, at this point maybe it's related to your use\ncase of the oral meta learning",
    "start": "1008954",
    "end": "1016510"
  },
  {
    "text": "model, right? I mean, in [INAUDIBLE] thinking\nif your use case is generally narrower then that means you\nhave more shared structure",
    "start": "1016510",
    "end": "1025380"
  },
  {
    "text": "information in your\ndifferent meta-tasks and that should increase,\nor at least intuitively",
    "start": "1025380",
    "end": "1031209"
  },
  {
    "text": "the performance of your model. So compared to like you\nhave a very wide use case,",
    "start": "1031210",
    "end": "1037150"
  },
  {
    "text": "and you train in a very\ndiverse set of tasks. Yeah, so if you have a very\nnarrow distribution of tasks,",
    "start": "1037150",
    "end": "1044740"
  },
  {
    "text": "then you may not need as many\nsamples from that distribution because you may be able\nto cover that distribution",
    "start": "1044740",
    "end": "1051250"
  },
  {
    "text": "with a smaller number of tasks. And so if you have a\nsmall number of tasks but your distribution\nis also narrow,",
    "start": "1051250",
    "end": "1057200"
  },
  {
    "text": "then you may not actually\nsee meta overfitting. It will just be kind of fitting\nthat more narrow distribution,",
    "start": "1057200",
    "end": "1063010"
  },
  {
    "text": "whereas if you have a very\nlarge distribution of tasks and a small number of tasks\nsampled from that distribution,",
    "start": "1063010",
    "end": "1069123"
  },
  {
    "text": "then you're much more\nlikely to see meta overfitting in a way that\nisn't actually capturing the full space of tasks.",
    "start": "1069123",
    "end": "1075766"
  },
  {
    "text": "Yeah, thanks.  OK, [MUTED] is asking, can\nwe view meta parameter theta",
    "start": "1075766",
    "end": "1084090"
  },
  {
    "text": "as a good prior from the\nBayesian perspective? And yeah, that's\nbasically right. So you can view theta as\nkind of a prior over phi.",
    "start": "1084090",
    "end": "1095130"
  },
  {
    "text": "And then when you're trying\nto estimate phi given theta and your data,\nyou can view that as like a form of\nposterior inference.",
    "start": "1095130",
    "end": "1104100"
  },
  {
    "text": "And there's also a question\nwhy everything but phase and amplitude. So phase and amplitude\nin this problem",
    "start": "1104100",
    "end": "1110040"
  },
  {
    "text": "are the things that\nare task-specific, and the general\nfamily of tasks is kind of the family of\nsinusoids for different phases",
    "start": "1110040",
    "end": "1118200"
  },
  {
    "text": "and amplitude. And so theta is capturing\nthe shared information, and the phase and the\namplitude are the thing",
    "start": "1118200",
    "end": "1124649"
  },
  {
    "text": "that isn't shared across tasks. ",
    "start": "1124650",
    "end": "1130510"
  },
  {
    "text": "OK, cool. So another last\nbit of motivation,",
    "start": "1130510",
    "end": "1137970"
  },
  {
    "start": "1132000",
    "end": "1447000"
  },
  {
    "text": "recall that kind of so far\nwe've been basically inferring",
    "start": "1137970",
    "end": "1143159"
  },
  {
    "text": "a set of parameters phi i given\na training dataset and theta.",
    "start": "1143160",
    "end": "1150510"
  },
  {
    "text": "And so far, we've just\nbeen learning a function that outputs phi i\ndeterministically.",
    "start": "1150510",
    "end": "1156278"
  },
  {
    "text": "It's essentially giving\nus a point estimate to this distribution, p. ",
    "start": "1156278",
    "end": "1163701"
  },
  {
    "text": "And this is fine in a number\nof different situations. Like in your homework\nthis is fine, but there are some\nsituations where",
    "start": "1163702",
    "end": "1169400"
  },
  {
    "text": "we may want to actually\nexpress this full distribution rather than only giving a\npoint estimate of phi i.",
    "start": "1169400",
    "end": "1178360"
  },
  {
    "text": "For example, if you only\nhave a really small number of data points, then\nthe true function",
    "start": "1178360",
    "end": "1184570"
  },
  {
    "text": "may actually be ambiguous,\neven if you have a prior. So for example,\nmaybe you're trying",
    "start": "1184570",
    "end": "1190960"
  },
  {
    "text": "to learn a classifier between\nthese images and these images,",
    "start": "1190960",
    "end": "1199279"
  },
  {
    "text": "and the challenge is if you\nwanted to learn a classifier, all the images on\nthe left are people that are smiling,\nwearing a hat, and young,",
    "start": "1199280",
    "end": "1206539"
  },
  {
    "text": "and all the people on the right\nare people who are not smiling, not wearing a hat,\nand not young.",
    "start": "1206540",
    "end": "1212032"
  },
  {
    "text": "And so if you want to\nlearn a classifier, and then you're given this\nexample of someone who's smiling and wearing\na hat but not young,",
    "start": "1212032",
    "end": "1219020"
  },
  {
    "text": "it's kind of ambiguous\nwhat the true answer is. You don't know if this\nclassifier is intended to discriminate on\nage or discriminate",
    "start": "1219020",
    "end": "1227180"
  },
  {
    "text": "on these other attributes. So in these sorts of cases\nwhere the problem is ambiguous,",
    "start": "1227180",
    "end": "1234110"
  },
  {
    "text": "it would be nice to be able\nto represent our uncertainty about this classifier,\nand then as a result",
    "start": "1234110",
    "end": "1239929"
  },
  {
    "text": "be able to represent our\nuncertainty about this test image. ",
    "start": "1239930",
    "end": "1248860"
  },
  {
    "text": "OK, another situation\nwhere we may want to be able to\nrepresent this distribution is if we want to be able\nto generate hypotheses",
    "start": "1248860",
    "end": "1255519"
  },
  {
    "text": "about the underlying function. If we want to build a sample\nfrom this distribution,",
    "start": "1255520",
    "end": "1260770"
  },
  {
    "text": "then we need to somehow\nrepresent that distribution. And once we can\ngenerate hypotheses,",
    "start": "1260770",
    "end": "1266799"
  },
  {
    "text": "then we may be\nable to kind of ask a user about which hypothesis\nis correct, for example.",
    "start": "1266800",
    "end": "1272799"
  },
  {
    "text": " And if you're kind of in\na safety-critical setting",
    "start": "1272800",
    "end": "1278210"
  },
  {
    "text": "such as like medical\nimaging, for example, it's important to be able to\ncommunicate your uncertainty",
    "start": "1278210",
    "end": "1283940"
  },
  {
    "text": "about the function to the\nuser so that you can get a second opinion, for example.",
    "start": "1283940",
    "end": "1289529"
  },
  {
    "text": "If you want to actively learn\nby actively asking for data, then this sort of\nuncertainty is really helpful",
    "start": "1289530",
    "end": "1295220"
  },
  {
    "text": "because it tells you\nwhich data points would be helpful for learning.",
    "start": "1295220",
    "end": "1301169"
  },
  {
    "text": "For example, in this kind\nof classification example, if you can get another\nexample of someone who's smiling, wearing\na hat, and young,",
    "start": "1301170",
    "end": "1307310"
  },
  {
    "text": "that's not going\nto be very helpful for a linear classifier. It's instead kind of\nlabels of these data points",
    "start": "1307310",
    "end": "1312980"
  },
  {
    "text": "that would be most helpful\nfor reducing your uncertainty about the function. ",
    "start": "1312980",
    "end": "1319418"
  },
  {
    "text": "It's worth mentioning\nthat there's actually a fair amount of work on active\nlearning with meta-learning. We won't go into it during\nthe lectures in this course,",
    "start": "1319418",
    "end": "1325830"
  },
  {
    "text": "but if you're interested\nin learning more there's a couple of\nreferences on the side. And lastly, this is also\npretty helpful and important",
    "start": "1325830",
    "end": "1333210"
  },
  {
    "text": "for learning to explore in\nmeta-reinforcement learning, and we'll talk about\nthis a bit more in--",
    "start": "1333210",
    "end": "1339442"
  },
  {
    "text": "I think in like a week\nand a half or two weeks. ",
    "start": "1339442",
    "end": "1344560"
  },
  {
    "text": "OK, it looks like there's a\nquestion in the chat saying,",
    "start": "1344560",
    "end": "1349660"
  },
  {
    "text": "is also learning over-- is learning a\ndistribution over phi i providing some sort\nof meta-regularization?",
    "start": "1349660",
    "end": "1358930"
  },
  {
    "text": "Yeah, so one of the benefits\nof Bayesian approaches also is that they often\nprovide some implicit forms",
    "start": "1358930",
    "end": "1365350"
  },
  {
    "text": "of regularization to\nencourage the model to represent its uncertainty\nrather than overfitting to kind",
    "start": "1365350",
    "end": "1371530"
  },
  {
    "text": "of a single solution\nthat it thinks might be the right answer but\nisn't quite the right answer.",
    "start": "1371530",
    "end": "1376900"
  },
  {
    "text": "[MUTED] is also asking,\nis active learning equivalent to exploration in RL?",
    "start": "1376900",
    "end": "1382850"
  },
  {
    "text": "They are a little bit different\nfrom a mechanistic point of view. When you actively\nlearn, typically it",
    "start": "1382850",
    "end": "1388130"
  },
  {
    "text": "corresponds to\nasking for queries-- like querying a user about\nthe label for a specific data",
    "start": "1388130",
    "end": "1394159"
  },
  {
    "text": "point, whereas exploration\nyou have a policy that is trying to collect\ndata in the environment",
    "start": "1394160",
    "end": "1400429"
  },
  {
    "text": "in a targeted way. They both correspond to kind\nof targeted data collection, but active learning is more\noften in a supervised learning",
    "start": "1400430",
    "end": "1407700"
  },
  {
    "text": "context whereas\nexploration is more of a reinforcement learning\ncontext where you have an agent in the environment.",
    "start": "1407700",
    "end": "1414100"
  },
  {
    "text": "[MUTED] is asking,\nisn't active learning too interactive learning?",
    "start": "1414100",
    "end": "1420940"
  },
  {
    "text": "Yeah, I guess active\nis the more formal term that I've heard in the\nmachine learning community,",
    "start": "1420940",
    "end": "1427429"
  },
  {
    "text": "but I think that interactive\nlearning may also be kind of a subfield\nof active learning.",
    "start": "1427430",
    "end": "1434380"
  },
  {
    "start": "1434380",
    "end": "1439390"
  },
  {
    "text": "OK, so now let's talk about\nsome approaches for actually",
    "start": "1439390",
    "end": "1447658"
  },
  {
    "start": "1447000",
    "end": "1946000"
  },
  {
    "text": "developing Bayesian\nmeta-learning approaches that can actually represent\ntheir uncertainty and represent these\ndistributions.",
    "start": "1447658",
    "end": "1454345"
  },
  {
    "text": " So it'll be helpful\nfirst to kind of go",
    "start": "1454345",
    "end": "1460600"
  },
  {
    "text": "back to the computation\ngraph perspective where we can view each of\nthese meta-learning algorithms as producing a prediction over\ny given your training data set",
    "start": "1460600",
    "end": "1469360"
  },
  {
    "text": "and your test input, and really\nthe first v0 of a Bayesian",
    "start": "1469360",
    "end": "1475540"
  },
  {
    "text": "meta-learning\nalgorithm is instead of having f deterministically\noutput put a label,",
    "start": "1475540",
    "end": "1483340"
  },
  {
    "text": "have it output the parameters\nof a distribution over y test,",
    "start": "1483340",
    "end": "1489029"
  },
  {
    "text": "or otherwise output\na distribution. This is actually\ntechnically what kind of a softmax cross\nentropy loss will already do.",
    "start": "1489030",
    "end": "1497130"
  },
  {
    "text": "It's already representing\na softmax distribution over the discrete values. But if you do regression and\nuse a mean squared error loss,",
    "start": "1497130",
    "end": "1505440"
  },
  {
    "text": "you'll only be getting the mean\nof a Gaussian distribution. You won't be getting\nthe variance. ",
    "start": "1505440",
    "end": "1512610"
  },
  {
    "text": "But anyway, you can have\nf output the parameters of a distribution.",
    "start": "1512610",
    "end": "1517880"
  },
  {
    "text": "This can be probability values\nin a discrete categorical distribution. It can be the mean and\nvariance of the Gaussian.",
    "start": "1517880",
    "end": "1523820"
  },
  {
    "text": "It can be the parameters\nof a mixture of Gaussians. Or if you have a\nmultidimensional y test,",
    "start": "1523820",
    "end": "1531080"
  },
  {
    "text": "then you could use an\nautoregressive model to kind of output a\nsequence of distributions where you're outputting\nsomething like p of y1",
    "start": "1531080",
    "end": "1540680"
  },
  {
    "text": "and p of y2 given\ny1, and so forth. ",
    "start": "1540680",
    "end": "1547760"
  },
  {
    "text": "And then once you output\nthis distribution, you can optimize it\nwith maximum likelihood.",
    "start": "1547760",
    "end": "1555440"
  },
  {
    "text": "So this is pretty simple. This will give you kind of a\ndistribution over your label",
    "start": "1555440",
    "end": "1560559"
  },
  {
    "text": "for the test inputs. ",
    "start": "1560560",
    "end": "1566453"
  },
  {
    "text": "It's also easy to combine\nwith a variety of methods. You just need to have\nthe output of your model be a distribution, but it\ndoesn't quite solve the problem",
    "start": "1566453",
    "end": "1575250"
  },
  {
    "text": "that we're after. It doesn't allow you to\nreason about uncertainty about the underlying\nfunction, so this only",
    "start": "1575250",
    "end": "1581550"
  },
  {
    "text": "gives you basically a\ndistribution over y test given",
    "start": "1581550",
    "end": "1587940"
  },
  {
    "text": "x test D train and theta. So it basically gives you this.",
    "start": "1587940",
    "end": "1594930"
  },
  {
    "text": "It doesn't give you a\ndistribution over phi i.",
    "start": "1594930",
    "end": "1602030"
  },
  {
    "text": "It'd be nice to have kind\nof this distribution, because this allows you to think\nabout the underlying function rather than just the\nindividual data points.",
    "start": "1602030",
    "end": "1609910"
  },
  {
    "text": " It's also limited. There's also kind of a\nlimited class of distributions",
    "start": "1609910",
    "end": "1616400"
  },
  {
    "text": "that you can\nrepresent over y test. If you instead represent\na distribution over phi i, then you may be able to get a\nmore expressive distribution",
    "start": "1616400",
    "end": "1624049"
  },
  {
    "text": "over y test.  And lastly, this\nkind of approach",
    "start": "1624050",
    "end": "1629720"
  },
  {
    "text": "actually tends to produce\npoorly calibrated uncertainty estimates.",
    "start": "1629720",
    "end": "1634789"
  },
  {
    "text": "Neural networks tend to\noverfit in some sense.",
    "start": "1634790",
    "end": "1640670"
  },
  {
    "text": "This allows you to represent\nthe uncertainty over noise in the data, but\nit doesn't allow you to represent\nuncertainty with regard",
    "start": "1640670",
    "end": "1647330"
  },
  {
    "text": "to the model parameters itself.  And if, for example, you give\nit an auto distribution input,",
    "start": "1647330",
    "end": "1655310"
  },
  {
    "text": "then this kind of\napproach tends to give you uncertainty estimates\nthat are overconfident.",
    "start": "1655310",
    "end": "1660546"
  },
  {
    "text": " OK, and then [MUTED]\nis asking in the chat,",
    "start": "1660546",
    "end": "1667080"
  },
  {
    "text": "is learning the\ndistribution more difficult? Yes, in general, these Bayesian\nmeta-learning algorithms",
    "start": "1667080",
    "end": "1673680"
  },
  {
    "text": "lead to kind of a more\ndifficult optimization process because you cared not\nabout just producing",
    "start": "1673680",
    "end": "1682002"
  },
  {
    "text": "a single value of your\ntask-specific parameters or a single output of your y. You need to actually represent\nthat full distribution, which",
    "start": "1682002",
    "end": "1689010"
  },
  {
    "text": "leads to a kind of-- which makes it more\nchallenging to optimize.",
    "start": "1689010",
    "end": "1696164"
  },
  {
    "text": "[MUTED] is asking, can you\nexplain the last con again? So to put it\nanother way, there's",
    "start": "1696164",
    "end": "1704350"
  },
  {
    "text": "two types of uncertainty. One type of\nuncertainty is what's called aleatoric uncertainty,\nand this is uncertainty",
    "start": "1704350",
    "end": "1711130"
  },
  {
    "text": "that's inherent to the data-- to the noise in the data.",
    "start": "1711130",
    "end": "1716330"
  },
  {
    "text": "And this kind of approach tends\nto be reasonable at capturing that kind of noise in the data.",
    "start": "1716330",
    "end": "1723680"
  },
  {
    "text": "The other type of uncertainty\nis called epistemic uncertainty,",
    "start": "1723680",
    "end": "1730090"
  },
  {
    "text": "which captures the uncertainty\nin your model itself. And this kind of approach\ntends to actually",
    "start": "1730090",
    "end": "1737740"
  },
  {
    "text": "be quite poor at estimating\nthe uncertainty of the model. And what I mean by uncertainty\nof the model is when you have",
    "start": "1737740",
    "end": "1748290"
  },
  {
    "text": "auto distribution\ndata, for example, or data that's a\nlittle bit different, then your model isn't going\nto perform as well because it",
    "start": "1748290",
    "end": "1755490"
  },
  {
    "text": "hasn't seen that kind of data. And so you want to\nbe able to represent your uncertainty over\nyour model parameters",
    "start": "1755490",
    "end": "1760972"
  },
  {
    "text": "for those data points. And this approach tends to\nbe actually quite ineffective",
    "start": "1760972",
    "end": "1767850"
  },
  {
    "text": "at representing that form\nof model uncertainty. ",
    "start": "1767850",
    "end": "1776090"
  },
  {
    "text": "Yeah, so I think\nthat question is-- [MUTED] is asking, what's the\ndifference between inferring p",
    "start": "1776090",
    "end": "1783687"
  },
  {
    "text": "of y test versus p of phi i? Can't we just infer p of\ny test from p of phi i?",
    "start": "1783687",
    "end": "1790490"
  },
  {
    "text": "That's true. Once we have p of phi\ni, we can kind of sample from this distribution\nat the top,",
    "start": "1790490",
    "end": "1796130"
  },
  {
    "text": "but we can't go the\nother way around. We can't figure out p of\nphi i from p of y test,",
    "start": "1796130",
    "end": "1802640"
  },
  {
    "text": "and this kind of\nleads to this thought exercise right here,\nwhich is that if we",
    "start": "1802640",
    "end": "1808238"
  },
  {
    "text": "can do maximum likelihood\ntraining on this distribution right here, well,\nwhy can't we just do the same kind of\nmaximum likelihood",
    "start": "1808238",
    "end": "1814820"
  },
  {
    "text": "training for this latter\ndistribution here? Any thoughts on why or why not--",
    "start": "1814820",
    "end": "1823860"
  },
  {
    "text": "basically, why or why not we\ncan't do maximum likelihood training on this\nlatter distribution? ",
    "start": "1823860",
    "end": "1832520"
  },
  {
    "text": "Yeah, so [MUTED] said\nthat we don't actually know the actual phi. We don't have labels for\nphi i, and therefore we",
    "start": "1832520",
    "end": "1842500"
  },
  {
    "text": "can't do supervised training\non this distribution because we don't\nhave the labels.",
    "start": "1842500",
    "end": "1848050"
  },
  {
    "text": "If we did have the labels, then\nwe could do this, but we don't. And that's why we can only\nkind of do maximum likelihood",
    "start": "1848050",
    "end": "1853900"
  },
  {
    "text": "training for this\nand not for this without introducing kind\nof extra assumptions",
    "start": "1853900",
    "end": "1859990"
  },
  {
    "text": "or approximations. ",
    "start": "1859990",
    "end": "1865330"
  },
  {
    "text": "OK, [INAUDIBLE] is\nasking, is it required that the label y be\na distribution when",
    "start": "1865330",
    "end": "1872049"
  },
  {
    "text": "meta-training a Bayesian model?  I guess there-- you can\nview point estimates",
    "start": "1872050",
    "end": "1880780"
  },
  {
    "text": "as kind of approximating\na Bayesian distribution. For example, if you train a\nmodel with mean squared error,",
    "start": "1880780",
    "end": "1888040"
  },
  {
    "text": "you can view that as the\noutput from the model as the mean of a Gaussian.",
    "start": "1888040",
    "end": "1895150"
  },
  {
    "text": "But if you want to\nkind of truly represent the distributions underlying\na probabilistic model,",
    "start": "1895150",
    "end": "1900280"
  },
  {
    "text": "then you'll want to represent\nthe full distribution. ",
    "start": "1900280",
    "end": "1908429"
  },
  {
    "text": "And then lastly\n[MUTED] is asking, does a Bayesian\nmeta-learning approach work only with optimization\nbased learning?",
    "start": "1908430",
    "end": "1914490"
  },
  {
    "text": "So this version works with\nany kind learning meta learning algorithm. You just need to have\nyour y test-- you just",
    "start": "1914490",
    "end": "1921832"
  },
  {
    "text": "need a output of\ndistribution over y test rather than a\ndeterministic prediction.",
    "start": "1921832",
    "end": "1928250"
  },
  {
    "text": "In the next part of the lecture,\nwe'll talk about various ways to get a distribution over phi\ni, both for black box methods",
    "start": "1928250",
    "end": "1938030"
  },
  {
    "text": "and for optimization\nbased methods. ",
    "start": "1938030",
    "end": "1943170"
  },
  {
    "text": "OK, great. So before we actually\ndive into algorithms",
    "start": "1943170",
    "end": "1950500"
  },
  {
    "start": "1946000",
    "end": "2347000"
  },
  {
    "text": "for getting a distribution over\nour task-specific parameters,",
    "start": "1950500",
    "end": "1956710"
  },
  {
    "text": "I want to briefly overview\nthe kind of toolbox that we have for\napproaching problems",
    "start": "1956710",
    "end": "1963280"
  },
  {
    "text": "in Bayesian deep learning. This is just a really\nbroad one slide overview",
    "start": "1963280",
    "end": "1969608"
  },
  {
    "text": "that kind of will give a\nsense for the tools that are out there. CS 236 provides a\nthorough treatment of this",
    "start": "1969608",
    "end": "1977067"
  },
  {
    "text": "if you're interested\nin learning more. And of course, we\ndon't have time to go through an\nentire course today.",
    "start": "1977067",
    "end": "1983990"
  },
  {
    "text": "So the general\ngoal of the toolbox is to be able to\nrepresent distributions with neural networks, and there\nare a variety of techniques",
    "start": "1983990",
    "end": "1992740"
  },
  {
    "text": "that allow you to do this. One of the most\npopular techniques, and one of the most\nsuccessful techniques",
    "start": "1992740",
    "end": "1998860"
  },
  {
    "text": "is to use what's called\nlatent variable models along with variational inference.",
    "start": "1998860",
    "end": "2005610"
  },
  {
    "text": "And these models try to\napproximate maximum likelihood training using--",
    "start": "2005610",
    "end": "2011865"
  },
  {
    "text": " try to approximate maximum\nlikelihood training within a latent variable\nmodel, and the way",
    "start": "2011865",
    "end": "2019330"
  },
  {
    "text": "they do this is they optimize a\nlower bound on the likelihood. And we'll actually get\ninto some of the details",
    "start": "2019330",
    "end": "2026230"
  },
  {
    "text": "of this method or this\napproach on the next slide.",
    "start": "2026230",
    "end": "2031462"
  },
  {
    "text": "And what I mean by\nkind of a latent variable model is that you\nhave your observed data, and you have some\nunobserved latent variable.",
    "start": "2031462",
    "end": "2039140"
  },
  {
    "text": "And this will allow\nyou to better represent the distribution\nover the data itself. ",
    "start": "2039140",
    "end": "2046200"
  },
  {
    "text": "The second class of methods is-- or a second class of methods\nis Bayesian ensembles.",
    "start": "2046200",
    "end": "2052040"
  },
  {
    "text": "This gives you a\nparticle-based representation of a distribution that\nbasically gives you samples from a distribution.",
    "start": "2052040",
    "end": "2058550"
  },
  {
    "text": "And the way that it\nworks is you just train separate models\non different bootstraps of the data.",
    "start": "2058550",
    "end": "2063719"
  },
  {
    "text": "So you basically resample\nthe data a few times, train models on different\nsamplings of the data.",
    "start": "2063719",
    "end": "2070020"
  },
  {
    "text": "And then the different\nmodels give you basically different estimates\nor different samples",
    "start": "2070020",
    "end": "2075800"
  },
  {
    "text": "from your distribution. So these are kind of two of the\nmore popular approaches and two",
    "start": "2075800",
    "end": "2083574"
  },
  {
    "text": "of the more approaches that\nwe'll talk about today. ",
    "start": "2083574",
    "end": "2089330"
  },
  {
    "text": "A couple other approaches. One is Bayesian\nneural networks, which represents a distribution\nover the space of parameters.",
    "start": "2089330",
    "end": "2095280"
  },
  {
    "text": "And this is similar to\nthe kind of regularization that we were talking about last\ntime to combat memorization.",
    "start": "2095280",
    "end": "2103430"
  },
  {
    "text": "Lastly, there was\nalso normalizing flows which allows\nyou to represent an invertible function between\na latent distribution and a data",
    "start": "2103430",
    "end": "2110870"
  },
  {
    "text": "distribution and can allow\nyou to essentially warp things from a Gaussian distribution,\nfor example, to a more",
    "start": "2110870",
    "end": "2117350"
  },
  {
    "text": "interesting nonlinear space.  Oh, and then\nlastly, there's also",
    "start": "2117350",
    "end": "2123849"
  },
  {
    "text": "energy based models and GANs. These estimate a normalized\ndensity of your distribution.",
    "start": "2123850",
    "end": "2132730"
  },
  {
    "text": "And the way that\nthey work is they tend to try to\npush down the data",
    "start": "2132730",
    "end": "2140290"
  },
  {
    "text": "and to basically represent\na lower energy for the data and push up everything else.",
    "start": "2140290",
    "end": "2147279"
  },
  {
    "text": "This is kind of akin to\na discriminator that's trying to say,\nlike, data is good, and everything else that's\ngenerated by the generator",
    "start": "2147280",
    "end": "2154150"
  },
  {
    "text": "is not on the data manifold.  So that's kind of a\nsummary of the toolbox.",
    "start": "2154150",
    "end": "2160560"
  },
  {
    "text": "We're really going to be\nfocusing on the first two classes of methods today.",
    "start": "2160560",
    "end": "2166892"
  },
  {
    "text": "And specifically,\nwe'll look at how we can leverage these first\ntwo in the context of meta learning. The others could also be useful\nin developing new methods.",
    "start": "2166892",
    "end": "2173930"
  },
  {
    "text": " Any questions about this\nkind of one slide overview",
    "start": "2173930",
    "end": "2181390"
  },
  {
    "text": "of Bayesian deep learning? ",
    "start": "2181390",
    "end": "2191430"
  },
  {
    "text": "OK, so we're first to look at\nhow variational methods can--",
    "start": "2191430",
    "end": "2201150"
  },
  {
    "text": "variatonal latent\nvariable models can allow us to build a\nBayesian meta learning method.",
    "start": "2201150",
    "end": "2208375"
  },
  {
    "text": "And so I'd like to\ngive some background on how these methods work.",
    "start": "2208375",
    "end": "2214760"
  },
  {
    "text": "And in particular, in\na simplified setting where we have just kind\nof a set of data points x,",
    "start": "2214760",
    "end": "2222570"
  },
  {
    "text": "maybe at the end of\nthose data points,",
    "start": "2222570",
    "end": "2227580"
  },
  {
    "text": "we assume that we have kind\nof this latent variable, z, that underlies our\ndata, and our goal",
    "start": "2227580",
    "end": "2233550"
  },
  {
    "text": "is going to be to represent\na distribution over x. And we want to represent\na expressive distribution",
    "start": "2233550",
    "end": "2239880"
  },
  {
    "text": "because maybe x is\nimages or x is language, and we want to be able to\ncapture something that's",
    "start": "2239880",
    "end": "2245760"
  },
  {
    "text": "more complex than a Gaussian\ndistribution over images, for example. ",
    "start": "2245760",
    "end": "2252260"
  },
  {
    "text": "So x is the observed variable,\nand z is the latent variable.",
    "start": "2252260",
    "end": "2257290"
  },
  {
    "text": "And then essentially,\nwhat variational gives us is the\nevidence lower bound.",
    "start": "2257290",
    "end": "2262630"
  },
  {
    "text": "That basically tells us that\nthe likelihood of our data is lower bounded\nby this expression",
    "start": "2262630",
    "end": "2270340"
  },
  {
    "text": "right here, which is\nbasically the samples from this variational\ndistribution, Q. Well,",
    "start": "2270340",
    "end": "2278607"
  },
  {
    "text": "an exploitation over samples\nfrom this variational distribution of the log\nprobability of the joint",
    "start": "2278607",
    "end": "2285790"
  },
  {
    "text": "plus the entropy of our\nvariational distribution.",
    "start": "2285790",
    "end": "2292073"
  },
  {
    "text": "And perhaps the more\ncommon way to write this is something that\nlooks like the likelihood",
    "start": "2292073",
    "end": "2298540"
  },
  {
    "text": "of the samples decoded from\nour latent space minus the KL",
    "start": "2298540",
    "end": "2304750"
  },
  {
    "text": "divergence between our\noperational distribution and our prior over z.",
    "start": "2304750",
    "end": "2310150"
  },
  {
    "text": " So before I go\nforward, I guess I'm",
    "start": "2310150",
    "end": "2316750"
  },
  {
    "text": "curious for how many\npeople would it be helpful if I actually derived this bound\non a whiteboard like thing.",
    "start": "2316750",
    "end": "2327920"
  },
  {
    "text": "So maybe say yes if that would\nbe helpful and no if you think",
    "start": "2327920",
    "end": "2333986"
  },
  {
    "text": "that would not be helpful. ",
    "start": "2333987",
    "end": "2339360"
  },
  {
    "text": "OK, it looks like from the\nchat and from the yes answers in the participants list\nthat it would be helpful.",
    "start": "2339360",
    "end": "2346238"
  },
  {
    "text": "So let's go through that. Cool. So we want to do maximum\nlikelihood training,",
    "start": "2346238",
    "end": "2351938"
  },
  {
    "start": "2347000",
    "end": "3030000"
  },
  {
    "text": "and specifically we want to\nbe able to maximize the log probability of our data.",
    "start": "2351938",
    "end": "2357020"
  },
  {
    "text": "The probability over data\nis going to be p of x. And we're going to model\np of x, this distribution,",
    "start": "2357020",
    "end": "2364210"
  },
  {
    "text": "with the latent-- by introducing a\nlatent variable. And so we can view this quantity\nas the log of this integral,",
    "start": "2364210",
    "end": "2379400"
  },
  {
    "text": "or we're basically going to\nbe introducing this latent variable z, and then integrating\nit out, and then taking",
    "start": "2379400",
    "end": "2385579"
  },
  {
    "text": "the log. So this is the introduction\nof the latent variable.",
    "start": "2385580",
    "end": "2391810"
  },
  {
    "text": "Now, what this kind of-- what variational\ninference intends to do",
    "start": "2391810",
    "end": "2401390"
  },
  {
    "text": "is to introduce what's called\na variational distribution. ",
    "start": "2401390",
    "end": "2408099"
  },
  {
    "text": "All denoted as q of z.  And the goal of q\nof z is going to aim",
    "start": "2408100",
    "end": "2415440"
  },
  {
    "text": "to try to help us estimate\nthe bound on our likelihood,",
    "start": "2415440",
    "end": "2420599"
  },
  {
    "text": "and q of z, this can\nreally be anything. ",
    "start": "2420600",
    "end": "2431760"
  },
  {
    "text": "You don't want it to be-- it\nhas to be a proper distribution, of course. But as you can see\nin this equation,",
    "start": "2431760",
    "end": "2437730"
  },
  {
    "text": "it doesn't really matter\nexactly what it is. There are some choices of q that\nwill lead to a better estimate.",
    "start": "2437730",
    "end": "2445230"
  },
  {
    "text": "Often in practice, you actually\nhave a variational distribution that is conditioned\non your data point.",
    "start": "2445230",
    "end": "2453624"
  },
  {
    "text": " This will kind of allow you\nto get a better estimate of z",
    "start": "2453624",
    "end": "2459776"
  },
  {
    "text": "for a certain data point, but\nin practice you can actually really choose whatever you\nwant to be on the right hand",
    "start": "2459776",
    "end": "2465470"
  },
  {
    "text": "side of the conditioning bar. OK, so this is still\nan equality so far.",
    "start": "2465470",
    "end": "2473290"
  },
  {
    "text": "And what you can do is you\ncan move this q over here,",
    "start": "2473290",
    "end": "2480360"
  },
  {
    "text": "and that gives you\nan expectation. So this equals the\nlog of the expectation",
    "start": "2480360",
    "end": "2487680"
  },
  {
    "text": "under q of p of x comma\nz divided by q of z dz.",
    "start": "2487680",
    "end": "2501510"
  },
  {
    "text": "So we're still at equality here. ",
    "start": "2501510",
    "end": "2507050"
  },
  {
    "text": "And then-- and this\nis nice because now we no longer have to compute\nthis integral over z.",
    "start": "2507050",
    "end": "2512390"
  },
  {
    "text": "We can essentially\napproximate this now by just drawing\nsamples from q of z.",
    "start": "2512390",
    "end": "2518490"
  },
  {
    "text": "Now, here's where the\nbound is going to come in. So we want to be\nable to estimate this by drawing samples\nfrom q of z, but of course,",
    "start": "2518490",
    "end": "2525800"
  },
  {
    "text": "because we have this log\nhere, that's a bit annoying because the expectation\nover q is on the outside.",
    "start": "2525800",
    "end": "2533297"
  },
  {
    "text": "And so like was\nmentioned in the chat, we can apply Jensen's\ninequality and basically bring",
    "start": "2533297",
    "end": "2540400"
  },
  {
    "text": "the log inside the expectation,\nand I'll actually write this",
    "start": "2540400",
    "end": "2549803"
  },
  {
    "text": "out a little bit more. And we get something that\nlooks like this when we bring the log inside the expectation.",
    "start": "2549803",
    "end": "2556150"
  },
  {
    "text": " Also I realize I\nshouldn't have a dz here. ",
    "start": "2556150",
    "end": "2564440"
  },
  {
    "text": "OK. And then when we write\nthis out in terms",
    "start": "2564440",
    "end": "2571180"
  },
  {
    "text": "of the first expression\nthat was on the slide, we get something that looks\nlike expectation under q",
    "start": "2571180",
    "end": "2578060"
  },
  {
    "text": "of z of log p of x comma\nz plus the entropy of q.",
    "start": "2578060",
    "end": "2587744"
  },
  {
    "text": "And this is basically what\nwas written on the slide. ",
    "start": "2587744",
    "end": "2594049"
  },
  {
    "text": "So the reason why you\nmay want to do this is that this is something\nthat we could optimize.",
    "start": "2594050",
    "end": "2601569"
  },
  {
    "text": "In particular, if,\nfor example, you set q of z given x to be\na Gaussian distribution,",
    "start": "2601570",
    "end": "2608450"
  },
  {
    "text": "then you can represent\nthis in closed form. And this is equal to\nlog of p of x given",
    "start": "2608450",
    "end": "2616615"
  },
  {
    "text": "z time plus log of p of z.",
    "start": "2616615",
    "end": "2624640"
  },
  {
    "text": "We often set p of z to be like\na normal Gaussian distribution,",
    "start": "2624640",
    "end": "2630640"
  },
  {
    "text": "and this is represented\nby a neural network.  So this may have\nparameters theta.",
    "start": "2630640",
    "end": "2638810"
  },
  {
    "text": "Oftentimes you have the\nvariational distribution of parameters phi. And you optimize--\nbasically when",
    "start": "2638810",
    "end": "2646170"
  },
  {
    "text": "you take a maximum\nover this, you also take a max over this\nboth with respect to phi",
    "start": "2646170",
    "end": "2653607"
  },
  {
    "text": "and with respect to theta. ",
    "start": "2653607",
    "end": "2659650"
  },
  {
    "text": "OK, so this is how we\nget this lower bound on the likelihood of our data.",
    "start": "2659650",
    "end": "2664810"
  },
  {
    "text": " And you can see that the\nterm that I just wrote out",
    "start": "2664810",
    "end": "2670540"
  },
  {
    "text": "is the same as what's written\nright here where you basically get the likelihood after\ndrawing samples from q",
    "start": "2670540",
    "end": "2677819"
  },
  {
    "text": "plus the entropy of q. Intuitively, you could\nalso view this as something that's trying to make\nq of z match p of z,",
    "start": "2677820",
    "end": "2688349"
  },
  {
    "text": "and then also maximizing\nthe likelihood of p of x given z when you're\nsampling from q of z.",
    "start": "2688350",
    "end": "2695262"
  },
  {
    "start": "2695262",
    "end": "2701127"
  },
  {
    "text": "A couple more things to mention. So p is the model here. p of x given z is represented\nwith a neural network.",
    "start": "2701127",
    "end": "2707230"
  },
  {
    "text": "p of z is represented with\njust some normal distribution.",
    "start": "2707230",
    "end": "2713255"
  },
  {
    "text": "The reason why you\nrepresent it as this is that the next layer of\np of z can transform it",
    "start": "2713255",
    "end": "2718299"
  },
  {
    "text": "into something that's more\ninteresting than that. So it doesn't actually\nreally matter. This can be really any\nsimple distribution.",
    "start": "2718300",
    "end": "2727560"
  },
  {
    "text": "And then q of z given\nx is often called the inference network or the\nvariational distribution.",
    "start": "2727560",
    "end": "2733110"
  },
  {
    "text": "You can think of it\nas something that is kind of trying to infer\nthe posterior over z given",
    "start": "2733110",
    "end": "2740460"
  },
  {
    "text": "your data point.  And again, really, q can\nbe whatever you want,",
    "start": "2740460",
    "end": "2749010"
  },
  {
    "text": "but it's helpful to optimize\nit with respect to your bound.",
    "start": "2749010",
    "end": "2755430"
  },
  {
    "text": " And basically, if you\nmaximize your bound,",
    "start": "2755430",
    "end": "2761320"
  },
  {
    "text": "then you'll make both respect\nto p and with respect to q, then you'll make it tighter.",
    "start": "2761320",
    "end": "2767770"
  },
  {
    "text": "So that's why you want\nto optimize both q and p.",
    "start": "2767770",
    "end": "2774294"
  },
  {
    "text": "Your model parameter's theta. Your variational\nparameters are phi.",
    "start": "2774294",
    "end": "2779540"
  },
  {
    "text": "Yeah. So any questions on this so far? ",
    "start": "2779540",
    "end": "2793690"
  },
  {
    "text": "OK. Now, one problem\nthat comes up is you need to back propagate\nthrough sampling.",
    "start": "2793690",
    "end": "2801400"
  },
  {
    "text": "So when you note that\nbasically both of these terms",
    "start": "2801400",
    "end": "2807950"
  },
  {
    "text": "depend on q in the\nexpectation, and we actually need to back propagate\ninto the parameters of q",
    "start": "2807950",
    "end": "2814220"
  },
  {
    "text": "when we optimize this. And so that means\nthat we need to--",
    "start": "2814220",
    "end": "2821559"
  },
  {
    "text": "yeah, if we want\nto optimize q, we need to back propagate\nthrough this sampling process",
    "start": "2821560",
    "end": "2826720"
  },
  {
    "text": "and compute the derivative\nof the expectation of q with respect to q. ",
    "start": "2826720",
    "end": "2832720"
  },
  {
    "text": "And so the\nreparameterization trick is kind of what allows\nyou to fix this problem.",
    "start": "2832720",
    "end": "2840520"
  },
  {
    "text": "So if you set q to be a\nGaussian distribution, not only is it easy\nto compute this KL",
    "start": "2840520",
    "end": "2845940"
  },
  {
    "text": "divergence, or\nequivalently this entropy, if you have a\nGaussian q, then you",
    "start": "2845940",
    "end": "2852780"
  },
  {
    "text": "can set q to be equal\nto the mean of q plus the standard\ndeviation times some noise",
    "start": "2852780",
    "end": "2860970"
  },
  {
    "text": "where noise is just drawn\nfrom this standard Gaussian distribution. And note that this function\nis differentiable with respect",
    "start": "2860970",
    "end": "2868539"
  },
  {
    "text": "to the mean and variance. And so you can\nessentially reparamaterize",
    "start": "2868540",
    "end": "2873740"
  },
  {
    "text": "this distribution in terms\nof the standard normal distribution and\nget something that is differentiable with respect\nto the mean and the variance,",
    "start": "2873740",
    "end": "2881120"
  },
  {
    "text": "and basically directly back\npropagate into the mean and variance of the\ndistribution, which",
    "start": "2881120",
    "end": "2887120"
  },
  {
    "text": "is pretty cool.  OK, there are a couple\nof questions in the chat.",
    "start": "2887120",
    "end": "2895256"
  },
  {
    "text": "So one question is, how will we\nleverage the variational model to do supervised learning tasks?",
    "start": "2895257",
    "end": "2901290"
  },
  {
    "text": "I'll get into this in a\nsecond, but the sneak peek into what it will look like\nis you can think of the latent",
    "start": "2901290",
    "end": "2907829"
  },
  {
    "text": "variable-- well, I guess there's\ndifferent ways to view it. But you can think, in\na meta learning model,",
    "start": "2907830",
    "end": "2913610"
  },
  {
    "text": "phi i becomes a latent variable,\nand your data becomes--",
    "start": "2913610",
    "end": "2920840"
  },
  {
    "text": "like D train becomes\nyour observed data. And so this framework will\nallow us to do inference",
    "start": "2920840",
    "end": "2928350"
  },
  {
    "text": "over our latent variable. [MUTED] is asking,\ncan you explain the difference between\nmodel parameters",
    "start": "2928350",
    "end": "2934820"
  },
  {
    "text": "and variational parameters? So the model parameters, I guess\nthere's a few different ways",
    "start": "2934820",
    "end": "2941540"
  },
  {
    "text": "to view it. One is kind of mechanistic\nand one is more theoretical.",
    "start": "2941540",
    "end": "2946980"
  },
  {
    "text": "But I guess from a\nmechanistic standpoint, you can view the\nmodel parameters as representing basically our\nmodel, basically modeling p",
    "start": "2946980",
    "end": "2959776"
  },
  {
    "text": "of z and p of x given z. And we want to sample\nfrom this model. We only use the parameters\nof our model here.",
    "start": "2959777",
    "end": "2969190"
  },
  {
    "text": "The variational parameters are\nwhat allow us to kind of better compute this lower\nbound on our likelihood,",
    "start": "2969190",
    "end": "2977140"
  },
  {
    "text": "and they're used\nduring training. But they aren't at all\nused during testing if you want to sample from this model.",
    "start": "2977140",
    "end": "2982960"
  },
  {
    "text": " So that's kind of what they\nare from a mechanistic point",
    "start": "2982960",
    "end": "2988750"
  },
  {
    "text": "of view and I guess also from\na conceptual point of view. ",
    "start": "2988750",
    "end": "3004192"
  },
  {
    "text": "Oh, and then lastly,\nthis is called amortized variational inference\nbecause this inference",
    "start": "3004192",
    "end": "3011520"
  },
  {
    "text": "where q is doing what's\ncalled amortized inference. Because instead of\ntrying to optimize over z for each example,\nyou're just trying",
    "start": "3011520",
    "end": "3019140"
  },
  {
    "text": "to estimate it by training a\nmodel that kind of estimates it for you. ",
    "start": "3019140",
    "end": "3026230"
  },
  {
    "text": "So how might we do meta learning\nleveraging these techniques? So we can consider\na setting where",
    "start": "3026230",
    "end": "3035329"
  },
  {
    "text": "we have a black box\nneural network that takes as input a\ntrained data set and outputs a distribution\nover q of phi.",
    "start": "3035330",
    "end": "3043609"
  },
  {
    "text": "And then we use phi\nto make predictions. Now, in the standard VAE, we\ncan view the observed variables",
    "start": "3043610",
    "end": "3050000"
  },
  {
    "text": "x, the latent variable z, and\nwe can derive a lower bound.",
    "start": "3050000",
    "end": "3055310"
  },
  {
    "text": "Now, in the meta\nlearning setting, we now have an observed\nvariable that's our data",
    "start": "3055310",
    "end": "3060550"
  },
  {
    "text": "and the latent variable\nwhich is our task parameters. It's kind of an\nanalogy that you can",
    "start": "3060550",
    "end": "3065620"
  },
  {
    "text": "make to the standard\nvariational autoencoder. And you can also use this\nto derive a lower bound",
    "start": "3065620",
    "end": "3073990"
  },
  {
    "text": "on the likelihood of your data\nthat looks something like this. So basically just analogous\nto the lower bound of the VAE",
    "start": "3073990",
    "end": "3082970"
  },
  {
    "text": "where you have some kind of\ninference network over phi. You maximize the likelihood\nof your data given phi,",
    "start": "3082970",
    "end": "3092570"
  },
  {
    "text": "and then also minimize\nthe KL divergence between your interest\nnetwork and your prior",
    "start": "3092570",
    "end": "3098210"
  },
  {
    "text": "over your parameters.  What should q condition on?",
    "start": "3098210",
    "end": "3104430"
  },
  {
    "text": "So in the v setting,\nq conditioned on x. In this setting, we're\ngoing to be conditioning",
    "start": "3104430",
    "end": "3109920"
  },
  {
    "text": "q on our training data. So we're basically\ngoing to be doing kind of approximate\ninference to infer",
    "start": "3109920",
    "end": "3118200"
  },
  {
    "text": "our task-specific parameters\ngiven our training data. This inference network\nis going to look a lot like the black\nbox meta-learner.",
    "start": "3118200",
    "end": "3125801"
  },
  {
    "text": " This equation just kind of\nreplaces the probability",
    "start": "3125801",
    "end": "3131422"
  },
  {
    "text": "of the data given phi is just\nthe probability of the labels given x test and phi.",
    "start": "3131422",
    "end": "3136579"
  },
  {
    "text": "And then lastly, where do\nthe meta parameters come in? The meta parameters will\nbe part of the model.",
    "start": "3136580",
    "end": "3144740"
  },
  {
    "text": "So whenever we try\nto estimate phi, we'll condition that\nprediction on theta,",
    "start": "3144740",
    "end": "3149990"
  },
  {
    "text": "on our meta parameters. So here, again,\nis the parameters",
    "start": "3149990",
    "end": "3160580"
  },
  {
    "text": "of this neural network. So this is like f\ndata, for example. ",
    "start": "3160580",
    "end": "3169300"
  },
  {
    "text": "OK, do you have a question? Is there after that that\n[INAUDIBLE] deviation? ",
    "start": "3169300",
    "end": "3176630"
  },
  {
    "text": "So yes. Well, so the encoder--",
    "start": "3176630",
    "end": "3184630"
  },
  {
    "text": "I guess this is\nequivalent to the encoder or the inference\nnetwork in a VAE.",
    "start": "3184630",
    "end": "3191950"
  },
  {
    "text": "That's basically you can view\nas analogous to the f data right here. OK. Basically it's [INAUDIBLE]\nbecause [INAUDIBLE]",
    "start": "3191950",
    "end": "3199120"
  },
  {
    "text": "of the d train [INAUDIBLE] then\nwe are [INAUDIBLE] which is a z [INAUDIBLE].",
    "start": "3199120",
    "end": "3205700"
  },
  {
    "text": "Yeah, so it's basically this\ninference network right here. OK. ",
    "start": "3205700",
    "end": "3213744"
  },
  {
    "text": "You could also\ncondition on theta here. This is kind of analogous to--",
    "start": "3213744",
    "end": "3219809"
  },
  {
    "text": "so like a model that doesn't\noutput all the parameters. It just outputs a subset\nof the parameters.",
    "start": "3219810",
    "end": "3226380"
  },
  {
    "text": "This just changes\nyour graphical model a little bit if you\ncondition on theta here such that you\nget something that looks more like theta phi i.",
    "start": "3226380",
    "end": "3234540"
  },
  {
    "text": "And then when you make\npredictions for y, you get something that looks\nalso like this, for example,",
    "start": "3234540",
    "end": "3241214"
  },
  {
    "text": "in your graphical model. So here it is like\nwe are getting",
    "start": "3241215",
    "end": "3246240"
  },
  {
    "text": "input theta and d train,\nand they're inside phi and then it goes from there.",
    "start": "3246240",
    "end": "3252720"
  },
  {
    "text": "Yeah. So you basically have\nyour inference network that infers the task-specific\nparameters given your data",
    "start": "3252720",
    "end": "3257809"
  },
  {
    "text": "and your meta parameters. You maximize the likelihood\nof the kind of labels under",
    "start": "3257810",
    "end": "3264079"
  },
  {
    "text": "your task-specific parameters,\nand then you additionally have this tail term that\nencourages this--",
    "start": "3264080",
    "end": "3270050"
  },
  {
    "text": "basically encourages\nthis inference network to produce a proper\ndistribution over phi.",
    "start": "3270050",
    "end": "3277290"
  },
  {
    "text": "If the theta is [INAUDIBLE]\ndistribution like [INAUDIBLE],, we are also learning\ntheta [INAUDIBLE]",
    "start": "3277290",
    "end": "3283570"
  },
  {
    "text": "the new theta to start with. Yeah, you're also-- theta\nis being optimized here.",
    "start": "3283570",
    "end": "3288599"
  },
  {
    "start": "3288600",
    "end": "3295340"
  },
  {
    "text": "OK, so the final objective\nis here for completeness. It's asking how many samples\ndo you use for training?",
    "start": "3295340",
    "end": "3301670"
  },
  {
    "text": "In a VAE and in\nthis setting, it's actually often fine in\npractice to just sample once",
    "start": "3301670",
    "end": "3308204"
  },
  {
    "text": "from this distribution,\nalthough you'll get kind of a tighter\nestimate of your likelihood if you sample multiple times.",
    "start": "3308205",
    "end": "3314330"
  },
  {
    "text": " OK, so this is one way to\nproduce a Bayesian meta",
    "start": "3314330",
    "end": "3324920"
  },
  {
    "text": "learning algorithm\nthat allows you to get a distribution over\nyour task-specific parameters",
    "start": "3324920",
    "end": "3331160"
  },
  {
    "start": "3326000",
    "end": "3599000"
  },
  {
    "text": "given your training data.  And this allows you to get\nnon-Gaussian distributions",
    "start": "3331160",
    "end": "3337620"
  },
  {
    "text": "over y test. You don't actually\nhave to-- you're never actually explicitly representing\nthe kind of distribution",
    "start": "3337620",
    "end": "3344520"
  },
  {
    "text": "of y test given x test,\nd train, and theta.",
    "start": "3344520",
    "end": "3351280"
  },
  {
    "text": "Instead, what you're\ndoing is you're first representing p of phi, and then\nyou're representing p of y test",
    "start": "3351280",
    "end": "3359770"
  },
  {
    "text": "given x test and phi. And because you don't actually\nhave to explicitly represent",
    "start": "3359770",
    "end": "3365780"
  },
  {
    "text": "this, that makes it\neasier to represent more complex distributions.",
    "start": "3365780",
    "end": "3372680"
  },
  {
    "text": "It also produces a\ndistribution over functions. It allows you to\nproduce a distribution over kind of task-specific\nparameters phi.",
    "start": "3372680",
    "end": "3381260"
  },
  {
    "text": "The downside is\nthat you can only represent Gaussian\ndistributions over p of phi i.",
    "start": "3381260",
    "end": "3388070"
  },
  {
    "text": "This is important so that\nyou can calculate the KL, and so that you can use the\nreparameterization trick.",
    "start": "3388070",
    "end": "3393440"
  },
  {
    "text": " This is kind of more of a-- it limits the expressivity\nof this distribution.",
    "start": "3393440",
    "end": "3401480"
  },
  {
    "text": "In practice, Gaussian\ndistributions over parameters are fairly effective for\nrepresenting distributions",
    "start": "3401480",
    "end": "3409790"
  },
  {
    "text": "because your parameters will be\ntransformed in a nonlinear way to produce predictions.",
    "start": "3409790",
    "end": "3415845"
  },
  {
    "text": " OK, so that's kind of a way\nto use variational inference",
    "start": "3415845",
    "end": "3424190"
  },
  {
    "text": "to develop a Bayesian black\nbox meta learning techniques. [MUTED] was asking, we can\ndo a mixture of Gaussians",
    "start": "3424190",
    "end": "3430580"
  },
  {
    "text": "too, right? There is a way to\napproximate the KL, but it's difficult to do\nthe reparameterization trick",
    "start": "3430580",
    "end": "3436670"
  },
  {
    "text": "with a mixture of Gaussians. There are some methods that\ntry to approximate it, though.",
    "start": "3436670",
    "end": "3443560"
  },
  {
    "text": " And then [MUTED] is\nasking, for non-Gaussian, can we not estimate KL\nusing Monte Carlo samples?",
    "start": "3443560",
    "end": "3451050"
  },
  {
    "text": "So you can estimate the KL. It's more difficult\nto back propagate through that sampling process,\nwhich essentially the--",
    "start": "3451050",
    "end": "3460777"
  },
  {
    "text": " basically, like this\nreparameterization trick",
    "start": "3460777",
    "end": "3466330"
  },
  {
    "text": "right here, this is really\neasy to do for Gaussians. It's a lot harder to\ndo for non-Gaussians. But if you're\ninterested in this,",
    "start": "3466330",
    "end": "3472690"
  },
  {
    "text": "there are methods that try\nto extend variational auto encoders to other distributions\nover your latent variable.",
    "start": "3472690",
    "end": "3478060"
  },
  {
    "text": " But really, the de\nfacto thing to do is to use a Gaussian\ndistribution.",
    "start": "3478060",
    "end": "3487070"
  },
  {
    "text": "OK, so that was the\nblack box method that was kind of\nusing a neural network to a distribution over phi i.",
    "start": "3487070",
    "end": "3492980"
  },
  {
    "text": "What about some\noptimization based methods? So the first thing\nworth mentioning",
    "start": "3492980",
    "end": "3498210"
  },
  {
    "text": "is this paper here\nthat cast actually",
    "start": "3498210",
    "end": "3505980"
  },
  {
    "text": "the kind of original\ndeterministic MAML algorithm as a Bayesian method\nvery approximately.",
    "start": "3505980",
    "end": "3514920"
  },
  {
    "text": "And the way that this\nworks is basically the same graphical\nmodel as before. It's just written a\nlittle bit differently",
    "start": "3514920",
    "end": "3521130"
  },
  {
    "text": "where we're not writing out\nthe train and the test set separately. And if we're trying\nto maximize--",
    "start": "3521130",
    "end": "3529750"
  },
  {
    "text": "if we want to kind of do maximum\nlikelihood in this model, we're maximizing the likelihood\nof the data given our meta",
    "start": "3529750",
    "end": "3536160"
  },
  {
    "text": "parameters, we can expand this\nout by introducing the latent variable phi i in here and\nthen integrate out phi i.",
    "start": "3536160",
    "end": "3546030"
  },
  {
    "text": "And what this paper\ndid is they said that, well, what if we very crudely\napproximate this integral",
    "start": "3546030",
    "end": "3552119"
  },
  {
    "text": "by the map estimate-- basically, the value of\nphi i that has the highest",
    "start": "3552120",
    "end": "3558630"
  },
  {
    "text": "likelihood? This is a very crude\napproximation to the integral, but if you have a very\npeaky distribution",
    "start": "3558630",
    "end": "3565050"
  },
  {
    "text": "it may be not that crude. But in any case, it's a\nfairly crude approximation.",
    "start": "3565050",
    "end": "3572589"
  },
  {
    "text": "But under this\ncrude approximation,",
    "start": "3572590",
    "end": "3578160"
  },
  {
    "text": "you can essentially show\nthat MAML corresponds to doing inference in this\ngraphical model because",
    "start": "3578160",
    "end": "3583890"
  },
  {
    "text": "of this prior result here. This prior result said\nthat gradient descent with early stopping\ncorresponds to map inference",
    "start": "3583890",
    "end": "3593400"
  },
  {
    "text": "under a Gaussian prior with the\nmean at the initial parameters,",
    "start": "3593400",
    "end": "3598760"
  },
  {
    "text": "and this was exact\nin the linear case and approximate in\nthe non-linear case.",
    "start": "3598760",
    "end": "3604910"
  },
  {
    "text": "So essentially, by making\nthis crude approximation and by also making\nthis approximation here in the nonlinear case,\nthis essentially provides",
    "start": "3604910",
    "end": "3613610"
  },
  {
    "text": "a Bayesian\ninterpretation of MAML where you can view MAML as\nproviding this map estimate,",
    "start": "3613610",
    "end": "3618890"
  },
  {
    "text": "using that map estimate\nto estimate this integral, and therefore, kind of\nshowing the correspondence",
    "start": "3618890",
    "end": "3627350"
  },
  {
    "text": "between the MAML algorithm\nand this graphical model here.",
    "start": "3627350",
    "end": "3633010"
  },
  {
    "text": "Of course, this is all\nkind of very crude, and also under\nthis interpretation",
    "start": "3633010",
    "end": "3638170"
  },
  {
    "text": "we still can't sample from\nthis distribution here. So it's not very useful\nfrom the Bayesian",
    "start": "3638170",
    "end": "3644080"
  },
  {
    "text": "from the kind of standpoint\nof representing distributions.",
    "start": "3644080",
    "end": "3650000"
  },
  {
    "text": "So there's a few\ndifferent things that you can do about this. One thing you could do is recall\nthe black box meta-learning",
    "start": "3650000",
    "end": "3658730"
  },
  {
    "text": "approach that we just\ntalked about, and remember that this kind of\ninference network,",
    "start": "3658730",
    "end": "3665190"
  },
  {
    "text": "this variational\ndistribution, can really be any arbitrary function.",
    "start": "3665190",
    "end": "3670765"
  },
  {
    "text": "So from this standpoint,\nyou could just put a gradient operator\ninside q and then",
    "start": "3670765",
    "end": "3678119"
  },
  {
    "text": "kind of have it correspond\nto an optimization. And so this course,\nthis is kind of what",
    "start": "3678120",
    "end": "3683490"
  },
  {
    "text": "this paper by Ravi and Beatson\ndid where they were-- where q basically corresponded\nto running SGD on the mean",
    "start": "3683490",
    "end": "3689310"
  },
  {
    "text": "and variance of\nneural network weights with respect to\nthe training data. ",
    "start": "3689310",
    "end": "3695770"
  },
  {
    "text": "And basically uses\nall the machinery-- they still optimize\nthis objective. They just change the form of q.",
    "start": "3695770",
    "end": "3701500"
  },
  {
    "text": "And the benefit of this\nis that it does give you a way to run gradient\ndescent at test time while still giving\nyou a distribution.",
    "start": "3701500",
    "end": "3708490"
  },
  {
    "text": "Again here, phi\ngiven theta is going to be modeled as Gaussian,\nwhich may have its downsides.",
    "start": "3708490",
    "end": "3717447"
  },
  {
    "text": " OK, so this is the first\nway that we could kind of",
    "start": "3717447",
    "end": "3726490"
  },
  {
    "text": "introduce a Bayesian\noptimization based meta-learning algorithm.",
    "start": "3726490",
    "end": "3731619"
  },
  {
    "text": "What about a\nnon-Gaussian posterior? So the simplest thing\nthat we could do",
    "start": "3731620",
    "end": "3740820"
  },
  {
    "text": "is to just do ensembles-- basically, the\ntechnique where you train multiple different\nneural networks on different",
    "start": "3740820",
    "end": "3747840"
  },
  {
    "text": "subsamples of the data. And so really, the simplest\nthing that you could do",
    "start": "3747840",
    "end": "3753330"
  },
  {
    "text": "is to train an ensemble\nof MAMLs where you just train M independent\nMAML models, and you get",
    "start": "3753330",
    "end": "3760378"
  },
  {
    "text": "something that looks like this.  And note that you can\nalso use ensembles",
    "start": "3760378",
    "end": "3766140"
  },
  {
    "text": "with black box and\nnon-parametric methods, as well. ",
    "start": "3766140",
    "end": "3772740"
  },
  {
    "text": "This actually works pretty\nwell, but it won't work well if your ensemble members are\ntoo similar to one another.",
    "start": "3772740",
    "end": "3779710"
  },
  {
    "text": "And this paper by\nKim et al proposed to use what's called\nStein variational gradient",
    "start": "3779710",
    "end": "3786270"
  },
  {
    "text": "to actually optimize for a\nmore diverse set of ensemble elements, which\nbasically just tries",
    "start": "3786270",
    "end": "3791910"
  },
  {
    "text": "to push different ensemble\nparticles away from one another to get a more diverse\nensemble of MAMLs,",
    "start": "3791910",
    "end": "3800660"
  },
  {
    "text": "and as a result, optimize\nfor distribution of particles that produce a high likelihood. ",
    "start": "3800660",
    "end": "3809263"
  },
  {
    "text": "The benefit of this\nis that it's simple, and it tends to work well. It also gives you a non-Gaussian\ndistribution, which is nice.",
    "start": "3809263",
    "end": "3816250"
  },
  {
    "text": "The downside of this\nis that it tends to be pretty expensive, because\nyou have to train complete M different model instances, which\nmay be a lot of computation",
    "start": "3816250",
    "end": "3824910"
  },
  {
    "text": "and memory, which is actually\nexactly what was brought up",
    "start": "3824910",
    "end": "3829970"
  },
  {
    "text": "in the chat. You could just kind\nof do this just on the last layer only, which\nwill be a little bit cheaper,",
    "start": "3829970",
    "end": "3839190"
  },
  {
    "text": "but then you only\nget a distribution over that last layer which may\nbe a little bit less expensive.",
    "start": "3839190",
    "end": "3844530"
  },
  {
    "text": "There's a question in\nchat about how large is M? I can't remember exactly how\nmany they used in this paper.",
    "start": "3844530",
    "end": "3851190"
  },
  {
    "text": "It's common to use\nsomething like 3 to 5 so that your computation\ndoesn't blow up too much.",
    "start": "3851190",
    "end": "3859880"
  },
  {
    "text": "Do you have a question? Yeah. I meant to ask like\nfor these MAMLs,",
    "start": "3859880",
    "end": "3865440"
  },
  {
    "text": "like different MAML\nmodels, so does it matter if these\nmodels are trained on the same data or\ndifferent subsamples of data",
    "start": "3865440",
    "end": "3872759"
  },
  {
    "text": "and bootstrap them up? Yeah, this is a good question. So if you use different\nbootstraps of your data,",
    "start": "3872760",
    "end": "3880690"
  },
  {
    "text": "then it has a proper\ninterpretation as representing a distribution. In practice, people often\nignore that and just train",
    "start": "3880690",
    "end": "3890280"
  },
  {
    "text": "different models with a\ndifferent initialization and a different order\nof mini batches.",
    "start": "3890280",
    "end": "3895680"
  },
  {
    "text": "And that often leads to\nensembles that work just as well as using different\nsamples of your data.",
    "start": "3895680",
    "end": "3902430"
  },
  {
    "text": " Yeah, so in practice, you can\noften skip the subsampling part",
    "start": "3902430",
    "end": "3908010"
  },
  {
    "text": "and just change\nthe initialization and change the order\nof the mini batches.",
    "start": "3908010",
    "end": "3914100"
  },
  {
    "text": "If you want something\nwith a theoretical kind",
    "start": "3914100",
    "end": "3919270"
  },
  {
    "text": "of backing behind it, then\nusing different bootstraps is the more correct thing to do. ",
    "start": "3919270",
    "end": "3930380"
  },
  {
    "text": "OK. Then lastly, I want\nto talk briefly about",
    "start": "3930380",
    "end": "3935480"
  },
  {
    "text": "whether we can model\nnon-Gaussian posterior over all parameters.",
    "start": "3935480",
    "end": "3941460"
  },
  {
    "text": "And we want to think about\nsampling parameter vectors with a procedure\nlike Monte Carlo.",
    "start": "3941460",
    "end": "3946710"
  },
  {
    "text": " One more question, actually,\nfrom the last slide.",
    "start": "3946710",
    "end": "3952119"
  },
  {
    "text": "Are they trained independently,\nor each iteration average loss from each model is\nused to update the weight?",
    "start": "3952120",
    "end": "3957240"
  },
  {
    "text": "They tend to be\ntrained independently. ",
    "start": "3957240",
    "end": "3964150"
  },
  {
    "text": "OK, so this last method\nthat we'll talk about,",
    "start": "3964150",
    "end": "3970395"
  },
  {
    "text": "the intuition\nbehind is that it's trying to learn a prior\nover our parameters where a kick in any\ndirection can put us",
    "start": "3970395",
    "end": "3978430"
  },
  {
    "text": "into different modes\nof that function. Essentially, something\nwhere we're like maybe this",
    "start": "3978430",
    "end": "3984670"
  },
  {
    "text": "is one mode of the\nfunction that we want to learn where we\nhave a classifier that's along the lines of--",
    "start": "3984670",
    "end": "3990130"
  },
  {
    "text": "that discriminates on\nthe means of smiling and young, one classifier\nthat discriminates on smiling and wearing a hat.",
    "start": "3990130",
    "end": "3996430"
  },
  {
    "text": "And we want a prior\nhere such that if you add a little bit of noise\nfrom this starting point,",
    "start": "3996430",
    "end": "4002700"
  },
  {
    "text": "you end up in one\nof these two modes. And then if you run\ngradient descent, you'll end up at\na kind of optimum",
    "start": "4002700",
    "end": "4010589"
  },
  {
    "text": "for each of those two modes. I just put down two\nmodes here because it's",
    "start": "4010590",
    "end": "4015760"
  },
  {
    "text": "easiest to visualize\nthat, but in practice you may have more than two modes. ",
    "start": "4015760",
    "end": "4022780"
  },
  {
    "text": "We're a bit low on time, so I\nmight go through this quickly.",
    "start": "4022780",
    "end": "4027880"
  },
  {
    "text": " The goal is we're going\nto be representing both the distribution over theta and\na distribution over phi given",
    "start": "4027880",
    "end": "4035840"
  },
  {
    "text": "theta, and our goal is to be\nable to sample from phi given r data.",
    "start": "4035840",
    "end": "4043190"
  },
  {
    "text": "This is independent of\nx tests, so you can-- once you condition on\nx train and y train,",
    "start": "4043190",
    "end": "4052350"
  },
  {
    "text": "you basically get\nthis independence. This is-- if we want to be\nable to approximate this,",
    "start": "4052350",
    "end": "4059310"
  },
  {
    "text": "you get this kind of\npretty intractable integral over all possible parameters\ntheta, which is not something",
    "start": "4059310",
    "end": "4066660"
  },
  {
    "text": "that we want to\ntry to approximate. And kind of the intuition\nbehind this approach",
    "start": "4066660",
    "end": "4071790"
  },
  {
    "text": "is that if we knew\np of phi given theta x train and y train for a\nsingle sample from p of theta,",
    "start": "4071790",
    "end": "4080290"
  },
  {
    "text": "then things get a lot easier. So if we knew what this was,\nthen we can just use ancestral",
    "start": "4080290",
    "end": "4085690"
  },
  {
    "text": "sampling to first\nsample from p of theta and then sample from\nthis distribution.",
    "start": "4085690",
    "end": "4094920"
  },
  {
    "text": "So the way that this works\nis we can basically, like",
    "start": "4094920",
    "end": "4100009"
  },
  {
    "text": "the Bayesian interpretation\nof MAML, very crudely approximate this\nas the map estimate",
    "start": "4100010",
    "end": "4106100"
  },
  {
    "text": "of phi, which is similar to\nrunning gradient descent,",
    "start": "4106100",
    "end": "4112630"
  },
  {
    "text": "and then do training\non the full thing with amortized\nvariational inference.",
    "start": "4112630",
    "end": "4117790"
  },
  {
    "text": "So I guess the most\nimportant thing, though, is what happens at test time.",
    "start": "4117790",
    "end": "4123100"
  },
  {
    "text": "Meta-training gives you a\ndistribution over theta, and what this corresponds\nto is basically",
    "start": "4123100",
    "end": "4132068"
  },
  {
    "text": "taking the mean of\nthis distribution, adding sampling from theta which\ncorresponds to adding noise",
    "start": "4132069",
    "end": "4140109"
  },
  {
    "text": "to the mean in correspondence\nto the variance, and then running\ngradient descent which",
    "start": "4140109",
    "end": "4147130"
  },
  {
    "text": "approximately corresponds\nto sampling from p of phi i.",
    "start": "4147130",
    "end": "4153318"
  },
  {
    "text": "And this is something\nthat gives you a kind of non-Gaussian\nposterior over phi.",
    "start": "4153319",
    "end": "4158869"
  },
  {
    "text": "It's also pretty\nsimple at test time and doesn't have to be trained\nwith an ensemble, for example.",
    "start": "4158870",
    "end": "4167127"
  },
  {
    "text": "The downside is that it\ninvolves a more complex training procedure that I didn't have\ntime to go through in detail.",
    "start": "4167127",
    "end": "4173200"
  },
  {
    "text": " OK, so to summarize\nthe methods, v0",
    "start": "4173200",
    "end": "4178714"
  },
  {
    "text": "is just outputting\na distribution over y test, which is simple but\ncan't reason about uncertainty",
    "start": "4178715",
    "end": "4184409"
  },
  {
    "text": "over the underlying function. We then talked about\nblack box approaches that can use\nvariational inference",
    "start": "4184410",
    "end": "4189839"
  },
  {
    "text": "to represent a distribution\nover a latent variable. This can represent\nnon-Gaussian distributions,",
    "start": "4189840",
    "end": "4195940"
  },
  {
    "text": "but it can only represent\nGaussian distributions over phi i. Then we also talked about\noptimization-based approaches",
    "start": "4195940",
    "end": "4202500"
  },
  {
    "text": "where you could use\namortized inference ensembles or a hybrid\ninference procedure,",
    "start": "4202500",
    "end": "4207870"
  },
  {
    "text": "and they each have\ntheir own pros and cons.",
    "start": "4207870",
    "end": "4213230"
  },
  {
    "text": "Where the first two\nare pretty simple but have kind of various\ndownsides such as being",
    "start": "4213230",
    "end": "4221120"
  },
  {
    "text": "a Gaussian posterior\nover phi or having to maintain M model instances.",
    "start": "4221120",
    "end": "4226940"
  },
  {
    "text": "The last one is more\ncomplex, but it does give you a non-Gaussian posterior. ",
    "start": "4226940",
    "end": "4234190"
  },
  {
    "text": "OK, so this is kind of a\nsummary of the methods. ",
    "start": "4234190",
    "end": "4240995"
  },
  {
    "text": "One thing that I'd like\nto go over at the end and wanted to make\nsure I have time for is to talk about what you can\ndo when you have these Bayesian",
    "start": "4240995",
    "end": "4247303"
  },
  {
    "text": "meta-learning methods,\nand how do you actually evaluate a Bayesian\nmeta-learning method?",
    "start": "4247303",
    "end": "4253090"
  },
  {
    "text": "So one thing you could do\nis use a standard benchmark like MiniImagenet, and this is\nnice in that it's standardized,",
    "start": "4253090",
    "end": "4259995"
  },
  {
    "text": "it has real images. It's a good check that the\napproach didn't break anything. But the downside is that\nmetrics like accuracy",
    "start": "4259995",
    "end": "4267030"
  },
  {
    "text": "don't actually measure whether\nyour uncertainty is accurate, and the task may not\nexhibit any ambiguity.",
    "start": "4267030",
    "end": "4274740"
  },
  {
    "text": "And representing\nuncertainty may not actually be beneficial in\nthese circumstances.",
    "start": "4274740",
    "end": "4281390"
  },
  {
    "text": "So what I want to\ntalk about briefly is what are some better problems\nand metrics for evaluating",
    "start": "4281390",
    "end": "4286750"
  },
  {
    "text": "these kinds of methods? And it really depends on the\nproblem that you care about. So one thing that you could\ndo is look at some toy",
    "start": "4286750",
    "end": "4295540"
  },
  {
    "text": "2D problems that allow you to\nvisualize things very easily, and specifically design\nthem to have ambiguity.",
    "start": "4295540",
    "end": "4302230"
  },
  {
    "text": "So for example, you\ncould train a model on both linear functions\nand sinusoid functions,",
    "start": "4302230",
    "end": "4307840"
  },
  {
    "text": "add some noise to\nthe data points which are shown in purple\nsuch that in some cases it's ambiguous whether\nit's a sinusoid function",
    "start": "4307840",
    "end": "4314710"
  },
  {
    "text": "or a linear function,\nand then you can visualize the different kind\nof functions that are predicted",
    "start": "4314710",
    "end": "4320470"
  },
  {
    "text": "by the model and see\nthat it can actually represent a distribution\nover those functions.",
    "start": "4320470",
    "end": "4325790"
  },
  {
    "text": "So for example, in this case,\nbecause all the data points are up here, it's unclear\nexactly which line",
    "start": "4325790",
    "end": "4332320"
  },
  {
    "text": "is the true line\nof the function. And in this case, because the\ndata points are here and up",
    "start": "4332320",
    "end": "4337659"
  },
  {
    "text": "here, and because there's\nnoise in the data, it's unclear if the\nfunction is a sinusoid or a linear function.",
    "start": "4337660",
    "end": "4343000"
  },
  {
    "text": " You can also design an\nambiguous classification problem",
    "start": "4343000",
    "end": "4349590"
  },
  {
    "text": "where all the problems\nin the data distribution",
    "start": "4349590",
    "end": "4355540"
  },
  {
    "text": "of circular decision\nboundaries, but you only give it one data point to\ninfer the function from.",
    "start": "4355540",
    "end": "4362930"
  },
  {
    "text": "And when you do that, if you\nonly give it this green plus, for example, then it's ambiguous\nwhere exactly the decision",
    "start": "4362930",
    "end": "4369880"
  },
  {
    "text": "boundary lies and\nyou can actually visualize the\ndifferent functions-- you can visualize basically\nthe different functions",
    "start": "4369880",
    "end": "4375560"
  },
  {
    "text": "as estimating based off\nof the single data point. ",
    "start": "4375560",
    "end": "4382199"
  },
  {
    "text": "So this is one example of how\nyou can visualize and measure whether or not the uncertainty\nin the model is accurate.",
    "start": "4382200",
    "end": "4389648"
  },
  {
    "text": "But of course, these\nare toy problems that may not translate\nto more complex problems.",
    "start": "4389648",
    "end": "4395560"
  },
  {
    "text": "Another thing you could do is\ntake a more ambiguous problem. For example, this paper\nlooked at the problem",
    "start": "4395560",
    "end": "4402630"
  },
  {
    "text": "of being able to generate\nimages given just one view.",
    "start": "4402630",
    "end": "4408280"
  },
  {
    "text": "So for example, it's\ngiven this view, and it's ambiguous\nbecause there are parts of the image that are excluded. So it doesn't know what the\nobject will look like when it's",
    "start": "4408280",
    "end": "4416070"
  },
  {
    "text": "flipped around 180 degrees. And by representing uncertainty\nwith the Versa approach, which",
    "start": "4416070",
    "end": "4422580"
  },
  {
    "text": "is I thought was actually\nsimilar to one of the things that we talked about today, it\ncan represent this uncertainty",
    "start": "4422580",
    "end": "4428880"
  },
  {
    "text": "in a much more crisp way\nthen a more naive approach. ",
    "start": "4428880",
    "end": "4435760"
  },
  {
    "text": "And this also leads to a\nbetter reconstruction accuracy because this is an ambiguous\nfew-shot learning problem.",
    "start": "4435760",
    "end": "4441400"
  },
  {
    "text": " Another thing that you could\ndo to evaluate these methods",
    "start": "4441400",
    "end": "4448590"
  },
  {
    "text": "is look at mode coverage and\nlikelihood on ambiguous tasks. So you could consider the set of\nambiguous classification tasks",
    "start": "4448590",
    "end": "4455910"
  },
  {
    "text": "that I talked about\nat the beginning, and then evaluate if you give\nit something ambiguous where",
    "start": "4455910",
    "end": "4461302"
  },
  {
    "text": "three of the\nattributes are shared between the positive\nand negative examples and there are three\npotential modes that share",
    "start": "4461303",
    "end": "4467550"
  },
  {
    "text": "different numbers of\nattributes, then you can see whether it's able to cover\nall three of these modes,",
    "start": "4467550",
    "end": "4475320"
  },
  {
    "text": "or basically, how many modes the\ndistribution will capture when",
    "start": "4475320",
    "end": "4481230"
  },
  {
    "text": "you sample from it, and you\ncan also measure the log likelihood of this. ",
    "start": "4481230",
    "end": "4489250"
  },
  {
    "text": "And then lastly, one\nthing you could do is look at what's called\na reliability diagram, and this is basically trying\nto see that for data points",
    "start": "4489250",
    "end": "4499110"
  },
  {
    "text": "that are more accurate-- for data points that it makes\naccurate predictions on,",
    "start": "4499110",
    "end": "4504420"
  },
  {
    "text": "is it confident in those? And for data points that\nit is less accurate on, is it less confident in those?",
    "start": "4504420",
    "end": "4510150"
  },
  {
    "text": "So this is plotting\nthe confidence that the classifier produces\nversus the accuracy.",
    "start": "4510150",
    "end": "4516270"
  },
  {
    "text": "And if you basically see\nthat the red bar has followed this line, that suggests that\nthe confidence is calibrated",
    "start": "4516270",
    "end": "4524010"
  },
  {
    "text": "to the true accuracy. Whereas if you look at kind\nof standard MAML, for example,",
    "start": "4524010",
    "end": "4529530"
  },
  {
    "text": "it's not very calibrated,\nand it doesn't-- there's kind of a\ndifference between this line",
    "start": "4529530",
    "end": "4534630"
  },
  {
    "text": "and where the bars are.  OK, oh, and then lastly, and\nactually the most interesting",
    "start": "4534630",
    "end": "4543360"
  },
  {
    "text": "thing that you could\ndo is actually evaluate whether it's useful\nfor active learning. So you can basically\nsay, if I give it",
    "start": "4543360",
    "end": "4550770"
  },
  {
    "text": "one, or two, or three, or\nfive additional data points, can it select data points that\nlead to lower and lower error",
    "start": "4550770",
    "end": "4558660"
  },
  {
    "text": "compared to just randomly\nselecting data points? So in both of these\ntwo experiments,",
    "start": "4558660",
    "end": "4564590"
  },
  {
    "text": "they sequentially\nchoose a data point, and they choose the data point\nwith the highest maximum--",
    "start": "4564590",
    "end": "4570680"
  },
  {
    "text": "with the highest\npredictive entropy, basically the highest\nuncertainty to be labeled,",
    "start": "4570680",
    "end": "4576110"
  },
  {
    "text": "versus just choosing\na random data point. And you see that\nyou can basically get a more accurate classifier\nfaster with fewer labels",
    "start": "4576110",
    "end": "4584620"
  },
  {
    "text": "by leveraging this uncertainty. So [MUTED] is asking, can\nwe use a simple approach",
    "start": "4584620",
    "end": "4590650"
  },
  {
    "text": "like mean field approximation\nfor variational inference? Will it work well? So the-- these methods\nare actually already doing",
    "start": "4590650",
    "end": "4603580"
  },
  {
    "text": "something like mean\nfield approximation where the distribution over\nthe latent variable",
    "start": "4603580",
    "end": "4613390"
  },
  {
    "text": "is kind of an independent\nGaussian distribution. So it's basically\nassuming that each of the dimensions in\nyour latent variable",
    "start": "4613390",
    "end": "4619540"
  },
  {
    "text": "are independent\nfrom one another.  Yeah, so is z kind\nof [INAUDIBLE]",
    "start": "4619540",
    "end": "4626750"
  },
  {
    "text": "and this idea of flowing\nthe [INAUDIBLE] space to make it non-Gaussian. I guess [INAUDIBLE] try that.",
    "start": "4626750",
    "end": "4633738"
  },
  {
    "text": "Yeah, so you could\ndefinitely use something like normalizing flows to get\na non-Gaussian distribution.",
    "start": "4633738",
    "end": "4640052"
  },
  {
    "text": "There may be works that\ndo something like that, but I don't know of any\noff the top of my head. Isn't it a benefit of doing\nlike additional [INAUDIBLE],,",
    "start": "4640052",
    "end": "4649130"
  },
  {
    "text": "is it in most cases, a\nsimple Gaussian might work. But when we add in\nnormal inferences,",
    "start": "4649130",
    "end": "4654645"
  },
  {
    "text": "we are having a\nbetter pattern down. And it means like [INAUDIBLE]\nif it may or may not be",
    "start": "4654645",
    "end": "4663160"
  },
  {
    "text": "[INAUDIBLE]. Because [INAUDIBLE] said\nthere must be some benefit of adding [INAUDIBLE].",
    "start": "4663160",
    "end": "4669460"
  },
  {
    "text": "Yeah, so my intuition\nis that if you can represent a\nnon-Gaussian distribution, you'll give more flexibility\nto the meta learning process.",
    "start": "4669460",
    "end": "4677530"
  },
  {
    "text": "And with more\nflexibility, you'll be able to fit the meta\ntraining data better.",
    "start": "4677530",
    "end": "4684340"
  },
  {
    "text": "That's a hypothesis. It's not-- I mean,\nperhaps there are",
    "start": "4684340",
    "end": "4689410"
  },
  {
    "text": "cases where a Gaussian\ndistribution is kind of flexible enough. It also seems, I guess, using\nnon-Gaussian distributions",
    "start": "4689410",
    "end": "4696710"
  },
  {
    "text": "also-- I guess placing Gaussian\ndistribution seemed like a pretty restrictive\nconstraint on the distribution",
    "start": "4696710",
    "end": "4704320"
  },
  {
    "text": "that you can represent. So my guess is that you\nwould be able to better fit the data with a\nnon-Gaussian distribution,",
    "start": "4704320",
    "end": "4712710"
  },
  {
    "text": "and it just kind of seems\naesthetically more appealing. But I haven't\nverified this myself.",
    "start": "4712710",
    "end": "4719619"
  },
  {
    "text": "[INAUDIBLE] training\nout of that training, just having a high\n[INAUDIBLE] doesn't really",
    "start": "4719620",
    "end": "4725449"
  },
  {
    "text": "mean that [INAUDIBLE]. But yeah, I agree. Having a non-Gaussian\n[INAUDIBLE] it probably",
    "start": "4725450",
    "end": "4732958"
  },
  {
    "text": "wouldn't better [INAUDIBLE]\nGaussian distribution on the [INAUDIBLE]. ",
    "start": "4732958",
    "end": "4739570"
  },
  {
    "text": "Can we also like, instead\nof like a [INAUDIBLE] kind of the line\nrepresentation, [INAUDIBLE]",
    "start": "4739570",
    "end": "4745750"
  },
  {
    "text": "to get through basically\nthe [INAUDIBLE],, like if you want to\nlearn [INAUDIBLE]",
    "start": "4745750",
    "end": "4752360"
  },
  {
    "text": "I can still learn it\nif you [INAUDIBLE]..  Yeah, so you can't train\nthe flow model directly",
    "start": "4752360",
    "end": "4759820"
  },
  {
    "text": "on the parameters itself because\nyou don't have supervision on the parameters, but\nyou can basically--",
    "start": "4759820",
    "end": "4765460"
  },
  {
    "text": "if you use it to represent a\ndistribution and then you use",
    "start": "4765460",
    "end": "4772030"
  },
  {
    "text": "that model to make predictions\non the data points, then you should be able to\nback propagate into the model--",
    "start": "4772030",
    "end": "4778840"
  },
  {
    "text": "into your normalizing\nflow model. One last question. So last week we had this\nfourth prediction [INAUDIBLE]",
    "start": "4778840",
    "end": "4784110"
  },
  {
    "text": "where we had a [INAUDIBLE]\nkind of information flow.",
    "start": "4784110",
    "end": "4789520"
  },
  {
    "text": "If we did the same thing\nin a variation of a mission more directly, can it support\nmagnetic variation here?",
    "start": "4789520",
    "end": "4796790"
  },
  {
    "text": "Like it's not going to-- or like as good as if you just\nuse this kind of information",
    "start": "4796790",
    "end": "4803730"
  },
  {
    "text": "flow in variation? But here, you're also\nlearning [INAUDIBLE].. Yeah, so all the works--",
    "start": "4803730",
    "end": "4810483"
  },
  {
    "text": "almost all the\nworks that I talked about today preceded the work\nthat we did on memorization.",
    "start": "4810483",
    "end": "4815980"
  },
  {
    "text": "So that wasn't something that we\nhad really an idea or a notion that we had formalized when we\nwere working on these Bayesian",
    "start": "4815980",
    "end": "4823150"
  },
  {
    "text": "modeling methods and when other\npeople were working on them. I do think that in principle\nthese methods should",
    "start": "4823150",
    "end": "4828280"
  },
  {
    "text": "be more robust to\nmemorization, but it's kind of a very active area\nand not something that I've",
    "start": "4828280",
    "end": "4835510"
  },
  {
    "text": "seen from these works yet. Be a good project\nto look at this kind of Bayesian approaches\nto this kind",
    "start": "4835510",
    "end": "4842560"
  },
  {
    "text": "of more complex [INAUDIBLE]\nkind of problems. Like see if it is\nreally good on like a--",
    "start": "4842560",
    "end": "4848883"
  },
  {
    "text": "Yeah, I think that it could\nbe reasonable to explore whether these Bayesian\nmeta-learning methods prevent",
    "start": "4848883",
    "end": "4854290"
  },
  {
    "text": "memorization. I see. What's your question?",
    "start": "4854290",
    "end": "4860739"
  },
  {
    "text": "I actually have two questions. So the first one is\nvery naive, so it's about the ensemble approach.",
    "start": "4860740",
    "end": "4867679"
  },
  {
    "text": "So say if we only had five\nMAMLs in the ensemble, so how can we make any\naccurate prediction",
    "start": "4867680",
    "end": "4876400"
  },
  {
    "text": "for the distributions\nof the parameters? So it really depends on what\nyou're using the ensemble for.",
    "start": "4876400",
    "end": "4884080"
  },
  {
    "text": "People typically look at-- use ensembles to get a rough\nestimate for uncertainty,",
    "start": "4884080",
    "end": "4889120"
  },
  {
    "text": "and the way that you can\ndo that with an ensemble is look at how much the\nensemble elements disagree with one another.",
    "start": "4889120",
    "end": "4895060"
  },
  {
    "text": "And empirically, people\nhave found that you only need like a few ensembles\nto be able to get",
    "start": "4895060",
    "end": "4900760"
  },
  {
    "text": "at a reasonable estimate of\nthat sort of disagreement. ",
    "start": "4900760",
    "end": "4906630"
  },
  {
    "text": "But yeah, it is a\nlittle bit strange that you wouldn't need\nmore than that, per se,",
    "start": "4906630",
    "end": "4912600"
  },
  {
    "text": "to get a good estimate. That's kind of what people\nhave observed in practice.",
    "start": "4912600",
    "end": "4917646"
  },
  {
    "text": "Yeah, so I have\nanother question. So kind of today,\nwith the Bayesian,",
    "start": "4917646",
    "end": "4922940"
  },
  {
    "text": "I notice you're\ntalking about kind of to capture the\nuncertainty in the models.",
    "start": "4922940",
    "end": "4929380"
  },
  {
    "text": "So you also mentioned that there\nare also uncertain limitation. So I'm wondering if we know\nsome of the uncertainty,",
    "start": "4929380",
    "end": "4936360"
  },
  {
    "text": "how can we leverage these\nproperties of the data? ",
    "start": "4936360",
    "end": "4946290"
  },
  {
    "text": "Yeah, so a lot of\nthese methods that can capture model\nuncertainty will also be able to capture uncertainty\nin the data, as well.",
    "start": "4946290",
    "end": "4954900"
  },
  {
    "text": " Capturing uncertainty\nin the data",
    "start": "4954900",
    "end": "4960060"
  },
  {
    "text": "is like the easier\nkind of uncertainty, essentially, compared\nto capturing uncertainty in the model.",
    "start": "4960060",
    "end": "4965730"
  },
  {
    "text": "Because when you train a\ndistribution to the data, yeah,",
    "start": "4965730",
    "end": "4972930"
  },
  {
    "text": "it should represent\nthat fairly readily. So I mean, if we explicitly know\nthe uncertainties in the data--",
    "start": "4972930",
    "end": "4981304"
  },
  {
    "text": "for example, we know\nwhen we take an image, we know, for example,\nhow the noises are",
    "start": "4981305",
    "end": "4987810"
  },
  {
    "text": "distributed in\nthis image, can we use this kind of information?",
    "start": "4987810",
    "end": "4995210"
  },
  {
    "text": "If you have knowledge of\nthe noise in your data, then you could\ndefinitely leverage that",
    "start": "4995210",
    "end": "5000280"
  },
  {
    "text": "by augmenting your kind\nof output distribution. Typically, when I say noise in\nthe data I mean more like noise",
    "start": "5000280",
    "end": "5008830"
  },
  {
    "text": "in kind of p of y\ngiven x, because this is what you're predicting.",
    "start": "5008830",
    "end": "5016480"
  },
  {
    "text": "So if you know the distribution\nof noise in your labels, then you can basically\ntack that noise model",
    "start": "5016480",
    "end": "5023620"
  },
  {
    "text": "onto the output of\nyour neural network. In practice, that sort\nof noise isn't that",
    "start": "5023620",
    "end": "5029290"
  },
  {
    "text": "hard to learn, though, either. Thank you so much. ",
    "start": "5029290",
    "end": "5035980"
  },
  {
    "text": "Hey. So yeah, the noise in the data. So version 0 was, I\nthink, what you mentioned,",
    "start": "5035980",
    "end": "5041800"
  },
  {
    "text": "it's the noise in the data. And then the rest\nof the thing is like uncertainty with the\nmodel parameters, right?",
    "start": "5041800",
    "end": "5048710"
  },
  {
    "text": "So in the last part\nof the lecture, when you were talking about\nthe evaluation in front",
    "start": "5048710",
    "end": "5054877"
  },
  {
    "text": "of the reliability\ndiagram plots, wouldn't just the uncertainty\non data show me this?",
    "start": "5054877",
    "end": "5064240"
  },
  {
    "text": "Like I'm trying\nto figure out how I would interpret uncertainty\nin the model with this.",
    "start": "5064240",
    "end": "5071850"
  },
  {
    "text": "If I just did uncertainty over\nmy data, and I did these plots,",
    "start": "5071850",
    "end": "5078310"
  },
  {
    "text": "I could gather, right? Like, my accuracy's higher. ",
    "start": "5078310",
    "end": "5084370"
  },
  {
    "text": "Yeah, so it depends\non your test data. If your test data is out of\ndistribution a little bit,",
    "start": "5084370",
    "end": "5090250"
  },
  {
    "text": "then you may be\ninaccurate on that data, but overconfident if you don't\nmeasure your model uncertainty.",
    "start": "5090250",
    "end": "5100025"
  },
  {
    "text": "If the data that\nyou're evaluating this is very much in\ndistribution, like you can evaluate this on\nyour training data,",
    "start": "5100025",
    "end": "5107560"
  },
  {
    "text": "then that would be kind\nof what you're saying. It would be just looking\nat the data uncertainty. I see, I see.",
    "start": "5107560",
    "end": "5112929"
  },
  {
    "text": "So then what you're suggesting\nis in this liability diagram, so then on the\nconfidence part, one",
    "start": "5112930",
    "end": "5119770"
  },
  {
    "text": "should do just uncertainty over\nthe model parameters, right? For the confidence?",
    "start": "5119770",
    "end": "5126520"
  },
  {
    "text": "Well, so the confidence score-- I guess-- I can't remember\nexactly how they were measuring",
    "start": "5126520",
    "end": "5133120"
  },
  {
    "text": "this in their method. But yeah, you basically\nwant to plot this confidence",
    "start": "5133120",
    "end": "5138730"
  },
  {
    "text": "in terms of the-- well, the variance and the\npredictions that the model",
    "start": "5138730",
    "end": "5145780"
  },
  {
    "text": "is making, essentially. And if it is accurately\nrepresenting model uncertainty,",
    "start": "5145780",
    "end": "5152240"
  },
  {
    "text": "then you would expect this\nto be more calibrated or more correct, especially for\ndata that's a little bit out",
    "start": "5152240",
    "end": "5160180"
  },
  {
    "text": "of distribution.  Does that answer your question?",
    "start": "5160180",
    "end": "5165619"
  },
  {
    "text": "Yeah. Thanks. OK, sounds good. I'll see everyone next week.",
    "start": "5165620",
    "end": "5172900"
  },
  {
    "start": "5172900",
    "end": "5177323"
  }
]