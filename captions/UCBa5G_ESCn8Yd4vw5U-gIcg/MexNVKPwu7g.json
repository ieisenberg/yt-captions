[
  {
    "text": "Now we're going on\nto a new topic, which",
    "start": "0",
    "end": "3020"
  },
  {
    "text": "is recurrent neural networks.",
    "start": "3020",
    "end": "5510"
  },
  {
    "text": "So like convolutional\nneural networks,",
    "start": "5510",
    "end": "8420"
  },
  {
    "text": "we're used for modeling images,\nrecurrent neural networks",
    "start": "8420",
    "end": "13340"
  },
  {
    "text": "are used for modeling sequences,\ndata that arise as sequences.",
    "start": "13340",
    "end": "19450"
  },
  {
    "text": "So here are some examples.",
    "start": "19450",
    "end": "21960"
  },
  {
    "text": "So documents are\nsequences of words",
    "start": "21960",
    "end": "24910"
  },
  {
    "text": "and their relative\npositions have meaning.",
    "start": "24910",
    "end": "27420"
  },
  {
    "text": "So the bag of words model,\nwe didn't take the order",
    "start": "27420",
    "end": "30640"
  },
  {
    "text": "of the words into account.",
    "start": "30640",
    "end": "32920"
  },
  {
    "text": "It was just a bag of which\nwords were used no matter",
    "start": "32920",
    "end": "35140"
  },
  {
    "text": "where they occurred.",
    "start": "35140",
    "end": "36520"
  },
  {
    "text": "Here we want to take\nthe sequence of words",
    "start": "36520",
    "end": "38950"
  },
  {
    "text": "into account and the order.",
    "start": "38950",
    "end": "41470"
  },
  {
    "text": "Well, time series\nsuch as weather data",
    "start": "41470",
    "end": "43780"
  },
  {
    "text": "or financial indices,\nthose are sequences.",
    "start": "43780",
    "end": "47940"
  },
  {
    "text": "Recorded speech or\nmusic is a sequence,",
    "start": "47940",
    "end": "50230"
  },
  {
    "text": "sequence of notes or phonemes.",
    "start": "50230",
    "end": "53670"
  },
  {
    "text": "Handwriting such as a\ndoctor's notes is a sequence.",
    "start": "53670",
    "end": "58409"
  },
  {
    "text": "So RNNs, that's the abbreviation\nfor recurrent neural networks,",
    "start": "58410",
    "end": "64510"
  },
  {
    "text": "they build models that take into\naccount the sequential nature",
    "start": "64510",
    "end": "67800"
  },
  {
    "text": "of the data.",
    "start": "67800",
    "end": "69650"
  },
  {
    "text": "And in doing so, they\nbuild a memory of the past.",
    "start": "69650",
    "end": "73610"
  },
  {
    "text": "Just to get into\nnotation, so the feature",
    "start": "73610",
    "end": "76340"
  },
  {
    "text": "for each observation, is\na sequence of vectors,",
    "start": "76340",
    "end": "81570"
  },
  {
    "text": "and the sequence is\nof length earlier.",
    "start": "81570",
    "end": "84970"
  },
  {
    "text": "And so X1, X2 up to Xl, each of\nthese are vectors of numbers.",
    "start": "84970",
    "end": "91620"
  },
  {
    "text": "So it's just like an\ninput feature vector,",
    "start": "91620",
    "end": "94780"
  },
  {
    "text": "but now we've got\na sequence of them.",
    "start": "94780",
    "end": "97390"
  },
  {
    "text": "The target Y is often\nof the usual kind, e.g.",
    "start": "97390",
    "end": "101020"
  },
  {
    "text": "a single variable\nsuch as sentiment",
    "start": "101020",
    "end": "103390"
  },
  {
    "text": "for the whole\ndocument, or perhaps",
    "start": "103390",
    "end": "107170"
  },
  {
    "text": "a one-hot vector for a\nmulticlass classification task,",
    "start": "107170",
    "end": "112040"
  },
  {
    "text": "but it's just a single response.",
    "start": "112040",
    "end": "115460"
  },
  {
    "text": "However, Y can\nalso be a sequence",
    "start": "115460",
    "end": "117770"
  },
  {
    "text": "such as the same document\nin a different language",
    "start": "117770",
    "end": "120409"
  },
  {
    "text": "when you want to do\nlanguage translation.",
    "start": "120410",
    "end": "123520"
  },
  {
    "text": "It's a more\ndifficult case, we're",
    "start": "123520",
    "end": "124899"
  },
  {
    "text": "going to deal with\nthe simple case",
    "start": "124900",
    "end": "126340"
  },
  {
    "text": "and work with the\nsentiment example first.",
    "start": "126340",
    "end": "129039"
  },
  {
    "text": "So here's a simple recurrent\nneural network architecture",
    "start": "129039",
    "end": "133570"
  },
  {
    "text": "diagram.",
    "start": "133570",
    "end": "135490"
  },
  {
    "text": "And we showed first in\nthis abbreviated form",
    "start": "135490",
    "end": "140590"
  },
  {
    "text": "and you can see there's\na cycle going on,",
    "start": "140590",
    "end": "143290"
  },
  {
    "text": "which emphasizes the\nrecurrent nature.",
    "start": "143290",
    "end": "146400"
  },
  {
    "text": "But then we represent\nit as a sequence.",
    "start": "146400",
    "end": "150180"
  },
  {
    "text": "So here you've got your\ninput sequence X1, X2,",
    "start": "150180",
    "end": "153900"
  },
  {
    "text": "X3 all the way till Xl.",
    "start": "153900",
    "end": "157470"
  },
  {
    "text": "And as you're\ngoing to see, we're",
    "start": "157470",
    "end": "159330"
  },
  {
    "text": "going to treat each\nof our observations",
    "start": "159330",
    "end": "161490"
  },
  {
    "text": "as sequences of the same length.",
    "start": "161490",
    "end": "164400"
  },
  {
    "text": "And if they're not, for whatever\nreasons, the same length,",
    "start": "164400",
    "end": "167519"
  },
  {
    "text": "we're going to force them to\nbe the same length in what we",
    "start": "167520",
    "end": "171180"
  },
  {
    "text": "do here.",
    "start": "171180",
    "end": "172650"
  },
  {
    "text": "But there's versions\nthat don't need that.",
    "start": "172650",
    "end": "176760"
  },
  {
    "text": "Now along with the\ninput sequence,",
    "start": "176760",
    "end": "179400"
  },
  {
    "text": "you've got a hidden\nlayer sequence.",
    "start": "179400",
    "end": "182849"
  },
  {
    "text": "So these are activations\njust like we had before,",
    "start": "182850",
    "end": "186090"
  },
  {
    "text": "but they march along in step\nwith the input sequence.",
    "start": "186090",
    "end": "189650"
  },
  {
    "text": "So there's A1, A2, up to Al.",
    "start": "189650",
    "end": "193560"
  },
  {
    "text": "In this diagram, each of\nthese A's are also a vector,",
    "start": "193560",
    "end": "197220"
  },
  {
    "text": "X1 is a vector of input\nnumbers at this first point",
    "start": "197220",
    "end": "201750"
  },
  {
    "text": "in the sequence.",
    "start": "201750",
    "end": "202890"
  },
  {
    "text": "A1 is going to be\na vector, maybe",
    "start": "202890",
    "end": "204930"
  },
  {
    "text": "a different size of activation\nunits at this first step",
    "start": "204930",
    "end": "208950"
  },
  {
    "text": "in the sequence.",
    "start": "208950",
    "end": "210150"
  },
  {
    "text": "And they all feed\ninto each other",
    "start": "210150",
    "end": "212459"
  },
  {
    "text": "or move along with the sequence.",
    "start": "212460",
    "end": "213800"
  },
  {
    "start": "213800",
    "end": "216600"
  },
  {
    "text": "Now you've got weights.",
    "start": "216600",
    "end": "218620"
  },
  {
    "text": "So let's look at the weights\nset, for example, goes into A2.",
    "start": "218620",
    "end": "222305"
  },
  {
    "start": "222305",
    "end": "224859"
  },
  {
    "text": "You get input into\nA2 from X2, that's",
    "start": "224860",
    "end": "228520"
  },
  {
    "text": "the corresponding element\nof the input sequence.",
    "start": "228520",
    "end": "232070"
  },
  {
    "text": "And you also get input into A2\nfrom the previous hidden vector",
    "start": "232070",
    "end": "236980"
  },
  {
    "text": "A1.",
    "start": "236980",
    "end": "238360"
  },
  {
    "text": "So A2 gets input from A1 and X2.",
    "start": "238360",
    "end": "243160"
  },
  {
    "text": "A3 gets input from x3 and A2.",
    "start": "243160",
    "end": "248200"
  },
  {
    "text": "And so what you can see is\nthat these A's are somehow",
    "start": "248200",
    "end": "252670"
  },
  {
    "text": "accumulating a memory of what's\nhappened in the sequence.",
    "start": "252670",
    "end": "257310"
  },
  {
    "text": "The memory part is carried\nforward by these A's.",
    "start": "257310",
    "end": "260790"
  },
  {
    "text": "And you update the memory\nby getting the memory",
    "start": "260790",
    "end": "264240"
  },
  {
    "text": "from the previous step and\naugmenting the information",
    "start": "264240",
    "end": "268889"
  },
  {
    "text": "from the next input\nvector in the sequence.",
    "start": "268890",
    "end": "273090"
  },
  {
    "text": "And that gives you A3.",
    "start": "273090",
    "end": "275610"
  },
  {
    "text": "The other thing\nhere is that you'll",
    "start": "275610",
    "end": "277199"
  },
  {
    "text": "notice is that the same weights\nare used at each of these steps",
    "start": "277200",
    "end": "281580"
  },
  {
    "text": "as we move along, and that's\nwhere the name recurrent comes",
    "start": "281580",
    "end": "284909"
  },
  {
    "text": "from, the same weights.",
    "start": "284910",
    "end": "288300"
  },
  {
    "text": "So the single set\nof weights W going",
    "start": "288300",
    "end": "290909"
  },
  {
    "text": "from the input vector\nto the hidden unit,",
    "start": "290910",
    "end": "295950"
  },
  {
    "text": "and the same weights you gained\nfrom the previous activation",
    "start": "295950",
    "end": "300000"
  },
  {
    "text": "vector to the current\nactivation vector.",
    "start": "300000",
    "end": "302270"
  },
  {
    "start": "302270",
    "end": "305340"
  },
  {
    "text": "We also shown set\nof output units.",
    "start": "305340",
    "end": "309780"
  },
  {
    "text": "And again, the same weights\ngo from the hidden unit",
    "start": "309780",
    "end": "314580"
  },
  {
    "text": "to the output unit.",
    "start": "314580",
    "end": "316530"
  },
  {
    "text": "See, these are named as B's.",
    "start": "316530",
    "end": "320370"
  },
  {
    "text": "And that means we can actually\nmeasure an output at each step",
    "start": "320370",
    "end": "323940"
  },
  {
    "text": "along the sequence.",
    "start": "323940",
    "end": "325150"
  },
  {
    "text": "But we're typically interested\nin the accumulated knowledge",
    "start": "325150",
    "end": "328440"
  },
  {
    "text": "of the output, and so we're\nonly interested in the last one,",
    "start": "328440",
    "end": "331410"
  },
  {
    "text": "which in this case\nis Y. But the Y",
    "start": "331410",
    "end": "334920"
  },
  {
    "text": "applications where you could\nbe interested in all of them.",
    "start": "334920",
    "end": "339840"
  },
  {
    "text": "So here the parameters that need\nto be learned are B, U and W.",
    "start": "339840",
    "end": "343949"
  },
  {
    "text": "So here's some more detail.",
    "start": "343950",
    "end": "346080"
  },
  {
    "text": "Suppose that lth element of\nthe sequence has p components,",
    "start": "346080",
    "end": "352360"
  },
  {
    "text": "remember each input is\na vector in the sequence",
    "start": "352360",
    "end": "356789"
  },
  {
    "text": "and let's say it's\ngot p components.",
    "start": "356790",
    "end": "359110"
  },
  {
    "text": "And let's suppose\neach hidden unit",
    "start": "359110",
    "end": "362699"
  },
  {
    "text": "in that sequence of hidden\nunits has got k components.",
    "start": "362700",
    "end": "366390"
  },
  {
    "text": "Then the computation of the k\ncomponents of the hidden units",
    "start": "366390",
    "end": "370470"
  },
  {
    "text": "Al is given by this expression.",
    "start": "370470",
    "end": "373840"
  },
  {
    "text": "So there's a bias or intercept.",
    "start": "373840",
    "end": "377169"
  },
  {
    "text": "Then there's a\nlinear combination",
    "start": "377170",
    "end": "379150"
  },
  {
    "text": "of the p values of the input\nvector at that point l,",
    "start": "379150",
    "end": "383290"
  },
  {
    "text": "and then a linear\ncombination of the activation",
    "start": "383290",
    "end": "387250"
  },
  {
    "text": "units at the previous step.",
    "start": "387250",
    "end": "389410"
  },
  {
    "text": "And then there's a\nnonlinearity such as ReLU.",
    "start": "389410",
    "end": "393790"
  },
  {
    "text": "In the output layer, for\nexample, is just a linear model,",
    "start": "393790",
    "end": "398770"
  },
  {
    "text": "or if it's a classification,\nit will be a softmax",
    "start": "398770",
    "end": "401979"
  },
  {
    "text": "or logistic transform.",
    "start": "401980",
    "end": "404320"
  },
  {
    "text": "As we said, we're often only\nconcerned with the prediction Ol",
    "start": "404320",
    "end": "408460"
  },
  {
    "text": "at the end of the\nsequence, this guy.",
    "start": "408460",
    "end": "413160"
  },
  {
    "text": "For example, sentiment\nof the document.",
    "start": "413160",
    "end": "415620"
  },
  {
    "text": "I just want to ask, but\ngo back to the picture.",
    "start": "415620",
    "end": "417738"
  },
  {
    "text": "I guess we can see\nhow it's different",
    "start": "417738",
    "end": "419280"
  },
  {
    "text": "than the bag of words we\ndid earlier in that because",
    "start": "419280",
    "end": "421680"
  },
  {
    "text": "of the ordering is definitely\ntaken into account,",
    "start": "421680",
    "end": "423410"
  },
  {
    "text": "is that right?",
    "start": "423410",
    "end": "423993"
  },
  {
    "text": "Yes, you're right.",
    "start": "423993",
    "end": "425020"
  },
  {
    "text": "And if we didn't have those\nuse that move from one unit",
    "start": "425020",
    "end": "429400"
  },
  {
    "text": "to the next with it, would it\nbe the same as the bag of words?",
    "start": "429400",
    "end": "433660"
  },
  {
    "text": "It still wouldn't\nbe the same, Rob,",
    "start": "433660",
    "end": "435490"
  },
  {
    "text": "but it would certainly be a\nmuch more complex network,",
    "start": "435490",
    "end": "439870"
  },
  {
    "text": "but it wouldn't be the same.",
    "start": "439870",
    "end": "441470"
  },
  {
    "text": "So there's quite a bit\nof different structure",
    "start": "441470",
    "end": "444190"
  },
  {
    "text": "going on here.",
    "start": "444190",
    "end": "445570"
  },
  {
    "text": "And we should emphasize,\nthis is a simple recurrent",
    "start": "445570",
    "end": "450310"
  },
  {
    "text": "neural network.",
    "start": "450310",
    "end": "451130"
  },
  {
    "text": "You can get more complex ones.",
    "start": "451130",
    "end": "452630"
  },
  {
    "text": "For example, you can\nhave two hidden layers,",
    "start": "452630",
    "end": "455600"
  },
  {
    "text": "so instead of going straight\nto the output layer,",
    "start": "455600",
    "end": "458560"
  },
  {
    "text": "you could have another layer\nof hidden units, a sequence.",
    "start": "458560",
    "end": "463360"
  },
  {
    "text": "So this sequence would be\ninput to the next sequence",
    "start": "463360",
    "end": "466870"
  },
  {
    "text": "in a fairly obvious way.",
    "start": "466870",
    "end": "468580"
  },
  {
    "text": "And there's lots of other ways\nyou can make this more general.",
    "start": "468580",
    "end": "472389"
  },
  {
    "text": "So if you look at the loss\nthat we're trying to minimize,",
    "start": "472390",
    "end": "475840"
  },
  {
    "text": "if we're only interested in a\nresponse from the last layer,",
    "start": "475840",
    "end": "481120"
  },
  {
    "text": "this is what we're\ngoing to look at.",
    "start": "481120",
    "end": "482780"
  },
  {
    "text": "We're going to look\nat the observed",
    "start": "482780",
    "end": "484238"
  },
  {
    "text": "response minus the output\nfrom the last hidden unit,",
    "start": "484238",
    "end": "488690"
  },
  {
    "text": "summed over observations from\n1 to n, say if we're using",
    "start": "488690",
    "end": "492280"
  },
  {
    "text": "squared error loss.",
    "start": "492280",
    "end": "493760"
  },
  {
    "text": "And if we expand\nthat out, you'll",
    "start": "493760",
    "end": "496510"
  },
  {
    "text": "see all the weights\nthat we need to learn.",
    "start": "496510",
    "end": "498260"
  },
  {
    "text": "There's the betas,\nwhich comprise",
    "start": "498260",
    "end": "500170"
  },
  {
    "text": "that B matrix of weights.",
    "start": "500170",
    "end": "501800"
  },
  {
    "text": "So remember there were weights\nB, U, and W. These are typically",
    "start": "501800",
    "end": "506979"
  },
  {
    "text": "matrices of weights\nthat have to be learned.",
    "start": "506980",
    "end": "510040"
  },
  {
    "text": "And they're the same\nthroughout here.",
    "start": "510040",
    "end": "512620"
  },
  {
    "text": "So there's the B's,\nthere's the W's, there's",
    "start": "512620",
    "end": "515830"
  },
  {
    "text": "the U's. But this last only\ninvolves the activation at point",
    "start": "515830",
    "end": "522349"
  },
  {
    "text": "L minus 1 in the sequence.",
    "start": "522350",
    "end": "524819"
  },
  {
    "text": "So what about all\nthe other guys?",
    "start": "524820",
    "end": "527570"
  },
  {
    "text": "Well, the thing is, these\nA's are defined recursively.",
    "start": "527570",
    "end": "532590"
  },
  {
    "text": "So the A at layer\nL minus 1 depends",
    "start": "532590",
    "end": "536645"
  },
  {
    "text": "on the A at layer L minus\n2 and so on and goes back.",
    "start": "536645",
    "end": "541520"
  },
  {
    "text": "So when you come to\nestimate the parameters,",
    "start": "541520",
    "end": "544370"
  },
  {
    "text": "you have to take all the steps\nin the sequence into account.",
    "start": "544370",
    "end": "547730"
  },
  {
    "text": "We're going to use an\nRNN and the IMDB reviews",
    "start": "547730",
    "end": "552350"
  },
  {
    "text": "as opposed to the\nbag of words model.",
    "start": "552350",
    "end": "555709"
  },
  {
    "text": "So now the document feature\nis a sequence of words.",
    "start": "555710",
    "end": "560130"
  },
  {
    "text": "Let's call them Wl from\n1 to l, and we typically",
    "start": "560130",
    "end": "565340"
  },
  {
    "text": "pad the documents to the\nsame number of words.",
    "start": "565340",
    "end": "568020"
  },
  {
    "text": "So here we're going to use the\nfirst 500 words in the document.",
    "start": "568020",
    "end": "570900"
  },
  {
    "text": "If it's longer than 500\nwords and if it's less,",
    "start": "570900",
    "end": "573230"
  },
  {
    "text": "we'll just truncate and\npad with blanks up to 500.",
    "start": "573230",
    "end": "577500"
  },
  {
    "text": "So now we need some way to\nrepresent each word as a vector.",
    "start": "577500",
    "end": "582080"
  },
  {
    "text": "Well, we're going to use a\none-hot encoded binary vector,",
    "start": "582080",
    "end": "585410"
  },
  {
    "text": "dummy variable of length 10,000.",
    "start": "585410",
    "end": "588269"
  },
  {
    "text": "Now that may seem crazy, that's\na vector of length 10,000",
    "start": "588270",
    "end": "592620"
  },
  {
    "text": "and all the entries are 0\nexcept one corresponding",
    "start": "592620",
    "end": "595710"
  },
  {
    "text": "to the position of that\nword in the dictionary.",
    "start": "595710",
    "end": "597950"
  },
  {
    "text": "So that's a very sparse feature\nvector, and we got one of those",
    "start": "597950",
    "end": "601710"
  },
  {
    "text": "at each of the 500\nsteps in the document.",
    "start": "601710",
    "end": "606600"
  },
  {
    "text": "So this is extremely sparse.",
    "start": "606600",
    "end": "608759"
  },
  {
    "text": "What we tend to use is a much\nlower dimensional pretrained",
    "start": "608760",
    "end": "614520"
  },
  {
    "text": "word embedding\nmatrix, and I'm going",
    "start": "614520",
    "end": "619040"
  },
  {
    "text": "to show you a picture of\nthat in the next slide.",
    "start": "619040",
    "end": "621440"
  },
  {
    "text": "And what this does is reduces\nthe binary feature vector",
    "start": "621440",
    "end": "624380"
  },
  {
    "text": "of length 10,000 to a real\nfeature vector of dimension some",
    "start": "624380",
    "end": "628940"
  },
  {
    "text": "number m much less than 10k,\nlike in the low hundreds.",
    "start": "628940",
    "end": "633340"
  },
  {
    "text": "So here's the idea.",
    "start": "633340",
    "end": "635060"
  },
  {
    "text": "So here's a sequence\nof words at the bottom.",
    "start": "635060",
    "end": "639480"
  },
  {
    "text": "This is one of the best\nforms, actually the best",
    "start": "639480",
    "end": "641790"
  },
  {
    "text": "I've ever seen.",
    "start": "641790",
    "end": "643946"
  },
  {
    "text": "By the way, what we're showing\nyou is the review where we only",
    "start": "643946",
    "end": "648149"
  },
  {
    "text": "represent the words in the\ndictionary of size 10,000.",
    "start": "648150",
    "end": "651250"
  },
  {
    "text": "So any additional\nwords are being deleted",
    "start": "651250",
    "end": "653310"
  },
  {
    "text": "and so you don't see them.",
    "start": "653310",
    "end": "655529"
  },
  {
    "text": "And so here this top image\nrepresents a one-hot encoding.",
    "start": "655530",
    "end": "660430"
  },
  {
    "text": "So this gets a 1 in this\nposition and the gray",
    "start": "660430",
    "end": "663149"
  },
  {
    "text": "all 0's, is gets a 1 in\nthis position, and so on.",
    "start": "663150",
    "end": "668412"
  },
  {
    "text": "And of course, it's 10,000 high\nnote whatever the number in this",
    "start": "668412",
    "end": "672480"
  },
  {
    "text": "picture.",
    "start": "672480",
    "end": "673500"
  },
  {
    "text": "Here's the word\nembedding, and you",
    "start": "673500",
    "end": "675540"
  },
  {
    "text": "can see it's a much\nsmaller set of numbers.",
    "start": "675540",
    "end": "677889"
  },
  {
    "text": "And they're not just 0's and\n1's, they're real numbers.",
    "start": "677890",
    "end": "680320"
  },
  {
    "text": "So we've depicted them\nas colors, maybe a heat",
    "start": "680320",
    "end": "685620"
  },
  {
    "text": "map of colors that\ngoes from blue, which",
    "start": "685620",
    "end": "687900"
  },
  {
    "text": "is cold to red, which is hot,\nwith yellows and whatever,",
    "start": "687900",
    "end": "691890"
  },
  {
    "text": "and oranges in between.",
    "start": "691890",
    "end": "694770"
  },
  {
    "text": "So now where do we get\nthis word embedding from,",
    "start": "694770",
    "end": "698295"
  },
  {
    "text": "this new representation?",
    "start": "698295",
    "end": "699940"
  },
  {
    "text": "So you can see whenever\nwe have a have, instead",
    "start": "699940",
    "end": "702000"
  },
  {
    "text": "of having this coding, we can\nuse these numbers instead.",
    "start": "702000",
    "end": "705930"
  },
  {
    "text": "Well, these word embeddings are\npretrained on very large corpora",
    "start": "705930",
    "end": "710070"
  },
  {
    "text": "of documents and\nthey use methods",
    "start": "710070",
    "end": "712410"
  },
  {
    "text": "similar to principal components\nto come up with this lower",
    "start": "712410",
    "end": "716069"
  },
  {
    "text": "dimensional representation.",
    "start": "716070",
    "end": "718710"
  },
  {
    "text": "And they're very clever,\nthey take into account things",
    "start": "718710",
    "end": "722010"
  },
  {
    "text": "like synonyms.",
    "start": "722010",
    "end": "723190"
  },
  {
    "text": "So if you have a word like male\nand female or boy and girl,",
    "start": "723190",
    "end": "732370"
  },
  {
    "text": "the relative\nposition between them",
    "start": "732370",
    "end": "734230"
  },
  {
    "text": "in this representation\nand king and queen",
    "start": "734230",
    "end": "736750"
  },
  {
    "text": "will be somewhat the same.",
    "start": "736750",
    "end": "738020"
  },
  {
    "text": "So they take into account these\nsemantic meanings of words",
    "start": "738020",
    "end": "744280"
  },
  {
    "text": "and also synonyms.",
    "start": "744280",
    "end": "746020"
  },
  {
    "text": "So two of the\nwell-known embeddings",
    "start": "746020",
    "end": "748480"
  },
  {
    "text": "are word2vec and GloVe, and\nthese are available on the web.",
    "start": "748480",
    "end": "755029"
  },
  {
    "text": "You can download them, you\ndecide how many dimensions",
    "start": "755030",
    "end": "758170"
  },
  {
    "text": "you want and you just\nplug it into your network.",
    "start": "758170",
    "end": "761620"
  },
  {
    "text": "So we used an RNN with a word\nembedding on the reviews data.",
    "start": "761620",
    "end": "767210"
  },
  {
    "text": "And after a lot of work, the\nresults are disappointing 76%",
    "start": "767210",
    "end": "770860"
  },
  {
    "text": "accuracy, remember, we\ngot close to 89% accuracy.",
    "start": "770860",
    "end": "776019"
  },
  {
    "text": "So then we fit a more exotic\nrecurrent neural network",
    "start": "776020",
    "end": "779290"
  },
  {
    "text": "than the one that we displayed.",
    "start": "779290",
    "end": "781540"
  },
  {
    "text": "It's once known as\nan LSTM, which stands",
    "start": "781540",
    "end": "784630"
  },
  {
    "text": "for long and short term memory.",
    "start": "784630",
    "end": "786790"
  },
  {
    "text": "And without getting\ninto great detail,",
    "start": "786790",
    "end": "789160"
  },
  {
    "text": "here Al receives\ninput both from Al",
    "start": "789160",
    "end": "793069"
  },
  {
    "text": "minus 1, the previous hidden\nunit, that's the short term",
    "start": "793070",
    "end": "797110"
  },
  {
    "text": "memory, as well as\nfrom a version that",
    "start": "797110",
    "end": "799360"
  },
  {
    "text": "reaches further back in time.",
    "start": "799360",
    "end": "800870"
  },
  {
    "text": "So you take big\nsteps back in time",
    "start": "800870",
    "end": "803020"
  },
  {
    "text": "and get an Al back\nfrom there as well.",
    "start": "803020",
    "end": "806770"
  },
  {
    "text": "So that's the long term memory.",
    "start": "806770",
    "end": "809080"
  },
  {
    "text": "And when you do that, it\ntakes much longer to train.",
    "start": "809080",
    "end": "812530"
  },
  {
    "text": "You now get 87% accuracy,\nwhich is slightly less than 88%",
    "start": "812530",
    "end": "817330"
  },
  {
    "text": "achieved by glmnet.",
    "start": "817330",
    "end": "818780"
  },
  {
    "text": "It takes a long time to learn.",
    "start": "818780",
    "end": "820450"
  },
  {
    "text": "Now, just like in the MNIST\ndata and in the ResNet and all",
    "start": "820450",
    "end": "824320"
  },
  {
    "text": "these data bases,\nthese IMDB reviews",
    "start": "824320",
    "end": "829390"
  },
  {
    "text": "have been used as a benchmark\nfor new RNN architectures.",
    "start": "829390",
    "end": "833740"
  },
  {
    "text": "So the best reported result\nfound at the time of writing",
    "start": "833740",
    "end": "837010"
  },
  {
    "text": "was around 95%, so of course,\na much more complex network.",
    "start": "837010",
    "end": "842500"
  },
  {
    "text": "If you look in section 10.5.1\nof the book, the second edition,",
    "start": "842500",
    "end": "847510"
  },
  {
    "text": "will point to a leaderboard\nwhere you can see who's winning",
    "start": "847510",
    "end": "851830"
  },
  {
    "text": "the race.",
    "start": "851830",
    "end": "852680"
  },
  {
    "text": "And again, subject to\nover looking at the data.",
    "start": "852680",
    "end": "859970"
  },
  {
    "text": "Now this is something\nwe've seen pretty often.",
    "start": "859970",
    "end": "862339"
  },
  {
    "text": "Deep nets have some\nspectacular successes,",
    "start": "862340",
    "end": "864740"
  },
  {
    "text": "but on a lot of problems, they\ndon't do better or even they",
    "start": "864740",
    "end": "868399"
  },
  {
    "text": "do worse than simpler methods.",
    "start": "868400",
    "end": "870122"
  },
  {
    "text": "You don't hear\nabout that very much",
    "start": "870122",
    "end": "871580"
  },
  {
    "text": "because it's harder to\npublish simple things, right?",
    "start": "871580",
    "end": "873747"
  },
  {
    "text": "That's right.",
    "start": "873747",
    "end": "875300"
  },
  {
    "text": "So just a reminder to not only\ntry deep nets and fancy methods,",
    "start": "875300",
    "end": "879380"
  },
  {
    "text": "but also try simpler methods\nbecause they often work as well",
    "start": "879380",
    "end": "882470"
  },
  {
    "text": "and are much simpler\nto understand.",
    "start": "882470",
    "end": "884500"
  }
]