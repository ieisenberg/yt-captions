[
  {
    "start": "0",
    "end": "4857"
  },
  {
    "text": "SPEAKER: Welcome back, everyone.",
    "start": "4857",
    "end": "6189"
  },
  {
    "text": "This is part 4 in our series\non analysis methods for NLP.",
    "start": "6190",
    "end": "9600"
  },
  {
    "text": "We've come to our third set of\nmethods, causal abstraction.",
    "start": "9600",
    "end": "13950"
  },
  {
    "text": "I've been heavily involved\nwith developing these methods.",
    "start": "13950",
    "end": "16890"
  },
  {
    "text": "I think they're tremendously\nexciting because they offer",
    "start": "16890",
    "end": "19349"
  },
  {
    "text": "a real opportunity for\ncausal concept level",
    "start": "19350",
    "end": "22980"
  },
  {
    "text": "explanations of how our\nNLP models are behaving.",
    "start": "22980",
    "end": "27119"
  },
  {
    "text": "Let's begin with\na kind of recipe",
    "start": "27120",
    "end": "29220"
  },
  {
    "text": "for this causal\nabstraction analysis.",
    "start": "29220",
    "end": "31830"
  },
  {
    "text": "Step 1, you state a\nhypothesis about some aspect",
    "start": "31830",
    "end": "35910"
  },
  {
    "text": "of your target model's\ncausal structure.",
    "start": "35910",
    "end": "38250"
  },
  {
    "text": "And you could express this\nas kind of a small computer",
    "start": "38250",
    "end": "41490"
  },
  {
    "text": "program.",
    "start": "41490",
    "end": "42810"
  },
  {
    "text": "In step 2, we're going to\nsearch for an alignment",
    "start": "42810",
    "end": "45930"
  },
  {
    "text": "between variables in this causal\nmodel we've defined and sets",
    "start": "45930",
    "end": "50250"
  },
  {
    "text": "of neurons in the target model.",
    "start": "50250",
    "end": "52950"
  },
  {
    "text": "This is a hypothesis about how\nthe roles for those variables",
    "start": "52950",
    "end": "56430"
  },
  {
    "text": "and sets of neurons\nalign with each other.",
    "start": "56430",
    "end": "60600"
  },
  {
    "text": "To do this kind of analysis,\nto assess these alignments,",
    "start": "60600",
    "end": "63719"
  },
  {
    "text": "we perform the\nfundamental operation",
    "start": "63720",
    "end": "65820"
  },
  {
    "text": "of causal abstraction analysis,\nthe interchange intervention.",
    "start": "65820",
    "end": "70020"
  },
  {
    "text": "And much of this\nscreencast is going",
    "start": "70020",
    "end": "72060"
  },
  {
    "text": "to be devoted to giving\nyou for a feel for how",
    "start": "72060",
    "end": "74490"
  },
  {
    "text": "interchange interventions work.",
    "start": "74490",
    "end": "77549"
  },
  {
    "text": "For a running\nexample, let's return",
    "start": "77550",
    "end": "79680"
  },
  {
    "text": "to our simple neural network\nthat takes in three numbers",
    "start": "79680",
    "end": "82590"
  },
  {
    "text": "and adds them together.",
    "start": "82590",
    "end": "84329"
  },
  {
    "text": "We assume that this network\nis successful at its task.",
    "start": "84330",
    "end": "87660"
  },
  {
    "text": "And the question is, in\nhuman interpretable terms,",
    "start": "87660",
    "end": "90930"
  },
  {
    "text": "how does the network\nperform this function?",
    "start": "90930",
    "end": "94830"
  },
  {
    "text": "As before, we can\nhypothesize a causal model",
    "start": "94830",
    "end": "98010"
  },
  {
    "text": "that's given in green here.",
    "start": "98010",
    "end": "99670"
  },
  {
    "text": "And the idea behind\nthis causal model",
    "start": "99670",
    "end": "101640"
  },
  {
    "text": "is that the network is\nadding together the first two",
    "start": "101640",
    "end": "104729"
  },
  {
    "text": "inputs to form an\nintermediate variable S1.",
    "start": "104730",
    "end": "108550"
  },
  {
    "text": "And then the third\ninput is copied over",
    "start": "108550",
    "end": "110740"
  },
  {
    "text": "into an intermediate variable w.",
    "start": "110740",
    "end": "112930"
  },
  {
    "text": "And S1 and w are the elements\nthat directly contribute",
    "start": "112930",
    "end": "117070"
  },
  {
    "text": "to the output of the model.",
    "start": "117070",
    "end": "118900"
  },
  {
    "text": "That's a hypothesis\nabout what might",
    "start": "118900",
    "end": "121030"
  },
  {
    "text": "be happening with our\notherwise opaque neural model.",
    "start": "121030",
    "end": "124310"
  },
  {
    "text": "And the question is, is\nthe hypothesis correct?",
    "start": "124310",
    "end": "127820"
  },
  {
    "text": "We're going to use interchange\ninterventions to help",
    "start": "127820",
    "end": "130340"
  },
  {
    "text": "us assess that hypothesis.",
    "start": "130340",
    "end": "132629"
  },
  {
    "text": "We'll break this down\ninto a few pieces.",
    "start": "132630",
    "end": "134550"
  },
  {
    "text": "First, we hypothesize that\nthe neural representation, L3,",
    "start": "134550",
    "end": "139340"
  },
  {
    "text": "plays the same role as S1.",
    "start": "139340",
    "end": "142760"
  },
  {
    "text": "Let's assess that idea.",
    "start": "142760",
    "end": "143930"
  },
  {
    "text": "The first intervention\nhappens on the causal model.",
    "start": "143930",
    "end": "147299"
  },
  {
    "text": "So we take our causal model,\nand we process example 1, 2, 5,",
    "start": "147300",
    "end": "150530"
  },
  {
    "text": "and we get 9.",
    "start": "150530",
    "end": "151790"
  },
  {
    "text": "We use that same causal model to\nprocess 4, 5, 6, and we get 15.",
    "start": "151790",
    "end": "157640"
  },
  {
    "text": "And now the intervention comes.",
    "start": "157640",
    "end": "159319"
  },
  {
    "text": "We're going to target\nthe S1 variable",
    "start": "159320",
    "end": "161480"
  },
  {
    "text": "for the right-hand\nexample that has value 9,",
    "start": "161480",
    "end": "164150"
  },
  {
    "text": "literally take that\nvalue, and place it",
    "start": "164150",
    "end": "166790"
  },
  {
    "text": "in the corresponding place\nin the left-hand example.",
    "start": "166790",
    "end": "170480"
  },
  {
    "text": "The causal model is\ncompletely understood by us.",
    "start": "170480",
    "end": "172930"
  },
  {
    "text": "Since so we know exactly\nwhat will happen now,",
    "start": "172930",
    "end": "175359"
  },
  {
    "text": "the output will change to 14.",
    "start": "175360",
    "end": "177670"
  },
  {
    "text": "The childNodes below the\nvariable that we intervened on,",
    "start": "177670",
    "end": "180970"
  },
  {
    "text": "don't matter in this case.",
    "start": "180970",
    "end": "182420"
  },
  {
    "text": "The intervention\nfully wipes them out.",
    "start": "182420",
    "end": "184420"
  },
  {
    "text": "And we're just adding\n9 and 5 together.",
    "start": "184420",
    "end": "188000"
  },
  {
    "text": "That's the causal model, we\nassume that we understand it",
    "start": "188000",
    "end": "191060"
  },
  {
    "text": "before we begin the analysis.",
    "start": "191060",
    "end": "193099"
  },
  {
    "text": "The interesting\npart comes when we",
    "start": "193100",
    "end": "194870"
  },
  {
    "text": "think about the neural model.",
    "start": "194870",
    "end": "196519"
  },
  {
    "text": "We don't know how this\nneural model works,",
    "start": "196520",
    "end": "198680"
  },
  {
    "text": "and we're going to try to use\nthese interventions to uncover",
    "start": "198680",
    "end": "201409"
  },
  {
    "text": "that.",
    "start": "201410",
    "end": "202100"
  },
  {
    "text": "So we process 1, 3, 5 with\nour neural model and we get 9.",
    "start": "202100",
    "end": "206900"
  },
  {
    "text": "We process 4, 5,\n6 and we get 15.",
    "start": "206900",
    "end": "210260"
  },
  {
    "text": "And now, we're going to\nintervene on the L3 state,",
    "start": "210260",
    "end": "213409"
  },
  {
    "text": "we target that in the\nright-hand example.",
    "start": "213410",
    "end": "215900"
  },
  {
    "text": "And we literally\ntake those values",
    "start": "215900",
    "end": "217939"
  },
  {
    "text": "and place them in the\ncorresponding spot",
    "start": "217940",
    "end": "220640"
  },
  {
    "text": "in the left-hand example.",
    "start": "220640",
    "end": "222380"
  },
  {
    "text": "And we study the output.",
    "start": "222380",
    "end": "224310"
  },
  {
    "text": "And if the output after\nthat intervention is 14,",
    "start": "224310",
    "end": "228290"
  },
  {
    "text": "then we have one\npiece of evidence",
    "start": "228290",
    "end": "230689"
  },
  {
    "text": "that L3 plays the same\ncausal role as S1.",
    "start": "230690",
    "end": "234920"
  },
  {
    "text": "And if we repeat\nthis intervention",
    "start": "234920",
    "end": "236750"
  },
  {
    "text": "for every conceivable\ninput to these models",
    "start": "236750",
    "end": "240290"
  },
  {
    "text": "and we always see this alignment\nbetween causal model and neural",
    "start": "240290",
    "end": "244189"
  },
  {
    "text": "model, we have proven that\nL3 plays the same causal role",
    "start": "244190",
    "end": "249090"
  },
  {
    "text": "as S1.",
    "start": "249090",
    "end": "250200"
  },
  {
    "text": "We can continue this\nfor other variables.",
    "start": "250200",
    "end": "252330"
  },
  {
    "text": "Let's target now L1.",
    "start": "252330",
    "end": "253740"
  },
  {
    "text": "Suppose we hypothesize\nthat it plays the same role",
    "start": "253740",
    "end": "256799"
  },
  {
    "text": "as W in the causal model.",
    "start": "256800",
    "end": "259500"
  },
  {
    "text": "Again, let's first intervene\non the causal model.",
    "start": "259500",
    "end": "261850"
  },
  {
    "text": "We target that w variable on the\nright-hand, we take that value,",
    "start": "261850",
    "end": "265980"
  },
  {
    "text": "and we place it in the\ncorresponding place",
    "start": "265980",
    "end": "268050"
  },
  {
    "text": "in the left-hand model.",
    "start": "268050",
    "end": "269580"
  },
  {
    "text": "We study the output that has\nchanged the output to 10.",
    "start": "269580",
    "end": "273270"
  },
  {
    "text": "And then we return to our neural\nmodels, parallel operation.",
    "start": "273270",
    "end": "276569"
  },
  {
    "text": "Target L1 on the right.",
    "start": "276570",
    "end": "278710"
  },
  {
    "text": "Take that value and\nliterally place it",
    "start": "278710",
    "end": "281039"
  },
  {
    "text": "into the corresponding\nspot in the left",
    "start": "281040",
    "end": "283320"
  },
  {
    "text": "and we study the output.",
    "start": "283320",
    "end": "284980"
  },
  {
    "text": "And again, if the\noutput is 10, we",
    "start": "284980",
    "end": "286740"
  },
  {
    "text": "have a single piece of evidence\nthat L1 and w are causally",
    "start": "286740",
    "end": "290970"
  },
  {
    "text": "aligned in this way.",
    "start": "290970",
    "end": "292600"
  },
  {
    "text": "And if we repeat\nthis intervention",
    "start": "292600",
    "end": "294270"
  },
  {
    "text": "for every possible input and\nalways see this correspondence,",
    "start": "294270",
    "end": "297569"
  },
  {
    "text": "we have proven that L1 and w\nplay the same causal roles.",
    "start": "297570",
    "end": "302520"
  },
  {
    "text": "And we could go\none step further.",
    "start": "302520",
    "end": "304259"
  },
  {
    "text": "Suppose we think about L2,\nsuppose we intervene on L2",
    "start": "304260",
    "end": "308310"
  },
  {
    "text": "in every way we can\nthink of and we never",
    "start": "308310",
    "end": "310980"
  },
  {
    "text": "see an impact on the output\nbehavior of the model.",
    "start": "310980",
    "end": "314190"
  },
  {
    "text": "In that way, we\nhave proven that L2",
    "start": "314190",
    "end": "316830"
  },
  {
    "text": "plays no causal role in\nthe input/output behavior",
    "start": "316830",
    "end": "319560"
  },
  {
    "text": "of this network.",
    "start": "319560",
    "end": "320940"
  },
  {
    "text": "And since we can assume that\nthe input variables are aligned",
    "start": "320940",
    "end": "324060"
  },
  {
    "text": "across causal and\nneural models, and we",
    "start": "324060",
    "end": "326250"
  },
  {
    "text": "can assume that the output\nvariables are aligned,",
    "start": "326250",
    "end": "328750"
  },
  {
    "text": "we have now fully proven all\nthese intervention experiments",
    "start": "328750",
    "end": "332700"
  },
  {
    "text": "that causal model in green is\nan abstraction of the otherwise",
    "start": "332700",
    "end": "337290"
  },
  {
    "text": "more complex neural model.",
    "start": "337290",
    "end": "339270"
  },
  {
    "text": "And that is exciting.",
    "start": "339270",
    "end": "340620"
  },
  {
    "text": "If we have actually\nestablished this,",
    "start": "340620",
    "end": "342430"
  },
  {
    "text": "then we are licensed to allow\nthe neural model to fall away.",
    "start": "342430",
    "end": "346080"
  },
  {
    "text": "And we can reason entirely\nin terms of the causal model,",
    "start": "346080",
    "end": "349500"
  },
  {
    "text": "secure that the two models\nare causally aligned.",
    "start": "349500",
    "end": "353550"
  },
  {
    "text": "They have the same\nunderlying mechanisms.",
    "start": "353550",
    "end": "357780"
  },
  {
    "text": "Now that is a kind of ideal of\ncausal abstraction analysis.",
    "start": "357780",
    "end": "361980"
  },
  {
    "text": "There are a few things\nfrom the real world",
    "start": "361980",
    "end": "364110"
  },
  {
    "text": "that are going to intervene.",
    "start": "364110",
    "end": "365520"
  },
  {
    "text": "The first is that\nwe can never perform",
    "start": "365520",
    "end": "367860"
  },
  {
    "text": "the full set of interventions.",
    "start": "367860",
    "end": "369360"
  },
  {
    "text": "For all realistic cases, there\nare too many inputs, even",
    "start": "369360",
    "end": "372840"
  },
  {
    "text": "for the case of my\ntiny addition network.",
    "start": "372840",
    "end": "375060"
  },
  {
    "text": "There is an infinitude\nof possible inputs.",
    "start": "375060",
    "end": "378240"
  },
  {
    "text": "We can't check them\nall, so we have",
    "start": "378240",
    "end": "379889"
  },
  {
    "text": "to pick a small\nsubset of examples.",
    "start": "379890",
    "end": "383190"
  },
  {
    "text": "And then otherwise,\nfor real models,",
    "start": "383190",
    "end": "385320"
  },
  {
    "text": "we're never going to see\nperfect causal abstraction",
    "start": "385320",
    "end": "388230"
  },
  {
    "text": "relationships because of the\nmessy nature of naturally",
    "start": "388230",
    "end": "391680"
  },
  {
    "text": "trained models that we use.",
    "start": "391680",
    "end": "394240"
  },
  {
    "text": "So we need some graded\nnotion of success.",
    "start": "394240",
    "end": "396729"
  },
  {
    "text": "And I think interchange\nintervention accuracy",
    "start": "396730",
    "end": "399180"
  },
  {
    "text": "is a good initial\nbaseline metric for that.",
    "start": "399180",
    "end": "402910"
  },
  {
    "text": "The IIA is the percentage\nof interchange interventions",
    "start": "402910",
    "end": "406500"
  },
  {
    "text": "that you performed\nthat lead to outputs",
    "start": "406500",
    "end": "408960"
  },
  {
    "text": "that match those\nof the causal model",
    "start": "408960",
    "end": "411120"
  },
  {
    "text": "under the chosen alignment.",
    "start": "411120",
    "end": "412470"
  },
  {
    "text": "So you can think of\nit as an accuracy",
    "start": "412470",
    "end": "414510"
  },
  {
    "text": "measure for your\nhypothesized alignment.",
    "start": "414510",
    "end": "418770"
  },
  {
    "text": "IIA is scaled in 0, 1 as with\na normal accuracy metric.",
    "start": "418770",
    "end": "423729"
  },
  {
    "text": "It can actually be\nabove task performance.",
    "start": "423730",
    "end": "426660"
  },
  {
    "text": "This is kind of striking, and it\nhas happened to us in practice.",
    "start": "426660",
    "end": "430320"
  },
  {
    "text": "If the interchange interventions\nput the model into a better",
    "start": "430320",
    "end": "433680"
  },
  {
    "text": "state than it was\nin originally, then",
    "start": "433680",
    "end": "435419"
  },
  {
    "text": "you might actually see\na boost in performance",
    "start": "435420",
    "end": "438090"
  },
  {
    "text": "from these Frankenstein\nexamples that you have created.",
    "start": "438090",
    "end": "442290"
  },
  {
    "text": "This is really fundamental here.",
    "start": "442290",
    "end": "444030"
  },
  {
    "text": "IIA is extremely\nsensitive to the set",
    "start": "444030",
    "end": "446639"
  },
  {
    "text": "of interchange interventions\nthat you decided to perform.",
    "start": "446640",
    "end": "449400"
  },
  {
    "text": "If you can't\nperform all of them,",
    "start": "449400",
    "end": "450840"
  },
  {
    "text": "you have to pick\na subset, and that",
    "start": "450840",
    "end": "452730"
  },
  {
    "text": "will be a factor in shaping\nyour accuracy results.",
    "start": "452730",
    "end": "456810"
  },
  {
    "text": "In particular, pay\nparticular attention",
    "start": "456810",
    "end": "459870"
  },
  {
    "text": "to how many interchange\ninterventions",
    "start": "459870",
    "end": "461760"
  },
  {
    "text": "should change the output label.",
    "start": "461760",
    "end": "463800"
  },
  {
    "text": "Those are the ones\nthat are really",
    "start": "463800",
    "end": "465360"
  },
  {
    "text": "providing causal\ninsights because you",
    "start": "465360",
    "end": "467669"
  },
  {
    "text": "see exactly what should\nhappen in terms of changes",
    "start": "467670",
    "end": "471030"
  },
  {
    "text": "once you have performed\nthe intervention.",
    "start": "471030",
    "end": "473860"
  },
  {
    "text": "So having an abundance of\nthese causally insightful",
    "start": "473860",
    "end": "477150"
  },
  {
    "text": "interventions is the\nmost powerful thing",
    "start": "477150",
    "end": "479130"
  },
  {
    "text": "you can do in terms of\nbuilding an argument.",
    "start": "479130",
    "end": "483660"
  },
  {
    "text": "Let me briefly\nsummarize some findings",
    "start": "483660",
    "end": "485850"
  },
  {
    "text": "from causal abstraction.",
    "start": "485850",
    "end": "487270"
  },
  {
    "text": "These are mostly from our work.",
    "start": "487270",
    "end": "489840"
  },
  {
    "text": "Fine-tuned BERT models succeed\nat hard, out-of-domain examples",
    "start": "489840",
    "end": "493830"
  },
  {
    "text": "involving lexical\nentailment and negation",
    "start": "493830",
    "end": "495930"
  },
  {
    "text": "because they are abstracted by\nsimple monotonicity programs.",
    "start": "495930",
    "end": "499590"
  },
  {
    "text": "And I emphasize \"because,\"\nand I wrote it in blue there,",
    "start": "499590",
    "end": "502540"
  },
  {
    "text": "because I am not being casual\nwith that causal language.",
    "start": "502540",
    "end": "505900"
  },
  {
    "text": "I really intend a causal claim.",
    "start": "505900",
    "end": "508330"
  },
  {
    "text": "That is the kind of thing\nthat causal abstraction",
    "start": "508330",
    "end": "511330"
  },
  {
    "text": "licenses you to be able to say.",
    "start": "511330",
    "end": "514539"
  },
  {
    "text": "Relatedly, fine-tuned\nBERT models",
    "start": "514539",
    "end": "517000"
  },
  {
    "text": "succeed at the MQNLI\ntask because they",
    "start": "517000",
    "end": "520690"
  },
  {
    "text": "find compositional solutions.",
    "start": "520690",
    "end": "522370"
  },
  {
    "text": "MQNLI is the multiply\nquantified NLI benchmark.",
    "start": "522370",
    "end": "525940"
  },
  {
    "text": "It's a synthetic benchmark\nfull of very intricate",
    "start": "525940",
    "end": "529180"
  },
  {
    "text": "compositional analyzes between\nquantifiers and modifiers",
    "start": "529180",
    "end": "533170"
  },
  {
    "text": "and so forth, a\nchallenging benchmark.",
    "start": "533170",
    "end": "535420"
  },
  {
    "text": "And we show with\ncausal abstraction",
    "start": "535420",
    "end": "537279"
  },
  {
    "text": "that models succeed\nto the extent",
    "start": "537280",
    "end": "539470"
  },
  {
    "text": "that they actually find\ncompositional solutions",
    "start": "539470",
    "end": "542439"
  },
  {
    "text": "to the task.",
    "start": "542440",
    "end": "544090"
  },
  {
    "text": "Models succeed at the MNIST\nPointer Value Retrieval Task",
    "start": "544090",
    "end": "547900"
  },
  {
    "text": "because they are abstracted\nby simple programs,",
    "start": "547900",
    "end": "550840"
  },
  {
    "text": "like if the digit is 6, then\nthe label is in the lower left.",
    "start": "550840",
    "end": "555330"
  },
  {
    "text": "A brief digression there.",
    "start": "555330",
    "end": "556750"
  },
  {
    "text": "I love these kinds\nof explanations.",
    "start": "556750",
    "end": "559020"
  },
  {
    "text": "That simple program\nthat I described",
    "start": "559020",
    "end": "561180"
  },
  {
    "text": "is more or less a\ndescription of the task.",
    "start": "561180",
    "end": "563649"
  },
  {
    "text": "And so it's\nwonderfully reassuring",
    "start": "563650",
    "end": "565320"
  },
  {
    "text": "to see that our\nexplanations actually",
    "start": "565320",
    "end": "567930"
  },
  {
    "text": "align with the task\nstructure for these very",
    "start": "567930",
    "end": "570420"
  },
  {
    "text": "successful models.",
    "start": "570420",
    "end": "571769"
  },
  {
    "text": "And another nice point\nhere is that we're",
    "start": "571770",
    "end": "573990"
  },
  {
    "text": "starting to see a blurring\nof the distinction",
    "start": "573990",
    "end": "576540"
  },
  {
    "text": "between neural models\nand symbolic models.",
    "start": "576540",
    "end": "579750"
  },
  {
    "text": "After all, if you can show\nthat the two are aligned",
    "start": "579750",
    "end": "582720"
  },
  {
    "text": "via causal abstraction,\nthen there really",
    "start": "582720",
    "end": "585180"
  },
  {
    "text": "is no meaningful difference\nbetween the two, which",
    "start": "585180",
    "end": "588450"
  },
  {
    "text": "leads you to wonder\nwhether there's truly",
    "start": "588450",
    "end": "590430"
  },
  {
    "text": "a meaningful difference between\nsymbolic AI and neural AI.",
    "start": "590430",
    "end": "594779"
  },
  {
    "text": "They can certainly\ncome together,",
    "start": "594780",
    "end": "596800"
  },
  {
    "text": "and you see them coming\ntogether in these analyses.",
    "start": "596800",
    "end": "600930"
  },
  {
    "text": "Finally, BART and T5 use\ncoherent entity and situation",
    "start": "600930",
    "end": "604890"
  },
  {
    "text": "representations that evolve\nas the discourse unfolds.",
    "start": "604890",
    "end": "608380"
  },
  {
    "text": "Li et al., 2021 use causal\nabstraction in order",
    "start": "608380",
    "end": "612060"
  },
  {
    "text": "to substantiate that claim.",
    "start": "612060",
    "end": "614190"
  },
  {
    "text": "Very exciting to see.",
    "start": "614190",
    "end": "617170"
  },
  {
    "text": "If you would like to get\nhands on with these ideas,",
    "start": "617170",
    "end": "619540"
  },
  {
    "text": "I would encourage you to\ncheck out our notebook.",
    "start": "619540",
    "end": "621699"
  },
  {
    "text": "It's called IIT equality.",
    "start": "621700",
    "end": "623530"
  },
  {
    "text": "It walks through causal\nabstraction analysis",
    "start": "623530",
    "end": "625780"
  },
  {
    "text": "using simple toy examples.",
    "start": "625780",
    "end": "628150"
  },
  {
    "text": "And then also shows\nyou how to apply",
    "start": "628150",
    "end": "630430"
  },
  {
    "text": "IIT, which is the next\ntopic we'll discuss.",
    "start": "630430",
    "end": "634600"
  },
  {
    "text": "There isn't time to\ncover this in detail,",
    "start": "634600",
    "end": "636800"
  },
  {
    "text": "but I did want to call out\nthat causal abstraction is",
    "start": "636800",
    "end": "639970"
  },
  {
    "text": "a toolkit corresponding\nto a large family",
    "start": "639970",
    "end": "643300"
  },
  {
    "text": "of intervention-based methods\nfor understanding our models.",
    "start": "643300",
    "end": "647110"
  },
  {
    "text": "I've listed a few other exciting\nentries in this literature",
    "start": "647110",
    "end": "650380"
  },
  {
    "text": "here.",
    "start": "650380",
    "end": "651170"
  },
  {
    "text": "And if you would like even more\nconnections to the literature,",
    "start": "651170",
    "end": "654260"
  },
  {
    "text": "I recommend this\nblog post that we",
    "start": "654260",
    "end": "655900"
  },
  {
    "text": "did, which kind of relates\na lot of these methods",
    "start": "655900",
    "end": "658630"
  },
  {
    "text": "to causal abstraction itself.",
    "start": "658630",
    "end": "662320"
  },
  {
    "text": "So let's return to\nour summary scorecard.",
    "start": "662320",
    "end": "664890"
  },
  {
    "text": "We're talking about\nintervention-based methods.",
    "start": "664890",
    "end": "667750"
  },
  {
    "text": "I claim that they can\ncharacterize representations",
    "start": "667750",
    "end": "670330"
  },
  {
    "text": "richly.",
    "start": "670330",
    "end": "670830"
  },
  {
    "text": "After all, we show how\nthose representations",
    "start": "670830",
    "end": "673080"
  },
  {
    "text": "correspond to interpretable\nhigh level variables.",
    "start": "673080",
    "end": "677110"
  },
  {
    "text": "I've also tried to argue that\nthis is a causal inference",
    "start": "677110",
    "end": "680130"
  },
  {
    "text": "method.",
    "start": "680130",
    "end": "681150"
  },
  {
    "text": "And I still have a smiley\nunder improved models.",
    "start": "681150",
    "end": "683910"
  },
  {
    "text": "I have not substantiated\nthat for you next,",
    "start": "683910",
    "end": "686670"
  },
  {
    "text": "but that is the next\ntask under the heading",
    "start": "686670",
    "end": "689279"
  },
  {
    "text": "of interchange\nintervention training.",
    "start": "689280",
    "end": "691960"
  },
  {
    "text": "So let's turn to that now, IIT.",
    "start": "691960",
    "end": "695730"
  },
  {
    "text": "The method is quite\nsimple and builds directly",
    "start": "695730",
    "end": "698910"
  },
  {
    "text": "on causal abstraction with\ninterchange interventions.",
    "start": "698910",
    "end": "702089"
  },
  {
    "text": "Here's a kind of summary diagram\nof interchange intervention",
    "start": "702090",
    "end": "706110"
  },
  {
    "text": "using our addition\nexample with the one twist",
    "start": "706110",
    "end": "709680"
  },
  {
    "text": "that you'll notice that\nmy intervention now for L3",
    "start": "709680",
    "end": "712890"
  },
  {
    "text": "has led to an incorrect result.\nWe wanted 14 and we got 4.",
    "start": "712890",
    "end": "718950"
  },
  {
    "text": "So we have, in some sense, shown\nthat our hypothesis alignment",
    "start": "718950",
    "end": "723060"
  },
  {
    "text": "between these variables\nis not correct.",
    "start": "723060",
    "end": "725830"
  },
  {
    "text": "But I think you can also\nsee in here an opportunity",
    "start": "725830",
    "end": "729000"
  },
  {
    "text": "to do better.",
    "start": "729000",
    "end": "729760"
  },
  {
    "text": "We can correct this\nmisalignment, if we want to.",
    "start": "729760",
    "end": "732660"
  },
  {
    "text": "After all, we know what\nthe label should have been,",
    "start": "732660",
    "end": "736079"
  },
  {
    "text": "and we know what it was, and\nthat gives us a gradient signal",
    "start": "736080",
    "end": "740700"
  },
  {
    "text": "that we can use to update\nthe parameters of this model",
    "start": "740700",
    "end": "744210"
  },
  {
    "text": "and make it more conform to\nour underlying causal model",
    "start": "744210",
    "end": "748380"
  },
  {
    "text": "under this alignment.",
    "start": "748380",
    "end": "749980"
  },
  {
    "text": "Let's see how that\nwould play out.",
    "start": "749980",
    "end": "751440"
  },
  {
    "text": "We get our error\nsignal and that flows",
    "start": "751440",
    "end": "753330"
  },
  {
    "text": "back as usual to the hidden\nstates L1, L2, and L3.",
    "start": "753330",
    "end": "758310"
  },
  {
    "text": "And for L1, the gradients flow\nback as usual to the input",
    "start": "758310",
    "end": "762120"
  },
  {
    "text": "states.",
    "start": "762120",
    "end": "762960"
  },
  {
    "text": "And the same thing\nis true for L2.",
    "start": "762960",
    "end": "765450"
  },
  {
    "text": "But for L3, we have a\nmore complicated update.",
    "start": "765450",
    "end": "768720"
  },
  {
    "text": "We have literally copied\nover the full computation",
    "start": "768720",
    "end": "772050"
  },
  {
    "text": "graph in the PyTorch\nsense, including",
    "start": "772050",
    "end": "774600"
  },
  {
    "text": "all the gradient information.",
    "start": "774600",
    "end": "776410"
  },
  {
    "text": "And so what we get for L3 is\na kind of double update coming",
    "start": "776410",
    "end": "780269"
  },
  {
    "text": "from our current\nexample as well as",
    "start": "780270",
    "end": "782580"
  },
  {
    "text": "the source example, which also\nprocessed that representation.",
    "start": "782580",
    "end": "786520"
  },
  {
    "text": "So we get a double update.",
    "start": "786520",
    "end": "788250"
  },
  {
    "text": "The result of repeatedly\nperforming these IIT updates",
    "start": "788250",
    "end": "792600"
  },
  {
    "text": "on these models using the\ncausal model for the labels,",
    "start": "792600",
    "end": "796019"
  },
  {
    "text": "as we've done here,\nis that we push",
    "start": "796020",
    "end": "798390"
  },
  {
    "text": "the model to\nmodularize information",
    "start": "798390",
    "end": "800940"
  },
  {
    "text": "about S1, in this case\nin the L3 variable.",
    "start": "800940",
    "end": "805000"
  },
  {
    "text": "So the importance of\nalignments kind of falls away",
    "start": "805000",
    "end": "808080"
  },
  {
    "text": "and the emphasis here is\non actually pushing models,",
    "start": "808080",
    "end": "811080"
  },
  {
    "text": "improving them by\nmaking them have",
    "start": "811080",
    "end": "813870"
  },
  {
    "text": "the causal structure that we\nhave hypothesized in the hopes",
    "start": "813870",
    "end": "817410"
  },
  {
    "text": "that they will then perform\nin more systematic ways",
    "start": "817410",
    "end": "820149"
  },
  {
    "text": "and be better at the\ntasks we've set for them.",
    "start": "820150",
    "end": "825000"
  },
  {
    "text": "Findings from IIT.",
    "start": "825000",
    "end": "827140"
  },
  {
    "text": "We showed that IIT achieves\nstate-of-the-art results",
    "start": "827140",
    "end": "831150"
  },
  {
    "text": "on that MNIST Pointer Value\nRetrieval Task that I mentioned",
    "start": "831150",
    "end": "834090"
  },
  {
    "text": "before, as well as ReaSCAN,\nwhich is a grounded language",
    "start": "834090",
    "end": "837000"
  },
  {
    "text": "understanding benchmark.",
    "start": "837000",
    "end": "838750"
  },
  {
    "text": "We also showed that it can\nbe used as a distillation",
    "start": "838750",
    "end": "842790"
  },
  {
    "text": "objective where\nessentially what we do",
    "start": "842790",
    "end": "845579"
  },
  {
    "text": "is distill teacher models\ninto student models,",
    "start": "845580",
    "end": "848640"
  },
  {
    "text": "forcing them not only to conform\nin their input/output behavior,",
    "start": "848640",
    "end": "852180"
  },
  {
    "text": "but also conform at the level of\ntheir internal representations",
    "start": "852180",
    "end": "856440"
  },
  {
    "text": "under the counterfactuals\nthat we create for IIT.",
    "start": "856440",
    "end": "859960"
  },
  {
    "text": "This is exciting to me\nbecause I think it's",
    "start": "859960",
    "end": "861810"
  },
  {
    "text": "a powerful distillation method.",
    "start": "861810",
    "end": "863820"
  },
  {
    "text": "And it also shows you that\nthe causal model that we use",
    "start": "863820",
    "end": "867330"
  },
  {
    "text": "for IIT can be quite abstract.",
    "start": "867330",
    "end": "869400"
  },
  {
    "text": "In this case, it's just a\nkind of high level constraint",
    "start": "869400",
    "end": "872280"
  },
  {
    "text": "on what we want the teacher and\nstudent models to look like.",
    "start": "872280",
    "end": "877290"
  },
  {
    "text": "We also showed\nthat it can be used",
    "start": "877290",
    "end": "879170"
  },
  {
    "text": "to induce internal\nrepresentations of characters",
    "start": "879170",
    "end": "882079"
  },
  {
    "text": "in language models that are\nbased in subword tokenization.",
    "start": "882080",
    "end": "885710"
  },
  {
    "text": "And we showed that this helps\nwith a variety of character",
    "start": "885710",
    "end": "888380"
  },
  {
    "text": "level games and tasks.",
    "start": "888380",
    "end": "890000"
  },
  {
    "text": "This is IIT being used\nto strike a balance.",
    "start": "890000",
    "end": "892820"
  },
  {
    "text": "Subword models seem to be\nour best language models,",
    "start": "892820",
    "end": "896180"
  },
  {
    "text": "but we have tasks that require\nknowledge of characters.",
    "start": "896180",
    "end": "899930"
  },
  {
    "text": "And what we do with IIT\nis imbue these models",
    "start": "899930",
    "end": "902420"
  },
  {
    "text": "with knowledge of characters\nin their internal states.",
    "start": "902420",
    "end": "906940"
  },
  {
    "text": "And finally, we\nrecently used IIT",
    "start": "906940",
    "end": "909280"
  },
  {
    "text": "to create concept level methods\nfor explaining model behavior.",
    "start": "909280",
    "end": "913240"
  },
  {
    "text": "That's a technique that we\ncall causal proxy models.",
    "start": "913240",
    "end": "916120"
  },
  {
    "text": "And it essentially leverages\nthe core insight of IIT.",
    "start": "916120",
    "end": "921730"
  },
  {
    "text": "Again, we have this course\nnotebook, IIT equality.",
    "start": "921730",
    "end": "924820"
  },
  {
    "text": "It covers abstraction\nanalysis, and then also",
    "start": "924820",
    "end": "927610"
  },
  {
    "text": "shows you how to train\nmodels in this IIT mode.",
    "start": "927610",
    "end": "932079"
  },
  {
    "text": "So we can return\nto our scorecard.",
    "start": "932080",
    "end": "934370"
  },
  {
    "text": "Now I have smileys\nacross the board.",
    "start": "934370",
    "end": "936740"
  },
  {
    "text": "And I claim that I have\njustified all of those smileys.",
    "start": "936740",
    "end": "939730"
  },
  {
    "text": "And I feel that this does point\nto intervention-based methods",
    "start": "939730",
    "end": "942970"
  },
  {
    "text": "as the best bet we have\nfor deeply understanding",
    "start": "942970",
    "end": "946990"
  },
  {
    "text": "how NLP models work.",
    "start": "946990",
    "end": "949950"
  },
  {
    "start": "949950",
    "end": "954000"
  }
]