[
  {
    "start": "0",
    "end": "113000"
  },
  {
    "text": "This is lecture 23 of CS229. Um, so, uh, today we're gonna just continue the- the, uh,",
    "start": "4280",
    "end": "12335"
  },
  {
    "text": "finals review that we started last class and, uh, we'll finish up the final review. And, uh, that's gonna be it.",
    "start": "12335",
    "end": "18730"
  },
  {
    "text": "So, uh, we- we might finish a little early today. All right.",
    "start": "18730",
    "end": "24385"
  },
  {
    "text": "Um, continuing the- the, um, the, uh, final review. So in the last class we started off with supervised learning.",
    "start": "24385",
    "end": "31795"
  },
  {
    "text": "We- we, uh, uh, we kind of again went through linear regression, all the different interpretations of linear regression,",
    "start": "31795",
    "end": "38440"
  },
  {
    "text": "uh, such as, uh, minimizing the square loss, the probabilistic interpretation, the projection interpretation,",
    "start": "38440",
    "end": "43689"
  },
  {
    "text": "the normal equations, um, and then we moved on to logistic regression, uh, which is, um, a model that you can use for classification.",
    "start": "43689",
    "end": "52405"
  },
  {
    "text": "Logistic regression outputs a probability, uh, for each, uh, for each example of, uh,",
    "start": "52405",
    "end": "57545"
  },
  {
    "text": "the probability that the- the, uh, uh, class is equal to 1, and then you can use a threshold and convert it into a classifier.",
    "start": "57545",
    "end": "63725"
  },
  {
    "text": "And then we, uh, spoke about Newton's method, uh, which is another optimization problem that works well for convex or concave problems.",
    "start": "63725",
    "end": "72305"
  },
  {
    "text": "And, uh, the- the key summary from Newton's method is that it can be very efficient, uh, in terms of converging quickly.",
    "start": "72305",
    "end": "78965"
  },
  {
    "text": "But Newton's method is kind of plug and play, which means it automatically performs optimization.",
    "start": "78965",
    "end": "85339"
  },
  {
    "text": "You don't need to specify whether you want to maximize a function or minimize a function. You just throw a function at it and it optimizes, uh,",
    "start": "85339",
    "end": "92600"
  },
  {
    "text": "by finding the nearest stationary point. Which means if your function is concave- convex,",
    "start": "92600",
    "end": "99530"
  },
  {
    "text": "then it automatically minimizes it for you. If it's concave, it automatically maximizes it for you.",
    "start": "99530",
    "end": "105305"
  },
  {
    "text": "If it is neither convex or concave, it just takes you to the nearest, um, um, nearest, uh, stationary point, right?",
    "start": "105305",
    "end": "111619"
  },
  {
    "text": "That was Newton's method. Moving on. So, uh, there was this other algorithm that we saw called the perceptron algorithm.",
    "start": "111620",
    "end": "120479"
  },
  {
    "start": "113000",
    "end": "485000"
  },
  {
    "text": "Okay? So the perceptron algorithm was a streaming algorithm. By streaming we mean it's an algorithm where you encounter one example at a time.",
    "start": "120980",
    "end": "129950"
  },
  {
    "text": "You know, think of it like stochastic gradient descent variable where you are encountering one example at a time, right?",
    "start": "129950",
    "end": "135405"
  },
  {
    "text": "And the perceptron algorithm had a very simple update rule, which was, uh,",
    "start": "135405",
    "end": "140645"
  },
  {
    "text": "Theta equals Theta plus Alpha",
    "start": "140645",
    "end": "145790"
  },
  {
    "text": "times y minus g of Theta transpose x.",
    "start": "145790",
    "end": "154790"
  },
  {
    "text": "Right? Where g was just an indicator function, which would return 1 if Theta transpose x is greater than or equal to 0,",
    "start": "154790",
    "end": "162950"
  },
  {
    "text": "or return 0 if Theta transpose x was less than 0.",
    "start": "162950",
    "end": "168474"
  },
  {
    "text": "So it's just the indicator function which returns 1 if z is greater than equal to 0,",
    "start": "168475",
    "end": "174395"
  },
  {
    "text": "return 0 if z is less than 0, right? Um, and the- the idea here was that supposing,",
    "start": "174395",
    "end": "182670"
  },
  {
    "text": "uh, you have some examples, let's, uh, let's use some colors. [NOISE] So let's say you have some positive examples.",
    "start": "182670",
    "end": "197670"
  },
  {
    "text": "Okay? And you have some negative examples.",
    "start": "198070",
    "end": "202410"
  },
  {
    "text": "And for our purposes, suppose this is the origin and you have",
    "start": "204980",
    "end": "210820"
  },
  {
    "text": "a Theta vector that's currently pointing in some direction, right? Generally you want Theta- the Theta vector to",
    "start": "210820",
    "end": "217240"
  },
  {
    "text": "point in the direction of your- of your, uh, positive class. So the- the intuition behind the way, um, um,",
    "start": "217240",
    "end": "224030"
  },
  {
    "text": "the perceptron algorithm works is that if your answer is correct, that is,",
    "start": "224030",
    "end": "230140"
  },
  {
    "text": "if Theta transpose x is greater than to 0, then this whole term evaluates to 0,",
    "start": "230140",
    "end": "235644"
  },
  {
    "text": "if you make a correct prediction, right? And so you- you- you don't change your, um, you don't change your parameters at all.",
    "start": "235645",
    "end": "242670"
  },
  {
    "text": "However, if you make a mistake, that is if the correct answer was 1 and you predicted a 0,",
    "start": "242670",
    "end": "248005"
  },
  {
    "text": "that means that, uh, you want to, uh, this will then end up,",
    "start": "248005",
    "end": "254490"
  },
  {
    "text": "uh, I forgot x here, okay? This will then end up adding a small, uh,",
    "start": "254490",
    "end": "260340"
  },
  {
    "text": "uh, small scalar times x to your parameter vector. Okay? Which means if this is, uh,",
    "start": "260340",
    "end": "266275"
  },
  {
    "text": "where a Theta is pointing and the new example, uh, x is,",
    "start": "266275",
    "end": "271919"
  },
  {
    "text": "let's say over here, um, over here,",
    "start": "271920",
    "end": "277075"
  },
  {
    "text": "then the decision boundary of the, uh, uh, current parameter vector would be something like this.",
    "start": "277075",
    "end": "286200"
  },
  {
    "text": "And let's say the, uh, uh,",
    "start": "286200",
    "end": "292215"
  },
  {
    "text": "to make it easier, let's say, uh, the- the, uh, uh, example was here.",
    "start": "292215",
    "end": "298740"
  },
  {
    "text": "And so the- the, uh, the decision boundary is like this. And so, um, this- this, uh,",
    "start": "298740",
    "end": "306175"
  },
  {
    "text": "example is now predicted as a negative because it is- it is on the other side of the decision boundary.",
    "start": "306175",
    "end": "311910"
  },
  {
    "text": "So what we wanna do is now is to take this x vector. Okay? This is the, uh,",
    "start": "311910",
    "end": "317270"
  },
  {
    "text": "x vector which got misclassified. Right? Take a small for a, uh, a small scalar times this, um,",
    "start": "317270",
    "end": "323690"
  },
  {
    "text": "uh, x vector and add it to Theta vector.",
    "start": "323690",
    "end": "329325"
  },
  {
    "text": "So which means you add a small component. All right? So that we call this Alpha times x,",
    "start": "329325",
    "end": "336630"
  },
  {
    "text": "and now this becomes the updated parameter vector. All right? And in this updated parameter vector for- for this,",
    "start": "336630",
    "end": "343824"
  },
  {
    "text": "the separating, uh, hyperplane which is perpendicular to it. This would be the updated separating hyperplane.",
    "start": "343825",
    "end": "351480"
  },
  {
    "text": "And now we see that the, uh, x is now correctly classified, right? So the- the general idea here,",
    "start": "351480",
    "end": "357075"
  },
  {
    "text": "the- the larger take-home message is that when you want some vector, Theta vector,",
    "start": "357075",
    "end": "363965"
  },
  {
    "text": "to take a value that's- that's closer or, uh, is oriented closer to some desired vector,",
    "start": "363965",
    "end": "371125"
  },
  {
    "text": "then a thing you can do, or an obvious thing you can do is to add",
    "start": "371125",
    "end": "376355"
  },
  {
    "text": "a small scalar times the vector to the desired vector, right?",
    "start": "376355",
    "end": "381500"
  },
  {
    "text": "That is, Theta plus Alpha times x,",
    "start": "381500",
    "end": "386370"
  },
  {
    "text": "dot product with x will always",
    "start": "387200",
    "end": "393050"
  },
  {
    "text": "be greater than Theta transpose x, right?",
    "start": "393050",
    "end": "398220"
  },
  {
    "text": "That's- that's- that's the take, uh, take-home summary that you wanna add a small, uh, uh, scalar times the vector to- to Theta to make Theta be oriented closer to,",
    "start": "398220",
    "end": "407300"
  },
  {
    "text": "uh, closer to x. I think that's last perceptron algorithm. Adding vectors make them- make them similar.",
    "start": "407300",
    "end": "413810"
  },
  {
    "text": "That's- that's the take-home message. Okay? And we saw, uh, in fact you- you also implemented the perceptron algorithm in, uh,",
    "start": "413810",
    "end": "422340"
  },
  {
    "text": "I think it was Homework 2 where, uh, we took this algorithm and then analyzed it. And- and, uh, you guys also, you know,",
    "start": "422340",
    "end": "429110"
  },
  {
    "text": "plotted the separating boundaries and whatnot, right? So that was- that was perceptron.",
    "start": "429110",
    "end": "434215"
  },
  {
    "text": "And in the perceptron algorithm, uh, there is theory which we did not cover in- in the course,",
    "start": "434215",
    "end": "439250"
  },
  {
    "text": "but this theory to show that, uh, if a separating hyperplane does exist,",
    "start": "439250",
    "end": "445145"
  },
  {
    "text": "then no matter in what order you present the examples to the learning algorithm,",
    "start": "445145",
    "end": "450664"
  },
  {
    "text": "it will eventually find some separating hyperplane. Okay? That was- that was, uh, that was the- the, uh, uh,",
    "start": "450665",
    "end": "457035"
  },
  {
    "text": "property of a perceptron algorithms that no matter what- what order you present the examples in your stream,",
    "start": "457035",
    "end": "462910"
  },
  {
    "text": "uh, as long as you- as you follow this update rule, if there is a- a separating hyperplane,",
    "start": "462910",
    "end": "469090"
  },
  {
    "text": "it will end up recovering that hyperplane. And once you get a- a- a separating hyperplane,",
    "start": "469090",
    "end": "474460"
  },
  {
    "text": "uh, all the updates beyond that point will always be 0. So it- it would have converged to,",
    "start": "474460",
    "end": "479995"
  },
  {
    "text": "uh, one of the- one of the possible separating hyperplanes. And after, uh, the perceptron,",
    "start": "479995",
    "end": "486100"
  },
  {
    "start": "485000",
    "end": "690000"
  },
  {
    "text": "we moved on to something called the exponential family. So exponential family is",
    "start": "486100",
    "end": "494440"
  },
  {
    "text": "a family of probability distributions which have the general form, p of y parameterized by Eta is equal to b of y times exponent",
    "start": "494440",
    "end": "508280"
  },
  {
    "text": "of eta transpose T of y minus a of Eta, right?",
    "start": "508280",
    "end": "517175"
  },
  {
    "text": "Where y is the- is the, uh, uh, uh, is the variable.",
    "start": "517175",
    "end": "523265"
  },
  {
    "text": "Y is the variable. T of y is called a sufficient statistic.",
    "start": "523265",
    "end": "529710"
  },
  {
    "text": "Statistic. And for most of the problems that we consider, especially with our generalized linear models,",
    "start": "531950",
    "end": "538585"
  },
  {
    "text": "T of y would mostly be equal to y for the purposes of our class, right?",
    "start": "538585",
    "end": "543975"
  },
  {
    "text": "So our T of y is the sufficient statistics. Statistic. Eta is called the natural parameter, natural parameter.",
    "start": "543975",
    "end": "556100"
  },
  {
    "text": "And b of y is called the base measure.",
    "start": "556100",
    "end": "560699"
  },
  {
    "text": "And a of Eta is called the log partition function.",
    "start": "562190",
    "end": "568650"
  },
  {
    "text": "Right? And this exponential family covers many different kinds of variables.",
    "start": "572880",
    "end": "579475"
  },
  {
    "text": "Exponential families can- can, um, um, encompasses discrete random variables,",
    "start": "579475",
    "end": "585069"
  },
  {
    "text": "continuous random variables, positive only random variables, uh, um, integer, uh, random variables.",
    "start": "585070",
    "end": "591370"
  },
  {
    "text": "Um, uh, it- it is- it is pretty, uh, pretty flexible in terms of the different kinds of supports the,",
    "start": "591370",
    "end": "597550"
  },
  {
    "text": "uh, random variable can have. And so, um, as a consequence, when we define a generalized linear model- generalized linear model,",
    "start": "597550",
    "end": "606505"
  },
  {
    "text": "where we take an exponential family and set Eta equals Theta_transpose_x,",
    "start": "606505",
    "end": "612625"
  },
  {
    "text": "where Theta is now your learnable parameter - learnable parameter",
    "start": "612625",
    "end": "619930"
  },
  {
    "text": "that we learned through gradient ascent or descent. And x's are the inputs that correspond to y, right?",
    "start": "619930",
    "end": "627100"
  },
  {
    "text": "Once we- once we kind of connect the inputs and outputs using this linear model,",
    "start": "627100",
    "end": "633370"
  },
  {
    "text": "then the- the generalized linear model can therefore be- is- is- is this more general form of regression,",
    "start": "633370",
    "end": "642930"
  },
  {
    "text": "classification, you know, um, uh, Poisson regression, etc, where depending on the data type of y or the support of y,",
    "start": "642930",
    "end": "650565"
  },
  {
    "text": "we get all these different- different, um, different models. For example, when y is just a real value, you get regression.",
    "start": "650565",
    "end": "658785"
  },
  {
    "text": "When y is binary between 0 and 1, you get a classification, okay?",
    "start": "658785",
    "end": "664490"
  },
  {
    "text": "And- and, um, based on- based on, uh,",
    "start": "664490",
    "end": "669850"
  },
  {
    "text": "this relation of how we extend exponential family to generalize linear model, where this is the linear model,",
    "start": "669850",
    "end": "677005"
  },
  {
    "text": "the sufficient- the natural parameter is- is set equal to, uh, Theta_transpose_x.",
    "start": "677005",
    "end": "683245"
  },
  {
    "text": "We get these, um, we get these, uh, uh, we also showed these special properties.",
    "start": "683245",
    "end": "689575"
  },
  {
    "text": "So first of all, the expectation of T_y,",
    "start": "689575",
    "end": "696100"
  },
  {
    "start": "690000",
    "end": "1210000"
  },
  {
    "text": "or in general just the, uh, expectation of y, where- where, uh,",
    "start": "696100",
    "end": "702385"
  },
  {
    "text": "T_y equals y is equal to a prime of Eta.",
    "start": "702385",
    "end": "708175"
  },
  {
    "text": "That is the derivative of the log partition function at Eta- valued to the Eta, okay?",
    "start": "708175",
    "end": "715839"
  },
  {
    "text": "And similarly, the variance of y or variance of, uh,",
    "start": "715840",
    "end": "723490"
  },
  {
    "text": "our T_y is equal to the second derivative, a double prime of Eta, right?",
    "start": "723490",
    "end": "731115"
  },
  {
    "text": "You showed this in, uh, Homework 1, uh, problem 4, right? And the- these- these two results can be",
    "start": "731115",
    "end": "738490"
  },
  {
    "text": "extended to generalized linear models as well, okay? So in generalized linear models, expectation of y given x parameterized by Theta is",
    "start": "738490",
    "end": "748750"
  },
  {
    "text": "now a prime of Theta_transpose_x times x, right?",
    "start": "748750",
    "end": "755860"
  },
  {
    "text": "It's a simple- simple extension from, uh, from- from, uh, exponential family to GLM.",
    "start": "755860",
    "end": "762820"
  },
  {
    "text": "So this is exponential family and this is GLM, okay?",
    "start": "762820",
    "end": "769645"
  },
  {
    "text": "And similarly, the variance of y given x parameterized by Theta is",
    "start": "769645",
    "end": "777550"
  },
  {
    "text": "now a double prime of Theta_transpose_x times x, x_transpose, right?",
    "start": "777550",
    "end": "786279"
  },
  {
    "text": "And using this property, we showed that generalized linear models are- are,",
    "start": "786280",
    "end": "792940"
  },
  {
    "text": "uh, uh, what- what we- what we, uh, showed is that this term over here is also the Hessian of the log-likelihood.",
    "start": "792940",
    "end": "804100"
  },
  {
    "text": "So the Hessian of the log-likelihood works out to be, uh, uh, this particular formula. Or in fact, it's going to be the sum over all the, uh, uh, x's.",
    "start": "804100",
    "end": "812095"
  },
  {
    "text": "And from this met, uh, this- this term, we can see that the Hessian is always some,",
    "start": "812095",
    "end": "819475"
  },
  {
    "text": "uh, you know what?",
    "start": "819475",
    "end": "825709"
  },
  {
    "text": "I think I wrote it the other way. Uh, [NOISE] this should be on the right.",
    "start": "826980",
    "end": "837545"
  },
  {
    "text": "So what- what we showed was that a double prime of, um, a double prime of y,",
    "start": "837545",
    "end": "847510"
  },
  {
    "text": "of sorry, of- of, uh, of Eta will be the variance of y given x parameterized by Theta times xx_transpose.",
    "start": "847510",
    "end": "861130"
  },
  {
    "text": "Yeah. And, uh, and here this will be, uh, uh a prime of- of,",
    "start": "861130",
    "end": "870170"
  },
  {
    "text": "um, uh, x_Theta will be- will be, uh, uh, the expectation of- of, uh,",
    "start": "870560",
    "end": "878230"
  },
  {
    "text": "uh y given x times, uh, times x, right?",
    "start": "878230",
    "end": "888355"
  },
  {
    "text": "So what we see is that, uh, the- it can be shown that the, uh, the- the Hessian of your log-likelihood will evaluate to this.",
    "start": "888355",
    "end": "896635"
  },
  {
    "text": "And this is always positive because there's the variance and this is PSD, right?",
    "start": "896635",
    "end": "904210"
  },
  {
    "text": "So the Hessian of the log-likelihood of a generalized linear model will always be",
    "start": "904210",
    "end": "909415"
  },
  {
    "text": "positive semi-definite and therefore, convex. Yes, question? [inaudible] from GLM to the exponential family.",
    "start": "909415",
    "end": "917785"
  },
  {
    "text": "GLM essentially is- how is it different from the linear model? So GLM is when we,",
    "start": "917785",
    "end": "924130"
  },
  {
    "text": "uh, we take an exponential family, right? And reparameterize the natural parameter with the,",
    "start": "924130",
    "end": "930865"
  },
  {
    "text": "uh, covariates of- of corresponding to that example. So that takes us from exponential family to GLM.",
    "start": "930865",
    "end": "937090"
  },
  {
    "text": "In exponential family, you only have y's, there is no x's there, right? If you want to use it as, you know,",
    "start": "937090",
    "end": "942580"
  },
  {
    "text": "some kind of a predictive model where given x, you want to predict y, this- the exponential family alone does not work because you- you have only y, right?",
    "start": "942580",
    "end": "950440"
  },
  {
    "text": "And the way you introduce your inputs is by reparameterizing the natural parameter as Theta_transpose_x,",
    "start": "950440",
    "end": "956349"
  },
  {
    "text": "where x's are the- are the, uh, inputs corresponding to y and Theta of the learnable parameters.",
    "start": "956349",
    "end": "961810"
  },
  {
    "text": "[inaudible] I couldn't understand. What, uh, additional benefit does this para- reparameterization give you over,",
    "start": "961810",
    "end": "970210"
  },
  {
    "text": "uh, for example, feature transform, feature maps? So feature, uh, uh, good question.",
    "start": "970210",
    "end": "975310"
  },
  {
    "text": "So feature maps are- is kind of orthogonal to this. So you can still use feature maps where instead of Theta_transpose_x,",
    "start": "975310",
    "end": "982015"
  },
  {
    "text": "you do Theta_ transpose_Phi_x. As- so you can- you can introduce feature maps into generalized linear models.",
    "start": "982015",
    "end": "989394"
  },
  {
    "text": "Uh, so it's- it's a separate concept introducing feature- feature, uh, feature maps. What- what, uh, GLM allows you to do is- is provide you a way",
    "start": "989395",
    "end": "997630"
  },
  {
    "text": "to connect y's- y's with x's, right? Without- without- without, uh, uh, GLM,",
    "start": "997630",
    "end": "1004350"
  },
  {
    "text": "you- you have only y's, there's no x's in- in exponential family. So again just to make sure that I am understanding this right.",
    "start": "1004350",
    "end": "1015270"
  },
  {
    "text": "Mm-hmm. Maybe [inaudible]. So in- in y equals Theta. So this is- this is Eta equals Theta transpose x.",
    "start": "1015270",
    "end": "1021959"
  },
  {
    "text": "[BACKGROUND] So, eh, it- it- it means that we assume that Eta or the natural parameter is equal to Theta transpose x,",
    "start": "1021960",
    "end": "1031370"
  },
  {
    "text": "and then y is sampled from the exponential family that has this Eta, right?",
    "start": "1031370",
    "end": "1036530"
  },
  {
    "text": "That's where the noise comes in. So y is gonna be a noisy version- noisy observation, and the noise follows this exponential family,",
    "start": "1036530",
    "end": "1044194"
  },
  {
    "text": "and the parameter of the exponential family will be Theta transpose x, right? Once you- once you- once you evaluate Theta transpose x, you get an Eta,",
    "start": "1044195",
    "end": "1051270"
  },
  {
    "text": "which will determine the exponential family, and from that, you assume that the y is gonna be a sample from that exponential family.",
    "start": "1051270",
    "end": "1058304"
  },
  {
    "text": "Yeah. Okay? So that's- that's, uh, exponential families.",
    "start": "1058305",
    "end": "1063570"
  },
  {
    "text": "And then we saw this- this, um, and then we saw this,",
    "start": "1063570",
    "end": "1069075"
  },
  {
    "text": "uh, connection later in the course between exponential families and maximum entropy, right?",
    "start": "1069075",
    "end": "1075059"
  },
  {
    "text": "So, um, performing maximum likelihood on exponential family. So, um, if you are given a dataset,",
    "start": "1075060",
    "end": "1083085"
  },
  {
    "text": "um, y^i, you know, i equals 1 through n,",
    "start": "1083085",
    "end": "1089720"
  },
  {
    "text": "when you perform MLE with some, uh, exponential family,",
    "start": "1089720",
    "end": "1095480"
  },
  {
    "text": "that is equivalent to performing maximum entropy,",
    "start": "1095480",
    "end": "1100515"
  },
  {
    "text": "that is max H of p. The constraint that the, uh,",
    "start": "1100515",
    "end": "1108480"
  },
  {
    "text": "expectation of some- some, um, um, um, statistic functions T of y is equal to the sample- sample averages,",
    "start": "1108480",
    "end": "1122250"
  },
  {
    "text": "i equals 1 to n, T of y^i.",
    "start": "1122250",
    "end": "1127260"
  },
  {
    "text": "Subject to this constraint, if we maximize the entropy, then you- the- the, uh,",
    "start": "1127260",
    "end": "1133590"
  },
  {
    "text": "solution that you get will lie in the exponential family, and the parameters of that exponential family will be the ones- will be",
    "start": "1133590",
    "end": "1142020"
  },
  {
    "text": "the same parameter that you get by performing maximum likelihood on the exponential family,",
    "start": "1142020",
    "end": "1147030"
  },
  {
    "text": "and the sufficient statistics that you used for the constraints are gonna be the sufficient statistics of that exponential family, right?",
    "start": "1147030",
    "end": "1154380"
  },
  {
    "text": "So that's the, you know, it's- it's this, uh, duality result where you can, you know, maximum likelihood and maximum entropy are like these,",
    "start": "1154380",
    "end": "1161310"
  },
  {
    "text": "uh, dual problems of each other, right? So that was, uh, maximum likelihood and maximum entropy.",
    "start": "1161310",
    "end": "1167850"
  },
  {
    "text": "[NOISE] And then we moved on to something",
    "start": "1167850",
    "end": "1172980"
  },
  {
    "text": "called generative models, right?",
    "start": "1172980",
    "end": "1180929"
  },
  {
    "text": "[NOISE] In- yes, question? [BACKGROUND]",
    "start": "1180930",
    "end": "1188009"
  },
  {
    "text": "Well, so, um, maximizing the entropy subject to these constraints is",
    "start": "1188010",
    "end": "1197040"
  },
  {
    "text": "the same as maximizing the likelihood of this dataset using an exponential family that has sufficient statistics equal to these functions.",
    "start": "1197040",
    "end": "1206190"
  },
  {
    "text": "[NOISE] And then we moved on to something called generative models,",
    "start": "1206190",
    "end": "1214769"
  },
  {
    "start": "1210000",
    "end": "1485000"
  },
  {
    "text": "[NOISE] generative models.",
    "start": "1214770",
    "end": "1224415"
  },
  {
    "text": "So the algorithms until then that we studied were modeling p of y given x. X was considered given, right?",
    "start": "1224415",
    "end": "1233325"
  },
  {
    "text": "We did not assign any kind of probability distribution to x's, right? They were given but we- we, um, uh,",
    "start": "1233325",
    "end": "1240179"
  },
  {
    "text": "we would assign some kind of a distribution to- to, uh, y. And then, uh, with- with generative models,",
    "start": "1240180",
    "end": "1246989"
  },
  {
    "text": "we now instead try to- so this was our discriminative models, right?",
    "start": "1246989",
    "end": "1255345"
  },
  {
    "text": "In discriminative models, we just model p of y given- given x, whereas in generative models,",
    "start": "1255345",
    "end": "1261250"
  },
  {
    "text": "we actually try to model x as well, right? Now, our interest here is p of y, x,",
    "start": "1261800",
    "end": "1268155"
  },
  {
    "text": "which is generally written as p of x given y times p of y.",
    "start": "1268155",
    "end": "1274170"
  },
  {
    "text": "So this is the joint, and over here,",
    "start": "1274170",
    "end": "1280440"
  },
  {
    "text": "p of y, we generally call it the prior, and if y is discrete, we call it the class prior.",
    "start": "1280440",
    "end": "1286990"
  },
  {
    "text": "And p of x given y, you can, um, uh, think of it as the likelihood function.",
    "start": "1288830",
    "end": "1294610"
  },
  {
    "text": "So, um, we model both- uh, both y's and x's or x given y's,",
    "start": "1296810",
    "end": "1303810"
  },
  {
    "text": "and depending on the- the datatype of x, we could either, um, um,",
    "start": "1303810",
    "end": "1310200"
  },
  {
    "text": "so x's could either be real valued, real valued, right?",
    "start": "1310200",
    "end": "1317144"
  },
  {
    "text": "Uh, for example, in GDA, y was- uh, uh, x was- x was, uh,",
    "start": "1317145",
    "end": "1322335"
  },
  {
    "text": "real valued or x could be discrete- discrete valued, right?",
    "start": "1322335",
    "end": "1329294"
  },
  {
    "text": "And we saw one example of discrete valued x which was naive Bayes, right?",
    "start": "1329295",
    "end": "1334710"
  },
  {
    "text": "So these are- these are these two generative algorithms that we saw where, uh, both these generative algorithms have discrete y's.",
    "start": "1334710",
    "end": "1342465"
  },
  {
    "text": "So the y's are discrete in- in both these versions, uh, uh, in both these examples, which means in a way we are trying to perform",
    "start": "1342465",
    "end": "1349350"
  },
  {
    "text": "classification because your y's were discrete, right? But the x's in one- one- uh,",
    "start": "1349350",
    "end": "1355260"
  },
  {
    "text": "in one case in GDA, x was real val- valued, and in the other case, in naive Bayes, x was discrete valued, okay?",
    "start": "1355260",
    "end": "1361905"
  },
  {
    "text": "And for GDA, the- the model was something like this.",
    "start": "1361905",
    "end": "1369720"
  },
  {
    "text": "So we assume p of y, the prior, was some Bernoulli- Bernoulli with,",
    "start": "1369720",
    "end": "1378419"
  },
  {
    "text": "uh, parameter Phi, correct? And p of x given y",
    "start": "1378420",
    "end": "1386070"
  },
  {
    "text": "was some kind of a normal distribution with mean Mu y.",
    "start": "1386070",
    "end": "1393539"
  },
  {
    "text": "So each y had its own Mu and a common covariance matrix, Sigma, right?",
    "start": "1393540",
    "end": "1401670"
  },
  {
    "text": "So this was- this was, uh, GDA. And the, uh, kind of, uh, picture to have in your mind is in GDA,",
    "start": "1401670",
    "end": "1409710"
  },
  {
    "text": "we have some x's. So- so, uh, some- some- some x's that come from y equals 0,",
    "start": "1409710",
    "end": "1418440"
  },
  {
    "text": "so let's just call them 0,",
    "start": "1418440",
    "end": "1420970"
  },
  {
    "text": "that- that have some kind of, uh, a covariant structure Sigma, right?",
    "start": "1424940",
    "end": "1430840"
  },
  {
    "text": "So this ellipse kind of, um, uh, you know, uniquely, uh, uh, uh, determines the- the, uh, covariant Sigma.",
    "start": "1432680",
    "end": "1440205"
  },
  {
    "text": "And then there is another, uh, another class y, so this corresponds to y equals 0, y equals 0,",
    "start": "1440205",
    "end": "1448785"
  },
  {
    "text": "and for y equals 1, we have this other that has- that is-",
    "start": "1448785",
    "end": "1458530"
  },
  {
    "text": "So the assumption is that y equals 1 also is",
    "start": "1462500",
    "end": "1467790"
  },
  {
    "text": "distributed according to a Gaussian with the same covariance structure as y equals 0, right?",
    "start": "1467790",
    "end": "1473070"
  },
  {
    "text": "So this is y equals 1, okay? So this- this is the picture to have in,",
    "start": "1473070",
    "end": "1478125"
  },
  {
    "text": "uh, uh, about GDA where, um, until we- we, uh, st- study generative algorithms,",
    "start": "1478125",
    "end": "1483825"
  },
  {
    "text": "we were not even visualizing x, right? So we would only focus on, you know, what y was.",
    "start": "1483825",
    "end": "1489120"
  },
  {
    "text": "We would plot, um, um, um, uh, you think of- think of, uh, uh, y and- and modeling y.",
    "start": "1489120",
    "end": "1495495"
  },
  {
    "text": "But now we are trying to assign probabilities to x directly, right?",
    "start": "1495495",
    "end": "1500580"
  },
  {
    "text": "So this is x1, x2, x1 or xd. And because of this equal covariance,",
    "start": "1500580",
    "end": "1510700"
  },
  {
    "text": "the- the posterior of P of y given x, P of y given x takes a form of",
    "start": "1511250",
    "end": "1519810"
  },
  {
    "text": "1 over 1 plus exponent of minus something, right?",
    "start": "1519810",
    "end": "1527355"
  },
  {
    "text": "And that's because it's- it's easy to see that if both of them have this- this,",
    "start": "1527355",
    "end": "1532690"
  },
  {
    "text": "uh, um, um, equal covariance structure, and suppose both of them have an equal number of examples,",
    "start": "1532690",
    "end": "1539220"
  },
  {
    "text": "then the line that, or the- the- the set of all points that gets equal likelihood both",
    "start": "1539220",
    "end": "1547500"
  },
  {
    "text": "from the Class 1 Gaussian and the Class 2 Gaussian is gonna be some straight line, okay?",
    "start": "1547500",
    "end": "1553845"
  },
  {
    "text": "And- and that kind of corresponds to, um, um, uh, uh, a form which can be expressed as a logistic regression because, uh,",
    "start": "1553845",
    "end": "1561585"
  },
  {
    "text": "in logistic regression, we also got, uh, linear separating boundaries, right? Now, if the equal covariance assumption is not met.",
    "start": "1561585",
    "end": "1569175"
  },
  {
    "text": "So instead if we had a normal with mu, a mean corresponding to the class and also covariance corresponding to",
    "start": "1569175",
    "end": "1577020"
  },
  {
    "text": "the class then we would have gotten instead, uh, one covariance that looks like this.",
    "start": "1577020",
    "end": "1583830"
  },
  {
    "text": "Another covariance probably looks like this, all right?",
    "start": "1583830",
    "end": "1588960"
  },
  {
    "text": "And now the, uh, set of all points that get equal likelihood under both these classes would probably look something like this, right?",
    "start": "1588960",
    "end": "1596415"
  },
  {
    "text": "It wouldn't be a straight line anymore. It would be a quadratic, right? So that's- that's GDA.",
    "start": "1596415",
    "end": "1602580"
  },
  {
    "text": "The posterior of the GDA is, uh, um, is a logistic, uh, takes on a logistic regression form y- you saw that in PS1, Q1, right?",
    "start": "1602580",
    "end": "1612815"
  },
  {
    "text": "And we also, kind of, uh, uh, briefly discussed, you know,",
    "start": "1612815",
    "end": "1617990"
  },
  {
    "text": "what are the cases when you wanna use logistic regression directly and what are the cases when you want to use GDA? Uh, yes, question?",
    "start": "1617990",
    "end": "1624780"
  },
  {
    "text": "Why did we make that assumption of equal covariance in mid range? Like we know how to do analysis with, uh, different covariance,",
    "start": "1624780",
    "end": "1633105"
  },
  {
    "text": "was is just to show that you get the logistic as a- as a posterior or I'm guessing you don't get a logistic as posterior [OVERLAPPING]",
    "start": "1633105",
    "end": "1641039"
  },
  {
    "text": "Well, so in this case, um, uh, so the question is, do we get a- a logistic as a posterior in this case. Uh, you can show that you do get a logistic,",
    "start": "1641040",
    "end": "1647640"
  },
  {
    "start": "1642000",
    "end": "1895000"
  },
  {
    "text": "uh, as a posterior in this case, but that logistic will have quadratic, uh, form, quadratic features of x's, right?",
    "start": "1647640",
    "end": "1654150"
  },
  {
    "text": "So you can show that this takes on, uh, a logistic regression form except that the- the, uh,",
    "start": "1654150",
    "end": "1659700"
  },
  {
    "text": "it's a logistic where you include quadratic features of your x's. And why did you assume initially that we have the same coordinates?",
    "start": "1659700",
    "end": "1666180"
  },
  {
    "text": "So- so- so, uh, so- so the- the question of why we assume something, the answer is always, if you assume something,",
    "start": "1666180",
    "end": "1672285"
  },
  {
    "text": "you get this, right? And- and, you know, why you assume something is most of the times it's because it gives you some kind of a mathematical convenience.",
    "start": "1672285",
    "end": "1680144"
  },
  {
    "text": "And then you kind of measure up against real data whether the assumption was reasonable or not, right? That's- that's- in general,",
    "start": "1680144",
    "end": "1685860"
  },
  {
    "text": "that's gonna be the answer for why we assume something for anything, you know, mostly in this course and in general, right?",
    "start": "1685860",
    "end": "1691320"
  },
  {
    "text": "Uh, you know, making some assumptions gives you, you know, some kind of mathematical convenience maybe, you know,",
    "start": "1691320",
    "end": "1696555"
  },
  {
    "text": "easy to implement or, you know, convexity or- or whatever. And that assumption may or may not hold true.",
    "start": "1696555",
    "end": "1702030"
  },
  {
    "text": "And then, um, so you made that assumption, you know, build a model or whatever, and then you fit your data and see if that assumption was valid or not. Next question.",
    "start": "1702030",
    "end": "1711540"
  },
  {
    "text": "Um, the significance of translating from this commutative and generative index, you know, it's not that apparent to me.",
    "start": "1711540",
    "end": "1717450"
  },
  {
    "text": "When would we want to use a discriminative and when would we want to use a generative? Yes. So I'm coming- I'm- I'm coming to that, uh, right away.",
    "start": "1717450",
    "end": "1722610"
  },
  {
    "text": "So then we kind of, uh, um, uh, kind of, um, a-a- asked the question, you know, what- under what cir- circumstances do we want to use logistic regression?",
    "start": "1722610",
    "end": "1730260"
  },
  {
    "text": "And when do we wanna use GDA? Right? And the- the- the answer to that is if you don't have a lot of",
    "start": "1730260",
    "end": "1737760"
  },
  {
    "text": "data and also if you are pretty sure that the modeling assumptions hold true.",
    "start": "1737760",
    "end": "1746160"
  },
  {
    "text": "In those set of situations using generative models is- is gonna be beneficial over using logistic regression.",
    "start": "1746160",
    "end": "1754800"
  },
  {
    "text": "However, if you have lots of data and, you know,",
    "start": "1754800",
    "end": "1759900"
  },
  {
    "text": "you're- you're not too sure of whether the modeling assumptions hold true or not, then you're better off using logistic regression.",
    "start": "1759900",
    "end": "1766920"
  },
  {
    "text": "In logistic regression, we made no assumption about the- about how x's are distributed.",
    "start": "1766920",
    "end": "1773219"
  },
  {
    "text": "The only model P of y given x, right? X was assumed to be given. So logistic regression is in general,",
    "start": "1773219",
    "end": "1780855"
  },
  {
    "text": "more robust to the kinds of, uh, x's that- that you may,",
    "start": "1780855",
    "end": "1785985"
  },
  {
    "text": "uh, uh, you may- you may actually encounter. However, in examples where the assumptions hold,",
    "start": "1785985",
    "end": "1793260"
  },
  {
    "text": "GDA will be more sample efficient, right? You need a lot fewer examples when the assumptions hold with",
    "start": "1793260",
    "end": "1798539"
  },
  {
    "text": "GDA as opposed to logistic regression, okay? And- and that's- that answer kind of holds for,",
    "start": "1798540",
    "end": "1804540"
  },
  {
    "text": "you know, all generative models. Um, when you want to- when you want to learn,",
    "start": "1804540",
    "end": "1809625"
  },
  {
    "text": "um, um, some kind of, um, um, machine learning model, right? So when you wanna build a model,",
    "start": "1809625",
    "end": "1814635"
  },
  {
    "text": "you can kind of give information to it either through assumptions or you can give it information from data, right?",
    "start": "1814635",
    "end": "1823575"
  },
  {
    "text": "And as long as your assumptions are true, feeding those assumptions is beneficial, right?",
    "start": "1823575",
    "end": "1831690"
  },
  {
    "text": "And you have these other, er, in this other case- so in- in- in GDA, in these generative models, you're feeding in a lot more assumptions,",
    "start": "1831690",
    "end": "1837750"
  },
  {
    "text": "a lot more prior knowledge, right? And as long as those knowledge is- is, uh, a-a- as long as those assumptions is- is true, it is beneficial.",
    "start": "1837750",
    "end": "1845565"
  },
  {
    "text": "You need a lot less data to make up for, um, um, uh, for the information, right? In discriminative models, you're feeding a lot fewer assumptions.",
    "start": "1845565",
    "end": "1854610"
  },
  {
    "text": "You're saying nothing about x's at all, right? Which means you need to make up for that lack",
    "start": "1854610",
    "end": "1860310"
  },
  {
    "text": "of information that you're feeding by having more data, right? And it- it- it fits the data no matter how, you know, um, uh,",
    "start": "1860310",
    "end": "1866730"
  },
  {
    "text": "it fits the data better because you're just making fewer assumptions about how the data is supposed to be, right?",
    "start": "1866730",
    "end": "1872025"
  },
  {
    "text": "So that's- that's gonna be a common difference between or- or a common, uh, way to think about when you wanna use",
    "start": "1872025",
    "end": "1878130"
  },
  {
    "text": "generative models versus when you wanna use discriminative models. Right, so that was, uh, GDA.",
    "start": "1878130",
    "end": "1886395"
  },
  {
    "text": "And then we kinda move on to naive Bayes, right? So Naive Bayes.",
    "start": "1886395",
    "end": "1892740"
  },
  {
    "text": "In Naive Bayes- so Naive Bayes is- is, uh, an algorithm that's commonly used with text classification, right?",
    "start": "1892740",
    "end": "1901125"
  },
  {
    "start": "1895000",
    "end": "2272000"
  },
  {
    "text": "Uh, in text classification, the inputs or the x's are discrete, you know,",
    "start": "1901125",
    "end": "1906360"
  },
  {
    "text": "they are discrete words that make up a sentence or a message. And in Naive Bayes,",
    "start": "1906360",
    "end": "1913155"
  },
  {
    "text": "we kind of saw these, um, so first of all, in Naive Bayes, we make this conditional independence assumption.",
    "start": "1913155",
    "end": "1919679"
  },
  {
    "text": "The conditional independence assumption tells us that x_i is independent of x_j given y.",
    "start": "1919680",
    "end": "1930330"
  },
  {
    "text": "Which means if you know what the class is, whether, you know,",
    "start": "1930330",
    "end": "1935490"
  },
  {
    "text": "for example, if you're doing spam classification, y can be 0 or 1, uh, spam versus no spam.",
    "start": "1935490",
    "end": "1940845"
  },
  {
    "text": "If you know what the class is, then the probability of observing one word is independent of the probability of,",
    "start": "1940845",
    "end": "1947400"
  },
  {
    "text": "uh, uh, observing another word, right? This does not mean x_i is independent of x_j, right?",
    "start": "1947400",
    "end": "1956700"
  },
  {
    "text": "So independence and conditional independence are two- two, uh, kind of orthogonal properties.",
    "start": "1956700",
    "end": "1962655"
  },
  {
    "text": "Uh, being conditionally independent says nothing about being independent and vice versa, right? In- in, uh, Naive Bayes,",
    "start": "1962655",
    "end": "1968970"
  },
  {
    "text": "we make this conditional independence assumption that if we know the class, then the probability of one word occurring is",
    "start": "1968970",
    "end": "1975600"
  },
  {
    "text": "independent of the probability of other words occurring, right? And this- this assumption may or may not hold true in practice.",
    "start": "1975600",
    "end": "1982935"
  },
  {
    "text": "But, um, but it so happens that even when these assumptions are not met, the models tend to work reasonably well with text, uh,",
    "start": "1982935",
    "end": "1990795"
  },
  {
    "text": "or textual data, especially for a classification. Yes, question? Sorry that- that, uh, doesn't hold, right?",
    "start": "1990795",
    "end": "1997190"
  },
  {
    "text": "Oh, sorry. Yeah, that doesn't hold. [NOISE] Right?",
    "start": "1997190",
    "end": "2002289"
  },
  {
    "text": "And we saw two- two different, uh, event models called the, uh, Bernoulli event model,",
    "start": "2002290",
    "end": "2009860"
  },
  {
    "text": "Bernoulli event model, and the- and the multinomial event model,",
    "start": "2010320",
    "end": "2019519"
  },
  {
    "text": "multinomial event model, right?",
    "start": "2021120",
    "end": "2027280"
  },
  {
    "text": "So the difference between the- these two is in, um, in both of- in both of, uh, um,",
    "start": "2027280",
    "end": "2033910"
  },
  {
    "text": "in both of these event models, we assume p of y is equal to Phi y. P of y equals Phi y, right?",
    "start": "2033910",
    "end": "2044980"
  },
  {
    "text": "So this is the, uh, class prior. Class prior, class prior.",
    "start": "2044980",
    "end": "2053570"
  },
  {
    "text": "So the class prior is what fraction of all the messages that you encounter will be spammy in general,",
    "start": "2054600",
    "end": "2060760"
  },
  {
    "text": "right, without even looking at the messages, uh, of- of the content of the message.",
    "start": "2060760",
    "end": "2065935"
  },
  {
    "text": "That's just the, uh, class prior. And then in the, uh, Bernoulli event model,",
    "start": "2065935",
    "end": "2071859"
  },
  {
    "text": "we say p of xj given y is equal",
    "start": "2071860",
    "end": "2078879"
  },
  {
    "text": "to Phi j given y according to,",
    "start": "2078880",
    "end": "2086049"
  },
  {
    "text": "uh, um, again, this is a Bernoulli distribution. Bernoulli distribution.",
    "start": "2086050",
    "end": "2095605"
  },
  {
    "text": "What that means is over here, xj is the jth word in the vocabulary, right?",
    "start": "2095605",
    "end": "2109279"
  },
  {
    "text": "What this means is, uh, we have one parameter per word in the vocabulary per class.",
    "start": "2109380",
    "end": "2117250"
  },
  {
    "text": "Okay? And the- the, uh, what the- what the parameter signifies is the probability that of the word,",
    "start": "2117250",
    "end": "2126819"
  },
  {
    "text": "the jth word in the vocabulary occurs in a given message. So the, uh, on the- on the- on the,",
    "start": "2126820",
    "end": "2135100"
  },
  {
    "text": "uh, on other hand with the multinomial event model, we have p of xj given y is equal to- is as, um,",
    "start": "2135100",
    "end": "2146380"
  },
  {
    "text": "equal to Phi j given- given y,",
    "start": "2146380",
    "end": "2152500"
  },
  {
    "text": "where this is the- is the, uh, jth multinomial distribution parameter or the categorical distribution parameter,",
    "start": "2152500",
    "end": "2161110"
  },
  {
    "text": "multinomial distribution,",
    "start": "2161110",
    "end": "2166449"
  },
  {
    "text": "and xj here refers to the jth word in a message.",
    "start": "2166449",
    "end": "2171460"
  },
  {
    "text": "[NOISE] Right?",
    "start": "2171460",
    "end": "2177849"
  },
  {
    "text": "So in the- in the Bernoulli event model, we care about what fraction of the message the word occurs versus does not occur.",
    "start": "2177850",
    "end": "2186490"
  },
  {
    "text": "We don't care how many times the word occurs in that message, okay? We're just- we're just, uh, counting what fraction of the messages does this word occur versus not occur.",
    "start": "2186490",
    "end": "2195265"
  },
  {
    "text": "It may occur 10,000 times in a message, but we just count it once that it occurred in this message.",
    "start": "2195265",
    "end": "2200650"
  },
  {
    "text": "Whereas in the multinomial event model, we are trying to build a histogram of- of",
    "start": "2200650",
    "end": "2207340"
  },
  {
    "text": "words that occur in all the messages of a given, uh, um, of a given class, right?",
    "start": "2207340",
    "end": "2213850"
  },
  {
    "text": "So, um, here, we basically end up counting the number of times the word appears across all messages in spammy,",
    "start": "2213850",
    "end": "2221319"
  },
  {
    "text": "uh, emails, and similarly, the number of times a word occurs across all messages in non-spammy emails, right?",
    "start": "2221320",
    "end": "2228730"
  },
  {
    "text": "And, um, and then, uh, uh, we count them, uh, and normalize them to get a multinomial distribution over the words.",
    "start": "2228730",
    "end": "2236770"
  },
  {
    "text": "So that's a multinomial event model, whereas in a Bernoulli event model, we kind of treat each word in the vocabulary as",
    "start": "2236770",
    "end": "2242859"
  },
  {
    "text": "a separate problem and estimates- estimate the- the, uh, Bernoulli parameter of what fraction of",
    "start": "2242860",
    "end": "2248950"
  },
  {
    "text": "messages does this word occur in the spammy emails, uh, or not. Yes, question.",
    "start": "2248950",
    "end": "2254575"
  },
  {
    "text": "The way I think about it, like, can you confirm if this is the, uh, right way? Basically, multinomial is the same as Bernoulli,",
    "start": "2254575",
    "end": "2263155"
  },
  {
    "text": "but what you do is you concatenate all the non-spammy messages and concatenate all the spammy messages and treat them",
    "start": "2263155",
    "end": "2269619"
  },
  {
    "text": "as two big messages and then just do Bernoulli. Uh, that's not the case. So, uh, um, the- the,",
    "start": "2269620",
    "end": "2275545"
  },
  {
    "start": "2272000",
    "end": "2417000"
  },
  {
    "text": "uh, the- the question was, in, uh, in multinomial, do you think of it as taking all the spammy messages,",
    "start": "2275545",
    "end": "2280944"
  },
  {
    "text": "concatenating it as one big message, just take all the, you know, non-spammy message concatenate against one big message,",
    "start": "2280945",
    "end": "2286485"
  },
  {
    "text": "take those two big messages and perform Bernoulli? Uh, that's not the case. Uh, because in Bernoulli,",
    "start": "2286485",
    "end": "2292484"
  },
  {
    "text": "we are only- we- we- we are, uh, counting the number of times or rather the number of messages in which a word appears.",
    "start": "2292485",
    "end": "2299850"
  },
  {
    "text": "So if you collapse everything into one message, then the answer is always 0 or 1. Does that appear in that one large message or not?",
    "start": "2299850",
    "end": "2306625"
  },
  {
    "text": "Whereas in- in, uh, multinomial what you wanna do is actually count the number of times the word repeats across all messages.",
    "start": "2306625",
    "end": "2312565"
  },
  {
    "text": "Right? So the two- the- the- the those two are- are- are, uh, are- need not to be the same. Right? And then, um,",
    "start": "2312565",
    "end": "2319599"
  },
  {
    "text": "we also saw this concept called Laplace smoothing, where in Laplace smoothing is a technique to handle rare words where in words- uh,",
    "start": "2319600",
    "end": "2327444"
  },
  {
    "text": "may- may occur very, uh, infrequently. And in Laplace smoothing the idea is for each of the,",
    "start": "2327445",
    "end": "2335860"
  },
  {
    "text": "uh, two different, uh, uh, event models, first, kind of precount that each event happens once.",
    "start": "2335860",
    "end": "2344140"
  },
  {
    "text": "And starting with a count of one for each of the events, then look at the data and- and start incrementing the counters.",
    "start": "2344140",
    "end": "2351205"
  },
  {
    "text": "Right? That's- that's the general idea of- of Laplace smoothing where you want to start with- instead of starting with a 0 prior,",
    "start": "2351205",
    "end": "2357924"
  },
  {
    "text": "where a 0 prior means you have no idea whether a message is, you know, a- a word occurs in a- a spammy email or not,",
    "start": "2357925",
    "end": "2364135"
  },
  {
    "text": "start with a prior where you assume you've seen, [NOISE] um, you know, one- one count of each event.",
    "start": "2364135",
    "end": "2371470"
  },
  {
    "text": "[NOISE] Right? Now so that means, uh, in the- in the Bernoulli case, each word assume that you've seen it",
    "start": "2371470",
    "end": "2378565"
  },
  {
    "text": "once in a spammy email and once in a non-spammy email, then you start incrementing the counters by looking at the emails and seeing whether,",
    "start": "2378565",
    "end": "2386995"
  },
  {
    "text": "uh, and- and checking whether the word occurred in it or not. Right? And similarly, in, uh, the multinomial event model, assume that, um,",
    "start": "2386995",
    "end": "2395140"
  },
  {
    "text": "every word occurs once in the spam class,",
    "start": "2395140",
    "end": "2400990"
  },
  {
    "text": "or the spam pool of messages, and occurs once in a non-spam pool of messages, and then construct the histogram of words from, you know,",
    "start": "2400990",
    "end": "2408595"
  },
  {
    "text": "this pool and that pool by adding on to that one, right, and then normalize it. So that's Laplace smoothing.",
    "start": "2408595",
    "end": "2414610"
  },
  {
    "text": "[NOISE] And that kind of wraps up generative models.",
    "start": "2414610",
    "end": "2421015"
  },
  {
    "start": "2417000",
    "end": "2482000"
  },
  {
    "text": "So those were basically the two models that we saw: GDA and Naive Bayes. GDA was for continuous x's,",
    "start": "2421015",
    "end": "2427105"
  },
  {
    "text": "naive Bayes was for discrete x's. And then, uh, after,",
    "start": "2427105",
    "end": "2433298"
  },
  {
    "text": "[NOISE] after, uh, uh, generative models, we move on to kernel methods.",
    "start": "2433299",
    "end": "2439090"
  },
  {
    "text": "So with kernel methods,",
    "start": "2439090",
    "end": "2444910"
  },
  {
    "text": "the motivation for kernel methods is having some kind of an efficient way to introduce feature maps.",
    "start": "2444910",
    "end": "2454540"
  },
  {
    "text": "Right? So we saw feature maps in homework 1, the last question,",
    "start": "2454540",
    "end": "2459835"
  },
  {
    "text": "where we implemented polynomial features for linear regression and we saw that by using feature maps,",
    "start": "2459835",
    "end": "2467935"
  },
  {
    "text": "even though we're fitting a linear model, the hypothesis that we get can be quite nonlinear or quite curvy.",
    "start": "2467935",
    "end": "2474460"
  },
  {
    "text": "Right? So linear regression is linear in its parameters or linear in its features.",
    "start": "2474460",
    "end": "2480025"
  },
  {
    "text": "It's not always linear in the- with respect to the original data. So by introducing these features, we can get, you know,",
    "start": "2480025",
    "end": "2485785"
  },
  {
    "start": "2482000",
    "end": "2632000"
  },
  {
    "text": "uh, pretty complex nonlinear models. And kernel methods is a way to- to,",
    "start": "2485785",
    "end": "2494234"
  },
  {
    "text": "uh, have these nonlinear features in an efficient way.",
    "start": "2494235",
    "end": "2500210"
  },
  {
    "text": "So a kernel, [NOISE] in order to define a kernel,",
    "start": "2501000",
    "end": "2509275"
  },
  {
    "text": "we start with a feature map. Kernel methods, right?",
    "start": "2509275",
    "end": "2516760"
  },
  {
    "text": "So we start with the feature map Phi of x, right? That takes R^d to R^p, right?",
    "start": "2516760",
    "end": "2527545"
  },
  {
    "text": "Where x is in R^d, that's the data that's given to you, right?",
    "start": "2527545",
    "end": "2532914"
  },
  {
    "text": "And p is the, uh, uh, uh, um, the dimension of the feature space.",
    "start": "2532915",
    "end": "2542810"
  },
  {
    "text": "And importantly, p can be infinite. Okay?",
    "start": "2544590",
    "end": "2553730"
  },
  {
    "text": "And now we define a kernel based on this feature map,",
    "start": "2555600",
    "end": "2561265"
  },
  {
    "text": "and say that a kernel corresponding to- a kernel is a function of two inputs,",
    "start": "2561265",
    "end": "2566589"
  },
  {
    "text": "let's call it x and x prime, corresponding to a feature map, if it evaluates to Phi of x transpose,",
    "start": "2566590",
    "end": "2574839"
  },
  {
    "text": "Phi of x prime, right? So a kernel takes as input two xs, like,",
    "start": "2574840",
    "end": "2581785"
  },
  {
    "text": "you know, before you map them to a f- a high dimensional feature space. And it evaluates the, uh,",
    "start": "2581785",
    "end": "2587545"
  },
  {
    "text": "output to have the same value as that you would get by mapping them to",
    "start": "2587545",
    "end": "2593890"
  },
  {
    "text": "the high dimensional feature space and taking an inner product, right? So mathematically, these two are equivalent,",
    "start": "2593890",
    "end": "2599184"
  },
  {
    "text": "but computationally or algorithmically, the kernel would generally perform some kind of a more efficient computation that gives you",
    "start": "2599185",
    "end": "2606820"
  },
  {
    "text": "the same answer as if you had constructed an explicit feature map and done an inner product between them, right?",
    "start": "2606820",
    "end": "2612520"
  },
  {
    "text": "So that- those are kernels. So kernels are- are functions of- of, uh, x and x prime, corresponding to some feature map such that they evaluate to, you know,",
    "start": "2612520",
    "end": "2619840"
  },
  {
    "text": "mathematically evaluate to the same answer as you would have, uh, uh, obtained, from the explicit feature map construction.",
    "start": "2619840",
    "end": "2626065"
  },
  {
    "text": "And then, um, there are a few properties of- of, uh, kernels.",
    "start": "2626065",
    "end": "2633130"
  },
  {
    "start": "2632000",
    "end": "3212000"
  },
  {
    "text": "So a- a function K is a kernel if it is symmetric, right?",
    "start": "2633130",
    "end": "2642849"
  },
  {
    "text": "If it is, um, symmetric, and if you were to take a collection of examples,",
    "start": "2642850",
    "end": "2648609"
  },
  {
    "text": "x^1 through, uh, um, so if you were to take x^1 through, let's say, x,",
    "start": "2648610",
    "end": "2655270"
  },
  {
    "text": "um, let's call it m, and construct a kernel matrix.",
    "start": "2655270",
    "end": "2660565"
  },
  {
    "text": "So kernel matrix is then a matrix of m by m, right?",
    "start": "2660565",
    "end": "2669910"
  },
  {
    "text": "And these examples can be any examples, these may not be a training set. Take any possible, uh, examples, uh, you can- you can, uh, uh, come up with.",
    "start": "2669910",
    "end": "2677740"
  },
  {
    "text": "And if you evaluate K of x^1, x^1,",
    "start": "2677740",
    "end": "2686270"
  },
  {
    "text": "K of x^1, x^m, and similarly,",
    "start": "2686270",
    "end": "2692770"
  },
  {
    "text": "K of x^m, x^m,",
    "start": "2692770",
    "end": "2699230"
  },
  {
    "text": "K of x^m, x^1.",
    "start": "2699230",
    "end": "2704859"
  },
  {
    "text": "So evaluate the, uh, kernel function on every pairwise, uh,",
    "start": "2704860",
    "end": "2709930"
  },
  {
    "text": "on- on every pair of, uh, uh, of- of, uh, two- two, um, two choices from the set.",
    "start": "2709930",
    "end": "2716710"
  },
  {
    "text": "And this matrix that you obtain, where each of these is just a scalar, right?",
    "start": "2716710",
    "end": "2723339"
  },
  {
    "text": "Each of these is a scalar, and you get this, uh, matrix, which is also called the kernel matrix,",
    "start": "2723340",
    "end": "2728740"
  },
  {
    "text": "this kernel matrix will be symmetric and positive semi-definite, right?",
    "start": "2728740",
    "end": "2735535"
  },
  {
    "text": "And we saw- we also saw this, uh, this theorem called Mercer's theorem- Mercer's theorem,",
    "start": "2735535",
    "end": "2745405"
  },
  {
    "text": "which says, in order for a function K to be a kernel,",
    "start": "2745405",
    "end": "2751930"
  },
  {
    "text": "it is necessary and sufficient for the corresponding kernel matrix to be symmetric and positive semi-definite, right?",
    "start": "2751930",
    "end": "2759520"
  },
  {
    "text": "So the- the, um, so the, uh, kernel matrix being symmetric and positive semi-definite,",
    "start": "2759520",
    "end": "2766825"
  },
  {
    "text": "is both a property of kernels, which means it is, uh, a necessary condition, and Mercer's theorem, uh,",
    "start": "2766825",
    "end": "2774940"
  },
  {
    "text": "tells us that it's also a sufficient condition that for any K, if this property holds, then K must be a kernel function, right?",
    "start": "2774940",
    "end": "2783789"
  },
  {
    "text": "And the Mercer's theorem, um, you know, one- one lose intuition there is,",
    "start": "2783790",
    "end": "2789055"
  },
  {
    "text": "the Mercer's theorem is essentially telling us that, um, any sub-matrix of a positive semi-definite matrix is also positive semi-definite.",
    "start": "2789055",
    "end": "2797785"
  },
  {
    "text": "So you can think of kernels as these, uh, PSD functions which are infinite-dimensional, um.",
    "start": "2797785",
    "end": "2804474"
  },
  {
    "text": "And a kernel matrix is, you know, um,",
    "start": "2804475",
    "end": "2809575"
  },
  {
    "text": "a choice of or some choice of specific input values,",
    "start": "2809575",
    "end": "2816415"
  },
  {
    "text": "evaluate the kernel at those points and extract it into a matrix. And Mercer's theorem essentially tells you that,",
    "start": "2816415",
    "end": "2823089"
  },
  {
    "text": "you know, any sub-matrix of this infinite-dimensional, uh, PSD matrix or PSD function,",
    "start": "2823090",
    "end": "2828700"
  },
  {
    "text": "must also, uh, um, um, is necessarily PS- uh, a positive semi-definite and vice versa. Yes, question?",
    "start": "2828700",
    "end": "2834925"
  },
  {
    "text": "Where does x^1 through x^m come from? It can be any x^1 through x^m- any arbitrary x^1 through x^m, right?",
    "start": "2834925",
    "end": "2840360"
  },
  {
    "text": "Any example. They need not be a training set, right? It's- it's just a property of this- of this, uh, function, right?",
    "start": "2840360",
    "end": "2845490"
  },
  {
    "text": "So that was- that was, uh, kernels, um. Yes, question? Can you clarify that sub-matrix theorem?",
    "start": "2845490",
    "end": "2851985"
  },
  {
    "text": "So, uh, you know, the- the, uh, Mercer's theorem tells us that- so this infinite-dimensional space that you see,",
    "start": "2851985",
    "end": "2859165"
  },
  {
    "text": "think of- think of this as the function K, um, where this is one input, the other input, and the values here are",
    "start": "2859165",
    "end": "2865480"
  },
  {
    "text": "the- the values that the function evaluates to, right? And now, uh, think of this as an infinite-dimensional matrix, right?",
    "start": "2865480",
    "end": "2872244"
  },
  {
    "text": "And now, if you just, you know, extract the evaluations of this function at certain points, right?",
    "start": "2872245",
    "end": "2878200"
  },
  {
    "text": "So this corresponds to x^1, this corresponds to x^2, you know,",
    "start": "2878200",
    "end": "2883485"
  },
  {
    "text": "and this corresponds to x^m, and similarly this corresponds to x^1, you know, x^m, right?",
    "start": "2883485",
    "end": "2890635"
  },
  {
    "text": "So basically you're just extracting these- these, um, extracting a sub-matrix out of this infinite-dimensional matrix here, right?",
    "start": "2890635",
    "end": "2897970"
  },
  {
    "text": "And Mercer's theorem is, uh, is a- is another, uh, um, is another way to say that, uh,",
    "start": "2897970",
    "end": "2904780"
  },
  {
    "text": "a matrix, in this case, the infinite-dimension matrix is positive semi-definite if and only if any sub-matrix is positive semi-definite.",
    "start": "2904780",
    "end": "2913494"
  },
  {
    "text": "So that's- that's, uh, that's Mercer's theorem, and that's, uh, kernels. And the kernel trick is basically, uh,",
    "start": "2913495",
    "end": "2921819"
  },
  {
    "text": "if we can rewrite our algorithm to be represented as inner products of xs,",
    "start": "2921820",
    "end": "2929889"
  },
  {
    "text": "then we can replace those inner products with kernel functions, right?",
    "start": "2929889",
    "end": "2934990"
  },
  {
    "text": "And that basically allows us to redefine parameters.",
    "start": "2934990",
    "end": "2940255"
  },
  {
    "text": "So if this is our design matrix x, where you have n examples,",
    "start": "2940255",
    "end": "2946075"
  },
  {
    "text": "and let's call it, uh, uh, p features. So- so this is Phi of x, right?",
    "start": "2946075",
    "end": "2954369"
  },
  {
    "text": "So there's some feature map. And, uh, you have- you have, uh, p features, and n examples, right?",
    "start": "2954370",
    "end": "2961120"
  },
  {
    "text": "Um, in a simplistic approach- in the, in the, in the naive approach, we would define a Theta vector, right?",
    "start": "2961120",
    "end": "2969984"
  },
  {
    "text": "So we would define the Theta vector in R^p, you know,",
    "start": "2969985",
    "end": "2976120"
  },
  {
    "text": "if- if you use a feature map p. And then we would use some kind of a gradient descent to keep updating these,",
    "start": "2976120",
    "end": "2982869"
  },
  {
    "text": "um, Theta vectors with each time step by minimizing the loss, right? So I think of this as Theta at time^0,",
    "start": "2982870",
    "end": "2989650"
  },
  {
    "text": "and then Theta at time^1, Theta at time^2, and so on. And gradient descent gives us,",
    "start": "2989650",
    "end": "2994705"
  },
  {
    "text": "uh, updated, uh, Theta vectors. With the kernel method, we can basically flip it around,",
    "start": "2994705",
    "end": "3000210"
  },
  {
    "text": "and say we define one coefficient per example. So this would be, call it Beta^0,",
    "start": "3000210",
    "end": "3008730"
  },
  {
    "text": "where we have one element per example, which kind of sets the weight of that example in a way, right?",
    "start": "3008730",
    "end": "3018450"
  },
  {
    "text": "And with- with kernel methods, we keep obtaining these updated Beta vectors,",
    "start": "3018450",
    "end": "3024940"
  },
  {
    "text": "right? So gradient descent keeps hopping us from here to here.",
    "start": "3032270",
    "end": "3038250"
  },
  {
    "text": "And similarly, if you- if you kernelize it, you- you, um, you work with coefficients for different examples, right?",
    "start": "3038250",
    "end": "3044985"
  },
  {
    "text": "And this- this method, therefore basically allows us, uh,",
    "start": "3044985",
    "end": "3050295"
  },
  {
    "text": "to have infinite-dimensional feature maps, right? The naive method would start failing as you extend the p to infinity,",
    "start": "3050295",
    "end": "3058575"
  },
  {
    "text": "because you need to maintain vectors of that length. Whereas with kernel methods, we are maintaining a vector of length equal to number of examples.",
    "start": "3058575",
    "end": "3066390"
  },
  {
    "text": "And so even- even, uh, this kind of allows you to use infinitely long feature maps.",
    "start": "3066390",
    "end": "3073125"
  },
  {
    "text": "And we- we use kernel- the, uh, uh, the kernel version to evaluate the, um,",
    "start": "3073125",
    "end": "3081045"
  },
  {
    "text": "evaluate, uh, uh, the infinite-dimensional, uh, inner product in some inexpensive way.",
    "start": "3081045",
    "end": "3088240"
  },
  {
    "text": "And we maintain a finite-dimensional, uh, uh, coefficient vectors, right?",
    "start": "3088340",
    "end": "3093779"
  },
  {
    "text": "So that's- that's, uh, kernel- that's the kernel trick. We, um, in the notes,",
    "start": "3093780",
    "end": "3099390"
  },
  {
    "text": "we have- we have, uh, described how to use the ker- apply the kernel, uh, kernel trick to linear regression.",
    "start": "3099390",
    "end": "3105885"
  },
  {
    "text": "And in one of the homework problems, you kernelize the Perceptron algorithm, right? In the Perceptron algorithm,",
    "start": "3105885",
    "end": "3110945"
  },
  {
    "text": "not only did you kernelize it, but you also made it work in a streaming setting, where you're obtaining one example at a time and the Beta vector would be",
    "start": "3110945",
    "end": "3118880"
  },
  {
    "text": "extended as you keep en- as you kept encountering new examples, right?",
    "start": "3118880",
    "end": "3124234"
  },
  {
    "text": "So the, uh, that was from, uh, kernelizing the perceptron, right?",
    "start": "3124235",
    "end": "3136520"
  },
  {
    "text": "And then we briefly covered support vector machines.",
    "start": "3136520",
    "end": "3142180"
  },
  {
    "text": "So support vector machine is a kernel-based classification algorithm. A kernelize perceptron is also a kernel-based classification algorithm.",
    "start": "3142180",
    "end": "3150609"
  },
  {
    "text": "The perce- the support vector machine, however, focuses on something called as the geometric margin",
    "start": "3150610",
    "end": "3155740"
  },
  {
    "text": "that defines the separating hyperplane, right? The, uh, the algorithm,",
    "start": "3155740",
    "end": "3162730"
  },
  {
    "text": "such as logistic regression, try to maximize the functional margin, which is different from the, uh, geometric margin.",
    "start": "3162730",
    "end": "3170125"
  },
  {
    "text": "And you saw the distinction between functional margin and geometric margin in the, uh, um, again,",
    "start": "3170125",
    "end": "3175619"
  },
  {
    "text": "in- in, uh, uh, P Set 2 question 1 on training stability of- of, uh, logistic regression.",
    "start": "3175620",
    "end": "3181260"
  },
  {
    "text": "Well, logistic regression, by trying to maximize the functional margin, would keep extending the margin all the way to",
    "start": "3181260",
    "end": "3188230"
  },
  {
    "text": "plus infinity because it found a well, eh, uh, uh, uh, a hyperplane that could be separated, and it would just gain for free uh,",
    "start": "3188230",
    "end": "3196130"
  },
  {
    "text": "uh, a higher functional margin by just scaling all the values up and it would just keep, uh, scaling it forever.",
    "start": "3196130",
    "end": "3201340"
  },
  {
    "text": "Whereas with support vector machine, you don't have such problems because you're trying to maximize the geometric margin.",
    "start": "3201340",
    "end": "3206740"
  },
  {
    "text": "And the geometric margin is the geometric margin, uh, because it's a geometric concept, right?",
    "start": "3206740",
    "end": "3212619"
  },
  {
    "start": "3212000",
    "end": "3352000"
  },
  {
    "text": "And the support vector machine also has this other nice benefit that",
    "start": "3212620",
    "end": "3217705"
  },
  {
    "text": "the coefficient vector that you end up with at the end of training is going to be sparse.",
    "start": "3217705",
    "end": "3225205"
  },
  {
    "text": "By sparse, it means that most of them will be zeros, except a few examples,",
    "start": "3225205",
    "end": "3232570"
  },
  {
    "text": "the rest will all be 0s , right? And these few examples which have",
    "start": "3232570",
    "end": "3237730"
  },
  {
    "text": "a non-zero coefficient are also called the support vectors, right? So the idea there is, let's say you have, uh, you know,",
    "start": "3237730",
    "end": "3245830"
  },
  {
    "text": "some classes over here and another class over here with examples.",
    "start": "3245830",
    "end": "3251470"
  },
  {
    "text": "Support vector machine tries to find this separating hyperplane that maximizes the geometric margin between the two classes.",
    "start": "3251470",
    "end": "3259974"
  },
  {
    "text": "And in order to, uh, [NOISE] decide this- this, uh, exact position of the separating hyperplane,",
    "start": "3259975",
    "end": "3266650"
  },
  {
    "text": "all what matters are these nearest examples from both classes, right? And these two- these two, uh, uh,",
    "start": "3266650",
    "end": "3274240"
  },
  {
    "text": "subsets of examples end up being the support vectors. And the- the, uh, the location of",
    "start": "3274240",
    "end": "3281440"
  },
  {
    "text": "the other examples don't matter so they get a coefficient of zero, but the nearest examples end up having non-zero coefficients,",
    "start": "3281440",
    "end": "3288655"
  },
  {
    "text": "and tho- those are also called the, uh, support vectors, right? So support vector machines are- are- are kind of this nice- they have this nice benefit",
    "start": "3288655",
    "end": "3298495"
  },
  {
    "text": "that you can use kernel methods and get scalability in terms of features to infinite dimensions.",
    "start": "3298495",
    "end": "3306040"
  },
  {
    "text": "But also, unlike other kernel methods where you need to hold on to the trainings- the entire training set into test-time,",
    "start": "3306040",
    "end": "3314755"
  },
  {
    "text": "support vector machines allow you to hold on to just these few support vectors into test-time, right?",
    "start": "3314755",
    "end": "3321490"
  },
  {
    "text": "So- so the sparse ql co-effi- the sparse coefficients that you get from support vector machines gives you",
    "start": "3321490",
    "end": "3328990"
  },
  {
    "text": "this nice benefit that you get scalability in terms of number of features and also scalability in terms of number of examples,",
    "start": "3328990",
    "end": "3336684"
  },
  {
    "text": "because you're only gonna hold on to, you know, a few support vectors into test-time.",
    "start": "3336684",
    "end": "3342234"
  },
  {
    "text": "Whereas with other kernel- kernel methods, do you generally need to hold on to the entire training set into test-time, right?",
    "start": "3342235",
    "end": "3349825"
  },
  {
    "text": "So that was, uh, support vector machines? [NOISE] And then we moved on to another kind of",
    "start": "3349825",
    "end": "3357160"
  },
  {
    "start": "3352000",
    "end": "3599000"
  },
  {
    "text": "kernel algorithm called Gaussian processes, right?",
    "start": "3357160",
    "end": "3365890"
  },
  {
    "text": "So Gaussian processes. So Gaussian processes is a kernel method for regression, right?",
    "start": "3365890",
    "end": "3374244"
  },
  {
    "text": "So support vector machine and kernelize perceptron were kernel methods for classification, right?",
    "start": "3374245",
    "end": "3380605"
  },
  {
    "text": "Gaussian processes is a kernel method for regression, right?",
    "start": "3380605",
    "end": "3390535"
  },
  {
    "text": "And with Gaussian processes, the- the, uh, the way we- we, uh,",
    "start": "3390535",
    "end": "3398290"
  },
  {
    "text": "define Gaussian processes is to generalize Gaussian distributions to infinite dimensions, right?",
    "start": "3398290",
    "end": "3404950"
  },
  {
    "text": "And in infinite dimensions, uh, so if we have a Gaussian vector, right?",
    "start": "3404950",
    "end": "3411339"
  },
  {
    "text": "Of, you know, uh, uh, of a certain dimension that has a certain mean and a certain covariance,",
    "start": "3411340",
    "end": "3422359"
  },
  {
    "text": "in a Gaussian process, you have this function,",
    "start": "3423210",
    "end": "3428785"
  },
  {
    "text": "where you are- where we are, uh, thinking of function as an infinitely long vector, right?",
    "start": "3428785",
    "end": "3435880"
  },
  {
    "text": "So it's- it's- it's like a continuous version of an array- of- of- of a vector which extends to infinity, right?",
    "start": "3435880",
    "end": "3443214"
  },
  {
    "text": "And this is a- a Gaussian process that has a mean function, which is,",
    "start": "3443215",
    "end": "3450220"
  },
  {
    "text": "again, infinite dimension, and instead of a matrix, you have a covariance function k, right?",
    "start": "3450220",
    "end": "3458950"
  },
  {
    "text": "So a Gaussian process is this infinite dimension extension of a multivariate Gaussian distribution, right?",
    "start": "3458950",
    "end": "3466885"
  },
  {
    "text": "And there are certain properties of multivariate Gaussian distribution that makes marginalization easy.",
    "start": "3466885",
    "end": "3473110"
  },
  {
    "text": "So if you want to marginalize out, one of the- one of the, uh, components of- of a- a Gaussian, uh, distribution,",
    "start": "3473110",
    "end": "3480310"
  },
  {
    "text": "you just ignore that component in the mean and ignore that row and column in the covariance function, right?",
    "start": "3480310",
    "end": "3485619"
  },
  {
    "text": "So marginalization is very easy in- in Gaussian distribution. So one way to think of the way we, uh,",
    "start": "3485620",
    "end": "3492430"
  },
  {
    "text": "apply Gaussian processes to, um, uh, uh, to, uh, uh, given- a dataset is to,",
    "start": "3492430",
    "end": "3498865"
  },
  {
    "text": "uh, it is an incorrect, uh, uh, technically incorrect,",
    "start": "3498865",
    "end": "3504250"
  },
  {
    "text": "but very useful in terms of, [NOISE] uh, understanding that you have this infinite dimensional Gaussian vector and we",
    "start": "3504250",
    "end": "3512440"
  },
  {
    "text": "marginalize out everything that is outside our training set and test set.",
    "start": "3512440",
    "end": "3517675"
  },
  {
    "text": "So choose only those points in this infinite- infinite,",
    "start": "3517675",
    "end": "3523780"
  },
  {
    "text": "uh, dimensional, uh, vector. Choose that finite subset of points,",
    "start": "3523780",
    "end": "3528924"
  },
  {
    "text": "where that finite subset is made up of examples in our training set and examples in our test set, right?",
    "start": "3528925",
    "end": "3535734"
  },
  {
    "text": "And that basically kind of condenses this infinitely, uh, infinite long mean function into a finite dimensional mean vector and an infinite,",
    "start": "3535735",
    "end": "3547135"
  },
  {
    "text": "you know, a- a covariance function into a covariance matrix. And this covariance matrix that you- that you obtain is exactly like this.",
    "start": "3547135",
    "end": "3557410"
  },
  {
    "text": "How you obtain a kernel matrix, uh, with kernel methods, you obtain a covariance matrix,",
    "start": "3557410",
    "end": "3563318"
  },
  {
    "text": "uh, by using, uh, the kernel function. And we saw that a kernel matrix must always be positive semi-definite,",
    "start": "3563319",
    "end": "3570325"
  },
  {
    "text": "Mercer's theorem told us, and covariance, uh, functions, uh, or covariance matrices need to be positive semi-definite,",
    "start": "3570325",
    "end": "3576955"
  },
  {
    "text": "so that kind of matches up, right? And once we obtain this, we just use the, uh,",
    "start": "3576955",
    "end": "3582955"
  },
  {
    "text": "conditioning of Gaussian distributions to condition on the training set",
    "start": "3582955",
    "end": "3588115"
  },
  {
    "text": "and obtain posterior on the test set on- on the examples that we want to make a prediction, right?",
    "start": "3588115",
    "end": "3593800"
  },
  {
    "text": "So that's- that's Gaussian processes. The mathematical notation can be a little messy,",
    "start": "3593800",
    "end": "3599275"
  },
  {
    "text": "but essentially what's happening there is- is just this, right? Marginalize out everything that's not necessary and use the conditioning rules, right?",
    "start": "3599275",
    "end": "3608245"
  },
  {
    "text": "The rules themselves are heavy on notation, but, you know, you just plug the- the, uh, the, uh- the- the correct pieces into the conditioning, uh,",
    "start": "3608245",
    "end": "3616119"
  },
  {
    "text": "rule and you get your predictive distribution on- on unseen examples, right?",
    "start": "3616120",
    "end": "3621145"
  },
  {
    "text": "So that's- that's Gaussian processes. [NOISE] Any questions on that? Yeah. Yes, question.",
    "start": "3621145",
    "end": "3628525"
  },
  {
    "text": "What defines what the kernel we use for that? Yeah, good question. So what kernel do we use?",
    "start": "3628525",
    "end": "3633820"
  },
  {
    "text": "And generally, the choice of kernel is a hyperparameter that you tune. You try different kernels and see what's best- what works best",
    "start": "3633820",
    "end": "3640510"
  },
  {
    "text": "instead of giving you good predictive power on a validation set, right? You generally- you generally play around with",
    "start": "3640510",
    "end": "3645760"
  },
  {
    "text": "a few different kernels and- and choose one that works best for you. [NOISE] So the- the, uh,",
    "start": "3645760",
    "end": "3653950"
  },
  {
    "text": "choice of which kernel you want to use is somewhat equivalent to the question of asking,",
    "start": "3653950",
    "end": "3661224"
  },
  {
    "text": "what feature map do you want to use with linear regression, it's- it's- it's very similar. [NOISE] right?",
    "start": "3661225",
    "end": "3667795"
  },
  {
    "text": "So then we moved on to neural networks. [NOISE]",
    "start": "3667795",
    "end": "3679895"
  },
  {
    "text": "So with neural networks. We wanna think of neural networks as these complex differentiable",
    "start": "3679895",
    "end": "3690900"
  },
  {
    "text": "layer-wise or composite functions that have parameters at",
    "start": "3691420",
    "end": "3697714"
  },
  {
    "text": "all layers with non-linearities, right?",
    "start": "3697715",
    "end": "3704435"
  },
  {
    "text": "So imagine, um, you're given- you're fitting an input vector x into a model, right?",
    "start": "3704435",
    "end": "3712670"
  },
  {
    "text": "So this is x in Rd, and so we're gonna- we're gonna talk about, uh,",
    "start": "3712670",
    "end": "3718880"
  },
  {
    "text": "what's commonly called as- as, uh, fully connected networks. [NOISE]",
    "start": "3718880",
    "end": "3731328"
  },
  {
    "text": "This is one specific architecture of neural networks where you start with, uh, um, the input x that you're feeding into your model,",
    "start": "3731329",
    "end": "3740464"
  },
  {
    "text": "and you take it through these hidden layers, right?",
    "start": "3740465",
    "end": "3748400"
  },
  {
    "text": "Where from the input layer, this is called, you know, you can think of it as layer 0, you go to hidden layer 1,",
    "start": "3748400",
    "end": "3754924"
  },
  {
    "text": "by multiplying this by a matrix W- Wx plus some bias b,",
    "start": "3754925",
    "end": "3765720"
  },
  {
    "text": "and running all of these through some non-linearity g, right?",
    "start": "3767050",
    "end": "3773420"
  },
  {
    "text": "So g of Wx plus b will give you this vector, right?",
    "start": "3773420",
    "end": "3778970"
  },
  {
    "text": "So W- W is, uh, a- a parameter that you wanna learn, right? And x is the input that's going in,",
    "start": "3778970",
    "end": "3786230"
  },
  {
    "text": "b is another set of parameters that you wanna learn, g is some element-wise non-linearity,",
    "start": "3786230",
    "end": "3791734"
  },
  {
    "text": "like the sigmoid function for example, right? And depending on the dimension of W, right?",
    "start": "3791735",
    "end": "3797300"
  },
  {
    "text": "W will generally, uh, be our, uh, let's- let's,",
    "start": "3797300",
    "end": "3803599"
  },
  {
    "text": "uh, um, um, let's call this, uh, k by d, right?",
    "start": "3803600",
    "end": "3810410"
  },
  {
    "text": "So Wx will therefore be, [NOISE] let's call it Rk,",
    "start": "3810410",
    "end": "3816980"
  },
  {
    "text": "where this has, um, uh, k components in it, and this had d components in it, right?",
    "start": "3816980",
    "end": "3825710"
  },
  {
    "text": "And then we basically uh, perform- perform, uh, another layer of the same.",
    "start": "3825710",
    "end": "3830975"
  },
  {
    "text": "So g of- so this is W1, you have another W2 of- of this which- let's call",
    "start": "3830975",
    "end": "3842590"
  },
  {
    "text": "it a1 plus b2, right?",
    "start": "3842590",
    "end": "3849985"
  },
  {
    "text": "And so on. Eventually, we bring it down to one scalar,",
    "start": "3849985",
    "end": "3855455"
  },
  {
    "text": "which we call it as y^, right? And then corresponding to this x,",
    "start": "3855455",
    "end": "3860570"
  },
  {
    "text": "there is another y. So you take the output of the network, take the actual output,",
    "start": "3860570",
    "end": "3866525"
  },
  {
    "text": "and combine the two into some kind of a loss, right? And this loss is a scalar.",
    "start": "3866525",
    "end": "3873350"
  },
  {
    "text": "These- these outputs and labels could possibly be- be vectors. So if you're doing multi-class classification,",
    "start": "3873350",
    "end": "3880040"
  },
  {
    "text": "this would give you- thi- this would be like the output of softmax, so it would be a full vector, and the, uh,",
    "start": "3880040",
    "end": "3885665"
  },
  {
    "text": "true- true, uh, label would be a one-hat vector of what the correct answer is, right? And- but the loss is always a scalar valued loss, right?",
    "start": "3885665",
    "end": "3894125"
  },
  {
    "text": "That's- that's, uh, that's the, uh, common convention. We start with a scalar value of loss, and now we want to- we wanna calculate the- the, uh,",
    "start": "3894125",
    "end": "3902810"
  },
  {
    "text": "the gradient of this loss, with respect to every parameter and every layer.",
    "start": "3902810",
    "end": "3909155"
  },
  {
    "text": "And in order to calculate that, we use an algorithm called backpropagation, and backpropagation is essentially just the multivariate,",
    "start": "3909155",
    "end": "3917960"
  },
  {
    "text": "uh, uh, uh, chain rule of calculus, right? So the way you wanna think of backpropagation is always start with one scalar parameter,",
    "start": "3917960",
    "end": "3927363"
  },
  {
    "text": "let's say W, um, um, i j, and try to calculate the partial of loss with",
    "start": "3927364",
    "end": "3937190"
  },
  {
    "text": "respect to partial of Wij of some layer l, right?",
    "start": "3937190",
    "end": "3944510"
  },
  {
    "text": "And this will always- must always evaluate to a scalar because you're- you're calculating",
    "start": "3944510",
    "end": "3951499"
  },
  {
    "text": "the- the gradient of a scalar with respect to a scalar. And the intermediate steps to calculate this will involve, you know,",
    "start": "3951499",
    "end": "3960235"
  },
  {
    "text": "going from the scalar to a vector, and from vector to vector, vector to vector, and finally vector two scalar, right?",
    "start": "3960235",
    "end": "3968204"
  },
  {
    "text": "And so- um, and each of those intermediate components are called Jacobians, right?",
    "start": "3968205",
    "end": "3974555"
  },
  {
    "text": "Going from vector to vector, and, and- um, uh, so each of those intermediate, uh, um, a- are called Jacobians.",
    "start": "3974555",
    "end": "3981460"
  },
  {
    "text": "And you will encounter- you'll generally see that the seq- that the chain of Jacobians that you encounter while calculating, uh,",
    "start": "3981460",
    "end": "3988855"
  },
  {
    "text": "this will generally be, you know, we have a row vector, matrix, matrix, matrix, and finally a column vector, right?",
    "start": "3988855",
    "end": "4000115"
  },
  {
    "text": "And once- once you- once you multiply them out, this will just evaluate to a scalar. This- this is- this is, uh, gonna be common.",
    "start": "4000115",
    "end": "4006970"
  },
  {
    "text": "And for layers where you are uh, performing a non-linearity, the Jacobian will be a diagonal matrix,",
    "start": "4006970",
    "end": "4014454"
  },
  {
    "text": "and for computational reasons, you- you know, you- you might want to, instead of, uh, uh, implementing also a diagonal matrix,",
    "start": "4014455",
    "end": "4021190"
  },
  {
    "text": "you might just wanna do element-wise multiplication, right? So, uh, perform this for every possible,",
    "start": "4021190",
    "end": "4028270"
  },
  {
    "text": "uh, uh, parameter in the network, and then you can use some- um, um,",
    "start": "4028270",
    "end": "4033370"
  },
  {
    "text": "you can identify common patterns and define update rule for the entire matrix in a more compact,",
    "start": "4033370",
    "end": "4039640"
  },
  {
    "text": "uh, compact form, right? And the- so- so all the- all that is basically just algebraic manipulation.",
    "start": "4039640",
    "end": "4045025"
  },
  {
    "text": "But the goal that we're uh, trying to achieve is to calculate the gradient of the loss, which is scalar, with respect to every scalar parameter, right?",
    "start": "4045025",
    "end": "4054160"
  },
  {
    "text": "And once you- once you calculate these gradients, it's basically just gradient descent, right?",
    "start": "4054160",
    "end": "4060070"
  },
  {
    "text": "Perform gradient descent on all these parameters simultaneously. Take a small step to minimize the loss with respect to all these parameters,",
    "start": "4060070",
    "end": "4067180"
  },
  {
    "text": "and take a small step and, you know, repeat until you converge. So that was neural networks, uh, and backpropagation.",
    "start": "4067180",
    "end": "4074695"
  },
  {
    "text": "And after neural networks, we moved on to some learning theory, basically bias-variance analysis.",
    "start": "4074695",
    "end": "4084370"
  },
  {
    "text": "So bias-variance analysis is probably the most important concept from- from the entire course,",
    "start": "4084370",
    "end": "4091810"
  },
  {
    "text": "and it's this bias-variance analysis that distinguishes machine learning from say,",
    "start": "4091810",
    "end": "4097359"
  },
  {
    "text": "you know, optimization, right? In- in, uh, in general optimization problems,",
    "start": "4097360",
    "end": "4103809"
  },
  {
    "text": "you're given some kind of a function or an objective, and you wanna maximize or minimize it, right?",
    "start": "4103810",
    "end": "4109015"
  },
  {
    "text": "And we use such optimization techniques like gradient methods, Newton's method, um, or even just close form expressions for maximizing or minimizing,",
    "start": "4109015",
    "end": "4117040"
  },
  {
    "text": "uh, some kind of a loss, right? But what distinguishes machine learning from just optimization,",
    "start": "4117040",
    "end": "4123535"
  },
  {
    "text": "is the bias-variance trade off. Which means, uh, wha- what it actually means is, our end goal is not really minimizing the training objective itself.",
    "start": "4123535",
    "end": "4133795"
  },
  {
    "text": "Yes, we are- we are minimizing the training objective, but our goal really is to perform well on unseen data, right?",
    "start": "4133795",
    "end": "4139930"
  },
  {
    "text": "And we're kind of doing this minimization of the training loss as a proxy,",
    "start": "4139930",
    "end": "4145390"
  },
  {
    "text": "with the hope that we're gonna do well on test data, right? So the- the, um, uh, um,",
    "start": "4145390",
    "end": "4152350"
  },
  {
    "text": "bias-variance analysis is- is a way to kind of decompose our test error or the encounter- the- the, uh,",
    "start": "4152350",
    "end": "4160674"
  },
  {
    "text": "error that we encounter at test-time, and break it down into sub-components, uh, for bias, variance and irreducible error, right?",
    "start": "4160675",
    "end": "4169270"
  },
  {
    "text": "So irreducible error, uh, and- and this thing decomposition generally, um, holds true for all losses.",
    "start": "4169270",
    "end": "4175870"
  },
  {
    "text": "But in the sp- in- in the specific case of a squared error loss, we get a very clean decomposition that mean squared error, you know,",
    "start": "4175870",
    "end": "4183955"
  },
  {
    "text": "on test-set or on a test example, is the sum of [NOISE] irreducible error",
    "start": "4183955",
    "end": "4191270"
  },
  {
    "text": "plus [NOISE] bias squared plus variance, right?",
    "start": "4192090",
    "end": "4202344"
  },
  {
    "text": "The- the fundamental assumption is that our data is noisy, right? And this noise can- can,",
    "start": "4202345",
    "end": "4209590"
  },
  {
    "text": "uh, can affect our test error in two ways. So first of all, the test example that we're gonna",
    "start": "4209590",
    "end": "4214750"
  },
  {
    "text": "encounter itself is gonna be noisy, right? And so the noise in the test example contributes to,",
    "start": "4214750",
    "end": "4221514"
  },
  {
    "text": "inde- irreducible error, right? So this is noise in test example.",
    "start": "4221515",
    "end": "4229940"
  },
  {
    "text": "This basically tells us that no matter what model you choose, right?",
    "start": "4229940",
    "end": "4235050"
  },
  {
    "text": "Choose the best possible model that can possibly be, you know, uh, uh, uh, um, possibly be imagined.",
    "start": "4235050",
    "end": "4240965"
  },
  {
    "text": "Even that model is gonna encounter some error at test-time because your test data itself is noisy, right?",
    "start": "4240965",
    "end": "4248500"
  },
  {
    "text": "So that's irreducible error. No matter what model you get, you're gonna- you- you're gonna pay this irreducible error penalty no matter what, right?",
    "start": "4248500",
    "end": "4255610"
  },
  {
    "text": "And then the bias and variance are basically- um,",
    "start": "4255610",
    "end": "4261085"
  },
  {
    "text": "so you can think of the- the noise in the training data that you have to be contributed- to be contributing to- to variance.",
    "start": "4261085",
    "end": "4270370"
  },
  {
    "text": "So training [NOISE] data noise",
    "start": "4270370",
    "end": "4276480"
  },
  {
    "text": "[NOISE] contributes to the variance of the model, right?",
    "start": "4276480",
    "end": "4286720"
  },
  {
    "text": "And bias is more or less, uh, kind of telling you how inflexible your model is, right?",
    "start": "4286720",
    "end": "4292630"
  },
  {
    "text": "So your data may be- may be saying some story, of course, a noisy story of how- of- of- of the pattern between your x's and y's,",
    "start": "4292630",
    "end": "4300370"
  },
  {
    "text": "but the model that you have may be limited to just linear models. Even though your data has a clear- clear,",
    "start": "4300370",
    "end": "4307135"
  },
  {
    "text": "let's say, you know, quadratic relationship, x and y,",
    "start": "4307135",
    "end": "4313540"
  },
  {
    "text": "where, you know, there's a clear quadratic linear, uh, relationship between x's and y's.",
    "start": "4313540",
    "end": "4318909"
  },
  {
    "text": "If you happen to choose linear models, then obviously your model is biased because you're limited to",
    "start": "4318910",
    "end": "4325150"
  },
  {
    "text": "solutions that look like this, right? So- so bias is mostly due to,",
    "start": "4325150",
    "end": "4333040"
  },
  {
    "text": "um, inflexibility in your- in your, uh, um, model class, inflexibility or limited capacity.",
    "start": "4333040",
    "end": "4346179"
  },
  {
    "text": "[NOISE] Of model class.",
    "start": "4346180",
    "end": "4353300"
  },
  {
    "text": "And variance is due to noise in- in your- in your, uh, training data, right?",
    "start": "4354270",
    "end": "4360640"
  },
  {
    "text": "And so, our goal is to not minimize the training loss,",
    "start": "4360640",
    "end": "4367315"
  },
  {
    "text": "but our goal is to actually minimize the test loss, right? Or the test error. And this test error has",
    "start": "4367315",
    "end": "4375040"
  },
  {
    "text": "one component called irreducible error for which you can do nothing about. So we focus on just bias and variance, right?",
    "start": "4375040",
    "end": "4381670"
  },
  {
    "text": "And the- the kind of action space that we have includes kind of contradictory actions, right?",
    "start": "4381670",
    "end": "4389680"
  },
  {
    "text": "So some action- no, one action is, you know, increase regularization. And increasing regularization will reduce variance.",
    "start": "4389680",
    "end": "4398844"
  },
  {
    "text": "But there is also another action called reduce regularization, which will re- which will- which will,",
    "start": "4398844",
    "end": "4405070"
  },
  {
    "text": "uh- uh, fight bias. So in order to decide which action you wanna take, you want to get a good sense of what is the contribution of",
    "start": "4405070",
    "end": "4413170"
  },
  {
    "text": "bias versus what's the contribution of variance in your test error. And a loose proxy for that is,",
    "start": "4413170",
    "end": "4420655"
  },
  {
    "text": "you can think of [NOISE] bias as training error [NOISE] and",
    "start": "4420655",
    "end": "4429635"
  },
  {
    "text": "variance as gap between",
    "start": "4429635",
    "end": "4435144"
  },
  {
    "text": "test or validation and training error.",
    "start": "4435145",
    "end": "4441110"
  },
  {
    "text": "[NOISE] So bias technically is not the training error,",
    "start": "4441110",
    "end": "4449215"
  },
  {
    "text": "but for the purposes of choosing the action to take, it works as a- as a sufficient proxy to think of,",
    "start": "4449215",
    "end": "4455665"
  },
  {
    "text": "uh, bias as training error. So if your training error is very high, then you wanna take steps that fight bias.",
    "start": "4455665",
    "end": "4462355"
  },
  {
    "text": "So in that way, it's- it's- it's a reasonable proxy for the purposes of the bias-variance analysis to think of bias as the training error.",
    "start": "4462355",
    "end": "4469480"
  },
  {
    "text": "And the variance is the gap between the- the test and- and, uh, um, test or validation and- and training error.",
    "start": "4469480",
    "end": "4476470"
  },
  {
    "text": "And we- we also discussed some- some actions, uh, in the- in the, uh,",
    "start": "4476470",
    "end": "4481690"
  },
  {
    "text": "in the previous class of what- what- what actions help fight bias versus what action helps fight variance.",
    "start": "4481690",
    "end": "4488365"
  },
  {
    "text": "And before you take any action, you always, always want to get a breakdown of test error and train error and- and,",
    "start": "4488365",
    "end": "4494425"
  },
  {
    "text": "uh, kind of make, um, a judgment call of whether you're facing a high bias problem or facing a high variance problem,",
    "start": "4494425",
    "end": "4501445"
  },
  {
    "text": "and accordingly, take an action that will reduce your overall test error.",
    "start": "4501445",
    "end": "4506485"
  },
  {
    "text": "All right. So that's- that's bias-variance problem. And this methodology, this bias-variance way of thinking holds for classification,",
    "start": "4506485",
    "end": "4514469"
  },
  {
    "text": "holds for regression, holds for supervised, holds for unsupervised. It- and it is- it is- it is kind of this-",
    "start": "4514470",
    "end": "4521555"
  },
  {
    "text": "this topic that permeates all- all the models and all the algorithms that we consider.",
    "start": "4521555",
    "end": "4526630"
  },
  {
    "text": "And it's probably the most useful when you're applying your model- ap- applying, uh, machine learning for a new problem that you're working on in practice.",
    "start": "4526630",
    "end": "4534940"
  },
  {
    "text": "Because the new- the new algorithm that you are working on in practice might be, you know, an algorithm that you've- that we've not studied in this class,",
    "start": "4534940",
    "end": "4542080"
  },
  {
    "text": "for example, random forests. But you can apply bias-variance for that algorithm as well. It works for all algorithms.",
    "start": "4542080",
    "end": "4549040"
  },
  {
    "text": "All right. So that was- that was, uh, bias-variance analysis. Then we studied regularization.",
    "start": "4549040",
    "end": "4556510"
  },
  {
    "text": "[NOISE] And I gave a Bayesian interpretation for regularization, right?",
    "start": "4556510",
    "end": "4564730"
  },
  {
    "text": "So regularization, um, is a way in which you add a penalty for large values of",
    "start": "4564730",
    "end": "4572260"
  },
  {
    "text": "your parameters where the intuition is that large values of parameters can result in very complex models.",
    "start": "4572260",
    "end": "4581935"
  },
  {
    "text": "And in order to kind of minimize the complexity of our models, we wanna limit the- the- the,",
    "start": "4581935",
    "end": "4587470"
  },
  {
    "text": "uh, values or the magnitudes of- of our parameters. And so we, um, if our,",
    "start": "4587470",
    "end": "4594205"
  },
  {
    "text": "um- um, so our loss, you know,",
    "start": "4594205",
    "end": "4599335"
  },
  {
    "text": "if it's generally some kind of i equals 1- i equals 1 to n.",
    "start": "4599335",
    "end": "4605605"
  },
  {
    "text": "Let's say y_i minus h Theta of x_i square.",
    "start": "4605605",
    "end": "4612370"
  },
  {
    "text": "If this is our regular loss, we want to augment it with some kind of a penalty for the loss,",
    "start": "4612370",
    "end": "4622410"
  },
  {
    "text": "uh, for- for the parameters, uh, being too large, right? And you can either, um- um, there are several choices here.",
    "start": "4622410",
    "end": "4629505"
  },
  {
    "text": "You may sometimes want to penalize the 1-norm of the- of the, uh, parameters.",
    "start": "4629505",
    "end": "4635905"
  },
  {
    "text": "And we- we saw that, uh, adding- adding the 2-norm for the parameters is",
    "start": "4635905",
    "end": "4641829"
  },
  {
    "text": "equal- is - has this Bayesian interpretation of performing MAP estimation with Gaussian prior [NOISE].",
    "start": "4641830",
    "end": "4653920"
  },
  {
    "text": "And per- with one, uh, with- with the 1-norm is equivalent to performing MAP estimation with the Laplace prior.",
    "start": "4653920",
    "end": "4660730"
  },
  {
    "text": "[NOISE]. And in your homework,",
    "start": "4660730",
    "end": "4665950"
  },
  {
    "text": "you also saw how to- how- you know, what the exact value of this Lambda term should",
    "start": "4665950",
    "end": "4671290"
  },
  {
    "text": "be depending on the parameters of the Gaussian and Laplace distribution. So that was regularization.",
    "start": "4671290",
    "end": "4678744"
  },
  {
    "text": "And after that, we move on to reinforcement learning, [NOISE] right?",
    "start": "4678745",
    "end": "4686110"
  },
  {
    "text": "Reinforcement learning is this- this, um, is slightly different from",
    "start": "4686110",
    "end": "4693040"
  },
  {
    "text": "the other algorithms in the sense that in the other algorithms, we make this IID assumption that for each prediction,",
    "start": "4693040",
    "end": "4699565"
  },
  {
    "text": "and each prediction or each example is completely independent of another example,",
    "start": "4699565",
    "end": "4705040"
  },
  {
    "text": "uh, on which we're making a prediction. Instead, where in reinforcement learning, we are in this sequential decision-making, uh,",
    "start": "4705040",
    "end": "4712540"
  },
  {
    "text": "situation where the predictions that we make at one, uh- uh,",
    "start": "4712540",
    "end": "4718120"
  },
  {
    "text": "at one situation will cho- will- will- will, uh, result in some action being taken.",
    "start": "4718120",
    "end": "4725020"
  },
  {
    "text": "And that action will decide the next state that we'll end up with. And therefore decide the next situation where our prediction is being made, right?",
    "start": "4725020",
    "end": "4733630"
  },
  {
    "text": "So the examples, if you think of each timestep, um, in- in reinforcement learning as an example,",
    "start": "4733630",
    "end": "4739615"
  },
  {
    "text": "these examples are not IID anymore, they're all kind of correlated. And in order to formalize reinforcement learning,",
    "start": "4739615",
    "end": "4747204"
  },
  {
    "text": "we define something called an MDP or a Markov decision process. That's a tuple of states, actions, transition probabilities,",
    "start": "4747205",
    "end": "4756595"
  },
  {
    "text": "PSA, in fact, a set of, uh, transition probabilities, a discount factor,",
    "start": "4756595",
    "end": "4761860"
  },
  {
    "text": "and a reward function, right? So the reward function- so we can be in one of the many states,",
    "start": "4761860",
    "end": "4769120"
  },
  {
    "text": "which is defined by the set S. We can take one of the many actions defined by the set of actions A.",
    "start": "4769120",
    "end": "4776485"
  },
  {
    "text": "And depending on which state we are and what action we take, we end up in a new state,",
    "start": "4776485",
    "end": "4782635"
  },
  {
    "text": "and that transition or the dynamics of moving to the next state is captured by this,",
    "start": "4782635",
    "end": "4788559"
  },
  {
    "text": "uh- uh, this- this set of transition probability vectors, PSA.",
    "start": "4788560",
    "end": "4793840"
  },
  {
    "text": "And at each state, when we arrive at a state, we obtain a reward that's defined by the reward function.",
    "start": "4793840",
    "end": "4801534"
  },
  {
    "text": "And there is this discount factor called Gamma, which- which basically tells the rewards that are",
    "start": "4801535",
    "end": "4808375"
  },
  {
    "text": "obtained sooner are better than rewards obtained later, right? And Gamma is generally a value between 0 and 1.",
    "start": "4808375",
    "end": "4816190"
  },
  {
    "text": "So this is the- the formalism in which we- we, uh, we, uh, attack the reinforcement learning problem.",
    "start": "4816190",
    "end": "4822895"
  },
  {
    "text": "And based on this MDP, we define these two, um, kind of related concepts called value and policy, right?",
    "start": "4822895",
    "end": "4830860"
  },
  {
    "text": "So value and policy. So policy is a mapping from state to action,",
    "start": "4830860",
    "end": "4841014"
  },
  {
    "text": "right? It's like a rule book. If you're in a particular state, what is the action that we should take, right?",
    "start": "4841015",
    "end": "4846415"
  },
  {
    "text": "This rule book is called the policy. And the value V that corresponds to a policy",
    "start": "4846415",
    "end": "4854620"
  },
  {
    "text": "Pi of a given state is basically telling us if we were to start in a state S,",
    "start": "4854620",
    "end": "4864805"
  },
  {
    "text": "and were to continue the trajectory by taking actions according to the policy Pi, right?",
    "start": "4864805",
    "end": "4871555"
  },
  {
    "text": "Which means when you are in state S, take action according to, you know, the value Pi of S evaluates to.",
    "start": "4871555",
    "end": "4878710"
  },
  {
    "text": "And that action, according to the dynamics, is gonna take to a random new state. And in that new state, which is random,",
    "start": "4878710",
    "end": "4886825"
  },
  {
    "text": "refer to the rule book and take an action according to that policy, right? And so on.",
    "start": "4886825",
    "end": "4891895"
  },
  {
    "text": "And if we were to repeat this- this pr- uh, this process over and over, where at each time, you know,",
    "start": "4891895",
    "end": "4898405"
  },
  {
    "text": "you- you- you can end up in a different state according to the dynamics. What is the average accumulated sum of discounted rewards?",
    "start": "4898405",
    "end": "4906655"
  },
  {
    "text": "And that is- is- is- is captured by this function called V Pi. Yes, question.",
    "start": "4906655",
    "end": "4912655"
  },
  {
    "text": "So, did- did you just say- did you just say same value in policy in the nation or- Just the concepts of policy and value.",
    "start": "4912655",
    "end": "4919990"
  },
  {
    "text": "So policy is just a rulebook, and value is the- is the long-term reward that you're",
    "start": "4919990",
    "end": "4926620"
  },
  {
    "text": "gonna accumulate by starting at S and following the rulebook pi, right?",
    "start": "4926620",
    "end": "4931580"
  },
  {
    "text": "And- and these two concepts, you know, uh, policy and value, are- are, kind of related.",
    "start": "4932220",
    "end": "4941140"
  },
  {
    "text": "So if you are given a policy Pi, you can perform something called as policy evaluation and get V^Pi- er, V_^Pi.",
    "start": "4941140",
    "end": "4955640"
  },
  {
    "text": "And to get V^Pi- you can- you can- you can obtain V^Pi using a simple, uh, um, um, set of linear- linear equations.",
    "start": "4956550",
    "end": "4964080"
  },
  {
    "text": "You can just solve a linear equations and get, um, the values, uh, for- for all the different states.",
    "start": "4964080",
    "end": "4969530"
  },
  {
    "text": "And if you are at- if- if you're given some policy V, that policy kind of implicitly defines- uh,",
    "start": "4969530",
    "end": "4977860"
  },
  {
    "text": "uh, I think I said it wrong. If you're given a value function V, that value function V implicitly defines a policy.",
    "start": "4977860",
    "end": "4985750"
  },
  {
    "text": "But the policy is to take action in a greedy way to maximize the po- the value of the next state.",
    "start": "4985750",
    "end": "4993520"
  },
  {
    "text": "So- um, so- so Pi of",
    "start": "4993520",
    "end": "4999460"
  },
  {
    "text": "S is equal to argmax of a,",
    "start": "4999460",
    "end": "5006070"
  },
  {
    "text": "expectation of V of",
    "start": "5006110",
    "end": "5014130"
  },
  {
    "text": "S prime where S prime comes from P_sa.",
    "start": "5014130",
    "end": "5019300"
  },
  {
    "text": "Choose an action that is going to maximize the expected value of the next state.",
    "start": "5020150",
    "end": "5026775"
  },
  {
    "text": "So value implicitly defines a policy, and a policy defines a value.",
    "start": "5026775",
    "end": "5034590"
  },
  {
    "text": "However, the subtlety is that this cycle is not- is- is- is,",
    "start": "5034590",
    "end": "5040574"
  },
  {
    "text": "uh- is not the inverse of the other direction, which means a policy will- will give you a particular value function.",
    "start": "5040575",
    "end": "5047490"
  },
  {
    "text": "But then if you take this value function and try to calculate the policy that- that, um, um, acts greedily according to this value function,",
    "start": "5047490",
    "end": "5055665"
  },
  {
    "text": "you will not get the same policy again. And that asymmetry is what is captured in policy iteration.",
    "start": "5055665",
    "end": "5062835"
  },
  {
    "text": "So policy iteration is an algorithm where we start with a random policy,",
    "start": "5062835",
    "end": "5072205"
  },
  {
    "text": "evaluate the corresponding value function, and take that evaluati- uh, value function,",
    "start": "5072205",
    "end": "5077375"
  },
  {
    "text": "and re-estimate the best possible policy you can come off, and then re- re- re-evaluate, uh,",
    "start": "5077375",
    "end": "5083685"
  },
  {
    "text": "the new policy- the- the new value function according to the policy, and go on and on until you converge. So that's policy- policy, uh, uh iteration.",
    "start": "5083685",
    "end": "5092025"
  },
  {
    "text": "Until your policy converges? Until your policy converges. And then there is this other- other, uh,",
    "start": "5092025",
    "end": "5099600"
  },
  {
    "text": "algorithm called value iteration. In value iteration, we have this- this,",
    "start": "5099600",
    "end": "5111705"
  },
  {
    "text": "uh, thing called the- the, uh, Bellman equation.",
    "start": "5111705",
    "end": "5115840"
  },
  {
    "text": "And the um- and the goal in- in, um,",
    "start": "5119660",
    "end": "5125100"
  },
  {
    "text": "in- in the value iteration is to estimate this function called V*,",
    "start": "5125100",
    "end": "5132015"
  },
  {
    "text": "which is called the optimal value function. And V*of S is basically, you know,",
    "start": "5132015",
    "end": "5138900"
  },
  {
    "text": "argmax of Pi of V of S. So basically,",
    "start": "5138900",
    "end": "5146260"
  },
  {
    "text": "uh- now, what is the best possible value that you can",
    "start": "5149030",
    "end": "5154980"
  },
  {
    "text": "have for a given state if you were to scan across all possible policies.",
    "start": "5154980",
    "end": "5160395"
  },
  {
    "text": "So that's- that's, uh, the optimal value function. And this optimal value function,",
    "start": "5160395",
    "end": "5167985"
  },
  {
    "text": "if we were to plug  it here, the policy that you're gonna recover is the optimal policy. Yes, question.",
    "start": "5167985",
    "end": "5175320"
  },
  {
    "text": "Shouldn't that be just a max because you're on such a policy? Yeah, you're right. This should- this should just be- yeah, you're right.",
    "start": "5175320",
    "end": "5182165"
  },
  {
    "text": "Thank you. So this just should be a max, right? So the- the, uh, idea with, uh- uh, with- with value iteration is that, uh,",
    "start": "5182165",
    "end": "5190190"
  },
  {
    "text": "value iteration has- you perform this update rule where V of S,",
    "start": "5190190",
    "end": "5196875"
  },
  {
    "text": "you set it equal to V of S plus, um- I'm sorry,",
    "start": "5196875",
    "end": "5206593"
  },
  {
    "text": "R of S plus, um, Gamma times max",
    "start": "5206593",
    "end": "5218610"
  },
  {
    "text": "a P_sa",
    "start": "5218610",
    "end": "5227909"
  },
  {
    "text": "of V S prime. [inaudible] Uh, I- Yes, you're right.",
    "start": "5227910",
    "end": "5238750"
  },
  {
    "text": "Thank you. All right, so this- this is um, this is also called the, uh, uh,",
    "start": "5241010",
    "end": "5247155"
  },
  {
    "text": "the Bellman backup operator where if you were to perform this over and over, you will recover V*.",
    "start": "5247155",
    "end": "5253200"
  },
  {
    "text": "And the intuition to have here is if this is the space of all value functions,",
    "start": "5253200",
    "end": "5260325"
  },
  {
    "text": "so this is a V of S_1 through V of S^S.",
    "start": "5260325",
    "end": "5268725"
  },
  {
    "text": "All the possible value function that is this optimal V*. And no matter where you start.",
    "start": "5268725",
    "end": "5277330"
  },
  {
    "text": "So you can call this as uh, thi- this operator as, you know, V is equal to Bellman operator of V. So take any two value uh,",
    "start": "5277580",
    "end": "5289935"
  },
  {
    "text": "any two value functions. Run both of them through the Bellman operator.",
    "start": "5289935",
    "end": "5296020"
  },
  {
    "text": "Now the distance between these two will always be smaller than the distance between the original two.",
    "start": "5298010",
    "end": "5307140"
  },
  {
    "text": "This distance will always be smaller. Which means the Bellman backup operator,",
    "start": "5307140",
    "end": "5312359"
  },
  {
    "text": "uh, or the Bellman operator is a contraction mapping. So the space is getting, um, um,",
    "start": "5312359",
    "end": "5318030"
  },
  {
    "text": "if- if you were to ru- run your examples over and over, uh, with these Bellman, uh,",
    "start": "5318030",
    "end": "5323114"
  },
  {
    "text": "operators, then they are kind of converging. And whenever you have a contraction mapping that",
    "start": "5323115",
    "end": "5328305"
  },
  {
    "text": "exists this point called the fixed point. And this fixed point is the- the optimal value function.",
    "start": "5328305",
    "end": "5339150"
  },
  {
    "text": "So the, uh, um, the value iteration takes your value function from any spa- from any point in the space,",
    "start": "5339150",
    "end": "5347715"
  },
  {
    "text": "you keep applying the Bellman operator, you're gonna eventually converge to the fixed point. So that's- that's, uh, uh,",
    "start": "5347715",
    "end": "5354660"
  },
  {
    "text": "value iteration running this over and over. And we- we also saw this other- um,",
    "start": "5354660",
    "end": "5364510"
  },
  {
    "text": "we also saw, uh, a variant of this called fitted value iteration.",
    "start": "5364850",
    "end": "5370150"
  },
  {
    "text": "Right? So fitted value iteration, where in fitted value iteration, we, we limit ourselves.",
    "start": "5374480",
    "end": "5383460"
  },
  {
    "text": "So supposing this is V-star, we limit ourselves to a class of value functions, right?",
    "start": "5383460",
    "end": "5394320"
  },
  {
    "text": "In this class of value functions, let's say thi- this could be the class that is like the output of a neural network or a linear model,",
    "start": "5394320",
    "end": "5401325"
  },
  {
    "text": "you know, any, any, any, any- uh, some class of value functions where we start with some point, you know,",
    "start": "5401325",
    "end": "5408840"
  },
  {
    "text": "some Theta that, that parameterizes this, uh, value function, and on this, we apply the Bellman backup operator.",
    "start": "5408840",
    "end": "5416010"
  },
  {
    "text": "The Bellman backup operator will take us to some new value function which may be outside our parameter space, all right?",
    "start": "5416010",
    "end": "5423660"
  },
  {
    "text": "And then we project this back onto- so this, this Bellman operator take- operator takes us here,",
    "start": "5423660",
    "end": "5430679"
  },
  {
    "text": "and then we project it back onto our class by, by trying to fit this, this, uh,",
    "start": "5430680",
    "end": "5437340"
  },
  {
    "text": "value function from a function in our class, say, you know, uh, say, uh, by minimizing the least square or,",
    "start": "5437340",
    "end": "5443744"
  },
  {
    "text": "or some such loss function, right? So that's like projecting this new function back into the class. And from here, again, apply the Bellman, uh,",
    "start": "5443745",
    "end": "5450570"
  },
  {
    "text": "operator, you get a new function, project it back, right? Again, apply the Bellman operator,",
    "start": "5450570",
    "end": "5457060"
  },
  {
    "text": "and then project it back, right? And this is called the fitted value iteration where you",
    "start": "5458750",
    "end": "5464760"
  },
  {
    "text": "are staying within this parameter- uh, this parameterized family of, of functions, right?",
    "start": "5464760",
    "end": "5471630"
  },
  {
    "text": "And the iteration takes you closer and closer to, to V-star, right? But, uh, this algorithm is not guaranteed to converge,",
    "start": "5471630",
    "end": "5479430"
  },
  {
    "text": "right, because this is not a true contraction mapping anymore, right? There's no, there's no, um, uh, there's no fixed point for this.",
    "start": "5479430",
    "end": "5485744"
  },
  {
    "text": "Um, and what- however, in practice, it, it tends to work well.",
    "start": "5485745",
    "end": "5491190"
  },
  {
    "text": "So that's- that's, uh, um, fitted value iteration. We, um, in, in your homework,",
    "start": "5491190",
    "end": "5498330"
  },
  {
    "text": "you implemented, uh, value iteration for the inverted pendulum problem, okay? We did not do fitted value iteration in the homework, you implemented this,",
    "start": "5498330",
    "end": "5506594"
  },
  {
    "text": "and you also implemented this in the context where PSA was not known, right?",
    "start": "5506595",
    "end": "5512700"
  },
  {
    "text": "So in the, in the homework, by, by running your, um, uh, simulator,",
    "start": "5512700",
    "end": "5517860"
  },
  {
    "text": "we also learned the transition probabilities as a separate problem, and using the- the, uh,",
    "start": "5517860",
    "end": "5523335"
  },
  {
    "text": "estimated transition probabilities, we learned the, uh, uh, val- we perform the value iteration. Yes, question?",
    "start": "5523335",
    "end": "5529230"
  },
  {
    "text": "What would fitted value iteration look like in code? Like what it- what kind of, uh, what kind [inaudible] [OVERLAPPING]",
    "start": "5529230",
    "end": "5535550"
  },
  {
    "text": "Yeah, so how would fitted value iteration look in code? So in co- in, in, in the homework, you represented value as",
    "start": "5535550",
    "end": "5542140"
  },
  {
    "text": "some vector whose length was the number of states you had, right?",
    "start": "5542140",
    "end": "5548130"
  },
  {
    "text": "In- um, which meant you could set each element of this value function freely to any value, right?",
    "start": "5548130",
    "end": "5557864"
  },
  {
    "text": "You could, you could, you could set this and not worry about what value was here, right? In fitted value iteration,",
    "start": "5557865",
    "end": "5565185"
  },
  {
    "text": "you are no longer representing your value function explicitly as an array. But instead, you will have V of s will be some, some, um,",
    "start": "5565185",
    "end": "5576300"
  },
  {
    "text": "let's call it h_Theta of s. So s will be the input to the model, the output of the model will be the corresponding value, right?",
    "start": "5576300",
    "end": "5586980"
  },
  {
    "text": "So this is fitted value iteration. So now, inste- initially, I have a problem when we were doing,",
    "start": "5586980",
    "end": "5593370"
  },
  {
    "text": "[NOISE] when we were doing value iteration was- we were using the Bellman backup operator to get v star but now,",
    "start": "5593370",
    "end": "5599594"
  },
  {
    "text": "we are trying to fit some theta. Exactly. So initially, with, with, with, with Bellman backup operator,",
    "start": "5599595",
    "end": "5605400"
  },
  {
    "text": "we directly work with this array and apply the Bellman backup operator until this entire array converges, right?",
    "start": "5605400",
    "end": "5611895"
  },
  {
    "text": "With fitted value iteration, uh, we describe the algorithm, uh, in the class and it's also in the notes.",
    "start": "5611895",
    "end": "5617534"
  },
  {
    "text": "Um, we, we limit ourselves to a representation like this, it's no longer an explicit array, right?",
    "start": "5617535",
    "end": "5624330"
  },
  {
    "text": "And on this, we- [NOISE] we apply the Bellman backup operator, obtained something called y, right?",
    "start": "5624330",
    "end": "5631620"
  },
  {
    "text": "And then find a new Theta, new Theta, uh,",
    "start": "5631620",
    "end": "5636780"
  },
  {
    "text": "arg min of h_Theta of s minus y squared,",
    "start": "5636780",
    "end": "5645510"
  },
  {
    "text": "something like this, okay? So we are kind of projecting the y back to the Theta space.",
    "start": "5645510",
    "end": "5652574"
  },
  {
    "text": "So the assumption- so the log that you do that that's the, that's the, lets say, for example, a linear regression.",
    "start": "5652575",
    "end": "5659010"
  },
  {
    "text": "So it's- your value can be expressed as Theta transpose s- Yeah. -is the assumption.",
    "start": "5659010",
    "end": "5664335"
  },
  {
    "text": "Exactly. So this could be a linear model where you represent your value as Theta transpose s, yeah. So you kind of lose flexibility,",
    "start": "5664335",
    "end": "5671805"
  },
  {
    "text": "but you also gain generalization. So if you, if you do well on one state and another state is very similar,",
    "start": "5671805",
    "end": "5677025"
  },
  {
    "text": "then you kind of have a good sense of the value of the other state, just because two states are similar. Whereas with an explicit array representation,",
    "start": "5677025",
    "end": "5683940"
  },
  {
    "text": "there's no hope of generalization, whatsoever. Can you, can you explain, can you explain the on policy versus off policy?",
    "start": "5683940",
    "end": "5693150"
  },
  {
    "text": "Uh, so on policy off policy is not relevant for the, for the, um, uh, review. So yeah, we still have a few more topics,",
    "start": "5693150",
    "end": "5699060"
  },
  {
    "text": "maybe I can talk about on policy of policy later, right? So, so after reinforcement learning,",
    "start": "5699060",
    "end": "5704760"
  },
  {
    "text": "we move on to unsupervised learning where in unsupervised learning, the- we are given a set of examples x's but there is no corresponding [NOISE] y.",
    "start": "5704760",
    "end": "5716730"
  },
  {
    "text": "And our, our task is to learn some interesting structure, okay?",
    "start": "5716730",
    "end": "5722175"
  },
  {
    "text": "And the structure we saw, uh, in- in the class, basically,",
    "start": "5722175",
    "end": "5727380"
  },
  {
    "text": "we, we look at two kinds of structures. The first one is, do the examples cluster in some way, right?",
    "start": "5727380",
    "end": "5734925"
  },
  {
    "text": "And the k-means was the first algorithm that we saw. And in k-means, the idea is you're given, you know,",
    "start": "5734925",
    "end": "5743220"
  },
  {
    "text": "some set of- some dataset,",
    "start": "5743220",
    "end": "5750359"
  },
  {
    "text": "you know, x_1 through xd, right?",
    "start": "5750359",
    "end": "5755639"
  },
  {
    "text": "And given this dataset, um, that's all what you are given, there are no labels.",
    "start": "5755640",
    "end": "5761085"
  },
  {
    "text": "You know, we don't have a y value that tells these- all these are class 1, class 2, class 3, that- we don't have any of that, right?",
    "start": "5761085",
    "end": "5768315"
  },
  {
    "text": "And our goal is to now, kind of, cluster them in some way, right?",
    "start": "5768315",
    "end": "5774420"
  },
  {
    "text": "And the- those were clustering algorithms. And there is this other kind of unsupervised learning,",
    "start": "5774420",
    "end": "5783610"
  },
  {
    "text": "um, where we are interested in finding subspaces, x_1 and xd, right?",
    "start": "5784160",
    "end": "5791470"
  },
  {
    "text": "If this is our, uh, set of examples, x's, there's no y's given to us, right?",
    "start": "5796790",
    "end": "5802275"
  },
  {
    "text": "Uh, in, in, in, in clustering, we are trying to group examples into different categories, right?",
    "start": "5802275",
    "end": "5808410"
  },
  {
    "text": "Whereas in subspace finding problems, we are trying to see if this high dimensional representation can be instead,",
    "start": "5808410",
    "end": "5815355"
  },
  {
    "text": "captured in a lower dimensional representation. So this data can be projected onto",
    "start": "5815355",
    "end": "5820995"
  },
  {
    "text": "a one-dimensional representation that pretty much captures all the variants, right?",
    "start": "5820995",
    "end": "5826725"
  },
  {
    "text": "And this was in PCA, [NOISE] right? So we had these, um, you know, uh, unsupervised learning,",
    "start": "5826725",
    "end": "5835140"
  },
  {
    "text": "we had clustering prob- problems, and subspace finding problems.",
    "start": "5835140",
    "end": "5842320"
  },
  {
    "text": "Okay? And in, in, in, in, uh, the two,",
    "start": "5843590",
    "end": "5849705"
  },
  {
    "text": "the two that, uh, we spoke about just now, you know, k-means and PCA, we call them non-probabilistic, non-probabilistic.",
    "start": "5849705",
    "end": "5861720"
  },
  {
    "text": "So k-means and PCA.",
    "start": "5861720",
    "end": "5867525"
  },
  {
    "text": "And we have probabilistic equivalence, right?",
    "start": "5867525",
    "end": "5875940"
  },
  {
    "text": "For clustering, we use the Gaussian mixture model, right? And for subspace finding,",
    "start": "5875940",
    "end": "5882180"
  },
  {
    "text": "we use factor analysis.",
    "start": "5882180",
    "end": "5884440"
  },
  {
    "text": "The- the rough intuition to have is think of clustering as the unsupervised version of classification,",
    "start": "5888590",
    "end": "5897150"
  },
  {
    "text": "and subspace finding as the unsupervised version of regression. Right? That's- that's- that's a lose- lose- lose analogy there.",
    "start": "5897150",
    "end": "5903989"
  },
  {
    "text": "Right? And we use- we also saw this algorithm called EM or",
    "start": "5903990",
    "end": "5909060"
  },
  {
    "text": "expectation-maximization that we applied to solve the probabilistic versions,",
    "start": "5909060",
    "end": "5914850"
  },
  {
    "text": "now both GMM and factor analysis using this algorithm called expectation-maximization.",
    "start": "5914850",
    "end": "5920145"
  },
  {
    "text": "Right? In expectation maximization, which is really an important uh, algorithm, especially if you want to get into machine learning research,",
    "start": "5920145",
    "end": "5928700"
  },
  {
    "text": "um, uh, you know, in- in, you know, some of the new hot topics like, you know, deep generative models, it's very important that you have a good understanding of",
    "start": "5928700",
    "end": "5936000"
  },
  {
    "text": "expectation-maximization and all its variants, um, because that's- that's really where things kind of start off.",
    "start": "5936000",
    "end": "5941445"
  },
  {
    "text": "Right? So in EM, we have parameters for a model.",
    "start": "5941445",
    "end": "5946695"
  },
  {
    "text": "So we- we start with a model, where here by model, we mean some kind of a probabilistic model, x,",
    "start": "5946695",
    "end": "5953505"
  },
  {
    "text": "z with some parameters Theta, which has parameters Theta,",
    "start": "5953505",
    "end": "5962385"
  },
  {
    "text": "and um, we call the observed components of our model as evidence.",
    "start": "5962385",
    "end": "5970050"
  },
  {
    "text": "So evidence is generally denoted as x.",
    "start": "5970050",
    "end": "5975750"
  },
  {
    "text": "And we- the other unobserved elements of the- of the- um, uh, of the model are called latent variables.",
    "start": "5975750",
    "end": "5984099"
  },
  {
    "text": "We call them z. Right? And in EM, what we do is this,",
    "start": "5986540",
    "end": "5993090"
  },
  {
    "text": "um, iterating, it- EM is an iterating algorithm where each iteration has two steps.",
    "start": "5993090",
    "end": "5999840"
  },
  {
    "text": "The E-step, we calculate the posterior, now Q of z^i is equal to P of z^i given x^i at the current value of Theta.",
    "start": "5999840",
    "end": "6012935"
  },
  {
    "text": "And in the M-step, right, we update Thetas to arg max",
    "start": "6012935",
    "end": "6022550"
  },
  {
    "text": "Theta of",
    "start": "6022550",
    "end": "6034638"
  },
  {
    "text": "expectation of z according to Q_i of log P of x,",
    "start": "6034639",
    "end": "6042920"
  },
  {
    "text": "z, Theta over Q_i of z.",
    "start": "6042920",
    "end": "6049655"
  },
  {
    "text": "Right? So here, the arg max Theta, the Theta that we are optimizing appears only here.",
    "start": "6049655",
    "end": "6056720"
  },
  {
    "text": "Okay? And- and by- by performing this- this over and over,",
    "start": "6056720",
    "end": "6063935"
  },
  {
    "text": "this term is also called the ELBO, right?",
    "start": "6063935",
    "end": "6069230"
  },
  {
    "text": "By performing this over and over, we saw that this algorithm will eventually converge. We saw a proof of convergence of the EM algorithm, um,",
    "start": "6069230",
    "end": "6077255"
  },
  {
    "text": "and- and uh, this- this- this algorithm will um, uh, eventually converge.",
    "start": "6077255",
    "end": "6082280"
  },
  {
    "text": "Right? So that was- that was uh, EM algorithm. The- we also saw another- another um,",
    "start": "6082280",
    "end": "6090380"
  },
  {
    "text": "um model called ICA, independent component analysis. We used it to- um, we use it to solve the uh,",
    "start": "6090380",
    "end": "6098960"
  },
  {
    "text": "a source separation problem in audio, right? You did that in your last homework where you're given some- some mixture of different signals,",
    "start": "6098960",
    "end": "6106535"
  },
  {
    "text": "you make a non-Gaussian assumption about the sources of those signals. And using those non-Gaussian assumption,",
    "start": "6106535",
    "end": "6113135"
  },
  {
    "text": "you are able to construct an unmixing matrix, and that unmixing matrix is calculated using maximum likelihood, right?",
    "start": "6113135",
    "end": "6120665"
  },
  {
    "text": "And, um, the- the unmixing matrix that you obtain will be able to separate your audio sources into distinct- um,",
    "start": "6120665",
    "end": "6128420"
  },
  {
    "text": "um, distinct original, um, audio components. Right? So we saw this in the homework,",
    "start": "6128420",
    "end": "6134615"
  },
  {
    "text": "and in- in- again in your homework- in the EM algorithm, we saw an extension of this for semi-supervised learning.",
    "start": "6134615",
    "end": "6142145"
  },
  {
    "text": "Right? So it's- it's- uh, I would say it's pretty important to kind of understand that step that lead from unsupervised to semi-supervised, how we went,",
    "start": "6142145",
    "end": "6150325"
  },
  {
    "text": "and try to get a real sense of, you know, what the- what the moving parts was in the proof for moving",
    "start": "6150325",
    "end": "6156520"
  },
  {
    "text": "from- from- from unsupervised uh, to semi-supervised.",
    "start": "6156520",
    "end": "6161660"
  },
  {
    "text": "And what else? That's- that's pretty much that we- that- uh,",
    "start": "6162010",
    "end": "6168620"
  },
  {
    "text": "that we covered in this course. Then we also- ah, um, in- in the last few lectures,",
    "start": "6168620",
    "end": "6174650"
  },
  {
    "text": "we saw the variational autoencoder. Again, in the variational autoencoder, we are- um,",
    "start": "6174650",
    "end": "6180950"
  },
  {
    "text": "the variational autoencoder is probably not- not- uh, not going to be in your exam but, you know, it's an important concept especially,",
    "start": "6180950",
    "end": "6187325"
  },
  {
    "text": "those who want to go into research, where you have the z, the latent variable, and x, the evidence.",
    "start": "6187325",
    "end": "6197640"
  },
  {
    "text": "In EM, you would construct a Q for each example in the posterior.",
    "start": "6197650",
    "end": "6204034"
  },
  {
    "text": "Whereas with the variational autoencoder, we construct an encoder network- neural network that",
    "start": "6204035",
    "end": "6211010"
  },
  {
    "text": "takes x as input and outputs z as the output. And given the latent variable to the evidence um,",
    "start": "6211010",
    "end": "6221485"
  },
  {
    "text": "in- in- in case of, for example, simple models, we call this the likelihood function, right?",
    "start": "6221485",
    "end": "6231320"
  },
  {
    "text": "But in- in a VA, we call this the decoder network. Right? And we train the- um,",
    "start": "6231320",
    "end": "6239540"
  },
  {
    "text": "um, so again, just like we saw in the- in the- in the fitted value iteration, right, in- in the original EM,",
    "start": "6239540",
    "end": "6245690"
  },
  {
    "text": "each example got its own Q function, right, for the- for the posterior, and each Q of each example could be set independently and freely.",
    "start": "6245690",
    "end": "6254780"
  },
  {
    "text": "Whereas with a variational autoencoder, we are trying to come up with this class. Now just, you know, have- have a similar picture in your mind.",
    "start": "6254780",
    "end": "6262040"
  },
  {
    "text": "You are trying to represent all the posteriors with some kind of a class. Right? And which is called amortized inference,",
    "start": "6262040",
    "end": "6267980"
  },
  {
    "text": "where instead of calculating a separate posterior for each, you construct this class where you feed x as the input and",
    "start": "6267980",
    "end": "6273829"
  },
  {
    "text": "your output will be the parameters of the corresponding Q distribution. Right? Um, that's- that's- uh,",
    "start": "6273830",
    "end": "6282739"
  },
  {
    "text": "and then in the last lecture, we covered evaluation metrics. Evaluation metrics are- are- we don't have time to review them in- um,",
    "start": "6282740",
    "end": "6290315"
  },
  {
    "text": "in today's lecture, but they are pretty- pretty straightforward, you can just look at the slides. So that completes ah, pretty much the- the uh, course review.",
    "start": "6290315",
    "end": "6299555"
  },
  {
    "text": "Um, we've covered a lot of material in this course. Another few- few kind of, ah, ah, ending remarks.",
    "start": "6299555",
    "end": "6306245"
  },
  {
    "text": "So with all the kind of, ah, techniques that you've learned here, right, you can use these techniques for lots of different purposes.",
    "start": "6306245",
    "end": "6314075"
  },
  {
    "text": "Right? Your interest might be in research, your interest might be more applied. A general, um, um,",
    "start": "6314075",
    "end": "6320930"
  },
  {
    "text": "kind of, um, um, ending note is to- you know, um, is to kind of uh, recognize the power that machine learning has.",
    "start": "6320930",
    "end": "6327995"
  },
  {
    "text": "You can build really powerful algorithms using machine learning. But hopefully, you know, also put some thought",
    "start": "6327995",
    "end": "6333710"
  },
  {
    "text": "into the problems that you're trying to solve with these a- algorithms. Um, machine learning is a great tool.",
    "start": "6333710",
    "end": "6340685"
  },
  {
    "text": "It gives you a lot of power, which means, you know, it can be used for good and it can be abused. Right? Always put some thought into- um,",
    "start": "6340685",
    "end": "6348440"
  },
  {
    "text": "before you jump into a problem and try to solve a problem using machine learning to think of what the possible side effects could be.",
    "start": "6348440",
    "end": "6355625"
  },
  {
    "text": "Right? So machine learning um, is- at the end of the day it is- it- it- it- it looks for correlation among examples.",
    "start": "6355625",
    "end": "6363170"
  },
  {
    "text": "It has no sense of causality. Right? And so the- the problems that- that are there with correlation, you know,",
    "start": "6363170",
    "end": "6370594"
  },
  {
    "text": "as opposed to causation, transfer over to machine learning in, you know, um, with- with- with the same redos and- and restrictions.",
    "start": "6370595",
    "end": "6378905"
  },
  {
    "text": "So, um, any bias that is there in your dataset will transfer over into your predictions.",
    "start": "6378905",
    "end": "6384380"
  },
  {
    "text": "By bias I- I don't mean bias in variance, but, you know, any other kinds of bias uh, and- and- and fairness issues that may be there",
    "start": "6384380",
    "end": "6391010"
  },
  {
    "text": "in- in- in the dataset that you've collected, which may be colle- which- uh, which may be, you know, not purely sampled.",
    "start": "6391010",
    "end": "6398690"
  },
  {
    "text": "All those issues will transfer over to the predictions that you make. Right? So always be skeptical about the way your data was collected,",
    "start": "6398690",
    "end": "6406295"
  },
  {
    "text": "and especially if you're gonna build some kind of a pipeline where actions are being taken depending on the predictions that you've made,",
    "start": "6406295",
    "end": "6414295"
  },
  {
    "text": "you need to be especially even more skeptical about your model. Right? At the same time, um,",
    "start": "6414295",
    "end": "6420115"
  },
  {
    "text": "the- you know, um, machine learning can be applied in lots of different fields. Uh, any field that collects data and needs predictions,",
    "start": "6420115",
    "end": "6428135"
  },
  {
    "text": "you can use machine learning there, which means the scope of application of machine learning is tremendous.",
    "start": "6428135",
    "end": "6433190"
  },
  {
    "text": "And probably ah, a- a good fraction of you might be interested in machine learning research,",
    "start": "6433190",
    "end": "6438590"
  },
  {
    "text": "but probably most of you are interested in applying machine learning to different- um, different areas.",
    "start": "6438590",
    "end": "6443885"
  },
  {
    "text": "So the hope is that, you know, the lessons that you've learned in this- in th- in this course, um,",
    "start": "6443885",
    "end": "6450740"
  },
  {
    "text": "especially the different set of tools that you learned as different models, and also the general principles like bias-variance analysis will help",
    "start": "6450740",
    "end": "6456830"
  },
  {
    "text": "you solve these problems in your respective fields, right? And for those of you who are interested in machine learning research,",
    "start": "6456830",
    "end": "6463435"
  },
  {
    "text": "right now machine learning research is super hot. There are lots of scope for- for doing, um,",
    "start": "6463435",
    "end": "6468600"
  },
  {
    "text": "you know, um, um, cutting edge, building edge uh, uh, research in machine learning.",
    "start": "6468600",
    "end": "6473829"
  },
  {
    "text": "Um, so good luck for those of you who are- who are um, um, interested in doing research,",
    "start": "6473830",
    "end": "6478909"
  },
  {
    "text": "happy to chat about research uh, offline. And to kind of uh,",
    "start": "6478910",
    "end": "6484385"
  },
  {
    "text": "wrap things up, uh, I hope you enjoyed this course. I hope you uh, enjoyed the material uh, and- that we've kind of covered.",
    "start": "6484385",
    "end": "6491640"
  },
  {
    "text": "I personally enjoyed teaching this course a lot. I myself learned a lot, by- you know, by just going through the process of teaching, I prob-, you know,",
    "start": "6491640",
    "end": "6499300"
  },
  {
    "text": "the half of machine learning what I know as of today, half of it probably I learned them in the last two months,",
    "start": "6499300",
    "end": "6504485"
  },
  {
    "text": "ah, in the process of preparing for the lectures. Um, and good luck on your finals.",
    "start": "6504485",
    "end": "6510875"
  },
  {
    "text": "Um, you know the final is- is designed to be hard, so, you know, study- study hard.",
    "start": "6510875",
    "end": "6516470"
  },
  {
    "text": "It's not long, but you know, it can be tricky. Uh, yeah, with that, ah, we'll- we'll-",
    "start": "6516470",
    "end": "6522470"
  },
  {
    "text": "we'll kind of end the last lecture. Uh, thanks everyone.",
    "start": "6522470",
    "end": "6526140"
  }
]