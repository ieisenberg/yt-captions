[
  {
    "start": "0",
    "end": "38000"
  },
  {
    "text": "So our next topic is boosting.",
    "start": "0",
    "end": "2240"
  },
  {
    "text": "And boosting is a relatively\nnew method as well.",
    "start": "2240",
    "end": "5910"
  },
  {
    "text": "And similar to bagging\nand random forest,",
    "start": "5910",
    "end": "8240"
  },
  {
    "text": "it builds it gives\nprediction models",
    "start": "8240",
    "end": "10370"
  },
  {
    "text": "that are averages over trees.",
    "start": "10370",
    "end": "13219"
  },
  {
    "text": "But there's a\nfundamental difference.",
    "start": "13220",
    "end": "15640"
  },
  {
    "text": "Random forest and bagging,\nthe trees that are averaged",
    "start": "15640",
    "end": "18980"
  },
  {
    "text": "are all equivalent and\nthe averaging is just",
    "start": "18980",
    "end": "23420"
  },
  {
    "text": "used to reduce variance.",
    "start": "23420",
    "end": "25430"
  },
  {
    "text": "With boosting, it's\na sequential method",
    "start": "25430",
    "end": "28010"
  },
  {
    "text": "and each of the trees\nthat's added into the mix",
    "start": "28010",
    "end": "31070"
  },
  {
    "text": "is added to improve\non the performance",
    "start": "31070",
    "end": "33545"
  },
  {
    "text": "of the previous\ncollection of trees.",
    "start": "33545",
    "end": "35609"
  },
  {
    "text": "So that's a\nfundamental difference.",
    "start": "35610",
    "end": "39360"
  },
  {
    "start": "38000",
    "end": "169000"
  },
  {
    "text": "We look at first boosting\nfor regression trees.",
    "start": "39360",
    "end": "42870"
  },
  {
    "text": "It's simpler to explain there.",
    "start": "42870",
    "end": "44760"
  },
  {
    "text": "And we just think of it\nas a sequential algorithm.",
    "start": "44760",
    "end": "47800"
  },
  {
    "text": "And the bottom line\nis with boosting,",
    "start": "47800",
    "end": "49800"
  },
  {
    "text": "what we do is we keep on\nfitting trees to the residuals",
    "start": "49800",
    "end": "53410"
  },
  {
    "text": "and so we improve the fit.",
    "start": "53410",
    "end": "54930"
  },
  {
    "text": "And so we can describe\nthat very easily here.",
    "start": "54930",
    "end": "59050"
  },
  {
    "text": "We start off and\nwe're trying to build",
    "start": "59050",
    "end": "61800"
  },
  {
    "text": "a function f of x, which is\ngoing to be an average of trees.",
    "start": "61800",
    "end": "66850"
  },
  {
    "text": "And this is evaluated\nat some point x,",
    "start": "66850",
    "end": "70770"
  },
  {
    "text": "so we think of it as a function.",
    "start": "70770",
    "end": "72509"
  },
  {
    "text": "It'll start off with 0 and\nthe residuals will just",
    "start": "72510",
    "end": "75210"
  },
  {
    "text": "be the data observations yi.",
    "start": "75210",
    "end": "78840"
  },
  {
    "text": "And what we're going\nto do is sequentially",
    "start": "78840",
    "end": "82710"
  },
  {
    "text": "going from b-- going\nfrom 1 up to capital b,",
    "start": "82710",
    "end": "85680"
  },
  {
    "text": "we'll just keep on going.",
    "start": "85680",
    "end": "87750"
  },
  {
    "text": "We're going to fit a\ntree with d splits,",
    "start": "87750",
    "end": "91690"
  },
  {
    "text": "in other words, d plus 1\nterminal nodes to the training",
    "start": "91690",
    "end": "94950"
  },
  {
    "text": "data x and r, where r\nis the current residual.",
    "start": "94950",
    "end": "98020"
  },
  {
    "text": "Initially, the residuals\nare just the observations.",
    "start": "98020",
    "end": "100810"
  },
  {
    "text": "So we build a relatively\nsmall tree to the residuals.",
    "start": "100810",
    "end": "104530"
  },
  {
    "text": "And then we're going to update\nthe function by adding that tree",
    "start": "104530",
    "end": "108130"
  },
  {
    "text": "into our current model.",
    "start": "108130",
    "end": "109770"
  },
  {
    "text": "So our current model\nstarts off at 0.",
    "start": "109770",
    "end": "112000"
  },
  {
    "text": "And now repeatedly, we're\ngoing to add in the tree",
    "start": "112000",
    "end": "114850"
  },
  {
    "text": "that we've just grown.",
    "start": "114850",
    "end": "116110"
  },
  {
    "text": "And when we add\nit in, we actually",
    "start": "116110",
    "end": "117700"
  },
  {
    "text": "shrink it down by\na factor lambda.",
    "start": "117700",
    "end": "120070"
  },
  {
    "text": "So there's these two components\ngrowing a tree to the residuals,",
    "start": "120070",
    "end": "123550"
  },
  {
    "text": "and then adding in some\nshrunken version of it",
    "start": "123550",
    "end": "126280"
  },
  {
    "text": "into your current model.",
    "start": "126280",
    "end": "127630"
  },
  {
    "text": "Now, lambda is\npretty small, right?",
    "start": "127630",
    "end": "128690"
  },
  {
    "text": "It's like we're going to\nsee about 0.01, for example,",
    "start": "128690",
    "end": "130897"
  },
  {
    "text": "as a value of lambda.",
    "start": "130898",
    "end": "131890"
  },
  {
    "text": "So really shrinking it down, OK?",
    "start": "131890",
    "end": "134485"
  },
  {
    "text": "And then, of course,\nyou update the residuals",
    "start": "134485",
    "end": "136840"
  },
  {
    "text": "because the\nresiduals will change",
    "start": "136840",
    "end": "138430"
  },
  {
    "text": "by a corresponding amount.",
    "start": "138430",
    "end": "140180"
  },
  {
    "text": "And so you keep on doing that.",
    "start": "140180",
    "end": "141609"
  },
  {
    "text": "Grow tree to the residuals,\nadd it into your function,",
    "start": "141610",
    "end": "144400"
  },
  {
    "text": "downweight the\nresiduals and continue.",
    "start": "144400",
    "end": "147970"
  },
  {
    "text": "And you can see at\nthe end of the day,",
    "start": "147970",
    "end": "150820"
  },
  {
    "text": "your boosted model\nhas this form.",
    "start": "150820",
    "end": "152900"
  },
  {
    "text": "It's a sum of\nshrunken trees, all",
    "start": "152900",
    "end": "157299"
  },
  {
    "text": "b of them grow into the data.",
    "start": "157300",
    "end": "159470"
  },
  {
    "text": "Now, these trees are not\nindependent of each other",
    "start": "159470",
    "end": "161778"
  },
  {
    "text": "like they were in random\nforest in boosting",
    "start": "161778",
    "end": "163570"
  },
  {
    "text": "because each tree was growing\nto the residuals left over",
    "start": "163570",
    "end": "166390"
  },
  {
    "text": "from the previous\ncollection of trees.",
    "start": "166390",
    "end": "170090"
  },
  {
    "start": "169000",
    "end": "236000"
  },
  {
    "text": "So what's the idea\nbehind this procedure?",
    "start": "170090",
    "end": "172050"
  },
  {
    "text": "Well, for a single tree, we can\nfit a large tree to the data",
    "start": "172050",
    "end": "176120"
  },
  {
    "text": "and fit the data hard with a\nlarge tree, we can overfit.",
    "start": "176120",
    "end": "179690"
  },
  {
    "text": "So in contrast, the\nidea of boosting",
    "start": "179690",
    "end": "181970"
  },
  {
    "text": "is to learn more slowly.",
    "start": "181970",
    "end": "183300"
  },
  {
    "text": "So we start with why,\nwe build a tree to why.",
    "start": "183300",
    "end": "187490"
  },
  {
    "text": "And it can sometimes\nbe a small tree.",
    "start": "187490",
    "end": "190010"
  },
  {
    "text": "But rather than\naccept the full tree,",
    "start": "190010",
    "end": "191689"
  },
  {
    "text": "we shrink it back by a lot.",
    "start": "191690",
    "end": "194540"
  },
  {
    "text": "For example, a factor of 0.01.",
    "start": "194540",
    "end": "196400"
  },
  {
    "text": "And then we take\nresiduals and repeat.",
    "start": "196400",
    "end": "198720"
  },
  {
    "text": "So the idea being that instead\nof-- to avoid overfitting,",
    "start": "198720",
    "end": "201170"
  },
  {
    "text": "we're going to fit very slowly\nand try to-- at each trades,",
    "start": "201170",
    "end": "205040"
  },
  {
    "text": "try to pick up a small piece of\nthe signal with the next tree.",
    "start": "205040",
    "end": "208519"
  },
  {
    "text": "So instead of trying\nto grab a lot of signal",
    "start": "208520",
    "end": "211430"
  },
  {
    "text": "over with a large\namount of fitting,",
    "start": "211430",
    "end": "212930"
  },
  {
    "text": "we're going to-- it fits\nvery slowly in small parts,",
    "start": "212930",
    "end": "216859"
  },
  {
    "text": "shrinking each time in order\nto approximate the signal",
    "start": "216860",
    "end": "222230"
  },
  {
    "text": "without overfitting.",
    "start": "222230",
    "end": "223550"
  },
  {
    "text": "And a nice consequence\nis we don't actually",
    "start": "223550",
    "end": "226760"
  },
  {
    "text": "have to grow very large trees\nas we did in random forests.",
    "start": "226760",
    "end": "230120"
  },
  {
    "text": "Quite often, smaller trees fit\nin this slow sequential manner",
    "start": "230120",
    "end": "233480"
  },
  {
    "text": "will be very effective.",
    "start": "233480",
    "end": "237040"
  },
  {
    "start": "236000",
    "end": "275000"
  },
  {
    "text": "Boosting also works\nfor classification",
    "start": "237040",
    "end": "239930"
  },
  {
    "text": "and it's similar in spirit,\nbut it's slightly more complex,",
    "start": "239930",
    "end": "243379"
  },
  {
    "text": "so we're not going\nto go in detail yet.",
    "start": "243380",
    "end": "246455"
  },
  {
    "text": "And in fact, we don't go into\nmore detail in the textbook.",
    "start": "246455",
    "end": "249650"
  },
  {
    "text": "But there's a detailed section\nin our other textbook Elements",
    "start": "249650",
    "end": "254709"
  },
  {
    "text": "of Statistical\nLearning in chapter 10,",
    "start": "254710",
    "end": "256778"
  },
  {
    "text": "and you can learn about\nhow boosting works",
    "start": "256779",
    "end": "260018"
  },
  {
    "text": "for classification.",
    "start": "260019",
    "end": "261690"
  },
  {
    "text": "Doesn't stop you using\nboosting for classification.",
    "start": "261690",
    "end": "264350"
  },
  {
    "text": "There's the R\npackage GBM, which we",
    "start": "264350",
    "end": "266200"
  },
  {
    "text": "use in the exercises\nand the examples",
    "start": "266200",
    "end": "268900"
  },
  {
    "text": "we'll use in the R\nsession, and that",
    "start": "268900",
    "end": "270729"
  },
  {
    "text": "handles a variety of regression\nand classification problems.",
    "start": "270730",
    "end": "274760"
  },
  {
    "text": "OK.",
    "start": "274760",
    "end": "275260"
  },
  {
    "start": "275000",
    "end": "342000"
  },
  {
    "text": "So let's see the results of\nboosting for the gene expression",
    "start": "275260",
    "end": "277960"
  },
  {
    "text": "data.",
    "start": "277960",
    "end": "280460"
  },
  {
    "text": "These are test errors.",
    "start": "280460",
    "end": "282350"
  },
  {
    "text": "The orange curve is\nboosting depth one.",
    "start": "282350",
    "end": "284870"
  },
  {
    "text": "So what that means is\nactually, each tree has",
    "start": "284870",
    "end": "287320"
  },
  {
    "text": "got a single split,\nsometimes called a stump.",
    "start": "287320",
    "end": "290140"
  },
  {
    "text": "So a very, very simple\ntree, which again,",
    "start": "290140",
    "end": "293590"
  },
  {
    "text": "might seem a little crazy,\nbut we're going to use 5,000",
    "start": "293590",
    "end": "296190"
  },
  {
    "text": "of them.",
    "start": "296190",
    "end": "296690"
  },
  {
    "text": "And when we're using in this\nsequential, slow fitting way,",
    "start": "296690",
    "end": "302080"
  },
  {
    "text": "it actually does quite well.",
    "start": "302080",
    "end": "303490"
  },
  {
    "text": "The error is about what?",
    "start": "303490",
    "end": "304569"
  },
  {
    "text": "About 7% or 8%.",
    "start": "304570",
    "end": "306280"
  },
  {
    "text": "Random forests--\nsorry, random forests",
    "start": "306280",
    "end": "309610"
  },
  {
    "text": "are a little higher here.",
    "start": "309610",
    "end": "310939"
  },
  {
    "text": "There were about 12% or 13%.",
    "start": "310940",
    "end": "315100"
  },
  {
    "text": "Using a depth two tree,\nwhich means two splits,",
    "start": "315100",
    "end": "319340"
  },
  {
    "text": "we do maybe a little worse\nthan the simple stump model.",
    "start": "319340",
    "end": "322970"
  },
  {
    "text": "So again, it's quite striking\nthat a very, very simple model",
    "start": "322970",
    "end": "325910"
  },
  {
    "text": "applied in a slow and sequential\nway, at the end of it,",
    "start": "325910",
    "end": "329780"
  },
  {
    "text": "we get an ensemble that\nactually predicts very well.",
    "start": "329780",
    "end": "332920"
  },
  {
    "text": "So it seems like the\ndepth of the tree",
    "start": "332920",
    "end": "334850"
  },
  {
    "text": "that you use in boosting\nbecomes a tuning parameter.",
    "start": "334850",
    "end": "338060"
  },
  {
    "text": "Exactly.",
    "start": "338060",
    "end": "338660"
  },
  {
    "text": "Well, thank you for the lead\nin to the next slide, which",
    "start": "338660",
    "end": "340993"
  },
  {
    "text": "is the tuning parameters.",
    "start": "340993",
    "end": "343980"
  },
  {
    "start": "342000",
    "end": "464000"
  },
  {
    "text": "So there's a bunch\nof tuning parameters",
    "start": "343980",
    "end": "345670"
  },
  {
    "text": "and Trevor just\nmentioned one, the depth.",
    "start": "345670",
    "end": "347378"
  },
  {
    "text": "Let me just put them all up.",
    "start": "347378",
    "end": "350569"
  },
  {
    "text": "Well, the third one\nhere I've written",
    "start": "350570",
    "end": "352540"
  },
  {
    "text": "is what Trevor just mentioned.",
    "start": "352540",
    "end": "353790"
  },
  {
    "text": "The number of splits of the\ntree is a tuning parameter.",
    "start": "353790",
    "end": "359670"
  },
  {
    "text": "The depth is called--\nsometimes called d is--",
    "start": "359670",
    "end": "362820"
  },
  {
    "text": "if d is 1, it's simply a stump,\nwhich we saw with successful",
    "start": "362820",
    "end": "365580"
  },
  {
    "text": "in the previous example.",
    "start": "365580",
    "end": "367470"
  },
  {
    "text": "And if we-- d is larger.",
    "start": "367470",
    "end": "369240"
  },
  {
    "text": "It allows the interaction\nbetween predictors.",
    "start": "369240",
    "end": "371229"
  },
  {
    "text": "So typically, one tries\na few values of d.",
    "start": "371230",
    "end": "373410"
  },
  {
    "text": "Maybe d equals 1, 2, 4 and 8.",
    "start": "373410",
    "end": "376310"
  },
  {
    "text": "That might be a typical\nexample, depending",
    "start": "376310",
    "end": "378060"
  },
  {
    "text": "on the size of your data set\nand the number of predictors.",
    "start": "378060",
    "end": "381419"
  },
  {
    "text": "So if d is 1, each\nlittle tree can only",
    "start": "381420",
    "end": "384150"
  },
  {
    "text": "involve a single variable.",
    "start": "384150",
    "end": "385560"
  },
  {
    "text": "So it's actually an additive\nfunction of single variables,",
    "start": "385560",
    "end": "388830"
  },
  {
    "text": "so there's no\ninteractions allowed.",
    "start": "388830",
    "end": "390490"
  },
  {
    "text": "And if d equals 2, it can\ninvolve at most 2 variables.",
    "start": "390490",
    "end": "393610"
  },
  {
    "text": "Right.",
    "start": "393610",
    "end": "394110"
  },
  {
    "text": "So pairwise interactions.",
    "start": "394110",
    "end": "396240"
  },
  {
    "text": "Interesting.",
    "start": "396240",
    "end": "397380"
  },
  {
    "text": "So that's one tuning parameter.",
    "start": "397380",
    "end": "402710"
  },
  {
    "text": "The number of trees is\nalso a tuning parameter.",
    "start": "402710",
    "end": "406340"
  },
  {
    "text": "Unlike in random forest\nwith a number of trees,",
    "start": "406340",
    "end": "409160"
  },
  {
    "text": "you just went far\nenough so that you--",
    "start": "409160",
    "end": "412250"
  },
  {
    "text": "until you stopped getting\nthe benefit of averaging.",
    "start": "412250",
    "end": "414780"
  },
  {
    "text": "Right.",
    "start": "414780",
    "end": "415280"
  },
  {
    "text": "I think it's still the case\nthat the number of trees",
    "start": "415280",
    "end": "417170"
  },
  {
    "text": "is not a hugely\nimportant parameter.",
    "start": "417170",
    "end": "418830"
  },
  {
    "text": "It's possible to overfit, but\nit takes a very large number",
    "start": "418830",
    "end": "421543"
  },
  {
    "text": "to typically start\nto cause overfitting,",
    "start": "421543",
    "end": "423210"
  },
  {
    "text": "and here we see we're out to\n5,000 and not much is really",
    "start": "423210",
    "end": "426007"
  },
  {
    "text": "happening yet in\nterms of overfitting.",
    "start": "426007",
    "end": "427590"
  },
  {
    "text": "I think it especially\ndepends on the problem.",
    "start": "427590",
    "end": "430550"
  },
  {
    "text": "In some problems, you'll see\nthe curves really going up",
    "start": "430550",
    "end": "433159"
  },
  {
    "text": "but with classification\nproblems,",
    "start": "433160",
    "end": "434960"
  },
  {
    "text": "it often just levels\nout like that.",
    "start": "434960",
    "end": "437210"
  },
  {
    "text": "So the other one is the\nshrinkage parameter.",
    "start": "437210",
    "end": "439710"
  },
  {
    "text": "Remember, every\ntime we grow a tree,",
    "start": "439710",
    "end": "441380"
  },
  {
    "text": "we don't accept the full tree.",
    "start": "441380",
    "end": "442650"
  },
  {
    "text": "Rather, we shrink it back\nby a quite small fraction",
    "start": "442650",
    "end": "446210"
  },
  {
    "text": "and typically, 0.01 or 0.001\nare the choices one uses.",
    "start": "446210",
    "end": "451460"
  },
  {
    "text": "So these three\ntuning parameters,",
    "start": "451460",
    "end": "455210"
  },
  {
    "text": "one can just try a\nfew values of each one",
    "start": "455210",
    "end": "458569"
  },
  {
    "text": "and look at the cross-validation\nerror over the grid",
    "start": "458570",
    "end": "461660"
  },
  {
    "text": "to choose good\nsets of parameters.",
    "start": "461660",
    "end": "465290"
  },
  {
    "start": "464000",
    "end": "527000"
  },
  {
    "text": "Here's a-- so I have a\ncouple more examples here",
    "start": "465290",
    "end": "467720"
  },
  {
    "text": "actually from our\nearlier data mining book.",
    "start": "467720",
    "end": "470520"
  },
  {
    "text": "This is the California Housing\ndata looking at housing prices.",
    "start": "470520",
    "end": "475759"
  },
  {
    "text": "And we have the test\nerror here as a function",
    "start": "475760",
    "end": "480790"
  },
  {
    "text": "of number of trees for a number\nof methods we've talked about.",
    "start": "480790",
    "end": "484630"
  },
  {
    "text": "Random forests.",
    "start": "484630",
    "end": "486040"
  },
  {
    "text": "M equals 2.",
    "start": "486040",
    "end": "487980"
  },
  {
    "text": "I have to remember how\nmany variables there are.",
    "start": "487980",
    "end": "489980"
  },
  {
    "text": "We'll have to look in chapter 10\nor 15 of our book to check that.",
    "start": "489980",
    "end": "495020"
  },
  {
    "text": "But using random forests\nwith only 2 variables allowed",
    "start": "495020",
    "end": "499240"
  },
  {
    "text": "at each split gives you\nan error of about 0.39.",
    "start": "499240",
    "end": "503810"
  },
  {
    "text": "Random forest with more splits\nimproves things up a bit.",
    "start": "503810",
    "end": "507980"
  },
  {
    "text": "GBM, gradient boosting machine.",
    "start": "507980",
    "end": "510690"
  },
  {
    "text": "This is the R package\nthat does boosting.",
    "start": "510690",
    "end": "514760"
  },
  {
    "text": "We'll see in the lab.",
    "start": "514760",
    "end": "516289"
  },
  {
    "text": "With depth 4 and 8, trees\nare doing somewhat better.",
    "start": "516289",
    "end": "521710"
  },
  {
    "text": "It looks like they're still\non their way down as well.",
    "start": "521710",
    "end": "523960"
  },
  {
    "text": "Exactly.",
    "start": "523960",
    "end": "524460"
  },
  {
    "text": "So maybe in this example, one\nshould have run even more trees.",
    "start": "524460",
    "end": "528610"
  },
  {
    "start": "527000",
    "end": "677000"
  },
  {
    "text": "Another example.",
    "start": "528610",
    "end": "530040"
  },
  {
    "text": "This is a spam data from our\nearlier book, chapter 15.",
    "start": "530040",
    "end": "533180"
  },
  {
    "text": "This is, again, a\ntwo class problem",
    "start": "533180",
    "end": "535270"
  },
  {
    "text": "with about 50 predictors.",
    "start": "535270",
    "end": "537730"
  },
  {
    "text": "What do we have here?",
    "start": "537730",
    "end": "539139"
  },
  {
    "text": "Well, let's first-- another case\nwhere a single tree is really",
    "start": "539140",
    "end": "542470"
  },
  {
    "text": "not very good.",
    "start": "542470",
    "end": "543939"
  },
  {
    "text": "We've even truncated\nthe scale here",
    "start": "543940",
    "end": "545620"
  },
  {
    "text": "because they're going\nsomewhere above 7% single tree.",
    "start": "545620",
    "end": "549430"
  },
  {
    "text": "Bagging.",
    "start": "549430",
    "end": "550720"
  },
  {
    "text": "As we bag, we level\noff at around 5.5%.",
    "start": "550720",
    "end": "555379"
  },
  {
    "text": "Random force, probably using the\ndefault of the square root of p",
    "start": "555380",
    "end": "559945"
  },
  {
    "text": "as the number of predictors\nto select at every node",
    "start": "559945",
    "end": "562070"
  },
  {
    "text": "has an error.",
    "start": "562070",
    "end": "562680"
  },
  {
    "text": "It reduces that by--",
    "start": "562680",
    "end": "564589"
  },
  {
    "text": "the scale is pretty\ncompressed 1/2%.",
    "start": "564590",
    "end": "567860"
  },
  {
    "text": "And then boosting five node\ntrees gets another maybe 1/2%.",
    "start": "567860",
    "end": "572079"
  },
  {
    "text": "So these two methods are\nquite good and they're--",
    "start": "572080",
    "end": "574398"
  },
  {
    "text": "I think we've seen\na lot of examples.",
    "start": "574398",
    "end": "575940"
  },
  {
    "text": "They're pretty comparable\nwhen you say in performance.",
    "start": "575940",
    "end": "578860"
  },
  {
    "text": "Those look like they might be\nquite two different curves,",
    "start": "578860",
    "end": "582130"
  },
  {
    "text": "but actually if you do the\nproper statistical test,",
    "start": "582130",
    "end": "584710"
  },
  {
    "text": "they are not\nsignificantly different.",
    "start": "584710",
    "end": "587530"
  },
  {
    "text": "OK.",
    "start": "587530",
    "end": "588340"
  },
  {
    "text": "Just actually last topic,\nthe importance of variables.",
    "start": "588340",
    "end": "592198"
  },
  {
    "text": "How do you measure the\nimportance of variables in trees",
    "start": "592198",
    "end": "594490"
  },
  {
    "text": "in bagging and boosting\nin random forests?",
    "start": "594490",
    "end": "597560"
  },
  {
    "text": "Well, there's no single\nparameter, no coefficient",
    "start": "597560",
    "end": "601630"
  },
  {
    "text": "with the standard\nerror you can refer",
    "start": "601630",
    "end": "603310"
  },
  {
    "text": "to because trees use variables\nin multiple places, right?",
    "start": "603310",
    "end": "606640"
  },
  {
    "text": "Every time you split, a variable\ncould participate in the split.",
    "start": "606640",
    "end": "610450"
  },
  {
    "text": "So what's done in bagging\nin random forest is we--",
    "start": "610450",
    "end": "615496"
  },
  {
    "text": "one records the\ntotal drop in RSS",
    "start": "615496",
    "end": "621310"
  },
  {
    "text": "for a given predictor over\nall splits in the tree.",
    "start": "621310",
    "end": "623960"
  },
  {
    "text": "So if we look at all\nthe possible splits,",
    "start": "623960",
    "end": "626135"
  },
  {
    "text": "we look to see whether\nthe variable was",
    "start": "626135",
    "end": "627760"
  },
  {
    "text": "involved in that split.",
    "start": "627760",
    "end": "628820"
  },
  {
    "text": "If it was, we measure how\nmuch it dropped the RSS,",
    "start": "628820",
    "end": "632410"
  },
  {
    "text": "and that's averaged\nover all the trees.",
    "start": "632410",
    "end": "634540"
  },
  {
    "text": "So the higher the better.",
    "start": "634540",
    "end": "637329"
  },
  {
    "text": "And a similar thing with Gini\nindex for classification trees.",
    "start": "637330",
    "end": "642750"
  },
  {
    "text": "So what you get is essentially\na qualitative ranking.",
    "start": "642750",
    "end": "646840"
  },
  {
    "text": "This is the variable\nimportance for the heart data,",
    "start": "646840",
    "end": "649270"
  },
  {
    "text": "and you can see the\nthallium stress test is",
    "start": "649270",
    "end": "652990"
  },
  {
    "text": "at a variable point of 100.",
    "start": "652990",
    "end": "654399"
  },
  {
    "text": "Things are usually normalized so\nthat the top variable has 100.",
    "start": "654400",
    "end": "657560"
  },
  {
    "text": "And you can see the other\nvariables are indicated here",
    "start": "657560",
    "end": "659860"
  },
  {
    "text": "in terms of their importance\nfrom calcium down to variables",
    "start": "659860",
    "end": "663880"
  },
  {
    "text": "with lower importance.",
    "start": "663880",
    "end": "665010"
  },
  {
    "text": "So these variable\nimportance plots",
    "start": "665010",
    "end": "666850"
  },
  {
    "text": "are quite an important part\nof the random forest toolkit.",
    "start": "666850",
    "end": "672199"
  },
  {
    "text": "Also used in boosting.",
    "start": "672200",
    "end": "673300"
  },
  {
    "text": "The same plot is\nused in boosting.",
    "start": "673300",
    "end": "676350"
  },
  {
    "text": "So to summarize, we've\ntalked about decision trees",
    "start": "676350",
    "end": "678690"
  },
  {
    "start": "677000",
    "end": "723000"
  },
  {
    "text": "and using ensembles of trees.",
    "start": "678690",
    "end": "680465"
  },
  {
    "start": "680465",
    "end": "683390"
  },
  {
    "text": "On the plus side, they're\nsimple and interpretable",
    "start": "683390",
    "end": "686010"
  },
  {
    "text": "when they're small.",
    "start": "686010",
    "end": "686850"
  },
  {
    "text": "But as we've seen in\nthe examples we've",
    "start": "686850",
    "end": "688592"
  },
  {
    "text": "seen in other examples,\nthey're often not",
    "start": "688592",
    "end": "690300"
  },
  {
    "text": "very competitive in terms\nof prediction error.",
    "start": "690300",
    "end": "692269"
  },
  {
    "text": "So some newer methods,\nbagging random forest",
    "start": "692270",
    "end": "695400"
  },
  {
    "text": "and boosting use trees in\ncombination as an ensemble",
    "start": "695400",
    "end": "700200"
  },
  {
    "text": "and in the process, they\ncan improve prediction error",
    "start": "700200",
    "end": "702480"
  },
  {
    "text": "quite considerably.",
    "start": "702480",
    "end": "704040"
  },
  {
    "text": "And the result is\nthat the last two",
    "start": "704040",
    "end": "706355"
  },
  {
    "text": "methods we talked about,\nrandom forest and boosting",
    "start": "706355",
    "end": "708480"
  },
  {
    "text": "are really among the state\nof the art techniques",
    "start": "708480",
    "end": "711540"
  },
  {
    "text": "for supervised learning.",
    "start": "711540",
    "end": "713290"
  },
  {
    "text": "So if you're interested\nin prediction and really",
    "start": "713290",
    "end": "715290"
  },
  {
    "text": "just prediction performance,\nthey're very good techniques.",
    "start": "715290",
    "end": "717839"
  },
  {
    "text": "For interpretation, as we've\nseen, they're more challenging.",
    "start": "717840",
    "end": "722300"
  },
  {
    "start": "722300",
    "end": "723000"
  }
]