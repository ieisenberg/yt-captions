[
  {
    "start": "0",
    "end": "18000"
  },
  {
    "start": "0",
    "end": "4747"
  },
  {
    "text": "SPEAKER: Welcome back, everyone.",
    "start": "4747",
    "end": "6080"
  },
  {
    "text": "This is part 5 in our series\non contextual representations.",
    "start": "6080",
    "end": "9639"
  },
  {
    "text": "We're going to focus on BERT.",
    "start": "9640",
    "end": "11350"
  },
  {
    "text": "BERT is the slightly\nolder sibling",
    "start": "11350",
    "end": "13540"
  },
  {
    "text": "to GPT but arguably just\nas important and famous.",
    "start": "13540",
    "end": "18790"
  },
  {
    "start": "18000",
    "end": "18000"
  },
  {
    "text": "Let's start with the core\nmodel structure for BERT.",
    "start": "18790",
    "end": "21670"
  },
  {
    "text": "This is mostly going\nto be combinations",
    "start": "21670",
    "end": "23980"
  },
  {
    "text": "of familiar elements at this\npoint, given that we've already",
    "start": "23980",
    "end": "27189"
  },
  {
    "text": "reviewed the transformer\narchitecture.",
    "start": "27190",
    "end": "29110"
  },
  {
    "text": "And BERT is essentially\njust an interesting use",
    "start": "29110",
    "end": "32049"
  },
  {
    "text": "of the transformer.",
    "start": "32049",
    "end": "34000"
  },
  {
    "text": "As usual to illustrate,\nI have our sequence,",
    "start": "34000",
    "end": "36340"
  },
  {
    "text": "the Rock rules at\nthe bottom here",
    "start": "36340",
    "end": "38680"
  },
  {
    "text": "but that sequence is\naugmented in a bunch",
    "start": "38680",
    "end": "41080"
  },
  {
    "text": "of BERT specific ways.",
    "start": "41080",
    "end": "42470"
  },
  {
    "text": "So all the way on the left\nhere we have a CLS token.",
    "start": "42470",
    "end": "44920"
  },
  {
    "text": "That's an important token\nfor the BERT architecture.",
    "start": "44920",
    "end": "47469"
  },
  {
    "text": "Every sequence begins\nwith the CLS token.",
    "start": "47470",
    "end": "50800"
  },
  {
    "text": "That has a positional encoding.",
    "start": "50800",
    "end": "52600"
  },
  {
    "text": "We also have a hierarchical\npositional encoding.",
    "start": "52600",
    "end": "55660"
  },
  {
    "text": "This is given by\nthe token sent A.",
    "start": "55660",
    "end": "57985"
  },
  {
    "text": "This won't be so interesting\nfor our illustration,",
    "start": "57985",
    "end": "61090"
  },
  {
    "text": "but as I mentioned before, for\nproblems like natural language",
    "start": "61090",
    "end": "64780"
  },
  {
    "text": "inference, we might\nhave a separate token",
    "start": "64780",
    "end": "66850"
  },
  {
    "text": "for the premise and a separate\none for the hypothesis",
    "start": "66850",
    "end": "70390"
  },
  {
    "text": "to help encode the fact that a\nword appearing in the premise",
    "start": "70390",
    "end": "74020"
  },
  {
    "text": "is a slightly different\noccurrence of that word",
    "start": "74020",
    "end": "76630"
  },
  {
    "text": "than when it appears\nin a hypothesis",
    "start": "76630",
    "end": "79009"
  },
  {
    "text": "and that generalizes to\nlots of different kinds",
    "start": "79010",
    "end": "82550"
  },
  {
    "text": "of hierarchical position\nfor different tasks",
    "start": "82550",
    "end": "85370"
  },
  {
    "text": "that we might pose.",
    "start": "85370",
    "end": "87290"
  },
  {
    "text": "But we have this very\nposition sensitive encoding",
    "start": "87290",
    "end": "90590"
  },
  {
    "text": "of our input sequence.",
    "start": "90590",
    "end": "92359"
  },
  {
    "text": "We look up the embedding\nrepresentations",
    "start": "92360",
    "end": "94820"
  },
  {
    "text": "for all those pieces\nas usual and then",
    "start": "94820",
    "end": "97520"
  },
  {
    "text": "we do an additive\ncombination of them",
    "start": "97520",
    "end": "99560"
  },
  {
    "text": "to get our first context\nsensitive encoding",
    "start": "99560",
    "end": "103460"
  },
  {
    "text": "of this input sequence in these\nvectors that are in green here.",
    "start": "103460",
    "end": "107150"
  },
  {
    "text": "And then just as\nwith GPT, we have",
    "start": "107150",
    "end": "109790"
  },
  {
    "text": "lots of transformer blocks,\npotentially dozens of them",
    "start": "109790",
    "end": "112880"
  },
  {
    "text": "repeated until we finally\nget to some output states,",
    "start": "112880",
    "end": "116479"
  },
  {
    "text": "which I've given\nin dark green here.",
    "start": "116480",
    "end": "118550"
  },
  {
    "text": "And those are going to be\nthe basis for further things",
    "start": "118550",
    "end": "121040"
  },
  {
    "text": "that we do with the model.",
    "start": "121040",
    "end": "123370"
  },
  {
    "text": "That's the structure.",
    "start": "123370",
    "end": "124630"
  },
  {
    "text": "Let's think about how\nwe train this artifact.",
    "start": "124630",
    "end": "127329"
  },
  {
    "start": "127000",
    "end": "127000"
  },
  {
    "text": "The core objective is masked\nlanguage modeling or MLM.",
    "start": "127330",
    "end": "132040"
  },
  {
    "text": "And the idea here is\nessentially that we're",
    "start": "132040",
    "end": "134530"
  },
  {
    "text": "going to mask out or obscure\nthe identities of some words",
    "start": "134530",
    "end": "138700"
  },
  {
    "text": "in the sequence and then have\nthe model try to reconstruct",
    "start": "138700",
    "end": "142300"
  },
  {
    "text": "the missing piece.",
    "start": "142300",
    "end": "143900"
  },
  {
    "text": "So for our sequence, we\ncould have a scenario where",
    "start": "143900",
    "end": "146170"
  },
  {
    "text": "we have no masking\non the word rules,",
    "start": "146170",
    "end": "148750"
  },
  {
    "text": "but we nonetheless train\nthe model to predict rules",
    "start": "148750",
    "end": "151810"
  },
  {
    "text": "at that time step.",
    "start": "151810",
    "end": "153010"
  },
  {
    "text": "That might be relatively easy\nas a reconstruction task.",
    "start": "153010",
    "end": "156519"
  },
  {
    "text": "Harder will be doing masking.",
    "start": "156520",
    "end": "158810"
  },
  {
    "text": "So in this case, we have\na special designated token",
    "start": "158810",
    "end": "161770"
  },
  {
    "text": "that we insert in the\nplace of the token rules,",
    "start": "161770",
    "end": "165100"
  },
  {
    "text": "and then we try to get\nthe model to a state",
    "start": "165100",
    "end": "168220"
  },
  {
    "text": "where it can reconstruct\nthat rules was the missing",
    "start": "168220",
    "end": "171010"
  },
  {
    "text": "piece using the full\nbidirectional context",
    "start": "171010",
    "end": "174129"
  },
  {
    "text": "around that point.",
    "start": "174130",
    "end": "175540"
  },
  {
    "text": "And then relatedly, in\naddition to masking,",
    "start": "175540",
    "end": "177879"
  },
  {
    "text": "we could do random\nword replacement.",
    "start": "177880",
    "end": "180250"
  },
  {
    "text": "So in this case, we simply\ntake the actual word,",
    "start": "180250",
    "end": "183080"
  },
  {
    "text": "in this case, rules\nand replace it",
    "start": "183080",
    "end": "184970"
  },
  {
    "text": "with a random one,\nlike every, and then",
    "start": "184970",
    "end": "187340"
  },
  {
    "text": "try to have the model\nlearn to predict",
    "start": "187340",
    "end": "189440"
  },
  {
    "text": "what was the actual\ntoken at that position.",
    "start": "189440",
    "end": "193100"
  },
  {
    "text": "And all of these things are\nusing the bidirectional context",
    "start": "193100",
    "end": "196400"
  },
  {
    "text": "of the model in order to do\nthis reconstruction task.",
    "start": "196400",
    "end": "200090"
  },
  {
    "text": "When we train this\nmodel, we mask out",
    "start": "200090",
    "end": "202250"
  },
  {
    "text": "only a small percentage\nof all the tokens,",
    "start": "202250",
    "end": "204830"
  },
  {
    "text": "mostly leaving the\nother ones in place,",
    "start": "204830",
    "end": "207390"
  },
  {
    "text": "so that the model\nhas lots of context",
    "start": "207390",
    "end": "209420"
  },
  {
    "text": "to use to predict the masked,\nor missing, or corrupted tokens.",
    "start": "209420",
    "end": "213830"
  },
  {
    "text": "That's actually a limitation\nof the model and inefficiency",
    "start": "213830",
    "end": "217070"
  },
  {
    "text": "in the MLM objective that\nElectra in particular",
    "start": "217070",
    "end": "220550"
  },
  {
    "text": "will seek to address.",
    "start": "220550",
    "end": "222950"
  },
  {
    "text": "Here is the MLM loss\nfunction in some detail.",
    "start": "222950",
    "end": "225980"
  },
  {
    "start": "223000",
    "end": "223000"
  },
  {
    "text": "Again, as before with\nthese loss functions,",
    "start": "225980",
    "end": "228440"
  },
  {
    "text": "there are a lot of details here,\nbut I think the crucial thing",
    "start": "228440",
    "end": "231410"
  },
  {
    "text": "to zoom in on is\nfirst the numerator.",
    "start": "231410",
    "end": "233960"
  },
  {
    "text": "It's very familiar from before.",
    "start": "233960",
    "end": "235700"
  },
  {
    "text": "We're going to use the embedding\nrepresentation of the token",
    "start": "235700",
    "end": "238489"
  },
  {
    "text": "that we want to predict.",
    "start": "238490",
    "end": "239810"
  },
  {
    "text": "And we're going to get a dot\nproduct of that with a model",
    "start": "239810",
    "end": "242630"
  },
  {
    "text": "representation.",
    "start": "242630",
    "end": "243590"
  },
  {
    "text": "In this case, we can use the\nentire surrounding context,",
    "start": "243590",
    "end": "247879"
  },
  {
    "text": "leaving out only the\nrepresentation at T,",
    "start": "247880",
    "end": "251000"
  },
  {
    "text": "whereas for the auto regressive\nobjective that we reviewed",
    "start": "251000",
    "end": "254240"
  },
  {
    "text": "before, we could only\nuse the preceding context",
    "start": "254240",
    "end": "257660"
  },
  {
    "text": "to make this prediction.",
    "start": "257660",
    "end": "259588"
  },
  {
    "text": "The other thing\nto notice here is",
    "start": "259589",
    "end": "261229"
  },
  {
    "text": "that we have this\nindicator function mt here,",
    "start": "261230",
    "end": "264420"
  },
  {
    "text": "which is going to be 1 if we're\nlooking at a mask token and 0",
    "start": "264420",
    "end": "268850"
  },
  {
    "text": "otherwise.",
    "start": "268850",
    "end": "269970"
  },
  {
    "text": "And so what that's\nessentially doing",
    "start": "269970",
    "end": "271580"
  },
  {
    "text": "is turning off this\nobjective for tokens",
    "start": "271580",
    "end": "274580"
  },
  {
    "text": "that we didn't mask out.",
    "start": "274580",
    "end": "275810"
  },
  {
    "text": "So we get a learning signal\nonly from the masked tokens",
    "start": "275810",
    "end": "279620"
  },
  {
    "text": "or the ones that\nwe have corrupted.",
    "start": "279620",
    "end": "281930"
  },
  {
    "text": "And that again feeds into an\ninefficiency of this objective",
    "start": "281930",
    "end": "286820"
  },
  {
    "text": "because we in effect do the\nwork of making predictions",
    "start": "286820",
    "end": "289700"
  },
  {
    "text": "for all the time steps\nbut get an error signal",
    "start": "289700",
    "end": "292430"
  },
  {
    "text": "for the loss function\nonly for the ones",
    "start": "292430",
    "end": "294919"
  },
  {
    "text": "that we have designated\nas masked in some sense.",
    "start": "294920",
    "end": "300020"
  },
  {
    "start": "299000",
    "end": "299000"
  },
  {
    "text": "For the BERT paper, they\nsupplemented the MLM objective",
    "start": "300020",
    "end": "303379"
  },
  {
    "text": "with a binary next\nsentence prediction task.",
    "start": "303380",
    "end": "306780"
  },
  {
    "text": "So in this case, we use\nour corpus resources",
    "start": "306780",
    "end": "309230"
  },
  {
    "text": "to create actual sentence\nsequences with all",
    "start": "309230",
    "end": "312110"
  },
  {
    "text": "of their special tokens\nin them and for sequences",
    "start": "312110",
    "end": "315229"
  },
  {
    "text": "that actually occurred in the\ncorpus, we label them IsNext.",
    "start": "315230",
    "end": "318530"
  },
  {
    "text": "And then for negative\ninstances, we",
    "start": "318530",
    "end": "320389"
  },
  {
    "text": "have randomly chosen sentences\nthat we pair up and label them",
    "start": "320390",
    "end": "324110"
  },
  {
    "text": "as NotNext.",
    "start": "324110",
    "end": "325370"
  },
  {
    "text": "The motivation for this\npart of the objective",
    "start": "325370",
    "end": "328010"
  },
  {
    "text": "is to help the model\nlearn some discourse level",
    "start": "328010",
    "end": "331220"
  },
  {
    "text": "information as part of learning\nhow to reconstruct sequences.",
    "start": "331220",
    "end": "334730"
  },
  {
    "text": "And I think that's a really\ninteresting intuition about how",
    "start": "334730",
    "end": "337940"
  },
  {
    "text": "we might bring in even\nricher notions of context",
    "start": "337940",
    "end": "340790"
  },
  {
    "text": "into the transformer\nrepresentations.",
    "start": "340790",
    "end": "344940"
  },
  {
    "start": "344000",
    "end": "344000"
  },
  {
    "text": "When we think about transfer\nlearning or fine tuning,",
    "start": "344940",
    "end": "348030"
  },
  {
    "text": "there are a few different\napproaches that we can take.",
    "start": "348030",
    "end": "350400"
  },
  {
    "text": "Here's a depiction of the\ntransformer architecture.",
    "start": "350400",
    "end": "353370"
  },
  {
    "text": "And the standard\nlightweight thing to do",
    "start": "353370",
    "end": "356520"
  },
  {
    "text": "is to build out task parameters\non top of the final output",
    "start": "356520",
    "end": "360660"
  },
  {
    "text": "representation\nabove the CLS token.",
    "start": "360660",
    "end": "363420"
  },
  {
    "text": "I think that works really well\nbecause the CLS token is used",
    "start": "363420",
    "end": "366510"
  },
  {
    "text": "as the first token in\nevery single sequence",
    "start": "366510",
    "end": "369600"
  },
  {
    "text": "that BERT processes.",
    "start": "369600",
    "end": "371310"
  },
  {
    "text": "And it's always in\nthat fixed position.",
    "start": "371310",
    "end": "373680"
  },
  {
    "text": "So it becomes a kind\nof constant element",
    "start": "373680",
    "end": "375900"
  },
  {
    "text": "that contains a\nlot of information",
    "start": "375900",
    "end": "377940"
  },
  {
    "text": "about the\ncorresponding sequence.",
    "start": "377940",
    "end": "380310"
  },
  {
    "text": "So the standard thing is to\nbuild like a few dense layers",
    "start": "380310",
    "end": "383250"
  },
  {
    "text": "on top of that and then maybe\ndo some classification learning",
    "start": "383250",
    "end": "387030"
  },
  {
    "text": "there.",
    "start": "387030",
    "end": "387790"
  },
  {
    "text": "But of course, as with GPT, we\nshouldn't feel limited by that",
    "start": "387790",
    "end": "391200"
  },
  {
    "text": "and a standard\nalternative to this",
    "start": "391200",
    "end": "393210"
  },
  {
    "text": "would be to pool together\nall of the output states",
    "start": "393210",
    "end": "396240"
  },
  {
    "text": "and then build the task\nparameters on top of that,",
    "start": "396240",
    "end": "399389"
  },
  {
    "text": "mean pooling or max pooling\nor whatever decision",
    "start": "399390",
    "end": "402480"
  },
  {
    "text": "you use to bring together\nall of the output states",
    "start": "402480",
    "end": "407020"
  },
  {
    "text": "to make predictions\nfor your task.",
    "start": "407020",
    "end": "408639"
  },
  {
    "text": "And that can be very\npowerful as well",
    "start": "408640",
    "end": "410890"
  },
  {
    "text": "because you bring in\nmuch more information",
    "start": "410890",
    "end": "412840"
  },
  {
    "text": "about the entire sequence.",
    "start": "412840",
    "end": "416000"
  },
  {
    "start": "415000",
    "end": "415000"
  },
  {
    "text": "I thought I would remind\nyou a little bit about how",
    "start": "416000",
    "end": "418250"
  },
  {
    "text": "tokenization works.",
    "start": "418250",
    "end": "419560"
  },
  {
    "text": "Remember that BERT has this\ntiny vocabulary and therefore",
    "start": "419560",
    "end": "423740"
  },
  {
    "text": "a tiny static embedding space.",
    "start": "423740",
    "end": "426630"
  },
  {
    "text": "And the reason it\ngets away with that",
    "start": "426630",
    "end": "428390"
  },
  {
    "text": "is because it does word\npiece tokenization, which",
    "start": "428390",
    "end": "431510"
  },
  {
    "text": "means that we have lots\nof these word pieces",
    "start": "431510",
    "end": "433700"
  },
  {
    "text": "indicated by these\ndouble hash marks here.",
    "start": "433700",
    "end": "436410"
  },
  {
    "text": "And that means that the model\nessentially never UNKs out",
    "start": "436410",
    "end": "439520"
  },
  {
    "text": "any of its input tokens\nbut rather breaks them down",
    "start": "439520",
    "end": "443000"
  },
  {
    "text": "into familiar pieces.",
    "start": "443000",
    "end": "444830"
  },
  {
    "text": "And then the intuition is that\nthe power of masked language",
    "start": "444830",
    "end": "449240"
  },
  {
    "text": "modeling in\nparticular will allow",
    "start": "449240",
    "end": "451280"
  },
  {
    "text": "us to learn internal\nrepresentations of things that",
    "start": "451280",
    "end": "454550"
  },
  {
    "text": "correspond even to words like\nencode, which got spread out",
    "start": "454550",
    "end": "458419"
  },
  {
    "text": "over multiple tokens.",
    "start": "458420",
    "end": "461660"
  },
  {
    "start": "461000",
    "end": "461000"
  },
  {
    "text": "Let's talk a little bit\nabout core model releases.",
    "start": "461660",
    "end": "465140"
  },
  {
    "text": "For the original\nBERT paper, I believe",
    "start": "465140",
    "end": "467810"
  },
  {
    "text": "they just did BERT-base\nand BERT-large",
    "start": "467810",
    "end": "469639"
  },
  {
    "text": "in cased and uncased variants.",
    "start": "469640",
    "end": "472130"
  },
  {
    "text": "I would recommend always using\nthe cased ones at this point.",
    "start": "472130",
    "end": "475310"
  },
  {
    "text": "Very happily, lots of teams,\nincluding the Google team,",
    "start": "475310",
    "end": "478590"
  },
  {
    "text": "have worked to develop\neven smaller ones.",
    "start": "478590",
    "end": "480660"
  },
  {
    "text": "So we have tiny, mini,\nsmall, and medium as well.",
    "start": "480660",
    "end": "484450"
  },
  {
    "text": "And this is really\nwelcome because it",
    "start": "484450",
    "end": "486120"
  },
  {
    "text": "means you can do a\nlot of development",
    "start": "486120",
    "end": "487800"
  },
  {
    "text": "on these tiny models and\nthen possibly scale up",
    "start": "487800",
    "end": "490740"
  },
  {
    "text": "to larger ones.",
    "start": "490740",
    "end": "491639"
  },
  {
    "text": "So for example, BERT-tiny\nhas just two layers,",
    "start": "491640",
    "end": "495120"
  },
  {
    "text": "that is two transformer\nblocks, relatively small model",
    "start": "495120",
    "end": "498660"
  },
  {
    "text": "dimensionality and relatively\nsmall expansion inside its feed",
    "start": "498660",
    "end": "502290"
  },
  {
    "text": "forward layer for a total\nnumber of parameters",
    "start": "502290",
    "end": "505050"
  },
  {
    "text": "of only 4 million.",
    "start": "505050",
    "end": "506430"
  },
  {
    "text": "I will say that is tiny,\nbut it's surprising",
    "start": "506430",
    "end": "509490"
  },
  {
    "text": "how much juice you can get\nout of it when you fine tune",
    "start": "509490",
    "end": "511919"
  },
  {
    "text": "it for tasks.",
    "start": "511920",
    "end": "513240"
  },
  {
    "text": "But then you can move on\nup to mini, small, medium,",
    "start": "513240",
    "end": "516219"
  },
  {
    "text": "and then large is the largest\nfrom the original release",
    "start": "516220",
    "end": "519360"
  },
  {
    "text": "at 24 layers, relatively\nlarge model dimensionality,",
    "start": "519360",
    "end": "523289"
  },
  {
    "text": "relatively large\nfeed forward layer",
    "start": "523289",
    "end": "525450"
  },
  {
    "text": "for a total number of parameters\nof around 340 million.",
    "start": "525450",
    "end": "531370"
  },
  {
    "text": "All of these models because\nall of them, as far as I know,",
    "start": "531370",
    "end": "534550"
  },
  {
    "text": "use absolute\npropositional embeddings",
    "start": "534550",
    "end": "537760"
  },
  {
    "text": "have a maximum\nsequence length of 512.",
    "start": "537760",
    "end": "540850"
  },
  {
    "text": "And that's an\nimportant limitation",
    "start": "540850",
    "end": "542319"
  },
  {
    "text": "that increasingly\nwe're feeling is",
    "start": "542320",
    "end": "544300"
  },
  {
    "text": "constraining the\nkinds of work we",
    "start": "544300",
    "end": "545830"
  },
  {
    "text": "can do with models like BERT.",
    "start": "545830",
    "end": "549040"
  },
  {
    "text": "There are many new releases.",
    "start": "549040",
    "end": "550519"
  },
  {
    "text": "And I would say to\nstay up to date,",
    "start": "550520",
    "end": "552015"
  },
  {
    "text": "you could check\nout Hugging Face,",
    "start": "552015",
    "end": "553390"
  },
  {
    "text": "which has variants of these\nmodels for different languages",
    "start": "553390",
    "end": "556180"
  },
  {
    "text": "and maybe some different sizes\nand other kinds of things.",
    "start": "556180",
    "end": "559180"
  },
  {
    "text": "Maybe, for example,\nthere are by now versions",
    "start": "559180",
    "end": "561580"
  },
  {
    "text": "that use relative propositional\nencoding, which would",
    "start": "561580",
    "end": "563860"
  },
  {
    "text": "be quite welcome I would say.",
    "start": "563860",
    "end": "567399"
  },
  {
    "start": "567000",
    "end": "567000"
  },
  {
    "text": "For BERT, some\nknown limitations.",
    "start": "567400",
    "end": "569080"
  },
  {
    "text": "And this will feed\ninto subsequent things",
    "start": "569080",
    "end": "571120"
  },
  {
    "text": "that we want to talk about with\nRoBERTa and Electra especially.",
    "start": "571120",
    "end": "574779"
  },
  {
    "text": "First, the original BERT\npaper is admirably detailed,",
    "start": "574780",
    "end": "579250"
  },
  {
    "text": "but it's still very partial\nin terms of ablation studies",
    "start": "579250",
    "end": "583120"
  },
  {
    "text": "and studies of how to\neffectively optimize the model.",
    "start": "583120",
    "end": "586520"
  },
  {
    "text": "And so that means\nthat we might not",
    "start": "586520",
    "end": "588460"
  },
  {
    "text": "be looking at the very best\nBERT that we could possibly have",
    "start": "588460",
    "end": "591820"
  },
  {
    "text": "if we explored more widely.",
    "start": "591820",
    "end": "595010"
  },
  {
    "text": "Devlin et al. also\nobserve a downside.",
    "start": "595010",
    "end": "598070"
  },
  {
    "text": "They say, \"the first\ndownside is that we're",
    "start": "598070",
    "end": "599990"
  },
  {
    "text": "creating a mismatch between\npre-training and fine tuning",
    "start": "599990",
    "end": "603380"
  },
  {
    "text": "since the mask token is never\nseen during fine tuning.\"",
    "start": "603380",
    "end": "606740"
  },
  {
    "text": "That is indeed unusual.",
    "start": "606740",
    "end": "608450"
  },
  {
    "text": "Remember, the mask token\nis a crucial element",
    "start": "608450",
    "end": "611060"
  },
  {
    "text": "in training the model\nagainst the MLM objective.",
    "start": "611060",
    "end": "614150"
  },
  {
    "text": "You introduce this foreign\nelement into that phase",
    "start": "614150",
    "end": "617210"
  },
  {
    "text": "that presumably you never\nsee when you do fine tuning,",
    "start": "617210",
    "end": "620690"
  },
  {
    "text": "and that could be dragging\ndown model performance.",
    "start": "620690",
    "end": "624888"
  },
  {
    "text": "And the second downside\nthat they mentioned",
    "start": "624888",
    "end": "626680"
  },
  {
    "text": "is one that I mentioned as well.",
    "start": "626680",
    "end": "628460"
  },
  {
    "text": "We're using only around 15% of\nthe tokens to make predictions.",
    "start": "628460",
    "end": "633440"
  },
  {
    "text": "We do all this work of\nprocessing these sequences,",
    "start": "633440",
    "end": "635710"
  },
  {
    "text": "but then we turn off\nthe modeling objective",
    "start": "635710",
    "end": "638410"
  },
  {
    "text": "for the tokens that\nwe didn't mask.",
    "start": "638410",
    "end": "640240"
  },
  {
    "text": "And we can mask only\na tiny number of them",
    "start": "640240",
    "end": "642760"
  },
  {
    "text": "because we need the\nbidirectional context",
    "start": "642760",
    "end": "645070"
  },
  {
    "text": "to do the reconstruction.",
    "start": "645070",
    "end": "646390"
  },
  {
    "text": "That's the essence of\nthe intuition there.",
    "start": "646390",
    "end": "648970"
  },
  {
    "text": "So that's obviously inefficient.",
    "start": "648970",
    "end": "651579"
  },
  {
    "text": "And the final one is intriguing.",
    "start": "651580",
    "end": "653470"
  },
  {
    "text": "I'll mention this only at\nthe end of this series.",
    "start": "653470",
    "end": "655569"
  },
  {
    "text": "This comes from the\nXLNET paper and they just",
    "start": "655570",
    "end": "658510"
  },
  {
    "text": "observed that BERT assumes\nthe predicted tokens are",
    "start": "658510",
    "end": "661060"
  },
  {
    "text": "independent of each other given\nthe unmasked tokens, which",
    "start": "661060",
    "end": "664779"
  },
  {
    "text": "is oversimplified as high\norder, long range dependency",
    "start": "664780",
    "end": "668590"
  },
  {
    "text": "is prevalent in\nnatural language.",
    "start": "668590",
    "end": "670600"
  },
  {
    "text": "And this is just the\nobservation that if you",
    "start": "670600",
    "end": "672670"
  },
  {
    "text": "do happen to mask out two\ntokens like New and York",
    "start": "672670",
    "end": "676360"
  },
  {
    "text": "from the place named\nNew York, the model",
    "start": "676360",
    "end": "678670"
  },
  {
    "text": "will try to reconstruct those\ntwo tokens independently",
    "start": "678670",
    "end": "681639"
  },
  {
    "text": "of each other even though we\ncan see that they have a very",
    "start": "681640",
    "end": "684710"
  },
  {
    "text": "clear statistical dependency.",
    "start": "684710",
    "end": "686990"
  },
  {
    "text": "The BERT objective\nsimply misses that",
    "start": "686990",
    "end": "689300"
  },
  {
    "text": "and I'll mention\nlater on about how",
    "start": "689300",
    "end": "691459"
  },
  {
    "text": "XLNET brings that\ndependency back in possibly",
    "start": "691460",
    "end": "695240"
  },
  {
    "text": "to very powerful effect.",
    "start": "695240",
    "end": "697810"
  },
  {
    "start": "697810",
    "end": "702000"
  }
]