[
  {
    "text": " Welcome to the fifth iteration\nof our CS25 Transformers class.",
    "start": "0",
    "end": "9799"
  },
  {
    "text": "So Div and I started this\nclass a long time ago after seeing how transformers\nand machine learning",
    "start": "9800",
    "end": "16129"
  },
  {
    "text": "in general and AI became\nsuch a prevalent thing and how we predicted\nhow it would become an even bigger part of our\nlives going forward, which does",
    "start": "16129",
    "end": "23750"
  },
  {
    "text": "seem to be the case. So as large language models\nand AI in general takes over",
    "start": "23750",
    "end": "28760"
  },
  {
    "text": "the world, whether it's through\nthings like ChatGPT or image generation models like Sora,\nvideo generation models like",
    "start": "28760",
    "end": "35300"
  },
  {
    "text": "Sora and so forth, we felt that\nhaving a class where people are",
    "start": "35300",
    "end": "40700"
  },
  {
    "text": "able to come and learn about\ntransformers, how they work, and especially hear from leading\nexperts in industry and academia",
    "start": "40700",
    "end": "48380"
  },
  {
    "text": "working on state-of-the-art\nresearch in this area, how that would be\nvery beneficial to everybody's learning and help\nus progress further within AI",
    "start": "48380",
    "end": "59239"
  },
  {
    "text": "and technology in general. So yeah, so welcome\nto our class. So how our class works is\ntypically each week we invite",
    "start": "59240",
    "end": "67380"
  },
  {
    "text": "a leading researcher from either\nindustry or academia to come speak about some\nstate-of-the-art topic they're working on\nin transformers.",
    "start": "67380",
    "end": "74969"
  },
  {
    "text": "So we have an exciting\nlineup of speakers prepared for you guys this quarter. And so this first lecture\nwill be delivered by us",
    "start": "74970",
    "end": "82090"
  },
  {
    "text": "where we'll go through the\nbasics of transformers. And then we divided this\nlecture a bit differently",
    "start": "82090",
    "end": "88140"
  },
  {
    "text": "from the previous\nlectures in that we have a section on pre-training\nand data strategies,",
    "start": "88140",
    "end": "94690"
  },
  {
    "text": "and then a section focused\nmore on post-training, which has become a very\npopular topic these days.",
    "start": "94690",
    "end": "100090"
  },
  {
    "text": "We'll also touch briefly on some\napplications of transformers and some remaining\nweaknesses or challenges",
    "start": "100090",
    "end": "107100"
  },
  {
    "text": "that we should hopefully address\nto be able to further improve the state of AI and our\nmachine learning models.",
    "start": "107100",
    "end": "114450"
  },
  {
    "text": "So yeah. I forgot I have control. Yeah, so we'll start with\nsome instructor introductions.",
    "start": "114450",
    "end": "122000"
  },
  {
    "text": "So we have a good team\nof co-instructors. So my name is Steven.",
    "start": "122000",
    "end": "128929"
  },
  {
    "text": "I'm a current third\nyear CS PhD here. Previously did my undergrad\nat Waterloo in Canada.",
    "start": "128930",
    "end": "135460"
  },
  {
    "text": "I've done some research\nin industry as well at Amazon and NVIDIA. And in general, my research\nhovers around natural language",
    "start": "135460",
    "end": "143019"
  },
  {
    "text": "processing, so machine\nlearning for language and text looking at things\nlike, can we improve the controllability\nand reasoning models",
    "start": "143020",
    "end": "149709"
  },
  {
    "text": "of large language models? And more recently, cognitive\nscience and psychology",
    "start": "149710",
    "end": "155050"
  },
  {
    "text": "inspired work especially\nbridging the gap, the data gap and the learning\nefficiencies between machine",
    "start": "155050",
    "end": "160390"
  },
  {
    "text": "learning models and\nhow the humans learn, how human children learn,\nand how our brains are able to learn so efficiently.",
    "start": "160390",
    "end": "166659"
  },
  {
    "text": "Also done some work with\nmultimodal as well as computer vision, so things like diffusion\nmodels and image generation.",
    "start": "166660",
    "end": "172879"
  },
  {
    "text": "And just for fun, I also run\nthe piano club here with Karan. And we have an upcoming\nconcert on April 11",
    "start": "172880",
    "end": "178880"
  },
  {
    "text": "in case you guys are interested. ",
    "start": "178880",
    "end": "184480"
  },
  {
    "text": "Hi everyone. I'm Karan, a second year\nelectrical engineering PhD student. I did my undergrad\nat Cal Poly San Luis",
    "start": "184480",
    "end": "190690"
  },
  {
    "text": "Obispo after which I was\na research scientist here now doing my PhD. I'm a little bit more on the\nmedical imaging and computer",
    "start": "190690",
    "end": "197770"
  },
  {
    "text": "vision side. So a lot of my current\nwork is at the intersection of computer vision\nand neuroscience",
    "start": "197770",
    "end": "203650"
  },
  {
    "text": "working with things like\nfMRI and ultrasound. And I currently work at\nthe STAI lab, a new lab",
    "start": "203650",
    "end": "210670"
  },
  {
    "text": "under Dr. Ehsan Adeli. ",
    "start": "210670",
    "end": "215950"
  },
  {
    "text": "Hi, everyone, I'm Chelsea. I'm a first year master's\nstudent in symbolic systems.",
    "start": "215950",
    "end": "221980"
  },
  {
    "text": "And my general\nresearch interests are in multi-agentic frameworks,\nself-improving AI agents,",
    "start": "221980",
    "end": "227800"
  },
  {
    "text": "and overall, just kind of\nimproving the interpretability and explainability of models.",
    "start": "227800",
    "end": "234190"
  },
  {
    "text": "So previously, I studied\napplied math and neuroscience, and I did a bunch of\ninterdisciplinary research",
    "start": "234190",
    "end": "239870"
  },
  {
    "text": "in computer vision,\nrobotics, cognitive science, things of that.",
    "start": "239870",
    "end": "245000"
  },
  {
    "text": "And currently, I'm working\npart time at AVC firm. And over the summer,\nI'll be interning",
    "start": "245000",
    "end": "250610"
  },
  {
    "text": "at a conversational AI startup\nas a machine learning engineer. So I'm very interested in\nexploring the startup scene here",
    "start": "250610",
    "end": "257810"
  },
  {
    "text": "at Stanford, so feel\nfree to reach out. Hi, everyone. I'm Jenny.",
    "start": "257810",
    "end": "263190"
  },
  {
    "text": "I'm a current student\nmajoring in synthesis as well as a sociology co-term\nhere at Stanford. My background is primarily in\ntechnology, ethics, and policy.",
    "start": "263190",
    "end": "271267"
  },
  {
    "text": "So if you have any questions\nor want to talk about that, I'd love to have a conversation. In the past, I've worked\ndoing product at D.E. Shaw",
    "start": "271267",
    "end": "279080"
  },
  {
    "text": "and also research in the\ntech ethics and policy space. And this summer, I'll\nbe working at Daydream, which is an AI fashion\ntech startup in New York.",
    "start": "279080",
    "end": "287330"
  },
  {
    "text": "And yeah, so Div was\nunable to join us today, but he's working\non his new agent",
    "start": "287330",
    "end": "294030"
  },
  {
    "text": "startup called AGI Inc.\nCurrently on leave from a CS PhD here. He's passionate about robotics,\nAI agents, and so forth.",
    "start": "294030",
    "end": "301599"
  },
  {
    "text": "And later this\nquarter, he'll likely be giving a lecture actually on\neverything to do with AI agents.",
    "start": "301600",
    "end": "306610"
  },
  {
    "text": "So if you're interested in that,\ndefinitely look forward to it. And previously, he's worked at\nNVIDIA, Google, and so forth.",
    "start": "306610",
    "end": "312840"
  },
  {
    "text": "And he's the one who started\nthis class in the first place.",
    "start": "312840",
    "end": "318810"
  },
  {
    "text": "All right. So I'll go over some of\nthe course logistics. So first announcement is\nwe have a new website up",
    "start": "318810",
    "end": "325350"
  },
  {
    "text": "that's just cs25.stanford.edu. And so all of our updates\nand as well as the speaker",
    "start": "325350",
    "end": "330480"
  },
  {
    "text": "lineup will be posted\nthere in the coming weeks. That will also be the link\nto share our Zoom with people",
    "start": "330480",
    "end": "335819"
  },
  {
    "text": "who are not Stanford affiliated\nor are on the waitlist or have not been able to gain\nadmission into the class. So we encourage everyone\nto share this class",
    "start": "335820",
    "end": "343020"
  },
  {
    "text": "with their network and ensure\nthat anyone can access it from Zoom. Yeah, so some takeaways\nfrom the course",
    "start": "343020",
    "end": "349930"
  },
  {
    "text": "include a better\nunderstanding of transformers and the underlying architecture\nof many of our large language",
    "start": "349930",
    "end": "355390"
  },
  {
    "text": "models. Guest speakers,\nwhich will be talking about applications in language,\nvision, biology, robotics,",
    "start": "355390",
    "end": "362330"
  },
  {
    "text": "and more. Exposure to new\nresearch especially from leading researchers\nall across the country.",
    "start": "362330",
    "end": "368710"
  },
  {
    "text": "Innovative methods that are\ndriving the next generation of models as well as key\nlimitations, open problems,",
    "start": "368710",
    "end": "374660"
  },
  {
    "text": "and the future of AI. Sorry, I believe this is Steven",
    "start": "374660",
    "end": "382840"
  },
  {
    "text": "OK. Next, I'll give a really\nbrief intro about transformers and how attention works.",
    "start": "382840",
    "end": "389020"
  },
  {
    "text": "So the first step for\nlanguage is word embeddings. So words aren't numbers,\nso we obviously can't just",
    "start": "389020",
    "end": "396220"
  },
  {
    "text": "pass them into a model as is. So the first step\nis converting them into dense vectors in a\nhigh-dimensional space.",
    "start": "396220",
    "end": "402610"
  },
  {
    "text": "This is done through\nvarious methods, but the goal is to capture\nsemantic similarity.",
    "start": "402610",
    "end": "407650"
  },
  {
    "text": "Essentially, that\ncat and dog are more similar than\ncat and car even though the latter is more\nsimilar from a character",
    "start": "407650",
    "end": "413509"
  },
  {
    "text": "standpoint. Doing so enables\nvisualization, learning with transformer models.",
    "start": "413510",
    "end": "418940"
  },
  {
    "text": "Or arithmetic like\nI've shown like king minus man plus queen\nwould approximately be",
    "start": "418940",
    "end": "423949"
  },
  {
    "text": "queen in some embedding space. And classical methods for this\nare like Word2Vec, FastTex,",
    "start": "423950",
    "end": "430340"
  },
  {
    "text": "and many more these days. But static embeddings, for\ninstance, giving the word bank.",
    "start": "430340",
    "end": "437730"
  },
  {
    "text": "The same meaning and\njust bank as in riverbank have limitations. Therefore, the current standard\nis using contextual embeddings,",
    "start": "437730",
    "end": "445860"
  },
  {
    "text": "which take into account the\ncontext and the sentence that a word is in.",
    "start": "445860",
    "end": "452120"
  },
  {
    "text": "Self-attention can\nbe applied to this to learn what to focus\non for a given token.",
    "start": "452120",
    "end": "457530"
  },
  {
    "text": "So to do this, you learn\nthree matrices, a query, key,",
    "start": "457530",
    "end": "462620"
  },
  {
    "text": "and value, which together\ncomprise the attention process. A quick analogy\nfor this is imagine",
    "start": "462620",
    "end": "468500"
  },
  {
    "text": "you're in a library looking\nfor a book on a certain topic. This would be your query.",
    "start": "468500",
    "end": "473880"
  },
  {
    "text": "Now, let's say each book\nhas some summary associated with it, a key.",
    "start": "473880",
    "end": "479669"
  },
  {
    "text": "You can match your query and\nkey and get access to the book that you're looking for.",
    "start": "479670",
    "end": "485020"
  },
  {
    "text": "The information inside the\nbook would be your value. So in attention, we do a\nsoft match over the values",
    "start": "485020",
    "end": "492900"
  },
  {
    "text": "to get info from,\nsay, multiple books, and this comprises the\nattention operation.",
    "start": "492900",
    "end": "499860"
  },
  {
    "text": "And as you can see in\nthis visualization, when you apply this to\nlanguage, you can see that across different\nlayers of the model,",
    "start": "499860",
    "end": "507069"
  },
  {
    "text": "different words have\nconnections to the rest of the words in the sentence.",
    "start": "507070",
    "end": "514140"
  },
  {
    "text": "The next component is positional\nencodings or embeddings, which add order to the sequence.",
    "start": "514140",
    "end": "520240"
  },
  {
    "text": "Without these, the\nmodel, since you have just linear\nmultiplications here,",
    "start": "520240",
    "end": "525760"
  },
  {
    "text": "would not know what the first\nor the last word in the sentence is. Therefore, you add\nsome notion of order",
    "start": "525760",
    "end": "532110"
  },
  {
    "text": "through, say, sinusoids. Or in the simplest\nform, you could think that the first word\nwould be zero, the second one,",
    "start": "532110",
    "end": "537785"
  },
  {
    "text": "the one, and on. Beyond this is basically just\nscaling through multiple layers",
    "start": "537785",
    "end": "544230"
  },
  {
    "text": "and multi-head\nattention, more heads to attend to different\nparts of the sentence. And more parameters\nmeans that you",
    "start": "544230",
    "end": "550110"
  },
  {
    "text": "can capture more\ndiverse relationships from your sequences. And this gives you\nthe final transformer.",
    "start": "550110",
    "end": "556870"
  },
  {
    "text": " Transformers today\nhave overtaken",
    "start": "556870",
    "end": "562920"
  },
  {
    "text": "pretty much every field from\nLLMs like GPT-4o, o3, DeepSeek, to vision with models that are\ngetting increasingly better",
    "start": "562920",
    "end": "570360"
  },
  {
    "text": "at segmentation and whatnot. Speech, biology, video, you'll\nsee a lot of these applications",
    "start": "570360",
    "end": "577769"
  },
  {
    "text": "throughout the quarter. With large language models,\nthese are essentially just scaled up versions of\nattention and the transformer",
    "start": "577770",
    "end": "585250"
  },
  {
    "text": "architecture. You essentially just throw\na large amount of data, general text data derived\nfrom the web, at these models",
    "start": "585250",
    "end": "593029"
  },
  {
    "text": "and they can learn\nvery well to model through a next token\nprediction objective language.",
    "start": "593030",
    "end": "598870"
  },
  {
    "text": "And as you scale up, we've seen\nthat emergent abilities pop up. So while at a smaller\nscale, you might not",
    "start": "598870",
    "end": "604660"
  },
  {
    "text": "be able to do a certain task. Once you get to a\ncertain scale, you just have a peak in the\nability to do that task.",
    "start": "604660",
    "end": "612139"
  },
  {
    "text": "Some disadvantages,\nthough, that these models have very high computational\ncosts and therefore also",
    "start": "612140",
    "end": "617620"
  },
  {
    "text": "concerns with climate\nand the carbon emissions they may produce.",
    "start": "617620",
    "end": "623230"
  },
  {
    "text": "And like I was mentioning\nwith larger models, they're very good at\ngeneralizing to many abilities",
    "start": "623230",
    "end": "631690"
  },
  {
    "text": "or tasks, and\nthey're essentially plug-and-play with few\nor zero-shot learning. ",
    "start": "631690",
    "end": "640270"
  },
  {
    "text": "All right. So now I'll talk a bit\nmore about pretraining. So as Karan explained how\nthe transformer works.",
    "start": "640270",
    "end": "646319"
  },
  {
    "text": "But typically with a\nlanguage model, especially a large language\nmodel, you typically divide it into two stages,\npretraining stage where",
    "start": "646320",
    "end": "653390"
  },
  {
    "text": "you train the neural\nnetwork from scratch from randomized or\ninitialized weights, randomly initialized weights to give\nit more general capabilities.",
    "start": "653390",
    "end": "661700"
  },
  {
    "text": "And a big portion of\nthis is the data itself. So the data is the\nfundamental fuel",
    "start": "661700",
    "end": "667850"
  },
  {
    "text": "that allows your model to\nlearn, because that's what the model is learning from.",
    "start": "667850",
    "end": "673340"
  },
  {
    "text": "So your goal typically, again,\nlike I said with pretraining, is to train on a\nlarge amount of data to obtain some sort of\ngeneral level of capabilities",
    "start": "673340",
    "end": "681649"
  },
  {
    "text": "and overall knowledge\nor intelligence. And this is arguably, again,\nthe most important aspect",
    "start": "681650",
    "end": "686990"
  },
  {
    "text": "of training,\nespecially pretraining. Especially because\nLLMs learn, again,",
    "start": "686990",
    "end": "692089"
  },
  {
    "text": "based on statistical\ndistributions predicting the next token given\nprevious tokens.",
    "start": "692090",
    "end": "697340"
  },
  {
    "text": "So to effectively learn\nthis, you typically need a large amount of data. So because of its importance,\nhow do we maximally leverage it?",
    "start": "697340",
    "end": "705370"
  },
  {
    "text": "So again, smart data\nstrategies for pretraining",
    "start": "705370",
    "end": "710640"
  },
  {
    "text": "is definitely one of the most\nimportant topics these days. So I'll briefly touch\nupon two of the topic",
    "start": "710640",
    "end": "716430"
  },
  {
    "text": "projects I recently worked\non two different scales. The first is looking\nat what makes small,",
    "start": "716430",
    "end": "722490"
  },
  {
    "text": "child-like data sets potentially\neffective for language learning especially\non the smaller scale.",
    "start": "722490",
    "end": "727600"
  },
  {
    "text": "And the second is looking\nat smart data strategies for training large models on\nbillions or trillions of tokens,",
    "start": "727600",
    "end": "736540"
  },
  {
    "text": "which is on a much larger scale. So why are humans able\nto learn so efficiently?",
    "start": "736540",
    "end": "744060"
  },
  {
    "text": "This looks at how human\nchildren learn and interact with the environment\nand learn language",
    "start": "744060",
    "end": "749610"
  },
  {
    "text": "compared to a model\nlike ChatGPT, which is a bit analogous to how\nthe human brain learns language and learns in\ngeneral compared to something",
    "start": "749610",
    "end": "756720"
  },
  {
    "text": "like a neural network. So some potential\nkey differences are that humans\nlearn continuously.",
    "start": "756720",
    "end": "762790"
  },
  {
    "text": "We're continually learning. We don't just pretrain. We don't just sit\nin a chair, have someone read the\nwhole internet to us,",
    "start": "762790",
    "end": "768513"
  },
  {
    "text": "and then we just stop\nlearning from there. So that's unlike a lot of\ncurrent models, which are more",
    "start": "768513",
    "end": "774339"
  },
  {
    "text": "single pass pretraining models. Further, we have more\ngoal-based approaches",
    "start": "774340",
    "end": "780100"
  },
  {
    "text": "to learning and interaction\nwith the environment. That's a major reason we learn. Whereas, again, these models\nare typically just pretraining",
    "start": "780100",
    "end": "786399"
  },
  {
    "text": "on large amounts of data\nusing next token prediction or autoregression. Further, we learned through\ncontinuous multimodal",
    "start": "786400",
    "end": "792700"
  },
  {
    "text": "or multisensory data. So it's not just text only. We're subconsciously\nexposed to probably hundreds",
    "start": "792700",
    "end": "798010"
  },
  {
    "text": "of sensors that guide the\nway we learn and approach our daily lives.",
    "start": "798010",
    "end": "803740"
  },
  {
    "text": "Further, I believe our\nbrains are fundamentally different in that\nwe learn probably in more structured or\nhierarchical manners,",
    "start": "803740",
    "end": "810260"
  },
  {
    "text": "for example, through\ncompositionality rather than, again, simply\nnext token prediction. And the focus of this\nproject in particular",
    "start": "810260",
    "end": "816940"
  },
  {
    "text": "is more on the data differences. So again, humans are exposed to\ndialogue from people we talk to,",
    "start": "816940",
    "end": "823600"
  },
  {
    "text": "storybooks,\nespecially children-- compared to large amounts\nof data on the internet.",
    "start": "823600",
    "end": "832300"
  },
  {
    "text": "So this is a work that\nwas published-- so why? Why do you care about\nsmall models and training",
    "start": "832300",
    "end": "838060"
  },
  {
    "text": "on small amounts of data? Well, this will greatly\nimprove the efficiency of training and using\nlarge language models,",
    "start": "838060",
    "end": "843590"
  },
  {
    "text": "and this will open the door\nto potential new use cases, for example, models that\ncan run on your phone",
    "start": "843590",
    "end": "849298"
  },
  {
    "text": "that you can run\nlocally and so forth for many different use cases. Smaller models and\ntrain on less data",
    "start": "849298",
    "end": "854980"
  },
  {
    "text": "are also more interpretable\nand easier to control or align whether it's for safety\npurposes to reduce bias",
    "start": "854980",
    "end": "861190"
  },
  {
    "text": "and so forth to ensure people\nare using them for safe reasons and you have appropriate\nguardrails in place.",
    "start": "861190",
    "end": "868149"
  },
  {
    "text": "This will also enhance the open\nsource availability allowing research and the usage\nof these models for more",
    "start": "868150",
    "end": "873700"
  },
  {
    "text": "people around the world\nrather than simply companies with large amounts of compute. And in general, this might\neven allow us to more",
    "start": "873700",
    "end": "880640"
  },
  {
    "text": "greatly understand the\nother direction, which is how humans are able to learn\nso effectively and efficiently.",
    "start": "880640",
    "end": "886410"
  },
  {
    "text": " Yep, so this work is titled\nIs Child-Directed Speech.",
    "start": "886410",
    "end": "891830"
  },
  {
    "text": "Effective Training Data\nfor Language Models, which I presented at EMNLP\nin Miami last November.",
    "start": "891830",
    "end": "899120"
  },
  {
    "text": "So again, the hypothesis\nhere is that children, we probably learned\nfundamentally different",
    "start": "899120",
    "end": "904880"
  },
  {
    "text": "from LLMs. This is why we're able to\nlearn on several magnitudes less language data, in\nparticular than many",
    "start": "904880",
    "end": "910500"
  },
  {
    "text": "of these large language\nmodels these days, which require trillions of tokens. Now, there's several hypotheses.",
    "start": "910500",
    "end": "915960"
  },
  {
    "text": "One is the data we receive\nas humans is different fundamentally from LLMs.",
    "start": "915960",
    "end": "921650"
  },
  {
    "text": "Rather than just training\non internet data, we actually interact\nwith people. We talk to people. We hear stories that our\nparents or teachers tell us",
    "start": "921650",
    "end": "931380"
  },
  {
    "text": "and so forth. The other is maybe\nthe human brain just fundamentally learns different. So our learning\nalgorithm is just",
    "start": "931380",
    "end": "937435"
  },
  {
    "text": "different from large\nlanguage models. And another is maybe it's the\nway or the structure in which we",
    "start": "937435",
    "end": "943230"
  },
  {
    "text": "receive this data. So any data we receive is\nsomewhat curricularized. We start off with simple data,\nsimple language as a child",
    "start": "943230",
    "end": "951300"
  },
  {
    "text": "and then learn more\ncomplex grammars. We hear more complex\nspeech from our parents,",
    "start": "951300",
    "end": "957300"
  },
  {
    "text": "co-workers, and so forth. Anything we do whether\nit's learning math, we start simple and then move\non to more difficult problems.",
    "start": "957300",
    "end": "964660"
  },
  {
    "text": "Whereas language\nmodels, you typically don't care too much about\nordering or curriculum, so there's multiple\ndifferent hypotheses here.",
    "start": "964660",
    "end": "971550"
  },
  {
    "text": "So in order to test some\nof these, what we did is we trained some small\nGPT-2 and RoBERTa models on five different data sets.",
    "start": "971550",
    "end": "977940"
  },
  {
    "text": "One is CHILDES which is\na natural conversation data with children. So this is transcribed.",
    "start": "977940",
    "end": "984269"
  },
  {
    "text": "And then we collected\na synthetic version called TinyDialogues, which\nI'll discuss more later. BabyLM, which is\na diverse mixture",
    "start": "984270",
    "end": "990900"
  },
  {
    "text": "of different types of data. This includes Reddit data,\nWikipedia data, and so forth. So this is closer to your\ntypical large language model",
    "start": "990900",
    "end": "997980"
  },
  {
    "text": "pretraining data. And then we also\ndid a bit of testing with Wikipedia as\nwell OpenSubtitles,",
    "start": "997980",
    "end": "1003209"
  },
  {
    "text": "so movie and TV transcriptions. So we collected\nTinyDialogues, and this",
    "start": "1003210",
    "end": "1009139"
  },
  {
    "text": "was inspired by the fact\nthat a lot of, again, I said, our learning as children\nis through conversations",
    "start": "1009140",
    "end": "1015110"
  },
  {
    "text": "with other people. And conversations\nnaturally lead to learning. We talk to someone. They give us feedback.",
    "start": "1015110",
    "end": "1020820"
  },
  {
    "text": "We reflect on how the\nconversation went. So it's both pure\nand self-reflection. Furthermore, conversations\nlead to not only learning",
    "start": "1020820",
    "end": "1028490"
  },
  {
    "text": "of knowledge but other things\nlike ethics and morals, for example, parents or\nteachers telling us as children what's right or wrong to do.",
    "start": "1028490",
    "end": "1035970"
  },
  {
    "text": "And there's many different\ntypes of conversations you can have with many\ndifferent types of people leading to a lot of\ndiversity and learning.",
    "start": "1035970",
    "end": "1042389"
  },
  {
    "text": "So what we did is\nwe collected a fully grammatical and\ncurricularized conversation data set with a limited\nchild-like restricted",
    "start": "1042390",
    "end": "1049610"
  },
  {
    "text": "vocabulary using GPT-4, and we\ncollected different examples that differ by child age,\nthe different participants",
    "start": "1049610",
    "end": "1057460"
  },
  {
    "text": "in the conversation,\nand so forth. And here's just some\nexamples of some data points",
    "start": "1057460",
    "end": "1065350"
  },
  {
    "text": "in our collected data set. So you see, as the age goes up,\nthe utterances or conversations",
    "start": "1065350",
    "end": "1070660"
  },
  {
    "text": "become more complex. They become longer. The participants also\ndiffer by age appropriately.",
    "start": "1070660",
    "end": "1079240"
  },
  {
    "text": "So we also ran an experiment,\na curriculum experiment where we ordered either\nby ascending age order.",
    "start": "1079240",
    "end": "1086380"
  },
  {
    "text": "So the model will first see\ntwo-year-old conversations and then five-year-old\nconversations and then",
    "start": "1086380",
    "end": "1091750"
  },
  {
    "text": "10-year-old, and so forth,\nversus descending order. Maybe it's possible a\nlanguage model might actually",
    "start": "1091750",
    "end": "1097600"
  },
  {
    "text": "learn somehow better from\nmore complex examples first and then of course the typical\nbaseline of randomly shuffling",
    "start": "1097600",
    "end": "1103690"
  },
  {
    "text": "all your data examples. So we have some basic\nevaluation metrics targeted",
    "start": "1103690",
    "end": "1108970"
  },
  {
    "text": "at fundamental capabilities. One is basic grammatical\nand syntactic knowledge, and another is a\nfree word association",
    "start": "1108970",
    "end": "1116350"
  },
  {
    "text": "metric called Word\nSimilarity for assessing more semantic knowledge. So you see here from\nthe different data sets",
    "start": "1116350",
    "end": "1122800"
  },
  {
    "text": "that actually it seems like\ntraining on child-like data is worse than a\nheterogeneous mixture of just internet data like BabyLM.",
    "start": "1122800",
    "end": "1129280"
  },
  {
    "text": "So both metrics degrade quite\nsubstantially especially on CHILDES, the more\nnatural conversation",
    "start": "1129280",
    "end": "1136750"
  },
  {
    "text": "data set between children\nand their caregivers. And you'll see, in\nterms of curriculum,",
    "start": "1136750",
    "end": "1142900"
  },
  {
    "text": "we don't see many substantial\ndifferences no matter what order you provide the\nexamples into the model, which",
    "start": "1142900",
    "end": "1148779"
  },
  {
    "text": "is, again, surprising,\nbecause, as humans, we go from simple to more difficult.",
    "start": "1148780",
    "end": "1153880"
  },
  {
    "text": "So looking more closely at\nconvergence behavior or loss curves, you'll see\nhere that the training",
    "start": "1153880",
    "end": "1160240"
  },
  {
    "text": "loss, has these sorts of--",
    "start": "1160240",
    "end": "1168550"
  },
  {
    "text": "a cyclical, depending on the\nbuckets you use for curriculum. But the validation loss,\nwhich is what you really",
    "start": "1168550",
    "end": "1174230"
  },
  {
    "text": "care about so the\ngeneralization and learning, it has the exact same\ntrend no matter what order you feed the\nexamples in, which is, again,",
    "start": "1174230",
    "end": "1181950"
  },
  {
    "text": "a very interesting finding. So overall, we see that diverse\ndata sources like BabyLM seem to provide\na better learning",
    "start": "1181950",
    "end": "1188060"
  },
  {
    "text": "signal for language models than\npurely child-directed speech. And we do see, however,\nthat our TinyDialogues data",
    "start": "1188060",
    "end": "1194179"
  },
  {
    "text": "set noticeably outperforms the\nnatural conversation data set, likely because that data set\nis very noisy whereas ours",
    "start": "1194180",
    "end": "1200179"
  },
  {
    "text": "is, again, synthetically\ncollected by GPT-4. And again, global\ndevelopmental ordering",
    "start": "1200180",
    "end": "1205635"
  },
  {
    "text": "using curriculum learning\nseems to have negligible impact on performance. So overall, we can\nconclude that it's possible",
    "start": "1205635",
    "end": "1213049"
  },
  {
    "text": "that other aspects of\nchildren's learning, not simply the data\nthey're exposed to, are responsible for their\nefficient language learning,",
    "start": "1213050",
    "end": "1219660"
  },
  {
    "text": "for example, learning from\nother types of information like multimodal information, or\nit's the fact that our learning",
    "start": "1219660",
    "end": "1225500"
  },
  {
    "text": "algorithm in our brain is\njust fundamentally different and more data efficient than\nlanguage modeling techniques.",
    "start": "1225500",
    "end": "1232260"
  },
  {
    "text": "So if you wish to learn more, we\nhave our data set released on. Hugging Face as\nwell as on GitHub.",
    "start": "1232260",
    "end": "1239580"
  },
  {
    "text": "And the papers up\non arXiv as well. So now, let's go bigger scale.",
    "start": "1239580",
    "end": "1245770"
  },
  {
    "text": "So we were investigating\nsmall models trained on small amounts of\ndata similar to a human child.",
    "start": "1245770",
    "end": "1250990"
  },
  {
    "text": "Now what about current large\nmodels, billions of parameters trained on trillions of tokens? So during my last\nsummer internship,",
    "start": "1250990",
    "end": "1258370"
  },
  {
    "text": "I worked on a project with\nNVIDIA titled Maximize Your Data's Potential,\nEnhancing LLM Accuracy",
    "start": "1258370",
    "end": "1263850"
  },
  {
    "text": "with Two-Phase Pretraining. So this is the optimized\ndata selection as well as",
    "start": "1263850",
    "end": "1269910"
  },
  {
    "text": "training strategies in\nlarge scale pre-training. So a lot of works like Llama\nhighlighted the effectiveness",
    "start": "1269910",
    "end": "1275490"
  },
  {
    "text": "of different sorts\nof data mixtures, but they don't really shed\nlight into the exact mixtures",
    "start": "1275490",
    "end": "1281490"
  },
  {
    "text": "and how these\ndecisions were made. Whereas, we know data\nblending and ordering is crucial to effective\nLLM pretraining.",
    "start": "1281490",
    "end": "1288820"
  },
  {
    "text": "So can we shed\nmore light on this, which is what our work does? So firstly, we formalize\nand systematically",
    "start": "1288820",
    "end": "1294540"
  },
  {
    "text": "evaluate this concept of\ntwo-phase pre-training. And we show that,\nempirically, it",
    "start": "1294540",
    "end": "1299580"
  },
  {
    "text": "improves over continuous\ntraining, which is typically what's done with LLM\ntraining, and you just feed in all the data\nrather than separating it",
    "start": "1299580",
    "end": "1306180"
  },
  {
    "text": "into particular buckets\nor a different schedule. We also do a\nfine-grained analysis",
    "start": "1306180",
    "end": "1311850"
  },
  {
    "text": "of data blending for these\ntwo pretraining phases. And we have this notion of\nprototyping blends on smaller",
    "start": "1311850",
    "end": "1319890"
  },
  {
    "text": "token counts before scaling up. So this two-phase\npretraining approach is inspired kind of by how\npretraining and post-training",
    "start": "1319890",
    "end": "1327840"
  },
  {
    "text": "works, which is the first\nphase is on more general data. So this is to\nlearn more broadly.",
    "start": "1327840",
    "end": "1333780"
  },
  {
    "text": "So it's on more diverse data. And the second is\nto shift to more high-quality, domain-specific\ndata such as math and so forth.",
    "start": "1333780",
    "end": "1340660"
  },
  {
    "text": "However, it's important\nto balance between quality and diversity in both\nphases, as if you operate any data set too much,\nit can lead to overfitting.",
    "start": "1340660",
    "end": "1350770"
  },
  {
    "text": "So firstly, does two-phase\ntraining actually help? So we found that all\nour Phase 2 blends",
    "start": "1350770",
    "end": "1357370"
  },
  {
    "text": "or our two-phase\npretraining experiments outperformed the baseline of\nsimply just continuing training",
    "start": "1357370",
    "end": "1363910"
  },
  {
    "text": "on a single phase. And this is noticeably better\nthan just a randomized mixture of both phases, as well\nas the natural data",
    "start": "1363910",
    "end": "1370750"
  },
  {
    "text": "distribution compared\nto our upsampled data distribution for Phase 2.",
    "start": "1370750",
    "end": "1376000"
  },
  {
    "text": "And we also showed that this\nis able to scale both on model scale and data scale. So if you pull up the token\ncounts as well as the model",
    "start": "1376000",
    "end": "1382862"
  },
  {
    "text": "size, we show that\nperformance further improves with our two-phase\npretraining compared",
    "start": "1382862",
    "end": "1388029"
  },
  {
    "text": "to a single phase. So this highlights\nalso the effectiveness of prototyping on smaller\ndata blends before scaling up.",
    "start": "1388030",
    "end": "1397630"
  },
  {
    "text": "And furthermore, we\ninvestigated the phase, the duration of Phase 2. So should we train on\ndiverse data for a little bit",
    "start": "1397630",
    "end": "1405679"
  },
  {
    "text": "and immediately switch to highly\nspecialized data like math, or should we wait longer? And what we found is\nperformance improves up",
    "start": "1405680",
    "end": "1411800"
  },
  {
    "text": "to a point around 40% until\nthere's diminishing returns likely from overfitting. Because specialized data,\nit's more specialized.",
    "start": "1411800",
    "end": "1419390"
  },
  {
    "text": "There's typically a\nlower number of it, and it's less diverse compared\nto things like web crawl data.",
    "start": "1419390",
    "end": "1425039"
  },
  {
    "text": "So too much of it can lead\nto detrimental or diminishing returns.",
    "start": "1425040",
    "end": "1430500"
  },
  {
    "text": "So overall, we see a\nwell-structured two-phase pretraining approach, with\ncareful data selection",
    "start": "1430500",
    "end": "1435740"
  },
  {
    "text": "and management, is essential\nfor optimizing LLM performance while maintaining\nscalability and robustness",
    "start": "1435740",
    "end": "1442040"
  },
  {
    "text": "across different\ndownstream tasks. And in case you're\ninterested, this paper is also-- preprint\nis up on arXiv.",
    "start": "1442040",
    "end": "1449930"
  },
  {
    "text": "So I guess the overall takeaway\nfrom these two projects and what I wanted to\nget at is on the fact",
    "start": "1449930",
    "end": "1455899"
  },
  {
    "text": "that data effectiveness\nespecially for pretraining, it's not just the\namount of data, but it's about the\nquality of the data,",
    "start": "1455900",
    "end": "1462102"
  },
  {
    "text": "the ordering and\nstructure of data, and how exactly you use it. So for our first\nproject, we saw there's",
    "start": "1462102",
    "end": "1467640"
  },
  {
    "text": "negligible impact of global\norder in small-scale training. But we saw that phase-based\ntraining for larger scales",
    "start": "1467640",
    "end": "1473790"
  },
  {
    "text": "is highly effective. And in general,\nsmart data decisions are essential for models\nto generalize across tasks.",
    "start": "1473790",
    "end": "1479620"
  },
  {
    "text": "So takeaway is our\nresearch underscores that effective language modeling\nisn't just about amassing data",
    "start": "1479620",
    "end": "1486309"
  },
  {
    "text": "but about smarter\ndata organization that harnesses its structure,\nquality, and characteristics.",
    "start": "1486310",
    "end": "1491320"
  },
  {
    "text": "And by continuing to refine\ndata-centric approaches, the future of LLM\ntraining promises",
    "start": "1491320",
    "end": "1496470"
  },
  {
    "text": "smarter, more efficient,\nand highly adaptable models.",
    "start": "1496470",
    "end": "1501630"
  },
  {
    "text": "So now, we'll be moving\nto the second stage after pretraining\nwhich is post-training, which Chelsea will talk about.",
    "start": "1501630",
    "end": "1508650"
  },
  {
    "text": "All right. So we have a pre-trained\nmodel, now what? How do we adapt to specific\ntasks and different domains?",
    "start": "1508650",
    "end": "1516990"
  },
  {
    "text": "So some major strategies include\nfine-tuning, for instance, like reinforcement learning\nwith human feedback",
    "start": "1516990",
    "end": "1523850"
  },
  {
    "text": "or some prompt-based methods,\nor some of RAG architecture and retrieval-based methods.",
    "start": "1523850",
    "end": "1529493"
  },
  {
    "start": "1529493",
    "end": "1538490"
  },
  {
    "text": "So one major approach is called\nchain-of-though reasoning. I'm sure you all have\nheard of it by now.",
    "start": "1538490",
    "end": "1544220"
  },
  {
    "text": "So it's essentially a prompting\ntechnique to think step by step. So it shows the intermediate\nsteps, provide guidance,",
    "start": "1544220",
    "end": "1552540"
  },
  {
    "text": "and this is similar to\nthe way how humans think. We can imagine that we\ntypically break down a problem",
    "start": "1552540",
    "end": "1559880"
  },
  {
    "text": "into subsequent steps\nto help us better understand the problem itself. And another benefit\nof chain of thought",
    "start": "1559880",
    "end": "1566990"
  },
  {
    "text": "is that it allows some\nof interpretable window into the behavior of the model.",
    "start": "1566990",
    "end": "1573170"
  },
  {
    "text": "And this can suggest that there\nis more knowledge embedded in the model's weights than\njust prompting a response.",
    "start": "1573170",
    "end": "1580273"
  },
  {
    "start": "1580273",
    "end": "1585880"
  },
  {
    "text": "So this here is an example\nof chain of thought. On the left, we have\nit solve a problem",
    "start": "1585880",
    "end": "1591820"
  },
  {
    "text": "in a one-shot manner,\nwhich turns out to get to the wrong answer. And on the right over\nthere, it produces",
    "start": "1591820",
    "end": "1599200"
  },
  {
    "text": "a sequence of reasoning chains. And ultimately, it arrives\nat the correct answer. ",
    "start": "1599200",
    "end": "1611049"
  },
  {
    "text": "So naturally, this\nbrings up an extension of chain-of-thought, which\nis called a tree-of-thought.",
    "start": "1611050",
    "end": "1617210"
  },
  {
    "text": "And this is another\nprompting technique. But instead of producing\na singular reasoning path as a chain-of-thought does, it\nconsiders multiple reasoning",
    "start": "1617210",
    "end": "1626080"
  },
  {
    "text": "trajectories and then uses some\nsort of self-evaluation process to decide on the final outputs\nsuch as majority voting.",
    "start": "1626080",
    "end": "1635750"
  },
  {
    "text": "So in the picture, you can\nsee that tree-of-thought generates different\nreasoning paths",
    "start": "1635750",
    "end": "1641210"
  },
  {
    "text": "and selects the\nbest one at the end. ",
    "start": "1641210",
    "end": "1648710"
  },
  {
    "text": "So another way is through\nprogram-of-thought. And this basically\ngenerates code",
    "start": "1648710",
    "end": "1654050"
  },
  {
    "text": "as the intermediate\nreasoning steps. And overall, what\nthis does is that it offloads some\nproblem-solving technique",
    "start": "1654050",
    "end": "1662330"
  },
  {
    "text": "to some code interpreter. So it formalizes\nlanguage into programs",
    "start": "1662330",
    "end": "1668450"
  },
  {
    "text": "to arrive at more\nprecise answers. ",
    "start": "1668450",
    "end": "1679250"
  },
  {
    "text": "So we have seen that this\nproblem decomposition seems helpful for different tasks.",
    "start": "1679250",
    "end": "1685730"
  },
  {
    "text": "So one way is through\nSocratic questioning, which is basically using\na self-questioning module",
    "start": "1685730",
    "end": "1692330"
  },
  {
    "text": "to propose subproblems\nrelated to the original and solves those in\na recursive manner.",
    "start": "1692330",
    "end": "1698730"
  },
  {
    "text": "So for instance, if\nthe question is like, what fills the balloons? This leads to the next\nsubquestion, which is like,",
    "start": "1698730",
    "end": "1706260"
  },
  {
    "text": "what can make a balloon float? And then by decomposing\nthe original problem into subsequent problems, it\ncan better solve at the end.",
    "start": "1706260",
    "end": "1716300"
  },
  {
    "start": "1716300",
    "end": "1722820"
  },
  {
    "text": "So finally, another problem\ndecomposition method is through computational graphs.",
    "start": "1722820",
    "end": "1728039"
  },
  {
    "text": "So this basically formulates\ncompositional tasks as a computation\ngraphs by breaking down",
    "start": "1728040",
    "end": "1734400"
  },
  {
    "text": "the reasoning into different\nsub-procedures and nodes. So the key takeaway here\nis that transformers",
    "start": "1734400",
    "end": "1741630"
  },
  {
    "text": "can solve compositional\ntasks by reducing reasoning into subgraphs, and this\nis without developing",
    "start": "1741630",
    "end": "1749790"
  },
  {
    "text": "some systematic\nproblem-solving skill. Right. So Chelsea touched on\nchain-of-thought and everything",
    "start": "1749790",
    "end": "1756190"
  },
  {
    "text": "that expands upon\nit or improves it. And that's mainly a\nprompting-based method",
    "start": "1756190",
    "end": "1761740"
  },
  {
    "text": "for inference time. Next, I'll be talking\nmore at reinforcement learning and feedback\nmechanisms, which are typically",
    "start": "1761740",
    "end": "1767158"
  },
  {
    "text": "used for things like further\nfine-tuning a pre-trained model. So the most popular\nis this thing",
    "start": "1767158",
    "end": "1772390"
  },
  {
    "text": "called Reinforcement Learning\nwith Human Feedback or RLHF. So this trains a reward model\ndirectly from human feedback.",
    "start": "1772390",
    "end": "1778910"
  },
  {
    "text": "So you do is you take\nyour pre-trained model, get it to generate\nseveral responses, and then you typically take\na pair, a pair of responses",
    "start": "1778910",
    "end": "1785680"
  },
  {
    "text": "and have humans rate\nwhich one they prefer. And you can train a reward\nmodel based on this basically",
    "start": "1785680",
    "end": "1791980"
  },
  {
    "text": "using a reinforcement\nlearning optimization algorithm such as PPO. ",
    "start": "1791980",
    "end": "1798580"
  },
  {
    "text": "Now, there's an improvement\nto PPO called DPO or Direct Preference Optimization. So this more directly\ntrains the model",
    "start": "1798580",
    "end": "1804940"
  },
  {
    "text": "to prefer outputs that\nhumans rank higher compared to having a\nseparate reward model, which is much more efficient.",
    "start": "1804940",
    "end": "1810520"
  },
  {
    "text": "So basically, you\ncan think of it as it more closely ties the\nreward directly into the loss",
    "start": "1810520",
    "end": "1817190"
  },
  {
    "text": "function itself by helping\nthe LLM to maximize the likelihood of generating\npreferred responses",
    "start": "1817190",
    "end": "1824270"
  },
  {
    "text": "and minimize the\nlikelihood of the responses that humans did not prefer. ",
    "start": "1824270",
    "end": "1831620"
  },
  {
    "text": "And there's a extension to\nRLHF, which is called RLAIF. So this is simply replacing\nthe human with an AI.",
    "start": "1831620",
    "end": "1839240"
  },
  {
    "text": "So you typically have\na pretty good LLM that's able to provide accurate\npreference judgments of which",
    "start": "1839240",
    "end": "1845060"
  },
  {
    "text": "response it prefers. And this is less\ncostly basically compared to human annotators.",
    "start": "1845060",
    "end": "1850890"
  },
  {
    "text": "And then basically\nyou do the same thing. You train a reward model\nbased on the LLM's preferences",
    "start": "1850890",
    "end": "1857809"
  },
  {
    "text": "instead. And actually, human evaluators\nfound that RLAIF-tuned outputs",
    "start": "1857810",
    "end": "1864320"
  },
  {
    "text": "were similar to RLHF,\nshowing that this is a more scalable and\ncost-efficient approach compared",
    "start": "1864320",
    "end": "1870799"
  },
  {
    "text": "to human feedback. But there's one\ndisadvantage here, which is it really depends on\nthe capabilities or the accuracy",
    "start": "1870800",
    "end": "1877860"
  },
  {
    "text": "of judgments of the\nLLM you're using to provide your preferences. So if you're using one that\nis incapable or very noisy,",
    "start": "1877860",
    "end": "1887110"
  },
  {
    "text": "then that's going to\nhurt your post-training.",
    "start": "1887110",
    "end": "1892260"
  },
  {
    "text": "The next is this\nthing very hot now which was used in\nDeepSeek both their R1,",
    "start": "1892260",
    "end": "1898740"
  },
  {
    "text": "as well as some other\nmodels, like their math ones. So this is called Group Relative\nPolicy Optimization or GRPO.",
    "start": "1898740",
    "end": "1906000"
  },
  {
    "text": "So this is a variant of the\nPPO optimization algorithm. But rather than ranking\nsimply pairs of responses,",
    "start": "1906000",
    "end": "1913240"
  },
  {
    "text": "it actually ranks a group of\nresponses in a different order. So this provides\nricher feedback,",
    "start": "1913240",
    "end": "1919925"
  },
  {
    "text": "which is more\nfine-grained and is much more efficient compared to\nsimply ranking pairs of outputs.",
    "start": "1919925",
    "end": "1926400"
  },
  {
    "text": "So this helps\nstabilize training, which is one reason DeepSeek\nis very much more data",
    "start": "1926400",
    "end": "1932370"
  },
  {
    "text": "and compute efficient. And also, they saw\nthat it improves",
    "start": "1932370",
    "end": "1937830"
  },
  {
    "text": "even things like LLM reasoning\nespecially on things like math. ",
    "start": "1937830",
    "end": "1944280"
  },
  {
    "text": "There's also been other\nvariations of RLHF and so forth. One is this thing called\nKahneman-Tversky Optimization.",
    "start": "1944280",
    "end": "1951640"
  },
  {
    "text": "Not sure if I'm pronouncing\nthat correctly, but KTO. So this modifies the standard\nloss function typically used",
    "start": "1951640",
    "end": "1958679"
  },
  {
    "text": "in post-training\nthings to account for human biases such\nas loss aversion.",
    "start": "1958680",
    "end": "1963700"
  },
  {
    "text": "So as humans, we care\nmore about minimizing disastrous or negative outcomes\nthan achieving positive ones.",
    "start": "1963700",
    "end": "1971289"
  },
  {
    "text": "We're more risk averse\nin the most case, although it's very\ndependent on the person. So they encouraged the AI to\nbehave in a similar manner",
    "start": "1971290",
    "end": "1979259"
  },
  {
    "text": "by avoiding negative\noutcomes, and this basically adjusts the training\nprocess to reflect this.",
    "start": "1979260",
    "end": "1985539"
  },
  {
    "text": "And they showed that this is\nable to improve performance on different tasks, although\nit depends on the task.",
    "start": "1985540",
    "end": "1992540"
  },
  {
    "text": "But overall, it shows\nmore human-like behavior on particular tasks. And these are just a subset\nof the RLHF and reinforcement",
    "start": "1992540",
    "end": "2001320"
  },
  {
    "text": "learning and\nfeedback-based algorithms. One I want to touch\nupon before I finish off",
    "start": "2001320",
    "end": "2008430"
  },
  {
    "text": "is this thing\ncalled personalizing RLHF with the variational\npreference learning. So the authors saw that\ndifferent demographics",
    "start": "2008430",
    "end": "2016230"
  },
  {
    "text": "have different preferences. So typical RLHF averages\neverything together.",
    "start": "2016230",
    "end": "2022200"
  },
  {
    "text": "So what the authors do is\nthey introduce a latent variable for every user\npreference profile, for example, a\ndifferent demographic",
    "start": "2022200",
    "end": "2028170"
  },
  {
    "text": "like children,\nadults, and so forth and trains reward models\nconditioned on these latent",
    "start": "2028170",
    "end": "2034650"
  },
  {
    "text": "vectors or factors. So this leads to something they\ncall pluralistic alignment, which is improving\nthe reward accuracy",
    "start": "2034650",
    "end": "2040980"
  },
  {
    "text": "for these particular\ndemographics or subgroups. So it enables a single\nmodel to adapt its behavior",
    "start": "2040980",
    "end": "2046860"
  },
  {
    "text": "to different preference profiles\nand different demographics or groups of people.",
    "start": "2046860",
    "end": "2053577"
  },
  {
    "text": "And now, I'll hand\nit back to Chelsea to talk about self-improving. ",
    "start": "2053578",
    "end": "2059949"
  },
  {
    "text": "All right. So yeah, let's talk a little bit\nabout self-improving AI agents.",
    "start": "2059949",
    "end": "2066009"
  },
  {
    "text": "So what exactly is an AI agent? So it's essentially a system\nthat perceives the environment,",
    "start": "2066010",
    "end": "2073129"
  },
  {
    "text": "makes decisions and takes\nactions towards achieving some specific goal.",
    "start": "2073130",
    "end": "2079270"
  },
  {
    "text": "And usually, this goal is given\nby the human so for instance like game-playing, task-solving,\nor research assistance.",
    "start": "2079270",
    "end": "2088570"
  },
  {
    "text": "And there are several\ncomponents of an AI agent. So, one, it's goal-directed.",
    "start": "2088570",
    "end": "2094030"
  },
  {
    "text": "Two, it can make\nits own decisions. Three, it can act iteratively.",
    "start": "2094030",
    "end": "2101230"
  },
  {
    "text": "Four, there's usually some\nmemory component to it and state tracking\ncomponent to it.",
    "start": "2101230",
    "end": "2107810"
  },
  {
    "text": "And finally, there's\nsome agents that can use some tools such as\nAPI calls or function calling.",
    "start": "2107810",
    "end": "2115640"
  },
  {
    "text": "And finally, it can learn\nand adapt on its own. ",
    "start": "2115640",
    "end": "2124704"
  },
  {
    "text": "OK. Yeah. So self-improvement,\nbasically, models can reflect on their own\noutputs leading to iterative",
    "start": "2124705",
    "end": "2133220"
  },
  {
    "text": "improvements over time. So this typically\nconsists of several steps.",
    "start": "2133220",
    "end": "2139400"
  },
  {
    "text": "There's some reflection of\nits own internal states. There's an explanation of\nits own reasoning process.",
    "start": "2139400",
    "end": "2148880"
  },
  {
    "text": "It can evaluate the\nquality of its own outputs. And finally, it can also\nsimulate multi-step reasoning",
    "start": "2148880",
    "end": "2155480"
  },
  {
    "text": "chains.  So one technique is refinement.",
    "start": "2155480",
    "end": "2162599"
  },
  {
    "text": "So this is where you have some\niterative prompting technique where an LLM critiques and\nimproves its own outputs.",
    "start": "2162600",
    "end": "2171349"
  },
  {
    "text": "So it generates some\ninitial response and then refines it over time.",
    "start": "2171350",
    "end": "2176720"
  },
  {
    "text": "And this uses feedback loops to\nenhance the overall performance.",
    "start": "2176720",
    "end": "2182510"
  },
  {
    "text": "So an example would be\nit generates some answer and then it evaluates itself for\nweaknesses and inconsistencies.",
    "start": "2182510",
    "end": "2191010"
  },
  {
    "text": "And finally, it\nrefines the response based on the own\nself-critique method.",
    "start": "2191010",
    "end": "2196015"
  },
  {
    "text": " Another technique is\ncalled self-reflection.",
    "start": "2196015",
    "end": "2202830"
  },
  {
    "text": "So this is where a model\nlearns from past mistakes and adjusts future responses\nbased on past failures.",
    "start": "2202830",
    "end": "2209880"
  },
  {
    "text": "So there usually is some\nlong-term memory component to this. And an example would be\nlike the model first detects",
    "start": "2209880",
    "end": "2217280"
  },
  {
    "text": "some of weak response\nfrom its own outputs, and then it reflects\non its own mistakes",
    "start": "2217280",
    "end": "2223370"
  },
  {
    "text": "and generates some\nimproved answer to it. And over multiple iterations,\naccuracy and reasoning",
    "start": "2223370",
    "end": "2231690"
  },
  {
    "text": "should improve over time. ",
    "start": "2231690",
    "end": "2237900"
  },
  {
    "text": "Another technique is called\nReAct, which is essentially just combining reasoning with\nexternal actions such as API",
    "start": "2237900",
    "end": "2246329"
  },
  {
    "text": "calls or retrievals\nfrom a database. And this is basically some model\nthat can interact dynamically",
    "start": "2246330",
    "end": "2252510"
  },
  {
    "text": "with its environment. So it gets feedback from taking\nmultiple action sequences",
    "start": "2252510",
    "end": "2259410"
  },
  {
    "text": "and incorporating\nthat into its outputs. So for instance, the model\nwill generate a reasoning plan,",
    "start": "2259410",
    "end": "2267060"
  },
  {
    "text": "and then it will call some\nexternal tool such as web search or some API call,\nand then this model",
    "start": "2267060",
    "end": "2274560"
  },
  {
    "text": "incorporates the retrieved\ndata into its final response. ",
    "start": "2274560",
    "end": "2281700"
  },
  {
    "text": "And finally, this leads us to a\nframework called Language Agent Tree Search.",
    "start": "2281700",
    "end": "2287650"
  },
  {
    "text": "So basically what\nLATS is is that it extends the ReAct framework to\nincorporate multiple planning",
    "start": "2287650",
    "end": "2293589"
  },
  {
    "text": "pathways. So you can think this like\nanalogous to chain-of-thought versus tree-of-thought.",
    "start": "2293590",
    "end": "2299290"
  },
  {
    "text": "It gathers feedback\nfrom every path to improve the future\nsearch process, which",
    "start": "2299290",
    "end": "2305410"
  },
  {
    "text": "is some verbal reinforcement\nlearning-inspired technique. And it uses Monte\nCarlo Tree Search",
    "start": "2305410",
    "end": "2312099"
  },
  {
    "text": "to optimize planning\ntrajectories, where in the tree structure,\nevery node represents a state",
    "start": "2312100",
    "end": "2318790"
  },
  {
    "text": "and every edge represents an\naction that the agent can take.",
    "start": "2318790",
    "end": "2323890"
  },
  {
    "text": "So an example would be it\ngenerates N best new action sequences, and then it will just\nexecute them all in parallel.",
    "start": "2323890",
    "end": "2332170"
  },
  {
    "text": "Then it will use some\nself-reflection technique to score each one,\nand then overall,",
    "start": "2332170",
    "end": "2338530"
  },
  {
    "text": "just continue exploring\nfrom the best state and update the probabilities\nof the past node.",
    "start": "2338530",
    "end": "2344619"
  },
  {
    "text": " And yeah.",
    "start": "2344620",
    "end": "2350180"
  },
  {
    "text": "All right. Next, I'll be talking about\na few other applications of transformers\noutside of language.",
    "start": "2350180",
    "end": "2356930"
  },
  {
    "text": "I'll start with Vision\nTransformers which have taken vision by storm.",
    "start": "2356930",
    "end": "2362150"
  },
  {
    "text": "The methodology here is that-- so as I talked about\ntransformers taking sequences,",
    "start": "2362150",
    "end": "2369910"
  },
  {
    "text": "but images aren't sequences. However, what the authors of\nthe ViT paper came up with",
    "start": "2369910",
    "end": "2376070"
  },
  {
    "text": "was to split an image up\ninto patches, which can then be embedded to form a sequence.",
    "start": "2376070",
    "end": "2383660"
  },
  {
    "text": "Passing this through\na simple transformer yielded very good results, for\ninstance, on classification,",
    "start": "2383660",
    "end": "2389640"
  },
  {
    "text": "just by adding an\nMLP head to the end. You might ask why apply\ntransformers to this problem",
    "start": "2389640",
    "end": "2396950"
  },
  {
    "text": "when CNNs are such a\nmainstay in the field. The main reason is that when\nyou have a very large data set,",
    "start": "2396950",
    "end": "2405059"
  },
  {
    "text": "say, in the tens of\nmillions of examples, transformers bring in\nless inductive biases.",
    "start": "2405060",
    "end": "2410670"
  },
  {
    "text": "CNNs assume locality and that\npixels are grouped together. Whereas with transformers\nand treating your images",
    "start": "2410670",
    "end": "2418260"
  },
  {
    "text": "as sequences, you can\nsee better results when you have enough\ndata to train them.",
    "start": "2418260",
    "end": "2424380"
  },
  {
    "text": "One common architecture\nthat was impacted by this was CLIP, which uses ViTs\nfor its image encoders.",
    "start": "2424380",
    "end": "2433500"
  },
  {
    "text": "This is the basis of\nmodels like GPT-4o or other vision-language\nmodels and essentially works",
    "start": "2433500",
    "end": "2442140"
  },
  {
    "text": "through contrastive learning. So you take a data set of\npaired images and text pairs,",
    "start": "2442140",
    "end": "2448349"
  },
  {
    "text": "and you train your model\nto align the encoder representations of both.",
    "start": "2448350",
    "end": "2454030"
  },
  {
    "text": "So if you have an image of\na cat and and the word cat, then you can learn to\nalign those embeddings.",
    "start": "2454030",
    "end": "2462960"
  },
  {
    "text": "And like I mentioned, these have\nbeen applied to vision-language models like GPT-4 or 4o.",
    "start": "2462960",
    "end": "2469710"
  },
  {
    "text": "The way these are trained is you\nconcatenate your encoded image",
    "start": "2469710",
    "end": "2475080"
  },
  {
    "text": "and text, and you can\ntrain in different stages such that your model\nlearns to take both",
    "start": "2475080",
    "end": "2481440"
  },
  {
    "text": "and to account\nfor its responses. And these have done very\nwell on benchmarks and tasks",
    "start": "2481440",
    "end": "2489069"
  },
  {
    "text": "for instance like test\nquestions like I've shown here.",
    "start": "2489070",
    "end": "2494730"
  },
  {
    "text": "Next, I'll talk about a bit\nof my work and neuroscience, which applies ViTs to\nother kinds of data.",
    "start": "2494730",
    "end": "2502650"
  },
  {
    "text": "So a mainstay in my field is\nfunctional magnetic resonance imaging, or fMRI.",
    "start": "2502650",
    "end": "2508230"
  },
  {
    "text": "Essentially, this captures\nthe amount of oxygen that each voxel part of your\nbrain is using at a given point,",
    "start": "2508230",
    "end": "2515069"
  },
  {
    "text": "and this provides a very\ndetailed proxy for the activity going on in your brain.",
    "start": "2515070",
    "end": "2520930"
  },
  {
    "text": "It can be used to diagnose\ndiseases and capture various amounts of data for\nbetter cognitive understanding.",
    "start": "2520930",
    "end": "2530380"
  },
  {
    "text": "However, this is very\nhigh dimensional. You might have a million or so\nvoxels or 100,000 in the brain.",
    "start": "2530380",
    "end": "2536810"
  },
  {
    "text": "So the first step to using this\ndata with transformer models is usually averaging\nacross well-known regions",
    "start": "2536810",
    "end": "2544210"
  },
  {
    "text": "or just grouping\ntogether voxels, and this gives you a more\ncomputationally tractable number",
    "start": "2544210",
    "end": "2550360"
  },
  {
    "text": "of parcels that\nyou can train on. A traditional tool in this field\nwas just to use linear pairwise",
    "start": "2550360",
    "end": "2557770"
  },
  {
    "text": "correlation maps, and just these\nwere enough to get pretty good diagnoses of things\nlike Parkinson's.",
    "start": "2557770",
    "end": "2564530"
  },
  {
    "text": "However, with the advent of tons\nof computer vision techniques, we can apply larger\nand more sophisticated",
    "start": "2564530",
    "end": "2571329"
  },
  {
    "text": "models to these tasks. One cool, large body\nof work in this area",
    "start": "2571330",
    "end": "2577849"
  },
  {
    "text": "is divvying up the brain into\ndifferent functional networks, so let's say like your vision\nsystem or your daydreaming",
    "start": "2577850",
    "end": "2586490"
  },
  {
    "text": "network or control, et cetera. And I'll get into how we\nuse this to guide our work.",
    "start": "2586490",
    "end": "2594770"
  },
  {
    "text": "So like I mentioned,\nearly ML models just took like linear\ncorrelation maps",
    "start": "2594770",
    "end": "2599880"
  },
  {
    "text": "so making lots of\nassumptions about the data and just applied typical\nneural networks to the task",
    "start": "2599880",
    "end": "2606740"
  },
  {
    "text": "for regression or classification\ntasks or, in some cases, graph-based analyses\nto try to get a deeper",
    "start": "2606740",
    "end": "2613400"
  },
  {
    "text": "understanding of how the\ndifferent parts of the brain interact with each other.",
    "start": "2613400",
    "end": "2619100"
  },
  {
    "text": "With computer vision,\nwe can take our raw data and just throw that at\na transformer model,",
    "start": "2619100",
    "end": "2625200"
  },
  {
    "text": "and that does very well as\na pre-training objective. So what we do is let's say\nwe have some number of ROIs",
    "start": "2625200",
    "end": "2632930"
  },
  {
    "text": "across time, we can just mask\nout some portion of that data,",
    "start": "2632930",
    "end": "2638490"
  },
  {
    "text": "pass the rest of the data\nthrough a transformer model, and have it predict\nthis portion. You repeat this across a large\ndata set and all of your ROIs,",
    "start": "2638490",
    "end": "2647950"
  },
  {
    "text": "and this provides a very\ngood self-supervised training objective for this task. So self-supervised\nessentially means that there",
    "start": "2647950",
    "end": "2655080"
  },
  {
    "text": "is no paired labeled data here. We are essentially\njust using our raw data",
    "start": "2655080",
    "end": "2660300"
  },
  {
    "text": "and posing our\nobjective such that we can learn directly off of it. ",
    "start": "2660300",
    "end": "2667140"
  },
  {
    "text": "Once you've trained\nthis model, you have these dense representations\ninside the model that",
    "start": "2667140",
    "end": "2673440"
  },
  {
    "text": "can be applied downstream\nto various tasks like predicting\npatient attributes or their risk of disease.",
    "start": "2673440",
    "end": "2679630"
  },
  {
    "text": "And you can also\nlook at the weights that your model has learned\nto analyses of brain networks.",
    "start": "2679630",
    "end": "2689760"
  },
  {
    "text": "So in brief, our\napproach essentially consists of taking the\nactivity in the entire brain,",
    "start": "2689760",
    "end": "2695750"
  },
  {
    "text": "partitioning out\nsome small region, let's say it's\nyour vision system. You pass the unmasked portion\ninto a transformer model, which",
    "start": "2695750",
    "end": "2704680"
  },
  {
    "text": "learns to predict\nthe masked portion, and you can compare this\nto your ground truth to provide your\ntraining objective.",
    "start": "2704680",
    "end": "2711920"
  },
  {
    "text": " One key thing we use\nhere is cross-attention.",
    "start": "2711920",
    "end": "2719360"
  },
  {
    "text": "So what we talked about\nbefore with language was self-attention,\nwherein you are attending to the current\nsequence you're looking at.",
    "start": "2719360",
    "end": "2727240"
  },
  {
    "text": "In cross-attention, you have\ntwo different sequences. Let's say in machine\ntranslation you have one",
    "start": "2727240",
    "end": "2732610"
  },
  {
    "text": "in English and in French. And essentially,\nyou apply attention between the two\nsequences instead",
    "start": "2732610",
    "end": "2739960"
  },
  {
    "text": "of just on a single sequence. So our most basic architecture\ntakes advantage of this",
    "start": "2739960",
    "end": "2747760"
  },
  {
    "text": "through just a singular\ncross-attention decoder. Having a very small model makes\nfor better interpretability.",
    "start": "2747760",
    "end": "2754380"
  },
  {
    "text": "And like I mentioned,\nthis model just learns to predict masked brain\nregions from unmasked ones.",
    "start": "2754380",
    "end": "2759980"
  },
  {
    "text": "Once we've done this, we can\nanalyze again the attention weights to gain a deeper\nunderstanding of networks",
    "start": "2759980",
    "end": "2765980"
  },
  {
    "text": "and also apply this\nto downstream tasks. So some modeling results.",
    "start": "2765980",
    "end": "2771020"
  },
  {
    "text": "Here, I've plotted\nlike the brain activity from different patients. And you can see that the\nmodel does pretty well",
    "start": "2771020",
    "end": "2777920"
  },
  {
    "text": "in matching the ground\ntruth for two networks that I've shown here, the\nsalience network, which",
    "start": "2777920",
    "end": "2784369"
  },
  {
    "text": "is involved in your senses\nand decision making, and the Default Mode\nNetwork or DMN, which",
    "start": "2784370",
    "end": "2791180"
  },
  {
    "text": "is responsible for daydreaming\nor just recapitulating your brain information when\nyou're not doing a certain task.",
    "start": "2791180",
    "end": "2798860"
  },
  {
    "text": "On the bottom, we have\nthe attention weights for this model, which I've split\nup by all of the other networks.",
    "start": "2798860",
    "end": "2806220"
  },
  {
    "text": "So for instance,\non the left, when predicting the salience network,\nwe can see from our model",
    "start": "2806220",
    "end": "2811430"
  },
  {
    "text": "that it is heavily dependent\non the default mode and control networks. So this gives us a\nbetter understanding",
    "start": "2811430",
    "end": "2817520"
  },
  {
    "text": "of how different brain networks\nare connected to each other or how they might share\ninformation inside the brain.",
    "start": "2817520",
    "end": "2825080"
  },
  {
    "text": "For other networks, though, like\nvision, these are more singular. We can't predict them very well.",
    "start": "2825080",
    "end": "2830660"
  },
  {
    "text": "Or subcortical regions, say,\nthose involved in memory, we also cannot\npredict very well. ",
    "start": "2830660",
    "end": "2838610"
  },
  {
    "text": "So this is all well and cool. We can predict brain\nactivity, but what can we do with this model?",
    "start": "2838610",
    "end": "2844700"
  },
  {
    "text": "If we simply replace\none component of the model with a learnable\ntoken, which corresponds",
    "start": "2844700",
    "end": "2851390"
  },
  {
    "text": "to predicting\nParkinson's disease, then we can use this model\nto predict that ailment.",
    "start": "2851390",
    "end": "2858900"
  },
  {
    "text": "So if you look on the right,\nafter some fine tuning on a labeled data set, we\ncan see some clustering",
    "start": "2858900",
    "end": "2864710"
  },
  {
    "text": "in the model's embedding,\nwhich corresponds to getting close to 70%\naccuracy in predicting",
    "start": "2864710",
    "end": "2871589"
  },
  {
    "text": "this disease, which is\nmuch higher than using the correlation-based based\nmethods or linear assumptions",
    "start": "2871590",
    "end": "2878250"
  },
  {
    "text": "that I talked about earlier.  All right.",
    "start": "2878250",
    "end": "2883720"
  },
  {
    "text": "So now that we have some\nbackground on these transformer models and a couple\nof their applications,",
    "start": "2883720",
    "end": "2888790"
  },
  {
    "text": "let's talk about the\nfuture and what's next. So overall, these\ntransformer models can enable a lot\nmore applications",
    "start": "2888790",
    "end": "2896010"
  },
  {
    "text": "across every\nindustry and sector. This includes generalist\nagents as well as longer video",
    "start": "2896010",
    "end": "2901200"
  },
  {
    "text": "understanding and generation\nacross the finance and business sector, domain-specific\nfoundation models like,",
    "start": "2901200",
    "end": "2907660"
  },
  {
    "text": "for example, one could imagine\na DoctorGPT or a LawyerGPT or an \"insert field\" GPT as well\nas potential real-world impacts",
    "start": "2907660",
    "end": "2916740"
  },
  {
    "text": "like personalized education\nand tutoring systems, advanced healthcare diagnostics,\nenvironmental monitoring",
    "start": "2916740",
    "end": "2923070"
  },
  {
    "text": "and protection, real-time\nmultilingual communication as well as in interactive\nenvironment and gaming,",
    "start": "2923070",
    "end": "2929330"
  },
  {
    "text": "for example,\nnon-playable characters. ",
    "start": "2929330",
    "end": "2935890"
  },
  {
    "text": "What is missing though? What information might\nwe need and what can we develop in the future?",
    "start": "2935890",
    "end": "2941420"
  },
  {
    "text": "Currently, we're missing\nreducing computation complexity,",
    "start": "2941420",
    "end": "2947030"
  },
  {
    "text": "enhancing human controllability,\nalignment with the language models of the human\nbrain, adaptive learning",
    "start": "2947030",
    "end": "2954430"
  },
  {
    "text": "and generalization across\ndifferent domains, and finally, multi-sensory, multimodal\nembodiment like intuitive",
    "start": "2954430",
    "end": "2961450"
  },
  {
    "text": "physics and commonsense. So one might consider\nthese barriers",
    "start": "2961450",
    "end": "2966490"
  },
  {
    "text": "to developing artificial\ngeneral intelligence, and these are some\nof the limitations of current transformer models.",
    "start": "2966490",
    "end": "2972210"
  },
  {
    "text": " Some other things\nthat are missing include infinite\nand external memory,",
    "start": "2972210",
    "end": "2978400"
  },
  {
    "text": "like Neural Turing Machines,\ninfinite self-improvement capabilities like continual\nor lifelong learning.",
    "start": "2978400",
    "end": "2984140"
  },
  {
    "text": "This is another central\ntenet of human learning that we're not able to\nreplicate at the moment. Complete autonomy, including\ncuriosity, desires, and goals,",
    "start": "2984140",
    "end": "2992900"
  },
  {
    "text": "and long-horizon\ndecision-making, as well as emotional intelligence,\nsocial understanding,",
    "start": "2992900",
    "end": "2998210"
  },
  {
    "text": "and of course, ethical\nreasoning and value alignment. ",
    "start": "2998210",
    "end": "3005380"
  },
  {
    "text": "All right. So there's still a plethora\nof remaining weaknesses or challenges around\ntransformers, large language",
    "start": "3005380",
    "end": "3012690"
  },
  {
    "text": "models, and AI in\ngeneral these days, so I'll touch upon a\ncouple of them briefly.",
    "start": "3012690",
    "end": "3017980"
  },
  {
    "text": "The first is, like I mentioned\nearlier, efficiency, being able to minify or have tiny\nLLMs or models that you",
    "start": "3017980",
    "end": "3025080"
  },
  {
    "text": "can run on your phone, on\nyour smartwatch, et cetera. So that's a big\ntrend these days is",
    "start": "3025080",
    "end": "3031830"
  },
  {
    "text": "using LLMs for everyday\napplications and purposes. And again, you want to be able\nto run them quickly and easily",
    "start": "3031830",
    "end": "3038700"
  },
  {
    "text": "on smaller devices. Right now, there\nis more and more work on smaller\nand more efficient",
    "start": "3038700",
    "end": "3043810"
  },
  {
    "text": "open-source models, things like\nDeepSeek, LLaMA and Mistral, but they're still somewhat\nlarge and a bit expensive",
    "start": "3043810",
    "end": "3051140"
  },
  {
    "text": "especially if you're\nlooking to finetune. They're still not accessible\nto everybody especially on smaller devices.",
    "start": "3051140",
    "end": "3057260"
  },
  {
    "text": "So in the future,\nagain, we want to aim to have the ability to finetune\nor run these models locally",
    "start": "3057260",
    "end": "3062860"
  },
  {
    "text": "on whatever device you want. The second is as our models,\nas our LLMs scale up,",
    "start": "3062860",
    "end": "3070930"
  },
  {
    "text": "trillions of parameters\ntrained on trillions of tokens across the internet,\nwhat happens",
    "start": "3070930",
    "end": "3076120"
  },
  {
    "text": "is this makes it a\nhuge black-box that is difficult to\nunderstand or interpret. It's hard to know what exactly\nis going on behind the scenes.",
    "start": "3076120",
    "end": "3082660"
  },
  {
    "text": "When you ask it to solve\nXYZ, and it comes up with answers ABC, how\nexactly did it get there?",
    "start": "3082660",
    "end": "3087680"
  },
  {
    "text": "Why did it choose\nthose answers instead? And so forth. So more work on interpretability\nfor LLMs will give us",
    "start": "3087680",
    "end": "3094869"
  },
  {
    "text": "a better idea of what\nor how to improve them, easier ways of controlling\nthem, and better",
    "start": "3094870",
    "end": "3101109"
  },
  {
    "text": "alignment, for example,\nbeing able to prevent them from producing\ncertain outputs that might be unsafe or unethical.",
    "start": "3101110",
    "end": "3108667"
  },
  {
    "text": "So there's this area, which\nhas gotten even more popular recently called mechanistic\ninterpretability, which is trying to understand\nhow individual components",
    "start": "3108667",
    "end": "3115650"
  },
  {
    "text": "or operations, even sometimes\ndown to the individual node level, so very\ngranular, in an ML model",
    "start": "3115650",
    "end": "3123070"
  },
  {
    "text": "contribute to its overall\ndecision-making process with the goal, again, of\nunpacking this black-box",
    "start": "3123070",
    "end": "3129300"
  },
  {
    "text": "for clear insight on how exactly\nthey work behind the scenes.",
    "start": "3129300",
    "end": "3134460"
  },
  {
    "text": "Next, I feel like\nwe're approaching or we're already seeing\ndiminishing returns",
    "start": "3134460",
    "end": "3140850"
  },
  {
    "text": "with simply scaling up. So larger models on\nmore data does not seem to be the be-all,\nend-all solution.",
    "start": "3140850",
    "end": "3147460"
  },
  {
    "text": "So one-size-fits-all and frozen\npre-trained models have already started, leading to\ndiminishing returns.",
    "start": "3147460",
    "end": "3152730"
  },
  {
    "text": "So again, pre-training\nperformance. So the first half of training\nLLMs, it's likely saturating.",
    "start": "3152730",
    "end": "3160810"
  },
  {
    "text": "Hence, there's been more focus\non post-training methods. Everything we've talked about,\nfeedback and RL mechanisms,",
    "start": "3160810",
    "end": "3166000"
  },
  {
    "text": "and prompting methods\nlike chain-of-thought, self-improvement and\nrefinement and so forth. However, all of these\npost-training mechanisms",
    "start": "3166000",
    "end": "3172578"
  },
  {
    "text": "are going to be\nfundamentally limited by the overall performance or\ncapabilities of the base model. So you can argue that the\npre-training is fundamentally",
    "start": "3172578",
    "end": "3180270"
  },
  {
    "text": "what gives the basis or\nthe foundational knowledge and capabilities to the model.",
    "start": "3180270",
    "end": "3185400"
  },
  {
    "text": "So we should not just stop\ninvestigating pre-training just because we're\nhitting scaling limits. Furthermore, too\nmuch post-training",
    "start": "3185400",
    "end": "3192299"
  },
  {
    "text": "can actually lead to an issue. This is called\ncatastrophic forgetting where the model forgets stuff\nit's learned beforehand.",
    "start": "3192300",
    "end": "3199870"
  },
  {
    "text": "For example, during\npre-training, because you're overloading it with\ntons of new information",
    "start": "3199870",
    "end": "3204900"
  },
  {
    "text": "in a new domain or a new\ntask during post-training. So how do we break through\nthis scaling law limit?",
    "start": "3204900",
    "end": "3212269"
  },
  {
    "text": " Some potential\nthings to investigate",
    "start": "3212270",
    "end": "3218670"
  },
  {
    "text": "would be new architectures. There are different things like\nMamba, state space machines,",
    "start": "3218670",
    "end": "3224630"
  },
  {
    "text": "those architectures,\nand it would be good to see more\ninvestigation on even non-transformer\narchitectures, which",
    "start": "3224630",
    "end": "3230690"
  },
  {
    "text": "is a bit ironic considering this\nclass is transformers united, but we always encourage\nmore diversity",
    "start": "3230690",
    "end": "3236960"
  },
  {
    "text": "and thinking outside the box. Also again, everything I\ntalked about high-quality data",
    "start": "3236960",
    "end": "3242150"
  },
  {
    "text": "and smart data ordering\nand structuring strategies and overall\nimproved training",
    "start": "3242150",
    "end": "3247430"
  },
  {
    "text": "procedures, improved\nalgorithms, loss functions, optimization algorithms\nand so forth.",
    "start": "3247430",
    "end": "3254090"
  },
  {
    "text": "Another goal, as we've\nmentioned several times, is to be able to bring these\nadvanced capabilities to smaller",
    "start": "3254090",
    "end": "3260119"
  },
  {
    "text": "models. Furthermore, we\nwould still encourage more theoretical and\ninterpretability research",
    "start": "3260120",
    "end": "3266490"
  },
  {
    "text": "including things like\ncognitive science and neuroscience-inspired\nwork, which Karan and I have talked\nabout some of that",
    "start": "3266490",
    "end": "3273260"
  },
  {
    "text": "that we've done recently. And so the next step will be\nmodels that are not just larger,",
    "start": "3273260",
    "end": "3278359"
  },
  {
    "text": "but ones that are more\nsmarter and more adaptable. ",
    "start": "3278360",
    "end": "3283390"
  },
  {
    "text": "So again, there's this one\nmajor thing or major weakness that I think still\nbridges the gap between AI",
    "start": "3283390",
    "end": "3292990"
  },
  {
    "text": "and humans, which is continual\nor lifelong learning so AI systems that can\ncontinuously improve by learning",
    "start": "3292990",
    "end": "3300039"
  },
  {
    "text": "after deployment,\nafter being pre-trained using implicit feedback,\nreal-word world experience, and so forth.",
    "start": "3300040",
    "end": "3305750"
  },
  {
    "text": "So essentially, this is\ninfinite and permanent fundamental self-improvement. We're not just talking\nabout RAG or retrieval",
    "start": "3305750",
    "end": "3312230"
  },
  {
    "text": "like putting knowledge\nin a retrieval database that you can\nretrieve at test time but updating the brain or\nthe weights of the model",
    "start": "3312230",
    "end": "3319510"
  },
  {
    "text": "continuously. So this is similar to us. So we're learning every day. I'm learning right\nnow by talking to you.",
    "start": "3319510",
    "end": "3325270"
  },
  {
    "text": "I learn every time I\ntalk to somebody else as I'm going through\nmy daily life. But these models, after\nthey're frozen or pre-trained,",
    "start": "3325270",
    "end": "3331190"
  },
  {
    "text": "that doesn't really happen. They only way they truly learn\nor their brain or weights are updated is through finetuning.",
    "start": "3331190",
    "end": "3337443"
  },
  {
    "text": "And again, we don't do that. We don't sit-in a chair\nevery three months and have someone reread\nthe internet to us",
    "start": "3337443",
    "end": "3342620"
  },
  {
    "text": "or something like that. So again, this is\nalmost wasted work.",
    "start": "3342620",
    "end": "3348652"
  },
  {
    "text": "So currently during\ninference, the models are not actually learning\nand updating their weights. When they're talking, when\nChatGPT is talking to you,",
    "start": "3348652",
    "end": "3354570"
  },
  {
    "text": "it's not truly updating\nits brain or weights. So this is a very challenging\nproblem but in our opinion",
    "start": "3354570",
    "end": "3361610"
  },
  {
    "text": "is likely one of the\nkeys potentially to AGI or truly human-like AI systems.",
    "start": "3361610",
    "end": "3367700"
  },
  {
    "text": "So there's different current\nwork that tries to tackle this. There's things like finetuning\na smaller model based",
    "start": "3367700",
    "end": "3373760"
  },
  {
    "text": "on traces from a larger model,\nthings like model distillation related to a lot of things\nlike improvement and so forth.",
    "start": "3373760",
    "end": "3381110"
  },
  {
    "text": "But this is, again, not\ntruly continual learning. So some questions are, what\nmechanisms could potentially",
    "start": "3381110",
    "end": "3387650"
  },
  {
    "text": "truly enable real\nlifelong learning and will this be\ngradient updates?",
    "start": "3387650",
    "end": "3393180"
  },
  {
    "text": "So actually updating the brain. Will it be things like\ntargeting particular nodes in the architecture? Will it be having things like\nparticular memory architectures",
    "start": "3393180",
    "end": "3401819"
  },
  {
    "text": "or different parts of the\nneural network solely focused on continuous updates and\nlearning or even things",
    "start": "3401820",
    "end": "3408150"
  },
  {
    "text": "meta-learning learning\nand looking more at the broad scale of\nthings or the broader scope?",
    "start": "3408150",
    "end": "3413950"
  },
  {
    "text": "So one line of work which\nhas seen a bit of traction is model editing. So this is related to work on\nmechanistic interpretability.",
    "start": "3413950",
    "end": "3421840"
  },
  {
    "text": "So instead of updating\nthe whole model, if we're given a new\nfact or a new data point, can we target specific nodes\nor neurons in the model",
    "start": "3421840",
    "end": "3430170"
  },
  {
    "text": "that we should update? So one work called Rank-One\nModel Editing or ROME",
    "start": "3430170",
    "end": "3435720"
  },
  {
    "text": "tries to do this through\ncausal intervention mechanisms to determine which\nneuron activations",
    "start": "3435720",
    "end": "3443010"
  },
  {
    "text": "most correspond to particular\nfactual predictions and then updating\nthem appropriately. But as you can possibly suspect,\nthis has a lot of weaknesses.",
    "start": "3443010",
    "end": "3451900"
  },
  {
    "text": "So firstly, this works mainly\nfor knowledge-based things or simple facts. What if we want to\nupdate the actual skills",
    "start": "3451900",
    "end": "3458770"
  },
  {
    "text": "or capabilities of a model? We want it to be better\nat math in general. We want it to be better at\nadvanced analogical reasoning,",
    "start": "3458770",
    "end": "3465440"
  },
  {
    "text": "unlike humans. Then something\nlike model editing based on factual predictions\ndoesn't seem like it'll work.",
    "start": "3465440",
    "end": "3472460"
  },
  {
    "text": "The second is these are\ntargeting one fact at a time, so it's not easy to propagate\nthese changes to other nodes",
    "start": "3472460",
    "end": "3480580"
  },
  {
    "text": "based on related facts. For example, let's\nsay someone's mother,",
    "start": "3480580",
    "end": "3488319"
  },
  {
    "text": "we want to update\na fact about them, then we should\nalso update a fact about that person's\nbrother, because they",
    "start": "3488320",
    "end": "3494530"
  },
  {
    "text": "have the same mother. But this approach\nwill only update it for the original\nperson in question,",
    "start": "3494530",
    "end": "3500570"
  },
  {
    "text": "but not any of the relatives. So this is just one example. So there's a lot of other works\nwhich have spun out recently",
    "start": "3500570",
    "end": "3507190"
  },
  {
    "text": "in continual learning, which\nis good that this area has seen more work. So I will very briefly\ndescribe some of these.",
    "start": "3507190",
    "end": "3515690"
  },
  {
    "text": "One is this thing called MEMIT,\nwhich is directly related to what I just said about\nROME, but it's mass editing",
    "start": "3515690",
    "end": "3521905"
  },
  {
    "text": "of factual knowledge. So instead of a simple\nfact or memory at a time, it's able to simultaneously\nmodify thousands, even",
    "start": "3521905",
    "end": "3528740"
  },
  {
    "text": "ones which might be related\nto each other, like I said, which is useful.",
    "start": "3528740",
    "end": "3533870"
  },
  {
    "text": "There's things, that CEM or\nContinue Evolving from Mistakes. So it actually identifies\nthe LLM mistakes",
    "start": "3533870",
    "end": "3539210"
  },
  {
    "text": "somewhat similar to what\nself-improvement Chelsea was talking about but\nincrementally updates the model to self-improve.",
    "start": "3539210",
    "end": "3545396"
  },
  {
    "text": " There's things like\nlifelong mixture of experts.",
    "start": "3545396",
    "end": "3551840"
  },
  {
    "text": "So what it does is instead of\nhaving a simple, fixed mixture of experts architecture, it\ncontinually adds new experts",
    "start": "3551840",
    "end": "3558350"
  },
  {
    "text": "for different domains over time\nwhile freezing potentially past experts which are\nno longer useful",
    "start": "3558350",
    "end": "3564470"
  },
  {
    "text": "or don't need to be\nupdated to avoid things like catastrophic forgetting. So this is a very\nsmart approach.",
    "start": "3564470",
    "end": "3570660"
  },
  {
    "text": "Another is called CLOB. So this enables continual\ntask learning using only prompting without updating\nmodel weights by summarizing",
    "start": "3570660",
    "end": "3577910"
  },
  {
    "text": "past knowledge into a\ncompressed prompt memory. However, not a criticism of\nthis work, but again, this",
    "start": "3577910",
    "end": "3584180"
  },
  {
    "text": "is not technically\nupdating the brain or the fundamental\ncapabilities of the model,",
    "start": "3584180",
    "end": "3589619"
  },
  {
    "text": "so this is more of a\nprompt-only approach. And another one of these is\ncalled progressive prompts,",
    "start": "3589620",
    "end": "3595530"
  },
  {
    "text": "which, again, alerts a soft\nprompt vector for each task and, again, progressively\ncompresses them",
    "start": "3595530",
    "end": "3601700"
  },
  {
    "text": "and composes them together. So allowing LLMs to\ncontinually learn without weight updates or\ncatastrophic forgetting.",
    "start": "3601700",
    "end": "3609859"
  },
  {
    "text": "But again, my opinion is\ntrue continual learning would update the brain or the weights\nof the model in some way,",
    "start": "3609860",
    "end": "3616310"
  },
  {
    "text": "I guess.  So thanks. That's mainly our lecture.",
    "start": "3616310",
    "end": "3622200"
  },
  {
    "text": "So we gave a brief overview of\ntransformers, how they work. Talked about pre-training\nand especially how data is important for that.",
    "start": "3622200",
    "end": "3628890"
  },
  {
    "text": "Various post-training\ntechniques, feedback mechanisms, prompting mechanisms\nlike chain-of-thought,",
    "start": "3628890",
    "end": "3634210"
  },
  {
    "text": "self-improvement, some\napplications to neuroscience, vision, and so forth, and some\nremaining weaknesses like things",
    "start": "3634210",
    "end": "3641100"
  },
  {
    "text": "like the lack of continual\nlearning and data efficiency, being able to scale down and\nrun these models on our phone.",
    "start": "3641100",
    "end": "3647850"
  },
  {
    "text": "So before we send you guys off-- I know we ended a bit early-- so this class going\nforwards, every week,",
    "start": "3647850",
    "end": "3654250"
  },
  {
    "text": "in case you haven't attended,\nwe'll have a speaker, typically from\nindustry or academia, come in to talk about\nthe state-of-the-art work",
    "start": "3654250",
    "end": "3660600"
  },
  {
    "text": "they're doing. And we have a cool\nlineup of speakers prepared for you guys for\nthe remainder of the quarter. And some more logistical things.",
    "start": "3660600",
    "end": "3666940"
  },
  {
    "text": "We'll be posting\nupdates about lectures and so forth on our\nwebsite through the mailing list, Discord, and\nso forth, so please",
    "start": "3666940",
    "end": "3673590"
  },
  {
    "text": "join those if you\nhaven't already. Thank you, guys. Hope you enjoyed\nthe first lecture. And if anybody\nhas any questions, feel free to come\nup and stay around.",
    "start": "3673590",
    "end": "3680500"
  },
  {
    "text": "But thanks. [APPLAUSE] ",
    "start": "3680500",
    "end": "3688000"
  }
]