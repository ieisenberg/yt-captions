[
  {
    "start": "0",
    "end": "135000"
  },
  {
    "text": "All right. So, homework two, you guys are probably starting to work on, and we're having sessions this week that",
    "start": "5360",
    "end": "10860"
  },
  {
    "text": "are good for if you don't have background in deep learning, and feel free to reach out on Piazza. Oh, yeah,",
    "start": "10860",
    "end": "17974"
  },
  {
    "text": "I just have a question about the project. I just want to make sure, it seemed currently with the note on Piazza that like, I-50 was the default suggested one.",
    "start": "17974",
    "end": "23009"
  },
  {
    "text": "Can we also do something outside of that? Oh, yeah, no,   question is a great one.",
    "start": "23010",
    "end": "29160"
  },
  {
    "text": "Yeah, there's a, the post on Piazza, you're always welcome to design your own project. That's always completely fine,",
    "start": "29160",
    "end": "35295"
  },
  {
    "text": "and a number of you have come talk to me about those, or talked to other TAs. These are an additional option. So, if people are interested in looking at either",
    "start": "35295",
    "end": "42710"
  },
  {
    "text": "the default project which we released yesterday, which has to do with bandits and warfarin, or if you want to look at some of the suggestions from senior PhD students or postdocs,",
    "start": "42710",
    "end": "53120"
  },
  {
    "text": "those are great opportunities. Particularly, I think if you haven't ever done reinforcement learning before, it's often I wouldn't expect at three weeks in",
    "start": "53120",
    "end": "59969"
  },
  {
    "text": "that you'd be able to define a state of the art project. So, if you're interested in learning more about RL research,",
    "start": "59970",
    "end": "65630"
  },
  {
    "text": "then it can be a really great opportunity to look at some of those suggested projects then reach out to people. All right.",
    "start": "65630",
    "end": "72005"
  },
  {
    "text": "The other thing that I just wanted to do a friendly reminder about is we explicitly post FAQs for each of the homeworks.",
    "start": "72005",
    "end": "79310"
  },
  {
    "text": "Um, and as some of the TAs are mentioning that some of the students coming into office hours right now might not have had a chance to look at those.",
    "start": "79310",
    "end": "85280"
  },
  {
    "text": "So, if you ever have a question when you're going over the homework, the first thing to do is to go to Piazza and",
    "start": "85280",
    "end": "91100"
  },
  {
    "text": "particularly to look at those pinned notes at the very top which have very common FAQs about the assignment.",
    "start": "91100",
    "end": "97150"
  },
  {
    "text": "So, make sure to read those before you go to office hours, and then, of course, feel free to come to office hours as well.",
    "start": "97150",
    "end": "103609"
  },
  {
    "text": "But those are a really good resource to look at. Any other questions? All right,",
    "start": "103610",
    "end": "111030"
  },
  {
    "text": "so just in terms of where we are in the course right now, we went through DQN on Monday. We're gonna talk today some a bit more about,",
    "start": "111030",
    "end": "118925"
  },
  {
    "text": "we can wrap up some of the stuff that I had to rush through at the end of Monday in terms of deep Q-learning and some of the recent extensions.",
    "start": "118925",
    "end": "125450"
  },
  {
    "text": "Then we're gonna talk some about imitation learning and large state spaces before next week starting to talk about policy gradient methods.",
    "start": "125450",
    "end": "132690"
  },
  {
    "text": "So just to, we'll start off with sort of a refresher from what DQN was doing,",
    "start": "134210",
    "end": "139550"
  },
  {
    "start": "135000",
    "end": "241000"
  },
  {
    "text": "DQN was this idea of combining between Q-learning and using deep neural networks as function approximators.",
    "start": "139550",
    "end": "146944"
  },
  {
    "text": "And the two key sort of algorithmic changes compared to prior work was that,",
    "start": "146945",
    "end": "152185"
  },
  {
    "text": "they used experience replay and fixed Q targets.",
    "start": "152185",
    "end": "158030"
  },
  {
    "text": "And by fixed Q targets there that was meaning that when we used our r, rt plus gamma, max over a,",
    "start": "158030",
    "end": "167269"
  },
  {
    "text": "Q of sta, st plus one, right.",
    "start": "167270",
    "end": "172540"
  },
  {
    "text": "That the weights that were used for that Q representation were fixed for a while.",
    "start": "172540",
    "end": "179060"
  },
  {
    "text": "So maybe we'd update those every 100 steps or every 50 episodes or some interval.",
    "start": "179060",
    "end": "185165"
  },
  {
    "text": "And, uh, so this provided a more stable target for supervised learning because the supervised learning part",
    "start": "185165",
    "end": "192770"
  },
  {
    "text": "again is that we were had this combination of, of we want to have weights and we want to minimize this error versus our current estimate,",
    "start": "192770",
    "end": "200540"
  },
  {
    "text": "sort of minimizing the TD error. So the way that this preceded is that we'd restore transition in a replay memory buffer.",
    "start": "200540",
    "end": "212045"
  },
  {
    "text": "We do mini batches, where we would sample a bunch of state extra word and next state tuples and",
    "start": "212045",
    "end": "217640"
  },
  {
    "text": "then do these backups where we're sort of updating our Q function and refitting our Q function.",
    "start": "217640",
    "end": "223295"
  },
  {
    "text": "Um, and like a lot of the linear value function methods we saw before, it uses stochastic gradient descent.",
    "start": "223295",
    "end": "229325"
  },
  {
    "text": "And the really cool thing about this is that they did it on 50 games. They used the same architecture for those 50 games and",
    "start": "229325",
    "end": "234560"
  },
  {
    "text": "the same hyper parameters and they got human level performance. So we've talked quite a lot about that before.",
    "start": "234560",
    "end": "239915"
  },
  {
    "text": "And then we sort of briefly talked about three sort of major extensions to that in the immediate following years.",
    "start": "239915",
    "end": "246590"
  },
  {
    "start": "241000",
    "end": "256000"
  },
  {
    "text": "And again, there's been a lot of extensions and a lot of work in deep reinforcement learning right now. The three of them were as follows.",
    "start": "246590",
    "end": "254495"
  },
  {
    "text": "The first was Double DQN. And we talked before we got it,",
    "start": "254495",
    "end": "259579"
  },
  {
    "start": "256000",
    "end": "679000"
  },
  {
    "text": "the function approximation talking about the issue with maximization bias, that when you're using the same representation",
    "start": "259580",
    "end": "267875"
  },
  {
    "text": "to pick an action and estimate the value of that action, you can get into a maximization bias problem.",
    "start": "267875",
    "end": "273350"
  },
  {
    "text": "And the way that that's avoided in Double DQN and I wanted to go over this again because I had a couple questions after class.",
    "start": "273350",
    "end": "279020"
  },
  {
    "text": "We didn't have much time to discuss it. Is what happens is we have a current queue network which is",
    "start": "279020",
    "end": "284630"
  },
  {
    "text": "parameterized by a set of weights and that is what is used to select actions. Just to be clear here,",
    "start": "284630",
    "end": "291220"
  },
  {
    "text": "often we're doing some sort of E-greedy method. So we'd used the current Q-network weights to decide on the best action.",
    "start": "291220",
    "end": "297590"
  },
  {
    "text": "And we would pick that with one minus epsilon probability. And then there's an older Q-network that is used to evaluate those actions.",
    "start": "297590",
    "end": "304520"
  },
  {
    "text": "So if we look at how we're gonna be changing our weights, we're gonna be having an action evaluation using these other weights,",
    "start": "304520",
    "end": "314480"
  },
  {
    "text": "w minus and then action selection using W. So when you look at this,",
    "start": "314480",
    "end": "320870"
  },
  {
    "text": "that might start to look pretty similar to what DQN was doing because DQN was saying we're gonna use a fixed set of weights for,",
    "start": "320870",
    "end": "329240"
  },
  {
    "text": "for these target updates. So what DQN was doing was this,",
    "start": "329240",
    "end": "334295"
  },
  {
    "text": "r plus Gamma Q. I'll write the max in, max a,",
    "start": "334295",
    "end": "346905"
  },
  {
    "text": "Q of s prime, a, w minus,",
    "start": "346905",
    "end": "353355"
  },
  {
    "text": "minus the current s. So in the normal DQN,",
    "start": "353355",
    "end": "360600"
  },
  {
    "text": "they were also using a w minus. But here in a Double DQN,",
    "start": "360600",
    "end": "367380"
  },
  {
    "text": "it can be a little bit different. And the reason it's a little bit different than what we just saw is that you can maintain two sets of",
    "start": "367380",
    "end": "373430"
  },
  {
    "text": "weights at all times and you can flip between them on every step or every batch. So when DQN was introduced,",
    "start": "373430",
    "end": "380480"
  },
  {
    "text": "it was more of an idea of you fix your weights. Let's say, from time step t to time step t plus 100,",
    "start": "380480",
    "end": "387410"
  },
  {
    "text": "use the same weights that whole time period for your target. In Double DQN, you don't necessarily have to do that.",
    "start": "387410",
    "end": "393409"
  },
  {
    "text": "You can flip back and forth between these which is what we'd seen with a double Q-learning that on,",
    "start": "393410",
    "end": "399870"
  },
  {
    "text": "you know, on step one you can use weights one to act and weights two to evaluate. On step two, you could do weights two to evaluate and weights one to act.",
    "start": "399870",
    "end": "407419"
  },
  {
    "text": "So it means that you can propagate information faster. So instead of waiting 50 episodes or 100 episodes to update,",
    "start": "407420",
    "end": "414530"
  },
  {
    "text": "um, the weights that you're using for your target, so again this is your target,",
    "start": "414530",
    "end": "419640"
  },
  {
    "text": "you can flip back and forth between them which allows you to update both networks a lot have,",
    "start": "420500",
    "end": "425675"
  },
  {
    "text": "update both set of net- network weights. The networks are identical. Yeah. Um, in general, when you're evaluating",
    "start": "425675",
    "end": "432229"
  },
  {
    "text": "these kinds of different approaches to improve these techniques, is there, is there a trade off between",
    "start": "432230",
    "end": "438259"
  },
  {
    "text": "how fast information propagates and then how unstable it is? So we might find that if the system we're trying to learn on is itself,",
    "start": "438260",
    "end": "445970"
  },
  {
    "text": "relatively well-behaved and stable, we want to pick something that has faster information propagation but if it's highly noisy or unstable that we need to do something that's more conservative.",
    "start": "445970",
    "end": "454775"
  },
  {
    "text": "uh, makes a good question which is, you know, is there generally a trade-off in terms of these methods between sort of",
    "start": "454775",
    "end": "460940"
  },
  {
    "text": "characterizing the stability of the system and then how fast you can propagate information back? Unfortunately, I feel like it's not very well characterized.",
    "start": "460940",
    "end": "467914"
  },
  {
    "text": "So I feel like most of the time, these are heuristics and people evaluate them, they evaluate them with a lot of different benchmarks",
    "start": "467915",
    "end": "474180"
  },
  {
    "text": "and that's sort of the way we get generalization. But I don't think that there's a good characterization systematically of how to characterize the stability of the system,",
    "start": "474180",
    "end": "482255"
  },
  {
    "text": "with these deep neural networks, particularly in the context of RL. So there's a lot of great opportunities for",
    "start": "482255",
    "end": "487670"
  },
  {
    "text": "theoretical analysis here too or just sort of more formal understanding. Right now, I think we're at the level of saying this either just seems to consistently work a bunch across",
    "start": "487670",
    "end": "494600"
  },
  {
    "text": "Atari games and maybe MuJoCo or it doesn't try to characterize the, the successes. Yeah. Is it",
    "start": "494600",
    "end": "502010"
  },
  {
    "text": "Yes. I was wondering if we kind of get a bit more about the switching then. You're representing like why, how.",
    "start": "502010",
    "end": "510835"
  },
  {
    "text": "Yeah. So, question is about, you know, how can we switch between these w and w minus,",
    "start": "510835",
    "end": "516070"
  },
  {
    "text": "and how would, you know, um, why and how would you do this? So, in the DQN setting,",
    "start": "516070",
    "end": "521905"
  },
  {
    "text": "um, you could set w minus. So at the beginning, w minus is equal to w on time step zero.",
    "start": "521905",
    "end": "528280"
  },
  {
    "text": "And then, in DQN you would keep w minus to be the same maybe for the next 50 episodes,",
    "start": "528280",
    "end": "533395"
  },
  {
    "text": "but you'd be updating w. And then 50 episodes in, you would update w minus. The downside about that which we talked a little bit about before is that",
    "start": "533395",
    "end": "542395"
  },
  {
    "text": "you're not using the information you're getting to update this estimate.",
    "start": "542395",
    "end": "547525"
  },
  {
    "text": "Okay. Because you're using that old stale set of w's. So essentially, you're just not using the information you've got over",
    "start": "547525",
    "end": "553660"
  },
  {
    "text": "those 50 episodes to update what would happen if you were caught in S prime and then took action a.",
    "start": "553660",
    "end": "558774"
  },
  {
    "text": "So, an alternative would be to flip between, let's say, instead of thinking of this as w and w minus, then you can think of it that way.",
    "start": "558775",
    "end": "566829"
  },
  {
    "text": "You can just think of maintaining two different sets of weights. And imagine, um, I'll say,",
    "start": "566830",
    "end": "572485"
  },
  {
    "text": "this is t time equals one, time equals two, time equals three. So, imagine that we're just picking between what are the weights that we used",
    "start": "572485",
    "end": "580480"
  },
  {
    "text": "to select an action and the weights that we use to evaluate the action. So, on the first time step, you could use this to evaluate and this to the- to select the action,",
    "start": "580480",
    "end": "589014"
  },
  {
    "text": "and then you could flip it back and forth. So that essentially means that,",
    "start": "589015",
    "end": "595045"
  },
  {
    "text": "both sets of weights are getting updated very frequently. So, instead of updating only one of the- one of them every 50 episodes,",
    "start": "595045",
    "end": "601405"
  },
  {
    "text": "you're- you're continuing to propagate that information back quickly. And there's of course tons of chart choices here about how frequently do you update,",
    "start": "601405",
    "end": "609190"
  },
  {
    "text": "you know, when do you switch back and forth between these. Um, you can think of all of those as hyper-parameters you can imagine tuning.",
    "start": "609190",
    "end": "615070"
  },
  {
    "text": "But this is instead of keeping that- keeping this target fixed for 50 steps, um, or, you know,",
    "start": "615070",
    "end": "621055"
  },
  {
    "text": "n steps these are all parameters, you could flip back and forth between them which is what double Q-learning did before. Yeah, .",
    "start": "621055",
    "end": "627655"
  },
  {
    "text": "Like in the normal DQN settings when we're using a target weight, uh, wouldn't that target weight like for action selection or",
    "start": "627655",
    "end": "634750"
  },
  {
    "text": "for like evaluation still be another queuing network. So, how is that was different from double DQN",
    "start": "634750",
    "end": "640990"
  },
  {
    "text": "except for the fact that you're searching double-Q more often? It is. Or more of the- what question is, how different is this from the previous year? It's almost identical.",
    "start": "640990",
    "end": "648415"
  },
  {
    "text": "So, I think the- the main difference here is that you could switch, uh, as long as you're maintaining some set of weights for your target.",
    "start": "648415",
    "end": "655165"
  },
  {
    "text": "This is saying you could sort of switch. Now you really just have- you've the same network, two sets of weights that you have to maintain in memory.",
    "start": "655165",
    "end": "660879"
  },
  {
    "text": "And what this is saying is, you can switch back and forth with those very frequently, um, and help avoid the maximization bias during that time.",
    "start": "660880",
    "end": "668590"
  },
  {
    "text": "It doesn't always work, it frequently helps. There is still the issue with stability, um, but it can be better and it avoids the maximization bias.",
    "start": "668590",
    "end": "676760"
  },
  {
    "text": "We also talked about prioritized experience replay. Um, we went through a small sort of tabular example where we looked at the impact of doing backups.",
    "start": "677010",
    "end": "685089"
  },
  {
    "start": "679000",
    "end": "740000"
  },
  {
    "text": "So, if we have this experience replay buffer of SAR S-prime tuples, which one should we use to do our backups and how do we propagate that information back?",
    "start": "685090",
    "end": "693760"
  },
  {
    "text": "Um, and the- in this algorithm, they say the- or in this paper, they talked about the fact, um,",
    "start": "693760",
    "end": "700060"
  },
  {
    "text": "if you can do this optimally. In some cases you might get an exponential speedup and convergence, uh, but it's hard to do that, it's computationally intensive.",
    "start": "700060",
    "end": "706839"
  },
  {
    "text": "So, what they proposed here is to prioritize something based on the size of sort of the DQN error.",
    "start": "706840",
    "end": "713709"
  },
  {
    "text": "The difference between the current estimate of it and your sort of target estimate that you're looking at.",
    "start": "713710",
    "end": "721329"
  },
  {
    "text": "And so, we talked about how you could use that as a priority, um, and it could be a stochastic priority,",
    "start": "721330",
    "end": "726490"
  },
  {
    "text": "uh, to try to select items. And we also talked about the fact that if you set Alpha equal to zero,",
    "start": "726490",
    "end": "732040"
  },
  {
    "text": "this becomes uniform and so then, there's no particular prioritization over your tuples.",
    "start": "732040",
    "end": "737260"
  },
  {
    "text": "Another thing that we had almost no time to talk about was dueling. So, dueling was a Best Paper, um, from,",
    "start": "737260",
    "end": "743380"
  },
  {
    "start": "740000",
    "end": "973000"
  },
  {
    "text": "uh, two- 2016, um, in ICML. Um, let me just give a little bit of",
    "start": "743380",
    "end": "749605"
  },
  {
    "text": "a refresher on this because we went through it very, very fast. So, the- the intuition here is that the features that you might need to write down the value of",
    "start": "749605",
    "end": "757149"
  },
  {
    "text": "a state might be different than those need to specify the relative benefit of different actions in that state.",
    "start": "757150",
    "end": "762970"
  },
  {
    "text": "And you want to understand the relative benefit of actions in order to decide what, what your policy should be.",
    "start": "762970",
    "end": "769524"
  },
  {
    "text": "So, um, looking at things like game score, it's obviously very relevant to the value.",
    "start": "769525",
    "end": "775465"
  },
  {
    "text": "Um, that it might be- you might want other features try to decide what actions to do right now in a game. And so, the advantage function that came up,",
    "start": "775465",
    "end": "782560"
  },
  {
    "text": "uh, that was designed by Baird a long time ago. And this is the same Baird that had that counter example to",
    "start": "782560",
    "end": "787839"
  },
  {
    "text": "show why value function approximation can be bad. Um, so, uh, Baird's work before it said,",
    "start": "787840",
    "end": "794200"
  },
  {
    "text": "well, look you can decompose, um, if you think of your Q function which is representing the value of a policy starting in",
    "start": "794200",
    "end": "800920"
  },
  {
    "text": "state and taking a particular action versus the value of just that state. So, this is sort of implicitly Q pi,",
    "start": "800920",
    "end": "808825"
  },
  {
    "text": "S pi of S. So, like what is the difference between- difference between taking",
    "start": "808825",
    "end": "815350"
  },
  {
    "text": "this particular action versus just following your policy from the current state? And he called this the advantage.",
    "start": "815350",
    "end": "820764"
  },
  {
    "text": "What's the advantage of that action for that state?",
    "start": "820764",
    "end": "824990"
  },
  {
    "text": "So, in dueling DQN, instead of having one network that just predicts Q functions,",
    "start": "827660",
    "end": "834825"
  },
  {
    "text": "they use an architecture that separates into predicting values and predicting these advantage functions and then",
    "start": "834825",
    "end": "840704"
  },
  {
    "text": "adds them back together with the idea being that you might get different sort of features here and here.",
    "start": "840705",
    "end": "847945"
  },
  {
    "text": "So, you have to decouple for a little bit to make sure that you're capturing the features that are relevant to capturing",
    "start": "847945",
    "end": "853030"
  },
  {
    "text": "the salient things you want to look at for Q's. Now, one thing that I mentioned very briefly last time is that,",
    "start": "853030",
    "end": "861685"
  },
  {
    "text": "um, is the- is the advantage function identifiable? And what do I mean by that in this case?",
    "start": "861685",
    "end": "867625"
  },
  {
    "text": "I mean that if you have a Q function which is what ultimately we're going to use, um, can we decompose it into a unique a pi and v pi.",
    "start": "867625",
    "end": "876025"
  },
  {
    "text": "So, here ultimately we want a cube. And the question is, if we then in our",
    "start": "876025",
    "end": "881080"
  },
  {
    "text": "architecture decomposing this into a value and an advantage, is there a unique way to do that? Is there? Um, but there isn't.",
    "start": "881080",
    "end": "888610"
  },
  {
    "text": "So, if you- if you add a constant to both Q and V,",
    "start": "888610",
    "end": "897654"
  },
  {
    "text": "um, then you can get the same advantage function.",
    "start": "897655",
    "end": "903070"
  },
  {
    "text": "So, there's not a unique, you can always shift your um, shift your awards by a constant and that's not going to change your policy,",
    "start": "903070",
    "end": "909714"
  },
  {
    "text": "it will change your value function. Um, I- so, there's lots of different ways to decompose your advantage function and your values,",
    "start": "909715",
    "end": "917425"
  },
  {
    "text": "it's not a unique decomposition. So, the way that they defined it there is to say, well,",
    "start": "917425",
    "end": "924370"
  },
  {
    "text": "let's force the advantage for state and action to be zero if A is the action taken.",
    "start": "924370",
    "end": "931255"
  },
  {
    "text": "So, here they compare it to the action that's taken if you're using sort of say, a greedy approach.",
    "start": "931255",
    "end": "939894"
  },
  {
    "text": "Um, and this is really just a way to- all of this we can think of it in some ways as an analogy to supervised learning.",
    "start": "939895",
    "end": "948385"
  },
  {
    "text": "And so, we want to have a stable target and we want to be able to learn these advantage functions and these value functions if we have lots and lots of data about them.",
    "start": "948385",
    "end": "956110"
  },
  {
    "text": "And so, this is sort of choosing a particular fixed point for how to define the advantage function. And then, they also said, well,",
    "start": "956110",
    "end": "962140"
  },
  {
    "text": "empirically you could just use the mean too. So, you could just average over your advantage functions, it's more of just a heuristic approach.",
    "start": "962140",
    "end": "970370"
  },
  {
    "text": "And what they find, again, so, we sort of we're layering up these additional techniques. We started with DQN, then we thought about adding, um,",
    "start": "970620",
    "end": "978670"
  },
  {
    "start": "973000",
    "end": "1059000"
  },
  {
    "text": "double-Q learning to DQN and then we thought about adding prioritized replay. And then this is dueling.",
    "start": "978670",
    "end": "984024"
  },
  {
    "text": "And what they find is  dueling versus double DQN with prioritized replay is a lot better most of the time.",
    "start": "984025",
    "end": "989650"
  },
  {
    "text": "Now, let me see if I can find Montezuma's. Yep. So, for Montezuma's this new method is basically no better.",
    "start": "989650",
    "end": "998335"
  },
  {
    "text": "Like none of these methods are really tackling hard exploration problems. But they are doing better ways of sort of propagating information",
    "start": "998335",
    "end": "1004949"
  },
  {
    "text": "in the network and trying to change the way we're training the network. Yeah, questions about that, and name first please. Can you speak a little bit louder,",
    "start": "1004950",
    "end": "1011850"
  },
  {
    "text": "I'm unable to hear you well. Okay. I'll try to speak a little bit louder. Can- can people in the back over there hear me or is it just him?",
    "start": "1011850",
    "end": "1020170"
  },
  {
    "text": "Okay. Good. All right. So, these were three of the methods that ended up making a big difference.",
    "start": "1020170",
    "end": "1025650"
  },
  {
    "text": "We talked very briefly about practical tips. Um, I won't go in these too much. The main thing is just that we try to actively encourage you to",
    "start": "1025650",
    "end": "1032640"
  },
  {
    "text": "build up your acuity representation first before you try on Atari. Um, you can try different forms of losses.",
    "start": "1032640",
    "end": "1038880"
  },
  {
    "text": "Uh, learning rate is important, but in this case, in our assignment we're going to be using",
    "start": "1038880",
    "end": "1044039"
  },
  {
    "text": "the Adam optimizer which means you don't have to worry too much about it. There's a issue of sort of trying different exploration schemes,",
    "start": "1044040",
    "end": "1051770"
  },
  {
    "text": "is something that we're going to talk about later in this class. So, for right now we're still thinking about just simple E-greedy approaches.",
    "start": "1051770",
    "end": "1058620"
  },
  {
    "text": "Um, a nice paper that came out, I think it was start of 2018, um,",
    "start": "1058810",
    "end": "1064605"
  },
  {
    "start": "1059000",
    "end": "1196000"
  },
  {
    "text": "was Rainbow which was a paper that basically just tried to combine a whole bunch of these recent methods,",
    "start": "1064605",
    "end": "1069945"
  },
  {
    "text": "um, to see really how big of an improvement do you get. Uh, now again, note in this case and we'll come back to this in just a couple of slides.",
    "start": "1069945",
    "end": "1077130"
  },
  {
    "text": "This is a lot of data for a lot of experience in the world, 200 million frames of experience.",
    "start": "1077130",
    "end": "1082725"
  },
  {
    "text": "But they developed an algorithm called Rainbow that combines a lot of the things we've just been talking about, double DQN, prioritized, and dueling,",
    "start": "1082725",
    "end": "1090795"
  },
  {
    "text": "as well as some other recent advances. Um, noisy is one that also tries to do some different forms of exploration.",
    "start": "1090795",
    "end": "1098085"
  },
  {
    "text": "And so, they found that kind of by adding these improvements together, then you could get a significant improvement.",
    "start": "1098085",
    "end": "1103720"
  },
  {
    "text": "I think this is a useful insight because often it's not clear whether or not these different gains are additive or if they're just, um,",
    "start": "1103720",
    "end": "1109865"
  },
  {
    "text": "sort of, um, you're, you're- they're kind of doing the same thing but maybe in a slightly different way.",
    "start": "1109865",
    "end": "1115515"
  },
  {
    "text": "And so, it's nice to see that in some of these cases these different sort of ideas are additive in terms of the resulting performance gain.",
    "start": "1115515",
    "end": "1121035"
  },
  {
    "text": "Um, these aren't sort of- this is still a very large amount of data. [NOISE]",
    "start": "1121035",
    "end": "1128580"
  },
  {
    "text": "Okay. So just to summarize which we're wrapping up where we are with model-free, ah, deep neural networks for RL right now.",
    "start": "1128580",
    "end": "1134669"
  },
  {
    "text": "Uh, they're very expressive function approximators. Uh, you should be able to understand how you represent the Q function,",
    "start": "1134670",
    "end": "1141000"
  },
  {
    "text": "and you could do some Monte Carlo-based methods or TD style methods. Um, and- and at this point,",
    "start": "1141000",
    "end": "1146760"
  },
  {
    "text": "it's sort of good to make sure you understand how you would do that with tabular methods, with linear value function methods,",
    "start": "1146760",
    "end": "1151980"
  },
  {
    "text": "and with deep neural networks. So it's sort of, algorithmically, it looks very similar across all of those but then you- in some cases,",
    "start": "1151980",
    "end": "1158850"
  },
  {
    "text": "you have to do this step of doing function approximation and in other cases you don't. Um, and then it'd be good to just make sure you can sort of list",
    "start": "1158850",
    "end": "1165690"
  },
  {
    "text": "a few extensions that help beyond DQN um, and why they do. All right.",
    "start": "1165690",
    "end": "1172274"
  },
  {
    "text": "So now let's go back to our, um, sort of, high level, uh, view of what we want from the reinforcement learning algorithms,",
    "start": "1172275",
    "end": "1178424"
  },
  {
    "text": "these are algorithms that are sort of doing optimization, handling generalization, ah, doing exploration and doing it at all statistically and computationally efficiently.",
    "start": "1178425",
    "end": "1186900"
  },
  {
    "text": "And we've just been spending quite a lot of time on looking at generalization as well as optimization. Um, but we haven't talked very much about efficiency.",
    "start": "1186900",
    "end": "1195310"
  },
  {
    "start": "1196000",
    "end": "1287000"
  },
  {
    "text": "So one of the challenges is- is, that, um, if you want to define efficiency formally like in terms of",
    "start": "1196030",
    "end": "1202520"
  },
  {
    "text": "how much data an agent lead, needs to learn to make a good decision. Um, there are hardness results, ah,",
    "start": "1202520",
    "end": "1208055"
  },
  {
    "text": "that- that are known so our lab has developed some, uh, lower bounds, other people have too. Um, uh, I think we now have",
    "start": "1208055",
    "end": "1214700"
  },
  {
    "text": "basically tight upper and lower bounds for the tabular MDP case, um, which indicate that there's some really pathological MDPs",
    "start": "1214700",
    "end": "1221940"
  },
  {
    "text": "out there for which we just need a lot of data, lot of data, though, you know, would not scale very well as you start to go up to really huge domains.",
    "start": "1221940",
    "end": "1228299"
  },
  {
    "text": "So some of these problems are really hard to do. Formerly, you would just need a lot of exploration. You can do something much better than E greedy but we'll talk about that soon.",
    "start": "1228300",
    "end": "1236130"
  },
  {
    "text": "But even when we do those much better things than E Greedy we can prove that it's still really hard to learn in those, we still might need a lot of data.",
    "start": "1236130",
    "end": "1242490"
  },
  {
    "text": "So an alternative is to say well there's lots of other supervision that we could have in the world to try to learn how to do things.",
    "start": "1242490",
    "end": "1248775"
  },
  {
    "text": "Um, and so how can we use that additional information in order to sort of speed reinforcement learning.",
    "start": "1248775",
    "end": "1254355"
  },
  {
    "text": "And so what we're going to do, is talk about some- about imitation learning today. And then we're going to start talking about policy search and policy gradient methods next week.",
    "start": "1254355",
    "end": "1261820"
  },
  {
    "text": "And those, you can also think of as another different way to impose structure, um, because in policy gradient methods,",
    "start": "1261820",
    "end": "1268669"
  },
  {
    "text": "you always have to define your policy class. Sometimes that can be a really rich policy class,",
    "start": "1268670",
    "end": "1274200"
  },
  {
    "text": "and so maybe that's not too much of a limitation, um, but other times, you're encoding domain knowledge by the class that you- that you represent.",
    "start": "1274200",
    "end": "1282430"
  },
  {
    "text": "Okay. And in particular, we're going to be thinking about imitation learning and large state spaces which is exactly the place where you might hope to benefit from,",
    "start": "1284390",
    "end": "1291629"
  },
  {
    "start": "1287000",
    "end": "1296000"
  },
  {
    "text": "um, additional help or supervision. So if we think about something like Montezuma's Revenge, um,",
    "start": "1291630",
    "end": "1299370"
  },
  {
    "start": "1296000",
    "end": "1412000"
  },
  {
    "text": "there's some nice work on looking at sort of how far did DQN get in this case.",
    "start": "1299370",
    "end": "1304934"
  },
  {
    "text": "So Montezuma's Revenge, for those of you who haven't played, it is sort of a, um, a long- very long horizon,",
    "start": "1304935",
    "end": "1310650"
  },
  {
    "text": "uh, game in which you're sort of trying to navigate through this world, and like pick up keys and make decisions.",
    "start": "1310650",
    "end": "1316800"
  },
  {
    "text": "Um, and it involves a lot of different rooms. So you can see here what the outline of the,",
    "start": "1316800",
    "end": "1322215"
  },
  {
    "text": "all the white squares are basically rooms. And on the left-hand side, um, sort of a DQN that was trained for 50 million frames,",
    "start": "1322215",
    "end": "1330225"
  },
  {
    "text": "um, only gets through the first two rooms. Like, it's just doing very badly. It's not making very much progress.",
    "start": "1330225",
    "end": "1337080"
  },
  {
    "text": "Um, whereas on the right-hand side, we see something which is explicitly trying to explore.",
    "start": "1337080",
    "end": "1342480"
  },
  {
    "text": "Um, and it uses some of the techniques that we'll be talking more about later. But notice that it still doesn't get all the way through the game.",
    "start": "1342480",
    "end": "1348765"
  },
  {
    "text": "Um, and so I think this sort of illustrates the fact that some of these games are really hard. Um, there has been some really nice additional progress since, um, ah,",
    "start": "1348765",
    "end": "1357315"
  },
  {
    "text": "both, ah, with me and Percy Liang's lab we now can basically solve Montezuma's. Um, and there's also been some really nice work from Uber AI lab on solving Montezuma's.",
    "start": "1357315",
    "end": "1366645"
  },
  {
    "text": "But a lot of the places that people originally got traction on this was by starting to use imitation learning and demonstrations.",
    "start": "1366645",
    "end": "1373274"
  },
  {
    "text": "Um, so in particular, if we think about cases where RL might work well, RL works,",
    "start": "1373275",
    "end": "1379620"
  },
  {
    "text": "you know, pretty well when it's easy or certainly we've seen a lot of success. So far when data is cheap and parallelization is easy,",
    "start": "1379620",
    "end": "1386309"
  },
  {
    "text": "and it can be much harder to use the methods that we've talked about so far when data is expensive, um, and when maybe failure is not tolerable.",
    "start": "1386310",
    "end": "1394184"
  },
  {
    "text": "So , if you tried to use the methods that we just described to learn to fly like a remote control helicopter.",
    "start": "1394185",
    "end": "1400679"
  },
  {
    "text": "Typically require a lot of helicopters [LAUGHTER] because it would be very expensive. And so there's many cases where this type of,",
    "start": "1400680",
    "end": "1407565"
  },
  {
    "text": "um, performance is just not gonna be practical. So one of the benefits is that if you can give the agent a lot of rewards,",
    "start": "1407565",
    "end": "1416009"
  },
  {
    "start": "1412000",
    "end": "1528000"
  },
  {
    "text": "you can shape behavior pretty quickly. Um, so one of the challenges in Montezuma's Revenge is that reward is very sparse that,",
    "start": "1416010",
    "end": "1422280"
  },
  {
    "text": "you know, the agent has to try lots of different things before it gets any signal of whether it's doing the right thing. Um, and where do these rewards come from?",
    "start": "1422280",
    "end": "1429525"
  },
  {
    "text": "I think it's generally it's actually a really deep question. Um, but for right now, let's think about sort of just even the challenge of specifying rewards.",
    "start": "1429525",
    "end": "1437100"
  },
  {
    "text": "Um, so if you manually design them, that might be pretty brittle depending on the task. Um, and an alternative is just to demonstrate.",
    "start": "1437100",
    "end": "1445395"
  },
  {
    "text": "So if you had to write down the reward function for driving a car, it's quite com- complicated, like you don't want to hit roads,",
    "start": "1445395",
    "end": "1450960"
  },
  {
    "text": "here or hit the ro- hit, um, people. You don't wanna, um, drive off the road. You want to get to your destination.",
    "start": "1450960",
    "end": "1456059"
  },
  {
    "text": "And so it's a  very complicated reward function to write down. But it's pretty easy for most of us to just drive to a destination and show an example of maybe an optimal behavior.",
    "start": "1456060",
    "end": "1464790"
  },
  {
    "text": "So that's sort of the idea behind learning from demonstrations. Um, there's been lots and lots of work on this but since people",
    "start": "1464790",
    "end": "1473070"
  },
  {
    "text": "started thinking about learning from demonstrations or imitation learning. I would argue probably this was started roughly 20 years ago,",
    "start": "1473070",
    "end": "1480660"
  },
  {
    "text": "around 1999, 2000 was a paper which started to think about learning rewards from demonstration.",
    "start": "1480660",
    "end": "1486255"
  },
  {
    "text": "Um, but then there's been lots of applications to it since. So thinking about it for things like highway driving, um, or navigation, or parking lot navigation.",
    "start": "1486255",
    "end": "1494010"
  },
  {
    "text": "There's a lot of these cases particularly in driving right now, but, ah, where people have been thinking and- and robotics too.",
    "start": "1494010",
    "end": "1499845"
  },
  {
    "text": "To think about how do you do, um, demonstrations of like how to pick up a cup or things like that to try to teach robots how to do those tasks.",
    "start": "1499845",
    "end": "1507195"
  },
  {
    "text": "Um, there's also some really interesting questions too about like, you know, how do you, uh,",
    "start": "1507195",
    "end": "1512385"
  },
  {
    "text": "do things like path planning or goal inference, and again these sorts of cases where it's quite complicated to write down a reward function directly or it might be brittle.",
    "start": "1512385",
    "end": "1520065"
  },
  {
    "text": "And the problem with brittle reward functions is that your agent will optimize to that and it may not be the behavior that you wanted.",
    "start": "1520065",
    "end": "1527560"
  },
  {
    "text": "So- so the setting from learning from demonstrations, and- and today I'm going to be somewhat informal",
    "start": "1527570",
    "end": "1533160"
  },
  {
    "start": "1528000",
    "end": "1583000"
  },
  {
    "text": "about whether I call things learning from demonstrations, um, there's also inverse RL. And there's also imitation learning.",
    "start": "1533160",
    "end": "1541299"
  },
  {
    "text": "And there are sort of differences but a lot of these things are somewhat interchangeable. Most of this is about the idea of saying that you're- you have some demonstration data,",
    "start": "1545450",
    "end": "1553860"
  },
  {
    "text": "and then you're going to use it, uh, in order to help boot- either bootstrap or completely learn a new policy.",
    "start": "1553860",
    "end": "1559620"
  },
  {
    "text": "So the idea is that you might get an expert and maybe they're a perfect expert or maybe they're a pretty good expert to provide some demonstration trajectories of,",
    "start": "1559620",
    "end": "1567135"
  },
  {
    "text": "um, taking actions, um, in states. And in many cases,",
    "start": "1567135",
    "end": "1572565"
  },
  {
    "text": "it'll be easier for people to do this but it's useful to think about when it's easier to specify one or the other,",
    "start": "1572565",
    "end": "1578055"
  },
  {
    "text": "and what situations are- are common for each. So what's the problem setup?",
    "start": "1578055",
    "end": "1584130"
  },
  {
    "start": "1583000",
    "end": "1783000"
  },
  {
    "text": "The problem setup is that we have this state space and action space. Um, some transition model that is typically unknown,",
    "start": "1584130",
    "end": "1590850"
  },
  {
    "text": "no reward function and instead sort of a set of teachers demonstrations from some particular we assume for now optimal policy.",
    "start": "1590850",
    "end": "1599055"
  },
  {
    "text": "Um, and the behavior cloning we're gonna say, how do we directly learn the teacher's policy?",
    "start": "1599055",
    "end": "1604980"
  },
  {
    "text": "So how do we match, how do we get sort of an approximation of pi star directly from those demonstrations?",
    "start": "1604980",
    "end": "1612960"
  },
  {
    "text": "Inverse RL is typically about saying, how can we recover the reward function? Once we have the reward function,",
    "start": "1612960",
    "end": "1618659"
  },
  {
    "text": "then we can use it to compute a policy. Uh, and that often- that last step often is combined with the apprenticeship learning.",
    "start": "1618660",
    "end": "1626445"
  },
  {
    "text": "So that we're both trying to get that reward function and then actually generate a good policy with that. In some cases, you might just want the reward function, um,",
    "start": "1626445",
    "end": "1635085"
  },
  {
    "text": "can anybody think of a case where you might be interested in just the reward function, maybe you don't want to recover the policy,",
    "start": "1635085",
    "end": "1640830"
  },
  {
    "text": "but you're just curious about what the reward function is of another agent. Okay.",
    "start": "1640830",
    "end": "1648000"
  },
  {
    "text": "If you're trying to understand, say, [inaudible]",
    "start": "1648000",
    "end": "1655250"
  },
  {
    "text": "Yeah, how the environment [inaudible]. Yeah,  I think is a great example.",
    "start": "1655250",
    "end": "1661970"
  },
  {
    "text": "So, in a lot of, um, uh, science, you know, biology et cetera you often want to understand the behavior of organisms or animals or things like that.",
    "start": "1661970",
    "end": "1668930"
  },
  {
    "text": "And so, if you can just look at their behavior, you could say track monkeys or things like that and then use that to back solve,",
    "start": "1668930",
    "end": "1675665"
  },
  {
    "text": "like, what is their reward function. What are the- the goals or preferences. Um, I think there's a number of cases where that's useful,",
    "start": "1675665",
    "end": "1682160"
  },
  {
    "text": "and maybe down the line, you know, maybe there's some optimization that'll happen but- but generally often there it's just about understanding,",
    "start": "1682160",
    "end": "1687380"
  },
  {
    "text": "like, what is the goal structure or what is the preference structure of- of the organism or individual. That could happen with people too that you might want to understand, like,",
    "start": "1687380",
    "end": "1694325"
  },
  {
    "text": "the choices people are making in terms of nav- you know, um, uh, commuting or in terms of buying preferences or things like that.",
    "start": "1694325",
    "end": "1701675"
  },
  {
    "text": "Maybe later you want to optimize for that but also you're just curious about how- how do people's behavior reveal the,",
    "start": "1701675",
    "end": "1708289"
  },
  {
    "text": "sort of, um, an underlying reward structure, underlying preference model. Yeah, Imitation learning just using the teacher's demonstration set,",
    "start": "1708290",
    "end": "1716225"
  },
  {
    "text": "like, an upper bound, I guess, or, uh, have there been cases such that, yes,",
    "start": "1716225",
    "end": "1721415"
  },
  {
    "text": "that, like, agent learns to perform better than the actor did. Like we find a new path side.",
    "start": "1721415",
    "end": "1727070"
  },
  {
    "text": "Yeah, asked a nice question of, like, is the expert's behavior an upper bound or are there cases also where the agent can go beyond this?",
    "start": "1727070",
    "end": "1735095"
  },
  {
    "text": "We're not gonna talk too much about this today, but there's a lot of work right now on combining imitation learning with RL.",
    "start": "1735095",
    "end": "1740480"
  },
  {
    "text": "So, um, uh, there's a lot of work on say like inverse RL plus RL.",
    "start": "1740480",
    "end": "1746525"
  },
  {
    "text": "Where for example, you might use this to s- bootstrap the system, um, and then your agent would continue to learn on top of this.",
    "start": "1746525",
    "end": "1752870"
  },
  {
    "text": "There's also some nice work from Pieter Abbeel's Group, um, where they looked at assuming that the expert was providing like a noisy demonstration of",
    "start": "1752870",
    "end": "1759560"
  },
  {
    "text": "an optimal path and then the goal is to learn the optimal path not the noisy demonstration of it. So, often you do want to go beyond the expert.",
    "start": "1759560",
    "end": "1766610"
  },
  {
    "text": "There's limitations to that, and we'll talk about that in a second actually. What- what are some of the limitations that you might have if",
    "start": "1766610",
    "end": "1772550"
  },
  {
    "text": "you don't get to continue to gather data in the new environment.",
    "start": "1772550",
    "end": "1776250"
  },
  {
    "text": "Okay, so let's start with behavioral cloning which is probably the simplest one, um, because essentially in behavioral cloning we're",
    "start": "1778330",
    "end": "1785540"
  },
  {
    "start": "1783000",
    "end": "2146000"
  },
  {
    "text": "just gonna treat this as a standard supervised learning problem. So, we're going to fix a policy class which means, sort of,",
    "start": "1785540",
    "end": "1791330"
  },
  {
    "text": "some way to represent, um, our mapping from states to actions. And this could be a deep neural network.",
    "start": "1791330",
    "end": "1797110"
  },
  {
    "text": "It could be a decision tree, could be lots of different things. Um, and then we're just going to estimate a policy from the training examples.",
    "start": "1797110",
    "end": "1803985"
  },
  {
    "text": "So, we're just gonna say, we saw all these times. We saw a state and an action from our expert and that's just our input output for our supervised learning model.",
    "start": "1803985",
    "end": "1811385"
  },
  {
    "text": "And we're just gonna learn a mapping from states to actions. And early on, so this has been around, uh,",
    "start": "1811385",
    "end": "1817715"
  },
  {
    "text": "really for quite a long time and I, uh, should have said more like 30 years. Um, there were some nice examples of doing this.",
    "start": "1817715",
    "end": "1824390"
  },
  {
    "text": "So, ALVINN, um, was a very early, uh, paper, uh, and system about thinking about, uh, driving on the road.",
    "start": "1824390",
    "end": "1831800"
  },
  {
    "text": "See, it was a neural network and it was trained, uh, at least in part using behavioral cloning or supervised learning to imitate trajectories.",
    "start": "1831800",
    "end": "1840929"
  },
  {
    "text": "Um, okay, so let's think about why this might go wrong. Um, and to first,",
    "start": "1841900",
    "end": "1848240"
  },
  {
    "text": "let's think about what happens in supervised learning. So, in supervised learning, um, we're gonna assume iid pairs s,",
    "start": "1848240",
    "end": "1854330"
  },
  {
    "text": "a and we're gonna ig- ignore the temporal structure. So, we're gonna- if we're just doing supervised learning in general,",
    "start": "1854330",
    "end": "1860000"
  },
  {
    "text": "we just imagine that we have these state-action pairs and then maybe we learn some classifier or,",
    "start": "1860000",
    "end": "1865580"
  },
  {
    "text": "um, yeah, let's just say a classifier, to classify what action, you know, we should do. And it might have some sort of errors.",
    "start": "1865580",
    "end": "1872345"
  },
  {
    "text": "It might have errors of, uh, well, you know, with probability epsilon. And so if we were thinking about doing this over the course",
    "start": "1872345",
    "end": "1879019"
  },
  {
    "text": "of T time steps than we might have, you know, sort of, an expected total number of errors of epsilon times T. So,",
    "start": "1879020",
    "end": "1887555"
  },
  {
    "text": "let's just take a second and think about what goes wrong when we're doing this in the supervised learning or in the- in, uh, in the RL context.",
    "start": "1887555",
    "end": "1893660"
  },
  {
    "text": "So, by the RL context, I mean, the fact that the decisions that we make influence the next state.",
    "start": "1893660",
    "end": "1899030"
  },
  {
    "text": "So, let's just take, like, one minute maybe talk to your neighbor and say, like, what do you think could be the problem with",
    "start": "1899030",
    "end": "1904100"
  },
  {
    "text": "behavioral cloning in these sorts of scenarios. And if, uh, that's a simple thing to",
    "start": "1904100",
    "end": "1909410"
  },
  {
    "text": "think about then maybe think about how you would address it. So, what you might do in that case if there is",
    "start": "1909410",
    "end": "1914600"
  },
  {
    "text": "problems that happen when we try to apply standard supervised learning to this case where it's really underlying, uh, an MDP.",
    "start": "1914600",
    "end": "1922730"
  },
  {
    "text": "[OVERLAPPING].",
    "start": "1922730",
    "end": "1981500"
  },
  {
    "text": "All right. So, first of all let's just make a guess. I'm going to ask you guys whether you think the, um,",
    "start": "1981500",
    "end": "1986524"
  },
  {
    "text": "the total expected errors if we're doing this in- in where the underlying world is an MDP, is gonna be greater than or less than the number of errors that",
    "start": "1986525",
    "end": "1994190"
  },
  {
    "text": "we'd expect according to a supervised learning approach. Um, so who thinks that we're going to have greater expected total errors?",
    "start": "1994190",
    "end": "2001880"
  },
  {
    "text": "Okay. Who think we're gonna have less? Who- many people must be confused.",
    "start": "2002040",
    "end": "2009159"
  },
  {
    "text": "[LAUGHTER] Okay. So, how about somebody who thinks the answer is that we're gonna have greater.",
    "start": "2009160",
    "end": "2014200"
  },
  {
    "text": "Maybe somebody who thinks that's the case could say why they think we might have more errors if the real world is MDP and we've tried to do",
    "start": "2014200",
    "end": "2021010"
  },
  {
    "text": "the supervised learning technique. Yeah, and name first, please. My idea is that kind of, like,",
    "start": "2021010",
    "end": "2026740"
  },
  {
    "text": "uh, as a human you're planning a more long-term horizon or, like,",
    "start": "2026740",
    "end": "2031900"
  },
  {
    "text": "you're doing one step and then you know how that action is gonna then give you, like, another sequence, but since we've",
    "start": "2031900",
    "end": "2038500"
  },
  {
    "text": "just had taken a state and action and then, like, predicting. Right from there we can't plan that long-term sequence,",
    "start": "2038500",
    "end": "2044860"
  },
  {
    "text": "so it's gonna, like, compound our errors as we go. That's right. So, when says that, correct, we will compound those errors,",
    "start": "2044860",
    "end": "2051609"
  },
  {
    "text": "and one of the- the challenging aspects of this is that the errors can compound a lot. Um, and this is because the distribution of states that",
    "start": "2051610",
    "end": "2059290"
  },
  {
    "text": "you get can- depends on the actions that you take. So, if you think about this like a navigation case, like,",
    "start": "2059290",
    "end": "2065050"
  },
  {
    "text": "if I was supposed to go out the right-hand door, um, and I watched go out this door and I saw that he went right.",
    "start": "2065050",
    "end": "2071620"
  },
  {
    "text": "And I- and I, you know, tried to learn a supervised learning, uh, classifier for what I should do here. But my supervised learner was a little bit broken,",
    "start": "2071620",
    "end": "2078909"
  },
  {
    "text": "and so instead of going right here, I actually went left. Well, now I'm in part of the room which never went",
    "start": "2078910",
    "end": "2084610"
  },
  {
    "text": "to because he was going over there to go to the door. And so, like, now I have no idea what to do here. All right, like, now I'm in the state distribution,",
    "start": "2084610",
    "end": "2093369"
  },
  {
    "text": "it's something that I haven't seen before, it's very likely that I'm going to make an error. In fact, my probability of error now may not be- my probability of error here is",
    "start": "2093370",
    "end": "2102190"
  },
  {
    "text": "under-assuming the fact that the data that you get in the future is the same distribution as the data you got in the past.",
    "start": "2102190",
    "end": "2107740"
  },
  {
    "text": "Our supervised learning guarantee is generally safe when we have them. Um, uh, that I- if- if your data is- comes from an iid distribution,",
    "start": "2107740",
    "end": "2116904"
  },
  {
    "text": "then in the future this is what your test error will be. The problem is, is that, in reinforcement learning or markup decision processes,",
    "start": "2116905",
    "end": "2123475"
  },
  {
    "text": "your actions depe- determine what is the data you're gonna see. So, the fact that instead of following the right action here,",
    "start": "2123475",
    "end": "2129775"
  },
  {
    "text": "um, I went over here. And now I have no data and my data distribution is different,",
    "start": "2129775",
    "end": "2135025"
  },
  {
    "text": "so now there's no guarantees from my supervised learning algorithm because my data is different. It's never been trained on anything like that so we can't generalize.",
    "start": "2135025",
    "end": "2142585"
  },
  {
    "text": "So that's exactly the problem. Um, and this was noted by, like, Drew Bagnell's group from CMU in 2011 of arguing that,",
    "start": "2142585",
    "end": "2150430"
  },
  {
    "start": "2146000",
    "end": "2293000"
  },
  {
    "text": "you know, this is a really big problem for what's called behavioral cloning. So, even though there had been some nice empirical demonstrations of it,",
    "start": "2150430",
    "end": "2156865"
  },
  {
    "text": "uh, he and his former students, Stephane Ross indicated why this might fundamentally be a very big problem an- and sort of ill- uh,",
    "start": "2156865",
    "end": "2164620"
  },
  {
    "text": "demonstrated some things that people had sometimes seen empirically. The idea is that as soon as you deviate, so this is the time where you make your mistake,",
    "start": "2164620",
    "end": "2171370"
  },
  {
    "text": "then essentially the whole rest of the trajectory might all be errors. You might make T more- T more errors.",
    "start": "2171370",
    "end": "2177010"
  },
  {
    "text": "Um, and so that means that the total number of errors that you make is not expected, uh, uh epsilon times T but it's epsilon times T squared, it's much worse.",
    "start": "2177010",
    "end": "2186970"
  },
  {
    "text": "And it's really due to these compounding errors leading to- you to",
    "start": "2186970",
    "end": "2192010"
  },
  {
    "text": "a place where your distribution of states is very different than what you have data about. And this issue will come up again, again,",
    "start": "2192010",
    "end": "2198430"
  },
  {
    "text": "this sort of equiv- this thinking about what is the distribution of states that you get, um, under, you know, the policy you're",
    "start": "2198430",
    "end": "2204970"
  },
  {
    "text": "following versus the policy that you want to follow. That issue comes up again, and again in- in reinforcement learning.",
    "start": "2204970",
    "end": "2210190"
  },
  {
    "text": "So, um, it really is just a foundational issue that, you know,",
    "start": "2210190",
    "end": "2215310"
  },
  {
    "text": "what is the data distribution you're gonna get under the policy that you've learned versus the true policy and looking at this mismatch.",
    "start": "2215310",
    "end": "2223345"
  },
  {
    "text": "And so once you go off the racecourse you're not gonna have any data about that. So, one of the ideas, uh,",
    "start": "2223345",
    "end": "2229540"
  },
  {
    "text": "that Drew Bagnell and his students came up with to think about this was to say, well, what if we could get more data?",
    "start": "2229540",
    "end": "2236650"
  },
  {
    "text": "So, what if when I, you know, my- I- I only have a little amount of data to start with. I've learned my per- my supervised learning policy to say,",
    "start": "2236650",
    "end": "2244645"
  },
  {
    "text": "you know, what shall I do in each state, and sometimes I make a mistake. So, sometimes, you know, I- I go out that way,",
    "start": "2244645",
    "end": "2249880"
  },
  {
    "text": "my race car drives off the racecourse. What if I could know what to do in that, in that state?",
    "start": "2249880",
    "end": "2255910"
  },
  {
    "text": "So, I reached a state that I don't have any data about, what if I could ask my expert, hey, what should I do now. So, like, I go over here and I'm like,",
    "start": "2255910",
    "end": "2262315"
  },
  {
    "text": "oh my god I don't know, you know, what to do now. And then you ask your expert,  and they're like, oh just turn right. It's fine, you can still get out of the right door, it's okay.",
    "start": "2262315",
    "end": "2269755"
  },
  {
    "text": "Um, so if you could ask your expert for labels, well, now you're getting labels about states that you are encountering.",
    "start": "2269755",
    "end": "2276790"
  },
  {
    "text": "And as long as all the- as long as you have data that covers all the states that you're gonna potentially experience,",
    "start": "2276790",
    "end": "2283315"
  },
  {
    "text": "then your- then your supervised learning should do pretty well. But the bigger issue tends to come up with the fact that you are",
    "start": "2283315",
    "end": "2288460"
  },
  {
    "text": "encountering states that you don't have any coverage of in your training data. So, the idea of DAGGER which is",
    "start": "2288460",
    "end": "2294250"
  },
  {
    "start": "2293000",
    "end": "2586000"
  },
  {
    "text": "a data set aggregation is essentially you just keep growing your data set. So, what happens in this case is you start off, you don't have any data.",
    "start": "2294250",
    "end": "2301494"
  },
  {
    "text": "You initialize your policy. You follow your policy. So, in this case we're gonna assume that you have an expert,",
    "start": "2301495",
    "end": "2307494"
  },
  {
    "text": "um, some expert policy. So, that might be, you know, an expert is taking some steps and then",
    "start": "2307495",
    "end": "2316430"
  },
  {
    "text": "also your other policy you can- you project a trajectory. So, sometimes you're following one policy sentence you're at.",
    "start": "2316430",
    "end": "2322610"
  },
  {
    "text": "And, ah, and then what you get is you get this state. You get to go ask your expert for every single state,",
    "start": "2322610",
    "end": "2330470"
  },
  {
    "text": "what would you have done here? So for every state that you encountered in that trajectory.",
    "start": "2330470",
    "end": "2337250"
  },
  {
    "text": "And then, you add all of those tuples to your dataset and you train your supervised learning policy on that.",
    "start": "2337250",
    "end": "2344525"
  },
  {
    "text": "So, everything inside of your dataset that you're using to train your policy on, is with an expert label and you're slowly growing the size of that dataset.",
    "start": "2344525",
    "end": "2354800"
  },
  {
    "text": "So, the idea is as you're getting more and more experts, ah, more and more labels of expert actions across the the trajectories you've actually seen.",
    "start": "2354800",
    "end": "2362540"
  },
  {
    "text": "And there's nice formal properties for this. So you can be guaranteed that you will converge to a good policy, um, ah,",
    "start": "2362540",
    "end": "2368615"
  },
  {
    "text": "by following this, um, under the induced state distribution. Yeah, .",
    "start": "2368615",
    "end": "2373865"
  },
  {
    "text": "Just to confirm, this is assuming we have pi star over all space, like when we're doing the case where we don't have the expert to go to.",
    "start": "2373865",
    "end": "2380600"
  },
  {
    "text": "Yeah, no this is a great question, so when I was looking at this just now, I would have to double-check. I think this is assuming your expert can give you the action there.",
    "start": "2380600",
    "end": "2386795"
  },
  {
    "text": "It doesn't assume that you have explicit access to pi star. Because if you had explicit access to pi star,  you wouldn't need to learn anything.",
    "start": "2386795",
    "end": "2393155"
  },
  {
    "text": "So, I think in this case it's like tossing a coin about whether the expert just directly gives you",
    "start": "2393155",
    "end": "2398599"
  },
  {
    "text": "the action in that case or whether you follow the other policy. Which because you have to have the expert around all the time anyway,",
    "start": "2398600",
    "end": "2405155"
  },
  {
    "text": "because they're always going to have to eventually tell you what they would have done there. Get- that's how you, excuse me, that dataset.",
    "start": "2405155",
    "end": "2411965"
  },
  {
    "text": "So, that's- there was one. I'm and, um, I'm curious about how is this done efficiently?",
    "start": "2411965",
    "end": "2420140"
  },
  {
    "text": "I can imagine that for some situations having a person set the command line while your, your GPU trains and inputs actions won't be efficient.",
    "start": "2420140",
    "end": "2431120"
  },
  {
    "text": "How do people generally do this in such a way that it doesn't require manual intervention?",
    "start": "2431120",
    "end": "2436895"
  },
  {
    "text": "Yeah, 's question is a great one which is, um, you know, well, this requires you have an expert either around for",
    "start": "2436895",
    "end": "2443000"
  },
  {
    "text": "every single step or like at the end of the trajectory that can go back and label everything. And that's incredibly expensive. And if you're doing this for, you know,",
    "start": "2443000",
    "end": "2449060"
  },
  {
    "text": "millions and millions of time steps that's completely intractable. I think that's why, um, this, this line of research has been less influential in certain ways than the,",
    "start": "2449060",
    "end": "2458965"
  },
  {
    "text": "some of the other techniques that we're going to see next in terms of how you do sort of inverse RL.",
    "start": "2458965",
    "end": "2464635"
  },
  {
    "text": "So, what this is really assuming is that you have this human in the loop that's really in the loop, um, ah, as opposed to just asking them to",
    "start": "2464635",
    "end": "2471500"
  },
  {
    "text": "provide demonstrations once and then your algorithm goes off. And I think that practically in most cases it's much more realistic to say,",
    "start": "2471500",
    "end": "2478625"
  },
  {
    "text": "you know, drive the car around the block 10 times, but then you can leave and then we'll do all of our RL versus saying I need you to be in the car or,",
    "start": "2478625",
    "end": "2485900"
  },
  {
    "text": "you know, like, label all of the trajectories that the car is doing and keep telling you whether it's right or wrong. I think this is just very label-intensive. It's very expensive.",
    "start": "2485900",
    "end": "2492380"
  },
  {
    "text": "So I think that, um, in some limited cases, like, if your action rate is very slow,",
    "start": "2492380",
    "end": "2498680"
  },
  {
    "text": "like, if your action rate is, you know, making decisions in the military or you know, others that are at",
    "start": "2498680",
    "end": "2503750"
  },
  {
    "text": "a very high level, with very sparse decisions, this can be very reasonable. Because you could basically throw, you know, infinite compute at it before, between each decision-making.",
    "start": "2503750",
    "end": "2511700"
  },
  {
    "text": "If you're doing this for sort of real-time hertz-level decisions, I think that's very hard. Okay, yeah, .",
    "start": "2511700",
    "end": "2520400"
  },
  {
    "text": "Will this be compatible with like an expert taking over the system. Right, like, somebody sitting behind the wheel letting an agent drive and then,",
    "start": "2520400",
    "end": "2528380"
  },
  {
    "text": "uh, like, recognizing that there's an emergency situation coming up and taking the wheel. Yeah so, like, um, so what said,",
    "start": "2528380",
    "end": "2535115"
  },
  {
    "text": "is this compatible with sort of a- an expert taking over? Yes. I mean I think. And that might be an easier way to get labels.",
    "start": "2535115",
    "end": "2541430"
  },
  {
    "text": "So you might say if you have an expert there, every action that's taken that's the same as the action the expert would take. Maybe they don't intervene. Otherwise, they only provide",
    "start": "2541430",
    "end": "2547910"
  },
  {
    "text": "labels or interventions when it would differ. But it still requires like an expert to be monitoring,",
    "start": "2547910",
    "end": "2553385"
  },
  {
    "text": "which is often still mentally challenging essentially. You know, it's still high-cost, okay.",
    "start": "2553385",
    "end": "2560130"
  },
  {
    "text": "All right. So that- that is a nice. I mean there- it's very nice to see sort of the formal characterization",
    "start": "2560920",
    "end": "2566869"
  },
  {
    "text": "of why behavioral cloning can be bad and what is the reason for this. Um, and I think that DAGGER is can be very useful in certain circumstances,",
    "start": "2566869",
    "end": "2575510"
  },
  {
    "text": "but there's a lot of cases where just practically it's much easier to get a- example demonstrations, um, and then assume that there's no longer a human in the loop.",
    "start": "2575510",
    "end": "2583175"
  },
  {
    "text": "All right. So inverse RL is more of one of the- the second categories.",
    "start": "2583175",
    "end": "2588230"
  },
  {
    "start": "2586000",
    "end": "3238000"
  },
  {
    "text": "So, what does- what happens in inverse RL? Well, first let's just think about you know, feature-based reward function.",
    "start": "2588230",
    "end": "2594365"
  },
  {
    "text": "So, well okay, we'll get to there in a second. So again, we're thinking about this case where we",
    "start": "2594365",
    "end": "2599930"
  },
  {
    "text": "have some transition model that we might not observe. Um, a- or maybe we're doing. A lot of the techniques here,",
    "start": "2599930",
    "end": "2606005"
  },
  {
    "text": "to start with, they didn't assume that you knew the transition model. [NOISE] That's pretty strong for a lot of real-world domains,",
    "start": "2606005",
    "end": "2611780"
  },
  {
    "text": "but in some cases that's reasonable. So, for right now, we're going to assume that the only thing that we don't know is the reward function.",
    "start": "2611780",
    "end": "2617674"
  },
  {
    "text": "There's some extensions to when you don't know the transition model too. Okay. So, then we have again our set of demonstrations and the goal",
    "start": "2617675",
    "end": "2624950"
  },
  {
    "text": "now is not to directly learn a policy but just to infer the reward function. So, if I don't tell you anything about the optimality of the teacher's policy,",
    "start": "2624950",
    "end": "2634955"
  },
  {
    "text": "what can we infer about the reward function? Like let's just say, like its not an expert, it's just demonstrations.",
    "start": "2634955",
    "end": "2641910"
  },
  {
    "text": "If you get a demonstration of a state, action state et cetera.",
    "start": "2642220",
    "end": "2647690"
  },
  {
    "text": "Can you infer anything about R? If you don't know anything about the optimality? [NOISE]",
    "start": "2647690",
    "end": "2663980"
  },
  {
    "text": "Like, I mean, would it be the same as samples, like as we get more demonstrations, R will approach like, R star, I guess.",
    "start": "2663980",
    "end": "2670130"
  },
  {
    "text": "This is assuming no assumptions about optimality. So, if I don't tell you anything about the optimality of the policy you're seeing,",
    "start": "2670130",
    "end": "2678079"
  },
  {
    "text": "is there any or any information you can gather in that case? I'd be able to say that the choice that the teacher made under",
    "start": "2678080",
    "end": "2685940"
  },
  {
    "text": "their policy just wrapped hot air under the reward each other function that the alternatives.",
    "start": "2685940",
    "end": "2692255"
  },
  {
    "text": "So is saying, for that particular person you could say something about, um, for their reward function,",
    "start": "2692255",
    "end": "2699365"
  },
  {
    "text": "assuming that they're a rational agent, that that- that was, um, higher in their own function. That's true. But if, if you wanted it to be about the general word function, um,",
    "start": "2699365",
    "end": "2707585"
  },
  {
    "text": "this would tell you maybe I'm understating it so that it seems a little bit more subtle than I mean it to be.",
    "start": "2707585",
    "end": "2714724"
  },
  {
    "text": "It doesn't tell you anything, right. Like if you- if I- if you see me like wandering around and like if you see an agent flailing around,",
    "start": "2714725",
    "end": "2721400"
  },
  {
    "text": "right, and and you know nothing about whether it's making good decisions or not with respect to the true reward function.",
    "start": "2721400",
    "end": "2726995"
  },
  {
    "text": "Demonstrations don't tell you anything, like they don't give you any information about the reward function unless you know something about, um.",
    "start": "2726995",
    "end": "2733670"
  },
  {
    "text": "Unless you're trying to make an assumption that the agent is acting rationally with respect to the true reward function.",
    "start": "2733670",
    "end": "2739595"
  },
  {
    "text": "Or maybe you get some information about their internal one like what was saying. But in general, if we don't make any assumptions about agent behavior and we don't, um,",
    "start": "2739595",
    "end": "2748100"
  },
  {
    "text": "and we don't assume that they're doing anything optimal with respect to the global reward function, there's no information you can get.",
    "start": "2748100",
    "end": "2754430"
  },
  {
    "text": "Now, the more challenging one is the next. So, let's assume that the teacher's policy is optimal with respect to",
    "start": "2754430",
    "end": "2759980"
  },
  {
    "text": "the true global reward function which the agent is also maybe going to want to optimize in the future. So, you get expert driver perfo- driver, um, driving around.",
    "start": "2759980",
    "end": "2768995"
  },
  {
    "text": "Um, this is like, think for a second about what you can infer about the reward function in this case.",
    "start": "2768995",
    "end": "2778520"
  },
  {
    "text": "Um, and whether in particular there's more than one reward function that could explain their behavior.",
    "start": "2778520",
    "end": "2785105"
  },
  {
    "text": "So think about whether it's unique. So let's say, imagine data has no issue. I give you 10 trillion examples of-",
    "start": "2785105",
    "end": "2791750"
  },
  {
    "text": "of the agent following the optimal policy. 10 trillion examples. Um, and you want to see if",
    "start": "2791750",
    "end": "2799369"
  },
  {
    "text": "you could learn what the reward function is and the question is, um, is there a single reward function that is consistent with their data or are there many?",
    "start": "2799370",
    "end": "2809375"
  },
  {
    "text": "Maybe take a second to talk to somebody around you, um, just to see whether there's,",
    "start": "2809375",
    "end": "2815075"
  },
  {
    "text": "the question really is there one reward function? Is there a unique reward function? If you have infinite data so it's not a data sparsity issue, um, versus money.",
    "start": "2815075",
    "end": "2823369"
  },
  {
    "text": "[OVERLAPPING]",
    "start": "2823370",
    "end": "2881210"
  },
  {
    "text": "All right, I'm going to do a quick poll. Okay, we're going to- I'm going to poll you guys then I'm going to ask whether you think there",
    "start": "2881210",
    "end": "2887300"
  },
  {
    "text": "is one reward function or if there's more than one reward function. So, who thinks there's a single reward function?",
    "start": "2887300",
    "end": "2893465"
  },
  {
    "text": "Infinite data, single reward function that's consistent. Who thinks there's more than one reward function?",
    "start": "2893465",
    "end": "2898970"
  },
  {
    "text": "Okay. Could someone give me a reward function that is always consistent with any optimal policy?",
    "start": "2898970",
    "end": "2907109"
  },
  {
    "text": "I guess that's what we need to do is in the first step just get the older apart that",
    "start": "2910640",
    "end": "2918769"
  },
  {
    "text": "the feature is going to have as a policy and then just be random after that.",
    "start": "2918770",
    "end": "2925310"
  },
  {
    "text": "is saying maybe on the first step, that you could maybe give most of the reward that, like, the agent would experience and then be random after that.",
    "start": "2925310",
    "end": "2932030"
  },
  {
    "text": "So, and that might depend on the state. I guess I was thinking of, if- can anybody tell me like, um, a number which would allow or, you know,",
    "start": "2932030",
    "end": "2940820"
  },
  {
    "text": "specification of a reward function like a constant, like a choice of a constant which would make any policy optimal. Yeah,",
    "start": "2940820",
    "end": "2950779"
  },
  {
    "text": ". You just give a reward of zero for every action. Sorry, yes. So, what says is exactly correct.",
    "start": "2950779",
    "end": "2956089"
  },
  {
    "text": "If you give a reward of zero, um, any policy is optimal in the respect that you'd never get any reward. Anywhere, it's a sad life unfortunately.",
    "start": "2956090",
    "end": "2962255"
  },
  {
    "text": "And, um, uh, in this case, all policies are optimal. Right. So, uh, so if you just observe trajectories,",
    "start": "2962255",
    "end": "2969829"
  },
  {
    "text": "then one reward function for which that policy is optimal at zero but it's not unique.",
    "start": "2969830",
    "end": "2975965"
  },
  {
    "text": "So, um, this issue was observed, I think it was by Andrew Ng and Stuart Russell back in 2000.",
    "start": "2975965",
    "end": "2987119"
  },
  {
    "text": "Right there is a paper talking about inverse RL where they noted this issue.",
    "start": "2987120",
    "end": "2992300"
  },
  {
    "text": "The problem is that this is, uh, not unique without further assumptions, there are many reward functions that are consistent.",
    "start": "2992300",
    "end": "2999275"
  },
  {
    "text": "Um, so, we have to- we're gonna have to think about",
    "start": "2999275",
    "end": "3006040"
  },
  {
    "text": "how do we break ties and how do we impose additional structure. Yeah in the back. If you have a consistent reward function,",
    "start": "3006040",
    "end": "3011920"
  },
  {
    "text": "for instance, if you add a constant and what are the rewards also? They're loss, oh, remind me of your name.",
    "start": "3011920",
    "end": "3019375"
  },
  {
    "text": ". . So, yeah what said is, there's, there's many many reward functions. So, if you have, um, a constant, er,",
    "start": "3019375",
    "end": "3026410"
  },
  {
    "text": "everything has the same, uh, any constant also be identical.",
    "start": "3026410",
    "end": "3031855"
  },
  {
    "text": "So, um, there's generally many different, um, reward functions that would all give you.",
    "start": "3031855",
    "end": "3038244"
  },
  {
    "text": "There are many different reward functions for which any policy is optimal. Instead, that would mean that if you're trying to",
    "start": "3038245",
    "end": "3044400"
  },
  {
    "text": "infer what function given some data there are many reward functions that you can write down so that that data would be optimal with respect to the reward function.",
    "start": "3044400",
    "end": "3051390"
  },
  {
    "text": "[NOISE] And, and that second part is really what we're trying to get as we're trying to sort of, uh, infer what reward function would make this data look like it's,",
    "start": "3051390",
    "end": "3062470"
  },
  {
    "text": "um, coming from an optimal policy, if we assume that the expert is optimal.",
    "start": "3062470",
    "end": "3067940"
  },
  {
    "text": "So, let's think about also how we do this in, um, enlarged state spaces. So, we're gonna think about linear value function approximators, um, because, again,",
    "start": "3068310",
    "end": "3076675"
  },
  {
    "text": "often the places where we particularly need to be sample efficient is when our state space is huge and we're not gonna be able to explore it efficiently.",
    "start": "3076675",
    "end": "3082300"
  },
  {
    "text": "So, let's think about linear value function approximator. And we're gonna think of this reward also as being linear over the features.",
    "start": "3082300",
    "end": "3089395"
  },
  {
    "text": "So, our reward function might be some weights times some featurized representation of our state space.",
    "start": "3089395",
    "end": "3096599"
  },
  {
    "text": "And the goal is to compute a good set of weights given our demonstration.",
    "start": "3096600",
    "end": "3101725"
  },
  {
    "text": "I already said that in general, this is not unique but, um, we're gonna try to figure out ways to do this in,",
    "start": "3101725",
    "end": "3107275"
  },
  {
    "text": "in different, um, methods. So, the value function for a policy Pi can be expressed as the follows- following,",
    "start": "3107275",
    "end": "3114790"
  },
  {
    "text": "and you just write it down as, uh, the expected value of the discounted sum of rewards,",
    "start": "3114790",
    "end": "3119845"
  },
  {
    "text": "and this is the states that you would reach under that policy. Under the distribution of states that you",
    "start": "3119845",
    "end": "3127059"
  },
  {
    "text": "get to under this policy, these are their words. So, now what we're gonna do is re-express this.",
    "start": "3127060",
    "end": "3132835"
  },
  {
    "text": "So, what we're doing now is that this is gonna be- we're assuming a linear representation of our reward function.",
    "start": "3132835",
    "end": "3143560"
  },
  {
    "text": "So, we can write or we can re-express it like this.",
    "start": "3143560",
    "end": "3148430"
  },
  {
    "text": "So, we can write it down in terms of the features of the state we reach at each time step times the weight.",
    "start": "3148650",
    "end": "3156160"
  },
  {
    "text": "And then because the weight vector is constant for everything, you can just move it out. And then you get this interesting expression which is you basically just have",
    "start": "3156160",
    "end": "3164770"
  },
  {
    "text": "this discounted sum of the state features that you encounter. And we're gonna call that Mu.",
    "start": "3164770",
    "end": "3171210"
  },
  {
    "text": "So we talked about this very briefly earlier, but, um, now we're talking about Mu as being sort of the discounted weighted frequency of state features under our policy.",
    "start": "3171210",
    "end": "3179645"
  },
  {
    "text": "How much time do you spend, um, uh, in different features, um, or you know that basically how much time you spend in",
    "start": "3179645",
    "end": "3186190"
  },
  {
    "text": "different states, sort of a featurized version of that, um, discounted by kind of when you reach those because some states you might reach really far in the future versus now.",
    "start": "3186190",
    "end": "3194065"
  },
  {
    "text": "So, it's related to the sort of stationary distributions we were talking about before, but now we're using discounting.",
    "start": "3194065",
    "end": "3200780"
  },
  {
    "text": "So, why is this good? Okay. So what, er, we're gonna say now is",
    "start": "3202170",
    "end": "3207820"
  },
  {
    "text": "that instead of thinking directly about reward functions, um, then we can start to think about distributions of states.",
    "start": "3207820",
    "end": "3215049"
  },
  {
    "text": "Um, and think about sort of the probability of, of reaching different distributions of states,",
    "start": "3215050",
    "end": "3220674"
  },
  {
    "text": "different state distributions, um, as representing different, uh, different policies essentially.",
    "start": "3220675",
    "end": "3227155"
  },
  {
    "text": "Different policies, um, for a particular reward function, um, would reach different distributions of states.",
    "start": "3227155",
    "end": "3233125"
  },
  {
    "text": "So, we can think about using this formulation for apprenticeship learning. So, in this case,",
    "start": "3233125",
    "end": "3239575"
  },
  {
    "start": "3238000",
    "end": "3402000"
  },
  {
    "text": "we have this nice setting for apprenticeship learning. Right now, um, we're using the linear value function approximation we call",
    "start": "3239575",
    "end": "3246250"
  },
  {
    "text": "it apprenticeship learning because we're learning like the agent is being an apprenticeship from, uh, from the, from the demonstrator.",
    "start": "3246250",
    "end": "3252700"
  },
  {
    "text": "So, now we have this discounted weighted frequency of the feature states. So, we're always sort of moving into the feature space of states now.",
    "start": "3252700",
    "end": "3260440"
  },
  {
    "text": "Um, and then we wanna note the following. So, if we define what is the value function for Pi star,",
    "start": "3260440",
    "end": "3270025"
  },
  {
    "text": "that's just equal to the expected discounted sum of rewards we reach. And by definition, that is better than the value for any other policy.",
    "start": "3270025",
    "end": "3279295"
  },
  {
    "text": "At least as good because either Pi is the same as Pi is the same as the optimal policy or it's different,",
    "start": "3279295",
    "end": "3284365"
  },
  {
    "text": "and this is just equal to the same thing, um, same reward function which we don't know,",
    "start": "3284365",
    "end": "3290995"
  },
  {
    "text": "but under a different distribution of states. It's under the distribution of states you'd get to if you follow this alternative policy.",
    "start": "3290995",
    "end": "3297980"
  },
  {
    "text": "And so, if we think that the expert's demonstrations are from the optimal policy,",
    "start": "3297990",
    "end": "3303280"
  },
  {
    "text": "in order to identify W, it's sufficient to find the W star such that, if you dot-product that with",
    "start": "3303280",
    "end": "3310240"
  },
  {
    "text": "the distribution of states you got to under the optimal policy. Remember, this is what we know,",
    "start": "3310240",
    "end": "3315744"
  },
  {
    "text": "these are- we can get this from our demonstrations. This has to look better than the distri- than",
    "start": "3315745",
    "end": "3323890"
  },
  {
    "text": "the same weight vector times the distribution of states you get to under any other policy.",
    "start": "3323890",
    "end": "3330019"
  },
  {
    "text": "Are there questions about that? So, it's by making this observation that the value of the optimal policy is directly",
    "start": "3331470",
    "end": "3338110"
  },
  {
    "text": "related to the distribution of states you get under it times this weight vector. And that value has to be higher than the value of, uh,",
    "start": "3338110",
    "end": "3347065"
  },
  {
    "text": "any other policy which is using that same weight vector and this gives you a different distribution of states. Yes, .",
    "start": "3347065",
    "end": "3354010"
  },
  {
    "text": "[inaudible] U in this case is sort of like the stationary distribution of",
    "start": "3354010",
    "end": "3371050"
  },
  {
    "text": "the proportions of its refugees state gave the policy. [NOISE] Yeah. Especially in serv- in terms of conceptualizing Mu.",
    "start": "3371050",
    "end": "3377410"
  },
  {
    "text": "What's the sort of a good way to think about it is it should we think of it as like the stationary distribution of states? Yeah. I think it's reasonable as everything that is",
    "start": "3377410",
    "end": "3383500"
  },
  {
    "text": "essentially the stationary distribution of states weighted with this discount factor on top of it.",
    "start": "3383500",
    "end": "3389150"
  },
  {
    "text": "So, it's very similar to the stationary distributions we saw before.",
    "start": "3389820",
    "end": "3395300"
  },
  {
    "text": "All right. So, essentially it's the same, we want to find a reward function so that the expert policy and the distribution of states",
    "start": "3400670",
    "end": "3406830"
  },
  {
    "start": "3402000",
    "end": "4404000"
  },
  {
    "text": "you reach under it looks better when you compute the value function compared to, um, that same weight vector under any other distribution of states.",
    "start": "3406830",
    "end": "3415170"
  },
  {
    "text": "Um, and so if we can find a policy so",
    "start": "3415170",
    "end": "3420390"
  },
  {
    "text": "that its distribution of states matches the distribution of states of our expert,",
    "start": "3420390",
    "end": "3425805"
  },
  {
    "text": "then we're gonna do pretty well. So, what this says here is that if we have",
    "start": "3425805",
    "end": "3430860"
  },
  {
    "text": "a policy so the dis-discounted distribution of states that we reach under it is close to",
    "start": "3430860",
    "end": "3436770"
  },
  {
    "text": "the distribution of states that you got from your demonstration, since it's the expert.",
    "start": "3436770",
    "end": "3441430"
  },
  {
    "text": "So, if that's small then your value function error will also be small.",
    "start": "3441950",
    "end": "3450010"
  },
  {
    "text": "So, for, for any w if you can- if you can basically match features,",
    "start": "3451520",
    "end": "3457140"
  },
  {
    "text": "match feature- expected features or matched distributions of states, then you're good. Then, then you found, um, er,",
    "start": "3457140",
    "end": "3464190"
  },
  {
    "text": "found a value that's going to have a very similar value to the true value, okay.",
    "start": "3464190",
    "end": "3469240"
  },
  {
    "text": "So, this actually means for any w_t here. So, it means no matter what the true reward function is,",
    "start": "3470110",
    "end": "3476960"
  },
  {
    "text": "if you can find a policy so that your, uh, state features match, then no matter what",
    "start": "3476960",
    "end": "3483180"
  },
  {
    "text": "the true reward function is you know that you're going to be close to the true value. Yeah, . So, this, uh, this w that we will- we will",
    "start": "3483180",
    "end": "3492300"
  },
  {
    "text": "be finding would be used to calculate I guess, an expectation. What your, I guess,",
    "start": "3492300",
    "end": "3498735"
  },
  {
    "text": "values or state. Right? Yes. You could- once you have a w you combine that with your mu's to compute,",
    "start": "3498735",
    "end": "3506475"
  },
  {
    "text": "like, a value of a state or you can sum over it. How do you, uh, I guess,",
    "start": "3506475",
    "end": "3511725"
  },
  {
    "text": "that translates directly to, being able to use that to make decisions when they're out of state?",
    "start": "3511725",
    "end": "3517170"
  },
  {
    "text": "So, I think, I think question is about saying, like, okay, so if we're getting these w's or, sort of, what are we solving for?",
    "start": "3517170",
    "end": "3522690"
  },
  {
    "text": "Are we solving for the policy, are we solving for w et cetera. In this case, I think, uh, a reasonable way to think about it is,",
    "start": "3522690",
    "end": "3528675"
  },
  {
    "text": "um, solving for w if it's solving for pi. So, what this is saying is that let's say you're optimizing over pi.",
    "start": "3528675",
    "end": "3535035"
  },
  {
    "text": "If you found a pi, so right now we know the transition model which is not always true,",
    "start": "3535035",
    "end": "3540315"
  },
  {
    "text": "but if you know the transition model, for a given pi you can compute mu because it's just following,",
    "start": "3540315",
    "end": "3546974"
  },
  {
    "text": "like, you could do Monte Carlo roll outs for example. So if you- someone's given you a pi and they tell you the transition model,",
    "start": "3546975",
    "end": "3552359"
  },
  {
    "text": "you can roll that out and you can estimate mu of pi. Then it's saying that if you do that, so let's say I have some policy,",
    "start": "3552360",
    "end": "3558555"
  },
  {
    "text": "I roll this out a bunch of times, I estimate my mu and I check whether that seems to be close to my mu of my demonstration policy.",
    "start": "3558555",
    "end": "3565185"
  },
  {
    "text": "If that's small, this is saying no matter what the real reward function is you're gonna have the same value as,",
    "start": "3565185",
    "end": "3572085"
  },
  {
    "text": "like, uh, what you're, like, like, you've matched, um, uh, the value that you would get under the expert policy.",
    "start": "3572085",
    "end": "3578010"
  },
  {
    "text": "So, you're good. You can just use this policy to act. [NOISE] Yeah.",
    "start": "3578010",
    "end": "3585315"
  },
  {
    "text": ". And I'm looking at the constraint on the [inaudible] for w,",
    "start": "3585315",
    "end": "3590760"
  },
  {
    "text": "and I, I don't quite see where that comes from. I'm curious since I'm missing it here or we haven't gone over it.",
    "start": "3590760",
    "end": "3597990"
  },
  {
    "text": "'s question is about why we have this constraint over w. Um, I- my- I would have to double-check the details to",
    "start": "3597990",
    "end": "3605430"
  },
  {
    "text": "be careful about this but I'm pretty sure it's there, so that, um, as we do these backups when we do this approximation,",
    "start": "3605430",
    "end": "3612120"
  },
  {
    "text": "that, um, your errors are all bounded [NOISE] so that things don't explode. Um, and that when you do this proof that- I think I'd have to double-check it,",
    "start": "3612120",
    "end": "3622770"
  },
  {
    "text": "but uh, but I think you basically use Holder's inequality and then you use the fact that the w is bounded to ensure that your ultimate value is bounded.",
    "start": "3622770",
    "end": "3630540"
  },
  {
    "text": "So, you can check that. [NOISE] In general,",
    "start": "3630540",
    "end": "3637590"
  },
  {
    "text": "you want your, your reward function to be bounded, um, particularly the- in the- even with just counting, like, it's useful.",
    "start": "3637590",
    "end": "3645045"
  },
  {
    "text": "You always need to make sure that your Bellman operator's like a contraction to have a hope of- I mean, we've already talked about the fact that with linear value function approximated,",
    "start": "3645045",
    "end": "3652125"
  },
  {
    "text": "you don't always converge, um, uh, but if your rewards are unbounded it gets worse. Yeah, .",
    "start": "3652125",
    "end": "3660630"
  },
  {
    "text": "Um, I'm trying to fit this into uh, other things I'm familiar with, is this basically, like,",
    "start": "3660630",
    "end": "3665820"
  },
  {
    "text": "a maximum likelihood way of looking at the policy, right? Like, if we flipped a coin 100 times and we got 99 heads and 1 tails,",
    "start": "3665820",
    "end": "3673920"
  },
  {
    "text": "it's possible that came from, uh, a fair coin. You know that uh, we can't discount that but it's unlikely, right?",
    "start": "3673920",
    "end": "3680550"
  },
  {
    "text": "So, if we observe some expert agent doing the same thing 100 times",
    "start": "3680550",
    "end": "3685950"
  },
  {
    "text": "that could come from a reward function that's zero everywhere but not as likely as some other reward function? Does that-",
    "start": "3685950",
    "end": "3691530"
  },
  {
    "text": "Oh, great question. So is asking, like, so is this, sort of, giving us some way to deal with the fact that the reward could be zero.",
    "start": "3691530",
    "end": "3697890"
  },
  {
    "text": "That we have this, sort of, unidentifiability problem. This does not handle that unfortunately.",
    "start": "3697890",
    "end": "3703440"
  },
  {
    "text": "So, um, er, this is still not guaranteeing that we couldn't, uh, learn a weight if it's zero everywhere,",
    "start": "3703440",
    "end": "3710099"
  },
  {
    "text": "but what this is saying here is that, um, instead of thinking about trying to learn the reward function directly,",
    "start": "3710100",
    "end": "3715545"
  },
  {
    "text": "if you match expected state features, um, then that's another way to",
    "start": "3715545",
    "end": "3720810"
  },
  {
    "text": "guarantee that your policy is basically doing the same thing as the expert. So, if you have a policy that basically it looks like- that",
    "start": "3720810",
    "end": "3726750"
  },
  {
    "text": "visits the same states in exactly the same frequency as what the expert does, then you've matched their policy.",
    "start": "3726750",
    "end": "3731790"
  },
  {
    "text": "And you still don't necessarily know that the w you've got is accurate or is a good estimate of the reward function but maybe you",
    "start": "3731790",
    "end": "3737849"
  },
  {
    "text": "don't need it because if you really just care about being able to match the expert's policy then you've matched it.",
    "start": "3737850",
    "end": "3744105"
  },
  {
    "text": "Because if you're- if you visit all the states with exactly the same frequency as what the expert does, you have identical policies.",
    "start": "3744105",
    "end": "3750915"
  },
  {
    "text": "So, it's, sort of, giving up on it. It's saying, well, we still don't know what the real reward function is but it doesn't matter because we've uncovered the expert policy. .",
    "start": "3750915",
    "end": "3758760"
  },
  {
    "text": "Um, is there, like, a, um, nonlinear analog to this that might be more effective?",
    "start": "3758760",
    "end": "3766335"
  },
  {
    "text": "Great question, yes. So, there's been a lot of work also on doing this with deep neural networks. I'll give a couple of pointers later to sort of, uh, other approaches.",
    "start": "3766335",
    "end": "3774279"
  },
  {
    "text": "Okay. So, this sort of observation led to an algorithm for learning the policy which is,",
    "start": "3774380",
    "end": "3780539"
  },
  {
    "text": "um, uh, you try to find some sort of reward function. Like, that means, you know, a w and choice of w,",
    "start": "3780540",
    "end": "3787590"
  },
  {
    "text": "um, such that the teacher looks better than everything else. Looks better than all the other controllers you've got before.",
    "start": "3787590",
    "end": "3795045"
  },
  {
    "text": "So, it makes it look like, sort of, the, um, this w for this state distribution looks better than w for all other distributions.",
    "start": "3795045",
    "end": "3806130"
  },
  {
    "text": "[NOISE] And then you find the optimal control policy for the current w which can allow you to then get new,",
    "start": "3806130",
    "end": "3815820"
  },
  {
    "text": "uh, mu's because you have your transition model here. And you repeat this until,",
    "start": "3815820",
    "end": "3822480"
  },
  {
    "text": "sort of, the gap is sufficiently small. Now, this is not perfect.",
    "start": "3822480",
    "end": "3829605"
  },
  {
    "text": "Um, if, if your expert policy is sub-optimal, it's a little tricky how to combine these.",
    "start": "3829605",
    "end": "3835785"
  },
  {
    "text": "Um, I don't want to dwell too much on this particular algorithm, it is not something most people use anymore.",
    "start": "3835785",
    "end": "3841529"
  },
  {
    "text": "Um, people would use, uh, more deep learning approaches, but I think that the,",
    "start": "3841530",
    "end": "3847155"
  },
  {
    "text": "the key things to understand from this is, sort of, this aspect of, kind of, if you match state features that that's sufficient to say that the policies are identical.",
    "start": "3847155",
    "end": "3856920"
  },
  {
    "text": "It's actually bigger than. Yeah, and [OVERLAPPING] remind me your name first, please. . . [NOISE] Is there any significance in using norm one",
    "start": "3856920",
    "end": "3864030"
  },
  {
    "text": "versus the other norm, like, norm two? Uh, goo- great question. question is why do we use norm one in equation seven.",
    "start": "3864030",
    "end": "3871560"
  },
  {
    "text": "That is actually important. Um, it's not necessarily the only choice, but here this is saying you have to match on all states.",
    "start": "3871560",
    "end": "3879850"
  },
  {
    "text": "That's what the norm one is saying here. So, you can think of mu pi as- are really being of s. I'm not showing",
    "start": "3879860",
    "end": "3886110"
  },
  {
    "text": "that explicit dependency here but it is of s. And so what norm one is saying is that, um, when you sum up all of those errors it has to be one.",
    "start": "3886110",
    "end": "3893609"
  },
  {
    "text": "So, you're really, you're evaluating the error over all of this. You could choose other things to change the analysis. Um, uh, norm one in an infinity norm",
    "start": "3893610",
    "end": "3900420"
  },
  {
    "text": "norm tend be particularly easy to reason about when you start to do, um, uh, when you're trying to bound the error in the value function.",
    "start": "3900420",
    "end": "3908590"
  },
  {
    "text": "Okay. So, um, there's still this ambiguity that we've talked about,",
    "start": "3913010",
    "end": "3919950"
  },
  {
    "text": "so there's the sort of infinite number of different reward functions, the algorithm that we just talked about doesn't solve that issue.",
    "start": "3919950",
    "end": "3925770"
  },
  {
    "text": "Um, and so there's been a lot of work on, on, on imitation, uh, learning and inverse reinforcement learning.",
    "start": "3925770",
    "end": "3933164"
  },
  {
    "text": "And two of the key papers are as follows. The first one is called Maximum Entropy Inverse RL.",
    "start": "3933165",
    "end": "3939345"
  },
  {
    "text": "And the idea here was to say we wa- I don't wanna pick something, uh,",
    "start": "3939345",
    "end": "3944400"
  },
  {
    "text": "which has the maximum uncertainty, uh, given that still respects the constraints of the data that we have from our expert.",
    "start": "3944400",
    "end": "3952690"
  },
  {
    "text": "So saying, we're really not sure what the reward function is, we may not really be sure what the policy is, but let's try to pick distributions that have maximum entropy,",
    "start": "3952690",
    "end": "3960285"
  },
  {
    "text": "sort of make Least Commitment, um, sort of the opposite of overfitting, you kinda want to like underfit as much as possible.",
    "start": "3960285",
    "end": "3966690"
  },
  {
    "text": "Um, and only makes sure that you match in these expected, uh, state frequencies.",
    "start": "3966690",
    "end": "3973365"
  },
  {
    "text": "So, both of these, the- both of these methods and a lot of the methods think",
    "start": "3973365",
    "end": "3978750"
  },
  {
    "text": "very carefully about the expected state frequencies you get, um, er- and comparing the da- the data you get versus,",
    "start": "3978750",
    "end": "3985035"
  },
  {
    "text": "um, the data you have from the demonstrated- demonstrator. Um, these type of methods can be extended to where the transition model is a node.",
    "start": "3985035",
    "end": "3992039"
  },
  {
    "text": "[NOISE] Often that requires access to a simulator.",
    "start": "3992040",
    "end": "4001460"
  },
  {
    "text": "Often it means, so you can imagine for the thing we had before, if you didn't have access to the transition model but did- you did have access to it actually in the real world,",
    "start": "4001460",
    "end": "4008780"
  },
  {
    "text": "you could just try out new policies, see where your distribution of states that are like and how that matches your, um, expert demonstration and in fact that's often what's done.",
    "start": "4008780",
    "end": "4016700"
  },
  {
    "text": "So, maximum entropy inverse RL has been hugely influential. And, and, and then the second one, and this is also a note is from, uh,",
    "start": "4016700",
    "end": "4023059"
  },
  {
    "text": "also from Drew Bagnell's group, who was the same person that came up with DAGGER, so that group's been thinking a lot about and did a lot of nice contributions to inverse RL.",
    "start": "4023060",
    "end": "4030694"
  },
  {
    "text": "And then, in terms of extending this to sort of much, um, broader function approximators,",
    "start": "4030695",
    "end": "4037849"
  },
  {
    "text": "um, er, Stefano Ermon who's here at Stanford extended this to using deep neural networks,",
    "start": "4037849",
    "end": "4045320"
  },
  {
    "text": "um, and again, is doing sort of this feature matching. So, the idea in this case,",
    "start": "4045320",
    "end": "4050450"
  },
  {
    "text": "both of these methods compared to the sort of the DAGGER work or assuming that you have a fixed set of trajectories at the beginning,",
    "start": "4050450",
    "end": "4055745"
  },
  {
    "text": "and then you're going to do more things for the future. And in particular, um, the general, uh, generative adversarial inverse imitation learning, um,",
    "start": "4055745",
    "end": "4063890"
  },
  {
    "text": "has these initial trajectories, and then it's gonna allow the agent to go out and gather more data. So, they can gather more data,",
    "start": "4063890",
    "end": "4069845"
  },
  {
    "text": "it can compute the, the state frequencies, um, it can also use sort of a discriminator to compare- one of the challenges is,",
    "start": "4069845",
    "end": "4077810"
  },
  {
    "text": "you know, writing down, um, the form of Mu, can be hard when you have a really really high dimensional state space.",
    "start": "4077810",
    "end": "4085685"
  },
  {
    "text": "So, writing down, you know, a distribution over images is hard. Um, so, what they do in this case,",
    "start": "4085685",
    "end": "4091849"
  },
  {
    "text": "they're mostly focusing on MuJoCo-style tasks like robotic-style tasks, where you'd have a lot of different joints, but it's still hard to write down,",
    "start": "4091850",
    "end": "4097759"
  },
  {
    "text": "you know, nice distributions over that. So, what they focus on in this case is thinking about things like a discriminator that could tell between",
    "start": "4097760",
    "end": "4103910"
  },
  {
    "text": "your expert demonstrations and the demonstration- and the trajectories that are being generated by an agent.",
    "start": "4103910",
    "end": "4108980"
  },
  {
    "text": "And so, if you can tell the difference between those, then you're not matching. That's it, that's a nice insight,",
    "start": "4108980",
    "end": "4114574"
  },
  {
    "text": "is to say that we could use these sort of, uh, discriminators, uh, again, is, you know,",
    "start": "4114575",
    "end": "4119900"
  },
  {
    "text": "the discriminator function, to try to figure out how do we quantify what it means to have the same state distribution in really high state- high dimensional state spaces.",
    "start": "4119900",
    "end": "4128239"
  },
  {
    "text": "So, um, uh, this is known as Gail. And there's been a lot of extensions to Gail as well. Yeah, .",
    "start": "4128240",
    "end": "4135949"
  },
  {
    "text": "Uh, earlier, we said that there could be real practical benefits to learning the reward function,",
    "start": "4135950",
    "end": "4141289"
  },
  {
    "text": "in certain situations, um, but it seems like a takeaway to those that we can't actually do that, is that the correct takeaway here?",
    "start": "4141290",
    "end": "4147589"
  },
  {
    "text": "Um, er, yeah. So, was saying earlier, well, earlier as you were arguing that maybe there are times where we really want the reward function,",
    "start": "4147590",
    "end": "4153199"
  },
  {
    "text": "um, but maybe you're telling us that we can't really do that, um, I think in this- so we've mostly",
    "start": "4153200",
    "end": "4158330"
  },
  {
    "text": "been talking about like frequentist-style statistics, er, when we're talking about statistical methods here, from that perspective, it's often very hard to uncover the word function.",
    "start": "4158330",
    "end": "4165830"
  },
  {
    "text": "One thing that's often people do when they want to say, understand animal behavior or things like that,",
    "start": "4165830",
    "end": "4170839"
  },
  {
    "text": "so you have a prior, you can do it another way to do this, is you have Bayesian prior reward functions, and then you do Bayesian updating,",
    "start": "4170840",
    "end": "4177140"
  },
  {
    "text": "so that given the data that you see, you try to refine your posterior over the possible reward functions.",
    "start": "4177140",
    "end": "4182389"
  },
  {
    "text": "So, it avoids like then you can just not have your prior cover that reward is a zero everywhere, for example.",
    "start": "4182390",
    "end": "4189139"
  },
  {
    "text": "Um, er, so, if you have a structured prior, that can be one way to still use information to try",
    "start": "4189140",
    "end": "4194570"
  },
  {
    "text": "to reduce your uncertainty over people's or agent's or, uh, animal's reward functions, yeah,",
    "start": "4194570",
    "end": "4200000"
  },
  {
    "text": "[inaudible].",
    "start": "4200000",
    "end": "4205610"
  },
  {
    "text": "Yeah. Um, it's a great,  says you know,",
    "start": "4205610",
    "end": "4218570"
  },
  {
    "text": "what are, what are realistic priors for word functions? Um, uh, it's a great question. I think mostly it depends on the domain.",
    "start": "4218570",
    "end": "4224855"
  },
  {
    "text": "Um, that people do use, uh, I think we'll talk a little bit about it for the exploration aspects, people do use priors over reward functions for exploration as well,",
    "start": "4224855",
    "end": "4232910"
  },
  {
    "text": "um, things like Thompson sampling require you to do that. If you want it to be as close to frequentist as possible, um,",
    "start": "4232910",
    "end": "4238505"
  },
  {
    "text": "often people do, uh, Dirichlet distributions, over multinomials or things like that, um, or Gaussians, and, um, uh,",
    "start": "4238505",
    "end": "4245030"
  },
  {
    "text": "and so you'd use conjugate, um, uh, uh, exponential families, so everything's conjugate, but those aren't necessarily realistic.",
    "start": "4245030",
    "end": "4251300"
  },
  {
    "text": "I think in, in real domains, um, the benefit probably of using these sort of priors would be to really encode domain knowledge about, you know,",
    "start": "4251300",
    "end": "4258860"
  },
  {
    "text": "uh, whether people are very sensitive, what sort of rewards you expect, uh, to be reasonable in these cases.",
    "start": "4258860",
    "end": "4265385"
  },
  {
    "text": "Yeah, I mean, I think I- to go back to point too, I think a lot of it does depend on what you want out.",
    "start": "4265385",
    "end": "4271400"
  },
  {
    "text": "So if you really want to just understand the reward function and the preference function, then we need to maybe do something Bayesian or we'd need to try to have a method that's gonna help us like cover it.",
    "start": "4271400",
    "end": "4278330"
  },
  {
    "text": "I think what a lot of other methods ended up saying is, well, maybe we care about the reward function, but mostly we just care about getting high performance.",
    "start": "4278330",
    "end": "4284675"
  },
  {
    "text": "So, if we can uncover a policy that's matching an expert policy, we're fine. Behavior cloning wasn't a good way to do that because errors compound,",
    "start": "4284675",
    "end": "4290900"
  },
  {
    "text": "but now there are these other ways that can do that better, and so we're fine with that part. And again, I just wanna emphasize like Sergei Lamin",
    "start": "4290900",
    "end": "4298100"
  },
  {
    "text": "and others have done work which really combines, like you can take Gail and, and then go beyond that in terms of, er, exploration,",
    "start": "4298100",
    "end": "4305330"
  },
  {
    "text": "so you can end up with a policy that's better than your demonstrator, which I think is good, because often, like if your demonstrator comes from YouTube, um, uh,",
    "start": "4305330",
    "end": "4312410"
  },
  {
    "text": "which is nice, so that's a freely available place to get demonstrations. Um, you don't actually know the quality, so often you might want to use that to sort of",
    "start": "4312410",
    "end": "4318949"
  },
  {
    "text": "bootstrap learning but not necessarily be limited by it. All right, so just to summarize,",
    "start": "4318950",
    "end": "4325115"
  },
  {
    "text": "um, you know, in practice, there's been an enormous amount of work on imitation learning, particularly in robotics, uh, but in lots of domains.",
    "start": "4325115",
    "end": "4331250"
  },
  {
    "text": "Um, and I think that, you know, if you're gonna leave class today and go out into industry, um, that imitation learning, uh,",
    "start": "4331250",
    "end": "4338014"
  },
  {
    "text": "can be very useful practically, uh, because often it's easier to get demonstrations and it can really bootstrap learning,",
    "start": "4338015",
    "end": "4345395"
  },
  {
    "text": "um, uh, complicated Atari games, et cetera. Um, but there's still a lot of challenges that remain, uh,",
    "start": "4345395",
    "end": "4350900"
  },
  {
    "text": "particularly, in a lot of the domains that I think about we don't know the optimal policy. Um, so, I think about, er,",
    "start": "4350900",
    "end": "4357650"
  },
  {
    "text": "healthcare or like customers or, um, education like intelligent tutoring systems,",
    "start": "4357650",
    "end": "4362870"
  },
  {
    "text": "and all of those one of the big challenges is that you don't know the optimal policy and you're maybe doing all this because you think you could do",
    "start": "4362870",
    "end": "4368090"
  },
  {
    "text": "something better than what's in the existing data. Um, so, that's, that's a big challenge. Um, and how do you combine sort of inverse RL?",
    "start": "4368090",
    "end": "4375755"
  },
  {
    "text": "Ah, and maybe online RL in a, in a safe way. So, one of the motivations I said for imitation learning was,",
    "start": "4375755",
    "end": "4381485"
  },
  {
    "text": "oh well if you want to be safe, um, but then if your, if your- the only safe things right now don't do very well, then you have to figure out how to do safe exploration in the future.",
    "start": "4381485",
    "end": "4390210"
  },
  {
    "text": "All right. I think that's everything for today, and I'll see you guys next week where we're gonna start to talk about policy search [NOISE].",
    "start": "4391120",
    "end": "4403630"
  }
]