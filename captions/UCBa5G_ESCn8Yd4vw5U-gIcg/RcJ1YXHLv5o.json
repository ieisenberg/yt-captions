[
  {
    "text": " And hello, everyone here.",
    "start": "0",
    "end": "6440"
  },
  {
    "text": "So today for our speaker,\nwe have Albert Jiang from Mistral AI and also from\nthe University of Cambridge.",
    "start": "6440",
    "end": "14660"
  },
  {
    "text": "Nice to meet you and thank\nyou for coming today and being our speaker. So Albert is AI\nscientist at Mistral AI",
    "start": "14660",
    "end": "23170"
  },
  {
    "text": "and a final year PhD student at\nthe computer science department of Cambridge University.",
    "start": "23170",
    "end": "29240"
  },
  {
    "text": "He works on language model\npre-training and reasoning at Mistral AI and\nlanguage models",
    "start": "29240",
    "end": "34930"
  },
  {
    "text": "for mathematics at Cambridge. So thank you for coming today. And today Albert\nis going to talk",
    "start": "34930",
    "end": "41140"
  },
  {
    "text": "about Mistral, a sparse mixture\nof experts, language models. And yeah, I think I will\nnow leave it for Albert.",
    "start": "41140",
    "end": "53170"
  },
  {
    "text": "All right, Emily. Thanks for the really\nnice introduction. So let me share my screen here.",
    "start": "53170",
    "end": "59470"
  },
  {
    "text": "So, yeah, thanks very much\nagain for the really nice introduction, Emily. So today I'll be talking about\nDemystifying Mistral of Experts.",
    "start": "59470",
    "end": "67330"
  },
  {
    "text": "And my name is Albert. I'm a AI scientist at\nMistral AI and a PhD student",
    "start": "67330",
    "end": "73000"
  },
  {
    "text": "at University of Cambridge. So the contents you'll\nbe hearing today. First, I will talk about\nthe architecture of, first,",
    "start": "73000",
    "end": "80549"
  },
  {
    "text": "the dense transformers\narchitecture just to review and also the sparse mixture\nof experts or SMoEs.",
    "start": "80550",
    "end": "87570"
  },
  {
    "text": "So after that, I'll be\ntalking about interpreting sparse mixture of experts models\nand the majority of this slides",
    "start": "87570",
    "end": "94920"
  },
  {
    "text": "will be based on the mixture of\nexperts paper that will put out earlier this year in January.",
    "start": "94920",
    "end": "100689"
  },
  {
    "text": "And throughout this talk,\nif you have any questions, feel free to raise your hand and\nI'll be really happy to answer.",
    "start": "100690",
    "end": "106150"
  },
  {
    "text": "And also I'll be posing\nsome open research questions because these things are\nreally very much under research",
    "start": "106150",
    "end": "113850"
  },
  {
    "text": "and we hope that more\nresearch can be done. And so the open-source community\ncan really enjoy the benefits. OK, let's get started.",
    "start": "113850",
    "end": "120510"
  },
  {
    "text": "So first, let's talk\nabout architecture. We can first talk about the\ndense transformer architecture",
    "start": "120510",
    "end": "127020"
  },
  {
    "text": "that a lot of people must\nhave already talked about. But here I want to focus\non the Mistral 7B model,",
    "start": "127020",
    "end": "133890"
  },
  {
    "text": "which is a dense transformer. The main difference between the\nMistral 7B model and I guess",
    "start": "133890",
    "end": "142000"
  },
  {
    "text": "the rest of the dense models\nis that it has good query attention, which is\ndifferent from the multi-head",
    "start": "142000",
    "end": "148629"
  },
  {
    "text": "and multi-query attention\nsomewhat in between. So for multi-head attention\nyou have the same number of--",
    "start": "148630",
    "end": "155320"
  },
  {
    "text": "the same number of keys,\nqueries, and values. And for multi-query attention\nyou have a lot of queries",
    "start": "155320",
    "end": "161650"
  },
  {
    "text": "and just one key and one value. And group-query\nattention are in between. And you have a lot of queries,\nmuch less query keys and values.",
    "start": "161650",
    "end": "170940"
  },
  {
    "text": " Also, there's another difference\nin the attention we're using",
    "start": "170940",
    "end": "177380"
  },
  {
    "text": "for Mistral 7B. That is we're using a\nsliding window attention. The idea being for some of the\nlower layers of the transformer,",
    "start": "177380",
    "end": "184940"
  },
  {
    "text": "the tokens-- so for each token\nposition, they attend",
    "start": "184940",
    "end": "190190"
  },
  {
    "text": "to a relatively short\nspan of previous tokens. And as you go deeper\nin the transformer,",
    "start": "190190",
    "end": "196220"
  },
  {
    "text": "the later token\npositions will get information transmitted from\nthe earlier token positions.",
    "start": "196220",
    "end": "201930"
  },
  {
    "text": "And neither of these\narchitectural designs are new. So these are already\npresent in previous works",
    "start": "201930",
    "end": "209570"
  },
  {
    "text": "from 2023, 2019, and 2020. So these are really nothing new.",
    "start": "209570",
    "end": "214620"
  },
  {
    "text": "But we decided to adopt these\narchitectural changes in Mistral 7B to make a better model.",
    "start": "214620",
    "end": "220250"
  },
  {
    "text": "So after writing\nthese slides, we realized there's actually a\nhuge number of decision choices",
    "start": "220250",
    "end": "227599"
  },
  {
    "text": "you have to make in designing\na transformer architecture, because there are\nso many details.",
    "start": "227600",
    "end": "233170"
  },
  {
    "text": "So just presenting this\ndoesn't feel sufficient. So I try to--",
    "start": "233170",
    "end": "238200"
  },
  {
    "text": "well, first, these are the\nstandard configurations for Mistral 7B. And also I just want to have\na very single, very, very",
    "start": "238200",
    "end": "247440"
  },
  {
    "text": "clean code in PyTorch\nthat just tells you how to write a single\ntransformer layer and I'll be adopting\nthe [INAUDIBLE] notation",
    "start": "247440",
    "end": "255569"
  },
  {
    "text": "by annotating the\ndimensions of every tensor as a suffix in\nthe variable name.",
    "start": "255570",
    "end": "262810"
  },
  {
    "text": "So let's say our\nsequence size is 8,000, 8k and that we have 32 query\nhats and 8 K and V here,",
    "start": "262810",
    "end": "275290"
  },
  {
    "text": "so this is quite standard\nfor group query attention. And our head dimensions are 128.",
    "start": "275290",
    "end": "281130"
  },
  {
    "text": "And our latent\ndimensions is 14k. So in order to write a\nsingle transformer layer,",
    "start": "281130",
    "end": "292860"
  },
  {
    "text": "first, you initialize your\nquery, key, and value matrices.",
    "start": "292860",
    "end": "298240"
  },
  {
    "text": "So here I wrote down\ntheir dimensions here. And notice that\nwe don't use bias, so there's no bias in\nany of these matrices.",
    "start": "298240",
    "end": "305250"
  },
  {
    "text": "And we also write\ndown our output matrix to know that the\noutput matrix has",
    "start": "305250",
    "end": "312540"
  },
  {
    "text": "the same dimensions as the\ntranspose of the career matrix.",
    "start": "312540",
    "end": "318420"
  },
  {
    "text": "So in order to write an\nattention forward layer, so here, let's assume that our\ninput x has dimension L times D.",
    "start": "318420",
    "end": "326670"
  },
  {
    "text": "So L is a sequence length and\nD is the latent dimension. So you first want to\nuse these matrices",
    "start": "326670",
    "end": "334970"
  },
  {
    "text": "to try to find what's your\nquery, key, and values are. So here their\ndimensions have been",
    "start": "334970",
    "end": "341610"
  },
  {
    "text": "normalized to L, N, H, L, K,\nH, and L, V, H, respectively.",
    "start": "341610",
    "end": "347090"
  },
  {
    "text": "And then you want to\napply rotary embedding to your keys and queries.",
    "start": "347090",
    "end": "352169"
  },
  {
    "text": "You don't really\napply that to values. And then you do a very\nstandard attention mechanism",
    "start": "352170",
    "end": "359180"
  },
  {
    "text": "with the queries,\nkeys and values. To note here, we\nactually repeat the keys",
    "start": "359180",
    "end": "364970"
  },
  {
    "text": "and values a couple of times\nto make their dimensions match with the queries\nand then we just",
    "start": "364970",
    "end": "371940"
  },
  {
    "text": "return the output multiplied\nwith the output matrix. So this is your standard\nattention forward layer.",
    "start": "371940",
    "end": "380230"
  },
  {
    "text": "And to have a transform layer,\nyou also want to tie that. You want to first\ndo a normalization.",
    "start": "380230",
    "end": "387400"
  },
  {
    "text": "Here, we use RSMNorm\nover the input and then we do\nattention forward.",
    "start": "387400",
    "end": "393960"
  },
  {
    "text": "And then we do an\nattention residual layer, followed by\nnormalization plus an MLP",
    "start": "393960",
    "end": "399720"
  },
  {
    "text": "plus a dense residual layer. So this is what's present\nin a transformer layer.",
    "start": "399720",
    "end": "406010"
  },
  {
    "text": "So with these\narchitecture designs, we managed to make a\nmodel that's quite good.",
    "start": "406010",
    "end": "412169"
  },
  {
    "text": "So this was released\nin September last year, and it was able to beat a\nlot of the Lambda2 models",
    "start": "412170",
    "end": "418350"
  },
  {
    "text": "back then despite\nbeing very small. OK, so let's now go to the\ninteresting bits, that's",
    "start": "418350",
    "end": "426160"
  },
  {
    "text": "mixture of experts. And mixture of experts\nis not a very new idea. It's been present forever.",
    "start": "426160",
    "end": "431720"
  },
  {
    "text": "And there are a couple of\nvery quite recent papers that really highlighted\nthe importance and what",
    "start": "431720",
    "end": "438760"
  },
  {
    "text": "are the benefits you're getting\nfrom these mixture of experts, layer models.",
    "start": "438760",
    "end": "445610"
  },
  {
    "text": "So I guess probably\nthe most well-known one is the switch transformers, that\nis the Google paper that scales",
    "start": "445610",
    "end": "455169"
  },
  {
    "text": "transformers to\ntrillion parameter models with simple and\nefficient sparsity.",
    "start": "455170",
    "end": "460360"
  },
  {
    "text": "And they discuss a lot about\nthe parallelism decisions you can make about data\nparallelism, model parallelism,",
    "start": "460360",
    "end": "466900"
  },
  {
    "text": "and expert parallelism. And for a mixture\nof experts layer,",
    "start": "466900",
    "end": "475490"
  },
  {
    "text": "it's even older\nidea so from 2017 also by quite famous\nvideo from Google.",
    "start": "475490",
    "end": "482849"
  },
  {
    "text": "And the idea is that\nyou try to take out-- so at the time they were still\nusing recurrent language models.",
    "start": "482850",
    "end": "490610"
  },
  {
    "text": "So the idea is that you try to\nhave a gating layer, a gating network that decides which\nexperts you should route to.",
    "start": "490610",
    "end": "498409"
  },
  {
    "text": "OK, so in our\nmixture of experts, in our implementation of the\nmixture of experts layer,",
    "start": "498410",
    "end": "507139"
  },
  {
    "text": "we're doing something\nvery similar. So we have the inputs that get\nsent to a router and the router",
    "start": "507140",
    "end": "514070"
  },
  {
    "text": "assign the gating weights and to\npick the top two experts to give",
    "start": "514070",
    "end": "519799"
  },
  {
    "text": "these inputs to. And after these experts\nindividually process the inputs,",
    "start": "519799",
    "end": "526850"
  },
  {
    "text": "the gating waves will be applied\nto the output of the experts and they get some to\nget through the results.",
    "start": "526850",
    "end": "532530"
  },
  {
    "text": "So if you write it in\nvery mathematically, you can say that the\ninput is x and the router",
    "start": "532530",
    "end": "540140"
  },
  {
    "text": "has a matrix multiply to it. And then you take the\nTopK experts out of that,",
    "start": "540140",
    "end": "545630"
  },
  {
    "text": "do a softmax, and these\nare your gating weights.",
    "start": "545630",
    "end": "551126"
  },
  {
    "text": "And output of the\nmixture of expert layer y is the softmax of the\ntop two gating weights",
    "start": "551126",
    "end": "559980"
  },
  {
    "text": "times the SwiGLU function over\nx for each of the experts.",
    "start": "559980",
    "end": "566899"
  },
  {
    "text": "OK, so a sparse mixture of\nexperts model is quite near",
    "start": "566900",
    "end": "573470"
  },
  {
    "text": "the cost performance parity\nfrontier because for each token,",
    "start": "573470",
    "end": "578600"
  },
  {
    "text": "you don't-- the token doesn't\nhave to go through the entire neural network parameters.",
    "start": "578600",
    "end": "583950"
  },
  {
    "text": "Instead, it can just go\nto the active parameters for that token. And this mixture of experts\nmodel can outperform Lambda2 7B",
    "start": "583950",
    "end": "593540"
  },
  {
    "text": "with about five times\nfaster inference. So we see it as a drop in\nreplacement for GPT-3.5,",
    "start": "593540",
    "end": "599959"
  },
  {
    "text": "which can also master quite\na few European languages in addition to English and it\ncan gracefully handle a context",
    "start": "599960",
    "end": "608029"
  },
  {
    "text": "32k tokens. So this mixture of experts model\nwas released under Apache 2.0",
    "start": "608030",
    "end": "614300"
  },
  {
    "text": "so you can free to use\nfor commercial purposes. Also, attach the configurations\nfor the mixture of experts model",
    "start": "614300",
    "end": "621020"
  },
  {
    "text": "here so you can refer this\nback to the Mistral 7B to see",
    "start": "621020",
    "end": "626430"
  },
  {
    "text": "the differences. OK, so I want to talk\nabout, what is MoE-fying?",
    "start": "626430",
    "end": "634800"
  },
  {
    "text": "Which is a verb I just created. The MLP layers\nactually give you. So the conventional wisdom is\nthat the MLPs in a transformer",
    "start": "634800",
    "end": "644310"
  },
  {
    "text": "network store knowledge\nand the attention stores algorithms or\nreasoning, or they implement",
    "start": "644310",
    "end": "650189"
  },
  {
    "text": "algorithms or reasoning. So by MoE-fying the\nMLP layers, we're",
    "start": "650190",
    "end": "655410"
  },
  {
    "text": "supposed to get a\nboost in knowledge. And indeed we try to benchmark\na mixture of 8 times 7B with",
    "start": "655410",
    "end": "665290"
  },
  {
    "text": "a Mistral 7B and a couple\nof the Lambda2 models. And in a couple of different\ncategories you can definitely",
    "start": "665290",
    "end": "673860"
  },
  {
    "text": "see that on the knowledge\nheavy tasks such as MMLU and knowledge, the\nMistral 8 times 7B model,",
    "start": "673860",
    "end": "682170"
  },
  {
    "text": "which is shaded in yellow here,\nis doing a lot better than Mistral 7B.",
    "start": "682170",
    "end": "687630"
  },
  {
    "text": "And on some of the reasoning,\ncomprehension and tasks, it seems to be doing a\nlittle bit better, but not",
    "start": "687630",
    "end": "697050"
  },
  {
    "text": "as much as it does on\nknowledge and MMLU. OK, and here is a\ndifferent graph.",
    "start": "697050",
    "end": "704780"
  },
  {
    "text": "Here, what we're trying to\nshow is that on the horizontal axis is the number\nof active parameters.",
    "start": "704780",
    "end": "712110"
  },
  {
    "text": "And on the vertical\naxis is the performance of the models on various\ntasks and the brown dots here",
    "start": "712110",
    "end": "722300"
  },
  {
    "text": "are two models, the\nseries of Lambda2 models. And the orange dots are\nMistral models from Mistral 7B",
    "start": "722300",
    "end": "728330"
  },
  {
    "text": "to Mistral 8 times 7B. So the Mistral 8 times 7B\nuses only 12.9 billion active",
    "start": "728330",
    "end": "736610"
  },
  {
    "text": "parameters, so\nclose to 13 billion, yet it can perform\nquite a bit better,",
    "start": "736610",
    "end": "741680"
  },
  {
    "text": "especially in the\nknowledge category. You can see the drop-- you can see the increase from\nMistral 7B to Mistral 8 times 7B",
    "start": "741680",
    "end": "748490"
  },
  {
    "text": "is quite huge here. There are some improvements\non other categories, but the knowledge\nimprovement is really great.",
    "start": "748490",
    "end": "755960"
  },
  {
    "text": "OK, so there is a\nresearch question that is if making\nyour language model--",
    "start": "755960",
    "end": "766400"
  },
  {
    "text": "if making the MLP layers\nof your language model can give you a huge\nboost in knowledge,",
    "start": "766400",
    "end": "773780"
  },
  {
    "text": "how about MoE-fying\nthe attention layers? So this is also very,\nwell, not very old,",
    "start": "773780",
    "end": "779510"
  },
  {
    "text": "but like quite old idea\nfrom 2022 in the switch transformer papers. So the idea is you can replace\nthe trainable Q, K, V matrices",
    "start": "779510",
    "end": "789260"
  },
  {
    "text": "with switch layers. So instead of doing a standard\nmatrix multiplication, you can do a MoE.",
    "start": "789260",
    "end": "795699"
  },
  {
    "text": "So you have a gating\nlayer and then some dense layers for the Q,\nK, V matrices and the problem",
    "start": "795700",
    "end": "804560"
  },
  {
    "text": "at the time was with stability. So if you bf16 precision\nto train your models,",
    "start": "804560",
    "end": "810890"
  },
  {
    "text": "they can sometimes diverge. But if you use f32, they don't\nand you get some performance",
    "start": "810890",
    "end": "816710"
  },
  {
    "text": "boost, which is nice. But at present it's mostly\nwe're training with bf16,",
    "start": "816710",
    "end": "822440"
  },
  {
    "text": "so we have to find a\nway to make it work. And so in order to\nmake this work, maybe",
    "start": "822440",
    "end": "830760"
  },
  {
    "text": "you can try better\nstability techniques, maybe some normalization, or maybe\nyou could use a different way",
    "start": "830760",
    "end": "838340"
  },
  {
    "text": "to MoE-fying the\nattention layers. So this is an open\nresearch question.",
    "start": "838340",
    "end": "843380"
  },
  {
    "text": "We would like to see some\nresearch to be done here. ",
    "start": "843380",
    "end": "849040"
  },
  {
    "text": "And after this, I\nwant to just talk about several myths\nthat seems to be present around the\nmixture of experts models.",
    "start": "849040",
    "end": "856070"
  },
  {
    "text": "So the myth 1 is that there are\neight experts in Mistral 8 times 7B. So this is probably our fault by\nnaming our model Mistral 8 times",
    "start": "856070",
    "end": "864940"
  },
  {
    "text": "7B. And so the truth is that every\ntransformer layer has 8 experts.",
    "start": "864940",
    "end": "872780"
  },
  {
    "text": "And these experts are\npermutation equivalent. So because the way it works is\nthat the gating network decides",
    "start": "872780",
    "end": "885880"
  },
  {
    "text": "how much weight to give\nto each of the experts and only picks the top two. ",
    "start": "885880",
    "end": "892680"
  },
  {
    "text": "So it doesn't matter\nhow you, sorry, so it doesn't matter\nhow you permute them,",
    "start": "892680",
    "end": "899862"
  },
  {
    "text": "they will give you\nthe same result. So that means\ninstead of 8 experts, you actually have 32\ntimes 8 experts in total.",
    "start": "899862",
    "end": "907590"
  },
  {
    "text": "And they're relatively\nindependent across layers.",
    "start": "907590",
    "end": "914020"
  },
  {
    "text": "The myth 2 is that there are 56\nbillion parameters in Mistral 8 times 7B.",
    "start": "914020",
    "end": "920230"
  },
  {
    "text": "But the gating and the\nattention layers are shared. So these are not in\nthe 8 times part.",
    "start": "920230",
    "end": "926629"
  },
  {
    "text": "And in fact, there are only\n46 times 7 billion parameters in total. And each token will\nactually only see",
    "start": "926630",
    "end": "933880"
  },
  {
    "text": "12.9 billion active\nparameters instead of 40. OK, so another thing that seems\nto be quite popular these days",
    "start": "933880",
    "end": "943810"
  },
  {
    "text": "is to compare everything,\ntying everything that's related to cost with the\nnumber of active parameters",
    "start": "943810",
    "end": "950980"
  },
  {
    "text": "as if they're proportional. And that's not quite true\nbecause Mistral 8 times 7B has",
    "start": "950980",
    "end": "956950"
  },
  {
    "text": "fewer active parameters\nthan Llama2 13B. But having this\nof expert routing",
    "start": "956950",
    "end": "962170"
  },
  {
    "text": "means that you need\nto send tokens around to different experts\nquite dynamically. So that means you have\nmore communication costs.",
    "start": "962170",
    "end": "969589"
  },
  {
    "text": "You cannot just pre-programmed\nwhich token will be sent to which expert.",
    "start": "969590",
    "end": "975040"
  },
  {
    "text": "While you gain much more in\nperformance divided by cost, the absolute cost\nis not proportional",
    "start": "975040",
    "end": "981430"
  },
  {
    "text": "to the active parameter count. So usually your mixture\nof experts model",
    "start": "981430",
    "end": "989589"
  },
  {
    "text": "if you have the active\nparameters, your actual cost of serving these\nmodels is actually",
    "start": "989590",
    "end": "995530"
  },
  {
    "text": "a bit more than the equivalent\ndense model with the same number",
    "start": "995530",
    "end": "1000640"
  },
  {
    "text": "of active parameters. OK, here's also a\nreally, I think,",
    "start": "1000640",
    "end": "1006410"
  },
  {
    "text": "really interesting research\nquestion is, how do you balance loads at inference time? So you're getting\nlayer might still",
    "start": "1006410",
    "end": "1012440"
  },
  {
    "text": "decide to do unbalanced loading\nat inference time, which makes inference slower. What that means is\nthat ideally you",
    "start": "1012440",
    "end": "1019040"
  },
  {
    "text": "want your 8 experts or\nhowever many experts you have, ideally, you want them to\nhandle the same number of tokens",
    "start": "1019040",
    "end": "1025550"
  },
  {
    "text": "such that you don't\nget the slowest, you don't always have to\nwait for the slowest expert.",
    "start": "1025550",
    "end": "1031760"
  },
  {
    "text": "And some nice ideas are around\nlike mixture of depth and also dynamic loading based on scores.",
    "start": "1031760",
    "end": "1037559"
  },
  {
    "text": "So some of the ideas are like if\nyou saturate one of the experts, maybe you can try to find\na neighbor of that expert",
    "start": "1037560",
    "end": "1046189"
  },
  {
    "text": "to give the token to,\nwhich is not saturated and then try to slowly fill up\nthe total number of expert times",
    "start": "1046190",
    "end": "1054380"
  },
  {
    "text": "token count you have. OK, another, I think quite\nopen research question",
    "start": "1054380",
    "end": "1062220"
  },
  {
    "text": "is, how can you compress SMoEs? And this comes from a screenshot\nsomeone took of Tim Dettmers",
    "start": "1062220",
    "end": "1071460"
  },
  {
    "text": "because I really trust this\nman's model compression",
    "start": "1071460",
    "end": "1077399"
  },
  {
    "text": "skills with my life. So Tim Dettmers\nsaid he thinks we",
    "start": "1077400",
    "end": "1082680"
  },
  {
    "text": "can compress a mixture\nof experts model to lower smaller\nthan 4 gigabytes.",
    "start": "1082680",
    "end": "1088530"
  },
  {
    "text": "So because MoE\ncompression is quite different from dense\ntransformer compression",
    "start": "1088530",
    "end": "1095730"
  },
  {
    "text": "because the regular\ntransformers, they're really, really\ndifficult to sparsify and this is really true for\nthe feedforward MLP layers.",
    "start": "1095730",
    "end": "1104350"
  },
  {
    "text": "But the MoE layers\nare quite different. So maybe there's something you\ncan do with MoE layers, such",
    "start": "1104350",
    "end": "1111060"
  },
  {
    "text": "that your compression is\nmuch, much more efficient. So by compression here,\nI mean sparse vacation.",
    "start": "1111060",
    "end": "1116930"
  },
  {
    "text": "So a lot of the-- so sparse vacation,\nnot in the sense of choosing only top two\nexperts instead of eight.",
    "start": "1116930",
    "end": "1124460"
  },
  {
    "text": "But I mean, a lot\nof the parameters might be doing not much work\nand you can share them out.",
    "start": "1124460",
    "end": "1131770"
  },
  {
    "text": "OK, so this was set by Tim\nDettmers in December last year.",
    "start": "1131770",
    "end": "1138130"
  },
  {
    "text": "And I don't think I've seen much\nconvincing work on sparsifying MoEs to an extreme.",
    "start": "1138130",
    "end": "1144590"
  },
  {
    "text": "But I realize more\nwork going on here. OK, now, I want to talk\nabout how to interpret",
    "start": "1144590",
    "end": "1154400"
  },
  {
    "text": "a mixture of expert models. So the mixture of expert models\nwith a sparse gating layer",
    "start": "1154400",
    "end": "1162590"
  },
  {
    "text": "actually provides you an\nincredible opportunity by giving you this\ngreat gating signals.",
    "start": "1162590",
    "end": "1168290"
  },
  {
    "text": "So deep neural networks are\ntraditionally very, very hard to interpret,\nthings all the weights",
    "start": "1168290",
    "end": "1173720"
  },
  {
    "text": "and activations being very\nhigh dimensional spaces. And the attention\nin transformers",
    "start": "1173720",
    "end": "1181760"
  },
  {
    "text": "potentially offers some\ninterpretation opportunity because you can look at\nhow much of the attention",
    "start": "1181760",
    "end": "1187880"
  },
  {
    "text": "weights get assigned\nto which tokens.",
    "start": "1187880",
    "end": "1193410"
  },
  {
    "text": "But you're essentially\npicking at, what is the model looking at by\nlooking at the attention scores?",
    "start": "1193410",
    "end": "1199470"
  },
  {
    "text": "But this also quickly\ngets very messy as you have a lot\nof attention heads. And it's hard to visualize\nand hard to interpret.",
    "start": "1199470",
    "end": "1208679"
  },
  {
    "text": "And the gating layer\nin SMoEs potentially tells you a bit more because\nit tells you which experts",
    "start": "1208680",
    "end": "1214830"
  },
  {
    "text": "is looking at which tokens. So can we make some\nsense out of this then?",
    "start": "1214830",
    "end": "1220530"
  },
  {
    "text": "So in our paper, we\ntry to find if there's some domain specialization\nwith this experts.",
    "start": "1220530",
    "end": "1227880"
  },
  {
    "text": "So we took the validation\nsplits of the Pile data sets. And so these are represented\nin different colors.",
    "start": "1227880",
    "end": "1235770"
  },
  {
    "text": "You can see the red is archived. The green one here is GitHub. And the blue one\nhere is PhilPapers.",
    "start": "1235770",
    "end": "1243669"
  },
  {
    "text": "The purple one is StackExchange\nand for layers 0 through 31. So these correspond to the\nshallowest layer, the mid-layer,",
    "start": "1243670",
    "end": "1253590"
  },
  {
    "text": "and the deepest layer. So this is the-- layer 31 is\nthe layer just before decoding.",
    "start": "1253590",
    "end": "1259680"
  },
  {
    "text": "So for each of these\nlayers, we plot how much each expert is selected.",
    "start": "1259680",
    "end": "1265470"
  },
  {
    "text": "So since we have eight experts,\nthe random selection chance",
    "start": "1265470",
    "end": "1272820"
  },
  {
    "text": "is 12.5% And you can\nsee that at layer 0. So this is the layer that's\nclosest to the raw tokens.",
    "start": "1272820",
    "end": "1281190"
  },
  {
    "text": "You will see that the\ndistribution is quite uniform. It's not very\nuniform, but quite.",
    "start": "1281190",
    "end": "1288420"
  },
  {
    "text": "So this might be telling us\nthat this layer is too shallow, that it's still doing a\nlot of syntactical stuff",
    "start": "1288420",
    "end": "1295470"
  },
  {
    "text": "that you don't get a\nlot of meaningful domain specialization going on.",
    "start": "1295470",
    "end": "1300640"
  },
  {
    "text": "And on the mid-layer,\nso here is potentially where we get a lot of\nsemantic information.",
    "start": "1300640",
    "end": "1308290"
  },
  {
    "text": "You can notice that\nexpert three here is doing something\nquite interesting.",
    "start": "1308290",
    "end": "1313420"
  },
  {
    "text": "So because it gets\nselected not very often",
    "start": "1313420",
    "end": "1320070"
  },
  {
    "text": "for any of the other\ncategories, but gets really selected quite a\nlot for DM Mathematics.",
    "start": "1320070",
    "end": "1325570"
  },
  {
    "text": "And second place is GitHub. So maybe this layer-- so maybe this expert in\nexpert three in layer 15",
    "start": "1325570",
    "end": "1331679"
  },
  {
    "text": "is doing something that's\nmath and code heavy. But this is quite speculative\nand we cannot really conclude",
    "start": "1331680",
    "end": "1338490"
  },
  {
    "text": "much out of it. And for layer 31, I think\nthe distribution go back to close to uniform again,\nalthough you get some spiking",
    "start": "1338490",
    "end": "1347340"
  },
  {
    "text": "distributions here and there\nthat specialize in mathematics it seems or since the data\nset is DeepMind mathematics",
    "start": "1347340",
    "end": "1355260"
  },
  {
    "text": "is more like arithmetics. OK, we also did some analysis\non consecutive tokens.",
    "start": "1355260",
    "end": "1364970"
  },
  {
    "text": "So the question to ask\nhere is whether two tokens that are consecutive get\nassigned to the same expert.",
    "start": "1364970",
    "end": "1372940"
  },
  {
    "text": "So here we have\nthe first choice. So that means if a token\nis given to expert I,",
    "start": "1372940",
    "end": "1380770"
  },
  {
    "text": "does the next token also\nget assigned to expert I?",
    "start": "1380770",
    "end": "1386140"
  },
  {
    "text": "And also we're still\ndoing this analysis on the three layers, layer\n0, layer 15, and layer 31",
    "start": "1386140",
    "end": "1392770"
  },
  {
    "text": "corresponding to three\ndifferent places in the network. And the validation set is still\nthe pole validation split.",
    "start": "1392770",
    "end": "1400110"
  },
  {
    "text": " So the random chance\nassignment score--",
    "start": "1400110",
    "end": "1406070"
  },
  {
    "text": "a random chance\nassignment will give you 12.5% in terms of how\nmany of the tokens get--",
    "start": "1406070",
    "end": "1416150"
  },
  {
    "text": "they are assigned\nto expert I, so get assigned to expert\nI. Sorry, the next token also gets assigned to expert I.",
    "start": "1416150",
    "end": "1422940"
  },
  {
    "text": "And we can see that\nat layer 0, this is slightly higher than\nrandom, but at layer 15,",
    "start": "1422940",
    "end": "1429380"
  },
  {
    "text": "this is significantly\nhigher than random. So well, it's almost\ndouble the random chance",
    "start": "1429380",
    "end": "1437600"
  },
  {
    "text": "that the first choice\nfor the gating layer",
    "start": "1437600",
    "end": "1443150"
  },
  {
    "text": "will actually assign the\nnext token to the same expert as this token.",
    "start": "1443150",
    "end": "1448470"
  },
  {
    "text": "And when go to the\nlast layer, you'll find that this transform\nregresses a little bit.",
    "start": "1448470",
    "end": "1455140"
  },
  {
    "text": "It's still quite significantly\nhigher than random, but it's a bit\nless than layer 15.",
    "start": "1455140",
    "end": "1462870"
  },
  {
    "text": "We also look at the\nfirst or second choice. So if this expert is not\nchosen as the best expert",
    "start": "1462870",
    "end": "1472270"
  },
  {
    "text": "to route this token to,\ncould it be the second best? So here the random chance\nprobability is around 46%.",
    "start": "1472270",
    "end": "1482520"
  },
  {
    "text": "And you can see for layer 0\nthis will have the same pattern. So for layer 0, the number of\ntimes that the next token gets",
    "start": "1482520",
    "end": "1492270"
  },
  {
    "text": "assigned to the first or\nsecond choice for this expert is around just slightly\nhigher than random.",
    "start": "1492270",
    "end": "1498480"
  },
  {
    "text": "And for layer 15,\nthis is really, really significantly higher. And for layer 31, it is\nregresses a little bit.",
    "start": "1498480",
    "end": "1504940"
  },
  {
    "text": "So I think there's certainly\nsome more detailed analysis to be done here that\nwe can conclude out",
    "start": "1504940",
    "end": "1510600"
  },
  {
    "text": "of this from these\nmixture of expert models.  Also, we try to visualize in\nthree examples, which experts",
    "start": "1510600",
    "end": "1520809"
  },
  {
    "text": "actually selected which tokens? So we have three examples.",
    "start": "1520810",
    "end": "1526630"
  },
  {
    "text": "So the first example is\nactually from the GitHub code",
    "start": "1526630",
    "end": "1532090"
  },
  {
    "text": "for the layer. And the second example are\nsimple arithmetic questions",
    "start": "1532090",
    "end": "1537490"
  },
  {
    "text": "I think from DM mathematics. And the third is from a simple\nmultiple choice question.",
    "start": "1537490",
    "end": "1542659"
  },
  {
    "text": "And this is for layer\n0, layer 15, layer 31. You can see that there doesn't\nseem to be much specialization.",
    "start": "1542660",
    "end": "1552340"
  },
  {
    "text": "So here, a lot of digits get\nassigned to the same-- sorry so the colors represent\ndifferent experts.",
    "start": "1552340",
    "end": "1559909"
  },
  {
    "text": "So here, all these digits get\nassigned to the same expert, but this seems to be it.",
    "start": "1559910",
    "end": "1566029"
  },
  {
    "text": "We don't see a lot of very\nclear distinction between which experts like which tokens.",
    "start": "1566030",
    "end": "1572940"
  },
  {
    "text": " So that really closely\nrelates to myth 4.",
    "start": "1572940",
    "end": "1579830"
  },
  {
    "text": "That is you really-- the myth 4 is that\nyou want experts to specialize in domains.",
    "start": "1579830",
    "end": "1584900"
  },
  {
    "text": "Let's say you have two experts\nspecializing in coding, so that means they basically\nif you have some code,",
    "start": "1584900",
    "end": "1591530"
  },
  {
    "text": "they'll just handle\nall the tokens. Then what do the other\nexperts do when you're coding?",
    "start": "1591530",
    "end": "1597409"
  },
  {
    "text": "So when you want to\ngenerate a lot of code, it seems that you will\nonly have the chance",
    "start": "1597410",
    "end": "1602420"
  },
  {
    "text": "to root all your tokens\nto these two experts and the other experts\nstand there and do nothing.",
    "start": "1602420",
    "end": "1608659"
  },
  {
    "text": "So you actually\nwant all the experts to be fully engaged\nat all the time to maximize your\ninference efficiency.",
    "start": "1608660",
    "end": "1616120"
  },
  {
    "text": "Language also is so complex\nthat manually specifying these domains that experts\nshould specialize over",
    "start": "1616120",
    "end": "1623590"
  },
  {
    "text": "seems to be a simplification\nto me because there might be a lot of underlying features\nthat the experts are actually",
    "start": "1623590",
    "end": "1630010"
  },
  {
    "text": "specializing over and just not\npresent at the very high domain",
    "start": "1630010",
    "end": "1635440"
  },
  {
    "text": "level. There is a treasure hunt. So this is why we love\nopen-source so much.",
    "start": "1635440",
    "end": "1642800"
  },
  {
    "text": "So after 24 hours of our Mistral\n8 times 7B model release,",
    "start": "1642800",
    "end": "1649630"
  },
  {
    "text": "there's a guy on a Chinese\nwebsite that just found out that there's one expert in one of\nthe layers that's particularly",
    "start": "1649630",
    "end": "1656980"
  },
  {
    "text": "crucial. So what they did\nwas that they try to remove the i-th expert\nfrom all of the layers.",
    "start": "1656980",
    "end": "1668960"
  },
  {
    "text": "And here you can see,\nwhat are the effects if you remove the i-th expert?",
    "start": "1668960",
    "end": "1675496"
  },
  {
    "text": "On the vertical axis\nis the MMLU score. It seems that if you\nremove the third expert,",
    "start": "1675496",
    "end": "1683590"
  },
  {
    "text": "then everything just collapses. The MMLU score is 0.63. So this is 0.63%, not 63%.",
    "start": "1683590",
    "end": "1694100"
  },
  {
    "text": "And for other experts, if\nyou remove the i-th expert, the MMLU scores drop a\nlittle bit, but not too much.",
    "start": "1694100",
    "end": "1703620"
  },
  {
    "text": "And they made this\nmeme about it. So the expert 3 seems\nto be doing all the work while all the experts are\nstanding around and do not much.",
    "start": "1703620",
    "end": "1712580"
  },
  {
    "text": "OK, so I think another really\nimportant research question here is, how do we interpret the\nmixture of expert decisions?",
    "start": "1712580",
    "end": "1721010"
  },
  {
    "text": "And what are the features\nthat they're learning? So the experts might\ncapture features that are very different\nconcepts than what",
    "start": "1721010",
    "end": "1728990"
  },
  {
    "text": "we perceive as concepts. And it might be more\nefficient to represent linear combinations of\nconcepts as long as they",
    "start": "1728990",
    "end": "1736519"
  },
  {
    "text": "span the same subspace. And how do we\nrecover that subspace or how do we recover\nsome of the sets",
    "start": "1736520",
    "end": "1746360"
  },
  {
    "text": "that we can actually understand? What are the underlying concepts\nthat they're actually learning?",
    "start": "1746360",
    "end": "1751730"
  },
  {
    "text": "OK, so I just want to\nalso just conclude here that a sparse mixture\nof experts models",
    "start": "1751730",
    "end": "1759500"
  },
  {
    "text": "are leveraged sparsely to\ngain a lot more knowledge and you can train very good\nmixture of experts, models",
    "start": "1759500",
    "end": "1767510"
  },
  {
    "text": "to be quite efficient\nat inference. And expert specialization\nis not as straightforward as one might think.",
    "start": "1767510",
    "end": "1773370"
  },
  {
    "text": "And just there's tons\nof doing architecture and interpretability research.",
    "start": "1773370",
    "end": "1778669"
  },
  {
    "text": "And also I just want to plug\nMistral AI co-founded by author",
    "start": "1778670",
    "end": "1785752"
  },
  {
    "text": "Timothee and Guillaume. And we have offices in Paris,\nLondon, San Francisco, Bay Area.",
    "start": "1785752",
    "end": "1792350"
  },
  {
    "text": "Currently, it's actually in\nPalo Alto with $500 million in funding. And if you enjoy solving the\nresearch questions that I just",
    "start": "1792350",
    "end": "1800149"
  },
  {
    "text": "mentioned, or doing open-source\nAI, or just generally empowering people with language models,\nyou're very welcome to join us.",
    "start": "1800150",
    "end": "1809510"
  },
  {
    "text": "Thank you very much. If any of you all\nhave any questions, feel free to come up and ask.",
    "start": "1809510",
    "end": "1815850"
  },
  {
    "text": "So I recently read\nthe paper from Meta. They are using the\nintensive model.",
    "start": "1815850",
    "end": "1823410"
  },
  {
    "text": "Not this one, 28 I think. Page 28. OK. Yeah, so which is--",
    "start": "1823410",
    "end": "1829799"
  },
  {
    "text": "I feel like it's the other\ndirection, opposite way, why you choose the sparse\nmixture of experts",
    "start": "1829800",
    "end": "1836190"
  },
  {
    "text": "rather than the other people. Now, I see most of them\nusing the intensive models",
    "start": "1836190",
    "end": "1843460"
  },
  {
    "text": "to do some of an edge\ndatabase in a confined scope.",
    "start": "1843460",
    "end": "1849549"
  },
  {
    "text": "You can have more knowledges\nin a confined scope.",
    "start": "1849550",
    "end": "1855782"
  },
  {
    "text": "I see the train\ngoes to that way. For example, you go to the\nEgypt museum here in, I think",
    "start": "1855782",
    "end": "1863550"
  },
  {
    "text": "[INAUDIBLE] somewhere, so\nyou can have the ChatGPT have the local knowledge, which you\ncannot get it from the internet,",
    "start": "1863550",
    "end": "1872440"
  },
  {
    "text": "but you can only cooperate\nwith the museum and get it. I think the intensive model\nallows this more domain",
    "start": "1872440",
    "end": "1882700"
  },
  {
    "text": "knowledge going deeper. But now here you are\nusing the sparse model",
    "start": "1882700",
    "end": "1888190"
  },
  {
    "text": "with mixture of experts. To me, it's the opposite. Can you explain why?",
    "start": "1888190",
    "end": "1894850"
  },
  {
    "text": "Sure, so if I understand\nthe question correctly, the question is\nasking, why are we",
    "start": "1894850",
    "end": "1900760"
  },
  {
    "text": "choosing sparse mixture\nof experts, models instead of a dense models?",
    "start": "1900760",
    "end": "1906080"
  },
  {
    "text": "There's no sparsity involved. So I think for edge devices,\ndense models have a lot",
    "start": "1906080",
    "end": "1913519"
  },
  {
    "text": "of potential because for sparse\nmixture of expert models, although you have sparsity\nin inference time--",
    "start": "1913520",
    "end": "1920940"
  },
  {
    "text": "so that means a token does not\nhave to see all the parameters, but rather just a\nsmall fraction of it--",
    "start": "1920940",
    "end": "1926990"
  },
  {
    "text": "you still have to load all\nof the experts into memory and that might pose a huge\nchallenge for edge devices.",
    "start": "1926990",
    "end": "1934880"
  },
  {
    "text": "So your phone might\nnot have 200 of memory",
    "start": "1934880",
    "end": "1942570"
  },
  {
    "text": "in order to run inference\nvery efficiently. But I guess for data\ncenters or for people",
    "start": "1942570",
    "end": "1948770"
  },
  {
    "text": "who are serving these models, it\nmight pose a advantage in terms",
    "start": "1948770",
    "end": "1955790"
  },
  {
    "text": "of inference cost because you\ncan have very good performance",
    "start": "1955790",
    "end": "1962480"
  },
  {
    "text": "divided by cost ratio. I see. So your point is because there's\nnot enough memory at the edge,",
    "start": "1962480",
    "end": "1970750"
  },
  {
    "text": "so for the edge it's better\nto use sparse model rather than dense model?",
    "start": "1970750",
    "end": "1976780"
  },
  {
    "text": "Sorry, no. I meant for edge I\nthink it's good to use dense models because\nfor sparse models,",
    "start": "1976780",
    "end": "1984280"
  },
  {
    "text": "you still have to load\neverything into memory. Yeah, so your use\ncase is more for a",
    "start": "1984280",
    "end": "1991840"
  },
  {
    "text": "closer to the\ncenter of the cloud? That's what do you mean?",
    "start": "1991840",
    "end": "1997030"
  },
  {
    "text": "Yeah. OK, got it. Thank you.",
    "start": "1997030",
    "end": "2002270"
  },
  {
    "text": "Thank you. Does anyone else have a\nquestion they'd like to ask? Yes.",
    "start": "2002270",
    "end": "2009050"
  },
  {
    "text": "Do you want to-- ",
    "start": "2009050",
    "end": "2016279"
  },
  {
    "text": "I have a question in\nregards to fine tuning. We've been working with multiple\nmodels Gemini, ChatGPT, Llama,",
    "start": "2016280",
    "end": "2025880"
  },
  {
    "text": "and there is a big\nproblem of fine tuning these models, especially\non tensor, on images.",
    "start": "2025880",
    "end": "2033020"
  },
  {
    "text": "They are having issues\nwith understanding. What can you say about that?",
    "start": "2033020",
    "end": "2039500"
  },
  {
    "text": "You mean in general fine tuning\nlanguage models on visual tasks?",
    "start": "2039500",
    "end": "2044520"
  },
  {
    "text": "Yeah.  OK, I guess that's a\nvery broad question.",
    "start": "2044520",
    "end": "2051550"
  },
  {
    "text": "What I can say about\nthat is there's-- I guess for ChatGPT and\nGemini, you don't really",
    "start": "2051550",
    "end": "2059530"
  },
  {
    "text": "have the model\nweights in your hand, so you are basically\nrelying on OpenAI or Google",
    "start": "2059530",
    "end": "2065050"
  },
  {
    "text": "to do more for you. So I guess you have less\ncontrol over your fine tuning,",
    "start": "2065050",
    "end": "2072760"
  },
  {
    "text": "whereas for an open-source model\nor I guess open-based model, you actually have\nweights in your hands.",
    "start": "2072760",
    "end": "2077840"
  },
  {
    "text": "So you can do Llama, you\ncan do full fine tuning. I guess you get more control. And yeah, I guess\nthat's in general",
    "start": "2077840",
    "end": "2086919"
  },
  {
    "text": "what I can say about fine tuning\nopen-source models versus closed",
    "start": "2086920",
    "end": "2092379"
  },
  {
    "text": "source models. But I guess that\nreally highly depends-- exactly what happens when\nyou're doing fine tuning really",
    "start": "2092380",
    "end": "2098140"
  },
  {
    "text": "depends on your use\ncase and your data. Yeah, I mean, I'm sorry\nI was a little broad.",
    "start": "2098140",
    "end": "2103369"
  },
  {
    "text": "So when we start transferring\nthem into a large data sets,",
    "start": "2103370",
    "end": "2109550"
  },
  {
    "text": "let's say 10,000 images or 5,000\nimages and you have to transfer",
    "start": "2109550",
    "end": "2115125"
  },
  {
    "text": "them into textual format\nbecause none of these models are actually tuned designed to\nbe fine tuned on images,",
    "start": "2115125",
    "end": "2120750"
  },
  {
    "text": "they don't understand it. So is this something\nyou see in development",
    "start": "2120750",
    "end": "2126320"
  },
  {
    "text": "in open-source models?  So by translating\nthese images into text,",
    "start": "2126320",
    "end": "2135849"
  },
  {
    "text": "you mean OCR or is\nthat something-- Yeah, digital format,\nthe textual format.",
    "start": "2135850",
    "end": "2141050"
  },
  {
    "text": "So they do understand\nthem as an image if you upload them into\ntext line you go to ChatGPT.",
    "start": "2141050",
    "end": "2148300"
  },
  {
    "text": "But when you start feeding\nthem through an APIs, you need to transfer them,\nso then it becomes a problem.",
    "start": "2148300",
    "end": "2154090"
  },
  {
    "text": "Is there any way to\novercome this issue with open-source models? ",
    "start": "2154090",
    "end": "2163269"
  },
  {
    "text": "I guess I'm not entirely\nsure what the root cause of this problem is.",
    "start": "2163270",
    "end": "2168640"
  },
  {
    "text": "But I think I'm sure that\nwith open-source models, you can actually-- well, you can\nsee what the waves are doing.",
    "start": "2168640",
    "end": "2174819"
  },
  {
    "text": "So you can actually\ntell what are-- you have more control\nof the waves basically.",
    "start": "2174820",
    "end": "2181589"
  },
  {
    "text": "So I think [INAUDIBLE]\nis definitely post the voltage here. ",
    "start": "2181590",
    "end": "2190790"
  },
  {
    "text": "Thank you. Thank you for the question. Does anyone else have any\nother questions in-person.",
    "start": "2190790",
    "end": "2198240"
  },
  {
    "text": "Yeah, I guess I\nwas curious where you think the improvements\nof mixture of experts",
    "start": "2198240",
    "end": "2204390"
  },
  {
    "text": "are coming from. I've heard that you can\nput different experts",
    "start": "2204390",
    "end": "2209830"
  },
  {
    "text": "or different feedforward\nnetworks on different GPUs and that allows it\nto be more parallel",
    "start": "2209830",
    "end": "2216640"
  },
  {
    "text": "and speed things up from\na compute perspective or is it more of\nthe sparsity itself",
    "start": "2216640",
    "end": "2222680"
  },
  {
    "text": "that is giving\nthese improvements? I guess, is there\nany insight on that?",
    "start": "2222680",
    "end": "2228750"
  },
  {
    "text": "I guess there are two\ndimensions to this. So in terms of improvement, the\none dimension is performance.",
    "start": "2228750",
    "end": "2236910"
  },
  {
    "text": "So as I said, the\nmixture of experts models have these MLP layers.",
    "start": "2236910",
    "end": "2243970"
  },
  {
    "text": "So you imagine your\noriginal MLP layers and then you make it\neight times wider,",
    "start": "2243970",
    "end": "2249000"
  },
  {
    "text": "you will be able to store a lot\nmore knowledge into these layers because you have more\nparameter counts.",
    "start": "2249000",
    "end": "2255720"
  },
  {
    "text": "So that's I guess on the\nperformance dimension that you can actually\nstore a lot more knowledge",
    "start": "2255720",
    "end": "2263100"
  },
  {
    "text": "into these models. And on the performance-- sorry, and the other dimension\nis the inference efficiency",
    "start": "2263100",
    "end": "2269790"
  },
  {
    "text": "dimension. So definitely go read the\nswitch transformer paper",
    "start": "2269790",
    "end": "2275130"
  },
  {
    "text": "where they discuss-- OK, I guess that's\ntraining efficiency, but they really discussed\nquite a lot of details",
    "start": "2275130",
    "end": "2283290"
  },
  {
    "text": "about data parallelism,\nmodel parallelism, and expert parallelism and how they\naffect the efficiency.",
    "start": "2283290",
    "end": "2290880"
  },
  {
    "text": "What's the communication\ncost there? What's the best way\nto train your models? And for these sparse mixture of\nexperts models since you only--",
    "start": "2290880",
    "end": "2301869"
  },
  {
    "text": "so you only select\n13 billion parameters for each token at\ninference time.",
    "start": "2301870",
    "end": "2308170"
  },
  {
    "text": "You are actually trying to\nselect the most relevant parameters for each token. So that can make inference\nquite a lot more efficient.",
    "start": "2308170",
    "end": "2318819"
  },
  {
    "text": "I hope that answered\nyour question. Yeah, no, that definitely did. I guess I was also\nwondering if you've",
    "start": "2318820",
    "end": "2325750"
  },
  {
    "text": "seen like the mixture\nof depths paper and that as a way of sparsity, is\nthat-- do you have any thoughts",
    "start": "2325750",
    "end": "2332680"
  },
  {
    "text": "on that? Yeah, I think\nsparsity helps when you can adaptive computation.",
    "start": "2332680",
    "end": "2340579"
  },
  {
    "text": "So the mixture of depth\nmodel is the best example",
    "start": "2340580",
    "end": "2345880"
  },
  {
    "text": "of adaptive\ncomputation, which means",
    "start": "2345880",
    "end": "2351400"
  },
  {
    "text": "for predicting different\ntokens you want, first, difference\nparameters to engage.",
    "start": "2351400",
    "end": "2358130"
  },
  {
    "text": "So in the mixture\nof depth paper, they selected different\nnumber of parameters",
    "start": "2358130",
    "end": "2365260"
  },
  {
    "text": "to engage in\ncalculating each token. And in our paper we\nselect different,",
    "start": "2365260",
    "end": "2372760"
  },
  {
    "text": "the same number of parameters,\nbut different experts.",
    "start": "2372760",
    "end": "2377810"
  },
  {
    "text": "So the difference is I guess\nquantity versus quality.",
    "start": "2377810",
    "end": "2384310"
  },
  {
    "text": "But they are two\ndifferent dimensions you can optimize over. You definitely want\nto have to select",
    "start": "2384310",
    "end": "2390040"
  },
  {
    "text": "the most relevant parameters\nand as few parameters as possible for\ndecoding your token.",
    "start": "2390040",
    "end": "2396920"
  },
  {
    "text": "Yeah, OK. Yeah, I think that's\nall my questions. Perfect, thank you.",
    "start": "2396920",
    "end": "2402690"
  },
  {
    "text": "We'll take one more in-person\nquestion if any of you all have anything. If not, we can turn it\nto the online questions.",
    "start": "2402690",
    "end": "2411845"
  },
  {
    "text": "Yes. ",
    "start": "2411845",
    "end": "2420700"
  },
  {
    "text": "Hey, thanks for the talk. Just one in the-- earlier the\nbeginning of the presentation",
    "start": "2420700",
    "end": "2426460"
  },
  {
    "text": "you mentioned, like routing\nand communication cost and I was wondering\nif you could talk about how that scales relative\nto the number of experts",
    "start": "2426460",
    "end": "2434440"
  },
  {
    "text": "you have in parameters. Yeah, sure.",
    "start": "2434440",
    "end": "2440640"
  },
  {
    "text": "So that depends on the, first,\nthe number of experts you have and also depends on how\nlarge each expert is.",
    "start": "2440640",
    "end": "2449130"
  },
  {
    "text": "So that can change the\nway you do parallelism. So we know that communication\nis really expensive",
    "start": "2449130",
    "end": "2456900"
  },
  {
    "text": "when you need to go\nfrom one to another and it's more expensive if\nyou want to go from one node to another.",
    "start": "2456900",
    "end": "2462059"
  },
  {
    "text": "And if you have a huge ton\nof experts that will not fit into just one\nnode, then you really",
    "start": "2462060",
    "end": "2470500"
  },
  {
    "text": "incur quite a big\ncommunication cost. And like how to scale\nbeyond that I guess",
    "start": "2470500",
    "end": "2478560"
  },
  {
    "text": "is a very open\nscientific questions. But essentially,\nthe communication",
    "start": "2478560",
    "end": "2485620"
  },
  {
    "text": "costs you incur is roughly\nproportional to the number",
    "start": "2485620",
    "end": "2491440"
  },
  {
    "text": "of token routing for between\nGPUs and then between nodes.",
    "start": "2491440",
    "end": "2497410"
  },
  {
    "text": "Thank you. OK, thank you for\nall the questions. We'll have Steven introduce\nsome of the questions that",
    "start": "2497410",
    "end": "2505450"
  },
  {
    "text": "were submitted by the\npeople joining us on Zoom. ",
    "start": "2505450",
    "end": "2511680"
  },
  {
    "text": "Hey. Yeah, can you hear me? Yeah. Thanks for the great talk.",
    "start": "2511680",
    "end": "2516870"
  },
  {
    "text": "Thanks. All right, so we\nhave some questions through Zoom and Slido. I'm going to pick and\nchoose some of them starting",
    "start": "2516870",
    "end": "2523890"
  },
  {
    "text": "with some Zoom questions maybe. So we have a question\nhere about, \"Any comments",
    "start": "2523890",
    "end": "2529890"
  },
  {
    "text": "on why potentially Lambda3 is\nnot using a mixture of experts? A lot of people suspected they\nmight after MySQL success.\"",
    "start": "2529890",
    "end": "2538530"
  },
  {
    "text": "I don't know if you would\nknow anything about that. Yeah, don't know. Just ask the Lambda\npeople, sorry.",
    "start": "2538530",
    "end": "2545400"
  },
  {
    "text": "Right. Let me see.",
    "start": "2545400",
    "end": "2550849"
  },
  {
    "text": "Here's a question. \"Do you foresee\nmixture of experts, techniques being incorporated\ninto the other large foundation",
    "start": "2550850",
    "end": "2557750"
  },
  {
    "text": "models or will it remain a\nsubset of models that are best for certain use cases?\"",
    "start": "2557750",
    "end": "2564940"
  },
  {
    "text": "Certainly, I think as my answer\nto the first in-person question",
    "start": "2564940",
    "end": "2570460"
  },
  {
    "text": "was, for as devices\nyou probably won't want to stick with dense\nmodels because of the memory",
    "start": "2570460",
    "end": "2576640"
  },
  {
    "text": "constraints. And for mixture\nof experts models, you really gain a\nlot in efficiency",
    "start": "2576640",
    "end": "2582550"
  },
  {
    "text": "if you can serve it at scale. So if you have high\nbatch sizes, you'll",
    "start": "2582550",
    "end": "2590290"
  },
  {
    "text": "potentially be doing better\nin terms of throughput than the dense models.",
    "start": "2590290",
    "end": "2595480"
  },
  {
    "text": "So yeah, it definitely\ndepends on your use case. Usually the larger the\nscale, the better--",
    "start": "2595480",
    "end": "2603820"
  },
  {
    "text": "well, I guess the more\nthe benefit of the mixture of experts model shine. And it's speculated that GPT\nis a mixture of experts model",
    "start": "2603820",
    "end": "2612310"
  },
  {
    "text": "if you haven't heard already. \"Can a mixture of experts models\noutperform domain specific",
    "start": "2612310",
    "end": "2617660"
  },
  {
    "text": "models at their respective\ntasks with experts?\"",
    "start": "2617660",
    "end": "2622990"
  },
  {
    "text": "So well, first,\nusually models that",
    "start": "2622990",
    "end": "2628210"
  },
  {
    "text": "have been trained for particular\ndomains are really hard to beat. And I think that's\nwhere also where",
    "start": "2628210",
    "end": "2634119"
  },
  {
    "text": "the purpose of continuous\npre-training and fine tuning comes in.",
    "start": "2634120",
    "end": "2639640"
  },
  {
    "text": "Let's say you first portray\na general text model and then you want to adapt\nit to the medical domain,",
    "start": "2639640",
    "end": "2644800"
  },
  {
    "text": "so you grab a lot\nof medical data and then you continuous\npre-training or you fine tune that, that model is going\nto be really hard to beat.",
    "start": "2644800",
    "end": "2651460"
  },
  {
    "text": "And I think the mixture\nof experts model, the experts don't really,\nat least in our case here,",
    "start": "2651460",
    "end": "2658690"
  },
  {
    "text": "the experts don't really\nfocus on traditional domains as we know it.",
    "start": "2658690",
    "end": "2663720"
  },
  {
    "text": "We don't have a medical expert. We don't have a coding expert. Rather they just try to extend--",
    "start": "2663720",
    "end": "2670820"
  },
  {
    "text": "they have concepts encoded in\na very non-interpretable way.",
    "start": "2670820",
    "end": "2676360"
  },
  {
    "text": "So I wouldn't say\nthat just taking one mixture of expert\nmodel will just",
    "start": "2676360",
    "end": "2681370"
  },
  {
    "text": "outperform all the other\ndomain focused models.",
    "start": "2681370",
    "end": "2687056"
  },
  {
    "text": "All right, that makes sense. Here's a question. \"Has there been any study on\nwhether the MMLU layer should",
    "start": "2687056",
    "end": "2694190"
  },
  {
    "text": "be both at the early\nlayers and deep layers? For old neural network ensemble\nmethods late fusion tends",
    "start": "2694190",
    "end": "2700700"
  },
  {
    "text": "to work better\nthan early fusion.\" ",
    "start": "2700700",
    "end": "2705900"
  },
  {
    "text": "OK. I think that's a really great\nquestion because I think from the inception for\nthe neural network,",
    "start": "2705900",
    "end": "2713540"
  },
  {
    "text": "we are trying to adopt this-- we have this tradition of\nhaving layers that look exactly",
    "start": "2713540",
    "end": "2723050"
  },
  {
    "text": "the same as at each other. And so that you can try to focus\non just designing this layer",
    "start": "2723050",
    "end": "2731180"
  },
  {
    "text": "to be as perfect as\npossible and then you can just copy paste\nthat a couple of times. But I think I've recently\nseen a paper, sorry,",
    "start": "2731180",
    "end": "2738860"
  },
  {
    "text": "I don't remember the\nname, but I think they're just trying to put\nlayers in random orders.",
    "start": "2738860",
    "end": "2744495"
  },
  {
    "text": "So sometimes it can\nbe attention first, sometimes it can be MMLU\nfirst and they can have",
    "start": "2744495",
    "end": "2750020"
  },
  {
    "text": "also some other wacky layers. But I think that\nperforms quite well.",
    "start": "2750020",
    "end": "2755180"
  },
  {
    "text": "It's unclear to me why that is. It could be just chance.",
    "start": "2755180",
    "end": "2760490"
  },
  {
    "text": "But I guess if you want to\ndesign, a generally really",
    "start": "2760490",
    "end": "2766280"
  },
  {
    "text": "perform really\nperformant model, it's really safe to have the same\narchitecture for every layer.",
    "start": "2766280",
    "end": "2773990"
  },
  {
    "text": "So yeah, so I guess\nthat's the safest choice. You can also do some\nneural architecture search",
    "start": "2773990",
    "end": "2780500"
  },
  {
    "text": "in order to optimize your layer\nor optimize your architecture, but that's a--",
    "start": "2780500",
    "end": "2786200"
  },
  {
    "text": "which is a much more\nprincipled approach than guessing that this\nlayer might work better. ",
    "start": "2786200",
    "end": "2793340"
  },
  {
    "text": "All right, thanks\nfor the answer. ",
    "start": "2793340",
    "end": "2798510"
  },
  {
    "text": "Asking, \"Could you talk a bit\nmore about how the eight experts are built or are they just fine\ntuned on different data sets?\"",
    "start": "2798510",
    "end": "2808030"
  },
  {
    "text": "So they are trained on\nroughly the same data sets.",
    "start": "2808030",
    "end": "2815590"
  },
  {
    "text": "But I'm not sure if I\ncan say more than that. But, yeah, sorry.",
    "start": "2815590",
    "end": "2821830"
  },
  {
    "text": "And here's a question. \"Some work has suggested\nthat learning to root doesn't perform any\nbetter than simply using,",
    "start": "2821830",
    "end": "2827319"
  },
  {
    "text": "for example, a random\nmapping of inputs to experts. Do you have any thoughts?\"",
    "start": "2827320",
    "end": "2834180"
  },
  {
    "text": "I think that I would definitely\nneed to see the paper first,",
    "start": "2834180",
    "end": "2841020"
  },
  {
    "text": "but I wouldn't be too\nsurprised if that's the case. I would still be surprised,\nbut not extremely surprised",
    "start": "2841020",
    "end": "2847290"
  },
  {
    "text": "because if you think about\nMoE-fying the MLP layers",
    "start": "2847290",
    "end": "2852660"
  },
  {
    "text": "as augmenting the\nknowledge capacity, then it seems that you can\npotentially just do that",
    "start": "2852660",
    "end": "2860760"
  },
  {
    "text": "in a very brute force way and\njust randomly mapping the tokens to random layer--",
    "start": "2860760",
    "end": "2866280"
  },
  {
    "text": "sorry, to random experts. I would still think\nthat the gating has the advantage of being\nable to choose experts more",
    "start": "2866280",
    "end": "2878660"
  },
  {
    "text": "intelligently than\njust randomly. So yeah, I don't\nknow that paper,",
    "start": "2878660",
    "end": "2884329"
  },
  {
    "text": "but I would love to see it. Someone was asking,\n\"Could you speak a bit about the general\ndevelopment process",
    "start": "2884330",
    "end": "2890450"
  },
  {
    "text": "from your perspective\nfor this model? What aspects, design choices,\nhyperparameters, and so forth?",
    "start": "2890450",
    "end": "2896450"
  },
  {
    "text": "Did you try that did not work\nbefore arriving at this specific architecture?\" ",
    "start": "2896450",
    "end": "2905360"
  },
  {
    "text": "So for this particular\narchitecture, the mixture of experts,\narchitecture, we know",
    "start": "2905360",
    "end": "2911359"
  },
  {
    "text": "that it will work\nbecause they're having a lot of successful\npapers beforehand.",
    "start": "2911360",
    "end": "2918350"
  },
  {
    "text": "In terms of what things\nyou need to consider, I think one really\ngood practice,",
    "start": "2918350",
    "end": "2923450"
  },
  {
    "text": "I guess for AI\ncompanies that have as a single developer\nis to always consi--",
    "start": "2923450",
    "end": "2930380"
  },
  {
    "text": "you always want to\ntake inference needs.",
    "start": "2930380",
    "end": "2936170"
  },
  {
    "text": "You want to think\nabout inference needs before you design the\narchitecture for a model. You don't want to--",
    "start": "2936170",
    "end": "2942040"
  },
  {
    "text": "so, for example, you don't\nwant to have a model that",
    "start": "2942040",
    "end": "2947060"
  },
  {
    "text": "just slightly exceeds what a\nsingle 80 gigabyte A100 can",
    "start": "2947060",
    "end": "2952730"
  },
  {
    "text": "contain and then you basically\nlose a lot of efficiency from just being the\noverhead being just a little",
    "start": "2952730",
    "end": "2959059"
  },
  {
    "text": "bit more than 8 gigabytes. So I think in terms of\ndesign, before training",
    "start": "2959060",
    "end": "2965670"
  },
  {
    "text": "the model you definitely\nwant to consider,",
    "start": "2965670",
    "end": "2971250"
  },
  {
    "text": "how to infer with this model? And in terms of the\nhyperparameters,",
    "start": "2971250",
    "end": "2977040"
  },
  {
    "text": "you definitely want to do\nsome scaling law before you--",
    "start": "2977040",
    "end": "2982380"
  },
  {
    "text": "some scaling law, I\nguess search and analysis before you train a model to\nmake sure that your model is",
    "start": "2982380",
    "end": "2988440"
  },
  {
    "text": "served at the best\nperformance cost ratio.",
    "start": "2988440",
    "end": "2994079"
  },
  {
    "text": "So yeah, that's it. Great.",
    "start": "2994080",
    "end": "2999240"
  },
  {
    "text": "Speaking of GPU memory,\nsomeone is asking, \"What is the inference runtime\nGPU memory footprint for the 7B",
    "start": "2999240",
    "end": "3007370"
  },
  {
    "text": "model versus the 8 by 7B? It would be great to understand\nthis for specific applications,",
    "start": "3007370",
    "end": "3012800"
  },
  {
    "text": "especially if Mistral has the\npotential to run on edge devices with sufficient GPU URAM.\"",
    "start": "3012800",
    "end": "3020780"
  },
  {
    "text": "Sorry, I don't think I got\nthe last part of the question. So edge devices and GPU?",
    "start": "3020780",
    "end": "3026990"
  },
  {
    "text": "No, they're just asking, what\nis the approximate inference runtime in terms of GPU memory\nfootprint for the 7B model",
    "start": "3026990",
    "end": "3035750"
  },
  {
    "text": "versus the 8 by 7B? Ah, OK. I see. So if you do, very naive--",
    "start": "3035750",
    "end": "3044403"
  },
  {
    "text": " if you do it very\nstraightforwardly,",
    "start": "3044403",
    "end": "3049580"
  },
  {
    "text": "that you load for the 8 times\n7B to all the experts in. And then the experts\nalways stay in GPU memory.",
    "start": "3049580",
    "end": "3059480"
  },
  {
    "text": "Then the difference between the\n8 times 7B and the 7B will be",
    "start": "3059480",
    "end": "3065210"
  },
  {
    "text": "the same as the ratio\nbetween 46.7 to 7. That's the memory requirement.",
    "start": "3065210",
    "end": "3072350"
  },
  {
    "text": "But at inference time since the\nmixture of experts model only has 13B active parameters, so\neach token gets to see much",
    "start": "3072350",
    "end": "3083000"
  },
  {
    "text": "fewer parameters than\nthere are in the memory. And you can do some\ninteresting things",
    "start": "3083000",
    "end": "3089350"
  },
  {
    "text": "like I think CPU,\nforgot the name for it, but you can keep some\nof the experts in CPU",
    "start": "3089350",
    "end": "3096680"
  },
  {
    "text": "and only load them\nwhen you need them. So I think that's something\nyou can do as well.",
    "start": "3096680",
    "end": "3102080"
  },
  {
    "text": "But then you also\nlose some efficiency because you need to\nconstantly transfer parameters",
    "start": "3102080",
    "end": "3108230"
  },
  {
    "text": "from CPU to GPU and vice versa. All right, that makes sense. Speaking of parameters,\nsomeone asked, \"Do you",
    "start": "3108230",
    "end": "3114230"
  },
  {
    "text": "have a rough rule\nof thumb for how to approximate how capable an\nMoE model would be in terms",
    "start": "3114230",
    "end": "3120830"
  },
  {
    "text": "of equivalent dense parameters. For example, I've heard that\ncalculating the geometric mean",
    "start": "3120830",
    "end": "3126170"
  },
  {
    "text": "of active parameters\nto total parameters is a good way to do\nthis, which would mean that 13 billion active\nparameters with 47",
    "start": "3126170",
    "end": "3134340"
  },
  {
    "text": "billion total parameters would\nbe equivalent to a 22 billion parameters dense model.",
    "start": "3134340",
    "end": "3140910"
  },
  {
    "text": "I'm wondering how\naccurate that would be.\" So I think that's a\ngood rule of thumb,",
    "start": "3140910",
    "end": "3149320"
  },
  {
    "text": "although that definitely\ndepends on how well you train these models and how many\ntokens do you put through them?",
    "start": "3149320",
    "end": "3159609"
  },
  {
    "text": "And how good are those tokens? If all other things\nbeing equal, I think that's a very\ngood rule of thumb.",
    "start": "3159610",
    "end": "3167580"
  },
  {
    "text": "I agreed. And I'm just looking at some\nof the upvoted Slido questions. Someone asks, \"For the\ntreasure hunted expert,",
    "start": "3167580",
    "end": "3174790"
  },
  {
    "text": "was there a reverse experiment\nof removing all experts except number 3?\" ",
    "start": "3174790",
    "end": "3182226"
  },
  {
    "text": "That's a great question. I think, yeah, if you can figure\nthat out I'll be all yours.",
    "start": "3182226",
    "end": "3187360"
  },
  {
    "text": "I'll be really\nhappy to hear that. So then you're just trying to-- I guess you're trying to trim\nthe 8 times 7B back into a 7B,",
    "start": "3187360",
    "end": "3198270"
  },
  {
    "text": "which might be an\ninteresting experiment to do. ",
    "start": "3198270",
    "end": "3203860"
  },
  {
    "text": "All right. Someone asked, \"What is your\nintuition about why the 8 by 7B",
    "start": "3203860",
    "end": "3208960"
  },
  {
    "text": "is significantly better at\nreasoning compared to the 7B? Are they learning better\ninternal algorithms?\"",
    "start": "3208960",
    "end": "3217180"
  },
  {
    "text": "That could be the case, although\nI think that's very speculative. So I think for a lot\nof these benchmarks,",
    "start": "3217180",
    "end": "3224830"
  },
  {
    "text": "for example, even for\nthe math benchmark, the--",
    "start": "3224830",
    "end": "3230860"
  },
  {
    "text": "what you do-- what the\nthings count as knowledge and what the things count as\nreasoning is quite ambiguous.",
    "start": "3230860",
    "end": "3238640"
  },
  {
    "text": "For example, if you\nare doing a math task and you can reason\nto get something out or you can just recall here's a\nLlama and I can just use that.",
    "start": "3238640",
    "end": "3247040"
  },
  {
    "text": "And so I think there's\na high ambiguity between what things\ncount as knowledge",
    "start": "3247040",
    "end": "3252309"
  },
  {
    "text": "and what things\ncounts as reasoning. So for this particular example,\nthe 8 times 7B definitely gains",
    "start": "3252310",
    "end": "3261190"
  },
  {
    "text": "a lot more knowledge. But does the gaining\nknowledge also",
    "start": "3261190",
    "end": "3267640"
  },
  {
    "text": "induce some change in\nits reasoning capability? I don't know. But I think that's just--",
    "start": "3267640",
    "end": "3274210"
  },
  {
    "text": "whatever I can say about\nit is highly speculative. But I'll say the benchmarks\nthemselves are also",
    "start": "3274210",
    "end": "3279640"
  },
  {
    "text": "very ambiguous. So I think it's worth finding\nout what exactly is doing.",
    "start": "3279640",
    "end": "3285920"
  },
  {
    "text": "Great. Another question\nabout GPU memory. Someone saying, \"In production\nwe're constantly bounded by GPU",
    "start": "3285920",
    "end": "3292450"
  },
  {
    "text": "memory instead of compute, will\nmixture of experts make it more challenging to serve any extra\ncost besides communication",
    "start": "3292450",
    "end": "3300340"
  },
  {
    "text": "overhead?\" So in terms of costs,\nI think communication--",
    "start": "3300340",
    "end": "3306270"
  },
  {
    "text": "yeah, so when you say you're\nbounded by computer memory, I assume that it's comparing\nthe 8 times 7B to 7B then, yes.",
    "start": "3306270",
    "end": "3315960"
  },
  {
    "text": "If you need to load all the\nexperts into the memory, then yeah, you're\ngoing to consume,",
    "start": "3315960",
    "end": "3321290"
  },
  {
    "text": "you're going to use a lot more. So the benefit or the advantage\nof the 8 times 7B over the seven",
    "start": "3321290",
    "end": "3334640"
  },
  {
    "text": "be is that at-- or let's say over an\nequivalent dense models",
    "start": "3334640",
    "end": "3342680"
  },
  {
    "text": "with similar active parameter\ncount is that at high batch size",
    "start": "3342680",
    "end": "3348530"
  },
  {
    "text": "you'll get more\nthroughput because you'll get different experts handling\nthe tokens at the same time.",
    "start": "3348530",
    "end": "3357620"
  },
  {
    "text": "So I'd say, yes,\nmixture of experts model definitely gives you a\nlittle bit more trouble in terms",
    "start": "3357620",
    "end": "3365380"
  },
  {
    "text": "of serving. But if you have high volume,\nit's definitely worth it",
    "start": "3365380",
    "end": "3370690"
  },
  {
    "text": "because you get\nmuch more efficient in processing these tokens. OK, great.",
    "start": "3370690",
    "end": "3376950"
  },
  {
    "text": "Here, I'll ask a few more since\nwe do have some extra time. But do let me know when\nyou might need to leave.",
    "start": "3376950",
    "end": "3384616"
  },
  {
    "text": "Sure. So someone is\nasking specifically about the architecture. They want you to clarify, \"Does\neach layer include attention",
    "start": "3384616",
    "end": "3392370"
  },
  {
    "text": "routing and experts or layers\nof attention, then routing, then experts?\"",
    "start": "3392370",
    "end": "3399349"
  },
  {
    "text": "OK, let me try to find it. ",
    "start": "3399350",
    "end": "3404880"
  },
  {
    "text": "Yes, so the architecture of\nMistral 8 times 7B is exactly",
    "start": "3404880",
    "end": "3413220"
  },
  {
    "text": "the same as Mistral\n7B as I've shown here. So in Mistral 7B, you first\ndo for each of the input,",
    "start": "3413220",
    "end": "3421920"
  },
  {
    "text": "you do normalization, you do\nattention, residual, norm, MLP, residual.",
    "start": "3421920",
    "end": "3427809"
  },
  {
    "text": "So it's attention\nfollowed by MLP. And the only difference\nis in the layer here.",
    "start": "3427810",
    "end": "3436590"
  },
  {
    "text": "So after the attention layer and\nafter the attention residual, instead of doing MLP,\ndo an MoE and MLP here.",
    "start": "3436590",
    "end": "3443950"
  },
  {
    "text": "So here instead\nof just using one matrix to process this hidden--",
    "start": "3443950",
    "end": "3454490"
  },
  {
    "text": "this latent representation\nyou use eight experts. You choose two out of eight\nexperts to process that.",
    "start": "3454490",
    "end": "3461060"
  },
  {
    "text": " Right, that makes sense.",
    "start": "3461060",
    "end": "3467859"
  },
  {
    "text": "All right, let's see.  Give me a second.",
    "start": "3467860",
    "end": "3473529"
  },
  {
    "text": " Last, Emily introduces\nextra load balancing losses",
    "start": "3473530",
    "end": "3482670"
  },
  {
    "text": "and loss function\ndiscontinuities. \"Did you run into any\ndifficulties while training due",
    "start": "3482670",
    "end": "3490440"
  },
  {
    "text": "to these complications?\" Yeah, that's a\nreally good question. So I think to try to\nclarify the question,",
    "start": "3490440",
    "end": "3498760"
  },
  {
    "text": "my understanding\nis that when you're training a mixture\nof experts models, you definitely want\neach of your experts",
    "start": "3498760",
    "end": "3504820"
  },
  {
    "text": "to be quite balanced\nin the sense that they handle a similar amount of\ntokens such that you're not--",
    "start": "3504820",
    "end": "3513339"
  },
  {
    "text": "I guess I was waiting for the\nslowest expert or the expert that handles the most.",
    "start": "3513340",
    "end": "3518890"
  },
  {
    "text": "So yes, you definitely\nneed to do something to make the load balance\na bit better in training.",
    "start": "3518890",
    "end": "3527240"
  },
  {
    "text": "But we didn't run\ninto any big troubles",
    "start": "3527240",
    "end": "3533560"
  },
  {
    "text": "during training in this model.  And here's a question about RAG.",
    "start": "3533560",
    "end": "3542450"
  },
  {
    "text": "\"Just to hear when an MoE\napproach would be preferred to a right approach in a\ngiven domain pros and cons,",
    "start": "3542450",
    "end": "3549880"
  },
  {
    "text": "et cetera.\" I think these are orthogonal. So you can use an\nMoE model to do RAG.",
    "start": "3549880",
    "end": "3559100"
  },
  {
    "text": "So I don't see why-- I don't see why there should\nbe a conflict between the two,",
    "start": "3559100",
    "end": "3564140"
  },
  {
    "text": "yeah. So you can do dense model,\nno RAG, dense model RAG,",
    "start": "3564140",
    "end": "3570270"
  },
  {
    "text": "MoE no RAG, MoE RAG. Right. And someone asked, \"Can you\npotentially swap out one expert",
    "start": "3570270",
    "end": "3579540"
  },
  {
    "text": "and insert a domain specific\nexpert that wasn't trained on the same training data set\nlike a customizable or modular",
    "start": "3579540",
    "end": "3587050"
  },
  {
    "text": "MoE?\" I see. I think that's possible given\nthat you also need to-- so after",
    "start": "3587050",
    "end": "3597220"
  },
  {
    "text": "you swap out one of the experts\nand replace it with a domain",
    "start": "3597220",
    "end": "3604090"
  },
  {
    "text": "specific expert, after\nyou do the swapping, you definitely need to train\nthis model a bit more such",
    "start": "3604090",
    "end": "3615400"
  },
  {
    "text": "as the gating layers\nknow how to route the-- let's say you have a\nmedical expert swapped in.",
    "start": "3615400",
    "end": "3623619"
  },
  {
    "text": "You need to train your\ngating layers a little bit such that it knows how to\nhandle medical situations",
    "start": "3623620",
    "end": "3630670"
  },
  {
    "text": "and to basically treat\nthis replaced expert quite differently. So I think it's totally\npossible and very exciting",
    "start": "3630670",
    "end": "3637780"
  },
  {
    "text": "research direction. They're being quite\ninteresting research done on merging and swapping\nand Franken-merge these models.",
    "start": "3637780",
    "end": "3647060"
  },
  {
    "text": "So I think that's something,\nsomething very exciting I'm looking forward to it. ",
    "start": "3647060",
    "end": "3655560"
  },
  {
    "text": "OK, great. I'll ask a two more\nquestions and that'll be it. So someone asked,\n\"During training,",
    "start": "3655560",
    "end": "3661680"
  },
  {
    "text": "is mixture of experts less\ncomputationally intensive since the gradients\nwould always be 7B?",
    "start": "3661680",
    "end": "3668910"
  },
  {
    "text": "How do the gradients back\npropagate through the routing then?\" So if I go to the routing layer,\nyou will see that the top--",
    "start": "3668910",
    "end": "3681650"
  },
  {
    "text": "so all the operations\nhere are differentiable. So there's no I guess\ndiscrete operations such",
    "start": "3681650",
    "end": "3691850"
  },
  {
    "text": "that the gradient\njust stops there. So this thing is entirely\nend-to-end differentiable.",
    "start": "3691850",
    "end": "3700700"
  },
  {
    "text": "Sorry, what's the first\npart of the question, again? They're asking if\ntraining MoEs is less",
    "start": "3700700",
    "end": "3706560"
  },
  {
    "text": "computationally intensive. So the cost of training\nMoEs is roughly",
    "start": "3706560",
    "end": "3712750"
  },
  {
    "text": "proportional to the number of\nactive parameters you have. And so it's roughly\nequivalent to training a 13B.",
    "start": "3712750",
    "end": "3721660"
  },
  {
    "text": "But you incur some extra\ncommunication cost. ",
    "start": "3721660",
    "end": "3729550"
  },
  {
    "text": "Yeah, great. Last question. Let's talk about even\nbigger MoE models.",
    "start": "3729550",
    "end": "3736840"
  },
  {
    "text": "Even if we're moving away from\nthe Pareto frontier like those 8 by 22, 8 by 30, or even 8 by 100\nplus billion parameter models,",
    "start": "3736840",
    "end": "3745570"
  },
  {
    "text": "do you see any further serving\nchallenges when one GPU cannot",
    "start": "3745570",
    "end": "3750670"
  },
  {
    "text": "even hold a full expert\nafter heavy quantization?\"",
    "start": "3750670",
    "end": "3756150"
  },
  {
    "text": "I see. Yes, so I think if\nyou have one GPU,",
    "start": "3756150",
    "end": "3761820"
  },
  {
    "text": "I think I would go for a dense\nmodel or a heavily quantized",
    "start": "3761820",
    "end": "3770070"
  },
  {
    "text": "mixture of experts model. You said about having\nmore marginal experts,",
    "start": "3770070",
    "end": "3776670"
  },
  {
    "text": "like having 128 experts, I think\nthat's something super exciting",
    "start": "3776670",
    "end": "3782040"
  },
  {
    "text": "because that basically allows\nyou to specialize your--",
    "start": "3782040",
    "end": "3788640"
  },
  {
    "text": "basically allows your experts\nto specialize a bit more and that's always exciting. You can always reduce\nthe number of--",
    "start": "3788640",
    "end": "3796440"
  },
  {
    "text": "you can always make the\nexperts specialize better and you can pick--",
    "start": "3796440",
    "end": "3802140"
  },
  {
    "text": "basically giving your gating\nlayers more power to choose, like which experts are\nthe best for this token.",
    "start": "3802140",
    "end": "3809490"
  },
  {
    "text": "I think for serving purposes,\nthat will definitely make things very, very hard.",
    "start": "3809490",
    "end": "3814650"
  },
  {
    "text": "Even after having quantization,\nif you have 128 experts, then--",
    "start": "3814650",
    "end": "3821609"
  },
  {
    "text": "and potentially\nyou we're talking about having the model\nin multiple nodes",
    "start": "3821610",
    "end": "3828690"
  },
  {
    "text": "that will make the-- I guess, both make\nthe implementation",
    "start": "3828690",
    "end": "3834269"
  },
  {
    "text": "harder and also the\ncommunication cost higher.",
    "start": "3834270",
    "end": "3839640"
  },
  {
    "text": "So that's something\nwe probably want to leave to a model provider\nthrough APIs like serving",
    "start": "3839640",
    "end": "3845309"
  },
  {
    "text": "[INAUDIBLE] yourself. But I think for mixture\nof experts model, Mistral 8 times 7B is\ndefinitely after quantization.",
    "start": "3845310",
    "end": "3852910"
  },
  {
    "text": "It's definitely very, very\ndoable on a single GPU. All right, great.",
    "start": "3852910",
    "end": "3858320"
  },
  {
    "text": "Thanks for answering\na bunch of questions from very curious folks. So thanks, again, Albert, for\nthe amazing talk and the time",
    "start": "3858320",
    "end": "3865990"
  },
  {
    "text": "today. ",
    "start": "3865990",
    "end": "3871000"
  }
]