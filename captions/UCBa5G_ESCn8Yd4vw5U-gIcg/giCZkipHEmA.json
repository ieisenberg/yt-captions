[
  {
    "text": "OK, so that's\ndiscriminant analysis",
    "start": "0",
    "end": "4040"
  },
  {
    "text": "when we use Gaussian densities.",
    "start": "4040",
    "end": "7290"
  },
  {
    "text": "But now the formula that we\nwrote down is quite general,",
    "start": "7290",
    "end": "10280"
  },
  {
    "text": "and you can use other\nestimates of densities",
    "start": "10280",
    "end": "15260"
  },
  {
    "text": "and plug them into this rule\nand get classification rules.",
    "start": "15260",
    "end": "21610"
  },
  {
    "text": "So up to now we've been\nusing Gaussian densities",
    "start": "21610",
    "end": "24250"
  },
  {
    "text": "with the same variance\nfor the x's in each class.",
    "start": "24250",
    "end": "28990"
  },
  {
    "text": "What if the variances are\ndifferent in each class.",
    "start": "28990",
    "end": "32829"
  },
  {
    "text": "Well, you can plug\nthose forms in.",
    "start": "32830",
    "end": "35240"
  },
  {
    "text": "And then remember we had that\nmagic cancelation because",
    "start": "35240",
    "end": "38650"
  },
  {
    "text": "of the equal variances?",
    "start": "38650",
    "end": "40100"
  },
  {
    "text": "Well, when the variances\nare different in each class,",
    "start": "40100",
    "end": "44260"
  },
  {
    "text": "the quadratic terms don't cancel\nand so now your discriminant",
    "start": "44260",
    "end": "47859"
  },
  {
    "text": "functions are going to be\nquadratic functions of x.",
    "start": "47860",
    "end": "50535"
  },
  {
    "start": "50535",
    "end": "53900"
  },
  {
    "text": "So that's one form, it's called\nquadratic discriminant analysis.",
    "start": "53900",
    "end": "58910"
  },
  {
    "text": "Another thing you\ncan do is you--",
    "start": "58910",
    "end": "62239"
  },
  {
    "text": "and especially this is useful\nwhen you have a large number",
    "start": "62240",
    "end": "64970"
  },
  {
    "text": "of features like the 4,000\nor so that Rob mentioned,",
    "start": "64970",
    "end": "68690"
  },
  {
    "text": "when you really wouldn't want\nto estimate these large variance",
    "start": "68690",
    "end": "73130"
  },
  {
    "text": "covariance matrices, you can\nassume that in each class",
    "start": "73130",
    "end": "77149"
  },
  {
    "text": "the density factors into\na product of densities.",
    "start": "77150",
    "end": "80420"
  },
  {
    "text": "That's amounts to saying that\nthe variables are conditionally",
    "start": "80420",
    "end": "84439"
  },
  {
    "text": "independent in each\nof the classes.",
    "start": "84440",
    "end": "87480"
  },
  {
    "text": "And if you do that and plug it\ninto this formula over here,",
    "start": "87480",
    "end": "91890"
  },
  {
    "text": "you get something known\nas naive Bayes classifier.",
    "start": "91890",
    "end": "96740"
  },
  {
    "text": "For linear\ndiscriminant analysis,",
    "start": "96740",
    "end": "98430"
  },
  {
    "text": "this means that the covariances\nsigma sub k are diagonal.",
    "start": "98430",
    "end": "101755"
  },
  {
    "start": "101755",
    "end": "104890"
  },
  {
    "text": "And, instead of estimating,\nthe covariance matrix,",
    "start": "104890",
    "end": "110290"
  },
  {
    "text": "if you've got p variables,\nit's got p squared parameters.",
    "start": "110290",
    "end": "113440"
  },
  {
    "text": "But if you assume this\ndiagonal that it's diagonal",
    "start": "113440",
    "end": "117280"
  },
  {
    "text": "and you need to estimate\np parameters again--",
    "start": "117280",
    "end": "119440"
  },
  {
    "text": "Now the assumption\nseems very crude.",
    "start": "119440",
    "end": "121270"
  },
  {
    "text": "I mean, its assumption is wrong.",
    "start": "121270",
    "end": "123280"
  },
  {
    "text": "It's a naive Bayes\nclassifier is actually",
    "start": "123280",
    "end": "125080"
  },
  {
    "text": "very useful in high\ndimensional problems.",
    "start": "125080",
    "end": "127390"
  },
  {
    "text": "And it's one actually where we\nturn to later in different forms",
    "start": "127390",
    "end": "130720"
  },
  {
    "text": "Right.",
    "start": "130720",
    "end": "131530"
  },
  {
    "text": "So in fact we probably\nthink it's always wrong,",
    "start": "131530",
    "end": "135458"
  },
  {
    "text": "wouldn't be wrong, right.",
    "start": "135458",
    "end": "136500"
  },
  {
    "text": "Yeah.",
    "start": "136500",
    "end": "137320"
  },
  {
    "text": "And so what happens is we end up\nwith quite flattened and maybe",
    "start": "137320",
    "end": "143980"
  },
  {
    "text": "biased estimates for\nthe probabilities,",
    "start": "143980",
    "end": "146239"
  },
  {
    "text": "but in terms of\nclassification, you just",
    "start": "146240",
    "end": "148150"
  },
  {
    "text": "need to know which probability\nis the largest to classify.",
    "start": "148150",
    "end": "151519"
  },
  {
    "text": "So you can tolerate\nquite a lot of bias",
    "start": "151520",
    "end": "153820"
  },
  {
    "text": "and still get good\nclassification performance.",
    "start": "153820",
    "end": "156960"
  },
  {
    "text": "And what you get in return\nis much reduced variance",
    "start": "156960",
    "end": "160090"
  },
  {
    "text": "from having to estimate\nfar fewer parameters.",
    "start": "160090",
    "end": "164550"
  },
  {
    "text": "And then there's much\nmore other general forms",
    "start": "164550",
    "end": "166890"
  },
  {
    "text": "where we don't assume\nGaussian at all.",
    "start": "166890",
    "end": "169330"
  },
  {
    "text": "We can estimate the densities\nusing our favorite density",
    "start": "169330",
    "end": "173460"
  },
  {
    "text": "estimation technique,\nand then go and plug them",
    "start": "173460",
    "end": "176070"
  },
  {
    "text": "back into this\nformula, and that'll",
    "start": "176070",
    "end": "177630"
  },
  {
    "text": "give you a classification rule.",
    "start": "177630",
    "end": "179790"
  },
  {
    "text": "That's a very general\napproach that can be used.",
    "start": "179790",
    "end": "183010"
  },
  {
    "text": "And in fact, many of the\nclassifiers that we know we",
    "start": "183010",
    "end": "186090"
  },
  {
    "text": "can understand from\nthis point of view.",
    "start": "186090",
    "end": "189420"
  },
  {
    "text": "So here we have it, quadratic\ndiscriminant analysis uses",
    "start": "189420",
    "end": "192480"
  },
  {
    "text": "a different covariance\nmatrix for each class,",
    "start": "192480",
    "end": "195090"
  },
  {
    "text": "and so there's no\ncancelation of the sigma.",
    "start": "195090",
    "end": "197500"
  },
  {
    "text": "So the discriminant\nfunctions now",
    "start": "197500",
    "end": "199950"
  },
  {
    "text": "have this distance term\nthat involves sigma sub",
    "start": "199950",
    "end": "206250"
  },
  {
    "text": "k, which is for the k-th class.",
    "start": "206250",
    "end": "208250"
  },
  {
    "text": "There's a term to do with\nthe prior probability",
    "start": "208250",
    "end": "212040"
  },
  {
    "text": "and there's a determinant\nterm that comes",
    "start": "212040",
    "end": "214200"
  },
  {
    "text": "from the covariance matrix.",
    "start": "214200",
    "end": "216670"
  },
  {
    "text": "And you can see it gives you a\ncurved discriminant boundary.",
    "start": "216670",
    "end": "222599"
  },
  {
    "text": "And the quadratic terms matter\nhere because they're different.",
    "start": "222600",
    "end": "228440"
  },
  {
    "text": "And so in the left\nplot here we see a case",
    "start": "228440",
    "end": "233860"
  },
  {
    "text": "when the true boundary\nreally should be linear,",
    "start": "233860",
    "end": "237620"
  },
  {
    "text": "that's the dotted curve.",
    "start": "237620",
    "end": "239090"
  },
  {
    "text": "And in this case, of course,\nlinear discriminant analysis",
    "start": "239090",
    "end": "242739"
  },
  {
    "text": "does a good job.",
    "start": "242740",
    "end": "243780"
  },
  {
    "text": "Quadratic discriminant\nanalysis curves somewhat",
    "start": "243780",
    "end": "246400"
  },
  {
    "text": "and gives a slightly\nbent boundary,",
    "start": "246400",
    "end": "248930"
  },
  {
    "text": "but it won't really affect\nmisclassification performance",
    "start": "248930",
    "end": "251950"
  },
  {
    "text": "that much.",
    "start": "251950",
    "end": "252940"
  },
  {
    "text": "In the right-hand plot\non the other hand,",
    "start": "252940",
    "end": "255910"
  },
  {
    "text": "the true data came\nfrom a situation",
    "start": "255910",
    "end": "259239"
  },
  {
    "text": "where the covariances\nwere different.",
    "start": "259240",
    "end": "261849"
  },
  {
    "text": "Bayes' decision\nboundary is curved",
    "start": "261850",
    "end": "265030"
  },
  {
    "text": "and the quadratic discriminant\nanalysis pretty much",
    "start": "265030",
    "end": "267400"
  },
  {
    "text": "got it, whereas linear\ndiscriminant analysis gives you",
    "start": "267400",
    "end": "270178"
  },
  {
    "text": "quite a different\nboundary in that case.",
    "start": "270178",
    "end": "271845"
  },
  {
    "start": "271845",
    "end": "275040"
  },
  {
    "text": "Quadratic discriminant\nanalysis is attractive",
    "start": "275040",
    "end": "277680"
  },
  {
    "text": "if the number of\nvariables is small.",
    "start": "277680",
    "end": "279600"
  },
  {
    "text": "When the number of variables\nor features is large,",
    "start": "279600",
    "end": "283200"
  },
  {
    "text": "you've got to estimate these\nbig covariance matrices",
    "start": "283200",
    "end": "286050"
  },
  {
    "text": "and things can break down.",
    "start": "286050",
    "end": "287979"
  },
  {
    "text": "And even for LDA,\nit can break down.",
    "start": "287980",
    "end": "290890"
  },
  {
    "text": "Here's where naive Bayes\nbecomes attractive.",
    "start": "290890",
    "end": "293790"
  },
  {
    "text": "It makes a much\nstronger assumption.",
    "start": "293790",
    "end": "295870"
  },
  {
    "text": "It assumes that the covariances\nin each of the classes,",
    "start": "295870",
    "end": "298979"
  },
  {
    "text": "although different,\nare diagonal,",
    "start": "298980",
    "end": "301440"
  },
  {
    "text": "and so that's much\nfewer parameters.",
    "start": "301440",
    "end": "304360"
  },
  {
    "text": "And now when you look at\nthe discriminant functions,",
    "start": "304360",
    "end": "308039"
  },
  {
    "text": "because diagonal\nand Gaussian means",
    "start": "308040",
    "end": "310680"
  },
  {
    "text": "that the densities\nare independent and so",
    "start": "310680",
    "end": "314490"
  },
  {
    "text": "we have this product here.",
    "start": "314490",
    "end": "315910"
  },
  {
    "text": "When we take logs, we get a\nrelatively simple expression,",
    "start": "315910",
    "end": "320400"
  },
  {
    "text": "which is in each class there's\na contribution of the feature",
    "start": "320400",
    "end": "324840"
  },
  {
    "text": "from the mean for the class\nscaled by the variance,",
    "start": "324840",
    "end": "328410"
  },
  {
    "text": "there's the determinant term\nand there's the prior term.",
    "start": "328410",
    "end": "331940"
  },
  {
    "text": "And so this is the\ndiscriminant function",
    "start": "331940",
    "end": "335900"
  },
  {
    "text": "for the k-th class\nfor naive Bayes,",
    "start": "335900",
    "end": "337552"
  },
  {
    "text": "and you compute one of these\nfor each of the classes,",
    "start": "337552",
    "end": "339760"
  },
  {
    "text": "then you classify.",
    "start": "339760",
    "end": "342460"
  },
  {
    "text": "You can use mixed\nfeatures for naive Bayes.",
    "start": "342460",
    "end": "345819"
  },
  {
    "text": "And in that case,\nwhat we mean by that",
    "start": "345820",
    "end": "349360"
  },
  {
    "text": "is some qualitative and\nsome quantitative features.",
    "start": "349360",
    "end": "352909"
  },
  {
    "text": "And so for the quantitative\nones we'd use the Gaussian",
    "start": "352910",
    "end": "355600"
  },
  {
    "text": "and for the qualitative ones\nwe'd replace the densities,",
    "start": "355600",
    "end": "360160"
  },
  {
    "text": "the Gaussian densities by just\nhistograms or probability mass",
    "start": "360160",
    "end": "364960"
  },
  {
    "text": "functions, in this case over\nthe discrete categories.",
    "start": "364960",
    "end": "369280"
  },
  {
    "text": "So naive Bayes is very handy\nfrom this point of view.",
    "start": "369280",
    "end": "374320"
  },
  {
    "text": "And even though it has\nstrong assumptions,",
    "start": "374320",
    "end": "376990"
  },
  {
    "text": "it often produces good\nclassification results",
    "start": "376990",
    "end": "380979"
  },
  {
    "text": "because often once again\nin classification we're",
    "start": "380980",
    "end": "383710"
  },
  {
    "text": "mainly concerned about\nwhich class has the highest",
    "start": "383710",
    "end": "386229"
  },
  {
    "text": "probability and not whether we\ngot the probabilities exactly",
    "start": "386230",
    "end": "389650"
  },
  {
    "text": "right.",
    "start": "389650",
    "end": "391580"
  },
  {
    "text": "OK, so we've seen two\nforms of classification,",
    "start": "391580",
    "end": "398090"
  },
  {
    "text": "logistic regression and\nlinear discriminant analysis,",
    "start": "398090",
    "end": "401389"
  },
  {
    "text": "and we saw its generalizations.",
    "start": "401390",
    "end": "403500"
  },
  {
    "start": "403500",
    "end": "406560"
  },
  {
    "text": "Now, it turns out--",
    "start": "406560",
    "end": "408780"
  },
  {
    "text": "how do they differ?",
    "start": "408780",
    "end": "411960"
  },
  {
    "text": "It seems there may be some\nsimilarities between the two.",
    "start": "411960",
    "end": "416190"
  },
  {
    "text": "Now it turns out you can show\nfor linear discriminant analysis",
    "start": "416190",
    "end": "419910"
  },
  {
    "text": "that if you take its--",
    "start": "419910",
    "end": "421530"
  },
  {
    "text": "we had expression\nfor the probabilities",
    "start": "421530",
    "end": "423450"
  },
  {
    "text": "for each of the classes.",
    "start": "423450",
    "end": "424763"
  },
  {
    "text": "So if you have two\nclasses, we can",
    "start": "424763",
    "end": "426180"
  },
  {
    "text": "show that if you\ntake the log odds,",
    "start": "426180",
    "end": "428460"
  },
  {
    "text": "just like we did in\nlogistic regression which",
    "start": "428460",
    "end": "431910"
  },
  {
    "text": "is the log of the\nprobability for class",
    "start": "431910",
    "end": "434220"
  },
  {
    "text": "one versus the\nprobability for class two.",
    "start": "434220",
    "end": "438030"
  },
  {
    "text": "It's a linear function\nof x, for two classes.",
    "start": "438030",
    "end": "442740"
  },
  {
    "text": "So it's got exactly the same\nform as logistic regression.",
    "start": "442740",
    "end": "447310"
  },
  {
    "text": "So they both give you\nlinear logistic models.",
    "start": "447310",
    "end": "451510"
  },
  {
    "text": "So the difference is in how\nthe parameters are estimated.",
    "start": "451510",
    "end": "457010"
  },
  {
    "text": "Logistic regression uses\nthe conditional likelihood",
    "start": "457010",
    "end": "460040"
  },
  {
    "text": "based on probability\nof y given x.",
    "start": "460040",
    "end": "462810"
  },
  {
    "text": "Remember, it was using\nthe probabilities",
    "start": "462810",
    "end": "465980"
  },
  {
    "text": "of a 1 or a 0 given x.",
    "start": "465980",
    "end": "470340"
  },
  {
    "text": "In each of the classes\nand in machine learning,",
    "start": "470340",
    "end": "473900"
  },
  {
    "text": "this is known as\ndiscriminative learning,",
    "start": "473900",
    "end": "477190"
  },
  {
    "text": "using the conditional\ndistribution of y given x.",
    "start": "477190",
    "end": "480850"
  },
  {
    "text": "Discriminant analysis turns\nout it's estimating these",
    "start": "480850",
    "end": "484150"
  },
  {
    "text": "parameters, over here,\nusing the full likelihood",
    "start": "484150",
    "end": "489310"
  },
  {
    "text": "because it's using\nthe distribution",
    "start": "489310",
    "end": "491950"
  },
  {
    "text": "of X's and Y's, whereas logistic\nregression was only using",
    "start": "491950",
    "end": "496650"
  },
  {
    "text": "the distribution of Y's.",
    "start": "496650",
    "end": "499560"
  },
  {
    "text": "And in that case, it's known\nas generative learning.",
    "start": "499560",
    "end": "503050"
  },
  {
    "text": "Remember we modeled\nthe means and variances",
    "start": "503050",
    "end": "506470"
  },
  {
    "text": "of x in each of\nthe classes and we",
    "start": "506470",
    "end": "508270"
  },
  {
    "text": "modeled the prior\nprobabilities, so",
    "start": "508270",
    "end": "510099"
  },
  {
    "text": "that can be seen as modeling the\njoint distribution of x and y.",
    "start": "510100",
    "end": "514929"
  },
  {
    "text": "So that's one way\nof seeing what's",
    "start": "514929",
    "end": "516490"
  },
  {
    "text": "different between the two.",
    "start": "516490",
    "end": "518620"
  },
  {
    "text": "But despite these\ndifferences in practice,",
    "start": "518620",
    "end": "520900"
  },
  {
    "text": "the results are\noften very similar,",
    "start": "520900",
    "end": "522940"
  },
  {
    "text": "and you can use one\nmethod or the other",
    "start": "522940",
    "end": "524865"
  },
  {
    "text": "and you're going to get\nvery similar results.",
    "start": "524865",
    "end": "526740"
  },
  {
    "start": "526740",
    "end": "531060"
  },
  {
    "text": "As a footnote,\nlogistic regression",
    "start": "531060",
    "end": "533970"
  },
  {
    "text": "can also fit\nquadratic boundaries.",
    "start": "533970",
    "end": "536110"
  },
  {
    "text": "We use quadratic\ndiscriminant analysis",
    "start": "536110",
    "end": "538410"
  },
  {
    "text": "to get quadratic boundaries, but\nwe can fit quadratic boundaries",
    "start": "538410",
    "end": "542670"
  },
  {
    "text": "by explicitly including\nquadratic terms in the model.",
    "start": "542670",
    "end": "545940"
  },
  {
    "text": "Just like we did in\nlinear regression,",
    "start": "545940",
    "end": "548020"
  },
  {
    "text": "in logistic regression, we can\nput in x squares and xi times",
    "start": "548020",
    "end": "552600"
  },
  {
    "text": "xj's and terms like that\nand just explicitly get",
    "start": "552600",
    "end": "555329"
  },
  {
    "text": "a quadratic boundary.",
    "start": "555330",
    "end": "558000"
  },
  {
    "text": "OK, so that's the\nend of this section.",
    "start": "558000",
    "end": "561600"
  },
  {
    "text": "That's an introduction\nto classification using",
    "start": "561600",
    "end": "564449"
  },
  {
    "text": "these two very popular methods.",
    "start": "564450",
    "end": "566320"
  },
  {
    "text": "And later on in\nthe class, you're",
    "start": "566320",
    "end": "567750"
  },
  {
    "text": "going to see that we're\ngoing to come back",
    "start": "567750",
    "end": "569610"
  },
  {
    "text": "to some of these\nmethods and look",
    "start": "569610",
    "end": "571170"
  },
  {
    "text": "at more general versions of them\nand build richer classification",
    "start": "571170",
    "end": "575790"
  },
  {
    "text": "rules.",
    "start": "575790",
    "end": "576370"
  },
  {
    "text": "And importantly, we'll discuss\nanother very popular method",
    "start": "576370",
    "end": "579870"
  },
  {
    "text": "called the support\nvector machine, which",
    "start": "579870",
    "end": "582120"
  },
  {
    "text": "is another approach\nto classification.",
    "start": "582120",
    "end": "584040"
  },
  {
    "text": "And by way of\ncoming attractions,",
    "start": "584040",
    "end": "585420"
  },
  {
    "text": "the next section is going\nto be on cross-validation",
    "start": "585420",
    "end": "587545"
  },
  {
    "text": "and bootstrap.",
    "start": "587545",
    "end": "588519"
  },
  {
    "text": "And in that section, we will\nget to meet Brad Efron, who's",
    "start": "588520",
    "end": "592020"
  },
  {
    "text": "our colleague and also he was my\nPhD supervisor many years ago,",
    "start": "592020",
    "end": "596738"
  },
  {
    "text": "and he was the person who\ninvented the bootstrap.",
    "start": "596738",
    "end": "598779"
  },
  {
    "text": "So he'll tell us a bit\nabout the bootstrap",
    "start": "598780",
    "end": "601050"
  },
  {
    "text": "and how he proposed\nit in his 1980 paper.",
    "start": "601050",
    "end": "605850"
  },
  {
    "text": "Fantastic.",
    "start": "605850",
    "end": "607500"
  },
  {
    "start": "607500",
    "end": "608000"
  }
]