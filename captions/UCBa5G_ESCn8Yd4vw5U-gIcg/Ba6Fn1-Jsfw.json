[
  {
    "start": "0",
    "end": "5225"
  },
  {
    "text": "OK, hi, everyone,\nBack for more CS224N.",
    "start": "5225",
    "end": "11880"
  },
  {
    "text": "OK, so for today, the plan\nis essentially a continuation",
    "start": "11880",
    "end": "18150"
  },
  {
    "text": "of what we started on Tuesday. So I'm going to say more\nabout language models",
    "start": "18150",
    "end": "25830"
  },
  {
    "text": "and more about RNNs, in\nparticular, introducing a more advanced form of\nrecurrent neural network,",
    "start": "25830",
    "end": "33340"
  },
  {
    "text": "which was for a\nwhile very dominant. LSTMs, we'll talk about those. And then in the latter\npart, has something",
    "start": "33340",
    "end": "40860"
  },
  {
    "text": "to be done with recurrent\nneural networks. We'll start looking at\nneural machine translation.",
    "start": "40860",
    "end": "49450"
  },
  {
    "text": "OK, so on Tuesday,\nwhat we did was we introduced language\nmodels, a system",
    "start": "49450",
    "end": "55030"
  },
  {
    "text": "that predicts the next word. And then I introduced\nrecurrent neural networks, so that's this new\nneural architecture",
    "start": "55030",
    "end": "62860"
  },
  {
    "text": "that can take sequential\ninput of any length and then apply the same\nweights at each step",
    "start": "62860",
    "end": "69040"
  },
  {
    "text": "and can optionally produce\noutput on each step. So these are two\ndistinct notions,",
    "start": "69040",
    "end": "75890"
  },
  {
    "text": "though they tend to go together. So recurrent neural network\ncan be used for other purposes",
    "start": "75890",
    "end": "83680"
  },
  {
    "text": "on any kinds of sequence,\nand I'll mention a few of those later today. And language modeling is\na traditional component",
    "start": "83680",
    "end": "91690"
  },
  {
    "text": "of many NLP tasks,\nanything to do with generating\ntext or estimating",
    "start": "91690",
    "end": "97060"
  },
  {
    "text": "likelihoods of pieces of text. And indeed, in the modern\ninstantiation of large language",
    "start": "97060",
    "end": "102430"
  },
  {
    "text": "models, essentially\neverything we do in NLP is being done by\nlanguage models.",
    "start": "102430",
    "end": "108590"
  },
  {
    "text": "So a language model,\none way to do it is with a recurrent\nneural network. It's certainly not the only way.",
    "start": "108590",
    "end": "115150"
  },
  {
    "text": "We also talked last time\nabout n-gram language models, which are language models.",
    "start": "115150",
    "end": "120180"
  },
  {
    "text": "And then starting\nnext week, we'll start to talk about\ntransformers, which are now",
    "start": "120180",
    "end": "125210"
  },
  {
    "text": "the most widespread\nway that's used for building language models.",
    "start": "125210",
    "end": "130315"
  },
  {
    "text": "So just to finish\noff a teeny bit that I didn't get to last\ntime on evaluating language models, well, one way to\nevaluate language models is what",
    "start": "130315",
    "end": "138520"
  },
  {
    "text": "I did in class last time,\ngenerate some text and say, hey, doesn't this text look good?",
    "start": "138520",
    "end": "144775"
  },
  {
    "text": "But often, we want something\nmore rigorous than that. And the standard way to\nevaluate language models",
    "start": "144775",
    "end": "152800"
  },
  {
    "text": "is to say, well, a language\nmodel scores a piece of text and says how likely it is.",
    "start": "152800",
    "end": "158890"
  },
  {
    "text": "And our standard for\ntext in a language",
    "start": "158890",
    "end": "164170"
  },
  {
    "text": "is stuff produced\nby human beings. So if we find a\nnew piece of text--",
    "start": "164170",
    "end": "170450"
  },
  {
    "text": "which wasn't text that\nthe model was trained on. We want some fresh\nevaluation data--",
    "start": "170450",
    "end": "176890"
  },
  {
    "text": "and we show it to\na language model, we can then ask\nthe language model",
    "start": "176890",
    "end": "182440"
  },
  {
    "text": "to predict the success\nof words of this text. And the better it\nis at doing that,",
    "start": "182440",
    "end": "188450"
  },
  {
    "text": "the better a\nlanguage model it is, because it's more accurately\nable to predict a human written",
    "start": "188450",
    "end": "195110"
  },
  {
    "text": "piece of text. And so the standard way\nthat that is measured is with this measure\nthat's called perplexity.",
    "start": "195110",
    "end": "202140"
  },
  {
    "text": "And so for perplexity,\nwe are taking the probability of a prediction\nfrom the language model.",
    "start": "202140",
    "end": "209520"
  },
  {
    "text": "We're inverting it, so instead\nof it being 0.002 or something,",
    "start": "209520",
    "end": "214860"
  },
  {
    "text": "we're turning into 500\nor something like that.",
    "start": "214860",
    "end": "220745"
  },
  {
    "text": "And then we're\ntaking those numbers. We're taking the product of them\nat each position in the text,",
    "start": "220745",
    "end": "227370"
  },
  {
    "text": "and then we're finding the\ngeometric average of them. So that's the measure\nthat's normally used.",
    "start": "227370",
    "end": "234540"
  },
  {
    "text": "But in this class,\nwe've been tending to look at negative\nlog likelihoods",
    "start": "234540",
    "end": "242750"
  },
  {
    "text": "and the idea of cross-entropy. And so what perplexity is,\nis it's just the exponential",
    "start": "242750",
    "end": "252620"
  },
  {
    "text": "of the cross entropy. So if you're already familiar\nwith negative log-likelihood, per word, negative\nlog-likelihoods,",
    "start": "252620",
    "end": "260370"
  },
  {
    "text": "if you just exponentiate that,\nyou then get the perplexity.",
    "start": "260370",
    "end": "265380"
  },
  {
    "text": "Now, there's one\nother little trick as to what base you use for your\nlogarithms and exponentials.",
    "start": "265380",
    "end": "272009"
  },
  {
    "text": "Traditionally, thinking\nof binary and stuff, a lot of the time, people use\nbase 2 for measuring perplexity.",
    "start": "272010",
    "end": "281405"
  },
  {
    "text": "That's gone out now. A lot of the time now, people\nare using natural logs. But if you're comparing\nperplexity numbers,",
    "start": "281405",
    "end": "288240"
  },
  {
    "text": "they're going to be\ndifferent depending on what base you're using for things. So you need to be aware of this.",
    "start": "288240",
    "end": "295630"
  },
  {
    "text": "So from a modern perspective,\nit kind of makes no sense",
    "start": "295630",
    "end": "302660"
  },
  {
    "text": "why perplexity is used. The story of why\nperplexity was used",
    "start": "302660",
    "end": "309919"
  },
  {
    "text": "was in the bad old days\nof symbolic artificial intelligence, when all of\nthose famous people like John",
    "start": "309920",
    "end": "317449"
  },
  {
    "text": "McCarthy and Ed\nFeigenbaum were around doing logical-based systems,\nsome people, then essentially",
    "start": "317450",
    "end": "326270"
  },
  {
    "text": "at IBM, including Fred\nJelinek, started exploring probabilistic methods\nfor speech recognition",
    "start": "326270",
    "end": "333530"
  },
  {
    "text": "and other similar methods. And the story Fred\nJelinek used to tell",
    "start": "333530",
    "end": "340430"
  },
  {
    "text": "was, well, at that time-- this was in the late\n'70s or early '80s--",
    "start": "340430",
    "end": "346010"
  },
  {
    "text": "that none of the AI people\nthat he was trying to talk to understood how to\ndo any real math",
    "start": "346010",
    "end": "353210"
  },
  {
    "text": "and didn't understand\nany information theory notions of doing things like\ncross-entropy or cross-entropy",
    "start": "353210",
    "end": "363260"
  },
  {
    "text": "rate. So he had to come up\nwith something simple that they could understand. And so what he came\nup with is by doing",
    "start": "363260",
    "end": "370130"
  },
  {
    "text": "this exponentiated perplexity. You can think of a\nperplexity number",
    "start": "370130",
    "end": "375790"
  },
  {
    "text": "as being equivalent to how many\nuniform choices you're choosing",
    "start": "375790",
    "end": "381070"
  },
  {
    "text": "between. So if the perplexity\nof something is 64,",
    "start": "381070",
    "end": "386270"
  },
  {
    "text": "that's like having a 64-sided\ndice that you're rolling at each time, and your chance of getting\na 1 on that is your chance",
    "start": "386270",
    "end": "394810"
  },
  {
    "text": "of guessing the right word. So that was why perplexity got\nintroduced, but it's stuck.",
    "start": "394810",
    "end": "401660"
  },
  {
    "text": "And so when you see scores\nfor language models, you generally still\nsee perplexities.",
    "start": "401660",
    "end": "406970"
  },
  {
    "text": "So a lower perplexity is better. So here are the kind of numbers\nand where progress was made",
    "start": "406970",
    "end": "415690"
  },
  {
    "text": "with neural language models. So before that, people used\nn-gram language models,",
    "start": "415690",
    "end": "422080"
  },
  {
    "text": "and people used\nclever ways to smooth them using methods\nI vaguely alluded to last time of this air case\nsmoothing and doing back off.",
    "start": "422080",
    "end": "431080"
  },
  {
    "text": "Actually, people use cleverer\nmethods around the 2000s decade.",
    "start": "431080",
    "end": "437789"
  },
  {
    "text": "The cleverest method known of\nsmoothing n-gram language models was this thing called\ninterpolated Kneser-Ney",
    "start": "437790",
    "end": "444260"
  },
  {
    "text": "smoothing. And so for a big language\nmodel using that,",
    "start": "444260",
    "end": "449760"
  },
  {
    "text": "the perplexity was about\n67, which in some sense, means that you weren't very good\nat predicting the next word.",
    "start": "449760",
    "end": "457240"
  },
  {
    "text": "But that had actually\nbeen enormous progress. When I was a young\nperson doing NLP,",
    "start": "457240",
    "end": "464112"
  },
  {
    "text": "perplexities were\nthree-figure numbers. You were commonly seeing\nperplexities of 150",
    "start": "464112",
    "end": "469970"
  },
  {
    "text": "or something like that. So progress was made. So when RNNs were\nfirst introduced,",
    "start": "469970",
    "end": "477750"
  },
  {
    "text": "people weren't\nreally actually able to do better with a pure\nRNN, but they could do better",
    "start": "477750",
    "end": "485990"
  },
  {
    "text": "by combining an RNN\nwith something else, such as a symbolic maximum\nentropy model, which",
    "start": "485990",
    "end": "492900"
  },
  {
    "text": "I'm not going to explain, but\nthose are numbers like that, 51. But where progress\nreally started to be made",
    "start": "492900",
    "end": "499860"
  },
  {
    "text": "was when LSTM started to be used\nas an improved RNN, which is what I'm going to come to next.",
    "start": "499860",
    "end": "505660"
  },
  {
    "text": "So here are some LSTM\nmodels, and now you're getting numbers like 43 and 30.",
    "start": "505660",
    "end": "512679"
  },
  {
    "text": "And so for 30, you've\nhalved the perplexity, which in cross-entropy terms means\nyou've reduced the cross-entropy",
    "start": "512679",
    "end": "522779"
  },
  {
    "text": "by about one bit, and so\nyou've made real progress in your language modeling.",
    "start": "522780",
    "end": "529150"
  },
  {
    "text": "Now, by modern standards, these\nnumbers are still really high.",
    "start": "529150",
    "end": "534480"
  },
  {
    "text": "For the best language\nmodels that we have now, you're getting perplexities\nin the single digits.",
    "start": "534480",
    "end": "539560"
  },
  {
    "text": "You're getting models that are\nvery often able to guess exactly the right word.",
    "start": "539560",
    "end": "544810"
  },
  {
    "text": "Though of course, not\nalways, because no one can predict what words are\ngoing to be said by someone next in a lot of circumstances.",
    "start": "544810",
    "end": "552810"
  },
  {
    "text": "OK, so to motivate\nLSTMs, they wanted",
    "start": "552810",
    "end": "558270"
  },
  {
    "text": "to say a bit about how\nthere are problems with RNNs and why that motivated\nfixing things.",
    "start": "558270",
    "end": "565300"
  },
  {
    "text": "And these are the problems\nof vanishing and exploding gradients. So what we wanted to\ndo was say, OK, we've",
    "start": "565300",
    "end": "572880"
  },
  {
    "text": "tried to predict a\nword at position 4, and often, we're\ngoing to get a--",
    "start": "572880",
    "end": "580050"
  },
  {
    "text": "we're not going to predict\nit with 100% probability. So we have a loss that's\na negative log-likelihood.",
    "start": "580050",
    "end": "586230"
  },
  {
    "text": "We get to that word. And we're going to\nwant to backpropagate that loss through the sequence\nand work out our gradients,",
    "start": "586230",
    "end": "596079"
  },
  {
    "text": "as we always do. Now, just one note\nabout something someone asked me after\nclass last time, I",
    "start": "596080",
    "end": "602970"
  },
  {
    "text": "showed it backpropagating\nthe whole sequence, but we're doing this\nat every time step. So we're going to\nbackpropagate a loss from time",
    "start": "602970",
    "end": "609779"
  },
  {
    "text": "step 2, backpropagate a loss\nfrom time step 3, 4, 5, 6, 7. We're doing it for each one.",
    "start": "609780",
    "end": "616120"
  },
  {
    "text": "And then one of the\nslides last time, we then discussed how we're\ngoing to sum all of those losses",
    "start": "616120",
    "end": "621450"
  },
  {
    "text": "or work out the average loss. But for doing this one,\nwhen we backpropagate",
    "start": "621450",
    "end": "627180"
  },
  {
    "text": "this loss, what happens? Well, what happens\nis we're going to do the same\nkind of chain rule",
    "start": "627180",
    "end": "634020"
  },
  {
    "text": "where we're multiplying\nthese partial derivatives at every time step?",
    "start": "634020",
    "end": "639310"
  },
  {
    "text": "And well, here we've\nonly got a few of them, but maybe we're going to\nhave a sequence 30 long,",
    "start": "639310",
    "end": "645390"
  },
  {
    "text": "and so we're going\nto be multiplying each time the partial of hk with\nrespect to the partial of hk",
    "start": "645390",
    "end": "654420"
  },
  {
    "text": "minus 1. And so what kind of effect\nis that going to have?",
    "start": "654420",
    "end": "660089"
  },
  {
    "text": "In particular, we might ask,\nwhat happens if these are small,",
    "start": "660090",
    "end": "665460"
  },
  {
    "text": "or what happens if\nthese are large? Well, if they're\nsmall, the gradient",
    "start": "665460",
    "end": "671040"
  },
  {
    "text": "will gradually get\nsmaller and smaller and disappear as\nwe backpropagated",
    "start": "671040",
    "end": "677970"
  },
  {
    "text": "along the sequence. Yeah? So why are we taking\npartial j over partial j?",
    "start": "677970",
    "end": "684910"
  },
  {
    "text": "Should we take\npartial j [INAUDIBLE]? ",
    "start": "684910",
    "end": "692600"
  },
  {
    "text": "Sure, we're doing that, as well. But in general, we have to\nwalk the partials along,",
    "start": "692600",
    "end": "702540"
  },
  {
    "text": "and then we then have\nthe W at the next step. ",
    "start": "702540",
    "end": "708760"
  },
  {
    "text": "If we're thinking of\nthe computation graph that we're doing the chain\nrule backwards along,",
    "start": "708760",
    "end": "715720"
  },
  {
    "text": "we're going to be going\nthrough a W at each step and then arriving at another H.",
    "start": "715720",
    "end": "728459"
  },
  {
    "text": "Yeah, so at this point, you\ncan do some math and thinking",
    "start": "728460",
    "end": "736620"
  },
  {
    "text": "about things. And there's a couple of papers\nthat are mentioned at the bottom",
    "start": "736620",
    "end": "741630"
  },
  {
    "text": "here, which I'm\nactually rushing ahead, not going to do very carefully.",
    "start": "741630",
    "end": "748515"
  },
  {
    "text": "But the point is that if\nyou're taking the partial of ht",
    "start": "748515",
    "end": "754440"
  },
  {
    "text": "with respect to ht\nminus 1, and if you make a simplifying assumption\nand say, suppose there",
    "start": "754440",
    "end": "761730"
  },
  {
    "text": "isn't a nonlinearity, suppose\nsigma is just the identity, then what the partial\nwill be is the matrix wh.",
    "start": "761730",
    "end": "770980"
  },
  {
    "text": "And so if you keep\non backpropagating along the recurrent neural\nnetwork, what you're",
    "start": "770980",
    "end": "778260"
  },
  {
    "text": "going to be doing is ending up\nwith powers of the matrix, wh.",
    "start": "778260",
    "end": "784260"
  },
  {
    "text": "And then there's\nthe question of, what happens when\nyou raise that matrix to higher and higher powers?",
    "start": "784260",
    "end": "790910"
  },
  {
    "text": "Well, at that point, you\ncan represent the matrix in terms of its eigenvectors\nand eigenvalues.",
    "start": "790910",
    "end": "798260"
  },
  {
    "text": "And then there are two\npossibilities, either all the eigenvalues are\nless than 1, and that",
    "start": "798260",
    "end": "804370"
  },
  {
    "text": "means that, that number will\nbe getting smaller and smaller",
    "start": "804370",
    "end": "809560"
  },
  {
    "text": "as you raise it\nto a higher power, or it can have eigenvalues\nthat are larger than 1,",
    "start": "809560",
    "end": "816890"
  },
  {
    "text": "and then things will\nget bigger and bigger as you go further back. So essentially, as you\nbackpropagate the gradients",
    "start": "816890",
    "end": "822760"
  },
  {
    "text": "backwards, unless things are\njust precisely corresponding",
    "start": "822760",
    "end": "828910"
  },
  {
    "text": "to the largest\neigenvector of the largest eigenvalue of\napproximately 1, you're",
    "start": "828910",
    "end": "836230"
  },
  {
    "text": "either going to get a\nvanishing or an explosion. And both of those will be bad.",
    "start": "836230",
    "end": "843399"
  },
  {
    "text": "So why is vanishing\ngradient a problem?",
    "start": "843400",
    "end": "848650"
  },
  {
    "text": "In a sense, you could\nthink it's not a problem. It's what should be happening\nbecause all else being equal,",
    "start": "848650",
    "end": "857410"
  },
  {
    "text": "the closest words are\nthe most relevant ones, and so that's where\nyou should be updating",
    "start": "857410",
    "end": "862840"
  },
  {
    "text": "your parameters the most. And to some extent, that's true. But nevertheless, this\nvanishing gradient in this model",
    "start": "862840",
    "end": "870910"
  },
  {
    "text": "happens much too severely,\nso that if you're looking at the loss\nfrom a later position",
    "start": "870910",
    "end": "877720"
  },
  {
    "text": "and comparing it to the loss\nfrom an earlier position, and then you're seeing\nhow things are updating,",
    "start": "877720",
    "end": "885740"
  },
  {
    "text": "it's primarily, the\nupdate is being determined by the very nearby loss and\nnot by the far-away loss,",
    "start": "885740",
    "end": "894850"
  },
  {
    "text": "that the gradient signal from\nfar away is much, much smaller. And well, that's bad because\noverall for language modeling,",
    "start": "894850",
    "end": "905879"
  },
  {
    "text": "there are lots of\ncases where we want to be able to transmit\nsignals a long distance. So here's my piece of text.",
    "start": "905880",
    "end": "913730"
  },
  {
    "text": "\"When she tried to\nprint her tickets, she found that the\nprinter was out of toner. She went to the stationery\nstore to buy more toner.",
    "start": "913730",
    "end": "920970"
  },
  {
    "text": "It was very overpriced. After installing the\ntoner into the printer, she finally printed her--\"",
    "start": "920970",
    "end": "927410"
  },
  {
    "text": "Ticket. Yeah, so to a human\nbeing, it's obvious. We can predict this with\npretty much probability 1,",
    "start": "927410",
    "end": "934250"
  },
  {
    "text": "so really low perplexity\nfor making this decision. But that depends on getting\nback to the tickets,",
    "start": "934250",
    "end": "945000"
  },
  {
    "text": "which are about\n20-odd words back. If you're just seeing,\ninstalling the toner",
    "start": "945000",
    "end": "950300"
  },
  {
    "text": "into the printer, she finally\nprinted her, could be anything. It could be her paper,\nher invitation, her novel,",
    "start": "950300",
    "end": "958250"
  },
  {
    "text": "lots of things it could be. You're certainly not\ngoing to guess tickets. So we want to have these really\nlong-distance dependencies,",
    "start": "958250",
    "end": "968830"
  },
  {
    "text": "but we're only going\nto be able to learn these long-distance dependencies\nif we're actually getting",
    "start": "968830",
    "end": "974070"
  },
  {
    "text": "sufficient signal\nbetween that position and when the word, tickets,\nappears near the beginning",
    "start": "974070",
    "end": "981270"
  },
  {
    "text": "that we can learn the fact\nthat having that tickets 20 words back is the good\npredictive thing for predicting",
    "start": "981270",
    "end": "988620"
  },
  {
    "text": "tickets here. And what we find is that when\nthe gradient becomes very small,",
    "start": "988620",
    "end": "998300"
  },
  {
    "text": "the RNN doesn't learn these\nlong-distance dependencies. And so it's unable to make these\npredictions well at test time.",
    "start": "998300",
    "end": "1008560"
  },
  {
    "text": "This is a very just rough\nback-of-the-envelope estimate.",
    "start": "1008560",
    "end": "1016900"
  },
  {
    "text": "But what people\nactually found is that with the kind of simple\nRNN that we've introduced up",
    "start": "1016900",
    "end": "1024359"
  },
  {
    "text": "until now, that the amount of\neffective conditioning you could get was about seven tokens back,\nthat if things were further",
    "start": "1024359",
    "end": "1033689"
  },
  {
    "text": "back than that, it just never\nlearned to condition on them. And so compared to when we were\ntalking about n-grams and I said",
    "start": "1033690",
    "end": "1042150"
  },
  {
    "text": "are usually the maximum people\ndid was 5 grams, occasionally a bit bigger because\nof the fact that there",
    "start": "1042150",
    "end": "1048119"
  },
  {
    "text": "was this exponential blowout,\nalthough in theory, we've now got a much better\nsolution, in practice",
    "start": "1048119",
    "end": "1054610"
  },
  {
    "text": "because of vanishing\ngradients, well, we're only getting the\nequivalent of 8 grams.",
    "start": "1054610",
    "end": "1059720"
  },
  {
    "text": "So we haven't made that much\nprogress, it feels like.",
    "start": "1059720",
    "end": "1065409"
  },
  {
    "text": "So there's a reverse\nproblem which can also happen of exploding gradients.",
    "start": "1065410",
    "end": "1070700"
  },
  {
    "text": "So if the gradient\nbecomes very large because the eigenvalues of\nthat matrix are large, well,",
    "start": "1070700",
    "end": "1078790"
  },
  {
    "text": "what we're doing for\nthe parameter update is we've got a learning\nrate, but essentially,",
    "start": "1078790",
    "end": "1084760"
  },
  {
    "text": "if the gradient is\nvery large, we're going to make a very, very\nlarge parameter update.",
    "start": "1084760",
    "end": "1090970"
  },
  {
    "text": "And that can cause\nvery bad updates because we're\nassuming that we're",
    "start": "1090970",
    "end": "1097450"
  },
  {
    "text": "taking a step in the\ndirection of the gradient, and well, we might\novershoot a little but we'll be in a roughly\nin the right zone.",
    "start": "1097450",
    "end": "1104990"
  },
  {
    "text": "But if we had an enormously\nexploded gradient, well, we could be walking\noff anywhere.",
    "start": "1104990",
    "end": "1113060"
  },
  {
    "text": "And we think we're\nheading to the Sierras, and we end up in Iowa\nor something like that.",
    "start": "1113060",
    "end": "1118960"
  },
  {
    "text": "We could just go arbitrarily\nfar, and where we're ending up, it might not be making\nany progress whatsoever.",
    "start": "1118960",
    "end": "1126850"
  },
  {
    "text": "So exploding gradients\nare a problem. They can also cause\ninfinities and NaNs,",
    "start": "1126850",
    "end": "1134030"
  },
  {
    "text": "and they're always a problem\nwhen you're training models. Now, for dealing with\nexploding gradients,",
    "start": "1134030",
    "end": "1143410"
  },
  {
    "text": "this is the accepted wisdom. This unfortunately doesn't--\nthis isn't highfaluting math,",
    "start": "1143410",
    "end": "1150679"
  },
  {
    "text": "really. What people use for exploding\ngradients is a crude hack. They clip gradients.",
    "start": "1150680",
    "end": "1157480"
  },
  {
    "text": "But it works really\nwell, and you really want to know about this\nbecause clipping gradients is",
    "start": "1157480",
    "end": "1164110"
  },
  {
    "text": "often essential to having neural\nnetworks not having problems. So what we do for\ngradient clipping",
    "start": "1164110",
    "end": "1170800"
  },
  {
    "text": "is we work out the\nnorm of the gradient. And if it seems too large--",
    "start": "1170800",
    "end": "1177279"
  },
  {
    "text": "and that varies, but that's\nnormally 5, 10, 20, something like that for a\nnorm of a gradient",
    "start": "1177280",
    "end": "1183520"
  },
  {
    "text": "is seen as the\nlimit of what's OK. If the norm of your\ngradient is too large, you just scale it down\nin every direction",
    "start": "1183520",
    "end": "1191140"
  },
  {
    "text": "and you apply a smaller\ngradient update. It works.",
    "start": "1191140",
    "end": "1197770"
  },
  {
    "text": "Yeah, so that\nproblem is solvable, but fixing the\nvanishing gradient",
    "start": "1197770",
    "end": "1204220"
  },
  {
    "text": "seemed a more difficult problem,\nthat this was the problem that our RNNs effectively\ncouldn't preserve information",
    "start": "1204220",
    "end": "1213280"
  },
  {
    "text": "over many time steps. And well, what seemed\nto be the problem there?",
    "start": "1213280",
    "end": "1219020"
  },
  {
    "text": "The problem seems to\nbe really that we've got sort of an architecture that\nmakes it very hard to preserve",
    "start": "1219020",
    "end": "1226390"
  },
  {
    "text": "information. So if we look at the hidden\nstate from one time step",
    "start": "1226390",
    "end": "1232270"
  },
  {
    "text": "to the next time step, it's\ncompletely being rewritten.",
    "start": "1232270",
    "end": "1237820"
  },
  {
    "text": "So we're taking the previous\ntime step's hidden vector.",
    "start": "1237820",
    "end": "1243429"
  },
  {
    "text": "We're multiplying it by a matrix\nwhich completely changes it in general, adding in\nother stuff from the input.",
    "start": "1243430",
    "end": "1250720"
  },
  {
    "text": "So if we'd like to\nsay, we'd like you to carry forward information.",
    "start": "1250720",
    "end": "1256140"
  },
  {
    "text": "There's useful\nstuff in ht minus 1. Can you just keep it\naround for a while?",
    "start": "1256140",
    "end": "1261490"
  },
  {
    "text": "It's not actually very easy\nto do in this formulation because trying to learn w\nvectors that will mostly",
    "start": "1261490",
    "end": "1270659"
  },
  {
    "text": "preserve what was there before\nisn't at all an obvious thing to do.",
    "start": "1270660",
    "end": "1275850"
  },
  {
    "text": "So the question\nwas, could we design an RNN which had a\nmemory where it was",
    "start": "1275850",
    "end": "1283020"
  },
  {
    "text": "easy to preserve information? Yes? So in one of the\nearlier slides, you mentioned the\nexponentiation happened.",
    "start": "1283020",
    "end": "1291360"
  },
  {
    "text": "During their analysis, they\nremoved the nonlinearity? Yeah. So does having a nonlinearity\nand doing the differentiation",
    "start": "1291360",
    "end": "1299700"
  },
  {
    "text": "prevent vanishing or exploding? No, it actually doesn't.",
    "start": "1299700",
    "end": "1304809"
  },
  {
    "text": "You can make an argument that it\nshould help because you've got-- effectively, if you've\ngot something like 10h,",
    "start": "1304810",
    "end": "1310900"
  },
  {
    "text": "you've got a\nflattening function. So it should help somewhat,\nbut it doesn't solve it,",
    "start": "1310900",
    "end": "1317100"
  },
  {
    "text": "even if you're using\na 10h nonlinearity. Well, I guess it should--",
    "start": "1317100",
    "end": "1322380"
  },
  {
    "text": "sorry, it should help\nwith exploding, though. Actually, even\nthat still happens,",
    "start": "1322380",
    "end": "1327950"
  },
  {
    "text": "but it definitely doesn't\nhelp with the vanishing. If you have a sigmoid,\nthen I think the partial",
    "start": "1327950",
    "end": "1333760"
  },
  {
    "text": "of that is 1 minus\nthe sigmoid value, so you're always pushing\nthe value between 0 and 1. So it's not going\nup or going down.",
    "start": "1333760",
    "end": "1340250"
  },
  {
    "text": "It's staying between\n0 and 1, so yeah. ",
    "start": "1340250",
    "end": "1347659"
  },
  {
    "text": "Is your sigmoid [INAUDIBLE]? ",
    "start": "1347660",
    "end": "1354150"
  },
  {
    "text": "Well, I guess wouldn't it\ngo up and down [INAUDIBLE]? So if you have a\nreally small volume, that becomes 1 minus\na really small value?",
    "start": "1354150",
    "end": "1360960"
  },
  {
    "text": "Let's say 1 times 1 minus sigma [INAUDIBLE] ",
    "start": "1360960",
    "end": "1369230"
  },
  {
    "text": "Yeah, so can we have a\ndifferent architecture,",
    "start": "1369230",
    "end": "1374580"
  },
  {
    "text": "so we have a memory\nthat you can add to? And so that led\ninto this new kind",
    "start": "1374580",
    "end": "1382070"
  },
  {
    "text": "of neural network, the LSTM. So this is going\nback a few years.",
    "start": "1382070",
    "end": "1387450"
  },
  {
    "text": "But at any rate, this was trying\nto improve series suggestions, and the big breakthrough\nthat was being described",
    "start": "1387450",
    "end": "1395899"
  },
  {
    "text": "was, oh, we are now using an\nLSTM in the keyboard prediction.",
    "start": "1395900",
    "end": "1401040"
  },
  {
    "text": "And the whole advantage\nof that was it was going to be able to predict\ncontext further back so you",
    "start": "1401040",
    "end": "1407360"
  },
  {
    "text": "could differentiate between,\nthe children are playing in the park, versus the Orioles\nare playing in the playoff.",
    "start": "1407360",
    "end": "1415250"
  },
  {
    "text": "OK, so the big thing that\nwas seen as very successful",
    "start": "1415250",
    "end": "1421880"
  },
  {
    "text": "was these LSTMs, Long\nShort-Term Memory.",
    "start": "1421880",
    "end": "1427220"
  },
  {
    "text": "Just to say a little\nbit of the history here, just on how to\nparse this name, I",
    "start": "1427220",
    "end": "1436080"
  },
  {
    "text": "think people often don't\neven understand it. So what you're wanting to do\nwas model short-term memory,",
    "start": "1436080",
    "end": "1443309"
  },
  {
    "text": "because so for humans,\npeople normally distinguish between the\nshort-term memory of stuff",
    "start": "1443310",
    "end": "1448710"
  },
  {
    "text": "that you heard recently versus\nthings that you've permanently stored away.",
    "start": "1448710",
    "end": "1454049"
  },
  {
    "text": "And the suggestion was,\nwell, in short-term memory, humans can remember\nstuff for quite a while.",
    "start": "1454050",
    "end": "1462400"
  },
  {
    "text": "If you're having a conversation,\nyou can still remember the thing that the person\nsaid a few turns ago",
    "start": "1462400",
    "end": "1468420"
  },
  {
    "text": "and the conversation to\nbring back up of, oh, didn't you say they talked\nlast weekend off or something?",
    "start": "1468420",
    "end": "1475895"
  },
  {
    "text": "And well, the problem\nwas the simple RNNs, their short-term memory was\nonly about seven tokens.",
    "start": "1475895",
    "end": "1483210"
  },
  {
    "text": "And so we'd like to make\nit better than that, and so we wanted long\nshort-term memory.",
    "start": "1483210",
    "end": "1488670"
  },
  {
    "text": "And that's where\nthis name came about. And so this was a type of\nrecurrent neural network",
    "start": "1488670",
    "end": "1495419"
  },
  {
    "text": "that was proposed by\nHochreiter and Schmidhuber",
    "start": "1495420",
    "end": "1501270"
  },
  {
    "text": "in 1997 as a solution\nto the problem. There's actually a second\nrelevant piece of work",
    "start": "1501270",
    "end": "1507660"
  },
  {
    "text": "that came a few years later. That first paper is the\none that everybody cites.",
    "start": "1507660",
    "end": "1513430"
  },
  {
    "text": "But there's been a second paper\nby Gers and Schmidhuber in 2000",
    "start": "1513430",
    "end": "1518680"
  },
  {
    "text": "which actually introduces\na crucial part of the LSTM as we've used it in\nthe 21st century that",
    "start": "1518680",
    "end": "1525300"
  },
  {
    "text": "wasn't in the original paper. And so there's an interesting\nstory of all of this.",
    "start": "1525300",
    "end": "1535930"
  },
  {
    "text": "So Jurgen Schmidhuber and his\nstudents did a lot of really",
    "start": "1535930",
    "end": "1542810"
  },
  {
    "text": "crucial foundational work in\nneural networks in these years,",
    "start": "1542810",
    "end": "1549690"
  },
  {
    "text": "in the later years of the '90s,\nwhen just about everybody else had given up on neural networks.",
    "start": "1549690",
    "end": "1558245"
  },
  {
    "text": "So unlike these days where\ndoing pioneering work in neural networks\nis a really good way",
    "start": "1558245",
    "end": "1565250"
  },
  {
    "text": "to get yourself hugely\ncompensated jobs at Google, Meta, or OpenAI, it really\nwasn't actually in these days.",
    "start": "1565250",
    "end": "1573539"
  },
  {
    "text": "So if you ask, what happened\nto these students of Hochreiter",
    "start": "1573540",
    "end": "1581450"
  },
  {
    "text": "and Gers, both of them\nare still in academia",
    "start": "1581450",
    "end": "1586710"
  },
  {
    "text": "but Gers seemed to give up on AI\nand neural networks altogether",
    "start": "1586710",
    "end": "1592370"
  },
  {
    "text": "and does stuff in the\narea of multimedia. And Sepp Hochreiter is\nstill in machine learning,",
    "start": "1592370",
    "end": "1601410"
  },
  {
    "text": "but for quite a long\ntime, he basically gave up on doing more general\nneural network stuff",
    "start": "1601410",
    "end": "1607730"
  },
  {
    "text": "and went into bioinformatics. So if you look at\nhis publications from about 2000, 2015, they\nwere all in bioinformatics,",
    "start": "1607730",
    "end": "1615890"
  },
  {
    "text": "and most of them weren't\nusing neural networks at all. Though nicely, he's actually\ngotten back into neural networks",
    "start": "1615890",
    "end": "1623900"
  },
  {
    "text": "more recently and is publishing\nin neural networks again. Yeah, so really\nnot much attention",
    "start": "1623900",
    "end": "1631070"
  },
  {
    "text": "was paid to this\nwork at the time. And so it only really\ngradually seeped out further.",
    "start": "1631070",
    "end": "1638540"
  },
  {
    "text": "So Schmidhuber had\na later student in the mid-2000s\ndecade, Alex Graves,",
    "start": "1638540",
    "end": "1646460"
  },
  {
    "text": "and Alex Graves did\nmore stuff with LSTMs.",
    "start": "1646460",
    "end": "1651899"
  },
  {
    "text": "And for people who've seen\nspeech recognition where people commonly do CTC\nloss and decoding,",
    "start": "1651900",
    "end": "1658710"
  },
  {
    "text": "Alex Graves invented that. But most crucially,\nAlex Graves then",
    "start": "1658710",
    "end": "1665370"
  },
  {
    "text": "went to Toronto to be a\npostdoc for Jeff Hinton. And that brought more\nattention to the fact",
    "start": "1665370",
    "end": "1672870"
  },
  {
    "text": "that LSTMs were a good model. And then Jeff Hinton\nwent to Google in 2013,",
    "start": "1672870",
    "end": "1680340"
  },
  {
    "text": "and that was then the use of\nLSTMs at Google in the 2014",
    "start": "1680340",
    "end": "1686789"
  },
  {
    "text": "to '16 period, was when they\nhit the world and became,",
    "start": "1686790",
    "end": "1692670"
  },
  {
    "text": "for a while, a completely\ndominant framework people use for neural networks.",
    "start": "1692670",
    "end": "1698929"
  },
  {
    "text": "In the world of,\nI guess, startups, this is what you call being\ntoo early for the first people.",
    "start": "1698930",
    "end": "1706970"
  },
  {
    "text": "Yeah, OK, long short-term\nmemories, back to the science,",
    "start": "1706970",
    "end": "1712159"
  },
  {
    "text": "so let's see. There's a slide here that talks\nabout long short-term memories,",
    "start": "1712160",
    "end": "1720720"
  },
  {
    "text": "but maybe I'll just\nskip straight ahead and start to show the pictures.",
    "start": "1720720",
    "end": "1725840"
  },
  {
    "text": "So we've still got a\nsequence of inputs xt, and the difference now is\ninside our neural network,",
    "start": "1725840",
    "end": "1733980"
  },
  {
    "text": "we're going to have two hidden\nthings, one that's still called the hidden state\nand the other one that's",
    "start": "1733980",
    "end": "1739940"
  },
  {
    "text": "referred to as the cell state. And so what we're\ngoing to do is we're",
    "start": "1739940",
    "end": "1747049"
  },
  {
    "text": "going to modulate\nhow these things get updated by introducing\nthe idea of gates.",
    "start": "1747050",
    "end": "1753240"
  },
  {
    "text": "And so gates are\ncalculated things, vectors,",
    "start": "1753240",
    "end": "1758730"
  },
  {
    "text": "whose values are\nprobabilities between 0 and 1, and they're things\nthat we're going",
    "start": "1758730",
    "end": "1764400"
  },
  {
    "text": "to use to turn things on or shut\nthem off in a probabilistic way. So we're going to control\nthe movement of information",
    "start": "1764400",
    "end": "1772350"
  },
  {
    "text": "by having gating. And so we're going to\ncalculate three gating vectors.",
    "start": "1772350",
    "end": "1778750"
  },
  {
    "text": "So these vectors are the same\nlength as our hidden states.",
    "start": "1778750",
    "end": "1784200"
  },
  {
    "text": "And so the way we calculate\nthese gating vectors is with an equation that\nlooks basically exactly",
    "start": "1784200",
    "end": "1792419"
  },
  {
    "text": "the same as what we were using\nfor recurrent neural networks. Apart from the sigma,\nthere is definitely",
    "start": "1792420",
    "end": "1799260"
  },
  {
    "text": "going to be the logistic\nthat goes between 0 and 1, so we get probabilities. And the three gates\nwe're going to calculate",
    "start": "1799260",
    "end": "1805860"
  },
  {
    "text": "is the forget gate\nwhich is going to say, how much do we\nremember of the previous time's",
    "start": "1805860",
    "end": "1812700"
  },
  {
    "text": "hidden state? I think the forget gate\nwas actually wrongly named. I think it makes more sense to\nthink of it as a remember gate",
    "start": "1812700",
    "end": "1819960"
  },
  {
    "text": "because it's\nactually calculating how much you're remembering.",
    "start": "1819960",
    "end": "1825030"
  },
  {
    "text": "OK, then we've\ngot an input gate, and the input gate is going\nto say, how much are you",
    "start": "1825030",
    "end": "1830669"
  },
  {
    "text": "going to pay attention to\nthe next input, the next xi, and put it into\nyour hidden state?",
    "start": "1830670",
    "end": "1837970"
  },
  {
    "text": "And then you have\nan output gate, and the output gate is going\nto control how much of what's",
    "start": "1837970",
    "end": "1844620"
  },
  {
    "text": "in the cell, which is\nyour primary memory, are you going to transfer\nover to the hidden state",
    "start": "1844620",
    "end": "1850860"
  },
  {
    "text": "of the network? OK, so once we have those gates,\nwhat we're then going to do",
    "start": "1850860",
    "end": "1860790"
  },
  {
    "text": "is have these\nequations, which are how we're going to update things.",
    "start": "1860790",
    "end": "1867460"
  },
  {
    "text": "So the first thing\nwe're going to do is work out a potential\nnew cell content.",
    "start": "1867460",
    "end": "1877029"
  },
  {
    "text": "So the new cell content\nis going to be calculated",
    "start": "1877030",
    "end": "1883380"
  },
  {
    "text": "using this exactly the\nsame kind of equation we saw last time for\nrecurrent neural networks.",
    "start": "1883380",
    "end": "1889060"
  },
  {
    "text": "We're going to have these\ntwo matrices, the cell w and the cell u,\nand we're going",
    "start": "1889060",
    "end": "1896070"
  },
  {
    "text": "to multiply 1 by the\nlast time's hidden state and the other by the new\ninput, add on a bias.",
    "start": "1896070",
    "end": "1903549"
  },
  {
    "text": "And that's a potential\nupdate to the cell. But then how we're actually\ngoing to update the cell",
    "start": "1903550",
    "end": "1910320"
  },
  {
    "text": "is by making use of our gates. So we're going to say the\nnew cell's content is going",
    "start": "1910320",
    "end": "1917340"
  },
  {
    "text": "to be the old cell's\ncontent Hadamard producted",
    "start": "1917340",
    "end": "1922740"
  },
  {
    "text": "with the forget gate, so\nthat's how much to remember of the previous cell's content,\nplus this calculated update",
    "start": "1922740",
    "end": "1932279"
  },
  {
    "text": "Hadamard producted with the\ninput gate, how much to pay attention to this new potential\nupdate that we've invented.",
    "start": "1932280",
    "end": "1941840"
  },
  {
    "text": "And then for calculating\nthe new hidden state, that's going to be the Hadamard\nproduct between the output gate",
    "start": "1941840",
    "end": "1952159"
  },
  {
    "text": "and our CT having been\nput through a tanh.",
    "start": "1952160",
    "end": "1959510"
  },
  {
    "text": "And one idea here\nis, we're thinking",
    "start": "1959510",
    "end": "1964970"
  },
  {
    "text": "about how much to\nkeep on remembering what we've had in the past. But for thinking about only\nsending some information",
    "start": "1964970",
    "end": "1975080"
  },
  {
    "text": "to the hidden state, a way\nto start thinking about that is, the hidden state of a\nrecurrent neural network",
    "start": "1975080",
    "end": "1982940"
  },
  {
    "text": "is doing multiple duty. On one part of it\nis we are going",
    "start": "1982940",
    "end": "1989120"
  },
  {
    "text": "to feed that into the output\nto predict the next token. But another thing\nthat's going to do",
    "start": "1989120",
    "end": "1996290"
  },
  {
    "text": "is we just want it to store\ninformation about the past that might come in useful\nlater and that we'd",
    "start": "1996290",
    "end": "2002289"
  },
  {
    "text": "like to have carried\nthrough the sequence. And so really only some of\nwhat's in the hidden state",
    "start": "2002290",
    "end": "2009309"
  },
  {
    "text": "we want to be using to\npredict the current word. Some of it isn't relevant to\npredicting the current word",
    "start": "2009310",
    "end": "2014850"
  },
  {
    "text": "but would be good stuff\nto for the future. So if the previous\nwords were sat in",
    "start": "2014850",
    "end": "2022880"
  },
  {
    "text": "for predicting the\nnext word, we basically just need to know we're\nin a sat-in context",
    "start": "2022880",
    "end": "2027950"
  },
  {
    "text": "where [INAUDIBLE]\nwill come next. But if earlier on\nthe sentence had",
    "start": "2027950",
    "end": "2033500"
  },
  {
    "text": "been saying the King of Prussia,\nsomewhere in the hidden state, we want to be keeping the\ninformation that there's",
    "start": "2033500",
    "end": "2039200"
  },
  {
    "text": "a King of Prussia because that\nmight be relevant for predicting future words. And so it makes\nsense that we only",
    "start": "2039200",
    "end": "2045770"
  },
  {
    "text": "want to have some of\nwhat's in our memory being used to predict the next\nword in the current context.",
    "start": "2045770",
    "end": "2052530"
  },
  {
    "text": "So the cell is our\nlong short-term memory, and then we're moving\nover to the hidden state",
    "start": "2052530",
    "end": "2058138"
  },
  {
    "text": "things that are going to\nbe relevant for generation. Yeah, I've said that.",
    "start": "2058139",
    "end": "2064399"
  },
  {
    "text": "OK, all these are vectors\nof the same length, n.",
    "start": "2064400",
    "end": "2070399"
  },
  {
    "text": "Yeah, so all of these\nthings, both the gates and the new values for\nthe cell and hidden state,",
    "start": "2070400",
    "end": "2077940"
  },
  {
    "text": "they're all vectors of length n. And part of how things\nactually get convenient",
    "start": "2077940",
    "end": "2085908"
  },
  {
    "text": "when you're actually running\nup these is up until here, all of these things have\nexactly the same shape.",
    "start": "2085909",
    "end": "2093210"
  },
  {
    "text": "So you can actually put them\nall together into a big matrix and do the computations\nof all four of these",
    "start": "2093210",
    "end": "2100400"
  },
  {
    "text": "in terms of one big\nmatrix, if you want. Question? [INAUDIBLE] calculation in\nthe hidden state update,",
    "start": "2100400",
    "end": "2109250"
  },
  {
    "text": "then the output gate\nshould have been expressed by both the input\ngates and the forget gate,",
    "start": "2109250",
    "end": "2114500"
  },
  {
    "text": "right?  You're saying if this\nbit wasn't here, then--",
    "start": "2114500",
    "end": "2122250"
  },
  {
    "text": "Then you would not\nneed an output gate. Well. Because [INAUDIBLE] would\nhave been able to express it,",
    "start": "2122250",
    "end": "2130230"
  },
  {
    "text": "account for it in some sense. My question is, how\nmuch does having--",
    "start": "2130230",
    "end": "2135790"
  },
  {
    "text": "Well, no, to the\nextent that you want to mask out part of\nwhat's in the cell",
    "start": "2135790",
    "end": "2142609"
  },
  {
    "text": "so it's not visible when you're\ngenerating the next token, isn't it still useful\nto have an output gate?",
    "start": "2142610",
    "end": "2149980"
  },
  {
    "text": "You can essentially have c\nis equal [INAUDIBLE] if you don't have that.",
    "start": "2149980",
    "end": "2155470"
  },
  {
    "text": "But you don't want\nht equal to CT. You want some of\nthe contents of ct",
    "start": "2155470",
    "end": "2160900"
  },
  {
    "text": "to be masked out so\nyou're not seeing it when generating the output. That masking would have been\naccounted for by [INAUDIBLE].",
    "start": "2160900",
    "end": "2168750"
  },
  {
    "text": "No, because you want\nto keep it in ct-- there's information you want\nto keep in ct for the future",
    "start": "2168750",
    "end": "2176490"
  },
  {
    "text": "but you don't want\nvisible when generating the current next word, yeah.",
    "start": "2176490",
    "end": "2181720"
  },
  {
    "text": " In some sense, a bit I have\nthe hardest part explaining",
    "start": "2181720",
    "end": "2187250"
  },
  {
    "text": "is, why is it necessarily\nbetter to have a tanh here?",
    "start": "2187250",
    "end": "2193140"
  },
  {
    "text": "You can argue that it's\na way of, this can just stay unbounded, real\nnumbers, and then this",
    "start": "2193140",
    "end": "2201589"
  },
  {
    "text": "is getting it back in the shape\nof, stays between 0 and 1, which is good for\nthe hidden state.",
    "start": "2201590",
    "end": "2208410"
  },
  {
    "text": "But it's a little bit-- I guess they did it that way,\nand it seemed to work well.",
    "start": "2208410",
    "end": "2213680"
  },
  {
    "text": "OK, here's another\nway of looking at it which may or may not be\nmore helpful, is a picture.",
    "start": "2213680",
    "end": "2220290"
  },
  {
    "text": "So at each time step,\nwe've got, as before,",
    "start": "2220290",
    "end": "2226850"
  },
  {
    "text": "an input, a hidden\nstate, and then we're going to calculate an output\nfrom that hidden state.",
    "start": "2226850",
    "end": "2233520"
  },
  {
    "text": "But we've got this more\ncomplex computational unit. And these pictures of this\nmore complex computational unit",
    "start": "2233520",
    "end": "2242210"
  },
  {
    "text": "were diagrams that were made\nby Chris Oler, who is someone who now works at Anthropic.",
    "start": "2242210",
    "end": "2251640"
  },
  {
    "text": "And so if you blow up in that,\nthis is showing the computation. So you're feeding along\nrecurrently the c cell",
    "start": "2251640",
    "end": "2263790"
  },
  {
    "text": "as the primary recurrent\nunit, but you've also got carried along h because h\nis being used to calculate stuff",
    "start": "2263790",
    "end": "2272220"
  },
  {
    "text": "for the next time step, and\nthen a new h is being generated. And so you're computing\nthe forget gate.",
    "start": "2272220",
    "end": "2279700"
  },
  {
    "text": "You're forgetting some\nof the cell content. You're computing an input gate. You're using that to compute\na potential new cell content.",
    "start": "2279700",
    "end": "2290145"
  },
  {
    "text": "You write some of\nthat into the cell depending on the input gate,\nthen you compute an output gate,",
    "start": "2290145",
    "end": "2298770"
  },
  {
    "text": "and then some of the cell will\ngo into the computation of h,",
    "start": "2298770",
    "end": "2306030"
  },
  {
    "text": "depending on the output gate. And then just like for the\nprevious recurrent neural",
    "start": "2306030",
    "end": "2311810"
  },
  {
    "text": "network, for working out what\nthe predicted next word is,",
    "start": "2311810",
    "end": "2317610"
  },
  {
    "text": "you're working out an\noutput layer by taking the h and doing another\nmatrix, uh plus b2,",
    "start": "2317610",
    "end": "2325280"
  },
  {
    "text": "and then using a softmax\non that to actually predict the next word.",
    "start": "2325280",
    "end": "2331190"
  },
  {
    "text": "OK, so this all\nseems very complex.",
    "start": "2331190",
    "end": "2337700"
  },
  {
    "text": "And back in-- do\nyou have a question? Yeah. I just had a quick question. So how were we\ndeciding the threshold?",
    "start": "2337700",
    "end": "2345839"
  },
  {
    "text": "I imagine it's just\nsome sort of threshold around the probability\nof what we're remembering",
    "start": "2345840",
    "end": "2351507"
  },
  {
    "text": "and what we're forgetting.  So we're getting more\nthan a threshold,",
    "start": "2351507",
    "end": "2359380"
  },
  {
    "text": "because we're calculating a\nwhole vector of forgetting and remembering.",
    "start": "2359380",
    "end": "2365030"
  },
  {
    "text": "So therefore, it can choose to\nsay, OK, dimensions 1 to 17,",
    "start": "2365030",
    "end": "2370070"
  },
  {
    "text": "keep all of that and throw\naway dimensions 18 to 22 or really probabilistically\nto different extents.",
    "start": "2370070",
    "end": "2377380"
  },
  {
    "text": "And so it's unspecified. It's up to it what it learns.",
    "start": "2377380",
    "end": "2382520"
  },
  {
    "text": "But we're hoping\nthat it will learn that certain kinds\nof information",
    "start": "2382520",
    "end": "2387670"
  },
  {
    "text": "is useful to keep carrying\nforward for at least a while. But then we can use both the\ncontents of the hidden state",
    "start": "2387670",
    "end": "2395740"
  },
  {
    "text": "and the cell-- sorry, of the next input\nto decide to throw away certain information.",
    "start": "2395740",
    "end": "2401599"
  },
  {
    "text": "So we might think that\nthere are certain cues. For example, if\nit sees the word,",
    "start": "2401600",
    "end": "2406869"
  },
  {
    "text": "next, it might think,\nOK, change of topic. Now would be a good time to\nforget more stuff and reset.",
    "start": "2406870",
    "end": "2412819"
  },
  {
    "text": "But it's learning which\ndimensions of this vector to hold around in an\nunconstrained way of whatever's",
    "start": "2412820",
    "end": "2419990"
  },
  {
    "text": "useful to do a better\njob at language modeling. OK, yeah, so this all looks like\na very complex and cantankerous",
    "start": "2419990",
    "end": "2431040"
  },
  {
    "text": "design. And quite honestly, when\nteaching this around 2016",
    "start": "2431040",
    "end": "2438465"
  },
  {
    "text": "and '17 and this was the\nbest kind of neural network we had for language\nmodeling, we literally",
    "start": "2438465",
    "end": "2445350"
  },
  {
    "text": "spent hours of class time going\nthrough LSTMs and variants",
    "start": "2445350",
    "end": "2450780"
  },
  {
    "text": "of LSTMs with\ndifferent properties because there are different\nways you can do the gating. You can have less gates or more\ngates and do different things.",
    "start": "2450780",
    "end": "2459190"
  },
  {
    "text": "And it seemed the most\nimportant thing to know. In 2024, it's probably not\nthe most important thing",
    "start": "2459190",
    "end": "2468000"
  },
  {
    "text": "to know but LSTMs are\na thing to be aware of.",
    "start": "2468000",
    "end": "2474610"
  },
  {
    "text": "We are going to use\nthem for assignment 3. But you can just ask\nPyTorch for an LSTM,",
    "start": "2474610",
    "end": "2482080"
  },
  {
    "text": "and it'll give you one that\ndoes all of this stuff. But there is one thing that I\nreally want to focus on as to,",
    "start": "2482080",
    "end": "2492325"
  },
  {
    "text": "what is the good thing\nthat an LSTM achieves? And really the\nsecret for why you",
    "start": "2492325",
    "end": "2497950"
  },
  {
    "text": "get this fundamentally different\nbehavior in an LSTM is you have that plus sign right there,\nthat for the simple recurrent",
    "start": "2497950",
    "end": "2508030"
  },
  {
    "text": "neural network, at each\ntime, the next hidden state was a result of\nmultiplicative stuff.",
    "start": "2508030",
    "end": "2515990"
  },
  {
    "text": "And therefore, it was very hard\njust to preserve information.",
    "start": "2515990",
    "end": "2521140"
  },
  {
    "text": "Whereas the essence\nof the LSTM is to say, well, look, you've got\nthis past memory of stuff",
    "start": "2521140",
    "end": "2528400"
  },
  {
    "text": "you've already seen. And what we want to do is\nadd some new information",
    "start": "2528400",
    "end": "2533650"
  },
  {
    "text": "to it, which fundamentally\nseems right for human memories,",
    "start": "2533650",
    "end": "2539049"
  },
  {
    "text": "that they're basically additive. And when I said, actually, it\nwas the second Gers paper that",
    "start": "2539050",
    "end": "2545710"
  },
  {
    "text": "introduced a crucial\npart of the LSTM, the first version of the LSTM\ndidn't have the forget gate.",
    "start": "2545710",
    "end": "2552470"
  },
  {
    "text": "So it was a purely\nadditive mechanism that you were deciding\nwhat to add to your memory",
    "start": "2552470",
    "end": "2558280"
  },
  {
    "text": "as you went along. But that proved to\nbe not quite perfect,",
    "start": "2558280",
    "end": "2563840"
  },
  {
    "text": "because if you keep on\nadding more and more stuff over a long sequence,\nthat tends to be dysfunctional after\na certain point.",
    "start": "2563840",
    "end": "2570650"
  },
  {
    "text": "And so the big improvement was\nthen to add this forget gate, so some of it went away. But nevertheless, having\nthings basically additive",
    "start": "2570650",
    "end": "2579430"
  },
  {
    "text": "fixes the problem\nof gradient flow. You no longer have\nvanishing gradients.",
    "start": "2579430",
    "end": "2586340"
  },
  {
    "text": "And it makes it something that\nseems much more memory-like. You're adding to the\nthings that you know.",
    "start": "2586340",
    "end": "2593140"
  },
  {
    "text": "OK, so the LSTM\narchitecture allows you to preserve information\nover many time sets in the cell.",
    "start": "2593140",
    "end": "2602470"
  },
  {
    "text": "So if you set the forget gate\nto 1 and the input gate to 0,",
    "start": "2602470",
    "end": "2607490"
  },
  {
    "text": "you are just linearly passing\nalong in the cell indefinitely the same information.",
    "start": "2607490",
    "end": "2615060"
  },
  {
    "text": "OK, it's not the only\nway that you can do long-distance information flow.",
    "start": "2615060",
    "end": "2620650"
  },
  {
    "text": "And we're going to\nlook increasingly in future lectures\nat other ways you",
    "start": "2620650",
    "end": "2626190"
  },
  {
    "text": "can do long-distance\ninformation flow, and just to give a bit\nof a peek about those now",
    "start": "2626190",
    "end": "2634680"
  },
  {
    "text": "and to think about\nother architectures. But there's a question, no? No question? Yes?",
    "start": "2634680",
    "end": "2641474"
  },
  {
    "text": "So since you're mentioning\nthe [INAUDIBLE],",
    "start": "2641475",
    "end": "2647580"
  },
  {
    "text": "does it help with\nexploding gradient at all? Does it make it worse? Is there no difference?",
    "start": "2647580",
    "end": "2652650"
  },
  {
    "text": "No, it also helps with\nexploding gradients. Because of the fact that\nyou're not doing this sequence,",
    "start": "2652650",
    "end": "2657810"
  },
  {
    "text": "it multiplies all the time, that\nyou have this addition operator. ",
    "start": "2657810",
    "end": "2667745"
  },
  {
    "text": "So one thing you\ncould wonder is that, is vanishing and\nexploding gradients",
    "start": "2667746",
    "end": "2673550"
  },
  {
    "text": "just a recurrent\nneural network problem? And it's not.",
    "start": "2673550",
    "end": "2679890"
  },
  {
    "text": "It occurs earlier and worse\nwhen you've got long sequences. But if you start building\na very deep neural network,",
    "start": "2679890",
    "end": "2687830"
  },
  {
    "text": "surely the same\nthing is happening. The parameters aren't the\nsame, so it's not quite just",
    "start": "2687830",
    "end": "2693700"
  },
  {
    "text": "raising one matrix to a power. But surely depending\non your matrices, you tend to have\nthe same problem",
    "start": "2693700",
    "end": "2700930"
  },
  {
    "text": "that either your\ngradients are disappearing or else they're exploding. And that's what people found.",
    "start": "2700930",
    "end": "2707600"
  },
  {
    "text": "And that was part of the\nreason why in the early days, people weren't very successful\nbuilding deep neural networks,",
    "start": "2707600",
    "end": "2714250"
  },
  {
    "text": "was because they suffered\nfrom problems of this sort, that if you had\nbasically vanishing",
    "start": "2714250",
    "end": "2719770"
  },
  {
    "text": "gradients in a deep\nneural network, you got very little gradient\nsignal in the lower layers.",
    "start": "2719770",
    "end": "2726569"
  },
  {
    "text": "Therefore, their parameters\ndidn't really update. Therefore, your model\ndidn't learn anything in the lower layers.",
    "start": "2726570",
    "end": "2732450"
  },
  {
    "text": "Therefore, the network\ndidn't work well. And that was part of why things\nwere stuck in the days around",
    "start": "2732450",
    "end": "2738740"
  },
  {
    "text": "the early 2000s of, deep\nnetworks didn't work. And so there are other ways you\ncan think about fixing that.",
    "start": "2738740",
    "end": "2746609"
  },
  {
    "text": "So one common way\nof fixing it is to add more direct connections.",
    "start": "2746610",
    "end": "2752190"
  },
  {
    "text": "So the problem when we went\nthrough our recurrent step",
    "start": "2752190",
    "end": "2758089"
  },
  {
    "text": "was, we had this in-between\nstuff of doing a matrix multiply",
    "start": "2758090",
    "end": "2763430"
  },
  {
    "text": "and blah, blah, blah. And that caused indirectness\nand the possibility",
    "start": "2763430",
    "end": "2770000"
  },
  {
    "text": "for things to either\nexplode or vanish. So this network is written\nsort of upside-down",
    "start": "2770000",
    "end": "2777289"
  },
  {
    "text": "when I stole the\npicture from the paper, so we'll just have\nto deal with that.",
    "start": "2777290",
    "end": "2782780"
  },
  {
    "text": "So we're going downwards\nfrom here to the next layer. So rather than going through\nweight layers and weight",
    "start": "2782780",
    "end": "2790730"
  },
  {
    "text": "layers which will\nstart to produce the same kind of\nproblems, what you can do is apply the same trick\nin a vertical network",
    "start": "2790730",
    "end": "2800720"
  },
  {
    "text": "and say, well, look,\nI can also just carry the input around\nwith an identity function",
    "start": "2800720",
    "end": "2806119"
  },
  {
    "text": "and add it on here. And so then, I've got this\ndirect carrying of information.",
    "start": "2806120",
    "end": "2811890"
  },
  {
    "text": "And so that led to\nthe residual network, which was what completely\ntransformed computer vision",
    "start": "2811890",
    "end": "2819049"
  },
  {
    "text": "models and made them\nmuch more learnable than pure networks that lack\nthese residual connections.",
    "start": "2819050",
    "end": "2829380"
  },
  {
    "text": "If you start heading\ndown that path, you can think, well, why only\nprovide these residual loops",
    "start": "2829380",
    "end": "2836400"
  },
  {
    "text": "that take you one step? Maybe I could directly\nconnect each layer",
    "start": "2836400",
    "end": "2842040"
  },
  {
    "text": "to all the successive layers. And so people played\nwith that idea, and that led to the\nso-called DenseNet,",
    "start": "2842040",
    "end": "2849510"
  },
  {
    "text": "where you have these\nskip connections linking to every other layer.",
    "start": "2849510",
    "end": "2855150"
  },
  {
    "text": "A variant of the residual\nnetwork, the ResNet,",
    "start": "2855150",
    "end": "2861599"
  },
  {
    "text": "which was actually,\nagain, introduced by Schmidhuber and\nstudents, was to say, well,",
    "start": "2861600",
    "end": "2866890"
  },
  {
    "text": "rather than just directly\nadding in the input summed",
    "start": "2866890",
    "end": "2872970"
  },
  {
    "text": "with the output of the neural\nnetwork layer, maybe again, we'd be better off\nhaving gating so that you",
    "start": "2872970",
    "end": "2879270"
  },
  {
    "text": "are deciding via gates\nhow much of the input to have skip around.",
    "start": "2879270",
    "end": "2884320"
  },
  {
    "text": "And so that led to a variant,\nthe HighwayNet, where you've got",
    "start": "2884320",
    "end": "2890470"
  },
  {
    "text": "gated residual networks, so\nvarious ideas of doing that.",
    "start": "2890470",
    "end": "2895650"
  },
  {
    "text": "I'm not going to say more\nabout that right now. I want to skip ahead and\ndo the rest of neural nets",
    "start": "2895650",
    "end": "2902110"
  },
  {
    "text": "and get on to\nmachine translation.",
    "start": "2902110",
    "end": "2907514"
  },
  {
    "text": "OK, so once you have RNNs\nwhere RNNs is including LSTMs,",
    "start": "2907514",
    "end": "2915150"
  },
  {
    "text": "normally in practice LSTMs, you\ncan use them for anything else where you're doing sequences.",
    "start": "2915150",
    "end": "2920549"
  },
  {
    "text": "And so there are lots of\nplaces they're used in NLP. So if you want to assign\nwords, parts of speech,",
    "start": "2920550",
    "end": "2927830"
  },
  {
    "text": "like nouns and verbs, that\nwould be commonly done with a part of\nspeech tagging LSTM.",
    "start": "2927830",
    "end": "2934400"
  },
  {
    "text": "If you want to be\nassigning named entity labels like location--",
    "start": "2934400",
    "end": "2940275"
  },
  {
    "text": "I did this toy version where\nwe were signing a label to the middle of a window. But if you wanted to assign\na label at each position,",
    "start": "2940275",
    "end": "2947400"
  },
  {
    "text": "you can use an LSTM for\nnamed entity recognition. You can use an RNN as an encoder\nmodel for a whole sentence.",
    "start": "2947400",
    "end": "2956940"
  },
  {
    "text": "So if we want to do\nsentiment classification to see whether a piece of\ntext is positive or negative,",
    "start": "2956940",
    "end": "2963140"
  },
  {
    "text": "we can, say, run an LSTM\nover it and then use",
    "start": "2963140",
    "end": "2968339"
  },
  {
    "text": "this as a representation\nof the sentence to work out whether it's a\npositive or negative piece",
    "start": "2968340",
    "end": "2975539"
  },
  {
    "text": "of text. And well, the simplest\nway of doing that is to use the final hidden\nstate, because after all,",
    "start": "2975540",
    "end": "2982720"
  },
  {
    "text": "that final hidden state\nis the hidden state you've gotten from having seen\nthe entire sentence,",
    "start": "2982720",
    "end": "2987990"
  },
  {
    "text": "and used that, and then\nhave a classification",
    "start": "2987990",
    "end": "2993119"
  },
  {
    "text": "layer, a logistic regression,\non top of that to give you positive or negative.",
    "start": "2993120",
    "end": "2998970"
  },
  {
    "text": "In practice though,\npeople have found it's often better to\nuse every hidden state",
    "start": "2998970",
    "end": "3004460"
  },
  {
    "text": "and take some kind of\nmean or element-wise max and feed that in as\nthe sentence encoding.",
    "start": "3004460",
    "end": "3013280"
  },
  {
    "text": "You can also use RNNs for\nlots of other purposes where you're using\nit to generate text",
    "start": "3013280",
    "end": "3020180"
  },
  {
    "text": "based on other information. So if you want to do speech\nrecognition, or summarization,",
    "start": "3020180",
    "end": "3026630"
  },
  {
    "text": "or machine translation\nthat we'll come to later, you can have an\ninput source, which",
    "start": "3026630",
    "end": "3034910"
  },
  {
    "text": "you'll use to\ncondition your network, and then you'll generate\nthe speech recognition",
    "start": "3034910",
    "end": "3042110"
  },
  {
    "text": "or the machine translation,\nas we'll see later. And so we refer to those as\nconditional language models,",
    "start": "3042110",
    "end": "3050060"
  },
  {
    "text": "because rather than\njust generating text starting from nothing,\nfrom a start token,",
    "start": "3050060",
    "end": "3055920"
  },
  {
    "text": "we're generating it conditioned\non some source of information.",
    "start": "3055920",
    "end": "3062089"
  },
  {
    "text": "One other idea on what normally\nhappens when people use these,",
    "start": "3062090",
    "end": "3070160"
  },
  {
    "text": "I suggested that we could do\nthis averaging at each position.",
    "start": "3070160",
    "end": "3078480"
  },
  {
    "text": "If you think about these\nhidden state representations, these hidden state\nrepresentations,",
    "start": "3078480",
    "end": "3085440"
  },
  {
    "text": "that representation isn't\nonly about the word, terribly. It has some information\nabout what came before it,",
    "start": "3085440",
    "end": "3092880"
  },
  {
    "text": "the movie was terribly. But it has no information\nabout what comes after it.",
    "start": "3092880",
    "end": "3099480"
  },
  {
    "text": "And well, you might\nthink you'd like to have a representation\nof terribly that",
    "start": "3099480",
    "end": "3105440"
  },
  {
    "text": "knows what came before it\nbut also what came after it.",
    "start": "3105440",
    "end": "3111329"
  },
  {
    "text": "And so people came up\nwith the next obvious idea to deal with that, which was\nto build a bidirectional LSTM.",
    "start": "3111330",
    "end": "3120780"
  },
  {
    "text": "So you ran a forward\nLSTM, and then you start another LSTM that's\nshown in that greenish teal,",
    "start": "3120780",
    "end": "3128460"
  },
  {
    "text": "and you ran it backwards. And so then you had a\nforwards-and-backwards vector at each position, and you\njust concatenated them both.",
    "start": "3128460",
    "end": "3137290"
  },
  {
    "text": "And then you had a two-sided\ncontext for a representation of word meaning.",
    "start": "3137290",
    "end": "3143650"
  },
  {
    "text": "And so these networks\nwere pretty widely used.",
    "start": "3143650",
    "end": "3149029"
  },
  {
    "text": "So we were running a\nforward RNN, a backward RNN, and concatenating\nthe states together.",
    "start": "3149030",
    "end": "3157440"
  },
  {
    "text": "And those were then\ncommonly written like this to suggest that\nin a compact way,",
    "start": "3157440",
    "end": "3163770"
  },
  {
    "text": "you're running a\nbidirectional RNN. And these were very popular\nfor language analysis.",
    "start": "3163770",
    "end": "3175589"
  },
  {
    "text": "They weren't workable if you\nwere wanting to generate text, but you were using them in a lot\nof places as a representation.",
    "start": "3175590",
    "end": "3184450"
  },
  {
    "text": "But more recently,\ntransformer models have normally taken\nover from that.",
    "start": "3184450",
    "end": "3189960"
  },
  {
    "text": "One more idea which we'll\nsee for machine translation is RNNs are deep in the sense\nthat they unroll over many time",
    "start": "3189960",
    "end": "3202020"
  },
  {
    "text": "steps, but up until\nnow, they've only been shallow RNNs in\nthe sense that we just had one hidden state.",
    "start": "3202020",
    "end": "3208600"
  },
  {
    "text": "But you can also make\nthem deep by having multiple layers of hidden\nstates, what's also",
    "start": "3208600",
    "end": "3214500"
  },
  {
    "text": "commonly called stacked RNNs. So you'd have several layers\nof RNNs built above each other.",
    "start": "3214500",
    "end": "3223070"
  },
  {
    "text": "And you might wonder, does\nthis really do anything? Are they just big\nvectors above the words?",
    "start": "3223070",
    "end": "3230630"
  },
  {
    "text": "But precisely because you\nhave this extra neural network layer between here and\nhere, you get exactly",
    "start": "3230630",
    "end": "3237970"
  },
  {
    "text": "the same power advantage you get\notherwise with neural networks that you can do successive\nlayers of feature extraction.",
    "start": "3237970",
    "end": "3245630"
  },
  {
    "text": "And so you get more power\nout of your neural network. To some extent, what people--",
    "start": "3245630",
    "end": "3254920"
  },
  {
    "text": "yeah, OK. To some extent, what people\nfound with RNNs in those",
    "start": "3254920",
    "end": "3264390"
  },
  {
    "text": "days is that having multiple\nlayers definitely helps.",
    "start": "3264390",
    "end": "3269769"
  },
  {
    "text": "But unlike what was\nhappening in those days with other kinds of neural\nnetworks for vision, et cetera,",
    "start": "3269770",
    "end": "3275560"
  },
  {
    "text": "people still use\nrelatively shallow RNNs. So you always got a lot of gains\nby having two layers rather than",
    "start": "3275560",
    "end": "3284250"
  },
  {
    "text": "one. But commonly, it started\nto be more iffy whether you got extra value from\nthree or four layers,",
    "start": "3284250",
    "end": "3290589"
  },
  {
    "text": "so commonly people were running\ntwo- or three-layer LSTMs, and that's what\npeople were using.",
    "start": "3290590",
    "end": "3296920"
  },
  {
    "text": "But that's completely\nchanged around in the world of transformers, where\nnowadays people are building",
    "start": "3296920",
    "end": "3303539"
  },
  {
    "text": "very deep transformer\nnetworks for doing language understanding.",
    "start": "3303540",
    "end": "3310740"
  },
  {
    "text": "OK, but I should skip ahead\nand say a few words before time runs out about\nmachine translation.",
    "start": "3310740",
    "end": "3318780"
  },
  {
    "text": "So machine translation is one\nof the key natural language processing tasks where\nwe're translating words",
    "start": "3318780",
    "end": "3326340"
  },
  {
    "text": "from sentences in one\nlanguage to sentences in another language. So we're starting off with a\nsentence in some language here,",
    "start": "3326340",
    "end": "3335319"
  },
  {
    "text": "French, and what we\nwant to do is output it in a different\nlanguage here, English.",
    "start": "3335320",
    "end": "3342900"
  },
  {
    "text": "So machine translation was\nactually where NLP started.",
    "start": "3342900",
    "end": "3351099"
  },
  {
    "text": "So in the early '50s, there\nwasn't artificial intelligence",
    "start": "3351100",
    "end": "3356250"
  },
  {
    "text": "yet, there wasn't\na field of NLP yet, but people started to work\non machine translation.",
    "start": "3356250",
    "end": "3365790"
  },
  {
    "text": "And the story of\nwhy people started to work on machine\ntranslation was essentially,",
    "start": "3365790",
    "end": "3371370"
  },
  {
    "text": "computers were first developed\nduring the Second World War. And during the Second\nWorld War, computers",
    "start": "3371370",
    "end": "3378090"
  },
  {
    "text": "were used for two things. One of them was calculating\nartillery targets, artillery",
    "start": "3378090",
    "end": "3384329"
  },
  {
    "text": "tables, to work out what\nangle to put your gun on to get it to land\nin the right place,",
    "start": "3384330",
    "end": "3389760"
  },
  {
    "text": "not very relevant\nto what we're doing. But the other thing that\ncomputers were used for",
    "start": "3389760",
    "end": "3395730"
  },
  {
    "text": "was code breaking. So after the Second World\nWar, it moved very quickly",
    "start": "3395730",
    "end": "3402720"
  },
  {
    "text": "into the Cold War,\nand there were concerns on both\nsides of keeping up",
    "start": "3402720",
    "end": "3410260"
  },
  {
    "text": "with the science that was\nbeing developed on both sides. And people had the\nidea of, gee, maybe",
    "start": "3410260",
    "end": "3417040"
  },
  {
    "text": "we could think of\ntranslation between languages as code breaking.",
    "start": "3417040",
    "end": "3423140"
  },
  {
    "text": "And that thought occurred to\nimportant, relevant people",
    "start": "3423140",
    "end": "3429010"
  },
  {
    "text": "in science funding agencies. And actually, lots\nand lots of funding was poured into\nthis idea of, can we",
    "start": "3429010",
    "end": "3436300"
  },
  {
    "text": "use computers to do machine\ntranslation between languages? And at the time in the\n'50s after some initial,",
    "start": "3436300",
    "end": "3446650"
  },
  {
    "text": "very impressive-looking\ncooked demos, it was basically\na complete flop.",
    "start": "3446650",
    "end": "3452480"
  },
  {
    "text": "And the reason-- there\nare lots of reasons why it was a complete flop. One was people knew almost\nnothing about the structure",
    "start": "3452480",
    "end": "3460090"
  },
  {
    "text": "of human languages. In particular, when\nI was mentioning the other day, the\nChomsky hierarchy",
    "start": "3460090",
    "end": "3466744"
  },
  {
    "text": "and knowing about\ncontext-free languages, the Chomsky hierarchy\nhadn't been invented yet.",
    "start": "3466745",
    "end": "3473339"
  },
  {
    "text": "The formal properties\nof languages hadn't really been explored. But also, the computers that\npeople had in the 1950s,",
    "start": "3473340",
    "end": "3483599"
  },
  {
    "text": "the amount of computing\npower, or memory, or anything like this that those\ncomputers had in those days was",
    "start": "3483600",
    "end": "3491580"
  },
  {
    "text": "laughable. These days, the little\npower brick for your laptop",
    "start": "3491580",
    "end": "3497610"
  },
  {
    "text": "has more computing\npower inside it than the big mainframe\ncomputers that they used to be using in those days.",
    "start": "3497610",
    "end": "3504580"
  },
  {
    "text": "So basically, people\nwere only able to build very simple lexicons and\nrule-based substitution rules,",
    "start": "3504580",
    "end": "3515400"
  },
  {
    "text": "and nothing like the\ncomplexity of human languages, which only gradually\npeople began to understand.",
    "start": "3515400",
    "end": "3522180"
  },
  {
    "text": "But machine translation\nstarted to become more alive in the 1990s and 2000s decades\nonce people started to build",
    "start": "3522180",
    "end": "3532300"
  },
  {
    "text": "empirical models\nover lots of data. And the approach, then, was\ncalled statistical machine",
    "start": "3532300",
    "end": "3538930"
  },
  {
    "text": "translation. And so when Google Translate was\nfirst introduced to the world,",
    "start": "3538930",
    "end": "3545539"
  },
  {
    "text": "it was the big\nunveiling to the world of statistical phrase-based\nmachine translation systems,",
    "start": "3545540",
    "end": "3552620"
  },
  {
    "text": "where what you were\ndoing was you're collecting a large amount\nof parallel data, words",
    "start": "3552620",
    "end": "3559390"
  },
  {
    "text": "that are being translated\nfrom one word to another. And not for all languages but\nfor quite a few languages,",
    "start": "3559390",
    "end": "3566180"
  },
  {
    "text": "there are quite a few\nsources of parallel data. So the European Union\ngenerates a huge amount",
    "start": "3566180",
    "end": "3571930"
  },
  {
    "text": "of parallel data among\nEuropean languages. There are places\nlike Hong Kong where",
    "start": "3571930",
    "end": "3577030"
  },
  {
    "text": "you get English, Chinese,\nif a certain dialect of Chinese, parallel data.",
    "start": "3577030",
    "end": "3583300"
  },
  {
    "text": "The UN generates a\nlot of parallel data, so getting sources of parallel\ndata and trying to build models.",
    "start": "3583300",
    "end": "3590990"
  },
  {
    "text": "And so the way it was done\nwas based on that model, we're going to try and\nlearn a probability",
    "start": "3590990",
    "end": "3596829"
  },
  {
    "text": "model for translation, so the\nprobability of a translation",
    "start": "3596830",
    "end": "3603610"
  },
  {
    "text": "given a source sentence. And the way it was\ndone at that time was breaking it down\nusing Bayes' rule",
    "start": "3603610",
    "end": "3610000"
  },
  {
    "text": "into two subproblems. So the probability of the\ntranslation, given the source,",
    "start": "3610000",
    "end": "3617359"
  },
  {
    "text": "is going to be the inverted\nprobability of the source given the translation times the\nprobability of the translation.",
    "start": "3617360",
    "end": "3625790"
  },
  {
    "text": "And you could think that\nthis makes it no simpler because you've just reversed\nthe order of x and y.",
    "start": "3625790",
    "end": "3635540"
  },
  {
    "text": "But the reason why\nit made it simpler and people were able\nto make progress was the translation\nmodel was treated",
    "start": "3635540",
    "end": "3642820"
  },
  {
    "text": "as a very simple model as to how\nwords tended to get translated",
    "start": "3642820",
    "end": "3648040"
  },
  {
    "text": "to words in the other language. And it didn't need\nto know anything about word order, grammar,\nstructure of the other language.",
    "start": "3648040",
    "end": "3656690"
  },
  {
    "text": "And then all of that was being\nhandled by just this probability of y, which was a pure\nlanguage model, as we've",
    "start": "3656690",
    "end": "3663849"
  },
  {
    "text": "talked about before. So you could have a simple\ntranslation model which just said, if you see the\nword, [FRENCH] in French,",
    "start": "3663850",
    "end": "3673430"
  },
  {
    "text": "you might want to translate\nit as, man, or person, or put some\nprobabilities on that.",
    "start": "3673430",
    "end": "3679250"
  },
  {
    "text": "And then most of the cleverness\nwas in the language model, which was telling you, what would be\na good sentence in the target",
    "start": "3679250",
    "end": "3687430"
  },
  {
    "text": "language? OK, and so that was important\nbecause translations",
    "start": "3687430",
    "end": "3695350"
  },
  {
    "text": "get pretty complicated. So you not only have to\nknow how to translate words,",
    "start": "3695350",
    "end": "3702860"
  },
  {
    "text": "and those translations\nof words vary in context, but you get a lot of reordering\nof words in sentences.",
    "start": "3702860",
    "end": "3711186"
  },
  {
    "text": " I'm not going to be able to\nspend a lot of time on this,",
    "start": "3711186",
    "end": "3716340"
  },
  {
    "text": "but here for a while was\nmy favorite example machine",
    "start": "3716340",
    "end": "3722930"
  },
  {
    "text": "translation sentence. So this is actually a\ntranslated sentence.",
    "start": "3722930",
    "end": "3729720"
  },
  {
    "text": "So the original comes from the\nbook, Guns, Germs, and Steel, if you're familiar\nwith that, but it",
    "start": "3729720",
    "end": "3736580"
  },
  {
    "text": "was that book by Jared Diamond. But this book was\ntranslated into Chinese.",
    "start": "3736580",
    "end": "3743340"
  },
  {
    "text": "So here's a sentence\nfrom the book in Chinese.",
    "start": "3743340",
    "end": "3748350"
  },
  {
    "text": "And I guess in the 2000s decade,\nI was involved in building",
    "start": "3748350",
    "end": "3753860"
  },
  {
    "text": "statistical machine\ntranslation systems, and I guess there was an MT\nevaluation that we did where",
    "start": "3753860",
    "end": "3762619"
  },
  {
    "text": "our system did terribly\non this sentence. And I tried it out\non Google Translate, and it also did terribly\nin this sentence.",
    "start": "3762620",
    "end": "3769620"
  },
  {
    "text": "So what the sentence\nshould say is, in 1519, 600 Spaniards landed in Mexico\nto conquer the Aztec empire",
    "start": "3769620",
    "end": "3778230"
  },
  {
    "text": "with a population\nof a few million. They lost 2/3 of their\nsoldiers in the initial clash.",
    "start": "3778230",
    "end": "3784620"
  },
  {
    "text": "So here's what Google\nTranslate said in 2009.",
    "start": "3784620",
    "end": "3790210"
  },
  {
    "text": "\"1519, 600 Spaniards landed\nin Mexico, millions of people,",
    "start": "3790210",
    "end": "3795990"
  },
  {
    "text": "to conquer the Aztec empire. The first 2/3 of soldiers\nagainst their loss.\"",
    "start": "3795990",
    "end": "3801330"
  },
  {
    "text": "Now, it's partly\nbad because the word choices and the translations\naren't very good.",
    "start": "3801330",
    "end": "3807390"
  },
  {
    "text": "But it's especially bad\nbecause it's just not",
    "start": "3807390",
    "end": "3812849"
  },
  {
    "text": "actually able to capture\nand use the modification relationships of the sentence.",
    "start": "3812850",
    "end": "3819849"
  },
  {
    "text": "So here's the part of the\nChinese that's saying, the Aztec empire, and\nover there in orange",
    "start": "3819850",
    "end": "3827400"
  },
  {
    "text": "is the few million people. And in Chinese, there's this\nexplicit little character here,",
    "start": "3827400",
    "end": "3834340"
  },
  {
    "text": "[CHINESE], which is saying,\nthat stuff in orange modifies this stuff\nin green, which",
    "start": "3834340",
    "end": "3839609"
  },
  {
    "text": "is what it's meant to be\nin the correct translation of Aztec empire with a\npopulation of a few million.",
    "start": "3839610",
    "end": "3846340"
  },
  {
    "text": "But Google Translate\ncompletely fails on that, and suddenly it's the\nmillions of people",
    "start": "3846340",
    "end": "3851940"
  },
  {
    "text": "who are going to be\nconquering the Aztec empire. And that is, in a way, the worst\nthing that's happening here.",
    "start": "3851940",
    "end": "3861280"
  },
  {
    "text": "Though, the 1519, 600, isn't\nexactly a very good translation, and the first 2/3 of soldiers\nagainst their loss isn't very",
    "start": "3861280",
    "end": "3869970"
  },
  {
    "text": "good either. So for a while, I used to update\nthis and see what happened.",
    "start": "3869970",
    "end": "3879136"
  },
  {
    "text": "In 2013, it almost seemed\nlike progress had been made,",
    "start": "3879136",
    "end": "3885010"
  },
  {
    "text": "but by 2015, it had\ngone downhill back to how it was before.",
    "start": "3885010",
    "end": "3890800"
  },
  {
    "text": "So it just seemed like\nthey got lucky in 2013 rather than the systems\nwere working any better.",
    "start": "3890800",
    "end": "3898710"
  },
  {
    "text": "And indeed, this seemed\nto be the problem. Although some kind\nof progress had",
    "start": "3898710",
    "end": "3904109"
  },
  {
    "text": "been made in\nmachine translation, these systems just never\nreally worked all that great.",
    "start": "3904110",
    "end": "3915270"
  },
  {
    "text": "And so that led to this\namazing breakthrough in 2014",
    "start": "3915270",
    "end": "3921300"
  },
  {
    "text": "where we then moved to\nneural machine translation, and neural machine\ntranslation was much better.",
    "start": "3921300",
    "end": "3929200"
  },
  {
    "text": "So what did we do in\nneural machine translation? So we built a neural\nmachine translation system",
    "start": "3929200",
    "end": "3936030"
  },
  {
    "text": "as a single end-to-end\nneural network. And that's been a powerful\nidea in neural network systems",
    "start": "3936030",
    "end": "3943380"
  },
  {
    "text": "in general, including in NLP. If we can just have\na single big system",
    "start": "3943380",
    "end": "3949110"
  },
  {
    "text": "and put a loss function\nat the end of it, and then we can backpropagate\nerrors right back down",
    "start": "3949110",
    "end": "3954900"
  },
  {
    "text": "through the system,\nit means we're aligning all of our learning for\nthe final task we want to do.",
    "start": "3954900",
    "end": "3961780"
  },
  {
    "text": "And that's been very effective,\nwhereas earlier models couldn't do that. So we built it with a\nsequence-to-sequence model.",
    "start": "3961780",
    "end": "3970210"
  },
  {
    "text": "So that sounds like\nour LSTMs, but it's meaning that we're going to\nhave two of them, one of them",
    "start": "3970210",
    "end": "3976829"
  },
  {
    "text": "to encode the sentence, the\nsource sentence, and one to produce the target sentence.",
    "start": "3976830",
    "end": "3983380"
  },
  {
    "text": "So that's what we're building. So for the source\nsentence, we're taking-- here it says RNN, but\nlet's just think LSTM",
    "start": "3983380",
    "end": "3990644"
  },
  {
    "text": "because that's what we're\ngoing to use in practice. It's much better. And so we're going\nto chunk through it,",
    "start": "3990645",
    "end": "3996550"
  },
  {
    "text": "encoding what we've\nread using an RNN.",
    "start": "3996550",
    "end": "4001740"
  },
  {
    "text": "So this RNN isn't going\nto output anything. We're just building\nup a hidden state that knows what's in\nthe source sentence.",
    "start": "4001740",
    "end": "4009110"
  },
  {
    "text": "So we get an encoding\nof the source sentence, and we're going to use that. Final hidden state to\ncondition the decoder RNN,",
    "start": "4009110",
    "end": "4018580"
  },
  {
    "text": "which is going to then\ngenerate the translation. So for the decoder\nRNN, it's also an LSTM,",
    "start": "4018580",
    "end": "4026290"
  },
  {
    "text": "but it's going to be an LSTM\nwith different parameters. So we're going to be learning\none LSTM with source encoding",
    "start": "4026290",
    "end": "4033030"
  },
  {
    "text": "parameters. And then for the\ndifferent language, we're learning a\ndifferent LSTM that will",
    "start": "4033030",
    "end": "4038130"
  },
  {
    "text": "know about the target language. And so we give it a start and\nsay, well, feed in your path--",
    "start": "4038130",
    "end": "4047890"
  },
  {
    "text": "what you've encoded\nfrom the encoder RNN as your starting point.",
    "start": "4047890",
    "end": "4053100"
  },
  {
    "text": "And then we're going to be-- that will be counted as\nthe previous hidden state",
    "start": "4053100",
    "end": "4058319"
  },
  {
    "text": "you're feeding into your LSTM. And then we're going to\ngenerate the first word",
    "start": "4058320",
    "end": "4063750"
  },
  {
    "text": "of the translation, and we'll\nthen copy that translated word down using this as a generative\nmodel, as I did last time.",
    "start": "4063750",
    "end": "4072640"
  },
  {
    "text": "And we start translating\nthrough, he hit me with a pie.",
    "start": "4072640",
    "end": "4078369"
  },
  {
    "text": "OK, so does that makes\nsense as a model?",
    "start": "4078370",
    "end": "4084530"
  },
  {
    "text": "Yeah, OK. So OK, there's\nsome notes, sorry.",
    "start": "4084530",
    "end": "4094055"
  },
  {
    "text": "Yeah, sorry, what I was\ngoing to say-- yeah, so the little pink note\nhere, so what I was showing",
    "start": "4094055",
    "end": "4099180"
  },
  {
    "text": "you is the picture of\nusing it at runtime.",
    "start": "4099180",
    "end": "4104979"
  },
  {
    "text": "At runtime, we're going\nto encode the source and then generate the\nwords of the translation.",
    "start": "4104979",
    "end": "4111000"
  },
  {
    "text": "At training time, we're\ngoing to have parallel text. We're going to have sentences\nand their translations.",
    "start": "4111000",
    "end": "4117789"
  },
  {
    "text": "We're going to run with\nthe same architecture. But as before, then,\nfor the decoder network,",
    "start": "4117790",
    "end": "4125920"
  },
  {
    "text": "we're going to try\nand predict each word and then say, what\nprobability did you assign to the actual next word?",
    "start": "4125920",
    "end": "4133059"
  },
  {
    "text": "And that will give us a loss. And we'll be calculating\nthe losses at each position, working out the average loss,\nworking out the gradients,",
    "start": "4133060",
    "end": "4140609"
  },
  {
    "text": "backpropagating them through the\nentire network, both the decoder RNN and the encoder\nRNN networks,",
    "start": "4140609",
    "end": "4148170"
  },
  {
    "text": "and updating all the\nparameters of our model. And that's the sense in which\nit's being trained end-to-end.",
    "start": "4148170",
    "end": "4155810"
  },
  {
    "text": "OK, so sequence-- so\nthis is a general notion",
    "start": "4155810",
    "end": "4161240"
  },
  {
    "text": "of an encoder-decoder model,\nwhich is a very general thing",
    "start": "4161240",
    "end": "4166278"
  },
  {
    "text": "that we use in all\nkinds of places, that we have one network that\nencodes something which produces",
    "start": "4166279",
    "end": "4173330"
  },
  {
    "text": "a representation, which will\nthen feed into another network that we'll use to\ndecode something.",
    "start": "4173330",
    "end": "4180119"
  },
  {
    "text": "And even when we go on to\ndo other things like use transformers rather\nthan LSTMs, we're",
    "start": "4180120",
    "end": "4186679"
  },
  {
    "text": "still commonly going\nto use these kind of encoder-decoder\nmodels, because if we want to do not only machine\ntranslation but other tasks",
    "start": "4186680",
    "end": "4195680"
  },
  {
    "text": "like summarization, or\ntext-to-speech, or other things",
    "start": "4195680",
    "end": "4204080"
  },
  {
    "text": "like that, we're going to\nbe in this space of using encoder-decoder networks. Yeah?",
    "start": "4204080",
    "end": "4209570"
  },
  {
    "text": "What is the difference between\nthis encoder-decoder model and just using a deeper neural\nnetwork with more layers?",
    "start": "4209570",
    "end": "4215156"
  },
  {
    "start": "4215156",
    "end": "4220219"
  },
  {
    "text": "Well, a lot. It's sequenced. So it has never been very--",
    "start": "4220220",
    "end": "4226220"
  },
  {
    "text": "you're meaning like,\nwhy don't you just build on top of the source, right? People have tried\nthat occasionally.",
    "start": "4226220",
    "end": "4233699"
  },
  {
    "text": "It's never been very successful. And I think part\nof the reason is all of what I was trying to show\nbefore about, all of the word",
    "start": "4233700",
    "end": "4241790"
  },
  {
    "text": "order changes around a\nlot between languages. And if you're just\ntrying to build stuff",
    "start": "4241790",
    "end": "4247880"
  },
  {
    "text": "on top of the source\nsentence, it's very hard to cope with that.",
    "start": "4247880",
    "end": "4253290"
  },
  {
    "text": "In particular, it's\nnot even the case that the length stays the same. One of the big ways in\nwhich languages vary",
    "start": "4253290",
    "end": "4261860"
  },
  {
    "text": "is what little words\nthat they have, so that in English\nyou're, putting in a lot of these auxiliary\nverbs and articles,",
    "start": "4261860",
    "end": "4268970"
  },
  {
    "text": "whereas if it's in Chinese,\nyou don't have any of those. And so you're either needing\nto, depending on direction,",
    "start": "4268970",
    "end": "4275520"
  },
  {
    "text": "add a lot of words or\nsubtract a lot of words, which is very hard to do\nif you're building on top of the source of it.",
    "start": "4275520",
    "end": "4284080"
  },
  {
    "text": "Is it quick? Yeah, so left side, is that\nsometimes bidirectional or just like the encoder?",
    "start": "4284080",
    "end": "4291655"
  },
  {
    "text": "Yeah, so you totally\nthink, and it could be that the\nencoder is bidirectional",
    "start": "4291655",
    "end": "4299740"
  },
  {
    "text": "and that might be better. For the famous\noriginal instantiation",
    "start": "4299740",
    "end": "4305140"
  },
  {
    "text": "of this that was done\nat Google, they actually didn't make it bidirectional. So it was simply taking\nthe final hidden state.",
    "start": "4305140",
    "end": "4311330"
  },
  {
    "text": "But that's absolutely an\nalternative that you could do. OK, yeah, so I'd it was\nusable for lots of things.",
    "start": "4311330",
    "end": "4325099"
  },
  {
    "text": "OK, yeah, so this is our\nconditional language model.",
    "start": "4325100",
    "end": "4331330"
  },
  {
    "text": "So we're now directly\ncalculating the probability of y given x, that the decoder\nmodel is generating a language",
    "start": "4331330",
    "end": "4342250"
  },
  {
    "text": "expression as a language model\ndirectly conditioned on x.",
    "start": "4342250",
    "end": "4347665"
  },
  {
    "text": "And so we train it with\na big parallel corpus. And that's the only case, I'm\ngoing to talk about today.",
    "start": "4347665",
    "end": "4354559"
  },
  {
    "text": "Recently, there's been\nsome interesting work on unsupervised\nmachine translation,",
    "start": "4354560",
    "end": "4359690"
  },
  {
    "text": "meaning that you got only\na little bit of information about how the languages relate.",
    "start": "4359690",
    "end": "4365170"
  },
  {
    "text": "You don't really have\na lot of parallel text. But I'm not going\nto cover that today. Yeah, so for training it,\nwe have paired sentences.",
    "start": "4365170",
    "end": "4376210"
  },
  {
    "text": "We work out our losses in the\npredictions at each position,",
    "start": "4376210",
    "end": "4381350"
  },
  {
    "text": "and then we're working\nout our average loss and backpropagating it through\nin a single system end-to-end",
    "start": "4381350",
    "end": "4387980"
  },
  {
    "text": "as described. Yeah, so in practice when people\nbuilt big machine translation",
    "start": "4387980",
    "end": "4396840"
  },
  {
    "text": "systems, this was one of the\nplaces where absolutely, it gave value to have\nmultilayer stacked LSTMs.",
    "start": "4396840",
    "end": "4406079"
  },
  {
    "text": "And so typically, people\nwere building a model, and you'll be building\na model, something",
    "start": "4406080",
    "end": "4411480"
  },
  {
    "text": "like this that's a\nmultilayer LSTM that's being used to encode and decode.",
    "start": "4411480",
    "end": "4418293"
  },
  {
    "text": " In my two minutes remaining,\nI just want to quickly say,",
    "start": "4418293",
    "end": "4427340"
  },
  {
    "text": "so building these neural machine\ntranslation systems was really",
    "start": "4427340",
    "end": "4432679"
  },
  {
    "text": "the first big success of\nnatural language processing,",
    "start": "4432680",
    "end": "4438060"
  },
  {
    "text": "deep learning. Now, in this sense,\nit depends on how you",
    "start": "4438060",
    "end": "4443450"
  },
  {
    "text": "define what parts of language. If you look at the\nhistory of the Renaissance",
    "start": "4443450",
    "end": "4449150"
  },
  {
    "text": "of deep learning,\nthe first place where deep learning\nwas highly successful",
    "start": "4449150",
    "end": "4454790"
  },
  {
    "text": "was in speech\nrecognition systems. The second place in which\nit was highly successful",
    "start": "4454790",
    "end": "4461180"
  },
  {
    "text": "was in object\nrecognition and vision. And then the third place\nthat was highly successful",
    "start": "4461180",
    "end": "4467600"
  },
  {
    "text": "was then building machine\ntranslation systems. So Google had a big statistical\nmachine translation system,",
    "start": "4467600",
    "end": "4477650"
  },
  {
    "text": "and it was only in\n2014 that people first",
    "start": "4477650",
    "end": "4484140"
  },
  {
    "text": "built this sort of LSTM deep\nlearning machine translation system.",
    "start": "4484140",
    "end": "4489610"
  },
  {
    "text": "But it was just\nobviously super good, and it was so super good\nthat in only two years,",
    "start": "4489610",
    "end": "4496080"
  },
  {
    "text": "it was then deployed\nas the live system that was being used at Google.",
    "start": "4496080",
    "end": "4501190"
  },
  {
    "text": "But it wasn't only\nused in Google, that neural machine translation\nwas just so much better",
    "start": "4501190",
    "end": "4508200"
  },
  {
    "text": "than what had come before\nthat by a couple of years after that, absolutely\neverybody, both US",
    "start": "4508200",
    "end": "4516780"
  },
  {
    "text": "companies and Chinese companies,\nMicrosoft, Facebook, Tencent, Baidu, everybody was using\nneural machine translation",
    "start": "4516780",
    "end": "4524700"
  },
  {
    "text": "systems because they are\njust much better systems. And so this was\nan amazing success",
    "start": "4524700",
    "end": "4530940"
  },
  {
    "text": "because statistical\nmachine translation systems like the Google\nsystem, that this",
    "start": "4530940",
    "end": "4536370"
  },
  {
    "text": "is something that had been\nworked on for about a decade. Hundreds of people\nhad worked on it.",
    "start": "4536370",
    "end": "4542470"
  },
  {
    "text": "There were millions\nof lines of code, lots of hacks built in\nfor particular languages",
    "start": "4542470",
    "end": "4548190"
  },
  {
    "text": "and language pairs. But really a simple, small\nneural machine translation",
    "start": "4548190",
    "end": "4555840"
  },
  {
    "text": "system was able to\nwork much better. There was an article\npublished about it",
    "start": "4555840",
    "end": "4561360"
  },
  {
    "text": "when it went live in\nthe New York Times that you can find in that link. It's a little bit of a\npraising piece where you",
    "start": "4561360",
    "end": "4570840"
  },
  {
    "text": "could be a little bit critical. But basically, it's talking\nabout how just the difference",
    "start": "4570840",
    "end": "4577860"
  },
  {
    "text": "in quality was so obvious that\neveryone immediately noticed, even before Google\nhad announced it of,",
    "start": "4577860",
    "end": "4584130"
  },
  {
    "text": "whoa, suddenly,\nmachine translation has gone so much better. OK, so that's basically today.",
    "start": "4584130",
    "end": "4591190"
  },
  {
    "text": "So for today, we've learnt\nthat LSTMs are powerful.",
    "start": "4591190",
    "end": "4596543"
  },
  {
    "text": "If you're doing something with\na recurrent neural network, you probably want\nto use an LSTM. You should know about the idea\nof clipping your gradients.",
    "start": "4596543",
    "end": "4604949"
  },
  {
    "text": "Bidirectional LSTMs are good\nwhen you've got an encoder but you can't use them\nto generate new text.",
    "start": "4604950",
    "end": "4613380"
  },
  {
    "text": "And encoder-decoder neural\nmachine translation systems were a great new technology\nthat advanced the field.",
    "start": "4613380",
    "end": "4621400"
  },
  {
    "text": "Thank you. ",
    "start": "4621400",
    "end": "4629000"
  }
]