[
  {
    "start": "0",
    "end": "5570"
  },
  {
    "text": "Hi, everyone. I hope everyone\nhad a great weekend and enjoyed the rain yesterday.",
    "start": "5570",
    "end": "14000"
  },
  {
    "text": "A couple of reminders,\nhomework 3 is due on Wednesday. The optional homework 4\nwill come out on Wednesday.",
    "start": "14000",
    "end": "20090"
  },
  {
    "text": "Also, for people who are\nremote and on Zoom, so far, I think everyone has been\nasking questions to chat,",
    "start": "20090",
    "end": "26570"
  },
  {
    "text": "but I wanted to have a quick\nreminder that if you want to-- instead of asking a\nquestion via chat,",
    "start": "26570",
    "end": "32810"
  },
  {
    "text": "you could also raise\nyour hand on Zoom, and when I call\non you, we should be able to hear you\nverbally, if you",
    "start": "32810",
    "end": "38329"
  },
  {
    "text": "want to ask verbal questions.  Cool. So today we're going to\nbe talking about meta",
    "start": "38330",
    "end": "45050"
  },
  {
    "text": "reinforcement learning. And we're going to be\ntalking about the problem statement for meta\nreinforcement learning.",
    "start": "45050",
    "end": "50520"
  },
  {
    "text": "And then we're\ngoing to be talking about two classes of\nmethods, Black box meta RL methods and optimization-based\nmeta RL methods.",
    "start": "50520",
    "end": "58370"
  },
  {
    "text": "Black box meta RL methods are\ngoing to come up in homework 4. And then on\nWednesday, we're also",
    "start": "58370",
    "end": "65690"
  },
  {
    "text": "going to talk about meta RL. But we're going to talk about\na specific part of the meta RL, problem statement, which\nis learning how to explore.",
    "start": "65690",
    "end": "73680"
  },
  {
    "text": "And this will really be\nthe focus of homework 4. But these methods will use some\nof the basics and black box",
    "start": "73680",
    "end": "79250"
  },
  {
    "text": "meta RL. And then the goals\nof the lecture will be to really try to\nunderstand the problem",
    "start": "79250",
    "end": "85370"
  },
  {
    "text": "statement of meta RL,\nhow it differs from kind of standard meta learning,\nas well as understand the basics of black box and\noptimization-based meta RL",
    "start": "85370",
    "end": "93555"
  },
  {
    "text": "methods.  So as a little bit\nof recap, previously",
    "start": "93555",
    "end": "101479"
  },
  {
    "text": "we talked about reinforcement\nlearning algorithms as kind of a primer on a lot\nof what we'll talk about today.",
    "start": "101480",
    "end": "111450"
  },
  {
    "text": "We also talked\nabout multitask RL. And we introduced both\nmodel-free methods as well as model-based methods.",
    "start": "111450",
    "end": "117570"
  },
  {
    "text": "And we kind of saw this\nkind of overall anatomy of a reinforcement\nlearning algorithm where you collect some data,\nyou estimate return in some way.",
    "start": "117570",
    "end": "127280"
  },
  {
    "text": "And then you use that to\nimprove your behavior. And of course, one of the things\nthat's pretty different about--",
    "start": "127280",
    "end": "133520"
  },
  {
    "text": "I guess there's two things\nthat are different about RL compared to supervised learning. One is that you\ncollect your own data. And the second is that, instead\nof getting direct labels,",
    "start": "133520",
    "end": "140780"
  },
  {
    "text": "you're given reward feedback. And this changes how the\nlearning algorithm operates.",
    "start": "140780",
    "end": "146323"
  },
  {
    "text": "And so we saw a\nfew different ways for estimating the return,\ndirectly through Monte Carlo,",
    "start": "146323",
    "end": "152510"
  },
  {
    "text": "or fitting a value\nfunction to estimate reward as well as just estimating the\ndynamics of the environment.",
    "start": "152510",
    "end": "160049"
  },
  {
    "text": "And then we also saw ways to use\nthese estimates of the return to improve your behavior\nthrough policy gradient,",
    "start": "160050",
    "end": "165830"
  },
  {
    "text": "through of taking the arg\nmax of your value function, and also through\nplanning with your model.",
    "start": "165830",
    "end": "171740"
  },
  {
    "text": " Great, and then before we talked\nabout reinforcement learning,",
    "start": "171740",
    "end": "177780"
  },
  {
    "text": "we also talked about these\ndifferent problem settings, including multi-task\nlearning, transfer learning, and meta learning.",
    "start": "177780",
    "end": "185060"
  },
  {
    "text": "In all of these\ndifferent scenarios, the tasks have to share\nsome amount of structure. And what we'll really\nbe looking at today",
    "start": "185060",
    "end": "191090"
  },
  {
    "text": "is how we can look at\nthe meta learning problem in a reinforcement learning\nscenario, where our goal is",
    "start": "191090",
    "end": "197240"
  },
  {
    "text": "to quickly learn a new\nreinforcement learning task, given experience from previous\nreinforcement learning tasks.",
    "start": "197240",
    "end": "204370"
  },
  {
    "text": "And one thing that's\ngoing to be different is that instead of just\ngiven data from these tasks, we're actually going\nto have to collect",
    "start": "204370",
    "end": "210350"
  },
  {
    "text": "the data from each of the\ntraining tasks ourselves.  And then we saw that a\nreinforcement learning",
    "start": "210350",
    "end": "217040"
  },
  {
    "text": "task corresponds to a\nMarkov decision process, with a state space, an action\nspace, essentially the domain",
    "start": "217040",
    "end": "225110"
  },
  {
    "text": "of the problem, the\ninitial state distribution, the dynamics of how the\nenvironment evolves over time,",
    "start": "225110",
    "end": "230870"
  },
  {
    "text": "as well as the reward function. And we saw that generally\nany of these things can vary across tasks, although\nin general, the dynamics",
    "start": "230870",
    "end": "238563"
  },
  {
    "text": "and the reward are\noften the things that are changing across tasks.",
    "start": "238563",
    "end": "244099"
  },
  {
    "text": "And so meta\nreinforcement learning is essentially going to\ncorrespond to meta learning but with reinforcement\nlearning tasks.",
    "start": "244100",
    "end": "250160"
  },
  {
    "text": "And where essentially we need to\ncollect data from our training tasks in a way that allows\nus to solve a new task",
    "start": "250160",
    "end": "256730"
  },
  {
    "text": "through exploration and\nadaptation in that new MDP. ",
    "start": "256730",
    "end": "265070"
  },
  {
    "text": "So concretely what\nthis looks like is-- before we talked about\nthe meta learning problem, we saw supervised learning.",
    "start": "265070",
    "end": "270680"
  },
  {
    "text": "And then we saw how meta\nlearning is essentially trying to kind of take as\ninput a small amount of data",
    "start": "270680",
    "end": "276440"
  },
  {
    "text": "for a new supervised learning\nproblem and a new example, and leverage the data in\norder to effectively predict",
    "start": "276440",
    "end": "283699"
  },
  {
    "text": "the label for the test example. So this was meta\nsupervised learning.",
    "start": "283700",
    "end": "288830"
  },
  {
    "text": "We did this with a\ndata set of data sets across all of our tasks,\nwhere each data set",
    "start": "288830",
    "end": "294680"
  },
  {
    "text": "has input-output pairs. And this view is useful\nbecause it essentially reduces the problem of meta\nlearning to learning something",
    "start": "294680",
    "end": "302030"
  },
  {
    "text": "that takes as input a data\nset and makes predictions about new examples. ",
    "start": "302030",
    "end": "308160"
  },
  {
    "text": "Now in meta\nreinforcement learning, we're no longer going\nto be trying to solve supervised learning tasks.",
    "start": "308160",
    "end": "315450"
  },
  {
    "text": "Instead, we want\nto learn a policy. So our input is going\nto be a state St.",
    "start": "315450",
    "end": "322590"
  },
  {
    "text": "Our policy is going to\noutput, which action should be taken at that state.",
    "start": "322590",
    "end": "327790"
  },
  {
    "text": "And so generally we'll be\nlearning this policy that maps from states to actions. ",
    "start": "327790",
    "end": "334970"
  },
  {
    "text": "And in reinforcement learning,\ninstead of having input-output pairs you're instead given--",
    "start": "334970",
    "end": "340580"
  },
  {
    "text": "or you instead have\nexperience in that MDP that has states, actions,\nrewards, and next states.",
    "start": "340580",
    "end": "349240"
  },
  {
    "text": "And so we're not told\ndirectly what to do. We're just given rewards\nfor behaviors that are good. And we try to maximize the\nrewards for those behaviors.",
    "start": "349240",
    "end": "356990"
  },
  {
    "text": " So this would mostly\nbe review up until now.",
    "start": "356990",
    "end": "365449"
  },
  {
    "text": "And in meta\nreinforcement learning, we want to be able\nto quickly solve a reinforcement learning task.",
    "start": "365450",
    "end": "371390"
  },
  {
    "text": "And so it's going to be\nquite analogous to the meta supervised learning scenario.",
    "start": "371390",
    "end": "377960"
  },
  {
    "text": "But our training data is not\ngoing to be input-output pairs. It's now going to be experience\ncollected in a reinforcement",
    "start": "377960",
    "end": "386600"
  },
  {
    "text": "learning task. And you can think\nof it as k episodes",
    "start": "386600",
    "end": "392150"
  },
  {
    "text": "from running some policy\npi, or k rollouts from pi.",
    "start": "392150",
    "end": "399530"
  },
  {
    "text": "And we want to be able to use\nk rollouts from some policy in order to maximize\nreward in that MDP.",
    "start": "399530",
    "end": "406789"
  },
  {
    "text": "And k might be very small. Typically in\nreinforcement learning, you may require a\nlot of experience.",
    "start": "406790",
    "end": "412430"
  },
  {
    "text": "We want to be able to solve\na new task with a very small amount of experience. And so given this experience,\nwe then want to be able to map",
    "start": "412430",
    "end": "419090"
  },
  {
    "text": "from states to actions and\nbe able to quickly learn",
    "start": "419090",
    "end": "425030"
  },
  {
    "text": "how to solve a new MDP\nfrom these k rollouts. ",
    "start": "425030",
    "end": "431500"
  },
  {
    "text": "Now the data for\nthis, we'll again have a data set of data sets. That's collected for each task.",
    "start": "431500",
    "end": "438910"
  },
  {
    "text": "But one thing that\nwill be different is that we won't necessarily\njust be given these data sets. The data from each\nof these tasks",
    "start": "438910",
    "end": "444700"
  },
  {
    "text": "actually has to be collected\nourselves through the meta reinforcement learning process.",
    "start": "444700",
    "end": "449935"
  },
  {
    "start": "449935",
    "end": "455440"
  },
  {
    "text": "So now the key parts\nof this problem is that we have to figure out\nhow to design and optimize f.",
    "start": "455440",
    "end": "461960"
  },
  {
    "text": "We saw this before. And actually we'll talk\na lot about this today and how this differs a little\nbit in the reinforcement",
    "start": "461960",
    "end": "468099"
  },
  {
    "text": "learning setting. Then we also need to learn how\nto collect appropriate data.",
    "start": "468100",
    "end": "473509"
  },
  {
    "text": "So we need to collect\nall of the Di's. We also need to learn\nwhat policy pi will",
    "start": "473510",
    "end": "479560"
  },
  {
    "text": "help us collect informative\ndata for solving the MDP. ",
    "start": "479560",
    "end": "485710"
  },
  {
    "text": "And so essentially we need\nto learn how to explore. And this is usually\npretty important",
    "start": "485710",
    "end": "492363"
  },
  {
    "text": "because if you just run a\nrandom policy for example, that might not give you\ninformative information about the task.",
    "start": "492363",
    "end": "497510"
  },
  {
    "text": "And if you have a good\nexploration policy, then this should allow you to\nvery quickly infer the task.",
    "start": "497510",
    "end": "503530"
  },
  {
    "text": "So you both need a good\nexploration strategy here as well as a good adaptation\nstrategy in f data.",
    "start": "503530",
    "end": "509050"
  },
  {
    "text": " Cool. And the other thing\nthat I'll note here",
    "start": "509050",
    "end": "514669"
  },
  {
    "text": "is that your exploration\npolicy pi doesn't necessarily need to be the same as the\npolicy that actually adapts to the task, which\nis represented by f.",
    "start": "514669",
    "end": "521281"
  },
  {
    "text": " So we can look at an example.",
    "start": "521281",
    "end": "527860"
  },
  {
    "text": "So say we want to learn how to\nquickly navigate in a new maze. So we collected some\nexperience in a maze.",
    "start": "527860",
    "end": "536460"
  },
  {
    "text": "So maybe we just\nhad one episode. So k equals 1. And we collected some\nexperience that's",
    "start": "536460",
    "end": "541680"
  },
  {
    "text": "shown in this trajectory\nthat goes from red eventually to blue. And the goal is to\nget to the blue box.",
    "start": "541680",
    "end": "548280"
  },
  {
    "text": "And you don't necessarily see-- it only sees where it kind\nof locally is in the maze.",
    "start": "548280",
    "end": "553440"
  },
  {
    "text": "It doesn't actually see the\nwhole picture of the maze. And so given this training data,\nthe single episode in the MDP,",
    "start": "553440",
    "end": "560819"
  },
  {
    "text": "we want to be able to ultimately\nget a policy that can then directly go into the\ngoal and directly solve the MDP very quickly.",
    "start": "560820",
    "end": "567480"
  },
  {
    "text": " So this is what we'd\nlike to be able to do. So this would correspond\nto meta test time.",
    "start": "567480",
    "end": "574320"
  },
  {
    "text": "And then what we want to\ndo during meta training is to prepare ourselves for\nsolving this meta test task.",
    "start": "574320",
    "end": "580500"
  },
  {
    "text": "And so this is what we\nwant to be able to do, and what we can do is we\ncan, at meta train time,",
    "start": "580500",
    "end": "586350"
  },
  {
    "text": "practice solving lots of\ndifferent kinds of mazes. So we can collect data\nin different tasks",
    "start": "586350",
    "end": "593610"
  },
  {
    "text": "where different mazes\ncorrespond to different tasks, and learn how to solve\neach of these mazes",
    "start": "593610",
    "end": "598980"
  },
  {
    "text": "so that when we're given\na new maze at test time, we can very quickly\nlearn how to navigate in that maze with maybe\njust a single episode.",
    "start": "598980",
    "end": "608900"
  },
  {
    "text": "And of course here are the\nexamples of different tasks correspond to different mazes. But the different tasks could\nbe something different as well.",
    "start": "608900",
    "end": "616010"
  },
  {
    "text": " [INAUDIBLE] ",
    "start": "616010",
    "end": "621025"
  },
  {
    "text": "Yeah, so the\nquestion is, can you have multiple\nexploration policies when you're collecting data? When you collect your\ntraining data in MDP,",
    "start": "621025",
    "end": "629300"
  },
  {
    "text": "it could really be\nwhatever you want. I guess I listed\na single policy.",
    "start": "629300",
    "end": "634760"
  },
  {
    "text": "That could also be a mixture\nof multiple different policies, such that you collect\ndata according",
    "start": "634760",
    "end": "640190"
  },
  {
    "text": "to that mixture of policies. ",
    "start": "640190",
    "end": "646630"
  },
  {
    "text": "The other thing is that the\nexploration policy can also be recurrent. And so it can incorporate\nits experience in the maze",
    "start": "646630",
    "end": "653380"
  },
  {
    "text": "so far when determining\nhow to continue to explore in that maze. ",
    "start": "653380",
    "end": "661950"
  },
  {
    "text": "And then one more note\nabout the meta RL problem is that this is the formulation\nthat I discussed before.",
    "start": "661950",
    "end": "668760"
  },
  {
    "text": "I'll refer to this as\nthe episodic variant of meta RL, where you're\ngiven k episodes from pi.",
    "start": "668760",
    "end": "675210"
  },
  {
    "text": "But the training data could\nalso be corresponding to k time",
    "start": "675210",
    "end": "681060"
  },
  {
    "text": "steps from your policy. You could actually\nlearn in a more kind of online\nfashion, when you're",
    "start": "681060",
    "end": "687570"
  },
  {
    "text": "given even less data from pi. Rather than k episodes,\nyou have k time steps.",
    "start": "687570",
    "end": "693725"
  },
  {
    "text": "And like I mentioned before,\nthe exploration policy and the adaptation\npolicy don't necessarily need to be the same. ",
    "start": "693725",
    "end": "703960"
  },
  {
    "text": "Cool, any questions on\nthe problem statement? ",
    "start": "703960",
    "end": "715940"
  },
  {
    "text": "OK, so essentially\nwhat we did is we just took the meta learning problem. And replaced the\ntasks with MDPs.",
    "start": "715940",
    "end": "722810"
  },
  {
    "text": "So how do we actually go about\nsolving this problem statement? We're going to look at two\ndifferent kinds of approaches,",
    "start": "722810",
    "end": "728150"
  },
  {
    "text": "black box approaches and\noptimization-based approaches. We're actually not\ngoing to talk at all",
    "start": "728150",
    "end": "734240"
  },
  {
    "text": "about non-parametric methods. And I'm curious if\nanyone has thoughts on why that's the\ncase, or why it",
    "start": "734240",
    "end": "741800"
  },
  {
    "text": "would be difficult to apply\nnon-parametric meta learning methods to meta RL. ",
    "start": "741800",
    "end": "749959"
  },
  {
    "text": "Yeah-- [INAUDIBLE] ",
    "start": "749960",
    "end": "756589"
  },
  {
    "text": "Yeah, exactly. So with non-parametric\nmethods, they're",
    "start": "756590",
    "end": "761660"
  },
  {
    "text": "really designed for\nclassification problems where you are trying to classify\namong a few different classes.",
    "start": "761660",
    "end": "767300"
  },
  {
    "text": "In general, when you're\ntrying to predict actions, sometimes these actions might\nbe continuous, in which case,",
    "start": "767300",
    "end": "772980"
  },
  {
    "text": "it's more of a\nregression problem and you just can't\nsolve it at all with non-parametric methods. And even in cases where you do\nhave a discrete action space,",
    "start": "772980",
    "end": "780290"
  },
  {
    "text": "it isn't really analogous\nto this concept of class. It's more analogous to which of\nthese actions should you take.",
    "start": "780290",
    "end": "787280"
  },
  {
    "text": "It seems possible that you could\napply non-parametric methods to meta RL when you\nhave discrete actions.",
    "start": "787280",
    "end": "793070"
  },
  {
    "text": "But thus far they\nhaven't been used at all because you don't really\nhave this notion of class. And it's not really a nearest\nneighbors problem anymore.",
    "start": "793070",
    "end": "800250"
  },
  {
    "text": "It's more a problem of using\nyour experience to figure out how to solve new environments. And you can't really\napply this sort",
    "start": "800250",
    "end": "806780"
  },
  {
    "text": "of idea of matching\nor nearest neighbors to solve these\nkinds of problems. ",
    "start": "806780",
    "end": "814130"
  },
  {
    "text": "OK, so we'll start\nwith black box methods. And in black box\nmeta learning before,",
    "start": "814130",
    "end": "823160"
  },
  {
    "text": "we saw that we can essentially\ntake this form where we have a function that\ntakes as input our data,",
    "start": "823160",
    "end": "828470"
  },
  {
    "text": "as well as our new test\ninput, and predicts the corresponding label. And in general, the trend\nwill almost be the same in RL",
    "start": "828470",
    "end": "836540"
  },
  {
    "text": "where we take as input a data\nset as well as a new state that we're observing. And we're going to predict\nthe corresponding action",
    "start": "836540",
    "end": "843500"
  },
  {
    "text": "for that state.  So for example,\nour black box model",
    "start": "843500",
    "end": "848780"
  },
  {
    "text": "could be a recurrent\nneural network that takes as input some of\nour training data seen so far,",
    "start": "848780",
    "end": "858350"
  },
  {
    "text": "as well as our\nquery data point t, and then predicts the\ncorresponding action.",
    "start": "858350",
    "end": "864860"
  },
  {
    "text": "Now one thing that\nyou can note here is that the training\ndata is actually going to be getting larger over time.",
    "start": "864860",
    "end": "870574"
  },
  {
    "text": "It's essentially going--\nat every single time step, we're just adding to\nour training data.",
    "start": "870575",
    "end": "875850"
  },
  {
    "text": "And so it's going to be\nincrementally growing, and essentially growing\non a rolling basis, where",
    "start": "875850",
    "end": "881660"
  },
  {
    "text": "kind of your query set\nat the current time step will then actually become\npart of the training data set at the next time step.",
    "start": "881660",
    "end": "887390"
  },
  {
    "text": " Yeah, so you can view it as\nessentially a recurrent policy.",
    "start": "887390",
    "end": "895160"
  },
  {
    "text": "It takes as input the\nstates and the rewards so that it can infer\nwhat the task is from that experience in\norder to figure out what",
    "start": "895160",
    "end": "902480"
  },
  {
    "text": "to do at the current state.  So now I have a couple\nof questions for you.",
    "start": "902480",
    "end": "910213"
  },
  {
    "text": "One thing you might\nnotice here is that we're passing in the\nstates and the rewards but we're not\npassing the actions.",
    "start": "910213",
    "end": "916100"
  },
  {
    "text": "Does anyone have\nany thoughts on why we don't need to\npass in the actions",
    "start": "916100",
    "end": "921110"
  },
  {
    "text": "in with the support set?  [INAUDIBLE]",
    "start": "921110",
    "end": "928533"
  },
  {
    "start": "928533",
    "end": "936330"
  },
  {
    "text": "So you're saying that the--  can you contribute any\nadditional information?",
    "start": "936330",
    "end": "942720"
  },
  {
    "text": "[INAUDIBLE]  So you're saying that given\nthe state that you transitioned",
    "start": "942720",
    "end": "949980"
  },
  {
    "text": "into, the past actions don't\nmatter because of the Markov property. Essentially, you\nknow what your--",
    "start": "949980",
    "end": "956390"
  },
  {
    "text": "you see the state\nthat you're in. And that tells you\nessentially about what",
    "start": "956390",
    "end": "962310"
  },
  {
    "text": "your previous action. That's sort of right. Any more to add? [INAUDIBLE]",
    "start": "962310",
    "end": "970020"
  },
  {
    "text": " Yeah, so the other thing is\nthat the policy is actually",
    "start": "970020",
    "end": "977670"
  },
  {
    "text": "predicting the actions itself. And so its hidden\nstate is actually going to contain information\nabout the previous action.",
    "start": "977670",
    "end": "982930"
  },
  {
    "text": "And so because the\nhidden state already contains all the\ninformation that's needed to predict that action,\nthen the action information",
    "start": "982930",
    "end": "989845"
  },
  {
    "text": "is actually already stored. And you don't need to\npass it in as input again. And one reason why that\ninformation is important",
    "start": "989845",
    "end": "996750"
  },
  {
    "text": "is that even despite\nthe Markov property, it is helpful to have\nthose previous actions because different tasks vary\nbased on different dynamics.",
    "start": "996750",
    "end": "1004790"
  },
  {
    "text": "If you actually want to\ninformation about SAS prime about what the dynamics are.",
    "start": "1004790",
    "end": "1010935"
  },
  {
    "text": "And so it is actually\nimportant to have that to know that action\ninformation because it allows you to identify\ndifferences in the dynamics.",
    "start": "1010935",
    "end": "1018080"
  },
  {
    "text": "But the hidden state\nalready captures it because the hidden\nstate is what was used to generate that action.",
    "start": "1018080",
    "end": "1023630"
  },
  {
    "start": "1023630",
    "end": "1029420"
  },
  {
    "text": "And then the other\nquestion is, how is this different\nfrom just doing like standard\nreinforcement learning",
    "start": "1029420",
    "end": "1035810"
  },
  {
    "text": "with a recurrent policy? So it is quite similar\nto just doing standard RL with a recurrent policy.",
    "start": "1035810",
    "end": "1041329"
  },
  {
    "text": "But there are a\ncouple of differences. And I'm curious if anyone\ncan spot any of them. ",
    "start": "1041329",
    "end": "1053090"
  },
  {
    "text": "Yeah-- [INAUDIBLE] ",
    "start": "1053090",
    "end": "1062149"
  },
  {
    "text": "Yeah so one thing is\ntypically in standard RL, we don't pass in\nthe previous reward function at each time step. We just pass in the state.",
    "start": "1062150",
    "end": "1069330"
  },
  {
    "text": "And so this is one\nthing that's different. Good. ",
    "start": "1069330",
    "end": "1076200"
  },
  {
    "text": "Any other things\nthat are different? ",
    "start": "1076200",
    "end": "1089139"
  },
  {
    "text": "So I guess the\nfirst thing that I had here is that the\nreward is passed as input. We're also training this\nRNN across multiple MDPs",
    "start": "1089140",
    "end": "1095400"
  },
  {
    "text": "rather than within a single MDP. And therefore the\nrewards might actually-- this is one reason why\nyou might need to pass",
    "start": "1095400",
    "end": "1101565"
  },
  {
    "text": "on the rewards as input. Also I can note that\nif the rewards are the same across tasks and the\nonly thing that's different",
    "start": "1101565",
    "end": "1106980"
  },
  {
    "text": "is the dynamics, then\nyou don't actually need to pass in the reward. And the other differences?",
    "start": "1106980",
    "end": "1113070"
  },
  {
    "start": "1113070",
    "end": "1119519"
  },
  {
    "text": "So one other thing\nthat's pretty important is that if you want to learn\nacross episodes, if you want",
    "start": "1119520",
    "end": "1125590"
  },
  {
    "text": "the thing to take as\ninput, incorporate a training data set that isn't\njust like a single episode,",
    "start": "1125590",
    "end": "1131620"
  },
  {
    "text": "but is actually\nmultiple episodes, then it's really\nimportant to maintain your hidden state across\ndifferent episodes",
    "start": "1131620",
    "end": "1137620"
  },
  {
    "text": "in your task. And so one of the things that's\nreally different about doing",
    "start": "1137620",
    "end": "1144148"
  },
  {
    "text": "RL with the standard\nrecurrent policy is that typically\nyou're just going to be maintaining your hidden\nstate within an episode.",
    "start": "1144148",
    "end": "1149590"
  },
  {
    "text": "Whereas here, you're\nactually going to be not resetting your\nhidden state at the beginning",
    "start": "1149590",
    "end": "1155050"
  },
  {
    "text": "of each episode. You're only going\nto be resetting it at the beginning of each task. And it's going to\ntry to essentially",
    "start": "1155050",
    "end": "1161080"
  },
  {
    "text": "aggregate information\nabout the task across these different episodes\nso that it can essentially",
    "start": "1161080",
    "end": "1166510"
  },
  {
    "text": "gain as much information\nand knowledge about the current\ntask as possible. ",
    "start": "1166510",
    "end": "1172330"
  },
  {
    "text": "Yeah-- [INAUDIBLE] ",
    "start": "1172330",
    "end": "1178120"
  },
  {
    "text": "Yeah so this particular\nplot doesn't necessarily imply it unless the\ndot dot dot kind of includes additional episodes.",
    "start": "1178120",
    "end": "1185980"
  },
  {
    "text": "I have another diagram\nthat illustrates this more explicitly. So here say your episodes\nare t times f long,",
    "start": "1185980",
    "end": "1193960"
  },
  {
    "text": "then you are going\nto have an arrow that continues that hidden state\nbetween the boundary of t",
    "start": "1193960",
    "end": "1199090"
  },
  {
    "text": "to zero. And then once you get to\nthe end of the last episode, you'll then--",
    "start": "1199090",
    "end": "1204510"
  },
  {
    "text": "these will have two\nepisodes-- if you only have two episodes then you'll\nkind of cut off at that point, and then start the new\nepisode for the new task",
    "start": "1204510",
    "end": "1213040"
  },
  {
    "text": "at timestamp zero. Yeah-- What's the purpose\nof having episodes,",
    "start": "1213040",
    "end": "1218830"
  },
  {
    "text": "multiple episodes [INAUDIBLE]",
    "start": "1218830",
    "end": "1225029"
  },
  {
    "text": "Yeah so the question is,\nwhat's the point of having different episodes now\ngiven that we're now kind of treating different\ntasks as different episodes?",
    "start": "1225030",
    "end": "1231880"
  },
  {
    "text": "And one of the things\nthat's important here is that S1 is going\nto be kind of drawn again",
    "start": "1231880",
    "end": "1236890"
  },
  {
    "text": "from the initial\nstate distribution. And so when you're\nexploring in your MDP, it can be helpful to\nsample multiple times",
    "start": "1236890",
    "end": "1245410"
  },
  {
    "text": "from the initial\nstate distribution and like sample multiple\npossible trajectories. And so we're going\nto essentially",
    "start": "1245410",
    "end": "1251350"
  },
  {
    "text": "have this notion of\nepisodes and maybe even a notion of\nmeta episode or trial",
    "start": "1251350",
    "end": "1256420"
  },
  {
    "text": "that captures all of the\nepisodes within a single task. And sometimes the literature\nrefers to that as trial.",
    "start": "1256420",
    "end": "1263380"
  },
  {
    "text": "I've also seen that's sometimes\nreferred to as meta episode because it captures multiple\nepisodes within a task.",
    "start": "1263380",
    "end": "1270180"
  },
  {
    "text": "Yeah-- [INAUDIBLE] ",
    "start": "1270180",
    "end": "1286905"
  },
  {
    "text": "Yeah, so the question is, isn't\nthis very difficult to optimize because like essentially\nwe need to remember a lot.",
    "start": "1286905",
    "end": "1293310"
  },
  {
    "text": "And we need to remember\nlike all the way back from the first episode. And if you have a\nlot of episodes,",
    "start": "1293310",
    "end": "1299280"
  },
  {
    "text": "then you essentially-- learning things from\nthe earlier episodes might be very difficult.\nThat's a great question.",
    "start": "1299280",
    "end": "1306030"
  },
  {
    "text": "We'll talk about this\nin a slide or two. But there are a couple\nof architectures that aren't recurrent\nnetworks, that can potentially",
    "start": "1306030",
    "end": "1315090"
  },
  {
    "text": "aggregate information\nin a way that isn't like this sequential. And those kinds of\narchitectures can",
    "start": "1315090",
    "end": "1320790"
  },
  {
    "text": "help the optimization a lot. You can think of, for example,\nthings like transformers,",
    "start": "1320790",
    "end": "1326190"
  },
  {
    "text": "or things like-- things that\nkind of take the episode information and then average\nit into a single embedding. That's going to make it\neasier to back propagate",
    "start": "1326190",
    "end": "1333510"
  },
  {
    "text": "into the first\nepisode for example. There are pros and cons\nto each approaches.",
    "start": "1333510",
    "end": "1339992"
  },
  {
    "text": "But some of those architectures\nmake it easier to optimize. ",
    "start": "1339992",
    "end": "1348820"
  },
  {
    "text": "So as an algorithm, what this\nlooks like is we first sample a task from our--",
    "start": "1348820",
    "end": "1357090"
  },
  {
    "text": "the set of tasks that we have. Then we're going to roll\nout this recurrent policy",
    "start": "1357090",
    "end": "1362100"
  },
  {
    "text": "for up to N episodes,\nwhere N is basically the horizon of the\nlearning problem.",
    "start": "1362100",
    "end": "1370600"
  },
  {
    "text": "And as I mentioned\nbefore, this D train is going to kind of be updated\nincrementally with each time step.",
    "start": "1370600",
    "end": "1375810"
  },
  {
    "text": "Is not actually--\nis no longer fixed. It's actually growing online. ",
    "start": "1375810",
    "end": "1381960"
  },
  {
    "text": "Then this will be under the\ndynamics in the reward for task i.",
    "start": "1381960",
    "end": "1387900"
  },
  {
    "text": "And then we'll store\nthe data that we collected in some replay\nbuffer for that task.",
    "start": "1387900",
    "end": "1394649"
  },
  {
    "text": "And then we can\nupdate the policy by sampling data from\nthat replay buffer, to essentially just optimize\nthis recurrent policy so",
    "start": "1394650",
    "end": "1402870"
  },
  {
    "text": "that it maximizes reward\nover all of your tasks.",
    "start": "1402870",
    "end": "1408300"
  },
  {
    "text": "And this last step can\nbe done with a variety of different\nlearning algorithms. It can be done with\npolicy gradients.",
    "start": "1408300",
    "end": "1414480"
  },
  {
    "text": "It could be done with\nQ-learning-based methods",
    "start": "1414480",
    "end": "1419850"
  },
  {
    "text": "for example as well.  So I was trying to this\ngeneral in the sense",
    "start": "1419850",
    "end": "1425310"
  },
  {
    "text": "that it can be applied with\na few different approaches. And then, of course, you\nalso repeat this process. Once you update\nyour policy, you'll",
    "start": "1425310",
    "end": "1430638"
  },
  {
    "text": "then collect data for another\ntask, and so on and so forth.",
    "start": "1430638",
    "end": "1435690"
  },
  {
    "text": "Yeah-- Is it important for the\nreward functions between tasks",
    "start": "1435690",
    "end": "1441650"
  },
  {
    "text": "to be sort of in the same\nmagnitude for the values?",
    "start": "1441650",
    "end": "1447380"
  },
  {
    "text": "Yeah, great question. So the question\nis, is it important for the reward function\nto be in the same value",
    "start": "1447380",
    "end": "1452630"
  },
  {
    "text": "for different tasks? In general, this\nis quite important, because if they are on\nvery different magnitudes,",
    "start": "1452630",
    "end": "1459560"
  },
  {
    "text": "then when you do\nthis last step, it's going to care a lot\nmore about the task that has a higher magnitude\nreward than about tasks that",
    "start": "1459560",
    "end": "1466250"
  },
  {
    "text": "have lower magnitude rewards. And so if you want\nit to ultimately be able to adapt to all\nof the tasks and the task distribution, you\ngenerally want them",
    "start": "1466250",
    "end": "1472880"
  },
  {
    "text": "to have similar magnitudes. ",
    "start": "1472880",
    "end": "1479820"
  },
  {
    "text": "Cool.  So that's what happens\nat meta training time.",
    "start": "1479820",
    "end": "1485430"
  },
  {
    "text": "And then once you learn this\nkind of recurrent policy that can learn how to adapt to\na task, then at meta test time,",
    "start": "1485430",
    "end": "1493919"
  },
  {
    "text": "you're given a new task. And you simply just take\nthe policy that you learned",
    "start": "1493920",
    "end": "1499770"
  },
  {
    "text": "and roll out that\npolicy for kind of the horizon of\nthe learning problem. ",
    "start": "1499770",
    "end": "1508090"
  },
  {
    "text": "Generally, you don't\nwant to roll it out for more than N episodes\nbecause if you roll out",
    "start": "1508090",
    "end": "1513610"
  },
  {
    "text": "an RNN, for example, for\nlonger than the horizon that it was trained, then\nit may not perform well.",
    "start": "1513610",
    "end": "1519100"
  },
  {
    "text": "Although one thing\nthat you could do is something where you try to\nfreeze the behavior that you",
    "start": "1519100",
    "end": "1524620"
  },
  {
    "text": "get at N episodes and no longer\nupdate that policy, and just keep on running that policy.",
    "start": "1524620",
    "end": "1530200"
  },
  {
    "text": "But if you do really care\nabout learning for N episodes and then continuing to kind\nof execute that policy,",
    "start": "1530200",
    "end": "1535490"
  },
  {
    "text": "you can also optimize for that\nbehavior during meta training. ",
    "start": "1535490",
    "end": "1543690"
  },
  {
    "text": "And then to touch a little bit\non architectures and optimizers for this, the approach of\nusing black box meta RL",
    "start": "1543690",
    "end": "1551790"
  },
  {
    "text": "is quite general. But you can-- there's of\ncourse lots of different design choices you can choose. So one of the first\napproaches for this",
    "start": "1551790",
    "end": "1561240"
  },
  {
    "text": "is referred to as RL\nsquared because they're trying to do fast reinforcement\nlearning via slow reinforcement learning.",
    "start": "1561240",
    "end": "1566370"
  },
  {
    "text": "And they essentially\ndid exactly what we talked about on the\nprevious slide, where they had this recurrent neural\nnetwork that takes this input",
    "start": "1566370",
    "end": "1575070"
  },
  {
    "text": "states and rewards, and is able\nto predict the next action.",
    "start": "1575070",
    "end": "1581130"
  },
  {
    "text": "And in these two papers, they\nused policy gradient methods to TRPO and A3C.",
    "start": "1581130",
    "end": "1587115"
  },
  {
    "text": " Instead of using your\ncurrent neural network,",
    "start": "1587115",
    "end": "1592440"
  },
  {
    "text": "you can also use an\nattention-based architecture that has kind of connections\nfrom the current timestamp all the way back to\nthat first timestamp.",
    "start": "1592440",
    "end": "1599370"
  },
  {
    "text": "And these methods are going\nto be better at propagating",
    "start": "1599370",
    "end": "1604380"
  },
  {
    "text": "across very long sequences. This particular architecture was\nalternating between attention",
    "start": "1604380",
    "end": "1610440"
  },
  {
    "text": "and 1D convolutions. But you could also have a purely\nattention-based architecture as well.",
    "start": "1610440",
    "end": "1616299"
  },
  {
    "text": "And then they were again\noptimizing the policy with TRPO, which is a\npolicy gradient method. ",
    "start": "1616300",
    "end": "1623710"
  },
  {
    "text": "And then the last\narchitecture that I'll mention is something more analogous\nto an architecture",
    "start": "1623710",
    "end": "1631679"
  },
  {
    "text": "that we saw before,\nwhere we essentially encode all of our transitions\ninto some embedding.",
    "start": "1631680",
    "end": "1639330"
  },
  {
    "text": "And then we average\nthat embedding, and then pass that averaged embedding\ninto our Q function",
    "start": "1639330",
    "end": "1645630"
  },
  {
    "text": "and into our policy. And so this is kind of more of\nlike a feedforward architecture in an average.",
    "start": "1645630",
    "end": "1651600"
  },
  {
    "text": "And then the actual\noptimizer that was used was soft actor critic, which\nis a value-based reinforcement",
    "start": "1651600",
    "end": "1657090"
  },
  {
    "text": "learning method. And you can note that there's\na Q function and a policy here. And that's kind of\nindicative of the fact",
    "start": "1657090",
    "end": "1664318"
  },
  {
    "text": "that it's a value-based method.  Yeah?",
    "start": "1664318",
    "end": "1671175"
  },
  {
    "text": "[INAUDIBLE] ",
    "start": "1671175",
    "end": "1679409"
  },
  {
    "text": "Yeah, so the question\nis, how do you choose the reinforcement\nlearning optimization?",
    "start": "1679410",
    "end": "1684467"
  },
  {
    "text": "Like is the reason why\nmultiple of these use TRPO? ",
    "start": "1684467",
    "end": "1690510"
  },
  {
    "text": "I guess we'll talk about\nthis in a second too, but I guess things like policy\ngradients are quite simple--",
    "start": "1690510",
    "end": "1699630"
  },
  {
    "text": "quite simple to\nimplement, and are also pretty easy to get to work\nif you give it enough data.",
    "start": "1699630",
    "end": "1709990"
  },
  {
    "text": "And so I think that these were\nlike two of the first meta RL, black box meta RL\nmethods to come out,",
    "start": "1709990",
    "end": "1716050"
  },
  {
    "text": "and so usually going with simple\nthings that are kind of easier to get to work that\ndon't require quite as much tuning is\nhelpful for that.",
    "start": "1716050",
    "end": "1723216"
  },
  {
    "text": "That said, methods\nthat are on policy tend to be very\ndata inefficient.",
    "start": "1723216",
    "end": "1729100"
  },
  {
    "text": "And so these methods are\nalso extremely data hungry during the meta\ntraining process.",
    "start": "1729100",
    "end": "1734140"
  },
  {
    "text": "And so that's a\nreason not to use policy gradient-based\nmethods and a reason to favor value-based methods.",
    "start": "1734140",
    "end": "1740175"
  },
  {
    "text": " Yeah?",
    "start": "1740175",
    "end": "1746190"
  },
  {
    "text": "[INAUDIBLE] ",
    "start": "1746190",
    "end": "1758010"
  },
  {
    "text": "Yeah, so in practice, you\nwill sample a mini batch of tasks on task one. So the question is there\na reason why we're just",
    "start": "1758010",
    "end": "1763990"
  },
  {
    "text": "sampling one task here? In practice, you will\nactually sample a mini batch, rather than just one task. And then you'll do this update\non that mini batch of tasks.",
    "start": "1763990",
    "end": "1771882"
  },
  {
    "text": "[INAUDIBLE] ",
    "start": "1771882",
    "end": "1778710"
  },
  {
    "text": "So you won't roll it out for-- so if you have k tasks and\nN episodes in your batch,",
    "start": "1778710",
    "end": "1788520"
  },
  {
    "text": "then you're not going to\nroll it out for k times N because you need to\nbreak the RNN state",
    "start": "1788520",
    "end": "1795360"
  },
  {
    "text": "at the boundary of each task. And so you'll roll it--\nyou can essentially roll it out in parallel\nfor each of the k tasks.",
    "start": "1795360",
    "end": "1803370"
  },
  {
    "text": "And so you will eventually get-- the data that you'll get\nwill be k times N episodes.",
    "start": "1803370",
    "end": "1809773"
  },
  {
    "text": "But you do want to make sure\nthat you cut the hidden state at the task boundary so\nthat it's only aggregating information about that task and\nnot about the previous tasks.",
    "start": "1809773",
    "end": "1817320"
  },
  {
    "text": " Yeah, and in general,\nI wrote it as one task",
    "start": "1817320",
    "end": "1823890"
  },
  {
    "text": "to make the notation simple. But in practice, you will want\nto use a mini batch of tasks.",
    "start": "1823890",
    "end": "1829289"
  },
  {
    "text": "Also if this-- sometimes\nthis policy update, it can be with that mini batch. It can also just sample\ndirectly from the replay buffer.",
    "start": "1829290",
    "end": "1835193"
  },
  {
    "text": "And so it doesn't\nnecessarily need to include the data\nthat you just collected. ",
    "start": "1835193",
    "end": "1848240"
  },
  {
    "text": "So I guess one other\nnote about architectures is that this is actually just\ndoing feedforward and averaging",
    "start": "1848240",
    "end": "1854810"
  },
  {
    "text": "over individual transitions. You can also do an RNN over\ntransitions within an episode,",
    "start": "1854810",
    "end": "1861920"
  },
  {
    "text": "and then average\nacross episodes. So you could sort of\npotentially mix and match different components here.",
    "start": "1861920",
    "end": "1867529"
  },
  {
    "text": " So let's look at a\ncouple of examples",
    "start": "1867530",
    "end": "1872790"
  },
  {
    "text": "of what these algorithms do. So the first example\nwill actually",
    "start": "1872790",
    "end": "1878310"
  },
  {
    "text": "be just the maze\nnavigation example that I talked\nabout before, where we want to be able to learn\nhow to navigate a maze.",
    "start": "1878310",
    "end": "1885360"
  },
  {
    "text": "And we're going to train on\n1,000 different small mazes. And then we're going to\nevaluate the algorithm's ability",
    "start": "1885360",
    "end": "1892710"
  },
  {
    "text": "to quickly learn how to solve\nnew mazes, both small mazes and large mazes.",
    "start": "1892710",
    "end": "1897990"
  },
  {
    "text": " And then after the kind\nof meta training process,",
    "start": "1897990",
    "end": "1905650"
  },
  {
    "text": "we'll look at some\nvideos of what it can do at meta test time. And so here's a video.",
    "start": "1905650",
    "end": "1911312"
  },
  {
    "text": "This is showing the first\nperson point of view. This is showing kind\nof a top-down view of what the agent is doing.",
    "start": "1911312",
    "end": "1917400"
  },
  {
    "text": "This is the kind of\nfirst episode of--",
    "start": "1917400",
    "end": "1922860"
  },
  {
    "text": "the first episode\nat meta test time, where it's essentially\nexploring the maze",
    "start": "1922860",
    "end": "1929130"
  },
  {
    "text": "and looking at\ndifferent parts of it. And this is the second\nepisode where it just",
    "start": "1929130",
    "end": "1934710"
  },
  {
    "text": "goes straight to the goal. And so we can probably just\ntry to play that again.",
    "start": "1934710",
    "end": "1941560"
  },
  {
    "text": "So here's the first episode\nwhere it's exploring. This is essentially to try to\nunderstand the MDP or the maze",
    "start": "1941560",
    "end": "1947910"
  },
  {
    "text": "that it's in. And then it's incrementally\nkind of updating the hidden state of the RNN\nbased on the training data",
    "start": "1947910",
    "end": "1954240"
  },
  {
    "text": "that it's seen so far. And then at the end\nof this episode, it's going to keep its\nhidden state the same",
    "start": "1954240",
    "end": "1960690"
  },
  {
    "text": "and then use a policy that\ncan adapt to the exploration episode that it\ncollected in order",
    "start": "1960690",
    "end": "1966480"
  },
  {
    "text": "to go straight to the goal.  The system can also work\non larger mazes as well.",
    "start": "1966480",
    "end": "1973049"
  },
  {
    "text": "So here's an example of a large\nmaze where it is currently exploring in the maze in the\nfirst episode, still exploring.",
    "start": "1973050",
    "end": "1984860"
  },
  {
    "text": "And then it sees the goal here. And then after that\nepisode, it can very quickly",
    "start": "1984860",
    "end": "1993020"
  },
  {
    "text": "go directly to the goal. So both of these\nare held up mazes that it hadn't seen\nduring training.",
    "start": "1993020",
    "end": "1999290"
  },
  {
    "text": "And in some ways, you can\nthink of the first episode as the support set,\nand the second episode as the query set.",
    "start": "1999290",
    "end": "2005308"
  },
  {
    "text": "But in this case,\nit is actually also just trying to run things\nonline as well, so yeah.",
    "start": "2005308",
    "end": "2011760"
  },
  {
    "text": " And then in terms of\nquantitative results,",
    "start": "2011760",
    "end": "2017640"
  },
  {
    "text": "this is comparing using an\nLSTM versus using the mixture",
    "start": "2017640",
    "end": "2023910"
  },
  {
    "text": "of attention and conflairs. And it's also comparing\nto just a random policy.",
    "start": "2023910",
    "end": "2029970"
  },
  {
    "text": "And what we see is-- I think this is evaluating\nthe number of time steps",
    "start": "2029970",
    "end": "2038550"
  },
  {
    "text": "that are needed to reach\nthe goal in each episode. And so the first\nthing that we see is that the random\nisn't doing very well.",
    "start": "2038550",
    "end": "2046991"
  },
  {
    "text": "We also see that\ngenerally it takes longer to find the goal in the\nlarge maze than the small maze. And then generally,\nwe see that it's",
    "start": "2046991",
    "end": "2053310"
  },
  {
    "text": "able to more quickly\nfind the goal in episode two than in episode one because\nit's leveraging the exploration",
    "start": "2053310",
    "end": "2059850"
  },
  {
    "text": "that it did in\nepisode one in order to find the goal in\nepisode two more quickly. ",
    "start": "2059850",
    "end": "2068477"
  },
  {
    "text": "Any questions on this? ",
    "start": "2068477",
    "end": "2076657"
  },
  {
    "text": "And one thing I didn't show here\nis the meta training process itself. Those videos were just\nfrom meta test time.",
    "start": "2076657",
    "end": "2081665"
  },
  {
    "text": "And during meta\ntraining time, it was essentially doing that same\nsort of two-episode process,",
    "start": "2081665",
    "end": "2087388"
  },
  {
    "text": "but across all of\nthese different mazes in its meta training set. And so it's kind of learning\nhow to explore and solve",
    "start": "2087389",
    "end": "2093270"
  },
  {
    "text": "these mazes so that when\nit's given a new maze, it can quickly solve that maze.",
    "start": "2093270",
    "end": "2098880"
  },
  {
    "text": " Zoom? ",
    "start": "2098880",
    "end": "2106520"
  },
  {
    "text": "Is there information\nabout duration the carrier is learning, sharing\nbetween mazes eventually?",
    "start": "2106520",
    "end": "2116270"
  },
  {
    "text": "So, what is the learner\nsharing across mazes? So one thing that you can\nsee is that because it's",
    "start": "2116270",
    "end": "2122550"
  },
  {
    "text": "doing better in episode\ntwo than in episode one, it is clearly\nstoring information in its hidden state that\ntells it about that maze.",
    "start": "2122550",
    "end": "2131345"
  },
  {
    "text": "It's going to basically\nbe encoding something about the geometry of the maze. I don't think that they did\nany sort of visualization",
    "start": "2131345",
    "end": "2138260"
  },
  {
    "text": "in the paper about whether\nit's storing the full geometry",
    "start": "2138260",
    "end": "2143630"
  },
  {
    "text": "or just storing like\nthe solution path, for example, to the goal. But that's something\nlike that could be interesting to visualize.",
    "start": "2143630",
    "end": "2151700"
  },
  {
    "text": "Did you have a question? [INAUDIBLE] ",
    "start": "2151700",
    "end": "2163350"
  },
  {
    "text": "But in this case, I feel\nlike the lost [INAUDIBLE] come from different\ndistribution [INAUDIBLE]",
    "start": "2163350",
    "end": "2170210"
  },
  {
    "text": "so from the same\nassumption [INAUDIBLE] Yeah, so great question.",
    "start": "2170210",
    "end": "2175500"
  },
  {
    "text": "The question is,\nwe assumed before that the kind of meta training\ntasks and the meta test tasks are the same.",
    "start": "2175500",
    "end": "2182892"
  },
  {
    "text": "It seems like now when we're\ntesting on large mazes, there's actually this mismatch.",
    "start": "2182893",
    "end": "2188030"
  },
  {
    "text": "So in general, we\ngenerally still need to make the same assumption that\ntraining and testing tasks are from the same distribution.",
    "start": "2188030",
    "end": "2195170"
  },
  {
    "text": "Qualitatively, they found\nthat the larger mazes were similar enough that it\nwas still able to generalize.",
    "start": "2195170",
    "end": "2200840"
  },
  {
    "text": "I expect that if you\ngave it like a maze that was 10 times as large, it\nprobably wouldn't work.",
    "start": "2200840",
    "end": "2207530"
  },
  {
    "text": "One reason why it might\nbe generalizing about is that it is still a\nfirst person observation. And so the data that\nit's seeing is still",
    "start": "2207530",
    "end": "2213868"
  },
  {
    "text": "going to be very similar between\nthe train and the test tasks. Like for example, the\ncorridors are the same size. The visuals are\ngenerally the same.",
    "start": "2213868",
    "end": "2222630"
  },
  {
    "text": "And so it is nice that\nit's able to generalize. It's also possible that\nin cases where there's a longer path towards\nthe goal, maybe",
    "start": "2222630",
    "end": "2229563"
  },
  {
    "text": "those are the scenarios where\nit's not able to do as well. Whereas if it's in a larger\nmaze but there's still a small path to the\ngoal, maybe it's",
    "start": "2229563",
    "end": "2235610"
  },
  {
    "text": "still able to do well because it\nsaw a smaller path to the goal during training.",
    "start": "2235610",
    "end": "2241340"
  },
  {
    "text": "Yeah? [INAUDIBLE] ",
    "start": "2241340",
    "end": "2249860"
  },
  {
    "text": "--within a task, the\nsupport set is [INAUDIBLE] ",
    "start": "2249860",
    "end": "2269410"
  },
  {
    "text": "Yeah so the question is,\nwhat is the support set in the query set? And what policy are we using\nto get the support set?",
    "start": "2269410",
    "end": "2275020"
  },
  {
    "text": "And what policy are we\nusing to get the query set? So you can essentially--\nyou can think",
    "start": "2275020",
    "end": "2280250"
  },
  {
    "text": "of it as support set being\nepisode one and query set being episode two. In reality, it's more like a\nrolling online basis, where",
    "start": "2280250",
    "end": "2287230"
  },
  {
    "text": "the support set is all the time\nsteps that you've seen so far. And the query set is\nthe current time step. And so you're\nactually going to be",
    "start": "2287230",
    "end": "2293440"
  },
  {
    "text": "training on support sets that\nhave lots of different sizes. And in terms of what\npolicy that you're using--",
    "start": "2293440",
    "end": "2300790"
  },
  {
    "text": "and this work is actually\nusing the same policy for collecting the first\nepisode and the second episode, which is just this recurrent\nneural network that",
    "start": "2300790",
    "end": "2307810"
  },
  {
    "text": "takes as input some hidden\nstate that encodes the support set seen so far. ",
    "start": "2307810",
    "end": "2315280"
  },
  {
    "text": "And that said, in practice, it\ncould be different policies. And we'll especially see in\nthe next lecture on Wednesday,",
    "start": "2315280",
    "end": "2323733"
  },
  {
    "text": "there are scenarios\nwhere you want them to be different policies. So as a kind of an example\nof that, say in the mazes,",
    "start": "2323733",
    "end": "2331427"
  },
  {
    "text": "say there on one of\nthe walls, there's actually a map that shows\nthe layout of the maze.",
    "start": "2331427",
    "end": "2337120"
  },
  {
    "text": "In that setting, in\nthe first episode, you want to just\ngo look at the map. You don't want to\ngo figure it out.",
    "start": "2337120",
    "end": "2343672"
  },
  {
    "text": "And so you want a\npolicy that looks at the map for\nexploration, and then uses that information from the map\nin order to solve the task.",
    "start": "2343672",
    "end": "2350753"
  },
  {
    "text": "And so in scenarios\nlike that, you often do want different, pretty\ndifferent policies.",
    "start": "2350753",
    "end": "2356920"
  },
  {
    "text": "Yeah? [INAUDIBLE] ",
    "start": "2356920",
    "end": "2382300"
  },
  {
    "text": "It seems like this is\ngoing to work decently well with on-policy methods. And it's going to be\na lot more complicated",
    "start": "2382300",
    "end": "2387862"
  },
  {
    "text": "for off-policy methods\nlike DQN or like Q-learning-based methods. Is that what you're asking?",
    "start": "2387862",
    "end": "2393740"
  },
  {
    "text": "[INAUDIBLE] ",
    "start": "2393740",
    "end": "2401589"
  },
  {
    "text": "Yeah, so there have been\nsuccessful instances of people training\nQ-learning algorithms with recurrent\nneural networks that",
    "start": "2401590",
    "end": "2407560"
  },
  {
    "text": "have kind of an online update. That said, generally,\nrecurrent policies are pretty difficult to\noptimize with Q-learning.",
    "start": "2407560",
    "end": "2416740"
  },
  {
    "text": "And there are a couple of\ntricks that people use. There's this paper\ncalled R2D2 that",
    "start": "2416740",
    "end": "2422080"
  },
  {
    "text": "shows that if you\nessentially store the hidden states in the\nbuffer rather than recomputing the hidden states, that\nactually works a lot better.",
    "start": "2422080",
    "end": "2430234"
  },
  {
    "text": "That said, it does\nget more complicated with value-based methods. And I can actually\nshow one example",
    "start": "2430235",
    "end": "2435317"
  },
  {
    "text": "of value based methods\nmaybe before getting to the other question. So in particular, we want to be\nable to use value-based methods",
    "start": "2435318",
    "end": "2443080"
  },
  {
    "text": "because they're a\nlot more efficient. This paper is going to look\nnot at navigation problems",
    "start": "2443080",
    "end": "2448180"
  },
  {
    "text": "but at continuous control\nproblems, where you might want the agent to run in\ndifferent directions",
    "start": "2448180",
    "end": "2453970"
  },
  {
    "text": "or at different velocities,\nor be able to adapt to different physical dynamics. ",
    "start": "2453970",
    "end": "2461800"
  },
  {
    "text": "And so in general,\nmeta reinforcement learning algorithms are very\nefficient at meta test time.",
    "start": "2461800",
    "end": "2467830"
  },
  {
    "text": "They can solve the task\nin like two episodes. But what's challenging is\nthe meta training process might be very inefficient with\nthings like policy gradients.",
    "start": "2467830",
    "end": "2475355"
  },
  {
    "text": "And so what we'd love to do\nis to use value-based methods or off-policy methods\nduring meta training in order to optimize for\nthe black box approach.",
    "start": "2475355",
    "end": "2482695"
  },
  {
    "text": " And so I guess, maybe\nI gave this one away,",
    "start": "2482695",
    "end": "2489340"
  },
  {
    "text": "but you would expect\noff-policy meta RL to be much more efficient\nthan on-policy meta RL",
    "start": "2489340",
    "end": "2494410"
  },
  {
    "text": "because these meta RL methods\nare going to inherit the sample complexity of the algorithm\nthat they're using to optimize",
    "start": "2494410",
    "end": "2500347"
  },
  {
    "text": "the meta learning process.  And so essentially\nwhat this work did",
    "start": "2500347",
    "end": "2507295"
  },
  {
    "text": "is it used the kind of\nfeedforward and average architecture in order to\ncompute a context just like I",
    "start": "2507295",
    "end": "2513151"
  },
  {
    "text": "showed on the previous slide. And it fed that context\ninto both the policy and into the critic.",
    "start": "2513152",
    "end": "2518500"
  },
  {
    "text": "And it found that if you\ndid something like that, you are able to actually solve\nthese meta RL tasks much more",
    "start": "2518500",
    "end": "2525490"
  },
  {
    "text": "efficiently than\non-policy methods. And so what these\nplots are showing is the x-axis is the amount\nof meta training time steps.",
    "start": "2525490",
    "end": "2532240"
  },
  {
    "text": "And the y-axis is how\nwell you're able to adapt. And the blue curve is\nthis algorithm that uses--",
    "start": "2532240",
    "end": "2539817"
  },
  {
    "text": "that builds upon a value-based\nor all method soft actor critic. And all the other methods\nare using policy gradient.",
    "start": "2539818",
    "end": "2547359"
  },
  {
    "text": "And so you see that there's kind\nof this pretty huge difference in sample efficiency. The on-policy methods,\ntheir asymptotic performance",
    "start": "2547360",
    "end": "2554950"
  },
  {
    "text": "is shown with the dashed line. And so we see that\nasymptotically, they do actually do pretty well. But the time it takes\nfor them to learn",
    "start": "2554950",
    "end": "2560990"
  },
  {
    "text": "is much, much slower than\nvalue-based RL methods. Does that answer your question?",
    "start": "2560990",
    "end": "2567690"
  },
  {
    "text": "Cool, did you have a\nquestion in the back? No? ",
    "start": "2567690",
    "end": "2577600"
  },
  {
    "text": "Cool, and then one\nsort digression is that we've talked\nabout black box methods.",
    "start": "2577600",
    "end": "2584390"
  },
  {
    "text": "And these actually look somewhat\nsimilar to multi-task policies.",
    "start": "2584390",
    "end": "2589769"
  },
  {
    "text": "So for example, in a\nmulti-task learning policy, we'll be conditioning\non a task identifier.",
    "start": "2589770",
    "end": "2595549"
  },
  {
    "text": "So for example,\nmaybe we have a robot and we want to stack\nin different locations. Z might be the location\nwhere we should stack.",
    "start": "2595550",
    "end": "2602240"
  },
  {
    "text": "Or maybe we want to walk\nin different directions. And Zi would encode the\ndirection that we want to walk. ",
    "start": "2602240",
    "end": "2609620"
  },
  {
    "text": "When we have these black\nbox meta learning methods, you can essentially view\nthe data or the experience",
    "start": "2609620",
    "end": "2617300"
  },
  {
    "text": "as the task identifier that\nyou're passing into the model.",
    "start": "2617300",
    "end": "2622340"
  },
  {
    "text": "So it's sort of like the\nsame thing as multi-task RL, except where\nexperience is helping you identify the task rather\nthan the explicit identifier.",
    "start": "2622340",
    "end": "2631100"
  },
  {
    "text": "And then of course\nyou have to figure out how to explore to\ncollect information like.",
    "start": "2631100",
    "end": "2636980"
  },
  {
    "text": "That and then the policy,\nthe multi-task policy",
    "start": "2636980",
    "end": "2642200"
  },
  {
    "text": "is kind of the last part\nof that, so essentially",
    "start": "2642200",
    "end": "2647869"
  },
  {
    "text": "like a multi-task policy with\nyour experience as a task identifier. ",
    "start": "2647870",
    "end": "2655698"
  },
  {
    "text": "We've also previously talked\na lot about goal condition policies in value functions.",
    "start": "2655698",
    "end": "2661200"
  },
  {
    "text": "One thing that's\nnice about meta RL is that rewards are essentially\nkind of a strict generalization of goals.",
    "start": "2661200",
    "end": "2667500"
  },
  {
    "text": "Goals can specify\nreaching certain states. Whereas rewards can specify all\nsorts of behaviors, including",
    "start": "2667500",
    "end": "2673380"
  },
  {
    "text": "reaching certain states. And so really, the\nmeta RL objective",
    "start": "2673380",
    "end": "2679020"
  },
  {
    "text": "is allowing you to\nadapt to new tasks from rewards for that task. Whereas goal-conditioned RL\nis allowing you to generalize",
    "start": "2679020",
    "end": "2686640"
  },
  {
    "text": "to reaching new goals.  You can also think of this as\nk shot adaptation versus zero",
    "start": "2686640",
    "end": "2693570"
  },
  {
    "text": "shot generalization. Yeah-- [INAUDIBLE]",
    "start": "2693570",
    "end": "2699550"
  },
  {
    "start": "2699550",
    "end": "2713020"
  },
  {
    "text": "So the question is,\nin the maze example, what was the reward function? Was it only like\nreaching a goal?",
    "start": "2713020",
    "end": "2719160"
  },
  {
    "text": "Or does it account for the time\nit takes to reach the goal?",
    "start": "2719160",
    "end": "2724829"
  },
  {
    "text": "I don't know exactly\nwhat they used. There are a few\ndifferent choices one could just be like\none for reaching the goal,",
    "start": "2724830",
    "end": "2730652"
  },
  {
    "text": "and zero not for\nreaching the goal. And if you have a discount\nfactor that's less than one, that it's going to\nencourage you to try",
    "start": "2730652",
    "end": "2736110"
  },
  {
    "text": "to reach the goal faster. Another thing that\nmight be common to do is to use a positive\nreward for reaching the goal and a small\nnegative reward",
    "start": "2736110",
    "end": "2742933"
  },
  {
    "text": "for every time step in which\nyou haven't reached the goal. And that will also encourage\nit to get there faster.",
    "start": "2742933",
    "end": "2748362"
  },
  {
    "text": "The last thing that\nyou could do is you could give it a shaped\nreward that indicates the distance to the goal.",
    "start": "2748362",
    "end": "2754380"
  },
  {
    "text": "That is something that\nyou could also use. Although that's going to give\nit a lot more information",
    "start": "2754380",
    "end": "2759670"
  },
  {
    "text": "about where the goal is\nand whether it's getting closer or further to the goal. It's more like a\ngame of hot cold than actually trying\nto navigate and find",
    "start": "2759670",
    "end": "2767430"
  },
  {
    "text": "where a certain object is. On that note, the other\nthing that I can mention is that the rewards that\nyou use in your support set,",
    "start": "2767430",
    "end": "2775140"
  },
  {
    "text": "that you feed into\nthe network, those could actually be\na different reward function than the one\nyou use to optimize.",
    "start": "2775140",
    "end": "2780570"
  },
  {
    "text": "And so one thing that you\ncould do that's kind of cool is give it a very\nsparse reward that it has to learn from\nin the support set.",
    "start": "2780570",
    "end": "2787470"
  },
  {
    "text": "But then train your-- train to have your whole\nmeta training process with more of a shaped reward.",
    "start": "2787470",
    "end": "2793680"
  },
  {
    "text": "And then you could essentially\nlearn how to explore and solve sparse reward tasks using\nmore dense rewards during meta",
    "start": "2793680",
    "end": "2801420"
  },
  {
    "text": "training. ",
    "start": "2801420",
    "end": "2807180"
  },
  {
    "text": "So to summarize\nblack box meta RL, these methods are quite\ngeneral and quite expressive.",
    "start": "2807180",
    "end": "2814140"
  },
  {
    "text": "There's of course a\nvariety of design choices in the architecture.",
    "start": "2814140",
    "end": "2819150"
  },
  {
    "text": "And they're generally\npretty easy to combine with different RL optimizers.",
    "start": "2819150",
    "end": "2827160"
  },
  {
    "text": "Essentially you'll\noptimize the behavior either with respect to\nsome Bellman objective",
    "start": "2827160",
    "end": "2832410"
  },
  {
    "text": "like in Q-learning, or with\nrespect to policy gradients.",
    "start": "2832410",
    "end": "2837450"
  },
  {
    "text": "They do tend to be somewhat\ndifficult to optimize, like we've talked about. Especially with\nrecurrent networks,",
    "start": "2837450",
    "end": "2842640"
  },
  {
    "text": "you have to back propagate\nthrough this long string of time steps. Also in general you are--",
    "start": "2842640",
    "end": "2849030"
  },
  {
    "text": "like in black box\nmeta learning, you're learning how to\nlearn from scratch. You're not building\nany structure into the meta learner.",
    "start": "2849030",
    "end": "2855280"
  },
  {
    "text": "And so this also makes\nit difficult to optimize. And then like we saw before, in\nterms of the data efficiency,",
    "start": "2855280",
    "end": "2862750"
  },
  {
    "text": "it's going to inherit its data\nefficiency from the outer RL optimizer that's used. And so if you use\npolicy gradients,",
    "start": "2862750",
    "end": "2869010"
  },
  {
    "text": "it's going to be very\ndata inefficient. And if you use\nvalue-based RL, it's going to be probably\na lot more efficient.",
    "start": "2869010",
    "end": "2875100"
  },
  {
    "text": " All right, any questions\non black box meta",
    "start": "2875100",
    "end": "2881335"
  },
  {
    "text": "RL before we move on to\noptimization-based methods? ",
    "start": "2881335",
    "end": "2890840"
  },
  {
    "text": "Cool, so like we\njust talked about,",
    "start": "2890840",
    "end": "2896090"
  },
  {
    "text": "black box meta learning methods\ncan be difficult to optimize. And so one thing that\nmight make it easier is to build in the\nstructure of optimization",
    "start": "2896090",
    "end": "2903080"
  },
  {
    "text": "into the meta learner.  And so if we want to turn\nthis kind of approach",
    "start": "2903080",
    "end": "2910430"
  },
  {
    "text": "into a meta RL algorithm, we\nhave a few different choices for doing this.",
    "start": "2910430",
    "end": "2916470"
  },
  {
    "text": "And in particular, one of\nthe choices we have is, what sort of\noptimization should be used in the inner loop\nof this optimization?",
    "start": "2916470",
    "end": "2925310"
  },
  {
    "text": " As a kind of a few\npossible choices,",
    "start": "2925310",
    "end": "2931010"
  },
  {
    "text": "we could use policy gradients. We could use Q-learning. Or we could also use\nsomething like model based RL.",
    "start": "2931010",
    "end": "2938803"
  },
  {
    "text": "I'm curious if\nanyone has thoughts on what some of the\ntrade offs might be for using these different\napproaches in the inner loop,",
    "start": "2938803",
    "end": "2945980"
  },
  {
    "text": "irrespective of what you\ndo in the outer loop. Yeah-- I guess from what we\ndiscussed [INAUDIBLE]",
    "start": "2945980",
    "end": "2954869"
  },
  {
    "start": "2954870",
    "end": "2970970"
  },
  {
    "text": "Yeah, so policy gradients can\nbe very sample inefficient. And so that might\nbe a reason not",
    "start": "2970970",
    "end": "2977210"
  },
  {
    "text": "to use them in the inner loop. Yeah. For model-based RL, this is like\nkind of similar to what he said",
    "start": "2977210",
    "end": "2985940"
  },
  {
    "text": "[INAUDIBLE]",
    "start": "2985940",
    "end": "2996859"
  },
  {
    "text": "Yeah, so if your state\nspace is very large and you're trying to solve\nthe maze navigation example,",
    "start": "2996860",
    "end": "3004060"
  },
  {
    "text": "for example, then your model\nmay actually be very inaccurate if your support set doesn't\ncover the entire relevant part",
    "start": "3004060",
    "end": "3010240"
  },
  {
    "text": "of the state space. Yeah-- [INAUDIBLE]",
    "start": "3010240",
    "end": "3016310"
  },
  {
    "text": " Yeah, so if the tasks all\nhave the same dynamics",
    "start": "3016310",
    "end": "3023500"
  },
  {
    "text": "but have different\nrewards, then you can actually just\nuse a single model. And you don't actually\nneed to adopt the model.",
    "start": "3023500",
    "end": "3028940"
  },
  {
    "text": "And so using MAML with\nmodel based RL probably makes the most sense\nwhen the dynamics are changing across tasks.",
    "start": "3028940",
    "end": "3034453"
  },
  {
    "text": " Any other points? ",
    "start": "3034453",
    "end": "3043240"
  },
  {
    "text": "Cool, so those are\nall great points. I'll overview some of\nthe kind of trade offs",
    "start": "3043240",
    "end": "3048640"
  },
  {
    "text": "and why we might use one of\nthese versus the other in MAML. So with policy gradients,\none thing that's nice",
    "start": "3048640",
    "end": "3054640"
  },
  {
    "text": "is that they are very\nclearly gradient-based. They very clearly\ngive you a gradient. And so that makes\nit nice to apply",
    "start": "3054640",
    "end": "3062050"
  },
  {
    "text": "a gradient in the inner loop. They are on policy. And so this means that they\nare going to be inefficient.",
    "start": "3062050",
    "end": "3069682"
  },
  {
    "text": "The other thing that's maybe\na little bit more subtle about policy gradients is\nthat they don't actually carry that much information,\nespecially",
    "start": "3069682",
    "end": "3076810"
  },
  {
    "text": "when you have sparse rewards. So say for example you're maybe\nin the maze navigation example,",
    "start": "3076810",
    "end": "3082970"
  },
  {
    "text": "or you're just trying to\nkind of learn something about the dynamics about the\nstructure of the dynamics in the environment.",
    "start": "3082970",
    "end": "3088600"
  },
  {
    "text": "Then if you look at\nthe policy gradient, if you remember\nit looks something like the gradient of log pi\nsummed over your time steps.",
    "start": "3088600",
    "end": "3102470"
  },
  {
    "text": "It looks something like this,\ntimes the sum of rewards for those time steps.",
    "start": "3102470",
    "end": "3108612"
  },
  {
    "text": "And if you're trying to learn\nsomething about the dynamics, and you end up-- maybe you're\nin the maze navigation example",
    "start": "3108612",
    "end": "3114700"
  },
  {
    "text": "and you collect a trajectory. And you didn't get to the goal. So you've got a reward of zero.",
    "start": "3114700",
    "end": "3121180"
  },
  {
    "text": "Then that means that\nthis is going to be zero. And then your entire\ngradient is going to be zero.",
    "start": "3121180",
    "end": "3127780"
  },
  {
    "text": "And that's really\nunfortunate, because that means you're not going\nto get any information about the dynamics\nof the environment",
    "start": "3127780",
    "end": "3134290"
  },
  {
    "text": "if you didn't get any reward. And so this is one reason\nwhy policy gradients",
    "start": "3134290",
    "end": "3139960"
  },
  {
    "text": "can be pretty difficult\nwith an algorithm like MAML. It's because that you\nwant the inner gradient",
    "start": "3139960",
    "end": "3145150"
  },
  {
    "text": "to be very informative\nabout the task. And whenever you're in\na sparse reward setting and you don't get\nany rewards, it's",
    "start": "3145150",
    "end": "3150910"
  },
  {
    "text": "not going to tell you any\ninformation about the task and about the environment.",
    "start": "3150910",
    "end": "3157105"
  },
  {
    "text": "So it can work well with\n[INAUDIBLE] rewards. But if you have sparse rewards,\nit can be rather problematic.",
    "start": "3157105",
    "end": "3164200"
  },
  {
    "text": "Yeah-- [INAUDIBLE] ",
    "start": "3164200",
    "end": "3172930"
  },
  {
    "text": "Yeah, so if you--  reward shaping will\ncertainly help.",
    "start": "3172930",
    "end": "3178720"
  },
  {
    "text": "And that will give you\na lot more information. If you give it like a negative\nreward for everything,",
    "start": "3178720",
    "end": "3185260"
  },
  {
    "text": "like maybe this is kind of a\nfixed concept of negative T because maybe you give\nit a reward of negative 1",
    "start": "3185260",
    "end": "3190420"
  },
  {
    "text": "every time step. This still isn't going to be\nthat informative because it's",
    "start": "3190420",
    "end": "3197740"
  },
  {
    "text": "for four different episodes. Like it's not going to\ntell you essentially which episodes are\nbetter than the other",
    "start": "3197740",
    "end": "3206900"
  },
  {
    "text": "if you're just getting\na reward of negative T for all of your episodes. ",
    "start": "3206900",
    "end": "3216300"
  },
  {
    "text": "Cool, so those are kind\nof the pros and cons of policy gradients. Now for Q-learning, Q-learning\nactually isn't gradient-based.",
    "start": "3216300",
    "end": "3227100"
  },
  {
    "text": "Q-learning learning is more\nlike dynamic programming. And if you look at the\nupdate for Q-learning,",
    "start": "3227100",
    "end": "3235800"
  },
  {
    "text": "it's something like kind of\nQ hat of s a, you're trying to make this match--",
    "start": "3235800",
    "end": "3241577"
  },
  {
    "text": "I guess you look at the\nloss function at something like the reward plus gamma times\nmax of a prime of Q of s prime",
    "start": "3241577",
    "end": "3253270"
  },
  {
    "text": "a prime. Something like that. And one thing that\nyou might notice is if you take a single\ngradient step on this objective,",
    "start": "3253270",
    "end": "3264090"
  },
  {
    "text": "then this is only going to\npropagate kind of one time step of information, because\nthis is the state in action",
    "start": "3264090",
    "end": "3270473"
  },
  {
    "text": "at the next time step. This is the state in action\nat the current time step. And so a single\ngradient update isn't",
    "start": "3270473",
    "end": "3276675"
  },
  {
    "text": "going to give you a lot\nof global information about the task. It's only going to give\nyou very local information",
    "start": "3276675",
    "end": "3281910"
  },
  {
    "text": "at a particular time step. And as a result, this means that\nto get a good kind of update",
    "start": "3281910",
    "end": "3290130"
  },
  {
    "text": "for your inner loop, you're\ngoing to need a lot of steps to propagate information from\nthe future to the current step.",
    "start": "3290130",
    "end": "3300869"
  },
  {
    "text": "And as a result,\nit can actually be very difficult to use\nQ-learning in the inner loop,",
    "start": "3300870",
    "end": "3307440"
  },
  {
    "text": "unless you take a very\nlarge number of steps. But then you have to\nalso back propagate through that very large number\nof steps in the inner loop.",
    "start": "3307440",
    "end": "3313980"
  },
  {
    "text": " It is going to be\noff-policy so it's going",
    "start": "3313980",
    "end": "3320160"
  },
  {
    "text": "to be more data efficient. That said, there's been\nvery few successful examples of optimization-based\nQ-learning, meta learning",
    "start": "3320160",
    "end": "3328420"
  },
  {
    "text": "methods.  And then one last\nexample is for model",
    "start": "3328420",
    "end": "3335520"
  },
  {
    "text": "based RL, learning\na model, if you want to adapt to\ndifferent dynamics,",
    "start": "3335520",
    "end": "3341640"
  },
  {
    "text": "model-based RL is just a\nsupervised learning problem on the model. And so updates to your model\nare going to be gradient-based.",
    "start": "3341640",
    "end": "3349740"
  },
  {
    "text": "Also, you can do this\nin an off-policy way. And so it can also\nbe data efficient.",
    "start": "3349740",
    "end": "3355050"
  },
  {
    "text": "This makes the\nmost sense of when you have a more local\nupdate and when you have",
    "start": "3355050",
    "end": "3360300"
  },
  {
    "text": "varying dynamics across tasks. But it does address\nsome of the shortcomings",
    "start": "3360300",
    "end": "3365550"
  },
  {
    "text": "that we see in policy\ngradients and Q-learning. ",
    "start": "3365550",
    "end": "3374190"
  },
  {
    "text": "So at a high level, essentially\noptimization-based meta RL",
    "start": "3374190",
    "end": "3381319"
  },
  {
    "text": "can look like embedding an\noptimization, where you choose different objectives\nfor the inner loop and different objectives\nfor the outer loop.",
    "start": "3381320",
    "end": "3388460"
  },
  {
    "text": "Next I'll just go through\ntwo instantiations of optimization-based\nmeta RL so we can look at some kind of\nconcrete examples of actually",
    "start": "3388460",
    "end": "3396560"
  },
  {
    "text": "implementing these algorithms. So the first thing\nthat we'll look at is MAML with policy gradients.",
    "start": "3396560",
    "end": "3403313"
  },
  {
    "text": "Like we mentioned, it\ncan be a terrible choice if you have sparse rewards. Although it can still be\nreasonable if you have shaped",
    "start": "3403313",
    "end": "3408890"
  },
  {
    "text": "rewards and plenty of data. So here's-- to help us\nremember the objectives,",
    "start": "3408890",
    "end": "3419970"
  },
  {
    "text": "here's the MAML objective. Here is the policy gradient\nobjective, which is hopefully the same as what I wrote here. I guess there's also kind of\nan expectation with respect",
    "start": "3419970",
    "end": "3427339"
  },
  {
    "text": "to pi theta. And so what the meta training\nprocess looks like is we",
    "start": "3427340",
    "end": "3432620"
  },
  {
    "text": "will sample a task. we'll then collect\ndata by rolling out",
    "start": "3432620",
    "end": "3438830"
  },
  {
    "text": "our policy, pi theta. This is the policy according to\nour set of initial parameters.",
    "start": "3438830",
    "end": "3445760"
  },
  {
    "text": "Then for the inner\nloop adaptation, we're going to compute a policy\ngradient using those rollouts,",
    "start": "3445760",
    "end": "3451400"
  },
  {
    "text": "by computing this objective. Or sorry, this is just the\ngradient, so by computing",
    "start": "3451400",
    "end": "3457160"
  },
  {
    "text": "the gradient. Then we're going to\ncollect data by rolling out",
    "start": "3457160",
    "end": "3462200"
  },
  {
    "text": "our adapted policy. And this will be\nused so that we can compute the policy\ngradient with respect",
    "start": "3462200",
    "end": "3469760"
  },
  {
    "text": "to of that adopted policy.  And so each of\nthese cases, we're",
    "start": "3469760",
    "end": "3476120"
  },
  {
    "text": "going to be using\na policy gradient both for the inner loop\nand a policy gradient for the outer loop objective.",
    "start": "3476120",
    "end": "3481920"
  },
  {
    "text": "Yeah-- [INAUDIBLE] ",
    "start": "3481920",
    "end": "3511100"
  },
  {
    "text": "Yeah, so the question\nis, for this step two, is there any benefit from\nusing your current policy versus a random policy?",
    "start": "3511100",
    "end": "3519680"
  },
  {
    "text": "Or some other policy? So the I guess the reason why we\nmight want to use pi theta here",
    "start": "3519680",
    "end": "3526190"
  },
  {
    "text": "is that, for this to be the\ncorrect policy gradient, your trajectories should\nbe drawn from pi theta.",
    "start": "3526190",
    "end": "3534089"
  },
  {
    "text": "And if it's drawn\nfrom another policy, then this gradient isn't going\nto be accurate or as accurate.",
    "start": "3534090",
    "end": "3540890"
  },
  {
    "text": "It's certainly not\ngoing to be consistent. It's not going to be something\nthat you will use to ultimately",
    "start": "3540890",
    "end": "3546049"
  },
  {
    "text": "improve your policy. And so if you do\nuse pi theta here, then you are kind\nof more guaranteed that this gradient is\ngoing to be pointing",
    "start": "3546050",
    "end": "3553160"
  },
  {
    "text": "in the right direction.  There are slightly more\noff-policy policy gradient",
    "start": "3553160",
    "end": "3560630"
  },
  {
    "text": "methods like PPO. And in those cases, you can\npotentially use older policies",
    "start": "3560630",
    "end": "3566000"
  },
  {
    "text": "to collect this. But in general, it's\ngoing to be cleaner to use your current policy.",
    "start": "3566000",
    "end": "3572295"
  },
  {
    "text": "One downside that\nyou might notice here is we are doing these\ntwo steps of collection. And this is going to\nget pretty sample--",
    "start": "3572295",
    "end": "3582350"
  },
  {
    "text": "expensive in terms\nof sample efficiency. ",
    "start": "3582350",
    "end": "3587480"
  },
  {
    "text": "And I guess one thing\nthat I'll also note is we talked about\npolicy gradient being sample inefficient\nin the inner loop.",
    "start": "3587480",
    "end": "3594080"
  },
  {
    "text": "Because we have to also rule\nout pi theta in every iteration, that means it's also sample\ninefficient at the outer loop as well, like at the level\nof the meta learning process.",
    "start": "3594080",
    "end": "3603290"
  },
  {
    "text": "I should also mention\nthat this is, of course, an iterative process. Yeah--",
    "start": "3603290",
    "end": "3609150"
  },
  {
    "text": "[INAUDIBLE]  So do we still pass previous\nreward into the policy?",
    "start": "3609150",
    "end": "3616390"
  },
  {
    "text": "In this case, no. The policy is just a policy that\nmaps from states to actions.",
    "start": "3616390",
    "end": "3621520"
  },
  {
    "text": "And the way that we're\ngoing to infer the task is through the\nparameters of that policy rather than passing\nthe rewards as input.",
    "start": "3621520",
    "end": "3628670"
  },
  {
    "text": "Of course, you could also\nimagine hybrid methods that are kind of both\nrecurrent and do the sort of gradient-based adaptation.",
    "start": "3628670",
    "end": "3636010"
  },
  {
    "text": "Although in the kind of standard\noptimization-based algorithm, we're just updating\nthe parameters rather",
    "start": "3636010",
    "end": "3641980"
  },
  {
    "text": "than passing in the rewards. ",
    "start": "3641980",
    "end": "3648630"
  },
  {
    "text": "Cool, and so the\nresult of this process is going to be a set of\ninitial parameters such that if you explore\nwith those parameters",
    "start": "3648630",
    "end": "3655350"
  },
  {
    "text": "and fine tune those\nparameters, you should be able to solve new tasks. And so what meta test\ntime looks like is we",
    "start": "3655350",
    "end": "3663450"
  },
  {
    "text": "are given a new task. We collect data\nfor that new task",
    "start": "3663450",
    "end": "3669029"
  },
  {
    "text": "by rolling out our policy with\nour trained meta parameters.",
    "start": "3669030",
    "end": "3674340"
  },
  {
    "text": "And then we're going\nto adapt our policy by running policy gradient. ",
    "start": "3674340",
    "end": "3686718"
  },
  {
    "text": "Yeah-- This [INAUDIBLE] process\ndoesn't explicitly",
    "start": "3686718",
    "end": "3691920"
  },
  {
    "text": "ensure that you\nget a [INAUDIBLE]",
    "start": "3691920",
    "end": "3698040"
  },
  {
    "text": "Good question. So the question is-- so pi theta-- we actually\nwant two things from pi theta.",
    "start": "3698040",
    "end": "3703230"
  },
  {
    "text": "We want it to be\ngood at exploration. And we want it to be\ngood so that if we run gradient descent, it\ngives us good parameters.",
    "start": "3703230",
    "end": "3709230"
  },
  {
    "text": "So we are actually asking a\nfair amount from pi theta. And so the question was,\nare we actually training it",
    "start": "3709230",
    "end": "3716190"
  },
  {
    "text": "for that first thing? Are we training it to\nbe good at exploration? So in some ways, it really\ndepends on how you actually",
    "start": "3716190",
    "end": "3725790"
  },
  {
    "text": "compute this outer loop update. So if you're thinking\nabout taking the gradient with respect to theta of\nyour adapted parameters,",
    "start": "3725790",
    "end": "3732990"
  },
  {
    "text": "there are two places\nwhere that comes in. One is, of course,\nlike in the log pi",
    "start": "3732990",
    "end": "3740850"
  },
  {
    "text": "thing, which will be very\nsimilar to the supervised case. The other is actually\nin this expectation.",
    "start": "3740850",
    "end": "3748440"
  },
  {
    "text": "And so ultimately you would\nlike to back propagate into this expectation.",
    "start": "3748440",
    "end": "3755730"
  },
  {
    "text": "Some implementations\nof the algorithm will kind of ignore this. And in those settings,\nit will not actually",
    "start": "3755730",
    "end": "3761040"
  },
  {
    "text": "be learning how to\nexplore very well. It's going to assume\nthat whatever parameters are good for adaptation are\nalso good for exploration,",
    "start": "3761040",
    "end": "3767109"
  },
  {
    "text": "which may be true if you\nhave dense enough rewards. In many cases, it\nalso won't be true.",
    "start": "3767110",
    "end": "3772793"
  },
  {
    "text": "There are also\napproaches that try to estimate, try to\nessentially figure out how to back propagate\ninto this and incorporate",
    "start": "3772793",
    "end": "3779670"
  },
  {
    "text": "the gradient with respect to\nthis part into the objective. And those algorithms can lead\nto a better exploration process.",
    "start": "3779670",
    "end": "3788040"
  },
  {
    "text": " And so one example\nof an algorithm that does that, if you're\ninterested in learning more,",
    "start": "3788040",
    "end": "3794790"
  },
  {
    "text": "is called Pro-MP. ",
    "start": "3794790",
    "end": "3800760"
  },
  {
    "text": "And it was written\nby Ignasi Clavera. Happy to send a\nreference after class.",
    "start": "3800760",
    "end": "3806160"
  },
  {
    "start": "3806160",
    "end": "3811650"
  },
  {
    "text": "Cool, one other thing that-- so I guess we'll look at\nan illustration of this.",
    "start": "3811650",
    "end": "3818850"
  },
  {
    "text": "And in particular,\nwe'll look at the task of running in\ndifferent directions",
    "start": "3818850",
    "end": "3824130"
  },
  {
    "text": "for this quadruped,\nsimulated quadruped.",
    "start": "3824130",
    "end": "3829289"
  },
  {
    "text": " And so first we can look\nat the policy with respect",
    "start": "3829290",
    "end": "3836882"
  },
  {
    "text": "to the train meta\nparameters with respect to the initialization. And this policy,\nthis is actually",
    "start": "3836882",
    "end": "3843097"
  },
  {
    "text": "with the variant of\nthe algorithm that mostly ignored this. And so it's just\ntrying to execute",
    "start": "3843097",
    "end": "3848700"
  },
  {
    "text": "a policy that will allow it\nto adapt in a single step. We end up getting\na policy that looks like this, that is essentially\nkind of running in place.",
    "start": "3848700",
    "end": "3856750"
  },
  {
    "text": "And so it seems like it's ready\nto be able to run in any given direction. Because this is a dense reward\ntask, this sort of exploration",
    "start": "3856750",
    "end": "3864793"
  },
  {
    "text": "is actually sufficient for\nfiguring out which direction it should run in. So then if you take a single\ngradient step with respect",
    "start": "3864793",
    "end": "3872250"
  },
  {
    "text": "to the task of\nrunning backward, you get a policy that\nlooks like this.",
    "start": "3872250",
    "end": "3878020"
  },
  {
    "text": "So it's able to, in\na single gradient, adapt its behavior\npretty significantly.",
    "start": "3878020",
    "end": "3885607"
  },
  {
    "text": "And then if you take a single\ngradient step with respect to the task of\nrunning forward, you get a policy that\nlooks like this.",
    "start": "3885607",
    "end": "3891760"
  },
  {
    "text": "So we again see that it's able\nto kind of pretty successfully adapt its behavior,\nat least in cases",
    "start": "3891760",
    "end": "3897192"
  },
  {
    "text": "where you have\nthese dense rewards. ",
    "start": "3897192",
    "end": "3903599"
  },
  {
    "text": "Cool, so that was an example\nof MAML with policy gradients.",
    "start": "3903600",
    "end": "3908950"
  },
  {
    "text": "We see that it can solve\nthese simple scenarios",
    "start": "3908950",
    "end": "3914380"
  },
  {
    "text": "when you have dense rewards. The meta training process\nfor this was quite slow. And we actually saw that in some\nof the plots that came before.",
    "start": "3914380",
    "end": "3925030"
  },
  {
    "text": "Now let's look at one more\nconcrete instantiation of MAML in meta RL.",
    "start": "3925030",
    "end": "3932290"
  },
  {
    "text": "In this case, we're going to\nbe looking at model based RL. And we're going to be\nlooking at a scenario",
    "start": "3932290",
    "end": "3937869"
  },
  {
    "text": "where only the dynamics\nare changing across tasks. We're also going to be\nlooking at an online variant.",
    "start": "3937870",
    "end": "3945430"
  },
  {
    "text": "And so in particular, in\nthese previous videos, we saw like you're\ncollecting some episodes",
    "start": "3945430",
    "end": "3950657"
  },
  {
    "text": "and then adapting\nfrom those episodes, and then getting a new policy. In this case, we're\nactually going to be--",
    "start": "3950657",
    "end": "3956950"
  },
  {
    "text": "our supports will just be\nin terms of time steps, not in terms of episodes. ",
    "start": "3956950",
    "end": "3964190"
  },
  {
    "text": "Great, so what we want to be\nable to do at meta test time is we want to be able to adapt\na model of the environment",
    "start": "3964190",
    "end": "3971710"
  },
  {
    "text": "to the last k time\nsteps of experience. And so, for example, if\nwe were running around",
    "start": "3971710",
    "end": "3981670"
  },
  {
    "text": "on different trains\nfor example, we want to take the last k\ntime steps of our experience of running, and\nuse that to adapt",
    "start": "3981670",
    "end": "3986920"
  },
  {
    "text": "our model to the kind of current\ndynamics of the environment.",
    "start": "3986920",
    "end": "3994359"
  },
  {
    "text": "And then once we have\nthis adapted model, then we can just run planning\nto figure out some actions that",
    "start": "3994360",
    "end": "4000060"
  },
  {
    "text": "will accomplish some reward.  So this is what we want to be\nable to do at meta test time.",
    "start": "4000060",
    "end": "4007200"
  },
  {
    "text": "And this adaptation\nprocess in step one is just going to\ncorrespond to fine tuning, to running a few steps\nof gradient descent.",
    "start": "4007200",
    "end": "4013920"
  },
  {
    "text": "And then during meta\ntraining, what we can do is we can collect a\nbunch of experience",
    "start": "4013920",
    "end": "4020580"
  },
  {
    "text": "using planning, using a\nrandom policy, and so forth. And the support set will be kind\nof k time steps of state action",
    "start": "4020580",
    "end": "4030509"
  },
  {
    "text": "pairs. This will be D train. And then the queries that\nwill just be the next k time",
    "start": "4030510",
    "end": "4037619"
  },
  {
    "text": "steps of experience. And so once we essentially have\nall of our experience collected",
    "start": "4037620",
    "end": "4046349"
  },
  {
    "text": "in some way, then we can\nconstruct these support sets and query sets just by looking\nat chunks of data, chunks",
    "start": "4046350",
    "end": "4053640"
  },
  {
    "text": "of contiguous time steps, and\nuse the first part the first k time steps as the support\nset and the next k time steps as the query set.",
    "start": "4053640",
    "end": "4061380"
  },
  {
    "text": "So essentially,\ndifferent tasks are going to correspond\nto different dynamics. But it's just going to\nassume that the dynamics are",
    "start": "4061380",
    "end": "4067260"
  },
  {
    "text": "locally consistent. And these different tasks\nwill end up, in practice, being different windows in time.",
    "start": "4067260",
    "end": "4073545"
  },
  {
    "text": " And so if, for example, you're\nrunning on different terrains.",
    "start": "4073545",
    "end": "4081160"
  },
  {
    "text": "And for like one\nday you're running on one terrain, a\ndifferent day you're running on a different\nterrain, then",
    "start": "4081160",
    "end": "4086560"
  },
  {
    "text": "this will basically create\nlots of different tasks from those different terrains.",
    "start": "4086560",
    "end": "4092530"
  },
  {
    "text": "Yeah-- [INAUDIBLE] ",
    "start": "4092530",
    "end": "4100736"
  },
  {
    "text": "Yeah, it doesn't\nhave to be the same. Actually, in this, it's\nactually h for the query set and k for the support set.",
    "start": "4100737",
    "end": "4107140"
  },
  {
    "text": "You want the support set size\nto be the same as what you're going to be doing at test time. But the query set\ncan be different.",
    "start": "4107140",
    "end": "4114969"
  },
  {
    "text": "You could choose it to be larger\nin order to get more signal. You can also choose\nit to be smaller if you think that your dynamics\nare changing very frequently.",
    "start": "4114970",
    "end": "4122200"
  },
  {
    "start": "4122200",
    "end": "4127370"
  },
  {
    "text": "Cool, and so with\nthis, we'll essentially be running the same\nMAML objective,",
    "start": "4127370",
    "end": "4132889"
  },
  {
    "text": "except we'll be using a loss\nfunction that corresponds to how well your model is\npredicting the next state,",
    "start": "4132890",
    "end": "4138439"
  },
  {
    "text": "given the current\nstate in action. ",
    "start": "4138439",
    "end": "4143830"
  },
  {
    "text": "And then I can-- if it's helpful, I can\nexplicitly write that out. So the loss function\nwill be, for a task i,",
    "start": "4143830",
    "end": "4152649"
  },
  {
    "text": "will be something like\nf of s a minus s prime.",
    "start": "4152649",
    "end": "4162160"
  },
  {
    "text": "And this will be\nsampled over kind of state action, next state\nfor our particular task.",
    "start": "4162160",
    "end": "4169525"
  },
  {
    "text": " So this is the loss function\nfor a particular task.",
    "start": "4169525",
    "end": "4175910"
  },
  {
    "text": "And then the MAML\nobjective will be the same as what we\nsaw before, where we adapt our model with\nrespect to the loss",
    "start": "4175910",
    "end": "4187689"
  },
  {
    "text": "function for a given task on\nsome set of data for that task.",
    "start": "4187689",
    "end": "4193707"
  },
  {
    "text": "This is what we want to\nbe able to do at test time when we're fine\ntuning our model. And then we're going to be\noptimizing for the initial set",
    "start": "4193707",
    "end": "4200590"
  },
  {
    "text": "of model parameters such\nthat this process works well. And so we can write this\nas kind of evaluating",
    "start": "4200590",
    "end": "4210100"
  },
  {
    "text": "how well this model does\non kind of held out data",
    "start": "4210100",
    "end": "4215350"
  },
  {
    "text": "from the next k time steps. ",
    "start": "4215350",
    "end": "4220450"
  },
  {
    "text": "And so then we'll be\nminimizing this with respect to our initial model parameters. ",
    "start": "4220450",
    "end": "4230730"
  },
  {
    "text": "And then we can look at some\nexamples of actually running this, where we're running\non different terrains",
    "start": "4230730",
    "end": "4237840"
  },
  {
    "text": "or running with\ndifferent dynamics. And first we'll just look at\na very simple baseline, which",
    "start": "4237840",
    "end": "4242910"
  },
  {
    "text": "is only doing model based RL. And what I mean by\nthat is we're going to try to fit a single model to\nall of the different dynamics",
    "start": "4242910",
    "end": "4250110"
  },
  {
    "text": "that the agent is\nencountering during training. And then at test time, we'll\nrun planning with respect",
    "start": "4250110",
    "end": "4255780"
  },
  {
    "text": "to that single model. And if we do that in\nthis first example,",
    "start": "4255780",
    "end": "4263489"
  },
  {
    "text": "one of the joints of the\nrobot becomes disabled. And training a single model\nacross those different dynamics",
    "start": "4263490",
    "end": "4269880"
  },
  {
    "text": "doesn't perform very well. And likewise, in\nthe second example,",
    "start": "4269880",
    "end": "4274980"
  },
  {
    "text": "these different platforms have\ndifferent damping, kind of like a dock.",
    "start": "4274980",
    "end": "4280290"
  },
  {
    "text": "And it isn't able to\nlearn a single model that can model the damping and\nthe dynamics of the robot.",
    "start": "4280290",
    "end": "4286980"
  },
  {
    "text": " Then if you instead run this\nmeta learning approach, where",
    "start": "4286980",
    "end": "4296370"
  },
  {
    "text": "you run MAML, learn the initial\nset of model parameters, with your last k time\nsteps of experience, you adapt your model parameters\nand plan with respect",
    "start": "4296370",
    "end": "4303930"
  },
  {
    "text": "to those adapted parameters. Then you get something that\ncan actually do pretty well.",
    "start": "4303930",
    "end": "4309840"
  },
  {
    "text": "And it's actually--\nyou don't actually see where the adaptation\nis happening because it's happening basically\nin real time,",
    "start": "4309840",
    "end": "4315900"
  },
  {
    "text": "where it's taking the\nlast k time steps, adapting its model with\nthose k time steps, and planning under\nthat adapted model.",
    "start": "4315900",
    "end": "4322233"
  },
  {
    "text": "And it's doing that at\nevery single time step. ",
    "start": "4322233",
    "end": "4329710"
  },
  {
    "text": "Cool, because this is\nnot policy gradients, because it is\nmodel-based RL, it's",
    "start": "4329710",
    "end": "4336790"
  },
  {
    "text": "quite efficient during\nmeta training time. And so this is a\nlearning curve that's showing how fast it's learning\nas a function of the meta",
    "start": "4336790",
    "end": "4344050"
  },
  {
    "text": "training time steps. The yellow curve is showing\nMAML with policy gradients.",
    "start": "4344050",
    "end": "4349840"
  },
  {
    "text": "And the green curve is showing\nMAML with model-based RL.",
    "start": "4349840",
    "end": "4354940"
  },
  {
    "text": "And so this illustrates\nkind of the difference in sample efficiency. And this means you can also\nrun it on like a real robot,",
    "start": "4354940",
    "end": "4362200"
  },
  {
    "text": "for example. So we can collect data\non different terrains. This robot is essentially\nlike hand-built.",
    "start": "4362200",
    "end": "4368500"
  },
  {
    "text": "And the dynamics\nare pretty different for different terrains,\nalso for different robots.",
    "start": "4368500",
    "end": "4376420"
  },
  {
    "text": "And then evaluate on\ndifferent scenarios. So one of the scenarios\nyou can evaluate on is take off the front right\nleg of the robot, which",
    "start": "4376420",
    "end": "4383020"
  },
  {
    "text": "changes the dynamics\npretty significantly. And then compare the behavior\njust with model-based RL.",
    "start": "4383020",
    "end": "4391240"
  },
  {
    "text": "Compared to MAML,\nwith model-based RL, we see that it kind of\nstruggles to walk along the straight line. It falls off to the right.",
    "start": "4391240",
    "end": "4398650"
  },
  {
    "text": "Whereas by adapting\nthe model to the latest dynamics at each time step, it's\nable to more effectively kind",
    "start": "4398650",
    "end": "4407290"
  },
  {
    "text": "of stay on that straight line. ",
    "start": "4407290",
    "end": "4413120"
  },
  {
    "text": "Yeah-- [INAUDIBLE] just learning\nabout the different dynamics,",
    "start": "4413120",
    "end": "4419126"
  },
  {
    "text": "[INAUDIBLE] ",
    "start": "4419126",
    "end": "4425480"
  },
  {
    "text": "Yeah, so it is essentially\nlearning an average dynamics model. And it's also not\nadapting that model.",
    "start": "4425480",
    "end": "4432900"
  },
  {
    "text": "And so you would expect that\nthe average dynamics model isn't going to be very good\nbecause it's trying to cover a pretty broad distribution.",
    "start": "4432900",
    "end": "4439219"
  },
  {
    "text": "And also because\nit's not adapting, it's also not going to be\nable to kind of specialize for any given dynamics.",
    "start": "4439220",
    "end": "4445940"
  },
  {
    "text": "You can also try to learn this\naverage and then adapt it. In general, that also\ndoesn't work well,",
    "start": "4445940",
    "end": "4451310"
  },
  {
    "text": "because it's not\nactually trained such that adapting with k time\nsteps would work well. Typically, you probably\nneed much more data in order",
    "start": "4451310",
    "end": "4458030"
  },
  {
    "text": "to effectively adapt\nor fine tune the model. ",
    "start": "4458030",
    "end": "4467330"
  },
  {
    "text": "So to recap, the two\nkinds of meta RL methods that we talked about, black box\nand optimization-based, black",
    "start": "4467330",
    "end": "4474370"
  },
  {
    "text": "box methods are quite\ngeneral and expressive. There's also a variety of design\nchoices in the architecture",
    "start": "4474370",
    "end": "4481090"
  },
  {
    "text": "and the objective. They tend to be more\ndifficult to optimize.",
    "start": "4481090",
    "end": "4487120"
  },
  {
    "text": "In optimization-based\nmeta RL, you're building in the inductive bias\nof things like policy gradients",
    "start": "4487120",
    "end": "4492429"
  },
  {
    "text": "or gradients with respect to\nyour model, which is nice. And it is relatively easy to\ncombine with policy gradients",
    "start": "4492430",
    "end": "4498070"
  },
  {
    "text": "and model-based methods. That said, policy\ngradients are very noisy. And it's also very\ndifficult to combine this",
    "start": "4498070",
    "end": "4505150"
  },
  {
    "text": "with value-based RL methods. Also one thing that I'll\nnote about the very top point",
    "start": "4505150",
    "end": "4511360"
  },
  {
    "text": "right here about the kind of\nbuilding in this optimization into the model is that,\nunlike in supervised learning,",
    "start": "4511360",
    "end": "4519640"
  },
  {
    "text": "in RL we don't actually have\nas good of optimizers for RL. And so this inductive\nbias becomes",
    "start": "4519640",
    "end": "4525909"
  },
  {
    "text": "less valuable as a result. And so I would say\nthat in general, I think that black box methods\nare generally more popular,",
    "start": "4525910",
    "end": "4533300"
  },
  {
    "text": "especially with model free RL.  And then both these\nmethods are going",
    "start": "4533300",
    "end": "4538570"
  },
  {
    "text": "to inherit the sample efficiency\nof outer RL optimizer. ",
    "start": "4538570",
    "end": "4546320"
  },
  {
    "text": "OK, great, so that's our recap. And that was most of\nthe plan for today.",
    "start": "4546320",
    "end": "4554970"
  },
  {
    "text": "Next time-- so today\nwe covered the basics. Next time, we're going\nto be talking a lot about learning how to explore.",
    "start": "4554970",
    "end": "4560270"
  },
  {
    "text": "Essentially, how do we\nlearn a policy theta such that when we explore\nwith that policy, it gives us a lot of\ninformation about the task?",
    "start": "4560270",
    "end": "4568880"
  },
  {
    "text": "And then a couple\nof other reminders is that homework 3 will\nbe due on Wednesday. And then the optional homework\n4 will go out on Wednesday.",
    "start": "4568880",
    "end": "4575889"
  },
  {
    "start": "4575890",
    "end": "4580000"
  }
]