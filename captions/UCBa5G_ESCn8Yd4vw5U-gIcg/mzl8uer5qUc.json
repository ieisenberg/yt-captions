[
  {
    "start": "0",
    "end": "5870"
  },
  {
    "text": "Awesome, OK. So we're going to jump back into\nwhere we left off last Thursday.",
    "start": "5870",
    "end": "11420"
  },
  {
    "text": "So if you remember,\nwe were trying to draw samples from the\ndistribution over failures. And we said that\nwe knew how to-- we",
    "start": "11420",
    "end": "18980"
  },
  {
    "text": "couldn't compute the normalized\ndensity of the failure distribution, but we could\ncompute the unnormalized density",
    "start": "18980",
    "end": "24830"
  },
  {
    "text": "by computing that numerator. And so we're talking\nabout different ways to then draw samples from\nthis unnormalized density,",
    "start": "24830",
    "end": "30630"
  },
  {
    "text": "so we could represent this\ndistribution implicitly with our samples. And so we talked about\nrejection sampling,",
    "start": "30630",
    "end": "36240"
  },
  {
    "text": "and we talked about\nMarkov Chain Monte Carlo. And we left off talking\nabout Markov Chain Monte",
    "start": "36240",
    "end": "42680"
  },
  {
    "text": "Carlo, and some\nof the challenges that we might end up with when\ndoing Markov Chain Monte Carlo.",
    "start": "42680",
    "end": "49070"
  },
  {
    "text": "And one of these challenges\nwas that everything works with Monte Carlo in the\nlimit of infinite samples.",
    "start": "49070",
    "end": "55110"
  },
  {
    "text": "So in the limit of\ninfinite samples Markov Chain Monte Carlo will\ngive US distribution samples",
    "start": "55110",
    "end": "60990"
  },
  {
    "text": "from the failure distribution. But we were saying we\ndon't have time for this. And so we need to\nfigure out what",
    "start": "60990",
    "end": "66840"
  },
  {
    "text": "to do in practice when\nwe're always going to have some finite set of samples.",
    "start": "66840",
    "end": "72360"
  },
  {
    "text": "And so we talked about some\ndifferent tricks to do. One was adding this\nburn in period, which people sometimes\ndo to just throw out",
    "start": "72360",
    "end": "78660"
  },
  {
    "text": "the first few samples. There was also this thing called\nthinning because the next sample",
    "start": "78660",
    "end": "85290"
  },
  {
    "text": "depends on the previous\nsample, the samples are actually correlated\nwithin the chain. So to get these\nuncorrelated samples,",
    "start": "85290",
    "end": "91335"
  },
  {
    "text": "people will often do this thing\ncalled thinning where they keep every fifth sample, 10th\nsample or something like that.",
    "start": "91335",
    "end": "97395"
  },
  {
    "text": "And then the last thing\nwe were talking about was this idea called smoothing. So that's what\nwe're going to dive",
    "start": "97395",
    "end": "102960"
  },
  {
    "text": "deeply into for the\nfirst part of today. And we had this Pluto notebook\nwhere we were looking at,",
    "start": "102960",
    "end": "108930"
  },
  {
    "text": "where essentially,\nthe idea was that we had now this bimodal\nfailure distribution.",
    "start": "108930",
    "end": "114370"
  },
  {
    "text": "And we saw that when we just\ntried to do this naive Markov Chain Monte Carlo to sample\nfrom this bimodal distribution,",
    "start": "114370",
    "end": "121430"
  },
  {
    "text": "we only got samples from\nwhichever failure mode we started in. And the reason for\nthat was because we",
    "start": "121430",
    "end": "127120"
  },
  {
    "text": "had this proposal\nor not proposal, but this distribution\nwe were using to propose the next sample.",
    "start": "127120",
    "end": "132980"
  },
  {
    "text": "And it was called a kernel, when\nwe were using this distribution, we saw that it assigned\nvery low likelihood,",
    "start": "132980",
    "end": "139250"
  },
  {
    "text": "no matter what we did to\npoints in this failure mode over on the other side.",
    "start": "139250",
    "end": "144828"
  },
  {
    "text": "And so it was very unlikely that\nwe would make that big jump, and actually switch\nover to a failure mode. So in the limit of\ninfinite samples,",
    "start": "144828",
    "end": "151100"
  },
  {
    "text": "this would eventually happen. But because we're\ntaking a finite sample, a lot of times our\nfinite sample is just",
    "start": "151100",
    "end": "156130"
  },
  {
    "text": "going to contain samples\nfrom this failure mode. So we said, let's try\nto help this algorithm",
    "start": "156130",
    "end": "161380"
  },
  {
    "text": "move between these failure modes\nby smoothing out the failure distribution.",
    "start": "161380",
    "end": "167170"
  },
  {
    "text": "And so the way we said\nto do this was we'll define this distance function\nthat we called delta. And we're just saying that this\nfunction should return zero",
    "start": "167170",
    "end": "175360"
  },
  {
    "text": "if the trajectory\nis a failure, and it should be positive otherwise. So it represents a trajectories\ndistance to failure.",
    "start": "175360",
    "end": "182330"
  },
  {
    "text": "And one really\neasy way to do this is to just take the max of\nthe robustness and zero. So the robustness\nwill be positive",
    "start": "182330",
    "end": "188330"
  },
  {
    "text": "when it's not a\nfailure, and then we'll make sure it's zero\nwhen it is a failure.",
    "start": "188330",
    "end": "194000"
  },
  {
    "text": "So then we said what we\ncould do is we could just rewrite our unnormalized density\nfor the failure distribution.",
    "start": "194000",
    "end": "200310"
  },
  {
    "text": "And instead of saying\ntau not in PSI, we can just use this\ncondition, which means the same thing, which is\nthat delta tau is equal to zero.",
    "start": "200310",
    "end": "208310"
  },
  {
    "text": "And so we said, OK,\nnow what we can do is we can say that this\nindicator function is actually",
    "start": "208310",
    "end": "214820"
  },
  {
    "text": "similar to a normal distribution\nwith a very small variance. So here we have this\nindicator function.",
    "start": "214820",
    "end": "221610"
  },
  {
    "text": "It's one at zero,\nwhen delta tau is zero and it's zero everywhere else.",
    "start": "221610",
    "end": "226680"
  },
  {
    "text": "And we can approximate that with\nthis smooth approximation, which is a normal distribution\ncentered at zero",
    "start": "226680",
    "end": "232849"
  },
  {
    "text": "with this small variance. So then what we could\ndo is we could just take our indicator function\nin our unnormalized density,",
    "start": "232850",
    "end": "240360"
  },
  {
    "text": "and replace it with this\nsmooth approximation. And the idea is now we'll\nassign non-zero probabilities",
    "start": "240360",
    "end": "247170"
  },
  {
    "text": "to success trajectories with\nhigher probability for successes that are closer to failure.",
    "start": "247170",
    "end": "255810"
  },
  {
    "text": "And what's cool is we could\nactually now take this epsilon parameter, which is the variance\nof that normal distribution,",
    "start": "255810",
    "end": "261549"
  },
  {
    "text": "we replace it with and use it to\ncontrol how much we're smoothing out this distribution. So when epsilon\nis zero that means",
    "start": "261550",
    "end": "269639"
  },
  {
    "text": "that there is no\nsmoothing, so the variance is zero or just\ncollapses back to this.",
    "start": "269640",
    "end": "274889"
  },
  {
    "text": "And then you can imagine\nwhen epsilon is infinity. If you imagine of\nan infinite variance",
    "start": "274890",
    "end": "280110"
  },
  {
    "text": "for a normal distribution,\nit's infinitely wide. It basically just\nbecomes uniform.",
    "start": "280110",
    "end": "285310"
  },
  {
    "text": "And so this is uniform now, and\nthen our unnormalized density just becomes the failure, the\nnominal trajectory distribution.",
    "start": "285310",
    "end": "294070"
  },
  {
    "text": "And so when epsilon\nis infinity, it approaches the nominal\ntrajectory distribution.",
    "start": "294070",
    "end": "301350"
  },
  {
    "text": "Is there any questions\nup to that point? Yeah. This is more of a--\ncan you say this again?",
    "start": "301350",
    "end": "308190"
  },
  {
    "text": "Can you say a little\nbit more about why",
    "start": "308190",
    "end": "313590"
  },
  {
    "text": "we're modulating p\nbar to be that, why is this the neutral probability?",
    "start": "313590",
    "end": "318870"
  },
  {
    "text": "Why are we-- make this equation? No the one above. This equation?",
    "start": "318870",
    "end": "323920"
  },
  {
    "text": "Yes. This is just the\nunnormalized density of the failure distribution. How do we get to\nthat to the next one?",
    "start": "323920",
    "end": "330340"
  },
  {
    "text": "To this one? Yeah. We just said we defined\nthis distance function in some way, common way to do\nit is max robustness and zero,",
    "start": "330340",
    "end": "336840"
  },
  {
    "text": "such that if that distance\nis zero, tau is a failure. And if it's not, then it'll be\npositive or it'll be a success.",
    "start": "336840",
    "end": "346090"
  },
  {
    "text": "And then basically\nwe can just say then the condition that\ntau is a failure is the same as the condition\nthat distance function evaluates",
    "start": "346090",
    "end": "353060"
  },
  {
    "text": "to zero. So we just replace this. ",
    "start": "353060",
    "end": "358597"
  },
  {
    "text": "What other questions\ndo you have on that?  Yeah.",
    "start": "358597",
    "end": "365070"
  },
  {
    "text": "So the smoothing just lets us\nexpand this distribution more.",
    "start": "365070",
    "end": "373770"
  },
  {
    "text": "I guess now since\nour distribution is larger in a\nsense, is it possible",
    "start": "373770",
    "end": "378960"
  },
  {
    "text": "that there's trajectories\nthat aren't as likely? Yeah, and more\nspecifically we're",
    "start": "378960",
    "end": "386053"
  },
  {
    "text": "now going to have a bunch of\nsamples that are not failures because we just assign some\nnon-zero probability to samples",
    "start": "386053",
    "end": "391170"
  },
  {
    "text": "that are not failures. And our failure\ndistribution should only consist of failures because it's\nthe distribution over failures.",
    "start": "391170",
    "end": "398370"
  },
  {
    "text": "But we can actually solve this\nproblem in that we can just reject all of the samples\nthat are not failures,",
    "start": "398370",
    "end": "404230"
  },
  {
    "text": "don't include them in\nour final set of samples, and we'll actually be left still\nwith samples from the failure",
    "start": "404230",
    "end": "409770"
  },
  {
    "text": "distribution. So this may seem intuitive\nbut it's not necessarily an obvious result to derive.",
    "start": "409770",
    "end": "415622"
  },
  {
    "text": "But one way you can think\nof this, and actually what this is completely\nequivalent to is doing rejection\nsampling, which",
    "start": "415622",
    "end": "421389"
  },
  {
    "text": "is what we did at the\nbeginning of lecture last week with our smooth\ndistribution as the proposal distribution.",
    "start": "421390",
    "end": "427053"
  },
  {
    "text": "And then we just reject all\nthe ones that aren't failures, and then the ones\nthat are left will be from the failure distribution.",
    "start": "427053",
    "end": "433875"
  },
  {
    "text": "So let me show you guys\nwhat this looks like. ",
    "start": "433875",
    "end": "439450"
  },
  {
    "text": "So here we have similar to\nwhat we saw last week, where",
    "start": "439450",
    "end": "444460"
  },
  {
    "text": "we're trying to now sample\nfrom this bimodal failure distribution. And the plot on the\ntop here in red,",
    "start": "444460",
    "end": "450807"
  },
  {
    "text": "we show the true\nfailure distribution that we're trying\nto sample from. And then in this purple\nline, is the smoothed version",
    "start": "450808",
    "end": "457360"
  },
  {
    "text": "with epsilon equal to 0.05. So we have just this tiny bit\nof smoothing that smooths out",
    "start": "457360",
    "end": "463330"
  },
  {
    "text": "this edge here, such that now we\nhave some non-zero probability out here. And then this is still\nsome really small",
    "start": "463330",
    "end": "469660"
  },
  {
    "text": "non-zero probability. Here we're showing\nthe chain of samples. So even still with this\nsmall amount of smoothing,",
    "start": "469660",
    "end": "475940"
  },
  {
    "text": "we didn't manage to jump over\ninto this other failure mode. But then what we do is we\njust do rejection sampling.",
    "start": "475940",
    "end": "481880"
  },
  {
    "text": "So that's what this part is\nshowing on all of those samples. Get rid of all the ones\nthat were not failures,",
    "start": "481880",
    "end": "487725"
  },
  {
    "text": "and then we're left with samples\nfrom the failure distribution. But we still have this\nissue where we never jumped over to the other mode.",
    "start": "487725",
    "end": "494240"
  },
  {
    "text": "And so all we really need to do\nis we smooth it out a tiny bit, but this is still a really big\njump for it to have to make.",
    "start": "494240",
    "end": "501229"
  },
  {
    "text": "And so what we can do is try\nto smooth it out a little more. So let's increase that\nparameter a bunch.",
    "start": "501230",
    "end": "506810"
  },
  {
    "text": "And now we can see this nice\nbig smooth distribution.",
    "start": "506810",
    "end": "511860"
  },
  {
    "text": "And we can see ourselves\njumping back and forth between these failure modes.",
    "start": "511860",
    "end": "518429"
  },
  {
    "text": "And now we have nice samples\nfrom both sides here. The one drawback is we're now\nsampling some non failures.",
    "start": "518429",
    "end": "524970"
  },
  {
    "text": "So we're wasting\nsome of our samples on samples that were\nultimately going to reject, but we needed those to be\nable to efficiently explore",
    "start": "524970",
    "end": "532380"
  },
  {
    "text": "the full space of\npossible failure modes. Yeah, and so have\nsomewhere in between,",
    "start": "532380",
    "end": "540670"
  },
  {
    "text": "I think I want to show you\nguys this to do this one. ",
    "start": "540670",
    "end": "550319"
  },
  {
    "text": "Yeah, so we have something\nsomewhat in between where we're almost jumping\nback and forth enough, but we don't jump back\nand forth quite enough",
    "start": "550320",
    "end": "556680"
  },
  {
    "text": "for our limited sample size. Sometimes we can\nget a few samples from our other failure mode, but\nnot enough to fully even it out.",
    "start": "556680",
    "end": "563153"
  },
  {
    "text": "So this is really\njust a parameter that you have to play around\nwith for your system, yeah.",
    "start": "563153",
    "end": "570510"
  },
  {
    "text": "So right now you're not\nplotting the next step proposal, the kernel or\nwhatever it's called.",
    "start": "570510",
    "end": "576400"
  },
  {
    "text": "It's not visualized? So could you also instead\nof doing that just widen your kernel distribution.",
    "start": "576400",
    "end": "583780"
  },
  {
    "text": "And then it would also manage\nto jump over at some point? Yeah, good question. So the question was,\ncan you just widen",
    "start": "583780",
    "end": "588910"
  },
  {
    "text": "your kernel\ndistribution and then it's more likely to\njump back and forth? You could do that. I think in general it\nwon't be as obvious,",
    "start": "588910",
    "end": "596505"
  },
  {
    "text": "what kernel is going to allow\nyou to jump back and forth. Imagine this is one\ndimensional, but imagine you have hundreds of dimensions.",
    "start": "596505",
    "end": "601940"
  },
  {
    "text": "It's not super obvious how to\njust design a kernel that's going to do that for you. And then also might not\nwant to make your kernel",
    "start": "601940",
    "end": "608890"
  },
  {
    "text": "too big because it might\ndecrease performance in other ways.",
    "start": "608890",
    "end": "614140"
  },
  {
    "text": "Thank you, and then\nanother question. Now let's say, this is what\nI have in my experiment,",
    "start": "614140",
    "end": "619960"
  },
  {
    "text": "is there in general and easy way\nthat I can tell that I actually haven't found a\ngood epsilon yet,",
    "start": "619960",
    "end": "626540"
  },
  {
    "text": "and I should keep looking\nbecause maybe this is what I measure\nand I'm like, oh,",
    "start": "626540",
    "end": "632300"
  },
  {
    "text": "I guess I found two different\nareas, and I modeled them. And then I'm just\ngoing to call it a day.",
    "start": "632300",
    "end": "637490"
  },
  {
    "text": "Can you look at this and\nbe like, oh, obviously I didn't do a good job yet? Yeah, that's a good question. So the question\nwas how do we know",
    "start": "637490",
    "end": "643750"
  },
  {
    "text": "if we found a good\nepsilon, if we don't know what the failure\ndistribution actually looks like? OK, so I'll give\nyou two answers.",
    "start": "643750",
    "end": "649750"
  },
  {
    "text": "The one answer when we do\nresearch papers is we'll run this for just a super long\ntime on the naive rejection",
    "start": "649750",
    "end": "656160"
  },
  {
    "text": "sampling thing, I think\nhas run experiments for 30 days on computer\nto get a baseline, and then you show that\nyour thing does it better",
    "start": "656160",
    "end": "663150"
  },
  {
    "text": "in three hours or whatever. And then you need to--\nin the real world, you're just going\nto have to trust",
    "start": "663150",
    "end": "668370"
  },
  {
    "text": "that your thing is working. I think in general, though,\nif you have more domain knowledge about\nyour system, then",
    "start": "668370",
    "end": "674485"
  },
  {
    "text": "you could understand if it\nseems to be missing things. I don't know is there\nanything else that we can do? Yeah, OK.",
    "start": "674485",
    "end": "681930"
  },
  {
    "text": "Any other questions on this? ",
    "start": "681930",
    "end": "686940"
  },
  {
    "text": "OK, let's go back here.",
    "start": "686940",
    "end": "693930"
  },
  {
    "text": "Yeah, sorry I missed\nthis, but can you describe at a high level\nwhat's the difference. So not at infinity, but\nat this certain fixed",
    "start": "693930",
    "end": "703380"
  },
  {
    "text": "amount of time, what's the\ndifference between something like rejection sampling versus\na Markov Chain Monte Carlo",
    "start": "703380",
    "end": "708990"
  },
  {
    "text": "because both are p weighted now. Both are now weighted. The proposal distribution we\ngive for in rejection sampling",
    "start": "708990",
    "end": "716459"
  },
  {
    "text": "is also ideally weighted\nby what we would guess is a failure distribution. ",
    "start": "716460",
    "end": "723000"
  },
  {
    "text": "In a sense like, does it\nperform better at any time? Does it?",
    "start": "723000",
    "end": "728995"
  },
  {
    "text": "The question is, what\nis the difference between this limited finite\nsample of Monte Carlo, and what is rejection sampling?",
    "start": "728995",
    "end": "734740"
  },
  {
    "text": "And well, there's\na few differences. I mean, one like in\nrejection sampling, your samples are not\ncorrelated with each other.",
    "start": "734740",
    "end": "739960"
  },
  {
    "text": "We're not trying to build\nout this big chain, which is why I said to\nbe careful trying to draw the parallels there.",
    "start": "739960",
    "end": "745889"
  },
  {
    "text": "And then also, I think what can\nbe really tough with rejection sampling is you have to come\nup with that proposal that completely bounds\nyour distribution.",
    "start": "745890",
    "end": "753490"
  },
  {
    "text": "And you have to pick that value\nC that gets that height up. And it's generally just\nvery difficult to do. And with Markov\nChain Monte Carlo,",
    "start": "753490",
    "end": "759510"
  },
  {
    "text": "you don't have to\ndo those things. Yeah, this is typically\nused more often. ",
    "start": "759510",
    "end": "769670"
  },
  {
    "text": "Yeah. Do you still have\nthose challenges when you run rejection sampling\non the smoothing markup?",
    "start": "769670",
    "end": "777890"
  },
  {
    "text": "Yeah good question. So the question was, do you\nstill have this challenge, because now we're\ndoing rejection sampling again right here.",
    "start": "777890",
    "end": "784940"
  },
  {
    "text": "No because you know that\nthe smooth density exactly",
    "start": "784940",
    "end": "790970"
  },
  {
    "text": "matches your\nfailure distribution in places where your\nfailure distribution has non-zero probability. And it just lies above the\nfailure distribution in places",
    "start": "790970",
    "end": "799459"
  },
  {
    "text": "where it has zero probability. So you already know that it's\nover bounding your failure distribution. So you don't have\nto worry about--",
    "start": "799460",
    "end": "805070"
  },
  {
    "text": "like you can set C equal to\n1 and you're perfectly fine. ",
    "start": "805070",
    "end": "815029"
  },
  {
    "text": "OK, cool. And so then the last thing to\nwrap up the failure distribution",
    "start": "815030",
    "end": "820819"
  },
  {
    "text": "is just a few quick tips\nfor scaling things up. So yeah, I wouldn't be\nshowing you these methods",
    "start": "820820",
    "end": "826140"
  },
  {
    "text": "if it wasn't possible\nto apply them to more than just a one\ndimensional Gaussian distribution. And we can in fact scale them up\nto these very high dimensional",
    "start": "826140",
    "end": "834090"
  },
  {
    "text": "problems. So for example, the pendulum we\nknow is around 50 dimensional, and we can still scale it up to\ndraw samples from the failure",
    "start": "834090",
    "end": "841560"
  },
  {
    "text": "distribution of the pendulum. So I'd recommend checking\nout these example problems in the book if you want to see\nexactly how all the code works.",
    "start": "841560",
    "end": "849180"
  },
  {
    "text": "But just to prove to\nyou that it does work, I'll show it to you real quick. So here's actually\ntrying to sample",
    "start": "849180",
    "end": "856800"
  },
  {
    "text": "from the failure\ndistribution of the pendulum, and you can see that we've\napplied some smoothing here.",
    "start": "856800",
    "end": "862510"
  },
  {
    "text": "So in the smooth distribution\nwe get some samples that are not failures. And then here we get we can\nreject those, and get samples",
    "start": "862510",
    "end": "870090"
  },
  {
    "text": "from the failure distribution. And so what's going on\nhere is these samples, they're not just like\n1D prints anymore.",
    "start": "870090",
    "end": "876640"
  },
  {
    "text": "These samples are\nfull trajectories. So they're samples\nof the initial state, and all of the disturbances\nfor those trajectories.",
    "start": "876640",
    "end": "883210"
  },
  {
    "text": "And so every time we\npropose a new one, we propose an entire\nnew trajectory and decide whether to\naccept or reject it.",
    "start": "883210",
    "end": "890668"
  },
  {
    "text": "And then also you might see,\nthis looks a little off balance. It's probably equally\nlikely that the pendulum falls to the right\nand falls to the left.",
    "start": "890668",
    "end": "896990"
  },
  {
    "text": "And so similar to up here where\nwe had this issue, where it wasn't flipping around enough. Maybe what we should do is\nincrease our smoothing parameter",
    "start": "896990",
    "end": "904090"
  },
  {
    "text": "a little bit. It takes a minute to run because\nthis is a bit of a bigger problem, but we should see\nthat this will balance out more",
    "start": "904090",
    "end": "912100"
  },
  {
    "text": "if we smooth it a little more. So we get a few more\nnon-failure trajectories,",
    "start": "912100",
    "end": "917330"
  },
  {
    "text": "but we're able to get\nthis nice representation of the distribution over\nfailures for the pendulum.",
    "start": "917330",
    "end": "923902"
  },
  {
    "text": "And you guys can\ncheck out the code for doing that, it's\nall in the notebook. So if you download\nthe notebook you're welcome to look through it.",
    "start": "923902",
    "end": "930352"
  },
  {
    "text": "I'll be honest, the code for\nthe notebooks I show in class is not the prettiest because\nI've been making them quite quickly before\nlecture, but you're",
    "start": "930353",
    "end": "936730"
  },
  {
    "text": "welcome to check\nout any of the code. OK, so that's one thing. Smoothing helps\nquite a bit to sample",
    "start": "936730",
    "end": "942529"
  },
  {
    "text": "from these failure distributions\nthat have multiple failure modes that are very complex. There's a lot of other\nthings that help.",
    "start": "942530",
    "end": "949170"
  },
  {
    "text": "There's a whole-- you'll\nprobably take a whole class on Markov Chain Monte Carlo. One thing that\nreally tends to help",
    "start": "949170",
    "end": "955040"
  },
  {
    "text": "is to use gradients of the\nlikelihood to define the kernel. So we were just defining\nthis naive Gaussian kernel,",
    "start": "955040",
    "end": "962600"
  },
  {
    "text": "basically makes it, so we do a\nrandom walk through the space that we could be sampling.",
    "start": "962600",
    "end": "968360"
  },
  {
    "text": "But in general,\nwe know that we're going to end up wanting to\nsample from high likelihood regions more often than we want\nto sample from low likelihood",
    "start": "968360",
    "end": "975020"
  },
  {
    "text": "regions. And so there's\nthese methods that end up using gradients,\nand a lot of inspiration from physical systems to random\nwalk, but in a nice direction",
    "start": "975020",
    "end": "984290"
  },
  {
    "text": "towards increasing likelihood. And then every once in\na while you'll decrease. And you can use the\ngradient to guide that walk.",
    "start": "984290",
    "end": "991070"
  },
  {
    "text": "So if you're interested in more\nspecifics on how that works, we talk about it\nin section 3.6.3.3.",
    "start": "991070",
    "end": "997550"
  },
  {
    "text": "There's also one that's very\nfamous called Hamiltonian Monte Carlo or HMC, you\nmay have heard of.",
    "start": "997550",
    "end": "1002600"
  },
  {
    "text": "And then the state of the\nart one that a lot of people use these days is NUTS or the\nNo-U-Turn Sampler, a funny name.",
    "start": "1002600",
    "end": "1011089"
  },
  {
    "text": "And yeah, these are\nall things that you could use to make your\nMCMC a bit more efficient.",
    "start": "1011090",
    "end": "1019190"
  },
  {
    "text": "And then secondly, another thing\nthat can help scale things up is more of just\na paradigm, which is probabilistic programming.",
    "start": "1019190",
    "end": "1025170"
  },
  {
    "text": "So we actually talked\nabout this a little bit when we were doing\nparameter estimation. But it basically allows us to\nspecify probabilistic models",
    "start": "1025170",
    "end": "1031429"
  },
  {
    "text": "as computer programs. And then the computer\nprobabilistic programming language can understand that\nand perform this inference",
    "start": "1031430",
    "end": "1038750"
  },
  {
    "text": "automatically for us. And so I just-- sorry, yeah-- both of\nthese are advanced topics",
    "start": "1038750",
    "end": "1045895"
  },
  {
    "text": "won't be on the\nquiz, but they're super interesting if you want\nto get further down this path. And I just want to\nshow you real quick",
    "start": "1045895",
    "end": "1052100"
  },
  {
    "text": "with probabilistic programming\nbecause it's so cool. Basically we have this\nalgorithm in the book for it.",
    "start": "1052100",
    "end": "1059320"
  },
  {
    "text": "And all of this will look\nprobably very familiar. So we say sample\nfailures, and we make this model for a\nprobabilistic programming",
    "start": "1059320",
    "end": "1066340"
  },
  {
    "text": "language called\nTuring.jl And basically it requires you make\nthis model function.",
    "start": "1066340",
    "end": "1071900"
  },
  {
    "text": "So we just modeled the\nrollout of the system, which is the way that we draw\na sample from our system.",
    "start": "1071900",
    "end": "1078220"
  },
  {
    "text": "And we basically say S is\nsampled from the initial state distribution, and each\nof our disturbances",
    "start": "1078220",
    "end": "1084789"
  },
  {
    "text": "are sampled from the\ndisturbance distribution. And then we just tell it that\nthe density of the distribution",
    "start": "1084790",
    "end": "1091450"
  },
  {
    "text": "we want to sample from is this\nsmoothed density for the failure distribution.",
    "start": "1091450",
    "end": "1097539"
  },
  {
    "text": "And then in turn we'll\njust sample from it for us, which is super cool. So I'm not expecting you to\nunderstand all of this syntax",
    "start": "1097540",
    "end": "1103540"
  },
  {
    "text": "right here, but what\nI hope you take away is, you can very easily\nspecify all of these things,",
    "start": "1103540",
    "end": "1109970"
  },
  {
    "text": "and Turing will just do\nthe inference for you. So if you want to run\nHamiltonian Monte Carlo or NUTS",
    "start": "1109970",
    "end": "1115450"
  },
  {
    "text": "or whatever, Turing\nhas implementations for all of those things,\nand it can draw samples from your failure distribution\nby just running this block",
    "start": "1115450",
    "end": "1122470"
  },
  {
    "text": "of code, which is pretty cool. So yeah, super cool.",
    "start": "1122470",
    "end": "1129130"
  },
  {
    "text": "That's it for this lecture. Are there any questions on that? Yes this is probably\nnot the best question",
    "start": "1129130",
    "end": "1138070"
  },
  {
    "text": "to ask when we're usually\nusing Julia in the course, but are there similar\nthings in Python",
    "start": "1138070",
    "end": "1143500"
  },
  {
    "text": "as well that we can Access? For the probabilistic\nprogramming? I think there's PyMC",
    "start": "1143500",
    "end": "1149350"
  },
  {
    "text": "Pyro? Pyro, it's a cool name.",
    "start": "1149350",
    "end": "1154880"
  },
  {
    "text": "I think you can use\nthem from Python Stan? Yeah, I think that's the most--",
    "start": "1154880",
    "end": "1160570"
  },
  {
    "text": "the OG, but I think it's in\nC++, but you can use it for-- It's harder to integrate\nPython in my experience,",
    "start": "1160570",
    "end": "1167740"
  },
  {
    "text": "where it's harder\nto do this sampling. Of course.",
    "start": "1167740",
    "end": "1173115"
  },
  {
    "text": "Oh yeah, it looks like stan\nasked you to define basically your prototypes in\nthe giant string. ",
    "start": "1173115",
    "end": "1179750"
  },
  {
    "text": "OK, awesome. So that was failure\ndistribution.",
    "start": "1179750",
    "end": "1185390"
  },
  {
    "text": "So going back through the\nsequence of things we did,",
    "start": "1185390",
    "end": "1190460"
  },
  {
    "text": "we started our failure\nanalysis with falsification. We were just trying to\nfind any failures we could. You did that for project 1.",
    "start": "1190460",
    "end": "1196919"
  },
  {
    "text": "Then we talked\nabout how to sample from the full distribution\nover failures, which is a little bit harder.",
    "start": "1196920",
    "end": "1202298"
  },
  {
    "text": "And now we're going to do\nsomething one step further even and try to estimate the\nprobability of failure.",
    "start": "1202298",
    "end": "1209179"
  },
  {
    "text": "In part 1, we're going to do\nthis also in Thursday's lecture. What is the\nprobability of failure?",
    "start": "1209180",
    "end": "1214950"
  },
  {
    "text": "So we're going to talk about\nwhat this means mathematically. Before I get into\nthis lecture, I'm",
    "start": "1214950",
    "end": "1221180"
  },
  {
    "text": "going to give a similar caveat\nthat I gave last lecture, which is the methods I'm going\nto talk about today, I think they took me a few\ntimes through before they fully",
    "start": "1221180",
    "end": "1228860"
  },
  {
    "text": "clicked in my mind. I think especially for\na lot of these methods, there's a lot of\ndifferent angles you could come at for\nunderstanding them,",
    "start": "1228860",
    "end": "1235810"
  },
  {
    "text": "and they're all valid. So I'm going to try to present\nas many different angles as I can today. But maybe after this lecture,\ngo check out the book,",
    "start": "1235810",
    "end": "1243950"
  },
  {
    "text": "maybe come back to the\nlecture, and then I'm always happy to explain\nin a different way on Ed or through office\nhours, if it's not",
    "start": "1243950",
    "end": "1250690"
  },
  {
    "text": "clicking in some way. But OK, so let's talk about\nprobability of failure. So mathematically, the\nprobability of failure",
    "start": "1250690",
    "end": "1257559"
  },
  {
    "text": "is defined as this\nexpectation, which is the expectation\nover trajectories",
    "start": "1257560",
    "end": "1262750"
  },
  {
    "text": "from the nominal\ntrajectory distribution. Basically just whether\nor not there a failure. So if you remember from\nI think it was chapter 3,",
    "start": "1262750",
    "end": "1270260"
  },
  {
    "text": "we were talking about metrics\nthat are either 1 or 0. So this is either 1 or 0. If you take an expectation of\nthem you get a probability.",
    "start": "1270260",
    "end": "1278500"
  },
  {
    "text": "And then furthermore,\nwe can just write out what this\nexpectation is as an integral.",
    "start": "1278500",
    "end": "1284390"
  },
  {
    "text": "And so we could say the\nprobability of failure is this integral over all\npossible trajectories of",
    "start": "1284390",
    "end": "1289870"
  },
  {
    "text": "whether or not they're a failure\nweighted by their likelihood under the nominal distribution.",
    "start": "1289870",
    "end": "1297260"
  },
  {
    "text": "And this might look\nfamiliar to you because from the lecture that\nwe were just talking about,",
    "start": "1297260",
    "end": "1302510"
  },
  {
    "text": "You'll notice that it\nmatches exactly what we had in the denominator of\nthe failure distribution.",
    "start": "1302510",
    "end": "1309290"
  },
  {
    "text": "And so it turns out that\nthe probability of failure is just the normalizing constant\nof the failure distribution.",
    "start": "1309290",
    "end": "1315660"
  },
  {
    "text": "So there's this connection here.  And so it's equal to this area\nunder the unnormalized failure",
    "start": "1315660",
    "end": "1326210"
  },
  {
    "text": "distribution density. And we just finished talking\nabout how this normalizing",
    "start": "1326210",
    "end": "1331640"
  },
  {
    "text": "constant is generally\nintractable to compute. Exactly, and that\nwas why we had to go through that whole business\nof trying to sample",
    "start": "1331640",
    "end": "1338090"
  },
  {
    "text": "from the failure distribution. And so in this\nlecture, we're going to talk about ways that we\ncould actually go ahead, and go",
    "start": "1338090",
    "end": "1343970"
  },
  {
    "text": "about estimating what\nthat denominator is, or in other words, what the\nprobability of failure is.",
    "start": "1343970",
    "end": "1349645"
  },
  {
    "text": " So plan for today,\nwe're going to discuss",
    "start": "1349645",
    "end": "1355110"
  },
  {
    "text": "a few methods to do that. And then we're going to continue\ndiscussing them on Thursday.",
    "start": "1355110",
    "end": "1360492"
  },
  {
    "text": "So we'll start with direct\nestimation, which we've already actually seen a couple\nof times in this class.",
    "start": "1360493",
    "end": "1365730"
  },
  {
    "text": "Then we'll go to an improvement\non direct estimation, which is importance sampling. So this is a super fundamental\nconcept for this class.",
    "start": "1365730",
    "end": "1372670"
  },
  {
    "text": "If you walk away\nwith a few things, I hope that this is one of them. It's super useful\nand really cool.",
    "start": "1372670",
    "end": "1379110"
  },
  {
    "text": "And then we'll talk\nabout some extensions of importance sampling,\nstarting with one called adaptive importance sampling.",
    "start": "1379110",
    "end": "1385500"
  },
  {
    "text": "And that's it for today,\nbut just another note. So I gave you a hint last\ntime when we got to fuzzing,",
    "start": "1385500",
    "end": "1390510"
  },
  {
    "text": "I was like, this is what I\nwould start with for project 1, importance sampling is what I\nwould start with for project 2.",
    "start": "1390510",
    "end": "1395710"
  },
  {
    "text": "So actually if you haven't\nchecked out project 2 yet, no worries. But essentially, what\nwe're going to do is just take all of the systems\nthat we use in project 1",
    "start": "1395710",
    "end": "1402960"
  },
  {
    "text": "to find failures\nof, and we're going to try to estimate the\nprobability of failure of those systems.",
    "start": "1402960",
    "end": "1408480"
  },
  {
    "text": "And you'll need to beat the\nbaseline of direct estimation. And one way to do that would\nbe to use importance sampling,",
    "start": "1408480",
    "end": "1414380"
  },
  {
    "text": "which we'll get into. But let's start with\ndirect estimation.",
    "start": "1414380",
    "end": "1419740"
  },
  {
    "text": "And let's imagine\nthat we're just going to produce samples\nfrom our nominal system.",
    "start": "1419740",
    "end": "1425210"
  },
  {
    "text": "And I'm going to break it\nup into this 2-step process. So there's nothing\ncrazy going on here. To produce a sample\nfrom our nominal system,",
    "start": "1425210",
    "end": "1431713"
  },
  {
    "text": "we're just going to say\nwe're going to perform a rollout of the system. So for example, maybe\nwe perform a rollout",
    "start": "1431713",
    "end": "1436750"
  },
  {
    "text": "of the inverted pendulum. And then the second\nstep is we're just going to label the outcome\nas 1 if the trajectory is",
    "start": "1436750",
    "end": "1443590"
  },
  {
    "text": "a failure and 0 otherwise. So we just check whether\nor not it's a failure. And imagine we do that\na whole bunch of times.",
    "start": "1443590",
    "end": "1449720"
  },
  {
    "text": "And now we have this data\nset of basically 1's and 0's. And it turns out we could\nthink of this process",
    "start": "1449720",
    "end": "1455620"
  },
  {
    "text": "as equivalent to\nproducing samples from a Bernoulli\ndistribution, which is defined by the parameter p fail.",
    "start": "1455620",
    "end": "1461537"
  },
  {
    "text": "So this all sounds really\nfancy, but all that's going on is I'm just saying, we\nhave this parameter p fail. And every time we\nsample our system,",
    "start": "1461537",
    "end": "1467950"
  },
  {
    "text": "we'll get a failure\nwith probability p fail and we'll get not failure with\nprobability 1 minus p fail. And that is just a\nBernoulli distribution",
    "start": "1467950",
    "end": "1474780"
  },
  {
    "text": "with that parameter. But the reason that I'm going\nthrough the trouble of writing",
    "start": "1474780",
    "end": "1479909"
  },
  {
    "text": "everything down like this\nis because we can actually then think of the problem as\njust a parameter estimation",
    "start": "1479910",
    "end": "1485940"
  },
  {
    "text": "problem, which is what we\nwere doing when we were doing the model building or the system\nmodeling chapter in chapter 2.",
    "start": "1485940",
    "end": "1492563"
  },
  {
    "text": "And so we could just use\nthose same exact techniques that we were using in\nchapter 2 to actually go ahead and estimate the\nprobability of failure.",
    "start": "1492563",
    "end": "1499879"
  },
  {
    "text": " And specifically the\ntechniques we talked about",
    "start": "1499880",
    "end": "1504900"
  },
  {
    "text": "for doing that were maximum\nlikelihood estimation, and Bayesian estimation. So we're going to see how\nwe can do both of those,",
    "start": "1504900",
    "end": "1512159"
  },
  {
    "text": "starting with the maximum\nlikelihood estimator. So it turns out that the\nmaximum likelihood estimator",
    "start": "1512160",
    "end": "1518437"
  },
  {
    "text": "for a Bernoulli\ndistribution, assuming we had a data set\nof these values of whether or not it\nfailed given samples",
    "start": "1518437",
    "end": "1524790"
  },
  {
    "text": "from our normal distribution\nis just exactly what you would think it would be. Probably if I came up to you\non the street and I was like,",
    "start": "1524790",
    "end": "1530380"
  },
  {
    "text": "here's a bunch of\ndata, tell me what you think the\nprobability of failure is, this is probably\nwhat you would do. You would just basically\ntake the number of failure",
    "start": "1530380",
    "end": "1537090"
  },
  {
    "text": "trajectories and divide it by\nthe total number of trajectories that I gave you. That's the maximum\nlikelihood estimate.",
    "start": "1537090",
    "end": "1544030"
  },
  {
    "start": "1544030",
    "end": "1599290"
  },
  {
    "text": "A bunch of pfail hats,\nthe expectation of that should be the true\nprobability of failure.",
    "start": "1599290",
    "end": "1606580"
  },
  {
    "text": "The next is consistency,\nwhich basically says an estimator\nis consistent if it converges to the true value in\nthe limit of infinite samples.",
    "start": "1606580",
    "end": "1615700"
  },
  {
    "text": "So in this case, we\nhave the limit of-- as the number of samples\nm approaches infinity,",
    "start": "1615700",
    "end": "1621730"
  },
  {
    "text": "of our estimate should equal\nthe true probability of failure. And then finally,\nthe variance is just",
    "start": "1621730",
    "end": "1627910"
  },
  {
    "text": "a quantity that tells us\nthe spread of the estimates around the true value.",
    "start": "1627910",
    "end": "1634180"
  },
  {
    "text": "So let me try to clear all of\nthis up with an image here. Oh, man I meant to cover these\nup and have you guess them.",
    "start": "1634180",
    "end": "1640850"
  },
  {
    "text": "Well, all right, I guess we're\njust going to go for it anyway. So this estimator\nover here, well,",
    "start": "1640850",
    "end": "1646978"
  },
  {
    "text": "let's start by looking at that. So all these plots, the\ntrue probability of failure is this dashed line\nthat you see here.",
    "start": "1646978",
    "end": "1655400"
  },
  {
    "text": "And so we see that the mean of\nthe estimate is that dark line, and then imagine we\nran a bunch of trials",
    "start": "1655400",
    "end": "1662299"
  },
  {
    "text": "and plotted the spread\nof all those estimates over those trials. That's what the\nshaded region is.",
    "start": "1662300",
    "end": "1668700"
  },
  {
    "text": "And then as we\nmove to the right, this is more samples that we\ngathered to create our estimate.",
    "start": "1668700",
    "end": "1674360"
  },
  {
    "text": "So in this case, the estimate\nover here is not unbiased. So it actually is biased because\nif we look at any point in time,",
    "start": "1674360",
    "end": "1681350"
  },
  {
    "text": "the expectation which is that\nblue line of our estimate",
    "start": "1681350",
    "end": "1686419"
  },
  {
    "text": "is not the probability\nof failure. It's got this bias to be above\nthe true probability of failure.",
    "start": "1686420",
    "end": "1693770"
  },
  {
    "text": "But it is consistent because\nif we look in the limit as we increase our\namount of samples, it's converging to the true\nprobability of failure.",
    "start": "1693770",
    "end": "1701710"
  },
  {
    "text": "What was that? Why is it biased? Why is it biased? So an estimate is unbiased\nif on expectation, it's",
    "start": "1701710",
    "end": "1711700"
  },
  {
    "text": "equal to the\nprobability of failure. If we look here, the\nexpectation is this blue line.",
    "start": "1711700",
    "end": "1717350"
  },
  {
    "text": "So it's expectation-- look for\nexample right at this point here, it's expectation\nis actually",
    "start": "1717350",
    "end": "1722860"
  },
  {
    "text": "above the true\nprobability of failure. And so it's biased. ",
    "start": "1722860",
    "end": "1730030"
  },
  {
    "text": "Because only on one side,\nyeah, that's exactly right. And then if we\nlook over here, we",
    "start": "1730030",
    "end": "1735840"
  },
  {
    "text": "have the opposite\nthing going on. So the expectation\nit actually does",
    "start": "1735840",
    "end": "1741370"
  },
  {
    "text": "equal the probability\nof failure, but it's not really converging\nto the true probability.",
    "start": "1741370",
    "end": "1747260"
  },
  {
    "text": "It's just like the variance is\nsaying wide for this whole time, and so it's actually\nnot consistent.",
    "start": "1747260",
    "end": "1753620"
  },
  {
    "text": "It doesn't converge to the\ntrue value in the limit of infinite samples.",
    "start": "1753620",
    "end": "1758710"
  },
  {
    "text": "And then finally this last one\nis both unbiased and consistent. You can see it's symmetric\naround the probability",
    "start": "1758710",
    "end": "1764030"
  },
  {
    "text": "of failure. That's another way\nto look at bias. And it's converging. You can see the variance\ndecreasing and decreasing",
    "start": "1764030",
    "end": "1770900"
  },
  {
    "text": "and decreasing as\nwe get more samples. And so it is consistent.",
    "start": "1770900",
    "end": "1776780"
  },
  {
    "text": "And then also just to illustrate\nvariance, so we say the sample-- or when we have a\nsmall number of samples",
    "start": "1776780",
    "end": "1783410"
  },
  {
    "text": "here we have a high variance. So there's a lot of\ndifferent estimates we might get if we only ran\na few samples, for example.",
    "start": "1783410",
    "end": "1791120"
  },
  {
    "text": "And then over here, if\nwe run lots of samples, then we expect that\nwe're going to get a pretty good estimate that's\nnear the probability of failure.",
    "start": "1791120",
    "end": "1798830"
  },
  {
    "text": "So ideally we have unbiased\nconsistent and low variance. That's what we want, but\nwe don't always get that.",
    "start": "1798830",
    "end": "1807410"
  },
  {
    "text": "Any questions on these metrics?  Yeah.",
    "start": "1807410",
    "end": "1813370"
  },
  {
    "text": "Does [INAUDIBLE] work\nbetter typically when you have more samples?",
    "start": "1813370",
    "end": "1819717"
  },
  {
    "text": "The question was, an\nestimator should always work better when you\nhave more samples? That would be, I think,\na desirable property.",
    "start": "1819717",
    "end": "1826510"
  },
  {
    "text": "I mean, I would\nlove my estimator to work perfectly\nwith one sample, but I think in general\nthat's a desirable property",
    "start": "1826510",
    "end": "1832020"
  },
  {
    "text": "and a property we often observe. But it's not always true. You could end up with\nan estimator like this",
    "start": "1832020",
    "end": "1837493"
  },
  {
    "text": "that doesn't converge. No matter how many samples you\ngive it, it doesn't get better. But that's not typically\nwhat we're looking for, yeah.",
    "start": "1837493",
    "end": "1845825"
  },
  {
    "text": "Several reinforcement\nlearning plots that Monte Carlo is like\nhigh variance but low bias.",
    "start": "1845826",
    "end": "1852460"
  },
  {
    "text": "Would you agree with that? That's a good question. So the question was\nin reinforcement learning they often\nsay Monte Carlo is",
    "start": "1852460",
    "end": "1858900"
  },
  {
    "text": "high variance but low bias. So we're going to\nsee in a second. Actually that leads me\nto the exact next slide,",
    "start": "1858900",
    "end": "1864940"
  },
  {
    "text": "which is let's look at what\nthese metrics are for this Monte Carlo direct estimator.",
    "start": "1864940",
    "end": "1870419"
  },
  {
    "text": "So it is unbiased. So we do expect it to\non expectation give us the true probability of failure.",
    "start": "1870420",
    "end": "1876809"
  },
  {
    "text": "And then specifically the\nvariance of this estimator, we can actually\ncalculate what it is. So we just take the variance\nof the Bernoulli distribution,",
    "start": "1876810",
    "end": "1883910"
  },
  {
    "text": "divide it by the\nnumber of samples. And so one thing that we can\nnow analyze this equation",
    "start": "1883910",
    "end": "1890590"
  },
  {
    "text": "here to understand\na little bit more about the performance\nof this estimator. And so for example, what happens\nto the variance as m approaches",
    "start": "1890590",
    "end": "1898953"
  },
  {
    "text": "infinity, as the\nnumber of samples we take approaches Infinity?",
    "start": "1898953",
    "end": "1904120"
  },
  {
    "text": "It goes to 0. And so that means\nthat it's consistent because we're going to\nconverge to the true value.",
    "start": "1904120",
    "end": "1910540"
  },
  {
    "text": "We have variance of 0, we'll\nget exactly the true value in the limit of\ninfinite samples, yeah. How do we know that is suffice?",
    "start": "1910540",
    "end": "1917470"
  },
  {
    "text": "I didn't prove it for you,\nbut you can prove that. What's changing in a\nsketch in the sense of--",
    "start": "1917470",
    "end": "1924070"
  },
  {
    "text": "I can see that it makes\nsense, but I can't actually see how it would\ngo about proving.",
    "start": "1924070",
    "end": "1930549"
  },
  {
    "text": "In the beginning, it's like\nit could be very wrong. And then you'll get-- it will get more for\nconsistent, which makes sense.",
    "start": "1930550",
    "end": "1938330"
  },
  {
    "text": "I guess it's\nequally likely to be very wrong in either direction. So on expectation yeah.",
    "start": "1938330",
    "end": "1944155"
  },
  {
    "text": "That's definitely not a proof,\nbut that's some intuition. I think I have a\nlittle note that's like you should make\nan exercise where",
    "start": "1944155",
    "end": "1950120"
  },
  {
    "text": "they prove that this is true. So I might look out\nfor the exercises, and I might have it in there,\nand you can give it a try.",
    "start": "1950120",
    "end": "1956419"
  },
  {
    "text": "Yeah, OK. Any other questions on that.",
    "start": "1956420",
    "end": "1961500"
  },
  {
    "text": "Does that answer the\nquestion you're asking? [INAUDIBLE] Good help you in this\nclass in other classes.",
    "start": "1961500",
    "end": "1968430"
  },
  {
    "text": "It's just the all\nencompassing class. [INAUDIBLE] That\nkeeps on giving.",
    "start": "1968430",
    "end": "1973700"
  },
  {
    "text": "Any other questions on this? ",
    "start": "1973700",
    "end": "1979580"
  },
  {
    "text": "OK, so that's one. OK, and then just to show how\nwe would do this empirically.",
    "start": "1979580",
    "end": "1985140"
  },
  {
    "text": "So in general, we don't actually\nknow the true probability of failure. But what we could\ndo to understand",
    "start": "1985140",
    "end": "1990830"
  },
  {
    "text": "the variance of an\nestimator is we could just imagine that I just did. Well, this is\nexactly what I did.",
    "start": "1990830",
    "end": "1996130"
  },
  {
    "text": "So I did like 10 trials\nwith $5,000 samples each. And then I cut each trial off\nat some value as we go across.",
    "start": "1996130",
    "end": "2005460"
  },
  {
    "text": "And so for example, I say like\nif I only had the first 1,000 samples of those 10 trials,\nwhat's my mean estimate",
    "start": "2005460",
    "end": "2012350"
  },
  {
    "text": "of the probability of failure,\nand then what's the spread of those 10 different estimates? So it does exactly\nwhat you would expect.",
    "start": "2012350",
    "end": "2018720"
  },
  {
    "text": "As we get more samples, we\nget a much smaller spread in those estimates. So that's how you can evaluate\nthese things empirically.",
    "start": "2018720",
    "end": "2028640"
  },
  {
    "text": "So this is of one\nobservation that we can make from this\nvariance, which is that as the number\nof samples increases,",
    "start": "2028640",
    "end": "2034980"
  },
  {
    "text": "the variance in the\nestimate will decrease. So this is intuitive. It's like what we were saying,\nwhen we get more samples,",
    "start": "2034980",
    "end": "2041670"
  },
  {
    "text": "we typically will\nget a better estimate of the probability of failure. And then the second thing that\nI want to observe here though,",
    "start": "2041670",
    "end": "2048780"
  },
  {
    "text": "is that as the\nprobability of failure decreases, so we typically\ndon't know what it is, but we might know the\norder of magnitude",
    "start": "2048780",
    "end": "2054929"
  },
  {
    "text": "of the probability of failure of\nthe system that we're analyzing. The variance in the\nestimate increases.",
    "start": "2054929",
    "end": "2061980"
  },
  {
    "text": "And so this is again bad news\nfor systems with rare failure events.",
    "start": "2061980",
    "end": "2067469"
  },
  {
    "text": "And we're going to discuss\nimportance sampling techniques as I mentioned that will\nhelp us address this issue.",
    "start": "2067469",
    "end": "2075750"
  },
  {
    "text": "So before we get into\nimportance sampling, let's talk about one more\nissue with maximum likelihood",
    "start": "2075750",
    "end": "2081658"
  },
  {
    "text": "estimation. So we'll use a collision\navoidance example. And so you could\nimagine that we want",
    "start": "2081659",
    "end": "2087239"
  },
  {
    "text": "to estimate the\nprobability of failure for an aircraft collision\navoidance system. And we have an aviation\nsafety database",
    "start": "2087239",
    "end": "2094109"
  },
  {
    "text": "that contains flight records\nfrom the month of December. And so then let's say that\nthere were no recorded",
    "start": "2094110",
    "end": "2100500"
  },
  {
    "text": "mid-air collisions in December. What would be the maximum\nlikelihood estimate of the probability of failure?",
    "start": "2100500",
    "end": "2107020"
  },
  {
    "text": "Yeah, Zero? Zero yeah, exactly right. But we're pretty sure it's\nnot zero actually, in fact,",
    "start": "2107020",
    "end": "2112510"
  },
  {
    "text": "very unfortunately, we know from\nlast week that it's not zero. And so we could use\nBayesian estimation instead",
    "start": "2112510",
    "end": "2120680"
  },
  {
    "text": "to produce a distribution over\nthe probability of failure. And so what we would do then\nis we'd say maybe it's zro,",
    "start": "2120680",
    "end": "2128549"
  },
  {
    "text": "but we also have this non-zero\nprobability that the probability of failure is something else.",
    "start": "2128550",
    "end": "2134510"
  },
  {
    "text": "So how do we do that? Well, remember these fun times\nwhere we were flipping frisbees.",
    "start": "2134510",
    "end": "2141380"
  },
  {
    "text": "And so we're going to\ndo basically exactly the same thing. So we said we wanted to\ninfer this distribution",
    "start": "2141380",
    "end": "2147860"
  },
  {
    "text": "over the probability of\nthe probability of failure given our data set.",
    "start": "2147860",
    "end": "2154670"
  },
  {
    "text": "And when we were\nflipping frisbees we said we flipped a\nwhole bunch of frisbees to gather this data set.",
    "start": "2154670",
    "end": "2159990"
  },
  {
    "text": "And we said our theta\nwas the probability of both frisbees landing\nin the same direction.",
    "start": "2159990",
    "end": "2165020"
  },
  {
    "text": "And then we said n was\nthe number of times they landed in the\nsame direction, m was the total number of flips.",
    "start": "2165020",
    "end": "2170910"
  },
  {
    "text": "And we were able to find\nout that our posterior distribution over theta. So this p theta given D\nfollowed this beta distribution.",
    "start": "2170910",
    "end": "2180240"
  },
  {
    "text": "So all we're going\nto do is reformat this for our probability\nof failure estimation, where we're going to say instead\nof collecting a bunch of data",
    "start": "2180240",
    "end": "2188670"
  },
  {
    "text": "by flipping\nfrisbees, we're going to do a bunch of\nrollouts of our system. And so now our theta, instead\nof being the probability",
    "start": "2188670",
    "end": "2196170"
  },
  {
    "text": "of the frisbess landing\nin the same direction, it's just going to be our\nprobability of failure.",
    "start": "2196170",
    "end": "2201990"
  },
  {
    "text": "N will be the number\nof failure trajectories we get in our rollouts. And then m will be the\ntotal number of trajectories",
    "start": "2201990",
    "end": "2209520"
  },
  {
    "text": "that we simulate.  So let's see how that looks.",
    "start": "2209520",
    "end": "2215660"
  },
  {
    "text": " So we can see here\nwe're simulating",
    "start": "2215660",
    "end": "2222720"
  },
  {
    "text": "our inverted pendulum. And here I'm showing the\nmaximum likelihood estimate. So we had eight failures\nand 50 trajectories.",
    "start": "2222720",
    "end": "2229450"
  },
  {
    "text": "So we get that the estimates\naround 0.16, for example.",
    "start": "2229450",
    "end": "2234640"
  },
  {
    "text": "But then, of course, we said\nif we have rare failures this can be an issue. So now let's imagine that we\nhave a very small perception",
    "start": "2234640",
    "end": "2246250"
  },
  {
    "text": "noise, so failures\nbecome more rare. Now we don't observe\nany failures. And we're estimating that the\nprobability of failure is zero.",
    "start": "2246250",
    "end": "2254349"
  },
  {
    "text": "And so what we might\ndo instead then is take a Bayesian\napproach where now we're",
    "start": "2254350",
    "end": "2259930"
  },
  {
    "text": "estimating a distribution over\nthis probability of failure. So here's our distribution over\nthe probability of failure.",
    "start": "2259930",
    "end": "2266420"
  },
  {
    "text": "We see that it's still\nlikely that it could be zero, but there's some\nnon-zero probability that's a little bit higher.",
    "start": "2266420",
    "end": "2272330"
  },
  {
    "text": "The probability of failure is\na little bit higher than zero. And you can imagine\nthat as we simulate",
    "start": "2272330",
    "end": "2278230"
  },
  {
    "text": "more and more trajectories and\nwe don't observe any failures, this distribution gets\nsharper and sharper, which",
    "start": "2278230",
    "end": "2284320"
  },
  {
    "text": "is hard to see with\nthe axes there, but it's basically\ngetting sharper.",
    "start": "2284320",
    "end": "2289530"
  },
  {
    "text": "And one thing that I think\nis really cool about Bayesian estimation is we can now make\nextra claims about our system.",
    "start": "2289530",
    "end": "2295560"
  },
  {
    "text": "So, for example, I could\nsay something like, what is the probability that\nthe probability of a failure",
    "start": "2295560",
    "end": "2301100"
  },
  {
    "text": "is less than 0.01? So let's say we know when\nwe deploy our system, we want to be sure that\nthe probability of failure",
    "start": "2301100",
    "end": "2307099"
  },
  {
    "text": "is less than 1% And so\nwe can actually assign, what is the probability that\nthat's the case by just looking",
    "start": "2307100",
    "end": "2313609"
  },
  {
    "text": "at the probability mass that's\nto the left of 0.01 on our plot here. And specifically\nthat's like equivalent",
    "start": "2313610",
    "end": "2319880"
  },
  {
    "text": "to just calling the CDF\nor Cumulative Distribution Function for our posterior.",
    "start": "2319880",
    "end": "2326138"
  },
  {
    "text": "And so we can just\ncall that function and we'll say the\nprobability that it's less than 0.01 is 0.637 for example.",
    "start": "2326138",
    "end": "2334549"
  },
  {
    "text": "And what that looks like on the\nplot, if we zoom in a little bit here, oops, that's\na little too much.",
    "start": "2334550",
    "end": "2341280"
  },
  {
    "text": "It's the probability mass\nthat's to the left of 0.1 in our distribution.",
    "start": "2341280",
    "end": "2348339"
  },
  {
    "text": "The other thing we could do\nis say, what value are we 95% confident that the\nprobability of failure",
    "start": "2348340",
    "end": "2353910"
  },
  {
    "text": "is less than. So then we need to\nsearch through here and find the point with which\nthere's 95% of the probability",
    "start": "2353910",
    "end": "2360839"
  },
  {
    "text": "mass to the left of it. And then we get 0.02.",
    "start": "2360840",
    "end": "2368830"
  },
  {
    "text": "So we're 95% confident that\nthe probability of failure is less than 0.029, whatever.",
    "start": "2368830",
    "end": "2376020"
  },
  {
    "text": "And that's what it\nlooks like on the plot. So here's the point at which\n95% of the probability mass",
    "start": "2376020",
    "end": "2381539"
  },
  {
    "text": "is to the left of it. But let's say, OK, so we haven't\nfound any probability of failure",
    "start": "2381540",
    "end": "2387120"
  },
  {
    "text": "or we haven't found\nany failures yet, so we think maybe the\nprobability of failure is even lower than this. And we want to prove that\nit's even lower than this.",
    "start": "2387120",
    "end": "2394059"
  },
  {
    "text": "What might we do to\nimprove our estimate?",
    "start": "2394060",
    "end": "2399735"
  },
  {
    "text": "Draw more samples? Yep, draw more samples. That's exactly right. So remember we had\nthis 0.637 and 0.02.",
    "start": "2399735",
    "end": "2407870"
  },
  {
    "text": "So now let's increase the\nnumber of samples to 150. We still don't\nobserve any failures.",
    "start": "2407870",
    "end": "2413450"
  },
  {
    "text": "And so now we're actually 78%\nsure that it's less than 0.01.",
    "start": "2413450",
    "end": "2419089"
  },
  {
    "text": "And we have the smaller\nnumber for 95% confidence one. So I think there\nwas a question that",
    "start": "2419090",
    "end": "2425500"
  },
  {
    "text": "came up last week or the week\nbefore of when do you stop. Or how do if you're\nconfident enough, that thing.",
    "start": "2425500",
    "end": "2430580"
  },
  {
    "text": "So this could potentially\ngive you that information.",
    "start": "2430580",
    "end": "2436300"
  },
  {
    "text": "So the cool thing about Bayesian\nestimation, like I just said, is it gives us a way to quantify\nthe confidence in our estimates,",
    "start": "2436300",
    "end": "2442340"
  },
  {
    "text": "which is a little bit\nmore difficult to do with the maximum likelihood,\nalthough there are ways,",
    "start": "2442340",
    "end": "2448490"
  },
  {
    "text": "and I think I have an\nexercise in the book about how we do that. But yeah, so we can do\nthese very cool things",
    "start": "2448490",
    "end": "2455410"
  },
  {
    "text": "like I showed in the notebook. So that was direct estimation.",
    "start": "2455410",
    "end": "2462610"
  },
  {
    "text": "So again, we have this issue\nwith rare failure events where this isn't\ngoing to work so well.",
    "start": "2462610",
    "end": "2467930"
  },
  {
    "text": "I'm going to use that idea\nto motivate this thing called importance sampling. So let's compare direct\nestimation and importance",
    "start": "2467930",
    "end": "2475640"
  },
  {
    "text": "sampling. So for direct estimation\nwhat we just did was we drew a bunch of samples\nfrom the nominal trajectory",
    "start": "2475640",
    "end": "2482000"
  },
  {
    "text": "distribution. And then we estimated the\nprobability of failure by just dividing the\nnumber of failure samples",
    "start": "2482000",
    "end": "2488330"
  },
  {
    "text": "by the total number of\nsamples that we saw. So what might be a\nproblem with this method?",
    "start": "2488330",
    "end": "2494329"
  },
  {
    "text": "This is a pretty\nrhetorical question because it just keeps coming\nup over and over again in this course. But it's going to be inefficient\nfor systems with rare failure",
    "start": "2494330",
    "end": "2500720"
  },
  {
    "text": "events. So you can imagine we\nwere talking about, a system has a failure\nprobability on the order of 10",
    "start": "2500720",
    "end": "2505760"
  },
  {
    "text": "to the minus 9, we might need\na billion simulations just to observe one failure. But we're not going to feel\nsuper confident in our estimate",
    "start": "2505760",
    "end": "2513033"
  },
  {
    "text": "of the probability\nof failure after just observing one failure. And so we're probably\ngoing to need even more than a\nbillion simulations",
    "start": "2513033",
    "end": "2519290"
  },
  {
    "text": "to actually estimate the\nprobability of failure. And this can be\nextremely inefficient.",
    "start": "2519290",
    "end": "2524369"
  },
  {
    "text": "And so that motivates the\nneed for another method, which is called importance sampling.",
    "start": "2524370",
    "end": "2529570"
  },
  {
    "text": "And the key difference here is\nthat instead of drawing samples from the nominal\ntrajectory distribution,",
    "start": "2529570",
    "end": "2535359"
  },
  {
    "text": "we're going to draw samples\nfrom this other distribution that we call a\nproposal distribution. So you can make this\nconnection here.",
    "start": "2535360",
    "end": "2540940"
  },
  {
    "text": "This is similar to the\nproposal distribution for rejection sampling, that's\nmore likely to produce failure",
    "start": "2540940",
    "end": "2547170"
  },
  {
    "text": "trajectories. And then so now we're actually\nprobably hopefully going to observe some\nfailures, which was",
    "start": "2547170",
    "end": "2553620"
  },
  {
    "text": "our issue that we were having\nwith these rare failure events. But if we were to just\ntake the average of that",
    "start": "2553620",
    "end": "2559200"
  },
  {
    "text": "divide the number\nof failures we saw by number of trajectories\nsimulated, that's going to give us a\ncompletely wrong estimate of the probability of failure\nbecause what's actually",
    "start": "2559200",
    "end": "2566309"
  },
  {
    "text": "going to happen\nin the real world is we're going to\nsee trajectories from this nominal\ntrajectory distribution. But it turns out that\nit's possible to still get",
    "start": "2566310",
    "end": "2573480"
  },
  {
    "text": "an estimate of the\nprobability of failure using samples from\nthis other distribution by reweighting them.",
    "start": "2573480",
    "end": "2580320"
  },
  {
    "text": "So let me show you\nwhat that looks like. OK, so the first\nthing that we're",
    "start": "2580320",
    "end": "2587710"
  },
  {
    "text": "going to do\nfollowing these steps is we're going to draw\nsamples from a proposal distribution that's more likely\nto produce failure trajectories.",
    "start": "2587710",
    "end": "2594619"
  },
  {
    "text": "So you might have some ideas\nof how you might get that, but we're not going to talk\nabout how we get it for now. But let's just say we have it,\nand we're going to call it Q",
    "start": "2594620",
    "end": "2602260"
  },
  {
    "text": "So we draw samples from\nthis proposal distribution, and now we want to estimate\nthe probability of failure.",
    "start": "2602260",
    "end": "2608602"
  },
  {
    "text": "So here's what we're\ngoing to start with. We're going to start with just\nthe definition I showed earlier for the probability of failure.",
    "start": "2608602",
    "end": "2614420"
  },
  {
    "text": "So it's equal to the expectation\nwhen we draw a trajectories from our nominal distribution\nof that indicator function of",
    "start": "2614420",
    "end": "2621820"
  },
  {
    "text": "whether or not\nthey're a failure. ",
    "start": "2621820",
    "end": "2627880"
  },
  {
    "text": "And then we also showed we could\nwrite this out as an integral, where we take the\nnominal density",
    "start": "2627880",
    "end": "2634030"
  },
  {
    "text": "and multiply it by this\nindicator function. ",
    "start": "2634030",
    "end": "2641400"
  },
  {
    "text": "And then now I'm\ngoing to do something that might seem a little\nbit weird at first, but I'm going to multiply\nthis equation by 1.",
    "start": "2641400",
    "end": "2650302"
  },
  {
    "text": "But the version of one\nthat I'm going to choose is q of tau over q of tau. So I'm essentially just\nmultiplying things by one.",
    "start": "2650302",
    "end": "2658570"
  },
  {
    "text": "But I've introduced q of\ntau into our formula now. And then we'll just write\neverything else as it was.",
    "start": "2658570",
    "end": "2666065"
  },
  {
    "text": " And then now all I'm going to\ndo is just switch the location",
    "start": "2666065",
    "end": "2673650"
  },
  {
    "text": "of these, not those of these.  Just switch the location of\nthe things that were there,",
    "start": "2673650",
    "end": "2680740"
  },
  {
    "text": "just to make it easier to\nsee what's about to happen. So this is still valid. All I did was switch\nthe ordering of things.",
    "start": "2680740",
    "end": "2687109"
  },
  {
    "start": "2687110",
    "end": "2695010"
  },
  {
    "text": "And so this might look a\nlittle bit like this equation where the expectation we\ntake the density that we're",
    "start": "2695010",
    "end": "2701890"
  },
  {
    "text": "sampling it from, multiply\nit by the thing that's inside the expectation. So this is actually\nequivalent to the expectation",
    "start": "2701890",
    "end": "2709250"
  },
  {
    "text": "if we were to draw samples from\nQ of tau of what's in here.",
    "start": "2709250",
    "end": "2716680"
  },
  {
    "text": "So of p of tau over q of\ntau times this indicator",
    "start": "2716680",
    "end": "2727480"
  },
  {
    "text": "OK. This is one of the most-- ",
    "start": "2727480",
    "end": "2735610"
  },
  {
    "text": "OK, does that help someone. Somewhat. Yeah, OK. I'm also going to\nafter this show",
    "start": "2735610",
    "end": "2740920"
  },
  {
    "text": "not handwritten version of it. So that should help as well. So yeah, I just derived\nthe fundamental idea",
    "start": "2740920",
    "end": "2750280"
  },
  {
    "text": "behind importance sampling. So all we did it might\nlook a little weird, but we just multiplied by 1\nand rearranged some stuff.",
    "start": "2750280",
    "end": "2755570"
  },
  {
    "text": "And we found out that we\ncould take an expectation over this other\ndistribution instead of the nominal\ndistribution that we want",
    "start": "2755570",
    "end": "2761810"
  },
  {
    "text": "that we were sampling from. And then to compute\nan expectation, you can just take an average\nof a bunch of samples.",
    "start": "2761810",
    "end": "2768560"
  },
  {
    "text": "And so you could imagine\nwhat we might want to do then is our estimate for pfail. So we'll call that p hat fail.",
    "start": "2768560",
    "end": "2777140"
  },
  {
    "text": "And then just going to\nbe equal to an estimate of this expectation.",
    "start": "2777140",
    "end": "2783576"
  },
  {
    "start": "2783576",
    "end": "2797930"
  },
  {
    "text": "And then I'm just\ngoing to rewrite this. ",
    "start": "2797930",
    "end": "2805369"
  },
  {
    "text": "I'm going to call\nthis part Wi just to make it look more similar\nto what we had before.",
    "start": "2805370",
    "end": "2812660"
  },
  {
    "text": "And this Wi is just equal\nto our p tau I over Q tau I.",
    "start": "2812660",
    "end": "2819660"
  },
  {
    "text": "So basically we just said we\ncan take this expectation here and we can estimate it from a\nbunch of samples from Q of tau",
    "start": "2819660",
    "end": "2826440"
  },
  {
    "text": "by just computing\nthat average there. Yeah. [INAUDIBLE]",
    "start": "2826440",
    "end": "2832080"
  },
  {
    "text": "Can you tell what is the\nnominal trajectory distribution, and q of tau is the\nproposal distribution that we're sampling\ntrajectories from instead.",
    "start": "2832080",
    "end": "2839610"
  },
  {
    "text": "Yeah So tau I did [INAUDIBLE]\nit sounds good doing that.",
    "start": "2839610",
    "end": "2847170"
  },
  {
    "text": "Thanks. ",
    "start": "2847170",
    "end": "2857220"
  },
  {
    "text": "OK, so let's look at this\nin slightly neater writing. ",
    "start": "2857220",
    "end": "2867029"
  },
  {
    "text": "So we said we're going to draw\ndistributions from this proposal",
    "start": "2867030",
    "end": "2872460"
  },
  {
    "text": "that we call Q. We\nderived this expectation. And we said one way we can\nestimate this expectation given",
    "start": "2872460",
    "end": "2878530"
  },
  {
    "text": "samples from Q is to just\ntake an average of this value for all of our samples from\nQ. And then we just replace",
    "start": "2878530",
    "end": "2886270"
  },
  {
    "text": "this p over P over Q with this\nW. And the reason for that-- and then by the way, what's\nsuper cool about this",
    "start": "2886270",
    "end": "2893200"
  },
  {
    "text": "is that this\nestimator is actually both an unbiased and\nconsistent estimator of the true\nprobability of failure,",
    "start": "2893200",
    "end": "2899333"
  },
  {
    "text": "even though we didn't\ntake distributions from the true\nnominal distribution. So that's super cool.",
    "start": "2899333",
    "end": "2905360"
  },
  {
    "text": "That's the huge insight\nbehind importance sampling. And just to make a\nfurther comparison",
    "start": "2905360",
    "end": "2911950"
  },
  {
    "text": "to show you that there's not\ntoo much craziness going on here for direct estimation,\nwe drew samples from P.",
    "start": "2911950",
    "end": "2917390"
  },
  {
    "text": "We drew samples from the\nnominal distribution, and then we just estimated\npfail by taking this sum here.",
    "start": "2917390",
    "end": "2923980"
  },
  {
    "text": "And then for\nimportance sampling, we draw samples from a\ndifferent distribution that we call Q. It's our\nproposal distribution.",
    "start": "2923980",
    "end": "2930380"
  },
  {
    "text": "And we just estimate the\nprobability of failure as a weighted sum of whether\nor not it was a failure.",
    "start": "2930380",
    "end": "2937380"
  },
  {
    "text": "So basically what we're\nsaying is like, we can't just treat all these\nsamples as the same because they didn't come\nfrom our true distribution.",
    "start": "2937380",
    "end": "2944310"
  },
  {
    "text": "We need to reweight\nthem according to what they would look like\nin our true distribution, in order to get an accurate\nestimate for the probability",
    "start": "2944310",
    "end": "2952220"
  },
  {
    "text": "of failure. ",
    "start": "2952220",
    "end": "2958490"
  },
  {
    "text": "Yeah.  You're on it today. ",
    "start": "2958490",
    "end": "2969260"
  },
  {
    "text": "Or I could put it over here. I just meant to\nwrite m times pfail.",
    "start": "2969260",
    "end": "2975290"
  },
  {
    "text": "OK, anyway.  Also, there's no pfail\nover here, p hat.",
    "start": "2975290",
    "end": "2982762"
  },
  {
    "text": "All right, I'm going to\nstop trying to do this. All right, here we go. Any questions up to this point?",
    "start": "2982762",
    "end": "2989060"
  },
  {
    "text": "Or other errors in my slides. Oh, a lot of hands went up\nafter I said that, yeah.",
    "start": "2989060",
    "end": "2995670"
  },
  {
    "text": "In reality, so much of\nyour quality and ability when you have your choice\nof nominal trajectory.",
    "start": "2995670",
    "end": "3004340"
  },
  {
    "text": "And so if I were to the-- causal example of people\ndon't run into cars with what probability zero?",
    "start": "3004340",
    "end": "3011930"
  },
  {
    "text": "Because I don't\nthink that's true. I've been into New\nYork City at 2:00 AM. [LAUGHTER]",
    "start": "3011930",
    "end": "3017240"
  },
  {
    "text": "So how do you really estimate\nwhat we talked about here,",
    "start": "3017240",
    "end": "3022860"
  },
  {
    "text": "about what it's like\nif you're experienced, if you're at the other-- if\nyour traffic is an experience",
    "start": "3022860",
    "end": "3031760"
  },
  {
    "text": "or experience or in\nVenezuela or America. The ground reality of\nthe nominal distribution",
    "start": "3031760",
    "end": "3037910"
  },
  {
    "text": "is much different. So like, it's never\ngoing to be true--",
    "start": "3037910",
    "end": "3045330"
  },
  {
    "text": "Yeah -- in some real course. Yeah. --fitting of what\nhappens in data.",
    "start": "3045330",
    "end": "3051730"
  },
  {
    "text": "Yeah, that's a good question. So the question is\nyou're modeling p, and then you're\ndoing all this stuff",
    "start": "3051730",
    "end": "3057090"
  },
  {
    "text": "to get an accurate estimate. But if p is not accurate, then\nthe whole thing is not accurate. And it's probably hard\nto get an accurate p.",
    "start": "3057090",
    "end": "3062700"
  },
  {
    "text": "Totally true. So that's why modeling\nis so important, that'd be my first answer.",
    "start": "3062700",
    "end": "3068820"
  },
  {
    "text": "There are a few\nthings you could do. So you could do a\nsensitivity analysis. So you could say\nif I got p wrong, and I got this amount wrong,\ndoes my probability of failure",
    "start": "3068820",
    "end": "3077160"
  },
  {
    "text": "change a lot? And if it doesn't,\nthen you can be, OK, well, maybe I'm still\ncomfortable with these results. If it does then you better\ndouble down and figure out p.",
    "start": "3077160",
    "end": "3084960"
  },
  {
    "text": "Maybe I'll let Michael\nspeak to this more, but I know for collision\navoidance what they did is they had nine months of\nradar data from the US airspace",
    "start": "3084960",
    "end": "3092339"
  },
  {
    "text": "that they could use to model\nthe behavior of aircraft. And so that allows you to\nget a pretty accurate model",
    "start": "3092340",
    "end": "3098640"
  },
  {
    "text": "in that case specifically, yeah. So those are some failures. So it doesn't actually encourage\nus to see more failures at all?",
    "start": "3098640",
    "end": "3107580"
  },
  {
    "text": "In terms of sampling from Q? I did not-- Well, in either case. In either direct estimation\nor importance sampling,",
    "start": "3107580",
    "end": "3113010"
  },
  {
    "text": "we actually have to see some\nfigures for either distribution that either values\nthe distributions. Either values\ndidn't do anything.",
    "start": "3113010",
    "end": "3120230"
  },
  {
    "text": "So when we do direct\nestimation, yeah, we need to see some\nfailures from p. Otherwise, we're just going\nto get an estimate of zero.",
    "start": "3120230",
    "end": "3127250"
  },
  {
    "text": "When we do this\nestimation, we need to see failures from Q. We're\nnever going to sample from P in importance sampling.",
    "start": "3127250",
    "end": "3133710"
  },
  {
    "text": "But we still have to\nsee some coverage. Yeah, you still have to\nsee some failures for Q, but you are going to\npick Q in a way, yeah,",
    "start": "3133710",
    "end": "3139730"
  },
  {
    "text": "and we're going to\ntalk pretty much for the rest of this lecture\nand next lecture about ways to pick Q. Yeah.",
    "start": "3139730",
    "end": "3146270"
  },
  {
    "text": "So you just said maybe you\nget P a little bit wrong. Can you again be like\nBayesian about this, and have some Bayesian,\nand then you do this",
    "start": "3146270",
    "end": "3153710"
  },
  {
    "text": "and you get again something\nlike a beta distribution or something like that. Is that what do people do that?",
    "start": "3153710",
    "end": "3160400"
  },
  {
    "text": "It's like, if we increase\nthe number of flight hours",
    "start": "3160400",
    "end": "3166880"
  },
  {
    "text": "that a pilot has, then I would\nexpect that you would get more safer pilot, versus if you just\nhave the statutory 50, which",
    "start": "3166880",
    "end": "3175349"
  },
  {
    "text": "is why no one gets their private\nlessons with just 50 hours, even though the statutory\nprogram is legal.",
    "start": "3175350",
    "end": "3180930"
  },
  {
    "text": "And so if I'm\nmodeling it now, well, my p failure given that other\npilots all have at least 2000",
    "start": "3180930",
    "end": "3187559"
  },
  {
    "text": "mile, 3,000 hours, well,\nthat's going to be a much safer independent of anything else.",
    "start": "3187560",
    "end": "3192880"
  },
  {
    "text": "So the best way to improve\nmy PAF or reduce my PAF fail is to make my nominal\ntrajectory safer.",
    "start": "3192880",
    "end": "3200790"
  },
  {
    "text": "Oh, sure, yeah, OK. So I think there's a lot to\nunpack there, but so one, we at-- if you want to actually\nchange the pfail of your system,",
    "start": "3200790",
    "end": "3207910"
  },
  {
    "text": "you might need to consider\ndesign considerations. But that's outside of the scope\nof this class, particularly.",
    "start": "3207910",
    "end": "3214980"
  },
  {
    "text": "I think what you're\ndescribing where a pilot that has\nmore flight hours is maybe less likely to\nbehave in a certain way",
    "start": "3214980",
    "end": "3220980"
  },
  {
    "text": "than a pilot that\nhas less training. That's getting into\nthis conditional idea.",
    "start": "3220980",
    "end": "3226572"
  },
  {
    "text": "So you might want to say what\nis the probability of failure for the inexperienced\npilot versus other. And that will change\nhow you model p.",
    "start": "3226572",
    "end": "3234010"
  },
  {
    "text": "I guess, when you gather\nnine months of radar data from the airspace,\nthe assumption is you're getting some data from\npilots that are experienced,",
    "start": "3234010",
    "end": "3240920"
  },
  {
    "text": "some that are inexperienced\nand in the frequency that you would expect to observe\nin normal operation of the airspace.",
    "start": "3240920",
    "end": "3246680"
  },
  {
    "text": "So you are getting both. And then for the\nBayesian question yeah, you could do that,\nwe're not going",
    "start": "3246680",
    "end": "3252580"
  },
  {
    "text": "to talk about it too much yet.  Other questions here?",
    "start": "3252580",
    "end": "3258910"
  },
  {
    "text": "Yeah. Why is the nominal\nover the proposal one?",
    "start": "3258910",
    "end": "3266080"
  },
  {
    "text": "Why is-- back here? Yeah. Which step are we looking at?",
    "start": "3266080",
    "end": "3271880"
  },
  {
    "text": "So I think the third step. I can't see. ",
    "start": "3271880",
    "end": "3277030"
  },
  {
    "text": "Here? Switch it to-- Yeah, So I just switched\nwhere P and Q are.",
    "start": "3277030",
    "end": "3285460"
  },
  {
    "text": "Yeah, just to make it\nlook more an expectation for the next step. ",
    "start": "3285460",
    "end": "3292940"
  },
  {
    "text": "OK, any other questions on this? Yeah. So just to be clear, we are\nassuming that we know both P",
    "start": "3292940",
    "end": "3299480"
  },
  {
    "text": "and Q, is that right? We are assuming that we\nknow both P and Q. Yes,",
    "start": "3299480",
    "end": "3305150"
  },
  {
    "text": "but we only draw samples from Q.",
    "start": "3305150",
    "end": "3311839"
  },
  {
    "text": "So oftentimes this weight here\nis called an importance weight. And the idea is\nthat samples that",
    "start": "3311840",
    "end": "3318320"
  },
  {
    "text": "are more likely under the\nnominal trajectory distribution, so this P tau I will have\na higher importance weight.",
    "start": "3318320",
    "end": "3324460"
  },
  {
    "text": " So here's just what the\nalgorithm looks like in code.",
    "start": "3324460",
    "end": "3329483"
  },
  {
    "text": "I'm going to go\nthrough this fast, but we draw samples\nfrom Q now instead of P.",
    "start": "3329483",
    "end": "3334940"
  },
  {
    "text": "Then we just evaluate their\nPDFs to compute these importance weights. And then we return this is\njust a code implementation",
    "start": "3334940",
    "end": "3343670"
  },
  {
    "text": "of this expression here. ",
    "start": "3343670",
    "end": "3349140"
  },
  {
    "text": "Let's look at what this\nactually looks like. So here's a cool\ngraphic of this.",
    "start": "3349140",
    "end": "3358410"
  },
  {
    "text": "So here's what's going on. There's a lot going\non in this graphic. But let's look at just the\ndirect estimation first.",
    "start": "3358410",
    "end": "3365460"
  },
  {
    "text": "So here we're doing exactly\nwhat we were saying. So we were trying to estimate\nthe probability of failure for this simple Gaussian system\nwhere it's a failure if it's",
    "start": "3365460",
    "end": "3372930"
  },
  {
    "text": "in this red region. So below some threshold. So what we could do is just\ndraw a bunch of samples from this distribution, take\nall the ones that are failures,",
    "start": "3372930",
    "end": "3381430"
  },
  {
    "text": "divide that by the\ntotal number of samples, and we get an estimate for\nthe probability of failure. So this is our\ndirect estimation.",
    "start": "3381430",
    "end": "3387640"
  },
  {
    "text": "And what I'm showing\nin this plot here is the number of\nsamples on the x-axis. And then the y-axis\nis the absolute value",
    "start": "3387640",
    "end": "3395310"
  },
  {
    "text": "of the estimation error. So because this is a small\nGaussian system we actually know the exact probability of\nfailure, typically we don't.",
    "start": "3395310",
    "end": "3401380"
  },
  {
    "text": "So I can tell you\nwhat the exact error in our estimate of the\nprobability of failure is. And then I run this\n10 different times.",
    "start": "3401380",
    "end": "3409039"
  },
  {
    "text": "I plot the mean error and\nthe probability of failure, and then the spread\nof those errors.",
    "start": "3409040",
    "end": "3414528"
  },
  {
    "text": "So ideally what we want\nto see in this plot is a low spread where every time\nit seems to work pretty well,",
    "start": "3414528",
    "end": "3419810"
  },
  {
    "text": "and then we want to\nhave low values such that our estimate of the\nprobability of failure has a low amount of error.",
    "start": "3419810",
    "end": "3426050"
  },
  {
    "text": "So ideally we want to\nbe down here at zero. So you can see the\ndirect estimate.",
    "start": "3426050",
    "end": "3431119"
  },
  {
    "text": "It's got this pretty big spread. It's got some error. And what we want to do\nwith importance sampling is given the same\nnumber of samples.",
    "start": "3431120",
    "end": "3437900"
  },
  {
    "text": "So we drew 500 samples\nto get all the way to this point at the end. We want to see if we can\nget a better estimate",
    "start": "3437900",
    "end": "3443830"
  },
  {
    "text": "of the probability of failure. And specifically we want to\nsample more failures to do that.",
    "start": "3443830",
    "end": "3449980"
  },
  {
    "text": "The more failures we sample\nwith a few other conditions, the better estimate\nthat we're going to get.",
    "start": "3449980",
    "end": "3455980"
  },
  {
    "text": "So now let's imagine that\nwe-- so here over this side, I'm going to be showing\nthe importance sampling distribution.",
    "start": "3455980",
    "end": "3461030"
  },
  {
    "text": "So right now exactly matches\nthe nominal distribution. So it's the same thing\nas direct estimation.",
    "start": "3461030",
    "end": "3466151"
  },
  {
    "text": "But now let's try to get\nan importance sampling distribution that will\nsample failures more often. So to do that we can move\nthe mean of this distribution",
    "start": "3466152",
    "end": "3474260"
  },
  {
    "text": "to the left here. And you can see already\nwe're doing a lot better. So this blue line here shows the\nerror of our importance sampling",
    "start": "3474260",
    "end": "3483380"
  },
  {
    "text": "estimate and the spread. So you can see that the\nvariance is much lower and the error is much lower.",
    "start": "3483380",
    "end": "3488700"
  },
  {
    "text": "So we've done much\nbetter just by biasing the distribution we sample to\nbe able to sample more failures.",
    "start": "3488700",
    "end": "3497090"
  },
  {
    "text": "There's another way to look\nat this that people often look at called\neffective sample size. If this doesn't resonate with\nyou or doesn't make any sense,",
    "start": "3497090",
    "end": "3504420"
  },
  {
    "text": "you can forget about it. But I think it's one useful\nway to look at things. And the idea is the\neffective sample size--",
    "start": "3504420",
    "end": "3511170"
  },
  {
    "text": "so when you're doing\ndirect estimation, the effective sample size is\njust the number of failures that you observe.",
    "start": "3511170",
    "end": "3516900"
  },
  {
    "text": "So here I think it's like\n13 or something like that. So it's this gray region here.",
    "start": "3516900",
    "end": "3522480"
  },
  {
    "text": "Now here we've moved\nour distribution over, and we've observed actually\nthis many failures.",
    "start": "3522480",
    "end": "3528220"
  },
  {
    "text": "So what this line shows here. But that's not our\neffective sample size because we observed\nthese failures,",
    "start": "3528220",
    "end": "3534010"
  },
  {
    "text": "but we observed them under\na different distribution. So they're not necessarily\nweighted the correct way under the distribution\nwe expect to see.",
    "start": "3534010",
    "end": "3541090"
  },
  {
    "text": "So our effective\nsample size is actually lower than the\nnumber of failures that we observed because\nthey're not all equally weighted",
    "start": "3541090",
    "end": "3550320"
  },
  {
    "text": "according to this nominal\ndistribution we expect to see. Yeah.",
    "start": "3550320",
    "end": "3556410"
  },
  {
    "text": "But all of these\nestimation schemes gives us a single value, right?",
    "start": "3556410",
    "end": "3566200"
  },
  {
    "text": "Yes. How do we get a\ndistribution out of that? I did it 10 times. Oh, cool. So the idea is\neventually we'll do",
    "start": "3566200",
    "end": "3573600"
  },
  {
    "text": "it like once if we\nhave good confidence that it'll always give\nus a good estimate. But clearly if I\ndid it 10 times here",
    "start": "3573600",
    "end": "3579350"
  },
  {
    "text": "and I got this whole spread\nof different estimates, we can't be so sure that when\nwe do it on the real thing,",
    "start": "3579350",
    "end": "3584400"
  },
  {
    "text": "that we're going to\nget something good. So that's why we want this\nlow variance because we want to when we\neventually do it, we're going to\nwant to do it once,",
    "start": "3584400",
    "end": "3590550"
  },
  {
    "text": "and we're going to want to trust\nthat value that we get out. But you did it 10 times? That means that there\nwas $5,000 samples now?",
    "start": "3590550",
    "end": "3598010"
  },
  {
    "text": "So over here, 500\nover here like 100. Yeah various number of samples\nfor that 10 all those 10 times.",
    "start": "3598010",
    "end": "3604490"
  },
  {
    "text": "And that 10 times? Yes 100 samples 10 times 500\nsamples 10 times, and so on,",
    "start": "3604490",
    "end": "3611420"
  },
  {
    "text": "yeah. And so here that's\nbeing plotted is X.",
    "start": "3611420",
    "end": "3616589"
  },
  {
    "text": "I think I actually did 5% to\n95% confidence or something like that.",
    "start": "3616590",
    "end": "3621997"
  },
  {
    "text": "Yeah, I'd have to\nlook at the code. ",
    "start": "3621997",
    "end": "3628250"
  },
  {
    "text": "So the idea here is we want\nto make our effective sample size as high as possible.",
    "start": "3628250",
    "end": "3633329"
  },
  {
    "text": "We want to get as many\nfailures as we can. And so to do that we want\nto sample more failures.",
    "start": "3633330",
    "end": "3639550"
  },
  {
    "text": "So let's see what happens if we\nkeep decreasing this mean here.",
    "start": "3639550",
    "end": "3645430"
  },
  {
    "text": "So now we were centered\nat this failure threshold. So we're sampling\na lot of failures.",
    "start": "3645430",
    "end": "3651170"
  },
  {
    "text": "Our effective sample size\nhas gone up a little bit. We're getting a\nlittle bit better. Maybe you could imagine we\nalso decrease the variance",
    "start": "3651170",
    "end": "3658120"
  },
  {
    "text": "here so that we sample more\nthings that are likely failures.",
    "start": "3658120",
    "end": "3663790"
  },
  {
    "text": "And now we're doing super, super\nwell because we're sampling a lot of likely failures.",
    "start": "3663790",
    "end": "3668839"
  },
  {
    "text": "Our effective sample\nsize is pretty high. But this can go wrong. So you could imagine\nif we did this,",
    "start": "3668840",
    "end": "3675940"
  },
  {
    "text": "and we weren't sampling\nmany failures at all, this is a really bad\nproposal distribution. We wouldn't want to pick\nthis, but we could also",
    "start": "3675940",
    "end": "3682960"
  },
  {
    "text": "pick a proposal distribution\nthat samples failures but also does really badly. And that's because it samples\na bunch of unlikely failures.",
    "start": "3682960",
    "end": "3690320"
  },
  {
    "text": "So we both want to pick a\nproposal that samples failures, but also samples likely\nfailures more often",
    "start": "3690320",
    "end": "3696730"
  },
  {
    "text": "than it samples unlikely\nfailures anyway. So questions on this before\nI close this graphic?",
    "start": "3696730",
    "end": "3704300"
  },
  {
    "text": "Yeah. So importance sampling has\nno knobs to fill at all except for the start\nfrom the last lecture,",
    "start": "3704300",
    "end": "3711410"
  },
  {
    "text": "which is how do we find\nfailures, is that correct? The main knob you can turn\nan importance sampling",
    "start": "3711410",
    "end": "3717640"
  },
  {
    "text": "is the number of samples and\nthe proposal distribution that you pick. Yeah, And the proposal is\nbasically last lecture.",
    "start": "3717640",
    "end": "3724520"
  },
  {
    "text": "How do you find failures\nin the first place? It's really the rest of last\nnine minutes of this lecture. In Thursday's\nlecture, we're going",
    "start": "3724520",
    "end": "3730360"
  },
  {
    "text": "to talk about how to find it. OK. Yeah, but I think\nwhat I'm about to show we'll clear it up a little bit.",
    "start": "3730360",
    "end": "3735450"
  },
  {
    "text": " So what I was\ntrying to show there",
    "start": "3735450",
    "end": "3741340"
  },
  {
    "text": "is that we want a proposal\ndistribution that samples, failures more often\nthan unlikely failures.",
    "start": "3741340",
    "end": "3746793"
  },
  {
    "text": "And so that might\nget you thinking like is there an\noptimal proposal distribution we can pick?",
    "start": "3746793",
    "end": "3751960"
  },
  {
    "text": "And so let's think about for\na second what that might be. So we're going to define the\noptimal proposal distribution.",
    "start": "3751960",
    "end": "3758400"
  },
  {
    "text": "So we have to say what\nwe mean by optimal. We're going to say it's\nthe one that minimizes the variance of the estimator.",
    "start": "3758400",
    "end": "3763890"
  },
  {
    "text": "So here's the variance. I'm not saying that you should\nknow how to derive this, but here's what the variance\nof the importance sampling",
    "start": "3763890",
    "end": "3770600"
  },
  {
    "text": "estimator looks like. And we want to see if we\ncan minimize this variance.",
    "start": "3770600",
    "end": "3777840"
  },
  {
    "text": "So I'm just going to give\nyou a proposal distribution to try out because I\nalready know the answer.",
    "start": "3777840",
    "end": "3785059"
  },
  {
    "text": "Let's see here. So let's try q of tau is equal\nto p of tau times this indicator",
    "start": "3785060",
    "end": "3799373"
  },
  {
    "text": "function.  over pfail.",
    "start": "3799373",
    "end": "3805940"
  },
  {
    "text": "OK, so what I'm going to do is-- What if I gave you a proposal\ndistribution that samples",
    "start": "3805940",
    "end": "3813830"
  },
  {
    "text": "zero failures whatsoever. I guarantee you,\nit's way outside of the scope of any\npossible failure.",
    "start": "3813830",
    "end": "3820440"
  },
  {
    "text": "Isn't that also minimizing the\nvariance of the p fail zero.",
    "start": "3820440",
    "end": "3826680"
  },
  {
    "text": "Pfail is zero. I give you a proposal--\nour failure regions, and somewhere on\nthe left I gave you",
    "start": "3826680",
    "end": "3832290"
  },
  {
    "text": "a proposal distribution\nthat's no variance whatsoever in my\nproposal, distribution is all the way to the right. ",
    "start": "3832290",
    "end": "3842609"
  },
  {
    "text": "Yeah, I think it would minimize\nthe variance of your estimator because you're always going\nto get an estimate of zero.",
    "start": "3842610",
    "end": "3848137"
  },
  {
    "text": "So variance isn't\nmaybe the only thing we want to look at because it\nwill minimize the variance. You'll always get\nan estimate of zero,",
    "start": "3848137",
    "end": "3854369"
  },
  {
    "text": "but that's not very useful. Why is it back up issue? Because this one won't always\ngive us an estimate of zero.",
    "start": "3854370",
    "end": "3860730"
  },
  {
    "text": "Oh wait, but we define the\noptimal distribution as well.",
    "start": "3860730",
    "end": "3866400"
  },
  {
    "text": "Yeah, so there's multiple that\nwill minimize this variance. Oh, I see. Yeah, we should pick the\none that's useful to us.",
    "start": "3866400",
    "end": "3871660"
  },
  {
    "text": "That one's not going\nto be useful, yeah, OK. So it turns out, spoiler alert. This is the one that's\ngoing to be useful to us.",
    "start": "3871660",
    "end": "3878500"
  },
  {
    "text": "And so you could imagine that\nif I go ahead and plug this in to our formula\nfor the variance, we're going to get this\nexpectation P of tau minus--",
    "start": "3878500",
    "end": "3894670"
  },
  {
    "text": "and so we're going to\nplug-in for this q of tau. We're going to plug-in\nthis thing here.",
    "start": "3894670",
    "end": "3900500"
  },
  {
    "text": "So that's going to be\nP of tau times pfail.",
    "start": "3900500",
    "end": "3911260"
  },
  {
    "text": "Sorry this is getting messy. All of this over q of tau.",
    "start": "3911260",
    "end": "3917980"
  },
  {
    "text": "And so if we look here\nstuff's going to cancel out. So this pfail is going to\ncancel out with this one, and then we have this minus\nthis, which is the same thing.",
    "start": "3917980",
    "end": "3925907"
  },
  {
    "text": "And so as you might\nhave been guessing we're going to get zero. So the lowest we can\nmake the variance",
    "start": "3925907",
    "end": "3930940"
  },
  {
    "text": "is zero, so the proposal\nthat minimizes this variance is going to be this one here.",
    "start": "3930940",
    "end": "3936160"
  },
  {
    "text": " Let me just, oops, make\nthis easier to see.",
    "start": "3936160",
    "end": "3944070"
  },
  {
    "text": "So we basically just showed\nthat the optimal proposal distribution is equal to\nthis distribution here.",
    "start": "3944070",
    "end": "3952460"
  },
  {
    "text": "Does this distribution look\nfamiliar to anyone, yeah? ",
    "start": "3952460",
    "end": "3958490"
  },
  {
    "text": "Yeah exactly right. It's the failure distribution. But as you just\nmentioned, it's written",
    "start": "3958490",
    "end": "3965930"
  },
  {
    "text": "in terms of the\nnormalizing constant, which we said was pfail,\nwhich is the thing that we're trying to estimate.",
    "start": "3965930",
    "end": "3972589"
  },
  {
    "text": "So we have a problem here where\nwe want to estimate pfail, and I just told you that the\noptimal distribution that we",
    "start": "3972590",
    "end": "3978500"
  },
  {
    "text": "should sample from if we want\nto estimate pfail is this, but we don't know\npfail, so then we don't",
    "start": "3978500",
    "end": "3983540"
  },
  {
    "text": "get to know this distribution. So the best thing\nthat we can do now",
    "start": "3983540",
    "end": "3990450"
  },
  {
    "text": "is just do our best to\nselect a proposal that is as close as possible to\nthe failure distribution.",
    "start": "3990450",
    "end": "3995980"
  },
  {
    "text": "So when we select our proposal,\nthis is what we have in mind. We want to get as close as we\ncan to the failure distribution.",
    "start": "3995980",
    "end": "4001745"
  },
  {
    "text": "If we knew the\nfailure distribution, we wouldn't be doing this\nwhole thing in the first place.",
    "start": "4001745",
    "end": "4007970"
  },
  {
    "text": "And one note here, that's\nimportant to think about is when we select\nthe proposal, we need to both be able to\ndraw samples from it,",
    "start": "4007970",
    "end": "4014760"
  },
  {
    "text": "and we need to be able to\ncompute the normalized density. So we could draw samples from\nour failure distribution. We talked about how to\ndo that last lecture.",
    "start": "4014760",
    "end": "4021360"
  },
  {
    "text": "But we were just\nsaying we don't know how to compute the\nnormalized density for it, so that's why we\ncan't do that OK.",
    "start": "4021360",
    "end": "4030890"
  },
  {
    "text": "So one thing that we might do-- so like we just said\nfrom previous lecture, we know how to sample from\nthe failure distribution,",
    "start": "4030890",
    "end": "4036577"
  },
  {
    "text": "but we don't know how to\ncompute its normalized density. And so what we\nmight want to do is we could try to fit a\ndistribution that we know how",
    "start": "4036577",
    "end": "4043850"
  },
  {
    "text": "to compute the normalized\ndensity for using samples from the failure distribution. So for example, we get a bunch\nof samples from the failure",
    "start": "4043850",
    "end": "4049630"
  },
  {
    "text": "distribution. And then maybe we\ntake those samples and we fit a Gaussian\ndistribution to them because we know the density\nof a Gaussian distribution",
    "start": "4049630",
    "end": "4056320"
  },
  {
    "text": "and we know how to\ndraw samples from it. So I think we have just\nenough time before we wrap up",
    "start": "4056320",
    "end": "4062470"
  },
  {
    "text": "for me to show you\nthis Pluto notebook, and then we'll have to pick up\nwhere we left off next lecture.",
    "start": "4062470",
    "end": "4068470"
  },
  {
    "text": "So what's going on\nhere is we draw samples from our failure distribution. We can do that using\nany method that we",
    "start": "4068470",
    "end": "4074589"
  },
  {
    "text": "talked about from the previous\nlecture for drawing samples. So here I'm using MCMC\nsampling to draw samples",
    "start": "4074590",
    "end": "4080589"
  },
  {
    "text": "from our simple\nGaussian distribution. And then all I do is call\nfit on those samples.",
    "start": "4080590",
    "end": "4086990"
  },
  {
    "text": "And I fit a normal\ndistribution to those samples.",
    "start": "4086990",
    "end": "4093700"
  },
  {
    "text": "And use that as\nour new proposal. So you can see the true failure\ndistribution is that red thing",
    "start": "4093700",
    "end": "4099460"
  },
  {
    "text": "there. The distribution that\nwe fit is the one that's got all the\nsamples on top of it.",
    "start": "4099460",
    "end": "4104830"
  },
  {
    "text": "And you can see that ends up\ndoing pretty well compared to direct estimation. So we fit a pretty good\nproposal doing this.",
    "start": "4104830",
    "end": "4112075"
  },
  {
    "text": " Yeah, are there any\nquestions on this?",
    "start": "4112075",
    "end": "4118068"
  },
  {
    "text": "Yeah, I understood it, I\nguess wrong before I thought that when you say we sample from\nQ, it's actually the process",
    "start": "4118069",
    "end": "4124700"
  },
  {
    "text": "that we discussed in the last\nlecture where we find failures, where we also sample somehow\nor generate failure examples.",
    "start": "4124700",
    "end": "4133799"
  },
  {
    "text": "And I thought this was\nexactly sampling from Q. But this is actually\nnot the case, and you're saying this or\nfinding failures like we did",
    "start": "4133800",
    "end": "4142460"
  },
  {
    "text": "in last lecture,\nwe cannot, I guess, compute the probability\nof the samples.",
    "start": "4142460",
    "end": "4147600"
  },
  {
    "text": "And this is why we can't use\nit to reweight our samples. Yeah, that's exactly right. So the key thing that\nI mentioned with Q",
    "start": "4147600",
    "end": "4153920"
  },
  {
    "text": "is we need to be able\nto sample from it, but we also need to\nbe able to compute the normalized density of\nit so that we can compute",
    "start": "4153920",
    "end": "4159470"
  },
  {
    "text": "those importance weights. And so because we could only\ndraw samples in the last lecture and we couldn't compute\nthe normalized density.",
    "start": "4159470",
    "end": "4165120"
  },
  {
    "text": "And in fact, in\ncomputing that requires us to know the\nprobability of failure. We can't use that\nas our distribution.",
    "start": "4165120",
    "end": "4173220"
  },
  {
    "text": "I mean, intuitively,\nit seems to me that somehow you could\nrejection sampling or something,",
    "start": "4173220",
    "end": "4178890"
  },
  {
    "text": "quantify the probability of\nany given sample coming out of rejection sampling. But maybe it's which would,\nI guess, satisfy this,",
    "start": "4178890",
    "end": "4186159"
  },
  {
    "text": "but maybe it's not\npossible, I don't know. Yeah there are--",
    "start": "4186160",
    "end": "4192210"
  },
  {
    "text": "So if you could sample from\nthe failure distribution, your importance weights\nwould be exactly equal to the\nprobability of failure,",
    "start": "4192210",
    "end": "4197820"
  },
  {
    "text": "and you'd have zero variance. I think it's one of those things\nyou have to just think about",
    "start": "4197820",
    "end": "4204900"
  },
  {
    "text": "for a while. I'll do that. Yeah. I think last week we said\nthat the proposal distribution",
    "start": "4204900",
    "end": "4211500"
  },
  {
    "text": "for the other sampling\nmethods have to be larger than our actual distribution. Is this the case\nhere as well or not?",
    "start": "4211500",
    "end": "4219150"
  },
  {
    "text": "It is not. So we said it had to bound\nthe failure distribution. I double check, but\nI think it's just",
    "start": "4219150",
    "end": "4225429"
  },
  {
    "text": "the supports need to overlap. So you need to make sure you're\nassigning non-zero probability in your proposal\ndistribution to places",
    "start": "4225430",
    "end": "4231602"
  },
  {
    "text": "where your other\ndistribution also assigns non-zero probability. Wait, what exactly\nare you saying?",
    "start": "4231603",
    "end": "4238719"
  },
  {
    "text": "Because we can get a bunch\nof samples from the failure distribition and do\na bunch of counts. What are we fitting today?",
    "start": "4238720",
    "end": "4244540"
  },
  {
    "text": "Any model class we\nthink is a good idea. So in this case, I just\nfit a Gaussian distribution",
    "start": "4244540",
    "end": "4249610"
  },
  {
    "text": "to those samples. But how does a series\nof failures at hand",
    "start": "4249610",
    "end": "4256480"
  },
  {
    "text": "tell you anything about\nthe probability of failure. Say, I had just excellent\nfailure detector",
    "start": "4256480",
    "end": "4262690"
  },
  {
    "text": "and I'll give you a\nbunch of failures. I'm failing to samples from\nthe failure distribution. So that should tell me what the\nfailure distribution is like.",
    "start": "4262690",
    "end": "4271150"
  },
  {
    "text": "Oh, I see what you mean, OK . Yeah, OK, I think we probably\nneed to wrap up there because we're over time.",
    "start": "4271150",
    "end": "4277060"
  },
  {
    "text": "But we'll pick up\nwith this on Thursday. ",
    "start": "4277060",
    "end": "4284000"
  }
]