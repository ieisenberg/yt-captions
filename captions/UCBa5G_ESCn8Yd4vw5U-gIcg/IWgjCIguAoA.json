[
  {
    "start": "0",
    "end": "18000"
  },
  {
    "start": "0",
    "end": "5740"
  },
  {
    "text": "OMAR KHATTAB: Hello, everyone.",
    "start": "5740",
    "end": "6990"
  },
  {
    "text": "Welcome to part 4 of our\nseries on NLU and IR.",
    "start": "6990",
    "end": "11059"
  },
  {
    "text": "This screencast will be\nthe second among three",
    "start": "11060",
    "end": "13190"
  },
  {
    "text": "of our videos on neural\ninformation retrieval.",
    "start": "13190",
    "end": "16358"
  },
  {
    "start": "16358",
    "end": "19170"
  },
  {
    "start": "18000",
    "end": "123000"
  },
  {
    "text": "Just to recap, this is the\nfunctional view of neural IR",
    "start": "19170",
    "end": "22530"
  },
  {
    "text": "that we left in the\nprevious screencast.",
    "start": "22530",
    "end": "25370"
  },
  {
    "text": "Our model will take a\nquery, and a document,",
    "start": "25370",
    "end": "28100"
  },
  {
    "text": "and will then\noutput a score that",
    "start": "28100",
    "end": "30530"
  },
  {
    "text": "will estimate the relevance\nof this document to the query.",
    "start": "30530",
    "end": "34490"
  },
  {
    "text": "We will sort the documents\nby the decreasing score",
    "start": "34490",
    "end": "37820"
  },
  {
    "text": "to get the top K results.",
    "start": "37820",
    "end": "38900"
  },
  {
    "start": "38900",
    "end": "41450"
  },
  {
    "text": "Let's begin with a\nvery effective paradigm",
    "start": "41450",
    "end": "44810"
  },
  {
    "text": "for building neural\nIR models, namely",
    "start": "44810",
    "end": "49060"
  },
  {
    "text": "query-document interaction.",
    "start": "49060",
    "end": "50637"
  },
  {
    "start": "50637",
    "end": "53440"
  },
  {
    "text": "So given a query and a\ndocument we'll tokenize them.",
    "start": "53440",
    "end": "57180"
  },
  {
    "text": "Then we'll embed\nthe tokens of each",
    "start": "57180",
    "end": "59100"
  },
  {
    "text": "into a static vector\nrepresentation.",
    "start": "59100",
    "end": "62989"
  },
  {
    "text": "So these could be GloVe\nvectors, for example,",
    "start": "62990",
    "end": "66100"
  },
  {
    "text": "or the initial\nrepresentations of BERT.",
    "start": "66100",
    "end": "71200"
  },
  {
    "text": "We'll then build what is called\na query document interaction",
    "start": "71200",
    "end": "74189"
  },
  {
    "text": "matrix.",
    "start": "74190",
    "end": "75180"
  },
  {
    "text": "This is typically nothing but\na matrix of cosine similarities",
    "start": "75180",
    "end": "79800"
  },
  {
    "text": "between each pair of\nwords across the query",
    "start": "79800",
    "end": "84342"
  },
  {
    "text": "and the document.",
    "start": "84342",
    "end": "85050"
  },
  {
    "start": "85050",
    "end": "88710"
  },
  {
    "text": "Now that we have\nthis matrix, we just",
    "start": "88710",
    "end": "90790"
  },
  {
    "text": "need to reduce it to\na single score that",
    "start": "90790",
    "end": "93070"
  },
  {
    "text": "estimates the relevance of\nour document to this query.",
    "start": "93070",
    "end": "98390"
  },
  {
    "text": "To do this, we'll\njust learn a bunch",
    "start": "98390",
    "end": "100130"
  },
  {
    "text": "of neural layers like\nconvolutional or linear layers",
    "start": "100130",
    "end": "103310"
  },
  {
    "text": "with pooling until we end\nup with a single score",
    "start": "103310",
    "end": "108130"
  },
  {
    "text": "for this query-document pair.",
    "start": "108130",
    "end": "109600"
  },
  {
    "start": "109600",
    "end": "112860"
  },
  {
    "text": "Many IR models out there fall\nin this category, especially",
    "start": "112860",
    "end": "116850"
  },
  {
    "text": "ones that were introduced\nbetween 2016 through 2018",
    "start": "116850",
    "end": "120479"
  },
  {
    "text": "or 2019.",
    "start": "120480",
    "end": "124560"
  },
  {
    "start": "123000",
    "end": "171000"
  },
  {
    "text": "With enough training\ndata, query-document human",
    "start": "124560",
    "end": "127790"
  },
  {
    "text": "interaction models can achieve\nconsiderably better quality",
    "start": "127790",
    "end": "131480"
  },
  {
    "text": "than Bag-of-Words\nmodels like BM 25.",
    "start": "131480",
    "end": "134269"
  },
  {
    "text": "And they can actually do that\nat a reasonable increase,",
    "start": "134270",
    "end": "137630"
  },
  {
    "text": "moderate increase in\ncomputational cost.",
    "start": "137630",
    "end": "139790"
  },
  {
    "start": "139790",
    "end": "143260"
  },
  {
    "text": "So as discussed in the\nprevious screencasts,",
    "start": "143260",
    "end": "146370"
  },
  {
    "text": "these models are typically\nused as the last stage",
    "start": "146370",
    "end": "149459"
  },
  {
    "text": "of every ranking pipeline.",
    "start": "149460",
    "end": "151320"
  },
  {
    "text": "And in particular,\nin this figure",
    "start": "151320",
    "end": "154650"
  },
  {
    "text": "here, they're used to\nre-rank the top 1,000",
    "start": "154650",
    "end": "156540"
  },
  {
    "text": "messages retrieved by BM25.",
    "start": "156540",
    "end": "159480"
  },
  {
    "text": "And this is done to make sure\nthat the latency is acceptable",
    "start": "159480",
    "end": "163170"
  },
  {
    "text": "while still improving\nthe MRR and the quality",
    "start": "163170",
    "end": "166530"
  },
  {
    "text": "over BM25 retrieval.",
    "start": "166530",
    "end": "168486"
  },
  {
    "start": "168486",
    "end": "175939"
  },
  {
    "start": "171000",
    "end": "242000"
  },
  {
    "text": "More recently, in\n2019, the IR community",
    "start": "175940",
    "end": "179420"
  },
  {
    "text": "discovered the power\nof BERT for ranking.",
    "start": "179420",
    "end": "182709"
  },
  {
    "text": "Functionally, this is very\nsimilar to the paradigm",
    "start": "182710",
    "end": "186880"
  },
  {
    "text": "that we just saw with\nquery-document interactions.",
    "start": "186880",
    "end": "190070"
  },
  {
    "text": "So here, we're going\nto feed BERT the query",
    "start": "190070",
    "end": "192190"
  },
  {
    "text": "and the document as one\nsequence with two segments.",
    "start": "192190",
    "end": "195370"
  },
  {
    "text": "One segment for the query and\none segment for the document as",
    "start": "195370",
    "end": "198370"
  },
  {
    "text": "shown.",
    "start": "198370",
    "end": "199709"
  },
  {
    "text": "We'll run this through\nall the layers of BERT.",
    "start": "199710",
    "end": "202850"
  },
  {
    "text": "And we'll finally extract the\nclass token embedding from BERT",
    "start": "202850",
    "end": "206600"
  },
  {
    "text": "and reduce it to a single score\nthrough a final linear head",
    "start": "206600",
    "end": "209780"
  },
  {
    "text": "on top of BERT.",
    "start": "209780",
    "end": "212350"
  },
  {
    "text": "As you can probably tell, this\nis nothing but a standard BERT",
    "start": "212350",
    "end": "215440"
  },
  {
    "text": "classifier, where\nwe're going to take",
    "start": "215440",
    "end": "217330"
  },
  {
    "text": "the scores or the\nconfidence that's",
    "start": "217330",
    "end": "220540"
  },
  {
    "text": "the output of the\nclassifier and use it",
    "start": "220540",
    "end": "222250"
  },
  {
    "text": "for ranking our passages.",
    "start": "222250",
    "end": "224790"
  },
  {
    "text": "And like any other\ntask with BERT,",
    "start": "224790",
    "end": "227159"
  },
  {
    "text": "we should first\nfine-tune this BERT model",
    "start": "227160",
    "end": "229710"
  },
  {
    "text": "with appropriate training data\nbefore we use it for our task.",
    "start": "229710",
    "end": "235000"
  },
  {
    "text": "We've discussed how\nto train our models",
    "start": "235000",
    "end": "237460"
  },
  {
    "text": "in the previous screencasts,\nso refer to that if you'd like.",
    "start": "237460",
    "end": "241000"
  },
  {
    "start": "241000",
    "end": "243860"
  },
  {
    "start": "242000",
    "end": "293000"
  },
  {
    "text": "So this really simple\nmodel on top of BERT",
    "start": "243860",
    "end": "246340"
  },
  {
    "text": "was the foundation for\ntremendous progress in search",
    "start": "246340",
    "end": "249310"
  },
  {
    "text": "over the past two years.",
    "start": "249310",
    "end": "252110"
  },
  {
    "text": "And in particular,\nit's worth mentioning",
    "start": "252110",
    "end": "255470"
  },
  {
    "text": "the first public\ninstance of this,",
    "start": "255470",
    "end": "257190"
  },
  {
    "text": "which was in January of 2019\non the MS MARCO Passage Ranking",
    "start": "257190",
    "end": "261320"
  },
  {
    "text": "task.",
    "start": "261320",
    "end": "262760"
  },
  {
    "text": "Here, Nogueira and Cho made a\nsimple BERT-based submission",
    "start": "262760",
    "end": "266900"
  },
  {
    "text": "to the leaderboard of MS\nMARCO that demonstrated",
    "start": "266900",
    "end": "270470"
  },
  {
    "text": "dramatic gains over the previous\nstate of the art submitted",
    "start": "270470",
    "end": "273680"
  },
  {
    "text": "just a few days prior.",
    "start": "273680",
    "end": "276150"
  },
  {
    "text": "By October of 2019,\nalmost exactly one year",
    "start": "276150",
    "end": "278820"
  },
  {
    "text": "after BERT originally\ncame out, Google",
    "start": "278820",
    "end": "281310"
  },
  {
    "text": "had publicly discussed\nthe use of BERT in search.",
    "start": "281310",
    "end": "284160"
  },
  {
    "text": "And Bing followed soon after\nin November of the same year.",
    "start": "284160",
    "end": "288090"
  },
  {
    "start": "288090",
    "end": "294240"
  },
  {
    "start": "293000",
    "end": "445000"
  },
  {
    "text": "But the story is actually\na bit more complicated.",
    "start": "294240",
    "end": "297090"
  },
  {
    "text": "These very large\ngains in quality",
    "start": "297090",
    "end": "299400"
  },
  {
    "text": "came at a drastic increase\nin computational cost,",
    "start": "299400",
    "end": "303180"
  },
  {
    "text": "which dictates\nlatency, and which",
    "start": "303180",
    "end": "306199"
  },
  {
    "text": "is very important for user\nexperience in search tasks",
    "start": "306200",
    "end": "309353"
  },
  {
    "text": "as we have discussed before.",
    "start": "309353",
    "end": "310520"
  },
  {
    "start": "310520",
    "end": "313229"
  },
  {
    "text": "So over a simple query\ndocument interaction",
    "start": "313230",
    "end": "317390"
  },
  {
    "text": "models like Duet or ConvKNRM,\nNogueira and Cho's BERT models",
    "start": "317390",
    "end": "322640"
  },
  {
    "text": "increase MRR by over 8 points\nbut also increase latency",
    "start": "322640",
    "end": "327770"
  },
  {
    "text": "to multiple seconds per query.",
    "start": "327770",
    "end": "331229"
  },
  {
    "text": "And so here it is natural for us\nto ask ourselves the question,",
    "start": "331230",
    "end": "336780"
  },
  {
    "text": "if we could achieve high\nMRR and low latency at once.",
    "start": "336780",
    "end": "342760"
  },
  {
    "text": "And it turns out that\nthe answer is yes,",
    "start": "342760",
    "end": "344860"
  },
  {
    "text": "but it will take a lot\nof progress to get there.",
    "start": "344860",
    "end": "347405"
  },
  {
    "text": "And we'll try to cover that\nin the rest of the screencast",
    "start": "347405",
    "end": "349780"
  },
  {
    "text": "in the next one.",
    "start": "349780",
    "end": "350860"
  },
  {
    "text": "So let's get started with that.",
    "start": "350860",
    "end": "354449"
  },
  {
    "text": "So to seek better trade offs\nbetween quality and latency,",
    "start": "354450",
    "end": "357420"
  },
  {
    "text": "which is our goal,\nlet's think about why",
    "start": "357420",
    "end": "359840"
  },
  {
    "text": "BERT rankers are so slow.",
    "start": "359840",
    "end": "362180"
  },
  {
    "text": "Our first observation\nhere will be",
    "start": "362180",
    "end": "364070"
  },
  {
    "text": "that BERT rankers\nare quite redundant",
    "start": "364070",
    "end": "366230"
  },
  {
    "text": "in their computations.",
    "start": "366230",
    "end": "368600"
  },
  {
    "text": "If you think about\nwhat BERT rankers do,",
    "start": "368600",
    "end": "371930"
  },
  {
    "text": "they need to compute a\ncontextualized representation",
    "start": "371930",
    "end": "375560"
  },
  {
    "text": "of the query for each\ndocument that we rank.",
    "start": "375560",
    "end": "378139"
  },
  {
    "text": "So that's 1,000 times\nfor 1,000 documents.",
    "start": "378140",
    "end": "381860"
  },
  {
    "text": "And they also must\nencode each document",
    "start": "381860",
    "end": "384229"
  },
  {
    "text": "for every single\nquery that comes along",
    "start": "384230",
    "end": "386783"
  },
  {
    "text": "that needs a score\nfor that document.",
    "start": "386783",
    "end": "388324"
  },
  {
    "start": "388325",
    "end": "390830"
  },
  {
    "text": "Of course, we have the documents\nin our collections in advance,",
    "start": "390830",
    "end": "394669"
  },
  {
    "text": "and we can do as\nmuch preprocessing",
    "start": "394670",
    "end": "396590"
  },
  {
    "text": "as we want on them offline\nbefore we get any queries.",
    "start": "396590",
    "end": "400650"
  },
  {
    "text": "So the question becomes,\ncan we somehow precompute",
    "start": "400650",
    "end": "403430"
  },
  {
    "text": "some form of document\nrepresentations",
    "start": "403430",
    "end": "405380"
  },
  {
    "text": "in advance once and for all\nusing these powerful models",
    "start": "405380",
    "end": "408500"
  },
  {
    "text": "that we have like BERT, and\nstore these representations",
    "start": "408500",
    "end": "411650"
  },
  {
    "text": "or cache them somewhere so\nwe can just use them quickly",
    "start": "411650",
    "end": "414889"
  },
  {
    "text": "every time we have\na query to answer?",
    "start": "414890",
    "end": "417510"
  },
  {
    "text": "This will be our\nguiding question",
    "start": "417510",
    "end": "419000"
  },
  {
    "text": "for the remainder of this\nand the next screencasts.",
    "start": "419000",
    "end": "423700"
  },
  {
    "text": "Of course, it is not actually\nobvious yet, at least,",
    "start": "423700",
    "end": "427210"
  },
  {
    "text": "if we can pre-compute such\nrepresentations in advance",
    "start": "427210",
    "end": "429850"
  },
  {
    "text": "without much loss in quality\nfor all we know so far,",
    "start": "429850",
    "end": "433910"
  },
  {
    "text": "there might be a lot\nof empirical value",
    "start": "433910",
    "end": "436900"
  },
  {
    "text": "in jointly representing\nqueries and documents at once.",
    "start": "436900",
    "end": "439810"
  },
  {
    "start": "439810",
    "end": "442470"
  },
  {
    "text": "But we'll put this\nhypothesis to the test.",
    "start": "442470",
    "end": "446070"
  },
  {
    "start": "445000",
    "end": "551000"
  },
  {
    "text": "The first approach to tame the\ncomputational latency of BERT",
    "start": "446070",
    "end": "449130"
  },
  {
    "text": "for IR is learning term weights.",
    "start": "449130",
    "end": "452680"
  },
  {
    "text": "The key observation here is that\nBag-of-Words models like BM25,",
    "start": "452680",
    "end": "457150"
  },
  {
    "text": "it composed the score\nof every document",
    "start": "457150",
    "end": "459340"
  },
  {
    "text": "into a summation of\nterm document weights,",
    "start": "459340",
    "end": "461889"
  },
  {
    "text": "and maybe, we can do the same.",
    "start": "461890",
    "end": "464210"
  },
  {
    "text": "So can we learn these\nterm weights with BERT",
    "start": "464210",
    "end": "466750"
  },
  {
    "text": "in particular?",
    "start": "466750",
    "end": "468570"
  },
  {
    "text": "A simple way to do this\nwould be to tokenize",
    "start": "468570",
    "end": "471690"
  },
  {
    "text": "a query and the document.",
    "start": "471690",
    "end": "473900"
  },
  {
    "text": "Feed BERT, only the\ndocument, and use",
    "start": "473900",
    "end": "478770"
  },
  {
    "text": "a linear layer to project\neach token in the document",
    "start": "478770",
    "end": "482530"
  },
  {
    "text": "into a single numeric score.",
    "start": "482530",
    "end": "485740"
  },
  {
    "text": "The idea here is that we\ncan save these document term",
    "start": "485740",
    "end": "488590"
  },
  {
    "text": "weights to the inverted\nindex just like we",
    "start": "488590",
    "end": "490750"
  },
  {
    "text": "did with BM25 and classical IR\nand quickly look up these term",
    "start": "490750",
    "end": "495940"
  },
  {
    "text": "weights when answering a query.",
    "start": "495940",
    "end": "498830"
  },
  {
    "text": "This makes sure we do not\nneed to use BERT at all when",
    "start": "498830",
    "end": "501639"
  },
  {
    "text": "answering a query as we just\nshifted all of our BERT work",
    "start": "501640",
    "end": "508050"
  },
  {
    "text": "offline to the indexing stage.",
    "start": "508050",
    "end": "509940"
  },
  {
    "start": "509940",
    "end": "514039"
  },
  {
    "text": "So this can be really great.",
    "start": "514039",
    "end": "515539"
  },
  {
    "text": "We now get to use BERT to\nlearn much stronger term",
    "start": "515539",
    "end": "518330"
  },
  {
    "text": "weights than BM25.",
    "start": "518330",
    "end": "519080"
  },
  {
    "start": "519080",
    "end": "521900"
  },
  {
    "text": "And DeepCT and doc2query\nare two major models",
    "start": "521900",
    "end": "525170"
  },
  {
    "text": "under this efficient paradigm.",
    "start": "525170",
    "end": "527850"
  },
  {
    "text": "As the figure shows\nat the bottom,",
    "start": "527850",
    "end": "530759"
  },
  {
    "text": "they indeed greatly\noutperform BM25 and MRR,",
    "start": "530760",
    "end": "533910"
  },
  {
    "text": "but actually, they\nhave comparable latency",
    "start": "533910",
    "end": "535680"
  },
  {
    "text": "because we're still\nusing an inverted index",
    "start": "535680",
    "end": "537510"
  },
  {
    "text": "to do that retrieval.",
    "start": "537510",
    "end": "539690"
  },
  {
    "text": "However, the downside\nis that our query",
    "start": "539690",
    "end": "542960"
  },
  {
    "text": "is back to being a\nBag-of-Words, and we",
    "start": "542960",
    "end": "545330"
  },
  {
    "text": "lose any deeper understanding\nof our queries beyond that.",
    "start": "545330",
    "end": "548570"
  },
  {
    "start": "548570",
    "end": "551300"
  },
  {
    "start": "551000",
    "end": "576000"
  },
  {
    "text": "So our central question\nremains whether we can jointly",
    "start": "551300",
    "end": "554500"
  },
  {
    "text": "achieve high MRR and\nlow computational cost.",
    "start": "554500",
    "end": "558500"
  },
  {
    "text": "And as we said before,\nthe answer is yes.",
    "start": "558500",
    "end": "560890"
  },
  {
    "text": "And to do this, we'll discuss\nin the next screencast",
    "start": "560890",
    "end": "564550"
  },
  {
    "text": "two very exciting paradigms\nof neural IR models",
    "start": "564550",
    "end": "567580"
  },
  {
    "text": "that get us close to this goal.",
    "start": "567580",
    "end": "570450"
  },
  {
    "start": "570450",
    "end": "575254"
  }
]