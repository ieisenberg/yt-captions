[
  {
    "start": "0",
    "end": "20000"
  },
  {
    "text": "Hi, everyone. Let's get started. So, uh, today we'll be covering",
    "start": "4640",
    "end": "10405"
  },
  {
    "text": "the things that we didn't get to last time with regard to, uh, meta-learning and black-box adaptation approaches to meta-learning,",
    "start": "10405",
    "end": "16685"
  },
  {
    "text": "and then we'll cover topics in optimization based approaches. Uh, so before we get started, a couple reminders.",
    "start": "16685",
    "end": "22720"
  },
  {
    "start": "20000",
    "end": "50000"
  },
  {
    "text": "So first, Homework 1 is due on Wednesday next week and that homework assignment is now out,",
    "start": "22720",
    "end": "27950"
  },
  {
    "text": "uh, and yeah, encourage you to- to get started on that, uh, early, and then also the first paper presentations",
    "start": "27950",
    "end": "36085"
  },
  {
    "text": "and discussions of papers will be happening on Wednesday this week, uh, and so please, uh,",
    "start": "36085",
    "end": "41865"
  },
  {
    "text": "show up so we can discuss those papers and also, uh, kind of be a part of the discussion for the students that are presenting, uh, that day as well.",
    "start": "41865",
    "end": "49760"
  },
  {
    "text": "Okay, um, so as I was mentioning today we'll first, uh, start by, uh,",
    "start": "49760",
    "end": "55379"
  },
  {
    "start": "50000",
    "end": "88000"
  },
  {
    "text": "actually by recapping the probabilistic formulation of meta-learning that I mentioned at the end of lecture last time,",
    "start": "55380",
    "end": "60965"
  },
  {
    "text": "and then cover kind of a general recipe for different meta-learning algorithms, uh, and cover black-box adaptation approaches.",
    "start": "60965",
    "end": "67610"
  },
  {
    "text": "These, uh, kind of these two things are the topic of Homework 1 where you'll be implementing, uh, a black box approach to meta-learning.",
    "start": "67610",
    "end": "74869"
  },
  {
    "text": "And then we'll be talking about optimization-based meta-learning, and this will actually be part of Homework 2,",
    "start": "74870",
    "end": "80485"
  },
  {
    "text": "uh, and, uh, the rest of Homework 2 will be covered, uh, in the next lecture on Monday next week.",
    "start": "80485",
    "end": "87109"
  },
  {
    "text": "Okay, so first, let's recap from last time. So we were talking about, uh, kind of a- a more intuitive or- or probabilistic",
    "start": "87110",
    "end": "94750"
  },
  {
    "start": "88000",
    "end": "215000"
  },
  {
    "text": "view to these meta-learning algorithms, uh, and in particular, we can view meta-learning as a process of learning,",
    "start": "94750",
    "end": "100690"
  },
  {
    "text": "a set of meta-parameters Theta, uh, that summarizes your meta-training data such that you can solve new tasks quickly.",
    "start": "100690",
    "end": "110120"
  },
  {
    "text": "Uh, and what this meta-training data looked like was, uh, you had a range of tasks, 1 through n, and for each task you had a train dataset and a test set,",
    "start": "110120",
    "end": "117135"
  },
  {
    "text": "uh, and the train dataset had k data points and the test set had k data points. Uh, and so in particular,",
    "start": "117135",
    "end": "123000"
  },
  {
    "text": "what meta-learning was trying to do was to optimize for a set of meta, uh, for a set of meta-parameters, uh, that, uh,",
    "start": "123000",
    "end": "130030"
  },
  {
    "text": "maximized the likelihood of those parameters. So in particular, you could view kind of the meta-train processes",
    "start": "130030",
    "end": "136460"
  },
  {
    "text": "optimizing for these meta-parameters and the adaptation process as adapting those parameters,",
    "start": "136460",
    "end": "143040"
  },
  {
    "text": "uh, to compute a set of parameters Phi, that, uh, can solve a new task given",
    "start": "143040",
    "end": "148420"
  },
  {
    "text": "a train dataset for that task and the meta-parameters that you learned. Uh, and so you could essentially view, uh,",
    "start": "148420",
    "end": "154890"
  },
  {
    "text": "kind of this adaptation process as this function f that's taking in a train dataset, uh,",
    "start": "154890",
    "end": "160160"
  },
  {
    "text": "and producing a new set of parameters five-star, uh, and kind of under this, uh,",
    "start": "160160",
    "end": "165230"
  },
  {
    "text": "view of, um, of that adaptation process, you can kind of view, uh, meta-learning as optimizing for",
    "start": "165230",
    "end": "172920"
  },
  {
    "text": "the meta-train parameter such that the task specific parameters uh, do well on held out data, your test set,",
    "start": "172920",
    "end": "179745"
  },
  {
    "text": "where the task specific parameters are computed according to your training dataset for that task.",
    "start": "179745",
    "end": "186545"
  },
  {
    "text": "Okay, so this is like essentially the probabilistic view on meta-learning where you can view, uh, kind of the meta-training process as trying to optimize for these, uh,",
    "start": "186545",
    "end": "195099"
  },
  {
    "text": "these kind of prior parameters such that adaptation leads to good performance.",
    "start": "195100",
    "end": "201210"
  },
  {
    "text": "Okay, so now I'd like to talk about how we actually kind of design algorithms that perform this optimization at- at a,",
    "start": "201370",
    "end": "210090"
  },
  {
    "text": "basically at a more mechanistic level and uncover kind of how you actually go about trying to implement some of these things.",
    "start": "210090",
    "end": "215375"
  },
  {
    "start": "215000",
    "end": "583000"
  },
  {
    "text": "So, uh, in particular we, like, can we think about a general recipe for meta-learning algorithms?",
    "start": "215375",
    "end": "220555"
  },
  {
    "text": "Uh, and before we actually cover a general recipe for the algorithms themselves, uh, we need to have a sense for how we're actually gonna be,",
    "start": "220555",
    "end": "226895"
  },
  {
    "text": "going to be evaluating these meta-learning algorithms. Um, so I want to first talk about how to evaluate, um, a meta-learning algorithm,",
    "start": "226895",
    "end": "234025"
  },
  {
    "text": "and, uh, kind of, the first thing worth mentioning here, uh, the first thing, uh, that we should mention is the- the Omniglot dataset.",
    "start": "234025",
    "end": "240780"
  },
  {
    "text": "So this is a dataset, uh, that was proposed by Brenden Lake e- et al in 2015, uh,",
    "start": "240780",
    "end": "246720"
  },
  {
    "text": "and it actually really, uh, kind of exemplifies some of the- the weak points with neural networks that they're gonna be learning from small amounts of data.",
    "start": "246720",
    "end": "253760"
  },
  {
    "text": "So, uh, this dataset has six hu- six fi- 1,600 characters from 50 different alphabets.",
    "start": "253760",
    "end": "259699"
  },
  {
    "text": "Uh, here's some examples of the, um, of the dataset. So there's different alphabets uh like Hebrew, Bengali etc, uh,",
    "start": "259700",
    "end": "268220"
  },
  {
    "text": "and each character has- has only 20 instances.",
    "start": "268220",
    "end": "273595"
  },
  {
    "text": "Uh, so unlike something, uh, like MNIST that has a few number of characters and a huge number of data points per character,",
    "start": "273595",
    "end": "281440"
  },
  {
    "text": "uh, in- in many ways this is sort of like the transpose. It has many classes and few examples per class.",
    "start": "281440",
    "end": "289250"
  },
  {
    "text": "Uh, and one of the things that I think is- is quite appealing like, uh, to a dataset like this is that,",
    "start": "289250",
    "end": "295205"
  },
  {
    "text": "the statistics of this dataset are in many ways more reflective of- of the types of things that we see in the real world.",
    "start": "295205",
    "end": "300850"
  },
  {
    "text": "Uh, for example, if you, uh, kind of are trying to learn how to recognize, uh, forks for example, you're not going to see thousands,",
    "start": "300850",
    "end": "307730"
  },
  {
    "text": "uh, thousands of different types of forks. You may see, uh, uh, a wide range of objects,",
    "start": "307730",
    "end": "314180"
  },
  {
    "text": "but you're only gonna see per object, a small number of instances of that object throughout your lifetime.",
    "start": "314180",
    "end": "319965"
  },
  {
    "text": "Um, okay, so this dataset has kind",
    "start": "319965",
    "end": "325889"
  },
  {
    "text": "of the breadth of classes and- and a small number of examples per class, uh, and they propose a few different ways you could try to use this dataset.",
    "start": "325890",
    "end": "332750"
  },
  {
    "text": "So they propose both few-shot discriminative learning as well as few-shot generative learning problems.",
    "start": "332750",
    "end": "338544"
  },
  {
    "text": "Uh, and in particular what these look like is, uh, the few-shot discriminative learning is given a few examples of new characters,",
    "start": "338545",
    "end": "346759"
  },
  {
    "text": "can you learn to classify between those characters? Uh, and the derivative problem is likewise given a few examples of some characters,",
    "start": "346760",
    "end": "354050"
  },
  {
    "text": "can you actually generate new instances of those characters? Uh, and they essentially show that things like deep neural networks,",
    "start": "354050",
    "end": "362040"
  },
  {
    "text": "uh, struggle at- at this sort of problem if you're gonna be, going to be training them from scratch because if you're only training them on a few examples,",
    "start": "362040",
    "end": "368289"
  },
  {
    "text": "uh, we know that deep neural networks do best when you have a large number of examples. Um, and initial approaches towards this kind of problem, um,",
    "start": "368290",
    "end": "376219"
  },
  {
    "text": "actually predating the Omniglot dataset itself, uh, instead used things like Bayesian models",
    "start": "376220",
    "end": "381950"
  },
  {
    "text": "and non-parametrics in order to solve this problem. Um, great, so this is kind of, uh,",
    "start": "381950",
    "end": "388865"
  },
  {
    "text": "one kind of canonical example for a meta-learning dataset and there are a wide range of others that have also been used for meta-learning more recently.",
    "start": "388865",
    "end": "395595"
  },
  {
    "text": "Uh, these include things like the MiniImagenet dataset, uh, the CIFAR dataset, um,",
    "start": "395595",
    "end": "400935"
  },
  {
    "text": "CUB, CelebA, uh, and a number of others. Uh, in all of these datasets,",
    "start": "400935",
    "end": "406260"
  },
  {
    "text": "kind of the goal is to, given a small number of examples, be able to learn something from that small dataset.",
    "start": "406260",
    "end": "413120"
  },
  {
    "text": "Okay so this is, uh, this is kind of on the dataset side, this is, this is the kind of datasets that you can use to evaluate a few-shot learning algorithm.",
    "start": "413120",
    "end": "420200"
  },
  {
    "text": "Uh, now how do we actually go about evaluating an algorithm on these datasets? Uh, so this is actually gonna look a lot like the tests that I gave you on",
    "start": "420200",
    "end": "427340"
  },
  {
    "text": "the first day where your goal is to classify new examples from a small dataset, and so in particular, let's say that we have a 5-way,",
    "start": "427340",
    "end": "433250"
  },
  {
    "text": "1-shot image classification problem. Uh, and in particular, we could have, um, one example of five different classes shown here.",
    "start": "433250",
    "end": "441710"
  },
  {
    "text": "Uh, way means the number of classes, shot means the number of examples per class, uh,",
    "start": "441710",
    "end": "447150"
  },
  {
    "text": "and then your goal is given these, uh, five examples, classify new examples as being among one of the five classes on the left.",
    "start": "447150",
    "end": "456740"
  },
  {
    "text": "Okay so this is, uh, the few-shot learning problem and in meta-learning, our goal is to be able to leverage data from",
    "start": "456740",
    "end": "463625"
  },
  {
    "text": "other image classes in order to solve this problem. Just like kind of, be- be able to leverage the meta-train dataset that I was mentioning",
    "start": "463625",
    "end": "469340"
  },
  {
    "text": "before in order to learn a few shot classifier that can, kind of, learn from these data points on the left.",
    "start": "469340",
    "end": "475970"
  },
  {
    "text": "So the way that we can do that, if we can structure, uh, the data into training sets and test sets,",
    "start": "475970",
    "end": "481759"
  },
  {
    "text": "just like I was mentioning before, where these are going to mimic what you're going to be seeing at test-time, matching meta-training time and meta-testing time.",
    "start": "481760",
    "end": "488134"
  },
  {
    "text": "So you can take five other image classes- classes and break it into a train set and a test set, and do this for a wide range of other image classes that you've seen in the past.",
    "start": "488135",
    "end": "497090"
  },
  {
    "text": "Uh, these will be your training classes, uh, and you'll perform meta-training across these training,",
    "start": "497090",
    "end": "503385"
  },
  {
    "text": "meta-training the classifier such that after it sees the images on the left, it can successfully classify images on the right.",
    "start": "503385",
    "end": "509965"
  },
  {
    "text": "Uh, and then critically after you do this, uh, you'll test it on held-out image classes as shown on the top, uh,",
    "start": "509965",
    "end": "516905"
  },
  {
    "text": "it will essentially be able to perform this few-shot learning problem. Uh, and this isn't specific to image classification,",
    "start": "516905",
    "end": "524540"
  },
  {
    "text": "you can replace image classification with a regression problem, language generation problems,",
    "start": "524540",
    "end": "529850"
  },
  {
    "text": "skill learning problems, uh, kind of, as I alluded to you in previous lectures, I- each of these tasks shown as rows is essentially a machine learning problem.",
    "start": "529850",
    "end": "540730"
  },
  {
    "text": "Okay, so any questions on this setup. Okay, yeah.",
    "start": "540730",
    "end": "550139"
  },
  {
    "text": "[inaudible] Yes. The nuance here is that,",
    "start": "550140",
    "end": "556270"
  },
  {
    "text": "in multitask learning your goal would be to try to solve all of the training tasks shown in this,",
    "start": "556270",
    "end": "561955"
  },
  {
    "text": "in this box, in the gray box. Whereas in meta-learning your goal is to use",
    "start": "561955",
    "end": "567565"
  },
  {
    "text": "these training tasks in order to solve new tasks with small amounts of data.",
    "start": "567565",
    "end": "572480"
  },
  {
    "text": "So kind of being able to actually evaluate on new tasks and quickly learn new tasks is the critical difference between the two problems.",
    "start": "572880",
    "end": "581420"
  },
  {
    "text": "Okay, um, so kind of more broadly and more",
    "start": "581580",
    "end": "586630"
  },
  {
    "start": "583000",
    "end": "880000"
  },
  {
    "text": "generally we can kind of view the meta-learning problem from a more mechanistic standpoint. Um, and so in particular if we say supervised learning is trying to learn",
    "start": "586630",
    "end": "594490"
  },
  {
    "text": "a mapping from X to Y given input output pairs, we can view meta-supervised learning as trying to learn from",
    "start": "594490",
    "end": "601960"
  },
  {
    "text": "a dataset to make- where this dataset contains k input output pairs for a k shot learning problem.",
    "start": "601960",
    "end": "609130"
  },
  {
    "text": "To make predictions about new test data point X test. So our goal is to kind of produce a function that takes as input",
    "start": "609130",
    "end": "617260"
  },
  {
    "text": "a training data set and a test input and produces the label corresponding to the test input, so there's a more mechanistic view of",
    "start": "617260",
    "end": "624460"
  },
  {
    "text": "meta-learning is essentially that we want to learn this function f. Um, the function f that takes in the training dataset and the test input and produces the label.",
    "start": "624460",
    "end": "631975"
  },
  {
    "text": "Now the way that we learn this, uh, this function is through a Meta-training dataset which contains",
    "start": "631975",
    "end": "638755"
  },
  {
    "text": "a set of tasks or set of datasets where each dataset consists of X Y pairs where you'll use at least k to be used for",
    "start": "638755",
    "end": "648369"
  },
  {
    "text": "the training dataset and at least one additional data point to be used to measure generalization to actually train it such that,",
    "start": "648369",
    "end": "657160"
  },
  {
    "text": "uh, it does well on new data points. Now, uh, why is this view use- view useful?",
    "start": "657160",
    "end": "664060"
  },
  {
    "text": "So we, we kind of saw the probabilistic viewpoint before, um, one of the nice things about this particular problem statement",
    "start": "664060",
    "end": "669130"
  },
  {
    "text": "is that it reduces the problem of Meta-learning to that of designing and optimizing this function f. Uh,",
    "start": "669130",
    "end": "675985"
  },
  {
    "text": "once you kind of design a function f and optim- and kind of decide how you want to optimize it,",
    "start": "675985",
    "end": "681399"
  },
  {
    "text": "uh, then you've created a Meta-learning algorithm. Okay, um, how does this connect to the probabilistic viewpoint, uh,",
    "start": "681400",
    "end": "690925"
  },
  {
    "text": "well you can view supervised learning as doing inference over parameters given a dataset.",
    "start": "690925",
    "end": "697495"
  },
  {
    "text": "Similarly, you can view the adaptation process of Meta-learning as doing",
    "start": "697495",
    "end": "702670"
  },
  {
    "text": "inference over your task specific parameters Phi i given a training dataset and,",
    "start": "702670",
    "end": "708024"
  },
  {
    "text": "uh, and a set of meta parameters and the Meta-learning optimization as doing, um,",
    "start": "708025",
    "end": "715180"
  },
  {
    "text": "maximum likelihood, uh, inference over the meta parameters, uh, over all of your training tasks.",
    "start": "715180",
    "end": "723889"
  },
  {
    "text": "Okay, um, any questions on kind of the problem setup before we get into algorithms. Yeah.",
    "start": "724440",
    "end": "733690"
  },
  {
    "text": "Um, is it important to use the proper value for k or is it-",
    "start": "733690",
    "end": "738740"
  },
  {
    "text": "Yeah, that's a good question. So typically algorithms assume that you know, um, you know something about the k that you'll be evaluated on at test time.",
    "start": "739500",
    "end": "748645"
  },
  {
    "text": "So if you're going to evaluate on 10 shot learning or 100 shot learning, then you'll, uh, train for those values,",
    "start": "748645",
    "end": "755094"
  },
  {
    "text": "uh, and you can train for, um, depending on the algorithm you can t- train for exactly the value that you think you're going to have at test time or a range of values,",
    "start": "755094",
    "end": "762010"
  },
  {
    "text": "such that, um, it can adopt to a range of dataset sizes. Yeah.",
    "start": "762010",
    "end": "769060"
  },
  {
    "text": "What happens when you don't know before [inaudible] all the tasks that you are going to [inaudible]",
    "start": "769060",
    "end": "775449"
  },
  {
    "text": "because of some feature parameter [inaudible].",
    "start": "775450",
    "end": "782440"
  },
  {
    "text": "So your question is what if you don't know the test task that you're gonna- going to be evaluated on? That- like say you are theoretically adding new [inaudible]",
    "start": "782440",
    "end": "792250"
  },
  {
    "text": "Yeah, so generally the assumption here, uh, is that the, the test task that you're being evaluated on",
    "start": "792250",
    "end": "800365"
  },
  {
    "text": "is from the distribution- the same distribution as the training tasks, uh, and what some, some algorithms do",
    "start": "800365",
    "end": "807850"
  },
  {
    "text": "better than others when you break that- break that assumption, uh, and I'll talk about that a bit more in the second half of this lecture.",
    "start": "807850",
    "end": "816144"
  },
  {
    "text": "Uh, there is also kinda this online setting where we're incrementally adding tasks and that's a setting that has been explored a little bit,",
    "start": "816145",
    "end": "824004"
  },
  {
    "text": "uh, and I'll talk a little bit about probably, um, as when we talk about lifelong learning in the course, uh,",
    "start": "824005",
    "end": "831070"
  },
  {
    "text": "and then we'll also talk a little bit about set, like later in the course about settings where you just know nothing and you just have like an unlabeled dataset and,",
    "start": "831070",
    "end": "837399"
  },
  {
    "text": "and how you might be able to try to construct tasks automatically. Was there another question? Yeah.",
    "start": "837400",
    "end": "845100"
  },
  {
    "text": "Is Meta-learning required to extend network structure?",
    "start": "845100",
    "end": "850480"
  },
  {
    "text": "So you're asking is it required to use the same network structure as supervised learning or-",
    "start": "850950",
    "end": "858980"
  },
  {
    "text": "Um, supervised learning. I guess we'll, we'll get into the, the kind of what different architectures you can use for different algorithms, uh,",
    "start": "860640",
    "end": "868495"
  },
  {
    "text": "later in the lecture and then if you still have the- a question you can ask it maybe towards the end.",
    "start": "868495",
    "end": "875900"
  },
  {
    "text": "Okay, great. So, uh, the general recipe for what an algorithm looks like, uh,",
    "start": "876450",
    "end": "883195"
  },
  {
    "start": "880000",
    "end": "1703000"
  },
  {
    "text": "is basically what I alluded to before is choose some form of this function that is, uh,",
    "start": "883195",
    "end": "890005"
  },
  {
    "text": "that could be probabilistic or it could be a deterministic function as I mentioned before where you're going to be outputting a set of task specific parameters given",
    "start": "890005",
    "end": "898120"
  },
  {
    "text": "a training dataset and your meta parameters [BACKGROUND] and then once you choose the form of this",
    "start": "898120",
    "end": "904030"
  },
  {
    "text": "then you need to just figure out how you want to choose to optimize your parameter's data with respect to your,",
    "start": "904030",
    "end": "910030"
  },
  {
    "text": "um, Meta-training dataset, this, this choice is usually somewhat relatively straightforward using standard,",
    "start": "910030",
    "end": "915925"
  },
  {
    "text": "uh, neural network optimizers. Okay. So this is kind of the general form, uh,",
    "start": "915925",
    "end": "921955"
  },
  {
    "text": "and most Meta-learning algorithms vary based off of the first plan, uh, basically how do you actually design this function that's going to infer",
    "start": "921955",
    "end": "928750"
  },
  {
    "text": "task-specific perimeters and so the first class of approaches that we'll look at are going to,",
    "start": "928750",
    "end": "934389"
  },
  {
    "text": "going to be considering, can we treat this, um, this, this distribution as an inference problem?",
    "start": "934390",
    "end": "941605"
  },
  {
    "text": "And in particular neural networks are pretty good at doing things like inference and so can we just treat this function as a neural network?",
    "start": "941605",
    "end": "950290"
  },
  {
    "text": "Um, and this is where, uh, what I'm gonna refer to as Black-Box approaches come in, so, um,",
    "start": "950290",
    "end": "957985"
  },
  {
    "text": "what these Black-Box adaptation approach is they essentially just train a neural network to represent this function right",
    "start": "957985",
    "end": "963340"
  },
  {
    "text": "here that's at- a neural network that's going to be outputting parameters given a training dataset and a set of meta parameters.",
    "start": "963340",
    "end": "970705"
  },
  {
    "text": "And so first for now we're going to be using a deterministic or point estimate of this distribution,",
    "start": "970705",
    "end": "976990"
  },
  {
    "text": "um and we'll kind of get back to Bayesian approaches in a couple of lectures, uh, and the way this looks like is you can have, uh,",
    "start": "976990",
    "end": "985165"
  },
  {
    "text": "some neural network that, uh, has parameters theta,",
    "start": "985165",
    "end": "990250"
  },
  {
    "text": "it takes as input the training data, it can take it as input in a sequential fashion or it can take it in, uh,",
    "start": "990250",
    "end": "997690"
  },
  {
    "text": "kind of all as one batch and it outputs a set of task specific parameters Phi i.",
    "start": "997690",
    "end": "1003240"
  },
  {
    "text": "Uh, then you have a separate neural network that's parameterized by Phi i that makes predictions about test data points.",
    "start": "1003240",
    "end": "1011950"
  },
  {
    "text": "And this is essentially your, uh, the test data, you can basically train this, uh,",
    "start": "1012170",
    "end": "1017235"
  },
  {
    "text": "train everything using your test dataset, DI test.",
    "start": "1017235",
    "end": "1022470"
  },
  {
    "text": "Uh, and so this is like really simple, uh, and some of the nice things about",
    "start": "1022470",
    "end": "1028439"
  },
  {
    "text": "this is we can just train it with standard supervised learning, um, so we can say that the, um, we want to be able to, uh,",
    "start": "1028440",
    "end": "1035069"
  },
  {
    "text": "maximize the probability of the labels under the distribution that G is producing,",
    "start": "1035070",
    "end": "1041610"
  },
  {
    "text": "uh, [BACKGROUND] for all of the test data points, uh, and for all of the tasks in your Meta-training dataset.",
    "start": "1041610",
    "end": "1049210"
  },
  {
    "text": "Um, so essentially what you're doing is you're training this neural network such that it outputs parameters that, uh,",
    "start": "1049760",
    "end": "1056580"
  },
  {
    "text": "represent an accurate classifier [BACKGROUND] Um, so if you denote this right-hand part as",
    "start": "1056580",
    "end": "1064290"
  },
  {
    "text": "the loss for a set of parameters Phi given a test data point, then you can essentially view this optimization as,",
    "start": "1064290",
    "end": "1072750"
  },
  {
    "text": "um, as the, uh, the loss function between the parameters that are outputted, uh,",
    "start": "1072750",
    "end": "1079289"
  },
  {
    "text": "will also ensure that takes some parameters that are outputted by f Theta, evaluate it on your test dataset averaged over all of your tasks.",
    "start": "1079290",
    "end": "1088450"
  },
  {
    "text": "Okay, um, any questions on this? Yeah. Uh, so when you evaluate your model which Phi i do you use?",
    "start": "1089150",
    "end": "1097125"
  },
  {
    "text": "Great, so when you evaluate your model you're given a new task, uh, and so you're given a training dataset for a new task and so what you do is your, uh,",
    "start": "1097125",
    "end": "1104780"
  },
  {
    "text": "for your test task you basically pass in that training dataset into your network f Theta, uh, and,",
    "start": "1104780",
    "end": "1110499"
  },
  {
    "text": "and produce your parameters for that task. Yeah.",
    "start": "1110499",
    "end": "1116370"
  },
  {
    "text": "So are those data and Phi i learned in this neural network? So in this case,",
    "start": "1116370",
    "end": "1122825"
  },
  {
    "text": "the, um, during the meta-training process, the parameters Theta are learned and the parameters Phi",
    "start": "1122825",
    "end": "1129740"
  },
  {
    "text": "i are somewhat dynamically computed per task. Um, so in this sense, Phi i is almost treated more as, um,",
    "start": "1129740",
    "end": "1137870"
  },
  {
    "text": "activations or a ten- or a tensor rather than actual parameters, um, which is somewhat of an interesting concept.",
    "start": "1137870",
    "end": "1145400"
  },
  {
    "text": "Um, the- yeah, basically you can ba- back-propagate the loss with respect to Phi into the meta-parameters Theta. Yeah.",
    "start": "1145400",
    "end": "1154910"
  },
  {
    "text": "[BACKGROUND]",
    "start": "1154910",
    "end": "1182180"
  },
  {
    "text": "Yeah. That's a good question. So, uh, the question is relating to the,",
    "start": "1182180",
    "end": "1187235"
  },
  {
    "text": "um, basically should you be, like, uh, in the homework where you're also passing in y test,",
    "start": "1187235",
    "end": "1193460"
  },
  {
    "text": "uh, as input to, uh, the right-hand side and, and, and zeroing it out. Uh, and the reason for that is that if you have this,",
    "start": "1193460",
    "end": "1200270"
  },
  {
    "text": "kind of, this type of architecture that's an LSTM, um, and you want to basically be sharing weights across time for each of these units,",
    "start": "1200270",
    "end": "1208145"
  },
  {
    "text": "then you want the shape of the tensors, uh, shape of the, the inputs at each,",
    "start": "1208145",
    "end": "1213710"
  },
  {
    "text": "uh, at each data point to be the same. Uh, and so if you want that, then you want to be able to basically pass in, um,",
    "start": "1213710",
    "end": "1221090"
  },
  {
    "text": "pass in the same shape, but, of course, you don't want to give it the, the ground truth label, uh, because the label is what it's supposed to be predicting.",
    "start": "1221090",
    "end": "1226400"
  },
  {
    "text": "[BACKGROUND] Um, so the question",
    "start": "1226400",
    "end": "1238850"
  },
  {
    "text": "was to zero out the label value or the embedding value? [BACKGROUND]. Of y. Um, I think that basically,",
    "start": "1238850",
    "end": "1246679"
  },
  {
    "text": "as long as you're not passing in y test as input in any way, er, you're, you're in good shape. Yeah.",
    "start": "1246680",
    "end": "1254090"
  },
  {
    "text": "[BACKGROUND]",
    "start": "1254090",
    "end": "1288770"
  },
  {
    "text": "Are you asking about- can you maybe repeat your question?",
    "start": "1288770",
    "end": "1295100"
  },
  {
    "text": "[BACKGROUND]",
    "start": "1295100",
    "end": "1304400"
  },
  {
    "text": "Oh, um, right. So you're asking basically that this top function here, is Theta an input or is it parameters?",
    "start": "1304400",
    "end": "1311540"
  },
  {
    "text": "[NOISE] Uh, in this case it is, um, it is the, kind, of the parameters of that model.",
    "start": "1311540",
    "end": "1316820"
  },
  {
    "text": "Uh, and so maybe a more standard notation would be to either put this Theta,",
    "start": "1316820",
    "end": "1321860"
  },
  {
    "text": "um, as subscript to the p or, uh, put a semicolon here to indicate that it is parameters rather than an input.",
    "start": "1321860",
    "end": "1328010"
  },
  {
    "text": "[BACKGROUND]",
    "start": "1328010",
    "end": "1333620"
  },
  {
    "text": "Uh, during meta-training, we're optimizing over Theta. [BACKGROUND]",
    "start": "1333620",
    "end": "1340790"
  },
  {
    "text": "Yeah. In the inner loop you're producing Phi. Yeah. [BACKGROUND]",
    "start": "1340790",
    "end": "1366620"
  },
  {
    "text": "Right. Yeah. So Phi- yeah, exactly. Phi is computed at test time given the training dataset as input. Yeah. Uh, and let's go,",
    "start": "1366620",
    "end": "1373580"
  },
  {
    "text": "let's go through a couple more of the details here, uh, before we answer any more questions about this. So the, um, kind of,",
    "start": "1373580",
    "end": "1379700"
  },
  {
    "text": "this is, um, the- I've just covered what the objective is. Now, let's, let's actually look at this as an algorithm. So, uh, what we do is we f- if we wanna actually optimize this, uh,",
    "start": "1379700",
    "end": "1388009"
  },
  {
    "text": "we first sample a task i or whatever meta-training tasks or a mini-batch of tasks.",
    "start": "1388010",
    "end": "1394115"
  },
  {
    "text": "Uh, then we sample disjoint datasets from that task dataset, uh, which we'll refer to as D train and D test.",
    "start": "1394115",
    "end": "1401434"
  },
  {
    "text": "So if this is all of the data that we have for task i, then what we're gonna wanna do is we're gonna wanna partition this",
    "start": "1401435",
    "end": "1407360"
  },
  {
    "text": "into a training dataset and a test set for that task. Uh, and so in particular what we can do is we can basically pick, uh,",
    "start": "1407360",
    "end": "1413750"
  },
  {
    "text": "randomly select half of them to be used for the training dataset and half of them to be used for the test set at this iteration of the algorithm.",
    "start": "1413750",
    "end": "1419840"
  },
  {
    "text": "Then, we'll take the training dataset, uh, the- what's in the green box and use that to",
    "start": "1419840",
    "end": "1425510"
  },
  {
    "text": "compute the task per- specific parameters Phi i. And then we'll update our meta-parameters using the gradient of",
    "start": "1425510",
    "end": "1434315"
  },
  {
    "text": "the objective with respect to the meta-parameters using the computed task-specific parameters.",
    "start": "1434315",
    "end": "1439730"
  },
  {
    "text": "[NOISE] Uh, and then we'll repeat this, uh, iteratively using your favorite, uh,",
    "start": "1439730",
    "end": "1446840"
  },
  {
    "text": "gradient descent op- uh, optimizer, things like Adam, SGD, Momentum, et cetera. Yeah.",
    "start": "1446840",
    "end": "1452059"
  },
  {
    "text": "[BACKGROUND] So, so the question was,",
    "start": "1452060",
    "end": "1458120"
  },
  {
    "text": "uh, we're not compu- computing gradients using the training dataset. So what we're using is we're, um, computing gradients using the meta-training dataset, uh, of tasks.",
    "start": "1458120",
    "end": "1465875"
  },
  {
    "text": "And so the task-specific parameters are computed using D train, and the, uh, then we evaluate",
    "start": "1465875",
    "end": "1473150"
  },
  {
    "text": "those parameters using the test data-set for that meta-training task. So we've, kind of, lifted the,",
    "start": "1473150",
    "end": "1478805"
  },
  {
    "text": "the training datasets from, kind of, training datasets and test datasets to meta-training sets and meta-test tasks. So. Yeah.",
    "start": "1478805",
    "end": "1484850"
  },
  {
    "text": "[BACKGROUND]",
    "start": "1484850",
    "end": "1493850"
  },
  {
    "text": "You're asking if Theta, Theta is all meta parameters? [BACKGROUND] Yeah. So all of Theta is, uh,",
    "start": "1493850",
    "end": "1499145"
  },
  {
    "text": "are the meta parameters and then Phi are considered the task-specific parameters or the-",
    "start": "1499145",
    "end": "1504169"
  },
  {
    "text": "Theta- Phi is essentially not considered part of th- the meta parameters. [BACKGROUND] Um, we'll get into what Phi might be in a second.",
    "start": "1504170",
    "end": "1515690"
  },
  {
    "text": "It could be the base- basically, it could be the, the parameters of an entire neural network, uh,",
    "start": "1515690",
    "end": "1521900"
  },
  {
    "text": "it could also be something that's more compact, and I'll talk about that in a second. Yeah. [BACKGROUND] Yes. Yeah. That's the met-",
    "start": "1521900",
    "end": "1532700"
  },
  {
    "text": "[OVERLAPPING] [BACKGROUND]",
    "start": "1532700",
    "end": "1538610"
  },
  {
    "text": "Yeah. Yeah. Exactly. So we haven't touched any of the meta-test tasks, uh, that are, kind of, held out from the,",
    "start": "1538610",
    "end": "1544550"
  },
  {
    "text": "uh, task distribution. Yeah. [BACKGROUND] Right. So in this case,",
    "start": "1544550",
    "end": "1550380"
  },
  {
    "text": "for this particular network architecture, the order of the training datasets mat- the order of the,",
    "start": "1561760",
    "end": "1567110"
  },
  {
    "text": "the data points matters. Uh, and this actually isn't necessarily a good property because in many cases, the,",
    "start": "1567110",
    "end": "1572554"
  },
  {
    "text": "the, you have data sets not data lists, uh, for which the order doesn't matter.",
    "start": "1572555",
    "end": "1577865"
  },
  {
    "text": "Um, and so we'll see some architectures that- later see some architectures where you- they are permutation invariant.",
    "start": "1577865",
    "end": "1584540"
  },
  {
    "text": "Yeah. [inaudible] .",
    "start": "1584540",
    "end": "1591420"
  },
  {
    "text": "So in this case we, we compute Phi in Step 3, and then we, uh,",
    "start": "1591420",
    "end": "1598290"
  },
  {
    "text": "update the meta parameters Theta. So we do not update Phi, uh, itself.",
    "start": "1598290",
    "end": "1603360"
  },
  {
    "text": "It's- that is basically dynamically computed at every iteration of the meta-training process.",
    "start": "1603360",
    "end": "1608865"
  },
  {
    "text": "Uh, and then at test time, we're also going to be computing Phi given our meta parameters Theta.",
    "start": "1608865",
    "end": "1614520"
  },
  {
    "text": "[NOISE] Yeah.",
    "start": "1614520",
    "end": "1619740"
  },
  {
    "text": "[inaudible] we compute Phi and then we do maximization step of Phi,",
    "start": "1619740",
    "end": "1626790"
  },
  {
    "text": "and then run that, like, updated Phi back to the network? Yeah. So it's similar to that.",
    "start": "1626790",
    "end": "1633420"
  },
  {
    "text": "So the- we essentially compute- you can, you can view the, the,",
    "start": "1633420",
    "end": "1638835"
  },
  {
    "text": "the computation of the gradient with respect to Theta as basically back-propagating the loss from,",
    "start": "1638835",
    "end": "1646650"
  },
  {
    "text": "from y into Phi back in- back all the way into theta. We don't ever use that gradient to, to update Phi.",
    "start": "1646650",
    "end": "1652710"
  },
  {
    "text": "We only use it to update Theta, but it has to go through Phi in order to compute that gradient. Yeah.",
    "start": "1652710",
    "end": "1662460"
  },
  {
    "text": "[BACKGROUND]",
    "start": "1662460",
    "end": "1674429"
  },
  {
    "text": "Um, I'll talk a bit about architectures in a minute, but one of the nice- one of the things about LSTMs and,",
    "start": "1674430",
    "end": "1679680"
  },
  {
    "text": "and RNNs is that they can, er, process variable amounts of data relatively easily.",
    "start": "1679680",
    "end": "1684810"
  },
  {
    "text": "Uh, so you don't have to assume any data- any particular dataset size, although you should probably train it for the largest possible dataset size.",
    "start": "1684810",
    "end": "1692565"
  },
  {
    "text": "Um, but there are other neural network architectures that I'll talk about that you can use [NOISE] for this as well. [NOISE] Okay.",
    "start": "1692565",
    "end": "1701070"
  },
  {
    "text": "Um, so- now one of the challenges with this approach is that if, uh, if Phi is literally representing all the parameters of another neural network, uh,",
    "start": "1701070",
    "end": "1709800"
  },
  {
    "start": "1703000",
    "end": "2340000"
  },
  {
    "text": "it may not be that scalable to actually output, um, all of those neural network parameters because neural networks can be very large.",
    "start": "1709800",
    "end": "1716925"
  },
  {
    "text": "So, um, there are a couple of approaches for dealing with this, but the main,",
    "start": "1716925",
    "end": "1722570"
  },
  {
    "text": "kind of, way you can think about doing this is you don't need to necessarily output all of the parameters of a neural network.",
    "start": "1722570",
    "end": "1728255"
  },
  {
    "text": "You could instead just output the sufficient statistics of that task such that you could effectively make predictions for that task.",
    "start": "1728255",
    "end": "1736200"
  },
  {
    "text": "Um, and so what this looks like is, instead of having a neural network that outputs all of the parameters Phi,",
    "start": "1736200",
    "end": "1741659"
  },
  {
    "text": "it will output, uh, some set of sufficient statistics h. Er, and then this, uh,",
    "start": "1741660",
    "end": "1748020"
  },
  {
    "text": "like, some, some lower-dimensional vector h. And then your, uh, your neural network on the right will use those, uh,",
    "start": "1748020",
    "end": "1755550"
  },
  {
    "text": "sufficient statistics as well as other parameters n theta in order to make predictions.",
    "start": "1755550",
    "end": "1760560"
  },
  {
    "text": "Uh, and so what this lower-dimensional vector h might represent",
    "start": "1760560",
    "end": "1766515"
  },
  {
    "text": "is things like contextual task information. Uh, and then your new parameters Phi i are gonna correspond to hi as well as, er,",
    "start": "1766515",
    "end": "1778289"
  },
  {
    "text": "part of theta that will parameterize g. Uh, and so essentially,",
    "start": "1778290",
    "end": "1784500"
  },
  {
    "text": "the way that you can view the size is you can, uh, basically view this if you, uh, basically view this as a single LSTM that's taking in, uh, data points.",
    "start": "1784500",
    "end": "1792299"
  },
  {
    "text": "Uh, so one of the reasons why I named this h, is h is often used for the hidden state of LS- of an LSTM.",
    "start": "1792300",
    "end": "1797625"
  },
  {
    "text": "If you basically share all the parameters, uh, between both f on the left and g as,",
    "start": "1797625",
    "end": "1803760"
  },
  {
    "text": "for example, an LSTM, uh, then the task-specific parameters Phi are",
    "start": "1803760",
    "end": "1808950"
  },
  {
    "text": "represented by the hidden state of that LSTM as well as the parameters of, uh, the function on the right that are shared with the LSTM parameters.",
    "start": "1808950",
    "end": "1818529"
  },
  {
    "text": "Um, so one interesting connection here is that if you recall, uh, multi-task learning where we were concatenating task information z into the network.",
    "start": "1818960",
    "end": "1828675"
  },
  {
    "text": "Uh, you could view h, uh, essentially as a summarization of the task that is used to make predictions for that task.",
    "start": "1828675",
    "end": "1836100"
  },
  {
    "text": "So h and z, er, are very similar. Uh, in this case, unlike z, in the multi-task learning setting we're actually",
    "start": "1836100",
    "end": "1842340"
  },
  {
    "text": "learning the task representation h in this case, and we're learning how to produce that task representation",
    "start": "1842340",
    "end": "1848805"
  },
  {
    "text": "h given a small dataset of that task. [NOISE] Okay.",
    "start": "1848805",
    "end": "1856110"
  },
  {
    "text": "And so the, the fully general form of black-box neural networks is",
    "start": "1856110",
    "end": "1862520"
  },
  {
    "text": "a function that takes as input a trained dataset and a test input and produces a test output, um,",
    "start": "1862520",
    "end": "1868205"
  },
  {
    "text": "where Phi is, is somewhere, uh, in the middle of this network and may not actually be something that is, uh,",
    "start": "1868205",
    "end": "1875445"
  },
  {
    "text": "actually representing parameters per se. Yeah. [BACKGROUND]",
    "start": "1875445",
    "end": "1884670"
  },
  {
    "text": "Right, so the question is, can you explain what theta g means here. Um, so here basically theta g, uh,",
    "start": "1884670",
    "end": "1889830"
  },
  {
    "text": "represents all of the other parameters that this, uh, that this network g is representing other than h. Uh, so, uh,",
    "start": "1889830",
    "end": "1900240"
  },
  {
    "text": "this neural network right here that's making prediction about test- making predictions about test inputs we'll take as input h and we'll also have other parameters Theta g that we'll use to make predictions.",
    "start": "1900240",
    "end": "1909914"
  },
  {
    "text": "Theta g will be a part of the, the full parameter vector Theta. Uh, it may also share parameters with this part of the network right here. Yeah.",
    "start": "1909915",
    "end": "1923310"
  },
  {
    "text": "[BACKGROUND]",
    "start": "1923310",
    "end": "1937800"
  },
  {
    "text": "Yeah. So in this case, you might be sharing more, uh, more parameters between test time and, and training time.",
    "start": "1937800",
    "end": "1943590"
  },
  {
    "text": "[NOISE] Okay. Um, so this is the,",
    "start": "1943590",
    "end": "1950294"
  },
  {
    "text": "kind of, overview of black-box approaches. And let's now talk about what sort of architectures we could use for this",
    "start": "1950295",
    "end": "1957330"
  },
  {
    "text": "function f. So one of the first, um, well, I guess,",
    "start": "1957330",
    "end": "1962865"
  },
  {
    "text": "it's hard to say what, what came- comes first in research in general, but, um, one of the earlier approaches to these sorts of black-box approaches, um,",
    "start": "1962865",
    "end": "1970470"
  },
  {
    "text": "is using LSTMs or Neural Turing Machines, uh, that take as input the test inputs and, uh, basically the dataset,",
    "start": "1970470",
    "end": "1978180"
  },
  {
    "text": "uh, and be able to make- and use that to make predictions about new data points.",
    "start": "1978180",
    "end": "1983310"
  },
  {
    "text": "Um, LSTMs are, are probably something that's familiar to you. Uh, Neural Turing Machines are something that have more of",
    "start": "1983310",
    "end": "1989850"
  },
  {
    "text": "an external memory mechanism for which it can essentially store, uh, information about the training data points and then",
    "start": "1989850",
    "end": "1996045"
  },
  {
    "text": "access that information when making predictions about new data points. Uh, and it's it d- does this in a differentiable way.",
    "start": "1996045",
    "end": "2002480"
  },
  {
    "text": "Uh, you can also use something that [NOISE] uh, so, kind of, as was noted before, this is",
    "start": "2002480",
    "end": "2007925"
  },
  {
    "text": "not permutation-invariant because you're taking the data points sequentially. Uh, and you could also use an architecture that is permutation-invariant by",
    "start": "2007925",
    "end": "2015125"
  },
  {
    "text": "having a feed-forward function that takes as input each of your training data points, uh, x and y, uh, x1,",
    "start": "2015125",
    "end": "2021665"
  },
  {
    "text": "y1, x2, y2, et cetera, uh, and then aggregates that information using something like an average operation,",
    "start": "2021665",
    "end": "2028865"
  },
  {
    "text": "uh, to compute, uh, something that looks like, uh, in this case, what's denoted as a or r. Uh,",
    "start": "2028865",
    "end": "2035149"
  },
  {
    "text": "and then that is passed into another feed-forward network to make predictions about new data points.",
    "start": "2035150",
    "end": "2042870"
  },
  {
    "text": "Um, beyond these, er, these two types of architectures, there's a wide range of others that have been proposed that",
    "start": "2044140",
    "end": "2050990"
  },
  {
    "text": "have used other memory mechanisms, um, as well in combination with, uh, ideas from, kind of,",
    "start": "2050990",
    "end": "2056990"
  },
  {
    "text": "having slower weights and faster weights. Um, often when people use the term slow weights and fast weights, they refer to the task-specific parameters as fast weights and,",
    "start": "2056990",
    "end": "2065060"
  },
  {
    "text": "and the meta parameters as slow weights, uh, because one of them is updated much more quickly than the other one.",
    "start": "2065060",
    "end": "2070490"
  },
  {
    "text": "Uh, this is a concept that, uh, was developed by, um, folks in neuroscience actually, uh,",
    "start": "2070490",
    "end": "2076250"
  },
  {
    "text": "that have looked at kind of, the, the, um, how weights have been changing and how, uh, how synapses change in the brain.",
    "start": "2076250",
    "end": "2083659"
  },
  {
    "text": "Uh, and then, uh, there's also an architecture that has used a combination of attention mechanisms and convolutions.",
    "start": "2083660",
    "end": "2090770"
  },
  {
    "text": "Uh, so in this case, convolutions are go- actually going to be not permutation-invariant although attention-based architectures can be permutation-invariant.",
    "start": "2090770",
    "end": "2098720"
  },
  {
    "text": "Um, and, kind of, as a representative approach of, kind of, the black-box approaches in general, um,",
    "start": "2098720",
    "end": "2105425"
  },
  {
    "text": "this, this type of method that uses, uh, that uses, uh, convolutions and attentions is",
    "start": "2105425",
    "end": "2110540"
  },
  {
    "text": "able to do quite well on things like Omniglot, uh, getting around 97-99% accuracy on,",
    "start": "2110540",
    "end": "2118595"
  },
  {
    "text": "uh, things ranging from 5-way one-shot to 20-way five-shot Omniglot, uh,",
    "start": "2118595",
    "end": "2124160"
  },
  {
    "text": "and also does well on the min- ImageNet dataset that is performing, uh, like, five-way classification for actually real images from the ImageNet dataset. Yeah.",
    "start": "2124160",
    "end": "2135230"
  },
  {
    "text": "[inaudible] ?",
    "start": "2135230",
    "end": "2154600"
  },
  {
    "text": "Um, so the question is the- like, is there any, um, mechanisms relating to neural?",
    "start": "2154600",
    "end": "2159760"
  },
  {
    "text": "[BACKGROUND]",
    "start": "2159760",
    "end": "2166240"
  },
  {
    "text": "What do you mean by HI? [BACKGROUND]",
    "start": "2166240",
    "end": "2171970"
  },
  {
    "text": "Right. Okay. Um, I guess, I'm not, uh,",
    "start": "2171970",
    "end": "2177205"
  },
  {
    "text": "I'm not too familiar with the, the neuroscience literature, uh, to be able to comment on that in a,",
    "start": "2177205",
    "end": "2182590"
  },
  {
    "text": "uh, in a competent way. The- I guess, one thing I will say that has been somewhat inspired by the neuroscience literature is that people have looked at,",
    "start": "2182590",
    "end": "2190240"
  },
  {
    "text": "um, things that look like LSTMs, but do more of a Hebbian rule- update rule on that,",
    "start": "2190240",
    "end": "2196390"
  },
  {
    "text": "uh, on H- on HI, um, in order to, uh, kind of,",
    "start": "2196390",
    "end": "2201550"
  },
  {
    "text": "update the sufficient statistics with respect to a given task given your training datasets.",
    "start": "2201550",
    "end": "2206815"
  },
  {
    "text": "Um, other works from, um, I guess, one thing that is perhaps worth noting that we will",
    "start": "2206815",
    "end": "2212800"
  },
  {
    "text": "actually cover in one of the reading sessions, um, is that the- I guess,",
    "start": "2212800",
    "end": "2217885"
  },
  {
    "text": "there are a number of, uh, neuroscience, uh, researchers at DeepMind that have looked at these types of meta-learning methods.",
    "start": "2217885",
    "end": "2225670"
  },
  {
    "text": "Uh, and they have focused on, on actually these types of meta-learning methods more so than optimization-based or pan- on parametric approaches,",
    "start": "2225670",
    "end": "2233020"
  },
  {
    "text": "uh, using things like, like LSTMs. [NOISE] Yeah. [inaudible] .",
    "start": "2233020",
    "end": "2249550"
  },
  {
    "text": "Yeah. So in general, I'm, I'm under the opinion that Omniglot performance has saturated, uh, for the most part.",
    "start": "2249550",
    "end": "2256120"
  },
  {
    "text": "So, um, one of the algorithms that we'll be talking about later in this lecture, uh, gets, like, 99.9% accuracy on five-way five-shot Omniglot.",
    "start": "2256120",
    "end": "2265300"
  },
  {
    "text": "Uh, things that aren't solved are generation of Omniglot digits.",
    "start": "2265300",
    "end": "2270850"
  },
  {
    "text": "That's certainly something that's a lot harder and was actually proposed in the original paper. Uh, also, um, this is a bit of a nuanced point,",
    "start": "2270850",
    "end": "2278680"
  },
  {
    "text": "but the tr- the meta-trained meta-test split that they proposed in the original Omniglot paper is actually not the one that's",
    "start": "2278680",
    "end": "2283900"
  },
  {
    "text": "used in all the machine learning papers because it is a bit, uh, it proposes a train test split that doesn't have",
    "start": "2283900",
    "end": "2289555"
  },
  {
    "text": "quite enough training data points for these models to not overfit a lot. Um, and so if, if you're look- interested in looking at very efficient learning, uh,",
    "start": "2289555",
    "end": "2296905"
  },
  {
    "text": "then you- I think that performance isn't quite as saturated when you move towards the original meta-train meta-test split.",
    "start": "2296905",
    "end": "2302484"
  },
  {
    "text": "Um, but then it's just a matter of putting inductive biases into your network.",
    "start": "2302485",
    "end": "2307070"
  },
  {
    "text": "Okay. Um, so in homework 1, uh, you'll be implementing the- kind of,",
    "start": "2308610",
    "end": "2313780"
  },
  {
    "text": "the data processing pipeline for these meta-training algorithms that involve actually taking the Omniglot dataset,",
    "start": "2313780",
    "end": "2319690"
  },
  {
    "text": "for example, and actually loading images and, and plugging them into a neural network. This is actually a pretty fundamental part of these algorithms.",
    "start": "2319690",
    "end": "2326050"
  },
  {
    "text": "Uh, you'll also be implementing a very simple black-box meta-learner, uh, and also training a few-shot Omniglot classifier.",
    "start": "2326050",
    "end": "2333310"
  },
  {
    "text": "Uh, and you can use, kind of, uh, you can somewhat compare it to, uh, some of the numbers in these papers.",
    "start": "2333310",
    "end": "2339290"
  },
  {
    "text": "Okay. Um, so to wrap up black-box adaptation, uh, the pros and cons of this approach is that,",
    "start": "2339360",
    "end": "2346930"
  },
  {
    "start": "2340000",
    "end": "2490000"
  },
  {
    "text": "first, it's very expressive. So given that neural networks are universal function approximators,",
    "start": "2346930",
    "end": "2352165"
  },
  {
    "text": "these methods can represent any function of your training dataset. Uh, and they're also very easy to combine with a variety of learning problems,",
    "start": "2352165",
    "end": "2361839"
  },
  {
    "text": "for example, supervised learning or reinforcement learning. Uh, and later in this course we'll talk about how we can combine these methods with reinforcement learning.",
    "start": "2361840",
    "end": "2368605"
  },
  {
    "text": "Um, it's, it's- the, kind of, the spoiler is that it's, it's very straight forward, uh, as you might imagine with these types of models.",
    "start": "2368605",
    "end": "2374860"
  },
  {
    "text": "The downside to this approach is that, uh, in general, these neural networks are, are fairly complex because they need to be",
    "start": "2374860",
    "end": "2381940"
  },
  {
    "text": "taking in datasets and making predictions about new data points. They essentially need to figure out how to learn from data, uh,",
    "start": "2381940",
    "end": "2387790"
  },
  {
    "text": "and they need to do those- do this in a completely- like, basically completely from scratch. Like, at initialization, these,",
    "start": "2387790",
    "end": "2393160"
  },
  {
    "text": "these LSTMs were not built as, as optimization procedures, and they need to learn those optimization procedures,",
    "start": "2393160",
    "end": "2399265"
  },
  {
    "text": "uh, from scratch from the meta-training data. Uh, and as a result, they're often fairly data inefficient.",
    "start": "2399265",
    "end": "2407155"
  },
  {
    "text": "Um, and by this, I mean not data inefficient at meta-test time, but they often require a large number of, um, kind of,",
    "start": "2407155",
    "end": "2414220"
  },
  {
    "text": "a large amount of meta-training data, a large number of tasks in order to perform well.",
    "start": "2414220",
    "end": "2420200"
  },
  {
    "text": "Okay. Any questions on black-box approaches before we move on? Yeah.",
    "start": "2420240",
    "end": "2426100"
  },
  {
    "text": "[BACKGROUND]",
    "start": "2426100",
    "end": "2434770"
  },
  {
    "text": "So, um, I guess,",
    "start": "2434770",
    "end": "2448560"
  },
  {
    "text": "the- so the question was, there are other algorithms that take X-test as input. Um, and you could certainly, like,",
    "start": "2448560",
    "end": "2456085"
  },
  {
    "text": "you could certainly integrate X-test as much as possible in, uh, on- into the,",
    "start": "2456085",
    "end": "2461515"
  },
  {
    "text": "kind of, left-hand side of this diagram. Um, it's still, kind of, part of the input. And if you look at, kind of, the, the general form of, um,",
    "start": "2461515",
    "end": "2469914"
  },
  {
    "text": "of these algorithms, it, it- something that takes in the train dataset and the test input and you can really",
    "start": "2469915",
    "end": "2475150"
  },
  {
    "text": "design whatever architecture you want to integrate those pieces of information. Whether or not they're integrated, kind of, somewhat separately,",
    "start": "2475150",
    "end": "2481464"
  },
  {
    "text": "or treated somewhat separately, or integrated, uh, in the same part of the network, that, that's up to you.",
    "start": "2481465",
    "end": "2486950"
  },
  {
    "text": "Okay. Great. So let's talk about, um, optimization-based approaches.",
    "start": "2487620",
    "end": "2494380"
  },
  {
    "start": "2490000",
    "end": "2533000"
  },
  {
    "text": "So the- I guess, the motivation here is that, uh, as we talked about a bit before,",
    "start": "2494380",
    "end": "2499480"
  },
  {
    "text": "if we want to infer all of the parameters of a neural network, uh, having a neural network output them isn't a very scalable way to do that.",
    "start": "2499480",
    "end": "2508120"
  },
  {
    "text": "Uh, and instead, what we could do is, instead of, uh, treating this function as an inference problem,",
    "start": "2508120",
    "end": "2515020"
  },
  {
    "text": "we can instead treat it as an optimization procedure. Uh, and this is similar to what we do in supervised learning.",
    "start": "2515020",
    "end": "2520840"
  },
  {
    "text": "We treat, uh, parameter- like, i- inference of our parameters as an optimization problem not as necessarily,",
    "start": "2520840",
    "end": "2527050"
  },
  {
    "text": "uh, an in- inference problem. Um, this is where optimization-based meta-learning approaches come in.",
    "start": "2527050",
    "end": "2533350"
  },
  {
    "start": "2533000",
    "end": "2630000"
  },
  {
    "text": "So the key idea behind these methods is that we're gonna acquire our task-specific parameters phi i through optimization.",
    "start": "2533350",
    "end": "2541089"
  },
  {
    "text": "And then we'll differentiate through that optimization procedure to the meta parameters to optimize",
    "start": "2541090",
    "end": "2546640"
  },
  {
    "text": "for a set of met- meta parameters such that that optimization procedure for phi i leads to good performance.",
    "start": "2546640",
    "end": "2553135"
  },
  {
    "text": "Um, so how do we get started here? So the- I guess, you can essentially break down the,",
    "start": "2553135",
    "end": "2560275"
  },
  {
    "text": "the meta-training problem as, uh, as, kind of, having these two terms. One that's maximizing the likelihood of your training data given",
    "start": "2560275",
    "end": "2566859"
  },
  {
    "text": "your task-specific parameters and one that is, uh, optimizing, uh, your, uh,",
    "start": "2566860",
    "end": "2573040"
  },
  {
    "text": "task- the likelihood of your task-specific parameters under your meta parameters. Um, and so you can view this, kind of, this,",
    "start": "2573040",
    "end": "2580569"
  },
  {
    "text": "this equation right here as the optimization procedure that you wanna be able to do,",
    "start": "2580570",
    "end": "2586570"
  },
  {
    "text": "uh, at test time and also the optimization procedure that you're going to be integrating into your meta -learning problem during meta-training.",
    "start": "2586570",
    "end": "2593110"
  },
  {
    "text": "One that's basically going to be taking into account the training dataset, uh, and your accuracy on the training dataset as well as, uh, your prior,",
    "start": "2593110",
    "end": "2601255"
  },
  {
    "text": "which is given by phi given data where, where your meta parameters are, are parameterizing your prior.",
    "start": "2601255",
    "end": "2607160"
  },
  {
    "text": "All right. So your meta parameters are serving as your prior. Um, and now we need to think about, well,",
    "start": "2607230",
    "end": "2612895"
  },
  {
    "text": "what form of prior, uh, should we, should we basically impose using our meta parameters.",
    "start": "2612895",
    "end": "2620275"
  },
  {
    "text": "Um, well, one very successful form of prior knowledge that we've used in deep learning optimization is the initialization.",
    "start": "2620275",
    "end": "2628720"
  },
  {
    "text": "Um, and in particular, one of the things that's been quite successful in deep learning is what's called fine-tuning",
    "start": "2628720",
    "end": "2633865"
  },
  {
    "text": "where we take some set of initial parameters and then run gradient descent on training data for some new task.",
    "start": "2633865",
    "end": "2641635"
  },
  {
    "text": "Uh, and typically, this is not for just a single gradient step as written here, but for many gradient steps.",
    "start": "2641635",
    "end": "2646930"
  },
  {
    "text": "Uh, and this has worked really well. So, for example, if you look at, um, so- something that pre-trains on im- on ImageNet versus training from scratch, uh,",
    "start": "2646930",
    "end": "2655285"
  },
  {
    "text": "those are the two rows shown here, and fine-tuning either on the PASCAL dataset or on the SUN dataset.",
    "start": "2655285",
    "end": "2661194"
  },
  {
    "text": "Uh, and we see a huge difference in performance using pre-training, which is, uh, labeled as original in this paper,",
    "start": "2661195",
    "end": "2667135"
  },
  {
    "text": "um, versus using a random initialization. Um, great.",
    "start": "2667135",
    "end": "2673329"
  },
  {
    "text": "So typically, like, this is i- in many ways a valid approach to, kind of, the meta-learning problem where you first train, uh,",
    "start": "2673330",
    "end": "2680500"
  },
  {
    "text": "a cert- or pre-train a set of parameters on your meta-training data and then fine-tune on your dataset at test-time.",
    "start": "2680500",
    "end": "2687715"
  },
  {
    "text": "Um, now some questions that might come up is, where do you get your pre-training parameters? Uh, the typical way to do this is through- for,",
    "start": "2687715",
    "end": "2695380"
  },
  {
    "text": "for, for vision problems, the typical way to do this is by pre-training on ImageNet classification as using supervised learning.",
    "start": "2695380",
    "end": "2701800"
  },
  {
    "text": "Um, in language, one very popular approach for doing this is using models trained on large language corporas,",
    "start": "2701800",
    "end": "2709375"
  },
  {
    "text": "um, models like BERT or language models, uh, or other unsupervised learning techniques.",
    "start": "2709375",
    "end": "2716425"
  },
  {
    "text": "So pre-training of neural networks actually has a very long history. Before even- well before ImageNet,",
    "start": "2716425",
    "end": "2721750"
  },
  {
    "text": "our people were pre-training their, their models using unsupervised learning techniques and then fine tuning them. Um, although other than language,",
    "start": "2721750",
    "end": "2730450"
  },
  {
    "text": "it's not sure if- uh, it's not clear if those approaches have really been that popular recently. Um, but really, like,",
    "start": "2730450",
    "end": "2735744"
  },
  {
    "text": "if you have some domain, um, in many ways, kind of, the thing to do is just to take- train on some very large and diverse dataset and then fine-tune these- fine-tune",
    "start": "2735745",
    "end": "2743140"
  },
  {
    "text": "those parameters on whatever dataset you actually want to perform, uh, inference on.",
    "start": "2743140",
    "end": "2749305"
  },
  {
    "text": "Um, and it's also worth mentioning that pre-trained models are often available online. Uh, and so you can- actually you don't even",
    "start": "2749305",
    "end": "2755260"
  },
  {
    "text": "necessarily need to do this stuff where you actually trained on ImageNet. You can just download the parameters and then fine-tune from there.",
    "start": "2755260",
    "end": "2761710"
  },
  {
    "text": "Um, and then, I guess, the other thing worth mentioning here is that, er, fine-tuning is a bit of an art, er,",
    "start": "2761710",
    "end": "2768174"
  },
  {
    "text": "like other, other aspects of deep learning unfortunately. Um, and so there's a, a range of common practices for,",
    "start": "2768175",
    "end": "2775015"
  },
  {
    "text": "uh, performing fine-tuning successfully. This includes things like fine-tuning with a smaller learning rate, um, using a lower learning rate for lower layers of the network.",
    "start": "2775015",
    "end": "2783510"
  },
  {
    "text": "Um, uh, typically the low layers are- for many fine-tuning problems, typically, the low-level features are the things that need the change the least and",
    "start": "2783510",
    "end": "2790230"
  },
  {
    "text": "the higher-level concepts are the things that need the change the most for a new task. Uh, you may actually freeze earlier layers of the network.",
    "start": "2790230",
    "end": "2798130"
  },
  {
    "text": "Potentially even basically setting them to- with, uh, setting a learning rate of zero for those layers.",
    "start": "2798130",
    "end": "2803575"
  },
  {
    "text": "Uh, you could also consider re-initializing the last layer. Um, and then typically people search over these hyperparameters using cross-validation.",
    "start": "2803575",
    "end": "2812440"
  },
  {
    "text": "Um, and then the last thing worth mentioning here is that architecture choices tend to matter, uh, a lot when, when choosing how to fine-tune.",
    "start": "2812440",
    "end": "2819760"
  },
  {
    "text": "Er, for example, things like residual networks tend to be actually quite good at fine-tuning, um, because the gradients flow,",
    "start": "2819760",
    "end": "2826570"
  },
  {
    "text": "um, flow relatively easily through various parts of the network when you have residual connections.",
    "start": "2826570",
    "end": "2832270"
  },
  {
    "text": "Yeah. [inaudible]",
    "start": "2832270",
    "end": "2846329"
  },
  {
    "start": "2835000",
    "end": "3230000"
  },
  {
    "text": "So you're asking, basically when you're, when you're fine tuning, you're not actually using any information about the target?",
    "start": "2846330",
    "end": "2851920"
  },
  {
    "text": "Either fine tuning or [inaudible] We never exclusively encode which task we are using.",
    "start": "2856670",
    "end": "2865724"
  },
  {
    "text": "It's never an argument about using procedures. Yeah. So you're saying that, basically, we never are passing in any information about",
    "start": "2865725",
    "end": "2873510"
  },
  {
    "text": "the task as input to this approach or to the black box approaches. Yes, that's, uh, that's correct.",
    "start": "2873510",
    "end": "2879345"
  },
  {
    "text": "There's actually, uh, well- for meta-learning- well, there's actually some nuanced reasons for not doing that,",
    "start": "2879345",
    "end": "2887505"
  },
  {
    "text": "uh, which is kind of interesting in some way. It seems like in many ways you should pass in as much information you have about a task to the models so that they can use it.",
    "start": "2887505",
    "end": "2895140"
  },
  {
    "text": "But in fine tuning, for example, um, if you say passed in a one-hot vector for ImageNet, and then pass it in a different one-hot vector for your test task,",
    "start": "2895140",
    "end": "2902730"
  },
  {
    "text": "then, uh, it actually may, like- that, that information will- first,",
    "start": "2902730",
    "end": "2907859"
  },
  {
    "text": "if they're just two separate one-hot vectors, like, the, the test tasks that you're doing, like, versus train time, they're completely distinct things to the network.",
    "start": "2907860",
    "end": "2915329"
  },
  {
    "text": "Um, and so that, that information isn't something that can actually be used during the fine tuning process to,",
    "start": "2915330",
    "end": "2922170"
  },
  {
    "text": "to help it because it- it's only ever seen one task. And so kind of looking at another task,",
    "start": "2922170",
    "end": "2927329"
  },
  {
    "text": "for example, won't tell, tell you that it's doing a different task. Um, is it-",
    "start": "2927330",
    "end": "2933450"
  },
  {
    "text": "Sorry. Go, go ahead. I just want my [inaudible] because the only agent or person that",
    "start": "2933450",
    "end": "2940290"
  },
  {
    "text": "knows third tasks is only the person that's training the network because when you're going to test,",
    "start": "2940290",
    "end": "2946755"
  },
  {
    "text": "you know you will test with the test set [inaudible] about task. But if someone gave me a network and I was training",
    "start": "2946755",
    "end": "2953849"
  },
  {
    "text": "in this manner and I just got a bunch of clear images, how do I know there's no index tracking, right?",
    "start": "2953850",
    "end": "2960450"
  },
  {
    "text": "Like we had earlier and we had to mention approaches. So basically, [inaudible] network,",
    "start": "2960450",
    "end": "2965700"
  },
  {
    "text": "there's no way to know what parameters to use for a specific task, right?",
    "start": "2965700",
    "end": "2972550"
  },
  {
    "text": "The earlier, I know, oh, we are trying to- we have like a vehicle on each side and, uh, [inaudible] so maybe one has one [inaudible].",
    "start": "2974330",
    "end": "2984540"
  },
  {
    "text": "Yeah. They're recording and then I'm like and I'm, oh, there's a vehicle I should use, there's one, there's an [inaudible] , I should use this.",
    "start": "2984540",
    "end": "2990330"
  },
  {
    "text": "Yes. So you're asking me, so you could basically tell the network, um, like, you could train it- you could pre-train it on, like,",
    "start": "2990330",
    "end": "2995640"
  },
  {
    "text": "multitask learning, say, like, first, you- I'm gonna train you on, like, recognizing animals and recognizing plants and recognizing cars and something,",
    "start": "2995640",
    "end": "3002270"
  },
  {
    "text": "and tell it that it's going to be doing that, um, and we're, like, fine tuning on that, for example.",
    "start": "3002270",
    "end": "3007570"
  },
  {
    "text": "Uh, in this case the pre-training is just single task. Uh, and the test task is also single task,",
    "start": "3007570",
    "end": "3012710"
  },
  {
    "text": "so you don't actually tell it any information about kind of what task it's solving because it's kind of, um,",
    "start": "3012710",
    "end": "3018109"
  },
  {
    "text": "assume that you're gonna be fine tuning it on, on a new task and kind of both pre-training and testing our,",
    "start": "3018110",
    "end": "3023360"
  },
  {
    "text": "um, our separate tasks. Um, I'll get to the, to the point about why meta-learning doesn't pass on task information a bit later. Yeah.",
    "start": "3023360",
    "end": "3033335"
  },
  {
    "text": "So in this case, we're fine tuning the task specific parameters, but can we also do fine tuning on the shared meta-parameters?",
    "start": "3033335",
    "end": "3040625"
  },
  {
    "text": "So in this case, we're actually not- um, I guess the- in this case, this is- uh,",
    "start": "3040625",
    "end": "3049115"
  },
  {
    "text": "there isn't really a distinction between task specific parameters and meta-parameters. So what we're doing is we're just pre-training parameters Theta, which, basically,",
    "start": "3049115",
    "end": "3058730"
  },
  {
    "text": "could be your meta p- could be your meta-parameters Theta, and then the optimization process is producing your task specific parameters, Phi.",
    "start": "3058730",
    "end": "3067410"
  },
  {
    "text": "So it then includes both of them basically? [NOISE] Um, I would actually",
    "start": "3068410",
    "end": "3074720"
  },
  {
    "text": "sa- I would actually say that the pre-train parameters Theta are the meta-parameters.",
    "start": "3074720",
    "end": "3081560"
  },
  {
    "text": "Okay. Uh, and that initialization is affecte- is basically serving as a prior on your optimization,",
    "start": "3081560",
    "end": "3088925"
  },
  {
    "text": "uh, and in a somewhat implicit way. Uh, because basically the, the meta-train- the pre-trained parameters are kind- like",
    "start": "3088925",
    "end": "3095720"
  },
  {
    "text": "affecting the solution that the fine tuning process will give you. Yeah. Is the optimization [inaudible] [NOISE] like more generally trained neural network,",
    "start": "3095720",
    "end": "3104960"
  },
  {
    "text": "is that more of like a pruning thing or like using just at specificity?",
    "start": "3104960",
    "end": "3110960"
  },
  {
    "text": "You're saying, is this- is the fine tuning procedure pruning the network?",
    "start": "3110960",
    "end": "3116310"
  },
  {
    "text": "Yeah. Like wh- is it reducing weights and then be like- or adding specifically, like increasing weights? So in this case, it's actually just changing the weights.",
    "start": "3118000",
    "end": "3123590"
  },
  {
    "text": "So it's not, uh, removing or adding weights to the network. But, but like generally,",
    "start": "3123590",
    "end": "3128960"
  },
  {
    "text": "in your experience, what have you seen? Do you know if there is a general problem for that? Uh, so you're asking what, what do,",
    "start": "3128960",
    "end": "3134975"
  },
  {
    "text": "what do fine tuning procedures end up doing? Do they- um, I think that the- kind of the accepted wisdom of,",
    "start": "3134975",
    "end": "3144800"
  },
  {
    "text": "of what these are - things are doing are reusing features, and often are changing how those features are used,",
    "start": "3144800",
    "end": "3150950"
  },
  {
    "text": "uh, for a new task, but not necessarily changing the features that much themselves. Um, so- and that's kind of how like, the,",
    "start": "3150950",
    "end": "3157940"
  },
  {
    "text": "the later layers of the network are changing a lot and the features themselves are not changing a lot. Um, although I don't know if anyone has actually proven anything related to that. Yeah.",
    "start": "3157940",
    "end": "3167480"
  },
  {
    "text": "So, uh, two quick questions. What is- [inaudible]",
    "start": "3167480",
    "end": "3177260"
  },
  {
    "text": "Yeah. So typically, you'll use the same architecture or you might, uh, use the same architecture but like chop off the last layer.",
    "start": "3177260",
    "end": "3183424"
  },
  {
    "text": "Oh, okay. And when you're doing these updates and fine tuning, are you- you said you only provide one task at a time.",
    "start": "3183425",
    "end": "3188960"
  },
  {
    "text": "So- Right. So, uh, we'll get, we'll get how, how we can integrate this into a meta-learning approach on the next slide.",
    "start": "3188960",
    "end": "3196580"
  },
  {
    "text": "Uh, but yeah. So typically, what you do is you just pre-train parameters on a single task,",
    "start": "3196580",
    "end": "3201770"
  },
  {
    "text": "uh, and then fine tune on your test task. Okay. Um, great. So uh,",
    "start": "3201770",
    "end": "3211430"
  },
  {
    "text": "one other example of where this has been used is using, um- basically pre-training using",
    "start": "3211430",
    "end": "3216500"
  },
  {
    "text": "language models and then fine tuning on text classification tasks. Uh, and the plans in here are pretty interesting.",
    "start": "3216500",
    "end": "3222470"
  },
  {
    "text": "So they're showing that, uh, on the X-axis as you vary the number of training examples you have for the test task,",
    "start": "3222470",
    "end": "3227885"
  },
  {
    "text": "how does the, um, performance on that task vary?",
    "start": "3227885",
    "end": "3233015"
  },
  {
    "start": "3230000",
    "end": "3340000"
  },
  {
    "text": "And so what we see first is that there's a big difference between training from scratch versus training- uh,",
    "start": "3233015",
    "end": "3238535"
  },
  {
    "text": "using pre-trained parameters from universal language models or ULM. Uh, so that's the gap between the blue lines and the orange and green lines.",
    "start": "3238535",
    "end": "3246080"
  },
  {
    "text": "And then, uh, the second thing that we see is that, as you have fewer examples, uh, in your new task data set, uh,",
    "start": "3246080",
    "end": "3253700"
  },
  {
    "text": "performance gets worse, our error goes up. Uh, and so essentially what we see is that when you only have, for example,",
    "start": "3253700",
    "end": "3261290"
  },
  {
    "text": "100 data points for your test task here, uh, your performance actually isn't very good on your test task.",
    "start": "3261290",
    "end": "3267965"
  },
  {
    "text": "Uh, and you can expect that as you actually decrease that even lower, you would do even worse.",
    "start": "3267965",
    "end": "3273244"
  },
  {
    "text": "And so, uh, essentially, fine-tuning is, is much less effective when you have smaller data sets.",
    "start": "3273245",
    "end": "3280550"
  },
  {
    "text": "And now motivated by this, how about we design a meta-learning algorithm with the goal of being able to fine tune with small amounts of data at test time?",
    "start": "3280550",
    "end": "3289519"
  },
  {
    "text": "Uh, and in particular, what we could try to do is take our fine tuning procedure",
    "start": "3289519",
    "end": "3295250"
  },
  {
    "text": "and evaluate how well those task-specific parameters did on a test data set or on new data points.",
    "start": "3295250",
    "end": "3303965"
  },
  {
    "text": "And then actually optimize for your pre-trained parameters such that fine tuning gives you a set of test- uh,",
    "start": "3303965",
    "end": "3311660"
  },
  {
    "text": "gives you a set of parameters that do well on the test data points. Uh, and you could do this optimization across all of the tasks",
    "start": "3311660",
    "end": "3318515"
  },
  {
    "text": "in your meta-training ta- in your meta-training data set, such that fine tuning with small amounts of data leads to good generalization.",
    "start": "3318515",
    "end": "3326855"
  },
  {
    "text": "Um, so essentially, it'll be training for, uh, a set of parameters Theta across",
    "start": "3326855",
    "end": "3332150"
  },
  {
    "text": "many different tasks such that it can transfer effectively via fine tuning.",
    "start": "3332150",
    "end": "3336930"
  },
  {
    "text": "Um, okay, so kind of at a more intuitive level what this might look like.",
    "start": "3337420",
    "end": "3343174"
  },
  {
    "start": "3340000",
    "end": "3438000"
  },
  {
    "text": "And say Theta is the parameter vector that you're meta-learning- uh, your meta-parameters, and Phi i star is the optimal parameter vector for task i.",
    "start": "3343175",
    "end": "3351500"
  },
  {
    "text": "Then you can view the meta-training process of this optimization as the thick black line,",
    "start": "3351500",
    "end": "3357155"
  },
  {
    "text": "where when you're at this point during the meta-training process, and you take a gradient step with respect to task three,",
    "start": "3357155",
    "end": "3362750"
  },
  {
    "text": "you're quite far from the optimum for task three. Whereas, at the end of the meta-training process",
    "start": "3362750",
    "end": "3367849"
  },
  {
    "text": "you take a gradient step with respect to task three, you're quite close to the optimum. And likewise, for a range of other tasks.",
    "start": "3367850",
    "end": "3374850"
  },
  {
    "text": "Um, and we refer to this as the Model-Agnostic Meta-Learning algorithm, uh, in the sense that, uh,",
    "start": "3375280",
    "end": "3382565"
  },
  {
    "text": "it embeds this optimization procedure in a way that's agnostic to the model that's used and the loss function that's used,",
    "start": "3382565",
    "end": "3388760"
  },
  {
    "text": "as long as both of them are amenable to gradient-based optimization. Um, and then one other thing worth noting here is that this,",
    "start": "3388760",
    "end": "3396470"
  },
  {
    "text": "this diagram, I think, can be helpful for get- getting across the intuition of the method. Uh, but at the same time, it can be a bit misleading.",
    "start": "3396470",
    "end": "3401975"
  },
  {
    "text": "First, because parameter vectors do not exist in, in two-dimensions, uh, and also, or,",
    "start": "3401975",
    "end": "3407390"
  },
  {
    "text": "or neural network parameters do not exist in two dimensions, typically. Uh, and then also, there often isn't a single optimum, but,",
    "start": "3407390",
    "end": "3412684"
  },
  {
    "text": "but actually a whole space of optimums, um, for a whole space of optima for,",
    "start": "3412685",
    "end": "3418160"
  },
  {
    "text": "uh, neural network parameters. And so in many ways it's more about, um, not necessarily reaching a center point for these different algorithms,",
    "start": "3418160",
    "end": "3424400"
  },
  {
    "text": "but reaching a point, um, such that fine tuning will eventually- um, will, will get you to, uh,",
    "start": "3424400",
    "end": "3429980"
  },
  {
    "text": "a good part of the parameter space with, uh, with, with small amounts of data.",
    "start": "3429980",
    "end": "3436110"
  },
  {
    "text": "Okay. Um, so that was the objective, uh, what does this look like as an algorithm?",
    "start": "3436600",
    "end": "3442820"
  },
  {
    "start": "3438000",
    "end": "3600000"
  },
  {
    "text": "So we can take, um, the black box adaptation approach that we mentioned before, uh, and, uh, adapt it to the,",
    "start": "3442820",
    "end": "3450530"
  },
  {
    "text": "the optimization-based meta-learning case. Uh, and essentially what this does is you first sample a task,",
    "start": "3450530",
    "end": "3456155"
  },
  {
    "text": "you can- you sample your data sets. Uh, then instead of computing your task-specific parameters using a neural network,",
    "start": "3456155",
    "end": "3462755"
  },
  {
    "text": "you're going to be computing them using one or a few steps of fine tuning. And then you update your meta-parameters by",
    "start": "3462755",
    "end": "3471740"
  },
  {
    "text": "differentiating through those fine-tuning steps into your- uh, into the parameter vector Theta- into your initial set of parameters.",
    "start": "3471740",
    "end": "3479400"
  },
  {
    "text": "Okay. Any questions on this before I get into a few of the details? Yeah.",
    "start": "3479560",
    "end": "3487460"
  },
  {
    "text": "If you were to initialize a multitask network with the- whatever these learned weights would be,",
    "start": "3487460",
    "end": "3494900"
  },
  {
    "text": "versus trained multitasks from scratch, do you think the multitask network could do better with this prior?",
    "start": "3494900",
    "end": "3503450"
  },
  {
    "text": "So you're asking, um, what if you use multitask learning as an initialization instead?",
    "start": "3503450",
    "end": "3509420"
  },
  {
    "text": "First, you- you do the string method for some- some theta and then I guess [inaudible] ,",
    "start": "3509420",
    "end": "3516290"
  },
  {
    "text": "and then you use those as your initial weights for the multitask learning.",
    "start": "3516290",
    "end": "3521720"
  },
  {
    "text": "I see, so you're saying that you can basically pre-train to do this meta-training process to get an initia- initial set of weights,",
    "start": "3521720",
    "end": "3527180"
  },
  {
    "text": "and then use that as an initialization for multi-task learning? Yeah. Um, you could certainly do that.",
    "start": "3527180",
    "end": "3534710"
  },
  {
    "text": "I guess, what this is doing is optimizing that the meta- the meta-learning process that I mentioned, uh, on the previous slide is optimizing for",
    "start": "3534710",
    "end": "3541370"
  },
  {
    "text": "fast adaptation to individual tasks given an individual dataset. You could also do the same thing for pairs of tasks or triplets of tasks.",
    "start": "3541370",
    "end": "3549680"
  },
  {
    "text": "Uh, if you wanted to explicitly pre-train for three-shot learning or it's not, it's not three shot learning, three task learning.",
    "start": "3549680",
    "end": "3556160"
  },
  {
    "text": "Um, you could- I, I guess you could also consider doing something like- like optimizing it for, uh,",
    "start": "3556160",
    "end": "3562835"
  },
  {
    "text": "optimizing it on, on for a single-task adaptation and then fine tuning it on multitask adaptation, or multitask learning.",
    "start": "3562835",
    "end": "3569840"
  },
  {
    "text": "Um, it's hard to say how well that would do because it's not actually explicitly training for what it's going to be doing at task time, but,",
    "start": "3569840",
    "end": "3575240"
  },
  {
    "text": "uh, conceivably could do something effective. Yeah.",
    "start": "3575240",
    "end": "3580700"
  },
  {
    "text": "When you talked about initializing the weights data from a pre-train network like it was that- was that just",
    "start": "3580700",
    "end": "3585890"
  },
  {
    "text": "motivation for this algorithm or do you actually do that with your parameters theta or could you start with random initialization?",
    "start": "3585890",
    "end": "3591530"
  },
  {
    "text": "Right. So in this case we start with a random initialization before this meta-training algorithm, and that was mostly serving as for- basically as- as",
    "start": "3591530",
    "end": "3598520"
  },
  {
    "text": "motivation for how well pre-training can work, uh, in a ra- range of settings. Yeah.",
    "start": "3598520",
    "end": "3604430"
  },
  {
    "text": "[LAUGHTER] So does this work, uh, well, even with, like, one-shot learning because it seems like even with this approach that you",
    "start": "3604430",
    "end": "3611645"
  },
  {
    "text": "could risk over fitting on [inaudible]? Yeah, so this approach actually works really well even for",
    "start": "3611645",
    "end": "3617300"
  },
  {
    "text": "one-shot learning, two-shot learning etc. It's competitive with, um, with the black box approaches that I mentioned previously.",
    "start": "3617300",
    "end": "3624890"
  },
  {
    "text": "So how long do you typically run that in optimization move [inaudible]? Right, so for the one-shot setting you can you- typically",
    "start": "3624890",
    "end": "3632135"
  },
  {
    "text": "the data optimization is somewhere between one and five gradient steps. Um, and even with only a few gradient steps you can get quite far.",
    "start": "3632135",
    "end": "3639230"
  },
  {
    "text": "[NOISE] Okay. So one thing worth mentioning about this algorithm is that it brings up",
    "start": "3639230",
    "end": "3645800"
  },
  {
    "text": "second-order derivatives because we are optimizing, um, basically because we're, ah,",
    "start": "3645800",
    "end": "3652894"
  },
  {
    "text": "we're optimizing for a set of, uh, meta parameters. Ah, so this is- we have this gradient.",
    "start": "3652895",
    "end": "3658235"
  },
  {
    "text": "And inside of, ah, this inductive term we also have this gradient right here.",
    "start": "3658235",
    "end": "3664830"
  },
  {
    "text": "Uh, so I know you might be a little bit worried about this. So I- for example, if you need to compute the full Hessian of,",
    "start": "3665530",
    "end": "3672535"
  },
  {
    "text": "uh, of the neural network, we would be in a bit of trouble. Uh, and, um, what if you want more than one integrated step.",
    "start": "3672535",
    "end": "3680180"
  },
  {
    "text": "Does that give us higher-order, um, higher-order derivatives? And so I wanna go through a bit on the whiteboard, ah,",
    "start": "3680180",
    "end": "3686300"
  },
  {
    "text": "what this- what actually the meta gradient update looks like, uh, such that we can, kind of,",
    "start": "3686300",
    "end": "3693020"
  },
  {
    "text": "figure out the answers to these questions. [NOISE] Great. So let's say that,",
    "start": "3693020",
    "end": "3699650"
  },
  {
    "text": "um, for the sake of notation. So, uh, in this case I was writing out a gradient step as the update procedure.",
    "start": "3699650",
    "end": "3706040"
  },
  {
    "text": "Uh, and- and in this case I'm just gonna, uh,",
    "start": "3706040",
    "end": "3710670"
  },
  {
    "text": "use u to denote the, um, the update rule and that's gonna be a function of theta",
    "start": "3711370",
    "end": "3716810"
  },
  {
    "text": "and your training data points D train. So this is, uh, this is basically one, uh,",
    "start": "3716810",
    "end": "3723500"
  },
  {
    "text": "or a few steps of gradient descent, theta minus alpha grad theta loss with respect to D train.",
    "start": "3723500",
    "end": "3729440"
  },
  {
    "text": "Um, and I'm gonna use, uh, so you, kind of, just write out some- some notation.",
    "start": "3729440",
    "end": "3736010"
  },
  {
    "text": "I'm gonna use d to denote the total derivatives [NOISE] uh,",
    "start": "3736010",
    "end": "3743240"
  },
  {
    "text": "and [NOISE] the, uh, nabla symbol to denote partial derivatives.",
    "start": "3743240",
    "end": "3749030"
  },
  {
    "text": "[NOISE] And we'll see why this distinction actually matters,",
    "start": "3749030",
    "end": "3755540"
  },
  {
    "text": "uh, in a second. And this is just for the purpose of the white board, in all the slides we'll just be using the nabla symbol- the gradient symbol,",
    "start": "3755540",
    "end": "3763460"
  },
  {
    "text": "um, for, for both. Okay. So as you can see on the bottom of this slide, um,",
    "start": "3763460",
    "end": "3771829"
  },
  {
    "text": "the optimization procedure that we have [NOISE] looks something like an optimization parameter- meta parameters theta over",
    "start": "3771830",
    "end": "3779630"
  },
  {
    "text": "our loss function with respect to our task specific parameters five. [NOISE] And our test data points.",
    "start": "3779630",
    "end": "3787370"
  },
  {
    "text": "I'm gonna drop the i from the notation here just for notational simplicity.",
    "start": "3787370",
    "end": "3792935"
  },
  {
    "text": "And this is the same as [NOISE] an optimization over meta parameters of L of our update rule with regard to our training dataset,",
    "start": "3792935",
    "end": "3803524"
  },
  {
    "text": "uh, and our test dataset.",
    "start": "3803524",
    "end": "3809450"
  },
  {
    "text": "Okay. So this should all be clear from the board. And in order to optimize this objective function,",
    "start": "3809450",
    "end": "3817010"
  },
  {
    "text": "we need to be able to get the derivatives of this objective with respect to our meta parameters if we want to optimize this with gradient-based optimization,",
    "start": "3817010",
    "end": "3824390"
  },
  {
    "text": "things like Adam for example. Um, and so to do this, we need to be able to get, uh,",
    "start": "3824390",
    "end": "3831710"
  },
  {
    "text": "the derivatives [NOISE] of this objective with respect to [NOISE] our meta parameters data.",
    "start": "3831710",
    "end": "3838849"
  },
  {
    "text": "Uh, and so let's try to actually write out what this meta gradient looks like.",
    "start": "3838850",
    "end": "3844340"
  },
  {
    "text": "Uh, so in particular we can view this meta gradient. Um, first we can basically,",
    "start": "3844340",
    "end": "3849575"
  },
  {
    "text": "uh, compute the- with the chain rule, compute the derivative of the outer function, uh, and then,",
    "start": "3849575",
    "end": "3855860"
  },
  {
    "text": "uh, use the chain rule to compute the derivative of the- the inside with respect to theta. So what this looks like is we'll take",
    "start": "3855860",
    "end": "3862970"
  },
  {
    "text": "the derivative with respect to, uh, I'll use, kind of,",
    "start": "3862970",
    "end": "3868865"
  },
  {
    "text": "a placeholder variable phi bar of, um, L of phi bar and D test [NOISE] evaluated at phi bar equals u,",
    "start": "3868865",
    "end": "3885109"
  },
  {
    "text": "um, of theta comma D train. So this is just the derivative of the outer objective, uh,",
    "start": "3885110",
    "end": "3891740"
  },
  {
    "text": "times the derivative of the, um,",
    "start": "3891740",
    "end": "3897455"
  },
  {
    "text": "the derivative of the update rule with respect to D train. Yeah.",
    "start": "3897455",
    "end": "3904970"
  },
  {
    "text": "Can you write a little larger [inaudible]. Yes, I can try to write larger from here forward.",
    "start": "3904970",
    "end": "3911255"
  },
  {
    "text": "Um, so basically this is the- the derivative, the derivative of the first- of the outer loss, and this is d phi d theta.",
    "start": "3911255",
    "end": "3918484"
  },
  {
    "text": "Um, this is why partial derivatives are- matter because if we, kind of, just wrote this as, uh, as a full derivative,",
    "start": "3918485",
    "end": "3924830"
  },
  {
    "text": "then this would just be exactly the same as the- or basically be very similar to the, um, to the, uh, what was originally written.",
    "start": "3924830",
    "end": "3931685"
  },
  {
    "text": "Okay. Um, right, great. So notice that this is, uh, this is, like,",
    "start": "3931685",
    "end": "3937520"
  },
  {
    "text": "ah, a row vector. Uh, I need to write bigger. [NOISE] And, uh, this is a matrix.",
    "start": "3937520",
    "end": "3948290"
  },
  {
    "text": "[NOISE] Um, and so the result is a row vector. Um, this can be computed with a single backward pass through the neural network.",
    "start": "3948290",
    "end": "3956900"
  },
  {
    "text": "So this is, uh, you could just set the parameters of your neural network to phi bar and then compute the derivative of this loss function with respect to those parameters.",
    "start": "3956900",
    "end": "3964220"
  },
  {
    "text": "So this is just one backward pass. Um, this is differentiating through the update process itself.",
    "start": "3964220",
    "end": "3971075"
  },
  {
    "text": "Um, so this is the part that is a little bit trickier to deal with.",
    "start": "3971075",
    "end": "3976700"
  },
  {
    "text": "Okay. Um, so let's try to actually compute what this looks like. So, um, [NOISE] we can let u theta D train.",
    "start": "3976700",
    "end": "3988685"
  },
  {
    "text": "Let's just start with the case where we have [NOISE] a single gradient step.",
    "start": "3988685",
    "end": "3993900"
  },
  {
    "text": "Um, [NOISE]",
    "start": "3994000",
    "end": "4004299"
  },
  {
    "text": "then in this case, uh, then we can try and take the derivative of this. So, uh, derivative of the update rule with respect to theta.",
    "start": "4004299",
    "end": "4016780"
  },
  {
    "text": "This is going to equal the identity matrix, uh, minus alpha,",
    "start": "4016780",
    "end": "4024100"
  },
  {
    "text": "uh, d d theta squared of L of theta,",
    "start": "4024100",
    "end": "4030099"
  },
  {
    "text": "um, comma D train. [NOISE] Uh, and this is the- [NOISE] this is the Hessian of the neural network.",
    "start": "4030099",
    "end": "4041540"
  },
  {
    "text": "Any questions with this? Okay. So, uh, if we didn't plug this term into here,",
    "start": "4043530",
    "end": "4052390"
  },
  {
    "text": "then what we get, uh, is that we- this is, we have this vector.",
    "start": "4052390",
    "end": "4057400"
  },
  {
    "text": "We simply need to be doing- do a vector matrix multiplication. Uh, and fortunately this means that we don't actually have to compute",
    "start": "4057400",
    "end": "4064630"
  },
  {
    "text": "the full Hessian of the neural network because we ha- because we have this. All we need to compute is this Hessian vector product.",
    "start": "4064630",
    "end": "4070960"
  },
  {
    "text": "Uh, and there are much more efficient ways to compute Hessian vector products via backpropagation for neural networks than,",
    "start": "4070960",
    "end": "4079090"
  },
  {
    "text": "uh, that don't require you to construct the entire Hessian of the neural network. Uh, it also turns out the standard neural network",
    "start": "4079090",
    "end": "4084970"
  },
  {
    "text": "different- automatic differentiation libraries like TensorFlow and PyTorch will actually perform this Hessian vector computation for you,",
    "start": "4084970",
    "end": "4093100"
  },
  {
    "text": "uh, such that you- in an efficient way that amounts to essentially performing, um,",
    "start": "4093100",
    "end": "4099505"
  },
  {
    "text": "additional backward passes such that you don't actually have to worry about coding this up yourself, um, which is very convenient.",
    "start": "4099505",
    "end": "4106029"
  },
  {
    "text": "[NOISE] Okay. So that's the case if we had just a single neural- a single gradient step in the,",
    "start": "4106030",
    "end": "4114600"
  },
  {
    "text": "um, in the inner loop. What if we have multiple inner gradient steps in the inner loop?",
    "start": "4114600",
    "end": "4120720"
  },
  {
    "text": "Uh, and so in particular what if we have, um, u theta comma D train.",
    "start": "4120720",
    "end": "4129525"
  },
  {
    "text": "What if we have two gradient steps. So this is gonna equal theta minus alpha d theta",
    "start": "4129525",
    "end": "4139895"
  },
  {
    "text": "of L theta D train. Uh, let's call this intermediate set of parameters theta prime.",
    "start": "4139895",
    "end": "4151855"
  },
  {
    "text": "And then we will have a second gradient step that is",
    "start": "4151855",
    "end": "4160674"
  },
  {
    "text": "with respect to theta prime of L of theta prime D train.",
    "start": "4160675",
    "end": "4169119"
  },
  {
    "text": "Is that behind the thing? I think that's still there. Okay. So this is two gradient steps.",
    "start": "4169120",
    "end": "4175075"
  },
  {
    "text": "Um, if we then want to compute the derivative of this, [NOISE] then what we get is,",
    "start": "4175075",
    "end": "4184000"
  },
  {
    "text": "we first get the first two terms that we had before which is the identity minus the,",
    "start": "4184000",
    "end": "4189565"
  },
  {
    "text": "um, minus the Hessian. [NOISE]",
    "start": "4189565",
    "end": "4198000"
  },
  {
    "text": "Uh, and then, what about the second term? So we want to compute the derivative of this last term with respect to the parameters Theta.",
    "start": "4198000",
    "end": "4206385"
  },
  {
    "text": "Uh, and what this looks like is, uh, first you compute the derivative of the outside.",
    "start": "4206385",
    "end": "4213120"
  },
  {
    "text": "So you get, uh, D Theta prime squared.",
    "start": "4213120",
    "end": "4218430"
  },
  {
    "text": "Uh, now I'll use Theta bar here of,",
    "start": "4218430",
    "end": "4223890"
  },
  {
    "text": "um, of Theta bar D train. This is going to be evaluated at, um, at",
    "start": "4223890",
    "end": "4231330"
  },
  {
    "text": "Theta prime times D Theta prime D Theta.",
    "start": "4231330",
    "end": "4237300"
  },
  {
    "text": "Er, and so, er, first this, this term right here is just equal to the first two terms.",
    "start": "4237300",
    "end": "4245805"
  },
  {
    "text": "Er, and one of the nice things that we get here is that, er, we don't get third-order terms here.",
    "start": "4245805",
    "end": "4251085"
  },
  {
    "text": "So we get the Hessian evaluated at Theta prime which is the parameters after the first gradient step.",
    "start": "4251085",
    "end": "4256965"
  },
  {
    "text": "Er, and we get the Hessian with respect to, er, the original parameters but we don't get anything,",
    "start": "4256965",
    "end": "4263625"
  },
  {
    "text": "um, any third order derivatives basically. Er, and again this is something that we can efficiently compute.",
    "start": "4263625",
    "end": "4270735"
  },
  {
    "text": "Well, if initially- basically compete with additional backward passes without having to basically construct,",
    "start": "4270735",
    "end": "4276630"
  },
  {
    "text": "er, any full Hessians or without having to compute higher-order derivatives which is nice.",
    "start": "4276630",
    "end": "4283750"
  },
  {
    "text": "Okay, er, and then as you might imagine if you continue to run this, um, continue to kind of compute this for, um,",
    "start": "4284360",
    "end": "4291465"
  },
  {
    "text": "for even more gradient steps in the inner loop, you basically continue to get these types of",
    "start": "4291465",
    "end": "4298260"
  },
  {
    "text": "terms that pop-up without higher-order terms. Okay. Any questions on, um,",
    "start": "4298260",
    "end": "4305969"
  },
  {
    "text": "on some of the math? Okay. So yeah.",
    "start": "4305970",
    "end": "4315179"
  },
  {
    "text": "Er, in the computation of the second derivative term, aren't you trying to take the derivative with respect to",
    "start": "4315180",
    "end": "4321390"
  },
  {
    "text": "Theta of the derivative of Theta prime. But then you wrote the derivative squared was like the Theta prime bar.",
    "start": "4321390",
    "end": "4327840"
  },
  {
    "text": "So how, how does that happen? So if you're trying to differentiate this third term,",
    "start": "4327840",
    "end": "4335230"
  },
  {
    "text": "um, with respect to Theta, you first kind of take the derivative of the outer function with respect to, um,",
    "start": "4335230",
    "end": "4341405"
  },
  {
    "text": "its arguments times the- times this term which is the d of the chain rule.",
    "start": "4341405",
    "end": "4348545"
  },
  {
    "text": "Okay and sorry that this likes to float upward but okay.",
    "start": "4348545",
    "end": "4354449"
  },
  {
    "text": "Cool. So, um, now we've talked about authorization ba- authorization based approaches,",
    "start": "4356290",
    "end": "4363015"
  },
  {
    "text": "um, or at least the kind of th- the basics of them. Let's think about how they compare to black-box approaches.",
    "start": "4363015",
    "end": "4369344"
  },
  {
    "text": "So you can view Black-Box adaptation as having this general form that it takes as input a training data set and a test input.",
    "start": "4369345",
    "end": "4376980"
  },
  {
    "text": "Um, for example using something like a recurrent neural network or something like that. Now, you could also view MAML, or model-agnostic meta-learning as also, er,",
    "start": "4376980",
    "end": "4388139"
  },
  {
    "text": "taking a training data set and a test input where you have this function",
    "start": "4388140",
    "end": "4393750"
  },
  {
    "text": "Phi that takes as input the test input and the parameters Phi are computed with gradient descent.",
    "start": "4393750",
    "end": "4400590"
  },
  {
    "text": "Um, so essentially you can view MAML as a computation graph with",
    "start": "4400590",
    "end": "4406440"
  },
  {
    "text": "this funny embedded gradient operator inside that computation graph. So if you kind of take this view,",
    "start": "4406440",
    "end": "4413580"
  },
  {
    "text": "that means you can potentially mix and match components of, um, of these approaches. For example, um, one paper that looks at can you learn init- initialization,",
    "start": "4413580",
    "end": "4423435"
  },
  {
    "text": "er, but replace the gradient update that MAML does with a learned neural network that produces that gradient update.",
    "start": "4423435",
    "end": "4429420"
  },
  {
    "text": "So for example instead of having, er, instead of learning the initialization then running gradient descent,",
    "start": "4429420",
    "end": "4434460"
  },
  {
    "text": "you could learn initialization and have a neural network output your, your gradient update. Er, and this was done in,er,",
    "start": "4434460",
    "end": "4441510"
  },
  {
    "text": "Ravi and Larochelle in 2017. And this paper actually precedes the MAML paper.",
    "start": "4441510",
    "end": "4446895"
  },
  {
    "text": "But I, I mentioned it here, er, just for the purpose of understanding different things.",
    "start": "4446895",
    "end": "4452830"
  },
  {
    "text": "Okay. Um, and this computation graph view of meta-learning will come back again, er, later.",
    "start": "4452840",
    "end": "4461985"
  },
  {
    "text": "Okay. Now, one other thing to think about, er, is some of like how these approaches not just compare conceptually,",
    "start": "4461985",
    "end": "4469890"
  },
  {
    "text": "but also in practice and in theory. So, um, one question to think about that was actually mentioned a bit before is,",
    "start": "4469890",
    "end": "4477620"
  },
  {
    "text": "er, what if your test task is different than the meta-training tasks that you were optimizing on?",
    "start": "4477620",
    "end": "4482960"
  },
  {
    "text": "And so this is a question that we studied empirically to some degree and we were aiming to",
    "start": "4482960",
    "end": "4489660"
  },
  {
    "text": "compare MAML to black-box type approaches, er, such as SNAIL that the, the, the architecture that used attention and convolutions, er,",
    "start": "4489660",
    "end": "4497340"
  },
  {
    "text": "as well as MetaNetworks which is also one of the architectures that I showed before. Er, and we looked at, er,",
    "start": "4497340",
    "end": "4502920"
  },
  {
    "text": "Omniglot classification where we tried to vary the tasks, er, and see how the performance did as you vary",
    "start": "4502920",
    "end": "4510150"
  },
  {
    "text": "the tasks away from the Meta-training distribution. So in this case the X-axis will show the task variability,",
    "start": "4510150",
    "end": "4516525"
  },
  {
    "text": "and the Y-axis is going to show performance. And so in the first study we looked at we, um,",
    "start": "4516525",
    "end": "4521864"
  },
  {
    "text": "we skewed the digits in the Omniglot data set. So it was trained on digits that were, er, kind of in the center.",
    "start": "4521865",
    "end": "4528344"
  },
  {
    "text": "And then we moved, er, kind of away from the meta-training task distribution training it on, er,",
    "start": "4528345",
    "end": "4533460"
  },
  {
    "text": "testing its ability to adopt to tasks that involve skewed digits. And what we saw at first is that all the approaches, er,",
    "start": "4533460",
    "end": "4540960"
  },
  {
    "text": "their performance deteriorated as you moved away from the meta-trained distribution. But we saw that, er,",
    "start": "4540960",
    "end": "4546390"
  },
  {
    "text": "algorithms like MAML are better able to perform these out of distribution tasks, er,",
    "start": "4546390",
    "end": "4552285"
  },
  {
    "text": "as you move away from the meta-training distribution because they're performing, er, an optimization procedure at test time.",
    "start": "4552285",
    "end": "4558945"
  },
  {
    "text": "So because you're running gradient descent at test time you can still expect it to give you some reasonable answer.",
    "start": "4558945",
    "end": "4564540"
  },
  {
    "text": "Er, at least an answer that achieves good accuracy on the training data set for example. Whereas black-box approaches that are,",
    "start": "4564540",
    "end": "4571275"
  },
  {
    "text": "are just taking in a data set as input and producing an answer. Um, when you, when you move away from the training distribution, there's really, er,",
    "start": "4571275",
    "end": "4577905"
  },
  {
    "text": "nothing that you can say about what those algorithms are doing. Because they- yeah. Yeah. Er, and then if you look at something like the scale of the digits, er,",
    "start": "4577905",
    "end": "4585960"
  },
  {
    "text": "we also see this sharp drop off as you move away from the training, er, meta-training data set. But we kind of consistently saw this pattern that",
    "start": "4585960",
    "end": "4592920"
  },
  {
    "text": "optimization based approaches were better at extrapolating because they were still giving you, um, a procedure at test time that looked like an optimization procedure.",
    "start": "4592920",
    "end": "4602590"
  },
  {
    "text": "Um, so this is one empirical trend that we noticed. Er, and then you might ask well we're embedding",
    "start": "4602780",
    "end": "4608400"
  },
  {
    "text": "the structure of optimization into the Mediterranean process. Does this come at a cost?",
    "start": "4608400",
    "end": "4613560"
  },
  {
    "text": "And in particular one very natural thing that was actually brought up a bit before is how far can you actually get with a single gradient step or a few gradient steps?",
    "start": "4613560",
    "end": "4620670"
  },
  {
    "text": "Are these methods actually as expressive as the black-box approaches that I mentioned before?",
    "start": "4620670",
    "end": "4626220"
  },
  {
    "text": "Um, and it turns out that you can show that, um, for a sufficiently deep function F,",
    "start": "4626220",
    "end": "4632670"
  },
  {
    "text": "the MAML algorithm, the MAML function that I mentioned before can approximate any function of the training data set and the test input.",
    "start": "4632670",
    "end": "4641864"
  },
  {
    "text": "Um, it can basically represent anything that the black-box approaches can represent, under a few, er, fairly mild assumptions.",
    "start": "4641865",
    "end": "4649560"
  },
  {
    "text": "Under the assumptions is that the inner learning rate is non-zero, er, that the loss function gradient doesn't lose information about the label,",
    "start": "4649560",
    "end": "4657210"
  },
  {
    "text": "the standard-like Mean Squared Error and Cross entropy loss functions fall under this category, er, and also that the data points in your training data set are unique.",
    "start": "4657210",
    "end": "4666550"
  },
  {
    "text": "And the reason why this is interesting, er, is that it means that MAML has the benefit of the inductive bias of",
    "start": "4666590",
    "end": "4673800"
  },
  {
    "text": "gradient descent without losing expressive power. Yeah.",
    "start": "4673800",
    "end": "4679170"
  },
  {
    "text": "What do you mean by inductive bias? Um, what I mean by that is that at initialization like even before you do any Meta-training for MAML,",
    "start": "4679170",
    "end": "4686820"
  },
  {
    "text": "you still have, er, an optimization procedure that's going to point you roughly in the right direction. So you're still running gradient descent and you'll still be",
    "start": "4686820",
    "end": "4693719"
  },
  {
    "text": "able to improve on your training data. Yeah.",
    "start": "4693720",
    "end": "4701489"
  },
  {
    "text": "Is that- are these assumptions- [inaudible] are there any number of gradient steps or just assuming [inaudible]",
    "start": "4701490",
    "end": "4706710"
  },
  {
    "text": "This is actually only for a single gradient step. What is sufficiently [inaudible]?",
    "start": "4706710",
    "end": "4711720"
  },
  {
    "text": "Very deep. [LAUGHTER]. Do you know how- is there like an order of the [inaudible]?",
    "start": "4711720",
    "end": "4717840"
  },
  {
    "text": "Exponential. Yeah so the,",
    "start": "4717840",
    "end": "4723375"
  },
  {
    "text": "the, the- I guess the assumptions that I listed here are very mild. The sufficiently deep function is, er, is not mild.",
    "start": "4723375",
    "end": "4728940"
  },
  {
    "text": "Er, it does need to be very deep. And you could probably relax this assumption if you made other assumptions about the gradient pointing in the right direction,",
    "start": "4728940",
    "end": "4737955"
  },
  {
    "text": "um, or other things about the, the optimization.",
    "start": "4737955",
    "end": "4743550"
  },
  {
    "text": "It sounds kind of like the sufficiently lied single hidden layer. Yeah. Yeah. Okay so we're running out of time.",
    "start": "4743550",
    "end": "4753045"
  },
  {
    "text": "Um, the- let's see. One thing I want to mention, um,",
    "start": "4753045",
    "end": "4760005"
  },
  {
    "text": "I guess we can probably just leave off where, um, leave off where I left off on Monday next week.",
    "start": "4760005",
    "end": "4769305"
  },
  {
    "text": "But we basically covered the basics of, of optimization-based meta-learning, er, and I'll cover, er,",
    "start": "4769305",
    "end": "4775320"
  },
  {
    "text": "I'll cover the rest of it and some of it. G- go into a bit more of the advanced topics on Monday next week. On Wednesday this week, we have,",
    "start": "4775320",
    "end": "4781680"
  },
  {
    "text": "um, applications of meta-learning, and multitask learning to things like imitation learning and generative models, drug discovery and machine translation.",
    "start": "4781680",
    "end": "4788160"
  },
  {
    "text": "Er, I think that this will actually be pretty exciting to actually see some of the real-world use cases of these algorithms. Er, these will be student presentations and discussions.",
    "start": "4788160",
    "end": "4795330"
  },
  {
    "text": "And then on Monday I'll wrap up optimization-based meta learning and cover, er, non-parametric methods and talk about how all of these different approaches compare.",
    "start": "4795330",
    "end": "4803159"
  },
  {
    "text": "Great. I'll see you on Wednesday.",
    "start": "4803160",
    "end": "4805690"
  }
]