[
  {
    "start": "0",
    "end": "508000"
  },
  {
    "text": "All right, we're gonna go ahead and get started. Um, I want to start the class with some stuff about some logistics,",
    "start": "4350",
    "end": "10290"
  },
  {
    "text": "um, as well as sort of to address some questions, um, that have come up on Piazza about the grades on the midterm and some people have",
    "start": "10290",
    "end": "16400"
  },
  {
    "text": "concerns about what that might mean for their grades on the final class. Um, it was interesting to go back and compare",
    "start": "16400",
    "end": "23170"
  },
  {
    "text": "the means and the distributions on the midterm last year, it's near identical. Um, so last year the mean was about 69%,",
    "start": "23170",
    "end": "30029"
  },
  {
    "text": "this year it was about 71%. Um, and you see pretty similar distributions. Uh, the one on the top is our- Oh,",
    "start": "30030",
    "end": "38269"
  },
  {
    "text": "no, the one on the bottom is ours. So you can see this is 2019, this is 2018. [NOISE] Okay.",
    "start": "38270",
    "end": "45075"
  },
  {
    "text": "So they look pretty similar distributions. Um, we don't do an official curve for the class.",
    "start": "45075",
    "end": "50110"
  },
  {
    "text": "If anybody's getting over 90%, I always consider, even if everybody gets over 90%, that that means that those people all",
    "start": "50110",
    "end": "57240"
  },
  {
    "text": "understand the material well enough to deserve an A. Um, and then if we have really, um, abnormal distributions that sometimes we curve below that.",
    "start": "57240",
    "end": "64725"
  },
  {
    "text": "But just to give you a sense, um, last year, about 42% of people got an A in the class. So for those of you that are concerned about your midterm performance and concerned about",
    "start": "64725",
    "end": "72299"
  },
  {
    "text": "your final grade and whether it's still possible to do well in the class, it definitely is. So, so does anybody have any questions about the midterm?",
    "start": "72300",
    "end": "79409"
  },
  {
    "text": "I know we've had some regrade- regrade requests and we're going through those as quickly as we can. Yeah. Is there any [inaudible] I'm supposed [NOISE] I'm very curious [inaudible]",
    "start": "79410",
    "end": "88870"
  },
  {
    "text": "but is there any distribution available per, [NOISE] per question? Cause like- I feel like, for example, for me,",
    "start": "88870",
    "end": "95550"
  },
  {
    "text": "I just ran out of time on the last question and I wonder if that's. Yes, we have that information.",
    "start": "95550",
    "end": "100560"
  },
  {
    "text": "Um, I'll double-check with the TAs that there's no reason we shouldn't release that, I don't think there is. So, um, we, Gradescope gives us",
    "start": "100560",
    "end": "106130"
  },
  {
    "text": "full distributions for all the questions. So we can release that. Um, a lot of people ran out of time, uh, the last problem was definitely the hardest.",
    "start": "106130",
    "end": "111970"
  },
  {
    "text": "So that was where we saw the biggest variation and so that's where we tried to be particularly careful on that rubric.",
    "start": "111970",
    "end": "117580"
  },
  {
    "text": "Um, and we very much tried to make sure that if you're doing algebraic mistakes throughout the exam, that that was worth very little and we were focusing on the conceptual understanding.",
    "start": "117580",
    "end": "126110"
  },
  {
    "text": "Any other questions about the midterm? So I'll just write down per- per problem breakdown.",
    "start": "126400",
    "end": "132650"
  },
  {
    "text": "Basically, when we're going through it, we try to look at any problem that had really high variance, um, and then step through the rubric again to make sure that we're being fair.",
    "start": "132650",
    "end": "140730"
  },
  {
    "text": "Oh, one other thing I- which is we're gonna continue to accept regrade requests for the midterm through Friday.",
    "start": "140730",
    "end": "146995"
  },
  {
    "text": "And after that, it will be closed. [NOISE] Okay. So that's the midterm.",
    "start": "146995",
    "end": "152095"
  },
  {
    "text": "Um, hopefully, that helps, sort of, quell some concerns from at least some of the people in my class.",
    "start": "152095",
    "end": "157540"
  },
  {
    "text": "The other thing that I wanted to bring up right now is, um, the quiz. So the quiz is gonna be in about two weeks,",
    "start": "157540",
    "end": "164330"
  },
  {
    "text": "a little less than two weeks. It's a weird format. We do this for a reason. Um, I think one of the big tensions in classes that have",
    "start": "164330",
    "end": "170920"
  },
  {
    "text": "big final projects is whether to do a big final project and a big final exam, uh, which I think is a lot of [LAUGHTER] a lot for students to do both on,",
    "start": "170920",
    "end": "179065"
  },
  {
    "text": "um, and otherwise why go to class after the midterm? [LAUGHTER] I mean, now why, why, you know, how do we, uh,",
    "start": "179065",
    "end": "184799"
  },
  {
    "text": "make sure that there's a reason to learn about the material in the second half of the course which we do think is valuable and particularly",
    "start": "184800",
    "end": "190490"
  },
  {
    "text": "is often covering [NOISE] more important recent topics, um, but without doing a really high-stakes large exam.",
    "start": "190490",
    "end": "196084"
  },
  {
    "text": "So I, I talked to a number of people in the sort of teaching, uh, the teaching center here called VPTL.",
    "start": "196085",
    "end": "202585"
  },
  {
    "text": "Um, and the idea that we came up with is to do a low-stakes quiz, and the idea is that it's fun.",
    "start": "202585",
    "end": "209295"
  },
  {
    "text": "Um, and I have heard from multiple people that this is actually true, that's the design of it. The design is it's gonna be a two-part quiz. It's multiple choice.",
    "start": "209295",
    "end": "216950"
  },
  {
    "text": "It's all supposed to be about, sort of, high-level conceptual questions. We'll release last year's, so you guys can see an exa, uh, example.",
    "start": "216950",
    "end": "223640"
  },
  {
    "text": "And so the idea is you do it in two parts. You first do it individually, that takes around 45 minutes, and then you'll be paired up with random groups and you will,",
    "start": "223640",
    "end": "232364"
  },
  {
    "text": "um, have to do one joint quiz. And your grade will be composed both of your individual part and your group part,",
    "start": "232365",
    "end": "239920"
  },
  {
    "text": "but you can only do better on your group part. So if your group does worse, then you're gonna just get the same as your individual grade.",
    "start": "239920",
    "end": "247415"
  },
  {
    "text": "So the reason that we do this is that, um, for the group, then it's a scratch off exam.",
    "start": "247415",
    "end": "253315"
  },
  {
    "text": "So you scratch off answers as you de- decide on them as a group, and the point is to- that you should be able to articulate why you",
    "start": "253315",
    "end": "261299"
  },
  {
    "text": "believe some of these answers are true or false and convince your classmates, and in doing so, that can be a really useful way",
    "start": "261300",
    "end": "266970"
  },
  {
    "text": "to think about really knowing the material well, um, and also hearing the perspectives of other people.",
    "start": "266970",
    "end": "272474"
  },
  {
    "text": "So that's how the last quiz goes. Um, [NOISE] again last year everybody- there was",
    "start": "272475",
    "end": "278790"
  },
  {
    "text": "some concern about it before on Piazza, people were concerned. There are certain game theoretic aspects that can come up.",
    "start": "278790",
    "end": "285135"
  },
  {
    "text": "Um, we carefully design this so that it's a very small part of your grade. Um, again you could only do better with the group than you can on your individual,",
    "start": "285135",
    "end": "293970"
  },
  {
    "text": "so it's carefully constructed. And empirically when we did this, there was lots of laughter and lots of people seem to really enjoy this aspect,",
    "start": "293970",
    "end": "302289"
  },
  {
    "text": "and we've thought about whether we'll do it in multiple parts of the class. But it's different, almost nobody has ever done an exam like this.",
    "start": "302290",
    "end": "308544"
  },
  {
    "text": "So does anybody have any questions about that? It's about 5% of your grade. Yeah. Uh, I remember you saying something earlier on in the course [NOISE] that you guys",
    "start": "308545",
    "end": "315840"
  },
  {
    "text": "have already decided the teams or will have decided the teams, is that true? Yeah. So we haven't already decided that the question, um,",
    "start": "315840",
    "end": "322539"
  },
  {
    "text": "was how are the teams assigned and have we already decided them, we'll do that by random assignment. It'll depend also on which SCPD students are taking the exam on campus or not, uh,",
    "start": "322540",
    "end": "330909"
  },
  {
    "text": "but we'll release this a few days before, um, and you'll be randomly allocated to a team and then you'll just sit with that team for that part of the exam.",
    "start": "330910",
    "end": "337940"
  },
  {
    "text": "[NOISE] Anybody have any other questions about that? And we'll- we'll release the, the sample one.",
    "start": "337940",
    "end": "344100"
  },
  {
    "text": "It will cover all of the course. So it'll be more heavily weighted to stuff that's happened since the midterm,",
    "start": "344100",
    "end": "349390"
  },
  {
    "text": "but anything from the whole course will be game, and the idea is that someone who has been attending lectures, um, or,",
    "start": "349390",
    "end": "355660"
  },
  {
    "text": "or watching lectures online, um, should have to study for, you know, on the order of a few hours and then be pretty well prepared for the exam.",
    "start": "355660",
    "end": "362750"
  },
  {
    "text": "You'll also be able to bring, uh, a cheat sheet, just like what you did for the midterm. [NOISE] Any other questions about that? Yeah.",
    "start": "362750",
    "end": "370870"
  },
  {
    "text": "And I feel like you probably mentioned this, but I'm just missing which part of the quiz is individual versus with other people?",
    "start": "371230",
    "end": "380920"
  },
  {
    "text": "Do we sit down individually and then after like 20 minutes go be with other people?",
    "start": "380920",
    "end": "385965"
  },
  {
    "text": "Yes. So, um, that was a good one. So how does- what is this individual group thing? So the idea is that you come in,",
    "start": "385965",
    "end": "391125"
  },
  {
    "text": "everybody gets an exam, you work on that exam, probably from around 45 minutes. Um, you hand it in when you're done,",
    "start": "391125",
    "end": "397450"
  },
  {
    "text": "then when everybody- when that part of the, the class is done, most people finish early, it depends, um, then you as a group get a new exam.",
    "start": "397450",
    "end": "405155"
  },
  {
    "text": "And you do exactly the same exam as before, but you just have to jointly agree on the answers.",
    "start": "405155",
    "end": "410100"
  },
  {
    "text": "And you scratch it off so we can see how many- it's, uh- how many you have to scratch off until you got the right answer.",
    "start": "411020",
    "end": "417115"
  },
  {
    "text": "But essentially, we can just see whether or not you got the answer on the first time or, or it took more than one.",
    "start": "417115",
    "end": "422320"
  },
  {
    "text": "Any other questions about the quiz? If you have any concerns about that, just write, um, email us on Piazza.",
    "start": "423560",
    "end": "429419"
  },
  {
    "text": "I'll just say briefly there- about what was the one of the concerns that came up last year. Um, the concern was game theory.",
    "start": "429420",
    "end": "434775"
  },
  {
    "text": "So you always get the max of, uh, your score versus the, the group score.",
    "start": "434775",
    "end": "439935"
  },
  {
    "text": "So what people said is well what you should do is you should answer the best you can on your individual part because that's worth the most credits,",
    "start": "439935",
    "end": "446509"
  },
  {
    "text": "and then on the group part, you should- if you were torn between two, you should get people to agree on the second answer to hedge your bets.",
    "start": "446510",
    "end": "453890"
  },
  {
    "text": "Um, [NOISE] you can do that if you want to [LAUGHTER] or try to, your or your group members can outweigh you.",
    "start": "453890",
    "end": "459065"
  },
  {
    "text": "Again, the group part last year was about 0.5%, so it's very small. Um, so there, there is that possibility- you would only want to do",
    "start": "459065",
    "end": "466840"
  },
  {
    "text": "that if you were genuinely really torn between two options, um, and again, there's only one right answer, so I,",
    "start": "466840",
    "end": "473260"
  },
  {
    "text": "I think that we've observed in practice, there was very little need to do game theoretic analysis of this.",
    "start": "473260",
    "end": "478510"
  },
  {
    "text": "[NOISE] But, no. Again, it's- we're always welcome to hear how people interpret these things.",
    "start": "478510",
    "end": "484160"
  },
  {
    "text": "All right. Any other questions about the quiz or logistics right now? We also- most of you who have already turned in your, uh,",
    "start": "484160",
    "end": "491555"
  },
  {
    "text": "m- milestone for the project, we'll be giving feedback on those over the next few days for those of you that are not doing the default.",
    "start": "491555",
    "end": "498510"
  },
  {
    "text": "Okay. All right. So I put this up before. This is for everybody that didn't see it before,",
    "start": "498690",
    "end": "505379"
  },
  {
    "text": "the grade distribution is basically identical. So today, we're gonna do the last part on fast learning.",
    "start": "505380",
    "end": "511889"
  },
  {
    "text": "Um, this is, uh, a really big topic, there's tons of work on it. Um, well- we'll spend some more time on it today,",
    "start": "511890",
    "end": "519300"
  },
  {
    "text": "and then on Monday, Chelsea Finn, who's, uh, just finished her P- PhD at Berkeley and she'll be joining the faculty here in the summer, um,",
    "start": "519300",
    "end": "525509"
  },
  {
    "text": "will come and talk about meta-learning, which is also a really exciting area, and she'll be talking about meta-learning for reinforcement learning,",
    "start": "525510",
    "end": "531495"
  },
  {
    "text": "where meta-learning is relevant to, sort of, multitask or transfer learning tasks. [NOISE] So just to",
    "start": "531495",
    "end": "538850"
  },
  {
    "text": "go refresh our minds about what we're talking about in terms of this fast learning, we're thinking about cases where data matters.",
    "start": "538850",
    "end": "544730"
  },
  {
    "text": "So things like healthcare, and education, and customers. Um, I was getting an invite to talk at Pinterest on Monday,",
    "start": "544730",
    "end": "551430"
  },
  {
    "text": "they definitely care about these types of ideas as well. Um, and we've been talking about two different settings: Bandits and Markov decision processes,",
    "start": "551430",
    "end": "558490"
  },
  {
    "text": "as well as frameworks for formerly understanding whether an algorithm is good or whether it is fast in terms of the amount of data it needs.",
    "start": "558490",
    "end": "566655"
  },
  {
    "text": "And I'll note there that we haven't talked much about computational complexity for this part of the course, but there are similar, um,",
    "start": "566655",
    "end": "573555"
  },
  {
    "text": "some of these frameworks can even easily be extended to talk about polynomial sample complexity. So often, you can extend these frameworks to also account",
    "start": "573555",
    "end": "580010"
  },
  {
    "text": "for computational complexity requirements. Okay. So let's continue with Markov decision processes.",
    "start": "580010",
    "end": "587205"
  },
  {
    "text": "What we started seeing last time is that we built up sort of this expertise on bandits so far, of thinking of a couple of the main ways we evaluate whether or not",
    "start": "587205",
    "end": "594040"
  },
  {
    "text": "a bandit algorithm is good and approaches to try to achieve that. So we talked about mathematical regret,",
    "start": "594040",
    "end": "600175"
  },
  {
    "text": "which was the difference between how well we could've acted and how well we did act in bandits,",
    "start": "600175",
    "end": "606120"
  },
  {
    "text": "and, um, a lot of the work in bandits focuses on regret. We also talked about two different types of techniques for trying to achieve low regret,",
    "start": "606120",
    "end": "614855"
  },
  {
    "text": "which was optimism under uncertainty, and then also Thompson sampling.",
    "start": "614855",
    "end": "620300"
  },
  {
    "text": "So trying to be Bayesian and explicitly represent posterior over what you think might happen when you pull an arm,",
    "start": "620300",
    "end": "626440"
  },
  {
    "text": "or take an action, and using that sort of information. [NOISE] Then last time we started talking about Markov decision processes",
    "start": "626440",
    "end": "632715"
  },
  {
    "text": "where I argued that very similar ideas are important here, but- but the problem is a lot more challenging in many ways.",
    "start": "632715",
    "end": "640045"
  },
  {
    "text": "I- and we were talking a little bit about probably approximately correct.",
    "start": "640045",
    "end": "645149"
  },
  {
    "start": "645000",
    "end": "1130000"
  },
  {
    "text": "So, in particular, we were talking, uh, a bit about model-based interval estimation,",
    "start": "645150",
    "end": "650415"
  },
  {
    "text": "which I mentioned was a probably approximately correct algorithm. And so just to remind ourselves, what did PAC mean?",
    "start": "650415",
    "end": "657139"
  },
  {
    "text": "[NOISE]. And some of you have seen this probably in machine learning.",
    "start": "657140",
    "end": "664065"
  },
  {
    "text": "So probably [NOISE] approximately correct",
    "start": "664065",
    "end": "669140"
  },
  {
    "text": "[NOISE].",
    "start": "669140",
    "end": "678070"
  },
  {
    "text": "Probably approximately correct RL algorithm is one that given an input, epsilon and delta.",
    "start": "678070",
    "end": "683910"
  },
  {
    "text": "So epsilon is gonna specify sort of how good close to optimal we want to be, and delta's gonna specify with what probability we're gonna want this to occur.",
    "start": "684040",
    "end": "693540"
  },
  {
    "text": "Um, with input epsilon and delta on all but N steps [NOISE].",
    "start": "693540",
    "end": "701480"
  },
  {
    "text": "Our algorithm will select",
    "start": "702990",
    "end": "710080"
  },
  {
    "text": "an action where there,",
    "start": "710080",
    "end": "716320"
  },
  {
    "text": "the Q value of that action, the true optimal Q value is greater than or equal to,",
    "start": "716320",
    "end": "724749"
  },
  {
    "text": "I'll write down it as V. The best possible you could have for that state,",
    "start": "724749",
    "end": "729950"
  },
  {
    "text": "minus epsilon [NOISE] with",
    "start": "730500",
    "end": "736060"
  },
  {
    "text": "probability at least 1 - delta, and throughout",
    "start": "736060",
    "end": "742930"
  },
  {
    "text": "today I'm going to be a little bit loose about constants, sometimes this will be 1 - 2 delta, sometimes there might be a little constant in front of here,",
    "start": "742930",
    "end": "749320"
  },
  {
    "text": "sometimes there might be a little constant in front of there. I'll put one here just so you can keep that in mind. There might be small constants there.",
    "start": "749320",
    "end": "755500"
  },
  {
    "text": "Those are just, there might be two or four. Um, but the important thing is that, that you're close- very close to optimal,",
    "start": "755500",
    "end": "762250"
  },
  {
    "text": "except for maybe a constant factor away, um, where N is a polynomial function",
    "start": "762250",
    "end": "769279"
  },
  {
    "text": "of S size of your state space size of your action space gamma,",
    "start": "769890",
    "end": "775135"
  },
  {
    "text": "um, epsilon, delta. Yeah.",
    "start": "775135",
    "end": "780400"
  },
  {
    "text": "1 over epsilon [inaudible] 1 over epsilon, yes great.",
    "start": "780400",
    "end": "785574"
  },
  {
    "text": "Question was good is, um, are these going to depend on epsilon or delta or 1 over epsilon or 1 over delta? Yes, in- inside of all the expressions they'll",
    "start": "785575",
    "end": "792310"
  },
  {
    "text": "end up being 1 over epsilon and 1 over delta. So you could equally write this as this.",
    "start": "792310",
    "end": "797440"
  },
  {
    "text": "[NOISE] Because essentially N is going to be larger,",
    "start": "797440",
    "end": "803770"
  },
  {
    "text": "if you want to be more accurate. So that's going to, um, as epsilon gets smaller, you're going to need more data to be more accurate.",
    "start": "803770",
    "end": "810070"
  },
  {
    "text": "And if you want to be more sure, you're going to be accurate, you're going to also scale up with that delta.",
    "start": "810070",
    "end": "815875"
  },
  {
    "text": "Okay, and I just want to before we kind of continue further. I'd like to briefly contrast this with regret",
    "start": "815875",
    "end": "821740"
  },
  {
    "text": "because in the bandit setting we mostly think about regret. But it's nice to think about what the difference is between PAC and regret,",
    "start": "821740",
    "end": "829765"
  },
  {
    "text": "particularly in online learning. Meaning like our algorithm's learning online in a MDP and it's learning forever.",
    "start": "829765",
    "end": "835389"
  },
  {
    "text": "Um, which is what regret is telling you. So regret is saying [NOISE],",
    "start": "835390",
    "end": "843070"
  },
  {
    "text": "Is that large enough in the back, can you see regret? Okay. So what regret is saying is let's say we start off at a state S_0.",
    "start": "843070",
    "end": "849770"
  },
  {
    "text": "Regret is saying, what if you did the optimal thing from then on wards. Like, how great would your life have been.",
    "start": "849770",
    "end": "856440"
  },
  {
    "text": "So if you had won that, you know, first coloring contest, and that set you up for Harvard, and set you up to the Supreme Court, like it's fabulous.",
    "start": "856440",
    "end": "864180"
  },
  {
    "text": "But if instead you didn't enter the, that, um, coloring contest, and you got down here instead.",
    "start": "864180",
    "end": "869400"
  },
  {
    "text": "So you could have had like a +10 there, but instead you had a 0. And you went to a different state, which, all right, you went to a different state in terms of MDP,",
    "start": "869400",
    "end": "877119"
  },
  {
    "text": "where now you're not the person that won the coloring contest. And so then, you know, [NOISE] your life trajectory was irreversibly ruined.",
    "start": "877119",
    "end": "886060"
  },
  {
    "text": "Um, in this case, [LAUGHTER] you are judged with respect to not just the,",
    "start": "886060",
    "end": "892945"
  },
  {
    "text": "the actions but the state distribution you could've got to under the optimal policy.",
    "start": "892945",
    "end": "898600"
  },
  {
    "text": "So you're always being judged with, what could I have reached if from the very beginning I always made optimal decisions?",
    "start": "898600",
    "end": "905320"
  },
  {
    "text": "I always went into the coloring contest. I always went to Harvard. You know, I never went to that Stanford place and, and you got up to the Supreme Court versus,",
    "start": "905320",
    "end": "912459"
  },
  {
    "text": "um, as soon as you make a different decision you might end up in a different states distribution.",
    "start": "912460",
    "end": "917545"
  },
  {
    "text": "But you're going to look at these gaps. Okay. So you're going to be judged by the state distribution you ended up",
    "start": "917545",
    "end": "925480"
  },
  {
    "text": "in and the rewards you got there versus the state distribution you'd get in under the optimal policy and rewards you get there.",
    "start": "925480",
    "end": "930985"
  },
  {
    "text": "So regret in some ways is a pretty harsh criteria. Because it's saying like you always have to be judged for,",
    "start": "930985",
    "end": "936490"
  },
  {
    "text": "like, if you'd made optimal decisions forever. PAC is much more reasonable in certain ways.",
    "start": "936490",
    "end": "942505"
  },
  {
    "text": "PAC says, \"I'm judging you under the state distribution you get to under your algorithm.\"",
    "start": "942505",
    "end": "948850"
  },
  {
    "text": "So because it says, it will take an action that's close to optimal for the state that you're in.",
    "start": "948850",
    "end": "955300"
  },
  {
    "text": "So what does PAC say? PAC says, okay you started off here. You didn't enter the coloring contest.",
    "start": "955300",
    "end": "961959"
  },
  {
    "text": "You went to there. Okay, that's too bad. Um, given that you could've then, you know, I don't know, entered the next coloring contest, or you didn't.",
    "start": "961960",
    "end": "970255"
  },
  {
    "text": "And I'm going to be judged by that local gap. I'm going to always only be judged by how optimal am",
    "start": "970255",
    "end": "975700"
  },
  {
    "text": "I given the distribution of states I'm getting to under my algorithm. So PAC can give,",
    "start": "975700",
    "end": "982584"
  },
  {
    "text": "have much smaller regret. I'm sorry, much smaller, sort of, um, negative, ah, differences compared to regret.",
    "start": "982585",
    "end": "990420"
  },
  {
    "text": "Because imagine you have a really harsh MDP, and you have to make the first right move and then you go to some wonderful land.",
    "start": "990420",
    "end": "995715"
  },
  {
    "text": "I'll see you always toil about in this horrible gridworld. Um, so in that case regret would compare you to,",
    "start": "995715",
    "end": "1001725"
  },
  {
    "text": "if you'd actually made the right first choice, whereas PAC would say, okay maybe made a bad first choice, but like you're making the best of things for where you're at,",
    "start": "1001725",
    "end": "1007785"
  },
  {
    "text": "and you're kind of be near optimal given this bad state-space you've ended up in.",
    "start": "1007785",
    "end": "1012550"
  },
  {
    "text": "So in some ways you can think of PAC is kind of making the most of the circumstances you've got yourself into [LAUGHTER],",
    "start": "1012890",
    "end": "1019260"
  },
  {
    "text": "whereas regret is always judging you from if you'd make good decisions from the beginning. I saw a question back there. Yeah. And everyone please remind me of your names.",
    "start": "1019260",
    "end": "1025890"
  },
  {
    "text": "I know, I'm trying hard, but I sometimes forget. Yeah. Episodic MDP's? Great questions. The question is does this extend to episodic MDPs?",
    "start": "1025890",
    "end": "1032790"
  },
  {
    "text": "Um, so in episodic MDPs just to recall, um, those are MDPs where we act for h steps or,",
    "start": "1032790",
    "end": "1039449"
  },
  {
    "text": "or a finite number of steps and then we reset. In episodic MDPs, regret and PAC are closer becau se normally,",
    "start": "1039450",
    "end": "1046454"
  },
  {
    "text": "the PAC guarantees we get in that case. I'm not going to talk too much about those today but, um,",
    "start": "1046455",
    "end": "1051794"
  },
  {
    "text": "are going to be with respect to the starting state. So you're going to look at, like V star of S0 versus Q star of S0,",
    "start": "1051795",
    "end": "1062820"
  },
  {
    "text": "of like the actions you're taking, or the policy you're following. So in those cases they start to be closer,",
    "start": "1062820",
    "end": "1069045"
  },
  {
    "text": "because you're always being judged from the starting state and you can reset. But in online, like continual learning,",
    "start": "1069045",
    "end": "1075600"
  },
  {
    "text": "um, for reinforcement learning they can be quite different. Because the state distributions could be so different. Yeah. Could you just explain more about where C1",
    "start": "1075600",
    "end": "1082860"
  },
  {
    "text": "and C2 come from because I don't see them as any like given parameters. Oh, yes. I just put up, question about C1 and C2.",
    "start": "1082860",
    "end": "1088274"
  },
  {
    "text": "I just, I'm going to be very loose with constants today. Most of these type of regret guarantees are all about orders of magnitude.",
    "start": "1088275",
    "end": "1095850"
  },
  {
    "text": "So it's stuff like is N a function of S to the 6 or is it a function of S to the 4.",
    "start": "1095850",
    "end": "1102735"
  },
  {
    "text": "And we generally don't worry about constants too much. So I just put these in there to say, some of the different theoretical bounds will have different constants there.",
    "start": "1102735",
    "end": "1111420"
  },
  {
    "text": "But for today we're just going to kind of ignore those but just so that you know that there might be constants there.",
    "start": "1111420",
    "end": "1116880"
  },
  {
    "text": "This might be 1 - 2 delta, for example, instead of 1 - delta. [NOISE] Okay, right so that's one of the differences between regret and PAC.",
    "start": "1116880",
    "end": "1127679"
  },
  {
    "text": "So let's go back to this algorithm now. Um, and I'll highlight, so we'll talk a little bit about generalization later today.",
    "start": "1127680",
    "end": "1134145"
  },
  {
    "start": "1130000",
    "end": "1339000"
  },
  {
    "text": "But I, I wanted to go through sort of one of, how do we start to think about whether an algorithm is PAC or not.",
    "start": "1134145",
    "end": "1140025"
  },
  {
    "text": "I told you that this algorithm is PAC. But I wanted to talk some about why it's PAC,",
    "start": "1140025",
    "end": "1145815"
  },
  {
    "text": "and what it means for an algorithm to be PAC, and are there general sorts of templates that we can use to show an algorithm is PAC.",
    "start": "1145815",
    "end": "1152835"
  },
  {
    "text": "All the stuff I'm going to talk about right now, involves tabular settings where we can write down,",
    "start": "1152835",
    "end": "1158040"
  },
  {
    "text": "um, the value function as a table. And later we'll talk some about how these ideas extend. We're particularly picking this algorithm",
    "start": "1158040",
    "end": "1165600"
  },
  {
    "text": "which is what's known as a reward bonus algorithm. So we have this nice little reward bonus here.",
    "start": "1165600",
    "end": "1171255"
  },
  {
    "text": "Because it's going to be easier to extend to the model-free case. Now one thing I just want to highlight when we look at the MBIE-EB is that,",
    "start": "1171255",
    "end": "1179325"
  },
  {
    "text": "if we go back and refresh our memories about this, what we are doing is we are computing the maximum likelihood estimate,",
    "start": "1179325",
    "end": "1185085"
  },
  {
    "text": "or otherwise known as just adding up the counts and dividing, of the empirical estimate of the transition model and the reward model for every state-action pair.",
    "start": "1185085",
    "end": "1192390"
  },
  {
    "text": "So we look at how many times we've been in a state-action pair, which next states we transition to, and we use that to construct an empirical model.",
    "start": "1192390",
    "end": "1199620"
  },
  {
    "text": "And we do the same for the reward structure. And then we want to figure out how to act, we take those empirical models,",
    "start": "1199620",
    "end": "1206190"
  },
  {
    "text": "and you can think of this, this operator here as if we're slightly changing our reward model.",
    "start": "1206190",
    "end": "1213300"
  },
  {
    "text": "So I put it here as the empirical reward plus this bonus. But you can alternatively think of this as like an R hat prime which is",
    "start": "1213300",
    "end": "1222419"
  },
  {
    "text": "equal to R hat of SA plus this bonus term. [NOISE] So you can think of this as like kind of defining a new MDP.",
    "start": "1222420",
    "end": "1234164"
  },
  {
    "text": "There's a new MDP where the transition model is T hat and the reward model is R hat prime.",
    "start": "1234165",
    "end": "1240965"
  },
  {
    "text": "Which is the empirical reward plus this bonus term. And it's- it's not a real MDP, but,",
    "start": "1240965",
    "end": "1246990"
  },
  {
    "text": "but that's an MDP we could solve and try to compute the optimal value for and that's what we're doing here is we construct this sort of optimistic MDP,",
    "start": "1246990",
    "end": "1254295"
  },
  {
    "text": "where we're using the empirical transition model. And then we use a reward model that has really large bonuses in places we haven't visited very much.",
    "start": "1254295",
    "end": "1262020"
  },
  {
    "text": "[NOISE] All right. And- and a key thing we're gonna see shortly",
    "start": "1262020",
    "end": "1268880"
  },
  {
    "text": "is the critical thing is how optimistic to be. Um, and there's been tons of work on- on trying to make things more or less optimistic.",
    "start": "1268880",
    "end": "1277730"
  },
  {
    "text": "And if we have time, I'll show you some other slides about some recent progress in this field.",
    "start": "1277730",
    "end": "1282740"
  },
  {
    "text": "Okay. So we talked before about this MBIE-EB PAC. And then now, let's talk a little bit about sort",
    "start": "1282740",
    "end": "1290090"
  },
  {
    "text": "of what are the sufficient conditions to make something PAC, and then, how does MBIE-EB satisfy those form of conditions.",
    "start": "1290090",
    "end": "1297245"
  },
  {
    "text": "So the conditions that I'm gonna talk about are derived basically from this paper, um,",
    "start": "1297245",
    "end": "1303830"
  },
  {
    "text": "with slight modifications that paper does not- I'll just write down here, does not analyze [NOISE] MBIE-EB.",
    "start": "1303830",
    "end": "1316040"
  },
  {
    "text": "So things would have to be a little bit different. But from a 30,000 foot perspective, this is basically,",
    "start": "1316040",
    "end": "1322010"
  },
  {
    "text": "a reasonable way to think about why MBIE is a- MBIE-EB is a PAC algorithm.",
    "start": "1322010",
    "end": "1327350"
  },
  {
    "text": "[NOISE] Okay. So let's unplug this",
    "start": "1327350",
    "end": "1333770"
  },
  {
    "text": "or yeah, oops, I'll put these up on the board,",
    "start": "1333770",
    "end": "1340400"
  },
  {
    "start": "1339000",
    "end": "1604000"
  },
  {
    "text": "because I think it's helpful to kind of see all of it at once. Okay. So what is a sufficient set of conditions to make something PAC?",
    "start": "1340400",
    "end": "1346760"
  },
  {
    "text": "[NOISE] I know that",
    "start": "1346760",
    "end": "1352250"
  },
  {
    "text": "I found this paper super helpful when I was starting to do PAC proofs in my PhD. So- so what's a sufficient [NOISE] conditions for PAC?",
    "start": "1352250",
    "end": "1366030"
  },
  {
    "text": "And the theory is beautiful. But even for those of you that aren't interested in theory, I think looking at this is helpful because it gives one",
    "start": "1369880",
    "end": "1376160"
  },
  {
    "text": "an intuition about what types of properties do your algorithms need to have in order to be efficient or wha- what types of",
    "start": "1376160",
    "end": "1381650"
  },
  {
    "text": "properties are sufficient for your algorithm to be efficient? Okay. So the first one is optimism.",
    "start": "1381650",
    "end": "1387695"
  },
  {
    "text": "Again, this is not the only set of conditions that are sufficient- that are sufficient for something to be PAC but here's a set.",
    "start": "1387695",
    "end": "1393890"
  },
  {
    "text": "So here's optimism. Optimism. Okay and optimism simply says that,",
    "start": "1393890",
    "end": "1404110"
  },
  {
    "text": "the computed value you use. Okay. So this is for this is s_t, this is a_t.",
    "start": "1404110",
    "end": "1410710"
  },
  {
    "text": "So this is the actual value we compute like from MBIE-EB that optimistic value we compute.",
    "start": "1410710",
    "end": "1417190"
  },
  {
    "text": "So this is the computed value of your algorithm [NOISE].",
    "start": "1417190",
    "end": "1424330"
  },
  {
    "text": "Has to be greater than or equal to the true optimal value for that state-action pair [NOISE] minus epsilon on all time steps [NOISE].",
    "start": "1424330",
    "end": "1433539"
  },
  {
    "text": "Okay. So it says that, whenever we are doing the MBIE-EB calculation,",
    "start": "1433540",
    "end": "1439265"
  },
  {
    "text": "when we've taken our empirical models and we add in that reward bonus, we have to pick a reward bonus so that whatever we compute for",
    "start": "1439265",
    "end": "1445520"
  },
  {
    "text": "the resulting state-action pair is optimistic minus some epsilon on all time steps.",
    "start": "1445520",
    "end": "1450890"
  },
  {
    "text": "All of this is only gonna need to hold with high probability but I'll just write it out like this. So this is the first condition.",
    "start": "1450890",
    "end": "1456185"
  },
  {
    "text": "The second condition is a little bit more [NOISE] subtle but I'll say more in a second about the specific cases for this.",
    "start": "1456185",
    "end": "1464524"
  },
  {
    "text": "So the second thing is what's known as accuracy. And I'll write down the accuracy first.",
    "start": "1464525",
    "end": "1469700"
  },
  {
    "text": "So the first thing says, you need to be optimistic on all time steps.",
    "start": "1469700",
    "end": "1475054"
  },
  {
    "text": "The second thing is that, you need to be accurate. Which means the V_t. This is again, what the algorithm computes [NOISE].",
    "start": "1475055",
    "end": "1482960"
  },
  {
    "text": "Your algorithm is gonna compute this. So it's what MBI computes using your optimistic model needs to",
    "start": "1482960",
    "end": "1489890"
  },
  {
    "text": "be - V pi t of a weird MDP.",
    "start": "1489890",
    "end": "1496040"
  },
  {
    "text": "And I'll tell you what that, about that weird MDP in a second. It is not the MDP",
    "start": "1496040",
    "end": "1501080"
  },
  {
    "text": "I just said that is the nice optimistic MDP. It is not the real MDP. It is an MDP that's sort of in the middle of those.",
    "start": "1501080",
    "end": "1508145"
  },
  {
    "text": "And this type of trick in RL comes up a lot where we sort of construct,",
    "start": "1508145",
    "end": "1513709"
  },
  {
    "text": "you've seen probably in several proofs now, where we add and subtract the same term which is sort of halfway in between say, two different Markov decision processes.",
    "start": "1513709",
    "end": "1521675"
  },
  {
    "text": "We're gonna play a similar trick here and we're gonna construct an MDP that is sort of half optimistic and is half like the real MDP.",
    "start": "1521675",
    "end": "1529910"
  },
  {
    "text": "Again, this doesn't exist in the real world we're just gonna use it as a tool for our analysis.",
    "start": "1529910",
    "end": "1535485"
  },
  {
    "text": "So I'll say what this is shortly. [NOISE] What there's different ways to define this but,",
    "start": "1535485",
    "end": "1540580"
  },
  {
    "text": "um, it says that, something that's gonna be closely related to both your optimistic [NOISE] MDP and the true MDP [NOISE] that your value.",
    "start": "1540580",
    "end": "1547880"
  },
  {
    "text": "So this is pi t is the policy you're actually executing at time step t. The- the value that you",
    "start": "1547880",
    "end": "1555410"
  },
  {
    "text": "compute has to be close to this sort of weird hybrid MDP [NOISE] within epsilon.",
    "start": "1555410",
    "end": "1563540"
  },
  {
    "text": "So it has to be pretty close to this other MDP. And the reason for this is that,",
    "start": "1563540",
    "end": "1568820"
  },
  {
    "text": "this is we're gonna be able to use this to try to bound how far away we can be from the real MDP.",
    "start": "1568820",
    "end": "1573950"
  },
  {
    "text": "So why are we gonna need this? We're gonna need this because optimism would be easy to hold by just setting our values super high and never updating.",
    "start": "1573950",
    "end": "1582755"
  },
  {
    "text": "So that's fine but you need to be able to use the information you have so that eventually you're gonna be acting near-optimally.",
    "start": "1582755",
    "end": "1588800"
  },
  {
    "text": "So if something really is bad, you don't want to be really optimistic forever. And so the accuracy condition is gonna say,",
    "start": "1588800",
    "end": "1595325"
  },
  {
    "text": "if we've got sort of enough information about some state-action pairs or value for some of those needs to be fairly close to the,",
    "start": "1595325",
    "end": "1602405"
  },
  {
    "text": "to a real value [NOISE]. Okay. And then, the third thing [NOISE] is bounded learning complexity.",
    "start": "1602405",
    "end": "1614674"
  },
  {
    "start": "1604000",
    "end": "1879000"
  },
  {
    "text": "Okay. [NOISE].",
    "start": "1614675",
    "end": "1628789"
  },
  {
    "text": "And this has two parts. This says, the total number of updates, total number of queue updates.",
    "start": "1628790",
    "end": "1639360"
  },
  {
    "text": "So in MBIE-EB, we would update our state-action values.",
    "start": "1640840",
    "end": "1646445"
  },
  {
    "text": "And we would rerun that sort of optimistic Q value iteration. The total number of times we do that has to be,",
    "start": "1646445",
    "end": "1653645"
  },
  {
    "text": "is gonna be bounded as is, the number of times [NOISE] we visit an unknown pair,",
    "start": "1653645",
    "end": "1662539"
  },
  {
    "text": "state-action pair and I'll say more what that is in a second [NOISE].",
    "start": "1662540",
    "end": "1668270"
  },
  {
    "text": "All right. So we're gonna classify all state-action pairs and we're in the tabular settings and this is reasonable for us to do,",
    "start": "1668270",
    "end": "1675200"
  },
  {
    "text": "um, we're going to end up classifying every single state-action pair into being either known or unknown. And we're gonna say the total number of times we visit",
    "start": "1675200",
    "end": "1682460"
  },
  {
    "text": "an unknown state-action pair both of these have to be bounded [NOISE] by some function.",
    "start": "1682460",
    "end": "1690830"
  },
  {
    "text": "It is a function of epsilon and delta. So it means you can't do an infinite number of queue updates and you can't",
    "start": "1690830",
    "end": "1699320"
  },
  {
    "text": "visit unknown state-action pairs an infinite number of times, like your algorithm can't. These are conditions on your algorithm.",
    "start": "1699320",
    "end": "1705695"
  },
  {
    "text": "Okay. So if you can satisfy all of these, then your algorithm is PAC.",
    "start": "1705695",
    "end": "1711139"
  },
  {
    "text": "So if 1 through 3 are satisfied [NOISE] then,",
    "start": "1711140",
    "end": "1721130"
  },
  {
    "text": "it'll be epsilon order epsilon optimal [NOISE] on",
    "start": "1721130",
    "end": "1729395"
  },
  {
    "text": "all but and I'll write this out here just so you can kind of get a sense of what these type of bounds can look like and all but N which is equal to order.",
    "start": "1729395",
    "end": "1738770"
  },
  {
    "text": "This- this bound is sample complexity divided",
    "start": "1738770",
    "end": "1744260"
  },
  {
    "text": "by epsilon times 1 - gamma squared times some log terms.",
    "start": "1744260",
    "end": "1750270"
  },
  {
    "text": "So essentially, this is saying that if you can be optimistic accurate with respect to some weird MDP I haven't told you",
    "start": "1750970",
    "end": "1757130"
  },
  {
    "text": "about and that if your total number of queue updates and the number of times you visit unknown state-action pairs which I often haven't told you about",
    "start": "1757130",
    "end": "1763790"
  },
  {
    "text": "exactly how we define that, if that is bounded then, you're gonna be PAC. You're gonna be near optimal on all time steps except for a number that",
    "start": "1763790",
    "end": "1772309"
  },
  {
    "text": "scales as a function of this which is also generally, a function of the size of the state space and action space.",
    "start": "1772310",
    "end": "1779705"
  },
  {
    "text": "This is also function of say [NOISE] at epsilon of 1 - delta.",
    "start": "1779705",
    "end": "1789309"
  },
  {
    "text": "So this is kind of a template. So if you can show that your algorithm satisfies these properties,",
    "start": "1789310",
    "end": "1794395"
  },
  {
    "text": "then you can show that it's PAC. All right. So how does MBIE-EB satisfy these properties?",
    "start": "1794395",
    "end": "1808174"
  },
  {
    "text": "Well, the first thing we need to show is that MBIE-EB is optimistic. Yeah question.",
    "start": "1808175",
    "end": "1814370"
  },
  {
    "text": "Ah, so the first question was for three, part one and part two, do they both have the same bounds,",
    "start": "1814370",
    "end": "1820264"
  },
  {
    "text": "or are there just two separate bounds with different magnitudes? Good question, he's asking about,",
    "start": "1820265",
    "end": "1825965"
  },
  {
    "text": "do you mean the epsilon there? The total number of Q-updates and the total number of times you visit other states. Uh, good question, um, so for part three,",
    "start": "1825965",
    "end": "1832580"
  },
  {
    "text": "as the total number of Q-updates the number of times you visit state-action pairs, um, they are going to be very closely-related essentially,",
    "start": "1832580",
    "end": "1839750"
  },
  {
    "text": "whenever you visited another state-action pair, then you can do a Q-update. For one and two epsilons-",
    "start": "1839750",
    "end": "1846895"
  },
  {
    "text": "are they the same? Yes, yeah. Good question. So that's what I thought you're asking. In one and two, um,",
    "start": "1846895",
    "end": "1852900"
  },
  {
    "text": "the question was, are epsilon the same? Yes, they're the same. Epsilon is same through 1, 2 and 3. So if you're designing your algorithm,",
    "start": "1852900",
    "end": "1859245"
  },
  {
    "text": "1 and 2 and 3 all have to be the same. Constants probably don't matter. It can, you know, be 1 minus.",
    "start": "1859245",
    "end": "1865919"
  },
  {
    "text": "In some of these cases you can be- have a constant in front of the epsilon. One just has to be a bit careful.",
    "start": "1865920",
    "end": "1870960"
  },
  {
    "text": "So for here, we'll just put them like that, okay? Same epsilon everywhere. Okay. So let's talk first about why MBIE-EB is optimistic.",
    "start": "1870960",
    "end": "1881505"
  },
  {
    "start": "1879000",
    "end": "2319000"
  },
  {
    "text": "Um, let's- actually, can we put this up please? I think that'll be better.",
    "start": "1881505",
    "end": "1887745"
  },
  {
    "text": "So I'm gonna just reput up MBIE's, um, bonus term so that you can, uh, see what that looks like.",
    "start": "1887745",
    "end": "1894540"
  },
  {
    "text": "Okay. So I think this is gonna go up in just a second, and then just to remind us what the update was for [inaudible].",
    "start": "1894540",
    "end": "1903690"
  },
  {
    "text": "So when- what we were doing in MBIE-EB is, we had a state-action pair.",
    "start": "1903690",
    "end": "1909405"
  },
  {
    "text": "We had our empirical reward for that state-action pair, plus our bonus, beta divided by square root of n(s,a).",
    "start": "1909405",
    "end": "1919679"
  },
  {
    "text": "Is that too small in the back, is that okay? It's okay? Okay, great. Um, I see at least one person nodding.",
    "start": "1919680",
    "end": "1926115"
  },
  {
    "text": "So this is our sort of optimistic reward. You can call this, like R tilde.",
    "start": "1926115",
    "end": "1931125"
  },
  {
    "text": "This is our optimistic reward. Plus sum over s'.",
    "start": "1931125",
    "end": "1937605"
  },
  {
    "text": "This is our, again, empirical transition model. S' given as a max over a' of Q tilde of s',a'.",
    "start": "1937605",
    "end": "1948045"
  },
  {
    "text": "Okay. So this would be a backup we could do.",
    "start": "1948045",
    "end": "1953640"
  },
  {
    "text": "So it's like a Bellman backup, with our optimistic reward bonus. And just to remind ourselves,",
    "start": "1953640",
    "end": "1960734"
  },
  {
    "text": "beta was still gonna be defined as 1 over 1 - gamma, square root of one half log 2 S,",
    "start": "1960734",
    "end": "1970649"
  },
  {
    "text": "A, M divided by delta. All right. So in this case,",
    "start": "1970650",
    "end": "1980010"
  },
  {
    "text": "what we wanna be able to show, we don't have to think about known or unknown state-action pairs yet. We want to show that this value,",
    "start": "1980010",
    "end": "1985860"
  },
  {
    "text": "when we compute it, is an upper bound to the true Q star, up to epsilon.",
    "start": "1985860",
    "end": "1991095"
  },
  {
    "text": "So we wanna be able to show this first optimism condition. We want to- what we're trying to argue right now is that, that beta is sufficiently large as a bonus,",
    "start": "1991095",
    "end": "1999495"
  },
  {
    "text": "that when we do this procedure we're gonna be optimistic. Okay. So let's step through it here.",
    "start": "1999495",
    "end": "2010174"
  },
  {
    "text": "Okay. So how do we show that? Let's think about a particular state-action pair.",
    "start": "2010175",
    "end": "2018335"
  },
  {
    "text": "So let's think about one state-action pair. So s,a and let's think about that we visited some n(s,a) times which is less than m, okay?",
    "start": "2018335",
    "end": "2031085"
  },
  {
    "text": "So in our algorithm, we are only gonna update our empirical estimates until we have m samples.",
    "start": "2031085",
    "end": "2039215"
  },
  {
    "text": "So [NOISE] we only use the first m samples",
    "start": "2039215",
    "end": "2047929"
  },
  {
    "text": "of s,a to compute",
    "start": "2047930",
    "end": "2055530"
  },
  {
    "text": "R hat and T, okay? After that we're gonna throw away our data.",
    "start": "2055600",
    "end": "2062520"
  },
  {
    "text": "So this is like saying the first, um, m times you visit this particular state-action pair, um,",
    "start": "2062740",
    "end": "2068030"
  },
  {
    "text": "you can use that data to try to compute an empirical model, and use it to compute an empirical model before you have m counts,",
    "start": "2068030",
    "end": "2074089"
  },
  {
    "text": "but after that you're never gonna update. I'll- I'll just put a side note in there which is, um,",
    "start": "2074090",
    "end": "2080450"
  },
  {
    "text": "[NOISE] you might think why [LAUGHTER] why should we do this? And in particular, uh, there's a really lovely description of",
    "start": "2080450",
    "end": "2087710"
  },
  {
    "text": "the whole field of machine learning by Tom Mitchell who's one of the- really, the founders of machine learning where he argues the whole discipline in machine learning.",
    "start": "2087710",
    "end": "2093800"
  },
  {
    "text": "The point is to look at, um, the foundations of how an agent can learn and also that we design algorithms that continue to improve with more data.",
    "start": "2093800",
    "end": "2102230"
  },
  {
    "text": "And this is violating that to some extent, because this is saying that even if you get 10 trillion examples of that state-action pair,",
    "start": "2102230",
    "end": "2108859"
  },
  {
    "text": "which surely would make your empirical model better, we're gonna throw all that data out. [NOISE] Just to give you a sense of why we do that or why this earlier analysis did that,",
    "start": "2108860",
    "end": "2117140"
  },
  {
    "text": "we do that for the high probability bound. The idea is that, um,",
    "start": "2117140",
    "end": "2122615"
  },
  {
    "text": "the high probability bound is gonna work, like what we saw for bandits of making sort of upper confidence bounds and kind of guaranteeing that our estimates,",
    "start": "2122615",
    "end": "2130248"
  },
  {
    "text": "say if the transition model are close to the true values. And those bounds all hold with high probability.",
    "start": "2130249",
    "end": "2136700"
  },
  {
    "text": "And so, who here has seen union bounds in different things? Okay. A few people, but most people have not.",
    "start": "2136700",
    "end": "2143240"
  },
  {
    "text": "So union bounds are a way to make sure that if you have a number of different events, all of which hold with high probability,",
    "start": "2143240",
    "end": "2148535"
  },
  {
    "text": "that the total of those events all hold with high probability. And that is essentially why here we only use a finite amount of data.",
    "start": "2148535",
    "end": "2157085"
  },
  {
    "text": "Intellectually, this is completely unsatisfying [LAUGHTER] um, because you should clearly be able to use more data and",
    "start": "2157085",
    "end": "2162230"
  },
  {
    "text": "your algorithm should do better with using more data, and empirically, we use all the data. Um, one thing that I find really satisfying for last few years is that, uh,",
    "start": "2162230",
    "end": "2170569"
  },
  {
    "text": "with my student and Tor Lattimore who's over- who's one of the authors of the bandit book that we recommend,",
    "start": "2170570",
    "end": "2175865"
  },
  {
    "text": "um, we showed that you can remove this restriction. You couldn't just continue to use data forever,",
    "start": "2175865",
    "end": "2181235"
  },
  {
    "text": "by using smarter things than union bounds. But regardless, for today, we're gonna- we'll do this. So we're gonna say- we're only gonna,",
    "start": "2181235",
    "end": "2188015"
  },
  {
    "text": "um, use up to m samples. Now, let's think about cases where n(s,a) is less than or equal to m. Okay.",
    "start": "2188015",
    "end": "2193730"
  },
  {
    "text": "So we have up to m samples, but in general, you know, it may be one, it might be two. Some number that is smaller than m,",
    "start": "2193730",
    "end": "2199715"
  },
  {
    "text": "which is some constant that we have not specified yet, okay? So what we're gonna look at is for this state-action pair.",
    "start": "2199715",
    "end": "2206420"
  },
  {
    "text": "We're gonna look at all of the experiences for that state-action pair. So let's call X_i,",
    "start": "2206420",
    "end": "2212840"
  },
  {
    "text": "to be defined to be r_i + gamma, V star of s_i.",
    "start": "2212840",
    "end": "2220055"
  },
  {
    "text": "This should look quite like what- these were the targets that we had in TD learning. Um, this is saying that the reward we got on the ith time,",
    "start": "2220055",
    "end": "2227900"
  },
  {
    "text": "we sampled s and a, and the next state we got 2 on the ith time we sampled s and a.",
    "start": "2227900",
    "end": "2234140"
  },
  {
    "text": "So this is from ith visit to s,a.",
    "start": "2234140",
    "end": "2240154"
  },
  {
    "text": "This is the next state. Next state [NOISE] Okay.",
    "start": "2240155",
    "end": "2247730"
  },
  {
    "text": "So we can define this. So we can think of each of these are gonna have an expectation of",
    "start": "2247730",
    "end": "2254330"
  },
  {
    "text": "the true Q star of s,a, okay?",
    "start": "2254330",
    "end": "2263225"
  },
  {
    "text": "Because I've just defined this. We don't have to know what V star is right now. We're just analyzing what would happen with these samples.",
    "start": "2263225",
    "end": "2269645"
  },
  {
    "text": "So if we define our samples to be r, the real reward we saw, the real next state we saw.",
    "start": "2269645",
    "end": "2274895"
  },
  {
    "text": "On average, this is really just Q star of s,a. All right. So if this is Q star of s,a,",
    "start": "2274895",
    "end": "2284075"
  },
  {
    "text": "we can think about how many samples do we need until we have a good approximation of Q star s,a,",
    "start": "2284075",
    "end": "2289535"
  },
  {
    "text": "or how far away can our average B, over the averaged- the real empirical average of X_is versus Q star",
    "start": "2289535",
    "end": "2297080"
  },
  {
    "text": "[NOISE] And we can do this using Hoeffding or other sorts of deviation bounds. A little bit like what we saw of our bandits.",
    "start": "2297080",
    "end": "2303950"
  },
  {
    "text": "So for bandits, we looked at if you have a distribution over rewards, if you have a finite number of samples of that,",
    "start": "2303950",
    "end": "2309500"
  },
  {
    "text": "how far can it be away from the true mean reward? And similarly, here we're gonna say if you have a finite number of",
    "start": "2309500",
    "end": "2314540"
  },
  {
    "text": "samples of the next state and the reward received, how far away can we be from the true Q star in this case?",
    "start": "2314540",
    "end": "2321125"
  },
  {
    "start": "2319000",
    "end": "2359000"
  },
  {
    "text": "Right. So there's some technical details here because one has to be a little bit careful about,",
    "start": "2321125",
    "end": "2328565"
  },
  {
    "text": "um, the fact that the data that we gather depends on the history.",
    "start": "2328565",
    "end": "2333619"
  },
  {
    "text": "So this is, again, one of the ch- more challenging things in, um, Markov decision processes in this sense,",
    "start": "2333620",
    "end": "2340145"
  },
  {
    "text": "or particularly Markov decision processes in that the data you gathered depends on your algorithm. So you're gonna get more samples for state-action pairs that you think are gonna be good,",
    "start": "2340145",
    "end": "2348635"
  },
  {
    "text": "and less samples for state-action pairs that you think are gonna be bad. So there's coupling.",
    "start": "2348635",
    "end": "2353690"
  },
  {
    "text": "The data isn't really IID, um, across the whole distribution, but sometimes conditioned on the fact that we're sampling for the state-action pair.",
    "start": "2353690",
    "end": "2362180"
  },
  {
    "start": "2359000",
    "end": "2647000"
  },
  {
    "text": "The next state and reward are IID because it's Markov. [NOISE] So just to",
    "start": "2362180",
    "end": "2368510"
  },
  {
    "text": "give some of- that's just to say we have to be a little bit careful here, but we can basically do the pro- use things like Hoeffding to say the probability that,",
    "start": "2368510",
    "end": "2377940"
  },
  {
    "text": "Q star of s,a, - 1 over n(s,a),",
    "start": "2379120",
    "end": "2387200"
  },
  {
    "text": "sum over i = 1 to n(s,a) of X_i,",
    "start": "2387200",
    "end": "2392734"
  },
  {
    "text": "where X_i is just what we defined up there. The probability that's greater than or equal to beta over square root of n,s,a,",
    "start": "2392735",
    "end": "2400080"
  },
  {
    "text": "is gonna be less than or equal to the exponential minus 2 beta squared,",
    "start": "2402270",
    "end": "2407485"
  },
  {
    "text": "1 minus gamma squared, okay? This is using like Hoeffding or like a similar type of deviation inequality.",
    "start": "2407485",
    "end": "2416360"
  },
  {
    "text": "You can also use ones that depend on martingales, for those of you that have seen some of those before. Regardless, this basically just allows us to say,",
    "start": "2416360",
    "end": "2423530"
  },
  {
    "text": "as you get more and more samples for this particular state-action pair, how far away could you be from optimal if you did know B star?",
    "start": "2423530",
    "end": "2429410"
  },
  {
    "text": "[NOISE] Now we know what beta is. I put it up there. So if we plug beta into here. So if you plug beta in [NOISE] the real value for beta in,",
    "start": "2429410",
    "end": "2438890"
  },
  {
    "text": "you get that this is gonna be equal to delta divided by 2 size of the state space,",
    "start": "2438890",
    "end": "2446269"
  },
  {
    "text": "size of the action space, m, okay? So it just says that this holds with high probability.",
    "start": "2446270",
    "end": "2452869"
  },
  {
    "text": "That the number- as you have more samples, um, the probability that you're gonna be far away from the true Q star is small, okay?",
    "start": "2452870",
    "end": "2462900"
  },
  {
    "text": "All right. So we can put that, like, substitute this result back in,",
    "start": "2462900",
    "end": "2468395"
  },
  {
    "text": "and we can say what that therefore means is that, if we look at the union bound across all of this,",
    "start": "2468395",
    "end": "2476060"
  },
  {
    "text": "um, well here, let me just write down one more thing which is what does this X_i actually look like.",
    "start": "2476060",
    "end": "2482270"
  },
  {
    "text": "So X_i, if you say 1 over n(s,a) of sum over i = 1 to n(s,a) of X_i.",
    "start": "2482270",
    "end": "2492940"
  },
  {
    "text": "What is that? That's actually just the equation that we had up there. Okay. So it's very similar to- it's your empirical reward,",
    "start": "2492940",
    "end": "2503403"
  },
  {
    "text": "plus gamma times T Hat s' s,a.",
    "start": "2503404",
    "end": "2510265"
  },
  {
    "text": "Almost that equation except for you've got B star here, okay?",
    "start": "2510265",
    "end": "2515640"
  },
  {
    "text": "So this should look really similar to that Q tilde up there. We're using the empirical rewards here in the emp- empirical transition.",
    "start": "2515890",
    "end": "2523279"
  },
  {
    "text": "The difference is that here we're using Q star and up there we're using Q tilde.",
    "start": "2523280",
    "end": "2528630"
  },
  {
    "text": "All right. So what this means is that if you have a number of samples,",
    "start": "2531820",
    "end": "2538905"
  },
  {
    "text": "then you can bound the difference between the thing that we're doing up the- the- this thing and the Q_star.",
    "start": "2538905",
    "end": "2545810"
  },
  {
    "text": "So let's do R_hat of s, a plus gamma sum over s_prime transition,",
    "start": "2545810",
    "end": "2553740"
  },
  {
    "text": "the empirical transition model, s, a, V_star of s prime,",
    "start": "2553740",
    "end": "2559825"
  },
  {
    "text": "minus Q_star s, a is greater than or equal to minus beta divided by square root n(s, a).",
    "start": "2559825",
    "end": "2568800"
  },
  {
    "text": "Okay? And this is gonna hold for all T, s, and a. All right.",
    "start": "2568800",
    "end": "2577560"
  },
  {
    "text": "So we've used Hoeffding and now we can relate, um, the empirical reward, the empirical transition model,",
    "start": "2577560",
    "end": "2582910"
  },
  {
    "text": "and if someone gave us the optimal Q to what Q_star is. And then, now, what we wanna do is compare this to what we're- that equation up here.",
    "start": "2582910",
    "end": "2593395"
  },
  {
    "text": "So I'll just- all right, this is one.",
    "start": "2593395",
    "end": "2598420"
  },
  {
    "text": "So that equation one up there is what we're actually doing in MBIE-EB. We keep doing that over and over again until it convergences.",
    "start": "2598420",
    "end": "2605375"
  },
  {
    "text": "So we take our empirical transition model, we take our empirical reward model, we add in this bonus,",
    "start": "2605375",
    "end": "2610954"
  },
  {
    "text": "we do value iteration until we converge. And what we would like to do now is to compare what happens to that quantity,",
    "start": "2610955",
    "end": "2618225"
  },
  {
    "text": "versus what is Q_star. And we want to show that that quantity up there is",
    "start": "2618225",
    "end": "2623930"
  },
  {
    "text": "going to be greater than or equal to Q_star, okay? So we're gonna do that by induction.",
    "start": "2623930",
    "end": "2630075"
  },
  {
    "text": "I think I'm gonna- [NOISE]. Okay.",
    "start": "2630075",
    "end": "2635970"
  },
  {
    "text": "So the proof is by induction. So what we're gonna do is we're gonna get Q_tilde,",
    "start": "2646810",
    "end": "2653175"
  },
  {
    "text": "i of s, a be the i-th iteration of value iteration.",
    "start": "2653175",
    "end": "2658490"
  },
  {
    "text": "So this is using that equation 1. So equation 1 up there.",
    "start": "2658490",
    "end": "2664170"
  },
  {
    "text": "And we're gonna let V_tilde_i equal the- for a value s just to be,",
    "start": "2664650",
    "end": "2672880"
  },
  {
    "text": "if we were to take the max action of that Q_tilde. Okay? For every state and action pair.",
    "start": "2672880",
    "end": "2680325"
  },
  {
    "text": "And what we're going to assume is that we initialize optimistically, and we're gonna initialize Q_tilde of 0 for s,",
    "start": "2680325",
    "end": "2689645"
  },
  {
    "text": "a equal to 1 over 1 minus gamma, which by definition is greater than Q_star,",
    "start": "2689645",
    "end": "2696450"
  },
  {
    "text": "is at least as good as Q_star. So that's our base case. And again, what is the- what are we trying to do?",
    "start": "2696450",
    "end": "2706280"
  },
  {
    "text": "We're trying to show optimism here for MBIE-EB. We're trying to say that if we do this procedure with MBIE-EB, we'll be optimistic.",
    "start": "2706280",
    "end": "2712545"
  },
  {
    "text": "We're going to do a proof by induction and this is the base case. So we start off, we initialize our Q_tilde optimistically,",
    "start": "2712545",
    "end": "2718559"
  },
  {
    "text": "and now we're gonna, um, assume that this holds. So we're gonna assume Q_tilde_i of s,",
    "start": "2718560",
    "end": "2729965"
  },
  {
    "text": "a is gonna- we're gonna assume that Q_tilde_i of s, a is greater than equal to Q_star,",
    "start": "2729965",
    "end": "2737620"
  },
  {
    "text": "of s, a for the previous time-step. Okay? So we're going to- All right.",
    "start": "2737620",
    "end": "2746210"
  },
  {
    "text": "So let's write out what Q_i + 1 is going to be. Q_tilde_i + 1 is going to be equal to for s,",
    "start": "2746210",
    "end": "2754369"
  },
  {
    "text": "a, is going to be equal to our empirical reward, plus gamma sum over s_prime,",
    "start": "2754370",
    "end": "2762025"
  },
  {
    "text": "or empirical transition model, times our V_tilde of i,",
    "start": "2762025",
    "end": "2771800"
  },
  {
    "text": "of s_prime, + beta over square root n(s, a).",
    "start": "2771800",
    "end": "2777180"
  },
  {
    "text": "Okay. That's just the same as equation 1.",
    "start": "2777180",
    "end": "2782474"
  },
  {
    "text": "So now we're going to say that this, by definition, is going to be greater than or equal to R_hat of s,",
    "start": "2782475",
    "end": "2791475"
  },
  {
    "text": "a, + gamma sum over s_prime, our empirical transition model,",
    "start": "2791475",
    "end": "2798085"
  },
  {
    "text": "times the true V_star. Because that's our induction- inductive hypothesis.",
    "start": "2798085",
    "end": "2804780"
  },
  {
    "text": "We assume that this held on the previous time-point. Okay. + beta, divided by square root n(s, a)",
    "start": "2804780",
    "end": "2811135"
  },
  {
    "text": "this is by my inductive hypothesis. We assume that we knew, we have a base case where this holds.",
    "start": "2811135",
    "end": "2818630"
  },
  {
    "text": "We're going to assume that, that held on the previous iteration, and then the last part that we need in here is that if we,",
    "start": "2818630",
    "end": "2825660"
  },
  {
    "text": "the- this part, we look at this part versus this part.",
    "start": "2825660",
    "end": "2831460"
  },
  {
    "text": "Okay? So if you rearrange this equation, then you can see that R_hat of s,",
    "start": "2831460",
    "end": "2841350"
  },
  {
    "text": "a, plus all this stuff, is greater than or equal to Q_star(s, a)",
    "start": "2841350",
    "end": "2848920"
  },
  {
    "text": "minus beta, square root n(s, a). Okay? So that means that if we substitute that back in, back in over here,",
    "start": "2848920",
    "end": "2860010"
  },
  {
    "text": "we can say this is equal to Q_star(s, a),",
    "start": "2860010",
    "end": "2865370"
  },
  {
    "text": "minus beta over square n(s, a), plus beta, square root n(s, a).",
    "start": "2865370",
    "end": "2874744"
  },
  {
    "text": "Should go to Q_star. Okay. So now we've shown optimism.",
    "start": "2874745",
    "end": "2883470"
  },
  {
    "text": "So the key idea in that case was to say, we know that we're getting to- that,",
    "start": "2883990",
    "end": "2891914"
  },
  {
    "text": "we're going to relate this to what would happen if we had the true Q_star, we showed that if we know the true Q_star on the next timestep,",
    "start": "2891915",
    "end": "2899135"
  },
  {
    "text": "then doing this one step backup, um, we can bound how far away we'd be from Q_star in terms of our function beta,",
    "start": "2899135",
    "end": "2905480"
  },
  {
    "text": "and then we can do an inductive, an inductive proof to show that if we were optimistic on the previous timestep,",
    "start": "2905480",
    "end": "2912250"
  },
  {
    "text": "we can always ensure that hel- held at the beginning, because we used optimistic initialization. Then we'll continue to be optimistic for all the,",
    "start": "2912250",
    "end": "2921400"
  },
  {
    "text": "for the resulting Q- Q_hat, Q_star. So this proves optimism.",
    "start": "2921400",
    "end": "2927380"
  },
  {
    "text": "Anybody who may have questions about that proof? Okay. So that's the proof of optimism.",
    "start": "2927380",
    "end": "2937200"
  },
  {
    "text": "The other key part, and I won't go through the other part in quite as much detail, but I'll, I'll talk about it briefly at a high level.",
    "start": "2937350",
    "end": "2944520"
  },
  {
    "text": "The other really important part is accuracy. I mean, bounded, I'll keep this up in case anyone's still writing.",
    "start": "2944520",
    "end": "2952925"
  },
  {
    "text": "Accuracy is really important, um, and the fact that you will eventually become accurate is important.",
    "start": "2952925",
    "end": "2960150"
  },
  {
    "text": "So the- the intuition for this part is that, um, you can think of defining a couple different,",
    "start": "2960150",
    "end": "2970010"
  },
  {
    "text": "well, you can think of defining things as being known or unknown. Okay. Somebody want me to keep this up or is everyone finished writing?",
    "start": "2970010",
    "end": "2977640"
  },
  {
    "text": "Raise your hand if you'd like it up. Okay. Okay. So in many, er,",
    "start": "2977640",
    "end": "2985595"
  },
  {
    "text": "PAC proofs for finite state-action pairs, there's this notion of knownness.",
    "start": "2985595",
    "end": "2990970"
  },
  {
    "text": "So what does it mean? So known state-action pairs.",
    "start": "2990970",
    "end": "2995680"
  },
  {
    "text": "Intuitively known state-action pairs are gonna be pairs for which we have",
    "start": "2999000",
    "end": "3004290"
  },
  {
    "text": "sufficient data that their estimated models are close to the true model.",
    "start": "3004290",
    "end": "3009765"
  },
  {
    "text": "So the intuition here, intuition (s, a) where R-hat",
    "start": "3009765",
    "end": "3019455"
  },
  {
    "text": "with (s, a) and T-hat of (s, a), s prime given (s, a) are close",
    "start": "3019455",
    "end": "3028785"
  },
  {
    "text": "to true R (s, a) and T (s, a).",
    "start": "3028785",
    "end": "3034920"
  },
  {
    "text": "[NOISE] So intuitively, if you get more and more data for a particular state-action pair,",
    "start": "3034920",
    "end": "3043244"
  },
  {
    "text": "we know from Hoeffding etc., that your estimated mean is gonna converge to the true mean,",
    "start": "3043245",
    "end": "3048765"
  },
  {
    "text": "and your transition model is also gonna converge to the true tradition model. And what we're doing here is we're sort of drawing a line in the sand and we're saying,",
    "start": "3048765",
    "end": "3056775"
  },
  {
    "text": "\"When are things- when do we have enough data for a state-action pair that we are satisfied with our empirical estimates?\"",
    "start": "3056775",
    "end": "3062970"
  },
  {
    "text": "Where we can bound how close our empirical estimates are to the true models. And if things are close enough,",
    "start": "3062970",
    "end": "3068970"
  },
  {
    "text": "then we know that using those allow us to compute a near-optimal policy. [NOISE] So if everything is known,",
    "start": "3068970",
    "end": "3077400"
  },
  {
    "text": "if all (s, a) pairs are known,",
    "start": "3077400",
    "end": "3081789"
  },
  {
    "text": "then we can show that the V_Pi star under your sort of empirical model.",
    "start": "3084470",
    "end": "3093224"
  },
  {
    "text": "I'll denote that as hat. So this is like your empirical model - V_Pi star,",
    "start": "3093225",
    "end": "3100815"
  },
  {
    "text": "and I'll put this under your empirical model for your- of your true model, um, that this is bounded.",
    "start": "3100815",
    "end": "3106500"
  },
  {
    "text": "[NOISE] I kinda go",
    "start": "3106500",
    "end": "3114360"
  },
  {
    "text": "through all the details so that you saw a little bit of this on the midterm. This is often known as the simulation lemma.",
    "start": "3114360",
    "end": "3119415"
  },
  {
    "text": "If things are close, like if your models are close, your transition model is close to the true transition model and your reward models are close to the true reward model,",
    "start": "3119415",
    "end": "3127245"
  },
  {
    "text": "then if you use those to compute a value function, your value functions are also close, which is a really cool idea.",
    "start": "3127245",
    "end": "3133650"
  },
  {
    "text": "It's basically saying, \"You can propagate the errors in your empirical model into the errors that you get in your value functions,",
    "start": "3133650",
    "end": "3138990"
  },
  {
    "text": "and the errors you get in your policies.\" So you can sort of propagate error. You can go propagate,",
    "start": "3138990",
    "end": "3145560"
  },
  {
    "text": "[NOISE] propagate empirical predictive error",
    "start": "3145560",
    "end": "3153210"
  },
  {
    "text": "[NOISE] to control error.",
    "start": "3153210",
    "end": "3160020"
  },
  {
    "text": "[NOISE] Bless you. [NOISE] Bless you. So this is- this is really nice because",
    "start": "3160020",
    "end": "3165180"
  },
  {
    "text": "you can say like, if you have good predictive error, then you can end up with small control error.",
    "start": "3165180",
    "end": "3171494"
  },
  {
    "text": "And the known state-action pairs are just providing a way to sort of quantify whether- you know,",
    "start": "3171495",
    "end": "3177450"
  },
  {
    "text": "what level do you need in order to be good enough, and the good enough you need is gonna depend on what that epsilon is you want.",
    "start": "3177450",
    "end": "3185609"
  },
  {
    "text": "So one really important idea is to think about these known state-action pairs, um,",
    "start": "3185610",
    "end": "3191535"
  },
  {
    "text": "and what they allow us to do in terms of defining some alternative MDPs.",
    "start": "3191535",
    "end": "3197789"
  },
  {
    "text": "So, in this case, I will erase this.",
    "start": "3197790",
    "end": "3202980"
  },
  {
    "text": "[NOISE] Just how you-",
    "start": "3202980",
    "end": "3210795"
  },
  {
    "text": "so you know how to alter some of the parts of the proof, um, we'll go forward. I won't go through all of it in detail,",
    "start": "3210795",
    "end": "3216375"
  },
  {
    "text": "but I'm very happy to talk about it offline. Um, [NOISE] so we can define two other sorts of MDPs.",
    "start": "3216375",
    "end": "3225330"
  },
  {
    "start": "3222000",
    "end": "3600000"
  },
  {
    "text": "We can ide- call an MDP M prime, which is equal- which is a MDP,",
    "start": "3225330",
    "end": "3233740"
  },
  {
    "text": "where for this (s,a) pair,",
    "start": "3233870",
    "end": "3238119"
  },
  {
    "text": "it's R and T. So its transition and its reward",
    "start": "3238880",
    "end": "3244200"
  },
  {
    "text": "dynamic- its dynamics and its reward model are equal to the true MDP on (s, a) in known set,",
    "start": "3244200",
    "end": "3258460"
  },
  {
    "text": "else, it's equal to the M tilde model.",
    "start": "3260660",
    "end": "3267809"
  },
  {
    "text": "So these are where we sort of start to define these slightly weird MDPs. So this is a model that is not quite",
    "start": "3267810",
    "end": "3273630"
  },
  {
    "text": "our M tilde model that we're using to do in that equation one, which we have these kind of like rewards plus bonuses plus our empirical transition model.",
    "start": "3273630",
    "end": "3282510"
  },
  {
    "text": "And it's not quite the real model. It's saying, on things that are known,",
    "start": "3282510",
    "end": "3288255"
  },
  {
    "text": "we're gonna use the real-world model. Again, we don't know what any of these are, these are just tools for analysis.",
    "start": "3288255",
    "end": "3293619"
  },
  {
    "text": "And then on state-action pairs which are unknown, we're gonna use this optimistic model. It defines an MDP.",
    "start": "3293620",
    "end": "3300515"
  },
  {
    "text": "Um, uh, and the reason that this is useful as we can end up using it to help figure out how does the MDP that we have relate to the real MDP on state-action pairs that are known?",
    "start": "3300515",
    "end": "3311835"
  },
  {
    "text": "Okay? This is what MDP we end up using for this, and then the other one is a similar one, which is M-hat prime,",
    "start": "3311835",
    "end": "3318210"
  },
  {
    "text": "which is an MDP equal to M-hat.",
    "start": "3318210",
    "end": "3324075"
  },
  {
    "text": "So this is just the, uh, uh, empirical- empirical estimates on",
    "start": "3324075",
    "end": "3335100"
  },
  {
    "text": "K and equal to M tilde on all others.",
    "start": "3335100",
    "end": "3340740"
  },
  {
    "text": "[NOISE] So on the known set.",
    "start": "3340740",
    "end": "3346030"
  },
  {
    "text": "Okay? So this is a MDP, where for the known set,",
    "start": "3346040",
    "end": "3351300"
  },
  {
    "text": "we use our empirical estimates, not the true- true est- not the true models, and then we use M tilde on all of these other ones.",
    "start": "3351300",
    "end": "3359500"
  },
  {
    "text": "So the idea is that we can use these different forms of MDPs and we qua- can",
    "start": "3360590",
    "end": "3365940"
  },
  {
    "text": "quantify how far off the value is that we get by computing,",
    "start": "3365940",
    "end": "3371115"
  },
  {
    "text": "uh, using our- our Q tilde versus the value on these other ones,",
    "start": "3371115",
    "end": "3376875"
  },
  {
    "text": "and we can use it to basically do a series of inequalities to relate the value we get by executing our policy versus the value we get,",
    "start": "3376875",
    "end": "3385650"
  },
  {
    "text": "um, in the true world, and to the optimal policy. So these are the types of tools that allow us to help prove",
    "start": "3385650",
    "end": "3393599"
  },
  {
    "text": "accuracy on state-action pairs that we know about. Yeah.",
    "start": "3393600",
    "end": "3401550"
  },
  {
    "text": "So is that M-hat and an M tilde? Yes. Whether.",
    "start": "3401550",
    "end": "3406740"
  },
  {
    "text": "It's M-hat. So M-hat here is the empirical estimates like the- um, uh, if you use just, um,",
    "start": "3406740",
    "end": "3412619"
  },
  {
    "text": "[NOISE] the counts of the rewards and the count of transition model. M tilde is the empirical estimates plus the bonus.",
    "start": "3412620",
    "end": "3418680"
  },
  {
    "text": "So the transition model in these two cases would be identical, but thi- this one would have the reward bonus there.",
    "start": "3418680",
    "end": "3425730"
  },
  {
    "text": "Okay. So intuitively, this, uh, allows us to help quantify the accuracy.",
    "start": "3425730",
    "end": "3431309"
  },
  {
    "text": "Um, [NOISE] the final thing that I guess I just want to mention briefly, uh, of how these types of proofs tends to work is that",
    "start": "3431310",
    "end": "3438570"
  },
  {
    "text": "we need this bounded learning complexity. We need to make sure that we're not gonna continuously update",
    "start": "3438570",
    "end": "3444060"
  },
  {
    "text": "our q function and we're not gonna continuously run into unknown state-action pairs. So the last part is to sort of, you know,",
    "start": "3444060",
    "end": "3451230"
  },
  {
    "text": "how would we prove 3, which is bounded learning complexity.",
    "start": "3451230",
    "end": "3457650"
  },
  {
    "text": "[NOISE] And the intuition",
    "start": "3457650",
    "end": "3465119"
  },
  {
    "text": "for this is a little bit like the general intuition for optimism. So the intuition for optimism was to say if you assume the world is awesome,",
    "start": "3465120",
    "end": "3473715"
  },
  {
    "text": "either the world is awesome, in which case you have low regret. In the case of PAC, that means,",
    "start": "3473715",
    "end": "3479895"
  },
  {
    "text": "if you assume the world is awesome and it really is awesome, that's like not making a mistake. That's like picking the really- the- the action that you pick is good.",
    "start": "3479895",
    "end": "3487035"
  },
  {
    "text": "So then you won't suffer, um, uh, a worse than epsilon decision.",
    "start": "3487035",
    "end": "3492164"
  },
  {
    "text": "And then we want to be able to say that the times where you don't make mistakes or where you do make mistakes,",
    "start": "3492165",
    "end": "3497370"
  },
  {
    "text": "where you pick an action that's bad, which is less than epsilon close is bounded, and that's what this is about.",
    "start": "3497370",
    "end": "3503760"
  },
  {
    "text": "And the key idea here is the pigeonhole principle.",
    "start": "3503760",
    "end": "3507940"
  },
  {
    "text": "So the idea is that if you think about- you don't have to have episodic MDPs,",
    "start": "3512390",
    "end": "3518910"
  },
  {
    "text": "but if you ha- um, you can think of dividing your stream of experience into episodes. And during each, um, sort of episode,",
    "start": "3518910",
    "end": "3525960"
  },
  {
    "text": "you can think about whether it's, uh, likely that you're gonna run into an unknown state-action pair. So you consider what's the probability that we reach an unknown state-action pair?",
    "start": "3525960",
    "end": "3537480"
  },
  {
    "text": "And remember, an unknown state-action pair here is one that we don't have good models of like we've only visited it once or something.",
    "start": "3537480",
    "end": "3543000"
  },
  {
    "text": "So what's the probability we're gonna reach an unknown state-action pair in T steps. [NOISE] Okay.",
    "start": "3543000",
    "end": "3549010"
  },
  {
    "text": "So what we can show in this case is that if this is low, this is small, if small,",
    "start": "3549350",
    "end": "3555645"
  },
  {
    "text": "that probability is small, we're being near accurate, near- we're being near optimal.",
    "start": "3555645",
    "end": "3561540"
  },
  {
    "text": "So if- if it's a really small probability that you're gonna reach anything that's unknown,",
    "start": "3561540",
    "end": "3567255"
  },
  {
    "text": "we can show that on the known state-action pairs you're being near optimal. So if you're unlikely to reach anything that where you have bad models of it,",
    "start": "3567255",
    "end": "3575505"
  },
  {
    "text": "you're gonna be near optimal. If it's large, [NOISE] this can't happen too many times.",
    "start": "3575505",
    "end": "3584115"
  },
  {
    "text": "So if it's large you're gonna visit it, it can't happen too many times. [NOISE].",
    "start": "3584115",
    "end": "3591579"
  },
  {
    "text": "Because if this is large, that means that you're really likely to reach an unknown state-action pair. Remember, I said that for every state-action pair,",
    "start": "3591580",
    "end": "3598404"
  },
  {
    "text": "you only update it at most m times. So by the Pigeonhole principle,",
    "start": "3598405",
    "end": "3604180"
  },
  {
    "text": "this probability cannot stay high for too long. Essentially, you're gonna have a function of like the number of states,",
    "start": "3604180",
    "end": "3612340"
  },
  {
    "text": "the number of actions, and m. It's larger than that. But this is saying that you need to be able to visit each state-action pair m times.",
    "start": "3612340",
    "end": "3621340"
  },
  {
    "text": "So that, that goes into our end bound here of the times we're gonna make mistakes. And we, we're- we might sort of reach things that are unknown.",
    "start": "3621340",
    "end": "3629174"
  },
  {
    "text": "Um, it might take us more steps and we might make some bad decisions along the way. But essentially, um, things can only be unknown for m steps for each state-action pair,",
    "start": "3629175",
    "end": "3638760"
  },
  {
    "text": "which means that our probability here has to be bounded. So eventually, everything has to be known or you have to be acting near optimally.",
    "start": "3638760",
    "end": "3647770"
  },
  {
    "text": "And that allows us to show that things have, uh, that things are PAC.",
    "start": "3647770",
    "end": "3654160"
  },
  {
    "text": "It has bounded. We're gonna make a bounded number of bad decisions. So either, we're not going to be reaching any part of the state-action space which is unknown.",
    "start": "3654160",
    "end": "3661435"
  },
  {
    "text": "So everything we reach, we have good models of them and we're using them to make good decisions, or we are reaching things, and then in that case,",
    "start": "3661435",
    "end": "3667869"
  },
  {
    "text": "we're getting information because we're getting a new observation of what it's like to be in that state-action pair and we get only,",
    "start": "3667870",
    "end": "3674155"
  },
  {
    "text": "something can only continue to be unknown until we get m counts. Do you mind putting down the thing again?",
    "start": "3674155",
    "end": "3682160"
  },
  {
    "text": "Okay. So that gives us an overview of why MBIE-EB is PAC as well as sort of why a lot of,",
    "start": "3692670",
    "end": "3701559"
  },
  {
    "text": "um, the types of proofs that you do to show things are PAC. And I think the, the key idea in this is really the sort of notion of optimism, and accuracy,",
    "start": "3701560",
    "end": "3709930"
  },
  {
    "text": "and the ability to make progress, ability not to be stuck always wandering, by decreasing these confidence intervals sufficiently fast.",
    "start": "3709930",
    "end": "3717920"
  },
  {
    "text": "Okay. So now let's go back to Bayesian-ness. So that kind of concludes the PAC MDP part for a while.",
    "start": "3719790",
    "end": "3728010"
  },
  {
    "text": "There's been a lot of exciting recent work in this area. Um, I guess let me see if I can just really quickly briefly bring this up.",
    "start": "3728010",
    "end": "3733650"
  },
  {
    "text": "Um, this is a form of presentation. One of the TAs is giving up at Berkeley today on some of our joint work together.",
    "start": "3733650",
    "end": "3739944"
  },
  {
    "text": "Just to highlight here, um, in terms of sort of PAC and regret analysis, there's been a lot of progress, uh,",
    "start": "3739945",
    "end": "3745330"
  },
  {
    "text": "on getting better approaches. Um, so over the last few years, I mean, some of my grad students, some other really",
    "start": "3745330",
    "end": "3751315"
  },
  {
    "text": "nice groups who've been doing a lot of work on this, and we're also now trying to have an analysis that is problem dependent, which means that if the algorithm has more structure,",
    "start": "3751315",
    "end": "3758380"
  },
  {
    "text": "then we should need less data in order to learn to make good decisions. Okay, so now let's be Bayesian and see how being Bayesian helps us.",
    "start": "3758380",
    "end": "3766675"
  },
  {
    "text": "So we saw Bayesian Bandits and in Bayesian Bandits we said, we're gonna assume that we have some parametric knowledge",
    "start": "3766675",
    "end": "3773184"
  },
  {
    "text": "about how the rewards are distributed. So we're going to think about there being a posterior distribution of rewards,",
    "start": "3773185",
    "end": "3779005"
  },
  {
    "text": "and we're going to use that to guide exploration. And we particularly talked about this notion of probability matching,",
    "start": "3779005",
    "end": "3785200"
  },
  {
    "text": "which is we want to select actions with the probability that they are optimal and it turns out that Thompson Sampling allowed us to do that.",
    "start": "3785200",
    "end": "3792805"
  },
  {
    "text": "So in these, sort of, approaches, it was very helpful if we have conjugate priors,",
    "start": "3792805",
    "end": "3799344"
  },
  {
    "text": "which allowed us to analytically compute the posterior over the rewards of an arm given",
    "start": "3799344",
    "end": "3804970"
  },
  {
    "text": "the data we've observed and our previous prior over the probability of different rewards for that arm.",
    "start": "3804970",
    "end": "3811195"
  },
  {
    "text": "So we saw as one example of that the Bernoulli, which means that the reward is just 0, 1 and the beta distribution,",
    "start": "3811195",
    "end": "3817645"
  },
  {
    "text": "and the beta distribution is one where we can think of the alphas as being counts of the number of times we've seen a +1,",
    "start": "3817645",
    "end": "3824005"
  },
  {
    "text": "and beta is the number of times we've seen the arm being 0 and as we get observations of one or either of those outcomes,",
    "start": "3824005",
    "end": "3831700"
  },
  {
    "text": "then we just update our beta distribution. So that allowed us to define Thompson Sampling for bandits,",
    "start": "3831700",
    "end": "3838960"
  },
  {
    "text": "which was this algorithm where we say at each time step, we first sample a particular reward for each of the different arms,",
    "start": "3838960",
    "end": "3845680"
  },
  {
    "text": "and then we sample a reward distribution we compute the expectation",
    "start": "3845680",
    "end": "3851080"
  },
  {
    "text": "for those reward distributions and we act accordingly. So we saw this for the toe example,",
    "start": "3851080",
    "end": "3856195"
  },
  {
    "text": "where we saw that we would sample different rewards, um, and then use these to act and at least in that example,",
    "start": "3856195",
    "end": "3861550"
  },
  {
    "text": "we were seeing that we happen to exploit much faster than what we were seeing with an upper confidence bound approach.",
    "start": "3861550",
    "end": "3867025"
  },
  {
    "text": "So a very similar thing can be done in the case of MDPs. So now in being Bayesian for Models-Based RL,",
    "start": "3867025",
    "end": "3875575"
  },
  {
    "text": "we're going to have a distribution over MDP models. So what's the difference here?",
    "start": "3875575",
    "end": "3880930"
  },
  {
    "text": "That means we're going to have both transitions and rewards.",
    "start": "3880930",
    "end": "3886700"
  },
  {
    "text": "So we're going to have the rewards should look very similar to bandits. So the rewards, very similar to bandits.",
    "start": "3888630",
    "end": "3898045"
  },
  {
    "text": "You can also use betas. If your reward distribution is 0, 1, you could also use betas and Bernoulli's.",
    "start": "3898045",
    "end": "3903460"
  },
  {
    "text": "So this is very similar to bandits. T is a bit different.",
    "start": "3903460",
    "end": "3908900"
  },
  {
    "text": "We're not gonna talk a lot about the different distributions you can use. But for example if we're in tabular domains,",
    "start": "3910220",
    "end": "3916859"
  },
  {
    "text": "T can be a multinomial, and the conjugate prior for multinomial is a Dirichlet.",
    "start": "3916860",
    "end": "3924920"
  },
  {
    "text": "Its conjugate. So if you want your,",
    "start": "3928740",
    "end": "3934090"
  },
  {
    "text": "your transition model to be a multinomial, which is the probability over all the other states and actions,",
    "start": "3934090",
    "end": "3939190"
  },
  {
    "text": "then a conjugate prior for that is a Dirichlet, which has a very nice intuitive, um, description similar to what beta is.",
    "start": "3939190",
    "end": "3947260"
  },
  {
    "text": "In beta, you could think of this as just being the number of times you've observed 1 or 0. If you look at a Dirichlet,",
    "start": "3947260",
    "end": "3953410"
  },
  {
    "text": "you can think of this as being the number of times you've reached each of the next states, so S1 S2 S3 S4.",
    "start": "3953410",
    "end": "3958975"
  },
  {
    "text": "So the Dirichlet distribution would be parameterized by a vector for one of each of the states.",
    "start": "3958975",
    "end": "3966140"
  },
  {
    "text": "And again, we can use this sort of posterior to update it to allow us to do exploration.",
    "start": "3966690",
    "end": "3972204"
  },
  {
    "text": "So in this case, we're going to sample an MDP from the posterior and then solve it.",
    "start": "3972205",
    "end": "3979494"
  },
  {
    "text": "So if we look at what the algorithm looks like, it's going to look very similar to Thompson Sampling for Bandits,",
    "start": "3979495",
    "end": "3984550"
  },
  {
    "text": "except for now we're going to start off and we have to define, um, a prior over the dynamics and reward model for every single state-action pair.",
    "start": "3984550",
    "end": "3991525"
  },
  {
    "text": "So notice this is tabular. We're assuming that we have a finite set of S and A.",
    "start": "3991525",
    "end": "4000190"
  },
  {
    "text": "So we can write this down. We can write down, we have one distribution for every single state-action pair for both the dynamics and the reward model,",
    "start": "4000410",
    "end": "4009045"
  },
  {
    "text": "and then what happens is that you sample an MDP from these distributions. So for every single state-action pair,",
    "start": "4009045",
    "end": "4014745"
  },
  {
    "text": "you sample the dynamics model and a reward model. And now you have an MDP,",
    "start": "4014745",
    "end": "4020324"
  },
  {
    "text": "and so once you have that MDP, you compute the optimal value for it. So this is obviously more computationally intensive than what we had for bandits,",
    "start": "4020324",
    "end": "4029474"
  },
  {
    "text": "but it's certainly a reasonable thing to do. And then you act optimally with respect to the Q-star you have for that sampled MDP.",
    "start": "4029475",
    "end": "4037349"
  },
  {
    "text": "So this is known as Thompson Sampling for MDPs. It also implements probability matching and empirically,",
    "start": "4037350",
    "end": "4045150"
  },
  {
    "text": "it can often do really well just like Thompson Sampling did really well often for bandits.",
    "start": "4045150",
    "end": "4050920"
  },
  {
    "text": "So I think that's probably all I'll say about Thompson Sampling for MDPs. There's been, uh, a number of different works on this.",
    "start": "4051620",
    "end": "4058680"
  },
  {
    "text": "Just to highlight some people that do some really nice work on this from Stanford. Ben Van Roy, Roy's group has a lot of work on this and",
    "start": "4058680",
    "end": "4067830"
  },
  {
    "text": "sometimes they call it posterior sampling for MDPs.",
    "start": "4067830",
    "end": "4074260"
  },
  {
    "text": "So people like- some of his former students like Dan Russo and Ian Osband. Ian's now at Deepmind.",
    "start": "4075080",
    "end": "4082155"
  },
  {
    "text": "Dan Russo is now at NYU. Uh, they've done some really nice work on these types of spaces. Yes?",
    "start": "4082155",
    "end": "4088620"
  },
  {
    "text": "[inaudible] generalized these like non-tabular MDPs? Yes. Great question. So the question was whether or not we can generalize this to non-tabular MDPs?",
    "start": "4088620",
    "end": "4096900"
  },
  {
    "text": "Yes, and I'll talk about that in a second. But kinda poorly. [LAUGHTER] But yes, that's the goal.",
    "start": "4096900",
    "end": "4103290"
  },
  {
    "text": "All right, anybody have any other questions about, sort of, finite state and action scenarios? Now we are going to talk a little bit about generalization.",
    "start": "4103290",
    "end": "4111040"
  },
  {
    "text": "Okay. All right. So of course, everything I've just been saying right now is for finite state and action spaces,",
    "start": "4112040",
    "end": "4120470"
  },
  {
    "text": "which is not very satisfying because if we think about these types of bounds, um, I said this was polynomial in the size of the state and action space.",
    "start": "4120470",
    "end": "4128555"
  },
  {
    "text": "So what if S is infinite? I mean, it says we can make",
    "start": "4128555",
    "end": "4133950"
  },
  {
    "text": "an infinite number of mistakes and that seems sort of unfortunate. Um, so it's not clear that this initial, uh,",
    "start": "4133950",
    "end": "4139275"
  },
  {
    "text": "framework is is it all helpful when your state space is either infinite or insanely large like the space of pixels.",
    "start": "4139275",
    "end": "4146130"
  },
  {
    "text": "Um, though- even though, the framing of this is really nice, we'd like to be able to take these types of ideas up to generalization,",
    "start": "4146130",
    "end": "4153585"
  },
  {
    "text": "but we'd like to figure out how to, how to use them in a way that can be practical. Now, when we start to- so this is a very active area of current research,",
    "start": "4153585",
    "end": "4162630"
  },
  {
    "text": "there has been a lot of different ideas about this for the last few years. I do want to highlight that on the theory side, we still have a long way to go.",
    "start": "4162630",
    "end": "4168720"
  },
  {
    "text": "Uh, as we talked about some with function approximation, even just function approximation and doing",
    "start": "4168720",
    "end": "4174120"
  },
  {
    "text": "control like off policy control like Q learning, we said that we didn't have good asymptotic guarantees for some of the basic algorithms.",
    "start": "4174120",
    "end": "4181665"
  },
  {
    "text": "Um, so if we don't even have good asymptotic guarantees, it's unlikely that we would have really nice finite.",
    "start": "4181665",
    "end": "4187275"
  },
  {
    "text": "These are often known as finite sample guarantees because they guarantee that the number of mistakes you're gonna make is finite.",
    "start": "4187275",
    "end": "4193875"
  },
  {
    "text": "So we have relatively little theory in this case it's something that my group is working on. There's several other groups that are also working actively on this,",
    "start": "4193875",
    "end": "4201000"
  },
  {
    "text": "but there's a lot still to be done. But there has been some really nice empirical results recently.",
    "start": "4201000",
    "end": "4206535"
  },
  {
    "text": "Okay. So let's think first about generalization and optimism, like optimism under uncertainty where we're now in a really really large state space.",
    "start": "4206535",
    "end": "4214005"
  },
  {
    "text": "So let's think about what might need to be modified if everything was extremely large. If we go back to this algorithm.",
    "start": "4214005",
    "end": "4219600"
  },
  {
    "text": "Well first of all, we had this, these sort of counts that",
    "start": "4219600",
    "end": "4225135"
  },
  {
    "text": "we're keeping track of sort of for every single state-action pair,",
    "start": "4225135",
    "end": "4231554"
  },
  {
    "text": "what the counts were. So this isn't going to scale because if",
    "start": "4231555",
    "end": "4236910"
  },
  {
    "text": "you're in a scenario where your state spaces is pixels, you may never see this same set of pixels again.",
    "start": "4236910",
    "end": "4242340"
  },
  {
    "text": "Does anybody have any ideas of how you might extend this to the deep learning setting?",
    "start": "4242340",
    "end": "4247599"
  },
  {
    "text": "Like what we would like in this case is some way to quantify our uncertainty, our uncertainty over sort of states and actions.",
    "start": "4247640",
    "end": "4255045"
  },
  {
    "text": "Um, but we don't wanna do it based on like raw counts because then everything will just be one forever. Yeah?",
    "start": "4255045",
    "end": "4262349"
  },
  {
    "text": "Could you use some form of like bounded VFA? So the suggestion was whether we could do some form of like a,",
    "start": "4262350",
    "end": "4269940"
  },
  {
    "text": "a VFA for example. Um, yes, we could imagine trying to use some form of, uh,",
    "start": "4269940",
    "end": "4275265"
  },
  {
    "text": "some sort of density model or some sort of way to try to either get an estimate of how much related information have we see,",
    "start": "4275265",
    "end": "4282765"
  },
  {
    "text": "or how many related states that we'd seen in this area. [inaudible]",
    "start": "4282765",
    "end": "4287885"
  },
  {
    "text": "Somewhere like being able to [NOISE] [inaudible] some sort of [inaudible].",
    "start": "4287885",
    "end": "4295760"
  },
  {
    "text": "Yeah. So [inaudible] to use some sort of embedding, absolutely. So another thing you could do here too is to do some sort of- form of compression of the state space.",
    "start": "4295760",
    "end": "4302060"
  },
  {
    "text": "One of the challenges [NOISE] is the right compression in the state space depends on the decisions you want to make. And so generally, it will be non-stationary.",
    "start": "4302060",
    "end": "4308929"
  },
  {
    "text": "But that's not necessarily bad. You might just want to go back and forth between those. So those sort of ideas are great. In general, we want some way to quantify our uncertainty that is going to have to generalize,",
    "start": "4308930",
    "end": "4317689"
  },
  {
    "text": "and say similar nearby states. Um, if we visited that area of the world a lot, then we should have less of a bonus on that in terms of optimism.",
    "start": "4317689",
    "end": "4326060"
  },
  {
    "text": "[NOISE] Okay. So as I said, the sort of counts of s, a and s, a, s' are not going to be useful if we're only going to encounter things once.",
    "start": "4326060",
    "end": "4334385"
  },
  {
    "text": "Um, another thing that I want to highlight is, the methods that I was talking about before were really model-based.",
    "start": "4334385",
    "end": "4339770"
  },
  {
    "text": "Lot of the work there is model-based. In contrast, a lot less of the things that- it's starting to change recently over the last year,",
    "start": "4339770",
    "end": "4346520"
  },
  {
    "text": "but in general there's been much less work on the model-based approaches for deep learning in terms of RL.",
    "start": "4346520",
    "end": "4353285"
  },
  {
    "text": "And I think that's because the model-free and the policy search techniques have generally been much better. In part because the error that you make in",
    "start": "4353285",
    "end": "4359900"
  },
  {
    "text": "your models accumulates a lot as you do planning. And so I always remember David Silvers' first talk about this,",
    "start": "4359900",
    "end": "4366020"
  },
  {
    "text": "or one of his earliest talks about this from, I think, 2014 where he showed this beautiful sort of model-based,",
    "start": "4366020",
    "end": "4371975"
  },
  {
    "text": "um, uh, simulation, which was horrible for planning. So the errors really have to be very good,",
    "start": "4371975",
    "end": "4377780"
  },
  {
    "text": "uh, the- the errors have to be very small, and it's not clear that the representation you get by maximizing for predictive loss is always going to be your best for planning.",
    "start": "4377780",
    "end": "4386315"
  },
  {
    "text": "So we'd really like to be able to take the ideas we saw before for things like MBIE-EB, um, and translate them over to the model-free approach, and think about some way to,",
    "start": "4386315",
    "end": "4394940"
  },
  {
    "text": "uh, to encode uncertainty in the deep neural network setting. So let's think about doing something like Q-learning, uh, like deep",
    "start": "4394940",
    "end": "4403789"
  },
  {
    "text": "Q-learning, and in this case- so I've been a little loose here, you could- this could be a target.",
    "start": "4403790",
    "end": "4408875"
  },
  {
    "text": "So target, could fix this. But think about something like sort of the general Q-learning and Q-target, um,",
    "start": "4408875",
    "end": "4416420"
  },
  {
    "text": "where we use the max of our current function approximation or we use the max of our current target function approximation.",
    "start": "4416420",
    "end": "4422795"
  },
  {
    "text": "So one idea for- inspired from MBIE-EB would be simply to include a bonus term.",
    "start": "4422795",
    "end": "4428150"
  },
  {
    "text": "So again, this could be, you know, a fixed target. But the idea here is that when you are updating your parameters,",
    "start": "4428150",
    "end": "4436309"
  },
  {
    "text": "just put in a bonus term for that particular state-action pair when you're",
    "start": "4436309",
    "end": "4442040"
  },
  {
    "text": "doing your Q-learning update or when you're refitting your weights. Of course, we have to know what that Q bonus would be,",
    "start": "4442040",
    "end": "4448190"
  },
  {
    "text": "but this would help with the planning aspect. So now we're in the model-free environment, or model-free setting,",
    "start": "4448190",
    "end": "4453200"
  },
  {
    "text": "and so we could- [NOISE] when we're doing our sort of Q-learning updates, let's insert a bonus term. So that's why I chose MBIE-EB is the algorithm to show you before",
    "start": "4453200",
    "end": "4461405"
  },
  {
    "text": "because I think that those sort of reward bonuses are much easier to extend to the model-free case.",
    "start": "4461405",
    "end": "4466699"
  },
  {
    "text": "Now, of course, the question is what should that reward bonus be? It's got to reflect some sort of uncertainty about",
    "start": "4466700",
    "end": "4473180"
  },
  {
    "text": "the future reward from that state-action pair. And there's a lot of different approaches that have been trying to make progress on this.",
    "start": "4473180",
    "end": "4479405"
  },
  {
    "text": "So Mark Bellemare, um, [NOISE] and some of the follow-up papers thought about sort of having a density model,",
    "start": "4479405",
    "end": "4486260"
  },
  {
    "text": "trying to, er, explicitly estimate a density model over the states or state-action pairs that you've visited. Um, other people have done sort of hash based approaches,",
    "start": "4486260",
    "end": "4495439"
  },
  {
    "text": "which is sort of more similar to the embedding in a way, try to hash your state space, and then use those- use counts over that hash state-space,",
    "start": "4495439",
    "end": "4501260"
  },
  {
    "text": "and then update your hash function over time. So that's some of the work that's come out of Berkeley.",
    "start": "4501260",
    "end": "4506630"
  },
  {
    "text": "So there- there's different, um, different ways to quantify this. Another thing I want to highlight here is that",
    "start": "4506630",
    "end": "4512119"
  },
  {
    "text": "these bonus terms are generally computed at the time of visit. When we looked at MBIE-EB, you could recompute these later.",
    "start": "4512120",
    "end": "4518780"
  },
  {
    "text": "So if you- if you're storing things in your episodic replay buffer, you might want to update those over time,",
    "start": "4518780",
    "end": "4525275"
  },
  {
    "text": "because those state-actions, if you now- later sample that reward,",
    "start": "4525275",
    "end": "4530300"
  },
  {
    "text": "a next state pair from your, uh, replay buffer, you may want to change that bonus term.",
    "start": "4530300",
    "end": "4535385"
  },
  {
    "text": "So there's a number of different subtleties when we try to bring us up to deep learning, but there's been some really encouraging progress.",
    "start": "4535385",
    "end": "4541385"
  },
  {
    "text": "Um, so let me just- just- so in this case,",
    "start": "4541385",
    "end": "4546974"
  },
  {
    "text": "what we can see is the initial work from Mark Bellemare's group, um, where we compare on Montezuma's Revenge,",
    "start": "4546975",
    "end": "4553120"
  },
  {
    "text": "which is considered one of the hardest Atari games, so the progress of a standard DQN agent that was using e-greedy,",
    "start": "4553120",
    "end": "4558910"
  },
  {
    "text": "um, there's a number of different rooms in Montezuma's Revenge. In this case, you can see after 50 million frames,",
    "start": "4558910",
    "end": "4563965"
  },
  {
    "text": "um, it was doing incredibly badly. Don't- it had only been through two of the rooms. Whereas if you use this sort of exploration bonus,",
    "start": "4563965",
    "end": "4570440"
  },
  {
    "text": "this one did much, much better, so just enormously better by being strategic. Okay? So I think that highlights",
    "start": "4570440",
    "end": "4576760"
  },
  {
    "text": "the empirical significance of doing this sort of strategic exploration. Um, let's think briefly about Thompson sampling.",
    "start": "4576760",
    "end": "4583675"
  },
  {
    "text": "So in this case, one of the ideas that we did a few years ago is to say,",
    "start": "4583675",
    "end": "4588880"
  },
  {
    "text": "you could do Thompson sampling both over your representation and the parameters of your model. What do I mean by that? I mean that if you have a really large state space,",
    "start": "4588880",
    "end": "4596360"
  },
  {
    "text": "you can imagine collapsing your states and doing a dynamic state aggregation, and sampling over possible state aggregations",
    "start": "4596360",
    "end": "4603095"
  },
  {
    "text": "as a way to sort of do collapsing your state space. Uh, and- and Thompson sampling can be extended to sampling over representations.",
    "start": "4603095",
    "end": "4611060"
  },
  {
    "text": "So that works well, but it doesn't scale up fully. When you want to really scale up,",
    "start": "4611060",
    "end": "4616835"
  },
  {
    "text": "if you want to be a model-free, it's a little bit different than what we saw before. Because before we saw model-based approaches,",
    "start": "4616835",
    "end": "4622655"
  },
  {
    "text": "and now we're sort of wanting to sample from a posterior over possible Q stars. And it's not clear how to write that down.",
    "start": "4622655",
    "end": "4629945"
  },
  {
    "text": "I don't think we've made good progress on that even at the tabular setting. Uh, but that's what we're trying to do- people who are",
    "start": "4629945",
    "end": "4635480"
  },
  {
    "text": "trying to do for the deep learning setting. And there's been a couple of different main approaches. Uh, one is again sort of from Ian Osband,",
    "start": "4635480",
    "end": "4641719"
  },
  {
    "text": "who was here for his PhD. They did bootstrap DQN, the idea here is that you can use bootstrapping on your samples,",
    "start": "4641720",
    "end": "4648710"
  },
  {
    "text": "[NOISE] and you can build a number of different agents. So now you kind of have C, Q values instead of just one Q value,",
    "start": "4648710",
    "end": "4655085"
  },
  {
    "text": "and then you can act optimistically with respect to that set. It's not incredibly effective.",
    "start": "4655085",
    "end": "4660320"
  },
  {
    "text": "It gives you some performance gain. Um, another thing that we did, uh, we sort of- we- someone I was working with before,",
    "start": "4660320",
    "end": "4667790"
  },
  {
    "text": "I was involved with one of the earlier versions of this, um, and since then they've been continuing to push it forward. The idea here is that you kind of fix your embedding,",
    "start": "4667790",
    "end": "4674990"
  },
  {
    "text": "you do linear regression on top. And that is super simple, but if you do Bayesian linear regression,",
    "start": "4674990",
    "end": "4680090"
  },
  {
    "text": "you get a notion of uncertainty, and that actually gives a lot of performance gains in a lot of cases.",
    "start": "4680090",
    "end": "4685250"
  },
  {
    "text": "So you can sort of have like a really little bit of uncertainty representation on top of a deep neural network.",
    "start": "4685250",
    "end": "4691505"
  },
  {
    "text": "But this is an active area. There's a lot of people thinking about this different type of work, uh,",
    "start": "4691505",
    "end": "4696785"
  },
  {
    "text": "and I think it's going to continue to be a really big area because we still haven't made sufficient progress, in how to do exploration plus generalization.",
    "start": "4696785",
    "end": "4704369"
  },
  {
    "text": "So just to summarize, I know we've done, um, quite a lot of theory in this section. The things that you should have- should make sure you are familiar with is to",
    "start": "4704500",
    "end": "4711500"
  },
  {
    "text": "understand what is the tension between exploration and exploitation in RL? Why this doesn't arise in on- other types of settings?",
    "start": "4711500",
    "end": "4717650"
  },
  {
    "text": "You should be able to define these different sorts of criteria for what it means to be a good algorithm in terms of PAC or regret,",
    "start": "4717650",
    "end": "4723574"
  },
  {
    "text": "um, and be able to map the sort of algorithms that we've discussed in detail in class to these different forms of criteria.",
    "start": "4723575",
    "end": "4730550"
  },
  {
    "text": "So if I say, you know, is this optimism under uncertainty approach, is that good for PAC or regret or both?",
    "start": "4730550",
    "end": "4736010"
  },
  {
    "text": "You should know that it's good for both. So that's the kind of high level that you should be able to understand from the things we've been doing,",
    "start": "4736010",
    "end": "4742610"
  },
  {
    "text": "and just know that there's a lot of really exciting work that's continuing to go on there, including defining new metrics of performance.",
    "start": "4742610",
    "end": "4748190"
  },
  {
    "text": "And next time, we'll hear about meta-learning. Thanks.",
    "start": "4748190",
    "end": "4751380"
  }
]