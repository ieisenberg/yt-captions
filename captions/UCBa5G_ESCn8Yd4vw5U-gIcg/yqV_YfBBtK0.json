[
  {
    "start": "0",
    "end": "4962"
  },
  {
    "text": "Welcome back, everyone.",
    "start": "4962",
    "end": "5920"
  },
  {
    "text": "This is part two in our series\non contextual representations.",
    "start": "5920",
    "end": "8910"
  },
  {
    "text": "We've come to the heart of it,\nthe transformer architecture.",
    "start": "8910",
    "end": "11940"
  },
  {
    "text": "While we're still\nfeeling fresh, I",
    "start": "11940",
    "end": "13920"
  },
  {
    "text": "propose that we just dive\ninto the core model structure.",
    "start": "13920",
    "end": "16950"
  },
  {
    "text": "And I'm going to introduce that\nby way of a simple example.",
    "start": "16950",
    "end": "20103"
  },
  {
    "text": "I've got that at the\nbottom of the slide here.",
    "start": "20103",
    "end": "22020"
  },
  {
    "text": "Our sentence is\n\"The Rock rules.\"",
    "start": "22020",
    "end": "24150"
  },
  {
    "text": "And I've paired each one of\nthose tokens with a token",
    "start": "24150",
    "end": "26939"
  },
  {
    "text": "representing its\nposition in the string.",
    "start": "26940",
    "end": "30350"
  },
  {
    "text": "The first thing that\nwe do in this model",
    "start": "30350",
    "end": "32119"
  },
  {
    "text": "is look up each one of those\ntokens in its own embedding",
    "start": "32119",
    "end": "36020"
  },
  {
    "text": "space.",
    "start": "36020",
    "end": "36660"
  },
  {
    "text": "So for word embeddings, we\nlook those up and get things",
    "start": "36660",
    "end": "39200"
  },
  {
    "text": "like x47, which is a\nvector corresponding",
    "start": "39200",
    "end": "42170"
  },
  {
    "text": "to the word \"the.\"",
    "start": "42170",
    "end": "43219"
  },
  {
    "text": "And that representation is\na static word representation",
    "start": "43220",
    "end": "46970"
  },
  {
    "text": "that's very similar\nconceptually to what",
    "start": "46970",
    "end": "49040"
  },
  {
    "text": "we had in the previous era with\nmodels like Word2Vec and GloVe.",
    "start": "49040",
    "end": "53510"
  },
  {
    "text": "We do something similar for\nthese positional tokens here",
    "start": "53510",
    "end": "56839"
  },
  {
    "text": "and get their vector\nrepresentations.",
    "start": "56840",
    "end": "59180"
  },
  {
    "text": "And then to combine\nthem, we simply",
    "start": "59180",
    "end": "60980"
  },
  {
    "text": "add them together dimension-wise\nto get the representations",
    "start": "60980",
    "end": "64849"
  },
  {
    "text": "that I have in green\nhere, which you",
    "start": "64849",
    "end": "66799"
  },
  {
    "text": "could think of as the first\ncontextual representations",
    "start": "66800",
    "end": "70310"
  },
  {
    "text": "that we have in this model.",
    "start": "70310",
    "end": "72700"
  },
  {
    "text": "On the right here, I've depicted\nthat calculation for the c",
    "start": "72700",
    "end": "76719"
  },
  {
    "text": "input part of this sequence.",
    "start": "76720",
    "end": "78772"
  },
  {
    "text": "And that's a pattern\nthat I'm going",
    "start": "78772",
    "end": "80230"
  },
  {
    "text": "to continue all the way up as\nwe build this transformer block.",
    "start": "80230",
    "end": "83560"
  },
  {
    "text": "Just showing the calculations\nfor the c dimension",
    "start": "83560",
    "end": "86619"
  },
  {
    "text": "because the calculations\nare entirely parallel",
    "start": "86620",
    "end": "89230"
  },
  {
    "text": "for a and for b.",
    "start": "89230",
    "end": "91240"
  },
  {
    "text": "So to get c input, we simply\nadd together x34 with p3.",
    "start": "91240",
    "end": "95439"
  },
  {
    "text": "And that gives us c input.",
    "start": "95440",
    "end": "98680"
  },
  {
    "text": "The next layer is\nthe attention layer.",
    "start": "98680",
    "end": "101283"
  },
  {
    "text": "This is the part\nof the model that",
    "start": "101283",
    "end": "102700"
  },
  {
    "text": "gives rise to that\nfamous paper title,",
    "start": "102700",
    "end": "105039"
  },
  {
    "text": "Attention is All You Need.",
    "start": "105040",
    "end": "106870"
  },
  {
    "text": "And the reason the paper has the\ntitle Attention is All You Need",
    "start": "106870",
    "end": "109870"
  },
  {
    "text": "is that the authors saw what was\nhappening in the previous era",
    "start": "109870",
    "end": "113860"
  },
  {
    "text": "with recurrent neural\nnetworks, where people",
    "start": "113860",
    "end": "115990"
  },
  {
    "text": "had recurrent\nmechanisms, and then they",
    "start": "115990",
    "end": "118060"
  },
  {
    "text": "added a bunch of\nattention mechanisms",
    "start": "118060",
    "end": "120430"
  },
  {
    "text": "on top of those recurrences\nto further connect everything",
    "start": "120430",
    "end": "124030"
  },
  {
    "text": "to everything else.",
    "start": "124030",
    "end": "124869"
  },
  {
    "text": "And what the paper\ntitle is saying",
    "start": "124870",
    "end": "126460"
  },
  {
    "text": "is you can get rid of\nthose recurrent connections",
    "start": "126460",
    "end": "129550"
  },
  {
    "text": "and rely entirely on attention.",
    "start": "129550",
    "end": "132260"
  },
  {
    "text": "Hence, attention\nis all you need.",
    "start": "132260",
    "end": "133970"
  },
  {
    "text": "That's an important\nhistorical note",
    "start": "133970",
    "end": "135470"
  },
  {
    "text": "because the transformer has\nmany other pieces as you'll see.",
    "start": "135470",
    "end": "139200"
  },
  {
    "text": "But they were saying in\nparticular, I believe,",
    "start": "139200",
    "end": "141379"
  },
  {
    "text": "that you could drop the\nrecurrent mechanisms.",
    "start": "141380",
    "end": "145390"
  },
  {
    "text": "The attention mechanism\nthat the transformer uses",
    "start": "145390",
    "end": "148210"
  },
  {
    "text": "is essentially the\nsame one that I",
    "start": "148210",
    "end": "150220"
  },
  {
    "text": "introduced in the\nprevious lecture coming",
    "start": "150220",
    "end": "152500"
  },
  {
    "text": "from the pre-transformer era.",
    "start": "152500",
    "end": "154490"
  },
  {
    "text": "It is a dot product-based\napproach to attention.",
    "start": "154490",
    "end": "158470"
  },
  {
    "text": "I've summarized that.",
    "start": "158470",
    "end": "159483"
  },
  {
    "text": "Here you can see\nin the numerator,",
    "start": "159483",
    "end": "160900"
  },
  {
    "text": "we have c input dot\nproduct with a input",
    "start": "160900",
    "end": "163599"
  },
  {
    "text": "and c input dot\nproduct with b input.",
    "start": "163600",
    "end": "166460"
  },
  {
    "text": "Let me show you what\nthose look like.",
    "start": "166460",
    "end": "168230"
  },
  {
    "text": "So here I've got,\ndepicted each dot product",
    "start": "168230",
    "end": "172300"
  },
  {
    "text": "as a dot and the\narrows going into it",
    "start": "172300",
    "end": "174940"
  },
  {
    "text": "correspond to the\ncomponents that",
    "start": "174940",
    "end": "176650"
  },
  {
    "text": "feed into that calculation.",
    "start": "176650",
    "end": "178250"
  },
  {
    "text": "So this dot here corresponds to\na input combined with C input.",
    "start": "178250",
    "end": "182740"
  },
  {
    "text": "And this one a\ninput with b input.",
    "start": "182740",
    "end": "185440"
  },
  {
    "text": "We do that same\nthing for the b step.",
    "start": "185440",
    "end": "187990"
  },
  {
    "text": "And then we do the same\nthing for the c step.",
    "start": "187990",
    "end": "190490"
  },
  {
    "text": "So the two dots that\nare depicted here",
    "start": "190490",
    "end": "192670"
  },
  {
    "text": "correspond to the\ntwo dot products",
    "start": "192670",
    "end": "194860"
  },
  {
    "text": "that are in this numerator.",
    "start": "194860",
    "end": "197070"
  },
  {
    "text": "One new thing that they did\nin the transformer paper",
    "start": "197070",
    "end": "199590"
  },
  {
    "text": "is normalize those dot products\nby the square root of dk.",
    "start": "199590",
    "end": "204330"
  },
  {
    "text": "dk is the dimensionality\nof the model.",
    "start": "204330",
    "end": "206820"
  },
  {
    "text": "It is the dimensionality\nof all the representations",
    "start": "206820",
    "end": "209550"
  },
  {
    "text": "that we have talked\nabout so far.",
    "start": "209550",
    "end": "211350"
  },
  {
    "text": "That's a really important\nelement of the transformer.",
    "start": "211350",
    "end": "214470"
  },
  {
    "text": "We're going to do a lot\nof additive combinations",
    "start": "214470",
    "end": "216840"
  },
  {
    "text": "in this model.",
    "start": "216840",
    "end": "217739"
  },
  {
    "text": "And that means that essentially,\nevery representation",
    "start": "217740",
    "end": "221100"
  },
  {
    "text": "has to have the same\ndimensionality, and that is dk.",
    "start": "221100",
    "end": "224160"
  },
  {
    "text": "There is one exception to\nthat, which I will return to.",
    "start": "224160",
    "end": "227200"
  },
  {
    "text": "But all the states that\nI depict on this slide",
    "start": "227200",
    "end": "229709"
  },
  {
    "text": "need to have dimensionality, dk.",
    "start": "229710",
    "end": "232090"
  },
  {
    "text": "And what the transformer\nauthors found",
    "start": "232090",
    "end": "234780"
  },
  {
    "text": "is that they got better\nscaling for the dot products",
    "start": "234780",
    "end": "237510"
  },
  {
    "text": "when they normalized by the\nsquare root of that model",
    "start": "237510",
    "end": "240569"
  },
  {
    "text": "dimensionality, as\na kind of heuristic.",
    "start": "240570",
    "end": "243810"
  },
  {
    "text": "So those normalized dot\nproducts give us a new vector,",
    "start": "243810",
    "end": "246800"
  },
  {
    "text": "alpha with a tilde on top.",
    "start": "246800",
    "end": "248690"
  },
  {
    "text": "We softmax normalize that.",
    "start": "248690",
    "end": "250830"
  },
  {
    "text": "And that gives us alpha,\nwhich you could think",
    "start": "250830",
    "end": "253010"
  },
  {
    "text": "of as kind of attention scores.",
    "start": "253010",
    "end": "255769"
  },
  {
    "text": "To get the actual attention\nrepresentation corresponding",
    "start": "255770",
    "end": "259220"
  },
  {
    "text": "to this block here, we take\neach component of this vector",
    "start": "259220",
    "end": "263360"
  },
  {
    "text": "alpha and multiply it by each\none of the representations",
    "start": "263360",
    "end": "267319"
  },
  {
    "text": "that we're attending to.",
    "start": "267320",
    "end": "268470"
  },
  {
    "text": "So alpha 1 by a input,\nalpha 2 by b input.",
    "start": "268470",
    "end": "272240"
  },
  {
    "text": "And then we sum those values\ntogether to get c attention.",
    "start": "272240",
    "end": "276770"
  },
  {
    "text": "And as a reminder, we have\nall these dense connections",
    "start": "276770",
    "end": "279979"
  },
  {
    "text": "for all of these\ndifferent states.",
    "start": "279980",
    "end": "281490"
  },
  {
    "text": "I'm just showing you the\none-- the calculations",
    "start": "281490",
    "end": "284210"
  },
  {
    "text": "for c attention.",
    "start": "284210",
    "end": "285350"
  },
  {
    "text": "And that's important because\nall those lines that are now",
    "start": "285350",
    "end": "288440"
  },
  {
    "text": "on the slide are really the only\nplace at which we knit together",
    "start": "288440",
    "end": "292760"
  },
  {
    "text": "all of these columns, which\nwill otherwise be operating",
    "start": "292760",
    "end": "295640"
  },
  {
    "text": "independently of each other.",
    "start": "295640",
    "end": "297300"
  },
  {
    "text": "So this really gives us\nall the dense connections",
    "start": "297300",
    "end": "299960"
  },
  {
    "text": "that we think are so powerful\nfor the transformer learning",
    "start": "299960",
    "end": "303419"
  },
  {
    "text": "what sequences are like.",
    "start": "303420",
    "end": "305800"
  },
  {
    "text": "Now, I do think that the\nrepresentations that I have",
    "start": "305800",
    "end": "308979"
  },
  {
    "text": "in orange are attention\nrepresentations,",
    "start": "308980",
    "end": "310750"
  },
  {
    "text": "but they're kind\nof raw materials,",
    "start": "310750",
    "end": "312980"
  },
  {
    "text": "because they're\nreally just recording",
    "start": "312980",
    "end": "314770"
  },
  {
    "text": "the kind of similarity between\nour target representation",
    "start": "314770",
    "end": "318310"
  },
  {
    "text": "and the representations\naround it.",
    "start": "318310",
    "end": "320560"
  },
  {
    "text": "To get an actual\nattention representation",
    "start": "320560",
    "end": "322960"
  },
  {
    "text": "in the transformer, what\nwe do is add together",
    "start": "322960",
    "end": "326289"
  },
  {
    "text": "these contextual representations\ndown here with these attention",
    "start": "326290",
    "end": "330400"
  },
  {
    "text": "values, and that gives us the\nrepresentations in yellow,",
    "start": "330400",
    "end": "333490"
  },
  {
    "text": "ca layer.",
    "start": "333490",
    "end": "335139"
  },
  {
    "text": "And those are kind of full\nfledged attention-based",
    "start": "335140",
    "end": "338380"
  },
  {
    "text": "representations.",
    "start": "338380",
    "end": "340060"
  },
  {
    "text": "I've depicted the\ncalculation over here,",
    "start": "340060",
    "end": "341980"
  },
  {
    "text": "and that includes\na nice reminder",
    "start": "341980",
    "end": "343360"
  },
  {
    "text": "that we actually apply\ndropout to the sum",
    "start": "343360",
    "end": "346599"
  },
  {
    "text": "of the orange and the green.",
    "start": "346600",
    "end": "348970"
  },
  {
    "text": "Dropout is a simple\nregularization technique",
    "start": "348970",
    "end": "351640"
  },
  {
    "text": "that will help the model to\nlearn diverse representations",
    "start": "351640",
    "end": "354760"
  },
  {
    "text": "as part of its training.",
    "start": "354760",
    "end": "357020"
  },
  {
    "text": "The next step is\nlayer normalization.",
    "start": "357020",
    "end": "359720"
  },
  {
    "text": "And this is simply going to\nhelp us with scaling the values.",
    "start": "359720",
    "end": "362318"
  },
  {
    "text": "We're going to adjust\nthem so that we",
    "start": "362318",
    "end": "363860"
  },
  {
    "text": "have 0 mean and a nice normal\ndistribution falling off",
    "start": "363860",
    "end": "367520"
  },
  {
    "text": "of that 0 mean.",
    "start": "367520",
    "end": "368690"
  },
  {
    "text": "And that's just a\nhappy place for machine",
    "start": "368690",
    "end": "371300"
  },
  {
    "text": "learning models in general.",
    "start": "371300",
    "end": "374169"
  },
  {
    "text": "The next step is really\ncrucially important.",
    "start": "374170",
    "end": "377330"
  },
  {
    "text": "These are the feed forward\ncomponents in the transformer.",
    "start": "377330",
    "end": "380830"
  },
  {
    "text": "I have depicted them as a\nsingle representation in blue,",
    "start": "380830",
    "end": "384129"
  },
  {
    "text": "but it's really important\nto see that this is actually",
    "start": "384130",
    "end": "386620"
  },
  {
    "text": "hiding two feed forward layers.",
    "start": "386620",
    "end": "389440"
  },
  {
    "text": "We take ca norm in\npurple here as the input.",
    "start": "389440",
    "end": "392890"
  },
  {
    "text": "And we feed that\nthrough a dense layer",
    "start": "392890",
    "end": "395140"
  },
  {
    "text": "with parameters w1 and b1.",
    "start": "395140",
    "end": "397750"
  },
  {
    "text": "And we apply a ReLu\nactivation to that.",
    "start": "397750",
    "end": "401581"
  },
  {
    "text": "That is fed into a second\ndense layer with parameters w2",
    "start": "401581",
    "end": "405699"
  },
  {
    "text": "and bias term b2.",
    "start": "405700",
    "end": "407230"
  },
  {
    "text": "And that gives us cff.",
    "start": "407230",
    "end": "409810"
  },
  {
    "text": "And this is important\nbecause many",
    "start": "409810",
    "end": "411970"
  },
  {
    "text": "of the parameters\nfor the transformer",
    "start": "411970",
    "end": "414160"
  },
  {
    "text": "are actually hidden away in\nthese feed forward layers.",
    "start": "414160",
    "end": "417130"
  },
  {
    "text": "And in fact, this\nis the one place",
    "start": "417130",
    "end": "419550"
  },
  {
    "text": "where we could depart from\nthis dimensionality decay",
    "start": "419550",
    "end": "423250"
  },
  {
    "text": "because a ca norm here has\ndimensionality decay by design.",
    "start": "423250",
    "end": "428740"
  },
  {
    "text": "But since we have two\nfeed forward layers,",
    "start": "428740",
    "end": "430870"
  },
  {
    "text": "we have the opportunity to\nexpand out to some larger",
    "start": "430870",
    "end": "434449"
  },
  {
    "text": "dimensionality if we want,\nas long as the output of that",
    "start": "434450",
    "end": "438380"
  },
  {
    "text": "goes back down to dk.",
    "start": "438380",
    "end": "440360"
  },
  {
    "text": "And as we'll see for\nsome of these very",
    "start": "440360",
    "end": "442069"
  },
  {
    "text": "large deployed\ntransformer architectures,",
    "start": "442070",
    "end": "444890"
  },
  {
    "text": "people have seized\nthis opportunity",
    "start": "444890",
    "end": "446960"
  },
  {
    "text": "to have really wide\ninternal layers in this feed",
    "start": "446960",
    "end": "450830"
  },
  {
    "text": "forward step.",
    "start": "450830",
    "end": "451639"
  },
  {
    "text": "And then of course, you\nhave to collapse back down.",
    "start": "451640",
    "end": "453990"
  },
  {
    "text": "And that might be\ngiving these models",
    "start": "453990",
    "end": "455699"
  },
  {
    "text": "a lot of their\nrepresentational power.",
    "start": "455700",
    "end": "459160"
  },
  {
    "text": "But we collapsed back\ndown to dk for cff here.",
    "start": "459160",
    "end": "462610"
  },
  {
    "text": "Then we have another addition\nof ca norm with caf--",
    "start": "462610",
    "end": "467530"
  },
  {
    "text": "cff to get cff layer\nhere in yellow.",
    "start": "467530",
    "end": "470440"
  },
  {
    "text": "And we have dropout\napplied to cff.",
    "start": "470440",
    "end": "472270"
  },
  {
    "text": "That's that regularization step.",
    "start": "472270",
    "end": "474550"
  },
  {
    "text": "And then finally, we have\na layer normalization step,",
    "start": "474550",
    "end": "477740"
  },
  {
    "text": "just as we had down\nhere, which will",
    "start": "477740",
    "end": "479319"
  },
  {
    "text": "help us with rescaling\nthe values that we've",
    "start": "479320",
    "end": "482140"
  },
  {
    "text": "produced thus far,\nand therefore,",
    "start": "482140",
    "end": "483850"
  },
  {
    "text": "help the model learn\nmore effectively.",
    "start": "483850",
    "end": "487390"
  },
  {
    "text": "That is the essence of the\ntransformer architecture.",
    "start": "487390",
    "end": "490780"
  },
  {
    "text": "There are a few more\ndetails to add on,",
    "start": "490780",
    "end": "493060"
  },
  {
    "text": "but I feel like this gives you\na good conceptual understanding.",
    "start": "493060",
    "end": "496389"
  },
  {
    "text": "We began with position\nsensitive versions",
    "start": "496390",
    "end": "499600"
  },
  {
    "text": "of static word embeddings.",
    "start": "499600",
    "end": "501820"
  },
  {
    "text": "We had these attention\nlayers down here.",
    "start": "501820",
    "end": "504800"
  },
  {
    "text": "And then we have the feed\nforward layers up here.",
    "start": "504800",
    "end": "507849"
  },
  {
    "text": "In between, we have\nsome regularization",
    "start": "507850",
    "end": "510520"
  },
  {
    "text": "and some normalization\nof the values.",
    "start": "510520",
    "end": "512500"
  },
  {
    "text": "But the essence of it\nis position sensitivity,",
    "start": "512500",
    "end": "515830"
  },
  {
    "text": "attention, feed forward.",
    "start": "515830",
    "end": "517809"
  },
  {
    "text": "And we are going to stack these\nblocks on top of each other.",
    "start": "517809",
    "end": "521090"
  },
  {
    "text": "And that's going to lead to lots\nmore representational power,",
    "start": "521090",
    "end": "523760"
  },
  {
    "text": "but all the blocks will\nfollow that same rhythm.",
    "start": "523760",
    "end": "527960"
  },
  {
    "text": "Since attention is so\nimportant for these models,",
    "start": "527960",
    "end": "530450"
  },
  {
    "text": "I thought I would linger a\nlittle bit over the attention",
    "start": "530450",
    "end": "533150"
  },
  {
    "text": "calculation.",
    "start": "533150",
    "end": "534530"
  },
  {
    "text": "What I've shown you so\nfar is the calculation",
    "start": "534530",
    "end": "537530"
  },
  {
    "text": "that I've given at the\ntop of the slide here,",
    "start": "537530",
    "end": "539640"
  },
  {
    "text": "which shows piece-wise how\nall of these dot products",
    "start": "539640",
    "end": "542060"
  },
  {
    "text": "come together and get\nrescaled and added in",
    "start": "542060",
    "end": "545060"
  },
  {
    "text": "to form c attention\nin this case.",
    "start": "545060",
    "end": "547730"
  },
  {
    "text": "In the Attention is\nAll You Need paper",
    "start": "547730",
    "end": "549889"
  },
  {
    "text": "and in a lot of the\nsubsequent literature,",
    "start": "549890",
    "end": "551870"
  },
  {
    "text": "that calculation is presented\nin this matrix format here.",
    "start": "551870",
    "end": "555900"
  },
  {
    "text": "And if you're like me,\nyou might not immediately",
    "start": "555900",
    "end": "558350"
  },
  {
    "text": "see how these two calculations\ncorrespond to each other.",
    "start": "558350",
    "end": "562380"
  },
  {
    "text": "And so what I've done is just\noffer you some simple code",
    "start": "562380",
    "end": "565670"
  },
  {
    "text": "that you could get hands on\nwith to convince yourself",
    "start": "565670",
    "end": "568639"
  },
  {
    "text": "that those two\ncalculations are the same.",
    "start": "568640",
    "end": "570980"
  },
  {
    "text": "And that might\nhelp you bootstrap",
    "start": "570980",
    "end": "572779"
  },
  {
    "text": "an understanding of\nwhat you typically",
    "start": "572780",
    "end": "575223"
  },
  {
    "text": "see in the literature.",
    "start": "575223",
    "end": "576140"
  },
  {
    "text": "And then you can go forth with\nthat more efficient matrix",
    "start": "576140",
    "end": "579560"
  },
  {
    "text": "version of the calculation.",
    "start": "579560",
    "end": "581060"
  },
  {
    "text": "Secure in the knowledge\nthat it corresponds",
    "start": "581060",
    "end": "584060"
  },
  {
    "text": "to the more piece-wise\nthing that I've",
    "start": "584060",
    "end": "585980"
  },
  {
    "text": "been depicting thus far.",
    "start": "585980",
    "end": "589010"
  },
  {
    "text": "The other major piece that I\nhave so far, not introduced",
    "start": "589010",
    "end": "592460"
  },
  {
    "text": "is multi-headed attention.",
    "start": "592460",
    "end": "594080"
  },
  {
    "text": "So far, I have showed\nyou effectively",
    "start": "594080",
    "end": "596660"
  },
  {
    "text": "single-headed attention.",
    "start": "596660",
    "end": "598670"
  },
  {
    "text": "So let's dive into what it\nmeans to be multi-headed.",
    "start": "598670",
    "end": "601459"
  },
  {
    "text": "I'm going to show you a worked\nexample with three heads.",
    "start": "601460",
    "end": "604970"
  },
  {
    "text": "The idea is actually\nvery simple,",
    "start": "604970",
    "end": "606829"
  },
  {
    "text": "but there are a lot\nof moving pieces.",
    "start": "606830",
    "end": "609000"
  },
  {
    "text": "So let's try to do this by\nway of a simple example.",
    "start": "609000",
    "end": "612150"
  },
  {
    "text": "I've got our usual sequence\nat the bottom here,",
    "start": "612150",
    "end": "614690"
  },
  {
    "text": "\"The Rock rules.\"",
    "start": "614690",
    "end": "615620"
  },
  {
    "text": "And I've got our usual three\ncontextual representations",
    "start": "615620",
    "end": "619820"
  },
  {
    "text": "given in green.",
    "start": "619820",
    "end": "621800"
  },
  {
    "text": "We are going to do three\nparallel calculations",
    "start": "621800",
    "end": "624980"
  },
  {
    "text": "corresponding to\nour three heads.",
    "start": "624980",
    "end": "626959"
  },
  {
    "text": "Here's the first head.",
    "start": "626960",
    "end": "628580"
  },
  {
    "text": "We do our same dot\nproducts as before.",
    "start": "628580",
    "end": "631710"
  },
  {
    "text": "And it is effectively\nthe same calculation",
    "start": "631710",
    "end": "634310"
  },
  {
    "text": "that leads to them\nwith the small twist",
    "start": "634310",
    "end": "636800"
  },
  {
    "text": "that we have introduced\na bunch of new parameters",
    "start": "636800",
    "end": "639680"
  },
  {
    "text": "into the calculation.",
    "start": "639680",
    "end": "640880"
  },
  {
    "text": "Those are wq1 for queries, wk1\nfor keys, and wv1 for values.",
    "start": "640880",
    "end": "649860"
  },
  {
    "text": "Those are depicted in\norange in this calculation.",
    "start": "649860",
    "end": "652209"
  },
  {
    "text": "And I put them in\norange to try to make",
    "start": "652210",
    "end": "654300"
  },
  {
    "text": "it easy to see that if we simply\nremove all of those learned",
    "start": "654300",
    "end": "657360"
  },
  {
    "text": "parameters, we get back to\nthe dot product calculation",
    "start": "657360",
    "end": "661230"
  },
  {
    "text": "that I was showing you before.",
    "start": "661230",
    "end": "662790"
  },
  {
    "text": "We've introduced\nthese new matrices",
    "start": "662790",
    "end": "665339"
  },
  {
    "text": "to provide more representational\npower inside this attention",
    "start": "665340",
    "end": "669540"
  },
  {
    "text": "block.",
    "start": "669540",
    "end": "670839"
  },
  {
    "text": "And the subscripts\n1 indicate that we",
    "start": "670840",
    "end": "673090"
  },
  {
    "text": "are dealing with parameters\nfor the first attention head.",
    "start": "673090",
    "end": "677290"
  },
  {
    "text": "We do the same thing for\nour second attention head,",
    "start": "677290",
    "end": "680290"
  },
  {
    "text": "all of those dot products,\nbut now augmented",
    "start": "680290",
    "end": "682660"
  },
  {
    "text": "with those new\nlearned parameters.",
    "start": "682660",
    "end": "684670"
  },
  {
    "text": "Same thing queries,\nkeys, and values,",
    "start": "684670",
    "end": "687639"
  },
  {
    "text": "but now two for the\nsecond attention head.",
    "start": "687640",
    "end": "691360"
  },
  {
    "text": "And we repeat exactly the same\nthing for the third attention",
    "start": "691360",
    "end": "694720"
  },
  {
    "text": "head, again, with\nparameters corresponding",
    "start": "694720",
    "end": "697480"
  },
  {
    "text": "to that third head.",
    "start": "697480",
    "end": "699649"
  },
  {
    "text": "And then to actually get back\nto the attention representations",
    "start": "699650",
    "end": "702830"
  },
  {
    "text": "that I was showing\nyou before, we",
    "start": "702830",
    "end": "704660"
  },
  {
    "text": "kind of reassemble the pieces.",
    "start": "704660",
    "end": "706670"
  },
  {
    "text": "So here is the attention\nrepresentation for a.",
    "start": "706670",
    "end": "710360"
  },
  {
    "text": "Here it is for b.",
    "start": "710360",
    "end": "712040"
  },
  {
    "text": "And here it is for c.",
    "start": "712040",
    "end": "713660"
  },
  {
    "text": "We've pieced together\nfrom all the things",
    "start": "713660",
    "end": "715730"
  },
  {
    "text": "that we did down here, these\nthree separate representations.",
    "start": "715730",
    "end": "719750"
  },
  {
    "text": "And those are what\nwas depicted in orange",
    "start": "719750",
    "end": "722330"
  },
  {
    "text": "on the previous slides.",
    "start": "722330",
    "end": "723780"
  },
  {
    "text": "But now you can see\nthat implicitly that",
    "start": "723780",
    "end": "726230"
  },
  {
    "text": "was probably a multi-headed\nattention process.",
    "start": "726230",
    "end": "731649"
  },
  {
    "text": "So now I think we can summarize.",
    "start": "731650",
    "end": "733530"
  },
  {
    "text": "Maybe the one big idea\nthat's worth repeating",
    "start": "733530",
    "end": "736530"
  },
  {
    "text": "is that we typically\nstack transformer blocks",
    "start": "736530",
    "end": "739380"
  },
  {
    "text": "on top of each other.",
    "start": "739380",
    "end": "740680"
  },
  {
    "text": "So this is the first block.",
    "start": "740680",
    "end": "742260"
  },
  {
    "text": "I've got c input coming\nin and c out here,",
    "start": "742260",
    "end": "744960"
  },
  {
    "text": "but c out could be the basis\nfor a second transformer block,",
    "start": "744960",
    "end": "749430"
  },
  {
    "text": "where those were the inputs.",
    "start": "749430",
    "end": "750810"
  },
  {
    "text": "And then, of course, we\ncould repeat that process.",
    "start": "750810",
    "end": "753090"
  },
  {
    "text": "And that is very typical\nto have 12, 24, maybe",
    "start": "753090",
    "end": "756960"
  },
  {
    "text": "even hundreds of\ntransformer blocks stacked",
    "start": "756960",
    "end": "759660"
  },
  {
    "text": "on top of each other.",
    "start": "759660",
    "end": "761310"
  },
  {
    "text": "And the other thing that's\nworth reminding yourself of",
    "start": "761310",
    "end": "763830"
  },
  {
    "text": "is that these\nrepresentations in orange",
    "start": "763830",
    "end": "766260"
  },
  {
    "text": "here are probably not\nsingle-headed attention",
    "start": "766260",
    "end": "768930"
  },
  {
    "text": "representations, but rather\nthe mother multi-headed ones,",
    "start": "768930",
    "end": "771899"
  },
  {
    "text": "where we piece together a\nbunch of component pieces",
    "start": "771900",
    "end": "775140"
  },
  {
    "text": "that themselves correspond to\na lot of learned parameters.",
    "start": "775140",
    "end": "779730"
  },
  {
    "text": "And that is, again, why this\nattention layer is so much",
    "start": "779730",
    "end": "783420"
  },
  {
    "text": "a part of the\ntransformer architecture,",
    "start": "783420",
    "end": "786100"
  },
  {
    "text": "in addition to the fact that\nthat's the one place where",
    "start": "786100",
    "end": "788850"
  },
  {
    "text": "all of these columns\nof representations",
    "start": "788850",
    "end": "791230"
  },
  {
    "text": "interact with each other.",
    "start": "791230",
    "end": "792889"
  },
  {
    "text": "So that probably\nfurther emphasizes",
    "start": "792890",
    "end": "795100"
  },
  {
    "text": "why the attention\nlayer is so important,",
    "start": "795100",
    "end": "797110"
  },
  {
    "text": "and why it's good to have lots\nof heads in there offering lots",
    "start": "797110",
    "end": "800110"
  },
  {
    "text": "of diversity for this\ncrucial interactional layer",
    "start": "800110",
    "end": "803709"
  },
  {
    "text": "across the different\nparts of the sequence.",
    "start": "803710",
    "end": "806335"
  },
  {
    "start": "806335",
    "end": "808950"
  },
  {
    "text": "So that is the essence of it.",
    "start": "808950",
    "end": "811450"
  },
  {
    "text": "And I hope that you\nare now in a position",
    "start": "811450",
    "end": "814110"
  },
  {
    "text": "to better understand\nthe famous transformer",
    "start": "814110",
    "end": "817649"
  },
  {
    "text": "diagram that appears\nin the Attention is",
    "start": "817650",
    "end": "820110"
  },
  {
    "text": "All You Need paper.",
    "start": "820110",
    "end": "821250"
  },
  {
    "text": "I will confess to you that\nI myself on first reading",
    "start": "821250",
    "end": "823710"
  },
  {
    "text": "did not understand this diagram.",
    "start": "823710",
    "end": "825540"
  },
  {
    "text": "But now I feel that\nI do understand it.",
    "start": "825540",
    "end": "828509"
  },
  {
    "text": "Reminder that in\nthat paper, they",
    "start": "828510",
    "end": "830760"
  },
  {
    "text": "are dealing mainly with\nsequence to sequence problems",
    "start": "830760",
    "end": "833470"
  },
  {
    "text": "so that they have an\nencoder and a decoder.",
    "start": "833470",
    "end": "836730"
  },
  {
    "text": "And so now we can see\nthat on the encoder side",
    "start": "836730",
    "end": "839550"
  },
  {
    "text": "here, what they've\ndepicted is repeated",
    "start": "839550",
    "end": "843180"
  },
  {
    "text": "for every step in\nthat encoder thing.",
    "start": "843180",
    "end": "845490"
  },
  {
    "text": "So every step in the sequence\nthat we're processing.",
    "start": "845490",
    "end": "848529"
  },
  {
    "text": "And once you see\nthat, you can see,",
    "start": "848530",
    "end": "850020"
  },
  {
    "text": "OK, they've used the same--",
    "start": "850020",
    "end": "851550"
  },
  {
    "text": "I use the same\ncolors that they did.",
    "start": "851550",
    "end": "853140"
  },
  {
    "text": "So red for the embeddings.",
    "start": "853140",
    "end": "855570"
  },
  {
    "text": "We have multi-headed attention,\nadditive and layer norm steps.",
    "start": "855570",
    "end": "860100"
  },
  {
    "text": "Then we have the feed forward\npart, more normalization,",
    "start": "860100",
    "end": "863819"
  },
  {
    "text": "and kind of adding together\nof different representations.",
    "start": "863820",
    "end": "866760"
  },
  {
    "text": "That's that same rhythm\nthat I pointed out before.",
    "start": "866760",
    "end": "869890"
  },
  {
    "text": "That's on the encoder side.",
    "start": "869890",
    "end": "871750"
  },
  {
    "text": "On the decoder side things\nget a little more complicated.",
    "start": "871750",
    "end": "874730"
  },
  {
    "text": "We're going to return to\nsome of these details.",
    "start": "874730",
    "end": "876889"
  },
  {
    "text": "But the important\nthing is that now we",
    "start": "876890",
    "end": "878500"
  },
  {
    "text": "need to do masked attention.",
    "start": "878500",
    "end": "880120"
  },
  {
    "text": "Because as we think\nabout decoding,",
    "start": "880120",
    "end": "882130"
  },
  {
    "text": "we need to be sure that\nour attention layer doesn't",
    "start": "882130",
    "end": "885250"
  },
  {
    "text": "look into the future.",
    "start": "885250",
    "end": "886510"
  },
  {
    "text": "We need to mask\nout future states",
    "start": "886510",
    "end": "888670"
  },
  {
    "text": "and look only into the past\nwhen we do those dot products.",
    "start": "888670",
    "end": "892310"
  },
  {
    "text": "So that's the masking down here.",
    "start": "892310",
    "end": "893770"
  },
  {
    "text": "But otherwise, the decoder\nhas the same exact structure",
    "start": "893770",
    "end": "897130"
  },
  {
    "text": "as the encoder.",
    "start": "897130",
    "end": "898600"
  },
  {
    "text": "They do have\nadditional parameters",
    "start": "898600",
    "end": "900250"
  },
  {
    "text": "on top here corresponding\nto output probabilities.",
    "start": "900250",
    "end": "903190"
  },
  {
    "text": "If we're doing something like\nmachine translation or language",
    "start": "903190",
    "end": "906160"
  },
  {
    "text": "modeling, we'll have those\nheads on every single state",
    "start": "906160",
    "end": "909490"
  },
  {
    "text": "in the decoder.",
    "start": "909490",
    "end": "910570"
  },
  {
    "text": "But if we're doing something\nlike classification,",
    "start": "910570",
    "end": "912940"
  },
  {
    "text": "we might have those\ntask specific parameters",
    "start": "912940",
    "end": "915790"
  },
  {
    "text": "only on one of the output\nstates, maybe the final one.",
    "start": "915790",
    "end": "920300"
  },
  {
    "text": "But other than that, you\ncan see the same pieces",
    "start": "920300",
    "end": "922760"
  },
  {
    "text": "that I've discussed\nbefore just presented",
    "start": "922760",
    "end": "925070"
  },
  {
    "text": "in this encoder-decoder phase.",
    "start": "925070",
    "end": "927380"
  },
  {
    "text": "So I hope that helps a little\nbit with the famous diagram.",
    "start": "927380",
    "end": "931280"
  },
  {
    "text": "The final thing I wanted\nto say under this heading",
    "start": "931280",
    "end": "933830"
  },
  {
    "text": "is just that you can get\nan even deeper feel for how",
    "start": "933830",
    "end": "936710"
  },
  {
    "text": "these models work by\ndownloading them and using",
    "start": "936710",
    "end": "939680"
  },
  {
    "text": "Hugging Face code to kind\nof inspect their structure.",
    "start": "939680",
    "end": "942890"
  },
  {
    "text": "I've done that on this\nslide with BERT Base",
    "start": "942890",
    "end": "945950"
  },
  {
    "text": "and this is really illuminating.",
    "start": "945950",
    "end": "947600"
  },
  {
    "text": "You see a lot of the pieces\nthat we've already discussed.",
    "start": "947600",
    "end": "950060"
  },
  {
    "text": "This is the BERT model.",
    "start": "950060",
    "end": "951680"
  },
  {
    "text": "It's got an embedding layer,\nwhich has word embeddings.",
    "start": "951680",
    "end": "954890"
  },
  {
    "text": "And you can see that there\nare about 30,000 items",
    "start": "954890",
    "end": "957410"
  },
  {
    "text": "in the embedding space,\neach one dimensionality 768.",
    "start": "957410",
    "end": "961100"
  },
  {
    "text": "That's dk that I\nemphasize so much.",
    "start": "961100",
    "end": "964220"
  },
  {
    "text": "The positional embeddings, we\nhave 512 positional embeddings.",
    "start": "964220",
    "end": "968069"
  },
  {
    "text": "So that will be our\nmaximum sequence length.",
    "start": "968070",
    "end": "970490"
  },
  {
    "text": "And those, by definition, have\nto have dimensionality 768",
    "start": "970490",
    "end": "974209"
  },
  {
    "text": "as well.",
    "start": "974210",
    "end": "975080"
  },
  {
    "text": "We'll return to these\ntoken type embeddings",
    "start": "975080",
    "end": "977390"
  },
  {
    "text": "when we talk about\nBERT in particular,",
    "start": "977390",
    "end": "980000"
  },
  {
    "text": "but that's kind of like\na poositional embedding.",
    "start": "980000",
    "end": "982790"
  },
  {
    "text": "Then we have layer\nnorm and dropout.",
    "start": "982790",
    "end": "984620"
  },
  {
    "text": "So that's kind of\nregularization of these values.",
    "start": "984620",
    "end": "987450"
  },
  {
    "text": "And then we have the layers.",
    "start": "987450",
    "end": "989180"
  },
  {
    "text": "And what you can see on this\nslide is just the first layer.",
    "start": "989180",
    "end": "991860"
  },
  {
    "text": "It's the same structure repeated\nfor all subsequent layers.",
    "start": "991860",
    "end": "995810"
  },
  {
    "text": "Down here, we have\nthe attention layer.",
    "start": "995810",
    "end": "997610"
  },
  {
    "text": "You see 768 all over the\nplace because that's dk",
    "start": "997610",
    "end": "1001420"
  },
  {
    "text": "and the model pretty\nmuch defines for us",
    "start": "1001420",
    "end": "1003459"
  },
  {
    "text": "that we need to have that same\ndimensionality everywhere.",
    "start": "1003460",
    "end": "1006340"
  },
  {
    "text": "The one exception\nis that when we",
    "start": "1006340",
    "end": "1008020"
  },
  {
    "text": "get down into the\nfeed forward layers,",
    "start": "1008020",
    "end": "1010390"
  },
  {
    "text": "we go from 768 out to 3072.",
    "start": "1010390",
    "end": "1014230"
  },
  {
    "text": "That's that intermediate part.",
    "start": "1014230",
    "end": "1016180"
  },
  {
    "text": "But then we have to go from\n3072 back to 768 for the output",
    "start": "1016180",
    "end": "1020710"
  },
  {
    "text": "so that we can stack\nthese components on top",
    "start": "1020710",
    "end": "1023290"
  },
  {
    "text": "of each other.",
    "start": "1023290",
    "end": "1023899"
  },
  {
    "text": "But you can see that\nopportunity there",
    "start": "1023900",
    "end": "1025869"
  },
  {
    "text": "to add a lot more parameters,\nand therefore a lot more",
    "start": "1025869",
    "end": "1029500"
  },
  {
    "text": "representational power.",
    "start": "1029500",
    "end": "1031630"
  },
  {
    "text": "And as I said, this would\ncontinue for all the layers.",
    "start": "1031630",
    "end": "1034660"
  },
  {
    "text": "And that's pretty much a\nsummary of the architecture.",
    "start": "1034660",
    "end": "1037569"
  },
  {
    "text": "And you can do this for\nlots of different models",
    "start": "1037569",
    "end": "1039880"
  },
  {
    "text": "with Hugging Face.",
    "start": "1039880",
    "end": "1040790"
  },
  {
    "text": "You can check out GPT\nand BERT and RoBERTa",
    "start": "1040790",
    "end": "1043369"
  },
  {
    "text": "and all the other\nmodels we talk about.",
    "start": "1043369",
    "end": "1045140"
  },
  {
    "text": "They'll differ subtly\nin their kind of graphs.",
    "start": "1045140",
    "end": "1048089"
  },
  {
    "text": "But I expect that you'll see a\nlot of the core pieces repeated",
    "start": "1048089",
    "end": "1051830"
  },
  {
    "text": "in various flavors as\nyou look at those models.",
    "start": "1051830",
    "end": "1055809"
  },
  {
    "start": "1055810",
    "end": "1060000"
  }
]