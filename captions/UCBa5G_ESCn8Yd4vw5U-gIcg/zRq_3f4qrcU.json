[
  {
    "text": "thank you all for having me it is my pleasure i think this was such a perfect intro so I'm just going to dive right",
    "start": "9200",
    "end": "15759"
  },
  {
    "text": "into it so what I personally think is the most exciting thing about being a",
    "start": "15759",
    "end": "21760"
  },
  {
    "text": "roboticist today is that robots kind of are doing stuff like when I started my",
    "start": "21760",
    "end": "27760"
  },
  {
    "text": "PhD almost 10 years ago I never expected that today I could go take a fully self-driving car ride in downtown San",
    "start": "27760",
    "end": "34600"
  },
  {
    "text": "Francisco or when I started my PhD and my adviser told me get the robot to pick up a cup and put it somewhere else it",
    "start": "34600",
    "end": "41840"
  },
  {
    "text": "was so painful but now as you can see from some of these demos from TRRI with",
    "start": "41840",
    "end": "46879"
  },
  {
    "text": "just a few demonstrations and a little bit of time our robots can actually finally pick and play",
    "start": "46879",
    "end": "52199"
  },
  {
    "text": "stuff at the same time it's precisely this widescale deployment and increased",
    "start": "52199",
    "end": "57360"
  },
  {
    "text": "capability that has gotten people all over the world concerned about safety and reliability uh this includes",
    "start": "57360",
    "end": "64878"
  },
  {
    "text": "industrial stakeholders for example DeepMind has an entire team on AI safety",
    "start": "64879",
    "end": "70000"
  },
  {
    "text": "and regulators like the prior administration had an entire executive order on AI safety",
    "start": "70000",
    "end": "76880"
  },
  {
    "text": "but the thing that comes to my mind is that when we deploy robots truly at",
    "start": "76880",
    "end": "83040"
  },
  {
    "text": "scale truly in the open world what safety means is actually a really really nuanced",
    "start": "83040",
    "end": "89159"
  },
  {
    "text": "concept i like to start with an example that might seem simple why does it seem",
    "start": "89159",
    "end": "94880"
  },
  {
    "text": "simple because I think we can all agree that a a straightforward specification of safety for something like driving is",
    "start": "94880",
    "end": "101520"
  },
  {
    "text": "don't collide and if we look at this example from Tesla driving on the Bay Bridge here in",
    "start": "101520",
    "end": "107680"
  },
  {
    "text": "San Francisco for some reason this Tesla Tesla self-driving car does a hard break",
    "start": "107680",
    "end": "113360"
  },
  {
    "text": "in the middle of this road causing a multi-car pileup now why do I like this example i",
    "start": "113360",
    "end": "119759"
  },
  {
    "text": "like this example because oftent times when I tell people that the safety spec is collision avoidance they tell me \"We'll just tell the robot to",
    "start": "119759",
    "end": "126040"
  },
  {
    "text": "stop.\" Okay but here it was precisely stopping that caused the safety",
    "start": "126040",
    "end": "131239"
  },
  {
    "text": "violation okay okay and that's because the first challenge we face in safety is even if the safety specification seems",
    "start": "131239",
    "end": "137840"
  },
  {
    "text": "simple decision making is very hard super super context",
    "start": "137840",
    "end": "143879"
  },
  {
    "text": "dependent however I think safety is much more than just these kinds of interactions consider the right hand",
    "start": "143879",
    "end": "149920"
  },
  {
    "text": "side this fantastic work from Professor Ani Majunar's group at Princeton in collaboration with DeepMind here a robot",
    "start": "149920",
    "end": "155840"
  },
  {
    "text": "is using a large language model as a task planner this task planner generates some options for the robot and in this",
    "start": "155840",
    "end": "161360"
  },
  {
    "text": "paper they argue rightfully so that a safe robot should be one that asks for help when it's uncertain there are",
    "start": "161360",
    "end": "166800"
  },
  {
    "text": "multiple options that are likely here what I also like about this example is it reveals another kind of safety",
    "start": "166800",
    "end": "172800"
  },
  {
    "text": "concern why are you even suggesting putting a metal bowl in the microwave and last but not least",
    "start": "172800",
    "end": "179120"
  },
  {
    "text": "consider this Roomba it literally breaks free into the open world for some reason",
    "start": "179120",
    "end": "184480"
  },
  {
    "text": "I don't know why this room does not understand the consequences of going a little too close to this edge and dip it",
    "start": "184480",
    "end": "190920"
  },
  {
    "text": "in okay so the fundamental question I'm really really interested in is what does it mean or how can we go about",
    "start": "190920",
    "end": "197680"
  },
  {
    "text": "generalizing robot safety and a lot of my technical training the way I think about these problems is is sort of",
    "start": "197680",
    "end": "204319"
  },
  {
    "text": "grounded in my prior work thinking about robust control theory now this type of safety has steadily been making progress",
    "start": "204319",
    "end": "210400"
  },
  {
    "text": "over the past several decades this is a very non-exhaustive list of increasingly more complex robots that are using these",
    "start": "210400",
    "end": "217280"
  },
  {
    "text": "kinds of frameworks for safe control now here safety typically means constraint satisfaction what I really like about",
    "start": "217280",
    "end": "223680"
  },
  {
    "text": "this mathematical framework in theory is that it really beautifully captures uh",
    "start": "223680",
    "end": "228799"
  },
  {
    "text": "something called feedback loops essentially how the robot's actions are going to influence long-term outcomes",
    "start": "228799",
    "end": "234879"
  },
  {
    "text": "in addition I particularly like how in this framework the detection of an unsafe event is coupled with a",
    "start": "234879",
    "end": "241599"
  },
  {
    "text": "mitigation strategy what the robot should actually do to resolve a safety",
    "start": "241599",
    "end": "247239"
  },
  {
    "text": "concern however for the longest time this very general mathematical framework has been limited to handdesigned state",
    "start": "247239",
    "end": "253599"
  },
  {
    "text": "representations handdesign dynamics and failure states any student who has worked with these tools knows the pain",
    "start": "253599",
    "end": "260000"
  },
  {
    "text": "knows the pain of writing down that state space writing down for example a collision checker it's hard it's a lot",
    "start": "260000",
    "end": "266320"
  },
  {
    "text": "of hard work that only experts can do at the same time simultaneously this",
    "start": "266320",
    "end": "271520"
  },
  {
    "text": "other domain has been also making a lot of progress very broadly machine learning but in this case I'm going to I'm going to focus in on generative AI",
    "start": "271520",
    "end": "277919"
  },
  {
    "text": "my first introduction to these tools was through trajectory forecasting why in trajectory forecasting we do not know",
    "start": "277919",
    "end": "284320"
  },
  {
    "text": "how to write down the state of what is going on in your brain i do not know what is going on in your head so what I",
    "start": "284320",
    "end": "289919"
  },
  {
    "text": "can do is I can use a lot of data to maybe infer this just from the highdimensional observations in other",
    "start": "289919",
    "end": "295040"
  },
  {
    "text": "words what these models really excel at is learning latent representations of the world that I can't write down in",
    "start": "295040",
    "end": "300320"
  },
  {
    "text": "first principles necessarily they also are really easily guided by non-expert feedback i'm sure",
    "start": "300320",
    "end": "306400"
  },
  {
    "text": "all of you have given a thumbs up thumbs down to chat GPT and gotten it to do a better thing you did not have to",
    "start": "306400",
    "end": "312639"
  },
  {
    "text": "understand how GPT works and it still did a thing that you wanted a little bit better the problem though with these",
    "start": "312639",
    "end": "319919"
  },
  {
    "text": "kinds of paradigms is oftentimes when we think about safety we often focus on something like anomaly detection or",
    "start": "319919",
    "end": "325360"
  },
  {
    "text": "uncertainty quantification which are very important for thinking about the component but they don't tell the robot",
    "start": "325360",
    "end": "331600"
  },
  {
    "text": "what to do so uh here's a really beautiful paper from yours uh Marco",
    "start": "331600",
    "end": "336960"
  },
  {
    "text": "Pomon's group and uh this this example I really love so it turns out these large language models possess the capability",
    "start": "336960",
    "end": "342800"
  },
  {
    "text": "to detect that this is a really weird scene it's a really weird scenario to see the cyber truck with this uh light",
    "start": "342800",
    "end": "348639"
  },
  {
    "text": "post on the back but the question remains even though I know it's weird what should the robot",
    "start": "348639",
    "end": "354639"
  },
  {
    "text": "do and so my hypothesis that my group has been exploring is how can we",
    "start": "354840",
    "end": "359919"
  },
  {
    "text": "systematically unite the best of both of these worlds how can we systematically take the things we know and love from",
    "start": "359919",
    "end": "365680"
  },
  {
    "text": "safe control and take these ideas or principles we see starting to work in generative modeling and start to",
    "start": "365680",
    "end": "372080"
  },
  {
    "text": "generalize robot safety to the truly open world now over the past two years my",
    "start": "372080",
    "end": "378240"
  },
  {
    "text": "group has done a bunch of work along these lines we've done things just focused on uh these generative models",
    "start": "378240",
    "end": "384479"
  },
  {
    "text": "used in robotics we've done things like alignment and quantification we've also looked at their intersection as well but",
    "start": "384479",
    "end": "390880"
  },
  {
    "text": "today my focus is going to be the following i want to start by laying down some foundations let's all get on the",
    "start": "390880",
    "end": "396720"
  },
  {
    "text": "same page with what I mean about safety more concretely and then I'm going to talk about two very very new works that",
    "start": "396720",
    "end": "402319"
  },
  {
    "text": "just came out because I know everyone only wants the hottest stuff off the press all right let's get let's let's uh",
    "start": "402319",
    "end": "408160"
  },
  {
    "text": "let's just get into it here so let's start by learning a little bit more about what I mean by safety and to",
    "start": "408160",
    "end": "414639"
  },
  {
    "text": "understand this let's take a look at a running example this is from some of my older work in fact in collaboration with professor Sil Bunel here and this was",
    "start": "414639",
    "end": "421759"
  },
  {
    "text": "some work during my PhD where this robot is navigating through its environment and it's using a policy that is really good at exploring it can kind of figure",
    "start": "421759",
    "end": "428560"
  },
  {
    "text": "out where to go to make its way towards some goal however this base policy I'm",
    "start": "428560",
    "end": "433680"
  },
  {
    "text": "going to call it is sometimes overly optimistic and decides to cut corners in ways that lead to unsafe",
    "start": "433680",
    "end": "440599"
  },
  {
    "text": "outcomes so I want to use this powerful base policy it is generally capable but",
    "start": "440599",
    "end": "446639"
  },
  {
    "text": "I want to minimally modify it to stay safe and the idea from control systems",
    "start": "446639",
    "end": "451759"
  },
  {
    "text": "we're going to use here is something called a safety filter which is essentially a mechanism by which we can monitor any policies output so that",
    "start": "451759",
    "end": "458800"
  },
  {
    "text": "present actions do not result in future failure the big trick is how do you get",
    "start": "458800",
    "end": "464520"
  },
  {
    "text": "it so to construct a safety filter you need four ingredients first you need a state space",
    "start": "464520",
    "end": "472400"
  },
  {
    "text": "in this low dimensional example it's easy positions velocity orientation you need a dynamical systems",
    "start": "472400",
    "end": "479199"
  },
  {
    "text": "model this can be a discrete time or a continuous time model it could be a high fidelity simulator the key is that based",
    "start": "479199",
    "end": "485440"
  },
  {
    "text": "on what the robot does the state evolves and oftentimes in robust control you'll have an additional input here D",
    "start": "485440",
    "end": "491759"
  },
  {
    "text": "which sometimes is called an opponent or a disturbance which lets us inject more robustness into the overall system for",
    "start": "491759",
    "end": "497759"
  },
  {
    "text": "example robustness to another agent or external forces or hard to model aspects of the",
    "start": "497759",
    "end": "503879"
  },
  {
    "text": "environment okay last but certainly not least we need to represent failure and here in this example if I know my",
    "start": "503879",
    "end": "509840"
  },
  {
    "text": "environment perfectly my failure set is easy it's going to be this subset of the state space subset where the obstacles",
    "start": "509840",
    "end": "517640"
  },
  {
    "text": "are okay so let's actually cook up a safety filter together i'm showing you the safety spec here one more time the",
    "start": "517640",
    "end": "524320"
  },
  {
    "text": "subset of the state space where the obstacles are and the particular synthesis technique we're going to be using for computing a safety filter is",
    "start": "524320",
    "end": "531200"
  },
  {
    "text": "something called Hamilton Jacobi reachability okay the way we're going to do this is we're going to encode the",
    "start": "531200",
    "end": "537040"
  },
  {
    "text": "failure set in this functional form via the sign distance function negative means you're inside of the obstacle positive means you're outside and we now",
    "start": "537040",
    "end": "544240"
  },
  {
    "text": "want to know for an initial state of the system like I'm here does there exist an action the robot can take so it steers",
    "start": "544240",
    "end": "550240"
  },
  {
    "text": "clear of the failure set and this has to be in theory true even despite this worstc case disturbance that I told you",
    "start": "550240",
    "end": "556399"
  },
  {
    "text": "about earlier now formally this is a safety critical game whose value is defined up",
    "start": "556399",
    "end": "562000"
  },
  {
    "text": "here for those of you who haven't seen this don't worry too much the most important takeaway here is intuitively",
    "start": "562000",
    "end": "567600"
  },
  {
    "text": "this value function remembers the closest our robot ever gets to failure",
    "start": "567600",
    "end": "572640"
  },
  {
    "text": "under the best strategy that it can possibly take now thanks to decades of progress we can",
    "start": "572640",
    "end": "580160"
  },
  {
    "text": "start to solve this safety game with a myriad of techniques in a lowdimensional setting you can use highfidelity PTE",
    "start": "580160",
    "end": "586560"
  },
  {
    "text": "solvers over a grid perhap perhaps uh for highdimensional systems you can maybe use something like adversarial",
    "start": "586560",
    "end": "592320"
  },
  {
    "text": "reinforcement learning or self-supervised learning and here is this value function being computed in",
    "start": "592320",
    "end": "597839"
  },
  {
    "text": "front of us just so we can see how one solver might act in the wild here so one",
    "start": "597839",
    "end": "603279"
  },
  {
    "text": "more time that value function is being computed backward in time essentially like a Bellman style",
    "start": "603279",
    "end": "609399"
  },
  {
    "text": "update okay let's take a look what we actually end up getting what is this safety filter we can extract two",
    "start": "609399",
    "end": "615279"
  },
  {
    "text": "quantities which define our safety filter first is a safety policy it tells us what to",
    "start": "615279",
    "end": "620440"
  },
  {
    "text": "do and second is a safety monitor or a safe set which tells us when do we have",
    "start": "620440",
    "end": "626399"
  },
  {
    "text": "to start doing something to preserve safety what is the last moment I could ever take a safe action and prevent",
    "start": "626399",
    "end": "632399"
  },
  {
    "text": "future failure and that's precisely the interior of this red boundary I'm showing you here here for different",
    "start": "632399",
    "end": "638000"
  },
  {
    "text": "initial states of our little robot vehicle I'm showing you what the optimal safety control might look like you're",
    "start": "638000",
    "end": "643040"
  },
  {
    "text": "steering you know really hard away from the obstacles it's very intuitive in this lowdimensional example",
    "start": "643040",
    "end": "649640"
  },
  {
    "text": "here to actually do filtering a really simple scheme you might employ looks something like this pick your favorite",
    "start": "649640",
    "end": "655600"
  },
  {
    "text": "policy as long as your monitor returns uh greater than or equal to zero or greater than zero here it says you're",
    "start": "655600",
    "end": "662000"
  },
  {
    "text": "safe no matter what you do if you get close to the boundary which is the last moment at which you have an opportunity",
    "start": "662000",
    "end": "668160"
  },
  {
    "text": "to make a safe decision before you're doomed you have to switch to the safety policy there are much smarter fancier",
    "start": "668160",
    "end": "675440"
  },
  {
    "text": "ways of doing safety filtering i'd highly encourage you to look at these two review papers if you want to look",
    "start": "675440",
    "end": "680720"
  },
  {
    "text": "for more details but this is the simplest scheme you could possibly instantiate all right if we put this",
    "start": "680720",
    "end": "687360"
  },
  {
    "text": "kind of methodology back in our running example you'll see that safety filter right here in the upper right hand side",
    "start": "687360",
    "end": "693120"
  },
  {
    "text": "that red boundary in this case is actually growing and changing as a function of the data we're observing in this case the data being u a new",
    "start": "693120",
    "end": "700079"
  },
  {
    "text": "occupancy map of where the obstacles are and you can see the safety polic policies preventing our kind of",
    "start": "700079",
    "end": "706360"
  },
  {
    "text": "explorationdriven policy from taking these overly optimistic actions that might lead it to",
    "start": "706360",
    "end": "712600"
  },
  {
    "text": "collide okay this is the setup this is the framework in which I like to think",
    "start": "712600",
    "end": "717760"
  },
  {
    "text": "about safety however if you really really look at a lot of safety papers that use these kinds of tools you're",
    "start": "717760",
    "end": "724560"
  },
  {
    "text": "going to see that under the hood the majority of safety representations are collision avoidance",
    "start": "724560",
    "end": "731600"
  },
  {
    "text": "so here I'm picking on a lot of my own work i'm also showing you a very non-exhaustive list of other folks in",
    "start": "731600",
    "end": "737600"
  },
  {
    "text": "the community and when you really stare at it it's going to be things like distance to other agents distance to",
    "start": "737600",
    "end": "742639"
  },
  {
    "text": "obstacles distance between my head and the ground it's collision this is a very important",
    "start": "742639",
    "end": "748639"
  },
  {
    "text": "representation of safety no doubt but these are the kinds of safety concerns I'm really interested in right now okay",
    "start": "748639",
    "end": "755839"
  },
  {
    "text": "i'm really interested in things like tearing breaking spilling okay okay so",
    "start": "755839",
    "end": "761040"
  },
  {
    "text": "so what's holding us back why can't we solve safety problems for these kinds of",
    "start": "761040",
    "end": "766200"
  },
  {
    "text": "representations well for this we have to go back to our four ingredients for safety and start doing a little",
    "start": "766200",
    "end": "771720"
  },
  {
    "text": "introspection so I hope all of you are awake we're going to do a little quiz now all right so here is an example this",
    "start": "771720",
    "end": "778560"
  },
  {
    "text": "is a robot manipulator in my lab there's a bag of Skittles and it is opened if my",
    "start": "778560",
    "end": "783680"
  },
  {
    "text": "robot happens to do something like this grab it from the bottom and pull up we see this spill failure happen",
    "start": "783680",
    "end": "790320"
  },
  {
    "text": "time for the quiz if you had to model a state space what would you put in your state",
    "start": "790320",
    "end": "796320"
  },
  {
    "text": "space feel free to call it out whether the bag is open okay nice",
    "start": "796320",
    "end": "802079"
  },
  {
    "text": "anything else which end the bag is open yep other things",
    "start": "802079",
    "end": "809560"
  },
  {
    "text": "where's the bag where's the gripper mhm yep pose of the bag pose the gripper yep what about this what about the Skittles",
    "start": "813519",
    "end": "821040"
  },
  {
    "text": "can we use insider information can we use insider information what does that mean",
    "start": "821040",
    "end": "828079"
  },
  {
    "text": "okay don't spoil it don't spoil it for me no no no but we're handd designing our state right we're trying to think about that first principles way I was",
    "start": "828079",
    "end": "834399"
  },
  {
    "text": "telling you about one time so when I gave this talk in the RARI one of the senior faculty was like \"Wait what about",
    "start": "834399",
    "end": "839920"
  },
  {
    "text": "the color of the Skittles do we have to include the color in the state?\" Maybe if my constraint is never drop those red",
    "start": "839920",
    "end": "845760"
  },
  {
    "text": "Skittles because they're my favorite you better have color in your state space what about",
    "start": "845760",
    "end": "851720"
  },
  {
    "text": "dynamics how would you model the dynamics we just we just talked about all these different components of the state I could put in here the you know",
    "start": "851720",
    "end": "858160"
  },
  {
    "text": "the bag orientation by the way no one said actually the the deformable bag structure sometimes people say like oh I",
    "start": "858160",
    "end": "864320"
  },
  {
    "text": "want to have a mesh to represent the bag itself because it's deformable okay so the weight the weight",
    "start": "864320",
    "end": "871600"
  },
  {
    "text": "yeah that could be something I need as well but what about the dynamics yeah you need to somehow model the like",
    "start": "871600",
    "end": "877760"
  },
  {
    "text": "way in which Skittles spill yeah maybe you might try to use some kind of like fluid dynamics model maybe here you",
    "start": "877760",
    "end": "884880"
  },
  {
    "text": "could totally try this out it's really hard to do though and last but not least",
    "start": "884880",
    "end": "890079"
  },
  {
    "text": "what's our failure set what's our failure set",
    "start": "890079",
    "end": "896079"
  },
  {
    "text": "yeah how would you mo like how would you mathematically encode a spill here what would you measure well the ones on the",
    "start": "896079",
    "end": "902720"
  },
  {
    "text": "table are okay it's bad well in this case good i like that i like that this",
    "start": "902720",
    "end": "909360"
  },
  {
    "text": "is a totally legit safety spec that is personalized to you some people would not eat it off the",
    "start": "909360",
    "end": "915519"
  },
  {
    "text": "table our tables in our lab are kind of dirty so maybe kind of similar to the ground in that case um but in this case",
    "start": "915519",
    "end": "921519"
  },
  {
    "text": "the safety spec is don't spill out of the bag but for this right when the Skittles are inside the bag you don't",
    "start": "921519",
    "end": "927839"
  },
  {
    "text": "even know where they are in the bag so how can you measure the distance between a Skittle and the table that's tough okay so so I would",
    "start": "927839",
    "end": "936320"
  },
  {
    "text": "love to solve this problem but the question is how do I go beyond collision avoidance as soon as I start thinking about these problems it just starts to",
    "start": "936320",
    "end": "942480"
  },
  {
    "text": "feel intractable and what we all just discovered together is essentially we need to somehow go",
    "start": "942480",
    "end": "949440"
  },
  {
    "text": "beyond these handdesigned state and dynamics representations because they are simply really really hard to",
    "start": "949440",
    "end": "956360"
  },
  {
    "text": "scale and this is where we're going to take our first really big leap okay we're going to go from all of these",
    "start": "956360",
    "end": "961759"
  },
  {
    "text": "foundations all the way over here and we're going to talk about how to generalize our safety principles okay",
    "start": "961759",
    "end": "969360"
  },
  {
    "text": "so here it might seem like all hope is lost it might seem like man we don't know how to write down state or failure",
    "start": "969360",
    "end": "974959"
  },
  {
    "text": "but there is one thing going for us which is that failure is observable from highdimensional observations if I show",
    "start": "974959",
    "end": "981360"
  },
  {
    "text": "you this image you say \"Yep there's definitely those Skittles on the table.\" The problem though is that to do",
    "start": "981360",
    "end": "988480"
  },
  {
    "text": "control to do planning I need to predict if the robot's actions are going to lead to this spill once I'm seeing",
    "start": "988480",
    "end": "995120"
  },
  {
    "text": "the spill it's too late it's too late and so this is where we had an idea",
    "start": "995120",
    "end": "1001120"
  },
  {
    "text": "where we said okay what if we start taking all those control theoretic tools we start doing safe control directly in",
    "start": "1001120",
    "end": "1007920"
  },
  {
    "text": "the latent space learned by these very modern generative world models and I'm going to go into these details in a",
    "start": "1007920",
    "end": "1014079"
  },
  {
    "text": "second here okay at a very high level it's going to mathematically look not too difficult but that's how it always",
    "start": "1014079",
    "end": "1020560"
  },
  {
    "text": "looks when you look at math um before we had this state space X we had our dynamics model and our failure set was a",
    "start": "1020560",
    "end": "1027038"
  },
  {
    "text": "subset now we just made everything Z latent space Z now if we dig a little",
    "start": "1027039",
    "end": "1032480"
  },
  {
    "text": "bit deeper here this is what we had before these were states we have this complex collision checker we had our",
    "start": "1032480",
    "end": "1038000"
  },
  {
    "text": "simulator or our first principles model now the equivalent of each of these models looks something like this our",
    "start": "1038000",
    "end": "1044558"
  },
  {
    "text": "state space is going to be an embedding of in theory a history of observations for example like images our",
    "start": "1044559",
    "end": "1053200"
  },
  {
    "text": "failure specification is now simply a classifier on the embedding",
    "start": "1053200",
    "end": "1058440"
  },
  {
    "text": "state telling us does this look like a failure and then our world model is",
    "start": "1058440",
    "end": "1065520"
  },
  {
    "text": "going to be our dynamics and let's first talk about what this really is so if I want to build a world model",
    "start": "1065520",
    "end": "1071760"
  },
  {
    "text": "this is essentially like a system ID problem it's very very hard problem but here's how they kind of work in this modern paradigm I give you a data set of",
    "start": "1071760",
    "end": "1079320"
  },
  {
    "text": "observation and action pairs the observation throughout this part of the talk are going to be proprioception data",
    "start": "1079320",
    "end": "1085360"
  },
  {
    "text": "for the robot plus the wrist camera and a third person camera that watches the scene and then we have the actions here",
    "start": "1085360",
    "end": "1091919"
  },
  {
    "text": "which are going to be delta endector poses given this data set I'm going to train the following model I'm going to",
    "start": "1091919",
    "end": "1098480"
  },
  {
    "text": "first train an encoder which is going to map observations to some posterior latent state",
    "start": "1098480",
    "end": "1105240"
  },
  {
    "text": "ZT and then given this I'm now going to train a dynamics model which is going to",
    "start": "1105240",
    "end": "1110480"
  },
  {
    "text": "predict how our current latent state and this corresponding action transition to the next latent state zhat uh T + one",
    "start": "1110480",
    "end": "1119440"
  },
  {
    "text": "and since we know the true next observation in our data set what we can do is we can train our model with a",
    "start": "1119440",
    "end": "1126000"
  },
  {
    "text": "variety of self-supervised losses such as reconstruction or teacher forcing when we're trying to make sure the model",
    "start": "1126000",
    "end": "1132000"
  },
  {
    "text": "is predicting or is imagining the next latency correctly i am not innovating on these",
    "start": "1132000",
    "end": "1138080"
  },
  {
    "text": "models by the way here we are building on really fantastic work from the modelbased RL community and from the",
    "start": "1138080",
    "end": "1143840"
  },
  {
    "text": "generative modeling community specifically the two works we build on are at the top there that's an older",
    "start": "1143840",
    "end": "1149200"
  },
  {
    "text": "paper called dreamer at the bottom this is a very new paper from Larl Pinto and Yanukun's group called Dino World Melod",
    "start": "1149200",
    "end": "1155200"
  },
  {
    "text": "and these are really nice modeling paradigms for getting this kind of structured model but there's much to be innovated on this as well if you're",
    "start": "1155200",
    "end": "1162280"
  },
  {
    "text": "curious okay we have two ingredients state dynamics what about our safety",
    "start": "1162280",
    "end": "1167559"
  },
  {
    "text": "spec okay well recall how I said that failures are observable this means that I can just look at my data set and I",
    "start": "1167559",
    "end": "1174160"
  },
  {
    "text": "could do something like label okay is the robot already in failure or not in these",
    "start": "1174160",
    "end": "1179480"
  },
  {
    "text": "observations okay so on the left hand side we're good on the right hand side the SK Skittles are already visible so",
    "start": "1179480",
    "end": "1184559"
  },
  {
    "text": "that means I'm already doomed we can encode each of these observations into our latent state and",
    "start": "1184559",
    "end": "1191039"
  },
  {
    "text": "now our safety spec problem becomes a classification problem on this latent state",
    "start": "1191039",
    "end": "1197360"
  },
  {
    "text": "uh I'm not going to show you the details of the actual loss function you can take a look at the paper but intuitively here's what it does essentially you're",
    "start": "1197360",
    "end": "1203760"
  },
  {
    "text": "penalizing latent states corresponding to observations in the failure set from being labeled positive and vice versa",
    "start": "1203760",
    "end": "1210880"
  },
  {
    "text": "why because if we go back to our reachability formulation we remember that the way we were encoding failure",
    "start": "1210880",
    "end": "1216720"
  },
  {
    "text": "was via that subzero level set of this sign distance function L here okay so we",
    "start": "1216720",
    "end": "1222240"
  },
  {
    "text": "need anything that any latent state that returns negative is going to be in our",
    "start": "1222240",
    "end": "1227200"
  },
  {
    "text": "failure okay great we have state we have dynamics we know our failure let's compute a safety filter and latent state",
    "start": "1227720",
    "end": "1234520"
  },
  {
    "text": "space this is formally the optimization problem we have to solve here now by the",
    "start": "1234520",
    "end": "1239760"
  },
  {
    "text": "way for those of you who are more familiar with for example model based reinforcement learning you'll notice that the key difference here is this min",
    "start": "1239760",
    "end": "1245600"
  },
  {
    "text": "over time rather than expected value you might see or sum over time and that's because we need to remember the worst",
    "start": "1245600",
    "end": "1251600"
  },
  {
    "text": "that ever happens the closest we ever get to failure all right so here is a",
    "start": "1251600",
    "end": "1257679"
  },
  {
    "text": "visualization of the latent state space by the way it is not going to be this pretty in reality this is just a graphic",
    "start": "1257679",
    "end": "1263440"
  },
  {
    "text": "um and what we're essentially doing is this inner L of X or L of Z sorry it's",
    "start": "1263440",
    "end": "1269120"
  },
  {
    "text": "going to define some subset of the latent states where if I chose a latent state here and I decoded it it would",
    "start": "1269120",
    "end": "1274880"
  },
  {
    "text": "show me something like this it would show me Skittles are on the table",
    "start": "1274880",
    "end": "1280159"
  },
  {
    "text": "now given any initial observation we encode it into our latent space shown here as a dot",
    "start": "1280159",
    "end": "1286360"
  },
  {
    "text": "Z0 and our safety analysis is now reasoning about the robot's best effort to",
    "start": "1286360",
    "end": "1292120"
  },
  {
    "text": "avoid this unsafe region this these failure regions in its imagination but",
    "start": "1292120",
    "end": "1297200"
  },
  {
    "text": "remember we're not doing any real robots in the environment we have built a model and now we're doing control in the",
    "start": "1297200",
    "end": "1305000"
  },
  {
    "text": "model if at if at any point you know the robot has tried a bunch of trajectories",
    "start": "1305000",
    "end": "1310880"
  },
  {
    "text": "and it is discovered all trajectories lead to failure that value function is",
    "start": "1310880",
    "end": "1316240"
  },
  {
    "text": "going to remember a negative number it's going to say any all roads lead to failure you are",
    "start": "1316240",
    "end": "1322120"
  },
  {
    "text": "doomed and that's where that men over time comes into play so in other words given the best",
    "start": "1322120",
    "end": "1328720"
  },
  {
    "text": "effort to avoid failure how close does the robot come to failing now to solve this problem more",
    "start": "1328720",
    "end": "1334720"
  },
  {
    "text": "tractably than just forward simulation we can derive a latent safety Bellman equation shown",
    "start": "1334720",
    "end": "1340679"
  },
  {
    "text": "here and if we were to solve for this value function backwards in time we",
    "start": "1340679",
    "end": "1345840"
  },
  {
    "text": "would see this latent unsafe set growing over time encoded as the latent state",
    "start": "1345840",
    "end": "1350960"
  },
  {
    "text": "where the value is less than zero and here's an example of one such state this situation where the robot has",
    "start": "1350960",
    "end": "1359360"
  },
  {
    "text": "already picked up the Skittles back so high up no matter what the robot does if it reorients or tilts or anything it",
    "start": "1359360",
    "end": "1366000"
  },
  {
    "text": "doesn't matter those Skittles are coming out and that's the gap between the failure set I see the Skittles on the",
    "start": "1366000",
    "end": "1371840"
  },
  {
    "text": "table and the unsafe set which is in the near future there's nothing I can do but those Skittles are going to come out",
    "start": "1371840",
    "end": "1379960"
  },
  {
    "text": "now the last but important thing I want to mention is this computation is you",
    "start": "1380039",
    "end": "1385280"
  },
  {
    "text": "know still subject to the latent state dimensionality uh and that's still very large so for",
    "start": "1385280",
    "end": "1391120"
  },
  {
    "text": "example in some of our experiments the latent state space Z is like 512 dimensional this is still much better",
    "start": "1391120",
    "end": "1398000"
  },
  {
    "text": "than the raw observation space to be clear given two wrist cameras three channels and 128 by 128 pixel",
    "start": "1398000",
    "end": "1405280"
  },
  {
    "text": "observation space that would be 98,000 dimensional so it's still better are",
    "start": "1405280",
    "end": "1411280"
  },
  {
    "text": "leveraging the compression from the high dimensional observations but it's still too high to",
    "start": "1411280",
    "end": "1417440"
  },
  {
    "text": "solve this problem exactly in a grid-based fashion and so we approximate the safety",
    "start": "1417440",
    "end": "1424799"
  },
  {
    "text": "value function by essentially doing reinforcement learning in the world model and this is just a modified",
    "start": "1424799",
    "end": "1431039"
  },
  {
    "text": "Bellman backup that is compatible with a lot of off-the-shelf RL techniques because it induces a nice contraction",
    "start": "1431039",
    "end": "1436799"
  },
  {
    "text": "mapping this is not our invention you can take a look at this paper for the details on the original uh connection",
    "start": "1436799",
    "end": "1443280"
  },
  {
    "text": "between reinforcement learning and reachability analysis all right ultimately after we",
    "start": "1443280",
    "end": "1449200"
  },
  {
    "text": "do this analysis offline we get our safety monitor and our safety policy co-optimized that's ready to be deployed",
    "start": "1449200",
    "end": "1456000"
  },
  {
    "text": "as a reminder here's the environment context here is the observation the robot sees it's going to get mapped to",
    "start": "1456000",
    "end": "1462000"
  },
  {
    "text": "some latent state Z and as the robot executes some nominal policy we see here",
    "start": "1462000",
    "end": "1467360"
  },
  {
    "text": "the laten state moving through the world model at some time ZT though our safety",
    "start": "1467360",
    "end": "1472640"
  },
  {
    "text": "monitor detects that if the robot continued along that nominal policy it would be doomed to fail and this is",
    "start": "1472640",
    "end": "1479840"
  },
  {
    "text": "where we take over we do that filtering technique and we say \"Nope you have to now apply that safety strategy",
    "start": "1479840",
    "end": "1488559"
  },
  {
    "text": "instead.\" Okay we have to do a sanity check though we've done a lot of approximations how",
    "start": "1489240",
    "end": "1495440"
  },
  {
    "text": "close does latent safety get to a privilege safety so here what we do is we do a",
    "start": "1495440",
    "end": "1502480"
  },
  {
    "text": "computation on a benchmark safe control task this is all of our favorite tasks in safe control it's a duben car 3D",
    "start": "1502480",
    "end": "1510159"
  },
  {
    "text": "model it only controls the angular velocity and there's this circle in the middle of the state space which is the",
    "start": "1510159",
    "end": "1515760"
  },
  {
    "text": "failure set our latent model sees these observations images plus the heading of",
    "start": "1515760",
    "end": "1522720"
  },
  {
    "text": "the robot by the way you might ask why heading so the image resolution we pass",
    "start": "1522720",
    "end": "1528559"
  },
  {
    "text": "into our world model is like 128 x 128 so actually the robot becomes a point in",
    "start": "1528559",
    "end": "1534240"
  },
  {
    "text": "the pixel space so it's really hard to tell what the orientation is if you had a higher fidelity um representation of",
    "start": "1534240",
    "end": "1541039"
  },
  {
    "text": "the image maybe you wouldn't need that data all right the data set is 2,00 random",
    "start": "1541039",
    "end": "1548360"
  },
  {
    "text": "trajectories of the robot moving around our world model is just an off-the-shelf",
    "start": "1548360",
    "end": "1553520"
  },
  {
    "text": "dreamer model that's a recurrent state space model and our failure classifier is trained on the same label data as our",
    "start": "1553520",
    "end": "1559039"
  },
  {
    "text": "world model okay here I'm showing",
    "start": "1559039",
    "end": "1565000"
  },
  {
    "text": "you on the left hand side the ground truth failure set inside of that dash",
    "start": "1565000",
    "end": "1571080"
  },
  {
    "text": "line and in the center on the right hand side I'm showing you the ground truth",
    "start": "1571080",
    "end": "1576240"
  },
  {
    "text": "unsafe set for two headings of the duben car theta= 0 is pointing like this",
    "start": "1576240",
    "end": "1581440"
  },
  {
    "text": "theta= p /2 is pointing like this and this is ground truth in the sense that we compute it via a super super",
    "start": "1581440",
    "end": "1587679"
  },
  {
    "text": "high fidelity gridbased solver if I give a reinforcement learning based",
    "start": "1587679",
    "end": "1595039"
  },
  {
    "text": "solver the privilege state and access to the ground truth dynamics this is how close we get to approximating the unsafe",
    "start": "1595039",
    "end": "1602039"
  },
  {
    "text": "set so you can think of this as any error is coming exclusively from RL",
    "start": "1602039",
    "end": "1607200"
  },
  {
    "text": "approximation but not from the representation of the problem and you can see the F1 score on the right hand",
    "start": "1607200",
    "end": "1612320"
  },
  {
    "text": "side so quite good this is uh across all uh XY theta",
    "start": "1612320",
    "end": "1618039"
  },
  {
    "text": "states here's latent safety okay so you'll see that the learned failure classifier a little bit more",
    "start": "1618039",
    "end": "1624880"
  },
  {
    "text": "conservative we incentivized it to be a little bit more conservative during training but the unsafe sets the",
    "start": "1624880",
    "end": "1631120"
  },
  {
    "text": "backwards reachable sets also quite high quality despite several of these",
    "start": "1631120",
    "end": "1636159"
  },
  {
    "text": "approximations so you can see the F1 score on the right hand side okay I want to really stress that the latent safety",
    "start": "1636159",
    "end": "1641840"
  },
  {
    "text": "only sees these observations but in this simple example we know which of these observations",
    "start": "1641840",
    "end": "1647520"
  },
  {
    "text": "correspond to which privileged states which is why we can do this controlled",
    "start": "1647520",
    "end": "1652399"
  },
  {
    "text": "analysis all right this is great but let's scale up to visual manipulation so here the robot has to pick up this green",
    "start": "1653400",
    "end": "1659840"
  },
  {
    "text": "block and it should never topple the red ones it's a very hard task they're really really really close to each other",
    "start": "1659840",
    "end": "1665919"
  },
  {
    "text": "even I if I teleop this would really struggle to do this now the nice thing about this problem is",
    "start": "1665919",
    "end": "1671760"
  },
  {
    "text": "that as a stakeholder I can easily identify what the failure set is and by the way the failure set is not a",
    "start": "1671760",
    "end": "1676960"
  },
  {
    "text": "collision avoidance spec the robot can touch push tilt those red blocks as long",
    "start": "1676960",
    "end": "1682080"
  },
  {
    "text": "as they do not fall over okay so let's see how our safety",
    "start": "1682080",
    "end": "1689200"
  },
  {
    "text": "filter how our models perform when we give them a known unsafe observation",
    "start": "1689200",
    "end": "1694520"
  },
  {
    "text": "sequence an action sequence so here is that video I just showed you but like timestamped like this and I'm going to",
    "start": "1694520",
    "end": "1701120"
  },
  {
    "text": "show you what both the failure classifier and the value function say over this",
    "start": "1701120",
    "end": "1706919"
  },
  {
    "text": "trajectory at the beginning of the trajectory life is good both of our methods say well at the start of the",
    "start": "1706919",
    "end": "1713679"
  },
  {
    "text": "trajectory you're not failing and you're not doomed same deal at the next uh snapshot",
    "start": "1713679",
    "end": "1720320"
  },
  {
    "text": "now things get interesting though because at t= 14 the robot has approached the center block so quickly",
    "start": "1720320",
    "end": "1726320"
  },
  {
    "text": "and it has ended up touching those red blocks with high enough force where soon",
    "start": "1726320",
    "end": "1731679"
  },
  {
    "text": "those blocks are doomed to fall and so this is why we see this",
    "start": "1731679",
    "end": "1737600"
  },
  {
    "text": "latent state already being in the subzero level set or in the backwards",
    "start": "1737600",
    "end": "1742640"
  },
  {
    "text": "reachable set that we computed it continues to say you are still doomed you are still doomed and at the last",
    "start": "1742640",
    "end": "1749279"
  },
  {
    "text": "time step our failure classifier says \"Yep now I definitely see those blocks falling down.\" You are in failure for",
    "start": "1749279",
    "end": "1756399"
  },
  {
    "text": "sure um let's see our latent safety filter uh protecting this base policy",
    "start": "1756520",
    "end": "1761600"
  },
  {
    "text": "from toppling over some red blocks uh the difference by the way between the left and right images is on the right",
    "start": "1761600",
    "end": "1766960"
  },
  {
    "text": "hand side it's just a slightly different initial condition of those blocks they're moved a little bit in front of the robot so it's two different initial",
    "start": "1766960",
    "end": "1773279"
  },
  {
    "text": "conditions uh the data you see here is exactly the same data that the robot gets to see every time you see the",
    "start": "1773279",
    "end": "1778919"
  },
  {
    "text": "border uh lighting up in green it means the safety controller is active rather than the base",
    "start": "1778919",
    "end": "1786039"
  },
  {
    "text": "policy and as the base policy approaches the blocks too fast the safety filter activates and it's slowing the robot",
    "start": "1786039",
    "end": "1792080"
  },
  {
    "text": "down only after the robot backs up a bit does the safety filter actually let go and let the robot finish the task and",
    "start": "1792080",
    "end": "1799200"
  },
  {
    "text": "you see that those red blocks are able to be perturred they can move around but they don't fall",
    "start": "1799200",
    "end": "1805360"
  },
  {
    "text": "down um I'm going to summarize some of our quantitative results here so what we",
    "start": "1805720",
    "end": "1810880"
  },
  {
    "text": "did was we said okay how much better is our latent safety filter compared to other latent control strategies you",
    "start": "1810880",
    "end": "1816880"
  },
  {
    "text": "could apply so all these methods use the same exact world model same exact data nothing is different the only difference",
    "start": "1816880",
    "end": "1822320"
  },
  {
    "text": "is the objective so the top baseline dreamer has soft constraints you're kind",
    "start": "1822320",
    "end": "1827919"
  },
  {
    "text": "of co-optimizing you know grabbing the block and not uh toppling over the red blocks the middle one is a constrained",
    "start": "1827919",
    "end": "1834720"
  },
  {
    "text": "MDP formulation you can think of this kind of like an empirical risk of violating a constraint and then ours is",
    "start": "1834720",
    "end": "1841039"
  },
  {
    "text": "this safety policy which shields and you'll see that across the successes violations and incompletion",
    "start": "1841039",
    "end": "1847600"
  },
  {
    "text": "rates uh ours is not perfect we still fail 20% of the time it's a really really hard task to do but what you'll",
    "start": "1847600",
    "end": "1855200"
  },
  {
    "text": "see is that this balance this balance between success and",
    "start": "1855200",
    "end": "1861240"
  },
  {
    "text": "failure is a little bit better than if you were for example tuning a risk threshold which is really sensitive tiny",
    "start": "1861240",
    "end": "1867279"
  },
  {
    "text": "change in the risk threshold huge performance difference those middle two rows and tuning rewards I'm sure as all",
    "start": "1867279",
    "end": "1873679"
  },
  {
    "text": "of you know it's quite difficult to actually really really find that sweet spot and that's what the first line is",
    "start": "1873679",
    "end": "1879799"
  },
  {
    "text": "showing okay but the ultimate test is going to be in hardware let's see if this actually works in hardware we're going to return to our",
    "start": "1879799",
    "end": "1886000"
  },
  {
    "text": "Skittles example this is the data the robot gets to see third person camera it's like right here looking like this",
    "start": "1886000",
    "end": "1892880"
  },
  {
    "text": "and the wrist camera let's watch one more time uh so my student here is going to be",
    "start": "1892880",
    "end": "1898760"
  },
  {
    "text": "teleoperating this robot and here we see that failure we saw from before where if my student happens to choose this wrong",
    "start": "1898760",
    "end": "1905399"
  },
  {
    "text": "grasp all the Skittles come flying out for world model training we use",
    "start": "1905399",
    "end": "1913480"
  },
  {
    "text": "1,300 trajectories collected on the robot of these 80% are",
    "start": "1913480",
    "end": "1919480"
  },
  {
    "text": "random so my student just samples from a gausian and just lets the robot bumble",
    "start": "1919480",
    "end": "1925159"
  },
  {
    "text": "around 10% of them are explicit safe demonstrations where my student was showing how to pick up a bag from the",
    "start": "1925159",
    "end": "1931519"
  },
  {
    "text": "top and then 10% of them are explicitly unsafe where a spill is induced by my",
    "start": "1931519",
    "end": "1937200"
  },
  {
    "text": "student why do we have to have this data well the model can't predict what it hasn't [Music]",
    "start": "1937200",
    "end": "1943960"
  },
  {
    "text": "seen okay given our trained failure classifier on this data we compute offline this backwards reachable set in",
    "start": "1943960",
    "end": "1949919"
  },
  {
    "text": "the world model via that process we saw before and then we can deploy okay on the left hand side you're",
    "start": "1949919",
    "end": "1959200"
  },
  {
    "text": "going to see that when my student chooses to grasp from the top of the open bag life is really good you see",
    "start": "1959200",
    "end": "1965760"
  },
  {
    "text": "that boundary green boundary is never showing up that means he's fully in control the entire time but in the middle what he's going",
    "start": "1965760",
    "end": "1972399"
  },
  {
    "text": "to do now is he's going to grab the bottom of the bag and start to shake side to side as he shakes you see that",
    "start": "1972399",
    "end": "1978720"
  },
  {
    "text": "occasionally that green light is flagging and it's because the filter is slowing him down because it knows that if he shakes too hard the Skittles will",
    "start": "1978720",
    "end": "1985200"
  },
  {
    "text": "come out last but not least if he grabs the bottom and pulls up really sharply it",
    "start": "1985200",
    "end": "1991760"
  },
  {
    "text": "completely overrides his commands um I also want to show you a continuous",
    "start": "1991760",
    "end": "1998559"
  },
  {
    "text": "like an uncut version of this if you're ever in Pittsburgh please come to my lab you can mess around with this it's a lot of fun um so once again I'm showing you",
    "start": "1998559",
    "end": "2006480"
  },
  {
    "text": "that robot POV on the top and then on the bottom I'm showing you the value of the next state assuming I executed the",
    "start": "2006480",
    "end": "2012440"
  },
  {
    "text": "teleoperator's actions so the value is really good at the beginning it's positive tele",
    "start": "2012440",
    "end": "2019279"
  },
  {
    "text": "operator is doing a good job when he starts to shake you start getting close to that boundary you're getting to a",
    "start": "2019279",
    "end": "2024320"
  },
  {
    "text": "danger zone and that's why the safety filter has to kick in then when he grabs the bottom and pulls up you're going to",
    "start": "2024320",
    "end": "2030320"
  },
  {
    "text": "see that value drop really really really sharply because if we continue to pull up we would definitely be",
    "start": "2030320",
    "end": "2037120"
  },
  {
    "text": "doomed the next thing I did was I bought all the Skittles I could find in the Pittsburgh neighborhood and I gave them",
    "start": "2037799",
    "end": "2044159"
  },
  {
    "text": "to my students to say \"Okay does it work with other Skittles?\" So remember we only trained",
    "start": "2044159",
    "end": "2049358"
  },
  {
    "text": "on the red Skittles we deployed here with sour wild berry and smoothie Skittles uh it generalizes we tried to",
    "start": "2049359",
    "end": "2057118"
  },
  {
    "text": "change the background it also works this is an indistribution orientation but it",
    "start": "2057119",
    "end": "2062560"
  },
  {
    "text": "also works out um I'm not going to cover uh another",
    "start": "2062560",
    "end": "2068560"
  },
  {
    "text": "example here that you can take a look at our paper we can take the same exact filter because it is policy agnostic and",
    "start": "2068560",
    "end": "2074560"
  },
  {
    "text": "instead of shielding a tea operator you can shield any other policy so we shielded for example a diffusion policy",
    "start": "2074560",
    "end": "2080079"
  },
  {
    "text": "that was pre-trained and we also find that it decreases the percentage of the Skittles that are",
    "start": "2080079",
    "end": "2086520"
  },
  {
    "text": "spilled but what I want to quickly show you guys is some dirty laundry so this is not perfect again it is learning",
    "start": "2086520",
    "end": "2092320"
  },
  {
    "text": "based we we are just beginning to understand the limits of these components and how they work together",
    "start": "2092320",
    "end": "2097440"
  },
  {
    "text": "this existence proof is exciting though some of the failures we've seen that are quite interesting are the following this",
    "start": "2097440",
    "end": "2102880"
  },
  {
    "text": "is an observability issue on the lefth hand side if you rotate the Skittles bag 180 degrees and so the opening is not",
    "start": "2102880",
    "end": "2110400"
  },
  {
    "text": "observable by either the wrist camera or the third person camera the filter is like \"Yeah it's a closed bag you're good to go.\" And that's why it doesn't",
    "start": "2110400",
    "end": "2119280"
  },
  {
    "text": "override okay on the right hand side uh I bought my students some peanut M&M's",
    "start": "2120119",
    "end": "2125440"
  },
  {
    "text": "peanut M&M's are much heavier than Skittles in fact I didn't appreciate all the different bag types by the way",
    "start": "2125440",
    "end": "2130880"
  },
  {
    "text": "before this project um peanut M&M's come in a papery bag skittles come in a",
    "start": "2130880",
    "end": "2137280"
  },
  {
    "text": "plasticky bag very different dynamics since we only trained on Skittles if I",
    "start": "2137280",
    "end": "2142400"
  },
  {
    "text": "if we put the M&M's bag down the filter is not reliable it's overly optimistic",
    "start": "2142400",
    "end": "2147599"
  },
  {
    "text": "and uh fails to protect the robot maybe this would go away with more data i",
    "start": "2147599",
    "end": "2153200"
  },
  {
    "text": "don't know but for this particular setting we didn't find this generalization across dynamics which is",
    "start": "2153200",
    "end": "2158800"
  },
  {
    "text": "not too surprising yes please question i mean",
    "start": "2158800",
    "end": "2164560"
  },
  {
    "text": "but the information that it's using is just visual information one end of the bag has a look of the bag and the other",
    "start": "2164560",
    "end": "2172560"
  },
  {
    "text": "end doesn't why should whether the M&M's are heavier or the bag is more papery",
    "start": "2172560",
    "end": "2180079"
  },
  {
    "text": "because the value function depends on the dynamics prediction and you can't accurately predict the dynamics of this",
    "start": "2180079",
    "end": "2185760"
  },
  {
    "text": "bag that's aggressively out of distribution so the model",
    "start": "2185760",
    "end": "2192760"
  },
  {
    "text": "So I thought the same thing and then my student told me to interact with this bag with the tea operation setup it is",
    "start": "2196320",
    "end": "2201680"
  },
  {
    "text": "really really hard again that paper versus plastic and the heaviness of those M&M Skittles in our tiny data",
    "start": "2201680",
    "end": "2209040"
  },
  {
    "text": "regime with just the Skittles it just doesn't generalize unfortunately how are",
    "start": "2209040",
    "end": "2214280"
  },
  {
    "text": "you and how does it Mhm and we you're not sensing are you",
    "start": "2214280",
    "end": "2220400"
  },
  {
    "text": "sensing forces um no maybe implicitly perception but Exactly yeah",
    "start": "2220400",
    "end": "2228320"
  },
  {
    "text": "yeah so again this might It's hard here by the way to disentangle is it an observability problem like maybe we need",
    "start": "2228320",
    "end": "2235280"
  },
  {
    "text": "more sensors is it a dynamics modeling like it's really hard to disentangle this we recently were trying to diagnose",
    "start": "2235280",
    "end": "2241040"
  },
  {
    "text": "if it could also be the visual differences like the text Eminem looks different than Skittles so we looked at",
    "start": "2241040",
    "end": "2246560"
  },
  {
    "text": "like uh so for example the the the model we used here uses dyno encodings we looked at the top three PCA dimensions",
    "start": "2246560",
    "end": "2252720"
  },
  {
    "text": "of the dyno embeddings to see if they're similar they look pretty similar maybe it's some higher order PCA components",
    "start": "2252720",
    "end": "2257760"
  },
  {
    "text": "that matter a lot i don't know yet it's really hard to diagnose this and if anyone has thoughts or would like to work on this please let me know because",
    "start": "2257760",
    "end": "2263680"
  },
  {
    "text": "I'm very curious uh about understanding the limits yes",
    "start": "2263680",
    "end": "2270200"
  },
  {
    "text": "kind of uncertain earlier can you do one thing where you can online escalate the",
    "start": "2271760",
    "end": "2278240"
  },
  {
    "text": "uncertainty of the dynamics which is more and you can see what's happening and maybe that would help with some it's",
    "start": "2278240",
    "end": "2285040"
  },
  {
    "text": "not full but might help someone yes uh we're working towards a coral paper that shows no don't apologize this is great i",
    "start": "2285040",
    "end": "2292320"
  },
  {
    "text": "had the same exact question as soon as we saw this so uh yeah we're thinking of how to inject or model uncertainty if",
    "start": "2292320",
    "end": "2298240"
  },
  {
    "text": "you do it carefully it can help a lot with the robustness we're not 100% there yet i think there's many smarter ways we",
    "start": "2298240",
    "end": "2304560"
  },
  {
    "text": "can do it uh we have just one attempt at the problem uh let's see so the the framing",
    "start": "2304560",
    "end": "2312000"
  },
  {
    "text": "of the problem at the beginning you have state space and then you have dynamics then you have the failure motor the guy",
    "start": "2312000",
    "end": "2320240"
  },
  {
    "text": "or whatever it is um and I'm thinking about a very specific",
    "start": "2320240",
    "end": "2328079"
  },
  {
    "text": "problem from the real world yeah you have a mobile robot you're trying to deploy it we want it to be safe safe",
    "start": "2328079",
    "end": "2333760"
  },
  {
    "text": "means don't fall down the stairs or insidiously an an escalator right because you don't see the escalator you",
    "start": "2333760",
    "end": "2340240"
  },
  {
    "text": "don't see the stairs until you're on it um the problem is misloization so you",
    "start": "2340240",
    "end": "2345599"
  },
  {
    "text": "have the state space on moving one how do you represent that in this framework because if you start off misloized all",
    "start": "2345599",
    "end": "2352240"
  },
  {
    "text": "frameworks it goes out the window um yeah maybe maybe my current the current",
    "start": "2352240",
    "end": "2358079"
  },
  {
    "text": "thing we're exploring which is maybe part of your question is",
    "start": "2358079",
    "end": "2363640"
  },
  {
    "text": "um essentially if you think about what the what the encoder is kind of trying",
    "start": "2363640",
    "end": "2369119"
  },
  {
    "text": "to do it's kind of trying to do state estimation for you and sometimes you need to gather more",
    "start": "2369119",
    "end": "2376000"
  },
  {
    "text": "data or like be a little bit more active about knowing whether or not you actually know where you are or what the",
    "start": "2376000",
    "end": "2383680"
  },
  {
    "text": "environment is like and so we've been thinking a little bit about what does it mean to do kind of like active exploration in these world models to get",
    "start": "2383680",
    "end": "2390720"
  },
  {
    "text": "a better sense or more confident sense of exactly where you are so then you can do safe control yeah the danger is of",
    "start": "2390720",
    "end": "2396960"
  },
  {
    "text": "course if you're actively exploring and you get onto the escalator and now it pulls you down you're in the bad",
    "start": "2396960",
    "end": "2403760"
  },
  {
    "text": "situation agreed agreed um what we ended up uh in sort of adding sensors that",
    "start": "2403760",
    "end": "2410480"
  },
  {
    "text": "give you secondary guarantees against that yeah and that by the way I think that",
    "start": "2410480",
    "end": "2416240"
  },
  {
    "text": "goes back to the discussion about observability like if you have more sensors then you can have a higher quality estimate of where you are and",
    "start": "2416240",
    "end": "2422160"
  },
  {
    "text": "therefore you have a better estimate of are you going to be doomed soon and so I think um we have other directions we're",
    "start": "2422160",
    "end": "2427440"
  },
  {
    "text": "thinking about about how to essentially use uh better other sensors to get a better more accurate representation of",
    "start": "2427440",
    "end": "2434079"
  },
  {
    "text": "if you're going to be failing or not there's a question in the back as well i was just wondering during data",
    "start": "2434079",
    "end": "2440079"
  },
  {
    "text": "collection um did you change the orientation of the bag yes that's why um",
    "start": "2440079",
    "end": "2446160"
  },
  {
    "text": "if I go back uh here so this is in distribution a different orientation of the bag but we never like the the the",
    "start": "2446160",
    "end": "2454720"
  },
  {
    "text": "full 180 tilt you just literally can't see the opening so like you don't exactly know how to interact with it",
    "start": "2454720",
    "end": "2463319"
  },
  {
    "text": "oh and if you say you just train an imitation learning pipeline where you",
    "start": "2463319",
    "end": "2469440"
  },
  {
    "text": "only show demonstration with correct yeah um how does it compare that",
    "start": "2469440",
    "end": "2474960"
  },
  {
    "text": "experiment for it because I would think maybe that would also work there you go so I didn't go to these details for the",
    "start": "2474960",
    "end": "2480800"
  },
  {
    "text": "sake of time you can take a look at the paper essentially if you just train it on safe data the policy is overly",
    "start": "2480800",
    "end": "2487319"
  },
  {
    "text": "optimistic yeah so your spill percentage is much higher again in theory if you",
    "start": "2487319",
    "end": "2493280"
  },
  {
    "text": "collect enough data you could eventually get there uh but if you have trained imitation based policies I don't know if",
    "start": "2493280",
    "end": "2499599"
  },
  {
    "text": "you have you know that it's uh non-trivial to get them to actually be really reliable",
    "start": "2499599",
    "end": "2505520"
  },
  {
    "text": "yes I had a question about um data collection also like with the 1300",
    "start": "2505520",
    "end": "2511040"
  },
  {
    "text": "random uh orientations was your PhD student like",
    "start": "2511040",
    "end": "2516200"
  },
  {
    "text": "manually doing all of those or was that So remember 80% were random the way that",
    "start": "2516200",
    "end": "2523520"
  },
  {
    "text": "we got this data was from sampling from a gausian in action space and then just executing it so 10,00 trajectories were",
    "start": "2523520",
    "end": "2531040"
  },
  {
    "text": "random but what does the like random sampling actually look like you just",
    "start": "2531040",
    "end": "2536760"
  },
  {
    "text": "like randomly choose an action and then you execute it and then you sample again and then you execute it you can take a",
    "start": "2536760",
    "end": "2545119"
  },
  {
    "text": "look at the paper as well for more details that's part of the beauty of doing something like uh building a model",
    "start": "2545119",
    "end": "2550319"
  },
  {
    "text": "is that actually you don't need perfect data if you think about how a lot of the times um data curation is very important",
    "start": "2550319",
    "end": "2557680"
  },
  {
    "text": "it's still important in building models too but even random data is really helpful because it gives you a model of",
    "start": "2557680",
    "end": "2563319"
  },
  {
    "text": "outcomes which is really really useful for planning while for example for something like imitation you'd only use that 10% of safe demos you would never",
    "start": "2563319",
    "end": "2570400"
  },
  {
    "text": "you would never want to like have your policy memorized random actions in the back yeah where the model",
    "start": "2570400",
    "end": "2577599"
  },
  {
    "text": "was focusing throughout the duration of of its action like was it looking for like oh is this like a white edge or",
    "start": "2577599",
    "end": "2584160"
  },
  {
    "text": "like how was it actually going that uh so in general model interpretability is very hard that's what I mentioned",
    "start": "2584160",
    "end": "2590000"
  },
  {
    "text": "earlier where like you can look at the uh let's say top three PCA dimensions of the encoded images as a proxy so for",
    "start": "2590000",
    "end": "2599040"
  },
  {
    "text": "example if you look at this in our case you'll see things like um the model has learned to segment out essentially the",
    "start": "2599040",
    "end": "2605760"
  },
  {
    "text": "bag from the background it knows that the gripper is different from the bag so it has some notion of like instance",
    "start": "2605760",
    "end": "2612599"
  },
  {
    "text": "segmentation but I don't claim to actually fully be able to interpret the model's understanding",
    "start": "2612599",
    "end": "2618960"
  },
  {
    "text": "yes what are the pros and cons of this method versus using like imitation",
    "start": "2618960",
    "end": "2624160"
  },
  {
    "text": "learning from a expert demonstration yeah so as I just mentioned I think that",
    "start": "2624160",
    "end": "2629760"
  },
  {
    "text": "uh there's nothing wrong with using imitation in fact that's a great way to get a base policy to do something",
    "start": "2629760",
    "end": "2635040"
  },
  {
    "text": "generally capable like I mentioned at the very beginning of my talk I think there's two things that are really",
    "start": "2635040",
    "end": "2640800"
  },
  {
    "text": "helpful one is from a data perspective when you train an imitation policy you're actually throwing out a lot of",
    "start": "2640800",
    "end": "2646079"
  },
  {
    "text": "data usually what you'll do is you'll like collect some data for the policy",
    "start": "2646079",
    "end": "2651280"
  },
  {
    "text": "that expert data and you train the policy you do some rollouts those rollouts by the way you never end up looking at again you just look at them",
    "start": "2651280",
    "end": "2657359"
  },
  {
    "text": "for eval then you see that your policy kind of sucks so you collect more data then you put it back into your policy",
    "start": "2657359",
    "end": "2662400"
  },
  {
    "text": "and you keep doing this and you end up using a tiny fraction of your data set for the actual policy what if you used",
    "start": "2662400",
    "end": "2668160"
  },
  {
    "text": "all that data you shoved it into a world model because that model doesn't care about the quality it just wants diversity to learn how to predict",
    "start": "2668160",
    "end": "2674960"
  },
  {
    "text": "outcomes of what's going to happen next so I think it's a really nice complimentary way of using all that data",
    "start": "2674960",
    "end": "2680800"
  },
  {
    "text": "you're generating to building a better understanding of the world and the second thing that this is really helpful",
    "start": "2680800",
    "end": "2685839"
  },
  {
    "text": "for is it again in my students experience it is very hard to actually",
    "start": "2685839",
    "end": "2691119"
  },
  {
    "text": "get these imitationbased policies to do the right thing and so this is an",
    "start": "2691119",
    "end": "2696319"
  },
  {
    "text": "additional layer you can have on top of this where you can try your hardest to get recovery behaviors and have the base",
    "start": "2696319",
    "end": "2701440"
  },
  {
    "text": "imitation policy do the right thing but you have this extra sanity check at the output that just makes sure that these",
    "start": "2701440",
    "end": "2707040"
  },
  {
    "text": "hard to model hard to encode uh constraints are actually enforced",
    "start": "2707040",
    "end": "2712960"
  },
  {
    "text": "yes so two questions first one for the very long flight is there a way for the",
    "start": "2712960",
    "end": "2718400"
  },
  {
    "text": "robot to look or for the sensors to look at the bag or maybe touch the bag a little bit and then realize that maybe you shouldn't pick it up because it's",
    "start": "2718400",
    "end": "2724880"
  },
  {
    "text": "unsure first question second question the Skittles oh okay um so first",
    "start": "2724880",
    "end": "2732079"
  },
  {
    "text": "question yes so this is related to some of the directions we're trying out in the lab now basically some active",
    "start": "2732079",
    "end": "2738119"
  },
  {
    "text": "perception like you need to model that it is hard to do though but there's",
    "start": "2738119",
    "end": "2743520"
  },
  {
    "text": "things one should do you just realize how it seems before oh",
    "start": "2743520",
    "end": "2750160"
  },
  {
    "text": "uh our coral work will give you some and then your second question no one eats the Skittles they're disgusting they",
    "start": "2750160",
    "end": "2756640"
  },
  {
    "text": "keep falling on the ground during our experiments so no one eats those but we're always finding more of them okay",
    "start": "2756640",
    "end": "2762240"
  },
  {
    "text": "uh hold on uh for the sake of time I want to make sure I get to the second half of my talk i'm happy to chat more",
    "start": "2762240",
    "end": "2767680"
  },
  {
    "text": "later as well so sorry to keep going for that all right let's see let's keep going though um and I want to quickly do",
    "start": "2767680",
    "end": "2772960"
  },
  {
    "text": "another uh deep dive into some of our recent stuff so remember I said safety spec is a classification problem who",
    "start": "2772960",
    "end": "2779680"
  },
  {
    "text": "gives you the labels okay this is who gives me my labels my hardworking student looking at those videos labeling",
    "start": "2779680",
    "end": "2786960"
  },
  {
    "text": "i would argue again watching videos to see where the Skittles are way easier than writing a collision checker way",
    "start": "2786960",
    "end": "2793359"
  },
  {
    "text": "easier than giving tons and tons of teleop demos but it's still a lot of",
    "start": "2793359",
    "end": "2798520"
  },
  {
    "text": "work however more fundamentally the actual thing that this reveals is that if you really think about it the failure",
    "start": "2798520",
    "end": "2805200"
  },
  {
    "text": "spec that tells you what is safe or unsafe it's very context dependent for",
    "start": "2805200",
    "end": "2810440"
  },
  {
    "text": "example grabbing the chip bag really really tight by the center okay that's safe if I'm making a crunchy topping",
    "start": "2810440",
    "end": "2817520"
  },
  {
    "text": "it's unsafe if I want to eat that snack later so what I really need to do is I need to",
    "start": "2817520",
    "end": "2822800"
  },
  {
    "text": "make sure that I can kind of adapt what I mean by safe or unsafe what I mean by that failure classifier depending on",
    "start": "2822800",
    "end": "2829040"
  },
  {
    "text": "these contexts and so here's where we asked ourselves can we somehow use knowledge in these pre-trained models to",
    "start": "2829040",
    "end": "2835280"
  },
  {
    "text": "be kind of a context dependent failure detector and this is where I'm going to touch on the last work here we're",
    "start": "2835280",
    "end": "2841760"
  },
  {
    "text": "calling it uh BLM in the loop policy steering all right here's the setup same",
    "start": "2841760",
    "end": "2847040"
  },
  {
    "text": "robot different task his task now is to serve a cup of water to the guest the",
    "start": "2847040",
    "end": "2852400"
  },
  {
    "text": "base policy since all of you are asking about imitation the base policy is an imitationbased policy it's a diffusion model trained on a bunch of diverse ways",
    "start": "2852400",
    "end": "2859440"
  },
  {
    "text": "of handling this cup okay what do you do at deployment time you sample from the generative model these are different",
    "start": "2859440",
    "end": "2865839"
  },
  {
    "text": "samples that come out from this multimodal policy the problem is that not all",
    "start": "2865839",
    "end": "2871119"
  },
  {
    "text": "sampled actions result in the same task performance for example if I happen to choose this mode this sample is really",
    "start": "2871119",
    "end": "2878800"
  },
  {
    "text": "good it grabs the cup by the handle but there's another situation I",
    "start": "2878800",
    "end": "2885599"
  },
  {
    "text": "can get into i sample this mode and in this mode the robot does the",
    "start": "2885599",
    "end": "2891400"
  },
  {
    "text": "task but it puts his fingers in my cup that's not great okay last but not least uh you can",
    "start": "2891400",
    "end": "2899920"
  },
  {
    "text": "also sample a mode that when you execute it just totally fails it's really hard to know this a",
    "start": "2899920",
    "end": "2906480"
  },
  {
    "text": "priority that's why you have to do tons and tons of rollouts in the real world to know if your imitation policy is really good",
    "start": "2906480",
    "end": "2912280"
  },
  {
    "text": "um now but the but the key thing to notice here is that this base policy actually does contain the right behavior",
    "start": "2912280",
    "end": "2918640"
  },
  {
    "text": "it is useful but what you need to do is verify that the robot sampled action plan is",
    "start": "2918640",
    "end": "2925200"
  },
  {
    "text": "actually going to lead to the outcomes you want okay mathematically how can we think",
    "start": "2925200",
    "end": "2931200"
  },
  {
    "text": "about this you can think of this problem of what I'm calling here policy steering there's a few other works that think about this problem as well um you can",
    "start": "2931200",
    "end": "2937920"
  },
  {
    "text": "think of this as a model predictive control problem mathematically here's what you're really doing under the",
    "start": "2937920",
    "end": "2943480"
  },
  {
    "text": "hood your optimization variable is to select a sample here I'm just showing",
    "start": "2943480",
    "end": "2948800"
  },
  {
    "text": "you three samples but the problem is that because your base model is generating",
    "start": "2948800",
    "end": "2954520"
  },
  {
    "text": "actions and what you really need to evaluate them is you need to know outcomes so you need to somehow",
    "start": "2954520",
    "end": "2960359"
  },
  {
    "text": "translate the low-level actions into outcomes for example maybe in theory you would need to predict the future",
    "start": "2960359",
    "end": "2966319"
  },
  {
    "text": "observation sequence given the current observation and the actions and then once you predict those outcomes then you",
    "start": "2966319",
    "end": "2972559"
  },
  {
    "text": "can evaluate them with some reward function that maybe is parameterized by some task context like serve the cup of water to the guest which in this case is",
    "start": "2972559",
    "end": "2979200"
  },
  {
    "text": "written in natural language this is a nice way of writing down the problem it is important to know what you",
    "start": "2979200",
    "end": "2984720"
  },
  {
    "text": "are solving but how do you actually solve it is the trick and so here we use the same key",
    "start": "2984720",
    "end": "2990319"
  },
  {
    "text": "idea we say let's reason about outcomes in the world model so here you we're predicting those",
    "start": "2990319",
    "end": "2996319"
  },
  {
    "text": "latent states again and then let's align a VLM to directly reason about those latent states for",
    "start": "2996319",
    "end": "3002400"
  },
  {
    "text": "evaluation that can easily be adapted depending on different contexts you give me here's how it works",
    "start": "3002400",
    "end": "3010160"
  },
  {
    "text": "we take those observations we encode them into our world model here are three latent state trajectories that",
    "start": "3010160",
    "end": "3017200"
  },
  {
    "text": "correspond to our three action plans that we sampled from the base model if I",
    "start": "3017200",
    "end": "3022480"
  },
  {
    "text": "decoded these trajectories they would look something like this you see these outcomes that correspond to what I just",
    "start": "3022480",
    "end": "3029599"
  },
  {
    "text": "showed you these are these different modes importantly though we are not passing the actual decoded images into",
    "start": "3029599",
    "end": "3035839"
  },
  {
    "text": "the VLM we have baselines like this please look at it does not work what we do instead is we pass the latent states",
    "start": "3035839",
    "end": "3042880"
  },
  {
    "text": "directly to the VLM and we get it to reason about the latent states directly via via",
    "start": "3042880",
    "end": "3050160"
  },
  {
    "text": "essentially posing a latent text alignment problem for the VLM so what we do is we take in",
    "start": "3050160",
    "end": "3056400"
  },
  {
    "text": "this case Llama 3.211B vision instruct and we fine-tune it via a video question answering problem we're essentially",
    "start": "3056400",
    "end": "3063280"
  },
  {
    "text": "given embeddings of the video that we have in our data set we also have",
    "start": "3063280",
    "end": "3068920"
  },
  {
    "text": "narrations right like descriptions of what is happening and then we can ask the model to narrate what is happening",
    "start": "3068920",
    "end": "3075359"
  },
  {
    "text": "in these latent states by converting the lowlevel embodied state Z into a textual",
    "start": "3075359",
    "end": "3083200"
  },
  {
    "text": "description you are putting these outcomes in the native representation that the LLM is capable of reasoning",
    "start": "3083200",
    "end": "3089359"
  },
  {
    "text": "about for really high quality evaluation and so what happens is we take those",
    "start": "3089359",
    "end": "3095920"
  },
  {
    "text": "lane states and here's the prompt we pass the the latent to line VLM we say",
    "start": "3095920",
    "end": "3101040"
  },
  {
    "text": "the robot aims to grasp a cup from the table provide a sentence that best describes these different lane states it",
    "start": "3101040",
    "end": "3106319"
  },
  {
    "text": "describes okay the robot's attempting to grasp the cup by the handle sees the cup through the interior or it fails to",
    "start": "3106319",
    "end": "3111760"
  },
  {
    "text": "achieve a secure grasp we call the same model or you can call a different model to now reason about these descriptions",
    "start": "3111760",
    "end": "3119119"
  },
  {
    "text": "and we say okay this is the task context you're serving a cup of water to the guest please select the best action plan",
    "start": "3119119",
    "end": "3125200"
  },
  {
    "text": "and the model says okay the chosen mode is one because the most suitable way to serve the cup without spilling or",
    "start": "3125200",
    "end": "3130720"
  },
  {
    "text": "contaminating the drinks is this one so essentially the VLM is also doing this arc",
    "start": "3130720",
    "end": "3136800"
  },
  {
    "text": "max all right so here's the result uh this is that same example I showed you before these are some of the samples",
    "start": "3137480",
    "end": "3143119"
  },
  {
    "text": "that you can possibly get uh and if you use our VLM in the loop policy steering method given this task context the robot",
    "start": "3143119",
    "end": "3150079"
  },
  {
    "text": "chooses to grab the cup by the handle but if you change the context you say the handle is covered in oil it'll",
    "start": "3150079",
    "end": "3155440"
  },
  {
    "text": "choose to grab it by the interior here's another example remember that chip bag example I showed you",
    "start": "3155440",
    "end": "3161359"
  },
  {
    "text": "before if you say avoid crushing the contents inside it grabs it by the edge if you say maximize stability it's going",
    "start": "3161359",
    "end": "3166800"
  },
  {
    "text": "to grab it by the middle same base policy we are not touching it we're not retraining it are",
    "start": "3166800",
    "end": "3172079"
  },
  {
    "text": "just scoring the generations and choosing only the ones that are aligned uh we also deployed this for a",
    "start": "3172079",
    "end": "3179760"
  },
  {
    "text": "longer horizon task which requires reasoning about interactions between multiple objects so here the first uh",
    "start": "3179760",
    "end": "3186240"
  },
  {
    "text": "situation our robot is in is it has to grab the fork there's many ways to grab it by the times or by the handle and so",
    "start": "3186240",
    "end": "3192640"
  },
  {
    "text": "the predicted latent states when they're decoded into textual descriptions reflect this and the VLM says \"Okay well",
    "start": "3192640",
    "end": "3198400"
  },
  {
    "text": "if you're uh trying to eat with this I should probably choose the mode that touches the base of the fork and not the",
    "start": "3198400",
    "end": "3204440"
  },
  {
    "text": "times.\" So that's what ends up happening and then we call our method one more time we say \"Okay what can",
    "start": "3204440",
    "end": "3211599"
  },
  {
    "text": "happen next?\" And it says \"Okay the robot can drop the fork outside the bowl it can fail to place it in the bowl or maybe you can release the fork inside",
    "start": "3211599",
    "end": "3217760"
  },
  {
    "text": "the bowl that's good that's the one I want.\" And then the model chooses that one it's not fast enough to run like",
    "start": "3217760",
    "end": "3224160"
  },
  {
    "text": "real time it we benchmarked it it takes about 3.7 seconds to call this you can",
    "start": "3224160",
    "end": "3229839"
  },
  {
    "text": "maybe get it faster with distillation techniques quantization whatever you want but this is more of like a highle",
    "start": "3229839",
    "end": "3235760"
  },
  {
    "text": "way of steering it it's an interesting question to ask like when should you stop to think that's a really cool",
    "start": "3235760",
    "end": "3242319"
  },
  {
    "text": "question that a lot of LLM folks are thinking about i think it'll have a non-trivial implication in embodied AI",
    "start": "3242319",
    "end": "3247599"
  },
  {
    "text": "too um but this is what we've what we've kind of seen now one question you might",
    "start": "3247599",
    "end": "3252880"
  },
  {
    "text": "ask is what's the benefit of this two-stage pipeline first predicting with the world model and then describing with",
    "start": "3252880",
    "end": "3259280"
  },
  {
    "text": "an aligned VLM and we actually tried this so here we have this one baseline this is the same exact llama model fine",
    "start": "3259280",
    "end": "3265440"
  },
  {
    "text": "tune on the same exact observation data and this model takes us input the future action sequence the current observation",
    "start": "3265440",
    "end": "3273119"
  },
  {
    "text": "and has to generate a textual description of what's going to happen in the future end to end there's no intermediate latent state translation",
    "start": "3273119",
    "end": "3280000"
  },
  {
    "text": "and we find that this endto-end model given the same exact data set really struggles to have high quality task",
    "start": "3280000",
    "end": "3286640"
  },
  {
    "text": "descriptions it's too imprecise if you take a look at why so",
    "start": "3286640",
    "end": "3291920"
  },
  {
    "text": "here's an example this is the from the cup and the bad task these are the images uh ground truth images that",
    "start": "3291920",
    "end": "3298319"
  },
  {
    "text": "happen during execution um here's what our method which remember has to first predict the",
    "start": "3298319",
    "end": "3304559"
  },
  {
    "text": "latent state and then describe it these are the descriptions that our methods has okay it says something like the",
    "start": "3304559",
    "end": "3310559"
  },
  {
    "text": "robot sees the cup through the interior or by the middle if you ask the endto-end VLM model with the same",
    "start": "3310559",
    "end": "3316880"
  },
  {
    "text": "training data set it struggles to capture really accurate motion details it says things like oh it grabs the mug",
    "start": "3316880",
    "end": "3322640"
  },
  {
    "text": "by the handle or by the corner again maybe given more data it would work but assuming the same exact",
    "start": "3322640",
    "end": "3329119"
  },
  {
    "text": "training set you can get a much higher quality understanding of the world by first having the world model which is",
    "start": "3329119",
    "end": "3335280"
  },
  {
    "text": "good at embodied reasoning give you the predicted embodied outcomes and then the LLM for just",
    "start": "3335280",
    "end": "3342640"
  },
  {
    "text": "translation okay so we've ended the deep dive i have one minute for",
    "start": "3343559",
    "end": "3349799"
  },
  {
    "text": "questions which I'm excited to take i'm just going to quickly wrap up here this was there's much much more to be done",
    "start": "3349799",
    "end": "3356160"
  },
  {
    "text": "these are just the first steps towards how we can kind of systematically unite the things that we know and love underly",
    "start": "3356160",
    "end": "3362319"
  },
  {
    "text": "sequential decision-m i.e control with some of these modern models to scale us up into the open world i want to say a",
    "start": "3362319",
    "end": "3369119"
  },
  {
    "text": "really big thank you to my students and my funders without whom this would not be possible and with that I'm really",
    "start": "3369119",
    "end": "3374240"
  },
  {
    "text": "excited to take your questions thank you",
    "start": "3374240",
    "end": "3379240"
  },
  {
    "text": "i think what we should do is take a few questions now and then if there's more of course they can um accost",
    "start": "3384160",
    "end": "3391599"
  },
  {
    "text": "so questions I saw some questions earlier so we can return yes so when you",
    "start": "3391599",
    "end": "3397920"
  },
  {
    "text": "implement the LLM um like kind of prompt the different steps for the robot to do",
    "start": "3397920",
    "end": "3405839"
  },
  {
    "text": "is the LLM also getting the inputs from the videos that the robot has so our",
    "start": "3405839",
    "end": "3412480"
  },
  {
    "text": "approach only gets the latent state predictions we have baselines in the paper that get the decoded images",
    "start": "3412480",
    "end": "3419520"
  },
  {
    "text": "actually I'll just show you really quick this just a screenshot from the paper so VLM image and VLM image Oracle are both",
    "start": "3419520",
    "end": "3426880"
  },
  {
    "text": "models that take as input images actually they're both GPT40 so VLM image takes the decoded",
    "start": "3426880",
    "end": "3434559"
  },
  {
    "text": "future image predictions and pass them in GPT4 sorry that's VLM image oracle",
    "start": "3434559",
    "end": "3439760"
  },
  {
    "text": "yeah sorry vlm image is the decodings vlm image oracle is the ground truth future observations",
    "start": "3439760",
    "end": "3446920"
  },
  {
    "text": "and given the current models GPT40 um they're they're not strong enough to",
    "start": "3446920",
    "end": "3454400"
  },
  {
    "text": "really narrate the fine brain details of these embodied outcomes but",
    "start": "3454400",
    "end": "3460880"
  },
  {
    "text": "from your is on the safety filter did you train the safety filter only with this one",
    "start": "3460880",
    "end": "3468079"
  },
  {
    "text": "failure type of like spilling or did you also include other failure types in this",
    "start": "3468079",
    "end": "3473200"
  },
  {
    "text": "example the hardware demo it's only that one failure spec Yeah so you could imagine and it's",
    "start": "3473200",
    "end": "3478240"
  },
  {
    "text": "something we've thought about like combining the adaptability of this BLM style approach with the latent safety",
    "start": "3478240",
    "end": "3483760"
  },
  {
    "text": "filter approach to kind of adapt it i thought there was one more question earlier somewhere here yes um I have a",
    "start": "3483760",
    "end": "3491359"
  },
  {
    "text": "more philosophical question I guess so a lot of your work revolves around kind of",
    "start": "3491359",
    "end": "3497760"
  },
  {
    "text": "generalizing safety to open world like the specification of the constraint",
    "start": "3497760",
    "end": "3503839"
  },
  {
    "text": "right but we're still using very classical techniques like reachability or",
    "start": "3503839",
    "end": "3509280"
  },
  {
    "text": "definition of safety as like set based safety but I kind of had this idea when",
    "start": "3509280",
    "end": "3514400"
  },
  {
    "text": "this gentleman brought up um like the different definitions of safety like if you drop skills on the table this",
    "start": "3514400",
    "end": "3521520"
  },
  {
    "text": "gentleman is like okay he's like okay with that right but to you um that",
    "start": "3521520",
    "end": "3528480"
  },
  {
    "text": "wouldn't be okay so it seems to me like there should be some sort of",
    "start": "3528480",
    "end": "3534160"
  },
  {
    "text": "Um I guess safety is kind of fuzzy in the real world right so maybe is there a",
    "start": "3534160",
    "end": "3539280"
  },
  {
    "text": "way to generalize um kind of safety beyond just set based safety and maybe generalize these kind",
    "start": "3539280",
    "end": "3547280"
  },
  {
    "text": "of set based reachability tools yeah maybe I'll answer your question a few parts so part number one is actually",
    "start": "3547280",
    "end": "3553760"
  },
  {
    "text": "related to uh this person's question here which is that if you if you think a",
    "start": "3553760",
    "end": "3559680"
  },
  {
    "text": "setbased representation is correct but you want to align it with a particular end user like your two different",
    "start": "3559680",
    "end": "3564720"
  },
  {
    "text": "definitions of that set then what you really need to do is somehow like parameterize maybe your safety filter",
    "start": "3564720",
    "end": "3570720"
  },
  {
    "text": "based on something that lets you easily adapt based on the context so that's one way you can handle that you're asking",
    "start": "3570720",
    "end": "3576960"
  },
  {
    "text": "though another question which is like okay what does safety even mean like is it always a setbased representation is",
    "start": "3576960",
    "end": "3583040"
  },
  {
    "text": "it something else um and by the way for me personally like safety is very like it it doesn't have to be the set based",
    "start": "3583040",
    "end": "3588559"
  },
  {
    "text": "representation i'll tell you when I think it's a right it's the right thing to to use for me these hard constraint",
    "start": "3588559",
    "end": "3595280"
  },
  {
    "text": "definitions are appropriate when the spec represents something",
    "start": "3595280",
    "end": "3602200"
  },
  {
    "text": "irreversible that's why collisions is oftentimes something that we represent in the setbased model because once I hit",
    "start": "3602200",
    "end": "3608319"
  },
  {
    "text": "someone I can't undo that if I break a glass it's never going to be the same it's true skittles I can put back in the",
    "start": "3608319",
    "end": "3613760"
  },
  {
    "text": "bag we just have to start somewhere but I think that the most appropriate context in which you want this hard boundary are genuinely irreversible",
    "start": "3613760",
    "end": "3621480"
  },
  {
    "text": "things after that there's a lot of different flavors of safety you can go for for example in the AI community you",
    "start": "3621480",
    "end": "3627839"
  },
  {
    "text": "know alignment is the way main like mechanism they talk about and this is a much more like reward function based way",
    "start": "3627839",
    "end": "3633119"
  },
  {
    "text": "of thinking about it it's a soft model and it's more about like are you doing what I want and that's a totally legit",
    "start": "3633119",
    "end": "3639040"
  },
  {
    "text": "other way of safety you can also look at some of our early works that I showed you at the very beginning for how we can do alignment for some of these visual",
    "start": "3639040",
    "end": "3645359"
  },
  {
    "text": "motor policies yeah yeah thanks for totally amazing",
    "start": "3645359",
    "end": "3650640"
  },
  {
    "text": "talk blew my mind i loved it thank you Andre um so my question has to do with how much how much data did you have to",
    "start": "3650640",
    "end": "3657920"
  },
  {
    "text": "handle label with the latent codes with narrations in order to align the normal",
    "start": "3657920",
    "end": "3664240"
  },
  {
    "text": "model it's the volume of data we're talking about that's a good question okay I am not going to give you the",
    "start": "3664240",
    "end": "3670400"
  },
  {
    "text": "accurate number because I can't recall it exactly if you take a look at the paper I think it was a few hundred",
    "start": "3670400",
    "end": "3675680"
  },
  {
    "text": "trajectories yeah I think it was on the order of like 700 800 but please check",
    "start": "3675680",
    "end": "3681040"
  },
  {
    "text": "the paper for the details follow up on that how much kind of iteration file and error did it take to figure out what was",
    "start": "3681040",
    "end": "3687520"
  },
  {
    "text": "important to narrate in the human oh that's a good question that's a good question honestly",
    "start": "3687520",
    "end": "3694240"
  },
  {
    "text": "my PhD student is probably much more qualified to answer that than me because she's the expert there i think uh my",
    "start": "3694240",
    "end": "3700480"
  },
  {
    "text": "intuition is something like this um so she was the one who did the narrations we didn't use for example",
    "start": "3700480",
    "end": "3707040"
  },
  {
    "text": "like a video narration tool that you could imagine scaling this up to do if you're using a video narration tool",
    "start": "3707040",
    "end": "3712240"
  },
  {
    "text": "though you need to now ask this question how do you get the automated tool to narrate at the right granularity for her",
    "start": "3712240",
    "end": "3718000"
  },
  {
    "text": "she kind of knew that like what the level of granular granularity should be because she's like a task designer and",
    "start": "3718000",
    "end": "3723920"
  },
  {
    "text": "so she knew that for example grasp poses matter a lot um but I think that your",
    "start": "3723920",
    "end": "3729839"
  },
  {
    "text": "question is revealing a really interesting thing if you really want to scale this up and I actually don't know",
    "start": "3729839",
    "end": "3735520"
  },
  {
    "text": "how I would do it off the top of my head but maybe we should work together and figure it out i love that question",
    "start": "3735520",
    "end": "3740799"
  },
  {
    "text": "that's a good one yeah in the back can you tell me about",
    "start": "3740799",
    "end": "3746640"
  },
  {
    "text": "What might be the problem current models",
    "start": "3746640",
    "end": "3752440"
  },
  {
    "text": "like model what might be something that can be improved on there are so many",
    "start": "3752440",
    "end": "3758799"
  },
  {
    "text": "things okay uh I'll tell you top two things we've been thinking about and trying to fix one is hallucinations so",
    "start": "3758799",
    "end": "3767200"
  },
  {
    "text": "specifically here's a problem we have so remember the Dubens car example we had such good coverage of all observation",
    "start": "3767200",
    "end": "3773040"
  },
  {
    "text": "action pairs that's why we got that really high fidelity backwards reachable set in the highdimensional case like with Skittles we never have perfect",
    "start": "3773040",
    "end": "3779359"
  },
  {
    "text": "coverage of all observation action pairs this means that what happens is when you ask for simulation of an action that is",
    "start": "3779359",
    "end": "3786319"
  },
  {
    "text": "out of your distribution you can imagine things that are totally implausible physically and so I think that's one",
    "start": "3786319",
    "end": "3792480"
  },
  {
    "text": "place where uncertainty quantification plays a really really really key role you need to know when you are imagining",
    "start": "3792480",
    "end": "3797760"
  },
  {
    "text": "things that are out of distribution and don't make sense uh that's something we've been thinking about recently and",
    "start": "3797760",
    "end": "3803520"
  },
  {
    "text": "then another problem that we've also seen is that the way that these models are trained for example if you do",
    "start": "3803520",
    "end": "3809599"
  },
  {
    "text": "something like image reconstruction a lot of the times you're focusing on parts of the visual environment that",
    "start": "3809599",
    "end": "3814880"
  },
  {
    "text": "might not be relevant for the task you're worried a lot about backgrounds or maybe hyperfixating on certain distractors uh and what happens is that",
    "start": "3814880",
    "end": "3821920"
  },
  {
    "text": "at deployment time if you add a new object or you somehow change the scene you're not guaranteed that it's actually going to generalize uh and so there are",
    "start": "3821920",
    "end": "3827920"
  },
  {
    "text": "some other ways we have been thinking about how to use very like very general",
    "start": "3827920",
    "end": "3834160"
  },
  {
    "text": "paradigms for just train the base model to be good in general and then at runtime making sure that it's only",
    "start": "3834160",
    "end": "3839599"
  },
  {
    "text": "paying attention to things that matter those are the top two things that come to my mind",
    "start": "3839599",
    "end": "3845799"
  },
  {
    "text": "okay sounds good all right thanks so much thank you",
    "start": "3845920",
    "end": "3853318"
  }
]