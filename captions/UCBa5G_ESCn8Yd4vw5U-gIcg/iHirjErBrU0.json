[
  {
    "start": "0",
    "end": "18000"
  },
  {
    "text": "yeah so so we do want to address exactly that the question that was raised so one",
    "start": "10559",
    "end": "16830"
  },
  {
    "text": "way to think about it that there's kind of a major conceptual elephant in all of systems neuroscience",
    "start": "16830",
    "end": "22560"
  },
  {
    "start": "18000",
    "end": "132000"
  },
  {
    "text": "which is the following how is it that we can record order say a hundred to a thousand neurons in regions deep within",
    "start": "22560",
    "end": "29500"
  },
  {
    "text": "the brain far from the sensory motor periphery and actually obtained scientifically interpretable results",
    "start": "29500",
    "end": "34780"
  },
  {
    "text": "that relate neural activity to behavior and cognition this is actually quite remarkable considering that these brain",
    "start": "34780",
    "end": "41199"
  },
  {
    "text": "regions contain about ten to the six to ten to the ninth neurons so so about five orders of magnitude more neurons",
    "start": "41199",
    "end": "46989"
  },
  {
    "text": "than we than we currently record so a question is how has systems neuroscience been as successful as it has been in",
    "start": "46989",
    "end": "53589"
  },
  {
    "text": "such an under sample measurement regime or is it worse are we completely misleading ourselves by recording so few",
    "start": "53589",
    "end": "60580"
  },
  {
    "text": "neurons are we getting a dramatically different view of the brain than we might get if we recorded all the neurons",
    "start": "60580",
    "end": "66210"
  },
  {
    "text": "so that's what we'd like to address now let's you think that there's just an experimental solution to this problem while it is the case that recording",
    "start": "66210",
    "end": "72729"
  },
  {
    "text": "technologies enable us to record more and more neurons in fact the number of neurons we can simultaneously record has",
    "start": "72729",
    "end": "78400"
  },
  {
    "text": "been has been growing exponentially so this is a type of Moore's Law type of growth but the doubling time is about",
    "start": "78400",
    "end": "84340"
  },
  {
    "text": "seven years right so right now we can record a hundred to a thousand so it",
    "start": "84340",
    "end": "89890"
  },
  {
    "text": "would take about a hundred twenty years to get five orders of magnitude more neurons to to fully record entire neural",
    "start": "89890",
    "end": "96190"
  },
  {
    "text": "circuits and complex mammalian brains controlling complicated behaviors so",
    "start": "96190",
    "end": "101230"
  },
  {
    "text": "we're gonna be in this under sample measurement regime for the foreseeable future and so what we need is we need a",
    "start": "101230",
    "end": "107110"
  },
  {
    "text": "theory of neural data analysis that tells us how and when statistical analyses applied to a small subset of",
    "start": "107110",
    "end": "113230"
  },
  {
    "text": "neurons reflect the collective dynamics of a much larger unobserved circuit in",
    "start": "113230",
    "end": "118659"
  },
  {
    "text": "which they're embedded so that's the question we'd like to address okay so for the outline of the talk we'll start",
    "start": "118659",
    "end": "124630"
  },
  {
    "text": "with kind of trial average data how do we talk about what I mean by all this",
    "start": "124630",
    "end": "130179"
  },
  {
    "text": "stuff Traverse data how do we accurately recover the dimensionality and shape of trial average neural dynamics and we'll",
    "start": "130179",
    "end": "136569"
  },
  {
    "start": "132000",
    "end": "515000"
  },
  {
    "text": "uncover this mathematical quantity which we call neural task complexity that both determines both the dimensionality and",
    "start": "136569",
    "end": "142450"
  },
  {
    "text": "recoverability of neural states-based dynamics then we'll move on to single trial data analysis you know how well",
    "start": "142450",
    "end": "148840"
  },
  {
    "text": "can we decode information from the brain on a moment by basis at each instant of time and will",
    "start": "148840",
    "end": "154900"
  },
  {
    "text": "find these phase transitions than our ability to decode information and accurately fit neural networks as a",
    "start": "154900",
    "end": "160540"
  },
  {
    "text": "function of the number of recorded neurons and and recorded time and then at the end I'll discuss an algorithm",
    "start": "160540",
    "end": "165880"
  },
  {
    "text": "which we call tensor components analysis it's it's a method to perform multi",
    "start": "165880",
    "end": "171280"
  },
  {
    "text": "timescale neural dimension reduction so neural dynamics exists on multiple time scales right um the dynamics of hundreds",
    "start": "171280",
    "end": "177340"
  },
  {
    "text": "of milliseconds mediating cognition behavior action perceptions and much much longer time scale hours to days",
    "start": "177340",
    "end": "183970"
  },
  {
    "text": "that kind of dynamics mediates learning so this algorithm can kind of",
    "start": "183970",
    "end": "190299"
  },
  {
    "text": "simultaneously reduce dimensionality across neurons individual trials and time across trials to recover so called",
    "start": "190299",
    "end": "196810"
  },
  {
    "text": "cell assemblies with coherent rapid dynamics reflecting cognition and slower dynamics reflecting learning so that's",
    "start": "196810",
    "end": "202720"
  },
  {
    "text": "kind of the outline of the talk so let's jump in how many of you have experience with neuroscience before I'm guessing",
    "start": "202720",
    "end": "208599"
  },
  {
    "text": "not many of you right ok so small small fraction ok so again just interrupt me with questions like",
    "start": "208599",
    "end": "214870"
  },
  {
    "text": "you know whenever you want and it's pretty pretty informal all right",
    "start": "214870",
    "end": "220090"
  },
  {
    "text": "so here's an example of a neuroscience experiment that this is the type of experiment from which we'll be analysing",
    "start": "220090",
    "end": "226450"
  },
  {
    "text": "data right so this is data from Krishna noise lab here at Stanford so the tip",
    "start": "226450",
    "end": "232420"
  },
  {
    "text": "and he and he studies motor control so the types of experiments that he does is he watches monkeys move their arms and",
    "start": "232420",
    "end": "238060"
  },
  {
    "text": "he records from the motor cortex of the monkey and he tries to relate in your all activity to behavior so here's",
    "start": "238060",
    "end": "244269"
  },
  {
    "text": "here's an experiment where a monkey is holding a central target then a reach target appears ok and then the monkey is",
    "start": "244269",
    "end": "253060"
  },
  {
    "text": "supposed to plan a reach to that target so you can look at motor planning in these types of experiments but then the",
    "start": "253060",
    "end": "258639"
  },
  {
    "text": "central target disappears ok that's the signal that the monk you should start reaching and then the monkey performs a",
    "start": "258639",
    "end": "264820"
  },
  {
    "text": "reach ok what happens is neural activity during the planning period stays largely stationary over time it doesn't change",
    "start": "264820",
    "end": "271270"
  },
  {
    "text": "much over time but the pattern of activity is indicative of where the monkey is going to move right but all",
    "start": "271270",
    "end": "277180"
  },
  {
    "text": "the action is happening here within this about 600 millisecond window it takes about 600 milliseconds from this monkey",
    "start": "277180",
    "end": "283300"
  },
  {
    "text": "to do this rapid reach and there's a lot of dynamical neural activity patterns during that 600 milliseconds across all the neurons okay",
    "start": "283300",
    "end": "291090"
  },
  {
    "text": "so they're recording from hundreds of neurons using an electrode array inserted into motor cortex and a more a",
    "start": "291090",
    "end": "298560"
  },
  {
    "text": "slightly more upstream region dorsal premotor cortex okay they can they can",
    "start": "298560",
    "end": "306570"
  },
  {
    "text": "record about a hundred and nine well isolated single units or a single neurons you know by measuring the",
    "start": "306570",
    "end": "312360"
  },
  {
    "text": "extracellular voltage coming from these neurons and that signifies when these neurons spike write or emit action",
    "start": "312360",
    "end": "319470"
  },
  {
    "text": "potentials so here's some examples of individual neuron firing rates you can",
    "start": "319470",
    "end": "325020"
  },
  {
    "text": "take the spikes and smooth them with a small Gaussian window and get smooth firing rates and then you can average",
    "start": "325020",
    "end": "330810"
  },
  {
    "text": "these over trials and this is what you get for say one neuron right this is the",
    "start": "330810",
    "end": "336360"
  },
  {
    "text": "neurons activity pattern over time over the six hundred millisecond window across different reaches in different",
    "start": "336360",
    "end": "342510"
  },
  {
    "text": "directions and a different reach and different reach speeds and extents okay",
    "start": "342510",
    "end": "348150"
  },
  {
    "text": "that's just one neuron here's another neuron it looks sort of completely different here's a third neuron it looks",
    "start": "348150",
    "end": "353970"
  },
  {
    "text": "it looks extremely different as well and there's about a hundred more neurons like this so these datasets are somewhat",
    "start": "353970",
    "end": "360120"
  },
  {
    "text": "complicated right there's a lot of complexity in this neural dynamics seemingly and so the question is how are",
    "start": "360120",
    "end": "365250"
  },
  {
    "text": "such datasets analyzed so just a very simple approach that a lot of neuroscientists take is you measure the",
    "start": "365250",
    "end": "372570"
  },
  {
    "text": "simultaneous dynamics of about n neurons during a task and then you find just say",
    "start": "372570",
    "end": "378120"
  },
  {
    "text": "for example using principal components analysis that the dynamics lives in quite a low dimensional space in the in",
    "start": "378120",
    "end": "383430"
  },
  {
    "text": "the ambience a hundred dimensional space of recorded neurons so remember in principal components analysis you take",
    "start": "383430",
    "end": "389040"
  },
  {
    "text": "every neural activity pattern across neurons that occurs in any instant of time and you decompose it into a set of",
    "start": "389040",
    "end": "395250"
  },
  {
    "text": "basis activity patterns and you decompose it using a time dependent set of waiting coefficients that's just PCA",
    "start": "395250",
    "end": "401820"
  },
  {
    "text": "and so you can visualize the dynamics in this low dimensional state space by the amplitude of each pattern present and",
    "start": "401820",
    "end": "409860"
  },
  {
    "text": "then you get this neural trajectory and what a lot of works have found is that you can explain a lot of the variants in",
    "start": "409860",
    "end": "416490"
  },
  {
    "text": "neural data using a relatively small dimensions in this case about 10 dimensions despite the fact that you",
    "start": "416490",
    "end": "421590"
  },
  {
    "text": "recorded 100 neurons so there's lots of",
    "start": "421590",
    "end": "426780"
  },
  {
    "text": "work in neuroscience that recovers this kind of state space dynamics and interesting the state space dynamics yields a remarkably insightful kind of",
    "start": "426780",
    "end": "435300"
  },
  {
    "text": "dynamical portrait of what computation the circuit is doing so for example these are the state space dynamics of a",
    "start": "435300",
    "end": "440850"
  },
  {
    "text": "locust antenna lobe when it's exposed to two different odors and as you can see the locust antenna lobe the transient",
    "start": "440850",
    "end": "447210"
  },
  {
    "text": "dynamics of the system contains a lot of information about the odor long before the dynamics reaches as a respective",
    "start": "447210",
    "end": "453630"
  },
  {
    "text": "fixed point which is odor dependent so it's processing information on the transients this is data for a monkey",
    "start": "453630",
    "end": "460080"
  },
  {
    "text": "prefrontal cortex where the monkeys trained to detect different frequencies",
    "start": "460080",
    "end": "465449"
  },
  {
    "text": "on its arm and you see that most of the variants in the neural data accounts for the passage of time in the task rather",
    "start": "465449",
    "end": "472440"
  },
  {
    "text": "than the different frequencies which is sort of interesting finding this is this is data for a monkey prefrontal cortex",
    "start": "472440",
    "end": "478800"
  },
  {
    "text": "during a context dependent decision-making task and it turned out that in this work done actually by",
    "start": "478800",
    "end": "486300"
  },
  {
    "text": "Christian Roy and Bill Newsome doesn't done by their labs the shape of these neural trajectories revealed an",
    "start": "486300",
    "end": "492570"
  },
  {
    "text": "algorithm for how the monkey is making decisions and also you can record from about all 80,000 neurons in the",
    "start": "492570",
    "end": "498360"
  },
  {
    "text": "zebrafish using calcium imaging and you can fit that entire dataset in about three dimensions and you're okay you",
    "start": "498360",
    "end": "503430"
  },
  {
    "text": "don't lose much variance so anyways this is kind of a modus operandi for competition on euro science with these",
    "start": "503430",
    "end": "508710"
  },
  {
    "text": "large data sets you recover the state space dynamics and it's usually often quite insightful and quite simple okay",
    "start": "508710",
    "end": "515490"
  },
  {
    "start": "515000",
    "end": "558000"
  },
  {
    "text": "but this raises some fundamental conceptual questions again can we trust these dynamical portraits of circuit",
    "start": "515490",
    "end": "521430"
  },
  {
    "text": "competition despite recording so few neurons how would the shape of these portraits change if we recorded more",
    "start": "521430",
    "end": "527430"
  },
  {
    "text": "neurons did we do we even get their shape right we make scientific inferences based on their shape did we get the shape right why is there",
    "start": "527430",
    "end": "533910"
  },
  {
    "text": "dimensionality so low what is the origin of the simplicity would it increase if we recorded more neurons and more",
    "start": "533910",
    "end": "540180"
  },
  {
    "text": "generally what if anything at all can we learn about a large about large dynamical networks it's such an overwhelming level of under sampling and",
    "start": "540180",
    "end": "546959"
  },
  {
    "text": "so basically can we obtain a predictive theory of experimental design that can you know tell us",
    "start": "546959",
    "end": "552750"
  },
  {
    "text": "many neurons we should record as a function of the complexity of the task or something like that so I don't have",
    "start": "552750",
    "end": "562770"
  },
  {
    "start": "558000",
    "end": "648000"
  },
  {
    "text": "the picture so I should say this was work done by a grad student in my lab app a Ron Gow ok so let me just give you",
    "start": "562770",
    "end": "569400"
  },
  {
    "text": "the take-home message of the first third of the talk which is basically we define this new mathematical quantity called",
    "start": "569400",
    "end": "575880"
  },
  {
    "text": "neural task complexity it takes into account two distinct sources of information the sort of complexity of",
    "start": "575880",
    "end": "581700"
  },
  {
    "text": "the task the monkey or the animal is asked to do and the smoothness of neural dynamics and we prove a theorem saying",
    "start": "581700",
    "end": "587490"
  },
  {
    "text": "that the dimensionality of neural data is upper bounded by this neural task complexity then we show that motor",
    "start": "587490",
    "end": "593940"
  },
  {
    "text": "cortical data is actually as high dimensional as possible given this neural task complexity which implies that in future experiments if we record",
    "start": "593940",
    "end": "600630"
  },
  {
    "text": "more neurons without an increase in the complexity of the task we won't necessarily get richer datasets as",
    "start": "600630",
    "end": "606090"
  },
  {
    "text": "measured at least by their dimensionality and moreover we can recover conditions for accurate recovery",
    "start": "606090",
    "end": "611760"
  },
  {
    "text": "of dynamic portraits we can show that the act of neural measurement is like doing a random projection as curious how",
    "start": "611760",
    "end": "617280"
  },
  {
    "text": "many of you are familiar with random projections this is the SI s Department so again a surprisingly small fraction",
    "start": "617280",
    "end": "624480"
  },
  {
    "text": "ok ok what we'll go over random projection theory but it's a very beautiful theory we can show that what",
    "start": "624480",
    "end": "631250"
  },
  {
    "text": "neurophysiologists have been doing for the last 50 years is essentially doing a random projection which is kind of an",
    "start": "631250",
    "end": "636420"
  },
  {
    "text": "interesting connection and then based on this we can show that past results ie",
    "start": "636420",
    "end": "642930"
  },
  {
    "text": "existing dynamic portraits are likely to be accurate despite recording so few neurons ok ok so what is this no I don't",
    "start": "642930",
    "end": "653070"
  },
  {
    "start": "648000",
    "end": "1264000"
  },
  {
    "text": "have the ok it's ok well so what is this neural task complexity so we can think",
    "start": "653070",
    "end": "660630"
  },
  {
    "text": "about the manifold of neural States that are that are explored by the experiment",
    "start": "660630",
    "end": "666450"
  },
  {
    "text": "as an embedding of the behavioral manifold into neural activity space right and the basic idea is here's a",
    "start": "666450",
    "end": "673589"
  },
  {
    "text": "monkey reaching to eight directions ok and you have some reach extend to reach",
    "start": "673589",
    "end": "678780"
  },
  {
    "text": "time ok and you trial average all of this so the manifold of behavioral States explored by the monkey is roughly",
    "start": "678780",
    "end": "685050"
  },
  {
    "text": "a cylinder ramit rise by the angle of the reach and the egg time into the reach okay or",
    "start": "685050",
    "end": "691620"
  },
  {
    "text": "duration of the reach okay now this trial average this manifold in under the",
    "start": "691620",
    "end": "696840"
  },
  {
    "text": "trial average neural dynamics gets embedded into some kind of complicated cylinder that we don't really understand fully okay but we can characterize the",
    "start": "696840",
    "end": "705990"
  },
  {
    "text": "complexity of this of this cylinder by two things roughly the volume of the of",
    "start": "705990",
    "end": "714450"
  },
  {
    "text": "the behavioral manifold which is the numerator of this of this quantity so so",
    "start": "714450",
    "end": "719910"
  },
  {
    "text": "let's say you have a general manifold that has K intrinsic coordinates right",
    "start": "719910",
    "end": "725820"
  },
  {
    "text": "here it's to intrinsic coordinates in this example a one egg one is theta sorry one is 2pi or the solid angle and",
    "start": "725820",
    "end": "733080"
  },
  {
    "text": "one is the duration the length of the reach so this has a certain volume but",
    "start": "733080",
    "end": "738840"
  },
  {
    "text": "that gets embedded into this into the space and there's some volume of a unit",
    "start": "738840",
    "end": "744750"
  },
  {
    "text": "cell of how much you move before things curve it's sort of like an autocorrelation length so let's say that",
    "start": "744750",
    "end": "750780"
  },
  {
    "text": "I moved in this direction or let's say that this this section or cell of the",
    "start": "750780",
    "end": "756750"
  },
  {
    "text": "manifold roughly got matched to this section or cell of the manifold where there wasn't much change in the in the",
    "start": "756750",
    "end": "763170"
  },
  {
    "text": "curvature then this entire section the manifold roughly explores only one",
    "start": "763170",
    "end": "768540"
  },
  {
    "text": "dimension in firing rate space but then let's say you have to move a certain distance along each dimension of this",
    "start": "768540",
    "end": "775500"
  },
  {
    "text": "manifold to get to another another cell that explores a different dimension right so then roughly the total number",
    "start": "775500",
    "end": "783120"
  },
  {
    "text": "of dimensions that could possibly be explored is the volume of the behavioral manifold divided by the volume of the",
    "start": "783120",
    "end": "790290"
  },
  {
    "text": "unit cell over which the manual the manifold doesn't really curve that much okay",
    "start": "790290",
    "end": "795930"
  },
  {
    "text": "so just intuitively this is what's the measure of neural task complexity our",
    "start": "795930",
    "end": "802110"
  },
  {
    "text": "theory gives us a way to extract these autocorrelation lengths and of course the experimentalist determines the",
    "start": "802110",
    "end": "809190"
  },
  {
    "text": "volume of the behavioral manifold that's explored and so this ratio is the neural task complexity and there's a constant",
    "start": "809190",
    "end": "814800"
  },
  {
    "text": "involved that's also given by the theory right so roughly you can prove a theorem that",
    "start": "814800",
    "end": "821670"
  },
  {
    "text": "says that the neural dimensionality is upper bounded by the minimum of this neural task complexity measure and the",
    "start": "821670",
    "end": "827370"
  },
  {
    "text": "number of recorded neurons so at least on a high level this should be overall intuitive a manifold that has larger",
    "start": "827370",
    "end": "833520"
  },
  {
    "text": "volume ie bigger numerator and is more Wiggly ie smaller denominator can",
    "start": "833520",
    "end": "840570"
  },
  {
    "text": "potentially explore more dimensions okay all right so now let's go back to the",
    "start": "840570",
    "end": "849209"
  },
  {
    "text": "neural data in primate motor cortex again there are about a hundred million neurons controlling 650 skeletal muscles",
    "start": "849209",
    "end": "855410"
  },
  {
    "text": "and in these experiments recorded about a hundred neurons the pca dimensionality",
    "start": "855410",
    "end": "861390"
  },
  {
    "text": "which is roughly the number of principal components you need to explain 70 percent of the variance across all eight",
    "start": "861390",
    "end": "868529"
  },
  {
    "text": "reaches is about seven the dimensionality to explain just one reach",
    "start": "868529",
    "end": "873630"
  },
  {
    "text": "the the the variance in your activity across one reach is about three point three there's a particular measure of",
    "start": "873630",
    "end": "881459"
  },
  {
    "text": "dimensionality that we're using called it's something called the participation ratio of the eigen value spectrum of the",
    "start": "881459",
    "end": "887490"
  },
  {
    "text": "neuron by neuron covariance matrix which roughly measures how many principal components are active okay and a certain",
    "start": "887490",
    "end": "896640"
  },
  {
    "text": "participation ratio for a wide variety of spectra corresponds to how many PCs you need to keep 70% of variance",
    "start": "896640",
    "end": "902880"
  },
  {
    "text": "explained it turns out that 70% is not an important number we could multiply the participation ratio by a constant",
    "start": "902880",
    "end": "909330"
  },
  {
    "text": "fraction and we could get up to 90% it would be the same exact theory okay so",
    "start": "909330",
    "end": "916130"
  },
  {
    "text": "so then again the the puzzle is these numbers are much smaller than the total",
    "start": "916130",
    "end": "921150"
  },
  {
    "text": "number of neurons and again why is that what's limiting the complexity have we found something intrinsic about brain",
    "start": "921150",
    "end": "926579"
  },
  {
    "text": "dynamics that makes it simple or is the simplicity of brain dynamics a consequence of the simplicity that's",
    "start": "926579",
    "end": "931800"
  },
  {
    "text": "asked the monkeys asked to perform okay so it turns out the latter is true if",
    "start": "931800",
    "end": "937140"
  },
  {
    "text": "you compute the task complexity of the neural task complexity it's about 4.2 for a single region 10 for multiple",
    "start": "937140",
    "end": "944070"
  },
  {
    "text": "regions ok so as you can see the neural dimensionality is close to this upper bound ok so this suggests",
    "start": "944070",
    "end": "951210"
  },
  {
    "text": "so the implication is the neural dimensionality is not necessarily small but it's almost as large as possible",
    "start": "951210",
    "end": "956370"
  },
  {
    "text": "given the simplicity of the task and this makes certain predictions if you",
    "start": "956370",
    "end": "961950"
  },
  {
    "text": "vary the complexity of the task by varying the duration of the reach or equivalently the amount of time of the",
    "start": "961950",
    "end": "969029"
  },
  {
    "text": "reach that you analyze then if indeed the dimensionality of the data is",
    "start": "969029",
    "end": "974310"
  },
  {
    "text": "limited by the complexity of the task then the dimensionality should vary linearly with that time that you keep",
    "start": "974310",
    "end": "979500"
  },
  {
    "text": "okay that's prediction number one prediction number two the number of neurons in the data set is kind of",
    "start": "979500",
    "end": "986700"
  },
  {
    "text": "irrelevant in this case if you vary the number of neurons in the data set the dimensionality should roughly be",
    "start": "986700",
    "end": "992640"
  },
  {
    "text": "unchanged right because it's the complexity of the task that's eliminating the dimensionality so it doesn't really matter that you measured",
    "start": "992640",
    "end": "998040"
  },
  {
    "text": "100 neurons if you reduce the neuron count you shouldn't change the dimensionality so prediction number 1 is",
    "start": "998040",
    "end": "1004400"
  },
  {
    "text": "verified right as you vary the task complexity by varying the duration of the research you analyze you find that",
    "start": "1004400",
    "end": "1010730"
  },
  {
    "text": "the dimensionality increases with the reach duration it hugs this kind of dimensionality upper bounds set by the",
    "start": "1010730",
    "end": "1016339"
  },
  {
    "text": "complexity of the task and - as you drop even 60% of the neurons the",
    "start": "1016339",
    "end": "1022310"
  },
  {
    "text": "dimensionality remains largely unchanged ok so ok oops ok so that's the the most",
    "start": "1022310",
    "end": "1031850"
  },
  {
    "text": "boring part of the talk the next most interesting part is what about the shape of these trajectories dimensionality is",
    "start": "1031850",
    "end": "1037160"
  },
  {
    "text": "one thing but did we get the entire shape of these trajectories correct ok so what just to introduce the ideas what",
    "start": "1037160",
    "end": "1044660"
  },
  {
    "text": "is an intuitive situation where you could record a subset of neurons in a circuit but still get its state space",
    "start": "1044660",
    "end": "1050929"
  },
  {
    "text": "dynamics correct so here's an example here's the simplest visualizable example",
    "start": "1050929",
    "end": "1056840"
  },
  {
    "text": "where you have 3 neurons so this is a firing rate of neuron one firing rate of neuron to firing rate of neuron 3",
    "start": "1056840",
    "end": "1062120"
  },
  {
    "text": "for whatever reason let's say neural activity patterns that are explored during during this this experiment live",
    "start": "1062120",
    "end": "1069620"
  },
  {
    "text": "on a one-dimensional manifold and it's curved but not too curved ok and it's roughly randomly oriented with respect",
    "start": "1069620",
    "end": "1076190"
  },
  {
    "text": "to the single neuron axes ok so then it's kind of clear that even if you don't measure all three neurons you",
    "start": "1076190",
    "end": "1082790"
  },
  {
    "text": "only let's say you only get to measure two of the neuron you see a shadow of the full state space",
    "start": "1082790",
    "end": "1088340"
  },
  {
    "text": "dynamics of the brain on to the coordinate subspace spanned by the two neurons that you do get to measure and",
    "start": "1088340",
    "end": "1093890"
  },
  {
    "text": "you can see that it doesn't matter which two neurons you measure you get the same answer and that answer is roughly",
    "start": "1093890",
    "end": "1101210"
  },
  {
    "text": "similar to what you get if you recorded all the neurons okay of course life doesn't need to be that rosy it could",
    "start": "1101210",
    "end": "1108440"
  },
  {
    "text": "have been the case that for example neuron 2 were doing all the work so that this manifold is access aligned to the",
    "start": "1108440",
    "end": "1114320"
  },
  {
    "text": "single neuron axes and then of course if you only get to record a subset of neurons that subset better including",
    "start": "1114320",
    "end": "1120679"
  },
  {
    "text": "your on to right so this is kind of a pessimistic picture okay this picture",
    "start": "1120679",
    "end": "1126710"
  },
  {
    "text": "will arise if and ural activity patterns are distributed across neurons okay okay",
    "start": "1126710",
    "end": "1134750"
  },
  {
    "text": "so let's actually hope for the best and hope that the brain is in this regime",
    "start": "1134750",
    "end": "1141860"
  },
  {
    "text": "and not in some other regime not in this regime okay then let's pursue the",
    "start": "1141860",
    "end": "1148280"
  },
  {
    "text": "theoretical consequences of the brain being in that regime and derive some theoretical consequences then test those",
    "start": "1148280",
    "end": "1154549"
  },
  {
    "text": "consequences in the actual brain to see if this regime is a good enough approximation okay so so this is a very",
    "start": "1154549",
    "end": "1162260"
  },
  {
    "text": "simple picture to analyze but how do we make it quantitative and think about it in much higher dimensions where the",
    "start": "1162260",
    "end": "1168260"
  },
  {
    "text": "number of recorded neurons is a hundred to a thousand the dimensionality of this manifold may be higher than one and so",
    "start": "1168260",
    "end": "1174260"
  },
  {
    "text": "on and so forth right so how do we generalize that well let's say that this neural manifold",
    "start": "1174260",
    "end": "1180200"
  },
  {
    "text": "is indeed randomly oriented with respect to the single neuron axes that's a statement that can be made precise but",
    "start": "1180200",
    "end": "1186350"
  },
  {
    "text": "we'll just keep it intuitive for now then an experiment that we can do ie",
    "start": "1186350",
    "end": "1191480"
  },
  {
    "text": "measure a random subset of M neurons becomes equivalent to an experiment we",
    "start": "1191480",
    "end": "1196790"
  },
  {
    "text": "cannot yet do which is measure and random linear combinations of all",
    "start": "1196790",
    "end": "1202040"
  },
  {
    "text": "neurons so imagine you have a magical electrode that measures a random weighted sum of all the neurons another",
    "start": "1202040",
    "end": "1208669"
  },
  {
    "text": "magical electrode that measures another random weighted sum of all the neurons and you have M of these electrodes right",
    "start": "1208669",
    "end": "1214900"
  },
  {
    "text": "okay so why are these two experiments completely equivalent to each other if the neural manifold is",
    "start": "1214900",
    "end": "1220400"
  },
  {
    "text": "randomly oriented well this experiment corresponds to projecting the neural",
    "start": "1220400",
    "end": "1225740"
  },
  {
    "text": "state space dynamics onto a coordinate subspace whereas this experiment corresponds to",
    "start": "1225740",
    "end": "1231890"
  },
  {
    "text": "projecting this neural dynamics onto a randomly oriented subspace ie it's a random projection so if the neural",
    "start": "1231890",
    "end": "1240680"
  },
  {
    "text": "manifold is randomly oriented with respect to the coordinate subspaces to begin with then projecting under a",
    "start": "1240680",
    "end": "1246230"
  },
  {
    "text": "coordinate subspace and projecting onto a random subspace are equivalent to each other right so if this assumption is",
    "start": "1246230",
    "end": "1252680"
  },
  {
    "text": "true then the act of neural measurement is equivalent to the act of random projection okay",
    "start": "1252680",
    "end": "1259100"
  },
  {
    "text": "so let's assume that's true for now okay and let's derive consequences so now",
    "start": "1259100",
    "end": "1264560"
  },
  {
    "start": "1264000",
    "end": "1692000"
  },
  {
    "text": "let's review the theory of random projections it's really a beautiful theory that underlies compressed sensing and and a lot of ideas in signal",
    "start": "1264560",
    "end": "1272480"
  },
  {
    "text": "recovery in an under sampled measurement regime in computer science and EE and signal processing so what happens often",
    "start": "1272480",
    "end": "1279830"
  },
  {
    "text": "is of course signals that were interested in live in a high ambient dimensional space say images the ambient",
    "start": "1279830",
    "end": "1286550"
  },
  {
    "text": "dimensionality is the number of pixels for grayscale images but of course the",
    "start": "1286550",
    "end": "1291620"
  },
  {
    "text": "signals of interest don't explore this full high dimensional space they live on a much lower dimensional space for",
    "start": "1291620",
    "end": "1298160"
  },
  {
    "text": "example sparse signals live in a union of coordinate sub sub spaces you could",
    "start": "1298160",
    "end": "1303260"
  },
  {
    "text": "have a finite number of points or you could have a smooth manifold okay now let's say that you randomly project this",
    "start": "1303260",
    "end": "1310490"
  },
  {
    "text": "space down to a lower dimensional space using a random projection matrix so",
    "start": "1310490",
    "end": "1315830"
  },
  {
    "text": "you're projecting n-dimensional space down to M dimensional space so a is an N by M matrix s as a signal of interest in",
    "start": "1315830",
    "end": "1323690"
  },
  {
    "text": "our context is the set of neural like neural firing rates of all the neurons in the brain region and X is a set of",
    "start": "1323690",
    "end": "1330260"
  },
  {
    "text": "neural firing rates of of M recorded neurons okay again the neural activity",
    "start": "1330260",
    "end": "1336290"
  },
  {
    "text": "patterns that we observe live in some locational sub manifold in n dimensional space and a critical question is when",
    "start": "1336290",
    "end": "1342560"
  },
  {
    "text": "will the geometry of this manifold be preserved under a random projection by the preservation of geometry we mean the",
    "start": "1342560",
    "end": "1348350"
  },
  {
    "text": "following let's consider two points in the high dimension space and consider the distance between",
    "start": "1348350",
    "end": "1354950"
  },
  {
    "text": "them measured under a Euclidean metric in the high dimensional space so that's sa minus SB for example then you project",
    "start": "1354950",
    "end": "1362330"
  },
  {
    "text": "it down to the low dimensional space and up to a normalization you measure the distance between the two points in the",
    "start": "1362330",
    "end": "1367610"
  },
  {
    "text": "low dimensional space so this is the fractional error in distances measured in the high dimensional space relative",
    "start": "1367610",
    "end": "1374570"
  },
  {
    "text": "to the low dimensional space okay now this is for two points of course you don't want to just preserve the",
    "start": "1374570",
    "end": "1380030"
  },
  {
    "text": "distances between two points you want to preserve distances between all possible pairs of points but not in the full",
    "start": "1380030",
    "end": "1385760"
  },
  {
    "text": "space only on the manifold that you care about okay so we can define the distortion as the max over all pairs of",
    "start": "1385760",
    "end": "1394520"
  },
  {
    "text": "points of this da be measure let's call that epsilon so epsilon is the worst case distortion over all pairs of points",
    "start": "1394520",
    "end": "1400970"
  },
  {
    "text": "okay epsilon is a random variable because your projection is a random",
    "start": "1400970",
    "end": "1406160"
  },
  {
    "text": "matrix okay or a random subspace okay so",
    "start": "1406160",
    "end": "1411470"
  },
  {
    "text": "there are theorems about the behavior of epsilon and its scaling with the number of dimensions and with n K and M",
    "start": "1411470",
    "end": "1418280"
  },
  {
    "text": "basically okay this is the most important theorem that will will invoke this is work by Richard Barone you can",
    "start": "1418280",
    "end": "1425720"
  },
  {
    "text": "Michael waken actually these are old slides we have some work on some work that sharpens these results and gets",
    "start": "1425720",
    "end": "1432080"
  },
  {
    "text": "better bounds it's on our website called random projections of random manifolds but basically the theorem is the",
    "start": "1432080",
    "end": "1439940"
  },
  {
    "text": "following as long and I'm gonna use neuroscience language here okay as long",
    "start": "1439940",
    "end": "1446060"
  },
  {
    "text": "as the number of recorded neurons M or the dimensionality of the subspace is bigger than up to an order of magnitude",
    "start": "1446060",
    "end": "1452710"
  },
  {
    "text": "one over epsilon squared times K the intrinsic dimensionality this manifold",
    "start": "1452710",
    "end": "1458060"
  },
  {
    "text": "times the logarithm of the curvature times the volume of the manifold okay then the worst case distortion is indeed",
    "start": "1458060",
    "end": "1467210"
  },
  {
    "text": "order epsilon with very very high probability over the random choice of projection okay so this is interesting",
    "start": "1467210",
    "end": "1474200"
  },
  {
    "text": "the ambient dimensionality in our context the total number of neurons in the brain region doesn't matter at all",
    "start": "1474200",
    "end": "1479570"
  },
  {
    "text": "what matters is the intrinsic dimensionality of your model manifold and it's curvature and volume",
    "start": "1479570",
    "end": "1485880"
  },
  {
    "text": "okay and of course the distortion level that you'd like to get right okay now this actually makes a striking",
    "start": "1485880",
    "end": "1493830"
  },
  {
    "text": "prediction okay that we can now test a neural data part of the business of a theoretical neuroscientist is taking",
    "start": "1493830",
    "end": "1499770"
  },
  {
    "text": "theory and coming up with a way to test it in neural data so this is the",
    "start": "1499770",
    "end": "1504870"
  },
  {
    "text": "prediction okay if you convert this random projection theory into",
    "start": "1504870",
    "end": "1510720"
  },
  {
    "text": "neuroscience language it's saying the number of neurons you need to get a certain distortion is proportional to",
    "start": "1510720",
    "end": "1517230"
  },
  {
    "text": "the logarithm of talent of this neural task complexity remember neural task complexity was the volume of the",
    "start": "1517230",
    "end": "1522419"
  },
  {
    "text": "manifold in the numerator times the curvature or one over the volume of an autocorrelation cell so actually this",
    "start": "1522419",
    "end": "1529440"
  },
  {
    "text": "log of C times volume mathematically becomes the logarithm of the same neural",
    "start": "1529440",
    "end": "1535110"
  },
  {
    "text": "task complexity measure that upper bounded dimensionality okay so what this is saying is it's saying that if you",
    "start": "1535110",
    "end": "1543030"
  },
  {
    "text": "were to reduce the number of neurons or change the complexity of the task it's saying that the ISO contours of constant",
    "start": "1543030",
    "end": "1549470"
  },
  {
    "text": "distortion form a straight line in a plane spanned by the number of neurons",
    "start": "1549470",
    "end": "1555000"
  },
  {
    "text": "you record and the logarithm of the complexity of the task in our case where",
    "start": "1555000",
    "end": "1560159"
  },
  {
    "text": "our task is just reaches along a single dimension or reaches along a single axis the complexity of task is the duration",
    "start": "1560159",
    "end": "1567480"
  },
  {
    "text": "of the reach okay so it's the logarithm of time now of course we'd like to record all of the neurons in in the",
    "start": "1567480",
    "end": "1574650"
  },
  {
    "text": "motor cortex under the most complicated behavior possible but we don't have that we have a data set with a finitely",
    "start": "1574650",
    "end": "1580350"
  },
  {
    "text": "complex task and we have about 109 neurons so what we can do is we can",
    "start": "1580350",
    "end": "1585990"
  },
  {
    "text": "again vary the complexity of the task by varying the amount of data that we analyze and we can sub sample neurons",
    "start": "1585990",
    "end": "1592320"
  },
  {
    "text": "and we can recover the state space dynamics of the circuit using a smaller number of neurons for any given task",
    "start": "1592320",
    "end": "1599730"
  },
  {
    "text": "complexity and compare its distortion relative to that which you would get if",
    "start": "1599730",
    "end": "1604799"
  },
  {
    "text": "you recorded all if you took all the neurons that you recorded okay so you will get some distortion it'll vary as a",
    "start": "1604799",
    "end": "1611220"
  },
  {
    "text": "function but what this is saying is on this weird plane that's linear in the number of neurons that you keep but",
    "start": "1611220",
    "end": "1616980"
  },
  {
    "text": "logarithmic in time if you vary along this axis along a",
    "start": "1616980",
    "end": "1622140"
  },
  {
    "text": "straight line you'll get the same distortion and let's say you allow yourself to be more generous for a given",
    "start": "1622140",
    "end": "1627480"
  },
  {
    "text": "task complexity you allow more neurons you'll get a lower Distortion but again there'll be a different ISO contour of",
    "start": "1627480",
    "end": "1633360"
  },
  {
    "text": "distortion that's again a straight line okay so this makes now a testable prediction we can actually do this in",
    "start": "1633360",
    "end": "1639810"
  },
  {
    "text": "neural data from the monkey's brain and actually plot these ISO contours of",
    "start": "1639810",
    "end": "1644850"
  },
  {
    "text": "constant distortion and what you find is that the prediction is strikingly verified so this is again the the number",
    "start": "1644850",
    "end": "1652020"
  },
  {
    "text": "of recorded neurons that we keep this is the neural task complexity this is a logarithmic axis this is a linear axis",
    "start": "1652020",
    "end": "1658470"
  },
  {
    "text": "this is the distortion level and the ISO contours are indeed straight lines now remember this isn't just an empty",
    "start": "1658470",
    "end": "1664590"
  },
  {
    "text": "mathematical test of a theory for which we already have a theorem ie ran two projection Theory this is a test of the",
    "start": "1664590",
    "end": "1670560"
  },
  {
    "text": "very assumption that random projection Theory constitutes a good model for the process of neural measurement okay and",
    "start": "1670560",
    "end": "1677400"
  },
  {
    "text": "it turns out it does seem to constitute a good model for that process at least as measured by this prediction or as",
    "start": "1677400",
    "end": "1684090"
  },
  {
    "text": "assessed by this prediction okay all right so that's the first third of the talk",
    "start": "1684090",
    "end": "1692870"
  },
  {
    "start": "1692000",
    "end": "2065000"
  },
  {
    "text": "again let me just summarize so basically whenever you look at the dimensionality",
    "start": "1692870",
    "end": "1698100"
  },
  {
    "text": "of neural dynamics in the brain to ask if it's complicated or not what you",
    "start": "1698100",
    "end": "1703140"
  },
  {
    "text": "really want to do is compare the dimensionality to the neural task complexity if it's as high as that then",
    "start": "1703140",
    "end": "1709440"
  },
  {
    "text": "your dimensionality is constrained by the complexity of the task and the smoothness and real data only if it's much less than that have you discovered",
    "start": "1709440",
    "end": "1715320"
  },
  {
    "text": "additional intrinsic dynamic constraints in the data beyond task complexity and smoothness and also this this random",
    "start": "1715320",
    "end": "1722820"
  },
  {
    "text": "projection Theory actually yields a surprisingly optimistic message for Neuroscience is that in many many cases",
    "start": "1722820",
    "end": "1729600"
  },
  {
    "text": "because the task isn't that complicated the neural manifold doesn't have that high volume is not that curved we can",
    "start": "1729600",
    "end": "1735720"
  },
  {
    "text": "get away with recording a very small number of neurons okay so that's really",
    "start": "1735720",
    "end": "1741480"
  },
  {
    "text": "good news actually for Neuroscience and as we start doing more and more complicated tasks it turns out that the",
    "start": "1741480",
    "end": "1749850"
  },
  {
    "text": "number of neurons we simultaneously record is not really the bottleneck it's exploring the manifold that becomes",
    "start": "1749850",
    "end": "1755740"
  },
  {
    "text": "the bottleneck and this actually impacts the design of technology in neuroscience",
    "start": "1755740",
    "end": "1760840"
  },
  {
    "text": "you would much rather have technologies that allow you to record neurons for long periods of time as you go to a more",
    "start": "1760840",
    "end": "1767350"
  },
  {
    "text": "complex task because the time it takes to explore a manifold grows exponentially with the dimensionality",
    "start": "1767350",
    "end": "1772510"
  },
  {
    "text": "intrinsic dimensionality is manifold but the number of neurons you need to record grows linearly okay so considerations like this can",
    "start": "1772510",
    "end": "1779800"
  },
  {
    "text": "drive can drive the design of technology as well in your sons ok so let's move on",
    "start": "1779800",
    "end": "1788620"
  },
  {
    "text": "to the second part of the talk would ya sure that full international space oh",
    "start": "1788620",
    "end": "1800620"
  },
  {
    "text": "yeah so we do it in the hundred and nine dimensional space compared to like M dimensional space where m is less than",
    "start": "1800620",
    "end": "1807070"
  },
  {
    "text": "100 to mention allottee really doesn't",
    "start": "1807070",
    "end": "1822040"
  },
  {
    "text": "matter right so even if you have like 10 to the 6 neurons you'd get a similar",
    "start": "1822040",
    "end": "1827440"
  },
  {
    "text": "answer because the the volume of the manifold is so small and its intrinsic",
    "start": "1827440",
    "end": "1833830"
  },
  {
    "text": "dimensionality is 2 right so if you have like no no that's a consequence of the",
    "start": "1833830",
    "end": "1839200"
  },
  {
    "text": "theory so okay so okay so what'swhat's theory and what's Assumption right so if",
    "start": "1839200",
    "end": "1846610"
  },
  {
    "text": "the random projection theory is a good model for the act of electrophysiology",
    "start": "1846610",
    "end": "1852190"
  },
  {
    "text": "measurement given that assumption we expect the exact same answer if we did the analysis I did with a hundred nine",
    "start": "1852190",
    "end": "1858490"
  },
  {
    "text": "year olds versus ten to the six neurons given the intrinsic dimensionality of the manifold is only two right two is",
    "start": "1858490",
    "end": "1865690"
  },
  {
    "text": "much much bigger than hundred nine yeah to dementia no no that's not an",
    "start": "1865690",
    "end": "1874620"
  },
  {
    "text": "assumption okay so yeah yeah yeah only",
    "start": "1874620",
    "end": "1882419"
  },
  {
    "text": "for this experiment right so for other experiments there will be a different dimensionality but then you plug in that",
    "start": "1882419",
    "end": "1888510"
  },
  {
    "text": "dimensionality into the theory and you get an answer for how many neurons if you take a subset of your 109 recorded",
    "start": "1888510",
    "end": "1897059"
  },
  {
    "text": "channels or multiple random randomly chosen subsets yeah how much variability",
    "start": "1897059",
    "end": "1903120"
  },
  {
    "text": "is there if you perform this analysis on the subsets you go between subsets yeah",
    "start": "1903120",
    "end": "1914760"
  },
  {
    "text": "so that the distortion between subsets it's similar to the distortion between any subset and the the full set right",
    "start": "1914760",
    "end": "1922890"
  },
  {
    "text": "because of triangle inequalities in geometry it's a similar answer actually I didn't even include this because this",
    "start": "1922890",
    "end": "1929010"
  },
  {
    "text": "is even more recent work that this hat actually has a really cool practical implication so when people record like",
    "start": "1929010",
    "end": "1937530"
  },
  {
    "text": "hundreds of neurons they stick an electrode array into the brain now each electrode could be listening for it to",
    "start": "1937530",
    "end": "1943049"
  },
  {
    "text": "multiple neurons so there's a complicated clustering problem each neuron leaves a slightly different",
    "start": "1943049",
    "end": "1948480"
  },
  {
    "text": "waveform on the electrode and so in order to get individual neurons you need",
    "start": "1948480",
    "end": "1953730"
  },
  {
    "text": "to solve a clustering problem it's called spike sorting right spike sorting is the bane of existence of experimental",
    "start": "1953730",
    "end": "1961020"
  },
  {
    "text": "neuroscientists right because you're measuring random linear combinations of neurons in your electrodes and you need",
    "start": "1961020",
    "end": "1966960"
  },
  {
    "text": "to deem ixth at random than your combination it turns out we have a separate paper under review and also in",
    "start": "1966960",
    "end": "1972120"
  },
  {
    "text": "the bio archive called accurate recovery of state space dynamics without spike sorting this theory says that if you",
    "start": "1972120",
    "end": "1979710"
  },
  {
    "text": "what I've told you one consequence of series is that if you record a hundred neurons out of 109 you get the right",
    "start": "1979710",
    "end": "1985679"
  },
  {
    "text": "stay space dynamics this also says if you don't do spike sorting but you just - you just look at the high frequency",
    "start": "1985679",
    "end": "1992669"
  },
  {
    "text": "activity on the on the electrodes and just do dimensional reduction directly in that without spike sorting you get",
    "start": "1992669",
    "end": "1999059"
  },
  {
    "text": "the same state space dynamics so what this does is it on like a whole bunch of data sitting on",
    "start": "1999059",
    "end": "2005210"
  },
  {
    "text": "hard drives that people have not spikes sorted and it makes that available for for analysis at least at the level",
    "start": "2005210",
    "end": "2010760"
  },
  {
    "text": "states based dynamics so that's another paper that's on our website and is under review as on bar archives so that was",
    "start": "2010760",
    "end": "2018110"
  },
  {
    "text": "done by Eric Troutman is the lead author in Christian choice lab into collaboration team I live in his so and",
    "start": "2018110",
    "end": "2025700"
  },
  {
    "text": "by the way this is also part of the reason why brain machine interfaces work why you can decode at these simple",
    "start": "2025700",
    "end": "2032360"
  },
  {
    "text": "movements from the brain by only recording 100 neurons of course if you try to decode more complicated movements",
    "start": "2032360",
    "end": "2038419"
  },
  {
    "text": "you probably have to record more neurons but this is going in a single trial data analysis which will because you need to",
    "start": "2038419",
    "end": "2043760"
  },
  {
    "text": "in a BMI you don't want to recover the trial average trajectory of the of the arm that doesn't really mean anything",
    "start": "2043760",
    "end": "2049638"
  },
  {
    "text": "for patient you want to decode the instantaneous intentions of the patient and translate them into the action the",
    "start": "2049639",
    "end": "2054888"
  },
  {
    "text": "prosthetic device so that's why single trial data analysis is of course extremely important so let's move on to",
    "start": "2054889",
    "end": "2060560"
  },
  {
    "text": "that unless there's any other questions on the first third okay so okay so what",
    "start": "2060560",
    "end": "2066500"
  },
  {
    "start": "2065000",
    "end": "2514000"
  },
  {
    "text": "do I mean by by a single trial theory right so here here's kind of the setup",
    "start": "2066500",
    "end": "2072440"
  },
  {
    "text": "okay we always have limited experimental resources the resources are M is the",
    "start": "2072440",
    "end": "2078020"
  },
  {
    "text": "number of neurons we can simultaneous record P or T is the number of training",
    "start": "2078020",
    "end": "2083450"
  },
  {
    "text": "stimuli or amount of time we can observe the brain okay let's say you're observing is let's say in a brain",
    "start": "2083450",
    "end": "2090679"
  },
  {
    "text": "machine to design a brain machine interface you're watching a monkey for example reaching multiple times on each",
    "start": "2090679",
    "end": "2097640"
  },
  {
    "text": "reach you record the individual trajectory of the reach and the individual neural tree the neural",
    "start": "2097640",
    "end": "2103040"
  },
  {
    "text": "trajectory in the brain right and you do this many times and you build up a dictionary so you get a certain amount",
    "start": "2103040",
    "end": "2109160"
  },
  {
    "text": "of training data then subsequently on held-out data say a single trajectory",
    "start": "2109160",
    "end": "2114890"
  },
  {
    "text": "from the monkey's brain neuro structure from the monkey's brain you'd like to construct a brain machine interface that",
    "start": "2114890",
    "end": "2120020"
  },
  {
    "text": "decodes the trajectory the monkeys wants to move its arm on a single trial right",
    "start": "2120020",
    "end": "2125030"
  },
  {
    "text": "so P or T is the amount of training data you have for subsequent single trial analyses and of course there's always",
    "start": "2125030",
    "end": "2131150"
  },
  {
    "text": "some signal-to-noise ratio or try to trial reproducibility in our data even if you show the same visual state",
    "start": "2131150",
    "end": "2136850"
  },
  {
    "text": "to a brain neural activity patterns will vary from trial to trial it's a major open question whether this variation is",
    "start": "2136850",
    "end": "2143120"
  },
  {
    "text": "irrelevant noise or it's just the rest of the brain thinking and therefore it's very interesting cognitive variables",
    "start": "2143120",
    "end": "2148520"
  },
  {
    "text": "that are changing from from trial to trial okay then you also have some measure of the complexity of the",
    "start": "2148520",
    "end": "2154490"
  },
  {
    "text": "experiment of the neural data so some measure of the complexity of the stimuli or behavior or latent variable latent",
    "start": "2154490",
    "end": "2161090"
  },
  {
    "text": "cognitive variables or thoughts or manifold the visited neural States so this is some complexity measure so then",
    "start": "2161090",
    "end": "2166700"
  },
  {
    "text": "the question is given these limited experimental resources and the complexity of the thing we're trying to decode or the neural data how do how",
    "start": "2166700",
    "end": "2173630"
  },
  {
    "text": "well can we decode behavior on single trials and how well can we learn the structure of unobserved layton cognitive",
    "start": "2173630",
    "end": "2178670"
  },
  {
    "text": "variables contributing to this trial to trial variability in the brain good we",
    "start": "2178670",
    "end": "2184310"
  },
  {
    "text": "ideally like to have a theory to help us design experiments before they're done okay so let's consider for example a",
    "start": "2184310",
    "end": "2193490"
  },
  {
    "text": "very simple relatively simple warm-up problem which is inferring latent subspaces in which data live so here's",
    "start": "2193490",
    "end": "2200960"
  },
  {
    "text": "here's the idea let's say that on individual trials there's some latent",
    "start": "2200960",
    "end": "2207230"
  },
  {
    "text": "space like a low dimensional space of things that the monkey can think movements that the monkey can imagine or",
    "start": "2207230",
    "end": "2214400"
  },
  {
    "text": "even stimuli that the monkey sees in which case they're not latent variables that actually observed variables okay so there's some low dimensional",
    "start": "2214400",
    "end": "2220880"
  },
  {
    "text": "space and on individual trials given by these white dots these latent variables take different values in this low",
    "start": "2220880",
    "end": "2226880"
  },
  {
    "text": "dimensional space okay now in the monkey's brain this is the full N",
    "start": "2226880",
    "end": "2232430"
  },
  {
    "text": "dimensional space of all the relevant neurons in the circuit for each of these latent space States there's a different",
    "start": "2232430",
    "end": "2239000"
  },
  {
    "text": "neural activity pattern and we can imagine just a simple linear embedding for example again if it's a non linear",
    "start": "2239000",
    "end": "2245660"
  },
  {
    "text": "curved embedding and the curvature is not that high then the these low",
    "start": "2245660",
    "end": "2250820"
  },
  {
    "text": "dimensional not - curved manifolds live within a subspace of dimensionality not",
    "start": "2250820",
    "end": "2256370"
  },
  {
    "text": "higher not much higher than the intrinsic dimensionality of the curved manifold okay but then again there's",
    "start": "2256370",
    "end": "2261650"
  },
  {
    "text": "noise there's trial trial variability in the data so you're not always living directly on the manifold you're living",
    "start": "2261650",
    "end": "2266930"
  },
  {
    "text": "off the manifold okay now so that's the state of the brain that's the assumed state of the brain in",
    "start": "2266930",
    "end": "2273199"
  },
  {
    "text": "this very simple model okay now what do we get to observe again we don't get to",
    "start": "2273199",
    "end": "2279739"
  },
  {
    "text": "observe all the activity in the brain we get to observe a subset of neural activity patterns okay and we only get",
    "start": "2279739",
    "end": "2287209"
  },
  {
    "text": "to observe the noisy activity patterns we do not get to see the latent subspace and we do not get to see the clean",
    "start": "2287209",
    "end": "2293589"
  },
  {
    "text": "single trial latent variables on trial to trial right so given only these X's",
    "start": "2293589",
    "end": "2299089"
  },
  {
    "text": "can we infer both the orientation of this latent subspace and on individual",
    "start": "2299089",
    "end": "2306410"
  },
  {
    "text": "trials can we denoise the data and extract from from the record M recorded",
    "start": "2306410",
    "end": "2311420"
  },
  {
    "text": "neurons the latent variable that was driving your elective 'ti okay whether its internal cognitive States or motor",
    "start": "2311420",
    "end": "2317420"
  },
  {
    "text": "actions or stimulus evoke response ins okay so the again the quantitative",
    "start": "2317420",
    "end": "2322939"
  },
  {
    "text": "parameters you have n neurons and total neurons on the circuit you only get to record M of them and you only get P",
    "start": "2322939",
    "end": "2329589"
  },
  {
    "text": "stimuli or P latent States okay and let's say the stimuli are encoded by",
    "start": "2329589",
    "end": "2335179"
  },
  {
    "text": "this K dimensional linear subspace okay then what regimes of M and P and K and",
    "start": "2335179",
    "end": "2341809"
  },
  {
    "text": "the SNR can we correctly recover both the subspace in its dimensionality okay",
    "start": "2341809",
    "end": "2349400"
  },
  {
    "text": "so we can formalize this algebraically as a low rank matrix denoising problem",
    "start": "2349400",
    "end": "2355099"
  },
  {
    "text": "as follows so let's imagine that the case stimuli or k latent variables that you see can",
    "start": "2355099",
    "end": "2362150"
  },
  {
    "text": "be arranged so IP the p latent variables that you see each of them is a core is a",
    "start": "2362150",
    "end": "2367219"
  },
  {
    "text": "point in a K dimensional space so they can be arranged as the columns of a K by P matrix so each column is one latent",
    "start": "2367219",
    "end": "2374329"
  },
  {
    "text": "variable and these are the different different trials that you see okay P trials",
    "start": "2374329",
    "end": "2379599"
  },
  {
    "text": "okay these latent states are embedded into the brain through some embedding matrix or in some",
    "start": "2379599",
    "end": "2385609"
  },
  {
    "text": "subspace and I'll describe that subspace by the columns of an orthonormal matrix",
    "start": "2385609",
    "end": "2390679"
  },
  {
    "text": "okay this is an N by K matrix now again",
    "start": "2390679",
    "end": "2396559"
  },
  {
    "text": "you don't get to observe all n neurons you only get to observe a subset of them and this is the random sampling operator",
    "start": "2396559",
    "end": "2403220"
  },
  {
    "text": "which is each row is just one neuron is active has a1 and the rest are zero right so these each row tells you which",
    "start": "2403220",
    "end": "2410839"
  },
  {
    "text": "neuron you're observing and there's M neurons okay plus you have observation",
    "start": "2410839",
    "end": "2417260"
  },
  {
    "text": "noise which I'm just going to assume is iid across neurons and trials it's kind of an idealization of the",
    "start": "2417260",
    "end": "2423950"
  },
  {
    "text": "problem all right and so this is your data your data is the matrix s times u",
    "start": "2423950",
    "end": "2432200"
  },
  {
    "text": "times X plus Z and what you'd like to do is from this data recover you or at",
    "start": "2432200",
    "end": "2440000"
  },
  {
    "text": "least the projection of U onto the recorded subspace and for each individual trial record the recover the",
    "start": "2440000",
    "end": "2446990"
  },
  {
    "text": "columns of X so X is hiding inside your data and you'd like to recover it okay",
    "start": "2446990",
    "end": "2452660"
  },
  {
    "text": "what do I mean by the SNR by the SNR I mean the the signal-to-noise ratio of",
    "start": "2452660",
    "end": "2459230"
  },
  {
    "text": "individual neurons so if I were to trial average right on four different latent",
    "start": "2459230",
    "end": "2466550"
  },
  {
    "text": "variables I'd get a different firing rate and if I take the squared that the power in the trial average firing rate",
    "start": "2466550",
    "end": "2472700"
  },
  {
    "text": "that's the variance of the signal but then if I condition on the latent variable and look at the variability",
    "start": "2472700",
    "end": "2478760"
  },
  {
    "text": "condition on that that's the single trial noise and that has a variance Sigma squared N and so the SNR is as",
    "start": "2478760",
    "end": "2484130"
  },
  {
    "text": "usual in in EE the variance of the signals of the variance of the noise okay so I have a well-defined problem in",
    "start": "2484130",
    "end": "2491690"
  },
  {
    "text": "the interests of how many of you really like math and theory here okay",
    "start": "2491690",
    "end": "2498290"
  },
  {
    "text": "interesting there are many willing to admit that they don't all right so uh-huh yeah good",
    "start": "2498290",
    "end": "2504410"
  },
  {
    "text": "for them yeah so I love math and theory but I'm gonna break my heart and hopefully not piss off a lot of people",
    "start": "2504410",
    "end": "2511339"
  },
  {
    "text": "and I'm gonna skip the theory but it involves really interesting random matrix theory and low rank matrix",
    "start": "2511339",
    "end": "2516500"
  },
  {
    "start": "2514000",
    "end": "2582000"
  },
  {
    "text": "denoising and high dimensional statistics and so on and so forth but I'm going to tell you the answer the",
    "start": "2516500",
    "end": "2523609"
  },
  {
    "text": "answer is the following as long as the product of the signal-to-noise ratio and",
    "start": "2523609",
    "end": "2531170"
  },
  {
    "text": "the square root of the number of recorded neurons and the number of training stimuli that you have",
    "start": "2531170",
    "end": "2536990"
  },
  {
    "text": "is bigger than the dimensionality of the latent subspace right and of course two",
    "start": "2536990",
    "end": "2543500"
  },
  {
    "text": "extra constraints the number of neurons must exceed the dimensionality of the latent subspace and the number of trials must exceed the dimensionality of the",
    "start": "2543500",
    "end": "2549800"
  },
  {
    "text": "Layton subspace but as long as these three conditions hold then you can do what I said we would like to do we can",
    "start": "2549800",
    "end": "2557810"
  },
  {
    "text": "accurately recover these unseen variables we can accurately recover the",
    "start": "2557810",
    "end": "2563750"
  },
  {
    "text": "orientation of the subspace and then subsequently on single trials if I give",
    "start": "2563750",
    "end": "2569510"
  },
  {
    "text": "you a new X here that I've you've never seen before you because you know the subspace you can project it down under",
    "start": "2569510",
    "end": "2575390"
  },
  {
    "text": "the subspace and recover the latent variables okay so this again makes a",
    "start": "2575390",
    "end": "2581240"
  },
  {
    "text": "striking prediction about neural data okay it says that there's a interesting",
    "start": "2581240",
    "end": "2586730"
  },
  {
    "start": "2582000",
    "end": "2618000"
  },
  {
    "text": "hyperbolic trade-off between two very different experimental resources the",
    "start": "2586730",
    "end": "2592160"
  },
  {
    "text": "number of neurons that you record and the number of trials that you take because your ability to do single trial",
    "start": "2592160",
    "end": "2599660"
  },
  {
    "text": "data analysis only depends on the product so you can always trade them off against each other okay so okay so what",
    "start": "2599660",
    "end": "2611869"
  },
  {
    "text": "this says is well we can we can do this in neural data again neural data collected from Krishna annoys lab so it",
    "start": "2611869",
    "end": "2619609"
  },
  {
    "start": "2618000",
    "end": "2726000"
  },
  {
    "text": "again says so now K has been replaced with D in this slide but but but it's the same concept it says there's a",
    "start": "2619609",
    "end": "2626240"
  },
  {
    "text": "hyperbolic phase transition boundary in the M by plea plane and by P plane number of recorded neurons a number of",
    "start": "2626240",
    "end": "2632210"
  },
  {
    "text": "trials okay so this we confirm this in simulations so here we simulate random",
    "start": "2632210",
    "end": "2638840"
  },
  {
    "text": "smooth neural trajectories with an SNR a very small SNR of 0.02 living in a four dimensional space and",
    "start": "2638840",
    "end": "2645530"
  },
  {
    "text": "we ask how many you know we we sub",
    "start": "2645530",
    "end": "2650600"
  },
  {
    "text": "sample a number of neurons we sub sample a number of trials and we ask how well can we will cover the dimensionality of",
    "start": "2650600",
    "end": "2656570"
  },
  {
    "text": "the sub space and the orientation the sub space so you can define an overlap between sub spaces where it's 1 if",
    "start": "2656570",
    "end": "2662690"
  },
  {
    "text": "they're identical and 0 if they're share no common dimensions they're completely orthogonal it generalizes as a cosine of",
    "start": "2662690",
    "end": "2668660"
  },
  {
    "text": "the angle between two one-dimensional some spaces okay and so as you can see there is this hyperbolic phase",
    "start": "2668660",
    "end": "2674640"
  },
  {
    "text": "transition boundary and simulated data but that's just a confirmation of our theory okay the question is is this",
    "start": "2674640",
    "end": "2680910"
  },
  {
    "text": "theory at all relevant to the brain because the brain isn't making the IAG assumptions that we're making it isn't",
    "start": "2680910",
    "end": "2687180"
  },
  {
    "text": "isn't isn't making a linear subspace assumption that we're making but we expect that qualitatively the structure",
    "start": "2687180",
    "end": "2694140"
  },
  {
    "text": "should show up partially because of what we know about universality in random matrix theory we used a random matrix",
    "start": "2694140",
    "end": "2700500"
  },
  {
    "text": "theory to derive if these this hyperbolic phase transition boundary and a lot of results in random matrix Theory",
    "start": "2700500",
    "end": "2706320"
  },
  {
    "text": "doesn't depend in detail on the distribution of individual elements in that random matrix so emergent properties like the eigen value spectrum",
    "start": "2706320",
    "end": "2712440"
  },
  {
    "text": "and so on and so forth don't depend on detail distributional assumptions about individual matrix elements they do",
    "start": "2712440",
    "end": "2719730"
  },
  {
    "text": "depend on having weak correlations between the matrix elements but any case we can test this in a monkey so what we",
    "start": "2719730",
    "end": "2726270"
  },
  {
    "start": "2726000",
    "end": "2874000"
  },
  {
    "text": "did was we again looked at data from the Shenoy lab the same data that i talked about before in a trial average setting",
    "start": "2726270",
    "end": "2732780"
  },
  {
    "text": "but we look at them in a single trial setting right and we use something called Gaussian process factor analysis",
    "start": "2732780",
    "end": "2739260"
  },
  {
    "text": "it's basically a souped up version of pca with some smoothing along the temporal direction to extract latent",
    "start": "2739260",
    "end": "2747270"
  },
  {
    "text": "trajectories neural trajectories in 147 trials for reaches to a single target",
    "start": "2747270",
    "end": "2752580"
  },
  {
    "text": "okay and we use a different algorithm even from from just low rank matrix",
    "start": "2752580",
    "end": "2759600"
  },
  {
    "text": "denoising we use we infer the dimensionality explored by these Leighton trajectories",
    "start": "2759600",
    "end": "2765510"
  },
  {
    "text": "using cross validated Gaussian process factor analysis so there's a parameter which is how many dimensions that you",
    "start": "2765510",
    "end": "2772740"
  },
  {
    "text": "keep and you can choose that parameter using cross-validation how well you predict single trials on held out data",
    "start": "2772740",
    "end": "2778050"
  },
  {
    "text": "and so on the noise in the data is pink and temporally correlated it's not iid",
    "start": "2778050",
    "end": "2783240"
  },
  {
    "text": "in time and trials so there's a different noise in the data okay but still sorry for the the resolution you",
    "start": "2783240",
    "end": "2791220"
  },
  {
    "text": "still see this hyperbolic trade-off between how well you infer the subspace",
    "start": "2791220",
    "end": "2797160"
  },
  {
    "text": "the dimensionality of the subspace and how well you get the right subspace now",
    "start": "2797160",
    "end": "2803400"
  },
  {
    "text": "again this is a situation where you only had a hundred nine neurons so we infer the",
    "start": "2803400",
    "end": "2808740"
  },
  {
    "text": "dimensionality of the subspace and the using all of the neurons and we define",
    "start": "2808740",
    "end": "2815400"
  },
  {
    "text": "the correct subspace to be whatever subspace we got using all of the neurons then we send all of the trials then we",
    "start": "2815400",
    "end": "2823620"
  },
  {
    "text": "sub sample neurons and we sub sample trials when we measure the dimensionality again and we measure the subspace overlap between the subspace",
    "start": "2823620",
    "end": "2830820"
  },
  {
    "text": "that we got using fewer neurons and fewer trials with the projection of the",
    "start": "2830820",
    "end": "2835920"
  },
  {
    "text": "quote/unquote true subspace using all neurons on all trials onto the subspace of the subset of neurons that we keep",
    "start": "2835920",
    "end": "2841710"
  },
  {
    "text": "okay but but to make a long story short you again see an interesting hyperbolic",
    "start": "2841710",
    "end": "2847110"
  },
  {
    "text": "trade-off between neurons and trials so this is kind of random matrix theory",
    "start": "2847110",
    "end": "2853020"
  },
  {
    "text": "operating in the brain right there is this trade-off between two very",
    "start": "2853020",
    "end": "2858780"
  },
  {
    "text": "different resources on neurons and trials so in future experiments it's",
    "start": "2858780",
    "end": "2866070"
  },
  {
    "text": "really important to be cognizant of this trade-off especially in the design of experiments before the data is actually",
    "start": "2866070",
    "end": "2872280"
  },
  {
    "text": "collected okay there's another super",
    "start": "2872280",
    "end": "2877470"
  },
  {
    "start": "2874000",
    "end": "3070000"
  },
  {
    "text": "interesting question that we can ask in terms of this what are we doing when",
    "start": "2877470",
    "end": "2882480"
  },
  {
    "text": "we're doing neuroscience so you're you're familiar with sort of deep learning and recurrent neural networks",
    "start": "2882480",
    "end": "2888000"
  },
  {
    "text": "and things like that so what people are doing is they're again taking neural X a neural data and they're recording a",
    "start": "2888000",
    "end": "2895260"
  },
  {
    "text": "subset of neurons looking at the dynamics and fitting a recurrent neural network to that dynamics okay now what",
    "start": "2895260",
    "end": "2901620"
  },
  {
    "text": "the heck is it that we're doing right how is it that a recurrent neural",
    "start": "2901620",
    "end": "2907290"
  },
  {
    "text": "network fit to a hundred neurons it all reflects anything about the dynamics of",
    "start": "2907290",
    "end": "2912780"
  },
  {
    "text": "the circuit you know using all the neurons is there anything about the",
    "start": "2912780",
    "end": "2917970"
  },
  {
    "text": "circuit dynamics that we can get right ie what the hell are we learning about the brain by fitting neural networks to",
    "start": "2917970",
    "end": "2924540"
  },
  {
    "text": "the brain use it to a small subset of neurons okay that's a super interesting question right",
    "start": "2924540",
    "end": "2930710"
  },
  {
    "text": "especially because everybody's doing it now and and and it's not clear what it means when you do it",
    "start": "2931280",
    "end": "2937160"
  },
  {
    "text": "right so we also ask this question okay so consider and we formalized it using",
    "start": "2937160",
    "end": "2943400"
  },
  {
    "text": "theory and we applied the theory to linear neural networks but then we did simulations and nonlinear neural",
    "start": "2943400",
    "end": "2948470"
  },
  {
    "text": "networks to show that the qualitative structure of the theory holds also nonlinear neural ironwork so what's the",
    "start": "2948470",
    "end": "2953930"
  },
  {
    "text": "setup of the problem we can formalize this promise follows so consider some high dimensional neural circuit with n",
    "start": "2953930",
    "end": "2960140"
  },
  {
    "text": "neurons again you only record to need to record M of them the notation is all the same for a finite amount of time T what",
    "start": "2960140",
    "end": "2967490"
  },
  {
    "text": "can we infer about the circuit dynamics when M is much smaller than n and T is not too large and of course in general",
    "start": "2967490",
    "end": "2972950"
  },
  {
    "text": "the answer is absolutely nothing okay it's a seriously pessimistic problem in this regime for arbitrary recurrent",
    "start": "2972950",
    "end": "2979970"
  },
  {
    "text": "neural networks however we might assume some underlying simplicity about the",
    "start": "2979970",
    "end": "2985549"
  },
  {
    "text": "recurrent Network dynamics for example for some reason than the the network dynamics may be forced to live in a",
    "start": "2985549",
    "end": "2992089"
  },
  {
    "text": "kennel subspace okay because of the recurrent connectivity now that we",
    "start": "2992089",
    "end": "2997490"
  },
  {
    "text": "imagine the recurrent connectivity is constraining the activity patterns to live in a low dimensional subspace okay",
    "start": "2997490",
    "end": "3003520"
  },
  {
    "text": "so then the question is for what region regimes of M and T and K can we",
    "start": "3003520",
    "end": "3008829"
  },
  {
    "text": "correctly recover the dynamical properties of the circuit so again we're so so we'll work out the theory in a",
    "start": "3008829",
    "end": "3014500"
  },
  {
    "text": "linear model so this is a linear neural network with a decay time constant of tau it's kind of a discrete-time version",
    "start": "3014500",
    "end": "3021160"
  },
  {
    "text": "there's a recurrent connectivity and this is a rank K matrix so basically",
    "start": "3021160",
    "end": "3027039"
  },
  {
    "text": "without this matrix activity would decay to the origin but because of the rank K",
    "start": "3027039",
    "end": "3033250"
  },
  {
    "text": "nature of this matrix it's amplifying activity patterns but only in K dimensions okay and there's through some",
    "start": "3033250",
    "end": "3039490"
  },
  {
    "text": "noisy input that's we're just going to take to be white noise here okay now again you don't get to observe the full",
    "start": "3039490",
    "end": "3044890"
  },
  {
    "text": "stage you only observe a subset and this is your random sampling matrix okay so",
    "start": "3044890",
    "end": "3050230"
  },
  {
    "text": "now what do people do they fit an autoregressive model to Y of T okay but",
    "start": "3050230",
    "end": "3057099"
  },
  {
    "text": "what the heck does the autoregressive model have fit to Y of T have to do with an autoregressive model fit to X of T",
    "start": "3057099",
    "end": "3063430"
  },
  {
    "text": "when M is much much less than n when Y is a much lower dimensional vector than",
    "start": "3063430",
    "end": "3068440"
  },
  {
    "text": "ax so um there's interesting methods for",
    "start": "3068440",
    "end": "3074999"
  },
  {
    "start": "3070000",
    "end": "3294000"
  },
  {
    "text": "fitting these using subspace identification and that leads to Hankel matrices you do low-rank decomposition",
    "start": "3074999",
    "end": "3080880"
  },
  {
    "text": "so these Henkel matrices and and there's some nice random matrix Theory you can apply to these but let me just tell you",
    "start": "3080880",
    "end": "3087900"
  },
  {
    "text": "the the answer okay it turns out that there's an again a phase transition in",
    "start": "3087900",
    "end": "3093839"
  },
  {
    "text": "the number of recorded neurons in the amount of time that you record in that in that plane okay",
    "start": "3093839",
    "end": "3100049"
  },
  {
    "text": "that whenever this so called Henkel matrix has a gap in its eigen value spectrum then so this is an example",
    "start": "3100049",
    "end": "3107339"
  },
  {
    "text": "where you have a thousand neurons a rank for connectivity matrix and and and this",
    "start": "3107339",
    "end": "3116069"
  },
  {
    "text": "is kind of the time the the timescale of single neurons right so now you get to",
    "start": "3116069",
    "end": "3121739"
  },
  {
    "text": "measure for a time duration of mm so so roughly the number of observations is 2,000 divided by nine that's the",
    "start": "3121739",
    "end": "3128640"
  },
  {
    "text": "dimensionless ratio of the recording time to the into time constant of the circuit and then the number of neurons",
    "start": "3128640",
    "end": "3134039"
  },
  {
    "text": "is 200 not a thousands you're recording one-fifth of the circuit and then you ask you know I fit an autoregressive",
    "start": "3134039",
    "end": "3141180"
  },
  {
    "text": "model to Y so I get an autoregressive matrix C right",
    "start": "3141180",
    "end": "3146220"
  },
  {
    "text": "oh sorry yes sorry I fit a latent variable model to Y and I get this",
    "start": "3146220",
    "end": "3152819"
  },
  {
    "text": "latent dynamics a that I get from a subset of neurons you could just ask did I get the eigenvalues of the full matrix",
    "start": "3152819",
    "end": "3159390"
  },
  {
    "text": "correct right just the just the 200 by 200 matrix that I get on my observed",
    "start": "3159390",
    "end": "3165839"
  },
  {
    "text": "data have the same eigen values as the thousand by thousand matrix okay and the",
    "start": "3165839",
    "end": "3170880"
  },
  {
    "text": "answer is for this regime yes right so the red are the true eigenvalues and the",
    "start": "3170880",
    "end": "3176849"
  },
  {
    "text": "x of the estimated eigenvalues you actually got the right answer so despite some something you recovered some aspect",
    "start": "3176849",
    "end": "3182849"
  },
  {
    "text": "of the dynamics of the full circuit ie no linear system the eigenvalues of the connectivity matrix okay they're complex",
    "start": "3182849",
    "end": "3189329"
  },
  {
    "text": "because they're not a symmetric matrix on the other hand if the Henkel matrix doesn't have a gap there are errors in",
    "start": "3189329",
    "end": "3194819"
  },
  {
    "text": "the estimate of the eigenvalues okay so for example with 50 neurons you cannot",
    "start": "3194819",
    "end": "3200099"
  },
  {
    "text": "do it where there's what 200 neurons you can for the same amount of recording time okay again there's some so again here's",
    "start": "3200099",
    "end": "3208330"
  },
  {
    "text": "the phase transition number of recorded neurons and in duration of time and there's a hyperbolic phase transition",
    "start": "3208330",
    "end": "3214930"
  },
  {
    "text": "boundary that we get from theory there's this is simulated data the transition in",
    "start": "3214930",
    "end": "3220869"
  },
  {
    "text": "the simulated data matches our theory on this side where you have the product of number of neurons and time exceeds the",
    "start": "3220869",
    "end": "3226810"
  },
  {
    "text": "threshold you get the correct I can values and here you get incorrect",
    "start": "3226810",
    "end": "3232630"
  },
  {
    "text": "eigen values okay that's kind of the take-home message so it's a very similar story a trade-off between two very",
    "start": "3232630",
    "end": "3238720"
  },
  {
    "text": "different parameters number of neurons in the amount of time you get to watch the circuit okay okay we tried this a",
    "start": "3238720",
    "end": "3248380"
  },
  {
    "text": "nonlinear neural networks and we observe similar hyperbolic trade-offs that shift",
    "start": "3248380",
    "end": "3253960"
  },
  {
    "text": "depending on the non-linearity okay so then the last part of the talk I which I",
    "start": "3253960",
    "end": "3260770"
  },
  {
    "text": "think I can finish in about 10 minutes yeah yeah okay this stuff actually is",
    "start": "3260770",
    "end": "3279430"
  },
  {
    "text": "out that the tensor components analysis this is impress it at neuron and it's on the buyer archive and on our website so",
    "start": "3279430",
    "end": "3286119"
  },
  {
    "text": "this last part is actually a practical like algorithm to solve the following problem okay and this is work done by",
    "start": "3286119",
    "end": "3295330"
  },
  {
    "start": "3294000",
    "end": "4545000"
  },
  {
    "text": "Alex Williams in my lab who kind of spearheaded this work so the goal is how",
    "start": "3295330",
    "end": "3301660"
  },
  {
    "text": "do we simultaneously reduce the dimensionality of data across again two very different dimensions time within",
    "start": "3301660",
    "end": "3308830"
  },
  {
    "text": "trials right so remember neural data is often organized into trials like the",
    "start": "3308830",
    "end": "3317260"
  },
  {
    "text": "monkey does something and maybe initially it doesn't do it well then it keeps doing it again and over time it",
    "start": "3317260",
    "end": "3323470"
  },
  {
    "text": "gets better and better at it okay and then so so now you have two very",
    "start": "3323470",
    "end": "3332380"
  },
  {
    "text": "different time scale dynamics within a single trial which could be a second the monkey has is",
    "start": "3332380",
    "end": "3337840"
  },
  {
    "text": "doing something like reaching or something like that the brain is having this churning activity that that that's",
    "start": "3337840",
    "end": "3342880"
  },
  {
    "text": "correlated with the reaching but then across many trials this dynamics is slowly changing to reflect learning or",
    "start": "3342880",
    "end": "3348760"
  },
  {
    "text": "performance or improvement right so how do we simultaneously obtain just low",
    "start": "3348760",
    "end": "3354010"
  },
  {
    "text": "dimensional descriptions of both time with neural dynamics within trials reflecting perception action",
    "start": "3354010",
    "end": "3359680"
  },
  {
    "text": "decision-making motor actions cooking cognition and so forth and time across trials reflecting learning and also",
    "start": "3359680",
    "end": "3366100"
  },
  {
    "text": "there's other really interesting state changes your brain like how much attention you're paying can Wayne from",
    "start": "3366100",
    "end": "3371800"
  },
  {
    "text": "wait wax and wane from time to time your arousal state your changes in cognitive state like monkeys could be thinking",
    "start": "3371800",
    "end": "3378160"
  },
  {
    "text": "different things on different trials that are not under experimental control and we like to extract all of that in an unsupervised manner and especially",
    "start": "3378160",
    "end": "3384870"
  },
  {
    "text": "changes in cognitive state that are not under experimental control must be extracted in an unsupervised manner",
    "start": "3384870",
    "end": "3390730"
  },
  {
    "text": "because you don't have a covariate with which to regress against neural data to discover dimensions in your elec tivity",
    "start": "3390730",
    "end": "3396250"
  },
  {
    "text": "space that encode that covariant okay so the idea we had was we treated these two",
    "start": "3396250",
    "end": "3402850"
  },
  {
    "text": "disparate time scales as two distinct dimensions of a third order tensor with the third axis being neurons okay so",
    "start": "3402850",
    "end": "3411940"
  },
  {
    "text": "here's the basic idea so we kind of discussed trial average pca at the beginning of the talk so here you have",
    "start": "3411940",
    "end": "3417880"
  },
  {
    "text": "neurons by time within trials and you have the spikes of neurons then you have another trial in another trial and let's",
    "start": "3417880",
    "end": "3424690"
  },
  {
    "text": "say you assume that all trials are identical then you just average all the trials and maybe smooth over time so you",
    "start": "3424690",
    "end": "3430180"
  },
  {
    "text": "get a matrix of neurons by time which reflects the trial average firing rates of all the neurons and you decompose it",
    "start": "3430180",
    "end": "3435730"
  },
  {
    "text": "using PCA so you you basically do a matrix factorization where you write this matrix as an outer product of",
    "start": "3435730",
    "end": "3441700"
  },
  {
    "text": "patterns across neurons and patterns across time right okay but that's",
    "start": "3441700",
    "end": "3447310"
  },
  {
    "text": "assuming that individual trials are statistically homogeneous right it's assuming stationarity fundamentally when",
    "start": "3447310",
    "end": "3454090"
  },
  {
    "text": "an animal is learning you cannot assume stationarity in fact learning is tautologically non-stationary right so",
    "start": "3454090",
    "end": "3461740"
  },
  {
    "text": "so trial averaging will make processes like learning and changes in cognitive state invisible right so one way to deal",
    "start": "3461740",
    "end": "3469090"
  },
  {
    "text": "with that is to take these matrices and just concatenate them across time and then do pca right so then you",
    "start": "3469090",
    "end": "3476890"
  },
  {
    "text": "decompose it into a pattern across neurons and then very very long patterns",
    "start": "3476890",
    "end": "3482200"
  },
  {
    "text": "across both time within trial and time across trials okay people sometimes do this but it isn't",
    "start": "3482200",
    "end": "3488350"
  },
  {
    "text": "very successful because there's so many parameters involved in this decomposition because there's nothing",
    "start": "3488350",
    "end": "3494050"
  },
  {
    "text": "constraining it's assuming that there's nothing common across trials right each individual trial could be completely",
    "start": "3494050",
    "end": "3500560"
  },
  {
    "text": "different it's not assuming anything common whenever you're doing single trial data analysis you're a tenth it",
    "start": "3500560",
    "end": "3506320"
  },
  {
    "text": "you're always making an assumption about something that's common across trials and something that varies across trials",
    "start": "3506320",
    "end": "3511960"
  },
  {
    "text": "okay and trial concatenated PC is not making any assumptions about what's common across trials there's too many",
    "start": "3511960",
    "end": "3518050"
  },
  {
    "text": "parameters so it's often not that useful in single trial data okay the approach",
    "start": "3518050",
    "end": "3523360"
  },
  {
    "text": "that we used was to use something called canonical poly attic tensor decomposition for neuroscientists we called it tensor components analysis to",
    "start": "3523360",
    "end": "3529450"
  },
  {
    "text": "rhyme with PCA so the TCA anyways but",
    "start": "3529450",
    "end": "3534640"
  },
  {
    "text": "but that the technique the mathematical technique that we're using is something called canonical poly attack tensor decomposition it's a generalization of",
    "start": "3534640",
    "end": "3540340"
  },
  {
    "text": "PC in the following sense you can take these neurons bye-bye time matrices and",
    "start": "3540340",
    "end": "3545650"
  },
  {
    "text": "stack them into a third order tensor and then decompose this tensor into a sum of",
    "start": "3545650",
    "end": "3552540"
  },
  {
    "text": "Rank 1 outer products of Rank 1 tensor or sum of Rank 1 tensors ok so each",
    "start": "3552540",
    "end": "3558670"
  },
  {
    "text": "component or each component gives you three factors it gives you a pattern of",
    "start": "3558670",
    "end": "3563770"
  },
  {
    "text": "activity across neurons which you can roughly think of a Salle assembly or a or a pattern that varies coherently",
    "start": "3563770",
    "end": "3571840"
  },
  {
    "text": "somehow right it gives you a temporal factor which is a kind of dynamical mode",
    "start": "3571840",
    "end": "3579100"
  },
  {
    "text": "shared by these neurons across time within trials okay and then it gives you",
    "start": "3579100",
    "end": "3585280"
  },
  {
    "text": "a trial of trial factor which is just an a scalar amplitude variable that scales",
    "start": "3585280",
    "end": "3592300"
  },
  {
    "text": "up or down this pattern across neurons in this pattern across time okay so it's",
    "start": "3592300",
    "end": "3598510"
  },
  {
    "text": "like a gain modulation factor right and this is this is just it's just what it does it's a",
    "start": "3598510",
    "end": "3603940"
  },
  {
    "text": "completely unsupervised algorithm you throw at the data center its data tensor it spits this stuff out okay",
    "start": "3603940",
    "end": "3610120"
  },
  {
    "text": "it turns out that there's very simple off-the-shelf algorithm is actually surprisingly useful in neuroscience so",
    "start": "3610120",
    "end": "3617430"
  },
  {
    "text": "one way to think about this is this is completely equivalent to a very simple",
    "start": "3617430",
    "end": "3623890"
  },
  {
    "text": "gain modulated neural network where let's say these are the neurons that you're observing or recording you assume",
    "start": "3623890",
    "end": "3630430"
  },
  {
    "text": "that there's some number of latent factors here corresponding to the number of outer products you have okay you",
    "start": "3630430",
    "end": "3637270"
  },
  {
    "text": "assume that each latent factor has a has its own temporal waveform the shape of",
    "start": "3637270",
    "end": "3643810"
  },
  {
    "text": "this temporal waveform is common across all trials so that's the assumption of commonality across all trials so this is",
    "start": "3643810",
    "end": "3650590"
  },
  {
    "text": "the shape of one factor waveform this is the shape of another and this in shape of another but what's changing across",
    "start": "3650590",
    "end": "3656260"
  },
  {
    "text": "trials is the amplitude of each waveform okay that varies across trials so",
    "start": "3656260",
    "end": "3661840"
  },
  {
    "text": "basically what you're doing is you're summarizing each trial let's say there's K factors you're summarizing it by K",
    "start": "3661840",
    "end": "3667780"
  },
  {
    "text": "numbers the amplitude of each waveform okay and that's what's driving the",
    "start": "3667780",
    "end": "3672790"
  },
  {
    "text": "activity and it turns out that this is if you do CP decomposition on just",
    "start": "3672790",
    "end": "3679180"
  },
  {
    "text": "observations of activity patterns and noisy versions of them you can accurately recover all of these factors",
    "start": "3679180",
    "end": "3686160"
  },
  {
    "text": "now what's really interesting is PCA is fundamentally a subspace recovery",
    "start": "3686160",
    "end": "3691810"
  },
  {
    "text": "algorithm right PCA only gives you unique principal components if you have an orthogonality assumption on your on",
    "start": "3691810",
    "end": "3698440"
  },
  {
    "text": "your principal components without such an orthogonal the assumption you just recover a subspace if you keep K",
    "start": "3698440",
    "end": "3704080"
  },
  {
    "text": "principal components here the individual factors actually matter right this",
    "start": "3704080",
    "end": "3709510"
  },
  {
    "text": "actually recovered the individual factors identically and it doesn't",
    "start": "3709510",
    "end": "3714640"
  },
  {
    "text": "require them to be orthogonal PCA applied to different unfoldings of the tensor doesn't work independent",
    "start": "3714640",
    "end": "3720040"
  },
  {
    "text": "components analysis doesn't work but but but the CP decomposition works okay that's kind of to be expected because we",
    "start": "3720040",
    "end": "3726550"
  },
  {
    "text": "did it on data drawn from the model and so that's it's to be expected yeah",
    "start": "3726550",
    "end": "3734280"
  },
  {
    "text": "who the corners you can keep getting practice and practice in practice yes we are various method yeah so we",
    "start": "3735520",
    "end": "3742940"
  },
  {
    "text": "have three methods for choosing the number of factors you weren't a reviewer for a paper yes we demolished that",
    "start": "3742940",
    "end": "3749540"
  },
  {
    "text": "review so uh we already had two two methods so I mean this is a problem with any unsupervised data analysis algorithm",
    "start": "3749540",
    "end": "3758720"
  },
  {
    "text": "right whether it's clustering how do you choose the number of clusters whether it's principal components how many components do you keep and so on",
    "start": "3758720",
    "end": "3764869"
  },
  {
    "text": "there's several methods you can use you can look at the reconstruction error as a function of the number of factors that",
    "start": "3764869",
    "end": "3770750"
  },
  {
    "text": "you keep and you might stop when you get diminishing returns you could look at the similarity of the factors that you",
    "start": "3770750",
    "end": "3777440"
  },
  {
    "text": "get because this is a non convex optimization problem so if you start from different racial conditions you may",
    "start": "3777440",
    "end": "3783110"
  },
  {
    "text": "get slightly different factors so you could run it multiple times from different initial conditions and align the factors and see how similar the",
    "start": "3783110",
    "end": "3789260"
  },
  {
    "text": "factors are as you start including more and more factors they'll stop being similar to each other the other thing",
    "start": "3789260",
    "end": "3795380"
  },
  {
    "text": "you can do is you can do cross-validation you can hold out stuff in the tensor okay and do the",
    "start": "3795380",
    "end": "3802160"
  },
  {
    "text": "decomposition only on elements you keep and ask how well you can predict the other elements all three methods are",
    "start": "3802160",
    "end": "3807470"
  },
  {
    "text": "great ways of choosing it but actually the other thing is as usual and unsupervised dimensional error reduction",
    "start": "3807470",
    "end": "3812600"
  },
  {
    "text": "techniques this number of factors that you keep allows you to trade-off between two very important variables the",
    "start": "3812600",
    "end": "3820160"
  },
  {
    "text": "complexity of the model and the interpretability of the model and we just observed that the answers that we got were pretty robust as we change the",
    "start": "3820160",
    "end": "3826130"
  },
  {
    "text": "number of factors usually what happens is as you start increasing factors individual factors split into two",
    "start": "3826130",
    "end": "3831650"
  },
  {
    "text": "factors and so on well all the other factors remain the same yeah",
    "start": "3831650",
    "end": "3839829"
  },
  {
    "text": "monkeys learning over time monkeys get faster so the never sampled excellent goes away dramatically excellent so um",
    "start": "3841770",
    "end": "3850369"
  },
  {
    "text": "there's various ways to deal with that you can time warp the data you could keep roughly the fastest trials and",
    "start": "3850369",
    "end": "3857790"
  },
  {
    "text": "truncate the data and so on there's various what you can align the data to interesting events and keep fixed",
    "start": "3857790",
    "end": "3865020"
  },
  {
    "text": "amounts of data around the interesting events like movement onset or something like that your scientists have dealt",
    "start": "3865020",
    "end": "3870480"
  },
  {
    "text": "with this time immemorial because every individual trial is a different duration and those are the kind of the techniques that people use and we use those same",
    "start": "3870480",
    "end": "3876390"
  },
  {
    "text": "techniques it's not a smooth expansion",
    "start": "3876390",
    "end": "3890660"
  },
  {
    "text": "to the monkey because we did this on the monkey so let me tell you what we did this so we did this in an artificial",
    "start": "3894770",
    "end": "3901140"
  },
  {
    "text": "recurrent neural network and it told us how the network learns we're actually using tensor components analysis tool to",
    "start": "3901140",
    "end": "3908640"
  },
  {
    "text": "analyze learning dynamics in deep deep networks actually and it seems to yield",
    "start": "3908640",
    "end": "3913950"
  },
  {
    "text": "very interesting results there this is that we also applied it to I have that",
    "start": "3913950",
    "end": "3919140"
  },
  {
    "text": "slide I don't have it on the item we also applied it to data from Rochester's lab also here at Stanford on mice",
    "start": "3919140",
    "end": "3925380"
  },
  {
    "text": "navigating a maze and it yielded interesting effects in the mouse but let me focus on the monkey here so this is",
    "start": "3925380",
    "end": "3932130"
  },
  {
    "text": "an interesting experiment how much time do I have okay so let me",
    "start": "3932130",
    "end": "3938700"
  },
  {
    "text": "try to do this in three minutes and I can stay afterwards for some questions but of course nobody needs to stay but",
    "start": "3938700",
    "end": "3945480"
  },
  {
    "text": "but here's the basic idea this is a BMI perturbation experiment so what they did",
    "start": "3945480",
    "end": "3951690"
  },
  {
    "text": "was they had a monkey reaching and they developed a brain machine interface the translated neural activity into the",
    "start": "3951690",
    "end": "3959280"
  },
  {
    "text": "intended direction of movement of the monkeys arm then they tied the monkeys arm down and they had a neural cursor",
    "start": "3959280",
    "end": "3965070"
  },
  {
    "text": "they had the monkeys brain directly control the cursor so because the BMI",
    "start": "3965070",
    "end": "3970920"
  },
  {
    "text": "was trained on the monkeys natural reaching patterns the monkey could control the cursor so if was asked to reach here the monkey could",
    "start": "3970920",
    "end": "3977540"
  },
  {
    "text": "think and make the cursor move here and so on okay and this is the these are individual trajectories to a particular",
    "start": "3977540",
    "end": "3984050"
  },
  {
    "text": "target and you can see the trajectories are not bad there's some overshoot and return and all that but it's sort of working then they do this vicious",
    "start": "3984050",
    "end": "3991010"
  },
  {
    "text": "perturbation where they rotate the the",
    "start": "3991010",
    "end": "3996760"
  },
  {
    "text": "cursor position cursor angle thirty degrees compared to what the monkey is expecting okay so now like if the",
    "start": "3996760",
    "end": "4005110"
  },
  {
    "text": "monkeys thinking that the cur that he wants to move the cursor this way the",
    "start": "4005110",
    "end": "4011020"
  },
  {
    "text": "cursor will actually move this way okay it's a vicious perturbation monkey doesn't get it initially right after the",
    "start": "4011020",
    "end": "4016870"
  },
  {
    "text": "perturbation the monkey is still thinking the cursor should move this way but lo and behold the cursor moves this",
    "start": "4016870",
    "end": "4022660"
  },
  {
    "text": "way because the experimentalist tricked the monkey and then over some late time the monkey makes a correction and then",
    "start": "4022660",
    "end": "4029830"
  },
  {
    "text": "moves moves back to here so these are trot individual trials right after the perturbation but then at much later",
    "start": "4029830",
    "end": "4037480"
  },
  {
    "text": "trials it starts to get better and it adapts the monkey's brain adapts or learns to deal with the rotated BMI",
    "start": "4037480",
    "end": "4043600"
  },
  {
    "text": "right and so the monkey starts to do more straight reaches okay now we want",
    "start": "4043600",
    "end": "4052000"
  },
  {
    "text": "to discover what's going on in the brain during this and of course individual neurons are super complicated it's hard",
    "start": "4052000",
    "end": "4058030"
  },
  {
    "text": "to understand by looking at them so we just through the neural data tenser which is again order a hundred neurons",
    "start": "4058030",
    "end": "4063610"
  },
  {
    "text": "by six hundred milliseconds by about a hundred you can see about a hundred trials okay and we found that three",
    "start": "4063610",
    "end": "4070480"
  },
  {
    "text": "components by the various measures of choosing the number of components were sufficient to explain most of the variance in the data okay and so we",
    "start": "4070480",
    "end": "4078010"
  },
  {
    "text": "recovered these three components so these so this component 1 2 & 3 ok these are the different cell assemblies",
    "start": "4078010",
    "end": "4084250"
  },
  {
    "text": "involved in each components but let's look at the the dynamics okay so the interesting one is the across trial so",
    "start": "4084250",
    "end": "4092950"
  },
  {
    "text": "you have some trials before the perturbation then a bunch of trials after the perturbation ok so there's",
    "start": "4092950",
    "end": "4100270"
  },
  {
    "text": "some component that's active before the perturbation and so this is like the baseline state of the brain ok and it's",
    "start": "4100270",
    "end": "4107950"
  },
  {
    "text": "active very very early in the reach its active super early like so it's implementing this kind of",
    "start": "4107950",
    "end": "4113500"
  },
  {
    "text": "movement okay interestingly the baseline state of the",
    "start": "4113500",
    "end": "4118540"
  },
  {
    "text": "brain which was good before the perturbation becomes maladaptive or bad after the perturbation but it doesn't",
    "start": "4118540",
    "end": "4124720"
  },
  {
    "text": "get extinguished the brain keeps it around what instead happens as two",
    "start": "4124720",
    "end": "4129759"
  },
  {
    "text": "components are born after the perturbation and they have different dynamics one gets active right after the",
    "start": "4129760",
    "end": "4137200"
  },
  {
    "text": "perturbation and within a trial it's active very late and what's that's doing",
    "start": "4137200",
    "end": "4142390"
  },
  {
    "text": "is it implements this late stage correction but then over time the late stage correction dies down and this",
    "start": "4142390",
    "end": "4151180"
  },
  {
    "text": "early stage correction or mid stage correction gets activated by the brain",
    "start": "4151180",
    "end": "4156430"
  },
  {
    "text": "and it's active in the middle period right and it's presumably implementing this and compensating for this thing",
    "start": "4156430",
    "end": "4162819"
  },
  {
    "text": "right and so that's how the brain is kind of doing it it has a it hasn't yeah",
    "start": "4162820",
    "end": "4169480"
  },
  {
    "text": "it has these two components that mediate the learning now remember that this was just extracted completely from the",
    "start": "4169480",
    "end": "4176049"
  },
  {
    "text": "monkey's brain this data analysis algorithm never had access to the behavior nevertheless do individual",
    "start": "4176050",
    "end": "4184000"
  },
  {
    "text": "components predict anything about the behavior so as you mentioned as the monkey gets better and better it gets",
    "start": "4184000",
    "end": "4189730"
  },
  {
    "text": "faster at the task right these reaches are pretty slow because it has to do a long correction these",
    "start": "4189730",
    "end": "4195850"
  },
  {
    "text": "reaches are faster so one behavioral one behavioral correlative learning is the time it",
    "start": "4195850",
    "end": "4202900"
  },
  {
    "text": "takes to acquire the target so this is just behavior okay this is on each trial",
    "start": "4202900",
    "end": "4208420"
  },
  {
    "text": "how long did it take to acquire the target it's doing really well and fast here after the prohibition goes up and",
    "start": "4208420",
    "end": "4213610"
  },
  {
    "text": "it comes down this looks a lot like this component extracted from the neural data okay they look strikingly similar it",
    "start": "4213610",
    "end": "4222310"
  },
  {
    "text": "turns out that they're actually quite identical on a trial by trial their basis so you can try to predict reach",
    "start": "4222310",
    "end": "4230560"
  },
  {
    "text": "time from this neural component directly and we did that and now this is data",
    "start": "4230560",
    "end": "4235690"
  },
  {
    "text": "from across all eight directions okay the black is behavior time to acquire",
    "start": "4235690",
    "end": "4242650"
  },
  {
    "text": "the target the blue is extracted from mural data alone okay and we're not doing a",
    "start": "4242650",
    "end": "4247869"
  },
  {
    "text": "regression because there's just a single factor we were just scaling one amplitude factor up to match best as",
    "start": "4247869",
    "end": "4253929"
  },
  {
    "text": "possible the time and you see there's a lot of Co variation across trials so this is what I mean by tensor components",
    "start": "4253929",
    "end": "4259809"
  },
  {
    "text": "analysis does D mixing I didn't show you this but in many many data sets individual factors tend to have a highly",
    "start": "4259809",
    "end": "4266459"
  },
  {
    "text": "interpretive --all meaning that can often relate to a quantitative aspect of",
    "start": "4266459",
    "end": "4272050"
  },
  {
    "text": "behavior in this particular case there's one neural component right which is a",
    "start": "4272050",
    "end": "4277869"
  },
  {
    "text": "corrective perturbation whose amplitude dies off and it predicts the time it",
    "start": "4277869",
    "end": "4283780"
  },
  {
    "text": "takes to do the reach okay and that's it so that's the end again pair on gouget a",
    "start": "4283780",
    "end": "4291010"
  },
  {
    "text": "lot of the work on the of the single trial theory and Alex Williams did a lot",
    "start": "4291010",
    "end": "4296320"
  },
  {
    "text": "of the work on tensor component analysis we actually have collaborations lots of experimental labs and also working theory of deep learning and stuff like",
    "start": "4296320",
    "end": "4302019"
  },
  {
    "text": "that but again Thanks so if you tried",
    "start": "4302019",
    "end": "4314409"
  },
  {
    "text": "anything other than monkeys yeah we actually work with mice with flies we",
    "start": "4314409",
    "end": "4319809"
  },
  {
    "text": "haven't have some work in humans what else salamanders yeah we're working a",
    "start": "4319809",
    "end": "4327550"
  },
  {
    "text": "bunch of things from the experimentalist different animals are better for",
    "start": "4327550",
    "end": "4332979"
  },
  {
    "text": "different questions was there a particular there presumably ethical",
    "start": "4332979",
    "end": "4342039"
  },
  {
    "text": "issues with working on people how do you what so when people it's more like psychology data that we work with so we",
    "start": "4342039",
    "end": "4349030"
  },
  {
    "text": "have some work on like a theories of how infants develop concepts and there's all",
    "start": "4349030",
    "end": "4355599"
  },
  {
    "text": "these psychological studies of when infants can learn categories and things like that and neural networks learn",
    "start": "4355599",
    "end": "4360969"
  },
  {
    "text": "categories of the surgeons I now makes it turns out neural networks and infants learn categories with the same we have a mathematical theory for why so",
    "start": "4360969",
    "end": "4368980"
  },
  {
    "text": "with humans mostly psychology data or fMRI data but I just kind of stayed away from that for my data just not because",
    "start": "4368980",
    "end": "4375190"
  },
  {
    "text": "it's not interesting but I've just swamped with electrophysiology data from",
    "start": "4375190",
    "end": "4380730"
  },
  {
    "text": "actually human data under the right circumstances yeah usually so there's various modes for measuring data from",
    "start": "4381510",
    "end": "4388030"
  },
  {
    "text": "the human brain there's fMRI there's EEG those are non-invasive then there's a cog which",
    "start": "4388030",
    "end": "4395080"
  },
  {
    "text": "you can often do since a epilepsy patients by putting surface electrodes on the surface of the brain so you don't",
    "start": "4395080",
    "end": "4400810"
  },
  {
    "text": "get low-pass filtering through the skull which you would get on surface EEG that's done by for example Joseph's part",
    "start": "4400810",
    "end": "4407140"
  },
  {
    "text": "VC here we've looked at some of that data but and then you can even put in",
    "start": "4407140",
    "end": "4413920"
  },
  {
    "text": "depth electrodes into the human brain again only in patients that require that kind of monitoring of the brain say to",
    "start": "4413920",
    "end": "4420070"
  },
  {
    "text": "excise some part of the brain for epilepsy or something like that yes a",
    "start": "4420070",
    "end": "4427180"
  },
  {
    "text": "deep brain stimulation for Parkinson's that's another method for treating Parkinson's that's done but it's very",
    "start": "4427180",
    "end": "4436240"
  },
  {
    "text": "hard to get multi electrode data from the human brain though that is being done by Krishna shenoy and Jamie",
    "start": "4436240",
    "end": "4441340"
  },
  {
    "text": "Henderson here I don't have a picture of how your probes look how do you get 100",
    "start": "4441340",
    "end": "4448810"
  },
  {
    "text": "probes something oh there's various ways to do this there's actually some new technology called neuro pixel probes which is this",
    "start": "4448810",
    "end": "4454990"
  },
  {
    "text": "linear array and you slowly squish it into the brain and wait for the brain to stabilize and then you have electrodes",
    "start": "4454990",
    "end": "4461470"
  },
  {
    "text": "all along their array right there's also surface electrodes which only allow you",
    "start": "4461470",
    "end": "4466780"
  },
  {
    "text": "to record from the surface of the brain and it's it's a bed of electrodes a to 2d array of electrodes and you stick it",
    "start": "4466780",
    "end": "4472570"
  },
  {
    "text": "in a little bit into the brain",
    "start": "4472570",
    "end": "4475619"
  },
  {
    "text": "there's also calcium imaging where you you can record fluorescence coming from",
    "start": "4479280",
    "end": "4484810"
  },
  {
    "text": "so calcium rushes in that gates an ion channel that gates a particular molecule",
    "start": "4484810",
    "end": "4490510"
  },
  {
    "text": "to fluoresce and you measure the fluorescence that's something that happens with these multi electrodes that you can",
    "start": "4490510",
    "end": "4497940"
  },
  {
    "text": "service some great that this stuff never personally use some legends every but how many hours how long a period of time",
    "start": "4497940",
    "end": "4506070"
  },
  {
    "text": "can you keep well that's a question yeah it varies from technology technology for",
    "start": "4506070",
    "end": "4513480"
  },
  {
    "text": "some you can keep them in for months but there's no guarantee that you'll get the",
    "start": "4513480",
    "end": "4519060"
  },
  {
    "text": "same neurons on day one versus day two versus day three there's some churn and",
    "start": "4519060",
    "end": "4524720"
  },
  {
    "text": "people believe to different degrees claims that you got the same you're on over multiple days if you want to do",
    "start": "4524720",
    "end": "4531090"
  },
  {
    "text": "that kind of stuff you uh you do calcium imaging and that's that you can get you",
    "start": "4531090",
    "end": "4539400"
  },
  {
    "text": "can stabili get neurons over many days that's in the same your",
    "start": "4539400",
    "end": "4544400"
  }
]