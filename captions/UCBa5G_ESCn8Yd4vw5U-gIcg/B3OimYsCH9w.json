[
  {
    "text": "Hello and welcome to this section on unsupervised learning.",
    "start": "5150",
    "end": "10000"
  },
  {
    "text": "So this is really a sudden shift in topic.",
    "start": "13100",
    "end": "20430"
  },
  {
    "text": "We're moving on to a new section within the class.",
    "start": "20430",
    "end": "25470"
  },
  {
    "text": "So far, everything we've talked about in the class has been supervised learning, and now we're gong to start talking about unsupervised learning.",
    "start": "25470",
    "end": "34380"
  },
  {
    "text": "And the idea in supervised learning is that we have pairs of records, u and v,",
    "start": "34380",
    "end": "40125"
  },
  {
    "text": "and we want to learn a model which predicts v given u. Um,",
    "start": "40125",
    "end": "49550"
  },
  {
    "text": "and it's called supervised learning because the- the vi's are",
    "start": "49550",
    "end": "54864"
  },
  {
    "text": "giving us information about what the right answer is in particular cases,",
    "start": "54865",
    "end": "61879"
  },
  {
    "text": "corresponding to the ui's. And this is supervising, is the idea the learning of the model.",
    "start": "61879",
    "end": "69025"
  },
  {
    "text": "In unsupervised learning, it's different. We only have records u,",
    "start": "69025",
    "end": "75305"
  },
  {
    "text": "and our goal is to build a model of the u's.",
    "start": "75305",
    "end": "80970"
  },
  {
    "text": "Um, and so we'd like to be able to do things such as reveal the structure of the set of possible u's.",
    "start": "80970",
    "end": "88189"
  },
  {
    "text": "Um, we'd like to be able to deal with missing entries in the u's, and figure out what they are.",
    "start": "88190",
    "end": "93650"
  },
  {
    "text": "That's called imputation. We'd like to be able to detect anomalies, unusual cases which we've not seen before.",
    "start": "93650",
    "end": "102430"
  },
  {
    "text": "And the idea of- of revealing structure or detecting anomalies are first kind of vague at this point,",
    "start": "102430",
    "end": "109250"
  },
  {
    "text": "and we'll make them a little bit more precise. And imputing missing entries, well, we'll see exactly how to do that.",
    "start": "109250",
    "end": "116830"
  },
  {
    "text": "So just as before, we work with embedded data, we take our",
    "start": "122720",
    "end": "129330"
  },
  {
    "text": "data u and we embed it into a feature vector x. X is Phi of u and x is some vector that lives in R_d.",
    "start": "129330",
    "end": "139075"
  },
  {
    "text": "And then we build our data model for the vectors x. Then when we need to,",
    "start": "139075",
    "end": "144380"
  },
  {
    "text": "we unembed to go back to the raw vector u. So from now on in this section,",
    "start": "144380",
    "end": "151370"
  },
  {
    "text": "we're going to work with the feature vector x. So we'll have embedded dataset x_1 through x_n,",
    "start": "151370",
    "end": "158735"
  },
  {
    "text": "each of these is a vector in R_d.",
    "start": "158735",
    "end": "161850"
  },
  {
    "text": "Now, the way we construct a model for this dataset is via a loss function.",
    "start": "165160",
    "end": "176195"
  },
  {
    "text": "Um, we'd like to have something which characterizes",
    "start": "176195",
    "end": "181910"
  },
  {
    "text": "the elements of the dataset which tells us what elements of the dataset look like or should look like.",
    "start": "181910",
    "end": "188819"
  },
  {
    "text": "And I'm going to do this with a loss function, which we could also call in this case an implausibility function.",
    "start": "189070",
    "end": "195100"
  },
  {
    "text": "It's a function on R_d, which is our space of possible x's, real valued,",
    "start": "195100",
    "end": "201075"
  },
  {
    "text": "and it tells us how implausible x is as a data point. So if l of x is small,",
    "start": "201075",
    "end": "207680"
  },
  {
    "text": "then x looks like our data. It's typical. And if l of x is large,",
    "start": "207680",
    "end": "213350"
  },
  {
    "text": "then x does not look like the data. Now, the model might be probabilistic.",
    "start": "213350",
    "end": "219585"
  },
  {
    "text": "So x might correspond to a probability distribution or a probability density, p of x,",
    "start": "219585",
    "end": "227010"
  },
  {
    "text": "and then we would take l of x's to be the negative log of p of x, which is the negative log probability density.",
    "start": "227010",
    "end": "235730"
  },
  {
    "text": "Uh, we might think of that as the negative log likelihood of x. Um,",
    "start": "235730",
    "end": "241890"
  },
  {
    "text": "other names for l of x we might talk about surprise or perplexity, um,",
    "start": "241890",
    "end": "246944"
  },
  {
    "text": "and l is often parameterized by a vector or Theta, and it might be a matrix Theta.",
    "start": "246945",
    "end": "253260"
  },
  {
    "text": "And so we'll put a subscript Theta on l, l sub Theta of x.",
    "start": "253260",
    "end": "259690"
  },
  {
    "text": "Uh, let's look at the simplest example.",
    "start": "261800",
    "end": "267180"
  },
  {
    "text": "Suppose, um, our data model is x is near a fixed vector Theta.",
    "start": "267180",
    "end": "274880"
  },
  {
    "text": "So the model here is parametrized by the vector Theta,",
    "start": "274880",
    "end": "280595"
  },
  {
    "text": "and now we're going to try and learn that from the data. Um, and you might have implausibility functions associated with this.",
    "start": "280595",
    "end": "288755"
  },
  {
    "text": "For example, you might have the square loss, which would be the sum of the squares of the 2-norm of x minus Theta.",
    "start": "288755",
    "end": "298180"
  },
  {
    "text": "Sorry, that's correct. Or the 1-norm of x minus Theta, which is the sum of the absolute values of x_i minus Theta_i.",
    "start": "298180",
    "end": "308360"
  },
  {
    "text": "Uh, a different data model",
    "start": "308360",
    "end": "316050"
  },
  {
    "text": "is the K-means data model. Here, the idea is, is that rather than just having one ideal point, one representative,",
    "start": "316050",
    "end": "325190"
  },
  {
    "text": "we have K of them, Theta 1 to Theta K, all vectors in R_d. And we believe that our data is close to one of these representatives.",
    "start": "325190",
    "end": "338825"
  },
  {
    "text": "Um, now we measure how implausible a",
    "start": "338825",
    "end": "345050"
  },
  {
    "text": "data point x is by saying, what's the distance between x and the closest representative?",
    "start": "345050",
    "end": "353455"
  },
  {
    "text": "Which is just the minimum from i is 1 to k of the norm of x minus Theta_i squared.",
    "start": "353455",
    "end": "360919"
  },
  {
    "text": "If it's the distance squared that we're using as our distance measure, we might equally well use, uh,",
    "start": "360920",
    "end": "367195"
  },
  {
    "text": "uh, a 1-norm or a different norm. Normally for K-means, K-means specifically means the squared distance,",
    "start": "367195",
    "end": "379005"
  },
  {
    "text": "the squared 2-norm distance. Then the model is parameterized by these, uh,",
    "start": "379005",
    "end": "387120"
  },
  {
    "text": "uh, k d-dimensional vectors, which we could equally well view as a d by k matrix,",
    "start": "387120",
    "end": "392885"
  },
  {
    "text": "which we'll call Theta, whose columns are Theta 1 to Theta K. Now,",
    "start": "392885",
    "end": "404660"
  },
  {
    "text": "it's worth looking at what the role of the loss function is in supervised versus unsupervised learning.",
    "start": "404660",
    "end": "410240"
  },
  {
    "text": "Uh, in supervised learning, the loss function is used to choose a particular predictor",
    "start": "410240",
    "end": "416210"
  },
  {
    "text": "from a family of predictors parameterized by Theta. And once we've chosen the predictor,",
    "start": "416210",
    "end": "424560"
  },
  {
    "text": "we no longer really care about the loss function. Uh, the predictor itself is our model of how x and y are related.",
    "start": "424560",
    "end": "435300"
  },
  {
    "text": "In unsupervised learning, the- the loss function plays",
    "start": "435650",
    "end": "440680"
  },
  {
    "text": "a slightly different role because it characterizes what the data looks like.",
    "start": "440680",
    "end": "445990"
  },
  {
    "text": "Um, but the loss function actually is the data model. Getting- getting the loss function,",
    "start": "445990",
    "end": "451680"
  },
  {
    "text": "that's- that's the primary goal of unsupervised learning.",
    "start": "451680",
    "end": "456830"
  },
  {
    "text": "So using this loss function enables us to build, for example, an anomaly detector.",
    "start": "461000",
    "end": "466669"
  },
  {
    "text": "Um, and that's a- a particular way of using a data model in order to identify anomalies,",
    "start": "466670",
    "end": "472629"
  },
  {
    "text": "suspicious feature vectors, feature vectors that aren't consistent with the other feature vectors that we've seen.",
    "start": "472629",
    "end": "478120"
  },
  {
    "text": "Uh, for example, the common application is, uh, let's say network traffic monitoring.",
    "start": "478120",
    "end": "485435"
  },
  {
    "text": "We might have feature vectors that includes statistics of the size and distribution of packets.",
    "start": "485435",
    "end": "493620"
  },
  {
    "text": "And what we do is we fit our data model. We have a loss function l parameterized by Theta.",
    "start": "493620",
    "end": "501620"
  },
  {
    "text": "We fit that by choosing the Theta, and then we'll say that,",
    "start": "501620",
    "end": "507945"
  },
  {
    "text": "if we look at all of our data, x_1 through x_n, we can look at the corresponding distribution of the loss function,",
    "start": "507945",
    "end": "517550"
  },
  {
    "text": "and we can say, let's find the percentile value,",
    "start": "517550",
    "end": "522680"
  },
  {
    "text": "say the 99th percentile value of that loss function. So that 99% of the values that we've observed of l of",
    "start": "522680",
    "end": "531560"
  },
  {
    "text": "x are less than or equal to t. And then when we're getting new data coming in,",
    "start": "531560",
    "end": "538495"
  },
  {
    "text": "as we're watching the network, if we- every time we get an x,",
    "start": "538495",
    "end": "544430"
  },
  {
    "text": "we evaluate the loss function on it, we'll flag it as anomalous if the loss function is greater than our threshold t,",
    "start": "544430",
    "end": "553175"
  },
  {
    "text": "is greater than that 99 percent- percentile value. Um, and this is,",
    "start": "553175",
    "end": "559650"
  },
  {
    "text": "uh, an anomaly detector.",
    "start": "559650",
    "end": "563260"
  },
  {
    "text": "We can also use our data model to impute missing entries.",
    "start": "569360",
    "end": "575055"
  },
  {
    "text": "So we'll suppose that x has some entries missing. We might label them with question marks or NA or",
    "start": "575055",
    "end": "582315"
  },
  {
    "text": "NaN for not available or for not a number, um, within the dataset.",
    "start": "582315",
    "end": "588765"
  },
  {
    "text": "And, uh, we'd like to fill in those missing data entries.",
    "start": "588765",
    "end": "598320"
  },
  {
    "text": "So, uh, uh, for any given vector x,",
    "start": "598320",
    "end": "605280"
  },
  {
    "text": "we've got a subset of the numbers 1 through d, which is the set of known entries.",
    "start": "605280",
    "end": "611460"
  },
  {
    "text": "We'll call that set script k for that data element x. And, um, uh, we're going to replace x with x hat.",
    "start": "611460",
    "end": "626505"
  },
  {
    "text": "And x hat is going to be such that, well, its value on the known entries,",
    "start": "626505",
    "end": "632715"
  },
  {
    "text": "on if i is one of the known- is one of the known entries, then we'll have x hat_i equal to x_i.",
    "start": "632715",
    "end": "639460"
  },
  {
    "text": "If i is not one of the known entries, well then we're going to have to fill it in and we're going to have x hat",
    "start": "639560",
    "end": "645690"
  },
  {
    "text": "_i is a question mark that we're going to have to replace. So on the slide here we have an example where,",
    "start": "645690",
    "end": "654675"
  },
  {
    "text": "um, uh, x here has,",
    "start": "654675",
    "end": "660404"
  },
  {
    "text": "um, a dimension 4 and k is 1, 3, which means we know the first and the third entry of x.",
    "start": "660405",
    "end": "673410"
  },
  {
    "text": "And our job is going to be to fill in these unknown entries.",
    "start": "673410",
    "end": "678704"
  },
  {
    "text": "Here we filled them in with minus 1.5 and 3.4.",
    "start": "678705",
    "end": "684300"
  },
  {
    "text": "And this quantity on the right is x hat. And in the first and the third entry of x hat,",
    "start": "684300",
    "end": "690735"
  },
  {
    "text": "are forced to be equal to the first and the third entries of x because we know the first and the third entries.",
    "start": "690735",
    "end": "698445"
  },
  {
    "text": "But our job is to fill in the others. And of course k's might be different for each of our different, uh, data records.",
    "start": "698445",
    "end": "709394"
  },
  {
    "text": "So for each i we might have a different k. So,",
    "start": "709395",
    "end": "717600"
  },
  {
    "text": "uh, an example of this is for say, a recommendation system. Um, uh, the features here are",
    "start": "717600",
    "end": "726194"
  },
  {
    "text": "different movies and, uh, the examples are customer ratings.",
    "start": "726195",
    "end": "735795"
  },
  {
    "text": "So we will have D, different movies and N, different customers, and each customer has filled in ratings for some of the movies.",
    "start": "735795",
    "end": "747450"
  },
  {
    "text": "So if D could be very large, if we look at the Netflix catalog and many, many thousands of different possible movies.",
    "start": "747450",
    "end": "754289"
  },
  {
    "text": "And so x, say is a vector in R, 10,000. But for each particular customer,",
    "start": "754289",
    "end": "762840"
  },
  {
    "text": "who may have only watched a few of the movies. And so most of the entries will be question marks, and then some of the entries will be rated.",
    "start": "762840",
    "end": "770970"
  },
  {
    "text": "Maybe they would be rated on a Likert scale from 1-5. Um, and then our job is to impute the entries, the missing entries.",
    "start": "770970",
    "end": "782925"
  },
  {
    "text": "And that tells us what rating the customer would give if they rated that particular movie.",
    "start": "782925",
    "end": "790150"
  },
  {
    "text": "And then we're going to use this information to actually make a recommendation. We will look for each customer at the imputed values and find,",
    "start": "790310",
    "end": "800640"
  },
  {
    "text": "um, those imputed values which are large. So movies that they would have rated large if only they'd rated it.",
    "start": "800640",
    "end": "810225"
  },
  {
    "text": "And we'll send a recommendation to them saying, \"You might like this movie.\"",
    "start": "810225",
    "end": "816160"
  },
  {
    "text": "And this is exactly, um, what is called the- the Netflix challenge where,",
    "start": "816860",
    "end": "825730"
  },
  {
    "text": "uh, uh, few years ago now there was a prize offered by Netflix for- to- to people to build a recommendation system,",
    "start": "826850",
    "end": "837000"
  },
  {
    "text": "uh, which could impute ratings in this way.",
    "start": "837000",
    "end": "841660"
  },
  {
    "text": "Another application would be to fit in missing features for supervised learning.",
    "start": "846770",
    "end": "852450"
  },
  {
    "text": "So you're trying to do supervised learning, you're trying to do classification or regression. And your x's have some missing features.",
    "start": "852450",
    "end": "862305"
  },
  {
    "text": "Um, and so far what we've done is remove the records that have fea- missing features.",
    "start": "862305",
    "end": "869700"
  },
  {
    "text": "Um, al- all of the methods we've seen so far, have required us to have every, uh, data record i,",
    "start": "869700",
    "end": "877680"
  },
  {
    "text": "have an x_i which is complete. It can't have any question marks for NA's in that.",
    "start": "877680",
    "end": "883440"
  },
  {
    "text": "Um, and sometimes you do lose a substantial fraction of the data. Um, so, uh, uh, so for example,",
    "start": "883440",
    "end": "893430"
  },
  {
    "text": "uh, some of the datasets we looked at earlier, for example, we looked at an Australian weather dataset.",
    "start": "893430",
    "end": "899790"
  },
  {
    "text": "Uh, we lost a substantial fraction of the data by eliminating those,",
    "start": "899790",
    "end": "905910"
  },
  {
    "text": "uh, those records which were just missing one element.",
    "start": "905910",
    "end": "911440"
  },
  {
    "text": "Um, and so an alternative approach would be to use into- imputation to fill in the missing feature entries.",
    "start": "911660",
    "end": "920220"
  },
  {
    "text": "And then use the filled-in dataset to do supervised learning.",
    "start": "920220",
    "end": "926680"
  },
  {
    "text": "Another thing you can use imputation for is to detect anomalous entries.",
    "start": "935120",
    "end": "940605"
  },
  {
    "text": "So here we're not trying to detect anomalous records, but anomalous entries.",
    "start": "940605",
    "end": "945720"
  },
  {
    "text": "So particular components of, uh, a particular x.",
    "start": "945720",
    "end": "952410"
  },
  {
    "text": "So what do we do? Well, for each i, we pretend that",
    "start": "952410",
    "end": "958470"
  },
  {
    "text": "the x_i can't be- the ith component of x is question mark is unknown.",
    "start": "958470",
    "end": "964545"
  },
  {
    "text": "And we impute to find x hat of i. We impute based on all the other entries of x.",
    "start": "964545",
    "end": "971820"
  },
  {
    "text": "And if x_i, and x hat_i are very different, well, then we'll flag x_i as anomalous.",
    "start": "971820",
    "end": "976950"
  },
  {
    "text": "Um, in the case of our movies example, we've identified movies which we would",
    "start": "976950",
    "end": "983910"
  },
  {
    "text": "expect based on all the other movies that the person has rated, that they would not have liked,",
    "start": "983910",
    "end": "990870"
  },
  {
    "text": "but actually they gave it a high rating or vice versa. [BACKGROUND]",
    "start": "990870",
    "end": "1002100"
  },
  {
    "text": "Um, now, the distinction between supervised learning and unsupervised learning is not so great.",
    "start": "1002100",
    "end": "1009240"
  },
  {
    "text": "Um, in particular, one can view supervised learning as a special case of imputation.",
    "start": "1009240",
    "end": "1016550"
  },
  {
    "text": "At least one can formulate supervised learning, as a special case of imputation.",
    "start": "1016550",
    "end": "1022205"
  },
  {
    "text": "Um, so suppose, we wanna predict y based on x. Well, and we have this training data of x_1, x_n, y_1, y_n.",
    "start": "1022205",
    "end": "1031755"
  },
  {
    "text": "We will construct a new set of records consisting of d plus m dimensional vectors, x tilde.",
    "start": "1031755",
    "end": "1039044"
  },
  {
    "text": "Each x tilde is simply x stacked up on top of y. Um, now,",
    "start": "1039045",
    "end": "1045795"
  },
  {
    "text": "we build a data model for x tilde using this training data and then we impute the last n entries of x tilde.",
    "start": "1045795",
    "end": "1055920"
  },
  {
    "text": "And it so happens that every record is missing the last n entries of x tilde.",
    "start": "1055920",
    "end": "1064725"
  },
  {
    "text": "Now, in order to use a data model to perform imputation,",
    "start": "1064725",
    "end": "1070304"
  },
  {
    "text": "we look at our vector x_i and we say,",
    "start": "1070305",
    "end": "1076800"
  },
  {
    "text": "well, we don't know what some of the entries are, but we know what the other entries are.",
    "start": "1076800",
    "end": "1083539"
  },
  {
    "text": "So the entries that we don't know, we're going to choose by picking them such that the loss function, l, is minimum.",
    "start": "1083540",
    "end": "1095005"
  },
  {
    "text": "So specifically, we say, well, I've got a loss function, it's a function of x.",
    "start": "1095005",
    "end": "1100125"
  },
  {
    "text": "We're going to fix the components that I know and I'm gonna allow myself to vary the components that I don't know.",
    "start": "1100125",
    "end": "1106750"
  },
  {
    "text": "And we minimize the implausibility, which is just the loss function.",
    "start": "1107060",
    "end": "1114010"
  },
  {
    "text": "This is a very natural thing to do, and it turns out that it works very well.",
    "start": "1114530",
    "end": "1120370"
  },
  {
    "text": "So here's an example. So here, we have a constant data model and-",
    "start": "1122600",
    "end": "1130350"
  },
  {
    "text": "so we have a bunch of data points here shown in red,",
    "start": "1130350",
    "end": "1135539"
  },
  {
    "text": "and we have our Theta, which is the parameter that specifies the model.",
    "start": "1135540",
    "end": "1144870"
  },
  {
    "text": "Our model says that we expect all of the data to be close to Theta.",
    "start": "1144870",
    "end": "1153105"
  },
  {
    "text": "In other words, we have a loss function, which is the norm of x minus Theta squared.",
    "start": "1153105",
    "end": "1159990"
  },
  {
    "text": "First thing we do is we pick the Theta given the data, and that gives us that the Theta is the mean of the data points.",
    "start": "1159990",
    "end": "1169575"
  },
  {
    "text": "The second thing we do is we say, well, we've got a Theta, and now we're going to find- solve an imputation problem.",
    "start": "1169575",
    "end": "1180270"
  },
  {
    "text": "In our imputation problem, we are given an x with a missing entry.",
    "start": "1180270",
    "end": "1188415"
  },
  {
    "text": "So we know x, it has two components.",
    "start": "1188415",
    "end": "1198165"
  },
  {
    "text": "The first one is unknown and the second one is 2.8, and that means that the true x is somewhere along this line,",
    "start": "1198165",
    "end": "1209950"
  },
  {
    "text": "somewhere along the line of vectors x, which have second component equal to 2.8.",
    "start": "1210500",
    "end": "1218625"
  },
  {
    "text": "We get to pick x_1, and the way we're gonna do that is by minimizing the loss function",
    "start": "1218625",
    "end": "1224970"
  },
  {
    "text": "over x_1 subject to the constraint that x_2 is 2.8,",
    "start": "1224970",
    "end": "1232690"
  },
  {
    "text": "and that gives us this point right here.",
    "start": "1232970",
    "end": "1239130"
  },
  {
    "text": "We get to move along this line, along that line, to get as close as possible as we can to Theta,",
    "start": "1239130",
    "end": "1248850"
  },
  {
    "text": "Theta being right there. And that turns out to be x hat 1 is 0.79.",
    "start": "1248850",
    "end": "1258880"
  },
  {
    "text": "If we have a k means data model, well, here, we have an x with some unknown entries.",
    "start": "1267650",
    "end": "1275145"
  },
  {
    "text": "Now, the loss function is the minimum over i of the norm of x minus Theta i squared.",
    "start": "1275145",
    "end": "1284070"
  },
  {
    "text": "So in other words, you look at role of your k different Theta vectors, your k of different archetypes.",
    "start": "1284070",
    "end": "1291135"
  },
  {
    "text": "Pick the one that's closest to x, and look at the distance between x, and that closest archetype squared,",
    "start": "1291135",
    "end": "1298485"
  },
  {
    "text": "and that gives you the loss. Now, we've got an x that has some missing entries.",
    "start": "1298485",
    "end": "1306045"
  },
  {
    "text": "And so when we do this- well, some of the entries are fixed, we can't do anything about those.",
    "start": "1306045",
    "end": "1311550"
  },
  {
    "text": "And, uh, the other entries, we get to minimize the loss function with respect to.",
    "start": "1311550",
    "end": "1318930"
  },
  {
    "text": "So the first thing we do is we say, well, let's find the nearest representative Theta j to x,",
    "start": "1318930",
    "end": "1326745"
  },
  {
    "text": "but we can only use the known entries. And that means that instead of looking at the 2-norm,",
    "start": "1326745",
    "end": "1332774"
  },
  {
    "text": "we have to look at the sum over all of the known entries,",
    "start": "1332775",
    "end": "1337980"
  },
  {
    "text": "the i's in k, and look at that sum of x_i minus the ith component to Theta_j squared.",
    "start": "1337980",
    "end": "1348840"
  },
  {
    "text": "That is the loss function.",
    "start": "1348840",
    "end": "1354150"
  },
  {
    "text": "Here, evaluated- let's include the minimum.",
    "start": "1354150",
    "end": "1359430"
  },
  {
    "text": "Evaluated where we've allowed the components that we don't know to be free.",
    "start": "1359430",
    "end": "1365325"
  },
  {
    "text": "And when we allow the components that we don't know to be free, well, they gravitate to be such that x_i is",
    "start": "1365325",
    "end": "1373965"
  },
  {
    "text": "equal to the ith component of Theta j because that minimizes that loss.",
    "start": "1373965",
    "end": "1380039"
  },
  {
    "text": "And so we end up with a loss that has only these terms left in it.",
    "start": "1380040",
    "end": "1387640"
  },
  {
    "text": "So just to be explicit about that, the minimum over i of",
    "start": "1390230",
    "end": "1398280"
  },
  {
    "text": "the norm of x minus Theta _i norm squared,",
    "start": "1398280",
    "end": "1404460"
  },
  {
    "text": "that's our l of x. And what we're trying to do is we're trying to minimize",
    "start": "1404460",
    "end": "1412245"
  },
  {
    "text": "l of x subject to the constraint that x hat i is equal to x_i for i in the known set.",
    "start": "1412245",
    "end": "1426970"
  },
  {
    "text": "And if I take the minimum of this l of x,",
    "start": "1427130",
    "end": "1434055"
  },
  {
    "text": "I end up with minimizing over x hat",
    "start": "1434055",
    "end": "1440445"
  },
  {
    "text": "the minimum over i of the norm x hat minus Theta.",
    "start": "1440445",
    "end": "1447570"
  },
  {
    "text": "Because I've used i twice, let's use j for one of them. So replace that one with a j.",
    "start": "1447570",
    "end": "1461860"
  },
  {
    "text": "x minimum over j minus theta j norm squared.",
    "start": "1467330",
    "end": "1477434"
  },
  {
    "text": "And this minimum on the outside is a minimum with respect to the unknown components.",
    "start": "1477435",
    "end": "1487240"
  },
  {
    "text": "Minimize with respect to",
    "start": "1488270",
    "end": "1493380"
  },
  {
    "text": "unknown components of x hat.",
    "start": "1493380",
    "end": "1502330"
  },
  {
    "text": "And if we minimize with respect to the unknown components of x hat, this quantity, well,",
    "start": "1502460",
    "end": "1510570"
  },
  {
    "text": "one thing I can do is swap the order of the minimizations,",
    "start": "1510570",
    "end": "1515745"
  },
  {
    "text": "and that becomes the minimum over j, the minimum over the unknowns",
    "start": "1515745",
    "end": "1523215"
  },
  {
    "text": "of the norm of x hat minus theta j.",
    "start": "1523215",
    "end": "1531059"
  },
  {
    "text": "And this, well, if I'm- if I'm allowed to choose the unknown components of x hat to minimize that quantity, what am I gonna do?",
    "start": "1531060",
    "end": "1540345"
  },
  {
    "text": "I'm gonna make the unknown components equal to the corresponding components of theta j.",
    "start": "1540345",
    "end": "1545775"
  },
  {
    "text": "And that leaves me with this part of the cost function here equal to just this.",
    "start": "1545775",
    "end": "1555130"
  },
  {
    "text": "So that, ah, that tells us how to pick the,",
    "start": "1557420",
    "end": "1563010"
  },
  {
    "text": "ah- the representative corresponding",
    "start": "1563010",
    "end": "1571890"
  },
  {
    "text": "to a particular x tells us how to pick j, and then what we want to do is we want to pick the unknown components.",
    "start": "1571890",
    "end": "1582270"
  },
  {
    "text": "Well, we already know what they are. They have to be equal such- they have to be such that x hat i is equal to the ith component of theta j.",
    "start": "1582270",
    "end": "1591130"
  },
  {
    "text": "So for the unknown entries, what we do is we guess the entries of the closest representative.",
    "start": "1591140",
    "end": "1600370"
  },
  {
    "text": "So we have a- a data model, and just like in supervised learning,",
    "start": "1600950",
    "end": "1608100"
  },
  {
    "text": "we need to be able to validate that our data model is actually good,",
    "start": "1608100",
    "end": "1614205"
  },
  {
    "text": "and our imputation method is actually good. And we do this in the following way.",
    "start": "1614205",
    "end": "1619635"
  },
  {
    "text": "We split the data into a training set and a test set, we use the training set to bill to the data model,",
    "start": "1619635",
    "end": "1626924"
  },
  {
    "text": "and then what we do is we look at the test set and we mask out some of the entries in the test set,",
    "start": "1626925",
    "end": "1635310"
  },
  {
    "text": "we pretend they're unknown. And we impute those entries and then look at the average error of the imputed values.",
    "start": "1635310",
    "end": "1647160"
  },
  {
    "text": "So the RMSE, for example. And that validates that our imputation method is working correctly.",
    "start": "1647160",
    "end": "1656880"
  },
  {
    "text": "And we would typically pick the training and the test split randomly or by K-fold validation,",
    "start": "1656880",
    "end": "1664230"
  },
  {
    "text": "and that would be fine, and then we would normally, to do the masking, we would typically mask randomly,",
    "start": "1664230",
    "end": "1670590"
  },
  {
    "text": "mask random entries within the test set.",
    "start": "1670590",
    "end": "1674049"
  },
  {
    "text": "So how do we fit a data model? We have, um, x_1 to x_n.",
    "start": "1680810",
    "end": "1688139"
  },
  {
    "text": "And ah, let's suppose we have no missing entries and we have a parameterized implausibility function l theta of x.",
    "start": "1688140",
    "end": "1699495"
  },
  {
    "text": "How do we choose the parameter theta? Well, we minimize the empirical risk, the average implausibility.",
    "start": "1699495",
    "end": "1710355"
  },
  {
    "text": "That's 1 on n times the sum from i is 1 to n of l theta of x_i.",
    "start": "1710355",
    "end": "1715740"
  },
  {
    "text": "We choose theta to minimize this, and we may have some constraints on theta.",
    "start": "1715740",
    "end": "1721980"
  },
  {
    "text": "Sometimes the allowed thetas are limited in some way.",
    "start": "1721980",
    "end": "1727630"
  },
  {
    "text": "Um, and then we choose the parameter theta so",
    "start": "1727630",
    "end": "1732960"
  },
  {
    "text": "that the observed data is the least implausible.",
    "start": "1732960",
    "end": "1738730"
  },
  {
    "text": "Let's look at the simplest case, the sum of squares loss. So loss function is l theta of x is the norm of x minus theta squared.",
    "start": "1742160",
    "end": "1751560"
  },
  {
    "text": "Empirical loss is the average of that quantity. And we've seen this before when we were looking at",
    "start": "1751560",
    "end": "1759420"
  },
  {
    "text": "the constant predictor problem with the square loss, that the best choice of theta,",
    "start": "1759420",
    "end": "1767835"
  },
  {
    "text": "the minimizing choice of theta, is the mean of the data vectors.",
    "start": "1767835",
    "end": "1772509"
  },
  {
    "text": "If we have the absolute loss,",
    "start": "1774080",
    "end": "1779490"
  },
  {
    "text": "some absolute loss, well again, this is, ah, exactly like the constant predictor case.",
    "start": "1779490",
    "end": "1786150"
  },
  {
    "text": "Here in the unsupervised case, we will have a loss function which is the norm of x minus theta.",
    "start": "1786150",
    "end": "1792270"
  },
  {
    "text": "The 1 norm is the sum of the absolute values of the components of x minus theta.",
    "start": "1792270",
    "end": "1798285"
  },
  {
    "text": "And we look at the empirical loss, and the optimal theta is the median of the data vectors x_1 through x_n.",
    "start": "1798285",
    "end": "1807450"
  },
  {
    "text": "And what this means is the element-wise median of the data vectors.",
    "start": "1807450",
    "end": "1812830"
  },
  {
    "text": "And the k means model goes like this.",
    "start": "1816800",
    "end": "1824340"
  },
  {
    "text": "The implausibility function or the loss function is the- the norm",
    "start": "1824340",
    "end": "1830580"
  },
  {
    "text": "squared of the distance between x and the closest archetype theta j,",
    "start": "1830580",
    "end": "1838710"
  },
  {
    "text": "and so our parameter is a d by k matrix. Equivalently, it's simply K d-dimensional vectors theta 1 through theta",
    "start": "1838710",
    "end": "1846900"
  },
  {
    "text": "k. And the empirical loss is therefore the,",
    "start": "1846900",
    "end": "1853260"
  },
  {
    "text": "ah, ah, average of that over all of the data points.",
    "start": "1853260",
    "end": "1859215"
  },
  {
    "text": "So it's one on n, the sum from i is 1 to n, and then for each i, we have to find the closest, ah,",
    "start": "1859215",
    "end": "1866865"
  },
  {
    "text": "archetypes theta j and look at the distance between x_i and theta j.",
    "start": "1866865",
    "end": "1872325"
  },
  {
    "text": "And of course, which closest archetype that is depends on which data point you have. And this is the- called the k-means objective function.",
    "start": "1872325",
    "end": "1882900"
  },
  {
    "text": "And we use an algorithm called the k-means algorithm to minimize this.",
    "start": "1882900",
    "end": "1889930"
  },
  {
    "text": "So this is one of the cases where we have a specific problem,",
    "start": "1892280",
    "end": "1897479"
  },
  {
    "text": "which is the k-means optimization problem we've discussed, and the algorithm has the same name,",
    "start": "1897479",
    "end": "1903000"
  },
  {
    "text": "which is a little confusing cause one could use different algorithms to solve the same problem.",
    "start": "1903000",
    "end": "1908865"
  },
  {
    "text": "So- but the most commonly used algorithm is called the k-means algorithm.",
    "start": "1908865",
    "end": "1914415"
  },
  {
    "text": "And the way it works is that for any given choice of these k vectors theta 1 through theta k,",
    "start": "1914415",
    "end": "1925125"
  },
  {
    "text": "well, I've got these k vectors and then I've got a bunch of data points.",
    "start": "1925125",
    "end": "1930315"
  },
  {
    "text": "And each data point, its contribution to the loss is the distance to its closest archetype.",
    "start": "1930315",
    "end": "1938100"
  },
  {
    "text": "And so we can say let's assign to each data point a corresponding archetype,",
    "start": "1938100",
    "end": "1945405"
  },
  {
    "text": "the archetype which is the closest to that data point.",
    "start": "1945405",
    "end": "1950820"
  },
  {
    "text": "And we're gonna label that assignment with a vector c in R_n.",
    "start": "1950820",
    "end": "1956789"
  },
  {
    "text": "And the idea is- is that c_i is a number between one and k,",
    "start": "1956790",
    "end": "1963720"
  },
  {
    "text": "which tells us which of the thetas has been assigned to that data point. Otherwise, which of the thetas is the closest to that data point.",
    "start": "1963720",
    "end": "1972880"
  },
  {
    "text": "And then this loss function, which is the empirical risk,",
    "start": "1973280",
    "end": "1979740"
  },
  {
    "text": "is one on n, the sum from i is 1 to n of the distance squared between x_i and its closest archetype.",
    "start": "1979740",
    "end": "1989820"
  },
  {
    "text": "Well, that's explicitly 1 on n times the sum from i is 1 to n of this quantity x_i minus theta c_i,",
    "start": "1989820",
    "end": "2000935"
  },
  {
    "text": "because c_i is the assignment.",
    "start": "2000935",
    "end": "2004530"
  },
  {
    "text": "Now, what we want to do is we want to choose both c and the thetas.",
    "start": "2006190",
    "end": "2013888"
  },
  {
    "text": "So we have to choose the assignment and choose the corresponding thetas.",
    "start": "2015870",
    "end": "2021380"
  },
  {
    "text": "Once we've chosen the thetas, the assignment is easy. All right? The assignment is that we assign to it,",
    "start": "2021380",
    "end": "2029059"
  },
  {
    "text": "to the ith data point, the closest archetype. So we find over j the minimum of x_i minus theta j,",
    "start": "2029060",
    "end": "2038120"
  },
  {
    "text": "the norm of x_i minus theta j squared. And that's what c_i is. It's- it's which of the js minimizes that quantity.",
    "start": "2038120",
    "end": "2046350"
  },
  {
    "text": "How do we minimize the Thetas? Well, we can minimize",
    "start": "2049080",
    "end": "2058990"
  },
  {
    "text": "this quantity once we fixed to the c_is.",
    "start": "2058990",
    "end": "2065440"
  },
  {
    "text": "So first of all, we'll do the assignment, we'll find the best c_is.",
    "start": "2065440",
    "end": "2070855"
  },
  {
    "text": "And then minimizing the c is just straight for- over the c is just straightforward because",
    "start": "2070855",
    "end": "2077350"
  },
  {
    "text": "each of these, this quantity",
    "start": "2077350",
    "end": "2083560"
  },
  {
    "text": "here splits up into k different terms. One for those data points assigned to Theta_1,",
    "start": "2083560",
    "end": "2093145"
  },
  {
    "text": "another for the data points assigned to Theta_2 and so on. And so it becomes-",
    "start": "2093145",
    "end": "2099440"
  },
  {
    "text": "well, it becomes this. It becomes, uh, 1 on n times the sum over all i such that c_i is 1 of",
    "start": "2112320",
    "end": "2124800"
  },
  {
    "text": "the norm of x_i minus Theta_1 squared plus 1 on n times the sum over all",
    "start": "2124800",
    "end": "2134490"
  },
  {
    "text": "i such that c_i is 2 over the norm of x_i minus Theta_2 squared and so on up to k.",
    "start": "2134490",
    "end": "2146330"
  },
  {
    "text": "And each one of those, we- we would like to minimize that sum over Theta_1 through Theta_k.",
    "start": "2159990",
    "end": "2166930"
  },
  {
    "text": "Instead to find the minimum over Theta_1, we minimize this over Theta_1.",
    "start": "2166930",
    "end": "2172240"
  },
  {
    "text": "And minimizing that over Theta_1 is simply step- tells us to set Theta_1 to be the mean of the corresponding x_is.",
    "start": "2172240",
    "end": "2180490"
  },
  {
    "text": "To minimize this over Theta_2, we set Theta_2 to be the mean of the corresponding x_is.",
    "start": "2180490",
    "end": "2188745"
  },
  {
    "text": "In other words, all of the x_is which had been assigned to category 2, and so on.",
    "start": "2188745",
    "end": "2197210"
  },
  {
    "text": "Once we do that, well, then we've got new Theta_is and that means that the assignment might change.",
    "start": "2197850",
    "end": "2205190"
  },
  {
    "text": "And so we reassign. We go- we- we go through and once again pick",
    "start": "2206550",
    "end": "2215320"
  },
  {
    "text": "the c_is that- that assign each x_i to the closest Theta_J.",
    "start": "2215320",
    "end": "2224050"
  },
  {
    "text": "That's a new assignment. And that means that the Thetas are gonna change. And so we go through and we update all the Thetas to be",
    "start": "2224050",
    "end": "2231700"
  },
  {
    "text": "the corresponding means of their assigned data points. And we alternate between these two steps.",
    "start": "2231700",
    "end": "2239080"
  },
  {
    "text": "Assign data points to archetypes,",
    "start": "2239080",
    "end": "2244915"
  },
  {
    "text": "then adjust the archetypes to be the mean or their assigned data points,",
    "start": "2244915",
    "end": "2251185"
  },
  {
    "text": "then assign data points to archetypes, and so on. And this is a heuristic for approximately minimizing this empirical risk.",
    "start": "2251185",
    "end": "2263000"
  },
  {
    "text": "Here's an example. So we start out with,",
    "start": "2264810",
    "end": "2271075"
  },
  {
    "text": "uh, some guesses for Theta,",
    "start": "2271075",
    "end": "2276325"
  },
  {
    "text": "and we might initialize those completely randomly. That would be very common. So here there are three guesses.",
    "start": "2276325",
    "end": "2282640"
  },
  {
    "text": "This is say, Theta_1, Theta_2, and Theta_3.",
    "start": "2282640",
    "end": "2289869"
  },
  {
    "text": "Um, and now, given those guesses for the Thetas,",
    "start": "2289870",
    "end": "2297490"
  },
  {
    "text": "we can make assignments. All of the points for which- which are closest to",
    "start": "2297490",
    "end": "2303099"
  },
  {
    "text": "Theta_1 than any of the other Thetas have been colored in red.",
    "start": "2303100",
    "end": "2308230"
  },
  {
    "text": "The points that are closest to Theta_2 are colored blue, and the points that are closest to Theta_3 are colored green.",
    "start": "2308230",
    "end": "2314560"
  },
  {
    "text": "Remember, we don't have any colors associated with our data. Our data is just points.",
    "start": "2314560",
    "end": "2319585"
  },
  {
    "text": "But once we've picked Thetas, we can label the data points.",
    "start": "2319585",
    "end": "2327520"
  },
  {
    "text": "And those labels are the c_is. So all of these red points, so those points for which c_i is 1,",
    "start": "2327520",
    "end": "2335185"
  },
  {
    "text": "the blue points are points which c_i is 2, and the green points are points which c_i is 3.",
    "start": "2335185",
    "end": "2341964"
  },
  {
    "text": "So now we've got an assignment, right? We've picked the c's by painting the points in their appropriate colors.",
    "start": "2341965",
    "end": "2351715"
  },
  {
    "text": "Now we can see that well, these Thetas don't minimize the empirical risk with those assignments,",
    "start": "2351715",
    "end": "2361285"
  },
  {
    "text": "because I can make the empirical risk smaller by moving this Theta to the mean of all of these data points,",
    "start": "2361285",
    "end": "2368560"
  },
  {
    "text": "which is somewhere here. And I can move this Theta to the mean if it's assigned to data points there.",
    "start": "2368560",
    "end": "2376015"
  },
  {
    "text": "And this one doesn't move very much. Maybe it moves a little bit in, uh, this direction, say.",
    "start": "2376015",
    "end": "2383230"
  },
  {
    "text": "So I move them and this is where they end up.",
    "start": "2383230",
    "end": "2387830"
  },
  {
    "text": "And now that I've moved them, I can reassign each data point to its closest archetype that changes their colors.",
    "start": "2388620",
    "end": "2397194"
  },
  {
    "text": "So now I've got the same data points, but I've recolored them according to their closest archetype.",
    "start": "2397195",
    "end": "2402505"
  },
  {
    "text": "So now these are the red ones here, these are the blue ones, these are the green ones.",
    "start": "2402505",
    "end": "2408954"
  },
  {
    "text": "And we can see that some data points have changed from red to blue and some have changed from blue to red. None of the green ones have changed.",
    "start": "2408955",
    "end": "2416870"
  },
  {
    "text": "And so now, I've reassigned data points. My archetypes are no longer in the best possible place.",
    "start": "2417030",
    "end": "2424345"
  },
  {
    "text": "I adjust my archetypes to be the mean of their corresponding data points.",
    "start": "2424345",
    "end": "2431095"
  },
  {
    "text": "So this one's going to move up a bit, this one's going to move down a bit. And, uh, I keep going until I converge,",
    "start": "2431095",
    "end": "2439630"
  },
  {
    "text": "alternating between assigning colors and taking the means.",
    "start": "2439630",
    "end": "2445039"
  },
  {
    "text": "And here's this little convergence you see where this is converged very quickly. After four iterations, we've completely converged.",
    "start": "2450360",
    "end": "2459880"
  },
  {
    "text": "And- and notice that this converges perfectly in the sense that once we've- uh,",
    "start": "2459880",
    "end": "2469960"
  },
  {
    "text": "once we're not changing the assignments c anymore,",
    "start": "2469960",
    "end": "2475555"
  },
  {
    "text": "well, then the Thetas don't change either. And so this isn't, uh, uh,",
    "start": "2475555",
    "end": "2481029"
  },
  {
    "text": "this reaches a point where the Thetas are stuck and no longer move at all.",
    "start": "2481030",
    "end": "2487580"
  },
  {
    "text": "Here, we can look at the, uh, both the te- the training loss and the test loss and the imputation error.",
    "start": "2492270",
    "end": "2502165"
  },
  {
    "text": "And so you can see here in blue, there's the training loss. Um, in green, you can see the test loss follows pretty closely.",
    "start": "2502165",
    "end": "2511449"
  },
  {
    "text": "And then the red is the imputation error. And this tells- and this is as a function of k,",
    "start": "2511449",
    "end": "2519145"
  },
  {
    "text": "the number- the parameter, which is the number of archetypes we're choosing.",
    "start": "2519145",
    "end": "2525250"
  },
  {
    "text": "And so how do we choose this? Well, this suggests that maybe we should pick a k",
    "start": "2525250",
    "end": "2532674"
  },
  {
    "text": "somewhere around 4 or 5 or maybe even 3.",
    "start": "2532675",
    "end": "2538195"
  },
  {
    "text": "Um, so I guess this is 1, this is 2, 3.",
    "start": "2538195",
    "end": "2543340"
  },
  {
    "text": "So we should pick a k that's 3, which is quite reasonable if we look at our dataset.",
    "start": "2543340",
    "end": "2548770"
  },
  {
    "text": "Our dataset really does split naturally into three clusters,",
    "start": "2548770",
    "end": "2555295"
  },
  {
    "text": "and our algorithm has found that.",
    "start": "2555295",
    "end": "2559130"
  },
  {
    "text": "And then we can validate by removing either",
    "start": "2562680",
    "end": "2567819"
  },
  {
    "text": "u_1 or u_2 from each record in the test set and computing the RMSE.",
    "start": "2567820",
    "end": "2574010"
  }
]