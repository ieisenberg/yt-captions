[
  {
    "text": "so i'm excited today to talk to you about how we can use human input in order to develop robust ai systems",
    "start": "11120",
    "end": "18000"
  },
  {
    "text": "and so if we look at our world today we're increasingly coming in contact with ai systems whether it be through social media recommender systems the",
    "start": "18000",
    "end": "25119"
  },
  {
    "text": "online advertisements that we see as we browse the web additionally behind the scenes more and more we're starting to see robots",
    "start": "25119",
    "end": "30320"
  },
  {
    "text": "working in warehouses and factories fulfilling orders helping us deal with supply chain issues and a lot of",
    "start": "30320",
    "end": "36079"
  },
  {
    "text": "companies are starting to think about how can we bring robots and other ai systems into hospitals into our homes",
    "start": "36079",
    "end": "41760"
  },
  {
    "text": "into grocery stores as well as in to our roads and driving on our streets and i think in the coming days and in",
    "start": "41760",
    "end": "48719"
  },
  {
    "text": "the future these a systems are going to become even more prevalent and so one of the things that really",
    "start": "48719",
    "end": "54160"
  },
  {
    "text": "motivates my research is this idea that we want to get these robots and ais systems out there in the real world",
    "start": "54160",
    "end": "59440"
  },
  {
    "text": "interacting with people learning from them adapting to their preferences and needs",
    "start": "59440",
    "end": "64559"
  },
  {
    "text": "and so one of the big problems that these systems face is what i'm going to call value alignment which is basically how do we",
    "start": "64559",
    "end": "71439"
  },
  {
    "text": "get these ai systems to do what we as humans actually want them to do and you might say well maybe that's not",
    "start": "71439",
    "end": "77840"
  },
  {
    "text": "that big of a problem you know i'm pretty good at writing down objective functions having ai systems optimize them and a lot of",
    "start": "77840",
    "end": "84240"
  },
  {
    "text": "objectives that we think about in robotics or machine learning do on the surface seem pretty well defined so we can think about classification we say",
    "start": "84240",
    "end": "91040"
  },
  {
    "text": "okay we just want to maximize our accuracy we'll just collect a lot of data label it have a test set",
    "start": "91040",
    "end": "97600"
  },
  {
    "text": "maybe that's a pretty simple objective similarly we might think about like a motion planning problem where it seems",
    "start": "97600",
    "end": "102799"
  },
  {
    "text": "pretty simple we can define a start the goal maybe some obstacles we want to avoid and then the objective also seems",
    "start": "102799",
    "end": "108159"
  },
  {
    "text": "pretty clear we want to go from a to b without colliding with something we also might think about robot grasping",
    "start": "108159",
    "end": "113920"
  },
  {
    "text": "as a pretty simple you know problem to specify an objective for so this is showing some of our work recently where",
    "start": "113920",
    "end": "119600"
  },
  {
    "text": "we're looking at how do we grasp unknown objects that may be adversarial and here we do give the robot just a very simple",
    "start": "119600",
    "end": "125360"
  },
  {
    "text": "objective of a reward of plus one every time it is able to successfully grasp the object a word of zero otherwise and",
    "start": "125360",
    "end": "131840"
  },
  {
    "text": "we use band algorithms to allow the system to iteratively over time improve its performance",
    "start": "131840",
    "end": "138959"
  },
  {
    "text": "but i think if we dig a little bit deeper into these problems we'll see that things maybe aren't quite as well defined as we they initially think so if",
    "start": "139040",
    "end": "145599"
  },
  {
    "text": "we think about classification i'd argue that no one really cares about just maximizing average accuracy there's",
    "start": "145599",
    "end": "151200"
  },
  {
    "text": "often subtle nuances in terms of how much we care about true positives or false positives and false negatives for",
    "start": "151200",
    "end": "157120"
  },
  {
    "text": "example a driving system we wanted to be like pretty good at detecting like stop lights and",
    "start": "157120",
    "end": "163200"
  },
  {
    "text": "stop signs but we may allow for some error in terms of misclassifying a car as a truck",
    "start": "163200",
    "end": "169120"
  },
  {
    "text": "and ideally we don't want to just have systems that can you know recognize things in image but we want to have systems that can drive down busy streets",
    "start": "169120",
    "end": "175519"
  },
  {
    "text": "in ways that are safe and efficient and think about what kind of objective function we'd have to write down to get",
    "start": "175519",
    "end": "181120"
  },
  {
    "text": "an ai system to do this is actually very challenging because of all the different tradeoffs and factors involved",
    "start": "181120",
    "end": "187680"
  },
  {
    "text": "similarly if we think about something like a motion planning problem things start to get a lot more difficult if we were maybe carrying a hot liquid now we",
    "start": "187680",
    "end": "194879"
  },
  {
    "text": "have all these very complex constraints we have to take into account and what if rather than just moving the end effector",
    "start": "194879",
    "end": "199920"
  },
  {
    "text": "we're now moving a person from point a to point b now these constraints are really like depend on the person often",
    "start": "199920",
    "end": "206400"
  },
  {
    "text": "very hard to write down and so this problem actually becomes very hard to specify",
    "start": "206400",
    "end": "212560"
  },
  {
    "text": "and then think about grasping you know what does it actually mean to grasp an object successfully the scratch looks okay turns out it's not",
    "start": "212560",
    "end": "219360"
  },
  {
    "text": "and really what makes the grass good depends on how we're going to use this grass about the affordances that we're going to do and really like we want to",
    "start": "219360",
    "end": "226239"
  },
  {
    "text": "move beyond just you know picking up an object to do something useful like coming into these kids room helping them",
    "start": "226239",
    "end": "231519"
  },
  {
    "text": "organize clean up their room and you know we don't want it to just you know maybe sweep all the toys under",
    "start": "231519",
    "end": "236879"
  },
  {
    "text": "the rug push them into the closet i tried that once as a kid my mom told me no it doesn't match my specification but",
    "start": "236879",
    "end": "242560"
  },
  {
    "text": "i think she would have trouble you know writing down her actual specification i would as well of what does it mean to",
    "start": "242560",
    "end": "249360"
  },
  {
    "text": "organize nicely and you know put away the toys in this room so i'd argue that if we think about the",
    "start": "249360",
    "end": "255120"
  },
  {
    "text": "problems that we're really excited about as roboticists and ai and machine learning learning researchers that these",
    "start": "255120",
    "end": "260799"
  },
  {
    "text": "objectives are often really hard to specify and i'd go further and say that i would",
    "start": "260799",
    "end": "266240"
  },
  {
    "text": "say a robust ai is really human-centered ai kind of going back to these problems we really need some interaction with the",
    "start": "266240",
    "end": "272160"
  },
  {
    "text": "person to help us figure out what these systems should be doing so i'm going to define robustness as",
    "start": "272160",
    "end": "278000"
  },
  {
    "text": "acceptable behavior in the presence of uncertainty and unusual circumstances",
    "start": "278000",
    "end": "283120"
  },
  {
    "text": "i'd like to highlight these keywords here acceptable behavior i think this really brings us back to",
    "start": "283120",
    "end": "288160"
  },
  {
    "text": "this idea of value alignment which is you know how do we get these systems to do what we as people really",
    "start": "288160",
    "end": "294080"
  },
  {
    "text": "want apologies on the link and so how does this work though when",
    "start": "294080",
    "end": "299280"
  },
  {
    "text": "we're learning from people this is often very difficult right people are noisy we make mistakes",
    "start": "299280",
    "end": "304720"
  },
  {
    "text": "often human input is ambiguous and looking at what people did in hindsight often just looks very irrational at",
    "start": "304720",
    "end": "311120"
  },
  {
    "text": "times but i'd argue that rather than trying to like minimize human input or treat human input as a disturbance we",
    "start": "311120",
    "end": "317919"
  },
  {
    "text": "really want to build ai systems from the ground up to think about how can we leverage human input how can we deal",
    "start": "317919",
    "end": "323759"
  },
  {
    "text": "with these uncertainties and ambiguities in order to get our a systems to behave the way we want",
    "start": "323759",
    "end": "328800"
  },
  {
    "text": "so kind of in a nutshell like the one sentence summary of the work that i do is i try and incorporate efficiently",
    "start": "328800",
    "end": "334720"
  },
  {
    "text": "human input into both the theory and the practice of robust ai systems and",
    "start": "334720",
    "end": "340320"
  },
  {
    "text": "kind of at high level i look at these three different areas so human factors robotics and machine",
    "start": "340320",
    "end": "345680"
  },
  {
    "text": "learning working within the union of all them and more often than not in the intersection of these so i've been",
    "start": "345680",
    "end": "351120"
  },
  {
    "text": "looking to work on a variety of really fun and fascinating problems ranging from human swarm interactions how do we",
    "start": "351120",
    "end": "356400"
  },
  {
    "text": "get humans to manage these multi-agent collectives looking at shared autonomy assistive robotics autonomous driving",
    "start": "356400",
    "end": "362639"
  },
  {
    "text": "how do we get these cars to navigate roads alongside humans using models of human intent as well as looking at",
    "start": "362639",
    "end": "368240"
  },
  {
    "text": "robust policy optimization so how can we develop algorithms that are robust to uncertainty and really hedge against",
    "start": "368240",
    "end": "374319"
  },
  {
    "text": "this uncertainty that they might have a lot of my work is focused on what we've talked about this idea of value alignment or learning reward functions",
    "start": "374319",
    "end": "380880"
  },
  {
    "text": "models of human intent as well as looking at ideas for interactive imitation learning and robust grasping and kidding of objects",
    "start": "380880",
    "end": "388240"
  },
  {
    "text": "so i don't have time to talk about all these today but i'd like to focus on these three topics",
    "start": "388240",
    "end": "393440"
  },
  {
    "text": "under this kind of larger umbrella of value alignment how do we learn specifications for a systems from human",
    "start": "393440",
    "end": "399120"
  },
  {
    "text": "input and there's these kind of fundamental challenges that i'm going to talk about today and one is how do we estimate",
    "start": "399120",
    "end": "405600"
  },
  {
    "text": "uncertainty so if i'm learning from a person how do i estimate and really think about you know",
    "start": "405600",
    "end": "411520"
  },
  {
    "text": "my uncertainty and the different hypotheses that might explain the human behavior also talk about how can we be robust to that",
    "start": "411520",
    "end": "417840"
  },
  {
    "text": "uncertainty how we hedge against risk and hedge our bets and then finally how do we actively",
    "start": "417840",
    "end": "423840"
  },
  {
    "text": "reduce the uncertainty how can we ask targeted queries that allow these a system to reduce uncertainty and really figure out what they need to know",
    "start": "423840",
    "end": "431599"
  },
  {
    "text": "and so a lot of my work builds on this idea of reinforcement learning which i think most of you are probably familiar with where we have an agent interacting",
    "start": "431599",
    "end": "436720"
  },
  {
    "text": "with environment taking actions receiving rewards and observations and as a lot of you probably know this",
    "start": "436720",
    "end": "443360"
  },
  {
    "text": "idea of reward engineering is actually quite difficult trying to figure out what reward function we should place there to get the desired behavior is",
    "start": "443360",
    "end": "450720"
  },
  {
    "text": "often challenging that often is this interactive process where an engineer is going to you know write down a reward function optimize it say oh no let me",
    "start": "450720",
    "end": "457039"
  },
  {
    "text": "tweak a few weights here and there and this is often a painful process and there's a lot of tasks that we could",
    "start": "457039",
    "end": "462560"
  },
  {
    "text": "say that probably showing is maybe easier than telling so driving is a classic example where it's really hard for me to",
    "start": "462560",
    "end": "468960"
  },
  {
    "text": "sit down and write down a reward function that describes all the important things for driving but it's very easy for me for for me to hop into",
    "start": "468960",
    "end": "475360"
  },
  {
    "text": "a car and show you an example of you know pretty good driving behavior and so a lot of my work looks at this",
    "start": "475360",
    "end": "480879"
  },
  {
    "text": "idea of inverse reinforcement learning where we're now taking the role of the observer we're watching a human interact",
    "start": "480879",
    "end": "486479"
  },
  {
    "text": "with the environment we assume that they have some underlying cost function or reward function they're trying to optimize and then the question here is",
    "start": "486479",
    "end": "493120"
  },
  {
    "text": "you know why did the human do what they did what was the human's reward function ideally we want to learn this function",
    "start": "493120",
    "end": "499440"
  },
  {
    "text": "you know we can take observations actions and map it to the scalar real valued reward signal telling us how good",
    "start": "499440",
    "end": "505599"
  },
  {
    "text": "we think the human thinks these actions or observations are and so you might ask well why not just",
    "start": "505599",
    "end": "511840"
  },
  {
    "text": "directly imitate if we're interested in getting behavior that's aligned with what the human does maybe we can just watch the human see what actions they",
    "start": "511840",
    "end": "518479"
  },
  {
    "text": "take based on different observations i'm just asking what would the human have done in this situation",
    "start": "518479",
    "end": "523839"
  },
  {
    "text": "and here we might want to learn like a policy right that maps from observations into actions just directly using",
    "start": "523839",
    "end": "530080"
  },
  {
    "text": "standard supervised learning and this is you know well studied and often you know works really well it's",
    "start": "530080",
    "end": "535440"
  },
  {
    "text": "called behavioral cloning but there are cases where we probably just don't want robots going through the motions so this is a funny video i found",
    "start": "535440",
    "end": "542160"
  },
  {
    "text": "of kind of this like satirical instance of robot just going through the motions just kind of like i know what i should do here and there",
    "start": "542160",
    "end": "548640"
  },
  {
    "text": "giving someone breakfast pouring the milk now i'm going to pick up the spoon and feed the person",
    "start": "548640",
    "end": "553839"
  },
  {
    "text": "and so this is designed to be funny but i think it drives home the point that we don't want ai systems or robots that",
    "start": "553839",
    "end": "559839"
  },
  {
    "text": "just go through the motions and don't understand why they're doing anything and this is not just satire so we've",
    "start": "559839",
    "end": "565519"
  },
  {
    "text": "actually seen this in the lab so this is a project that i've been working on with an undergrad where we're learning how to",
    "start": "565519",
    "end": "570720"
  },
  {
    "text": "do these assistive feeding and assistive care tasks with a human and we tried using behavioral cloning where we gave a",
    "start": "570720",
    "end": "576959"
  },
  {
    "text": "couple really good demonstrations to the robot and these and then we did behavioral cloning to learn a policy",
    "start": "576959",
    "end": "582240"
  },
  {
    "text": "however when things changed a little bit when the human's head is in a slightly different position the robot starts in slightly different position the robot",
    "start": "582240",
    "end": "588800"
  },
  {
    "text": "just doesn't really know what to do it's just kind of learned to mimic the motions it knows it should get close to the human but because things are a",
    "start": "588800",
    "end": "594240"
  },
  {
    "text": "little bit different than what it was trained on it just diverges and ends up just kind of hovering over the person's head just totally missing the mark",
    "start": "594240",
    "end": "600640"
  },
  {
    "text": "however if we learn a reward function that actually maps what the human really wants and allows us to optimize to",
    "start": "600640",
    "end": "606880"
  },
  {
    "text": "achieve that objective then we can get really good performance and actually accomplish a task even in different situations",
    "start": "606880",
    "end": "614480"
  },
  {
    "text": "so hopefully i've convinced you now that infrared human intent is important for robustness by the end of the talk i'm",
    "start": "614560",
    "end": "620079"
  },
  {
    "text": "actually going to return to this idea of behavioral cloning and talk about how we can use human interactions to make that",
    "start": "620079",
    "end": "625279"
  },
  {
    "text": "more robust as well so we've talked a lot about this idea of learning reward functions",
    "start": "625279",
    "end": "631279"
  },
  {
    "text": "from human demonstrations that we can then turn into a policy via policy optimization",
    "start": "631279",
    "end": "636399"
  },
  {
    "text": "and this idea like we talked about called inverse reinforcement learning but there are some common problems with a lot of these approaches",
    "start": "636399",
    "end": "642800"
  },
  {
    "text": "one is that typically prior work couldn't do much better than the demonstrator and this is often by design",
    "start": "642800",
    "end": "649200"
  },
  {
    "text": "a lot of these systems are trying to find a reward function that if i optimize it leads to the behavior that i saw in the demonstrator",
    "start": "649200",
    "end": "655600"
  },
  {
    "text": "and so we're trying to find a reward function that rationalizes what the demonstrator did you know warts and all so if the demonstrator makes some",
    "start": "655600",
    "end": "660959"
  },
  {
    "text": "mistakes we're going to say hey that was probably intentional and we can't really do much better",
    "start": "660959",
    "end": "666160"
  },
  {
    "text": "another problem is that these problems are often hard to scale the complex problems because often they have on the",
    "start": "666160",
    "end": "672320"
  },
  {
    "text": "inner loop some kind of an mdp solver so something that's running optimal control or reinforcement learning over",
    "start": "672320",
    "end": "678000"
  },
  {
    "text": "and over again so we had this idea back in 2019 that we could use pre-ranked demonstrations in order to try and address these problems",
    "start": "678000",
    "end": "684959"
  },
  {
    "text": "one thing that's nice is if we have a set of ranked demonstrations so example a which is better than example b",
    "start": "684959",
    "end": "690560"
  },
  {
    "text": "which is better than example c we can now think of a reward function that tries to explain that ranking but potentially allows us to extrapolate and",
    "start": "690560",
    "end": "697040"
  },
  {
    "text": "think about things that might be even better than the best example we've seen and what's cool is this is actually going to turn reward learning now into a",
    "start": "697040",
    "end": "702959"
  },
  {
    "text": "supervised learning problem making things a lot more efficient and scalable so we call this approach trajectory",
    "start": "702959",
    "end": "708959"
  },
  {
    "text": "ranked reward extrapolation or t-rex where the idea is that we're going to take a set of pre-ranked demonstrations",
    "start": "708959",
    "end": "715040"
  },
  {
    "text": "here's a simulated half sheet environment the robot is trying to run to the right and we want to run the",
    "start": "715040",
    "end": "720160"
  },
  {
    "text": "right as far and as fast as it can we give it a set of examples ranging from it's just pretty awful scooting out its",
    "start": "720160",
    "end": "725279"
  },
  {
    "text": "face to something that's kind of like a little toddler that can kind of run along but occasionally face plans and we",
    "start": "725279",
    "end": "730639"
  },
  {
    "text": "give it about 15 of these examples ranked and we learn a reward function and the cool thing is when we optimize",
    "start": "730639",
    "end": "736320"
  },
  {
    "text": "this reward function learn from sub-optimal behavior we get really good behavior that runs about twice as fast and far as the best demonstration that",
    "start": "736320",
    "end": "743040"
  },
  {
    "text": "we've seen so how does this work so i'd like to kind of ground things a little bit so",
    "start": "743040",
    "end": "748240"
  },
  {
    "text": "what does actually mean to learn a reward function so this is a function right that maps from states s into the",
    "start": "748240",
    "end": "753360"
  },
  {
    "text": "real valued numbers and it's typically parametrized by some model represented by theta it could",
    "start": "753360",
    "end": "760160"
  },
  {
    "text": "be like a linear model or a deep neural network model and so some examples of states it could be like robot joints",
    "start": "760160",
    "end": "766160"
  },
  {
    "text": "angles and velocities in this case maybe we'd want a map you know something like this state to a positive reward maybe something is face",
    "start": "766160",
    "end": "772720"
  },
  {
    "text": "planting to a negative reward it could also be short sequences of images if we're doing a visual task",
    "start": "772720",
    "end": "778320"
  },
  {
    "text": "so maybe it's a short snippet of this you know agent driving and you can see here it's passing a few cars so maybe",
    "start": "778320",
    "end": "783440"
  },
  {
    "text": "that's like a good thing here it's getting passed maybe that's a bad thing ideally as a designer i don't have to go",
    "start": "783440",
    "end": "789040"
  },
  {
    "text": "in and specify all these numbers but i can learn how to map from states to rewards from",
    "start": "789040",
    "end": "794880"
  },
  {
    "text": "data and so the way trx works is we have a set of ranked",
    "start": "794880",
    "end": "800639"
  },
  {
    "text": "demonstrations and we're going to focus on just two of these and if i prefer trajectory 2 tau 2 over",
    "start": "800639",
    "end": "806079"
  },
  {
    "text": "trajectory 1 well it ought to be the case that my reward function i'm learning r sub theta gives a higher",
    "start": "806079",
    "end": "812320"
  },
  {
    "text": "reward to the better ranked trajectory and a lower reward to the lower ranked trajectory especially when i accumulate",
    "start": "812320",
    "end": "817440"
  },
  {
    "text": "this reward over these trajectories that's what's cool is we can now just train this using a standard bradley",
    "start": "817440",
    "end": "822480"
  },
  {
    "text": "terry pairwise ranking loss i like to think about this in terms of kind of a binary classification problem",
    "start": "822480",
    "end": "827839"
  },
  {
    "text": "where you could think about the logits as the accumulated rewards on these two trajectories and we can equivalently",
    "start": "827839",
    "end": "833600"
  },
  {
    "text": "minimize a cross-entropy loss and so this allows us to back propagate into a deep neural network",
    "start": "833600",
    "end": "839040"
  },
  {
    "text": "and train it to match up with these rankings and learn a reward function that satisfies those labels",
    "start": "839040",
    "end": "845279"
  },
  {
    "text": "and so what's cool about this is that given pre-ranked demonstrations reward learning is now just a standard",
    "start": "845279",
    "end": "850560"
  },
  {
    "text": "supervised learning problem we've removed the mtp solver from the inner loop making this very scalable and",
    "start": "850560",
    "end": "855839"
  },
  {
    "text": "tractable and so this allows us to you know extend this work to learn from",
    "start": "855839",
    "end": "862160"
  },
  {
    "text": "offline demonstrations for the first time to do complex tasks like atari games where we don't have access to the score but we're given a set of 12 ranked",
    "start": "862160",
    "end": "869120"
  },
  {
    "text": "demonstrations where this is the best demonstration in our set and you can see that the demonstrator here is you're",
    "start": "869120",
    "end": "874320"
  },
  {
    "text": "doing an okay job but occasionally they're going to ram into cars and then we learn a t-rex policy from",
    "start": "874320",
    "end": "880320"
  },
  {
    "text": "this which does much better so if we look at the game scores which the agent doesn't have access to we can see it's achieving",
    "start": "880320",
    "end": "886399"
  },
  {
    "text": "a score of 520 much better than the best demonstration so you might ask you know what if what",
    "start": "886399",
    "end": "892800"
  },
  {
    "text": "if you don't have explicit preference labels and so we've also looked at ideas of learning from a learner so if i can",
    "start": "892800",
    "end": "898480"
  },
  {
    "text": "watch a learner over time who's maybe like on average improving we can actually use the time stamps of",
    "start": "898480",
    "end": "903519"
  },
  {
    "text": "different trajectories that we've seen and this actually works pretty well to give us labels we've also looked at work",
    "start": "903519",
    "end": "908720"
  },
  {
    "text": "on automatically generating preference labels by adding noise into a behavioral cloned policy",
    "start": "908720",
    "end": "915839"
  },
  {
    "text": "and so i'd like to kind of take a step back though because i think one of the cool things about this is that we now have these ai systems that can",
    "start": "916480",
    "end": "922800"
  },
  {
    "text": "efficiently infer human intent from suboptimal demonstrations which is something that we see that people can do from a very young age so",
    "start": "922800",
    "end": "929759"
  },
  {
    "text": "for lots of studies showing that the humans even from like age like one or one and a half can kind of tell what a",
    "start": "929759",
    "end": "934959"
  },
  {
    "text": "human wanted to do even when the human you know makes a mistake and so you can see there's like obviously a lot of cues",
    "start": "934959",
    "end": "940320"
  },
  {
    "text": "going on here there's like facial cues kind of verbal cues but this you know kid says hey i know what you're trying",
    "start": "940320",
    "end": "945440"
  },
  {
    "text": "to do let me help you out here and he's able to put this block on there and so i think it's really cool that we have",
    "start": "945440",
    "end": "950639"
  },
  {
    "text": "we're starting to see ai systems that can do similar things that can learn from some helpful demonstrations and kind of infer human intent even if it's",
    "start": "950639",
    "end": "957519"
  },
  {
    "text": "not you know optimally shown so i'd like to now talk about how do we",
    "start": "957519",
    "end": "963600"
  },
  {
    "text": "estimate uncertainty so we talked about t-rex which is a cool of learning reward functions from human",
    "start": "963600",
    "end": "969120"
  },
  {
    "text": "input from sub-optimal rank demonstrations but it's only learning this single reward function estimate",
    "start": "969120",
    "end": "974639"
  },
  {
    "text": "this maximum likelihood estimate and so this can lead to problems so we found",
    "start": "974639",
    "end": "980000"
  },
  {
    "text": "that when we trained on some human demonstrations of mrs pac-man this is actually what the agent learned to do",
    "start": "980000",
    "end": "985199"
  },
  {
    "text": "so i learned that there's this cool little loophole that you can go through this tunnel and it confuses the ghosts",
    "start": "985199",
    "end": "990480"
  },
  {
    "text": "really badly and so they never eat you you can live forever but it never actually learned to like eat the pellets",
    "start": "990480",
    "end": "996399"
  },
  {
    "text": "it just learned that hey dying is really bad living for a long time is good let me optimize that similarly for this space game where",
    "start": "996399",
    "end": "1003440"
  },
  {
    "text": "you're like driving a spaceship trying to shoot enemy ships and discover this cool hack that you can like mash your keys back and forth and you stay out of",
    "start": "1003440",
    "end": "1009759"
  },
  {
    "text": "the enemy firing lanes you live for a really long time you never get hit awesome i'm doing really well",
    "start": "1009759",
    "end": "1015839"
  },
  {
    "text": "and not really though and so you can see how these are over fit to the spurious correlations in the data while it may be",
    "start": "1015839",
    "end": "1021759"
  },
  {
    "text": "true that better ranked demonstrations do last longer maybe do avoid ghosts more that's not really the only",
    "start": "1021759",
    "end": "1027918"
  },
  {
    "text": "hypothesis that explains this data and there's no way of having these con on this consideration of alternative",
    "start": "1027919",
    "end": "1034079"
  },
  {
    "text": "hypotheses if we're just learning this maximum likelihood estimate so what would we like to do well ideally we'd",
    "start": "1034079",
    "end": "1040000"
  },
  {
    "text": "like to be bayesian we'd like to learn an entire distribution over likely reward functions given this human data",
    "start": "1040000",
    "end": "1046640"
  },
  {
    "text": "so we can think about uncertainty and risk and entertain multiple reasons that might explain this data",
    "start": "1046640",
    "end": "1053039"
  },
  {
    "text": "and so this is a well studied field called bayesian word inference or beijing inverse reinforcement learning",
    "start": "1053039",
    "end": "1058240"
  },
  {
    "text": "where we're trying to essentially learn this posterior distribution the probability density function over reward",
    "start": "1058240",
    "end": "1064080"
  },
  {
    "text": "functions given human data and we can use your favorite bayes rule to look at",
    "start": "1064080",
    "end": "1069520"
  },
  {
    "text": "this in terms of a likelihood function what's the probability of this data given a reward function hypothesis along",
    "start": "1069520",
    "end": "1074720"
  },
  {
    "text": "with the prior so here r is a reward function d is the human data and ideally we'd like to be",
    "start": "1074720",
    "end": "1080720"
  },
  {
    "text": "able to sample from this posterior distribution and get lots of hypotheses that match the human data",
    "start": "1080720",
    "end": "1087520"
  },
  {
    "text": "and typically when you do this the kind of crux of the problem is figuring out this likelihood function what's the likelihood of the human producing data d",
    "start": "1087520",
    "end": "1094480"
  },
  {
    "text": "given the hypothesis that made their optimizing reward function r and so to ground this a little more",
    "start": "1094480",
    "end": "1099760"
  },
  {
    "text": "let's say i'm in a certain state the demonstrator takes action a but there were these alternative actions they could have taken",
    "start": "1099760",
    "end": "1105360"
  },
  {
    "text": "and so one common way of formulating this likelihood function is to say well the likelihood of this state action pair",
    "start": "1105360",
    "end": "1110880"
  },
  {
    "text": "is going to be equal to this ratio here where on the numerator i have the action the demonstrator took",
    "start": "1110880",
    "end": "1116320"
  },
  {
    "text": "and then the q value representing the goodness of taking that action and then acting optimally thereafter and dividing",
    "start": "1116320",
    "end": "1122400"
  },
  {
    "text": "the denominator by the goodness of all the other actions that the demonstrator might have taken so i'm kind of",
    "start": "1122400",
    "end": "1127520"
  },
  {
    "text": "performing this counter factual and saying how good is this action you showed me versus the other alternatives",
    "start": "1127520",
    "end": "1132880"
  },
  {
    "text": "under this reward function hypothesis but if we think about trying to find",
    "start": "1132880",
    "end": "1137919"
  },
  {
    "text": "these q values for different reward functions this ends up being intractable pretty quickly",
    "start": "1137919",
    "end": "1143600"
  },
  {
    "text": "because basically it requires solving an optimal control of reinforcement learning for every hypothesis i want to evaluate",
    "start": "1143600",
    "end": "1150080"
  },
  {
    "text": "so we wanted to try and make this more efficient and so our idea was that we could do fast bayesian inference by",
    "start": "1150080",
    "end": "1156000"
  },
  {
    "text": "taking advantage of these two components so first we're going to learn this low dimensional latent space that we're",
    "start": "1156000",
    "end": "1161520"
  },
  {
    "text": "going to pre-train so we're going to learn this nice embedding space to make things more efficient so we can go from high dimensional inputs down to a lower",
    "start": "1161520",
    "end": "1167760"
  },
  {
    "text": "dimensional space that will make our bayesian inference more tractable and second we're going to use a preference based likelihood in order to",
    "start": "1167760",
    "end": "1174480"
  },
  {
    "text": "make this calculation a lot faster and avoid having to run a reinforcement learning algorithm every time we want to evaluate a new reward function",
    "start": "1174480",
    "end": "1181600"
  },
  {
    "text": "so we call this approach bayesian reward extrapolation or bayesian rex where the idea is that we're going to do this kind",
    "start": "1181600",
    "end": "1187200"
  },
  {
    "text": "of feature pre-training to learn an embedding space so we have the same input as before with t-rex we have these",
    "start": "1187200",
    "end": "1192720"
  },
  {
    "text": "ranked demonstrations and they're going to be in some high-dimensional space maybe image spaces here and basically",
    "start": "1192720",
    "end": "1198080"
  },
  {
    "text": "what we're going to do is we're going to feed these into a deep neural network convolutional neural network that's going to embed them down to a lower",
    "start": "1198080",
    "end": "1203679"
  },
  {
    "text": "dimensional space so we have this future embedding that we're trying to train and the question is how do we how do we train this",
    "start": "1203679",
    "end": "1209760"
  },
  {
    "text": "embedding and so we've looked at several different ways of doing this one way you can do this is have several different",
    "start": "1209760",
    "end": "1215360"
  },
  {
    "text": "self-supervised or unsupervised task losses so for example we looked at trying like a variational autoencoder that tries to",
    "start": "1215360",
    "end": "1222080"
  },
  {
    "text": "map from an image embed it down and then reconstruct we also looked at ideas like temporal distance so given the",
    "start": "1222080",
    "end": "1228320"
  },
  {
    "text": "embeddings of two different states from the same trajectory can i predict how far apart they are as well as doing inverse dynamics and forward dynamics",
    "start": "1228320",
    "end": "1235200"
  },
  {
    "text": "all using the same shared future embedding but we found that actually just doing",
    "start": "1235200",
    "end": "1240400"
  },
  {
    "text": "unsupervised or self-supervised training wasn't as good as when we added a little bit of actual supervised learning",
    "start": "1240400",
    "end": "1247440"
  },
  {
    "text": "and interestingly just doing supervised learning actually didn't do as well as adding in some unsupervised",
    "start": "1247440",
    "end": "1252640"
  },
  {
    "text": "self-supervised learning i think yeah the intuition here that we have and this is still kind of an open",
    "start": "1252640",
    "end": "1257840"
  },
  {
    "text": "question of why but i think the reason is that these self-supervised tasks are going to learn lots of interesting",
    "start": "1257840",
    "end": "1263039"
  },
  {
    "text": "features but they may not be actually relevant to what the human wants at all whereas if",
    "start": "1263039",
    "end": "1268240"
  },
  {
    "text": "we just use the supervised training data we might overfit to spurious correlations and just hope focus on one",
    "start": "1268240",
    "end": "1274880"
  },
  {
    "text": "thing that explains the rankings and so by having this kind of mixture of auxiliary functions we're able to train this light in space that",
    "start": "1274880",
    "end": "1282000"
  },
  {
    "text": "captures a lot of interesting information relative to a variety of different possible human intentions and",
    "start": "1282000",
    "end": "1288159"
  },
  {
    "text": "what's nice is we can now freeze these weights here and we now have this feature extractor that works on these high dimensional inputs",
    "start": "1288159",
    "end": "1294640"
  },
  {
    "text": "and so this gives us the feature pre-training and we can now just define a linear reward function on top of that so we have these features s",
    "start": "1294640",
    "end": "1301679"
  },
  {
    "text": "from our latent space and we can define a reward function as just a linear combination of these with this weight",
    "start": "1301679",
    "end": "1307039"
  },
  {
    "text": "vector theta and so what's cool about this is it's going to make things really efficient because we can take these pre-ranked",
    "start": "1307039",
    "end": "1312960"
  },
  {
    "text": "demonstrations that we have we can embed them in the latent space just by adding up the latent space for every state and",
    "start": "1312960",
    "end": "1319520"
  },
  {
    "text": "adding that all together and this gives us the set of you know ranked demonstration essentially feature counts",
    "start": "1319520",
    "end": "1325840"
  },
  {
    "text": "in this latent space that we've learned and so the reason this is important is",
    "start": "1325840",
    "end": "1331039"
  },
  {
    "text": "because we have a linear reward function if we looked at what our predicted trajectory return is",
    "start": "1331039",
    "end": "1337200"
  },
  {
    "text": "for a trajectory tau we're going to have to sum up all of the states in that trajectory well this is essentially if",
    "start": "1337200",
    "end": "1343039"
  },
  {
    "text": "we put in the linear reward function we can pull out the theta vector and really all it is is just this dot product if we",
    "start": "1343039",
    "end": "1348320"
  },
  {
    "text": "want to predict a trajectory's return we just take our weight vector hypothesis dot product with this expected feature",
    "start": "1348320",
    "end": "1354480"
  },
  {
    "text": "count vector and so what's cool is this is going to make our inference really fast so we're",
    "start": "1354480",
    "end": "1359840"
  },
  {
    "text": "doing markov chain monte carlo inference where at a high level basically all we're asking is given a hypothesis for",
    "start": "1359840",
    "end": "1366720"
  },
  {
    "text": "your reward function given these ranked demonstrations you've given me how likely is this",
    "start": "1366720",
    "end": "1372240"
  },
  {
    "text": "and what's cool is we have these preference labels right this is kind of our input data we want to say how likely",
    "start": "1372240",
    "end": "1377280"
  },
  {
    "text": "is this is this trajectory preference well if we looked at the predicted returns",
    "start": "1377280",
    "end": "1382559"
  },
  {
    "text": "so i have a theta vector i want to say how likely is it that you were operating on this reward function well if you were",
    "start": "1382559",
    "end": "1388640"
  },
  {
    "text": "then probably you would have had a higher reward for the better range trajectory and i can quickly calculate",
    "start": "1388640",
    "end": "1393840"
  },
  {
    "text": "how well this theta vector matches up with your preferences just via these dot products",
    "start": "1393840",
    "end": "1399440"
  },
  {
    "text": "and the only thing i need to like vary is theta because i've pre-computed everything else so this makes it nice we can now use",
    "start": "1399440",
    "end": "1405440"
  },
  {
    "text": "this likelihood function that's based off these preferences and with just a couple dot products we can efficiently",
    "start": "1405440",
    "end": "1411679"
  },
  {
    "text": "evaluate the likelihood of any hypothesize reward function without having any need of touching the mdp no",
    "start": "1411679",
    "end": "1418000"
  },
  {
    "text": "rollouts no mdp solver and so this allows us to repeatedly sample from this posterior distribution",
    "start": "1418000",
    "end": "1423600"
  },
  {
    "text": "really efficiently and this is cool this is the first bayesian reward inference algorithm that's able",
    "start": "1423600",
    "end": "1429200"
  },
  {
    "text": "to scale to these visual high dimensional control tasks and to give you an idea of the actual efficiency",
    "start": "1429200",
    "end": "1434640"
  },
  {
    "text": "here so we're able to generate about 10 000 samples of likely reward functions less than a minute just running on my laptop no gpus or anything fancy whereas",
    "start": "1434640",
    "end": "1442559"
  },
  {
    "text": "if i were to use a standard classical beijing irl approach that would sample a reward function",
    "start": "1442559",
    "end": "1447760"
  },
  {
    "text": "run a reinforcement learning algorithm evaluate a likelihood you were talking maybe you know five or plus hours for a",
    "start": "1447760",
    "end": "1453039"
  },
  {
    "text": "single sample assistant close this allows us to do this efficient bayesian inference on these high dimensional control tasks for",
    "start": "1453039",
    "end": "1459520"
  },
  {
    "text": "the first time and this works really well so this is like learning atari breakout game and so",
    "start": "1459520",
    "end": "1465760"
  },
  {
    "text": "you see here the agent is able to learn from just a set of ranked sub-option demonstrations in",
    "start": "1465760",
    "end": "1471440"
  },
  {
    "text": "order to be able to play the game really well and compared to some other approaches so if you look at the best of the 12",
    "start": "1471440",
    "end": "1476880"
  },
  {
    "text": "demonstrations these demonstrations actually never are that good this is actually a quite complex game when you start hitting the higher and lower",
    "start": "1476880",
    "end": "1483039"
  },
  {
    "text": "levels and just running behavioral cloning it doesn't do that well you can look at some other approaches",
    "start": "1483039",
    "end": "1488400"
  },
  {
    "text": "such as a generative adversarial approach and these methods are often really unstable and are hard to apply to",
    "start": "1488400",
    "end": "1493520"
  },
  {
    "text": "these visual domains if you look at t-rex it does much better and bayesian rex which is actually",
    "start": "1493520",
    "end": "1499200"
  },
  {
    "text": "learning this posterior if we take the the mean reward function from this posterior and optimize it we actually",
    "start": "1499200",
    "end": "1504880"
  },
  {
    "text": "often do even better than just learning this like emily estimate",
    "start": "1504880",
    "end": "1510000"
  },
  {
    "text": "and so just using the reward function in this way like just taking the expectation of the distribution is not",
    "start": "1510880",
    "end": "1517279"
  },
  {
    "text": "kind of the best use of having a full bayesian posterior so we've also looked at what can you do when you have this",
    "start": "1517279",
    "end": "1522640"
  },
  {
    "text": "posterior distribution and one of the nice things you can do is you can come up with these high confidence performance bounds given a policy or",
    "start": "1522640",
    "end": "1528400"
  },
  {
    "text": "trajectory i can now with high confidence tell you whether this policy is better than this other policy using",
    "start": "1528400",
    "end": "1533440"
  },
  {
    "text": "this distribution we've also shown that we can use this to detect misaligned reward functions so look at this example",
    "start": "1533440",
    "end": "1539279"
  },
  {
    "text": "here from this beanwriter example if i have a distribution over likely reward functions i can now take a policy",
    "start": "1539279",
    "end": "1545120"
  },
  {
    "text": "such as this one and i can evaluate it very efficiently because of this linear reward function over a variety of",
    "start": "1545120",
    "end": "1551200"
  },
  {
    "text": "different hypotheses and we found that looking at the tails of these distributions actually is a really good",
    "start": "1551200",
    "end": "1556640"
  },
  {
    "text": "indicator of when this agent is performing this kind of reward hacking behavior optimizing a misaligned reward",
    "start": "1556640",
    "end": "1561919"
  },
  {
    "text": "function because essentially having really long tails tells me that there are lots of reward hub function hypotheses that looked likely given the",
    "start": "1561919",
    "end": "1568080"
  },
  {
    "text": "data but they might say that this is really bad because maybe these other reward functions hypotheses say that like you know destroying enemy ships is important",
    "start": "1568080",
    "end": "1574240"
  },
  {
    "text": "or advancing through the levels and so by having this distribution we're able to identify cases where we might be you",
    "start": "1574240",
    "end": "1579919"
  },
  {
    "text": "know very misaligned cool so i'd like to now take a step back",
    "start": "1579919",
    "end": "1585600"
  },
  {
    "text": "and think about given that we can now estimate these reward function distributions",
    "start": "1585600",
    "end": "1590880"
  },
  {
    "text": "how can we be robust to this uncertainty and so we're going to assume that we can now sample",
    "start": "1590880",
    "end": "1597120"
  },
  {
    "text": "a posterior distribution of likely roller functions given this human data the question is given that i'm uncertain",
    "start": "1597120",
    "end": "1603440"
  },
  {
    "text": "over what the true objective is how should i act how should an ai system act if it doesn't know its true objective or",
    "start": "1603440",
    "end": "1608720"
  },
  {
    "text": "reward function ideally we want to have systems that are robust to this uncertainty and so a lot of prior work typically",
    "start": "1608720",
    "end": "1614960"
  },
  {
    "text": "would optimize you know for a single reward function maybe it would be like the most likely reward function or maybe",
    "start": "1614960",
    "end": "1620240"
  },
  {
    "text": "the expected reward function under a distribution and we've seen examples right where the most likely reward function just like",
    "start": "1620240",
    "end": "1627200"
  },
  {
    "text": "overfits dispersed correlations is not what we want similarly i'd like to argue that we often don't want to just do well on",
    "start": "1627200",
    "end": "1633360"
  },
  {
    "text": "expectation so if we think about maybe like a self-driving car driving down the road maybe it detects some obstacle",
    "start": "1633360",
    "end": "1639840"
  },
  {
    "text": "maybe a box in the road and it might be that you know with high likelihood it thinks an expectation that you know box",
    "start": "1639840",
    "end": "1645279"
  },
  {
    "text": "is empty it can just run over it nothing will happen but there's some chance that maybe if it runs over the box maybe it",
    "start": "1645279",
    "end": "1651760"
  },
  {
    "text": "contains some object that might damage the car or hurt the passengers and so rather than you know driving through",
    "start": "1651760",
    "end": "1656799"
  },
  {
    "text": "which might an expectation work the best because it minimizes you know weaving on the road it gets you to the destination",
    "start": "1656799",
    "end": "1662720"
  },
  {
    "text": "a little faster we probably want our systems to be a little more risk-averse you know maybe slow down actually avoid",
    "start": "1662720",
    "end": "1669279"
  },
  {
    "text": "that object i think that's probably what most of us would do when driving down this road and so we turned to finance",
    "start": "1669279",
    "end": "1676720"
  },
  {
    "text": "to think about how can we deal with this idea of thinking about risk think about risk and reward trade-offs because in",
    "start": "1676720",
    "end": "1682880"
  },
  {
    "text": "computational finance yeah that's really kind of the name of the game and there are a couple different metrics",
    "start": "1682880",
    "end": "1688559"
  },
  {
    "text": "that are often used in finance one is this idea of a value at risk and so this is essentially just like a one minus",
    "start": "1688559",
    "end": "1695360"
  },
  {
    "text": "alpha quantile outcome and so if we're looking at this random variable x and we",
    "start": "1695360",
    "end": "1700480"
  },
  {
    "text": "have some distribution the value at risk is given a certain alpha level is going to be the level at which we have",
    "start": "1700480",
    "end": "1708320"
  },
  {
    "text": "say like 95 percent of the distribution above that point and so if you think about this in terms of like you know",
    "start": "1708320",
    "end": "1714159"
  },
  {
    "text": "performance we might be able to say hey 95 of the time my performance is no worse than x amount so kind of just",
    "start": "1714159",
    "end": "1719760"
  },
  {
    "text": "taking this like percentile shot now this is nice but it turns out optimizing for value risk",
    "start": "1719760",
    "end": "1726640"
  },
  {
    "text": "actually ends up being a difficult problem and it ignores the tails you know ignores what's going to happen in this red section maybe in like the five",
    "start": "1726640",
    "end": "1733200"
  },
  {
    "text": "percent worst case outcomes and so often what people have been doing and more and more this is becoming more",
    "start": "1733200",
    "end": "1738399"
  },
  {
    "text": "popular this idea of conditional value at risk so we're going to condition on being kind of in this tail of the",
    "start": "1738399",
    "end": "1744559"
  },
  {
    "text": "distribution this red shaded portion and we're going to say given that i'm in like the 5 worst case outcomes",
    "start": "1744559",
    "end": "1750240"
  },
  {
    "text": "how much do i expect that what do i expect to happen so we'll look at this expectation as we might say that you",
    "start": "1750240",
    "end": "1756480"
  },
  {
    "text": "know how bad is my performance going to be an expectation given that it's like the five percent worst case",
    "start": "1756480",
    "end": "1762080"
  },
  {
    "text": "and what's cool now is maximizing c bar ends up being a very tractable optimization problem and we're actually",
    "start": "1762080",
    "end": "1768240"
  },
  {
    "text": "taking into account kind of the tails of this distribution really thinking about how do we be robust to some of these",
    "start": "1768240",
    "end": "1773919"
  },
  {
    "text": "kind of worst case outcomes and so we proposed this approach called bayesian robust optimization for",
    "start": "1773919",
    "end": "1780320"
  },
  {
    "text": "imitation learning or broil where basically what we want to do is we want to directly optimize the policy to be",
    "start": "1780320",
    "end": "1785840"
  },
  {
    "text": "maximally robust to a distribution over reward functions so rather than a standard rl approach that assumes that i",
    "start": "1785840",
    "end": "1792159"
  },
  {
    "text": "have this magical reward function that's given to me that's perfect i can just optimize that we now say okay let's",
    "start": "1792159",
    "end": "1797600"
  },
  {
    "text": "let's take a step back let's assume this reward function isn't perfect in fact let's just say we have a distribution of reward functions now we want to optimize",
    "start": "1797600",
    "end": "1804640"
  },
  {
    "text": "our behavior such that we can maximize this conditional valid risk when these",
    "start": "1804640",
    "end": "1810320"
  },
  {
    "text": "reward functions are taken from this distribution and so the way we do that is we're going to try and make sure that we're not performing really poorly under",
    "start": "1810320",
    "end": "1816640"
  },
  {
    "text": "any one of these reward functions because that will lead us to have you know a high conditional valid risk and",
    "start": "1816640",
    "end": "1822000"
  },
  {
    "text": "what's really cool is we've shown that you can actually efficiently solve for a policy that optimizes the objective just using linear programming",
    "start": "1822000",
    "end": "1828880"
  },
  {
    "text": "so it's super efficient as long as you have a discrete mdp so discrete states discrete actions",
    "start": "1828880",
    "end": "1834480"
  },
  {
    "text": "and what's even more cool is that you can actually extend this continuous states and continuous actions really easily using a ppo style policy gradient",
    "start": "1834480",
    "end": "1842720"
  },
  {
    "text": "and so we have some recent work showing that with just a few changes to a standard policy gradient algorithm",
    "start": "1842720",
    "end": "1848559"
  },
  {
    "text": "we can actually now make it robust to an entire distribution of reward functions and use kind of all the",
    "start": "1848559",
    "end": "1854640"
  },
  {
    "text": "standard tricks that we use in our state-of-the-art reinforcement learning algorithms to make the scale the interesting problems",
    "start": "1854640",
    "end": "1860399"
  },
  {
    "text": "and it's kind of a cute example of this we've shown that this is really nice for hedging against multiple objectives so",
    "start": "1860399",
    "end": "1866159"
  },
  {
    "text": "we looked at this atari boxing game where there's kind of these competing objectives where i'm trying to score points on the opponent but avoid getting",
    "start": "1866159",
    "end": "1872559"
  },
  {
    "text": "punched myself and we gave some some awful ranked demonstrations ran bayesian rex to get a posterior distribution over",
    "start": "1872559",
    "end": "1879360"
  },
  {
    "text": "likely reward functions and we evaluated a whole bunch of different approaches to try and get a policy so we looked at risk neutral",
    "start": "1879360",
    "end": "1886399"
  },
  {
    "text": "approaches like behavioral cloning methods like adversarial imitation learning there are risk averse versions",
    "start": "1886399",
    "end": "1892240"
  },
  {
    "text": "of adversarial limitation learning we looked at maximizing you know the most likely reward function maybe the mean reward function",
    "start": "1892240",
    "end": "1898080"
  },
  {
    "text": "and across the board everything performed much worse than the broil approach which is trying to hedge",
    "start": "1898080",
    "end": "1903600"
  },
  {
    "text": "against its risk kind of playing whack-a-mole with the different hypotheses that it might have and really trying to find a reward function that",
    "start": "1903600",
    "end": "1910399"
  },
  {
    "text": "performs well across the distribution allowing it to balance these multiple competing objectives",
    "start": "1910399",
    "end": "1917200"
  },
  {
    "text": "and so i think this is really exciting and i think kind of the high level take-home message here is that ai systems that hedge against their",
    "start": "1917679",
    "end": "1923279"
  },
  {
    "text": "uncertainty are going to be a lot more robust than ones that try and you know uniquely identify one reward function",
    "start": "1923279",
    "end": "1928960"
  },
  {
    "text": "that they think explains the human data and i think this is really important because a lot of problems that we think",
    "start": "1928960",
    "end": "1934320"
  },
  {
    "text": "about are inherently kind of multi-objective and that's like a natural way to think about them a natural way to express them and really",
    "start": "1934320",
    "end": "1940159"
  },
  {
    "text": "having these methods that can balance these different objectives is really important especially if we think about ideas like aligning behavior to multiple",
    "start": "1940159",
    "end": "1947360"
  },
  {
    "text": "people it's kind of like multi-agent value alignment problem i think extending these ideas that we have for",
    "start": "1947360",
    "end": "1952480"
  },
  {
    "text": "broil to allow systems to perform well under multiple people's preferences is really exciting making sure that we're",
    "start": "1952480",
    "end": "1958320"
  },
  {
    "text": "not performing really poorly or under some you know small subset of these people's values",
    "start": "1958320",
    "end": "1964880"
  },
  {
    "text": "cool and so with that i'd now like to pop back up and then move on to this question of how do we actively reduce",
    "start": "1964880",
    "end": "1971279"
  },
  {
    "text": "uncertainty because it might be the case that i just have so much uncertainty that even if i try and like hedge all my",
    "start": "1971279",
    "end": "1976480"
  },
  {
    "text": "bets it just kind of leads to something nonsensical i just really need more information",
    "start": "1976480",
    "end": "1982000"
  },
  {
    "text": "so ideally we have these as systems that can ask for help i think anyone that's like taught a class where you know none of the",
    "start": "1982000",
    "end": "1988640"
  },
  {
    "text": "students respond none of them ask questions i think it's especially pertinent in like zoom days when you",
    "start": "1988640",
    "end": "1993679"
  },
  {
    "text": "like wouldn't even see half the people in the audience it's really hard to be able to tailor the information that",
    "start": "1993679",
    "end": "1998960"
  },
  {
    "text": "you're giving if you're like teaching a class or giving instruction to someone they like never ask you questions never",
    "start": "1998960",
    "end": "2004159"
  },
  {
    "text": "show you what they're uncertain about and ideally we want our ais systems move from being passive learners to being",
    "start": "2004159",
    "end": "2009279"
  },
  {
    "text": "active learners that you know we'll figuratively raise our hands maybe with a robot actually physically raise our hands and ask for",
    "start": "2009279",
    "end": "2015360"
  },
  {
    "text": "help we want systems that know what they know but also know what they're uncertain about and this will allow people to give the appropriate",
    "start": "2015360",
    "end": "2021279"
  },
  {
    "text": "information and allow them to learn much more efficiently and so the question here that we want to look at is when should a robot ask for",
    "start": "2021279",
    "end": "2028880"
  },
  {
    "text": "help and so when we started thinking about this one of the things that first came to mind was that if the robot finds",
    "start": "2028880",
    "end": "2035440"
  },
  {
    "text": "itself in a novel state then this is a point where where it",
    "start": "2035440",
    "end": "2042159"
  },
  {
    "text": "should ask for help so for example maybe i've given a lot of data of a self-driving car just self-driving car",
    "start": "2042159",
    "end": "2047200"
  },
  {
    "text": "you know driving down roads but now you know it finds itself off-road so because it's off distribution it's probably",
    "start": "2047200",
    "end": "2052960"
  },
  {
    "text": "going to make a mistake and so it should probably ask for help at this point it should probably call into the person say hey can you help me out i'm not sure",
    "start": "2052960",
    "end": "2059040"
  },
  {
    "text": "what i should do and so it kind of makes sense that novel states are going to be risky and we should ask for help there what we also",
    "start": "2059040",
    "end": "2065440"
  },
  {
    "text": "realized is that there actually are other states it may not be novel but still are risky for example you can",
    "start": "2065440",
    "end": "2071440"
  },
  {
    "text": "think about this like narrow mountain to pass that i've seen lots of examples of driving down but i know there's like a",
    "start": "2071440",
    "end": "2076720"
  },
  {
    "text": "very small margin for error and i know i'm still like not that great at driving here and so even though this isn't novel",
    "start": "2076720",
    "end": "2082720"
  },
  {
    "text": "to me if my policy of performance is not kind of up to snuff i still might think this is risky and i might benefit from",
    "start": "2082720",
    "end": "2088878"
  },
  {
    "text": "some human interventions and some human help and so we wanted to look at this",
    "start": "2088879",
    "end": "2095599"
  },
  {
    "text": "and allow these robot systems to ask for help in states that are novel and risky and allow that to improve their policy",
    "start": "2095599",
    "end": "2102720"
  },
  {
    "text": "and so we had this idea of using interactive imitation learning where if you remember from before we're",
    "start": "2102720",
    "end": "2108480"
  },
  {
    "text": "basically taking this behavioral cloning approach we're going to directly map from states to actions we're going to learn from data and transfer that into a",
    "start": "2108480",
    "end": "2114880"
  },
  {
    "text": "policy but crucially we're going to maintain our uncertainty over what we think we should be doing and use",
    "start": "2114880",
    "end": "2121200"
  },
  {
    "text": "active queries in order to try and reduce that uncertainty and improve our robustness over time",
    "start": "2121200",
    "end": "2127920"
  },
  {
    "text": "and so this is one of the example tasks that we looked at where we're trying to pick up this washer here on the left",
    "start": "2127920",
    "end": "2132960"
  },
  {
    "text": "place it on this peg down here on the bottom and there's some kind of inherent bottlenecks here where it's often",
    "start": "2132960",
    "end": "2139040"
  },
  {
    "text": "a little tricky to grasp that washer exactly right and then when we bring it over here there's like kind of a small",
    "start": "2139040",
    "end": "2144480"
  },
  {
    "text": "margin for error in terms of placing it on this peg and so we found this a nice test bed for testing out these ideas of",
    "start": "2144480",
    "end": "2150640"
  },
  {
    "text": "novelty and risk in particular we call our approach thrifty dagger so this is an interactive",
    "start": "2150640",
    "end": "2156000"
  },
  {
    "text": "learning approach where we're be iteratively collecting data from a human and using that to improve our model",
    "start": "2156000",
    "end": "2161200"
  },
  {
    "text": "and so we assume that we have this human operator that we can kind of call into kind of like a call center type approach",
    "start": "2161200",
    "end": "2166720"
  },
  {
    "text": "and ask them to come in and help us out and in particular we're going to take as input here our state",
    "start": "2166720",
    "end": "2173040"
  },
  {
    "text": "as well as some hyper parameter that determines how often the human wants to be you know bugged or kind of",
    "start": "2173040",
    "end": "2178960"
  },
  {
    "text": "how how much effort they can invest in helping us out and we're basically going to take these and we're going to ask you",
    "start": "2178960",
    "end": "2185200"
  },
  {
    "text": "know is the state novel and so here's an example of a novel state where the robots ended up grasping this watch in a",
    "start": "2185200",
    "end": "2190320"
  },
  {
    "text": "kind of awkward position and if we detect this as novel we're going to ask the human for help they're going to come",
    "start": "2190320",
    "end": "2195359"
  },
  {
    "text": "in tell operate the system get us unstuck if it's not novel we're going to then",
    "start": "2195359",
    "end": "2200640"
  },
  {
    "text": "check well is this risky based off you know previous data and we're going to assume here that we have some binary indicator of task",
    "start": "2200640",
    "end": "2207520"
  },
  {
    "text": "completion and so we can train in states that we've seen a lot of data this kind of risk metric that tells us how likely",
    "start": "2207520",
    "end": "2213520"
  },
  {
    "text": "am i to be able to complete this task from the state as if risky we're also going to ask for",
    "start": "2213520",
    "end": "2218640"
  },
  {
    "text": "the human operator to come in and help so for example in this case this washer is in a really awkward position the robot doesn't think it's going to be",
    "start": "2218640",
    "end": "2224640"
  },
  {
    "text": "able to complete this task however if it's not novel and it's not risky we're going to let the robot just",
    "start": "2224640",
    "end": "2230000"
  },
  {
    "text": "operate do its own thing and roll out its policy and crucially it's always going to take the human intervention data use that to",
    "start": "2230000",
    "end": "2236320"
  },
  {
    "text": "improve its policy over time and the reason we have this kind of a gating approach is that if the state is",
    "start": "2236320",
    "end": "2243440"
  },
  {
    "text": "novel we haven't seen much data there so we really can't trust our risk metric and so we're just going to like",
    "start": "2243440",
    "end": "2249440"
  },
  {
    "text": "immediately kind of say hey human i need help if it isn't novel then that's where we're going to look at this risk metric",
    "start": "2249440",
    "end": "2255280"
  },
  {
    "text": "and say okay how likely am i to succeed and if i'm very unlikely i'm also going to ask for help",
    "start": "2255280",
    "end": "2260960"
  },
  {
    "text": "so to kind of ground this so here's an actual execution of the algorithm where it starts off in autonomous mode it's going to go down try and grasp this",
    "start": "2260960",
    "end": "2267200"
  },
  {
    "text": "washer but it misses and so it detects now that this is a novel state",
    "start": "2267200",
    "end": "2272720"
  },
  {
    "text": "it's going to ask for human supervision we come and tell operate the robot at some point it now detects okay like i'm",
    "start": "2272720",
    "end": "2278320"
  },
  {
    "text": "back in distribution things are not novel risky anymore takes over switches back into a toms slot lets the human go do their own task",
    "start": "2278320",
    "end": "2285520"
  },
  {
    "text": "and finishes the completion of the putting the washer over the peg",
    "start": "2285520",
    "end": "2290560"
  },
  {
    "text": "here's another example where just right at the beginning just due to the initial state configuration the robot detects",
    "start": "2290560",
    "end": "2296079"
  },
  {
    "text": "that this is risky for it it doesn't think it's going to be able to grasp the washer and so it asks for human assistance to",
    "start": "2296079",
    "end": "2302960"
  },
  {
    "text": "help it get lined up and then at some point it detects okay not risky anymore not novel anymore",
    "start": "2302960",
    "end": "2310000"
  },
  {
    "text": "switches into autonomous mode a human can go back and do their own thing and then the robot is able to successfully",
    "start": "2310000",
    "end": "2315440"
  },
  {
    "text": "autonomously complete this task",
    "start": "2315440",
    "end": "2318880"
  },
  {
    "text": "and we've also looked at kind of scaling us up to more complex manipulation so we're looking at this kind of deformable",
    "start": "2321839",
    "end": "2327040"
  },
  {
    "text": "manipulation we're doing this complex kind of figure eight rope manipulation kind of figure eight tying task and here",
    "start": "2327040",
    "end": "2333359"
  },
  {
    "text": "we're using this differential surgical robot to give the demonstrations and interventions and we find that because this is such a",
    "start": "2333359",
    "end": "2340720"
  },
  {
    "text": "long horizon task and we're using you know pixel input to try and train the robot that behavioral cloning approaches",
    "start": "2340720",
    "end": "2346960"
  },
  {
    "text": "really just can't make very much progress at all with an equivalent amount of data but what's cool is if we use kind of",
    "start": "2346960",
    "end": "2353839"
  },
  {
    "text": "equivalent amount of data but have active queries so we have this system actually asking for helping areas where it needs help",
    "start": "2353839",
    "end": "2360160"
  },
  {
    "text": "we are able to get often really good fully autonomous executions of this long range task",
    "start": "2360160",
    "end": "2366480"
  },
  {
    "text": "what's also nice is like even at execution time if the robot ever detects that it's in like a novel or a risky",
    "start": "2366480",
    "end": "2371920"
  },
  {
    "text": "state it can always ask for help to get unstuck so you'll see here as it's coming around for this last bend it's",
    "start": "2371920",
    "end": "2377680"
  },
  {
    "text": "going to get slightly stuck it switches back into supervisor mode we apply just a simple correction and then",
    "start": "2377680",
    "end": "2383520"
  },
  {
    "text": "takes back over autonomously and is able to finish the task",
    "start": "2383520",
    "end": "2388240"
  },
  {
    "text": "so one of the motivations for like this a project was this idea that",
    "start": "2389839",
    "end": "2394880"
  },
  {
    "text": "it's becoming increasingly more common that we have these kind of call center type situations where we have one or small",
    "start": "2394880",
    "end": "2401280"
  },
  {
    "text": "number of people managing large fleets of robots so all these companies up here are actively working on are actually",
    "start": "2401280",
    "end": "2406960"
  },
  {
    "text": "deploying systems like this where they have a small number of human operators that are helping out robots whether it",
    "start": "2406960",
    "end": "2414160"
  },
  {
    "text": "be like autonomous forklifts or cars and if we have a small number of human",
    "start": "2414160",
    "end": "2419440"
  },
  {
    "text": "operators you know managing a huge fleet we really want to care about how much burden we put on the person we",
    "start": "2419440",
    "end": "2425200"
  },
  {
    "text": "can't have these people micromanaging every single robot and so to kind of test out whether this thrifty dagger",
    "start": "2425200",
    "end": "2431119"
  },
  {
    "text": "approach really helped in these situations we designed a simple user study where we had 10 subjects come in",
    "start": "2431119",
    "end": "2436319"
  },
  {
    "text": "and we asked them to control three different robots in simulation they're all performing this kind of washer peg",
    "start": "2436319",
    "end": "2441359"
  },
  {
    "text": "insertion task at the same time and we looked at two different approaches",
    "start": "2441359",
    "end": "2446400"
  },
  {
    "text": "one was a robot gated approach which is like thrifty dagger and we also compared to some other prior approaches where the",
    "start": "2446400",
    "end": "2452960"
  },
  {
    "text": "robot is the one asking for help is actively asking for help from the human when it's not asking for help the human can do their own task and so we gave the",
    "start": "2452960",
    "end": "2459839"
  },
  {
    "text": "human this task of doing this concentration game this memory matching game where you flip over tiles and try and find matches",
    "start": "2459839",
    "end": "2466560"
  },
  {
    "text": "we also compare this to a human gated approach where the human is now micromanaging",
    "start": "2466560",
    "end": "2471760"
  },
  {
    "text": "everything they can see little thumbnail views of all the different robots at all times they can switch to whatever robot",
    "start": "2471760",
    "end": "2477200"
  },
  {
    "text": "they want at any time they can come in and provide an intervention if they think the robot needs help",
    "start": "2477200",
    "end": "2483119"
  },
  {
    "text": "and so you can see here the different approaches that we looked at and we found that so hd dagger is the",
    "start": "2483119",
    "end": "2488480"
  },
  {
    "text": "one where the human gets to decide when they intervene safe dagger lazy jagger and thrifty dagger all active robot",
    "start": "2488480",
    "end": "2493680"
  },
  {
    "text": "gated approaches where the robot decides when they need help and what's cool is when we looked at the qualitative results after this we found",
    "start": "2493680",
    "end": "2500319"
  },
  {
    "text": "that overall people found thrifty dagger to be much lower in terms of the mental demand and much lower in terms of",
    "start": "2500319",
    "end": "2506680"
  },
  {
    "text": "frustration so they weren't constantly being thrashed where the robot like was queering for help letting them go back",
    "start": "2506680",
    "end": "2512800"
  },
  {
    "text": "to concentration then queering for help again and they also were able to like you know accomplish their task and the",
    "start": "2512800",
    "end": "2518079"
  },
  {
    "text": "robots only asked for help when they really needed it and what i think is even more cool if we look at the",
    "start": "2518079",
    "end": "2523280"
  },
  {
    "text": "quantitative results we found that compared to the next best baseline that thrifty dagger had 21 fewer human",
    "start": "2523280",
    "end": "2529359"
  },
  {
    "text": "interventions and so this allowed the human to do their task better they got 57 more",
    "start": "2529359",
    "end": "2535200"
  },
  {
    "text": "concentrations pairs found but there was this really cool synergy where we actually had the robots doing better as",
    "start": "2535200",
    "end": "2541280"
  },
  {
    "text": "well and so because the robots were actively asking for help at the right times the human was able to do their",
    "start": "2541280",
    "end": "2546480"
  },
  {
    "text": "tests better and the robots were able to get more throughput as well and so i think this is really cool",
    "start": "2546480",
    "end": "2552480"
  },
  {
    "text": "thinking about you know future applications where i think more and more like i said we're going to have small groups of humans managing fleets of",
    "start": "2552480",
    "end": "2558480"
  },
  {
    "text": "robots and i think it's really important that we have these robots ask for ways ask for help in ways that you minimize",
    "start": "2558480",
    "end": "2564640"
  },
  {
    "text": "the super visor burden but also allow these systems to be you know robust and safe",
    "start": "2564640",
    "end": "2571760"
  },
  {
    "text": "cool so at this point i'd like to kind of think about like where do we go from now",
    "start": "2573200",
    "end": "2578319"
  },
  {
    "text": "so what kind of ideas am i excited about in the future and kind of popping back up to our original statement at the beginning i'm",
    "start": "2578319",
    "end": "2584880"
  },
  {
    "text": "still very interested in this high-level goal of using human input in order to enable robust ai systems",
    "start": "2584880",
    "end": "2591680"
  },
  {
    "text": "and we've talked a lot about today kind of learning models of human intent right learning these reward functions from",
    "start": "2591680",
    "end": "2597200"
  },
  {
    "text": "human data how do we represent our uncertainty over the reward function how can be robust how can we actively reduce",
    "start": "2597200",
    "end": "2602960"
  },
  {
    "text": "that uncertainty i think if we think about kind of this end goal of trying to get these systems",
    "start": "2602960",
    "end": "2608160"
  },
  {
    "text": "out the real world interacting with people there's a lot more than just kind of understanding these reward functions",
    "start": "2608160",
    "end": "2614720"
  },
  {
    "text": "and if you think about these problems i talked about the beginning there's all sorts of different facets of uncertainty that we really want these systems to be",
    "start": "2614720",
    "end": "2620960"
  },
  {
    "text": "able to deal with in order to be safe and robust and so i think there's a lot of really interesting you know future directions",
    "start": "2620960",
    "end": "2627920"
  },
  {
    "text": "building off this work and so if we think about uncertainty i've talked a lot about how we can model",
    "start": "2627920",
    "end": "2633440"
  },
  {
    "text": "efficiently reward function uncertainty or even like action uncertainty but there's also uncertainty that these",
    "start": "2633440",
    "end": "2638880"
  },
  {
    "text": "robots are going to have over their dynamics over how the world works over object properties and affordances and",
    "start": "2638880",
    "end": "2644880"
  },
  {
    "text": "even uncertainty over human rationality you know how rationals you human how good are they giving me this type of",
    "start": "2644880",
    "end": "2650160"
  },
  {
    "text": "feedback and ideally we can use efficient interactions with people in order to",
    "start": "2650160",
    "end": "2655359"
  },
  {
    "text": "help robots reduce our uncertainty and manage their uncertainty in all these different aspects if we think about human input",
    "start": "2655359",
    "end": "2662079"
  },
  {
    "text": "i've talked a lot about demonstrations and preference rankings but there's so much more that we can leverage in terms",
    "start": "2662079",
    "end": "2668720"
  },
  {
    "text": "of how we learn from people on a day-to-day basis so i think about how i might learn from you or how you might learn from others",
    "start": "2668720",
    "end": "2674880"
  },
  {
    "text": "we often learn from natural language maybe one might learn from like an e-stop or i just tell you hey stop you're about to do something bad",
    "start": "2674880",
    "end": "2681599"
  },
  {
    "text": "we can learn from corrections even just looking at body language you know gaze and facial expression",
    "start": "2681599",
    "end": "2687040"
  },
  {
    "text": "so i'm really interested in figuring out how we can leverage and fuse these disparate modalities in order to learn",
    "start": "2687040",
    "end": "2693280"
  },
  {
    "text": "better and reduce our uncertainty i've also talked a lot about robustness and in particular kind of the focus of",
    "start": "2693280",
    "end": "2699520"
  },
  {
    "text": "the talk today was on these ideas of robustness in terms of performance bounds or being robust to a distribution",
    "start": "2699520",
    "end": "2706640"
  },
  {
    "text": "but i think robustness also is very tightly coupled to these ideas of verification and certifiability and also",
    "start": "2706640",
    "end": "2712640"
  },
  {
    "text": "explainability and interpretability if we really want to have systems aligned with what we want we need to be",
    "start": "2712640",
    "end": "2719440"
  },
  {
    "text": "able to like introspect and see and make sure that they can explain and then we can interpret their behavior",
    "start": "2719440",
    "end": "2726319"
  },
  {
    "text": "and so some of the things i'm excited about in the future are active queries that target different",
    "start": "2726319",
    "end": "2731920"
  },
  {
    "text": "types of feedback and so in particular if we're learning from multiple different ways a person can give us information it'd be great if",
    "start": "2731920",
    "end": "2738079"
  },
  {
    "text": "these as systems could actively ask not just for like good demonstrations but even think at a meta",
    "start": "2738079",
    "end": "2743520"
  },
  {
    "text": "level of like should i even ask for a demonstration now or maybe i should ask for a correction and to do this you have to think about you know how good is the",
    "start": "2743520",
    "end": "2749359"
  },
  {
    "text": "human at giving me corrections versus demonstrations how much cognitive burden is there going to be on the human because i want to minimize that but also",
    "start": "2749359",
    "end": "2755680"
  },
  {
    "text": "how much information gain can i get that's going to depend on you how good the human is at giving these different",
    "start": "2755680",
    "end": "2760800"
  },
  {
    "text": "types of feedback i'm also really interested in looking at how we can leverage efficient interactions with a human to enable rl",
    "start": "2760800",
    "end": "2767920"
  },
  {
    "text": "systems to more safely explore in particular how can we learn constraints",
    "start": "2767920",
    "end": "2773359"
  },
  {
    "text": "from human interactions and efficient ways that allow these systems to be able to learn autonomously more efficiently",
    "start": "2773359",
    "end": "2778880"
  },
  {
    "text": "by allowing them to kind of remove areas of the state space that are are dangerous",
    "start": "2778880",
    "end": "2783920"
  },
  {
    "text": "or that might be constraining and i think a lot of the stuff i talked about today about learning reward functions can be extended to learning",
    "start": "2783920",
    "end": "2790000"
  },
  {
    "text": "maybe constraint functions and things like that and finally i'm also very interested in this idea of the interpretability and",
    "start": "2790000",
    "end": "2796319"
  },
  {
    "text": "explainability so we've done some work in the past for like the t-rex paper trying to kind of introspect into what",
    "start": "2796319",
    "end": "2801920"
  },
  {
    "text": "these like neural networks have learned so one of the ways we looked at was like finding observations that the network predicted had like maybe a minimum or a",
    "start": "2801920",
    "end": "2808319"
  },
  {
    "text": "maximum reward so in this case this was an observation that had predicted low reward where you can see that these cars",
    "start": "2808319",
    "end": "2814480"
  },
  {
    "text": "are like making more progress and so it's kind of falling back and that was kind of a good indication that this system had kind of learned the",
    "start": "2814480",
    "end": "2820560"
  },
  {
    "text": "right thing however for this breakout example when we tried to see kind of what things the",
    "start": "2820560",
    "end": "2825920"
  },
  {
    "text": "atari agent was paying attention to we found out that really all it was learning to do was just count bricks and actually had never learned to detect",
    "start": "2825920",
    "end": "2832079"
  },
  {
    "text": "that moment when the ball hits the brick and so this is like an interesting kind of correlation and a problem with this",
    "start": "2832079",
    "end": "2839119"
  },
  {
    "text": "system that we were able to kind of uncover by doing some introspection and so i think developing more tools that",
    "start": "2839119",
    "end": "2844319"
  },
  {
    "text": "allow us to figure out whether these ais systems are doing the right thing for the right reasons is very interesting to me in particular i've done some",
    "start": "2844319",
    "end": "2851440"
  },
  {
    "text": "kind of preliminary theoretical work doing my phd on this idea of machine teaching for reward learning algorithms so how can we",
    "start": "2851440",
    "end": "2858000"
  },
  {
    "text": "design the maximally informative set of demonstrations that we can give to an ad system to teach them a particular reward",
    "start": "2858000",
    "end": "2864400"
  },
  {
    "text": "function i think you can also think about flipping this on its head and think about how can any system design a",
    "start": "2864400",
    "end": "2869599"
  },
  {
    "text": "maximum informative set of data to give to a human to show it what it's learned i'm also very interested in this idea of",
    "start": "2869599",
    "end": "2875839"
  },
  {
    "text": "trying to design these drivers tests for ai systems so once again we've done some really interesting preliminary stuff on",
    "start": "2875839",
    "end": "2881760"
  },
  {
    "text": "kind of the theory behind this idea what we call value alignment verification we're trying to design these really",
    "start": "2881760",
    "end": "2887040"
  },
  {
    "text": "efficient tests that we can give to an ai system and based off its answers we can verify whether it's going to be",
    "start": "2887040",
    "end": "2892720"
  },
  {
    "text": "aligned with a human's intent or not i think there's a lot of interesting work to go here where we can try and design",
    "start": "2892720",
    "end": "2898480"
  },
  {
    "text": "these unit tests or drivers tests for these as systems and so i think there's a wide variety of applications that can benefit from these",
    "start": "2898480",
    "end": "2904800"
  },
  {
    "text": "types of approaches ranging from robotics to rehab and assistive robotics looking at",
    "start": "2904800",
    "end": "2910720"
  },
  {
    "text": "dealing with medical healthcare records trying to learn good robust policies and",
    "start": "2910720",
    "end": "2916000"
  },
  {
    "text": "even online education a lot of my work has been on how can we get a human to teach a robot but i think you could also",
    "start": "2916000",
    "end": "2922000"
  },
  {
    "text": "think about how we get an ai system to teach a human and kind of flip the tables and i think there's a lot of",
    "start": "2922000",
    "end": "2927280"
  },
  {
    "text": "analogies that can be made so with that i'd like to end by thanking",
    "start": "2927280",
    "end": "2932319"
  },
  {
    "text": "all my brilliant collaborators who have had the privilege of working with and i'd like to open up for questions thanks",
    "start": "2932319",
    "end": "2939359"
  },
  {
    "text": "yeah one of the problems i kind of showed is that if we just have these pairwise preferences we're hoping that we can extrapolate in",
    "start": "2940559",
    "end": "2945760"
  },
  {
    "text": "the right direction but sometimes we don't and so i think yeah leveraging a little bit more information",
    "start": "2945760",
    "end": "2951359"
  },
  {
    "text": "i think could definitely make these approaches yeah work even better",
    "start": "2951359",
    "end": "2956760"
  },
  {
    "text": "so we've looked at both yeah we've looked at humans giving kind of pedagogic demonstrations of like here's something really bad do something",
    "start": "2961200",
    "end": "2967599"
  },
  {
    "text": "mediocre here's something like as good as i can do we've also for like a lot of our like big experiments just",
    "start": "2967599",
    "end": "2972720"
  },
  {
    "text": "checkpointed rl agents to kind of simulate different levels and yeah and we found like you know good",
    "start": "2972720",
    "end": "2979359"
  },
  {
    "text": "results using both approaches but i'm definitely super interested in the future in",
    "start": "2979359",
    "end": "2984880"
  },
  {
    "text": "yeah like digging more deeply i think we haven't still done i think sufficient work on the human factors side of this",
    "start": "2984880",
    "end": "2990800"
  },
  {
    "text": "in terms of seeing like how burdensome is it for a person to give rank demonstrations i mean i think sorting a whole bunch of",
    "start": "2990800",
    "end": "2997280"
  },
  {
    "text": "demonstrations is probably not what we want people to do but if i'm giving maybe demonstrations in a sequence i think it's very easy for a person to say",
    "start": "2997280",
    "end": "3003680"
  },
  {
    "text": "hey that was worse than last time or whoops i really messed up for hey i like did really good and give you kind of these like pairwise preferences in a",
    "start": "3003680",
    "end": "3010079"
  },
  {
    "text": "sequence we've also found it's like pretty easy for a person to kind of like be pedagogic and say like hey let me show you like something bad okay here's",
    "start": "3010079",
    "end": "3016400"
  },
  {
    "text": "something better you said something like even better than that and so kind of online giving these preferences",
    "start": "3016400",
    "end": "3022800"
  },
  {
    "text": "and i'm also very interested in like just watching a person learn how to do a task over time and seeing if we can",
    "start": "3022800",
    "end": "3028800"
  },
  {
    "text": "kind of use that to also get more insight into what they want we've shown good results watching an rl agent learn",
    "start": "3028800",
    "end": "3035440"
  },
  {
    "text": "over time but haven't like extended that to actually watching real people learn which i think is like an interesting",
    "start": "3035440",
    "end": "3040960"
  },
  {
    "text": "kind of area to see if that would work it's a great question yeah thank you yeah",
    "start": "3040960",
    "end": "3046720"
  },
  {
    "text": "how do you solve for a field where you what you want to achieve better than human performance or",
    "start": "3046720",
    "end": "3052400"
  },
  {
    "text": "better than that state-of-the-art how do you solve for like",
    "start": "3052400",
    "end": "3057680"
  },
  {
    "text": "reaching another city of performance when you use basically the administration coming from",
    "start": "3057680",
    "end": "3063440"
  },
  {
    "text": "so can you if you use demonstration coming for you yeah don't race",
    "start": "3063440",
    "end": "3069119"
  },
  {
    "text": "level and how do you solve for it in field where you want to reach by another level of performance",
    "start": "3069119",
    "end": "3076079"
  },
  {
    "text": "yeah that's a great question yeah so if i understand what you're asking like how do you how do you achieve this kind of superhuman performance and so i think",
    "start": "3076079",
    "end": "3083200"
  },
  {
    "text": "it's it's difficult one of the ways that we found that is kind of a natural way to get around",
    "start": "3083200",
    "end": "3088720"
  },
  {
    "text": "that is rather than kind of asking the human for demonstrations and trying to directly imitate that is we can ask for",
    "start": "3088720",
    "end": "3094640"
  },
  {
    "text": "these preferences over demonstrations and then that gives us some sense of kind of what things the human wave was",
    "start": "3094640",
    "end": "3100319"
  },
  {
    "text": "trying to avoid what things they were trying to do if i can kind of figure out what makes one trajectory better or worse than",
    "start": "3100319",
    "end": "3106160"
  },
  {
    "text": "another one then hopefully i've kind of attached on to like the key things that make things good or bad and if i'm like",
    "start": "3106160",
    "end": "3111839"
  },
  {
    "text": "a really efficient or really fast or really dexterous robot maybe i could do the good things more and the bad things less and so we found that this is like",
    "start": "3111839",
    "end": "3118640"
  },
  {
    "text": "one way of achieving that kind of like better than demonstrator performance",
    "start": "3118640",
    "end": "3124440"
  },
  {
    "text": "yeah great question yes um so in these slides on uh on this like",
    "start": "3124640",
    "end": "3133119"
  },
  {
    "text": "efficient beige and t-rex it seems that one of the benefits of",
    "start": "3133119",
    "end": "3140240"
  },
  {
    "text": "t-rex is that you need very few demonstrations to achieve good performance",
    "start": "3140240",
    "end": "3145599"
  },
  {
    "text": "but then you have this new development of training the um the cnn to learn the",
    "start": "3145599",
    "end": "3151839"
  },
  {
    "text": "embedding so how do you like how much more data do you end up needing",
    "start": "3151839",
    "end": "3157359"
  },
  {
    "text": "that's a good question so for the bayesian rex work so we had we actually used the same data that we had",
    "start": "3157359",
    "end": "3164800"
  },
  {
    "text": "collected for the t-rex paper and then we just applied these self-supervised and kind of unsupervised",
    "start": "3164800",
    "end": "3170240"
  },
  {
    "text": "task losses on top of that because we had found that just kind of using the supervised rankings to try and",
    "start": "3170240",
    "end": "3176640"
  },
  {
    "text": "learn this embedding didn't work that well and so we wanted to have like a richer embedding space because",
    "start": "3176640",
    "end": "3182480"
  },
  {
    "text": "remember kind of the key idea here is that we wanted a rich posterior distribution that would think about multiple hypotheses to",
    "start": "3182480",
    "end": "3188880"
  },
  {
    "text": "explain the data and so yeah so we're learning this latent space using these demonstrations",
    "start": "3188880",
    "end": "3194480"
  },
  {
    "text": "but also taking the demonstration data and we can train like an inverse dynamics model for a dynamics model",
    "start": "3194480",
    "end": "3200160"
  },
  {
    "text": "we can do like these kind of unsupervised kind of like auto encoder type losses on that i",
    "start": "3200160",
    "end": "3206559"
  },
  {
    "text": "think because we had like a wide range of demonstrations ranging from like really awful to like you know decent that gave",
    "start": "3206559",
    "end": "3213760"
  },
  {
    "text": "us a lot of good data for training these auxiliary losses but i definitely think yeah some situations you probably could",
    "start": "3213760",
    "end": "3219520"
  },
  {
    "text": "also benefit a lot probably in all situations could benefit a lot from having some offline data as well",
    "start": "3219520",
    "end": "3224559"
  },
  {
    "text": "which the agent could maybe explore maybe with some intrinsic motivation or some random interactions or maybe just",
    "start": "3224559",
    "end": "3230400"
  },
  {
    "text": "from like prior data that you have i think you could also use that data to try and learn this this embedding",
    "start": "3230400",
    "end": "3236880"
  },
  {
    "text": "function and that will help you also do the better bayesian inference",
    "start": "3236880",
    "end": "3242240"
  },
  {
    "text": "yes",
    "start": "3245359",
    "end": "3248359"
  },
  {
    "text": "okay now you should be able to figure it out from here or is that something that only the robot",
    "start": "3253280",
    "end": "3258720"
  },
  {
    "text": "decides when it doesn't need help anymore that's a great question so",
    "start": "3258720",
    "end": "3265599"
  },
  {
    "text": "yeah so what we looked at was where the robot got to decide",
    "start": "3265599",
    "end": "3271200"
  },
  {
    "text": "when to call in for help and the robot got to decide when to like let the human go back but i think that's like a really",
    "start": "3271200",
    "end": "3276960"
  },
  {
    "text": "interesting idea of having the human like say okay like i'm gonna like just like let you swim now let's see how well you",
    "start": "3276960",
    "end": "3283599"
  },
  {
    "text": "do and yeah we did not look at them i think that's like yeah a really interesting question about like",
    "start": "3283599",
    "end": "3289839"
  },
  {
    "text": "yeah how can the robot kind of use that like signal from the human maybe to learn better about",
    "start": "3289839",
    "end": "3296480"
  },
  {
    "text": "like yeah how much uncertainty it should have in different states or what it should do and yeah i think that's i think that's",
    "start": "3296480",
    "end": "3301760"
  },
  {
    "text": "often what these people do right like we're kind of like i'm teaching my kids so i have three kids and yeah i was",
    "start": "3301760",
    "end": "3306880"
  },
  {
    "text": "teaching my daughter how to like bicycle a couple years ago and you know i would kind of be pushing her to be like eventually i would like let go before",
    "start": "3306880",
    "end": "3312640"
  },
  {
    "text": "she said that she was ready and she she'd be peddling along because i was like pretty confident she could do it so i think we as people often will like say",
    "start": "3312640",
    "end": "3319359"
  },
  {
    "text": "okay like now you try it now i'm gonna stop helping you and yeah i don't know much work looking",
    "start": "3319359",
    "end": "3325280"
  },
  {
    "text": "at that but i think that's really interesting yeah",
    "start": "3325280",
    "end": "3330799"
  },
  {
    "text": "another question would be we feel different situation and like about financial market rates reward and like",
    "start": "3330799",
    "end": "3337520"
  },
  {
    "text": "transporting the rubber transporting some things how do you kind of flourise the risk and in the way",
    "start": "3337520",
    "end": "3343280"
  },
  {
    "text": "that you input into robot like how do you categorize different situations in terms",
    "start": "3343280",
    "end": "3348960"
  },
  {
    "text": "of risk yeah i think that's a good question so we're using yeah these financial models",
    "start": "3348960",
    "end": "3355359"
  },
  {
    "text": "of like value at risk and there is kind of this like hyper parameter for a lot of these models",
    "start": "3355359",
    "end": "3360480"
  },
  {
    "text": "which is like this alpha which kind of determines like how far out into the tail you kind of care about and",
    "start": "3360480",
    "end": "3366960"
  },
  {
    "text": "typically people just kind of set this at like like five percent i care about like kind of the five percent worst case outcomes",
    "start": "3366960",
    "end": "3372480"
  },
  {
    "text": "that's optimized by conditional value risk there but i think depending on the situation people's risk aversion definitely",
    "start": "3372480",
    "end": "3378319"
  },
  {
    "text": "changes and so i think yeah figuring out algorithms that can like allow you to calibrate to",
    "start": "3378319",
    "end": "3383839"
  },
  {
    "text": "a person's risk aversion could be a really interesting i mean we're kind of just like like",
    "start": "3383839",
    "end": "3388960"
  },
  {
    "text": "assuming in a lot of our work that humans just like optimizing some reward function we're trying to learn from",
    "start": "3388960",
    "end": "3394400"
  },
  {
    "text": "preferences like what that reward function might be or from demonstrations but you could also maybe try and learn a person's level of risk aversion i",
    "start": "3394400",
    "end": "3400720"
  },
  {
    "text": "think from watching demonstrations or looking at preferences and so yeah so that's something that we've kind of just",
    "start": "3400720",
    "end": "3406079"
  },
  {
    "text": "like set is like this hyperparameter but i think in practice you would definitely want to adapt to the domain i think",
    "start": "3406079",
    "end": "3412240"
  },
  {
    "text": "that's like a nice thing is that if you can adapt that you now have ai systems that can range from being like purely",
    "start": "3412240",
    "end": "3418319"
  },
  {
    "text": "kind of like maximizing expectation to like purely being like robust and we have done work",
    "start": "3418319",
    "end": "3424319"
  },
  {
    "text": "looking at this kind of efficient frontier between these two extremes and so by varying",
    "start": "3424319",
    "end": "3430559"
  },
  {
    "text": "these different parameters in the models we've actually looked at if you take like a convex combination of",
    "start": "3430559",
    "end": "3437119"
  },
  {
    "text": "objectives where one objective is just maximize expectation the other objective is maximize c bar and you can take like",
    "start": "3437119",
    "end": "3442799"
  },
  {
    "text": "a weighted combination of these to kind of carve out this pareto frontier of policies that range from things that you'll look good in expectations but",
    "start": "3442799",
    "end": "3448960"
  },
  {
    "text": "maybe have like wide tails to things that like look worse than expectation but like really hedge against risk",
    "start": "3448960",
    "end": "3455119"
  },
  {
    "text": "and i think yeah thinking about like ways to like show that to a person and help them figure out where on that efficient frontier they want to be is",
    "start": "3455119",
    "end": "3461520"
  },
  {
    "text": "like an interesting area of future work",
    "start": "3461520",
    "end": "3465040"
  },
  {
    "text": "to fail when this is so like if you don't have this is a function but like",
    "start": "3469359",
    "end": "3475599"
  },
  {
    "text": "the value that you're plotting in the c bar plot isn't as clear as in",
    "start": "3475599",
    "end": "3481280"
  },
  {
    "text": "what do you do is that is that question um",
    "start": "3481280",
    "end": "3487880"
  },
  {
    "text": "like two different like risk metrics these are like two different like evaluations of risk yeah that's a good question so we've",
    "start": "3498400",
    "end": "3505040"
  },
  {
    "text": "been looking at cases where we do have this like scalar notion of risk and",
    "start": "3505040",
    "end": "3511280"
  },
  {
    "text": "yeah i'm sure someone's probably looked at that but off the top of my head i don't know how you might deal with that but i think that's yeah an interesting",
    "start": "3511280",
    "end": "3516720"
  },
  {
    "text": "question yeah so i might be this is not my area of",
    "start": "3516720",
    "end": "3523280"
  },
  {
    "text": "expertise so it may not be a great question but a lot of these you're doing with a lot of human interactions you have the robots feeding people what",
    "start": "3523280",
    "end": "3530480"
  },
  {
    "text": "happens when you find your system and you have your set of reward functions but there's like",
    "start": "3530480",
    "end": "3537200"
  },
  {
    "text": "a very possible negative outcome like where you won't status maybe your",
    "start": "3537200",
    "end": "3542880"
  },
  {
    "text": "reward sack function is satisfied but the outcome is possibly hurting some third member how do you go",
    "start": "3542880",
    "end": "3550400"
  },
  {
    "text": "about like with the simple work functions you provided like i guess is there any way",
    "start": "3550400",
    "end": "3555520"
  },
  {
    "text": "to make sure you don't like technically you'd still be completing your reward to some highest level since you know maybe it's",
    "start": "3555520",
    "end": "3562240"
  },
  {
    "text": "building something you still built it but in the process like you're hurt you cause damage",
    "start": "3562240",
    "end": "3567440"
  },
  {
    "text": "somewhere else how do you balance that with your award function or is there a way to include that",
    "start": "3567440",
    "end": "3572799"
  },
  {
    "text": "yeah so i think one way would be to take this like robust optimization approach we've been",
    "start": "3572799",
    "end": "3578480"
  },
  {
    "text": "looking at where it sounds like you kind of have these two reward functions right under reward function and i'm doing really good",
    "start": "3578480",
    "end": "3584160"
  },
  {
    "text": "underwater function b i'm doing like terrible and so that's kind of the idea behind this royal algorithm is i want to",
    "start": "3584160",
    "end": "3589760"
  },
  {
    "text": "optimize a policy that hedges against these two different reward functions and tries to perform well under both and at",
    "start": "3589760",
    "end": "3596640"
  },
  {
    "text": "least not terribly under either and so i think that's like one potential approach that yeah could be applicable",
    "start": "3596640",
    "end": "3602240"
  },
  {
    "text": "this kind of setting where you have multiple different computing objectives or competing reward functions you want to make sure that",
    "start": "3602240",
    "end": "3607760"
  },
  {
    "text": "you're not performing terribly under any one of them but still you could add in like i talked about before a little bit of incentive",
    "start": "3607760",
    "end": "3613920"
  },
  {
    "text": "just to perform you know well in expectation because i think there's lots of different kind of i guess flavors of this type of",
    "start": "3613920",
    "end": "3619520"
  },
  {
    "text": "optimization but in particular something i'm very excited about is like how do you balance these computing objectives",
    "start": "3619520",
    "end": "3625200"
  },
  {
    "text": "i think the yeah the algorithm is one way of doing that",
    "start": "3625200",
    "end": "3629839"
  },
  {
    "text": "um given time limit i guess that's it for the question let things again [Applause]",
    "start": "3630799",
    "end": "3639499"
  }
]