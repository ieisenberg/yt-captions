[
  {
    "start": "0",
    "end": "16000"
  },
  {
    "start": "0",
    "end": "5700"
  },
  {
    "text": "OMAR KHATTAB: Hello, everyone.",
    "start": "5700",
    "end": "6950"
  },
  {
    "text": "Welcome to part five of\nour series on NLU and IR.",
    "start": "6950",
    "end": "10510"
  },
  {
    "text": "This screencast will be\nthe third among three",
    "start": "10510",
    "end": "12880"
  },
  {
    "text": "of our videos on neural IR.",
    "start": "12880",
    "end": "16914"
  },
  {
    "start": "16000",
    "end": "50000"
  },
  {
    "text": "In the previous screencast we\ndiscussed learning term weights",
    "start": "16914",
    "end": "20350"
  },
  {
    "text": "as a paradigm for building\nneural IR models that are",
    "start": "20350",
    "end": "23080"
  },
  {
    "text": "both efficient and effective.",
    "start": "23080",
    "end": "25480"
  },
  {
    "text": "We mentioned two such models\nfrom the IR literature, DeepCT",
    "start": "25480",
    "end": "29289"
  },
  {
    "text": "and doc2query.",
    "start": "29290",
    "end": "32008"
  },
  {
    "text": "Both of which, despite\noutperforming BM25 in MRR,",
    "start": "32009",
    "end": "35610"
  },
  {
    "text": "still left a very large\nmargin to the quality",
    "start": "35610",
    "end": "39030"
  },
  {
    "text": "that we see with BERT.",
    "start": "39030",
    "end": "41620"
  },
  {
    "text": "We asked ourselves\ncan we achieve",
    "start": "41620",
    "end": "43600"
  },
  {
    "text": "high MRR and low computational\ncost at the same time.",
    "start": "43600",
    "end": "48610"
  },
  {
    "text": "Can we do better?",
    "start": "48610",
    "end": "50610"
  },
  {
    "start": "50000",
    "end": "166000"
  },
  {
    "text": "To answer this\nquestion, let us begin",
    "start": "50610",
    "end": "52710"
  },
  {
    "text": "exploring more\nexpressive paradigms",
    "start": "52710",
    "end": "54750"
  },
  {
    "text": "for efficient neural IR.",
    "start": "54750",
    "end": "57450"
  },
  {
    "text": "The next paradigm here is\nthe representation similarity",
    "start": "57450",
    "end": "60660"
  },
  {
    "text": "paradigm.",
    "start": "60660",
    "end": "63350"
  },
  {
    "text": "In the representation\nsimilarity paradigm",
    "start": "63350",
    "end": "65390"
  },
  {
    "text": "we begin by tokenizing the\nquery and the document.",
    "start": "65390",
    "end": "68650"
  },
  {
    "text": "And we feed each of\nthem independently",
    "start": "68650",
    "end": "71110"
  },
  {
    "text": "through an encoder,\nlike BERT, for example.",
    "start": "71110",
    "end": "74620"
  },
  {
    "text": "This encoder is\nthen used to produce",
    "start": "74620",
    "end": "76450"
  },
  {
    "text": "a single-vector\nrepresentation, for the query",
    "start": "76450",
    "end": "79579"
  },
  {
    "text": "and for the\ndocument, separately.",
    "start": "79580",
    "end": "82770"
  },
  {
    "text": "So for BERT we could take\nthis through the class token,",
    "start": "82770",
    "end": "85200"
  },
  {
    "text": "for example and take\nthe output embeddings,",
    "start": "85200",
    "end": "88009"
  },
  {
    "text": "or we could average all\nthe final layer outputs.",
    "start": "88010",
    "end": "92640"
  },
  {
    "text": "Once we have those, we finally\ncalculate the relevant score",
    "start": "92640",
    "end": "95370"
  },
  {
    "text": "of this document to our\nquery as a single dot",
    "start": "95370",
    "end": "98010"
  },
  {
    "text": "product between two vectors.",
    "start": "98010",
    "end": "99915"
  },
  {
    "start": "99915",
    "end": "104860"
  },
  {
    "text": "This paradigm is very\nefficient for retrieval.",
    "start": "104860",
    "end": "108270"
  },
  {
    "text": "First each document\ncan be represented",
    "start": "108270",
    "end": "110399"
  },
  {
    "text": "as a vector offline.",
    "start": "110400",
    "end": "113110"
  },
  {
    "text": "And this precomputed\nrepresentation",
    "start": "113110",
    "end": "114910"
  },
  {
    "text": "can be stored on disk before we\neven start conducting search.",
    "start": "114910",
    "end": "120220"
  },
  {
    "text": "Moreover, the\nsimilarity computation",
    "start": "120220",
    "end": "122350"
  },
  {
    "text": "between a query\nand a document here",
    "start": "122350",
    "end": "123970"
  },
  {
    "text": "is very cheap and\nthus very efficient,",
    "start": "123970",
    "end": "126550"
  },
  {
    "text": "as it's just a single\nproduct between two vectors.",
    "start": "126550",
    "end": "128965"
  },
  {
    "start": "128965",
    "end": "132010"
  },
  {
    "text": "A very large number of IR models\nare Representations Similarity",
    "start": "132010",
    "end": "135220"
  },
  {
    "text": "models.",
    "start": "135220",
    "end": "136420"
  },
  {
    "text": "Many of those\nactually precede BERT.",
    "start": "136420",
    "end": "138910"
  },
  {
    "text": "Like DSSM and SNRM.",
    "start": "138910",
    "end": "142252"
  },
  {
    "text": "But the last year\nand a half, we've",
    "start": "142252",
    "end": "143709"
  },
  {
    "text": "seen numerous similarity models\nbased on BERT for IR tasks,",
    "start": "143710",
    "end": "147860"
  },
  {
    "text": "including SBERT, ORQA, DPR,\nDE-BERT and ANCE, among others.",
    "start": "147860",
    "end": "154600"
  },
  {
    "text": "Many of these models were\nactually proposed concurrently",
    "start": "154600",
    "end": "158140"
  },
  {
    "text": "with each other, and\ntheir primary differences",
    "start": "158140",
    "end": "160630"
  },
  {
    "text": "lie in the specific\ntasks that each one",
    "start": "160630",
    "end": "162580"
  },
  {
    "text": "targets and the supervision\napproach each one suggests.",
    "start": "162580",
    "end": "166500"
  },
  {
    "start": "166000",
    "end": "267000"
  },
  {
    "text": "So let us delve deeper\ninto a representative",
    "start": "166500",
    "end": "169260"
  },
  {
    "text": "in one of the earlier and most\npopular models among those.",
    "start": "169260",
    "end": "172860"
  },
  {
    "text": "This is the Dense\nPassage Retriever,",
    "start": "172860",
    "end": "174990"
  },
  {
    "text": "or DPR by Karpukhin\net al., which appeared",
    "start": "174990",
    "end": "178290"
  },
  {
    "text": "at EMNLP just a few months ago.",
    "start": "178290",
    "end": "181319"
  },
  {
    "text": "DPR encodes each\npassage, or document,",
    "start": "181320",
    "end": "184190"
  },
  {
    "text": "as a 768-dimentional vector,\nand similarly for each query.",
    "start": "184190",
    "end": "189920"
  },
  {
    "text": "During training DPR\nproduces a similarity score",
    "start": "189920",
    "end": "193700"
  },
  {
    "text": "between the query and\nthe positive passage--",
    "start": "193700",
    "end": "195625"
  },
  {
    "text": "so that's the\nrelevant passage we",
    "start": "195625",
    "end": "197000"
  },
  {
    "text": "wanted to retrieve-- as\nwell as between the query",
    "start": "197000",
    "end": "199370"
  },
  {
    "text": "and a few negatives.",
    "start": "199370",
    "end": "201170"
  },
  {
    "text": "Some of them are sampled\nfrom the BM25 top-100",
    "start": "201170",
    "end": "204020"
  },
  {
    "text": "and others are\nin-batch negatives,",
    "start": "204020",
    "end": "206130"
  },
  {
    "text": "which are actually positives,\nbut for other queries",
    "start": "206130",
    "end": "208460"
  },
  {
    "text": "in the same training batch.",
    "start": "208460",
    "end": "211310"
  },
  {
    "text": "Once DPR has all of those\nscores during training,",
    "start": "211310",
    "end": "213980"
  },
  {
    "text": "it then optimizes a\nclassification loss,",
    "start": "213980",
    "end": "216769"
  },
  {
    "text": "Namely N-way classification loss\nwith softmax-- cross entropy",
    "start": "216770",
    "end": "220925"
  },
  {
    "text": "loss with softmax over the\nscores of one positive and all",
    "start": "220925",
    "end": "224150"
  },
  {
    "text": "of these negatives, with\nthe target of selecting",
    "start": "224150",
    "end": "227409"
  },
  {
    "text": "the positive passage, of course.",
    "start": "227410",
    "end": "231060"
  },
  {
    "text": "DPR was not tested on\nthe MS MARCO dataset",
    "start": "231060",
    "end": "233700"
  },
  {
    "text": "by the original authors.",
    "start": "233700",
    "end": "235739"
  },
  {
    "text": "But subsequent work\nby Xiong et al.",
    "start": "235740",
    "end": "238740"
  },
  {
    "text": "bests a DPR like retriever on\nMS MARCO and achieves 31% MRR.",
    "start": "238740",
    "end": "244530"
  },
  {
    "text": "They also then suggest more\nsophisticated approaches",
    "start": "244530",
    "end": "248459"
  },
  {
    "text": "for supervision\nwhich can increase",
    "start": "248460",
    "end": "250020"
  },
  {
    "text": "this MRR by a couple of points.",
    "start": "250020",
    "end": "252270"
  },
  {
    "text": "So both of these demonstrate\nconsiderable progress",
    "start": "252270",
    "end": "255780"
  },
  {
    "text": "over the learned\nterm weight models",
    "start": "255780",
    "end": "258060"
  },
  {
    "text": "that we looked at before,\nlike DeepCT or doc2query.",
    "start": "258060",
    "end": "262060"
  },
  {
    "text": "But they still substantially\ntrail behind BERT's much higher",
    "start": "262060",
    "end": "265300"
  },
  {
    "text": "effectiveness.",
    "start": "265300",
    "end": "267699"
  },
  {
    "start": "267000",
    "end": "341000"
  },
  {
    "text": "So why is that?",
    "start": "267700",
    "end": "270750"
  },
  {
    "text": "As it turns out representation\nsimilarity models",
    "start": "270750",
    "end": "273000"
  },
  {
    "text": "suffer from two major downsides\nwhen it comes to IR tasks.",
    "start": "273000",
    "end": "278420"
  },
  {
    "text": "First, are their\nsingle-vector representations,",
    "start": "278420",
    "end": "282050"
  },
  {
    "text": "which involve cramming each\nquery and each document",
    "start": "282050",
    "end": "285680"
  },
  {
    "text": "into one rather low\ndimensional vector.",
    "start": "285680",
    "end": "289630"
  },
  {
    "text": "Second is their lack of\nfine grained interactions",
    "start": "289630",
    "end": "292330"
  },
  {
    "text": "during matching.",
    "start": "292330",
    "end": "293860"
  },
  {
    "text": "Representation similarity\nmodels estimate relevance as one",
    "start": "293860",
    "end": "296796"
  },
  {
    "text": "dot product between two vectors.",
    "start": "296796",
    "end": "299229"
  },
  {
    "text": "And as they lose the\nterm level interactions",
    "start": "299230",
    "end": "301720"
  },
  {
    "text": "between the query terms\nand the document terms",
    "start": "301720",
    "end": "304030"
  },
  {
    "text": "that we had in query document\ninteraction models like BERT--",
    "start": "304030",
    "end": "307030"
  },
  {
    "text": "in fact, even simple\nterm weighting models",
    "start": "307030",
    "end": "309970"
  },
  {
    "text": "like BM25 or DeepCT\nhad, by design,",
    "start": "309970",
    "end": "313930"
  },
  {
    "text": "some element of term\nlevel matching there",
    "start": "313930",
    "end": "316120"
  },
  {
    "text": "that we lose here.",
    "start": "316120",
    "end": "319530"
  },
  {
    "text": "So our next natural\nquestion then",
    "start": "319530",
    "end": "321500"
  },
  {
    "text": "becomes can we obtain\nthese efficiency benefits",
    "start": "321500",
    "end": "324560"
  },
  {
    "text": "of precomputation that we get\nfrom representation similarity",
    "start": "324560",
    "end": "328130"
  },
  {
    "text": "models while still keeping\nthe fine-grained term level",
    "start": "328130",
    "end": "331790"
  },
  {
    "text": "interactions that\nwe used to have",
    "start": "331790",
    "end": "333590"
  },
  {
    "text": "before with a model\nlike BERT or DeepCT?",
    "start": "333590",
    "end": "336290"
  },
  {
    "start": "336290",
    "end": "342200"
  },
  {
    "start": "341000",
    "end": "428000"
  },
  {
    "text": "Toward answering\nthat question, I",
    "start": "342200",
    "end": "344540"
  },
  {
    "text": "think it helps to\nreview the neural IR",
    "start": "344540",
    "end": "346430"
  },
  {
    "text": "paradigms we've seen so far.",
    "start": "346430",
    "end": "350440"
  },
  {
    "text": "On the left hand side, we looked\nat the learned term weights",
    "start": "350440",
    "end": "354820"
  },
  {
    "text": "paradigm.",
    "start": "354820",
    "end": "356200"
  },
  {
    "text": "These models offered independent\nand dependent encoding",
    "start": "356200",
    "end": "359170"
  },
  {
    "text": "of queries and documents,\nwhich was great for efficiency,",
    "start": "359170",
    "end": "361930"
  },
  {
    "text": "but they forced us to work\nwith a bag of words query",
    "start": "361930",
    "end": "364630"
  },
  {
    "text": "that loses all context.",
    "start": "364630",
    "end": "366880"
  },
  {
    "text": "And thus were not as competitive\nas we wanted them to be.",
    "start": "366880",
    "end": "370740"
  },
  {
    "text": "We then explored the\nrepresentation similarity",
    "start": "370740",
    "end": "373110"
  },
  {
    "text": "models, which also allowed us to\ncomplete independent encodings",
    "start": "373110",
    "end": "376530"
  },
  {
    "text": "of queries and documents,\nwhich again was",
    "start": "376530",
    "end": "378570"
  },
  {
    "text": "really useful for efficiency.",
    "start": "378570",
    "end": "382150"
  },
  {
    "text": "But this time we were forced\nto work with single vector",
    "start": "382150",
    "end": "385080"
  },
  {
    "text": "representations and we lost\nour fine-grained term level",
    "start": "385080",
    "end": "388539"
  },
  {
    "text": "interactions, which\nwe intuitively",
    "start": "388540",
    "end": "391150"
  },
  {
    "text": "believe to be very useful\nfor matching in IR tasks.",
    "start": "391150",
    "end": "394840"
  },
  {
    "start": "394840",
    "end": "397510"
  },
  {
    "text": "On the right hand side we\nlooked initially, actually,",
    "start": "397510",
    "end": "400600"
  },
  {
    "text": "at the query document\ninteraction models",
    "start": "400600",
    "end": "402550"
  },
  {
    "text": "like standard BERT classifiers.",
    "start": "402550",
    "end": "405330"
  },
  {
    "text": "These offered very high\naccuracy but were extremely",
    "start": "405330",
    "end": "409500"
  },
  {
    "text": "expensive to use because the\nentire computation for one",
    "start": "409500",
    "end": "413040"
  },
  {
    "text": "document depended on both\nthe query and the document.",
    "start": "413040",
    "end": "416340"
  },
  {
    "text": "We simply couldn't\ndo any precomputation",
    "start": "416340",
    "end": "418770"
  },
  {
    "text": "in this case offline in advance.",
    "start": "418770",
    "end": "422770"
  },
  {
    "text": "So can we somehow combine the\nadvantages of all these three",
    "start": "422770",
    "end": "426009"
  },
  {
    "text": "paradigms at once?",
    "start": "426010",
    "end": "429030"
  },
  {
    "start": "428000",
    "end": "548000"
  },
  {
    "text": "Before we answer\nthat question there's",
    "start": "429030",
    "end": "430889"
  },
  {
    "text": "actually one final feature,\none final capability",
    "start": "430890",
    "end": "434880"
  },
  {
    "text": "of the first two paradigms\nthat we should discuss.",
    "start": "434880",
    "end": "437910"
  },
  {
    "text": "So query document\ninteraction models,",
    "start": "437910",
    "end": "440190"
  },
  {
    "text": "which are quite\nexpensive, they forced us",
    "start": "440190",
    "end": "442350"
  },
  {
    "text": "to use a re-ranking pipeline.",
    "start": "442350",
    "end": "444630"
  },
  {
    "text": "This is a pipeline where\nwe re-scored the top 1,000",
    "start": "444630",
    "end": "447420"
  },
  {
    "text": "documents that we already\nretrieved by BM25.",
    "start": "447420",
    "end": "449520"
  },
  {
    "start": "449520",
    "end": "452919"
  },
  {
    "text": "Sometimes that's OK,\nbut in many cases",
    "start": "452920",
    "end": "455350"
  },
  {
    "text": "this can be a problem\nbecause it ties our recall",
    "start": "455350",
    "end": "459160"
  },
  {
    "text": "to the recall of BM25, which is\nultimately a model that relies",
    "start": "459160",
    "end": "463180"
  },
  {
    "text": "on finding terms that match\nexactly across queries",
    "start": "463180",
    "end": "466419"
  },
  {
    "text": "and documents, and\nso it can be quite",
    "start": "466420",
    "end": "468310"
  },
  {
    "text": "restrictive in many cases.",
    "start": "468310",
    "end": "470980"
  },
  {
    "text": "When recall is an\nimportant consideration,",
    "start": "470980",
    "end": "474250"
  },
  {
    "text": "you often want our neural\nmodel that we trained",
    "start": "474250",
    "end": "477130"
  },
  {
    "text": "to do end to end retrieval.",
    "start": "477130",
    "end": "479920"
  },
  {
    "text": "That is, to search quickly\nover all the documents",
    "start": "479920",
    "end": "483070"
  },
  {
    "text": "in our collection\ndirectly without",
    "start": "483070",
    "end": "485290"
  },
  {
    "text": "the re-ranking pipeline.",
    "start": "485290",
    "end": "488170"
  },
  {
    "text": "Learning term weights and\nrepresentation similarity",
    "start": "488170",
    "end": "491290"
  },
  {
    "text": "models that we've looked\nat so far alleviate",
    "start": "491290",
    "end": "494230"
  },
  {
    "text": "this constraint and this is\na big advantage for them.",
    "start": "494230",
    "end": "497870"
  },
  {
    "text": "So, specifically, when\nwe learn term weights,",
    "start": "497870",
    "end": "501110"
  },
  {
    "text": "we can save these weights\nin the inverted index",
    "start": "501110",
    "end": "503240"
  },
  {
    "text": "just like with BM25,\nand that allows",
    "start": "503240",
    "end": "506419"
  },
  {
    "text": "us to obtain fast retrieval.",
    "start": "506420",
    "end": "510100"
  },
  {
    "text": "When we learn vector\nrepresentations,",
    "start": "510100",
    "end": "512049"
  },
  {
    "text": "it also turns out\nthat we can index",
    "start": "512049",
    "end": "514150"
  },
  {
    "text": "these vectors using libraries\nfor fast vector similarity",
    "start": "514150",
    "end": "517719"
  },
  {
    "text": "search, like FAISS,\nf-a-i- double s.",
    "start": "517720",
    "end": "522139"
  },
  {
    "text": "This relies on efficient\ndata structures",
    "start": "522140",
    "end": "524330"
  },
  {
    "text": "that support pruning,\nwhich is basically",
    "start": "524330",
    "end": "527150"
  },
  {
    "text": "finding the top-K matches,\nsay the top 10 or the top 100",
    "start": "527150",
    "end": "530480"
  },
  {
    "text": "matches, without having\nto exhaustively enumerate",
    "start": "530480",
    "end": "534050"
  },
  {
    "text": "all possible candidates.",
    "start": "534050",
    "end": "537149"
  },
  {
    "text": "The details of search with\nthese pruning data structures",
    "start": "537150",
    "end": "540270"
  },
  {
    "text": "is beyond our scope,\nbut it's really useful",
    "start": "540270",
    "end": "542880"
  },
  {
    "text": "to be aware of this\nimportant capability for end",
    "start": "542880",
    "end": "545790"
  },
  {
    "text": "to end retrieval.",
    "start": "545790",
    "end": "549440"
  },
  {
    "start": "548000",
    "end": "720000"
  },
  {
    "text": "OK, so let's go back to\nour last main question.",
    "start": "549440",
    "end": "552990"
  },
  {
    "text": "Can we obtain the\nefficiency benefits",
    "start": "552990",
    "end": "554839"
  },
  {
    "text": "of precomputation while still\nhaving the fine-grained term",
    "start": "554840",
    "end": "558590"
  },
  {
    "text": "level interactions\nthat we used to have?",
    "start": "558590",
    "end": "562230"
  },
  {
    "text": "The neural IR paradigm that\nwill allow us to do this",
    "start": "562230",
    "end": "564990"
  },
  {
    "text": "is called Late Interaction,\nand this is something",
    "start": "564990",
    "end": "567600"
  },
  {
    "text": "that I've worked on\nhere at Stanford.",
    "start": "567600",
    "end": "570100"
  },
  {
    "text": "So let's build late\ninteraction from the ground up.",
    "start": "570100",
    "end": "573800"
  },
  {
    "text": "We'll start, as usual\nwith tokenization",
    "start": "573800",
    "end": "576170"
  },
  {
    "text": "of the query and the document.",
    "start": "576170",
    "end": "579269"
  },
  {
    "text": "We'll seek to\nindependently encode",
    "start": "579270",
    "end": "581460"
  },
  {
    "text": "the query and the document\nbut into fine-grained",
    "start": "581460",
    "end": "584400"
  },
  {
    "text": "representations this time.",
    "start": "584400",
    "end": "587880"
  },
  {
    "text": "So, as you can see on\nthe left hand side,",
    "start": "587880",
    "end": "589830"
  },
  {
    "text": "this is actually not hard.",
    "start": "589830",
    "end": "591230"
  },
  {
    "text": "As it's shown, we can feed\ntwo copies of BERT, the query,",
    "start": "591230",
    "end": "595190"
  },
  {
    "text": "and the document separately, and\nkeep all the output embeddings",
    "start": "595190",
    "end": "598565"
  },
  {
    "text": "for responding to\nall the tokens as",
    "start": "598565",
    "end": "600980"
  },
  {
    "text": "our fine-grained\nrepresentation for the query",
    "start": "600980",
    "end": "604220"
  },
  {
    "text": "and for the document.",
    "start": "604220",
    "end": "606819"
  },
  {
    "text": "OK.",
    "start": "606820",
    "end": "607320"
  },
  {
    "text": "So, we're only going to be\ndone here once we actually",
    "start": "607320",
    "end": "610800"
  },
  {
    "text": "close this loop.",
    "start": "610800",
    "end": "612130"
  },
  {
    "text": "Right?",
    "start": "612130",
    "end": "612630"
  },
  {
    "text": "We still need to estimate\nrelevance between this query",
    "start": "612630",
    "end": "615570"
  },
  {
    "text": "and that document.",
    "start": "615570",
    "end": "617570"
  },
  {
    "text": "Essentially we have\ntwo matrices and we",
    "start": "617570",
    "end": "619980"
  },
  {
    "text": "need a notion of similarity\nbetween these two matrices",
    "start": "619980",
    "end": "623240"
  },
  {
    "text": "or these two bags of vectors.",
    "start": "623240",
    "end": "625145"
  },
  {
    "start": "625145",
    "end": "628190"
  },
  {
    "text": "However, not every\napproach will suffice.",
    "start": "628190",
    "end": "630830"
  },
  {
    "text": "We insist that we get a\nscalable mechanism that",
    "start": "630830",
    "end": "633740"
  },
  {
    "text": "allows us to use vector\nsimilarity search with pruning",
    "start": "633740",
    "end": "637339"
  },
  {
    "text": "to conduct end to end\nretrieval in a scalable fashion",
    "start": "637340",
    "end": "640910"
  },
  {
    "text": "across the entire collection.",
    "start": "640910",
    "end": "644519"
  },
  {
    "text": "In doing this, it turns out\nthat a very simple interaction",
    "start": "644520",
    "end": "649080"
  },
  {
    "text": "mechanism offers both\nscaling and high quality.",
    "start": "649080",
    "end": "652260"
  },
  {
    "start": "652260",
    "end": "654830"
  },
  {
    "text": "So here's what we'll do.",
    "start": "654830",
    "end": "656290"
  },
  {
    "text": "For each query embedding,\nas I show here,",
    "start": "656290",
    "end": "659800"
  },
  {
    "text": "we compute a maximum\nsimilarity score",
    "start": "659800",
    "end": "662080"
  },
  {
    "text": "across all of the\ndocument embeddings.",
    "start": "662080",
    "end": "666290"
  },
  {
    "text": "So this is just going to be\na cosine similarity giving us",
    "start": "666290",
    "end": "670430"
  },
  {
    "text": "a single partial score\nfor this query term, which",
    "start": "670430",
    "end": "674630"
  },
  {
    "text": "is the maximum cosine\nsimilarity across all",
    "start": "674630",
    "end": "677510"
  },
  {
    "text": "of the blue embeddings\nin this case.",
    "start": "677510",
    "end": "679010"
  },
  {
    "start": "679010",
    "end": "683950"
  },
  {
    "text": "We'll repeat this here for\nall the query embeddings",
    "start": "683950",
    "end": "687300"
  },
  {
    "text": "and we'll simply sum all of\nthese maximum similarity scores",
    "start": "687300",
    "end": "691830"
  },
  {
    "text": "to get our final score\nfor this document.",
    "start": "691830",
    "end": "696260"
  },
  {
    "text": "So, we will refer to\nthis general paradigm",
    "start": "696260",
    "end": "698440"
  },
  {
    "text": "here as late interaction, and to\nthis specific model shown here",
    "start": "698440",
    "end": "702790"
  },
  {
    "text": "on top of BERT as ColBERT.",
    "start": "702790",
    "end": "705540"
  },
  {
    "text": "And the intuition is simple.",
    "start": "705540",
    "end": "707279"
  },
  {
    "text": "For every term in\nthe query, we're",
    "start": "707280",
    "end": "709650"
  },
  {
    "text": "just trying to softly\nand contextually locate",
    "start": "709650",
    "end": "713190"
  },
  {
    "text": "that term in the document,\nassigning a score to how",
    "start": "713190",
    "end": "716940"
  },
  {
    "text": "successful this matching was.",
    "start": "716940",
    "end": "718470"
  },
  {
    "start": "718470",
    "end": "721120"
  },
  {
    "start": "720000",
    "end": "857000"
  },
  {
    "text": "Let me illustrate this with a\nreal example from the MS MARCO",
    "start": "721120",
    "end": "724360"
  },
  {
    "text": "ranking development\nset, and I hope",
    "start": "724360",
    "end": "725950"
  },
  {
    "text": "it will be quite\nintuitive once you see it.",
    "start": "725950",
    "end": "728940"
  },
  {
    "text": "At the top is a query\nand at the bottom",
    "start": "728940",
    "end": "731250"
  },
  {
    "text": "is a portion of\nthe correct passage",
    "start": "731250",
    "end": "732870"
  },
  {
    "text": "that ColBERT retrieves\nat position one.",
    "start": "732870",
    "end": "736029"
  },
  {
    "text": "Because we have the simple\nlate interaction mechanism,",
    "start": "736030",
    "end": "739210"
  },
  {
    "text": "we can actually\nexplore the behavior.",
    "start": "739210",
    "end": "741490"
  },
  {
    "text": "And we can see in this\nparticular example that ColBERT",
    "start": "741490",
    "end": "744550"
  },
  {
    "text": "matches, through maximum\nsimilarity operators, the word",
    "start": "744550",
    "end": "749200"
  },
  {
    "text": "\"when\" in the question with\nthe word \"on\" in the phrase",
    "start": "749200",
    "end": "752620"
  },
  {
    "text": "\"on August 8,\" which is a\ndate as we might expect.",
    "start": "752620",
    "end": "757410"
  },
  {
    "text": "It matches the\nword \"transformers\"",
    "start": "757410",
    "end": "759209"
  },
  {
    "text": "with the same word\nin the document.",
    "start": "759210",
    "end": "761070"
  },
  {
    "text": "It matches \"cartoon\"\nwith \"animated.\"",
    "start": "761070",
    "end": "763560"
  },
  {
    "text": "And it matches the individual\nwords, \"come\" and \"out,\"",
    "start": "763560",
    "end": "766170"
  },
  {
    "text": "with the term\n\"released\" in the phrase",
    "start": "766170",
    "end": "769829"
  },
  {
    "text": "\"it was released on August\n8th\" in the document,",
    "start": "769830",
    "end": "775140"
  },
  {
    "text": "as we might intuitively expect.",
    "start": "775140",
    "end": "777300"
  },
  {
    "text": "So, we're basically just\ntrying to contextually match",
    "start": "777300",
    "end": "779519"
  },
  {
    "text": "these query terms\nin the document",
    "start": "779520",
    "end": "782960"
  },
  {
    "text": "and assign some matching\nscore for each of these terms.",
    "start": "782960",
    "end": "786960"
  },
  {
    "text": "So notice here and remember\nthat ColBERT represents",
    "start": "786960",
    "end": "790100"
  },
  {
    "text": "each document as a dense\nmatrix of many vectors",
    "start": "790100",
    "end": "793339"
  },
  {
    "text": "and in particular\none vector per token.",
    "start": "793340",
    "end": "795770"
  },
  {
    "text": "And this differs from\nthe representation",
    "start": "795770",
    "end": "797660"
  },
  {
    "text": "similarity models\nwe looked at before,",
    "start": "797660",
    "end": "800430"
  },
  {
    "text": "which try to cram each\ndocument into one vector.",
    "start": "800430",
    "end": "803029"
  },
  {
    "text": "And what makes this possible\nis the maximum similarity",
    "start": "803030",
    "end": "806510"
  },
  {
    "text": "operators that we have on top\nof these matrix representations.",
    "start": "806510",
    "end": "811670"
  },
  {
    "text": "So how well does ColBERT do?",
    "start": "811670",
    "end": "815620"
  },
  {
    "text": "And how does it do\nwith this gap that we",
    "start": "815620",
    "end": "817420"
  },
  {
    "text": "have here between\nefficient models and highly",
    "start": "817420",
    "end": "820149"
  },
  {
    "text": "effective ones?",
    "start": "820150",
    "end": "822460"
  },
  {
    "text": "Well, by redesigning\nthe model architecture",
    "start": "822460",
    "end": "825310"
  },
  {
    "text": "and offering a late\ninteraction paradigm,",
    "start": "825310",
    "end": "827620"
  },
  {
    "text": "ColBERT allows us to achieve\nquality comparative with BERT",
    "start": "827620",
    "end": "830710"
  },
  {
    "text": "at a small fraction\nof the costs.",
    "start": "830710",
    "end": "832165"
  },
  {
    "start": "832165",
    "end": "834699"
  },
  {
    "text": "Perhaps more\nimportantly, ColBERT",
    "start": "834700",
    "end": "836900"
  },
  {
    "text": "can scale through the entire\ncollection due to pruning",
    "start": "836900",
    "end": "839680"
  },
  {
    "text": "with end to end retrieval.",
    "start": "839680",
    "end": "841570"
  },
  {
    "text": "All 9 million passages\nhere in this case,",
    "start": "841570",
    "end": "844600"
  },
  {
    "text": "while maintaining\nsubsecond latencies,",
    "start": "844600",
    "end": "848720"
  },
  {
    "text": "And thus it allows\nmuch higher recall",
    "start": "848720",
    "end": "850699"
  },
  {
    "text": "than traditional re-ranking\npipelines permit.",
    "start": "850700",
    "end": "853070"
  },
  {
    "start": "853070",
    "end": "858170"
  },
  {
    "start": "857000",
    "end": "921000"
  },
  {
    "text": "All right.",
    "start": "858170",
    "end": "858820"
  },
  {
    "text": "So far we've looked at in-domain\neffectiveness evaluations.",
    "start": "858820",
    "end": "861970"
  },
  {
    "text": "Basically cases where we\nhad training and evaluation",
    "start": "861970",
    "end": "865060"
  },
  {
    "text": "data for the IR task at hand,\nwhich was MS MARCO, so far.",
    "start": "865060",
    "end": "869940"
  },
  {
    "text": "But we often want\nto use retrieval",
    "start": "869940",
    "end": "873450"
  },
  {
    "text": "in new out-of-domain settings.",
    "start": "873450",
    "end": "875040"
  },
  {
    "text": "We just want to throw our search\nengine at a difficult problem",
    "start": "875040",
    "end": "878759"
  },
  {
    "text": "without training data,\nwithout validation data,",
    "start": "878760",
    "end": "881490"
  },
  {
    "text": "and see it perform well.",
    "start": "881490",
    "end": "884520"
  },
  {
    "text": "We briefly discussed\nBEIR before,",
    "start": "884520",
    "end": "887620"
  },
  {
    "text": "which is a recent\neffort to test IR",
    "start": "887620",
    "end": "891420"
  },
  {
    "text": "models in a zero-shot setting\nwhere the models are trained",
    "start": "891420",
    "end": "895230"
  },
  {
    "text": "on one IR task and\nthen they're fixed,",
    "start": "895230",
    "end": "898290"
  },
  {
    "text": "and then they are tested on\na completely different set",
    "start": "898290",
    "end": "901980"
  },
  {
    "text": "of tasks.",
    "start": "901980",
    "end": "903389"
  },
  {
    "text": "BEIR includes 17 IR data sets\nand there are nine different IR",
    "start": "903390",
    "end": "908640"
  },
  {
    "text": "tasks or scenarios, and\nthe authors, Nandan et al.,",
    "start": "908640",
    "end": "912090"
  },
  {
    "text": "compared a lot of the IR\nmodels that we discussed today",
    "start": "912090",
    "end": "914820"
  },
  {
    "text": "in a zero-shot manner\nagainst each other",
    "start": "914820",
    "end": "917370"
  },
  {
    "text": "across all of these tasks.",
    "start": "917370",
    "end": "919525"
  },
  {
    "text": "So let's take a look.",
    "start": "919525",
    "end": "920400"
  },
  {
    "start": "920400",
    "end": "923550"
  },
  {
    "start": "921000",
    "end": "980000"
  },
  {
    "text": "Here we have BM25 results for\nan interaction model, which",
    "start": "923550",
    "end": "928769"
  },
  {
    "text": "is in this case ELECTRA, which\ntends to perform slightly",
    "start": "928770",
    "end": "931980"
  },
  {
    "text": "better than BERT for ranking.",
    "start": "931980",
    "end": "933690"
  },
  {
    "text": "We have two representation\nsimilarity models, DPR",
    "start": "933690",
    "end": "936810"
  },
  {
    "text": "and SBERT.",
    "start": "936810",
    "end": "938580"
  },
  {
    "text": "And we have a late interaction\nmodel, which is ColBERT.",
    "start": "938580",
    "end": "942720"
  },
  {
    "text": "The best in each row-- in\neach IR task is shown in bold.",
    "start": "942720",
    "end": "947670"
  },
  {
    "text": "And we see that across all tasks\nthe strongest model at NDCG@10",
    "start": "947670",
    "end": "953339"
  },
  {
    "text": "is always one of\nthe three models",
    "start": "953340",
    "end": "955140"
  },
  {
    "text": "that involve term level\ninteractions, which",
    "start": "955140",
    "end": "957420"
  },
  {
    "text": "are ELECTRA, ColBERT, and BM25.",
    "start": "957420",
    "end": "961220"
  },
  {
    "text": "Interestingly, the\nsingle vector approaches,",
    "start": "961220",
    "end": "963889"
  },
  {
    "text": "which seemed quite\npromising so far,",
    "start": "963890",
    "end": "966650"
  },
  {
    "text": "failed to generalize robustly\naccording to these results.",
    "start": "966650",
    "end": "971110"
  },
  {
    "text": "Whereas ColBERT, which\nis also a fast model,",
    "start": "971110",
    "end": "974360"
  },
  {
    "text": "almost matches the quality of\nthe expensive ELECTRA ranker.",
    "start": "974360",
    "end": "977630"
  },
  {
    "start": "977630",
    "end": "980620"
  },
  {
    "start": "980000",
    "end": "1040000"
  },
  {
    "text": "The results, so far, were\non the metric NDCG@10,",
    "start": "980620",
    "end": "983710"
  },
  {
    "text": "which is a precision\noriented metric--",
    "start": "983710",
    "end": "985870"
  },
  {
    "text": "looks at the top results.",
    "start": "985870",
    "end": "987970"
  },
  {
    "text": "But here I have the\nauthor's results",
    "start": "987970",
    "end": "991300"
  },
  {
    "text": "after the task level aggregation\nconsidering recall at 100.",
    "start": "991300",
    "end": "997540"
  },
  {
    "text": "And here, although we see\nthat the results are rather",
    "start": "997540",
    "end": "1005570"
  },
  {
    "text": "similar when we consider\nrecall, one major difference",
    "start": "1005570",
    "end": "1010190"
  },
  {
    "text": "is that ColBERT's late\ninteraction mechanism, which",
    "start": "1010190",
    "end": "1012680"
  },
  {
    "text": "allows it to conduct end to end\nretrieval with high quality,",
    "start": "1012680",
    "end": "1016399"
  },
  {
    "text": "allows it to achieve the\nstrongest recall in this case.",
    "start": "1016400",
    "end": "1021100"
  },
  {
    "text": "And so we can\nconclude, basically,",
    "start": "1021100",
    "end": "1022649"
  },
  {
    "text": "that scalable\nfine-grained interaction",
    "start": "1022650",
    "end": "1025290"
  },
  {
    "text": "is key to robustly high recall.",
    "start": "1025290",
    "end": "1029699"
  },
  {
    "text": "Of course notice that the\nBM25 and ELECTRA recall here",
    "start": "1029700",
    "end": "1033689"
  },
  {
    "text": "is the same since ELECTRA\njust re-scores the top 100,",
    "start": "1033690",
    "end": "1037439"
  },
  {
    "text": "in this case, from BM25.",
    "start": "1037440",
    "end": "1041059"
  },
  {
    "start": "1040000",
    "end": "1078000"
  },
  {
    "text": "So this concludes our neural\nIR section of the NLU plus IR",
    "start": "1041060",
    "end": "1044780"
  },
  {
    "text": "series.",
    "start": "1044780",
    "end": "1046250"
  },
  {
    "text": "In the next screencast,\nwe will discuss",
    "start": "1046250",
    "end": "1049010"
  },
  {
    "text": "how scalability with\nthese retriever models",
    "start": "1049010",
    "end": "1052340"
  },
  {
    "text": "can actually drive\nlarge gains in quality,",
    "start": "1052340",
    "end": "1054840"
  },
  {
    "text": "not just speed,\nwhich we haven't seen",
    "start": "1054840",
    "end": "1057350"
  },
  {
    "text": "so far except on\nthe recall case,",
    "start": "1057350",
    "end": "1059809"
  },
  {
    "text": "and how tuning a\nneural IR model fits",
    "start": "1059810",
    "end": "1062120"
  },
  {
    "text": "into a larger downstream\nopen domain NLU task.",
    "start": "1062120",
    "end": "1066230"
  },
  {
    "start": "1066230",
    "end": "1077483"
  }
]