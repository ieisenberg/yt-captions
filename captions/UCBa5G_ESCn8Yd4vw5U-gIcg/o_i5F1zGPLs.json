[
  {
    "start": "0",
    "end": "368000"
  },
  {
    "text": "So before we get started, I'm just gonna say a brief note about the logistics for the midterm. We're gonna be split across two rooms,",
    "start": "5240",
    "end": "11770"
  },
  {
    "text": "the room you're in, it depends on your normal Stanford ID, whatever the first letter is. We'll send an email out about this to confirm it.",
    "start": "11770",
    "end": "19960"
  },
  {
    "text": "But we're gonna be in either Gates B1 or Cubberley Auditorium, and it depends on the first- the first letter of your Stanford ID.",
    "start": "19960",
    "end": "27865"
  },
  {
    "text": "In addition, you're allowed to have one page of notes, typed or written is fine, one sided.",
    "start": "27865",
    "end": "33790"
  },
  {
    "text": "Anybody have any other questions about the midterm? Okay, well, reach out to us on Piazza if you have any questions about the midterm.",
    "start": "33790",
    "end": "42504"
  },
  {
    "text": "Um, what we'll do today is we're gonna split- we're gonna go finish up the rest of policy gradient.",
    "start": "42505",
    "end": "48530"
  },
  {
    "text": "So in terms of where we are in the class, right now. We are almost done with policy search.",
    "start": "48530",
    "end": "54260"
  },
  {
    "text": "We're gonna have the midterm on Wednesday, Monday is a holiday and then we'll also be releasing the last homework this week,",
    "start": "54260",
    "end": "60110"
  },
  {
    "text": "which will be over policy search. And so we're gonna have policy search, and then we're gonna have the project.",
    "start": "60110",
    "end": "65840"
  },
  {
    "text": "That's the remaining sort of main assignments for the term. And then we're gonna be getting into fast exploration and sort of",
    "start": "65840",
    "end": "71720"
  },
  {
    "text": "fast reinforcement learning after we come back from the midterm. So I wanted to make sure to get through policy search",
    "start": "71720",
    "end": "77390"
  },
  {
    "text": "today because you're gonna have the assignment released later this week. So we'll spend hopefully around like 20 to 25 minutes on policy search,",
    "start": "77390",
    "end": "83180"
  },
  {
    "text": "and then we're gonna do a brief review before we get into the midterm- before- about the midterm material.",
    "start": "83180",
    "end": "88300"
  },
  {
    "text": "Does anybody have any questions? Oh, and just a friendly reminder to please say",
    "start": "88300",
    "end": "93920"
  },
  {
    "text": "your name whenever you ask a question because it helps me remember. It also helps everybody else learn your names as well. All right.",
    "start": "93920",
    "end": "100530"
  },
  {
    "text": "So where we were is that for the last couple of lectures, we've been starting to talk about policy-based reinforcement learning,",
    "start": "100530",
    "end": "105979"
  },
  {
    "text": "where we're specifically trying to find a parameterized policy to learn how to make good decisions in the environment.",
    "start": "105980",
    "end": "112250"
  },
  {
    "text": "And so just like what we saw with value function approximation and what you're doing with Atari, for our policy parameterization we're gonna assume there's some vector of parameters.",
    "start": "112250",
    "end": "121010"
  },
  {
    "text": "We can represent policies by things like softmax or by a deep neural network.",
    "start": "121010",
    "end": "126610"
  },
  {
    "text": "And then we're gonna wanna be able to take gradients of these types of policies in order to learn a policy that has a high value.",
    "start": "126610",
    "end": "134070"
  },
  {
    "text": "So we introduced sort of the vanilla policy gradient algorithm, where the idea is that you start off,",
    "start": "134260",
    "end": "140620"
  },
  {
    "text": "your initialize your policy in some way. And you also have some baseline, and then across different iterations you run out your current policy.",
    "start": "140620",
    "end": "149300"
  },
  {
    "text": "And the goal is we're, by running these out that we're gonna be able to estimate the gradient. So we're gonna be doing this part to estimate the gradient of our policy at the current, so",
    "start": "149300",
    "end": "160680"
  },
  {
    "text": "we wanna get sort of dV d theta with respect to our current policy.",
    "start": "160680",
    "end": "166280"
  },
  {
    "text": "So what we talked about is that you would run out trajectories from your current policy. So you'd use your policy to execute in the environment.",
    "start": "166280",
    "end": "172925"
  },
  {
    "text": "You would get state action rewards, next state, next section, next rewards. And then you would look at returns and advantage estimates,",
    "start": "172925",
    "end": "181430"
  },
  {
    "text": "which compared your returns to a baseline, could refit your baseline, and then you could update your policy.",
    "start": "181430",
    "end": "188105"
  },
  {
    "text": "And so this was sort of the most vanilla policy gradient algorithm we talked about. And then we started saying that there's a number of different choices um,",
    "start": "188105",
    "end": "195650"
  },
  {
    "text": "that we're making in this algorithm and almost all sort of policy gradient based algorithms are gonna form uh,",
    "start": "195650",
    "end": "200885"
  },
  {
    "text": "follow this type of formula. So, in particular, we are making a decision that's",
    "start": "200885",
    "end": "206480"
  },
  {
    "text": "sort of estimating some sort of return or targets. Often we're picking a baseline,",
    "start": "206480",
    "end": "212540"
  },
  {
    "text": "and then we had to make some decision about after we compute the gradient how far along the gradient do we go?",
    "start": "212540",
    "end": "219095"
  },
  {
    "text": "So this is sort of helping us to determine how far do we move on our gradient?",
    "start": "219095",
    "end": "227310"
  },
  {
    "text": "Okay, so for the first part, we talked about how do we estimate sort of the value of where we are right now,",
    "start": "235820",
    "end": "241360"
  },
  {
    "text": "that we're gonna be using to try to estimate our gradient. And we talked about the fact that the most vanilla thing that we could",
    "start": "241360",
    "end": "247970"
  },
  {
    "text": "do is just roll out the policy and look at the returns, that this was really similar to what we'd seen in Monte Carlo estimates.",
    "start": "247970",
    "end": "255390"
  },
  {
    "text": "So we could do that, we could get sort of an estimate of the value function by just rolling out the policy for one episode,",
    "start": "255470",
    "end": "262550"
  },
  {
    "text": "but that was just like what we saw in Monte Carlo, an unbiased estimator but high variance.",
    "start": "262550",
    "end": "268460"
  },
  {
    "text": "And so we talked about how we could play all the sorts of- we use all the same tools as what we've been doing in the past to try to balance between bias and variance.",
    "start": "268460",
    "end": "278035"
  },
  {
    "text": "So in particular, we talked about how we could introduce bias using bootstrapping and function approximation.",
    "start": "278035",
    "end": "283775"
  },
  {
    "text": "Just like what we saw in TD MC and just like what we talked about with value function approximation.",
    "start": "283775",
    "end": "289145"
  },
  {
    "text": "So we repeatedly see these same sorts of ideas of the fact that we're trying to understand what the value is of a particular policy.",
    "start": "289145",
    "end": "296599"
  },
  {
    "text": "And when we're estimating that value, then we can trade off between getting sort of unbiased estimators of how",
    "start": "296599",
    "end": "302960"
  },
  {
    "text": "good decisions are versus biased estimators um, that might allow us to propagate information more",
    "start": "302960",
    "end": "308870"
  },
  {
    "text": "quickly and allow us to learn to make better decisions faster. We also talked about the fact that they're actor-critic methods that both maintain",
    "start": "308870",
    "end": "317030"
  },
  {
    "text": "an explicit parameterized representation of the policy and a parameterized representation of the value function.",
    "start": "317030",
    "end": "323525"
  },
  {
    "text": "But the thing that we really started getting into last time is to say, \"Well, both you know there are all these sort of existing techniques that we know",
    "start": "323525",
    "end": "330710"
  },
  {
    "text": "of to try to estimate these targets and estimate the value function.\" But then there's this additional question of how far do we move along the gradient?",
    "start": "330710",
    "end": "339785"
  },
  {
    "text": "So once we estimate the gradient of the policy, we need to figure out how far along that gradient do we",
    "start": "339785",
    "end": "346340"
  },
  {
    "text": "go in terms of computing a new policy. And the reason we argued this was particularly important",
    "start": "346340",
    "end": "351770"
  },
  {
    "text": "in reinforcement learning versus supervised learning, is that whatever step we take, whatever new policy we look at,",
    "start": "351770",
    "end": "358220"
  },
  {
    "text": "is gonna determine the data we get next. And so it was a particularly important for us to think about how",
    "start": "358220",
    "end": "363500"
  },
  {
    "text": "far along do we wanna go on our gradient to get a new policy. And one desirable property we were talking about is,",
    "start": "363500",
    "end": "371570"
  },
  {
    "start": "368000",
    "end": "603000"
  },
  {
    "text": "how do we ensure monotonic improvement? So what we would like here is we'd really like monotonic improvement, that's our goal.",
    "start": "371570",
    "end": "385265"
  },
  {
    "text": "And we talked about wanting monotonic improvement, which was not guaranteed in DQN or a lot of other algorithms.",
    "start": "385265",
    "end": "392615"
  },
  {
    "text": "Because in a lot of high stakes domains, like finance, customers, patients,",
    "start": "392615",
    "end": "397935"
  },
  {
    "text": "you might really wanna ensure that the new policy you're deploying is expected to be better at least in expectation before you deploy it.",
    "start": "397935",
    "end": "405220"
  },
  {
    "text": "So we talked about wanting this, but there's this big problem that we don't have data from the new policies that we're considering.",
    "start": "405220",
    "end": "411919"
  },
  {
    "text": "And we don't wanna try out all the possible next policies because some of them might be bad. And so we wanted to try to use our existing data to figure out how do we",
    "start": "411920",
    "end": "419810"
  },
  {
    "text": "take a step and determine a new policy that we think is gonna be good.",
    "start": "419810",
    "end": "424260"
  },
  {
    "text": "So, in particular, our main goal for policy gradients is to try",
    "start": "424970",
    "end": "430280"
  },
  {
    "text": "to find a set of policy parameters that maximize our value function. And the challenge is that we currently have access to",
    "start": "430280",
    "end": "438080"
  },
  {
    "text": "data that was gathered with our current policy, which we're gonna call pi old.",
    "start": "438080",
    "end": "443640"
  },
  {
    "text": "That's parameterized by a set of thetas, that we can also denote by theta old.",
    "start": "446310",
    "end": "452860"
  },
  {
    "text": "And- and throughout policy, the policy gradient lectures that I've been through, going back and forth between talking about policies and talking about thetas.",
    "start": "452860",
    "end": "460465"
  },
  {
    "text": "But it's good just to remember that there's, sort of, this direct mapping between pi and theta. You know there's- for each- for a policy,",
    "start": "460465",
    "end": "467770"
  },
  {
    "text": "it's exactly defined by a set of parameters. [NOISE] So whether we're talking about policies,",
    "start": "467770",
    "end": "476860"
  },
  {
    "text": "or we're talking about parameters, those are referring to exactly the same thing. Um, so the challenge is- is that we have data from our current policy,",
    "start": "476860",
    "end": "485380"
  },
  {
    "text": "um, which has some set of parameters, and we want to predict the value of a different policy. And so this is a challenge with off policy learning.",
    "start": "485380",
    "end": "493070"
  },
  {
    "text": "So what we tal- were talking about last time, um, is how do we express the value of a policy in terms of stuff that we do know?",
    "start": "494700",
    "end": "502944"
  },
  {
    "text": "Um, and we talked about how we could write it down in terms of an advantage over the current policy. So if we think about a value being parameterized by",
    "start": "502945",
    "end": "511569"
  },
  {
    "text": "a new set- a new policy with a new set of theta tilde parameters, it's equal to the value of another policy parameterized",
    "start": "511570",
    "end": "520090"
  },
  {
    "text": "by a set of Theta plus the expected advantage. So we can write that as the distribution of states that we would expect to get to under",
    "start": "520090",
    "end": "529930"
  },
  {
    "text": "the new policy times the advantage we would get under the old policy if we were to follow the new policy.",
    "start": "529930",
    "end": "538790"
  },
  {
    "text": "And the reason- what- what we're trying to do in this case is just to- to keep us thinking about what the main goal is here.",
    "start": "540930",
    "end": "547255"
  },
  {
    "text": "Is what we're trying to do is figure out a way to do policy gradient where we're guaranteed to have monotonic improvement,",
    "start": "547255",
    "end": "552505"
  },
  {
    "text": "where our new policy is gonna be guaranteed to be better than our old policy. But we wanna do this without actually trying out our new policy.",
    "start": "552505",
    "end": "559959"
  },
  {
    "text": "So we are trying to re-express what the value is of a new policy in terms of quantities we have access to.",
    "start": "559960",
    "end": "566395"
  },
  {
    "text": "So what do we have access to? We have access to existing samples from the current policy, and we wanna use those and the returns we've observed in order to",
    "start": "566395",
    "end": "574780"
  },
  {
    "text": "estimate the value of a new policy before we deploy it. So that's kinda where we're trying to get to.",
    "start": "574780",
    "end": "581365"
  },
  {
    "text": "And we noticed here that maybe, you know, we can have access to an explicit form of a new policy,",
    "start": "581365",
    "end": "586839"
  },
  {
    "text": "that's like whatever new parameters we're considering putting into our neural network. Um, and we could imagine estimating the advantage function,",
    "start": "586840",
    "end": "594624"
  },
  {
    "text": "but we don't know the state distribution under the new policy, because that would require us actually to run it.",
    "start": "594624",
    "end": "602060"
  },
  {
    "start": "603000",
    "end": "1036000"
  },
  {
    "text": "So what we talked about is- let's just define a new objective function, um, which is just a different objective function.",
    "start": "603270",
    "end": "611320"
  },
  {
    "text": "Might be good, might be bad. I'm going to argue to it's a good- it's good but this right now is just a quantity that we can optimize.",
    "start": "611320",
    "end": "617440"
  },
  {
    "text": "So the quantity that we can optimize here, we're gonna call that sort of this new objective function,",
    "start": "617440",
    "end": "623035"
  },
  {
    "text": "and it is going to be a function of the previous value. So here, remember this is always equal to [NOISE] direct mapping between thetas and pis.",
    "start": "623035",
    "end": "633040"
  },
  {
    "text": "So we're going to just say, it looks like the objective function we just talked about, which really was the value of the new policy,",
    "start": "633040",
    "end": "639774"
  },
  {
    "text": "but we don't know what that stationary weighted distribution is, of states under the new policy.",
    "start": "639775",
    "end": "645025"
  },
  {
    "text": "So we're just gonna substitute in the stationary distribution under the current policy.",
    "start": "645025",
    "end": "650660"
  },
  {
    "text": "Now in general, this is not gonna be- so it's not going to be equal to your new policy distribution.",
    "start": "650760",
    "end": "660505"
  },
  {
    "text": "The only time you're gonna get the same state distribution under two policies is generally if they're identical.",
    "start": "660505",
    "end": "667885"
  },
  {
    "text": "Occasionally, you can get the same state distribution under two different policies, but then that means they have the same value.",
    "start": "667885",
    "end": "674230"
  },
  {
    "text": "So in general, we're going to expect that this is going to be different but we're going to ignore that for now. We're just going to say this is an objective function,",
    "start": "674230",
    "end": "680200"
  },
  {
    "text": "this is something we can optimize. And a nice thing about this is that we have samples from the current policy.",
    "start": "680200",
    "end": "686320"
  },
  {
    "text": "So we can imagine just using those samples to estimate this expectation.",
    "start": "686320",
    "end": "691790"
  },
  {
    "text": "The thing that I also want us to note here is that, this is just this new objective function called",
    "start": "691860",
    "end": "697360"
  },
  {
    "text": "L. If you evaluate the objective function L, um, at the current policy,",
    "start": "697360",
    "end": "703420"
  },
  {
    "text": "so if you plug in your old policy into your objective function, it's exactly equal to the value of your current policy.",
    "start": "703420",
    "end": "711600"
  },
  {
    "text": "So this second term becomes 0, [NOISE] because the advantage of the existing policy over the existing policy is 0.",
    "start": "711600",
    "end": "722210"
  },
  {
    "text": "So this objective function is exactly equal to the value of the old policy,",
    "start": "722210",
    "end": "727230"
  },
  {
    "text": "if you evaluated the old policy, and another case is for new policies it's gonna be something different. Yes.",
    "start": "727230",
    "end": "733775"
  },
  {
    "text": "How's this similar to importance sampling? That's a great question. You asked, \"How is this similar to importance sampling?\"",
    "start": "733775",
    "end": "741070"
  },
  {
    "text": "Um, if we were gonna do importance- Well, it's different in a number of ways. Um, in importance sampling,",
    "start": "741070",
    "end": "747270"
  },
  {
    "text": "what we tend to do is we re-weight, um, uh, the distribution that we want by the distribution that we have.",
    "start": "747270",
    "end": "754035"
  },
  {
    "text": "Um, in this case we're looking, and we normally do that on a per state level. In this case we are looking at the stationary distribution over states.",
    "start": "754035",
    "end": "763095"
  },
  {
    "text": "It's actually a really cool paper that just came out in NeurIPS, like a month ago, um,",
    "start": "763095",
    "end": "768480"
  },
  {
    "text": "[NOISE] 2018, with Lihong Li and some other colleagues.",
    "start": "768480",
    "end": "774654"
  },
  {
    "text": "Um, they looked at, how would you re-weight stationary distributions to try to get off policy estimates of the value function?",
    "start": "774655",
    "end": "782665"
  },
  {
    "text": "Um, and so to try to directly re-weight what, like, mu pi would be versus mu pi tilde.",
    "start": "782665",
    "end": "788455"
  },
  {
    "text": "So we're not doing that here, there's some really nice ideas in that that could help really reduce the variance in long horizon problems.",
    "start": "788455",
    "end": "795115"
  },
  {
    "text": "Um, in this case, we're just substituting, so we're ignoring the difference. We're not doing importance sampling, we're just pretending that the distribution of states that we get to is exactly the same.",
    "start": "795115",
    "end": "804295"
  },
  {
    "text": "It's not, but we're gonna show that this is going to end up being a useful lower bound to what we wanna- what we actually want to optimize.",
    "start": "804295",
    "end": "812300"
  },
  {
    "text": "Okay. So you might say, if you take this objective function which might be good, might not be good.",
    "start": "813810",
    "end": "820450"
  },
  {
    "text": "If we optimize with respect to it, do we have any guarantees on whether the new value function that we get if we optimize with",
    "start": "820450",
    "end": "826570"
  },
  {
    "text": "respect to this wrong objective function is better than the old value function? Because remember that's where we're trying to go to.",
    "start": "826570",
    "end": "832315"
  },
  {
    "text": "We don't really care what we're optimizing, what we care about is that the resulting value function we get out is actually better than the old value function.",
    "start": "832315",
    "end": "839290"
  },
  {
    "text": "[NOISE] So last time I said that if you have a mixture policy which blend between your current policy and a new policy,",
    "start": "839290",
    "end": "846685"
  },
  {
    "text": "so let's say you have a pi old and you have some other policy, I haven't said how you get it,",
    "start": "846685",
    "end": "852760"
  },
  {
    "text": "but just say you have some other policy, and that defines your new policy. So with probability 1 - alpha,",
    "start": "852760",
    "end": "857875"
  },
  {
    "text": "you take the same action as you used to. With probability alpha, you take a new action. In this case you can guarantee a lower bound on the value of the new policy.",
    "start": "857875",
    "end": "867535"
  },
  {
    "text": "So the value of the new policy is greater than or equal to this objective function we have here,",
    "start": "867535",
    "end": "873399"
  },
  {
    "text": "minus this particular quantity. So that says that if you optimize with respect to this weird L objective function,",
    "start": "873400",
    "end": "883600"
  },
  {
    "text": "you can actually get bounds on how good your new policy is. So that seems promising,",
    "start": "883600",
    "end": "888685"
  },
  {
    "text": "but in general we're not going to want to just consider mixture policies. Okay. So what this theorem says is that,",
    "start": "888685",
    "end": "896440"
  },
  {
    "text": "for any stochastic policy, not just this weird mixture, you can get a bound on the performance by using this slightly strange objective function.",
    "start": "896440",
    "end": "905880"
  },
  {
    "text": "So in particular, um, define the distance of total variation as follows. So DTV between two policies,",
    "start": "905880",
    "end": "914000"
  },
  {
    "text": "so I'm using a dot there to denote that there's, um, there's a number of different actions.",
    "start": "914000",
    "end": "920930"
  },
  {
    "text": "The- the policies there are denoting a probability distribution over actions. This is equal to the max over all a,",
    "start": "920930",
    "end": "928310"
  },
  {
    "text": "the distance between the probability that each of the two policies put on that action.",
    "start": "928310",
    "end": "935970"
  },
  {
    "text": "So it's giving us sort of a maximum difference, in what's the probability of an action under one policy versus the other policy.",
    "start": "936900",
    "end": "944380"
  },
  {
    "text": "And then we can do- [NOISE] Bless you. -we can do D max of total variation by taking the max of that quantity over all states.",
    "start": "944380",
    "end": "952720"
  },
  {
    "text": "So it's essentially saying, over all states was the biggest difference that the two policies give over a particular action.",
    "start": "952720",
    "end": "959350"
  },
  {
    "text": "So where do they most differ? And then what this theorem says is that, if you have that quantity,",
    "start": "959350",
    "end": "965439"
  },
  {
    "text": "in general we're not gonna be able to evaluate that. But what that's saying is that, if you know what that quantity is,",
    "start": "965439",
    "end": "971095"
  },
  {
    "text": "then you can define that. If you use this objective function L, that the new value of your policy is",
    "start": "971095",
    "end": "978855"
  },
  {
    "text": "at least the objective function you compute minus this quantity, it's a function of the distance of total variation,",
    "start": "978855",
    "end": "985240"
  },
  {
    "text": "the max distance and total variation. So this gives us some confidence that if we were to",
    "start": "985240",
    "end": "990800"
  },
  {
    "text": "optimize with respect to the objective function L, then we can get a bound on the value function.",
    "start": "990800",
    "end": "996899"
  },
  {
    "text": "Now this distance- this max or the total variation distance isn't particularly easy to work with.",
    "start": "997060",
    "end": "1003084"
  },
  {
    "text": "So we can use the fact that the square of it is upper bounded by the KL divergence,",
    "start": "1003085",
    "end": "1008559"
  },
  {
    "text": "and then get a new bound which is a little bit easier to work with. That looks at the KL divergence between the two policies,",
    "start": "1008559",
    "end": "1015790"
  },
  {
    "text": "and we again get the similar bound. Okay. So why is this useful?",
    "start": "1015790",
    "end": "1021930"
  },
  {
    "text": "So what I've told you right now is that we have this new objective function. If we use this new objective function, we could- in principle get this lower bound on the performance of the new policy.",
    "start": "1021930",
    "end": "1031029"
  },
  {
    "text": "So how do we use this to ensure that we wanna get monotonic improvement? So the goal is monotonic improvement.",
    "start": "1031030",
    "end": "1039790"
  },
  {
    "start": "1036000",
    "end": "1473000"
  },
  {
    "text": "We want to have the V_ Pi i + 1 is greater than or equal to V_ Pi i. That is our goal.",
    "start": "1039790",
    "end": "1047355"
  },
  {
    "text": "So i is iterations, we want that the new policy that we deploy is actually better than the policy we had before. So how are we gonna do this?",
    "start": "1047355",
    "end": "1053770"
  },
  {
    "text": "So what we're gonna say is, first we have this objective function here, this lower bound objective function,",
    "start": "1053770",
    "end": "1059020"
  },
  {
    "text": "[NOISE] and what we're going to define is that Mi of pi i",
    "start": "1059020",
    "end": "1064705"
  },
  {
    "text": "is equal to L of pi i pi.",
    "start": "1064705",
    "end": "1071559"
  },
  {
    "text": "So I'm just copying the equation from the previous, um, previous slide,",
    "start": "1071560",
    "end": "1077590"
  },
  {
    "text": "- 4 epsilon gamma divided by 1 - gamma squared DKL_ max of pi i.",
    "start": "1077590",
    "end": "1089570"
  },
  {
    "text": "Okay. So this is the lower bound. That's what we just defined on the previous slide. Okay? So what we said here is that,",
    "start": "1089900",
    "end": "1098180"
  },
  {
    "text": "the value of our new policy- so this is equal to this M function I've defined.",
    "start": "1098180",
    "end": "1103340"
  },
  {
    "text": "So we've said that the new value is gonna be at least as good as this lower bound.",
    "start": "1103340",
    "end": "1108480"
  },
  {
    "text": "So we're gonna say V_i + 1 is gonna be equal to Mi of pi i + 1,",
    "start": "1108480",
    "end": "1118140"
  },
  {
    "text": "which is equal to L pi i + 1. I'm just writing out what the definition is here.",
    "start": "1118140",
    "end": "1126250"
  },
  {
    "text": "And again, what we're trying to do here is get to the point where we're confident that we can get something that's better than our old value function.",
    "start": "1129880",
    "end": "1137705"
  },
  {
    "text": "Now, the thing that I want to now look at is, well, what is- if we were to evaluate the lower bound at the current policy what would that be?",
    "start": "1137705",
    "end": "1145700"
  },
  {
    "text": "So let's look at Mi of pi_i. So that's going to be equal to L pi_i of pi_i.",
    "start": "1145700",
    "end": "1152450"
  },
  {
    "text": "I'm just plugging it into this equation up there, - 4 epsilon gamma - gamma squared DKL max of pi_i, pi_i.",
    "start": "1152450",
    "end": "1164240"
  },
  {
    "text": "Okay, so why is this nice? Well, this is nice because",
    "start": "1164240",
    "end": "1169580"
  },
  {
    "text": "the KL diver- divergence between two identical policies is 0. So these are exactly the same.",
    "start": "1169580",
    "end": "1175415"
  },
  {
    "text": "This is equal to 0. So now, this is just equal to L pi_i of pi_i.",
    "start": "1175415",
    "end": "1181669"
  },
  {
    "text": "But what I told you before is that if we go back a few slides to what the definition is of pi_i,",
    "start": "1181670",
    "end": "1188375"
  },
  {
    "text": "L of pi_i is that if you evaluate it at the current policy it's just equal to the value of that policy, okay?",
    "start": "1188375",
    "end": "1199325"
  },
  {
    "text": "So if we evaluate this objective function at the current policy, it's just the same as the value of the current policy.",
    "start": "1199325",
    "end": "1206015"
  },
  {
    "text": "So now, if we go back here this is just equal to V of pi_i.",
    "start": "1206015",
    "end": "1211320"
  },
  {
    "text": "Okay. So what does this say? This says, that if I wanna look at how my I- my-",
    "start": "1211690",
    "end": "1218045"
  },
  {
    "text": "the value of my i + 1 policy looks compared to the value of my old policy,",
    "start": "1218045",
    "end": "1223264"
  },
  {
    "text": "we know that's greater than or equal to Mi of pi_i + 1.",
    "start": "1223264",
    "end": "1228690"
  },
  {
    "text": "So because we said that the V that we knew from this theorem, that the new value of the policy is greater",
    "start": "1228910",
    "end": "1235520"
  },
  {
    "text": "than or equal to this lower bound we computed. So it's greater than or equal to Mi of pi i + 1 - Mi of pi_i.",
    "start": "1235520",
    "end": "1243270"
  },
  {
    "text": "So what does this say? This says that if your new value function has a better lower bound than your old value function,",
    "start": "1244840",
    "end": "1252020"
  },
  {
    "text": "you have monotonic improvement. So if this is greater than 0, then monotonic improvement,",
    "start": "1252020",
    "end": "1259680"
  },
  {
    "text": "which means that if you optimize with respect to this lower bound and",
    "start": "1261430",
    "end": "1266750"
  },
  {
    "text": "you can evaluate that quantity and your new lower bound is higher than your old lower bound, then your value has to be better.",
    "start": "1266750",
    "end": "1274725"
  },
  {
    "text": "So we can guarantee monotonic improvement. Yes. So just to clarify. So for these value comparisons,",
    "start": "1274725",
    "end": "1281845"
  },
  {
    "text": "are we implicitly considering as an infinity norm, in terms of saying one is better? Yes, generally. Yeah, I think, I mean,",
    "start": "1281845",
    "end": "1288950"
  },
  {
    "text": "they probably go through with L squared 2. But yeah, yeah. Um, question is whether or not we're always defining",
    "start": "1288950",
    "end": "1295010"
  },
  {
    "text": "this with respect to L infinity norm almost always. Um, ah, there certainly is some analysis particularly when we",
    "start": "1295010",
    "end": "1300290"
  },
  {
    "text": "get into a function approximation which looks at an L2 norm. Um, but most of this is all with respect to an L infinity norm,",
    "start": "1300290",
    "end": "1306529"
  },
  {
    "text": "which means that when we're looking at this, for example, we're looking at, um, ensuring that for all states, um,",
    "start": "1306530",
    "end": "1312965"
  },
  {
    "text": "the value of those states is at least as good as the previous value of the states. Yes.",
    "start": "1312965",
    "end": "1319325"
  },
  {
    "text": "So the- the claim was if our lower bound improves, then it must be the case that what is the lower bound must also be improving, right?",
    "start": "1319325",
    "end": "1326690"
  },
  {
    "text": "Yeah. Uh, so there's never a case where, for example, your lower bound might improve even though the actual value of the policy",
    "start": "1326690",
    "end": "1332000"
  },
  {
    "text": "evaluated decreases, it seems like. That's right. So what this- this is- what this",
    "start": "1332000",
    "end": "1338765"
  },
  {
    "text": "is asking us if you know this lower bound and what that relates to the actual value. What this is stating is that if you improve",
    "start": "1338765",
    "end": "1344780"
  },
  {
    "text": "your two low- like if you have a lower bound of your existing policy and you get a new lower bound with for some new policy and that new lower bound is",
    "start": "1344780",
    "end": "1351320"
  },
  {
    "text": "higher than your lower bound of the other one, that you're guaranteed to be improving. So this is what guarantee- so this is assuming you can solve this.",
    "start": "1351320",
    "end": "1359900"
  },
  {
    "text": "But if you, um, if you can get this lower- if you optimize with respect to this lower-bound quantity,",
    "start": "1359900",
    "end": "1366845"
  },
  {
    "text": "um, because when you plug in the lower bound for the current policy under that, that's exactly equal to th- the value of",
    "start": "1366845",
    "end": "1373460"
  },
  {
    "text": "that policy then you are guaranteed to be improving. Because you're basically saying, here's my lower- here's my existing value.",
    "start": "1373460",
    "end": "1379775"
  },
  {
    "text": "I have something whose lower bound is better than my existing value. And so I know my new thing has to be better. Okay. Yeah.",
    "start": "1379775",
    "end": "1387545"
  },
  {
    "text": "How do you like, um, you get the epsilon term back because it seems like the epsilon changes depending on your pi and it's also like a global property.",
    "start": "1387545",
    "end": "1396924"
  },
  {
    "text": "Absolutely. So, um, now this discussion is a great one. I- few might ask us about this. Um, ah, so note this,",
    "start": "1396925",
    "end": "1402950"
  },
  {
    "text": "that your lower bound is in terms of epsilon. Epsilon is a max over all states and actions of your advantage.",
    "start": "1402950",
    "end": "1408785"
  },
  {
    "text": "Um, in principle, you could evaluate this [LAUGHTER] particularly if you're at a discrete state and action space.",
    "start": "1408785",
    "end": "1415549"
  },
  {
    "text": "I- in practice, that's something that you would not wanna do. Um, this, I view this part as sort of",
    "start": "1415550",
    "end": "1421715"
  },
  {
    "text": "saying this is formally if you could evaluate this lower bound. Um, and what we're gonna do now is talk about, um, a more practical algorithm which tries to take,",
    "start": "1421715",
    "end": "1429605"
  },
  {
    "text": "um, this guarantee of conservative policy improvement and actually make it practical, in terms of quantities that are a little bit easier to compute.",
    "start": "1429605",
    "end": "1437405"
  },
  {
    "text": "Because that's right. Yeah, in general, it- it would be very hard to evaluate this this epsi- uh, this epsilon. Now you could take upper or lower bounds on it.",
    "start": "1437405",
    "end": "1444065"
  },
  {
    "text": "Um, but you won't generally know what this epsilon is. And note that this, uh, as co- just pointing out,",
    "start": "1444065",
    "end": "1449360"
  },
  {
    "text": "this epsilon is dependent on the policy. Um, so- okay. All right. But this is pretty cool.",
    "start": "1449360",
    "end": "1456800"
  },
  {
    "text": "So it means that you can do this guaranteed improvement. This is a form of mineral- minimization maximization. Um, and it's this nice idea of sort of saying you can have this new",
    "start": "1456800",
    "end": "1464480"
  },
  {
    "text": "lower bound that's guaranteed to be better than the value of your current policy. So you can get this sort of conservative monotonic improving policy.",
    "start": "1464480",
    "end": "1471750"
  },
  {
    "text": "All right. So I just- I wanna make sure we have enough time to go through some of the midterm review.",
    "start": "1472090",
    "end": "1477920"
  },
  {
    "start": "1473000",
    "end": "1778000"
  },
  {
    "text": "But I wanna briefly talk about how we would make this practical. Particularly because, um, trust",
    "start": "1477920",
    "end": "1483410"
  },
  {
    "text": "region policy optimization is an extremely popular policy gradient algorithm. So I think it's useful for you guys just to be aware of.",
    "start": "1483410",
    "end": "1490130"
  },
  {
    "text": "Um, you- some of you might use it in some of your projects. Um, we won't cover it in uh- won't be a mandatory part of the homework,",
    "start": "1490130",
    "end": "1496850"
  },
  {
    "text": "um, or on the midterm. But I think it's just a useful idea to be familiar with. So again, if we look at sort of what this objective function was that we just discussed.",
    "start": "1496850",
    "end": "1505700"
  },
  {
    "text": "We said we had this L function and then we turned it into a lower bound by subtracting off this constant that might be hard for us to compute.",
    "start": "1505700",
    "end": "1513470"
  },
  {
    "text": "And so what we do in this case is we take this constant here and we turned it into a hyperparameter.",
    "start": "1513470",
    "end": "1519710"
  },
  {
    "text": "So you could turn it into a constant C. Um, but the problem wi- always is that even if you could compute this,",
    "start": "1519710",
    "end": "1526745"
  },
  {
    "text": "um, often we don't know what this is. But even if you could compute it or compute a bound on it, um, generally, if we use this,",
    "start": "1526745",
    "end": "1533135"
  },
  {
    "text": "we would take very small step sizes. So intuitively, this is because, um, you know,",
    "start": "1533135",
    "end": "1539495"
  },
  {
    "text": "it's often very hard to extrapolate far away, um, from your current policy. And so this would say if you wanna be really sure that",
    "start": "1539495",
    "end": "1546530"
  },
  {
    "text": "your new value is better than your old value, then just take a very small step size. And intuitively, it's because if you change your policy very,",
    "start": "1546530",
    "end": "1554450"
  },
  {
    "text": "very small amounts at least under some smoothness guarantees, um, the value of your policy can't change that much.",
    "start": "1554450",
    "end": "1560165"
  },
  {
    "text": "You know, it should also be intuitive that, like, your gradient is often a pretty good estimate very close to your current value of your function.",
    "start": "1560165",
    "end": "1567320"
  },
  {
    "text": "But we also need to quickly try to get to a good policy so this is not generally practical.",
    "start": "1567320",
    "end": "1572390"
  },
  {
    "text": "Um, and so the idea of sort of TRPO, one of the main ideas is to think of it being kind of a trusted region.",
    "start": "1572390",
    "end": "1579080"
  },
  {
    "text": "Um, and- and use this to constrain our step sizes. So again, if we go back to this sort of the gene- generic, um, ah,",
    "start": "1579080",
    "end": "1585995"
  },
  {
    "text": "template for policy gradient algorithms, we have to make this choice of how far out to step in our gradient.",
    "start": "1585995",
    "end": "1591455"
  },
  {
    "text": "Um, and the idea is we're going to sort of define a constraint. So we're going to have our objective function here.",
    "start": "1591455",
    "end": "1597170"
  },
  {
    "text": "And instead of explicitly subtracting off our lower bound, we're just going to say you could move.",
    "start": "1597170",
    "end": "1602345"
  },
  {
    "text": "You can change your gradient but not too far. We're gonna put, uh, a constraint on how far the KL divergence can be as a way to just sort of say",
    "start": "1602345",
    "end": "1610025"
  },
  {
    "text": "you're kind of having this region of which in your- in your parameter space that allows you to know how far you can change your policy.",
    "start": "1610025",
    "end": "1616595"
  },
  {
    "text": "Okay. Yeah. All right. So I'm just going to talk very briefly about how this is instantiated.",
    "start": "1616595",
    "end": "1623870"
  },
  {
    "text": "Um, so the main idea is that if we look at, um, what these objective functions are,",
    "start": "1623870",
    "end": "1630020"
  },
  {
    "text": "um, this may or may not be easy for us to evaluate. So if we look back at what,",
    "start": "1630020",
    "end": "1635090"
  },
  {
    "text": "um, our theta is, um, even here, right, we have sort of our discounted visitation weights under the current policy,",
    "start": "1635090",
    "end": "1642065"
  },
  {
    "text": "but we don't have direct access to that. We have only access to samples from rolling out our current policy.",
    "start": "1642065",
    "end": "1647409"
  },
  {
    "text": "So the first idea is that instead of taking an explicit sum over the state space, where that state space might be, you know,",
    "start": "1647410",
    "end": "1653485"
  },
  {
    "text": "continuous and in-infinite, we're just going to look at the states that were actually sampled by our current old policy and re-weight them.",
    "start": "1653485",
    "end": "1661385"
  },
  {
    "text": "So that's the first depa- the first substitution we do. Yeah. What we're trying to do right now is say we have",
    "start": "1661385",
    "end": "1666830"
  },
  {
    "text": "this objective function and we wanna make it so that this can be part of an algorithm where we can compute all the quantities we need to in order",
    "start": "1666830",
    "end": "1672890"
  },
  {
    "text": "to take a step size where we think the new policy is gonna be better. Um, the second thing we do and this relates to,",
    "start": "1672890",
    "end": "1680435"
  },
  {
    "text": "um, question about importance sampling. Um, I- is we have this second quantity in here,",
    "start": "1680435",
    "end": "1686390"
  },
  {
    "text": "where this is the probability of an action under our new policy. Um, we do have access to that, in the sense that,",
    "start": "1686390",
    "end": "1694370"
  },
  {
    "text": "if someone gives us a state we can tell, um, we can say exactly what our probability would be under all the actions.",
    "start": "1694370",
    "end": "1699770"
  },
  {
    "text": "But again, this often can be a continuous set.",
    "start": "1699770",
    "end": "1704490"
  },
  {
    "text": "And so instead of doing sort of this continuous set, we are just going to say we're gonna use importance sampling and we can take samples.",
    "start": "1707210",
    "end": "1715590"
  },
  {
    "text": "This is typically goi- going to be from pi old. So we look at what times we have taken an action given our current policy and we",
    "start": "1715590",
    "end": "1721890"
  },
  {
    "text": "re-weight them according to the probability we would have taken those actions that drive the new policy. So it allows us to approximate that expectation using data that we have.",
    "start": "1721890",
    "end": "1731320"
  },
  {
    "text": "And then the third substitution is switching the advantage back to the Q function, and it's just important to note that all of these three substitutions don't",
    "start": "1732320",
    "end": "1740400"
  },
  {
    "text": "change the solution to the object- to the optimization problem. These are all sort of taking at, uh,",
    "start": "1740400",
    "end": "1747045"
  },
  {
    "text": "these different substitutions or different ways to evaluate these quantities, okay?",
    "start": "1747045",
    "end": "1754125"
  },
  {
    "text": "So we end up with the following: um, uh, we have this objective function that we are optimizing.",
    "start": "1754125",
    "end": "1759254"
  },
  {
    "text": "This is after we've done the substitutions I just mentioned, and we have this constraint on how far away we can be.",
    "start": "1759255",
    "end": "1764700"
  },
  {
    "text": "Um, and empirically, they generally just sample, um, this sort of alternative sampling distribution Q is just your existing old policy.",
    "start": "1764700",
    "end": "1772950"
  },
  {
    "text": "So there's a bunch of other stuff in the paper. It's a really nice paper. Um, a lot of really interesting ideas.",
    "start": "1772950",
    "end": "1778289"
  },
  {
    "start": "1778000",
    "end": "2557000"
  },
  {
    "text": "Uh, I will just, I will skip through sort of exactly how they do some of the additional details.",
    "start": "1778290",
    "end": "1784050"
  },
  {
    "text": "There's some nice complexity there. Um, but I will just say briefly the main thing they're doing here is they're sort of running a policy.",
    "start": "1784050",
    "end": "1790515"
  },
  {
    "text": "They're computing this gradient. They have to, um, consider these constraints, um, and they do this sort of line search with a KL constraint.",
    "start": "1790515",
    "end": "1798419"
  },
  {
    "text": "And perhaps the most important thing is just to be aware of this and just sort of understand kind of them being inspired by this conservative policy improvement,",
    "start": "1798420",
    "end": "1806085"
  },
  {
    "text": "and then trying to make that more practical and fast. Um, they've applied it",
    "start": "1806085",
    "end": "1811380"
  },
  {
    "text": "to a lot of different problems. Um, there's some really nice stuff on locomotion controllers, cases where you have continuous action spaces, continuous state spaces.",
    "start": "1811380",
    "end": "1819210"
  },
  {
    "text": "These are cases where policy gradient is often very helpful, uh, and they have some very nice results.",
    "start": "1819210",
    "end": "1824985"
  },
  {
    "text": "Um, I won't step through this here. Um, the main thing to know is that empirically this is a really good tool to know about.",
    "start": "1824985",
    "end": "1832005"
  },
  {
    "text": "Often, if you're doing policy gradient-style approaches, TRPO can be a very useful thing to build on, um, and it's been incredibly influential.",
    "start": "1832005",
    "end": "1838575"
  },
  {
    "text": "This was came out in ICML in 2015. There's hundreds of citations to it already. So this has sort of become one of the main benchmarks for policy gradient.",
    "start": "1838575",
    "end": "1847809"
  },
  {
    "text": "Okay. So if we go back just to kinda what the, to summarize what the policy gradient algorithm template is,",
    "start": "1848240",
    "end": "1854235"
  },
  {
    "text": "whether you're looking at the existing algorithms or whether you're trying to define your own, generally, they look like something like the following.",
    "start": "1854235",
    "end": "1860130"
  },
  {
    "text": "For each iteration, you run your policy out and you gather trajectories of data by running that policy.",
    "start": "1860130",
    "end": "1865575"
  },
  {
    "text": "You compute some target that might be just the rewards, that might be a Q function.",
    "start": "1865575",
    "end": "1870585"
  },
  {
    "text": "We can trade off between bias and variance in that, um, and then we use that to estimate the policy gradient,",
    "start": "1870585",
    "end": "1876600"
  },
  {
    "text": "and then we may want to smartly take a step along that gradient to try to ensure monotonic improvement.",
    "start": "1876600",
    "end": "1882820"
  },
  {
    "text": "Um, the things to be aware of and some of the things you're going to have practice",
    "start": "1883220",
    "end": "1888330"
  },
  {
    "text": "on soon i- is that you should be very familiar with these sort of vanilla approaches and REINFORCE, um, and this general template and sort of understand how some of",
    "start": "1888330",
    "end": "1895680"
  },
  {
    "text": "the different algorithms we're talking about might instantiate these different things. Um, you don't have to derive and remember",
    "start": "1895680",
    "end": "1901830"
  },
  {
    "text": "all the formula that I just went through quickly for TRPO,um, and you will have the opportunity to practice these more in homework 3,",
    "start": "1901830",
    "end": "1909105"
  },
  {
    "text": "but we'll only cover these lightly in terms of the midterm. All right. So is somebody may have any questions about this before we go into",
    "start": "1909105",
    "end": "1916155"
  },
  {
    "text": "sort of a short overview of the stuff we have done so far for- before the mid-term? [inaudible]",
    "start": "1916155",
    "end": "1921539"
  },
  {
    "text": "Okay. All right. Let's switch over.",
    "start": "1921540",
    "end": "1925570"
  },
  {
    "text": "Okay. So what this is going be is sort of a very short recap of what we have,",
    "start": "1927110",
    "end": "1933570"
  },
  {
    "text": "uh, done so far. And in terms of why this is useful, um, there's certainly a lot of good evidence from learning",
    "start": "1933570",
    "end": "1939300"
  },
  {
    "text": "sciences that space repetition of ideas is really helpful, as is forced recall, which is one of the other benefits of doing exams.",
    "start": "1939300",
    "end": "1946245"
  },
  {
    "text": "Um, uh, so, uh, that's what we are going to do today is just sort of do a quick recap of a lot of the different main ideas.",
    "start": "1946245",
    "end": "1952320"
  },
  {
    "text": "So again, uh, reinforcement learning generally involves optimization, delayed consequences, generalization, and exploration.",
    "start": "1952320",
    "end": "1959385"
  },
  {
    "text": "We haven't really talked about that yet. Um, so that's not really going to be on the midterm.",
    "start": "1959385",
    "end": "1964620"
  },
  {
    "text": "Um, we are going to start talking a lot more about that post the mid-term. It's an incredibly important topic, one I think is super fascinating and one of the main reasons why RL is interesting.",
    "start": "1964620",
    "end": "1973470"
  },
  {
    "text": "Um, but these other things are really important too and we spent some time on those so far. So in terms of thinking about the mid-term and indeed thinking about the class,",
    "start": "1973470",
    "end": "1982590"
  },
  {
    "text": "um, on the very first day, I put up this sort of blizzard of learning objectives, um, and I just want to highlight, uh,",
    "start": "1982590",
    "end": "1989610"
  },
  {
    "text": "a few of these which are the things that I mentioned were going to explicitly evaluated in the exam,",
    "start": "1989610",
    "end": "1995100"
  },
  {
    "text": "which is that, um, by the end of the class including on the exam, um, you should be very familiar with sort of what are the key features of reinforcement ment-",
    "start": "1995100",
    "end": "2002090"
  },
  {
    "text": "learning that make it different than other machine learning problems, and other AI problems. So we spent some time on that on the first day,",
    "start": "2002090",
    "end": "2008540"
  },
  {
    "text": "um, and I've sort of tried to talk about it throughout. But the fact that the agent is collecting its own data and that the data,",
    "start": "2008540",
    "end": "2014930"
  },
  {
    "text": "um, it gathers influences the policies it can learn. So we sort of have this censored data issue.",
    "start": "2014930",
    "end": "2021185"
  },
  {
    "text": "The agent can't know about other lives that didn't li- didn't live, it makes a very big difference compared to supervised learning.",
    "start": "2021185",
    "end": "2028309"
  },
  {
    "text": "Um, a second really important thing is that if you are given an application problem, um,",
    "start": "2028310",
    "end": "2033485"
  },
  {
    "text": "it's important to try to know why or why not to formula- formulate it as a reinforcement learning problem.",
    "start": "2033485",
    "end": "2039290"
  },
  {
    "text": "Um, and if so, how you would. Generally, there's not a single answer to this. So it's good to think of like what is one or more way to define the state-space,",
    "start": "2039290",
    "end": "2047210"
  },
  {
    "text": "the action space, the dynamics, and the reward model, um, and what algorithm you would suggest from class to try to tackle it.",
    "start": "2047210",
    "end": "2055099"
  },
  {
    "text": "This is in general sort of, uh, something that you'll probably run into much more than like looking at any particular algorithm,",
    "start": "2055100",
    "end": "2061580"
  },
  {
    "text": "um, particularly in industry. And then a third thing that I think is really important is to understand how we decide whether or not an RL algorithm is good.",
    "start": "2061580",
    "end": "2068615"
  },
  {
    "text": "And so what is the criteria for performance and evaluation we can use to sort of evaluate,",
    "start": "2068615",
    "end": "2073760"
  },
  {
    "text": "um, what are the benefits, strengths and weaknesses of different algorithms and how they compare. So this could be things like bias and variance.",
    "start": "2073760",
    "end": "2080629"
  },
  {
    "text": "It also could be computational complexity, um, or sample efficiency or other aspects.",
    "start": "2080630",
    "end": "2086190"
  },
  {
    "text": "So what we have covered so far is planning where we know how the world works, um,",
    "start": "2087130",
    "end": "2092704"
  },
  {
    "text": "policy evaluation, um, model free learning how to make good decisions, value function approximation, and then imitation learning and policy search.",
    "start": "2092705",
    "end": "2101660"
  },
  {
    "text": "And we've have also talked about the fact that for reinforcement learning in general, you can think of either trying to find a value function of policy or a model,",
    "start": "2101660",
    "end": "2110150"
  },
  {
    "text": "and that model is sufficient to generate a value function which is sufficient to generate a policy,",
    "start": "2110150",
    "end": "2115850"
  },
  {
    "text": "um, but they are not all necessary that you don't have to have a model in order to get a policy.",
    "start": "2115850",
    "end": "2121590"
  },
  {
    "text": "So, um, I will go through this part pretty fast and so I think a lot of you guys have also seen some of this stuff,",
    "start": "2123340",
    "end": "2130340"
  },
  {
    "text": "um, in previous classes. So we're- almost everything we have been talking about so far assumes the world is a Markov decision process.",
    "start": "2130340",
    "end": "2136520"
  },
  {
    "text": "But I have mentioned that often the world is not a Markov decision process. Um, and in the MDP case,",
    "start": "2136520",
    "end": "2141980"
  },
  {
    "text": "we assume that the, the state is sufficient. Um, a sufficient statistic of all the prior history.",
    "start": "2141980",
    "end": "2147275"
  },
  {
    "text": "So we don't have to keep track of the full set of states and observations and actions rewards from the whole time period,",
    "start": "2147275",
    "end": "2153964"
  },
  {
    "text": "but we can just look at the current observation in order to make good decisions in the world.",
    "start": "2153965",
    "end": "2158579"
  },
  {
    "text": "Um, in terms of this, i- it's very useful to know what the Markov property is, why it's important,",
    "start": "2159370",
    "end": "2165335"
  },
  {
    "text": "why it might be violated, what are things like models, value, functions, and queues, um,",
    "start": "2165335",
    "end": "2170690"
  },
  {
    "text": "and what is planning, and what is the difference. So in planning, we assume that you are given a model of how the world works,",
    "start": "2170690",
    "end": "2176750"
  },
  {
    "text": "you know the dynamics model, you know the reward model, it still can be really hard to figure out how to act. This is like knowing the game of Go, and it's still really,",
    "start": "2176750",
    "end": "2184369"
  },
  {
    "text": "really computationally intensive and tricky to try to figure out what's the optimal decision to take in Go even though you",
    "start": "2184370",
    "end": "2190040"
  },
  {
    "text": "know all the dynamics and all of the rewards. In learning, we don't know the dynamics and rewards and we still",
    "start": "2190040",
    "end": "2196400"
  },
  {
    "text": "have to gather data in order to learn a good policy, which has a high value, a high discounted expected sum of rewards.",
    "start": "2196400",
    "end": "2202745"
  },
  {
    "text": "We talked about the Bellman backup operator, which is a contraction. If your discount factor is less than 1, um,",
    "start": "2202745",
    "end": "2208430"
  },
  {
    "text": "which means that with repeated applications you are guaranteed to converge to a single fixed point.",
    "start": "2208430",
    "end": "2213480"
  },
  {
    "text": "We talked about value versus policy iteration. In value iteration on the iteration k,",
    "start": "2213880",
    "end": "2220415"
  },
  {
    "text": "you are always computing the optimal value is if you only get to make k decisions, um, and then you use that to back up aga- and get the k + 1 policy.",
    "start": "2220415",
    "end": "2230810"
  },
  {
    "text": "In policy iteration, you always have a policy and the value of that policy if you were to act using it forever.",
    "start": "2230810",
    "end": "2237290"
  },
  {
    "text": "Um, but it might not be a very good policy and then you update this.",
    "start": "2237290",
    "end": "2242510"
  },
  {
    "text": "And as we have seen, it's closely related to sort of policy gradient-style algorithms where",
    "start": "2242510",
    "end": "2247670"
  },
  {
    "text": "you sort of try to estimate the gradient of a policy. So in policy iteration generally,",
    "start": "2247670",
    "end": "2254359"
  },
  {
    "text": "and similar to what we have been seeing in policy gradient approaches, we intermix evaluation and improvement. So we compute the value of a policy and then we",
    "start": "2254360",
    "end": "2260930"
  },
  {
    "text": "use that in order to take a step and improve it. Um, if we are in the case of being model-free, um,",
    "start": "2260930",
    "end": "2267260"
  },
  {
    "text": "and not having extra model, we often want to compute Q-values instead so that we can directly improve the policy.",
    "start": "2267260",
    "end": "2274440"
  },
  {
    "text": "So let's just take a quick second. Um, so these are check your understandings, they're good things to go back through.",
    "start": "2275740",
    "end": "2281360"
  },
  {
    "text": "These are all sort of like, you know, um, sm- small conceptual questions of the type that we might ask you on the exam.",
    "start": "2281360",
    "end": "2287225"
  },
  {
    "text": "So let's just take a minute, um, to check our understanding and think about for a finite state and action MDP, the lookup table representation,",
    "start": "2287225",
    "end": "2293870"
  },
  {
    "text": "which means that we just have a table entry for each state and action. Um, gamma less than 1, uh,",
    "start": "2293870",
    "end": "2299900"
  },
  {
    "text": "does the initial setting of the value function impact the final computed values? Why or why not? Does value iteration and policy iteration always yield the same solution?",
    "start": "2299900",
    "end": "2308045"
  },
  {
    "text": "And, um, is the number of iterations needed for poli- policy iteration in a finite state and action MDP bounded?",
    "start": "2308045",
    "end": "2314990"
  },
  {
    "text": "And if, so how many? Let's just take a minute and think about those.",
    "start": "2314990",
    "end": "2320180"
  },
  {
    "text": "Feel free to talk to somebody next to you. [BACKGROUND].",
    "start": "2320180",
    "end": "2374920"
  },
  {
    "text": "All right. We're-",
    "start": "2374920",
    "end": "2376700"
  },
  {
    "text": "We're gonna vote. So um, I'm gonna ask, who thinks that the initial setting of value- of the value function does not matter?",
    "start": "2381370",
    "end": "2390950"
  },
  {
    "text": "Great. Yes, it does not matter, and it doesn't matter, so no. Doesn't matter and why not?",
    "start": "2390950",
    "end": "2396680"
  },
  {
    "text": "Because there's only a single fixed point. Because it's like in- uh, the Bellman operator's a contraction operator. Just single.",
    "start": "2396680",
    "end": "2406110"
  },
  {
    "text": "And how about- who thinks the value iteration and policy iteration always yield the same solution?",
    "start": "2406390",
    "end": "2412310"
  },
  {
    "text": "Yes? No. Who thinks what- uh, why no.",
    "start": "2412310",
    "end": "2417680"
  },
  {
    "text": "Give me an example where they might not. [NOISE] Yeah. Verbal [inaudible].",
    "start": "2417680",
    "end": "2424160"
  },
  {
    "text": "Exactly, yeah. So that it is correct. You- you're gonna get the same, uh, value function. So it depends which way you're trying to answer this.",
    "start": "2424160",
    "end": "2429485"
  },
  {
    "text": "They're gonna have the same value, they could have different policies.",
    "start": "2429485",
    "end": "2434490"
  },
  {
    "text": "And that's possible if there's more than one policy that has the same optimal [NOISE] value.",
    "start": "2435280",
    "end": "2440720"
  },
  {
    "text": "That can come up because often there's, um, multiple policies where you're splitting ties.",
    "start": "2440720",
    "end": "2446224"
  },
  {
    "text": "Um, I- who thinks that the number of iterations needed for policy iteration is bounded? [NOISE] That's right.",
    "start": "2446225",
    "end": "2453215"
  },
  {
    "text": "Um, anyone wanna tell me how many it is? A S. [NOISE] That's the- that's the total number of policies,",
    "start": "2453215",
    "end": "2463445"
  },
  {
    "text": "um, and policy iteration in tabular MDPs. With like pol- or policy improvement in tabular MDPs,",
    "start": "2463445",
    "end": "2470255"
  },
  {
    "text": "you're guaranteed to be monotonically improving. So you can at most go through every policy once and then you're done.",
    "start": "2470255",
    "end": "2476330"
  },
  {
    "text": "So it sort of relates to what we were just talking about. In that case, um, you definitely get guaranteed policy improvement,",
    "start": "2476330",
    "end": "2482599"
  },
  {
    "text": "because every- there's no function approximation there, there's no errors, you exactly know what the current value is, then you can take a monotonic improvement step.",
    "start": "2482600",
    "end": "2490290"
  },
  {
    "text": "All right. So now we're gonna talk briefly of a refresher on model-free policy evaluation.",
    "start": "2490480",
    "end": "2495619"
  },
  {
    "text": "Um, so this is model-free policy evaluation was this sort of passive reinforcement learning,",
    "start": "2495620",
    "end": "2500990"
  },
  {
    "text": "um, where we're just trying to understand how good an existing policy is. Ideally, with not too much data.",
    "start": "2500990",
    "end": "2507695"
  },
  {
    "text": "Um, and so we want to either directly estimate the Q function or the value function of a policy.",
    "start": "2507695",
    "end": "2514890"
  },
  {
    "text": "And so we talked mostly in this case about episodic domains. When I say episodic domains, I mean,",
    "start": "2515050",
    "end": "2521359"
  },
  {
    "text": "that we are gonna act in the world for a fixed number of steps, or where we are in a setting where we know we have terminal states,",
    "start": "2521360",
    "end": "2527375"
  },
  {
    "text": "so we know the episodes will end. With probability 1, they have to end. Um, and then at that point you reset to a start state with some fixed distribution.",
    "start": "2527375",
    "end": "2536690"
  },
  {
    "text": "[NOISE] And in Monte Carlo approaches, we directly average the episodic rewards. It's pretty simple.",
    "start": "2536690",
    "end": "2543350"
  },
  {
    "text": "We take our existing policy, we run it out for H steps or until the end of, um, the episode. We reset, we repeat that a whole bunch of times, then we just average.",
    "start": "2543350",
    "end": "2551269"
  },
  {
    "text": "Um, but in TD learning or Q learning, we use a target to bootstrap. And I know you guys have seen this a number of times,",
    "start": "2551270",
    "end": "2558680"
  },
  {
    "start": "2557000",
    "end": "2756000"
  },
  {
    "text": "but just as a refresher, um, and I like these diagrams to start thinking about the distinctions. So when we've talked about dynamic programming here,",
    "start": "2558680",
    "end": "2565910"
  },
  {
    "text": "we've thought about the case where we know the transition model, we know the reward model. So when we think about what the value is of a policy,",
    "start": "2565910",
    "end": "2574085"
  },
  {
    "text": "it's exactly equal to the expected distribution of states and actions we would encounter by following this policy of the reward",
    "start": "2574085",
    "end": "2582380"
  },
  {
    "text": "we would get plus gamma times the value of the next state. So note that when we think about this expectation here there's really an s prime.",
    "start": "2582380",
    "end": "2591660"
  },
  {
    "text": "So that expectation is thinking about all the next states that we might get to. And so in dynamic programming,",
    "start": "2597790",
    "end": "2605120"
  },
  {
    "text": "we just explicitly think about that sum. That sum over all the next states we much reach- might reach,",
    "start": "2605120",
    "end": "2611030"
  },
  {
    "text": "and the value of each of those states. So if we had started in a state, we take an action, we get to some next new states.",
    "start": "2611030",
    "end": "2617780"
  },
  {
    "text": "In general, we could repeat this process all the way out till we reach, you know, the horizon H or terminal states.",
    "start": "2617780",
    "end": "2624560"
  },
  {
    "text": "Um, and this- what we think of here is taking an expectation over the next states that we would reach.",
    "start": "2624560",
    "end": "2630830"
  },
  {
    "text": "And what we do in dynamic programming is instead bootstrap. So what we mean by bootstrap here,",
    "start": "2630830",
    "end": "2638490"
  },
  {
    "text": "is that instead of building this whole tree, we keep [NOISE] track of what the value is of all the states,",
    "start": "2639340",
    "end": "2645020"
  },
  {
    "text": "and we use that to take an explicit expectation over the next states we'd reach, and average over the value of those next states.",
    "start": "2645020",
    "end": "2651665"
  },
  {
    "text": "And note that in this case, we're assuming we know the model. Now, there are ways to extend this where we don't know the model,",
    "start": "2651665",
    "end": "2658325"
  },
  {
    "text": "but we haven't talked very much about those, this term so, um. But when I say dynamic programming here,",
    "start": "2658325",
    "end": "2664595"
  },
  {
    "text": "I mean, that unless we otherwise specified that we know the models of the world. So this is a case where we're bootstrapping because we are- our update is using V,",
    "start": "2664595",
    "end": "2674180"
  },
  {
    "text": "for V uses an estimate. Okay? 'Cause those values are not going to be perfect estimates",
    "start": "2674180",
    "end": "2680059"
  },
  {
    "text": "of the true expected discounted reward for those next states, because we're still computing them. So then we looked at Monte Carlo policy evaluation,",
    "start": "2680060",
    "end": "2687875"
  },
  {
    "text": "and it looks pretty similar in many ways except for what we're doing is we're running a trajectory all the way out to the horizon,",
    "start": "2687875",
    "end": "2694715"
  },
  {
    "text": "we're adding up all the rewards, and that is our target. And when we say that policy, um,",
    "start": "2694715",
    "end": "2701630"
  },
  {
    "text": "evaluation with Monte Carlo is sampling, it means we're sampling the return. What is the expectation that we're approximating?",
    "start": "2701630",
    "end": "2708110"
  },
  {
    "text": "We're expectation- uh, we're approximating an expectation over that probability of s prime. [NOISE] So we only got a single s prime,",
    "start": "2708110",
    "end": "2716539"
  },
  {
    "text": "instead of getting an expectation over all the next ones. And the problem with that is that we said it was high-variance,",
    "start": "2716540",
    "end": "2722689"
  },
  {
    "text": "even though it's unbiased. And then we talked about combining these ideas with temporal difference methods,",
    "start": "2722689",
    "end": "2728840"
  },
  {
    "text": "um, where we're both gonna bootstrap and sample. So we're sampling because [NOISE] we are only looking at a single next state,",
    "start": "2728840",
    "end": "2736580"
  },
  {
    "text": "[NOISE] and we're bootstrapping [NOISE] because we're plugging in our estimate of V. So we're sampling a single s,",
    "start": "2736580",
    "end": "2745625"
  },
  {
    "text": "t+1, and we're bootstrapping because we're not rolling all the way out like we did with Monte Carlo, which is plugging in our current estimate of that value function.",
    "start": "2745625",
    "end": "2753420"
  },
  {
    "text": "So let's do another quick understanding of, um, for each of these cases, um,",
    "start": "2756130",
    "end": "2761420"
  },
  {
    "text": "it's good to know whether- uh, whether it applies to dynamic programming, um,",
    "start": "2761420",
    "end": "2766520"
  },
  {
    "text": "which requires you to know the models, um, Monte Carlo or TD learning. So is it usable when we don't know the models of the current domain?",
    "start": "2766520",
    "end": "2774005"
  },
  {
    "text": "Does it handle continuin- continuing non episodic domains? Does it handle non-Markovian domains?",
    "start": "2774005",
    "end": "2779645"
  },
  {
    "text": "Um, let me be clear by that- clear what I mean by that. You can always apply any algorithm to anything.",
    "start": "2779645",
    "end": "2785915"
  },
  {
    "text": "It just may give you garbage out. And so my question is, um, when you- when I say handling non-Markovian domains,",
    "start": "2785915",
    "end": "2791795"
  },
  {
    "text": "is it guaranteed to do something good or does it fumbling-fundamentally make a Markov assumption? Um, does it converge to the true value of the policy and the limit of updates?",
    "start": "2791795",
    "end": "2800465"
  },
  {
    "text": "Right now, we're thinking about tabular case. So the value function is exactly representable and",
    "start": "2800465",
    "end": "2808085"
  },
  {
    "text": "is it giving us an unbiased estimate of the value along the way?",
    "start": "2808085",
    "end": "2812250"
  },
  {
    "text": "The estimates still might be consistent, which means that eventually with an updater they converge to the right thing,",
    "start": "2813160",
    "end": "2819020"
  },
  {
    "text": "but they could give you biased estimates along the way. So again, let's just spend like a minute or two and, um, they are- this- there's just binary answers to each of these.",
    "start": "2819020",
    "end": "2826220"
  },
  {
    "text": "So yes or no for each of them. And feel free to talk to somebody next to you. [BACKGROUND]",
    "start": "2826220",
    "end": "2885275"
  },
  {
    "text": "Converge to different quality [OVERLAPPING]. But you could start with [BACKGROUND].",
    "start": "2885275",
    "end": "2893030"
  },
  {
    "text": "Yeah, it's a great question. Policy iteration [inaudible] good question.",
    "start": "2893030",
    "end": "2900920"
  },
  {
    "text": "[BACKGROUND]",
    "start": "2900920",
    "end": "2911030"
  },
  {
    "text": "All right. I'm gonna ask people to vote again. Okay, so, um, I'll just ask you to raise your hands if the answer is, yes.",
    "start": "2911030",
    "end": "2917595"
  },
  {
    "text": "So, um, is DP, is dynamic programming usable when there are no models of the current domain?",
    "start": "2917595",
    "end": "2923724"
  },
  {
    "text": "No. Is Monte Carlo usable? Yes. Is TD usable?",
    "start": "2923725",
    "end": "2930194"
  },
  {
    "text": "Great. Okay, um, does DP handle continuing non-episodic domains? Raise your hand if, yes.",
    "start": "2930195",
    "end": "2937775"
  },
  {
    "text": "Correct. Yep. So, you can use dynamic programming. You can use Bellman operators and contractions even if the,",
    "start": "2937775",
    "end": "2943280"
  },
  {
    "text": "you know, for infinite horizon domains. You generally want your gamma function to be less than one, so your values don't explode.",
    "start": "2943280",
    "end": "2949355"
  },
  {
    "text": "Um, uh, but you can do it. It's fine. What about Monte Carlo estimates? No. Monte Carlo only updates when you get to the end of an episode.",
    "start": "2949355",
    "end": "2958099"
  },
  {
    "text": "TD estimates? Yes. Great. Um, does DP handle non-Markovian domains?",
    "start": "2958100",
    "end": "2964694"
  },
  {
    "text": "No. No. Does Monte Carlo? Yes. Yes. TD? No. Again, you can run all of these things wherever you want,",
    "start": "2964694",
    "end": "2972589"
  },
  {
    "text": "but- Converges to the true value of the policy and the limit of updates for DP?",
    "start": "2972590",
    "end": "2979085"
  },
  {
    "text": "Yes. Yes, Monte Carlo? Yes. Yes. TD? Yes. Yes. Unbiased estimate of the value,",
    "start": "2979085",
    "end": "2986450"
  },
  {
    "text": "DP it's kind of not applicable, because we're not really using data. It's sort of a little bit different. Um, Monte Carlo was an unbiased estimate of the value.",
    "start": "2986450",
    "end": "2994085"
  },
  {
    "text": "Yes. Yes, TD? No. Great. Okay. So, um,",
    "start": "2994085",
    "end": "2999365"
  },
  {
    "text": "and if we're asking you about this in the exam we'd be sure to clarify whether we're talking about the tabular setting or",
    "start": "2999365",
    "end": "3004720"
  },
  {
    "text": "the function approximation setting where everything can be very different. Yes? Can you explain exactly why TD doesn't work for non-Markovian?",
    "start": "3004720",
    "end": "3012115"
  },
  {
    "text": "Yeah. So, yeah that's a good one. So why does TD not work for Markovian? Um, it's because it's fundamentally making a Markov assumption,",
    "start": "3012115",
    "end": "3020250"
  },
  {
    "text": "um, about the domain. And the reason that comes up is here. So the way it is writing down the value function,",
    "start": "3020250",
    "end": "3026670"
  },
  {
    "text": "is it saying that the expected discounted sum of rewards from the current state is exactly equal to the immediate reward plus",
    "start": "3026670",
    "end": "3034089"
  },
  {
    "text": "the discounted future sum of rewards for each of the next states, where that's encapsulated only by St + 1.",
    "start": "3034090",
    "end": "3040569"
  },
  {
    "text": "So that is where you're making the Markovian assumption, because your aliases- if, um, if you have an observation space which was",
    "start": "3040570",
    "end": "3046735"
  },
  {
    "text": "aliased that would ignore the whole history. Whereas Monte Carlo is summing up all the rewards from",
    "start": "3046735",
    "end": "3052000"
  },
  {
    "text": "that current state onwards. Good question. [inaudible] has that assumed Markovian process?",
    "start": "3052000",
    "end": "3062410"
  },
  {
    "text": "Great question, and remind me your name. Saying, um, we talked almost everything we've been talking about is TD",
    "start": "3062410",
    "end": "3069760"
  },
  {
    "text": "0 where we just have this reward plus gamma times the value function, but we also talked briefly about N step.",
    "start": "3069760",
    "end": "3075805"
  },
  {
    "text": "Um, where you sort of would do r1 + r2 + gamma times r2 et cetera.",
    "start": "3075805",
    "end": "3081099"
  },
  {
    "text": "So for the n-step you'd have something like this. You'd have rt + gamma rt + 1 + gamma squared",
    "start": "3081100",
    "end": "3088135"
  },
  {
    "text": "rt + 2 + gamma cubed V of st + 3.",
    "start": "3088135",
    "end": "3093910"
  },
  {
    "text": "So that would be like an n-step. Um, and that is essentially making different notions of Markovian assumptions.",
    "start": "3093910",
    "end": "3101185"
  },
  {
    "text": "Because you can have continuums you can either have completely non-Markovian domains or we can have things like n-step Markov domains.",
    "start": "3101185",
    "end": "3106795"
  },
  {
    "text": "Which essentially means that if you're making it- keeping track of a certain amount of history.",
    "start": "3106795",
    "end": "3114390"
  },
  {
    "text": "[NOISE] So um, just to sort of give",
    "start": "3114390",
    "end": "3120970"
  },
  {
    "start": "3118000",
    "end": "3600000"
  },
  {
    "text": "an example that similar to some of the ones that we've seen before. We can think of something like a random walk process.",
    "start": "3120970",
    "end": "3126670"
  },
  {
    "text": "So imagine that we have a domain where we have three states and two terminal states. So we always start in state B and",
    "start": "3126670",
    "end": "3133780"
  },
  {
    "text": "then with probability of 50% we go left or right. Um, and if you reached either the black nodes then the process terminates.",
    "start": "3133780",
    "end": "3143815"
  },
  {
    "text": "Um, and when you get there either you get + 1 on this one or you get 0.",
    "start": "3143815",
    "end": "3149185"
  },
  {
    "text": "And it's a random walk with equal probability, um, until you get to a terminal state and then the process ends.",
    "start": "3149185",
    "end": "3156740"
  },
  {
    "text": "And so in this case, we could try to compute like what is the true value of the state. Um, so the true value of a state in this case, um,",
    "start": "3156930",
    "end": "3166434"
  },
  {
    "text": "would involve us thinking about what is the, uh, distribution of states that you would visit under this random walk pro- process.",
    "start": "3166435",
    "end": "3173664"
  },
  {
    "text": "So for example, um, if we think about what the value is of- I'll do this here.",
    "start": "3173665",
    "end": "3179559"
  },
  {
    "text": "So if you think about what the value is of state C,",
    "start": "3179560",
    "end": "3185890"
  },
  {
    "text": "that's always gonna be equal to the immediate reward plus gamma times the sum over the next states,",
    "start": "3185890",
    "end": "3192820"
  },
  {
    "text": "value of S prime. Well, let's call this one like I know,",
    "start": "3193400",
    "end": "3199110"
  },
  {
    "text": "SD and this one S0. So SD's value, is always gonna be equal to + 1.",
    "start": "3199110",
    "end": "3206790"
  },
  {
    "text": "So V of SD is equal to + 1, um, because you get that reward and then it terminates.",
    "start": "3206790",
    "end": "3212279"
  },
  {
    "text": "So this would say with, um, gamma times half probability you would go to the value of SB,",
    "start": "3212280",
    "end": "3219645"
  },
  {
    "text": "SB plus half you get 1.",
    "start": "3219645",
    "end": "3223720"
  },
  {
    "text": "And eventually if you look at this distribution it's gonna be, um, so you could do this process for each of the different states.",
    "start": "3225030",
    "end": "3232525"
  },
  {
    "text": "And what you would find when you do this is that you get through this random walk terminating on the right side or the left side in terms of the probability distribution",
    "start": "3232525",
    "end": "3239904"
  },
  {
    "text": "and you could compute the values for this. Um, in an exam, we would probably make this a little bit easier,",
    "start": "3239904",
    "end": "3247210"
  },
  {
    "text": "but it's good to be able to sort of look at this example and work through it, um, and see what this part would be in terms of the value function.",
    "start": "3247210",
    "end": "3253765"
  },
  {
    "text": "Um, then the next question is let's imagine that we have a particular trajectory, we want to compare what would happen under different algorithms.",
    "start": "3253765",
    "end": "3261610"
  },
  {
    "text": "So let's imagine what we have is we have, um, a trajectory where you go BC,",
    "start": "3261610",
    "end": "3266740"
  },
  {
    "text": "BC terminal + 1. So that's our episode.",
    "start": "3266740",
    "end": "3275410"
  },
  {
    "text": "So what is the first visit Monte Carlo estimate of B?",
    "start": "3275410",
    "end": "3282039"
  },
  {
    "text": "One. One. That's right.",
    "start": "3282040",
    "end": "3287365"
  },
  {
    "text": "So, so V of B is equal to 1. Why is that? Because what we do in Monte Carlo is we add up, versus at Monte Carlo,",
    "start": "3287365",
    "end": "3294250"
  },
  {
    "text": "we look at the first time we visited the state and we add up all the rewards we get from that state till the end of the episode.",
    "start": "3294250",
    "end": "3299980"
  },
  {
    "text": "In this case, that reward is just 1. So, the estimate of this would be 1.",
    "start": "3299980",
    "end": "3306350"
  },
  {
    "text": "The only other, you know, thing that we might want to know about there is if you're doing sort of this sliding average",
    "start": "3306360",
    "end": "3312130"
  },
  {
    "text": "like an alpha estimate to update the Monte Carlo estimate, you'd want to know what the initial values were and what alpha was.",
    "start": "3312130",
    "end": "3318640"
  },
  {
    "text": "But let's imagine that here you just look at exactly taking that return. So this is equal to the return starting at B going to the end of the episode.",
    "start": "3318640",
    "end": "3328970"
  },
  {
    "text": "So then the next question is, um, what are the TD learning updates given the data in this order?",
    "start": "3329220",
    "end": "3335619"
  },
  {
    "text": "C terminal + 1 BC0, CB0 with a learning rate of A.",
    "start": "3335620",
    "end": "3341780"
  },
  {
    "text": "Maybe just take like a minute or two and, um, do one or two of these updates.",
    "start": "3342300",
    "end": "3349285"
  },
  {
    "text": "And then think about what would happen if we reverse the order of the data with the same learning rate.",
    "start": "3349285",
    "end": "3355509"
  },
  {
    "text": "So, this relates to a point we've talked about a couple of times about whether or not the order of updates we do given some set of data matters in terms of the values we compute.",
    "start": "3355510",
    "end": "3365109"
  },
  {
    "text": "So I guess I would go at this in the following way. I would first commit yourself either way whether or",
    "start": "3365110",
    "end": "3370690"
  },
  {
    "text": "not the order matters in terms of the values we're gonna compute, um, and then try to compute one or two of them.",
    "start": "3370690",
    "end": "3377570"
  },
  {
    "text": "So let's just spend like a minute or two to decide whether or not the order matters here",
    "start": "3378990",
    "end": "3385869"
  },
  {
    "text": "[NOISE] in terms of the resulting values and then we can also compute.",
    "start": "3385870",
    "end": "3392950"
  },
  {
    "text": "[NOISE]",
    "start": "3392950",
    "end": "3465250"
  },
  {
    "text": "I know I'm not giving you guys enough time to do all the computations here, but this is mostly to just sort of do that forced recall aspect to try to",
    "start": "3465250",
    "end": "3471160"
  },
  {
    "text": "remember exactly what the formulas are and then remember whether or not this matters. So I'm just gonna ask you to vote. Um, who here thinks that the order matters in terms of some of the values we compute?",
    "start": "3471160",
    "end": "3479559"
  },
  {
    "text": "[NOISE] It's right. No, it won't always. Sometimes do- you can do things in different orders um,",
    "start": "3479560",
    "end": "3484839"
  },
  {
    "text": "the fact that we've emphasized a lot might lead you to believe that it always matters, but it doesn't always matter. But in this case it does.",
    "start": "3484840",
    "end": "3490210"
  },
  {
    "text": "So um, in this case, if we look at what the value is in the first-order, what we would do is we'd say V of C = 0 + alpha 1 - 0.",
    "start": "3490210",
    "end": "3499570"
  },
  {
    "text": "So the new reward we've observed. That would be alpha, then when we're computing the value of B,",
    "start": "3499570",
    "end": "3506380"
  },
  {
    "text": "we could use the new B of C we just computed because when we have this update, now we've already got a non-zero estimate for V of C. Um,",
    "start": "3506380",
    "end": "3515545"
  },
  {
    "text": "note to be precise, I should have told you here exactly how we're initializing all the values. So in this case, we've implicitly assumed that",
    "start": "3515545",
    "end": "3522370"
  },
  {
    "text": "the initial values are 0 which matters a lot.",
    "start": "3522370",
    "end": "3528460"
  },
  {
    "text": "[NOISE] We'll talk some more, um, in a week or two about",
    "start": "3528460",
    "end": "3533499"
  },
  {
    "text": "smarter exploration and the fact that being optimistic often really is very helpful. One challenge can be in deep neural networks is",
    "start": "3533499",
    "end": "3539890"
  },
  {
    "text": "how to set things so that they're optimistic. Um, but in this case, so we're assuming that everything is 0,",
    "start": "3539890",
    "end": "3545395"
  },
  {
    "text": "so V of B will be alpha squared, V of C will be the following expression. Um, these are basically me just applying TD learning to these cases.",
    "start": "3545395",
    "end": "3553060"
  },
  {
    "text": "[inaudible] They're in the second line, yeah.",
    "start": "3553060",
    "end": "3561220"
  },
  {
    "text": "Good catch. [NOISE] In the, where?",
    "start": "3561220",
    "end": "3567625"
  },
  {
    "text": "Should be gamma squared. Gamma squared, yeah, yes,",
    "start": "3567625",
    "end": "3573535"
  },
  {
    "text": "yeah, that final expression. Thanks. Um, so which appears in the third line.",
    "start": "3573535",
    "end": "3580240"
  },
  {
    "text": "[LAUGHTER] If we do it in the reverse order, V of C will be 0 for our first update because C goes to B,",
    "start": "3580240",
    "end": "3588880"
  },
  {
    "text": "B, B of B is 0. Then when we update BC0, the value of C is still 0,",
    "start": "3588880",
    "end": "3594745"
  },
  {
    "text": "and we only update V of C in the final one. So this just points out that order matters.",
    "start": "3594745",
    "end": "3600100"
  },
  {
    "text": "This comes up also when we're doing function approximation and episodic replay.",
    "start": "3600100",
    "end": "3604370"
  },
  {
    "text": "Just in general, when we think about policy evaluation algorithms. It's good to be aware of the bias-variance tradeoff,",
    "start": "3605880",
    "end": "3611244"
  },
  {
    "text": "data efficiency, and computational efficiency. TD learning tends to be pretty good on computational efficiency,",
    "start": "3611245",
    "end": "3616435"
  },
  {
    "text": "um data efficiency-wise, it depends a little bit. Sometimes if you do experience replay with TD, it gets better.",
    "start": "3616435",
    "end": "3623454"
  },
  {
    "text": "So um, it's useful to think about there's often a lot of variants of these algorithms.",
    "start": "3623455",
    "end": "3629515"
  },
  {
    "text": "And so just being precise in whatever your, whatever your state is. And if you're just assuming the vanilla version, we're using or if you're like,",
    "start": "3629515",
    "end": "3635110"
  },
  {
    "text": "well if you do this additional experience replay, this is how it can change. Okay. Now let's think about how we can do model free learning to make good decisions.",
    "start": "3635110",
    "end": "3645025"
  },
  {
    "text": "Um, we've talked a lot about Q learning. Q learning is a bootstrapping technique that assumes Markovian, a Markovian world.",
    "start": "3645025",
    "end": "3652480"
  },
  {
    "text": "Where we say that the value- the Q value is gonna be um approximated by the reward plus gamma times max over a prime with the next Q function.",
    "start": "3652480",
    "end": "3661734"
  },
  {
    "text": "And we can use that as sort of our target, and then we do the slow slewing. We sort of have this updated learning rate, and our learning rate,",
    "start": "3661735",
    "end": "3668410"
  },
  {
    "text": "[NOISE] where we're slewing between the one sample we just saw versus,",
    "start": "3668410",
    "end": "3675535"
  },
  {
    "text": "um, our previous estimate. And we slowly slew this towards, um, we generally decrease alpha over time to try to converge Q to a single value.",
    "start": "3675535",
    "end": "3685460"
  },
  {
    "text": "Uh, we talked about some conditions under which for Q learning to converge. Again this is all under sort of well,",
    "start": "3686850",
    "end": "3693070"
  },
  {
    "text": "this is both under reachability assumptions and also we're right now we're talking about the tabular setting. [NOISE] So there's no function approximation going on.",
    "start": "3693070",
    "end": "3700960"
  },
  {
    "text": "[NOISE] So if you act randomly, Q-learning will converge to Q star under mild reachability assumptions,",
    "start": "3700960",
    "end": "3706540"
  },
  {
    "text": "um, which means that, you know, you can't have a helicopter which if you crash it, the world is over and you can't get any more samples.",
    "start": "3706540",
    "end": "3712269"
  },
  {
    "text": "So you have to be able to sort of repeatedly visit all the states, an infinite number of times and try all of the actions an infinite number of times.",
    "start": "3712270",
    "end": "3718900"
  },
  {
    "text": "Um, [NOISE] and it has this interesting property that, um, when you are doing Q-learning you can use data gathered by",
    "start": "3718900",
    "end": "3724720"
  },
  {
    "text": "one policy to estimate the value of another policy. So this is where we're trying to estimate the optimal Q function,",
    "start": "3724720",
    "end": "3730555"
  },
  {
    "text": "but we can use for example a random data, random samples, [NOISE] or random policy to try to estimate that.",
    "start": "3730555",
    "end": "3736315"
  },
  {
    "text": "And the reason for that is because we're doing this max. We're always looking at what's the best thing we could do next.",
    "start": "3736315",
    "end": "3742585"
  },
  {
    "text": "So that's a pretty cool property. Um, so then if we sort of think about in this case,",
    "start": "3742585",
    "end": "3749650"
  },
  {
    "text": "um, there's some different things, we'll go through these I guess just briefly. Um, if you have a Q-learning policy, um,",
    "start": "3749650",
    "end": "3759355"
  },
  {
    "text": "which has e-greedy, e-greedy here is with probability 1 - epsilon.",
    "start": "3759355",
    "end": "3765205"
  },
  {
    "text": "You take the action which is expected to be best under your current Q function. Um, and with probability epsilon, you would act randomly.",
    "start": "3765205",
    "end": "3772105"
  },
  {
    "text": "So if you were in a lookup table, this is guaranteed to converge to the optimal policy and the limit of infinite data.",
    "start": "3772105",
    "end": "3779215"
  },
  {
    "text": "So this is yes, with mild reachability. [NOISE] Um, for this second one can we use Monte Carlo estimation",
    "start": "3779215",
    "end": "3793270"
  },
  {
    "text": "and MDPs with large state spaces? Let's vote if yes.",
    "start": "3793270",
    "end": "3798640"
  },
  {
    "text": "[NOISE] Whatever, I'll take a second, and just talk to your neighbor, and we'll vote again.",
    "start": "3798640",
    "end": "3804760"
  },
  {
    "text": "[NOISE] I'm not saying that those people are wrong, I'm just saying that since most people didn't vote, I'm assuming that most people would benefit from just thinking about it for a sec.",
    "start": "3804760",
    "end": "3813160"
  },
  {
    "text": "[NOISE]",
    "start": "3813160",
    "end": "3815180"
  },
  {
    "text": "All right. Let's vote again. Um, vote if you think",
    "start": "3859180",
    "end": "3865599"
  },
  {
    "text": "Monte Carlo estimation can be used in MDPs with large state spaces. Yes. Yeah. So it's not, um,",
    "start": "3865600",
    "end": "3872470"
  },
  {
    "text": "it's not- you're not restricted to whether it's a large state space or not, you can use Monte Carlo estimation there, um, I, that, that's really, I guess it can.",
    "start": "3872470",
    "end": "3881320"
  },
  {
    "text": "So no Monte Carlo can be used.",
    "start": "3881320",
    "end": "3886390"
  },
  {
    "text": "That number can be point positive depending [inaudible]",
    "start": "3886390",
    "end": "3892510"
  },
  {
    "text": "Yes. It's a great question. So, um, uh, Monte Carlo, if you- the number of data points per state could be very low.",
    "start": "3892510",
    "end": "3899470"
  },
  {
    "text": "If you have a single start state, that's not too bad. If you have a distribution of starts space- states, that can be trickier, or we're gonna want to move",
    "start": "3899470",
    "end": "3905080"
  },
  {
    "text": "into the value function approximation setting. But there's nothing a priori, which means you can't apply it. If you can put in there it might be really bad.",
    "start": "3905080",
    "end": "3911920"
  },
  {
    "text": "[LAUGHTER] Um, we might need to start doing, ah, function approximation. The last thing I put on there- this is something that's,",
    "start": "3911920",
    "end": "3919365"
  },
  {
    "text": "um, we haven't discussed a lot yet, um, but I think it's an interesting start for one to sort of start",
    "start": "3919365",
    "end": "3924900"
  },
  {
    "text": "connecting these between the dynamic programming aspects we've talked about. Um, a model-based reinforcement learning is not",
    "start": "3924900",
    "end": "3931720"
  },
  {
    "text": "necessarily always more data efficient than model-free, though we- we talked mostly about model-free.",
    "start": "3931720",
    "end": "3936820"
  },
  {
    "text": "Um, so we haven't discussed this too much, but, well, it's a good thing to be thinking about particularly as we start getting into exploration.",
    "start": "3936820",
    "end": "3943450"
  },
  {
    "text": "Um, and I mentioned briefly before that there's a nice new paper by Wen Sun and some colleagues at MSR,",
    "start": "3943450",
    "end": "3949345"
  },
  {
    "text": "Microsoft Research New York City, that is showing that in some cases, um, model-based is strictly better than model-free.",
    "start": "3949345",
    "end": "3957505"
  },
  {
    "text": "And the intuition there is that you can compactly represent the model, but you can't compactly represent the value function.",
    "start": "3957505",
    "end": "3963250"
  },
  {
    "text": "So you don't need a lot of parameters to learn the model, and then you can plan with it, but if you try to directly learn the value function,",
    "start": "3963250",
    "end": "3968560"
  },
  {
    "text": "you need a lot more. All right. So as we're sort of starting to move into an even just in that discussion,",
    "start": "3968560",
    "end": "3975865"
  },
  {
    "text": "a lot of our recent focus has been in value function approximation. I'm including in homework 2.",
    "start": "3975865",
    "end": "3981055"
  },
  {
    "text": "So um, we talked about if you were looking at Monte Carlo methods versus TD learning,",
    "start": "3981055",
    "end": "3986740"
  },
  {
    "text": "um, what sort of convergence guarantees do we have in the on policy case? So this is, um, important to emphasize.",
    "start": "3986740",
    "end": "3996250"
  },
  {
    "text": "So we're looking at evaluating the value of a single policy, and we talked about how we can think about the on policy stationary distribution.",
    "start": "3996250",
    "end": "4003900"
  },
  {
    "text": "That when we define a single policy, then, uh, [NOISE] and we run it, that's like we're inducing a Markov reward process or a Markov chain,",
    "start": "4003900",
    "end": "4010650"
  },
  {
    "text": "and we think- can think about the stationary distribution of states that we would visit under that policy. Um, and we talked about convergence properties.",
    "start": "4010650",
    "end": "4018280"
  },
  {
    "text": "And in particular, we said that what Monte Carlo does, um, no matter what sort of function approximator you are using,",
    "start": "4018280",
    "end": "4025010"
  },
  {
    "text": "is it tries to minimize, um, the mean squared error. So this style of techniques we've talked about with Monte Carlo, um,",
    "start": "4025010",
    "end": "4031859"
  },
  {
    "text": "is that it simply tries to minimize the mean squared error of your data. [NOISE] And so we can think about this",
    "start": "4031860",
    "end": "4040560"
  },
  {
    "text": "for linear value function approximators shouldn't- this also holds for other value function approximation- mators,",
    "start": "4040560",
    "end": "4045600"
  },
  {
    "text": "it's gonna minimize the error. [NOISE] In the case of linear value function approximation with TD learner- learning,",
    "start": "4045600",
    "end": "4052935"
  },
  {
    "text": "it is gonna converge to a constant factor of the best mean squared error. And what does that mean in this case?",
    "start": "4052935",
    "end": "4058530"
  },
  {
    "text": "Um, here, what we have is- we might have a gap,",
    "start": "4058530",
    "end": "4065835"
  },
  {
    "text": "so particularly if you have some like linear value function approximators, um, you just might not be able to write to exactly represent the value of all the states,",
    "start": "4065835",
    "end": "4073305"
  },
  {
    "text": "use the- the chosen like a parametric family that you have.",
    "start": "4073305",
    "end": "4078839"
  },
  {
    "text": "And so there might fundamentally just be a gap between, um, the value function that's representable with the space that you have,",
    "start": "4078840",
    "end": "4086550"
  },
  {
    "text": "and the true value function. I often like to think about like this. There's a nice picture and set no partner about this two.",
    "start": "4086550",
    "end": "4092565"
  },
  {
    "text": "This is sort of this, ah, showing with your set of W, what are the value functions you can represent,",
    "start": "4092565",
    "end": "4097845"
  },
  {
    "text": "and it might be that your variable value function lives up here. You just can't with- for example,",
    "start": "4097845",
    "end": "4103740"
  },
  {
    "text": "with a line be able to represent all of the true value functions. Um, if you think about this in two dimen- in, um, another dimension,",
    "start": "4103740",
    "end": "4110009"
  },
  {
    "text": "you can imagine for a state maybe your real value function looks something like this, but you are using a line approximator,",
    "start": "4110010",
    "end": "4115350"
  },
  {
    "text": "so you just can't represent that exactly a- a straight line. So Monte Carlo converges to the best mean squared error possible,",
    "start": "4115350",
    "end": "4125040"
  },
  {
    "text": "giving your value function approximator space, and TD learner converges to that times this additional factor.",
    "start": "4125040",
    "end": "4131310"
  },
  {
    "text": "[NOISE] Oop, see.",
    "start": "4131310",
    "end": "4136890"
  },
  {
    "text": "Well, I think that's not going to like that. Okay. Um, so note that there's this.",
    "start": "4136890",
    "end": "4143520"
  },
  {
    "text": "[NOISE]",
    "start": "4143520",
    "end": "4155160"
  },
  {
    "text": "Okay. Well, now I'm just gonna make that not do that anymore, all right.",
    "start": "4155160",
    "end": "4167319"
  },
  {
    "text": "We talked about the fact that when you're doing off policy learning, Q-learning with function approximation can diverge,",
    "start": "4177860",
    "end": "4183765"
  },
  {
    "text": "which means that it doesn't even converge with infinite amounts of data. This is even separate than what it might converge to if it converges.",
    "start": "4183765",
    "end": "4192255"
  },
  {
    "text": "It just says that the actual- your parameters just may never stop changing if you're doing sort of gradient updates. Yeah.",
    "start": "4192255",
    "end": "4198480"
  },
  {
    "text": "Can we initialize the function approximator [NOISE] in those parameters in such a way that [inaudible] push convergence and not guarantee it's not like?",
    "start": "4198480",
    "end": "4207480"
  },
  {
    "text": "Well, we have conditions on whether or not, um, the initialization of the parameters helps determine whether or not, for example, you might converge or diverge.",
    "start": "4207480",
    "end": "4213885"
  },
  {
    "text": "Um, it's an interesting question, I don't think there's work that I know that formally tries to characterize this, like, you know,",
    "start": "4213885",
    "end": "4219060"
  },
  {
    "text": "are there places where you could formally do this, um, so that, ah, in terms of your gradients,",
    "start": "4219060",
    "end": "4225660"
  },
  {
    "text": "for example, they wouldn't start to explode? There might be, I suspect it depends a lot on the problem. And I also suspect that there might be",
    "start": "4225660",
    "end": "4231300"
  },
  {
    "text": "pathological examples that you can construct where it's hard to do. But certainly worth a try. You can also ob- ob- observe whether or not",
    "start": "4231300",
    "end": "4237300"
  },
  {
    "text": "your parameter estimates are continuing to change. Um, we talked quite a lot,",
    "start": "4237300",
    "end": "4243405"
  },
  {
    "text": "you guys had a lot of practice with deep learning and model-free Q-learning, um, where we looked at having this Q-learning target and the Q network,",
    "start": "4243405",
    "end": "4248969"
  },
  {
    "text": "and we're doing stochastic gradient descent. Um, we're using a deep neural network to approximate Q. [NOISE] Um, we talked about the- some of the challenges with",
    "start": "4248970",
    "end": "4257219"
  },
  {
    "text": "this divergence might be that we have these correlated local updates. The value of a state is often very closely related",
    "start": "4257220",
    "end": "4263040"
  },
  {
    "text": "to the value of its next successor state. Um, and that also by changing these targets frequently,",
    "start": "4263040",
    "end": "4270465"
  },
  {
    "text": "um, then that might cause, ah, instability. So that's a lot of the recent progress over",
    "start": "4270465",
    "end": "4275580"
  },
  {
    "text": "roughly the last five years has been in sort of ways to modify this equation in order to make it more stable when you're doing gradient descent.",
    "start": "4275580",
    "end": "4283035"
  },
  {
    "text": "Um, and in DQN, it's sort of both introduced that we should do experience replay,",
    "start": "4283035",
    "end": "4288270"
  },
  {
    "text": "so don't use each data point once, um, and also fix the target for a while. So you're sort of saying, \"I'm going to use this.\"",
    "start": "4288270",
    "end": "4293955"
  },
  {
    "text": "A fixed value function approximator my next state for a while, and then we can minimize our mean squared error.",
    "start": "4293955",
    "end": "4299980"
  },
  {
    "text": "We talked about the fact that experience replay is particularly hugely helpful, um, and the targets is also quite.",
    "start": "4300380",
    "end": "4306315"
  },
  {
    "text": "Ah, and there aren't good guarantees yet on convergence, though there's a lot of interesting work that's being done in this space.",
    "start": "4306315",
    "end": "4314520"
  },
  {
    "text": "People are very interested in trying to understand the formal properties of these type of networks. Um, we also talked about double Q,",
    "start": "4314520",
    "end": "4320700"
  },
  {
    "text": "dueling, um, and, uh, like prioritized replay, um, as things that we could look at to try to",
    "start": "4320700",
    "end": "4326760"
  },
  {
    "text": "improve how quickly our Q functions converge to something reasonable.",
    "start": "4326760",
    "end": "4331300"
  },
  {
    "text": "So I think this is the last one for today. Um, ah so quick question.",
    "start": "4332300",
    "end": "4337680"
  },
  {
    "text": "Um, in finite state spaces with features that can represent the true value function. Does TD learning with value function approximation always find",
    "start": "4337680",
    "end": "4344520"
  },
  {
    "text": "the true value function of the policy given sufficient data?",
    "start": "4344520",
    "end": "4348700"
  },
  {
    "text": "So this is for TD learning. So we're- we're essentially doing policy evaluation right now.",
    "start": "4350150",
    "end": "4356850"
  },
  {
    "text": "So in this case, are we guaranteed to find the true value function given sufficient data?",
    "start": "4356850",
    "end": "4363820"
  },
  {
    "text": "Maybe take one, chat with a neighbor for one minute and then I'll ask people to vote.",
    "start": "4366320",
    "end": "4371474"
  },
  {
    "text": "Should we assume this is on on policy? We're gonna assume this is on policy,",
    "start": "4371475",
    "end": "4376210"
  },
  {
    "text": "or at least with sufficient amounts of data from the on policy distribution. [BACKGROUND]",
    "start": "4377780",
    "end": "4407790"
  },
  {
    "text": "Who wants to vote yes that we do find the true value function approximator? That's right. Okay, so how could we have checked this?",
    "start": "4407790",
    "end": "4415469"
  },
  {
    "text": "So if we go back to what I was saying over here. What I said is that we are going to converge to",
    "start": "4415470",
    "end": "4421680"
  },
  {
    "text": "a constant factor of the best mean squared error. This mean squared error is always 0, if you can exactly represent the value in the current space.",
    "start": "4421680",
    "end": "4431230"
  },
  {
    "text": "So that additional sort of constant factor is just a constant factor times 0.",
    "start": "4431240",
    "end": "4436815"
  },
  {
    "text": "So in this case, yes.",
    "start": "4436815",
    "end": "4440890"
  },
  {
    "text": "So I- because I said here that it- with features that can represent the true value function.",
    "start": "4441890",
    "end": "4448489"
  },
  {
    "text": "So we've said that it is perfectly possible to represent the value function of this policy in the features that are given to you and so it will be possible to achieve that. Yeah.",
    "start": "4448490",
    "end": "4457970"
  },
  {
    "text": "Is it true for a non-linear parameterization? For TD learning? Yeah. So for policy evaluation, um, uh,",
    "start": "4457970",
    "end": "4465389"
  },
  {
    "text": "if you have a nonlinear- if you have features- like if you have a general representation that allows you to exactly",
    "start": "4465390",
    "end": "4471630"
  },
  {
    "text": "represent the value function and you're doing on policy learning. So you're doing TD learning, um, you will be able to get zero error,",
    "start": "4471630",
    "end": "4477780"
  },
  {
    "text": "with infinite, you know, sufficient data etc. Finite data, this is all of it. Yeah. One part that I-",
    "start": "4477780",
    "end": "4483809"
  },
  {
    "text": "Remind me your name, please. I, I got a little confused given that we, we might have all of the features that we want,",
    "start": "4483810",
    "end": "4491045"
  },
  {
    "text": "but we might not have any representative value function approximation",
    "start": "4491045",
    "end": "4496820"
  },
  {
    "text": "that would actually be able to generate the Qs. Like the- a- are those two things like identical?",
    "start": "4496820",
    "end": "4503205"
  },
  {
    "text": "Like, I guess the way I was thinking about this was we might have all the features, but we might not find a space of functions that",
    "start": "4503205",
    "end": "4509910"
  },
  {
    "text": "actually would be able to represent the value function. Okay. So I think the question is say, okay.",
    "start": "4509910",
    "end": "4515430"
  },
  {
    "text": "Well, what if we had a lot of features, but like- does that actually give us a parameterization of the value function that can represent the true value function?",
    "start": "4515430",
    "end": "4522179"
  },
  {
    "text": "When I say here sort of features and representation I mean that we have picked a function class that can exactly represent the value function,",
    "start": "4522180",
    "end": "4529739"
  },
  {
    "text": "if we have an algorithm to try to fit it well enough. So, um what I'm assuming- what I'm saying in this case is",
    "start": "4529740",
    "end": "4535230"
  },
  {
    "text": "that if your value function- if someone could- if an oracle could give you those features- the- the- the parameter vector that would make,",
    "start": "4535230",
    "end": "4542969"
  },
  {
    "text": "um, uh, that zero, that TD learning can find it. So this essentially like [inaudible] because we can generate the table, keep it.",
    "start": "4542970",
    "end": "4551205"
  },
  {
    "text": "It doesn't have to be tabular. So to go over- this does not to have to only hold for tabular cases.",
    "start": "4551205",
    "end": "4556315"
  },
  {
    "text": "It's that if like- so if we look at, um, something here. Let's imagine this is your state space.",
    "start": "4556315",
    "end": "4562340"
  },
  {
    "text": "This is your value function. So if someone gives you, uh, a line, or a quadratic,",
    "start": "4562340",
    "end": "4567950"
  },
  {
    "text": "or a deep neural network with enough parameters to exactly represent that line, what this statement is saying is that",
    "start": "4567950",
    "end": "4573590"
  },
  {
    "text": "TD learning can find- can fit those parameters exactly. This is not true when we start to go into Q learning.",
    "start": "4573590",
    "end": "4580304"
  },
  {
    "text": "So in some cases, you can have a representation that could optimally represent the value function, but you can't find it.",
    "start": "4580305",
    "end": "4586679"
  },
  {
    "text": "Like Q learning will not identify it. So that's sort of the difference that we're trying to make here, is that in TD learning, if that exists and you're on policy, you can find it.",
    "start": "4586680",
    "end": "4594135"
  },
  {
    "text": "Q learning, you may not be able to. Yeah. There's a question in the back. Your name first, please. So can I just clarify this is to whether the value approximation is linear or nonlinear?",
    "start": "4594135",
    "end": "4603465"
  },
  {
    "text": "Yes. Yes. So what's we're trying- this is true, um, for generic representations.",
    "start": "4603465",
    "end": "4612795"
  },
  {
    "text": "If your representation, whether it's linear, or tabular or, um,",
    "start": "4612795",
    "end": "4617895"
  },
  {
    "text": "tabular generally always assume it's- it's exact. So linear or otherwise, then- then this is- this is true. Yes.",
    "start": "4617895",
    "end": "4625695"
  },
  {
    "text": "Uh, does this have anything to do with if our value function approximator is a contracting operator or not? So.",
    "start": "4625695",
    "end": "4634829"
  },
  {
    "text": "Yeah. It's a great question. You asked whether or not this has to do with whether or not our value function approximator is a contraction.",
    "start": "4634830",
    "end": "4640785"
  },
  {
    "text": "You can think of when we're doing this sort of TD learning, that we have two steps. We're kinda doing our approximated Bellman.",
    "start": "4640785",
    "end": "4646155"
  },
  {
    "text": "And our Bellman operator, if we could do it exactly we know is a contraction, then we have to do this additional part of fitting a function.",
    "start": "4646155",
    "end": "4651885"
  },
  {
    "text": "And if you can exactly fit your function, um, then you're not going to introduce additional error during that part.",
    "start": "4651885",
    "end": "4657659"
  },
  {
    "text": "Um, and that's- that's one of- that's one of the benefits in here. That cannot- that can start to be divert-",
    "start": "4657660",
    "end": "4664440"
  },
  {
    "text": "So in this case again, it's all on policies so it's much closer to the supervised learning setting. When you start to be off policy this gets more complicated. Question or?",
    "start": "4664440",
    "end": "4672960"
  },
  {
    "text": "No. Okay. All right. So let's just go really briefly through imitation learning, um, and policy search.",
    "start": "4672960",
    "end": "4679350"
  },
  {
    "text": "This will be kind of at the same lighter level that, um, you'd be expected to know it for the exam. You haven't had the chance to practice either of these,",
    "start": "4679350",
    "end": "4685679"
  },
  {
    "text": "um, except where from lecture. So imitation learning was the idea that the specification of reward functions can be really complicated.",
    "start": "4685680",
    "end": "4692975"
  },
  {
    "text": "What if we could just have people demonstrate procedures and then learn from them? Behavior cloning is where we're doing supervised learning.",
    "start": "4692975",
    "end": "4699724"
  },
  {
    "text": "So we're trying to learn a mapping of actions to states, and we're treating this as a supervised learning problem.",
    "start": "4699725",
    "end": "4706530"
  },
  {
    "text": "So we just look at for an expert, pairs of states and action,",
    "start": "4706530",
    "end": "4711555"
  },
  {
    "text": "and you can try to fit your favorite machine learning supervised, like classification algorithm to- to predict that.",
    "start": "4711555",
    "end": "4718139"
  },
  {
    "text": "And the thing that can go wrong in this case is that your state distribution that you induce under your approximate policy,",
    "start": "4718140",
    "end": "4725895"
  },
  {
    "text": "that's trying to mimic the expert, can be different than, um, the- the distribution of states you'd reach on to the expert policy.",
    "start": "4725895",
    "end": "4732465"
  },
  {
    "text": "Which means that you can end up with these sort of different state distributions, and you don't know what the right thing is",
    "start": "4732465",
    "end": "4739410"
  },
  {
    "text": "to do under these new states because you don't have any data about that. So things can go pretty badly in some of those cases.",
    "start": "4739410",
    "end": "4745620"
  },
  {
    "text": "We talked about imitation learning, where the idea is that we have again trajectories of- of demonstrations.",
    "start": "4745620",
    "end": "4751784"
  },
  {
    "text": "And now the goal is to directly learn rewards.",
    "start": "4751785",
    "end": "4758295"
  },
  {
    "text": "A good thing to rethink about here is how many reward functions are compatible with an expert's demonstration.",
    "start": "4758295",
    "end": "4765255"
  },
  {
    "text": "We talked about this before. If it's not clear, feel free to reach out to me either at the end of class or, um, on Piazza.",
    "start": "4765255",
    "end": "4772005"
  },
  {
    "text": "And we talked about policy search. So just really briefly. These are the types of levels of,",
    "start": "4772005",
    "end": "4777960"
  },
  {
    "text": "um, questions I would expect you to be familiar with. So why do we want to do stochastic parametrized policies?",
    "start": "4777960",
    "end": "4783284"
  },
  {
    "text": "Can be a nice way to put it in domain knowledge. It can help us with non-Markovian structure. We talked about aliasing,",
    "start": "4783285",
    "end": "4789960"
  },
  {
    "text": "and we talked about game theory settings, where deterministic policies would do badly, but stochastic ones would do well.",
    "start": "4789960",
    "end": "4796485"
  },
  {
    "text": "Um, policy gradient methods are not the only form of policy search. We talked about exoskeleton optimization by my colleague Steve Collins,",
    "start": "4796485",
    "end": "4804645"
  },
  {
    "text": "and the fact that, um, that worked pretty well. But generally, we're going to talk mostly about gradients. Um, the likelihood ratio policy gradient method does",
    "start": "4804645",
    "end": "4812070"
  },
  {
    "text": "not need us to have the dynamics model, which is really important because when we don't have it.",
    "start": "4812070",
    "end": "4817800"
  },
  {
    "text": "And then two ideas to reduce the variance of a policy gradient estimator is to use the temporal structure.",
    "start": "4817800",
    "end": "4825550"
  },
  {
    "text": "And here, it involves the fact that the reward you get at a timestep now can't be influenced by your future decisions because of the structure of time.",
    "start": "4826670",
    "end": "4835605"
  },
  {
    "text": "And then, um- and secondly baselines. So that's kind of the level,",
    "start": "4835605",
    "end": "4841830"
  },
  {
    "text": "the sort of- the stuff we talked about in class, but not deep procedural knowledge. So just to summarize.",
    "start": "4841830",
    "end": "4847830"
  },
  {
    "text": "Um, recommendations would be to go through lecture notes, look at things, like check your understanding. If you want to look at existing, uh,",
    "start": "4847830",
    "end": "4853680"
  },
  {
    "text": "additional examples going through session, um, notes can be useful. Um, the practice midterms particularly",
    "start": "4853680",
    "end": "4859380"
  },
  {
    "text": "last year will be more similar to the one from two years ago. Um, if you see some topic that we haven't covered in this class,",
    "start": "4859380",
    "end": "4865215"
  },
  {
    "text": "it's not going to be covered on the midterm, um, but feel free to reach out to us if you have any questions, and you can bring a one-sided one page of notes that's",
    "start": "4865215",
    "end": "4871830"
  },
  {
    "text": "handwritten or typed. Okay. Good luck.",
    "start": "4871830",
    "end": "4875920"
  }
]