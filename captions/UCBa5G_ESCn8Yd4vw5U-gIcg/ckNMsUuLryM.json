[
  {
    "start": "0",
    "end": "6130"
  },
  {
    "text": "We're glad to have Angela\nFan today with us here. And she's a research scientist\nat Meta AI Research in New York,",
    "start": "6130",
    "end": "14890"
  },
  {
    "text": "focusing on research in\ntext generation mainly. And currently, she's\nworking on language modeling",
    "start": "14890",
    "end": "20500"
  },
  {
    "text": "and developing the line\nAI agents meta products. And recent research products\ninclude No Language Left",
    "start": "20500",
    "end": "27520"
  },
  {
    "text": "Behind, which we'll be\ntalking briefly about today, universal speech translation\nfor unwritten languages",
    "start": "27520",
    "end": "33340"
  },
  {
    "text": "as well as Llama2. So give it up for\nAngela, I guess.",
    "start": "33340",
    "end": "39519"
  },
  {
    "text": "All right, thank\nyou all so much. So yeah, when I got this\nemail, I was like, oh, I should probably\ntalk about lama2,",
    "start": "39520",
    "end": "44740"
  },
  {
    "text": "but then I noticed\nyou have Sharon, who is like a 10x\nbetter speaker than me. So I was like, OK,\nmaybe not Llama2.",
    "start": "44740",
    "end": "51670"
  },
  {
    "text": "But then I thought I maybe would\ncover this project that we did called No Language Left\nBehind, which could be also",
    "start": "51670",
    "end": "57820"
  },
  {
    "text": "very relevant to this class. And so when you think about\na lot of text generation",
    "start": "57820",
    "end": "63100"
  },
  {
    "text": "technology, most of it,\nuntil fairly recently, has been really\nfocused on English.",
    "start": "63100",
    "end": "69950"
  },
  {
    "text": "But there are actually more\nthan 3,000 written languages worldwide. And for me, this is extremely\npersonally meaningful",
    "start": "69950",
    "end": "76729"
  },
  {
    "text": "because, actually, English\nis my third language. So it's really important--",
    "start": "76730",
    "end": "82770"
  },
  {
    "text": "yeah. So it's really also very\npersonally meaningful. And when you think about some of\nthe multilingual technology that",
    "start": "82770",
    "end": "90110"
  },
  {
    "text": "permeates, it's not like we've\nnever worked on multilingual. Actually, when speaking\nabout generative AI,",
    "start": "90110",
    "end": "95900"
  },
  {
    "text": "I actually think\ntranslation is one of the most commercially\nsuccessful and widespread applications of generative AI.",
    "start": "95900",
    "end": "102020"
  },
  {
    "text": "I mean, ultimately\ntranslation models. They are like conditional\nlanguage models. And so when you think about\ntraveling or something",
    "start": "102020",
    "end": "110060"
  },
  {
    "text": "like that, or my sister\nis taking Spanish, so just like doing her\nSpanish homework, we have a lot of tools\nthat exist today.",
    "start": "110060",
    "end": "116479"
  },
  {
    "text": "So things like Google Translate\ncover around 130 languages, Microsoft Translate about 110.",
    "start": "116480",
    "end": "121970"
  },
  {
    "text": "This might be a\nlittle bit outdated, since I pulled the\nstatistics a little bit ago.",
    "start": "121970",
    "end": "127160"
  },
  {
    "text": "But the project for\nNo Language Left Behind, it started\nfrom a very simple ask. OK, there's 3,000\nlanguages worldwide.",
    "start": "127160",
    "end": "133790"
  },
  {
    "text": "Maybe it'll be like pretty\nhard to get to all 3,000, since some of them are pretty\nrare, and not spoken by many.",
    "start": "133790",
    "end": "141530"
  },
  {
    "text": "But there are still hundreds\nof languages spoken by millions and millions of people.",
    "start": "141530",
    "end": "146730"
  },
  {
    "text": "And so we were like,\nOK, no big deal. Let's just start from the\n100-ish that we have today",
    "start": "146730",
    "end": "152280"
  },
  {
    "text": "and just go for like a doubling. What would it take\nto actually be able to double this\nkind of coverage?",
    "start": "152280",
    "end": "158099"
  },
  {
    "text": "And, of course, just saying that\nyou support a bunch of languages is not the goal. We actually want to create\nhigh-quality safe translations",
    "start": "158100",
    "end": "165600"
  },
  {
    "text": "that would be usable by people. Just like if you're\ngoing on vacation today, you're instinct is to\nwhip out your phone",
    "start": "165600",
    "end": "171420"
  },
  {
    "text": "and get on the\nGoogle Translate app. And so the backdrop\nto this project",
    "start": "171420",
    "end": "177110"
  },
  {
    "text": "was that there was actually a\nlot of progress in translation. So historically, there's\nbeen a lot of focus",
    "start": "177110",
    "end": "182180"
  },
  {
    "text": "on what we call higher\nresource languages, and these are not\nnecessarily languages that are spoken by the\nmost people in the world.",
    "start": "182180",
    "end": "188300"
  },
  {
    "text": "But when we say\nhigher resource, it means the most amount of data. And so you can\nthink about things like Euro Parl or translations\nfrom the European parliament.",
    "start": "188300",
    "end": "197450"
  },
  {
    "text": "And those served\nas the foundation for a lot of\ntranslation development. And more recently,\nthere's been a great focus",
    "start": "197450",
    "end": "203990"
  },
  {
    "text": "on low-resource languages. And it's been driven across\nthe research community with groups like Ghana NLP,\nMasakhane, America's NLP.",
    "start": "203990",
    "end": "211970"
  },
  {
    "text": "And these are all really\nexciting developments. And so these have led to a\nlot of development of new data",
    "start": "211970",
    "end": "217489"
  },
  {
    "text": "sets as well as criticisms\nof existing data sets, and also work\non new languages,",
    "start": "217490",
    "end": "223430"
  },
  {
    "text": "and usually, languages\nthat people speak and they care a lot about. And we found this\nreally, really exciting.",
    "start": "223430",
    "end": "229459"
  },
  {
    "text": "And so looking at a lot\nof this, a bunch of us got together at fair and\nstarted thinking like,",
    "start": "229460",
    "end": "234629"
  },
  {
    "text": "OK, we actually speak some\npretty low-resource languages from Catalan to\nAssamese and so on.",
    "start": "234630",
    "end": "240930"
  },
  {
    "text": "And so we started this as\nkind of like a big passionate research project. And so today, I want\nto cover a little bit",
    "start": "240930",
    "end": "246630"
  },
  {
    "text": "about our high-level approach\nto this problem, which is a little bit interdisciplinary. I want to talk about\nhow we actually",
    "start": "246630",
    "end": "253200"
  },
  {
    "text": "created the data sets to be able\nto support this kind of work. Of course, I want to\ntalk about the models,",
    "start": "253200",
    "end": "259380"
  },
  {
    "text": "since this is a class\nabout transformers. One note here, I\nthink that's actually very interesting in\nterms of translation",
    "start": "259380",
    "end": "264870"
  },
  {
    "text": "as a research direction is that\nactually, a lot of innovations have been done in translation.",
    "start": "264870",
    "end": "271237"
  },
  {
    "text": "The original transformer\npaper, I think, is one of them, and which makes always\ntranslation a quite interesting",
    "start": "271237",
    "end": "276480"
  },
  {
    "text": "area to work on because I feel\nlike it's a very mature research area as well. So it is like, OK,\nif your architecture",
    "start": "276480",
    "end": "282990"
  },
  {
    "text": "works in translation, it\nprobably works very generally. So that's also one of the\nthings that excites me about translation research.",
    "start": "282990",
    "end": "289668"
  },
  {
    "text": "Then I want to talk\nabout evaluation, like how are we actually\nmeasuring and ensuring the quality of\nthese translations",
    "start": "289668",
    "end": "294780"
  },
  {
    "text": "are good and safe for people. And then I want to\nend with a little bit of high-level thoughts about\nfuture directions and things",
    "start": "294780",
    "end": "302550"
  },
  {
    "text": "that I hope that we can\nwork on in the future. So I want to start\nwith our approach.",
    "start": "302550",
    "end": "307980"
  },
  {
    "text": "I think the most important\nthing in research is to know that we're\nworking on a real problem, especially when it's\nreally close to people,",
    "start": "307980",
    "end": "315240"
  },
  {
    "text": "like translation. And I think in many\nareas, like when I was working on\nOn-Device AI, for example,",
    "start": "315240",
    "end": "320460"
  },
  {
    "text": "I feel like I had like a\nresearch problem in mind, but it was very,\nvery disconnected from the practical problem\nof actually putting models",
    "start": "320460",
    "end": "327389"
  },
  {
    "text": "on phones. And so this was something that\nwas really important to us. And so we actually\nstarted the project",
    "start": "327390",
    "end": "332730"
  },
  {
    "text": "by focusing on a\nsocial-sciences-type approach or sociology-type approach.",
    "start": "332730",
    "end": "339000"
  },
  {
    "text": "And we actually did\na lot of interviews with low-resource speakers. And so we met with about 44\ndifferent native speakers",
    "start": "339000",
    "end": "346320"
  },
  {
    "text": "that spoke 36 different\nlanguages across North America. I will say that a lot of them\nare immigrants to the US,",
    "start": "346320",
    "end": "353250"
  },
  {
    "text": "since that was the easiest\nkind of cohort to recruit. And we learned a lot\nof different things",
    "start": "353250",
    "end": "359090"
  },
  {
    "text": "about how they approach\nlow-resource languages, but also the technological\nneed that they have.",
    "start": "359090",
    "end": "364300"
  },
  {
    "text": "Because I think it's\neasy to be like, hey, I have this cool background. I have this cool problem,\nand I want to solve it.",
    "start": "364300",
    "end": "370009"
  },
  {
    "text": "But I think it's very important\nto actually talk to the people. This is a problem that\nneeds to be solved. And so we learned that there's\ngreat fear, in general,",
    "start": "370010",
    "end": "377960"
  },
  {
    "text": "that low-resource languages\nmight be undergoing a state of decline, partially\nbecause a lot of education",
    "start": "377960",
    "end": "383569"
  },
  {
    "text": "is shifting to languages\nlike Hindi or English or Mandarin Chinese,\nfor example.",
    "start": "383570",
    "end": "390410"
  },
  {
    "text": "And there's a lot of excitement\nto be included in existing translation systems. And people said,\nthey have always",
    "start": "390410",
    "end": "395840"
  },
  {
    "text": "tried to use Google Translate\nor Microsoft Translate in their existing languages.",
    "start": "395840",
    "end": "400880"
  },
  {
    "text": "But ultimately, they\nfound that the quality is really insufficient\nfor reliable usage. So if you think about--",
    "start": "400880",
    "end": "406555"
  },
  {
    "text": "well, I was going to say\nwhen I was in high school, but you all are probably like\nsubstantially younger than me. So maybe 10 or so\nyears ago and you",
    "start": "406555",
    "end": "413600"
  },
  {
    "text": "try to use Google Translate\nfor your Spanish homework, your Spanish\nteacher could always identify that it was not a\nhuman-written translation.",
    "start": "413600",
    "end": "420522"
  },
  {
    "text": "And so you would get marks\noff, but that's not really the case for some of the\nhigh-resource languages today.",
    "start": "420522",
    "end": "425790"
  },
  {
    "text": "And so I think, as with all\nthings in machine learning, it really starts from\na data perspective. Why can't we just train\nmodels in hundreds",
    "start": "425790",
    "end": "432600"
  },
  {
    "text": "of languages or large language\nmodels in hundreds of languages? It's because we don't have\nthe data to support it.",
    "start": "432600",
    "end": "437650"
  },
  {
    "text": "And so I want to talk\nfirst about evaluation data sets because I think\nit's extremely important to nail evaluation.",
    "start": "437650",
    "end": "444570"
  },
  {
    "text": "And then I'll talk\nabout training. So for an evaluation data\nset for this work we started,",
    "start": "444570",
    "end": "451260"
  },
  {
    "text": "this FLORES effort, it stands\nfor Facebook Low Resource. I guess we're called\nMeta now, but I didn't think FLORES was\na very good renaming.",
    "start": "451260",
    "end": "459210"
  },
  {
    "text": "So we'll still\ncalling it FLORES. So this was something\nwe originally started for just two languages\nin this first paper, EMNLP,",
    "start": "459210",
    "end": "467340"
  },
  {
    "text": "many years ago. So it was just for\nNepali and Sinhala, and we later extended it\nto incorporate two more",
    "start": "467340",
    "end": "472710"
  },
  {
    "text": "languages in a release. Afterwards, we thought\na lot about OK, FLORES was really useful\nfor the community.",
    "start": "472710",
    "end": "480130"
  },
  {
    "text": "How can we extend\nit to 100 languages? And so that was his\nfollow-up work that we did. I think we had an ACL or WMT.",
    "start": "480130",
    "end": "487720"
  },
  {
    "text": "And then in this project,\nwe were like, OK, how can we go from FLORES-101\nto FLORES-200 to really go",
    "start": "487720",
    "end": "494710"
  },
  {
    "text": "for the doubling effect? And so what is FLORES? Well, it's in the name. It's a focus on\nlow-resource languages.",
    "start": "494710",
    "end": "501220"
  },
  {
    "text": "So we do include some\nhigher resource languages like German or Hindi or so on,\nalmost for calibration effect",
    "start": "501220",
    "end": "508030"
  },
  {
    "text": "as well. But the majority of the\nfocus is on these lower and mid-resource languages. It's the first large-scale,\nmany-to-many machine translation",
    "start": "508030",
    "end": "517450"
  },
  {
    "text": "evaluation data set, which\nmeans that we take all of the sentences in English. And then we translate them to\nall of the languages, which",
    "start": "517450",
    "end": "523929"
  },
  {
    "text": "means that you would\nbe able to evaluate any cross pair of languages. So for example, like\nChinese to French.",
    "start": "523929",
    "end": "530290"
  },
  {
    "text": "I lived in France\nfor many years, so it's very personally\nrelevant to me. Of course, 200 languages,\nalso in the name.",
    "start": "530290",
    "end": "536480"
  },
  {
    "text": "There's a broad diversity of\ndifferent domains and topics. I think this is important when\ndesigning an evaluation data",
    "start": "536480",
    "end": "542830"
  },
  {
    "text": "set, which is very top-of-mind\nfor anybody interested in language modeling research,\nbecause the way people train",
    "start": "542830",
    "end": "550580"
  },
  {
    "text": "machine translation\nmodels and the way people use them are often\nvery different. And so if you only benchmark\nyour data set, for example,",
    "start": "550580",
    "end": "557990"
  },
  {
    "text": "on news, which is very common\nin translation research, then you don't really\npick up the fact that people talk about such\na wide variety of things",
    "start": "557990",
    "end": "565380"
  },
  {
    "text": "and have different\ncasual conversations that they need translated,\nofficial documents, and so on.",
    "start": "565380",
    "end": "570710"
  },
  {
    "text": "It's also\ndocument-level data set. This is not something that I\nthink the community is broadly leveraging right now, but\nthe way it's translated",
    "start": "570710",
    "end": "578090"
  },
  {
    "text": "is that you can have\ndocument-level context. And so translators are\nprovided the entire document",
    "start": "578090",
    "end": "583339"
  },
  {
    "text": "to translate from. And we also provide the entire\ndocument for evaluation. And we translate\nmultiple sentences",
    "start": "583340",
    "end": "589400"
  },
  {
    "text": "from the same paragraph. And so this was like a\npotential research direction that we wanted to\nmake sure we covered,",
    "start": "589400",
    "end": "594890"
  },
  {
    "text": "models that needed\npotentially more context, because a lot of\ntranslation work is done at the sentence level.",
    "start": "594890",
    "end": "600710"
  },
  {
    "text": "So how do we actually\nensure that this data set was high-quality? So the first step is\nthat we take a document.",
    "start": "600710",
    "end": "607370"
  },
  {
    "text": "Well, actually, first step is\nalignment on language standards. So this is very\nimportant because when you're translating\nFrench or Chinese,",
    "start": "607370",
    "end": "614690"
  },
  {
    "text": "I think most people have a\nstrong understanding of what it means to produce a good\nFrench or good Chinese.",
    "start": "614690",
    "end": "620540"
  },
  {
    "text": "And there are a lot of\nprofessional translators hired in these languages. But when you go to lower\nresource languages,",
    "start": "620540",
    "end": "626420"
  },
  {
    "text": "it's not necessarily\nthe case that there's like a glowing\ntranslation industry around translating a\nlower resource language.",
    "start": "626420",
    "end": "635240"
  },
  {
    "text": "And so one of the first\nthings is actually to align on like, what is\na high-quality translation? And so there's actually\na lot of challenges here.",
    "start": "635240",
    "end": "642407"
  },
  {
    "text": "So there are certain\nlow-resource languages, where there's different\ncompeting language standards, or there's very high\nvariance in different regions",
    "start": "642407",
    "end": "649820"
  },
  {
    "text": "on how languages are spoken. And so this step is a\npretty critical one. So then what we do is\nwe take the document.",
    "start": "649820",
    "end": "656180"
  },
  {
    "text": "We send it to one\ngroup of translators, and they do the first\ntranslation step. Then we do some\nautomatic checking.",
    "start": "656180",
    "end": "663110"
  },
  {
    "text": "If the input\nsentence was 10 words and the output\nsentence is 300 words, it's like, most likely\nsomething went wrong.",
    "start": "663110",
    "end": "669860"
  },
  {
    "text": "And so we send that back. Otherwise, we'll send it onwards\nto a separate, completely",
    "start": "669860",
    "end": "675290"
  },
  {
    "text": "independent set of\ntranslators that do review. And so they try to rate\nthe quality of this.",
    "start": "675290",
    "end": "681290"
  },
  {
    "text": "And if the quality doesn't\npass a sufficient bar, it gets sent back to the\noriginal set of translators",
    "start": "681290",
    "end": "686329"
  },
  {
    "text": "to edit. And they go through and\naddress all of the feedback. And then if it's good enough,\nthen it enters our data set.",
    "start": "686330",
    "end": "694430"
  },
  {
    "text": "And so there's many\nchallenges here. The first one, of\ncourse, is just finding translators, and also\nfinding more translators.",
    "start": "694430",
    "end": "701360"
  },
  {
    "text": "There was a certain issue\nthat we ran into, for example, that in a certain country, the\ninternet was not available.",
    "start": "701360",
    "end": "707930"
  },
  {
    "text": "And so it's a lot\nof recruitment. The other one, of course,\nis language standardization.",
    "start": "707930",
    "end": "714199"
  },
  {
    "text": "I think I briefly\nmentioned this before, but there's a lot of different\nchallenges in just understanding",
    "start": "714200",
    "end": "720260"
  },
  {
    "text": "like, what is a\nhigh-quality translation? For example, the\nlow-resource language Breton. There's two competing groups\non how do you write Breton.",
    "start": "720260",
    "end": "728360"
  },
  {
    "text": "So it's very difficult to\nresolve some of those things. And the final thing\nis that there's actually a lot of variation,\neven in languages like Arabic.",
    "start": "728360",
    "end": "737480"
  },
  {
    "text": "Moroccan Arabic is very\ndifferent from Jordanian Arabic and so on.",
    "start": "737480",
    "end": "742550"
  },
  {
    "text": "And there are also\ncertain regions that they speak\nthe same language. But due to historical reasons,\nthey write in different scripts.",
    "start": "742550",
    "end": "749150"
  },
  {
    "text": "And so one of the\nthings we actually did was like, if there are languages\nwritten in multiple scripts, we actually supported the\ncollection of a multiple script",
    "start": "749150",
    "end": "756740"
  },
  {
    "text": "evaluation. And I think this is really\nimportant because if you're building an\nunderlying technology",
    "start": "756740",
    "end": "761930"
  },
  {
    "text": "and you only choose\none, then I think you risk just naturally\nsupporting one over the other,",
    "start": "761930",
    "end": "768080"
  },
  {
    "text": "when we really should be a more\nneutral technology provider. And so this is something\nthat we export a lot",
    "start": "768080",
    "end": "774029"
  },
  {
    "text": "as well as exploring\ndifferent variants of Arabic. This is also open-source. If you just go to this\nlink, you can just",
    "start": "774030",
    "end": "780148"
  },
  {
    "text": "download all of the\ntext files for this. With evaluation done, I\nwant to talk a little bit",
    "start": "780148",
    "end": "785970"
  },
  {
    "text": "about how we collected some\nof these training data sets. The first thing I\nwant to talk about is this data set we\ncreated called NLLB-Seed,",
    "start": "785970",
    "end": "793650"
  },
  {
    "text": "and the idea of this is,\nit's a really seed data set of high-quality\ntranslations and languages",
    "start": "793650",
    "end": "799590"
  },
  {
    "text": "that really don't have anything. Why? Because, well, you can't\nstart from nothing. You got to bootstrap\nfrom somewhere.",
    "start": "799590",
    "end": "806070"
  },
  {
    "text": "A lot of people have been using\nthe Bible as a way to bootstrap, but it's very limited domain,\nobviously, very religious text.",
    "start": "806070",
    "end": "814140"
  },
  {
    "text": "And so we created this\ndata set, NLLB-Seed, for languages that really\ndon't have anything",
    "start": "814140",
    "end": "820470"
  },
  {
    "text": "to get started from. It's only about 5,000 sentences,\nso it's nothing crazy. But it supports a lot\nof different use cases,",
    "start": "820470",
    "end": "826500"
  },
  {
    "text": "like training language\nidentification models or sentence encoders,\nn-gram language models,",
    "start": "826500",
    "end": "831760"
  },
  {
    "text": "all of these things that\nI'm about to talk about in our data set pipeline. So it covers 43 languages,\nabout 6,000 sentences.",
    "start": "831760",
    "end": "839740"
  },
  {
    "text": "And the way we decide\nto sample it is focused on really general content. So Wikipedia has this\narticle of like, hey,",
    "start": "839740",
    "end": "846310"
  },
  {
    "text": "if you're going to start\nlike a new Wikipedia in your new language-- I think Wikipedia has 309-ish,\nWikipedia's last I checked.",
    "start": "846310",
    "end": "853750"
  },
  {
    "text": "Here's a list of articles\nthat every Wikipedia in a new language should have. And so that's where we sampled\nthis original content from.",
    "start": "853750",
    "end": "860529"
  },
  {
    "text": "And, of course, it's\nalso open-source if you want to download it. So what we ended up doing to\nget large-scale training data",
    "start": "860530",
    "end": "868660"
  },
  {
    "text": "is using mining. So this is not something we\npioneered in this project. We have a bunch of\ndifferent previous work.",
    "start": "868660",
    "end": "875560"
  },
  {
    "text": "So we started from WikiMatrix. We were like, hey, there's\na lot of different sentences",
    "start": "875560",
    "end": "880630"
  },
  {
    "text": "in Wikipedia on\ndifferent languages that we should be\nable to match up. And so we try to do\nthat with Wikipedia",
    "start": "880630",
    "end": "886240"
  },
  {
    "text": "to get machine\ntranslation training data. We extended that to the web\nin the CCMatrix project.",
    "start": "886240",
    "end": "892360"
  },
  {
    "text": "And then we extended it to\nvery, very large scale mining on all cross pairs\nin this project",
    "start": "892360",
    "end": "897430"
  },
  {
    "text": "on beyond English-centric\nmultilingual machine translation. We really tried to ditch English\nas a central pivot language.",
    "start": "897430",
    "end": "904120"
  },
  {
    "text": "And so the way this\nwhole data mining thing works is that it\nfocuses on sentence alignment.",
    "start": "904120",
    "end": "909490"
  },
  {
    "text": "So everyone is probably\nsuper familiar with this because this is how language\nmodels are built now. It's like, you take common crawl\nor any other open-source dump",
    "start": "909490",
    "end": "917019"
  },
  {
    "text": "of the web-- I don't know-- like\nRedPajama, or whatever you want, CCNet, whatever\nyou want to use these days.",
    "start": "917020",
    "end": "922149"
  },
  {
    "text": "And you take all of the data. You extract all of the text. A lot of HTML parsing,\nand so on, goes into it.",
    "start": "922150",
    "end": "928630"
  },
  {
    "text": "And the idea is that we want to\ntry to find matching text that could be a translation. So we shatter it\nall into sentences.",
    "start": "928630",
    "end": "935170"
  },
  {
    "text": "We embed them with different\nsentence encoder models. And then we do a match\nto try to understand",
    "start": "935170",
    "end": "940600"
  },
  {
    "text": "in a multilingual space\nif the sentences match. And so one of the biggest\nchallenges to this",
    "start": "940600",
    "end": "947050"
  },
  {
    "text": "is that the quality of\nthe sentence encoding is very important. So if your sentence encoding\nis not very accurate,",
    "start": "947050",
    "end": "952720"
  },
  {
    "text": "then it's impossible to match\nin this dimensional space the idea of the\nmeaning being the same.",
    "start": "952720",
    "end": "958430"
  },
  {
    "text": "And so one of the big things we\ntry to do here in this project was try to improve the quality\nof the sentence encoders.",
    "start": "958430",
    "end": "965259"
  },
  {
    "text": "And so one of the big\nthings that we did was train sentence encoders\nwith masked language modeling.",
    "start": "965260",
    "end": "970270"
  },
  {
    "text": "You see that on the left. But we also use\nmultilingual distillation, which you see on the right.",
    "start": "970270",
    "end": "976330"
  },
  {
    "text": "And so previous approaches\nto sentence encoders and the trend in the research\ncommunity for a while",
    "start": "976330",
    "end": "982150"
  },
  {
    "text": "was to really try to embed all\nlanguages in the same sentence encoder model. So projects like XLM-R, for\nexample, are in that direction,",
    "start": "982150",
    "end": "990940"
  },
  {
    "text": "I think, is pretty widely used. The challenge with\nthis when you're training a low-resource\nmodel is that a lot",
    "start": "990940",
    "end": "997090"
  },
  {
    "text": "of your high-resource\ndata just overwhelms your low-resource data. And so you don't end up with\na very high-quality sentence",
    "start": "997090",
    "end": "1004620"
  },
  {
    "text": "encoder for those languages. So what we ended up doing is\nwe had a multilingual teacher model, and we distilled\na bunch of student models",
    "start": "1004620",
    "end": "1012389"
  },
  {
    "text": "that are specialized to\ndifferent language families that are low resource.",
    "start": "1012390",
    "end": "1017520"
  },
  {
    "text": "And so this enables the\nquality to be pretty high. So the way that\ndistillation works is that the teacher and\nthe student model both",
    "start": "1017520",
    "end": "1024510"
  },
  {
    "text": "see the same data. And then we try to minimize the\ncosine loss between the sentence embeddings that they produce.",
    "start": "1024510",
    "end": "1032285"
  },
  {
    "text": "I think an important question\nthat you can ask here is like, why do you need to\ndo multilingual distillation?",
    "start": "1032285",
    "end": "1037680"
  },
  {
    "text": "Why can't you just train a bunch\nof different student models, like one per language family?",
    "start": "1037680",
    "end": "1042959"
  },
  {
    "text": "Why even care\nabout distillation? And the reason is\nbecause if you're going to use a bunch of\nsentence encoders for mining,",
    "start": "1042960",
    "end": "1049740"
  },
  {
    "text": "the important thing is that they\nall exist in the same embedding space. If you train one separate model\nand another separate model,",
    "start": "1049740",
    "end": "1056730"
  },
  {
    "text": "there's nothing\nconstraining them so that you can mine all of\nthe data against each other.",
    "start": "1056730",
    "end": "1061830"
  },
  {
    "text": "And so one of the\nthings we found is that by starting everything\nfrom the same teacher model and trying to use this\ncosine loss to minimize",
    "start": "1061830",
    "end": "1068940"
  },
  {
    "text": "the distance between\nembeddings, you are able to have this\nconstrained space where you can mine every language\nagainst every other,",
    "start": "1068940",
    "end": "1076020"
  },
  {
    "text": "even if you have\ndifferent student models. And so this graph\non the y-axis, it",
    "start": "1076020",
    "end": "1081860"
  },
  {
    "text": "shows the error rate of mining. And so lower is better. And on the x-axis,\nit shows a bunch",
    "start": "1081860",
    "end": "1088460"
  },
  {
    "text": "of different\nlow-resource languages. So for example, the\nfirst one is Urdu. The second one is Telugu.",
    "start": "1088460",
    "end": "1094039"
  },
  {
    "text": "Third one is Tagalog, and so on. And so the gray bar here is\nthe original laser paper.",
    "start": "1094040",
    "end": "1099560"
  },
  {
    "text": "So this is a paper we put\nout maybe in 2018-ish, and we had all of\nthese languages. We counted them as included.",
    "start": "1099560",
    "end": "1105500"
  },
  {
    "text": "But as you can\nsee, the error rate is extremely, extremely\nhigh for these languages. So even though\nthey were included,",
    "start": "1105500",
    "end": "1112010"
  },
  {
    "text": "couldn't really be\nused for high quality. And the blue bar\nis the laser model that we trained based on the\ntechnique I just described",
    "start": "1112010",
    "end": "1119480"
  },
  {
    "text": "in the previous slide. And you can see that-- I think the most important\npoint is that you can barely see the blue bars. So it was very effective, even\nfor these previous languages",
    "start": "1119480",
    "end": "1127250"
  },
  {
    "text": "that people had thought we\nhad previously embedded. And then so now, how\ndoes this kind of thing",
    "start": "1127250",
    "end": "1133460"
  },
  {
    "text": "fit into a whole data\npipeline around this approach? So one of the most\nimportant things",
    "start": "1133460",
    "end": "1138720"
  },
  {
    "text": "is when you download\nthe data from the web, you don't really know\nwhat language it's in.",
    "start": "1138720",
    "end": "1144120"
  },
  {
    "text": "And so this is part of all\nof the large-scale data cleaning that goes into training\nlarge language models today.",
    "start": "1144120",
    "end": "1150299"
  },
  {
    "text": "And so the way we identify\ndifferent languages is, is through simple\nclassification models",
    "start": "1150300",
    "end": "1155580"
  },
  {
    "text": "called Language\nIdentification Models. And I think it's a\nclassification model.",
    "start": "1155580",
    "end": "1160710"
  },
  {
    "text": "And so people think it's\neasier than it actually is. But I think some of\nthe major challenges",
    "start": "1160710",
    "end": "1166410"
  },
  {
    "text": "are that there are so\nmany different languages. They're written in\nmany different ways, and web text is very casual.",
    "start": "1166410",
    "end": "1173760"
  },
  {
    "text": "And so it can be very\ndifficult to actually train a good classification model\nthat can generalize to that.",
    "start": "1173760",
    "end": "1178799"
  },
  {
    "text": "And so what we did is we\nhad our LID training data. And we produced a Language\nIdentification Model, LID.",
    "start": "1178800",
    "end": "1187200"
  },
  {
    "text": "And then we actually did human\nevaluation to label errors coming from the LID system\nto iteratively improve",
    "start": "1187200",
    "end": "1193889"
  },
  {
    "text": "this on web text itself,\nto improve the quality of the specific model. Then after we produce\nthis LID model,",
    "start": "1193890",
    "end": "1200580"
  },
  {
    "text": "then we insert all\nof our Common Crawl where the web\narrow is coming in. And we do a ton of\nfiltering and cleaning.",
    "start": "1200580",
    "end": "1206520"
  },
  {
    "text": "And this produces a huge corpus\nof different monolingual data that you can then use\nfor training anything.",
    "start": "1206520",
    "end": "1213780"
  },
  {
    "text": "Afterwards, we\ntrain our encoder, what I described on\nthe previous text. And then we convert\nthis monolingual data",
    "start": "1213780",
    "end": "1220290"
  },
  {
    "text": "into what we call Mined Bitexts. So these are a huge data\nset of things that we think are translations of each other.",
    "start": "1220290",
    "end": "1227580"
  },
  {
    "text": "And then, finally, what\nwe do is we actually try to validate that these are\nreal Mined Bitexts by training",
    "start": "1227580",
    "end": "1233640"
  },
  {
    "text": "very small bilingual translation\nmodels in order to see",
    "start": "1233640",
    "end": "1239550"
  },
  {
    "text": "what the quality is like. And I think this is important\nbecause the data development cycle and the end task that\nit's being used for, you don't",
    "start": "1239550",
    "end": "1248190"
  },
  {
    "text": "want to completely separate it. An analogy to large\nlanguage model training today is that when you're\ndoing your pre-training,",
    "start": "1248190",
    "end": "1256080"
  },
  {
    "text": "you don't want someone to\njust deliver you a data. The data mix of your different\ndata sets is very important.",
    "start": "1256080",
    "end": "1261660"
  },
  {
    "text": "And it's pretty similar here. And I think one\nof the highlights that we did here\nis really focused",
    "start": "1261660",
    "end": "1268420"
  },
  {
    "text": "on the human evaluation of the\nlanguage identification model because that actually\nimproves the quality of all",
    "start": "1268420",
    "end": "1274270"
  },
  {
    "text": "of the underlying data if\nyou just more accurately know what language it's in.",
    "start": "1274270",
    "end": "1279320"
  },
  {
    "text": "And this entire data\npipeline is actually open-source in this library. And we had an NLP\npaper describing it.",
    "start": "1279320",
    "end": "1285178"
  },
  {
    "text": "The reason why I thought\nthis was important is that because I think\ndata cleaning is actually such a fundamental\nunderlying thing that",
    "start": "1285178",
    "end": "1291590"
  },
  {
    "text": "drives model quality and\npeople's data pipelines. It's like I have this\nscript and this other thing, and this other thing.",
    "start": "1291590",
    "end": "1297422"
  },
  {
    "text": "And so it's actually,\nI think, very important to be able to\nrecreate it and rerun it as part of almost\nyour research that you",
    "start": "1297422",
    "end": "1305390"
  },
  {
    "text": "would do as follow-up work. And so this is why\nwe open-sourced it.",
    "start": "1305390",
    "end": "1310460"
  },
  {
    "text": "A few reflection things. For low-resource\nlanguages, even though we",
    "start": "1310460",
    "end": "1315480"
  },
  {
    "text": "did a large-scale mining,\nI think monolingual data is the limiting factor. There are many\nlanguages that do not",
    "start": "1315480",
    "end": "1321420"
  },
  {
    "text": "have a huge amount of\ntext written online. And so it can be very\nchallenging to get",
    "start": "1321420",
    "end": "1326820"
  },
  {
    "text": "a large amount. Further, I think languages\nand unique scripts can be extremely hard to get\ngood representations of if you",
    "start": "1326820",
    "end": "1334440"
  },
  {
    "text": "don't have very much data. There are certain languages\nas well where they were historically\nwritten in a new script,",
    "start": "1334440",
    "end": "1340230"
  },
  {
    "text": "but now the\ngovernment would like to write it in a totally new\none, like the Ol Chiki script,",
    "start": "1340230",
    "end": "1345690"
  },
  {
    "text": "for example. And so there's not\na lot of content to represent these\nscripts, so it's hard to learn representations.",
    "start": "1345690",
    "end": "1352200"
  },
  {
    "text": "And then further, a\nlot of the content we create, even after mining,\nit's a fairly limited domain,",
    "start": "1352200",
    "end": "1358649"
  },
  {
    "text": "often religious content. So with data discussed, I\nwant to segue a little bit",
    "start": "1358650",
    "end": "1365760"
  },
  {
    "text": "into some of the modeling\nwork, just to start with a high-level picture.",
    "start": "1365760",
    "end": "1371700"
  },
  {
    "text": "I think there's three\nmajor challenges when you talk about large-scale\nmultilingual modeling,",
    "start": "1371700",
    "end": "1376840"
  },
  {
    "text": "and these pretty much apply\nto language models as well. The first one is effective\ndata augmentation",
    "start": "1376840",
    "end": "1384080"
  },
  {
    "text": "for low-resource languages. How can you prevent the\nlow-resource language data from just being\ncompletely drowned out",
    "start": "1384080",
    "end": "1390980"
  },
  {
    "text": "by the time you've seen all of\nyour words of German or Russian? I think there's also a question\nof scalability of the model.",
    "start": "1390980",
    "end": "1398120"
  },
  {
    "text": "So even if you train\nvery large-scale models, how do you prevent\nthe representations of different languages from\ninterfering with each other?",
    "start": "1398120",
    "end": "1405470"
  },
  {
    "text": "And that leads to the last\npoint as well of like, if you give the model\nvery limited capacity,",
    "start": "1405470",
    "end": "1410570"
  },
  {
    "text": "then, of course, it may\nnot have the capacity to model all of these\ndifferent languages. And so you also\nneed to accelerate",
    "start": "1410570",
    "end": "1416900"
  },
  {
    "text": "the scale of the model. And so preliminary for those who\nmay not have seen a translation",
    "start": "1416900",
    "end": "1424975"
  },
  {
    "text": "system before-- I don't know how many of\nyou that practically is-- so we use standard\nsequence-to-sequence models.",
    "start": "1424975",
    "end": "1430490"
  },
  {
    "text": "So the input text,\nthe choral thing, is what you want to translate,\nenters a transformer decoder",
    "start": "1430490",
    "end": "1435529"
  },
  {
    "text": "model. That, then, with the\nattention mechanism, goes to a transformer\ndecoder model. And then it decodes\nautoregressively",
    "start": "1435530",
    "end": "1442910"
  },
  {
    "text": "the actual translation, which\nyou can see here in yellow. And so I want to talk a little\nbit about how the data looks",
    "start": "1442910",
    "end": "1450890"
  },
  {
    "text": "as we feed it into the models. So there's a few\ndifferent ways that you might want to think about data. So you want to be like,\nOK, did a human look at it",
    "start": "1450890",
    "end": "1458360"
  },
  {
    "text": "and decide that these two\nsentences are translations? Or are they noisy? Also, is it limited in size?",
    "start": "1458360",
    "end": "1464965"
  },
  {
    "text": "Another thing you\ncan think about is, is the data quality\ndependent on some other factor? And so that's the\nmodel-dependent thing.",
    "start": "1464965",
    "end": "1471200"
  },
  {
    "text": "In which case, the\ndata quality may be capped by the quality\nof that dependency.",
    "start": "1471200",
    "end": "1476600"
  },
  {
    "text": "And so I think you can think a\nlittle bit the ideal data set. It would be like, humans have\nreviewed every bit of it.",
    "start": "1476600",
    "end": "1483440"
  },
  {
    "text": "It's not noisy at all. We have an infinite\namount, and it doesn't have any dependencies\non any other models.",
    "start": "1483440",
    "end": "1489409"
  },
  {
    "text": "It's just pure quality. But in reality, closer to\nwhat we have are these. So we have a bunch of\ndifferent data sources.",
    "start": "1489410",
    "end": "1496340"
  },
  {
    "text": "We have the seed data that I\ndiscussed way back in the talk, where it's a small amount\nof really high-quality,",
    "start": "1496340",
    "end": "1503059"
  },
  {
    "text": "human-aligned data,\nbut the only problem is that it's limited in size. It's 6,000 sentences\nper language.",
    "start": "1503060",
    "end": "1509490"
  },
  {
    "text": "We have the PublicBitext. So this is data that people\nhave created over many years of working in translation.",
    "start": "1509490",
    "end": "1515580"
  },
  {
    "text": "You can download it from the\nopus corpus, for example, mostly has not been\nreviewed by humans,",
    "start": "1515580",
    "end": "1522390"
  },
  {
    "text": "so pretty extremely noisy. And many languages, it's\njust coming from the Bible, so the size is quite limited.",
    "start": "1522390",
    "end": "1528540"
  },
  {
    "text": "You have our Mined data. So this is not\nhuman-aligned either,",
    "start": "1528540",
    "end": "1535019"
  },
  {
    "text": "but it does have a\nmodel dependency. It's dependent on the quality\nof the sentence encoders. And we have two other sources\nof data from back translation.",
    "start": "1535020",
    "end": "1543600"
  },
  {
    "text": "So the idea of back\ntranslation, it's a model augmentation\ntechnique heavily used in machine\ntranslation, where you",
    "start": "1543600",
    "end": "1549510"
  },
  {
    "text": "use a model to produce pseudo\ntranslations like silver data. And we use two\ndifferent techniques",
    "start": "1549510",
    "end": "1555990"
  },
  {
    "text": "to produce these back\ntranslations that also are dependent on\nthe underlying model used to make the translations.",
    "start": "1555990",
    "end": "1562350"
  },
  {
    "text": "So this is a picture of a\nhigh-level different data sources and how\nyou want to think about the quality and\nthe different axes.",
    "start": "1562350",
    "end": "1568805"
  },
  {
    "text": "And so if we put them all\ntogether, what do we get? So the y-axis here is the\nnumber of training pairs,",
    "start": "1568805",
    "end": "1574590"
  },
  {
    "text": "and the x-axis here is the\nlanguages sorted by resource. So you can see on\nthe left-hand side,",
    "start": "1574590",
    "end": "1580560"
  },
  {
    "text": "you have your low-resource\nlanguages like Wolof. And on your right-hand\nside, you've got your high-resource\nlanguages like French.",
    "start": "1580560",
    "end": "1587220"
  },
  {
    "text": "The peak is English, of course. And so if you just look at\nwhat's available publicly, this is a distribution you get.",
    "start": "1587220",
    "end": "1594000"
  },
  {
    "text": "And you'll see a huge, huge\nfall-off pretty quickly. And then if you add in the data\nthat we have created from mining",
    "start": "1594000",
    "end": "1602010"
  },
  {
    "text": "and back translation,\nour goal is basically to make the distribution\na little bit more uniform.",
    "start": "1602010",
    "end": "1607440"
  },
  {
    "text": "It's very hard on the\nextremely low resource side, of course, but to make\nit a little bit more uniform so",
    "start": "1607440",
    "end": "1613500"
  },
  {
    "text": "that you don't just immediately\noverfit on your low resource languages before\nyou've even seen three",
    "start": "1613500",
    "end": "1619260"
  },
  {
    "text": "shards of your German data. With that kind of\ndata strategy in mind,",
    "start": "1619260",
    "end": "1624840"
  },
  {
    "text": "I want to talk a little bit\nabout mixture of experts. So this is something that we\nexplored quite aggressively",
    "start": "1624840",
    "end": "1631080"
  },
  {
    "text": "in the translation space\nfor a number of years. We could have this\nequal conversation",
    "start": "1631080",
    "end": "1636223"
  },
  {
    "text": "about some of the\ndebates going on like, do you want sparse or\ndense, architectures for large language models?",
    "start": "1636223",
    "end": "1641910"
  },
  {
    "text": "But essentially,\nmixture of experts enables massive\nscale because you don't have to just scale\nyour dense trunk model.",
    "start": "1641910",
    "end": "1650280"
  },
  {
    "text": "But you can have a bunch of\ndifferent separate experts that you activate per token.",
    "start": "1650280",
    "end": "1655680"
  },
  {
    "text": "It also allows you to\navoid language interference because the idea is that\nthe different experts,",
    "start": "1655680",
    "end": "1660840"
  },
  {
    "text": "they could specialize\nto specific languages. Unfortunately, it adds\na ton of capacity,",
    "start": "1660840",
    "end": "1666120"
  },
  {
    "text": "so it becomes pretty\neasy to overfit. So I'm going to talk a little\nbit about this overfitting",
    "start": "1666120",
    "end": "1672060"
  },
  {
    "text": "phenomenon. So the top set of graphs that\nwe're going to talk about is for the language Congo.",
    "start": "1672060",
    "end": "1678600"
  },
  {
    "text": "And then the bottom set\nof languages is French. So you really want to compare\na low-resource language on top",
    "start": "1678600",
    "end": "1684820"
  },
  {
    "text": "with a high-resource\nlanguage on bottom. So if you just take your dense\nmodel, traditional transformer",
    "start": "1684820",
    "end": "1690153"
  },
  {
    "text": "sequence-to-sequence\narchitecture, that's this graph\nthat you're showing. So there's a little\nbit of overfitting",
    "start": "1690153",
    "end": "1695590"
  },
  {
    "text": "on the low-resource language,\nbut you can pretty much regularize this with\nstandard dropout techniques.",
    "start": "1695590",
    "end": "1700630"
  },
  {
    "text": "So there's not a big\nproblem and on French. You basically have\nno real problem.",
    "start": "1700630",
    "end": "1706010"
  },
  {
    "text": "However, the minute you switch\nfrom like a dense architecture to a token-level\nMoE architecture,",
    "start": "1706010",
    "end": "1711350"
  },
  {
    "text": "you just experience\na massive overfitting on the low-resource language. So the green line here is just\ndemonstrating without dropout",
    "start": "1711350",
    "end": "1719150"
  },
  {
    "text": "the overfitting. And then if you add dropout,\nyou get a little bit better performance, but it's still\noverfitting quite a bit,",
    "start": "1719150",
    "end": "1726200"
  },
  {
    "text": "essentially by 12k updates. There's no real point\nin continuing training.",
    "start": "1726200",
    "end": "1731630"
  },
  {
    "text": "You're burning GPU, basically. And so one of the things we\nactually worked on quite a bit",
    "start": "1731630",
    "end": "1736640"
  },
  {
    "text": "was trying to figure out how to\nproperly regularize these MoE architectures with this\nspecific masking technique",
    "start": "1736640",
    "end": "1744080"
  },
  {
    "text": "on the gating function that\ndecides which expert to route to in your MoE architecture,\nto just try to pull back some",
    "start": "1744080",
    "end": "1751880"
  },
  {
    "text": "of this overfitting effect. So if you look in the top-right\ngraph, the purple line,",
    "start": "1751880",
    "end": "1757010"
  },
  {
    "text": "you still see some\nsuccessful regularization.",
    "start": "1757010",
    "end": "1762860"
  },
  {
    "text": "Another thing that we did\nto control the overfitting effect-- it's actually quite\nbeing used in language models",
    "start": "1762860",
    "end": "1769130"
  },
  {
    "text": "today as well-- is\ncurriculum learning. And the idea of\nthis is, how are we going to stage when\nlanguages are introduced?",
    "start": "1769130",
    "end": "1777080"
  },
  {
    "text": "And so what we did was we\ntried to train a vanilla model. And then we started\nto measure when",
    "start": "1777080",
    "end": "1782210"
  },
  {
    "text": "the languages begin to overfit. And then we basically bucket\nthem into different sections.",
    "start": "1782210",
    "end": "1787429"
  },
  {
    "text": "And so for high-resource\nlanguages like French, you want to start it early. And it needs to be\ntrained the entire way.",
    "start": "1787430",
    "end": "1793220"
  },
  {
    "text": "But for a lower resource\nlanguage like Wolof, after maybe like 100k\nupdates, it's done.",
    "start": "1793220",
    "end": "1799100"
  },
  {
    "text": "So the rest of the time,\nit's just overfitting. And so it actually gets\nworse the more you train it. So what we did is we moved\nsome of those lower resource",
    "start": "1799100",
    "end": "1805892"
  },
  {
    "text": "languages, and we\ninserted them much later into the training schedule. So you start training\nyour high resource,",
    "start": "1805892",
    "end": "1811430"
  },
  {
    "text": "then you start training\nyour mid-resource, and then your low resource, and then\nyour very low resource. And so by the end,\neverything in theory",
    "start": "1811430",
    "end": "1818570"
  },
  {
    "text": "has trained and is not\nas overfit as it would be without this kind of technique.",
    "start": "1818570",
    "end": "1823810"
  },
  {
    "text": "So I want to show some results. So first, I want to show\nresults on existing data sets. So before we get\nto 200 languages,",
    "start": "1823810",
    "end": "1830890"
  },
  {
    "text": "let's just talk\nabout 100 languages. And so this is the\nFLORES-101 devtest it's important to compare\nto this because this",
    "start": "1830890",
    "end": "1837880"
  },
  {
    "text": "is where existing benchmarks\nin the community lie. Whereas on 200, of course,\nwe can put up anything",
    "start": "1837880",
    "end": "1844210"
  },
  {
    "text": "because it's the\nfirst work on that. So the first column is\ntranslating out of English.",
    "start": "1844210",
    "end": "1851050"
  },
  {
    "text": "So English to Chinese,\nEnglish to Icelandic, anything like that. The second column is\ntranslating into English.",
    "start": "1851050",
    "end": "1857140"
  },
  {
    "text": "So Chinese to English. The third column, xx-yy, is\ntranslating any cross pair",
    "start": "1857140",
    "end": "1863200"
  },
  {
    "text": "not involving English. And the last column\nis the average. So if you look at the\nfirst set of rows,",
    "start": "1863200",
    "end": "1868480"
  },
  {
    "text": "this is a comparison\non models that cover 87 different languages. So there was this\npaper, M2M-100.",
    "start": "1868480",
    "end": "1874510"
  },
  {
    "text": "There was also\nthis Deepnet paper. So you can see the\naverage blue score. Blue is a standard\ntranslation metric,",
    "start": "1874510",
    "end": "1880800"
  },
  {
    "text": "essentially a metric\nof word overlap. So we're looking\nat blue score here. And so you can see the last\nrow, and it will be 200.",
    "start": "1880800",
    "end": "1889270"
  },
  {
    "text": "Even though we\ncover 200 languages, the blue score is substantially\nabove some of the existing work.",
    "start": "1889270",
    "end": "1895029"
  },
  {
    "text": "Now, if we look\nat 101 languages, only the DeltaLM paper\nfrom Microsoft at the time",
    "start": "1895030",
    "end": "1900130"
  },
  {
    "text": "covered that number\nof languages. And so if you compare on all\nof the different cross sets,",
    "start": "1900130",
    "end": "1905620"
  },
  {
    "text": "similarly, you see that this\nno language left behind. Model is much stronger\nin terms of blue.",
    "start": "1905620",
    "end": "1911800"
  },
  {
    "text": "One thing, really quick, on the\nvariance of these blue numbers, I think it's important\nto understand, is something statistically\nsignificant or not?",
    "start": "1911800",
    "end": "1918280"
  },
  {
    "text": "I think about 0.5 blue\nis the general plus-minus",
    "start": "1918280",
    "end": "1923650"
  },
  {
    "text": "that you'll see. And so if it's above\nthat, it's usually a statistically\nsignificant improvement.",
    "start": "1923650",
    "end": "1931630"
  },
  {
    "text": "So now, I want to talk a little\nbit about FLORES-200 results. So here are similar, the first\nchunk of columns translating out",
    "start": "1931630",
    "end": "1937990"
  },
  {
    "text": "of English. Then next chunk is\ntranslating into English. Then you have your cross pairs. And then you have your average.",
    "start": "1937990",
    "end": "1945409"
  },
  {
    "text": "So we have this\nblue metric as well. We also have a\ncharacter-level metric,",
    "start": "1945410",
    "end": "1950690"
  },
  {
    "text": "based on chrF plus plus that's\ncommonly used in the translation community. So I think, looking at\nthese numbers, of course,",
    "start": "1950690",
    "end": "1957470"
  },
  {
    "text": "there's no baseline work\nto compare to unlike on the previous slide. And so when we get to human\nevaluation and a little bit,",
    "start": "1957470",
    "end": "1963980"
  },
  {
    "text": "it'll be more concrete. But I think, generally,\none of the rules of thumb",
    "start": "1963980",
    "end": "1969049"
  },
  {
    "text": "I have for these types\nof numbers is around 30 is pretty reasonably\nbecomes usable.",
    "start": "1969050",
    "end": "1976790"
  },
  {
    "text": "And I think another\nthing, like if you compare these supervised pairs\nto zero-shot pairs,",
    "start": "1976790",
    "end": "1982460"
  },
  {
    "text": "I think we don't\nsee a huge drop off on zero-shot, which\nindicates the model has some sort of\ngeneralization, even",
    "start": "1982460",
    "end": "1988669"
  },
  {
    "text": "if it didn't see that\ntranslation pair directly during training. Another way to\ncalibrate some of this",
    "start": "1988670",
    "end": "1994820"
  },
  {
    "text": "is to compare to\nGoogle Translate. And so if you compare\nto Google Translate, No Language Left Behind\nis quite a bit better",
    "start": "1994820",
    "end": "2001419"
  },
  {
    "text": "at translating into English, and\nnot as good as translating out of English. Although if you like\naverage across everything,",
    "start": "2001420",
    "end": "2008230"
  },
  {
    "text": "it's a little bit better. I want to talk a little\nbit about human evaluation",
    "start": "2008230",
    "end": "2013610"
  },
  {
    "text": "as well to complement\nsome of our discussion on automatic evaluation. And so I think\nautomatic metrics--",
    "start": "2013610",
    "end": "2020870"
  },
  {
    "text": "fast, really good for\nresearch iteration, impossible to move\nforward without. But human evaluation is\nreally the real deal here.",
    "start": "2020870",
    "end": "2029309"
  },
  {
    "text": "And so we had this\npaper at AMTA on how to make this human evaluation\nvery consistent and scalable",
    "start": "2029310",
    "end": "2035809"
  },
  {
    "text": "across different language pairs. I think this goes back to\nthe evaluation data set point",
    "start": "2035810",
    "end": "2040850"
  },
  {
    "text": "that I was making at the\nbeginning of the talk, where if you're a professional\nGerman translator,",
    "start": "2040850",
    "end": "2045917"
  },
  {
    "text": "you're really good at\nevaluating the quality of your German translation. But beyond that, there's\nnot a lot of consistency.",
    "start": "2045917",
    "end": "2053750"
  },
  {
    "text": "And if you evaluate translation\non a five-point scale, like a five translating\nbetween two languages",
    "start": "2053750",
    "end": "2060596"
  },
  {
    "text": "and like a three translating\nbetween other two languages like, are those\nreally comparable? And so we had this entire\nexperiment methodology",
    "start": "2060596",
    "end": "2067610"
  },
  {
    "text": "on how we might want to\nmake this a little bit more comparable. So I want to show some\nresults now on this.",
    "start": "2067610",
    "end": "2075339"
  },
  {
    "text": "So the y-axis here-- so the metric is called XSTS,\nsome metric for how we're",
    "start": "2075340",
    "end": "2080489"
  },
  {
    "text": "doing this human evaluation. The y-axis here is\nactually the delta. ",
    "start": "2080489",
    "end": "2086940"
  },
  {
    "text": "It's a five-point scale, so\nit's a delta, not the raw score.",
    "start": "2086940",
    "end": "2091980"
  },
  {
    "text": "The x-axis here is a bunch of\ndifferent translation directions that we evaluated. So the gray set is\ntranslating into English.",
    "start": "2091980",
    "end": "2099599"
  },
  {
    "text": "The green set is translating\nnon-English directions, so like French to Wolof.",
    "start": "2099600",
    "end": "2106859"
  },
  {
    "text": "And then the blue set is\ntranslating out of English. And so what you're looking\nfor is a positive delta,",
    "start": "2106860",
    "end": "2113250"
  },
  {
    "text": "indicates that our modeling\narchitecture is much better. So what the delta is between is\na baseline transformer model,",
    "start": "2113250",
    "end": "2122190"
  },
  {
    "text": "just trained on all of our data,\nversus the final No Language Left Behind model\nthat we created.",
    "start": "2122190",
    "end": "2127349"
  },
  {
    "text": "So the data is actually\nthe same for both of them. That's how we get\nall 200 languages. So we're just measuring\nhere the human eval",
    "start": "2127350",
    "end": "2134310"
  },
  {
    "text": "of the modeling improvements. And so you can see most of the\ndelta is pretty noticeable.",
    "start": "2134310",
    "end": "2141329"
  },
  {
    "text": "Some of them, not so much. I don't know. Zulu to English, we didn't\nseem to improve very much.",
    "start": "2141330",
    "end": "2148400"
  },
  {
    "text": "But in general,\nit's an improvement detectable by human evaluation. You might also ask, OK, what is\nthe statistically significant",
    "start": "2148400",
    "end": "2155210"
  },
  {
    "text": "difference here between\nabout 0.2 to 0.3 plus or minus is something\nthat's pretty noticeable.",
    "start": "2155210",
    "end": "2162529"
  },
  {
    "text": "And above 0.5, it's\nvery noticeable. One of the things that I also\nwant to get at in evaluation",
    "start": "2162530",
    "end": "2170450"
  },
  {
    "text": "is that there's many different\nfacets of model evaluation.",
    "start": "2170450",
    "end": "2175550"
  },
  {
    "text": "And I think if you look at\nall of the different LLM leaderboards or the\ntransparency reports or whatever, you begin to\ninternalize this pretty quickly.",
    "start": "2175550",
    "end": "2183319"
  },
  {
    "text": "But what we just looked at are\njust very high-level summary numbers, and they\ndon't really tell you",
    "start": "2183320",
    "end": "2188480"
  },
  {
    "text": "what exactly are the errors. And is it ultimately\nusable by people? Is it a safe thing that\npeople can rely on?",
    "start": "2188480",
    "end": "2195170"
  },
  {
    "text": "And so one of the things\nwe really focused on is user safety. And some of that manifests\nin some of the toxicity",
    "start": "2195170",
    "end": "2202760"
  },
  {
    "text": "work that we did. And the driving thing here\nis that not all errors in translation are made equal.",
    "start": "2202760",
    "end": "2208310"
  },
  {
    "text": "So during COVID,\nthere was this one that really went viral\ncirculating around, but the message during COVID\nis you got to wash your hands.",
    "start": "2208310",
    "end": "2215319"
  },
  {
    "text": "But the translation\nproducer was like, you got to hold hands, which I think\nis like exactly the opposite",
    "start": "2215320",
    "end": "2220400"
  },
  {
    "text": "of what you want to do. And other types of\nmeasurement errors are really important as well.",
    "start": "2220400",
    "end": "2225480"
  },
  {
    "text": "So if you're telling someone\nhow far they want to go and you're like, hey, you\nwant to travel 5 kilometers,",
    "start": "2225480",
    "end": "2230640"
  },
  {
    "text": "and then your translation\nis travel 500 kilometers, it's a completely\ndifferent type of issue.",
    "start": "2230640",
    "end": "2236550"
  },
  {
    "text": "And so what we did\nfor toxicity, which is a big focus for\nthis work, is that we collected different toxicity\nlists for all 200 languages.",
    "start": "2236550",
    "end": "2245490"
  },
  {
    "text": "And so why do I care\nso much about toxicity? I think it's a\nuser safety thing. So if you input some\nperfectly benign text",
    "start": "2245490",
    "end": "2252450"
  },
  {
    "text": "and then the output\nis profanity, I think it's just like\nreally unexpected, and it breaks a lot of\ntrust in the system.",
    "start": "2252450",
    "end": "2258990"
  },
  {
    "text": "And it's an extremely poor\nexperience for people. That, being said, it's also a\nvery, very challenging thing",
    "start": "2258990",
    "end": "2265080"
  },
  {
    "text": "because it's extremely\nculturally specific. So things that are slurs or\ninsults in certain languages,",
    "start": "2265080",
    "end": "2272220"
  },
  {
    "text": "they don't really\ngeneralize across cultures, which means that\nthings like this",
    "start": "2272220",
    "end": "2277680"
  },
  {
    "text": "are very challenging to create. And I also was very\ninterested in this direction because I think it's\nbroadly useful for all sorts",
    "start": "2277680",
    "end": "2284880"
  },
  {
    "text": "of different type of detection,\nthings that you need to do, and also mitigation. And so even though\nwe develop this",
    "start": "2284880",
    "end": "2290970"
  },
  {
    "text": "in the context of\ntranslation, it can be used very broadly in\nother types of NLP applications.",
    "start": "2290970",
    "end": "2297780"
  },
  {
    "text": "This is also open-source. You can download it. You have to type in a\nlittle password that's in the github repo just so\nthat you don't accidentally",
    "start": "2297780",
    "end": "2304980"
  },
  {
    "text": "download and realize you\nhave files of curse words all over your computer.",
    "start": "2304980",
    "end": "2310570"
  },
  {
    "text": "So I want to end a little\nbit with some thoughts about future directions. And before I get there, there's\na 190-page paper that writes up",
    "start": "2310570",
    "end": "2320110"
  },
  {
    "text": "all of this in far greater\ndetail in case you're curious. So a few future\ndirections that I",
    "start": "2320110",
    "end": "2327300"
  },
  {
    "text": "think I'm really interested\nin, and some of these are also very applicable\nto things like speech,",
    "start": "2327300",
    "end": "2332819"
  },
  {
    "text": "is that I think one of them\nis more explicit multilingual. So I think a lot of\napproaches to multilingual",
    "start": "2332820",
    "end": "2340290"
  },
  {
    "text": "have been like, hey, we\nhave this thing that's working well for one language. Let's try to scale it to a\nbunch of different languages.",
    "start": "2340290",
    "end": "2347275"
  },
  {
    "text": "And then we're going to put\nthem all in the same modeling bucket and just hope\nthat the model learns",
    "start": "2347275",
    "end": "2352650"
  },
  {
    "text": "all of these different\nrepresentations. But I think there's a lot of\npotential room for explicitly",
    "start": "2352650",
    "end": "2358920"
  },
  {
    "text": "bringing in, like the fact that\nyou know it's multilingual, and so the architecture more.",
    "start": "2358920",
    "end": "2365100"
  },
  {
    "text": "And so it's possible to capture\nmore nuances between languages",
    "start": "2365100",
    "end": "2372000"
  },
  {
    "text": "or different relationships\nbetween languages. And the other one is continued\nsupport for everyone.",
    "start": "2372000",
    "end": "2377910"
  },
  {
    "text": "I think it's like something\nreflecting on this project, is that going from 100 to 200\nwas already pretty challenging,",
    "start": "2377910",
    "end": "2385240"
  },
  {
    "text": "but going beyond a\nlot of the techniques that we developed here are\nnot necessarily that scalable.",
    "start": "2385240",
    "end": "2391150"
  },
  {
    "text": "This is actually what\ninspired some of our work on speech translation as well. So if you recently saw\nthe seamless M42 release",
    "start": "2391150",
    "end": "2398260"
  },
  {
    "text": "or the unwritten\nlanguages-- we did a lot of modeling of Hoquiam. And I think that goes\ninto this direction",
    "start": "2398260",
    "end": "2403869"
  },
  {
    "text": "really well because many of\nthe languages that people want to use are spoken first\nlanguages, and not",
    "start": "2403870",
    "end": "2410140"
  },
  {
    "text": "necessarily primarily written. And then I think the\nlast thing that I'm still really passionate about is\ncontinued increase ease of use",
    "start": "2410140",
    "end": "2418660"
  },
  {
    "text": "and training of these\nmodels, and democratization for the community. So one of the things that we\ntried to do in this work is just",
    "start": "2418660",
    "end": "2426010"
  },
  {
    "text": "really, really clearly write\ndown everything that we did and open-source, even the data\npipeline and things like that.",
    "start": "2426010",
    "end": "2432667"
  },
  {
    "text": "And so that's where you\nget all of the repos that I linked in,\nlike a huge write-up.",
    "start": "2432667",
    "end": "2437950"
  },
  {
    "text": "But I think if\nsomeone were to try to reproduce this for their own\nlanguage-- and many people have. I'm not saying that hasn't been.",
    "start": "2437950",
    "end": "2444070"
  },
  {
    "text": "But it's like, if you\nwanted to do this, it would be extremely,\nextremely hard",
    "start": "2444070",
    "end": "2449319"
  },
  {
    "text": "because there's just like so\nmuch different things going on. So I think most\nof what we've seen is people have\ndownloaded the base model",
    "start": "2449320",
    "end": "2455980"
  },
  {
    "text": "and fine-tuned it for\ntheir own language. But it's pretty\nhard to just like add on many, many more\nlanguages to the system",
    "start": "2455980",
    "end": "2463090"
  },
  {
    "text": "because of how complicated\nall of the moving parts are. And so I feel like something\nfor the translation community",
    "start": "2463090",
    "end": "2468910"
  },
  {
    "text": "overall is, how do we simplify\na lot of these things? And I think that's where a\nlot of fundamental modeling",
    "start": "2468910",
    "end": "2475960"
  },
  {
    "text": "innovation could help us get to. And so, yeah, I got a\nchance to give this talk,",
    "start": "2475960",
    "end": "2481390"
  },
  {
    "text": "but, of course, the\nwork is like being done by a huge team of\npeople that I've cited here.",
    "start": "2481390",
    "end": "2487120"
  },
  {
    "text": "And yeah, if you want to use any\nof this or read more about it, everything is linked from\nthis main github repo",
    "start": "2487120",
    "end": "2493990"
  },
  {
    "text": "here in fairseq. And you can click on\neverything else afterwards.",
    "start": "2493990",
    "end": "2499420"
  },
  {
    "text": "But yeah, maybe I'll\ngo back to Steven if we have any questions\nor anything else like that.",
    "start": "2499420",
    "end": "2505130"
  },
  {
    "text": "All right. No, thanks for the great talk. Yeah, if anybody\nhas any questions, feel free to unmute and ask.",
    "start": "2505130",
    "end": "2510310"
  },
  {
    "start": "2510310",
    "end": "2517220"
  },
  {
    "text": "Did you consult with a\nlot of native speakers for profanities in\nthis type of stuff?",
    "start": "2517220",
    "end": "2524300"
  },
  {
    "text": "How were you able to get access\nto the low-quality languages",
    "start": "2524300",
    "end": "2529310"
  },
  {
    "text": "or low-resource languages, and\nmake sure the translations are correct? Yeah, yeah, that's a\nreally good question.",
    "start": "2529310",
    "end": "2535420"
  },
  {
    "text": "I mean, I think it's the\nmost important to consult a bunch of native speakers\nacross the entire development",
    "start": "2535420",
    "end": "2540920"
  },
  {
    "text": "process. So part of our original\nthing was interviewing a bunch of people\nto understand what they're looking for in a\nhigh-quality translation.",
    "start": "2540920",
    "end": "2548480"
  },
  {
    "text": "And then we have an entire\nprofessional translation team hired, which took quite\na long time to find,",
    "start": "2548480",
    "end": "2555920"
  },
  {
    "text": "to consult with\nalong the process. And then right now, we also\nhave some of the things",
    "start": "2555920",
    "end": "2562970"
  },
  {
    "text": "like toxicity lists, are\nopen to the community. So if you make like\na pull request, we try to validate that.",
    "start": "2562970",
    "end": "2568700"
  },
  {
    "text": "It's like a useful addition, and\nthen try to merge it in as well. ",
    "start": "2568700",
    "end": "2576650"
  },
  {
    "text": "We have a question in the room. Let's see if it comes over soon. Go ahead. So I'll see, Steve.",
    "start": "2576650",
    "end": "2582980"
  },
  {
    "text": "Yeah. So did you spend most of your\ntime in a data pipeline space?",
    "start": "2582980",
    "end": "2589010"
  },
  {
    "text": "Yeah. Yeah, good. Yeah, a good question. I think the question\nis, did you spend",
    "start": "2589010",
    "end": "2594960"
  },
  {
    "text": "most of your time in\nthe data pipeline today? It ended up being about 50-50\ndata, or more like driving work,",
    "start": "2594960",
    "end": "2603119"
  },
  {
    "text": "and then 50-50 on the other side\nlike modeling and evaluation work. Because once the\ndata is set, then",
    "start": "2603120",
    "end": "2609539"
  },
  {
    "text": "there is a lot of iteration\non the modeling side to figure out, OK, how much\nof the data should we use?",
    "start": "2609540",
    "end": "2615628"
  },
  {
    "text": "How should we portion the data? How do we prevent overfitting? What is the right architecture? But a lot of work\ngoes into the data.",
    "start": "2615628",
    "end": "2622410"
  },
  {
    "text": "Because I think if you don't\nhave high-quality data, you just can't get a good model.",
    "start": "2622410",
    "end": "2628900"
  },
  {
    "text": "And for data mining, how\ndo you mine the data? Do you use Selenium? Or how do you mine the web?",
    "start": "2628900",
    "end": "2636860"
  },
  {
    "text": "Yeah. So for the web, we\nstart with Common Crawl. So we downloaded all of the\ndifferent dumps of Common Crawl.",
    "start": "2636860",
    "end": "2643030"
  },
  {
    "text": "And then we use HTML parser. I think, now, if you download,\nfor example, the RedPajama",
    "start": "2643030",
    "end": "2648640"
  },
  {
    "text": "data set, they've done a lot\nof this parsing and stuff. And then we have large-scale\npipelines that are set up.",
    "start": "2648640",
    "end": "2655870"
  },
  {
    "text": "You can use Spark, for example,\nto process these things, to split all of the\ndifferent sentences",
    "start": "2655870",
    "end": "2661030"
  },
  {
    "text": "out, run your language\nidentification. You can do different\nheuristic cleaning.",
    "start": "2661030",
    "end": "2666135"
  },
  {
    "text": "There are certain\nlanguages where it's actually very challenging\nto identify what is a sentence. I think in Thai,\nthere is no period.",
    "start": "2666135",
    "end": "2673960"
  },
  {
    "text": "So you have to use\na different models to identify what is a sentence\nand parse some of those things",
    "start": "2673960",
    "end": "2679450"
  },
  {
    "text": "out. And then we end up with\nour monolingual data dump.",
    "start": "2679450",
    "end": "2686070"
  },
  {
    "text": "But what is Common Crawl? Is it software that you\nuse for data science?",
    "start": "2686070",
    "end": "2691434"
  },
  {
    "text": "Oh, yeah, yeah. Common Crawl is like\nan open-source version of the web that runs, I\nthink, maybe quarterly.",
    "start": "2691435",
    "end": "2697763"
  },
  {
    "text": "I would have to check. But yeah, if you go\nto commoncrawl.org, you can download it. But warning, it's very large.",
    "start": "2697763",
    "end": "2703495"
  },
  {
    "start": "2703495",
    "end": "2712680"
  },
  {
    "text": "I have a question. You might have\nmentioned this briefly, but I'm wondering how ChatGPT\nand GPT-4 does on this.",
    "start": "2712680",
    "end": "2720090"
  },
  {
    "text": "Does just more scale\nand pre-training data help as well for low-resource\nmachine translation?",
    "start": "2720090",
    "end": "2726660"
  },
  {
    "text": "Yeah, yeah, good question. Actually, there have\nbeen some studies done on how these systems work.",
    "start": "2726660",
    "end": "2731710"
  },
  {
    "text": "I think for\nhigh-resource languages, it's actually quite\nbeneficial to scale up. I think part of that is\nbecause the models have",
    "start": "2731710",
    "end": "2738690"
  },
  {
    "text": "some innate generalization,\nand so one of the challenges that people talk about different\nthings in different languages.",
    "start": "2738690",
    "end": "2743820"
  },
  {
    "text": "So seeing that knowledge\nin another language can actually help\nthe generalization. But on low-resource languages,\nyeah, the performance",
    "start": "2743820",
    "end": "2752880"
  },
  {
    "text": "is pretty difficult, especially\non some of these translation benchmarks. I also think that\nlanguage models,",
    "start": "2752880",
    "end": "2760020"
  },
  {
    "text": "in terms of being trained\nfor a translation objective, tend to score worse on\ntranslation benchmarks",
    "start": "2760020",
    "end": "2765299"
  },
  {
    "text": "because language models\nare approximately capturing the same thing. Whereas translation\nmodels, you're really try to align the\nmeaning a little bit more.",
    "start": "2765300",
    "end": "2773770"
  },
  {
    "text": "But yeah. So I think for low resource,\nstill pretty challenging. But yeah. One thing that's interesting\nis for most English language",
    "start": "2773770",
    "end": "2781180"
  },
  {
    "text": "models, they can actually\ndo a reasonable job at producing other\nlanguages because it's impossible to get rid\nof other languages",
    "start": "2781180",
    "end": "2787180"
  },
  {
    "text": "in your English-specific data. So things like French or\nGerman will work reasonably.",
    "start": "2787180",
    "end": "2792520"
  },
  {
    "text": " So just to clarify. You said language models trained\nwith a translation objective",
    "start": "2792520",
    "end": "2802320"
  },
  {
    "text": "do better because-- They tend to do better. If you fine-tune for\nthe translation task,",
    "start": "2802320",
    "end": "2809150"
  },
  {
    "text": "it will tend to do better. Well, that makes sense\ncompared to, for example, some few-shot\nin-context examples.",
    "start": "2809150",
    "end": "2817290"
  },
  {
    "text": "Exactly. Exactly. Now, one other\nquestion is, do you",
    "start": "2817290",
    "end": "2823910"
  },
  {
    "text": "see this being similar\nto, for example, fine-tuning on particular\nexpert domains, which might also have less data and\nlow resource, and as",
    "start": "2823910",
    "end": "2834440"
  },
  {
    "text": "well as domain-specific\njargon and so forth? Yeah. I mean, I think if we were\nto restart this project now,",
    "start": "2834440",
    "end": "2841780"
  },
  {
    "text": "I think that would be one of the\nfirst things we also explored or at least an extremely\nstrong baseline, where",
    "start": "2841780",
    "end": "2847089"
  },
  {
    "text": "if you take some of the data\nand you try to fine-tune, or try to do domain adaptation,\nI think that's also",
    "start": "2847090",
    "end": "2853750"
  },
  {
    "text": "where some of the\nretrieval-type approaches go in for translation, but also\nlarge language modeling work,",
    "start": "2853750",
    "end": "2860410"
  },
  {
    "text": "where you try to have a separate\ndomain that you can retrieve some text in for adaptation.",
    "start": "2860410",
    "end": "2865810"
  },
  {
    "text": "I think all of those approaches\nare pretty promising. ",
    "start": "2865810",
    "end": "2874410"
  },
  {
    "text": "Oh, great. Any other questions?  One quick one on the point\nof zero probability on one",
    "start": "2874410",
    "end": "2882140"
  },
  {
    "text": "of the slides. I think you showed\nsome peak results with zero-shot that were higher\nthan just the base model.",
    "start": "2882140",
    "end": "2890210"
  },
  {
    "text": "Do you think that's\nbecause there might still be some overfitting on those\nlow-resource languages?",
    "start": "2890210",
    "end": "2895760"
  },
  {
    "text": "Yeah, good question. So for our\nlarge-scale mining, we don't mind every single\npossible cross pair.",
    "start": "2895760",
    "end": "2902780"
  },
  {
    "text": "So like Icelandic\nWolof, it's probably like not like the most\nin-demand translation direction.",
    "start": "2902780",
    "end": "2909890"
  },
  {
    "text": "And so we did not\nmind all 200 times 200 because it's really producing\na combinatorial explosion.",
    "start": "2909890",
    "end": "2915830"
  },
  {
    "text": "And so that's where the\nzero-shot results come from, where we don't have training\ndata directionally in that pair,",
    "start": "2915830",
    "end": "2923359"
  },
  {
    "text": "but the model has seen both\nthe input and the output. And so I think those\nresults are pretty good.",
    "start": "2923360",
    "end": "2930020"
  },
  {
    "text": "Well, they're good for\ncertain language languages, which I think goes to show\nthe generalization capability.",
    "start": "2930020",
    "end": "2936470"
  },
  {
    "text": "And it's not as critical to\nhave every single pair covered, but many of them\nare not as good.",
    "start": "2936470",
    "end": "2942480"
  },
  {
    "text": "And so you see overall the\nperformance is lower, even though on certain languages. It can perform better\nthan you expect,",
    "start": "2942480",
    "end": "2947767"
  },
  {
    "text": "but that's because it has\nseen the input and the output. It's not zero-shot on\ncompletely unseen language. ",
    "start": "2947767",
    "end": "2961950"
  },
  {
    "text": "I have a question. But I wanted you to\nalso do something",
    "start": "2961950",
    "end": "2968580"
  },
  {
    "text": "related to transcription\nor audio information.",
    "start": "2968580",
    "end": "2974200"
  },
  {
    "text": "Yeah. Good question. So in this project, no. Not so much transcription. But we had a follow-up work\nthat we released, actually,",
    "start": "2974200",
    "end": "2980950"
  },
  {
    "text": "just like a month\nor so ago, called SeamlessM4T, which is like a\njoint model for both speech",
    "start": "2980950",
    "end": "2986890"
  },
  {
    "text": "and text translation. And that's where we do leverage\na lot of audio transcription because that also has--",
    "start": "2986890",
    "end": "2992860"
  },
  {
    "text": "it helps us bridge the\nspoken data and the text data to leverage both\nof them together. ",
    "start": "2992860",
    "end": "3001302"
  },
  {
    "text": "Thank you. ",
    "start": "3001302",
    "end": "3008240"
  },
  {
    "text": "But just to clarify the\nsupervised fine tuning, it worked better compared\nto other methods?",
    "start": "3008240",
    "end": "3016423"
  },
  {
    "text": "So actually, in this work--\nit was a couple of years ago now, so supervised fine\ntuning wasn't as common",
    "start": "3016423",
    "end": "3022130"
  },
  {
    "text": "as it was now. But I think in the\nliterature, if you want to use a large language\nmodel to do translation,",
    "start": "3022130",
    "end": "3027975"
  },
  {
    "text": "it's currently best, yeah, if\nyou do some supervised fine tuning. I'm just wondering about that\nbecause the way, as humans, we",
    "start": "3027975",
    "end": "3034850"
  },
  {
    "text": "don't just learn\nby looking at pairs of the same thing in\ndifferent languages",
    "start": "3034850",
    "end": "3040130"
  },
  {
    "text": "and memorizing how to map\nfrom one to the other. We learn in a more\nunsupervised way,",
    "start": "3040130",
    "end": "3046220"
  },
  {
    "text": "where if we know\nboth languages, then we can naturally\ntranslate between them.",
    "start": "3046220",
    "end": "3051695"
  },
  {
    "text": " But I guess it makes\nsense for an LLM why having supervised\nexamples would help.",
    "start": "3051695",
    "end": "3060280"
  },
  {
    "text": "Yeah. Yeah. I mean, I think as the base\nfoundation model continues",
    "start": "3060280",
    "end": "3065450"
  },
  {
    "text": "to improve in quality, I think\nthat's where the quality will probably improve, and you don't\nneed less and less fine tuning.",
    "start": "3065450",
    "end": "3071040"
  },
  {
    "text": "I mean, I think that's\nlike the OpenAI approach. If you have the best\nfoundation model, then you don't need as much\ndomain-specific fine tuning.",
    "start": "3071040",
    "end": "3077930"
  },
  {
    "text": "And I think at the\nstart when I started working on text\ngeneration, there was translation researchers\nand summarization researchers",
    "start": "3077930",
    "end": "3083852"
  },
  {
    "text": "and question-answering\nresearchers. And they worked very\ndifferently But now, it's all driven by the\nsame underlying thing,",
    "start": "3083852",
    "end": "3089960"
  },
  {
    "text": "and you're not like a\nspecialized summarization researcher anymore. ",
    "start": "3089960",
    "end": "3096360"
  },
  {
    "text": "I think that makes\na lot of sense.  Do we have any other questions?",
    "start": "3096360",
    "end": "3102900"
  },
  {
    "text": " Any question? ",
    "start": "3102900",
    "end": "3115080"
  },
  {
    "text": "Any more in-person questions? I don't think so.",
    "start": "3115080",
    "end": "3120510"
  },
  {
    "text": "OK, great. All right. Well, thank you, Angela,\nfor the very interesting and a great talk again,\nand for taking the time.",
    "start": "3120510",
    "end": "3130560"
  },
  {
    "text": "Yeah. We hope that you\ncan keep in touch. And if anybody has\nany other questions,",
    "start": "3130560",
    "end": "3135810"
  },
  {
    "text": "feel free to get in\ntouch with Angela. All right. Thanks so much for\nhaving me today.",
    "start": "3135810",
    "end": "3141378"
  },
  {
    "text": "Bye, everyone. ",
    "start": "3141378",
    "end": "3148000"
  }
]