[
  {
    "start": "0",
    "end": "6000"
  },
  {
    "start": "0",
    "end": "6030"
  },
  {
    "start": "6000",
    "end": "15000"
  },
  {
    "text": "Hi.",
    "start": "6030",
    "end": "6550"
  },
  {
    "text": "In this module, I'm\ngoing to show you",
    "start": "6550",
    "end": "8092"
  },
  {
    "text": "how you can use the machinery\nof linear predictors",
    "start": "8092",
    "end": "10650"
  },
  {
    "text": "that we've developed so far to\nget some non-linear predictors.",
    "start": "10650",
    "end": "15377"
  },
  {
    "start": "15000",
    "end": "85000"
  },
  {
    "text": "So we're going to first\nfocus on regression,",
    "start": "15377",
    "end": "17210"
  },
  {
    "text": "and then later talk\nabout classification.",
    "start": "17210",
    "end": "19759"
  },
  {
    "text": "So remembering regression.",
    "start": "19760",
    "end": "20958"
  },
  {
    "text": "We're given some training data.",
    "start": "20958",
    "end": "22250"
  },
  {
    "text": "We have a learning algorithm\nthat produces a predictor.",
    "start": "22250",
    "end": "25700"
  },
  {
    "text": "And the first key question\nor design decision",
    "start": "25700",
    "end": "28220"
  },
  {
    "text": "is which predictors is a\nlearning algorithm allowed",
    "start": "28220",
    "end": "31189"
  },
  {
    "text": "to choose from?",
    "start": "31190",
    "end": "32270"
  },
  {
    "text": "That's the question of\nthe hypothesis class.",
    "start": "32270",
    "end": "36170"
  },
  {
    "text": "So for linear predictors,\nremember that the hypothesis",
    "start": "36170",
    "end": "41269"
  },
  {
    "text": "class is defined to be\nthe set of all predictors,",
    "start": "41270",
    "end": "45230"
  },
  {
    "text": "f of x, equals\nsome weight vector",
    "start": "45230",
    "end": "48559"
  },
  {
    "text": "dot feature vector phi of x.",
    "start": "48560",
    "end": "52370"
  },
  {
    "text": "And we allow the weight\nvector to range freely",
    "start": "52370",
    "end": "55129"
  },
  {
    "text": "over all d-dimensional\nreal vectors.",
    "start": "55130",
    "end": "58080"
  },
  {
    "text": "OK.",
    "start": "58080",
    "end": "58580"
  },
  {
    "text": "So if we take phi of x equals\n1, x, like we did before,",
    "start": "58580",
    "end": "63280"
  },
  {
    "text": "then we can get some lines.",
    "start": "63280",
    "end": "67010"
  },
  {
    "text": "So if we set the weight\nvector to be 1, 0.57,",
    "start": "67010",
    "end": "71050"
  },
  {
    "text": "then we get this line\nwith intercept at 1",
    "start": "71050",
    "end": "74200"
  },
  {
    "text": "and a slope of 0.57.",
    "start": "74200",
    "end": "76450"
  },
  {
    "text": "And here's a purple one\nwith the intercept of 2",
    "start": "76450",
    "end": "80560"
  },
  {
    "text": "and a slope of 0.2.",
    "start": "80560",
    "end": "83600"
  },
  {
    "text": "All is good.",
    "start": "83600",
    "end": "85590"
  },
  {
    "start": "85000",
    "end": "110000"
  },
  {
    "text": "But what happens if we get\ndata that looks like this?",
    "start": "85590",
    "end": "88662"
  },
  {
    "text": "If you try to fit\na line to it you",
    "start": "88663",
    "end": "90080"
  },
  {
    "text": "won't be very happy with this.",
    "start": "90080",
    "end": "92450"
  },
  {
    "text": "You really want to fit\nsome sort of nonlinear",
    "start": "92450",
    "end": "94820"
  },
  {
    "text": "predictor, something that can\ncurve around to fit the data.",
    "start": "94820",
    "end": "99640"
  },
  {
    "text": "So your first reaction might\nbe to reach for something",
    "start": "99640",
    "end": "102159"
  },
  {
    "text": "like neural networks or\ndecision trees, something",
    "start": "102160",
    "end": "104290"
  },
  {
    "text": "that's more complex.",
    "start": "104290",
    "end": "106150"
  },
  {
    "text": "But let's see how far\nwe can get it with just",
    "start": "106150",
    "end": "108370"
  },
  {
    "text": "using linear predictors.",
    "start": "108370",
    "end": "111420"
  },
  {
    "start": "110000",
    "end": "236000"
  },
  {
    "text": "So the key thing is\nthat the feature vector",
    "start": "111420",
    "end": "116869"
  },
  {
    "text": "can be arbitrary.",
    "start": "116870",
    "end": "119190"
  },
  {
    "text": "So let's take the feature\nvector to be 1, x as before,",
    "start": "119190",
    "end": "123020"
  },
  {
    "text": "but let's just add on an x\nsquared term, just for fun.",
    "start": "123020",
    "end": "128399"
  },
  {
    "text": "And so, for example,\nif we feed x equals 3,",
    "start": "128400",
    "end": "131930"
  },
  {
    "text": "then we get the\nfeature vector 1, 3, 9.",
    "start": "131930",
    "end": "136560"
  },
  {
    "text": "Let's define some weights--",
    "start": "136560",
    "end": "138000"
  },
  {
    "text": "2, 1, 0.2.",
    "start": "138000",
    "end": "141240"
  },
  {
    "text": "And let's plot what that\nfunction looks like.",
    "start": "141240",
    "end": "144870"
  },
  {
    "text": "And we get a nice curve.",
    "start": "144870",
    "end": "149440"
  },
  {
    "text": "So that's a\nnon-linear predictor.",
    "start": "149440",
    "end": "150990"
  },
  {
    "text": "So it has an intercept of 2,\na slope of 1, at the origin,",
    "start": "150990",
    "end": "156630"
  },
  {
    "text": "and a curvature of negative 0.2.",
    "start": "156630",
    "end": "160600"
  },
  {
    "text": "Here's another one--\n4, minus 1, 0.1.",
    "start": "160600",
    "end": "166150"
  },
  {
    "text": "Here's an intercept of\n4, a slope of minus 1,",
    "start": "166150",
    "end": "169930"
  },
  {
    "text": "and a curvature of 0.1.",
    "start": "169930",
    "end": "173959"
  },
  {
    "text": "And here's another one--",
    "start": "173960",
    "end": "175730"
  },
  {
    "text": "1, 1, 0.",
    "start": "175730",
    "end": "176370"
  },
  {
    "text": "So what does this one look like?",
    "start": "176370",
    "end": "178010"
  },
  {
    "text": "This one just looks like\na line because we've",
    "start": "178010",
    "end": "180349"
  },
  {
    "text": "used a 0 weight on\nthis x squared term,",
    "start": "180350",
    "end": "183200"
  },
  {
    "text": "so it just reduces to\na linear predictor.",
    "start": "183200",
    "end": "186470"
  },
  {
    "text": "In general, we can\ndefine a family",
    "start": "186470",
    "end": "188270"
  },
  {
    "text": "of all quadratic predictors\nthat looks like this, by ranging",
    "start": "188270",
    "end": "193460"
  },
  {
    "text": "the weight vector, really, over\nall three-dimensional vectors.",
    "start": "193460",
    "end": "199130"
  },
  {
    "text": "So here is our first example of\ngetting a non-linear predictor,",
    "start": "199130",
    "end": "202330"
  },
  {
    "text": "in particular\nquadratic predictors",
    "start": "202330",
    "end": "204100"
  },
  {
    "text": "just by changing phi.",
    "start": "204100",
    "end": "206830"
  },
  {
    "text": "So one small note here\nis that in one dimension",
    "start": "206830",
    "end": "210370"
  },
  {
    "text": "x squared is just\na single feature.",
    "start": "210370",
    "end": "214620"
  },
  {
    "text": "But if x were d-dimensional\nto begin with, then",
    "start": "214620",
    "end": "217549"
  },
  {
    "text": "to get the full range\nof quadratic predictors,",
    "start": "217550",
    "end": "220370"
  },
  {
    "text": "we would need d squared\nfeatures, one for every xi,",
    "start": "220370",
    "end": "223840"
  },
  {
    "text": "xj pair.",
    "start": "223840",
    "end": "226700"
  },
  {
    "text": "That would be a lot.",
    "start": "226700",
    "end": "228470"
  },
  {
    "text": "So that's one\nslight disadvantage",
    "start": "228470",
    "end": "230390"
  },
  {
    "text": "of using the machinery\nof linear predictors",
    "start": "230390",
    "end": "232700"
  },
  {
    "text": "to get non-linear predictors.",
    "start": "232700",
    "end": "236290"
  },
  {
    "start": "236000",
    "end": "373000"
  },
  {
    "text": "Let's move on.",
    "start": "236290",
    "end": "237829"
  },
  {
    "text": "So quadratic\npredictors are great,",
    "start": "237830",
    "end": "240130"
  },
  {
    "text": "but they can only\nkind of vary smoothly.",
    "start": "240130",
    "end": "243190"
  },
  {
    "text": "What happens if you want a\nfunction that looks like this?",
    "start": "243190",
    "end": "249370"
  },
  {
    "text": "So here's an example of a\npiecewise constant predictor.",
    "start": "249370",
    "end": "253560"
  },
  {
    "text": "And we can get this\npredictor, also,",
    "start": "253560",
    "end": "256049"
  },
  {
    "text": "by just re-imagining\nwhat a feature vector is.",
    "start": "256050",
    "end": "258659"
  },
  {
    "text": "So here is--",
    "start": "258660",
    "end": "260910"
  },
  {
    "text": "I'm going to define\nphi of x equals--",
    "start": "260910",
    "end": "263640"
  },
  {
    "text": "and the first--",
    "start": "263640",
    "end": "264810"
  },
  {
    "text": "I'm going to carve up the input\nspace into a bunch of regions",
    "start": "264810",
    "end": "268380"
  },
  {
    "text": "and define a feature to be\nwhether x lies in that region",
    "start": "268380",
    "end": "272580"
  },
  {
    "text": "or not.",
    "start": "272580",
    "end": "273840"
  },
  {
    "text": "The first feature is test\nwhether x is between 0 and 1,",
    "start": "273840",
    "end": "277770"
  },
  {
    "text": "and the indicator\nfunction will return 1",
    "start": "277770",
    "end": "280229"
  },
  {
    "text": "if that's true and 0 otherwise.",
    "start": "280230",
    "end": "282840"
  },
  {
    "text": "The second one is going to test\nbetween 1 and 2, and so on.",
    "start": "282840",
    "end": "288350"
  },
  {
    "text": "So here's an example.",
    "start": "288350",
    "end": "289490"
  },
  {
    "text": "If you punch in 2.3, that is\n0 on all the features/regions",
    "start": "289490",
    "end": "295330"
  },
  {
    "text": "except for this one.",
    "start": "295330",
    "end": "299479"
  },
  {
    "text": "OK, so if I set the weight\nvector corresponding",
    "start": "299480",
    "end": "303050"
  },
  {
    "text": "to 1, 2, 4, 4, 3, then\nI get this function.",
    "start": "303050",
    "end": "306690"
  },
  {
    "text": "And notice that each weight is\njust identifying the function",
    "start": "306690",
    "end": "310970"
  },
  {
    "text": "value of that region.",
    "start": "310970",
    "end": "313440"
  },
  {
    "text": "So between 0 and 1,\nthe feature vector is--",
    "start": "313440",
    "end": "319030"
  },
  {
    "text": "sorry, the function is at 1, and\nthen it's 2, and then it's 4,",
    "start": "319030",
    "end": "323980"
  },
  {
    "text": "and then it's 3, OK?",
    "start": "323980",
    "end": "326110"
  },
  {
    "text": "So here's another one.",
    "start": "326110",
    "end": "329099"
  },
  {
    "text": "It's 4, and then 3, 3, 2, 1.5.",
    "start": "329100",
    "end": "334520"
  },
  {
    "text": "And again, in general,\nthe set of predictors",
    "start": "334520",
    "end": "338180"
  },
  {
    "text": "is w dot phi of x, where\nw can range freely.",
    "start": "338180",
    "end": "343940"
  },
  {
    "text": "So this is a\ngeneral technique of",
    "start": "343940",
    "end": "346480"
  },
  {
    "text": "piecewise constant\nfunctions, which",
    "start": "346480",
    "end": "348970"
  },
  {
    "text": "can give you expressive\nnon-linear predictors",
    "start": "348970",
    "end": "351220"
  },
  {
    "text": "by partitioning the input space.",
    "start": "351220",
    "end": "353970"
  },
  {
    "text": "So again, a caveat\nis that everything",
    "start": "353970",
    "end": "356570"
  },
  {
    "text": "looks nice in one dimension.",
    "start": "356570",
    "end": "358440"
  },
  {
    "text": "But if the x were d-dimensions\nand each dimension",
    "start": "358440",
    "end": "363320"
  },
  {
    "text": "were carved up into\nB regions, then you",
    "start": "363320",
    "end": "365450"
  },
  {
    "text": "have B to the D\ndifferent features, which",
    "start": "365450",
    "end": "367760"
  },
  {
    "text": "is an exponential\nnumber of features,",
    "start": "367760",
    "end": "369690"
  },
  {
    "text": "which is a kind of no-go.",
    "start": "369690",
    "end": "370910"
  },
  {
    "start": "370910",
    "end": "374560"
  },
  {
    "start": "373000",
    "end": "442000"
  },
  {
    "text": "So you can kind of\nget the idea now,",
    "start": "374560",
    "end": "377410"
  },
  {
    "text": "but let's just do\nanother example.",
    "start": "377410",
    "end": "379660"
  },
  {
    "text": "Suppose you're trying\nto predict a function",
    "start": "379660",
    "end": "382930"
  },
  {
    "text": "with some periodic\nstructure, like you're",
    "start": "382930",
    "end": "384669"
  },
  {
    "text": "trying to predict\ntraffic patterns",
    "start": "384670",
    "end": "387070"
  },
  {
    "text": "or sales across a year.",
    "start": "387070",
    "end": "389440"
  },
  {
    "text": "So imagine that you want\nto get a function that",
    "start": "389440",
    "end": "391990"
  },
  {
    "text": "looks like this, OK?",
    "start": "391990",
    "end": "394840"
  },
  {
    "text": "So let's see if we can hack\ntogether a feature vector that",
    "start": "394840",
    "end": "398440"
  },
  {
    "text": "does that.",
    "start": "398440",
    "end": "399160"
  },
  {
    "text": "So phi of x equals\n1x and x squared.",
    "start": "399160",
    "end": "402500"
  },
  {
    "text": "So put it in the quadratic.",
    "start": "402500",
    "end": "403780"
  },
  {
    "text": "And now, let's add a cosine 3x.",
    "start": "403780",
    "end": "408040"
  },
  {
    "text": "It's kind of arbitrary.",
    "start": "408040",
    "end": "410680"
  },
  {
    "text": "So here's an example.",
    "start": "410680",
    "end": "411979"
  },
  {
    "text": "If you punch 2 into x, then\nyou get this feature vector.",
    "start": "411980",
    "end": "416440"
  },
  {
    "text": "If you define the\nweights in a certain way,",
    "start": "416440",
    "end": "419510"
  },
  {
    "text": "then you get that red curve.",
    "start": "419510",
    "end": "422330"
  },
  {
    "text": "You can define the\nweights this way,",
    "start": "422330",
    "end": "424930"
  },
  {
    "text": "and then you get the purple\ncurve, and then so on.",
    "start": "424930",
    "end": "429190"
  },
  {
    "text": "So here the kind of a key idea\nis that you can really go wild.",
    "start": "429190",
    "end": "433410"
  },
  {
    "text": "You can throw in\nany sort of features",
    "start": "433410",
    "end": "435060"
  },
  {
    "text": "you want and get all sorts\nof wacky-looking predictors,",
    "start": "435060",
    "end": "439139"
  },
  {
    "text": "all using the machinery\nof a linear predictor.",
    "start": "439140",
    "end": "443270"
  },
  {
    "start": "442000",
    "end": "540000"
  },
  {
    "text": "So you might say, wait\na minute, wait a minute,",
    "start": "443270",
    "end": "445960"
  },
  {
    "text": "how were we able to do this?",
    "start": "445960",
    "end": "448360"
  },
  {
    "text": "If all this expressive\nnon-linear capabilities,",
    "start": "448360",
    "end": "451039"
  },
  {
    "text": "when we haven't really changed\nthe learning algorithm or it's",
    "start": "451040",
    "end": "454420"
  },
  {
    "text": "still supposed to be a\nlinear predictor, right?",
    "start": "454420",
    "end": "457990"
  },
  {
    "text": "Well, that's because the\nword linear is a little bit",
    "start": "457990",
    "end": "461740"
  },
  {
    "text": "ambiguous here.",
    "start": "461740",
    "end": "463470"
  },
  {
    "text": "So remember, the prediction is\nw dot phi x, so that's a score.",
    "start": "463470",
    "end": "468490"
  },
  {
    "text": "And the question\nis, linear in what?",
    "start": "468490",
    "end": "471610"
  },
  {
    "text": "So is a score linear in w?",
    "start": "471610",
    "end": "474669"
  },
  {
    "text": "Yes, because the score is\njust some constant times w.",
    "start": "474670",
    "end": "478720"
  },
  {
    "text": "Is the linear in phi of x?",
    "start": "478720",
    "end": "480670"
  },
  {
    "text": "Yes, because it's\nsomething times phi of x.",
    "start": "480670",
    "end": "486450"
  },
  {
    "text": "How about, is it linear in x?",
    "start": "486450",
    "end": "489150"
  },
  {
    "text": "Well, the answer is no, because\nphi of x can be arbitrary.",
    "start": "489150",
    "end": "492320"
  },
  {
    "text": "So it doesn't have\nto be linear x.",
    "start": "492320",
    "end": "494940"
  },
  {
    "text": "And the key idea\nbehind non-linearity",
    "start": "494940",
    "end": "497010"
  },
  {
    "text": "is that there's two\nways of viewing it.",
    "start": "497010",
    "end": "498990"
  },
  {
    "text": "From the point of view of\ngaining expressive non-linear",
    "start": "498990",
    "end": "501840"
  },
  {
    "text": "predictors, this is\ngreat because you",
    "start": "501840",
    "end": "503639"
  },
  {
    "text": "can define phi of x to\nbe something and get",
    "start": "503640",
    "end": "506820"
  },
  {
    "text": "arbitrary non-linear\nfunctions on.",
    "start": "506820",
    "end": "509820"
  },
  {
    "text": "But from the point of view of\nhaving to learn such a model,",
    "start": "509820",
    "end": "514400"
  },
  {
    "text": "it's actually great\nbecause the score",
    "start": "514400",
    "end": "516260"
  },
  {
    "text": "is a linear function of w.",
    "start": "516260",
    "end": "518890"
  },
  {
    "text": "And when you're learning, you\ntake the gradient with respect",
    "start": "518890",
    "end": "521390"
  },
  {
    "text": "to w, so it's just a--",
    "start": "521390",
    "end": "522979"
  },
  {
    "text": "a score is just a linear\nfunction, so life is great.",
    "start": "522980",
    "end": "526699"
  },
  {
    "text": "In fact, the learning algorithm\ndoesn't even care what phi is.",
    "start": "526700",
    "end": "529340"
  },
  {
    "text": "It only looks at the data\nthrough the lens of phi of x.",
    "start": "529340",
    "end": "532220"
  },
  {
    "text": "It doesn't know whether you\ngave it x and then applied phi",
    "start": "532220",
    "end": "535490"
  },
  {
    "text": "or you just gave it\nphi of x directly.",
    "start": "535490",
    "end": "537200"
  },
  {
    "start": "537200",
    "end": "540740"
  },
  {
    "start": "540000",
    "end": "605000"
  },
  {
    "text": "OK.",
    "start": "540740",
    "end": "541240"
  },
  {
    "text": "So now, let's turn from\nregression to classification.",
    "start": "541240",
    "end": "544630"
  },
  {
    "text": "The story is pretty\nmuch the same.",
    "start": "544630",
    "end": "547305"
  },
  {
    "text": "You can define\narbitrary features",
    "start": "547305",
    "end": "548680"
  },
  {
    "text": "and get non-linear classifiers.",
    "start": "548680",
    "end": "551080"
  },
  {
    "text": "But just a kind of\nreview, remember,",
    "start": "551080",
    "end": "553595"
  },
  {
    "text": "on linear classification,\nyou defined two dimensions.",
    "start": "553595",
    "end": "557110"
  },
  {
    "text": "You defined the feature\nvector to be x1, x2.",
    "start": "557110",
    "end": "560950"
  },
  {
    "text": "And if you define the\npredictor as now a sign here,",
    "start": "560950",
    "end": "567040"
  },
  {
    "text": "and the sign allows you\nto define this decision",
    "start": "567040",
    "end": "571750"
  },
  {
    "text": "boundary, which separates the\nregion of the space, which",
    "start": "571750",
    "end": "575325"
  },
  {
    "text": "is labeled plus from the\nregion of the space, which",
    "start": "575325",
    "end": "577450"
  },
  {
    "text": "is labeled minus, OK?",
    "start": "577450",
    "end": "579850"
  },
  {
    "text": "So now, what does\nnonlinear mean?",
    "start": "579850",
    "end": "581630"
  },
  {
    "text": "Well, if you look at f of x,\nbecause of the sign function,",
    "start": "581630",
    "end": "584290"
  },
  {
    "text": "it's already non-linear, so\nit doesn't really make sense.",
    "start": "584290",
    "end": "587060"
  },
  {
    "text": "So instead, non-linearity\nfor a classification",
    "start": "587060",
    "end": "590290"
  },
  {
    "text": "means whether the decision\nboundary is linear or not.",
    "start": "590290",
    "end": "594699"
  },
  {
    "text": "In particular, here\nit is as a line.",
    "start": "594700",
    "end": "597550"
  },
  {
    "text": "And if we define the\nfeature vector as x1,",
    "start": "597550",
    "end": "600180"
  },
  {
    "text": "x2, then we just get a line.",
    "start": "600180",
    "end": "601740"
  },
  {
    "start": "601740",
    "end": "604580"
  },
  {
    "text": "So now, let's try to do\nsomething a little bit more",
    "start": "604580",
    "end": "607550"
  },
  {
    "start": "605000",
    "end": "726000"
  },
  {
    "text": "interesting.",
    "start": "607550",
    "end": "608779"
  },
  {
    "text": "So let's see if we can define\na quadratic classifier.",
    "start": "608780",
    "end": "611600"
  },
  {
    "text": "Suppose, we wanted to\ndefine a classifier that",
    "start": "611600",
    "end": "615319"
  },
  {
    "text": "looks like this.",
    "start": "615320",
    "end": "616470"
  },
  {
    "text": "So it's a circle.",
    "start": "616470",
    "end": "617629"
  },
  {
    "text": "The decision\nboundary is a circle,",
    "start": "617630",
    "end": "619280"
  },
  {
    "text": "where inside the circle,\nwe want to label as plus,",
    "start": "619280",
    "end": "623000"
  },
  {
    "text": "and outside, we want\nto label as minus.",
    "start": "623000",
    "end": "626480"
  },
  {
    "text": "OK, so how are we\ngoing to do that?",
    "start": "626480",
    "end": "629170"
  },
  {
    "text": "Well, let's start\nwith a feature vector",
    "start": "629170",
    "end": "631920"
  },
  {
    "text": "equals x1, x2, as we had before.",
    "start": "631920",
    "end": "636310"
  },
  {
    "text": "And now, we're just going to\ntack on a quadratic term--",
    "start": "636310",
    "end": "639630"
  },
  {
    "text": "x1 squared plus x2 squared, OK?",
    "start": "639630",
    "end": "644750"
  },
  {
    "text": "And now, if you define the\ncorresponding weight vector",
    "start": "644750",
    "end": "647690"
  },
  {
    "text": "to be 2, 2, minus\n1, then I claim",
    "start": "647690",
    "end": "650750"
  },
  {
    "text": "that this gives you\nexactly this decision",
    "start": "650750",
    "end": "653570"
  },
  {
    "text": "boundary, which is a circle.",
    "start": "653570",
    "end": "656950"
  },
  {
    "text": "So there's some algebra\nthat you can do,",
    "start": "656950",
    "end": "658780"
  },
  {
    "text": "which I'm going to skip over.",
    "start": "658780",
    "end": "661210"
  },
  {
    "text": "But what you can do is, you\ncan rewrite this expression",
    "start": "661210",
    "end": "664440"
  },
  {
    "text": "as follows.",
    "start": "664440",
    "end": "666440"
  },
  {
    "text": "So f of x, the same\nf of x is equal to 1",
    "start": "666440",
    "end": "670600"
  },
  {
    "text": "if this quadratic form is\nless than or equal to 2.",
    "start": "670600",
    "end": "674079"
  },
  {
    "text": "So what is this?",
    "start": "674080",
    "end": "674920"
  },
  {
    "text": "You might remember from\nalgebra or trigonometry days",
    "start": "674920",
    "end": "679300"
  },
  {
    "text": "that this is the\nsquared distance",
    "start": "679300",
    "end": "682450"
  },
  {
    "text": "of a point to the 0.11, OK?",
    "start": "682450",
    "end": "685870"
  },
  {
    "text": "So in particular, if I\nconstrain the squared distance",
    "start": "685870",
    "end": "689080"
  },
  {
    "text": "to be less, we go\nto 2, then this",
    "start": "689080",
    "end": "691720"
  },
  {
    "text": "is the region of points within\nradius of square root of 2",
    "start": "691720",
    "end": "698230"
  },
  {
    "text": "of a circle, centered at 1, 1,\nwhich is exactly what this is.",
    "start": "698230",
    "end": "703649"
  },
  {
    "text": "And everything else is\nclassified as minus 1.",
    "start": "703650",
    "end": "708100"
  },
  {
    "text": "The decision the boundary we\ngot successfully to be a circle.",
    "start": "708100",
    "end": "711790"
  },
  {
    "start": "711790",
    "end": "714670"
  },
  {
    "text": "OK, so let me try to take one\nmore step, to try to reconcile",
    "start": "714670",
    "end": "720279"
  },
  {
    "text": "this tension between linear in\nphi of x and non-linear in x.",
    "start": "720280",
    "end": "726460"
  },
  {
    "start": "726000",
    "end": "790000"
  },
  {
    "text": "So what we're going to\ndo here is, remember,",
    "start": "726460",
    "end": "729430"
  },
  {
    "text": "the input space x, this\ndecision boundary is a circle.",
    "start": "729430",
    "end": "735050"
  },
  {
    "text": "And in feature space, you can\nsee that the decision boundary",
    "start": "735050",
    "end": "738940"
  },
  {
    "text": "is a line.",
    "start": "738940",
    "end": "740690"
  },
  {
    "text": "So here is a cool\nanimation that I",
    "start": "740690",
    "end": "743680"
  },
  {
    "text": "found on YouTube,\nwhich, I think,",
    "start": "743680",
    "end": "745779"
  },
  {
    "text": "really nicely illustrates this.",
    "start": "745780",
    "end": "748073"
  },
  {
    "text": "So this is done in a context\nof asking if the value of phi",
    "start": "748073",
    "end": "750490"
  },
  {
    "text": "is the same.",
    "start": "750490",
    "end": "751130"
  },
  {
    "text": "So here, we have points\ninside the circle",
    "start": "751130",
    "end": "754242"
  },
  {
    "text": "and outside the circle.",
    "start": "754242",
    "end": "755200"
  },
  {
    "text": "In the ambient x space,\nthey're not separable.",
    "start": "755200",
    "end": "759340"
  },
  {
    "text": "But what we're\ngoing to do is we're",
    "start": "759340",
    "end": "760960"
  },
  {
    "text": "going to apply the feature map.",
    "start": "760960",
    "end": "762520"
  },
  {
    "text": "And the feature map, remember,\nadds this third dimension--",
    "start": "762520",
    "end": "765970"
  },
  {
    "text": "x1 squared plus x2 squared.",
    "start": "765970",
    "end": "767810"
  },
  {
    "text": "And now, we're in feature\nspace, which is 3D.",
    "start": "767810",
    "end": "771010"
  },
  {
    "text": "And in 3D, we can actually\nslice a linear predictor",
    "start": "771010",
    "end": "774550"
  },
  {
    "text": "that separates the red\nand the blue points,",
    "start": "774550",
    "end": "779010"
  },
  {
    "text": "and that separation induces\na circle in the original 2D",
    "start": "779010",
    "end": "785110"
  },
  {
    "text": "space.",
    "start": "785110",
    "end": "785610"
  },
  {
    "start": "785610",
    "end": "790700"
  },
  {
    "start": "790000",
    "end": "845000"
  },
  {
    "text": "OK, to summarize, so\nlinear is ambiguous.",
    "start": "790700",
    "end": "795990"
  },
  {
    "text": "So we have a predictor\nin a case of regression,",
    "start": "795990",
    "end": "799980"
  },
  {
    "text": "which is w dot phi of x.",
    "start": "799980",
    "end": "801829"
  },
  {
    "text": "It's linear in w of phi of\nx, but it's non-linear in x.",
    "start": "801830",
    "end": "806030"
  },
  {
    "text": "And this is what allows us to\nget non-linear predictors using",
    "start": "806030",
    "end": "808970"
  },
  {
    "text": "the machinery of a\nlinear predictor.",
    "start": "808970",
    "end": "811370"
  },
  {
    "text": "We saw for regression,\na non-linearity talks",
    "start": "811370",
    "end": "813732"
  },
  {
    "text": "about the predictor directly.",
    "start": "813732",
    "end": "814940"
  },
  {
    "text": "And in classification, we talk\nabout the decision boundary.",
    "start": "814940",
    "end": "818390"
  },
  {
    "text": "We also saw many types\nof non-linear features,",
    "start": "818390",
    "end": "820760"
  },
  {
    "text": "quadratic features, piecewise\nconstant, periodic features.",
    "start": "820760",
    "end": "824090"
  },
  {
    "text": "And again, you can kind of\nmake up your own features",
    "start": "824090",
    "end": "826790"
  },
  {
    "text": "for the application\nyou have in mind.",
    "start": "826790",
    "end": "830310"
  },
  {
    "text": "So next time someone\non the street",
    "start": "830310",
    "end": "832370"
  },
  {
    "text": "asks you about\nlinear predictors,",
    "start": "832370",
    "end": "834260"
  },
  {
    "text": "you first have to\nclarify, linear in what?",
    "start": "834260",
    "end": "838200"
  },
  {
    "text": "OK, that's the end.",
    "start": "838200",
    "end": "840080"
  },
  {
    "start": "840080",
    "end": "845000"
  }
]