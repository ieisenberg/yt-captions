[
  {
    "start": "0",
    "end": "4950"
  },
  {
    "text": "Welcome back, everyone.",
    "start": "4950",
    "end": "6100"
  },
  {
    "text": "This is part 2 in our series\non methods and metrics.",
    "start": "6100",
    "end": "9090"
  },
  {
    "text": "In part 1, I introduced\nsome really high-level",
    "start": "9090",
    "end": "11700"
  },
  {
    "text": "overarching themes around\nmethods and metrics.",
    "start": "11700",
    "end": "14700"
  },
  {
    "text": "We're now going to do a deep\ntechnical dive on classifier",
    "start": "14700",
    "end": "17910"
  },
  {
    "text": "metrics because I\nimagine many of you",
    "start": "17910",
    "end": "20160"
  },
  {
    "text": "will be dealing with classifiers\nfor your final project.",
    "start": "20160",
    "end": "23160"
  },
  {
    "text": "Even though this is a\ndeep technical dive,",
    "start": "23160",
    "end": "25170"
  },
  {
    "text": "I'd like to keep our eyes\non those high level themes.",
    "start": "25170",
    "end": "28950"
  },
  {
    "text": "In fact, let's start\nthere with an overview",
    "start": "28950",
    "end": "31290"
  },
  {
    "text": "of thinking about this.",
    "start": "31290",
    "end": "32940"
  },
  {
    "text": "Carrying over a lesson\nfrom the first screencast,",
    "start": "32940",
    "end": "35530"
  },
  {
    "text": "I would emphasize that\ndifferent evaluation metrics",
    "start": "35530",
    "end": "38129"
  },
  {
    "text": "encode different values.",
    "start": "38130",
    "end": "40600"
  },
  {
    "text": "And that means that\nchoosing a metric",
    "start": "40600",
    "end": "42420"
  },
  {
    "text": "is a crucial aspect of\nyour experimental work.",
    "start": "42420",
    "end": "45370"
  },
  {
    "text": "You need to think about your\nhypotheses and your data",
    "start": "45370",
    "end": "48690"
  },
  {
    "text": "and your models and\nhow all of those",
    "start": "48690",
    "end": "50789"
  },
  {
    "text": "come together to inform\ngood choices around metrics,",
    "start": "50790",
    "end": "54060"
  },
  {
    "text": "even if you are fitting\nsomething as seemingly",
    "start": "54060",
    "end": "56790"
  },
  {
    "text": "simple as a classifier.",
    "start": "56790",
    "end": "58830"
  },
  {
    "text": "You should feel free to motivate\nnew metrics and specific uses",
    "start": "58830",
    "end": "63000"
  },
  {
    "text": "of existing metrics depending\non what your goals are.",
    "start": "63000",
    "end": "66243"
  },
  {
    "text": "That's something\nthat I emphasized",
    "start": "66243",
    "end": "67659"
  },
  {
    "text": "in part 1 of this series\nthat we should really",
    "start": "67660",
    "end": "70540"
  },
  {
    "text": "be thinking about how our\nmetrics align with the things",
    "start": "70540",
    "end": "73540"
  },
  {
    "text": "that we're actually\ntrying to do.",
    "start": "73540",
    "end": "75850"
  },
  {
    "text": "For established tasks, I\ngrant that there will usually",
    "start": "75850",
    "end": "79030"
  },
  {
    "text": "be pressure to use specific\nmetrics, the ones that",
    "start": "79030",
    "end": "82299"
  },
  {
    "text": "are in the leaderboard or\nin the prior literature,",
    "start": "82300",
    "end": "84880"
  },
  {
    "text": "but you should feel empowered\nto push back if you feel",
    "start": "84880",
    "end": "87871"
  },
  {
    "text": "that's the right thing to do.",
    "start": "87872",
    "end": "89080"
  },
  {
    "text": "After all, areas of research can\nstagnate due to poor metrics.",
    "start": "89080",
    "end": "93550"
  },
  {
    "text": "So we all have to be\nvigilant and push back",
    "start": "93550",
    "end": "96310"
  },
  {
    "text": "if we think that a metric\nis just leading us astray.",
    "start": "96310",
    "end": "101799"
  },
  {
    "text": "In that spirit, let's start\nwith confusion matrices,",
    "start": "101800",
    "end": "105370"
  },
  {
    "text": "the basis for many\ncalculations in this area.",
    "start": "105370",
    "end": "108250"
  },
  {
    "text": "As a running example I'm going\nto use a ternary sentiment",
    "start": "108250",
    "end": "111310"
  },
  {
    "text": "problem.",
    "start": "111310",
    "end": "112189"
  },
  {
    "text": "I'll have the gold labels across\nthe rows and predictions going",
    "start": "112190",
    "end": "116440"
  },
  {
    "text": "down the columns.",
    "start": "116440",
    "end": "117530"
  },
  {
    "text": "And so this confusion matrix\nis saying, for example,",
    "start": "117530",
    "end": "120020"
  },
  {
    "text": "that there were 15 cases\nthat were gold positive",
    "start": "120020",
    "end": "123670"
  },
  {
    "text": "and that the system predicted\nas positive, whereas there",
    "start": "123670",
    "end": "127600"
  },
  {
    "text": "are hundreds cases\nthat were gold positive",
    "start": "127600",
    "end": "129880"
  },
  {
    "text": "and the system\npredicted neutral.",
    "start": "129880",
    "end": "132670"
  },
  {
    "text": "In the spirit of taking\nnothing for granted,",
    "start": "132670",
    "end": "134990"
  },
  {
    "text": "let me emphasize for you\nthat a threshold was likely",
    "start": "134990",
    "end": "137800"
  },
  {
    "text": "imposed for these categorical\npredictions coming",
    "start": "137800",
    "end": "140350"
  },
  {
    "text": "from the model,\nespecially if you",
    "start": "140350",
    "end": "142300"
  },
  {
    "text": "have a probabilistic\nclassifier, what you got out",
    "start": "142300",
    "end": "144700"
  },
  {
    "text": "was a probability distribution\nover the three classes",
    "start": "144700",
    "end": "148360"
  },
  {
    "text": "in this case, and you\napplied some decision rule",
    "start": "148360",
    "end": "151420"
  },
  {
    "text": "to figure out which one would\ncount as the actual prediction.",
    "start": "151420",
    "end": "154910"
  },
  {
    "text": "So obviously, different\ndecision rules",
    "start": "154910",
    "end": "157090"
  },
  {
    "text": "will give you very\ndifferent tables of results.",
    "start": "157090",
    "end": "159920"
  },
  {
    "text": "And so in the\nbackground, you should",
    "start": "159920",
    "end": "161590"
  },
  {
    "text": "have in mind that is\nan ingredient here.",
    "start": "161590",
    "end": "163660"
  },
  {
    "text": "And in fact, at the\nend of the slideshow,",
    "start": "163660",
    "end": "165730"
  },
  {
    "text": "I'll suggest a metric that\nallows you to pull back",
    "start": "165730",
    "end": "168580"
  },
  {
    "text": "from that assumption.",
    "start": "168580",
    "end": "170920"
  },
  {
    "text": "Another thing that's\nworth keeping track of",
    "start": "170920",
    "end": "172810"
  },
  {
    "text": "is the support, that\nis the number of cases",
    "start": "172810",
    "end": "175450"
  },
  {
    "text": "that for the gold data fall\ninto each one of the categories.",
    "start": "175450",
    "end": "179290"
  },
  {
    "text": "Here you can see that\nit is highly imbalanced",
    "start": "179290",
    "end": "181840"
  },
  {
    "text": "and you should have in mind\nthat will be an important factor",
    "start": "181840",
    "end": "185170"
  },
  {
    "text": "in choosing a good metric.",
    "start": "185170",
    "end": "188500"
  },
  {
    "text": "Let's begin with accuracy.",
    "start": "188500",
    "end": "190910"
  },
  {
    "text": "Accuracy is the\ncorrect predictions",
    "start": "190910",
    "end": "192730"
  },
  {
    "text": "divided by the total\nnumber of examples.",
    "start": "192730",
    "end": "195519"
  },
  {
    "text": "Given a confusion\ntable like this,",
    "start": "195520",
    "end": "197560"
  },
  {
    "text": "that means that we sum up\nall the diagonal elements",
    "start": "197560",
    "end": "200230"
  },
  {
    "text": "and divide that by the sum of\nall the elements in the table.",
    "start": "200230",
    "end": "203590"
  },
  {
    "text": "That's accuracy.",
    "start": "203590",
    "end": "205030"
  },
  {
    "text": "The balance for accuracy are\n0 and 1, with 0 the worst,",
    "start": "205030",
    "end": "208630"
  },
  {
    "text": "and 1 the best.",
    "start": "208630",
    "end": "210220"
  },
  {
    "text": "The value encoded in accuracy\nis just in the simplest terms--",
    "start": "210220",
    "end": "215230"
  },
  {
    "text": "how often is the system correct?",
    "start": "215230",
    "end": "218790"
  },
  {
    "text": "That actually relates\nto two weaknesses.",
    "start": "218790",
    "end": "220890"
  },
  {
    "text": "First, there is no\nper-class metric.",
    "start": "220890",
    "end": "223170"
  },
  {
    "text": "We have to do this\nover the entire table.",
    "start": "223170",
    "end": "225750"
  },
  {
    "text": "Second, we have a\ncomplete failure",
    "start": "225750",
    "end": "228000"
  },
  {
    "text": "to control for class size.",
    "start": "228000",
    "end": "230130"
  },
  {
    "text": "Think about that value\nencoded-- how often is",
    "start": "230130",
    "end": "232590"
  },
  {
    "text": "the system correct?",
    "start": "232590",
    "end": "233489"
  },
  {
    "text": "That is insensitive to\nthe different classes",
    "start": "233490",
    "end": "236730"
  },
  {
    "text": "that you have in your\nsystem and the way",
    "start": "236730",
    "end": "239340"
  },
  {
    "text": "it makes predictions\nfor those classes.",
    "start": "239340",
    "end": "241050"
  },
  {
    "text": "It is just looking at\nthe raw number of times",
    "start": "241050",
    "end": "243870"
  },
  {
    "text": "that you made the right guess.",
    "start": "243870",
    "end": "245879"
  },
  {
    "text": "And actually, our table\nis a good illustration",
    "start": "245880",
    "end": "248220"
  },
  {
    "text": "of how this can be problematic.",
    "start": "248220",
    "end": "249990"
  },
  {
    "text": "Essentially, all of the\ntrue cases are neutral",
    "start": "249990",
    "end": "252840"
  },
  {
    "text": "and essentially all of the\npredictions are neutral",
    "start": "252840",
    "end": "255540"
  },
  {
    "text": "and as a result,\nit hardly matters",
    "start": "255540",
    "end": "257729"
  },
  {
    "text": "what you do for this system in\nterms of positive and negative",
    "start": "257730",
    "end": "260790"
  },
  {
    "text": "because accuracy will be\ntotally dominated by performance",
    "start": "260790",
    "end": "264930"
  },
  {
    "text": "on that neutral category.",
    "start": "264930",
    "end": "266759"
  },
  {
    "text": "That could be good.",
    "start": "266760",
    "end": "268050"
  },
  {
    "text": "It's giving us a picture\nof how your system performs",
    "start": "268050",
    "end": "270539"
  },
  {
    "text": "on the most frequent case,\nand it will reflect the value",
    "start": "270540",
    "end": "273600"
  },
  {
    "text": "that I've suggested here, but\nit might be directly at odds",
    "start": "273600",
    "end": "277350"
  },
  {
    "text": "with our goals of really\ndoing well on even",
    "start": "277350",
    "end": "280300"
  },
  {
    "text": "the smallest categories.",
    "start": "280300",
    "end": "283190"
  },
  {
    "text": "Suppose that you do have\na goal of doing well",
    "start": "283190",
    "end": "286460"
  },
  {
    "text": "even on the small categories.",
    "start": "286460",
    "end": "288289"
  },
  {
    "text": "I'm going to offer you\nsome metrics for that.",
    "start": "288290",
    "end": "290820"
  },
  {
    "text": "But one thing you should\nkeep in mind, again,",
    "start": "290820",
    "end": "293330"
  },
  {
    "text": "is that if you are using\na cross entropy loss,",
    "start": "293330",
    "end": "296449"
  },
  {
    "text": "you are implicitly optimizing\nyour model for accuracy",
    "start": "296450",
    "end": "299900"
  },
  {
    "text": "because accuracy is\ninversely proportional",
    "start": "299900",
    "end": "303139"
  },
  {
    "text": "to the negative log loss.",
    "start": "303140",
    "end": "305030"
  },
  {
    "text": "That is, the cross entropy loss.",
    "start": "305030",
    "end": "307260"
  },
  {
    "text": "So you might set\ngoals for yourself",
    "start": "307260",
    "end": "309290"
  },
  {
    "text": "that are like good macro F1.",
    "start": "309290",
    "end": "311300"
  },
  {
    "text": "That's a metric I'll introduce\nin a second, but keep in mind",
    "start": "311300",
    "end": "314449"
  },
  {
    "text": "that your system is actually\noriented toward accuracy.",
    "start": "314450",
    "end": "317690"
  },
  {
    "text": "And that will have consequences.",
    "start": "317690",
    "end": "319230"
  },
  {
    "text": "For example,\noptimization processes",
    "start": "319230",
    "end": "321410"
  },
  {
    "text": "tend to favor the\nlargest classes.",
    "start": "321410",
    "end": "324360"
  },
  {
    "text": "And this is a picture of why\nthat happens for classifiers.",
    "start": "324360",
    "end": "329120"
  },
  {
    "text": "One other technical\nnote that I wanted",
    "start": "329120",
    "end": "330889"
  },
  {
    "text": "to make-- the cross\nentropy loss is actually",
    "start": "330890",
    "end": "333230"
  },
  {
    "text": "a special case of the\nKL-divergence loss,",
    "start": "333230",
    "end": "336170"
  },
  {
    "text": "and that's kind of\naccuracy for soft labels",
    "start": "336170",
    "end": "338870"
  },
  {
    "text": "where you have a\nfull probability",
    "start": "338870",
    "end": "340580"
  },
  {
    "text": "distribution over the classes.",
    "start": "340580",
    "end": "342919"
  },
  {
    "text": "The reason we\noften simplify this",
    "start": "342920",
    "end": "344660"
  },
  {
    "text": "away is that typically\nfor classifiers, we",
    "start": "344660",
    "end": "346790"
  },
  {
    "text": "have a one hot vector.",
    "start": "346790",
    "end": "348410"
  },
  {
    "text": "There's exactly one label\ndimension that is true.",
    "start": "348410",
    "end": "351510"
  },
  {
    "text": "And that means that for all\nother classes, the false ones",
    "start": "351510",
    "end": "355190"
  },
  {
    "text": "this ends up being a\ntotal of 0, and that",
    "start": "355190",
    "end": "358100"
  },
  {
    "text": "means we can simplify it\ndown to the negative log",
    "start": "358100",
    "end": "361010"
  },
  {
    "text": "of the true class.",
    "start": "361010",
    "end": "362540"
  },
  {
    "text": "And that's how you get back to\nthese standard formulations,",
    "start": "362540",
    "end": "366620"
  },
  {
    "text": "but this is the\ngeneral formulation",
    "start": "366620",
    "end": "368360"
  },
  {
    "text": "and you can in principle\nlearn from distributions",
    "start": "368360",
    "end": "370729"
  },
  {
    "text": "over the labels that you have.",
    "start": "370730",
    "end": "373400"
  },
  {
    "text": "And that will be fundamentally\nthe same kind of operation",
    "start": "373400",
    "end": "376040"
  },
  {
    "text": "with the same inbuilt biases.",
    "start": "376040",
    "end": "379980"
  },
  {
    "text": "But we do want to move\naway from raw accuracy.",
    "start": "379980",
    "end": "382740"
  },
  {
    "text": "And the first step to\ndoing that is precision.",
    "start": "382740",
    "end": "385590"
  },
  {
    "text": "The precision for a class k\nis the correct predictions",
    "start": "385590",
    "end": "389040"
  },
  {
    "text": "for k divided by the sum\nof all guesses for k.",
    "start": "389040",
    "end": "392580"
  },
  {
    "text": "So we're going to kind of\noperate column-wise here.",
    "start": "392580",
    "end": "395520"
  },
  {
    "text": "And here I've shown you the\ncalculation for precision",
    "start": "395520",
    "end": "398220"
  },
  {
    "text": "for the positive class, and we\ncould do similar calculations",
    "start": "398220",
    "end": "401790"
  },
  {
    "text": "for the negative\nand neutral classes.",
    "start": "401790",
    "end": "404040"
  },
  {
    "text": "The bounds of\nprecision are 0 and 1,",
    "start": "404040",
    "end": "406380"
  },
  {
    "text": "with 0 the worst, and 1 the\nbest, with a small caveat",
    "start": "406380",
    "end": "409770"
  },
  {
    "text": "that precision is\nundefined for cases",
    "start": "409770",
    "end": "412379"
  },
  {
    "text": "where you would\nneed to divide by 0.",
    "start": "412380",
    "end": "414330"
  },
  {
    "text": "So we just map those\nto 0, typically.",
    "start": "414330",
    "end": "416669"
  },
  {
    "text": "And sometimes, if\nyou're using Scikit,",
    "start": "416670",
    "end": "418560"
  },
  {
    "text": "you see lots of\nwarnings about metrics",
    "start": "418560",
    "end": "421320"
  },
  {
    "text": "when you encounter this case.",
    "start": "421320",
    "end": "423820"
  },
  {
    "text": "The value encoded is\npenalizing incorrect guesses",
    "start": "423820",
    "end": "428190"
  },
  {
    "text": "and that leads directly\nto the weakness.",
    "start": "428190",
    "end": "430320"
  },
  {
    "text": "You can achieve high\nprecision for a class k",
    "start": "430320",
    "end": "433080"
  },
  {
    "text": "simply by rarely guessing k.",
    "start": "433080",
    "end": "436349"
  },
  {
    "text": "If you just make sure you're\nvery cautious about this class,",
    "start": "436350",
    "end": "440300"
  },
  {
    "text": "you will get high\nprecision in all likelihood",
    "start": "440300",
    "end": "442930"
  },
  {
    "text": "but that's not necessarily\nthe full set of values",
    "start": "442930",
    "end": "445840"
  },
  {
    "text": "we want to encode.",
    "start": "445840",
    "end": "447430"
  },
  {
    "text": "So typically we balance\nthat with recall.",
    "start": "447430",
    "end": "451090"
  },
  {
    "text": "The recall for class k is\nthe correct predictions",
    "start": "451090",
    "end": "454090"
  },
  {
    "text": "for k divided by the sum\nof all true members of k.",
    "start": "454090",
    "end": "457270"
  },
  {
    "text": "So here we're going\nto operate row wise,",
    "start": "457270",
    "end": "459490"
  },
  {
    "text": "and I've given you\nthe sample calculation",
    "start": "459490",
    "end": "461620"
  },
  {
    "text": "for the positive class.",
    "start": "461620",
    "end": "463600"
  },
  {
    "text": "The bounds are 0 and 1, with\n0 the worst, and 1 the best.",
    "start": "463600",
    "end": "467200"
  },
  {
    "text": "The value encoded\nis that we're going",
    "start": "467200",
    "end": "469690"
  },
  {
    "text": "to penalize missed true cases.",
    "start": "469690",
    "end": "472580"
  },
  {
    "text": "So it is a kind of\ndual of precision,",
    "start": "472580",
    "end": "476110"
  },
  {
    "text": "and that encodes its\nweakness as well.",
    "start": "476110",
    "end": "478300"
  },
  {
    "text": "We can achieve high recall for\nk simply by always guessing k.",
    "start": "478300",
    "end": "483190"
  },
  {
    "text": "If I want to be sure I\ndon't miss any examples,",
    "start": "483190",
    "end": "485320"
  },
  {
    "text": "I'll just guess constantly\nand increase my chances",
    "start": "485320",
    "end": "487990"
  },
  {
    "text": "of not having any misses.",
    "start": "487990",
    "end": "489849"
  },
  {
    "text": "And now you can\nsee very directly",
    "start": "489850",
    "end": "491523"
  },
  {
    "text": "that we should balance this\nagainst precision, which",
    "start": "491523",
    "end": "493690"
  },
  {
    "text": "is imposing the opposite value.",
    "start": "493690",
    "end": "495970"
  },
  {
    "text": "And that's the usual\nmotivation for F scores.",
    "start": "495970",
    "end": "499420"
  },
  {
    "text": "Usually F1 scores, but\nwe can, in principle,",
    "start": "499420",
    "end": "502150"
  },
  {
    "text": "have this weight\nbeta, which will",
    "start": "502150",
    "end": "504130"
  },
  {
    "text": "control the degree to which\nwe favor precision and recall.",
    "start": "504130",
    "end": "507700"
  },
  {
    "text": "And again, no need\nto go on autopilot.",
    "start": "507700",
    "end": "510370"
  },
  {
    "text": "There are scenarios where\nprecision is important",
    "start": "510370",
    "end": "512770"
  },
  {
    "text": "and scenarios where\nrecall is important",
    "start": "512770",
    "end": "514900"
  },
  {
    "text": "and you could use beta\nto align your metrics",
    "start": "514900",
    "end": "517990"
  },
  {
    "text": "with those high level\nvalues that you have.",
    "start": "517990",
    "end": "520839"
  },
  {
    "text": "But by default, it's one\nwhich is an even balance.",
    "start": "520840",
    "end": "523549"
  },
  {
    "text": "And what we're doing is\nsimply the harmonic mean",
    "start": "523549",
    "end": "526060"
  },
  {
    "text": "of precision and recall.",
    "start": "526060",
    "end": "528279"
  },
  {
    "text": "This can be a per-class\nnotion, so I've",
    "start": "528280",
    "end": "530710"
  },
  {
    "text": "given the F1 scores\nalong the rows",
    "start": "530710",
    "end": "532930"
  },
  {
    "text": "for each one of those classes.",
    "start": "532930",
    "end": "535730"
  },
  {
    "text": "The bounds are 0 and 1, with\n0 the worst and 1 the best,",
    "start": "535730",
    "end": "538720"
  },
  {
    "text": "and this is always going\nto be between precision",
    "start": "538720",
    "end": "541389"
  },
  {
    "text": "and recall as the harmonic\nmean of those two.",
    "start": "541390",
    "end": "544820"
  },
  {
    "text": "The value encoded is\nsomething like this.",
    "start": "544820",
    "end": "547040"
  },
  {
    "text": "How much do the\npredictions for class k",
    "start": "547040",
    "end": "549980"
  },
  {
    "text": "align with true\ninstances of k, with beta",
    "start": "549980",
    "end": "553670"
  },
  {
    "text": "controlling the weight placed\non precision and recall?",
    "start": "553670",
    "end": "556470"
  },
  {
    "text": "So it's kind of like both\nprecision and recall have",
    "start": "556470",
    "end": "559490"
  },
  {
    "text": "been baked into this notion\nof aligning with the truth.",
    "start": "559490",
    "end": "563450"
  },
  {
    "text": "The weaknesses.",
    "start": "563450",
    "end": "564800"
  },
  {
    "text": "There's no normalization for\nthe size of the data set,",
    "start": "564800",
    "end": "568519"
  },
  {
    "text": "and it ignores all the values\noff the row and column for k.",
    "start": "568520",
    "end": "572540"
  },
  {
    "text": "If I'm doing the F1\nfor the positive class,",
    "start": "572540",
    "end": "575870"
  },
  {
    "text": "I don't pay attention to\nany of these other values",
    "start": "575870",
    "end": "578300"
  },
  {
    "text": "no matter how many\nexamples there",
    "start": "578300",
    "end": "580399"
  },
  {
    "text": "are in those off elements.",
    "start": "580400",
    "end": "583140"
  },
  {
    "text": "And so that's a kind\nof structural bias",
    "start": "583140",
    "end": "585050"
  },
  {
    "text": "that gets built in\nhere kind of a place",
    "start": "585050",
    "end": "587570"
  },
  {
    "text": "that these metrics miss\nwhen you think per class.",
    "start": "587570",
    "end": "592180"
  },
  {
    "text": "We can average F scores\nin multiple ways.",
    "start": "592180",
    "end": "595270"
  },
  {
    "text": "I'm going to talk about\nthree macro-averaging,",
    "start": "595270",
    "end": "597820"
  },
  {
    "text": "weighted averaging,\nand micro-averaging.",
    "start": "597820",
    "end": "600700"
  },
  {
    "text": "Let's start with macro.",
    "start": "600700",
    "end": "602030"
  },
  {
    "text": "This is the most dominant\nchoice in the field,",
    "start": "602030",
    "end": "604640"
  },
  {
    "text": "and the reason is\nthat we, as NL peers,",
    "start": "604640",
    "end": "606730"
  },
  {
    "text": "tend to care about categories\nno matter how large or small",
    "start": "606730",
    "end": "610300"
  },
  {
    "text": "they are.",
    "start": "610300",
    "end": "610870"
  },
  {
    "text": "And if anything, we often care\nmore about the small classes",
    "start": "610870",
    "end": "614680"
  },
  {
    "text": "than the large ones because\nthey're interesting or hard.",
    "start": "614680",
    "end": "617890"
  },
  {
    "text": "The macro average is simply\ngoing to average across them",
    "start": "617890",
    "end": "620620"
  },
  {
    "text": "numerically.",
    "start": "620620",
    "end": "621310"
  },
  {
    "text": "So I simply do the average\nof these three numbers.",
    "start": "621310",
    "end": "623930"
  },
  {
    "text": "So it gives equal\nweight to all three.",
    "start": "623930",
    "end": "625810"
  },
  {
    "text": "Bounds are 0 and 1.",
    "start": "625810",
    "end": "627340"
  },
  {
    "text": "0 the worst, and 1 the best.",
    "start": "627340",
    "end": "628870"
  },
  {
    "text": "The value encoded is, as\nI said, same as F scores",
    "start": "628870",
    "end": "632380"
  },
  {
    "text": "plus the assumption that\nall classes are equal,",
    "start": "632380",
    "end": "634850"
  },
  {
    "text": "regardless of size or support.",
    "start": "634850",
    "end": "637480"
  },
  {
    "text": "The weaknesses--\na classifier that",
    "start": "637480",
    "end": "640000"
  },
  {
    "text": "does well only on\nsmall classes might not",
    "start": "640000",
    "end": "643090"
  },
  {
    "text": "do well in the real world.",
    "start": "643090",
    "end": "644420"
  },
  {
    "text": "That's the kind of\ndual of carrying",
    "start": "644420",
    "end": "645910"
  },
  {
    "text": "a lot about small classes.",
    "start": "645910",
    "end": "647259"
  },
  {
    "text": "Suppose you do obsess\nover positive and negative",
    "start": "647260",
    "end": "649930"
  },
  {
    "text": "in the scenario and you\ndo really well on them",
    "start": "649930",
    "end": "652649"
  },
  {
    "text": "but at the cost of neutral.",
    "start": "652650",
    "end": "654200"
  },
  {
    "text": "Well, in the real\nworld, your system",
    "start": "654200",
    "end": "656210"
  },
  {
    "text": "is encountering\nmostly neutral cases.",
    "start": "656210",
    "end": "659170"
  },
  {
    "text": "If it's failing\non them, it might",
    "start": "659170",
    "end": "660980"
  },
  {
    "text": "look like a really bad system.",
    "start": "660980",
    "end": "664410"
  },
  {
    "text": "A classifier that does\nwell only on large classes",
    "start": "664410",
    "end": "666690"
  },
  {
    "text": "might do poorly on small\nbut vital smaller classes.",
    "start": "666690",
    "end": "669630"
  },
  {
    "text": "So this is the case\nwhere you might really",
    "start": "669630",
    "end": "672450"
  },
  {
    "text": "care about those small\nclasses even more than you",
    "start": "672450",
    "end": "675150"
  },
  {
    "text": "care about the large one.",
    "start": "675150",
    "end": "676600"
  },
  {
    "text": "And that's not reflected\nin the macro average",
    "start": "676600",
    "end": "678750"
  },
  {
    "text": "because it simply takes\nthem all as equal weight.",
    "start": "678750",
    "end": "683904"
  },
  {
    "text": "Weighted average F scores\nis a straightforward way",
    "start": "683904",
    "end": "686850"
  },
  {
    "text": "to average where you simply take\ninto account the total support.",
    "start": "686850",
    "end": "690209"
  },
  {
    "text": "And so it's a straight up\nweighted numerical average",
    "start": "690210",
    "end": "694260"
  },
  {
    "text": "of the three F1 scores.",
    "start": "694260",
    "end": "696960"
  },
  {
    "text": "The bounds are 0 and 1.",
    "start": "696960",
    "end": "698550"
  },
  {
    "text": "0 the worst, 1 the best.",
    "start": "698550",
    "end": "700350"
  },
  {
    "text": "The value encoded is\nthe same as the F scores",
    "start": "700350",
    "end": "703560"
  },
  {
    "text": "plus the assumption that class\nsize does, in fact, matter",
    "start": "703560",
    "end": "706228"
  },
  {
    "text": "in this case.",
    "start": "706228",
    "end": "706770"
  },
  {
    "text": "So this will be\nmore like accuracy.",
    "start": "706770",
    "end": "708810"
  },
  {
    "text": "The weakness, of course, is\nthat large classes will heavily",
    "start": "708810",
    "end": "712260"
  },
  {
    "text": "dominate.",
    "start": "712260",
    "end": "713160"
  },
  {
    "text": "So we're back to\nthat same weakness",
    "start": "713160",
    "end": "714930"
  },
  {
    "text": "that we had for accuracy.",
    "start": "714930",
    "end": "717950"
  },
  {
    "text": "The final way of\naveraging F scores",
    "start": "717950",
    "end": "720140"
  },
  {
    "text": "is called micro averaging.",
    "start": "720140",
    "end": "721790"
  },
  {
    "text": "What you do here is take\neach one of the classes",
    "start": "721790",
    "end": "724459"
  },
  {
    "text": "and form its own\nbinary confusion",
    "start": "724460",
    "end": "727610"
  },
  {
    "text": "matrix for that class, and\nthen you add them together,",
    "start": "727610",
    "end": "731250"
  },
  {
    "text": "and you get a\nsingle binary table.",
    "start": "731250",
    "end": "734880"
  },
  {
    "text": "The properties for this\nare, again, bounds 0 and 1.",
    "start": "734880",
    "end": "737550"
  },
  {
    "text": "0 the worst, and 1 the best.",
    "start": "737550",
    "end": "738870"
  },
  {
    "text": "The value encoded is\nexactly the same as accuracy",
    "start": "738870",
    "end": "742830"
  },
  {
    "text": "if you focus on the yes\ncategory in that final table",
    "start": "742830",
    "end": "746700"
  },
  {
    "text": "that you constructed.",
    "start": "746700",
    "end": "747810"
  },
  {
    "text": "It is exactly the\nsame as accuracy.",
    "start": "747810",
    "end": "750700"
  },
  {
    "text": "So the weakness are same as F\nscores, plus a score for yes",
    "start": "750700",
    "end": "754830"
  },
  {
    "text": "and a score for no, which is\nkind of annoying because what",
    "start": "754830",
    "end": "757950"
  },
  {
    "text": "do you do with the no category?",
    "start": "757950",
    "end": "759300"
  },
  {
    "text": "You have to focus on yes.",
    "start": "759300",
    "end": "760440"
  },
  {
    "text": "So there's no single number but\nthe yes one was just accuracy",
    "start": "760440",
    "end": "763840"
  },
  {
    "text": "after all.",
    "start": "763840",
    "end": "764340"
  },
  {
    "text": "And as far as I can\ntell is the only one",
    "start": "764340",
    "end": "766680"
  },
  {
    "text": "that everyone pays attention to.",
    "start": "766680",
    "end": "768820"
  },
  {
    "text": "So overall, I feel\nlike, at this point,",
    "start": "768820",
    "end": "771120"
  },
  {
    "text": "you could just ignore\nmicro average F scores.",
    "start": "771120",
    "end": "773910"
  },
  {
    "text": "You still see them\nin the literature",
    "start": "773910",
    "end": "775889"
  },
  {
    "text": "and in results tables\nfrom Scikit, I believe.",
    "start": "775890",
    "end": "778560"
  },
  {
    "text": "But overall, it's\nbasically macro average",
    "start": "778560",
    "end": "781410"
  },
  {
    "text": "to abstract away\nfrom class size,",
    "start": "781410",
    "end": "783540"
  },
  {
    "text": "or weighted average to bring\nin the overall class size",
    "start": "783540",
    "end": "787500"
  },
  {
    "text": "as an element in the metric.",
    "start": "787500",
    "end": "789390"
  },
  {
    "text": "Those are the two that I would\nsuggest going forward and only",
    "start": "789390",
    "end": "792330"
  },
  {
    "text": "for fully balanced problems\nshould you fall back",
    "start": "792330",
    "end": "795490"
  },
  {
    "text": "to accuracy where class size\nwon't be an interfering factor.",
    "start": "795490",
    "end": "800240"
  },
  {
    "text": "Finally, I wanted to\nreturn to that observation",
    "start": "800240",
    "end": "802870"
  },
  {
    "text": "I made at the start\nthat it's kind",
    "start": "802870",
    "end": "804520"
  },
  {
    "text": "of irksome that we need to\nalways impose a decision",
    "start": "804520",
    "end": "807910"
  },
  {
    "text": "boundary.",
    "start": "807910",
    "end": "808750"
  },
  {
    "text": "We have to do the same thing\nwith precision and recall.",
    "start": "808750",
    "end": "811690"
  },
  {
    "text": "We could think very\ndifferently about this.",
    "start": "811690",
    "end": "814090"
  },
  {
    "text": "We could have, for example,\nprecision and recall curves",
    "start": "814090",
    "end": "816880"
  },
  {
    "text": "that would allow us to explore\nthe full range of possible ways",
    "start": "816880",
    "end": "820510"
  },
  {
    "text": "that our system could make\npredictions given a decision",
    "start": "820510",
    "end": "823570"
  },
  {
    "text": "boundary.",
    "start": "823570",
    "end": "824260"
  },
  {
    "text": "This offers much\nmore information",
    "start": "824260",
    "end": "826210"
  },
  {
    "text": "about trade-offs between\nthese two pressures",
    "start": "826210",
    "end": "828670"
  },
  {
    "text": "and could make it\nmuch easier for people",
    "start": "828670",
    "end": "831190"
  },
  {
    "text": "to align system choices\nwith the underlying values",
    "start": "831190",
    "end": "835960"
  },
  {
    "text": "that they have for their system.",
    "start": "835960",
    "end": "837760"
  },
  {
    "text": "So I know it's impractical to\nask this because the field is",
    "start": "837760",
    "end": "840820"
  },
  {
    "text": "fairly focused on\nsingle summary numbers,",
    "start": "840820",
    "end": "843097"
  },
  {
    "text": "but I think it could\nbe interesting to think",
    "start": "843097",
    "end": "844930"
  },
  {
    "text": "about precision recall curves\nto get much more information.",
    "start": "844930",
    "end": "848480"
  },
  {
    "text": "And then if we do need to\nchoose a single number,",
    "start": "848480",
    "end": "851350"
  },
  {
    "text": "average precision is a\nsummary of this curve that",
    "start": "851350",
    "end": "854170"
  },
  {
    "text": "again avoids the\ndecision about how",
    "start": "854170",
    "end": "856690"
  },
  {
    "text": "we weight precision\nand recall, and brings",
    "start": "856690",
    "end": "858790"
  },
  {
    "text": "in much more information.",
    "start": "858790",
    "end": "860320"
  },
  {
    "text": "You'll recognize\nthis as analogous",
    "start": "860320",
    "end": "862330"
  },
  {
    "text": "to the average\nprecision calculation",
    "start": "862330",
    "end": "864640"
  },
  {
    "text": "that we did in the context\nof information retrieval",
    "start": "864640",
    "end": "867370"
  },
  {
    "text": "where, again, it offered a\nvery nuanced lesson about how",
    "start": "867370",
    "end": "870790"
  },
  {
    "text": "systems were doing.",
    "start": "870790",
    "end": "872970"
  },
  {
    "start": "872970",
    "end": "878000"
  }
]