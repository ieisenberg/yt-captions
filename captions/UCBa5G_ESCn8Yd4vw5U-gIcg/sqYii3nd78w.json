[
  {
    "start": "0",
    "end": "6235"
  },
  {
    "text": "Hey, everybody. Welcome back. We're going to start to talk\nabout fast or data-efficient reinforcement learning. Before we do that, we're going\nto start with a refresher",
    "start": "6235",
    "end": "13720"
  },
  {
    "text": "knowledge. ",
    "start": "13720",
    "end": "34940"
  },
  {
    "text": "One of the things that's fairly\ngood evidence about in terms of learning is that spaced\nrepetition is helpful, so I'll try to\nperiodically bring up",
    "start": "34940",
    "end": "41250"
  },
  {
    "text": "ideas that came up\nearlier in the quarter when we do the refresher\nunderstandings. ",
    "start": "41250",
    "end": "113700"
  },
  {
    "text": "All right. Why don't you turn\nto a neighbor and see if you got the same answer. ",
    "start": "113700",
    "end": "196160"
  },
  {
    "text": "All right. So we'll go ahead\nand get started. For those of you that just\ncame in, feel free to vote. The first one is that\nimportance sampling does not",
    "start": "196160",
    "end": "203220"
  },
  {
    "text": "leverage the Markov assumption. Do not need this. So this is false. Important sampling can work\nwith non-Markov systems, which",
    "start": "203220",
    "end": "211262"
  },
  {
    "text": "is one of the benefits of it. It makes very little assumptions\nover the data distribution generating process.",
    "start": "211262",
    "end": "217030"
  },
  {
    "text": "So it can be used in\njust similar to how we could use Monte Carlo\nmethods to estimate the value of a policy\nthrough rollouts.",
    "start": "217030",
    "end": "223630"
  },
  {
    "text": "This also is very general. Let's go through the\nnext one as well.",
    "start": "223630",
    "end": "228750"
  },
  {
    "text": "So let's now use the\nMarkov assumption. For this one, the\nfirst one is true.",
    "start": "228750",
    "end": "237127"
  },
  {
    "text": "So we can think of using the\nadvantage function of one policy and samples from the other. The second is that\nwe can importance",
    "start": "237128",
    "end": "244530"
  },
  {
    "text": "weight between the\ntwo policies and get the samples from policy one. So it's not really\nan exact bound,",
    "start": "244530",
    "end": "251349"
  },
  {
    "text": "but it turns out we can\nbound how off that is. And the reason it's not\nexact is because we're using samples of\nstates from one policy,",
    "start": "251350",
    "end": "258950"
  },
  {
    "text": "whereas in reality, the\nother policy might visit different types of states.",
    "start": "258950",
    "end": "264550"
  },
  {
    "text": "And PPO uses these\ntypes of ideas. ",
    "start": "264550",
    "end": "270420"
  },
  {
    "text": "And the approximation\nerror is bounded by the average over\nthe states visited by one policy between\nthe two policies.",
    "start": "270420",
    "end": "277310"
  },
  {
    "text": "So this is trying\nto say, how bad is this approximation when we use\njust samples from one policy,",
    "start": "277310",
    "end": "285110"
  },
  {
    "text": "OK? That was one of the really nice\ninsights of that prior work, is to show you\nactually could bound",
    "start": "285110",
    "end": "290500"
  },
  {
    "text": "what is the error in the\napp-- the approximation error that we induce by\npretending that we'd",
    "start": "290500",
    "end": "295509"
  },
  {
    "text": "get to the same states\nunder policy two compared to policy one. ",
    "start": "295510",
    "end": "301310"
  },
  {
    "text": "Awesome. So last time we talked a bit\nabout learning from prior data. And really, the\nlast few lectures,",
    "start": "301310",
    "end": "306380"
  },
  {
    "text": "we've been talking about how\nto learn from human feedback or from past\ndemonstrations of people or historical data we have.",
    "start": "306380",
    "end": "313463"
  },
  {
    "text": "And now we're going\nto switch, and we're going to think more about,\nwell, what if we can actually gather that data? And of course, that's where\nwe started at the beginning.",
    "start": "313463",
    "end": "320690"
  },
  {
    "text": "We thought about-- if\nwe are quite early on, we thought about how\nto evaluate policies, if we could gather data.",
    "start": "320690",
    "end": "327440"
  },
  {
    "text": "But we didn't think a lot about\nhow that data was gathered. We talked about epsilon-greedy,\nand we'll talk more",
    "start": "327440",
    "end": "332710"
  },
  {
    "text": "about epsilon-greedy\ntoday, but we didn't think super strategically\nover the influence of the way we",
    "start": "332710",
    "end": "338920"
  },
  {
    "text": "were gathering data. And so for the\nnext few lectures, we're going to talk\nabout that a lot. And that's really a\ncritical part, particularly",
    "start": "338920",
    "end": "345220"
  },
  {
    "text": "for online\nreinforcement learning. It's like, how do we\nactually gather the data we need in order to learn\nto make good decisions?",
    "start": "345220",
    "end": "350925"
  },
  {
    "text": "And can we do this-- are there\nbetter or worse ways to do this? ",
    "start": "350925",
    "end": "356065"
  },
  {
    "text": "So one of the things I want to\nemphasize when we start thinking about this part\nof the course is,",
    "start": "356065",
    "end": "361210"
  },
  {
    "text": "a lot of reinforcement\nlearning, particularly if you have simulated\nenvironments, focuses on computational\nefficiency.",
    "start": "361210",
    "end": "367480"
  },
  {
    "text": "So we think about\nthings like any place where you have a simulator. So if we want to do Atari\nor if we want to do MuJoCo,",
    "start": "367480",
    "end": "377080"
  },
  {
    "text": "in these cases,\ncomputational time is essentially the same as\ndata, because you can either",
    "start": "377080",
    "end": "383440"
  },
  {
    "text": "be using that additional\ncomputational time to sample from your\nsimulator or to actually spend more time computing\na Q function or policy.",
    "start": "383440",
    "end": "391819"
  },
  {
    "text": "And so to some\nextent, simulators blend the difference between\ncomputational efficiency",
    "start": "391820",
    "end": "398530"
  },
  {
    "text": "and data efficiency because\nit's all just computation. Like, you have a simulator and\nyou can either give you data",
    "start": "398530",
    "end": "404530"
  },
  {
    "text": "or you can use it to\ndo Bellman backups or whatever else you\nwant, but you could just count how much total resources\nyou're using essentially",
    "start": "404530",
    "end": "411280"
  },
  {
    "text": "in terms of computation. There are a lot of\nother domains where computation is really\nseparate from samples, like,",
    "start": "411280",
    "end": "418400"
  },
  {
    "text": "from actual data. So this is data. And these are a lot of\nthe application areas that I tend to think about\nand a number of other people",
    "start": "418400",
    "end": "425420"
  },
  {
    "text": "think about as well. So if you think about something\nlike using mobile phones",
    "start": "425420",
    "end": "431630"
  },
  {
    "text": "for health\ninterventions, or if you",
    "start": "431630",
    "end": "437130"
  },
  {
    "text": "think about consumer\nmarketing, like, which ad to show\nto people, or you",
    "start": "437130",
    "end": "443340"
  },
  {
    "text": "think about\neducational technology,",
    "start": "443340",
    "end": "449100"
  },
  {
    "text": "or you think about climate.  I'll do environmental policies.",
    "start": "449100",
    "end": "460950"
  },
  {
    "text": "In all of these cases,\nthere's a real world that's happening out there. There's a real students\nor there's real patients",
    "start": "460950",
    "end": "466710"
  },
  {
    "text": "or there's-- where you're trying\nto decide, say, policies to encourage wildlife\nconservation or others.",
    "start": "466710",
    "end": "475050"
  },
  {
    "text": "And so you have computers you\ncan use to compute that policy, and then you have\nreal world data.",
    "start": "475050",
    "end": "480150"
  },
  {
    "text": "And the real world data\nI'll call samples or sample efficiency here,\nand you care often",
    "start": "480150",
    "end": "485370"
  },
  {
    "text": "a lot about that real world\ndata and ho-- squeezing the most you can out of it. So in particular, you might\nimagine that if you have, say,",
    "start": "485370",
    "end": "492639"
  },
  {
    "text": "data from 500,000 patients\nor something like that, that's quite a lot, bless you,\nbut it's not nearly as large",
    "start": "492640",
    "end": "499061"
  },
  {
    "text": "as what you would normally have\nin the case of, say, Atari, where you can just run\nthe simulator forever. Or in the case of\nthings like AlphaGo,",
    "start": "499062",
    "end": "505870"
  },
  {
    "text": "where again, you could\njust play the board game, go against each other-- against\nsimulated agents forever.",
    "start": "505870",
    "end": "511800"
  },
  {
    "text": "So a lot of the\nthings we're going to be talking about over\nthe next few lectures are just going to assume that\nwe care about this because we",
    "start": "511800",
    "end": "517409"
  },
  {
    "text": "can't get infinite data. So we have to be-- we're\nthinking about cases where like, these are\ncoming from patients or they're coming from students.",
    "start": "517409",
    "end": "523548"
  },
  {
    "text": "And so we want to be much\nmore careful with the data we're gathering. And think about how\nwe could maximize",
    "start": "523549",
    "end": "528850"
  },
  {
    "text": "the information we\nget out of those to try to make good decisions. So when we start\nto do that, there's",
    "start": "528850",
    "end": "535810"
  },
  {
    "text": "a number of different things\nwe want to consider in terms of how good are the\ndifferent algorithms we're going to consider?",
    "start": "535810",
    "end": "542320"
  },
  {
    "text": "So one thing might be that\nif it converges at all-- and we've seen before that\nfrom the deadly triad,",
    "start": "542320",
    "end": "550310"
  },
  {
    "text": "we're not always\nguaranteed to converge. So we've seen that\nfor some settings",
    "start": "550310",
    "end": "559270"
  },
  {
    "text": "where this is not guaranteed\nor it hasn't been proven yet.",
    "start": "559270",
    "end": "564312"
  },
  {
    "text": "So you're not even\nnecessarily guaranteed to converge to anything. It might chatter. It might oscillate. It might not go to\nanything stable.",
    "start": "564312",
    "end": "570569"
  },
  {
    "text": "Second question you\nmight ask is if you're going to be guaranteed to\nconverge to the optimal policy. And then a third thing\nthat might be really",
    "start": "570570",
    "end": "577060"
  },
  {
    "text": "important is, how quickly? So in this case, it's\ngoing to be, how much data? ",
    "start": "577060",
    "end": "584667"
  },
  {
    "text": "And we're going to see\ndifferent types of measures to evaluate different\nreinforcement learning algorithms. So let me just give\nyou an illustration too",
    "start": "584667",
    "end": "591230"
  },
  {
    "text": "of why these things might\nlook quite different. So imagine that\nyou have something like this where this is time,\nwhere this is a reward, OK?",
    "start": "591230",
    "end": "601430"
  },
  {
    "text": "So you have really\ndifferent algorithms. You could have\nalgorithms that look like this, that might\nbe one algorithm,",
    "start": "601430",
    "end": "609410"
  },
  {
    "text": "or you could have\nan algorithm that looks like this, really smooth,\nand you could have algorithms",
    "start": "609410",
    "end": "617310"
  },
  {
    "text": "that in general, maybe\nmost of the time do great, but periodically make\nterrible mistakes.",
    "start": "617310",
    "end": "623070"
  },
  {
    "text": "Versus could have\nanother algorithm which never does awesome,\nbut is always pretty good.",
    "start": "623070",
    "end": "629850"
  },
  {
    "text": "And those are really\ndifferent types of behavior. So if you think about that in\nterms of, say, an AI clinician,",
    "start": "629850",
    "end": "635560"
  },
  {
    "text": "you could have an AI clinician\nthat on average helps,",
    "start": "635560",
    "end": "640710"
  },
  {
    "text": "like, let's say, 80% of\nyour desired outcomes. Like, it helps you manage\nyour blood pressure with 80% accurate fidelity.",
    "start": "640710",
    "end": "647280"
  },
  {
    "text": "Or it could be that for\n80% of the population, it helps them completely\nmanage their blood pressure.",
    "start": "647280",
    "end": "652330"
  },
  {
    "text": "And for 20% of them, it fails. So those are really different\ntypes of performance guarantees,",
    "start": "652330",
    "end": "657850"
  },
  {
    "text": "and we'll think about\nwhether trading off between those and what\nsort of algorithms guarantee us to have different\nsorts of performance.",
    "start": "657850",
    "end": "663730"
  },
  {
    "text": " So we'll start to introduce\ndifferent types of settings",
    "start": "663730",
    "end": "670139"
  },
  {
    "text": "and ways to evaluate the\nquality of algorithms, and we're going to\nstart with bandits. And we've talked very\nbriefly about bandits",
    "start": "670140",
    "end": "676800"
  },
  {
    "text": "in the context of ChatGPT\nand preference learning. We'll talk a lot\nmore about them now, and then we'll move back into\nthe Markov decision process",
    "start": "676800",
    "end": "684150"
  },
  {
    "text": "case. A lot of the ideas from\nbandits will turn out to exactly or quite\neasily translate over",
    "start": "684150",
    "end": "690330"
  },
  {
    "text": "to the RL setting. OK. All right. So let's dive in.",
    "start": "690330",
    "end": "696310"
  },
  {
    "text": "So what is a bandit? So a bandit is a really,\nreally simple RL problem.",
    "start": "696310",
    "end": "701855"
  },
  {
    "text": "They've been studied since,\nI think at least, like, around the 1920s. There's a very long history of\nresearch on multi-armed bandits.",
    "start": "701855",
    "end": "709620"
  },
  {
    "text": "It's been used for all\nsorts of application areas. So let's describe what it is. So the idea in this case\nis that there's no states.",
    "start": "709620",
    "end": "717300"
  },
  {
    "text": "There's just a\nfinite set of arms. And arms are the\nsame as what we've been calling actions before.",
    "start": "717300",
    "end": "722830"
  },
  {
    "text": "So as a concrete\nexample, you might think of there being\nlike 20 different ads you could show customers.",
    "start": "722830",
    "end": "728710"
  },
  {
    "text": "And we're going to\nassume that there's a probability distribution\nover rewards for each arm.",
    "start": "728710",
    "end": "734350"
  },
  {
    "text": "So maybe on average,\nthis gives you 90% click through rate\nfor this particular ad,",
    "start": "734350",
    "end": "741170"
  },
  {
    "text": "and this other ad gives\nyou 20% click through rate. But that's not known. That's not observed.",
    "start": "741170",
    "end": "746568"
  },
  {
    "text": "And what will happen\nis that each time step you get to select\none of the actions, and then the environment\nwill sample a reward",
    "start": "746568",
    "end": "753100"
  },
  {
    "text": "from that stochastic variable. So if the click through rate\nis 90% for that particular arm,",
    "start": "753100",
    "end": "758415"
  },
  {
    "text": "most of the time,\nyou'll get a 1, and sometimes people\nwon't click on it. And the goal is to maximize\nyour cumulative reward.",
    "start": "758415",
    "end": "764959"
  },
  {
    "text": "So overall time\nsteps that you get by most amount of, say, clicks.",
    "start": "764960",
    "end": "770530"
  },
  {
    "text": "And this is a very\nsimple setting, but it's been used\nextensively in a lot of areas. You could think about\nthis for something",
    "start": "770530",
    "end": "775927"
  },
  {
    "text": "like, how could I-- if\nI was doing something like a clinical trial, how might\nI randomize the next person",
    "start": "775927",
    "end": "781660"
  },
  {
    "text": "over what treatment to get,\na treatment or a control, for example, for ads, for\nmany, many different types",
    "start": "781660",
    "end": "787570"
  },
  {
    "text": "of application areas? So I'm going to go through-- I'm going to have\nsome running examples",
    "start": "787570",
    "end": "793030"
  },
  {
    "text": "for this part of the\ncourse, and we're going to have a sort\nof a silly one that's going to be illustrative. So let's imagine that we're\ntrying to treat patients",
    "start": "793030",
    "end": "798867"
  },
  {
    "text": "with broken toes. This is nothing to do\nwith medical stuff, so this is not medical advice. Imagine you have three different\noptions-- you could do surgery.",
    "start": "798867",
    "end": "806029"
  },
  {
    "text": "You could buddy tape the\nbroken toe to another toe, or you could do nothing. And your outcome measure\nis a binary variable about",
    "start": "806030",
    "end": "812440"
  },
  {
    "text": "whether or not that toe\nis healed or not healed after six weeks. So that's our setting.",
    "start": "812440",
    "end": "818223"
  },
  {
    "text": "We've got broken toes. We want to figure out,\nwhat's the best strategy for healing them? And we're not going to\ndo a clinical trial.",
    "start": "818223",
    "end": "823250"
  },
  {
    "text": "Instead, we're\njust going to say, well, sometimes people come\nin and they've broken toes, and I'm going to\ntry to figure out over time, which thing is best.",
    "start": "823250",
    "end": "831561"
  },
  {
    "text": "All right. So in this case, we're\ngoing to model it",
    "start": "831562",
    "end": "836699"
  },
  {
    "text": "as a multi-armed\nbandit with three arms. The arms are the--\nand we're going to model each arm as\na Bernoulli variable",
    "start": "836700",
    "end": "843149"
  },
  {
    "text": "with an unknown\nparameter theta I. So let's just do a quick check. Your understanding about\nthe framework of bandits.",
    "start": "843150",
    "end": "848855"
  },
  {
    "start": "848855",
    "end": "915649"
  },
  {
    "text": "OK. Great. I think most people are\nconverging on these already. Yes. Pulling an arm or\ntaking an action",
    "start": "915650",
    "end": "922050"
  },
  {
    "text": "is just the action\nwe're actually doing. The second one, this is a better\nfit to the problem than an MDP because we're only going to\nmake one decision per patient.",
    "start": "922050",
    "end": "929118"
  },
  {
    "text": "And we're also going to assume\nthat whatever decision-- whether [INAUDIBLE]\ntoe heals after we",
    "start": "929118",
    "end": "935040"
  },
  {
    "text": "do this is independent of\nwhether or not when Sophie shows up, what we do. So these are totally\nindependent processes.",
    "start": "935040",
    "end": "942254"
  },
  {
    "text": "The next person to show up. So we don't have any\nsequential dependence, even though we're making\na sequence of decisions.",
    "start": "942255",
    "end": "947472"
  },
  {
    "text": "It's like, each time point,\nthere's a new person. We're just going to decide\nwhat to do for them. And yes, this is right.",
    "start": "947472",
    "end": "953110"
  },
  {
    "text": "So if your theta I\nis between 0 and 1, meaning your outcomes\nare not deterministic, sometimes you'll heal,\nsometimes you won't.",
    "start": "953110",
    "end": "960780"
  },
  {
    "text": "OK. So one thing that we\ncould do to solve this would be to use-- yeah. So to confirm there is no\ntime point dependence you have",
    "start": "960780",
    "end": "969570"
  },
  {
    "text": "to probability distribution. It has to be the same in\nevery single [INAUDIBLE]. Great question. We're going to assume for now\nthat everything's stationary,",
    "start": "969570",
    "end": "976279"
  },
  {
    "text": "meaning that reward\nprobability distribution is the same at every time step. So there's lots of really\ninteresting questions",
    "start": "976280",
    "end": "982150"
  },
  {
    "text": "around nonstationarity. Our labs don't work on that. There's lots of other really\ninteresting work on this, like with time point\ndetection and change points.",
    "start": "982150",
    "end": "988660"
  },
  {
    "text": "For now, we're going to\nsee about stationary. And that would include the\nfact that we don't suddenly get a new distribution of people\nfor whom different things work.",
    "start": "988660",
    "end": "996010"
  },
  {
    "text": "Good question. All right. So one thing you could imagine\ndoing is just to be greedy.",
    "start": "996010",
    "end": "1001070"
  },
  {
    "text": "So what we're going to do in\nthis case, we're going to use Q today not to denote a state\naction or discounted sum",
    "start": "1001070",
    "end": "1006930"
  },
  {
    "text": "of future rewards, or you\ncan think of it like that, except for there's no state,\nthere's a single state. And it's only over actions and\nit's only the immediate reward.",
    "start": "1006930",
    "end": "1015930"
  },
  {
    "text": "So what Q here would\ndenote is what is just the expected reward of RMA?",
    "start": "1015930",
    "end": "1021673"
  },
  {
    "text": "And we can just estimate\nthat by counting. We can just look\nup every other time we did surgery, what were the\noutcomes for that individual?",
    "start": "1021674",
    "end": "1029649"
  },
  {
    "text": "And we can average. And what the greedy\nalgorithm does is they just selects the\naction with the highest value",
    "start": "1029650",
    "end": "1035550"
  },
  {
    "text": "and takes that action, observes\nthe outcome, and repeats. ",
    "start": "1035550",
    "end": "1041459"
  },
  {
    "text": "So let's think about what\nhappens when we do that. So if you have this setting--",
    "start": "1041460",
    "end": "1046650"
  },
  {
    "text": "imagine that this really is\nthe true set of parameters. So surgery, in this case,\nin our fake example,",
    "start": "1046650",
    "end": "1052690"
  },
  {
    "text": "is actually the most effective\nbuddy taping the second, and doing nothing is\nnot very effective.",
    "start": "1052690",
    "end": "1059040"
  },
  {
    "text": "So imagine this. So you start off-- and this\nis pretty common with a lot of bandit algorithms.",
    "start": "1059040",
    "end": "1065140"
  },
  {
    "text": "If you have a small,\nfinite set of actions, often, you'll just start off and\nyou'll sample everything once. Now, when you start to get into\nreally large action spaces,",
    "start": "1065140",
    "end": "1072759"
  },
  {
    "text": "like, all of the ads you\ncould recommend to people, we'll have to do\nsomething smarter. But in this case, you can just\nsample all the actions once,",
    "start": "1072760",
    "end": "1079410"
  },
  {
    "text": "and let's see what\nyou would observe. So in this case,\nimagine that you get the first observation\nhere is 0 for arm one,",
    "start": "1079410",
    "end": "1087370"
  },
  {
    "text": "it's 1 for arm 2,\nand 0 for arm 3. So which arm-- this is\nnot meant to be tricky.",
    "start": "1087370",
    "end": "1093520"
  },
  {
    "text": "Which arm would you select next\nunder the greedy algorithm? ",
    "start": "1093520",
    "end": "1105430"
  },
  {
    "text": "And which of them\nhas the highest? [INAUDIBLE] Great. Exactly. So you would just--\nthere would be--",
    "start": "1105430",
    "end": "1111789"
  },
  {
    "text": "deterministically, the\nprobability of picking a 2 would be equal to 1. You're just determined to\ntake whichever one looks best.",
    "start": "1111790",
    "end": "1119350"
  },
  {
    "text": "So would that be good or bad? ",
    "start": "1119350",
    "end": "1126710"
  },
  {
    "text": "Bad. And in particular, would you\never select the optimal action? No.",
    "start": "1126710",
    "end": "1132010"
  },
  {
    "text": "So you actually couldn't-- so it will never\nfind it, because you",
    "start": "1132010",
    "end": "1138159"
  },
  {
    "text": "have a really low estimate of\nthe true value of these two. Your average for a2 can\nnever drop down to 0,",
    "start": "1138160",
    "end": "1145840"
  },
  {
    "text": "because you've\ngot at least 1, 1. And so even if you\nget 0s forever, which you're unlikely\nto get two for a2,",
    "start": "1145840",
    "end": "1151100"
  },
  {
    "text": "you're never going\nto sample a 1 again. So what we would\nsay in this case is that this means\nthat you will not",
    "start": "1151100",
    "end": "1157360"
  },
  {
    "text": "converge to the optimal\naction, and this algorithm is not very good. And we'll formalize what we mean\nby not very good in a second.",
    "start": "1157360",
    "end": "1164380"
  },
  {
    "text": "So this just is to illustrate\nwhy you should not just be greedy, that you can lock\non to the suboptimal action",
    "start": "1164380",
    "end": "1170050"
  },
  {
    "text": "forever. This highlights why you need\nto do some form of exploration because you can, in fact,\nmake an infinite number",
    "start": "1170050",
    "end": "1177820"
  },
  {
    "text": "of bad decisions. So how do we quantify\nwhat it means to make an infinite number\nof good or bad decisions?",
    "start": "1177820",
    "end": "1184299"
  },
  {
    "text": "We're going to use\nthe word regret. And we're going to\nmean regret in the case of sequential\ndecision making, OK?",
    "start": "1184300",
    "end": "1192875"
  },
  {
    "text": "So the idea in this\ncase is that we're going to think\nformally about, what is the difference\nbetween the decisions",
    "start": "1192875",
    "end": "1198520"
  },
  {
    "text": "that our algorithm makes\nand the optimal decisions? And then we're going\nto score the algorithm based on what the gap is.",
    "start": "1198520",
    "end": "1205450"
  },
  {
    "text": "So in particular,\nthe optimal value, just like what we've\nseen in the past,",
    "start": "1205450",
    "end": "1211670"
  },
  {
    "text": "is the maximum\noverall, the Q value, so it's whichever arm has the\nbest, highest expected reward,",
    "start": "1211670",
    "end": "1217630"
  },
  {
    "text": "and the regret is\nthe opportunity loss. You could also think of\nthis as the difference--",
    "start": "1217630",
    "end": "1222960"
  },
  {
    "text": "is the advantage. The advantage of the\noptimal action compared to the action that's taken.",
    "start": "1222960",
    "end": "1228000"
  },
  {
    "text": "And so your regret, just like\nwe often use it colloquially, is the gap between what the\nagent could have achieved",
    "start": "1228000",
    "end": "1234480"
  },
  {
    "text": "and what it actually got. We're going to focus here of\nlooking at these in expectation.",
    "start": "1234480",
    "end": "1240010"
  },
  {
    "text": "Of course, due to\nstochasticity, there could be times where\nthe particular reward you get for a suboptimal\naction might be higher",
    "start": "1240010",
    "end": "1246179"
  },
  {
    "text": "than the action-- the reward you'd get\nfor the optimal action because of stochasticity. But we're just going to\nfocus here on expectations.",
    "start": "1246180",
    "end": "1252825"
  },
  {
    "text": "So we're always comparing\nthe expected reward of the optimal arm\nto the expected reward of the suboptimal arm.",
    "start": "1252825",
    "end": "1259410"
  },
  {
    "text": "So that's regret. So how do we compute it? We're going to think about\ncomparing this over all time",
    "start": "1259410",
    "end": "1267210"
  },
  {
    "text": "steps, and we're\ngoing to maximize cumulative reward, which\nis equivalent to minimizing total regret.",
    "start": "1267210",
    "end": "1272850"
  },
  {
    "text": "Because remember, this is\nunknown, but it's fixed. So we really want to\nmaximize our total reward,",
    "start": "1272850",
    "end": "1281000"
  },
  {
    "text": "and we can either think of\nthat as you're maximizing the Q you got over all\ntime steps or we're",
    "start": "1281000",
    "end": "1286840"
  },
  {
    "text": "minimizing the total regret. And normally in bandits, we talk\nabout minimizing total regret",
    "start": "1286840",
    "end": "1291850"
  },
  {
    "text": "instead of maximizing\ntotal reward. All right. Let's see how we can think about\nhow big the regret will be.",
    "start": "1291850",
    "end": "1298539"
  },
  {
    "text": "So let's let Nt(a) be\nthe number of times action a has been\nselected at time step t.",
    "start": "1298540",
    "end": "1310720"
  },
  {
    "text": "So that means that if your\nagent has made t decisions, you count up and see, how many\ntimes did I take action a1?",
    "start": "1310720",
    "end": "1317030"
  },
  {
    "text": "How many times did\nI take action a2? How many times did\nI take action a3? The gap for a particular\narm is essentially",
    "start": "1317030",
    "end": "1324160"
  },
  {
    "text": "its advantage of a star over a.",
    "start": "1324160",
    "end": "1331740"
  },
  {
    "text": "So it's just the\ndifference between what is the expected reward the\noptimal action would have gotten minus the expected\nreward you get",
    "start": "1331740",
    "end": "1338370"
  },
  {
    "text": "under this alternative action? And we often call this the gap. I think the literature developed\nsomewhat independently,",
    "start": "1338370",
    "end": "1345100"
  },
  {
    "text": "and so I think that's\nwhy people don't commonly call it the advantage. In the case of bandits, they\ntypically call it the gap.",
    "start": "1345100",
    "end": "1350968"
  },
  {
    "text": "And the gap will turn out\nto be pretty important, because as you might start\nto think about intuitively,",
    "start": "1350968",
    "end": "1356920"
  },
  {
    "text": "depending on the\nsize of the gap, it's going to be easier\nor harder to learn which of two actions is better.",
    "start": "1356920",
    "end": "1362520"
  },
  {
    "text": "So if the gaps are really large\nbetween action 1 and action 2, which means they have really\ndifferent expected rewards,",
    "start": "1362520",
    "end": "1368422"
  },
  {
    "text": "you're going to need less\nsamples to figure that out. If the gaps are really,\nreally small, generally, you need a lot more data, OK?",
    "start": "1368422",
    "end": "1374570"
  },
  {
    "text": "So again, it's going\nto just be a function of the gaps and the accounts. So we can just think\nof the number of times",
    "start": "1374570",
    "end": "1380700"
  },
  {
    "text": "that you took each action\nand the difference between-- and this gap. The difference between\nthe optimal action",
    "start": "1380700",
    "end": "1386130"
  },
  {
    "text": "you should have taken and\nthe reward you actually got. And so our expected\nregret here is just",
    "start": "1386130",
    "end": "1392320"
  },
  {
    "text": "going to be the sum of times you\ntake each action times the gap.",
    "start": "1392320",
    "end": "1399409"
  },
  {
    "text": "And so what that\nmeans intuitively is that we do not want\nto take actions which",
    "start": "1399410",
    "end": "1404900"
  },
  {
    "text": "have large gaps\nvery much, and it's more OK if we take more\nof actions that are close to the optimal action.",
    "start": "1404900",
    "end": "1411740"
  },
  {
    "text": "And a lot of algorithms-- for a lot of algorithms,\nwhat we try to do is we try to bound this quantity.",
    "start": "1411740",
    "end": "1417970"
  },
  {
    "text": "So we try to say, in\nadvance-- in general, this is something\nthat we can't know because this requires\naccess to what is ever",
    "start": "1417970",
    "end": "1423380"
  },
  {
    "text": "is the optimal\naction and its value. And we don't know\neither of those things. But what we can\ndo is we can have",
    "start": "1423380",
    "end": "1428750"
  },
  {
    "text": "algorithms where we\ncan prove something about how the regret grows. OK.",
    "start": "1428750",
    "end": "1433910"
  },
  {
    "text": "All right. Let's just see what I mean\njust to instantiate that. OK. So again, we can't do\nthis in the real world,",
    "start": "1433910",
    "end": "1440050"
  },
  {
    "text": "but we can do this\nfor a toy example. Let's just think\nabout what the regret",
    "start": "1440050",
    "end": "1446010"
  },
  {
    "text": "would look like in this case. So this would be\na series-- if you were running your\ngreedy algorithm,",
    "start": "1446010",
    "end": "1451750"
  },
  {
    "text": "so this is the actions. This is time. This is 1, 2, 3, 4, 5.",
    "start": "1451750",
    "end": "1457620"
  },
  {
    "text": "So we first take\nall of our actions. In each of those cases, the\ntrue optimal action was a 1,",
    "start": "1457620",
    "end": "1463750"
  },
  {
    "text": "and our observed reward was 1\nand our regret was as follows. So a1 really is\nthe optimal action,",
    "start": "1463750",
    "end": "1470679"
  },
  {
    "text": "so we have 0 regret there. The second one, our\nregret was this.",
    "start": "1470680",
    "end": "1477760"
  },
  {
    "text": "And for the third one,\nour regret was this. ",
    "start": "1477760",
    "end": "1486410"
  },
  {
    "text": "So this just shows you\nwhat the size would be. And so this here is\nactually the gap.",
    "start": "1486410",
    "end": "1493830"
  },
  {
    "text": "It's the gap between the\noptimal arm and the arm that you're taking. So this just shows you\nhow regret can grow.",
    "start": "1493830",
    "end": "1500040"
  },
  {
    "text": "And as you might expect, if\nyou make bad decisions forever, you're going to\nget linear regret.",
    "start": "1500040",
    "end": "1505940"
  },
  {
    "text": "So for example here\nin the greedy case, if we now take a3\nforever, our regret",
    "start": "1505940",
    "end": "1511520"
  },
  {
    "text": "is going to be the total number\nof time steps t times 0.85, because that's how\nmuch we're losing",
    "start": "1511520",
    "end": "1517280"
  },
  {
    "text": "for every single decision, and\nthen we sum them all up, OK?",
    "start": "1517280",
    "end": "1525300"
  },
  {
    "text": "All right. So in general, it can linear\nin the number of decisions. And so part of-- the main thing\nwe're going to be trying to do",
    "start": "1525300",
    "end": "1532240"
  },
  {
    "text": "is ideally, you would have\nconstant regret or zero regret. What I mean by\nconstant regret would",
    "start": "1532240",
    "end": "1537280"
  },
  {
    "text": "mean that you make a finite\nnumber of bad decisions. So if you can figure out what\nthe optimal arm is and then",
    "start": "1537280",
    "end": "1543202"
  },
  {
    "text": "take that forever,\nthen you'll have constant regret, because\nit just is going to be, say, I make 10 decisions,\nthen I learn the optimal arm,",
    "start": "1543202",
    "end": "1549639"
  },
  {
    "text": "and then I make the\noptimal thing forever. That's generally\npretty hard to do.",
    "start": "1549640",
    "end": "1554950"
  },
  {
    "text": "In the worst case,\nyou'll be linear. You'll make a mistake on every\nsingle arm decision forever.",
    "start": "1554950",
    "end": "1560270"
  },
  {
    "text": "And typically, what\nwe're hoping to find is we're hoping to\nhave sublinear regret. So it still might\ngrow with the number",
    "start": "1560270",
    "end": "1565960"
  },
  {
    "text": "of time steps, the number\nof decisions you're making, but it's not going to be linear.",
    "start": "1565960",
    "end": "1571220"
  },
  {
    "text": "OK. And we'll see a lot\nmore about that. OK.",
    "start": "1571220",
    "end": "1576700"
  },
  {
    "text": "All right. So what we're going\nto think of next is-- we've seen these before,\nthe epsilon greedy algorithms.",
    "start": "1576700",
    "end": "1582480"
  },
  {
    "text": "So let's think about what\nsort of regret epsilon greedy will have. We've seen that\ngreedy can be linear. Now, let's see if there's\nsome better things we can do.",
    "start": "1582480",
    "end": "1590450"
  },
  {
    "text": "OK? So in this case, we're going to\ndo just to refresh our memories. With probability 1 minus\nepsilon, we're going to select--",
    "start": "1590450",
    "end": "1597837"
  },
  {
    "text": "we're going to be greedy. We're going to select whichever\naction is the arg max. And otherwise, we're going\nto select a random action.",
    "start": "1597838",
    "end": "1604760"
  },
  {
    "text": "And that means that\nepsilon amount of the time, we're going to be making\nsome suboptimal decision.",
    "start": "1604760",
    "end": "1610790"
  },
  {
    "text": "Because unless all of\nyour arms are optimal and your gaps are 0, in which\ncase it doesn't matter what arm you're picking, if you\nselect things at random,",
    "start": "1610790",
    "end": "1618149"
  },
  {
    "text": "you're always going to be making\nsome bad decision at each time point, OK?",
    "start": "1618150",
    "end": "1624056"
  },
  {
    "text": "So what does this look like? So what this would look like in\nthis case is, imagine, again,",
    "start": "1624056",
    "end": "1629610"
  },
  {
    "text": "we sample all three\narms to start. This is our epsilon. I'm just going to work out\nwhat it will look like. So with this case,\nwe're going to--",
    "start": "1629610",
    "end": "1635960"
  },
  {
    "text": "90% probability, we're\ngoing to be greedy. And in that case, we will\ntake action a1 and a2,",
    "start": "1635960",
    "end": "1648890"
  },
  {
    "text": "each with probability--\nassume that you split ties, 45% probability.",
    "start": "1648890",
    "end": "1654500"
  },
  {
    "text": "And then with 10%\nprobability, we will take all of\nthe other actions. So we'll have 3.3%\na1, a2, and a3.",
    "start": "1654500",
    "end": "1665780"
  },
  {
    "text": "So that's just to be\nconcrete about what that would look like in this case. ",
    "start": "1665780",
    "end": "1673770"
  },
  {
    "text": "I'll skip through this. So the question here is, what\nwill this regret look like?",
    "start": "1673770",
    "end": "1682068"
  },
  {
    "text": "So now we want to try to\ncompute this for epsilon greedy to think about\nwhether we'll have sublinear",
    "start": "1682068",
    "end": "1687110"
  },
  {
    "text": "regret for epsilon greedy. OK? ",
    "start": "1687110",
    "end": "1693260"
  },
  {
    "text": "All right. So let's assume that we're in\na setting where there always",
    "start": "1693260",
    "end": "1700490"
  },
  {
    "text": "exists at least one action\nsuch that the gap is nonzero. That means that not\nall arms are tied.",
    "start": "1700490",
    "end": "1707038"
  },
  {
    "text": "If all arms are tied,\nagain, doesn't really matter what you do because\neverything is the same. And so it doesn't matter\nwhat action you take.",
    "start": "1707038",
    "end": "1712617"
  },
  {
    "text": "So this makes it a nontrivial\ndecision making problem. So let's think about it\nin terms of our thing,",
    "start": "1712617",
    "end": "1719220"
  },
  {
    "text": "whether or not epsilon equals\n0.1 can have linear regret",
    "start": "1719220",
    "end": "1724340"
  },
  {
    "text": "and whether epsilon equals\n0 can have linear regret.  As in, this is generally\ntrying to think about,",
    "start": "1724340",
    "end": "1731299"
  },
  {
    "text": "are there settings of epsilon\nfor which you could get linear regret and maybe settings of\nepsilon where you couldn't? I don't know if this\nis actually on the--",
    "start": "1731300",
    "end": "1738270"
  },
  {
    "text": "there are [INAUDIBLE]. You don't know what? I don't know if this one\nis actually on the post.",
    "start": "1738270",
    "end": "1744790"
  },
  {
    "text": "We've answered all\nthree different. I wonder if that was missed. ",
    "start": "1744790",
    "end": "1752930"
  },
  {
    "text": "All right. Well, if it's not on\nEd, fell free just to-- I have it. Oh, OK.",
    "start": "1752930",
    "end": "1757990"
  },
  {
    "text": "I can check and see-- I wonder if something's\ngot missed in the last one. Ooh.",
    "start": "1757990",
    "end": "1763450"
  },
  {
    "text": "OK. Hold on. I'll post those in. But feel free just to\nthink for a second, and then I'll ask you to\ntalk to your neighbor.",
    "start": "1763450",
    "end": "1769399"
  },
  {
    "text": "Let me see. I think something's got mangled. ",
    "start": "1769400",
    "end": "1776809"
  },
  {
    "text": "Which ones are mangled? This one.  OK.",
    "start": "1776810",
    "end": "1782025"
  },
  {
    "start": "1782025",
    "end": "1796890"
  },
  {
    "text": "I think this should\nbe there now. You can check. I just updated Ed Don't\ntell me that didn't work.",
    "start": "1796890",
    "end": "1804290"
  },
  {
    "text": " Does that work? Looks like? Great. ",
    "start": "1804290",
    "end": "1829500"
  },
  {
    "text": "I think most people\nagree on this, but maybe we'll\njust do one minute and check with the\nneighbor and just check you got the same thing.",
    "start": "1829500",
    "end": "1834548"
  },
  {
    "start": "1834548",
    "end": "1895610"
  },
  {
    "text": "All right. I'm going to interrupt\nyou for a second. So I think one way that's\nuseful to think about this",
    "start": "1895610",
    "end": "1902130"
  },
  {
    "text": "is when we think about how many\ntimes we sample things, all of the arms are going\nto have a lower bound",
    "start": "1902130",
    "end": "1907350"
  },
  {
    "text": "on the number of\ntimes we sample them, which is at least epsilon\ndivided by the number of actions",
    "start": "1907350",
    "end": "1912480"
  },
  {
    "text": "times t, where t is the total\nnumber of decisions we make.",
    "start": "1912480",
    "end": "1917790"
  },
  {
    "text": "And so I think that\ncan be a helpful way to think about this, that\nyou see there's a T here times some constant.",
    "start": "1917790",
    "end": "1924539"
  },
  {
    "text": "Because there's a big T\nhere times a constant, that means you're going to\nhave at least linear regret.",
    "start": "1924540",
    "end": "1929760"
  },
  {
    "text": "So if epsilon is greater than\n0, you will have linear regret.",
    "start": "1929760",
    "end": "1934920"
  },
  {
    "text": "And if epsilon is equal\nto 0, you're greedy. And we just saw that that\ncan have linear regret. So in either of these two cases,\nunfortunately, both of these--",
    "start": "1934920",
    "end": "1945090"
  },
  {
    "text": "both are true.  Somebody have any\nquestions about that?",
    "start": "1945090",
    "end": "1950955"
  },
  {
    "text": " Now, it turns out there are\ncertainly better and worse ways",
    "start": "1950955",
    "end": "1956240"
  },
  {
    "text": "of setting epsilon. But if you just set\nepsilon in a static way, it can be pretty bad.",
    "start": "1956240",
    "end": "1962550"
  },
  {
    "text": "And as you might remember\nfrom a long time ago, sometimes we talked about\ndecaying epsilon over time.",
    "start": "1962550",
    "end": "1968070"
  },
  {
    "text": "And so that can\nmatter a lot, too. But static epsilon is not great. All right.",
    "start": "1968070",
    "end": "1973410"
  },
  {
    "text": "So let's look at what\nthis can look like. If you think about how regret\nis growing over time steps,",
    "start": "1973410",
    "end": "1979710"
  },
  {
    "text": "these are very common plots. When you look at bandits\nor some other approaches, we'll see if we consider\nwhat total regret is,",
    "start": "1979710",
    "end": "1985679"
  },
  {
    "text": "you'd like a regret to be 0. If you make it greedy,\nit can be linear.",
    "start": "1985680",
    "end": "1991202"
  },
  {
    "text": "If you make it\nepsilon greedy, it's normally a little bit better,\nbut it's still linear. If you decay it, it\ncan get a lot closer.",
    "start": "1991202",
    "end": "1999169"
  },
  {
    "text": "And it is going to be\npossible to be sublinear for good choices of algorithms.",
    "start": "1999170",
    "end": "2005620"
  },
  {
    "text": "One of the challenges\nfor this is that it can turn out that there\ncan be some pretty good choices of epsilon, but it often\ndepends on dependent properties",
    "start": "2005620",
    "end": "2013570"
  },
  {
    "text": "that we don't in advance. So we need to have\nan algorithm, which before knowing how anything\nabout the problem in terms",
    "start": "2013570",
    "end": "2020830"
  },
  {
    "text": "of the gaps or\nanything like that, can be guaranteed to\nhave sublinear regret.",
    "start": "2020830",
    "end": "2025929"
  },
  {
    "text": "So first of all, let's think\nabout what type of regret bounds we might get and is\nthere reasons for hope?",
    "start": "2025930",
    "end": "2032990"
  },
  {
    "text": "So a problem-independent\nbound talks about, how does the regret\ngrow as a function of t for any possible problem\nthat you might be given?",
    "start": "2032990",
    "end": "2042940"
  },
  {
    "text": "So what this might say\nis, I might give you an algorithm which is\nguaranteed to be sublinear in t no matter what bandit\nproblem you put me in.",
    "start": "2042940",
    "end": "2050161"
  },
  {
    "text": "So that's just an algorithm\nthat will work well for any potential\ndomain you put in, and it'll make a bound\non its performance.",
    "start": "2050162",
    "end": "2057580"
  },
  {
    "text": "Instance-dependent or\nproblem-dependent bounds bound things as a\nfunction of the gap.",
    "start": "2057580",
    "end": "2065230"
  },
  {
    "text": "And one of the\nreally elegant things of problem-dependent\nbounds is that it doesn't mean the algorithm\nhas to know the gaps.",
    "start": "2065230",
    "end": "2070638"
  },
  {
    "text": "It just means that if it\nturns out the problem is easy, like, there are\nreally large gaps, you will have a\nmuch better regret.",
    "start": "2070639",
    "end": "2077739"
  },
  {
    "text": "And so some of my labs work and\na number of other people too, were often very\ninterested in this.",
    "start": "2077739",
    "end": "2083929"
  },
  {
    "text": "And I think at a\nhigh level, what this means is you have\nan algorithm that's adaptive to the problem. So it means that\nyour algorithm will",
    "start": "2083929",
    "end": "2089138"
  },
  {
    "text": "be guaranteed to do\nreally well on the problem if the problem is\neasier to learn in. And if it's harder, well,\nthen you can't do well anyway.",
    "start": "2089139",
    "end": "2095620"
  },
  {
    "text": "It'll do as well as it can. So we'll talk about bounds\nthat are both types of these.",
    "start": "2095620",
    "end": "2100980"
  },
  {
    "text": "In general, is the gap\nusually less than the value if we're considering\nonly the words?",
    "start": "2100980",
    "end": "2109880"
  },
  {
    "text": "And we usually consider only\nrewards between 0 and 1? Great question. Totally depends on the domain.",
    "start": "2109880",
    "end": "2115400"
  },
  {
    "text": "So if you're looking at\nBernoulli's, then it's naturally between 0 and 1. Other domains might\nbe very different.",
    "start": "2115400",
    "end": "2121700"
  },
  {
    "text": "You can always normalize it. I think whether the domain has\nreally big gaps really depends. So if you think about something\nclick through rates for ads,",
    "start": "2121700",
    "end": "2129607"
  },
  {
    "text": "click through rates are really,\nreally hard to optimize. It's often like\n0.01 versus 0.011.",
    "start": "2129607",
    "end": "2136099"
  },
  {
    "text": "Nobody likes ads. So in those cases,\nthe differences, the gaps we're looking at,\ncould often be really tiny.",
    "start": "2136100",
    "end": "2142508"
  },
  {
    "text": "And so you'll generally\nneed a lot of data, and having smart data efficient\nalgorithms will matter a lot.",
    "start": "2142508",
    "end": "2148100"
  },
  {
    "text": "There might be other cases\nwhere there's really big gaps. If the problem has really\nbig gaps, it's really easy.",
    "start": "2148100",
    "end": "2154680"
  },
  {
    "text": "And so it tends to not matter\ntoo much what you do there, because you can\nquickly estimate them. Great question.",
    "start": "2154680",
    "end": "2160910"
  },
  {
    "text": "OK. All right. So here's a reason for hope. So there's a nice lower\nbound by Lai and Robins--",
    "start": "2160910",
    "end": "2168000"
  },
  {
    "text": "I think this was around\n1950s, it's been a long time, which tries to think about,\nwhat's the minimum regret",
    "start": "2168000",
    "end": "2174570"
  },
  {
    "text": "you're going to get as a\nfunction of the problem? And so this means\nthat any algorithm",
    "start": "2174570",
    "end": "2180270"
  },
  {
    "text": "is going to suffer at least\nthis much in terms of regret. So it says, you're going\nto at least be log t,",
    "start": "2180270",
    "end": "2187483"
  },
  {
    "text": "like, the number of time steps,\nnumber of decisions you've made. And for any arm for\nwhich it is suboptimal,",
    "start": "2187483",
    "end": "2193420"
  },
  {
    "text": "you're going to suffer this\nin terms of a KL difference between your distribution of\nrewards you get on your arm",
    "start": "2193420",
    "end": "2199920"
  },
  {
    "text": "on that arm versus\nthe optimal arm and with the gap\non the numerator.",
    "start": "2199920",
    "end": "2205170"
  },
  {
    "text": "But this should be promising\nbecause it's sublinear. It's log. It's not linear.",
    "start": "2205170",
    "end": "2210640"
  },
  {
    "text": "Which means that the lower\nbound says, according to this, it is not yet impossible to try\nto have sublinear regret, OK?",
    "start": "2210640",
    "end": "2218160"
  },
  {
    "text": "And this would be considered\na problem-dependent or incident-dependent\nbound because this holds based on the unknown gaps.",
    "start": "2218160",
    "end": "2223940"
  },
  {
    "text": " OK. So now we're going to see\none of my favorite ideas",
    "start": "2223940",
    "end": "2230460"
  },
  {
    "text": "in the course, which is\noptimism under uncertainty, which gives us--",
    "start": "2230460",
    "end": "2235940"
  },
  {
    "text": "I think it's a lovely\nprinciple because it shows why it's provably optimal\nto be optimistic about things, which is beautiful.",
    "start": "2235940",
    "end": "2242657"
  },
  {
    "text": "And it's going to be one\nof the first things we're going to see that's\ngoing to allow us to have sublinear regret. OK.",
    "start": "2242657",
    "end": "2247824"
  },
  {
    "text": "So why is optimism\ngood and what do we mean by optimism in this case? What we mean is, we're going\nto choose actions or arms,",
    "start": "2247824",
    "end": "2256440"
  },
  {
    "text": "some typo there, that\nmight have a high value. Well, what happens when we\nchoose things that are good?",
    "start": "2256440",
    "end": "2265560"
  },
  {
    "text": "So one thing that can happen is\nwe actually get high reward, OK?",
    "start": "2265560",
    "end": "2272055"
  },
  {
    "text": "So that's good, because\nthat's our goal, because we want to get high reward. We want to maximize\nreward/minimize cost.",
    "start": "2272055",
    "end": "2277310"
  },
  {
    "text": "What's the other thing\nthat can happen if we pick something that might be good? Might have high reward. ",
    "start": "2277310",
    "end": "2290550"
  },
  {
    "text": "Low reward. Low reward. Exactly. OK. Yeah, those are the\nonly two things, you can either get higher\nor you can lower award.",
    "start": "2290550",
    "end": "2296802"
  },
  {
    "text": "What happens if\nthere's low reward? I mean, of course, there's that. But aside from\nthat, what happens,",
    "start": "2296802",
    "end": "2302140"
  },
  {
    "text": "do you think, probably\nto our estimates, those Q estimates if\nwe get low reward? Yeah. [INAUDIBLE]",
    "start": "2302140",
    "end": "2309330"
  },
  {
    "text": "Exactly. Yeah, exactly. Remind me your name. Yeah. So what you said\nis exactly right.",
    "start": "2309330",
    "end": "2314450"
  },
  {
    "text": "So basically, either\nyou get high reward or you learned\nsomething new, OK?",
    "start": "2314450",
    "end": "2319809"
  },
  {
    "text": "So the other alternative\nis you get low reward and you learn\nsomething, and you're going to improve your estimates.",
    "start": "2319810",
    "end": "2327010"
  },
  {
    "text": "And from the point of\nview of a reinforcement learning algorithm or abandoned\nalgorithm, both of these are really valuable.",
    "start": "2327010",
    "end": "2332517"
  },
  {
    "text": "Because either you're\nactually achieving your goal or you are learning something\nso that in the future, you won't make bad\ndecisions in the future, OK?",
    "start": "2332517",
    "end": "2339230"
  },
  {
    "text": "So that is why optimism\nis-- we're going to see provably optimal, OK?",
    "start": "2339230",
    "end": "2345450"
  },
  {
    "text": "All right. Now, of course,\nthat means that we have to have an algorithm that\nleverages the information we get when we see low rewards.",
    "start": "2345450",
    "end": "2351193"
  },
  {
    "text": "So we're going to have\nto be formal about what it means to might. We're going to formalize this\nas quantifying our uncertainty.",
    "start": "2351193",
    "end": "2360877"
  },
  {
    "text": "So we're going to need to be\nprecise over our confidence intervals or uncertainty\nbounds, and then use that to make decisions.",
    "start": "2360877",
    "end": "2367050"
  },
  {
    "text": "OK. So in particular,\nwhat we're going to do is we are\ngoing to estimate an upper confidence\nbound for each action",
    "start": "2367050",
    "end": "2373319"
  },
  {
    "text": "value such that that\nconfidence bound-- that upper confidence bounds\nholds with high probability.",
    "start": "2373320",
    "end": "2379680"
  },
  {
    "text": "So we're going to\nmake sure-- we're going to be frequentist today. We're not going to be Bayesians. Don't worry if you haven't\ndone a lot on either of those.",
    "start": "2379680",
    "end": "2385570"
  },
  {
    "text": "But we're going to focus today\non just high probability bounds. So we're going to\nneed a Ut of a, where that holds with\nhigh probability,",
    "start": "2385570",
    "end": "2393850"
  },
  {
    "text": "and we're going\nto want this to be dependent on how many times\nwe've selected the arm. There are lots of ways\nto quantify uncertainty.",
    "start": "2393850",
    "end": "2400630"
  },
  {
    "text": "We're going to focus today\non a frequentist view and just thinking about counts. And then the way\nwe're going to behave,",
    "start": "2400630",
    "end": "2406330"
  },
  {
    "text": "the way that our agent\nis going to take actions, is just going to pick whichever\naction has the highest upper confidence bound.",
    "start": "2406330",
    "end": "2412440"
  },
  {
    "text": "And there's a whole\nsuite of algorithms that are called UCB algorithms. So there are many\nalgorithms that",
    "start": "2412440",
    "end": "2417780"
  },
  {
    "text": "are variants of this notion.  There's also ones\nthat are called",
    "start": "2417780",
    "end": "2423940"
  },
  {
    "text": "Optimism in the Face of\nUncertainty, OFU, OK? So it's a really simple idea.",
    "start": "2423940",
    "end": "2430339"
  },
  {
    "text": "And now, the question is going\nto be how well does this perform and how do we quantify\nthe uncertainty?",
    "start": "2430340",
    "end": "2437319"
  },
  {
    "text": "So let's go through\nHoeffding's inequality. We're going to use\nit in homework 3, but I'm curious who has\nseen it in previous classes.",
    "start": "2437320",
    "end": "2444590"
  },
  {
    "text": "OK. Maybe a couple of people. But most people, I wouldn't\nexpect you to know. So Hoeffding inequality is\na really useful inequality.",
    "start": "2444590",
    "end": "2451930"
  },
  {
    "text": "The idea of it is we're\njust going to think about, how different can our observed\naverage be from the true mean?",
    "start": "2451930",
    "end": "2460330"
  },
  {
    "text": "So let's say we\nhave n samples that are somewhere between 0 and 1.",
    "start": "2460330",
    "end": "2466480"
  },
  {
    "text": "And this is our\ntrue expectation, so is their true mean, which\nwe don't know what it is,",
    "start": "2466480",
    "end": "2471750"
  },
  {
    "text": "and this is our sample mean,\njust over the n samples. What Hoeffding's\ninequality says is",
    "start": "2471750",
    "end": "2477220"
  },
  {
    "text": "that the difference between\nyour empirical estimate and the true\nestimate, if they're",
    "start": "2477220",
    "end": "2484869"
  },
  {
    "text": "off by U, then the\nprobability of that happening is going down exponentially. Which essentially means\nthat as you have more data,",
    "start": "2484870",
    "end": "2494678"
  },
  {
    "text": "the chance that your\nempirical estimate is really different than your true mean is\ngoing down exponentially fast.",
    "start": "2494678",
    "end": "2501180"
  },
  {
    "text": "If you can't have your\nempirical average be 30 and the real thing is 2000,\nif you have a lot of data,",
    "start": "2501180",
    "end": "2507570"
  },
  {
    "text": "you're going to converge\non the true mean. Which is, of course,\nwhat you would hope,",
    "start": "2507570",
    "end": "2513360"
  },
  {
    "text": "but that this is a formal\nthing about what the rate is. So let's just look for a\nsecond and think a little bit",
    "start": "2513360",
    "end": "2518540"
  },
  {
    "text": "about what this can imply. So let's look at this part. Let's say-- I'm going to do\nit for the absolute, value",
    "start": "2518540",
    "end": "2524270"
  },
  {
    "text": "probability of E of X minus\nXn, so this is, again, just our empirical mean.",
    "start": "2524270",
    "end": "2531950"
  },
  {
    "text": "The probability this\nis greater than mu. So this gap between\nour empirical average and the true one is.",
    "start": "2531950",
    "end": "2537290"
  },
  {
    "text": "And so just to back up, why\nare we doing all of this? We're going to\nwant to figure out a way to get an upper bound on\nwhat the real mean is of this.",
    "start": "2537290",
    "end": "2545060"
  },
  {
    "text": "And so what this\nequation is going to allow us to do is\nto try to figure out, how big do we need to set\nU to be in order for us",
    "start": "2545060",
    "end": "2551900"
  },
  {
    "text": "to get an upper bound on what\nthe true expected reward might be for a particular arm?",
    "start": "2551900",
    "end": "2557660"
  },
  {
    "text": "OK. All right. So let's see how we can do that. All right. So we're going to say,\nthis is less than--",
    "start": "2557660",
    "end": "2562840"
  },
  {
    "text": "I've got an absolute\nvalue here, so we're going to use this version. ",
    "start": "2562840",
    "end": "2568800"
  },
  {
    "text": "OK. And we're going to\nset this to delta. So this is going to be the\nconfidence with which we want this confidence\ninterval to hold.",
    "start": "2568800",
    "end": "2576500"
  },
  {
    "text": "So this is going\nto be, want the CI to hold with this probability\nwith 1 minus delta probability.",
    "start": "2576500",
    "end": "2585643"
  },
  {
    "text": "So we're going to\ntry to construct an upper confidence bound that\nholds with least probability 1 minus delta, OK?",
    "start": "2585643",
    "end": "2591333"
  },
  {
    "text": "So let's just do this. And now we're going to focus\non this hand, this side. So we're just going\nto do some algebra.",
    "start": "2591333",
    "end": "2599780"
  },
  {
    "text": "Let's go to delta over 2. That means U squared is\nequal to 1 over n log of 2",
    "start": "2599780",
    "end": "2606290"
  },
  {
    "text": "over delta, which means U\nis equal to square root-- ",
    "start": "2606290",
    "end": "2615300"
  },
  {
    "text": "OK. So this gives us\nour range, and it says if we want the probability\nthat our empirical estimate",
    "start": "2615300",
    "end": "2623010"
  },
  {
    "text": "differs from the true\nmean by no more than U,",
    "start": "2623010",
    "end": "2628890"
  },
  {
    "text": "then it is sufficient to\nset U equal to this, OK? So that means that we can\nsay that Xn minus mu is",
    "start": "2628890",
    "end": "2639990"
  },
  {
    "text": "less than or equal to\nexpected value of x, which is less than or equal to Xn plus\nU with probability greater than",
    "start": "2639990",
    "end": "2649080"
  },
  {
    "text": "or equal to 1 minus delta. So that just created our\nupper confidence bound. So they said with\nhigh probability,",
    "start": "2649080",
    "end": "2656109"
  },
  {
    "text": "I can take my empirical\nestimate, I added my mu-- my mu here note, just depends\non the number of samples",
    "start": "2656110",
    "end": "2661950"
  },
  {
    "text": "that I have, and that gives\nme my upper confidence bound. ",
    "start": "2661950",
    "end": "2667770"
  },
  {
    "text": "So we can use this. We can use this given our data. It just requires us to count how\nmany times we've sampled things,",
    "start": "2667770",
    "end": "2673158"
  },
  {
    "text": "compute the average, and then\nadd on this additional bonus. We often call these bonus\nterms in these cases.",
    "start": "2673158",
    "end": "2679150"
  },
  {
    "text": "Sorry. So this is going to\ncreate the UCB1 algorithm. Which is at every\ntime step, we're",
    "start": "2679150",
    "end": "2684839"
  },
  {
    "text": "just going to compute--\nthis is again-- remember, the Q hat is this the\nempirical average. ",
    "start": "2684840",
    "end": "2696480"
  },
  {
    "text": "And then we add on\nthis bonus term. OK.",
    "start": "2696480",
    "end": "2702826"
  },
  {
    "text": "And this is, again,\njust the number of samples of a\nafter t time steps.",
    "start": "2702826",
    "end": "2712890"
  },
  {
    "text": " OK.",
    "start": "2712890",
    "end": "2718060"
  },
  {
    "text": "And for those of you\nfamiliar with things like union bonds and stuff,\nwe'll come to that shortly. So this is-- we haven't\nreally fully made",
    "start": "2718060",
    "end": "2724788"
  },
  {
    "text": "sure that all of these\nconfidence intervals are going to hold\nover all time steps, so we'll be a little bit\nmore careful about what",
    "start": "2724788",
    "end": "2731200"
  },
  {
    "text": "delta needs to be soon. Yeah. It's called UCB1. Like, why is it 1?",
    "start": "2731200",
    "end": "2736570"
  },
  {
    "text": "[INAUDIBLE] There's a lot of different\nvariants of the UCB algorithm. I think this is one\nof the first ones.",
    "start": "2736570",
    "end": "2742700"
  },
  {
    "text": "It was, I think,\nAuer, U-E-R 2002.",
    "start": "2742700",
    "end": "2748270"
  },
  {
    "text": "I think it's the one they\nnamed first in their paper. OK?",
    "start": "2748270",
    "end": "2753560"
  },
  {
    "text": "But this notion of\noptimism under uncertainty is certainly around\nbefore the 2000. But I think this is the\npaper where they first",
    "start": "2753560",
    "end": "2760250"
  },
  {
    "text": "did some of these nice proofs. OK. All right.",
    "start": "2760250",
    "end": "2766480"
  },
  {
    "text": "OK. So let's think\nabout how different that algorithm would look like\nin our types of settings, OK?",
    "start": "2766480",
    "end": "2772446"
  },
  {
    "text": "So we're going to use\noptimism under uncertainty. And what we're going\nto do in this case is,",
    "start": "2772446",
    "end": "2780660"
  },
  {
    "text": "we're first going to sample each\narm once, so same as before,",
    "start": "2780660",
    "end": "2786059"
  },
  {
    "text": "and this is what\nwe're going to get. And now what we\ndo is we're going to compute those upper\nconfidence bounds, OK?",
    "start": "2786060",
    "end": "2793020"
  },
  {
    "text": "So what we want to do is compute\nthis upper confidence bounds for each of the arms. So UCB of A1 to A3, OK?",
    "start": "2793020",
    "end": "2803190"
  },
  {
    "text": "And so this would be 1\nplus square root of 2 log",
    "start": "2803190",
    "end": "2809430"
  },
  {
    "text": "or delta over 1, same for this\none, and then 0 plus square root",
    "start": "2809430",
    "end": "2816000"
  },
  {
    "text": "2 log 1 over delta.  OK.",
    "start": "2816000",
    "end": "2821220"
  },
  {
    "text": "So in this case, you would pick\na1 or a2 with equal probability because the upper confidence\nbound is identical.",
    "start": "2821220",
    "end": "2827470"
  },
  {
    "text": " OK. So we select the arg max.",
    "start": "2827470",
    "end": "2833549"
  },
  {
    "text": "Let's say that we pick a--",
    "start": "2833550",
    "end": "2839510"
  },
  {
    "text": "OK? And now we're going\nto, again, compute the upper confidence bound. So in this case,\nwhat would happen",
    "start": "2839510",
    "end": "2845960"
  },
  {
    "text": "is you would still have--\nyou'd have the following. You would have UCB of a1 is\nequal to 1 plus square root 2",
    "start": "2845960",
    "end": "2855800"
  },
  {
    "text": "log 1 over delta\ndivide by 2, UCB a2 is",
    "start": "2855800",
    "end": "2861530"
  },
  {
    "text": "equal to 1 plus square root\n2 log 1 over delta over 1,",
    "start": "2861530",
    "end": "2867080"
  },
  {
    "text": "and UCB a3 is equal to 0\nplus square root 2 log 1",
    "start": "2867080",
    "end": "2873830"
  },
  {
    "text": "over delta divided by 1. So you can see here\nis that we've now",
    "start": "2873830",
    "end": "2880610"
  },
  {
    "text": "reduced our upper confidence\nbound, because we've learned something new. Now, in this case, we happen to\nhave also gotten high reward.",
    "start": "2880610",
    "end": "2886880"
  },
  {
    "text": "But either way, we\nlearned something new. We could shrink our\nconfidence intervals because we have\nadditional accounts. Just to make sure I'm\nunderstanding correctly,",
    "start": "2886880",
    "end": "2894190"
  },
  {
    "text": "the delta is something that\nwe would select to figure out or to choose our\nconfidence bounds?",
    "start": "2894190",
    "end": "2900710"
  },
  {
    "text": "Yeah, great question. So, yes, we haven't talked a\nlot about how we set delta. Going to be a couple\ncriteria for it.",
    "start": "2900710",
    "end": "2905720"
  },
  {
    "text": "In general, we're going to need\nall of these confidence bounds to hold for all time\nsteps for all arms.",
    "start": "2905720",
    "end": "2911300"
  },
  {
    "text": "So we're going to need\nto do some union bounding to make sure all of them\nsimultaneously hold,",
    "start": "2911300",
    "end": "2916930"
  },
  {
    "text": "because we want to have\nit with high probability, that all of these things\nare valid at the same time.",
    "start": "2916930",
    "end": "2923405"
  },
  {
    "text": "In the simplest setting, we\nknow how many total decisions we're making, and so we need to\nuse that information as well.",
    "start": "2923405",
    "end": "2929170"
  },
  {
    "text": "And then you can use those two\nthings to bound the regret, as we'll see.",
    "start": "2929170",
    "end": "2934660"
  },
  {
    "text": "So you can see, this is why it's\na bit different than greedy. Because we are still using\nour empirical averages",
    "start": "2934660",
    "end": "2940312"
  },
  {
    "text": "but then these\nconfidence intervals are going to change so that\nover time, these will alternate",
    "start": "2940312",
    "end": "2946960"
  },
  {
    "text": "often, depending on what\nrewards you're getting, and you may\nperiodically take a3.",
    "start": "2946960",
    "end": "2952220"
  },
  {
    "text": "Because with that little data,\nthere is some probability that a3 is just as\ngood as a1 and a2,",
    "start": "2952220",
    "end": "2958497"
  },
  {
    "text": "particularly after you\nget additional data. So we'll alternate\nbetween the arms based on these upper\nconfidence bounds.",
    "start": "2958498",
    "end": "2963835"
  },
  {
    "text": " OK.",
    "start": "2963835",
    "end": "2969539"
  },
  {
    "text": "Let's go ahead-- let's\nskip through those here. Let's go to here. OK. So this is-- we're just asking--",
    "start": "2969540",
    "end": "2976030"
  },
  {
    "text": "it's a little bit subtle. If you have a fixed\nnumber of time steps, like, you know the\ntotal you're going",
    "start": "2976030",
    "end": "2981570"
  },
  {
    "text": "to make, like, T decisions,\nyou can set t to be roughly--",
    "start": "2981570",
    "end": "2987250"
  },
  {
    "text": "you probably want\nthis divide by a. This is because you\ncould use a union bound.",
    "start": "2987250",
    "end": "2993640"
  },
  {
    "text": "So why are we doing this? We want these upper\nconfidence bounds to be valid, and we need them to be valid\nat every single time step",
    "start": "2993640",
    "end": "2999420"
  },
  {
    "text": "because we are using\nthem to make decisions. So this is also related to\nfalse discovery and other things",
    "start": "2999420",
    "end": "3006385"
  },
  {
    "text": "like that if you've heard\nabout them in machine learning. So what we're going\nto use here is we're going to think about\nall of these as being events",
    "start": "3006385",
    "end": "3012272"
  },
  {
    "text": "that these confidence\nbounds hold, and what we mean by that\nis that they really do contain the true value--",
    "start": "3012272",
    "end": "3018869"
  },
  {
    "text": "the true unknown value\nwith high probability. So what we're going to\nsay is the probability that all of these\nevents hold, which",
    "start": "3018870",
    "end": "3024260"
  },
  {
    "text": "means that all of our\nconfidence intervals are valid for all of the arms,\nfor all of the time steps, we're just going to\nuse a union bound,",
    "start": "3024260",
    "end": "3030910"
  },
  {
    "text": "which says we're just\ngoing to sum over the probability of each of\nthem over all of those events. So that would be roughly\nthe number of arms times T.",
    "start": "3030910",
    "end": "3039210"
  },
  {
    "text": "And so that's why you can then\njust divide your confidence interval, your delta--",
    "start": "3039210",
    "end": "3045299"
  },
  {
    "text": "so you can just\ndivide your delta into delta divided by t\ntimes the size of your a, and that generally\nis sufficient.",
    "start": "3045300",
    "end": "3052440"
  },
  {
    "text": "And just to think\nabout what that will do in terms of your\nbounds, so remember, we had a log 1 over delta term.",
    "start": "3052440",
    "end": "3058930"
  },
  {
    "text": "So that means you would\nget something like this, log t a divided by delta.",
    "start": "3058930",
    "end": "3064940"
  },
  {
    "text": " So generally, the union building\nblows up your long term.",
    "start": "3064940",
    "end": "3071310"
  },
  {
    "text": "There's various\napproaches, including law of iterated\nlogarithms and others to try to get this\nterm to be smaller.",
    "start": "3071310",
    "end": "3078638"
  },
  {
    "text": "So you can do tighter\nthings on this.  All right.",
    "start": "3078638",
    "end": "3085230"
  },
  {
    "text": "OK. So let's think about-- I promised you that\nwe're going to be able to use this type of\nidea to get sublinear regret.",
    "start": "3085230",
    "end": "3091650"
  },
  {
    "text": "So let's go through\na proof sketch to think about how\nthis actually enables",
    "start": "3091650",
    "end": "3096790"
  },
  {
    "text": "us to get much better\nperformance than what we've seen before. All right.",
    "start": "3096790",
    "end": "3102790"
  },
  {
    "text": "So what this statement says--\nand I'll just put a pointer in. So it's in the references\nunder the website,",
    "start": "3102790",
    "end": "3110630"
  },
  {
    "text": "but there's a great book on-- I think it's just called Bandit\nAlgorithms by Tor Lattimore",
    "start": "3110630",
    "end": "3121920"
  },
  {
    "text": "and Csaba Szepesárí,\nwhich I think maybe",
    "start": "3121920",
    "end": "3127270"
  },
  {
    "text": "came out in 2019 or 2000. I'm trying to remember,\nbut they have a great book. So it came out of a\nseries of blog posts",
    "start": "3127270",
    "end": "3133335"
  },
  {
    "text": "they were doing on\nmulti-armed bandits, and then they turned\nit into a book. And so this is a\nreally nice one. And if you go there, I think\napproximately chapter 7,",
    "start": "3133335",
    "end": "3141849"
  },
  {
    "text": "they're going to do a much more\nrigorous version of this proof compared to what\nI'm doing today. What I'm going to\ntry to do today",
    "start": "3141850",
    "end": "3147730"
  },
  {
    "text": "is just to give you a\nflavor of types of bounds that you might want to prove\nin these sorts of cases",
    "start": "3147730",
    "end": "3153370"
  },
  {
    "text": "and how we end up\ngetting sublinear regret. So what this result\nsays is the following.",
    "start": "3153370",
    "end": "3160370"
  },
  {
    "text": "If you think back,\nwhat we said before is we could bound the expected\nregret by how many times",
    "start": "3160370",
    "end": "3167109"
  },
  {
    "text": "we make-- we choose an arm and how\nmuch gap or loss we have whenever we choose it.",
    "start": "3167110",
    "end": "3173440"
  },
  {
    "text": "And so one thing\nthat we could do is then try to just\nthink about, well, we-- we don't know what the\ngaps are, but the gaps, we",
    "start": "3173440",
    "end": "3179650"
  },
  {
    "text": "can just write down as the\ndifference between the expected reward of that arm versus\nthe true reward of that arm.",
    "start": "3179650",
    "end": "3184860"
  },
  {
    "text": "That's not something\nwe can influence. The thing that we can\ninfluence is how many times we're selecting bad arms.",
    "start": "3184860",
    "end": "3190640"
  },
  {
    "text": "So what this says is that\nif an arm is suboptimal, the number of times that we\npull it, number of times,",
    "start": "3190640",
    "end": "3196620"
  },
  {
    "text": "we take that action in\nupper confidence bounds, scales as a constant C prime-- not going to tell\nyou what that is.",
    "start": "3196620",
    "end": "3203130"
  },
  {
    "text": "Often in the algorithms,\nthey don't tell you what that is either. I mean, it'll be somewhere\nin the fine print. The point is that constant can't\ndepend on parts of the domain.",
    "start": "3203130",
    "end": "3212266"
  },
  {
    "text": "So it can't depend on the number\nof arms or the gaps or things like that. It could be like\n37, for example.",
    "start": "3212267",
    "end": "3218180"
  },
  {
    "text": "So a constant times\nlog of 1 over delta, delta squared plus pi\nsquared over 3 plus 1, OK?",
    "start": "3218180",
    "end": "3227975"
  },
  {
    "text": "So why is this\ninteresting, before we get into how do we prove this? This is interesting because\nit says if the gap is large,",
    "start": "3227975",
    "end": "3235440"
  },
  {
    "text": "we're going to take\nit many less times. So if the gap is\nreally small, then it",
    "start": "3235440",
    "end": "3241010"
  },
  {
    "text": "means that we're\ngoing to-- we might sample that action a lot more. And if the gap is large, we're\ngoing to take it less, OK?",
    "start": "3241010",
    "end": "3248810"
  },
  {
    "text": "And then we can combine\nthat with this equation. And what happens\nin that case is--",
    "start": "3248810",
    "end": "3256276"
  },
  {
    "text": "I'll go through that\npart before we actually think about-- so\nwhat we're going to focus on doing a\nproof sketch of for today",
    "start": "3256277",
    "end": "3261470"
  },
  {
    "text": "is to focus on this part. But let's just think,\nif we could prove that, why that would show the second.",
    "start": "3261470",
    "end": "3267480"
  },
  {
    "text": "Well, what we would get in\nthis case is we would say, we'd get this term\nplugged into here.",
    "start": "3267480",
    "end": "3274450"
  },
  {
    "text": "And the main thing that\nwould happen there is this would become delta,\nbecause we've multiplied it",
    "start": "3274450",
    "end": "3281200"
  },
  {
    "text": "by a delta on top. And then here, if you\nassume that everything",
    "start": "3281200",
    "end": "3286510"
  },
  {
    "text": "is bounded between 0 and 1, then\nthe deltas are at most 1, 2.",
    "start": "3286510",
    "end": "3292250"
  },
  {
    "text": "So you can get-- this is just\nthe number of actions times 1 plus pi squared\nover 3, this term.",
    "start": "3292250",
    "end": "3299290"
  },
  {
    "text": "So this just shows what\nyour total regret would be, in this case, your\ntotal expected regret.",
    "start": "3299290",
    "end": "3305382"
  },
  {
    "text": "As I said, there's quite\na bit more subtleties to the formal proof, but this\njust gives sort of a rough idea.",
    "start": "3305383",
    "end": "3310440"
  },
  {
    "text": "So we have any questions\non that before we dig into how we show\nthe first part, which is the total number of times\nwe're going to take arms,",
    "start": "3310440",
    "end": "3317210"
  },
  {
    "text": "we're going to pull a\nparticular arm, scales with 1 over the size of\nthe gap squared. ",
    "start": "3317210",
    "end": "3330260"
  },
  {
    "text": "All right. Let's go through it. So this is going to heavily\nrely on the Hoeffding",
    "start": "3330260",
    "end": "3336000"
  },
  {
    "text": "inequality and the\nupper confidence bounds. So remember what we\nsaw before is let's",
    "start": "3336000",
    "end": "3343140"
  },
  {
    "text": "imagine that we've got this. So we're going to say, this\nwas our upper confidence bound. So we had this upper\nconfidence bound.",
    "start": "3343140",
    "end": "3350550"
  },
  {
    "text": "And again, I'm going to\nbe loose with the deltas.",
    "start": "3350550",
    "end": "3358480"
  },
  {
    "text": "OK. We'd have to be a little bit\nmore formal about it in general, but let's look at this. So this is going to\nbe the true value,",
    "start": "3358480",
    "end": "3367710"
  },
  {
    "text": "and this is our\nempirical estimate, OK?",
    "start": "3367710",
    "end": "3373770"
  },
  {
    "text": "So what Hoeffding inequality\nhad told us is to say, the difference between\nthe true expected value",
    "start": "3373770",
    "end": "3378990"
  },
  {
    "text": "for an arm and your\nempirical average is greater than this quantity,\nour upper confidence bound,",
    "start": "3378990",
    "end": "3387630"
  },
  {
    "text": "with probability no more\nthan 1 delta over T, OK?",
    "start": "3387630",
    "end": "3393039"
  },
  {
    "text": "So now let's think\nabout the following. Let's think about\nthe times we pull",
    "start": "3393040",
    "end": "3401620"
  },
  {
    "text": "a, which is not equal to\na star, and delta a, which",
    "start": "3401620",
    "end": "3407380"
  },
  {
    "text": "is not equal to 0. These are the only things we\ncare about in terms of regret. If we're pulling a\nstar, we have 0 regret.",
    "start": "3407380",
    "end": "3414770"
  },
  {
    "text": "If we are pulling an arm\nthat has delta a equals 0, that also means that it has\nzero regret, because it means",
    "start": "3414770",
    "end": "3421510"
  },
  {
    "text": "it's tied with an optimal arm. So the only things that\nwe care about bounding here is to think about\nfor that Nt of a,",
    "start": "3421510",
    "end": "3428320"
  },
  {
    "text": "how many times are we pulling\narms that are not optimal, OK? All right.",
    "start": "3428320",
    "end": "3433400"
  },
  {
    "text": "So what we're going to\ndo is observed a couple. So if the confidence\ninterval holds,",
    "start": "3433400",
    "end": "3441289"
  },
  {
    "text": "so we can think\nof if this holds, then we have the following. We have the Q(a) minus C log\nover delta divided by Nt(a).",
    "start": "3441290",
    "end": "3454049"
  },
  {
    "text": "So Here I'll say,\nif one holds, which",
    "start": "3454050",
    "end": "3463570"
  },
  {
    "text": "is less than or equal to Qt\nhat of a, which is less than",
    "start": "3463570",
    "end": "3468820"
  },
  {
    "text": "or equal to Q of a plus\nsquare root C log 1 over delta divided by Nt of a.",
    "start": "3468820",
    "end": "3477640"
  },
  {
    "text": "This just says, if your\nconfidence intervals holds, what it means for it to hold\nis that confidence interval is wide enough that it\ncontains your true value.",
    "start": "3477640",
    "end": "3485320"
  },
  {
    "text": "And the upper confidence\npart is higher than that, and the lower confidence bound\nis lower than your true value. So this is just holds if our\nconfidence intervals hold, OK?",
    "start": "3485320",
    "end": "3494200"
  },
  {
    "text": "Now, if we pull a\ninstead of a star--",
    "start": "3494200",
    "end": "3499690"
  },
  {
    "text": "so under UCB algorithm,\nwe have the following.",
    "start": "3499690",
    "end": "3509040"
  },
  {
    "text": "We know that the upper\nconfidence bound of a",
    "start": "3509040",
    "end": "3514290"
  },
  {
    "text": "was greater than the upper-- ",
    "start": "3514290",
    "end": "3522320"
  },
  {
    "text": "because that's why you pick\nthis alternative action. So in this case, if we\npull this arm a, that",
    "start": "3522320",
    "end": "3528417"
  },
  {
    "text": "means that it's upper\nconfidence bound was different than the\nupper confidence bound of the optimal action,\nand it was more preferred.",
    "start": "3528417",
    "end": "3534960"
  },
  {
    "text": "So that's the only time we\never take the wrong action, is if it's upper\nconfidence bound is higher than the other actions.",
    "start": "3534960",
    "end": "3540650"
  },
  {
    "text": "OK. So let's write down\nwhat that means in terms of its upper bounds. So the definition\nof upper bounds",
    "start": "3540650",
    "end": "3546860"
  },
  {
    "text": "here is that Qt of a\nplus square root C log 1",
    "start": "3546860",
    "end": "3552770"
  },
  {
    "text": "over delta divided by Nt of a\nis greater than Qt of a star",
    "start": "3552770",
    "end": "3561080"
  },
  {
    "text": "plus C log 1 over delta\ndivided by Nt of a star, OK?",
    "start": "3561080",
    "end": "3571370"
  },
  {
    "text": "Because that's just the\ndefinition of our two upper confidence bound. So it says, OK, I'm only going\nto take this other non optimal",
    "start": "3571370",
    "end": "3577040"
  },
  {
    "text": "action because it's upper\nconfidence bound was actually higher than the upper confidence\nbound of the optimal action, OK?",
    "start": "3577040",
    "end": "3583275"
  },
  {
    "text": "And then we notice-- so\nlet's just label them.",
    "start": "3583275",
    "end": "3588690"
  },
  {
    "text": "So we're going to call this 2. I'm going to call this three. So now we're going to\nsubstitute in from 2.",
    "start": "3588690",
    "end": "3599369"
  },
  {
    "text": "OK. All right. So we know that this\nis greater than Qt",
    "start": "3599370",
    "end": "3606900"
  },
  {
    "text": "of a star from equation\n2, because we know",
    "start": "3606900",
    "end": "3612660"
  },
  {
    "text": "that the upper confidence\nbound on the optimal action also holds, so it's\nupper confidence bound",
    "start": "3612660",
    "end": "3617820"
  },
  {
    "text": "has to be higher\nthan its true value. OK?",
    "start": "3617820",
    "end": "3623804"
  },
  {
    "text": "All right. So now what do I have? I have that Q--",
    "start": "3623804",
    "end": "3634840"
  },
  {
    "text": "and let me write\none more thing here. So similarly-- let's check\nthat I get that right.",
    "start": "3634840",
    "end": "3640400"
  },
  {
    "text": "1, 2, 3-- good.",
    "start": "3640400",
    "end": "3648809"
  },
  {
    "text": "Hold on. I just to make sure\nI got that one right. ",
    "start": "3648810",
    "end": "3662340"
  },
  {
    "text": "Yes. OK. So that means that\nQt of-- oh, hold on.",
    "start": "3662340",
    "end": "3672590"
  },
  {
    "text": "All right. So this is going to\nmean that Q of a plus--",
    "start": "3672590",
    "end": "3681347"
  },
  {
    "text": "I'm confusing myself,\nbut I'll figure it out in a second, of Nt of a times\n2 is greater than Q of--",
    "start": "3681347",
    "end": "3695720"
  },
  {
    "text": "oops, I should have written 0. OK. ",
    "start": "3695720",
    "end": "3703319"
  },
  {
    "text": "OK. So let me just make sure\nI did that correctly, because I want that to\nend up going in this case.",
    "start": "3703320",
    "end": "3710450"
  },
  {
    "text": "Let me just make sure that\nI did that in the right way. ",
    "start": "3710450",
    "end": "3717050"
  },
  {
    "text": "I feel like I'm\noff by a constant.  All right.",
    "start": "3717050",
    "end": "3722240"
  },
  {
    "text": "I'll double check the\nconstants afterwards. I'll just write a note. And so I'll check the constants.",
    "start": "3722240",
    "end": "3728855"
  },
  {
    "text": "OK. But the main formula\nis going to be fine. even if you drop\nthe two here, that would--",
    "start": "3728855",
    "end": "3735181"
  },
  {
    "text": "So what are we going\nto have in this case-- ",
    "start": "3735181",
    "end": "3740218"
  },
  {
    "text": "something's bothering me. I'll see if I can figure\nit out in a second. So what we want to argue in this\ncase is that the Q of a that",
    "start": "3740218",
    "end": "3752260"
  },
  {
    "text": "we have plus two of the\nconfidence intervals is going to be greater\nthan Q of a star.",
    "start": "3752260",
    "end": "3759100"
  },
  {
    "text": "And I'm confusing\nmyself slightly now, and I'll check into it later. But what this would\nmean in this case is,",
    "start": "3759100",
    "end": "3764573"
  },
  {
    "text": "let's assume this\nholds for a sec. I'll make sure I get the\nexplanation for next week or I'll just put it on Ed. What we would have\nin this case is",
    "start": "3764573",
    "end": "3771020"
  },
  {
    "text": "we're going to have that\n2 square root C log 1 over delta over\nNt of a is greater",
    "start": "3771020",
    "end": "3781069"
  },
  {
    "text": "than Q of a star minus Q of a,\nwhich is equal to delta A. Let's",
    "start": "3781070",
    "end": "3790040"
  },
  {
    "text": "go to the next slide. OK. So if we have in this case,\nwhat we can then argue",
    "start": "3790040",
    "end": "3798579"
  },
  {
    "text": "is that in this situation,\nwhat we have here is",
    "start": "3798580",
    "end": "3805420"
  },
  {
    "text": "that we can rearrange\nthis to the other side. So let me just do the\nalgebra for that part.",
    "start": "3805420",
    "end": "3812559"
  },
  {
    "text": "So what we're going\nto have is we're going to say that\nfour times C log 1",
    "start": "3812560",
    "end": "3818230"
  },
  {
    "text": "over delta divided by Nt of a is\ngreater than or equal to delta",
    "start": "3818230",
    "end": "3824619"
  },
  {
    "text": "a squared. Which means that if we\nrearrange this here, we have Nt of a is less than\nor equal to 4C log over delta",
    "start": "3824620",
    "end": "3835325"
  },
  {
    "text": "divide by delta a squared, OK? And that looks really like this.",
    "start": "3835325",
    "end": "3843880"
  },
  {
    "text": "OK.  So what does this\nsay intuitively?",
    "start": "3843880",
    "end": "3849140"
  },
  {
    "text": "Intuitively, this is saying,\nif your confidence bounds hold and you use them\nto make decisions,",
    "start": "3849140",
    "end": "3855570"
  },
  {
    "text": "then if those confidence\nbounds are holding, then the only time that you\nmake a decision that is wrong",
    "start": "3855570",
    "end": "3861680"
  },
  {
    "text": "is where these confidence\nbounds is large enough that it overwhelms the gap. ",
    "start": "3861680",
    "end": "3868670"
  },
  {
    "text": "And the number of times\nthat that can occur is finite, because\nthe gap is nonzero.",
    "start": "3868670",
    "end": "3875240"
  },
  {
    "text": "And since we know from\nHoeffding's inequality that the size of the\nconfidence intervals are going down over\ntime, eventually,",
    "start": "3875240",
    "end": "3881780"
  },
  {
    "text": "they will get\nsmaller than the gap. So we're going to take these\nsuboptimal actions less and less",
    "start": "3881780",
    "end": "3888440"
  },
  {
    "text": "often according to how quickly\nyour confidence intervals are contracting relative to\nthe gap in these cases.",
    "start": "3888440",
    "end": "3894745"
  },
  {
    "text": " Do we have any\nquestions about that?",
    "start": "3894745",
    "end": "3900670"
  },
  {
    "start": "3900670",
    "end": "3906510"
  },
  {
    "text": "OK. All right. So what this means is\nthen when we look at this,",
    "start": "3906510",
    "end": "3912560"
  },
  {
    "text": "we end up getting\nthat it achieves logarithmic asymptotic regret\nas a function of log t,",
    "start": "3912560",
    "end": "3919720"
  },
  {
    "text": "because we had the log t here\ninside of the number of times",
    "start": "3919720",
    "end": "3924790"
  },
  {
    "text": "we're taking these\nsuboptimal actions. And what you can see in these\ncases is that over time--",
    "start": "3924790",
    "end": "3931400"
  },
  {
    "text": "so this is a\nprevious result where we look at the amount\nof data that we have and what is the best performance\nthat we have over time.",
    "start": "3931400",
    "end": "3938870"
  },
  {
    "text": "If you tune epsilon greedy,\nit can definitely get better. But also, UCB logs\ndefinitely have",
    "start": "3938870",
    "end": "3944380"
  },
  {
    "text": "this nice logarithmic shape. If you have the right-- if you set the\nconstants correctly.",
    "start": "3944380",
    "end": "3950940"
  },
  {
    "text": "Now empirically,\noften, it will end up being that the\nconstants matter a lot. And so if you set\nthe constants wrong",
    "start": "3950940",
    "end": "3957075"
  },
  {
    "text": "or if you set the\nconstants often to the theoretically\nprescribed value, it'll often explore\nfor a long time.",
    "start": "3957075",
    "end": "3962230"
  },
  {
    "text": "So you can often be more\naggressive than that in terms of the resulting bounds. ",
    "start": "3962230",
    "end": "3971799"
  },
  {
    "text": "So an alternative we\ncould have done to UCB is to always select the arm\nwith the highest lower bound.",
    "start": "3971800",
    "end": "3978819"
  },
  {
    "text": "This can yield linear regret. ",
    "start": "3978820",
    "end": "3989900"
  },
  {
    "text": "So I think that's a useful\nthing to think about. This is optional, but you can\ndo the Check your understanding",
    "start": "3989900",
    "end": "3997549"
  },
  {
    "text": "to think about, why can this\nlead to linear negative regret?",
    "start": "3997550",
    "end": "4003590"
  },
  {
    "text": "And it's helpful to think about\nthe upper confidence bound case and why that one works\nand why this wouldn't.",
    "start": "4003590",
    "end": "4008865"
  },
  {
    "start": "4008865",
    "end": "4046530"
  },
  {
    "text": "So in particular,\nI guess, imagine this was on an exam,\nwhat I would be looking for in this case is for you to\nconstruct a multi-armed bandit",
    "start": "4046530",
    "end": "4053520"
  },
  {
    "text": "case for which selecting\nbased on this criteria would give you linear regret.",
    "start": "4053520",
    "end": "4058740"
  },
  {
    "text": "So if you think\nback to the example I showed you for greedy where we\nconsidered a particular sequence of arm pools such that\nyou would never recover",
    "start": "4058740",
    "end": "4066210"
  },
  {
    "text": "and you'd get linear\nregret in that case, think about this\nsort of setting too. Where based on some\nconfidence intervals,",
    "start": "4066210",
    "end": "4074530"
  },
  {
    "text": "if you select\nwhichever one looks like it's better in\nterms of its lower bound, that you would never recover\nand select the optimal action.",
    "start": "4074530",
    "end": "4080765"
  },
  {
    "start": "4080765",
    "end": "4088680"
  },
  {
    "text": "So I had a question\nabout the slides before where we were assuming\nthat the condition was met.",
    "start": "4088680",
    "end": "4100929"
  },
  {
    "text": "Then I'm assuming the\nother parts came from where the condition isn't met. That's right.",
    "start": "4100930",
    "end": "4106250"
  },
  {
    "text": "Yeah. So in those cases, if you\nset the delta correctly, you can say-- so with high\nprobability, you're going",
    "start": "4106250",
    "end": "4112568"
  },
  {
    "text": "to want this to hold\nfor all time steps, and then there's going to be\nthis small amount of probability that it doesn't hold. And then you can\nargue in that case",
    "start": "4112569",
    "end": "4119170"
  },
  {
    "text": "that the regret is going to be\nbounded from those time points. So you split the--\nit's a good question.",
    "start": "4119170",
    "end": "4124549"
  },
  {
    "text": "You split the expectation\ninto a high probability event and the low probability event. ",
    "start": "4124550",
    "end": "4133370"
  },
  {
    "text": "So why don't we-- why don't you talk\nto a neighbor and see if you got the same thing? ",
    "start": "4133370",
    "end": "4143220"
  },
  {
    "text": "At least one person already\nhas the right answer. [SIDE CONVERSATION]",
    "start": "4143220",
    "end": "4150109"
  },
  {
    "start": "4150110",
    "end": "4163027"
  },
  {
    "text": "Yeah. Oh, good. ",
    "start": "4163028",
    "end": "4251080"
  },
  {
    "text": "OK. I'm going to interrupt you for\na sec for-- interrupt you now. Where would I have to put the\nmean and the upper bound for a2",
    "start": "4251080",
    "end": "4261310"
  },
  {
    "text": "so that being pessimistic fails? ",
    "start": "4261310",
    "end": "4268750"
  },
  {
    "text": "So according to the\nalgorithm, here, if we select the arm with\nthe highest lower bound,",
    "start": "4268750",
    "end": "4274160"
  },
  {
    "text": "we would select a1, because\na2 has a lower bound. But where would I have to put\nits upper bound and its mean for",
    "start": "4274160",
    "end": "4280390"
  },
  {
    "text": "it truly to be-- for us to have linear regret? ",
    "start": "4280390",
    "end": "4291800"
  },
  {
    "text": "So here, I put a\nreward on the y-axis.  At least one person said\nthe right thing in there,",
    "start": "4291800",
    "end": "4298170"
  },
  {
    "text": "so I know one of\nyou guys know this. The mean of a2 [INAUDIBLE]\nshould be higher",
    "start": "4298170",
    "end": "4305460"
  },
  {
    "text": "than the mean of a1. Yeah. And then the other bound\nshould be high as well? It should be really high. Yeah, that's right.",
    "start": "4305460",
    "end": "4310800"
  },
  {
    "text": "So for example, could have this. So you could be really\nuncertain about it.",
    "start": "4310800",
    "end": "4318140"
  },
  {
    "text": "Its lower bound is lower. Once you pick a1, the\nlower bound here--",
    "start": "4318140",
    "end": "4324500"
  },
  {
    "text": "this is an expectation, is\nonly going to get closer. Like, the lower bound-- these are valid\nconfidence intervals.",
    "start": "4324500",
    "end": "4330540"
  },
  {
    "text": "This lower bound really is\nsmaller than the mean of a1. Which means on average,\nwhenever we sample a1,",
    "start": "4330540",
    "end": "4336930"
  },
  {
    "text": "this is really just\ngoing to shrink, which means we'll never pull a2.",
    "start": "4336930",
    "end": "4342836"
  },
  {
    "text": "A2's upper confidence\nbound is higher than a1. So under UCB, we\nwould learn this.",
    "start": "4342836",
    "end": "4348180"
  },
  {
    "text": "But if you're pessimistic-- in some way, if you think about\nit for upper confidence bounds,",
    "start": "4348180",
    "end": "4353610"
  },
  {
    "text": "if you're optimistic,\neither you're correct or you learned something. The problem with\nbeing pessimistic",
    "start": "4353610",
    "end": "4359060"
  },
  {
    "text": "is that you may not learn\nanything, because you're not updating your other bounds.",
    "start": "4359060",
    "end": "4365250"
  },
  {
    "text": "OK. I realized where I\nwas being confused, so let me go back and\njust correct that here. OK. So how did I get these?",
    "start": "4365250",
    "end": "4371579"
  },
  {
    "text": "So let me just clarify. It was this step. I was confusing myself. OK. So we had this\nparticular equation,",
    "start": "4371580",
    "end": "4376739"
  },
  {
    "text": "that the empirical average\nplus its upper confidence bound was bigger than the optimal\narm's empirical average",
    "start": "4376740",
    "end": "4382200"
  },
  {
    "text": "plus its upper bound. What I did from equation\n2 is I reminded--",
    "start": "4382200",
    "end": "4389910"
  },
  {
    "text": "remind ourselves that the\nempirical average is always less than or equal to the true value\nplus the upper confidence bound.",
    "start": "4389910",
    "end": "4397469"
  },
  {
    "text": "So we substitute that in for\nQt to get the Q(a) plus 2 times",
    "start": "4397470",
    "end": "4402960"
  },
  {
    "text": "the bound. OK. So that's why this works out. So you just substitute with\nthis upper bound into here.",
    "start": "4402960",
    "end": "4410740"
  },
  {
    "text": "So then it gets another-- it gets Q of a plus this upper\nbound plus this upper bound,",
    "start": "4410740",
    "end": "4416050"
  },
  {
    "text": "which means this\nbound becomes 2. So that's where that came from. ",
    "start": "4416050",
    "end": "4422280"
  },
  {
    "text": "OK. So this is the first\nalgorithm we've seen which has provably\nsublinear regret, which is really nice. It's also really easy\nto implement, certainly",
    "start": "4422280",
    "end": "4429130"
  },
  {
    "text": "when you have counts. But all of this stuff can\nbe extended to much more complicated settings. And so there's a lot of work\nof thinking about for function",
    "start": "4429130",
    "end": "4436480"
  },
  {
    "text": "approximation and\nRL, and we'll see all of those, of ways to\nthink about formalizing",
    "start": "4436480",
    "end": "4442449"
  },
  {
    "text": "this optimism under\nuncertainty principle in order to make decisions\nwhen we don't know what",
    "start": "4442450",
    "end": "4448119"
  },
  {
    "text": "the outcomes will be in order\nto reduce our regret over time.",
    "start": "4448120",
    "end": "4453190"
  },
  {
    "text": "So what we're going\nto see next time is we're going to see\nmore fast learning, but we're also going\nto think about it",
    "start": "4453190",
    "end": "4458743"
  },
  {
    "text": "from a very different\nperspective called Bayesian bandits where we think\nof it not being these just fixed",
    "start": "4458743",
    "end": "4464530"
  },
  {
    "text": "upper and lower rectangular\nconfidence intervals, but we think of having\na prior over what the distribution is going to\nbe of the rewards for each arm.",
    "start": "4464530",
    "end": "4472210"
  },
  {
    "text": "And then in that\ncase, we can also introduce algorithms that\nend up being somewhat similar to optimism\nin certain ways",
    "start": "4472210",
    "end": "4479200"
  },
  {
    "text": "as ways to use those prior\ninformations to figure out how to quickly gather data and\nstart to make good decisions.",
    "start": "4479200",
    "end": "4485370"
  },
  {
    "text": "So we'll see that next week. Thanks. ",
    "start": "4485370",
    "end": "4493000"
  }
]