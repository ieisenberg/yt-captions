[
  {
    "start": "0",
    "end": "110000"
  },
  {
    "start": "0",
    "end": "5478"
  },
  {
    "text": "OK. Hi, everyone. Welcome back to CS224N. ",
    "start": "5478",
    "end": "10810"
  },
  {
    "text": "So today is a pretty\nkey lecture where we get through a number\nof important topics",
    "start": "10810",
    "end": "17140"
  },
  {
    "text": "for neural networks,\nespecially as it is applied to natural language processing. So right at the\nend of last time,",
    "start": "17140",
    "end": "23500"
  },
  {
    "text": "I started into recurrent\nneural networks. So we'll talk in detail more\nabout recurrent neural networks",
    "start": "23500",
    "end": "28750"
  },
  {
    "text": "in the first part of the class. And we emphasized\nlanguage models.",
    "start": "28750",
    "end": "34820"
  },
  {
    "text": "But then also getting a\nbit beyond that and then looked at more advanced kinds\nof recurrent neural networks",
    "start": "34820",
    "end": "41859"
  },
  {
    "text": "towards the end\npart of the class. I just wanted to sort of say\na word before getting underway",
    "start": "41860",
    "end": "49149"
  },
  {
    "text": "about the final project. So hopefully, by\nnow, you've started looking at assignment\n3, which is",
    "start": "49150",
    "end": "55360"
  },
  {
    "text": "the middle of the five\nassignments for the first half of the course. And then in the second half of\nthe course, most of your effort",
    "start": "55360",
    "end": "61870"
  },
  {
    "text": "goes into a final project. So next week, the\nThursday lecture is going to be\nabout final projects",
    "start": "61870",
    "end": "68470"
  },
  {
    "text": "and choosing the final\nproject, and tips for final projects, et cetera. So it's fine to delay\nthinking about final projects",
    "start": "68470",
    "end": "75490"
  },
  {
    "text": "until next week if you want. But you shouldn't\ndelay it too long because we do want\nyou to get underway",
    "start": "75490",
    "end": "81640"
  },
  {
    "text": "with what topic you're going\nto do for your final project. If you are thinking\nabout final projects,",
    "start": "81640",
    "end": "86710"
  },
  {
    "text": "you can find some\ninfo on the website, but note that the info\nthat's there at the moment",
    "start": "86710",
    "end": "91840"
  },
  {
    "text": "is still last\nyear's information, and it will be being updated\nover the coming week.",
    "start": "91840",
    "end": "98040"
  },
  {
    "text": "We'll also talk about\nproject mentors. If you've got ideas of\npeople who on your own you can line up as\na mentor, that now",
    "start": "98040",
    "end": "104720"
  },
  {
    "text": "would be a good time\nto ask them about it. And we'll sort of talk about\nwhat the alternatives are.",
    "start": "104720",
    "end": "111070"
  },
  {
    "start": "110000",
    "end": "182000"
  },
  {
    "text": "OK. So last lecture, I\nintroduced the idea",
    "start": "111070",
    "end": "116250"
  },
  {
    "text": "of language models, so\nprobabilistic models that predict the probability of next\nwords after a word sequence,",
    "start": "116250",
    "end": "123300"
  },
  {
    "text": "and then we looked at\nn-gram language models and started into recurrent\nneural network models.",
    "start": "123300",
    "end": "128830"
  },
  {
    "text": "So today, we're\ngoing to talk more about the simple RNNs we saw\nbefore, talking about training",
    "start": "128830",
    "end": "134880"
  },
  {
    "text": "RNNs and uses of RNNs. But then we'll also\nlook into the problems that occur with RNNs and\nhow we might fix them.",
    "start": "134880",
    "end": "143490"
  },
  {
    "text": "These will motivate a more\nsophisticated RNN architecture called LSTMS, and we'll talk\nabout other more complex RNN",
    "start": "143490",
    "end": "150930"
  },
  {
    "text": "options, bidirectional\nRNNs, and multilayer RNNs. Then next Tuesday,\nwe're essentially",
    "start": "150930",
    "end": "158790"
  },
  {
    "text": "going to further\nexploit and build on the RNN based architectures\nthat we've been looking at",
    "start": "158790",
    "end": "165990"
  },
  {
    "text": "to discuss how to build a\nneural machine translation system with the\nsequence-to-sequence",
    "start": "165990",
    "end": "171510"
  },
  {
    "text": "and model with attention. And effectively\nit's that model is what you'll use in\nassignment 4, but it also",
    "start": "171510",
    "end": "178500"
  },
  {
    "text": "means that we'll be\nusing all of the stuff that we're talking about today. OK.",
    "start": "178500",
    "end": "183820"
  },
  {
    "text": "So if you remember\nfrom last time, this was the idea of a simple\nrecurrent neural network",
    "start": "183820",
    "end": "189460"
  },
  {
    "text": "language model. So we had a sequence of words\nas our context for which we've",
    "start": "189460",
    "end": "195700"
  },
  {
    "text": "looked up word embeddings. And then the recurrent\nneural network model ran this recurrent layer,\nwhere at each point,",
    "start": "195700",
    "end": "204580"
  },
  {
    "text": "we have a previous hidden\nstate, which can just be 0 at the beginning\nof a sequence,",
    "start": "204580",
    "end": "211090"
  },
  {
    "text": "and you have feeding it in\nto the next hidden state, the previous hidden state,\nand transform the encoding",
    "start": "211090",
    "end": "220960"
  },
  {
    "text": "of a word using this\nrecurrent neural network equation that I have on the\nleft that's very central.",
    "start": "220960",
    "end": "228040"
  },
  {
    "text": "And based on that, you compute\na new hidden representation for the next time step.",
    "start": "228040",
    "end": "234159"
  },
  {
    "text": "And you can repeat that along\nfor successive time steps.",
    "start": "234160",
    "end": "239200"
  },
  {
    "text": "Now, we also usually want\nour recurrent neural networks to produce outputs.",
    "start": "239200",
    "end": "244930"
  },
  {
    "text": "So I only show it\nat the end here. But at each time\nstep we're then also",
    "start": "244930",
    "end": "250840"
  },
  {
    "text": "going to generate an output. And so to do that, we're\nfeeding the hidden layer",
    "start": "250840",
    "end": "256299"
  },
  {
    "text": "into a softmax layer. So we're doing another matrix\nmultiply, add on a bias, put it through the\nsoftmax equation.",
    "start": "256300",
    "end": "263350"
  },
  {
    "text": "And that then gives\nthe probability distribution over words. And we can use that to\npredict how likely it",
    "start": "263350",
    "end": "270070"
  },
  {
    "text": "is that different\nwords are going to occur after the\nstudents opened their.",
    "start": "270070",
    "end": "276740"
  },
  {
    "start": "276000",
    "end": "323000"
  },
  {
    "text": "OK. So I introduced that\nmodel, but I hadn't really gone through the specifics of\nhow we train this model, how",
    "start": "276740",
    "end": "284500"
  },
  {
    "text": "we use it and evaluate it. So let me go through this now.",
    "start": "284500",
    "end": "290360"
  },
  {
    "text": "So here's how we train\nan RNN language model. We get a big corpus of\ntext, just a lot of text.",
    "start": "290360",
    "end": "297169"
  },
  {
    "text": "And so we can regard that\nas just a long sequence of words, x1 to xT.",
    "start": "297170",
    "end": "302979"
  },
  {
    "text": "And what we're going to do\nis feed it into the RNN-LM. So for each\nposition, we're going",
    "start": "302980",
    "end": "309760"
  },
  {
    "text": "to take prefixes\nof that sequence. And based on each\nprefix, we're going",
    "start": "309760",
    "end": "316060"
  },
  {
    "text": "to want to predict the\nprobability distribution for the word that comes next.",
    "start": "316060",
    "end": "323540"
  },
  {
    "start": "323000",
    "end": "593000"
  },
  {
    "text": "And then we're going\nto train our model by assessing how good\na job we do about that.",
    "start": "323540",
    "end": "330150"
  },
  {
    "text": "And so the loss function we use\nis the loss function, normally referred to as cross-entropy\nloss in the literature,",
    "start": "330150",
    "end": "338010"
  },
  {
    "text": "which is this negative\nlog likelihood loss. So we are going to predict\nsome word to come next.",
    "start": "338010",
    "end": "345630"
  },
  {
    "text": "Well, we have a probability\ndistribution over predictions of what word comes next.",
    "start": "345630",
    "end": "351139"
  },
  {
    "text": "And actually, there was an\nactual next word in the text. And so we say, well,\nwhat probability",
    "start": "351140",
    "end": "356330"
  },
  {
    "text": "did you give to that word? And maybe we gave it a\nprobability estimate of 0.01. Well, it would have been great\nif we'd given a probability",
    "start": "356330",
    "end": "363500"
  },
  {
    "text": "estimate of almost\n1, because that meant we're almost\ncertain that what did come next in our model.",
    "start": "363500",
    "end": "370560"
  },
  {
    "text": "And so we'll take a\nloss to the extent that we are giving\nthe actual next word",
    "start": "370560",
    "end": "376610"
  },
  {
    "text": "a predicted probability\nof less than 1. So then getting an\nidea of how well",
    "start": "376610",
    "end": "382870"
  },
  {
    "text": "we're doing over\nthe entire corpus, we work out that\nloss at each position",
    "start": "382870",
    "end": "390159"
  },
  {
    "text": "and then we work out the average\nloss of the entire training set. So let's just go\nthrough that again",
    "start": "390160",
    "end": "396760"
  },
  {
    "text": "more graphically in the\nnext couple of slides. So down at the bottom,\nhere is our corpus of text.",
    "start": "396760",
    "end": "404439"
  },
  {
    "text": "We're running it through\nour simple recurrent neural network. And at each position, we've\npredicting a probability",
    "start": "404440",
    "end": "412570"
  },
  {
    "text": "distribution over words. We then say, well,\nactually, in each position,",
    "start": "412570",
    "end": "420050"
  },
  {
    "text": "we know what word\nis actually next. So when we're at time step\n1, the actual next word",
    "start": "420050",
    "end": "426310"
  },
  {
    "text": "is students, because we can see\nit just to the right of us here and we say, what\nprobability estimate",
    "start": "426310",
    "end": "431439"
  },
  {
    "text": "did you give to students and to\nthe extent that it's not high? I.e., it's not 1.",
    "start": "431440",
    "end": "436780"
  },
  {
    "text": "We take a loss. And then we go on\nto the time step 2, and we say, well,\nat time step 2,",
    "start": "436780",
    "end": "443289"
  },
  {
    "text": "you predict the probability\ndistribution over words. The actual next word is opened.",
    "start": "443290",
    "end": "448604"
  },
  {
    "text": "So to the extent\nthat you haven't given the high\nprobability to open, you take a loss and\nthen that repeats.",
    "start": "448605",
    "end": "455230"
  },
  {
    "text": "And time step 4, we're hoping\nthe model will predict their. At time step 4, we are hoping\nthe model will predict exams.",
    "start": "455230",
    "end": "463360"
  },
  {
    "text": "And then to work out our overall\nloss within averaging out",
    "start": "463360",
    "end": "469000"
  },
  {
    "text": "per time step loss. So in a way this is a\npretty obvious thing to do.",
    "start": "469000",
    "end": "475689"
  },
  {
    "text": "But note that there is\na little subtlety here. And in particular,\nthis algorithm",
    "start": "475690",
    "end": "481599"
  },
  {
    "text": "is referred to in the\nliterature as teacher forcing. And so what does that mean?",
    "start": "481600",
    "end": "487420"
  },
  {
    "text": "Well, you can\nimagine what you can do with a recurrent\nneural network is, say, okay, just\nstart generating.",
    "start": "487420",
    "end": "495340"
  },
  {
    "text": "Maybe I'll give you a\nhint as to where to start. I'll say the sentence\nstarts, the students, and then let it run and see\nwhat it generates coming next.",
    "start": "495340",
    "end": "504970"
  },
  {
    "text": "It might start\nsaying, the students have been locked\nout of the classroom or whatever it is, right?",
    "start": "504970",
    "end": "511630"
  },
  {
    "text": "And that, we could say\nis, well, that's not very close to what\nthe actual text says. And somehow we want\nto learn from that.",
    "start": "511630",
    "end": "518529"
  },
  {
    "text": "And if you go in that\ndirection, there's a space of things\nyou can do that leads into more complex algorithms\nsuch as reinforcement learning.",
    "start": "518530",
    "end": "527240"
  },
  {
    "text": "But from the perspective of\ntraining these neural models, that's unduly complex\nand unnecessary.",
    "start": "527240",
    "end": "535399"
  },
  {
    "text": "So we have this very\nsimple way of doing things, which is\nwhat we do is just",
    "start": "535400",
    "end": "541329"
  },
  {
    "text": "predict one time step forward. So we say, we know that\nthe prefix is the students.",
    "start": "541330",
    "end": "547360"
  },
  {
    "text": "Predict the probability\ndistribution over the next word. It's good to the extent\nthat you give probability",
    "start": "547360",
    "end": "553060"
  },
  {
    "text": "mass to opened. OK, now the prefix is\nthe students opened. Predict a probability\ndistribution",
    "start": "553060",
    "end": "559630"
  },
  {
    "text": "over the next word. It's good to the extent that you\ngive probability mass to their. And so effectively,\nat each step,",
    "start": "559630",
    "end": "568480"
  },
  {
    "text": "we are resetting to what\nwas actually in the corpus. So you know it's possible\nafter the students opened,",
    "start": "568480",
    "end": "574060"
  },
  {
    "text": "the model thinks that by\nfar the most probable thing to come next is a, or the, say.",
    "start": "574060",
    "end": "580120"
  },
  {
    "text": "I mean, we don't actually\nuse what the model suggested. We penalize the model for\nnot having suggested their.",
    "start": "580120",
    "end": "586839"
  },
  {
    "text": "But then we just go with\nwhat's actually in the corpus and ask it to predict again. ",
    "start": "586840",
    "end": "595760"
  },
  {
    "text": "This is just a\nlittle side thing. But it's an important\npart to know",
    "start": "595760",
    "end": "601790"
  },
  {
    "text": "if you're actually training\nyour own neural language model. I sort of present it as one huge\ncorpus that we chug through.",
    "start": "601790",
    "end": "609800"
  },
  {
    "text": "But in practice, we don't chug\nthrough a whole corpus one step at a time.",
    "start": "609800",
    "end": "615889"
  },
  {
    "text": "What we do is we\ncut the whole corpus into shorter pieces,\nwhich might commonly",
    "start": "615890",
    "end": "622010"
  },
  {
    "text": "be sentences or\ndocuments or sometimes they're literally just pieces\nthat are chopped, right?",
    "start": "622010",
    "end": "627270"
  },
  {
    "text": "So you'll recall that stochastic\ngradient descent allows us to compute a loss and\ngradients for a small chunk",
    "start": "627270",
    "end": "633019"
  },
  {
    "text": "of data and update. So what we do is we\ntake these small pieces, compute gradients from those,\nand update weights and repeat.",
    "start": "633020",
    "end": "641930"
  },
  {
    "text": "And in particular, we get a\nlot more speed and efficiency in training if we\naren't actually",
    "start": "641930",
    "end": "648740"
  },
  {
    "text": "doing an update for just\none sentence at a time but actually a\nbatch of sentences.",
    "start": "648740",
    "end": "654560"
  },
  {
    "text": "So typically what\nwe'll actually do is we'll feed to the\nmodel 32 sentences,",
    "start": "654560",
    "end": "659810"
  },
  {
    "text": "say, of a similar\nlength at the same time, compute gradients for them,\nupdate weights, and then get",
    "start": "659810",
    "end": "666829"
  },
  {
    "text": "another batch of\nsentences to train on.  How do we train?",
    "start": "666830",
    "end": "672660"
  },
  {
    "start": "670000",
    "end": "828000"
  },
  {
    "text": "I haven't sort of gone\nthrough the details of this. I mean, in one sense, the answer\nis just like we talked about",
    "start": "672660",
    "end": "679770"
  },
  {
    "text": "in lecture 3. We used backpropagation to\nget gradients and update parameters.",
    "start": "679770",
    "end": "685380"
  },
  {
    "text": "But let's take at least a minute\nto go through the differences and subtleties of the\nrecurrent neural network case.",
    "start": "685380",
    "end": "693690"
  },
  {
    "text": "And the central\nthing that's a bit-- as before, we're\ngoing to take our loss",
    "start": "693690",
    "end": "699870"
  },
  {
    "text": "and we're going to\nback propagate it to all of the parameters of the\nnetwork, everything from word",
    "start": "699870",
    "end": "706110"
  },
  {
    "text": "embeddings to biases, et cetera. But the central bit that's\na little bit different",
    "start": "706110",
    "end": "711779"
  },
  {
    "text": "and is more complicated is that\nwe have this WH matrix that",
    "start": "711780",
    "end": "718050"
  },
  {
    "text": "runs along the sequence\nthat we keep on applying to update our hidden state.",
    "start": "718050",
    "end": "725200"
  },
  {
    "text": "So what's the derivative\nof Jt of theta with respect to the\nrepeated weight matrix Wh?",
    "start": "725200",
    "end": "733170"
  },
  {
    "text": "And well, the answer to\nthat is that what we do is we look at it\nin each position",
    "start": "733170",
    "end": "742399"
  },
  {
    "text": "and work out what the partials\nare of Jt with respect to Wh",
    "start": "742400",
    "end": "749810"
  },
  {
    "text": "in position 1 or position 2,\nposition 3, position 4, et",
    "start": "749810",
    "end": "754940"
  },
  {
    "text": "cetera, right\nalong the sequence. And we just sum up\nall of those partials. And that gives us partial for\nJt with respect to Wh overall.",
    "start": "754940",
    "end": "766880"
  },
  {
    "text": "So the answer for\nrecurrent neural networks is the gradient with\nrespect to repeated",
    "start": "766880",
    "end": "773839"
  },
  {
    "text": "weight in our current\nnetwork is the sum of the gradient with respect\nto each time it appears.",
    "start": "773840",
    "end": "781420"
  },
  {
    "text": "And let me just then go through\na little why that is the case.",
    "start": "781420",
    "end": "786440"
  },
  {
    "text": "But before I do that, let\nme just note one gocha. I mean, it's just not\nthe case that this",
    "start": "786440",
    "end": "794030"
  },
  {
    "text": "means it equals t\ntimes the partial of Jt",
    "start": "794030",
    "end": "800210"
  },
  {
    "text": "with respect to Wh. Because we're using Wh\nhere, here, here, here, here",
    "start": "800210",
    "end": "805820"
  },
  {
    "text": "through the sequence. And for each of the\nplaces we use it, there's a different\nupstream gradient",
    "start": "805820",
    "end": "811819"
  },
  {
    "text": "that's being fed into it. So each of the\nvalues in this sum will be completely\ndifferent from each other.",
    "start": "811820",
    "end": "819890"
  },
  {
    "text": "Well, why we get this\nanswer is essentially a consequence of what we talked\nabout in the third lecture.",
    "start": "819890",
    "end": "829560"
  },
  {
    "start": "828000",
    "end": "1089000"
  },
  {
    "text": "So to take the simplest\ncase of it, right, that if you have a\nmultivariable function f(x,",
    "start": "829560",
    "end": "835340"
  },
  {
    "text": "y) and you have two\nsingle variable functions,",
    "start": "835340",
    "end": "840920"
  },
  {
    "text": "x(t) and y(t) which are\nfed one input t, well,",
    "start": "840920",
    "end": "845959"
  },
  {
    "text": "then the simple version of\nworking out the derivative",
    "start": "845960",
    "end": "852020"
  },
  {
    "text": "of this function is you take\nthe derivative down one path and you take the derivative\ndown the other path.",
    "start": "852020",
    "end": "859500"
  },
  {
    "text": "And so, in the\nslides in lecture 3, that was what was summarized\non a couple of slides",
    "start": "859500",
    "end": "866420"
  },
  {
    "text": "by the slogan gradient\nsum at outward branches. So t has outward branches.",
    "start": "866420",
    "end": "873079"
  },
  {
    "text": "And so you take gradient\nhere on the left, gradient on the right, and you\nsum them together.",
    "start": "873080",
    "end": "879720"
  },
  {
    "text": "And so really what's happening\nwith a recurrent neural network is just many pieces\ngeneralization of this.",
    "start": "879720",
    "end": "887760"
  },
  {
    "text": "So we have one Wh\nmatrix and we're using it to keep on updating the\nhidden state at time 1, time 2,",
    "start": "887760",
    "end": "895700"
  },
  {
    "text": "time 3, right through time t. And so what we're going\nto get is that this",
    "start": "895700",
    "end": "903410"
  },
  {
    "text": "has a lot of outward branches. And we're going to\nsum the gradient path",
    "start": "903410",
    "end": "911000"
  },
  {
    "text": "at each one of them. But what is this\ngradient path here?",
    "start": "911000",
    "end": "916580"
  },
  {
    "text": "It kind of goes down here\nand then goes down there. But actually the bottom\npart is that we're just",
    "start": "916580",
    "end": "923630"
  },
  {
    "text": "using Wh at each position. So we have the partial\nWh used at position i",
    "start": "923630",
    "end": "931310"
  },
  {
    "text": "with respect to\nthe partial of Wh which is just our weight\nmatrix for our recurrent neural",
    "start": "931310",
    "end": "937100"
  },
  {
    "text": "network. So that's just one. Because we're just using\nthe same matrix everywhere.",
    "start": "937100",
    "end": "943500"
  },
  {
    "text": "And so we are just then summing\nthe partials in each position",
    "start": "943500",
    "end": "949160"
  },
  {
    "text": "that we use that.  OK, practically, what\ndoes that mean in terms",
    "start": "949160",
    "end": "956110"
  },
  {
    "text": "of how you compute this? Well, if you're doing\nit by hand, what happens",
    "start": "956110",
    "end": "962980"
  },
  {
    "text": "is you start at the end just\nlike the general lecture 3 story. You work out derivatives with\nrespect to the hidden layer",
    "start": "962980",
    "end": "974500"
  },
  {
    "text": "and then with respect to\nWh at the last time step. And so that gives you\none update for Wh.",
    "start": "974500",
    "end": "983170"
  },
  {
    "text": "But then you continue\npassing the gradient back to the t minus 1 time step.",
    "start": "983170",
    "end": "988430"
  },
  {
    "text": "And after a couple more\nsteps of the chain rule, you get another update for Wh.",
    "start": "988430",
    "end": "994330"
  },
  {
    "text": "And you simply sum that onto\nyour previous update for Wh. And then you go to ht minus 2.",
    "start": "994330",
    "end": "1001140"
  },
  {
    "text": "You get another update for Wh. And you sum that onto\nyour update for Wh. And you go back all\nthe way and you sum up",
    "start": "1001140",
    "end": "1008970"
  },
  {
    "text": "the gradients as you go. And that gives you a\ntotal update for Wh.",
    "start": "1008970",
    "end": "1015990"
  },
  {
    "text": "And so there's sort\nof two tricks here. And I'll just mention\nthe two tricks.",
    "start": "1015990",
    "end": "1021630"
  },
  {
    "text": "You have to kind of separately\nsum the updates for Wh. And then once you've finished,\napply them all at once.",
    "start": "1021630",
    "end": "1029109"
  },
  {
    "text": "You don't want to actually\nbe changing the Wh matrix as you go. Because that's then\ninvalid because",
    "start": "1029109",
    "end": "1035520"
  },
  {
    "text": "the forward calculations were\ndone with the constant Wh that you had from the previous\nstate all through the network.",
    "start": "1035520",
    "end": "1045760"
  },
  {
    "text": "The second trick\nis, well, if you're doing this for sentences,\nyou can normally just go back",
    "start": "1045760",
    "end": "1051420"
  },
  {
    "text": "to the beginning\nof the sentence. But if you've got\nvery long sequences, this can really slow\nyou down if you're",
    "start": "1051420",
    "end": "1058530"
  },
  {
    "text": "having to sort of run\nthis algorithm back for a huge amount of time.",
    "start": "1058530",
    "end": "1063850"
  },
  {
    "text": "So something people\ncommonly do is what's called truncated back\npropagation through time where",
    "start": "1063850",
    "end": "1069840"
  },
  {
    "text": "you choose some\nconstant, say 20, and you say, well,\nI'm just going to run this back propagation\nfor 20 time steps, sum those 20",
    "start": "1069840",
    "end": "1078900"
  },
  {
    "text": "gradients, and\nthen I'm just done. That's what I'll update\nthe Wh matrix with.",
    "start": "1078900",
    "end": "1086280"
  },
  {
    "text": "And that works just fine.  So now given a corpus, we\ncan train a simple RNN.",
    "start": "1086280",
    "end": "1097940"
  },
  {
    "start": "1089000",
    "end": "1253000"
  },
  {
    "text": "So that's good progress. But this is a model that can\nalso generate text in general.",
    "start": "1097940",
    "end": "1105190"
  },
  {
    "text": "So how do we generate text? Well, just like in our\nn-gram language model,",
    "start": "1105190",
    "end": "1110600"
  },
  {
    "text": "we're going to generate\ntext by repeated sampling. So we're going to start\noff with an initial state",
    "start": "1110600",
    "end": "1119700"
  },
  {
    "text": "and this slide is imperfect.",
    "start": "1119700",
    "end": "1124830"
  },
  {
    "text": "So the initial state\nfor the hidden state is normally just taken\nas a zero vector.",
    "start": "1124830",
    "end": "1132290"
  },
  {
    "text": "And while then we need to have\nsomething for a first input. And on this slide,\nthe first input",
    "start": "1132290",
    "end": "1137480"
  },
  {
    "text": "is shown as the first word, my. And if you want to\nfeed a starting point, you could feed my.",
    "start": "1137480",
    "end": "1143540"
  },
  {
    "text": "But a lot of the\ntime, you'd like to generate a\nsentence from nothing. And if you want to do\nthat, what's conventional",
    "start": "1143540",
    "end": "1149780"
  },
  {
    "text": "is to additionally have a\nbeginning of sequence token which is a special token.",
    "start": "1149780",
    "end": "1154820"
  },
  {
    "text": "So you'll feed in the\nbeginning of sequence token in at the beginning\nas the first token. It has an embedding.",
    "start": "1154820",
    "end": "1161180"
  },
  {
    "text": "And then you use the RNN update. And then you generate using\nthe softmax a next word.",
    "start": "1161180",
    "end": "1168770"
  },
  {
    "text": "And well, you generate a\nprobability distribution over next words.",
    "start": "1168770",
    "end": "1174649"
  },
  {
    "text": "And then at that point,\nyou sample from that. And it chooses some\nword like favorite.",
    "start": "1174650",
    "end": "1180259"
  },
  {
    "text": "And so then, the trick\nis for doing generation that you take this word that\nyou sampled and you copy it",
    "start": "1180260",
    "end": "1187310"
  },
  {
    "text": "back down to the input and then\nyou feed it in as an input. Next step of your RNN,\nsample from the softmax,",
    "start": "1187310",
    "end": "1195050"
  },
  {
    "text": "get another word, and just keep\nrepeating this over and over again. And you start\ngenerating the text.",
    "start": "1195050",
    "end": "1202580"
  },
  {
    "text": "And how you end is as well as\nhaving a beginning of sequence",
    "start": "1202580",
    "end": "1207919"
  },
  {
    "text": "special symbol, you\nusually have an end of sequence special symbol. And at some point, the\nrecurrent neural network",
    "start": "1207920",
    "end": "1215600"
  },
  {
    "text": "will generate the end\nof sequence symbol. And then you say, OK, I'm done.",
    "start": "1215600",
    "end": "1221760"
  },
  {
    "text": "I'm finished generating text. So before going on for more\nof the difficult content",
    "start": "1221760",
    "end": "1230230"
  },
  {
    "text": "of the lecture, we can just have\na little bit of fun with this and try training up\nand generating text",
    "start": "1230230",
    "end": "1237460"
  },
  {
    "text": "with our recurrent\nneural network model. So you can generate--\nyou can train an RNN on any kind of text.",
    "start": "1237460",
    "end": "1243919"
  },
  {
    "text": "And so that means one of the\nfun things that you can do is generate text\nin different styles",
    "start": "1243920",
    "end": "1250150"
  },
  {
    "text": "based on what you\ncould train it from.  So here, Harry Potter,\nthere is a fair amount",
    "start": "1250150",
    "end": "1258950"
  },
  {
    "start": "1253000",
    "end": "1458000"
  },
  {
    "text": "of corpus of text. So you can train an RNN-LM\nL on the Harry Potter books. And then say, go off\nand generate some text.",
    "start": "1258950",
    "end": "1266480"
  },
  {
    "text": "And it'll generate\ntext like this. \"Sorry,\" Harry\nshouted, panicking-- \"I'll leave those brooms\nin London, are they?\"",
    "start": "1266480",
    "end": "1273950"
  },
  {
    "text": "\"No idea,\" said Nearly Headless\nNick, casting low close by Cedric, carrying the\nlast bit of treacle Charms",
    "start": "1273950",
    "end": "1279890"
  },
  {
    "text": "from Harry's shoulder,\nand to answer him the common room perched\nupon it, four arms held a shining knob from\nwhen the spider hadn't",
    "start": "1279890",
    "end": "1287270"
  },
  {
    "text": "felt it seemed. He reached the teams too. Well, so on the one\nhand, that's still",
    "start": "1287270",
    "end": "1294260"
  },
  {
    "text": "kind of a bit\nincoherent as a story. On the other hand, it sort\nof sounds like Harry Potter.",
    "start": "1294260",
    "end": "1300590"
  },
  {
    "text": "It's certainly the kind of\nvocabulary and constructions it uses. And I think you'd agree that,\neven though it gets sort",
    "start": "1300590",
    "end": "1309320"
  },
  {
    "text": "of incoherent, it's sort\nof more coherent than what we got from an\nn-gram language model",
    "start": "1309320",
    "end": "1315890"
  },
  {
    "text": "when I showed a generation\nin the last lecture. You can choose a very\ndifferent style of text.",
    "start": "1315890",
    "end": "1324740"
  },
  {
    "text": "So you could, instead, train the\nmodel on a bunch of cook books. And if you do that,\nyou can then say,",
    "start": "1324740",
    "end": "1331820"
  },
  {
    "text": "generate based on what you've\nlearned about cook books. And it'll just\ngenerate a recipe.",
    "start": "1331820",
    "end": "1337920"
  },
  {
    "text": "So here's a recipe,\nchocolate ranch barbecue.",
    "start": "1337920",
    "end": "1342980"
  },
  {
    "text": "Categories, yield six servings. 2 tablespoons of\nParmesan cheese, chopped.",
    "start": "1342980",
    "end": "1349250"
  },
  {
    "text": "1 cup of coconut milk. Three eggs beaten. Place each pasta\nover layers of lumps.",
    "start": "1349250",
    "end": "1355730"
  },
  {
    "text": "Shape mixture into the moderate\noven and simmer untill firm. Serve hot in bodied fresh,\nmustard, orange, and cheese.",
    "start": "1355730",
    "end": "1364130"
  },
  {
    "text": "Combine the cheese\nand salt together, the dough in a large\nskillet, add the ingredients",
    "start": "1364130",
    "end": "1369860"
  },
  {
    "text": "and stir in the\nchocolate and pepper. So, you know, this\nrecipe makes no sense.",
    "start": "1369860",
    "end": "1376860"
  },
  {
    "text": "And it's sufficiently\nincoherent. There's actually even\nno danger that you'll try cooking this at home.",
    "start": "1376860",
    "end": "1382940"
  },
  {
    "text": "But something that's interesting\nis although this really just",
    "start": "1382940",
    "end": "1388519"
  },
  {
    "text": "isn't a recipe and\nthe things that are done in the\ninstructions have",
    "start": "1388520",
    "end": "1393980"
  },
  {
    "text": "no relation to the ingredients,\nthe thing that's interesting",
    "start": "1393980",
    "end": "1399260"
  },
  {
    "text": "that it has learned is this\nrecurrent neural network model is that it's really mastered the\noverall structure of a recipe.",
    "start": "1399260",
    "end": "1407300"
  },
  {
    "text": "It knows the recipe has a title. It often tells you about\nhow many people it serves.",
    "start": "1407300",
    "end": "1412980"
  },
  {
    "text": "It lists the ingredients. And then it has\ninstructions to make it. So that's sort of fairly\nimpressive in some sense",
    "start": "1412980",
    "end": "1420650"
  },
  {
    "text": "for high level text structuring. So the one other thing\nI wanted to mention",
    "start": "1420650",
    "end": "1427290"
  },
  {
    "text": "was when I say you can\ntrain an RNN language model on any kind of text, the\nother difference from where",
    "start": "1427290",
    "end": "1433919"
  },
  {
    "text": "we were in n-gram\nlanguage models was on n-gram\nlanguage models, that just meant counting\nn-grams and meant",
    "start": "1433920",
    "end": "1440340"
  },
  {
    "text": "it took 2 minutes,\neven on a large corpus, with any modern computer.",
    "start": "1440340",
    "end": "1445950"
  },
  {
    "text": "Training your RNN\nLM actually can then be a time intensive activity. And you can spend\nhours doing that,",
    "start": "1445950",
    "end": "1452520"
  },
  {
    "text": "as you might find next\nweek when you're training machine translation models. ",
    "start": "1452520",
    "end": "1459770"
  },
  {
    "start": "1458000",
    "end": "1670000"
  },
  {
    "text": "OK, how do we decide if\nour models are good or not? ",
    "start": "1459770",
    "end": "1466200"
  },
  {
    "text": "So the standard evaluation\nmetric for language models is what's called perplexity.",
    "start": "1466200",
    "end": "1473750"
  },
  {
    "text": "And what perplexity is is kind\nof like when you were training",
    "start": "1473750",
    "end": "1481520"
  },
  {
    "text": "your model, you use\nteacher forcing over a piece of text that's a\ndifferent piece of text which",
    "start": "1481520",
    "end": "1489529"
  },
  {
    "text": "isn't text that was\nin the training data. And you say, well,\ngiven a sequence",
    "start": "1489530",
    "end": "1495140"
  },
  {
    "text": "of t words, what\nprobability do you give to the actual t plus 1-th word?",
    "start": "1495140",
    "end": "1501620"
  },
  {
    "text": "And you repeat that\nat each position. And then you take the\ninverse of that probability",
    "start": "1501620",
    "end": "1508430"
  },
  {
    "text": "and raise it to the 1 on t for\nthe length of your text sample.",
    "start": "1508430",
    "end": "1514460"
  },
  {
    "text": "And that number\nis the perplexity. So it's a geometric mean of\nthe inverse probabilities.",
    "start": "1514460",
    "end": "1522330"
  },
  {
    "text": "Now after that explanation,\nperhaps an easier way to think of it is that\nthe perplexity is simply",
    "start": "1522330",
    "end": "1532340"
  },
  {
    "text": "the cross entropy\nloss that I introduced before exponentiated. ",
    "start": "1532340",
    "end": "1540020"
  },
  {
    "text": "But it's now the\nother way around. So low perplexity is better.",
    "start": "1540020",
    "end": "1546110"
  },
  {
    "text": "So there's actually\nan interesting story about these perplexities. So a famous figure in the\ndevelopment of probabilistic",
    "start": "1546110",
    "end": "1554570"
  },
  {
    "text": "and machine learning approaches\nto natural language processing is Fred Jelinek who\ndied a few years ago.",
    "start": "1554570",
    "end": "1561620"
  },
  {
    "text": "And he was trying to interest\npeople in the idea of using",
    "start": "1561620",
    "end": "1568960"
  },
  {
    "text": "probability models and machine\nlearning for natural language processing at a time-- i.e.,\nthis is the 1970s and early",
    "start": "1568960",
    "end": "1576820"
  },
  {
    "text": "1980s-- when nearly everyone\nin the field of AI",
    "start": "1576820",
    "end": "1582790"
  },
  {
    "text": "was still in the thrall\nof logic-based models and blackboard\narchitectures and things",
    "start": "1582790",
    "end": "1589090"
  },
  {
    "text": "like that for artificial\nintelligence systems. And so Fred Jelinek was\nactually an information theorist",
    "start": "1589090",
    "end": "1595630"
  },
  {
    "text": "by background, then got\ninterested in working",
    "start": "1595630",
    "end": "1601510"
  },
  {
    "text": "with speech and\nthen language data. So at that time,\nthe stuff that's--",
    "start": "1601510",
    "end": "1607690"
  },
  {
    "text": "this sort of exponential or\nusing cross-entropy losses was completely bread and\nbutter to Fred Jelinek",
    "start": "1607690",
    "end": "1615760"
  },
  {
    "text": "that he'd found\nthat no one in AI could understand the\nbottom half of the slide.",
    "start": "1615760",
    "end": "1621650"
  },
  {
    "text": "And so he wanted to come\nup with something simple that AI people at that\ntime could understand.",
    "start": "1621650",
    "end": "1627160"
  },
  {
    "text": "And perplexity has a kind\nof a simple interpretation you can tell people.",
    "start": "1627160",
    "end": "1633350"
  },
  {
    "text": "So if you get a\nperplexity of 53, that means how uncertain\nyou are of the next word",
    "start": "1633350",
    "end": "1642670"
  },
  {
    "text": "is equivalent to the\nuncertainty of that you're tossing a 53-sided dice\nand it coming up as a 1, right?",
    "start": "1642670",
    "end": "1652180"
  },
  {
    "text": "So that was kind of an\neasy, simple metric. And so he introduced that idea.",
    "start": "1652180",
    "end": "1660400"
  },
  {
    "text": "But you know, I\nguess things stick. And to this day, everyone\nevaluates their language models",
    "start": "1660400",
    "end": "1667420"
  },
  {
    "text": "by providing perplexity numbers. And so here are some\nperplexity numbers. So traditional n-gram\nlanguage models commonly",
    "start": "1667420",
    "end": "1675730"
  },
  {
    "start": "1670000",
    "end": "1772000"
  },
  {
    "text": "had perplexities over 100. But if you made them really\nbig and really carefully,",
    "start": "1675730",
    "end": "1682240"
  },
  {
    "text": "you could get them down\ninto a number like 67. As people started to\nbuild more advanced",
    "start": "1682240",
    "end": "1689140"
  },
  {
    "text": "recurrent neural networks,\nespecially as they moved beyond the kind\nof simple RNNs, which",
    "start": "1689140",
    "end": "1695500"
  },
  {
    "text": "is all I've shown you\nso far, which one of is in the second line of the slide,\ninto LSTMs, which I talk about",
    "start": "1695500",
    "end": "1703750"
  },
  {
    "text": "later in this\ncourse, that people started producing much\nbetter perplexities.",
    "start": "1703750",
    "end": "1710440"
  },
  {
    "text": "And here, we're getting\nperplexities down to 30. And this is results actually\nfrom a few years ago.",
    "start": "1710440",
    "end": "1717580"
  },
  {
    "text": "So nowadays people\nget perplexities of even lower than 30. You have to be realistic in\nwhat you can expect, right?",
    "start": "1717580",
    "end": "1725049"
  },
  {
    "text": "Because if you are\njust generating a text, some words are\nalmost determined.",
    "start": "1725050",
    "end": "1731020"
  },
  {
    "text": "So if it's something like\nSue gave the man a napkin.",
    "start": "1731020",
    "end": "1738260"
  },
  {
    "text": "He said thank. Basically 100%, you should\nbe able to say the word that comes next is you.",
    "start": "1738260",
    "end": "1745059"
  },
  {
    "text": "And so that you can\npredict really well. But if it's a lot\nof other sentences",
    "start": "1745060",
    "end": "1750920"
  },
  {
    "text": "like, he looked out the\nwindow and saw something,",
    "start": "1750920",
    "end": "1756170"
  },
  {
    "text": "right, no probability\nmodel in the world can give a very good\nestimate of what's actually going to be\ncoming next at that point.",
    "start": "1756170",
    "end": "1763409"
  },
  {
    "text": "And so that gives us the\nsort of residual uncertainty that leads to perplexities\nthat are, on average,",
    "start": "1763410",
    "end": "1769399"
  },
  {
    "text": "might be around 20 or something. ",
    "start": "1769400",
    "end": "1775299"
  },
  {
    "start": "1772000",
    "end": "1869000"
  },
  {
    "text": "So we've talked a lot\nabout language models now. Why should we care\nabout language modeling?",
    "start": "1775300",
    "end": "1782300"
  },
  {
    "text": "Well, there's sort of an\nintellectual scientific answer that says, this is a\nbenchmark task, right?",
    "start": "1782300",
    "end": "1789850"
  },
  {
    "text": "If want we want to do is\nbuild machine learning models of language and our\nability to predict",
    "start": "1789850",
    "end": "1795820"
  },
  {
    "text": "what word will come next in the\ncontext, that shows how well we understand both the\nstructure of language",
    "start": "1795820",
    "end": "1802900"
  },
  {
    "text": "and the structure\nof the human world that language talks about.",
    "start": "1802900",
    "end": "1808000"
  },
  {
    "text": "But there's a much\nmore practical answer than that, which is\nlanguage models are really",
    "start": "1808000",
    "end": "1815470"
  },
  {
    "text": "the secret tool of natural\nlanguage processing. So if you're talking\nto any NLP person",
    "start": "1815470",
    "end": "1823390"
  },
  {
    "text": "and you've got\nalmost any task, it's quite likely they'll say, oh,\nI bet we could use a language",
    "start": "1823390",
    "end": "1831790"
  },
  {
    "text": "model for that. And so language\nmodels are sort of used as not the whole solution,\nbut a part of almost any task.",
    "start": "1831790",
    "end": "1841480"
  },
  {
    "text": "Any task that involves\ngenerating or estimating the probability of text.",
    "start": "1841480",
    "end": "1846710"
  },
  {
    "text": "So you can use it for predictive\ntyping, speech recognition, grammar correction, identifying\nauthors, machine translation,",
    "start": "1846710",
    "end": "1854860"
  },
  {
    "text": "summarization, dialogue,\njust about anything you do with natural language\ninvolves language models.",
    "start": "1854860",
    "end": "1860230"
  },
  {
    "text": "And we'll see examples of\nthat in following classes, including next Tuesday where\nwe're using language models",
    "start": "1860230",
    "end": "1867250"
  },
  {
    "text": "for machine translation. OK, so a language\nmodel is just a system",
    "start": "1867250",
    "end": "1873670"
  },
  {
    "start": "1869000",
    "end": "1913000"
  },
  {
    "text": "that predicts the next word. A recurrent neural network is\na family of neural networks",
    "start": "1873670",
    "end": "1881240"
  },
  {
    "text": "which can take sequential\ninput of any length, they reuse the same\nweights to generate",
    "start": "1881240",
    "end": "1888370"
  },
  {
    "text": "a hidden state and optionally,\nbut commonly, an output on each step.",
    "start": "1888370",
    "end": "1893800"
  },
  {
    "text": "Note that these two\nthings are different. So we've talked about\ntwo ways that you",
    "start": "1893800",
    "end": "1900440"
  },
  {
    "text": "could build language models. But one of them is\nRNNs being a great way.",
    "start": "1900440",
    "end": "1905450"
  },
  {
    "text": "But RNNs can also be used\nfor a lot of other things. So let me just quickly\npreview a few other things",
    "start": "1905450",
    "end": "1911090"
  },
  {
    "text": "you can do with RNNs. So there are lots\nof tasks that people want to do in NLP which\nare referred to as sequence",
    "start": "1911090",
    "end": "1918740"
  },
  {
    "start": "1913000",
    "end": "2063000"
  },
  {
    "text": "tagging tasks where we'd\nlike to take words of text and do some kind\nof classification",
    "start": "1918740",
    "end": "1925940"
  },
  {
    "text": "along the sequence. So one simple common one is\nto give words parts of speech.",
    "start": "1925940",
    "end": "1932059"
  },
  {
    "text": "The is a determiner.\nstartled is an adjective. Cat is a noun. Knocked is a verb.",
    "start": "1932060",
    "end": "1937640"
  },
  {
    "text": "And well, you can do\nthis straightforwardly by using a recurrent\nneural network",
    "start": "1937640",
    "end": "1943340"
  },
  {
    "text": "as a sequential\nclassifier where it's now going to generate\nparts of speech",
    "start": "1943340",
    "end": "1949040"
  },
  {
    "text": "rather than the next word. You can use a recurrent\nneural network, the sentiment",
    "start": "1949040",
    "end": "1954640"
  },
  {
    "text": "classification. Well, this time\nwe don't actually want to generate an output\nat each word, necessarily.",
    "start": "1954640",
    "end": "1963040"
  },
  {
    "text": "But we want to know what the\noverall sentiment looks like. So somehow we want to get\nout a sentence encoding",
    "start": "1963040",
    "end": "1969400"
  },
  {
    "text": "that we can perhaps put through\nanother neural network layer to judge whether the sentence\nis positive or negative.",
    "start": "1969400",
    "end": "1977809"
  },
  {
    "text": "Well, the simplest\nway to do that is to think, well,\nafter I've run",
    "start": "1977810",
    "end": "1984190"
  },
  {
    "text": "my LSTM through the\nwhole sentence, actually",
    "start": "1984190",
    "end": "1989230"
  },
  {
    "text": "this final hidden state, it's\nencoded the whole sentence. Because remember I\nupdated that hidden state",
    "start": "1989230",
    "end": "1995019"
  },
  {
    "text": "based on each previous word. And so you could say that\nthis is the whole meaning of the sentence.",
    "start": "1995020",
    "end": "2000670"
  },
  {
    "text": "So let's just say that\nis the sentence encoding and then put an extra classifier\nlayer on that with something",
    "start": "2000670",
    "end": "2009059"
  },
  {
    "text": "like a softmax classifier. That method has been used. And it actually works\nreasonably well.",
    "start": "2009060",
    "end": "2015320"
  },
  {
    "text": "And if you sort of train\nthis model end to end, well, it's actually then\nmotivated to preserve sentiment",
    "start": "2015320",
    "end": "2022230"
  },
  {
    "text": "information in the hidden state\nof the recurrent neural network because that will allow\nit to better predict",
    "start": "2022230",
    "end": "2028080"
  },
  {
    "text": "the sentiment of the\nwhole sentence, which is the final task and\nhence loss function",
    "start": "2028080",
    "end": "2033990"
  },
  {
    "text": "that we're giving the network. But it turns out that you can\ncommonly do better than that",
    "start": "2033990",
    "end": "2039000"
  },
  {
    "text": "by actually doing things\nlike feeding all hidden states into the\nsentence encoding,",
    "start": "2039000",
    "end": "2045450"
  },
  {
    "text": "perhaps by making\nthe sentence encoding an element-wise max or an\nelement-wise mean of all",
    "start": "2045450",
    "end": "2053190"
  },
  {
    "text": "the hidden states. Because this then\nmore symmetrically encodes the hidden state\nover each time step.",
    "start": "2053190",
    "end": "2060960"
  },
  {
    "text": " Another big use of\nrecurrent neural networks",
    "start": "2060960",
    "end": "2067719"
  },
  {
    "start": "2063000",
    "end": "2128000"
  },
  {
    "text": "is what I'll call language\nencoder module uses.",
    "start": "2067719",
    "end": "2073540"
  },
  {
    "text": "So any time you have some\ntext, for example here we have a question, what\nnationality was Beethoven,",
    "start": "2073540",
    "end": "2081638"
  },
  {
    "text": "we'd like to construct some\nkind of neural representation of this.",
    "start": "2081639",
    "end": "2086719"
  },
  {
    "text": "So one way to do it is to run\na recurrent neural network",
    "start": "2086719",
    "end": "2091929"
  },
  {
    "text": "over it. And then just like last\ntime, to either take the final hidden state or\ntake some kind of function",
    "start": "2091929",
    "end": "2100150"
  },
  {
    "text": "of all the hidden states\nand say that's the sentence representation. And we could do the same\nthing for the context.",
    "start": "2100150",
    "end": "2108670"
  },
  {
    "text": "So for question\nanswering, we're going to build some more neural\nnet structure on top of that.",
    "start": "2108670",
    "end": "2114190"
  },
  {
    "text": "And we'll learn more about\nthat in a couple of weeks when we have the question\nanswering lecture.",
    "start": "2114190",
    "end": "2119710"
  },
  {
    "text": "But the key thing is\nwhat we've built so far, we used to get sentence\nrepresentation.",
    "start": "2119710",
    "end": "2126170"
  },
  {
    "text": "So it's a language\nencoder module. So that was the\nlanguage encoding part.",
    "start": "2126170",
    "end": "2131590"
  },
  {
    "start": "2128000",
    "end": "2218000"
  },
  {
    "text": "We can also use RNNs to\ndecode into language. And that's commonly used in\nspeech recognition, machine",
    "start": "2131590",
    "end": "2139870"
  },
  {
    "text": "translation, summarization. So if we have a\nspeech recognizer, the input is an audio signal.",
    "start": "2139870",
    "end": "2146470"
  },
  {
    "text": "And what we want to do is\ndecode that into language. Well, what we could do\nis use some function",
    "start": "2146470",
    "end": "2153490"
  },
  {
    "text": "of the input, which\nis probably itself going to be a neural net,\nas the initial hidden",
    "start": "2153490",
    "end": "2160870"
  },
  {
    "text": "state of our RNN-LM. And then we say, start\ngenerating text based on that.",
    "start": "2160870",
    "end": "2169480"
  },
  {
    "text": "And so it should\nthen, we generate word at a time by the method\nthat we just looked at.",
    "start": "2169480",
    "end": "2175480"
  },
  {
    "text": "We turn the speech into text. So this is an example of a\nconditional language model",
    "start": "2175480",
    "end": "2181450"
  },
  {
    "text": "because we're now generating\ntext conditioned on the speech signal. And a lot of the time, you can\ndo interesting more advanced",
    "start": "2181450",
    "end": "2190630"
  },
  {
    "text": "things with recurrent\nneural networks by building conditional\nlanguage models.",
    "start": "2190630",
    "end": "2196280"
  },
  {
    "text": "Another place you can use\nconditional language models is for text\nclassification tasks,",
    "start": "2196280",
    "end": "2202270"
  },
  {
    "text": "including sentiment\nclassification. So if you can condition\nyour language model",
    "start": "2202270",
    "end": "2209560"
  },
  {
    "text": "based on a kind\nof sentiment, you can build a kind of\nclassifier for that. And another use that we'll\nsee a lot of next class",
    "start": "2209560",
    "end": "2216190"
  },
  {
    "text": "is for machine translation.  OK, so that's the\nend of the intro",
    "start": "2216190",
    "end": "2223020"
  },
  {
    "start": "2218000",
    "end": "2571000"
  },
  {
    "text": "to doing things with recurrent\nneural networks and language",
    "start": "2223020",
    "end": "2229580"
  },
  {
    "text": "models. Now, I want to move on and\ntell you about the fact that everything\nis not perfect and",
    "start": "2229580",
    "end": "2237590"
  },
  {
    "text": "these recurrent\nneural networks tend to have a couple of problems. And we'll talk about those.",
    "start": "2237590",
    "end": "2244890"
  },
  {
    "text": "And then, in part,\nthat will then motivate coming up with a\nmore advanced recurrent neural network architecture.",
    "start": "2244890",
    "end": "2251730"
  },
  {
    "text": "So the first problem\nto be mentioned is the idea of what's\ncalled vanishing gradients.",
    "start": "2251730",
    "end": "2259009"
  },
  {
    "text": "And what does that mean? Well, at the end\nof our sequence, we have some overall loss\nthat we're calculating.",
    "start": "2259010",
    "end": "2268580"
  },
  {
    "text": "And well, what we want to do\nis back propagate that loss.",
    "start": "2268580",
    "end": "2273830"
  },
  {
    "text": "And we want to back propagate\nit right along the sequence. And so we're working out the\npartials of J4 with respect",
    "start": "2273830",
    "end": "2283040"
  },
  {
    "text": "to the hidden state at time 1. And when we have\na longer sequence, we'll be working out the\npartials of J20 with respect",
    "start": "2283040",
    "end": "2290150"
  },
  {
    "text": "to the hidden state at time 1. And how do we do that? Well, how we do it is by\ncomposition and the chain rule.",
    "start": "2290150",
    "end": "2299960"
  },
  {
    "text": "We've got a big long chain\nrule along the whole sequence.",
    "start": "2299960",
    "end": "2305780"
  },
  {
    "text": "Well, if we're doing that, we're\nmultiplying a ton of things",
    "start": "2305780",
    "end": "2312110"
  },
  {
    "text": "together. And so the danger of\nwhat tends to happen",
    "start": "2312110",
    "end": "2317540"
  },
  {
    "text": "is that as we do these\nmultiplications, a lot of time",
    "start": "2317540",
    "end": "2323030"
  },
  {
    "text": "these partials between\nsuccessive hidden states become small.",
    "start": "2323030",
    "end": "2329250"
  },
  {
    "text": "And so what happens\nis as we go along, the gradient gets smaller\nand smaller and smaller,",
    "start": "2329250",
    "end": "2336170"
  },
  {
    "text": "and starts to peter out. And to the extent\nthat it peters out,",
    "start": "2336170",
    "end": "2341960"
  },
  {
    "text": "well, then we've kind of\ngotten no upstream gradient and therefore we won't be\nchanging the parameters at all.",
    "start": "2341960",
    "end": "2349100"
  },
  {
    "text": "And that turns out to\nbe pretty problematic. ",
    "start": "2349100",
    "end": "2354980"
  },
  {
    "text": "So the next couple of slides\nsort of say a little bit about the why and\nhow this happens.",
    "start": "2354980",
    "end": "2364450"
  },
  {
    "text": "What's presented here is a\nkind of only semi-formal wave",
    "start": "2364450",
    "end": "2370700"
  },
  {
    "text": "your hands at the kind of\nproblems that you might expect. If you really want\nto sort of get",
    "start": "2370700",
    "end": "2375910"
  },
  {
    "text": "into all the\ndetails of this, you should look at the\ncouple of papers that are mentioned\nin small print",
    "start": "2375910",
    "end": "2382180"
  },
  {
    "text": "at the bottom of the slide. But at any rate, if\nyou remember that this is our basic recurrent\nneural network equation,",
    "start": "2382180",
    "end": "2390520"
  },
  {
    "text": "let's consider an easy case. Suppose we sort of get\nrid of our non-linearity",
    "start": "2390520",
    "end": "2397330"
  },
  {
    "text": "and just assume that it's\nan identity function. OK, so then when\nwe're working out",
    "start": "2397330",
    "end": "2402849"
  },
  {
    "text": "the partials of the\nhidden state with respect to the previous hidden\nstate, we can work those out",
    "start": "2402850",
    "end": "2409270"
  },
  {
    "text": "in the usual way according\nto the chain rule. And then, if sigma is simply\nthe identity function,",
    "start": "2409270",
    "end": "2421030"
  },
  {
    "text": "well then, everything\ngets really easy for us. So only-- the sigma\njust goes away.",
    "start": "2421030",
    "end": "2428050"
  },
  {
    "text": "And only the first term\ninvolves h at time t minus 1.",
    "start": "2428050",
    "end": "2433820"
  },
  {
    "text": "So the later terms go away. And so our gradient\nends up as Wh.",
    "start": "2433820",
    "end": "2441080"
  },
  {
    "text": "Well, that's doing it\nfor just one time step. What happens when\nyou want to work out",
    "start": "2441080",
    "end": "2446230"
  },
  {
    "text": "these partials a number\nof time steps away? So we want to work it out,\nthe partial of time step i",
    "start": "2446230",
    "end": "2455800"
  },
  {
    "text": "with respect to j. Well, what we end\nup with is a product",
    "start": "2455800",
    "end": "2462580"
  },
  {
    "text": "of the partials of\nsuccessive time steps. And well, each of those\nis coming out as Wh",
    "start": "2462580",
    "end": "2473349"
  },
  {
    "text": "and so we end up getting Wh\nraised to the l-th power.",
    "start": "2473350",
    "end": "2481331"
  },
  {
    "text": "And while our\npotential problem is that if Wh is small\nin some sense,",
    "start": "2481331",
    "end": "2487990"
  },
  {
    "text": "then this term gets\nexponentially problematic. It becomes vanishingly\nsmall as our sequence length",
    "start": "2487990",
    "end": "2494830"
  },
  {
    "text": "becomes long. Well, what can we mean by small? Well, a matrix is small\nif its eigenvalues",
    "start": "2494830",
    "end": "2502915"
  },
  {
    "text": "are all less than 1. So we can rewrite\nwhat's happening with this success\nof multiplication,",
    "start": "2502915",
    "end": "2509950"
  },
  {
    "text": "using eigenvalues\nand eigenvectors. And I should say that all\neigenvalues less than 1",
    "start": "2509950",
    "end": "2517299"
  },
  {
    "text": "is sufficient, but not\nnecessary conditioned for what I'm about to say, right? So we can rewrite things using\nthe eigenvectors as a basis.",
    "start": "2517300",
    "end": "2528240"
  },
  {
    "text": "And if we do that, we end\nup getting the eigenvalues",
    "start": "2528240",
    "end": "2535630"
  },
  {
    "text": "being raised to the lth power. And so if all of our\neigenvalues are less than 1,",
    "start": "2535630",
    "end": "2542359"
  },
  {
    "text": "if we're taking a\nnumber less than 1, then raising it\nto the lth power, that's going to approach 0\nas the sequence length grows.",
    "start": "2542360",
    "end": "2551080"
  },
  {
    "text": "And so the gradient vanishes. OK, now the reality is\nmore complex than that.",
    "start": "2551080",
    "end": "2556690"
  },
  {
    "text": "Because actually, we always use\na nonlinear activation sigma. But in principle, it's sort\nof the same thing apart from,",
    "start": "2556690",
    "end": "2565270"
  },
  {
    "text": "we have to consider in the\neffect of the nonlinear activation.",
    "start": "2565270",
    "end": "2571440"
  },
  {
    "start": "2571000",
    "end": "2646000"
  },
  {
    "text": "OK, so why is this a problem\nthat the gradients disappear? Well, suppose we're wanting to\nlook at the influence of time",
    "start": "2571440",
    "end": "2580131"
  },
  {
    "text": "steps well in the future, on\nthe representations we want",
    "start": "2580132",
    "end": "2585960"
  },
  {
    "text": "to have early in the sentence. Well, what's happening\nlate in the sentence",
    "start": "2585960",
    "end": "2592560"
  },
  {
    "text": "just isn't going to be giving\nmuch information about, what we should be storing in\nthe h at time 1 vector.",
    "start": "2592560",
    "end": "2601799"
  },
  {
    "text": "Whereas on the other hand,\nthe loss at timestep 2 is going to be giving\na lot of information",
    "start": "2601800",
    "end": "2608340"
  },
  {
    "text": "at what should be stored in the\nhidden vector at timestep 1. So the end result of\nthat is that what happens",
    "start": "2608340",
    "end": "2618060"
  },
  {
    "text": "is that, these\nsimple RNN's are very good at modeling nearby effects.",
    "start": "2618060",
    "end": "2625359"
  },
  {
    "text": "But they're not good at all\nat modeling long-term effects. Because the gradient signal from\nfar away is just lost too much.",
    "start": "2625360",
    "end": "2634500"
  },
  {
    "text": "And therefore, the model\nnever effectively gets to learn what information\nfrom far away,",
    "start": "2634500",
    "end": "2642750"
  },
  {
    "text": "it would be useful to\npreserve into the future. So let's consider\nthat concretely,",
    "start": "2642750",
    "end": "2648599"
  },
  {
    "start": "2646000",
    "end": "2762000"
  },
  {
    "text": "for the example of language\nmodels that we've worked on. So here's a piece of text.",
    "start": "2648600",
    "end": "2655800"
  },
  {
    "text": "When she tried to\nprint her tickets, she found that the\nprinter was out of toner. She went to the stationery\nstore to buy more toner.",
    "start": "2655800",
    "end": "2663240"
  },
  {
    "text": "It was very overpriced. After installing the\ntoner into the printer, she finally printed her--",
    "start": "2663240",
    "end": "2669450"
  },
  {
    "text": "and well, you're all\nsmart human beings. I trust you can all guess what\nthe word that comes next is.",
    "start": "2669450",
    "end": "2675750"
  },
  {
    "text": "It should be tickets. But well, the problem\nis that for the RNN",
    "start": "2675750",
    "end": "2681630"
  },
  {
    "text": "to start to learn\ncases like this, it would have to carry through\nin its hidden state, a memory",
    "start": "2681630",
    "end": "2689190"
  },
  {
    "text": "of the word tickets for\nsort of, whatever it is, about 30 hidden state updates.",
    "start": "2689190",
    "end": "2695700"
  },
  {
    "text": "And well, we'll train\non this example. And so we'll be wanting\nit to predict tickets",
    "start": "2695700",
    "end": "2703500"
  },
  {
    "text": "as the next word. And so a gradient update\nwill be sent right back through the hidden states\nof the LSTM corresponding",
    "start": "2703500",
    "end": "2712200"
  },
  {
    "text": "to this sentence. And that should\ntell the model, it's",
    "start": "2712200",
    "end": "2718050"
  },
  {
    "text": "good to preserve information\nabout the word tickets, because that might be\nuseful in the future. Here it was useful\nin the future.",
    "start": "2718050",
    "end": "2725369"
  },
  {
    "text": "But the problem is that, the\ngradient signal will just become far too weak out\nafter a bunch of words.",
    "start": "2725370",
    "end": "2733230"
  },
  {
    "text": "And it just never\nlearns that dependency. And so what we find\nin practice is,",
    "start": "2733230",
    "end": "2740040"
  },
  {
    "text": "the model is just\nunable to predict similar long-distance\ndependencies at test time.",
    "start": "2740040",
    "end": "2745265"
  },
  {
    "text": "And I've spent quite a long\ntime on vanishing gradients. And then really,\nvanishing gradients",
    "start": "2745265",
    "end": "2751410"
  },
  {
    "text": "are the big problem\nin practice with using",
    "start": "2751410",
    "end": "2757890"
  },
  {
    "text": "recurrent neural networks\nor the long sequences. But I have to do\njustice to the fact",
    "start": "2757890",
    "end": "2764310"
  },
  {
    "start": "2762000",
    "end": "2906000"
  },
  {
    "text": "that you could actually, also\nhave the opposite problem. You can also have\nexploding gradients.",
    "start": "2764310",
    "end": "2769690"
  },
  {
    "text": "So if a gradient becomes too\nbig, that's also a problem.",
    "start": "2769690",
    "end": "2775940"
  },
  {
    "text": "And it's a problem because\nthe stochastic gradient update step becomes too big, right?",
    "start": "2775940",
    "end": "2783150"
  },
  {
    "text": "So remember, our\nparameter update is based on the product of the\nlearning rate and the gradient.",
    "start": "2783150",
    "end": "2790720"
  },
  {
    "text": "So if your gradient is huge,\nright, you've calculated, oh, it's got a\nlot of slope here,",
    "start": "2790720",
    "end": "2796079"
  },
  {
    "text": "this has a slope of 10,000,\nthen your parameter update",
    "start": "2796080",
    "end": "2802080"
  },
  {
    "text": "can be arbitrarily large. And that's potentially\nproblematic.",
    "start": "2802080",
    "end": "2807600"
  },
  {
    "text": "That can cause a bad update,\nwhere you take a huge step and you end up at a weird and\nbad parameter configuration.",
    "start": "2807600",
    "end": "2816300"
  },
  {
    "text": "So you think you're\ncoming up with a-- to a steep hill to climb. And while you want to\nbe climbing the hill",
    "start": "2816300",
    "end": "2823380"
  },
  {
    "text": "to high likelihood,\nthat actually the gradient is so steep that\nyou make an enormous update.",
    "start": "2823380",
    "end": "2831450"
  },
  {
    "text": "And then suddenly, your\nparameters are over in Iowa. And you've lost your\nhill altogether. There's also the\npractical difficulty",
    "start": "2831450",
    "end": "2837920"
  },
  {
    "text": "that we only have\nso much resolution in our floating point numbers. So if your gradient\ngets too steep,",
    "start": "2837920",
    "end": "2845070"
  },
  {
    "text": "you start getting not a numbers\nin your calculations, which ruin all your hard\ntraining work.",
    "start": "2845070",
    "end": "2852700"
  },
  {
    "text": "We use kind of an\neasy fix to this, which is called gradient\nclipping, which is, we",
    "start": "2852700",
    "end": "2859230"
  },
  {
    "text": "choose some reasonable number. And we say, we're just not going\nto deal with gradients that",
    "start": "2859230",
    "end": "2865410"
  },
  {
    "text": "are bigger than this number. A commonly used number is 20. Some thing that's got a range\nof spread, but not that high.",
    "start": "2865410",
    "end": "2873089"
  },
  {
    "text": "You can use 10, 100, something. We're sort of in that range.",
    "start": "2873090",
    "end": "2878500"
  },
  {
    "text": "And if the norm of the gradient\nis greater than that threshold, we simply just scale it down,\nwhich means that we then",
    "start": "2878500",
    "end": "2887280"
  },
  {
    "text": "make a smaller gradient update. So we're still moving in\nexactly the same direction,",
    "start": "2887280",
    "end": "2894720"
  },
  {
    "text": "but we're taking a smaller step. So doing this gradient\nclipping is important.",
    "start": "2894720",
    "end": "2902430"
  },
  {
    "text": "But it's an easy\nproblem to solve. ",
    "start": "2902430",
    "end": "2908349"
  },
  {
    "text": "OK, so the thing that we've\nstill got left to solve is, how to really solve this\nproblem of vanishing gradients.",
    "start": "2908350",
    "end": "2919299"
  },
  {
    "text": "So the problem is,\nyeah, these RNNs just can't preserve information\nover many timesteps.",
    "start": "2919300",
    "end": "2927250"
  },
  {
    "text": "And one way to think\nabout that intuitively is, at each timestep,\nwe have a hidden state.",
    "start": "2927250",
    "end": "2936410"
  },
  {
    "text": "And the hidden state is\nbeing completely changed at each timestep.",
    "start": "2936410",
    "end": "2942800"
  },
  {
    "text": "And it's being changed in\na multiplicative manner by multiplying by Wh,\nand then putting it",
    "start": "2942800",
    "end": "2949250"
  },
  {
    "text": "through a nonlinearity. Maybe we could make\nsome more progress,",
    "start": "2949250",
    "end": "2957890"
  },
  {
    "text": "if we could more\nflexibly maintain a memory in our recurrent\nneural network, which",
    "start": "2957890",
    "end": "2966290"
  },
  {
    "text": "we can manipulate in a\nmore flexible manner, that allows us to more easily\npreserve information.",
    "start": "2966290",
    "end": "2974940"
  },
  {
    "text": "And so this was an idea that\npeople started thinking about. And actually, they started\nthinking about it a long time",
    "start": "2974940",
    "end": "2982730"
  },
  {
    "start": "2982000",
    "end": "3599000"
  },
  {
    "text": "ago in the late 1990s.",
    "start": "2982730",
    "end": "2988560"
  },
  {
    "text": "And Hochreiter and\nSchmidhuber came up with this idea that got called\nlong short-term memory RNNs.",
    "start": "2988560",
    "end": "2997440"
  },
  {
    "text": "So a solution to the problem\nof vanishing gradients. I mean, so this 1997\npaper is the paper",
    "start": "2997440",
    "end": "3004730"
  },
  {
    "text": "you always see cited for LSTMs. But actually, in terms of what\nwe now understand as an LSTM,",
    "start": "3004730",
    "end": "3013309"
  },
  {
    "text": "it was missing part of it. In fact, it's missing what,\nin retrospect, has turned out",
    "start": "3013310",
    "end": "3018590"
  },
  {
    "text": "to be the most important\npart of the modern LSTM.",
    "start": "3018590",
    "end": "3024050"
  },
  {
    "text": "So really, in some\nsense, the real paper that the modern LSTM is due to\nis this slightly later paper",
    "start": "3024050",
    "end": "3032000"
  },
  {
    "text": "by Gers, still Schmidhuber,\nand Cummins from 2000, which additionally\nintroduces the forget gate",
    "start": "3032000",
    "end": "3038720"
  },
  {
    "text": "that I'll explain in a minute. Yeah, so this was\nsome very clever stuff",
    "start": "3038720",
    "end": "3046320"
  },
  {
    "text": "that was introduced. And it turned out later to\nhave an enormous impact.",
    "start": "3046320",
    "end": "3052890"
  },
  {
    "text": "If I just diverge from\nthe technical part for one more moment.",
    "start": "3052890",
    "end": "3058440"
  },
  {
    "text": "That for those of\nyou who, these days, think that mastering neural\nnetworks is the path to fame",
    "start": "3058440",
    "end": "3066990"
  },
  {
    "text": "and fortune, the funny\nthing is, at the time that this work was done, that\njust was not true, right?",
    "start": "3066990",
    "end": "3075750"
  },
  {
    "text": "Very few people were\ninterested in neural networks. And although long short-term\nmemories have turned out",
    "start": "3075750",
    "end": "3083520"
  },
  {
    "text": "to be one of the most important,\nsuccessful, and influential ideas in neural networks\nfor the following 25 years,",
    "start": "3083520",
    "end": "3092940"
  },
  {
    "text": "really, the original authors\ndidn't get recognition for that. So both of them\nare now professors",
    "start": "3092940",
    "end": "3099119"
  },
  {
    "text": "at German universities. But Hochreiter moved over\ninto doing bioinformatics work",
    "start": "3099120",
    "end": "3107370"
  },
  {
    "text": "to find something to do. And Gers, actually, is doing\nkind of multimedia studies.",
    "start": "3107370",
    "end": "3115880"
  },
  {
    "text": "So that's the fates of history. ",
    "start": "3115880",
    "end": "3121650"
  },
  {
    "text": "OK, so what is an LSTM? So a crucial\ninnovation of an LSTM",
    "start": "3121650",
    "end": "3128490"
  },
  {
    "text": "is to say, well,\nrather than just having one hidden vector in\nthe recurrent model,",
    "start": "3128490",
    "end": "3135660"
  },
  {
    "text": "we're going to build a model\nwith two hidden vectors",
    "start": "3135660",
    "end": "3143010"
  },
  {
    "text": "at each timestep,\none of which is still called the hidden state\nh, and the other of which",
    "start": "3143010",
    "end": "3149579"
  },
  {
    "text": "is called the cell state. Now arguably, in retrospect,\nthese were named wrongly.",
    "start": "3149580",
    "end": "3157172"
  },
  {
    "text": "Because as you'll see,\nwhen we look at it in more detail, in\nsome sense, the cell is more equivalent to the\nhidden state of the simple RNN",
    "start": "3157172",
    "end": "3165150"
  },
  {
    "text": "than vice versa. But we're just going with the\nnames that everybody uses.",
    "start": "3165150",
    "end": "3170350"
  },
  {
    "text": "So both of these are\nvectors of length n. And it's going to\nbe the cell that stores long-term information.",
    "start": "3170350",
    "end": "3178830"
  },
  {
    "text": "And so we want to\nhave something that's more like memory, meaning\nlike RAM in the computer.",
    "start": "3178830",
    "end": "3186490"
  },
  {
    "text": "So the cell is designed\nso you can read from it, you can erase parts of\nit, and you can write",
    "start": "3186490",
    "end": "3193050"
  },
  {
    "text": "new information to the cell. And the interesting\npart of an LSTM is then,",
    "start": "3193050",
    "end": "3199740"
  },
  {
    "text": "it's got control structures\nto decide how you do that. So the selection of\nwhich information",
    "start": "3199740",
    "end": "3206580"
  },
  {
    "text": "to erase, write, and\nread is controlled by probabilistic gates.",
    "start": "3206580",
    "end": "3211770"
  },
  {
    "text": "So the gates are also\nvectors of length n. And on each\ntimestep, we work out",
    "start": "3211770",
    "end": "3219119"
  },
  {
    "text": "a state for the gate vectors. So each element of the gate\nvectors is a probability.",
    "start": "3219120",
    "end": "3224490"
  },
  {
    "text": "So they can be open probability,\n1, closed probability, 0, or somewhere in between.",
    "start": "3224490",
    "end": "3230490"
  },
  {
    "text": "And their value will be\nsaying, how much do you erase? How much do you write?",
    "start": "3230490",
    "end": "3236490"
  },
  {
    "text": "How much do you read? And so these are dynamic\ngates, with a value",
    "start": "3236490",
    "end": "3241530"
  },
  {
    "text": "that's computed based\non the current context. ",
    "start": "3241530",
    "end": "3246869"
  },
  {
    "text": "OK, so in this next slide,\nwe go through the equations of an LSTM. But following this, there\nare some more graphic slides,",
    "start": "3246870",
    "end": "3254320"
  },
  {
    "text": "which will probably be\neasier to absorb, right? So we, again-- just\nlike before, it's",
    "start": "3254320",
    "end": "3259770"
  },
  {
    "text": "a recurrent neural network. We have a sequence\nof inputs, xt.",
    "start": "3259770",
    "end": "3265350"
  },
  {
    "text": "And we're going to,\nat each timestep, compute a cell state\nand a hidden state.",
    "start": "3265350",
    "end": "3270670"
  },
  {
    "text": "So how do we do that? So firstly, we're\ngoing to compute values of the three gates.",
    "start": "3270670",
    "end": "3277950"
  },
  {
    "text": "And so we're computing\nthe gate values, using an equation that's\nidentical to the equation",
    "start": "3277950",
    "end": "3286049"
  },
  {
    "text": "for the simple recurrent\nneural network. But in particular--\nwhoops, sorry, I'll",
    "start": "3286050",
    "end": "3293420"
  },
  {
    "text": "just say what the\ngates are first. So there's a forget\ngate, which we will control what is kept in\nthe cell at the next timestep,",
    "start": "3293420",
    "end": "3303180"
  },
  {
    "text": "versus what is forgotten. There's an input\ngate, which is going to determine which parts of\na calculated new cell content",
    "start": "3303180",
    "end": "3311940"
  },
  {
    "text": "get written to the cell memory. And there's an\noutput gate, which is going to control what\nparts of the cell memory",
    "start": "3311940",
    "end": "3319020"
  },
  {
    "text": "are moved over into\nthe hidden state. And so each of these is\nusing the logistic function.",
    "start": "3319020",
    "end": "3326220"
  },
  {
    "text": "Because we want them\nto be, in each element of this vector, a\nprobability which",
    "start": "3326220",
    "end": "3332220"
  },
  {
    "text": "will say whether\nto fully forget, partially forget,\nor fully remember.",
    "start": "3332220",
    "end": "3339230"
  },
  {
    "text": "Yeah, and the equation\nfor each of these is exactly like the\nsimple RNN equation.",
    "start": "3339230",
    "end": "3344790"
  },
  {
    "text": "But note, of course, that\nwe've got different parameters for each one. So we've got forgetting\nweight matrix",
    "start": "3344790",
    "end": "3351349"
  },
  {
    "text": "W with a forgetting bias,\nand a forgetting multiplier",
    "start": "3351350",
    "end": "3357933"
  },
  {
    "text": "of the input.  So then we have the other\nequations that are really",
    "start": "3357933",
    "end": "3365369"
  },
  {
    "text": "the mechanics of the LSTM. So we have something that will\ncalculate a new cell content.",
    "start": "3365370",
    "end": "3374620"
  },
  {
    "text": "So this is our candidate update. And so for calculating\nthe candidate update,",
    "start": "3374620",
    "end": "3380789"
  },
  {
    "text": "we're, again, essentially using\nexactly the same simple RNN equation.",
    "start": "3380790",
    "end": "3386100"
  },
  {
    "text": "Apart from now, it's\nusual to use tahh. So you get something that,\nas discussed last time,",
    "start": "3386100",
    "end": "3392220"
  },
  {
    "text": "is balanced around 0. Okay, so then to actually\nupdate things, we use our gates.",
    "start": "3392220",
    "end": "3400020"
  },
  {
    "text": "So for our new cell\ncontent, what the idea is, is that we want to\nremember some, but probably",
    "start": "3400020",
    "end": "3407819"
  },
  {
    "text": "not all of what we had in the\ncell from previous timesteps. And we want to store some, but\nprobably not all of the value",
    "start": "3407820",
    "end": "3419130"
  },
  {
    "text": "that we've calculated\nas the new cell update.",
    "start": "3419130",
    "end": "3424769"
  },
  {
    "text": "And so the way we do that is, we\ntake the previous cell content.",
    "start": "3424770",
    "end": "3431220"
  },
  {
    "text": "And then, we take its Hadamard\nproduct with the forget vector.",
    "start": "3431220",
    "end": "3437490"
  },
  {
    "text": "And then, we add to it\nthe Hadamard product of the input gate times\nthe candidate cell update.",
    "start": "3437490",
    "end": "3445905"
  },
  {
    "text": " And then for working out\nthe new hidden state,",
    "start": "3445905",
    "end": "3453619"
  },
  {
    "text": "we then work out which\nparts of the cell to expose in the hidden state.",
    "start": "3453620",
    "end": "3461059"
  },
  {
    "text": "And so after taking a tanh\ntransform of the cell,",
    "start": "3461060",
    "end": "3466160"
  },
  {
    "text": "we then take the Hadamard\nproduct with the output gate. And that gives us our\nhidden representation.",
    "start": "3466160",
    "end": "3472280"
  },
  {
    "text": "And it's this hidden\nrepresentation that we then put\nthrough a softmax layer,",
    "start": "3472280",
    "end": "3477890"
  },
  {
    "text": "to generate our next output\nof our LSTM recurrent neural network.",
    "start": "3477890",
    "end": "3483170"
  },
  {
    "text": "Yeah, so the gates and the\nthings that they're put with",
    "start": "3483170",
    "end": "3491030"
  },
  {
    "text": "are vectors of size n. And what we're doing is, we're\ntaking each element of them",
    "start": "3491030",
    "end": "3496400"
  },
  {
    "text": "and multiplying them\nelement-wise to work out a new vector. And then, we get two vectors\nthat we're adding together.",
    "start": "3496400",
    "end": "3504480"
  },
  {
    "text": "So this way of doing\nthings element-wise, you sort of don't really see in\nstandard linear algebra course.",
    "start": "3504480",
    "end": "3512930"
  },
  {
    "text": "It's referred to as\nthe Hadamard product. It's represented by\nsome kind of circle.",
    "start": "3512930",
    "end": "3519170"
  },
  {
    "text": "Actually, in more\nmodern work, it's been more usual to represent\nit with this slightly bigger circle, with the dot at the\nmiddle as the Hadamard product",
    "start": "3519170",
    "end": "3527350"
  },
  {
    "text": "symbol. And someday, I'll change\nthese slides to be like that. But I was lazy in\nredoing the equations.",
    "start": "3527350",
    "end": "3534530"
  },
  {
    "text": "But the other notation\nyou do see quite often is, just using the\nsame little circle",
    "start": "3534530",
    "end": "3539630"
  },
  {
    "text": "that you use for\nfunction composition, to represent Hadamard product.",
    "start": "3539630",
    "end": "3544880"
  },
  {
    "text": "Okay, so all of these things\nare being done as vectors of the same length n.",
    "start": "3544880",
    "end": "3550790"
  },
  {
    "text": "And the other thing\nthat you might notice is that the candidate update,\nand the forget input and output",
    "start": "3550790",
    "end": "3560630"
  },
  {
    "text": "gates, all have a\nvery similar form. The only difference is three\nlogistics and one tanh.",
    "start": "3560630",
    "end": "3568130"
  },
  {
    "text": "And none of them\ndepend on each other. So all four of those can\nbe calculated in parallel.",
    "start": "3568130",
    "end": "3574630"
  },
  {
    "text": "And if you want to have an\nefficient LSTM implementation, that's what you do.",
    "start": "3574630",
    "end": "3580070"
  },
  {
    "text": "OK, so here's the more\ngraphical presentation of this. So these pictures\ncome from Chris Olah.",
    "start": "3580070",
    "end": "3587299"
  },
  {
    "text": "And I guess he did such a\nnice job at producing pictures for LSTMs, that almost\neveryone uses them these days.",
    "start": "3587300",
    "end": "3595730"
  },
  {
    "text": "And so this sort of pulls\napart the computation graph",
    "start": "3595730",
    "end": "3600740"
  },
  {
    "text": "of an LSTM unit. So blowing this up, you've got\nfrom the previous timestep,",
    "start": "3600740",
    "end": "3608390"
  },
  {
    "text": "both your cell and\nhidden recurrent vectors.",
    "start": "3608390",
    "end": "3614000"
  },
  {
    "text": "And so you feed\nthe hidden vector from the previous timestep\nand the new input xt",
    "start": "3614000",
    "end": "3623490"
  },
  {
    "text": "into the computation of the\ngates, which is happening down at the bottom. So you compute the forget gate.",
    "start": "3623490",
    "end": "3629700"
  },
  {
    "text": "And then, you use the forget\ngate in a Hadamard product, here drawn as actually\na time symbol.",
    "start": "3629700",
    "end": "3637170"
  },
  {
    "text": "So forget some cell content. You work out the input gate. And then, using the input gate\nand a regular recurrent neural",
    "start": "3637170",
    "end": "3647490"
  },
  {
    "text": "network-like computation, you\ncan compute candidate new cell content.",
    "start": "3647490",
    "end": "3654030"
  },
  {
    "text": "And so then, you add\nthose two together to get the new cell content,\nwhich then heads out",
    "start": "3654030",
    "end": "3662369"
  },
  {
    "text": "as the new cell\ncontent, at time t. But then you also have\nworked out an output gate.",
    "start": "3662370",
    "end": "3669870"
  },
  {
    "text": "And so then you take\nthe cell content, put it through\nanother nonlinearity,",
    "start": "3669870",
    "end": "3676410"
  },
  {
    "text": "and Hadamard product it\nwith the output gate.",
    "start": "3676410",
    "end": "3681510"
  },
  {
    "text": "And that then gives you\nthe new hidden state. So this is all kind of complex.",
    "start": "3681510",
    "end": "3689440"
  },
  {
    "text": "But as to understanding why\nsomething that's different is happening here,\nthe thing to notice",
    "start": "3689440",
    "end": "3696309"
  },
  {
    "text": "is that the cell\nstate from t minus 1 is passing right through this\nto be the cell state at time t,",
    "start": "3696310",
    "end": "3706540"
  },
  {
    "text": "without very much\nhappening to it. So some of it is being\ndeleted by the forget gate.",
    "start": "3706540",
    "end": "3715180"
  },
  {
    "text": "And then some new stuff\nis being written to it, as a result of using this\ncandidate new cell content.",
    "start": "3715180",
    "end": "3724869"
  },
  {
    "text": "But the real secret\nof the LSTM is",
    "start": "3724870",
    "end": "3731620"
  },
  {
    "text": "that new stuff is\njust being added to the cell with\nan addition, right?",
    "start": "3731620",
    "end": "3737270"
  },
  {
    "text": "So in the simple RNN,\nat each successive step, you are doing a multiplication.",
    "start": "3737270",
    "end": "3743410"
  },
  {
    "text": "And that makes it\nincredibly difficult to learn to preserve\ninformation in the hidden state,",
    "start": "3743410",
    "end": "3750880"
  },
  {
    "text": "over a long period of time. It's not completely impossible. But it's a very\ndifficult thing to learn.",
    "start": "3750880",
    "end": "3757840"
  },
  {
    "text": "Whereas with this new\nLSTM architecture, it's trivial to preserve\ninformation in the cell",
    "start": "3757840",
    "end": "3764620"
  },
  {
    "text": "from one timestep to the next. You just don't forget it. And it'll carry\nright through with,",
    "start": "3764620",
    "end": "3771580"
  },
  {
    "text": "perhaps some new stuff\nadded in to also remember. And so that's the sense in\nwhich the cell behaves much more",
    "start": "3771580",
    "end": "3780610"
  },
  {
    "text": "like RAM, in a conventional\ncomputer that is storing stuff. And extra stuff can\nbe stored into it.",
    "start": "3780610",
    "end": "3787359"
  },
  {
    "text": "And other stuff can be deleted\nfrom it, as you go along. ",
    "start": "3787360",
    "end": "3793240"
  },
  {
    "text": "OK, so the LSTM\narchitecture makes it much easier to preserve\ninformation for many timesteps,",
    "start": "3793240",
    "end": "3799670"
  },
  {
    "text": "right?  So in particular, standard\npractice with LSTMs",
    "start": "3799670",
    "end": "3807760"
  },
  {
    "text": "is to initialize the forget\ngate to a 1 vector, which is just so that a starting point\nis to say, preserve everything",
    "start": "3807760",
    "end": "3816670"
  },
  {
    "text": "from previous timesteps. And then, it is then\nlearning when it's",
    "start": "3816670",
    "end": "3822100"
  },
  {
    "text": "appropriate to forget stuff. All right. In contrast, it's very hard to\nget a simple RNN to preserve",
    "start": "3822100",
    "end": "3830750"
  },
  {
    "text": "stuff for a very long time. I mean, what does\nthat actually mean?",
    "start": "3830750",
    "end": "3836520"
  },
  {
    "text": "Well, I've put down\nsome numbers here. I mean, what you get in practice\ndepends on a million things.",
    "start": "3836520",
    "end": "3845220"
  },
  {
    "text": "It depends on the\nnature of your data, and how much data\nyou have, and what dimensionality your hidden\nstates are, blurdy, blurdy,",
    "start": "3845220",
    "end": "3853100"
  },
  {
    "text": "blur. But just to give you some\nidea of what's going on is, typically, if you\ntrain a simple recurrent",
    "start": "3853100",
    "end": "3861950"
  },
  {
    "text": "neural network, that its\neffective memory, its ability to be able to use things in the\npast to condition the future,",
    "start": "3861950",
    "end": "3869060"
  },
  {
    "text": "goes for about seven timesteps. You just really can't get it\nto remember stuff further back",
    "start": "3869060",
    "end": "3874670"
  },
  {
    "text": "in the past than that. Whereas for the LSTM,\nit's not complete magic.",
    "start": "3874670",
    "end": "3882335"
  },
  {
    "text": "It doesn't work forever. But it's effectively able\nto remember and use things",
    "start": "3882335",
    "end": "3888770"
  },
  {
    "text": "from much, much further back. So typically, you find\nthat with an LSTM,",
    "start": "3888770",
    "end": "3894019"
  },
  {
    "text": "you can effectively remember and\nuse things about 100 time steps back. And that's just\nenormously more useful",
    "start": "3894020",
    "end": "3901700"
  },
  {
    "text": "for a lot of the natural\nlanguage understanding tasks that we want to do.",
    "start": "3901700",
    "end": "3908220"
  },
  {
    "text": "And so that was precisely what\nthe LSTM was designed to do.",
    "start": "3908220",
    "end": "3913440"
  },
  {
    "text": "And I mean, so in particular,\njust going back to its name, quite a few people\nmisparsed its name.",
    "start": "3913440",
    "end": "3919760"
  },
  {
    "text": "The idea of its\nname was, there's a concept of short-term memory,\nwhich comes from psychology.",
    "start": "3919760",
    "end": "3925970"
  },
  {
    "text": "And it had been\nsuggested for simple RNNs that the hidden state of the\nRNN could be a model of humans'",
    "start": "3925970",
    "end": "3933800"
  },
  {
    "text": "short-term memory. And then, there would be\nsomething somewhere else that would deal with\nhuman long-term memory.",
    "start": "3933800",
    "end": "3941310"
  },
  {
    "text": "But while people had found that\nthis only gave you a very short short-term memory, so what\nHochreiter and Schmidhuber were",
    "start": "3941310",
    "end": "3950180"
  },
  {
    "text": "interested in was\nhow we could give-- construct models with a\nlong short-term memory.",
    "start": "3950180",
    "end": "3957960"
  },
  {
    "text": "And so that, then, gave\nus this name of LSTM.",
    "start": "3957960",
    "end": "3963050"
  },
  {
    "text": "LSTMs don't guarantee that there\nare no vanishing or exploding gradients. But in practice, they provide--",
    "start": "3963050",
    "end": "3970970"
  },
  {
    "text": "they don't tend to explode\nnearly the same way again. That plus sign is crucial,\nrather than a multiplication.",
    "start": "3970970",
    "end": "3978770"
  },
  {
    "text": "And so they're a much\nmore effective way of learning long-distance\ndependencies. ",
    "start": "3978770",
    "end": "3986410"
  },
  {
    "text": "OK, so despite the fact\nthat LSTMs were developed around 1997, 2000, it was\nreally only in the early 2010s,",
    "start": "3986410",
    "end": "3998500"
  },
  {
    "text": "that the world woke up to them\nand how successful they were. So it was really\naround 2013 to 2015",
    "start": "3998500",
    "end": "4006599"
  },
  {
    "text": "that LSTMs sort\nof hit the world, achieving state\nof the art results",
    "start": "4006600",
    "end": "4012060"
  },
  {
    "text": "on all kinds of problems. One of the first\nbig demonstrations was for handwriting recognition,\nthen speech recognition.",
    "start": "4012060",
    "end": "4020369"
  },
  {
    "text": "But then going on to a lot\nof natural language tasks, including machine translation,\nparsing, vision and language",
    "start": "4020370",
    "end": "4028410"
  },
  {
    "text": "tasks like image captioning,\nas well, of course, using them for language models. And around these years, LSTMs\nbecame the dominant approach",
    "start": "4028410",
    "end": "4038220"
  },
  {
    "text": "for most NLP tasks. The easiest way to build\na good strong model was to approach the\nproblem with an LSTM.",
    "start": "4038220",
    "end": "4046470"
  },
  {
    "text": "So now in 2021,\nactually, LSTMs are starting to be supplanted,\nor have been supplanted",
    "start": "4046470",
    "end": "4053190"
  },
  {
    "text": "by other approaches,\nparticularly transformer models, which we'll get to in\nthe class in a couple of weeks",
    "start": "4053190",
    "end": "4059760"
  },
  {
    "text": "time. So this is the sort of\npicture you can see. So for many years, there's\nbeen a machine translation",
    "start": "4059760",
    "end": "4066480"
  },
  {
    "text": "conference. And so a bake off competition\ncalled WMT, Workshop on Machine Translation.",
    "start": "4066480",
    "end": "4073180"
  },
  {
    "text": "So if you look at the\nhistory of that, in WMT 2014,",
    "start": "4073180",
    "end": "4078930"
  },
  {
    "text": "there was zero neural\nmachine translation systems in the competition. 2014 was actually\nthe first year,",
    "start": "4078930",
    "end": "4086730"
  },
  {
    "text": "that the success of LSTMs\nfor machine translation",
    "start": "4086730",
    "end": "4092609"
  },
  {
    "text": "was proven in a\nconference paper. But nothing occurred\nin this competition.",
    "start": "4092610",
    "end": "4098670"
  },
  {
    "text": "By 2016, everyone had jumped\non LSTMs as working great.",
    "start": "4098670",
    "end": "4106770"
  },
  {
    "text": "And lots of people, including\nthe winner of the competition, was using an LSTM model.",
    "start": "4106770",
    "end": "4112620"
  },
  {
    "text": "If you then jump ahead\nto 2019, then there's relatively little use of LSTMs.",
    "start": "4112620",
    "end": "4120720"
  },
  {
    "text": "And the vast majority of people\nare now using transformers. So things change quickly\nin neural network land.",
    "start": "4120720",
    "end": "4127380"
  },
  {
    "text": "And I keep on having to\nrewrite these lectures. ",
    "start": "4127380",
    "end": "4132830"
  },
  {
    "text": "So a quick further note\non vanishing and exploding gradients. Is it only a problem with\nrecurrent neural networks?",
    "start": "4132830",
    "end": "4140390"
  },
  {
    "text": "It's not. It's actually a problem\nthat also occurs anywhere where you have a lot\nof depth, including",
    "start": "4140390",
    "end": "4147109"
  },
  {
    "text": "feedforward and convolutional\nneural networks. Any time when you've got long\nsequences of chain rules, which",
    "start": "4147109",
    "end": "4156020"
  },
  {
    "text": "give you multiplications,\nthe gradient can become vanishingly\nsmall as it backpropagates.",
    "start": "4156020",
    "end": "4162799"
  },
  {
    "text": "And so generally, lower\nlayers are learned very slowly and are hard to train.",
    "start": "4162800",
    "end": "4169470"
  },
  {
    "text": "So there's been a lot of\neffort in other places as well, to come up with different\narchitectures that",
    "start": "4169470",
    "end": "4177229"
  },
  {
    "text": "let you learn more\nefficiently in deep networks.",
    "start": "4177229",
    "end": "4182270"
  },
  {
    "text": "And the commonest\nway to do that is to add more direct\nconnections, that",
    "start": "4182270",
    "end": "4187430"
  },
  {
    "text": "allow the gradient to flow. So the big thing and vision\nin the last few years",
    "start": "4187430",
    "end": "4192890"
  },
  {
    "text": "has been ResNets,\nwhere the Res stands for residual connections. And so the way they're\nmade-- this picture",
    "start": "4192890",
    "end": "4200210"
  },
  {
    "text": "is upside down, so the\ninput is at the top, is that you have these\nsort of two paths that",
    "start": "4200210",
    "end": "4207780"
  },
  {
    "text": "are summed together. One path is just\nan identity path. And the other one goes through\nsome neural network layers.",
    "start": "4207780",
    "end": "4214700"
  },
  {
    "text": "And so therefore,\nits default behavior is just to preserve\nthe input, which",
    "start": "4214700",
    "end": "4221270"
  },
  {
    "text": "might sound a little bit like\nwhat we just saw for LSTMs. There are other methods.",
    "start": "4221270",
    "end": "4226730"
  },
  {
    "text": "There have been DenseNets,\nwhere you add skip connections forward to every latent layer.",
    "start": "4226730",
    "end": "4233210"
  },
  {
    "text": "HighwayNets were also actually\ndeveloped by Schmidhuber, and sort of reminiscent of\nwhat was done with LSTMs.",
    "start": "4233210",
    "end": "4241580"
  },
  {
    "text": "So rather than just having\nan identity connection, as a ResNet has, it\nintroduces an extra gate.",
    "start": "4241580",
    "end": "4249260"
  },
  {
    "text": "So it looks more like\nan LSTM, which says, how much to send the input\nthrough the highway versus how",
    "start": "4249260",
    "end": "4257630"
  },
  {
    "text": "much to put it through\na neural net layer? And those two are then\ncombined into the output.",
    "start": "4257630",
    "end": "4263090"
  },
  {
    "text": " So essentially, this\nproblem occurs anywhere",
    "start": "4263090",
    "end": "4271300"
  },
  {
    "text": "when you have a lot of depth in\nyour layers of neural network. But it first arose--",
    "start": "4271300",
    "end": "4278200"
  },
  {
    "text": "and it turns out\nto be especially problematic with\nrecurrent neural networks.",
    "start": "4278200",
    "end": "4283630"
  },
  {
    "text": "They are particularly\nunstable, because of the fact that you've got this one weight\nmatrix that you're repeatedly",
    "start": "4283630",
    "end": "4291310"
  },
  {
    "text": "using through the time sequence. ",
    "start": "4291310",
    "end": "4300180"
  },
  {
    "text": "OK, so-- Chris, we've got a couple\nof questions, more or less, about whether you would\never want to use an--",
    "start": "4300180",
    "end": "4306960"
  },
  {
    "text": "like a simple RNN\ninstead of an LSTM? How does the LSTM learn\nwhat to do with its gates?",
    "start": "4306960",
    "end": "4313679"
  },
  {
    "text": "Can you opine on those things? Sure. So I think, basically\nthe answer is,",
    "start": "4313680",
    "end": "4321450"
  },
  {
    "text": "you should never use a\nsimple RNN these days. You should always use an LSTM.",
    "start": "4321450",
    "end": "4328072"
  },
  {
    "text": "I mean, obviously, that\ndepends on what you're doing. If you're wanting to do some\nanalytical paper or something,",
    "start": "4328072",
    "end": "4333929"
  },
  {
    "text": "you might prefer a simple RNN. And it is the case\nthat you can actually",
    "start": "4333930",
    "end": "4341180"
  },
  {
    "text": "get decent results\nwith simple RNNs, providing you are very careful\nto make sure that things aren't",
    "start": "4341180",
    "end": "4347420"
  },
  {
    "text": "exploding nor vanishing. ",
    "start": "4347420",
    "end": "4352980"
  },
  {
    "text": "But in practice,\ngetting simple RNNs to work and preserve\nlong context",
    "start": "4352980",
    "end": "4359550"
  },
  {
    "text": "is incredibly difficult,\nwhere you can train LSTMs and they will just work.",
    "start": "4359550",
    "end": "4364940"
  },
  {
    "text": "So really, you should\nalways just use an LSTM. Now wait, the\nsecond question was?",
    "start": "4364940",
    "end": "4372930"
  },
  {
    "text": "I think there's a\nbit of confusion about whether the gates\nare learning differently.",
    "start": "4372930",
    "end": "4378000"
  },
  {
    "text": "Oh, yeah. So the gates are\nalso just learned.",
    "start": "4378000",
    "end": "4384219"
  },
  {
    "text": "So if we go back\nto these equations,",
    "start": "4384220",
    "end": "4389490"
  },
  {
    "text": "this is the complete model. And when we're training\nthe model, every one",
    "start": "4389490",
    "end": "4394920"
  },
  {
    "text": "of these parameters, so\nall of these W, U, and b's,",
    "start": "4394920",
    "end": "4400530"
  },
  {
    "text": "everything is simultaneously\nbeing trained by backprop.",
    "start": "4400530",
    "end": "4405570"
  },
  {
    "text": "So that what you hope,\nand indeed it works, is the model is\nlearning, what stuff",
    "start": "4405570",
    "end": "4413520"
  },
  {
    "text": "should I remember for a\nlong time, versus what stuff should I forget? What things in the\ninput are important,",
    "start": "4413520",
    "end": "4420390"
  },
  {
    "text": "versus what things in the\ninput don't really matter? So it can learn things\nlike function words,",
    "start": "4420390",
    "end": "4426960"
  },
  {
    "text": "like a and the\ndon't really matter, even though everyone\nuses them in English. So you can just not\nworry about those.",
    "start": "4426960",
    "end": "4434280"
  },
  {
    "text": "So all of this is learned. And the models do, actually,\nsuccessfully learn gate values",
    "start": "4434280",
    "end": "4440040"
  },
  {
    "text": "about what information is\nuseful to preserve long-term, versus what information\nis really only",
    "start": "4440040",
    "end": "4446640"
  },
  {
    "text": "useful short-term, for\npredicting the next one or two words.",
    "start": "4446640",
    "end": "4451679"
  },
  {
    "text": "Finally, the gradient\nimprovements, due to the-- so",
    "start": "4451680",
    "end": "4457350"
  },
  {
    "text": "you said that the\naddition is really important between the new cell\ncandidate and the cell state. And I don't think-- at\nleast, a couple of students",
    "start": "4457350",
    "end": "4463530"
  },
  {
    "text": "have sort of questioned that. So if you want to go over that\nagain, that might be useful.",
    "start": "4463530",
    "end": "4468710"
  },
  {
    "text": "Sure.  So what we would like is\nan easy way for memory",
    "start": "4468710",
    "end": "4477700"
  },
  {
    "text": "to be preserved long-term. And one way, which\nis what ResNets use,",
    "start": "4477700",
    "end": "4486720"
  },
  {
    "text": "is just to completely have a\ndirect path from Ct minus 1 to Ct, and will preserve\nentirely the history.",
    "start": "4486720",
    "end": "4495929"
  },
  {
    "text": "So kind of there's\na default action of preserving information\nabout the past long-term.",
    "start": "4495930",
    "end": "4503630"
  },
  {
    "text": "LSTMs don't quite do that. But they allow that\nfunction to be easy.",
    "start": "4503630",
    "end": "4510090"
  },
  {
    "text": "So you start off with\nthe previous cell state. And you can forget some\nof it by the forget gate.",
    "start": "4510090",
    "end": "4517170"
  },
  {
    "text": "So you can delete stuff\nout of your memory. That's a useful operation. And then, well,\nyou're going to be",
    "start": "4517170",
    "end": "4522480"
  },
  {
    "text": "able to update the\ncontent of the cell with the write operation that\noccurs in the plus, where,",
    "start": "4522480",
    "end": "4530820"
  },
  {
    "text": "depending on the input\ngate, some parts of what's in the cell will be added too.",
    "start": "4530820",
    "end": "4536610"
  },
  {
    "text": "But you can think of\nthat adding as overlaying extra information. Everything that was in the\ncell that wasn't forgotten",
    "start": "4536610",
    "end": "4544620"
  },
  {
    "text": "is still continuing on\nto the next timestep. ",
    "start": "4544620",
    "end": "4550010"
  },
  {
    "text": "And in particular, when you're\ndoing the backpropagation through time, that there isn't--",
    "start": "4550010",
    "end": "4558079"
  },
  {
    "text": "I want to say, there\nisn't a multiplication between Ct and Ct minus 1. And there's this unfortunate\ntime symbol here.",
    "start": "4558080",
    "end": "4566540"
  },
  {
    "text": "But remember that's the\nHadamard product, which is zeroing out part of\nit with the forget gate.",
    "start": "4566540",
    "end": "4571670"
  },
  {
    "text": "It's not a multiplication by a\nmatrix, like in the simple RNN. ",
    "start": "4571670",
    "end": "4582560"
  },
  {
    "text": "I hope that's good. OK, so there are a\ncouple of other things that I wanted to get\nthrough before the end.",
    "start": "4582560",
    "end": "4590510"
  },
  {
    "text": "I guess I'm not going to\nhave time to do both of them, I think. So I'll do the last one,\nprobably, next time.",
    "start": "4590510",
    "end": "4595679"
  },
  {
    "text": "Though these are\nactually simple and easy things, that they\ncomplete our picture.",
    "start": "4595680",
    "end": "4603420"
  },
  {
    "text": "So I sort of briefly\nalluded to this example of sentiment classification,\nwhere what we could do",
    "start": "4603420",
    "end": "4609560"
  },
  {
    "text": "is run an RNN, maybe an\nLSTM, over a sentence,",
    "start": "4609560",
    "end": "4615050"
  },
  {
    "text": "call this our representation\nof the sentence, and you feed it into\na softmax classifier",
    "start": "4615050",
    "end": "4624260"
  },
  {
    "text": "to classify for sentiment. So what we're actually\nsaying there is that,",
    "start": "4624260",
    "end": "4630320"
  },
  {
    "text": "we can regard the hidden\nstate as a representation of a word in context.",
    "start": "4630320",
    "end": "4637670"
  },
  {
    "text": "That the low, that we have just\na word vector for terribly. But we then looked\nat our context",
    "start": "4637670",
    "end": "4645800"
  },
  {
    "text": "and say, OK, we've now created\na hidden state representation",
    "start": "4645800",
    "end": "4651349"
  },
  {
    "text": "for the word terribly, in\nthe context of the movie was. And that proves to be\na really useful idea,",
    "start": "4651350",
    "end": "4659150"
  },
  {
    "text": "because words have different\nmeanings in different contexts. But it seems like there's a\ndefect of what we've done here,",
    "start": "4659150",
    "end": "4666830"
  },
  {
    "text": "because our context only\ncontains information from the left.",
    "start": "4666830",
    "end": "4672500"
  },
  {
    "text": "What about right context? Surely, it would also be useful\nto have the meaning of terribly",
    "start": "4672500",
    "end": "4678469"
  },
  {
    "text": "depend on exciting. Because often, words\nmean different things based on what follows them.",
    "start": "4678470",
    "end": "4686660"
  },
  {
    "text": "So if you have\nsomething like red wine, it means something quite\ndifferent from a red light.",
    "start": "4686660",
    "end": "4694239"
  },
  {
    "text": "So how could we deal with that? Well, an easy way to deal\nwith that would be to say,",
    "start": "4694240",
    "end": "4700130"
  },
  {
    "text": "well, if we're just\nwanting to come up with a neural encoding\nof a sentence, we could have a second RNN with\ncompletely separate parameters",
    "start": "4700130",
    "end": "4708880"
  },
  {
    "text": "learned. And we could run it backwards\nthrough the sentence to get a backward\nrepresentation of each word.",
    "start": "4708880",
    "end": "4716680"
  },
  {
    "text": "And then, we could get\nan overall representation of each word and context by\njust concatenating those two",
    "start": "4716680",
    "end": "4723370"
  },
  {
    "text": "representations. And now we've got a\nrepresentation of terribly that has both left\nand right context.",
    "start": "4723370",
    "end": "4732889"
  },
  {
    "text": "So we're simply\nrunning a forward RNN. And when I say RNN\nin here, that just",
    "start": "4732890",
    "end": "4739329"
  },
  {
    "text": "means any kind of\nrecurrent neural network. So commonly, it'll be an LSTM.",
    "start": "4739330",
    "end": "4744670"
  },
  {
    "text": "And a backward one. And then at each\ntimestep, we're just concatenating their\nrepresentations,",
    "start": "4744670",
    "end": "4751810"
  },
  {
    "text": "with each of these\nhaving separate weights. And so then we regard\nthis concatenated thing",
    "start": "4751810",
    "end": "4758260"
  },
  {
    "text": "as the hidden state, the\ncontextual representation of a token at a particular\ntime, that we pass forward.",
    "start": "4758260",
    "end": "4766239"
  },
  {
    "text": "This is so common, that people\nuse a shortcut to denote that.",
    "start": "4766240",
    "end": "4772090"
  },
  {
    "text": "And they'll just draw this\npicture with two-sided arrows. And when you see that picture\nwith two-sided arrows,",
    "start": "4772090",
    "end": "4779260"
  },
  {
    "text": "it means that you're running\ntwo RNNs, one in each direction.",
    "start": "4779260",
    "end": "4786789"
  },
  {
    "text": "And then, concatenating their\nresults at each timestep. And that's what you're going\nto use later in the model.",
    "start": "4786790",
    "end": "4792670"
  },
  {
    "text": " Okay, so if you're doing\nan encoding problem,",
    "start": "4792670",
    "end": "4800710"
  },
  {
    "text": "like for sentiment\nclassification or question answering, using bidirectional\nRNNs is a great thing to do.",
    "start": "4800710",
    "end": "4810190"
  },
  {
    "text": "But they are only applicable\nif you have access to the entire input sequence.",
    "start": "4810190",
    "end": "4815770"
  },
  {
    "text": "They're not applicable\nto language modeling. Because in a language\nmodel, necessarily, you",
    "start": "4815770",
    "end": "4822100"
  },
  {
    "text": "have to generate\nthe next word based on only the preceding context.",
    "start": "4822100",
    "end": "4827710"
  },
  {
    "text": "But if you do have the\nentire input sequence, that bidirectionality\ngives you greater power.",
    "start": "4827710",
    "end": "4833679"
  },
  {
    "text": "And indeed, that's been an\nidea that people have built on in subsequent work.",
    "start": "4833680",
    "end": "4839060"
  },
  {
    "text": "So when we get to transformers\nin a couple of weeks, we'll spend plenty of time\non the BERT model, where",
    "start": "4839060",
    "end": "4847360"
  },
  {
    "text": "that acronym stands for\nBidirectional Encoder Representations\nfrom Transformers.",
    "start": "4847360",
    "end": "4853300"
  },
  {
    "text": "So part of what's important in\nthat model is the transformer.",
    "start": "4853300",
    "end": "4858730"
  },
  {
    "text": "But really, a central\npoint of the paper was to say that you could build\nmore powerful models using",
    "start": "4858730",
    "end": "4865540"
  },
  {
    "text": "transformers by, again,\nexploiting bidirectionality.",
    "start": "4865540",
    "end": "4873070"
  },
  {
    "text": "OK, there's one teeny\nbit left on RNNs. But I'll sneak it\ninto next class. And I'll call it\nthe end for today.",
    "start": "4873070",
    "end": "4880190"
  },
  {
    "text": "And if there are\nother things you'd like to ask questions about,\nyou can find me on Nooks",
    "start": "4880190",
    "end": "4885910"
  },
  {
    "text": "again in just a minute. Okay, so I'll see you\nagain next Tuesday.",
    "start": "4885910",
    "end": "4892949"
  },
  {
    "start": "4892950",
    "end": "4898000"
  }
]