[
  {
    "start": "0",
    "end": "70000"
  },
  {
    "start": "0",
    "end": "4352"
  },
  {
    "text": "CHRISTOPHER POTTS:\nHello, everyone.",
    "start": "4352",
    "end": "5810"
  },
  {
    "text": "Welcome back.",
    "start": "5810",
    "end": "6420"
  },
  {
    "text": "This is part 5 in our\nseries on distributed word",
    "start": "6420",
    "end": "8660"
  },
  {
    "text": "representations.",
    "start": "8660",
    "end": "9387"
  },
  {
    "text": "We're going to be talking\nabout dimensionality reduction",
    "start": "9387",
    "end": "11720"
  },
  {
    "text": "techniques.",
    "start": "11720",
    "end": "12650"
  },
  {
    "text": "We saw in the\nprevious screencast",
    "start": "12650",
    "end": "14330"
  },
  {
    "text": "that reweighting\nis a powerful tool",
    "start": "14330",
    "end": "16160"
  },
  {
    "text": "for finding latent semantic\ninformation in count matrices.",
    "start": "16160",
    "end": "20557"
  },
  {
    "text": "We're going to push\nthat even further.",
    "start": "20557",
    "end": "22140"
  },
  {
    "text": "The promise of dimensionality\nreduction techniques",
    "start": "22140",
    "end": "24590"
  },
  {
    "text": "is that they can\ncapture higher order",
    "start": "24590",
    "end": "26480"
  },
  {
    "text": "notions of co-occurrence\ncorresponding",
    "start": "26480",
    "end": "28220"
  },
  {
    "text": "to even deeper sorts of\nsemantic relatedness.",
    "start": "28220",
    "end": "33500"
  },
  {
    "text": "There's a wide world of these\ndimensionality reduction",
    "start": "33500",
    "end": "35750"
  },
  {
    "text": "techniques.",
    "start": "35750",
    "end": "36260"
  },
  {
    "text": "I've chosen three\nthat we're going",
    "start": "36260",
    "end": "37677"
  },
  {
    "text": "to focus on as interesting\nrepresentatives of a much",
    "start": "37677",
    "end": "40160"
  },
  {
    "text": "larger space.",
    "start": "40160",
    "end": "41570"
  },
  {
    "text": "We'll look at latent\nsemantic analysis, which",
    "start": "41570",
    "end": "43489"
  },
  {
    "text": "is a classic linear method.",
    "start": "43490",
    "end": "45410"
  },
  {
    "text": "Then we'll talk about\nautoencoders and newer,",
    "start": "45410",
    "end": "47450"
  },
  {
    "text": "powerful deep learning mode for\nlearning reduced dimensional",
    "start": "47450",
    "end": "50870"
  },
  {
    "text": "representations.",
    "start": "50870",
    "end": "52250"
  },
  {
    "text": "And then finally, GloVe, which\nis a simple yet very powerful",
    "start": "52250",
    "end": "55580"
  },
  {
    "text": "method that, as you'll\nsee, has a deep connection",
    "start": "55580",
    "end": "58040"
  },
  {
    "text": "to pointwise mutual information.",
    "start": "58040",
    "end": "60350"
  },
  {
    "text": "And then I'm going to\nclose by talking briefly",
    "start": "60350",
    "end": "62690"
  },
  {
    "text": "about visualization,\nwhich is another kind",
    "start": "62690",
    "end": "64610"
  },
  {
    "text": "of dimensionality\nreduction technique",
    "start": "64610",
    "end": "66380"
  },
  {
    "text": "that we might use for\nvery different purposes.",
    "start": "66380",
    "end": "70172"
  },
  {
    "start": "70000",
    "end": "118000"
  },
  {
    "text": "So let's begin with\nLatent Semantic",
    "start": "70172",
    "end": "71630"
  },
  {
    "text": "Analysis, a classic method.",
    "start": "71630",
    "end": "73369"
  },
  {
    "text": "The paper is due to\nDeerwester et al, 1990.",
    "start": "73370",
    "end": "75950"
  },
  {
    "text": "That's a classic paper\nthat really made a splash.",
    "start": "75950",
    "end": "78770"
  },
  {
    "text": "It's one of the--",
    "start": "78770",
    "end": "79670"
  },
  {
    "text": "LSA is now one of the oldest,\nmost widely used dimensionality",
    "start": "79670",
    "end": "82549"
  },
  {
    "text": "reduction techniques, not only\nin scientific research but also",
    "start": "82550",
    "end": "85700"
  },
  {
    "text": "an industry.",
    "start": "85700",
    "end": "86310"
  },
  {
    "text": "I think it was really\neye opening for people",
    "start": "86310",
    "end": "88189"
  },
  {
    "text": "at the time of the\npaper's appearance",
    "start": "88190",
    "end": "91010"
  },
  {
    "text": "to see just how powerful\nthis technique could be,",
    "start": "91010",
    "end": "94070"
  },
  {
    "text": "especially in contexts\ninvolving information retrieval.",
    "start": "94070",
    "end": "98360"
  },
  {
    "text": "The method is also known\nas Truncated Singular Value",
    "start": "98360",
    "end": "101630"
  },
  {
    "text": "Decomposition.",
    "start": "101630",
    "end": "102350"
  },
  {
    "text": "And I'll explain why\nthat is in a second.",
    "start": "102350",
    "end": "104250"
  },
  {
    "text": "The final thing I want\nto say at this high level",
    "start": "104250",
    "end": "106250"
  },
  {
    "text": "is just that LSA remains\na very powerful baseline,",
    "start": "106250",
    "end": "109070"
  },
  {
    "text": "especially when part of a\npipeline of other reweighting",
    "start": "109070",
    "end": "112080"
  },
  {
    "text": "methods.",
    "start": "112080",
    "end": "112970"
  },
  {
    "text": "So it should probably be\nin your results table.",
    "start": "112970",
    "end": "115220"
  },
  {
    "text": "And it's often very\ndifficult to beat.",
    "start": "115220",
    "end": "119310"
  },
  {
    "start": "118000",
    "end": "222000"
  },
  {
    "text": "Now I think we can't, in\nthe time allotted to us,",
    "start": "119310",
    "end": "121799"
  },
  {
    "text": "cover all of the technical\ndetails surrounding",
    "start": "121800",
    "end": "124140"
  },
  {
    "text": "latent semantic analysis.",
    "start": "124140",
    "end": "125250"
  },
  {
    "text": "In my experience,\nthis would be kind",
    "start": "125250",
    "end": "126750"
  },
  {
    "text": "of the culmination of a full\ncourse in linear algebra.",
    "start": "126750",
    "end": "129750"
  },
  {
    "text": "But I do think I can convey\nthe guiding intuitions.",
    "start": "129750",
    "end": "132240"
  },
  {
    "text": "And that will help you with\nresponsible use of the method.",
    "start": "132240",
    "end": "135205"
  },
  {
    "text": "So let's imagine that we have\nthis simple two-dimensional",
    "start": "135205",
    "end": "137580"
  },
  {
    "text": "vector space model.",
    "start": "137580",
    "end": "138720"
  },
  {
    "text": "I've got four points,\nA, B, C, and D",
    "start": "138720",
    "end": "141090"
  },
  {
    "text": "arrayed out in this\ntwo-dimensional space.",
    "start": "141090",
    "end": "143400"
  },
  {
    "text": "I think we're all familiar with\nfitting linear models, which",
    "start": "143400",
    "end": "146159"
  },
  {
    "text": "capture the largest source\nof variation in the data.",
    "start": "146160",
    "end": "149160"
  },
  {
    "text": "That's this orange line here.",
    "start": "149160",
    "end": "150688"
  },
  {
    "text": "And the perspective I\nwould encourage you to take",
    "start": "150688",
    "end": "152730"
  },
  {
    "text": "is that we can think of\nthat linear regression model",
    "start": "152730",
    "end": "156180"
  },
  {
    "text": "as performing dimensionality\nreduction in that it encourages",
    "start": "156180",
    "end": "159209"
  },
  {
    "text": "us to project points like B\nand C down onto that line.",
    "start": "159210",
    "end": "164250"
  },
  {
    "text": "And then projecting them\ndown onto that line,",
    "start": "164250",
    "end": "166290"
  },
  {
    "text": "essentially in abstracting away\nfrom their point of variation",
    "start": "166290",
    "end": "169590"
  },
  {
    "text": "along the y-axis, we can\nsee the sense in which",
    "start": "169590",
    "end": "172260"
  },
  {
    "text": "they are abstractly similar.",
    "start": "172260",
    "end": "173780"
  },
  {
    "text": "They're close together in this\nreduced dimensional space.",
    "start": "173780",
    "end": "177720"
  },
  {
    "text": "Now with a linear\nmodel, we captured",
    "start": "177720",
    "end": "179220"
  },
  {
    "text": "the source of greatest variation\nin this little data set.",
    "start": "179220",
    "end": "181920"
  },
  {
    "text": "In the high dimensional space,\nwe could continue fitting lines",
    "start": "181920",
    "end": "185580"
  },
  {
    "text": "to other sources of\nvariation in the data,",
    "start": "185580",
    "end": "187590"
  },
  {
    "text": "other axes of variation.",
    "start": "187590",
    "end": "188989"
  },
  {
    "text": "So here's a blue line here that\ncaptures the next dimension.",
    "start": "188990",
    "end": "192090"
  },
  {
    "text": "And we could, again,\nproject points like A and C",
    "start": "192090",
    "end": "194250"
  },
  {
    "text": "down onto that line.",
    "start": "194250",
    "end": "195210"
  },
  {
    "text": "And that would capture\nthe abstract sense",
    "start": "195210",
    "end": "197070"
  },
  {
    "text": "in which A and C,\nalthough very spread out",
    "start": "197070",
    "end": "199500"
  },
  {
    "text": "along the x dimension,\nare very close together",
    "start": "199500",
    "end": "202230"
  },
  {
    "text": "along the y dimension.",
    "start": "202230",
    "end": "203790"
  },
  {
    "text": "And of course, if we had more\ndimensions in this vector space",
    "start": "203790",
    "end": "206579"
  },
  {
    "text": "model, we could\ncontinue to perform",
    "start": "206580",
    "end": "208680"
  },
  {
    "text": "these cuts and\ndimensionality reductions,",
    "start": "208680",
    "end": "210930"
  },
  {
    "text": "capturing ever more abstract\nnotions of similarity",
    "start": "210930",
    "end": "214590"
  },
  {
    "text": "along these different axes.",
    "start": "214590",
    "end": "216269"
  },
  {
    "text": "And that is, in\nessence, what LSA",
    "start": "216270",
    "end": "218220"
  },
  {
    "text": "is going to do for us in\nour really large matrices.",
    "start": "218220",
    "end": "223110"
  },
  {
    "start": "222000",
    "end": "286000"
  },
  {
    "text": "The fundamental\nmethod, as I said,",
    "start": "223110",
    "end": "224830"
  },
  {
    "text": "is singular value decomposition.",
    "start": "224830",
    "end": "226630"
  },
  {
    "text": "This is a theorem\nfrom linear algebra",
    "start": "226630",
    "end": "228270"
  },
  {
    "text": "that says any matrix\nof dimension n by n",
    "start": "228270",
    "end": "231900"
  },
  {
    "text": "can be decomposed into the\nproduct of three matrices, T,",
    "start": "231900",
    "end": "235319"
  },
  {
    "text": "S, and D, with the\ndimensions given.",
    "start": "235320",
    "end": "238420"
  },
  {
    "text": "Here's a more concrete example.",
    "start": "238420",
    "end": "240630"
  },
  {
    "text": "Start with this matrix\nof dimension 3 by 4.",
    "start": "240630",
    "end": "243120"
  },
  {
    "text": "We learn the term matrix, which\nis full of length normalized",
    "start": "243120",
    "end": "247620"
  },
  {
    "text": "orthogonal vectors.",
    "start": "247620",
    "end": "249299"
  },
  {
    "text": "We have this matrix of singular\nvalues along the diagonal.",
    "start": "249300",
    "end": "253150"
  },
  {
    "text": "They are organized from largest\nto smallest, corresponding",
    "start": "253150",
    "end": "255840"
  },
  {
    "text": "to the greatest to least source\nof variation in the data.",
    "start": "255840",
    "end": "259560"
  },
  {
    "text": "And then we have the document\nor columnized matrix,",
    "start": "259560",
    "end": "262019"
  },
  {
    "text": "which is also length normalized\nand orthogonal in its space.",
    "start": "262019",
    "end": "266400"
  },
  {
    "text": "And the theorem here is\nthat we can reconstruct A",
    "start": "266400",
    "end": "268590"
  },
  {
    "text": "from these three matrices.",
    "start": "268590",
    "end": "270630"
  },
  {
    "text": "Of course, we don't want\nto precisely reconstruct",
    "start": "270630",
    "end": "272840"
  },
  {
    "text": "A. That probably wouldn't\naccomplish very much for us.",
    "start": "272840",
    "end": "275520"
  },
  {
    "text": "But what we can do\nis use this to learn",
    "start": "275520",
    "end": "277889"
  },
  {
    "text": "reduced dimensional\nrepresentations of A",
    "start": "277890",
    "end": "280350"
  },
  {
    "text": "by being selective\nabout which term",
    "start": "280350",
    "end": "282750"
  },
  {
    "text": "and singular value dimensions\nwe include in the model.",
    "start": "282750",
    "end": "286518"
  },
  {
    "start": "286000",
    "end": "394000"
  },
  {
    "text": "Let me walk you through an\nexample of how that happens.",
    "start": "286518",
    "end": "288810"
  },
  {
    "text": "And first, let me\nmotivate this a little bit",
    "start": "288810",
    "end": "290643"
  },
  {
    "text": "with an idealized\nlinguistic case.",
    "start": "290643",
    "end": "292840"
  },
  {
    "text": "So I've got up here a\nword by document matrix.",
    "start": "292840",
    "end": "296460"
  },
  {
    "text": "Its vocabulary is gnarly,\nwicked, awesome, lame,",
    "start": "296460",
    "end": "299008"
  },
  {
    "text": "and terrible.",
    "start": "299008",
    "end": "299550"
  },
  {
    "text": "And the conceit of my example\nis that both gnarly and wicked",
    "start": "299550",
    "end": "302909"
  },
  {
    "text": "are positive terms.",
    "start": "302910",
    "end": "303990"
  },
  {
    "text": "So they tend to co-occur\nwith awesome and not",
    "start": "303990",
    "end": "306319"
  },
  {
    "text": "co-occur with lame and terrible.",
    "start": "306320",
    "end": "308350"
  },
  {
    "text": "However, gnarly and wicked never\noccur in the same doc here.",
    "start": "308350",
    "end": "311140"
  },
  {
    "text": "The idea is that gnarly is a\nslang positive term associated",
    "start": "311140",
    "end": "314880"
  },
  {
    "text": "with the West Coast\nof the United States.",
    "start": "314880",
    "end": "316890"
  },
  {
    "text": "And wicked is a slang term\nassociated with the East",
    "start": "316890",
    "end": "320040"
  },
  {
    "text": "Coast of the United States.",
    "start": "320040",
    "end": "321510"
  },
  {
    "text": "In virtue of that\nidealized dialect split,",
    "start": "321510",
    "end": "324420"
  },
  {
    "text": "they never occur in\nthe same document.",
    "start": "324420",
    "end": "326670"
  },
  {
    "text": "But nonetheless, they\nhave similar neighbors",
    "start": "326670",
    "end": "328740"
  },
  {
    "text": "in this vector space.",
    "start": "328740",
    "end": "330000"
  },
  {
    "text": "And that's the kind of abstract\nnotion of co-occurrence",
    "start": "330000",
    "end": "332490"
  },
  {
    "text": "that we want to capture.",
    "start": "332490",
    "end": "334940"
  },
  {
    "text": "If we simply use our\nstandard distance",
    "start": "334940",
    "end": "337610"
  },
  {
    "text": "measures and reweighting\ntechniques and so forth,",
    "start": "337610",
    "end": "339949"
  },
  {
    "text": "we will not capture that\nmore abstract notion",
    "start": "339950",
    "end": "342110"
  },
  {
    "text": "of co-occurrence.",
    "start": "342110",
    "end": "342919"
  },
  {
    "text": "Here, distances in\nthis raw vector space,",
    "start": "342920",
    "end": "345560"
  },
  {
    "text": "gnarly, awesome, terrible,\nand wicked, wicked",
    "start": "345560",
    "end": "348020"
  },
  {
    "text": "is farther away from gnarly\neven than terrible is.",
    "start": "348020",
    "end": "350569"
  },
  {
    "text": "So we've got a sentiment\nconfusion and really just",
    "start": "350570",
    "end": "353150"
  },
  {
    "text": "not the result we\nwere shooting for.",
    "start": "353150",
    "end": "355860"
  },
  {
    "text": "So we perform singular\nvalue decomposition",
    "start": "355860",
    "end": "359240"
  },
  {
    "text": "into these three matrices.",
    "start": "359240",
    "end": "360889"
  },
  {
    "text": "And then the truncated\npart is that we're",
    "start": "360890",
    "end": "362600"
  },
  {
    "text": "going to consider just the\nfirst two dimensions of the term",
    "start": "362600",
    "end": "365270"
  },
  {
    "text": "matrix corresponding to these\ntwo singular values capturing",
    "start": "365270",
    "end": "369110"
  },
  {
    "text": "the top two sources of\nvariation in the data.",
    "start": "369110",
    "end": "373229"
  },
  {
    "text": "So we multiply those\ntogether and we",
    "start": "373230",
    "end": "374990"
  },
  {
    "text": "get this reduced dimensional\nmatrix down here, 2",
    "start": "374990",
    "end": "378139"
  },
  {
    "text": "by the size of the vocabulary.",
    "start": "378140",
    "end": "380370"
  },
  {
    "text": "And if we do distance\nmeasures in that space,",
    "start": "380370",
    "end": "382820"
  },
  {
    "text": "just as we were hoping, gnarly\nand wicked are now neighbors.",
    "start": "382820",
    "end": "386030"
  },
  {
    "text": "The method has captured\nthat more abstract notion",
    "start": "386030",
    "end": "389060"
  },
  {
    "text": "of having the same\nneighbors as the other word.",
    "start": "389060",
    "end": "391760"
  },
  {
    "start": "391760",
    "end": "394658"
  },
  {
    "start": "394000",
    "end": "464000"
  },
  {
    "text": "In the previous\nlecture, I encouraged",
    "start": "394658",
    "end": "396199"
  },
  {
    "text": "you to think about what\nyou're doing to a matrix",
    "start": "396200",
    "end": "398748"
  },
  {
    "text": "when you perform some kind\nof reweighting scheme.",
    "start": "398748",
    "end": "400789"
  },
  {
    "text": "Let's extend that to these\ndimensionality reduction",
    "start": "400790",
    "end": "403190"
  },
  {
    "text": "techniques.",
    "start": "403190",
    "end": "403780"
  },
  {
    "text": "So here's a picture of what LSA\ndoes starting with a raw count",
    "start": "403780",
    "end": "407480"
  },
  {
    "text": "distribution over here.",
    "start": "407480",
    "end": "409340"
  },
  {
    "text": "If I just run LSA on that\nraw count distribution,",
    "start": "409340",
    "end": "412610"
  },
  {
    "text": "I get what looks also like a\nvery difficult distribution",
    "start": "412610",
    "end": "416000"
  },
  {
    "text": "of values.",
    "start": "416000",
    "end": "416960"
  },
  {
    "text": "The cell values are\nvery spread out.",
    "start": "416960",
    "end": "419389"
  },
  {
    "text": "And they have a lot of the\nmass centered around 0 here,",
    "start": "419390",
    "end": "423230"
  },
  {
    "text": "corresponding to the peak\nin the raw counts over here",
    "start": "423230",
    "end": "425720"
  },
  {
    "text": "near 0 as well.",
    "start": "425720",
    "end": "427028"
  },
  {
    "text": "So that doesn't look like\nwe've done very much in terms",
    "start": "427028",
    "end": "429320"
  },
  {
    "text": "of taming the kind of\nuntractable skewed distribution",
    "start": "429320",
    "end": "432200"
  },
  {
    "text": "we started with.",
    "start": "432200",
    "end": "433430"
  },
  {
    "text": "However, if instead we take\nthe raw counts and first feed",
    "start": "433430",
    "end": "436280"
  },
  {
    "text": "them through PMI,\nwhich as we saw",
    "start": "436280",
    "end": "437960"
  },
  {
    "text": "before gives us this nice\ndistribution of values, highly",
    "start": "437960",
    "end": "441229"
  },
  {
    "text": "constrained along the\nx-axis, and then we run LSA,",
    "start": "441230",
    "end": "444770"
  },
  {
    "text": "we retain a lot of\nthose good properties.",
    "start": "444770",
    "end": "446750"
  },
  {
    "text": "The values are somewhat\nmore spread out",
    "start": "446750",
    "end": "448940"
  },
  {
    "text": "but still nicely distributed.",
    "start": "448940",
    "end": "450830"
  },
  {
    "text": "This looks like a\nmuch happier input",
    "start": "450830",
    "end": "452659"
  },
  {
    "text": "to downstream analytic methods\nthan the top version here.",
    "start": "452660",
    "end": "455978"
  },
  {
    "text": "And I think this is\nbeginning to show",
    "start": "455978",
    "end": "457520"
  },
  {
    "text": "that it can be powerful\nto pipeline reweighting",
    "start": "457520",
    "end": "460849"
  },
  {
    "text": "and dimensionality\nreduction techniques.",
    "start": "460850",
    "end": "463745"
  },
  {
    "text": "Another note I would\nwant to make, how do you",
    "start": "463745",
    "end": "465620"
  },
  {
    "start": "464000",
    "end": "531000"
  },
  {
    "text": "choose the dimensionality\nfor, let's say, it",
    "start": "465620",
    "end": "467479"
  },
  {
    "text": "has this variable k\ncorresponding to the number",
    "start": "467480",
    "end": "469940"
  },
  {
    "text": "of dimensions that you keep.",
    "start": "469940",
    "end": "471890"
  },
  {
    "text": "If you read the\nliterature on LSA,",
    "start": "471890",
    "end": "473733"
  },
  {
    "text": "they often imagine this kind\nof what I've called the dream",
    "start": "473733",
    "end": "476150"
  },
  {
    "text": "scenario where you plot\nthe singular values",
    "start": "476150",
    "end": "479150"
  },
  {
    "text": "and you see that a lot\nof them are very large.",
    "start": "479150",
    "end": "481430"
  },
  {
    "text": "And then there's\na sudden drop off.",
    "start": "481430",
    "end": "483350"
  },
  {
    "text": "And if you do see\nthis, then it's",
    "start": "483350",
    "end": "484845"
  },
  {
    "text": "obvious that you should pick\nthe point of the sudden drop",
    "start": "484845",
    "end": "487220"
  },
  {
    "text": "off as your k.",
    "start": "487220",
    "end": "488330"
  },
  {
    "text": "So here, you would pick k as 20.",
    "start": "488330",
    "end": "489998"
  },
  {
    "text": "And you'd be confident that\nyou had captured almost all",
    "start": "489998",
    "end": "492289"
  },
  {
    "text": "the variation in your data in\nthe reduced dimensional space",
    "start": "492290",
    "end": "495020"
  },
  {
    "text": "you were creating.",
    "start": "495020",
    "end": "496699"
  },
  {
    "text": "Unfortunately, for the kinds\nof matrices and problems",
    "start": "496700",
    "end": "500150"
  },
  {
    "text": "that we're looking at, I really\nnever see the dream scenario.",
    "start": "500150",
    "end": "502805"
  },
  {
    "text": "What I see looks\nsomething much more",
    "start": "502805",
    "end": "504590"
  },
  {
    "text": "like this, where you have\nkind of a sudden drop",
    "start": "504590",
    "end": "506930"
  },
  {
    "text": "off early and then a long\ndecline and maybe a sudden drop",
    "start": "506930",
    "end": "509960"
  },
  {
    "text": "off at the end.",
    "start": "509960",
    "end": "511039"
  },
  {
    "text": "And it's basically totally\nunclear where in the space",
    "start": "511040",
    "end": "513770"
  },
  {
    "text": "you should pick k.",
    "start": "513770",
    "end": "515240"
  },
  {
    "text": "And the result is\nthat k is often",
    "start": "515240",
    "end": "517130"
  },
  {
    "text": "chosen kind of empirically\nas a hyperparameter to",
    "start": "517130",
    "end": "520817"
  },
  {
    "text": "and against whatever\nproblem you're actually",
    "start": "520817",
    "end": "522650"
  },
  {
    "text": "trying to solve.",
    "start": "522650",
    "end": "524070"
  },
  {
    "text": "If, in doing this work, you\ndo see the dream scenario,",
    "start": "524070",
    "end": "526700"
  },
  {
    "text": "please do write to me.",
    "start": "526700",
    "end": "527720"
  },
  {
    "text": "It would be very exciting\nto see that happen.",
    "start": "527720",
    "end": "532009"
  },
  {
    "start": "531000",
    "end": "564000"
  },
  {
    "text": "LSA is just one of a large\nfamily of matrix decomposition",
    "start": "532010",
    "end": "535640"
  },
  {
    "text": "methods.",
    "start": "535640",
    "end": "536880"
  },
  {
    "text": "Here's a list of a few of them.",
    "start": "536880",
    "end": "538460"
  },
  {
    "text": "And a lot of them are\nimplemented in scikit-learn",
    "start": "538460",
    "end": "540920"
  },
  {
    "text": "in its decomposition library.",
    "start": "540920",
    "end": "542959"
  },
  {
    "text": "And I would encourage you\nto try them out and just see",
    "start": "542960",
    "end": "545240"
  },
  {
    "text": "how they perform on problems\nthat you're trying to solve.",
    "start": "545240",
    "end": "549399"
  },
  {
    "text": "And finally, here's\na little bit of code.",
    "start": "549400",
    "end": "551230"
  },
  {
    "text": "vsm.lsa with k set\nto 100 gives me back",
    "start": "551230",
    "end": "555760"
  },
  {
    "text": "a reduced dimensional\nversion of that matrix,",
    "start": "555760",
    "end": "557980"
  },
  {
    "text": "keeping the same\nvocabulary, of course,",
    "start": "557980",
    "end": "559930"
  },
  {
    "text": "but now with only 100\ncolumn dimensions.",
    "start": "559930",
    "end": "562240"
  },
  {
    "start": "562240",
    "end": "564890"
  },
  {
    "start": "564000",
    "end": "589000"
  },
  {
    "text": "Let's move to autoencoders.",
    "start": "564890",
    "end": "566180"
  },
  {
    "text": "This will be a point\nof contast with LSA.",
    "start": "566180",
    "end": "568279"
  },
  {
    "text": "Because this is a much\nmore powerful method.",
    "start": "568280",
    "end": "571042"
  },
  {
    "text": "So here's the overview.",
    "start": "571042",
    "end": "572000"
  },
  {
    "text": "Autoencoders are\na flexible class",
    "start": "572000",
    "end": "573740"
  },
  {
    "text": "of deep learning architectures\nfor learning reduced",
    "start": "573740",
    "end": "576020"
  },
  {
    "text": "dimensional representations.",
    "start": "576020",
    "end": "577880"
  },
  {
    "text": "If you want to hear much more\nabout this class of models,",
    "start": "577880",
    "end": "580490"
  },
  {
    "text": "I would encourage you to read\nchapter 14 of the Goodfellow et",
    "start": "580490",
    "end": "583310"
  },
  {
    "text": "al book, Deep Learning.",
    "start": "583310",
    "end": "584830"
  },
  {
    "text": "It has a lot of\ndetails and a lot",
    "start": "584830",
    "end": "586460"
  },
  {
    "text": "of variations on this theme.",
    "start": "586460",
    "end": "589070"
  },
  {
    "start": "589000",
    "end": "752000"
  },
  {
    "text": "Here is the basic\nautoencoder model.",
    "start": "589070",
    "end": "591500"
  },
  {
    "text": "The input would be\nthe, say, that vectors",
    "start": "591500",
    "end": "594350"
  },
  {
    "text": "from the rows in our matrices.",
    "start": "594350",
    "end": "596462"
  },
  {
    "text": "So this could be the\ncounts or something",
    "start": "596463",
    "end": "598130"
  },
  {
    "text": "that you've done to the counts.",
    "start": "598130",
    "end": "600370"
  },
  {
    "text": "Those are fed through a hidden\nlayer of representation.",
    "start": "600370",
    "end": "603310"
  },
  {
    "text": "And then the goal\nof this model is",
    "start": "603310",
    "end": "606040"
  },
  {
    "text": "to try to literally\nreconstruct the input.",
    "start": "606040",
    "end": "609339"
  },
  {
    "text": "Now, that might be\ntrivial if h had",
    "start": "609340",
    "end": "611620"
  },
  {
    "text": "the same dimensionality as x.",
    "start": "611620",
    "end": "613270"
  },
  {
    "text": "But the whole idea\nhere is that you're",
    "start": "613270",
    "end": "614950"
  },
  {
    "text": "going to feed the input\nthrough a very narrow pipe",
    "start": "614950",
    "end": "617920"
  },
  {
    "text": "and then try to\nreconstruct the input.",
    "start": "617920",
    "end": "620649"
  },
  {
    "text": "Given that you're feeding it\nthrough a potentially very",
    "start": "620650",
    "end": "622990"
  },
  {
    "text": "narrow pipe, it's\nunlikely that you'll",
    "start": "622990",
    "end": "624880"
  },
  {
    "text": "be able to fully\nreconstruct the inputs.",
    "start": "624880",
    "end": "627310"
  },
  {
    "text": "But the idea is\nthat the model will",
    "start": "627310",
    "end": "629020"
  },
  {
    "text": "learn to reconstruct the\nimportant sources of variation",
    "start": "629020",
    "end": "631990"
  },
  {
    "text": "in performing this\nautoencoding step.",
    "start": "631990",
    "end": "635180"
  },
  {
    "text": "And then when we use these\nmodels for representation",
    "start": "635180",
    "end": "638529"
  },
  {
    "text": "learning in the mode that\nwe've been in for this unit,",
    "start": "638530",
    "end": "641120"
  },
  {
    "text": "the representation\nthat we choose",
    "start": "641120",
    "end": "642550"
  },
  {
    "text": "is this hidden unit here.",
    "start": "642550",
    "end": "644230"
  },
  {
    "text": "We typically don't\ncare about what",
    "start": "644230",
    "end": "646000"
  },
  {
    "text": "was reconstructed on\nthe output, but rather",
    "start": "646000",
    "end": "648280"
  },
  {
    "text": "only about the hidden reduced\ndimensional representation",
    "start": "648280",
    "end": "651190"
  },
  {
    "text": "that the model learned.",
    "start": "651190",
    "end": "652270"
  },
  {
    "text": "This slide has a bunch of\nother annotations on it.",
    "start": "652270",
    "end": "654470"
  },
  {
    "text": "And the reason I included them\nis that the course repository",
    "start": "654470",
    "end": "657759"
  },
  {
    "text": "includes a reference\nimplementation",
    "start": "657760",
    "end": "659237"
  },
  {
    "text": "of an autoencoder and all the\nother deep learning models",
    "start": "659237",
    "end": "661570"
  },
  {
    "text": "that we cover in pure NumPy.",
    "start": "661570",
    "end": "663532"
  },
  {
    "text": "And so if you want\nit to understand",
    "start": "663532",
    "end": "664990"
  },
  {
    "text": "all of the technical details\nof how the model is constructed",
    "start": "664990",
    "end": "667779"
  },
  {
    "text": "and optimized, you could use\nthis as a kind of cheat sheet",
    "start": "667780",
    "end": "670870"
  },
  {
    "text": "to understand how\nthe code works.",
    "start": "670870",
    "end": "673200"
  },
  {
    "text": "I think the fundamental idea\nthat you want to have is simply",
    "start": "673200",
    "end": "675700"
  },
  {
    "text": "that the model is trying\nto reconstruct its inputs.",
    "start": "675700",
    "end": "678430"
  },
  {
    "text": "The error signal that\nwe get is the difference",
    "start": "678430",
    "end": "680500"
  },
  {
    "text": "between the reconstructed\nand actual input.",
    "start": "680500",
    "end": "683300"
  },
  {
    "text": "And that error signal\nis what we use to update",
    "start": "683300",
    "end": "685360"
  },
  {
    "text": "the parameters of the model.",
    "start": "685360",
    "end": "689071"
  },
  {
    "text": "Final thing I would\nmention here is",
    "start": "689072",
    "end": "690530"
  },
  {
    "text": "that it could be very\ndifficult for this model",
    "start": "690530",
    "end": "693020"
  },
  {
    "text": "if you feed in the raw\ncurrent vectors down here.",
    "start": "693020",
    "end": "695180"
  },
  {
    "text": "They have very high\ndimensionality.",
    "start": "695180",
    "end": "696740"
  },
  {
    "text": "And their distribution is\nhighly skewed as we've seen.",
    "start": "696740",
    "end": "699440"
  },
  {
    "text": "So it can be very productive to\ndo a little bit of reweighting",
    "start": "699440",
    "end": "702200"
  },
  {
    "text": "and maybe even dimensionality\nreduction with LSA",
    "start": "702200",
    "end": "705470"
  },
  {
    "text": "before you start feeding\ninputs into this model.",
    "start": "705470",
    "end": "708620"
  },
  {
    "text": "Of course, it could\nstill be meaningful,",
    "start": "708620",
    "end": "710360"
  },
  {
    "text": "even if you've done LSA\nas a pre-processing step,",
    "start": "710360",
    "end": "713029"
  },
  {
    "text": "to learn a hidden\ndimensional representation",
    "start": "713030",
    "end": "715010"
  },
  {
    "text": "because this model is presumably\ncapable of learning even more",
    "start": "715010",
    "end": "718730"
  },
  {
    "text": "abstract notions than LSA is\nin virtue of its non-linearity",
    "start": "718730",
    "end": "723050"
  },
  {
    "text": "at this hidden layer.",
    "start": "723050",
    "end": "726050"
  },
  {
    "text": "And here's a bit of code\njust showing how this works,",
    "start": "726050",
    "end": "728600"
  },
  {
    "text": "using both the reference\nimplementation that I mentioned",
    "start": "728600",
    "end": "731029"
  },
  {
    "text": "as well as a faster\nand more flexible",
    "start": "731030",
    "end": "732830"
  },
  {
    "text": "Torch autoencoder which is\nalso included in the course",
    "start": "732830",
    "end": "736370"
  },
  {
    "text": "repository.",
    "start": "736370",
    "end": "737480"
  },
  {
    "text": "I think the only interface\nthing to mention here",
    "start": "737480",
    "end": "739490"
  },
  {
    "text": "is that these models\nhave a fit method,",
    "start": "739490",
    "end": "741740"
  },
  {
    "text": "like all the other machine\nlearning models for this.",
    "start": "741740",
    "end": "744050"
  },
  {
    "text": "But the fit method\nreturns that hidden",
    "start": "744050",
    "end": "746450"
  },
  {
    "text": "dimensional\nrepresentation, the target",
    "start": "746450",
    "end": "748400"
  },
  {
    "text": "for our learning\nin this context,",
    "start": "748400",
    "end": "750000"
  },
  {
    "text": "which is a bit non-standard.",
    "start": "750000",
    "end": "751520"
  },
  {
    "text": "But it's the\nintended application",
    "start": "751520",
    "end": "753680"
  },
  {
    "start": "752000",
    "end": "793000"
  },
  {
    "text": "for this kind of\nrepresentation learning.",
    "start": "753680",
    "end": "757352"
  },
  {
    "text": "The other thing I\nwould mention is,",
    "start": "757352",
    "end": "758810"
  },
  {
    "text": "so let's see how well the\nautoencoder is performing.",
    "start": "758810",
    "end": "761890"
  },
  {
    "text": "This is the raw distances in\nthe giga5 matrix for finance.",
    "start": "761890",
    "end": "765370"
  },
  {
    "text": "This is the count matrix.",
    "start": "765370",
    "end": "766480"
  },
  {
    "text": "It doesn't look great.",
    "start": "766480",
    "end": "768070"
  },
  {
    "text": "If we run the autoencoder\ndirectly on the count matrix,",
    "start": "768070",
    "end": "770950"
  },
  {
    "text": "it looks a little better.",
    "start": "770950",
    "end": "772120"
  },
  {
    "text": "But it's still not excellent.",
    "start": "772120",
    "end": "773860"
  },
  {
    "text": "If we think of this\nas part of a pipeline",
    "start": "773860",
    "end": "775779"
  },
  {
    "text": "where we've first done positive\npointwise mutual information,",
    "start": "775780",
    "end": "779120"
  },
  {
    "text": "and then LSA at\ndimension 100, and then",
    "start": "779120",
    "end": "781180"
  },
  {
    "text": "dub the autoencoding\nstep, it starts",
    "start": "781180",
    "end": "783279"
  },
  {
    "text": "to look like a really good and\ninteresting semantic space.",
    "start": "783280",
    "end": "785870"
  },
  {
    "text": "And I think that's pointing\nout the power of including",
    "start": "785870",
    "end": "788120"
  },
  {
    "text": "the autoencoder in a larger\npipeline of preprocessing",
    "start": "788120",
    "end": "791230"
  },
  {
    "text": "on the count matrices.",
    "start": "791230",
    "end": "793329"
  },
  {
    "start": "793000",
    "end": "799000"
  },
  {
    "text": "Let's turn to GloVe\nfor Global Vectors",
    "start": "793330",
    "end": "795250"
  },
  {
    "text": "for the final major unit\nfor this screencast.",
    "start": "795250",
    "end": "798480"
  },
  {
    "text": "Here's a brief overview.",
    "start": "798480",
    "end": "799480"
  },
  {
    "start": "799000",
    "end": "874000"
  },
  {
    "text": "GloVe was introduced by Jeffrey\nPennington, Richard Socher,",
    "start": "799480",
    "end": "802779"
  },
  {
    "text": "and Chris Manning, a\nStanford team, in 2014.",
    "start": "802780",
    "end": "805660"
  },
  {
    "text": "Roughly speaking,\nthe guiding idea here",
    "start": "805660",
    "end": "807670"
  },
  {
    "text": "is that we want to\nlearn vectors for words",
    "start": "807670",
    "end": "810399"
  },
  {
    "text": "such that the dot\nproduct of those vectors",
    "start": "810400",
    "end": "812680"
  },
  {
    "text": "is proportional to the log\nprobability of co-occurrence",
    "start": "812680",
    "end": "815649"
  },
  {
    "text": "for those words.",
    "start": "815650",
    "end": "816730"
  },
  {
    "text": "And I'll elaborate\non that in a second.",
    "start": "816730",
    "end": "819930"
  },
  {
    "text": "For doing computational\nwork, we can",
    "start": "819930",
    "end": "822000"
  },
  {
    "text": "rely on the implementation\ntorch.glove.py",
    "start": "822000",
    "end": "824430"
  },
  {
    "text": "which is in the course repo.",
    "start": "824430",
    "end": "825920"
  },
  {
    "text": "I'll mention that\nthere's also a reference",
    "start": "825920",
    "end": "827670"
  },
  {
    "text": "implementation in bsm.py.",
    "start": "827670",
    "end": "829440"
  },
  {
    "text": "It's very slow but it kind\nof transparently implements",
    "start": "829440",
    "end": "832140"
  },
  {
    "text": "the core GloVe\nalgorithm so it could",
    "start": "832140",
    "end": "833760"
  },
  {
    "text": "be interesting to inspect.",
    "start": "833760",
    "end": "835765"
  },
  {
    "text": "And then if you're\ndoing practical work",
    "start": "835765",
    "end": "837390"
  },
  {
    "text": "with really large corpora and\nreally large vocabularies,",
    "start": "837390",
    "end": "840060"
  },
  {
    "text": "I would encourage you to use the\nGloVe team's C implementation.",
    "start": "840060",
    "end": "842970"
  },
  {
    "text": "It's an outstanding\nsoftware artifact",
    "start": "842970",
    "end": "844800"
  },
  {
    "text": "that will allow you to learn\nlots of good representations",
    "start": "844800",
    "end": "847649"
  },
  {
    "text": "quickly.",
    "start": "847650",
    "end": "848367"
  },
  {
    "text": "And that kind of brings\nme to my last point.",
    "start": "848367",
    "end": "850200"
  },
  {
    "text": "I just want to mention\nthat the GloVe team was",
    "start": "850200",
    "end": "852480"
  },
  {
    "text": "among the first teams in\nNLP to release not just data",
    "start": "852480",
    "end": "855810"
  },
  {
    "text": "and code but pre-trained\nmodel parameters.",
    "start": "855810",
    "end": "859170"
  },
  {
    "text": "Everyone does that these days.",
    "start": "859170",
    "end": "860613"
  },
  {
    "text": "But it was rare at the time.",
    "start": "860613",
    "end": "861779"
  },
  {
    "text": "And I think this team\nwas kind of really",
    "start": "861780",
    "end": "864150"
  },
  {
    "text": "forward thinking in seeing\nthe value of releasing",
    "start": "864150",
    "end": "866730"
  },
  {
    "text": "these centralized resources.",
    "start": "866730",
    "end": "867990"
  },
  {
    "text": "And a lot of really\ninteresting work",
    "start": "867990",
    "end": "870000"
  },
  {
    "text": "happened with GloVe\nvectors as a foundation.",
    "start": "870000",
    "end": "874762"
  },
  {
    "start": "874000",
    "end": "988000"
  },
  {
    "text": "All right, so let's think\nabout the technical aspects",
    "start": "874762",
    "end": "876970"
  },
  {
    "text": "of this model.",
    "start": "876970",
    "end": "877553"
  },
  {
    "text": "This is the GloVe objective.",
    "start": "877553",
    "end": "878769"
  },
  {
    "text": "And you're going to see,\npointwise, mutual information",
    "start": "878770",
    "end": "881560"
  },
  {
    "text": "kind of creep into this\npicture in an interesting way.",
    "start": "881560",
    "end": "884668"
  },
  {
    "text": "So this is equation\n6 from the paper.",
    "start": "884668",
    "end": "886210"
  },
  {
    "text": "It's kind of an idealized\nobjective for the GloVe model.",
    "start": "886210",
    "end": "888940"
  },
  {
    "text": "And it says what I said before.",
    "start": "888940",
    "end": "890680"
  },
  {
    "text": "We have a row vector and a\ncolumn vector, wi and wk.",
    "start": "890680",
    "end": "895708"
  },
  {
    "text": "We're going to get\ntheir dot product.",
    "start": "895708",
    "end": "897250"
  },
  {
    "text": "And the goal is to learn\nto have that dot product be",
    "start": "897250",
    "end": "900310"
  },
  {
    "text": "proportional to the log of the\nprobability of co-occurrence",
    "start": "900310",
    "end": "903160"
  },
  {
    "text": "of word i and word k.",
    "start": "903160",
    "end": "904959"
  },
  {
    "text": "Where the probability\nof co-occurence",
    "start": "904960",
    "end": "906700"
  },
  {
    "text": "is defined in the way that\nwe defined it before when",
    "start": "906700",
    "end": "909340"
  },
  {
    "text": "we were talking about\nrow normalization.",
    "start": "909340",
    "end": "911230"
  },
  {
    "text": "It's just done in log space.",
    "start": "911230",
    "end": "912790"
  },
  {
    "text": "This is the co-occurrence count.",
    "start": "912790",
    "end": "914680"
  },
  {
    "text": "This is the sum of all\nthe counts along that row.",
    "start": "914680",
    "end": "917589"
  },
  {
    "text": "And basically in\nlog space, we're",
    "start": "917590",
    "end": "918970"
  },
  {
    "text": "just dividing this\nvalue by this value.",
    "start": "918970",
    "end": "921920"
  },
  {
    "text": "So keep that in mind.",
    "start": "921920",
    "end": "923209"
  },
  {
    "text": "Now the reason they have\nonly the row represented",
    "start": "923210",
    "end": "925288"
  },
  {
    "text": "is that in the paper\nthey're assuming",
    "start": "925288",
    "end": "926830"
  },
  {
    "text": "that the rows and columns in\nthe underlying count matrix",
    "start": "926830",
    "end": "929770"
  },
  {
    "text": "are identical.",
    "start": "929770",
    "end": "930868"
  },
  {
    "text": "And so we don't need\nto include both.",
    "start": "930868",
    "end": "932410"
  },
  {
    "text": "However, if we did allow that\nthe row and context could",
    "start": "932410",
    "end": "935410"
  },
  {
    "text": "be different, we\nwould just elaborate",
    "start": "935410",
    "end": "937209"
  },
  {
    "text": "equation 6 to have a slightly\ndifferent denominator, right?",
    "start": "937210",
    "end": "940300"
  },
  {
    "text": "We would have the product of\nthe row sum and the column sum,",
    "start": "940300",
    "end": "943930"
  },
  {
    "text": "and take the log of that\nand subtract that out.",
    "start": "943930",
    "end": "946925"
  },
  {
    "text": "And that would be kind of our\ngoal for learning these dot",
    "start": "946925",
    "end": "949300"
  },
  {
    "text": "products here.",
    "start": "949300",
    "end": "950620"
  },
  {
    "text": "But aha, this is\nwhere PMI sneaks in.",
    "start": "950620",
    "end": "952690"
  },
  {
    "text": "Because that simply is\nthe PMI objective, right?",
    "start": "952690",
    "end": "955780"
  },
  {
    "text": "Where we stated that is\nthe log of the probability",
    "start": "955780",
    "end": "958630"
  },
  {
    "text": "of co-occurrence divided\nby the product of the row",
    "start": "958630",
    "end": "961210"
  },
  {
    "text": "and the column\nprobabilities, here they've",
    "start": "961210",
    "end": "963160"
  },
  {
    "text": "just stated exactly that\ncalculation in log space.",
    "start": "963160",
    "end": "965740"
  },
  {
    "text": "And these are numerically\nequivalent by the equivalence",
    "start": "965740",
    "end": "968350"
  },
  {
    "text": "of log of x over y\nbeing the same as log",
    "start": "968350",
    "end": "971199"
  },
  {
    "text": "of x minus the log of y.",
    "start": "971200",
    "end": "973312"
  },
  {
    "text": "So that's the deep connection\nthat I was highlighting",
    "start": "973312",
    "end": "975520"
  },
  {
    "text": "between GloVe and PMI.",
    "start": "975520",
    "end": "977038"
  },
  {
    "text": "And I think that's\nreally interesting",
    "start": "977038",
    "end": "978580"
  },
  {
    "text": "because it shows that\nfundamentally we're",
    "start": "978580",
    "end": "980530"
  },
  {
    "text": "testing a very similar\nhypothesis using",
    "start": "980530",
    "end": "983170"
  },
  {
    "text": "very similar notions of\nrow and column context.",
    "start": "983170",
    "end": "987070"
  },
  {
    "text": "Now the GloVe team\ndoesn't just stop there.",
    "start": "987070",
    "end": "989542"
  },
  {
    "start": "988000",
    "end": "1066000"
  },
  {
    "text": "The GloVe objective is\nactually much more interesting",
    "start": "989542",
    "end": "991750"
  },
  {
    "text": "as an elaboration of\nthat core PMI idea.",
    "start": "991750",
    "end": "995260"
  },
  {
    "text": "But it's worth\nhaving PMI in mind",
    "start": "995260",
    "end": "996880"
  },
  {
    "text": "because it's there\nthroughout this presentation.",
    "start": "996880",
    "end": "1000090"
  },
  {
    "text": "In the paper, they state this\nis a kind of idealized objective",
    "start": "1000090",
    "end": "1003000"
  },
  {
    "text": "where we're going to have the\ndot product, as I said before,",
    "start": "1003000",
    "end": "1005500"
  },
  {
    "text": "and two bias terms.",
    "start": "1005500",
    "end": "1006360"
  },
  {
    "text": "And the goal will be to make\nthat equivalent to the log",
    "start": "1006360",
    "end": "1008850"
  },
  {
    "text": "of the co-occurrence count.",
    "start": "1008850",
    "end": "1010589"
  },
  {
    "text": "That has some\nundesirable properties",
    "start": "1010590",
    "end": "1012738"
  },
  {
    "text": "from the point of view\nof machine learning.",
    "start": "1012738",
    "end": "1014529"
  },
  {
    "text": "So they propose, in the\nend, a weighted version",
    "start": "1014530",
    "end": "1017310"
  },
  {
    "text": "of that objective.",
    "start": "1017310",
    "end": "1018630"
  },
  {
    "text": "You can see we still have\nthe product of the row",
    "start": "1018630",
    "end": "1021180"
  },
  {
    "text": "and the column vectors\nand two biased terms.",
    "start": "1021180",
    "end": "1023820"
  },
  {
    "text": "We're going to subtract out the\nlog of the co-occurance count",
    "start": "1023820",
    "end": "1026910"
  },
  {
    "text": "and take the square of that.",
    "start": "1026910",
    "end": "1028140"
  },
  {
    "text": "And that's going\nto be weighted by f",
    "start": "1028140",
    "end": "1030074"
  },
  {
    "text": "of the co-occurrence count,\nwhere f is a function that you",
    "start": "1030075",
    "end": "1033419"
  },
  {
    "text": "can define by hand.",
    "start": "1033420",
    "end": "1034227"
  },
  {
    "text": "And what they do in the paper\nis that it was two parameters, x",
    "start": "1034227",
    "end": "1036810"
  },
  {
    "text": "max and alpha.",
    "start": "1036810",
    "end": "1039180"
  },
  {
    "text": "For any count that\nis above x max,",
    "start": "1039180",
    "end": "1041250"
  },
  {
    "text": "we're going to set it to 1, kind\nof flatten out all the really",
    "start": "1041250",
    "end": "1044520"
  },
  {
    "text": "large counts.",
    "start": "1044520",
    "end": "1045599"
  },
  {
    "text": "Everything that's\nbelow x max, we",
    "start": "1045599",
    "end": "1047490"
  },
  {
    "text": "will take as a\nproportion of x max",
    "start": "1047490",
    "end": "1049380"
  },
  {
    "text": "with some exponential scaling\nas specified by alpha.",
    "start": "1049380",
    "end": "1053400"
  },
  {
    "text": "That's the function there.",
    "start": "1053400",
    "end": "1055120"
  },
  {
    "text": "And typically, alpha is set\nto 0.75 and x max to 100.",
    "start": "1055120",
    "end": "1058500"
  },
  {
    "text": "But I encourage you to\nbe critical in thinking",
    "start": "1058500",
    "end": "1060870"
  },
  {
    "text": "about both those choices and\nhow they relate to your data.",
    "start": "1060870",
    "end": "1063930"
  },
  {
    "text": "I'll return to that in a second.",
    "start": "1063930",
    "end": "1066920"
  },
  {
    "start": "1066000",
    "end": "1098000"
  },
  {
    "text": "So GloVe really has these\nthree hyperparameters,",
    "start": "1066920",
    "end": "1068940"
  },
  {
    "text": "the dimensionality of the\nlearned representations, x max",
    "start": "1068940",
    "end": "1074159"
  },
  {
    "text": "which is going to have\nthis flattening effect,",
    "start": "1074160",
    "end": "1076110"
  },
  {
    "text": "and alpha which is going\nto scale the values, right?",
    "start": "1076110",
    "end": "1078790"
  },
  {
    "text": "And so here's an example of\nhow those are interacting,",
    "start": "1078790",
    "end": "1081360"
  },
  {
    "text": "x max and alpha.",
    "start": "1081360",
    "end": "1082320"
  },
  {
    "text": "If I start with\nthis vector, 100 99,",
    "start": "1082320",
    "end": "1085480"
  },
  {
    "text": "75, 10, and 1, the function\nf, as we specified it,",
    "start": "1085480",
    "end": "1089520"
  },
  {
    "text": "is going to flatten that out\ninto 1, 99, 81, 18, and 0.3.",
    "start": "1089520",
    "end": "1094860"
  },
  {
    "text": "You should just be aware\nthat kind of flattening",
    "start": "1094860",
    "end": "1096929"
  },
  {
    "text": "is happening.",
    "start": "1096930",
    "end": "1099260"
  },
  {
    "start": "1098000",
    "end": "1273000"
  },
  {
    "text": "So GloVe learning, so it's kind\nof interesting to think about",
    "start": "1099260",
    "end": "1102220"
  },
  {
    "text": "it analytically about how\nGloVe manages to learn",
    "start": "1102220",
    "end": "1104750"
  },
  {
    "text": "interesting representations.",
    "start": "1104750",
    "end": "1106185"
  },
  {
    "text": "And one thing that might be\non your mind is the question,",
    "start": "1106185",
    "end": "1108560"
  },
  {
    "text": "can it actually learn higher\norder notions of co-occurrence?",
    "start": "1108560",
    "end": "1111690"
  },
  {
    "text": "That's been the major selling\npoint of this lecture.",
    "start": "1111690",
    "end": "1114259"
  },
  {
    "text": "I gave that example involving\n\"gnarly\" and \"wicked\" with LSA.",
    "start": "1114260",
    "end": "1117620"
  },
  {
    "text": "Is GloVe going to be\nable to do that, right?",
    "start": "1117620",
    "end": "1119670"
  },
  {
    "text": "We could just pose\nthat as a question.",
    "start": "1119670",
    "end": "1122490"
  },
  {
    "text": "So let's start that and see what\nhappens, see how this works.",
    "start": "1122490",
    "end": "1125540"
  },
  {
    "text": "The loss calculations\nfor GloVe, this",
    "start": "1125540",
    "end": "1127160"
  },
  {
    "text": "is a kind of simplified version\nof the derivative of the model.",
    "start": "1127160",
    "end": "1131150"
  },
  {
    "text": "And we're going to\nshow how GloVe manages",
    "start": "1131150",
    "end": "1133070"
  },
  {
    "text": "to pull \"gnarly\" and\n\"wicked\" toward off center,",
    "start": "1133070",
    "end": "1135830"
  },
  {
    "text": "that little idealized\nspace that I used before.",
    "start": "1135830",
    "end": "1138912"
  },
  {
    "text": "I'm going to leave out the\nbias terms for simplicity.",
    "start": "1138912",
    "end": "1141120"
  },
  {
    "text": "But we could bring those in.",
    "start": "1141120",
    "end": "1142760"
  },
  {
    "text": "And so here's how this\nis going to proceed.",
    "start": "1142760",
    "end": "1145070"
  },
  {
    "text": "What I've done, just from\nthis idealized example,",
    "start": "1145070",
    "end": "1147409"
  },
  {
    "text": "is begin in a GloVe space where\nwicked and gnarly are as far",
    "start": "1147410",
    "end": "1150740"
  },
  {
    "text": "apart as I could make\nthem, so as different",
    "start": "1150740",
    "end": "1152540"
  },
  {
    "text": "as I could possibly make them.",
    "start": "1152540",
    "end": "1154190"
  },
  {
    "text": "But I've got awesome\nand terrible.",
    "start": "1154190",
    "end": "1155899"
  },
  {
    "text": "And awesome is kind of\nclose to gnarly already.",
    "start": "1155900",
    "end": "1158720"
  },
  {
    "text": "What you'll see is that\nafter just one iteration",
    "start": "1158720",
    "end": "1161090"
  },
  {
    "text": "of the model, what has happened\nis that wicked and gnarly",
    "start": "1161090",
    "end": "1164960"
  },
  {
    "text": "have been pulled toward awesome.",
    "start": "1164960",
    "end": "1166687"
  },
  {
    "text": "And that's just the kind\nof effect that we wanted.",
    "start": "1166687",
    "end": "1168770"
  },
  {
    "text": "That's the sense in which GloVe\ncan capture these higher order",
    "start": "1168770",
    "end": "1171920"
  },
  {
    "text": "notions of co-occurrence.",
    "start": "1171920",
    "end": "1173940"
  },
  {
    "text": "Just in a little\nmore detail, you",
    "start": "1173940",
    "end": "1175727"
  },
  {
    "text": "might want to study\nthis on your own.",
    "start": "1175728",
    "end": "1177270"
  },
  {
    "text": "But the high level overview\nof exactly how that learning",
    "start": "1177270",
    "end": "1179630"
  },
  {
    "text": "happens proceeds as follows.",
    "start": "1179630",
    "end": "1181560"
  },
  {
    "text": "We start from these\ncounts up here.",
    "start": "1181560",
    "end": "1183500"
  },
  {
    "text": "And the crucial\nassumption I'm making",
    "start": "1183500",
    "end": "1185150"
  },
  {
    "text": "is that wicked and\ngnarly never co-occur.",
    "start": "1185150",
    "end": "1188390"
  },
  {
    "text": "But they occur a\nlot with awesome.",
    "start": "1188390",
    "end": "1190340"
  },
  {
    "text": "And awesome will be kind of the\ngravitational pull that makes",
    "start": "1190340",
    "end": "1193490"
  },
  {
    "text": "gnarly and wicked look similar.",
    "start": "1193490",
    "end": "1196220"
  },
  {
    "text": "Keep in mind that because\nof that function f, by",
    "start": "1196220",
    "end": "1198752"
  },
  {
    "text": "and large with GloVe we're\ndealing not with the raw counts",
    "start": "1198753",
    "end": "1201170"
  },
  {
    "text": "but rather with the\nreweighted matrix.",
    "start": "1201170",
    "end": "1203030"
  },
  {
    "text": "And that preserves this property\nthat may never co-occur.",
    "start": "1203030",
    "end": "1205980"
  },
  {
    "text": "It gives differently\nscaled values",
    "start": "1205980",
    "end": "1207710"
  },
  {
    "text": "for the rest of the\nco-occurrence or pseudo",
    "start": "1207710",
    "end": "1209690"
  },
  {
    "text": "co-occurrence probabilities.",
    "start": "1209690",
    "end": "1212487"
  },
  {
    "text": "Right, and here's what\nwe're going to track.",
    "start": "1212487",
    "end": "1214320"
  },
  {
    "text": "This is the \"gnarly\"\nvector in 0.",
    "start": "1214320",
    "end": "1216408"
  },
  {
    "text": "And you can see I've made\nthem as far apart as I could.",
    "start": "1216408",
    "end": "1218700"
  },
  {
    "text": "They're kind of\nopposed to each other.",
    "start": "1218700",
    "end": "1220529"
  },
  {
    "text": "But we're going to see how they\nget pulled toward \"awesome\"",
    "start": "1220530",
    "end": "1222988"
  },
  {
    "text": "in the context vector.",
    "start": "1222988",
    "end": "1224880"
  },
  {
    "text": "So this is that\nloss calculation.",
    "start": "1224880",
    "end": "1226340"
  },
  {
    "text": "I have just plugged in\nall the values here.",
    "start": "1226340",
    "end": "1228620"
  },
  {
    "text": "And you can see that we get\nthis initial set of losses.",
    "start": "1228620",
    "end": "1232100"
  },
  {
    "text": "That's after one iteration, and\nwe update the weight matrices.",
    "start": "1232100",
    "end": "1235460"
  },
  {
    "text": "And we perform one\nmore round of learning.",
    "start": "1235460",
    "end": "1237289"
  },
  {
    "text": "And you can see that\nboth of these models,",
    "start": "1237290",
    "end": "1239480"
  },
  {
    "text": "the values here are getting\nlarger corresponding to getting",
    "start": "1239480",
    "end": "1242330"
  },
  {
    "text": "pulled closer and\ncloser toward awesome.",
    "start": "1242330",
    "end": "1244640"
  },
  {
    "text": "And you can see that\ngraphically happening over here",
    "start": "1244640",
    "end": "1247280"
  },
  {
    "text": "in these plots on the left.",
    "start": "1247280",
    "end": "1248790"
  },
  {
    "text": "And as I do more iterations\nof the GloVe model,",
    "start": "1248790",
    "end": "1251510"
  },
  {
    "text": "this effect is just going\nto strengthen corresponding",
    "start": "1251510",
    "end": "1253940"
  },
  {
    "text": "to wicked and gnarly getting\npulled toward awesome",
    "start": "1253940",
    "end": "1256759"
  },
  {
    "text": "and away from terrible as a\nresult of these underlying",
    "start": "1256760",
    "end": "1259670"
  },
  {
    "text": "counts.",
    "start": "1259670",
    "end": "1260600"
  },
  {
    "text": "And I take this as good\nevidence that GloVe,",
    "start": "1260600",
    "end": "1262820"
  },
  {
    "text": "like the other methods\nwe've discussed,",
    "start": "1262820",
    "end": "1264649"
  },
  {
    "text": "is capable of capturing\nthose higher order",
    "start": "1264650",
    "end": "1266960"
  },
  {
    "text": "notions of co-occurrence that\nwe're so interested in pursuing",
    "start": "1266960",
    "end": "1269948"
  },
  {
    "text": "with these methods.",
    "start": "1269948",
    "end": "1270740"
  },
  {
    "start": "1270740",
    "end": "1273790"
  },
  {
    "start": "1273000",
    "end": "1324000"
  },
  {
    "text": "Let's close the loop also.",
    "start": "1273790",
    "end": "1274990"
  },
  {
    "text": "We have those central questions.",
    "start": "1274990",
    "end": "1276380"
  },
  {
    "text": "What is GloVe doing to\nour underlying spaces?",
    "start": "1276380",
    "end": "1278380"
  },
  {
    "text": "With GloVe, because of\nthe design of the matrix,",
    "start": "1278380",
    "end": "1280600"
  },
  {
    "text": "we have to begin from word by\nword co-occurrence matrices",
    "start": "1280600",
    "end": "1285610"
  },
  {
    "text": "accounts.",
    "start": "1285610",
    "end": "1286900"
  },
  {
    "text": "So we begin with these\nraw count values.",
    "start": "1286900",
    "end": "1288580"
  },
  {
    "text": "And GloVe is one-stop shopping.",
    "start": "1288580",
    "end": "1290210"
  },
  {
    "text": "It's going to take\nus all the way",
    "start": "1290210",
    "end": "1291585"
  },
  {
    "text": "to these reduced\ndimensional representations.",
    "start": "1291585",
    "end": "1293919"
  },
  {
    "text": "And boy, by the\ncriteria we've set up,",
    "start": "1293920",
    "end": "1296020"
  },
  {
    "text": "does GloVe do an\noutstanding job.",
    "start": "1296020",
    "end": "1297710"
  },
  {
    "text": "This is the result of running\nGloVe at dimension 50.",
    "start": "1297710",
    "end": "1301720"
  },
  {
    "text": "And you can see that the values\nare extremely well scaled",
    "start": "1301720",
    "end": "1304360"
  },
  {
    "text": "between negative 2 and 2 and\nnicely, normally distributed.",
    "start": "1304360",
    "end": "1307840"
  },
  {
    "text": "This is an outstanding input to\nmodern machine learning models.",
    "start": "1307840",
    "end": "1312260"
  },
  {
    "text": "And I think this is probably a\nnon-trivial aspect of why GloVe",
    "start": "1312260",
    "end": "1316180"
  },
  {
    "text": "has been so successful as\na kind of pre-trained basis",
    "start": "1316180",
    "end": "1320020"
  },
  {
    "text": "for a lot of subsequent\nmachine learning architectures.",
    "start": "1320020",
    "end": "1324800"
  },
  {
    "start": "1324000",
    "end": "1417000"
  },
  {
    "text": "And then here's a\nlittle bit of code,",
    "start": "1324800",
    "end": "1326450"
  },
  {
    "text": "just showing you how you can\nwork with these interfaces",
    "start": "1326450",
    "end": "1328880"
  },
  {
    "text": "using our code base.",
    "start": "1328880",
    "end": "1330283"
  },
  {
    "text": "The one thing I\nwanted to call out",
    "start": "1330283",
    "end": "1331700"
  },
  {
    "text": "is that I'm trying\nto be careful.",
    "start": "1331700",
    "end": "1333075"
  },
  {
    "text": "I have defined this\nfunction percentage",
    "start": "1333075",
    "end": "1335269"
  },
  {
    "text": "non-zero values above.",
    "start": "1335270",
    "end": "1337280"
  },
  {
    "text": "And you can set x max\nhere and just feed",
    "start": "1337280",
    "end": "1339500"
  },
  {
    "text": "in a matrix and\nstudy what percentage",
    "start": "1339500",
    "end": "1341840"
  },
  {
    "text": "of the values in\nthat matrix are going",
    "start": "1341840",
    "end": "1343760"
  },
  {
    "text": "to get flattened out to 1\nas a result of the x max",
    "start": "1343760",
    "end": "1346700"
  },
  {
    "text": "that you've chosen.",
    "start": "1346700",
    "end": "1347870"
  },
  {
    "text": "And this value really varies\nby the design of the matrix.",
    "start": "1347870",
    "end": "1351330"
  },
  {
    "text": "If I feed in yelp5, only\nabout 5% of the values",
    "start": "1351330",
    "end": "1355669"
  },
  {
    "text": "are getting flattened out.",
    "start": "1355670",
    "end": "1356760"
  },
  {
    "text": "But if I feed in yelp20,\nwhich is much denser",
    "start": "1356760",
    "end": "1359285"
  },
  {
    "text": "and has much higher\ncounts, 20% of the values",
    "start": "1359285",
    "end": "1362750"
  },
  {
    "text": "are getting flattened out to 1.",
    "start": "1362750",
    "end": "1364820"
  },
  {
    "text": "If this number gets\ntoo high, the matrix",
    "start": "1364820",
    "end": "1366740"
  },
  {
    "text": "might become\ncompletely homogeneous.",
    "start": "1366740",
    "end": "1368510"
  },
  {
    "text": "And so we should really be aware\nof how the setting of x max",
    "start": "1368510",
    "end": "1372530"
  },
  {
    "text": "is affecting the kind of\nlearning that we could even",
    "start": "1372530",
    "end": "1374750"
  },
  {
    "text": "be performing with GloVe.",
    "start": "1374750",
    "end": "1376490"
  },
  {
    "text": "And it might turn out that\nthis is even more important",
    "start": "1376490",
    "end": "1379340"
  },
  {
    "text": "than the number of iterations\nor the dimensionality",
    "start": "1379340",
    "end": "1382220"
  },
  {
    "text": "of the representations\nthat you learn.",
    "start": "1382220",
    "end": "1384272"
  },
  {
    "text": "Once you've made those\nchoices, though, the interface",
    "start": "1384272",
    "end": "1386480"
  },
  {
    "text": "is pretty clear.",
    "start": "1386480",
    "end": "1387200"
  },
  {
    "text": "And the fit method, as\nwith the autoencoder,",
    "start": "1387200",
    "end": "1389870"
  },
  {
    "text": "returns the matrix of\nlearned representations",
    "start": "1389870",
    "end": "1392570"
  },
  {
    "text": "that we want to use for\nthe current purposes.",
    "start": "1392570",
    "end": "1395000"
  },
  {
    "text": "And then finally, I've\nincluded a score method.",
    "start": "1395000",
    "end": "1398150"
  },
  {
    "text": "And the score method\nis literally just",
    "start": "1398150",
    "end": "1399950"
  },
  {
    "text": "testing to see how well the\nvectors that you've learned",
    "start": "1399950",
    "end": "1403490"
  },
  {
    "text": "correspond to the GloVe\nobjective of having the dot",
    "start": "1403490",
    "end": "1405620"
  },
  {
    "text": "products be\nproportional to the log",
    "start": "1405620",
    "end": "1408830"
  },
  {
    "text": "of the co-occurrence\nprobabilities.",
    "start": "1408830",
    "end": "1410419"
  },
  {
    "text": "Since you can get\na score for that,",
    "start": "1410420",
    "end": "1411878"
  },
  {
    "text": "we're doing pretty\nwell here, let's say",
    "start": "1411878",
    "end": "1413510"
  },
  {
    "text": "for a large empirical matrix.",
    "start": "1413510",
    "end": "1414935"
  },
  {
    "start": "1414935",
    "end": "1418080"
  },
  {
    "start": "1417000",
    "end": "1581000"
  },
  {
    "text": "Final section, let's just say\na bit about visualization.",
    "start": "1418080",
    "end": "1420840"
  },
  {
    "text": "And this is a dimensionality\nreduction technique",
    "start": "1420840",
    "end": "1423059"
  },
  {
    "text": "in the sense that\nthe whole point",
    "start": "1423060",
    "end": "1424440"
  },
  {
    "text": "is to try to flatten out a\nvery high dimensional space",
    "start": "1424440",
    "end": "1427139"
  },
  {
    "text": "into possibly two\nor three dimensions.",
    "start": "1427140",
    "end": "1429690"
  },
  {
    "text": "You have to recognize\nthat inevitably this will",
    "start": "1429690",
    "end": "1431940"
  },
  {
    "text": "involve a lot of compromises.",
    "start": "1431940",
    "end": "1433590"
  },
  {
    "text": "It's just impossible to capture\nall the sources of variation",
    "start": "1433590",
    "end": "1437070"
  },
  {
    "text": "in your underlying matrix\nin just a few dimensions.",
    "start": "1437070",
    "end": "1440259"
  },
  {
    "text": "But nonetheless, this\ncan be productive.",
    "start": "1440260",
    "end": "1442710"
  },
  {
    "text": "I think it's especially\nvaluable if you pair it",
    "start": "1442710",
    "end": "1445740"
  },
  {
    "text": "with some kind of hands-on\nqualitative exploration using",
    "start": "1445740",
    "end": "1448860"
  },
  {
    "text": "something like BSN neighbors\nto understand at a low level",
    "start": "1448860",
    "end": "1452429"
  },
  {
    "text": "what your matrix encodes.",
    "start": "1452430",
    "end": "1454020"
  },
  {
    "text": "And then the high\nlevel visualizations",
    "start": "1454020",
    "end": "1455670"
  },
  {
    "text": "can be a kind of\ncounterpart to that.",
    "start": "1455670",
    "end": "1458795"
  },
  {
    "text": "There are many visualization\ntechniques and a lot of them",
    "start": "1458795",
    "end": "1461170"
  },
  {
    "text": "are implemented in the\nscikit manifold package.",
    "start": "1461170",
    "end": "1464890"
  },
  {
    "text": "So I encourage you to use them.",
    "start": "1464890",
    "end": "1466520"
  },
  {
    "text": "I'm going to show you some\nresults from t-SNE which",
    "start": "1466520",
    "end": "1469630"
  },
  {
    "text": "stands for t-distributed\nstochastic neighbor embedding.",
    "start": "1469630",
    "end": "1473557"
  },
  {
    "text": "There are lots of user\nguides there that you",
    "start": "1473557",
    "end": "1475390"
  },
  {
    "text": "can study for more details.",
    "start": "1475390",
    "end": "1476680"
  },
  {
    "text": "Let me just give\nyou the high level.",
    "start": "1476680",
    "end": "1478330"
  },
  {
    "text": "This is t-SNE run on\nour giga20 matrix.",
    "start": "1478330",
    "end": "1481480"
  },
  {
    "text": "I think this is typical of\npretty good output from t-SNE.",
    "start": "1481480",
    "end": "1484429"
  },
  {
    "text": "So what we're seeing here is\nsome pockets of high density.",
    "start": "1484430",
    "end": "1487660"
  },
  {
    "text": "Those are areas of\nlocal coherence.",
    "start": "1487660",
    "end": "1489820"
  },
  {
    "text": "Globally, we should be\ncareful not to over interpret",
    "start": "1489820",
    "end": "1492399"
  },
  {
    "text": "this entire diagram\nbecause as you rerun",
    "start": "1492400",
    "end": "1494890"
  },
  {
    "text": "the model with\ndifferent random seeds,",
    "start": "1494890",
    "end": "1496480"
  },
  {
    "text": "you'll see that it gets kind of\nreoriented in different parts",
    "start": "1496480",
    "end": "1499630"
  },
  {
    "text": "or correspondingly close\nto different other parts.",
    "start": "1499630",
    "end": "1502460"
  },
  {
    "text": "But what you can count\non pretty reliably",
    "start": "1502460",
    "end": "1504760"
  },
  {
    "text": "is that these local\npockets of coherence",
    "start": "1504760",
    "end": "1507610"
  },
  {
    "text": "correspond to coherence parts of\nthe space that you've defined.",
    "start": "1507610",
    "end": "1511640"
  },
  {
    "text": "And if you zoom in on\nthem, you can assess",
    "start": "1511640",
    "end": "1514060"
  },
  {
    "text": "what the model has uncovered.",
    "start": "1514060",
    "end": "1515590"
  },
  {
    "text": "So for this giga21,\nfor example, I",
    "start": "1515590",
    "end": "1517570"
  },
  {
    "text": "think we see prominent clusters\ncorresponding to things",
    "start": "1517570",
    "end": "1519940"
  },
  {
    "text": "like cooking and conflict.",
    "start": "1519940",
    "end": "1522549"
  },
  {
    "text": "If we do the same thing\nfor our Yelp matrix,",
    "start": "1522550",
    "end": "1524950"
  },
  {
    "text": "again, this looks\npretty good in terms",
    "start": "1524950",
    "end": "1527110"
  },
  {
    "text": "of having some substructure\nthat we can analyze.",
    "start": "1527110",
    "end": "1529870"
  },
  {
    "text": "And if we zoom in, we do see\nclusters like positive terms",
    "start": "1529870",
    "end": "1533350"
  },
  {
    "text": "and negative terms corresponding\nto the evaluative setting",
    "start": "1533350",
    "end": "1536620"
  },
  {
    "text": "of these Yelp reviews.",
    "start": "1536620",
    "end": "1537877"
  },
  {
    "text": "So this is all very\nencouraging and suggests",
    "start": "1537877",
    "end": "1539710"
  },
  {
    "text": "that the underlying spaces\nhave some really interesting",
    "start": "1539710",
    "end": "1542380"
  },
  {
    "text": "structure that might be useful\nfor subsequent analysis.",
    "start": "1542380",
    "end": "1547257"
  },
  {
    "text": "And here are some code snippets.",
    "start": "1547257",
    "end": "1548590"
  },
  {
    "text": "We have this simple\nwrapper around",
    "start": "1548590",
    "end": "1550840"
  },
  {
    "text": "the scikit t-SNE\nimplementation that",
    "start": "1550840",
    "end": "1553120"
  },
  {
    "text": "will allow you to flexibly\nwork with this stuff using",
    "start": "1553120",
    "end": "1556270"
  },
  {
    "text": "the account matrices\nfrom our unit.",
    "start": "1556270",
    "end": "1558910"
  },
  {
    "text": "And I'm just mentioning\nhere that it's pretty easy",
    "start": "1558910",
    "end": "1561190"
  },
  {
    "text": "if you want it to color code\nthe words in your vocabulary,",
    "start": "1561190",
    "end": "1563740"
  },
  {
    "text": "say, according to a sentiment\nlexicon or some other kind",
    "start": "1563740",
    "end": "1566559"
  },
  {
    "text": "of lexicon.",
    "start": "1566560",
    "end": "1567610"
  },
  {
    "text": "That could be a way for\nyou to reveal exactly what",
    "start": "1567610",
    "end": "1570040"
  },
  {
    "text": "structure your model has been\nable to uncover with respect",
    "start": "1570040",
    "end": "1572950"
  },
  {
    "text": "to those underlying labels.",
    "start": "1572950",
    "end": "1574299"
  },
  {
    "text": "And that can be useful.",
    "start": "1574300",
    "end": "1576360"
  },
  {
    "start": "1576360",
    "end": "1580246"
  }
]