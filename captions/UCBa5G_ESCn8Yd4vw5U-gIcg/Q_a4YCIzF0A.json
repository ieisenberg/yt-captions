[
  {
    "start": "0",
    "end": "78000"
  },
  {
    "text": "thank you so much for the very kind introduction",
    "start": "10650",
    "end": "18230"
  },
  {
    "text": "one of the main drives during my PhD which has been the question of how do we",
    "start": "23820",
    "end": "29130"
  },
  {
    "text": "keep autonomous systems in particular robotic systems including drones so driving cars home robots and even more",
    "start": "29130",
    "end": "37770"
  },
  {
    "text": "generally AI and automation systems safe how do we keep these systems safe and",
    "start": "37770",
    "end": "42899"
  },
  {
    "text": "especially how do we keep them safe in a world that is increasingly complex where we're deploying these systems when they",
    "start": "42899",
    "end": "48899"
  },
  {
    "text": "have to interact with humans and what does it even mean to have safety assurances for these systems given that",
    "start": "48899",
    "end": "55499"
  },
  {
    "text": "there's so much uncertainty going on in the world and how should this inform our design of robotics so I'm going to be",
    "start": "55499",
    "end": "61640"
  },
  {
    "text": "giving you only some partial answers which is the best I could get during my",
    "start": "61640",
    "end": "67920"
  },
  {
    "text": "PhD and also primarily living it with some open questions so my goal at the end of this talk is that you'll be quite",
    "start": "67920",
    "end": "74130"
  },
  {
    "text": "dissatisfied and hopefully a little intrigued to keep looking into these questions yourselves so as all of you",
    "start": "74130",
    "end": "82470"
  },
  {
    "text": "know robotic systems have really been gaining applications in the last few",
    "start": "82470",
    "end": "88860"
  },
  {
    "text": "years and there's a lot of excitement about still driving cars drones for a variety of applications including",
    "start": "88860",
    "end": "95360"
  },
  {
    "text": "transportation of goods there's actually rhetoric systems that are being deployed",
    "start": "95360",
    "end": "100440"
  },
  {
    "text": "for delivery right now on the Berkeley campus I don't know if you have these here at Stanford they're kind of",
    "start": "100440",
    "end": "105570"
  },
  {
    "text": "ridiculous but they sort of get from A to B it's unclear how autonomous they actually are and people sometimes have",
    "start": "105570",
    "end": "110880"
  },
  {
    "text": "fun kicking them around we're just an interesting human robot interaction problem but also you know applications",
    "start": "110880",
    "end": "116190"
  },
  {
    "text": "like manufacturing and surgery and the truth is that this opens a lot of very",
    "start": "116190",
    "end": "121800"
  },
  {
    "text": "exciting opportunities which is why we want to be able to deploy these systems but at the same time we have a number of",
    "start": "121800",
    "end": "127830"
  },
  {
    "text": "important challenges and one of the most important is how do we keep these systems safe because traditionally when we think about robotic systems as",
    "start": "127830",
    "end": "133920"
  },
  {
    "text": "they've have existed over the last few decades this is how we've gone about safety we just keep the robot in a cage",
    "start": "133920",
    "end": "140340"
  },
  {
    "text": "separated from humans and we slap threatening sticker on it making sure that nobody gets near it or else this is what",
    "start": "140340",
    "end": "146730"
  },
  {
    "text": "will happen to you but you know as we explore the possibilities of robotics",
    "start": "146730",
    "end": "153450"
  },
  {
    "text": "and we start deploying them on more open environments where there is more complexity and more uncertainty it",
    "start": "153450",
    "end": "158700"
  },
  {
    "text": "really becomes very difficult to guarantee that these systems will be able to to actually stay safe this is",
    "start": "158700",
    "end": "167730"
  },
  {
    "text": "video from the year that I spent in industry before starting my PhD and as",
    "start": "167730",
    "end": "172769"
  },
  {
    "text": "you can see we were going about safety with a rather rudimentary method it kind of worked pretty effectively we want to",
    "start": "172769",
    "end": "179310"
  },
  {
    "text": "make sure that the drone wasn't going to fly into the neighbor's house and it did the trick but obviously it wasn't a very",
    "start": "179310",
    "end": "184650"
  },
  {
    "text": "principled method so when I started at Berkley we started you know exploring",
    "start": "184650",
    "end": "190200"
  },
  {
    "text": "exciting applications also with drones and so we had this system that was able",
    "start": "190200",
    "end": "195269"
  },
  {
    "text": "to fly following some safety algorithms but every now and then we had something like this happen which was quite quite",
    "start": "195269",
    "end": "202100"
  },
  {
    "text": "quite terrible if you're a grad student and this happens to you you know that the next couple of weeks are not going to be a lot of fun and really what we",
    "start": "202100",
    "end": "210329"
  },
  {
    "text": "are you know at this point we were really asking okay what can we do to make sure that we can run experiments",
    "start": "210329",
    "end": "215910"
  },
  {
    "text": "for example running reinforcement learning algorithms from our drones without the risk of these things",
    "start": "215910",
    "end": "220950"
  },
  {
    "text": "happening and so we you know went back to the drawing board my first proposal was that we used ropes but that didn't",
    "start": "220950",
    "end": "227070"
  },
  {
    "text": "really fly so we went back to the drawing board and we came up with some theoretical safety guarantees that made sure that whenever the drone came into a",
    "start": "227070",
    "end": "233970"
  },
  {
    "text": "situation where it was theoretically predicted that it could hit the ground or hit the ceiling we would actually",
    "start": "233970",
    "end": "239489"
  },
  {
    "text": "override it in a probably safe manner now the problem with provably safe guarantees and we're going to be talking",
    "start": "239489",
    "end": "245370"
  },
  {
    "text": "a lot about this in the next few minutes is that they're they're only going to",
    "start": "245370",
    "end": "251280"
  },
  {
    "text": "work for as long as the reality that you're dealing with has to do with the model that you're using for it so we",
    "start": "251280",
    "end": "258109"
  },
  {
    "text": "decided to put this to a test by turning on a big fan and starting blowing air sideways how the quadrotor and what you",
    "start": "258109",
    "end": "264450"
  },
  {
    "text": "can see here is that when the system comes back down and flies into this air stream that it's not part of the",
    "start": "264450",
    "end": "270389"
  },
  {
    "text": "theoretical model that we based occurrences on things actually start to go a little bit wrong and eventually",
    "start": "270389",
    "end": "277800"
  },
  {
    "text": "the system is taken so far away from the operating conditions that it was assuming that as you can see the safety",
    "start": "277800",
    "end": "283410"
  },
  {
    "text": "guarantees no longer apply and it actually hits the ground and this is this was only more true when we started",
    "start": "283410",
    "end": "290310"
  },
  {
    "text": "trying to fly drones around humans so here we have a little drone and supposed to be navigating around human",
    "start": "290310",
    "end": "296460"
  },
  {
    "text": "pedestrians here we have our canonical human being Sylvia who is also a grad student at Berkeley and so what we found",
    "start": "296460",
    "end": "304349"
  },
  {
    "text": "was that whenever the human moved in a way that was roughly following the predictions made by the system then everything went fine but when the human",
    "start": "304349",
    "end": "310919"
  },
  {
    "text": "did something unexpected like in this case going around this coffee spill that we drew on the floor the drone gets confused doesn't know",
    "start": "310919",
    "end": "316919"
  },
  {
    "text": "what it's doing initially crashed into Sylvia and of course you can see how upset she is so at the end of the day",
    "start": "316919",
    "end": "322620"
  },
  {
    "text": "what we get from this is that sure we can we can compute theoretical guarantees and this is a good starting",
    "start": "322620",
    "end": "327870"
  },
  {
    "text": "point when we try to give assurances for safety for our systems but ultimately any guarantee that we're able to compute",
    "start": "327870",
    "end": "334380"
  },
  {
    "text": "and to reason about it's only as good as a theoretical model that it is based on and ultimately modeling error is",
    "start": "334380",
    "end": "341280"
  },
  {
    "text": "something that is inevitable whenever you're dealing with a sufficiently complicated system alternately a",
    "start": "341280",
    "end": "347250"
  },
  {
    "text": "sufficiently interesting system that you'd like to do anything with it and menu this is not only a problem that",
    "start": "347250",
    "end": "352620"
  },
  {
    "text": "we're having in a university lab if you look at what's happening these days with the prototypes that are being deployed",
    "start": "352620",
    "end": "358590"
  },
  {
    "text": "of self-driving cars this issue of modeling error is a central problem in",
    "start": "358590",
    "end": "364500"
  },
  {
    "text": "trying to give safety assurances so in these two examples you basically have two cases where the ubirr car and the",
    "start": "364500",
    "end": "370440"
  },
  {
    "text": "Google car at the time were both unable to predict where the human was going to be and what the human was going to do at",
    "start": "370440",
    "end": "377370"
  },
  {
    "text": "a given point in time in the case in the left human driver made a left turn the",
    "start": "377370",
    "end": "383610"
  },
  {
    "text": "robot wasn't even expecting there to be a human there but they're there they were and in the case in the right there",
    "start": "383610",
    "end": "389370"
  },
  {
    "text": "was a bus driver who thought it was their turn to go and the car thought that it was its turn to go on the ended up on the same place at the same time so",
    "start": "389370",
    "end": "396199"
  },
  {
    "text": "really this notion of correctly reasoning about the world and",
    "start": "396199",
    "end": "402870"
  },
  {
    "text": "acknowledging that your models can sometimes be flawed is really important when it comes to giving safety guarantees now this is important and in",
    "start": "402870",
    "end": "410279"
  },
  {
    "start": "407000",
    "end": "595000"
  },
  {
    "text": "part and in fact it's think that it's worrying more and more people as we're starting to see these AI",
    "start": "410279",
    "end": "415590"
  },
  {
    "text": "and robotics technologies get deployed and developed and if you look at declarations made you know statements",
    "start": "415590",
    "end": "421199"
  },
  {
    "text": "made in the last year or two by different governments you see things like human beings will only be able to",
    "start": "421199",
    "end": "427400"
  },
  {
    "text": "confidently and fully reap the benefits of AI systems if they can really trust the technology which was stated by the",
    "start": "427400",
    "end": "433889"
  },
  {
    "text": "European Commission a couple years earlier the United States White House",
    "start": "433889",
    "end": "439190"
  },
  {
    "text": "stated the public safety must be protected as these technologies are tested and begin to mature and only last",
    "start": "439190",
    "end": "446130"
  },
  {
    "text": "year the Deputy Secretary General with Chinese Academy of Sciences said that to truly harvest the benefits of AI we must",
    "start": "446130",
    "end": "452699"
  },
  {
    "text": "first ensure it's a cure-all and reliable deployment so or development and if you look really at the common",
    "start": "452699",
    "end": "458820"
  },
  {
    "text": "factors here you can see that trust safety controllability security reliability are quite pervasive in you",
    "start": "458820",
    "end": "466110"
  },
  {
    "text": "know even government concerns about AI so what I'll try to convince you about in the next few minutes is that if we",
    "start": "466110",
    "end": "472259"
  },
  {
    "text": "want to deploy high-stakes automated systems with these robotic systems for more general AI systems even you know",
    "start": "472259",
    "end": "478229"
  },
  {
    "text": "going as far as your Facebook ad optimizer these systems really should be",
    "start": "478229",
    "end": "484259"
  },
  {
    "text": "able to reason about what kinds of guarantees and assurances they can give and how reliable these guarantees are in",
    "start": "484259",
    "end": "489900"
  },
  {
    "text": "light of the actual evidence that they're getting from the world so how much should I trust my guarantees given",
    "start": "489900",
    "end": "495630"
  },
  {
    "text": "that the world is not identical to my model and so this is really a crucial",
    "start": "495630",
    "end": "501210"
  },
  {
    "text": "distinction that I think we really need to make when thinking about safety assurance it is one thing to give a",
    "start": "501210",
    "end": "507479"
  },
  {
    "text": "guarantee which is ultimately a mathematical a formal statement about an abstraction of the world which is a",
    "start": "507479",
    "end": "513779"
  },
  {
    "text": "model and it's not identical to the world and giving an assurance understood here as a high confidence statement",
    "start": "513779",
    "end": "521459"
  },
  {
    "text": "about the real system which is not the same thing as the model and this can be based on model guarantees you can have a",
    "start": "521459",
    "end": "528089"
  },
  {
    "text": "model you can make some statements about the model but then you really need to breach I need you to not refuse to",
    "start": "528089",
    "end": "535140"
  },
  {
    "text": "bridge this reality gap between the model and the real system and so there's",
    "start": "535140",
    "end": "541440"
  },
  {
    "text": "ways in which we can do this and I will be talking about them today I'm going to be talking first",
    "start": "541440",
    "end": "547230"
  },
  {
    "text": "briefly about what it means to give guarantees about a model of the system so I'll be talking a little bit about",
    "start": "547230",
    "end": "552330"
  },
  {
    "text": "safety analysis and then three different ways in which we can actually try to",
    "start": "552330",
    "end": "557820"
  },
  {
    "text": "give guarantees all right so let's start with safety verification which is how to",
    "start": "557820",
    "end": "563940"
  },
  {
    "text": "give theoretical guarantees on the safer for safe operation of a system model and hereby safe we mean that we won't want",
    "start": "563940",
    "end": "570360"
  },
  {
    "text": "to make sure that we can avoid a certain set of forbidden failure states what is",
    "start": "570360",
    "end": "576000"
  },
  {
    "text": "a failure state it can be anything from an actual physical collision to the robot violating some rule that we've",
    "start": "576000",
    "end": "582270"
  },
  {
    "text": "specified for it for example we might decide that it's already a safety violation for a self-driving car to be",
    "start": "582270",
    "end": "588240"
  },
  {
    "text": "driving in the wrong direction on the road so the way we formalize this is",
    "start": "588240",
    "end": "597390"
  },
  {
    "start": "595000",
    "end": "813000"
  },
  {
    "text": "through dynamical systems theory so we think of the state of the system which can be the robot but also other",
    "start": "597390",
    "end": "603900"
  },
  {
    "text": "variables in the world like for example the positions of other vehicles or other agents and we reason about the evolution",
    "start": "603900",
    "end": "609840"
  },
  {
    "text": "of this state with some differential equation and what we say is the evolution of the state can be affected",
    "start": "609840",
    "end": "615600"
  },
  {
    "text": "it can depend on the state itself what is already going on but also on some decisions that we make the control input",
    "start": "615600",
    "end": "621810"
  },
  {
    "text": "and possibly on some other external variables that we don't control and this disturbance term can capture anything",
    "start": "621810",
    "end": "628470"
  },
  {
    "text": "from the actual actions of other agents in the environment to in fact any form",
    "start": "628470",
    "end": "633690"
  },
  {
    "text": "of modeling error so if we're not sure about what really is going to happen we can capture this disturbance we can",
    "start": "633690",
    "end": "639870"
  },
  {
    "text": "capture this uncertainty about some deformation disturbance input that can",
    "start": "639870",
    "end": "645120"
  },
  {
    "text": "really affect the evolution of the dynamics so under those conditions we can specify a set of forbidden or",
    "start": "645120",
    "end": "652110"
  },
  {
    "text": "failure States and reason about whether the trajectory of the system as it also over time will",
    "start": "652110",
    "end": "658410"
  },
  {
    "text": "at all times stay clear of these obstacles now unfortunately what this",
    "start": "658410",
    "end": "663990"
  },
  {
    "text": "trajectory does or in some cases fortunately will depend on the decisions that we make over time therefore we're",
    "start": "663990",
    "end": "670950"
  },
  {
    "text": "not really just asking well the system always be safe but rather is there something that we can do with our",
    "start": "670950",
    "end": "676410"
  },
  {
    "text": "control in put to keep the system safe at all times here in general throughout the talk I'm",
    "start": "676410",
    "end": "682829"
  },
  {
    "text": "going to be using boldface to refer to trajectories or signals over time and regular font to mean the instantaneous",
    "start": "682829",
    "end": "689339"
  },
  {
    "text": "value of a certain control like control action or state over time one way of",
    "start": "689339",
    "end": "695759"
  },
  {
    "text": "which we we can capture whether or not the system is violating the constraints is by using some form of metric for",
    "start": "695759",
    "end": "701339"
  },
  {
    "text": "example some distance some sign distance that is positive when you're outside or the failure states and negative when",
    "start": "701339",
    "end": "707160"
  },
  {
    "text": "you're inside it'll be all the more negative the more deeply you violate the constraints all the more positive the",
    "start": "707160",
    "end": "712649"
  },
  {
    "text": "more margin you have and so we can reason for these two trajectories about the evolution of this distance over time",
    "start": "712649",
    "end": "718800"
  },
  {
    "text": "or this margin and the first thing to notice is that taking an average of this",
    "start": "718800",
    "end": "724199"
  },
  {
    "text": "margin or any sort of sum it's actually inadequate if you look at the integral of these two curves in both cases the",
    "start": "724199",
    "end": "730139"
  },
  {
    "text": "integral is positive however in one of the cases the system is really violating the constraints this is an important",
    "start": "730139",
    "end": "735300"
  },
  {
    "text": "point to make because a lot of the optimal control formulations that are out there including reinforcement",
    "start": "735300",
    "end": "741000"
  },
  {
    "text": "learning tend to reason about the sum of rewards over time which is really inadequate to capture something like the",
    "start": "741000",
    "end": "747300"
  },
  {
    "text": "worst case the minimum reached by the function over time which is really what we care about when we ask questions",
    "start": "747300",
    "end": "753240"
  },
  {
    "text": "about safety so instead we'd like to reason about the minimum of L reached",
    "start": "753240",
    "end": "759240"
  },
  {
    "text": "over time and this is often called a reach ability problem where ultimately what we have is the minimum or the",
    "start": "759240",
    "end": "764699"
  },
  {
    "text": "infimum of the signal over time and long story short we can reason about this and",
    "start": "764699",
    "end": "772230"
  },
  {
    "text": "we can obtain it through dynamic programming for example propagating things backward in time making the",
    "start": "772230",
    "end": "777360"
  },
  {
    "text": "optimal decision at every instant in time I'm not going to get into all the technical details of this but this is generally stuff that you can find also",
    "start": "777360",
    "end": "783959"
  },
  {
    "text": "feel free to stick around at the end ask me for references but if we do this for the top trajectory we see that this is",
    "start": "783959",
    "end": "789750"
  },
  {
    "text": "okay it's a safe trajectory because the minimum over time it's positive if we propagate in a similar way through the",
    "start": "789750",
    "end": "797009"
  },
  {
    "text": "second trajectory we see that at some point it becomes negative we keep this minimum value as we propagate the value",
    "start": "797009",
    "end": "803040"
  },
  {
    "text": "function to the present and we see that this is an unsafe trajectory because we will eventually hit the",
    "start": "803040",
    "end": "809840"
  },
  {
    "text": "the unsafe or sorry the failure states okay so that's the trajectory that we don't want the way this actually works",
    "start": "809840",
    "end": "816800"
  },
  {
    "start": "813000",
    "end": "1276000"
  },
  {
    "text": "when we go from individual trajectories to the full analysis over the state space is what's called hamilton-jacobi",
    "start": "816800",
    "end": "824150"
  },
  {
    "text": "safety analysis and we can visualize it here suppose this is our much smaller failure set we're going to now look at",
    "start": "824150",
    "end": "831920"
  },
  {
    "text": "it kind of in 3d we're going to have the initial distance function which is a cone in this case we're using euclidean",
    "start": "831920",
    "end": "837170"
  },
  {
    "text": "distance to the failure disk and then we can propagate the dynamics of the system backwards in time making the optimal",
    "start": "837170",
    "end": "844160"
  },
  {
    "text": "decisions and also considering the worst case of the disturbance so that we're protecting not only about some nominal",
    "start": "844160",
    "end": "850970"
  },
  {
    "text": "model but against all possible realizations of our uncertainty and when",
    "start": "850970",
    "end": "856520"
  },
  {
    "text": "you end up with this propagation you can see that the value function has sort of lift out of the original cone and now",
    "start": "856520",
    "end": "864140"
  },
  {
    "text": "we're left with this boundary which is what separates safe States from unsafe",
    "start": "864140",
    "end": "870230"
  },
  {
    "text": "States and so we have this important notion of the safe set which is a set of states from which the controller can",
    "start": "870230",
    "end": "876170"
  },
  {
    "text": "take some action that will keep the system from entering the cone sorry from entering in this case the disc for all",
    "start": "876170",
    "end": "882140"
  },
  {
    "text": "time if you're here if you're inside you already have a negative value which means that even under your best effort there might be a realization of the",
    "start": "882140",
    "end": "888830"
  },
  {
    "text": "system that will actually drag you into the cone so this this safety analysis is",
    "start": "888830",
    "end": "895550"
  },
  {
    "text": "actually very useful because it allows us to obtain really the mathematically best effort that we could possibly make",
    "start": "895550",
    "end": "903110"
  },
  {
    "text": "to keep the system safe which means that it can often reveal some non-intuitive strategies to keep safety that might not",
    "start": "903110",
    "end": "910820"
  },
  {
    "text": "be directly achievable by just having some engineers sit down and hard code them into the system so whenever we can",
    "start": "910820",
    "end": "917870"
  },
  {
    "text": "compute these solutions it's actually extremely helpful and an example of this was a project that we did with NASA a",
    "start": "917870",
    "end": "922910"
  },
  {
    "text": "few years ago where they wanted to be able to have essentially an air traffic management system that would scale",
    "start": "922910",
    "end": "929000"
  },
  {
    "text": "better than just having human air-traffic controllers so that we could",
    "start": "929000",
    "end": "934550"
  },
  {
    "text": "have in the order of hundreds thousands or tens of thousands of vehicles flying in the same airspace for example over",
    "start": "934550",
    "end": "940430"
  },
  {
    "text": "the San Francisco Bay Area at the same time and the way we went about this was to",
    "start": "940430",
    "end": "945500"
  },
  {
    "text": "break down the system into sequential planning for all the trajectories",
    "start": "945500",
    "end": "951259"
  },
  {
    "text": "because in fact NASA was going to have a first-come-first-served system so what they wanted to do was have the first",
    "start": "951259",
    "end": "956420"
  },
  {
    "text": "vehicle obtain the same trajectory then the same tradition vehicle come in request a trajectory and obtained",
    "start": "956420",
    "end": "962360"
  },
  {
    "text": "something I would be saved without perturbing the trajectory already given to the first vehicle and so on and so forth",
    "start": "962360",
    "end": "967519"
  },
  {
    "text": "so what we were able to do was we actually developed a new theoretical tools for time varying safety analysis",
    "start": "967519",
    "end": "974720"
  },
  {
    "text": "and ultimately what this did was we first took the first vehicle which was",
    "start": "974720",
    "end": "979790"
  },
  {
    "text": "the highest priority vehicle and we gave it a trajectory by computing the safety",
    "start": "979790",
    "end": "984949"
  },
  {
    "text": "analysis here we see the safely reachable set that we were talking about earlier and then the first vehicle becomes a moving obstacle for the second",
    "start": "984949",
    "end": "992629"
  },
  {
    "text": "vehicle and you see that it cuts into the backward time propagation of the second vehicle safe set and the same",
    "start": "992629",
    "end": "998329"
  },
  {
    "text": "thing happens here for the first two vehicles becoming obstacles for the third and the first three vehicles becoming obstacles for the fourth so",
    "start": "998329",
    "end": "1004990"
  },
  {
    "text": "when you run the safety computations the advantage is that you can actually run each one in the state space of a single",
    "start": "1004990",
    "end": "1011709"
  },
  {
    "text": "vehicle so that you don't get this combinatorial blow-up that you would normally get if you try to do all of the",
    "start": "1011709",
    "end": "1017920"
  },
  {
    "text": "vehicles at the same time and it turns out that this scales linearly with the number of vehicles because each time",
    "start": "1017920",
    "end": "1023620"
  },
  {
    "text": "you're only doing computation in the space of that single vehicle and also it",
    "start": "1023620",
    "end": "1028870"
  },
  {
    "text": "gives you strict safety guarantees as long as things are according to the model and it also gives you the optimal",
    "start": "1028870",
    "end": "1038678"
  },
  {
    "text": "trajectories for these vehicles in the sense of being the shortest the quickest trajectories to get from A to B subject",
    "start": "1038679",
    "end": "1044470"
  },
  {
    "text": "to the priority ordering which is what NASA is giving us so this is for for",
    "start": "1044470",
    "end": "1049870"
  },
  {
    "text": "vehicles but you don't have to take my word for it that it works here it is in a simulation with 50 vehicles flying",
    "start": "1049870",
    "end": "1057460"
  },
  {
    "text": "over San Francisco we have another one with 200 vehicles over the Bay Area but I think it's actually less interesting what you can actually see here is",
    "start": "1057460",
    "end": "1064409"
  },
  {
    "text": "vehicles flying with different wind conditions and different timing conditions and you see that all of them",
    "start": "1064409",
    "end": "1070840"
  },
  {
    "text": "are actually able to compute their trajectories and and complete them without coming into collisions with one",
    "start": "1070840",
    "end": "1076059"
  },
  {
    "text": "another's with one another the wind speed is important because while we are taking this as a",
    "start": "1076059",
    "end": "1081669"
  },
  {
    "text": "theoretical safety guarantee in fact this is something that will apply as long as your error the error between the",
    "start": "1081669",
    "end": "1089350"
  },
  {
    "text": "model and the real system is bounded by a difference in your in your evolution",
    "start": "1089350",
    "end": "1095500"
  },
  {
    "text": "of the of the position of the vehicle that can be interpreted as wind speed of",
    "start": "1095500",
    "end": "1100929"
  },
  {
    "text": "up to 11 meters per second so this doesn't necessarily mean it has to be exactly one speed it could also be other",
    "start": "1100929",
    "end": "1106330"
  },
  {
    "text": "things like Gus it could be capturing the delay in your system when it tries to make a term but it actually takes a",
    "start": "1106330",
    "end": "1111460"
  },
  {
    "text": "while as long as it can be captured under virtual wind speed of 11 meters per second the guarantee actually",
    "start": "1111460",
    "end": "1117970"
  },
  {
    "text": "applies to the physical system and so this is a nice segue into the next part",
    "start": "1117970",
    "end": "1123970"
  },
  {
    "text": "of the talk which is okay so we have this your article guarantee how do we how do we make sure that it actually",
    "start": "1123970",
    "end": "1130090"
  },
  {
    "text": "applies to the physical system or to what extent can we make sure that it applies to the physical system you can",
    "start": "1130090",
    "end": "1136059"
  },
  {
    "text": "argue that in fact there is no way to make 100% sure because fundamentally and model and reality are two different",
    "start": "1136059",
    "end": "1142870"
  },
  {
    "text": "things the model is a mathematical object and then reality is something made of atoms about which we can't really write theoretical statements we",
    "start": "1142870",
    "end": "1150340"
  },
  {
    "text": "can make an abstraction of the reality and then make theoretical statements about that the reality is not it's a type error right you can't have",
    "start": "1150340",
    "end": "1156960"
  },
  {
    "text": "theoretical guarantee about the physical world all right but we can't do things",
    "start": "1156960",
    "end": "1164529"
  },
  {
    "text": "so one of the first things that we wanted to do was see if we could apply some learning based methods to to our",
    "start": "1164529",
    "end": "1171610"
  },
  {
    "text": "robotic systems and this is something that is becoming more and more attractive because learning based methods like reinforcement learning you",
    "start": "1171610",
    "end": "1178029"
  },
  {
    "text": "can see a couple of examples over here are becoming are being proven to be very",
    "start": "1178029",
    "end": "1183730"
  },
  {
    "text": "powerful at planning over long horizons with complex dynamics multi-agent systems at this is alpha star that was",
    "start": "1183730",
    "end": "1190110"
  },
  {
    "text": "released by a deep mind or at least published by the mind very recently and",
    "start": "1190110",
    "end": "1197010"
  },
  {
    "text": "ultimately what all of these reinforcement learning methods are doing is they are finding in inferring some",
    "start": "1197010",
    "end": "1203230"
  },
  {
    "text": "structure about the dynamics of the system and then computing good first effort approximations to the solution of",
    "start": "1203230",
    "end": "1209799"
  },
  {
    "text": "an optimal control problem he you see humanoid which is a high dimensional dynamical system and it's",
    "start": "1209799",
    "end": "1216510"
  },
  {
    "text": "actually performing fairly well you know over the irregular terrain jumping around I've seen pretty well given that",
    "start": "1216510",
    "end": "1223190"
  },
  {
    "text": "it's not explicitly using a model of the world however the problem with these systems is that they are good at average",
    "start": "1223190",
    "end": "1228299"
  },
  {
    "text": "performance but sometimes they have pretty pretty terrible worst-case",
    "start": "1228299",
    "end": "1233429"
  },
  {
    "text": "performance and it's very difficult to come up with any sort of guarantees as to when something like that is going to",
    "start": "1233429",
    "end": "1239580"
  },
  {
    "text": "happen and so the thing that we wanted to do is sort of get over this this this",
    "start": "1239580",
    "end": "1246779"
  },
  {
    "text": "limitation and apply these kinds of techniques to systems that can break and",
    "start": "1246779",
    "end": "1252269"
  },
  {
    "text": "cause damage like robotic systems so here we have the issue that we were",
    "start": "1252269",
    "end": "1257370"
  },
  {
    "text": "talking about earlier which is if we only reason and optimize an average performance we're not really capturing worst-case worst-case outcomes so how do",
    "start": "1257370",
    "end": "1266639"
  },
  {
    "text": "we go and apply these kinds of systems or these types of techniques or to robotic systems where a worst-case",
    "start": "1266639",
    "end": "1273090"
  },
  {
    "text": "outcome can actually be quite costly well we can actually use the safety analysis and the structure that comes",
    "start": "1273090",
    "end": "1280200"
  },
  {
    "start": "1276000",
    "end": "1367000"
  },
  {
    "text": "out of the safety now this is to provide some very nice properties around the learning system so it turns out that you",
    "start": "1280200",
    "end": "1286950"
  },
  {
    "text": "can use the safety analysis to provide what's called a safety envelope that will that will basically treat the safe",
    "start": "1286950",
    "end": "1293399"
  },
  {
    "text": "set as a safety bubble within which you can do whatever you want and then when you reach the boundary of the safety",
    "start": "1293399",
    "end": "1298409"
  },
  {
    "text": "bubble you'll need to take the safety action to stay inside of it so here's how it works here we have our robotic",
    "start": "1298409",
    "end": "1304889"
  },
  {
    "text": "system that's the ceiling the floor and I'm plotting vertical position on the vertical axis and then velocity on the",
    "start": "1304889",
    "end": "1311460"
  },
  {
    "text": "horizontal axis so basically the more to the left you are the faster you're moving down the more to the right you are the faster you're moving up and so",
    "start": "1311460",
    "end": "1318779"
  },
  {
    "text": "in this picture with this relatively simple dynamics model where you are controlling the acceleration with your",
    "start": "1318779",
    "end": "1325080"
  },
  {
    "text": "rotor thrust you can actually compute these sort of parabolic curves that tell",
    "start": "1325080",
    "end": "1330990"
  },
  {
    "text": "you that you're unsafe if you're near the ceiling and moving up very fast and also if you're near the floor moving down very fast and you're safe if you're",
    "start": "1330990",
    "end": "1338190"
  },
  {
    "text": "somewhere in the middle well the next hearing that we have as I was saying informally is that if you start in n",
    "start": "1338190",
    "end": "1344669"
  },
  {
    "text": "stayed inside of the safe set then you actually have a control invariant set",
    "start": "1344669",
    "end": "1350279"
  },
  {
    "text": "when you get to the boundary you're applying the safe action you do whatever you want on the inside you get to the boundary again and you apply the safe action and that is guaranteed to push",
    "start": "1350279",
    "end": "1356429"
  },
  {
    "text": "you back in as long as the amount of mortal error that you have is contained",
    "start": "1356429",
    "end": "1362100"
  },
  {
    "text": "by some initial worst case bound that you had pre computed or pre estimated so",
    "start": "1362100",
    "end": "1368070"
  },
  {
    "text": "the way this works in practice is we have for example the autonomous quadrotor here flying with some vikon",
    "start": "1368070",
    "end": "1374249"
  },
  {
    "text": "system and we're going to try to do some simple reinforcement learning algorithm but here the catch is that we're going",
    "start": "1374249",
    "end": "1379830"
  },
  {
    "text": "to initialize all the feature weights to zero so what happens here is that we're going to start with a really really bad",
    "start": "1379830",
    "end": "1385139"
  },
  {
    "text": "guess as to how to fly the quadrotor now normally if we just let the quadrotor",
    "start": "1385139",
    "end": "1391919"
  },
  {
    "text": "fly under these conditions what is going to do is it's going to crash into the ground fortunately this is know what",
    "start": "1391919",
    "end": "1397200"
  },
  {
    "text": "happens because we have the quadrotor inside of the safety envelope so whenever the learning algorithm tries to crash the quadrotor the safety override",
    "start": "1397200",
    "end": "1404009"
  },
  {
    "text": "says no you don't and takes over and so we're first for the first few seconds you can see that the learning algorithm",
    "start": "1404009",
    "end": "1409710"
  },
  {
    "text": "doesn't have very good ideas but after about 30 seconds or so it starts figuring out what it needs to do the fly",
    "start": "1409710",
    "end": "1415200"
  },
  {
    "text": "and it ends up flying up and down following this trajectory reference that we were giving it so here the remarkable thing is know",
    "start": "1415200",
    "end": "1421649"
  },
  {
    "text": "that we're beating performance and learning systems try anything like that but rather that we're able to do this",
    "start": "1421649",
    "end": "1426899"
  },
  {
    "text": "with a system with a learning algorithm that initially had a really really poor idea of what to do and still we never",
    "start": "1426899",
    "end": "1434039"
  },
  {
    "text": "crashed we never ended up having to end the experiment because the court culture broke now of course the limitation here",
    "start": "1434039",
    "end": "1442559"
  },
  {
    "text": "is what happens when you add the fan what happens when your model guarantees are actually not accurate anymore and so",
    "start": "1442559",
    "end": "1448169"
  },
  {
    "text": "we are really back to this picture of the mind of the mind the gap we have some guarantees based on Newton's second",
    "start": "1448169",
    "end": "1454950"
  },
  {
    "text": "law and first principles models we have the safety envelope protection but we're not really accounting for for example",
    "start": "1454950",
    "end": "1460710"
  },
  {
    "text": "strong wind coupling between the vertical and the lateral dynamics or any other form of external perturbations",
    "start": "1460710",
    "end": "1465989"
  },
  {
    "text": "that we that we just forgot to model right like for example the ground effect so the way we are one way in which we",
    "start": "1465989",
    "end": "1474029"
  },
  {
    "start": "1471000",
    "end": "1700000"
  },
  {
    "text": "can get around this is it turns out that the structure of safety analysis gives us much more than just this one",
    "start": "1474029",
    "end": "1480300"
  },
  {
    "text": "layer of protection in fact it turns out that we can replace a zero here by any",
    "start": "1480300",
    "end": "1486090"
  },
  {
    "text": "alpha greater than zero and it turns out that the safety policy renders any level",
    "start": "1486090",
    "end": "1492420"
  },
  {
    "text": "set of the value function that is above zero so any of these nested sets controlled invariant which means that",
    "start": "1492420",
    "end": "1499800"
  },
  {
    "text": "rather than just a safety set or a safety bubble what we have can be",
    "start": "1499800",
    "end": "1504810"
  },
  {
    "text": "thought of as a safety onion so what we can do is we can use safety analysis to",
    "start": "1504810",
    "end": "1512480"
  },
  {
    "text": "compute the value function and therefore all of the layers of this onion and then use the data that we obtain as we are",
    "start": "1512480",
    "end": "1519900"
  },
  {
    "text": "exploring the world to gauge how much we should trust the system and at what point we're at a at a point where it",
    "start": "1519900",
    "end": "1526890"
  },
  {
    "text": "might be that outer layers of the onion won't actually be trustworthy because the model might be wrong at the outer",
    "start": "1526890",
    "end": "1533160"
  },
  {
    "text": "layers as long as the model is right at a single layer we can use that layer to protect ourselves and so basically this",
    "start": "1533160",
    "end": "1540150"
  },
  {
    "text": "is the approach that we follow I won't get into the details of how we reason about you know when the model is right",
    "start": "1540150",
    "end": "1546840"
  },
  {
    "text": "or wrong there's a variety of models that you can use for that you can use the calcium process you can use some other form of anomaly detection but what",
    "start": "1546840",
    "end": "1552990"
  },
  {
    "text": "matters here is that when the quadrotor comes down there's the same video that you saw before but kind of zoomed out it",
    "start": "1552990",
    "end": "1560040"
  },
  {
    "text": "actually realizes that the model it's actually not trustworthy so you're gonna see two possible futures the the one",
    "start": "1560040",
    "end": "1567210"
  },
  {
    "text": "that's ghosted is the one that you saw initially where the system is blindly trusting its model-based guarantees the",
    "start": "1567210",
    "end": "1574290"
  },
  {
    "text": "second one you can see that very quickly it realized that its model was not trustworthy so it's saying I'm not gonna",
    "start": "1574290",
    "end": "1580530"
  },
  {
    "text": "use these layers of the onion because I don't trust them I'm going to stay at this height and I won't go down until",
    "start": "1580530",
    "end": "1585930"
  },
  {
    "text": "and unless I start getting new data that tells me that it's actually safe to fly in that region or it leaves that my",
    "start": "1585930",
    "end": "1592290"
  },
  {
    "text": "model is sufficiently accurate in that region that I can trust my safety analysis and so let me just say for time",
    "start": "1592290",
    "end": "1599910"
  },
  {
    "text": "okay very good and so the reason that this is interesting and that I think",
    "start": "1599910",
    "end": "1605100"
  },
  {
    "text": "it's a good technique to have in general is that it doesn't matter how good our models are at the end of the day we're",
    "start": "1605100",
    "end": "1611430"
  },
  {
    "text": "always going to some discrepancy between the model and reality and any safety guarantee that we",
    "start": "1611430",
    "end": "1616860"
  },
  {
    "text": "have that is strictly based on the model can eventually be broken by reality and the real world is really good at making",
    "start": "1616860",
    "end": "1622920"
  },
  {
    "text": "fun of us and our best efforts so one thing that we can do to make the system a bit more resilient is to have it",
    "start": "1622920",
    "end": "1628710"
  },
  {
    "text": "actively monitor this reality gap between its model and its mobile base guarantees and what's actually going on",
    "start": "1628710",
    "end": "1636000"
  },
  {
    "text": "in the world and it turns out and this is also important that we don't need the model to be correct everywhere we just",
    "start": "1636000",
    "end": "1641400"
  },
  {
    "text": "need it to be sufficiently correct for our guarantees to apply in an intelligent way so if we have the system",
    "start": "1641400",
    "end": "1647730"
  },
  {
    "text": "plan incorporating the possibility that its model might be wrong either now or",
    "start": "1647730",
    "end": "1653340"
  },
  {
    "text": "in the future then it automatically becomes much more robust to model error",
    "start": "1653340",
    "end": "1658350"
  },
  {
    "text": "and so this is a general principle that can be exploited in different ways and so what I'm going to show you now is how",
    "start": "1658350",
    "end": "1664920"
  },
  {
    "text": "could we apply this to in particular the interaction between robots and people sure you could take people and say okay",
    "start": "1664920",
    "end": "1671040"
  },
  {
    "text": "the person has a disturbance I don't know what they're gonna do I'm gonna do some worst-case analysis where the person could go in any direction and you",
    "start": "1671040",
    "end": "1676740"
  },
  {
    "text": "would get something probably you would get overly conservative behavior and it turns out that people while they are",
    "start": "1676740",
    "end": "1682530"
  },
  {
    "text": "extremely difficult to model accurately actually follow very structured behavior a lot of the time and cognitive science",
    "start": "1682530",
    "end": "1689460"
  },
  {
    "text": "over the last few years has given us very powerful tools with which we can attempt to model with a reasonable",
    "start": "1689460",
    "end": "1695429"
  },
  {
    "text": "degree of accuracy what people might or might not do in certain contexts now the",
    "start": "1695429",
    "end": "1700679"
  },
  {
    "start": "1700000",
    "end": "1914000"
  },
  {
    "text": "interaction between humans and robots is actually very tricky and historically there's been cases where it's been great",
    "start": "1700679",
    "end": "1707429"
  },
  {
    "text": "to have the human and cases where it's been quite tragic to have the human so",
    "start": "1707429",
    "end": "1714210"
  },
  {
    "text": "it's to me interesting that both of these accidents happen in 2009 there's",
    "start": "1714210",
    "end": "1719460"
  },
  {
    "text": "one that's usually known as The Miracle on the Hudson and now it has a movie about it that some of you might have",
    "start": "1719460",
    "end": "1724620"
  },
  {
    "text": "seen it's called Sully it's got Tom Hanks in it playing the the main pilot and ultimately what happened was that",
    "start": "1724620",
    "end": "1729960"
  },
  {
    "text": "the plane took off and there's something that we can talk a little bit more about in",
    "start": "1729960",
    "end": "1735570"
  },
  {
    "text": "maybe in the discussion after the talk and something very unusual happened which was that both of the engines got",
    "start": "1735570",
    "end": "1741950"
  },
  {
    "text": "taken down by Bert strikes now when we design an airplane we often assume that",
    "start": "1741950",
    "end": "1747359"
  },
  {
    "text": "it's extremely unlikely that both engines are going to get taken down it turns out that if the bird strike you",
    "start": "1747359",
    "end": "1752700"
  },
  {
    "text": "know if there's a sufficiently large group of birds it's not impossible that both of your engines will get taken down",
    "start": "1752700",
    "end": "1758009"
  },
  {
    "text": "and this is exactly what happened thanks to their being a human pilot here human judgment was the the key the key",
    "start": "1758009",
    "end": "1764969"
  },
  {
    "text": "factor in realizing that none of the nearby airports were safely reachable but you could safely land in the river",
    "start": "1764969",
    "end": "1771419"
  },
  {
    "text": "and so this is what the pilot did and everyone on board got saved in the same year unfortunately there was a tragic",
    "start": "1771419",
    "end": "1777779"
  },
  {
    "text": "case of the interaction between the human autumn and the automation leading to a crash this was an Air France Air",
    "start": "1777779",
    "end": "1785039"
  },
  {
    "text": "France flight that was flying from Brazil to France over the Atlantic a fairly non critical failure happened",
    "start": "1785039",
    "end": "1791789"
  },
  {
    "text": "where the pitot tubes was which measure the airspeed of the aircraft got frozen and so the auto pilot said hey I'm",
    "start": "1791789",
    "end": "1797789"
  },
  {
    "text": "confused I don't know what's happening you take over and then the pilots were very confused as to what was happening",
    "start": "1797789",
    "end": "1804359"
  },
  {
    "text": "the error message was extremely long they thought that the plane might be stalling and that the speed was very low",
    "start": "1804359",
    "end": "1810809"
  },
  {
    "text": "because the pitot tubes were frozen and therefore reported reporting very low speeds so they actually started speeding",
    "start": "1810809",
    "end": "1816809"
  },
  {
    "text": "up they increased a Siraj the plane started climbing and eventually it entered a stole and then the stall",
    "start": "1816809",
    "end": "1823169"
  },
  {
    "text": "warning went out but the pilots weren't trusting the stall warning because at this point they weren't trusting the sensors and eventually three minutes",
    "start": "1823169",
    "end": "1828899"
  },
  {
    "text": "after the original failure the year the aircraft crashed after what was",
    "start": "1828899",
    "end": "1834239"
  },
  {
    "text": "initially a pretty trivial failure of the pilots had just kept flying they would have reached Paris safely so here",
    "start": "1834239",
    "end": "1841440"
  },
  {
    "text": "the you know the lesson is that we really have to be careful about how we design the automation because it should",
    "start": "1841440",
    "end": "1848669"
  },
  {
    "text": "really be accounting for how humans are going to interact with it and this can often be very hard to model I don't know",
    "start": "1848669",
    "end": "1855719"
  },
  {
    "text": "if you have been following the case of the Boeing 330 so yeah 737 max but it's",
    "start": "1855719",
    "end": "1863219"
  },
  {
    "text": "been it's been possibly the the most",
    "start": "1863219",
    "end": "1868800"
  },
  {
    "text": "you know the the biggest crisis that Boeing has gone through at least in recent decades and it has to do with",
    "start": "1868800",
    "end": "1874679"
  },
  {
    "text": "precisely a safety system interacting in the wrong way with the crew making wrong",
    "start": "1874679",
    "end": "1882780"
  },
  {
    "text": "decisions but then making wrong assumptions about how the crew was going to respond to it and it's you know",
    "start": "1882780",
    "end": "1887790"
  },
  {
    "text": "already cost hundreds of lives fortunately these planes are now grounded so it's you know you can fly safely but it's really something that is",
    "start": "1887790",
    "end": "1895200"
  },
  {
    "text": "it's getting Boeing to really rethink how they're going about their safety analysis so anyway I want to show you",
    "start": "1895200",
    "end": "1902640"
  },
  {
    "text": "this little example this project that we ran where we had this drone flying around the human and what is at least",
    "start": "1902640",
    "end": "1908550"
  },
  {
    "text": "one possible way in which you can reason about safety accounting for how much you should trust your mall of the of the human so in the last few years there's",
    "start": "1908550",
    "end": "1916350"
  },
  {
    "start": "1914000",
    "end": "2154000"
  },
  {
    "text": "been and in fact in the last few decades there's been a model that's been very successful and very useful of for",
    "start": "1916350",
    "end": "1922679"
  },
  {
    "text": "predicting human actions which is what's called a noisy rational human model or a soft knacks model sometimes people call",
    "start": "1922679",
    "end": "1928200"
  },
  {
    "text": "it the loose choice rule because it was introduced in originally by mathematical",
    "start": "1928200",
    "end": "1933929"
  },
  {
    "text": "psychology and econometrics in the 50s and it's really found a lot of useful applications in robotics and in",
    "start": "1933929",
    "end": "1940320"
  },
  {
    "text": "artificial intelligence and so what this model does is it says well I don't know exactly what the human is going to do at",
    "start": "1940320",
    "end": "1945360"
  },
  {
    "text": "any given point in time but what I can tell you is that there are certain things that are more likely than others and how the robot reason is about the",
    "start": "1945360",
    "end": "1951750"
  },
  {
    "text": "likelihood of different actions is by looking at how efficient each action",
    "start": "1951750",
    "end": "1957179"
  },
  {
    "text": "seems to be for the certain thing that the human might want to do so the robot has some utility based model of the",
    "start": "1957179",
    "end": "1965640"
  },
  {
    "text": "human objectives and so it says if it looks like the human for example is trying to walk to the door here we have",
    "start": "1965640",
    "end": "1970770"
  },
  {
    "text": "theta parameterizing what the human might want to do say that Thea is leave the room then the human is more likely",
    "start": "1970770",
    "end": "1976410"
  },
  {
    "text": "to walk towards the door a little less likely to walk to walk indirectly towards the door and very unlikely to",
    "start": "1976410",
    "end": "1981780"
  },
  {
    "text": "walk in the opposite direction so these models work fairly well and the robot for example can use them to navigate",
    "start": "1981780",
    "end": "1987300"
  },
  {
    "text": "around the human now the problem comes when the human starts walking towards the door and then suddenly something",
    "start": "1987300",
    "end": "1992820"
  },
  {
    "text": "happens that was no part of the model at all in the first place for example Herbie flies into the room the robot",
    "start": "1992820",
    "end": "1998040"
  },
  {
    "text": "probably can't even sense the presence of the bee but if it could it probably doesn't really understand the very peculiar relationship that",
    "start": "1998040",
    "end": "2003739"
  },
  {
    "text": "people have with bees and therefore this is a completely unexpected",
    "start": "2003739",
    "end": "2008960"
  },
  {
    "text": "you know reaction of the human and so what can we do given that people will sometimes do things that are unexpected",
    "start": "2008960",
    "end": "2014509"
  },
  {
    "text": "and they might violate all our assumptions about them well we did a little experiment in the lab it's",
    "start": "2014509",
    "end": "2020960"
  },
  {
    "text": "complicated to do experiments with bees so instead we had coffee spill and then again it wasn't even a real coffee spill",
    "start": "2020960",
    "end": "2027169"
  },
  {
    "text": "but here's the figure of coffee spill and we can see that when the human deviates what is happening here and I'll",
    "start": "2027169",
    "end": "2033289"
  },
  {
    "text": "play this video again is that the prediction that we had of the human going towards the goal is actually no",
    "start": "2033289",
    "end": "2039409"
  },
  {
    "text": "longer accurate so let me go back here a couple of steps so you can see that even",
    "start": "2039409",
    "end": "2045499"
  },
  {
    "text": "though the human is deviating because there's this coffee spill over there the predictions still say no now the humans",
    "start": "2045499",
    "end": "2050898"
  },
  {
    "text": "gotta go straight to the goal and eventually the robot gets to this point where the trajectory isn't feasible and it doesn't know what to do now of course",
    "start": "2050899",
    "end": "2056480"
  },
  {
    "text": "you could you know you could have something better than this you could have a last resort kind of like physics based avoidance mechanism but really",
    "start": "2056480",
    "end": "2063079"
  },
  {
    "text": "here the point that we're trying to make is that the predictions are wrong and the predictions may be wrong can get you",
    "start": "2063079",
    "end": "2068480"
  },
  {
    "text": "in trouble in the best case you will have to give the robot some sort of like emergency avoidance maneuver and this of",
    "start": "2068480",
    "end": "2074839"
  },
  {
    "text": "course works if it's a little drone but if you're talking about a car on the highway you can already be in a lot of trouble because you made the wrong",
    "start": "2074839",
    "end": "2081049"
  },
  {
    "text": "prediction so here the main principle that we're trying to to apply is well if",
    "start": "2081049",
    "end": "2088099"
  },
  {
    "text": "the human is not following the model it doesn't make sense to get mad at the human for not following the model but it",
    "start": "2088099",
    "end": "2093858"
  },
  {
    "text": "makes sense to be a little bit more skeptical about the model that we have and ultimately by the way this is true of any model you can have a very simple",
    "start": "2093859",
    "end": "2099770"
  },
  {
    "text": "kind of like model we sell the human might want to do one of three things and there's how they go about it or you",
    "start": "2099770",
    "end": "2105049"
  },
  {
    "text": "could have an extremely complex neural network based model where you've observed a million humans in rooms like",
    "start": "2105049",
    "end": "2111740"
  },
  {
    "text": "these and you've generalized and it works very well 99% of the time but there's always this tail of the",
    "start": "2111740",
    "end": "2116900"
  },
  {
    "text": "distribution that you have to be very careful with because there can always be something that you're not capturing and",
    "start": "2116900",
    "end": "2121910"
  },
  {
    "text": "in the case of humans I probably don't need to convince you that this is true that even if you train a neural network",
    "start": "2121910",
    "end": "2127400"
  },
  {
    "text": "and it's a very good neural network with millions of neurons it's not really going to capture everything that we do so how do we go about",
    "start": "2127400",
    "end": "2135650"
  },
  {
    "text": "suspecting the model here we have the human the goal-driven behavior of the human and we have some noise irrational",
    "start": "2135650",
    "end": "2142100"
  },
  {
    "text": "model or some probabilistic model of what he was going to do but ultimately B's coffee spills any sort of unmodeled",
    "start": "2142100",
    "end": "2149210"
  },
  {
    "text": "intent or circumstances can really violate all of our assumptions well it",
    "start": "2149210",
    "end": "2155540"
  },
  {
    "start": "2154000",
    "end": "2309000"
  },
  {
    "text": "turns out that in the models and the reason about actions probabilistically there are usually parameters that are",
    "start": "2155540",
    "end": "2161990"
  },
  {
    "text": "very helpful in in in modulating the spread of the distribution so to speak",
    "start": "2161990",
    "end": "2168080"
  },
  {
    "text": "in the case of noise irrationality the entropy of the distribution is directly regulated by this beta parameter so the",
    "start": "2168080",
    "end": "2175070"
  },
  {
    "text": "higher the the beta parameter which is often called rationality the rationality coefficient I think rather incorrectly",
    "start": "2175070",
    "end": "2182240"
  },
  {
    "text": "because it's like you're blaming the human whenever they don't follow the model you become just being irrational you can think of it as the degree of",
    "start": "2182240",
    "end": "2188900"
  },
  {
    "text": "confidence that you should have in the model so when beta is very high you should expect actions to",
    "start": "2188900",
    "end": "2195680"
  },
  {
    "text": "probabilistically concentrate a lot around the optimal ones whereas when beta is very low you should",
    "start": "2195680",
    "end": "2200930"
  },
  {
    "text": "expect things to be a little bit more noisy and the human to with the relatively high probability take actions",
    "start": "2200930",
    "end": "2206990"
  },
  {
    "text": "that are not actually that efficient for the goal because a human is actually doing something else that your goal is",
    "start": "2206990",
    "end": "2212270"
  },
  {
    "text": "not really capturing very well so what we do is instead of treating it as a fixed parameter which is how it's",
    "start": "2212270",
    "end": "2217460"
  },
  {
    "text": "usually treated we say okay why don't we reason about this parameter beta under a Bayesian framework so we think of it as",
    "start": "2217460",
    "end": "2223640"
  },
  {
    "text": "a hidden state that can change over time meaning that my model can worked very well for a while and then certainly degrade relatively quickly when the bee",
    "start": "2223640",
    "end": "2230000"
  },
  {
    "text": "flies into the room and so whenever something happens that we're not",
    "start": "2230000",
    "end": "2235400"
  },
  {
    "text": "expecting the robot can now reason about the posterior distribution of this",
    "start": "2235400",
    "end": "2240580"
  },
  {
    "text": "confidence parameter given the prior distribution that we had a moment ago",
    "start": "2240580",
    "end": "2246260"
  },
  {
    "text": "and then how probable is that the human would take this action if this parameter",
    "start": "2246260",
    "end": "2252530"
  },
  {
    "text": "had high value versus if it had a low value so we can actually do this update because it's a scalar parameter we can",
    "start": "2252530",
    "end": "2257690"
  },
  {
    "text": "actually do this even numerically very quickly over time and we can do it in real time and so what happens is the",
    "start": "2257690",
    "end": "2263600"
  },
  {
    "text": "robot very quickly updates this distribution about what the human is likely to do",
    "start": "2263600",
    "end": "2268730"
  },
  {
    "text": "so when the human is not acting according to your model your distribution becomes much more cloudy and uncertain and as a result of the",
    "start": "2268730",
    "end": "2275690"
  },
  {
    "text": "robot becomes more conservative and avoids getting into big trouble with the human now a question that is that is",
    "start": "2275690",
    "end": "2285320"
  },
  {
    "text": "legitimate it's can we how do we you know combine this probabilistic notion of how the human is going to move with",
    "start": "2285320",
    "end": "2290660"
  },
  {
    "text": "some worst-case analysis about for example when we were talking about the dynamics of the quadrotor before having",
    "start": "2290660",
    "end": "2298820"
  },
  {
    "text": "some sort of bounded disturbance about that you know the physics of the robot are not exactly this particular model",
    "start": "2298820",
    "end": "2304550"
  },
  {
    "text": "but there's only going to be this much ground effect or delay in your rotor lag well it turns out that a thing that you",
    "start": "2304550",
    "end": "2311270"
  },
  {
    "start": "2309000",
    "end": "2552000"
  },
  {
    "text": "can do is you can actually put the two together and say I'm going to reason about where the robot should be at a",
    "start": "2311270",
    "end": "2316400"
  },
  {
    "text": "given point in time but in practice I'm going to reason about where the human might be at a given point in time in",
    "start": "2316400",
    "end": "2323360"
  },
  {
    "text": "practice the robot it's not exactly going to be at this point but I can actually compute using safety analysis a",
    "start": "2323360",
    "end": "2328940"
  },
  {
    "text": "worst-case tracking error bound for where the robot will be now there's different ways you can do that this",
    "start": "2328940",
    "end": "2335170"
  },
  {
    "text": "reach ability based model there's actually another method that professor",
    "start": "2335170",
    "end": "2341360"
  },
  {
    "text": "Provo and Marco where are you oh and some of his students have been working on which also gives you some some bounce",
    "start": "2341360",
    "end": "2347180"
  },
  {
    "text": "on the on the amount of error that you're going to have when you track if you use any of these methods what you",
    "start": "2347180",
    "end": "2352310"
  },
  {
    "text": "can now do is you can say well okay the robot can be in any of these locations over time let me project this said onto the",
    "start": "2352310",
    "end": "2359630"
  },
  {
    "text": "probability distribution of where the human might be over time and when I integrate the amount of probability mass that I have inside of this set that is",
    "start": "2359630",
    "end": "2367550"
  },
  {
    "text": "essentially the probability of crashing into the human given the worst-case",
    "start": "2367550",
    "end": "2373180"
  },
  {
    "text": "tracking errors that I could have for the fiscal motion of the of the vehicle",
    "start": "2373180",
    "end": "2378260"
  },
  {
    "text": "and when you combine these two techniques it turns out that you can actually provide some real-time assurances about how likely the",
    "start": "2378260",
    "end": "2385610"
  },
  {
    "text": "quadrotor is going to be to stay clear of the human whenever you are trying to",
    "start": "2385610",
    "end": "2392120"
  },
  {
    "text": "physically track these trajectories and so what's good about this is that if the human starts behaving in a strange way",
    "start": "2392120",
    "end": "2397340"
  },
  {
    "text": "this distribution will automatically become much more blurry and so your",
    "start": "2397340",
    "end": "2402580"
  },
  {
    "text": "automatic computation of these trajectories will automatically become more conservative so here's a little example we have the human moving around",
    "start": "2402580",
    "end": "2410620"
  },
  {
    "text": "like this this in this case we're going to keep confidence fixed so we're not actually reasoning about how well the",
    "start": "2410620",
    "end": "2416260"
  },
  {
    "text": "model is performing you can see that the human goes to the first known goal everything's fine starts moving to the second known goal",
    "start": "2416260",
    "end": "2421960"
  },
  {
    "text": "and the model is able to make sense of it and now the human starts moving in a to a third goal that we have no idea",
    "start": "2421960",
    "end": "2427000"
  },
  {
    "text": "about and you can see that the predictions are ridiculous they're just not doing anything useful the robot is convinced that the human is going to",
    "start": "2427000",
    "end": "2432910"
  },
  {
    "text": "turn around immediately and start going to this goal because it doesn't realize that the human is no longer following",
    "start": "2432910",
    "end": "2438310"
  },
  {
    "text": "the model in a reasonable way now there's a relatively simple model but even if we had a more complex model",
    "start": "2438310",
    "end": "2444550"
  },
  {
    "text": "ultimately this thing is qualitatively what is going to happen your model will",
    "start": "2444550",
    "end": "2449560"
  },
  {
    "text": "make wrong predictions and still those predictions even if they are probabilistic will be overly confident because you're not reasoning about",
    "start": "2449560",
    "end": "2456040"
  },
  {
    "text": "whether or not you should trust your model instead with the Bayesian confidence method that we're using here",
    "start": "2456040",
    "end": "2462160"
  },
  {
    "text": "you'll see a very similar behavior for as long as the human is going to one of the known goals but as soon as the human",
    "start": "2462160",
    "end": "2467590"
  },
  {
    "text": "starts moving to the third goal and this is in fact the same recorded trajectory of the human you can see that suddenly",
    "start": "2467590",
    "end": "2474070"
  },
  {
    "text": "you have a much more uncertain prediction and so the robot decides to",
    "start": "2474070",
    "end": "2480430"
  },
  {
    "text": "take a more conservative path so that even though the human is doing strange things that the model doesn't understand",
    "start": "2480430",
    "end": "2485710"
  },
  {
    "text": "there won't be a collision so you're kind of modulating the amount of conservative that conservativeness that",
    "start": "2485710",
    "end": "2491320"
  },
  {
    "text": "you need your robot to exercise based on how confused it is about what the human",
    "start": "2491320",
    "end": "2496570"
  },
  {
    "text": "is doing ok very good I'll just show you a final video of the of the end result this was actually short by Wired who are",
    "start": "2496570",
    "end": "2503800"
  },
  {
    "text": "much better than us at making cool video so I just wanted to show you because it's very nice you can see exactly",
    "start": "2503800",
    "end": "2509260"
  },
  {
    "text": "what's happening as soon as Andrea starts deviating from the behavior that you can expect you see",
    "start": "2509260",
    "end": "2514270"
  },
  {
    "text": "that the robot is already maneuvering to stay clear of her and indeed gets out of the way and avoids collisions even",
    "start": "2514270",
    "end": "2521860"
  },
  {
    "text": "though it doesn't exactly know what she's going to do it's becoming more conservative the second it realizes that its model is not really being very good",
    "start": "2521860",
    "end": "2528760"
  },
  {
    "text": "at predicting alright but there's probably one I don't know I have this video twice there's probably one",
    "start": "2528760",
    "end": "2536560"
  },
  {
    "text": "you know there's kind of an elephant in the room in this whole analysis that we're doing and some of you might be thinking about it which is okay but what",
    "start": "2536560",
    "end": "2545260"
  },
  {
    "text": "about interaction the human here appears to just be doing its thing her thing and the robot is actually getting in the way",
    "start": "2545260",
    "end": "2550900"
  },
  {
    "text": "isn't that going to affect what the human does and indeed this is a very important question that we're kind of",
    "start": "2550900",
    "end": "2556900"
  },
  {
    "start": "2552000",
    "end": "2655000"
  },
  {
    "text": "overlooking in that in that work and it's extremely important in things like autonomous driving and you know robots",
    "start": "2556900",
    "end": "2563470"
  },
  {
    "text": "in the home which is well the robot is thinking about what the human is going to do but the human souls are thinking about what the robot is going to do",
    "start": "2563470",
    "end": "2569440"
  },
  {
    "text": "therefore the model of the robot should really have you know the little bubble",
    "start": "2569440",
    "end": "2574900"
  },
  {
    "text": "inside there where the human is thinking back about what the robot is doing and this is also true of the you know of the",
    "start": "2574900",
    "end": "2580000"
  },
  {
    "text": "humans model of the robot so now this kind of gets you into this sort of infinite regress where the human is",
    "start": "2580000",
    "end": "2585370"
  },
  {
    "text": "thinking about what the robot think that is you know that is thinking the robots gonna think it's thinking and",
    "start": "2585370",
    "end": "2590850"
  },
  {
    "text": "unfortunately this becomes very intractable very quickly and even you know the companies deploying",
    "start": "2590850",
    "end": "2596380"
  },
  {
    "text": "self-driving car systems are trying to issue this kind of treatment as much as possible but what you can see is that",
    "start": "2596380",
    "end": "2601960"
  },
  {
    "text": "when you just try to assume that people are doing their thing you can become way",
    "start": "2601960",
    "end": "2607600"
  },
  {
    "text": "way too conservative and so this is a video I'll play it again it came out last year some person on Twitter this is",
    "start": "2607600",
    "end": "2613810"
  },
  {
    "text": "the Hawaii mo vehicle trying to merge onto a highway that sure has some traffic but it's not terrible and it",
    "start": "2613810",
    "end": "2619780"
  },
  {
    "text": "just can't find an opening because it's you know convinced that nobody is going to let it go and eventually emerge",
    "start": "2619780",
    "end": "2625540"
  },
  {
    "text": "yourself obviously in order to be able to drive competently you need to be actively reasoning about how people are going to respond to you and this is how",
    "start": "2625540",
    "end": "2631780"
  },
  {
    "text": "all of us who at least have driver's license reason we're where we were",
    "start": "2631780",
    "end": "2637270"
  },
  {
    "text": "driving right so we really need these systems to be able to do this competently but also without being overly aggressive and just saying like",
    "start": "2637270",
    "end": "2643900"
  },
  {
    "text": "well I'm just gonna do it and you have to let me go so coordination here is",
    "start": "2643900",
    "end": "2648970"
  },
  {
    "text": "ultimately very important by the way people are much better at coordination than we typically realize there's a",
    "start": "2648970",
    "end": "2654340"
  },
  {
    "text": "little experiment that I like to run because it's actually at least it surprised me a lot when somebody told me",
    "start": "2654340",
    "end": "2660250"
  },
  {
    "start": "2655000",
    "end": "2750000"
  },
  {
    "text": "this the first time so we're gonna play a very quick game which is basically heads or tails but we're actually not",
    "start": "2660250",
    "end": "2667540"
  },
  {
    "text": "playing against each other we're playing all together so the objective is to say the same thing as everybody else so basically I'm",
    "start": "2667540",
    "end": "2674530"
  },
  {
    "text": "going to count to three after three we're all going to say either heads or tails and we win if everybody says the",
    "start": "2674530",
    "end": "2682119"
  },
  {
    "text": "same thing and we lose it you know people start saying different things that make sense and we can't talk to",
    "start": "2682119",
    "end": "2687220"
  },
  {
    "text": "each other no winking no nothing okay so we're going to go out to three I'm gonna say one two three yeah and the",
    "start": "2687220",
    "end": "2693340"
  },
  {
    "text": "area is gonna be the answer okay already all right let's do it one two three heads okay",
    "start": "2693340",
    "end": "2701700"
  },
  {
    "text": "95% hints something like that okay very good so what happens here is that you're",
    "start": "2701700",
    "end": "2707530"
  },
  {
    "text": "going we don't have time to get into the whole details but every person is making you know some quick and first about our",
    "start": "2707530",
    "end": "2713950"
  },
  {
    "text": "people slightly more likely to say one thing than the other okay maybe heads wait if I'm thinking this and everyone",
    "start": "2713950",
    "end": "2719140"
  },
  {
    "text": "is thinking this then we're totally all going to say heads okay let's say heads so this is very subtle and it's very",
    "start": "2719140",
    "end": "2725020"
  },
  {
    "text": "difficult for robots to reason about it but still this kind of thing plays an important part every time we're negotiating an intersection so what one",
    "start": "2725020",
    "end": "2733180"
  },
  {
    "text": "of the things that I've been trying to look at it's can we use game theory to try to reason about this sort of",
    "start": "2733180",
    "end": "2738430"
  },
  {
    "text": "interplay between what I think you're gonna do and what I'm going to do based on what you're trying to make me do etc",
    "start": "2738430",
    "end": "2744150"
  },
  {
    "text": "it turns out that there are game theoretic tools that you can apply without you know thanks blowing up and",
    "start": "2744150",
    "end": "2749530"
  },
  {
    "text": "becoming intractable sorry and so one of the one of the things that you can do is",
    "start": "2749530",
    "end": "2755619"
  },
  {
    "start": "2750000",
    "end": "2836000"
  },
  {
    "text": "you can have this tactical level at which you're reasoning about trajectories and you can have a relatively simple interaction model",
    "start": "2755619",
    "end": "2761130"
  },
  {
    "text": "where for example you assume that their human is going to respond to what you do but they're not necessarily also",
    "start": "2761130",
    "end": "2767650"
  },
  {
    "text": "strategically trying to affect what you're doing and that's fine over a short horizon of about half a second so",
    "start": "2767650",
    "end": "2772869"
  },
  {
    "text": "you know the typical trajectory optimizers tend to reason in this this kind of level but if you're trying to",
    "start": "2772869",
    "end": "2778240"
  },
  {
    "text": "plan a complex maneuver like an you know crossing an intersection or an overtaking maneuver or reasoning about",
    "start": "2778240",
    "end": "2783820"
  },
  {
    "text": "who goes first before overtaking a truck you really need to reason on a longer horizon say five or ten seconds and so",
    "start": "2783820",
    "end": "2791050"
  },
  {
    "text": "when when you're using when you're reasoning over that kind of horizon it actually becomes very relevant to think",
    "start": "2791050",
    "end": "2796690"
  },
  {
    "text": "about how we're going to strategically influence each other and so for that you",
    "start": "2796690",
    "end": "2801790"
  },
  {
    "text": "need to use something that is more coupled where your decisions and the decisions of others are coupled and that is dynamic game",
    "start": "2801790",
    "end": "2807730"
  },
  {
    "text": "theory and so what we do to keep things strength able in this case as we say okay let's do a hierarchical analysis",
    "start": "2807730",
    "end": "2813790"
  },
  {
    "text": "where we use the full dynamical model here with a simplified interaction model and then at a high level we use a",
    "start": "2813790",
    "end": "2820440"
  },
  {
    "text": "simplified dynamical model where we don't have all that you know all the details of the tire in contact with the",
    "start": "2820440",
    "end": "2826030"
  },
  {
    "text": "ground and whatnot but instead we're going to have a more accurate interaction model between the automation",
    "start": "2826030",
    "end": "2832210"
  },
  {
    "text": "and the human again using some human model that might be flawed and so the",
    "start": "2832210",
    "end": "2837310"
  },
  {
    "start": "2836000",
    "end": "2975000"
  },
  {
    "text": "way this works for the strategic planner to give you a very quick picture you don't need to look at the algorithm too",
    "start": "2837310",
    "end": "2842530"
  },
  {
    "text": "much as we consider we discretize the world into a number of states and we consider how much the human and the",
    "start": "2842530",
    "end": "2849220"
  },
  {
    "text": "robot like each of these states the robot here is in yellow and we say okay if you go one step back in time and you",
    "start": "2849220",
    "end": "2857170"
  },
  {
    "text": "consider each of these states for example this one the robot has a bunch of different actions that it can take",
    "start": "2857170",
    "end": "2862510"
  },
  {
    "text": "and based on the action that it takes the human will be more or less happy with the situation and will be more or",
    "start": "2862510",
    "end": "2869110"
  },
  {
    "text": "less happy to take different actions so you're now reasoning probabilistically as before about how the human might",
    "start": "2869110",
    "end": "2874120"
  },
  {
    "text": "respond to each of the possible actions that you could take and so you say well actually don't like this very much let",
    "start": "2874120",
    "end": "2880000"
  },
  {
    "text": "me try a different action see how the human might respond probabilistically and by doing this you determine the best",
    "start": "2880000",
    "end": "2885190"
  },
  {
    "text": "action that the robot should take at each particular state at each point in time and you can continue doing this for",
    "start": "2885190",
    "end": "2891580"
  },
  {
    "text": "all the states and then you can do this in backward time it turns out that this row over here can be done simultaneously",
    "start": "2891580",
    "end": "2898090"
  },
  {
    "text": "so you can actually exploit massively parallel architectures so you can actually do this whole computation very",
    "start": "2898090",
    "end": "2904150"
  },
  {
    "text": "quickly for a time horizon and because we have a relatively simplified model of the world you can actually have this",
    "start": "2904150",
    "end": "2911410"
  },
  {
    "text": "thing compute a value function for you which is based on the game theoretic interaction and what you can do now is",
    "start": "2911410",
    "end": "2917200"
  },
  {
    "text": "you say okay here's my trajectory optimization that I'm using with my standard method that may be Google or",
    "start": "2917200",
    "end": "2924580"
  },
  {
    "text": "way mo or uber are utilizing in their in their planning so here in my",
    "start": "2924580",
    "end": "2930700"
  },
  {
    "text": "trajectories and here the human trajectories and what you do now is you add",
    "start": "2930700",
    "end": "2936020"
  },
  {
    "text": "a final value a terminal value term terminal reward that captures the",
    "start": "2936020",
    "end": "2941390"
  },
  {
    "text": "strategic value so this is similar to what you do if you're playing a game of chess and you do some tree exploration and then ultimately you use a heuristic",
    "start": "2941390",
    "end": "2947390"
  },
  {
    "text": "to determine what you think is going to happen from now on and so it turns out that by doing this you get an Augmented",
    "start": "2947390",
    "end": "2956090"
  },
  {
    "text": "version of the trajectory optimization where you're actually saying well where do I want to end up at the end of my",
    "start": "2956090",
    "end": "2961460"
  },
  {
    "text": "half second horizon where is the interaction going to be more beneficial to me we're using a model of this",
    "start": "2961460",
    "end": "2967310"
  },
  {
    "text": "interaction that is accounting for the strategic influence between the two players and it turns out that the - this",
    "start": "2967310",
    "end": "2973070"
  },
  {
    "text": "actually works fairly well and I want to show you a couple of examples so in the first case here we have an overtaking",
    "start": "2973070",
    "end": "2978740"
  },
  {
    "start": "2975000",
    "end": "3168000"
  },
  {
    "text": "maneuver and what you're seeing over here is you know a color coding of the",
    "start": "2978740",
    "end": "2983860"
  },
  {
    "text": "values that the robot sees relates it to the human so blue is high and read it",
    "start": "2983860",
    "end": "2989900"
  },
  {
    "text": "slow excuse me and as you can see when the robot approaches the human there's a list a tiny blue stain over here that",
    "start": "2989900",
    "end": "2996770"
  },
  {
    "text": "it's actually going to start growing and the reason that it that it is there are that it's growing is that the robot",
    "start": "2996770",
    "end": "3001780"
  },
  {
    "text": "nodes are there some it has some ability to push the human to influence a human to incentivize the lane change so in the",
    "start": "3001780",
    "end": "3007720"
  },
  {
    "text": "right case the human changes lane in the left case she doesn't and what you can see is that there's a gradient of value",
    "start": "3007720",
    "end": "3014550"
  },
  {
    "text": "pulling the robot sucking the robot into the maneuver because even though it's only planning with a short time horizon",
    "start": "3014550",
    "end": "3020260"
  },
  {
    "text": "which is what you can see here with the little transparent cars I'll tell you this review again the value is actually",
    "start": "3020260",
    "end": "3026650"
  },
  {
    "text": "already giving you the value the value function here shown in colors it's",
    "start": "3026650",
    "end": "3033220"
  },
  {
    "text": "already giving you information about how the interaction is going to go when you go forward so here you actually make the human move and then the value extends",
    "start": "3033220",
    "end": "3040000"
  },
  {
    "text": "over here here you just switch because you have this blue carpet that it's pulling you effectively like a potential",
    "start": "3040000",
    "end": "3045820"
  },
  {
    "text": "field into the overtaking maneuver so it's a nice and tractable way of reasoning with the longer horizon but",
    "start": "3045820",
    "end": "3052720"
  },
  {
    "text": "also doing this accounting for the interaction now interestingly I don't have a video for this as the confidence",
    "start": "3052720",
    "end": "3058570"
  },
  {
    "text": "that you have on the human goes down so say that you're doing something like what we were doing before and you're saying hey the human is acting in a",
    "start": "3058570",
    "end": "3064240"
  },
  {
    "text": "strange way and it's wiggling around when the confidence goes down the human becomes more likely to do",
    "start": "3064240",
    "end": "3069460"
  },
  {
    "text": "anything that is not following your model and then this value actually becomes much less pronounced and in fact",
    "start": "3069460",
    "end": "3075609"
  },
  {
    "text": "it gets to the point where it becomes unsafe to attempt an overtaking maneuver and the robot will actually stay back",
    "start": "3075609",
    "end": "3081940"
  },
  {
    "text": "and avoid getting close to the human because it doesn't understand what the human is going to do so that's sort of the kind of behavior that you would",
    "start": "3081940",
    "end": "3087520"
  },
  {
    "text": "intuitively expect now another example that we really wanted to to cover and",
    "start": "3087520",
    "end": "3092859"
  },
  {
    "text": "with this I'm almost out of time was what happens when you're getting to this typical situation where you're going to",
    "start": "3092859",
    "end": "3098980"
  },
  {
    "text": "overtake a truck and then out of the corner of your eye you catch somebody who is accelerating on the other lane",
    "start": "3098980",
    "end": "3104260"
  },
  {
    "text": "and they're about to Tory cut cut you of right the second before you ever take the truck right this is probably",
    "start": "3104260",
    "end": "3109690"
  },
  {
    "text": "something that's happened to many of us at some point in time well this is a very game theoretic kind of situation",
    "start": "3109690",
    "end": "3115240"
  },
  {
    "text": "right you sort of feel the the game theoretic pressure here in the test where you're like well ok I can accelerate and cut the other person off",
    "start": "3115240",
    "end": "3121570"
  },
  {
    "text": "and make it clear that they can't go or I can decelerate and let them go with a little bit of margin or I can keep doing",
    "start": "3121570",
    "end": "3126760"
  },
  {
    "text": "what I'm doing and maintain speed but then they're probably still going to go for it and it's probably not going to be safe so it turns out that when you throw",
    "start": "3126760",
    "end": "3135490"
  },
  {
    "text": "this kind of analysis in this case you'll see that the white car here decides to slow down and let the yellow",
    "start": "3135490",
    "end": "3141849"
  },
  {
    "text": "car go in this case it actually starts accelerating and decides that it's not going to let the yellow car go and this",
    "start": "3141849",
    "end": "3147640"
  },
  {
    "text": "basically changes with the initial conditions depending on who's going faster and how far away you are from the from the truck but it's interesting that",
    "start": "3147640",
    "end": "3154660"
  },
  {
    "text": "basically these two kinds of strategies do in fact emerge where the in this case the white car is really trying to affect",
    "start": "3154660",
    "end": "3161170"
  },
  {
    "text": "the behavior of the of the aggressive car whether or not it takes any weight",
    "start": "3161170",
    "end": "3168730"
  },
  {
    "start": "3168000",
    "end": "3270000"
  },
  {
    "text": "so I wanted to end with a quick reflection about you know we've been talking about robotic systems but the truth is that AI systems are you know",
    "start": "3168730",
    "end": "3175060"
  },
  {
    "text": "more and more pervasive and not only in the form of robots but also in the form of a lot of complex platforms that we're",
    "start": "3175060",
    "end": "3183220"
  },
  {
    "text": "interacting with every day and that usually have this very simple model where they assume that this is us and",
    "start": "3183220",
    "end": "3190150"
  },
  {
    "text": "this is them and all they're doing is they're providing services once as a one-time thing and they're really",
    "start": "3190150",
    "end": "3195910"
  },
  {
    "text": "failing to acknowledge that there is a closed-loop dynamic between us and the system and you know every time you give",
    "start": "3195910",
    "end": "3202839"
  },
  {
    "text": "a recommendation this affects the opinions that the person has it affects a new input that the person gives you and every time you",
    "start": "3202839",
    "end": "3209920"
  },
  {
    "text": "give them you know recommendations sometimes false positives negatives or you favor a certain kind of content over",
    "start": "3209920",
    "end": "3216550"
  },
  {
    "text": "other kind of content this really effects it changes the way the person is",
    "start": "3216550",
    "end": "3223450"
  },
  {
    "text": "thinking of the way the person is interacting with the model I'm sorry with the automation which in turns in turn affects the model that the",
    "start": "3223450",
    "end": "3229840"
  },
  {
    "text": "automation is forming off the person and so there I think there are some important and kind of overlooked effects",
    "start": "3229840",
    "end": "3235660"
  },
  {
    "text": "at least from the technical point of view and people are starting to talk about this in the news about social",
    "start": "3235660",
    "end": "3241540"
  },
  {
    "text": "polarization through social media etc but I really think that there's you know here there's an opportunity to try to",
    "start": "3241540",
    "end": "3246910"
  },
  {
    "text": "analyze how this all works but unfortunately an important thing to",
    "start": "3246910",
    "end": "3252700"
  },
  {
    "text": "realize is that this becomes very intractable very quickly now the effects",
    "start": "3252700",
    "end": "3258160"
  },
  {
    "text": "that this thing is having on every human are actually also having effects globally on you know political",
    "start": "3258160",
    "end": "3263560"
  },
  {
    "text": "polarization social inequality and so it's very hard to do but I really think",
    "start": "3263560",
    "end": "3268600"
  },
  {
    "text": "that we should make an effort to reason about the closed loop between automation systems AI systems and people and really",
    "start": "3268600",
    "end": "3277240"
  },
  {
    "text": "trying to reason about what are the what are the consequences of these interactions and to what extent we can",
    "start": "3277240",
    "end": "3282580"
  },
  {
    "text": "try to design systems that will prevent certain kinds of damage that is undesired in the interest of time I'm",
    "start": "3282580",
    "end": "3290530"
  },
  {
    "text": "going to skip a couple of slides that I had in the end the at the very end I do",
    "start": "3290530",
    "end": "3296740"
  },
  {
    "text": "want to say that a couple of assumptions that break that break a lot are",
    "start": "3296740",
    "end": "3302830"
  },
  {
    "text": "assumptions made by some companies commercializing semi automated vehicles",
    "start": "3302830",
    "end": "3309670"
  },
  {
    "text": "for example the assumption that the person will take over whenever the system makes a mistake and that the person will be able to take over in less",
    "start": "3309670",
    "end": "3315490"
  },
  {
    "text": "than a second to save their own lives this is something that sometimes it's true like this person here who grabs the",
    "start": "3315490",
    "end": "3321430"
  },
  {
    "text": "wheel right before the car autonomously drives into the vehicle in the oncoming lane but sometimes it's not true and",
    "start": "3321430",
    "end": "3328330"
  },
  {
    "text": "there have been accidents precisely due to this failure to acknowledge that the",
    "start": "3328330",
    "end": "3334120"
  },
  {
    "text": "interaction between the human and the and the robot is simply not that easy and not that ideal and you know in the",
    "start": "3334120",
    "end": "3340610"
  },
  {
    "text": "case of the mcat system I really think that bowing is learning the lesson the hard way that it's not ok to just assume",
    "start": "3340610",
    "end": "3346490"
  },
  {
    "text": "that the pilot will immediately take over especially when like in their case they didn't even educate the pilots",
    "start": "3346490",
    "end": "3353450"
  },
  {
    "text": "about the existence of this system in the first place so with that you know a quick final",
    "start": "3353450",
    "end": "3361100"
  },
  {
    "start": "3359000",
    "end": "3507000"
  },
  {
    "text": "reflection I think we can make these robotic and AI sisters more reliable in the future and I think that some things",
    "start": "3361100",
    "end": "3369980"
  },
  {
    "text": "we can do is actively reason about how we can uphold these assurances and these guarantees monitoring the validity of",
    "start": "3369980",
    "end": "3377330"
  },
  {
    "text": "the assumptions that we make enlighten the observed in light of the observed data and remaining humble especially",
    "start": "3377330",
    "end": "3382760"
  },
  {
    "text": "when predicting human behavior and trying to even figure out what human values are and what humans want the",
    "start": "3382760",
    "end": "3388580"
  },
  {
    "text": "automation system to do which is I think relevant to discussing things like you know social media automation etc and",
    "start": "3388580",
    "end": "3397070"
  },
  {
    "text": "finally I think that one of the reasons that this kind of safety analysis is very useful and can be useful going forward is that it really helps us think",
    "start": "3397070",
    "end": "3404270"
  },
  {
    "text": "hard and flesh out what are the assumptions that we're making and what are the conditions under which we think",
    "start": "3404270",
    "end": "3410180"
  },
  {
    "text": "it's acceptable for a system to fail maybe society accepts that if both engines get taken down by birds at the",
    "start": "3410180",
    "end": "3416900"
  },
  {
    "text": "same time the plane might go down and maybe that is just ok that I'm not sure we accept that one thing that we",
    "start": "3416900",
    "end": "3422540"
  },
  {
    "text": "certainly accept is that if both pilots on a plane have a heart attack at exactly the same time then the systems",
    "start": "3422540",
    "end": "3427970"
  },
  {
    "text": "gonna go down and we get on planes every no everyday you know pretty pretty often many of us and we're kind of implicitly",
    "start": "3427970",
    "end": "3434720"
  },
  {
    "text": "accepting that if both pilots die or become incapacitated at the same time we might really be in trouble we just think",
    "start": "3434720",
    "end": "3440720"
  },
  {
    "text": "that this is extremely unlikely and that if this were to happen you know we sort of make peace with the fact that that",
    "start": "3440720",
    "end": "3446330"
  },
  {
    "text": "this is a possible failure now so fleshing out what are these assumptions what are the conditions under which a",
    "start": "3446330",
    "end": "3451460"
  },
  {
    "text": "system might fail I think can be extremely useful in terms of deploying",
    "start": "3451460",
    "end": "3456770"
  },
  {
    "text": "automation systems that even if they're not perfect are highly reliable when they're supposed to be reliable I'd much",
    "start": "3456770",
    "end": "3463100"
  },
  {
    "text": "rather get in a car that tells me this car is going to drive safely unless you",
    "start": "3463100",
    "end": "3469670"
  },
  {
    "text": "it gets attacked at the same time by the person to the right to the person to the left and the person in front and it doesn't have a way out then just getting",
    "start": "3469670",
    "end": "3476150"
  },
  {
    "text": "in a card that says this car is saved 99.9 percent of the time period right so",
    "start": "3476150",
    "end": "3482210"
  },
  {
    "text": "I mean maybe something to think about when it comes to designing these kinds of systems I think that if we get these",
    "start": "3482210",
    "end": "3488150"
  },
  {
    "text": "things right we will be able to have a future in which all of our automated systems will be so trustworthy that will",
    "start": "3488150",
    "end": "3494630"
  },
  {
    "text": "be perfectly okay with letting our children go outside and play with them but really in order to get from here to",
    "start": "3494630",
    "end": "3501529"
  },
  {
    "text": "that future we should very much mind the gap so with that thank you very much I'm happy to take any questions",
    "start": "3501529",
    "end": "3509349"
  },
  {
    "start": "3507000",
    "end": "3599000"
  },
  {
    "text": "all right yes right so that's very that",
    "start": "3517339",
    "end": "3545880"
  },
  {
    "text": "is a very good question and in fact what I would argue is that you should always be recomputing your model of what the",
    "start": "3545880",
    "end": "3551549"
  },
  {
    "text": "human is going to do but also know that this is not sufficient because when you are recomputing them all of what the",
    "start": "3551549",
    "end": "3557250"
  },
  {
    "text": "human is going to do you're going to do this within some space of possible models right and the problem is that",
    "start": "3557250",
    "end": "3563279"
  },
  {
    "text": "this space of models is fundamentally what might not contain reality and so",
    "start": "3563279",
    "end": "3568319"
  },
  {
    "text": "you might improve the model but that model might still not be able to capture what the human is actually doing and",
    "start": "3568319",
    "end": "3574140"
  },
  {
    "text": "what the human is really going to do so in fact what happens is you always have an uncertainty region around the human all you're doing with this with this",
    "start": "3574140",
    "end": "3581599"
  },
  {
    "text": "adaptation if you will is you're changing the shape of that uncertainty but even when the human is behaving",
    "start": "3581599",
    "end": "3587309"
  },
  {
    "text": "pretty accurately according to your model you do have an ax certainly we region around the human and it should always be that way I think the the most",
    "start": "3587309",
    "end": "3593910"
  },
  {
    "text": "dangerous thing that a robot or an automated system can ever do is to have a perfectly clear-cut prediction of what",
    "start": "3593910",
    "end": "3599759"
  },
  {
    "text": "the humans going to do right that can that is always going to end up tragically sooner or later",
    "start": "3599759",
    "end": "3606680"
  }
]