[
  {
    "start": "0",
    "end": "5560"
  },
  {
    "text": "All right. Good morning, everyone. The lecture today is going\nto focus on online planning",
    "start": "5560",
    "end": "11290"
  },
  {
    "text": "and policy search. And we're going to start\noff talking about some of the differences in offline\nversus online planning",
    "start": "11290",
    "end": "18760"
  },
  {
    "text": "and then move into the actual\nonline planning algorithms themselves and discuss\nsome of the strengths",
    "start": "18760",
    "end": "23890"
  },
  {
    "text": "and weaknesses of each one. And we'll also briefly look at\nsome examples of hybrid planning",
    "start": "23890",
    "end": "29830"
  },
  {
    "text": "approaches that have seen\nsome large success recently, like AlphaZero style\napproaches, and how",
    "start": "29830",
    "end": "36250"
  },
  {
    "text": "some of these hybrid\nplanning methods can be extended to language\nmodel applications as well.",
    "start": "36250",
    "end": "41860"
  },
  {
    "text": "And then we'll wrap it up at\nthe end with policy search. And so the goal throughout\nall of this like",
    "start": "41860",
    "end": "47980"
  },
  {
    "text": "always is to have\nthis connection back to the bigger picture. So if you ever feel like\nyou're losing that connection",
    "start": "47980",
    "end": "54070"
  },
  {
    "text": "throughout any of this, just\nfeel free, pop your hand up and ask a question. So starting off with offline\nversus online planning methods.",
    "start": "54070",
    "end": "63379"
  },
  {
    "text": "So offline policies\ntypically are pre-computed before\nany actions are",
    "start": "63380",
    "end": "68810"
  },
  {
    "text": "taken in the actual\nreal-world problem. And often these offline\npolicies are solving",
    "start": "68810",
    "end": "75050"
  },
  {
    "text": "over the entire state space. OK, so they're trying\nto prescribe an action to take from any\nstate that we're in.",
    "start": "75050",
    "end": "82580"
  },
  {
    "text": "Online policies,\non the other hand, are finding actions\nbased on reasoning about the reachable state space.",
    "start": "82580",
    "end": "89280"
  },
  {
    "text": "So they're planning forward\nfrom the initial state and looking at all of the\ndifferent possible transitions",
    "start": "89280",
    "end": "95300"
  },
  {
    "text": "that could occur from this\ninitial planning state. And typically, these\nonline planning methods",
    "start": "95300",
    "end": "102770"
  },
  {
    "text": "are using this receding\nhorizon planning scheme to plan over that\nreachable state space.",
    "start": "102770",
    "end": "108300"
  },
  {
    "text": "And so this idea of the\nreachable state space and receding horizon\nplanning are the foundation",
    "start": "108300",
    "end": "114440"
  },
  {
    "text": "here of all of the\nonline planning methods that we're going to\nbe talking about. So we're going to dive into\nboth of these concepts,",
    "start": "114440",
    "end": "120840"
  },
  {
    "text": "lay the foundation\nfor online planning, and then go into the\nalgorithms themselves.",
    "start": "120840",
    "end": "127000"
  },
  {
    "text": "So looking at the idea here\nof the reachable state space and receding horizon\nplanning, starting",
    "start": "127000",
    "end": "132340"
  },
  {
    "text": "with the reachable\nstate space, I think this is a great graphic\nto get the intuition here",
    "start": "132340",
    "end": "137410"
  },
  {
    "text": "for what we're talking about and\nconnecting it back to your life. So your reachable\nstate space today",
    "start": "137410",
    "end": "143470"
  },
  {
    "text": "is all of these states that\nlie in the future for you. So all of the different\nactions that you could take,",
    "start": "143470",
    "end": "148490"
  },
  {
    "text": "whether you accept\na certain job offer or move to a\ndifferent city, those are all different\nactions that will",
    "start": "148490",
    "end": "153879"
  },
  {
    "text": "affect the future\ntrajectory of your life. So that's what's reachable\nfrom your current state today.",
    "start": "153880",
    "end": "160400"
  },
  {
    "text": "What's no longer\nreachable to you is the actions that you took\nwhen you were 12 years old.",
    "start": "160400",
    "end": "165820"
  },
  {
    "text": "So whatever you did when\nyou were 12 no longer reachable from\nyour current state. And I do-- I recognize\nwe're at Stanford.",
    "start": "165820",
    "end": "173080"
  },
  {
    "text": "A lot of you are extreme\nhigh overachievers. So if you are 12 years old\nand you're taking this class, then that is still in your\nreachable state space.",
    "start": "173080",
    "end": "180200"
  },
  {
    "text": "And I will correct that for you. But I think for most of us,\nit's no longer reachable.",
    "start": "180200",
    "end": "185260"
  },
  {
    "text": "So connecting this same\nidea here to a game setup and using chess as this example,\nthe reachable state space",
    "start": "185260",
    "end": "193190"
  },
  {
    "text": "here would be looking at\nplanning from this current board state of the chess\nboard that we show there",
    "start": "193190",
    "end": "198769"
  },
  {
    "text": "and looking at all of\nthe different actions that we could take\nfrom that initial state and how they would\ntransition us.",
    "start": "198770",
    "end": "205260"
  },
  {
    "text": "So this would be\nlooking at maybe I'm going to move my pawn to d5\nfrom that initial board state.",
    "start": "205260",
    "end": "211260"
  },
  {
    "text": "What state would that\ntransition me into? And then what are the other\nstates that I should move to from there?",
    "start": "211260",
    "end": "217670"
  },
  {
    "text": "So offline policies,\non the other hand, they're going to be\nlooking at planning from every possible state.",
    "start": "217670",
    "end": "224450"
  },
  {
    "text": "They want to prescribe an\naction to take from any state that you could be in. So the offline\npolicies are going",
    "start": "224450",
    "end": "230840"
  },
  {
    "text": "to be concerned with the\nbeginning game strategy. So the opening\nmoves-- they're going to be concerned with mid game\nand end game all sort of in one,",
    "start": "230840",
    "end": "238770"
  },
  {
    "text": "whereas the online\npolicies are only going to be focused on planning\nfrom that current board state. So that's highlighting the\ndifference here between the two.",
    "start": "238770",
    "end": "246770"
  },
  {
    "text": "And this is directly\nconnected to this idea of receding horizon planning.",
    "start": "246770",
    "end": "252050"
  },
  {
    "text": "Because in receding\nhorizon planning, we are essentially just planning\nover that reachable state space.",
    "start": "252050",
    "end": "258061"
  },
  {
    "text": "So the way that\nthis works is we're going to plan from\nthe current state up to some depth or horizon D.",
    "start": "258061",
    "end": "264350"
  },
  {
    "text": "We're going to take an\naction, and then we're going to just transition to\nthat new state and replan.",
    "start": "264350",
    "end": "269650"
  },
  {
    "text": "So this is very similar to\nwhat we've already seen. And so in this\nchess example, we've already shown the planning\nhere up to depth or horizon D.",
    "start": "269650",
    "end": "278070"
  },
  {
    "text": "We're then going to take one\nof those initial actions. So let's say this is like\nmoving our pawn to d5.",
    "start": "278070",
    "end": "284210"
  },
  {
    "text": "We take that action and then we\ntransition into that new state. And then all that's\ngoing to happen",
    "start": "284210",
    "end": "290150"
  },
  {
    "text": "is we just replan\nfrom that new state and we repeat this process. And so that's where\nthis receding horizon",
    "start": "290150",
    "end": "296180"
  },
  {
    "text": "aspect is coming in, is that we\nare planning to that horizon. We then take an action and we\njust shift that horizon over.",
    "start": "296180",
    "end": "302603"
  },
  {
    "text": "So you're never actually\nreaching that horizon that you're planning to. You're just\ncontinuously shifting it as you take those actions and\ntransition into the new states.",
    "start": "302603",
    "end": "311510"
  },
  {
    "text": "And so a great\nreal-world example of this receding\nhorizon planning is in autonomous driving.",
    "start": "311510",
    "end": "317040"
  },
  {
    "text": "And this is an example\nfrom Waymo here. And what you're seeing\nis the car's current plan",
    "start": "317040",
    "end": "323840"
  },
  {
    "text": "is shown in green there. And so as more information\nbecomes available to the car,",
    "start": "323840",
    "end": "328920"
  },
  {
    "text": "it's going to update its\nplan as it sees that car veer into the intersection. So it's modifying\nthe plan, replanning,",
    "start": "328920",
    "end": "335910"
  },
  {
    "text": "executing that new updated\nplan and continuing on. So this is a great\nexample of how you're planning up\nto some horizon,",
    "start": "335910",
    "end": "342479"
  },
  {
    "text": "you're taking in\nthat new information, and then you're replanning as\nnew information comes online.",
    "start": "342480",
    "end": "347520"
  },
  {
    "text": "Yes. Between offline\nand online methods, is there one that's better\nat taking into account all",
    "start": "347520",
    "end": "354050"
  },
  {
    "text": "the future rewards versus-- Yeah, so the question was\nbetween offline and online planning, is there\none that's better",
    "start": "354050",
    "end": "360740"
  },
  {
    "text": "in taking into account\nthe future rewards or future information\nthat you receive?",
    "start": "360740",
    "end": "368246"
  },
  {
    "text": "I'd say no, not necessarily. It would depend on, one,\nthe problem that you're working with, the size of the\nstate space and action spaces",
    "start": "368246",
    "end": "375930"
  },
  {
    "text": "that you're dealing with. And later, we'll talk about the\nhybrid planning methods, which are really seeking to\ncombine the strengths",
    "start": "375930",
    "end": "382530"
  },
  {
    "text": "and weaknesses of online\nand offline planning into one approach there. But yeah, that's\na great question.",
    "start": "382530",
    "end": "388070"
  },
  {
    "text": " So connecting this\nreceding horizon planning",
    "start": "388070",
    "end": "394600"
  },
  {
    "text": "back to what we've seen in this\nagent acting in the environment, taking an action,\nreceiving some observation,",
    "start": "394600",
    "end": "402550"
  },
  {
    "text": "and then replanning from there. That's exactly what receding\nhorizon planning is doing. And it's just relating that to\nthis idea of the reachable state",
    "start": "402550",
    "end": "410800"
  },
  {
    "text": "space and planning\npurely over what's reachable from\nthat current state. That's the big idea\nhere with the foundation",
    "start": "410800",
    "end": "418600"
  },
  {
    "text": "of the reachable state space\nand receding horizon planning. And so we're going to be\nbuilding on this foundation",
    "start": "418600",
    "end": "424300"
  },
  {
    "text": "as we go throughout\nthe lecture today. And so now kind of zooming\nout here to the big picture.",
    "start": "424300",
    "end": "431890"
  },
  {
    "text": "A common theme that\nwe're going to see with these online\nplanning methods is that typically\nthe more time we",
    "start": "431890",
    "end": "436960"
  },
  {
    "text": "spend searching or\nplanning forward over different possible futures,\nthe better our resulting policy",
    "start": "436960",
    "end": "442720"
  },
  {
    "text": "is going to perform. And a way to get\nsome intuition here is if you're familiar\nwith this idea of system 1",
    "start": "442720",
    "end": "449560"
  },
  {
    "text": "and system 2 level thinking,\nwhere system 1 thinking is this fast intuitive quick\nreaction style thinking,",
    "start": "449560",
    "end": "457690"
  },
  {
    "text": "and system 2 is more of this\nslow, deliberative thought process where you're thinking\nover possible actions.",
    "start": "457690",
    "end": "464720"
  },
  {
    "text": "You're spending more time\nthinking before acting. And so we're going to\nsee this trade-off here",
    "start": "464720",
    "end": "471460"
  },
  {
    "text": "between the system\n1 and system 2 styles of thinking\nin online planning. And so a great example\nhere connecting this back",
    "start": "471460",
    "end": "477759"
  },
  {
    "text": "to our chess example\nis bullet chess. And so bullet chess would be\nthis analogy to the system 1",
    "start": "477760",
    "end": "485470"
  },
  {
    "text": "type of thinking here where\nin this bullet chess scenario,",
    "start": "485470",
    "end": "490940"
  },
  {
    "text": "it's a version of chess\nwhere each player has 1 minute on the clock to\nuse for the entire game. So this requires the\nplayers to think and move",
    "start": "490940",
    "end": "499000"
  },
  {
    "text": "pretty quickly where\nthey would have to rely on this intuition and\npre-memorized board patterns",
    "start": "499000",
    "end": "505449"
  },
  {
    "text": "as they move pieces\naround quickly. And so that would\nbe this example",
    "start": "505450",
    "end": "511420"
  },
  {
    "text": "of system 1 style of thinking\napplied to chess, where you're just quickly moving from\nboard state to board state",
    "start": "511420",
    "end": "517150"
  },
  {
    "text": "without spending too much\ntime thinking about what is my opponent going to\ndo in response to this.",
    "start": "517150",
    "end": "523020"
  },
  {
    "text": "The other side of this\nis system 2 thinking, where you would now be\nspending more time evaluating",
    "start": "523020",
    "end": "529500"
  },
  {
    "text": "many different possible\nfutures that could occur. So this would be, for example,\nyou're thinking about, OK,",
    "start": "529500",
    "end": "535690"
  },
  {
    "text": "if I move my pawn to d5,\nwhat is my opponent going to play in response to that? And then how am I going to play\nin response to their response?",
    "start": "535690",
    "end": "542910"
  },
  {
    "text": "And then how are they going to\nplay in response to my response? And so rolling this out\nseveral steps into the future",
    "start": "542910",
    "end": "548310"
  },
  {
    "text": "and then going back to\nthe start and saying, now what if instead of moving\nmy pawn first, I move my Bishop?",
    "start": "548310",
    "end": "553403"
  },
  {
    "text": "And how will that\nchange their response? So you're doing many possible\ndifferent evaluations here of this strategy\ninto the future",
    "start": "553403",
    "end": "560009"
  },
  {
    "text": "instead of just doing\nthese quick reactive moves. So that's this difference\nhere between the system 1",
    "start": "560010",
    "end": "566700"
  },
  {
    "text": "and system 2 style of thinking. And so this is directly\nrelated to what",
    "start": "566700",
    "end": "571995"
  },
  {
    "text": "we're going to be talking\nabout now, which is this idea of policy rollouts. And so we're going to use\nthis idea of policy rollouts",
    "start": "571995",
    "end": "579030"
  },
  {
    "text": "here to be able to estimate\nthe utility of a policy. That's where we're\ngoing in this direction.",
    "start": "579030",
    "end": "584480"
  },
  {
    "text": "But first, just what is\na policy rollout here? So in order to perform a policy\nrollout, we need two things.",
    "start": "584480",
    "end": "591520"
  },
  {
    "text": "The first thing that\nwe need is a policy. So recall that we have\nthis policy pi of s.",
    "start": "591520",
    "end": "597130"
  },
  {
    "text": "And this is prescribing\nan action for us to take in some state s.",
    "start": "597130",
    "end": "602270"
  },
  {
    "text": "And so for our rollout policies,\ntypically our rollout policies are going to be\nstochastic, meaning that this is a distribution.",
    "start": "602270",
    "end": "608630"
  },
  {
    "text": "So given some state, we have\na distribution over actions that we will take. So we can sample an action from\nthis stochastic rollout policy.",
    "start": "608630",
    "end": "617959"
  },
  {
    "text": "So that's the first thing that\nwe need to perform a rollout. The second is a model. So we need a model\nof our transitions",
    "start": "617960",
    "end": "624340"
  },
  {
    "text": "and a model of our rewards. So we're going to have some\ntransition function here that tells us, given an action\nin a specific state, what",
    "start": "624340",
    "end": "632500"
  },
  {
    "text": "states am I likely to\ntransition to next. So this allows us to sample\nthese successor state",
    "start": "632500",
    "end": "637690"
  },
  {
    "text": "transitions from this\ntransition model. And then we also have\nour reward model.",
    "start": "637690",
    "end": "643430"
  },
  {
    "text": "So we have this R s,a that gives\nus a reward for taking action a in state s. So this is what we need here to\nperform these policy rollouts.",
    "start": "643430",
    "end": "652690"
  },
  {
    "text": "Once we have these, we can\ngo through our policy rollout example. So looking at this\nboard state that we're",
    "start": "652690",
    "end": "658360"
  },
  {
    "text": "going to be starting from, we're\ngoing to use our rollout policy. And so our rollout policy here\ncould just be a random action.",
    "start": "658360",
    "end": "665060"
  },
  {
    "text": "So it could be just moving\npieces around randomly or maybe we have some of\noffline learn policy",
    "start": "665060",
    "end": "670240"
  },
  {
    "text": "specifically tailored to chess\nthat we're going to use to play. Regardless of what our\nrollout policy looks like,",
    "start": "670240",
    "end": "675740"
  },
  {
    "text": "we're going to walk\nthrough this example. OK, so we have our initial\nstarting board state. We're then going to sample an\naction from our rollout policy.",
    "start": "675740",
    "end": "685010"
  },
  {
    "text": "So our stochastic\nrollout policy is going to be this\ndistribution over actions, and we're just sampling\nan action to take.",
    "start": "685010",
    "end": "691400"
  },
  {
    "text": "Let's say that the action\nwe get is move Knight to e4. So we're going to\nmove our Knight to E4,",
    "start": "691400",
    "end": "697030"
  },
  {
    "text": "and then we're going to sample\na transition from our transition model. And so what we're doing here,\njust to simplify things a bit",
    "start": "697030",
    "end": "706133"
  },
  {
    "text": "because we do have an opponent\nthat we're playing against. So we're baking their\nopponent strategy into the transition model.",
    "start": "706133",
    "end": "712880"
  },
  {
    "text": "So our opponent, for example,\ncould be playing randomly, and we're modeling them\nas just a random agent.",
    "start": "712880",
    "end": "718130"
  },
  {
    "text": "Or we could, again,\nhave some other policy that we've modeled\nfor our opponent.",
    "start": "718130",
    "end": "723350"
  },
  {
    "text": "And that's baked into\nthe transition model. So when we sample a state\nfrom the transition model, we're assuming that's\nincluded the opponents play",
    "start": "723350",
    "end": "730150"
  },
  {
    "text": "inside of that, just to keep\nthings simple in this example. OK, so we sample a state\ntransition from that--",
    "start": "730150",
    "end": "736540"
  },
  {
    "text": "from our transition model. We receive a reward, and then\nwe transition to that new state.",
    "start": "736540",
    "end": "741650"
  },
  {
    "text": "So here, in response to\nus playing Knight to e4, our opponent move\ntheir pawn to d5.",
    "start": "741650",
    "end": "747960"
  },
  {
    "text": "And now we're just going to\nrepeat this from this new state. So we're just going to sample an\naction from our rollout policy.",
    "start": "747960",
    "end": "754710"
  },
  {
    "text": "Now that action is Bishop to d5. So we move our Bishop to\nd5, and we sample an action",
    "start": "754710",
    "end": "760200"
  },
  {
    "text": "from our transition model,\nand we receive a reward. So then we're going to\ntransition to that new state.",
    "start": "760200",
    "end": "765399"
  },
  {
    "text": "Now it looks like our\nopponent played Queen to d3 and took our Bishop. So we're probably\ngoing to receive",
    "start": "765400",
    "end": "770760"
  },
  {
    "text": "a negative reward for that-- Queen to d5, sorry. But yeah, we're still going to\nreceive a negative reward there.",
    "start": "770760",
    "end": "778513"
  },
  {
    "text": "And then this is just\ngoing to continue. So we're just going to sample a\nnew action from this new state and continue on rolling this\npolicy out into the future.",
    "start": "778513",
    "end": "787140"
  },
  {
    "text": "And so this is going to continue\nuntil we reach some depth or horizon here, d.",
    "start": "787140",
    "end": "792200"
  },
  {
    "text": "And that's going to be the\nextent of our policy rollout. And so the result of this\npolicy rollout is a trajectory.",
    "start": "792200",
    "end": "799340"
  },
  {
    "text": "We've received this\ntrajectory represented by tau here of\nstates and actions that we took along\nthis policy rollout.",
    "start": "799340",
    "end": "807330"
  },
  {
    "text": "And we also got\nrewards as we went. So we can keep track of\nthe discounted-- the sum of discounted rewards\nhere that we received",
    "start": "807330",
    "end": "814220"
  },
  {
    "text": "along that rollout trajectory. And so this gives us\nan estimate of how",
    "start": "814220",
    "end": "819430"
  },
  {
    "text": "did our policy do along\nthis specific rollout. And we can repeat this now,\nnot just over one single policy",
    "start": "819430",
    "end": "827520"
  },
  {
    "text": "rollout, but we can do this\nover multiple different policy rollouts. And this is the exact same\nidea that we looked at before",
    "start": "827520",
    "end": "834300"
  },
  {
    "text": "of seeing evaluating multiple\ndifferent possible futures using this policy rollout.",
    "start": "834300",
    "end": "839950"
  },
  {
    "text": "And so, for each one\nof these rollouts, we're going to get\na reward for each of those rollout trajectories.",
    "start": "839950",
    "end": "845500"
  },
  {
    "text": "And that's going to\nallow us now to estimate the utility of following\nthis particular policy.",
    "start": "845500",
    "end": "850550"
  },
  {
    "text": "So all we did was we just\nstarted from this initial state. We followed our\nrollout policy by just",
    "start": "850550",
    "end": "856019"
  },
  {
    "text": "sampling actions and states\nfrom our transition model. We got rewards for doing that. And now we're going to use those\nrewards to estimate the utility",
    "start": "856020",
    "end": "864060"
  },
  {
    "text": "of this particular policy. And that's going to bring us\nto our first online planning algorithm that we're\ngoing to talk about.",
    "start": "864060",
    "end": "870610"
  },
  {
    "text": "But first, how can we\nactually do this estimation? So we saw that\nthe reward that we got along a specific\ntrajectory tau",
    "start": "870610",
    "end": "878160"
  },
  {
    "text": "is just the sum from t equals\n1 up to that rollout depth d.",
    "start": "878160",
    "end": "883589"
  },
  {
    "text": "And then it's just\nthe discounted reward. So we have some discount factor\nand the reward that we received.",
    "start": "883590",
    "end": "890230"
  },
  {
    "text": "So now we want to\nbe able to estimate the utility of our rollout\npolicy that we're using.",
    "start": "890230",
    "end": "895770"
  },
  {
    "text": "So the utility of\nthe rollout policy, we're going to\nwrite as U pi of r. And this r here\nis just to denote",
    "start": "895770",
    "end": "901800"
  },
  {
    "text": "that this is following\nour rollout policy because there's going to be a\ndistinction here in a second. So that r is just saying\nthis is our rollout policy",
    "start": "901800",
    "end": "909480"
  },
  {
    "text": "that we're using. So the utility, when\nfollowing our rollout policy from a specific\nstate, we're just",
    "start": "909480",
    "end": "915330"
  },
  {
    "text": "going to approximate this as\nthe average of the rewards",
    "start": "915330",
    "end": "920400"
  },
  {
    "text": "that we received. So if we performed\nm policy rollouts, then we're just going to average\nup those rewards that we got",
    "start": "920400",
    "end": "926529"
  },
  {
    "text": "along the m policy rollouts. So this is just\ngoing to be 1 over m. And then just all we're doing\nis taking the average here",
    "start": "926530",
    "end": "934600"
  },
  {
    "text": "of those rollout trajectories. So we just performed\nm policy rollouts.",
    "start": "934600",
    "end": "940010"
  },
  {
    "text": "We got m trajectories\ncorresponding to that. And then we're just averaging\nthose rewards there.",
    "start": "940010",
    "end": "946220"
  },
  {
    "text": "And so we're just simply\napproximating the utility from this state following\nour particular rollout policy",
    "start": "946220",
    "end": "951730"
  },
  {
    "text": "using those m policy rollouts.  And so this is going\nto bring us now",
    "start": "951730",
    "end": "958880"
  },
  {
    "text": "to the first online planning\nalgorithm that we're going to be looking at. And that is look\nahead with rollouts.",
    "start": "958880",
    "end": "965370"
  },
  {
    "text": "So the way the look ahead\nwith rollouts algorithm works is kind of just in the name.",
    "start": "965370",
    "end": "970580"
  },
  {
    "text": "We've seen the look ahead\nequation before, which is given by the following.",
    "start": "970580",
    "end": "977040"
  },
  {
    "text": "So we're going to have the\nimmediate reward that we receive plus the discount\nfactor times the sum",
    "start": "977040",
    "end": "983490"
  },
  {
    "text": "over all of the possible states\nthat we could transition to, the probability that we're\ntransitioning into those states,",
    "start": "983490",
    "end": "989590"
  },
  {
    "text": "given that we took\naction a in state s, and then the utility of\nbeing in that new state.",
    "start": "989590",
    "end": "994959"
  },
  {
    "text": "So this is just the\nlook ahead equation that we've seen previously. And we saw that we\ncould extract a policy",
    "start": "994960",
    "end": "1001580"
  },
  {
    "text": "from this look ahead\nequation simply by taking the\nargmax over actions.",
    "start": "1001580",
    "end": "1006720"
  },
  {
    "text": "So that's how we can\nextract the policy from this lookahead equation. So if we just now take the\nargmax here over actions,",
    "start": "1006720",
    "end": "1019550"
  },
  {
    "text": "that's going to give us this\npolicy that we can extract. So now notice the\ndifference here.",
    "start": "1019550",
    "end": "1025949"
  },
  {
    "text": "This is the online\nplanning algorithm policy, the look ahead with rollouts\npolicy that we're using.",
    "start": "1025950",
    "end": "1031289"
  },
  {
    "text": "It's different from\nthe rollout policy. And so what we're\nusing this here for--",
    "start": "1031290",
    "end": "1036310"
  },
  {
    "text": "so look ahead with\nrollouts, we're simply going to be\nestimating this utility. So instead of exactly\ncomputing this utility,",
    "start": "1036310",
    "end": "1043337"
  },
  {
    "text": "we're now just going to\nestimate this utility from state s prime using m policy rollouts.",
    "start": "1043337",
    "end": "1048730"
  },
  {
    "text": "So that's all we're doing. We're just saying\nthat this is going to be replaced by using this\nrollout policy to estimate",
    "start": "1048730",
    "end": "1056370"
  },
  {
    "text": "this utility from state s. So all lookahead with\nrollouts is doing is taking that\nlookahead equation.",
    "start": "1056370",
    "end": "1063450"
  },
  {
    "text": "And we're just simply\nreplacing this utility here and estimating it with\nm policy rollouts using",
    "start": "1063450",
    "end": "1069150"
  },
  {
    "text": "some rollout policy. So that's all that's going on\nin lookahead with rollouts. Question?",
    "start": "1069150",
    "end": "1075100"
  },
  {
    "text": "The original lookahead\nequations was-- that was like [INAUDIBLE].",
    "start": "1075100",
    "end": "1082169"
  },
  {
    "text": "It was like trying to see\nthat, really [INAUDIBLE].",
    "start": "1082170",
    "end": "1088848"
  },
  {
    "text": " Yeah, so the\nquestion was, what's",
    "start": "1088848",
    "end": "1095620"
  },
  {
    "text": "the main difference\nbetween this lookahead with rollouts and the\noriginal lookahead equation?",
    "start": "1095620",
    "end": "1100850"
  },
  {
    "text": "So the original\nlookahead equation-- we would be going over all-- so\nall actions, all possible states",
    "start": "1100850",
    "end": "1106330"
  },
  {
    "text": "we could transition to, and then\nevaluating this U of s prime. So we have this\nrecursive step of then",
    "start": "1106330",
    "end": "1112480"
  },
  {
    "text": "we would have to look\nat the utility of being in state s prime, which\nwould then evaluate",
    "start": "1112480",
    "end": "1117790"
  },
  {
    "text": "to the immediate reward. And so you'd be going\non into the future until you reach some base case\ndepth that you're searching to.",
    "start": "1117790",
    "end": "1124753"
  },
  {
    "text": "So that's one of the\ndifferences, which is now just being wrapped into\nthis policy rollout",
    "start": "1124753",
    "end": "1129850"
  },
  {
    "text": "where the depth that we're\ngoing to in the rollout-- remember, so we have\nthis rollout depth here.",
    "start": "1129850",
    "end": "1134960"
  },
  {
    "text": "So if we're going\nto a depth of 10, that's going to be maybe\na worse approximation",
    "start": "1134960",
    "end": "1140200"
  },
  {
    "text": "than if we were going to a\ndepth of 100 in our rollouts. How can we substitute in the\nutility of the rollout pi",
    "start": "1140200",
    "end": "1151660"
  },
  {
    "text": "when we may not go to\nall the possible states that it actually\ndoes transition to?",
    "start": "1151660",
    "end": "1157550"
  },
  {
    "text": "Yeah, that's a great question. So the question was, how can\nwe use this substitution here",
    "start": "1157550",
    "end": "1163220"
  },
  {
    "text": "when our rollouts might not go\nto all of the possible states that you could transition to?",
    "start": "1163220",
    "end": "1168222"
  },
  {
    "text": "That's a great\nquestion and that's one of the limitations of using\npolicy rollouts to estimate",
    "start": "1168223",
    "end": "1173780"
  },
  {
    "text": "the utility. Is that you might\nnot necessarily visit all of the possible\nstate transitions.",
    "start": "1173780",
    "end": "1179010"
  },
  {
    "text": "So if you're only using a\nlimited number of rollouts, then your variance estimate of\nthe rewards that you receive",
    "start": "1179010",
    "end": "1184340"
  },
  {
    "text": "are going to be high. So the variance will\ndecrease with the number of rollout policies\nthat you're performing.",
    "start": "1184340",
    "end": "1190440"
  },
  {
    "text": "So that's sort of--\nthe trade off is that-- one of the limitations\nhere is that you have to do this\nover all actions,",
    "start": "1190440",
    "end": "1197550"
  },
  {
    "text": "all next state transitions\nthat you could go to and then perform m policy rollouts\nin each of those.",
    "start": "1197550",
    "end": "1204720"
  },
  {
    "text": "So if you have a large state\nand large action space, this is going to get a little\ncomputationally intensive there.",
    "start": "1204720",
    "end": "1210460"
  },
  {
    "text": " And then the other\nlimitation here",
    "start": "1210460",
    "end": "1217100"
  },
  {
    "text": "is that it also requires\nhaving that roll out policy to begin with. So if your roll out\npolicy to start with",
    "start": "1217100",
    "end": "1223460"
  },
  {
    "text": "is a random roll out\npolicy, then you're likely going to improve\nupon that random roll",
    "start": "1223460",
    "end": "1229100"
  },
  {
    "text": "out policy because you're\nacting greedily with respect to that roll out policy. But just improving over\na random roll out policy",
    "start": "1229100",
    "end": "1236480"
  },
  {
    "text": "might not be that good in the\ngrand scheme of things applying that to your actual problem. Because if you're just\ndoing better than random,",
    "start": "1236480",
    "end": "1242850"
  },
  {
    "text": "sure, you're improving,\nbut you might not be performing that well\nin the actual problem.",
    "start": "1242850",
    "end": "1248220"
  },
  {
    "text": "So those are some\nof the drawbacks here of the lookahead\nwith roll outs method.",
    "start": "1248220",
    "end": "1254210"
  },
  {
    "text": "So that's going to bring us\nto our next online planning algorithm, which\nis forward search.",
    "start": "1254210",
    "end": "1259639"
  },
  {
    "text": "And so forward\nsearch is focused-- instead of just\napproximating the utility",
    "start": "1259640",
    "end": "1265160"
  },
  {
    "text": "by using a few roll outs\nfrom a roll out policy, forward search is looking at\nbuilding the entire search",
    "start": "1265160",
    "end": "1271010"
  },
  {
    "text": "tree of all states and action\ntransitions up to some depth. And so that's the way that\nwe're able to determine now",
    "start": "1271010",
    "end": "1278690"
  },
  {
    "text": "the best action to take\nfrom an initial state by looking at all possible\ntransitions that could occur.",
    "start": "1278690",
    "end": "1284100"
  },
  {
    "text": "And so this visualization\nhere that we're showing is for three states and two\nactions up to a depth of 2.",
    "start": "1284100",
    "end": "1291370"
  },
  {
    "text": "And so when we talk\nabout depth here in these tree search\nbased methods,",
    "start": "1291370",
    "end": "1296990"
  },
  {
    "text": "what the depth means\nis we're starting at some initial\nstate shown by s. And then going to a depth of 1\nwould be this first branching",
    "start": "1296990",
    "end": "1305000"
  },
  {
    "text": "on the states. So that's depth 1--\nand then depth two would be at the\nbottom here doing",
    "start": "1305000",
    "end": "1310100"
  },
  {
    "text": "that next level of branching. And so we're going\nto be talking about",
    "start": "1310100",
    "end": "1315110"
  },
  {
    "text": "the computational complexity\nhere of these algorithms. And for forward\nsearch, we're going",
    "start": "1315110",
    "end": "1321650"
  },
  {
    "text": "to be looking at first\nthe branching factor. And so the branching\nfactor for forward search is going to be the size of\nthe state space times the size",
    "start": "1321650",
    "end": "1331850"
  },
  {
    "text": "of the action space. And so what this branching\nfactor here is telling us is how quickly is\nthis tree going",
    "start": "1331850",
    "end": "1338240"
  },
  {
    "text": "to grow as we go down in depth. So every time we add on\nanother depth to this tree,",
    "start": "1338240",
    "end": "1344790"
  },
  {
    "text": "we're going to grow\nthat by a factor of the size of the state's space\ntimes the size of the action space.",
    "start": "1344790",
    "end": "1350310"
  },
  {
    "text": "The reason for\nthat is because you can see in this\nvisualization that we are branching on all of the\nstates and all of the actions.",
    "start": "1350310",
    "end": "1357150"
  },
  {
    "text": "So that's the factor by which\nthis tree is going to grow. So that's this idea\nof branching factor.",
    "start": "1357150",
    "end": "1362840"
  },
  {
    "text": "And the computational complexity\nof the forward search algorithm is directly related to this\nidea of branching factor.",
    "start": "1362840",
    "end": "1368820"
  },
  {
    "text": "So the computational\ncomplexity here, using this big O\nnotation is just",
    "start": "1368820",
    "end": "1373950"
  },
  {
    "text": "the size of the state space, the\nsize of the action space up to-- raised to the power of d so\nthe depth of the search tree",
    "start": "1373950",
    "end": "1382140"
  },
  {
    "text": "that we're going to. And if you haven't seen\nthis big O notation here before for\ncomputational complexity,",
    "start": "1382140",
    "end": "1387941"
  },
  {
    "text": "you can think about it. This is just a way that we\ncan compare algorithms and get a sense of how much\nwork do we expect",
    "start": "1387942",
    "end": "1393630"
  },
  {
    "text": "to have to do when we're\nperforming this specific search algorithm. So a good way to look at\nthis is in our example.",
    "start": "1393630",
    "end": "1402000"
  },
  {
    "text": "We have three\nstates, two actions. So 3 times 2 to a\ndepth of 1 is 6.",
    "start": "1402000",
    "end": "1407560"
  },
  {
    "text": "And you can see at\nthat first step, we have six nodes in that\nlayer of the search week.",
    "start": "1407560",
    "end": "1413130"
  },
  {
    "text": "Going to a depth of 2,\nwe would have 3 times 2-- 6 raised to the power\nof 2, which would be 36.",
    "start": "1413130",
    "end": "1419320"
  },
  {
    "text": "And there are 36 states\nthat we've now branched on in that second depth. And so this would continue.",
    "start": "1419320",
    "end": "1424600"
  },
  {
    "text": "And you can see that this even\nwith only three states and two actions, this is going\nto blow up quite quickly,",
    "start": "1424600",
    "end": "1429940"
  },
  {
    "text": "going to a depth of 10 or so. And so that is-- one of the limitations\nhere of forward search",
    "start": "1429940",
    "end": "1437040"
  },
  {
    "text": "is that while it's nice\nto be able to construct the entire search tree and get\nthis perfect solution to it,",
    "start": "1437040",
    "end": "1445350"
  },
  {
    "text": "we're limited here by\nthe branching factor. s times a is not great,\nespecially when we're",
    "start": "1445350",
    "end": "1451110"
  },
  {
    "text": "working with real-world problems\nthat have extremely large state spaces and action spaces.",
    "start": "1451110",
    "end": "1456520"
  },
  {
    "text": "And then you're raising that\nto the power of the depth that you're going to. So it's just-- it becomes\nnot super practical",
    "start": "1456520",
    "end": "1463440"
  },
  {
    "text": "in some large\nreal-world problems. And so that's where the branch\nand bound algorithm comes in",
    "start": "1463440",
    "end": "1469230"
  },
  {
    "text": "is it's attempting to reduce\nthis exponential complexity in forward search by reasoning\nabout bounds on the value",
    "start": "1469230",
    "end": "1477990"
  },
  {
    "text": "function. And so branch and\nbound requires knowing a lower bound on the value\nfunction and an upper bound",
    "start": "1477990",
    "end": "1485520"
  },
  {
    "text": "on the action value function. And so with these bounds\non the-- with the upper",
    "start": "1485520",
    "end": "1491640"
  },
  {
    "text": "and lower bounds here\nwhat this allows us to do is it allows us to prune\nparts of the search tree.",
    "start": "1491640",
    "end": "1497120"
  },
  {
    "text": "And so pruning is\ngoing to occur when the upper bound of\nan action at a state",
    "start": "1497120",
    "end": "1502260"
  },
  {
    "text": "is lower than the lower bound\nof a previously explored action from that state.",
    "start": "1502260",
    "end": "1507330"
  },
  {
    "text": "I think when you see\nit written like this, you're like, I have no\nidea what that is saying or why that would be the case.",
    "start": "1507330",
    "end": "1513460"
  },
  {
    "text": "So I think it's helpful\nto see an example and then come back\nto that statement and it will make a\nlittle bit more sense.",
    "start": "1513460",
    "end": "1520110"
  },
  {
    "text": "But essentially what this\nis saying-- so first, just looking at this,\nwe have this upper bound on the action value function.",
    "start": "1520110",
    "end": "1526575"
  },
  {
    "text": " So this is saying this is the\nbest possible that we could ever",
    "start": "1526575",
    "end": "1532420"
  },
  {
    "text": "hope to achieve by taking\naction a in state s. So we could never do any\nbetter than this upper bound",
    "start": "1532420",
    "end": "1538810"
  },
  {
    "text": "estimate here. And so what we're\nsaying here is maybe we've explored a different\npart of the search tree already",
    "start": "1538810",
    "end": "1545560"
  },
  {
    "text": "and we've gotten some\nutility estimate that we're going to call U best here. And so we used our\nlower bound estimate",
    "start": "1545560",
    "end": "1552490"
  },
  {
    "text": "to come up with\nthis best utility estimate on some side\nof the search tree that we've already explored.",
    "start": "1552490",
    "end": "1558667"
  },
  {
    "text": "Now, what we're\nsaying is we're coming to some new part of the search\ntree we haven't yet seen. And we're looking at an action\nfrom that particular state.",
    "start": "1558667",
    "end": "1566010"
  },
  {
    "text": "If this upper bound here,\nmeaning the best possible I could ever hope\nto achieve by taking this action in this state, is\nless than that best utility",
    "start": "1566010",
    "end": "1574887"
  },
  {
    "text": "that I've already seen so far\non some other side of the search tree, then it makes no sense to\ntake this action from that state",
    "start": "1574887",
    "end": "1581130"
  },
  {
    "text": "because we know for\ncertain we could never-- we could never do\nany better than what we've already seen over there.",
    "start": "1581130",
    "end": "1586900"
  },
  {
    "text": "So if our upper bound\nestimate is less than the best that we've already seen,\ndon't waste our time",
    "start": "1586900",
    "end": "1593100"
  },
  {
    "text": "exploring that part\nof the search tree because we know we're not\ngoing to do any better. That's the whole intuition\nhere in branch and bound.",
    "start": "1593100",
    "end": "1600130"
  },
  {
    "text": "And so how this works, just\nwith this visual example here-- this is that same\nsearch tree that we",
    "start": "1600130",
    "end": "1607080"
  },
  {
    "text": "looked at for forward search. So let's say that\nwe've already explored this left side of\nthe search tree",
    "start": "1607080",
    "end": "1612299"
  },
  {
    "text": "and we haven't done any\npruning in this case. So we've explored\nthat whole left side,",
    "start": "1612300",
    "end": "1617950"
  },
  {
    "text": "and we get some best utility\nestimate using our lower bound on the value function.",
    "start": "1617950",
    "end": "1623380"
  },
  {
    "text": "So we get our best\nutility estimate from that left side\nof the search tree. And now, we're going to look\nat the right side of the search",
    "start": "1623380",
    "end": "1629350"
  },
  {
    "text": "tree. So we're looking at\nthis initial action to take from that initial state. And we're going to ask, is the\nupper bound estimate here--",
    "start": "1629350",
    "end": "1636440"
  },
  {
    "text": "so the upper bound on\nour Q value function, meaning the best\npossible we could ever hope to achieve on this\nright side of the search tree",
    "start": "1636440",
    "end": "1642340"
  },
  {
    "text": "is that less than the best\nutility I saw on the left side? And if it is, then\nwe're just going",
    "start": "1642340",
    "end": "1647889"
  },
  {
    "text": "to prune off the rest\nof that search tree and not bother wasting our\ntime exploring anything",
    "start": "1647890",
    "end": "1652929"
  },
  {
    "text": "that lies on that left side. Question? When you calculate,\nU best versus",
    "start": "1652930",
    "end": "1659400"
  },
  {
    "text": "the Q value of\nthis [? product, ?] how far down are you\ngoing in [INAUDIBLE]?",
    "start": "1659400",
    "end": "1666790"
  },
  {
    "text": " Yeah, great question. So the question was,\nwhen you're calculating",
    "start": "1666790",
    "end": "1672985"
  },
  {
    "text": "U best utility estimate versus\nthe upper bound on the action",
    "start": "1672985",
    "end": "1679270"
  },
  {
    "text": "value function, how far down in\nthe search tree are you going? And so to get U best,\nwe are going all the way",
    "start": "1679270",
    "end": "1685929"
  },
  {
    "text": "down to the depth\nof the search tree. So we're going to\nthe very base level. And once you get here,\nyou have some-- that's",
    "start": "1685930",
    "end": "1692710"
  },
  {
    "text": "where the lower bound comes in. So this is where you would\nsay, OK, beyond this, I'm going to estimate the\nutility at this state using",
    "start": "1692710",
    "end": "1700480"
  },
  {
    "text": "that lower bound estimate. And then that gets\nsent back up the tree. And that's why you end up with\nU best being a lower bound",
    "start": "1700480",
    "end": "1706913"
  },
  {
    "text": "is because you use this lower\nbound estimate at the bottom of the search tree. The upper bound estimate,\nthough, on the Q function",
    "start": "1706913",
    "end": "1713650"
  },
  {
    "text": "is only looking at\nthat state action pair. So that's where\nthis would require you to have some of offline\npriori knowledge of what",
    "start": "1713650",
    "end": "1721540"
  },
  {
    "text": "that upper bound is going to be. Does that make sense? Was there another question?",
    "start": "1721540",
    "end": "1728260"
  },
  {
    "text": "No. OK.  So right.",
    "start": "1728260",
    "end": "1733470"
  },
  {
    "text": "One of the drawbacks\nhere of branch and bound is that it requires knowing\nthese upper and lower bounds.",
    "start": "1733470",
    "end": "1739020"
  },
  {
    "text": "And where do you get these? Well, you have to come\nup with them yourself. And so in some game\ntype environments,",
    "start": "1739020",
    "end": "1745080"
  },
  {
    "text": "you probably have a reasonable\nestimate of an upper and lower bound on\nthe value function.",
    "start": "1745080",
    "end": "1751090"
  },
  {
    "text": "But in some real world\nproblems where you're actually having to go and learn\nyour reward function itself from data,\nyou might not really",
    "start": "1751090",
    "end": "1758190"
  },
  {
    "text": "have a good\nunderstanding of what are the bounds on this actual\nfunction that I'm working with. So that's one of\nthe drawbacks here,",
    "start": "1758190",
    "end": "1764559"
  },
  {
    "text": "is that if you don't\nhave good bounds, the worst case computational\ncomplexity of branch and bound is going to be the exact\nsame as forward search.",
    "start": "1764560",
    "end": "1772320"
  },
  {
    "text": "The reason for that is\nbecause if your bounds aren't that tight, meaning\nyou have loose bounds",
    "start": "1772320",
    "end": "1777330"
  },
  {
    "text": "and they're really\nconservative estimates, you're not going to do any\npruning of the search tree. And so if you don't do any\npruning of the search tree,",
    "start": "1777330",
    "end": "1784480"
  },
  {
    "text": "you just end up with\nback into forward search. And so if you're interested in\nseeing of worked out example",
    "start": "1784480",
    "end": "1791560"
  },
  {
    "text": "on a dice rolling game\nhere, we posted that online that you can look at.",
    "start": "1791560",
    "end": "1796810"
  },
  {
    "text": "And it also walks through\nthe code that's in the book and relates those two to the\nexample, if you want to do that.",
    "start": "1796810",
    "end": "1803750"
  },
  {
    "text": "And if you're watching online\nlater that will be up for you as well. ",
    "start": "1803750",
    "end": "1810450"
  },
  {
    "text": "All right. So that brings us now\nto our next algorithm, which is sparse sampling. And so sparse sampling\nis trying to get",
    "start": "1810450",
    "end": "1817530"
  },
  {
    "text": "around this\ncomputational complexity that we have right now\nof S times A to the D. So that's really the goal\nhere of sparse sampling,",
    "start": "1817530",
    "end": "1824470"
  },
  {
    "text": "is reducing this branching\nfactor of forward search and branch and bound. And the way that it does\nthis is instead of branching",
    "start": "1824470",
    "end": "1831840"
  },
  {
    "text": "on all of the\npossible next states, it's only going to consider\na limited number of samples",
    "start": "1831840",
    "end": "1837450"
  },
  {
    "text": "of that state transition. And so when we're\ndoing this, we're now just taking samples\nof that state transition.",
    "start": "1837450",
    "end": "1843910"
  },
  {
    "text": "So we're not branching\non all possible states. And as a result, we now\nhave this approximation that's introduced.",
    "start": "1843910",
    "end": "1849730"
  },
  {
    "text": "So sparse sampling\nis introducing this approximate sampling\nbased method into the search.",
    "start": "1849730",
    "end": "1855640"
  },
  {
    "text": "And so what this looks\nlike visually here is this is our original search\ntree from forward search.",
    "start": "1855640",
    "end": "1861910"
  },
  {
    "text": "And what sparse\nsampling is going to do is that instead of branching\non all of the states, it's just going to look\nat samples of that state",
    "start": "1861910",
    "end": "1868950"
  },
  {
    "text": "transition. So we're still branching\non all of the actions, but now we're only\ntaking in this example",
    "start": "1868950",
    "end": "1874060"
  },
  {
    "text": "two samples of the\nstate transition, and those are shown in\nthose dotted lines there. So we're sampling\ntwo state transitions",
    "start": "1874060",
    "end": "1881140"
  },
  {
    "text": "instead of branching on all\nof the state transitions that could occur. And so as a result\nof this, we end up",
    "start": "1881140",
    "end": "1886900"
  },
  {
    "text": "with a computational complexity\nhere of this would be-- is everyone able\nto see this board?",
    "start": "1886900",
    "end": "1894770"
  },
  {
    "text": "Yeah. OK, cool. So we end up with this\ncomputational complexity now. Instead of the size\nof the state space,",
    "start": "1894770",
    "end": "1900750"
  },
  {
    "text": "we're only taking m samples\nhere of the state transition. So now we have a\nfactor of m times A",
    "start": "1900750",
    "end": "1906590"
  },
  {
    "text": "to the d, where\npresumably m is smaller than A otherwise\nwe wouldn't really",
    "start": "1906590",
    "end": "1912380"
  },
  {
    "text": "be using sparse sampling here. OK, so we've reduced our\ncomputational complexity now from this S times A to the\nd to m times A to the d,",
    "start": "1912380",
    "end": "1920390"
  },
  {
    "text": "using this sampling\nbased approach. ",
    "start": "1920390",
    "end": "1927100"
  },
  {
    "text": "The drawback here\nof this algorithm is that you now rely\non, well, how many samples are you going to take.",
    "start": "1927100",
    "end": "1932920"
  },
  {
    "text": "If your state space\nis 10 to the minus 20 and you're only taking\nfive samples of that state",
    "start": "1932920",
    "end": "1938710"
  },
  {
    "text": "transition, you're not going to\nget a very good understanding of how your transitioning\nbased on the actions",
    "start": "1938710",
    "end": "1944498"
  },
  {
    "text": "that you're taking. So now you have this trade off\ndo I take too many samples, do I not take enough samples,\nhow do I balance that trade off.",
    "start": "1944498",
    "end": "1951760"
  },
  {
    "text": "And so just summarizing\nthe algorithms that we've seen so far we've\nseen forward search, branch and bound, and sparse sampling.",
    "start": "1951760",
    "end": "1958102"
  },
  {
    "text": "Forward search and\nbranch and bound have a computational complexity\nof S times A to the d.",
    "start": "1958102",
    "end": "1963190"
  },
  {
    "text": "And I think a good\nway to remember this is that S times\nA to the d spells sad.",
    "start": "1963190",
    "end": "1968260"
  },
  {
    "text": "And that is a sad branching\nfactor that you have because it's just\ngoing to blow up as you go further down\nin the search tree.",
    "start": "1968260",
    "end": "1975400"
  },
  {
    "text": "Sparse sampling then simplifies\nthis right to m times A to the d. But now you're a\nlittle mad that you've",
    "start": "1975400",
    "end": "1981490"
  },
  {
    "text": "had to use the sampling\nbased techniques to get this approximation there. So I think that's a good way to\nremember the differences here",
    "start": "1981490",
    "end": "1989360"
  },
  {
    "text": "in the algorithms is that\nforward search, branch and bound make you sad. Sparse sampling makes you mad.",
    "start": "1989360",
    "end": "1996320"
  },
  {
    "text": "And now to just gauge\nour understanding here. If we use sparse\nsampling with m--",
    "start": "1996320",
    "end": "2003180"
  },
  {
    "text": "so the number of samples here\nequal to the size of the state space, is that equivalent\nto forward search?",
    "start": "2003180",
    "end": "2009200"
  },
  {
    "text": "Any ideas? Yes. No, because you could just\nbe sampling the same action over again.",
    "start": "2009200",
    "end": "2015220"
  },
  {
    "text": "That's exactly right. So looking at this, the answer\nis no, they're not equivalent.",
    "start": "2015220",
    "end": "2021580"
  },
  {
    "text": "And you might be\ntempted to say, well, if we're taking m here equal\nto the size of the state space,",
    "start": "2021580",
    "end": "2029770"
  },
  {
    "text": "well, they have the same\ncomputational complexity, so they must be the same, right. But that's not the case\nbecause what could happen",
    "start": "2029770",
    "end": "2036490"
  },
  {
    "text": "is that in our\nbranching that occurs. We have this onto\nthe action and then",
    "start": "2036490",
    "end": "2042250"
  },
  {
    "text": "we're sampling these\npossible state transitions. You could end up\nsampling the same state",
    "start": "2042250",
    "end": "2048919"
  },
  {
    "text": "from that particular\naction multiple times. It's not necessarily\nthe case that you're going to get a unique state\ntransition sample every time.",
    "start": "2048920",
    "end": "2056550"
  },
  {
    "text": "In fact, if you have a\nreally large state space, it's very unlikely\nthat you're going to build up, almost\nimpossible that you're",
    "start": "2056550",
    "end": "2062539"
  },
  {
    "text": "going to build up the search\ntree, exactly uniquely with all of those state\nsamples that you're taking.",
    "start": "2062540",
    "end": "2068280"
  },
  {
    "text": "So it's possible that you could\nget super lucky and you sample a unique state transition\nevery single time,",
    "start": "2068280",
    "end": "2075330"
  },
  {
    "text": "but that's very unlikely\nin a large state space. ",
    "start": "2075330",
    "end": "2081299"
  },
  {
    "text": "OK. So that brings us now to\nMonte Carlo Tree Search. And Monte Carlo Tree\nSearch, I would say,",
    "start": "2081300",
    "end": "2087638"
  },
  {
    "text": "is a pretty popular\nonline planning algorithm for a few different reasons. One of the main ones being that\nit's practical and somewhat",
    "start": "2087639",
    "end": "2095638"
  },
  {
    "text": "scalable to larger problems. So the way that Monte\nCarlo Tree Search works",
    "start": "2095639",
    "end": "2101039"
  },
  {
    "text": "is by running\nthese m simulations from the current state. So that's how it's\nattempting to avoid",
    "start": "2101040",
    "end": "2106980"
  },
  {
    "text": "this exponential complexity\nis by running m simulations, sort of like these rollouts.",
    "start": "2106980",
    "end": "2112120"
  },
  {
    "text": "But we'll talk about\nhow they're different. And so just like in\nour policy rollouts,",
    "start": "2112120",
    "end": "2117570"
  },
  {
    "text": "we require this model of\nthe transition function and the reward function\nduring the tree search.",
    "start": "2117570",
    "end": "2123650"
  },
  {
    "text": "So we have to have this\nmodel in order to run MCTS. And during the\nsimulations, we're",
    "start": "2123650",
    "end": "2129930"
  },
  {
    "text": "going to be keeping\ntrack of two things here. So the first one that we're\ngoing to be keeping track of",
    "start": "2129930",
    "end": "2136700"
  },
  {
    "text": "is the Q function. And so we're just going to as\nwe build up this search tree,",
    "start": "2136700",
    "end": "2142470"
  },
  {
    "text": "we're just going to have this\nestimate of the Q function here. And then the other thing\nwe're going to be tracking",
    "start": "2142470",
    "end": "2149180"
  },
  {
    "text": "is just the counts. So this function\nhere, Nsa, is just counting the number of times\nwe're taking action in state s.",
    "start": "2149180",
    "end": "2157410"
  },
  {
    "text": "So if we take\naction a in state s, we're just going to\nincrement this count by 1. So we're purely just counting\nevery time we take this action.",
    "start": "2157410",
    "end": "2164990"
  },
  {
    "text": "And we can\nequivalently also keep track of the number\nof times we visited a state by just marginalizing\nout over the actions here.",
    "start": "2164990",
    "end": "2171900"
  },
  {
    "text": "So if we just take the sum\nover the actions of Nsa, we're just tracking\nnow the number",
    "start": "2171900",
    "end": "2177530"
  },
  {
    "text": "of times we've been in a state. So that's all we're tracking\nhere in MCTS as we build up the tree is this estimate\nof the Q function",
    "start": "2177530",
    "end": "2184400"
  },
  {
    "text": "and the number of\ntimes we've taken an action in a particular state. ",
    "start": "2184400",
    "end": "2190720"
  },
  {
    "text": "And then at the end, after we've\nbuilt up the search tree here, one method that we\ncan use is let's",
    "start": "2190720",
    "end": "2195910"
  },
  {
    "text": "just choose the action that\nmaximizes the Q function. There's other\nversions that you can use could choose one that\nmaximizes the action that you",
    "start": "2195910",
    "end": "2204160"
  },
  {
    "text": "saw the most number of times. So whatever maximize\nthe counts, you can do a combination of\na weighting between the Q",
    "start": "2204160",
    "end": "2210160"
  },
  {
    "text": "function and the counts. There's different combinations. To keep it simple\nhere, let's just say that we're going to do\nthe max for the Q function.",
    "start": "2210160",
    "end": "2217980"
  },
  {
    "text": "OK, so how MCTS works\nat the high level here before diving\ninto an example is",
    "start": "2217980",
    "end": "2223620"
  },
  {
    "text": "we have these four\ndifferent steps. So the first step is selection. We have to select an action\nthat we want to expand further.",
    "start": "2223620",
    "end": "2232060"
  },
  {
    "text": "So this is saying,\nwhich action do we think is promising\nto look at more and build up the search\ntree in that direction more?",
    "start": "2232060",
    "end": "2240120"
  },
  {
    "text": "So we'll talk about how we\nactually choose actions here in a second, what we're actually\nusing to select these actions.",
    "start": "2240120",
    "end": "2247029"
  },
  {
    "text": "But for now, let's say that\nwe selected this action in the tree. The next step\ninvolves expansion.",
    "start": "2247030",
    "end": "2252340"
  },
  {
    "text": "So expansion is purely just\nsampling that state transition after taking that action.",
    "start": "2252340",
    "end": "2257470"
  },
  {
    "text": "So we just select the\naction and then we sample a state transition, and\nthat is the expansion step.",
    "start": "2257470",
    "end": "2263620"
  },
  {
    "text": "The next step is\nthe simulation step. So this could involve\nusing a rollout policy.",
    "start": "2263620",
    "end": "2269130"
  },
  {
    "text": "So if we used a\nrandom rollout policy, this would involve just\nperforming 10 random actions",
    "start": "2269130",
    "end": "2274210"
  },
  {
    "text": "into the future and estimating\nthe utility at that state there. You don't have to use a\nrollout policy, though.",
    "start": "2274210",
    "end": "2280370"
  },
  {
    "text": "There's other methods\nthat you can use. For example, you\ncould use an offline learn neural network that\nestimates the utility of being",
    "start": "2280370",
    "end": "2287320"
  },
  {
    "text": "in a particular state. And then in this\nsimulation step, you're just querying that\nneural network of saying, hey,",
    "start": "2287320",
    "end": "2292819"
  },
  {
    "text": "what's the utility\nestimate in that state? And using that instead\nof the rollout policy. So I bring that up\nbecause we're going",
    "start": "2292820",
    "end": "2299170"
  },
  {
    "text": "to talk about hybrid\nplanning methods later. And I just want you\nto have that sense of, where does that fit\nin to this MCTS setup?",
    "start": "2299170",
    "end": "2306590"
  },
  {
    "text": "So you don't necessarily have\nto use a rollout policy here. For the example that we're\ngoing to be looking at, we will.",
    "start": "2306590",
    "end": "2313460"
  },
  {
    "text": "And then after the\nsimulation step, you're just taking\nthat utility estimate and you're updating\nit back up the tree.",
    "start": "2313460",
    "end": "2319940"
  },
  {
    "text": "And so then you're just\nrepeating this process over and over of selection,\nexpansion, simulation,",
    "start": "2319940",
    "end": "2325400"
  },
  {
    "text": "and update. And that is MCTS\nat the high level.",
    "start": "2325400",
    "end": "2330730"
  },
  {
    "text": "So there's a few\nthings that we need to talk about here\nin a bit more detail. First is, how are we actually\ndoing this action selection?",
    "start": "2330730",
    "end": "2339260"
  },
  {
    "text": "So there's different exploration\nstrategies that we can use. A pretty popular one\nand commonly used",
    "start": "2339260",
    "end": "2347120"
  },
  {
    "text": "is the upper confidence bound. So the upper\nconfidence bound metric is just the action value\nfunction plus this constant C.",
    "start": "2347120",
    "end": "2359430"
  },
  {
    "text": "So this is a hyperparameter\nhere, the exploration constant. And then it's times this\nlog of the number of times",
    "start": "2359430",
    "end": "2367730"
  },
  {
    "text": "we've been in a particular\nstate over the number of times-- just the counts here that\nwe took action a in state s.",
    "start": "2367730",
    "end": "2374539"
  },
  {
    "text": "So what is this telling us? Well, it's sort\nof incentivizing-- this right term is\nincentivizing us to take actions",
    "start": "2374540",
    "end": "2381830"
  },
  {
    "text": "that we haven't yet seen. How is it doing that? Well, notice if the\ncount function here is--",
    "start": "2381830",
    "end": "2388160"
  },
  {
    "text": "so if we've never taken an\naction in a particular state, this is going to be 0. So this right side term\nis going to be infinity.",
    "start": "2388160",
    "end": "2395130"
  },
  {
    "text": "And so this is going to\nforce us to choose actions that we haven't tried yet. So this is like the\nexploration term.",
    "start": "2395130",
    "end": "2402120"
  },
  {
    "text": "We're being incentivized\nto try actions that we haven't tried yet. This term here is more\nof the exploitation term.",
    "start": "2402120",
    "end": "2409200"
  },
  {
    "text": "So this is telling\nus to take actions that we've seen from\nparticular states that have showed us to perform well.",
    "start": "2409200",
    "end": "2415920"
  },
  {
    "text": "So if we've seen it in\nthe past to perform well, this is giving us that\nincentive to try it again.",
    "start": "2415920",
    "end": "2421740"
  },
  {
    "text": "And so that's how the\nupper confidence bound is balancing this exploration\nwith the exploitation term.",
    "start": "2421740",
    "end": "2427359"
  },
  {
    "text": "And it's doing that\nthrough this constant C. So that's weighting\nhow much we're valuing the exploration\nbonus versus how much we're",
    "start": "2427360",
    "end": "2434430"
  },
  {
    "text": "valuing the exploitation term. So that's the strategy here. Yes.",
    "start": "2434430",
    "end": "2440049"
  },
  {
    "text": "So to clarify, we start off\nwith an action value function. And then we just keep\nupdating that as we go?",
    "start": "2440050",
    "end": "2445510"
  },
  {
    "text": "Yes. Because our end goal here is to\nget a fully updated action value function so we know\nwhat policy to perform?",
    "start": "2445510",
    "end": "2452635"
  },
  {
    "text": "What was the last\npart of the question? Is the end goal to get\nan updated action value function so we know what\npolicy we should [INAUDIBLE]?",
    "start": "2452635",
    "end": "2460190"
  },
  {
    "text": "I see. Yeah. Yeah. So the question\nwas, is the end goal of doing all this to get an\nupdated estimate of the action",
    "start": "2460190",
    "end": "2466740"
  },
  {
    "text": "value function so that we\nknow which policy to use? And yes, that is the ultimate\ngoal is that at the end,",
    "start": "2466740",
    "end": "2473512"
  },
  {
    "text": "after building up\nthis search tree, we want to take the action. In this case, we're using one\nthat's greedy with respect",
    "start": "2473512",
    "end": "2479490"
  },
  {
    "text": "to the action value function. But like we mentioned,\nyou don't necessarily have to do that version of it. But yeah, the goal here is to\nbuild up this Q estimate here",
    "start": "2479490",
    "end": "2489450"
  },
  {
    "text": "to get it as good as possible to\nthe actual real-world problem. ",
    "start": "2489450",
    "end": "2495855"
  },
  {
    "text": "So that's the selection step. We're using this weighting\nbetween the exploration",
    "start": "2495855",
    "end": "2500910"
  },
  {
    "text": "term and the exploitation term. And so this is a common\ntrade-off that we have of, should we try out actions\nthat we haven't seen yet?",
    "start": "2500910",
    "end": "2508300"
  },
  {
    "text": "Or should we go with what we've\nalready seen to be promising? And so how MCTS\nis balancing this, if you're using the\nupper confidence bound",
    "start": "2508300",
    "end": "2515250"
  },
  {
    "text": "is through this term here\nbetween that exploitation term and the exploration term.",
    "start": "2515250",
    "end": "2523440"
  },
  {
    "text": "So I think it helps to\nsee an actual example here to get a sense of\nhow MCTS works.",
    "start": "2523440",
    "end": "2529480"
  },
  {
    "text": "So we're going to be looking at\n2048 to build up this example. And if you haven't played 2048,\nit's a pretty simple game.",
    "start": "2529480",
    "end": "2536860"
  },
  {
    "text": "So we'll walk through it here. Initially, you're playing\non this 4 by 4 board. And the game starts\noff with a 2 or a 4",
    "start": "2536860",
    "end": "2544320"
  },
  {
    "text": "randomly appearing\nsomewhere on the board. And you then have four\nactions that you can take.",
    "start": "2544320",
    "end": "2549640"
  },
  {
    "text": "So you can move left,\nright, up, or down. And if you played\nit on the phone, you're literally just\nswiping left, up, down.",
    "start": "2549640",
    "end": "2555130"
  },
  {
    "text": "So if you move to the left here,\nwhat happens is all of the tiles are going to slide to the\nleft and then a new 2 or 4",
    "start": "2555130",
    "end": "2562410"
  },
  {
    "text": "is going to appear\nsomewhere else on the board. So if you move left again,\nthe tiles will slide left",
    "start": "2562410",
    "end": "2568320"
  },
  {
    "text": "and then a 2 or a 4 again is\nrandomly generated somewhere on the board. And so the goal of\nthe game here is",
    "start": "2568320",
    "end": "2574260"
  },
  {
    "text": "to get the tiles\nto add up to 2048. And so the way that\nyou add tiles up is just by combining like tiles.",
    "start": "2574260",
    "end": "2580330"
  },
  {
    "text": "So if you slide down\nnow, the 2 and the 2 are going to combine\nand add up to four.",
    "start": "2580330",
    "end": "2585380"
  },
  {
    "text": "And so that's how you\nget the tiles to add up. Now if we were to swipe left\nagain, we would get the 4",
    "start": "2585380",
    "end": "2590560"
  },
  {
    "text": "and the 4 combine to make 8. And you just keep doing this\nuntil you add up to 2048.",
    "start": "2590560",
    "end": "2596380"
  },
  {
    "text": "And so the general\nstrategy here is that you want to\nkeep your high value tiles in one of the corners.",
    "start": "2596380",
    "end": "2601837"
  },
  {
    "text": "It doesn't matter which corner\nyou're keeping those high value tiles in because you\nhave a symmetric board, but that's roughly\nthe general strategy.",
    "start": "2601837",
    "end": "2609319"
  },
  {
    "text": "So looking at how MCTS\nwould build this up starting from that\ninitial board state.",
    "start": "2609320",
    "end": "2614640"
  },
  {
    "text": "So we open up the game. This is the first board\nstate that we see, a 2 or a 4 is randomly generated\non that board.",
    "start": "2614640",
    "end": "2621240"
  },
  {
    "text": "In the first step here of MCTS,\nwe're in this selection step, but we haven't seen\nany actions, right?",
    "start": "2621240",
    "end": "2627920"
  },
  {
    "text": "So we're just going to\ninitialize everything to 0. So all of the four\nactions available to us, we're just initializing\nthose to 0.",
    "start": "2627920",
    "end": "2635690"
  },
  {
    "text": "And that ends the first step. It was just purely\ninitialization here. We can't really do\nanything after that.",
    "start": "2635690",
    "end": "2640920"
  },
  {
    "text": "So now in the second\nstep, we're going to come back to these actions\nthat we've initialized and we're going to look at our\nupper confidence bound metric.",
    "start": "2640920",
    "end": "2647930"
  },
  {
    "text": "And we're going to\nsay, OK, we want to select the one with the\nhighest upper confidence bound. But in this case, they all\nhave the same number of counts,",
    "start": "2647930",
    "end": "2654810"
  },
  {
    "text": "they all have the\nsame Q estimate, so we're just going\nto arbitrarily select the left action because it's\nthe first one in our ordering,",
    "start": "2654810",
    "end": "2662250"
  },
  {
    "text": "because all of\nthese, they would all have an exploration\nbonus here of infinity. So we're just arbitrarily going\nto choose left in this example.",
    "start": "2662250",
    "end": "2670310"
  },
  {
    "text": "So we've done our\nselection step. Now we are going to go\nto the expansion step.",
    "start": "2670310",
    "end": "2675420"
  },
  {
    "text": "And remember, the\nexpansion step is just sampling a state transition\nfrom our transition function. So you notice we slid\nthe tiles to the left",
    "start": "2675420",
    "end": "2682910"
  },
  {
    "text": "and then a new 2 or 4 was\nrandomly generated somewhere on the board. So that's our sample of\nthe state transition.",
    "start": "2682910",
    "end": "2689970"
  },
  {
    "text": "And then we've initialized all\nof the counts and Q estimates from that new\nsuccessor state to 0.",
    "start": "2689970",
    "end": "2695640"
  },
  {
    "text": "So now we go into the simulation\nstep, and in this example, we're going to use a\nrandom rollout policy here.",
    "start": "2695640",
    "end": "2701887"
  },
  {
    "text": "So we're just going to\nsay for this example, we're taking 10 random\nactions into the future. So this is literally just\ngoing to be randomly swiping",
    "start": "2701887",
    "end": "2708690"
  },
  {
    "text": "left, right, up, down down\nfor 10 actions into the future and that will bring us\nto a new board state.",
    "start": "2708690",
    "end": "2714660"
  },
  {
    "text": "So we've kept track\nof the rewards that we received along\nthis random rollout. And so from that, we get\nthis utility estimate of 72",
    "start": "2714660",
    "end": "2722910"
  },
  {
    "text": "from this simulation step. And we're now going to\nupdate that back up the tree.",
    "start": "2722910",
    "end": "2728380"
  },
  {
    "text": "So we're going to update our\nvisitation counts now to one. And then our Q estimate\njust becomes 72",
    "start": "2728380",
    "end": "2734530"
  },
  {
    "text": "since this is the only\nutility estimate we've seen from that state so far. And that would conclude\nthis first expansion",
    "start": "2734530",
    "end": "2741960"
  },
  {
    "text": "of this left action. So now we're going to go back. We finished these four steps. We're going to go back up to\nthe top of the search tree,",
    "start": "2741960",
    "end": "2748773"
  },
  {
    "text": "and now we're back in\nthe selection step. So in the second\nselection, which action should I select now?",
    "start": "2748773",
    "end": "2754290"
  },
  {
    "text": "Well, now we actually have a\nvalue for the upper confidence bound here for the left action.",
    "start": "2754290",
    "end": "2759760"
  },
  {
    "text": "And we're using exploration\nconstant there of 100. So this would evaluate to 72.",
    "start": "2759760",
    "end": "2765850"
  },
  {
    "text": "But notice for the down,\nright, and up actions, those are all going to, again, be\njust infinite exploration bonus.",
    "start": "2765850",
    "end": "2773050"
  },
  {
    "text": "And so, again, we're\njust arbitrarily going to select the down action there\nsince we haven't seen it yet",
    "start": "2773050",
    "end": "2778380"
  },
  {
    "text": "and they all have that same\ninfinite exploration bonus. So we select down. We're now going to expand\non the down action.",
    "start": "2778380",
    "end": "2785340"
  },
  {
    "text": "And so we sample a\nstate transition. You can see we slid\nthe tiles down, new 2 or 4 appeared\nsomewhere on the board.",
    "start": "2785340",
    "end": "2792960"
  },
  {
    "text": "And so that concludes\nour expansion step. And now we just\nforward-simulate, so we take 10 random actions\nfrom this new successor state.",
    "start": "2792960",
    "end": "2802560"
  },
  {
    "text": "That brings us to\nthis new board state. And we kept track of\nthe rewards that we received along this rollout.",
    "start": "2802560",
    "end": "2809610"
  },
  {
    "text": "And so that gives us\nthis utility estimate of 44, which we're then going\nto update back up the tree.",
    "start": "2809610",
    "end": "2816340"
  },
  {
    "text": "And so we update\nour visitation count by 1 and our estimate of 44.",
    "start": "2816340",
    "end": "2821470"
  },
  {
    "text": "And now this would just repeat. We've done this\nfor left and down. You can see that right and\nup are both going down,",
    "start": "2821470",
    "end": "2827815"
  },
  {
    "text": "that have that infinite\nexploration bonus still. So in the next two\nsteps, we're just first going to choose\nthe right action,",
    "start": "2827815",
    "end": "2833809"
  },
  {
    "text": "do the exact same thing we\njust did, and then we're going to choose the\nup action after that and do the exact same thing.",
    "start": "2833810",
    "end": "2839420"
  },
  {
    "text": "So after doing right and up\nfollowing that same procedure, we're going to end up with the\nsearch tree looking something",
    "start": "2839420",
    "end": "2844720"
  },
  {
    "text": "like this. And now it gets\ninteresting again, because now, which action\nare we going to choose?",
    "start": "2844720",
    "end": "2851120"
  },
  {
    "text": "Any thoughts? Up. Up? Yeah, that is correct. We're going to\nchoose the up action.",
    "start": "2851120",
    "end": "2857110"
  },
  {
    "text": "Why are we choosing\nthe up action? Well, remember that we have the\nupper confidence bound here,",
    "start": "2857110",
    "end": "2862180"
  },
  {
    "text": "and all of the counts for\nthe specific actions are one. They all have the same\ncounts, so they all",
    "start": "2862180",
    "end": "2868540"
  },
  {
    "text": "have the same exploration bonus. So we're not concerned with\nexploration in this case.",
    "start": "2868540",
    "end": "2873830"
  },
  {
    "text": "It's purely looking at an\nexploitative step of which one has the highest Q estimate.",
    "start": "2873830",
    "end": "2879020"
  },
  {
    "text": "So now we're saying, I\nwant to continue exploring, continue pulling the action\nthat has the highest Q estimate",
    "start": "2879020",
    "end": "2885950"
  },
  {
    "text": "because that's what I saw\nto be best in the past. So we're going to choose\nthe up action now.",
    "start": "2885950",
    "end": "2891930"
  },
  {
    "text": "And we're going to now perform\nexpansion on this up action. But in this case, when\nwe sample a new state,",
    "start": "2891930",
    "end": "2898410"
  },
  {
    "text": "it's very unlikely that we\nwould see the exact same state that we already saw\nbecause that 2 or a 4 is randomly being generated.",
    "start": "2898410",
    "end": "2904542"
  },
  {
    "text": "So it's not likely\nthat we're going to see that same exact state\nand we end up with a new state transition here.",
    "start": "2904542",
    "end": "2910800"
  },
  {
    "text": "So we're now going to\nperform that simulation step from this new\nstate that we've seen. That's going to bring\nus to this board state",
    "start": "2910800",
    "end": "2917550"
  },
  {
    "text": "and we get this particular\nutility estimate of 44 that we're then going to\nupdate back up the tree.",
    "start": "2917550",
    "end": "2924730"
  },
  {
    "text": "And so now we're just taking an\naverage of the utility estimates that we've seen by\ntaking this up action.",
    "start": "2924730",
    "end": "2930670"
  },
  {
    "text": "So the first time we got 88,\nthe second time we got 44, the average of those two is 66.",
    "start": "2930670",
    "end": "2936380"
  },
  {
    "text": "And then we've also updated\nthe visitation counts by 1, which gets us to 2. Question.",
    "start": "2936380",
    "end": "2941460"
  },
  {
    "text": "Can you just say again\nwhat conceptually Q is. Yes. So the question is, what is Q?",
    "start": "2941460",
    "end": "2948119"
  },
  {
    "text": "Q is the action\nvalue function, which tells us the utility of taking\nan action in a particular state.",
    "start": "2948120",
    "end": "2957070"
  },
  {
    "text": "So the Q function that we've\nseen, if we write it out",
    "start": "2957070",
    "end": "2962770"
  },
  {
    "text": "explicitly, it's just\nthe immediate reward that we get plus the\ndiscount factor times the sum",
    "start": "2962770",
    "end": "2970599"
  },
  {
    "text": "over all states that\nwe could transition to, the transition\nprobability, and then",
    "start": "2970600",
    "end": "2976869"
  },
  {
    "text": "the utility of being\nin that next state. So we are passing in\nthe action to take here.",
    "start": "2976870",
    "end": "2983900"
  },
  {
    "text": "And so that's\nforcing that forward with that particular action. ",
    "start": "2983900",
    "end": "2991390"
  },
  {
    "text": "Any other questions? Yes. In the context of this\ngame, 2048, how exactly is reward formulated based\non the board configuration?",
    "start": "2991390",
    "end": "2999050"
  },
  {
    "text": "Yeah, that's a great question. So the question\nwas, for 2048, how is the reward being formulated\nhere based on the board state?",
    "start": "2999050",
    "end": "3007390"
  },
  {
    "text": "So there's a couple\nexamples in the book showing different weighting functions\nthat you can do on the tiles.",
    "start": "3007390",
    "end": "3013829"
  },
  {
    "text": "So one example is that you\ncan have higher weights in the corners to\nincentivize your agent",
    "start": "3013830",
    "end": "3019950"
  },
  {
    "text": "to put all of the high value\ntiles in those corners. You can do an even weighting. So this example is just\nusing an even weighting",
    "start": "3019950",
    "end": "3026340"
  },
  {
    "text": "and just summing up\nthe tiles on the board. But yeah, there's different\nheuristics that you can use,",
    "start": "3026340",
    "end": "3032100"
  },
  {
    "text": "or you can use just an end\nstate of the game of 1 or 0, did you win?",
    "start": "3032100",
    "end": "3037482"
  },
  {
    "text": "That's a little bit more\nchallenging because then you have to search for a\nreally long depth horizon",
    "start": "3037482",
    "end": "3042810"
  },
  {
    "text": "to get any signal there. But yeah, there's\ndifferent utility functions",
    "start": "3042810",
    "end": "3048080"
  },
  {
    "text": "you can use there.  OK, great.",
    "start": "3048080",
    "end": "3053570"
  },
  {
    "text": "So after doing\nthis update, we're left with this search tree. And this process would just\nliterally repeat over and over",
    "start": "3053570",
    "end": "3061900"
  },
  {
    "text": "until you've either run out\nof number of simulations to perform. So in MCTS, you set the\nnumber of simulations",
    "start": "3061900",
    "end": "3068470"
  },
  {
    "text": "that you're performing. You can also specify\na time limit. So if you only want the\nalgorithm to run for one minute,",
    "start": "3068470",
    "end": "3074269"
  },
  {
    "text": "you can do that. And whatever the Q estimates\nare after that one minute are returned and then you can\nmake your action based on that.",
    "start": "3074270",
    "end": "3082700"
  },
  {
    "text": "So it's an any time\nalgorithm, meaning you can stop it whenever, and\nthe estimates that are returned are what you end up with.",
    "start": "3082700",
    "end": "3088660"
  },
  {
    "text": "Yes.  So I guess for the\nup action, the reason",
    "start": "3088660",
    "end": "3094770"
  },
  {
    "text": "why we have 2 is because we just\nrandomly generate another one. Is that right? Yeah. So the question is for the up\naction, why are the counts two?",
    "start": "3094770",
    "end": "3103460"
  },
  {
    "text": "[INAUDIBLE] So why do we have two\nboards for this [INAUDIBLE]?",
    "start": "3103460",
    "end": "3111170"
  },
  {
    "text": "Yeah. So the question is, why do\nwe have two new board States. And that was because we selected\nthe up action to expand on.",
    "start": "3111170",
    "end": "3118800"
  },
  {
    "text": "And in that expansion, we\nsample a new state transition. And so in 2048,\nit's very unlikely",
    "start": "3118800",
    "end": "3124402"
  },
  {
    "text": "that we would end up\nwith the exact same state that we saw the first\ntime because every time we do that state sample, a 2\nor a 4 can randomly appear",
    "start": "3124403",
    "end": "3131990"
  },
  {
    "text": "in any of those open squares. So it's very unlikely that\nwe would get a 2 showing up in that exact bottom left square\nthat we saw the first time.",
    "start": "3131990",
    "end": "3139880"
  },
  {
    "text": "So that's why we end up\nwith this new state that's different from the\nfirst one that we saw.",
    "start": "3139880",
    "end": "3145090"
  },
  {
    "text": "And also, I guess with Monte\nCarlo Tree Search, is it usual--",
    "start": "3145090",
    "end": "3151260"
  },
  {
    "text": "or would you always\nexpect to have gone through every single\naction at least once just because of the upper\nbound or the [INAUDIBLE].",
    "start": "3151260",
    "end": "3159950"
  },
  {
    "text": "Yeah. Yeah. Yeah. So the question was\nwith MCTS, would you expect to always go through\nevery single action at least",
    "start": "3159950",
    "end": "3168010"
  },
  {
    "text": "once? And if you are using the\nupper confidence bound, then the answer is\nyes with a caveat",
    "start": "3168010",
    "end": "3175190"
  },
  {
    "text": "because if you don't\nhave-- like say you have a really large\naction space and you only have a limited\nnumber of iterations,",
    "start": "3175190",
    "end": "3181707"
  },
  {
    "text": "you might not be\nable to branch on all of those actions\nin those iterations that you're performing. But if you have\nenough iterations",
    "start": "3181707",
    "end": "3187760"
  },
  {
    "text": "and you're using upper\nconfidence bound, then the answer would\nbe yes because you'll have this infinite\nexploration bonus.",
    "start": "3187760",
    "end": "3193838"
  },
  {
    "text": "And so you're always going\nto choose an action that you haven't yet seen relative to-- I mean, unless you have\nan infinite Q value maybe.",
    "start": "3193838",
    "end": "3200670"
  },
  {
    "text": "But then it's like, well,\nwhat's your Q value? Which isn't really reasonable. So yes, with a star.",
    "start": "3200670",
    "end": "3207540"
  },
  {
    "text": "Yeah. Can you explain how the name\nupper confidence bound actually relates to the expression\nthat you showed us.",
    "start": "3207540",
    "end": "3213670"
  },
  {
    "text": "Yes. So the question is, where does\nthe name upper confidence bound come from? And you'll see that.",
    "start": "3213670",
    "end": "3219630"
  },
  {
    "text": "I think it's in\nchapter 15 when we talk about multiarmed\nbanded problems. And so upper confidence bound\nis an exploration strategy",
    "start": "3219630",
    "end": "3227040"
  },
  {
    "text": "that was originally developed\nfor those multiarmed banded. So how are you choosing\nwhich lever of the multiarmed",
    "start": "3227040",
    "end": "3234510"
  },
  {
    "text": "banded to pull. And I don't want to get too\nahead of ourselves skipping to that chapter, but\nessentially, it's",
    "start": "3234510",
    "end": "3240660"
  },
  {
    "text": "coming from that\nmulti-armed banded problem. Yes. Is one of the\ndrawbacks that we have",
    "start": "3240660",
    "end": "3246450"
  },
  {
    "text": "to keep track of\neach possible state, and as you continue to go deeper\ninto your number of steps,",
    "start": "3246450",
    "end": "3252220"
  },
  {
    "text": "that takes up a huge\namount of memory? Yeah. So the question was,\nis one of the drawbacks of this algorithm\nthat you have to keep",
    "start": "3252220",
    "end": "3258300"
  },
  {
    "text": "track of each possible state\nthat you could transition to? And I would say\nthat is a drawback",
    "start": "3258300",
    "end": "3266927"
  },
  {
    "text": "relative to the other algorithms\nthat we've seen though, like forward search branch and\nbound where you're doing that, branching on all of\nthe state transitions,",
    "start": "3266927",
    "end": "3273430"
  },
  {
    "text": "this remedies that and you\nonly have to store those state transitions once you see them.",
    "start": "3273430",
    "end": "3278560"
  },
  {
    "text": "So you're not storing\nevery possible state that you could transition to. You're just keeping\ntrack of it as you go.",
    "start": "3278560",
    "end": "3285030"
  },
  {
    "text": "But what we'll see next-- let me just show you\nthis really quick.",
    "start": "3285030",
    "end": "3290890"
  },
  {
    "text": "So if you ran this MCTS\nalgorithm here on 2048 for 100 simulations, this\nis the resulting search tree",
    "start": "3290890",
    "end": "3298500"
  },
  {
    "text": "that you would end up with. And here, the blue is\nshowing higher Q values.",
    "start": "3298500",
    "end": "3304840"
  },
  {
    "text": "Red is showing lower Q values. And you can see that\nafter 100 simulations, we have a pretty\nshallow search tree.",
    "start": "3304840",
    "end": "3310840"
  },
  {
    "text": "And this goes back to your\nquestion of the reason that that's occurring\nis because we have all of these\nsuccessor states",
    "start": "3310840",
    "end": "3317280"
  },
  {
    "text": "that we could transition to. In 2048, it's very unlikely that\nwe would see that same successor",
    "start": "3317280",
    "end": "3323760"
  },
  {
    "text": "state again. So we have this branching\nthat's occurring on the actions, and we're ending\nup with the tree",
    "start": "3323760",
    "end": "3330060"
  },
  {
    "text": "growing wider before\nit gets any deeper. And so that's one of\nthe limitations when you have this large\nbranching factor still,",
    "start": "3330060",
    "end": "3336540"
  },
  {
    "text": "your tree, even after\ntaking all of these samples, like the furthest we've gotten\nhere is just a depth of 3. ",
    "start": "3336540",
    "end": "3344400"
  },
  {
    "text": "[INAUDIBLE]  Sorry. [INAUDIBLE] to overcome that\nit would grow more deeper than",
    "start": "3344400",
    "end": "3352569"
  },
  {
    "text": "[INAUDIBLE]. Yeah, that's a great point. So the point was that you\ncould change the value of C",
    "start": "3352570",
    "end": "3358110"
  },
  {
    "text": "here to overcome this\nshallowness in the search. And that's exactly right. So if you want to value\nthe exploration terms less,",
    "start": "3358110",
    "end": "3366690"
  },
  {
    "text": "then you could do that. You're still going to\nhave the infinite count issue on that first step.",
    "start": "3366690",
    "end": "3373210"
  },
  {
    "text": "But there's ways\naround that as well if you want to enforce\ngoing deeper in the tree. ",
    "start": "3373210",
    "end": "3382200"
  },
  {
    "text": "OK. And then just really quickly\nwrapping up this example, if we were to continue\non-- we're not. I think we get the\nidea here of MCTS.",
    "start": "3382200",
    "end": "3389359"
  },
  {
    "text": "But if we were going to\ncontinue now, which action would we select after this? ",
    "start": "3389360",
    "end": "3397180"
  },
  {
    "text": "Left? Yes, that is correct. We would select left. The reason for that\nis because if we",
    "start": "3397180",
    "end": "3404140"
  },
  {
    "text": "look at left, down,\nand right, they all have the same exploration\nbonus because they have the same number of counts.",
    "start": "3404140",
    "end": "3409970"
  },
  {
    "text": "And so we're looking at\nthat pure exploitation, so we'd select 72. But now looking\nat left versus up,",
    "start": "3409970",
    "end": "3416000"
  },
  {
    "text": "we would see that left\nhas the higher exploration bonus because it's\nbeen visited less,",
    "start": "3416000",
    "end": "3421130"
  },
  {
    "text": "and then it also has\nthe higher Q estimate. So it's winning out on both of\nthose, and we'd go with left.",
    "start": "3421130",
    "end": "3426150"
  },
  {
    "text": " All right. So now just to make sure\nwe're all on the same page",
    "start": "3426150",
    "end": "3434190"
  },
  {
    "text": "here, how this would work if\nwe're playing with an autonomous agent using this MCTS\napproach in 2048,",
    "start": "3434190",
    "end": "3440500"
  },
  {
    "text": "the real board state that\nwould be in the real world is shown on the right. And the agent sees\nthis board state,",
    "start": "3440500",
    "end": "3446490"
  },
  {
    "text": "and it then goes and\ndoes all of this-- not offline\ncomputation, but it's doing the computation in its\nhead of building up this search",
    "start": "3446490",
    "end": "3454230"
  },
  {
    "text": "tree. And so all of these\nforward simulation and rollouts that\nare occurring are not happening in the actual game.",
    "start": "3454230",
    "end": "3460590"
  },
  {
    "text": "It's based on the agent's\nmodel of the game. And so the whole search tree is\ngenerated inside of the agent",
    "start": "3460590",
    "end": "3466680"
  },
  {
    "text": "internally. And then at the end, the agent\nis going to say, OK, take the left action. We take the left\naction, and that's",
    "start": "3466680",
    "end": "3472380"
  },
  {
    "text": "what changes the actual\nreal-world board state. And then the process\nrepeats again after that.",
    "start": "3472380",
    "end": "3478930"
  },
  {
    "text": "So just clarifying\nthe difference there between where the state\ntransitions and everything is",
    "start": "3478930",
    "end": "3484470"
  },
  {
    "text": "occurring is all being\nsimulated by the agent and then it's acting in\nthat real-world environment",
    "start": "3484470",
    "end": "3490400"
  },
  {
    "text": "after that. And so-- yes. Question. You said the process\nrepeats after that.",
    "start": "3490400",
    "end": "3497320"
  },
  {
    "text": "You still maintain\nthe same tree, but you continue from the\nactual state you're in. Yeah.",
    "start": "3497320",
    "end": "3502360"
  },
  {
    "text": "So the question was, when the\nprocess repeats after that, so when you transition\nto a new state,",
    "start": "3502360",
    "end": "3507740"
  },
  {
    "text": "do you keep the\nsame search tree? And the answer\nis, if you're just doing vanilla-based\nimplementation of MCTS,",
    "start": "3507740",
    "end": "3515510"
  },
  {
    "text": "you might not necessarily\nhave that caching occurring of the counts and\nthe Q estimates. But if you're doing a\nefficient implementation,",
    "start": "3515510",
    "end": "3523279"
  },
  {
    "text": "you should be storing\nthat as you continue on. So there's a few different\nvariants here to mention.",
    "start": "3523280",
    "end": "3531340"
  },
  {
    "text": "Just the tree search depth\nand the rollout depth, those are two different things. So in the rollout\ndepth in that example,",
    "start": "3531340",
    "end": "3537470"
  },
  {
    "text": "we were doing a\nrollout depth of 10, just taking 10 random actions. You could also\ninstead do rollouts",
    "start": "3537470",
    "end": "3544660"
  },
  {
    "text": "until you reach\nan end game state, so until there's an outcome of\na winner or a loser in the game. So that's one\nvariant of the tree",
    "start": "3544660",
    "end": "3552010"
  },
  {
    "text": "search versus rollout depth. There's also that\nvalue function estimate that I mentioned earlier. You don't necessarily have\nto use the rollouts again.",
    "start": "3552010",
    "end": "3559400"
  },
  {
    "text": "You can use that offline\nlearned estimate. And that would be this\nhybrid planning style that we're going to\nbe talking about next.",
    "start": "3559400",
    "end": "3566150"
  },
  {
    "text": "The exploration strategy\nthat you're using, you don't have to use\nupper confidence bound. And the source of the\nmodel here is a big one.",
    "start": "3566150",
    "end": "3573740"
  },
  {
    "text": "MCTS really only\nmakes sense when you have a good understanding of\nthe transition and reward model.",
    "start": "3573740",
    "end": "3580049"
  },
  {
    "text": "Because if you don't have a good\nunderstanding of those models, then you're going to be wasting\nall of this time building up",
    "start": "3580050",
    "end": "3586100"
  },
  {
    "text": "this search tree with all of\nthese state transitions that aren't actually\nrealistic for the problem",
    "start": "3586100",
    "end": "3591200"
  },
  {
    "text": "that you're interested. So when you're evaluating,\nshould I use MCTS or not, it comes back to, well, do\nI have a good understanding",
    "start": "3591200",
    "end": "3598430"
  },
  {
    "text": "of that transition reward model? If the answer is no,\nthen you probably shouldn't be using this\nmodel-based tree search here.",
    "start": "3598430",
    "end": "3605190"
  },
  {
    "text": "So that's a big problem when\nyou're looking at applying this to real-world problems. Yes.",
    "start": "3605190",
    "end": "3611300"
  },
  {
    "text": "On that topic, with\nthat transition model, like when we're in the\nsimulation step of the 2048",
    "start": "3611300",
    "end": "3617330"
  },
  {
    "text": "game, it sounded like we\nwere doing like a depth-- yeah, a depth [? where ?]\nwe're simulating",
    "start": "3617330",
    "end": "3623540"
  },
  {
    "text": "10 steps in the future with\ntotally stochastic actions like after we've chosen\nthe initial action. So is that in our\ntransition model",
    "start": "3623540",
    "end": "3629990"
  },
  {
    "text": "then just like 0.25,\n0.25, 0.25, 0.25 like up, down, left, right after\nwe've chosen that action?",
    "start": "3629990",
    "end": "3637620"
  },
  {
    "text": "Yeah. OK. So the question was about\nthe transition model we were using in the rollouts.",
    "start": "3637620",
    "end": "3643549"
  },
  {
    "text": "In the rollouts, I guess\nwe have a difference between the rollout policy,\nwhich that would be 0.25, 0.25,",
    "start": "3643550",
    "end": "3649760"
  },
  {
    "text": "0.25 probability for each\naction that you could take. But then the state\ntransitions that occur",
    "start": "3649760",
    "end": "3655549"
  },
  {
    "text": "as a result of those random\nrollouts random action selection is going to be based\non our game simulator",
    "start": "3655550",
    "end": "3662180"
  },
  {
    "text": "that we have internally to us. So how we sample\nthat state transition is going to be based on that.",
    "start": "3662180",
    "end": "3668210"
  },
  {
    "text": "Where are the random 2\nor 4 pops up on the board would be T, and then which\ndirection that we go in would be",
    "start": "3668210",
    "end": "3674059"
  },
  {
    "text": "[INAUDIBLE]. Exactly. Yeah. Yep. So just to clarify, if\nyou couldn't hear that, it was where you're going\nis or where the 2 or the 4",
    "start": "3674060",
    "end": "3683230"
  },
  {
    "text": "is being generated, that's\nyour transition model. And then the action\nthat you're selecting is your rollout policy.",
    "start": "3683230",
    "end": "3689119"
  },
  {
    "text": "Yes. So we're not running this\nexploration/exploitation step on the future rollout steps.",
    "start": "3689120",
    "end": "3695710"
  },
  {
    "text": "That's correct. You're not using\nthe upper confidence bound in the rollouts. Yeah.",
    "start": "3695710",
    "end": "3702770"
  },
  {
    "text": "Yes. And then [INAUDIBLE] gap that we\nsaw in the example, [INAUDIBLE] that we reached.",
    "start": "3702770",
    "end": "3708089"
  },
  {
    "text": "Yep. And then the rollout depth\nwould be that 10 that we made. That's correct. Yes. So the question was about the\ntree search depth and rollout",
    "start": "3708090",
    "end": "3714950"
  },
  {
    "text": "depth. So in the example, it\nwas rollout depth of 10. And then the tree\nsearch depth that we showed after running\n100 simulations was--",
    "start": "3714950",
    "end": "3722210"
  },
  {
    "text": "that we got to was\nthree at that point. ",
    "start": "3722210",
    "end": "3727299"
  },
  {
    "text": "And then one just to\nmention at the bottom there is progressive widening. So we saw that issue with the\nshallow search tree occurring.",
    "start": "3727300",
    "end": "3735940"
  },
  {
    "text": "So one method was to reduce\nthe exploration constant. Another way is that you can use\nthis progressive widening, which",
    "start": "3735940",
    "end": "3742410"
  },
  {
    "text": "is limiting the number\nof branches that can occur on your\nstates or your actions",
    "start": "3742410",
    "end": "3747700"
  },
  {
    "text": "so that you artificially force\nthe tree to go deeper before it gets any wider. So that's another method\nthat you can use there.",
    "start": "3747700",
    "end": "3755050"
  },
  {
    "text": "So this is just to\nshow you that there's quite a few different\nvariations that you can implement with MCTS.",
    "start": "3755050",
    "end": "3761680"
  },
  {
    "text": "And that's another reason that\nit's a versatile algorithm to use.",
    "start": "3761680",
    "end": "3767250"
  },
  {
    "text": "All right. And so now connecting this to\nsome of the hybrid planning methods, I think most of you\nhave probably at least heard",
    "start": "3767250",
    "end": "3774750"
  },
  {
    "text": "of AlphaGo from DeepMind. And there's several\ndifferent versions of AlphaGo that have been released.",
    "start": "3774750",
    "end": "3780490"
  },
  {
    "text": "There was the original\nand then AlphaGo Zero, which is what we're going\nto briefly talk about today. And then AlphaZero\ncame later, which",
    "start": "3780490",
    "end": "3786869"
  },
  {
    "text": "was a generalization of the\nalgorithm to other games as well. And so AlphaGo Zero achieved\nsuperhuman performance",
    "start": "3786870",
    "end": "3795180"
  },
  {
    "text": "in the game of Go. And the way that it did this\nwas through this hybrid planning approach of an offline\nlearn neural network",
    "start": "3795180",
    "end": "3801510"
  },
  {
    "text": "in combination with\nMCTS and self-play. So the way the\nself-play step worked",
    "start": "3801510",
    "end": "3806640"
  },
  {
    "text": "was essentially having the\nagent play against itself. So starting with this initial\nboard state shown by S 1,",
    "start": "3806640",
    "end": "3813970"
  },
  {
    "text": "the agent performs\nMonte Carlo Tree Search using the offline learn\nneural network in the tree search.",
    "start": "3813970",
    "end": "3819400"
  },
  {
    "text": "It produces this policy\nfrom the tree search and then samples an action to\nplay based on that tree search",
    "start": "3819400",
    "end": "3825630"
  },
  {
    "text": "that it performed. In the next state now,\nit's the opponent's turn. And the opponent is going\nto play the exact same way",
    "start": "3825630",
    "end": "3831819"
  },
  {
    "text": "that the first agent did. So using the same offline learn\nneural network, same MCTS setup,",
    "start": "3831820",
    "end": "3837760"
  },
  {
    "text": "it builds up the search tree\nand then plays an action sampled from that policy. And this process\nof the self-play",
    "start": "3837760",
    "end": "3843309"
  },
  {
    "text": "just continues until\nthe end game is reached. And at that point, we have this\nindication of who won the game.",
    "start": "3843310",
    "end": "3849440"
  },
  {
    "text": "So that Z variable is telling\nus which of the agents won. So now we have this\nself play demonstration.",
    "start": "3849440",
    "end": "3856190"
  },
  {
    "text": "And we can use this then\nto update the offline learn neural network. So how that network\nset up was basically",
    "start": "3856190",
    "end": "3863590"
  },
  {
    "text": "it takes as input the raw\nboard state, so that S1, and then it outputs two things. So the first output is this\nvector of action probabilities,",
    "start": "3863590",
    "end": "3872000"
  },
  {
    "text": "so that's the P1. And that's telling\nyou the probability of selecting each action.",
    "start": "3872000",
    "end": "3877380"
  },
  {
    "text": "And then the second output\nis the value estimate. And the value estimate there is\njust mapping to the probability",
    "start": "3877380",
    "end": "3884310"
  },
  {
    "text": "that the current agent\nends up winning the game. So that's this idea\nof what is, the value",
    "start": "3884310",
    "end": "3889530"
  },
  {
    "text": "of being in a particular\nstate in this case? And so you can now do\nthis right for each state",
    "start": "3889530",
    "end": "3895020"
  },
  {
    "text": "that you saw during the\nself-play iterations and update the\nnetwork parameters based on those examples that you\nsaw during the self-play step.",
    "start": "3895020",
    "end": "3904160"
  },
  {
    "text": "And so the outcome there,\nZ, of who won the game is then passed back through to\nall of these different training",
    "start": "3904160",
    "end": "3909359"
  },
  {
    "text": "examples. And you're trying to match\nthe action probabilities to what was produced by\nthe tree search setup.",
    "start": "3909360",
    "end": "3915579"
  },
  {
    "text": "And so you just\nrepeat this process of self-play and then\nnetwork parameter updates.",
    "start": "3915580",
    "end": "3920950"
  },
  {
    "text": "And just by doing that\nrepetitive fashion of this self-play\nand network update,",
    "start": "3920950",
    "end": "3926980"
  },
  {
    "text": "AlphaGo Zero was able to\nachieve superhuman performance. So just looking at some\nof the results here, you can see relative to\nprevious versions of AlphaGo",
    "start": "3926980",
    "end": "3935340"
  },
  {
    "text": "and then AlphaGo Zero. And here the y-axis is\nshowing the Elo rating,",
    "start": "3935340",
    "end": "3940700"
  },
  {
    "text": "which is a way of\nmeasuring human performance in these games. And the cool thing\nhere is that if you",
    "start": "3940700",
    "end": "3946000"
  },
  {
    "text": "look at just the raw network,\nso just the raw offline learn neural network shown\non the far left,",
    "start": "3946000",
    "end": "3952000"
  },
  {
    "text": "and if you play just with\nrespect to that network, so just the action probabilities that\nwere output and the value",
    "start": "3952000",
    "end": "3958300"
  },
  {
    "text": "estimates and playing\nwith respect to those, you're actually doing\nquite a bit worse than the AlphaGo Zero algorithm.",
    "start": "3958300",
    "end": "3964850"
  },
  {
    "text": "And so mapping this back to what\nwe talked about at the start with that system 1,\nsystem 2 level analogy",
    "start": "3964850",
    "end": "3971740"
  },
  {
    "text": "and extending this analogy,\nthat's kind of a system 1 style approach where\nyou're just quickly,",
    "start": "3971740",
    "end": "3976990"
  },
  {
    "text": "greedily taking this intuitive\nboard state into account and then playing with\nrespect to that intuition.",
    "start": "3976990",
    "end": "3983210"
  },
  {
    "text": "Whereas when you add\non this system 2 level style of thinking, of looking\nat multiple possible different",
    "start": "3983210",
    "end": "3988630"
  },
  {
    "text": "actions, using this\nintuition to guide you, you're able to get quite a\nbit of performance boost.",
    "start": "3988630",
    "end": "3995020"
  },
  {
    "text": "And so still\ncurrently, no one yet has trained a raw neural network\nthat is superhuman in Go.",
    "start": "3995020",
    "end": "4001480"
  },
  {
    "text": "And it's definitely possible\nthat this happens soon. But as of right now,\nno one has done it yet.",
    "start": "4001480",
    "end": "4006569"
  },
  {
    "text": "DeepMind released a\npaper earlier this year that achieved grandmaster\nperformance in chess",
    "start": "4006570",
    "end": "4011790"
  },
  {
    "text": "without using search, but\nhasn't been done yet for Go. Certainly not saying\nit's not possible.",
    "start": "4011790",
    "end": "4017440"
  },
  {
    "text": "It probably will be done\neventually, but just not yet. Yes. Question.",
    "start": "4017440",
    "end": "4022776"
  },
  {
    "text": "So the plane by itself\nisn't just making samples. It's like [INAUDIBLE]\nwhen you're actually",
    "start": "4022776",
    "end": "4029250"
  },
  {
    "text": "playing the game, [INAUDIBLE]\nare you using search",
    "start": "4029250",
    "end": "4034860"
  },
  {
    "text": "or are you playing specific\nactions to the search, which is like [INAUDIBLE]\nor is it other way?",
    "start": "4034860",
    "end": "4041220"
  },
  {
    "text": "Yeah. Good question. So the question was\nin the self-play step, how is the search coming into\nto the algorithm basically.",
    "start": "4041220",
    "end": "4050260"
  },
  {
    "text": "And so in the\nself-play step here-- let me go back--",
    "start": "4050260",
    "end": "4055800"
  },
  {
    "text": "right here. So yeah, essentially for\neach of those board states, the current agent\nplaying is building up",
    "start": "4055800",
    "end": "4061620"
  },
  {
    "text": "that MCTS search tree using the\nnetwork estimates as it goes. And then it's sampling an\naction from that policy",
    "start": "4061620",
    "end": "4069180"
  },
  {
    "text": "that's built up as a\nresult of this search tree. And then it goes to\nthe next agent to play.",
    "start": "4069180",
    "end": "4074505"
  },
  {
    "text": "Yes. So when they do\nself-play, do both players share the same MCTS tree or do\nthey have two separate trees?",
    "start": "4074505",
    "end": "4083080"
  },
  {
    "text": "Good question. I believe they both have\ndifferent search trees, so I don't think that the\nsearch trees are being shared.",
    "start": "4083080",
    "end": "4089672"
  },
  {
    "text": "But I would need to\ndouble check on that. ",
    "start": "4089672",
    "end": "4096672"
  },
  {
    "text": "So I think a really\ninteresting area is extending these hybrid\nplanning approaches",
    "start": "4096673",
    "end": "4105219"
  },
  {
    "text": "to language model applications. And the same idea here of system\n1 and system 2 kind of applies.",
    "start": "4105220",
    "end": "4111982"
  },
  {
    "text": "And we're just going to\nbriefly touch on this. This would be like a whole other\nlecture or class in itself, but I think it's\ncool to relate it",
    "start": "4111982",
    "end": "4118299"
  },
  {
    "text": "to what we're seeing here with\nthe online planning algorithms and how it's connecting to\nsome of the current research.",
    "start": "4118300",
    "end": "4124278"
  },
  {
    "text": "And when I say that-- this is a field that's\nadvancing very rapidly--",
    "start": "4124279",
    "end": "4129290"
  },
  {
    "text": "I think that's a\nhuge understatement because there are so many people\nworking on this right now.",
    "start": "4129290",
    "end": "4134778"
  },
  {
    "text": "But just briefly,\nlooking at this, if the current\nlanguage model set up with these autoregressive\nnext token prediction",
    "start": "4134779",
    "end": "4142089"
  },
  {
    "text": "is basically looking at, OK, I\nchunk up the input into tokens, and then I'm purely trying to\npredict just the next token",
    "start": "4142090",
    "end": "4149560"
  },
  {
    "text": "based on that sequence\nthat I've seen. And so as a result of this,\nthis is like that system 1 level",
    "start": "4149560",
    "end": "4156520"
  },
  {
    "text": "approach where we're\njust connecting it to the AlphaZero style example. It's kind of playing\nwith the raw network.",
    "start": "4156520",
    "end": "4162139"
  },
  {
    "text": "You're just taking the\nnext action with respect to that raw network. In this case, the raw network\nbeing the language model itself.",
    "start": "4162140",
    "end": "4169170"
  },
  {
    "text": "And you're kind of playing\ngreedily with respect to that. So the system 2\nlevel thinking here",
    "start": "4169170",
    "end": "4174740"
  },
  {
    "text": "and what people are\npretty excited about is, what if you now take this\nhybrid planning approach instead",
    "start": "4174740",
    "end": "4180528"
  },
  {
    "text": "of just that purely\nnext token prediction, you're searching over this\nfuture sequences of tokens",
    "start": "4180529",
    "end": "4186080"
  },
  {
    "text": "that could occur here. And as a result, you can build\nup this better strategy using",
    "start": "4186080",
    "end": "4192200"
  },
  {
    "text": "the hybrid planning approach. So I think that's a pretty\ninteresting area, certainly not the only area to make\nimprovements here.",
    "start": "4192200",
    "end": "4198870"
  },
  {
    "text": "But it's a direct\nconnection to what we were just looking at here in\nthe hybrid planning approaches.",
    "start": "4198870",
    "end": "4205190"
  },
  {
    "text": "And then the last thing\njust to touch on here in these hybrid\nplanning methods, this is a paper\nfrom Andy Jones that",
    "start": "4205190",
    "end": "4212210"
  },
  {
    "text": "was looking at using an Alpha\nZero style approach on a board game called hex.",
    "start": "4212210",
    "end": "4217570"
  },
  {
    "text": "And so the main takeaway\nhere is that you have the train time compute\non the x-axis, test time",
    "start": "4217570",
    "end": "4222830"
  },
  {
    "text": "compute on the y-axis. And the lines here are\nshowing the minimum train test",
    "start": "4222830",
    "end": "4228530"
  },
  {
    "text": "time compute required\nto achieve a certain Elo rating in the game. And so this is showing we can\nthink about this in our setup",
    "start": "4228530",
    "end": "4236750"
  },
  {
    "text": "that we're looking at. It's kind of offline on the\nx-axis, online on the y-axis,",
    "start": "4236750",
    "end": "4242699"
  },
  {
    "text": "and how are you spending your\ncompute between those two? So what this is telling you\nis that there's roughly a 10x,",
    "start": "4242700",
    "end": "4248909"
  },
  {
    "text": "so if you reduce your\ntrain time compute by 10x, you would need to increase your\ntest time or online compute",
    "start": "4248910",
    "end": "4257030"
  },
  {
    "text": "by 15. So there's that 10\nto 15 trade off here. But what that's saying\nis that if you're",
    "start": "4257030",
    "end": "4263420"
  },
  {
    "text": "investing a lot of money in your\noffline learned neural network, you could save that\nmoney and instead",
    "start": "4263420",
    "end": "4269370"
  },
  {
    "text": "invest it in some online compute\nand achieve the same performance scaling down by a\nfactor of 10 and scaling",
    "start": "4269370",
    "end": "4275310"
  },
  {
    "text": "up by a factor of 15. So that's just one thing to note\nis that in these hybrid planning methods, you have this\ntrade-off of where",
    "start": "4275310",
    "end": "4282030"
  },
  {
    "text": "are you devoting your compute. And oftentimes, you can gain\nsome performance or cost savings based on how you're\nallocating it there.",
    "start": "4282030",
    "end": "4288910"
  },
  {
    "text": "So I think that's just an\ninteresting example, again, relating to what we\nwere looking at here.",
    "start": "4288910",
    "end": "4296100"
  },
  {
    "text": "All right. So that brings us\nnow to policy search. And policy search is\nnow focused on searching",
    "start": "4296100",
    "end": "4302969"
  },
  {
    "text": "the space of policies without\ndirectly computing a value function. And so the reason for this\nis that the policy space,",
    "start": "4302970",
    "end": "4310179"
  },
  {
    "text": "typically, depending on how\nwe've parameterized our policy, is going to be lower\ndimensional and can",
    "start": "4310180",
    "end": "4315449"
  },
  {
    "text": "be searched more efficiently\nthan searching over purely the state space. And so how this is\ngoing to work is",
    "start": "4315450",
    "end": "4321720"
  },
  {
    "text": "that we are going to introduce\nour parameterized policy. ",
    "start": "4321720",
    "end": "4331640"
  },
  {
    "text": "And so we're going to have\nthis vector of parameters here, theta. So we're just going to\nhave theta 1 to theta n.",
    "start": "4331640",
    "end": "4339730"
  },
  {
    "text": "So we have n parameters here. These could be the weights and\nbiases of a neural network. They could just be the\nparameters of a linear function",
    "start": "4339730",
    "end": "4347059"
  },
  {
    "text": "or any type of function. So we just have these\nparameters here. And now we're representing\nour policy, pi.",
    "start": "4347060",
    "end": "4354585"
  },
  {
    "text": "And we're going to\nwrite it as pi theta just to denote that\nit's parameterized by this vector of parameters.",
    "start": "4354585",
    "end": "4360699"
  },
  {
    "text": "So we're still taking in\na state and we're still outputting an action to take\nfrom that particular state.",
    "start": "4360700",
    "end": "4366300"
  },
  {
    "text": "It's just now we're representing\nthis policy function or distribution by\nthese parameters, theta.",
    "start": "4366300",
    "end": "4372150"
  },
  {
    "text": "So what's going to happen now\nis that we want to basically figure out how should we update\nthese parameters in such a way",
    "start": "4372150",
    "end": "4378740"
  },
  {
    "text": "that we improve our policy. That's where we're\ngoing with this. And just like what\nwe saw before was",
    "start": "4378740",
    "end": "4386360"
  },
  {
    "text": "that we could use these\nrollouts to estimate the utility of a particular policy.",
    "start": "4386360",
    "end": "4391880"
  },
  {
    "text": "So now when we talk about\nthe utility of our policy, we're going to say the utility\nhere of following policy pi",
    "start": "4391880",
    "end": "4397970"
  },
  {
    "text": "theta. And we can equivalently write\nthis as just u of theta.",
    "start": "4397970",
    "end": "4403730"
  },
  {
    "text": "And you'll often see it just\ndropped like that because, really, what you're\ntalking about is the utility of the\nparameters of that policy.",
    "start": "4403730",
    "end": "4409650"
  },
  {
    "text": "So we're concerned about\nimproving those parameters. And we can equivalently\ndo the same idea here",
    "start": "4409650",
    "end": "4415190"
  },
  {
    "text": "of the policy rollouts and\napproximate this utility just using m policy\nrollouts and getting",
    "start": "4415190",
    "end": "4423140"
  },
  {
    "text": "those rollout trajectories here. So with a current\npolicy parameterization,",
    "start": "4423140",
    "end": "4430530"
  },
  {
    "text": "so we have some current\npolicy parameter vector here, we're then going to perform\nm policy rollouts using",
    "start": "4430530",
    "end": "4436699"
  },
  {
    "text": "that policy and then\nget this estimate of the utility of those\nspecific policy parameters.",
    "start": "4436700",
    "end": "4442340"
  },
  {
    "text": "So same idea, just now using a\ndifferent policy set up here. ",
    "start": "4442340",
    "end": "4449130"
  },
  {
    "text": "So the first method\nhere for policy search that we're going to be\ntalking about is local search.",
    "start": "4449130",
    "end": "4455170"
  },
  {
    "text": "And the way that local\nsearch works is we start-- in this example. We have a policy with two\nparameters, theta 1 and theta 2.",
    "start": "4455170",
    "end": "4463300"
  },
  {
    "text": "And the current\npolicy parameters are shown in the center there,\nso that center white dot.",
    "start": "4463300",
    "end": "4468430"
  },
  {
    "text": "The background is showing\nthe true utility function. So we don't actually know this. This is unknown to us.",
    "start": "4468430",
    "end": "4474400"
  },
  {
    "text": "But it's shown in the\nbackground for reference. And so local search works by\ntaking a plus and minus step",
    "start": "4474400",
    "end": "4480180"
  },
  {
    "text": "in each parameter direction. So for theta 1,\nwe're going to take a plus alpha step is\nthe step size here",
    "start": "4480180",
    "end": "4486480"
  },
  {
    "text": "and a minus alpha step. For theta 2, we take a plus\nalpha step, a minus alpha step. And then for each one of\nthose candidate points,",
    "start": "4486480",
    "end": "4493360"
  },
  {
    "text": "so we have n parameters here--\nso we're evaluating 2 times n candidate points--",
    "start": "4493360",
    "end": "4499079"
  },
  {
    "text": "for each of those\ncandidate points, we're going to perform m policy\nrollouts to get a utility estimate at that point.",
    "start": "4499080",
    "end": "4505619"
  },
  {
    "text": "So in this case, we see, from\ndoing those policy rollouts, that the best performing\npoint is on the left",
    "start": "4505620",
    "end": "4511120"
  },
  {
    "text": "because the blue is higher\nutility in this example. So we are going to step now to\nthat left best performing point",
    "start": "4511120",
    "end": "4518449"
  },
  {
    "text": "and then just\nrepeat the process. So evaluate each of\nthese candidate points using m policy\nrollouts and then step",
    "start": "4518450",
    "end": "4524300"
  },
  {
    "text": "to the best performing one,\nwhich is at the bottom. So we're going to step\nto that bottom point and then just, again,\nrepeat the process.",
    "start": "4524300",
    "end": "4531060"
  },
  {
    "text": "So here, again, the\nbest performing point is at the bottom. So we step down there. And then that will\nrepeat one more time.",
    "start": "4531060",
    "end": "4537269"
  },
  {
    "text": "And now what\nhappens in this case is that we evaluate\nthe candidate points, but none of them are any\nbetter than our current policy.",
    "start": "4537270",
    "end": "4545720"
  },
  {
    "text": "So what do we do in this case? Well, we're just going to\nshrink that alpha factor now. So we just shrink down,\nand in the next iteration,",
    "start": "4545720",
    "end": "4552889"
  },
  {
    "text": "we've just stepped\na little bit less. And we check to see if\nany of those outperform. None of them do.",
    "start": "4552890",
    "end": "4558300"
  },
  {
    "text": "So we just shrink alpha again. And we'll just keep\nshrinking alpha until we reach some\nconvergence criteria.",
    "start": "4558300",
    "end": "4565320"
  },
  {
    "text": "And that will be the termination\nof the local search algorithm. So we're just purely\nmaking these plus and minus",
    "start": "4565320",
    "end": "4571490"
  },
  {
    "text": "local changes to\nthe current policy and seeing how that results in\nchanges to the policy vector.",
    "start": "4571490",
    "end": "4579710"
  },
  {
    "text": "And so some of\nthe drawbacks here is that we have to use\nm rollouts for each",
    "start": "4579710",
    "end": "4586190"
  },
  {
    "text": "of these candidate points. So we have to n candidate\npoints and performing m rollouts for each one of those.",
    "start": "4586190",
    "end": "4592080"
  },
  {
    "text": "So if you have a really\nlarge parameterization, you have a billion\nparameters in your policy, then that's going to be\nquite intensive to do.",
    "start": "4592080",
    "end": "4599820"
  },
  {
    "text": "And it also requires setting\nthat initial alpha step size that you're using. So that now becomes your\nparameter to play with.",
    "start": "4599820",
    "end": "4607960"
  },
  {
    "text": "And so that's what brings us\nto the cross entropy method. And the cross\nentropy method seeks",
    "start": "4607960",
    "end": "4613660"
  },
  {
    "text": "to improve on the\nweaknesses of local search by using a search distribution\nthat we are representing",
    "start": "4613660",
    "end": "4620440"
  },
  {
    "text": "by this p theta of psi here. And so, again,\nthis is still just",
    "start": "4620440",
    "end": "4627429"
  },
  {
    "text": "our policy vector of parameters. But now we have this other\nset of parameters here",
    "start": "4627430",
    "end": "4632660"
  },
  {
    "text": "which parameterize our\nsearch distribution. So if we're using a\nGaussian distribution,",
    "start": "4632660",
    "end": "4638060"
  },
  {
    "text": "this is just the\nmean and covariance of the Gaussian distribution. All right. And what we're saying then\nis that our policy parameters",
    "start": "4638060",
    "end": "4644409"
  },
  {
    "text": "are distributed according to\nthis Gaussian distribution. You can use other\ndistributions in here as well.",
    "start": "4644410",
    "end": "4649730"
  },
  {
    "text": "In this example,\nwe're going to be using a Gaussian distribution. So how this is going\nto work is we're",
    "start": "4649730",
    "end": "4654880"
  },
  {
    "text": "showing now the same\ntheta 1, theta 2 setup. And the contours of the Gaussian\nare shown in white there.",
    "start": "4654880",
    "end": "4661820"
  },
  {
    "text": "So we're just using a\nGaussian distribution. And then we're going to\ntake k samples of theta",
    "start": "4661820",
    "end": "4668090"
  },
  {
    "text": "from this search distribution. And those white\npoints that you see. So we've sampled k points\nfrom this search distribution.",
    "start": "4668090",
    "end": "4675090"
  },
  {
    "text": "And then we're going to evaluate\neach one of those samples. We could use policy rollouts.",
    "start": "4675090",
    "end": "4680417"
  },
  {
    "text": "Again, you don't have\nto use policy rollouts to get this utility estimate. There's other ways\nthat you could do it. In this example, we're\nusing policy rollouts.",
    "start": "4680417",
    "end": "4687719"
  },
  {
    "text": "So we evaluate each one of\nthose using policy rollouts. And then we're going to\nrefit the search distribution",
    "start": "4687720",
    "end": "4693620"
  },
  {
    "text": "to the ones that are\nperforming the best. And then we just repeat this. So in this case, the\nbest performing samples",
    "start": "4693620",
    "end": "4700880"
  },
  {
    "text": "are shown in red here. So those are the ones\nthat perform the best. And we're then just going to\nrefit the search distribution",
    "start": "4700880",
    "end": "4706550"
  },
  {
    "text": "to those best\nperforming samples. So we refit. This is what the new search\ndistribution looks like.",
    "start": "4706550",
    "end": "4712318"
  },
  {
    "text": "And then we're going\nto repeat the process. So now we're going to, again,\ntake k samples from that search",
    "start": "4712318",
    "end": "4717679"
  },
  {
    "text": "distribution. We're going to evaluate them all\nand then refit the distribution. Now you can see the new best\nperforming are shown in red.",
    "start": "4717680",
    "end": "4725040"
  },
  {
    "text": "So we refit and then\nrepeat the process. And we just keep doing this\nuntil some convergence criteria",
    "start": "4725040",
    "end": "4730640"
  },
  {
    "text": "is met again. And so you notice here, we\nfind a different local optima,",
    "start": "4730640",
    "end": "4736019"
  },
  {
    "text": "so we converge to a\ndifferent local optima. But also what's the point\nof doing this, right?",
    "start": "4736020",
    "end": "4741750"
  },
  {
    "text": "How is this different\nfrom local search. Well, remember, in local search,\nwe're evaluating 2n of these",
    "start": "4741750",
    "end": "4747080"
  },
  {
    "text": "candidate points. And each candidate point is only\nchanging one of the n parameters",
    "start": "4747080",
    "end": "4752240"
  },
  {
    "text": "at a time. So the cross entropy method is a\nlittle bit more efficient in how it's distributing these out. And we also have control\nover the number of samples",
    "start": "4752240",
    "end": "4760800"
  },
  {
    "text": "that we're taking. So we are setting\nthe k factor here. The drawback, though,\nis that we now",
    "start": "4760800",
    "end": "4766460"
  },
  {
    "text": "have to choose this number\nof best performing samples. So how many samples\ndo we keep around",
    "start": "4766460",
    "end": "4771559"
  },
  {
    "text": "now becomes the trade-off. Because on one side\nof the spectrum, if you choose to keep around too\nmany best performing samples,",
    "start": "4771560",
    "end": "4778380"
  },
  {
    "text": "your search distribution is not\ngoing to change much iteration to iteration. But if you take too few samples\nto keep around and refit to,",
    "start": "4778380",
    "end": "4786000"
  },
  {
    "text": "then you're going to collapse\nthat search distribution very quickly. So you have this trade-off\nof keeping around too many",
    "start": "4786000",
    "end": "4791780"
  },
  {
    "text": "and not keeping\naround enough samples, and that now becomes your\ntuning parameter here.",
    "start": "4791780",
    "end": "4797420"
  },
  {
    "text": "So just wrapping up what\nwe've seen with policy search, we saw that we could still use\nthe same idea of policy rollouts",
    "start": "4797420",
    "end": "4804050"
  },
  {
    "text": "to get an estimate of how our\nparameterized policy is doing. And then we also saw\nthat local search is just",
    "start": "4804050",
    "end": "4810470"
  },
  {
    "text": "focus on these local\nchanges to the policy. And then the cross\nentropy method introduces this\nsearch distribution",
    "start": "4810470",
    "end": "4817760"
  },
  {
    "text": "to improve upon that. And so wrapping up now with\nonline planning and policy",
    "start": "4817760",
    "end": "4822890"
  },
  {
    "text": "search, we focus today\nright on methods for online planning that we're reasoning\nabout states reachable",
    "start": "4822890",
    "end": "4829430"
  },
  {
    "text": "from the current\nstate that we're in and using this receding\nhorizon planning scheme.",
    "start": "4829430",
    "end": "4834540"
  },
  {
    "text": "And there was a variety of\ndifferent approaches looking at pruning the state space,\nsampling state transitions,",
    "start": "4834540",
    "end": "4840860"
  },
  {
    "text": "or replanning along\ntrajectories that we've seen to be more promising. And then we wrapped up\nthere with policy search.",
    "start": "4840860",
    "end": "4847050"
  },
  {
    "text": "So I hope you found that\ninteresting and helpful. And I'm happy to take any\nquestions offline now.",
    "start": "4847050",
    "end": "4852360"
  },
  {
    "text": "So thank you. ",
    "start": "4852360",
    "end": "4859000"
  }
]