[
  {
    "start": "0",
    "end": "1530"
  },
  {
    "text": "So what I need is actually\na way to estimate the test",
    "start": "1530",
    "end": "4410"
  },
  {
    "text": "error for each of these\nmodels, m0, m1, m2, all the way",
    "start": "4410",
    "end": "8309"
  },
  {
    "text": "through m p, so that I\ncan choose among them.",
    "start": "8310",
    "end": "12030"
  },
  {
    "text": "And basically, in order to\nestimate the test error,",
    "start": "12030",
    "end": "14460"
  },
  {
    "text": "I have two approaches.",
    "start": "14460",
    "end": "16049"
  },
  {
    "text": "And one approach is that I can\nindirectly estimate the test",
    "start": "16050",
    "end": "18810"
  },
  {
    "text": "error by somehow computing\nthe training error",
    "start": "18810",
    "end": "21449"
  },
  {
    "text": "and then adjusting it.",
    "start": "21450",
    "end": "23010"
  },
  {
    "text": "And the idea behind\nthis adjustment",
    "start": "23010",
    "end": "24750"
  },
  {
    "text": "is that if I can somehow adjust\nthe training error to account",
    "start": "24750",
    "end": "27990"
  },
  {
    "text": "for the bias due to\noverfitting, then that",
    "start": "27990",
    "end": "31340"
  },
  {
    "text": "can give me an\nestimate of test error",
    "start": "31340",
    "end": "36928"
  },
  {
    "text": "that's, again, based\non the training error,",
    "start": "36928",
    "end": "38720"
  },
  {
    "text": "but somehow looks more\nlike a test error.",
    "start": "38720",
    "end": "42187"
  },
  {
    "text": "And the alternative\napproach is that I",
    "start": "42187",
    "end": "43770"
  },
  {
    "text": "can try to directly\nestimate the test error.",
    "start": "43770",
    "end": "46090"
  },
  {
    "text": "And I can do that using\nsome of the approaches that",
    "start": "46090",
    "end": "48240"
  },
  {
    "text": "are in chapter 5 of this book.",
    "start": "48240",
    "end": "50835"
  },
  {
    "text": "And those involve\neither cross-validation",
    "start": "50835",
    "end": "53340"
  },
  {
    "text": "or a validation set approach.",
    "start": "53340",
    "end": "55170"
  },
  {
    "text": "And so that's a\nreally direct approach",
    "start": "55170",
    "end": "57179"
  },
  {
    "text": "to estimating the test\nerror, where I fit models",
    "start": "57180",
    "end": "59790"
  },
  {
    "text": "on part of the data and then I\nevaluate them on a holdout set.",
    "start": "59790",
    "end": "63150"
  },
  {
    "text": "So we're now going to talk\nabout both of these approaches.",
    "start": "63150",
    "end": "66010"
  },
  {
    "start": "66010",
    "end": "70150"
  },
  {
    "text": "So C p, AIC, BIC, and\nadjusted R squared all",
    "start": "70150",
    "end": "75130"
  },
  {
    "text": "adjust the training\nerror in order",
    "start": "75130",
    "end": "76600"
  },
  {
    "text": "to give us an estimate\nof the test error.",
    "start": "76600",
    "end": "78850"
  },
  {
    "text": "And they all can\nbe used to select",
    "start": "78850",
    "end": "81130"
  },
  {
    "text": "among models with different\nnumbers of variables.",
    "start": "81130",
    "end": "83799"
  },
  {
    "text": "So we're now going to look at\na figure showing us C p, BIC,",
    "start": "83800",
    "end": "87310"
  },
  {
    "text": "and adjusted R squared for\nthe best model of each size",
    "start": "87310",
    "end": "90520"
  },
  {
    "text": "that we get using best subset\nselection on the Credit data.",
    "start": "90520",
    "end": "93558"
  },
  {
    "text": "So we're first going\nto look at this figure",
    "start": "93558",
    "end": "95350"
  },
  {
    "text": "and then we'll talk about how\nthese quantities are defined.",
    "start": "95350",
    "end": "99490"
  },
  {
    "text": "So again, this is on\nthe credit data example.",
    "start": "99490",
    "end": "102310"
  },
  {
    "text": "And on the x-axis here, we\nhave the number of predictors",
    "start": "102310",
    "end": "105189"
  },
  {
    "text": "in each of these figures.",
    "start": "105190",
    "end": "106960"
  },
  {
    "text": "And on the y-axis,\nwe have C p, BIC,",
    "start": "106960",
    "end": "111490"
  },
  {
    "text": "which stands for Bayesian\nInformation Criterion,",
    "start": "111490",
    "end": "114189"
  },
  {
    "text": "and adjusted R squared.",
    "start": "114190",
    "end": "115990"
  },
  {
    "text": "And again, we'll\ndefine all three",
    "start": "115990",
    "end": "117520"
  },
  {
    "text": "of these quantities in a minute.",
    "start": "117520",
    "end": "119359"
  },
  {
    "text": "But the idea is,\nroughly speaking,",
    "start": "119360",
    "end": "121600"
  },
  {
    "text": "we want these\nquantities to be small.",
    "start": "121600",
    "end": "123310"
  },
  {
    "text": "So we prefer a model in which C\np, BIC are as small as possible.",
    "start": "123310",
    "end": "128810"
  },
  {
    "text": "And actually, I misspoke.",
    "start": "128810",
    "end": "129918"
  },
  {
    "text": "We want adjusted R-squared\nto be as large as possible.",
    "start": "129919",
    "end": "133040"
  },
  {
    "text": "So if I look at the\nshape of this curve,",
    "start": "133040",
    "end": "135319"
  },
  {
    "text": "I can see that C p\nis minimized when",
    "start": "135320",
    "end": "139410"
  },
  {
    "text": "we look at the model\nwith six predictors.",
    "start": "139410",
    "end": "142290"
  },
  {
    "text": "BIC is smallest when we look at\nthe model with four predictors.",
    "start": "142290",
    "end": "146010"
  },
  {
    "text": "And adjusted\nR-squared is smallest",
    "start": "146010",
    "end": "148110"
  },
  {
    "text": "when we look at a model\nwith six predictors again.",
    "start": "148110",
    "end": "151090"
  },
  {
    "text": "So that suggests that\nwe should use somewhere",
    "start": "151090",
    "end": "153129"
  },
  {
    "text": "between 4 and 6 predictors.",
    "start": "153130",
    "end": "155128"
  },
  {
    "text": "And actually, if we look at\nthese figures a little more",
    "start": "155128",
    "end": "157420"
  },
  {
    "text": "closely, we can see that,\nbasically, these curves",
    "start": "157420",
    "end": "160450"
  },
  {
    "text": "are more or less flat after we\nget to around 3 or 4 predictors.",
    "start": "160450",
    "end": "164840"
  },
  {
    "text": "And so, on the basis\nof these figures,",
    "start": "164840",
    "end": "167080"
  },
  {
    "text": "I would say I really don't\nthink we need more than 3,",
    "start": "167080",
    "end": "169960"
  },
  {
    "text": "or, max, 4 predictors to do a\ngood prediction on this credit",
    "start": "169960",
    "end": "173380"
  },
  {
    "text": "data.",
    "start": "173380",
    "end": "174370"
  },
  {
    "text": "So now I've scribbled\nall over the slide.",
    "start": "174370",
    "end": "176560"
  },
  {
    "text": "Oops.",
    "start": "176560",
    "end": "177340"
  },
  {
    "text": "Just one thing.",
    "start": "177340",
    "end": "178265"
  },
  {
    "text": "On this picture, it's\nhard to see here,",
    "start": "178265",
    "end": "179890"
  },
  {
    "text": "but actually, the curve is going\nup right as we go to the right,",
    "start": "179890",
    "end": "182770"
  },
  {
    "text": "despite the fact it's flat.",
    "start": "182770",
    "end": "184198"
  },
  {
    "text": "That's right, yeah.",
    "start": "184198",
    "end": "184990"
  },
  {
    "text": "Unlike the RSS curve.",
    "start": "184990",
    "end": "185870"
  },
  {
    "text": "Exactly.",
    "start": "185870",
    "end": "186370"
  },
  {
    "text": "It's a little hard to see, but\nthis is slightly increasing.",
    "start": "186370",
    "end": "189190"
  },
  {
    "text": "Its smallest with 4 predictors.",
    "start": "189190",
    "end": "190840"
  },
  {
    "text": "And then it goes\nup a little bit.",
    "start": "190840",
    "end": "192500"
  },
  {
    "text": "But I don't really think that\nthere's compelling evidence here",
    "start": "192500",
    "end": "195370"
  },
  {
    "text": "that 4 is really better\nthan 3 or better than five,",
    "start": "195370",
    "end": "198780"
  },
  {
    "text": "so if it were me, I think\nsimpler is always better",
    "start": "198780",
    "end": "201770"
  },
  {
    "text": "so I'd probably choose a model\nwith 3 predictors, maximum four",
    "start": "201770",
    "end": "205550"
  },
  {
    "text": "predictors.",
    "start": "205550",
    "end": "206050"
  },
  {
    "text": "Yeah, agree.",
    "start": "206050",
    "end": "206820"
  },
  {
    "start": "206820",
    "end": "210270"
  },
  {
    "text": "Great, so now we're going\nto talk about Mallow's C p.",
    "start": "210270",
    "end": "213480"
  },
  {
    "text": "And once again, this\nis an adjustment",
    "start": "213480",
    "end": "215580"
  },
  {
    "text": "to the training R-squared--\nthe training RSS that gives us",
    "start": "215580",
    "end": "218850"
  },
  {
    "text": "an estimate for the test RSS.",
    "start": "218850",
    "end": "220560"
  },
  {
    "text": "And it's defined\nin this formula.",
    "start": "220560",
    "end": "222640"
  },
  {
    "text": "So let's say we're looking\nat a model with d predictors.",
    "start": "222640",
    "end": "227238"
  },
  {
    "text": "So then we're going to\ncalculate the RSS for that model",
    "start": "227238",
    "end": "229530"
  },
  {
    "text": "with d predictors.",
    "start": "229530",
    "end": "230640"
  },
  {
    "text": "And we're going to\nadd to the RSS 2 times",
    "start": "230640",
    "end": "233130"
  },
  {
    "text": "d, where, again, d is\nthe number of predictors,",
    "start": "233130",
    "end": "236250"
  },
  {
    "text": "times sigma hat squared,\nwhere sigma hat squared is",
    "start": "236250",
    "end": "239100"
  },
  {
    "text": "an estimate of the variance\nassociated with each error",
    "start": "239100",
    "end": "242430"
  },
  {
    "text": "epsilon in the linear model.",
    "start": "242430",
    "end": "246209"
  },
  {
    "text": "And so the idea is we can\ncalculate C p for those models,",
    "start": "246210",
    "end": "250090"
  },
  {
    "text": "m0, m1, m2, through\nm p, that we were",
    "start": "250090",
    "end": "252360"
  },
  {
    "text": "looking at a few minutes ago.",
    "start": "252360",
    "end": "254020"
  },
  {
    "text": "And we can just choose the\nmodel with the smallest C p.",
    "start": "254020",
    "end": "258190"
  },
  {
    "text": "So if we're looking\nat the model m3,",
    "start": "258190",
    "end": "262010"
  },
  {
    "text": "then that model contains three\npredictors and an intercept.",
    "start": "262010",
    "end": "264980"
  },
  {
    "text": "So that model has d equals 4.",
    "start": "264980",
    "end": "267380"
  },
  {
    "text": "And we can calculate the\nRSS for the model m3.",
    "start": "267380",
    "end": "270380"
  },
  {
    "text": "And we just calculate\nsigma hat squared.",
    "start": "270380",
    "end": "272980"
  },
  {
    "text": "There's a formula for that.",
    "start": "272980",
    "end": "274460"
  },
  {
    "text": "It gives us a C p.",
    "start": "274460",
    "end": "276560"
  },
  {
    "text": "And out of all of these\nmodels, m0 to m p,",
    "start": "276560",
    "end": "278767"
  },
  {
    "text": "we're just going to choose\nthe one for which the C",
    "start": "278767",
    "end": "280850"
  },
  {
    "text": "p is smallest because\nthat's the one that we",
    "start": "280850",
    "end": "282683"
  },
  {
    "text": "believe is going to have\nthe smallest test set RSS.",
    "start": "282683",
    "end": "285218"
  },
  {
    "text": "Now, just to clarify a bit\nabout the sigma hat squared,",
    "start": "285218",
    "end": "287509"
  },
  {
    "text": "first of all, if p\nis bigger than n,",
    "start": "287510",
    "end": "289250"
  },
  {
    "text": "we're going to have a\nproblem because typically,",
    "start": "289250",
    "end": "292370"
  },
  {
    "text": "sigma hat squared,\nthe same values--",
    "start": "292370",
    "end": "294216"
  },
  {
    "text": "the same estimate is used for\nall models being compared.",
    "start": "294217",
    "end": "296550"
  },
  {
    "text": "So usually, what you do\nis you fit the full model,",
    "start": "296550",
    "end": "299060"
  },
  {
    "text": "all p predictors,\nand you take the mean",
    "start": "299060",
    "end": "300950"
  },
  {
    "text": "squared residual for that model\nto give you sigma hat squared.",
    "start": "300950",
    "end": "305063"
  },
  {
    "text": "So that's the way you do it.",
    "start": "305063",
    "end": "306229"
  },
  {
    "text": "And of course, that's going\nto create a problem when",
    "start": "306230",
    "end": "307580"
  },
  {
    "text": "p is bigger than n because\nthat full model was not defined",
    "start": "307580",
    "end": "310580"
  },
  {
    "text": "and the error will be 0.",
    "start": "310580",
    "end": "312000"
  },
  {
    "text": "So already, we see that C p\nis restricted to cases where",
    "start": "312000",
    "end": "315380"
  },
  {
    "text": "you've got n bigger than p.",
    "start": "315380",
    "end": "317630"
  },
  {
    "text": "That's right.",
    "start": "317630",
    "end": "318830"
  },
  {
    "text": "And even if p is\nclose to n, you're",
    "start": "318830",
    "end": "320509"
  },
  {
    "text": "going to have a problem because\nyour estimate of sigma squared",
    "start": "320510",
    "end": "323093"
  },
  {
    "text": "might be far too low.",
    "start": "323093",
    "end": "325889"
  },
  {
    "text": "So that's Mallow's C p.",
    "start": "325890",
    "end": "327960"
  },
  {
    "text": "And then another very\nclosely related idea",
    "start": "327960",
    "end": "330690"
  },
  {
    "text": "is called the AIC criterion.",
    "start": "330690",
    "end": "332500"
  },
  {
    "text": "So the AIC stands for Akaike\nInformation Criterion.",
    "start": "332500",
    "end": "337250"
  },
  {
    "text": "Akaike was the name of the guy\nwho came up with this idea.",
    "start": "337250",
    "end": "339930"
  },
  {
    "text": "And the way that this is defined\nis negative 2 log L plus 2 times",
    "start": "339930",
    "end": "344340"
  },
  {
    "text": "d, where d is, once again,\nthe number of predictors",
    "start": "344340",
    "end": "347910"
  },
  {
    "text": "in the model that\nI'm looking at.",
    "start": "347910",
    "end": "349330"
  },
  {
    "text": "So for m3, d equals 4.",
    "start": "349330",
    "end": "351479"
  },
  {
    "text": "And now capital L here\nis the maximized value",
    "start": "351480",
    "end": "355470"
  },
  {
    "text": "of the likelihood function\nfor the estimated model.",
    "start": "355470",
    "end": "358200"
  },
  {
    "text": "So this looks a little\nbit complicated.",
    "start": "358200",
    "end": "360330"
  },
  {
    "text": "And in fact, it's written\nin this very general way",
    "start": "360330",
    "end": "363360"
  },
  {
    "text": "because AIC is a quantity\nthat we can calculate",
    "start": "363360",
    "end": "365729"
  },
  {
    "text": "for many different model types.",
    "start": "365730",
    "end": "367050"
  },
  {
    "text": "Not just linear models, but also\nlogistic regression, and so on.",
    "start": "367050",
    "end": "370960"
  },
  {
    "text": "But it turns out that in\nthe case of a linear model,",
    "start": "370960",
    "end": "373630"
  },
  {
    "text": "negative 2 log L is just equal\nto RSS over sigma hat squared.",
    "start": "373630",
    "end": "381730"
  },
  {
    "text": "So if you look at that\nand you plug in RSS",
    "start": "381730",
    "end": "384400"
  },
  {
    "text": "over sigma hat squared\nfor negative 2 log L,",
    "start": "384400",
    "end": "386830"
  },
  {
    "text": "then what you realize is that\nAIC and Mallows C p are actually",
    "start": "386830",
    "end": "391659"
  },
  {
    "text": "proportional to each other.",
    "start": "391660",
    "end": "393370"
  },
  {
    "text": "And since we're just going to\nchoose the model for which C",
    "start": "393370",
    "end": "397074"
  },
  {
    "text": "p is smallest, that's\nequivalent to choosing",
    "start": "397075",
    "end": "399190"
  },
  {
    "text": "the model for which\nAIC is smallest.",
    "start": "399190",
    "end": "402330"
  },
  {
    "text": "So AIC and C p are actually\nreally the same thing",
    "start": "402330",
    "end": "405449"
  },
  {
    "text": "for linear models, but\nfor other types of models,",
    "start": "405450",
    "end": "409260"
  },
  {
    "text": "these things are not the same\nand AIC is a good approach.",
    "start": "409260",
    "end": "411695"
  },
  {
    "start": "411695",
    "end": "415780"
  },
  {
    "text": "So we've talked\nabout C p and AIC.",
    "start": "415780",
    "end": "418840"
  },
  {
    "text": "And another very\nrelated idea here",
    "start": "418840",
    "end": "420970"
  },
  {
    "text": "is the BIC, where B\nstands for Bayesian.",
    "start": "420970",
    "end": "423890"
  },
  {
    "text": "So this is the Bayesian\nInformation Criterion.",
    "start": "423890",
    "end": "426380"
  },
  {
    "text": "And it's like the AIC\nand the Mallow's C p",
    "start": "426380",
    "end": "429790"
  },
  {
    "text": "but it comes from a\nBayesian argument.",
    "start": "429790",
    "end": "432140"
  },
  {
    "text": "And once again, we've got\na very similar formula.",
    "start": "432140",
    "end": "434840"
  },
  {
    "text": "We calculate the\nresidual sum of squares.",
    "start": "434840",
    "end": "437860"
  },
  {
    "text": "And then we add an\nadjustment term,",
    "start": "437860",
    "end": "440110"
  },
  {
    "text": "which is the log of the\nnumber of observations",
    "start": "440110",
    "end": "442780"
  },
  {
    "text": "times d, which is, once again,\nthe number of predictors",
    "start": "442780",
    "end": "446050"
  },
  {
    "text": "in the model I'm looking at.",
    "start": "446050",
    "end": "447610"
  },
  {
    "text": "So like m3, since it has three\npredictors and an intercept,",
    "start": "447610",
    "end": "451159"
  },
  {
    "text": "m3 has d equals 4.",
    "start": "451160",
    "end": "453220"
  },
  {
    "text": "And once again,\nsigma hat squared",
    "start": "453220",
    "end": "455110"
  },
  {
    "text": "is an estimate of the error\nvariance, which may or may not",
    "start": "455110",
    "end": "458050"
  },
  {
    "text": "be available depending on\nwhether n is greater than p",
    "start": "458050",
    "end": "461440"
  },
  {
    "text": "or less than p.",
    "start": "461440",
    "end": "463560"
  },
  {
    "text": "And so once again,\nwith BIC, we're",
    "start": "463560",
    "end": "465810"
  },
  {
    "text": "estimating the test set RSS or\nrather, the average test set RSS",
    "start": "465810",
    "end": "471540"
  },
  {
    "text": "across the observations.",
    "start": "471540",
    "end": "473220"
  },
  {
    "text": "And so we want it to be\nas small as possible,",
    "start": "473220",
    "end": "477645"
  },
  {
    "text": "so we're going to choose the\nmodel with the smallest BIC.",
    "start": "477645",
    "end": "480020"
  },
  {
    "start": "480020",
    "end": "482580"
  },
  {
    "text": "So what's the difference\nbetween BIC and AIC?",
    "start": "482580",
    "end": "485849"
  },
  {
    "text": "Well, remember, AIC, it looked\njust like this, but in AIC,",
    "start": "485850",
    "end": "491700"
  },
  {
    "text": "this term was actually\n2 d sigma hat squared.",
    "start": "491700",
    "end": "496392"
  },
  {
    "text": "So the only difference\nbetween AIC and BIC",
    "start": "496392",
    "end": "499410"
  },
  {
    "text": "is the choice of log n versus 2.",
    "start": "499410",
    "end": "502200"
  },
  {
    "text": "BIC has this log n\nhere and AIC has a 2.",
    "start": "502200",
    "end": "509370"
  },
  {
    "text": "And so, in general, if\nn is greater than 7,",
    "start": "509370",
    "end": "514820"
  },
  {
    "text": "then log n is greater than 2.",
    "start": "514820",
    "end": "517268"
  },
  {
    "start": "517269",
    "end": "520519"
  },
  {
    "text": "And so what that\nmeans is that if you",
    "start": "520520",
    "end": "522200"
  },
  {
    "text": "have more than seven\nobservations in your data,",
    "start": "522200",
    "end": "524480"
  },
  {
    "text": "BIC is going to put more of\na penalty on a large model.",
    "start": "524480",
    "end": "528779"
  },
  {
    "text": "And in other words, BIC\nis going to tend to choose",
    "start": "528780",
    "end": "530900"
  },
  {
    "text": "smaller models than AIC is.",
    "start": "530900",
    "end": "534700"
  },
  {
    "text": "So BIC is going to give\nyou the selection of models",
    "start": "534700",
    "end": "537970"
  },
  {
    "text": "that have fewer variables\nthan either C p or AIC.",
    "start": "537970",
    "end": "542459"
  },
  {
    "start": "542460",
    "end": "545940"
  },
  {
    "text": "So we see that these three\nideas, BIC, C p, and AIC,",
    "start": "545940",
    "end": "549360"
  },
  {
    "text": "are really almost identical.",
    "start": "549360",
    "end": "550820"
  },
  {
    "text": "They just have slightly\ndifferent formulas.",
    "start": "550820",
    "end": "553800"
  },
  {
    "text": "We want to minimize them.",
    "start": "553800",
    "end": "554880"
  },
  {
    "text": "And they all require an\nestimate for sigma hat squared,",
    "start": "554880",
    "end": "558720"
  },
  {
    "text": "which, again, is not\navailable necessarily.",
    "start": "558720",
    "end": "561930"
  },
  {
    "text": "It's only going to be available\nif n is greater than p.",
    "start": "561930",
    "end": "566120"
  },
  {
    "text": "So the last of these\napproaches that I'm",
    "start": "566120",
    "end": "568460"
  },
  {
    "text": "going to talk about that\nindirectly adjusts the training",
    "start": "568460",
    "end": "571310"
  },
  {
    "text": "error to get an estimate\nof the test error",
    "start": "571310",
    "end": "573440"
  },
  {
    "text": "is the adjusted R-squared.",
    "start": "573440",
    "end": "575900"
  },
  {
    "text": "And so we saw in chapter 3\nthe idea of the R-squared.",
    "start": "575900",
    "end": "580590"
  },
  {
    "text": "And remember,\nR-squared was defined--",
    "start": "580590",
    "end": "582800"
  },
  {
    "text": "just as a little refresher,\nR-squared is defined as 1",
    "start": "582800",
    "end": "586610"
  },
  {
    "text": "minus the Residual\nSum of Squares divided",
    "start": "586610",
    "end": "590029"
  },
  {
    "text": "by the total sum of\nsquares, where, in case we",
    "start": "590030",
    "end": "594710"
  },
  {
    "text": "need a reminder, the\ntotal sum of squares",
    "start": "594710",
    "end": "597350"
  },
  {
    "text": "is just the sum of y\ni minus y bar squared.",
    "start": "597350",
    "end": "602509"
  },
  {
    "text": "So y bar is the\naverage response.",
    "start": "602510",
    "end": "604760"
  },
  {
    "text": "y i is the i'th response,\nand we're just taking the sum",
    "start": "604760",
    "end": "607640"
  },
  {
    "text": "of those squared values.",
    "start": "607640",
    "end": "609410"
  },
  {
    "text": "And so this was the R-squared.",
    "start": "609410",
    "end": "611129"
  },
  {
    "text": "And as we know, a big\nR-squared indicates a model",
    "start": "611130",
    "end": "614090"
  },
  {
    "text": "that really fits the data well.",
    "start": "614090",
    "end": "615540"
  },
  {
    "text": "But unfortunately, you\ncan't compare models",
    "start": "615540",
    "end": "618139"
  },
  {
    "text": "by just taking the--\nyou can't compare models",
    "start": "618140",
    "end": "620090"
  },
  {
    "text": "of different sizes by just\ntaking the one with the biggest",
    "start": "620090",
    "end": "622507"
  },
  {
    "text": "R-squared because\nyou can't compare",
    "start": "622507",
    "end": "624560"
  },
  {
    "text": "the R-squared of a model\nwith three variables",
    "start": "624560",
    "end": "626540"
  },
  {
    "text": "to the R-squared of a model with\neight variables, for instance.",
    "start": "626540",
    "end": "629910"
  },
  {
    "text": "So the adjusted R-squared\ntries to fix this.",
    "start": "629910",
    "end": "632233"
  },
  {
    "text": "And the way that it\ndoes that is that it",
    "start": "632233",
    "end": "633899"
  },
  {
    "text": "makes you pay a price\nfor having a large model.",
    "start": "633900",
    "end": "636570"
  },
  {
    "text": "So the idea is adjusted\nR-squared adjusts the R-squared",
    "start": "636570",
    "end": "639900"
  },
  {
    "text": "so that the values that\nyou get are comparable",
    "start": "639900",
    "end": "642390"
  },
  {
    "text": "even if the numbers of\npredictors are different.",
    "start": "642390",
    "end": "645080"
  },
  {
    "text": "So the way that it does this is\nby adding a denominator to RSS",
    "start": "645080",
    "end": "652330"
  },
  {
    "text": "and to TSS in this ratio.",
    "start": "652330",
    "end": "654230"
  },
  {
    "text": "So instead of just taking\n1 minus RSS over TSS,",
    "start": "654230",
    "end": "658089"
  },
  {
    "text": "we take 1 minus RSS\nover n minus d minus 1,",
    "start": "658090",
    "end": "662610"
  },
  {
    "text": "divided by TSS over n\nminus 1, where, again, d",
    "start": "662610",
    "end": "666805"
  },
  {
    "text": "is the number of\nvariables in the model",
    "start": "666805",
    "end": "668430"
  },
  {
    "text": "that we're considering.",
    "start": "668430",
    "end": "670250"
  },
  {
    "text": "And so, basically, the idea\nhere is that when D is large,",
    "start": "670250",
    "end": "675065"
  },
  {
    "text": "this denominator\nis really large.",
    "start": "675065",
    "end": "677190"
  },
  {
    "text": "And so you're dividing the\nRSS by a really big number,",
    "start": "677190",
    "end": "680180"
  },
  {
    "text": "and you're going to end up\nwith a smaller R-squared.",
    "start": "680180",
    "end": "682760"
  },
  {
    "text": "So what's happening\nis that we're",
    "start": "682760",
    "end": "684620"
  },
  {
    "text": "going to pay a price\nfor having a large model",
    "start": "684620",
    "end": "688070"
  },
  {
    "text": "in the adjusted R-squared.",
    "start": "688070",
    "end": "690220"
  },
  {
    "text": "Unlike the classical\nR-squared, where",
    "start": "690220",
    "end": "692379"
  },
  {
    "text": "we pay no price for\nhaving a large model",
    "start": "692380",
    "end": "694450"
  },
  {
    "text": "with a lot of features.",
    "start": "694450",
    "end": "696770"
  },
  {
    "text": "So the adjusted R-squared,\nwe want it to be large.",
    "start": "696770",
    "end": "700310"
  },
  {
    "text": "If it's large, then\nthat indicates a model",
    "start": "700310",
    "end": "702380"
  },
  {
    "text": "that really fits the data well.",
    "start": "702380",
    "end": "704120"
  },
  {
    "text": "And again, the idea is that\nadjusted R-squared is something",
    "start": "704120",
    "end": "707240"
  },
  {
    "text": "that we can actually\ncompare in a meaningful way",
    "start": "707240",
    "end": "709580"
  },
  {
    "text": "regardless of the number\nof predictors in the model.",
    "start": "709580",
    "end": "711960"
  },
  {
    "text": "Something I just noticed,\nlooks like it doesn't require",
    "start": "711960",
    "end": "715070"
  },
  {
    "text": "an estimate of sigma squared.",
    "start": "715070",
    "end": "716600"
  },
  {
    "text": "That's good.",
    "start": "716600",
    "end": "717556"
  },
  {
    "text": "And you can also apply it\nwhen p is bigger than n.",
    "start": "717557",
    "end": "719640"
  },
  {
    "text": "Yeah, that's right.",
    "start": "719640",
    "end": "720620"
  },
  {
    "text": "So that's a really\nnice advantage of RSS.",
    "start": "720620",
    "end": "723802"
  },
  {
    "text": "As Rob said, we don't need to\nestimate sigma squared, which",
    "start": "723802",
    "end": "726260"
  },
  {
    "text": "can be a problem.",
    "start": "726260",
    "end": "727500"
  },
  {
    "text": "And in principle, we can apply\nit when p is larger than n.",
    "start": "727500",
    "end": "732220"
  },
  {
    "text": "So we want a large value\nof adjusted R-squared.",
    "start": "732220",
    "end": "737560"
  },
  {
    "text": "And so the adjusted R-squared,\nin practice, people really",
    "start": "737560",
    "end": "740730"
  },
  {
    "text": "like it.",
    "start": "740730",
    "end": "741230"
  },
  {
    "text": "It tends to work really well.",
    "start": "741230",
    "end": "742660"
  },
  {
    "text": "So some statisticians don't like\nit as much as C p, AIC, and BIC.",
    "start": "742660",
    "end": "746860"
  },
  {
    "text": "And the reason is because\nit works well empirically,",
    "start": "746860",
    "end": "749390"
  },
  {
    "text": "but some statisticians\nfeel that it",
    "start": "749390",
    "end": "751330"
  },
  {
    "text": "doesn't have the\ntheoretical backing",
    "start": "751330",
    "end": "753280"
  },
  {
    "text": "of some other approaches.",
    "start": "753280",
    "end": "754660"
  },
  {
    "text": "What do you think of that, Rob?",
    "start": "754660",
    "end": "756190"
  },
  {
    "text": "That's true.",
    "start": "756190",
    "end": "756788"
  },
  {
    "text": "There is a bias in our\nfield towards things which",
    "start": "756788",
    "end": "758830"
  },
  {
    "text": "have more theory\nbehind them, and I",
    "start": "758830",
    "end": "761090"
  },
  {
    "text": "guess this an example of that.",
    "start": "761090",
    "end": "763038"
  },
  {
    "text": "But one nice thing\nabout adjusted R-squared",
    "start": "763038",
    "end": "764830"
  },
  {
    "text": "is if you're\nworking with someone",
    "start": "764830",
    "end": "766630"
  },
  {
    "text": "who's not a statistician--\nlike scientists",
    "start": "766630",
    "end": "770080"
  },
  {
    "text": "who aren't statisticians are\nreally familiar with R squared.",
    "start": "770080",
    "end": "772730"
  },
  {
    "text": "And so to understand R\nsquared, adjusting R-squared",
    "start": "772730",
    "end": "776079"
  },
  {
    "text": "is just a really small one-off.",
    "start": "776080",
    "end": "778430"
  },
  {
    "text": "And it's easier to explain\nto someone in a certain sense",
    "start": "778430",
    "end": "781000"
  },
  {
    "text": "than AIC, C p, or\nBIC, and so that's",
    "start": "781000",
    "end": "783550"
  },
  {
    "text": "one really nice thing about it.",
    "start": "783550",
    "end": "785089"
  },
  {
    "text": "But adjusted R-squared,\nyou can't really",
    "start": "785090",
    "end": "787000"
  },
  {
    "text": "generalize to other\ntypes of models.",
    "start": "787000",
    "end": "788600"
  },
  {
    "text": "So if you have logistic\nregression, you can't do this.",
    "start": "788600",
    "end": "791089"
  },
  {
    "text": "So you'll see in\nthe next section,",
    "start": "791090",
    "end": "792510"
  },
  {
    "text": "we'll talk about\ncross-validation,",
    "start": "792510",
    "end": "793927"
  },
  {
    "text": "which is our favorite method,\nwhich you can generalize.",
    "start": "793927",
    "end": "797623"
  },
  {
    "text": "And one major advantage is\nyou don't need to know d.",
    "start": "797623",
    "end": "799790"
  },
  {
    "text": "So the d in this method,\nin adjusted R-squared, in C",
    "start": "799790",
    "end": "803779"
  },
  {
    "text": "p, and AIC, is the\nnumber of parameters.",
    "start": "803780",
    "end": "805820"
  },
  {
    "text": "But in some methods like ridge\nregression and the lasso,",
    "start": "805820",
    "end": "808520"
  },
  {
    "text": "which we'll also talk about,\nagain, in a few minutes,",
    "start": "808520",
    "end": "813170"
  },
  {
    "text": "the value of d is not\neven known so we can't",
    "start": "813170",
    "end": "815255"
  },
  {
    "text": "apply any of these methods.",
    "start": "815255",
    "end": "816380"
  },
  {
    "text": "But cross-validation\ncan still be applied.",
    "start": "816380",
    "end": "818160"
  },
  {
    "text": "Yeah, that's true.",
    "start": "818160",
    "end": "818910"
  },
  {
    "text": "So in this whole\ndiscussion, I've",
    "start": "818910",
    "end": "820040"
  },
  {
    "text": "been talking about\nleast squares models",
    "start": "820040",
    "end": "821839"
  },
  {
    "text": "and then I've been occasionally\nmentioning logistic regression,",
    "start": "821840",
    "end": "824810"
  },
  {
    "text": "but I could have some\ntotally crazy model",
    "start": "824810",
    "end": "827330"
  },
  {
    "text": "that I come up with\nthat is something",
    "start": "827330",
    "end": "829520"
  },
  {
    "text": "that nobody's ever seen before.",
    "start": "829520",
    "end": "830820"
  },
  {
    "text": "And it would be totally\nhopeless to apply",
    "start": "830820",
    "end": "833990"
  },
  {
    "text": "an AIC type of idea to it or an\nadjusted R-squared type of idea.",
    "start": "833990",
    "end": "837450"
  },
  {
    "text": "But I can always\nperform cross-validation",
    "start": "837450",
    "end": "839630"
  },
  {
    "text": "or the validation set approach\nno matter how wacky my model is.",
    "start": "839630",
    "end": "842985"
  },
  {
    "text": "And that's actually\na really nice thing",
    "start": "842985",
    "end": "844610"
  },
  {
    "text": "about those two approaches.",
    "start": "844610",
    "end": "847600"
  }
]