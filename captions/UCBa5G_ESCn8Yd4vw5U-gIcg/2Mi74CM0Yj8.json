[
  {
    "start": "0",
    "end": "234000"
  },
  {
    "text": "thank you guys for coming it's really thrilled this is my first in-person",
    "start": "10880",
    "end": "16640"
  },
  {
    "text": "official talk in front of the audience in two years plus so i'm super excited to",
    "start": "16640",
    "end": "23680"
  },
  {
    "text": "talk the talk about some of our recent research so give us some some of",
    "start": "23680",
    "end": "31679"
  },
  {
    "text": "the overview kind of along the topics and the autonomy that's kind of near and dear to my heart and kind of first at",
    "start": "31679",
    "end": "37200"
  },
  {
    "text": "the end uh some thoughts for kind of where the future might be what might be open problems",
    "start": "37200",
    "end": "43200"
  },
  {
    "text": "so autonomous systems really especially service robots have",
    "start": "43200",
    "end": "48320"
  },
  {
    "text": "potential to help so many peoples make their lives better and they make their lives better completing",
    "start": "48320",
    "end": "55440"
  },
  {
    "text": "tasks that elderly people people with disabilities and so on cannot do it by themselves",
    "start": "55440",
    "end": "62640"
  },
  {
    "text": "right on the other hand the digital agent this would be like your google assistant type of thing or so on also",
    "start": "62640",
    "end": "68720"
  },
  {
    "text": "bring elderly and people that don't have access to a technology because of the",
    "start": "68720",
    "end": "74880"
  },
  {
    "text": "uh i don't know motion problems not being able to interface with the devices and so on or just not kind of technology",
    "start": "74880",
    "end": "82799"
  },
  {
    "text": "technologically apt be able to complete tasks for them uh",
    "start": "82799",
    "end": "87840"
  },
  {
    "text": "and kind of bring them to the to be able to do kind of chores and so",
    "start": "87840",
    "end": "93680"
  },
  {
    "text": "on that are not possible right now so what do we need for kind of these agents",
    "start": "93680",
    "end": "101439"
  },
  {
    "text": "to be able to do they need to complete the autonomous tasks in real world",
    "start": "101439",
    "end": "107040"
  },
  {
    "text": "the whether that means uh and that means a long people among human",
    "start": "107040",
    "end": "112320"
  },
  {
    "text": "rights environments are changing all the time rooms are changing furniture is changing",
    "start": "112320",
    "end": "118640"
  },
  {
    "text": "there are sensors uncertainties and all the nine yards around that they also need to run on real robots and",
    "start": "118640",
    "end": "126560"
  },
  {
    "text": "robots come in all kinds of shapes and sizes and their geometry their dynamics right their sensors and",
    "start": "126560",
    "end": "134319"
  },
  {
    "text": "even their compute impacts how that motion or these tasks",
    "start": "134319",
    "end": "139599"
  },
  {
    "text": "actually get manifested and these things need to come together in addition the training becomes uh we",
    "start": "139599",
    "end": "147200"
  },
  {
    "text": "want generalization we want the robots to work in large variety of the environments so we",
    "start": "147200",
    "end": "152640"
  },
  {
    "text": "cannot possibly go and hard code everything which kind of brings us to the learning and the generalization becomes a b",
    "start": "152640",
    "end": "160319"
  },
  {
    "text": "issue or the question how to do and how do we get practical training and",
    "start": "160319",
    "end": "165599"
  },
  {
    "text": "deploying in this real world so that's kind of the setup that",
    "start": "165599",
    "end": "170959"
  },
  {
    "text": "the problem space that we want to go by so i'll talk about two broad topics",
    "start": "170959",
    "end": "177280"
  },
  {
    "text": "one is first is your zero shot transfer the idea is and there's lots of",
    "start": "177280",
    "end": "182879"
  },
  {
    "text": "different perspectives in the space do we train from the real data do we train on real systems are we training",
    "start": "182879",
    "end": "188560"
  },
  {
    "text": "simulation and so on for the purpose of this talk i'm going to talk about training and simulation because",
    "start": "188560",
    "end": "195280"
  },
  {
    "text": "i don't know it's safe it's safer it's cheaper potentially but it has the issue that we create a sim to real gap so",
    "start": "195280",
    "end": "202319"
  },
  {
    "text": "let's talk about how to ensure zero shot transfer from training and simulation to the real robots and then",
    "start": "202319",
    "end": "209840"
  },
  {
    "text": "once we can transfer the policies just plop it on the robot and it runs we can talk about how to get",
    "start": "209840",
    "end": "216879"
  },
  {
    "text": "better policies and generalization that's going to be second part of the talk",
    "start": "216879",
    "end": "221920"
  },
  {
    "text": "so three themes there mobile navigation with the noisy sensors",
    "start": "222159",
    "end": "228799"
  },
  {
    "text": "safety critical visual navigation and research constraint systems",
    "start": "228799",
    "end": "234239"
  },
  {
    "start": "234000",
    "end": "1066000"
  },
  {
    "text": "the for the mobile navigation let's say in this work we want to have",
    "start": "234239",
    "end": "239680"
  },
  {
    "text": "robot go from a to b in unstructured environment avoiding obstacles and so on",
    "start": "239680",
    "end": "245360"
  },
  {
    "text": "we assume that there is no map that it can rely on it just needs to basically sense what the",
    "start": "245360",
    "end": "251760"
  },
  {
    "text": "space is and it's a goal condition policy so it has a goal to go to",
    "start": "251760",
    "end": "256959"
  },
  {
    "text": "and just needs to get there the best they can kind of like i do here like if i'm gonna",
    "start": "256959",
    "end": "262079"
  },
  {
    "text": "go exit i'm probably gonna go this way and go around so reinforcement learning kind of comes",
    "start": "262079",
    "end": "267280"
  },
  {
    "text": "naturally to that observations here are lidar observations and we kind of have",
    "start": "267280",
    "end": "272320"
  },
  {
    "text": "several of them in the goal the actions are the linear and uh",
    "start": "272320",
    "end": "278400"
  },
  {
    "text": "angular velocity of the robot and the training environment we use here",
    "start": "278400",
    "end": "283759"
  },
  {
    "text": "is a very simple top-down map with static of",
    "start": "283759",
    "end": "288880"
  },
  {
    "text": "with static obstacles without any dynamics and whatnot it's really like a 2d map",
    "start": "288880",
    "end": "294840"
  },
  {
    "text": "projection and then the objective really that we want to do here is we completed",
    "start": "294840",
    "end": "301199"
  },
  {
    "text": "the task or we didn't we reached our goal or we didn't reach the goal however",
    "start": "301199",
    "end": "307360"
  },
  {
    "text": "training rl this is a semi long horizon task that's not very scalable so becomes",
    "start": "307360",
    "end": "313520"
  },
  {
    "text": "question of what is the reward and generally we have a good intuition there about the features that can kind",
    "start": "313520",
    "end": "319840"
  },
  {
    "text": "of influence that reward and this is the interesting reward they kind of give agent",
    "start": "319840",
    "end": "325039"
  },
  {
    "text": "internal representation am i progressing well towards the task so these things would be the distance to",
    "start": "325039",
    "end": "332000"
  },
  {
    "text": "the goal the distance to the nearest obstacle the angle right and so on we have good sense",
    "start": "332000",
    "end": "339759"
  },
  {
    "text": "of what might influence that decision but not how they relate each other and what the weights are for that function",
    "start": "339759",
    "end": "347520"
  },
  {
    "text": "the second question here is how do we select neural network architecture for this how many",
    "start": "347520",
    "end": "353759"
  },
  {
    "text": "layers how many how they are deep do we do this or do we do that right and you kind of do that with a trial and",
    "start": "353759",
    "end": "359520"
  },
  {
    "text": "error so because this is a very trial and error i'll talk more about that later",
    "start": "359520",
    "end": "365520"
  },
  {
    "text": "but to fear you say okay how about we train a population of agents we do this by trial",
    "start": "365520",
    "end": "372479"
  },
  {
    "text": "and error let's automate that process so we start with a future",
    "start": "372479",
    "end": "378160"
  },
  {
    "text": "basically parametrization of the reward based on these features we randomly",
    "start": "378160",
    "end": "383919"
  },
  {
    "text": "initialize the rewards on the population the nh trains with that",
    "start": "383919",
    "end": "391520"
  },
  {
    "text": "uh with a different kind of parametrization of the reward once we train the agent we evaluated on our",
    "start": "391520",
    "end": "396880"
  },
  {
    "text": "system how well it performs what how many times basically it reached",
    "start": "396880",
    "end": "402000"
  },
  {
    "text": "the goal and that's our task objective in this case and then we put that in the mutator that",
    "start": "402000",
    "end": "408000"
  },
  {
    "text": "then proposes the next parametrization for the reward and spawns the new agent and so on",
    "start": "408000",
    "end": "414960"
  },
  {
    "text": "it's not cheap the but it does produce good policy once we're done and",
    "start": "414960",
    "end": "420319"
  },
  {
    "text": "in this case this is about thousand policies running we select the reward and then we go on",
    "start": "420319",
    "end": "425440"
  },
  {
    "text": "the second phase where now we fix the reward interesting reward and we're trying to fit the neural network",
    "start": "425440",
    "end": "431840"
  },
  {
    "text": "architecture for that reward so again sure um i'm wondering",
    "start": "431840",
    "end": "439039"
  },
  {
    "text": "just about the reward portion of this how does this relate to inverse reinforcement learning where you also",
    "start": "439039",
    "end": "444560"
  },
  {
    "text": "can learn the reward from some demonstration so this would be uh one thing that we found",
    "start": "444560",
    "end": "451759"
  },
  {
    "text": "is that these are these truistic rewards so the",
    "start": "451759",
    "end": "457440"
  },
  {
    "text": "inverse reinforcement learning would want to kind of get okay this is my task characteristics right i want to be i",
    "start": "457440",
    "end": "465360"
  },
  {
    "text": "don't know conservative or the preferences don't want to go slow take more risk or not",
    "start": "465360",
    "end": "471440"
  },
  {
    "text": "the intrinsic reward is the reward that helps agents learn faster and have better behavior and what we",
    "start": "471440",
    "end": "478879"
  },
  {
    "text": "found is that there is a codependency between the loss function that we're",
    "start": "478879",
    "end": "483919"
  },
  {
    "text": "using for training the agent the rl loss and the reward that it comes up",
    "start": "483919",
    "end": "490400"
  },
  {
    "text": "because in a way reward kind of leads the agent in that",
    "start": "490400",
    "end": "495599"
  },
  {
    "text": "space and produces the training and even reparameterizing the existing reward",
    "start": "495599",
    "end": "501440"
  },
  {
    "text": "can improve the the training so",
    "start": "501440",
    "end": "506879"
  },
  {
    "text": "the questioner so",
    "start": "507520",
    "end": "513360"
  },
  {
    "text": "in a way inverse reinforcement learning learns the task objective at least in my mind",
    "start": "513360",
    "end": "520640"
  },
  {
    "text": "okay so for neural network architecture we're now fitting the neural network architecture for the reward and again",
    "start": "520640",
    "end": "527600"
  },
  {
    "text": "parameterize the neural network architecture start with a random parametrization",
    "start": "527600",
    "end": "532959"
  },
  {
    "text": "and we evaluated then now and against the cumulative reward and we repeat the process",
    "start": "532959",
    "end": "539200"
  },
  {
    "text": "and now we have our policies so the training algorithm basically we take this very simple environment there",
    "start": "539200",
    "end": "545600"
  },
  {
    "text": "is the video of the environment really super simple it does do some basic domain",
    "start": "545600",
    "end": "550800"
  },
  {
    "text": "randomization in terms of adding noise to the sensors adding noise to the kinematic model to the action space and",
    "start": "550800",
    "end": "556560"
  },
  {
    "text": "so on and do the then these two loop learning reward and the neural network",
    "start": "556560",
    "end": "564399"
  },
  {
    "text": "architecture and that transfer zero shot transfer plop it on the robot and it runs in in",
    "start": "564399",
    "end": "570240"
  },
  {
    "text": "the environments around the people even on a different kinematic of the",
    "start": "570240",
    "end": "576000"
  },
  {
    "text": "robot the difference between fetch and freight is about 100 pounds and the same process works for",
    "start": "576000",
    "end": "583360"
  },
  {
    "text": "modrical environments which are non-linear systems uh even the",
    "start": "583360",
    "end": "588399"
  },
  {
    "text": "system is very king of the economic constraints such as cars and asteroid dynamics",
    "start": "588399",
    "end": "595440"
  },
  {
    "text": "and here is a video in the this is basically a start we ask robot",
    "start": "595440",
    "end": "601519"
  },
  {
    "text": "to go from the fixed position to that sink and we repeated that same trajectory over and over or same",
    "start": "601519",
    "end": "608640"
  },
  {
    "text": "task and basically people were just showing up now the path is blocked and it",
    "start": "608640",
    "end": "614240"
  },
  {
    "text": "basically needs to go and adapt and go around this person is around so needs to adapt to that to",
    "start": "614240",
    "end": "621040"
  },
  {
    "text": "reach their goal again no map just kind of lidar sensors yeah this is blocked so i'm gonna react",
    "start": "621040",
    "end": "627200"
  },
  {
    "text": "and go to the left and this is not staged the custodian showed up",
    "start": "627200",
    "end": "635279"
  },
  {
    "text": "so this is the real world right this is the things that you can program ahead of time this person kind of leaving now it",
    "start": "637200",
    "end": "643279"
  },
  {
    "text": "started going left and now it's kind of found that there is a clearance opening and so on",
    "start": "643279",
    "end": "649759"
  },
  {
    "text": "this works so then the question was does it work beyond the",
    "start": "651920",
    "end": "657360"
  },
  {
    "text": "robotic beyond the navigation and we tried it on the benchmarks so mujapat has different reinforcement learning",
    "start": "657360",
    "end": "663600"
  },
  {
    "text": "algorithms and this is where kind of found out that yeah there is the dependency between the",
    "start": "663600",
    "end": "670160"
  },
  {
    "text": "loss functions and the or the rl algorithms in the reward",
    "start": "670160",
    "end": "676640"
  },
  {
    "text": "so we found that the more complex the task is such as humanoid which kind of has high degrees of freedom and so on",
    "start": "676640",
    "end": "683839"
  },
  {
    "text": "the more this reparametrization of the reward helps so the orange line there is",
    "start": "683839",
    "end": "689839"
  },
  {
    "text": "learning the reward and blue line is tuning the hyper parameters using the",
    "start": "689839",
    "end": "695839"
  },
  {
    "text": "same method the next we ask whether it makes sense to do",
    "start": "695839",
    "end": "702720"
  },
  {
    "text": "a simple objective such as just get to the goal in terms of this mutual task is go as fast as you can or go as high as",
    "start": "702720",
    "end": "709760"
  },
  {
    "text": "you can or do we use as our task objective the reward function is",
    "start": "709760",
    "end": "714880"
  },
  {
    "text": "standard in mudraco tasks which has three parameters on the preferences",
    "start": "714880",
    "end": "721760"
  },
  {
    "text": "and the delta is basically the difference between these two",
    "start": "721760",
    "end": "726800"
  },
  {
    "text": "red lines which is very minuscule so your mileage may vary whether you will",
    "start": "726800",
    "end": "732160"
  },
  {
    "text": "need to kind of construct more complex reward or so on you just kind of want to",
    "start": "732160",
    "end": "737760"
  },
  {
    "text": "do the objective i suspect that it also depends on the application",
    "start": "737760",
    "end": "744160"
  },
  {
    "text": "and then the question that we third question that we asked there is if i we have a fixed",
    "start": "744160",
    "end": "750320"
  },
  {
    "text": "training budget do i spend that budget learning the reward or do i spend it tuning the hyper",
    "start": "750320",
    "end": "756880"
  },
  {
    "text": "parameters and there's a difference between orange and blue line here so",
    "start": "756880",
    "end": "762639"
  },
  {
    "text": "if you're starting with reasonable hyper parameters you're more likely to get better bigger",
    "start": "762639",
    "end": "769120"
  },
  {
    "text": "bang for the buck doing the reward search",
    "start": "769120",
    "end": "774320"
  },
  {
    "text": "so next i want to talk about the visual navigation",
    "start": "775040",
    "end": "780560"
  },
  {
    "text": "and the idea here so with the mobile navigation we had this very simple",
    "start": "780560",
    "end": "785839"
  },
  {
    "text": "simulator that we're using now it's lighter so lighter it's a lot easier to simulate and so on how about",
    "start": "785839",
    "end": "793279"
  },
  {
    "text": "if we go after visual navigation around humans this",
    "start": "793279",
    "end": "799120"
  },
  {
    "text": "collaboration with claire tomlin's lab at berkeley the we have two options right we train on",
    "start": "799120",
    "end": "806160"
  },
  {
    "text": "robot which is good because then we're not dealing with the photorealism and seem to real gap from",
    "start": "806160",
    "end": "812160"
  },
  {
    "text": "the visual sensors however we have people and do we have policies that are safe to",
    "start": "812160",
    "end": "818800"
  },
  {
    "text": "train around people and do we have the right representation of people",
    "start": "818800",
    "end": "823920"
  },
  {
    "text": "how we can get enough volunteers right to deal with that flip side is if we train in simulation",
    "start": "823920",
    "end": "830880"
  },
  {
    "text": "how do we construct the data set it's not simple and how do we deal with the symptom real",
    "start": "830880",
    "end": "836639"
  },
  {
    "text": "gap so the idea is yeah let's go in simulation",
    "start": "836639",
    "end": "843720"
  },
  {
    "text": "we construct a simulator that is combination of two data sets",
    "start": "844639",
    "end": "849839"
  },
  {
    "text": "one is stanford's large scale 3d indoor space data set",
    "start": "849839",
    "end": "855760"
  },
  {
    "text": "that gives us a realistic indoor spaces that we know we can transfer to",
    "start": "855760",
    "end": "861600"
  },
  {
    "text": "to the robot the second one is surreal it's a data set of human models",
    "start": "861600",
    "end": "868160"
  },
  {
    "text": "that then we can go and parametrize and we can change their height we can change their body type they can change the",
    "start": "868160",
    "end": "874000"
  },
  {
    "text": "texture skin color and whatnot the and we can attach them different behaviors",
    "start": "874000",
    "end": "879839"
  },
  {
    "text": "policies that can control them and from there now we have a simulator that we can just parameterize with",
    "start": "879839",
    "end": "886079"
  },
  {
    "text": "different parameters and create many many different environments and humans so here are some examples",
    "start": "886079",
    "end": "892959"
  },
  {
    "text": "here that the simulator renders how they are placed in the scene",
    "start": "892959",
    "end": "900160"
  },
  {
    "text": "the so now we can play we can create a data set and start",
    "start": "900160",
    "end": "905199"
  },
  {
    "text": "training so how do we go about training we first set up the simulator and we practice",
    "start": "905199",
    "end": "910800"
  },
  {
    "text": "right we randomly initialize the poses we let them move it's a simulator we",
    "start": "910800",
    "end": "916880"
  },
  {
    "text": "create we create a solution with npc find whichever the associated the waypoint the where the",
    "start": "916880",
    "end": "923680"
  },
  {
    "text": "agent is going with respect to a human in the scene and what is the cost that comes with that and that's the data set",
    "start": "923680",
    "end": "931040"
  },
  {
    "text": "then from the data set that associate my rgb image that robot sees and the",
    "start": "931040",
    "end": "936959"
  },
  {
    "text": "waypoint we predict the cost and then in runtime we have that model",
    "start": "936959",
    "end": "943120"
  },
  {
    "text": "that then planning phase saying okay they're all my way points where should i go",
    "start": "943120",
    "end": "948240"
  },
  {
    "text": "this way point that's a cost to just select the one with the minimum cost and that actually transfers uh zero shot",
    "start": "948240",
    "end": "956560"
  },
  {
    "text": "to reality without kind of totally bypassing all the privacy concerns associated with training with",
    "start": "956560",
    "end": "963279"
  },
  {
    "text": "humans and we also see that for example in this picture",
    "start": "963279",
    "end": "971440"
  },
  {
    "text": "that when we have human and the legs are slightly moving the model is going to go around behind",
    "start": "971440",
    "end": "978880"
  },
  {
    "text": "the human whereas when a human is standing static it's going to go in front of the human",
    "start": "978880",
    "end": "985360"
  },
  {
    "text": "and we're kind of figuring that maybe it's kind of learning to associate how the positioning of the legs with the",
    "start": "985360",
    "end": "990959"
  },
  {
    "text": "motion and direction where it's going we're trying to make sense i also see that the baseline which was",
    "start": "990959",
    "end": "996639"
  },
  {
    "text": "trained without humans on the scene cuts off in front of people whereas the one that actually had the",
    "start": "996639",
    "end": "1004160"
  },
  {
    "text": "humans in the data set does not and even though we didn't train it we trained it only with a single human in",
    "start": "1004160",
    "end": "1010480"
  },
  {
    "text": "the scene that actually can generalize with some limitations to having two",
    "start": "1010480",
    "end": "1017759"
  },
  {
    "text": "humans in the scene and here is a video additional caveat here the model is",
    "start": "1017759",
    "end": "1023360"
  },
  {
    "text": "trained with the 65 degree camera and this camera on the robot has 45 degree",
    "start": "1023360",
    "end": "1028880"
  },
  {
    "text": "field of view so it goes here and avoids the human",
    "start": "1028880",
    "end": "1036319"
  },
  {
    "text": "there's another human is running coming from around the corner",
    "start": "1036559",
    "end": "1042720"
  },
  {
    "text": "and the rubber kind of goes around it",
    "start": "1042720",
    "end": "1046959"
  },
  {
    "text": "so that's exciting to be able to have this it's a fairly complex problem",
    "start": "1049679",
    "end": "1055520"
  },
  {
    "text": "to be able and last kind of in this set that i want to share with you",
    "start": "1055520",
    "end": "1062799"
  },
  {
    "text": "is let's say quadrotors and nano drones specifically one thing that we don't think often",
    "start": "1062799",
    "end": "1070080"
  },
  {
    "start": "1066000",
    "end": "1693000"
  },
  {
    "text": "this is the crazy fly it has whooping 33 kilobytes of the free ram",
    "start": "1070080",
    "end": "1076080"
  },
  {
    "text": "and weighs one ounce and cheap",
    "start": "1076080",
    "end": "1081200"
  },
  {
    "text": "so what do you do with it how do you make it autonomous one thing we're swooping under the scene",
    "start": "1081200",
    "end": "1086960"
  },
  {
    "text": "is that we're training this model on gpus and whatnot and you just cannot put the gpu on this thing right on the other",
    "start": "1086960",
    "end": "1094320"
  },
  {
    "text": "hand they can be very useful because hey they're cheap you can build a bunch",
    "start": "1094320",
    "end": "1099520"
  },
  {
    "text": "of them uh you can use them for gas leak detection for inspections",
    "start": "1099520",
    "end": "1106160"
  },
  {
    "text": "in kind of swarms and so on if you can kind of make that system fully autonomous",
    "start": "1106160",
    "end": "1112559"
  },
  {
    "text": "the this is collaboration with vijay janaparei for from harvard",
    "start": "1112559",
    "end": "1117760"
  },
  {
    "text": "so sure enough it turns out that between say intel core",
    "start": "1117760",
    "end": "1122799"
  },
  {
    "text": "i7 that we might be using for training and raspberry pi that might be on a robot",
    "start": "1122799",
    "end": "1128880"
  },
  {
    "text": "not only they're cheaper but like the cpu frequency is very very different right",
    "start": "1128880",
    "end": "1134080"
  },
  {
    "text": "and the power usage in all the nine yards and when we run two trajectories",
    "start": "1134080",
    "end": "1139600"
  },
  {
    "text": "the purple one is trained on the is executed on the same processor where it was trained whereas the green one it was",
    "start": "1139600",
    "end": "1146320"
  },
  {
    "text": "trained on intel processor but then execute it on raspberry pi",
    "start": "1146320",
    "end": "1152160"
  },
  {
    "text": "and there is big gap between the trajectories which is",
    "start": "1152160",
    "end": "1158480"
  },
  {
    "text": "the another source of sim to real gap something that we normally don't think",
    "start": "1158480",
    "end": "1163520"
  },
  {
    "text": "about but it's there and what ends up being is kind of we're",
    "start": "1163520",
    "end": "1169679"
  },
  {
    "text": "fixing bugs right on because probably just not doing what we're telling to do",
    "start": "1169679",
    "end": "1175600"
  },
  {
    "text": "so what we can do about that we're building this is available in open",
    "start": "1175600",
    "end": "1180640"
  },
  {
    "text": "source the ear learning which uses technical hardware in the",
    "start": "1180640",
    "end": "1186320"
  },
  {
    "text": "loop basically we run on the desktop our simulator",
    "start": "1186320",
    "end": "1191360"
  },
  {
    "text": "and we have the target processor we can plug in into the simulator and we're evaluating",
    "start": "1191360",
    "end": "1197840"
  },
  {
    "text": "the policies we evaluate the policies on the target simulator but they're being shown on the",
    "start": "1197840",
    "end": "1204320"
  },
  {
    "text": "uh sorry they're being shown at the",
    "start": "1204320",
    "end": "1211280"
  },
  {
    "text": "at the screen and so on so that way no robots are hurt during these experiments if they crash they crash",
    "start": "1211280",
    "end": "1217520"
  },
  {
    "text": "right plus we can kind of get important data about the latencies and so on and build that in the simulator",
    "start": "1217520",
    "end": "1224960"
  },
  {
    "text": "the so looking at the performance with the hardware in the loop is up to 38 percent",
    "start": "1224960",
    "end": "1230640"
  },
  {
    "text": "difference in the performance so policy that maybe had a 30 success rate it has",
    "start": "1230640",
    "end": "1235679"
  },
  {
    "text": "only 25 percent rate on the raspberry pi and so on the",
    "start": "1235679",
    "end": "1241760"
  },
  {
    "text": "but we can again taking that model putting in a simulator",
    "start": "1241760",
    "end": "1247120"
  },
  {
    "text": "we can do that so now let's talk about source finding how do we use this nanodrone",
    "start": "1247120",
    "end": "1253039"
  },
  {
    "text": "and teach it or train it to autonomously search for light",
    "start": "1253039",
    "end": "1258640"
  },
  {
    "text": "use light but you can replace the sensor so we built we put the light sensor on on",
    "start": "1258640",
    "end": "1264880"
  },
  {
    "text": "on the nano drone and the network needs to fit on the target processor in",
    "start": "1264880",
    "end": "1272240"
  },
  {
    "text": "the ram so the observation space cannot be huge we cannot put the camera so we just use the four",
    "start": "1272240",
    "end": "1278720"
  },
  {
    "text": "laser readings in four different directions and the action space is",
    "start": "1278720",
    "end": "1285120"
  },
  {
    "text": "uh very simple forward straight up and down it's more of the planning and then there's a controller underneath that's",
    "start": "1285120",
    "end": "1290480"
  },
  {
    "text": "going to go and track that trajectory and then for the light model that's going to simulate that we collect the",
    "start": "1290480",
    "end": "1296880"
  },
  {
    "text": "data observe how the light changes based on the distance and what not build that in a simulator and train it with",
    "start": "1296880",
    "end": "1304320"
  },
  {
    "text": "reinforcement learning and then use the okay let's see",
    "start": "1304320",
    "end": "1313120"
  },
  {
    "text": "so this is again that policy trend goes directly on the robot it go towards the light source",
    "start": "1313679",
    "end": "1319840"
  },
  {
    "text": "and so on so this is just kind of to think of whether from the machine learning",
    "start": "1319840",
    "end": "1325360"
  },
  {
    "text": "perspective i want to know if i have a model what processor do i",
    "start": "1325360",
    "end": "1330480"
  },
  {
    "text": "need to have or how large it needs to be or if i know",
    "start": "1330480",
    "end": "1335760"
  },
  {
    "text": "what my processor is how big the network should be and i think it's important source of sim to",
    "start": "1335760",
    "end": "1342240"
  },
  {
    "text": "real gap that we don't talk much so that runs up this line talk about",
    "start": "1342240",
    "end": "1350240"
  },
  {
    "text": "three sources of symptom real gap and potentially how to address them using a for mobile navigation very",
    "start": "1350240",
    "end": "1356400"
  },
  {
    "text": "simplified simulator and reward learning with neural network architecture",
    "start": "1356400",
    "end": "1362559"
  },
  {
    "text": "on the visual robot navigation combining two data sets",
    "start": "1362559",
    "end": "1368640"
  },
  {
    "text": "of the buildings and humans and creating a data set that we can kind of train and",
    "start": "1368640",
    "end": "1374559"
  },
  {
    "text": "avoid the privacy and safety issues associated with training in reality",
    "start": "1374559",
    "end": "1380720"
  },
  {
    "text": "and we use the self-supervision we generalize as the previous experience then as a model to aid the planning at",
    "start": "1380720",
    "end": "1387919"
  },
  {
    "text": "the runtime the to to train the model and enable it to run on the system",
    "start": "1387919",
    "end": "1394960"
  },
  {
    "text": "and finally we talked about the modeling the target processor and system design for these compact models and resource",
    "start": "1394960",
    "end": "1402320"
  },
  {
    "text": "constraint any questions here yeah i'm not sure if i missed it but did you",
    "start": "1402320",
    "end": "1408880"
  },
  {
    "text": "uh did it did i ever come up uh like do you run into any problems with like visibility changing between the",
    "start": "1408880",
    "end": "1414799"
  },
  {
    "text": "simulation and when you run it in real life or is that not a problem can you repeat the question what's",
    "start": "1414799",
    "end": "1420880"
  },
  {
    "text": "changing between the uh so feasibility like in the context of like the the middle section like safety critical",
    "start": "1420880",
    "end": "1427120"
  },
  {
    "text": "visual navigation you mentioned that you train using npc in the simulation and then you try to implement it in the real",
    "start": "1427120",
    "end": "1432559"
  },
  {
    "text": "world i'm just wondering like did you observe any changes in feasibility between those two domains or no",
    "start": "1432559",
    "end": "1438559"
  },
  {
    "text": "uh the feasibility changes because of the camera differences field of view the",
    "start": "1438559",
    "end": "1445120"
  },
  {
    "text": "mpc basically we have a number of the mpcs just use the computer cost",
    "start": "1445120",
    "end": "1452720"
  },
  {
    "text": "as a prediction right and then on on the on the robot once we have the trajectory",
    "start": "1452720",
    "end": "1458080"
  },
  {
    "text": "we have a controller that kind of goes and execute it towards the policy",
    "start": "1458080",
    "end": "1463120"
  },
  {
    "text": "as a separate so these two are kept the same at least in this case",
    "start": "1463120",
    "end": "1469279"
  },
  {
    "text": "but",
    "start": "1469279",
    "end": "1471440"
  },
  {
    "text": "when the robot can go reachable did you find uh that the policy sometimes",
    "start": "1474320",
    "end": "1480320"
  },
  {
    "text": "reaches a local minimum the robot is stuck or do you know if the robot was able to is it actually actively",
    "start": "1480320",
    "end": "1486640"
  },
  {
    "text": "exploring the map and building an internal representation or is it locally trying to reach the goal",
    "start": "1486640",
    "end": "1493360"
  },
  {
    "text": "good question it's reactive policy so it doesn't have a map and basically you can kind of think of",
    "start": "1493360",
    "end": "1500159"
  },
  {
    "text": "it that i need to be after this 300 meters in this direction down",
    "start": "1500159",
    "end": "1507760"
  },
  {
    "text": "in campus i don't know the map but that's kind of my so what am i going to do all right i'm",
    "start": "1507760",
    "end": "1513120"
  },
  {
    "text": "going to start going this way and i'm going to try to kind of keep the direction and there's a giant wall in",
    "start": "1513120",
    "end": "1518480"
  },
  {
    "text": "front of me i'm probably going to get stuck and the training we found during the",
    "start": "1518480",
    "end": "1523760"
  },
  {
    "text": "training that more training",
    "start": "1523760",
    "end": "1529520"
  },
  {
    "text": "helps specifically when we're giving the distances to the goal when we're",
    "start": "1529520",
    "end": "1534799"
  },
  {
    "text": "training how far are my goals that that's there is like a distribution so it's going to be able to go further",
    "start": "1534799",
    "end": "1541760"
  },
  {
    "text": "but not too much so if you were training into 10 meters it's going to be able maybe to go to 15 20 but not further",
    "start": "1541760",
    "end": "1548720"
  },
  {
    "text": "and then in the later that's not published we went and did the log of the distances",
    "start": "1548720",
    "end": "1555600"
  },
  {
    "text": "so at that point it really had no no constraint even if it was really kind",
    "start": "1555600",
    "end": "1561919"
  },
  {
    "text": "of entrenched in kind of like there was the goal was not reachable because it",
    "start": "1561919",
    "end": "1569279"
  },
  {
    "text": "was in inside the box or something like that you were gonna keep trying to go towards it and indefinitely but",
    "start": "1569279",
    "end": "1576480"
  },
  {
    "text": "he was able to kind of go up to 200 250 meters to find",
    "start": "1576480",
    "end": "1582960"
  },
  {
    "text": "the goal through kind of their corridors and whatnot",
    "start": "1582960",
    "end": "1588240"
  },
  {
    "text": "the architecture no in the in this case there is no lstm architecture",
    "start": "1588880",
    "end": "1594400"
  },
  {
    "text": "we do we do feed three last observations of the sensors which kind of gives it",
    "start": "1594400",
    "end": "1601360"
  },
  {
    "text": "noisy estimates of speed",
    "start": "1601360",
    "end": "1606000"
  },
  {
    "text": "sure any other questions before for the first part",
    "start": "1608480",
    "end": "1613679"
  },
  {
    "text": "i saw that you trained the policy and then you tried it on different robots so does the policy learn the profile of",
    "start": "1613679",
    "end": "1620240"
  },
  {
    "text": "the sensor what if for different robots some of the lighters are noisier or some of the lighters have a shorter range",
    "start": "1620240",
    "end": "1627360"
  },
  {
    "text": "what does that look like so going",
    "start": "1627360",
    "end": "1631919"
  },
  {
    "text": "differential drive to differential drive we didn't do additional training",
    "start": "1633760",
    "end": "1639840"
  },
  {
    "text": "the reason this sort of kind of worked is we actually had a fun demo where",
    "start": "1639840",
    "end": "1645440"
  },
  {
    "text": "we're kind of having people sit on faith and drive around we should start adding this",
    "start": "1645440",
    "end": "1653279"
  },
  {
    "text": "in the talk the reason why that intuitively works is because we're doing",
    "start": "1653279",
    "end": "1658480"
  },
  {
    "text": "basically planning at 10 hertz so if it's not quite there where we thought it was going to be it kind of",
    "start": "1658480",
    "end": "1663679"
  },
  {
    "text": "starts all over again for the other types of robots are car in the asteroid and so on we retrain the",
    "start": "1663679",
    "end": "1670159"
  },
  {
    "text": "model for we're basically using the same methodology",
    "start": "1670159",
    "end": "1675360"
  },
  {
    "text": "does it answer your question yeah all right",
    "start": "1675360",
    "end": "1680840"
  },
  {
    "text": "so on the second part let's talk about how to make to learn",
    "start": "1680840",
    "end": "1687039"
  },
  {
    "text": "better stuff and more complicated things so better generalization and harder",
    "start": "1687039",
    "end": "1692640"
  },
  {
    "text": "problems the i already started like with with the",
    "start": "1692640",
    "end": "1697760"
  },
  {
    "start": "1693000",
    "end": "1785000"
  },
  {
    "text": "reward search in neural network architecture when training reinforcement learning agents",
    "start": "1697760",
    "end": "1702960"
  },
  {
    "text": "we have lots of decisions to make the there are questions around how to",
    "start": "1702960",
    "end": "1708240"
  },
  {
    "text": "select reward neural network architecture which algorithm to choose what environment to train on and",
    "start": "1708240",
    "end": "1715520"
  },
  {
    "text": "the reality is as an engineer or researcher the you keep interacting with the system",
    "start": "1715520",
    "end": "1722000"
  },
  {
    "text": "observing what it does you make a new decision and so on that's the iterative process",
    "start": "1722000",
    "end": "1727520"
  },
  {
    "text": "reality is it's interaction between you as an engineer and the system that you're training and you're observing",
    "start": "1727520",
    "end": "1734559"
  },
  {
    "text": "you're collecting the data and you're making your decision in the sequential decision making process",
    "start": "1734559",
    "end": "1740799"
  },
  {
    "text": "right and we can kind of see this reinforcement learning system is a complex system",
    "start": "1740799",
    "end": "1746960"
  },
  {
    "text": "in a sense that there are co-dependencies between its inner parts what the reward is and the loss function",
    "start": "1746960",
    "end": "1753600"
  },
  {
    "text": "right and the reward that we still kind of just barely understand so if this is interaction between",
    "start": "1753600",
    "end": "1762320"
  },
  {
    "text": "a decision maker yourself and a system well then it should be learned",
    "start": "1762320",
    "end": "1768960"
  },
  {
    "text": "and you can remove yourself from the loop so we talked about two different ways to do this for the rewarded neural network",
    "start": "1768960",
    "end": "1775360"
  },
  {
    "text": "architecture but next thing i want to talk about the learning loss functions",
    "start": "1775360",
    "end": "1781440"
  },
  {
    "text": "and choosing the environments and learning curriculum so the learning loss functions",
    "start": "1781440",
    "end": "1788240"
  },
  {
    "start": "1785000",
    "end": "1791000"
  },
  {
    "text": "this was work that was at iclear this year the idea is there's lots of research",
    "start": "1788240",
    "end": "1794480"
  },
  {
    "start": "1791000",
    "end": "2166000"
  },
  {
    "text": "over the last couple of years on reinforcement learning algorithms and seems like is a new algorithm coming",
    "start": "1794480",
    "end": "1800240"
  },
  {
    "text": "every i don't know two weeks or not people are tweaking this people",
    "start": "1800240",
    "end": "1805760"
  },
  {
    "text": "are picking that which kind of raises fundamental questions is the way we're developing these algorithms sound",
    "start": "1805760",
    "end": "1812640"
  },
  {
    "text": "are the benchmarks we're using sound because if they're not sound then why we're kind of keep changing this when",
    "start": "1812640",
    "end": "1819440"
  },
  {
    "text": "you have domain specific or whatnot but let's kind of that's kind of more of the longer term question",
    "start": "1819440",
    "end": "1824880"
  },
  {
    "text": "anyway the since there's lots of tweaks and improvements can we automate the process",
    "start": "1824880",
    "end": "1830720"
  },
  {
    "text": "and can we learn new algorithm and the insight is that the loss function here is really a computational",
    "start": "1830720",
    "end": "1837039"
  },
  {
    "text": "graph this is a dq and loss function where it kind of adds certain terms and so on",
    "start": "1837039",
    "end": "1843200"
  },
  {
    "text": "so if this is a computational graph can we use evolutionary methods to replace pieces reconnect the graphs and",
    "start": "1843200",
    "end": "1850320"
  },
  {
    "text": "maybe come up with something better and that's what we kind of set out to do so almost similar idea like we did with",
    "start": "1850320",
    "end": "1857120"
  },
  {
    "text": "the rewards right but now in the search space here is graphs",
    "start": "1857120",
    "end": "1862960"
  },
  {
    "text": "the we have our set of training environments that use three",
    "start": "1862960",
    "end": "1868880"
  },
  {
    "text": "first one is hurdle environment which is the inverted pendulum and the idea is",
    "start": "1868880",
    "end": "1873919"
  },
  {
    "text": "it's very easy to fail and it's fast to fail and if you can train on the inverted pendulum you have no business",
    "start": "1873919",
    "end": "1879279"
  },
  {
    "text": "don't go going further and then if you graduate then you get two very simple maze problems to train",
    "start": "1879279",
    "end": "1885840"
  },
  {
    "text": "on and then once the algorithm is or the loss function is evaluated it's",
    "start": "1885840",
    "end": "1890960"
  },
  {
    "text": "evaluated against well using the same the performance on these two environments",
    "start": "1890960",
    "end": "1897039"
  },
  {
    "text": "we send that to the mutator that maintains the population there is a trick here that because the",
    "start": "1897039",
    "end": "1904320"
  },
  {
    "text": "search space is large we also hash these functions loss functions so if we see that",
    "start": "1904320",
    "end": "1910960"
  },
  {
    "text": "the function with the same hash already appeared we're now going to retrain it again but we're going to just use that",
    "start": "1910960",
    "end": "1916480"
  },
  {
    "text": "performance as well yeah i'm wondering how are the viral algorithms that you take parameterize",
    "start": "1916480",
    "end": "1923919"
  },
  {
    "text": "like what are the things you're mutating about so we're mutating the graph okay the",
    "start": "1923919",
    "end": "1930480"
  },
  {
    "text": "yeah so here here was the graph and say okay the graph the operations are",
    "start": "1930480",
    "end": "1935919"
  },
  {
    "text": "additions multiplication summations query yeah and then",
    "start": "1935919",
    "end": "1941519"
  },
  {
    "text": "connections and when we're proposing when the mutator proposes the new graph beyond",
    "start": "1941519",
    "end": "1946880"
  },
  {
    "text": "the hashing it also looks that it's well posed which means that the input or state in action and reward",
    "start": "1946880",
    "end": "1954799"
  },
  {
    "text": "and output is a scalar so anything that's not a scalar is pruned and if it doesn't produce scaling scalar it just",
    "start": "1954799",
    "end": "1962559"
  },
  {
    "text": "lost away so that what comes out of the mutator is actually where form graph that stands a chance",
    "start": "1962559",
    "end": "1970159"
  },
  {
    "text": "and then we can start from scratch in training in which case we actually rediscovered the td algorithm",
    "start": "1970159",
    "end": "1977200"
  },
  {
    "text": "the or we can bootstrap it with in this case with dqn",
    "start": "1977200",
    "end": "1982640"
  },
  {
    "text": "and we'll kind of see we test it on the test environment that are not seen",
    "start": "1982640",
    "end": "1988960"
  },
  {
    "text": "so bootstrapped through the dqn we get with this algorithm which we call dq and regularized or dq",
    "start": "1988960",
    "end": "1995519"
  },
  {
    "text": "and reg the what was interesting it's really a version of the entropy regularization",
    "start": "1995519",
    "end": "2002559"
  },
  {
    "text": "it has a term that early on in training it kind of tampers down the queue function it's very similar to like a cql",
    "start": "2002559",
    "end": "2010240"
  },
  {
    "text": "or one one of these new ones that are kind of coming out more recently",
    "start": "2010240",
    "end": "2018080"
  },
  {
    "text": "the it does regularize uh generalized to unseen mini grids",
    "start": "2018080",
    "end": "2023760"
  },
  {
    "text": "the purple line is the new algorithm and then the westerner like ddqn",
    "start": "2023760",
    "end": "2028960"
  },
  {
    "text": "and cql and so on and there is a video it almost seems like it aids with the",
    "start": "2028960",
    "end": "2034240"
  },
  {
    "text": "exploration which is kind of weird that's not what was optimized for or so on",
    "start": "2034240",
    "end": "2040080"
  },
  {
    "text": "the second one is it generalized atari games we try this",
    "start": "2040080",
    "end": "2045200"
  },
  {
    "text": "four the numbers on the left are reported from their respective papers and it beats all four of them",
    "start": "2045200",
    "end": "2051839"
  },
  {
    "text": "which is very interesting never seen visual observations right and so on",
    "start": "2051839",
    "end": "2058800"
  },
  {
    "text": "now cool thing about it this is a graph output is a math formula",
    "start": "2058800",
    "end": "2066638"
  },
  {
    "text": "so it's completely interpretable and we can use the same methods that we now analyze",
    "start": "2066639",
    "end": "2073358"
  },
  {
    "text": "human-created algorithms they can serve these learned algorithms can serve as an",
    "start": "2073359",
    "end": "2078398"
  },
  {
    "text": "inspiration for human design right or can be served and so on",
    "start": "2078399",
    "end": "2083520"
  },
  {
    "text": "so it's consistently delivered up to the constant so change the seeds this guy",
    "start": "2083520",
    "end": "2089520"
  },
  {
    "text": "keeps coming up with this search space we open sourced and made available",
    "start": "2089520",
    "end": "2096320"
  },
  {
    "text": "database of top thousand performing algorithms if anybody is interested in",
    "start": "2096320",
    "end": "2101520"
  },
  {
    "text": "kind of doing a theoretical analysis on them and so on is there",
    "start": "2101520",
    "end": "2106880"
  },
  {
    "text": "what's interesting is this is really first time that we have tens of thousands or twenty thousand",
    "start": "2106880",
    "end": "2113280"
  },
  {
    "text": "tens of thousands reinforcement learning algorithms at the disposal",
    "start": "2113280",
    "end": "2119440"
  },
  {
    "text": "and we haven't like as a community we didn't have that before",
    "start": "2119440",
    "end": "2124560"
  },
  {
    "text": "so what's interesting when i kind of plot the histogram of the performance is that even though we have so many",
    "start": "2124560",
    "end": "2132079"
  },
  {
    "text": "algorithms we maybe only have or the order of 100 different performances",
    "start": "2132079",
    "end": "2139200"
  },
  {
    "text": "which makes me wonder i don't have an answer for this i'd love to hear an answer is is there some kind of strange",
    "start": "2139200",
    "end": "2144640"
  },
  {
    "text": "equivalence class that we're not aware of that many of these loss functions and so on",
    "start": "2144640",
    "end": "2151520"
  },
  {
    "text": "map to the same performance again database is there i'd love to kind of hear an answer on that",
    "start": "2151520",
    "end": "2158800"
  },
  {
    "text": "d and so on and we're experimenting now with the continuous action algorithms in this space but anyway",
    "start": "2158800",
    "end": "2166960"
  },
  {
    "start": "2166000",
    "end": "2196000"
  },
  {
    "text": "so the second part and we have okay good with time talk about generative",
    "start": "2167200",
    "end": "2173599"
  },
  {
    "text": "curriculum for compositional tasks this is kind of a question what if we want to do a really complex tasks",
    "start": "2173599",
    "end": "2180880"
  },
  {
    "text": "tasks that are kind of growing in complex complexity and having emerging complexity instead of what kind of",
    "start": "2180880",
    "end": "2187119"
  },
  {
    "text": "robotics is doing now maybe we're learning fixed number of tasks and so on the question is what environments do we",
    "start": "2187119",
    "end": "2193599"
  },
  {
    "text": "feed them and how we train them so for this i want to kind of take a step",
    "start": "2193599",
    "end": "2199760"
  },
  {
    "start": "2196000",
    "end": "2377000"
  },
  {
    "text": "aside from robotics for a moment and talk about the autonomous web navigation",
    "start": "2199760",
    "end": "2205200"
  },
  {
    "text": "and we first say okay while we're talking about web navigation i'm going to argue that this is a mobile",
    "start": "2205200",
    "end": "2210480"
  },
  {
    "text": "navigation task so consider a task where you're trying to purchase a flight or purchase something",
    "start": "2210480",
    "end": "2218079"
  },
  {
    "text": "on amazon or or order a food you have a set of pages",
    "start": "2218079",
    "end": "2224560"
  },
  {
    "text": "each page even on the same website changes right sometimes you have different ad",
    "start": "2224560",
    "end": "2231520"
  },
  {
    "text": "sometimes you have this or that the you",
    "start": "2231520",
    "end": "2236800"
  },
  {
    "text": "enter information you change the state of that's your manipulation aspect right after several manipulation",
    "start": "2236800",
    "end": "2244480"
  },
  {
    "text": "aspects you hit that submit button you move to the next room right",
    "start": "2244480",
    "end": "2250160"
  },
  {
    "text": "it's a mobile manipulation there are things that you can mess up there the",
    "start": "2250160",
    "end": "2255200"
  },
  {
    "text": "you can order thousand of something not a good thing right the",
    "start": "2255200",
    "end": "2261599"
  },
  {
    "text": "there are things that that ends going contact customer service",
    "start": "2261599",
    "end": "2267040"
  },
  {
    "text": "that's completely not relevant to your task and kind of gets gets you derailed and so on",
    "start": "2267040",
    "end": "2272720"
  },
  {
    "text": "action space is huge you need to know what information to input where action space changes from page to page from",
    "start": "2272720",
    "end": "2280880"
  },
  {
    "text": "element to element what is it you put there and then primitives what are in",
    "start": "2280880",
    "end": "2285920"
  },
  {
    "text": "this case things like okay enter first name or better dates enter date okay is this a drop down is",
    "start": "2285920",
    "end": "2293680"
  },
  {
    "text": "it a text is it is it a little pop pop out right so there are many many",
    "start": "2293680",
    "end": "2300640"
  },
  {
    "text": "different manifestations how that can do and in this case we actually work from the html side and the dom side so the",
    "start": "2300640",
    "end": "2307680"
  },
  {
    "text": "variation of how even the date or first name or something can manifest the semantic",
    "start": "2307680",
    "end": "2315119"
  },
  {
    "text": "primitive how it can manifest their countable ways",
    "start": "2315119",
    "end": "2320160"
  },
  {
    "text": "to do that so it's very complex space to work with and one thing that we",
    "start": "2320160",
    "end": "2326640"
  },
  {
    "text": "do as people is we do this zero shot okay maybe you kind of go to a new website to do something",
    "start": "2326640",
    "end": "2334160"
  },
  {
    "text": "and you kind of do it right you you figure it out without kind of additional training and so on so can we have a",
    "start": "2334160",
    "end": "2342079"
  },
  {
    "text": "single agent that solves any task in these class of related tasks",
    "start": "2342079",
    "end": "2348160"
  },
  {
    "text": "the so where is kind of the current state of the art in in the web navigation space",
    "start": "2348160",
    "end": "2354079"
  },
  {
    "text": "commercial websites so like southwest and so on have dom trees with 30 000 elements",
    "start": "2354079",
    "end": "2361680"
  },
  {
    "text": "on on the on the web page the benchmark that",
    "start": "2361680",
    "end": "2367200"
  },
  {
    "text": "existed has hundred and there is a huge gap that's kind of",
    "start": "2367200",
    "end": "2373119"
  },
  {
    "text": "what's needed for the real world and kind of where the benchmarks are and then the previous state of the art",
    "start": "2373119",
    "end": "2379839"
  },
  {
    "start": "2377000",
    "end": "2599000"
  },
  {
    "text": "in a sense was learning for a single",
    "start": "2379839",
    "end": "2385440"
  },
  {
    "text": "domain so single website and what not using millions of examples",
    "start": "2385440",
    "end": "2391599"
  },
  {
    "text": "and they're just not going to fly in in reality so we want to do autonomous navigation",
    "start": "2391599",
    "end": "2399440"
  },
  {
    "text": "in these unreal real environments which means again we need to train in",
    "start": "2399440",
    "end": "2405359"
  },
  {
    "text": "simulation and the classic methods and whatnot they're just not going to work we need to go towards some sort of curriculum",
    "start": "2405359",
    "end": "2413040"
  },
  {
    "text": "right and maybe we have this interaction of a model basically the idea is to have",
    "start": "2413040",
    "end": "2419599"
  },
  {
    "text": "interaction between a model that observes how students or the",
    "start": "2419599",
    "end": "2424960"
  },
  {
    "text": "agents under training are performing and then it proposes the environment",
    "start": "2424960",
    "end": "2431119"
  },
  {
    "text": "that is the right level for their skill level and kind of goes back and forth",
    "start": "2431119",
    "end": "2436720"
  },
  {
    "text": "so but first we now need to know what we're learning so we formalize this uh task space as",
    "start": "2436720",
    "end": "2443040"
  },
  {
    "text": "petrinas and if you're not familiar with patronage they're graphs",
    "start": "2443040",
    "end": "2448640"
  },
  {
    "text": "and there are two types of nodes there are circles which are states and there are rectangles which are",
    "start": "2448640",
    "end": "2454160"
  },
  {
    "text": "triggers which an external agent basically triggers an action and then there is",
    "start": "2454160",
    "end": "2460400"
  },
  {
    "text": "like this little circle that says okay now we're kind of doing a state transition between them",
    "start": "2460400",
    "end": "2468000"
  },
  {
    "text": "and this really nicely describes we make a connection between petri net and",
    "start": "2468000",
    "end": "2473599"
  },
  {
    "text": "bomb mdps what are the observations right and so on by using petronas we can formalize",
    "start": "2473599",
    "end": "2480880"
  },
  {
    "text": "what the task space is so for the web navigation in particular",
    "start": "2480880",
    "end": "2487760"
  },
  {
    "text": "we can say that our primitives are first names and so on and there is associated for that",
    "start": "2487760",
    "end": "2495040"
  },
  {
    "text": "semantic we have a number or the set of primitives that kind of match",
    "start": "2495040",
    "end": "2500880"
  },
  {
    "text": "the and then the pages we have a gate which is basically a bottleneck state and then",
    "start": "2500880",
    "end": "2506720"
  },
  {
    "text": "the next state and so on so there is a structure to that task and since there is one-to-one mapping",
    "start": "2506720",
    "end": "2512240"
  },
  {
    "text": "between the petrinet and palm mdp our problem becomes then that we want to learn a",
    "start": "2512240",
    "end": "2518640"
  },
  {
    "text": "single policy that maximize expecting return or that family",
    "start": "2518640",
    "end": "2524640"
  },
  {
    "text": "of palm mdps that are defined or induced by the set of the petrinets",
    "start": "2524640",
    "end": "2531359"
  },
  {
    "text": "so now that we have structure of the task we can create a simulator",
    "start": "2531359",
    "end": "2537200"
  },
  {
    "text": "that builds these websites because the petrinas tells us where the connections are and then",
    "start": "2537200",
    "end": "2543440"
  },
  {
    "text": "primitives can be sampled out of the database as as they are needed so this is precisely what we do for",
    "start": "2543440",
    "end": "2550160"
  },
  {
    "text": "example here this is just kind of illustrative for the uh southwest website that there are",
    "start": "2550160",
    "end": "2556720"
  },
  {
    "text": "different uh kind of paths to to generate the branches and whatnot",
    "start": "2556720",
    "end": "2562720"
  },
  {
    "text": "so the first step for the simulator is that we create a set of primitive objects this would be our little dom",
    "start": "2562720",
    "end": "2568800"
  },
  {
    "text": "elements that you can scrape from web and whatnot and the now that we have",
    "start": "2568800",
    "end": "2575280"
  },
  {
    "text": "the to create the website out of that set of primitives we can decide how many pages we have and sample out of it and",
    "start": "2575280",
    "end": "2582720"
  },
  {
    "text": "connect them the and then on top of that",
    "start": "2582720",
    "end": "2588640"
  },
  {
    "text": "the simulator just kind of allows us to do that the question now is okay which ones should we create how do we create",
    "start": "2588640",
    "end": "2595119"
  },
  {
    "text": "and so on and this is where it kind of gets into the methods so we call this compositional design of",
    "start": "2595119",
    "end": "2601520"
  },
  {
    "start": "2599000",
    "end": "2818000"
  },
  {
    "text": "the environments or the code and it is a multi-agent system right",
    "start": "2601520",
    "end": "2606960"
  },
  {
    "text": "the we have a designer lstm that first selects how many pages are",
    "start": "2606960",
    "end": "2613119"
  },
  {
    "text": "going to be on the website and then goes in and basically the codes",
    "start": "2613119",
    "end": "2618800"
  },
  {
    "text": "and selects the primitives and connects them into websites",
    "start": "2618800",
    "end": "2624079"
  },
  {
    "text": "that created website which by the way it's not semantically correct because there is nothing to forces semantics we",
    "start": "2624079",
    "end": "2630960"
  },
  {
    "text": "can give have 35 dates there",
    "start": "2630960",
    "end": "2636400"
  },
  {
    "text": "and so on is being presented to the age population of agents and the",
    "start": "2636400",
    "end": "2642880"
  },
  {
    "text": "training they are trained on that environment for a number of step and then the designer observes",
    "start": "2642880",
    "end": "2650400"
  },
  {
    "text": "the regret which is estimated as the range of",
    "start": "2650400",
    "end": "2656240"
  },
  {
    "text": "how well this population of agents did so did they all do well or did they not",
    "start": "2656240",
    "end": "2661280"
  },
  {
    "text": "do well and so on or there's like a if there is a variety that's a good thing",
    "start": "2661280",
    "end": "2667040"
  },
  {
    "text": "and then the second one is what's the best agent's performance so these are the two signals",
    "start": "2667040",
    "end": "2673119"
  },
  {
    "text": "and from there the designer has this two-part laws that",
    "start": "2673119",
    "end": "2678560"
  },
  {
    "text": "first part of the losses this size is the budget so if the agent the best performing",
    "start": "2678560",
    "end": "2683839"
  },
  {
    "text": "agent is not doing well it's basically encouraged to drop the difficulty and kind of make things",
    "start": "2683839",
    "end": "2691040"
  },
  {
    "text": "easier and if the best performing agent is doing well it's encouraged to increase",
    "start": "2691040",
    "end": "2696319"
  },
  {
    "text": "the difficulty and make things more challenging and complete more complicated tasks",
    "start": "2696319",
    "end": "2702000"
  },
  {
    "text": "and the second part of the last function is the regret which kind of looks at the range",
    "start": "2702000",
    "end": "2707760"
  },
  {
    "text": "of the how well the agents are performing and if they're though all performing",
    "start": "2707760",
    "end": "2713520"
  },
  {
    "text": "differently it's encouraged to pick different primitives to make things more interesting and challenging and so on so",
    "start": "2713520",
    "end": "2721200"
  },
  {
    "text": "it's not kind of all uniform so basically we have two",
    "start": "2721200",
    "end": "2727200"
  },
  {
    "text": "models being co-trained at the same time really not knowing about each other",
    "start": "2727200",
    "end": "2732640"
  },
  {
    "text": "the and then for the evaluation we manually created websites that",
    "start": "2732640",
    "end": "2739760"
  },
  {
    "text": "are not any of these generated and we evaluate them and these are semantically correct",
    "start": "2739760",
    "end": "2747119"
  },
  {
    "text": "so what are the results the first we notice that early on in",
    "start": "2747119",
    "end": "2752240"
  },
  {
    "text": "training they're very simple couple of fields and so on later on they",
    "start": "2752240",
    "end": "2757280"
  },
  {
    "text": "become more complex which is kind of cool that's kind of what we expected second the blue line is the performance",
    "start": "2757280",
    "end": "2763599"
  },
  {
    "text": "of our model the red and green lines are",
    "start": "2763599",
    "end": "2769040"
  },
  {
    "text": "domain randomization manual curriculum and paired with just has one of the laws",
    "start": "2769040",
    "end": "2775839"
  },
  {
    "text": "doesn't have the budgeting loss and so on so clearly this one keeps",
    "start": "2775839",
    "end": "2781119"
  },
  {
    "text": "going the end we kind of see generalization",
    "start": "2781119",
    "end": "2786480"
  },
  {
    "text": "the across the time and then the third thing we notice that the distribution of the primitives that",
    "start": "2786480",
    "end": "2792640"
  },
  {
    "text": "are being selected shifts during the training early on it kind of basically more or",
    "start": "2792640",
    "end": "2798480"
  },
  {
    "text": "less uniformly uh selects the primitives where the later on kind of starts",
    "start": "2798480",
    "end": "2804319"
  },
  {
    "text": "selecting the elements that are more relevant to the task so there would be less of images added",
    "start": "2804319",
    "end": "2811200"
  },
  {
    "text": "and more of okay give me the date field and stuff that the agents need to practice",
    "start": "2811200",
    "end": "2818240"
  },
  {
    "start": "2818000",
    "end": "2934000"
  },
  {
    "text": "the and then the question was okay this so this is a synthetic benchmark that we",
    "start": "2818720",
    "end": "2825280"
  },
  {
    "text": "created 40 primitives and so on does they hold water in reality",
    "start": "2825280",
    "end": "2830480"
  },
  {
    "text": "so for that we went and scraped 3 500",
    "start": "2830480",
    "end": "2835520"
  },
  {
    "text": "this kind of dom elements primitives of the real websites indeed this induces the learnable space",
    "start": "2835520",
    "end": "2842880"
  },
  {
    "text": "of 10 to the power 31 so it's kind of huge the and the difference between this is that",
    "start": "2842880",
    "end": "2850640"
  },
  {
    "text": "now the primitives out of distribution from those that we have in our simulator",
    "start": "2850640",
    "end": "2857040"
  },
  {
    "text": "the this is like human coded or machine coded right and so on so this particular set",
    "start": "2857040",
    "end": "2864079"
  },
  {
    "text": "of the primitives that we used was kind of heavily focused over its password changing",
    "start": "2864079",
    "end": "2869440"
  },
  {
    "text": "the it has a lot of login and password in terms of the primitive and",
    "start": "2869440",
    "end": "2875359"
  },
  {
    "text": "not many of the other so in terms of the results the this is on the benchmark the login",
    "start": "2875359",
    "end": "2883520"
  },
  {
    "text": "is the highest performing with about 60 plus success rate whereas the others that",
    "start": "2883520",
    "end": "2888960"
  },
  {
    "text": "does not have the proper distribution of the primitives are much lower about 20s",
    "start": "2888960",
    "end": "2895760"
  },
  {
    "text": "but then we went and evaluated that model on the in distribution the held",
    "start": "2895760",
    "end": "2902240"
  },
  {
    "text": "out set that came out of the the real website so at this point now they're in distribution primitive",
    "start": "2902240",
    "end": "2909200"
  },
  {
    "text": "and we're getting accuracy about 93 the which kind of",
    "start": "2909200",
    "end": "2914880"
  },
  {
    "text": "i'm super excited about this because it means that the benchmark is actually",
    "start": "2914880",
    "end": "2921680"
  },
  {
    "text": "refl accurately reflecting the difficulty of the tasks we want to learn",
    "start": "2921760",
    "end": "2926880"
  },
  {
    "text": "and the so the the benchmark is open source now and and it's available",
    "start": "2926880",
    "end": "2932640"
  },
  {
    "text": "so we have we're good and good with that so on better generalization we talked",
    "start": "2932640",
    "end": "2938240"
  },
  {
    "start": "2934000",
    "end": "3061000"
  },
  {
    "text": "about learning reinforcement learning algorithms and exciting part thing about",
    "start": "2938240",
    "end": "2944079"
  },
  {
    "text": "that is that these are interpretable loss functions that we can use in co-design",
    "start": "2944079",
    "end": "2951119"
  },
  {
    "text": "with humans or use our tool set that we know and love to to learn more about reinforcement",
    "start": "2951119",
    "end": "2957280"
  },
  {
    "text": "learning algorithms uh also on this line we saw that evolution",
    "start": "2957280",
    "end": "2963680"
  },
  {
    "text": "changes the energy agent itself whereas on the web navigation",
    "start": "2963680",
    "end": "2968839"
  },
  {
    "text": "part we have a very large task space and we're learning a generative model",
    "start": "2968839",
    "end": "2975839"
  },
  {
    "text": "that learns the curriculum and just changes the training data for the agent and the training",
    "start": "2975839",
    "end": "2981839"
  },
  {
    "text": "so it's a very different paradigm for the learned curriculum",
    "start": "2981839",
    "end": "2987040"
  },
  {
    "text": "the curriculum learning only observes how the agents under training are performing",
    "start": "2987040",
    "end": "2994800"
  },
  {
    "text": "they can be we are using rl here can be imitation learning hackie can be humans",
    "start": "2994800",
    "end": "3000480"
  },
  {
    "text": "right as long as we just can observe how well they're doing",
    "start": "3000480",
    "end": "3005599"
  },
  {
    "text": "that's all good and fine the and then on the other hand from the perspective of the reinforcement",
    "start": "3005599",
    "end": "3011920"
  },
  {
    "text": "learning agent that we're training here it doesn't know anything about the curriculum",
    "start": "3011920",
    "end": "3017119"
  },
  {
    "text": "right and the only thing it knows is okay this is the environment i should train on",
    "start": "3017119",
    "end": "3022160"
  },
  {
    "text": "this is the exercise i should do and so on and where it's coming from and so on it has no idea",
    "start": "3022160",
    "end": "3029200"
  },
  {
    "text": "and the only difference is between this work and the one that i said oh it takes million samples",
    "start": "3029200",
    "end": "3034960"
  },
  {
    "text": "to learn on a single environment neural network architecture reward all the same",
    "start": "3034960",
    "end": "3040960"
  },
  {
    "text": "the only thing that we change is what is being fed to this agent and that's kind of mind-boggling right",
    "start": "3040960",
    "end": "3048160"
  },
  {
    "text": "that so i think there's more to be said about",
    "start": "3048160",
    "end": "3054319"
  },
  {
    "text": "that so you can see that these kind of things can be learned",
    "start": "3054319",
    "end": "3060160"
  },
  {
    "text": "but what are the next steps then",
    "start": "3060160",
    "end": "3063680"
  },
  {
    "start": "3061000",
    "end": "3553000"
  },
  {
    "text": "i'm really curious about this what is the fundamental role of nurture versus nature in rl",
    "start": "3066640",
    "end": "3072559"
  },
  {
    "text": "going back in the difference between evolutionary algorithms change the neural network architectural laws and",
    "start": "3072559",
    "end": "3079040"
  },
  {
    "text": "whatnot the systems perspective of the agents and",
    "start": "3079040",
    "end": "3084400"
  },
  {
    "text": "the web navigation part and the learn curriculum just changes what the agent is seeing",
    "start": "3084400",
    "end": "3089839"
  },
  {
    "text": "right the data what is the interplay how do we combine the two should we combine them do we do",
    "start": "3089839",
    "end": "3097280"
  },
  {
    "text": "i don't know i would love to see i don't have answers i'd like to see kind of more going down that direction",
    "start": "3097280",
    "end": "3104000"
  },
  {
    "text": "second question is should we do there has been like endless",
    "start": "3104000",
    "end": "3109599"
  },
  {
    "text": "debate around do we do simulation in the loop or data only and so on there's been so many debates",
    "start": "3109599",
    "end": "3116480"
  },
  {
    "text": "and questions and so on with a recent raise of large language models",
    "start": "3116480",
    "end": "3123599"
  },
  {
    "text": "which are kind of showing that with lots of data maybe we don't need a simulator on the other hand in robotics",
    "start": "3123599",
    "end": "3131040"
  },
  {
    "text": "can we ever have that much data i don't know and one thing that kind of supervised",
    "start": "3131040",
    "end": "3137200"
  },
  {
    "text": "models are doing well are they doing well when the data is clean",
    "start": "3137200",
    "end": "3142240"
  },
  {
    "text": "robotics by nature the robotics is not clean the sensors are noisy and so on so i think the",
    "start": "3142240",
    "end": "3149280"
  },
  {
    "text": "supervised community can learn something from robotics in terms of how to deal with the noisy",
    "start": "3149280",
    "end": "3154559"
  },
  {
    "text": "stuff the question is whether robotics can kind of go and abandon simulators and what not or what",
    "start": "3154559",
    "end": "3161599"
  },
  {
    "text": "should be a happy compromise but i would expect that actually might have some consensus on that debate in the next",
    "start": "3161599",
    "end": "3168000"
  },
  {
    "text": "couple years we'll see third part that i'm kind of interested",
    "start": "3168000",
    "end": "3174960"
  },
  {
    "text": "is i'm going to talk a bit about the constraint systems",
    "start": "3174960",
    "end": "3181040"
  },
  {
    "text": "is can we have can we get to the point that we have",
    "start": "3181040",
    "end": "3186480"
  },
  {
    "text": "application targeted system where we can co-design with the application target application",
    "start": "3186480",
    "end": "3193119"
  },
  {
    "text": "in mind then we can design both software and the hardware architecture some of",
    "start": "3193119",
    "end": "3198720"
  },
  {
    "text": "our preliminary work so we can perform we can get maybe",
    "start": "3198720",
    "end": "3204319"
  },
  {
    "text": "two to three times performance gains if we co-design accelerators with the neural network architectures",
    "start": "3204319",
    "end": "3210720"
  },
  {
    "text": "and so on but being able to say this is my specs for the task and",
    "start": "3210720",
    "end": "3216079"
  },
  {
    "text": "the environments that i want whatever they are and you kind of click a button and you have your system designed and your chip",
    "start": "3216079",
    "end": "3222240"
  },
  {
    "text": "shipped that would be really exciting and then last but not least",
    "start": "3222240",
    "end": "3229920"
  },
  {
    "text": "one of the major hurdles that prevents adoption of autonomous",
    "start": "3229920",
    "end": "3235920"
  },
  {
    "text": "systems and robotics and so on is user trust well 50 percent i think it's 53 percent",
    "start": "3235920",
    "end": "3243680"
  },
  {
    "text": "of users do not want autonomous data that whether it's robots or driving",
    "start": "3243680",
    "end": "3250160"
  },
  {
    "text": "cars surgeons and whatnot in their lives because they don't trust",
    "start": "3250160",
    "end": "3255200"
  },
  {
    "text": "the system so what are the methods that we can build that user trust",
    "start": "3255200",
    "end": "3260880"
  },
  {
    "text": "interact with the user verification methods large benchmarks",
    "start": "3260880",
    "end": "3266000"
  },
  {
    "text": "and so on to actually solve that because if we don't have user trust we're just doing academic research",
    "start": "3266000",
    "end": "3272960"
  },
  {
    "text": "we're not going to change anybody's lives and the studies i kind of just my thoughts",
    "start": "3272960",
    "end": "3279040"
  },
  {
    "text": "and with that i want to thank my collaborators and thank you i think we have a few more questions for",
    "start": "3279040",
    "end": "3286880"
  },
  {
    "text": "thank you so much uh alexandra so are there any questions in the audience also",
    "start": "3290480",
    "end": "3296480"
  },
  {
    "text": "uh people on zoom feel also free to unmute yourself and ask the questions so there's the first one there",
    "start": "3296480",
    "end": "3304319"
  },
  {
    "text": "the environment uh the mobile installation uh it reminded me a lot of uh generative serial networks and so i",
    "start": "3305200",
    "end": "3311440"
  },
  {
    "text": "was wondering if you ran into training stability issues there as well where it would get stuck in open minima",
    "start": "3311440",
    "end": "3318079"
  },
  {
    "text": "or maybe would optimize so that all of the environments were very simple but very hard and",
    "start": "3318079",
    "end": "3323119"
  },
  {
    "text": "wonderful so the first shot will be to be for the so we started with paired method",
    "start": "3323119",
    "end": "3330400"
  },
  {
    "text": "that came out of the last year or so that only used the",
    "start": "3330400",
    "end": "3335680"
  },
  {
    "text": "regret loss and there was talking in the minima it wasn't learning and so on and that",
    "start": "3335680",
    "end": "3341920"
  },
  {
    "text": "actually prompted us towards a expanding with the budget tecano allows",
    "start": "3341920",
    "end": "3346960"
  },
  {
    "text": "us to do these long horizon tasks the because then we can kind of control",
    "start": "3346960",
    "end": "3352400"
  },
  {
    "text": "the difficulty and second it really prompted us",
    "start": "3352400",
    "end": "3357440"
  },
  {
    "text": "towards thinking about the structure of the task what turned out is that the task that appeared was originally",
    "start": "3357440",
    "end": "3364799"
  },
  {
    "text": "uh developed for was not was for the open-ended tasks",
    "start": "3364799",
    "end": "3370079"
  },
  {
    "text": "and the feasibility of the task was not was basically what paired was originally",
    "start": "3370079",
    "end": "3377440"
  },
  {
    "text": "differentiating for to create the feasible tasks and so on in this case",
    "start": "3377440",
    "end": "3383839"
  },
  {
    "text": "we saw that if we say this there is a structure in this task",
    "start": "3383839",
    "end": "3389280"
  },
  {
    "text": "and we can kind of define them grammar question than is how do you define different tests but beyond that that",
    "start": "3389280",
    "end": "3394720"
  },
  {
    "text": "they are feasible your providing environment is solvable and that kind of helped going out of the",
    "start": "3394720",
    "end": "3401359"
  },
  {
    "text": "local minima and stopping the",
    "start": "3401359",
    "end": "3406960"
  },
  {
    "text": "yeah i think this more question",
    "start": "3406960",
    "end": "3412798"
  },
  {
    "text": "when you're testing a new environment did you have a way to quantify the difference between the training",
    "start": "3414480",
    "end": "3420480"
  },
  {
    "text": "environments and the testing environment and did you observe like performance degradation then",
    "start": "3420480",
    "end": "3426480"
  },
  {
    "text": "the environment is now too different than what he's trained for that's a good",
    "start": "3426480",
    "end": "3432400"
  },
  {
    "text": "question there was another line of work that i didn't talk about that was looking at",
    "start": "3432400",
    "end": "3438319"
  },
  {
    "text": "the metric between the new and this was kind of more the fine tuning the but not in this case it's actually",
    "start": "3438319",
    "end": "3444000"
  },
  {
    "text": "very good question if i know observe the the similarities",
    "start": "3444000",
    "end": "3449920"
  },
  {
    "text": "finally the similarity metric is not a most real thing though",
    "start": "3449920",
    "end": "3454720"
  },
  {
    "text": "any more questions also anyone on zoom uh happy to hear your questions if you",
    "start": "3455440",
    "end": "3460480"
  },
  {
    "text": "want to unmute yourself where the i would guess the nature part",
    "start": "3460480",
    "end": "3467680"
  },
  {
    "text": "of it or like what we know so far from the structure robotics methods can come in and",
    "start": "3467680",
    "end": "3472799"
  },
  {
    "text": "kind of tease out what the just learning from scratch could not do right now at this point how do you",
    "start": "3472799",
    "end": "3479359"
  },
  {
    "text": "think this would proceed that's a very good question the",
    "start": "3479359",
    "end": "3484799"
  },
  {
    "text": "one thing is for sure roboticists need to be much more careful",
    "start": "3484799",
    "end": "3491280"
  },
  {
    "text": "and selective around the data because we don't have as much data",
    "start": "3491280",
    "end": "3498160"
  },
  {
    "text": "the and i the big question whether we'll ever have",
    "start": "3498160",
    "end": "3503200"
  },
  {
    "text": "second part is that the data we have is not perfect coming up with",
    "start": "3503200",
    "end": "3508640"
  },
  {
    "text": "good data is difficult and it's more difficult for robotics the because i think there are some",
    "start": "3508640",
    "end": "3514240"
  },
  {
    "text": "uncertainty there that are just interesting to the system and we cannot get",
    "start": "3514240",
    "end": "3519760"
  },
  {
    "text": "past that so i i think the opportunity is really for robotics then since we're kind of",
    "start": "3519760",
    "end": "3525119"
  },
  {
    "text": "working in this space of what what can we do best with the cheapest data that we can get by",
    "start": "3525119",
    "end": "3532799"
  },
  {
    "text": "to then use these methods and and apply them to to non-robotics domains",
    "start": "3532799",
    "end": "3540640"
  },
  {
    "text": "okay well in that case thank you again thank you for coming",
    "start": "3540640",
    "end": "3545920"
  },
  {
    "text": "[Applause]",
    "start": "3545920",
    "end": "3550389"
  }
]