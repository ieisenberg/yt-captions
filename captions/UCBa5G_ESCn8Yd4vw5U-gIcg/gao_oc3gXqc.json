[
  {
    "start": "0",
    "end": "92000"
  },
  {
    "text": "thank you thank you for the nice introduction um and thank everyone for coming um just yeah it's it's great to",
    "start": "11200",
    "end": "17840"
  },
  {
    "text": "see all the excitement around around this area so um unfortunately maybe um",
    "start": "17840",
    "end": "23359"
  },
  {
    "text": "compared to what people may be expecting um I'm not going to talk about why deep learning is great in a lot of details",
    "start": "23359",
    "end": "29240"
  },
  {
    "text": "I'm not going to spent a lot of time talking about applications um I'm hopefully I'm I'm hopeful that people",
    "start": "29240",
    "end": "35200"
  },
  {
    "text": "have maybe heard a little bit of enough of that um and I'd like to spend a lot of the time during this talk uh focusing",
    "start": "35200",
    "end": "41520"
  },
  {
    "text": "on a clear opportunity for improving the performance of deep learning um I'm also",
    "start": "41520",
    "end": "48280"
  },
  {
    "text": "hoping to convince you that performance actually matters for these workloads and computers aren't even close to being",
    "start": "48280",
    "end": "54480"
  },
  {
    "text": "fast enough um this is going to be a pretty technical talk um I'm going to",
    "start": "54480",
    "end": "60000"
  },
  {
    "text": "focus on a lot of open research areas here where I don't think anybody knows the answer to to these questions so I'm",
    "start": "60000",
    "end": "66000"
  },
  {
    "text": "going to pose some questions um as we go along here and I'm hoping that some of",
    "start": "66000",
    "end": "71280"
  },
  {
    "text": "the work that people are doing here um can help make progress on some of these problems um I'm going to try to convince",
    "start": "71280",
    "end": "78640"
  },
  {
    "text": "you that if we do make progress um the result will be worth",
    "start": "78640",
    "end": "84640"
  },
  {
    "text": "it okay so let's I'm going to start by briefly summarizing the success in De",
    "start": "84640",
    "end": "89920"
  },
  {
    "text": "learning um this year um and I'm going to focus on two results so the first",
    "start": "89920",
    "end": "95360"
  },
  {
    "start": "92000",
    "end": "266000"
  },
  {
    "text": "result um I guess it's on on your uh on your left is the um progress made in",
    "start": "95360",
    "end": "101479"
  },
  {
    "text": "image recognition so this is plotting um accuracy on uh the imag net Benchmark",
    "start": "101479",
    "end": "106680"
  },
  {
    "text": "challenge um over ye over years um so in 2011 we would think of as the uh first",
    "start": "106680",
    "end": "113159"
  },
  {
    "text": "time that deeping um algorithms were really applied to this problem and you can just see this very steady um",
    "start": "113159",
    "end": "119159"
  },
  {
    "text": "Improvement and performance over time um after that point um the other um Tech the other",
    "start": "119159",
    "end": "126000"
  },
  {
    "text": "technology that I'd like to focus on is uh speech recognition um this is work that we did at B research U this is",
    "start": "126000",
    "end": "132040"
  },
  {
    "text": "shown on um on on the other side of the figure um we're looking at a single deep",
    "start": "132040",
    "end": "137400"
  },
  {
    "text": "recurrent neural network um that's performing a task of speech recognition so it's converting from um audio data",
    "start": "137400",
    "end": "144879"
  },
  {
    "text": "into a textural representation um there's practic there are very few other components in this",
    "start": "144879",
    "end": "149920"
  },
  {
    "text": "system other than the Deep neural network um I'll talk about this in a little bit more detail but in two",
    "start": "149920",
    "end": "156080"
  },
  {
    "text": "languages with the same algorithm so in English and in Mandarin we're able to",
    "start": "156080",
    "end": "161519"
  },
  {
    "text": "approach human level performance where human level performance is measured by querying or tasking a number of humans",
    "start": "161519",
    "end": "170159"
  },
  {
    "text": "um to listen to audio and write down what they think the answer is um having them even debate you know with a small",
    "start": "170159",
    "end": "176480"
  },
  {
    "text": "group of people um to form a consensus um and comparing the result of that against the result of the system and we",
    "start": "176480",
    "end": "183360"
  },
  {
    "text": "actually match performance in in mandrin in many situations um and it's right so the idea",
    "start": "183360",
    "end": "191480"
  },
  {
    "text": "here is that um these are really examples um people may think that deep learning really applies well to vision",
    "start": "191480",
    "end": "198319"
  },
  {
    "text": "problems and it may be hard to think of problems outside of vision um as being",
    "start": "198319",
    "end": "203519"
  },
  {
    "text": "accessible to deep learning um I think that the Deep speech work is actually important um because it shows that deep",
    "start": "203519",
    "end": "210640"
  },
  {
    "text": "learning can be applied to other problem domains as well um so we applying it here to speech",
    "start": "210640",
    "end": "215840"
  },
  {
    "text": "recognition um there have been recent successes also in natural language processing um and reinforcement learning",
    "start": "215840",
    "end": "222959"
  },
  {
    "text": "um so the uh recent result that people may have may have heard of is the um uh",
    "start": "222959",
    "end": "228599"
  },
  {
    "text": "a single um or a deep Rec or a deep neural network uh playing the game of Go",
    "start": "228599",
    "end": "233760"
  },
  {
    "text": "and beating the world champion um this is work done by Google's Deep Mind Group um and so there're just examples of deep",
    "start": "233760",
    "end": "241480"
  },
  {
    "text": "learning um starting out in vision and being applied successfully to other domains as well um so we see this trend",
    "start": "241480",
    "end": "248560"
  },
  {
    "text": "continuing into the future um and let's so so there's one maybe main reason why",
    "start": "248560",
    "end": "256440"
  },
  {
    "text": "why this is being driven for there's one main cause um that's actually creating",
    "start": "256440",
    "end": "261720"
  },
  {
    "text": "this opportunity so I'm going to focus on that a little bit um I'm going to focus on it by examining the um deep",
    "start": "261720",
    "end": "267560"
  },
  {
    "start": "266000",
    "end": "417000"
  },
  {
    "text": "speech 2 system in a little bit more detail so um this figure really tries to",
    "start": "267560",
    "end": "273199"
  },
  {
    "text": "show uh how the algorithm Works in terms of the basic building blocks so on one side we have an input audio waveform in",
    "start": "273199",
    "end": "279880"
  },
  {
    "text": "the middle we have a deep convolutional recurrent neural network um we have a CTC cost function this is just a minor",
    "start": "279880",
    "end": "286440"
  },
  {
    "text": "detail just think of this as an error function that's grading the network in terms of how good the prediction is",
    "start": "286440",
    "end": "291479"
  },
  {
    "text": "compared to a reference um these are trained in supervised learning settings so we have a large data set a lot of",
    "start": "291479",
    "end": "298479"
  },
  {
    "text": "audio clips we have a lot of reference labels um and we're making small updates",
    "start": "298479",
    "end": "303840"
  },
  {
    "text": "to the network to try to match um match the reference labels so one thing that's",
    "start": "303840",
    "end": "309680"
  },
  {
    "text": "really important to realize here is if you look at the network architectures between um the algorithm that's used for",
    "start": "309680",
    "end": "316039"
  },
  {
    "text": "English recognition and the algorithm that's used for Mandarin recognition the only thing that changes is the output",
    "start": "316039",
    "end": "321720"
  },
  {
    "text": "character set so in the first case we're outputting English characters and the",
    "start": "321720",
    "end": "326800"
  },
  {
    "text": "second case we're outputting uh Mandarin characters irly there a much larger set of characters in Chinese it's more um",
    "start": "326800",
    "end": "334280"
  },
  {
    "text": "not more like 30 it's more like uh 5,000 10,000 um 50,000 if you if you want to",
    "start": "334280",
    "end": "340440"
  },
  {
    "text": "include absolutely everything but most people usually don't use the complete set um so with basically the same",
    "start": "340440",
    "end": "348199"
  },
  {
    "text": "algorithm it's a very Manor change to think about just changing the output set um we can approach human level accuracy",
    "start": "348199",
    "end": "354400"
  },
  {
    "text": "with the same algorithm but there's a wrinkle here um these networks are extreme extr expensive to train so one",
    "start": "354400",
    "end": "362759"
  },
  {
    "text": "rule of thumb that we have is basically right so one rule of thumb",
    "start": "362759",
    "end": "368520"
  },
  {
    "text": "that we have is we're going to train these systems on the biggest fastest computer that we have and we're going to",
    "start": "368520",
    "end": "375360"
  },
  {
    "text": "do it you know not for any any specific amount of time we're going to do it up to the point where we're you know we get",
    "start": "375360",
    "end": "381720"
  },
  {
    "text": "bored or we get impatient um and this is usually about two two weeks to about a",
    "start": "381720",
    "end": "387479"
  },
  {
    "text": "month and after that if your experiments starts taking maybe a year two years um",
    "start": "387479",
    "end": "392520"
  },
  {
    "text": "you start wondering you know is there something better that I can do with my time um so the 20x of flops of work kind",
    "start": "392520",
    "end": "398280"
  },
  {
    "text": "of translates into about um two weeks or about a month um on a very high-end uh",
    "start": "398280",
    "end": "404520"
  },
  {
    "text": "High throughput optimized system so let's look at that all right so that that kind of is getting towards",
    "start": "404520",
    "end": "411400"
  },
  {
    "text": "this motivation where we just need we need faster or we need more performance um I want to look at that just in one",
    "start": "411400",
    "end": "417560"
  },
  {
    "start": "417000",
    "end": "518000"
  },
  {
    "text": "more detail here this is the absolute most important result for deep learning",
    "start": "417560",
    "end": "423000"
  },
  {
    "text": "this is why deep Learning Works um so you know perhaps in the past you may",
    "start": "423000",
    "end": "428680"
  },
  {
    "text": "have seen a graph like this um that's actually kind of a cartoon graph that's just trying to show a relationship where",
    "start": "428680",
    "end": "435039"
  },
  {
    "text": "on one aess we have the data set size and on the other access we either have an accuracy or an error rate um on a on",
    "start": "435039",
    "end": "441879"
  },
  {
    "text": "a problem so this is looking um at the Deep speech uh model in this example",
    "start": "441879",
    "end": "447199"
  },
  {
    "text": "this is a real graph this is is real data so we see a power law relationship",
    "start": "447199",
    "end": "453199"
  },
  {
    "text": "between the data set size and the accuracy right this is the reason why we",
    "start": "453199",
    "end": "459680"
  },
  {
    "text": "care about compute large internet companies basically have infinite data 10,000 hours of data is super tiny it is",
    "start": "459680",
    "end": "467440"
  },
  {
    "text": "it is minuscule compared to the scale of data that's available on the Internet it's also you also May Wonder",
    "start": "467440",
    "end": "475000"
  },
  {
    "text": "well maybe that's true for unlabeled data but what about labeled data isn't it really expensive to get labeled data",
    "start": "475000",
    "end": "480840"
  },
  {
    "text": "this is just a money problem it's not actually very expensive in an absolute sense in compared to the scale of",
    "start": "480840",
    "end": "487879"
  },
  {
    "text": "computer that you have to run this on especially um to gather a data set of this size the reason why this graph",
    "start": "487879",
    "end": "494440"
  },
  {
    "text": "doesn't keep extending to the right is because we don't know how to map these algorithms onto even bigger",
    "start": "494440",
    "end": "502360"
  },
  {
    "text": "computers so this is the promise of deep learning does this hold up on other",
    "start": "502479",
    "end": "507680"
  },
  {
    "text": "application domains and does this graph continue off to the right with larger data",
    "start": "507680",
    "end": "514479"
  },
  {
    "text": "sets so we hope it does we also believe that there's an",
    "start": "514479",
    "end": "520560"
  },
  {
    "start": "518000",
    "end": "915000"
  },
  {
    "text": "opportunity to improve performance um deep learning has been one maybe one of",
    "start": "520560",
    "end": "526760"
  },
  {
    "text": "the first drivers major drivers of using high performance processors like gpus",
    "start": "526760",
    "end": "532399"
  },
  {
    "text": "high throughput processors like gpus um to accelerate model training or to accelerate this workload um",
    "start": "532399",
    "end": "540200"
  },
  {
    "text": "it took a lot of work actually to Port these algorithms onto gpus um gpus are notoriously hard to program there's a",
    "start": "540200",
    "end": "547079"
  },
  {
    "text": "lot of parallelism it's really there's a lot of complexity there's a lot of latency it's hard to get good throughput",
    "start": "547079",
    "end": "553320"
  },
  {
    "text": "out of them for many algorithms this is not an easy problem at all deep learning is relatively easy from that perspective",
    "start": "553320",
    "end": "559360"
  },
  {
    "text": "it's very regular it's very much aligned with traditional HPC workloads like big",
    "start": "559360",
    "end": "565279"
  },
  {
    "text": "dense linear algebra problems um and so this state-of-the-art right now is",
    "start": "565279",
    "end": "570959"
  },
  {
    "text": "running deep learning training algorithms on one or a few gpus but",
    "start": "570959",
    "end": "577079"
  },
  {
    "text": "there's a huge gap between one and a few gpus or a few gpus and the biggest",
    "start": "577079",
    "end": "582120"
  },
  {
    "text": "supercomputer that we could think of building um so this graph tries to show",
    "start": "582120",
    "end": "587160"
  },
  {
    "text": "um a titanex GPU which is a relatively high-end GPU from Nvidia um as a green",
    "start": "587160",
    "end": "592959"
  },
  {
    "text": "data point here and the gap between that uh single GPU and the largest publicly",
    "start": "592959",
    "end": "598320"
  },
  {
    "text": "available or public released um supercomputer in the world is about 10,000 times so there's a huge",
    "start": "598320",
    "end": "604880"
  },
  {
    "text": "opportunity to push the performance of um deep learning training forward um and",
    "start": "604880",
    "end": "611200"
  },
  {
    "text": "to take advantage of larger data sets hopefully unlocking even better accuracy even better or even lower error rates on",
    "start": "611200",
    "end": "618680"
  },
  {
    "text": "hard problems in artificial intelligence like speech recognition Vision decision making natural",
    "start": "618680",
    "end": "626680"
  },
  {
    "text": "language okay there's a go ahead 10,000 Gap because",
    "start": "626680",
    "end": "634560"
  },
  {
    "text": "it's not to build super computer with that many gpus no not at all this you",
    "start": "634560",
    "end": "641120"
  },
  {
    "text": "know the this the points up here have 10,000 20,000 gpus in them or other high",
    "start": "641120",
    "end": "648079"
  },
  {
    "text": "performance processors um I'd also like to point out that um the top 500",
    "start": "648079",
    "end": "653279"
  },
  {
    "text": "supercomputer list is the publicly available list that this data is based off of um there are definitely fter",
    "start": "653279",
    "end": "659560"
  },
  {
    "text": "computers in this in the world this is the public list",
    "start": "659560",
    "end": "664959"
  },
  {
    "text": "um there's at least I I would guess there's maybe one or two more ORS of magnitude in",
    "start": "664959",
    "end": "671319"
  },
  {
    "text": "here okay so I tried to um draw a little cartoon uh that describes what model",
    "start": "672760",
    "end": "679079"
  },
  {
    "text": "training looks like so you know we have this huge opportunity to improve performance um we have this workload we",
    "start": "679079",
    "end": "685880"
  },
  {
    "text": "really like to accelerate um the first step is probably developing a better understanding of that workload so this",
    "start": "685880",
    "end": "692120"
  },
  {
    "text": "is a cartoon representation that tries to uh capture the major components of deep learning model",
    "start": "692120",
    "end": "698160"
  },
  {
    "text": "training um so at the top we have a training data set the size of these data sets varies by application um usually",
    "start": "698160",
    "end": "705399"
  },
  {
    "text": "because we're compute limited this isn't as big of a data set as you might find in other domains this isn't 10 10",
    "start": "705399",
    "end": "711680"
  },
  {
    "text": "pedabytes of data this is maybe a couple terabytes of data um it depends on the",
    "start": "711680",
    "end": "717600"
  },
  {
    "text": "representation so images would be different than speech would be different than language um it kind of goes in the",
    "start": "717600",
    "end": "722880"
  },
  {
    "text": "order of language is the most compressed um speech is the next most compressed",
    "start": "722880",
    "end": "728160"
  },
  {
    "text": "and images are the least compressed so you just mentioned 10 pedabytes and you also mentioned the",
    "start": "728160",
    "end": "734240"
  },
  {
    "text": "number of exabytes as well too um you also mentioned you're at about 10,000",
    "start": "734240",
    "end": "739480"
  },
  {
    "text": "conversations I guess or oh okay so this is hours of audio okay",
    "start": "739480",
    "end": "745440"
  },
  {
    "text": "hours of audio so the question BRS up you know in order to solve some these problems how much storage do you need to",
    "start": "745440",
    "end": "752959"
  },
  {
    "text": "correspond to the number of pites does it correspond to and's other wall do it other otherwise know um so just in terms",
    "start": "752959",
    "end": "760720"
  },
  {
    "text": "of some concrete numbers I think 10,000 hours of audio here is in the range of 1",
    "start": "760720",
    "end": "765760"
  },
  {
    "text": "to 10 terabyte um again it depends on the application um usually so you see this",
    "start": "765760",
    "end": "771880"
  },
  {
    "text": "um Power law relationship right so you would need so the way to think about this perhaps in the context of speech is",
    "start": "771880",
    "end": "778480"
  },
  {
    "text": "if we want from 10 terabytes to 100 terabytes we'd get something like a 30",
    "start": "778480",
    "end": "783880"
  },
  {
    "text": "or 40% reduction in error rate so you do need a larger and larger data set to get",
    "start": "783880",
    "end": "791720"
  },
  {
    "text": "um to get better performance I just trying I'm just trying to so so you're you're telling me",
    "start": "791720",
    "end": "797760"
  },
  {
    "text": "that you have a sort of a slight imbalance between the need or Cycles to",
    "start": "797760",
    "end": "804839"
  },
  {
    "text": "the amount of storage you really need more Cycles to storage yeah yes especially compared to other",
    "start": "804839",
    "end": "810920"
  },
  {
    "text": "applications that's sort of B's definition of what constitutes a super computer where it turns out compute",
    "start": "810920",
    "end": "817199"
  },
  {
    "text": "problem an iio bound problem if you heard that sort of right",
    "start": "817199",
    "end": "823680"
  },
  {
    "text": "I so I I don't think we're there I don't think I don't think this workload is is actually becoming IO bound um I I'll",
    "start": "823680",
    "end": "830880"
  },
  {
    "text": "cover that a little bit in more detail there there's some subtle stage that you need you need to do including temporary",
    "start": "830880",
    "end": "836040"
  },
  {
    "text": "storage in order to in order to analyze the amount of audio you have there I mean you wouldn't you take 10,000 hours",
    "start": "836040",
    "end": "843720"
  },
  {
    "text": "multiply that obviously by some sampling rate to get some some amount of storage and you're talking about terabytes but",
    "start": "843720",
    "end": "850600"
  },
  {
    "text": "that's like six orders of magnitude to exabytes yeah right jumped over the p in",
    "start": "850600",
    "end": "857639"
  },
  {
    "text": "between oh sorry when I when I mentioned um EXA here I meant I meant flops not",
    "start": "857639",
    "end": "863800"
  },
  {
    "text": "bites yeah but my my my point is andal made this quote other law he he",
    "start": "863800",
    "end": "871199"
  },
  {
    "text": "basically said for every myth you tended to need a megabyte and you need a megabyte per second this little sort of",
    "start": "871199",
    "end": "876839"
  },
  {
    "text": "triangle it's in this paper I mean it's only three pages long deep learning guys blew that out of the water yeah that's",
    "start": "876839",
    "end": "883720"
  },
  {
    "text": "what over these stupid these things repeatedly band now you they invent they",
    "start": "883720",
    "end": "889279"
  },
  {
    "text": "invented an algorithm that blows that out of The Wire yeah it's it's so you",
    "start": "889279",
    "end": "894399"
  },
  {
    "text": "don't have a sense of how much storage you're actually going to need for this 20 ex ex blocks it's much less it's much",
    "start": "894399",
    "end": "900399"
  },
  {
    "text": "less it's a different kind of relationship instead of lightweight compute and very heavyweight storage you",
    "start": "900399",
    "end": "905680"
  },
  {
    "text": "have an inverse relationship you have very heavyweight compute very lightweight storage capacity",
    "start": "905680",
    "end": "911440"
  },
  {
    "text": "requirements okay right it's something that's fundamentally different about these",
    "start": "911440",
    "end": "917480"
  },
  {
    "start": "915000",
    "end": "1229000"
  },
  {
    "text": "workloads um the access pattern is also a little bit strange um so here I have",
    "start": "917480",
    "end": "923959"
  },
  {
    "text": "this as a a shuffle operation here so you have your 10 terabytes data said um",
    "start": "923959",
    "end": "929839"
  },
  {
    "text": "it's broken out into possibly millions of individual training samples they may be a very you know different sizes we'll",
    "start": "929839",
    "end": "936120"
  },
  {
    "text": "cover that a little bit later um and you're randomly accessing it so the common algorithm that you use to train",
    "start": "936120",
    "end": "942839"
  },
  {
    "text": "these models is called stochastic radiant descent the stochastic component means you want to Traverse the data set",
    "start": "942839",
    "end": "948319"
  },
  {
    "text": "in a random order um this is actually part of the uh",
    "start": "948319",
    "end": "953600"
  },
  {
    "text": "part of why the algorithm Works um of course there are a lot of details there but this is this is really what people",
    "start": "953600",
    "end": "960040"
  },
  {
    "text": "use in practice there are a lot of Alternatives but this is the thing that people commonly use in practice so you end up with this um random traversal of",
    "start": "960040",
    "end": "967000"
  },
  {
    "text": "the data set um and you end up Gathering um a number of training samples you",
    "start": "967000",
    "end": "973720"
  },
  {
    "text": "process these in parallel this is one of the main sources of parallelism that you get this is called Data parallelism in",
    "start": "973720",
    "end": "981440"
  },
  {
    "text": "machine learning terminology which is different than the common usage of data parilis in high performance Computing",
    "start": "981440",
    "end": "987279"
  },
  {
    "text": "terminology um data par ISM here means that you're running um the same neural",
    "start": "987279",
    "end": "992480"
  },
  {
    "text": "network model over different samples drawn out of your data",
    "start": "992480",
    "end": "997720"
  },
  {
    "text": "set okay so internally in the model because big multi-layer convolutional",
    "start": "997720",
    "end": "1004880"
  },
  {
    "text": "and recurrent neural networks um they involve a lot of dense linear algebra operations there's a lot of inherent",
    "start": "1004880",
    "end": "1010480"
  },
  {
    "text": "parallelism in those operations um so this is what we call model parallelism this parallelism that",
    "start": "1010480",
    "end": "1016839"
  },
  {
    "text": "is inherent in the evaluation of the model um in terms of the workload you",
    "start": "1016839",
    "end": "1023000"
  },
  {
    "text": "usually have a forward and backward um operation so the forward operation evaluates the network the backward",
    "start": "1023000",
    "end": "1029199"
  },
  {
    "text": "operation uh computes gradients with respect to the model parameters um there usually the way that",
    "start": "1029199",
    "end": "1036720"
  },
  {
    "text": "um these networks are designed these operations are about equal in terms of the amount of compute performed um",
    "start": "1036720",
    "end": "1042959"
  },
  {
    "text": "they're actually very efficient um if you think about a lot of other problems where you can't compute gradients",
    "start": "1042959",
    "end": "1049320"
  },
  {
    "text": "analytically it may be much more expensive um to to do this type of thing but with neuron networks just because of",
    "start": "1049320",
    "end": "1056360"
  },
  {
    "text": "the way that they're constructed this operation is extremely efficient okay so let's see so the way",
    "start": "1056360",
    "end": "1064280"
  },
  {
    "text": "that you train your model is um you Traverse your data set in this random order you gather manyi batches you",
    "start": "1064280",
    "end": "1070039"
  },
  {
    "text": "perform forward backward propagation you compute a gradient you take a step um",
    "start": "1070039",
    "end": "1075280"
  },
  {
    "text": "and then you repeat the process until you've cycled through your data set a number number of times it's really",
    "start": "1075280",
    "end": "1080799"
  },
  {
    "text": "important um to realize one aspect of this that's that's maybe that may not really be apparent at the first time you",
    "start": "1080799",
    "end": "1087240"
  },
  {
    "text": "look at it this algorithm is amazingly work efficient so what do I mean by work efficient I mean if you think about all",
    "start": "1087240",
    "end": "1094000"
  },
  {
    "text": "of the different algorithms you could use to modify the weights of a of a model like this and find a good",
    "start": "1094000",
    "end": "1100080"
  },
  {
    "text": "solution um and you think about the amount of computation implied by these different algorithms this algorithm",
    "start": "1100080",
    "end": "1106720"
  },
  {
    "text": "implies about um 20 accesses to every",
    "start": "1106720",
    "end": "1112159"
  },
  {
    "text": "point in your data set so what does that mean so it means that",
    "start": "1112159",
    "end": "1119159"
  },
  {
    "text": "you kind of look at every example in your data set maybe 10 or 20 times you",
    "start": "1119159",
    "end": "1124559"
  },
  {
    "text": "do the amount of work implied by the model parameters which is just you know linear in the number of parameters in",
    "start": "1124559",
    "end": "1129840"
  },
  {
    "text": "the model um and you can actually find a good solution to many hard problems like",
    "start": "1129840",
    "end": "1137159"
  },
  {
    "text": "Vision or speech Rec it's really amazing that this works as effectively as it",
    "start": "1137159",
    "end": "1143760"
  },
  {
    "text": "does I'm not going to say anything about why that is I have no idea why that",
    "start": "1143760",
    "end": "1148919"
  },
  {
    "text": "is um but it's a really amazing effect okay so one thing that people",
    "start": "1148919",
    "end": "1155200"
  },
  {
    "text": "also may not realize here from a computational perspective is that because we're using the stochastic gradient descent um we have a sequential",
    "start": "1155200",
    "end": "1162159"
  },
  {
    "text": "dependence between uh steps or between iterations um this is one of the main",
    "start": "1162159",
    "end": "1167720"
  },
  {
    "text": "difficulties in scaling in this type of approach on larger machines is that really you just run out of parallelism",
    "start": "1167720",
    "end": "1174159"
  },
  {
    "text": "there's a lot of parallelism in here but not enough for a giant",
    "start": "1174159",
    "end": "1179158"
  },
  {
    "text": "machine okay so given these characteristics um the next question is",
    "start": "1180120",
    "end": "1186600"
  },
  {
    "text": "clearly you know how do we exploit this parallelism how do we exploit the characteristics of this workload to get",
    "start": "1186600",
    "end": "1192679"
  },
  {
    "text": "better performance on real systems so um I want to start this conversation by",
    "start": "1192679",
    "end": "1199159"
  },
  {
    "text": "focusing on the pitfalls because I think that there are a few issues here that",
    "start": "1199159",
    "end": "1206159"
  },
  {
    "text": "aren't really very obvious that people commonly fall into and you know as a",
    "start": "1206159",
    "end": "1212039"
  },
  {
    "text": "researcher working in this area we have to be very mindful of these issues so that we can actually solve a real",
    "start": "1212039",
    "end": "1218240"
  },
  {
    "text": "problem um that's actually you know makes progress on accelerating training and not um just convince or fools us",
    "start": "1218240",
    "end": "1225799"
  },
  {
    "text": "into thinking that we are so this is the first one so beware of ignoring work",
    "start": "1225799",
    "end": "1231440"
  },
  {
    "start": "1229000",
    "end": "1457000"
  },
  {
    "text": "efficiency so what does this mean um many optimizations May trade um work",
    "start": "1231440",
    "end": "1237480"
  },
  {
    "text": "efficiency for throughput so what do I mean by this if we go back to the",
    "start": "1237480",
    "end": "1242640"
  },
  {
    "text": "previous um idea of the how the workload is actually structured um you can it's",
    "start": "1242640",
    "end": "1247960"
  },
  {
    "text": "very easy often times to um basically get a higher throughput on a machine so",
    "start": "1247960",
    "end": "1253320"
  },
  {
    "text": "get more parallelism run at a run at a higher throughput um you know process your data set faster but at the same",
    "start": "1253320",
    "end": "1260760"
  },
  {
    "text": "time increase the total amount of work that you have to do in order to reach the same result so this is just one",
    "start": "1260760",
    "end": "1267280"
  },
  {
    "text": "effect that I wanted to focus on that's a really common Pitfall here so this is um the effect of increasing the mini",
    "start": "1267280",
    "end": "1273640"
  },
  {
    "text": "batch size which is the number of samples that you use to estimate your gradient um as you increase this to a",
    "start": "1273640",
    "end": "1280760"
  },
  {
    "text": "point it doesn't really matter the idea is that um getting a better estimate of the gradient is actually useful to some",
    "start": "1280760",
    "end": "1286840"
  },
  {
    "text": "extent because it reduces noise but you don't want it to be too big um if you make it too big like let's",
    "start": "1286840",
    "end": "1293080"
  },
  {
    "text": "imagine we made it comically big like making it the entire data set size do you imagine that having a really",
    "start": "1293080",
    "end": "1300360"
  },
  {
    "text": "accurate estimate of a gradient um would allow you to make very fast progress in",
    "start": "1300360",
    "end": "1305760"
  },
  {
    "text": "a 100 million Dimension space do you really think that one step you know or",
    "start": "1305760",
    "end": "1311360"
  },
  {
    "text": "20 steps in 100 million Dimension space um can do as well as millions or",
    "start": "1311360",
    "end": "1316799"
  },
  {
    "text": "billions of smaller steps um so usually this is the effect that",
    "start": "1316799",
    "end": "1321880"
  },
  {
    "text": "you see you can increase the mini batch size to some point and you um maintain a",
    "start": "1321880",
    "end": "1327080"
  },
  {
    "text": "constant amount of computational work and after some point which is problem dependent it usually",
    "start": "1327080",
    "end": "1332240"
  },
  {
    "text": "increases um and you're trading basically better throughput more parilis for doing more work which actually isn't",
    "start": "1332240",
    "end": "1339520"
  },
  {
    "text": "getting you anywhere okay so um let's see my my",
    "start": "1339520",
    "end": "1349000"
  },
  {
    "text": "friend uh Brian katar was always um always interested or always you know",
    "start": "1349000",
    "end": "1354440"
  },
  {
    "text": "good at making this point there's no lower bound on how bad a baseline can be",
    "start": "1354440",
    "end": "1360640"
  },
  {
    "text": "okay so one thing that we're one common trick that we're doing here is we're looking at all the parallelism available",
    "start": "1360640",
    "end": "1365880"
  },
  {
    "text": "in our application and we're saying can we spread this out on more machines right and if we do this effectively we",
    "start": "1365880",
    "end": "1372000"
  },
  {
    "text": "can get better scaling or better throughput if we compare the performance on one machine against the performance",
    "start": "1372000",
    "end": "1377159"
  },
  {
    "text": "on 10 or 100 machines um if we're doing a good job we can usually see increased",
    "start": "1377159",
    "end": "1382720"
  },
  {
    "text": "performance with more with more computational resources but this is only true if your Baseline is good if your",
    "start": "1382720",
    "end": "1389360"
  },
  {
    "text": "performance on a single machine is any good at all and it's sometimes hard to",
    "start": "1389360",
    "end": "1395039"
  },
  {
    "text": "say what is good if you're starting from an implementation that you don't understand very well or you're looking",
    "start": "1395039",
    "end": "1400919"
  },
  {
    "text": "at a result in literature that um is reporting something like uh the time you",
    "start": "1400919",
    "end": "1407679"
  },
  {
    "text": "know in an absolute sense um it's very hard to say how efficient that implementation actually is so there's",
    "start": "1407679",
    "end": "1414080"
  },
  {
    "text": "this very nice framework um at uh commonly used in Nvidia that's called this uh concept of a speed of light of",
    "start": "1414080",
    "end": "1421320"
  },
  {
    "text": "an algorithm or speed of light of um of hardware and the idea here is that",
    "start": "1421320",
    "end": "1427640"
  },
  {
    "text": "there's a fundamental limit based on the hardware platform that you have based on",
    "start": "1427640",
    "end": "1432799"
  },
  {
    "text": "the physical assumptions um that were used to design that Hardware platform of what is the best possible performance",
    "start": "1432799",
    "end": "1439000"
  },
  {
    "text": "that you can do and so the point is don't measure yourself against an arbitrary implementation that you have",
    "start": "1439000",
    "end": "1444840"
  },
  {
    "text": "no idea how efficient it is measure measure yourself against the best that you can do so the best that you can do",
    "start": "1444840",
    "end": "1450960"
  },
  {
    "text": "is a theoretical maximum performance um on the processor that you're running on",
    "start": "1450960",
    "end": "1456080"
  },
  {
    "text": "so you know this cartoon here we showing you know you can slow down an application arbitrarily by adding in you",
    "start": "1456080",
    "end": "1463720"
  },
  {
    "start": "1457000",
    "end": "1508000"
  },
  {
    "text": "know hundreds of layers of abstraction and it makes it really easy to show good scalability but this isn't really very",
    "start": "1463720",
    "end": "1469520"
  },
  {
    "text": "useful to anybody so try to measure yourself against the maximum possible performance",
    "start": "1469520",
    "end": "1477039"
  },
  {
    "text": "possible um and not any Baseline just ignore the baselines don't even don't even compare against them against",
    "start": "1477039",
    "end": "1485398"
  },
  {
    "text": "them okay so I wanted to spend some time um going through a bunch of Technologies",
    "start": "1485679",
    "end": "1493360"
  },
  {
    "text": "um that have been applied to improving performance on this time type of workload that already work that are used",
    "start": "1493360",
    "end": "1500279"
  },
  {
    "text": "in practice I just want to spend some time trying to establish what the state-of-the-art actually",
    "start": "1500279",
    "end": "1506840"
  },
  {
    "text": "is okay so one clear trend is dense uh compute Hardware so there's a clear",
    "start": "1507279",
    "end": "1515080"
  },
  {
    "start": "1508000",
    "end": "1620000"
  },
  {
    "text": "Trend towards building the highest throughput processors possible which are",
    "start": "1515080",
    "end": "1520679"
  },
  {
    "text": "currently um currently gpus although I hate using the word GPU because they're really just parallel processors there",
    "start": "1520679",
    "end": "1527000"
  },
  {
    "text": "are a lot of other parallel process processors that are very efficient that aren't called gpus I'm just going to I'm",
    "start": "1527000",
    "end": "1532120"
  },
  {
    "text": "so I'm going to stick with gpus right now because maybe it's a common terminology but really just densest",
    "start": "1532120",
    "end": "1537840"
  },
  {
    "text": "highest throughput parallel processor that you can think of um so the concept of density is extending to the node",
    "start": "1537840",
    "end": "1544600"
  },
  {
    "text": "architecture so this is showing two node configurations that we use at Buu um one is a dual CPU configuration um we use a",
    "start": "1544600",
    "end": "1551600"
  },
  {
    "text": "four-way PCI switch each switch hosts four gpus um so these are all in one for you",
    "start": "1551600",
    "end": "1558399"
  },
  {
    "text": "box this puts it at about a 3 3 kilowatt box um we also have a we also have an",
    "start": "1558399",
    "end": "1566039"
  },
  {
    "text": "even denser machine um where this really just pushes the limits of of physical integration we have 16 gpus in one 4u",
    "start": "1566039",
    "end": "1573640"
  },
  {
    "text": "box this is more like a five six kilowatt box um there's a clear Trend in",
    "start": "1573640",
    "end": "1579399"
  },
  {
    "text": "here which is kind of interesting which is we end up just spending all of our resources all of our money on gpus and",
    "start": "1579399",
    "end": "1586600"
  },
  {
    "text": "we're basically just using the CPU here to run a driver and to access the data",
    "start": "1586600",
    "end": "1592120"
  },
  {
    "text": "set and orchestrate the network communications um but the reason why",
    "start": "1592120",
    "end": "1598159"
  },
  {
    "text": "we're doing all of this is there's a realization that data movement is expensive the further you have to move",
    "start": "1598159",
    "end": "1604240"
  },
  {
    "text": "data physically the more energy you have to pay to do that so we're trying to put",
    "start": "1604240",
    "end": "1609640"
  },
  {
    "text": "um the densest possible processors as close together as",
    "start": "1609640",
    "end": "1614799"
  },
  {
    "text": "possible so this is a clear trend",
    "start": "1614799",
    "end": "1619639"
  },
  {
    "start": "1620000",
    "end": "2137000"
  },
  {
    "text": "all right another um clear trend is to focus on Fast tightly coupled um network",
    "start": "1620760",
    "end": "1627039"
  },
  {
    "text": "interfaces fast interconnects so this is something that's already a clear Trend in high performance Computing if you",
    "start": "1627039",
    "end": "1633640"
  },
  {
    "text": "look at how U modern internet data centers are configured this is not how they're configured at all um there are",
    "start": "1633640",
    "end": "1640159"
  },
  {
    "text": "some you know cost issues associated with this like how much money do you really want to spend on your network um",
    "start": "1640159",
    "end": "1646360"
  },
  {
    "text": "there's also technology and and software stack um issues here um but for deep",
    "start": "1646360",
    "end": "1653640"
  },
  {
    "text": "learning because we actually there's actually this huge benefit of having dense compute um in order to ex to",
    "start": "1653640",
    "end": "1660240"
  },
  {
    "text": "expand beyond what you can fit into a single box you have to worry about the interconnect um and so we have to we",
    "start": "1660240",
    "end": "1666200"
  },
  {
    "text": "have to spend a lot of resources using using fast interconnects um so we would use",
    "start": "1666200",
    "end": "1672279"
  },
  {
    "text": "something like um 50 to 100 gbit per second um infin band or ethernet links",
    "start": "1672279",
    "end": "1677519"
  },
  {
    "text": "between box is um we'd also use MPI so why do we use MPI um rather than you",
    "start": "1677519",
    "end": "1685240"
  },
  {
    "text": "know some more flexible uh communication protocol um MPI is just really well",
    "start": "1685240",
    "end": "1691159"
  },
  {
    "text": "optimized on these types of Hardware platforms it's actually hard to get the maximum bandwidth possible out of an",
    "start": "1691159",
    "end": "1697840"
  },
  {
    "text": "interconnect with just off-the-shelf software if you try just running you know even something like IP over",
    "start": "1697840",
    "end": "1704519"
  },
  {
    "text": "infiniband you typically see you know 2X or more slow down um layering on more",
    "start": "1704519",
    "end": "1710399"
  },
  {
    "text": "levels of abstraction typically reduces performance at these speeds as well um we we use MPI really not um not for any",
    "start": "1710399",
    "end": "1719679"
  },
  {
    "text": "other reason than the software is well optimized for high performance on these interconnects um also the",
    "start": "1719679",
    "end": "1726200"
  },
  {
    "text": "characteristics of these workloads are a little bit different than what you would normally use um MPI for so MPI um is",
    "start": "1726200",
    "end": "1733600"
  },
  {
    "text": "very highly optimized around um small messages so a lot there it's really",
    "start": "1733600",
    "end": "1738679"
  },
  {
    "text": "sensitive to latency there's this concept in high performance computer Computing called strong scaling where",
    "start": "1738679",
    "end": "1744159"
  },
  {
    "text": "you really care about the latency units of Works get small um and so they finish",
    "start": "1744159",
    "end": "1749960"
  },
  {
    "text": "quickly and so you if you take too long in terms of latency in your interconnect",
    "start": "1749960",
    "end": "1755080"
  },
  {
    "text": "um you can end up being limited by that and so there's a lot of optimizations around latency reduction those are",
    "start": "1755080",
    "end": "1761240"
  },
  {
    "text": "actually not the right thing to do um for deep learning training we have giant models a common operation that we do",
    "start": "1761240",
    "end": "1767840"
  },
  {
    "text": "with giant models is that we synchronize them over a lot of different machines this is just a lot of parallel bandwidth",
    "start": "1767840",
    "end": "1774559"
  },
  {
    "text": "intensive data transfer going on here um and so the default algorithms that um",
    "start": "1774559",
    "end": "1780720"
  },
  {
    "text": "MPI uses for uh exchanging data in an operation called an all reduce um are",
    "start": "1780720",
    "end": "1785840"
  },
  {
    "text": "tuned for latency and not bandwidth so there are algorithms like ring algorithms um that are more uh work",
    "start": "1785840",
    "end": "1793320"
  },
  {
    "text": "efficient in terms of the total amount of data that they send over the network um and so by moving to these algorithms",
    "start": "1793320",
    "end": "1799000"
  },
  {
    "text": "we can see very significant um speed UPS so fast inter connects uh care a lot",
    "start": "1799000",
    "end": "1807159"
  },
  {
    "text": "about uh the total amount of data moved over the network um this is a clear Trend as well go ahead so Toler",
    "start": "1807159",
    "end": "1816600"
  },
  {
    "text": "perodic yes we per yeah fall tolerance here um it's a really good question I'll",
    "start": "1816600",
    "end": "1823039"
  },
  {
    "text": "get i'll get into it a little bit in more more detail in a minute but um yeah the state-of-the-art is",
    "start": "1823039",
    "end": "1829240"
  },
  {
    "text": "checkpointing um models run for weeks you know there so so the overhead of",
    "start": "1829240",
    "end": "1834799"
  },
  {
    "text": "checkpointing somewhat infrequently isn't too bad um yeah I I'll talk about this a little",
    "start": "1834799",
    "end": "1841720"
  },
  {
    "text": "bit there's a really weird effect that comes in here that's based on um the robustness of these models to um to",
    "start": "1841720",
    "end": "1849159"
  },
  {
    "text": "small errors these models are actually very robust to small errors so we'll get",
    "start": "1849159",
    "end": "1854480"
  },
  {
    "text": "into that in a minute go ahead for the",
    "start": "1854480",
    "end": "1861159"
  },
  {
    "text": "themselves trees the prev yes have you",
    "start": "1861159",
    "end": "1866559"
  },
  {
    "text": "considered some sort of direct no to node link with RDM um let's see so in terms of",
    "start": "1866559",
    "end": "1874080"
  },
  {
    "text": "off-the-shelf Hardware that you can buy easily um it's hard to get direct direct",
    "start": "1874080",
    "end": "1879200"
  },
  {
    "text": "links you can you can do RDMA typically um over PCI switches um it turns out",
    "start": "1879200",
    "end": "1885440"
  },
  {
    "text": "that we're actually you know with this kind of effect we're in a more um bandwith sensitive regime than a latency",
    "start": "1885440",
    "end": "1890480"
  },
  {
    "text": "sensitive regime RDMA really helps you a lot more in a latency sensitive regime um so it hasn't become uh too much of an",
    "start": "1890480",
    "end": "1897760"
  },
  {
    "text": "issue yet I feel like going forward there are huge advantages in terms of um being able to build higher band with",
    "start": "1897760",
    "end": "1904080"
  },
  {
    "text": "interconnects and reducing the energy consumption in interconnects uh from tighter integration and more direct",
    "start": "1904080",
    "end": "1909320"
  },
  {
    "text": "links um really I view this as being a problem um just with the hardware",
    "start": "1909320",
    "end": "1915399"
  },
  {
    "text": "technology because this hasn't people haven't been aware of the need for faster and lower um or higher band with",
    "start": "1915399",
    "end": "1921320"
  },
  {
    "text": "interconnects people haven't built these these links yet right now these sches if",
    "start": "1921320",
    "end": "1927159"
  },
  {
    "text": "the gpus want to shuffle data across they have to go back up the PCI tree and come back down up to the nearest level",
    "start": "1927159",
    "end": "1934279"
  },
  {
    "text": "essentially yes that's true um so different yeah I I would say the most",
    "start": "1934279",
    "end": "1939480"
  },
  {
    "text": "common uh data access pattern right now is this ring algorithm so you end up having uh gpus communicating with their",
    "start": "1939480",
    "end": "1945960"
  },
  {
    "text": "nearest neighbor sending like very large data transfers very bandwidth sensitive",
    "start": "1945960",
    "end": "1951399"
  },
  {
    "text": "data transfers over individual links um so you can orchestrate it over a tree like this clearly if you have a tree",
    "start": "1951399",
    "end": "1958480"
  },
  {
    "text": "we're not you know this isn't um the most efficient use of all of our routing resources just go near Swit come back",
    "start": "1958480",
    "end": "1968840"
  },
  {
    "text": "that's that's what you ideally want to do it's one thing that's very important is keeping in mind uh the placement of",
    "start": "1968840",
    "end": "1975159"
  },
  {
    "text": "individual gpus so to worry about the physic phical placement not just the logical placement um so if you're doing",
    "start": "1975159",
    "end": "1981880"
  },
  {
    "text": "one of these ring algorithms you do not want to be for example um hopping all the way across uh your system uh when",
    "start": "1981880",
    "end": "1989799"
  },
  {
    "text": "individ when pairs of nodes are communicating so it's very important to have um some information about the",
    "start": "1989799",
    "end": "1995600"
  },
  {
    "text": "physical topology of your network and to really understand for the system that you're running on what is the physical Network",
    "start": "1995600",
    "end": "2001919"
  },
  {
    "text": "topology does that make sense okay",
    "start": "2001919",
    "end": "2008919"
  },
  {
    "text": "okay okay so right so one solution to um basically",
    "start": "2008919",
    "end": "2016960"
  },
  {
    "text": "reducing the amount of or improving the amount of time it takes to to do these",
    "start": "2016960",
    "end": "2022120"
  },
  {
    "text": "data exchanges between models mainly for data parallelism um is just building faster interconnects using um reduction",
    "start": "2022120",
    "end": "2029799"
  },
  {
    "text": "algorithms that are highly optimized um there are other approaches also that work um at the algorithmic level um that",
    "start": "2029799",
    "end": "2036880"
  },
  {
    "text": "try and reduce total amount of communication um so for systems with weak interconnects or less well",
    "start": "2036880",
    "end": "2042559"
  },
  {
    "text": "optimized interconnects um it's more common for this to be a problem and for these techniques to apply so there's",
    "start": "2042559",
    "end": "2049118"
  },
  {
    "text": "three main um sets of techniques that are used here uh one is called asynchronous uh stochastic gradient",
    "start": "2049119",
    "end": "2054760"
  },
  {
    "text": "descent one is called Butterfly mixing or butterfly SGD um and one is called uh",
    "start": "2054760",
    "end": "2060358"
  },
  {
    "text": "sparse or quantized SGD so I can kind of go through these um they all kind of",
    "start": "2060359",
    "end": "2066040"
  },
  {
    "text": "work uh to reduce the bandwidth requirements they all they all introduce",
    "start": "2066040",
    "end": "2071760"
  },
  {
    "text": "some kind of concept of a delay or some kind of concept of using stale",
    "start": "2071760",
    "end": "2076878"
  },
  {
    "text": "information um and so because of this from an optimization perspective they're always less efficient um The Hope is is",
    "start": "2076879",
    "end": "2084960"
  },
  {
    "text": "that the trade-off is worth it that you can you know be a little bit less efficient your stale information is not",
    "start": "2084960",
    "end": "2091800"
  },
  {
    "text": "hurting you too much um and the benefit that you get in terms of latency",
    "start": "2091800",
    "end": "2096878"
  },
  {
    "text": "tolerance or with reduction um gives you a large enough speed up on your whole system that you can overcome that that",
    "start": "2096879",
    "end": "2104400"
  },
  {
    "text": "uh reduced efficiency so asynchronous SGD is the most common implementation here I want",
    "start": "2104400",
    "end": "2110160"
  },
  {
    "text": "to make one point about this that I don't think is really welln um asynchronous SGD is popular probably",
    "start": "2110160",
    "end": "2116680"
  },
  {
    "text": "because it's very easy to implement um it's it's easy to implement because you can just imagine taking a bunch of nodes",
    "start": "2116680",
    "end": "2122760"
  },
  {
    "text": "they're all trading models simultaneously um you can take synchronous communication and just say",
    "start": "2122760",
    "end": "2128640"
  },
  {
    "text": "well send it asynchronously so the receivers will receive it they'll apply the updates whenever they get it um and",
    "start": "2128640",
    "end": "2135079"
  },
  {
    "text": "that gives you a bunch of really nice effects actually so you do um",
    "start": "2135079",
    "end": "2140359"
  },
  {
    "start": "2137000",
    "end": "2329000"
  },
  {
    "text": "synchronization less often so this reduces the total amount of bandwidth you need um it also gives you uh latency",
    "start": "2140359",
    "end": "2147320"
  },
  {
    "text": "tolerance so your updates can be delayed you might um see an update multiple iterations",
    "start": "2147320",
    "end": "2152920"
  },
  {
    "text": "later um this is this gives you more latency tolerance um you also Al get a",
    "start": "2152920",
    "end": "2158119"
  },
  {
    "text": "nice effect of load balancing so if everybody is sending data to one",
    "start": "2158119",
    "end": "2163160"
  },
  {
    "text": "bottleneck um the the network protocols will naturally apply back pressure the",
    "start": "2163160",
    "end": "2168359"
  },
  {
    "text": "rate will be slowed down um and if one and let's say for example if one model",
    "start": "2168359",
    "end": "2174480"
  },
  {
    "text": "is sending a lot more data uh to a synchronization point or to a centralized server um and another one is",
    "start": "2174480",
    "end": "2180160"
  },
  {
    "text": "sending much less you get some kind of Natural Balance of the bandwidth um the one that's sending a lot of data it may",
    "start": "2180160",
    "end": "2187240"
  },
  {
    "text": "just take longer for that data to actually be sent so then it might send it at a less frequency and so you get a",
    "start": "2187240",
    "end": "2192440"
  },
  {
    "text": "natural um load balancing effect um the biggest problem with all",
    "start": "2192440",
    "end": "2198480"
  },
  {
    "text": "of this is that you get non-determinism so the way I like to describe this is that imagine that you have a knob on",
    "start": "2198480",
    "end": "2205880"
  },
  {
    "text": "your network and the knob controls the clock speed of your network of all of your network links so if I you know as a",
    "start": "2205880",
    "end": "2213359"
  },
  {
    "text": "a malicious person can go over and just turn that knob while your algorith is running and I increase the clock speed",
    "start": "2213359",
    "end": "2220839"
  },
  {
    "text": "maybe your model is working just fine but now I can go and turn it the other way and I can dial down the clock speed",
    "start": "2220839",
    "end": "2227040"
  },
  {
    "text": "and you as a user is like looking at your algorithm um watching your training curve go down hopefully um converging to",
    "start": "2227040",
    "end": "2233960"
  },
  {
    "text": "a better solution as I turn down the clock speed low enough um my network can",
    "start": "2233960",
    "end": "2239119"
  },
  {
    "text": "now stop converging I can now get errors in my network um and so with real",
    "start": "2239119",
    "end": "2245599"
  },
  {
    "text": "systems where um interconnect performance is very non-deterministic",
    "start": "2245599",
    "end": "2250920"
  },
  {
    "text": "you have Dynam Dynamic clock frequency scaling going on um you have congestion you might have other applications",
    "start": "2250920",
    "end": "2256920"
  },
  {
    "text": "running on your network um this effect here is really dangerous for",
    "start": "2256920",
    "end": "2262160"
  },
  {
    "text": "reproducibility and for debugging so imagine you have a model and you ran for",
    "start": "2262160",
    "end": "2267200"
  },
  {
    "text": "a month and it achieved a wonderful result and now you want to reproduce it",
    "start": "2267200",
    "end": "2272920"
  },
  {
    "text": "and you want to tweak it in some way or you want to design an experiment where you say what happens if I add 10% more",
    "start": "2272920",
    "end": "2278960"
  },
  {
    "text": "data and I run the same experiment because of the dynamic conditions in my network I might end up with a worse",
    "start": "2278960",
    "end": "2285240"
  },
  {
    "text": "result this might lead me to a false conclusion similarly if my network",
    "start": "2285240",
    "end": "2290319"
  },
  {
    "text": "crashes for a real software bug or a numerical Precision error um and I try and restart it to try",
    "start": "2290319",
    "end": "2299040"
  },
  {
    "text": "and reproduce this error I may never see it again because my training is now non-deterministic so my main point here",
    "start": "2299040",
    "end": "2305680"
  },
  {
    "text": "is just that you can have everything good here with removing all the bad things here you can just add delays and",
    "start": "2305680",
    "end": "2314200"
  },
  {
    "text": "um synchronize less frequently and do explicit load balancing and you can get all of the same effects with a purely",
    "start": "2314200",
    "end": "2320880"
  },
  {
    "text": "deterministic algorithm there's no reason not to do this other than just writing the",
    "start": "2320880",
    "end": "2326960"
  },
  {
    "text": "code okay so let's see so I'm just going to",
    "start": "2327920",
    "end": "2334000"
  },
  {
    "start": "2329000",
    "end": "2429000"
  },
  {
    "text": "continue on through um a bunch of these different uh optimization and different uh Technologies used in practice so I",
    "start": "2334000",
    "end": "2340960"
  },
  {
    "text": "want to mention one thing about optimized kernels um part of the really",
    "start": "2340960",
    "end": "2346040"
  },
  {
    "text": "nice um effective gpus or the really nice programming environment around gpus",
    "start": "2346040",
    "end": "2351200"
  },
  {
    "text": "is that they're actually are programmable you can actually get in and edit the source code of individual",
    "start": "2351200",
    "end": "2356560"
  },
  {
    "text": "algorithmic building blocks so I can go rewrite my Matrix multiply code or my",
    "start": "2356560",
    "end": "2361640"
  },
  {
    "text": "convolution code and I can uh swap out better algorithms if I find them so um",
    "start": "2361640",
    "end": "2368040"
  },
  {
    "text": "using uh fft methods using uh winegrad methods for convolutions uh result in",
    "start": "2368040",
    "end": "2373319"
  },
  {
    "text": "better work efficiency for convolutions um so there's been uh a significant",
    "start": "2373319",
    "end": "2378720"
  },
  {
    "text": "amount of work that's gone into this area that's actually U made the building blocks um that the higher level networks",
    "start": "2378720",
    "end": "2384319"
  },
  {
    "text": "will run on more efficient um so persistent rnns are another example of this this is some",
    "start": "2384319",
    "end": "2389880"
  },
  {
    "text": "work that we did at at Buu um The Main Idea here is just to realize that um",
    "start": "2389880",
    "end": "2395839"
  },
  {
    "text": "weights and recurrent neural networks are constant over multiple time steps um a straightforward way of implementing",
    "start": "2395839",
    "end": "2402160"
  },
  {
    "text": "this and the most common way of implementing this is just using Matrix multiply operations um but those operations don't",
    "start": "2402160",
    "end": "2408800"
  },
  {
    "text": "exploit the fact that weights are constant so if you actually realize that you can write a custom Matrix multiply",
    "start": "2408800",
    "end": "2415119"
  },
  {
    "text": "implementation that caches individual tiles throughout the memory hierarchy of a GPU and avoid the main cost which is",
    "start": "2415119",
    "end": "2423720"
  },
  {
    "text": "loading the weights over and over and over again",
    "start": "2423720",
    "end": "2428400"
  },
  {
    "start": "2429000",
    "end": "2766000"
  },
  {
    "text": "okay um my figure got really shrunk oh well um okay so I I want to so there",
    "start": "2429400",
    "end": "2435800"
  },
  {
    "text": "were some questions before about the um I requirements of these systems um so I wanted to take just a moment to talk",
    "start": "2435800",
    "end": "2441920"
  },
  {
    "text": "about them uh kind of more explicitly so this is the requirement for the Deep speech system um and this is kind of",
    "start": "2441920",
    "end": "2447960"
  },
  {
    "text": "maybe a simplistic way of characterizing the IR requirement um we basically need for every 25 terlop sustained which is",
    "start": "2447960",
    "end": "2456200"
  },
  {
    "text": "maybe what we get on eight gpus we need about 100 megabytes per second of random",
    "start": "2456200",
    "end": "2462040"
  },
  {
    "text": "um 16 kiloby to 1 Megabyte accesses there's a distribution it's kind of kind of normal",
    "start": "2462040",
    "end": "2469319"
  },
  {
    "text": "um over uh between a minimum value of 16 kilobytes and a maximum value of about 1",
    "start": "2469319",
    "end": "2474920"
  },
  {
    "text": "Megabyte um so this is tough for a spinning disc to keep up",
    "start": "2474920",
    "end": "2480079"
  },
  {
    "text": "with um and and over a very large machine so",
    "start": "2480079",
    "end": "2485960"
  },
  {
    "text": "over something like 30 or 40 compute nodes each with 8 gpus or 16 gpus in them it actually adds up to be a very",
    "start": "2485960",
    "end": "2492800"
  },
  {
    "text": "significant amount of bandwidth and it's random access bandwidth the data set size is also not tiny it's",
    "start": "2492800",
    "end": "2500079"
  },
  {
    "text": "terabytes so we essentially built a uh 40 terab IO node um that connects over",
    "start": "2500079",
    "end": "2506319"
  },
  {
    "text": "infiniband it delivers about 6 gigabyt per second of 64k random",
    "start": "2506319",
    "end": "2511359"
  },
  {
    "text": "reads um as far as I know there aren't off-the-shelf uh file systems or or IO",
    "start": "2511359",
    "end": "2517960"
  },
  {
    "text": "systems that can sustain these rates um if we were to add more nodes here or if",
    "start": "2517960",
    "end": "2523839"
  },
  {
    "text": "we were to significantly bump up the performance of individual nodes this would become an even bigger problem we'd",
    "start": "2523839",
    "end": "2530119"
  },
  {
    "text": "need even bigger uh total total memory or total um IO capacity and even more",
    "start": "2530119",
    "end": "2536800"
  },
  {
    "text": "random access bandwidth so it's not killing us yet but it's right on the",
    "start": "2536800",
    "end": "2542760"
  },
  {
    "text": "edge just so I understand the problem there this is when You're Building these",
    "start": "2542760",
    "end": "2547960"
  },
  {
    "text": "mini batches right that's the primary data and so if you just rated the",
    "start": "2547960",
    "end": "2553559"
  },
  {
    "text": "out of it if you just had all your yourself your data across a large number of diss then you could pipeline that and",
    "start": "2553559",
    "end": "2562200"
  },
  {
    "text": "avoid the problem and scale up as long as you keep the total amount of data per",
    "start": "2562200",
    "end": "2568559"
  },
  {
    "text": "disc constant yes you just keep scaling it and that would always work yes absolutely I think that's I think that's",
    "start": "2568559",
    "end": "2574520"
  },
  {
    "text": "definitely right I think that's not what you're doing here I just didn't follow it's not what we're doing here I think",
    "start": "2574520",
    "end": "2579839"
  },
  {
    "text": "for a few reasons um part of it might have just been Legacy like maybe the right solution here is just to buy a big",
    "start": "2579839",
    "end": "2585200"
  },
  {
    "text": "luster cluster or a big gpfs cluster um usually when we've priced these things",
    "start": "2585200",
    "end": "2590440"
  },
  {
    "text": "out if we actually buy the amount of bandwidth and the amount of um IO performance that we need it ends up",
    "start": "2590440",
    "end": "2596200"
  },
  {
    "text": "being multiple factors on top of the com the cost of the compute nodes so we might spend maybe a million dollar or",
    "start": "2596200",
    "end": "2602720"
  },
  {
    "text": "something for these compute nodes it might cost us 10 million or $20 million to get a file system system maybe that's",
    "start": "2602720",
    "end": "2608680"
  },
  {
    "text": "a solvable problem I'm not I'm not sure want store the the training data",
    "start": "2608680",
    "end": "2615160"
  },
  {
    "text": "locally on the nodes on the computer so that's an option I think it becomes it's more viable with small data sets right",
    "start": "2615160",
    "end": "2622559"
  },
  {
    "text": "so if you have a data set that's much smaller than you know 40 terabytes then maybe you're fine if you're around one",
    "start": "2622559",
    "end": "2628839"
  },
  {
    "text": "terabyte or 5 terabyt something like that um but you have this effect where as you get more compute you really want",
    "start": "2628839",
    "end": "2634880"
  },
  {
    "text": "to have to access more data um why not just data on and then arrange",
    "start": "2634880",
    "end": "2642800"
  },
  {
    "text": "the mini batches such that uh the compute nodes are only pulling samples from data that they have",
    "start": "2642800",
    "end": "2649200"
  },
  {
    "text": "loc right so I think they're just software problems with doing that because imagine um let's see imagine",
    "start": "2649200",
    "end": "2658079"
  },
  {
    "text": "that you're not always using the whole cluster like imagine that you have jobs of different sizes and shapes running on",
    "start": "2658079",
    "end": "2664160"
  },
  {
    "text": "this cluster um it's very hard to think about a replication pattern that satisfies all of those constraints other",
    "start": "2664160",
    "end": "2670359"
  },
  {
    "text": "than replicating everything everywhere does that make sense",
    "start": "2670359",
    "end": "2676160"
  },
  {
    "text": "yeah go ahead think you might have answered this um so sounds like you're",
    "start": "2676720",
    "end": "2683200"
  },
  {
    "text": "looking at your data um source as essentially a",
    "start": "2683200",
    "end": "2689680"
  },
  {
    "text": "FIS oh dat auster you",
    "start": "2689680",
    "end": "2695520"
  },
  {
    "text": "saids um you know you can build very fast block IO systems which sort of do",
    "start": "2695520",
    "end": "2703480"
  },
  {
    "text": "more lower level Io if you could store your data in a repository that wasn't a p system have",
    "start": "2703480",
    "end": "2710680"
  },
  {
    "text": "you looked at that right okay so um so the question I guess is why do we store",
    "start": "2710680",
    "end": "2715960"
  },
  {
    "text": "with a file system maybe with a posix interface rather than some more specialized storage like a key Value Store Block store um uh We've definitely",
    "start": "2715960",
    "end": "2724559"
  },
  {
    "text": "looked at that I think the the explanation here was really more of um really it was just an example I I the",
    "start": "2724559",
    "end": "2732400"
  },
  {
    "text": "system that we're actually using is a custom system that doesn't uh that doesn't present a posix interface we",
    "start": "2732400",
    "end": "2738559"
  },
  {
    "text": "actually do have a block based interface",
    "start": "2738559",
    "end": "2746119"
  },
  {
    "text": "okay all right so that's kind of what people are doing today um I wanted to",
    "start": "2746119",
    "end": "2751720"
  },
  {
    "text": "spend a few minutes talking about things that are just to me at least and probably to to other people working in",
    "start": "2751720",
    "end": "2757559"
  },
  {
    "text": "this area these are clear improvements that will obviously work and nobody has implemented them yet so these are the",
    "start": "2757559",
    "end": "2764160"
  },
  {
    "text": "low hanging fruit all right this is a really big one um this is uh memory efficient back",
    "start": "2764160",
    "end": "2770920"
  },
  {
    "start": "2766000",
    "end": "3053000"
  },
  {
    "text": "propagation there was a paper um by Alex Graves and others um from from Google",
    "start": "2770920",
    "end": "2777480"
  },
  {
    "text": "about this but really so the the main idea here is that uh there's actually a",
    "start": "2777480",
    "end": "2782960"
  },
  {
    "text": "lot of memory that goes into back propagation storing activation for back propagation the common way that people",
    "start": "2782960",
    "end": "2789720"
  },
  {
    "text": "implement this is imagine that this line um hopefully you can see the mouse yes all right so imagine that this line uh",
    "start": "2789720",
    "end": "2797760"
  },
  {
    "text": "is the forward propagation of the network and each of these individual smaller solid lines here are represent",
    "start": "2797760",
    "end": "2803240"
  },
  {
    "text": "evaluation of individual operations or individual layers um you kind of walk all the way from the very beginning the",
    "start": "2803240",
    "end": "2809200"
  },
  {
    "text": "inputs of the network all the way to the end and Inter at the intermediate points these solid lines you save uh all of the",
    "start": "2809200",
    "end": "2816359"
  },
  {
    "text": "the active ation or all of the um the edges in the network that cross from one layer into the next",
    "start": "2816359",
    "end": "2823480"
  },
  {
    "text": "um then on back propagation you go uh you go the other way you do the reverse operation and you use this memory that's",
    "start": "2823480",
    "end": "2830880"
  },
  {
    "text": "stored um and it's actually uh essential for computing gradients um so this is how you you",
    "start": "2830880",
    "end": "2836960"
  },
  {
    "text": "usually uh perform this operation um this okay so there's there's an",
    "start": "2836960",
    "end": "2843520"
  },
  {
    "text": "observation here which is that uh you don't actually have to store any of this um you can recompute it um and so you",
    "start": "2843520",
    "end": "2851440"
  },
  {
    "text": "can trade the memory storage uh that you're using to save these activations",
    "start": "2851440",
    "end": "2856520"
  },
  {
    "text": "against uh extra computation being done and there's you know many different ways",
    "start": "2856520",
    "end": "2861920"
  },
  {
    "text": "many different possible schedules of doing this if it's if you have a fixed",
    "start": "2861920",
    "end": "2867359"
  },
  {
    "text": "uh Network architecture and you have a fixed memory budget there's probably an optimal solution to this problem um that",
    "start": "2867359",
    "end": "2872839"
  },
  {
    "text": "you can set up and solve for explicitly um there's a really clever heris that does it in a divide and conquer way that",
    "start": "2872839",
    "end": "2880160"
  },
  {
    "text": "essentially gives you um it's basically it's basically 30% more",
    "start": "2880160",
    "end": "2886119"
  },
  {
    "text": "compute for logarithmic uh storage of the activations so it's a very simple",
    "start": "2886119",
    "end": "2892880"
  },
  {
    "text": "algorithm uh you could apply it to any network this was originally proposed for back propagation Through Time mainly",
    "start": "2892880",
    "end": "2898960"
  },
  {
    "text": "because back propagation through time kind of unrolls a network over many time steps and if you have a lot of time",
    "start": "2898960",
    "end": "2904760"
  },
  {
    "text": "steps this uh line here can get really long and you can endend up saving a lot of memory and so it's clear that you",
    "start": "2904760",
    "end": "2911079"
  },
  {
    "text": "could do it there um and it would be beneficial there but really this applies to arbitrary graphs think of your neural",
    "start": "2911079",
    "end": "2916920"
  },
  {
    "text": "network unrolled over time as a um directed ayli ayylic computation graph",
    "start": "2916920",
    "end": "2923280"
  },
  {
    "text": "this technique can apply to um any kind of computation graph so just take the critical path imagine it as a line",
    "start": "2923280",
    "end": "2930359"
  },
  {
    "text": "divide and conquer um and you end up with a logarithmic requirement for memory rather than",
    "start": "2930359",
    "end": "2938040"
  },
  {
    "text": "n so it's a huge benefit if you have 100 layers or you have a thousand time",
    "start": "2938040",
    "end": "2944599"
  },
  {
    "text": "steps so Frameworks don't implement this yet they really should do you CL this",
    "start": "2944599",
    "end": "2950200"
  },
  {
    "text": "gives you the same answer as an optimal solution that the right efficient back",
    "start": "2950200",
    "end": "2956599"
  },
  {
    "text": "propop gives you the same answer as the normal yeah it will give you the same answer assum assuming that um all of the",
    "start": "2956599",
    "end": "2962760"
  },
  {
    "text": "operations are deterministic",
    "start": "2962760",
    "end": "2966359"
  },
  {
    "text": "uh so you're trading uh computer uh operations with memory IO",
    "start": "2968319",
    "end": "2976240"
  },
  {
    "text": "operations uh which which way are you going on the tradeoff okay so the idea so we're tra",
    "start": "2976240",
    "end": "2983280"
  },
  {
    "text": "so the question is are we trading um computer operations for memory operations uh to some extent that's true",
    "start": "2983280",
    "end": "2989200"
  },
  {
    "text": "what's more important actually is that we're trading compute operations for memory capacity so the memory capacity",
    "start": "2989200",
    "end": "2996040"
  },
  {
    "text": "especially in highs speed memories like the GPU um gddr memories or high bandwidth memories is pretty small",
    "start": "2996040",
    "end": "3002599"
  },
  {
    "text": "actually so it's easy to be constrained not by the total amount of compute that you have but by the total amount of",
    "start": "3002599",
    "end": "3009680"
  },
  {
    "text": "memory required to store your model so here we're doing more compute to need less",
    "start": "3009680",
    "end": "3018520"
  },
  {
    "text": "memory okay all right this is one um all",
    "start": "3019160",
    "end": "3024760"
  },
  {
    "text": "right so I've got maybe one or two more of these here uh just in terms of uh timing is it",
    "start": "3024760",
    "end": "3032880"
  },
  {
    "text": "5:30 or 8:00 a.m. the camera stops at some point",
    "start": "3032880",
    "end": "3039799"
  },
  {
    "text": "camera stops but if you're not here okay we can camera stops at 545 yeah all right",
    "start": "3039799",
    "end": "3046400"
  },
  {
    "text": "perfect okay so um all right so this was memory efficient back propagation uh the",
    "start": "3046440",
    "end": "3052680"
  },
  {
    "text": "next one I want to talk about is model parallelism um model par par ism is basically taking your big uh neural",
    "start": "3052680",
    "end": "3059640"
  },
  {
    "start": "3053000",
    "end": "3458000"
  },
  {
    "text": "network and realizing that there's a lot of parallelism in the individual operations that make up the network um",
    "start": "3059640",
    "end": "3066480"
  },
  {
    "text": "this graph on uh on on this side is trying to show you um the performance",
    "start": "3066480",
    "end": "3072200"
  },
  {
    "text": "characteristics in terms of the basic performance characteristics of different levels of the memory hierarchy of a",
    "start": "3072200",
    "end": "3078359"
  },
  {
    "text": "machine that you could build today so the lowest level is a thread we can work up to an SM or a core you can have an",
    "start": "3078359",
    "end": "3084760"
  },
  {
    "text": "entire processor entire GPU you can look at the node level and you can look at the rack level um for each of these you",
    "start": "3084760",
    "end": "3091520"
  },
  {
    "text": "have a different total amount of compute you have a different total amount of memory capacity and then you have different uh bandwidth and latencies to",
    "start": "3091520",
    "end": "3098880"
  },
  {
    "text": "these different um these different levels so an interesting aspect of this that's already taken advantage of by the",
    "start": "3098880",
    "end": "3105720"
  },
  {
    "text": "fact that people run models on one GPU at a time is that you need a certain",
    "start": "3105720",
    "end": "3111640"
  },
  {
    "text": "size of a model um this is different depending on different types of models so I made a simplifying assumption here",
    "start": "3111640",
    "end": "3118240"
  },
  {
    "text": "that we're just using fully connected layers or recurrent neural network layers um for a convolution these",
    "start": "3118240",
    "end": "3124079"
  },
  {
    "text": "numbers wouldn't be right but this is just to kind of give you some intuition um basically the idea here is on the",
    "start": "3124079",
    "end": "3130559"
  },
  {
    "text": "other side of the figure you see these um these blocks so these represent uh dense 2D matrices kind of an",
    "start": "3130559",
    "end": "3137079"
  },
  {
    "text": "approximation of a neural network layer um at the lowest level you would need",
    "start": "3137079",
    "end": "3142280"
  },
  {
    "text": "only an 8 by8 Matrix to saturate a single thread to get full util ization out of a thread As you move up the",
    "start": "3142280",
    "end": "3149040"
  },
  {
    "text": "memory hierarchy you need bigger and bigger layers to fully utilize the",
    "start": "3149040",
    "end": "3154119"
  },
  {
    "text": "machine there are this is taking into account all of these constraints real systems would have even more constraints",
    "start": "3154119",
    "end": "3160520"
  },
  {
    "text": "than this but this simplified model goes a long way it's very it's very easy to figure out what these sizes would be",
    "start": "3160520",
    "end": "3166480"
  },
  {
    "text": "with a simplified model like this and they're not too far off okay so by the time you get to a GPU",
    "start": "3166480",
    "end": "3174000"
  },
  {
    "text": "level you might need 500 Wat wide layers or 500 unit layers um at a single node",
    "start": "3174000",
    "end": "3180480"
  },
  {
    "text": "level you might need 2,000 wide layers um at a rack level you might need um",
    "start": "3180480",
    "end": "3185599"
  },
  {
    "text": "almost 10,000 uh size layers um the interesting thing here is really that um people kind",
    "start": "3185599",
    "end": "3194119"
  },
  {
    "text": "of go up to the size of a network that fits on a GPU and then they stop um part",
    "start": "3194119",
    "end": "3199520"
  },
  {
    "text": "of this is because they don't have good software support for this in order to spread out your computation even further",
    "start": "3199520",
    "end": "3206000"
  },
  {
    "text": "you have to think about breaking up individual linear algebra operations over multiple nodes and writing you know",
    "start": "3206000",
    "end": "3213160"
  },
  {
    "text": "people for the longest time have struggled with the problem of writing maintainable software that does that but",
    "start": "3213160",
    "end": "3218520"
  },
  {
    "text": "from a technical perspective and from a performance perspective it should just work and when we've built",
    "start": "3218520",
    "end": "3224000"
  },
  {
    "text": "implementations of this it does work you do get good utilization um the question here is",
    "start": "3224000",
    "end": "3229319"
  },
  {
    "text": "really just how do you get maintainable software that's portable from one system to another that has this ability so I",
    "start": "3229319",
    "end": "3236160"
  },
  {
    "text": "feel like there's a lot of lwh hanging fruit here um if existing Frameworks can solve the software engineering problem",
    "start": "3236160",
    "end": "3242799"
  },
  {
    "text": "um there's a clear path towards getting good utilization out of much larger machines by running larger models I'm",
    "start": "3242799",
    "end": "3250240"
  },
  {
    "text": "confused on the right hand side there's different sizes are those limited by the",
    "start": "3250240",
    "end": "3256200"
  },
  {
    "text": "memory are those limited by the processor um is that is that a",
    "start": "3256200",
    "end": "3261440"
  },
  {
    "text": "connection to the memory let's see so to some extent you're limited by",
    "start": "3261440",
    "end": "3267480"
  },
  {
    "text": "everything those numbers you got right so these are um the primary limitation",
    "start": "3267480",
    "end": "3274319"
  },
  {
    "text": "uh so we're using all of these aspects here in the performance model to come up with these numbers the primary",
    "start": "3274319",
    "end": "3279880"
  },
  {
    "text": "limitations um are the memory bandwith and something called um the amount of",
    "start": "3279880",
    "end": "3285520"
  },
  {
    "text": "reuse so for each bite of memory um or sorry for yeah for each bite of memory",
    "start": "3285520",
    "end": "3291359"
  },
  {
    "text": "uh that we load from whatever the highest level memory is that needs to fit uh like maybe an 8 by 8,000 Matrix",
    "start": "3291359",
    "end": "3298520"
  },
  {
    "text": "um how many times do we or how many math operations do we perform on it so different machines yeah it's it's it's",
    "start": "3298520",
    "end": "3305760"
  },
  {
    "text": "really all of these together I think this model this is referred to as the multi- bulk synchronous parallel model",
    "start": "3305760",
    "end": "3311680"
  },
  {
    "text": "um is trying to drill down all the complexities involved in this into four numbers for each level of the memory",
    "start": "3311680",
    "end": "3317319"
  },
  {
    "text": "hierarchy it makes it tractable to do this kind of with pen and paper um but",
    "start": "3317319",
    "end": "3323400"
  },
  {
    "text": "it's in reality the the you know there are a lot of other things that go into this go",
    "start": "3323400",
    "end": "3331119"
  },
  {
    "text": "ahead rack when I was wondering why there isn't 10 times as much memory",
    "start": "3333480",
    "end": "3338880"
  },
  {
    "text": "bandwidth in a whole Rack in a single note uh So within a node you're using",
    "start": "3338880",
    "end": "3344839"
  },
  {
    "text": "PCI and between nodes you're using infin band or ethernut and there's just a difference in bandwidth between",
    "start": "3344839",
    "end": "3352039"
  },
  {
    "text": "those go ahead the top post one oh I'm sorry yeah I'm",
    "start": "3352039",
    "end": "3359079"
  },
  {
    "text": "I'm sorry about that that's definitely a typo on the slide um I'll fix that so these should be",
    "start": "3359079",
    "end": "3365558"
  },
  {
    "text": "Nan yes that would be",
    "start": "3367039",
    "end": "3371599"
  },
  {
    "text": "frightening okay any other questions that 2 G flops per thread",
    "start": "3373720",
    "end": "3381960"
  },
  {
    "text": "that's actually useful computation you can do for thread that's it's not just",
    "start": "3381960",
    "end": "3388160"
  },
  {
    "text": "like a clock rate times the number instructions per cycle okay these are all theoretical Peaks um on this per on",
    "start": "3388160",
    "end": "3394359"
  },
  {
    "text": "this processor you can get 80 to 90% of",
    "start": "3394359",
    "end": "3399000"
  },
  {
    "text": "it okay all right so this is model parallelism um if you had a bigger model",
    "start": "3401400",
    "end": "3407640"
  },
  {
    "text": "you could run on a lot bigger machine there's one thing that I didn't really mention at the very beginning that's relevant here is that as you get more",
    "start": "3407640",
    "end": "3415280"
  },
  {
    "text": "data as you have a a larger data set you typically also need a larger model to absorb it um the relationship between",
    "start": "3415280",
    "end": "3421799"
  },
  {
    "text": "these is somewhat complicated um but you do generally have this kind of effect there is an advantage of using bigger",
    "start": "3421799",
    "end": "3428079"
  },
  {
    "text": "models yeah I guess I was GNA ask something along those lines if you your model isn't that",
    "start": "3428079",
    "end": "3434319"
  },
  {
    "text": "large I mean do you can you can you paral can you still parallelize up if",
    "start": "3434319",
    "end": "3441319"
  },
  {
    "text": "your model isn't that large can you still uh use a machine like this um",
    "start": "3441319",
    "end": "3447440"
  },
  {
    "text": "you may be able to but you probably have to rely primarily on data parallelism and as we saw earlier there's a limit to",
    "start": "3447440",
    "end": "3453720"
  },
  {
    "text": "that as well okay all right",
    "start": "3453720",
    "end": "3461280"
  },
  {
    "start": "3458000",
    "end": "3584000"
  },
  {
    "text": "um let's see so there's another clear Trend I think towards a lower Precision",
    "start": "3461280",
    "end": "3466960"
  },
  {
    "text": "um arithmetic so this this graph is maybe a little bit hard to read here but the the",
    "start": "3466960",
    "end": "3472720"
  },
  {
    "text": "main point is uh we start from a uh Titan X GPU um as a Baseline and all of",
    "start": "3472720",
    "end": "3479920"
  },
  {
    "text": "the points in this graph represent uh different types of models so these are different types of speech recognition",
    "start": "3479920",
    "end": "3485480"
  },
  {
    "text": "models so from a theoretical perspective um we look at the expected performance",
    "start": "3485480",
    "end": "3491200"
  },
  {
    "text": "of those models um on different gpus one uh graph here represents a different",
    "start": "3491200",
    "end": "3497079"
  },
  {
    "text": "each graph represents a different GPU um and so the the main idea is that over",
    "start": "3497079",
    "end": "3503400"
  },
  {
    "text": "and to the right right so like moving over and moving moving to the right is better performance um so we look at multiple",
    "start": "3503400",
    "end": "3510119"
  },
  {
    "text": "changes that we could make to a GPU and we're trying to say here given all the things we could do to our computer",
    "start": "3510119",
    "end": "3516039"
  },
  {
    "text": "system what change would be the most important or what would be what would improve performance the most so we look",
    "start": "3516039",
    "end": "3521799"
  },
  {
    "text": "at moving from 32-bit floating point to 16 bit floating Point increasing the memory bandwidth from about 300 gbt to a",
    "start": "3521799",
    "end": "3529280"
  },
  {
    "text": "second to 1 terabyte per second um doubling the number of cores doubling the on chip U memory",
    "start": "3529280",
    "end": "3536599"
  },
  {
    "text": "and reducing the off chip memory latency by by uh cutting it in",
    "start": "3536599",
    "end": "3542880"
  },
  {
    "text": "half okay so the takeaway from this is pretty much everything helps um memory",
    "start": "3542880",
    "end": "3548440"
  },
  {
    "text": "bandwidth helps the least um moving to lower Precision helps the most the",
    "start": "3548440",
    "end": "3553559"
  },
  {
    "text": "reason for this is you kind of get multiple compounding effects you get higher throughput in the same power",
    "start": "3553559",
    "end": "3559039"
  },
  {
    "text": "budget you get um basically more onchip memory capacity effective onchip memory",
    "start": "3559039",
    "end": "3565079"
  },
  {
    "text": "capacity because now all of your weights take less memory to store and uh they",
    "start": "3565079",
    "end": "3570359"
  },
  {
    "text": "also take less um interconnect bandwidth to move around your",
    "start": "3570359",
    "end": "3575760"
  },
  {
    "text": "processor go ahead how much can you turn that can I go to like four bits so here",
    "start": "3575760",
    "end": "3582559"
  },
  {
    "text": "is the other side of it however low Precision training is",
    "start": "3582559",
    "end": "3587760"
  },
  {
    "start": "3584000",
    "end": "5012000"
  },
  {
    "text": "hard um so this is uh trying to show um",
    "start": "3587760",
    "end": "3593160"
  },
  {
    "text": "just some experiments that we've that we've done at at Buu on our speech recognition models training in in 16bit",
    "start": "3593160",
    "end": "3599240"
  },
  {
    "text": "Precision um so the blue curve here is a training curve uh using single Precision",
    "start": "3599240",
    "end": "3604520"
  },
  {
    "text": "of a of a good highly optimized model um the red and the Green version are um",
    "start": "3604520",
    "end": "3609640"
  },
  {
    "text": "half Precision with no other modifications than just taking every single data structure in the whole",
    "start": "3609640",
    "end": "3614760"
  },
  {
    "text": "network and converting it into half Precision um and uh the orange the",
    "start": "3614760",
    "end": "3620079"
  },
  {
    "text": "orange graph here is basically sometimes like looking at each um each data structure individually on a",
    "start": "3620079",
    "end": "3627119"
  },
  {
    "text": "case-by Case basis flipping some of them over to 16 bit um also uh when we do",
    "start": "3627119",
    "end": "3632839"
  },
  {
    "text": "math operations like when we're doing convolutions or Matrix multiply operations we use mixed Precision so we",
    "start": "3632839",
    "end": "3639760"
  },
  {
    "text": "actually there's actually an effect that we see where um if you use uh low",
    "start": "3639760",
    "end": "3644880"
  },
  {
    "text": "Precision everywhere you get these large um accumulated errors um and so we want",
    "start": "3644880",
    "end": "3649960"
  },
  {
    "text": "to do most of the math in low precision and then accumulate into higher Precision um in order to not have these",
    "start": "3649960",
    "end": "3656359"
  },
  {
    "text": "errors become significant over time so yes you can usually get it to",
    "start": "3656359",
    "end": "3661799"
  },
  {
    "text": "work um we can we can push it lower although it's hard for us to do these experiments because to push to lower",
    "start": "3661799",
    "end": "3667960"
  },
  {
    "text": "Precision we have to emulate it and it's really slow to do these experiments um but usually the process is you start by",
    "start": "3667960",
    "end": "3674119"
  },
  {
    "text": "flipping everything and then you know 10 things break in different ways and you have to diagnose them one by",
    "start": "3674119",
    "end": "3682000"
  },
  {
    "text": "one um eventually we get to a point where we do the vast majority of the math in low",
    "start": "3682000",
    "end": "3688119"
  },
  {
    "text": "Precision can you train you train Precision for size of the model at all",
    "start": "3688119",
    "end": "3695720"
  },
  {
    "text": "so can we trade Precision for the size of the model um I don't think that we've really done that kind of experiment in a",
    "start": "3695720",
    "end": "3702079"
  },
  {
    "text": "lot of detail I think there's definitely an opportunity for more work in this area",
    "start": "3702079",
    "end": "3709319"
  },
  {
    "text": "maybe sure",
    "start": "3710880",
    "end": "3714880"
  },
  {
    "text": "reducing over okay does the does the error um",
    "start": "3715960",
    "end": "3721599"
  },
  {
    "text": "help for regularization so uh the solid lines here um are the training error and",
    "start": "3721599",
    "end": "3727599"
  },
  {
    "text": "the um the dash lines are the validation error um the reason why there's a gap",
    "start": "3727599",
    "end": "3732720"
  },
  {
    "text": "between them often is because we add in noise so the training eror looks worse than it actually is um so you can see",
    "start": "3732720",
    "end": "3740319"
  },
  {
    "text": "that uh you know maybe to some extent like actually if you look at the blue and the orange curve here where one is",
    "start": "3740319",
    "end": "3746760"
  },
  {
    "text": "mixed Precision one is single Precision the orange uh validation here is actually slightly better it's hard for",
    "start": "3746760",
    "end": "3752240"
  },
  {
    "text": "me to trust a difference that small um but I think it's possible that you would see a regularization effect",
    "start": "3752240",
    "end": "3758880"
  },
  {
    "text": "there's a number of other papers on this on quantize training and on a paper called D sparse dense training um that I",
    "start": "3758880",
    "end": "3765799"
  },
  {
    "text": "think shows an effect like this do you ever try to match the",
    "start": "3765799",
    "end": "3771079"
  },
  {
    "text": "regularization you're using uh so turn down other regularization when you go",
    "start": "3771079",
    "end": "3776599"
  },
  {
    "text": "down toow position okay so uh it's important to uh just say say what our",
    "start": "3776599",
    "end": "3781680"
  },
  {
    "text": "perspective is on regularization um regularization is much more important in a small data regime so in the regime",
    "start": "3781680",
    "end": "3788599"
  },
  {
    "text": "that we're in where we basically have infinite data it's not nearly as important right regularization in the",
    "start": "3788599",
    "end": "3794039"
  },
  {
    "text": "limit doesn't matter if you look at every sample only",
    "start": "3794039",
    "end": "3798440"
  },
  {
    "text": "once so um if you have a problem like if you have a small data set perhaps that",
    "start": "3799079",
    "end": "3804960"
  },
  {
    "text": "would be very helpful to do this we don't have a lot of experience or experimental results here because we're not really operating in that",
    "start": "3804960",
    "end": "3812599"
  },
  {
    "text": "regime okay all right um let's see so this is",
    "start": "3813960",
    "end": "3821039"
  },
  {
    "text": "this is something so I've given this talk at a bunch of places this is the part of the uh the talk that I usually give to um people who build Hardware",
    "start": "3821039",
    "end": "3828880"
  },
  {
    "text": "because commonly the most important missing piece of Hardware is the software",
    "start": "3828880",
    "end": "3835960"
  },
  {
    "text": "um and it's really important to be able to actually uh use these like it's really important if we want to use like",
    "start": "3835960",
    "end": "3842359"
  },
  {
    "text": "new hardware with really great performance characteristics and there's no software and your Hardware is",
    "start": "3842359",
    "end": "3848000"
  },
  {
    "text": "basically some complicated data flow M you know hugely parallel fine grain",
    "start": "3848000",
    "end": "3853799"
  },
  {
    "text": "synchronous um thing that maybe has great theoretical performance um you know it would",
    "start": "3853799",
    "end": "3860039"
  },
  {
    "text": "probably take us months or years to write efficient convolutions um efficient uh linear algebra operations",
    "start": "3860039",
    "end": "3866839"
  },
  {
    "text": "for these things so there's a huge amount of effort involved in that and actually if you think about it the API",
    "start": "3866839",
    "end": "3872160"
  },
  {
    "text": "surface for deep learning is actually pretty narrow you think of things that are narrow like um database operations",
    "start": "3872160",
    "end": "3878599"
  },
  {
    "text": "like um sqls being like a really narrow interface and it's really valuable there a narrow interface because people don't",
    "start": "3878599",
    "end": "3884720"
  },
  {
    "text": "have to implement too many different things you don't need general purpose C",
    "start": "3884720",
    "end": "3889839"
  },
  {
    "text": "python level um programming interface to run these algorithms you really just",
    "start": "3889839",
    "end": "3895000"
  },
  {
    "text": "need glass convolutions Malo and M Copy right it's a really small interface um",
    "start": "3895000",
    "end": "3902359"
  },
  {
    "text": "so Nvidia did a really great job of actually just coming out with libraries that work well and are highly optimized",
    "start": "3902359",
    "end": "3908799"
  },
  {
    "text": "for these things but it's amazing that these libraries are missing on a lot of",
    "start": "3908799",
    "end": "3914119"
  },
  {
    "text": "popular Hardware platforms yeah so however you implement",
    "start": "3914119",
    "end": "3920400"
  },
  {
    "text": "them it's fine just that's the interface that that we use for these applications",
    "start": "3920400",
    "end": "3926920"
  },
  {
    "text": "all right so the last part of the talk I want to spend a few minutes um talking about some open research problems um",
    "start": "3927520",
    "end": "3933920"
  },
  {
    "text": "these are things given the problem setup given there's a clear value in improving performance um what are directions that",
    "start": "3933920",
    "end": "3941240"
  },
  {
    "text": "we can go in uh to try and that might be promising okay clearly faster processors",
    "start": "3941240",
    "end": "3948839"
  },
  {
    "text": "would help um I think there are a lot of people already working on this but I think there's an important um aspect of",
    "start": "3948839",
    "end": "3955760"
  },
  {
    "text": "dorning where because we have this um Power law effect going on with data set size and we are compute limited we don't",
    "start": "3955760",
    "end": "3962799"
  },
  {
    "text": "just want 2x better performance or 4X better performance we want a THX better",
    "start": "3962799",
    "end": "3968200"
  },
  {
    "text": "we want it to be much much better than it currently is so it might actually cause us to fundamentally think about um",
    "start": "3968200",
    "end": "3976480"
  },
  {
    "text": "what are the limits given current technology if we just went started from a clean slate what's the best that we",
    "start": "3976480",
    "end": "3982760"
  },
  {
    "text": "could possibly do you know I've talked to um a bunch of people about this I know um Bill Al in particular doesn't",
    "start": "3982760",
    "end": "3988559"
  },
  {
    "text": "think that there's more than a than a 2X uh improvement over gpus and maybe he's right about that um but I think it would",
    "start": "3988559",
    "end": "3995240"
  },
  {
    "text": "be interesting to consider this and I think it's also important um to think further into the future if we do have um",
    "start": "3995240",
    "end": "4003319"
  },
  {
    "text": "there may be questions of like well we need like if you're building computer hardware and you're saying you know our",
    "start": "4003319",
    "end": "4009319"
  },
  {
    "text": "computers really fast enough is my laptop really fast enough is my cell phone really fast enough is my cluster",
    "start": "4009319",
    "end": "4014680"
  },
  {
    "text": "really F already f fast enough maybe it is for a lot of applications but it's not for deep learning so let's see how",
    "start": "4014680",
    "end": "4021079"
  },
  {
    "text": "far we can really go can we actually get to 5 nanometers can we actually push further than that how far does 3D",
    "start": "4021079",
    "end": "4027920"
  },
  {
    "text": "integration take us there's a clear Advantage here these things have really great deep neural networks have really",
    "start": "4027920",
    "end": "4034000"
  },
  {
    "text": "great um advantages in terms of uh the efficiency of the computational building",
    "start": "4034000",
    "end": "4039359"
  },
  {
    "text": "blocks it's hard to think of something more efficient than a convolution from a computational perspective in terms of",
    "start": "4039359",
    "end": "4045079"
  },
  {
    "text": "how much data you actually need to move how much State you actually need to move around a processor maybe we actually",
    "start": "4045079",
    "end": "4051440"
  },
  {
    "text": "could build um a very tightly integrated 3D stacked processor um with very very",
    "start": "4051440",
    "end": "4058160"
  },
  {
    "text": "low energy um implementing",
    "start": "4058160",
    "end": "4063000"
  },
  {
    "text": "convolutions okay so there's this interesting effect um so that so that",
    "start": "4065000",
    "end": "4070200"
  },
  {
    "text": "was faster processors could we train networks with a sparse representation we use dens algebra for everything that's",
    "start": "4070200",
    "end": "4076720"
  },
  {
    "text": "the state-ofthe-art could we switch to a sparse representation be clear that that would reduce the total amount of work",
    "start": "4076720",
    "end": "4082720"
  },
  {
    "text": "that we would do but can we actually get this to work in practice so you might",
    "start": "4082720",
    "end": "4088200"
  },
  {
    "text": "think that this is um promising because of uh work that's been done um on taking",
    "start": "4088200",
    "end": "4095799"
  },
  {
    "text": "dense or taking models that were trained in a dense fashion um and then applying",
    "start": "4095799",
    "end": "4100880"
  },
  {
    "text": "pruning or quantization uh to reduce them down into a spar presentation this",
    "start": "4100880",
    "end": "4106640"
  },
  {
    "text": "has been done successfully after the model has already been trained so you get basically no performance loss in",
    "start": "4106640",
    "end": "4113600"
  },
  {
    "text": "terms of accuracy for something like a 10 to 100 times reduction in the total amount of computational work that you do",
    "start": "4113600",
    "end": "4120679"
  },
  {
    "text": "moving from a dense to a sparse representation could we get this to work for inference I think to some extent",
    "start": "4120679",
    "end": "4126920"
  },
  {
    "text": "this is a Chicken and the Egg and and an egg problem because people um really don't have a",
    "start": "4126920",
    "end": "4134238"
  },
  {
    "text": "lot of motivation to try getting training to work if hardware and software libraries hardware",
    "start": "4134239",
    "end": "4139798"
  },
  {
    "text": "implementations and software libraries can't take advantage of it most of the time when we actually run sparse",
    "start": "4139799",
    "end": "4145960"
  },
  {
    "text": "implementations um through existing deep learning Frameworks we emulate it so it's actually slower than dense",
    "start": "4145960",
    "end": "4153679"
  },
  {
    "text": "training and it's very hard to actually do these experiments and demonstrate a benefit because what you'd really like",
    "start": "4153679",
    "end": "4159520"
  },
  {
    "text": "to do is not run the same siiz model you'd really like to run a much bigger model or on a much bigger data",
    "start": "4159520",
    "end": "4166278"
  },
  {
    "text": "set um and see if you get results from training in a sparse fashion there's",
    "start": "4166279",
    "end": "4171960"
  },
  {
    "text": "also a lot of options about exactly how you would do this like how you rematerialize weights um you typically",
    "start": "4171960",
    "end": "4179000"
  },
  {
    "text": "you're thinking of um looking at the space of all functions represented by um",
    "start": "4179000",
    "end": "4184278"
  },
  {
    "text": "represented by a single neural network um and you might think about",
    "start": "4184279",
    "end": "4189838"
  },
  {
    "text": "constraining that in a way where at a given point you might materialize only a certain number of values but you like to",
    "start": "4189839",
    "end": "4194960"
  },
  {
    "text": "have the option of eventually um representing the entire space of functions um it's not really",
    "start": "4194960",
    "end": "4201640"
  },
  {
    "text": "straightforward to think about how to do that um with the existing algorithms that people are using you take a",
    "start": "4201640",
    "end": "4208159"
  },
  {
    "text": "gradient you get you know some step size over all parameters how do you know which ones to materialize and which ones",
    "start": "4208159",
    "end": "4214080"
  },
  {
    "text": "not to it's not really clear um but there would be a big Advantage if someone figure this",
    "start": "4214080",
    "end": "4221840"
  },
  {
    "text": "out all right um I I think there's so there's also this clear limit that we ran into previously it's different for",
    "start": "4222600",
    "end": "4229159"
  },
  {
    "text": "different problems but we ran into this limit of we increased the amount of data parallelism that's one way of scaling to",
    "start": "4229159",
    "end": "4236000"
  },
  {
    "text": "larger machines but if we do it too much um we eventually make our optimization",
    "start": "4236000",
    "end": "4241280"
  },
  {
    "text": "algorithms less efficient so we converge slower so it begs the question are there",
    "start": "4241280",
    "end": "4246920"
  },
  {
    "text": "optimization algorithms that can tolerate more data parallelism more than SGD I think there are some experimental",
    "start": "4246920",
    "end": "4253640"
  },
  {
    "text": "results that indicate that they're might be so there's an interesting effect that people have observed where when you're",
    "start": "4253640",
    "end": "4260400"
  },
  {
    "text": "trading a model you can typically increase the batch size um over time",
    "start": "4260400",
    "end": "4266320"
  },
  {
    "text": "without impacting work efficiency exactly what that effect looks like um",
    "start": "4266320",
    "end": "4271800"
  },
  {
    "text": "is is somewhat hard to measure it's definitely problem domain specific but I",
    "start": "4271800",
    "end": "4276880"
  },
  {
    "text": "think um effects like that where you can clearly observe them um demonstrate that there might be some um underlying",
    "start": "4276880",
    "end": "4283360"
  },
  {
    "text": "principle here like if there's some way of detecting when it's safe to grow the batch size um or whether there's some",
    "start": "4283360",
    "end": "4290320"
  },
  {
    "text": "optimization algorithm that really isn't sensitive to this that can average over",
    "start": "4290320",
    "end": "4295520"
  },
  {
    "text": "larger larger data sets and then make better decisions than first order methods um I think there's a possibility",
    "start": "4295520",
    "end": "4303199"
  },
  {
    "text": "for this but no one's really demonstrated this yet",
    "start": "4303199",
    "end": "4308080"
  },
  {
    "text": "either all right so there's also the other side of it",
    "start": "4310400",
    "end": "4316120"
  },
  {
    "text": "uh which is take the existing algorithms they're already amazingly work efficient let's make them even more efficient um",
    "start": "4316120",
    "end": "4322760"
  },
  {
    "text": "so there's a there maybe three common Trends here uh that have shown promise on small scale and might scale up to",
    "start": "4322760",
    "end": "4329960"
  },
  {
    "text": "much larger problems um so one is hard mining hard mining is basically looking",
    "start": "4329960",
    "end": "4335159"
  },
  {
    "text": "at your data set and taking this realization that some things um especially towards the end of training",
    "start": "4335159",
    "end": "4341600"
  },
  {
    "text": "you're typically just nailing some examples like some speech Rec maybe in speech recognition some utterances are",
    "start": "4341600",
    "end": "4346960"
  },
  {
    "text": "just really easy or maybe in Vision a vision problem like maybe pictures of cars are just really easy and you're",
    "start": "4346960",
    "end": "4352960"
  },
  {
    "text": "having a really hard time distinguishing between different breeds of dogs so you can measure the difficulty when you're",
    "start": "4352960",
    "end": "4359400"
  },
  {
    "text": "training of individual samples and as you get better at some of your data set you can shift your focus towards the",
    "start": "4359400",
    "end": "4365280"
  },
  {
    "text": "things that are um actually harder for you um so this is hard mining um",
    "start": "4365280",
    "end": "4370520"
  },
  {
    "text": "curriculum learning tries to do something similar but isn't really very Dynamic um you kind of look at the problem as a person um or using some",
    "start": "4370520",
    "end": "4378560"
  },
  {
    "text": "kind of other method other than the network and you try and grade examples so you don't want to start the network",
    "start": "4378560",
    "end": "4383760"
  },
  {
    "text": "with really difficult problems because it probably won't be able to find a good solution randomly start with very easy",
    "start": "4383760",
    "end": "4389360"
  },
  {
    "text": "problems and work up to harder problems um and active learning basically applies the same thing uh",
    "start": "4389360",
    "end": "4397239"
  },
  {
    "text": "to let's see the same kinds of effects um to unsupervised data so imagine that",
    "start": "4397239",
    "end": "4403600"
  },
  {
    "text": "you have this large unlabeled training set um you let you can easily do passes",
    "start": "4403600",
    "end": "4409040"
  },
  {
    "text": "of a train model over your um large unsupervised unlabeled training de training set um you can use a model to",
    "start": "4409040",
    "end": "4415719"
  },
  {
    "text": "basically mine through the large data set and tell you which examples are the most promising to label and to add into",
    "start": "4415719",
    "end": "4421639"
  },
  {
    "text": "your data set next so if you have a fixed computational budget you can over time iterate through this multiple times",
    "start": "4421639",
    "end": "4428239"
  },
  {
    "text": "and improve the quality of your training sets and make your learning more",
    "start": "4428239",
    "end": "4433120"
  },
  {
    "text": "efficient all right so pretty much uh getting to the end",
    "start": "4433800",
    "end": "4438960"
  },
  {
    "text": "here hopefully um hopefully I've uh talked about or convinced you that",
    "start": "4438960",
    "end": "4444360"
  },
  {
    "text": "there's a lot of Promise in improving performance for deep learning algorithms um that we have a lot of tools we have a",
    "start": "4444360",
    "end": "4450400"
  },
  {
    "text": "lot of directions that we can go to make progress here um so I want to leave with a",
    "start": "4450400",
    "end": "4455840"
  },
  {
    "text": "challenge so I've I'm not entirely sure if this is going to work um so I tried to at some",
    "start": "4455840",
    "end": "4464320"
  },
  {
    "text": "point sit down and say for the data sets that we have and my understanding of",
    "start": "4464320",
    "end": "4469400"
  },
  {
    "text": "Technology scaling and computer architecture what is the best case scenario like absolutely everything",
    "start": "4469400",
    "end": "4476840"
  },
  {
    "text": "perfectly aligns we invent wonderful new technologies technology scaling goes down to like half of a nanometer um what",
    "start": "4476840",
    "end": "4485199"
  },
  {
    "text": "is the best possible performance that we could hope to achieve so my my feeling was that it was around 20 pedop flops 32",
    "start": "4485199",
    "end": "4492920"
  },
  {
    "text": "tabt of ram at 300 terab per second in 300 watts this sounds absolutely insane",
    "start": "4492920",
    "end": "4499760"
  },
  {
    "text": "but deep learning is an application that could use this so I want to issue a",
    "start": "4499760",
    "end": "4504960"
  },
  {
    "text": "challenge to people here to try and build this",
    "start": "4504960",
    "end": "4510280"
  },
  {
    "text": "system how much um $300 of course yes of course it has to be cheap how much is",
    "start": "4512960",
    "end": "4518840"
  },
  {
    "text": "this worth to me I don't know I I think the way I think about this is how much is it worth to solve speech recognition",
    "start": "4518840",
    "end": "4525040"
  },
  {
    "text": "to solve computer vision or to solve language understanding there's no good that comes how that Worth to Society",
    "start": "4525040",
    "end": "4530920"
  },
  {
    "text": "speech recognition I'm not a business person just T yeah nothing good one the",
    "start": "4530920",
    "end": "4537000"
  },
  {
    "text": "first thing the consumer will ask is is Windows compatible yeah unfortunately I think um",
    "start": "4537000",
    "end": "4543400"
  },
  {
    "text": "or fortunately perhaps uh I wouldn't want to think about running an operating system on a",
    "start": "4543400",
    "end": "4548480"
  },
  {
    "text": "machine that was optimized for for running a neural network that sounds like a way harder",
    "start": "4548480",
    "end": "4554600"
  },
  {
    "text": "problem than this okay so be happy to take any",
    "start": "4554600",
    "end": "4559719"
  },
  {
    "text": "questions um anybody's interested in uh asking me more questions about this or",
    "start": "4559719",
    "end": "4565000"
  },
  {
    "text": "contacting me I'll be happy to answer questions um please come visit us at Buu if you're interested in learning about",
    "start": "4565000",
    "end": "4570560"
  },
  {
    "text": "more of these topics um this is really just a survey of a few things we could go a lot more deep into these topics if",
    "start": "4570560",
    "end": "4575679"
  },
  {
    "text": "you're interested we're also hiring um we definitely need more people to work on these",
    "start": "4575679",
    "end": "4581800"
  },
  {
    "text": "problems thank you [Applause]",
    "start": "4581800",
    "end": "4590360"
  },
  {
    "text": "yeah typical deep learning startup is wat what drives that is that the",
    "start": "4590360",
    "end": "4599040"
  },
  {
    "text": "cost um what drives 300 watts I think it's just keeping the existing um power",
    "start": "4599040",
    "end": "4605520"
  },
  {
    "text": "power envelope for a single processor constant um I think there are possibly thermal limitations and uh power supply",
    "start": "4605520",
    "end": "4612679"
  },
  {
    "text": "limitations that might keep you keep you that but if if you know if you can improve performance per",
    "start": "4612679",
    "end": "4619920"
  },
  {
    "text": "watt and you need to break that assumption go for",
    "start": "4619920",
    "end": "4624678"
  },
  {
    "text": "it gu uh what your network I guess I'm coming back to your async GD uh G",
    "start": "4625400",
    "end": "4634199"
  },
  {
    "text": "SGD what's the network utilization what okay what is the network so the question is what is the",
    "start": "4634199",
    "end": "4640199"
  },
  {
    "text": "network utilization like for um asynchronous SGD so it really depends on",
    "start": "4640199",
    "end": "4645600"
  },
  {
    "text": "the model um I think for the speech recognition models over about let's say",
    "start": "4645600",
    "end": "4651560"
  },
  {
    "text": "32 to about 128 gpus um it's about even without using asynchronous",
    "start": "4651560",
    "end": "4658880"
  },
  {
    "text": "SGD even without using asynchronous SGD we're somewhere around um maybe 1",
    "start": "4658880",
    "end": "4664600"
  },
  {
    "text": "Gigabyte per second uh per link over all all of the um all the PCI links and all",
    "start": "4664600",
    "end": "4671920"
  },
  {
    "text": "of the um infin band links somewhere between one and and maybe 4 gabyt per second uh fully",
    "start": "4671920",
    "end": "4679520"
  },
  {
    "text": "utilized does that make sense does that mean you're running a fairly acceptable",
    "start": "4679520",
    "end": "4684920"
  },
  {
    "text": "utilization on this when you got 100 gbit link you're running a 25% utilization roughly and the network",
    "start": "4684920",
    "end": "4692239"
  },
  {
    "text": "running at that utilization doesn't generally especially with more of the incremental updates is not seeing huge",
    "start": "4692239",
    "end": "4699000"
  },
  {
    "text": "amount of congestion so I'm wondering if you could turn your argument against this async approach around and say well",
    "start": "4699000",
    "end": "4706040"
  },
  {
    "text": "you're running a reasonable utilization you're not going to see dramatic effects and there's not not really going to let",
    "start": "4706040",
    "end": "4712320"
  },
  {
    "text": "somebody come in there and cool with the clock rate so you know it's re reasonably stable and then if you see",
    "start": "4712320",
    "end": "4719199"
  },
  {
    "text": "nonre reproducibility with your model and you run it in that mode then there's some instability that you're going to",
    "start": "4719199",
    "end": "4725360"
  },
  {
    "text": "see in practice because you're taking training data you're training this stuff is not going to see the same inputs in",
    "start": "4725360",
    "end": "4731600"
  },
  {
    "text": "reality so if it's unstable there in what you're doing then it's probably fundamentally unstable the",
    "start": "4731600",
    "end": "4738320"
  },
  {
    "text": "result I see so the the question is um imagine in this situation you're not uh",
    "start": "4738320",
    "end": "4744480"
  },
  {
    "text": "interconnect limited so if you are using an asynchronous method you're unlikely to see non-determinism um and so in that",
    "start": "4744480",
    "end": "4752719"
  },
  {
    "text": "case the thing break you know thing is really unstable and if it's unstable",
    "start": "4752719",
    "end": "4758199"
  },
  {
    "text": "then it's not going to be useful in reality right the errors that are introduced are I mean there's already a",
    "start": "4758199",
    "end": "4764360"
  },
  {
    "text": "lot of your training and what it sees it's input and are variance than sure so one",
    "start": "4764360",
    "end": "4772120"
  },
  {
    "text": "way to think about this perhaps could be that um the asynchronous methods they're really only useful if you are at that",
    "start": "4772120",
    "end": "4778480"
  },
  {
    "text": "interconnected limit for the speech recognition models we typically don't use them because we're not near that limit so there's no maybe there's no",
    "start": "4778480",
    "end": "4786159"
  },
  {
    "text": "harm but there's also no benefit uh to using those methods um if you want to",
    "start": "4786159",
    "end": "4791800"
  },
  {
    "text": "introduce uh you know um stoas itic it into your training you can do that with",
    "start": "4791800",
    "end": "4797480"
  },
  {
    "text": "random like explicit sources of random number generation either in model initialization or training set traversal",
    "start": "4797480",
    "end": "4804400"
  },
  {
    "text": "um so I I definitely do agree with you that it is a good idea uh to introduce some amount of Randomness into training",
    "start": "4804400",
    "end": "4810320"
  },
  {
    "text": "um to make sure that the model is lost um let's see so but this this uh",
    "start": "4810320",
    "end": "4817320"
  },
  {
    "text": "statement that um the speech recognition models are not sensitive to interconnect bandwidth at least at the scales that we're running them right now um this",
    "start": "4817320",
    "end": "4824920"
  },
  {
    "text": "doesn't apply to all models so language models and translation models in particular are running at the",
    "start": "4824920",
    "end": "4830480"
  },
  {
    "text": "interconnect limit they're limited by interconnect bandwidth even given the types of systems that we have um the",
    "start": "4830480",
    "end": "4837040"
  },
  {
    "text": "reason for that is because they're typically unrolled they're less comput intensive so they do less computation um",
    "start": "4837040",
    "end": "4844400"
  },
  {
    "text": "for uh every sample um so there's a whole range of",
    "start": "4844400",
    "end": "4850239"
  },
  {
    "text": "models and this is really just a tool you shouldn't think of it as always wanting to use this but use it if it's",
    "start": "4850239",
    "end": "4858520"
  },
  {
    "text": "appropriate that question is there a difference between average interconnect",
    "start": "4862120",
    "end": "4867159"
  },
  {
    "text": "usage versus the peak usage the algori result in a usage locational times that",
    "start": "4867159",
    "end": "4872760"
  },
  {
    "text": "create isues all right so the question is um are there differences in network",
    "start": "4872760",
    "end": "4878159"
  },
  {
    "text": "uh band with utilization over time um so it's generally possible if you have a",
    "start": "4878159",
    "end": "4884600"
  },
  {
    "text": "model that's very well balanced right like a model that if given the uh",
    "start": "4884600",
    "end": "4889920"
  },
  {
    "text": "computational through given the network bandwidth um then and they're about",
    "start": "4889920",
    "end": "4895080"
  },
  {
    "text": "equally balanced it's generally possible to schedule things although it's difficult so that they're perfectly overlapped um you never get perfect",
    "start": "4895080",
    "end": "4901960"
  },
  {
    "text": "overlap but you can get pretty high um overlap that being said it's hard to do that I think most Frameworks don't do",
    "start": "4901960",
    "end": "4908880"
  },
  {
    "text": "this um basically um in large layers uh there's just a lot of parallelism so you",
    "start": "4908880",
    "end": "4915120"
  },
  {
    "text": "can think about um all of the model weights and especially during back propagation um you can think of back",
    "start": "4915120",
    "end": "4921600"
  },
  {
    "text": "propagation as exposing some of the weights as being available for um for",
    "start": "4921600",
    "end": "4926960"
  },
  {
    "text": "synchronization so you can kind of overlap if you break things up and to find enough pieces you can overlap",
    "start": "4926960",
    "end": "4932840"
  },
  {
    "text": "synchronize these weights do back propagation for the next set of weights and kind of cascade that through the",
    "start": "4932840",
    "end": "4938159"
  },
  {
    "text": "entire network um as long as you can schedule things like that you have a global visibility into the whole problem",
    "start": "4938159",
    "end": "4945320"
  },
  {
    "text": "you can usually overlap things well um that being said if you're unbalanced right like for different models you",
    "start": "4945320",
    "end": "4952280"
  },
  {
    "text": "might have different interconnect to compute ratios um if you swing it very far on one end of the spectrum or",
    "start": "4952280",
    "end": "4958760"
  },
  {
    "text": "another you'll be limited by one or the other so you'll swing from like if you're very compute limited you'll swing",
    "start": "4958760",
    "end": "4964159"
  },
  {
    "text": "from phases where you're doing just all compute and then a little bit of bandwidth or if your network limited you'll swing from phases we you're doing",
    "start": "4964159",
    "end": "4970120"
  },
  {
    "text": "all Network to just a little bit of Compu um if you're not sens ensi to these things like if you're not actually",
    "start": "4970120",
    "end": "4976280"
  },
  {
    "text": "looking at the characteristic of your system uh it's very common that you just end up in one of those two",
    "start": "4976280",
    "end": "4982719"
  },
  {
    "text": "situations it's a really it's an important Point actually that there's a big space of models um there's a and",
    "start": "4982719",
    "end": "4990360"
  },
  {
    "text": "there's a smaller space that works well for a given application like for speech recognition or Vision there's an even",
    "start": "4990360",
    "end": "4996159"
  },
  {
    "text": "smaller set that runs efficiently on hardw",
    "start": "4996159",
    "end": "5001159"
  }
]