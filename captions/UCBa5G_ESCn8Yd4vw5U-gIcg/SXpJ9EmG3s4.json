[
  {
    "start": "0",
    "end": "5490"
  },
  {
    "text": "OK. Awesome. We're going to get started. So my name is Jesse Mu,\nI'm a PhD student in the CS",
    "start": "5490",
    "end": "13730"
  },
  {
    "text": "Department here working\nwith the NLP group, and really excited to be talking\nabout the topic of today's",
    "start": "13730",
    "end": "20119"
  },
  {
    "text": "lecture, which is on\nprompting, instruction, fine tuning and RLHF. So this is all stuff that\nhas been super hot recently",
    "start": "20120",
    "end": "29090"
  },
  {
    "text": "because of all of\nthe latest craze about chatbots,\nChatGPT, Bing et cetera.",
    "start": "29090",
    "end": "34528"
  },
  {
    "text": "And we're going to hopefully\nget somewhat of an understanding as to how these\nsystems are trained.",
    "start": "34528",
    "end": "40800"
  },
  {
    "text": "OK. So before that, some\ncourse logistics things. So project proposals,\nboth custom and final",
    "start": "40800",
    "end": "46160"
  },
  {
    "text": "were due a few minutes ago. So if you haven't done that,\nthis is a nice reminder.",
    "start": "46160",
    "end": "52129"
  },
  {
    "text": "We're in the process of\nassigning mentors of projects, so we'll give feedback soon.",
    "start": "52130",
    "end": "57500"
  },
  {
    "text": "Besides that, assignment 5\nis due on Friday at midnight. We still recommend using\nColab for the assignments,",
    "start": "57500",
    "end": "64069"
  },
  {
    "text": "even if you've had AWS\nor Azure credits granted. If that doesn't work,\nthere's instructions for how to connect\nto a Kaggle notebook",
    "start": "64069",
    "end": "70375"
  },
  {
    "text": "where you will also\nbe able to use GPUs. Look for that post on Ed. And then finally, also\njust posted on Ed by John",
    "start": "70375",
    "end": "77540"
  },
  {
    "text": "is a course feedback survey. So this is part of your\nparticipation grade. Please fill that in\nby Sunday 11:59 PM.",
    "start": "77540",
    "end": "85200"
  },
  {
    "text": "OK?  OK. So let's get into\nthis lecture, which",
    "start": "85200",
    "end": "92770"
  },
  {
    "text": "is going to be about what we are\ntrying to do with these larger and larger models, right?",
    "start": "92770",
    "end": "98020"
  },
  {
    "text": "Over the years, the\ncompute for these models have just gone up\nhundreds of powers of 10.",
    "start": "98020",
    "end": "106600"
  },
  {
    "text": "Trained on more and\nmore data, right? So larger and larger\nmodels, they're seeing more and more\ndata, and in lecture 10,",
    "start": "106600",
    "end": "115119"
  },
  {
    "text": "if you recall this\nslide, we talked a little bit about what happens\nwhen you do pre-training.",
    "start": "115120",
    "end": "120759"
  },
  {
    "text": "And as you begin to really\nlearn to predict the missing sentence in certain\ntexts, right, you learn things like syntax,\nco-reference, sentiment, et",
    "start": "120760",
    "end": "129668"
  },
  {
    "text": "cetera. But in this lecture, we're\ngoing to take it a little bit further, and really take this\nidea to its logical conclusion.",
    "start": "129669",
    "end": "135290"
  },
  {
    "text": "So if you really follow\nthis idea of-- we're just going to train a\ngiant language model on all of the world's text,\nyou really begin",
    "start": "135290",
    "end": "142629"
  },
  {
    "text": "to see language models sort of\nin a way as rudimentary world models. So maybe they're not very\ngood at World models,",
    "start": "142630",
    "end": "148588"
  },
  {
    "text": "but they kind of have to be\ndoing some implicit world modeling just because we\nhave so much information on the internet, and so much\nof human collective knowledge",
    "start": "148588",
    "end": "156230"
  },
  {
    "text": "is transcribed and written\nfor us on the internet, right? So if you are really\ngood at predicting the next word in text,\nwhat do you learn to do?",
    "start": "156230",
    "end": "163340"
  },
  {
    "text": "There's evidence that\nthese large language models are to some degree learning\nto represent and think",
    "start": "163340",
    "end": "168409"
  },
  {
    "text": "about agents, and humans,\nand the beliefs, and actions that they might take. So here's an example\nfrom a recent paper where",
    "start": "168410",
    "end": "175340"
  },
  {
    "text": "we are talking about\nsomeone named Pat watching a demonstration of a bowling\nball and a leaf being dropped",
    "start": "175340",
    "end": "180860"
  },
  {
    "text": "at the same time in\na vacuum chamber. And the idea is-- here we're saying Pat\nis a physicist, right?",
    "start": "180860",
    "end": "187280"
  },
  {
    "text": "So if Pat is a physicist, and\nwe ask for the language model's next continuation\nof this sentence,",
    "start": "187280",
    "end": "193130"
  },
  {
    "text": "because he's a physicist,\nwe do some inference about what kind of\nknowledge Pat has, and Pat will predict that\nthe bowling ball and the leaf",
    "start": "193130",
    "end": "199519"
  },
  {
    "text": "will fall at the same time. But if we change the sentence\nof the prompt and we say, well, Pat has actually never\nseen this demonstration",
    "start": "199520",
    "end": "205837"
  },
  {
    "text": "before, then Pat will\npredict that the bowling ball will fall to the ground\nfirst, which is wrong, right? So if you get really good at\npredicting the next sentence",
    "start": "205837",
    "end": "213110"
  },
  {
    "text": "in text, you also\nto some degree have to learn to predict an agent's\nbeliefs, their backgrounds,",
    "start": "213110",
    "end": "218840"
  },
  {
    "text": "common knowledge, and\nwhat they might do next. So not just that, of\ncourse, if we continue",
    "start": "218840",
    "end": "224597"
  },
  {
    "text": "browsing the internet, we see a\nlot of encyclopedic knowledge, so maybe language\nmodels are actually good at solving math\nreasoning problems",
    "start": "224597",
    "end": "231335"
  },
  {
    "text": "if they've seen enough\ndemonstrations of math on the internet. Code, of course, code generation\nis a really exciting topic",
    "start": "231335",
    "end": "238879"
  },
  {
    "text": "that people are looking into,\nand we'll give a presentation on that in a few weeks. Even medicine, right,\nwe're beginning",
    "start": "238880",
    "end": "245270"
  },
  {
    "text": "to think about language models\ntrained on medical texts and being applied to the\nsciences and whatnot.",
    "start": "245270",
    "end": "250290"
  },
  {
    "text": "So this is what happens when\nwe really take this language modeling idea seriously. And this is a resulted in\na resurgence of interest",
    "start": "250290",
    "end": "257690"
  },
  {
    "text": "in building language models\nthat are basically assistants, right? You can give them any\ntask under the sun.",
    "start": "257690",
    "end": "264020"
  },
  {
    "text": "I want to create a\nthree course meal, and a language model should\nbe able to take a good stab",
    "start": "264020",
    "end": "269420"
  },
  {
    "text": "at being able to do this. This is kind of the promise\nof language modeling. But of course, there's\na lot of steps required",
    "start": "269420",
    "end": "276320"
  },
  {
    "text": "to get from this, from our basic\nlanguage modeling objective, and that's what this lecture\nis going to be about.",
    "start": "276320",
    "end": "282780"
  },
  {
    "text": "So how do we get\nfrom just predicting the next word in a\nsentence to something like ChatGPT, which you can\nreally ask it to do anything?",
    "start": "282780",
    "end": "290050"
  },
  {
    "text": "And it might fail sometimes,\nbut it's getting really, really convincingly good\nat some things.",
    "start": "290050",
    "end": "296159"
  },
  {
    "text": "OK. So this is the lecture plan. Basically, I'm\ngoing to talk about, as we're working with these\nlarge language models,",
    "start": "296160",
    "end": "301550"
  },
  {
    "text": "we've come up with\nincreasingly complex ways of steering the language models\ncloser and closer to something",
    "start": "301550",
    "end": "306620"
  },
  {
    "text": "like ChatGPT. So we'll start with Zero-Shot\nand Few-Shot learning, then instruction fine tuning.",
    "start": "306620",
    "end": "311880"
  },
  {
    "text": "And then, Reinforcement Learning\nfrom Human Feedback or RLHF. ",
    "start": "311880",
    "end": "318950"
  },
  {
    "text": "OK. So let's first talk about\nFew-Shot and Zero-Shot learning.",
    "start": "318950",
    "end": "324590"
  },
  {
    "text": "And in order to do\nso, we're, again, going to kind of build off\nof the pre-training lecture, last Tuesday. So in the pre-training lecture,\nJohn talked about these models",
    "start": "324590",
    "end": "333530"
  },
  {
    "text": "like GPT, Generative\nPretrained Transformer, that are these decoder\nonly language models.",
    "start": "333530",
    "end": "339150"
  },
  {
    "text": "So they're just trained\nto predict the next word in a corpus of text. And back in 2018, was the\nfirst iteration of this model,",
    "start": "339150",
    "end": "347120"
  },
  {
    "text": "and it was 117\nmillion parameters. So at the time,\nit was pretty big. Nowadays, it's\ndefinitely much smaller.",
    "start": "347120",
    "end": "353790"
  },
  {
    "text": "And again, it's just a vanilla\ntransformer decoder using the techniques that you've seen,\nand it's trained on a corpus of books, so about\n4.6gb of text.",
    "start": "353790",
    "end": "362360"
  },
  {
    "text": "And what GPT showed\nwas the promise at doing this simple\nlanguage modeling objective and serving as an effective\npretrained technique",
    "start": "362360",
    "end": "369800"
  },
  {
    "text": "for various downstream tasks\nthat you might care about. So if you wanted to apply it to\nsomething like natural language",
    "start": "369800",
    "end": "374960"
  },
  {
    "text": "inference, you would take\nyour premise sentence and your hypothesis sentence,\nconcatenate them and then",
    "start": "374960",
    "end": "380460"
  },
  {
    "text": "maybe train a linear classifier\non the last representation the model produces. ",
    "start": "380460",
    "end": "386490"
  },
  {
    "text": "OK. But that was 3, 4, 5 years ago. What has changed since then?",
    "start": "386490",
    "end": "393810"
  },
  {
    "text": "So they came out with GPT-2. So GPT-2 was released\nthe next year, in 2019.",
    "start": "393810",
    "end": "399020"
  },
  {
    "text": "This is 1.5 billion parameters. So it's the same\narchitecture as GPT, but just an order of magnitude\nbigger, and also trained",
    "start": "399020",
    "end": "407090"
  },
  {
    "text": "on much more data. So we went from 4\ngigabytes of books to 40 gigabytes of\ninternet text data.",
    "start": "407090",
    "end": "413750"
  },
  {
    "text": "So they produced a data\nset called WebText, this is produced by\nscraping a bunch of links, the comments on Reddit.",
    "start": "413750",
    "end": "419150"
  },
  {
    "text": "So the idea is that the\nweb contains a lot of spam, maybe a lot of low\nquality information, but they took links\nthat were posted",
    "start": "419150",
    "end": "425180"
  },
  {
    "text": "on Reddit that had at\nleast a few upvotes, so humans maybe look\nthrough it and said, you know, this is a useful post.",
    "start": "425180",
    "end": "430190"
  },
  {
    "text": "So that was kind of a rough\nproxy of human quality, and that's how they collected\nthis large data set.",
    "start": "430190",
    "end": "436580"
  },
  {
    "text": "And so if you look at\nthe size of GPT in 2018, we can draw a bigger dot, which\nis the size of GPT-2 in 2019.",
    "start": "436580",
    "end": "444560"
  },
  {
    "text": "And one might ask, how\nmuch better does this do? What does this buy you?",
    "start": "444560",
    "end": "450240"
  },
  {
    "text": "So the authors of GPT-2\ntitled their paper, Language Models are\nUnsupervised Multitask Learners,",
    "start": "450240",
    "end": "456390"
  },
  {
    "text": "and that kind of gives\nyou a hint as to what the key takeaway\nthey found was, which is this unsupervised\nmultitasking part.",
    "start": "456390",
    "end": "464410"
  },
  {
    "text": "So basically, I think, the\nkey takeaway from GPT-2 was this idea that\nlanguage models",
    "start": "464410",
    "end": "469800"
  },
  {
    "text": "can display Zero-Shot learning. So what I mean by\nZero-Shot learning is, you can do many tasks that the\nmodel may not have actually",
    "start": "469800",
    "end": "477180"
  },
  {
    "text": "explicitly been trained for\nwith no gradient updates. So you just kind\nof query the model",
    "start": "477180",
    "end": "482669"
  },
  {
    "text": "by simply specifying the right\nsequence prediction problem. So if you care about question\nanswering, for example,",
    "start": "482670",
    "end": "488635"
  },
  {
    "text": "you might include your passage,\nlike, a Wikipedia article about Tom Brady. And then, you'll add a question. So a question, where\nwas Tom Brady born?",
    "start": "488635",
    "end": "495180"
  },
  {
    "text": "And then include an answer,\nlike A followed by colon, and then just ask the model to\npredict the next token, right?",
    "start": "495180",
    "end": "501360"
  },
  {
    "text": "You've kind of\njerry-rigged the model into doing question answering.",
    "start": "501360",
    "end": "506460"
  },
  {
    "text": "For other tasks like\nclassification tasks, another thing you can do is\ncompare different probabilities of sequences.",
    "start": "506460",
    "end": "512200"
  },
  {
    "text": "So this task is called the\nWinograd Schema Challenge. It's a pronoun resolution\ntask, so the task",
    "start": "512200",
    "end": "517590"
  },
  {
    "text": "is to kind of resolve\na pronoun, which requires some world knowledge. So one example is\nsomething like, the cat",
    "start": "517590",
    "end": "523110"
  },
  {
    "text": "couldn't fit into the hat\nbecause it was too big. And the question is whether\n\"it\" refers to the cat",
    "start": "523110",
    "end": "528209"
  },
  {
    "text": "or to the hat, right? And in this case,\nit makes most sense for it to refer to the\ncat because things fitting",
    "start": "528210",
    "end": "534570"
  },
  {
    "text": "into things because\nthey're too big, you need to use\nsome world knowledge to kind of resolve that.",
    "start": "534570",
    "end": "539770"
  },
  {
    "text": "So the way that you get\nZero-Shot predictions for this task out of a\nlanguage model like GPT 2 is you just ask\nthe language model",
    "start": "539770",
    "end": "546750"
  },
  {
    "text": "which sequence is more likely. Is the probability of the\ncat couldn't fit into the hat",
    "start": "546750",
    "end": "552180"
  },
  {
    "text": "because the cat\nwas too big deemed more likely by the language\nmodel than the probability",
    "start": "552180",
    "end": "557310"
  },
  {
    "text": "that the cat couldn't\nfit into the hat because the hat was too big? You can score those\nsequences because this",
    "start": "557310",
    "end": "562470"
  },
  {
    "text": "is a language model. And from there, you get\nyour Zero-Shot prediction. And you can end up doing\nfairly well on this task.",
    "start": "562470",
    "end": "570070"
  },
  {
    "text": "Any questions about this? OK. Yeah, so digging a little\nbit more into the results,",
    "start": "570070",
    "end": "576850"
  },
  {
    "text": "GPT 2, at the time, beat\nthe state of the art on a bunch of language\nmodeling benchmarks",
    "start": "576850",
    "end": "581920"
  },
  {
    "text": "with no task\nspecific fine tuning. So no traditional fine\ntune on a training set and then test\non a testing set.",
    "start": "581920",
    "end": "588350"
  },
  {
    "text": "So here's an example\nof such a task. This is a language modeling\ntask called LAMBADA, where the goal is to\npredict a missing word,",
    "start": "588350",
    "end": "594250"
  },
  {
    "text": "and the idea is that the\nword that you need to predict depends on some discourse\nearlier in the sentence",
    "start": "594250",
    "end": "599740"
  },
  {
    "text": "or earlier a few sentences ago. And by simply training\nyour language model,",
    "start": "599740",
    "end": "604930"
  },
  {
    "text": "and then running it\non the LAMBADA task, you end up doing better than\nthe supervised fine tuned state",
    "start": "604930",
    "end": "610149"
  },
  {
    "text": "of the art at the time,\nand across a wide variety of other tasks as well. ",
    "start": "610150",
    "end": "618320"
  },
  {
    "text": "OK.  Another kind of interesting\nbehavior they observed,",
    "start": "618320",
    "end": "623860"
  },
  {
    "text": "and so you'll see\nhints of things that we now take for\ngranted in this paper,",
    "start": "623860",
    "end": "629230"
  },
  {
    "text": "is that you can get\ninteresting Zero-Shot behavior as long as you take\nsome liberties with how you specify the task.",
    "start": "629230",
    "end": "635420"
  },
  {
    "text": "So for example, let's imagine\nthat we want our model to do summarization. Even though GPT-2 was\njust a language model,",
    "start": "635420",
    "end": "641650"
  },
  {
    "text": "how can we make it\ndo summarization? The idea they\nexplored was, we're going to take an article,\nsome news article, and then",
    "start": "641650",
    "end": "648940"
  },
  {
    "text": "at the end, we're going to\nappend the TLDR sign, right, the TLDR token. So this stands for\nToo Long, Didn't Read.",
    "start": "648940",
    "end": "655040"
  },
  {
    "text": "It's used a lot on\nReddit to just say if you didn't want to\nread the above stuff, here's a few sentences\nthat summarizes it, right?",
    "start": "655040",
    "end": "660580"
  },
  {
    "text": "So if you ask the\nmodel to predict what follows after\nthe TLDR token, right, you might\nexpect that it'll",
    "start": "660580",
    "end": "666760"
  },
  {
    "text": "generate some sort of summary. OK. And this is kind\nof early whispers at this term that we\nnow call prompting,",
    "start": "666760",
    "end": "673390"
  },
  {
    "text": "which is thinking\nof the right way to define a task such that\nyour model will do the behavior that you want it to do.",
    "start": "673390",
    "end": "679360"
  },
  {
    "text": " So if we look at the\nperformance we actually observed on this task,\nhere at the bottom",
    "start": "679360",
    "end": "685910"
  },
  {
    "text": "is a random baseline, so you\njust select three sentences from the article, and the\nscores that we're using here",
    "start": "685910",
    "end": "691640"
  },
  {
    "text": "are Rouge scores, if you\nremember the natural language generation lecture. GPT-2 is right above.",
    "start": "691640",
    "end": "697167"
  },
  {
    "text": "So it's not actually\nthat good, right, like, it only does maybe\na little bit or barely any better than the\nrandom baseline,",
    "start": "697167",
    "end": "703070"
  },
  {
    "text": "but it is approaching\napproaches that-- or supervised approaches that\nare actually explicitly fine",
    "start": "703070",
    "end": "708709"
  },
  {
    "text": "tuned to do summarization. And of course, at\nthe time, it still underperformed the\nstate of the art,",
    "start": "708710",
    "end": "715160"
  },
  {
    "text": "but this really\nshowed the promise of getting language models\nto do things that maybe they weren't trained to do. ",
    "start": "715160",
    "end": "723810"
  },
  {
    "text": "OK. So that was GPT-2, that was\n2019, now, here's 2020, GPT-3.",
    "start": "723810",
    "end": "729710"
  },
  {
    "text": "So GPT-3 is 175\nbillion parameters. So it's another increase in\nsize by an order of magnitude.",
    "start": "729710",
    "end": "736497"
  },
  {
    "text": "And at the time, it\nwas unprecedented. I think it still is kind of\noverwhelmingly large for most people and data.",
    "start": "736497",
    "end": "742730"
  },
  {
    "text": "So they scaled up\nthe data once again. OK, so what does this buy you? This paper's title was\ncalled Language Models",
    "start": "742730",
    "end": "748970"
  },
  {
    "text": "are Few-Shot Learners. So what does that mean? So the key takeaway from GPT-3\nwas emergent Few-Shot learning.",
    "start": "748970",
    "end": "757350"
  },
  {
    "text": "So the idea here is,\nsure, GPT can still do Zero-Shot\nlearning, but now, you",
    "start": "757350",
    "end": "762680"
  },
  {
    "text": "can specify a task by basically\ngiving examples of the task before asking it to predict the\nexample that you care about.",
    "start": "762680",
    "end": "769940"
  },
  {
    "text": "OK. So this is often called\nin-context learning to stress that there\nare no gradient updates",
    "start": "769940",
    "end": "775520"
  },
  {
    "text": "being performed when\nyou learn a new task. You're basically constructing a\ntiny little training data set,",
    "start": "775520",
    "end": "780769"
  },
  {
    "text": "and just including\nit in the prompt, including it in the context\nwindow of your transformer, and then asking it to pick up\non what the task is and then",
    "start": "780770",
    "end": "787620"
  },
  {
    "text": "predict the right answer. And this is in contrast\nto a separate literature on Few-Shot learning,\nwhich assumes that you",
    "start": "787620",
    "end": "794310"
  },
  {
    "text": "can do gradient updates, right? In this case, it's really\njust a frozen language model. ",
    "start": "794310",
    "end": "800950"
  },
  {
    "text": "So Few-Shot learning works,\nand it's really impressive. So here's a graph. SuperGLUE here is kind of a\nwide coverage natural language",
    "start": "800950",
    "end": "808390"
  },
  {
    "text": "understanding benchmark,\nand what they did was they took GPT-3, and\nthis data point here",
    "start": "808390",
    "end": "813610"
  },
  {
    "text": "is what you get when you just do\nZero-Shot learning with GPT-3. So you provide an English\ndescription of the task",
    "start": "813610",
    "end": "819790"
  },
  {
    "text": "to be completed, and then you\nask it to complete the task. Just by providing one\nexample, so one shot,",
    "start": "819790",
    "end": "827340"
  },
  {
    "text": "you get like a 10%\naccuracy increase. So you give, not only\nthe natural language task description, but also an example\ninput and an example output,",
    "start": "827340",
    "end": "835290"
  },
  {
    "text": "and you ask it to\ndecode the next output. And as you increase\nto more shots, you do get better and better\nscores, although, of course,",
    "start": "835290",
    "end": "844260"
  },
  {
    "text": "you get diminishing\nreturns after a while. But what you can notice is that\nFew-Shot GPT3, so no gradient",
    "start": "844260",
    "end": "849690"
  },
  {
    "text": "updates, is doing as well as\nor outperforming BERT fine tuned on the SuperGLUE\ntask explicitly.",
    "start": "849690",
    "end": "858813"
  },
  {
    "text": "Any questions about this?  So one thing that I\nthink is really exciting",
    "start": "858813",
    "end": "865640"
  },
  {
    "text": "is that you might think, OK,\nin Few-Shot learning, whatever, it's just memorizing. Maybe there's a lot\nof examples of needing",
    "start": "865640",
    "end": "871670"
  },
  {
    "text": "to do a Few-Shot learning in\nthe internet text data, right? And that's true, but I\nthink there's also evidence",
    "start": "871670",
    "end": "876890"
  },
  {
    "text": "that GPT-3 is really learning\nto do some sort of kind of on the fly\noptimization or reasoning.",
    "start": "876890",
    "end": "883220"
  },
  {
    "text": "And so the evidence\nfor this comes in the form of these synthetic\nword unscrambling tasks. So the authors came\nup with a bunch",
    "start": "883220",
    "end": "889040"
  },
  {
    "text": "of simple kind of letter\nmanipulation tasks that are probably unlikely to\nexist in internet text data.",
    "start": "889040",
    "end": "894870"
  },
  {
    "text": "So these include things like\ncycling through the letters to get the kind of\nuncycled version of a word, so converting from\npleap to apple,",
    "start": "894870",
    "end": "903170"
  },
  {
    "text": "removing characters\nadded to a word or even just reversing\nwords, right? And what you see\nhere is performance",
    "start": "903170",
    "end": "909380"
  },
  {
    "text": "as you do a Few-Shot learning,\nas you increase the model size. And what you can see is that the\nability to do Few-Shot learning",
    "start": "909380",
    "end": "918050"
  },
  {
    "text": "is kind of an emergent\nproperty of model scale, so at the very\nlargest model, we're actually seeing a model be\nable to do this exclusively",
    "start": "918050",
    "end": "925329"
  },
  {
    "text": "in context.  OK.",
    "start": "925330",
    "end": "930990"
  },
  {
    "text": "Question? Yeah. I've noticed the reversed\nwords are horrible, like the performance.",
    "start": "930990",
    "end": "936660"
  },
  {
    "text": "Yeah, Yeah. So the question was the reversed\nwords line is still low. Yeah, that's an\nexample of a task",
    "start": "936660",
    "end": "942950"
  },
  {
    "text": "that this model still\ncan't solve yet. Although, I'm not sure\nif we evaluated it with newer and newer models,\nmaybe the latest versions",
    "start": "942950",
    "end": "949610"
  },
  {
    "text": "can indeed actually\nsolve that task. Yeah, question? Is there's some intuition\nfor why this emerges as a result of model scale?",
    "start": "949610",
    "end": "955940"
  },
  {
    "text": "I think that's a highly\nactive area of research, and there's been papers\npublished every week on this. So I think there's a lot of\ninteresting experiments that",
    "start": "955940",
    "end": "963173"
  },
  {
    "text": "really try to dissect either\nwith like synthetic tasks, like, can GPT-3 learn linear\nregression in context?",
    "start": "963173",
    "end": "969860"
  },
  {
    "text": "And there's some model\ninterpretability tasks, like, what in the attention layers,\nor what in the hidden states",
    "start": "969860",
    "end": "975079"
  },
  {
    "text": "are resulting in this\nkind of emergent learning? But yeah, I'd have\nto just refer you to the recent\nliterature on that.",
    "start": "975080",
    "end": "980720"
  },
  {
    "text": " Anything else?",
    "start": "980720",
    "end": "986250"
  },
  {
    "text": "Awesome. OK. So just to summarize,\ntraditional fine tuning here is on the right, right?",
    "start": "986250",
    "end": "991760"
  },
  {
    "text": "We take a bunch of examples\nof a task that we care about, we give it to our\nmodel, and then we do a gradient\nstep on each example.",
    "start": "991760",
    "end": "997100"
  },
  {
    "text": "And then at the\nend, we hopefully, get a model that can\ndo well on some output. And in this new kind of paradigm\nof just prompting a language",
    "start": "997100",
    "end": "1002950"
  },
  {
    "text": "model, we just have a\nfrozen language model, and we just give some\nexamples and ask the model to predict the right answer.",
    "start": "1002950",
    "end": "1008437"
  },
  {
    "text": " OK. So you might think,\nand you'd be right,",
    "start": "1008437",
    "end": "1015079"
  },
  {
    "text": "that there are some\nlimits of prompting. Well, there's a lot of limits\nof prompting, but especially for tasks that are too hard.",
    "start": "1015080",
    "end": "1020639"
  },
  {
    "text": "There are a lot of tasks that\nmaybe seem too difficult, especially ones that involve\nmaybe richer reasoning steps",
    "start": "1020640",
    "end": "1026150"
  },
  {
    "text": "or needing to synthesize\nmultiple pieces of information. And these are tasks that humans\nstruggle with too, right?",
    "start": "1026150",
    "end": "1032359"
  },
  {
    "text": "So one example is GPT-3-- I don't have the result,\nthe actual graph here, but it was famously\nbad at doing addition",
    "start": "1032359",
    "end": "1038599"
  },
  {
    "text": "for much larger digits. And so if you prompt GPT-3\nwith a bunch of examples of addition, it won't do it\ncorrect, but part of the reason",
    "start": "1038599",
    "end": "1046765"
  },
  {
    "text": "is because humans are also\npretty bad at doing this in one step, right? Like, if I asked you to just\nadd these two numbers on the fly",
    "start": "1046765",
    "end": "1052460"
  },
  {
    "text": "and didn't give you\na pencil and paper you'd have a pretty\nhard time with it. So one observation is that you\ncan just change the prompts",
    "start": "1052460",
    "end": "1060290"
  },
  {
    "text": "and hopefully get some better\nperformance out of this. So there's this idea of doing\nchain of thought prompting,",
    "start": "1060290",
    "end": "1066800"
  },
  {
    "text": "where in standard prompting,\nwe give some examples of a task that we'd like to complete.",
    "start": "1066800",
    "end": "1072100"
  },
  {
    "text": "So here is an example\nof a math word problem. And I told you\nthat what we would do is we would give the\nquestion and then the answer,",
    "start": "1072100",
    "end": "1078930"
  },
  {
    "text": "and then for a data point\nthat we actually care about, we ask the model to\npredict the answer.",
    "start": "1078930",
    "end": "1084785"
  },
  {
    "text": "And the model will try to\nproduce the right answer, and it's just wrong. OK. So the idea of\nchain-of-thought prompting",
    "start": "1084785",
    "end": "1091380"
  },
  {
    "text": "is to actually demonstrate\nwhat kind of reasoning you want the model to complete.",
    "start": "1091380",
    "end": "1096730"
  },
  {
    "text": "So in your prompt, you\nnot only put the question, but you also put an answer and\nthe kinds of reasoning steps",
    "start": "1096730",
    "end": "1102990"
  },
  {
    "text": "that are required to arrive\nat the correct answer. So here is actually\nsome reasoning of how you actually would answer\nthis tennis ball question,",
    "start": "1102990",
    "end": "1109408"
  },
  {
    "text": "and then get the right answer. And because a language model\nis incentivized to just follow",
    "start": "1109408",
    "end": "1114600"
  },
  {
    "text": "the pattern and\ncontinue the prompt, if you give it another\nquestion, it will, in turn, produce an answer--",
    "start": "1114600",
    "end": "1121260"
  },
  {
    "text": "Sorry, a rationale\nfollowed by an answer. So you're kind of asking the\nlanguage model to work through",
    "start": "1121260",
    "end": "1127350"
  },
  {
    "text": "the steps yourself. And by doing so, you end up\ngetting some questions right when you otherwise might not.",
    "start": "1127350",
    "end": "1134370"
  },
  {
    "text": "So super simple\nidea, but it's shown to be extremely effective. So here is this middle school\nmath word problems benchmark,",
    "start": "1134370",
    "end": "1141690"
  },
  {
    "text": "and again, as we scale\nup the model for GPT and some other kinds of\nmodels, being able to do",
    "start": "1141690",
    "end": "1147870"
  },
  {
    "text": "chain-of-thought\nprompting emerges, right? So we really see a\nperformance approaching",
    "start": "1147870",
    "end": "1152940"
  },
  {
    "text": "that of supervised baselines for\nthese larger and larger models. ",
    "start": "1152940",
    "end": "1159490"
  },
  {
    "text": "Questions? Yeah. Isn't that like seemingly\nthe problem with the addition",
    "start": "1159490",
    "end": "1165540"
  },
  {
    "text": "of the large numbers? Do you have results on\nhow chain-of-thought prompting but with larger\nnumbers than middle school math",
    "start": "1165540",
    "end": "1172770"
  },
  {
    "text": "problems? Yeah. So the question is,\ndoes chain-of-thought prompting work for\nthose addition problems",
    "start": "1172770",
    "end": "1179010"
  },
  {
    "text": "that I had presented? Yeah, there should be some\nresults in the actual paper, they're just not here,\nbut you can take a look.",
    "start": "1179010",
    "end": "1185730"
  },
  {
    "text": "Yeah. Yeah, question? --of how the model was\ntrained without doing gradient update by just thinking\nZero-Shot and [INAUDIBLE]",
    "start": "1185730",
    "end": "1192870"
  },
  {
    "text": "Intuition about how\nthe model is learning without gradient updates. Yeah, so this is\nrelated to the question asked earlier about how is\nthis actually happening.",
    "start": "1192870",
    "end": "1201240"
  },
  {
    "text": "That is-- yeah, again, it's\nan active area of research. So my understanding\nof the literature",
    "start": "1201240",
    "end": "1206247"
  },
  {
    "text": "is something like, you can show\nthat models are kind of almost doing in-context gradient\ndescent as it's encoding",
    "start": "1206247",
    "end": "1211799"
  },
  {
    "text": "a prompt, and you can\nanalyze this with like model interpretability experiments. But I-- yeah, I'm\nhappy to suggest papers",
    "start": "1211800",
    "end": "1219030"
  },
  {
    "text": "afterwards that kind of\ndeal with this problem more carefully. ",
    "start": "1219030",
    "end": "1226679"
  },
  {
    "text": "Cool. OK, so a follow up work to\nthis asks the question of,",
    "start": "1226680",
    "end": "1234419"
  },
  {
    "text": "do we actually even need\nexamples of reasoning? Do we actually need\nto collect humans working through these problems?",
    "start": "1234420",
    "end": "1240300"
  },
  {
    "text": "Can we actually just ask the\nmodel to reason through things? Just ask it nicely, right?",
    "start": "1240300",
    "end": "1245610"
  },
  {
    "text": "So this introduced\nthis idea called Zero-Shot chain-of-thought\nprompting. And it was honestly, I think,\nprobably the highest impact",
    "start": "1245610",
    "end": "1253290"
  },
  {
    "text": "to simple idea ratio\nI've seen in a paper. Where it's like, the\nsimplest possible thing where instead of doing this\nchain-of-thought stuff,",
    "start": "1253290",
    "end": "1259290"
  },
  {
    "text": "you just ask the question. And then the answer, you\nfirst prepend the token, let's think step by step.",
    "start": "1259290",
    "end": "1266550"
  },
  {
    "text": "And the model will\ndecode as if it had said, let's think step by\nstep, and it will work through some reasoning\nand produce the right answer.",
    "start": "1266550",
    "end": "1272375"
  },
  {
    "text": " So does this work on some\narithmetic benchmarks?",
    "start": "1272375",
    "end": "1277860"
  },
  {
    "text": "Here's what happens when\nyou prompt the model, just Zero-Shot, so just asking it\nto produce the answer right away without any reasoning.",
    "start": "1277860",
    "end": "1284280"
  },
  {
    "text": "Few-Shots, so giving some\nexamples of inputs and outputs. And this is Zero-Shot\nchain-of-thought,",
    "start": "1284280",
    "end": "1289390"
  },
  {
    "text": "so just asking the model\nto think through things, you get crazy good accuracy.",
    "start": "1289390",
    "end": "1295330"
  },
  {
    "text": "When we compare it to actually\ndoing manual chain-of-thought, you still do better with\nmanual chains-of-thought,",
    "start": "1295330",
    "end": "1300423"
  },
  {
    "text": "but that just goes to show\nyou how simple of an idea this is, and ends up producing\nimproved performance numbers.",
    "start": "1300423",
    "end": "1305695"
  },
  {
    "text": " So the funny part of\nthis paper was, why use",
    "start": "1305695",
    "end": "1311259"
  },
  {
    "text": "let's think by step by step? They used actually a lot of\nprompts and tried them out. So here, Zero-Shot\nbaseline performance.",
    "start": "1311260",
    "end": "1317260"
  },
  {
    "text": "They tried out a bunch of\ndifferent prefixes, the answers after the proof, let's\nthink about this logically.",
    "start": "1317260",
    "end": "1323470"
  },
  {
    "text": "And they found that let's think\nstep by step was the best one. It turns out, this was\nactually built upon--",
    "start": "1323470",
    "end": "1328960"
  },
  {
    "text": "later in the year, where\nthey actually used a language model to search through the\nbest possible strings that",
    "start": "1328960",
    "end": "1334180"
  },
  {
    "text": "would maximize performance on\nthis task, which is probably like gross overfitting,\nbut the best prompt they found was let's work this\nout step by step in a step",
    "start": "1334180",
    "end": "1342580"
  },
  {
    "text": "by step way to be sure that\nwe have the right answer. So the right answer\nthing is kind of presuming that you get\nthe answer right, right?",
    "start": "1342580",
    "end": "1348533"
  },
  {
    "text": "It's like giving the model\nsome confidence in itself. OK. So this might seem to you\nlike a total dark arcane art,",
    "start": "1348533",
    "end": "1356628"
  },
  {
    "text": "and that's because it is. Like, we really\nhave no intuition as to what's going\non here, or we're",
    "start": "1356628",
    "end": "1361780"
  },
  {
    "text": "trying to build some intuition. But as a result, and\nI'm sure you've seen-- if you spend time\nin tech circles",
    "start": "1361780",
    "end": "1368060"
  },
  {
    "text": "or you've seen on\nthe internet, there's this whole new idea of prompt\nengineering being an emerging science and profession.",
    "start": "1368060",
    "end": "1373620"
  },
  {
    "text": "So this includes things like\nasking a model for reasoning, it includes jailbreaking\nlanguage models, so telling them to do\nthings that they otherwise",
    "start": "1373620",
    "end": "1381410"
  },
  {
    "text": "aren't trained to do. Even AI art, like, DALL-e\nor stable diffusion, this idea of constructing\nthese really complex",
    "start": "1381410",
    "end": "1388850"
  },
  {
    "text": "prompts to get model\noutputs that you want, that's also prompting. Anecdotally, I've\nheard of people saying,",
    "start": "1388850",
    "end": "1394458"
  },
  {
    "text": "I'm going to use a\ncode generation model, but I'm going to include the\nGoogle code header in first because that will make more\nprofessional or bug free code,",
    "start": "1394458",
    "end": "1401120"
  },
  {
    "text": "depending on how much you\nbelieve in Google, but, yeah. And there's a Wikipedia\narticle on this now,",
    "start": "1401120",
    "end": "1407390"
  },
  {
    "text": "and there's even\nstartups that are hiring for prompt engineers,\nand they pay quite well. So if you want to be\na prompt engineer, definitely practice your\nGPT whispering skills.",
    "start": "1407390",
    "end": "1415610"
  },
  {
    "text": "[CHUCKLES] I have a question. You have a question?",
    "start": "1415610",
    "end": "1422250"
  },
  {
    "text": "Sorry, I'm-- yes, you go-- yeah, go ahead. A few slides ago you said, a LM\ndesign that was like this long,",
    "start": "1422250",
    "end": "1430289"
  },
  {
    "text": "how can you get LM to\ndesign an input like-- I think they treated it a\nreinforcement learning problem,",
    "start": "1430290",
    "end": "1436900"
  },
  {
    "text": "but I'll just direct\nyou to this paper at the bottom to\nlearn more details. Yeah, it's the Zhou\net Al 2022 paper.",
    "start": "1436900",
    "end": "1443220"
  },
  {
    "text": "Yeah. Yeah, question? So I'm just a bit curious about\nhow they provided feedback.",
    "start": "1443220",
    "end": "1450660"
  },
  {
    "text": "So in case the model was\nnot given the right answer, were there prompts to say\nthat that's not right?",
    "start": "1450660",
    "end": "1457270"
  },
  {
    "text": "Maybe think about this\ndifferent approach. How was feedback provided? They don't think about\nfeedback in this kind",
    "start": "1457270",
    "end": "1463470"
  },
  {
    "text": "of chain-of-thought\nprompting experiments. They just-- if the model\ngets the answer wrong, then it gets the answer\nwrong, and we just evaluate accuracy, right?",
    "start": "1463470",
    "end": "1469170"
  },
  {
    "text": "But this idea of incorporating\nAI feedback, I think, is quite interesting. And, I think, you'll see\nsome, maybe, hints of--",
    "start": "1469170",
    "end": "1475648"
  },
  {
    "text": "or discussion of that later on. Yeah. Questions? ",
    "start": "1475648",
    "end": "1484039"
  },
  {
    "text": "OK, awesome.  OK so talking about\nthese three things,",
    "start": "1484040",
    "end": "1490920"
  },
  {
    "text": "I'm going to talk about kind\nof the benefits and limitations of the various different things\nthat we could be doing here. So for Zero-Shot and\nFew-Shot In-Context Learning,",
    "start": "1490920",
    "end": "1498850"
  },
  {
    "text": "the benefit is you don't\nneed any fine tuning, and you can kind of carefully\nconstruct your prompts to hopefully get\nbetter performance.",
    "start": "1498850",
    "end": "1505899"
  },
  {
    "text": "The downsides are there\nare limits to what you can fit in context, right? Transformers have a\nfixed context window",
    "start": "1505900",
    "end": "1510940"
  },
  {
    "text": "of say 1,000 or a\nfew thousand tokens. And, I think, as you\nwill probably find out,",
    "start": "1510940",
    "end": "1516039"
  },
  {
    "text": "for really complex\ntasks, you are indeed going to need some\ngradient steps. So you're going to need\nsome sort of fine tuning.",
    "start": "1516040",
    "end": "1522914"
  },
  {
    "text": "But that brings us to the\nnext part of the lecture. So that's Instruction\nFinetuning. ",
    "start": "1522915",
    "end": "1529759"
  },
  {
    "text": "OK. So the idea of\ninstruction finetuning is that, sure, these models are\npretty good at doing prompting,",
    "start": "1529760",
    "end": "1535298"
  },
  {
    "text": "you can get them to do\nreally interesting things, but there is still\na problem, which is that language models are\ntrained to predict the most",
    "start": "1535298",
    "end": "1541840"
  },
  {
    "text": "likely continuation of\ntokens, and that is not the same as what we want\nlanguage models to do, which is to assist people.",
    "start": "1541840",
    "end": "1548299"
  },
  {
    "text": "So as an example, if I give\nGPT-3 this kind of prompt, explain the moon landing,\nGPT-3 is trained to predict,",
    "start": "1548300",
    "end": "1554860"
  },
  {
    "text": "if I saw this on the\ninternet somewhere, what is the most\nlikely continuation? Well, maybe someone was coming\nup with a list of things",
    "start": "1554860",
    "end": "1560740"
  },
  {
    "text": "to do with a six-year-old. So it's just predicting\na list of other tasks, it's not answering\nyour question.",
    "start": "1560740",
    "end": "1566680"
  },
  {
    "text": "And so the issue here is that\nlanguage models are not-- the term is, aligned\nwith user intent.",
    "start": "1566680",
    "end": "1572890"
  },
  {
    "text": "So how might we\nbetter align models with user intent for this case?",
    "start": "1572890",
    "end": "1578049"
  },
  {
    "text": "Well, super simple\nanswer, right? We're machine learners,\nlet's do machine learning. So we're going to collect--\nyou'd ask a human, give me",
    "start": "1578050",
    "end": "1585490"
  },
  {
    "text": "the right answer, right? Give me the way that a\nlanguage model should respond according to this prompt. And let's just do\nfinetuning, OK.",
    "start": "1585490",
    "end": "1593750"
  },
  {
    "text": " So this is a slide from\nthe pre-training lecture.",
    "start": "1593750",
    "end": "1599500"
  },
  {
    "text": "Again, pre-training can\nimprove NLP applications by serving as parameter\ninitialization.",
    "start": "1599500",
    "end": "1605630"
  },
  {
    "text": "So this kind of pipeline, I\nthink, you are familiar with. And the difference\nhere is that instead",
    "start": "1605630",
    "end": "1611440"
  },
  {
    "text": "of finetuning on a single\ndownstream task of interest, like, sentiment analysis,\nwhat we're going to do is we're going to\nfinetune on many tasks.",
    "start": "1611440",
    "end": "1618350"
  },
  {
    "text": "So we have a lot of\ntasks, and the hope is that we can then generalize\nto other unseen tasks at task time.",
    "start": "1618350",
    "end": "1625370"
  },
  {
    "text": "So as you might expect, data and\nscale is kind of key for this to work.",
    "start": "1625370",
    "end": "1630560"
  },
  {
    "text": "So we're going to collect\na bunch of examples of instruction, output,\npairs across many tasks",
    "start": "1630560",
    "end": "1636070"
  },
  {
    "text": "and then finetune\nour language model, and then evaluate\ngeneralization to unseen tasks. ",
    "start": "1636070",
    "end": "1645070"
  },
  {
    "text": "Yeah, so data and\nscale is important. So as an example, one recent\ndata set that was published for this is called the\nSuper-NaturalInstructions data",
    "start": "1645070",
    "end": "1652060"
  },
  {
    "text": "set, it contains\nover 1.600 tasks, containing 3 million examples.",
    "start": "1652060",
    "end": "1657830"
  },
  {
    "text": "So this includes translation,\nquestion answering, question generation, even\ncoding, mathematical reasoning,",
    "start": "1657830",
    "end": "1663730"
  },
  {
    "text": "et cetera. And when you look at this,\nyou really begin to think, well, is this\nactually finetuning",
    "start": "1663730",
    "end": "1669525"
  },
  {
    "text": "or is this just\nmore pre-training? And it's actually both, right? We're kind of blurring\nthe lines here, where the amount of scale\nthat we're training this",
    "start": "1669525",
    "end": "1676583"
  },
  {
    "text": "on, basically, it is kind of\na still general, but slightly more specific than\nlanguage modeling",
    "start": "1676583",
    "end": "1681880"
  },
  {
    "text": "type of pre-training task. OK. So one question I have is, now\nthat we are training our model",
    "start": "1681880",
    "end": "1689150"
  },
  {
    "text": "on so many tasks, how do we\nevaluate such a model, right? Because you can't\nreally say, OK, can you now do\nsentiment analysis well?",
    "start": "1689150",
    "end": "1695935"
  },
  {
    "text": "Like, the scale\nof tasks that you want to evaluate this language\nmodel on is much greater.",
    "start": "1695935",
    "end": "1701970"
  },
  {
    "text": "So just as a brief\naside, a lot of research has been going into\nbuilding up these benchmarks",
    "start": "1701970",
    "end": "1707750"
  },
  {
    "text": "for these massive\nmultitask language models, and seeing to what\ndegree they can do, not only just one task, but\njust a variety of tasks.",
    "start": "1707750",
    "end": "1715139"
  },
  {
    "text": "So this is the massive\nmultitask language understanding benchmark or MMLU. It consists of a\nbunch of benchmarks",
    "start": "1715140",
    "end": "1721490"
  },
  {
    "text": "for measuring language\nmodel performance on a bunch of knowledge\nintensive tasks that you would expect a\nhigh school or college",
    "start": "1721490",
    "end": "1727190"
  },
  {
    "text": "student to complete. So you're testing\na language model, not only on sentiment analysis,\nbut on astronomy, and logic,",
    "start": "1727190",
    "end": "1735140"
  },
  {
    "text": "and European history. And here are some numbers\nwhere, at the time, GPT-3 is like not that\ngood, but it's certainly",
    "start": "1735140",
    "end": "1741860"
  },
  {
    "text": "above a random baseline\non all of these tasks. ",
    "start": "1741860",
    "end": "1747060"
  },
  {
    "text": "Here's another example. So this is the beyond the\nImitation Game benchmark, or Big-Bench. This has like a billion\nauthors because it",
    "start": "1747060",
    "end": "1753290"
  },
  {
    "text": "was a huge collaborative\neffort, and this is a word cloud of the\ntasks that were evaluated.",
    "start": "1753290",
    "end": "1759720"
  },
  {
    "text": "And it really contains\nsome very esoteric tasks. So this is an example\nof one task included,",
    "start": "1759720",
    "end": "1765080"
  },
  {
    "text": "where you have to-- given\nKanji or Japanese character in ASCII Art, you\nneed to predict the meaning of the\ncharacter, right?",
    "start": "1765080",
    "end": "1771020"
  },
  {
    "text": "So we're really stress\ntesting these language models. ",
    "start": "1771020",
    "end": "1776890"
  },
  {
    "text": "OK. So instruction\nfinetuning, does it work? Recall the-- there's a\nT5 encoder-decoder model.",
    "start": "1776890",
    "end": "1784780"
  },
  {
    "text": "So this is kind of Google's\nencoder-decoder model, where it's pre-trained on\nthis span corruption task. So if you don't\nremember that, you",
    "start": "1784780",
    "end": "1790807"
  },
  {
    "text": "can refer back to that lecture. But the authors released a\nnewer version called FLAN-T5,",
    "start": "1790807",
    "end": "1796080"
  },
  {
    "text": "so FLAN stands for\nfinetuning language models. And this is T5 models trained\non an additional 1.800 tasks,",
    "start": "1796080",
    "end": "1801990"
  },
  {
    "text": "which include the natural\ninstructions data set that I just mentioned. And if we average across\nboth the Big-Bench and MMLU",
    "start": "1801990",
    "end": "1809070"
  },
  {
    "text": "performance and\nnormalize it, what we see is that instruction\nfinetuning works. And crucially, the\nbigger the model,",
    "start": "1809070",
    "end": "1816240"
  },
  {
    "text": "the bigger the benefit that\nyou get from doing instruction finetuning, right? So it's really the\nlarge models that stand",
    "start": "1816240",
    "end": "1821430"
  },
  {
    "text": "to do well from finetuning. And you might look\nat this and say,",
    "start": "1821430",
    "end": "1826570"
  },
  {
    "text": "this is kind of sad\nfor academics or anyone without a massive\nGPU cluster, right? It's like, who can run an\n11 billion parameter model?",
    "start": "1826570",
    "end": "1833345"
  },
  {
    "text": "I guess the one silver lining,\nif you look at the results here, are the 80\nmillion model, which",
    "start": "1833345",
    "end": "1838360"
  },
  {
    "text": "is the smallest one, if you\nlook at after finetuning, it ends up performing about\nas well as the un-finetuned 11",
    "start": "1838360",
    "end": "1844178"
  },
  {
    "text": "billion parameter model, right? So there's a lot of examples\nin the literature about smaller",
    "start": "1844178",
    "end": "1849610"
  },
  {
    "text": "instruction finetuned\npre-trained models outperforming larger\nmodels that are many, many more times the size, right?",
    "start": "1849610",
    "end": "1855549"
  },
  {
    "text": "So hopefully, there's still\nsome hope for people with just like a few GPUs. ",
    "start": "1855550",
    "end": "1861560"
  },
  {
    "text": "Any questions? Awesome. In order to really\nunderstand the capabilities,",
    "start": "1861560",
    "end": "1867430"
  },
  {
    "text": "I highly recommend that you\njust try it out yourself. So FLAN T-5 is hosted\non Hugging Face. I think, Hugging Face has\na demo where you can just",
    "start": "1867430",
    "end": "1874540"
  },
  {
    "text": "type in a little query,\nask it to do anything, see what it does, but there\nare qualitative examples",
    "start": "1874540",
    "end": "1880750"
  },
  {
    "text": "of this working. So for questions where a\nnon-instruction finetuned model will just kind of waffle on\nand not answer the question,",
    "start": "1880750",
    "end": "1887440"
  },
  {
    "text": "doing instruction finetuning\nwill get your model to much more accurately\nreason through things and give you the right answer.",
    "start": "1887440",
    "end": "1892950"
  },
  {
    "text": " OK.",
    "start": "1892950",
    "end": "1898630"
  },
  {
    "text": "So that was\ninstruction finetuning. Positives of this\nmethod, super simple,",
    "start": "1898630",
    "end": "1904450"
  },
  {
    "text": "super straightforward, it's\njust doing finetuning, right? And you see this\nreally cool ability to generalize to unseen tasks.",
    "start": "1904450",
    "end": "1912580"
  },
  {
    "text": "In terms of\nnegatives, does anyone have any ideas for why\nthere might be downsides",
    "start": "1912580",
    "end": "1918130"
  },
  {
    "text": "of instruction finetuning? anyone want to comment? ",
    "start": "1918130",
    "end": "1924120"
  },
  {
    "text": "Yeah. It seems like it suffers\nfrom the same negatives of any human source data.",
    "start": "1924120",
    "end": "1931039"
  },
  {
    "text": "Yeah. Looks more like it's hard to\nget people to bite the input. You don't know, like, different\npeople think different inputs",
    "start": "1931040",
    "end": "1938260"
  },
  {
    "text": "about it. Yeah. Stuff like that. Yeah. Yeah, exactly. So comments are, well, it's\nhard and annoying to get",
    "start": "1938260",
    "end": "1944980"
  },
  {
    "text": "human labels, and\nit's expensive. That's something that\ndefinitely matters. And that last part you mentioned\nabout there might be-- humans",
    "start": "1944980",
    "end": "1951340"
  },
  {
    "text": "might disagree on what\nthe right label is, yeah, that's increasingly a problem. Yeah. So what are the limitations?",
    "start": "1951340",
    "end": "1958059"
  },
  {
    "text": "The obvious limitation is money. Collecting ground truth\ndata for so many tasks",
    "start": "1958060",
    "end": "1963070"
  },
  {
    "text": "costs a lot of money. Subtler limitations include the\none that you were mentioning. So as we begin to ask for\nmore creative and open ended",
    "start": "1963070",
    "end": "1971260"
  },
  {
    "text": "tasks from our models,\nthere are tasks where there is no right answer,\nand it's a little bit weird to say, this is an example of\nhow to write some story, right?",
    "start": "1971260",
    "end": "1979990"
  },
  {
    "text": "So write me a story about a\ndog and our pet grasshopper. Like, there is not\none answer to this, but if we were to only\nto collect one or two",
    "start": "1979990",
    "end": "1986470"
  },
  {
    "text": "demonstrations, the language\nmodeling objective would say, you should put all of\nyour probability mass",
    "start": "1986470",
    "end": "1991790"
  },
  {
    "text": "on the two ways that two humans\nwrote this answer, right? When in reality,\nthere's no right answer.",
    "start": "1991790",
    "end": "1999500"
  },
  {
    "text": "Another problem, which is\nrelated kind of fundamentally to language modeling\nin the first place, is that language\nmodeling as an objective,",
    "start": "1999500",
    "end": "2005770"
  },
  {
    "text": "penalizes all token\nlevel mistakes equally. So what I mean by that is,\nif you were asking a language",
    "start": "2005770",
    "end": "2011740"
  },
  {
    "text": "model, for example, to\npredict the sentence, Avatar is a fantasy TV show. And you were asking\nit, and let's imagine",
    "start": "2011740",
    "end": "2019180"
  },
  {
    "text": "that the LM mispredicted\nadventure instead of fantasy, right? So adventure is a mistake,\nit's not the right word,",
    "start": "2019180",
    "end": "2026650"
  },
  {
    "text": "but it is equally as bad as\nif the model were to predict something like musical, right?",
    "start": "2026650",
    "end": "2033010"
  },
  {
    "text": "But the problem is that\nAvatar is an adventure TV show is still true, right? So it's not necessarily\na bad thing.",
    "start": "2033010",
    "end": "2038440"
  },
  {
    "text": "Whereas, Avatar is a\nmusical is just false. So under the language\nmodeling objective,",
    "start": "2038440",
    "end": "2044308"
  },
  {
    "text": "if the model were\nequally confidence, you would pay an equal\npenalty, an equal loss penalty, for predicting either\nof those tokens wrong.",
    "start": "2044308",
    "end": "2050530"
  },
  {
    "text": "But it's clear that\nthis objective is not actually aligned\nwith what users want,",
    "start": "2050530",
    "end": "2055580"
  },
  {
    "text": "which is maybe truth for\ncreativity, or generally, just this idea of human preferences.",
    "start": "2055580",
    "end": "2061820"
  },
  {
    "text": "Yeah, question? Can we do something like\nmultiply the penalty by like the distance from\nthe word embedding in order",
    "start": "2061820",
    "end": "2069500"
  },
  {
    "text": "to reduce this, because musical\nwould have a higher distance away than adventure.",
    "start": "2069500",
    "end": "2075169"
  },
  {
    "text": "Yeah, that's an\ninteresting question. It's an interesting idea. I haven't heard of people doing\nthat, but it seems plausible.",
    "start": "2075170",
    "end": "2082965"
  },
  {
    "text": "I guess, one issue\nis you might come up with adversarial settings\nwhere maybe the word embedding distance is also not telling\nyou the right thing, right?",
    "start": "2082965",
    "end": "2090199"
  },
  {
    "text": "So for example, show\nand musical maybe are very close together\nbecause they're both shows or things to\nwatch, but they are in false,",
    "start": "2090199",
    "end": "2097700"
  },
  {
    "text": "in veracity, they're completely\ndifferent, one is true, one is false. So, yeah, you can\ntry it, although I",
    "start": "2097700",
    "end": "2103549"
  },
  {
    "text": "think there might be some\ntricky edge cases like that. ",
    "start": "2103550",
    "end": "2109410"
  },
  {
    "text": "Cool. OK. So in the next part\nof the talk, we're going to actually explicitly try\nto satisfy human preferences,",
    "start": "2109410",
    "end": "2116148"
  },
  {
    "text": "and come up with a mathematical\nframework for doing so. ",
    "start": "2116148",
    "end": "2122210"
  },
  {
    "text": "And, yeah, so these\nare the limitations as I just mentioned. So this is where we\nget into reinforcement",
    "start": "2122210",
    "end": "2127760"
  },
  {
    "text": "learning from human feedback. ",
    "start": "2127760",
    "end": "2132840"
  },
  {
    "text": "OK. So RLHF. So let's say we were training\na language model on some task,",
    "start": "2132840",
    "end": "2139740"
  },
  {
    "text": "like, summarization. And let's imagine that for\neach language model sample, S,",
    "start": "2139740",
    "end": "2145020"
  },
  {
    "text": "let's imagine that we\nhad a way to obtain a human reward of that summary.",
    "start": "2145020",
    "end": "2150090"
  },
  {
    "text": "So we could score this summary\nwith a reward function, which we'll call R of s, and the\nhigher the reward, the better.",
    "start": "2150090",
    "end": "2158710"
  },
  {
    "text": "So let's imagine we're\nsummarizing this article, and we have this summary,\nwhich maybe is pretty good.",
    "start": "2158710",
    "end": "2164670"
  },
  {
    "text": "Let's say we had\nanother summary, maybe it's a bit worse. And if we were able to ask\na human to just rate all",
    "start": "2164670",
    "end": "2171840"
  },
  {
    "text": "these outputs,\nthen the objective that we want to maximize\nor satisfy is very obvious. We just want to maximize the\nexpected reward of samples",
    "start": "2171840",
    "end": "2179310"
  },
  {
    "text": "from our language model, right? So in expectation,\nas we take samples from our language\nmodel, p theta,",
    "start": "2179310",
    "end": "2185670"
  },
  {
    "text": "we just want to maximize\nthe reward of those samples. Fairly straightforward.",
    "start": "2185670",
    "end": "2191256"
  },
  {
    "text": " Oh, and for\nmathematical simplicity, here I'm kind of\nassuming that there's",
    "start": "2191256",
    "end": "2197683"
  },
  {
    "text": "only one task or one prompt. So let's imagine\nwe were just trying to summarize like this article. But we can talk about\nhow to extend it to like",
    "start": "2197683",
    "end": "2203570"
  },
  {
    "text": "multiple prompts later on.  OK, so this kind of\ntask is the domain",
    "start": "2203570",
    "end": "2210110"
  },
  {
    "text": "of reinforcement learning. So I'm not going\nto presume there's any knowledge of\nreinforcement learning, although I'm sure some of you\nare quite familiar with it,",
    "start": "2210110",
    "end": "2216745"
  },
  {
    "text": "probably even more\nfamiliar than I am. But the field of\nreinforcement learning has studied these kinds of\nproblems, these optimization",
    "start": "2216745",
    "end": "2223100"
  },
  {
    "text": "problems, of how to\noptimize something while you're kind of simulating\nthe optimization for many years",
    "start": "2223100",
    "end": "2228320"
  },
  {
    "text": "now. And in 2013, there\nwas a resurgence of interest in\nreinforcement learning",
    "start": "2228320",
    "end": "2233540"
  },
  {
    "text": "for deep learning specifically. So you might have seen\nthese results from DeepMind about an agent learning to play\nAtari games, an agent mastering",
    "start": "2233540",
    "end": "2240860"
  },
  {
    "text": "Go, much earlier than expected. But interestingly, I\nthink, the interest",
    "start": "2240860",
    "end": "2246170"
  },
  {
    "text": "in applying reinforcement\nlearning to modern LMs is a bit newer on\nthe other hand. And I think the\nearliest success story,",
    "start": "2246170",
    "end": "2252417"
  },
  {
    "text": "or one of the earliest\nsuccess stories was only in 2019, for example. So why might this be the case?",
    "start": "2252417",
    "end": "2258410"
  },
  {
    "text": "There's a few reasons. I think in general, the\nfield had kind of this sense that reinforcement learning\nwith language models",
    "start": "2258410",
    "end": "2264440"
  },
  {
    "text": "was really hard to get right,\npartially, because language models are very complicated.",
    "start": "2264440",
    "end": "2269772"
  },
  {
    "text": "And if you think\nof language models as actors that have\nan action space where they can spit out any sentence,\nthat's a lot of sentences,",
    "start": "2269772",
    "end": "2276660"
  },
  {
    "text": "right? So it's like a very complex\nspace to explore in. So it still is a\nreally hard problem. So that's part of the reason.",
    "start": "2276660",
    "end": "2282480"
  },
  {
    "text": "But also practically,\nI think, there have been these\nnewer algorithms that seem to work much better for\ndeep neural models, including",
    "start": "2282480",
    "end": "2289770"
  },
  {
    "text": "language models, and\nthese include algorithms like proximal\npolicy optimization, but we won't get\ninto the details",
    "start": "2289770",
    "end": "2295260"
  },
  {
    "text": "of that for this course. But these are the\nreasons why we've been really interested\nin this idea of doing",
    "start": "2295260",
    "end": "2301595"
  },
  {
    "text": "RL with language models.  OK.",
    "start": "2301595",
    "end": "2307730"
  },
  {
    "text": "So how do we actually\nmaximize this objective? I've written it\ndown, and ideally, we should just change\nour parameters theta",
    "start": "2307730",
    "end": "2313885"
  },
  {
    "text": "so that reward is high, right? But it's not really\nclear how to do so. So when we think\nabout it, I mean,",
    "start": "2313885",
    "end": "2320240"
  },
  {
    "text": "what have we learned\nin the class thus far? We know that we can do gradient\ndescent or gradient ascent. So let's try doing\ngradient ascent.",
    "start": "2320240",
    "end": "2326395"
  },
  {
    "text": "We're going to maximize\nthis objective, so we're going to step in the\ndirection of steepest gradient. But this quickly\nbecomes a problem,",
    "start": "2326395",
    "end": "2332520"
  },
  {
    "text": "which is, what is this quantity\nand how do we evaluate it? How do we estimate\nthis expectation",
    "start": "2332520",
    "end": "2338630"
  },
  {
    "text": "given that the variables of\nthe gradient that we're taking, theta, appear in the\nsample of the expectation.",
    "start": "2338630",
    "end": "2345980"
  },
  {
    "text": "And the second is, what if\nour reward function is not differentiable? Like, human judgments\nare not differentiable,",
    "start": "2345980",
    "end": "2351380"
  },
  {
    "text": "we can't backprop through them. And so we need this to be able\nto work with a Black box reward function. ",
    "start": "2351380",
    "end": "2358607"
  },
  {
    "text": "So there's a class of methods\nin reinforcement learning called policy gradient methods that\ngives us tools for estimating",
    "start": "2358607",
    "end": "2364640"
  },
  {
    "text": "and optimizing this objective. And for the purposes\nof this course, I'm going to try to\ndescribe kind of the highest",
    "start": "2364640",
    "end": "2372110"
  },
  {
    "text": "level possible\nintuition for this, which kind of looks at the math\nand shows what's going on here.",
    "start": "2372110",
    "end": "2377940"
  },
  {
    "text": "But it is going to emit\na lot of the details, and a full treatment of\nreinforcement learning is definitely outside of\nthe scope of this course.",
    "start": "2377940",
    "end": "2384390"
  },
  {
    "text": "So if you're more interested\nin this kind of content, you should check out\nCS234 reinforcement learning, for example.",
    "start": "2384390",
    "end": "2390958"
  },
  {
    "text": "And in general, I think, this\nis going to get a little mathy, but it's totally fine if\nyou don't understand it, we will talk, we will\nregroup at the end",
    "start": "2390958",
    "end": "2397487"
  },
  {
    "text": "and just show like what this\nmeans for how to do RLHF.",
    "start": "2397487",
    "end": "2402890"
  },
  {
    "text": "But what I'm going to\ndo is just describe how we actually\nestimate this objective. So we want to obtain\nthis gradient,",
    "start": "2402890",
    "end": "2409340"
  },
  {
    "text": "so it's the gradient\nof the expectation of the reward of samples\nfrom our language model.",
    "start": "2409340",
    "end": "2414990"
  },
  {
    "text": "And if we do the math,\nif we break this apart, this is our definition of\nwhat an expectation is, right? We're going to sum over\nall sentences weighted",
    "start": "2414990",
    "end": "2421680"
  },
  {
    "text": "by the probability. And due to the linearity\nof the gradient, we can put the gradient\noperator inside of the sum.",
    "start": "2421680",
    "end": "2428010"
  },
  {
    "text": " Now what we're\ngoing to do is we're going to use a very\nhandy trick known",
    "start": "2428010",
    "end": "2434370"
  },
  {
    "text": "as a log derivative trick. And this is called a trick,\nbut it's really just the chain rule. But let's just see\nwhat happens when",
    "start": "2434370",
    "end": "2440250"
  },
  {
    "text": "we take the gradient of the\nlog probability of a sample from our language model.",
    "start": "2440250",
    "end": "2445960"
  },
  {
    "text": "So if I take the gradient, then\nhow do we use the chain rule? So the gradient of\nthe log of something is going to be 1 over that\nsomething times the gradient",
    "start": "2445960",
    "end": "2453119"
  },
  {
    "text": "of the middle of that something,\nso 1 over P theta of s times the gradient. And if we rearrange, we see\nthat we can alternatively",
    "start": "2453120",
    "end": "2460350"
  },
  {
    "text": "write the gradient\nof P theta of s as this product, so P theta of s\ntimes the gradient of the log P",
    "start": "2460350",
    "end": "2466140"
  },
  {
    "text": "theta of s. And we can plug this back in. ",
    "start": "2466140",
    "end": "2471922"
  },
  {
    "text": "And the reason why\nwe're doing this is because we're going to\nconvert this into a form where the expectation\nis easy to estimate.",
    "start": "2471922",
    "end": "2478730"
  },
  {
    "text": "So we plug it back in,\nthat gives us this, and if you squint quite closely\nat this last equation here,",
    "start": "2478730",
    "end": "2485650"
  },
  {
    "text": "this first part here,\nis the definition of an expectation, right? We are summing over a bunch\nof samples from our model,",
    "start": "2485650",
    "end": "2492430"
  },
  {
    "text": "and we are weighing\nit by the probability of that sample, which means\nthat we can rewrite it as an expectation. And in particular, it's an\nexpectation of this quantity",
    "start": "2492430",
    "end": "2500110"
  },
  {
    "text": "here. So let's just rewrite it. And this gives us our kind of\nnewer form of this objective.",
    "start": "2500110",
    "end": "2506840"
  },
  {
    "text": "So these two are equivalent\nthe top here and the bottom. And what has happened\nhere is we've",
    "start": "2506840",
    "end": "2512080"
  },
  {
    "text": "kind of shoved the gradient\ninside of the expectation, if that makes sense. So why is this useful?",
    "start": "2512080",
    "end": "2518742"
  },
  {
    "text": "Or does anyone have\nany questions on this before I move on? If you don't understand\nit that's fine as well, because it doesn't--",
    "start": "2518742",
    "end": "2524830"
  },
  {
    "text": "I mean, we will understand\nthe intuition behind it later. ",
    "start": "2524830",
    "end": "2531140"
  },
  {
    "text": "OK. OK. So we've converted\nthis into this,",
    "start": "2531140",
    "end": "2537420"
  },
  {
    "text": "and we've put the gradient\ninside the expectation, which means we can now approximate\nthis objective with Monte Carlo",
    "start": "2537420",
    "end": "2542790"
  },
  {
    "text": "samples. So the way to approximate\nany expectation is to just take a bunch of\nsamples and then average them.",
    "start": "2542790",
    "end": "2549240"
  },
  {
    "text": "So approximately, this\nis equal to sampling a finite number of\nsamples from our model, and then summing up the average\nof the reward times the log",
    "start": "2549240",
    "end": "2557700"
  },
  {
    "text": "probability, the gradient of the\nlog probability of that sample. And that gives us\nthis update rule,",
    "start": "2557700",
    "end": "2565347"
  },
  {
    "text": "plugging it back in for\nthat gradient ascent step that we wanted. So what is this?",
    "start": "2565347",
    "end": "2571770"
  },
  {
    "text": "What does this mean? Let's think about\na very simple case, imagine the reward\nwas a binary reward.",
    "start": "2571770",
    "end": "2579370"
  },
  {
    "text": "So it was either 0 or 1. So for example, imagine we\nwere trying to train a language model to talk about cats. So whenever it utters a\nsentence with a word cat,",
    "start": "2579370",
    "end": "2586020"
  },
  {
    "text": "we give it a one\nreward, otherwise, we give it a 0 reward. OK. Now if our reward is\nbinary, does anyone",
    "start": "2586020",
    "end": "2592289"
  },
  {
    "text": "know what this objective kind\nof reduces to or look like?",
    "start": "2592290",
    "end": "2597500"
  },
  {
    "text": "Any ideas? ",
    "start": "2597500",
    "end": "2603240"
  },
  {
    "text": "I've lost everyone,\nthat's fine too. [LAUGHTER] Yeah. So the reward would just be\nlike an indicator function.",
    "start": "2603240",
    "end": "2610260"
  },
  {
    "text": "Yeah, that's right.  So basically, to answer, the\nreward would be 0 everywhere,",
    "start": "2610260",
    "end": "2620240"
  },
  {
    "text": "except for sentences that\ncontain the word cat, right? And in that case, it would be 1. So basically, that\nwould just look",
    "start": "2620240",
    "end": "2626480"
  },
  {
    "text": "like kind of vanilla\ngradient descent, just on sentences that\ncontain the word cat.",
    "start": "2626480",
    "end": "2632360"
  },
  {
    "text": "So kind of to generalize\nthis to the more general case where the reward is scalar,\nwhat this is looking like,",
    "start": "2632360",
    "end": "2638630"
  },
  {
    "text": "if you look at it, is if R\nis very high, very positive, then we're multiplying the\ngradient of that sample",
    "start": "2638630",
    "end": "2645200"
  },
  {
    "text": "by a large number. And so our objective will\ntry to take gradient steps in the direction of maximizing\nthe probability of producing",
    "start": "2645200",
    "end": "2651440"
  },
  {
    "text": "that sample again, producing the\nsample that led to high reward. And on the other hand, if\nR is low or even negative,",
    "start": "2651440",
    "end": "2658220"
  },
  {
    "text": "then we will actively take steps\nto minimize the probability of that happening again. And that's like the\nEnglish intuition",
    "start": "2658220",
    "end": "2664040"
  },
  {
    "text": "of what's going on here. The reason why we call\nit reinforcement learning is because we want to\nreinforce good actions",
    "start": "2664040",
    "end": "2669770"
  },
  {
    "text": "and increase the probability\nthat they happen again in the future. And hopefully, this intuitively\nmakes sense to all of you.",
    "start": "2669770",
    "end": "2675420"
  },
  {
    "text": "Let's say you're playing a\nvideo game and on one run you get a super high score\nand you think to yourself, oh, that was really good,\nlike, whatever I did that time,",
    "start": "2675420",
    "end": "2682485"
  },
  {
    "text": "I should do again in the future. This is what we're\ntrying to capture with this kind of update.",
    "start": "2682485",
    "end": "2687930"
  },
  {
    "text": "Question? Is there any reason that we\nuse policy gradient and not like value iteration\nor other methods?",
    "start": "2687930",
    "end": "2695130"
  },
  {
    "text": " Yeah. You can do a lot of things. I think, there have\nbeen methods for doing q-learning, offline learning,\net cetera with language models.",
    "start": "2695130",
    "end": "2703140"
  },
  {
    "text": "I think, the design space\nhas been very underexplored, so there's a lot of low\nhanging fruit out there for people who are willing to\nthink about what fancy things",
    "start": "2703140",
    "end": "2710368"
  },
  {
    "text": "we can do in RL and apply them\nto this language modeling case. Yeah. ",
    "start": "2710368",
    "end": "2716752"
  },
  {
    "text": "And in practice, what we\nuse is not the simple thing, but we use a fancier thing,\nthat is proximal policy optimization.",
    "start": "2716752",
    "end": "2722110"
  },
  {
    "text": "Question? Do you know, like, the\nspace are like super big. Like, almost infinite. Yeah.",
    "start": "2722110",
    "end": "2727900"
  },
  {
    "text": "So that's the challenge. So one thing that I\nhaven't mentioned here is that right now I'm\ntalking about entire samples",
    "start": "2727900",
    "end": "2735040"
  },
  {
    "text": "of sentences, which is\nlike a massive space. In practice, when we\ndo RL, we actually do it the level of\ngenerating individual tokens.",
    "start": "2735040",
    "end": "2741280"
  },
  {
    "text": "So each token is-- let's say GPT has 50,000 tokens. So it's a pretty\nlarge action space,",
    "start": "2741280",
    "end": "2746890"
  },
  {
    "text": "but it's still manageable. But yeah. So that kind of answers\nthis question I was asking,",
    "start": "2746890",
    "end": "2753020"
  },
  {
    "text": "which is, can you see any\nproblems with this objective? Which is that this is a\nvery simplified objective, there is a lot more tricks\nneeded to make this work.",
    "start": "2753020",
    "end": "2760180"
  },
  {
    "text": "But hopefully,\nthis is giving you kind of the high level intuition\nas to what we're trying to do in the first place.",
    "start": "2760180",
    "end": "2765400"
  },
  {
    "text": " All right?",
    "start": "2765400",
    "end": "2771890"
  },
  {
    "text": "OK. So now we are set, right? We have a bunch of samples\nfrom a language model,",
    "start": "2771890",
    "end": "2777580"
  },
  {
    "text": "and for any arbitrary\nreward function, like, we're just asking a\nhuman to rate these samples, we can maximize that reward.",
    "start": "2777580",
    "end": "2785000"
  },
  {
    "text": "So we're done. OK, so not so fast,\nthere's a few problems. The first is the same as\nin the instruction fine",
    "start": "2785000",
    "end": "2791830"
  },
  {
    "text": "tuning case, which is that\nkeeping a human in the loop is expensive. Like, I don't really want to\nsupervise every single output",
    "start": "2791830",
    "end": "2797530"
  },
  {
    "text": "from a language model. I don't know if you all want to. So what can we do to fix this?",
    "start": "2797530",
    "end": "2804280"
  },
  {
    "text": "So one idea is,\ninstead of needing to ask humans for preferences\nevery single time, you can actually build a\nmodel of their preferences.",
    "start": "2804280",
    "end": "2811030"
  },
  {
    "text": "Like, literally, just train an\nNLP model of their preferences. So this idea was kind\nof first introduced",
    "start": "2811030",
    "end": "2817210"
  },
  {
    "text": "outside of language modeling\nby this paper, Knox and Stone. And they called it\nTAMER, but we're",
    "start": "2817210",
    "end": "2822609"
  },
  {
    "text": "going to see it\nre-implemented in this idea where we're going to\ntrain a language model. We'll call it a reward\nmodel, RM, just parameterized",
    "start": "2822610",
    "end": "2830079"
  },
  {
    "text": "by phi, to predict\nhuman preferences from an annotated data set. And then when doing\nRLHF, we're going",
    "start": "2830080",
    "end": "2836840"
  },
  {
    "text": "to optimize for the\nreward model rewards, instead of actual human rewards.",
    "start": "2836840",
    "end": "2841850"
  },
  {
    "text": "OK. Here's another\nconceptual problem. So here's a new sample for\nour summarization task.",
    "start": "2841850",
    "end": "2849680"
  },
  {
    "text": "What is the score\nof this sample? Anyone, give me a number. Does anyone want to\nrate this sample?",
    "start": "2849680",
    "end": "2856760"
  },
  {
    "text": "It's like a 3, 6, what scale\nare we using, et cetera?",
    "start": "2856760",
    "end": "2862300"
  },
  {
    "text": "So the issue here is\nthat human judgments can be noisy and miscalibrated\nwhen you ask people for things",
    "start": "2862300",
    "end": "2867630"
  },
  {
    "text": "alone, right? So one workaround for\nthis problem is instead",
    "start": "2867630",
    "end": "2873420"
  },
  {
    "text": "of asking for\ndirect ratings, ask humans to compare two summaries\nand judge which one is better.",
    "start": "2873420",
    "end": "2879240"
  },
  {
    "text": "This has been shown, I\nthink, in a variety of fields where people work with human\nsubjects and human responses",
    "start": "2879240",
    "end": "2884430"
  },
  {
    "text": "to be more reliable. This includes psychology\nand medicine, et cetera. So in other words,\ninstead of asking humans",
    "start": "2884430",
    "end": "2891360"
  },
  {
    "text": "to just give absolute\nscores, we're going to ask humans to\ncompare different samples, and rate which one is better.",
    "start": "2891360",
    "end": "2897730"
  },
  {
    "text": "So as an example,\nmaybe this first sample is better than\nthe middle sample, and it's better than\nthe last sample.",
    "start": "2897730",
    "end": "2905340"
  },
  {
    "text": "Now that we have these\npairwise comparisons, our reward model is going\nto generate latent scores.",
    "start": "2905340",
    "end": "2910720"
  },
  {
    "text": "So implicit scores based on\nthis pairwise comparison data. So our reward model\nis a language model",
    "start": "2910720",
    "end": "2916200"
  },
  {
    "text": "that takes in a possible\nsample, and then it's going to produce a number, which\nis the score, or the reward.",
    "start": "2916200",
    "end": "2923230"
  },
  {
    "text": "And the way that we're\ngoing to train this model, and again, you don't really need\nto know too much of the details here, but this is a classic\nkind of statistical comparison",
    "start": "2923230",
    "end": "2930820"
  },
  {
    "text": "model, is via the following\nloss where the reward model, essentially, should\njust predict a higher",
    "start": "2930820",
    "end": "2936850"
  },
  {
    "text": "score if a sample is judged to\nbe better than another sample. So in expectation, if we sample\nwinning samples and losing",
    "start": "2936850",
    "end": "2945100"
  },
  {
    "text": "samples from our data set, then\nif you look at this term here, the score of the higher\nsample should be higher",
    "start": "2945100",
    "end": "2953020"
  },
  {
    "text": "than the score of\nthe losing sample. Does that make sense?",
    "start": "2953020",
    "end": "2959190"
  },
  {
    "text": "And in doing so, by just\ntraining on this objective, you will get a\nlanguage model that will learn to assign numerical\nscores to things, which",
    "start": "2959190",
    "end": "2965510"
  },
  {
    "text": "indicate their relative\npreference over other samples, and we can use those\noutputs as rewards.",
    "start": "2965510",
    "end": "2972095"
  },
  {
    "text": " Yeah? Is there some\nnormalization either",
    "start": "2972095",
    "end": "2979160"
  },
  {
    "text": "in the output or somewhere\nelse, because it looks like-- Yeah, so, I don't remember if\nit happens during training,",
    "start": "2979160",
    "end": "2988002"
  },
  {
    "text": "but certainly, after\nyou've trained this model, you normalize the reward\nmodel so that the score is-- the expectation of the\nscores is 0, because that's",
    "start": "2988002",
    "end": "2994070"
  },
  {
    "text": "good for reinforcement learning\nand things like that as well. ",
    "start": "2994070",
    "end": "2999430"
  },
  {
    "text": "Yeah, question? [INAUDIBLE] the fact that even\nthough these are noisy, like,",
    "start": "2999430",
    "end": "3004770"
  },
  {
    "text": "they could be like-- some people\ncould view S3 as better than S1, or how do we account\nfor it even though, like,",
    "start": "3004770",
    "end": "3010800"
  },
  {
    "text": "when it's noisy, like, the\nbordering, the ordering, could still be-- Yeah. I think that's just\nkind of limitations",
    "start": "3010800",
    "end": "3017220"
  },
  {
    "text": "with asking for these\npreferences in the first place, is that humans will\ndisagree, right? So we really have\nno ground truth,",
    "start": "3017220",
    "end": "3023850"
  },
  {
    "text": "unless we maybe ask like\nan ensemble of humans, for example, that's just\na limitation with this. I think, hopefully, in the\nlimit with enough data,",
    "start": "3023850",
    "end": "3030790"
  },
  {
    "text": "this kind of noise washes out,\nbut it's certainly an issue. And this next slide will\nalso kind of touch on this.",
    "start": "3030790",
    "end": "3037960"
  },
  {
    "text": "So does the reward model work? Can we actually learn to model\nhuman preferences in this way? This is obviously an\nimportant standard check",
    "start": "3037960",
    "end": "3044190"
  },
  {
    "text": "before we actually try to\noptimize this objective. And they measured this. So this is kind of\nevaluating the reward",
    "start": "3044190",
    "end": "3051090"
  },
  {
    "text": "model on a standard\nkind of validation set. So can the reward model predict\noutcomes for data points",
    "start": "3051090",
    "end": "3056910"
  },
  {
    "text": "that they have not\nseen during training, and does it change based on\nmodel size or amount of data?",
    "start": "3056910",
    "end": "3063670"
  },
  {
    "text": "And if you noticed here,\nthere's one dash line, which is the human\nbaseline, which is, if you ask a human\nto predict the outcome,",
    "start": "3063670",
    "end": "3069730"
  },
  {
    "text": "a human does not get 100%\naccuracy because humans disagree, all right? And even in ensemble\nof, let's say,",
    "start": "3069730",
    "end": "3075250"
  },
  {
    "text": "five humans also doesn't\nget 100% accuracy because humans have\ndifferent preferences. But the key takeaway here\nis that for the largest",
    "start": "3075250",
    "end": "3083230"
  },
  {
    "text": "possible model, and\nfor enough data, a reward model, at least\nsome of the validation set that they used is kind of\napproaching the performance",
    "start": "3083230",
    "end": "3091180"
  },
  {
    "text": "of a single human person. And that's kind of a\ngreen light that maybe we can try this out and\nsee what happens.",
    "start": "3091180",
    "end": "3096835"
  },
  {
    "start": "3096835",
    "end": "3102109"
  },
  {
    "text": "OK. So if there are\nno questions, this is kind of the\ncomponents of RLHF.",
    "start": "3102110",
    "end": "3107390"
  },
  {
    "text": "So we have a\npre-trained model, maybe it's instruction\nfinetuned, which we're going to call p of PT.",
    "start": "3107390",
    "end": "3114490"
  },
  {
    "text": "We have a reward\nmodel, which produces scalar rewards for\nlanguage model outputs, and it is trained on a data\nset of human comparisons.",
    "start": "3114490",
    "end": "3122260"
  },
  {
    "text": "And we have a method,\npolicy gradient, for arbitrarily optimizing\nlanguage model parameters",
    "start": "3122260",
    "end": "3127420"
  },
  {
    "text": "towards some reward function. And so now, if you\nwant to do RLHF, you clone the pre-trained model.",
    "start": "3127420",
    "end": "3134500"
  },
  {
    "text": "We're going to call this a copy\nof the model, which is the RM model, with parameters\ntheta that we're actually",
    "start": "3134500",
    "end": "3139750"
  },
  {
    "text": "going to optimize. And we're going to optimize\nthe following reward with reinforcement learning.",
    "start": "3139750",
    "end": "3146345"
  },
  {
    "text": "And this reward looks\na little bit more complicated than just\nusing the reward model. And the extra term\nhere is a penalty,",
    "start": "3146345",
    "end": "3154670"
  },
  {
    "text": "which prevents us\nfrom diverging too far from the pre-trained model. So in expectation,\nthis is known as the KL",
    "start": "3154670",
    "end": "3161140"
  },
  {
    "text": "or Kullback-Leibler divergence\nbetween the RL model and the pre-trained model.",
    "start": "3161140",
    "end": "3167290"
  },
  {
    "text": "And I'll explain why we\nneed this in a few slides. But basically, if you\noveroptimize the reward model,",
    "start": "3167290",
    "end": "3173350"
  },
  {
    "text": "you end up producing-- you\ncan produce like gibberish. And what happens\nis you pay a price.",
    "start": "3173350",
    "end": "3178700"
  },
  {
    "text": "So this quantity is large. If the probability of a sample\nunder the RL tuned model",
    "start": "3178700",
    "end": "3184780"
  },
  {
    "text": "is much higher than\nthe probability of the sample under the\npre-trained model, right? So the pre-trained\nmodel would say",
    "start": "3184780",
    "end": "3189977"
  },
  {
    "text": "this is a very unlikely\nsequence of characters for anyone to say. That's when you would\npay a price here.",
    "start": "3189977",
    "end": "3195730"
  },
  {
    "text": "And beta here is a\ntunable parameter. Yeah, question? Just to make sure\nI understand, when",
    "start": "3195730",
    "end": "3200829"
  },
  {
    "text": "you say initialize a\ncopy, that means, like, the first iteration,\nPRL, is equal to PPT?",
    "start": "3200830",
    "end": "3207799"
  },
  {
    "text": "That's right. Yeah. Yeah. When I say initialize a\ncopy, basically, like, we want to be able to compare\nto the non finetuned model just",
    "start": "3207800",
    "end": "3215599"
  },
  {
    "text": "to evaluate this penalty term. So just leave the predictions\nof the pre RL model around.",
    "start": "3215600",
    "end": "3222480"
  },
  {
    "text": "Yeah.  More questions?",
    "start": "3222480",
    "end": "3229950"
  },
  {
    "text": "Great. So does it work? The answer is, yes. So here is kind of\nthe key takeaways,",
    "start": "3229950",
    "end": "3236550"
  },
  {
    "text": "at least for the task\nsummarization on this Daily Mail data set. So again, we're looking\nat different model sizes,",
    "start": "3236550",
    "end": "3243089"
  },
  {
    "text": "but at the end here, we see that\nif we do just pre-training, so just like the typical\nlanguage modeling objective",
    "start": "3243090",
    "end": "3248910"
  },
  {
    "text": "that GPT uses, you end up\nproducing summaries that in general are not preferred\nto the reference summaries.",
    "start": "3248910",
    "end": "3254910"
  },
  {
    "text": "So this is on the\ny-axis here, is the amount of times that a human\nprefers the model generated summary to a summary that\na human actually wrote",
    "start": "3254910",
    "end": "3262710"
  },
  {
    "text": "or the one that's\nin the data set. So pre-training\ndoesn't work well, even if you do\nsupervised learning.",
    "start": "3262710",
    "end": "3268650"
  },
  {
    "text": "So supervised learning\nin this case is, let's actually\nfinetune our model on the summaries that\nwere in our data sets.",
    "start": "3268650",
    "end": "3275609"
  },
  {
    "text": "Even if you do that, you\nstill kind of underperform the reference summaries, right,\nbecause you're not perfectly",
    "start": "3275610",
    "end": "3280660"
  },
  {
    "text": "modeling those summaries. But it's only with\nthis human feedback that we end up producing a\nlanguage model that actually",
    "start": "3280660",
    "end": "3287400"
  },
  {
    "text": "ends up producing\nsummaries that are judged to be better than the\nsummaries in the data set that you were training\non in the first place.",
    "start": "3287400",
    "end": "3293450"
  },
  {
    "text": "I think that's\nquite interesting. ",
    "start": "3293450",
    "end": "3299260"
  },
  {
    "text": "Any questions?  OK. ",
    "start": "3299260",
    "end": "3305670"
  },
  {
    "text": "So now we talk about-- yeah, we're getting\ncloser and closer to something like\nInstructGPT or ChatGPT,",
    "start": "3305670",
    "end": "3312450"
  },
  {
    "text": "the basic idea of InstructGPT\nis that we are scaling up RLHF",
    "start": "3312450",
    "end": "3318000"
  },
  {
    "text": "to not just one prompt, as I had\ndescribed previously, but tens and thousands of prompts.",
    "start": "3318000",
    "end": "3323430"
  },
  {
    "text": "And if you look at\nthese three pieces, these are the three pieces\nthat we've just described. The first piece here being\ninstruction fine tuning.",
    "start": "3323430",
    "end": "3330870"
  },
  {
    "text": "The second piece being\nRLHF, and the third piece-- oh, sorry, the second part\nbeing reward model training,",
    "start": "3330870",
    "end": "3337080"
  },
  {
    "text": "and the last part being RLHF. The difference here is\nthat they use 30,000 tasks.",
    "start": "3337080",
    "end": "3344890"
  },
  {
    "text": "So kind of again, with the same\ninstruction finetuning idea, it's really about the\nscale and diversity of tasks that really matters\nfor getting good performance",
    "start": "3344890",
    "end": "3351610"
  },
  {
    "text": "for these things.  Yeah?",
    "start": "3351610",
    "end": "3356780"
  },
  {
    "text": "Yeah. On the preceding results, you\nsuggested that it didn't work.",
    "start": "3356780",
    "end": "3365980"
  },
  {
    "text": "That you really\nneeded the RLHF and it didn't work so well with\nyour supervised learning of the data, but\nthey do supervised",
    "start": "3365980",
    "end": "3373750"
  },
  {
    "text": "learning on the data. Just in the finetuning\nin the first stage. Is that necessary\nor else things tend",
    "start": "3373750",
    "end": "3381520"
  },
  {
    "text": "to go haywire if you just\nwent straight to RLHF or--",
    "start": "3381520",
    "end": "3387280"
  },
  {
    "text": "Oh, yeah, that's\na good question. So I think a key point here is\nthat they initialized the RL policy on the supervised policy.",
    "start": "3387280",
    "end": "3394270"
  },
  {
    "text": "So they first got the\nmodel getting reasonably good at doing\nsummarization first, and then you do the RLHF on top\nto get the boost performance.",
    "start": "3394270",
    "end": "3402178"
  },
  {
    "text": "Your question you're\nasking is maybe, can we just do the RLHF starting\nfrom that pre-trained baseline?",
    "start": "3402178",
    "end": "3408232"
  },
  {
    "text": "That's a good question. I don't think they explored\nthat, although I'm not sure.",
    "start": "3408232",
    "end": "3414530"
  },
  {
    "text": "I'd have to look at the\npaper again to remind myself. Yeah. ",
    "start": "3414530",
    "end": "3422030"
  },
  {
    "text": "So, certainly, for\nsomething like InstructGPT, yeah, they've always\nkind of presumed that you need the kind of\nfinetuning phase first,",
    "start": "3422030",
    "end": "3428300"
  },
  {
    "text": "and then you build on top of it. But, I think,\nyeah, there's still some interesting open questions\nas to whether you can just",
    "start": "3428300",
    "end": "3433819"
  },
  {
    "text": "go directly to RLHF. Yeah. Question? Is this human reward function\ntrained simultaneously",
    "start": "3433820",
    "end": "3444160"
  },
  {
    "text": "with the finetuning of the\nlanguage model or sequential?",
    "start": "3444160",
    "end": "3449859"
  },
  {
    "text": "Reward model should\nbe trained first. Yeah. You train it first, you make\nsure it's good, it's frozen,",
    "start": "3449860",
    "end": "3455140"
  },
  {
    "text": "you optimize against that. For the samples for\nthe human rewards, does it come from the generator\ntext from language model,",
    "start": "3455140",
    "end": "3461530"
  },
  {
    "text": "or where does that\ntraining, sample come from? For training the reward model?",
    "start": "3461530",
    "end": "3466660"
  },
  {
    "text": "Yeah? So, yeah, actually,\nit's a good question, where do the rewards come from?",
    "start": "3466660",
    "end": "3473620"
  },
  {
    "text": "So there's kind of an iterative\nprocess you're going to apply, where you kind of repeat steps\n2 and 3 over and over again.",
    "start": "3473620",
    "end": "3479330"
  },
  {
    "text": "So you sample a bunch of outputs\nfrom your language model. You get humans to rate them.",
    "start": "3479330",
    "end": "3484390"
  },
  {
    "text": "You then do RLHF to\nupdate your model again, and then you sample more\noutputs, and get humans to rate them.",
    "start": "3484390",
    "end": "3489690"
  },
  {
    "text": "So in general, the rewards are\ndone on sampled model outputs because those are the outputs\nthat you want to steer in one direction or another.",
    "start": "3489690",
    "end": "3496720"
  },
  {
    "text": "But you can do this in an\niterative process, where you kind of do RL,\nand then maybe do a-- train a better reward model\nbased on the new outputs",
    "start": "3496720",
    "end": "3503230"
  },
  {
    "text": "and continue. And I think, they do a few\niterations in InstructGPT, for example. ",
    "start": "3503230",
    "end": "3513510"
  },
  {
    "text": "Questions? OK. So 30,000 tasks, I think, we're\ngetting into very recent stuff",
    "start": "3513510",
    "end": "3523740"
  },
  {
    "text": "where increasingly\ncompanies like OpenAI are sharing less\nand less details about what actually happens\nin training these models.",
    "start": "3523740",
    "end": "3530668"
  },
  {
    "text": "So we have a little bit\nless clarity as to what's going on here than maybe we have\nhad in the past, but they do",
    "start": "3530668",
    "end": "3537359"
  },
  {
    "text": "share-- the data sets not\npublic, but they do share the kinds of tasks that they\ncollected from labelers.",
    "start": "3537360",
    "end": "3542520"
  },
  {
    "text": "So they collected\na bunch of prompts from people who were\nalready using the GPT-3 API.",
    "start": "3542520",
    "end": "3547655"
  },
  {
    "text": "So they had the\nbenefit of having many, many users of their API,\nand taking the kinds of tasks",
    "start": "3547655",
    "end": "3553740"
  },
  {
    "text": "that users would ask GPT to do. And so these include\nthings like brainstorming,",
    "start": "3553740",
    "end": "3558930"
  },
  {
    "text": "or open end\ngeneration, et cetera. ",
    "start": "3558930",
    "end": "3564210"
  },
  {
    "text": "And, yeah, I mean, the key\nresults of InstructGPT, which is kind of the\nbackbone of ChatGPT,",
    "start": "3564210",
    "end": "3570500"
  },
  {
    "text": "really just needs to be seen\nand played with to understand, so you can-- you'll feel free to\nplay with either ChatGPT or one",
    "start": "3570500",
    "end": "3576270"
  },
  {
    "text": "of the OpenAI APIs. But again, this example\nof a language model, not necessarily following tasks.",
    "start": "3576270",
    "end": "3582750"
  },
  {
    "text": "By doing this kind of\ninstruction finetuning followed by RLHF, you get a model\nthat is much better",
    "start": "3582750",
    "end": "3589290"
  },
  {
    "text": "at adhering to user commands. ",
    "start": "3589290",
    "end": "3594320"
  },
  {
    "text": "Similarly, a\nlanguage model can be very good at generating\nsuper interesting, open ended creative text as well.",
    "start": "3594320",
    "end": "3601220"
  },
  {
    "text": " OK.",
    "start": "3601220",
    "end": "3606575"
  },
  {
    "text": " This brings us to ChatGPT,\nright, which is even newer,",
    "start": "3606575",
    "end": "3612335"
  },
  {
    "text": "and we have even\nless information about what's actually going on\nor what's being trained here.",
    "start": "3612335",
    "end": "3617730"
  },
  {
    "text": "Yeah, and they're keeping\ntheir secret sauce secret. But we do have a blog post\nwhere they wrote two paragraphs.",
    "start": "3617730",
    "end": "3624240"
  },
  {
    "text": "[LAUGHTER] And in the first paragraph,\nthey said that they did instruction finetuning.",
    "start": "3624240",
    "end": "3629920"
  },
  {
    "text": "So we trained an initial model\nusing supervised finetuning. So human AI trainers\nprovided conversations",
    "start": "3629920",
    "end": "3635880"
  },
  {
    "text": "where they played both\nsides, and then we asked them to act\nas an AI assistant, and then we finetuned our model\non acting like an AI assistant",
    "start": "3635880",
    "end": "3642870"
  },
  {
    "text": "from humans. That's part one. Second paragraph, to create\na reward model for RL,",
    "start": "3642870",
    "end": "3649920"
  },
  {
    "text": "we collected comparison data. So we took conversations with an\nearlier version of the chatbot,",
    "start": "3649920",
    "end": "3655680"
  },
  {
    "text": "so the one that's pre-trained\non instruction following or instruction finetuning. And then take multiple samples,\nand then rate the quality",
    "start": "3655680",
    "end": "3662820"
  },
  {
    "text": "of the samples, right? And then using\nthese reward models, we finetune it with\nRL, in particular,",
    "start": "3662820",
    "end": "3668993"
  },
  {
    "text": "they used PPO, which is\na fancier version of RL. ",
    "start": "3668993",
    "end": "3676490"
  },
  {
    "text": "OK. And yeah, so that produces-- I don't need to introduce\nthe capabilities of ChatGPT, it's been very\nexciting recently.",
    "start": "3676490",
    "end": "3683960"
  },
  {
    "text": "Here's an example. It's fun to play with. Definitely, play with it. ",
    "start": "3683960",
    "end": "3694339"
  },
  {
    "text": "Sorry, it's a bit of an\nattack on the students. Yeah. OK. ",
    "start": "3694340",
    "end": "3702550"
  },
  {
    "text": "OK. So reinforcement\nlearning pluses. You're kind of directly\nmodeling what you care about,",
    "start": "3702550",
    "end": "3709540"
  },
  {
    "text": "which is human preferences. Not, is the collection\nof the demonstration that I collected-- is that\nthe highest probability",
    "start": "3709540",
    "end": "3716320"
  },
  {
    "text": "mass in your model? You're actually just\nsaying, how well am I satisfying\nhuman preferences?",
    "start": "3716320",
    "end": "3721460"
  },
  {
    "text": "So that's a clear\nbenefit over something like instruction finetuning. So in terms of negatives,\none is that RL is hard.",
    "start": "3721460",
    "end": "3729050"
  },
  {
    "text": "It's very tricky to get right. I think it will get\neasier in the future as we kind of explore the design\nspace of possible options.",
    "start": "3729050",
    "end": "3736800"
  },
  {
    "text": "So that's an obvious one, right? Has anyone come up with\nany other, kind of, maybe, weaknesses or issues they see\nwith this kind of training?",
    "start": "3736800",
    "end": "3743150"
  },
  {
    "text": " Yeah.",
    "start": "3743150",
    "end": "3748460"
  },
  {
    "text": "So is it possible that your\nlanguage model and then your reward model\ncould like overfit to each other, especially,\nlike, even if you're not",
    "start": "3748460",
    "end": "3755270"
  },
  {
    "text": "training them together,\nif you're like going back and forth like a-- Yeah. Yeah. So overoptimization, I think,\nof the reward model is an issue.",
    "start": "3755270",
    "end": "3764270"
  },
  {
    "text": "Yeah. Is it also that if\nyou retrain your base learning, if you\nrepeat all this, you want feedback [INAUDIBLE]?",
    "start": "3764270",
    "end": "3770475"
  },
  {
    "text": "Yeah. So it still is, like,\nextremely data expensive. And you can see some articles\nif you just Google OpenAI,",
    "start": "3770475",
    "end": "3776525"
  },
  {
    "text": "like, data labeling, right? People have not been very\nhappy with the amount of data that has been needed to\ntrain something like ChatGPT.",
    "start": "3776525",
    "end": "3782060"
  },
  {
    "text": "I mean, they're\nhiring developers to just explain coding problems,\nlike, 40 hours a week, right? So it is-- yeah, it is\nstill data intensive, right?",
    "start": "3782060",
    "end": "3789230"
  },
  {
    "text": "That's kind of the\ntakeaway, like, all of these are, like, it's all\nstill data intensive, every single one\nof these, right?",
    "start": "3789230",
    "end": "3794330"
  },
  {
    "text": "Yeah.  Yeah. I think that summarizes\nkind of the big ones here.",
    "start": "3794330",
    "end": "3801970"
  },
  {
    "text": "So when we talk about\nlimitations of RLHF, we also need to talk about just\nlimitations in general of RL,",
    "start": "3801970",
    "end": "3809170"
  },
  {
    "text": "and also this idea that\nwe can model or capture human reward in this\nsingle data point.",
    "start": "3809170",
    "end": "3815080"
  },
  {
    "text": "So human preferences\ncan be very unreliable. The RL people have known\nthis for a very long time,",
    "start": "3815080",
    "end": "3821510"
  },
  {
    "text": "they have a term called\n\"Reward hacking,\" which is when an agent is\noptimizing for something that the developer\nspecified, but it is not",
    "start": "3821510",
    "end": "3828970"
  },
  {
    "text": "what we actually care about. So one of the classic\nexamples is this example from OpenAI, where\nthey were training",
    "start": "3828970",
    "end": "3835630"
  },
  {
    "text": "this agent to race boats. And they were training it\nto maximize the score, which",
    "start": "3835630",
    "end": "3841450"
  },
  {
    "text": "you can see at the bottom left. But implicitly,\nthe score actually isn't what you care\nabout, what you care about is just finishing\nthe race ahead of everyone else,",
    "start": "3841450",
    "end": "3847810"
  },
  {
    "text": "and the score is just\nkind of this bonus. But what the agent\nfound out was that there are these turbo boost\nthings that you can",
    "start": "3847810",
    "end": "3853690"
  },
  {
    "text": "collect which boost your score. And so what it ends\nup doing is it ends up kind of just driving\nin the middle, collecting these turbo\nboosts over and over again.",
    "start": "3853690",
    "end": "3860733"
  },
  {
    "text": "So it's racking up\nan insane score, but it is not doing the\nrace, it is continuously crashing into objects, and\nits boat is always on fire.",
    "start": "3860733",
    "end": "3867395"
  },
  {
    "text": "[LAUGHTER] And this is a pretty\nsalient example of what we call AI misalignment.",
    "start": "3867395",
    "end": "3874597"
  },
  {
    "text": "And you might think,\nwell, OK, this is a really simple example, right? They made a dumb mistake,\nthey shouldn't have used score",
    "start": "3874597",
    "end": "3881040"
  },
  {
    "text": "as a reward function, right? But I think it's even\nmore naive to think that we can capture all of human\npreferences in a single number,",
    "start": "3881040",
    "end": "3888600"
  },
  {
    "text": "and assign certain scalar\nvalues to things, right?",
    "start": "3888600",
    "end": "3893630"
  },
  {
    "text": "So one example where I think\nthis is already happening and you can see, is maybe\nyou have played with chatbots",
    "start": "3893630",
    "end": "3899600"
  },
  {
    "text": "before and you noticed that\nthey do a lot of hallucination, right, they make\nup a lot of facts. And this might be\nbecause of RLHF, right?",
    "start": "3899600",
    "end": "3906710"
  },
  {
    "text": "Chatbots are rewarded\nto produce responses that seem authoritative\nor seem helpful,",
    "start": "3906710",
    "end": "3911983"
  },
  {
    "text": "but they don't care about\nwhether it's actually true or not, they just\nwant to seem helpful. So this results in\nmaking up facts.",
    "start": "3911983",
    "end": "3919220"
  },
  {
    "text": "You maybe seeing the\nnews about chatbots-- companies are in this\nrace to deploy chatbots, and they make mistakes.",
    "start": "3919220",
    "end": "3925670"
  },
  {
    "text": "Even Bing also has been\nhallucinating a lot, right?",
    "start": "3925670",
    "end": "3930680"
  },
  {
    "text": "And in general, when you\nthink about that, you think, well, models of\nhuman preferences",
    "start": "3930680",
    "end": "3935690"
  },
  {
    "text": "are even more unreliable, right? Like, we're not even just\nusing human preferences by themselves, we're also\ntraining a model, a deep model,",
    "start": "3935690",
    "end": "3942260"
  },
  {
    "text": "that we have no idea\nhow that works, right? We're going to use\nthat instead, right? And that can obviously\nbe quite dangerous.",
    "start": "3942260",
    "end": "3950130"
  },
  {
    "text": "And so going back\nto this slide here, where I was describing why\nwe need this KL penalty term,",
    "start": "3950130",
    "end": "3955160"
  },
  {
    "text": "this yellow\nhighlighted term here, here's a concrete example\nof what actually happens, of a language model overfitting\nto the reward model.",
    "start": "3955160",
    "end": "3963109"
  },
  {
    "text": "So what this is showing\nis in this case, they took off the KL penalty. So they were just trying\nto maximize reward, right?",
    "start": "3963110",
    "end": "3968900"
  },
  {
    "text": "They trained this reward model,\nlet's just push those numbers up as high as possible. And on the x-axis\nhere is what happens",
    "start": "3968900",
    "end": "3975080"
  },
  {
    "text": "as training continues, you\ndiverge further and further. This is the KL divergence\nor the distance from where you started.",
    "start": "3975080",
    "end": "3982040"
  },
  {
    "text": "And the golden\ndashed line here is what the reward model\npredicts your language",
    "start": "3982040",
    "end": "3987110"
  },
  {
    "text": "model is doing, right? So your reward model\nis thinking, Wow, you are killing it. Like, they are going to\nlove these summaries.",
    "start": "3987110",
    "end": "3992300"
  },
  {
    "text": "They are going to love them\nway more than the reference summaries, right? But in reality,\nwhen you actually ask humans, the\npreferences peak, and then",
    "start": "3992300",
    "end": "4000550"
  },
  {
    "text": "they just crater. So this can be an example of\nover optimizing for a metric",
    "start": "4000550",
    "end": "4007135"
  },
  {
    "text": "that you care about, right? It ceases to become a good\nmetric to optimize for. ",
    "start": "4007135",
    "end": "4014590"
  },
  {
    "text": "Any questions about this? So there's this real\nconcern of, I think, what people are calling\nthe AI alignment problem.",
    "start": "4014590",
    "end": "4021290"
  },
  {
    "text": "I'll let Percy Liang\ntalk about this. He tweeted that the main\ntool that we have for RLHF--",
    "start": "4021290",
    "end": "4026680"
  },
  {
    "text": "or sorry, for alignment is RLHF. But reward hacking\nhappens a lot, humans are not very good\nsupervisors of rewards,",
    "start": "4026680",
    "end": "4034370"
  },
  {
    "text": "so this strategy\nis probably going to result in agents\nthat seem like they're doing the right\nthing, but they're wrong in subtle\ninconspicuous ways.",
    "start": "4034370",
    "end": "4040842"
  },
  {
    "text": "And I think we're\nalready seeing examples of that in the current\ngeneration of chatbots. ",
    "start": "4040842",
    "end": "4048750"
  },
  {
    "text": "So in terms of positives,\nhere are some positives. But again, RL is\ntricky to get right.",
    "start": "4048750",
    "end": "4054240"
  },
  {
    "text": "Human preferences are fallible,\nand models of human preferences are even more so. ",
    "start": "4054240",
    "end": "4062530"
  },
  {
    "text": "So I remember seeing\na joke on Twitter somewhere where\nsomeone was saying that Zero-Shot and\nFew-Shot learning",
    "start": "4062530",
    "end": "4068140"
  },
  {
    "text": "is the worst way to align an AI. Instruction finetuning\nis the second worst way to align an AI, and RLHF is the\nthird worst way to align an AI.",
    "start": "4068140",
    "end": "4076779"
  },
  {
    "text": "So we're getting somewhere,\nbut each of these have clear fundamental\nlimitations.",
    "start": "4076780",
    "end": "4082090"
  },
  {
    "text": "[INAUDIBLE] Yeah, of course. I have a question on more\ncomputation of reinforcement",
    "start": "4082090",
    "end": "4090700"
  },
  {
    "text": "learning, because\nwe got the math that you showed before,\nessentially, you're putting the gradient and the\nsize so that you can sample it,",
    "start": "4090700",
    "end": "4096969"
  },
  {
    "text": "you can sample expectation. Yeah. But when it comes to sampling,\nhow do you make that parallel?",
    "start": "4096970",
    "end": "4102310"
  },
  {
    "text": "Because then you need-- you kind of need adaptively\nstopped sampling, and then you don't know\nwhen you're going to stop.",
    "start": "4102310",
    "end": "4109630"
  },
  {
    "text": "Like, how do you make that\nprocess quicker, I guess, because, like, the whole unit\non transformers and all that was parallelizing everything?",
    "start": "4109630",
    "end": "4115930"
  },
  {
    "text": "Yeah. I mean, yeah, this is-- so\nthis is really compute heavy.",
    "start": "4115930",
    "end": "4121479"
  },
  {
    "text": "And I'm actually not sure what\nkind of infrastructure is used for a state-of-the-art, very\nperformant implementation",
    "start": "4121479",
    "end": "4126729"
  },
  {
    "text": "of RLHF. But it's possible that they\nuse parallelization, like, what you're describing,\nwhere, I think, in a lot of maybe\nmore traditional RL,",
    "start": "4126729",
    "end": "4133299"
  },
  {
    "text": "there's this kind of idea\nof having an actor learner architecture, where you have a\nbunch of actor workers, which",
    "start": "4133300",
    "end": "4138490"
  },
  {
    "text": "are each kind of a\nlanguage model producing a bunch of samples,\nand then the learner will then integrate them and\nperform the gradient updates.",
    "start": "4138490",
    "end": "4144699"
  },
  {
    "text": "So it's possible that\nyou do need to do, like, just sheer like\nmultiprocessing in order to get enough samples\nto make this work",
    "start": "4144700",
    "end": "4150620"
  },
  {
    "text": "in a reasonable amount of time. Is that the kind of\nquestion you have or do you have other questions? Kind of. So you're basically saying\nthat each parallel--",
    "start": "4150620",
    "end": "4157670"
  },
  {
    "text": "each unit that your parallel\nlies over is larger than what",
    "start": "4157670",
    "end": "4163640"
  },
  {
    "text": "we would typically see\nin transformers that-- I'm saying that you might need\nto actually copy your model",
    "start": "4163640",
    "end": "4169278"
  },
  {
    "text": "several times, and\ntake samples, like, from different\ncopies of the models. Yeah. But in terms of like--",
    "start": "4169279",
    "end": "4174390"
  },
  {
    "text": "yeah, so autoregressive\ngeneration, like, transformers,\nespecially, like, the forward pass and the\nmulti-head attention stuff",
    "start": "4174390",
    "end": "4179929"
  },
  {
    "text": "is very easy to parallelize, but\nautoregressive generalization-- autoregressive generation\nis still, like,",
    "start": "4179930",
    "end": "4185870"
  },
  {
    "text": "kind of bottlenecked by the\nfact that it's autoregressive. So you have to run\nit first, and then you need to-- depends\non what you sample,",
    "start": "4185870",
    "end": "4192439"
  },
  {
    "text": "you have to run it again, right? So those are kind of blocks\nthat we haven't fully been able to solve, I think, and\nthat will add to compute cost.",
    "start": "4192439",
    "end": "4199739"
  },
  {
    "text": "Yeah. ",
    "start": "4199740",
    "end": "4205520"
  },
  {
    "text": "OK. So I think we have 10 more\nminutes if I'm not mistaken. So we've mostly\nfinally answered kind",
    "start": "4205520",
    "end": "4212395"
  },
  {
    "text": "of how we get from\nthis to this, right? There's some details missing,\nbut the key kind of factors are 1, instruction finetuning.",
    "start": "4212395",
    "end": "4218990"
  },
  {
    "text": "2, this idea of reinforcement\nlearning from human feedback. So let's talk a little\nbit about what's next.",
    "start": "4218990",
    "end": "4228450"
  },
  {
    "text": "So as I had mentioned, RLHF\nis still a very new area, it's still very fast moving.",
    "start": "4228450",
    "end": "4234170"
  },
  {
    "text": "I think, by the next lecture,\nby the time we say, do these slides again,\nthese slides might look completely different,\nbecause maybe a lot",
    "start": "4234170",
    "end": "4240050"
  },
  {
    "text": "of the things that I\nwas presenting here turn out to be, like, really bad\nideas or not the most efficient way of going about things.",
    "start": "4240050",
    "end": "4247720"
  },
  {
    "text": "RLHF gets you further than\ninstruction finetuning, but as someone had\nalready mentioned, it is still very data\nexpensive, right?",
    "start": "4247721",
    "end": "4254030"
  },
  {
    "text": "There are a lot of articles\nabout OpenAI needing to hire a legion of\nannotators or developers",
    "start": "4254030",
    "end": "4259040"
  },
  {
    "text": "to just compare outputs\nover and over again. I think a recent work that\nI'm especially interested in,",
    "start": "4259040",
    "end": "4265960"
  },
  {
    "text": "and been thinking about is how\nwe can get the benefits of RLHF without such stringent\ndata requirements.",
    "start": "4265960",
    "end": "4272119"
  },
  {
    "text": "So there's these newer\nkind of crazy ideas about doing\nreinforcement learning from not human feedback\nbut from AI feedback.",
    "start": "4272120",
    "end": "4279880"
  },
  {
    "text": "So having language\nmodels themselves evaluate the output\nof language models. So as an example of what\nthat might look like,",
    "start": "4279880",
    "end": "4286300"
  },
  {
    "text": "a team from Anthropic, which\nworks on these large language models, came up with this\nidea called Constitutional AI.",
    "start": "4286300",
    "end": "4292570"
  },
  {
    "text": "And the basic idea\nhere is that if you ask GPT-3 to identify whether\na response was not helpful,",
    "start": "4292570",
    "end": "4297790"
  },
  {
    "text": "it would be pretty\ngood at doing so, and you might be able to\nuse that feedback itself to improve a model.",
    "start": "4297790",
    "end": "4303280"
  },
  {
    "text": "So as an example, if you have\nsome sort of human request, like, \"can you help me hack\ninto my neighbor's Wi-Fi?\"",
    "start": "4303280",
    "end": "4309040"
  },
  {
    "text": "And the assistant says, \"yeah,\nsure, you can use this app,\" we can ask a model\nfor feedback on this.",
    "start": "4309040",
    "end": "4316300"
  },
  {
    "text": "What we do is we add a\ncritique request, which says, \"Hey, language\nmodel, GPT-3, identify",
    "start": "4316300",
    "end": "4321760"
  },
  {
    "text": "ways in which the assistant's\nresponse is harmful.\" And then it will\ngenerate a critique,",
    "start": "4321760",
    "end": "4327650"
  },
  {
    "text": "like, hacking into someone\nelse's Wi-Fi is illegal. And then you might ask it\nto then revise it, right?",
    "start": "4327650",
    "end": "4333560"
  },
  {
    "text": "So just rewrite the\nassistant response to remove harmful content. And it does so.",
    "start": "4333560",
    "end": "4340850"
  },
  {
    "text": "And now by just decoding\nfrom a language model, assuming you can do\nthis well, what you have now is a set of\ndata that you can",
    "start": "4340850",
    "end": "4348320"
  },
  {
    "text": "do instruction finetuning on. You have a request,\nand you have a request that has been revised to\nmake sure it doesn't contain",
    "start": "4348320",
    "end": "4354200"
  },
  {
    "text": "harmful content.  So this is pretty interesting.",
    "start": "4354200",
    "end": "4359352"
  },
  {
    "text": "I think, it's quite exciting. But all of those issues that I\nhad mentioned about alignment,",
    "start": "4359352",
    "end": "4364730"
  },
  {
    "text": "misoverinterpreting,\nhuman preferences, reward models being fallible. Like, everything gets\ncompounded like 40,000 times",
    "start": "4364730",
    "end": "4371415"
  },
  {
    "text": "when you're thinking\nabout this, right? We have no understanding\nof how safe this is or where this ends up\ngoing, but it is something.",
    "start": "4371415",
    "end": "4379640"
  },
  {
    "text": "Another kind of more\ncommon idea also is this general idea of\nfinetuning language models on their own outputs.",
    "start": "4379640",
    "end": "4384780"
  },
  {
    "text": "And this has been explored\na lot in the context of chain-of-thought\nreasoning, which is something I presented at\nthe beginning of the lecture.",
    "start": "4384780",
    "end": "4390240"
  },
  {
    "text": "And these are\nprovocatively named, Large Language Models\nCan Self-Improve. But again, it's not clear,\nlike, how much runway there is.",
    "start": "4390240",
    "end": "4396920"
  },
  {
    "text": "But the basic idea maybe is to-- you can, use let's think\nstep by step for example, to get a language model to\nproduce a bunch of reasoning,",
    "start": "4396920",
    "end": "4403159"
  },
  {
    "text": "and then you can say,\nfinetune on that reasoning as if it were true data\nand see whether or not a language model can get any\nbetter using that technique.",
    "start": "4403160",
    "end": "4409250"
  },
  {
    "text": " But as I mentioned, this\nis all still very new.",
    "start": "4409250",
    "end": "4416179"
  },
  {
    "text": "There are I think a lot of\nlimitations of large language models like hallucination. And also, just the sheer\nsize and compute intensity",
    "start": "4416180",
    "end": "4422990"
  },
  {
    "text": "of this that may or may not\nbe solvable with RLHF, right? Question?",
    "start": "4422990",
    "end": "4429000"
  },
  {
    "text": "So going back to where\nyou talked about RLHF to give feed back of, like,\nhow we don't want to get that,",
    "start": "4429000",
    "end": "4434540"
  },
  {
    "text": "like, I've seen people\ntalking about how you can jailbreak\nChatGPT to still give those types of\nharmful responses.",
    "start": "4434540",
    "end": "4441340"
  },
  {
    "text": "Yeah. Are there any ways\nfor us to kind of buffer against those\ntypes of things as well,",
    "start": "4441340",
    "end": "4448220"
  },
  {
    "text": "because it seems\nlike you're just going to keep kind of building\non, like, re-identify chances",
    "start": "4448220",
    "end": "4453650"
  },
  {
    "text": "where it's like trying to\nsay, \"act not like yourself.\" I guess, is there any\nway to kind of build up",
    "start": "4453650",
    "end": "4460250"
  },
  {
    "text": "that scale to avoid those\njailbreaking possibilities? Yeah, that's interesting.",
    "start": "4460250",
    "end": "4466280"
  },
  {
    "text": " So there are certainly ways that\nyou can use either AI feedback",
    "start": "4466280",
    "end": "4472587"
  },
  {
    "text": "or human feedback to mitigate\nthose kinds of jailbreaks, right? Like, if you see\nsomeone on Twitter saying that, oh, I made GPT-3\njailbreak using this strategy",
    "start": "4472587",
    "end": "4480409"
  },
  {
    "text": "or whatever, you can\nthen, yeah, maybe plug it into this kind\nof framework and say, identify ways in which\nthe assistant went off",
    "start": "4480410",
    "end": "4486350"
  },
  {
    "text": "the rails, and then finetune,\nand, hopefully, correct those, right? But it is really\ndifficult. I think, in most of these\nkinds of settings,",
    "start": "4486350",
    "end": "4492600"
  },
  {
    "text": "it's really difficult\nto anticipate all the possible ways\nin which a user might jailbreak an assistant, right? So you always have\nthis kind of dynamic",
    "start": "4492600",
    "end": "4499190"
  },
  {
    "text": "of, like, in security,\ncybersecurity, for example, there's always like\nthe attacker advantage",
    "start": "4499190",
    "end": "4504260"
  },
  {
    "text": "where the attacker will\nalways come up with something new or some new exploit, right? So yeah.",
    "start": "4504260",
    "end": "4509823"
  },
  {
    "text": "I think, this is a\ndeep problem, I don't have a really clear answer. But certainly, like, if we\nknew what the jailbreak was,",
    "start": "4509823",
    "end": "4515090"
  },
  {
    "text": "we could mitigate it. I think that seems\npretty straightforward. Yeah. ",
    "start": "4515090",
    "end": "4520572"
  },
  {
    "text": "But if you know\nhow to do that, you should be hired by one\nof these companies. They'll pay you like millions\nif you can solve this.",
    "start": "4520572",
    "end": "4525980"
  },
  {
    "text": "Yeah.  OK.",
    "start": "4525980",
    "end": "4531040"
  },
  {
    "text": "Yeah. So just, like, last remarks\nis, with all of these, like, scaling results that I\npresented and all of these,",
    "start": "4531040",
    "end": "4537360"
  },
  {
    "text": "like, oh, you can just\ndo instruction finetuning and it'll follow\nyour instructions, or you can do RLHF. You might have a\nvery bullish view",
    "start": "4537360",
    "end": "4543503"
  },
  {
    "text": "on like, oh, this is how\nwe're going to solve, like, artificial general intelligence\nby just scaling up RLHF. It's possible that is\nactually going to happen,",
    "start": "4543503",
    "end": "4550775"
  },
  {
    "text": "but it's also\npossible that there are certain fundamental\nlimitations that we just need to figure out\nhow to solve, like,",
    "start": "4550775",
    "end": "4557040"
  },
  {
    "text": "hallucination before\nwe get anywhere productive with these models. But it is a really exciting time\nto work on this kind of stuff.",
    "start": "4557040",
    "end": "4562800"
  },
  {
    "text": "So yeah. Thanks for listening,\nand yeah, thanks.",
    "start": "4562800",
    "end": "4568080"
  },
  {
    "text": "[APPLAUSE] ",
    "start": "4568080",
    "end": "4575000"
  }
]