[
  {
    "start": "0",
    "end": "5330"
  },
  {
    "text": "The plan for today is to talk\nabout latent variable models. So just as a recap,\nwhat we've seen so far",
    "start": "5330",
    "end": "12770"
  },
  {
    "text": "is the first kind of family\nof generative models, the autoregressive\nones, where the key idea",
    "start": "12770",
    "end": "20240"
  },
  {
    "text": "is that we use chain rule to\ndescribe a joint probability distribution as a\nproduct of conditionals,",
    "start": "20240",
    "end": "28460"
  },
  {
    "text": "and then we essentially try to\napproximate the conditionals using some kind\nof neural network.",
    "start": "28460",
    "end": "35510"
  },
  {
    "text": "And we've seen several options\nfor doing that, including RNNs, CNNs, transformers.",
    "start": "35510",
    "end": "43520"
  },
  {
    "text": "At the end of the day,\nthe core underlying idea is, really, this autoregressive\nfactorization of the joint.",
    "start": "43520",
    "end": "52040"
  },
  {
    "text": "And we've seen that\nautoregressive models are good because they give\nyou access to the likelihood.",
    "start": "52040",
    "end": "59280"
  },
  {
    "text": "It's relatively easy to evaluate\nthe probability of any data point. You just multiply\ntogether the conditionals.",
    "start": "59280",
    "end": "66270"
  },
  {
    "text": "And what this means is\nthat you can train them by maximum likelihood. You have a training\ndata set, you",
    "start": "66270",
    "end": "72330"
  },
  {
    "text": "can evaluate the probability\nassigned by your model to the data, and you can\noptimize the parameters",
    "start": "72330",
    "end": "77970"
  },
  {
    "text": "of your probability distribution\nto maximize the probability of the data set you are given.",
    "start": "77970",
    "end": "84600"
  },
  {
    "text": "And you can use the\nlikelihood to do other things like, for\nexample, anomaly detection.",
    "start": "84600",
    "end": "90240"
  },
  {
    "text": "The cons of\nautoregressive models is that, well, first of all,\nyou have to pick an ordering",
    "start": "90240",
    "end": "95790"
  },
  {
    "text": "and sometimes it's\nstraightforward to do it, sometimes it can be\ntricky to figure out",
    "start": "95790",
    "end": "101220"
  },
  {
    "text": "what is the right ordering that\nyou're going to use to construct the chain rule factorization.",
    "start": "101220",
    "end": "106920"
  },
  {
    "text": "Generation is slow. So even if you use\nan architecture that",
    "start": "106920",
    "end": "111960"
  },
  {
    "text": "allows you to compute all\nthe conditionals basically in parallel like a\ntransformer, the challenge",
    "start": "111960",
    "end": "120150"
  },
  {
    "text": "is that at generation, you\nhave to generate, basically, one variable at a time. And so that can be slow.",
    "start": "120150",
    "end": "127229"
  },
  {
    "text": "And another thing is\nthat it's not obvious how you can get features from\nthe data in an unsupervised way.",
    "start": "127230",
    "end": "134160"
  },
  {
    "text": "And that, we'll see,\nis one of the things that we're going to be able to\ndo using latent variable models.",
    "start": "134160",
    "end": "140480"
  },
  {
    "text": "And so the plan for\ntoday is to cover-- start talking about\nlatent variable models. We'll start from simple\nones like mixture models",
    "start": "140480",
    "end": "147070"
  },
  {
    "text": "and then we'll\nstart the discussion of the variational\nautoencoder or the VA,",
    "start": "147070",
    "end": "153285"
  },
  {
    "text": "and we'll see how\nto do inference and learning when you\nhave latent variables. So the high level motivation\nfor building or using",
    "start": "153285",
    "end": "163170"
  },
  {
    "text": "a latent variable\nmodel is that when you're trying to model\na complicated data set,",
    "start": "163170",
    "end": "171270"
  },
  {
    "text": "for example, a data set of\nimages of people like this one,",
    "start": "171270",
    "end": "176310"
  },
  {
    "text": "the problem is typically\nhard because there is a lot of variability\nthat you have to capture.",
    "start": "176310",
    "end": "182790"
  },
  {
    "text": "For example, in this case, there\nmight be a lot of variability",
    "start": "182790",
    "end": "188099"
  },
  {
    "text": "because people\nhave different age, people have different poses,\npeople have different hair",
    "start": "188100",
    "end": "193350"
  },
  {
    "text": "colors, eye colors. And so all these things\nlead to very different kind of values for the pixels that\nyou have in the data set.",
    "start": "193350",
    "end": "203150"
  },
  {
    "text": "And so the problem is that-- if you somehow had access to\nthese sort of annotations,",
    "start": "203150",
    "end": "210230"
  },
  {
    "text": "perhaps you would\nbe able to do-- it would be easier to\nmodel the distribution because you could\nbuild separate models.",
    "start": "210230",
    "end": "217220"
  },
  {
    "text": "That way, you're kind of\nconditioning on hair color or the eye color or\nthe age or whatever",
    "start": "217220",
    "end": "222680"
  },
  {
    "text": "attribute you have access to. But unless you have\nsort of annotations,",
    "start": "222680",
    "end": "230099"
  },
  {
    "text": "all you have access to\nis a bunch of images. And although you\nbelieve that you can kind of see that there\nis this latent structure,",
    "start": "230100",
    "end": "236640"
  },
  {
    "text": "it's not annotated. So it's not obvious how\nyou take advantage of it.",
    "start": "236640",
    "end": "241950"
  },
  {
    "text": "And so the idea of\nlatent variable models is to essentially add a\nbunch of random variables,",
    "start": "241950",
    "end": "250019"
  },
  {
    "text": "which we're going\nto denote z, which are supposed to capture\nall these latent",
    "start": "250020",
    "end": "256620"
  },
  {
    "text": "factors of variation. So even though we only\ncare about modeling pixels in the images, we're\ngoing to incorporate",
    "start": "256620",
    "end": "263490"
  },
  {
    "text": "a bunch of other random\nvariables in our model. And we're going to call\nthese random variables",
    "start": "263490",
    "end": "268770"
  },
  {
    "text": "latent or hidden\nbecause they are not observed in the data set.",
    "start": "268770",
    "end": "274170"
  },
  {
    "text": "We only get to see the\npixel values, the x part, but we don't get to see\nthe corresponding values",
    "start": "274170",
    "end": "280290"
  },
  {
    "text": "for the latent\nfactors of variation. And by doing this so we\ngot several advantages,",
    "start": "280290",
    "end": "287400"
  },
  {
    "text": "we're going to get more\nflexible model families. And if you can fit a\nmodel reasonably well,",
    "start": "287400",
    "end": "295280"
  },
  {
    "text": "then we might also be able to\nextract these latent variables",
    "start": "295280",
    "end": "300889"
  },
  {
    "text": "given the pixel values. And if you're doing a\ngood job at modeling",
    "start": "300890",
    "end": "305990"
  },
  {
    "text": "these common characteristics\nthat the different data points have, then you\nmight use these features",
    "start": "305990",
    "end": "311719"
  },
  {
    "text": "to do other things. You might try to predict--",
    "start": "311720",
    "end": "317120"
  },
  {
    "text": "if you have a\nclassification task, it might be easier\nto train a model that",
    "start": "317120",
    "end": "322250"
  },
  {
    "text": "works at the level of these\nlatent variables as opposed to the direct pixel values. Because often, you\nmight be able to--",
    "start": "322250",
    "end": "330080"
  },
  {
    "text": "you might need a small\nnumber of latent variables to describe a much more\nhigh-dimensional data set,",
    "start": "330080",
    "end": "339530"
  },
  {
    "text": "like images, for example. So the high level,\nkind of trying",
    "start": "339530",
    "end": "346470"
  },
  {
    "text": "to formalize a little bit of\nthis intuition, what we want to do is we want to\nhave a joint probability",
    "start": "346470",
    "end": "352320"
  },
  {
    "text": "distribution between the\nx, which are basically all the pixel\nvalues that we have in an image, and these\nlatent variables, z.",
    "start": "352320",
    "end": "360000"
  },
  {
    "text": "And so here I'm showing\nthe x shaded, meaning that it's observed, and\nthe z variables are white",
    "start": "360000",
    "end": "366480"
  },
  {
    "text": "and they're not shaded\nbecause this basically means for every data point,\nwe don't get to see--",
    "start": "366480",
    "end": "371699"
  },
  {
    "text": "we don't have a notations\nfor the corresponding latent variables. And conceptually, you can\nthink of a Bayesian network",
    "start": "371700",
    "end": "379208"
  },
  {
    "text": "that might look something\nlike this, right. Where there is the pixel\nvalues that you get to see and then there is a bunch of\nlatent factors of variation",
    "start": "379208",
    "end": "385919"
  },
  {
    "text": "that would be\nhelpful in describing the different types of images\nthat you might have access",
    "start": "385920",
    "end": "394290"
  },
  {
    "text": "to in your data set. And these latent\nvariables, again, they",
    "start": "394290",
    "end": "401130"
  },
  {
    "text": "might correspond to these\nhigh-level features. And if z is chosen properly,\nyou get several advantages",
    "start": "401130",
    "end": "410669"
  },
  {
    "text": "because it might be\na lot easier to model p of x given z as opposed to the\nmarginal distribution p of x.",
    "start": "410670",
    "end": "417410"
  },
  {
    "text": "But if you somehow are able\nto cluster the data points and divide them into\ndifferent groups,",
    "start": "417410",
    "end": "423460"
  },
  {
    "text": "then modeling the images that\nbelong to every particular group separately, which is kind\nof what this p of x given z",
    "start": "423460",
    "end": "430710"
  },
  {
    "text": "would do, could be much\neasier because at that point, there is a lot less\nvariation that you have to capture once you\ncondition on these latent",
    "start": "430710",
    "end": "439950"
  },
  {
    "text": "features. And the other good\nthing that you have access to if you do\nthis is that if then you",
    "start": "439950",
    "end": "447810"
  },
  {
    "text": "try to infer the latent\nvariables for our new data point, x, then you can\nidentify these features.",
    "start": "447810",
    "end": "456220"
  },
  {
    "text": "And so, again, this\nis sort of like going towards the representation\nlearning angle or the computer",
    "start": "456220",
    "end": "463930"
  },
  {
    "text": "vision as inverse graphics. Somehow, if you have\na good generative model that can\nproduce images based",
    "start": "463930",
    "end": "469750"
  },
  {
    "text": "on a set of latent variables, if\nyou can then infer these latent variables, then you might be\ndiscovering features structure",
    "start": "469750",
    "end": "478270"
  },
  {
    "text": "that you can use for\ndifferent sorts of problems. And the problem is\nthat it might be",
    "start": "478270",
    "end": "486120"
  },
  {
    "text": "very hard to specify a\ngraphical model like this and specify all\nthe conditionals.",
    "start": "486120",
    "end": "492210"
  },
  {
    "text": "And so as usual, instead of\ntaking the graphical model view or the Bayesian network\nview that we have here,",
    "start": "492210",
    "end": "498120"
  },
  {
    "text": "we're going to try to\nuse deep neural networks to do the work for us.",
    "start": "498120",
    "end": "504400"
  },
  {
    "text": "And so what we're\ngoing to do instead is we're still going to\nkeep that kind of structure where we have a set of observed\nvariables, x, and latent",
    "start": "504400",
    "end": "512669"
  },
  {
    "text": "variables, z, but\nwe're not going to have anything\ninterpretable in terms of how the random variables are related\nto each other or what they mean.",
    "start": "512669",
    "end": "521759"
  },
  {
    "text": "We're just going to\nassume that there is a set of random variables,\nz, that are somewhat simple.",
    "start": "521760",
    "end": "529050"
  },
  {
    "text": "For example, they\nmight be distributed according to a simple\nGaussian distribution.",
    "start": "529050",
    "end": "535050"
  },
  {
    "text": "And then we model the\nconditional distribution of x given z, again,\nusing, basically,",
    "start": "535050",
    "end": "541620"
  },
  {
    "text": "some kind of deep\ngenerative model where we have a\nsimple distribution,",
    "start": "541620",
    "end": "547980"
  },
  {
    "text": "let's say a Gaussian, but the\nparameters of this distribution depend, in some potentially\ncomplicated way,",
    "start": "547980",
    "end": "556199"
  },
  {
    "text": "on the latent variables\nthrough, let's say, a couple of neural\nnetworks, mu, theta, and sigma",
    "start": "556200",
    "end": "562902"
  },
  {
    "text": "theta, that are\nbasically giving us the mean and the standard\ndeviation that we're expecting",
    "start": "562902",
    "end": "569160"
  },
  {
    "text": "for x given that the\nlatent variables take a particular value.",
    "start": "569160",
    "end": "574819"
  },
  {
    "text": "And so, again, because, at this\npoint, the latent variables, they don't have any\npre-specified semantic,",
    "start": "574820",
    "end": "583660"
  },
  {
    "text": "then we're sort of like hoping\nthat by fitting this model,",
    "start": "583660",
    "end": "589180"
  },
  {
    "text": "let's say by maximum\nlikelihood, we end up somehow discovering\ninteresting latent structures.",
    "start": "589180",
    "end": "596240"
  },
  {
    "text": "And as usual, this\nis basically-- this is an unsupervised\nlearning problem.",
    "start": "596240",
    "end": "601830"
  },
  {
    "text": "So it's kind of ill-defined\nbecause what does it mean that the structure\nis meaningful?",
    "start": "601830",
    "end": "607280"
  },
  {
    "text": "What is it that we're\nactually after here? It's not obvious. But the intuition\nis that hopefully",
    "start": "607280",
    "end": "615050"
  },
  {
    "text": "by trying to model the data\nusing these latent variables, we might discover some\ninteresting structure.",
    "start": "615050",
    "end": "621440"
  },
  {
    "text": "Some interesting correspondence\nbetween x and z that then",
    "start": "621440",
    "end": "628820"
  },
  {
    "text": "would, first of all,\nmake learning easier because we are able to model\na distribution over images,",
    "start": "628820",
    "end": "635660"
  },
  {
    "text": "x, using something\nlike a Gaussian. And then by inferring\nthe latent variables",
    "start": "635660",
    "end": "641930"
  },
  {
    "text": "given the observed\none, given the x, we're hopefully going to\ndiscover interesting features",
    "start": "641930",
    "end": "648440"
  },
  {
    "text": "that then we can use to analyze\nthe data or do transfer learning",
    "start": "648440",
    "end": "653510"
  },
  {
    "text": "or whatever you want. Question. [INAUDIBLE]",
    "start": "653510",
    "end": "660044"
  },
  {
    "text": " Can you repeat. At z. How you managed to update\nz, the latent variable",
    "start": "660044",
    "end": "667080"
  },
  {
    "text": "of [INAUDIBLE] neural network? So the question is,\nhow do we change z when we fit the neural network?",
    "start": "667080",
    "end": "672780"
  },
  {
    "text": "Yeah, so we'll see\nhow we do learning. That's the challenge. So the challenge is that\nthe z variables are not",
    "start": "672780",
    "end": "677910"
  },
  {
    "text": "observed during training. And so it's not obvious\nhow you should update the parameters of this neural\nnetwork that gives you,",
    "start": "677910",
    "end": "684690"
  },
  {
    "text": "essentially, the x as a function\nof z when you don't know what z was.",
    "start": "684690",
    "end": "690240"
  },
  {
    "text": "And so, intuitively,\nyou're going to have to guess what is the\nvalue of z for any given x,",
    "start": "690240",
    "end": "696270"
  },
  {
    "text": "and you're going to use\nsome kind of procedure to try to fit this model. So if you've seen EM,\nit's going to have",
    "start": "696270",
    "end": "703529"
  },
  {
    "text": "the flavor of an\nEM-like procedure where we're going to try to\nguess a value for the latent variables and then we're\ngoing to try to fit the model.",
    "start": "703530",
    "end": "712170"
  },
  {
    "text": "So is that image,\nx, here-- how is that being represented\nas autoregressively being generated?",
    "start": "712170",
    "end": "718639"
  },
  {
    "text": "The question is, is x being\nrepresented autoregressively? In this case, there is no\nautoregressive structure.",
    "start": "718640",
    "end": "725300"
  },
  {
    "text": "So x given z is just a\nGaussian distribution. So something very simple.",
    "start": "725300",
    "end": "731150"
  },
  {
    "text": "The parameters of\nthis Gaussian are determined through this\npotentially very complicated",
    "start": "731150",
    "end": "736730"
  },
  {
    "text": "non-linear relationship\nwith respect to z. And as we'll see, even though\np of x given z is very simple,",
    "start": "736730",
    "end": "743490"
  },
  {
    "text": "it's just a Gaussian,\nand you would never expect that a single Gaussian\nis sufficiently flexible",
    "start": "743490",
    "end": "748670"
  },
  {
    "text": "to model anything interesting\nbecause you have these latent variables.",
    "start": "748670",
    "end": "754519"
  },
  {
    "text": "As we discussed\nbefore, if you somehow are able to cluster the data\npoints in a reasonable way, then within the cluster, which\nis kind of what this object is,",
    "start": "754520",
    "end": "763399"
  },
  {
    "text": "you might be able to get\naway with a very simple kind of distribution. And that's the idea behind\na latent variable model.",
    "start": "763400",
    "end": "771690"
  },
  {
    "text": "In the last lecture, we spoke\nabout how we basically refactor the joint distribution and\nindividual conditionals",
    "start": "771690",
    "end": "777960"
  },
  {
    "text": "and then learn your networks for\nthese individual conditionals, right. Is it common practice that\nthese individual conditionals",
    "start": "777960",
    "end": "784980"
  },
  {
    "text": "are modeled with a Gaussian\nor are there approaches, like classical-- like\ncommon approaches where",
    "start": "784980",
    "end": "791250"
  },
  {
    "text": "we use different distributions? And also, then\nthese models that we have for the mean and\nthe variation, are they--",
    "start": "791250",
    "end": "800790"
  },
  {
    "text": "is this one model or\nfor each conditional, is it a different model? Yeah, so the question is, I\nguess, what sort of mu's and z--",
    "start": "800790",
    "end": "812430"
  },
  {
    "text": "what kind of functions do we\nuse here and are they different for every z?",
    "start": "812430",
    "end": "817500"
  },
  {
    "text": "In this case, the\nfunctions are the same. So there is a\nsingle function that",
    "start": "817500",
    "end": "822540"
  },
  {
    "text": "is then going to give you\ndifferent outputs when you fit in the different z values.",
    "start": "822540",
    "end": "827889"
  },
  {
    "text": "So the functions are fixed. The other question is, well,\ndoes it have to be a Gaussian?",
    "start": "827890",
    "end": "833620"
  },
  {
    "text": "Not necessarily. You can use an autoregressive\nmodel there if you wanted to.",
    "start": "833620",
    "end": "839500"
  },
  {
    "text": "The strategy behind the\nlatent variable model is to usually choose these\nconditionals to be simple",
    "start": "839500",
    "end": "846340"
  },
  {
    "text": "because, again, you have this\nclustering kind of behavior and so you might\nbe able to get away with a simple p of x given z.",
    "start": "846340",
    "end": "852220"
  },
  {
    "text": "But you can certainly-- this is sort of like the mix and\nmatch kind part of this course,",
    "start": "852220",
    "end": "859040"
  },
  {
    "text": "you can get a different\nkind of generative model by replacing these\np of x given z",
    "start": "859040",
    "end": "864130"
  },
  {
    "text": "with an autoregressive\nmodel that gives you even more flexibility. But the story behind the\nvariational autoencoder",
    "start": "864130",
    "end": "871650"
  },
  {
    "text": "is to keep that simple. Maybe I don't fully\nunderstand but can you clarify the big\npicture motivation",
    "start": "871650",
    "end": "877920"
  },
  {
    "text": "for why we want to model p of x\ngiven z and p of x in this case. So the goal is, as usual--\nso the question is, why do we",
    "start": "877920",
    "end": "884490"
  },
  {
    "text": "need p of x given z and p of x? So the goal is to always\njust model p of x. So that's the same as in\nthe autoregressive model,",
    "start": "884490",
    "end": "891300"
  },
  {
    "text": "you want to be able to fit\na probability distribution over these x variables, which\nare the ones you have access to,",
    "start": "891300",
    "end": "897150"
  },
  {
    "text": "the pixels, whatever. The motivation for\nusing the z variable is that, well, one, it\nmight make your life easier",
    "start": "897150",
    "end": "904860"
  },
  {
    "text": "in the sense that\nif you somehow are able to cluster the data using\nthe z variables, then learning",
    "start": "904860",
    "end": "911430"
  },
  {
    "text": "becomes easier. The second one is that being\nable to infer the latent",
    "start": "911430",
    "end": "917670"
  },
  {
    "text": "variables might be\nuseful in itself because, really, maybe what\nyou're after is not generating",
    "start": "917670",
    "end": "922770"
  },
  {
    "text": "images but understanding\nwhat sort of latent factors or variations exist\nin your data set.",
    "start": "922770",
    "end": "930230"
  },
  {
    "text": "And so-- [INAUDIBLE] The prior here in this\ncase is very simple. It's just a Gaussian.",
    "start": "930230",
    "end": "936310"
  },
  {
    "text": "But yeah, you could have more\ncomplicated priors, of course. [INAUDIBLE] in the\nprevious slides that how",
    "start": "936310",
    "end": "942279"
  },
  {
    "text": "z can be intuitively thought\nof as unique features. Over here, is it a way\nto get a sense of what",
    "start": "942280",
    "end": "948970"
  },
  {
    "text": "these learned features are? Yeah, so that's what I was\nsaying, sort of that you just",
    "start": "948970",
    "end": "955779"
  },
  {
    "text": "hope that you discover\nsomething meaningful. You can certainly test. Once you've trained\nthe model, you",
    "start": "955780",
    "end": "961300"
  },
  {
    "text": "can certainly change, let's\nsay, one of the z variables and see how that affects the\nimages that you generate.",
    "start": "961300",
    "end": "968595"
  },
  {
    "text": "So you can certainly test\nwhether you've discovered something meaningful or not. ",
    "start": "968595",
    "end": "975000"
  },
  {
    "text": "It might not be-- whether you discover\nsomething meaningful or is not guaranteed by\nthe learning objective.",
    "start": "975000",
    "end": "983080"
  },
  {
    "text": "Is the number of\nlatent variables just a hyperparameter\nat the end of training? So the question is that is\nthe number of latent variables",
    "start": "983080",
    "end": "989950"
  },
  {
    "text": "a hyperparameter? Yes. So in this model\nfor learning how",
    "start": "989950",
    "end": "995519"
  },
  {
    "text": "to model p of x and\nthen conditioning on z. [INAUDIBLE]",
    "start": "995520",
    "end": "1001470"
  },
  {
    "text": " How are we supposed to sample\nand then do estimation?",
    "start": "1001470",
    "end": "1007338"
  },
  {
    "text": "So the question is, if we\nuse this kind of model, how do we sample? How do we do density estimation? So sampling is easy because what\nyou do is you first sample--",
    "start": "1007338",
    "end": "1014930"
  },
  {
    "text": "this is-- you can think of it as an\nautoregressive model over two groups of variables,\nthe z's and the x's.",
    "start": "1014930",
    "end": "1021290"
  },
  {
    "text": "And so what you would do is\nyou would first choose a latent factor variation. So you sample z from a Gaussian,\nwhich we know how to do.",
    "start": "1021290",
    "end": "1027709"
  },
  {
    "text": "It's trivial. Then you feed z through these\ntwo neural networks and you get a mean and and\na covariance matrix.",
    "start": "1027710",
    "end": "1035214"
  },
  {
    "text": "Then that defines\nanother Gaussian, then you sample\nfrom that Gaussian. So sampling is very easy.",
    "start": "1035215",
    "end": "1041689"
  },
  {
    "text": "Evaluating p of x, as we'll\nsee, that's the challenge. That's kind of like\nthe no free lunch part.",
    "start": "1041690",
    "end": "1046730"
  },
  {
    "text": "Everything seems great\nexcept that evaluating p of x, which is kind of doing\nthe estimation, becomes hard.",
    "start": "1046730",
    "end": "1053149"
  },
  {
    "text": "And that's what's\ngoing to come up next.  Yeah.",
    "start": "1053150",
    "end": "1059020"
  },
  {
    "text": "This does not seem like\nend-to-end differentiable, right? Yeah. So the question is, is this\nend-to-end differentiable?",
    "start": "1059020",
    "end": "1064660"
  },
  {
    "text": "How do you train it? Yeah, that's going to be\nthe topic of this lecture. Yeah.",
    "start": "1064660",
    "end": "1070720"
  },
  {
    "text": "All right, so let's see, first,\nas a warmup, the simplest",
    "start": "1070720",
    "end": "1076090"
  },
  {
    "text": "kind of latent variable model\nthat you can think of which you might have seen before, that's\nthe mixture of Gaussians, right.",
    "start": "1076090",
    "end": "1082360"
  },
  {
    "text": "So again, we have this\nsimple Bayes' net z pointing to x and you can think\nof a mixture of Gaussian",
    "start": "1082360",
    "end": "1092320"
  },
  {
    "text": "as being a shallow\nlatent variable model where there is no deep\nneural network involved.",
    "start": "1092320",
    "end": "1098620"
  },
  {
    "text": "In this case, z is just a\ncategorical random variable, which determines the\nmixture component.",
    "start": "1098620",
    "end": "1104950"
  },
  {
    "text": "Let's say there is\nk mixtures here. And then p of x given\nz, again, is a Gaussian,",
    "start": "1104950",
    "end": "1110529"
  },
  {
    "text": "and then you have some\nkind of lookup table here that would tell you\nwhat is the mean and what is the covariance\nfor mixture component k?",
    "start": "1110530",
    "end": "1120850"
  },
  {
    "text": "There's k mixtures. And so you have k means\nand k covariances, and that defines a\ngenerative model.",
    "start": "1120850",
    "end": "1127679"
  },
  {
    "text": "So to sample. Again, you would sample first\na mixture component, the z, and then you would\nsample x from a Gaussian",
    "start": "1127680",
    "end": "1133130"
  },
  {
    "text": "with the corresponding\nmean and covariance. And so it would look\nsomething like this.",
    "start": "1133130",
    "end": "1139520"
  },
  {
    "text": "So if x is two-dimensional,\nso there is x1 and x2,",
    "start": "1139520",
    "end": "1144640"
  },
  {
    "text": "then each of these\nGaussians would be a two-dimensional\nGaussian, and these Gaussians",
    "start": "1144640",
    "end": "1151120"
  },
  {
    "text": "will have different means,\nsay, mu1, mu2, and mu3. They will have\ndifferent covariances",
    "start": "1151120",
    "end": "1156370"
  },
  {
    "text": "and so it might look\nsomething like this. ",
    "start": "1156370",
    "end": "1163640"
  },
  {
    "text": "So the generative process,\nagain, is you pick a component and then you sample a data\npoint from that Gaussian.",
    "start": "1163640",
    "end": "1168669"
  },
  {
    "text": "So maybe you uniformly pick or\nwhatever is the prior over z. Maybe you sample k, you\ncan sample z, you get two,",
    "start": "1168670",
    "end": "1177380"
  },
  {
    "text": "and then you have to pick a\npoint distributed according to a Gaussian with mean here\nand covariance shaped like that.",
    "start": "1177380",
    "end": "1185340"
  },
  {
    "text": "And so forth.  This is useful,\nagain, kind of like",
    "start": "1185340",
    "end": "1191940"
  },
  {
    "text": "if you think about the\nclustering interpretation. You can think of\nthis kind of model as giving you one way of\nperforming clustering, which",
    "start": "1191940",
    "end": "1202679"
  },
  {
    "text": "is sort of like a basic kind\nof unsupervised learning task.",
    "start": "1202680",
    "end": "1207810"
  },
  {
    "text": "This is an example where you\nhave a data set collected",
    "start": "1207810",
    "end": "1212820"
  },
  {
    "text": "for the Old Faithful geyser\nand Yellowstone National Park. And then each data\npoint here corresponds",
    "start": "1212820",
    "end": "1219780"
  },
  {
    "text": "to an eruption of the geyser. And then you can see\nthere's two features here, the duration of the eruption and\nthe time between two eruptions.",
    "start": "1219780",
    "end": "1228180"
  },
  {
    "text": "And the data set\nlooks like this. So you can see there is\nsome kind of relationship between these two things.",
    "start": "1228180",
    "end": "1233640"
  },
  {
    "text": "And the larger the interval\nbetween two eruptions, the longer then the\nfollowing eruption is.",
    "start": "1233640",
    "end": "1240560"
  },
  {
    "text": "And you try to model this\nusing a single Gaussian. If you fit the parameters,\nit's going to look like this.",
    "start": "1240560",
    "end": "1246870"
  },
  {
    "text": "You're going to\nput the mean here and you're going to choose a\ncovariance that kind of captures that correlation\nbetween the features.",
    "start": "1246870",
    "end": "1253020"
  },
  {
    "text": "And you can see it's\nnot doing a great job. You're putting a lot\nof probability mass here where there\nis no actual data.",
    "start": "1253020",
    "end": "1260950"
  },
  {
    "text": "But that's the best you can\ndo if you're forced to pick a Gaussian as your model.",
    "start": "1260950",
    "end": "1266045"
  },
  {
    "text": "But if you look at\nthe data, it kind of looks like there is\ntwo types of eruptions. There is type 1 that\nbehaves like this,",
    "start": "1266045",
    "end": "1272200"
  },
  {
    "text": "and type 2 that\nbehaves like this. And so you're going to get a\nmuch better fit to the data if you have two Gaussians,\na mixture of two Gaussians,",
    "start": "1272200",
    "end": "1279700"
  },
  {
    "text": "that kind of look like that. And if you can somehow fit\nthis model automatically,",
    "start": "1279700",
    "end": "1286659"
  },
  {
    "text": "then by inferring the z variable\ngiven the x, figuring out",
    "start": "1286660",
    "end": "1291670"
  },
  {
    "text": "if a point belongs to the\nblue or the red mixture, you can identify which type of\neruption you're dealing with.",
    "start": "1291670",
    "end": "1301060"
  },
  {
    "text": "Again, this is really this\nidea of identifying features based on the observed data.",
    "start": "1301060",
    "end": "1307700"
  },
  {
    "text": "And again, you can see that\nthis is kind of ill-posed because it's\nunsupervised learning and we're hoping to discover\nsome meaningful structure,",
    "start": "1307700",
    "end": "1315529"
  },
  {
    "text": "but it's not clear that\nthis is always possible. It's not clear what it\nmeans to find good structure",
    "start": "1315530",
    "end": "1321370"
  },
  {
    "text": "or what's a good cluster-- clustering, right. You might have\ndifferent definitions",
    "start": "1321370",
    "end": "1326830"
  },
  {
    "text": "of what a good clustering is and\nthis will give you a clustering, whether it is the\none that you want",
    "start": "1326830",
    "end": "1332980"
  },
  {
    "text": "or the best one\nis not guaranteed. ",
    "start": "1332980",
    "end": "1339180"
  },
  {
    "text": "And so yeah, you can\nimagine that you can use it to do unsupervised learning. You can have-- if you have\nmore mixture components,",
    "start": "1339180",
    "end": "1344545"
  },
  {
    "text": "you have a data set\nthat looks like this, then you might want to\nmodel it, let's say, using",
    "start": "1344545",
    "end": "1349830"
  },
  {
    "text": "a mixture of three Gaussians. And again, identifying\nthe mixture component,",
    "start": "1349830",
    "end": "1355090"
  },
  {
    "text": "which is the color\nhere, will tell you for which component the\ndata point is coming from.",
    "start": "1355090",
    "end": "1361770"
  },
  {
    "text": "And it tells you something\nabout how to cluster the data points in different ways. ",
    "start": "1361770",
    "end": "1368900"
  },
  {
    "text": "Would it be reasonable to\nguess that this would fail very hard on image classification?",
    "start": "1368900",
    "end": "1376470"
  },
  {
    "text": "So the question is, will\nthis fail very hard on image? Probably. You wouldn't expect.",
    "start": "1376470",
    "end": "1383150"
  },
  {
    "text": "Unless k is extremely large if\nyou have, say, a mixture of two",
    "start": "1383150",
    "end": "1388460"
  },
  {
    "text": "Gaussians, then you would-- let's say if you have\na single Gaussian, then you would choose the mean\nto be the mean of all the images",
    "start": "1388460",
    "end": "1394768"
  },
  {
    "text": "and then you put some kind\nof standard deviation, and you can imagine you're\ngoing to get a blob. It's not going to be very good.",
    "start": "1394768",
    "end": "1399980"
  },
  {
    "text": "Even if you are able to divide\nyour training set into two groups and fit two\nseparate Gaussians, it's still not going\nto work very well.",
    "start": "1399980",
    "end": "1407090"
  },
  {
    "text": "If k becomes extremely\nlarge, in theory, you can approximate anything\nso eventually it would work.",
    "start": "1407090",
    "end": "1413210"
  },
  {
    "text": "But yeah, in practice,\nit would require a k that is extremely large. ",
    "start": "1413210",
    "end": "1421140"
  },
  {
    "text": "Cool. And here is actually an\nexample on image data.",
    "start": "1421140",
    "end": "1427230"
  },
  {
    "text": "Again, this is on MNIST and\nthis is the latent space, z.",
    "start": "1427230",
    "end": "1432630"
  },
  {
    "text": "This is a projection of that. But you can imagine that one\naxis is z1, another axis is z2.",
    "start": "1432630",
    "end": "1439800"
  },
  {
    "text": "And then you take your\nMNIST data set and then you try to figure out\nwhere it lands in z.",
    "start": "1439800",
    "end": "1446880"
  },
  {
    "text": "Each data point where\nit lands in z space. And you can kind of\nsee that, again, it's",
    "start": "1446880",
    "end": "1452760"
  },
  {
    "text": "able to do some reasonable\nclustering in the sense that data points that actually\nbelong to the same class, which",
    "start": "1452760",
    "end": "1460380"
  },
  {
    "text": "was not known to the\ngenerative model, for example, red points\nhere corresponds to digit 2,",
    "start": "1460380",
    "end": "1466293"
  },
  {
    "text": "and you can see that\nthey are kind of like all grouped together. They all have similar z values\nafter training this model.",
    "start": "1466293",
    "end": "1474169"
  },
  {
    "text": "And so there's not a single\ncluster here for the 2s. There is two of them.",
    "start": "1474170",
    "end": "1480640"
  },
  {
    "text": "Maybe the points in this cluster\nhave a slightly different style",
    "start": "1480640",
    "end": "1485650"
  },
  {
    "text": "than the points in this cluster. I mean, it's hard to say\nexactly what the clustering is doing here. And again, it hints at the\nfact that unsupervised learning",
    "start": "1485650",
    "end": "1494260"
  },
  {
    "text": "is hard. But this is sort of\nlike the intuition for what you might hope\nto get if you try to do",
    "start": "1494260",
    "end": "1499750"
  },
  {
    "text": "this on an image data set. You might hope to be able to\ndiscover different classes. You might be able\n[INAUDIBLE] different styles.",
    "start": "1499750",
    "end": "1506110"
  },
  {
    "text": "And you're hoping to\ndiscover that automatically by fitting a latent\nvariable model and just looking at the kind\nof z's that you discover.",
    "start": "1506110",
    "end": "1514296"
  },
  {
    "text": "[INAUDIBLE] ",
    "start": "1514296",
    "end": "1519450"
  },
  {
    "text": "Model, how you actually\nlearn these posteriors. Yeah. So the question is,\nhow do you learn them?",
    "start": "1519450",
    "end": "1524710"
  },
  {
    "text": "And I haven't talked about it. It's going to be the-- we're going to go through\nthat in this lecture.",
    "start": "1524710",
    "end": "1530662"
  },
  {
    "text": "[INAUDIBLE] Trained the model before doing\nthe dimensionality reduction into two dimensions, and\nthen you plot it [INAUDIBLE]..",
    "start": "1530662",
    "end": "1538420"
  },
  {
    "text": "It actually looks better. It looks like it\nwould make more sense if you did the dimensionality\nreduction first and then tried",
    "start": "1538420",
    "end": "1546767"
  },
  {
    "text": "to learn a mixture of Gaussians. It looks more like it could\nbe expressed that way. So there is no mixture of\nGaussians learned here.",
    "start": "1546767",
    "end": "1553990"
  },
  {
    "text": "So this is sort of showing-- this is more like the\nresults of training a deep generative models\nwhere the z's are actually not",
    "start": "1553990",
    "end": "1562380"
  },
  {
    "text": "even categorical. The z's variables here\nare more like Gaussian or [? real-valued. ?]",
    "start": "1562380",
    "end": "1569470"
  },
  {
    "text": "And so what I'm plotting here\nis for each data point, what is the corresponding\ninferred value of z, which",
    "start": "1569470",
    "end": "1577980"
  },
  {
    "text": "is no longer a number,\nit's kind of like a point in this two-dimensional space. And it just so\nhappened that it's",
    "start": "1577980",
    "end": "1584130"
  },
  {
    "text": "finding something reasonable. But again, it's not guaranteed.",
    "start": "1584130",
    "end": "1589794"
  },
  {
    "text": " Yeah. So when we start\nsampling from a VAE,",
    "start": "1589794",
    "end": "1595165"
  },
  {
    "text": "do we normalize it to a mean and\nstandard deviation of 0 and 1?",
    "start": "1595165",
    "end": "1600310"
  },
  {
    "text": "But in this case, if we can\ndo dimensionality reduction, we then fit a model on this to\nthen have a [INAUDIBLE] sample",
    "start": "1600310",
    "end": "1608860"
  },
  {
    "text": "from this. Not normal. Yeah. So I think the question\nis whether, I guess, in the model I had\nhere, the p of z",
    "start": "1608860",
    "end": "1616690"
  },
  {
    "text": "was a simple distribution,\nlike a Gaussian. And perhaps if you look\nat this latent space,",
    "start": "1616690",
    "end": "1622570"
  },
  {
    "text": "for example, which is\nactually not a VAE, I think, but then that's why it\nmight not look like a Gaussian.",
    "start": "1622570",
    "end": "1627830"
  },
  {
    "text": "It has a bunch of holes\nso you might be better off having a mixture of Gaussians,\nfor example, for p of z.",
    "start": "1627830",
    "end": "1633340"
  },
  {
    "text": "And you might actually\ntry to learn the p of z as part of the model. And you can certainly do that. ",
    "start": "1633340",
    "end": "1641550"
  },
  {
    "text": "Cool. So an alternative\nmotivation is that it's a very powerful way of\ncombining simple models",
    "start": "1641550",
    "end": "1648690"
  },
  {
    "text": "and get a more\nexpressive one out. Using latent\nvariables allows you to have this kind of\nmixture model behavior which",
    "start": "1648690",
    "end": "1656940"
  },
  {
    "text": "is a very good way of building\nvery flexible generative models.",
    "start": "1656940",
    "end": "1663100"
  },
  {
    "text": "And you can see in the example\nof the mixture of Gaussian,",
    "start": "1663100",
    "end": "1668410"
  },
  {
    "text": "if you have three Gaussians,\na blue, an orange, and a green one,\nand you can see they",
    "start": "1668410",
    "end": "1673910"
  },
  {
    "text": "have different means and\ndifferent standard deviations so they have all\nthese bell curves. If you think about the\ncorresponding marginal",
    "start": "1673910",
    "end": "1681280"
  },
  {
    "text": "distribution over x has this\nvery interesting red shape.",
    "start": "1681280",
    "end": "1687230"
  },
  {
    "text": "So even though each of\nthe individual components is pretty simple,\nit's just a bell curve and there's not too much\nyou can do about changing",
    "start": "1687230",
    "end": "1694090"
  },
  {
    "text": "the shape of the function. The moment you start\nmixing them together, you can get much more\ninteresting shapes",
    "start": "1694090",
    "end": "1700090"
  },
  {
    "text": "for the probability\ndensity that you get. And the reason is\nthat when you want",
    "start": "1700090",
    "end": "1706279"
  },
  {
    "text": "to evaluate the probability\nunder this mixture model, the probability of a data\npoint, x, what is that object?",
    "start": "1706280",
    "end": "1714299"
  },
  {
    "text": "What is the marginal? You basically need\nto say, what was the probability of generating\nthat point under the blue curve,",
    "start": "1714300",
    "end": "1720530"
  },
  {
    "text": "plus the probability\nof generating that point under the orange\ncurve, and plus the probability under the green curve?",
    "start": "1720530",
    "end": "1728000"
  },
  {
    "text": "And this is just the definition\nof the marginal probability.",
    "start": "1728000",
    "end": "1733200"
  },
  {
    "text": "You marginalize out the z. In this case, the\njoint is just something that looks like\nthis, where p of z",
    "start": "1733200",
    "end": "1739549"
  },
  {
    "text": "is just a categorical\ndistribution. And the p of x given z is,\nagain, something very simple.",
    "start": "1739550",
    "end": "1744950"
  },
  {
    "text": "Just a Gaussian\nwith different means and different\nstandard deviations. And so you can see that\neven though the components,",
    "start": "1744950",
    "end": "1751880"
  },
  {
    "text": "the p of x given z's, are\nsuper simple, just Gaussians, the marginal that you get is\nmuch more interesting in terms",
    "start": "1751880",
    "end": "1759380"
  },
  {
    "text": "of the shape that you can get.  And that's sort of\none way to think",
    "start": "1759380",
    "end": "1767170"
  },
  {
    "text": "about why this variational\nautoencoders are so powerful because\nit's basically the same thing,\nexcept that now you",
    "start": "1767170",
    "end": "1773620"
  },
  {
    "text": "don't have a finite number\nof mixture components. So the z variable is no longer a\ncategorical random variable, 1,",
    "start": "1773620",
    "end": "1781029"
  },
  {
    "text": "2, 3, 4, 5k. Now, z can take an infinite\nnumber of different values.",
    "start": "1781030",
    "end": "1786640"
  },
  {
    "text": "There is a Gaussian distribution\nthat you sample z from. So there is-- essentially,\nyou can think of it",
    "start": "1786640",
    "end": "1792250"
  },
  {
    "text": "as an infinite number\nof mixture components. So even though p of x given\nz is, again, a Gaussian,",
    "start": "1792250",
    "end": "1799789"
  },
  {
    "text": "now we have a mixture of an\ninfinite number of Gaussians.",
    "start": "1799790",
    "end": "1804900"
  },
  {
    "text": "And what we're giving up is that\nin this Gaussian mixture model",
    "start": "1804900",
    "end": "1813140"
  },
  {
    "text": "case, we were able\nto choose the mean and the standard deviations\nof these Gaussians any way",
    "start": "1813140",
    "end": "1818299"
  },
  {
    "text": "we wanted because you\nbasically have a lookup table. And so you have\ncomplete flexibility",
    "start": "1818300",
    "end": "1823669"
  },
  {
    "text": "in choosing the mean and\nthe standard deviation of the Gaussians. In the VAE world, the means\nand the standard deviations",
    "start": "1823670",
    "end": "1831110"
  },
  {
    "text": "of all these Gaussians\nare not arbitrary.",
    "start": "1831110",
    "end": "1836120"
  },
  {
    "text": "They are chosen by feeding z\nthrough this neural network.",
    "start": "1836120",
    "end": "1841520"
  },
  {
    "text": "Through two neural networks,\nlet's say mu and sigma, that will basically give you the\nmean and the standard deviation",
    "start": "1841520",
    "end": "1849440"
  },
  {
    "text": "for that Gaussian component. There's no longer\na lookup table. Now, it's whatever you can\ndescribe using a neural network.",
    "start": "1849440",
    "end": "1859680"
  },
  {
    "text": "[INAUDIBLE] ",
    "start": "1859680",
    "end": "1869200"
  },
  {
    "text": "So there is still-- basically, z can take an\ninfinite number of values because this continues.",
    "start": "1869200",
    "end": "1876060"
  },
  {
    "text": "[INAUDIBLE] Before, it could only\ntake k different values. Yeah. ",
    "start": "1876060",
    "end": "1882339"
  },
  {
    "text": "[INAUDIBLE]  The question is that can't we\njust use a uniform distribution?",
    "start": "1882339",
    "end": "1889690"
  },
  {
    "text": "Yeah, you can. This is just-- I'm just\nshowing a Gaussian. But yeah, a uniform\ndistribution would work as well.",
    "start": "1889690",
    "end": "1895180"
  },
  {
    "text": "Yeah. You said z is the input\nto neural networks. Does that mean we\nfirst sample from z",
    "start": "1895180",
    "end": "1900280"
  },
  {
    "text": "and then input into\nthe neural network? To sample-- the\nprocess to sample, it would be the same as before.",
    "start": "1900280",
    "end": "1905782"
  },
  {
    "text": "Like in the Gaussian\nmixture model, you pick a component,\nsample a value of z.",
    "start": "1905782",
    "end": "1911020"
  },
  {
    "text": "Then you do your lookup,\nyou get the mean, the standard deviation, and then\nyou sample from the Gaussian. The sampling process\nhere is the same.",
    "start": "1911020",
    "end": "1917500"
  },
  {
    "text": "You sample a z, now\nit's a Gaussian. Then you feed it through\nthe two neural networks to get the means and\nthe standard deviations",
    "start": "1917500",
    "end": "1924288"
  },
  {
    "text": "of the corresponding\nGaussian, and then you sample from p of x given z. So you sample from a Gaussian\nwith that mean and standard",
    "start": "1924288",
    "end": "1930460"
  },
  {
    "text": "deviation. I'm a little confused about\nwhy the mixture makes sense.",
    "start": "1930460",
    "end": "1936380"
  },
  {
    "text": "Let's say you have a unique\ncluster corresponding to 6 and another corresponding\nto 7, and now you're",
    "start": "1936380",
    "end": "1941600"
  },
  {
    "text": "creating a mixture that's\nhalfway in between. But in reality, there's\nprobably no digit that's halfway between 6 and 7.",
    "start": "1941600",
    "end": "1947360"
  },
  {
    "text": "So why does this work\nbetter in practice? Why does it work better on\nthe finite mixture component?",
    "start": "1947360",
    "end": "1953250"
  },
  {
    "text": "Yeah. So the question is, would you\nwant z to be discrete, I guess, or continuous?",
    "start": "1953250",
    "end": "1958380"
  },
  {
    "text": "And if you're trying to model\ndiscrete sort of clusters, is this the right\nway of doing things?",
    "start": "1958380",
    "end": "1964610"
  },
  {
    "text": "And yeah, you're right. That's sort of like\nhere z is continuous so you have some\nway of transitioning",
    "start": "1964610",
    "end": "1970250"
  },
  {
    "text": "between the clusters, which\nmight or might not make sense. And it might need to find\nstrange axis of variation",
    "start": "1970250",
    "end": "1977930"
  },
  {
    "text": "to make that happen. You can also have a mixture\nof discrete and continuous, like this is the\nsetting that VAE uses",
    "start": "1977930",
    "end": "1985820"
  },
  {
    "text": "and it tends to work\nwell in practice. [INAUDIBLE] off of\nthat, what does it mean, intuitively, to have a\ndifferent number of clusters?",
    "start": "1985820",
    "end": "1994029"
  },
  {
    "text": "Well, I mean,\nintuitively, it just means that z is no longer a\ncategorical random variable",
    "start": "1994030",
    "end": "1999370"
  },
  {
    "text": "so there's not a finite number\nof choices that you can make, but it's more like what I had\nhere where the z variables can",
    "start": "1999370",
    "end": "2008100"
  },
  {
    "text": "take values in these 2D space. And so there is not\nreally even necessarily a notion of where you have\nto pick either here or here,",
    "start": "2008100",
    "end": "2016350"
  },
  {
    "text": "you can be in between. It's like a point [INAUDIBLE] ",
    "start": "2016350",
    "end": "2026470"
  },
  {
    "text": "Yeah. Since z is described by\na Gaussian distribution, does that mean\nlike it's mean it's",
    "start": "2026470",
    "end": "2031679"
  },
  {
    "text": "like some average of the\npossible latent variables or something that-- of all [INAUDIBLE]?",
    "start": "2031680",
    "end": "2037881"
  },
  {
    "text": "Can you repeat the question. Yeah. Can you interpret the mean\nof z as some average latent",
    "start": "2037881",
    "end": "2043710"
  },
  {
    "text": "representation of all the data? Yeah, in some sense, yes. And then usually\nthe average is 0,",
    "start": "2043710",
    "end": "2048840"
  },
  {
    "text": "so you're basically forcing\nthe average of all the latent representations of the data\npoints to be centered at 0.",
    "start": "2048840",
    "end": "2055079"
  },
  {
    "text": "So yeah. I mean, it depends on\nthe training objective. But using the training\nobjective that we will see in this lecture,\nyes, that would be the effect.",
    "start": "2055080",
    "end": "2064929"
  },
  {
    "text": "Yeah. When you had a categorical\nrepresentation for z, for the distribution, was it\nlike a uniform probability",
    "start": "2064929",
    "end": "2072780"
  },
  {
    "text": "across the categories? Yeah, the question is, in\na mixture of Gaussians,",
    "start": "2072780",
    "end": "2078480"
  },
  {
    "text": "does Z have to be a\nuniform distribution? It doesn't have to. It can have-- I was being curious why--",
    "start": "2078480",
    "end": "2084960"
  },
  {
    "text": "how it works for z to be a\nnormal distribution here, or versus a uniform,\nor how you're choosing",
    "start": "2084960",
    "end": "2091710"
  },
  {
    "text": "z to be a normal distribution. Yeah. So that's kind of like, to some\nextent, an arbitrary choice,",
    "start": "2091710",
    "end": "2097320"
  },
  {
    "text": "you can choose other priors. The key insight is\nthat it just has to be something simple\nthat you're going",
    "start": "2097320",
    "end": "2103290"
  },
  {
    "text": "to sample from efficiently. The conditional,\nthe p of x given z, are also something simple.",
    "start": "2103290",
    "end": "2108930"
  },
  {
    "text": "Here, I'm using a Gaussian, but\nyou'd use a factorized logistic, or it can be anything\nas long as it's simple.",
    "start": "2108930",
    "end": "2116549"
  },
  {
    "text": "And all the complexity is\nreally in these neural networks that would figure out\nhow to map the latent",
    "start": "2116550",
    "end": "2122580"
  },
  {
    "text": "variable to the parameters\nof this simple distribution. And you're going to get\ndifferent results depending on the choices you make.",
    "start": "2122580",
    "end": "2128950"
  },
  {
    "text": "This is sort of like\nthe simplest Gaussian and [INAUDIBLE]. But yeah. In a GMM, do we have a neural\nnetwork for each value of z",
    "start": "2128950",
    "end": "2138810"
  },
  {
    "text": "or do we have just a\nsingle neural network that you're [INAUDIBLE]. One for the mean, one\nfor standard deviaton.",
    "start": "2138810",
    "end": "2145178"
  },
  {
    "text": "In a GMM, there is\nno neural network. It's just a loop. So it's the most\nflexible kind of mapping",
    "start": "2145178",
    "end": "2151851"
  },
  {
    "text": "can think of because\nyou're allowed to choose any value you want\nfor the different values",
    "start": "2151852",
    "end": "2158170"
  },
  {
    "text": "that z can take. So it's more like a\nBayesian network world where you're allowed-- it's a lookup table.",
    "start": "2158170",
    "end": "2164510"
  },
  {
    "text": "Which is great because\nit's super flexible, it's bad because it\ndoesn't scale the moment you have many, many--",
    "start": "2164510",
    "end": "2170560"
  },
  {
    "text": "How do you actually\nfeed those values? How to feed it? Yeah, we'll see that soon. So just to clarify, are we\ngoing to update the prior z",
    "start": "2170560",
    "end": "2179650"
  },
  {
    "text": "while we are fitting the model? You could. In this world, I'm assuming\nthat the prior is fixed.",
    "start": "2179650",
    "end": "2186477"
  },
  {
    "text": "There is no learnable parameter. As usual, the\nlearnable parameters are the theta,\nwhich, in this case, would be the parameters of\nthese two neural networks.",
    "start": "2186477",
    "end": "2193580"
  },
  {
    "text": "So you might have\na very simple-- a very shallow linear neural\nnetwork where to get the mean,",
    "start": "2193580",
    "end": "2199900"
  },
  {
    "text": "you just take a linear\ncombination of the z variables and then you apply\nsome non-linearity. And then similarly, another\nsimple neural network",
    "start": "2199900",
    "end": "2207760"
  },
  {
    "text": "that would give\nyou the parameters of the covariance matrix. Perhaps you can make a diagonal\nor something like this.",
    "start": "2207760",
    "end": "2215258"
  },
  {
    "text": "I just want to\nclarify the intuition. So in the mixture, you're\nsaying you're summing over a bunch of normals? Switching over\nhere, you're saying,",
    "start": "2215258",
    "end": "2221440"
  },
  {
    "text": "you can represent that\nsum as a single normal, and you're directly predicting\nthe mean and variance",
    "start": "2221440",
    "end": "2227859"
  },
  {
    "text": "for that normal to the matrices? So the marginal--\nlet's see if I have it. So the marginal basically\nbecomes an integral.",
    "start": "2227860",
    "end": "2235750"
  },
  {
    "text": "So instead of being a sum\nover all possible values of z, you have an integral over\nall the possible values",
    "start": "2235750",
    "end": "2241270"
  },
  {
    "text": "that z can take. But it's the same machinery. Yeah.",
    "start": "2241270",
    "end": "2246799"
  },
  {
    "text": "[INAUDIBLE] is a dimension\nof much lower or much higher than x? Good question.",
    "start": "2246800",
    "end": "2251930"
  },
  {
    "text": "Yeah. So what is the dimension\nof z in practice? Typically, the dimension\nof z would be much lower",
    "start": "2251930",
    "end": "2257030"
  },
  {
    "text": "than the dimensionality of x. And the hope is that,\nyeah, you might discover a small number of latent\nfactors of variation",
    "start": "2257030",
    "end": "2263780"
  },
  {
    "text": "that describe your data.  Oh, the training?",
    "start": "2263780",
    "end": "2268800"
  },
  {
    "text": "I will talk about it soon. Yeah. Does it even make sense\nfor z to be greater than-- the dimensionality of\nz to be greater than x?",
    "start": "2268800",
    "end": "2275770"
  },
  {
    "text": "That's it. Yeah. So we'll see another generative\nmodel that will basically be identical to this, except\nthat z will have exactly",
    "start": "2275770",
    "end": "2283710"
  },
  {
    "text": "the same dimensionality of x. And that's, for example,\nwhat a diffusion model does.",
    "start": "2283710",
    "end": "2289319"
  },
  {
    "text": "So you might not\nnecessarily always want to reduce the dimensionality. Having the same\ndimensionality will",
    "start": "2289320",
    "end": "2295020"
  },
  {
    "text": "allow you to have nice\ncomputational properties. ",
    "start": "2295020",
    "end": "2302170"
  },
  {
    "text": "Yeah. Is it possible to have a more-- put more information into\nthe prior as opposed to just,",
    "start": "2302170",
    "end": "2310400"
  },
  {
    "text": "I don't know, something\nfrom standard Gaussian? Yes. Yes. You can certainly\nput more information. And I think there are two\nways to think about it.",
    "start": "2310400",
    "end": "2317212"
  },
  {
    "text": "One is if the prior is\nmore complex, instead of having a Gaussian, you can\nput an autoregressive model",
    "start": "2317212",
    "end": "2322630"
  },
  {
    "text": "over the latent variables. You're going to get an even more\nflexible kind of distribution.",
    "start": "2322630",
    "end": "2327920"
  },
  {
    "text": "The other way to do it\nwould be if you already have some prior knowledge\nabout the types of--",
    "start": "2327920",
    "end": "2333490"
  },
  {
    "text": "maybe that there is\na bunch of classes, there's 1,000 classes\nor 10 classes, then maybe you want to have one\ncategorical random variable.",
    "start": "2333490",
    "end": "2341510"
  },
  {
    "text": "And so if you have some prior\nover what kind of latent factors",
    "start": "2341510",
    "end": "2346720"
  },
  {
    "text": "of variation you expect\nto exist in the data, you can try to capture that\nby choosing suitable priors.",
    "start": "2346720",
    "end": "2353710"
  },
  {
    "text": "So in your shallow\nlatent variable models, your number of\nGaussians is equal to k.",
    "start": "2353710",
    "end": "2359380"
  },
  {
    "text": "Here, since this is a normal\ndistribution, your number of k is less [INAUDIBLE].",
    "start": "2359380",
    "end": "2365115"
  },
  {
    "text": "Yeah. Yeah. Exactly.  Cool.",
    "start": "2365115",
    "end": "2370350"
  },
  {
    "text": "And then so what\nyou would do then is you would somehow try\nto fit this model to data.",
    "start": "2370350",
    "end": "2375849"
  },
  {
    "text": "And in this case, the parameters\nthat you can choose from-- that you can choose are\nall this neural network.",
    "start": "2375850",
    "end": "2382455"
  },
  {
    "text": "The parameters of what these two\nneural networks, mu and sigma. ",
    "start": "2382455",
    "end": "2387630"
  },
  {
    "text": "And again, the takeaway is the\nsame as the Gaussian mixture model. Even though p of x\ngiven z is super simple,",
    "start": "2387630",
    "end": "2394870"
  },
  {
    "text": "it's just a Gaussian,\nthe marginal that you get over the x\nvariables is very flexible.",
    "start": "2394870",
    "end": "2402590"
  },
  {
    "text": "It's kind of like a\nbig mixture model. And so that's kind of\nlike the recap, two ways",
    "start": "2402590",
    "end": "2409130"
  },
  {
    "text": "to think about it. One is to define\ncomplicated marginals in terms of simple\nconditionals, and then",
    "start": "2409130",
    "end": "2416420"
  },
  {
    "text": "this idea of using the latent\nvariables to cluster data points. And, again, being able to model\nthem through relatively simple",
    "start": "2416420",
    "end": "2424880"
  },
  {
    "text": "conditionals once\nyou've clustered them. On the previous slide,\nfor our covariance matrix,",
    "start": "2424880",
    "end": "2432860"
  },
  {
    "text": "why are we exponentiating\nthe output of [INAUDIBLE]?? Just because it has to be\npositive semi-definite.",
    "start": "2432860",
    "end": "2440099"
  },
  {
    "text": "Then we're just\nmodeling as a diagonal? Yeah, which is, again,\na modeling choice but you could make\nit not diagonal.",
    "start": "2440100",
    "end": "2445980"
  },
  {
    "text": " Cool. And now we'll see the\nno free lunch part,",
    "start": "2445980",
    "end": "2452680"
  },
  {
    "text": "which is going to be much harder\nto learn these models compared to autoregressive,\nfully observed models",
    "start": "2452680",
    "end": "2459090"
  },
  {
    "text": "that we've seen so far. And the problem is\nthat, basically, you",
    "start": "2459090",
    "end": "2464930"
  },
  {
    "text": "have missing values. And so what happens is\nsomething like this.",
    "start": "2464930",
    "end": "2472520"
  },
  {
    "text": "Imagine that you still want to\ntrain a autoregressive model but now some of your\ndata is missing.",
    "start": "2472520",
    "end": "2478910"
  },
  {
    "text": "So you still want to fit\nan autoregressive model over the pixel\nvalues but now you don't know the value of\nthe top half of the images.",
    "start": "2478910",
    "end": "2488420"
  },
  {
    "text": "So what do you need to do? Well. There is two sets of\nvariables here again. There is the part\nthat you get to see",
    "start": "2488420",
    "end": "2493690"
  },
  {
    "text": "and then there is some\npart that you don't get to see that is latent. And then there is a\njoint distribution.",
    "start": "2493690",
    "end": "2501520"
  },
  {
    "text": "So your Pixel CNN would\ntell you the relationship between the x variables\nand the z variables.",
    "start": "2501520",
    "end": "2508030"
  },
  {
    "text": "So you can choose-- you can complete the green\npart, the missing part, any way you want.",
    "start": "2508030",
    "end": "2514080"
  },
  {
    "text": "And let's say your\nautoregressive model will tell you how likely\nthe full image is",
    "start": "2514080",
    "end": "2519450"
  },
  {
    "text": "because if you have a joint\ndistribution over z and x, are the challenge\nis that you only",
    "start": "2519450",
    "end": "2526950"
  },
  {
    "text": "get to see the observed part, so\nyou only get to see the x part. And so you need to\nbe able to evaluate",
    "start": "2526950",
    "end": "2534210"
  },
  {
    "text": "what is the probability\nof observing, let's say, this bottom half of a digit.",
    "start": "2534210",
    "end": "2541140"
  },
  {
    "text": "And in order to do that,\nagain, we have to marginalize. So you have to basically look at\nall possible ways of completing",
    "start": "2541140",
    "end": "2547260"
  },
  {
    "text": "that image and you have to\nsum the probabilities of all these possible completions.",
    "start": "2547260",
    "end": "2554640"
  },
  {
    "text": "And even though the joint is\neasy to evaluate because maybe it's just a product\nof conditionals",
    "start": "2554640",
    "end": "2560100"
  },
  {
    "text": "or like in the VAE case,\nit's just the product of two Gaussians, basically.",
    "start": "2560100",
    "end": "2565559"
  },
  {
    "text": "You have this\nmixture in behavior. Just like in the\nmixture of Gaussian when you evaluate the\nprobability over just",
    "start": "2565560",
    "end": "2573480"
  },
  {
    "text": "the x part, you have to\nsum out all the things over all possible values of\nthe unobserved variables.",
    "start": "2573480",
    "end": "2580770"
  },
  {
    "text": "You have to look at all\npossible completions and you have to check how likely\nthe different completions are",
    "start": "2580770",
    "end": "2586350"
  },
  {
    "text": "and you sum them up. Just like in the mixture\nof Gaussian case, you need to sum the probability\nunder each mixture component.",
    "start": "2586350",
    "end": "2594180"
  },
  {
    "text": "The same thing. And the problem is that\nthere is potentially too many possible completions.",
    "start": "2594180",
    "end": "2600498"
  },
  {
    "text": "Like in the mixture\nof Gaussian case, maybe you only have\nk possible values that the z variable can take.",
    "start": "2600498",
    "end": "2606250"
  },
  {
    "text": "And so this thing\nis easy to evaluate. You can just brute force it. But if you have a\nhigh-dimensional latent",
    "start": "2606250",
    "end": "2615870"
  },
  {
    "text": "variable z, this sum can be\nextremely expensive to evaluate.",
    "start": "2615870",
    "end": "2622213"
  },
  {
    "text": "You got flexibility\nbecause you're mixing a lot of\ndistributions but then you pay a price because it's\nhard to evaluate that quantity.",
    "start": "2622213",
    "end": "2631616"
  },
  {
    "text": "[INAUDIBLE] will get elevated? That's how we [INAUDIBLE]\nknow those z's. So how would you\nsum over the z's?",
    "start": "2631616",
    "end": "2636850"
  },
  {
    "text": "Well, you sum over all\npossible completions. So you would have to\nput all white pixels",
    "start": "2636850",
    "end": "2642440"
  },
  {
    "text": "and then you check\nhow likely is that? Probably very low. Then you try all\nblack pixels, and then you try all possible\ncombinations,",
    "start": "2642440",
    "end": "2648260"
  },
  {
    "text": "and then you check the\nprobability of each one of them and you sum them up. ",
    "start": "2648260",
    "end": "2653440"
  },
  {
    "text": "Instead of having the z\nto be the full completion, can the z just be\nlike some digit?",
    "start": "2653440",
    "end": "2658570"
  },
  {
    "text": "Like the 2 digit-- I mean, assume I know the\ntrue digit, that's the 9 or 6",
    "start": "2658570",
    "end": "2663760"
  },
  {
    "text": "or something. Yeah. In that case, you would have a\nlatent variable that is maybe categorical, and\nthat's what you would do if you're trying to\ninfer a digit identity.",
    "start": "2663760",
    "end": "2672460"
  },
  {
    "text": " Cool. And variational autoencoder,\nyou have the same thing.",
    "start": "2672460",
    "end": "2681410"
  },
  {
    "text": "The z's are not observed\nat training time. So at training time, you\nonly get to see the x part.",
    "start": "2681410",
    "end": "2686520"
  },
  {
    "text": "So when you want to evaluate\nthe probability of observing a particular x, you have to\ngo through all possible values",
    "start": "2686520",
    "end": "2692510"
  },
  {
    "text": "that the z variables\ncan take and you have to figure out\nhow likely was I",
    "start": "2692510",
    "end": "2697910"
  },
  {
    "text": "to generate that particular x? And the z variable\nis not even discrete",
    "start": "2697910",
    "end": "2703260"
  },
  {
    "text": "so if you want to evaluate\nthe probability of generating a particular x, you have\nto actually integrate",
    "start": "2703260",
    "end": "2711080"
  },
  {
    "text": "over all possible values\nthat the variables can take. You have to go through\nall possible choices",
    "start": "2711080",
    "end": "2716430"
  },
  {
    "text": "of the z variable, you have\nto see where it would map to. You would check the probability\nunder that Gaussian mixture",
    "start": "2716430",
    "end": "2723210"
  },
  {
    "text": "component and then\nyou integrate them up. Again, you can imagine that this\nis super expensive because--",
    "start": "2723210",
    "end": "2732330"
  },
  {
    "text": "yeah. Especially if you have\na reasonable number of latent variables, the\ncurse of dimensionality,",
    "start": "2732330",
    "end": "2739349"
  },
  {
    "text": "this is very expensive\nto even numerically approximate this\nkind of integral. ",
    "start": "2739350",
    "end": "2747427"
  },
  {
    "text": "But that's where the\nflexibility comes from. If you're integrating over\nan infinite number of mixture",
    "start": "2747427",
    "end": "2752620"
  },
  {
    "text": "components. [INAUDIBLE] ",
    "start": "2752620",
    "end": "2759450"
  },
  {
    "text": "Not just for each z. Do you just calculate the\nprobability of x given z?",
    "start": "2759450",
    "end": "2765970"
  },
  {
    "text": "Yeah. So for every z, you\ncan evaluate the joint. Just like here, for\nevery value of z,",
    "start": "2765970",
    "end": "2773520"
  },
  {
    "text": "I can evaluate p of x comma z. I can just check the z and map\nit through the neural networks.",
    "start": "2773520",
    "end": "2780240"
  },
  {
    "text": "I get a Gaussian, I, can\nevaluate the probabilities. But-- [INAUDIBLE]",
    "start": "2780240",
    "end": "2785430"
  },
  {
    "text": "[? It's not ?] possible because\nI don't know what was the z corresponding to that\ndata point, right. The z is not observed so\nI have to try all of them.",
    "start": "2785430",
    "end": "2794740"
  },
  {
    "text": "Just like here, I only get\nto see the bottom part. I don't know what was the top\npart for that particular image.",
    "start": "2794740",
    "end": "2803260"
  },
  {
    "text": "I have to guess. I have to try every possible way\nof completing that data point",
    "start": "2803260",
    "end": "2808540"
  },
  {
    "text": "and I have to sum them up. [INAUDIBLE] Z's, right? Of the z's.",
    "start": "2808540",
    "end": "2813940"
  },
  {
    "text": "How does-- Yes. How does all\npossibility of z relate to comparing all possible\nways [INAUDIBLE]??",
    "start": "2813940",
    "end": "2821885"
  },
  {
    "text": "In this case, I'm assuming\nthat the z variables represent the top part, the\nunobserved part. ",
    "start": "2821885",
    "end": "2828930"
  },
  {
    "text": "Cool. So that's sort of the\nchallenge, evaluating this marginal\nprobability of x, you",
    "start": "2828930",
    "end": "2835142"
  },
  {
    "text": "need to integrate over\nall the possible values of the z variables. And so that's the setting,\nyou have a joint distribution",
    "start": "2835143",
    "end": "2846110"
  },
  {
    "text": "over x and z. Yeah. [INAUDIBLE] assuming\nyou knew what",
    "start": "2846110",
    "end": "2851390"
  },
  {
    "text": "we were looking at,\nprobably observing x, x bar. So you know what\nx bar is and you",
    "start": "2851390",
    "end": "2856640"
  },
  {
    "text": "want to predict what\nis the most likely. [INAUDIBLE] that\nshould be over there, then you also have to go through\nevery single possible x value?",
    "start": "2856640",
    "end": "2865640"
  },
  {
    "text": "Click over here, you're\nlooking at the probability of x equals x bar and\nnow you have to loop over",
    "start": "2865640",
    "end": "2871040"
  },
  {
    "text": "all possibilities again. But what if you\ndon't actually know that the training\ndata point is x bar",
    "start": "2871040",
    "end": "2877250"
  },
  {
    "text": "and you want to\nactually solve that? So that can also work. So the setting that\nwe're going to consider",
    "start": "2877250",
    "end": "2883160"
  },
  {
    "text": "is one where in the data set,\nthe x variables are always observed. You could also think\nabout a setting",
    "start": "2883160",
    "end": "2888710"
  },
  {
    "text": "where you have some missing\ndata and some of the x variables are missing themselves.",
    "start": "2888710",
    "end": "2895790"
  },
  {
    "text": "And then you have-- You have [? through. ?] Yeah. Yeah. So in the previous\nslide, the capital Z,",
    "start": "2895790",
    "end": "2902420"
  },
  {
    "text": "is it the same as the normal\ndistribution [INAUDIBLE]?? Yeah. Yeah.",
    "start": "2902420",
    "end": "2908000"
  },
  {
    "text": "OK. And so we have to-- so instead of summation,\nwe have integral now.",
    "start": "2908000",
    "end": "2913910"
  },
  {
    "text": "How are we supposed to\ncompute the integral? Do we sample infinitely? [INAUDIBLE] So we'll see how\nto do [INAUDIBLE]..",
    "start": "2913910",
    "end": "2920910"
  },
  {
    "text": "So that's kind of\nlike the setting. We have a data set but\nfor every data point, we only get to see\nthe x variables",
    "start": "2920910",
    "end": "2927290"
  },
  {
    "text": "and the z variables are missing. They are unobserved. ",
    "start": "2927290",
    "end": "2932960"
  },
  {
    "text": "And then-- so you\ncan think of the data set as being a collection\nof [INAUDIBLE] images, x1",
    "start": "2932960",
    "end": "2938930"
  },
  {
    "text": "through xn.  And what we would\nlike to do is we",
    "start": "2938930",
    "end": "2944090"
  },
  {
    "text": "would like to still do\nmaximum likelihood learning. So we would still like to try to\nfind a choice of parameters that",
    "start": "2944090",
    "end": "2950390"
  },
  {
    "text": "maximize the probability\nof basically generating that particular data set. It's the same objective\nthat we had before.",
    "start": "2950390",
    "end": "2958080"
  },
  {
    "text": "Let's try to find\ntheta that maximizes the probability of the data\nmaximum likelihood estimation.",
    "start": "2958080",
    "end": "2965990"
  },
  {
    "text": "Or equivalently, the average\nlog likelihood of the data points, if you apply a log.",
    "start": "2965990",
    "end": "2972779"
  },
  {
    "text": "And the problem is that\nevaluating the probability",
    "start": "2972780",
    "end": "2979150"
  },
  {
    "text": "of a data point under\nthis mixture model is expensive because you have\nto sum over all possible values",
    "start": "2979150",
    "end": "2985630"
  },
  {
    "text": "that the z variable can\ntake for that data point. ",
    "start": "2985630",
    "end": "2990950"
  },
  {
    "text": "And so evaluating this\nquantity can be intractable.",
    "start": "2990950",
    "end": "2996000"
  },
  {
    "text": "And just as an\nexample, let's say that you have 30 binary\nlatent variables,",
    "start": "2996000",
    "end": "3001950"
  },
  {
    "text": "then that sum involves\n2 to the 30 terms. So it's just way too expensive\nto compute this thing.",
    "start": "3001950",
    "end": "3011900"
  },
  {
    "text": "So if the z variables can\nonly take k different values like a Gaussian mixture\nmodel, you can do it. You can brute force it.",
    "start": "3011900",
    "end": "3017920"
  },
  {
    "text": "But if you have many\nlatent variables, you cannot evaluate that\nquantity efficiently.",
    "start": "3017920",
    "end": "3025240"
  },
  {
    "text": "And for continuous\nvariables, you have an integral instead, which,\nagain, is tricky to evaluate.",
    "start": "3025240",
    "end": "3032400"
  },
  {
    "text": "And if you are hoping that\nmaybe we only need gradients because at the end of the day,\nwe just care about optimizing,",
    "start": "3032400",
    "end": "3040050"
  },
  {
    "text": "gradients are also\nexpensive to compute. So trying to do gradient\nascent on that quantity",
    "start": "3040050",
    "end": "3046470"
  },
  {
    "text": "is not feasible directly. So we need some kind\nof approximations",
    "start": "3046470",
    "end": "3053170"
  },
  {
    "text": "and it has to be very cheap\nbecause think about it, you need to be able to go\nover the data set many times,",
    "start": "3053170",
    "end": "3060400"
  },
  {
    "text": "and you need to be\nable to evaluate the gradient for every data\npoint, possibly, many times.",
    "start": "3060400",
    "end": "3067210"
  },
  {
    "text": "So this approximation\nhere has to be very cheap. ",
    "start": "3067210",
    "end": "3073590"
  },
  {
    "text": "And one natural\nway to try it would be to try to do Monte\nCarlo kind of thing, right.",
    "start": "3073590",
    "end": "3081090"
  },
  {
    "text": "Basically, this\nquantity would require us to sum over all\npossible values of z,",
    "start": "3081090",
    "end": "3087030"
  },
  {
    "text": "and instead, we could\ntry to just sample a few and get an approximation.",
    "start": "3087030",
    "end": "3094250"
  },
  {
    "text": "And that's the usual recipe that\nwe've seen in the last lecture. The idea is that we have a sum\nthat we're trying to compute,",
    "start": "3094250",
    "end": "3104610"
  },
  {
    "text": "we can try to rewrite that sum\nas an expectation, essentially. So we can-- if there are\ncapital Z, basically,",
    "start": "3104610",
    "end": "3114790"
  },
  {
    "text": "possible values that these\nz variables can take, we can multiply and\ndivide by the total number",
    "start": "3114790",
    "end": "3121300"
  },
  {
    "text": "of entries in this sum. And then this object here\nbecomes an expectation",
    "start": "3121300",
    "end": "3126760"
  },
  {
    "text": "with respect to a\nuniform distribution. And now we can\napply Monte-carlo.",
    "start": "3126760",
    "end": "3132995"
  },
  {
    "text": "Whenever you have\nan expectation, you can approximate it\nwith a sample average. So you could say, let's\napproximate this sum",
    "start": "3132995",
    "end": "3140310"
  },
  {
    "text": "with a sample average. So essentially,\nyou would randomly",
    "start": "3140310",
    "end": "3146070"
  },
  {
    "text": "sample a bunch of values\nof z, and then you would approximate\nthe expectation",
    "start": "3146070",
    "end": "3151590"
  },
  {
    "text": "with the sample average. You check how likely these\ncompletions are under the joint",
    "start": "3151590",
    "end": "3156900"
  },
  {
    "text": "and then you rescale\nappropriately. ",
    "start": "3156900",
    "end": "3163440"
  },
  {
    "text": "This would be cheaper\nbecause you just need to check k\ncompletions instead of all the possible\ncompletions that you",
    "start": "3163440",
    "end": "3171690"
  },
  {
    "text": "would have to deal with. ",
    "start": "3171690",
    "end": "3177440"
  },
  {
    "text": "Yeah. Is the z is an element\nof [INAUDIBLE],, is that just a subset of\nall possible values of z?",
    "start": "3177440",
    "end": "3184859"
  },
  {
    "text": "That would be all\npossible values of z. It's just I needed a set. [INAUDIBLE]",
    "start": "3184860",
    "end": "3190600"
  },
  {
    "text": "We're sampling. So we sample uniformly at\nrandom, and then we rescale.",
    "start": "3190600",
    "end": "3196930"
  },
  {
    "text": "And this is more tractable\nfrom [INAUDIBLE] before, because [INAUDIBLE] sampling\na discrete number of values.",
    "start": "3196930",
    "end": "3202599"
  },
  {
    "text": "And a small number,\nthat's the key thing. You just k of them, could be 1. The cheapest way is to choose\nk1 here, just sample 1.",
    "start": "3202600",
    "end": "3210820"
  },
  {
    "text": "You look at the joint\nlikelihood of that completion, and then you rescale\nappropriately.",
    "start": "3210820",
    "end": "3217300"
  },
  {
    "text": "And that would be\na valid estimator for the quantity of interest. ",
    "start": "3217300",
    "end": "3225970"
  },
  {
    "text": "Over here, you're sampling\nfrom the uniform z. I think when we did\nMonte Carlo last class, we were sampling from the\nprobability distribution of z.",
    "start": "3225970",
    "end": "3232779"
  },
  {
    "text": "So is there a reason\nwhy we treat every z as equally [INAUDIBLE] before?",
    "start": "3232780",
    "end": "3238322"
  },
  {
    "text": "It's just because\nwe have a sample. So it's so if you can see\nI'm multiplying and divided",
    "start": "3238322",
    "end": "3244270"
  },
  {
    "text": "by the total number of\nthings so that then this becomes an expectation\nwith respect to a uniform distribution.",
    "start": "3244270",
    "end": "3249430"
  },
  {
    "text": "That's the trick, basically. [INAUDIBLE] five times more\nlikely than [INAUDIBLE]..",
    "start": "3249430",
    "end": "3254598"
  },
  {
    "text": "That's the problem. That's why this is not\ngoing to work, basically. ",
    "start": "3254598",
    "end": "3259768"
  },
  {
    "text": "I'm just trying to\nunderstand the rationale of trying to sample the z\nfrom the uniform distribution.",
    "start": "3259768",
    "end": "3265460"
  },
  {
    "text": "Is it because we knew that\nwe're never going to see the z, so we gave up on\nthat, so we just",
    "start": "3265460",
    "end": "3271250"
  },
  {
    "text": "do like the uniform, which\nis like almost no priors? So this is not going to be--",
    "start": "3271250",
    "end": "3277640"
  },
  {
    "text": "you are getting at why this\nis not a great solution. This is a first attempt\ndoing things uniformly.",
    "start": "3277640",
    "end": "3284480"
  },
  {
    "text": "It's cheap, but it's not going\nto work in practice because it--",
    "start": "3284480",
    "end": "3291955"
  },
  {
    "text": "I think what you're\nsuggesting is that if you think about randomly\nguessing the z's, most likely",
    "start": "3291955",
    "end": "3297200"
  },
  {
    "text": "you're not going to hit\nthe values of z's that have enough probability\nunder the joint.",
    "start": "3297200",
    "end": "3304410"
  },
  {
    "text": "And so most of the\ncompletions that you guess by choosing uniform\nrandom don't makes sense, so they would have a\nvery low value of theta.",
    "start": "3304410",
    "end": "3312810"
  },
  {
    "text": "And so although this,\ntechnically speaking, is an unbiased\nestimator, the variance would be so big that it's not\ngoing to work in practice.",
    "start": "3312810",
    "end": "3322260"
  },
  {
    "text": "So somehow, as I think\nyou were suggesting, we need a smarter way of\nselecting these latent",
    "start": "3322260",
    "end": "3330080"
  },
  {
    "text": "variables. We don't want a\nsample uniformly, we want to sample them trying to\nguess the ones that make sense.",
    "start": "3330080",
    "end": "3337895"
  },
  {
    "text": " I'm sorry, this is kind of\nlike a big picture question. But when we started off,\nyou were describing z",
    "start": "3337895",
    "end": "3345020"
  },
  {
    "text": "as important features,\nof which, I think, when we tend to\nthink of features,",
    "start": "3345020",
    "end": "3350690"
  },
  {
    "text": "a person can have eye\ncolor and also hair color. But I feel like\nthe way that we've been treating z so far\nhas been as like a class",
    "start": "3350690",
    "end": "3357680"
  },
  {
    "text": "or membership in a group. So for example, even in the\nvariational autoencoder case,",
    "start": "3357680",
    "end": "3363380"
  },
  {
    "text": "it's like you choose one scalar\nvalue as between 0 and 1. But it's like one value--",
    "start": "3363380",
    "end": "3369900"
  },
  {
    "text": "are we just like not\nthere yet in terms of different model\n[INAUDIBLE] get to the feature representation?",
    "start": "3369900",
    "end": "3375800"
  },
  {
    "text": "Or is there a connection there? I think it's a great\nquestion, and it's",
    "start": "3375800",
    "end": "3381260"
  },
  {
    "text": "what these z's would\neven end up representing. Well, there is a first\nquestion, whether they",
    "start": "3381260",
    "end": "3386330"
  },
  {
    "text": "are discrete or\ncontinuous, and that depends on just\nhow you model them and doesn't matter too much.",
    "start": "3386330",
    "end": "3392120"
  },
  {
    "text": "Whether they end up representing\nthe things that matter, like the hair color or the\neye color, it's questionable.",
    "start": "3392120",
    "end": "3400010"
  },
  {
    "text": "Right here, we just try\nto do maximum likelihood, we just try to fit the\ndata as well as we can.",
    "start": "3400010",
    "end": "3405750"
  },
  {
    "text": "And we're going to try\nto use these latent variables to fit the data. That's what the model\nis going to try to do",
    "start": "3405750",
    "end": "3411869"
  },
  {
    "text": "if you use this objective. Whether you end up with\nsomething meaningful or not",
    "start": "3411870",
    "end": "3417390"
  },
  {
    "text": "is not necessarily guaranteed. You end up with some\nlatent variables, such that if you\nsample from them",
    "start": "3417390",
    "end": "3422700"
  },
  {
    "text": "and you fit them through\nthis model, you get, hopefully, good images\nor good distribution that",
    "start": "3422700",
    "end": "3428820"
  },
  {
    "text": "is very similar to the one\nin the training set, which means that these\nlatent variables do capture the important\nlatent factors of variation",
    "start": "3428820",
    "end": "3436470"
  },
  {
    "text": "in the data. Whether they\ncorrespond to something semantically meaningful is\nabsolutely not guaranteed.",
    "start": "3436470",
    "end": "3445320"
  },
  {
    "text": "Does that answer your question? Not really, but\nI'll keep listening.",
    "start": "3445320",
    "end": "3451990"
  },
  {
    "text": "Ask again. I'm sure somebody else has\nthe same question here, too. Oh you wanted me\nto say it again?",
    "start": "3451990",
    "end": "3457560"
  },
  {
    "text": "Sure, yeah. Sorry to take up\neveryone's time. I guess just the\nidea is even with",
    "start": "3457560",
    "end": "3463980"
  },
  {
    "text": "the variational\nautoencoder, we're sampling z from a distribution. But then ultimately, z\nis like one scalar value.",
    "start": "3463980",
    "end": "3471599"
  },
  {
    "text": "Correct? Yeah. But I feel like when-- so for example,\nwith the MNIST, it",
    "start": "3471600",
    "end": "3477090"
  },
  {
    "text": "was like, maybe just to go back\nto the categorical z model, you sample like which digit it\nis you're trying to represent,",
    "start": "3477090",
    "end": "3484740"
  },
  {
    "text": "and then that gives you\nsome Gaussian distribution. But if it's something\nlike a picture of someone,",
    "start": "3484740",
    "end": "3491970"
  },
  {
    "text": "it's not the case that there's\nonly one class they fall into. There are many features,\nwhich can coexist.",
    "start": "3491970",
    "end": "3500260"
  },
  {
    "text": "So somebody can have\nblue eyes and black hair, someone can have brown\neyes and black hair. How are we\nrepresenting that here?",
    "start": "3500260",
    "end": "3506859"
  },
  {
    "text": "Or are we not\nrepresenting that yet? We are to some extent,\nexcept up to the fact that they're not going to\nbe necessarily meaningful,",
    "start": "3506860",
    "end": "3513710"
  },
  {
    "text": "but we are using multiple\nlatent factors of variation. So every x will\nbasically be mapped.",
    "start": "3513710",
    "end": "3520420"
  },
  {
    "text": "There's many different I guess-- there is many different\nz's that could",
    "start": "3520420",
    "end": "3525430"
  },
  {
    "text": "generate that particular x. There is some that are\nmore likely than others. Given x, when you try\nto infer p of z given x,",
    "start": "3525430",
    "end": "3534160"
  },
  {
    "text": "you're guessing what\nare the latent features for this particular data point.",
    "start": "3534160",
    "end": "3539930"
  },
  {
    "text": "And if you look at what you get,\nindeed, you end up with soft-- if the z variables\nare continuous,",
    "start": "3539930",
    "end": "3548540"
  },
  {
    "text": "then you don't end up with\na discrete clustering thing, you end up with two values.",
    "start": "3548540",
    "end": "3554620"
  },
  {
    "text": "You end up with 0.5, 0.5. These yellow points end up\nhaving z1 right around 0.5, 0.5.",
    "start": "3554620",
    "end": "3566350"
  },
  {
    "text": "It doesn't have a\nspecific meaning, except that all the\npoints that correspond to that class, that digit\nend up having similar values",
    "start": "3566350",
    "end": "3574840"
  },
  {
    "text": "of the variables. [INAUDIBLE] an attempt of\ntrying to answer that question.",
    "start": "3574840",
    "end": "3580680"
  },
  {
    "text": "So if my understanding\nis correct, the z you are dealing\nwith right now does not have to be a scalar,\nit can be [INAUDIBLE]..",
    "start": "3580680",
    "end": "3587460"
  },
  {
    "text": "So the first element of the\nz vector, if it makes sense, it could be the hair color, and\nthe second one could be the eye",
    "start": "3587460",
    "end": "3594320"
  },
  {
    "text": "color, so is a combination\nof local features right? Yes, there could\nbe multiple z's.",
    "start": "3594320",
    "end": "3600410"
  },
  {
    "text": "So there's not a single\nz, there is multiple z's. In this case, there's\ntwo, z1 and z2,",
    "start": "3600410",
    "end": "3606020"
  },
  {
    "text": "and they capture two out\nof [INAUDIBLE] factors of variation.",
    "start": "3606020",
    "end": "3611180"
  },
  {
    "text": "[INAUDIBLE] That's the problem.",
    "start": "3611180",
    "end": "3617340"
  },
  {
    "text": "What I have here is you have the\n30 binary features that you--",
    "start": "3617340",
    "end": "3623730"
  },
  {
    "text": "binary latent features. They can all be just zero\none then you have two",
    "start": "3623730",
    "end": "3629099"
  },
  {
    "text": "to the 30 basically different\npossible cluster assignments and then you can't sum them up. Basically, that's the problem.",
    "start": "3629100",
    "end": "3635040"
  },
  {
    "text": " What is the semantic\nmeaning of the z's?",
    "start": "3635040",
    "end": "3640210"
  },
  {
    "text": "Can you do anything to increase\nwhether they're independent or you're not based\nin parameters?",
    "start": "3640210",
    "end": "3646740"
  },
  {
    "text": "There is a whole field of\ndisentangled representation learning, where people\nhave been trying",
    "start": "3646740",
    "end": "3652680"
  },
  {
    "text": "many different ways of trying\nto come up with models where the latent variables\nhave a better--",
    "start": "3652680",
    "end": "3659040"
  },
  {
    "text": "are more meaningful. There are unfortunately\ntheorems showing that it's impossible to do it in general.",
    "start": "3659040",
    "end": "3664350"
  },
  {
    "text": "Practically, people have been\nable to get reasonable results. But there are some\nfundamental limitations",
    "start": "3664350",
    "end": "3670170"
  },
  {
    "text": "to what you can do, because\nthe problem is essentially ill-posed. [INAUDIBLE] Disentangled representation.",
    "start": "3670170",
    "end": "3677609"
  },
  {
    "text": "The above example where we\nhave 30 binary latent features, so we have 30 z's, so\ndo we say that these 30",
    "start": "3677610",
    "end": "3685100"
  },
  {
    "text": "z's follows a\nnormal distribution, or that each of the z\nfollows normal distribution?",
    "start": "3685100",
    "end": "3691200"
  },
  {
    "text": "So if the z's are discrete,\nthen it wouldn't be normal, it would be like\neach one of them",
    "start": "3691200",
    "end": "3696890"
  },
  {
    "text": "can come from a simple\nBernoulli distribution. If z could be a Gaussian random\nvector, in which case, you",
    "start": "3696890",
    "end": "3703450"
  },
  {
    "text": "would have the\nintegral, both cases are pretty hard, basically. ",
    "start": "3703450",
    "end": "3710560"
  },
  {
    "text": "So big picture questions-- so if\nwe're having the data set that",
    "start": "3710560",
    "end": "3716020"
  },
  {
    "text": "have the labels,\ncan I say we just-- because here, we're using\nthese latent variables",
    "start": "3716020",
    "end": "3723140"
  },
  {
    "text": "to try to make our\nday easier to compute the marginal distribution.",
    "start": "3723140",
    "end": "3729700"
  },
  {
    "text": "So I was just trying to think\n[INAUDIBLE] connections.",
    "start": "3729700",
    "end": "3734859"
  },
  {
    "text": "So I guess there are two\nanswers there, if sometimes, you get to see the z\nvalues, maybe you",
    "start": "3734860",
    "end": "3740500"
  },
  {
    "text": "have an annotator willing to\nlabel these things for you, you can imagine that you can--",
    "start": "3740500",
    "end": "3747790"
  },
  {
    "text": "it's not hard to modify\nthis learning objective, where when the value of that\nvariable, you don't sum over it,",
    "start": "3747790",
    "end": "3754730"
  },
  {
    "text": "you just plug in the true value. And so you can do some kind of\nsemi-supervised learning, where sometimes this\nvariable is observed,",
    "start": "3754730",
    "end": "3762760"
  },
  {
    "text": "and sometimes it's not. [INAUDIBLE] unsupervised\nlearning manner.",
    "start": "3762760",
    "end": "3768580"
  },
  {
    "text": "This is pure, but it seems very\neasy to adjust the settings. Sometimes the z\n[? variable will be ?] [? observed ?] is\nthe same thing,",
    "start": "3768580",
    "end": "3774454"
  },
  {
    "text": "except the notation\nbecomes ugly, because then you have some\npoints where you have it, some don't.",
    "start": "3774455",
    "end": "3779490"
  },
  {
    "text": "[INAUDIBLE] learn\nsome latent variables",
    "start": "3779490",
    "end": "3785070"
  },
  {
    "text": "that have semantic\nmeanings or they could not. But if you have\nhave labels, that's",
    "start": "3785070",
    "end": "3790350"
  },
  {
    "text": "a good way to steer them in\nthe direction that you want. ",
    "start": "3790350",
    "end": "3797430"
  },
  {
    "text": "Cool. OK, so this was the\nvanilla kind of setting, where you could just\ntry a bunch of choices",
    "start": "3797430",
    "end": "3804990"
  },
  {
    "text": "for the random\nvariable at random and hope that you get something. But this is not\nquite going to work.",
    "start": "3804990",
    "end": "3812220"
  },
  {
    "text": "And so we need a better way of\nguessing the latent variables for each data point.",
    "start": "3812220",
    "end": "3819400"
  },
  {
    "text": "And so the way to do\nit is using something called importance sampling where\ninstead of sampling uniformly",
    "start": "3819400",
    "end": "3825839"
  },
  {
    "text": "at random we're going to try to\nsample the important completions more often.",
    "start": "3825840",
    "end": "3832390"
  },
  {
    "text": "So recall, this is\nthe object we want it's this marginal\nprobability where you have to sum over all\npossible values of the latent",
    "start": "3832390",
    "end": "3839109"
  },
  {
    "text": "variables. And now, what we can do is\nwe can multiply and divide",
    "start": "3839110",
    "end": "3845170"
  },
  {
    "text": "by this q of z, where q is an\narbitrary distribution that you can use to choose\ncompletions, choose values",
    "start": "3845170",
    "end": "3853180"
  },
  {
    "text": "for the latent variables. And this is one, so you can\nmultiply and divide it by q,",
    "start": "3853180",
    "end": "3859720"
  },
  {
    "text": "is fine. And now again, we're\nback to the setting where we have an expectation\nwith respect the reqpect of q",
    "start": "3859720",
    "end": "3867970"
  },
  {
    "text": "of this ratio of\nprobabilities, the probability under the true model and the\nprobability under this proposal",
    "start": "3867970",
    "end": "3874930"
  },
  {
    "text": "distribution, or\nthis way that you're using to guess the completion\nfor the latent variables.",
    "start": "3874930",
    "end": "3883300"
  },
  {
    "text": " And now, what we can do,\nagain, is just Monte Carlo.",
    "start": "3883300",
    "end": "3890130"
  },
  {
    "text": "Again, this is still\nan expectation, it's still intractable\nin general, but we can try to do the usual\ntrick of let's sample a bunch",
    "start": "3890130",
    "end": "3897960"
  },
  {
    "text": "of z's. Now we don't sample\nthem uniformly, we sample them according to\nthis proposal distribution q",
    "start": "3897960",
    "end": "3904080"
  },
  {
    "text": "which can be anything you want. And then we approximate\nthe expectation with this sample average,\njust like before.",
    "start": "3904080",
    "end": "3911619"
  },
  {
    "text": "Now the sample average\nhas the importance weight",
    "start": "3911620",
    "end": "3919690"
  },
  {
    "text": "that you have to account\nfor in the denominator, because the expression\ninside the expectation has this q in the denominator,\nso we have to put it here.",
    "start": "3919690",
    "end": "3929431"
  },
  {
    "text": "This q's just something\nthat we're modeling. So we'll see how to choose q. For now this works regardless\nof how you choose q.",
    "start": "3929431",
    "end": "3937059"
  },
  {
    "text": "I think what's a\ngood choice for q, intuitively you want to put\nprobability mass on the z's that",
    "start": "3937060",
    "end": "3945220"
  },
  {
    "text": "are likely under the\njoint distribution. If you'd like to somehow be\nable to sample the z's, that",
    "start": "3945220",
    "end": "3951910"
  },
  {
    "text": "makes sense. So you have a current joint\ndistribution between x and z,",
    "start": "3951910",
    "end": "3957339"
  },
  {
    "text": "and you want to\nchoose the z's that make sense, that are\nthe completions that are consistent with\nwhat you observe.",
    "start": "3957340",
    "end": "3964915"
  },
  {
    "text": " Then you're not just sampling\nfrom z, like ideally you",
    "start": "3964915",
    "end": "3970880"
  },
  {
    "text": "want to sample from z given x. So this is for a\nparticular data point, so I'm doing it for a single x.",
    "start": "3970880",
    "end": "3977780"
  },
  {
    "text": "You're perfectly right\nthat this choice of q has to depend on the\nx on what you see.",
    "start": "3977780",
    "end": "3983880"
  },
  {
    "text": "But for now, this is\na single data point, so I can just have a single\nq, that's supposed to work. ",
    "start": "3983880",
    "end": "3991890"
  },
  {
    "text": "And regardless basically\nof how you choose q, this is an unbiased\nestimator, meaning",
    "start": "3991890",
    "end": "3997799"
  },
  {
    "text": "that even if you\nchoose a single sample, we know the expected value\nof the sample average",
    "start": "3997800",
    "end": "4005150"
  },
  {
    "text": "is the object we want. So equivalently, if you\nwant to think about it,",
    "start": "4005150",
    "end": "4010770"
  },
  {
    "text": "you could say if you were to\nrepeat this experiment, a very large number of times,\nand average the results,",
    "start": "4010770",
    "end": "4016619"
  },
  {
    "text": "you would get the true value. So this is a reasonable\nkind of estimate.",
    "start": "4016620",
    "end": "4023720"
  },
  {
    "text": " Now the slight issue is\nthat what we care about",
    "start": "4023720",
    "end": "4035500"
  },
  {
    "text": "is not the probability\nof a data point, but we care about the log\nprobability of a data point.",
    "start": "4035500",
    "end": "4042660"
  },
  {
    "text": "What we care about is optimize\nthe average log likelihood of the data points,\nso we need to apply",
    "start": "4042660",
    "end": "4048930"
  },
  {
    "text": "a log to the expression. So we could try to\njust apply a log",
    "start": "4048930",
    "end": "4055650"
  },
  {
    "text": "on both sides of this equation\nand get this kind of estimate for the log likelihood,\nbut there is a problem.",
    "start": "4055650",
    "end": "4066490"
  },
  {
    "text": "So for example, if you were\nto choose a single data, a single sample. so if k here is 1,\nso you just sample",
    "start": "4066490",
    "end": "4073530"
  },
  {
    "text": "a single possible\ncompletion, and then you evaluate that\nestimator that way,",
    "start": "4073530",
    "end": "4080290"
  },
  {
    "text": "so it's just the ratio\nof the two probabilities. You can see that this\nis no longer unbiased,",
    "start": "4080290",
    "end": "4088950"
  },
  {
    "text": "the expectation of the\nlog is not the same as the log of the expectation.",
    "start": "4088950",
    "end": "4095890"
  },
  {
    "text": "So if you take an\nexpectation of this object here, even though\nthe expectation",
    "start": "4095890",
    "end": "4101130"
  },
  {
    "text": "of the right-hand side\nhere is what we want,",
    "start": "4101130",
    "end": "4106979"
  },
  {
    "text": "when you apply a\nlog, there is bias. ",
    "start": "4106979",
    "end": "4114120"
  },
  {
    "text": "And we can actually\nfigure out what that bias is, so recall\nthat what we want is this,",
    "start": "4114120",
    "end": "4123028"
  },
  {
    "text": "we want the log\nmarginal probability, which we can write down as\nthis importance sampling",
    "start": "4123029",
    "end": "4129179"
  },
  {
    "text": "distribution. And we know that the log is\na concave function, which",
    "start": "4129180",
    "end": "4138528"
  },
  {
    "text": "means that if you have\ntwo points, x and x prime, and you take a\ncombination of the two,",
    "start": "4138529",
    "end": "4144240"
  },
  {
    "text": "and you evaluate\nthe log, this is above the linear combination of\nthe two values of the function.",
    "start": "4144240",
    "end": "4150529"
  },
  {
    "text": " And what this means\nis that, because",
    "start": "4150530",
    "end": "4159960"
  },
  {
    "text": "of this concavity\nproperty, we can basically work out what happens if we\nswap the order of logarithm",
    "start": "4159960",
    "end": "4167370"
  },
  {
    "text": "and expectation. So if we put the expectation\noutside of the log, we're going to get a bound\non the quantity that we want.",
    "start": "4167370",
    "end": "4176140"
  },
  {
    "text": "So there is this thing called\nJensen's inequality, which basically says that the\nlogarithm of the expectation",
    "start": "4176140",
    "end": "4183060"
  },
  {
    "text": "of some function,\nany function, which is just this quantity\nhere, is at least as",
    "start": "4183060",
    "end": "4190609"
  },
  {
    "text": "large as the\nexpectation of the log. ",
    "start": "4190609",
    "end": "4198710"
  },
  {
    "text": "And like the picture\nis again, you have a log which is\na concave function, and so if you have two\npoints fz1 and fz2,",
    "start": "4198710",
    "end": "4206930"
  },
  {
    "text": "and you take the linear\ncombination of that, you're always below\nwhat you would get if you were to apply the log.",
    "start": "4206930",
    "end": "4213140"
  },
  {
    "start": "4213140",
    "end": "4220610"
  },
  {
    "text": "And so in our world,\nwhat this means is that, for this particular\nchoice of f of z,",
    "start": "4220610",
    "end": "4229000"
  },
  {
    "text": "which is what we have\nhere, this density ratio, the log of the\nestimator is at least",
    "start": "4229000",
    "end": "4235990"
  },
  {
    "text": "as large as the\naverage of the log. ",
    "start": "4235990",
    "end": "4247710"
  },
  {
    "text": "So what we have\nhere, if we do this, is a lower bound on the\nobject that we want.",
    "start": "4247710",
    "end": "4254680"
  },
  {
    "text": "So on the left, we\nhave the thing we want, which is the log marginal\nprobability of a data point, and on the right, we have a\nquantity, which we can estimate,",
    "start": "4254680",
    "end": "4264490"
  },
  {
    "text": "which is a bunch of samples from\nq, and we evaluate this log, there's a lower bound.",
    "start": "4264490",
    "end": "4271050"
  },
  {
    "text": "Which is not bad,\nbecause what this means is that if we were to optimize\nthe quantity on the right,",
    "start": "4271050",
    "end": "4276860"
  },
  {
    "text": "the thing we care about\nwould also go up hopefully.",
    "start": "4276860",
    "end": "4281900"
  },
  {
    "text": "It has to be at least\nas large as whatever we find by optimizing the\nquantity on the right.",
    "start": "4281900",
    "end": "4288469"
  },
  {
    "text": " Just thinking of what and why\nwe care about the log of this,",
    "start": "4288470",
    "end": "4294590"
  },
  {
    "text": "and not the quantity itself. Yeah, because if you\nrecall, what we want to do is we care about doing\nmaximum likelihood,",
    "start": "4294590",
    "end": "4302280"
  },
  {
    "text": "so what we care\nabout is this, so we",
    "start": "4302280",
    "end": "4309139"
  },
  {
    "text": "want to go through all the data\npoints, and for every data point we want to evaluate the log\nprobability of that data point.",
    "start": "4309140",
    "end": "4317449"
  },
  {
    "text": "And so that's the quantity that\nwe'd like to take gradients of and would like to optimize.",
    "start": "4317450",
    "end": "4324130"
  },
  {
    "text": "And the good news is that we\ncan get a lower bound on that",
    "start": "4324130",
    "end": "4330429"
  },
  {
    "text": "quantity through this machinery. Where is it, I think here.",
    "start": "4330430",
    "end": "4336400"
  },
  {
    "text": " And then the strategy is\nbasically going to be,",
    "start": "4336400",
    "end": "4344330"
  },
  {
    "text": "let's try to optimize\nthis lower bound.",
    "start": "4344330",
    "end": "4349680"
  },
  {
    "text": "And what we will see\nsoon is that the choice of q, the way you decide\nhow to sample the latent",
    "start": "4349680",
    "end": "4356239"
  },
  {
    "text": "variables basically controls\nhow tight this lower bound is. So if you have a good choice for\nq then the lower bound is tight,",
    "start": "4356240",
    "end": "4365210"
  },
  {
    "text": "and this basically becomes\na very good approximation to the quantity we\nactually care about,",
    "start": "4365210",
    "end": "4370710"
  },
  {
    "text": "which is the log\nmarginal probability. ",
    "start": "4370710",
    "end": "4376020"
  },
  {
    "text": "What's stopping us\nfrom taking the sample average of the logarithms\nof the [INAUDIBLE]..",
    "start": "4376020",
    "end": "4384450"
  },
  {
    "text": "That's what we're going to do, So this one is,\nthe right hand side",
    "start": "4384450",
    "end": "4391860"
  },
  {
    "text": "that's what we're going\nto actually optimize. OK, the [INAUDIBLE]\nwhat we want exactly",
    "start": "4391860",
    "end": "4399530"
  },
  {
    "text": "the log marginal probability,\nthe right-hand side is the thing we can\nactually easily evaluate",
    "start": "4399530",
    "end": "4405679"
  },
  {
    "text": "to sample a bunch of q's and\nthen get this log density ratio.",
    "start": "4405680",
    "end": "4410950"
  },
  {
    "text": "So whether this\nmethod works or not depends on how well we\nchoose q [INAUDIBLE].. Yes. ",
    "start": "4410950",
    "end": "4421040"
  },
  {
    "text": "And this thing that\nyou see on the right is something you might\nhave heard of before,",
    "start": "4421040",
    "end": "4426170"
  },
  {
    "text": "is the evidence lower bound, the\nelbow, which is a lower bound",
    "start": "4426170",
    "end": "4431540"
  },
  {
    "text": "on the probability of\nevidence, which is basically the probability of x. So x is the evidence,\nx is the thing",
    "start": "4431540",
    "end": "4437870"
  },
  {
    "text": "you get to see is\nthe observed part. ",
    "start": "4437870",
    "end": "4443059"
  },
  {
    "text": "The log probability\nof evidence is the thing you would\nlike to optimize, but it's tricky\nto evaluate that.",
    "start": "4443060",
    "end": "4450880"
  },
  {
    "text": "And so instead, we have\nthis evidence lower bound, the elbow, which is\nthe thing we can actually",
    "start": "4450880",
    "end": "4456809"
  },
  {
    "text": "compute and optimize. Can you remind me again,\nwhy we need [INAUDIBLE]?? Is it because the\ninitial function",
    "start": "4456810",
    "end": "4463580"
  },
  {
    "text": "that we have is not something\nthat we can very easily compute",
    "start": "4463580",
    "end": "4469380"
  },
  {
    "text": "the minimum of? Yeah, so the original thing\nthat you would like to have is this, which is still tricky.",
    "start": "4469380",
    "end": "4477540"
  },
  {
    "text": " The expectation itself is not\nsomething you can evaluate,",
    "start": "4477540",
    "end": "4484660"
  },
  {
    "text": "so you would have to\ndo a sample average. And you can do the sample\naverage inside or outside",
    "start": "4484660",
    "end": "4490739"
  },
  {
    "text": "if you do the\nsimplest case where, let's say you choose k equals 1,\nthen you see that you basically",
    "start": "4490740",
    "end": "4500770"
  },
  {
    "text": "end up, which would be the\ncheapest way of doing things, where you take a\nsingle sample, so what people would do in practice.",
    "start": "4500770",
    "end": "4507610"
  },
  {
    "text": "Then you see that if you\ntake the expectation of that, you end up with the\nexpectation of the log instead",
    "start": "4507610",
    "end": "4516130"
  },
  {
    "text": "of the log of the expectation. So since this is not a\ncorrect approximation, [INAUDIBLE] it's\nan approximation--",
    "start": "4516130",
    "end": "4523070"
  },
  {
    "text": "It happens to be a decent one,\nbecause it's a lower bound, and so it's not going to hurt us\ntoo much, because we optimizing",
    "start": "4523070",
    "end": "4530060"
  },
  {
    "text": "a lower bound. If you maximize the lower\nbound, the true quantity is always going to be above,\nand so it's also going to go up.",
    "start": "4530060",
    "end": "4537040"
  },
  {
    "text": "Yeah, so what you're saying\nthat if we sample from here, we just end up getting a log of\nthe expectation, which is not what we want, since we can\nuse Jensen's inequality",
    "start": "4537040",
    "end": "4544640"
  },
  {
    "text": "to describe the function as\nthe expectation of a log, and we can solve for\nwhatever its minimum is,",
    "start": "4544640",
    "end": "4552170"
  },
  {
    "text": "and that'll be a minimum of\nthe other function as well. So Jensen basically\njust tells you what happens if you were\nto do this approximation,",
    "start": "4552170",
    "end": "4559070"
  },
  {
    "text": "where you take the\nexpectation, and then the log. So now we can just\nfind the middle of one",
    "start": "4559070",
    "end": "4568300"
  },
  {
    "text": "of these functions, is what\nwe're actually maximizing here, and we can see this one, which\nis the same as maximizing",
    "start": "4568300",
    "end": "4575380"
  },
  {
    "text": "[INAUDIBLE] the\nother one is above. And so right, and\nwe'll see that the gap",
    "start": "4575380",
    "end": "4580900"
  },
  {
    "text": "here is not too bad, as\nlong as we choose a good q.",
    "start": "4580900",
    "end": "4586409"
  },
  {
    "text": "Let's talk about [INAUDIBLE]\nstuff, in that case, if your objective is minimizing\nthe quantity on the left,",
    "start": "4586410",
    "end": "4592180"
  },
  {
    "text": "then you can use\nJensen's inequality, is that the right solution,\nthat this only works [INAUDIBLE]",
    "start": "4592180",
    "end": "4599170"
  },
  {
    "text": "Yeah. It only makes sense to\nmaximize a lower bound to the function you want,\nbecause otherwise, yeah,",
    "start": "4599170",
    "end": "4605580"
  },
  {
    "text": "it's not clear what the\nrelationship would look like. [INAUDIBLE] because we want to\ngo in the reverse direction,",
    "start": "4605580",
    "end": "4610960"
  },
  {
    "text": "it's very hard [INAUDIBLE].  KUBO's, there is the\nelbow and the KUBO,",
    "start": "4610960",
    "end": "4619349"
  },
  {
    "text": "and then there's a\nbunch of techniques that people have come\nup with the upper bounds",
    "start": "4619350",
    "end": "4624570"
  },
  {
    "text": "to these quantities,\nbut then it's much trickier to get an upper\nbound and a lower bound.",
    "start": "4624570",
    "end": "4629650"
  },
  {
    "text": "And intuitively, it's because,\nif you just sample a few these, it's very hard to\nknow whether you're",
    "start": "4629650",
    "end": "4635940"
  },
  {
    "text": "missing some very important\nones, which is what you would need to get an upper bound.",
    "start": "4635940",
    "end": "4642180"
  },
  {
    "text": "While it's relatively\neasy to say that if I've seen so\nmany z's that have a certain amount of probability\nmass, there must be others.",
    "start": "4642180",
    "end": "4649870"
  },
  {
    "text": "So it's always easier to get a\nlower bound and an upper bound, because the upper bound would\nrequire you to say you rule out",
    "start": "4649870",
    "end": "4657030"
  },
  {
    "text": "that there are many z's that\nhave a very high probability somewhere and you haven't\nseen it, that's the intuition.",
    "start": "4657030",
    "end": "4664853"
  },
  {
    "text": "Yeah, I guess this\nis not related, but is there a way to quantify\nhow tight the bound is?",
    "start": "4664853",
    "end": "4670680"
  },
  {
    "text": "So there is a way to quantify\nhow tight the bound is. So we know that,\nfor any choice of q,",
    "start": "4670680",
    "end": "4675960"
  },
  {
    "text": "you have this nice lower bound\non the quantity we care about. This is the quantity\nwe care about,",
    "start": "4675960",
    "end": "4682500"
  },
  {
    "text": "and we got a lower bound\nfor any choice of q. ",
    "start": "4682500",
    "end": "4687840"
  },
  {
    "text": "If you expand this\nthing, you're going to get a decomposition where you\nhave just the log of the ratio",
    "start": "4687840",
    "end": "4697590"
  },
  {
    "text": "is the difference\nof the logs, and you can see that this\nquantity here is what we've seen in the last\nlecture being the entropy of q.",
    "start": "4697590",
    "end": "4706020"
  },
  {
    "text": "And so you can also\nrewrite this expression as the sum of two terms. The average log joint\nprobability under q,",
    "start": "4706020",
    "end": "4713970"
  },
  {
    "text": "and then you have\nthe entropy under q. And it turns out\nthat if q is chosen",
    "start": "4713970",
    "end": "4722160"
  },
  {
    "text": "to be the conditional\ndistribution of z given x under the model, then this\ninequality becomes an equality.",
    "start": "4722160",
    "end": "4732400"
  },
  {
    "text": "So the bound becomes tight,\nand there is no approximation basically at that point.",
    "start": "4732400",
    "end": "4739627"
  },
  {
    "text": "And so essentially\nwhat this is saying is that the best way of\nguessing the z variables",
    "start": "4739627",
    "end": "4746100"
  },
  {
    "text": "is to actually use the\nposterior distribution according to the model.",
    "start": "4746100",
    "end": "4751810"
  },
  {
    "text": "You have a joint\ndistribution between x and z that defines a conditional\nfor the z variables given",
    "start": "4751810",
    "end": "4757930"
  },
  {
    "text": "the x1's, and that would be\nthe optimal way of guessing the latent variables.",
    "start": "4757930",
    "end": "4765300"
  },
  {
    "text": "The problem is that this is not\ngoing to be easy to evaluate, and so that's why we'll\nneed other things, but this would be\nthe optimal way",
    "start": "4765300",
    "end": "4771060"
  },
  {
    "text": "of choosing the distribution. And incidentally, if you've\nseen the EM algorithm,",
    "start": "4771060",
    "end": "4777210"
  },
  {
    "text": "that's what you need\nin the e step of EM. ",
    "start": "4777210",
    "end": "4783030"
  },
  {
    "text": "And there are some very\nclose connections between EM and what we're doing here.",
    "start": "4783030",
    "end": "4789320"
  },
  {
    "text": "Some says that's the best way of\ninferring the latent variables, is to use the true\nposterior distribution.",
    "start": "4789320",
    "end": "4796639"
  },
  {
    "text": "Originally, we had a\nz, we were going to x. But if you also need to go from\nx to z then you have a cycle.",
    "start": "4796640",
    "end": "4802880"
  },
  {
    "text": "How do you consider\nthe computational graph in this case? Yeah, so essentially\nwhat this would",
    "start": "4802880",
    "end": "4808250"
  },
  {
    "text": "require you is, to say given\nan x, if you have a VAE,",
    "start": "4808250",
    "end": "4813440"
  },
  {
    "text": "you would have to figure\nout what kind of inputs should I fit into my\nneural networks that would produce this kind of x.",
    "start": "4813440",
    "end": "4820650"
  },
  {
    "text": "So of have to invert\nthe neural network, and you need to\nfigure out what were",
    "start": "4820650",
    "end": "4826580"
  },
  {
    "text": "the likely inputs to the neural\nnetworks that would produce the x that I'm given, which\nis in general pretty hard,",
    "start": "4826580",
    "end": "4834590"
  },
  {
    "text": "as we'll see, but we can\ntry to approximate that. ",
    "start": "4834590",
    "end": "4839790"
  },
  {
    "text": "Cool. I think we're out of time, so\nthis is probably a good place",
    "start": "4839790",
    "end": "4844890"
  },
  {
    "text": "to stop. What we'll see is then the\nmachinery for training of VAE will involve optimizing\nboth p and q,",
    "start": "4844890",
    "end": "4852810"
  },
  {
    "text": "and that's what we're going\nto see in the next lecture. ",
    "start": "4852810",
    "end": "4862000"
  }
]