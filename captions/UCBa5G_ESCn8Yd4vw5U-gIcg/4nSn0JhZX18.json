[
  {
    "start": "0",
    "end": "35000"
  },
  {
    "text": "thank you everyone for coming I'd like to talk about our deep learning processors Andy said so yeah I'm one",
    "start": "10560",
    "end": "16440"
  },
  {
    "text": "thing I should mention dis petaa scale what I mean by that is we do mean a petaflop but these are going to be 8-bit",
    "start": "16440",
    "end": "22710"
  },
  {
    "text": "and 16-bit deep learning flops so not I Triple E double precision flops so if any of you are from the HPC community",
    "start": "22710",
    "end": "28830"
  },
  {
    "text": "and don't get too excited there huh it's not I Triple E but yeah so just a quick",
    "start": "28830",
    "end": "37500"
  },
  {
    "start": "35000",
    "end": "83000"
  },
  {
    "text": "gauge how many if you're familiar with deep learning yeah yeah all right so",
    "start": "37500",
    "end": "43710"
  },
  {
    "text": "just oh one thing before I start on our name used to be in Gemini LLC does our",
    "start": "43710",
    "end": "49680"
  },
  {
    "text": "legal name but now with Y Combinator were changing to about this so just to clear up any confusion there so I've",
    "start": "49680",
    "end": "56489"
  },
  {
    "text": "just slide basically for the sake of completeness basically deep learning workloads as you know have require huge",
    "start": "56489",
    "end": "62820"
  },
  {
    "text": "amounts of data to be shuffled for pretty long distances and to do once in",
    "start": "62820",
    "end": "69060"
  },
  {
    "text": "a while you do have to compute on them so basically that's kind of the workload",
    "start": "69060",
    "end": "74250"
  },
  {
    "text": "were trying to accelerate I have an image of a continent up there of course we l STM's and our n NS are also pretty",
    "start": "74250",
    "end": "81420"
  },
  {
    "text": "common so yeah and of course in this day and age if you want to build a chip then",
    "start": "81420",
    "end": "88229"
  },
  {
    "start": "83000",
    "end": "120000"
  },
  {
    "text": "it better be for and it's a specialized chip so not a general-purpose processor then it better be for a worth workload",
    "start": "88229",
    "end": "94770"
  },
  {
    "text": "worth accelerating and so rather than bore you with a bunch of bullet points I just thought I'd show this the chart",
    "start": "94770",
    "end": "101729"
  },
  {
    "text": "which is a little bit outdated as you can see but I think gets to a general point across so deep learning is a worth",
    "start": "101729",
    "end": "107070"
  },
  {
    "text": "workload worth accelerating percentage",
    "start": "107070",
    "end": "112170"
  },
  {
    "text": "and error rate yeah error rate on image net man things have improved as well and",
    "start": "112170",
    "end": "118770"
  },
  {
    "text": "so from 15 so when we first started this company basically we wanted to provide",
    "start": "118770",
    "end": "125700"
  },
  {
    "start": "120000",
    "end": "205000"
  },
  {
    "text": "really an order of magnitude over well over well two orders of magnitude over GPUs but also kind of order of magnitude",
    "start": "125700",
    "end": "132660"
  },
  {
    "text": "on what's the best you could do if you were kind of optimizing for compute and so what we had is a pretty ambitious",
    "start": "132660",
    "end": "138930"
  },
  {
    "text": "target I guess you could say is we wanted to cram a petaflop and again these are 8-bit",
    "start": "138930",
    "end": "143940"
  },
  {
    "text": "sixteen-bit flops into a TDP and something similar to de volta and around 300 watts right so and other thing about",
    "start": "143940",
    "end": "152610"
  },
  {
    "text": "this is we didn't just want to say oh we have a petaflop of available performance you could get on some magical workload",
    "start": "152610",
    "end": "157950"
  },
  {
    "text": "that it's like an unicorn right no we wanted to get a petaflop that you can actually see in real life on real world",
    "start": "157950",
    "end": "164790"
  },
  {
    "text": "networks and so just for example there I have that is a hyperlink to accelerate",
    "start": "164790",
    "end": "170220"
  },
  {
    "text": "that benchmark de volta on LST NS and they saw that it wasn't nearly as good as what advert advertised hundred-twenty",
    "start": "170220",
    "end": "178620"
  },
  {
    "text": "teraflops was and the primary reason for that really is you simply can't feed those computing units fast enough and",
    "start": "178620",
    "end": "185250"
  },
  {
    "text": "really the primary reason for this if you think about what the state of DRAM memory systems are right now is write",
    "start": "185250",
    "end": "190710"
  },
  {
    "text": "write something like I've been with memory tied minute with - I think yeah so - with us on something order of",
    "start": "190710",
    "end": "195900"
  },
  {
    "text": "several hundred gigabytes and if you just cram a petaflop under you won't be able to feed the thing enough and so",
    "start": "195900",
    "end": "206520"
  },
  {
    "start": "205000",
    "end": "271000"
  },
  {
    "text": "can't we just wait until Moore's law saves us right so this kind of questioned its well for any proof pretty",
    "start": "206520",
    "end": "213390"
  },
  {
    "text": "much any previous special-purpose processor excluding perhaps a couple of niche applications it just basically",
    "start": "213390",
    "end": "219720"
  },
  {
    "text": "been Moore's law that's kind of wiped everything in way preventing general operating a special purpose entrance",
    "start": "219720",
    "end": "226709"
  },
  {
    "text": "from making ahead headway into it and really to answer ties not really because this is a point of some contention like",
    "start": "226709",
    "end": "233280"
  },
  {
    "text": "whether or not Moore's law itself has actually slowed down you know but more importantly regardless of whether or not",
    "start": "233280",
    "end": "238410"
  },
  {
    "text": "you think Moore's law itself has actually failed is that this is a very nice thing called Dennard scaling for",
    "start": "238410",
    "end": "243840"
  },
  {
    "text": "those of you who don't know Dennard scaling is basically you know it is a rough approximation it's that we have",
    "start": "243840",
    "end": "249840"
  },
  {
    "text": "doubled the number of transistors but they're approximately half the power consumption due to the reduction in the nominal VDD and as a result we're able",
    "start": "249840",
    "end": "257430"
  },
  {
    "text": "to in the same power consumption get double number of transistors and everything's dandy and that really hasn't been true from like the 40",
    "start": "257430",
    "end": "263669"
  },
  {
    "text": "nanometer indefinitely from 28 nanometer node and this one is actually from Nvidia that points that out so and just",
    "start": "263669",
    "end": "272310"
  },
  {
    "text": "to give it an example of what we what we didn't want in any petascale duplieren",
    "start": "272310",
    "end": "277830"
  },
  {
    "text": "processor we made this is again not to I should note that this is an April Fool's joke that Adam type it out so I'm not",
    "start": "277830",
    "end": "283650"
  },
  {
    "text": "trying to denigrate add up to have a great company but basically you know there's like it it's no use if all it is",
    "start": "283650",
    "end": "289080"
  },
  {
    "text": "is a completely fixed purpose matrix multiplier with no on-chip memory and it consumes like a kilowatt of power right",
    "start": "289080",
    "end": "294840"
  },
  {
    "text": "it's not really super useful so that's what we really don't want one to make when we're talking about a petascale",
    "start": "294840",
    "end": "300330"
  },
  {
    "text": "deep-learning processor so in order to reach our goal someone ambitious goal",
    "start": "300330",
    "end": "306930"
  },
  {
    "start": "303000",
    "end": "355000"
  },
  {
    "text": "then we need to be pretty much ten times better than what our current batch of offerings are and the question I",
    "start": "306930",
    "end": "312420"
  },
  {
    "text": "immediately get is how on earth is that possible right these are supposed to be processors already optimized for deep learning how can you be even hope to get",
    "start": "312420",
    "end": "319710"
  },
  {
    "text": "ten times better than what they're getting and basically the question is start where there's three doors - right",
    "start": "319710",
    "end": "325020"
  },
  {
    "text": "so right now compute for a most part for deep learning operations as measured in something under order of femtojoules",
    "start": "325020",
    "end": "331590"
  },
  {
    "text": "but the cost of doing access to memory let's say high bandwidth memory it's",
    "start": "331590",
    "end": "337110"
  },
  {
    "text": "going to be an order of pika jewel so like if you see something floating around like okay we have several peda",
    "start": "337110",
    "end": "342570"
  },
  {
    "text": "ops of performance then you quickly find out that you need some hundreds of kilowatts to feed that thing with memory",
    "start": "342570",
    "end": "348930"
  },
  {
    "text": "which isn't really isn't really feasible so so again basically what's the",
    "start": "348930",
    "end": "358260"
  },
  {
    "start": "355000",
    "end": "419000"
  },
  {
    "text": "obstacle to our little deep learning supercomputer on a chip right and if you're from the semiconductor industry you know that has a completely different",
    "start": "358260",
    "end": "363990"
  },
  {
    "text": "connotation there but basically the problem is really data movement as I say um so the problem that I run into a loft",
    "start": "363990",
    "end": "372630"
  },
  {
    "text": "of time when I try to bring this up is that people say okay well what if we just in some isolated cases we can put",
    "start": "372630",
    "end": "377730"
  },
  {
    "text": "all our memory on die doesn't does that not solve our problem and answer that is really no because yes you solve one",
    "start": "377730",
    "end": "383870"
  },
  {
    "text": "serious component which is two off trip memory access but what you don't solve is two on chip data movement problem and",
    "start": "383870",
    "end": "389820"
  },
  {
    "text": "that's characterized by gonna be the RC quadratic problem right two interconnects and this is the problems I",
    "start": "389820",
    "end": "395490"
  },
  {
    "text": "can think coming more into focus as exascale kind of grapples with these issues and hence the timely source",
    "start": "395490",
    "end": "401730"
  },
  {
    "text": "they're from horse Simon at Lawrence Berkeley National Laboratories and really the issue that he points out is a",
    "start": "401730",
    "end": "407160"
  },
  {
    "text": "look it costs us even more to move data on chip than it does to do an operate upon it so this is a really a problem",
    "start": "407160",
    "end": "413020"
  },
  {
    "text": "that needs to be tackled that at least for most part isn't being addressed right now so criteria reiterate the same",
    "start": "413020",
    "end": "421750"
  },
  {
    "start": "419000",
    "end": "511000"
  },
  {
    "text": "point that really this ten to twenty percent of the problem is kind of into",
    "start": "421750",
    "end": "427030"
  },
  {
    "text": "focus here right so we've seen companies and a lot of literature as well say okay well we can do away with floating point",
    "start": "427030",
    "end": "433240"
  },
  {
    "text": "we can stick there just fixed point operations and that biases some power buys a some performance even but really",
    "start": "433240",
    "end": "440470"
  },
  {
    "text": "you're you're missing the eighty nine percent to eighty ninety percent of the problem which is eighty nine percent",
    "start": "440470",
    "end": "448030"
  },
  {
    "text": "problem which is the data movement cost and a memory cost and as you can see of course LST M is gonna require more but",
    "start": "448030",
    "end": "454990"
  },
  {
    "text": "chances are as the models progress and progress through time they're going to",
    "start": "454990",
    "end": "460690"
  },
  {
    "text": "get more complicated require even more memory and that's going to become a serious well it is already a balton life",
    "start": "460690",
    "end": "466210"
  },
  {
    "text": "but going to continue to strangle the system right and so the source for this is from DARPA which itself is actually",
    "start": "466210",
    "end": "471670"
  },
  {
    "text": "from a source here at Stanford so much",
    "start": "471670",
    "end": "484090"
  },
  {
    "text": "much slower than yours if your memories you know you could optimize this but it'll just move you I mean it seems like",
    "start": "484090",
    "end": "489460"
  },
  {
    "text": "you're even much worse by language I know right so I was a different problem but like when I talk about data movement",
    "start": "489460",
    "end": "496000"
  },
  {
    "text": "here I maybe I should've clarified I meant more data movement for the activations rights and so they're gonna be very large activation tensors and",
    "start": "496000",
    "end": "501640"
  },
  {
    "text": "weight memory so they move it in that way yeah yeah in Toronto yeah I'm a",
    "start": "501640",
    "end": "507190"
  },
  {
    "text": "sorry maybe should have I mean there are other ways to deal with Iowa as well right but yeah it is a problem so yeah",
    "start": "507190",
    "end": "514180"
  },
  {
    "start": "511000",
    "end": "532000"
  },
  {
    "text": "this slide is pretty sparse so for us no one no one solution is really going to",
    "start": "514180",
    "end": "519930"
  },
  {
    "text": "tackle the entire entirety of the problem so we really need to take a tax",
    "start": "519930",
    "end": "525580"
  },
  {
    "text": "problem along multiple angles so we do it at the architectural level and circuit and device level as you can see",
    "start": "525580",
    "end": "531010"
  },
  {
    "text": "up there so let's dive into the architecture it's a is again optimized",
    "start": "531010",
    "end": "537730"
  },
  {
    "start": "532000",
    "end": "761000"
  },
  {
    "text": "to it's optimized to reduce data movement right as I said and so to give",
    "start": "537730",
    "end": "542860"
  },
  {
    "text": "you some concrete numbers on that on average this on 28 nanometer by the ways so we're basically getting the",
    "start": "542860",
    "end": "548920"
  },
  {
    "text": "average interconnect length of 50 microns so if you're if you have some idea of what CMOS interconnect modeling",
    "start": "548920",
    "end": "554889"
  },
  {
    "text": "looks like for the most part this has been the resistance negligible regime and for a most part at least on 20 an",
    "start": "554889",
    "end": "561220"
  },
  {
    "text": "animator for us we can get away with not having a repeater so this is a big advantage in terms of power obviously",
    "start": "561220",
    "end": "566550"
  },
  {
    "text": "those repeaters also leak like crazy but in terms of the data movement problem it does reduce it reduce the cost of that",
    "start": "566550",
    "end": "573190"
  },
  {
    "text": "data Muse been quite significantly just at the architectural level it is a dataflow architecture and it's dataflow",
    "start": "573190",
    "end": "580029"
  },
  {
    "text": "architectures are kind of well I guess they've been thrown by the wayside a little bit because in a way the ideal",
    "start": "580029",
    "end": "586779"
  },
  {
    "text": "workloads Furnham hasn't really been found deep learning pretty much is in many ways the ideal workload for it",
    "start": "586779",
    "end": "592540"
  },
  {
    "text": "since the primary form of deep learning",
    "start": "592540",
    "end": "598660"
  },
  {
    "text": "I guess primary form of expression for deep learning is a computational graph",
    "start": "598660",
    "end": "604899"
  },
  {
    "text": "right tensorflow grasp I torch both my torches dynamic but attentive de what you're really modeling is a",
    "start": "604899",
    "end": "610089"
  },
  {
    "text": "computational graph and that map's really well to dataflow architectures and if you're familiar with the",
    "start": "610089",
    "end": "615910"
  },
  {
    "text": "literature on adult on a kind of East source of neural network processors is",
    "start": "615910",
    "end": "621339"
  },
  {
    "text": "that there's a lot of data reuse optimizations that are possible particularly for convolution which is",
    "start": "621339",
    "end": "626500"
  },
  {
    "text": "that you bring the data in from some memory bank you can actually reuse and",
    "start": "626500",
    "end": "631750"
  },
  {
    "text": "do some work on it when more work on it before you have to put it back when you bring it back in again and so as a result you can save a significant amount",
    "start": "631750",
    "end": "638680"
  },
  {
    "text": "of power and everything obviously and so there's been a couple of examples in literature the iris iris isn't one that",
    "start": "638680",
    "end": "645250"
  },
  {
    "text": "comes to mind from MIT although basically we're a decent bit ahead of literature as of the moment right now so this is important because",
    "start": "645250",
    "end": "651880"
  },
  {
    "text": "for the majority of workloads probably most people aren't going to be running some really exotic network so for the",
    "start": "651880",
    "end": "658329"
  },
  {
    "text": "majority of workloads you're going to see a pretty good speed up on that which is important to getting to our goal we",
    "start": "658329",
    "end": "663370"
  },
  {
    "text": "have what we call it a tensor native memory architecture so a little bit of a buzzword in a way but basically what",
    "start": "663370",
    "end": "670089"
  },
  {
    "text": "this essentially means is that instead of having something like a scratch pad memory or something you basically have something where you address large",
    "start": "670089",
    "end": "676120"
  },
  {
    "text": "contiguous regions of memory and it's a tensor and it does slices of the tensor and so a slice of that large",
    "start": "676120",
    "end": "682880"
  },
  {
    "text": "continuous sector and this is useful because in deep learning you're very rarely going to access like let's say",
    "start": "682880",
    "end": "688820"
  },
  {
    "text": "one specific point randomly so as a result by making by setting up the memory structure in this way you're able",
    "start": "688820",
    "end": "694880"
  },
  {
    "text": "to get more bandwidth and from it you're able to and be able to lower power as well and allows you to set up this is",
    "start": "694880",
    "end": "701630"
  },
  {
    "text": "more a circuit level thing that allows you to set up some pretty nice bit line capacitance production which is in",
    "start": "701630",
    "end": "706820"
  },
  {
    "text": "pretty much any modern memory array as a significant source of the power consumption and one final thing is that",
    "start": "706820",
    "end": "712640"
  },
  {
    "text": "we pipeline to expose parallelism so GPUs nowadays right so the way you get",
    "start": "712640",
    "end": "717709"
  },
  {
    "text": "throughput and performance is to load up 256 or so images at the same time running through in parallel and that's",
    "start": "717709",
    "end": "723860"
  },
  {
    "text": "how you get throughput oh we don't do that we basically didn't have the entire thing as a giant pipeline that's and",
    "start": "723860",
    "end": "730220"
  },
  {
    "text": "that way is you go through - of course you couldn't expose some really nice parallelism and a big kicker for us is",
    "start": "730220",
    "end": "736250"
  },
  {
    "text": "that one DLE ardeal these are core equivalent basically it's a deep learning element",
    "start": "736250",
    "end": "741500"
  },
  {
    "text": "it has all the memory it needs so not only is there no memory required off die but there is also no memory required",
    "start": "741500",
    "end": "748899"
  },
  {
    "text": "outside of the local region which is about 500 microns by 500 microns outside",
    "start": "748899",
    "end": "754699"
  },
  {
    "text": "the one Deeley so that saves on ship date and movement quite significantly",
    "start": "754699",
    "end": "760509"
  },
  {
    "text": "yeah so here is kind of just a high-level overview of what we're looking at",
    "start": "760839",
    "end": "765860"
  },
  {
    "start": "761000",
    "end": "1080000"
  },
  {
    "text": "we just have 9d Elise I want to show a gds - here but we're thinking of changing foundries so wasn't able to do",
    "start": "765860",
    "end": "772190"
  },
  {
    "text": "that plus it's kind of hard to see what all the wires in the way but just I have nine of them for illustration purposes here just know for petascale trip",
    "start": "772190",
    "end": "778880"
  },
  {
    "text": "obviously nine won't fly you're gonna need something like 2400 of them and in terms of al used our friends at about",
    "start": "778880",
    "end": "784070"
  },
  {
    "text": "200k al use so basically as you can see they're just a little more in this but",
    "start": "784070",
    "end": "790550"
  },
  {
    "text": "we haven't finished filing on that so but basically in general what we want is one deal these should approximately map",
    "start": "790550",
    "end": "796880"
  },
  {
    "text": "to one layer this is something pretty nice so you take a resonant lay it out pretty easy for to lay out for any",
    "start": "796880",
    "end": "803029"
  },
  {
    "text": "further software side of things so to give you an idea of about how big these steelies are and how I guess performant",
    "start": "803029",
    "end": "810079"
  },
  {
    "text": "each one is obviously it's possible that some layers are going to be much bigger and in this case you can several dealies can",
    "start": "810079",
    "end": "816800"
  },
  {
    "text": "basically work together and the most common way to do this is going to be that one dle basically operates by",
    "start": "816800",
    "end": "822140"
  },
  {
    "text": "taking all the memory from the other dailies because most more likely than that the layers are not going to fit",
    "start": "822140",
    "end": "827300"
  },
  {
    "text": "into one dle by because it doesn't have enough memory and since the memory is the the most at least it's close to at",
    "start": "827300",
    "end": "835550"
  },
  {
    "text": "least around 80 to 90 percent of to die of each dle so it's probably going to take the memory from neighboring dailies",
    "start": "835550",
    "end": "842180"
  },
  {
    "text": "and so the wit interconnection works is also very as you can see it's pretty simple it's a grid layout with one with",
    "start": "842180",
    "end": "849110"
  },
  {
    "text": "one exception which is what these things which we call flyover networks which are basically as they sound they allow",
    "start": "849110",
    "end": "854950"
  },
  {
    "text": "instead of having neighbor nearest neighbor connections they allow connections to kind of further dailies",
    "start": "854950",
    "end": "860840"
  },
  {
    "text": "and a reason why that's useful is because we're starting to add decent called skip connections pretty common to",
    "start": "860840",
    "end": "867130"
  },
  {
    "text": "pretty common nowadays but basically the general idea is that normal well normal",
    "start": "867130",
    "end": "872390"
  },
  {
    "text": "well previously like something like Alex net would just have layer 1 feeds into layer 2 fees into layer 3 and so forth",
    "start": "872390",
    "end": "878450"
  },
  {
    "text": "so very nice and simple DLE mapping but now we have res nets which maybe have one to four layer one",
    "start": "878450",
    "end": "885680"
  },
  {
    "text": "to four has a connection and you know that that's possible in a grid layout but also more inefficient so these",
    "start": "885680",
    "end": "891230"
  },
  {
    "text": "flyover networks basically allow for faster communication there and we there's a level 1 and level 2 right now",
    "start": "891230",
    "end": "897950"
  },
  {
    "text": "I just have one for illustration purposes and this is useful because the",
    "start": "897950",
    "end": "903530"
  },
  {
    "text": "layer 1 basically interconnects 3 dailies so in a row and in a column and",
    "start": "903530",
    "end": "908680"
  },
  {
    "text": "level 2 network interconnects the entire row of them so that's for more general purpose programmability it's slower of",
    "start": "908680",
    "end": "915680"
  },
  {
    "text": "course but allows for more general-purpose programming and there's no real okay there's no major",
    "start": "915680",
    "end": "922610"
  },
  {
    "text": "instruction decoders like you're probably used to like you have some amount of instruction memory and you",
    "start": "922610",
    "end": "928550"
  },
  {
    "text": "have some dynamic instruction decoder and you can program it that way instead it's programs like C GRA would be so",
    "start": "928550",
    "end": "935120"
  },
  {
    "text": "like a coarse grained reconfigurable array it's similar to that's kind of an analogy in a way so basically you",
    "start": "935120",
    "end": "940640"
  },
  {
    "text": "statically reconfigure it and if you're using something on dynamic graph stand we have the mechanism or that as well basically you lay out",
    "start": "940640",
    "end": "946880"
  },
  {
    "text": "how it could be how all the possibilities the graph could take and then you decide dynamically where it to",
    "start": "946880",
    "end": "952310"
  },
  {
    "text": "go so kind of like tensorflow fold if you're familiar with that mapped onto hardware an entire trip is asynchronous",
    "start": "952310",
    "end": "957320"
  },
  {
    "text": "so these so these aisle interfaces basically their job is to make the chip look synchronous from to the outside",
    "start": "957320",
    "end": "964340"
  },
  {
    "text": "world but internally it's asynchronous so this input basically receives its input at the clock signals and and then",
    "start": "964340",
    "end": "972500"
  },
  {
    "text": "basically that part is much simpler it simply is the clock is simply seen as a",
    "start": "972500",
    "end": "977750"
  },
  {
    "text": "synchronous validation token and on the outsider is a little bit more work for the output interface which basically has",
    "start": "977750",
    "end": "983630"
  },
  {
    "text": "to take basically has to ready - I'm ready the a synchronous and asynchronous",
    "start": "983630",
    "end": "989590"
  },
  {
    "text": "basically amount of data being flowed through and turning that into some sort of clocked at the outside world is going",
    "start": "989590",
    "end": "995540"
  },
  {
    "text": "to see and these each have small buffers associated with them as well yeah yeah",
    "start": "995540",
    "end": "1002290"
  },
  {
    "text": "yeah it's a distributive dealing so",
    "start": "1002290",
    "end": "1010330"
  },
  {
    "text": "probably means you you have to fuse them so that would be done statically so you take one step where you load all the",
    "start": "1010330",
    "end": "1016420"
  },
  {
    "text": "memory yeah and then you yeah so there would be one step reconfigure basically how the memory is going to be used so",
    "start": "1016420",
    "end": "1022240"
  },
  {
    "text": "for example if I were to do something like a convolutional net I don't have a huge I have to unroll that outside and",
    "start": "1022240",
    "end": "1028660"
  },
  {
    "text": "then load it in you know how it's you do like say a three by three you have sure",
    "start": "1028660",
    "end": "1034000"
  },
  {
    "text": "horse take the image then you have to I'm rolling into a night at nine folds inner image right you mean for the in",
    "start": "1034000",
    "end": "1040959"
  },
  {
    "text": "Toccoa implementations yeah I mean I don't know is there a better implementation yeah we basically do the",
    "start": "1040960",
    "end": "1046240"
  },
  {
    "text": "spatial convolution directly but the way it works is basically arm so yeah so in",
    "start": "1046240",
    "end": "1051730"
  },
  {
    "text": "the if the input interfaces will like I guess this kind of store the entirety left up input as much as you want until",
    "start": "1051730",
    "end": "1059230"
  },
  {
    "text": "it goes into DL you spit maybe we're not answering their question yeah yeah and",
    "start": "1059230",
    "end": "1069280"
  },
  {
    "text": "here's kind of general block diagram it's kind of coarse-grained but gives you a general idea of what it looks like",
    "start": "1069280",
    "end": "1076320"
  },
  {
    "text": "yeah do you have questions oh okay yeah so we",
    "start": "1076320",
    "end": "1081750"
  },
  {
    "start": "1080000",
    "end": "1340000"
  },
  {
    "text": "did say that we have circuit level improvements so I guess an underappreciated source of data movement",
    "start": "1081750",
    "end": "1087450"
  },
  {
    "text": "and I put that in quotes because when you think of data movement you usually think shuffling large activation tensors or large weight matrices but there's",
    "start": "1087450",
    "end": "1094980"
  },
  {
    "text": "also on clock lines which do transmit some form of information so in a way",
    "start": "1094980",
    "end": "1099990"
  },
  {
    "text": "kind of data movement but not exactly so basically the reason why this is",
    "start": "1099990",
    "end": "1105150"
  },
  {
    "text": "important now it or actually has been historically as well as because as consumes a significant amount of power and there's other issues with it as well",
    "start": "1105150",
    "end": "1111720"
  },
  {
    "text": "that as maybe a consumer you don't care too much about but the designer has to deal with a clock skew jitter and all",
    "start": "1111720",
    "end": "1117300"
  },
  {
    "text": "these issues which are really irritating but so historically the solution to this has been something called asynchronous",
    "start": "1117300",
    "end": "1122309"
  },
  {
    "text": "logic which is this somewhat bold notion of let's say let's get rid of the clock and basically put the onus on the",
    "start": "1122309",
    "end": "1128130"
  },
  {
    "text": "circuit elements to perform data validity checks but basically the problem is it historically there's",
    "start": "1128130",
    "end": "1134460"
  },
  {
    "text": "something like a 2x or greater overhead like if you look at for example null convention logic you're pretty much",
    "start": "1134460",
    "end": "1140640"
  },
  {
    "text": "going to have a dual real implementation where you essentially end up doubling the number of rails for every boolean",
    "start": "1140640",
    "end": "1145950"
  },
  {
    "text": "value you would have and you'd have to have possibly more depending on your implementation you might need hysteresis",
    "start": "1145950",
    "end": "1151460"
  },
  {
    "text": "holdback and other additional transistors as well and the problem is",
    "start": "1151460",
    "end": "1156540"
  },
  {
    "text": "it's it's not really super feasible to put that a into a commercial product and B to make that necessarily super",
    "start": "1156540",
    "end": "1162240"
  },
  {
    "text": "performance such that it doesn't impact your area too much and so our implemented and this is at the 4-bit ALU",
    "start": "1162240",
    "end": "1167670"
  },
  {
    "text": "level for a full cost of implementation is 10% so if you've been into any of the async conferences you'll know that",
    "start": "1167670",
    "end": "1173820"
  },
  {
    "text": "that's probably quite a leap from what you're used to saying and I do need to mention it's a full custom design so",
    "start": "1173820",
    "end": "1180780"
  },
  {
    "text": "it's that doesn't mean that we can synthesize any arbitrary circuit into that but for the purposes of our deep",
    "start": "1180780",
    "end": "1186420"
  },
  {
    "text": "learning chip it's adequate and getting very high performance so yeah and of",
    "start": "1186420",
    "end": "1192840"
  },
  {
    "text": "course you get all the additional performance and other guarantees of async if you can satisfy the QDI",
    "start": "1192840",
    "end": "1198990"
  },
  {
    "text": "requirements and you're probably depending on well we use qvi but if you can satisfy those QDI requirements then",
    "start": "1198990",
    "end": "1205170"
  },
  {
    "text": "you get the functionality immune to process variations much higher performance",
    "start": "1205170",
    "end": "1210720"
  },
  {
    "text": "we have about a 12 gigahertz clock and I of course gave her some clock run quotes because that's just an average number of",
    "start": "1210720",
    "end": "1217080"
  },
  {
    "text": "oscillations per second I instead and this is on 20 again so yeah and to give an example this is our simulated data at",
    "start": "1217080",
    "end": "1223530"
  },
  {
    "text": "0.7 bdd so basically these 4-bit ALU is our pipeline to make full max and you",
    "start": "1223530",
    "end": "1231120"
  },
  {
    "text": "can see well below 80 second delay and interesting thing about this these clock",
    "start": "1231120",
    "end": "1237090"
  },
  {
    "text": "rates so to speak is that the memory access is actually much closer to being done within one cycle as long as the",
    "start": "1237090",
    "end": "1243840"
  },
  {
    "text": "pipeline can be warmed up and so what we do is this really cool thing where we actually use the dynamic logic in cents",
    "start": "1243840",
    "end": "1249690"
  },
  {
    "text": "amps so we're actually able to utilize this to pipeline the memory and almost",
    "start": "1249690",
    "end": "1255900"
  },
  {
    "text": "access to memory and what one oscillation cycle would look at look like and so this is something somewhat",
    "start": "1255900",
    "end": "1261450"
  },
  {
    "text": "similar to if you look at CPUs for example they might be running at 4 gigahertz but that's only going to be available if you're selling it into",
    "start": "1261450",
    "end": "1268200"
  },
  {
    "text": "registers but for the most part of your l1 data cache for example might require something like 4 or 5 cycles to access",
    "start": "1268200",
    "end": "1274679"
  },
  {
    "text": "and of course more cycles more deeper you go into IRP the worse it is so we're actually able to a pipeline I get",
    "start": "1274679",
    "end": "1280380"
  },
  {
    "text": "something pretty close to running at what are the logic elements are running out which is pretty important to",
    "start": "1280380",
    "end": "1285690"
  },
  {
    "text": "maintaining good performance so just to give some data on in some pretty",
    "start": "1285690",
    "end": "1291870"
  },
  {
    "text": "waveforms I just add unready signal which is pretty much critical to all asynchronous implementations I have to",
    "start": "1291870",
    "end": "1298620"
  },
  {
    "text": "put almost because there's some exotic ones like a fan junction out there so compared this is compared to standard",
    "start": "1298620",
    "end": "1304049"
  },
  {
    "text": "cell implementation again on 28 nanometer depending on the voltage it's going to vary depending on the voltage",
    "start": "1304049",
    "end": "1309330"
  },
  {
    "text": "is going to vary so we're we aim to run around a nominal VDD of 0.7 volts so",
    "start": "1309330",
    "end": "1315510"
  },
  {
    "text": "that we get some very nice advantages are much faster and much lower power than a standard cellular implementation",
    "start": "1315510",
    "end": "1324500"
  },
  {
    "text": "huh in a waveform think the green yeah",
    "start": "1324650",
    "end": "1333510"
  },
  {
    "text": "the green ones isn't the red is standard Sofia",
    "start": "1333510",
    "end": "1337700"
  },
  {
    "text": "yeah so uh I did mention that getting moving data off diets extremely",
    "start": "1339860",
    "end": "1346259"
  },
  {
    "start": "1340000",
    "end": "1507000"
  },
  {
    "text": "expensive so basically the question is if how can we fix that right so the immediate solution as well let's just",
    "start": "1346259",
    "end": "1352860"
  },
  {
    "text": "put everything on to the diet unfortunately for the majority of use cases this isn't really super feasible you can only get so much memory with the",
    "start": "1352860",
    "end": "1360149"
  },
  {
    "text": "60s for himself so the solution our solution is well let's get get rid of 60s for himself and",
    "start": "1360149",
    "end": "1366179"
  },
  {
    "text": "so new memories holds new memory technologies in general kind of have a troubled past",
    "start": "1366179",
    "end": "1371639"
  },
  {
    "text": "so for us we have to have a couple of requirement which is that if we if in order for it to work especially being a",
    "start": "1371639",
    "end": "1377999"
  },
  {
    "text": "start-up we need to have a company which is we can't have any new materials - absolutely critical and we can't even have any process modifications so this",
    "start": "1377999",
    "end": "1385080"
  },
  {
    "text": "includes the new litho steps or anything of that variety so we were actually able to get that and we call a zero change to",
    "start": "1385080",
    "end": "1391230"
  },
  {
    "text": "the process itself after a zero change silicon photonics which means basically we're able to use a standard process and",
    "start": "1391230",
    "end": "1396629"
  },
  {
    "text": "get this memory cell on there but basically we're I should mention one thing we do need one one feature which",
    "start": "1396629",
    "end": "1404639"
  },
  {
    "text": "is generally considered an RF feature but is on 28 16 7 and we've caught up to",
    "start": "1404639",
    "end": "1410730"
  },
  {
    "text": "TSMC Samsung Global Foundries and they have no no plans to deprecate that feature so far but for the sake of",
    "start": "1410730",
    "end": "1417720"
  },
  {
    "text": "completion I do need to include that we need that feature for the cell to function and so it's not something",
    "start": "1417720",
    "end": "1423659"
  },
  {
    "text": "exotic like am i I'm capacitor but it is necessary but the results are quite nice so we're able to get 5 times denser than",
    "start": "1423659",
    "end": "1429629"
  },
  {
    "text": "the standard 60s Ram if we enable multilevel approximately 10 times denser and then again the standard 60s for him",
    "start": "1429629",
    "end": "1436649"
  },
  {
    "text": "and of course the big advantage here is in leakage SRAM leakage for all the IQ dominates a large portion especially",
    "start": "1436649",
    "end": "1442499"
  },
  {
    "text": "when you have a lot of memory probably dominates a leakage of the chip so it's very important that we reduce leakage",
    "start": "1442499",
    "end": "1448590"
  },
  {
    "text": "per bit there and as far as yeah in terms of the metal stack requirement",
    "start": "1448590",
    "end": "1454499"
  },
  {
    "text": "this new test room and also how does",
    "start": "1454499",
    "end": "1462269"
  },
  {
    "text": "that affect routing dusty so there's nothing in there's nothing in the metal stocks that diseases this is purely at",
    "start": "1462269",
    "end": "1468359"
  },
  {
    "text": "the transistor in substrate level yeah or how many for the actual wiring just",
    "start": "1468359",
    "end": "1474840"
  },
  {
    "text": "for a single bit cell how many ml layers are using a DA would depend on the implementation you could",
    "start": "1474840",
    "end": "1480900"
  },
  {
    "text": "you can use to start yeah yeah for the the standard lines yeah yeah and so of",
    "start": "1480900",
    "end": "1488850"
  },
  {
    "text": "course the question is does this thing actually exist will it work and well we're pretty certain in confident to the will we've done some tea CAD simulations",
    "start": "1488850",
    "end": "1495360"
  },
  {
    "text": "they're pretty promising for getting multi-level operations so fingers crossed for that and next month we've got a shuttle going out so yeah so 3",
    "start": "1495360",
    "end": "1509580"
  },
  {
    "start": "1507000",
    "end": "1632000"
  },
  {
    "text": "sucking so this is another way we're able to reduce data movement which is let's get more memory onto the onto the",
    "start": "1509580",
    "end": "1516540"
  },
  {
    "text": "chip right so one way to do is 3d stocking number of problems with current with the current ways we do 3d stocking wide",
    "start": "1516540",
    "end": "1522960"
  },
  {
    "text": "isn't necessarily super popular one of them is without a doubt through silicon via TSP has a lot of disadvantages he",
    "start": "1522960",
    "end": "1530550"
  },
  {
    "text": "had a ski lodge keep out zones and requirement for electrostatic discharge protection could go on and on parasitic",
    "start": "1530550",
    "end": "1537000"
  },
  {
    "text": "capacitances etc but point is something better is really desired so this is kind",
    "start": "1537000",
    "end": "1543420"
  },
  {
    "text": "of our wireless link as you can see very good numbers um compared to what the TSV would look like out of 40 gigabits per",
    "start": "1543420",
    "end": "1550200"
  },
  {
    "text": "second is to demonstrate the total the total bandwidth that you can push through each link doesn't necessarily",
    "start": "1550200",
    "end": "1556440"
  },
  {
    "text": "mean that will I use 30s or something to actually push this but just that's what the link itself is capable of and it",
    "start": "1556440",
    "end": "1563940"
  },
  {
    "text": "does it only uses the metal layers so the it doesn't it doesn't go through it and it's wireless after all so basically",
    "start": "1563940",
    "end": "1571350"
  },
  {
    "text": "that's really advantageous since you can actually do work under there you have memory under there and it's release 'full and because of that again we don't",
    "start": "1571350",
    "end": "1578040"
  },
  {
    "text": "need electrostatic discharge protection which is really nice as it saves a significant amount of power especially",
    "start": "1578040",
    "end": "1583890"
  },
  {
    "text": "when you compare something like a TSV there is some crosstalk right now we're just dealing with that by spacing them",
    "start": "1583890",
    "end": "1589380"
  },
  {
    "text": "far apart which deals with the issue to drive bandwidth higher you can use techniques like multiplexing and thing",
    "start": "1589380",
    "end": "1594630"
  },
  {
    "text": "so yeah I have a pretty waveform there as you can see probably won't see this beautiful way from in silicon but based",
    "start": "1594630",
    "end": "1601350"
  },
  {
    "text": "on the comparisons we've gone 2 tsps and doctor coils it seems to be doing pretty well and again",
    "start": "1601350",
    "end": "1606420"
  },
  {
    "text": "these are all on 20 an animator on this as you can see a Fanta joules per bit that's pretty much as cheap as going if",
    "start": "1606420",
    "end": "1611790"
  },
  {
    "text": "maybe like 150 micron stage over on diets on to dye itself so you save a",
    "start": "1611790",
    "end": "1618390"
  },
  {
    "text": "significant amount of power simply by just having the memory 3d integrated on",
    "start": "1618390",
    "end": "1623940"
  },
  {
    "text": "there yeah yeah so we have a comparison",
    "start": "1623940",
    "end": "1634650"
  },
  {
    "text": "so this is just a show to maximum achievable bandwidth per square millimeter so the TSV is you don't get very much on",
    "start": "1634650",
    "end": "1640790"
  },
  {
    "text": "foreign gigabits per second pretty high power consumption for a bit there's one",
    "start": "1640790",
    "end": "1645930"
  },
  {
    "text": "big problem with that coefficient though is that there's no logic at all so the entire region of theis basically useless",
    "start": "1645930",
    "end": "1651110"
  },
  {
    "text": "in doctor clothes do much better about double the bandwidth similar amount of power now you can drive dot lower that",
    "start": "1651110",
    "end": "1657690"
  },
  {
    "text": "power lower or the bandwidth higher that's going to increase coil area or decrease or increase power consumption",
    "start": "1657690",
    "end": "1665040"
  },
  {
    "text": "if you want to drive to abandon with higher so I chose this to the optimal trade-off it's possible to get to stun to ten Fanta joules per bit but at the",
    "start": "1665040",
    "end": "1670980"
  },
  {
    "text": "cost of approximately quadrupling the area of that so that is there something but they do get logic underneath it",
    "start": "1670980",
    "end": "1677190"
  },
  {
    "text": "should we're able to drive up to tenter bits per second through these links per square millimeter at a significantly",
    "start": "1677190",
    "end": "1683760"
  },
  {
    "text": "reduced power consumption again we also get logic underneath it and we can drive the bandwidth even higher with",
    "start": "1683760",
    "end": "1689370"
  },
  {
    "text": "multiplexing we don't really have plans to do that right now but it's a possibility if yeah analog feature yeah all the",
    "start": "1689370",
    "end": "1700050"
  },
  {
    "text": "shooting on your red curve glitch before the transition what's that about the",
    "start": "1700050",
    "end": "1706110"
  },
  {
    "text": "simulation of that code because the inductive coupling or orange",
    "start": "1706110",
    "end": "1713120"
  },
  {
    "text": "undershooting machine of the red curve like real has a two bit slower",
    "start": "1713120",
    "end": "1719160"
  },
  {
    "text": "yeah I did not investigate no normally have that overshoot at the end of it",
    "start": "1719160",
    "end": "1725460"
  },
  {
    "text": "yeah anyway maybe after diagram reverse the digital signal processing scope",
    "start": "1725460",
    "end": "1732180"
  },
  {
    "text": "okay I think it's a digital signal processor in fact that's how we can yeah",
    "start": "1732180",
    "end": "1737520"
  },
  {
    "text": "we can talk more about well do you have this radiation pattern that sets your desk your wireless connection yeah one",
    "start": "1737520",
    "end": "1746910"
  },
  {
    "text": "of things though it's wireless yes yeah we can talk more about that okay but yeah hope your secrets so you know",
    "start": "1746910",
    "end": "1769470"
  },
  {
    "text": "because you're not using yeah yep but it seems to me but you have a coupling you",
    "start": "1769470",
    "end": "1776100"
  },
  {
    "text": "know whether it's going wirelessly or with a wire there still I don't see how I can go down by I guess maybe I don't",
    "start": "1776100",
    "end": "1784890"
  },
  {
    "text": "understand why it goes down oil because it's still coupled even though it's wireless right so okay there's a couple of reasons why first of all tsps has a",
    "start": "1784890",
    "end": "1791610"
  },
  {
    "text": "very high sorts atticus constants are very large the other reason is TSA you need electrostatic discharge protection",
    "start": "1791610",
    "end": "1797340"
  },
  {
    "text": "which adds a decent amount to it I mean again it's it doesn't we're not the only one we're not the only ones that has",
    "start": "1797340",
    "end": "1802500"
  },
  {
    "text": "power consumption this law I as I said through chip it's possible with inductive coupling to get your power",
    "start": "1802500",
    "end": "1807780"
  },
  {
    "text": "consumption down to around 10 fentanyl's I think for a bit but AD that's at the cost of signaling increased area the",
    "start": "1807780",
    "end": "1820020"
  },
  {
    "start": "1818000",
    "end": "1916000"
  },
  {
    "text": "other big problem the other big problem with 3d stocking has been basically the",
    "start": "1820020",
    "end": "1825300"
  },
  {
    "text": "heat problem right so if you have 2 Dyson and your total TDP or something like in the range of 300 watts it's not gonna it's not going to be very",
    "start": "1825300",
    "end": "1831720"
  },
  {
    "text": "thermally feasible to stock these two dice on top of each other and so basically that for the majority of high",
    "start": "1831720",
    "end": "1837840"
  },
  {
    "text": "performance applications just as kind of help to really limit just straight 3d integration I mean 2.5 D is still",
    "start": "1837840",
    "end": "1844020"
  },
  {
    "text": "feasible a straight 3d not so much and so basically the goal is so we need to",
    "start": "1844020",
    "end": "1849960"
  },
  {
    "text": "find some way to deal with the thermal problems the gigantic issue especially if we're talking about something like",
    "start": "1849960",
    "end": "1855840"
  },
  {
    "text": "Volta TDP dice so so here are some here's some not our as you can see",
    "start": "1855840",
    "end": "1863070"
  },
  {
    "text": "obviously not ours but some thermal analysis models that we",
    "start": "1863070",
    "end": "1868240"
  },
  {
    "text": "and disclose hours here for some reason for good reasons but we'll eventually obviously but as you can see the general",
    "start": "1868240",
    "end": "1874960"
  },
  {
    "text": "point still stands there basically these compute regions and of course the center of Todai have these hotspots and",
    "start": "1874960",
    "end": "1881190"
  },
  {
    "text": "basically we need to find so there's an observation basically and we need to find some way to deal with it and other",
    "start": "1881190",
    "end": "1887530"
  },
  {
    "text": "observation is that these hotspots also especially appear around compute region for the most part sans computer or on",
    "start": "1887530",
    "end": "1894309"
  },
  {
    "text": "average going to have higher thermal thermal density than the memory regions the memory regions may have they may",
    "start": "1894309",
    "end": "1900520"
  },
  {
    "text": "comprise more power power consumption overall in the entire die but on a per",
    "start": "1900520",
    "end": "1906820"
  },
  {
    "text": "per per capita basis I guess they have a higher compute region to have a higher",
    "start": "1906820",
    "end": "1912010"
  },
  {
    "text": "thermal density yeah so there's a couple",
    "start": "1912010",
    "end": "1917350"
  },
  {
    "start": "1916000",
    "end": "2120000"
  },
  {
    "text": "of ways we had actually attacked a thermal wall on this is one of them the other ones we haven't finished patents",
    "start": "1917350",
    "end": "1922840"
  },
  {
    "text": "on so excuse me for that but basically the idea is that well let's take a it's",
    "start": "1922840",
    "end": "1927910"
  },
  {
    "text": "basically exploit what we've learned from thermal analyses and it's kind of obviously applied so what do we get well",
    "start": "1927910",
    "end": "1934270"
  },
  {
    "text": "the first thing we get is well memory regions are cooler than compute regions at least per at least in terms of",
    "start": "1934270",
    "end": "1940600"
  },
  {
    "text": "density so let's not put them on top of each other let's only put memory regions on top of each other and leave compute",
    "start": "1940600",
    "end": "1946000"
  },
  {
    "text": "regions exposed we also this thing called thermal multiplexing which is basically and not acts not dynamically",
    "start": "1946000",
    "end": "1952600"
  },
  {
    "text": "activate regions of to die that are kind of directly on top of each other since that means basically on the low thermal",
    "start": "1952600",
    "end": "1958690"
  },
  {
    "text": "resistance paths that are available here are already going to be saturated and of course more heat in one region not a",
    "start": "1958690",
    "end": "1963700"
  },
  {
    "text": "good thing if you're familiar with the literature on power multiplexing this is somewhat similar in the sense that we're",
    "start": "1963700",
    "end": "1970330"
  },
  {
    "text": "doing it in three dimensions instead of two dimensions and we're doing it at doing it on a more I guess on a finer",
    "start": "1970330",
    "end": "1977620"
  },
  {
    "text": "grained basis rather than on upper core basis so for example these ones are probably going to be our region's off",
    "start": "1977620",
    "end": "1983710"
  },
  {
    "text": "the memory rate that are activated so this is so the first one is what we don't want second to is in through time",
    "start": "1983710",
    "end": "1990309"
  },
  {
    "text": "basically how we access them so we might dynamically activate these regions and these regions and we can schedule this",
    "start": "1990309",
    "end": "1997120"
  },
  {
    "text": "out because the deep learning workload is deterministic enough",
    "start": "1997120",
    "end": "2001550"
  },
  {
    "text": "yeah yeah before operations begin we're going to schedule how these are going to be activated so right but I mean in",
    "start": "2004620",
    "end": "2017020"
  },
  {
    "text": "general you're not going to be able to dynamically activate call if you're dying in any case right at maximum yeah I mean this can be done on a very",
    "start": "2017020",
    "end": "2024940"
  },
  {
    "text": "fine-grained basis like so like yeah my",
    "start": "2024940",
    "end": "2032290"
  },
  {
    "text": "plan just know whatever excess talker is talk right you know they basically activate everything all the time well",
    "start": "2032290",
    "end": "2039280"
  },
  {
    "text": "right right so but well for one this thing generally happens in memory since it's primarily memory gets going to be",
    "start": "2039280",
    "end": "2045580"
  },
  {
    "text": "stocked so it's easier to do that there I mean they well in a way like at least",
    "start": "2045580",
    "end": "2051550"
  },
  {
    "text": "they occupy most of her compute regions when it's pipelined but there's definitely still Riyad at least for most",
    "start": "2051550",
    "end": "2057340"
  },
  {
    "text": "matrices and most more close there's going to be periods of time where parts of it aren't activated right like you're",
    "start": "2057340",
    "end": "2064030"
  },
  {
    "text": "very rarely going to get full utilization if you look at the TPU v1 paper which I presume you're referring to on they only get like really good",
    "start": "2064030",
    "end": "2071590"
  },
  {
    "text": "utilization on like one or two workloads for the majority of four networks are only getting something like I don't",
    "start": "2071590",
    "end": "2077110"
  },
  {
    "text": "remember the exact numbers but one of us as low as 11 percent so maybe all our memory sometimes it's not a problem",
    "start": "2077110",
    "end": "2083200"
  },
  {
    "text": "yeah so yeah that's what we're doing here anyway so this wouldn't be really confusing so what would you see as a",
    "start": "2083200",
    "end": "2088990"
  },
  {
    "text": "like guests of activity factor or the logic portion of the chip so for our",
    "start": "2088990",
    "end": "2094510"
  },
  {
    "text": "chip or yeah um probably in the 50 percentage range that I would give you a delivery of petaflop okay yes yeah and",
    "start": "2094510",
    "end": "2103210"
  },
  {
    "text": "that's the equivalent of your switching activity yes researching activity and though into logic only yeah oh so this",
    "start": "2103210",
    "end": "2111640"
  },
  {
    "text": "scheduling is done by the compiler ahead of time yeah so I did promise in my",
    "start": "2111640",
    "end": "2121990"
  },
  {
    "start": "2120000",
    "end": "2366000"
  },
  {
    "text": "abstract comparison to some other up to deep learning chips before I start I just want to make it very clear I don't",
    "start": "2121990",
    "end": "2128500"
  },
  {
    "text": "mean this is an attack on anyone and I want to just say that when I really I",
    "start": "2128500",
    "end": "2133750"
  },
  {
    "text": "guess getting going on about these external memories the reason it's not necessarily",
    "start": "2133750",
    "end": "2139930"
  },
  {
    "text": "a bad design choice given the current technology to use off trip memory and a high bandwidth memory formats so just",
    "start": "2139930",
    "end": "2147010"
  },
  {
    "text": "keep that and bear that in mind ah so let's start here first of all they've publicly stated that alder",
    "start": "2147010",
    "end": "2153580"
  },
  {
    "text": "memory is off die and immediately as you can see it's going to be a little bit of a problem to bring all your memory in",
    "start": "2153580",
    "end": "2159520"
  },
  {
    "text": "off diets simply want to be infeasible right so dot right there pretty much is going to be the and if that a graph core for one",
    "start": "2159520",
    "end": "2167170"
  },
  {
    "text": "at least they do huh get the idea right you want to reduce data movement and you want to increase memory bandwidth since",
    "start": "2167170",
    "end": "2173050"
  },
  {
    "text": "these are the real bottlenecks in performance but unfortunately they all have a couple of issues first of all",
    "start": "2173050",
    "end": "2179950"
  },
  {
    "text": "this all to all interconnect which is necessitated on both dice which is necessitated by their ah which",
    "start": "2179950",
    "end": "2187510"
  },
  {
    "text": "necessitated by basically their synchronization scheme they call it bulk synchronous parallel but basically these",
    "start": "2187510",
    "end": "2193330"
  },
  {
    "text": "things are going to be serious bottlenecks especially as you scale it onwards like the I guess if you general",
    "start": "2193330",
    "end": "2199510"
  },
  {
    "text": "think was at the back end of the line is not so glamorous as the front end of the line but to interconnect delay in the",
    "start": "2199510",
    "end": "2204520"
  },
  {
    "text": "back end if the line is really delimitation limited limiting factors here and as you can see if you have this",
    "start": "2204520",
    "end": "2209770"
  },
  {
    "text": "which is going to be long wires it's going to dominate and eventually if you keep scaling this as you get more",
    "start": "2209770",
    "end": "2215980"
  },
  {
    "text": "transistors but not necessarily improvement interconnects it's gonna start to strangle the design and of",
    "start": "2215980",
    "end": "2221290"
  },
  {
    "text": "course there is also these um chip to chip links they only have 300 megabytes on die so that 600 megabytes total but",
    "start": "2221290",
    "end": "2227650"
  },
  {
    "text": "only six and remember it's on die on each die so in order to go to the next site you need to go through these I",
    "start": "2227650",
    "end": "2233440"
  },
  {
    "text": "presume use of service links and those are going to be under loom realm of at least 10 people joules per byte depending on what you use so those are",
    "start": "2233440",
    "end": "2241240"
  },
  {
    "text": "going to be very expensive as well so it's a really close to 300 megabytes on die and even then 600 megabytes you know",
    "start": "2241240",
    "end": "2247720"
  },
  {
    "text": "depending on what memory are not memory whatever data sets you're using on it is",
    "start": "2247720",
    "end": "2253150"
  },
  {
    "text": "probably not going to be enough and depending on what model you're running as well there I mean no 1080p images are",
    "start": "2253150",
    "end": "2259300"
  },
  {
    "text": "quite a big bitter bigger than the image net images sizes that we use so we're able to get our memory on chip to about",
    "start": "2259300",
    "end": "2266620"
  },
  {
    "text": "128 an ohmmeter to about half gigabytes so this is pretty much close enough to fit most of the models",
    "start": "2266620",
    "end": "2272470"
  },
  {
    "text": "that you're running today pretty nicely compact neon dye and on 7 and the other",
    "start": "2272470",
    "end": "2279670"
  },
  {
    "text": "FinFET process that are much smaller were able to get much more memory into 6 to 8 gigabyte range and of course we",
    "start": "2279670",
    "end": "2285340"
  },
  {
    "text": "have about five times as much overall peak performance that we see there anyone have a question yeah 600 square",
    "start": "2285340",
    "end": "2293560"
  },
  {
    "text": "millimeter and with the 3d information yeah wait now six hours for a millimeter on on top of that we have to read an to",
    "start": "2293560",
    "end": "2300130"
  },
  {
    "text": "briefing on it okay so on just the logic die or earth so that one half is full",
    "start": "2300130",
    "end": "2305800"
  },
  {
    "text": "dies yeah yeah this is just to give an",
    "start": "2305800",
    "end": "2316990"
  },
  {
    "text": "example what it could be but I mean for in okay little dip it depends on what",
    "start": "2316990",
    "end": "2322540"
  },
  {
    "text": "arm we can drive does higher actually we have plans but that we have plans to drive that higher you know they pull off",
    "start": "2322540",
    "end": "2332290"
  },
  {
    "text": "the memory wall yeah three times earlier",
    "start": "2332290",
    "end": "2337590"
  },
  {
    "text": "well okay well I'll I'll it in terms of data set eyelid like so say the data",
    "start": "2339510",
    "end": "2345940"
  },
  {
    "text": "side that's a different story but in terms of memory just in general memory aisle where you probably will reach that",
    "start": "2345940",
    "end": "2352800"
  },
  {
    "text": "sure it'll be layer but my point is for like probably let's say nine percent of workloads this will be enough and we",
    "start": "2352800",
    "end": "2358120"
  },
  {
    "text": "have plans to drive this higher as well that we you know haven't been able to publicly reveal yet so so compared to oh",
    "start": "2358120",
    "end": "2366970"
  },
  {
    "text": "this trip actually more information became available out just this morning so the general point still stands going",
    "start": "2366970",
    "end": "2372670"
  },
  {
    "text": "to off chip memories might be very expensive about hundred twenty times more expensive instead of two forty times the other thing is basically each",
    "start": "2372670",
    "end": "2379360"
  },
  {
    "text": "they have about 30 megabytes of total memory on-die information as of this morning they have and they're pretty",
    "start": "2379360",
    "end": "2386020"
  },
  {
    "text": "much they're relying upon the software manage memory on high bandwidth memory",
    "start": "2386020",
    "end": "2391620"
  },
  {
    "text": "and to manage that interspersed answers across memory so I'm going to that's",
    "start": "2391620",
    "end": "2397030"
  },
  {
    "text": "approximately what they said in the release so I'm going to assume that to me and they say they're relying heavily upon their",
    "start": "2397030",
    "end": "2402520"
  },
  {
    "text": "been with memory too due to heavy lifting and that's going to be extremely energy intensive to do another interesting thing is DS about just large",
    "start": "2402520",
    "end": "2409510"
  },
  {
    "text": "memories in general not necessarily specific to Nirvana but the physical interfaces for high bandwidth memory if",
    "start": "2409510",
    "end": "2414670"
  },
  {
    "text": "you've looked at let's say die shots of a really large GPU like a Fiji or something you'll notice that the HBM",
    "start": "2414670",
    "end": "2420490"
  },
  {
    "text": "physical interface is actually quite large so even going through that will inish it will incur some level of data",
    "start": "2420490",
    "end": "2425920"
  },
  {
    "text": "movement cost on-die of course nothing compared to how much this is going to cost but just something keep in mind",
    "start": "2425920",
    "end": "2432630"
  },
  {
    "start": "2434000",
    "end": "2525000"
  },
  {
    "text": "I'll compare it to the TPU so TPU v1 I'm probably a little bit rushed which",
    "start": "2434040",
    "end": "2440470"
  },
  {
    "text": "explains a couple of decisions ITB v2 may be a bit of a better comparison but just - nonetheless to",
    "start": "2440470",
    "end": "2446860"
  },
  {
    "text": "give an example here basically they have this unified memory which is really kind",
    "start": "2446860",
    "end": "2452860"
  },
  {
    "text": "of throw some off to a large extent they're only able to get about 24 megabytes onto there which is enough for",
    "start": "2452860",
    "end": "2458980"
  },
  {
    "text": "some cases but still not still not going to be ideal and plusted by having such a",
    "start": "2458980",
    "end": "2464500"
  },
  {
    "text": "large single memory raid being monolithic I feel me that's how their actual implementation is it's going to be very expensive in terms of just in",
    "start": "2464500",
    "end": "2471130"
  },
  {
    "text": "general accessing that thing data movement on the die itself just even inside a memory array and basically the",
    "start": "2471130",
    "end": "2478390"
  },
  {
    "text": "in you can probably incur some pretty gigantic capacitances on two-bit lines",
    "start": "2478390",
    "end": "2484150"
  },
  {
    "text": "if you have such a large memory array and the other thing is there the bandwidth to and from their from their",
    "start": "2484150",
    "end": "2492670"
  },
  {
    "text": "memory render matrix multiply and just in general they're computing it's is very low we're able to get around and",
    "start": "2492670",
    "end": "2497950"
  },
  {
    "text": "we're from a hundred to and I think a thousand times yeah a thousand times that in aggregate bandwidth so a",
    "start": "2497950",
    "end": "2503110"
  },
  {
    "text": "significant greater there and basically the two costs they really suffer from are from going to be on chip data",
    "start": "2503110",
    "end": "2508660"
  },
  {
    "text": "movement and on the first generation to ended up accessing drm as well so that's gonna be an enormous ly expensive but",
    "start": "2508660",
    "end": "2514660"
  },
  {
    "text": "again the TPU b2 doesn't use that so maybe a better comparison should be there yeah yeah so again it's a better",
    "start": "2514660",
    "end": "2526900"
  },
  {
    "start": "2525000",
    "end": "2566000"
  },
  {
    "text": "comparison probably to TPU v2 so again the problem here is really the high",
    "start": "2526900",
    "end": "2532030"
  },
  {
    "text": "bandwidth memory I'm going off diet just going to be too expensive to really be useful day out fixed systole stay",
    "start": "2532030",
    "end": "2538900"
  },
  {
    "text": "Seder systolic erase systolic array matrix multiplier units so these things really don't have any program ability so",
    "start": "2538900",
    "end": "2545410"
  },
  {
    "text": "I mean that's could be a disadvantage if you're Google scale you probably can make another chip if something else",
    "start": "2545410",
    "end": "2550720"
  },
  {
    "text": "becomes common but it's just something important to keep in mind would be would be that and of course this seems to be a",
    "start": "2550720",
    "end": "2556300"
  },
  {
    "text": "largely monolithic design as well on here with some advantages since I split into two at least there and I also get",
    "start": "2556300",
    "end": "2567370"
  },
  {
    "start": "2566000",
    "end": "2722000"
  },
  {
    "text": "asked a lot what about like analog approaches right have you considered analog approaches and so well yes we",
    "start": "2567370",
    "end": "2572950"
  },
  {
    "text": "have actually we did a little side incursion on that a couple years ago but",
    "start": "2572950",
    "end": "2578830"
  },
  {
    "text": "basically there were some we found some serious problems with analog approaches in general so even for deep learning as",
    "start": "2578830",
    "end": "2585220"
  },
  {
    "text": "it turns out that deep learning can't tolerate like zeros are like really low",
    "start": "2585220",
    "end": "2592360"
  },
  {
    "text": "precision it does have some precision Browns it's more tolerant to lower precision than let's say a HPC workload but for the",
    "start": "2592360",
    "end": "2600370"
  },
  {
    "text": "most part it's still going to have some level of intolerance to low precision so as far as I've seen in literature",
    "start": "2600370",
    "end": "2606640"
  },
  {
    "text": "there's been no evidence of that there's been at least one common I think mythic that said that they can retain accuracy",
    "start": "2606640",
    "end": "2612040"
  },
  {
    "text": "okay I'll give them that but Miss Lee the current approach is first of all our flash memory-based this is a bit of a",
    "start": "2612040",
    "end": "2617350"
  },
  {
    "text": "big problem because there are the endurance issue and this is for top of declined SLC nor flash by the ways so",
    "start": "2617350",
    "end": "2623320"
  },
  {
    "text": "this is an Intendant more for iot sort of applications I presume not really for",
    "start": "2623320",
    "end": "2628390"
  },
  {
    "text": "high performance applications where you're continuously just running data the other thing is it requires very",
    "start": "2628390",
    "end": "2634390"
  },
  {
    "text": "intense power intensive area intensive and also very hard to scale are the 80",
    "start": "2634390",
    "end": "2640060"
  },
  {
    "text": "the 80 season backs right so those are going to be a serious issue as well the",
    "start": "2640060",
    "end": "2645820"
  },
  {
    "text": "flash memory itself if you've seen a Ganta or just even casually looked at a",
    "start": "2645820",
    "end": "2651040"
  },
  {
    "text": "flash memory scaling conference you'll see that scaling of a floating gate transistor is extremely difficult to do",
    "start": "2651040",
    "end": "2656740"
  },
  {
    "text": "and so that could possibly be an issue right and the final thing is it really doesn't so much salsa date a movement",
    "start": "2656740",
    "end": "2662440"
  },
  {
    "text": "problem and the reason for that is because so you in one processing element are not pressing on one memory array you",
    "start": "2662440",
    "end": "2668830"
  },
  {
    "text": "can do process let's say right analog computation but now if you want to move that to another let's say region of to die which you'll",
    "start": "2668830",
    "end": "2674589"
  },
  {
    "text": "need to no matter how dense it is then you're still going to need to transmit it over some sort of lines and the",
    "start": "2674589",
    "end": "2680529"
  },
  {
    "text": "problem in interconnect lines here is that if you seen just current sensing",
    "start": "2680529",
    "end": "2687400"
  },
  {
    "text": "current sensing by the ways has been proposed as an alternative to current interconnection schemes to increase the",
    "start": "2687400",
    "end": "2694839"
  },
  {
    "text": "speed and but if you look at it it looks a lot like a damped harmonic oscillator that degres extremely quickly so it's",
    "start": "2694839",
    "end": "2701769"
  },
  {
    "text": "pretty much impossible to retain a good analog signal going over those lines and you're going to need to add some sort of",
    "start": "2701769",
    "end": "2707559"
  },
  {
    "text": "compensation like and most likely this is going to mean you're turning into digital signal which you're back to",
    "start": "2707559",
    "end": "2713140"
  },
  {
    "text": "square one in terms of data movement there yeah and I mean so there's",
    "start": "2713140",
    "end": "2718210"
  },
  {
    "text": "difficulties with that as well and so finally we get to the part where okay",
    "start": "2718210",
    "end": "2723849"
  },
  {
    "start": "2722000",
    "end": "2795000"
  },
  {
    "text": "this is great I don't care how good if a processor you have how to actually use it right maybe I'm a deep learning deep learning guy how to actually use this",
    "start": "2723849",
    "end": "2729999"
  },
  {
    "text": "thing right so this is really nice because the deep learning community us and all deferments have started to settle around this abstraction for",
    "start": "2729999",
    "end": "2737049"
  },
  {
    "text": "intermediate representations called the computational graph and most real Ness is Facebook Microsoft and Amazon have",
    "start": "2737049",
    "end": "2742450"
  },
  {
    "text": "gone in on this project which they call Onix and I think i pronouncing that correctly at least and basically does a",
    "start": "2742450",
    "end": "2747759"
  },
  {
    "text": "lot of - a lot of - work for us which is basically tensorflow towards whatever cafe whatever framework you want to use",
    "start": "2747759",
    "end": "2754180"
  },
  {
    "text": "it emits some sort of computational graph intermediate representation for example Onix has already been integrated",
    "start": "2754180",
    "end": "2759549"
  },
  {
    "text": "I think with PI torch and and tensorflow is a work in progress I think and import so basically we would intercept this and",
    "start": "2759549",
    "end": "2767380"
  },
  {
    "text": "we have our little graph compiler which isn't so much a compiler plus a little bit of a misnomer since it's relatively",
    "start": "2767380",
    "end": "2773739"
  },
  {
    "text": "simple to lay it out on to the actual hardware but we call the compiler sure DNN see see it's our nickname for it",
    "start": "2773739",
    "end": "2780970"
  },
  {
    "text": "after GCC but the real takeaway point here is that for you as the user don't see anything right so you write your",
    "start": "2780970",
    "end": "2787210"
  },
  {
    "text": "tensorflow code you go tomorrow and it runs exactly the same as it except you'll note yeah",
    "start": "2787210",
    "end": "2793589"
  },
  {
    "text": "leadership weight from this high level yeah condition is busy it's it's I mean",
    "start": "2794160",
    "end": "2800580"
  },
  {
    "text": "okay well compared to like something if we were trying to I would say poor LLVM canoe trip okay yeah I mean ithi will be",
    "start": "2800580",
    "end": "2806850"
  },
  {
    "text": "relative amigas compared to the yeah I suppose relative but I mean come yeah",
    "start": "2806850",
    "end": "2812040"
  },
  {
    "text": "compared to something like what like suppose us reporting LLVM to like a need to company architecture that would be",
    "start": "2812040",
    "end": "2817980"
  },
  {
    "text": "pretty difficult right why I'm not a compiler guy yeah you don't yeah you",
    "start": "2817980",
    "end": "2831360"
  },
  {
    "text": "don't see many of the mechanisms we have to make it easier for it but I'm busily rehab on Midgley the action more depth I",
    "start": "2831360",
    "end": "2839480"
  },
  {
    "text": "guess more at bar like will mean up to detail levels is that is a lot not a lot",
    "start": "2839480",
    "end": "2847140"
  },
  {
    "text": "but least a little bit differ from what you see like it's not clocked but it's done in such a way that the compiler",
    "start": "2847140",
    "end": "2853170"
  },
  {
    "text": "sees some some types of guarantees on it so it's easier for it's easier for it to schedule yeah there's a lot of stuff",
    "start": "2853170",
    "end": "2859230"
  },
  {
    "text": "dating we're able to review arcing talk more automatic so basic most general",
    "start": "2859230",
    "end": "2864840"
  },
  {
    "text": "solution as long as it works for let's say 99% of the cases we are happy yes",
    "start": "2864840",
    "end": "2870000"
  },
  {
    "text": "and like that last personification it's gonna run really slow but it's well you",
    "start": "2870000",
    "end": "2875850"
  },
  {
    "text": "know this like first shows where that's going to be the majority of applications yeah yeah yeah so the custom layer",
    "start": "2875850",
    "end": "2888360"
  },
  {
    "text": "basically becomes a little bit more difficult you but you have to implement it as some sort of some representation",
    "start": "2888360",
    "end": "2894870"
  },
  {
    "text": "of functions we're also from just operations I mean on in computational",
    "start": "2894870",
    "end": "2900390"
  },
  {
    "text": "graph so let's say your simple example would be like let's say I want implement",
    "start": "2900390",
    "end": "2905400"
  },
  {
    "text": "dilate the confluence right so then I would on represent it as some sort of",
    "start": "2905400",
    "end": "2911390"
  },
  {
    "text": "nesting out let's say addition and convolution functions in the computational graph and then we simply",
    "start": "2911390",
    "end": "2917010"
  },
  {
    "text": "take that in compile is just this normal the the issue arises when you start to use things like if you have a call to",
    "start": "2917010",
    "end": "2923070"
  },
  {
    "text": "library or something then you have to and then that's when things get hairy but I said that's when we - that's when the",
    "start": "2923070",
    "end": "2929490"
  },
  {
    "text": "compliation becomes difficult - but for cases like that it's going to slow down other chips quite a decent bit as well",
    "start": "2929490",
    "end": "2934860"
  },
  {
    "text": "but yeah so if we're basically again for this is antenna probably 99% of more flow this is going to be a perfect fit",
    "start": "2934860",
    "end": "2940830"
  },
  {
    "text": "yeah so working on basically confidence",
    "start": "2940830",
    "end": "2946170"
  },
  {
    "text": "Ellis teams are n ends the various application special transformer networks all sorts of things they're really the",
    "start": "2946170",
    "end": "2953040"
  },
  {
    "text": "only thing that's being it yeah yeah I'm fully connected of course that's the basis but I really it only becomes",
    "start": "2953040",
    "end": "2958800"
  },
  {
    "text": "difficult when you have so different amounts of control flow really that's when difficulty arises yeah you can",
    "start": "2958800",
    "end": "2968430"
  },
  {
    "text": "write your own but here again here's think the biggest thing here is that control flow basically slows it down so",
    "start": "2968430",
    "end": "2975390"
  },
  {
    "text": "I mean as a general heuristic control for slows it down yeah hi there's a",
    "start": "2975390",
    "end": "2980700"
  },
  {
    "text": "little bit more but again it depends on",
    "start": "2980700",
    "end": "2987600"
  },
  {
    "text": "how much but yeah it's really hard to estimate also depends on what it is right like if you have something similar like simple like what a rally would be",
    "start": "2987600",
    "end": "2993960"
  },
  {
    "text": "that may not be at all slow down but if it's something really complicated then like a very long switch or something",
    "start": "2993960",
    "end": "3000680"
  },
  {
    "text": "that could slow yeah um so basically",
    "start": "3000680",
    "end": "3005810"
  },
  {
    "text": "we'll do it for both but um our first chip for business reasons has to be an inference chip but we intend to",
    "start": "3005810",
    "end": "3012200"
  },
  {
    "text": "architecture to do both yeah then I mean",
    "start": "3012200",
    "end": "3018710"
  },
  {
    "text": "this is that's intended for a data center type we can cut that die up - yeah this was very interesting but are",
    "start": "3018710",
    "end": "3025880"
  },
  {
    "text": "you familiar with the similar system developed by Professor Jack Dennis at",
    "start": "3025880",
    "end": "3031460"
  },
  {
    "text": "MIT perchance I suspect you're not the data flow he he started pushing that",
    "start": "3031460",
    "end": "3039470"
  },
  {
    "text": "fifty five years ago without much success yeah that's what but it's similar enough",
    "start": "3039470",
    "end": "3047060"
  },
  {
    "text": "that since I opposed all patents I will",
    "start": "3047060",
    "end": "3052850"
  },
  {
    "text": "tip him off to try to blow a hole in yours",
    "start": "3052850",
    "end": "3057880"
  },
  {
    "text": "in terms of purple oz levels like what what do you see a difference between your matrix multiplication level 3 vs",
    "start": "3061749",
    "end": "3069440"
  },
  {
    "text": "level 1 level 2 no data on that yet I'm not gonna speculate well this isn't",
    "start": "3069440",
    "end": "3076130"
  },
  {
    "text": "intended to be like a like a this isn't intended to be like a kind of HPC trip",
    "start": "3076130",
    "end": "3081980"
  },
  {
    "text": "or Isis and we've been our benchmarking more like BG like we're close like well",
    "start": "3081980",
    "end": "3087739"
  },
  {
    "text": "not in features you so much but rather than that stuff like that how do you see the performance difference between your",
    "start": "3087739",
    "end": "3093200"
  },
  {
    "text": "sort of Alex net CNN versus you know now it's TM what sort of performance",
    "start": "3093200",
    "end": "3098720"
  },
  {
    "text": "drop-off yeah okay well this is entirely independent how large LST M is like any",
    "start": "3098720",
    "end": "3104809"
  },
  {
    "text": "model size estimate million activations million activations probably should you",
    "start": "3104809",
    "end": "3110930"
  },
  {
    "text": "cease you see something no drop-off at all so so each one of your en n units",
    "start": "3110930",
    "end": "3116059"
  },
  {
    "text": "contains one activation or no so so on our dealies yeah yeah for dealies have",
    "start": "3116059",
    "end": "3122470"
  },
  {
    "text": "large obviously there with their activation weight and memory is sector so basically its sector changeable so",
    "start": "3122470",
    "end": "3129710"
  },
  {
    "text": "they like there's I think there's six sectors we're going for it now yeah so basically the dos sectors contain some",
    "start": "3129710",
    "end": "3135859"
  },
  {
    "text": "amount of memory which can be reconfigured to be on all six four member mates only or all six four",
    "start": "3135859",
    "end": "3141589"
  },
  {
    "text": "activations and anything in between so dot so basically the point is that it",
    "start": "3141589",
    "end": "3147920"
  },
  {
    "text": "would be reconfigured approximately to how much activations you would have but it's definitely not one it's a lot more",
    "start": "3147920",
    "end": "3153140"
  },
  {
    "text": "I mean we only have like 2400 of these on a pretty large die right so you'd have fit several activations onto one",
    "start": "3153140",
    "end": "3159410"
  },
  {
    "text": "right yeah yeah versus in matrix multiplication where when you're",
    "start": "3159410",
    "end": "3164420"
  },
  {
    "text": "multiplying two matrices a and B and you're repeatedly accessing element a",
    "start": "3164420",
    "end": "3170059"
  },
  {
    "text": "matrix a or each multiplication there would be a drop off in your case versus",
    "start": "3170059",
    "end": "3176210"
  },
  {
    "text": "the scenario where you're not having that kind of cash friendly nature right right so this is again works it's not",
    "start": "3176210",
    "end": "3182509"
  },
  {
    "text": "necessarily intended to be for general purpose because if you write something look and let me answer Patrick so for",
    "start": "3182509",
    "end": "3188210"
  },
  {
    "text": "generally the way we have Alice name implemented right now is also like up on our side but it's implemented a little",
    "start": "3188210",
    "end": "3194940"
  },
  {
    "text": "bit more in a hardwired way so dead that's not so much of an issue I'm right now but if you go write something let's",
    "start": "3194940",
    "end": "3201000"
  },
  {
    "text": "say that's kind of looks similar to an LST M but isn't kind of is doesn't have let's say preset preset hardware to be",
    "start": "3201000",
    "end": "3208830"
  },
  {
    "text": "able to assist it you'll see a drop off there sure what is like the kind of a",
    "start": "3208830",
    "end": "3215460"
  },
  {
    "text": "deep form of your layers is a matrix vector product rather than a matrix matrix product yes because you know",
    "start": "3215460",
    "end": "3222630"
  },
  {
    "text": "existing architectures with their caching systems are so good at matrix multiplication that that's what they're",
    "start": "3222630",
    "end": "3229170"
  },
  {
    "text": "you know the problems reformed into level three blahs one rather than level",
    "start": "3229170",
    "end": "3234420"
  },
  {
    "text": "two and so if you're doing it a level two to form that reduces the amount of",
    "start": "3234420",
    "end": "3240180"
  },
  {
    "text": "work you actually need to do I mean yeah but again okay well this is kind of bringing back to the date for the most",
    "start": "3240180",
    "end": "3246630"
  },
  {
    "text": "part we see our performance drop off not necessarily when to compute requirements are different but when des memory",
    "start": "3246630",
    "end": "3252170"
  },
  {
    "text": "systems are different for the most part we don't see computing impacting the compute probably generally doesn't",
    "start": "3252170",
    "end": "3258120"
  },
  {
    "text": "impact is it that much in terms of how much it is it's going to be more impactful when the memory when let's say",
    "start": "3258120",
    "end": "3264090"
  },
  {
    "text": "we have way more memory in one layer and way less on other then it's we see memory being the primary difference in",
    "start": "3264090",
    "end": "3270660"
  },
  {
    "text": "terms of how much performance we compute you're still doing most each other that I get ride together it's multiply add its that yeah you're",
    "start": "3270660",
    "end": "3277590"
  },
  {
    "text": "just executing different yes yeah yeah I got what you mean by Mike yeah thanks a",
    "start": "3277590",
    "end": "3285120"
  },
  {
    "text": "question are these really matrices are rank one outer products amazed Darrell well you nested from called a rank one",
    "start": "3285120",
    "end": "3292920"
  },
  {
    "text": "alright the Google times finally admitted that tenses are I please rank to make our ch'yeah",
    "start": "3292920",
    "end": "3298620"
  },
  {
    "text": "the sum of two our products yeah it just like books yeah it's so is that the same thing here yeah because then you should",
    "start": "3298620",
    "end": "3304560"
  },
  {
    "text": "use tensor flow misses all I mean it is not tensile nobody perplexes I will what",
    "start": "3304560",
    "end": "3311820"
  },
  {
    "text": "I will okay I will agree that there's aa Arizona said and we use the word tensor a little bit too much in deep learning",
    "start": "3311820",
    "end": "3317700"
  },
  {
    "text": "but I mean no totally that ain't no tenths of a business bank",
    "start": "3317700",
    "end": "3324530"
  },
  {
    "text": "wanna make to matrix and it is not attention that's how you're gonna matrix actually other comments by precision or",
    "start": "3324530",
    "end": "3331670"
  },
  {
    "text": "more efficient I'm not the one who named okay if it was me I wouldn't have named",
    "start": "3331670",
    "end": "3340340"
  },
  {
    "text": "but yeah yeah so yeah of course someone is that I don't yeah yeah exactly",
    "start": "3340340",
    "end": "3347869"
  },
  {
    "text": "I mean tensor sounds better than matrix I suspect that's why Google yeah so yeah",
    "start": "3347869",
    "end": "3353420"
  },
  {
    "text": "time I'm basically okay when can I yeah",
    "start": "3353420",
    "end": "3366170"
  },
  {
    "text": "so one can I actually get one of course as I said the MPW is going to validate what I suppose could be a call that more",
    "start": "3366170",
    "end": "3371270"
  },
  {
    "text": "it's risky technologies to three stocking in the memory cell everything else will be if things go to plan we",
    "start": "3371270",
    "end": "3378170"
  },
  {
    "text": "hope to get some engineering samples shipping to some early early partners and eventually get a 28 something",
    "start": "3378170",
    "end": "3384710"
  },
  {
    "text": "shipping on 28 and eventually all the way to the 780 meter FinFET eventually that will be a mask owned yeah I think",
    "start": "3384710",
    "end": "3391580"
  },
  {
    "text": "that's about it",
    "start": "3391580",
    "end": "3394600"
  },
  {
    "text": "you",
    "start": "3400170",
    "end": "3402230"
  }
]