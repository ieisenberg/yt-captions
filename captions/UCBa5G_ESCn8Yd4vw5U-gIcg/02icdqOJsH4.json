[
  {
    "start": "0",
    "end": "11000"
  },
  {
    "start": "0",
    "end": "510"
  },
  {
    "text": "OK we saw there were situations\nwhere soft margin wasn't",
    "start": "510",
    "end": "4260"
  },
  {
    "text": "going to help us.",
    "start": "4260",
    "end": "5250"
  },
  {
    "text": "And so, we're going to find\nways of overcoming this problem.",
    "start": "5250",
    "end": "9490"
  },
  {
    "text": "And a natural way to do that\nis using feature expansion.",
    "start": "9490",
    "end": "13809"
  },
  {
    "start": "11000",
    "end": "125000"
  },
  {
    "text": "So what we can do,\none simple way,",
    "start": "13810",
    "end": "16079"
  },
  {
    "text": "is a standard trick\nis in larger features",
    "start": "16079",
    "end": "18450"
  },
  {
    "text": "by including transformations,\nsuch as polynomials.",
    "start": "18450",
    "end": "22990"
  },
  {
    "text": "Right?",
    "start": "22990",
    "end": "23789"
  },
  {
    "text": "So we started off with just--\nin this example, just X1 and X2.",
    "start": "23790",
    "end": "27330"
  },
  {
    "text": "We can add in X1 squared, X2\nsquared, X1 cubed, X1X2, and so",
    "start": "27330",
    "end": "35520"
  },
  {
    "text": "on.",
    "start": "35520",
    "end": "36425"
  },
  {
    "text": "Polynomial expansions.",
    "start": "36425",
    "end": "37930"
  },
  {
    "text": "So you can go from a\np-dimensional space,",
    "start": "37930",
    "end": "40800"
  },
  {
    "text": "in this case, 2 to a\nhigher dimensional space.",
    "start": "40800",
    "end": "43960"
  },
  {
    "text": "And the more transformed\nvariables you add,",
    "start": "43960",
    "end": "47160"
  },
  {
    "text": "the more likely you are to\nbe able to get separation",
    "start": "47160",
    "end": "50070"
  },
  {
    "text": "in this higher\ndimensional space.",
    "start": "50070",
    "end": "51798"
  },
  {
    "text": "I just noticed that\nthe M there, of course,",
    "start": "51798",
    "end": "53590"
  },
  {
    "text": "is not the same M as\nwe use for margin.",
    "start": "53590",
    "end": "55480"
  },
  {
    "text": "Oh.",
    "start": "55480",
    "end": "55980"
  },
  {
    "text": "So that's--",
    "start": "55980",
    "end": "56480"
  },
  {
    "text": "Good point.",
    "start": "56480",
    "end": "57065"
  },
  {
    "text": "Maybe we should use\na different letter.",
    "start": "57065",
    "end": "58690"
  },
  {
    "text": "OK.",
    "start": "58690",
    "end": "59219"
  },
  {
    "text": "OK.",
    "start": "59220",
    "end": "60050"
  },
  {
    "text": "Thank you.",
    "start": "60050",
    "end": "60940"
  },
  {
    "text": "That was a bad choice of letter.",
    "start": "60940",
    "end": "64900"
  },
  {
    "text": "That's usually the\nkind of mistake I make.",
    "start": "64900",
    "end": "66650"
  },
  {
    "text": "[CHUCKLES]",
    "start": "66650",
    "end": "67417"
  },
  {
    "text": "I'm glad I caught\nyou on one, finally.",
    "start": "67417",
    "end": "69000"
  },
  {
    "text": "It's a kind of\nmistake I hate making.",
    "start": "69000",
    "end": "72480"
  },
  {
    "text": "So in larger feature\nspace, and then fit",
    "start": "72480",
    "end": "75390"
  },
  {
    "text": "a linear support-vector\nmachine in the large space.",
    "start": "75390",
    "end": "78240"
  },
  {
    "text": "And then when you\nproject it down",
    "start": "78240",
    "end": "80610"
  },
  {
    "text": "to the original\nspace, the results--",
    "start": "80610",
    "end": "82150"
  },
  {
    "text": "it results in a\nnon-linear decision",
    "start": "82150",
    "end": "84150"
  },
  {
    "text": "boundary in the original space.",
    "start": "84150",
    "end": "86670"
  },
  {
    "text": "So for example, suppose we\nuse degree 2 polynomials,",
    "start": "86670",
    "end": "92310"
  },
  {
    "text": "so we will use X1, X2, X1\nsquared, X2 squared, and X1",
    "start": "92310",
    "end": "96180"
  },
  {
    "text": "times X2.",
    "start": "96180",
    "end": "97350"
  },
  {
    "text": "So that's a basis for\nfitting a general polynomial",
    "start": "97350",
    "end": "100830"
  },
  {
    "text": "in two variables of degree 2.",
    "start": "100830",
    "end": "103050"
  },
  {
    "text": "Well, in the enlarged\nspace, the decision boundary",
    "start": "103050",
    "end": "105360"
  },
  {
    "text": "would take this form.",
    "start": "105360",
    "end": "106580"
  },
  {
    "text": "It's linear in\nthe new variables.",
    "start": "106580",
    "end": "110460"
  },
  {
    "text": "So there's a coefficient\nfor each of them.",
    "start": "110460",
    "end": "112659"
  },
  {
    "text": "So that was the form of our\nseparate-- of our hyperplane.",
    "start": "112660",
    "end": "117090"
  },
  {
    "text": "But, of course, in the\noriginal variables,",
    "start": "117090",
    "end": "119130"
  },
  {
    "text": "it's nonlinear because we've\ngot squares and cross products.",
    "start": "119130",
    "end": "123000"
  },
  {
    "text": "OK?",
    "start": "123000",
    "end": "124446"
  },
  {
    "text": "And this is what it results\nin-- for the same example",
    "start": "124446",
    "end": "128259"
  },
  {
    "start": "125000",
    "end": "208000"
  },
  {
    "text": "that we saw.",
    "start": "128259",
    "end": "129669"
  },
  {
    "text": "The decision boundary, you can\nsee it's kind of split in two",
    "start": "129669",
    "end": "132700"
  },
  {
    "text": "now.",
    "start": "132700",
    "end": "133200"
  },
  {
    "text": "So remember, how do we get\nthese decision boundaries?",
    "start": "133200",
    "end": "136239"
  },
  {
    "text": "In the five-dimensional space,\nthere's a single linear decision",
    "start": "136240",
    "end": "140590"
  },
  {
    "text": "boundary.",
    "start": "140590",
    "end": "141860"
  },
  {
    "text": "OK?",
    "start": "141860",
    "end": "142360"
  },
  {
    "text": "And we can color all\nthe points on one side,",
    "start": "142360",
    "end": "144640"
  },
  {
    "text": "blue, and on the\nother side, mauve.",
    "start": "144640",
    "end": "146780"
  },
  {
    "text": "Well, if you\nproject it down into",
    "start": "146780",
    "end": "148600"
  },
  {
    "text": "this two-dimensional picture\nof the original variables,",
    "start": "148600",
    "end": "151030"
  },
  {
    "text": "this is what it looks like.",
    "start": "151030",
    "end": "152670"
  },
  {
    "text": "So these are conic sections\nof a quadratic polynomial",
    "start": "152670",
    "end": "157690"
  },
  {
    "text": "and this is what they look like.",
    "start": "157690",
    "end": "159980"
  },
  {
    "text": "And it's curious because in\nthe five-dimensional space,",
    "start": "159980",
    "end": "162970"
  },
  {
    "text": "there's a margin on either side\nof the linear boundary, which",
    "start": "162970",
    "end": "166300"
  },
  {
    "text": "are the dotted lines.",
    "start": "166300",
    "end": "167480"
  },
  {
    "text": "Well, they also project down.",
    "start": "167480",
    "end": "168890"
  },
  {
    "text": "And so there's a pair of margins\nfor this piece of the curve",
    "start": "168890",
    "end": "172390"
  },
  {
    "text": "and one for this.",
    "start": "172390",
    "end": "173480"
  },
  {
    "text": "You'll notice these\nare non-linear,",
    "start": "173480",
    "end": "174980"
  },
  {
    "text": "and they even ends up\nin two pieces here.",
    "start": "174980",
    "end": "177220"
  },
  {
    "text": "And importantly, in this case,\nthis does a really good job",
    "start": "177220",
    "end": "181450"
  },
  {
    "text": "in separating the two classes.",
    "start": "181450",
    "end": "183830"
  },
  {
    "text": "OK?",
    "start": "183830",
    "end": "184330"
  },
  {
    "start": "184330",
    "end": "187140"
  },
  {
    "text": "Here's the equation.",
    "start": "187140",
    "end": "189480"
  },
  {
    "text": "In this case, it's a cubic--",
    "start": "189480",
    "end": "191800"
  },
  {
    "text": "we used a cubic\npolynomial in this case.",
    "start": "191800",
    "end": "194140"
  },
  {
    "text": "So there's nine transform\nvariables altogether.",
    "start": "194140",
    "end": "199000"
  },
  {
    "text": "And so, it starts getting\na little unwieldy,",
    "start": "199000",
    "end": "201097"
  },
  {
    "text": "the dimension of the space\nis getting a little big here,",
    "start": "201097",
    "end": "203430"
  },
  {
    "text": "but it does-- it\nsolves the problem.",
    "start": "203430",
    "end": "205879"
  },
  {
    "start": "205880",
    "end": "209450"
  },
  {
    "start": "208000",
    "end": "256000"
  },
  {
    "text": "OK, well, so we seem to\nhave a way of getting around",
    "start": "209450",
    "end": "213379"
  },
  {
    "text": "these situations where we\ncan't separate the data.",
    "start": "213380",
    "end": "217130"
  },
  {
    "text": "Now, polynomials aren't the\ngreatest choice because--",
    "start": "217130",
    "end": "219950"
  },
  {
    "text": "especially in high dimensions,\nthey get wild rather fast.",
    "start": "219950",
    "end": "223020"
  },
  {
    "text": "We know, even in\nregression, we don't",
    "start": "223020",
    "end": "225110"
  },
  {
    "text": "like doing polynomial regression\nwith degree bigger than 3.",
    "start": "225110",
    "end": "228870"
  },
  {
    "text": "And if you've got\na large p to start",
    "start": "228870",
    "end": "232159"
  },
  {
    "text": "with and if you have a full--",
    "start": "232160",
    "end": "233900"
  },
  {
    "text": "even cubic polynomial space,\nit's quite a large space.",
    "start": "233900",
    "end": "237769"
  },
  {
    "text": "So it turns out there's a more\nelegant and controlled way",
    "start": "237770",
    "end": "240680"
  },
  {
    "text": "to introduce non-linearities\nin support-vector classifiers.",
    "start": "240680",
    "end": "244379"
  },
  {
    "text": "And this is through the use\nof what's known as kernels.",
    "start": "244380",
    "end": "247100"
  },
  {
    "text": "But before we get\ninto these, we need",
    "start": "247100",
    "end": "248960"
  },
  {
    "text": "to understand the\nrole of inner products",
    "start": "248960",
    "end": "252020"
  },
  {
    "text": "in support-vector classifiers.",
    "start": "252020",
    "end": "256019"
  },
  {
    "start": "256000",
    "end": "403000"
  },
  {
    "text": "So inner products, if\nyou've got a set of vectors,",
    "start": "256019",
    "end": "260980"
  },
  {
    "text": "so we've got xi and xi prime.",
    "start": "260980",
    "end": "263380"
  },
  {
    "text": "Now remember xi is a p-vector.",
    "start": "263380",
    "end": "265510"
  },
  {
    "text": "It's the set of variables\nfor observation i,",
    "start": "265510",
    "end": "268590"
  },
  {
    "text": "and xi prime is\nanother p vector.",
    "start": "268590",
    "end": "271150"
  },
  {
    "text": "And this angle notation\nhere means inner product",
    "start": "271150",
    "end": "274919"
  },
  {
    "text": "between these two vectors.",
    "start": "274920",
    "end": "276290"
  },
  {
    "text": "And this is just the\nsum of cross products",
    "start": "276290",
    "end": "278040"
  },
  {
    "text": "of each of the individual\ncomponents of the vector.",
    "start": "278040",
    "end": "280378"
  },
  {
    "text": "So that's called the inner\nproduct between two vectors.",
    "start": "280378",
    "end": "282669"
  },
  {
    "text": "And we write it in this\ncompact way over here.",
    "start": "282670",
    "end": "287820"
  },
  {
    "text": "So using that notation, we can\nwrite a linear support vector",
    "start": "287820",
    "end": "291660"
  },
  {
    "text": "classifier, you can write\nit in this form over here.",
    "start": "291660",
    "end": "295410"
  },
  {
    "text": "And so here, we have\nan inner product",
    "start": "295410",
    "end": "300240"
  },
  {
    "text": "between each of the xi's in\nthe data and the target point.",
    "start": "300240",
    "end": "306319"
  },
  {
    "text": "So we're thinking of this as\na function of a new point x.",
    "start": "306320",
    "end": "309520"
  },
  {
    "text": "And it turns out you can write\nthe support-vector machine",
    "start": "309520",
    "end": "313690"
  },
  {
    "text": "classifier solution\nin this form.",
    "start": "313690",
    "end": "316637"
  },
  {
    "text": "Now, you're not going\nto be able to see that",
    "start": "316637",
    "end": "318470"
  },
  {
    "text": "just from looking at the\nformula for the linear function.",
    "start": "318470",
    "end": "322010"
  },
  {
    "text": "This turns out to be how it\ncomes out in the solution.",
    "start": "322010",
    "end": "326000"
  },
  {
    "text": "But it's going to be important.",
    "start": "326000",
    "end": "330000"
  },
  {
    "text": "It also turns out to\nestimate the parameters.",
    "start": "330000",
    "end": "332650"
  },
  {
    "text": "So now we've got n\nparameters instead of the p.",
    "start": "332650",
    "end": "335400"
  },
  {
    "text": "And n could be bigger than p.",
    "start": "335400",
    "end": "337800"
  },
  {
    "text": "To estimate the\nparameters, n beta 0,",
    "start": "337800",
    "end": "341610"
  },
  {
    "text": "all we need are all the pairwise\ninner products between all",
    "start": "341610",
    "end": "345240"
  },
  {
    "text": "the endpoints in the data set.",
    "start": "345240",
    "end": "347780"
  },
  {
    "text": "So there's going to be an n\nby n inner product matrix.",
    "start": "347780",
    "end": "351980"
  },
  {
    "text": "And it turns out that if we have\nthat and that's all we have,",
    "start": "351980",
    "end": "356090"
  },
  {
    "text": "we can fit the support\nvector classifier.",
    "start": "356090",
    "end": "359340"
  },
  {
    "text": "And represented in\nthis form, and it's",
    "start": "359340",
    "end": "361020"
  },
  {
    "text": "going to give us the same\nsolution as what we had before.",
    "start": "361020",
    "end": "363395"
  },
  {
    "start": "363395",
    "end": "368580"
  },
  {
    "text": "Now, when you do\nthat, what happens",
    "start": "368580",
    "end": "371129"
  },
  {
    "text": "is that many of the\nalphas end up being 0.",
    "start": "371130",
    "end": "374880"
  },
  {
    "text": "So here, we put\nhats on the alphas.",
    "start": "374880",
    "end": "377250"
  },
  {
    "text": "And many of them are\n0 and so the only ones",
    "start": "377250",
    "end": "379920"
  },
  {
    "text": "we need to include here\nare the ones that aren't 0.",
    "start": "379920",
    "end": "382660"
  },
  {
    "text": "And so we call those the support\nset, or the support vectors.",
    "start": "382660",
    "end": "386910"
  },
  {
    "text": "And so those are the\nalphas that are not 0",
    "start": "386910",
    "end": "390220"
  },
  {
    "text": "and each alpha is tied to one of\nthe original points or vectors",
    "start": "390220",
    "end": "394170"
  },
  {
    "text": "in the original data set.",
    "start": "394170",
    "end": "396090"
  },
  {
    "text": "So we'll go back to slide 8\nand see which ones those are.",
    "start": "396090",
    "end": "399590"
  },
  {
    "start": "399590",
    "end": "402360"
  },
  {
    "text": "So let's get back\nhere to slide 8.",
    "start": "402360",
    "end": "404439"
  },
  {
    "start": "403000",
    "end": "550000"
  },
  {
    "text": "Here's slide 8.",
    "start": "404440",
    "end": "405930"
  },
  {
    "text": "So which are the support\nvectors or the points",
    "start": "405930",
    "end": "409470"
  },
  {
    "text": "with nonzero alphas?",
    "start": "409470",
    "end": "411990"
  },
  {
    "text": "Well, in this left\npicture over here,",
    "start": "411990",
    "end": "413639"
  },
  {
    "text": "there's the two circle points.",
    "start": "413640",
    "end": "415030"
  },
  {
    "text": "Those are support vectors and\nalso any points on the margin.",
    "start": "415030",
    "end": "419915"
  },
  {
    "text": "So it looks like this\none and this one,",
    "start": "419915",
    "end": "421540"
  },
  {
    "text": "and possibly, this one.",
    "start": "421540",
    "end": "424270"
  },
  {
    "text": "So in this case, it'd\nbe five support vectors.",
    "start": "424270",
    "end": "427599"
  },
  {
    "text": "So if all the points in the\npicture, only five of them",
    "start": "427600",
    "end": "430030"
  },
  {
    "text": "would have nonzero alphas.",
    "start": "430030",
    "end": "431830"
  },
  {
    "text": "And it sort of\nmakes sense, right?",
    "start": "431830",
    "end": "433780"
  },
  {
    "text": "Certainly the ones on\nthe margin make sense",
    "start": "433780",
    "end": "435940"
  },
  {
    "text": "because they define\nin the direction.",
    "start": "435940",
    "end": "438010"
  },
  {
    "text": "But the ones inside\nare also going",
    "start": "438010",
    "end": "439572"
  },
  {
    "text": "to define the direction\nbecause remember,",
    "start": "439572",
    "end": "441280"
  },
  {
    "text": "they have these little\nepsilons tagged onto them.",
    "start": "441280",
    "end": "445840"
  },
  {
    "text": "And so, they're going to play\nsome role in the definition",
    "start": "445840",
    "end": "448419"
  },
  {
    "text": "of the direction.",
    "start": "448420",
    "end": "450570"
  },
  {
    "text": "In this one over here,\nthere's 1, 2, 3, 4, 5, 6,",
    "start": "450570",
    "end": "456450"
  },
  {
    "text": "7 support points.",
    "start": "456450",
    "end": "458740"
  },
  {
    "text": "And just to add to that, if a\npoint is not a support point,",
    "start": "458740",
    "end": "461340"
  },
  {
    "text": "it means it's on the\nright side of its margin.",
    "start": "461340",
    "end": "463389"
  },
  {
    "text": "And if we moved it, but we\nkept it in the same area,",
    "start": "463390",
    "end": "466380"
  },
  {
    "text": "it's not going to\naffect the solution.",
    "start": "466380",
    "end": "467980"
  },
  {
    "text": "So they're not support points.",
    "start": "467980",
    "end": "470047"
  },
  {
    "text": "And one more thing to add,\nthis is the kind of sparsity,",
    "start": "470047",
    "end": "472380"
  },
  {
    "text": "but it's kind of a\nfunny sparsity, right?",
    "start": "472380",
    "end": "474088"
  },
  {
    "text": "Remember, we talked\nabout the Lasso--",
    "start": "474088",
    "end": "476310"
  },
  {
    "text": "it produced a sparse\nsolution in the coefficients",
    "start": "476310",
    "end": "479130"
  },
  {
    "text": "of the features.",
    "start": "479130",
    "end": "480032"
  },
  {
    "text": "This is a kind of sparsity,\nbut it's different.",
    "start": "480032",
    "end": "481990"
  },
  {
    "text": "It's actually in the\ndata space, right?",
    "start": "481990",
    "end": "483669"
  },
  {
    "text": "Because now we're assigning\na weight to each data point",
    "start": "483670",
    "end": "485820"
  },
  {
    "text": "and some of these weights are\n0, and the ones that are nonzero",
    "start": "485820",
    "end": "488403"
  },
  {
    "text": "are called the support points.",
    "start": "488403",
    "end": "489810"
  },
  {
    "text": "So sparsity but in the data\nspace, not the feature space.",
    "start": "489810",
    "end": "494160"
  },
  {
    "text": "That's a good point, Robin.",
    "start": "494160",
    "end": "495300"
  },
  {
    "text": "You know, and when you think\nof it like that, you say,",
    "start": "495300",
    "end": "497550"
  },
  {
    "text": "suppose you had 1,000 points\nand it ends up that there's 10",
    "start": "497550",
    "end": "501090"
  },
  {
    "text": "support points.",
    "start": "501090",
    "end": "502180"
  },
  {
    "text": "You think, oh great, you could\nhave thrown away the other 990",
    "start": "502180",
    "end": "505740"
  },
  {
    "text": "points.",
    "start": "505740",
    "end": "506880"
  },
  {
    "text": "Well, not really,\nbecause you had",
    "start": "506880",
    "end": "508713"
  },
  {
    "text": "to have them all there to decide\nwhich ones would be the support",
    "start": "508713",
    "end": "511380"
  },
  {
    "text": "points.",
    "start": "511380",
    "end": "511995"
  },
  {
    "text": "Once you found them,\nyou can throw them away,",
    "start": "511995",
    "end": "513870"
  },
  {
    "text": "but it doesn't really help\nmuch in the computations.",
    "start": "513870",
    "end": "516339"
  },
  {
    "text": "Right.",
    "start": "516340",
    "end": "517590"
  },
  {
    "text": "So it's a very interesting\ntopic, support-vector machines.",
    "start": "517590",
    "end": "520809"
  },
  {
    "text": "And it took us a while to\nunderstand the details.",
    "start": "520809",
    "end": "526190"
  },
  {
    "text": "OK, so that's inner products.",
    "start": "526190",
    "end": "528650"
  },
  {
    "text": "And what about kernels?",
    "start": "528650",
    "end": "531050"
  },
  {
    "text": "So from what we saw\npreviously, if we can compute",
    "start": "531050",
    "end": "535649"
  },
  {
    "text": "the inner products between\nall pairs of observations,",
    "start": "535650",
    "end": "539340"
  },
  {
    "text": "and if we can also compute\nthe inner products between all",
    "start": "539340",
    "end": "542280"
  },
  {
    "text": "the training observations\nand a new test point, then",
    "start": "542280",
    "end": "545550"
  },
  {
    "text": "we can both fit the\nsupport vector machine",
    "start": "545550",
    "end": "547680"
  },
  {
    "text": "and evaluate the function.",
    "start": "547680",
    "end": "549870"
  },
  {
    "text": "OK?",
    "start": "549870",
    "end": "551130"
  },
  {
    "start": "550000",
    "end": "719000"
  },
  {
    "text": "So this can be quite abstract.",
    "start": "551130",
    "end": "553390"
  },
  {
    "text": "First of all, an inner\nproduct is like a similarity.",
    "start": "553390",
    "end": "556810"
  },
  {
    "text": "And so, if you didn't\nreally have data",
    "start": "556810",
    "end": "559230"
  },
  {
    "text": "and you just had similarities,\npairwise similarities",
    "start": "559230",
    "end": "563010"
  },
  {
    "text": "between observations,\nin principle,",
    "start": "563010",
    "end": "564810"
  },
  {
    "text": "you could come up\nwith a classifier.",
    "start": "564810",
    "end": "566310"
  },
  {
    "text": "And there have been some\ninteresting examples of that.",
    "start": "566310",
    "end": "569620"
  },
  {
    "text": "So it allows us to be\na bit more abstract.",
    "start": "569620",
    "end": "573300"
  },
  {
    "text": "But more importantly, there's\nsome special kernel functions.",
    "start": "573300",
    "end": "577930"
  },
  {
    "text": "So these are-- a kernel function\nis a function of two arguments.",
    "start": "577930",
    "end": "581580"
  },
  {
    "text": "In this case, two\np vectors, right.",
    "start": "581580",
    "end": "584910"
  },
  {
    "text": "So it's called a\nbivariate function.",
    "start": "584910",
    "end": "587519"
  },
  {
    "text": "And these kernels compute\nthe inner products for us.",
    "start": "587520",
    "end": "594120"
  },
  {
    "text": "And you might not even know\nwhat the feature space is",
    "start": "594120",
    "end": "597300"
  },
  {
    "text": "in which they compute\nin the inner product,",
    "start": "597300",
    "end": "599100"
  },
  {
    "text": "but they can be thought\nof as doing that.",
    "start": "599100",
    "end": "601410"
  },
  {
    "text": "But a concrete example\nis with polynomials.",
    "start": "601410",
    "end": "604569"
  },
  {
    "text": "So here's a bivariate\nkernel function.",
    "start": "604570",
    "end": "606670"
  },
  {
    "text": "Look at it.",
    "start": "606670",
    "end": "608100"
  },
  {
    "text": "This part here is\njust the inner product",
    "start": "608100",
    "end": "610079"
  },
  {
    "text": "between the original\nvectors, xi and xi prime.",
    "start": "610080",
    "end": "614010"
  },
  {
    "text": "We add a 1 to it, and we\nraise it all to the power d.",
    "start": "614010",
    "end": "619110"
  },
  {
    "text": "And it turns out that\nthis kernel computes",
    "start": "619110",
    "end": "622350"
  },
  {
    "text": "the inner product in the\nfeature expansion space",
    "start": "622350",
    "end": "624990"
  },
  {
    "text": "that we get by expanding these\nvectors in-- these p components",
    "start": "624990",
    "end": "630270"
  },
  {
    "text": "into a basis for d\ndimensional polynomials.",
    "start": "630270",
    "end": "633900"
  },
  {
    "text": "Or sorry, d-- degree\nd polynomials.",
    "start": "633900",
    "end": "636840"
  },
  {
    "text": "Well that can be a huge space.",
    "start": "636840",
    "end": "638550"
  },
  {
    "text": "If p is large and D is large,\nthe number of basis functions",
    "start": "638550",
    "end": "643029"
  },
  {
    "text": "is actually p plus d, choose\nd, which grows very fast,",
    "start": "643030",
    "end": "649810"
  },
  {
    "text": "as p and d grow fast.",
    "start": "649810",
    "end": "651940"
  },
  {
    "text": "So that's a very big\ndimensional space potentially,",
    "start": "651940",
    "end": "655000"
  },
  {
    "text": "but we don't need\nto actually visit",
    "start": "655000",
    "end": "656980"
  },
  {
    "text": "that space because\nthis function will",
    "start": "656980",
    "end": "658990"
  },
  {
    "text": "compute those inner products.",
    "start": "658990",
    "end": "661170"
  },
  {
    "text": "That's sort of like magic.",
    "start": "661170",
    "end": "662589"
  },
  {
    "text": "You've got a kernel\nfunction that",
    "start": "662590",
    "end": "664020"
  },
  {
    "text": "computes this inner\nproduct in this very",
    "start": "664020",
    "end": "666090"
  },
  {
    "text": "high dimensional space.",
    "start": "666090",
    "end": "669040"
  },
  {
    "text": "So try a little\nExample with p is 2.",
    "start": "669040",
    "end": "674600"
  },
  {
    "text": "So like in our example, we had\ntwo components in x, and d is 2.",
    "start": "674600",
    "end": "678910"
  },
  {
    "text": "And just expand this\nfunction and see",
    "start": "678910",
    "end": "680628"
  },
  {
    "text": "what you get, and you'll\nsee that indeed it",
    "start": "680628",
    "end": "682420"
  },
  {
    "text": "does just what I said.",
    "start": "682420",
    "end": "685519"
  },
  {
    "text": "And having done that, we\ncan therefore compute the n",
    "start": "685520",
    "end": "690920"
  },
  {
    "text": "by n inner product matrix\nfor all the n observations",
    "start": "690920",
    "end": "695300"
  },
  {
    "text": "by evaluating this\nfunction at each pair.",
    "start": "695300",
    "end": "698120"
  },
  {
    "text": "And then we get a solution,\nwhich is of this form",
    "start": "698120",
    "end": "701779"
  },
  {
    "text": "because again, k is\ncomputing the inner product",
    "start": "701780",
    "end": "704420"
  },
  {
    "text": "between the target point x and\neach of the xi's in the sample.",
    "start": "704420",
    "end": "708779"
  },
  {
    "text": "And once again,\nthese alpha i's are",
    "start": "708780",
    "end": "710900"
  },
  {
    "text": "going to be nonzero for only\nthose points in the support set.",
    "start": "710900",
    "end": "714590"
  },
  {
    "text": "So that's where the kernel comes\ninto the support vector machine.",
    "start": "714590",
    "end": "718980"
  },
  {
    "text": "One of the most popular kernels\nis called the radial kernel.",
    "start": "718980",
    "end": "722519"
  },
  {
    "start": "719000",
    "end": "905000"
  },
  {
    "text": "And it's defined like this.",
    "start": "722520",
    "end": "724920"
  },
  {
    "text": "If you know what a multivariate\nGaussian distribution looks",
    "start": "724920",
    "end": "727800"
  },
  {
    "text": "like, this is the important\npart of that distribution.",
    "start": "727800",
    "end": "731430"
  },
  {
    "text": "It's a sum of squares\nbetween the components xi",
    "start": "731430",
    "end": "734790"
  },
  {
    "text": "and xi prime, each\nof the elements.",
    "start": "734790",
    "end": "737850"
  },
  {
    "text": "There's a gamma here, so\nthere's a [INAUDIBLE] that's",
    "start": "737850",
    "end": "740160"
  },
  {
    "text": "raised to the power e.",
    "start": "740160",
    "end": "743389"
  },
  {
    "text": "And this is an inner product\nin an abstract, infinite",
    "start": "743390",
    "end": "747960"
  },
  {
    "text": "dimensional feature space.",
    "start": "747960",
    "end": "750880"
  },
  {
    "text": "So this is a very high\ndimensional space.",
    "start": "750880",
    "end": "753350"
  },
  {
    "text": "So high we could never visit it.",
    "start": "753350",
    "end": "755089"
  },
  {
    "text": "Yet this kernel computes\nthe inner product for us.",
    "start": "755090",
    "end": "758760"
  },
  {
    "text": "And we can therefore fit\na support vector machine.",
    "start": "758760",
    "end": "761640"
  },
  {
    "text": "And in our example where we\nhad the two classes separating",
    "start": "761640",
    "end": "765990"
  },
  {
    "text": "the mauve class, the\ntwo blue classes,",
    "start": "765990",
    "end": "768149"
  },
  {
    "text": "this is the solution\nthat it gave.",
    "start": "768150",
    "end": "770170"
  },
  {
    "text": "And again, it seems to have\ndone a really good job.",
    "start": "770170",
    "end": "773160"
  },
  {
    "text": "OK?",
    "start": "773160",
    "end": "775056"
  },
  {
    "text": "So, some things got\na little weird here.",
    "start": "775056",
    "end": "777300"
  },
  {
    "text": "How can we fit models in an\ninfinite dimensional feature",
    "start": "777300",
    "end": "780029"
  },
  {
    "text": "space, and not be\noverfitting the data?",
    "start": "780030",
    "end": "783640"
  },
  {
    "text": "Well, it turns out\nthat even though it's",
    "start": "783640",
    "end": "785680"
  },
  {
    "text": "an infinite dimensional feature\nspace, many of the dimensions",
    "start": "785680",
    "end": "788800"
  },
  {
    "text": "are squashed down heavily.",
    "start": "788800",
    "end": "790480"
  },
  {
    "text": "In fact, almost\nsquashed down to 0.",
    "start": "790480",
    "end": "793329"
  },
  {
    "text": "So it's a very high\ndimensional feature space,",
    "start": "793330",
    "end": "795850"
  },
  {
    "text": "but most of the space\nis squashed down.",
    "start": "795850",
    "end": "798699"
  },
  {
    "text": "And the dimensions\nthat are squashed down",
    "start": "798700",
    "end": "802840"
  },
  {
    "text": "tend to be the more\nwiggly dimensions,",
    "start": "802840",
    "end": "804640"
  },
  {
    "text": "the smoother ones are\nsquashed down less.",
    "start": "804640",
    "end": "808270"
  },
  {
    "text": "You'll notice if you went\nthrough this little example",
    "start": "808270",
    "end": "811300"
  },
  {
    "text": "over here, you'll notice\nthat it did give you",
    "start": "811300",
    "end": "815170"
  },
  {
    "text": "the inner product between\na degree 2 polynomial,",
    "start": "815170",
    "end": "817873"
  },
  {
    "text": "but there were coefficients\nin front of these.",
    "start": "817873",
    "end": "819790"
  },
  {
    "text": "And those are what give\nyou the squashing factors.",
    "start": "819790",
    "end": "822889"
  },
  {
    "text": "So Trevor, in that example,\nin polynomial kernels,",
    "start": "822890",
    "end": "825130"
  },
  {
    "text": "I could take d to\nbe a million, right?",
    "start": "825130",
    "end": "827410"
  },
  {
    "text": "And then I have a huge number\nof polynomial functions.",
    "start": "827410",
    "end": "830019"
  },
  {
    "text": "Which if I did with the feature\nexpansion method, I would--",
    "start": "830020",
    "end": "832537"
  },
  {
    "text": "things would get out of control.",
    "start": "832537",
    "end": "833870"
  },
  {
    "text": "So, what happens here?",
    "start": "833870",
    "end": "835690"
  },
  {
    "text": "We run into trouble\nraising power of--",
    "start": "835690",
    "end": "837530"
  },
  {
    "text": "Right.",
    "start": "837530",
    "end": "838030"
  },
  {
    "text": "--to a million.",
    "start": "838030",
    "end": "838850"
  },
  {
    "text": "Yeah, with a polynomial kernel,\nI can get away with that.",
    "start": "838850",
    "end": "840740"
  },
  {
    "text": "You can get away with it.",
    "start": "840740",
    "end": "841730"
  },
  {
    "text": "Yeah.",
    "start": "841730",
    "end": "841899"
  },
  {
    "text": "And that's because\nof the squishing.",
    "start": "841900",
    "end": "843130"
  },
  {
    "text": "Because all the squishing down.",
    "start": "843130",
    "end": "844422"
  },
  {
    "text": "Yeah.",
    "start": "844422",
    "end": "846670"
  },
  {
    "text": "OK.",
    "start": "846670",
    "end": "847170"
  },
  {
    "text": "So a radial kernel\nis very popular",
    "start": "847170",
    "end": "850279"
  },
  {
    "text": "and it's one of the most\npopular kernels that's",
    "start": "850280",
    "end": "852500"
  },
  {
    "text": "used for nonlinear\nsupport vector machines.",
    "start": "852500",
    "end": "856710"
  },
  {
    "text": "And there are other kernels, but\nthis is the one people usually",
    "start": "856710",
    "end": "860600"
  },
  {
    "text": "go to.",
    "start": "860600",
    "end": "861480"
  },
  {
    "text": "And this gamma is\na tuning parameter.",
    "start": "861480",
    "end": "864120"
  },
  {
    "text": "And if you make--",
    "start": "864120",
    "end": "869120"
  },
  {
    "text": "you can think of it\nlike the standard--",
    "start": "869120",
    "end": "871460"
  },
  {
    "text": "it's like 1 over the standard\ndeviation of the Gaussian.",
    "start": "871460",
    "end": "875330"
  },
  {
    "text": "So if gamma is\nreally large, it's",
    "start": "875330",
    "end": "877730"
  },
  {
    "text": "like having a small\nstandard deviation",
    "start": "877730",
    "end": "879920"
  },
  {
    "text": "and you get much more\nwiggly decision boundaries.",
    "start": "879920",
    "end": "884610"
  },
  {
    "text": "Whereas if gamma is small,\nthe decision boundaries",
    "start": "884610",
    "end": "887510"
  },
  {
    "text": "get smoother.",
    "start": "887510",
    "end": "888355"
  },
  {
    "start": "888355",
    "end": "892540"
  },
  {
    "text": "So we're going to take this\nmachinery in the next segment",
    "start": "892540",
    "end": "897730"
  },
  {
    "text": "and look at an example.",
    "start": "897730",
    "end": "901500"
  },
  {
    "start": "901500",
    "end": "904000"
  }
]