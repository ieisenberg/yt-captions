[
  {
    "start": "0",
    "end": "5475"
  },
  {
    "text": "Hey, everybody. Welcome back. We're going to go ahead and\nget started with our refresh your understanding. ",
    "start": "5475",
    "end": "15069"
  },
  {
    "text": "[SIDE CONVERSATIONS] ",
    "start": "15069",
    "end": "29650"
  },
  {
    "text": "OK. Hopefully, everyone had a chance\nto think about this a little bit more. So let's go through the answers. The first one is true.",
    "start": "29650",
    "end": "35660"
  },
  {
    "text": " So if you are trying to\nevaluate the value of-- this",
    "start": "35660",
    "end": "42150"
  },
  {
    "text": "is in the tabular case. So this is where\nwe're assuming we're going to sample each\ntuple at random,",
    "start": "42150",
    "end": "47742"
  },
  {
    "text": "and then we do a\nQ-learning update. And we do this an\ninfinite amount of times. We know for a standard\ntabular learning,",
    "start": "47742",
    "end": "54809"
  },
  {
    "text": "we can converge to the\ntrue value of a policy",
    "start": "54810",
    "end": "59970"
  },
  {
    "text": "under-- as long as our\nlearning rate schedule is such. So if there's an existing\nlearning rate schedule under--",
    "start": "59970",
    "end": "65489"
  },
  {
    "text": "if you're decaying your learning\nrate at the right level, then you will converge to the\ntrue Q value in the tabular",
    "start": "65489",
    "end": "72120"
  },
  {
    "text": "case, because there's no\nfunction approximation that's happening there. In the second case,\nthis is also true.",
    "start": "72120",
    "end": "79009"
  },
  {
    "text": "So we talked a bit\nabout how we could think about doing these things\nin a batch way, where we do it",
    "start": "79010",
    "end": "84540"
  },
  {
    "text": "over, and over, and over again. We take our existing\ndata, and we run it through our either TD-learning\nupdate, or our [INAUDIBLE]",
    "start": "84540",
    "end": "91320"
  },
  {
    "text": "update, or other things. And we said that the\nTD-learning updates,",
    "start": "91320",
    "end": "96759"
  },
  {
    "text": "if you do it in a batch way,\nare equivalent to just taking a certainty equivalent model,\nwhich means you estimate",
    "start": "96760",
    "end": "103690"
  },
  {
    "text": "the dynamics model and you\nestimate the reward model-- excuse me-- from\nyour existing data,",
    "start": "103690",
    "end": "108799"
  },
  {
    "text": "and then you do\ndynamic programming. So that's what we saw-- I think we saw\nthat in Lecture 3.",
    "start": "108800",
    "end": "114430"
  },
  {
    "text": "This one is false. Does somebody want to\nsay why it's false? This one is not true.",
    "start": "114430",
    "end": "120090"
  },
  {
    "text": " There's a number of reasons\nwhy it could be false.",
    "start": "120090",
    "end": "125460"
  },
  {
    "text": "Anybody want to share why? Why is DQN not guaranteed\nto necessarily converge to the optimal Q function?",
    "start": "125460",
    "end": "131905"
  },
  {
    "start": "131905",
    "end": "137680"
  },
  {
    "text": "Yeah. Remind me your name. [MUTED] would you need to\nenforce a certain number",
    "start": "137680",
    "end": "143900"
  },
  {
    "text": "of iterations for it to have\nany chance of converging at all? So good point\n[? related to that. ?]",
    "start": "143900",
    "end": "149520"
  },
  {
    "text": "So certainly, if you don't do\nenough iterations, but even if you do an infinite\nnumber of iterations, it also might not be\nguaranteed to converge.",
    "start": "149520",
    "end": "155853"
  },
  {
    "text": "Can anybody tell me why\neven with [? infinite-- ?] [INAUDIBLE] you certainly\nneed a lot of iterations. But even if you had\na lot of iterations,",
    "start": "155854",
    "end": "162420"
  },
  {
    "text": "you still might not be\nguaranteed to converge. I think here it helps to\nthink about what we often",
    "start": "162420",
    "end": "167719"
  },
  {
    "text": "call realizability, which\nis we don't know what the functional form\nis of Q. And so you could think of the fact that--",
    "start": "167720",
    "end": "174350"
  },
  {
    "text": "I'm going to draw it\nin-- as if the state space was one dimensional. But in general, of\ncourse, the state space",
    "start": "174350",
    "end": "180650"
  },
  {
    "text": "is like this vector\nor it's images, and so it's really\nhigh dimensional. But imagine that it\nwas one dimensional.",
    "start": "180650",
    "end": "187800"
  },
  {
    "text": "Even here, you don't know\nwhat your V function or your Q function might look like. And so if you are using\nthe wrong approximator,",
    "start": "187800",
    "end": "195450"
  },
  {
    "text": "if you are using,\nsay, a line instead of a multi-degree polynomial,\nthen no matter how much data",
    "start": "195450",
    "end": "203058"
  },
  {
    "text": "you have, you're not going\nto converge to the optimal Q function. [INAUDIBLE] Because you\njust can't even realize it.",
    "start": "203058",
    "end": "212450"
  },
  {
    "text": "So in general-- and there's all\nsorts of additional instability things that mean we\ncan't be guaranteed",
    "start": "212450",
    "end": "218300"
  },
  {
    "text": "it's going to converge. So we're not guaranteed\nit'll converge. But empirically, it\noften does pretty well.",
    "start": "218300",
    "end": "223730"
  },
  {
    "text": "So we'll see--\n[INAUDIBLE] If you look at the empirical results,\nit often does really quite well.",
    "start": "223730",
    "end": "230599"
  },
  {
    "text": "Great. So far in the class, we've\ntalked a lot about these value function based methods,\nwhere we thought",
    "start": "230600",
    "end": "236870"
  },
  {
    "text": "about having an\nexplicit representation of the expected sum\nof discounted rewards",
    "start": "236870",
    "end": "242030"
  },
  {
    "text": "starting in a state or\nstarting in a particular state and action. And so we talked a lot about\nvalue functions and Q functions.",
    "start": "242030",
    "end": "249350"
  },
  {
    "text": "And now, we're going to talk\na lot about policy search. And so we're going\nto still think",
    "start": "249350",
    "end": "255110"
  },
  {
    "text": "about there being\nthis policy, which is a mapping of\nstates to actions or a mapping from\nstates and actions",
    "start": "255110",
    "end": "262580"
  },
  {
    "text": "to a number between 0 and\n1, such that it sums to 1,",
    "start": "262580",
    "end": "267830"
  },
  {
    "text": "because we always have to do at\nleast one action in every state. But we don't necessarily have to\nhave an explicit representation",
    "start": "267830",
    "end": "274040"
  },
  {
    "text": "of the value function anymore. So these have been very\npopular and important.",
    "start": "274040",
    "end": "281700"
  },
  {
    "text": "And if we think back to what\nour RL algorithms involve, they involve optimization,\ndelayed consequences,",
    "start": "281700",
    "end": "288150"
  },
  {
    "text": "exploration, and generalization. And we've seen examples of\nall of these ideas so far.",
    "start": "288150",
    "end": "293452"
  },
  {
    "text": "And we'll go a lot\nmore into some of them as we go through the course. But one thing you might\nbe wondering about",
    "start": "293452",
    "end": "298790"
  },
  {
    "text": "is, could we play\nthe trick that's often done in computer science\nand try to reduce reinforcement",
    "start": "298790",
    "end": "304909"
  },
  {
    "text": "learning to another problem? So could we do something like\njust like online optimization? So we know that we don't\nknow how the world works.",
    "start": "304910",
    "end": "313129"
  },
  {
    "text": "And we're trying to find\na good control policy. But could we do something\nsort of online optimization",
    "start": "313130",
    "end": "319130"
  },
  {
    "text": "where we're trying to\nsearch for a good policy? And in this way, you can\nthink of policy gradient",
    "start": "319130",
    "end": "324680"
  },
  {
    "text": "being related at a high\nlevel to this type of idea. It's not a\nreduction-based approach, but it's thinking about, well,\ncan we just directly search",
    "start": "324680",
    "end": "332330"
  },
  {
    "text": "to find a good policy. And policy gradient methods\nhave been extremely influential, particularly over the\nlast 5 to 10 years.",
    "start": "332330",
    "end": "339220"
  },
  {
    "text": "So they're used\nfor lots of areas. They're used for things\nlike sequence level training with recurrent neural networks.",
    "start": "339220",
    "end": "344310"
  },
  {
    "text": "That was based on\nREINFORCE, which is an algorithm we're going to\ngo through today, that has been",
    "start": "344310",
    "end": "349889"
  },
  {
    "text": "used for things of end to end\ntraining of deep visuomotor policies. So this was really influential\nwork in the robotics community",
    "start": "349890",
    "end": "357479"
  },
  {
    "text": "about a decade ago. And I'm just going to show\nyou a quick video of it. So this is work that was\ndone by Professor Chelsea",
    "start": "357480",
    "end": "364380"
  },
  {
    "text": "Finn as part of her PhD\nthesis, along with Sergey Levine and others at Berkeley.",
    "start": "364380",
    "end": "370110"
  },
  {
    "text": "And let's see if this\nwill work with audio. [VIDEO PLAYBACK] - [INAUDIBLE]",
    "start": "370110",
    "end": "377756"
  },
  {
    "start": "377756",
    "end": "387199"
  },
  {
    "text": "[END PLAYBACK] So what you can see there\nthat what they're doing is--",
    "start": "387199",
    "end": "393432"
  },
  {
    "text": "what they're going to be\ntrying to do is learn from-- so they showed you a really-- they showed a big network.",
    "start": "393432",
    "end": "398740"
  },
  {
    "text": "And what they're trying to\ndo is go directly from pixels to learn what the\nrobot should do.",
    "start": "398740",
    "end": "404220"
  },
  {
    "text": "And this is one of\nthe first examples of people trying to do\nthis directly from images. Let's just go back to some of\nthe tasks that they're using.",
    "start": "404220",
    "end": "411230"
  },
  {
    "start": "411230",
    "end": "438800"
  },
  {
    "text": "And so that was part\nof the motivation, too, is that they want to\nbe able to learn these tasks in a way that will generalize.",
    "start": "438800",
    "end": "444010"
  },
  {
    "text": "[INAUDIBLE] ",
    "start": "444010",
    "end": "458700"
  },
  {
    "text": "So this is another\nexample of trying to do direct policy\ngradient methods in order",
    "start": "458700",
    "end": "466260"
  },
  {
    "text": "to go from really large\ncomplex state spaces into direct decisions.",
    "start": "466260",
    "end": "472680"
  },
  {
    "text": "Now, in Homework 2, and we\nhaven't covered PPO yet, but you're going to be\nimplementing Proximal Policy",
    "start": "472680",
    "end": "478290"
  },
  {
    "text": "Optimization, which is\none of the methods that build on the methods that we're\ngoing to talk about today.",
    "start": "478290",
    "end": "483600"
  },
  {
    "text": "And that was used-- PPO was used as part\nof training ChatGPT. So as you can see, all\nof these algorithms",
    "start": "483600",
    "end": "488940"
  },
  {
    "text": "have become incredibly\ninfluential, in part, because\nthey can scale really well to extremely\ncomplex inputs,",
    "start": "488940",
    "end": "494500"
  },
  {
    "text": "whether it be images, or\nhigh-dimensional robotic tasks, or even things like\nnatural language. And so they're very powerful.",
    "start": "494500",
    "end": "501669"
  },
  {
    "text": "They're often used\nin conjunction with things like\nstate action values, as we'll talk about later.",
    "start": "501670",
    "end": "507190"
  },
  {
    "text": "But you don't have to\nuse them with them. So they're really useful sort of\nclass of things to know about.",
    "start": "507190",
    "end": "513640"
  },
  {
    "text": "So in particular, just\nlike how last time we saw that you could approximate\na state action value or a value",
    "start": "513640",
    "end": "520750"
  },
  {
    "text": "function with a\nset of parameters-- so we can do function\napproximation. In those cases, we thought\nof directly learning a value",
    "start": "520750",
    "end": "529060"
  },
  {
    "text": "function or a state\naction value function, and then it generate a policy\nfrom the state action value.",
    "start": "529060",
    "end": "534620"
  },
  {
    "text": "So something like\ne-greedy, where we either take what the Q-value suggests\nis the best action or we act",
    "start": "534620",
    "end": "539860"
  },
  {
    "text": "randomly. And what we're going\nto do today instead-- and I'll try to be careful\nabout not using the same--",
    "start": "539860",
    "end": "546130"
  },
  {
    "text": "we used w before to parameterize\nour state action values. And I'm going to try to\nbe careful about using theta just to make it clear.",
    "start": "546130",
    "end": "553000"
  },
  {
    "text": "We're going to directly\nparameterize the policy. And we're going to try to\nlearn parameterized policies. So we can think of these as\nlike deep convolutional neural",
    "start": "553000",
    "end": "560500"
  },
  {
    "text": "networks, which at the end\nwill output either an action or, if we have an\naction as input,",
    "start": "560500",
    "end": "566290"
  },
  {
    "text": "will output a probability. And the goal in this\ncase, as is normal, is that we want to find a\nway to act in the world that",
    "start": "566290",
    "end": "573130"
  },
  {
    "text": "will give us high reward. So we want to find a policy\nwith the highest value function of V pi.",
    "start": "573130",
    "end": "578510"
  },
  {
    "text": "And we're, again, not\ngoing to be focusing on model-based learning. So we're still going to try to\ndirectly learn from experience.",
    "start": "578510",
    "end": "584950"
  },
  {
    "text": "And we're not going to\nassume we have access or that we're explicitly\nbuilding a model. ",
    "start": "584950",
    "end": "591310"
  },
  {
    "text": "And I think one of the things\nthat's helpful to think about is there's these sort of\ndifferent views or lenses into reinforcement learning.",
    "start": "591310",
    "end": "597287"
  },
  {
    "text": "So this is a nice picture\nfrom David Silver, who's an amazing person\nin reinforcement learning. He was one of the\nmain leads on AlphaGo",
    "start": "597287",
    "end": "604030"
  },
  {
    "text": "and a number of other\nincredible papers. So you can think of it as\nyou have some methods, which are value-based.",
    "start": "604030",
    "end": "609860"
  },
  {
    "text": "We're explicitly building\na value function. We have other ones\nthat are policy-based. And the ones that are\nin the intersection",
    "start": "609860",
    "end": "615460"
  },
  {
    "text": "are often known as\nactor-critic methods. Who here has either implemented\nor heard of actor-critic methods",
    "start": "615460",
    "end": "621070"
  },
  {
    "text": "before? OK, so some people,\nbut not everyone. They're extremely popular. And in actor-critic\nmethods, you will often",
    "start": "621070",
    "end": "627940"
  },
  {
    "text": "combine between the benefits\nof value-based and policy. And so, for example, AlphaGo\nis an actor-critic method",
    "start": "627940",
    "end": "635110"
  },
  {
    "text": "in the sense that\nit is often having an explicit representation\nof the policy and of a value function.",
    "start": "635110",
    "end": "642520"
  },
  {
    "text": "So we'll get to actor-critic\nmethods later today. We're going to focus\non policy-based. ",
    "start": "642520",
    "end": "649420"
  },
  {
    "text": "So now that we're going\nto-- most of the time, we've thought about\npolicy so far, we thought about deterministic\npolicies or e-greedy policies.",
    "start": "649420",
    "end": "656287"
  },
  {
    "text": "And now, we're going to\nthink much more generally about stochastic policies. And that's going\nto be important,",
    "start": "656288",
    "end": "662650"
  },
  {
    "text": "because as we saw\nlast time, if you only have a deterministic policy,\nit's much harder to learn about actions you don't try.",
    "start": "662650",
    "end": "669170"
  },
  {
    "text": "Whereas, now we're\ngoing to think about having\nstochastic policies, where you're going to be\ngetting information about lots",
    "start": "669170",
    "end": "674602"
  },
  {
    "text": "of different actions. So let's think about\na particular example, also to illustrate\nsome of the things",
    "start": "674602",
    "end": "680920"
  },
  {
    "text": "that policy gradient methods\nare going to help us handle. So who here has played\nrock, paper, scissors?",
    "start": "680920",
    "end": "688110"
  },
  {
    "text": "Most people, I think. It's called\n\"roshambo\" in Chinese. It's a very popular game\nthroughout the world.",
    "start": "688110",
    "end": "693810"
  },
  {
    "text": "It's a stochastic game\nwhere each side can pick a particular\nstrategy and the state--",
    "start": "693810",
    "end": "703287"
  },
  {
    "text": "you can think of\nthere being a state, you could keep track of\nwhat your opponent has done over time. So think for a second about\nwhether a deterministic policy",
    "start": "703288",
    "end": "711899"
  },
  {
    "text": "can be optimal if you're\nplaying this game repeatedly.",
    "start": "711900",
    "end": "716950"
  },
  {
    "text": "So raise your hand if\na deterministic policy can be optimal. ",
    "start": "716950",
    "end": "722820"
  },
  {
    "text": "Raise your hand if you think a\nstochastic policy is optimal. OK, someone who said\nstochastic explain why.",
    "start": "722820",
    "end": "729030"
  },
  {
    "text": " Yes. [INAUDIBLE] is like circular.",
    "start": "729030",
    "end": "736530"
  },
  {
    "text": "There's no [? 1 ?] [? plus ?]\n[? 1 ?] [INAUDIBLE] That's right. Yeah, so there's no best--",
    "start": "736530",
    "end": "742420"
  },
  {
    "text": "there's nothing that\nstrictly dominates all the other strategies. And also, if you're\ndeterministic, what can your opponent do?",
    "start": "742420",
    "end": "749399"
  },
  {
    "text": "Like, if I say I'm always\ngoing to pick paper, what does my opponent do?",
    "start": "749400",
    "end": "755390"
  },
  {
    "text": "Yeah, they're always going\nto pick the other one, like rock [? to ?] [? beat. ?] So anything you do\nthat's deterministic",
    "start": "755390",
    "end": "762170"
  },
  {
    "text": "can be exploited\nby your opponent if you are playing repeatedly. And so the optimal thing to\ndo here is to be stochastic.",
    "start": "762170",
    "end": "769399"
  },
  {
    "text": "So the optimal policy has\nto be stochastic here. Otherwise, all\ndeterministic policies are strictly dominated by\ngood stochastic policy.",
    "start": "769400",
    "end": "778590"
  },
  {
    "text": "And now, you might\nthink, all right, well, that sounds different than\nwhat we've seen so far. But one of the challenges here\nis the system is not Markov.",
    "start": "778590",
    "end": "786820"
  },
  {
    "text": "So it's not stochastic what\nyour adversary will play next. They're not random-- or\nit might be if they're",
    "start": "786820",
    "end": "792430"
  },
  {
    "text": "playing a stochastic policy. [CLEARS THROAT] Excuse me. But in general,\n[? they ?] can react to what you've seen so far.",
    "start": "792430",
    "end": "797529"
  },
  {
    "text": "And it's not just like a random\nenvironment, like a coin flip on the next time.",
    "start": "797530",
    "end": "802740"
  },
  {
    "text": "And in this case, actually,\na uniform random policy is optimal. It's a Nash equilibrium. ",
    "start": "802740",
    "end": "810050"
  },
  {
    "text": "So that's one case where\nhaving a stochastic policy would be really helpful. So you could just have a\nfixed stochastic policy,",
    "start": "810050",
    "end": "816442"
  },
  {
    "text": "and it would be optimal,\nbut you couldn't necessarily write this down\neasily as a Q-function and just take the argmax.",
    "start": "816442",
    "end": "822310"
  },
  {
    "text": "There is not a deterministic\npolicy for this environment that is optimal. And so it's less clear how you\nwould write that down directly",
    "start": "822310",
    "end": "829959"
  },
  {
    "text": "in terms of a Q-function, in\npart, because the system is not Markov. So here's another example\nwhere we might want",
    "start": "829960",
    "end": "836480"
  },
  {
    "text": "to have stochastic policies. And it's where we have aliasing\nor partial observability. So imagine this\ncase where you have",
    "start": "836480",
    "end": "842870"
  },
  {
    "text": "a robot that's walking along. And maybe they have sensors\nso they can tell how far they are from the walls.",
    "start": "842870",
    "end": "848130"
  },
  {
    "text": "But under those sensors, these\ntwo gray boxes look identical.",
    "start": "848130",
    "end": "853740"
  },
  {
    "text": "Because like from the\nagent's point of view, if they have only immediate\nsensors, both of those places",
    "start": "853740",
    "end": "858750"
  },
  {
    "text": "will look identical. And so they can't distinguish\nthose gray states. And imagine that you just--\nbecause you have a feature",
    "start": "858750",
    "end": "866370"
  },
  {
    "text": "representation that just tells\nyou about what you're-- whether you have a wall to the north,\nto the east, to the south,",
    "start": "866370",
    "end": "871410"
  },
  {
    "text": "or to the west, so those two\ngray states would look identical if that was your\nfeature representation.",
    "start": "871410",
    "end": "876510"
  },
  {
    "text": "So you could have a value-based\nreinforcement learning representation where you use\nan approximate value function,",
    "start": "876510",
    "end": "882850"
  },
  {
    "text": "where you take in this as\nthe state representation, or you could have a policy-based\none that takes in those.",
    "start": "882850",
    "end": "891080"
  },
  {
    "text": "So the challenge here is\nthat if you're value-based, you have to do the same thing\nin those two gray states,",
    "start": "891080",
    "end": "898459"
  },
  {
    "text": "because you can't\ndistinguish them. So from your perspective, it's\nlike you're in the same place no matter which of those\ntwo places you're in.",
    "start": "898460",
    "end": "905390"
  },
  {
    "text": "So if you're going to do\na value function based, and then extract a\ndeterministic policy, you would either\nalways have to go",
    "start": "905390",
    "end": "912680"
  },
  {
    "text": "say to the left in those cases\nor always go to the right. ",
    "start": "912680",
    "end": "917750"
  },
  {
    "text": "And neither of those\nwould always be good. ",
    "start": "917750",
    "end": "924770"
  },
  {
    "text": "So under aliasing,\nmeaning that we don't know whether which of\nthe two gray states we're in when we're\nin one of them,",
    "start": "924770",
    "end": "930710"
  },
  {
    "text": "an optimal deterministic\npolicy will always move west in both states\nor east in both states. And either way, it might\nget stuck and never",
    "start": "930710",
    "end": "937160"
  },
  {
    "text": "be able to reach the money. And that's what's\ngoing to happen if we do a value-based\nreinforcement learning approach.",
    "start": "937160",
    "end": "944650"
  },
  {
    "text": "So that's not great. You're going to traverse\nthis for a long time. You're not going to be\ngetting high reward. What could you do if you wanted\nto have a stochastic policy?",
    "start": "944650",
    "end": "952905"
  },
  {
    "text": " So that allows you to act\nrandomly or stochastically",
    "start": "952905",
    "end": "961410"
  },
  {
    "text": "in any state. What do you think would\nbe the right thing to do in the gray states if you\ncould have a stochastic policy? ",
    "start": "961410",
    "end": "969240"
  },
  {
    "text": "With just some probability,\nyou go either east or west. Yeah, exactly.",
    "start": "969240",
    "end": "975630"
  },
  {
    "text": "So you could just randomize it. So an optimal stochastic policy\nwill randomly move east or west",
    "start": "975630",
    "end": "981037"
  },
  {
    "text": "in the gray state, because it\ndoesn't know which one it's in. And half the time, that'll\nbe the right thing to do.",
    "start": "981038",
    "end": "987040"
  },
  {
    "text": "So now that means much more of\nthe time, it'll go into here. And it generally will reach\nthe goal state pretty quickly.",
    "start": "987040",
    "end": "994860"
  },
  {
    "text": "So this is another case where\nthe system is not Markov. This is not [INAUDIBLE]\nthe state features.",
    "start": "994860",
    "end": "1005640"
  },
  {
    "text": "So because we have\naliasing, meaning the system is\npartially observable, it is not a Markov system.",
    "start": "1005640",
    "end": "1010930"
  },
  {
    "text": "One way to handle\nthat is to treat it as a partially observable\nMarkov decision process. [INAUDIBLE] talks a lot\nabout those in his classes.",
    "start": "1010930",
    "end": "1018690"
  },
  {
    "text": "But an alternative is to\nuse a stochastic policy. And you can also\n[? act ?] very well here.",
    "start": "1018690",
    "end": "1025550"
  },
  {
    "text": "So those are two examples\nof the type of thing that might be able to be easy to\nhandle with policy gradient",
    "start": "1025550",
    "end": "1032209"
  },
  {
    "text": "methods or stochastic\npolicies that might be hard to tackle\nwith the type of methods we've seen so far.",
    "start": "1032210",
    "end": "1038180"
  },
  {
    "text": "So now, we have to think\nabout if we have policies, and, in general, we're going\nto want them to be stochastic, how are we going to learn\nwhat are good policies?",
    "start": "1038180",
    "end": "1045500"
  },
  {
    "text": "Like, we have this-- now,\nwe have a function space over policies. And we want to learn which\nof them have good values.",
    "start": "1045500",
    "end": "1052639"
  },
  {
    "text": "So if we're in an\nepisodic environment, we can use the policy\nvalue at the start state.",
    "start": "1052640",
    "end": "1058470"
  },
  {
    "text": "So we can just say I'm going\nto similar to the Monte Carlo methods, if I start\nin this state, I run this policy, what\nwould be my expected reward",
    "start": "1058470",
    "end": "1065360"
  },
  {
    "text": "be until the end of the episode? We're going to mostly focus\non the episodic case today,",
    "start": "1065360",
    "end": "1070520"
  },
  {
    "text": "but you can extend these to more\nof an infinite horizon case.",
    "start": "1070520",
    "end": "1075920"
  },
  {
    "text": "All right, so once we\nthink of it in this way, we can really think, OK, this\nsounds like an optimization problem.",
    "start": "1075920",
    "end": "1081380"
  },
  {
    "text": "So we really just want to\nfind the parameters that maximize the value.",
    "start": "1081380",
    "end": "1087000"
  },
  {
    "text": "So you could say-- here, you can think of\nthis as being your thetas. So I'm just drawing\nit in one dimensional.",
    "start": "1087000",
    "end": "1093220"
  },
  {
    "text": "But in general, this could\nbe all the parameters in a deep neural network. And then, this is V of theta\nof a particular starting state.",
    "start": "1093220",
    "end": "1102330"
  },
  {
    "text": "It might look like this. And what your goal\nwould be is to find",
    "start": "1102330",
    "end": "1108660"
  },
  {
    "text": "the parameters of\nyour policy that maximize the value function.",
    "start": "1108660",
    "end": "1114200"
  },
  {
    "text": "And so this is an\noptimization problem, but it's a hard\noptimization problem, because we don't\nhave that function.",
    "start": "1114200",
    "end": "1120270"
  },
  {
    "text": "You can only estimate\nit through data. And so you can imagine,\nlike you start off, you have no idea\nhow theta maps to V.",
    "start": "1120270",
    "end": "1127650"
  },
  {
    "text": "And then you have to\nlearn that over time. ",
    "start": "1127650",
    "end": "1134060"
  },
  {
    "text": "So once we think of it as\nan optimization problem, where we don't know\nwhat the function is, there are a lot of\ndifferent methods",
    "start": "1134060",
    "end": "1140030"
  },
  {
    "text": "we could think about to\ntry to solve this problem. And what we're going\nto focus on today, mostly, is ones that are\ngoing to exploit something",
    "start": "1140030",
    "end": "1146539"
  },
  {
    "text": "about the structure of\nsequential decision processes. But there are methods that\ncompletely ignore all of this.",
    "start": "1146540",
    "end": "1154970"
  },
  {
    "text": "And in particular, you\ncan even use things that completely ignore gradients. So you can do things like hill\nclimbing, or genetic algorithms,",
    "start": "1154970",
    "end": "1161450"
  },
  {
    "text": "or cross entropy methods,\nwhere you may not think of any of the type of\nstructure of the parameter",
    "start": "1161450",
    "end": "1167389"
  },
  {
    "text": "space. And in some cases, that\ncan work really well.",
    "start": "1167390",
    "end": "1172710"
  },
  {
    "text": "So there's a really nice example\nby my colleague, Steven Collins, who's over in the mechanical\nengineering department.",
    "start": "1172710",
    "end": "1179370"
  },
  {
    "text": "He does some really\ninteresting work on-- oh, yeah. [INAUDIBLE] ",
    "start": "1179370",
    "end": "1191470"
  },
  {
    "text": "Yes, but you can make it a\ndistribution over actions. So you can output\nsomething between 0 and 1.",
    "start": "1191470",
    "end": "1198010"
  },
  {
    "text": "And you can have\nan input action. And so [INAUDIBLE] [? have ?]\na stochastic policy. [INAUDIBLE]",
    "start": "1198010",
    "end": "1203500"
  },
  {
    "text": " You would then compute it\nfor all of the actions, then you would have\nto have some-- yeah, you'd have to pick\na random number,",
    "start": "1203500",
    "end": "1208868"
  },
  {
    "text": "and then use that to select. Yeah. Yeah, good question. ",
    "start": "1208868",
    "end": "1215240"
  },
  {
    "text": "So my colleague Steven Collins\nover at mechanical engineering does a lot of work\non exoskeletons. And there are lots of\nreasons exoskeletons",
    "start": "1215240",
    "end": "1221750"
  },
  {
    "text": "could be really helpful,\nparticularly for people that have motor impairments. But one of the\nchallenges of them is that you have to have them\nactually help people walk.",
    "start": "1221750",
    "end": "1230310"
  },
  {
    "text": "So if you clamp something\non to, say, your leg, then the way my physical\nconfiguration is",
    "start": "1230310",
    "end": "1236405"
  },
  {
    "text": "may not be the same as your\nphysical configuration. I'm pretty tall. And so you'd really\nlike to make sure that this helps each individual\nin the best way possible.",
    "start": "1236405",
    "end": "1243840"
  },
  {
    "text": "But you don't want it to have\nto learn over the course of two years how to best\noptimize to someone,",
    "start": "1243840",
    "end": "1249710"
  },
  {
    "text": "because they're not going\nto wait that long to use it. So what they did is\nthey use policy methods,",
    "start": "1249710",
    "end": "1254940"
  },
  {
    "text": "policy search methods to quickly\npersonalize the parameters of an exoskeleton.",
    "start": "1254940",
    "end": "1260120"
  },
  {
    "text": "And they called this\nhuman-in-the-loop exoskeleton optimization. So the idea in this case is what\nthey're trying to figure out",
    "start": "1260120",
    "end": "1266360"
  },
  {
    "text": "is what is the parameters\nof their exoskeleton. And what they're looking\nat is, essentially, how much it helps you walk.",
    "start": "1266360",
    "end": "1272549"
  },
  {
    "text": "So how much it reduces\nthe effort needed to walk. And so what they\ncould do in this case, they're not using a\ngradient-based method.",
    "start": "1272550",
    "end": "1279740"
  },
  {
    "text": "They're using just CMA-ES, which\nis-- [? so the treatment is ?] continuous\noptimization, is they'd",
    "start": "1279740",
    "end": "1284780"
  },
  {
    "text": "have people walk under a few\ndifferent control parameters. They would see which of those\nseem to be most effective,",
    "start": "1284780",
    "end": "1291690"
  },
  {
    "text": "and then they would\nmove the policies they try in that direction\nwith some stochasticity.",
    "start": "1291690",
    "end": "1296780"
  },
  {
    "text": "And I think it was within maybe\ntwo or three hours using this, they could find substantially\nbetter policies.",
    "start": "1296780",
    "end": "1302760"
  },
  {
    "text": "I think it increased metabolic\nefficiency like maybe by 20% or 30%. It was pretty remarkable.",
    "start": "1302760",
    "end": "1308390"
  },
  {
    "text": "And so this was\npublished in Science about seven or eight years ago. But that's another\nexample of a place where",
    "start": "1308390",
    "end": "1313580"
  },
  {
    "text": "you can do this sort\nof online optimization, but you don't\nnecessarily have to think about the temporal\nstructure of the policy.",
    "start": "1313580",
    "end": "1320438"
  },
  {
    "text": "Can I ask a question? Yeah. [INAUDIBLE] [? --with ?]\na default policy,",
    "start": "1320438",
    "end": "1326809"
  },
  {
    "text": "and then try to improve\nthat default policies? Great question. Yeah, so in all of\nthese cases, we're going to have to assume that\nwe initialize our policy",
    "start": "1326810",
    "end": "1333518"
  },
  {
    "text": "parameterization in some way. Just like how we initialized\nour value function to 0 to start, now we're going\nto-- or if you had it",
    "start": "1333518",
    "end": "1340283"
  },
  {
    "text": "for the deep Q-network, it would\nbe whatever your neural network parameters were. Yeah, great question.",
    "start": "1340283",
    "end": "1345505"
  },
  {
    "text": " Now, it's just useful\nto know about these,",
    "start": "1345505",
    "end": "1351530"
  },
  {
    "text": "because they often\nwork pretty well. So I think sometimes\nwe like to leverage",
    "start": "1351530",
    "end": "1357290"
  },
  {
    "text": "the structure specific to, say,\nour Markov decision process. But in some cases, just\nleveraging these ones,",
    "start": "1357290",
    "end": "1362330"
  },
  {
    "text": "which may not use very\nmuch structure at all, can actually do really well. So it's just good\nto keep in mind",
    "start": "1362330",
    "end": "1368240"
  },
  {
    "text": "that there are a lot of ways\nto do online optimization. All right, so this is often\na great baseline to try.",
    "start": "1368240",
    "end": "1374600"
  },
  {
    "text": "The great thing about this is\nit can work with any policy parameterizations, even if it's\nnot differentiable, because it's",
    "start": "1374600",
    "end": "1380590"
  },
  {
    "text": "not using gradients. So it doesn't need\nto be differentiable. And it's also often very\neasy to parallelize.",
    "start": "1380590",
    "end": "1386510"
  },
  {
    "text": "So CMA-ES, for those of you\nwho haven't seen it before, you'll have a number\nof different policies",
    "start": "1386510",
    "end": "1392917"
  },
  {
    "text": "you kind of try in\nparallel, and then you'll use that to update and shift\nto another set of policies.",
    "start": "1392917",
    "end": "1398480"
  },
  {
    "text": "And that's what they did,\nProfessor Collins did. And in a lot of cases, the\nproblems that we think about",
    "start": "1398480",
    "end": "1403899"
  },
  {
    "text": "are places where you'll\nhave many customers. You'll have many robot arms, and\nso you can parallelize things.",
    "start": "1403900",
    "end": "1409720"
  },
  {
    "text": "One of the limitations is that\nit's often less data efficient, because it's ignoring\nthe temporal structure. So if you have temporal\nstructure or if you have",
    "start": "1409720",
    "end": "1416890"
  },
  {
    "text": "a [? gradient ?] information,\nit may be more effective to use [? that. ?]",
    "start": "1416890",
    "end": "1423059"
  },
  {
    "text": "So what we're going to\nfocus on in this class is differentiable methods.",
    "start": "1423060",
    "end": "1428170"
  },
  {
    "text": "So we're going to\nfocus on places where we can do stochastic\ngradient descent, including on the\npolicy parametrization.",
    "start": "1428170",
    "end": "1435330"
  },
  {
    "text": "So if we have our\npolicy parameterized by a deep neural network, we\ncan propagate through that",
    "start": "1435330",
    "end": "1440580"
  },
  {
    "text": "and update those parameters. So we're going to focus\nhere mostly on methods",
    "start": "1440580",
    "end": "1448530"
  },
  {
    "text": "that do use gradient descent\nand that often leverage the sequential\nstructure that we're making a sequence of decisions\nand we want to optimize to make",
    "start": "1448530",
    "end": "1456120"
  },
  {
    "text": "those sequence of decisions. So to do that, we're going to\nexplicitly define the gradient.",
    "start": "1456120",
    "end": "1463149"
  },
  {
    "text": "And we're going to write down\nthe value function in terms-- as a function of the\npolicy parameters,",
    "start": "1463150",
    "end": "1468159"
  },
  {
    "text": "so that we can be clear that\nthis value function relies",
    "start": "1468160",
    "end": "1474090"
  },
  {
    "text": "on those policies. And we're going to focus today\non episodic Markov decision processes, where we go\nfor a single episode,",
    "start": "1474090",
    "end": "1481210"
  },
  {
    "text": "stop, reset, and keep going. ",
    "start": "1481210",
    "end": "1486450"
  },
  {
    "text": "So now, what we're\ngoing to do is we're only going to\nbe trying to get, in general, to a local maximum.",
    "start": "1486450",
    "end": "1492660"
  },
  {
    "text": "Now, it's possible you're\nlucky and you're sort of convex in the space of the value--",
    "start": "1492660",
    "end": "1499080"
  },
  {
    "text": "in space of the\npolicy parameters. But in general, we're not\ngoing to assume convexity. So at best, we're going\nto hope to just get",
    "start": "1499080",
    "end": "1505260"
  },
  {
    "text": "to some sort of local\nmaxima in our space. So if we have-- again, if we only\nhad one parameter,",
    "start": "1505260",
    "end": "1512440"
  },
  {
    "text": "and we have something like\nthis, we might get to here. We might get to here.",
    "start": "1512440",
    "end": "1517940"
  },
  {
    "text": "In general, we're not going\nto make global optimality guarantees. This is in big contrast to the\ntabular cases we saw before.",
    "start": "1517940",
    "end": "1525620"
  },
  {
    "text": "We were guaranteed to get to\nthe optimal Q-function, optimal value function. Now, we're just\ngoing to hope to--",
    "start": "1525620",
    "end": "1531760"
  },
  {
    "text": "given our policy\nparameterization, let's try to get to what's a\nlocal optima in that policy",
    "start": "1531760",
    "end": "1538870"
  },
  {
    "text": "parameterization. So it's sort of a policy\nspecific-- policy class specific guarantee.",
    "start": "1538870",
    "end": "1545019"
  },
  {
    "text": "And it's only a local optima. And what we'll be\ndoing is we're just going to be trying to take\nthe gradient of the policy",
    "start": "1545020",
    "end": "1550740"
  },
  {
    "text": "with respect to the parameters. And as usual, we're going to\nhave a step size parameter.",
    "start": "1550740",
    "end": "1555960"
  },
  {
    "text": "So we're going to take the\ngradient of the value function with respect to the parameters\nand take a small step.",
    "start": "1555960",
    "end": "1561049"
  },
  {
    "text": "And the key thing is\ngoing to be thinking about places where we can\ndo this all directly using",
    "start": "1561050",
    "end": "1566630"
  },
  {
    "text": "smooth functions. ",
    "start": "1566630",
    "end": "1572256"
  },
  {
    "text": "Now, one way you could\ndo this-- of course, when you see this\nnow, you probably immediately think\nof autodiff methods and think about we can just\nback propagate, et cetera.",
    "start": "1572256",
    "end": "1579590"
  },
  {
    "text": "But it's worth noting that\nwhen these methods began to start to get popular,\nthey didn't necessarily",
    "start": "1579590",
    "end": "1585280"
  },
  {
    "text": "have autodiff yet. I know. There was research then still.",
    "start": "1585280",
    "end": "1590350"
  },
  {
    "text": "And one of the\nthings people started thinking about for this is how\nyou could use this for robotics. So this is a nice paper\nfrom 2004, so 20 years ago,",
    "start": "1590350",
    "end": "1598419"
  },
  {
    "text": "by Peter Stone's group. It was right around then-- I think maybe RoboCop was maybe\n60 years old then or something.",
    "start": "1598420",
    "end": "1604720"
  },
  {
    "text": "I think they started\nit back in 1998 or so. So there are these\nlittle quadruped robots. And the goal was to think\nabout getting robotics",
    "start": "1604720",
    "end": "1611680"
  },
  {
    "text": "to the stage where you could\nhave robots play human players. I think that was the goal\nby either 2030 or 2050.",
    "start": "1611680",
    "end": "1618470"
  },
  {
    "text": "I forget. But what they were just going to\nstart with-- it was quadrupeds. So one of the big\nchallenges at the beginning,",
    "start": "1618470",
    "end": "1623620"
  },
  {
    "text": "because everywhere, you start\nwith the beginning challenges, and you go from\nthere, it was just getting them to\nwalk fast enough.",
    "start": "1623620",
    "end": "1629210"
  },
  {
    "text": "So if they're going\nto score goals, and they're going to compete,\nyou need them to walk quickly. And so there's this\nquestion of just how",
    "start": "1629210",
    "end": "1635360"
  },
  {
    "text": "do you learn fast walks, so\nthat they can-- sort of trying to teach robots to run.",
    "start": "1635360",
    "end": "1641030"
  },
  {
    "text": "And what they found here is that\nthey could use policy methods and policy search methods just\nto learn a faster way for it",
    "start": "1641030",
    "end": "1647480"
  },
  {
    "text": "to walk. And so they\nparameterize the curve of how the foot moves\nas a set of parameters.",
    "start": "1647480",
    "end": "1654050"
  },
  {
    "text": "And that defined the policy\nfor moving those joints. And then, what they\ndid is they just had these walk back and\nforth many, many times.",
    "start": "1654050",
    "end": "1661238"
  },
  {
    "text": "And what they would\ndo is they'd have them walk with some\nparticular policy parameters.",
    "start": "1661238",
    "end": "1667250"
  },
  {
    "text": "They would see how\nfast they walked. They would do finite\ndifferent methods. So they weren't trying\nto explicitly do",
    "start": "1667250",
    "end": "1672560"
  },
  {
    "text": "autodiff or anything\nthere, and then they would slightly change the\npolicy parameters and repeat.",
    "start": "1672560",
    "end": "1679179"
  },
  {
    "text": "And they learned to\nsubstantially faster walk during that time. And I think it took maybe\naround four hours or so,",
    "start": "1679180",
    "end": "1686750"
  },
  {
    "text": "but they just had to replace\nthe batteries a couple of times. So just an example to say like,\nit's lovely to have autodiff.",
    "start": "1686750",
    "end": "1693010"
  },
  {
    "text": "You can do really\ncomplicated things now. But these methods can work\neven in really basic settings, particularly where you think you\nhave pretty bad models of how",
    "start": "1693010",
    "end": "1700680"
  },
  {
    "text": "the world works. And so now, you can just\nbe directly data driven. And why is this hard\nproblem, for those of you",
    "start": "1700680",
    "end": "1706180"
  },
  {
    "text": "who haven't done robotics? It involves a whole\nbunch of contact forces.",
    "start": "1706180",
    "end": "1711510"
  },
  {
    "text": "The ground may be-- well, they have to learn\non this particular ground. You may not know, because\nit's commercial hardware.",
    "start": "1711510",
    "end": "1718570"
  },
  {
    "text": "You may not know exactly\nall the parameters that the designers put in. So you can just be data driven.",
    "start": "1718570",
    "end": "1724270"
  },
  {
    "text": "OK, as opposed to maybe\nhaving a physics simulator. All right, so just\nto summarize so far,",
    "start": "1724270",
    "end": "1730820"
  },
  {
    "text": "the benefits of\npolicy-based RL is that we're going to have\noften better convergence",
    "start": "1730820",
    "end": "1736548"
  },
  {
    "text": "properties, because we're often\ngoing to be able to guarantee that we get to a local optima. Whereas, we didn't have\nthat for deep Q-learning.",
    "start": "1736548",
    "end": "1743242"
  },
  {
    "text": "They're often really effective\nin high dimensional or continuous action spaces. And you can learn\nstochastic policies.",
    "start": "1743242",
    "end": "1750070"
  },
  {
    "text": "But the methods\nwe've seen so far might be more inefficient\nand higher variance. And we often only get to\nsomething in the local optima.",
    "start": "1750070",
    "end": "1758470"
  },
  {
    "text": "And we'll see some things to\nhelp with of the inefficiency in a second. ",
    "start": "1758470",
    "end": "1764270"
  },
  {
    "text": "All right, so now, what\nwe're going to dive into is how do we do this when\nwe are willing to have differentiable policies.",
    "start": "1764270",
    "end": "1769350"
  },
  {
    "start": "1769350",
    "end": "1774400"
  },
  {
    "text": "So the hope is that we can\nactually compute the policy gradient analytically,\nso we don't have to do it",
    "start": "1774400",
    "end": "1780820"
  },
  {
    "text": "with finite differences. And we're going to\nfocus on policies where it's differentiable\nas long as it's non-0.",
    "start": "1780820",
    "end": "1788178"
  },
  {
    "text": "So we're going to assume\nthat we can always compute the gradient of the\npolicy parameters themselves.",
    "start": "1788178",
    "end": "1793518"
  },
  {
    "text": "And there are a number\nof different classes we can do this for. And there are many popular\nclasses, including, of course,",
    "start": "1793518",
    "end": "1799760"
  },
  {
    "text": "deep neural networks. So popular ones\nare often softmax.",
    "start": "1799760",
    "end": "1806600"
  },
  {
    "text": "Softmax is used all the time. I'll explain what\nis in a second. Gaussian and neural networks.",
    "start": "1806600",
    "end": "1811855"
  },
  {
    "text": "And again, just\nto be clear here, what I mean by a\npolicy class is what is the functional\nform we are using",
    "start": "1811855",
    "end": "1817570"
  },
  {
    "text": "to give us a probability\nof an action given a state.",
    "start": "1817570",
    "end": "1825049"
  },
  {
    "text": "So are we having\nsomething like-- well, I guess we can just\nsee on the next slide what this will look like.",
    "start": "1825050",
    "end": "1830070"
  },
  {
    "text": " So we're going to assume\nI'm going to give you some examples of those of\nwhat softmax, and Gaussian,",
    "start": "1830070",
    "end": "1838535"
  },
  {
    "text": "and neural networks look\nlike in a second in terms of how we differentiate them. But these are just\ndifferent ways for us",
    "start": "1838535",
    "end": "1844502"
  },
  {
    "text": "to parameterize what is the\nprobability of an action given a state. Actually, I guess I'll give\na quick example of Gaussian.",
    "start": "1844503",
    "end": "1851160"
  },
  {
    "text": "For a Gaussian,\nyou could imagine-- let's imagine I have a robot. And I'm trying to figure out,\nsay, how much speed to apply.",
    "start": "1851160",
    "end": "1859019"
  },
  {
    "text": "Then, you might\nhave a policy class that says the action I take is\nequal to a Gaussian centered",
    "start": "1859020",
    "end": "1866430"
  },
  {
    "text": "around 0.5 with some\nstandard deviation.",
    "start": "1866430",
    "end": "1871680"
  },
  {
    "text": "So it would be a\nstochastic policy. And it would say the\naverage amount of speed",
    "start": "1871680",
    "end": "1877183"
  },
  {
    "text": "you're going to apply\nis 0.5, but you're going to have some\nvariability around that. That would give you some\nstochastic behavior.",
    "start": "1877183",
    "end": "1884490"
  },
  {
    "text": "So sometimes, your robot\nwould go really slowly. Sometimes, it would go fast. Sometimes, it would go\nin a negative direction.",
    "start": "1884490",
    "end": "1891070"
  },
  {
    "text": "OK, so let's keep assuming that\nthe policy is differentiable. Whenever it's non-0,\nwe know the gradient.",
    "start": "1891070",
    "end": "1896390"
  },
  {
    "text": "That still doesn't tell us how\nto solve policy gradient methods yet, because what we want to do\nis take derivatives of the value",
    "start": "1896390",
    "end": "1903100"
  },
  {
    "text": "function. So we want to say I want to find\nthe maximum, the policy that has the best value\nfunction, which",
    "start": "1903100",
    "end": "1909159"
  },
  {
    "text": "means I'm going to need to take\nthe derivative of the value function with respect to\nthe policy parameters.",
    "start": "1909160",
    "end": "1915322"
  },
  {
    "text": "So remember that\nthe policy value, the value of the initial\nstarting state under a policy, is going to be the\nexpected sum of rewards.",
    "start": "1915322",
    "end": "1924410"
  },
  {
    "text": "We don't have to use\ndiscounting for most of today if we assume it's finite. So I'll just say\nwe're going to assume",
    "start": "1924410",
    "end": "1931020"
  },
  {
    "text": "we're in the episodic case. So this is finite. So no discount\n[? counting ?] for now.",
    "start": "1931020",
    "end": "1938670"
  },
  {
    "text": " So we don't need\ndiscounting for now, because it's always a finite\nlength, so we're never",
    "start": "1938670",
    "end": "1945240"
  },
  {
    "text": "going to have infinite reward. So the policy value is\njust the expected sum of discounted rewards\nwhen we follow",
    "start": "1945240",
    "end": "1951060"
  },
  {
    "text": "the policy\nparameterized by theta till the end of the\nepisode, starting from the state [? s0. ?] And\nthere are lots of different ways",
    "start": "1951060",
    "end": "1960620"
  },
  {
    "text": "for us to write this down. So one way is for us to write\ndown [INAUDIBLE] r is equal to--",
    "start": "1960620",
    "end": "1967350"
  },
  {
    "text": "well, it's equal to the\nstate action value averaged over the probability of us\ntaking each of those actions",
    "start": "1967350",
    "end": "1973800"
  },
  {
    "text": "under our policy. So this here just says, what\nis the probability of me",
    "start": "1973800",
    "end": "1979290"
  },
  {
    "text": "taking this action? Starting state s0. If I have policy\nparameterized by theta times",
    "start": "1979290",
    "end": "1985000"
  },
  {
    "text": "what is my Q value. Starting in that state,\ntaking that particular action, and then following that\npolicy for the rest of it.",
    "start": "1985000",
    "end": "1993409"
  },
  {
    "text": "So this is one way to\nwrite it, but we can also think of a quite\ndifferent way, which is let's think about trajectories.",
    "start": "1993410",
    "end": "2000799"
  },
  {
    "text": "All right. Don't want these\n[INAUDIBLE] in a second. So this is a trajectory.",
    "start": "2000800",
    "end": "2007070"
  },
  {
    "text": "What's a trajectory? That's going to be s0, and then\nit's going to be an action, and then s1, dot, dot, dot--",
    "start": "2007070",
    "end": "2013275"
  },
  {
    "text": " sampled from pi theta.",
    "start": "2013275",
    "end": "2023200"
  },
  {
    "text": "So another way we can\nthink of the value-- and then, this is going to be\nthe reward for that trajectory,",
    "start": "2023201",
    "end": "2031570"
  },
  {
    "text": "that whole trajectory. ",
    "start": "2031570",
    "end": "2036690"
  },
  {
    "text": "And we've called\n[? R G ?] before. So sometimes-- I'll just\nwrite down that in case.",
    "start": "2036690",
    "end": "2043450"
  },
  {
    "text": "[? Sometimes-- ?] [INAUDIBLE]",
    "start": "2043450",
    "end": "2051739"
  },
  {
    "text": "So another way we can\nthink of the value is we say, well,\nlet's just sum over all possible trajectories we\ncould reach under this policy.",
    "start": "2051739",
    "end": "2058610"
  },
  {
    "text": "And what would be the reward\nfor each of those trajectories? And I'm just going to\ntake a weighted sum. Now, of course, you\nmight be thinking",
    "start": "2058610",
    "end": "2064940"
  },
  {
    "text": "that's totally intractable. And yes, in general, if you\nhave a really long trajectory,",
    "start": "2064940",
    "end": "2070679"
  },
  {
    "text": "then it's going\nto be [INAUDIBLE]. And you have a really\nlarge state space. And you could reach many states. In general, it's not going to be\npossible to actually enumerate",
    "start": "2070679",
    "end": "2077105"
  },
  {
    "text": "this. But this is mathematically\nwell defined. This is just an expectation\nover the reward of trajectories.",
    "start": "2077105",
    "end": "2084090"
  },
  {
    "text": "And we know whenever\nwe see expectations that we can approximate\nthose with finite samples. You can think of just\ntaking n samples, just",
    "start": "2084090",
    "end": "2090949"
  },
  {
    "text": "like what we saw with\nMonte Carlo methods, and using that to\napproximate a trajectory.",
    "start": "2090949",
    "end": "2096199"
  },
  {
    "text": "So in general, this\nis intractable. In general, intractable, but\nwe can approximate by sampling.",
    "start": "2096199",
    "end": "2109855"
  },
  {
    "start": "2109855",
    "end": "2115810"
  },
  {
    "text": "So this is one way\nwe could write down. So this is another\nalso a valid way to write down what is the\nvalue of starting this state",
    "start": "2115810",
    "end": "2122470"
  },
  {
    "text": "and following the policy.  So I've written that down more\nneatly here p of [? tau ?] theta",
    "start": "2122470",
    "end": "2129734"
  },
  {
    "text": "is the probability\nof our trajectories. [INAUDIBLE] [? when ?] you\nexecute that policy starting state s0, and that is the sum\nof the rewards for trajectory.",
    "start": "2129735",
    "end": "2137330"
  },
  {
    "text": "In this class, we're going to\nfocus on this latter definition. But instead of setting\n[? bar, ?] they",
    "start": "2137330",
    "end": "2142670"
  },
  {
    "text": "have a nice way to think about\npolicy gradient methods that starts from the\nother definition. So you can always look at that.",
    "start": "2142670",
    "end": "2148550"
  },
  {
    "text": "But both are totally\nvalid definitions. ",
    "start": "2148550",
    "end": "2154670"
  },
  {
    "text": "So now, we're going to focus on\nthinking about likelihood ratio policies. So we're going to be\nthinking about this case",
    "start": "2154670",
    "end": "2160750"
  },
  {
    "text": "where we have a distribution\nover trajectories, and then what is\nthe sum of rewards for each of those trajectories.",
    "start": "2160750",
    "end": "2167290"
  },
  {
    "text": "So we have our value function. And now, what we want to\ndo is find the argmax,",
    "start": "2167290",
    "end": "2172720"
  },
  {
    "text": "so that we maximize [INAUDIBLE]\nhaving probability of getting trajectories with high reward.",
    "start": "2172720",
    "end": "2180635"
  },
  {
    "text": "So that's nice. So instead of just thinking\nabout the value function, we now can think of\nit as, OK, I want to have policies that induce\ntrajectories through the state",
    "start": "2180635",
    "end": "2187310"
  },
  {
    "text": "space, through the\nstate and action space that give me high reward. ",
    "start": "2187310",
    "end": "2192497"
  },
  {
    "text": "So what we're going to\nneed to be able to do is to take a gradient\nthrough the right hand side.",
    "start": "2192497",
    "end": "2200540"
  },
  {
    "text": "So that's what we're\ngoing to do now. ",
    "start": "2200540",
    "end": "2205870"
  },
  {
    "text": "[INAUDIBLE] OK, so we're going to\ntake the gradient of this. Because once we have the\ngradient of the value function",
    "start": "2205870",
    "end": "2211550"
  },
  {
    "text": "with respect to the\npolicy parameters, we can update our\npolicy parameters to increase, hopefully,\nthe value of the policy",
    "start": "2211550",
    "end": "2218090"
  },
  {
    "text": "that we're at. So what we're going\nto do is we're going to say we're going to\ntake the gradient with respect to the right hand side.",
    "start": "2218090",
    "end": "2224190"
  },
  {
    "text": "We can rewrite this by\npushing in the gradient. ",
    "start": "2224190",
    "end": "2235115"
  },
  {
    "text": "Now, R of tau doesn't depend\non the policy parameters. That's just what is\nthe reward once you've told me what a trajectory is.",
    "start": "2235115",
    "end": "2241490"
  },
  {
    "text": "So we can put that\non the other side. ",
    "start": "2241490",
    "end": "2247920"
  },
  {
    "text": "So the only part that depends\non the policy parameters. And now, I'm going\nto play a trick.",
    "start": "2247920",
    "end": "2255060"
  },
  {
    "text": "I'm going to note that this\nis going to be equal to--",
    "start": "2255060",
    "end": "2263337"
  },
  {
    "text": "well, I'm going to\ndo something that's going to seem not very\nhelpful for a second, and then we'll see\nwhy it's helpful.",
    "start": "2263337",
    "end": "2268797"
  },
  {
    "text": "I'm just going to multiply\nand divide by the probability of a trajectory. ",
    "start": "2268797",
    "end": "2275890"
  },
  {
    "text": "I haven't done anything. I've just multiplied\nby 1, and I've happened to multiply\nby the top and bottom",
    "start": "2275890",
    "end": "2281440"
  },
  {
    "text": "by the probability\nof that trajectory. But then, I'm going to note\nthat the derivative with respect",
    "start": "2281440",
    "end": "2289660"
  },
  {
    "text": "to log of the\ntrajectory and theta is just equal to 1 over the\nprobability of the trajectory",
    "start": "2289660",
    "end": "2296934"
  },
  {
    "text": "of theta times the\nderivative with respect to the trajectory and theta.",
    "start": "2296935",
    "end": "2305458"
  },
  {
    "text": "Because the derivative\nof log is just equal to 1 over the value\ntimes the derivative of the thing inside the log.",
    "start": "2305458",
    "end": "2311619"
  },
  {
    "text": "So that looks exactly like this. So that's the trick\nthat we're playing here.",
    "start": "2311620",
    "end": "2319020"
  },
  {
    "text": "And so we can rewrite this\nthen as the probability. And I'll tell you why\nwe did this in a second.",
    "start": "2319020",
    "end": "2324445"
  },
  {
    "start": "2324445",
    "end": "2334515"
  },
  {
    "text": "All right, let me\njust rewrite it one more time, so\nit's more easy to see.",
    "start": "2334515",
    "end": "2339660"
  },
  {
    "start": "2339660",
    "end": "2350109"
  },
  {
    "text": "Why did we do this? The reason we did this\nis that, in general, it's going to be hard for\nus to think about--",
    "start": "2350110",
    "end": "2355600"
  },
  {
    "text": "or it might be tricky\nfor us to think about how do we propagate our\nderivative through something that's an expectation.",
    "start": "2355600",
    "end": "2361040"
  },
  {
    "text": "We had an expectation over\nall the trajectories weighted by the reward of\nthose trajectories. We now want to take a\ngradient with respect to it.",
    "start": "2361040",
    "end": "2368590"
  },
  {
    "text": "We want to end up with something\nthat is computable from samples, because it's easy for\nus to get samples.",
    "start": "2368590",
    "end": "2374000"
  },
  {
    "text": "We can actually run our\npolicy in the environment. So by playing this\ntrick, what we now have is something that we can\nalso sample, because this is now",
    "start": "2374000",
    "end": "2381670"
  },
  {
    "text": "an expectation over trajectories\nof the reward of the trajectory weighted by the gradient of\nthe log of the probability",
    "start": "2381670",
    "end": "2388359"
  },
  {
    "text": "of that trajectory. And we'll talk soon about\nhow you compute this part, but this expectation\ncan be sampled.",
    "start": "2388360",
    "end": "2398020"
  },
  {
    "text": "Because this is just a\nprobability over trajectories. And so we could sample,\nsay, hundreds of them and approximate that\nouter expectation.",
    "start": "2398020",
    "end": "2405980"
  },
  {
    "text": "So that's one of the right\nreasons why this is-- I'm just writing this out more\nneatly here on the next slide. This is called the likelihood\nratio, this term here.",
    "start": "2405980",
    "end": "2416030"
  },
  {
    "text": "And so that's one of the\nbenefits to doing this, is that we want to end up with\nsomething that is computable. We want to be able to get\nthis gradient with respect",
    "start": "2416030",
    "end": "2422617"
  },
  {
    "text": "to the value function for\nthe policy parameters. And so this is going\nto give us something that we can approximate with\nsamples and we can compute.",
    "start": "2422617",
    "end": "2429049"
  },
  {
    "text": " All right, now, you still might\nbe a little bit concerned,",
    "start": "2429050",
    "end": "2434660"
  },
  {
    "text": "because-- all right, maybe\nyou think, yeah, I can maybe compute this\nby writing things out",
    "start": "2434660",
    "end": "2440605"
  },
  {
    "text": "in the environment,\nbut I'm still going to have to\ntake this derivative. And how am I going to do that?",
    "start": "2440605",
    "end": "2445700"
  },
  {
    "text": "And what does it\nend up depending on? So let's do a next step.",
    "start": "2445700",
    "end": "2450740"
  },
  {
    "text": "So as I said, what\nwe're going to do here, this is an expectation,\nso an expectation.",
    "start": "2450740",
    "end": "2456156"
  },
  {
    "text": " We're going to approximate\nthat expectation",
    "start": "2456156",
    "end": "2461540"
  },
  {
    "text": "with an empirical estimate. So we're just going\nto-- instead of actually taking all possible\ntrajectories,",
    "start": "2461540",
    "end": "2469000"
  },
  {
    "text": "particularly in the\ncase of vision input, you could imagine that\nwould be completely insane. So we're just going\nto approximate it",
    "start": "2469000",
    "end": "2474772"
  },
  {
    "text": "by taking m samples. But we still have\nto handle this. So that's what we're\ngoing to do next.",
    "start": "2474772",
    "end": "2481490"
  },
  {
    "text": "So this first part\nshould all seem clear. The second part should, at\nleast certainly, for most of us,",
    "start": "2481490",
    "end": "2487040"
  },
  {
    "text": "would not be clear yet about\nhow we do that second part. OK, so what do we do with that?",
    "start": "2487040",
    "end": "2492732"
  },
  {
    "text": "What we're going\nto do now is we're going to decompose that latter\npart into states and actions.",
    "start": "2492732",
    "end": "2500099"
  },
  {
    "text": "So remember that\nwhat this means here is this is going to be\na particular trajectory",
    "start": "2500100",
    "end": "2505650"
  },
  {
    "text": "we get by following\na policy for t steps or until the end of the episode.",
    "start": "2505650",
    "end": "2510740"
  },
  {
    "text": "OK. So let me just\nremind ourselves what t is going to look like here. ",
    "start": "2510740",
    "end": "2520480"
  },
  {
    "text": "So this is going to be\nlike time step here. I'm using the\nsubscript as time step.",
    "start": "2520480",
    "end": "2525890"
  },
  {
    "text": "OK, so let's just write\nout what a trajectory is and what those\nprobabilities are.",
    "start": "2525890",
    "end": "2532120"
  },
  {
    "text": "Are we assuming or\napproximating the probability of the trajectory\njust to be 1/n?",
    "start": "2532120",
    "end": "2538070"
  },
  {
    "text": "No, good question or sorry. Yes, for this part? Yes. Yes. We're assuming that we\nare [INAUDIBLE] for each",
    "start": "2538070",
    "end": "2544970"
  },
  {
    "text": "of the trajectories, we're\nusing a Monte Carlo estimate. We're just using 1/m. But if some trajectories\nare more likely than others,",
    "start": "2544970",
    "end": "2551220"
  },
  {
    "text": "they'll appear more\nin that set of m. Yeah, good question.",
    "start": "2551220",
    "end": "2556610"
  },
  {
    "text": "OK, so let's now try to\nexpress what the probability is of a trajectory.",
    "start": "2556610",
    "end": "2561819"
  },
  {
    "text": "OK, so the probability of a\ntrajectory we can write out as follows.",
    "start": "2561820",
    "end": "2567850"
  },
  {
    "text": "So we're going to still\nhave that outside log.",
    "start": "2567850",
    "end": "2573350"
  },
  {
    "text": "We're going to do the following. OK. I'm going to say mu of s0 is\nequal to the probability of s0.",
    "start": "2573350",
    "end": "2580519"
  },
  {
    "text": "That's just like what is\nour probability distribution over our starting state. OK, so that's mu.",
    "start": "2580520",
    "end": "2585740"
  },
  {
    "text": "And then what we're going\nto have is the following. t equals 0 to T minus 1.",
    "start": "2585740",
    "end": "2594329"
  },
  {
    "text": "We're going to have our policy.  So this is going to say,\nwhat is the probability",
    "start": "2594330",
    "end": "2601890"
  },
  {
    "text": "that I pick action I picked,\ngiven the current state I'm in times the probability of st\nplus 1, given s0 to t, a0 to t.",
    "start": "2601890",
    "end": "2618580"
  },
  {
    "text": " So what I've done is\nI've just written out.",
    "start": "2618580",
    "end": "2624540"
  },
  {
    "text": "What is happening in my\ntrajectory here, as I start, I have some distribution\nover C in this initial state.",
    "start": "2624540",
    "end": "2631140"
  },
  {
    "text": "Under my policy, I have some\nprobability of picking a0, and that's here. Then I'm going to\nassume for a second",
    "start": "2631140",
    "end": "2638070"
  },
  {
    "text": "that rewards are deterministic. But you could add in\na reward term here. And then I'm going\nto say, well, what's",
    "start": "2638070",
    "end": "2643110"
  },
  {
    "text": "the chance that I\nget to state s1, given my history, given the\nprevious states and the actions?",
    "start": "2643110",
    "end": "2650730"
  },
  {
    "text": "So I've just written this\nout as a joint probability. And now what I can\ndo is I can use",
    "start": "2650730",
    "end": "2657059"
  },
  {
    "text": "the fact that log of a\ntimes b is equal to log of a plus log of b. So I'm just going to\ndecompose all these terms.",
    "start": "2657060",
    "end": "2662310"
  },
  {
    "text": "So I'm not applying\nmy gradient yet, but I'm just going to have log\nof mu of s0 plus sum over t",
    "start": "2662310",
    "end": "2671490"
  },
  {
    "text": "equals 0 to T minus 1\njust plus sum over t",
    "start": "2671490",
    "end": "2679540"
  },
  {
    "text": "equals 0 to T minus 1. ",
    "start": "2679540",
    "end": "2689680"
  },
  {
    "text": "Open the log. ",
    "start": "2689680",
    "end": "2699420"
  },
  {
    "text": "Sorry, it's a bit messy. I'll make sure to\nadd a clean version. ",
    "start": "2699420",
    "end": "2705540"
  },
  {
    "text": "So what I've done is I've\njust decomposed my log. But now this is really nice\nbecause this term is not",
    "start": "2705540",
    "end": "2713910"
  },
  {
    "text": "a function of theta. This is just my initial\nstarting state distribution. It has nothing to\ndo with my policy.",
    "start": "2713910",
    "end": "2719110"
  },
  {
    "text": "So this drops out. Does this part\ndepend on my policy? Yes.",
    "start": "2719110",
    "end": "2724690"
  },
  {
    "text": "Does this part\ndepend on my policy? No. No. So when we take the derivative\nof it, it disappears.",
    "start": "2724690",
    "end": "2731930"
  },
  {
    "text": "So that is beautiful\nbecause now it means we don't have to know\nabout our dynamics model. So the only term that is still\naround after this is this thing.",
    "start": "2731930",
    "end": "2747550"
  },
  {
    "start": "2747550",
    "end": "2757970"
  },
  {
    "text": "All right. So this is great\nbecause now we don't",
    "start": "2757970",
    "end": "2763770"
  },
  {
    "text": "depend on our dynamics model. We have written down what this\nterm is as a function of--",
    "start": "2763770",
    "end": "2773935"
  },
  {
    "text": "so we're just doing\nthis term right now as just the sum\nof the derivative of the log of the policy\nat that particular point.",
    "start": "2773935",
    "end": "2781800"
  },
  {
    "text": "So we're summing up for each of\nthe different actions we took along the way, what was the\nlog of their probability",
    "start": "2781800",
    "end": "2787625"
  },
  {
    "text": "and taking the derivative\nof that whole term. ",
    "start": "2787625",
    "end": "2794340"
  },
  {
    "text": "All right. So we don't need any dynamics\nmodel, which is great.",
    "start": "2794340",
    "end": "2800880"
  },
  {
    "text": "And I'm just going\nto say here, I'm going to make sure that\nsomething is consistent here. Oh, yeah.",
    "start": "2800880",
    "end": "2807120"
  },
  {
    "text": "I had a question on the slide\nwith [? all the math. ?] With all the math? Yeah. Uh-huh.",
    "start": "2807120",
    "end": "2812430"
  },
  {
    "text": "So in the dynamics model, the ps\nat t plus 1, for the given part,",
    "start": "2812430",
    "end": "2820150"
  },
  {
    "text": "why did we look at the\nentire history of not just the past state and action? Great question.",
    "start": "2820150",
    "end": "2826059"
  },
  {
    "text": "So what I've written about-- so this question is a good one. I wrote down here the dynamics\nin a really general form.",
    "start": "2826060",
    "end": "2833050"
  },
  {
    "text": "I am writing them\ndown and I'm not making the Markov assumption. We could make the\nMarkov assumption.",
    "start": "2833050",
    "end": "2838151"
  },
  {
    "text": "But what I wanted\nto point out here is that you don't have to\nmake the Markov assumption. It does not matter.",
    "start": "2838152",
    "end": "2843190"
  },
  {
    "text": "So because the\ndynamics model are independent of your policy,\nwhen you take the derivative,",
    "start": "2843190",
    "end": "2849280"
  },
  {
    "text": "they completely drop out,\nwhether they are Markov, whether they are\nnon-Markov, et cetera. And so that's really nice.",
    "start": "2849280",
    "end": "2854589"
  },
  {
    "text": "It shows that in\nthis case, it's not making the Markov assumption.",
    "start": "2854590",
    "end": "2860490"
  },
  {
    "text": "Now, I did make the Markov\nassumption somewhere. I made it here because I\nassumed that I made the Markov assumption in the sense I\nassumed my policy was Markov.",
    "start": "2860490",
    "end": "2867260"
  },
  {
    "text": "My policy is only depending\non the current state. But your policy also could\ndepend on a history of states.",
    "start": "2867260",
    "end": "2873690"
  },
  {
    "text": "You could have a recurrent\nneural network or any of the other representations\nyou might want to choose there,",
    "start": "2873690",
    "end": "2880660"
  },
  {
    "text": "and then this would just\ndepend on your history. ",
    "start": "2880660",
    "end": "2886480"
  },
  {
    "text": "Good question. All right. So I just want to go,\nand I want to make sure that I wrote it down neatly in\nterms of the most general form.",
    "start": "2886480",
    "end": "2893720"
  },
  {
    "text": "That's why I'm skipping\nthis right now. One of the things to note\nhere in terms of just notation is that people often call this\nthing here a score function.",
    "start": "2893720",
    "end": "2901610"
  },
  {
    "text": "So this derivative with\nrespect to log of the policy itself, we often call\na score function.",
    "start": "2901610",
    "end": "2907530"
  },
  {
    "text": " So in general, the nice thing\nis that it's generally not",
    "start": "2907530",
    "end": "2917540"
  },
  {
    "text": "very hard to compute\nthe score function. So if you have a\ndifferentiable function, we can compute the score\nfunction pretty easily",
    "start": "2917540",
    "end": "2923810"
  },
  {
    "text": "in many cases. Let me just make\nthis a bit smaller. OK. So let's see what\nthat might look",
    "start": "2923810",
    "end": "2929599"
  },
  {
    "text": "like for a couple of\ndifferent policy classes. So one thing we could do, which\nis a pretty popular thing to do,",
    "start": "2929600",
    "end": "2936450"
  },
  {
    "text": "is to do a softmax policy. So the idea in this\ncase is that let's take",
    "start": "2936450",
    "end": "2941990"
  },
  {
    "text": "a linear combination\nof features, so phi s, a dot\nproduct with theta.",
    "start": "2941990",
    "end": "2947810"
  },
  {
    "text": "And then you could say the\nprobability of your action is proportional to the\nexponentiated weight.",
    "start": "2947810",
    "end": "2954980"
  },
  {
    "text": "So you take the\nexponent of that dot product between the features,\nand then you normalize it.",
    "start": "2954980",
    "end": "2960050"
  },
  {
    "text": "And that gives you generally\na stochastic policy. You can also have a temperature\nparameter in there if you want.",
    "start": "2960050",
    "end": "2966525"
  },
  {
    "text": "And the nice thing about\nthis is that we can write it. We can take the derivative\nof this very easily.",
    "start": "2966525",
    "end": "2971750"
  },
  {
    "text": "So we can just do that quickly\nhere just to illustrate. So what this is just to\nillustrate that it is often",
    "start": "2971750",
    "end": "2978680"
  },
  {
    "text": "very feasible to take the\nderivative with respect to the policy parameterization.",
    "start": "2978680",
    "end": "2983870"
  },
  {
    "text": "So this is just going to be\nthe derivative of the log of e to phi of s, a t theta\ndivided by phi s, a OK.",
    "start": "2983870",
    "end": "2998800"
  },
  {
    "text": "So we can do this here, and\nwe can rewrite this here as-- ",
    "start": "2998800",
    "end": "3024556"
  },
  {
    "text": "and so this is just\ngoing to be equal to phi s, a minus [INAUDIBLE].",
    "start": "3024556",
    "end": "3031291"
  },
  {
    "start": "3031291",
    "end": "3044005"
  },
  {
    "text": "So I'm just taking\nthe derivative of this for a particular theta.",
    "start": "3044005",
    "end": "3049599"
  },
  {
    "text": "And so we can just rewrite that\nas phi s, a minus sum a theta.",
    "start": "3049600",
    "end": "3058503"
  },
  {
    "start": "3058503",
    "end": "3066460"
  },
  {
    "text": "OK. So what I've done\nhere is I've taken the derivative with\nrespect to this function for a particular theta.",
    "start": "3066460",
    "end": "3073230"
  },
  {
    "text": "And then what I've\nsaid here is, well, you could notice that this\nhere is exactly just equal to my pi theta of s, a.",
    "start": "3073230",
    "end": "3081599"
  },
  {
    "text": "So it's like I'm getting this\nweighting over the features. OK, put this on the\nnext slide neatly.",
    "start": "3081600",
    "end": "3088780"
  },
  {
    "text": "OK, so the score function\nfor the softmax policy is just going to be\nequal to the feature s,",
    "start": "3088780",
    "end": "3096245"
  },
  {
    "text": "a phi s, a minus\nthe expected value of the policy of the features.",
    "start": "3096246",
    "end": "3103490"
  },
  {
    "text": "Yeah. I'm sorry. What does phi usually mean? Great question. Well, if phi could\nbe, for example,",
    "start": "3103490",
    "end": "3110470"
  },
  {
    "text": "you could think of it as like if\nyou have a large neural network that's doing some\nrepresentation, it could be the last layer,\nlike the second to last layer.",
    "start": "3110470",
    "end": "3117767"
  },
  {
    "text": "And then you could just do like\na linear dot product of that. ",
    "start": "3117768",
    "end": "3122880"
  },
  {
    "text": "Yeah, that's a good question. Or in case of customers,\nit could be a whole bunch of different features. And then you have different\ngroups over there.",
    "start": "3122880",
    "end": "3129297"
  },
  {
    "text": " All right. So this is also possible\nto do for other functions.",
    "start": "3129297",
    "end": "3136330"
  },
  {
    "text": "So for Gaussians, we\noften want to think about that for\ncontinuous action spaces which are really useful for\nrobotics, where you might",
    "start": "3136330",
    "end": "3143178"
  },
  {
    "text": "have continuous torques or\ncontinuous accelerations, et cetera. You can think of there\nbeing a mean, which",
    "start": "3143178",
    "end": "3148190"
  },
  {
    "text": "is a linear combination\nof some state features. Your variance might be fixed or\nit could also be parameterized,",
    "start": "3148190",
    "end": "3153780"
  },
  {
    "text": "and then your policy\nis a Gaussian. So maybe you're sampling\nsome particular action",
    "start": "3153780",
    "end": "3159000"
  },
  {
    "text": "dependent on your state\nalong with some variance. And then you can\nagain, just directly compute what the\nscore function would",
    "start": "3159000",
    "end": "3166470"
  },
  {
    "text": "be in this case in closed form. But in general, you're\noften probably going to be using this with\ndeep neural networks.",
    "start": "3166470",
    "end": "3172190"
  },
  {
    "text": "And then you can\njust use autodiff to do this just to\nillustrate that there's a number of different\nfunctional forms where you can compute\nthis analytically.",
    "start": "3172190",
    "end": "3180980"
  },
  {
    "text": "OK. All right. So just to recap this,\nwhat we've shown so far is that we can\nhave policy methods",
    "start": "3180980",
    "end": "3186890"
  },
  {
    "text": "where we have a direct\nparameterization of the policy. We can write down\nthe value function as being a weighted sum over\nthe trajectories generated",
    "start": "3186890",
    "end": "3193172"
  },
  {
    "text": "by that policy times the reward. It turns out that when we want\nto take the derivative of that, we can re-express\nit so that we just",
    "start": "3193172",
    "end": "3199490"
  },
  {
    "text": "think of we don't need\nthe dynamics model, and we're weighing\nthese score functions.",
    "start": "3199490",
    "end": "3207030"
  },
  {
    "text": "So now let's just\ndo a small check your understanding about\nlikelihood ratio, score function",
    "start": "3207030",
    "end": "3214530"
  },
  {
    "text": "policy gradients. And so I'd like\nyou to do is say, does it require that your reward\nfunction is differentiable?",
    "start": "3214530",
    "end": "3220390"
  },
  {
    "text": "Can you only use it with\nMarkov decision process? Is it useful mostly for infinite\nhorizon tasks? a and b; a, b,",
    "start": "3220390",
    "end": "3226619"
  },
  {
    "text": "and c; none of the\nabove. or not sure? Let's just take a\nsecond to do that. ",
    "start": "3226620",
    "end": "3239970"
  },
  {
    "text": "All right. We have a good\nsplit of opinions. Nobody is not sure, but\nthere is a lot of spread.",
    "start": "3239970",
    "end": "3245827"
  },
  {
    "text": "So why don't you\ntalk to your neighbor and see if we can come\nto more consensus. [SIDE CONVERSATION]",
    "start": "3245827",
    "end": "3253482"
  },
  {
    "start": "3253482",
    "end": "3268990"
  },
  {
    "text": "I'm sorry to interrupt\nsome good discussions, but I want to make sure we\nget through reinforced today. So there's a little\nbit of a tricky one.",
    "start": "3268990",
    "end": "3275922"
  },
  {
    "text": "In fact, when I was giving\nit to one of my TAs, I forgot to put a\nnone of the above, and he was like,\nwait, what the hell.",
    "start": "3275922",
    "end": "3282010"
  },
  {
    "text": "So it's none of the above. And so the first one's part\nof the actual elegant aspect",
    "start": "3282010",
    "end": "3287260"
  },
  {
    "text": "of policy gradients. So as you can see here, you\nneed the policy function to be differentiable,\nbut the reward function",
    "start": "3287260",
    "end": "3294490"
  },
  {
    "text": "does not have to be. The reward function\nis not a function of the policy in the way\nthat we've written it here.",
    "start": "3294490",
    "end": "3299788"
  },
  {
    "text": "So that's pretty elegant. So that has motivated people\nin a really wide range of areas where you really might\nhave very complicated reward",
    "start": "3299788",
    "end": "3306520"
  },
  {
    "text": "functions to be interested\nin using what we're going to see soon, which is reinforce,\nwhich is based on this idea",
    "start": "3306520",
    "end": "3312520"
  },
  {
    "text": "because you just need the\npolicy parameterization to be differentiable. So that's really cool.",
    "start": "3312520",
    "end": "3317980"
  },
  {
    "text": "B doesn't have to be\nMarkov because as we saw, the dynamics model drops out.",
    "start": "3317980",
    "end": "3323410"
  },
  {
    "text": "And so what you're\nsaying in that case, it doesn't appear at all. So it doesn't need to be Markov.",
    "start": "3323410",
    "end": "3328560"
  },
  {
    "text": "You don't need\ndifferentiability. And we are assuming\nthat it's finite horizon so that we can\nactually-- episodic",
    "start": "3328560",
    "end": "3334640"
  },
  {
    "text": "so we can get m more than one. If it was infinite horizon,\nwe'd only get m equals 1.",
    "start": "3334640",
    "end": "3339980"
  },
  {
    "text": "So all three of these are false.  So let me just make\nsure I circle that.",
    "start": "3339980",
    "end": "3348150"
  },
  {
    "text": " OK. So just to give brief\nintuitions because to make sure",
    "start": "3348150",
    "end": "3353590"
  },
  {
    "text": "that we get to\nreinforce, you can think of if this is a generic\nway of writing this down,",
    "start": "3353590",
    "end": "3358730"
  },
  {
    "text": "we have some function\ntimes the derivative of log of some other\nprobability function.",
    "start": "3358730",
    "end": "3364030"
  },
  {
    "text": "And you can think\nof this first part as measuring how\ngood a sample is. And what the idea is that\nwhen you have the derivative,",
    "start": "3364030",
    "end": "3370670"
  },
  {
    "text": "you're trying to move up the\nlog probability of samples that have high reward.",
    "start": "3370670",
    "end": "3377220"
  },
  {
    "text": "Because you generally\nwant policies that visit parts of the\nstate and action space where you get high reward. So that's the intuition.",
    "start": "3377220",
    "end": "3383290"
  },
  {
    "text": "And the nice thing\nis that f doesn't have to be differentiable. It could be discontinuous.",
    "start": "3383290",
    "end": "3388330"
  },
  {
    "text": "It could be unknown as long as\nyou can get samples from it. So it can be extremely\nflexible to what",
    "start": "3388330",
    "end": "3393480"
  },
  {
    "text": "is that reward or\nobjective function. So I put a couple\nof slides here.",
    "start": "3393480",
    "end": "3401410"
  },
  {
    "text": "I believe it was\nJohn [? Schulman ?] who originally had these ones. I put some credits at the front.",
    "start": "3401410",
    "end": "3407180"
  },
  {
    "text": "But you can think of taking\na combination between what the probability is of\nyour input of your x as well as your function.",
    "start": "3407180",
    "end": "3413120"
  },
  {
    "text": "So in our case, that's going\nto be the reward function. This is generally\ngoing to be the reward function over trajectories, and\nthis is going to be our policy.",
    "start": "3413120",
    "end": "3420330"
  },
  {
    "text": " It gives us probabilities\nof the trajectories. And so you can think of\ncombining between these two",
    "start": "3420330",
    "end": "3427690"
  },
  {
    "text": "to actually change\nyour parameter space. So just to give a\nlittle bit of intuition",
    "start": "3427690",
    "end": "3433970"
  },
  {
    "text": "over what this sort of\ngradient estimation is doing. ",
    "start": "3433970",
    "end": "3441057"
  },
  {
    "text": "So in general, we can also\nwrite down a policy gradient theorem, which says, we\ncould either use something",
    "start": "3441058",
    "end": "3447250"
  },
  {
    "text": "like episodic reward. Or we could be trying to look\nat average reward per time step.",
    "start": "3447250",
    "end": "3452849"
  },
  {
    "text": "Or we could be trying to\nlook at average value. And in all of these\ncases, we can end up",
    "start": "3452850",
    "end": "3458010"
  },
  {
    "text": "writing something that looks\nreally similar to the equation I showed you before, which is\nthe derivative with respect",
    "start": "3458010",
    "end": "3464250"
  },
  {
    "text": "to these value\nfunctions or something like a value function is\ngoing to look something like the derivative\nwith the score function,",
    "start": "3464250",
    "end": "3471990"
  },
  {
    "text": "the expected value of\nthe trajectories you're going to get, of the log of the\nparameters times the Q function",
    "start": "3471990",
    "end": "3479250"
  },
  {
    "text": "or the return for that\nparticular state action",
    "start": "3479250",
    "end": "3485180"
  },
  {
    "text": "pair following the policy. And there's a nice derivation\nin Sutton and Barto about that.",
    "start": "3485180",
    "end": "3491660"
  },
  {
    "text": "At a high level, I think the\nuseful thing to know here is just that can extend\nit beyond just thinking",
    "start": "3491660",
    "end": "3497690"
  },
  {
    "text": "of like the sample of return. And we can think of\nthere being Q functions. All right.",
    "start": "3497690",
    "end": "3503099"
  },
  {
    "text": "Now what I've shown you so far\nis something that is correct, and we can turn it\ninto an algorithm,",
    "start": "3503100",
    "end": "3508860"
  },
  {
    "text": "but it does not leverage much\nof the temporal structure. So what do I mean by that? So what we've written down\nhere is a valid gradient.",
    "start": "3508860",
    "end": "3516450"
  },
  {
    "text": "It's unbiased, but\nit can be very noisy. So we're estimating this\nby Monte Carlo method",
    "start": "3516450",
    "end": "3522650"
  },
  {
    "text": "because we have these m samples. And as we know from Monte\nCarlo methods before, they are unbiased, but they\ncan be very high variance.",
    "start": "3522650",
    "end": "3529910"
  },
  {
    "text": "And so some of the ways to\nmake this more practical, and what I mean by that is a\nbetter estimate of the gradient",
    "start": "3529910",
    "end": "3535490"
  },
  {
    "text": "and hopefully with less\ndata, because ultimately, we're going to have to\nbe using this information to update our weights to\ntry to get to a good policy.",
    "start": "3535490",
    "end": "3543397"
  },
  {
    "text": "So we want this to\nbe data efficient-- is we can try to leverage\nthe temporal structure, and we can also\ninclude baselines.",
    "start": "3543397",
    "end": "3549500"
  },
  {
    "text": "All right. So let's first see the\ntemporal structure. So what we've done before is\nwe've summed up all the rewards",
    "start": "3549500",
    "end": "3556049"
  },
  {
    "text": "from a whole\ntrajectory, and we've multiplied it by the sum\nof the score function",
    "start": "3556050",
    "end": "3561480"
  },
  {
    "text": "for the whole trajectory. That's what we've done so far. We can instead\nthink of it as, what",
    "start": "3561480",
    "end": "3566790"
  },
  {
    "text": "if we have the gradient\nestimator for a single reward term? So this is just\nfor one time step.",
    "start": "3566790",
    "end": "3574560"
  },
  {
    "text": "We can think of it\nfor there, which is we have that single\ntime step times the score",
    "start": "3574560",
    "end": "3580290"
  },
  {
    "text": "function for the\nremaining time steps or for the time steps\nup to that point.",
    "start": "3580290",
    "end": "3587208"
  },
  {
    "text": "So it's like we just think\nof the partial trajectory until we got that reward. ",
    "start": "3587208",
    "end": "3593410"
  },
  {
    "text": "So we want to think about\nthe derivative of this. This is the reward we\ngot at this time point. So instead of having\nthis whole sum,",
    "start": "3593410",
    "end": "3599630"
  },
  {
    "text": "we just think of, well, what is\nthe trajectory that we got up to that time point and all\nof their score functions?",
    "start": "3599630",
    "end": "3605869"
  },
  {
    "text": " Does that make sense? I remember having\nquestions about that part.",
    "start": "3605870",
    "end": "3612273"
  },
  {
    "text": " OK, so this is like for a\nsingle time step t prime.",
    "start": "3612273",
    "end": "3619350"
  },
  {
    "text": "And so now what we\ncan do is we can sum this over all time steps.",
    "start": "3619350",
    "end": "3625470"
  },
  {
    "text": "So instead of having the sum\nof all rewards times this, we can say, well, we know\nthat for one time step,",
    "start": "3625470",
    "end": "3632790"
  },
  {
    "text": "it is equal to the expected\nvalue of the reward for that time step\ntimes the score",
    "start": "3632790",
    "end": "3638630"
  },
  {
    "text": "functions up to that point. So let's just\nrewrite it like that. Now we're just going\nto sum over the rewards we got for all time steps.",
    "start": "3638630",
    "end": "3644125"
  },
  {
    "text": " All right. So now what we can do is we\ncan do slight rearrangement.",
    "start": "3644125",
    "end": "3653190"
  },
  {
    "text": "So what we can notice is\nthat for each of the points, so you can think\nof it as, I have--",
    "start": "3653190",
    "end": "3660310"
  },
  {
    "text": "so this is t, 0, 1, 2, 3. And you can think of\nall of these score",
    "start": "3660310",
    "end": "3668490"
  },
  {
    "text": "functions I have\nat each time point. So the score function\nat time step 0",
    "start": "3668490",
    "end": "3674700"
  },
  {
    "text": "is going to appear for r0,\nr1, r2, r3, dot, dot dot.",
    "start": "3674700",
    "end": "3682109"
  },
  {
    "text": "So that's what I've\njust done here. I've said this\nfirst term is going to appear with a reward function\nof all of the subsequent time",
    "start": "3682110",
    "end": "3692330"
  },
  {
    "text": "points. Because that decision happened,\nand then we got a reward,",
    "start": "3692330",
    "end": "3697390"
  },
  {
    "text": "and then we got a whole\nbunch of rewards later.  On the first time\nstep, it can affect",
    "start": "3697390",
    "end": "3704369"
  },
  {
    "text": "the reward you\nget at time step 1 and all the feature after that.",
    "start": "3704370",
    "end": "3709430"
  },
  {
    "text": "So this term here, this score\nfunction can influence-- the one that we\nget on time step 1",
    "start": "3709430",
    "end": "3715970"
  },
  {
    "text": "can influence time\nstep 1 all the way out to the end of the episode. The one we get on time step\n2 can influence time step 2",
    "start": "3715970",
    "end": "3723350"
  },
  {
    "text": "all the way out to the\nend of the episode. So essentially this\nis like saying,",
    "start": "3723350",
    "end": "3728400"
  },
  {
    "text": "my reward on time step 3 cannot\nbe impacted by decisions I make on time step 4.",
    "start": "3728400",
    "end": "3733470"
  },
  {
    "text": "Time only flows one way. So if we think about what\nthose score functions were",
    "start": "3733470",
    "end": "3738890"
  },
  {
    "text": "and like we think of\nthe trajectories that were generated, they're\na temporal structure. And so it means that\nwe cannot have--",
    "start": "3738890",
    "end": "3747240"
  },
  {
    "text": "if we change the policy\nparameters such that decisions in the future change, that\ncan't affect my reward",
    "start": "3747240",
    "end": "3752880"
  },
  {
    "text": "on earlier time steps. So this is leveraging\nthe temporal structure. So this just allows us to\nrewrite the equation so that now",
    "start": "3752880",
    "end": "3762300"
  },
  {
    "text": "we have for each of the\ndifferent score functions essentially which of the\nrewards they influence.",
    "start": "3762300",
    "end": "3769019"
  },
  {
    "text": "And the reason this\nis important is because here you could see\nthat we're multiplying each",
    "start": "3769020",
    "end": "3774539"
  },
  {
    "text": "of the score functions\nby all of the rewards, and now we're only\ngoing to multiply them",
    "start": "3774540",
    "end": "3779850"
  },
  {
    "text": "by the rewards they influence. And so in general, that's going\nto be way less than having the full set of rewards.",
    "start": "3779850",
    "end": "3785970"
  },
  {
    "text": "So this is going to reduce\nthe variance of our estimator without causing any bias,\njust leveraging the fact",
    "start": "3785970",
    "end": "3791550"
  },
  {
    "text": "that decisions in the\nfuture can't affect your rewards in the past. ",
    "start": "3791550",
    "end": "3798480"
  },
  {
    "text": "All right. So that is one of\nthe first things that we're going\nto do in this case.",
    "start": "3798480",
    "end": "3804000"
  },
  {
    "text": "So we're going to write-- so remember in this case that\nif we sum up all the rewards",
    "start": "3804000",
    "end": "3809597"
  },
  {
    "text": "from the current\ntime step to the end, we just called that the return. We've seen that before\nfrom Monte Carlo.",
    "start": "3809597",
    "end": "3815609"
  },
  {
    "text": "So we can just rewrite\nthis expression like that. And that gives us the\nreinforce algorithm.",
    "start": "3815610",
    "end": "3821859"
  },
  {
    "text": "So this is the\nreinforce algorithm that has been incredibly\ninfluential in NLP and robotics and\nmany, many areas.",
    "start": "3821860",
    "end": "3829420"
  },
  {
    "text": "And so what this says\nhere is that the way we change our parameter\nis just our learning rate",
    "start": "3829420",
    "end": "3835650"
  },
  {
    "text": "times our score function\ntimes the return we got from that time step\ntill the end of the episode.",
    "start": "3835650",
    "end": "3841643"
  },
  {
    "text": "So we still have to wait\ntill the end of the episode to update anything. But what happens is\nwe run a full episode",
    "start": "3841643",
    "end": "3846990"
  },
  {
    "text": "with our current policy. And then for each\ntime step, we slightly change our policy parameters by\nusing a learning rate, the score",
    "start": "3846990",
    "end": "3854460"
  },
  {
    "text": "function for that time\nstep plus the return we got from that time step\ntill the end of the episode.",
    "start": "3854460",
    "end": "3860530"
  },
  {
    "text": "And then we just step through\nthat for the whole episode. And that's given us\nT different updates to our policy parameterization.",
    "start": "3860530",
    "end": "3868109"
  },
  {
    "text": "And then we just repeat over\nand over and over again. And what that guarantees\nto us is that eventually we",
    "start": "3868110",
    "end": "3874620"
  },
  {
    "text": "will land in a local\noptima of the value function for the policy\nparameterization.",
    "start": "3874620",
    "end": "3880330"
  },
  {
    "text": " So this is called Monte\nCarlo policy gradient",
    "start": "3880330",
    "end": "3886000"
  },
  {
    "text": "or known as the reinforce. I believe this was in roughly\n1992, so about 30 years ago.",
    "start": "3886000",
    "end": "3893200"
  },
  {
    "text": "And it's been many, many, many\npolicy gradient algorithms are built on this idea. ",
    "start": "3893200",
    "end": "3901580"
  },
  {
    "text": "Now, when you're\nlooking at this, you might still be concerned,\nfrom remembering back from the Monte Carlo\nmethods we've covered,",
    "start": "3901580",
    "end": "3908280"
  },
  {
    "text": "that this estimate G can\noften be pretty high variance. So in general, if you're\njust directly averaging",
    "start": "3908280",
    "end": "3915500"
  },
  {
    "text": "over sample returns, that\nmight be high variance. So one of the next\nfixes we can do,",
    "start": "3915500",
    "end": "3920510"
  },
  {
    "text": "and we'll get to this\nmore on Wednesday, is to introduce a baseline.",
    "start": "3920510",
    "end": "3927150"
  },
  {
    "text": "And I'll just say\nthe goals here is that we're going\nto hopefully try to converge as quickly as\npossible to local optima.",
    "start": "3927150",
    "end": "3934140"
  },
  {
    "text": "So we want to\nreduce the variance over our gradient estimate.",
    "start": "3934140",
    "end": "3939780"
  },
  {
    "text": "And so the baseline is going\nto allow us to hopefully reduce, well, in general,\nyes, reduce the variance",
    "start": "3939780",
    "end": "3945450"
  },
  {
    "text": "over this estimation process. And we'll see two ideas next,\nwhich is introducing a baseline",
    "start": "3945450",
    "end": "3955210"
  },
  {
    "text": "and then thinking about an\nalternative to the Monte Carlo returns. So those are the ideas that\nwe're going to go through next.",
    "start": "3955210",
    "end": "3962450"
  },
  {
    "text": "I guess I'll just\ndo one more thing, and we will go through\nthe proof of it next time. So I'll just introduce\nthe concept of baseline",
    "start": "3962450",
    "end": "3968350"
  },
  {
    "text": "and then we'll\nprove it next time. So the idea in this case\nis that we're just going to subtract something off.",
    "start": "3968350",
    "end": "3973692"
  },
  {
    "text": "And we're going to\nsubtract something off that only depends on the state. This only depends on the state.",
    "start": "3973693",
    "end": "3985180"
  },
  {
    "text": "OK, so this is not a\nfunction of your policy, only depends on the state. And it will turn out, and\nwe'll prove this next time--",
    "start": "3985180",
    "end": "3992200"
  },
  {
    "text": "it's pretty elegant-- that for\nany choice of something that only depends on your state,\nthe gradient estimator is still",
    "start": "3992200",
    "end": "3998560"
  },
  {
    "text": "unbiased. So you couldn't subtract\noff anything there. That is only a\nfunction of your state,",
    "start": "3998560",
    "end": "4004560"
  },
  {
    "text": "and you didn't change the\nbias of your estimator, which is wild. And we'll prove that next time.",
    "start": "4004560",
    "end": "4012375"
  },
  {
    "text": "But the goal is that\nwe can hopefully reduce the variance of\nour estimated gradient by subtracting off\nthe right thing.",
    "start": "4012375",
    "end": "4020210"
  },
  {
    "text": "And just intuitively, the way\nto think about the baseline is that you don't necessarily\njust care about whether or not",
    "start": "4020210",
    "end": "4026850"
  },
  {
    "text": "the gradient is\npositive or negative and whether the returns\nwere good or bad.",
    "start": "4026850",
    "end": "4032303"
  },
  {
    "text": "You might care about, well,\nhow much better or worse are these returns\ncompared to something else I could have done?",
    "start": "4032303",
    "end": "4037830"
  },
  {
    "text": "Like, I want to know whether\nthis policy A is better than policy B. And maybe both of\nthem give you positive returns.",
    "start": "4037830",
    "end": "4043210"
  },
  {
    "text": "One of them gives you 100\nand of them gives you 90, but you'd really like\nthe one with 100. So you'd really like to\nmove your policy parameters",
    "start": "4043210",
    "end": "4050369"
  },
  {
    "text": "in the direction\nof stuff that is better than other alternatives. And that's the\nidea of a baseline,",
    "start": "4050370",
    "end": "4055823"
  },
  {
    "text": "is to say like,\nwell, maybe I know that I could probably\nalways get like 90 for this particular state. How much better is this\npolicy for this state compared",
    "start": "4055823",
    "end": "4063180"
  },
  {
    "text": "to something I\ncould do on average? And so we're going to\nintuitively increase the log probability of an action\nproportionally to how much",
    "start": "4063180",
    "end": "4070710"
  },
  {
    "text": "its returns were better than\nexpected, where the baseline is giving you that expected value.",
    "start": "4070710",
    "end": "4077400"
  },
  {
    "text": "And we'll see\nformally on Wednesday how by doing this\nwith the baseline, it doesn't introduce any bias.",
    "start": "4077400",
    "end": "4083382"
  },
  {
    "text": "So it's going to\nbe one of the ways that we're going to\nget better gradients. The other thing that we're\ngoing to do on Wednesday",
    "start": "4083382",
    "end": "4089280"
  },
  {
    "text": "is we're at least going to\nstart talking about PPO, which is part of your homework\n2, bless you, which",
    "start": "4089280",
    "end": "4095940"
  },
  {
    "text": "is going to involve more ways to\nbe more efficient and effective in the policies that we do. I'll see you then.",
    "start": "4095940",
    "end": "4101528"
  },
  {
    "text": "Thanks ",
    "start": "4101529",
    "end": "4108000"
  }
]