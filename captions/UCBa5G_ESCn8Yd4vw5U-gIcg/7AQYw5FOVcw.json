[
  {
    "start": "0",
    "end": "330000"
  },
  {
    "text": "So I think I just spend\nlike five minutes, just briefly review on the\nbackpropagation last time.",
    "start": "5080",
    "end": "11130"
  },
  {
    "text": "I think I was running\nbehind last time. So I didn't have time to\nexplain this figure, which",
    "start": "11130",
    "end": "16299"
  },
  {
    "text": "I think probably would\nbe useful as a high level summary of what's happening. I'm going to omit\nall the details.",
    "start": "16299",
    "end": "22349"
  },
  {
    "text": "So I guess I'm\ndrawing in this way. Like this is the forward path. This is how you define\nnetwork and the loss function.",
    "start": "22350",
    "end": "29109"
  },
  {
    "text": "So you start with some example\nx, and then you have some-- I guess, this is a matrix\nvector multiplication or matrix",
    "start": "29109",
    "end": "36520"
  },
  {
    "text": "multiplication if you\nhave multiple examples. But this is a matrix vector\nmultiplication module,",
    "start": "36520",
    "end": "41940"
  },
  {
    "text": "and you take x inner product\nmultiplied with w and b.",
    "start": "41940",
    "end": "48190"
  },
  {
    "text": "And then get some activation,\nthe pre-activation. And you get some\npost activation. You take some matrix\nvector multiplication.",
    "start": "48190",
    "end": "55629"
  },
  {
    "text": "And you get-- I guess I'm using\nmatrix multiplication, but actually it's matrix\nvector multiplication.",
    "start": "55629",
    "end": "61739"
  },
  {
    "text": "And you get to activation. And then you do this. This Is.",
    "start": "61739",
    "end": "66760"
  },
  {
    "text": "How you define a loss,\nhow you define the model. I guess the output of\nthe model, I think, last time we used tau, which\nis the output of the model.",
    "start": "66760",
    "end": "74510"
  },
  {
    "text": "And then you have something\nthat defines the loss. This is the so-called\nforward path.",
    "start": "74511",
    "end": "82380"
  },
  {
    "text": "And in some sense, you can\nsummarize the backpropagation",
    "start": "82380",
    "end": "87548"
  },
  {
    "text": "in a way like follows. So basically, if\nI draw it, so this",
    "start": "87549",
    "end": "93270"
  },
  {
    "text": "is, of course,\nwhat you really do is you implement\nthis in computer. But if you draw\nit, in some sense,",
    "start": "93270",
    "end": "98939"
  },
  {
    "text": "you are doing it\nin a backward way. So what you do is you\nsay you first compute, if you look at the flow,\nthe data flow or the process",
    "start": "98939",
    "end": "107130"
  },
  {
    "text": "of the backprop process. So you compute the loss with\nrespect to the output first.",
    "start": "107130",
    "end": "116368"
  },
  {
    "text": "And this is often very easy. This is like just take the-- because the loss is\nsomething like y minus tau",
    "start": "116369",
    "end": "122118"
  },
  {
    "text": "squared, times a half. And this one is just\na very simple formula. And then you compute the\nderivative of the loss",
    "start": "122119",
    "end": "132660"
  },
  {
    "text": "with respect to-- here, I only have three layers. And then you compute\nthe derivative of loss",
    "start": "132660",
    "end": "139830"
  },
  {
    "text": "with respect to z2.",
    "start": "139830",
    "end": "145270"
  },
  {
    "text": "And then you compute\nthe derivative for loss with respect to a1.",
    "start": "145270",
    "end": "152209"
  },
  {
    "text": "And then something like this. This is the order of the\ncomputation you want.",
    "start": "152210",
    "end": "158760"
  },
  {
    "text": "It's kind of like\nyou are accessing this network in a backward\nfashion in some sense.",
    "start": "158760",
    "end": "165810"
  },
  {
    "text": "But how do you do this,\neach of this arrow? So this is by the lemma\nthat we discussed. So I think we have three\nlemmas or three abstractions.",
    "start": "165810",
    "end": "173920"
  },
  {
    "text": "So and each of\nthese arrow is using one of those three lemmas. And now you can see what\nthis does kind of like lemmas",
    "start": "173920",
    "end": "181480"
  },
  {
    "text": "are for. Those lemmas,\nbasically, are saying that if you know\ndg over the d tau,",
    "start": "181480",
    "end": "186819"
  },
  {
    "text": "how do you compute dg over da? And there's another\nlemma, which says that if you know how\nto compute dg over da,",
    "start": "186819",
    "end": "193180"
  },
  {
    "text": "how do you compute dg over dz? And all of those lemma is about\nthis kind of relationship.",
    "start": "193180",
    "end": "198489"
  },
  {
    "text": "If you know how to compute\nthe derivative with respect to the output of some module,\nsuppose this is a module,",
    "start": "198489",
    "end": "206030"
  },
  {
    "text": "tau is the output\nof this module. So if you know how to compute dj\nover the output of the module,",
    "start": "206030",
    "end": "213069"
  },
  {
    "text": "then you want to\nknow how to compute the derivative with respect\nto input of the module.",
    "start": "213069",
    "end": "219019"
  },
  {
    "text": "So all of the three lemmas\nare doing this list. I'm not going to the\ndetails because we don't have enough\ntime to review again,",
    "start": "219019",
    "end": "225280"
  },
  {
    "text": "but that's the basic idea. And also, there's\nanother thing, which is like this is only about\nthe derivative with respect",
    "start": "225280",
    "end": "232799"
  },
  {
    "text": "to activation. You can also compute a\nderivative with respect to the weights, right?",
    "start": "232799",
    "end": "237959"
  },
  {
    "text": "So if you know this\nquantity, then-- I think if you\nknow this quantity,",
    "start": "237959",
    "end": "243180"
  },
  {
    "text": "then how to compute the\nderivative with back to the last layer weight.",
    "start": "243180",
    "end": "249140"
  },
  {
    "text": "And if you know this quantity,\nthen from this quantity,",
    "start": "249140",
    "end": "254790"
  },
  {
    "text": "you know how to compute the\nderivative with respect to w2. And from this\nquantity, you know how",
    "start": "254790",
    "end": "261769"
  },
  {
    "text": "to complete a derivative\nwith respect to w1. And also the same thing for b's.",
    "start": "261769",
    "end": "277620"
  },
  {
    "text": "And this kind of-- the last row, this quantity,\nright, so they don't depend on, for example, after you get this,\nyou can convert this, right?",
    "start": "277620",
    "end": "284729"
  },
  {
    "text": "And after you've\ngot this quantity, you can come up with\nthese two quantities. But this row, the derivative\nwith respect to activations,",
    "start": "284729",
    "end": "292760"
  },
  {
    "text": "you can only do it sequentially. You can not say, you compute\nthis before you do this.",
    "start": "292760",
    "end": "298030"
  },
  {
    "text": "So this arrow is kind of the\norders of the dependencies between these quantities.",
    "start": "298030",
    "end": "304690"
  },
  {
    "text": "And each of these\narrow is basically done by one of the lemma\nthat we discussed last time.",
    "start": "304690",
    "end": "310139"
  },
  {
    "text": "Each of the lemma is kind\nof dealing with this. Any questions? This \nis just an extension",
    "start": "310140",
    "end": "320180"
  },
  {
    "text": "of the last five minutes of the previous lecture. I didn't have enough time\nto elaborate on this.",
    "start": "320180",
    "end": "334580"
  },
  {
    "start": "330000",
    "end": "640000"
  },
  {
    "text": "OK.",
    "start": "334580",
    "end": "342939"
  },
  {
    "text": "So good. So now in this lecture,\nand the lecture afterwards,",
    "start": "342940",
    "end": "348228"
  },
  {
    "text": "we are talking about, I\nguess, a few concepts. One concept is called\ngeneralization,",
    "start": "348229",
    "end": "357600"
  },
  {
    "text": "which is the main\npoint of this lecture. And also, next\nlecture, we're going to talk about the concept\nof regularization.",
    "start": "357600",
    "end": "364729"
  },
  {
    "text": "And next lecture we\nalso talk about some of the practical\nviewpoint of ML,",
    "start": "364730",
    "end": "371650"
  },
  {
    "text": "like how do you relate to your\nmodel, what you have to do in this whole process, right? So like you start\nwith the data process,",
    "start": "371650",
    "end": "378289"
  },
  {
    "text": "and then you have\nto tune the model, and then maybe you have to\ngo back to change your data, so and so forth.",
    "start": "378289",
    "end": "383919"
  },
  {
    "text": "So basically, in\nthese two lectures, I think we're going to\ndiscuss this concept.",
    "start": "383919",
    "end": "389599"
  },
  {
    "text": "I think that generalization\nis probably the main thing that we are talking about here.",
    "start": "389599",
    "end": "395020"
  },
  {
    "text": "So generalization, as you\ncan see, as you can guess, it's really just about how\nwell your model is performing,",
    "start": "395020",
    "end": "403060"
  },
  {
    "text": "and syntax examples. So we're going to\ndiscuss how do you make sure your model can also\ngeneralize to unseen test",
    "start": "403060",
    "end": "411220"
  },
  {
    "text": "examples. So far, we only\ntalk about training. So we have some examples,\nwhich we have seen when",
    "start": "411220",
    "end": "417000"
  },
  {
    "text": "they are training data sets. And we fit some model on them. So now, what we care about\nwhether this model will work",
    "start": "417000",
    "end": "424169"
  },
  {
    "text": "for future unseen examples. And we are going to discuss a\nbunch of concepts, the balance",
    "start": "424169",
    "end": "432630"
  },
  {
    "text": "variance trade-off, which is\na kind of a principle when you think about how\ntest error changes",
    "start": "432630",
    "end": "438190"
  },
  {
    "text": "as you change model complexity. And we are going to talk about\nsome of the new phenomena people have found in\ndeep learning, which",
    "start": "438190",
    "end": "444180"
  },
  {
    "text": "is a little bit different from\nthe classical understanding. OK. So I guess, that's just a\nvery high level overview.",
    "start": "444180",
    "end": "451138"
  },
  {
    "text": "I guess, I use a\nlot of buzzwords. I'm not expecting everyone\nto follow everything. So let me maybe be concrete.",
    "start": "451139",
    "end": "459490"
  },
  {
    "text": "OK. So I guess, let me start with\nsome kind of basic notations and notions.",
    "start": "459490",
    "end": "465490"
  },
  {
    "text": "So I guess, some basic\nnotions, one thing is this so-called training\nloss, which you probably",
    "start": "465490",
    "end": "471659"
  },
  {
    "text": "already know what it means. Training loss or sometimes,\nit's called training error. Sometimes, it's\ncalled training cost.",
    "start": "471659",
    "end": "477419"
  },
  {
    "text": "I think, in this\nlecture, sometimes, we use the word cost. So they all mean the\nsimilar type of concepts.",
    "start": "477419",
    "end": "483569"
  },
  {
    "text": "Sometimes, people\nuse loss to refer certain kind of losses and error\nto refers to certain other type",
    "start": "483569",
    "end": "490030"
  },
  {
    "text": "of losses. But from the purpose\nof this lecture, they all mean same thing.",
    "start": "490030",
    "end": "495810"
  },
  {
    "text": "This is what you care\nabout in a training. For example, if you care\nabout the square loss,",
    "start": "495810",
    "end": "500850"
  },
  {
    "text": "then the training loss\nwould just be this. I think we have\nto write down-- we have written down this\nequation a lot of times.",
    "start": "500850",
    "end": "507410"
  },
  {
    "text": "This is the loss function\nyou care about when you have square loss.",
    "start": "507410",
    "end": "516140"
  },
  {
    "text": "And other loss could\nbe cross-entropy loss. It could be like MLE, the\nmaximum likelihood estimator.",
    "start": "516140",
    "end": "524590"
  },
  {
    "text": "I think that's\nactually one principle to derive the training loss. You derive the maximum\nlikelihood estimator",
    "start": "524590",
    "end": "531209"
  },
  {
    "text": "for data sets. And that you use that\nas your training loss. You use the active\nlog-likelihood",
    "start": "531209",
    "end": "536370"
  },
  {
    "text": "as the training loss. So this is basically, so\nfar, what we have focused on in the last few weeks.",
    "start": "536370",
    "end": "541490"
  },
  {
    "text": "So how do you get\na training loss, and how do you really implement\nthis, and optimize this, right?",
    "start": "541490",
    "end": "547100"
  },
  {
    "text": "So there are many\nways to optimize it. For example, in one\nof the lectures, we use the analytical formulas. So we have the GDA.",
    "start": "547100",
    "end": "554290"
  },
  {
    "text": "We analytically compute what is\nthe minimum loss, the minimizer",
    "start": "554290",
    "end": "560850"
  },
  {
    "text": "of the negative log-likelihood. And all of the\nother lectures, we are using numerical algorithm\nto minimize this loss.",
    "start": "560850",
    "end": "568949"
  },
  {
    "text": "So like for example,\nlike in deep learning, we are using stochastic\ngradient descent.",
    "start": "568949",
    "end": "574339"
  },
  {
    "text": "And then we have talked\nabout Newton's method, so and so forth. But so far, everything\nwe have talked about is this loss\nfunction when we try",
    "start": "574339",
    "end": "579910"
  },
  {
    "text": "to find the minimizer\nof this loss function. Not exactly this\none, but like other--",
    "start": "579910",
    "end": "586600"
  },
  {
    "text": "but it's always a\nloss function defined on the training examples. OK.",
    "start": "586600",
    "end": "591610"
  },
  {
    "text": "So now, suppose\nyou have obtained-- so suppose we have\nsome parameter theta. So suppose we have\nobtained some theta,",
    "start": "591610",
    "end": "602519"
  },
  {
    "text": "how do you evaluate whether\nyour theta is good or not? So ideally, you want the\nmodel to not only perform",
    "start": "602520",
    "end": "607779"
  },
  {
    "text": "well on the training data\nbecause for the training data, you already know the prediction. Why you care about\nletting the model to predict something\nyou already know?",
    "start": "607779",
    "end": "615910"
  },
  {
    "text": "So what you really\ncare about is you care about you want to\nevaluate on unseen examples.",
    "start": "615910",
    "end": "627130"
  },
  {
    "text": "So that's why the test loss\nis defined on unseen examples. And I'm going to\nuse this notion.",
    "start": "627130",
    "end": "634120"
  },
  {
    "text": "So suppose, say, you draw-- the process is that you draw\nsome new example, x comma y,",
    "start": "634120",
    "end": "645670"
  },
  {
    "start": "640000",
    "end": "1050000"
  },
  {
    "text": "from some distribution\nD. And often, this is called test distribution.",
    "start": "645670",
    "end": "654910"
  },
  {
    "text": "And then you evaluate what's the\nexpected loss on this new test",
    "start": "654910",
    "end": "662410"
  },
  {
    "text": "example. So you look at l theta, which\nis the expected loss of--",
    "start": "662410",
    "end": "673500"
  },
  {
    "text": "the expectation is\nover the randomness of this new example drawn\nfrom this test distribution.",
    "start": "673500",
    "end": "681880"
  },
  {
    "text": "So what's important is\nthat this x and y is not seen in a training. It's a new fresh example.",
    "start": "681880",
    "end": "688790"
  },
  {
    "text": "And of course, here, I'm\ndefining it as expectation. So actually, in place,\nI'm taking average",
    "start": "688790",
    "end": "694200"
  },
  {
    "text": "over the entire distribution. So if you really want\nto do it empirically,",
    "start": "694200",
    "end": "699350"
  },
  {
    "text": "what it really means is that\nyou draw a bunch of examples. Maybe let's call\nit x1 test, test 1.",
    "start": "699350",
    "end": "712550"
  },
  {
    "text": "You draw maybe n of this. These are not the examples\nyou have used for training.",
    "start": "712550",
    "end": "720440"
  },
  {
    "text": "These are new examples you\ndraw during the test time. You draw them from d, iid\nfrom this distribution d.",
    "start": "720440",
    "end": "729040"
  },
  {
    "text": "And then you evaluate\nthe error on this set. And you evaluate average error\nor average loss on this set,",
    "start": "729040",
    "end": "742470"
  },
  {
    "text": "on the test set. Because you know that if you\nevaluate on this test test,",
    "start": "742470",
    "end": "750360"
  },
  {
    "text": "it's pretty much just\napproximating this expectation. You are just using\nan empirical way to estimate the expected value.",
    "start": "750360",
    "end": "758329"
  },
  {
    "text": "It's an estimate,\nand the expectation",
    "start": "758329",
    "end": "764810"
  },
  {
    "text": "of any random variable. One way to do it is you\njust draw multiple copies from the same distribution, and\nyou take the empirical average.",
    "start": "764810",
    "end": "772130"
  },
  {
    "text": "That's why the test set\nis a reasonable estimate for the test error.",
    "start": "772130",
    "end": "780220"
  },
  {
    "text": "And just to be clear,\nthese test examples, you haven't seen them\nin a training set. They are something you draw.",
    "start": "780220",
    "end": "788079"
  },
  {
    "text": "You can draw them in advance,\nbut you cannot let them to be seen in the training process.",
    "start": "788080",
    "end": "799880"
  },
  {
    "text": "And there is a notion\ncalled generalization gap.",
    "start": "799880",
    "end": "808519"
  },
  {
    "text": "So I guess, this notion,\noften people called-- this is basically talking about\nthe difference between the test",
    "start": "808519",
    "end": "816730"
  },
  {
    "text": "loss and the training loss. And oftentimes, it's\nnot always true. But oftentimes,\nthe training loss",
    "start": "816730",
    "end": "822240"
  },
  {
    "text": "is less than the test loss. When you test, you find that\nyour model is not as good as you thought before\non the training set.",
    "start": "822240",
    "end": "828700"
  },
  {
    "text": "Sometimes, it's\nprobably a little worse. Sometimes, it's a lot worse. Sometimes, they\nare very similar.",
    "start": "828700",
    "end": "834430"
  },
  {
    "text": "But generally, you\nshouldn't expect that your test performance\nis dramatically better than the training performance.",
    "start": "834430",
    "end": "839980"
  },
  {
    "text": "And of course, in\nextreme cases, you can design the set,\nsuch that this happens.",
    "start": "839980",
    "end": "845500"
  },
  {
    "text": "But I think in realistic\npractical situations, I don't think you should\nexpect that at all.",
    "start": "845500",
    "end": "851490"
  },
  {
    "text": "So it's often the case that this\ngap is either very close to 0, or maybe a slightly\nnegative, slightly positive,",
    "start": "851490",
    "end": "858459"
  },
  {
    "text": "or is much bigger than 0. So you want this gap to\nbe as small as possible.",
    "start": "858460",
    "end": "863949"
  },
  {
    "text": "So basically, in some sense,\nyou care about two quantities. You care about the training\nloss and you care about the gap.",
    "start": "863949",
    "end": "871150"
  },
  {
    "text": "You want both of\nthese two to be small. If both of these to be small--\nor if both of them are small, then the sum of\nthem will be small.",
    "start": "871150",
    "end": "877670"
  },
  {
    "text": "And that's why your\ntest loss is small. That's the hope. Your hope is this both\nof these two are small.",
    "start": "877670",
    "end": "888350"
  },
  {
    "text": "OK.",
    "start": "888350",
    "end": "893470"
  },
  {
    "text": "So this one is something you\ncan control in some sense. This is what you try\nto optimize for, right? But this one is harder to\ncontrol because you don't--",
    "start": "893470",
    "end": "901950"
  },
  {
    "text": "because you cannot say,\nI'm going to find a theta, such as l theta is small. Because if you do\nthat, empirically, you",
    "start": "901950",
    "end": "908100"
  },
  {
    "text": "try to optimize theta such\nthat the test loss is small, then you have to see\nthe test data set.",
    "start": "908100",
    "end": "917690"
  },
  {
    "text": "So that's why you cannot really\neasily control this because you are not allowed to test the set.",
    "start": "917690",
    "end": "923070"
  },
  {
    "text": "Like you cannot choose your\ntheta based on the loss. You can only choose theta first,\nand then you evaluate loss,",
    "start": "923070",
    "end": "928600"
  },
  {
    "text": "but not vice versa. So that's why the\ngeneralization gap is something that is very hard to control.",
    "start": "928600",
    "end": "934800"
  },
  {
    "text": "At least, you cannot\ndirectly control it. And the point of this\nlecture is to discuss",
    "start": "934800",
    "end": "940079"
  },
  {
    "text": "in what cases you can somewhat\nknow this is not too big. Like when you can hope\nthat this is not too big?",
    "start": "940079",
    "end": "950040"
  },
  {
    "text": "OK. And then before going\nto do more details,",
    "start": "950040",
    "end": "955269"
  },
  {
    "text": "let me also define two\nnotation-- two kind of like commonly used terminology.",
    "start": "955269",
    "end": "961040"
  },
  {
    "text": "So of course, we are\ndealing with the case when l theta-- so we are\nmostly concerned about the case",
    "start": "961040",
    "end": "966250"
  },
  {
    "text": "when l theta is too big. So if l theta is\nsmall, that's great. You don't have to\nworry about anything.",
    "start": "966250",
    "end": "971589"
  },
  {
    "text": "So when l theta is\nbig, the question is, what do we do to change it?",
    "start": "971589",
    "end": "976899"
  },
  {
    "text": "Like if you observe\nthat your test loss is very big, then what\nyou can do to make it smaller?",
    "start": "976899",
    "end": "982410"
  },
  {
    "text": "That's the question\nyou want to study. And typically, when\nl theta is big, there are two failure\nmode in some sense.",
    "start": "982410",
    "end": "992589"
  },
  {
    "text": "These are not supposed to be-- these are not supposed\nto be comprehensive.",
    "start": "992589",
    "end": "998639"
  },
  {
    "text": "But I think, typically, you\nare in either one of these two failure mode.",
    "start": "998639",
    "end": "1003670"
  },
  {
    "text": "So one of the failure mode\nis called failure patterns. So one of the failure mode\nis called overfitting.",
    "start": "1003670",
    "end": "1013880"
  },
  {
    "text": "And so overfitting,\nI'm going to discuss a lot about overfitting.",
    "start": "1013880",
    "end": "1018889"
  },
  {
    "text": "But the first other bit is\nthat the typical situation of overfitting is that the\ntraining loss, j, is small,",
    "start": "1018889",
    "end": "1026530"
  },
  {
    "text": "but the test loss is big. So you have this big\ngeneralization gap.",
    "start": "1026530",
    "end": "1032829"
  },
  {
    "text": "So you have a discrepancy\nbetween training and test. At least, that's not a\ndefinition for overfitting,",
    "start": "1032829",
    "end": "1039620"
  },
  {
    "text": "but that's a very typical\ncharacteristic of overfitting.",
    "start": "1039620",
    "end": "1048010"
  },
  {
    "text": "So for example, I guess, I'll\nprobably draw this very often-- I'll draw this figure very\noften in this lecture.",
    "start": "1048010",
    "end": "1055670"
  },
  {
    "start": "1050000",
    "end": "1320000"
  },
  {
    "text": "So suppose you have\nsome x and some y. You have some data set. I guess, the one\nexample I'm going to do",
    "start": "1055670",
    "end": "1063171"
  },
  {
    "text": "is that I'm going\nto have some data set that lives very close\nto this quadratic function.",
    "start": "1063171",
    "end": "1072040"
  },
  {
    "text": "The data are\napproximately quadratic. So x and y. So it has a\none-dimensional problem. So given x, you\nwant to predict y.",
    "start": "1072040",
    "end": "1078370"
  },
  {
    "text": "And you observe some--",
    "start": "1078370",
    "end": "1083390"
  },
  {
    "text": "so you have a data set. For example, you\nhave four points. So each point is like\nthis, maybe this,",
    "start": "1083390",
    "end": "1088960"
  },
  {
    "text": "and something like this,\nmaybe something like this. You see these four\nblue points, and you",
    "start": "1088960",
    "end": "1094039"
  },
  {
    "text": "want to fit a line to it\nor fit some curve to it. And the question is what\ncurve you are going to fit?",
    "start": "1094039",
    "end": "1101270"
  },
  {
    "text": "So suppose, you fit\nsomething crazy like this. Let me try to see what\ncolor I'm using for this.",
    "start": "1101270",
    "end": "1110770"
  },
  {
    "text": "Sorry. One moment. Let me think about how do I use\nthe color in a consistent way.",
    "start": "1110770",
    "end": "1123410"
  },
  {
    "text": "So I guess if you fit-- I'm going to use black\nfor the model you fit. So suppose your model you\nfit is something like this.",
    "start": "1123410",
    "end": "1133539"
  },
  {
    "text": "I'm drawing something crazy. So this model is--",
    "start": "1133539",
    "end": "1141890"
  },
  {
    "text": "I intend to make this model\nto pass these points, exactly. So this model face the data,\nthe forward training data",
    "start": "1141890",
    "end": "1149480"
  },
  {
    "text": "perfectly. So the j theta is really small. It's kind of close to 0.",
    "start": "1149480",
    "end": "1156080"
  },
  {
    "text": "But you can imagine this\nmodel shouldn't generalize to anything examples.",
    "start": "1156080",
    "end": "1161140"
  },
  {
    "text": "So suppose you-- if you\ngenerate some examples,",
    "start": "1161140",
    "end": "1168070"
  },
  {
    "text": "and you kind of believe\nthat, and the examples also are kind of similar, like have\na quadratic relationship, you",
    "start": "1168070",
    "end": "1174930"
  },
  {
    "text": "generate something like\nthis, maybe somewhere here, maybe some are here. Then you can see that the\nfitting to the right point",
    "start": "1174930",
    "end": "1181970"
  },
  {
    "text": "becomes very worse. So much worse--\nbecomes much worse. So the test loss is very big.",
    "start": "1181970",
    "end": "1188399"
  },
  {
    "text": "So this is a typical\nsituation of overfitting. In some sense, you are saying\nthat you fit the data very",
    "start": "1188400",
    "end": "1194580"
  },
  {
    "text": "well, but you are\noverfitting in the sense that you only focus\non the training data,",
    "start": "1194580",
    "end": "1201410"
  },
  {
    "text": "but you kind of like forgot\nabout the test performance. I will discuss why\nthis will happen.",
    "start": "1201410",
    "end": "1207760"
  },
  {
    "text": "I guess, you can probably\nguess, but this is so far, I'm just defining roughly\nwhat overfitting means.",
    "start": "1207760",
    "end": "1215259"
  },
  {
    "text": "So it means that you are not-- you fit the training data,\nbut you don't generalize.",
    "start": "1215260",
    "end": "1220860"
  },
  {
    "text": "And another notion is\ncalled underfitting. An underfitting,\nbasically, just means",
    "start": "1220860",
    "end": "1232580"
  },
  {
    "text": "that you face\nsomething like this. Maybe let's say you fit this. Suppose this is\nanother model you fit.",
    "start": "1232580",
    "end": "1239900"
  },
  {
    "text": "So underfitting this means that\nboth the j theta is also big.",
    "start": "1239900",
    "end": "1247929"
  },
  {
    "text": "So even your model doesn't even\ndo well on the training set. And that is basically\nmeans underfitting.",
    "start": "1247929",
    "end": "1255630"
  },
  {
    "text": "So as the word suggests, that\nyou are not fitting the data.",
    "start": "1255630",
    "end": "1264000"
  },
  {
    "text": "And whether you are\nin the overfitting regime or the underfitting\nregime or in a nicer regime, depends a lot on\ndifferent things.",
    "start": "1264000",
    "end": "1270879"
  },
  {
    "text": "And one kind of decision we\nare trying to discuss today is that what is the\nright model complexity.",
    "start": "1270880",
    "end": "1282470"
  },
  {
    "text": "So like what are\nwe going to use? Linear model, maybe\nuse quadratic, or maybe",
    "start": "1282470",
    "end": "1289121"
  },
  {
    "text": "fifth degree polynomial,\nor neural network, so on and so forth.",
    "start": "1289121",
    "end": "1296899"
  },
  {
    "text": "So we're going to discuss\nwhat will happen if you change your model complexity,\nand whether in what cases,",
    "start": "1296900",
    "end": "1304090"
  },
  {
    "text": "you may underfit, in what\ncases you may overfit, and what is the best response.",
    "start": "1304090",
    "end": "1312559"
  },
  {
    "text": "Any questions so far?",
    "start": "1312560",
    "end": "1319270"
  },
  {
    "text": "And kind of like as a\nspoiler, in some sense, like we're going\nto discuss two--",
    "start": "1319270",
    "end": "1325400"
  },
  {
    "text": "we are going to decompose\nthe test error, l theta. The test error is the\ntest loss, and l theta.",
    "start": "1325400",
    "end": "1333640"
  },
  {
    "text": "We're going to decompose\nthis into two terms. Actually, I'm not going to\nshow it, mathematically,",
    "start": "1333640",
    "end": "1339890"
  },
  {
    "text": "because I don't think I\nhave enough time to do that. But intuitively, you are\ngoing to decompose the test into two terms, which is\ncalled-- one is called bias.",
    "start": "1339890",
    "end": "1348350"
  },
  {
    "text": "And technically,\nit's bias squared because the bias is defined as\nthe square root of this term.",
    "start": "1348350",
    "end": "1353559"
  },
  {
    "text": "So plus variance,\nSo you're going to have this you're going\nto define these two terms",
    "start": "1353560",
    "end": "1359169"
  },
  {
    "text": "and say that these two terms,\nif you take the sum of them, it will be the test error.",
    "start": "1359170",
    "end": "1364380"
  },
  {
    "text": "And these two terms\nhas this property that the bias is going to\nbe an increasing function.",
    "start": "1364380",
    "end": "1374740"
  },
  {
    "text": "So we are going to see\nsomething like this. The bias is going to be\na decreasing function as the model complexity.",
    "start": "1374740",
    "end": "1382860"
  },
  {
    "text": "I haven't told you what the\nbias is, what the variance is. I'm just giving you\nkind of like a spoiler",
    "start": "1382860",
    "end": "1392059"
  },
  {
    "text": "on what kind of things\nwe're going to discuss. So the bias is\nsomething like this. And the variance is\nsomething like this.",
    "start": "1392059",
    "end": "1400778"
  },
  {
    "text": "So these are-- basically,\nyou are kind of like trying to figure\nout the underlying kind of like mechanisms.",
    "start": "1400779",
    "end": "1406980"
  },
  {
    "text": "So the mechanism is that if\nyou change the model complexity to make it more complex, then\nyour variance will be bigger",
    "start": "1406980",
    "end": "1412860"
  },
  {
    "text": "and bias will be smaller. And your sum of\nthese two functions, which is a test error, will\nbe something like this.",
    "start": "1412860",
    "end": "1420890"
  },
  {
    "text": "And then the best one will\nbe something in the middle. So this is kind of\nthe quick overview",
    "start": "1420890",
    "end": "1428990"
  },
  {
    "text": "of what we're going to discuss. All right. OK.",
    "start": "1428990",
    "end": "1434190"
  },
  {
    "text": "So now, I'm going to define bias\nand variance in a little bit",
    "start": "1434190",
    "end": "1441039"
  },
  {
    "text": "more formal ways. Still not very formal,\nlike I'm going to start",
    "start": "1441040",
    "end": "1447559"
  },
  {
    "text": "with it's a gradual process. I'm going to have a little more\nformal definition of the bias.",
    "start": "1447559",
    "end": "1453980"
  },
  {
    "text": "OK. And I'll show some examples. So any questions so far?",
    "start": "1453980",
    "end": "1461250"
  },
  {
    "text": "Why is the bias [INAUDIBLE]. Why is bias this crazy? Oh, squared, I mean.",
    "start": "1461250",
    "end": "1467059"
  },
  {
    "text": "Oh, this is just-- maybe I should draw. This is just because it's\nkind of a unit thing. You define a bias to be the--",
    "start": "1467060",
    "end": "1475029"
  },
  {
    "text": "it's just a-- how do I say this? It's a definition.",
    "start": "1475029",
    "end": "1480940"
  },
  {
    "text": "Actually, some people call the\nbias square bias, actually, in some literature.",
    "start": "1480940",
    "end": "1486260"
  },
  {
    "text": "Sometimes, people\ntake a square root. It's just how do you\nchoose the right unit. Yeah. And when I say\nbias, I don't really",
    "start": "1486260",
    "end": "1494870"
  },
  {
    "text": "distinguish whether\nit's squared or not. OK. So I guess, what\nI'm going to do is",
    "start": "1494870",
    "end": "1500799"
  },
  {
    "start": "1500000",
    "end": "1920000"
  },
  {
    "text": "I'm going to have\na running example, which is basically like this. And I'm going to\nkind of try what",
    "start": "1500799",
    "end": "1508981"
  },
  {
    "text": "happens with linear,\nfifth-degree polynomial.",
    "start": "1508981",
    "end": "1514330"
  },
  {
    "text": "And kind of use this\nkind of as a thought experiment to demonstrate\nthese quantities.",
    "start": "1514330",
    "end": "1519870"
  },
  {
    "text": "So let's start with linear. And sometimes, this is\na thought experiment,",
    "start": "1519870",
    "end": "1526898"
  },
  {
    "text": "but actually, we have some real\ndata experiments in the lecture notes. Here, I'm just drawing this.",
    "start": "1526899",
    "end": "1534120"
  },
  {
    "text": "But I think it's\npretty much the same. So suppose you-- OK. Maybe I'll set up\njust really quick.",
    "start": "1534120",
    "end": "1541549"
  },
  {
    "text": "So my running\nexample is basically like what I drew above.",
    "start": "1541549",
    "end": "1548380"
  },
  {
    "text": "So I'm going to have\nsome training examples. And these training examples\nare something like yi",
    "start": "1548380",
    "end": "1557679"
  },
  {
    "text": "is equals close to a\nquadratic function. Quadratic, which is\njust this quadratic,",
    "start": "1557679",
    "end": "1568210"
  },
  {
    "text": "imagine, of xi, plus\na little bit noise. This is a small noise.",
    "start": "1568210",
    "end": "1575129"
  },
  {
    "text": "So that's why these\nblue points are not exactly lies on the quadratic. It's just there's a\nlittle bit fluctuation.",
    "start": "1575130",
    "end": "1582320"
  },
  {
    "text": "And sometimes, I think,\nI guess this quadratic. Sometimes, they are\ncalled this h star xi.",
    "start": "1582320",
    "end": "1588799"
  },
  {
    "text": "Just for the sake of\nterminology, I think, sometimes, they call\nthis the ground truth.",
    "start": "1588799",
    "end": "1598110"
  },
  {
    "text": "And sometimes, the true function\nyou are trying to find out. But of course,\nyou don't know it.",
    "start": "1598110",
    "end": "1604000"
  },
  {
    "text": "You want to try to recover it. And I'm going to do a\nthought experiment first. I'm going to do a\nfew experiments.",
    "start": "1604000",
    "end": "1609539"
  },
  {
    "text": "I'm going to start\nwith linear model, and then I'm going to try\nfifth-degree polynomial, and then I'm going\nto try quadratic.",
    "start": "1609540",
    "end": "1617480"
  },
  {
    "text": "So linear model. Suppose you have a linear model. I guess, you can probably\nsee, guys, what will happen.",
    "start": "1617480",
    "end": "1628399"
  },
  {
    "text": "So I would draw this again. So you have these four data\npoints, something like this.",
    "start": "1628399",
    "end": "1640940"
  },
  {
    "text": "Then what happens\nwith linear model is that you have these\nfour points, what's the best linear fit?",
    "start": "1640940",
    "end": "1646920"
  },
  {
    "text": "Probably would be\nsomething like maybe this for this particular data set.",
    "start": "1646920",
    "end": "1658518"
  },
  {
    "text": "And you can see what\nare happening here. So maybe-- let me\nsee-- how do I--",
    "start": "1658519",
    "end": "1670260"
  },
  {
    "text": "maybe let me erase\nthis for a moment.",
    "start": "1670260",
    "end": "1677220"
  },
  {
    "text": "I'm going to redraw this again. So for linear\nmodels, I guess, you",
    "start": "1677220",
    "end": "1683220"
  },
  {
    "text": "can see a bunch of properties. So you can see that\nthere's a large training error, training loss or\ntraining-- let's call",
    "start": "1683220",
    "end": "1690350"
  },
  {
    "text": "it loss just for consistency. There's a large training\nloss because I guess--",
    "start": "1690350",
    "end": "1704440"
  },
  {
    "text": "what's your prediction\non the training data set. This is your\nprediction for this x. So this is x1, and the\nprediction is here.",
    "start": "1704440",
    "end": "1713519"
  },
  {
    "text": "And the prediction\nfor x2 is here. The prediction of x3 is here. The prediction for x4 is here. And you look at the distance\nbetween the prediction",
    "start": "1713519",
    "end": "1721390"
  },
  {
    "text": "and the true label. You see that the\ndistance is pretty big.",
    "start": "1721390",
    "end": "1726408"
  },
  {
    "text": "The training error\nis pretty big. So this is underfitting,\nby our definition",
    "start": "1726409",
    "end": "1734789"
  },
  {
    "text": "of underfitting because\nthe tuning is already big. And now let's think about\nso what you should blame.",
    "start": "1734789",
    "end": "1745429"
  },
  {
    "text": "Why the training is big? What's the culprit? The culprit, I would\nargue, is that it's",
    "start": "1745429",
    "end": "1752740"
  },
  {
    "text": "just because no any linear\nmodel can fit your data. It's not just-- no any\nlinear model can work.",
    "start": "1752740",
    "end": "1765250"
  },
  {
    "text": "And it's not because you\ndon't even have enough data, it's just because even\nyou have more data,",
    "start": "1765250",
    "end": "1770590"
  },
  {
    "text": "a linear model\nwouldn't work as well. So this is just because\nthe linear model is not",
    "start": "1770590",
    "end": "1776870"
  },
  {
    "text": "expressive enough.",
    "start": "1776870",
    "end": "1785760"
  },
  {
    "text": "And this is called bias. So this is called bias. So when in this kind of\nsettings things happens,",
    "start": "1785760",
    "end": "1794919"
  },
  {
    "text": "like you have the bias. So the bias is\nbasically like it's",
    "start": "1794919",
    "end": "1800880"
  },
  {
    "text": "saying that the reason why-- I don't know exactly\nwhy people call it bias in the very first time.",
    "start": "1800880",
    "end": "1806250"
  },
  {
    "text": "But I think you can-- see kind of the relationship. The thing is that you are\nimposing additional structure.",
    "start": "1806250",
    "end": "1813169"
  },
  {
    "text": "So you are imposing\na linear structure, but the true data is not linear. So it doesn't\nmatter how many data you see, as long\nyou impose this,",
    "start": "1813170",
    "end": "1820412"
  },
  {
    "text": "you just insist that I just\nbelieve that this thing is linear, you're going to fail.",
    "start": "1820412",
    "end": "1826000"
  },
  {
    "text": "Because this is the wrong\nbelief about the relationship between y and x. So that's why this\nis called bias.",
    "start": "1826000",
    "end": "1832470"
  },
  {
    "text": "And you cannot mitigate-- cannot be mitigated by\nmore data, as I said.",
    "start": "1832470",
    "end": "1846010"
  },
  {
    "text": "And actually, it can also not\nbe mitigated by less noise, even though there is--",
    "start": "1846010",
    "end": "1851148"
  },
  {
    "text": "and by less noise data.",
    "start": "1851149",
    "end": "1857220"
  },
  {
    "text": "Because even you have more\ndata and with less noise,",
    "start": "1857220",
    "end": "1862360"
  },
  {
    "text": "you can imagine what happens. So suppose, you see\na little more data.",
    "start": "1862360",
    "end": "1868510"
  },
  {
    "text": "Suppose you see some more\ndata as training data.",
    "start": "1868510",
    "end": "1878139"
  },
  {
    "text": "And maybe let's say, you\njust-- suppose in extreme case, you just see everything\nexactly on this quadratic line",
    "start": "1878139",
    "end": "1885028"
  },
  {
    "text": "without any noise,\nstill, if you think about what's the best fit. For example, let's\nsay just see all",
    "start": "1885029",
    "end": "1890470"
  },
  {
    "text": "of these blue and green points. And what's the best fit? The best fit probably\nwould change a little bit.",
    "start": "1890470",
    "end": "1896130"
  },
  {
    "text": "That's true. It probably wouldn't\nbe exactly this. Maybe it would be-- I guess, it would be\nsomething like this.",
    "start": "1896130",
    "end": "1901750"
  },
  {
    "text": "Maybe something like this. I don't know. You have to trade off here. Because whatever you\nfit, if you fit this,",
    "start": "1901750",
    "end": "1907620"
  },
  {
    "text": "then you don't fit\nsome of these examples. If you do-- there's\nno any option. Like whatever, it just because\nthe model cannot represent",
    "start": "1907620",
    "end": "1916580"
  },
  {
    "text": "quadratic function. That's it. So that's the typical situation,\nwhere you have a large bias.",
    "start": "1916580",
    "end": "1925990"
  },
  {
    "start": "1920000",
    "end": "2085000"
  },
  {
    "text": "And mathematically, so\nthe way you define bias, so here, I'm just only talking\nabout some characteristics",
    "start": "1925990",
    "end": "1932250"
  },
  {
    "text": "of having a large bias. So mathematically, one\nway to define a bias is that you can\nsay this is the--",
    "start": "1932250",
    "end": "1939510"
  },
  {
    "text": "So bias is-- I guess, actually, there's\nsome approximation here,",
    "start": "1939510",
    "end": "1946010"
  },
  {
    "text": "depending on what\nexactly your model is. But roughly speaking, it's\nthe best error or loss,",
    "start": "1946010",
    "end": "1959549"
  },
  {
    "text": "you can get with\neven infinite data. So I guess, suppose\nyou have infinite data,",
    "start": "1959549",
    "end": "1970020"
  },
  {
    "text": "you have a data set\nwith infinite data, following the same\nkind of property, so like all generated from\nthis quadratic noise, then",
    "start": "1970020",
    "end": "1976600"
  },
  {
    "text": "what's the best you can do? And that's called bias. And you can kind of see\nthat it's probably important",
    "start": "1976600",
    "end": "1983169"
  },
  {
    "text": "for bias to be small\nbecause if bias is large, even with infinite data,\nyou cannot do anything.",
    "start": "1983169",
    "end": "1989750"
  },
  {
    "text": "And that's the problem\nwith linear models. Any you question?",
    "start": "1989750",
    "end": "1996070"
  },
  {
    "text": "Can bias because something\nlike the distance",
    "start": "1996070",
    "end": "2003490"
  },
  {
    "text": "from the [INAUDIBLE] model\nor something like that? I think that's pretty much--",
    "start": "2003490",
    "end": "2009919"
  },
  {
    "text": "so for this case, they\npretty much are the same. So basically, so\nin this case, it's",
    "start": "2009919",
    "end": "2016600"
  },
  {
    "text": "exactly true that the bias\nis the best linear model. So the closest-- like\nthe closest linear--",
    "start": "2016600",
    "end": "2024679"
  },
  {
    "text": "the model that is closest-- the linear model that is\nclosest to the ground truth.",
    "start": "2024679",
    "end": "2030330"
  },
  {
    "text": "And that error, that\ncloseness, is the bias.",
    "start": "2030330",
    "end": "2035419"
  },
  {
    "text": "Because when you\ngenerate infinite data, basically you just generate the\nground truth, the whole line, if you have no noise.",
    "start": "2035419",
    "end": "2040580"
  },
  {
    "text": "And by that, you mean,\nlike I don't know. Within the same model. You're not changing\nthe model, right? You are not changing\nthe model path.",
    "start": "2040580",
    "end": "2047700"
  },
  {
    "text": "You're only using linear. Like you cannot-- OK. So bias would be\nthe best [INAUDIBLE]",
    "start": "2047700",
    "end": "2053608"
  },
  {
    "text": "linear model [INAUDIBLE]. Exactly. So in some sense,\ntechnically is you say bias is property of the\nfamily of models, right?",
    "start": "2053609",
    "end": "2063299"
  },
  {
    "text": "So the linear model family\nhas a large bias, right?",
    "start": "2063299",
    "end": "2071179"
  },
  {
    "text": "Yeah. We are always talking\nabout model family. So we are talking about either\nlinear model family, the family",
    "start": "2071180",
    "end": "2076570"
  },
  {
    "text": "of linear models or the family\nof fifth-degree polynomial or the family of quadratics. OK.",
    "start": "2076570",
    "end": "2082358"
  },
  {
    "text": "Cool. So this is the bias. And now let me talk\nabout the variance.",
    "start": "2082359",
    "end": "2089940"
  },
  {
    "text": "And here, there is-- I'll come back to the\nvariance for this model. But here, the variance is,\nin some sense, you can say,",
    "start": "2089940",
    "end": "2096240"
  },
  {
    "text": "it's not very important. Only the bias is the culprit. And now, I'm going to show\ncases where the variance is",
    "start": "2096240",
    "end": "2102920"
  },
  {
    "text": "the culprit to blame for. So I guess, I'm\ngoing to redraw this.",
    "start": "2102920",
    "end": "2112310"
  },
  {
    "text": "So you have-- [INAUDIBLE] four points.",
    "start": "2112310",
    "end": "2120859"
  },
  {
    "text": "So now, I'm going to fit\na fifth-degree polynomial.",
    "start": "2120859",
    "end": "2142309"
  },
  {
    "text": "So the model is\nsomething like h theta x is some theta 5x to\nthe 5 plus up to theta 0.",
    "start": "2142310",
    "end": "2151369"
  },
  {
    "text": "But recall that we can do\nthis with linear regression because you just-- this is still\nlinear in the theta.",
    "start": "2151369",
    "end": "2158250"
  },
  {
    "text": "We have a homework\nquestion on this. We also talk about how to\ndo this with kernel methods if you care about efficiency,\nso on and so forth, right?",
    "start": "2158250",
    "end": "2165680"
  },
  {
    "text": "So we are able to fit this. And in the lecture\nnotes, actually, there are some visualizations\nof the real models",
    "start": "2165680",
    "end": "2172170"
  },
  {
    "text": "you're going to fit. So here, I'm just\ngoing to draw it. So if you fit the\nfifth-degree polynomial,",
    "start": "2172170",
    "end": "2178200"
  },
  {
    "text": "so probably, you're going to\nget-- a fifth-degree polynomial can go up and down so\nmany times, several times.",
    "start": "2178200",
    "end": "2184060"
  },
  {
    "text": "I think, technically, a\nfifth-degree polynomial, you can have, I think, four local\nmaximum or minimum, four",
    "start": "2184060",
    "end": "2194290"
  },
  {
    "text": "or five, something like that. So the higher the\ndegree is the more times",
    "start": "2194290",
    "end": "2200420"
  },
  {
    "text": "you can go up and down. Because if you have a quadratic,\nthe only thing you can do",
    "start": "2200420",
    "end": "2205480"
  },
  {
    "text": "is this, or maybe this. And for cubic, you can do this. And for fourth-degree\npolynomial,",
    "start": "2205480",
    "end": "2210660"
  },
  {
    "text": "you can probably do\nsomething like this. So the exact details\nhere don't matter.",
    "start": "2210660",
    "end": "2216079"
  },
  {
    "text": "So just the point is that if you\nhave high degree polynomials, you can be more flexible.",
    "start": "2216080",
    "end": "2222140"
  },
  {
    "text": "And then if you fit the data-- if you fit the polynomial\nto the data, then possible,",
    "start": "2222140",
    "end": "2229220"
  },
  {
    "text": "you're going to get something\nkind of pretty flexible, something like this.",
    "start": "2229220",
    "end": "2238240"
  },
  {
    "text": "And actually, if you\nreally look up some-- like this is not\nrequired for this course, but if you look up the\nbook for the calculus",
    "start": "2238240",
    "end": "2244369"
  },
  {
    "text": "of like polynomials, you know\nthat if you have four points, there's always a\nfifth-degree polynomial",
    "start": "2244370",
    "end": "2249480"
  },
  {
    "text": "with a path for all of them. So in some sense, if you\ndon't have enough points, and your degree is\nhigh enough, then you",
    "start": "2249480",
    "end": "2256619"
  },
  {
    "text": "can always make the training\nerror 0, literally 0. So in this case, the training\nerror is literally 0.",
    "start": "2256619",
    "end": "2272849"
  },
  {
    "text": "So I guess, this is expected. And the thing is that\nthis is overfitting. So what's the problem here?",
    "start": "2272849",
    "end": "2278140"
  },
  {
    "text": "Why it's overfitting? So why the test is not good? So in some sense, the intuition\nis that this kind of model",
    "start": "2278140",
    "end": "2286730"
  },
  {
    "text": "fits. So it's fit to the\nspurious patterns in the \nsmall and noise",
    "start": "2286730",
    "end": "2298579"
  },
  {
    "text": "data, small and noise data.",
    "start": "2298580",
    "end": "2304510"
  },
  {
    "text": "So this is because you\ndon't have enough data, and your model\ntries to explain all of this small\nperturbations, small noise.",
    "start": "2304510",
    "end": "2310890"
  },
  {
    "text": "And because it overexpressed\nthe small noise, at loss, it kind of like didn't\npay enough attention",
    "start": "2310890",
    "end": "2317830"
  },
  {
    "text": "to the more important stuff. And the reason why\nyou can overfit to the small noise,\nthe following data,",
    "start": "2317830",
    "end": "2324349"
  },
  {
    "text": "is because you are so flexible. So whatever patterns you\nsee in this four points,",
    "start": "2324349",
    "end": "2331530"
  },
  {
    "text": "as long as you just have\nfour points, whatever crazy patterns you\nsee, you can always find a degree of 5\npolynomial to explain it.",
    "start": "2331530",
    "end": "2339829"
  },
  {
    "text": "So whatever patterns you\nsee in four data points, like you can explain it. So that doesn't sounds right. So like how come your model\ncan explain like everything",
    "start": "2339829",
    "end": "2347910"
  },
  {
    "text": "and anything like a random. So basically, you are\nlooking at-- you are kind of like overfitting to\nthe spurious patterns,",
    "start": "2347910",
    "end": "2356060"
  },
  {
    "text": "but instead of the big pattern. So the big pattern is this. The spurious patterns are the\nfluctuations in some sense.",
    "start": "2356060",
    "end": "2364460"
  },
  {
    "text": "And so in other\nwords, I think you are explaining the noise\ninstead of the ground truth.",
    "start": "2364460",
    "end": "2373180"
  },
  {
    "text": "And again, how do you make\nthis intuition a more formal? OK. I'm not going to go\nvery, very formal,",
    "start": "2373180",
    "end": "2379350"
  },
  {
    "text": "but to some more\nkind of things I can say about this\nintuition is that this is saying that you are sensitive.",
    "start": "2379350",
    "end": "2388500"
  },
  {
    "text": "Your model is sensitive or\nmaybe kind of like specific",
    "start": "2388500",
    "end": "2393890"
  },
  {
    "text": "to the noise. How do I formulate this?",
    "start": "2393890",
    "end": "2399220"
  },
  {
    "text": "Like one way to\nkind of formulate this a little bit\nmore mathematically is that you can consider\nto redraw the samples.",
    "start": "2399220",
    "end": "2411010"
  },
  {
    "text": "And you ask whether after\nyou read all the samples, are you going to see\nthe same model again?",
    "start": "2411010",
    "end": "2416650"
  },
  {
    "text": "So you redraw some\nnew samples with different spurious patterns. They are spurious\nbecause they are noise.",
    "start": "2416650",
    "end": "2423299"
  },
  {
    "text": "If your model is specific to\nthe spurious patterns, that means that if you redraw,\nyou are going to--",
    "start": "2423300",
    "end": "2428849"
  },
  {
    "text": "you're going to learn the\nnew spurious patterns. And you are going to\nhave a different model. And if you are not specific or\nsensitive to spurious patterns,",
    "start": "2428850",
    "end": "2435470"
  },
  {
    "text": "even you have a new\ndata set, you probably shouldn't change much,\nbut you should still be somewhat the same.",
    "start": "2435470",
    "end": "2440980"
  },
  {
    "text": "You should still opt\nfor the same model. And it turns out that if you\nhave the 5-degree polynomial , you redraw the data sets, then\nyou will find a new model.",
    "start": "2440980",
    "end": "2449369"
  },
  {
    "text": "So what happens is that suppose\nyou reach all the data sets--",
    "start": "2449369",
    "end": "2454750"
  },
  {
    "text": "in the lecture notes, there are\nsome real experiments, again, but here, I'm just\ngoing to draw them.",
    "start": "2454750",
    "end": "2460940"
  },
  {
    "text": "So suppose, for example, now\nyou still have the same ground truth, but you observe some--",
    "start": "2460940",
    "end": "2467770"
  },
  {
    "text": "maybe, let's say, here, I'm\ngoing to have something a point like this, maybe they're\nlike point like this.",
    "start": "2467770",
    "end": "2474670"
  },
  {
    "text": "Maybe we want to keep this. And I'm going to try to make\nthe pattern rather different.",
    "start": "2474670",
    "end": "2480339"
  },
  {
    "text": "Then, maybe you're going\nto get something different. Maybe, I don't know.",
    "start": "2480339",
    "end": "2487030"
  },
  {
    "text": "You try to find out what the\ndegree of the polynomial, maybe you want to get\nsomething like this.",
    "start": "2487030",
    "end": "2492670"
  },
  {
    "text": "OK. Actually, these two\nare still similar, but I can't draw anything. Empirically, you will see\nthat they will be different,",
    "start": "2492670",
    "end": "2499580"
  },
  {
    "text": "just because any small\nperturbations of this would change a lot. But maybe, you got this.",
    "start": "2499580",
    "end": "2505150"
  },
  {
    "text": "Actually, you can also\ndo some local thing, where suppose you move this\npoints a little bit lower, then you probably will\nchange this function a lot.",
    "start": "2505150",
    "end": "2512930"
  },
  {
    "text": "So just because you are very\nsensitive to the data points. I guess, we got the\nsame number of samples.",
    "start": "2512930",
    "end": "2518900"
  },
  {
    "text": "[INAUDIBLE] not using\nthe number of samples. So far, I'm saying that you\ndraw the same number of samples",
    "start": "2518900",
    "end": "2525540"
  },
  {
    "text": "with similar ground truth-- the same ground truth\nand the solution. But just their\nrandomness are different.",
    "start": "2525540",
    "end": "2531760"
  },
  {
    "text": "You are using different noise.",
    "start": "2531760",
    "end": "2538900"
  },
  {
    "text": "And that's a good question. That's exactly what I'm\ngoing to talk about next. OK.",
    "start": "2538900",
    "end": "2544550"
  },
  {
    "text": "Sorry. One moment before that. So basically, OK, just\nto summarize here,",
    "start": "2544550",
    "end": "2549730"
  },
  {
    "text": "if you redraw all the\nexamples and you find that a large variation between-- so suppose, you have a--",
    "start": "2549730",
    "end": "2555359"
  },
  {
    "text": "so you so you have-- so you call this--",
    "start": "2555359",
    "end": "2561480"
  },
  {
    "text": "So basically, you\ndefine a variance to be, in some\nsense, the variations",
    "start": "2561480",
    "end": "2570680"
  },
  {
    "text": "across models learned\non different data sets.",
    "start": "2570680",
    "end": "2586869"
  },
  {
    "text": "So for example, you\ndraw five data sets, so each data set has\nfour examples, maybe.",
    "start": "2586869",
    "end": "2593320"
  },
  {
    "text": "And you do this experiment. And you get five models learned\non five different data sets. So if you see a\nlot of differences",
    "start": "2593320",
    "end": "2599220"
  },
  {
    "text": "between these\nmodels, so then, that means you have large variance.",
    "start": "2599220",
    "end": "2604730"
  },
  {
    "text": "And if you don't see\na lot of differences, then you don't have\na large variance. That's the somewhat\nformal definition of this.",
    "start": "2604730",
    "end": "2611790"
  },
  {
    "text": "We will have a little more\nformal version of this, but this is the idea. So maybe, for example, if\nyou get a new data set,",
    "start": "2611790",
    "end": "2619579"
  },
  {
    "text": "you get something like maybe\nhere, here, here, here,",
    "start": "2619579",
    "end": "2625170"
  },
  {
    "text": "and maybe you're going to\nlearn something very different, maybe something like this.",
    "start": "2625170",
    "end": "2630550"
  },
  {
    "text": "So here, at least,\nyou can see this one is very different from this one\nbecause on the left hand side",
    "start": "2630550",
    "end": "2635740"
  },
  {
    "text": "here, you are going up,\nhere, you are going down. So that suggests that\nyou have large variance.",
    "start": "2635740",
    "end": "2641500"
  },
  {
    "text": "And now talking about\ndata, so suppose, so this one of the\ncharacteristic of variance",
    "start": "2641500",
    "end": "2650280"
  },
  {
    "text": "is that variance is\nsomething that can be reduced if you have more data.",
    "start": "2650280",
    "end": "2657640"
  },
  {
    "text": "And in some sense, the variance\nis caused by lack of data. And it can be mitigated\nif you have more data.",
    "start": "2657640",
    "end": "2664089"
  },
  {
    "text": "So let me continue here. I should just keep all of\nthese markers in my hand.",
    "start": "2664089",
    "end": "2671119"
  },
  {
    "text": "Otherwise, I have to\nwalk back and forth.",
    "start": "2671119",
    "end": "2676760"
  },
  {
    "text": "OK. So the variance,\nand sometimes, you",
    "start": "2676760",
    "end": "2682328"
  },
  {
    "text": "can say this is caused,\nat least, partially, at least one cause is that\nthis is caused by lack of data.",
    "start": "2682329",
    "end": "2693000"
  },
  {
    "text": "And of course, it's probably,\nyou cannot say this is only caused by loss of data because\nif you have a different model",
    "start": "2693000",
    "end": "2702260"
  },
  {
    "text": "of a variance-- and sometimes there\nare two reasons. One thing is like you\nhave lack of data, and the other is you have\ntoo expressive models.",
    "start": "2702260",
    "end": "2714078"
  },
  {
    "text": "And these two things are kind\nof like relative to each other. So if you have a very\nexpressive model,",
    "start": "2714079",
    "end": "2721390"
  },
  {
    "text": "but your data is really, really\nbig, then probably, it's OK. On the other hand, if you\nhave not too many data,",
    "start": "2721390",
    "end": "2726950"
  },
  {
    "text": "but you have very,\nvery simple model, then it's probably still OK.",
    "start": "2726950",
    "end": "2732470"
  },
  {
    "text": "And as you can see, then, if\nthis are the issue, the reason, then how do you\nmitigate the variance,",
    "start": "2732470",
    "end": "2739349"
  },
  {
    "text": "then the mitigation\nis just that-- the mitigation is that\neither you get more data",
    "start": "2739349",
    "end": "2749579"
  },
  {
    "text": "or you have simpler model.",
    "start": "2749579",
    "end": "2756140"
  },
  {
    "text": "So technically, you\ndon't have more data. If you have more data,\nyou should already use them already.",
    "start": "2756140",
    "end": "2762490"
  },
  {
    "text": "But for the understanding,\nlet's see, for example, what happens if you have\nmore data with this thing.",
    "start": "2762490",
    "end": "2771039"
  },
  {
    "text": "Suppose you have more\ndata, and you still fit a fifth-degree polynomial.",
    "start": "2771040",
    "end": "2789559"
  },
  {
    "text": "So suppose you have\na lot more data. This is the ground truth.",
    "start": "2789560",
    "end": "2798119"
  },
  {
    "text": "And you observe a\nlot of more data.",
    "start": "2798119",
    "end": "2804809"
  },
  {
    "text": "So you have a million\ndata, roughly.",
    "start": "2804810",
    "end": "2810750"
  },
  {
    "text": "There's a little bit\nfluctuation, of course. So now you want to fit a\nfifth-degree polynomial.",
    "start": "2810750",
    "end": "2817869"
  },
  {
    "text": "What happens will be\nthat this is probably not entirely obvious-- OK. One obvious thing is that you\nprobably wouldn't do anything",
    "start": "2817869",
    "end": "2825100"
  },
  {
    "text": "like crazy as this right. Because if you do\nthis crazy thing,",
    "start": "2825100",
    "end": "2830250"
  },
  {
    "text": "maybe this crazy thing\ngoes through some points, but you cannot go\nthrough all the points.",
    "start": "2830250",
    "end": "2835570"
  },
  {
    "text": "Like for example,\nyou can see here, there's a big match between\nthis part and this point.",
    "start": "2835570",
    "end": "2843640"
  },
  {
    "text": "And here, you have\nsome mismatch, right? So this one wouldn't give you\neven a small training error.",
    "start": "2843640",
    "end": "2851760"
  },
  {
    "text": "So this is not a best model\nfit on the training data. So what you really will fit,\nlike if you minimize the error",
    "start": "2851760",
    "end": "2857869"
  },
  {
    "text": "on the training data with this\nso many training examples, then what you will get is\nprobably something like this.",
    "start": "2857870",
    "end": "2864020"
  },
  {
    "text": "More like this. Maybe there are still\nsome small fluctuations.",
    "start": "2864020",
    "end": "2870460"
  },
  {
    "text": "It's not like necessarily\nmatching exactly the ground truth, but you have\na small fluctuation, but it will be\nsomething like this.",
    "start": "2870460",
    "end": "2876640"
  },
  {
    "text": "Because if you don't\ndo this, then you wouldn't fit the training\ndata as good as well.",
    "start": "2876640",
    "end": "2885420"
  },
  {
    "text": "This is kind of more\nlike a quadratic. But a fifth-degree polynomial--\nthe family of the degree 5",
    "start": "2885420",
    "end": "2892349"
  },
  {
    "text": "polynomial contains the\nfamily of quadratic function because you can just set your\ntheta 5, theta 4 to be 0,",
    "start": "2892349",
    "end": "2898880"
  },
  {
    "text": "then you get a quadratic. So empirically,\nwhat you're going do to find is that probably, if\nyou really look at the details,",
    "start": "2898880",
    "end": "2906130"
  },
  {
    "text": "the best fit model is\nto degree 5 polynomial. But the set of five, therefore,\nthe first few coefficients",
    "start": "2906130",
    "end": "2912430"
  },
  {
    "text": "are very, very small. So effectively, you are just\nvery close to a quadratic. [INAUDIBLE] What is\nsuppose to be errors",
    "start": "2912430",
    "end": "2921580"
  },
  {
    "text": "because with complicated\nmodels, it's harder to train [INAUDIBLE] because it's so\nmany more possible local minima",
    "start": "2921580",
    "end": "2930230"
  },
  {
    "text": "[INAUDIBLE] So the question is that\nanother possibility is that a failure\nmode is that you just",
    "start": "2930230",
    "end": "2936250"
  },
  {
    "text": "couldn't find this\ndegree 5 polynomial because some optimization issue.",
    "start": "2936250",
    "end": "2942490"
  },
  {
    "text": "Even though there\nexist one, that is very good that fits the\ndata, but you couldn't find it. That's probably not true\nfor degree fifth polynomial",
    "start": "2942490",
    "end": "2949588"
  },
  {
    "text": "for this one toy example, just\nbecause this is very simple. But it could be possible\nfor some other cases, where",
    "start": "2949589",
    "end": "2958160"
  },
  {
    "text": "the model does exist,\nbut you can find it. So this is something that\nwe don't discuss, at least, in the scope of this lecture.",
    "start": "2958160",
    "end": "2965560"
  },
  {
    "text": "So in this lecture, we are\nassuming that you can just-- optimization always works. You always find the best model.",
    "start": "2965560",
    "end": "2970609"
  },
  {
    "text": "So if it exists,\nthen you can find it. So that's why I'm like in this\ncase, even have a lot of data,",
    "start": "2970609",
    "end": "2981410"
  },
  {
    "text": "and even you have a very\ncomplex model as a degree 5 polynomial or even degree",
    "start": "2981410",
    "end": "2986970"
  },
  {
    "text": "there's always exist\none model that works, which is like something like\nthis, like the ground truth.",
    "start": "2986970",
    "end": "2993720"
  },
  {
    "text": "And we'll find it. For this case, definitely,\nwe will find it because it's a linear\nregression problem. You will find the best model.",
    "start": "2993720",
    "end": "3001430"
  },
  {
    "text": "OK. Cool. And also, another, maybe\njust to answer the question.",
    "start": "3001430",
    "end": "3010220"
  },
  {
    "text": "In some sense, the problem\nyou are referring to is easier to detect in\nsome sense to some extent.",
    "start": "3010220",
    "end": "3015280"
  },
  {
    "text": "It's not always true because\nat least, you can detect that from the training. So here, we are more talking\nabout generalization.",
    "start": "3015280",
    "end": "3022710"
  },
  {
    "text": "So OK. Cool. So any other questions?",
    "start": "3022710",
    "end": "3028270"
  },
  {
    "start": "3023000",
    "end": "3286000"
  },
  {
    "text": "What happens to the [INAUDIBLE]",
    "start": "3028270",
    "end": "3036540"
  },
  {
    "text": "You got more data. You're getting more data. Yeah. Yeah. So here, when I say more data,\nI really mean that you have--",
    "start": "3036540",
    "end": "3043089"
  },
  {
    "text": "you just collect--\nyou have more data from the same distribution. From the same distribution.",
    "start": "3043089",
    "end": "3048190"
  },
  {
    "text": "From the same distribution. Yeah. Yeah. So like if you collect\nmore data from--",
    "start": "3048190",
    "end": "3053790"
  },
  {
    "text": "yeah. So like in some sense, you\nkind of like the mindset--",
    "start": "3053790",
    "end": "3059700"
  },
  {
    "text": "I'm not saying\nthis is universally applicable to every situation,\nbut the mindset we are in is that, for example, you have--",
    "start": "3059700",
    "end": "3065901"
  },
  {
    "text": "how do I say that? You have a lot of\nlike medical images.",
    "start": "3065901",
    "end": "3073680"
  },
  {
    "text": "So like they are--\nfor example, there is a million patients with the\ncancer diagnosis kind of thing.",
    "start": "3073680",
    "end": "3079890"
  },
  {
    "text": "But not all of the\ndata are labeled. So like only probably,\nat the beginning,",
    "start": "3079890",
    "end": "3086160"
  },
  {
    "text": "four images that are\nlabeled as cancer or not, so on and so forth. But these four\nimages are samples",
    "start": "3086160",
    "end": "3092430"
  },
  {
    "text": "from this big population. And now, I'm asking I found\nout my variance is very big.",
    "start": "3092430",
    "end": "3097920"
  },
  {
    "text": "So how do I mitigate that? So probably one thing is that\nI can just sample more data from the same--",
    "start": "3097920",
    "end": "3104210"
  },
  {
    "text": "I have like 1 million\nand label examples. I had four labeled\nones, and now I say,",
    "start": "3104210",
    "end": "3110190"
  },
  {
    "text": "I'm going to\ncollect more labels. So I sample like another\nlike 100 examples",
    "start": "3110190",
    "end": "3115200"
  },
  {
    "text": "from the same distribution,\nand then I label them. And then I run the\nalgorithm, and the variance will be smaller.",
    "start": "3115200",
    "end": "3120830"
  },
  {
    "text": "[INAUDIBLE] Actually, [INAUDIBLE] to\nground truths of the data.",
    "start": "3120830",
    "end": "3126490"
  },
  {
    "text": "How do we know the [INAUDIBLE]\nthe ground truth [INAUDIBLE]",
    "start": "3126490",
    "end": "3133200"
  },
  {
    "text": "it is a linear structure. So the question is\nthat how do you-- like if you don't\nknow the ground truth,",
    "start": "3133200",
    "end": "3139760"
  },
  {
    "text": "so how do you know that you\nare having a large bias?",
    "start": "3139760",
    "end": "3146520"
  },
  {
    "text": "You cannot really exactly know. When you don't know ground\ntruth, so all of these are so far are for\nanalysis purpose.",
    "start": "3146520",
    "end": "3151940"
  },
  {
    "text": "So when we don't know\nthe ground truth, you cannot really exact like-- let me think.",
    "start": "3151940",
    "end": "3158090"
  },
  {
    "text": "Yeah. When we don't know\nthe ground truth, I think you cannot\nexactly compute the bias.",
    "start": "3158090",
    "end": "3168150"
  },
  {
    "text": "Because the definition\nof the bias, actually, requires you\nto sample a lot of data.",
    "start": "3168150",
    "end": "3173690"
  },
  {
    "text": "So you also don't\nhave infinite data. So there is no way you can\nevaluate the bias, exactly.",
    "start": "3173690",
    "end": "3179090"
  },
  {
    "text": "So typically, what\nyou do is you say, you fit the data on\nthe training set. And you see you're underfitting.",
    "start": "3179090",
    "end": "3186119"
  },
  {
    "text": "And that's when you say-- underfitting means you have\na large training error.",
    "start": "3186119",
    "end": "3192460"
  },
  {
    "text": "And that's when you\nstart to believe that you have a large bias. For overfitting the graph\nthat's right behind you,",
    "start": "3192460",
    "end": "3201568"
  },
  {
    "text": "the bias square [INAUDIBLE]\nwhat's the third one? The third one is\nthe sum of them.",
    "start": "3201569",
    "end": "3207130"
  },
  {
    "text": "This is the test error. OK.",
    "start": "3207130",
    "end": "3212299"
  },
  {
    "text": "And the bias is\nthe total of them? Bias is this one.",
    "start": "3212299",
    "end": "3219140"
  },
  {
    "text": "[INAUDIBLE] I'll discuss it. I'll discuss that in a moment. Because I didn't--\nwhen I draw this,",
    "start": "3219140",
    "end": "3224539"
  },
  {
    "text": "I didn't even tell\nyou what this is. I'll go back to\ncome back to this.",
    "start": "3224540",
    "end": "3232260"
  },
  {
    "text": "Are we [INAUDIBLE]",
    "start": "3232260",
    "end": "3244520"
  },
  {
    "text": "For ",
    "start": "3244520",
    "end": "3254670"
  },
  {
    "text": "highly imbalanced data set? So maybe let's\ndiscuss this offline. I'm not sure whether this--",
    "start": "3254670",
    "end": "3260110"
  },
  {
    "text": "I think, it probably\nrequires more-- the imbalanced data\nset is pretty often.",
    "start": "3260110",
    "end": "3265298"
  },
  {
    "text": "Like we have research on that. But maybe it's not exactly\nrelated to the context here. Maybe we can discuss offline.",
    "start": "3265299",
    "end": "3272430"
  },
  {
    "text": "Any other questions? OK. I think I do have something\nto say about the variance,",
    "start": "3272430",
    "end": "3281079"
  },
  {
    "text": "and then I'll come\nback to the trade-off. All right. OK.",
    "start": "3281079",
    "end": "3286380"
  },
  {
    "start": "3286000",
    "end": "3599000"
  },
  {
    "text": "So now, let's see. So let's briefly summarize. So basically, if\nyou have a bias,",
    "start": "3286380",
    "end": "3308780"
  },
  {
    "text": "this is really just about the\nlack of models expressivity. It's something intrinsic,\nnothing to do with data, right?",
    "start": "3308780",
    "end": "3314310"
  },
  {
    "text": "This is just the lack of-- if you have large\nbias, that means you have a lack of expressivity.",
    "start": "3314310",
    "end": "3322579"
  },
  {
    "text": "The model is not\nexpressive enough. Doesn't depend too\nmuch on the data.",
    "start": "3322579",
    "end": "3333510"
  },
  {
    "text": "I guess, for linear\nmodels, you can just say,",
    "start": "3333510",
    "end": "3343690"
  },
  {
    "text": "doesn't depend on\nthe [INAUDIBLE] of data for non-linear models. There is some\ntechnicality, which you don't have to make--\nlike the only reason",
    "start": "3343690",
    "end": "3350700"
  },
  {
    "text": "why I had much is just because\nthere's some technicality that prevented me to say this is\nexactly irrelevant to number",
    "start": "3350700",
    "end": "3358220"
  },
  {
    "text": "of data. But you should basically just\nbelieve that it's intuitive. It's not a notion about\nhow many data you have.",
    "start": "3358220",
    "end": "3364500"
  },
  {
    "text": "It's really about how\nexpressive your model is. And variance, if you\nhave a large variance,",
    "start": "3364500",
    "end": "3374860"
  },
  {
    "text": "then it could be two things. One is lack of data. And another thing is that you\nhave too complex of a model.",
    "start": "3374860",
    "end": "3382930"
  },
  {
    "text": "I guess, I'm just\nrepeating and summarizing. And then, I guess, we\ncan see this trade-off.",
    "start": "3382930",
    "end": "3390470"
  },
  {
    "text": "So I guess, I'll go to here. And also, there is way if I\nwere to prove that test is equal to a bias plus variance.",
    "start": "3390470",
    "end": "3398150"
  },
  {
    "text": "I don't think I have-- I will see what I have\ntime to discuss that. But you can also prove the\ntest error is equal to bias",
    "start": "3398150",
    "end": "3406440"
  },
  {
    "text": "squared plus variance. But maybe, let's just\ndraw this from scratch.",
    "start": "3406440",
    "end": "3413450"
  },
  {
    "text": "So this side is the\nmodel complexity. So let's first think about\nhow to draw the bias.",
    "start": "3413450",
    "end": "3419519"
  },
  {
    "text": "This is the test--\nthis is how do you draw the bias on this curve\nas smaller complexity change.",
    "start": "3419520",
    "end": "3425400"
  },
  {
    "text": "So we say that\nthe bias is large, it's because the model\nis not expressive enough.",
    "start": "3425400",
    "end": "3430780"
  },
  {
    "text": "So that means that if your\nmodel is more expressive, then your bias should decrease. So that's why the bias\nis a decreasing function",
    "start": "3430780",
    "end": "3436630"
  },
  {
    "text": "as the model complexity. So this is the bias. And now let's think\nabout how do you draw",
    "start": "3436630",
    "end": "3442048"
  },
  {
    "text": "the variance on this thing. So we said the variance\nis caused because you have too complex of model.",
    "start": "3442049",
    "end": "3448029"
  },
  {
    "text": "That means if your model\nis more and more complex, then you should have\nbigger and bigger variance. That's why the\nvariance is like this.",
    "start": "3448030",
    "end": "3454410"
  },
  {
    "text": "And a test error\nis the sum of them. So the test error is\nlike a U curve thing.",
    "start": "3454410",
    "end": "3461640"
  },
  {
    "text": "So the test error--",
    "start": "3461640",
    "end": "3470740"
  },
  {
    "text": "So the test error is\nthe sum of these two. And so the question\nyou want to answer",
    "start": "3470740",
    "end": "3484109"
  },
  {
    "text": "is that if you change the\nmodel complexity, what is the best test error, right? So it means that it's\nsomewhere in the middle.",
    "start": "3484109",
    "end": "3492960"
  },
  {
    "text": "So actually, I'm\ngoing to tell you something different\nfrom this in a moment.",
    "start": "3492960",
    "end": "3499820"
  },
  {
    "text": "But suppose you believe\nin this, then what the conclusion, the\nimplication of this is that you should somehow kind\nof find a sweet spot when you",
    "start": "3499820",
    "end": "3510080"
  },
  {
    "text": "choose the model complexity. So for example, maybe\nat the beginning you find that your\ntraining error is very low.",
    "start": "3510080",
    "end": "3516450"
  },
  {
    "text": "Sorry, training\nerror is very high, which means our\nbias is very high. So suppose your model\ncomplex is here.",
    "start": "3516450",
    "end": "3522640"
  },
  {
    "text": "Then suppose your model\ncomplex is very small. Then what happens\nis the bias is high.",
    "start": "3522640",
    "end": "3527770"
  },
  {
    "text": "And the bias is high, it\nmeans you are underfitting. It means that your\ntraining error is big. So basically, when you see\nthe training error is big,",
    "start": "3527770",
    "end": "3533940"
  },
  {
    "text": "you kind of see your biases. You kind of believe that\nyour bias is too high, so that's why you should\nincrease the model complexity.",
    "start": "3533940",
    "end": "3540369"
  },
  {
    "text": "And at some point, you find\nthat you are in other regime, where the variance is too\nhigh, then you should stop.",
    "start": "3540370",
    "end": "3546510"
  },
  {
    "text": "So basically, you increase the\nmodel complexity to some extent until your bias and variance\nhas a right trade-off.",
    "start": "3546510",
    "end": "3553740"
  },
  {
    "text": "One of bias and\nvariance first change,",
    "start": "3553740",
    "end": "3559109"
  },
  {
    "text": "did you use different\ntype of model [INAUDIBLE]",
    "start": "3559109",
    "end": "3569329"
  },
  {
    "text": "So I think this figure,\nso this is the-- OK. You ask a good question. So here, this is\nthe model complexity",
    "start": "3569330",
    "end": "3574780"
  },
  {
    "text": "of the model you use to\nlearn your parametrics model.",
    "start": "3574780",
    "end": "3580829"
  },
  {
    "text": "So when you're asking about\nwhat happens if the ground truth is different. I think this is not\nvery sensitive to what",
    "start": "3580829",
    "end": "3587980"
  },
  {
    "text": "the ground truth is, right? There's always a trade-off. But where the trade-off comes\nfrom, where the sweet spot is,",
    "start": "3587980",
    "end": "3594430"
  },
  {
    "text": "would depend on\nthe ground truth. So for example, actually,\nthat's a very good question. For example, suppose\nyou, for this data set,",
    "start": "3594430",
    "end": "3603079"
  },
  {
    "text": "So probably, the best\nthing is to use quadratic. Quadratic has small enough\nbias because quadratic",
    "start": "3603079",
    "end": "3611559"
  },
  {
    "text": "is, in principle, expressive\nenough to express our data. So that's why quadratic\nhas small bias.",
    "start": "3611559",
    "end": "3617680"
  },
  {
    "text": "And also, quadratic is\nprobably, among all the models with small bias,\namong all the models",
    "start": "3617680",
    "end": "3623700"
  },
  {
    "text": "that can express your function,\nquadratic is the least complex. So that's why you use quadratic.",
    "start": "3623700",
    "end": "3629839"
  },
  {
    "text": "That's probably\nthe best solution. And if you really run the\nalgorithm, the quadratic, you would probably recover\nsomething very close.",
    "start": "3629839",
    "end": "3638880"
  },
  {
    "text": "But if you're going\nto, it is cubic, then maybe the sweet spot\nis like the best trade-off is achieved at cubic, maybe.",
    "start": "3638880",
    "end": "3646559"
  },
  {
    "text": "They don't necessarily\nhave to match each other because it also depends on\nthe data, how many data.",
    "start": "3646559",
    "end": "3652099"
  },
  {
    "text": "For example, suppose\nyou are-- maybe let's give you an example. Suppose to say,\nyour ground truth",
    "start": "3652099",
    "end": "3657371"
  },
  {
    "text": "is a degree 10 polynomial. But it's somewhat look\nlike a linear function. So suppose your ground\ntruth is like almost linear,",
    "start": "3657371",
    "end": "3669619"
  },
  {
    "text": "but with a little of a kind\nof like small fluctuation. But you don't have\na lot of data. You just have like\nfive data points.",
    "start": "3669619",
    "end": "3676559"
  },
  {
    "text": "So you just have five\ntraining data points. And now, if you want the\nbias to be literally 0, then,",
    "start": "3676560",
    "end": "3683269"
  },
  {
    "text": "of course, you should\nuse degree 10 polynomial because that's only case\nyou are expressive enough.",
    "start": "3683270",
    "end": "3688539"
  },
  {
    "text": "But then your\nvariance is too big. So the trade-off here probably\nis closer to be a linear.",
    "start": "3688539",
    "end": "3693579"
  },
  {
    "text": "Because if you use\na linear, your bias is not zero, but still\nsmall enough, right?",
    "start": "3693579",
    "end": "3700059"
  },
  {
    "text": "And in that case, the\nvariance is small. So the bias, the\ntrade-off, depends on,",
    "start": "3700059",
    "end": "3707200"
  },
  {
    "text": "for example, how many\ndata you have as well. [INAUDIBLE] We just [INAUDIBLE]\nfor the loss function.",
    "start": "3707200",
    "end": "3717818"
  },
  {
    "text": "So how can [INAUDIBLE]. That's a good question. And the answer to\nthat is that no,",
    "start": "3717819",
    "end": "3723809"
  },
  {
    "text": "you cannot compute\nthe bias and variance. And all of this, all of\nwhat we discussed today is more about some\ninternal understanding.",
    "start": "3723809",
    "end": "3730849"
  },
  {
    "text": "So this bias and various\nis not something you can-- at least, in some case you,\ncan estimate them a little bit.",
    "start": "3730849",
    "end": "3737490"
  },
  {
    "text": "But typically, you\nprobably shouldn't really actively estimate the bias\nand variance in your--",
    "start": "3737490",
    "end": "3744619"
  },
  {
    "text": "these are mostly just for-- its internal understanding\nfor our research,",
    "start": "3744619",
    "end": "3750130"
  },
  {
    "text": "for ourselves, but not\nnecessarily something you, empirically, evaluate.",
    "start": "3750130",
    "end": "3755880"
  },
  {
    "text": "So I guess, so one\nquestion, I guess, many of you probably\nare wondering, if all of these quantities\ncannot be even evaluated,",
    "start": "3755880",
    "end": "3763069"
  },
  {
    "text": "how do you choose\nthe right trade-off? What's the optimal\nmodel complexity? So what you do is actually,\nthat's going to be,",
    "start": "3763070",
    "end": "3769710"
  },
  {
    "text": "I think, what we discuss\nmostly next week, next lecture. So this Wednesday, next week.",
    "start": "3769710",
    "end": "3777369"
  },
  {
    "text": "Yeah. So the variance and bias\nare just for understanding.",
    "start": "3777369",
    "end": "3783279"
  },
  {
    "text": "Empirically, what you\nreally do is that you try a lot of different models.",
    "start": "3783280",
    "end": "3789029"
  },
  {
    "text": "And you select based\non a validation set. But this picture would help\nyou a little bit in some sense.",
    "start": "3789030",
    "end": "3797740"
  },
  {
    "text": "Because for example,\nsuppose you have tried this and this\nand this and this, suppose you have twice\nfour model complexity.",
    "start": "3797740",
    "end": "3803980"
  },
  {
    "text": "And suppose, you believe\nthat this is a U curve, the test error is a U curve. Then should you try\neven bigger models,",
    "start": "3803980",
    "end": "3811210"
  },
  {
    "text": "bigger family of models? Probably, you should. Because you kind of believe\nthat it will be even worse.",
    "start": "3811210",
    "end": "3817230"
  },
  {
    "text": "So you should just try\neven more in the middle. So that's what this\nunderstanding will help you.",
    "start": "3817230",
    "end": "3830620"
  },
  {
    "text": "OK.",
    "start": "3830620",
    "end": "3838730"
  },
  {
    "text": "So there is some more\nformal definition",
    "start": "3838730",
    "end": "3846088"
  },
  {
    "text": "of the bias and variance. And that's in the lecture\nnotes in section 8.1.",
    "start": "3846089",
    "end": "3851539"
  },
  {
    "text": "I think I don't have time to\ndiscuss the formal definition. Even I give the\ndefinition, I probably",
    "start": "3851539",
    "end": "3858119"
  },
  {
    "text": "wouldn't be able to\ngive you the proof. The proof is actually\nrelatively simple. So if you are interested, you\ncan read that section yourself.",
    "start": "3858119",
    "end": "3864859"
  },
  {
    "text": "I don't think it's required\nfor the exam or anything, but it's a relatively simple\nword if you're interested.",
    "start": "3864859",
    "end": "3873380"
  },
  {
    "text": "And also, just this kind of\nbias and variance trade-off, it's not that always easy\nto achieve, mathematically.",
    "start": "3873380",
    "end": "3880009"
  },
  {
    "text": "So for square loss, there is a\nclassic, well-established kind",
    "start": "3880009",
    "end": "3885130"
  },
  {
    "text": "of decomposition. But if you don't\nhave square loss, you don't have MSE, that\nmeans squared error, if you have cross entropy loss.",
    "start": "3885130",
    "end": "3891230"
  },
  {
    "text": "Actually, it was\nan open question. How do you formally\ndecompose this?",
    "start": "3891230",
    "end": "3896970"
  },
  {
    "text": "So all the intuitions do apply. But like how do you do the\nmathematical decomposition",
    "start": "3896970",
    "end": "3902200"
  },
  {
    "text": "is actually pretty challenging. So that's why in\nthe lecture notes,",
    "start": "3902200",
    "end": "3907540"
  },
  {
    "text": "we only talk about square loss. And anywhere, if you read any\ntextbook or any literature,",
    "start": "3907540",
    "end": "3912570"
  },
  {
    "text": "probably, they will\ntalk about square loss. But the intuition is\nstill kind of fun.",
    "start": "3912570",
    "end": "3919000"
  },
  {
    "text": "So if you don't care about what\nexactly definition of bias is. So I will spend the next 20\nminutes to talk about a new--",
    "start": "3919000",
    "end": "3929040"
  },
  {
    "text": "something that is actually\nchallenging this picture. So something that-- so this is\nmaybe just follow more context.",
    "start": "3929040",
    "end": "3936058"
  },
  {
    "text": "So this kind of like\na U curve test error and bias-variance trade-off. This has been like\ndiscovered or kind",
    "start": "3936059",
    "end": "3944170"
  },
  {
    "text": "of like analyzed for I don't\nknow how many years, maybe like 40 years or\nsomething like that.",
    "start": "3944170",
    "end": "3950619"
  },
  {
    "text": "I'm not a historian,\nso I don't know exactly which is the first time\nthis is discovered. But this is like a very classic.",
    "start": "3950619",
    "end": "3958900"
  },
  {
    "text": "However, people\nrealize that there are some issues with\nthis understanding,",
    "start": "3958900",
    "end": "3964140"
  },
  {
    "text": "especially we realize that\nin deep learning, like you--",
    "start": "3964140",
    "end": "3970160"
  },
  {
    "text": "actually, people\nstart to realize this in deep, learning but\nactually, it turns out that even this understanding\nhas an issue for linear models.",
    "start": "3970160",
    "end": "3978450"
  },
  {
    "text": "So this understanding\nis not complete. It misses some other things.",
    "start": "3978450",
    "end": "3985700"
  },
  {
    "text": "So that's what I'm\ngoing to talk about. And this is an area of research\nproductive in the last,",
    "start": "3985700",
    "end": "3995010"
  },
  {
    "text": "probably, three or four years. So let me try to find\nout where should I erase.",
    "start": "3995010",
    "end": "4021480"
  },
  {
    "text": "So this phenomenon that\npeople observe, empirically, at the beginning, and then\nanalyzed theoretically,",
    "start": "4021480",
    "end": "4028599"
  },
  {
    "text": "this phenomenon is\ncalled double descent. If you are a historian,\nthen I think actually",
    "start": "4028599",
    "end": "4039430"
  },
  {
    "text": "this phenomenon actually dates\nback to something like 1990. Some papers, actually,\nat that time,",
    "start": "4039430",
    "end": "4047000"
  },
  {
    "text": "also point out this issue. But I think it just becomes\npopularized and more relevant",
    "start": "4047000",
    "end": "4052539"
  },
  {
    "text": "these days. And what does this mean is\nthat, so basically, I've",
    "start": "4052539",
    "end": "4057700"
  },
  {
    "text": "told you that this\nis test error. This is model complexity.",
    "start": "4057700",
    "end": "4062760"
  },
  {
    "text": "I guess, technically,\nhere, I'm writing the number of parameters\nbecause I want to be precise.",
    "start": "4062760",
    "end": "4069109"
  },
  {
    "text": "Like I'm measuring\nthe model complexity by how many parameters you have. And the classical\nbelief, as we discussed,",
    "start": "4069109",
    "end": "4075840"
  },
  {
    "text": "is that this test error\nshould have this U curve. Something like this.",
    "start": "4075840",
    "end": "4081260"
  },
  {
    "text": "But then, people realized\nthat this is a striking thing. So people realize that if\nyou increase your model",
    "start": "4081260",
    "end": "4086950"
  },
  {
    "text": "number of parameters\neven more, at some point, you will see that it\nwill be like this.",
    "start": "4086950",
    "end": "4096429"
  },
  {
    "text": "So basically, this is the\nnew regime that people got. This is the second\ndescent of the test error.",
    "start": "4096429",
    "end": "4105519"
  },
  {
    "text": "That's why it's called\ndouble descent because there is a decent here,\nthere's a descent here. Everything in the\nblue part, is what",
    "start": "4105520",
    "end": "4114298"
  },
  {
    "text": "people didn't realize as\nmuch as in the last four years, last four or five years.",
    "start": "4114299",
    "end": "4122318"
  },
  {
    "text": "And these are the so-called\noverparameterized regime.",
    "start": "4122319",
    "end": "4132048"
  },
  {
    "text": "So which means that\nin this regime, typically, the\nnumber of parameters is larger than the\nnumber of data points.",
    "start": "4132049",
    "end": "4140970"
  },
  {
    "text": "In some sense,\nthis is the regime that if you ask someone 20\nyears ago, then they will say,",
    "start": "4140970",
    "end": "4150400"
  },
  {
    "text": "this regime is just that no\ngo zone because you should see very, very bad test error",
    "start": "4150400",
    "end": "4156818"
  },
  {
    "text": "But it turns out that\nif you have more-- you make it even\nmore extreme, you make the number of parameters\nbigger in the number of data",
    "start": "4156819",
    "end": "4163350"
  },
  {
    "text": "points, you may actually, not\nin all cases, but in some cases,",
    "start": "4163350",
    "end": "4169738"
  },
  {
    "text": "you may see the-- actually, I would say, in some\ncases, like in many cases.",
    "start": "4169739",
    "end": "4179080"
  },
  {
    "text": "I'm not sure how\nto quantify this, but at least in a lot of cases,\nyou will see a second descent.",
    "start": "4179080",
    "end": "4186870"
  },
  {
    "text": "So that's the striking thing. Is this because\nwe are [INAUDIBLE]",
    "start": "4186870",
    "end": "4196610"
  },
  {
    "text": "the one with much more data? Not directly, I say.",
    "start": "4196610",
    "end": "4202070"
  },
  {
    "text": "Because this is-- at\nleast, on the surface, if you look at\nthis, so this regime",
    "start": "4202070",
    "end": "4208929"
  },
  {
    "text": "is the regime, where\nthe parameters is bigger than the number of data points.",
    "start": "4208929",
    "end": "4214220"
  },
  {
    "text": "So if you want to find the\nright course, I'm not saying-- like you probably will say, at\nleast, to be in this regime,",
    "start": "4214220",
    "end": "4221280"
  },
  {
    "text": "probably, you need to compute. You need a lot of\ncompute because probably, like 10 years ago\nor 20 years ago,",
    "start": "4221280",
    "end": "4228560"
  },
  {
    "text": "you cannot even afford to run\nexperiments in this regime because you don't want to\nuse that many parameters because you don't\nhave enough compute.",
    "start": "4228560",
    "end": "4234989"
  },
  {
    "text": "But of course, nowadays, we\nalso have more data points.",
    "start": "4234990",
    "end": "4241449"
  },
  {
    "text": "And because we have\nnow more data points, because we are\nusing networks, we run larger and larger\nexperiments, indeed,",
    "start": "4241449",
    "end": "4248350"
  },
  {
    "text": "it's correlated with\nmore data points. Like we do see more data\npoints in these days.",
    "start": "4248350",
    "end": "4257040"
  },
  {
    "text": "And this is the so-called\ndouble descent phenomenon. And this kind of mysterious.",
    "start": "4257040",
    "end": "4263010"
  },
  {
    "text": "It's about less\nmysterious these days like after people have studied\nthis in the last five years",
    "start": "4263010",
    "end": "4268710"
  },
  {
    "text": "very carefully. I would talk about some of\nthe explanations, intuitions. But before that,\nlet me also give",
    "start": "4268710",
    "end": "4275678"
  },
  {
    "text": "another related\nphenomenon, which is also called double descent,\nbut it's called data wise",
    "start": "4275679",
    "end": "4283329"
  },
  {
    "text": "double descent. So here, I'm doing a similar--",
    "start": "4283330",
    "end": "4288980"
  },
  {
    "text": "I'm just showing\na similar graph. But on the x-axis,\nI'm going to change the number of data points.",
    "start": "4288980",
    "end": "4296830"
  },
  {
    "text": "So here, the y-axis\nis still a test error.",
    "start": "4296830",
    "end": "4309760"
  },
  {
    "text": "And the x-axis is the\nnumber of data points.",
    "start": "4309760",
    "end": "4315050"
  },
  {
    "text": "OK. Maybe you have a guess first.",
    "start": "4315050",
    "end": "4320239"
  },
  {
    "text": "What this curve\nshould look like? As you have more data points,\nhow does the test error change?",
    "start": "4320240",
    "end": "4327460"
  },
  {
    "text": "Right. The guess would be the test\nerror would be decreasing.",
    "start": "4327460",
    "end": "4334448"
  },
  {
    "text": "Because I guess, here, at least\nif you believe in this bias and variance of\nintuition, then the bias",
    "start": "4334449",
    "end": "4341450"
  },
  {
    "text": "doesn't seems to depend\nmuch on the data. The variance will be\nsmaller and smaller",
    "start": "4341450",
    "end": "4346829"
  },
  {
    "text": "as you have more and more data. So then what you-- if\nyou believe in that,",
    "start": "4346830",
    "end": "4353869"
  },
  {
    "text": "then you should say that OK,\nthe test should look at this. And it should\ncontinue to decrease as you have more and more data.",
    "start": "4353870",
    "end": "4360469"
  },
  {
    "text": "And it turns out that,\nactually, in many cases, what happens is that the\ntest error will look",
    "start": "4360469",
    "end": "4367139"
  },
  {
    "text": "like this, or increase,\nat some point, and it will decrease again.",
    "start": "4367139",
    "end": "4375850"
  },
  {
    "text": "And this peak here is kind\nof similar to the peak here. So this peak is often\nhappening when--",
    "start": "4375850",
    "end": "4385280"
  },
  {
    "text": "it's roughly equal to d.",
    "start": "4385280",
    "end": "4392448"
  },
  {
    "text": "I guess, by the way,\nhere, like there-- this is active research area,\nso I'm not being very precise",
    "start": "4392449",
    "end": "4398670"
  },
  {
    "text": "in every places. So this is number of examples.",
    "start": "4398670",
    "end": "4404020"
  },
  {
    "text": "This is number of examples. This is number of parameters.",
    "start": "4404020",
    "end": "4411290"
  },
  {
    "text": "So what I set here, I think,\nis basically mostly kind of 100% correct\nfor linear models.",
    "start": "4411290",
    "end": "4418659"
  },
  {
    "text": "But for nonlinear models,\nwhether this is exactly is equal to d or not is in 2d or\nthe relationship is less clear.",
    "start": "4418659",
    "end": "4426619"
  },
  {
    "text": "But let's suppose,\nwhen you think about relatively simple\nmodels, then when",
    "start": "4426619",
    "end": "4431980"
  },
  {
    "text": "n, the number of data\npoints, is closer to the number of parameters,\nthen in this case, you're going to see a peak.",
    "start": "4431980",
    "end": "4438530"
  },
  {
    "text": "And then after that,\nyou have more data. It actually helps. I saw some questions.",
    "start": "4438530",
    "end": "4445530"
  },
  {
    "text": "So the original double\ndescent, does that like",
    "start": "4445530",
    "end": "4451309"
  },
  {
    "text": "continue to decrease or does\nit eventually increase again? So in the first. Yeah. So in the first figure.",
    "start": "4451310",
    "end": "4456510"
  },
  {
    "text": "This is a good question. So I think I've seen,\nempirically, on both cases.",
    "start": "4456510",
    "end": "4463480"
  },
  {
    "text": "So sometimes, it does increase\nagain a little bit, but often, not much. And sometimes, it\njust keeps decreasing.",
    "start": "4463480",
    "end": "4469500"
  },
  {
    "text": "And sometimes, it plateaus. So I think that's why\npeople probably don't",
    "start": "4469500",
    "end": "4474760"
  },
  {
    "text": "study that part that much. Can you start running again. So this is, again, more\nthan this function.",
    "start": "4474760",
    "end": "4483239"
  },
  {
    "text": "This was like this\nhas been for a while. For this one?",
    "start": "4483239",
    "end": "4488440"
  },
  {
    "text": "Yeah. This phenomenon. This one, I think, is also-- actually, the paper that first\nsystematically discussed this is like 2020.",
    "start": "4488440",
    "end": "4494239"
  },
  {
    "text": "About that peak, when\nwas that discovered? The peak? Yeah. This is discovered in\nthe same paper, the peak.",
    "start": "4494239",
    "end": "4503290"
  },
  {
    "text": "It's not monotone. The fact that\nthere exists a peak was also discovered\nright, essentially. Yeah.",
    "start": "4503290",
    "end": "4509160"
  },
  {
    "text": "I think, at least, they might\nlike [INAUDIBLE] learning happens so often that\nsomeone does something,",
    "start": "4509160",
    "end": "4515809"
  },
  {
    "text": "and then the community\nforgot about it. That's possible. But at least I\nwould say, at least,",
    "start": "4515810",
    "end": "4522428"
  },
  {
    "text": "it's only until 2020 that most\npeople start to realize this. And because of that paper.",
    "start": "4522429",
    "end": "4529010"
  },
  {
    "text": "And what's in the [INAUDIBLE]? I think the paper just called\nmodel wise double descent",
    "start": "4529010",
    "end": "4534510"
  },
  {
    "text": "or something like that. I'm sorry. Data wise double descent.",
    "start": "4534510",
    "end": "4541138"
  },
  {
    "text": "Because this is data wise\nbecause you are changing the number of data points.",
    "start": "4541139",
    "end": "4547430"
  },
  {
    "text": "OK. OK. This sounds like a\nmysterious enough. So like a very,\nvery interesting. And what's the explanation?",
    "start": "4547430",
    "end": "4553449"
  },
  {
    "text": "In the last few\nyears, people try to explain what happens,\nand try to reconcile",
    "start": "4553449",
    "end": "4560290"
  },
  {
    "text": "with our old\nunderstanding about this.",
    "start": "4560290",
    "end": "4565430"
  },
  {
    "text": "And also, this is an\nimportant question because this regime, this\nblue regime, is actually--",
    "start": "4565430",
    "end": "4572150"
  },
  {
    "text": "actually, it's not\nclear whether when you run like a\nclassical linear, models I don't think necessarily,\nyou are in this regime.",
    "start": "4572150",
    "end": "4578699"
  },
  {
    "text": "But at least, it's pretty\nclear that it's more true",
    "start": "4578699",
    "end": "4586179"
  },
  {
    "text": "that for deep learning,\nyou are basically always in this regime.",
    "start": "4586179",
    "end": "4593559"
  },
  {
    "text": "I guess this is still-- nothing is never\nuniversally true. But I think for most of\nthe vision experiments,",
    "start": "4593560",
    "end": "4600039"
  },
  {
    "text": "you are in this regime, where\nyou have more parameters than a data point. So this is something\nthat is really",
    "start": "4600040",
    "end": "4605989"
  },
  {
    "text": "like empirically irrelevant. So that's why people\nreally care about it.",
    "start": "4605989",
    "end": "4612150"
  },
  {
    "text": "And maybe another\nthing I need to clarify is that I think I\nprobably mentioned",
    "start": "4612150",
    "end": "4618409"
  },
  {
    "text": "that the study about linear\nmodels, the phenomenon of linear models is\nmore kind of clear,",
    "start": "4618410",
    "end": "4626730"
  },
  {
    "text": "like there are a lot of studies. And we have pretty\ngood conclusion. And what I mean by that is\nthat even within linear models,",
    "start": "4626730",
    "end": "4632320"
  },
  {
    "text": "you can try to change\nthe model complexity. So what that means is that you\njust insist that you always",
    "start": "4632320",
    "end": "4638160"
  },
  {
    "text": "use linear model. But what you change is\nthat you try to decide how many features you use. So you can start with only using\none feature or two features",
    "start": "4638160",
    "end": "4646150"
  },
  {
    "text": "like for example, in\nthe house price, where you can use the square\nfoot as the single feature, or you can collect a\nbunch of other features.",
    "start": "4646150",
    "end": "4652520"
  },
  {
    "text": "So keep adding more\nand more features. That means you have more\nand more parameters. So even within\nlinear models, you",
    "start": "4652520",
    "end": "4658329"
  },
  {
    "text": "can still change the complexity,\njust to clarify that. And most of this\ntheoretical study, I think,",
    "start": "4658330",
    "end": "4664690"
  },
  {
    "text": "are for linear models. And they are pretty\nprecise these days. And I'm going to try\nto kind of roughly",
    "start": "4664690",
    "end": "4670310"
  },
  {
    "text": "summarize the intuition from the\nstudy of this double descent. So the intuition, I think, I'm\ngoing to list a few of them.",
    "start": "4670310",
    "end": "4689950"
  },
  {
    "text": "So some intuition\nand explanations.",
    "start": "4689950",
    "end": "4698490"
  },
  {
    "text": "And these explanations are\nmostly for linear models. So I think the first\nthing to realize",
    "start": "4698490",
    "end": "4705449"
  },
  {
    "text": "is that this peak,\nso you can argue what is the most exciting\nor surprising thing",
    "start": "4705450",
    "end": "4711820"
  },
  {
    "text": "about this graph. But let's first talk about a\npeak, this peak in the middle.",
    "start": "4711820",
    "end": "4717230"
  },
  {
    "text": "So I think the first thing\nis that in some sense, people realize that the\nexisting algorithms, especially",
    "start": "4717230",
    "end": "4727570"
  },
  {
    "text": "if you just talk\nabout, for example, simple gradient descent\nor stochastic gradient descent for linear models.",
    "start": "4727570",
    "end": "4733270"
  },
  {
    "text": "So the existing algorithms\nunderperform dramatically when",
    "start": "4733270",
    "end": "4746861"
  },
  {
    "text": "it's close to d.",
    "start": "4746861",
    "end": "4753280"
  },
  {
    "text": "So both these two peaks\nare basically like this. So here, you are changing n,\nthe number of data points.",
    "start": "4753280",
    "end": "4758639"
  },
  {
    "text": "And you found that when n is\nclose to d, you had to pick. And here, you are changing\nthe number of parameters, you are changing d, the\nnumber of features you use.",
    "start": "4758640",
    "end": "4765940"
  },
  {
    "text": "And we realized when\nd is kind of above n, above the number of data\npoints, you had a peak.",
    "start": "4765940",
    "end": "4772130"
  },
  {
    "text": "So both of these two\npeaks are showing up here. It's just that you are changing\nthe axis in some sense.",
    "start": "4772130",
    "end": "4778860"
  },
  {
    "text": "So this is also when\nn is close to d,",
    "start": "4778860",
    "end": "4784290"
  },
  {
    "text": "when the number of data points\nis close to number of features.",
    "start": "4784290",
    "end": "4791780"
  },
  {
    "text": "And the explanation is such is\nthe algorithms, the existing algorithms, or the algorithms\nyou are visualizing here.",
    "start": "4791780",
    "end": "4797639"
  },
  {
    "text": "So when you visualize\nthis, where you do rank-- you do use some algorithms\nto learn the parameters.",
    "start": "4797639",
    "end": "4803960"
  },
  {
    "text": "So that particular\nalgorithm that you use to produce this\ngraph, it really underperforms very dramatically.",
    "start": "4803960",
    "end": "4810720"
  },
  {
    "text": "It's not really saying\nthat when n is close to d, the real test error\nshould be this.",
    "start": "4810720",
    "end": "4816030"
  },
  {
    "text": "It's just saying that\nthis algorithm is bad. If you change your\nalgorithm, you probably wouldn't see this peak.",
    "start": "4816030",
    "end": "4822960"
  },
  {
    "text": "So that's why the peak shows up. OK.",
    "start": "4822960",
    "end": "4828460"
  },
  {
    "text": "And what's wrong with the-- the existing\nalgorithm, I really, just mean that for example, some\njust basic gradient descent.",
    "start": "4828460",
    "end": "4837158"
  },
  {
    "text": "So for linear models,\nmaybe this is-- I say, this is\nfor linear models.",
    "start": "4837159",
    "end": "4845620"
  },
  {
    "text": "So what goes wrong with the\nso-called existing algorithm? So this basically gradient\ndescent algorithms.",
    "start": "4845620",
    "end": "4852409"
  },
  {
    "text": "What goes wrong is that a norm\nof the theta, the linear models",
    "start": "4852409",
    "end": "4861670"
  },
  {
    "text": "you learned, is very big.",
    "start": "4861670",
    "end": "4867440"
  },
  {
    "text": "It's very big when n\nis roughly equals d.",
    "start": "4867440",
    "end": "4875530"
  },
  {
    "text": "And we kind of believe that this\nis, at least, a partial reason for why this leads to a peak.",
    "start": "4875530",
    "end": "4880760"
  },
  {
    "text": "So this gives the peak. So even though-- so OK. So I guess, let me\ndraw something here.",
    "start": "4880760",
    "end": "4887670"
  },
  {
    "text": "We have some real\nexperimental, real data, in the lecture notes. But if you draw that norm--",
    "start": "4887670",
    "end": "4898239"
  },
  {
    "text": "so suppose you change\nthe number of parameters,",
    "start": "4898239",
    "end": "4903300"
  },
  {
    "text": "which means you add more and\nmore features in your data set, so that you have more\nand more parameters.",
    "start": "4903300",
    "end": "4909560"
  },
  {
    "text": "And if you visualize\nthe norm in the y-axis, you're going to see\nsomething like this.",
    "start": "4909560",
    "end": "4916309"
  },
  {
    "text": "And this peak here is\nroughly corresponds to n is close to d, which is kind\nof similar to these peaks.",
    "start": "4916310",
    "end": "4925410"
  },
  {
    "text": "So basically, even\nthough, suppose you compare this experiment\nand this experiment,",
    "start": "4925410",
    "end": "4932420"
  },
  {
    "text": "so here, you have more\nparameters than this here. But when you have more\nparameters, maybe sometimes,",
    "start": "4932420",
    "end": "4939570"
  },
  {
    "text": "you have lower smaller norm. So the norm when n is close\nto d, for some reason,",
    "start": "4939570",
    "end": "4946550"
  },
  {
    "text": "it is very, very big. Actually, we know the reasons. The reason is that some random\nmatrix is not well behaved",
    "start": "4946550",
    "end": "4953530"
  },
  {
    "text": "when n is close to d. But I guess we are not\ngoing to go into that. But at least, the\nimmediate reason is that when n is\nclose to d, somehow,",
    "start": "4953530",
    "end": "4960900"
  },
  {
    "text": "this algorithm is producing a\nvery large norm on classifier",
    "start": "4960900",
    "end": "4965920"
  },
  {
    "text": "theta, which is you can argue\nthat if the norm is too big, then your model is too complex.",
    "start": "4965920",
    "end": "4974190"
  },
  {
    "text": "So in some sense, this is saying\nthat your model is actually very complex.",
    "start": "4974190",
    "end": "4980480"
  },
  {
    "text": "So very complex on\n[INAUDIBLE] to the norm.",
    "start": "4980480",
    "end": "4993510"
  },
  {
    "text": "So this model, it\nseem it doesn't have a lot of parameters\ncompared to, for example, this model.",
    "start": "4993510",
    "end": "4998700"
  },
  {
    "text": "So if you compare this\nmodel and this model. So this model seems to have\nless parameter than this. That's by definition.",
    "start": "4998700",
    "end": "5004160"
  },
  {
    "text": "The norm is actually very big. So in some sense, if you use\nthe norm as the complexity, actually, these peaks\nhave large complexity.",
    "start": "5004160",
    "end": "5012710"
  },
  {
    "text": "[INAUDIBLE] [INAUDIBLE] Exactly. One [INAUDIBLE]",
    "start": "5012710",
    "end": "5020720"
  },
  {
    "text": "That's a great question.",
    "start": "5020720",
    "end": "5025880"
  },
  {
    "text": "So you got that. I'm implying that the\nnorm seems to be a better metric for the complexity.",
    "start": "5025880",
    "end": "5031559"
  },
  {
    "text": "So what is the right\nmeasure for complexity? So this is a very\ndifficult question. Like for different situations,\nyou have different answers.",
    "start": "5031560",
    "end": "5039660"
  },
  {
    "text": "But there is no\nuniversal answer.",
    "start": "5039660",
    "end": "5044890"
  },
  {
    "text": "But norm could be\none complex measure. In some sense, the norm is also\na way to describe how many,",
    "start": "5044890",
    "end": "5050020"
  },
  {
    "text": "like suppose you have\na small norm ball. So you have fewer choices to\nfit your data in some sense.",
    "start": "5050020",
    "end": "5056530"
  },
  {
    "text": "So you have fewer degree\nof freedom if you have-- like you have fewer options, in\nsome sense, to fit your data.",
    "start": "5056530",
    "end": "5064209"
  },
  {
    "text": "So that's restrict\nthe complexity. And which norm, that's actually,\nfor different situations,",
    "start": "5064209",
    "end": "5070300"
  },
  {
    "text": "you can argue which norm\nis the right complexity. Actually, there's probably\nno universal answer.",
    "start": "5070300",
    "end": "5076469"
  },
  {
    "text": "But I guess what\nI'm trying to say here is that the\nnumber of parameters is also not necessarily the\nright complexity measure.",
    "start": "5076469",
    "end": "5083670"
  },
  {
    "text": "Because even you\nhave more parameters, suppose all the parameters\nare very, very close to 0, that's probably also\nvery simple model",
    "start": "5083670",
    "end": "5089270"
  },
  {
    "text": "because those parameters\nare not really working. But if you have just\na few parameters, but the norm is really, really\nbig, maybe you can use the--",
    "start": "5089270",
    "end": "5098060"
  },
  {
    "text": "maybe you should also\ncall it very complex. So my short answer is that there\nis no universal answer to this.",
    "start": "5098060",
    "end": "5107020"
  },
  {
    "text": "The point is that probably\nthe number of parameters is not the only\ncomplex measurement. And for linear model,\nit just happens",
    "start": "5107020",
    "end": "5114409"
  },
  {
    "text": "that for mathematical\nreasons, I think l2 norm behaves really nice. It seems to relate to a lot\nof fundamental properties",
    "start": "5114410",
    "end": "5119850"
  },
  {
    "text": "like maybe you can\nargue l2 norm is useful because you are measuring a\nsquare error in many cases.",
    "start": "5119850",
    "end": "5127159"
  },
  {
    "text": "And it's nice with the linear\nalgebra, so on and so forth.",
    "start": "5127159",
    "end": "5135510"
  },
  {
    "text": "OK. So I guess, let me-- I'm running a little\nbit late, but I think I'm almost done here. So here, it's just saying\nthat at least for this case,",
    "start": "5135510",
    "end": "5144699"
  },
  {
    "text": "it sounds like norm\nseems to be a slightly better complex measurement. And actually, if you--",
    "start": "5144700",
    "end": "5151369"
  },
  {
    "text": "and you can test this\nhypothesis in some sense. So you can say that OK, I'm\nsaying here the existing algorithm underperforms.",
    "start": "5151369",
    "end": "5157690"
  },
  {
    "text": "But if you have a new\nalgorithm, that's regularized, suppose you recognize the norm. I guess, I haven't\ntold you exactly what",
    "start": "5157690",
    "end": "5164790"
  },
  {
    "text": "regularization means. But here, just what\nI mean is that you try to find a model such\nthat enormous small.",
    "start": "5164790",
    "end": "5172580"
  },
  {
    "text": "So you add an\nadditional term that tries to make the norm small. So you don't only train\non the training loss,",
    "start": "5172580",
    "end": "5178070"
  },
  {
    "text": "but also you try to\nmake the norm smaller. Then you're going to\nsee something like this.",
    "start": "5178070",
    "end": "5185719"
  },
  {
    "text": "So regularization would\nmitigate this to some extent. I would discuss more\nabout regularization in the next lecture.",
    "start": "5185719",
    "end": "5191780"
  },
  {
    "text": "But here, it really just\nmeans that you don't only care about training\nloss, but also you try to find a model\nwith small norms.",
    "start": "5191780",
    "end": "5200280"
  },
  {
    "text": "And you have some kind\nof balance between them. So you can sacrifice a\nlittle bit of training error,",
    "start": "5200280",
    "end": "5205469"
  },
  {
    "text": "but you insist that\nyour norm is small, then you can see this rise.",
    "start": "5205469",
    "end": "5211880"
  },
  {
    "text": "So that, in some sense,\nexplains, partially, why you had the peak. Because the peak is caused\nbecause the algorithm",
    "start": "5211880",
    "end": "5218530"
  },
  {
    "text": "was suboptimal. Your algorithm didn't use\nthe right complexity measure. And you can fix that\npeak by adding norm.",
    "start": "5218530",
    "end": "5225170"
  },
  {
    "text": "But there's one more question,\nwhich is there is no peak, but why there's no ascent?",
    "start": "5225170",
    "end": "5233250"
  },
  {
    "text": "So suppose you just see this. Actually here, you will also\nsee this, something like this.",
    "start": "5233250",
    "end": "5243100"
  },
  {
    "text": "So this figure is actually\npretty reasonable. Because if your data\npoint is increasing, you probably should\njust have one decrease,",
    "start": "5243100",
    "end": "5248989"
  },
  {
    "text": "like you just keep decreasing. You just keep decreasing\nthe test error.",
    "start": "5248989",
    "end": "5254469"
  },
  {
    "text": "So this one, let's\nsay, we are OK with it. We are happy if you see\njust a single decrease.",
    "start": "5254469",
    "end": "5261050"
  },
  {
    "text": "But here, suppose you\nsee a single descent, I feel like it's kind of\narguable whether you're",
    "start": "5261050",
    "end": "5267888"
  },
  {
    "text": "should be happy with\nthis answer because why,",
    "start": "5267889",
    "end": "5273160"
  },
  {
    "text": "when the number of\nparameter is so huge, you can still generalize? So why, when you use,\nfor example, a million parameters, and you\njust have five examples,",
    "start": "5273160",
    "end": "5279760"
  },
  {
    "text": "why you can still generalize? Why you don't have\nascent, eventually? In many cases, you\ndon't have ascent.",
    "start": "5279760",
    "end": "5286219"
  },
  {
    "text": "And in many cases, the\nbest one is just you have more and more parameters.",
    "start": "5286220",
    "end": "5291950"
  },
  {
    "text": "And actually, for\nexample, another question is when number of\nparameter is bigger than the number of\ndata points, sometimes, you are thinking\nthis is the-- you",
    "start": "5291950",
    "end": "5298889"
  },
  {
    "text": "have too many degree\nof freedom to fit all the specifics of the data set. You shouldn't generalize.",
    "start": "5298889",
    "end": "5304699"
  },
  {
    "text": "But actually, empirically,\nyou do work pretty well. So that's the last,\nand sometimes,",
    "start": "5304699",
    "end": "5310949"
  },
  {
    "text": "another missing\npoint, missing part. And this part, we also have\nsome explanation for that.",
    "start": "5310949",
    "end": "5316930"
  },
  {
    "text": "And the explanation is that-- so n is much, much\nbigger than d--",
    "start": "5316930",
    "end": "5324350"
  },
  {
    "text": "sorry. d is these much-- the number of parameters\nis much bigger than the d.",
    "start": "5324350",
    "end": "5336770"
  },
  {
    "text": "Sorry, much bigger than the\nn, the number of data points. So the thing is\nthat even though it",
    "start": "5336770",
    "end": "5345449"
  },
  {
    "text": "sounds like you are supposed\nto overfit, but actually, the norm is small.",
    "start": "5345449",
    "end": "5350460"
  },
  {
    "text": "But why the norm is small? Why, when you have\nso many parameters, you still learn\nvery simple model?",
    "start": "5350460",
    "end": "5356510"
  },
  {
    "text": "The reason is that\nsomehow, there is some implicit\nregularization effect, which",
    "start": "5356510",
    "end": "5372219"
  },
  {
    "text": "makes the norm small. So when I applied this method,\nall of these experiments,",
    "start": "5372219",
    "end": "5377770"
  },
  {
    "text": "they need to have\nany regularization. I didn't have any\nexplicit encouragement to make the norm small. So that's why the\nnorm here is very big.",
    "start": "5377770",
    "end": "5384080"
  },
  {
    "text": "But the norm here is small? The reason is that your\noptimization algorithm",
    "start": "5384080",
    "end": "5390270"
  },
  {
    "text": "has some implicit encouragement\nto make the norm small, which is not used, which is\nnot explicitly written",
    "start": "5390270",
    "end": "5397131"
  },
  {
    "text": "in the loss function. And that's something\nI'm going to discuss, I think, more next time.",
    "start": "5397131",
    "end": "5406050"
  },
  {
    "text": "So for this lecture,\nI think I'm just-- so we're going to discuss\nthis more next time. So the high level thing is\njust that something else is",
    "start": "5406050",
    "end": "5414829"
  },
  {
    "text": "driving the norm to be small. Thanks.",
    "start": "5414830",
    "end": "5416388"
  }
]