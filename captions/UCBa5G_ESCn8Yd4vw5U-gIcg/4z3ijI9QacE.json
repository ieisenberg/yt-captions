[
  {
    "start": "0",
    "end": "5740"
  },
  {
    "text": "Cool. So now that we're at the start\nof week five of the quarter, I'd figured I'd give a\nlittle bit of a roadmap.",
    "start": "5740",
    "end": "11580"
  },
  {
    "text": "So far we've seen multi-task\nlearning and transfer learning basics. And then we covered some of the\ncore meta learning algorithms.",
    "start": "11580",
    "end": "18570"
  },
  {
    "text": "And last week we covered\ncore unsupervised pre-training algorithms. And really at this point\nwe'll start to dive",
    "start": "18570",
    "end": "23939"
  },
  {
    "text": "into slightly more\nadvanced topics. And in particular,\nthis week we're",
    "start": "23940",
    "end": "29880"
  },
  {
    "text": "going to focus on two different\nadvanced meta-learning topics. Today, we're going to be\ntalking about task construction",
    "start": "29880",
    "end": "35399"
  },
  {
    "text": "and some challenges that\ncan come up and also some opportunities\nfor constructing tasks in different ways.",
    "start": "35400",
    "end": "40890"
  },
  {
    "text": "And on Wednesday, we'll be\ntalking about large scale meta optimization and ways that you\ncan do things at a larger scale",
    "start": "40890",
    "end": "50460"
  },
  {
    "text": "basically. And then next week,\nwe'll be talking about some Bayesian\nmeta-learning ideas including",
    "start": "50460",
    "end": "57480"
  },
  {
    "text": "actually a bit of a crash\ncourse on variational inference. Which I think should\nbe useful for either",
    "start": "57480",
    "end": "65099"
  },
  {
    "text": "as a refresher for some of\nyou but also for other people. Something that's really\nuseful to know about",
    "start": "65099",
    "end": "70615"
  },
  {
    "text": "if you aren't familiar\nwith it already.  Awesome. So today we're\ngoing to be talking",
    "start": "70615",
    "end": "76727"
  },
  {
    "text": "about task construction. And really the\nquestion of the day will be how should\nwe define tasks",
    "start": "76727",
    "end": "82690"
  },
  {
    "text": "for meta-learning in order\nto get good performance? And so this is going to\ncover both the problem that",
    "start": "82690",
    "end": "90640"
  },
  {
    "text": "can arise in meta-learning,\nwhich is often referred to as memorization. And it happens when\ntasks are constructed",
    "start": "90640",
    "end": "98020"
  },
  {
    "text": "in a particular way. And then we'll also talk about\nnot just a problem but also an interesting opportunity\nof trying to construct tasks",
    "start": "98020",
    "end": "104950"
  },
  {
    "text": "from unlabeled data and\naugment the set of tasks that you have available.",
    "start": "104950",
    "end": "111062"
  },
  {
    "text": "The first part of\nthe lecture will be covered in homework four. Homework four is optional.",
    "start": "111063",
    "end": "116470"
  },
  {
    "text": "But if you do choose\nto do that, then a lot of the stuff that we'll\ntalk about in the first half of the lecture is quite\nrelevant for that.",
    "start": "116470",
    "end": "124428"
  },
  {
    "text": "And so the goals for\nthe end of the lecture is to understand\nwhen memorization can happen in meta-learning.",
    "start": "124428",
    "end": "129490"
  },
  {
    "text": "And also understand techniques\nfor constructing tasks automatically. ",
    "start": "129490",
    "end": "135909"
  },
  {
    "text": "Awesome. So first let's\nrecap a little bit of what we've covered so far.",
    "start": "135910",
    "end": "141659"
  },
  {
    "text": "And in particular,\nI mentioned trying to clarify some of\nthe terminology. So this is the terminology\nthat we've been using in class,",
    "start": "141660",
    "end": "149250"
  },
  {
    "text": "where through all the lectures\nwhere kind of essentially we have a set of tasks.",
    "start": "149250",
    "end": "156750"
  },
  {
    "text": "We have a set of\nmeta-training tasks and the set of\nmeta-testing tasks. And everything in the\ngreen boxes on the left",
    "start": "156750",
    "end": "162840"
  },
  {
    "text": "is a training data set or\na support set per task. And everything in the\nred box is what we",
    "start": "162840",
    "end": "169439"
  },
  {
    "text": "use to measure generalization. And this is referred\nto often as the test set for that task or the\nquery set for that task.",
    "start": "169440",
    "end": "176580"
  },
  {
    "text": "And one thing that we\nnoted was that you can-- we're actually using these\nlike these test sets on the top",
    "start": "176580",
    "end": "183180"
  },
  {
    "text": "during the\nmeta-training process. So we're training\non the test set. But we're still\nevaluating on something",
    "start": "183180",
    "end": "189212"
  },
  {
    "text": "that is completely new. Which is, are these new\nheld out meta test tasks. Now, one clarification and one\npoint of confusion that came up",
    "start": "189212",
    "end": "196500"
  },
  {
    "text": "is sometimes in\nthe homeworks, we are referring to\nmeta-training and meta test",
    "start": "196500",
    "end": "201719"
  },
  {
    "text": "as sometimes being referred\nto as train and test. This is really\nconfusing and ambiguous.",
    "start": "201720",
    "end": "207280"
  },
  {
    "text": "And so it's something\nthat we'll try to fix in future\niterations of the course.",
    "start": "207280",
    "end": "214170"
  },
  {
    "text": "And to somewhat try to make it\nless ambiguous in the homework. The train sets will refer\nto support and query.",
    "start": "214170",
    "end": "220650"
  },
  {
    "text": "And so that was differentiating\nit from train and test but also already still\nsomewhat confusing.",
    "start": "220650",
    "end": "227070"
  },
  {
    "text": "And hopefully this clarification\nhelps resolve some confusion there. ",
    "start": "227070",
    "end": "234080"
  },
  {
    "text": "Awesome. And then beyond the\nsetup, we've talked",
    "start": "234080",
    "end": "239270"
  },
  {
    "text": "about Black-box\nmeta-learning where the key idea is to parameterize\na learner as a neural network.",
    "start": "239270",
    "end": "247349"
  },
  {
    "text": "And this network\nis the inner loop of the meta-learning process. It learns from a few examples.",
    "start": "247350",
    "end": "252975"
  },
  {
    "text": "Sometimes this learning\nprocess is also called in-context learning. And you'll see that\nterminology used a lot in the NLP literature and\nalso used in homework three.",
    "start": "252975",
    "end": "262078"
  },
  {
    "text": "And then when we actually\ngo about training this neural network to\nlearn from these examples, that's the meta-learning process\nor the outer loop process.",
    "start": "262078",
    "end": "271275"
  },
  {
    "text": "The benefits of\nBlack-box meta-learning is that it's very expressive. You can represent-- this inner\nloop can represent a wide range",
    "start": "271275",
    "end": "277130"
  },
  {
    "text": "of learning procedures. But the downside\nis that it can be a challenging optimization\nproblem, which you probably saw in homework one.",
    "start": "277130",
    "end": "285800"
  },
  {
    "text": "We've also covered\noptimization-based meta-learning that\nembeds gradient descent inside the\nmeta-learning process.",
    "start": "285800",
    "end": "291380"
  },
  {
    "text": "This incorporates the\nstructure of optimization that we know and love. But it requires a second\norder optimization.",
    "start": "291380",
    "end": "297500"
  },
  {
    "text": "And so in homework\ntwo, you may have found that second\norder optimization can make it slower.",
    "start": "297500",
    "end": "302990"
  },
  {
    "text": "A little bit harder\nto deal with. And then lastly, we also\ntalked about non-parametric meta-learning where you\ncreate an embedding of all",
    "start": "302990",
    "end": "310790"
  },
  {
    "text": "of your examples and then do\nsome form of nearest neighbors to compare the test example\nwith your training examples.",
    "start": "310790",
    "end": "317780"
  },
  {
    "text": "And output the label\nthat you think-- of the example that you\nthink you're closest to.",
    "start": "317780",
    "end": "323630"
  },
  {
    "text": " Great. And so as you are hopefully\nseeing in homework two,",
    "start": "323630",
    "end": "329720"
  },
  {
    "text": "this can be easier to optimize\nand computationally faster. But it is largely restricted\nto classification problems.",
    "start": "329720",
    "end": "335840"
  },
  {
    "text": " And then lastly, in terms of\nthe task construction process,",
    "start": "335840",
    "end": "342960"
  },
  {
    "text": "what we've seen in the\nfirst two homeworks is kind of N-way\nclassification problem.",
    "start": "342960",
    "end": "348130"
  },
  {
    "text": "But the same ideas\ncan be applied to having different\ntasks correspond to different regions\nor corresponding",
    "start": "348130",
    "end": "354729"
  },
  {
    "text": "to different robotics tasks. And all of these settings,\nyou need a construct",
    "start": "354730",
    "end": "360040"
  },
  {
    "text": "a set of tasks using\na significant amount of labeled data. ",
    "start": "360040",
    "end": "367110"
  },
  {
    "text": "Great. So now let's talk a little\nbit about the main topic",
    "start": "367110",
    "end": "372590"
  },
  {
    "text": "of this lecture, which is\nmemorization and unsupervised meta-learning. And so I'd like\nto start this off",
    "start": "372590",
    "end": "377720"
  },
  {
    "text": "with a bit of a thought\nexercise for you. And in particular,\nthis is a one picture",
    "start": "377720",
    "end": "384050"
  },
  {
    "text": "of a Black-box meta-learner that\ntakes its input a training data set and a test\ninput and gives you",
    "start": "384050",
    "end": "389960"
  },
  {
    "text": "a predicted label\nfor that test input. And the thought exercise is\nwhen you have-- you train this",
    "start": "389960",
    "end": "398690"
  },
  {
    "text": "on a set of tasks. Each task is indexed by I. And one thing you\ncould consider doing",
    "start": "398690",
    "end": "404780"
  },
  {
    "text": "is passing in the task index, a\none-hot identifier of the task index into your\nBlack-box meta-learner.",
    "start": "404780",
    "end": "412320"
  },
  {
    "text": "And my thought\nexercise is first is when you pass in both the\ntraining data set and the task",
    "start": "412320",
    "end": "419550"
  },
  {
    "text": "identifier what would\nhappen during meta-training? [INAUDIBLE] ",
    "start": "419550",
    "end": "432970"
  },
  {
    "text": "So the response was it might\nlearn to associate the task",
    "start": "432970",
    "end": "438250"
  },
  {
    "text": "identifier and the training\ndata set and memorize it. What is it?",
    "start": "438250",
    "end": "443281"
  },
  {
    "text": "Just-- so like remember\nthe [INAUDIBLE].. ",
    "start": "443281",
    "end": "461680"
  },
  {
    "text": "Yeah. So the examples that show\nup in the training data set are pretty complicated. Whereas this one hot vector\nis not very complicated.",
    "start": "461680",
    "end": "469270"
  },
  {
    "text": "And so what it\nmight do is it might learn to rely on\nthe task identifier rather than using\nthe training data.",
    "start": "469270",
    "end": "476760"
  },
  {
    "text": "And during meta-training this\nprobably isn't a problem.",
    "start": "476760",
    "end": "482490"
  },
  {
    "text": "Basically, it might just learn\nto rely on this task identifier instead. Basically, the task identifier\nand the training data",
    "start": "482490",
    "end": "488070"
  },
  {
    "text": "are redundant. They both encode\ninformation about the task. ",
    "start": "488070",
    "end": "494220"
  },
  {
    "text": "And then my next\nquestion is if it does start to rely on this task\nidentifier what would happen",
    "start": "494220",
    "end": "502949"
  },
  {
    "text": "at meta-test time if you\npass in a new training data set and a task identifier\nfor a new task?",
    "start": "502950",
    "end": "510009"
  },
  {
    "text": "And so this would be\na one-hot vector for-- a one-hot vector\nthat's different",
    "start": "510010",
    "end": "515099"
  },
  {
    "text": "than the one-hot vector\nthat it saw during training in particular, meta-training.",
    "start": "515100",
    "end": "520110"
  },
  {
    "text": "Yeah? Performance would probably\nbe really terrible because they would try to rely\non the task identifier again.",
    "start": "520110",
    "end": "527269"
  },
  {
    "text": "But since it's a one\nhard representation, it has no correspondence\nwith any of the ones that seen before.",
    "start": "527270",
    "end": "532800"
  },
  {
    "text": "Yeah. So the answer was\nthe performance would probably be\npretty terrible because the one-hot\nvector for the new task",
    "start": "532800",
    "end": "539000"
  },
  {
    "text": "is different from any of the\none-hot vectors it saw before. And because it's\nrelying on that,",
    "start": "539000",
    "end": "544680"
  },
  {
    "text": "it wouldn't be able to\nactually perform the new task. So it won't-- in sum, it won't\ngeneralize to the new task.",
    "start": "544680",
    "end": "551827"
  },
  {
    "text": "OK. So this seems like an\ninitial thought exercise. Now, a second\nthought exercise what",
    "start": "551827",
    "end": "559400"
  },
  {
    "text": "if instead of giving it\na one-hot vector what if we gave it a paragraph that\ndescribes what it should do.",
    "start": "559400",
    "end": "569780"
  },
  {
    "text": "Like, how to do the task\nor what the task is. Does anyone have\nthoughts on what would happen during\nmeta-training and perhaps also",
    "start": "569780",
    "end": "577100"
  },
  {
    "text": "during meta-testing? [INAUDIBLE] ",
    "start": "577100",
    "end": "602440"
  },
  {
    "text": "Yeah. So the paragraph is going\nto be a lot more complicated than a one-hot vector. And what it depends-- what\nthe network decides to use",
    "start": "602440",
    "end": "609360"
  },
  {
    "text": "will depend on how complicated\nthe paragraph is versus how complicated it is to discern\nthe task from the training",
    "start": "609360",
    "end": "615090"
  },
  {
    "text": "data. And so it depends on\nbasically whether it's easier for the neural network\nto use the description",
    "start": "615090",
    "end": "622020"
  },
  {
    "text": "or to use the data. And similarly, if in\nterms of generalization",
    "start": "622020",
    "end": "628140"
  },
  {
    "text": "to new tasks, in\nsome ways, it depends on what it decides to\nuse during meta-training. And also the structure\nof that paragraph.",
    "start": "628140",
    "end": "635070"
  },
  {
    "text": "It may be that it can\nactually generalize to the new task using the\nparagraph description if it has",
    "start": "635070",
    "end": "640800"
  },
  {
    "text": "kind of learned sufficiently\ngeneral representations of natural language text. But it could also\nbe that maybe it",
    "start": "640800",
    "end": "648060"
  },
  {
    "text": "sees a new word in the new\nparagraph for the new task that it hasn't seen before. And it can't interpret what\nthe goal of the task is.",
    "start": "648060",
    "end": "656680"
  },
  {
    "text": "OK. So these are two initial\nthought exercises. And to start kind of getting\nyou thinking a little bit.",
    "start": "656680",
    "end": "662850"
  },
  {
    "text": "Now, the key problem that\narises in both of these thought exercises is that the model can\nminimize the meta-training loss",
    "start": "662850",
    "end": "671220"
  },
  {
    "text": "without actually looking\nat the support set or without actually\nlooking at the training data for your task.",
    "start": "671220",
    "end": "677820"
  },
  {
    "text": "And this means that it won't\nactually necessarily learn a learning procedure that\nrelies on that training data.",
    "start": "677820",
    "end": "686050"
  },
  {
    "text": " And so if we go back to how\nwe've been constructing tasks",
    "start": "686050",
    "end": "691810"
  },
  {
    "text": "for few shot\nclassification problems, typically, we'll do it\nsomething like this.",
    "start": "691810",
    "end": "697029"
  },
  {
    "text": "Where for task one,\nwe sample some images. We assign some labels in\nan arbitrary or random way.",
    "start": "697030",
    "end": "703900"
  },
  {
    "text": "But we make sure\nthat the labels are consistent between\ntraining and test or between support and query.",
    "start": "703900",
    "end": "709900"
  },
  {
    "text": "And likewise for\nthe second task, we'll again sample\na new set of labels,",
    "start": "709900",
    "end": "716589"
  },
  {
    "text": "arbitrarily assign\nlabels to images. And we'll do this for\neach of the tasks.",
    "start": "716590",
    "end": "722530"
  },
  {
    "text": "And one thing that\nyou can note here is when we do this\nacross different tasks the piano class, for\nexample, has a label of 4",
    "start": "722530",
    "end": "730860"
  },
  {
    "text": "for the first task. And it has a label of\n3 for the third task. And so this means\nthat when we randomly",
    "start": "730860",
    "end": "738340"
  },
  {
    "text": "assign class labels to\nimage classes for each task, the tasks are\nmutually exclusive.",
    "start": "738340",
    "end": "745339"
  },
  {
    "text": "And what I mean by\nthat is there doesn't exist a single\nneural network that",
    "start": "745340",
    "end": "752000"
  },
  {
    "text": "can classify the images in task\n1 and the images in class 3.",
    "start": "752000",
    "end": "759860"
  },
  {
    "text": "Because you have a fundamentally\ndifferent label being assigned to the same image class.",
    "start": "759860",
    "end": "765720"
  },
  {
    "text": "And as a result of the fact\nthat the tasks are mutually exclusive, it\nmeans that in order to actually solve and\nbe able to minimize",
    "start": "765720",
    "end": "774260"
  },
  {
    "text": "the meta-training loss,\nthey have to actually use the training data. They have to look at\nthe assignment of labels",
    "start": "774260",
    "end": "779840"
  },
  {
    "text": "to images in the\ntraining data in order to accurately make\npredictions on the test",
    "start": "779840",
    "end": "785510"
  },
  {
    "text": "sets or the query sets.  Cool.",
    "start": "785510",
    "end": "790520"
  },
  {
    "text": "Everyone following? OK. ",
    "start": "790520",
    "end": "797920"
  },
  {
    "text": "Now, the third\nthought exercise is what if we assign\nlabels in a way that was consistent across tasks?",
    "start": "797920",
    "end": "804399"
  },
  {
    "text": "And so in particular,\nsay that we assigned the labels like this.",
    "start": "804400",
    "end": "809510"
  },
  {
    "text": "And what I mean\nby consistent is I mean that the image\nclass of piano",
    "start": "809510",
    "end": "815830"
  },
  {
    "text": "or the image class\nof this breed of dog is always going to be the\nsame across all of the tasks.",
    "start": "815830",
    "end": "821899"
  },
  {
    "text": "And so this means that the tasks\nare not mutually exclusive. And a single function could\nsolve all of the tasks.",
    "start": "821900",
    "end": "830400"
  },
  {
    "text": "And so then my\nquestion to you is what would happen in this case?",
    "start": "830400",
    "end": "838410"
  },
  {
    "text": "Yeah? [INAUDIBLE] ",
    "start": "838410",
    "end": "846960"
  },
  {
    "text": "Yeah. So just like the previous\nthought exercises, you would still memorize. And you wouldn't\nactually learn how",
    "start": "846960",
    "end": "853280"
  },
  {
    "text": "to learn because you can just\nlearn a single function that classifies rather than\nrelying on the support data.",
    "start": "853280",
    "end": "860170"
  },
  {
    "text": "Yeah? Wouldn't it also be the case\nfor your prior examples, you would have seen\nin the support set? Would that resolve the problem?",
    "start": "860170",
    "end": "865882"
  },
  {
    "start": "865882",
    "end": "874420"
  },
  {
    "text": "You're saying that--\nyou're asking is it also a problem that for\nthe query images",
    "start": "874420",
    "end": "879970"
  },
  {
    "text": "you would have seen\nthem in the support. Yeah, so for different\ntasks, would that [INAUDIBLE] then for task 3 it is\nin the support set. ",
    "start": "879970",
    "end": "901880"
  },
  {
    "text": "Got it. So the question is in some\ncases like the same image is in the query set in\nsome task-- for some task",
    "start": "901880",
    "end": "907220"
  },
  {
    "text": "in the support set\nfor other tasks. And also this class is\nappearing multiple times.",
    "start": "907220",
    "end": "913160"
  },
  {
    "text": "That's actually not a problem. And I would guess that\nfor most of you the way that you implemented\nhomework one,",
    "start": "913160",
    "end": "919699"
  },
  {
    "text": "you may have actually had\nimages appear sometimes in the support set and\nsometimes in the query set.",
    "start": "919700",
    "end": "925130"
  },
  {
    "text": "You can get a little bit\nmore out of your data if you have it appear in-- have it be allowed to be\nappearing in either the support",
    "start": "925130",
    "end": "931785"
  },
  {
    "text": "set or the query set.  Cool. So in this case,\nthe network can just",
    "start": "931785",
    "end": "938070"
  },
  {
    "text": "learn to classify the inputs\nirrespective of the training set. ",
    "start": "938070",
    "end": "945290"
  },
  {
    "text": "And so in particular\nwhat can happen is that when you train a\nBlack-box meta-learner, it can basically learn\nto just ignore this input",
    "start": "945290",
    "end": "951769"
  },
  {
    "text": "and just classify based\noff of the x-test. Based on the query example.",
    "start": "951770",
    "end": "958310"
  },
  {
    "text": "And likewise even\nfor an optimization based meta-learner, it can also\nlearn to basically ignore this",
    "start": "958310",
    "end": "964310"
  },
  {
    "text": "and primarily rely on just\nthe initial network in order to make predictions.",
    "start": "964310",
    "end": "969905"
  },
  {
    "text": "It's a little bit more\ndifficult in this case 'cause it actually has\nto learn how to ignore the gradient descent step.",
    "start": "969905",
    "end": "975030"
  },
  {
    "text": "But it can figure out-- it can also figure\nout how to do that. Yeah.",
    "start": "975030",
    "end": "980709"
  },
  {
    "text": "[INAUDIBLE] ",
    "start": "980710",
    "end": "993830"
  },
  {
    "text": "Yeah. That's a great question. So the question was one\nthing you may have noted here is that there are actually\nmultiple meanings.",
    "start": "993830",
    "end": "1000810"
  },
  {
    "text": "Like, even though the\nlabeling is consistent, there are multiple\nmeanings for a given label. So like four means both\na carousel and a piano,",
    "start": "1000810",
    "end": "1008709"
  },
  {
    "text": "for example. And so this does mean\nthat the network does need to have enough\ncapacity in order",
    "start": "1008710",
    "end": "1014940"
  },
  {
    "text": "to be able to lump\nthose concepts together. And we'll actually start\nto see some experiments.",
    "start": "1014940",
    "end": "1020670"
  },
  {
    "text": "And this may also start to\nhint towards some solutions to the problem. ",
    "start": "1020670",
    "end": "1029189"
  },
  {
    "text": "Cool. So it can ignore things. And if you actually run--",
    "start": "1029190",
    "end": "1034709"
  },
  {
    "text": "if you run meta-training on this\nexact example that I gave you and then evaluate it on examples\nfrom new classes, its ability",
    "start": "1034710",
    "end": "1043439"
  },
  {
    "text": "to learn from a new\ndata set, the accuracy",
    "start": "1043440",
    "end": "1049529"
  },
  {
    "text": "goes to around 7.8% or 50% in\nthese two different problems.",
    "start": "1049530",
    "end": "1056640"
  },
  {
    "text": "And this is for\nthe MAML algorithm. So it's even something that has\nto use a gradient descent step. And so it's actually running\nfine tuning at test time.",
    "start": "1056640",
    "end": "1063390"
  },
  {
    "text": "It's just not giving you\na very accurate model",
    "start": "1063390",
    "end": "1068430"
  },
  {
    "text": "on these new classes. If you did evaluate it on the\nclasses it saw during training, it would be able to\ngive you good accuracy",
    "start": "1068430",
    "end": "1074640"
  },
  {
    "text": "because those are\nclasses that it saw. ",
    "start": "1074640",
    "end": "1080820"
  },
  {
    "text": "Cool. So then there's this question of\nwe're not shuffling the labels.",
    "start": "1080820",
    "end": "1085950"
  },
  {
    "text": "And we don't shuffle the labels. This is a big problem. Our performance really plummets. But is this actually\na problem in practice.",
    "start": "1085950",
    "end": "1094340"
  },
  {
    "text": "And for image classification,\nwe can just shuffle the labels. It's something that we can do.",
    "start": "1094340",
    "end": "1103080"
  },
  {
    "text": "It's also very easy to do. And so it's not really a\nproblem for image classification problems.",
    "start": "1103080",
    "end": "1109190"
  },
  {
    "text": "Like I mentioned, it's\nalso not a problem if you see the same\nimage classes as what you saw during meta-training.",
    "start": "1109190",
    "end": "1117940"
  },
  {
    "text": "But it's actually going to\nbe a problem in some cases. If you want to be able to\nadapt with data for new tasks.",
    "start": "1117940",
    "end": "1125300"
  },
  {
    "text": "And if you aren't\nlooking at problems exactly like some of the few\nshort classification problems that we saw before and that\nwe've seen in the homeworks.",
    "start": "1125300",
    "end": "1133450"
  },
  {
    "text": "So let's look at a\nfew examples where this may actually be a problem.",
    "start": "1133450",
    "end": "1138830"
  },
  {
    "text": "So the first example\nis a robotics task.",
    "start": "1138830",
    "end": "1145000"
  },
  {
    "text": "So say we want\ndifferent tasks that are going to correspond\nto manipulating objects",
    "start": "1145000",
    "end": "1150100"
  },
  {
    "text": "in different ways. So one task might be\nto close the drawer. Another task might be to pick\nup the hammer and hit a nail.",
    "start": "1150100",
    "end": "1156700"
  },
  {
    "text": "Another task might be to\nstack these two blocks. This seems like a very\nnatural task distribution",
    "start": "1156700",
    "end": "1162880"
  },
  {
    "text": "for a meta-learning system. And maybe kind of the new\ntask, the meta-test task",
    "start": "1162880",
    "end": "1169210"
  },
  {
    "text": "is to close the box. Now, this is a case where\nthis can be a problem.",
    "start": "1169210",
    "end": "1175570"
  },
  {
    "text": "It can be a problem in a\ncouple of different scenarios. One is that if you're doing\nthis from image observations,",
    "start": "1175570",
    "end": "1183440"
  },
  {
    "text": "the system can just look\nat the initial image. See what's in the initial image.",
    "start": "1183440",
    "end": "1189140"
  },
  {
    "text": "If it sees that there is an open\ndraw in the initial image then the task is to close the drawer. Or if it sees that there is\na hammer in front of a nail",
    "start": "1189140",
    "end": "1195950"
  },
  {
    "text": "then the task must be\nto hammer the nail. And so that's one scenario\nwhere it doesn't actually",
    "start": "1195950",
    "end": "1203310"
  },
  {
    "text": "have to meta-learn\nin order to minimize the meta-training loss.",
    "start": "1203310",
    "end": "1208559"
  },
  {
    "text": "Another scenario is\nmaybe we want to-- maybe we aren't in an\nimage based setting. Maybe it just get\nsome way points",
    "start": "1208560",
    "end": "1214020"
  },
  {
    "text": "or maybe the initial image\nis somewhat ambiguous. We might be in a\nscenario where maybe we",
    "start": "1214020",
    "end": "1219930"
  },
  {
    "text": "want to tell it like give it\na language description of what the task is. Like close the drawer\nor hammer the hammer.",
    "start": "1219930",
    "end": "1228029"
  },
  {
    "text": "In those cases, if you also\ngive it a task description which should be helpful. We're giving it\nmore information.",
    "start": "1228030",
    "end": "1233550"
  },
  {
    "text": "That will also\nmake it much harder to generalize to a new task. ",
    "start": "1233550",
    "end": "1242020"
  },
  {
    "text": "So this is one example. One more example. Let's consider a post\nprediction tasks.",
    "start": "1242020",
    "end": "1248970"
  },
  {
    "text": "So say you want to\npredict the orientation of an object in an image. ",
    "start": "1248970",
    "end": "1255919"
  },
  {
    "text": "And so maybe your\nmeta-training task you're predicting the\norientation of a couch. In another case,\nyou're predicting the orientation of- I think\nthis is a very old monitor",
    "start": "1255920",
    "end": "1265600"
  },
  {
    "text": "or a television. And then at meta-test\ntime, you want to be able to predict the\norientation of a chair.",
    "start": "1265600",
    "end": "1273450"
  },
  {
    "text": "This is also a case where\nyou can look at the image and you don't actually need\nthe meta-training data in order",
    "start": "1273450",
    "end": "1279060"
  },
  {
    "text": "to be able to predict\nthe orientation. But if you give it\na new object, it may be ambiguous what\nthe canonical orientation",
    "start": "1279060",
    "end": "1286320"
  },
  {
    "text": "of that object is. And because of that ambiguity,\nit actually-- you do want it to actually\nlook at the training set",
    "start": "1286320",
    "end": "1292740"
  },
  {
    "text": "oh, sorry the support set or the\ntraining set for that test task in order to actually\nfigure out what",
    "start": "1292740",
    "end": "1298769"
  },
  {
    "text": "the canonical orientation\nis and how to predict the orientation of the object. ",
    "start": "1298770",
    "end": "1307140"
  },
  {
    "text": "Cool. So this is another example\nwhere memorization can occur. And in this case,\nit would memorize the canonical orientations\nof the objects seen",
    "start": "1307140",
    "end": "1314880"
  },
  {
    "text": "during meta-training. And it would no longer-- it wouldn't be\nable to figure out",
    "start": "1314880",
    "end": "1320190"
  },
  {
    "text": "what the canonical\norientation of the new objects are because it didn't actually\nlearn to pay attention",
    "start": "1320190",
    "end": "1326820"
  },
  {
    "text": "to the support set. ",
    "start": "1326820",
    "end": "1335260"
  },
  {
    "text": "Cool. So do people agree\nthat this is a problem? ",
    "start": "1335260",
    "end": "1343260"
  },
  {
    "text": "Cool. OK. More or less. Now, there's the question\nof whether we can actually",
    "start": "1343260",
    "end": "1349150"
  },
  {
    "text": "do something about this. And so to think about whether\nwe can do something about it.",
    "start": "1349150",
    "end": "1354950"
  },
  {
    "text": "Let's try to formalize the\nproblem a little bit more. And in particular, we saw\nthat if the tasks are mutually",
    "start": "1354950",
    "end": "1363340"
  },
  {
    "text": "exclusive, then a single\nfunction cannot solve all of the tasks. And this could be due\nto label shuffling.",
    "start": "1363340",
    "end": "1369850"
  },
  {
    "text": "It could be due to\nhiding information. And in this setting, the\nmeta-learning process",
    "start": "1369850",
    "end": "1376300"
  },
  {
    "text": "will not memorize. And you should be able to\ngeneralize to new tasks. ",
    "start": "1376300",
    "end": "1382300"
  },
  {
    "text": "On the flip side, if the tasks\nare not mutually exclusive, then a single function\ncan solve all the tasks.",
    "start": "1382300",
    "end": "1388720"
  },
  {
    "text": "And it can just ignore\nthe training data.",
    "start": "1388720",
    "end": "1393730"
  },
  {
    "text": "And in this latter case\nmemorization can occur. And the reason why\nmemorization can occur",
    "start": "1393730",
    "end": "1399340"
  },
  {
    "text": "is that there's actually\nmultiple solutions to the meta-learning problem. There's multiple ways to\nminimize the meta-training loss",
    "start": "1399340",
    "end": "1406630"
  },
  {
    "text": "function. And in particular, if we view\nthe meta-learning process",
    "start": "1406630",
    "end": "1412070"
  },
  {
    "text": "as taking as input, the training\ndata set and the test input",
    "start": "1412070",
    "end": "1417289"
  },
  {
    "text": "and relying on our\nmeta parameters theta. One solution to the\npose example would",
    "start": "1417290",
    "end": "1423890"
  },
  {
    "text": "be to memorize the\ncanonical pose information in your meta parameters\nand ignore the support set.",
    "start": "1423890",
    "end": "1431059"
  },
  {
    "text": "And then another\nsolution which is one that we would often want\nif we want to generalize",
    "start": "1431060",
    "end": "1436190"
  },
  {
    "text": "is to carry no information\nabout canonical pose information in theta and instead\ntry to acquire it",
    "start": "1436190",
    "end": "1442730"
  },
  {
    "text": "from the task training set. And so here are two solutions.",
    "start": "1442730",
    "end": "1448527"
  },
  {
    "text": "And in general,\nthere's actually going to be an entire spectrum of\nsolutions where you could--",
    "start": "1448527",
    "end": "1453830"
  },
  {
    "text": "based on how the\ninformation flows. So you could store some\ninformation in theta and acquire some information\nfrom the support set.",
    "start": "1453830",
    "end": "1460550"
  },
  {
    "text": "And likewise, basically\nthere's a spectrum of solutions in between these two extremes.",
    "start": "1460550",
    "end": "1466508"
  },
  {
    "text": "Although in practice,\nyou'll probably get either one of\nthese two extremes.",
    "start": "1466508",
    "end": "1472090"
  },
  {
    "text": "Yeah. [INAUDIBLE] has\nsome objects which are not in the [INAUDIBLE]\nhyperparameterization of [INAUDIBLE]",
    "start": "1472090",
    "end": "1486230"
  },
  {
    "text": "Yeah. So the question is if we have\na meta validation set or a set of validation\ntasks, we should be",
    "start": "1486230",
    "end": "1494080"
  },
  {
    "text": "able to tell if our model is\ngeneralizing to new tasks. And as a result, maybe we\ncan tune the hyper-parameters in some way in order\nto actually encourage",
    "start": "1494080",
    "end": "1501580"
  },
  {
    "text": "it to generalize better. And in general,\nabsolutely you'll",
    "start": "1501580",
    "end": "1508169"
  },
  {
    "text": "see that it won't generalize\nto validation tasks. The tricky part is how do\nyou tune the hyper-parameters",
    "start": "1508170",
    "end": "1513810"
  },
  {
    "text": "in order to actually get\nit to generalize versus not generalize. And so you can\ndiagnose this problem",
    "start": "1513810",
    "end": "1521820"
  },
  {
    "text": "with held out validation tasks. But you can't\nnecessarily solve it.",
    "start": "1521820",
    "end": "1527610"
  },
  {
    "text": "You can maybe try to tweak\nthe learning rate in some way to try to get it to\ndo the right thing.",
    "start": "1527610",
    "end": "1534095"
  },
  {
    "text": "But in general,\nit's tricky to do it purely based on learning\nrate or purely based off of like the size\nof the architecture.",
    "start": "1534095",
    "end": "1540690"
  },
  {
    "text": "[INAUDIBLE] ",
    "start": "1540690",
    "end": "1548040"
  },
  {
    "text": "So the question was\nif you pass this into a hyper-parameter\noptimizer, would it be able to\nfix this problem?",
    "start": "1548040",
    "end": "1553770"
  },
  {
    "text": "And it really depends\non the hyper-parameters that you expose to the model.",
    "start": "1553770",
    "end": "1559657"
  },
  {
    "text": "And in particular,\nwe're going to need some form of regularization to\nencourage this to not happen.",
    "start": "1559657",
    "end": "1567000"
  },
  {
    "text": "I guess the other thing that\nI'll mention is in general. In general, I see this solution\nhappen more than this solution.",
    "start": "1567000",
    "end": "1574799"
  },
  {
    "text": " And I think that that's\nbecause acquiring information",
    "start": "1574800",
    "end": "1581070"
  },
  {
    "text": "from D-train can be\nan imperfect process. You actually have to acquire\nit from a more complex object.",
    "start": "1581070",
    "end": "1587230"
  },
  {
    "text": "And as a result,\noftentimes you can actually get a slightly lower\nmeta-training error",
    "start": "1587230",
    "end": "1592350"
  },
  {
    "text": "with this solution than\nwith this solution. ",
    "start": "1592350",
    "end": "1598630"
  },
  {
    "text": "Cool. So if we view this kind\nfrom a standpoint of how",
    "start": "1598630",
    "end": "1605650"
  },
  {
    "text": "the information is\nflowing, we might be able to think about trying to\ncontrol that information flow.",
    "start": "1605650",
    "end": "1611560"
  },
  {
    "text": "And in particular, we think\nof the meta-training process",
    "start": "1611560",
    "end": "1619030"
  },
  {
    "text": "as something like this.",
    "start": "1619030",
    "end": "1624070"
  },
  {
    "text": "And there's a number\nof different-- there's basically\nthree objects that you",
    "start": "1624070",
    "end": "1630210"
  },
  {
    "text": "can use to get a prediction. There is information in theta. Information in D-train.",
    "start": "1630210",
    "end": "1635310"
  },
  {
    "text": "And information in x-test. And we're primarily going\nto focus on the first two.",
    "start": "1635310",
    "end": "1643470"
  },
  {
    "text": "Information in theta and\ninformation in D-train. And when we think about how\nD-train and theta affect",
    "start": "1643470",
    "end": "1654179"
  },
  {
    "text": "the corresponding\nprediction, we can",
    "start": "1654180",
    "end": "1659270"
  },
  {
    "text": "try to think about\nbasically encouraging it to use this information\nmore so than using",
    "start": "1659270",
    "end": "1665030"
  },
  {
    "text": "the information from theta.  Now, one thing that\nwould be nice to do",
    "start": "1665030",
    "end": "1672940"
  },
  {
    "text": "is if we could try to\nmaximize the information. The mutual information\nbetween the support set",
    "start": "1672940",
    "end": "1681460"
  },
  {
    "text": "and the prediction. If we could maximize\nthat mutual information, I think that would\ngive us exactly what we",
    "start": "1681460",
    "end": "1687490"
  },
  {
    "text": "want because then we'd\nbe encouraging it to use information from D-train.",
    "start": "1687490",
    "end": "1693899"
  },
  {
    "text": "And this wouldn't in any way\nbe competing with optimizing the meta-training loss.",
    "start": "1693900",
    "end": "1700080"
  },
  {
    "text": "But unfortunately, optimizing\nmutual information. Like actually estimating\nthis and optimizing it is pretty difficult.\nAnd then so instead",
    "start": "1700080",
    "end": "1708780"
  },
  {
    "text": "what we're going\nto try to do is try to minimize the information\ncoming from theta.",
    "start": "1708780",
    "end": "1714149"
  },
  {
    "text": " And that's something\nthat's easier to do.",
    "start": "1714150",
    "end": "1719360"
  },
  {
    "text": "Of course, if we only\nminimize the information coming from theta then we won't\nbe able to do things well.",
    "start": "1719360",
    "end": "1727480"
  },
  {
    "text": "Yeah? Can you clarify what you mean\nby maximizing mutual information between [INAUDIBLE]?",
    "start": "1727480",
    "end": "1733980"
  },
  {
    "text": "Yeah. So the question\nis, can I clarify what I mean between maximizing\nthe mutual information between the support set and\nthe query set prediction.",
    "start": "1733980",
    "end": "1744680"
  },
  {
    "text": "I guess, I mean that in\nthe mathematical sense. So if you view these as two\ndifferent random variables,",
    "start": "1744680",
    "end": "1750320"
  },
  {
    "text": "then you can define\nthe mutual information between two variables.",
    "start": "1750320",
    "end": "1756670"
  },
  {
    "text": "Like A and B. As I guess there's\na number of different ways to define it. But one way to define it\nis in terms of the entropy",
    "start": "1756670",
    "end": "1765460"
  },
  {
    "text": "of two different variables. Another way to define\nit is the KL divergence between the joint distribution\nand the joint distribution",
    "start": "1765460",
    "end": "1784052"
  },
  {
    "text": "and the kind of the\nproduct of the marginals. And basically,\nmutual information",
    "start": "1784052",
    "end": "1789760"
  },
  {
    "text": "is going to be telling you like\nhow interrelated are these two random variables?",
    "start": "1789760",
    "end": "1795610"
  },
  {
    "text": "And if they're\ncompletely independent then they have zero\nmutual information.",
    "start": "1795610",
    "end": "1802420"
  },
  {
    "text": " And by actually\ntrying to maximize",
    "start": "1802420",
    "end": "1807795"
  },
  {
    "text": "that mutual\ninformation, it means that we want y to actually\nchange when D-train changes.",
    "start": "1807795",
    "end": "1814400"
  },
  {
    "text": "And so if we're able to\nmaximize that quantity, that means that when we change\nD-train y will actually change.",
    "start": "1814400",
    "end": "1820833"
  },
  {
    "text": "And that means that it will\nactually rely on D-train rather than completely\nignoring it. Yeah.",
    "start": "1820833",
    "end": "1826520"
  },
  {
    "text": "[INAUDIBLE] fine tune with\nthe training data set? ",
    "start": "1826520",
    "end": "1835180"
  },
  {
    "text": "Sorry. Can you-- I missed like\none word in the first part, is theta the-- [INAUDIBLE] which has\nthe general information",
    "start": "1835180",
    "end": "1846420"
  },
  {
    "text": "about shared tasks\nand then we're trying to fine-tune with the\ntraining data set that we have.",
    "start": "1846420",
    "end": "1851980"
  },
  {
    "text": "Yeah. So you can think of theta\nas the shared information. Although in this case, I'm\nactually just referring to it",
    "start": "1851980",
    "end": "1857340"
  },
  {
    "text": "as the meta parameters. Like the parameters that you're\noptimizing during the meta training process. ",
    "start": "1857340",
    "end": "1864100"
  },
  {
    "text": "But yeah, when you think\nof it as a random variable, sometimes it can be helpful\nto think of it also in terms of the shared structure.",
    "start": "1864100",
    "end": "1871130"
  },
  {
    "text": "But here really\nI'm referring to it as the meta-training parameters.",
    "start": "1871130",
    "end": "1876700"
  },
  {
    "text": " Cool. So it'd be awesome if we could\nmaximize the mutual information",
    "start": "1876700",
    "end": "1883660"
  },
  {
    "text": "'cause then we'd be telling it\nto rely on D-train to basically make different predictions\nfor different D-train.",
    "start": "1883660",
    "end": "1889720"
  },
  {
    "text": "Unfortunately, it's difficult\nto optimize that quantity. And so instead what\nwe can try to do",
    "start": "1889720",
    "end": "1895299"
  },
  {
    "text": "is to minimize information\ncoming from theta.",
    "start": "1895300",
    "end": "1901480"
  },
  {
    "text": "And of course, we can't\nonly minimize information coming from theta\nbecause if we did",
    "start": "1901480",
    "end": "1907540"
  },
  {
    "text": "that, then we wouldn't store\nany information in theta at all. And we then wouldn't be\nable to do very well.",
    "start": "1907540",
    "end": "1914890"
  },
  {
    "text": "We just basically be learning\nfrom scratch in that case. And so instead what\nwe're going to do",
    "start": "1914890",
    "end": "1920220"
  },
  {
    "text": "is we're going to try to\nminimize the information theta and also minimize the\nmeta-training loss.",
    "start": "1920220",
    "end": "1927480"
  },
  {
    "text": "And we'll need to balance\nthose two terms such that we're getting kind\nof a meta-training loss",
    "start": "1927480",
    "end": "1934590"
  },
  {
    "text": "that we're happy with\nwhile also trying to minimize the information\nin theta as much as we can.",
    "start": "1934590",
    "end": "1943150"
  },
  {
    "text": "Now, the way that we\nactually go about doing this relies on kind of some knowledge\nof what's called an information",
    "start": "1943150",
    "end": "1951510"
  },
  {
    "text": "bottleneck. And we'll actually be covering\ninformation bottlenecks on Monday next week.",
    "start": "1951510",
    "end": "1958630"
  },
  {
    "text": "So I don't want to go too\nmuch into it right now. But the intuition behind\nwhat we'll be doing",
    "start": "1958630",
    "end": "1964080"
  },
  {
    "text": "is we're essentially just going\nto be adding noise to theta.",
    "start": "1964080",
    "end": "1969570"
  },
  {
    "text": "And when you add\nnoise to theta, you're removing information\nfrom what's in theta.",
    "start": "1969570",
    "end": "1977120"
  },
  {
    "text": "You're increasing the entropy\nof that random variable. And so by doing so by\nbasically trying to minimize",
    "start": "1977120",
    "end": "1984020"
  },
  {
    "text": "training loss under a noisy-- minimize meta-training loss\nunder a noisy version of theta",
    "start": "1984020",
    "end": "1989284"
  },
  {
    "text": "that means that you're going to\nbe trying to, basically telling it to be able to\ndo meta-training",
    "start": "1989285",
    "end": "1994370"
  },
  {
    "text": "with less information in theta\nas compared to previously. ",
    "start": "1994370",
    "end": "2000850"
  },
  {
    "text": "And so this kind of\nspecific equation for what this will\nlook like is you have your meta-training\nloss function,",
    "start": "2000850",
    "end": "2007380"
  },
  {
    "text": "that I've just\nwritten out as l here. And the way that you write out\na typical information bottleneck",
    "start": "2007380",
    "end": "2016710"
  },
  {
    "text": "is the KL divergence between\nyour distribution over theta. And some prior\ndistribution, which",
    "start": "2016710",
    "end": "2026340"
  },
  {
    "text": "is just kind of a standard\nGaussian distribution. And the reason why this\ncorresponds to adding noise",
    "start": "2026340",
    "end": "2031350"
  },
  {
    "text": "is Q is going to be a\nGaussian distribution with a mean of theta\nand a variance of kind",
    "start": "2031350",
    "end": "2040080"
  },
  {
    "text": "of a diagonal variance. And so Gaussian\ndistribution-- basically",
    "start": "2040080",
    "end": "2046843"
  },
  {
    "text": "it's a sample from that\nGaussian distribution. It corresponds to adding noise\nto the mean of the Gaussian distribution.",
    "start": "2046843",
    "end": "2052888"
  },
  {
    "text": "And hence why you can think\nabout it as adding noise to the variable. ",
    "start": "2052889",
    "end": "2059648"
  },
  {
    "text": "And so essentially what\nthis is going to do is it's going to try to place\nprecedents on using information from the support set over\nusing information from theta",
    "start": "2059648",
    "end": "2068112"
  },
  {
    "text": "because using\ninformation from data is now going to be\nharder because now you have to use\ninformation from theta",
    "start": "2068112",
    "end": "2073888"
  },
  {
    "text": "after noise has\nbeen added to theta. And you resample\nthe noise every time you use the meta\nparameters theta.",
    "start": "2073889",
    "end": "2080578"
  },
  {
    "start": "2080579",
    "end": "2086260"
  },
  {
    "text": "Yeah? [INAUDIBLE] to calculate,\nactually calculate the [INAUDIBLE] which predicts\nthe task identifier from theta and then minimize it, so\n[INAUDIBLE] so for example you can actually take\nthe labels from like, [INAUDIBLE] you\nknow which option they're coming from, right? So you can have\na loss that tries to disentangle the information? ",
    "start": "2086260",
    "end": "2116350"
  },
  {
    "text": "Yeah. So it's interesting idea. So here we're just\nkind of blindly trying",
    "start": "2116350",
    "end": "2121540"
  },
  {
    "text": "to add noise and reduce\ninformation from theta. Maybe we can try to do it in\na slightly more targeted way",
    "start": "2121540",
    "end": "2126940"
  },
  {
    "text": "by trying to explicitly\nsay you shouldn't be able to predict the memorized\ninformation from theta.",
    "start": "2126940",
    "end": "2134237"
  },
  {
    "text": "You were suggesting\nthat you shouldn't be able to predict the\nkind of the task identifier",
    "start": "2134237",
    "end": "2139240"
  },
  {
    "text": "which wouldn't quite work. The tricky part is that\nlike in the pose example,",
    "start": "2139240",
    "end": "2145940"
  },
  {
    "text": "the memory information\nbasically corresponds to the canonical\norientation of each",
    "start": "2145940",
    "end": "2151330"
  },
  {
    "text": "of the meta-training objects\nand has it for all the objects. ",
    "start": "2151330",
    "end": "2158109"
  },
  {
    "text": "And it's a-- well,\nI don't see any way to try to hit that\ninformation in a targeted way.",
    "start": "2158110",
    "end": "2165200"
  },
  {
    "text": "But we can also discuss it. And if any good\nideas come out of it then it could be\ninteresting to explore. ",
    "start": "2165200",
    "end": "2172650"
  },
  {
    "text": "Yeah? That's your support set\nfrom the meta parameters? And then try to predict the\ntask identifier for that",
    "start": "2172650",
    "end": "2182849"
  },
  {
    "text": "and that's what restrict your\ninformation about the task conversation into\nthe meta-parameters?",
    "start": "2182850",
    "end": "2189180"
  },
  {
    "text": "So you're saying that-- are you saying that\nerase this arrow and have an arrow\nfrom here to here?",
    "start": "2189180",
    "end": "2194580"
  },
  {
    "text": "Or are you saying to add an\narrow from D-train to theta? And then--",
    "start": "2194580",
    "end": "2200820"
  },
  {
    "text": "Just trying to for one\npossible interpretation of what my colleague\nsaid, would be to take your support data point\nand get it through the meta",
    "start": "2200820",
    "end": "2210150"
  },
  {
    "text": "parameters and then convert\nto [INAUDIBLE] learn a neural network that will\ntry to predict the task",
    "start": "2210150",
    "end": "2216210"
  },
  {
    "text": "identifier that would restrict\nyour stored information",
    "start": "2216210",
    "end": "2221520"
  },
  {
    "text": "about tasks in the\nmeta parameters. Yeah. So you could try\nto have something",
    "start": "2221520",
    "end": "2227767"
  },
  {
    "text": "like on top of the activations\nthat basically has it such that it can't predict the\ntask identifier from that.",
    "start": "2227767",
    "end": "2234780"
  },
  {
    "text": "The tricky part is you don't\nwant to restrict information from D-train in\nany way, you only",
    "start": "2234780",
    "end": "2240210"
  },
  {
    "text": "want to restrict it from\ntheta and not from D-train. But maybe we can also discuss it\na little bit more after class.",
    "start": "2240210",
    "end": "2245460"
  },
  {
    "text": " Cool.",
    "start": "2245460",
    "end": "2251400"
  },
  {
    "text": "Now, this regularizer that\nadds noise to weights,",
    "start": "2251400",
    "end": "2259740"
  },
  {
    "text": "it's actually used in other\nparts of deep learning. It's often referred to\nas Bayes' by backprop",
    "start": "2259740",
    "end": "2265310"
  },
  {
    "text": "where you're basically\nimposing a Gaussian distribution on the weights\nof a neural network.",
    "start": "2265310",
    "end": "2270730"
  },
  {
    "text": "And once you define this\nregularizer which is basically just adding noise\nto the weights, you can really combine it with\nyour favorite meta-learning",
    "start": "2270730",
    "end": "2277830"
  },
  {
    "text": "algorithm. And it's applicable to\nany of the algorithms that we've talked\nabout in this class.",
    "start": "2277830",
    "end": "2282915"
  },
  {
    "text": " Cool.",
    "start": "2282915",
    "end": "2288055"
  },
  {
    "text": "So how well does this work? So I mentioned that this\nregularizer is actually-- it's been used in kind of\nstandard deep learning.",
    "start": "2288055",
    "end": "2294823"
  },
  {
    "text": "It hasn't actually\nbeen that successful in standard deep learning. Usually you can get a few\npercentage improvement.",
    "start": "2294823",
    "end": "2302145"
  },
  {
    "text": "But if we look at the\nmeta-learning settings, it actually tends to have\na more drastic effect. So for example, if you\ntake the Omniglot data",
    "start": "2302145",
    "end": "2310373"
  },
  {
    "text": "set which you've been working\nwith in your homework. And you remove the\nlabel shuffling. Such that the labels are\nconsistent across tasks.",
    "start": "2310373",
    "end": "2317970"
  },
  {
    "text": "We'll refer to this\nas non-mutually exclusive omniglot. The original MAML\nalgorithm in this case",
    "start": "2317970",
    "end": "2325859"
  },
  {
    "text": "will get very low performance. There's another\nwork that proposed a form of regularization\nthat may actually be somewhat",
    "start": "2325860",
    "end": "2332470"
  },
  {
    "text": "similar to what was suggested. But it also doesn't\nwork very well.",
    "start": "2332470",
    "end": "2337650"
  },
  {
    "text": "Whereas if you do this approach\nof adding noise to the weights, you're actually able\nto recover a lot",
    "start": "2337650",
    "end": "2344160"
  },
  {
    "text": "of the performance in comparison\nto using label shuffling. ",
    "start": "2344160",
    "end": "2350750"
  },
  {
    "text": "And similarly, on the\npose prediction task which is more interesting\nthan the Omniglot tasks,",
    "start": "2350750",
    "end": "2357589"
  },
  {
    "text": "it's harder to just\nshuffle the labels. If you look at the\nmean squared error between the\npredicted orientation",
    "start": "2357590",
    "end": "2365660"
  },
  {
    "text": "and the correct\norientation, we see that we can get a much\nlower mean squared error when incorporating this\nform of meta regularization.",
    "start": "2365660",
    "end": "2376010"
  },
  {
    "text": "And you see an improvement both\nwith the MAML algorithm as well as an algorithm called\nconditional neural processes",
    "start": "2376010",
    "end": "2382100"
  },
  {
    "text": "which are a form of a\nBlack-box meta-learner. ",
    "start": "2382100",
    "end": "2387390"
  },
  {
    "text": "Yeah? If we-- [INAUDIBLE]\nweights of the network.",
    "start": "2387390",
    "end": "2396380"
  },
  {
    "text": "How would this\ncompare to just like essentially keeping the earlier\nlayers of the network fixed",
    "start": "2396380",
    "end": "2402420"
  },
  {
    "text": "but then distributing it slicing\nthrough us layer setting up a completely random.",
    "start": "2402420",
    "end": "2407440"
  },
  {
    "text": "So then you can\nreally took it out of the train of like the\nfinal prediction task",
    "start": "2407440",
    "end": "2414690"
  },
  {
    "text": "but it's still like\nlearning the picture. So it's like the\ncoordinates in that sense? So the question is, how\nwould-- so this is adding noise",
    "start": "2414690",
    "end": "2421770"
  },
  {
    "text": "to all the weights. So how would this\ncompare to kind of meta training the earlier\nlayers as normal but then",
    "start": "2421770",
    "end": "2427710"
  },
  {
    "text": "like always re-initializing\nthe last layer. And you'd also do that\nduring meta-training. And then fully train the\nlast layer in the inner loop.",
    "start": "2427710",
    "end": "2434340"
  },
  {
    "text": "Yeah. So I think that something\nlike that would--",
    "start": "2434340",
    "end": "2442000"
  },
  {
    "text": " off the top of my head I\nthink that something like that would work.",
    "start": "2442000",
    "end": "2447500"
  },
  {
    "text": "In general, things like\nprototypical networks actually often don't suffer from\nthis memorization problem",
    "start": "2447500",
    "end": "2452777"
  },
  {
    "text": "because you're forcing it\nto do nearest neighbors. It has to use that\nlearning algorithm. And it's much more difficult\nto kind of memorize how to map",
    "start": "2452777",
    "end": "2462740"
  },
  {
    "text": "from inputs to labels\nwhen you literally have to do these nearest\nneighbor comparisons.",
    "start": "2462740",
    "end": "2468260"
  },
  {
    "text": "And so given the\nrelation to things like prototypical\nnetworks, I would expect that to work\nespecially for classification.",
    "start": "2468260",
    "end": "2474033"
  },
  {
    "text": "I might have to think about it a\nlittle bit more for regression. But I suspect that\nsomething like that",
    "start": "2474033",
    "end": "2479240"
  },
  {
    "text": "might work because\nyou're forcing it to learn part of the network\nfrom scratch in the inner loop.",
    "start": "2479240",
    "end": "2489170"
  },
  {
    "text": "But I may also need to\nthink about it more a bit. Yeah. Yeah?",
    "start": "2489170",
    "end": "2494188"
  },
  {
    "text": "What's the intuition\nI guess between adding noise versus something like\ndropout guess a similar thing?",
    "start": "2494188",
    "end": "2499950"
  },
  {
    "text": "Yeah. So the question is, what's the\ndifference between adding noise versus something like dropout? So in this paper\nwe actually looked",
    "start": "2499950",
    "end": "2508790"
  },
  {
    "text": "at adding noise to the\nweights and also adding noise to the activations. And something like\ndropout is going to be a lot more\nsimilar to adding noise",
    "start": "2508790",
    "end": "2515107"
  },
  {
    "text": "to the activations. And we found that adding\nnoise to the activations",
    "start": "2515107",
    "end": "2520790"
  },
  {
    "text": "actually worked in some\ncases but didn't work well in all cases. And so the reason why there's\na W here is it kind of",
    "start": "2520790",
    "end": "2527427"
  },
  {
    "text": "refers to a diagnosis of the\nweights versus adding noise to the activations. And so you can see the\nactivations results in the paper.",
    "start": "2527427",
    "end": "2535460"
  },
  {
    "text": "The part of the challenge with\nadding noise to the activations or doing something like\ndropout is it can still--",
    "start": "2535460",
    "end": "2545710"
  },
  {
    "text": "I guess it depends on which\nlayer you add noise to. But essentially say you\nhave a neural network that",
    "start": "2545710",
    "end": "2558069"
  },
  {
    "text": "is making predictions about\na corresponding input. ",
    "start": "2558070",
    "end": "2564970"
  },
  {
    "text": "So then say you decide\nto like I don't know, try to add noise here or maybe\nyou can even just add noise.",
    "start": "2564970",
    "end": "2572380"
  },
  {
    "text": "Let's say you add\nnoise right here. You're basically just\ntrying to minimize and have less information here.",
    "start": "2572380",
    "end": "2579670"
  },
  {
    "text": "But what the network\ncan do is especially for classification\ntasks, the amount",
    "start": "2579670",
    "end": "2585040"
  },
  {
    "text": "of information in a\nlabel is very small. If you're doing like 10\nway classification then",
    "start": "2585040",
    "end": "2591250"
  },
  {
    "text": "it's just log 10. And so if you're trying\nto minimize information",
    "start": "2591250",
    "end": "2597819"
  },
  {
    "text": "and activations, it can\nactually minimize information just by basically\nstuffing the label here.",
    "start": "2597820",
    "end": "2603473"
  },
  {
    "text": "And that's very, very\nlittle information that it needs to store. And that's actually\nless information than what it needs to memorize.",
    "start": "2603473",
    "end": "2610070"
  },
  {
    "text": "And so I don't know if that\nwas the most clear explanation. But basically, it's the place\nwhere it's memorizing is really",
    "start": "2610070",
    "end": "2620859"
  },
  {
    "text": "in the weights of this\nneural network and not in the activations of\nthis neural network.",
    "start": "2620860",
    "end": "2626545"
  },
  {
    "text": "But the short answer is\nit works a lot better on the weights than\nthe activations. ",
    "start": "2626545",
    "end": "2635190"
  },
  {
    "text": "Cool. And then yeah, I\nguess we all have",
    "start": "2635190",
    "end": "2640290"
  },
  {
    "text": "drop out in these comparisons. But you can also\njust try applying things like weight decay or a\nversion of Bayes by backprop.",
    "start": "2640290",
    "end": "2649559"
  },
  {
    "text": "And we see that it\nworks much better. It works much better\nwith the regularization",
    "start": "2649560",
    "end": "2655839"
  },
  {
    "text": "that we've talked about. ",
    "start": "2655840",
    "end": "2660920"
  },
  {
    "text": "Cool. Now, one thing that I'd\nlike to briefly mention for people who are more\ntheoretically inclined",
    "start": "2660920",
    "end": "2667220"
  },
  {
    "text": "and want to dig a\nlittle bit deeper into this form of\nregularizer, you",
    "start": "2667220",
    "end": "2672940"
  },
  {
    "text": "could ask like we've seen\nin practice it really improves generalization a lot. But does it actually do so kind\nof in a more provable sense?",
    "start": "2672940",
    "end": "2681520"
  },
  {
    "text": "And there's a way\nto kind of basically derive a generalization\nbound for this approach.",
    "start": "2681520",
    "end": "2690220"
  },
  {
    "text": "And it looks rather complicated. Something like this. But basically what\nthis amounts to is",
    "start": "2690220",
    "end": "2695293"
  },
  {
    "text": "we're trying to bound\nthe generalization error or basically how well will\nthis meta-training algorithm generalize to new\nmeta test tasks.",
    "start": "2695293",
    "end": "2702770"
  },
  {
    "text": "So that's the left hand side. The right hand side is\na bound on the error. And so this corresponds to the\nerror on the meta-training set",
    "start": "2702770",
    "end": "2710920"
  },
  {
    "text": "plus the generalization gap. And this term in the\ngeneralization gap",
    "start": "2710920",
    "end": "2716500"
  },
  {
    "text": "is exactly the\nmeta regularization that we're applying when we\nadd noise to the weights.",
    "start": "2716500",
    "end": "2725170"
  },
  {
    "text": "So really the gist of this\nis that you can actually",
    "start": "2725170",
    "end": "2730760"
  },
  {
    "text": "theoretically show that you can\nbound the generalization error. And actually improve\ngeneralization.",
    "start": "2730760",
    "end": "2736280"
  },
  {
    "text": "Or at least you bound\non the generalization by using this form\nof regularization.",
    "start": "2736280",
    "end": "2742680"
  },
  {
    "text": "Cool. So to summarize the\nmemorization problem. We talked about this--",
    "start": "2742680",
    "end": "2750420"
  },
  {
    "text": "basically this\nform of overfitting in meta-learning where\nyou're memorizing",
    "start": "2750420",
    "end": "2755609"
  },
  {
    "text": "a mapping from x to y. You're memorizing a function.",
    "start": "2755610",
    "end": "2761099"
  },
  {
    "text": "And basically you're\nmemorizing the x to y function rather than\nactually using the training",
    "start": "2761100",
    "end": "2766332"
  },
  {
    "text": "data rather than\nlearning to learn.  And this is analogous\nto standard overfitting",
    "start": "2766332",
    "end": "2773400"
  },
  {
    "text": "in supervised learning\nwhere you instead of memorizing functions, you're\nmemorizing individual training",
    "start": "2773400",
    "end": "2778920"
  },
  {
    "text": "data points.  And similarly you can sort of\nthink of meta regularization",
    "start": "2778920",
    "end": "2787100"
  },
  {
    "text": "as also being analogous\nto standard regularization in supervised learning. ",
    "start": "2787100",
    "end": "2792527"
  },
  {
    "text": "Although in supervised\nlearning you're kind of regularizing\nthe hypothesis class. You can also think of\nthis as regularizing",
    "start": "2792527",
    "end": "2798610"
  },
  {
    "text": "the hypothesis class\nof the meta-learner. And this is a particular\ninterpretation of controlling",
    "start": "2798610",
    "end": "2805180"
  },
  {
    "text": "information flow. And you can view the kind of\nthe specific meta regularization",
    "start": "2805180",
    "end": "2811839"
  },
  {
    "text": "that we talked about as trying\nto minimize the description length of the meta parameters.",
    "start": "2811840",
    "end": "2818170"
  },
  {
    "text": "And so these are also\nsomewhat analogous. Although, like I briefly\nmentioned before,",
    "start": "2818170",
    "end": "2824240"
  },
  {
    "text": "we actually see a much\nmore drastic improvement from applying this\nregularization",
    "start": "2824240",
    "end": "2829359"
  },
  {
    "text": "in some meta learning\nproblems in comparison to standard regularization\nin deep learning. ",
    "start": "2829360",
    "end": "2838370"
  },
  {
    "text": "Any questions on memorization. Yeah? You mean like\ndescription length? That's a great question.",
    "start": "2838370",
    "end": "2843990"
  },
  {
    "text": "Do I want to go into that? I guess I put it on the slide. So description length is--",
    "start": "2843990",
    "end": "2852630"
  },
  {
    "text": " maybe I shouldn't have\nput it on the side.",
    "start": "2852630",
    "end": "2857670"
  },
  {
    "text": "I think I'd rather\nnot go into it now",
    "start": "2857670",
    "end": "2862920"
  },
  {
    "text": "but happy to talk about\nit in office hours. And if you're interested\nin learning more, you can read up on the\nMDL principle which",
    "start": "2862920",
    "end": "2869760"
  },
  {
    "text": "is the minimum description\nlength principle and Kolmogorov complexity. And I guess the\ngist of it is it's",
    "start": "2869760",
    "end": "2876330"
  },
  {
    "text": "thinking about the\nlength of the program that it takes in order\nto express a function.",
    "start": "2876330",
    "end": "2883020"
  },
  {
    "text": "The length of the\nminimum program. Yeah? Isn't that essentially\njust the amount of information that's stored?",
    "start": "2883020",
    "end": "2889440"
  },
  {
    "text": "Yeah. Yeah. Basically it's a\nmeasure of information. ",
    "start": "2889440",
    "end": "2897405"
  },
  {
    "text": "Cool. OK. So we've talked about a problem\nthat comes up if you construct",
    "start": "2897405",
    "end": "2903589"
  },
  {
    "text": "tasks in a certain way. And I'd encourage you to for\nthose of you that are doing projects along the lines\nof few-shot meta learning,",
    "start": "2903590",
    "end": "2910310"
  },
  {
    "text": "I'd encourage yourself\nto kind of ask yourself if your project--",
    "start": "2910310",
    "end": "2915710"
  },
  {
    "text": "if this sort of issue might\ncome up in your project. ",
    "start": "2915710",
    "end": "2921020"
  },
  {
    "text": "And if so you can maybe\nconsider revising your project or think about the\nregularization techniques that we've talked about\nor possibly develop",
    "start": "2921020",
    "end": "2928390"
  },
  {
    "text": "a new regularization technique. Cool. And then the second part\nof this lecture, I'd",
    "start": "2928390",
    "end": "2935310"
  },
  {
    "text": "like to talk a little\nbit about tasks. Well, another form of\ntask construction which is",
    "start": "2935310",
    "end": "2941730"
  },
  {
    "text": "unsupervised task construction. And earlier in this\nlecture, we revisited",
    "start": "2941730",
    "end": "2947520"
  },
  {
    "text": "a few different examples\nof classification, imitation learning, and\nland cover classification.",
    "start": "2947520",
    "end": "2956795"
  },
  {
    "text": "And these are all\nscenarios where we assume that we\nhave labeled data from lots of different tasks.",
    "start": "2956795",
    "end": "2962940"
  },
  {
    "text": "And there might be\nscenarios where we only have unlabeled data. Or maybe we have a\nmix of unlabeled data",
    "start": "2962940",
    "end": "2968190"
  },
  {
    "text": "and labeled data. Last week, we\ntalked about a class of approaches that can handle\nthe setting where we only",
    "start": "2968190",
    "end": "2975690"
  },
  {
    "text": "have labeled data\nduring pre-training. And we did this with different\nunsupervised pre-training",
    "start": "2975690",
    "end": "2980730"
  },
  {
    "text": "algorithms like\ncontrastive learning and like masked autoencoders.",
    "start": "2980730",
    "end": "2986220"
  },
  {
    "text": "And we pre-trained\nour representation. And then fine tuned something\non top of that representation.",
    "start": "2986220",
    "end": "2991740"
  },
  {
    "text": " And today, we're going to talk\nabout algorithms that actually",
    "start": "2991740",
    "end": "2997860"
  },
  {
    "text": "end up looking very similar\nto those previous algorithms except instead of just\nrunning fine tuning on top",
    "start": "2997860",
    "end": "3003410"
  },
  {
    "text": "of the representation,\nwe're actually going to be doing\nexplicit meta-learning and doing the same kind\nof few-shot learning",
    "start": "3003410",
    "end": "3008630"
  },
  {
    "text": "that we've seen in algorithms\nlike Black-box meta-learning and non-parametric\nmethods and so forth.",
    "start": "3008630",
    "end": "3014090"
  },
  {
    "text": " And the general recipe\nfor this algorithm",
    "start": "3014090",
    "end": "3025130"
  },
  {
    "text": "is going to be something like\nwe are given an unlabeled data set then we will propose\ntasks from that data set.",
    "start": "3025130",
    "end": "3031829"
  },
  {
    "text": "And then run meta-learning\non the task that we proposed. And this is going to look\na lot like the recipe",
    "start": "3031830",
    "end": "3038720"
  },
  {
    "text": "that we saw for contrastive\nlearning as well.",
    "start": "3038720",
    "end": "3044359"
  },
  {
    "text": "Where we are basically\nconstructing tasks using different\naugmentation functions. ",
    "start": "3044360",
    "end": "3051589"
  },
  {
    "text": "Of course, the key step\nhere is this middle step. It's easy to get-- well,\ngiven unlabeled data",
    "start": "3051590",
    "end": "3056720"
  },
  {
    "text": "is just a given\nrunning meta-learning is what we've already talked\nabout over the past few weeks.",
    "start": "3056720",
    "end": "3062279"
  },
  {
    "text": "So the big question is,\nhow do we propose tasks? And yeah, our goal will be to\ntry to automatically construct",
    "start": "3062280",
    "end": "3070200"
  },
  {
    "text": "tasks from unlabeled data. Now, I'm curious if\nanyone has thoughts",
    "start": "3070200",
    "end": "3075690"
  },
  {
    "text": "on if you were to try\nto kind of set out some desiderata\nfor a set of tasks,",
    "start": "3075690",
    "end": "3081420"
  },
  {
    "text": "do you have thoughts on what\nyou would want generally that task set to look like? ",
    "start": "3081420",
    "end": "3104180"
  },
  {
    "text": "I feel like it depends on the\ndata that we're dealing with. But for the case of\nimages, for example,",
    "start": "3104180",
    "end": "3112330"
  },
  {
    "text": "I think it will be\nsimilar to contrastive, as in similar instances\nshould be like the same class.",
    "start": "3112330",
    "end": "3118810"
  },
  {
    "text": "And then I would like to\nhave a diverse set of labels",
    "start": "3118810",
    "end": "3124233"
  },
  {
    "text": "for my classes. I don't know.  Yeah. So it's going to depend a\nbit on the data definitely.",
    "start": "3124233",
    "end": "3132130"
  },
  {
    "text": "And also it's going to\ndepend a bit on the tasks. So if at meta test\ntime you expect to be doing a 10 way\nclassification task,",
    "start": "3132130",
    "end": "3139100"
  },
  {
    "text": "then the tasks you construct\nduring this process will be different\nthan if you think you're going to be doing like\n1,000 way classification.",
    "start": "3139100",
    "end": "3145222"
  },
  {
    "text": "And so you want to\nconstruct things that look similar\nto what you are going to see at meta-test time. You can also imagine\ndoing something",
    "start": "3145222",
    "end": "3150730"
  },
  {
    "text": "similar to contrastive learning. Yeah? It's kind of how you\ndo the [INAUDIBLE]..",
    "start": "3150730",
    "end": "3157890"
  },
  {
    "text": "So like they should\nbe prominent enough in the data set to be like\nto individually be classified",
    "start": "3157890",
    "end": "3163690"
  },
  {
    "text": "as a cluster and the difference\nbetween those clusters or [INAUDIBLE] that\nis really prominent.",
    "start": "3163690",
    "end": "3169418"
  },
  {
    "text": "Did you look at the next slide? [INAUDIBLE]",
    "start": "3169418",
    "end": "3175490"
  },
  {
    "text": "Yeah. So to repeat the-- I guess the--\nmaybe to summarize,",
    "start": "3175490",
    "end": "3182490"
  },
  {
    "text": "maybe we want the\ntask to be something like what you get from\nk means to be somewhat distinct from each other.",
    "start": "3182490",
    "end": "3187650"
  },
  {
    "text": "While also being\nsomewhat diverse I guess. So yeah, what I\nput on the slides",
    "start": "3187650",
    "end": "3194480"
  },
  {
    "text": "is I think that we really\nwant two key criteria. One is that we want the\ntask to be pretty diverse.",
    "start": "3194480",
    "end": "3202405"
  },
  {
    "text": "We want them--\nand the reason why we want this is\nthat we want them to be able to cover the test\ntask that we are likely to see",
    "start": "3202405",
    "end": "3209630"
  },
  {
    "text": "during meta test time. And of course, if\nyou know information about what you're going to\nsee about meta-test time,",
    "start": "3209630",
    "end": "3215580"
  },
  {
    "text": "like this 10 way versus 1,000\nway you can incorporate that and actually make it\nless diverse to make it more similar to that.",
    "start": "3215580",
    "end": "3221203"
  },
  {
    "text": "And then the second\nthing you want is you want it to\nhave some structure. So if you propose\ncompletely random tasks then",
    "start": "3221203",
    "end": "3229520"
  },
  {
    "text": "your meta-learner\nwon't actually be able to learn those tasks\nwith a few examples. And this maybe gets\na little bit up the k",
    "start": "3229520",
    "end": "3235640"
  },
  {
    "text": "means comment a\nlittle bit, which is that if you cluster data\nkind of intuitively what that's",
    "start": "3235640",
    "end": "3242600"
  },
  {
    "text": "doing is finding the\nstructure underlying the data. And we can try to\nleverage that structure when during the\nmeta-learning process.",
    "start": "3242600",
    "end": "3251474"
  },
  {
    "text": "Yeah. [INAUDIBLE] Is there a way to formally\ndefine covering the test tasks?",
    "start": "3251475",
    "end": "3258395"
  },
  {
    "text": " It's easy to define if you can\nrepresent the task distribution",
    "start": "3258395",
    "end": "3267700"
  },
  {
    "text": "in a parametric manner. If there is some parameter\nunderlying the task",
    "start": "3267700",
    "end": "3273610"
  },
  {
    "text": "distribution, then\nyou can say is this in the support\nof the distribution.",
    "start": "3273610",
    "end": "3278750"
  },
  {
    "text": "If the tasks are not\npossible to describe in a parametric\nmanner, if they're more non-parametric like one\ntask is to pick up a shoe",
    "start": "3278750",
    "end": "3286119"
  },
  {
    "text": "and another test\nis to push a chair. Like maybe you can parameterize\nthe shape of the chair and the shape of the shoe.",
    "start": "3286120",
    "end": "3292123"
  },
  {
    "text": "But in general, that's\nsomething that's much harder to characterize. ",
    "start": "3292123",
    "end": "3300010"
  },
  {
    "text": "Cool. So we'll talk about\nhow we might try to cover diverse and\nstructured tasks,",
    "start": "3300010",
    "end": "3307010"
  },
  {
    "text": "both for image\ndata and text data. This is really the\nmain overview slide.",
    "start": "3307010",
    "end": "3312490"
  },
  {
    "text": "And then on the next\nfew slides, we'll basically just go through\na few examples for how",
    "start": "3312490",
    "end": "3317560"
  },
  {
    "text": "and walk through examples\nfor constructing tasks that maybe look like this. And also like how\nthese algorithms",
    "start": "3317560",
    "end": "3323510"
  },
  {
    "text": "are used in practice.  So the first example\nwill actually",
    "start": "3323510",
    "end": "3329510"
  },
  {
    "text": "be using something a lot\nlike k means clustering. And in particular, we're going\nto be doing this with images.",
    "start": "3329510",
    "end": "3336810"
  },
  {
    "text": "And if you just have\nimage observations, it's hard to run\nclustering on raw pixels.",
    "start": "3336810",
    "end": "3343490"
  },
  {
    "text": "And so what we'll first\ndo is run unsupervised learning in order to get\nsome embedding space, a lower dimensional embedding space.",
    "start": "3343490",
    "end": "3351050"
  },
  {
    "text": "And once we have this lower\ndimensional embedding space, it's going to be much\neasier to construct tasks.",
    "start": "3351050",
    "end": "3357550"
  },
  {
    "text": "And in particular, if we want to\nconstruct a classification task then we can find\ndifferent clusters",
    "start": "3357550",
    "end": "3364030"
  },
  {
    "text": "in this embedding space. And define a task as\ndiscriminating between examples",
    "start": "3364030",
    "end": "3369400"
  },
  {
    "text": "from those clusters. So for example, one three\nway classification task",
    "start": "3369400",
    "end": "3375190"
  },
  {
    "text": "would to be able to classify\nbetween purple examples, green examples, and\norange examples.",
    "start": "3375190",
    "end": "3381750"
  },
  {
    "text": "And another task\nmight be to classify between the red dashed cluster\nand the yellow dashed cluster.",
    "start": "3381750",
    "end": "3390970"
  },
  {
    "text": "And by running clustering\nto get these tasks,",
    "start": "3390970",
    "end": "3396590"
  },
  {
    "text": "we're going to get parts of the\nspace that have a little bit more structure. In contrast we\njust randomly kind",
    "start": "3396590",
    "end": "3402270"
  },
  {
    "text": "of sliced like hyper-planes\nlike decision boundaries through the space. So we may end up kind of getting\nmuch less structure because you",
    "start": "3402270",
    "end": "3410720"
  },
  {
    "text": "may get kind of two examples\nthat are very close together but that actually\nhave different labels. ",
    "start": "3410720",
    "end": "3417470"
  },
  {
    "text": "And so in particular,\nto propose a task. Once we have this embedding\nspace we first sample.",
    "start": "3417470",
    "end": "3425680"
  },
  {
    "text": "If you want to do a two\nway classification task, we sample two clusters. So like the red cluster\nand the blue cluster.",
    "start": "3425680",
    "end": "3431940"
  },
  {
    "text": "We sample two images\nfrom those two clusters. So the blue image\nand the red image.",
    "start": "3431940",
    "end": "3438210"
  },
  {
    "text": "And then that will give us\na support set for that task. And then to get a query\nset, we then sample",
    "start": "3438210",
    "end": "3445380"
  },
  {
    "text": "two additional examples\nfrom each of those clusters and get a corresponding\nquery set for that task.",
    "start": "3445380",
    "end": "3454869"
  },
  {
    "text": "And then once we go and\ntry to sample another task, we can simply sample\nanother set of clusters",
    "start": "3454870",
    "end": "3459940"
  },
  {
    "text": "like the purple cluster\nand the green cluster. And then if we\nwanted to sample--",
    "start": "3459940",
    "end": "3465105"
  },
  {
    "text": "these are two way\nclassification tasks. If we wanted to do an N way\nclassification task then we could sample end clusters and\ndiscriminate between examples",
    "start": "3465105",
    "end": "3471530"
  },
  {
    "text": "from those N clusters. And you could note that\nthese tasks don't correspond",
    "start": "3471530",
    "end": "3479869"
  },
  {
    "text": "to exactly\ncategorizing different like typical image categories. But they still have a\nlot of structure to them.",
    "start": "3479870",
    "end": "3486600"
  },
  {
    "text": "For example, the\nfirst task corresponds to classifying if something\nis like a circular shape",
    "start": "3486600",
    "end": "3492890"
  },
  {
    "text": "versus if you have\na pair of objects. And the second is maybe\nclassifying wine bottles",
    "start": "3492890",
    "end": "3499340"
  },
  {
    "text": "or maybe bottles of\nbeverages and something",
    "start": "3499340",
    "end": "3504590"
  },
  {
    "text": "that also has a\nparticular kind of shape. ",
    "start": "3504590",
    "end": "3511260"
  },
  {
    "text": "Cool. And then once we have\ndefined these tasks, we just run meta-learning\non those tasks. ",
    "start": "3511260",
    "end": "3518760"
  },
  {
    "text": "So this is one way to get\ntasks that are fairly diverse and have are fairly structured.",
    "start": "3518760",
    "end": "3524720"
  },
  {
    "text": "And the result of this process\nshould be a representation",
    "start": "3524720",
    "end": "3529910"
  },
  {
    "text": "or a meta-learning model\nthat is particularly well suited for downstream\nfew shot classification tasks.",
    "start": "3529910",
    "end": "3536965"
  },
  {
    "text": "And it should be\nsomething that's actually better than the\ninitial representation that we used in step one.",
    "start": "3536965",
    "end": "3543710"
  },
  {
    "text": "And the reason for that\nis we're explicitly going to be optimizing for\nfew-shot generalization",
    "start": "3543710",
    "end": "3549890"
  },
  {
    "text": "in these examples. And kind of few-shot\nN way classification depending on whatever form\nof N way classification",
    "start": "3549890",
    "end": "3558410"
  },
  {
    "text": "we set up in step two.  And so there's a\nfew different ways",
    "start": "3558410",
    "end": "3564040"
  },
  {
    "text": "to actually instantiate\nthe different steps here. So the first step\nwe could use some",
    "start": "3564040",
    "end": "3569319"
  },
  {
    "text": "unsupervised learning methods. This paper is a\ncouple of years old. So it's going to use some\nunsupervised learning methods",
    "start": "3569320",
    "end": "3575807"
  },
  {
    "text": "that are a couple of years old. But you can also plug in\nsomething more recent as well. Clustering to\nautomatically construct",
    "start": "3575808",
    "end": "3582340"
  },
  {
    "text": "tasks for unsupervised\nmeta-learning or CACTUs. And then you can combine this\nwith a meta-learning algorithm",
    "start": "3582340",
    "end": "3589210"
  },
  {
    "text": "that operates on those tasks\nlike MAML or prototypical networks.",
    "start": "3589210",
    "end": "3596060"
  },
  {
    "text": "And then we can look\nat some of the results. So we looked at\nusing this approach",
    "start": "3596060",
    "end": "3604309"
  },
  {
    "text": "to construct tasks\nin combination with both MAML and\nprototypical networks. And so if you combine\nCACTUs with MAML,",
    "start": "3604310",
    "end": "3610940"
  },
  {
    "text": "this is the result that you get. And we evaluate on\nminiImageNet, which",
    "start": "3610940",
    "end": "3618320"
  },
  {
    "text": "if you give full labels\nyou get an accuracy of 62%.",
    "start": "3618320",
    "end": "3624050"
  },
  {
    "text": "If you just do-- I guess you can first evaluate\na few different unsupervised baselines, which\njust corresponded",
    "start": "3624050",
    "end": "3630980"
  },
  {
    "text": "to doing k-nearest neighbors\nlogistic regression or an MLP on top of the\npre-trained representation.",
    "start": "3630980",
    "end": "3637940"
  },
  {
    "text": "And you get a few-shot accuracy\nthat's pretty low around 30%.",
    "start": "3637940",
    "end": "3643950"
  },
  {
    "text": "You can also do something\nthat uses clustering. And then contrast,\nif you actually",
    "start": "3643950",
    "end": "3650210"
  },
  {
    "text": "run this approach to\nconstruct the tasks and do meta-learning\non top of those tasks,",
    "start": "3650210",
    "end": "3655310"
  },
  {
    "text": "you get an accuracy that's\nmuch higher, around 51%. And if you use a\nrepresentation that's",
    "start": "3655310",
    "end": "3661823"
  },
  {
    "text": "a bit better, the deep\ncluster representation, you actually do even\nbetter than that. ",
    "start": "3661823",
    "end": "3669060"
  },
  {
    "text": "Yeah.  [INAUDIBLE] ",
    "start": "3669060",
    "end": "3683838"
  },
  {
    "text": "Oh, are you specifically\nreferring to this method right here? Yeah. [INAUDIBLE] ",
    "start": "3683838",
    "end": "3694737"
  },
  {
    "text": "Yeah, so basically all\nfour of these methods correspond to\ndifferent ways of using the unsupervised\nrepresentation on its own.",
    "start": "3694737",
    "end": "3702180"
  },
  {
    "text": "And basically using different\nclassifiers or something on top of that.",
    "start": "3702180",
    "end": "3708295"
  },
  {
    "text": "Yeah. So are there labels at\ntest time, meta test time? ",
    "start": "3708295",
    "end": "3713365"
  },
  {
    "text": "Oh, awesome question. Yeah, are there labels\nat meta-test time? Yes. So in this case, there\nare 25 labeled examples",
    "start": "3713365",
    "end": "3721780"
  },
  {
    "text": "at meta-test time. So the meta-training side\nis completely unlabeled. And then at meta-test\ntime you run",
    "start": "3721780",
    "end": "3727050"
  },
  {
    "text": "it the same exact\nway as kind of normal labeled meta-training time.",
    "start": "3727050",
    "end": "3733020"
  },
  {
    "text": "Where you are given a very\nsmall training data set. In this case like 25\nexamples, five examples",
    "start": "3733020",
    "end": "3739349"
  },
  {
    "text": "of five different classes. [INAUDIBLE] at\nmeta training time? ",
    "start": "3739350",
    "end": "3745630"
  },
  {
    "text": "Yeah. So basically at\nmeta train time we are kind of creating\nour own labels",
    "start": "3745630",
    "end": "3750690"
  },
  {
    "text": "and meta-training on that\nand then at meta test time we're using real labels.",
    "start": "3750690",
    "end": "3755990"
  },
  {
    "text": "Yeah? View this process\nas first you have the unsupervised projection\ninto a low dimensional space.",
    "start": "3755990",
    "end": "3763920"
  },
  {
    "text": "And then each task is\nlearning a distortion of that space that moves\nsome images closer together",
    "start": "3763920",
    "end": "3771080"
  },
  {
    "text": "and other images farther away. And then if so can\nyou characterize the kinds of\ndistortions that can",
    "start": "3771080",
    "end": "3777320"
  },
  {
    "text": "be learned via this process. Like there's kind of a family\nof allowable transformations.",
    "start": "3777320",
    "end": "3783500"
  },
  {
    "text": "Yeah. So you can definitely view this\nas like distorting the space",
    "start": "3783500",
    "end": "3788540"
  },
  {
    "text": "in different ways. And then learning like\nlearning to classify for those different distortions.",
    "start": "3788540",
    "end": "3796160"
  },
  {
    "text": "You're asking are there\nways to kind of characterize the distortions that\ncould be represented?",
    "start": "3796160",
    "end": "3801559"
  },
  {
    "text": "Yeah. My question is it seems\nlike the adaptability that",
    "start": "3801560",
    "end": "3807050"
  },
  {
    "text": "depends a lot on the initial\nunsupervised clustering. Like you really just\ntransferred a lot of the burden.",
    "start": "3807050",
    "end": "3813900"
  },
  {
    "text": "And if it produces clusters\nthat don't correspond to all of your meta-test task\nrequirements then it seems like",
    "start": "3813900",
    "end": "3819109"
  },
  {
    "text": "it's limited by like well how\nmuch can you distort or change the space after it's\ndone constructing-- Yeah, absolutely.",
    "start": "3819110",
    "end": "3824730"
  },
  {
    "text": "So it's going to rely a lot\non the initial clustering. And I guess I should\nclarify that in this method",
    "start": "3824730",
    "end": "3832190"
  },
  {
    "text": "you cluster more than once. And you could-- if\nI remember correctly",
    "start": "3832190",
    "end": "3840140"
  },
  {
    "text": "before clustering more\nthan like each time, we scaled and I think we like\napplied some random scaling",
    "start": "3840140",
    "end": "3847820"
  },
  {
    "text": "to the embedding. And that was one\nway to distort it.",
    "start": "3847820",
    "end": "3853730"
  },
  {
    "text": "You could also\nimagine fancier ways to distort the space as well. You could also run it\nlike-- you could also",
    "start": "3853730",
    "end": "3858950"
  },
  {
    "text": "have multiple pre-trained\nrepresentations and presumably they'll have\ndifferent things that they pick up on or different\nhigh level features.",
    "start": "3858950",
    "end": "3864980"
  },
  {
    "text": "And that may give you\neven more diverse tasks. And so this really connects\nto the diversity of tasks",
    "start": "3864980",
    "end": "3870168"
  },
  {
    "text": "that you get. And I suspect that\nthere is in general, it seems like it's\nactually able to learn",
    "start": "3870168",
    "end": "3875630"
  },
  {
    "text": "pretty diverse tasks that seem\nto cover things pretty well, especially given\nthat the gap here is only like a 9% gap\nbetween supervised.",
    "start": "3875630",
    "end": "3883100"
  },
  {
    "text": "But there's probably also a\nlot of room for improvement. ",
    "start": "3883100",
    "end": "3890030"
  },
  {
    "text": "And these are results with MAML\nand miniImage 5-way-5-shot. But you kind of\nsee a similar trend",
    "start": "3890030",
    "end": "3896510"
  },
  {
    "text": "with different embedding\nmethods with different data sets and also with\nprototypical networks.",
    "start": "3896510",
    "end": "3902250"
  },
  {
    "text": "Prototypical numbers did\nunderperform in some cases. But in general, they're\nboth able to do pretty well.",
    "start": "3902250",
    "end": "3907820"
  },
  {
    "text": " Cool.",
    "start": "3907820",
    "end": "3912839"
  },
  {
    "text": "So that was one way\nto construct tasks. And coming back to some\nof what we saw with contrastive learning.",
    "start": "3912840",
    "end": "3918309"
  },
  {
    "text": "You can also construct tasks in\na very similar way to what we-- to what we saw with\ncontrastive learning.",
    "start": "3918310",
    "end": "3924190"
  },
  {
    "text": "So for image data\nin particular, we",
    "start": "3924190",
    "end": "3929550"
  },
  {
    "text": "know that an image label\nwon't change if you apply a certain transformation. Like if you drop out\nsome of the pixels",
    "start": "3929550",
    "end": "3936095"
  },
  {
    "text": "or translate the image a little\nbit or reflect the image. And so if you take Omniglot and\nyou apply a transform like this",
    "start": "3936095",
    "end": "3943650"
  },
  {
    "text": "or if you kind of flip a\nminiImageNet image you-- the underlying categorization\nof the image won't change.",
    "start": "3943650",
    "end": "3953570"
  },
  {
    "text": "And so we can use this\nto construct tasks. So we can construct tasks\nin the same exact way as what we saw in\ncontrastive learning.",
    "start": "3953570",
    "end": "3959589"
  },
  {
    "text": "Where we first randomly\nsample some images, assign a label to those images.",
    "start": "3959590",
    "end": "3965860"
  },
  {
    "text": "And then for each\nof those images, we also augment those\nimages in some way. And then assign the\nsame label to those.",
    "start": "3965860",
    "end": "3973840"
  },
  {
    "text": "And then the first image\nset becomes our support set and the second\nbecomes our query set.",
    "start": "3973840",
    "end": "3980680"
  },
  {
    "text": "And you could also\naugment in a different way for your support set as\nwell if you didn't want",
    "start": "3980680",
    "end": "3985765"
  },
  {
    "text": "to use the original images. ",
    "start": "3985765",
    "end": "3993279"
  },
  {
    "text": "And so in practice, you\ncan use whatever you know about the image domain. So for Omniglot you\nwould use something",
    "start": "3993280",
    "end": "3998745"
  },
  {
    "text": "like translation\nand pixel dropout. You wouldn't want\nto flip the image 'cause that may actually\nflip the category",
    "start": "3998745",
    "end": "4007500"
  },
  {
    "text": "label of that character. Whereas from miniImagenet\nyou would probably",
    "start": "4007500",
    "end": "4012720"
  },
  {
    "text": "do things like flipping. And so these auto\naugment here, which corresponds to a number of\ndifferent transformations.",
    "start": "4012720",
    "end": "4020799"
  },
  {
    "text": "And if you use this approach\nfor task construction,",
    "start": "4020800",
    "end": "4027100"
  },
  {
    "text": "it gets really strong\nOmniglot performance. And in particular\nthe performance",
    "start": "4027100",
    "end": "4033700"
  },
  {
    "text": "is kind of in the 80s to 90s\non Omniglot in many cases.",
    "start": "4033700",
    "end": "4038800"
  },
  {
    "text": "And in the five\nshot setting this actually gets pretty close to\na fully supervised baseline on Omniglot.",
    "start": "4038800",
    "end": "4044890"
  },
  {
    "text": "And this makes sense\nbecause we know a lot about the structure\nof Omniglot images.",
    "start": "4044890",
    "end": "4050319"
  },
  {
    "text": "We have a lot of domain\nknowledge about that. Whereas for\nminiImagenet it ends up doing somewhat similarly\nto the clustering",
    "start": "4050320",
    "end": "4056830"
  },
  {
    "text": "approach that we\ntalked about before. And in some cases\nit kind of slightly",
    "start": "4056830",
    "end": "4061839"
  },
  {
    "text": "underperforms that approach. And that also makes sense. We have I think a little\nbit less domain knowledge about the kind of\ntransformations that",
    "start": "4061840",
    "end": "4069099"
  },
  {
    "text": "will affect natural images. ",
    "start": "4069100",
    "end": "4077500"
  },
  {
    "text": "Cool. So those were a couple\nof examples with images and how to construct\nimages with--",
    "start": "4077500",
    "end": "4082630"
  },
  {
    "text": "how to construct tasks when\nyou have unlabeled images. You can also apply\nthe same recipe with text as well with a\ndifferent task construction",
    "start": "4082630",
    "end": "4090670"
  },
  {
    "text": "technique. One option, which\nwe've seen before is to formulate a\nlanguage modeling problem.",
    "start": "4090670",
    "end": "4097450"
  },
  {
    "text": "Where a sequence\nthat supports that is a sequence of characters\nand the query set is the following\nsequence of characters.",
    "start": "4097450",
    "end": "4104318"
  },
  {
    "text": "And we've seen\nthis example where the inner loop of the\nmeta-learning process",
    "start": "4104319",
    "end": "4111278"
  },
  {
    "text": "goes through each\nof these examples. You might have these\nexamples as your support set and these examples\nas your query set.",
    "start": "4111279",
    "end": "4118060"
  },
  {
    "text": "And you have different\ntasks like translating between languages, math\nproblems or spelling correction.",
    "start": "4118060",
    "end": "4125049"
  },
  {
    "text": " So this is an example, a way to\nconstruct meta-training tasks",
    "start": "4125050",
    "end": "4131339"
  },
  {
    "text": "with just purely unlabeled text. But we might not want to use\nthis option in all cases.",
    "start": "4131340",
    "end": "4136920"
  },
  {
    "text": "For example, it may be harder\nto combine with optimization based meta-learning. And it's also not\nnecessarily the best",
    "start": "4136920",
    "end": "4143310"
  },
  {
    "text": "suited for simple binary\nclassification tasks. ",
    "start": "4143310",
    "end": "4149770"
  },
  {
    "text": "The second option,\nwhich is basically the other form of generative\npre-training we talked about",
    "start": "4149770",
    "end": "4155130"
  },
  {
    "text": "is to use masked\nlanguage modeling. And the tasks here\nwill just correspond",
    "start": "4155130",
    "end": "4161139"
  },
  {
    "text": "to masking different words. And concretely, the way\nthat you can do this",
    "start": "4161140",
    "end": "4166290"
  },
  {
    "text": "is for a given task, sample\na set of unique words,",
    "start": "4166290",
    "end": "4172500"
  },
  {
    "text": "of N unique words and\nassign each of those words a unique ID.",
    "start": "4172500",
    "end": "4178899"
  },
  {
    "text": "So for example, you could\nsample the word Democratic and the word capital. And assign an ID of 1 to\nDemocratic and an ID of 2",
    "start": "4178899",
    "end": "4187318"
  },
  {
    "text": "to capital. And then you can\nsample sentences",
    "start": "4187319",
    "end": "4193770"
  },
  {
    "text": "with one of those two words,\nand mask out the word.",
    "start": "4193770",
    "end": "4199360"
  },
  {
    "text": "And then the task is to classify\nwhich word was masked out.",
    "start": "4199360",
    "end": "4204957"
  },
  {
    "text": "And so this is going to give\nyou an N-way classification problem if you sample N words\n'cause you'll be basically",
    "start": "4204957",
    "end": "4210740"
  },
  {
    "text": "be given like, for these N\nwords which of those words corresponds to the\nmasked out word?",
    "start": "4210740",
    "end": "4217630"
  },
  {
    "text": "And this allows you to construct\nkind of text classification problems without\nactually having access",
    "start": "4217630",
    "end": "4223190"
  },
  {
    "text": "to any labeled text data. ",
    "start": "4223190",
    "end": "4229940"
  },
  {
    "text": "And so kind of concretely for\nthe Democratic and capital example, we can have a support\nset that looks like this.",
    "start": "4229940",
    "end": "4238310"
  },
  {
    "text": "Where M corresponds to\nthe masked out word. The sentence corresponds\nto the input.",
    "start": "4238310",
    "end": "4243770"
  },
  {
    "text": "The class corresponds\nto the label. And we have two\nexamples from class 1",
    "start": "4243770",
    "end": "4249560"
  },
  {
    "text": "and two examples from class 2. And then the query\nwill correspond",
    "start": "4249560",
    "end": "4254730"
  },
  {
    "text": "to a new sentence that\nalso has a word masked out. And you need to be able\nto predict whether or not",
    "start": "4254730",
    "end": "4261239"
  },
  {
    "text": "the label is 1 or 2. Or basically whether the word\nis Democratic or capital. ",
    "start": "4261240",
    "end": "4270150"
  },
  {
    "text": "Cool. So with this kind\nof approach you can apply it either in\na fully unsupervised way",
    "start": "4270150",
    "end": "4276510"
  },
  {
    "text": "or you could\nactually-- and really with the other\napproaches too, you can combine it with\nsome supervised tasks",
    "start": "4276510",
    "end": "4281910"
  },
  {
    "text": "if you have some\ntasks available. And so here are some kind\nof what the results look",
    "start": "4281910",
    "end": "4287670"
  },
  {
    "text": "like if you either apply it\nin a fully unsupervised way or everything on the right\nhand side of the table",
    "start": "4287670",
    "end": "4296140"
  },
  {
    "text": "is a more semi supervised\nor supervised approach. BERT corresponds\nto kind of what we",
    "start": "4296140",
    "end": "4302500"
  },
  {
    "text": "talked about last week with\nstandard self-supervised learning. SMLMT is the proposed\nunsupervised approach.",
    "start": "4302500",
    "end": "4311425"
  },
  {
    "text": " Multitask-BERT corresponds to\nmulti task learning and fine",
    "start": "4311425",
    "end": "4318080"
  },
  {
    "text": "tuning on the supervised tasks. There's also an optimization\nbased meta-learner that's",
    "start": "4318080",
    "end": "4324830"
  },
  {
    "text": "applied only to the\nsupervised tasks as well as a hybrid method\nthat does meta-learning on the proposed tasks\nand the supervised tasks.",
    "start": "4324830",
    "end": "4332918"
  },
  {
    "text": "So this is more of like a\nsemi supervised meta-learning approach. And what we see by incorporating\nthese tasks generated from",
    "start": "4332918",
    "end": "4343250"
  },
  {
    "text": "the unlabeled data is able to do\nsignificantly better than only using the labelled data.",
    "start": "4343250",
    "end": "4349100"
  },
  {
    "text": "And also only using the labelled\ndata both for the multi task",
    "start": "4349100",
    "end": "4354920"
  },
  {
    "text": "learning and also\nfor meta-learning. And also we see that for\neach of these different text",
    "start": "4354920",
    "end": "4362960"
  },
  {
    "text": "classification problems, in\nmany cases this is able to--",
    "start": "4362960",
    "end": "4368600"
  },
  {
    "text": "or in some cases this\nis able to outperform a BERT based pre-training.",
    "start": "4368600",
    "end": "4373788"
  },
  {
    "text": " Cool. There's already\na lot of results.",
    "start": "4373788",
    "end": "4379265"
  },
  {
    "text": "But if you want\nmore results there's kind of more in the\npaper that's linked. ",
    "start": "4379265",
    "end": "4385700"
  },
  {
    "text": "Cool. So in the second\npart of this lecture, we talked about\nkind of this form",
    "start": "4385700",
    "end": "4391170"
  },
  {
    "text": "of unsupervised pre-training. It ends up looking a lot like\nthe unsupervised pre-training",
    "start": "4391170",
    "end": "4396260"
  },
  {
    "text": "that we talked about last week,\nwhere we saw masked language modeling. We saw autoregressive\nlanguage modeling.",
    "start": "4396260",
    "end": "4402350"
  },
  {
    "text": "We saw like generating\ntasks from augmentations. It has this kind of recipe of\ngiven an unlabeled data set,",
    "start": "4402350",
    "end": "4410553"
  },
  {
    "text": "propose tasks with these\ndifferent techniques. And then run meta-learning on\nit and really the key difference between these methods and the\nones that we saw last week",
    "start": "4410553",
    "end": "4418220"
  },
  {
    "text": "is that we're actually going\nto be explicitly running meta-learning and using\nkind of the meta-learner",
    "start": "4418220",
    "end": "4423830"
  },
  {
    "text": "to perform few-shot learning\nrather than just doing fine tuning on top of\nthis pre-trained-- on top",
    "start": "4423830",
    "end": "4429260"
  },
  {
    "text": "of some pre-trained\nrepresentation. And we also covered\na few different ways",
    "start": "4429260",
    "end": "4435340"
  },
  {
    "text": "to propose tasks that have been\nsuggested in prior work, which",
    "start": "4435340",
    "end": "4440650"
  },
  {
    "text": "includes classifying\nbetween augmented images and different image instances\nand classifying a masked word.",
    "start": "4440650",
    "end": "4448780"
  },
  {
    "text": " Cool. Any questions on--",
    "start": "4448780",
    "end": "4455400"
  },
  {
    "text": "Yeah? [INAUDIBLE] if you have a\nlarge number of [INAUDIBLE] is there a way to do\nfew shot, would it happen in the sense of\nunsupervised pretraining",
    "start": "4455400",
    "end": "4460511"
  },
  {
    "text": "[INAUDIBLE]",
    "start": "4460511",
    "end": "4474200"
  },
  {
    "text": "Given enough test task\ndata or given enough data on the meta test task?",
    "start": "4474200",
    "end": "4482760"
  },
  {
    "text": "Yeah. So the question is if you\nhave enough meta test data, would kind of the unsupervised\npre-training methods",
    "start": "4482760",
    "end": "4489560"
  },
  {
    "text": "that fine tuned\ntheir representation start to outperform\nwhat we've talked about? And yeah, the answer is yes.",
    "start": "4489560",
    "end": "4495050"
  },
  {
    "text": "So these kind of\napproaches are really going to shine if\nyou're in a few shot regime at meta-test time because\nthey're explicitly optimizing",
    "start": "4495050",
    "end": "4502370"
  },
  {
    "text": "for few shot learning. And the settings in which\nunsupervised pre training are going to do\nwell are settings",
    "start": "4502370",
    "end": "4508520"
  },
  {
    "text": "where you have a bit more\ndata of your target task. Because you're\nnot actually-- you",
    "start": "4508520",
    "end": "4513530"
  },
  {
    "text": "don't need to explicitly\noptimize for few shot learning, you can simply fine tune with\na reasonable starting point. ",
    "start": "4513530",
    "end": "4520116"
  },
  {
    "text": "[INAUDIBLE] is that\nactually in the few shot, we don't have a\nmasked word, right? So is it, would it kind of\nhave a sentence without a mask, and [INAUDIBLE]",
    "start": "4520117",
    "end": "4531900"
  },
  {
    "text": "Yeah. This is a great question. So all of these tasks-- it's a good observation. So all of these tasks\nlike you see this m token,",
    "start": "4531900",
    "end": "4541120"
  },
  {
    "text": "this masked token in the\nsentences and at test time you often won't\nhave masked tokens.",
    "start": "4541120",
    "end": "4546220"
  },
  {
    "text": "And so I think that's\npart of the reason why this is particularly well\nsuited as my supervised approach",
    "start": "4546220",
    "end": "4552239"
  },
  {
    "text": "more so than an\nunsupervised approach. If you really kind of go\nthrough these numbers carefully,",
    "start": "4552240",
    "end": "4558060"
  },
  {
    "text": "be nice if there\nwas like a bar plot. But if you go through them\nquickly or more thoroughly,",
    "start": "4558060",
    "end": "4563067"
  },
  {
    "text": "you'll see that the\nsemi-supervised approach is more performant. And I suspect that this is\nbecause there's a bit more",
    "start": "4563067",
    "end": "4569790"
  },
  {
    "text": "of a distribution shift\nbetween the unlabeled tasks and the labelled tasks. ",
    "start": "4569790",
    "end": "4576710"
  },
  {
    "text": "[INAUDIBLE] Yeah. So there is one task where\nunsupervised is actually better",
    "start": "4576710",
    "end": "4585570"
  },
  {
    "text": "than all of the approaches\nthat use labelled data. I'm not sure exactly\nwhy that's the case. I'd have to kind of look at\nthe paper a little bit more",
    "start": "4585570",
    "end": "4591780"
  },
  {
    "text": "closely. But yeah, it's kind of\nan interesting finding. ",
    "start": "4591780",
    "end": "4601980"
  },
  {
    "text": "Awesome. Cool. So that's it for today.",
    "start": "4601980",
    "end": "4607130"
  },
  {
    "start": "4607130",
    "end": "4612000"
  }
]