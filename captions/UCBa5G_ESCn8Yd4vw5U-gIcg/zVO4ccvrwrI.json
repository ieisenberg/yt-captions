[
  {
    "start": "0",
    "end": "5240"
  },
  {
    "text": "So let me-- I just\nwanted to quickly review self-concordance,\njust to make sure that the whole thing at\na high level is clear.",
    "start": "5240",
    "end": "14500"
  },
  {
    "text": "The basic idea is to make\nan analysis of convergence",
    "start": "14500",
    "end": "19750"
  },
  {
    "text": "of Newton's method that\nis, like Newton's method, independent of affine\nchanges of coordinates.",
    "start": "19750",
    "end": "27550"
  },
  {
    "text": "And this is to correct\nthis embarrassment that the implementation,\nthe actual algorithm and the implementation,\nis actually",
    "start": "27550",
    "end": "34480"
  },
  {
    "text": "more sophisticated than-- well, was more sophisticated\nthan the existing",
    "start": "34480",
    "end": "40150"
  },
  {
    "text": "mathematical analyses\nof convergence. And what you'll\nrecall last time is",
    "start": "40150",
    "end": "45940"
  },
  {
    "text": "that it all comes down to this. As you know, the analysis\nof Newton's method comes down to how big\nis the third derivative?",
    "start": "45940",
    "end": "54013"
  },
  {
    "text": "Because that's\nwhat it depends on. Third derivative is 0,\nthen Newton's method",
    "start": "54013",
    "end": "59140"
  },
  {
    "text": "converges in one step. If it's small,\nthat's what tells you it's going to work pretty\nwell, or something like that.",
    "start": "59140",
    "end": "64750"
  },
  {
    "text": "It basically says, how fast\ndoes the Hessian change as you move around? That's what it is.",
    "start": "64750",
    "end": "70360"
  },
  {
    "text": "OK. The classical\nanalysis just says-- just uses that. It uses the most obvious\nmeasure of the size",
    "start": "70360",
    "end": "77930"
  },
  {
    "text": "of the third derivative. If it was on for n\nequals 1, it would just be f prime prime prime\nabsolute value, which is like",
    "start": "77930",
    "end": "85207"
  },
  {
    "text": "if someone walks up to you\nin the street and says, give me a measure of the\nsize of the third derivative. That should be the\nfirst thing you say.",
    "start": "85207",
    "end": "91460"
  },
  {
    "text": "Minor problem with that is\nthat is not independent, that is not independent,\nof changes of coordinates.",
    "start": "91460",
    "end": "97609"
  },
  {
    "text": "And so if you're given the\nhomework exercise of figuring out a way to say that the third\nderivative is small that will",
    "start": "97610",
    "end": "107120"
  },
  {
    "text": "be independent of an affine\nchange of coordinates-- in other words,\nwhen you do this--",
    "start": "107120",
    "end": "113270"
  },
  {
    "text": "it's going to lead you\nto basically comparing the absolute value of the third\nderivative to the 3/2 power",
    "start": "113270",
    "end": "118670"
  },
  {
    "text": "of the second\nderivative, period. And you could put another\nnumber for 2 there. You could divide it and call\nit something else, whatever",
    "start": "118670",
    "end": "126570"
  },
  {
    "text": "you like. But this is-- from\nthat point of view, otherwise this looks\nlike entirely mysterious, like where did that come from.",
    "start": "126570",
    "end": "132620"
  },
  {
    "text": "But that's all it is. What happens when you\nadopt this way to say",
    "start": "132620",
    "end": "138910"
  },
  {
    "text": "that the Hessian varies smoothly\nor not much is weird stuff",
    "start": "138910",
    "end": "146170"
  },
  {
    "text": "happens. So a lot of functions that we\nactually work with actually satisfy this property, this\nso-called self-concordant",
    "start": "146170",
    "end": "156430"
  },
  {
    "text": "property. And I'm not going\nto go into this.",
    "start": "156430",
    "end": "162210"
  },
  {
    "text": "And you can actually look\ninto how you would show this. Not all convex functions-- in\nfact, not all convex functions",
    "start": "162210",
    "end": "168540"
  },
  {
    "text": "that we are interested\nin are self-concordant, but a whole lot are.",
    "start": "168540",
    "end": "174690"
  },
  {
    "text": "When you repeat the analysis,\nan amazing thing comes out. Basically, what comes\nout is something",
    "start": "174690",
    "end": "180540"
  },
  {
    "text": "that I am unable to make\nfun of, because the way I make fun of it-- typical\nconvergence analysis is, here's",
    "start": "180540",
    "end": "188129"
  },
  {
    "text": "my prototype convergence\nanalysis, if, and then a long list of things that you\ncould never possibly verify",
    "start": "188130",
    "end": "194190"
  },
  {
    "text": "or know, hold then, and then\nit gives a silly conclusion, which is also weak and\npathetic and doesn't really",
    "start": "194190",
    "end": "201870"
  },
  {
    "text": "even say anything. So that's my model of what I see\na convergence analysis looking like.",
    "start": "201870",
    "end": "207680"
  },
  {
    "text": "This, this I can't make fun\nof, because it basically starts by there's nothing\nunknown in the hypothesis.",
    "start": "207680",
    "end": "216650"
  },
  {
    "text": "It's just if it's\nself-concordant. That's it. And we know for a whole\nbunch of functions that we actually\ncare about, it is.",
    "start": "216650",
    "end": "222890"
  },
  {
    "text": "And then there are\nparameters in the analysis. The analysis is very simple. It simply looks like this.",
    "start": "222890",
    "end": "228240"
  },
  {
    "text": "It says-- oh, and by the\nway, it's measured by lambda, which you'll remember is the\ngradient times the Hessian",
    "start": "228240",
    "end": "235400"
  },
  {
    "text": "inverse times the\ngradient square-- it's actually the square\nroot of that or something. That is actually a\nmeasure of the gradient",
    "start": "235400",
    "end": "245160"
  },
  {
    "text": "which is independent of an\naffine change of coordinates. So if you have this\nbeautiful algorithm, and then the last thing you\nhave in your code is something--",
    "start": "245160",
    "end": "252440"
  },
  {
    "text": "is a convergence\nthing that looks like norm of the\ngradient, congratulations, it's now dependent,\nweakly dependent,",
    "start": "252440",
    "end": "261206"
  },
  {
    "text": "on changes of coordinates. OK. And it looks the same. It basically says that for a\nwhile, you are guaranteed--",
    "start": "261206",
    "end": "272420"
  },
  {
    "text": "that there's a guaranteed\nincrease of-- decrease of gamma in the function value.",
    "start": "272420",
    "end": "278270"
  },
  {
    "text": "And then once you're less\nthan this critical value eta and for lambda,\nthen in fact, you",
    "start": "278270",
    "end": "283700"
  },
  {
    "text": "get this quadratic convergence. What's missing are\nall those crazy things like m and l, which you\nwould not normally know.",
    "start": "283700",
    "end": "291698"
  },
  {
    "text": "I mean, there would\nbe isolated cases where I would know m and l,\nbut mostly you would not.",
    "start": "291698",
    "end": "296942"
  },
  {
    "text": "Then you get a complexity\nbound that looks like this. And what's amazing is that\neverything here, you know.",
    "start": "296942",
    "end": "303229"
  },
  {
    "text": "I mean-- oh, one minor-- we\nstill have the embarrassment of p* here. We will talk about that.",
    "start": "303230",
    "end": "309570"
  },
  {
    "text": "We'll see this will\ncome up maybe next week in interior point methods. We'll take care of that.",
    "start": "309570",
    "end": "316977"
  },
  {
    "text": "You can still make\nfun of it that way, but it's fine because if I\nreplace p* with any lower bound",
    "start": "316977",
    "end": "322940"
  },
  {
    "text": "on p*, then I still get an upper\nbound on the number of steps.",
    "start": "322940",
    "end": "328470"
  },
  {
    "text": "Any-- yeah. So that's the idea. OK. Yeah, and if you work this out--",
    "start": "328470",
    "end": "334890"
  },
  {
    "text": "this is a sloppy analysis\nthat [INAUDIBLE] and I did-- you get something like\n375 times the difference.",
    "start": "334890",
    "end": "342099"
  },
  {
    "text": "So in fact, if you do a\nmore careful analysis, you can get like 11.",
    "start": "342100",
    "end": "347102"
  },
  {
    "text": "I think that's the one that\nNesterov and Nemirovski came up with. What's interesting\nis that actually",
    "start": "347102",
    "end": "353310"
  },
  {
    "text": "does give you the behavior. This looks like, I mean, maybe\nthere's a better lower bound",
    "start": "353310",
    "end": "358560"
  },
  {
    "text": "that has this slope. We could work out\nwhat that slope is. But ours is just like\na straight-- almost a",
    "start": "358560",
    "end": "363870"
  },
  {
    "text": "straight line going up. And 11-- yeah, 11 would\nbe-- eh, it's still--",
    "start": "363870",
    "end": "370030"
  },
  {
    "text": "it's only off by a factor of\n10 because it looks like if you take--",
    "start": "370030",
    "end": "375035"
  },
  {
    "text": "if you took c equals-- I don't know. We could take a look at\nthis, what that slope is. But maybe it's like 2\nor something like that?",
    "start": "375035",
    "end": "382320"
  },
  {
    "text": "That would probably be a\npretty good estimate of it. OK, so-- now we'll talk--",
    "start": "382320",
    "end": "390449"
  },
  {
    "text": "so just to finish up the\ndiscussion of self-concordance,",
    "start": "390450",
    "end": "395520"
  },
  {
    "text": "mostly it's used by people\nto prove convergence of, for example,\ninterior point methods,",
    "start": "395520",
    "end": "400770"
  },
  {
    "text": "which will be our next topic. So that's what it's\nusually used for. People have observed\nthat formulations",
    "start": "400770",
    "end": "407662"
  },
  {
    "text": "that involve self-concordant\nfunctions actually do work better in practice. But I think no one would claim\nanything strong about that.",
    "start": "407662",
    "end": "416189"
  },
  {
    "text": "And for sure, if you were\nimplementing a method and something wasn't\nself-concordant and it worked",
    "start": "416190",
    "end": "421380"
  },
  {
    "text": "well against-- carefully against a whole bunch\nof test problems and things like that, no one would\nworry about it one bit.",
    "start": "421380",
    "end": "429000"
  },
  {
    "text": "Just to make sure that the\nwhole story on self-concordance",
    "start": "429000",
    "end": "434550"
  },
  {
    "text": "is clear. OK. Now we get to something\nmuch more important, which is actually the implementation.",
    "start": "434550",
    "end": "441030"
  },
  {
    "text": "So you know that\nbasically, it comes down to forming this Hessian\nhere and the gradient,",
    "start": "441030",
    "end": "448389"
  },
  {
    "text": "and then solving this\nset of equations, which is linear algebra. And in fact, you could\neven say if someone",
    "start": "448390",
    "end": "455448"
  },
  {
    "text": "says, what are you doing? You say, I'm minimizing\na smooth function. How are you doing it? And you say, well,\nI'm solving a sequence",
    "start": "455448",
    "end": "461260"
  },
  {
    "text": "of quadratic\nminimization problems. And they go, uh, OK, and?",
    "start": "461260",
    "end": "466810"
  },
  {
    "text": "And you go, ah,\nquadratic minimization, that's linear algebra. So we've reduced minimizing\na smooth convex function",
    "start": "466810",
    "end": "475540"
  },
  {
    "text": "to minimizing a sequence\nof, I don't know, some tens or whatever of\nquadratic minimizations, each",
    "start": "475540",
    "end": "484090"
  },
  {
    "text": "of which is actually\njust linear algebra. By the way, that means that\nyou can import all the stuff you know about linear algebra\nwith structure and banded",
    "start": "484090",
    "end": "491607"
  },
  {
    "text": "and sparse and all\nthat kind of stuff, and it all just\nslots right in here. OK.",
    "start": "491607",
    "end": "497784"
  },
  {
    "text": "If there's no\nstructure of any kind, that's just going to\nbe like n cubed flops for an unstructured system.",
    "start": "497785",
    "end": "504800"
  },
  {
    "text": "If H were like sparse\nor banded, it's actually going to be a\nlot faster to solve--",
    "start": "504800",
    "end": "511210"
  },
  {
    "text": "I mean, if you know\nwhat you're doing. Actually, there's many\nmistakes people can make.",
    "start": "511210",
    "end": "516870"
  },
  {
    "text": "One would be forming H\nand having it be dense. Even if there's\n0's all over it--",
    "start": "516870",
    "end": "523479"
  },
  {
    "text": "I mean, you should check that. But even if there's 0's all\nover it, it's not going to--",
    "start": "523480",
    "end": "528670"
  },
  {
    "text": "some numerical\nmethod is not going to exploit that necessarily. OK. So, all right. ",
    "start": "528670",
    "end": "536660"
  },
  {
    "text": "Let's see. What you're going to\nwant to do is actually make a map in your head so you\nknow what kind of applications",
    "start": "536660",
    "end": "545600"
  },
  {
    "text": "involve a Hessian\nthat has structure. So here's a very\ncommon one that occurs,",
    "start": "545600",
    "end": "554330"
  },
  {
    "text": "is that a function\nhere is going to be-- it's separable. So it's a function-- these\nare functions of one variable.",
    "start": "554330",
    "end": "561440"
  },
  {
    "text": "These xi's are the components. So it's separable. So it's a separable\nfunction plus a function",
    "start": "561440",
    "end": "569240"
  },
  {
    "text": "of an affine function\nof your variables. So this is extremely common\nform that's going to come up.",
    "start": "569240",
    "end": "576057"
  },
  {
    "text": "In fact, I guess\nwe're going to see it in just a couple of minutes\nin some applications, right?",
    "start": "576057",
    "end": "581510"
  },
  {
    "text": "So here, if you work out\nwhat the Hessian of this is, the Hessian of a separable\nfunction is diagonal.",
    "start": "581510",
    "end": "588200"
  },
  {
    "text": "I mean, in fact, that's kind\nof if and only if, right? If you Hessian is diagonal,\nthen it's the sum--",
    "start": "588200",
    "end": "593870"
  },
  {
    "text": "it's a separable function. Because it basically means\ncross-derivatives are 0. Like, partial xi\npartial xj of f is 0.",
    "start": "593870",
    "end": "602810"
  },
  {
    "text": "And so that's going to tell\nyou that it looks like this. OK. Now, the Hessian of this,\nyou can work it out.",
    "start": "602810",
    "end": "611970"
  },
  {
    "text": "But it's actually this. It's A transpose-- so here,\nwe're assuming A is wide.",
    "start": "611970",
    "end": "617850"
  },
  {
    "text": "It is going to be A\ntranspose then H, which is the Hessian of this\nthing, H0, and then times A.",
    "start": "617850",
    "end": "626130"
  },
  {
    "text": "So if you look at\nthis long enough, you realize like, oh, my god,\nthat is diagonal plus low rank.",
    "start": "626130",
    "end": "632399"
  },
  {
    "text": "And so once again, once your\nlinear algebra structure center",
    "start": "632400",
    "end": "638760"
  },
  {
    "text": "of your brain is\nall up to speed, that should be blinking saying,\nyou can solve this faster.",
    "start": "638760",
    "end": "646460"
  },
  {
    "text": "OK. And that's the right-- that's correct, right? So if you just ignore that\nand form an n by n matrix",
    "start": "646460",
    "end": "653210"
  },
  {
    "text": "then call a Cholesky\nfactorization, then each Newton step is going\nto cost you order n cubed.",
    "start": "653210",
    "end": "658639"
  },
  {
    "text": "So that's going to be\nthe naive implementation. But if you know that it's\ndiagonal plus low rank,",
    "start": "658640",
    "end": "664670"
  },
  {
    "text": "you can exploit that. And that might look\nsomething like this. First, you would do a\nCholesky factorization.",
    "start": "664670",
    "end": "671370"
  },
  {
    "text": "This is a p cubed\noperation here. Then you would write out\nthe Newton system as this.",
    "start": "671370",
    "end": "678680"
  },
  {
    "text": "You'd actually literally\nintroduce a new variable, w. Then you eliminate delta x from\nit, and you'd end up with this.",
    "start": "678680",
    "end": "685440"
  },
  {
    "text": "And you'd recognize these\nthings as Schur complements of various things. And then you can work out\nwhat everything happens here.",
    "start": "685440",
    "end": "693270"
  },
  {
    "text": "I'm not going to go into\nthe details of this. But it turns out, actually\njust forming this matrix",
    "start": "693270",
    "end": "698390"
  },
  {
    "text": "here is the dominant factor. And that's 2p squared n. This is to be compared\nto n cubed, right?",
    "start": "698390",
    "end": "704840"
  },
  {
    "text": "So if p is like 1/100 of n,\nthen you just saved yourself",
    "start": "704840",
    "end": "711120"
  },
  {
    "text": "10,000x. Everybody following this? So anyway, so this is the kind\nof thing you're-- so when you",
    "start": "711120",
    "end": "717630"
  },
  {
    "text": "see things like this-- and we\ncould do things like this-- what would-- what\nis the Hessian--",
    "start": "717630",
    "end": "723990"
  },
  {
    "text": "what does it mean for\nthe Hessian to be banded? How about that? ",
    "start": "723990",
    "end": "730240"
  },
  {
    "text": "Just in words, what\ndoes that mean? ",
    "start": "730240",
    "end": "737380"
  },
  {
    "text": "Is there any suggestion? Well, it just means\nthat there are no--",
    "start": "737380",
    "end": "743730"
  },
  {
    "text": "I'm going to call-- when I take a function,\nI go partial squared f partial xi partial xj.",
    "start": "743730",
    "end": "749868"
  },
  {
    "text": "If that's not 0,\nI'm going to call it an interaction between i and j. So what this says is\ninteractions are local.",
    "start": "749868",
    "end": "756450"
  },
  {
    "text": "It says that those are 0 unless\ni minus j is really small-- like less than\nhalf the bandwidth.",
    "start": "756450",
    "end": "763120"
  },
  {
    "text": "So that's what it means. There's a lot of\napplications that would be things like\ncontrol, signal processing.",
    "start": "763120",
    "end": "769710"
  },
  {
    "text": "I could go on and on. I mean, these are entire fields\nwould have that structure.",
    "start": "769710",
    "end": "775319"
  },
  {
    "text": "And by the way, if the Hessian\nis banded, how fast can I do-- can I compute a Newton step?",
    "start": "775320",
    "end": "780930"
  },
  {
    "text": " In n, if it's banded?",
    "start": "780930",
    "end": "786550"
  },
  {
    "start": "786550",
    "end": "793310"
  },
  {
    "text": "It's just over n because the\nfactorization is going to be--",
    "start": "793310",
    "end": "798680"
  },
  {
    "text": "well, it's going to actually\nlook exactly like this here. It's going to be this thing. So what that says\nis, I don't know.",
    "start": "798680",
    "end": "805100"
  },
  {
    "text": "I mean, just for fun, if I had\nto solve an optimal control problem with, I don't know,\na couple thousand steps",
    "start": "805100",
    "end": "812990"
  },
  {
    "text": "by Newton's method, someone\nwant to make a quick guess as to how long that might take?",
    "start": "812990",
    "end": "820430"
  },
  {
    "text": "Well, let's say the\nstate dimension is 10 and, I don't know,\nthere's five inputs. It doesn't matter. It's some optimal\ncontrol problem.",
    "start": "820430",
    "end": "828350"
  },
  {
    "text": "Yeah, so if you\njust wrote it out, it might take a second or two.",
    "start": "828350",
    "end": "834800"
  },
  {
    "text": "But if you knew\nwhat you were doing, it would be submillisecond.",
    "start": "834800",
    "end": "841279"
  },
  {
    "text": "And this is actually how\na lot of things work. So this is the\nkey to everything. These are super\nimportant things to know.",
    "start": "841280",
    "end": "849080"
  },
  {
    "text": "You just have to\nknow these, period. So OK. Yeah?",
    "start": "849080",
    "end": "854620"
  },
  {
    "text": "What happens when you\nintroduce the w there? Or what was the\nmotivation for that?",
    "start": "854620",
    "end": "859730"
  },
  {
    "text": "Oh, this is simply\njust doing it by-- I mean, you could go\nback and we can call it all sorts of things. w is\nunelimination, if you like.",
    "start": "859730",
    "end": "868279"
  },
  {
    "text": "Yeah, that's one way. Or this is actually just doing-- this is exploiting diagonal\nplus low rank or whatever.",
    "start": "868280",
    "end": "875825"
  },
  {
    "text": "Once you expand it,\nthat's what happens. Yeah. So that's all this is. OK.",
    "start": "875825",
    "end": "881160"
  },
  {
    "text": " OK. So next up-- and\nthis is material",
    "start": "881160",
    "end": "888970"
  },
  {
    "text": "that you will need to\ndo this homework thing. Actually, we're\nprobably not going to-- I'll say a few things\nabout it, but you actually",
    "start": "888970",
    "end": "895339"
  },
  {
    "text": "are definitely going to have\nto read the book carefully. It's just a couple of pages to\nget the details of this method.",
    "start": "895340",
    "end": "902730"
  },
  {
    "text": "OK. So let's look at equality\nconstrained minimization. So basic problem is we have\na smooth, twice continuously",
    "start": "902730",
    "end": "911660"
  },
  {
    "text": "differentiable\nconvex function here. And we're going to minimize\nhere f subject to Ax equals b.",
    "start": "911660",
    "end": "918485"
  },
  {
    "text": "And this basically says\nthese optimality conditions are necessary and sufficient.",
    "start": "918485",
    "end": "923779"
  },
  {
    "text": "It says that when the gradient\nis in the range of A transpose, and you're primal feasible,\nthen that's optimal.",
    "start": "923780",
    "end": "932690"
  },
  {
    "text": "And that's if and only if. There's no-- there's\nnothing else here.",
    "start": "932690",
    "end": "938125"
  },
  {
    "text": "It's just that. No qualifications or\nanything like that. This is assuming f is convex.",
    "start": "938125",
    "end": "943620"
  },
  {
    "text": "OK.  OK, so here's a classic example.",
    "start": "943620",
    "end": "951510"
  },
  {
    "text": "It's quadratic. So here's an\nequality constrained quadratic minimization problem.",
    "start": "951510",
    "end": "957280"
  },
  {
    "text": "Looks like this, right? And the optimality condition\nnow, the Hessian is just P.",
    "start": "957280",
    "end": "962520"
  },
  {
    "text": "The gradient is going to be Px\nplus q or something like that.",
    "start": "962520",
    "end": "968010"
  },
  {
    "text": "That's the gradient. And then the\noptimality condition, you can write out this way. So that's what it is.",
    "start": "968010",
    "end": "974400"
  },
  {
    "text": "This is the-- yeah, these are\nthe optimality conditions. These are literally these\ntwo equations here, here.",
    "start": "974400",
    "end": "983760"
  },
  {
    "text": "The difference is this\nequation-- these are linear actually, right? That's a set of\nlinear equations. That's linear.",
    "start": "983760",
    "end": "989459"
  },
  {
    "text": "And if f is quadratic, its\ngradient is an affine function. And so the KKT conditions become\na set of linear equations.",
    "start": "989460",
    "end": "996910"
  },
  {
    "text": "And once again, it's\njust all linear algebra. So that-- which--",
    "start": "996910",
    "end": "1002770"
  },
  {
    "text": "good. OK. So when you set it up,\nit looks like this. This is a very famous matrix. It's actually called\nthe KKT matrix.",
    "start": "1002770",
    "end": "1010600"
  },
  {
    "text": "And it comes up in pretty much\nevery field I can imagine. It comes up in--\nit's in mechanics.",
    "start": "1010600",
    "end": "1017250"
  },
  {
    "text": "Certainly, it's in\nelectrical engineering. It's in lots and lots\nand lots of fields. ",
    "start": "1017250",
    "end": "1023170"
  },
  {
    "text": "OK. So let's see. Oh, you can work out this thing\nis going to be nonsingular.",
    "start": "1023170",
    "end": "1032929"
  },
  {
    "text": "It is certainly not\npositive definite. In fact, it's what people\ncall quasi-semi-definite.",
    "start": "1032930",
    "end": "1039140"
  },
  {
    "text": "So this KKT matrix\nhere is invertible",
    "start": "1039140",
    "end": "1044180"
  },
  {
    "text": "if and only if, roughly\nspeaking, there's nothing-- the null space of A does not\nintersect the null space of P.",
    "start": "1044180",
    "end": "1052370"
  },
  {
    "text": "So if there's a\ncommon vector that both satisfies Ax equals\n0 and Px equals 0,",
    "start": "1052370",
    "end": "1059299"
  },
  {
    "text": "then this thing is singular. And that's if and only if.",
    "start": "1059300",
    "end": "1066530"
  },
  {
    "text": "And there's many, many,\nmany ways to express that. And you probably don't even have\nto worry about that too much.",
    "start": "1066530",
    "end": "1075020"
  },
  {
    "text": "But that's what that is. OK.  So by the way, to\nevaluate this, to solve",
    "start": "1075020",
    "end": "1083910"
  },
  {
    "text": "that, we'll see that there's a\nlot of options you could use. One is you could use an LDL\ntranspose factorization, which",
    "start": "1083910",
    "end": "1091080"
  },
  {
    "text": "is-- that's a\nCholesky factorization but with different signs. And when you do that, that's\none perfectly good way.",
    "start": "1091080",
    "end": "1098400"
  },
  {
    "text": "And there's others we\ncould talk-- we will be able to talk about later. OK.",
    "start": "1098400",
    "end": "1104649"
  },
  {
    "text": "So the first thing-- I don't know if you remember,\nbut basically in week two,",
    "start": "1104650",
    "end": "1110003"
  },
  {
    "text": "I pointed out you could actually\neliminate equality constraints. You just don't need them.",
    "start": "1110003",
    "end": "1115150"
  },
  {
    "text": "I mean, they don't\nactually bring any-- we can just get rid of them\narbitrarily, pull them back. It doesn't make any difference.",
    "start": "1115150",
    "end": "1121010"
  },
  {
    "text": "And the way you do that\nis if you have an equality constraint Ax equals b,\nthen you parameterize it",
    "start": "1121010",
    "end": "1126820"
  },
  {
    "text": "as the image of an\naffine function. And that requires you to\nfind a particular solution,",
    "start": "1126820",
    "end": "1133630"
  },
  {
    "text": "say, x hat. And then you find a\nmatrix F, whose range is the null space of A. So\nthat's actually all-- yeah,",
    "start": "1133630",
    "end": "1142840"
  },
  {
    "text": "that's actually all you need. And there's very\nsimple ways to do that. You could do a QR factorization\non A transpose or something.",
    "start": "1142840",
    "end": "1150520"
  },
  {
    "text": "This is just linear\nalgebra at that point. OK. And that's the idea.",
    "start": "1150520",
    "end": "1156070"
  },
  {
    "text": "Then what you do-- it says\nthat this is a parameter-- this is a-- you'd call this a\nfree parameter parameterization",
    "start": "1156070",
    "end": "1163390"
  },
  {
    "text": "of this set, which\nis affine set, which is described by constraints.",
    "start": "1163390",
    "end": "1168700"
  },
  {
    "text": "So that's how you'd\ndescribe that. OK. Then you just solve this\nproblem because everything",
    "start": "1168700",
    "end": "1175690"
  },
  {
    "text": "that satisfies Ax equals\nb has exactly this form. And now this thing is just--\nthat's an unconstrained problem",
    "start": "1175690",
    "end": "1182020"
  },
  {
    "text": "with some variable z. I could do all sorts of things. I could actually-- oh, once\nI know, by the way, z*,",
    "start": "1182020",
    "end": "1191560"
  },
  {
    "text": "I can reconstruct x* and even\nnu* as well by these methods.",
    "start": "1191560",
    "end": "1198620"
  },
  {
    "text": "OK, so that's an\nequivalent problem. And this just works\nby Newton's method.",
    "start": "1198620",
    "end": "1204260"
  },
  {
    "text": "So we just use Newton's\nmethod to solve that. And I'll just give\nyou a quick example.",
    "start": "1204260",
    "end": "1211930"
  },
  {
    "text": "Here's a famous problem. It's resource allocation. So x1 up to xn are a bunch of\nresources that I am going to--",
    "start": "1211930",
    "end": "1221889"
  },
  {
    "text": "those are the resources\nallocated to agents 1 through n. That's what the xi's are.\nb is my total budget.",
    "start": "1221890",
    "end": "1228003"
  },
  {
    "text": "That's how much\nof my resource I'm going to allocate across years. You can make the\nx's non-negative. It wouldn't make any difference.",
    "start": "1228003",
    "end": "1234640"
  },
  {
    "text": "And then I have a cost\nfunction for each of my agents, like f1 of x1 is the cost\nif I allocate x1 to agent 1.",
    "start": "1234640",
    "end": "1243549"
  },
  {
    "text": "So that's how that works. So this is the total cost. And you have this\nequality constraint.",
    "start": "1243550",
    "end": "1250150"
  },
  {
    "text": "One way to eliminate it-- I mean, this is easy-- is what you do is you leave\nx1 through xn minus 1 alone.",
    "start": "1250150",
    "end": "1257200"
  },
  {
    "text": "They're going to be-- we just keep them. But we write xn as b minus them.",
    "start": "1257200",
    "end": "1262630"
  },
  {
    "text": "We just remove this. Then you put a function\nthat looks like this. So here, my z's are really\njust x1 to x n minus 1.",
    "start": "1262630",
    "end": "1270740"
  },
  {
    "text": "When I multiply by f-- I mean, you can check that this\none-- this thing is a basis.",
    "start": "1270740",
    "end": "1276680"
  },
  {
    "text": "I mean, by construction\nthis works. You can check that the\nrange of this matrix",
    "start": "1276680",
    "end": "1282530"
  },
  {
    "text": "is exactly the null\nspace of this matrix. That matrix is just a\n1's vector transpose.",
    "start": "1282530",
    "end": "1287540"
  },
  {
    "text": "So that's what that\nis in Ax equals b. OK. So then you end up with\nthis reduced problem.",
    "start": "1287540",
    "end": "1292640"
  },
  {
    "text": "There you go. Oh, any comments\nabout that function?",
    "start": "1292640",
    "end": "1300180"
  },
  {
    "text": "Anything you like about it? Can we all visualize\nits Hessian, please? What is the Hessian\nof this function?",
    "start": "1300180",
    "end": "1306480"
  },
  {
    "start": "1306480",
    "end": "1311760"
  },
  {
    "text": "What is it? It's what? An arrow. You know, it's not arrow. It's very close.",
    "start": "1311760",
    "end": "1317550"
  },
  {
    "text": "If you were to do one\nelimination, it would be arrow. But it's very close to arrow. It's not arrow. This is dense.",
    "start": "1317550",
    "end": "1322603"
  },
  {
    "text": "It's completely dense. But what is it? What's the Hessian of this part? Diagonal.",
    "start": "1322603",
    "end": "1328190"
  },
  {
    "text": "Diagonal. OK. How about the Hessian of this? What is it?",
    "start": "1328190",
    "end": "1333490"
  },
  {
    "text": "It's rank 1, right? You can just check. It's going to be-- actually, it's literally\nthe matrix of 1's.",
    "start": "1333490",
    "end": "1340840"
  },
  {
    "text": "It's actually 1's 1's\ntranspose times a number. That's what it is right. So yeah, I realize you're\njust seeing this all now.",
    "start": "1340840",
    "end": "1348820"
  },
  {
    "text": "But this is the\nimportant thing-- this is what you need\nto do, is to be able--",
    "start": "1348820",
    "end": "1354220"
  },
  {
    "text": "to actually be able to recognize\nthese things in the wild because you're\ngoing to want to be able to see things like this.",
    "start": "1354220",
    "end": "1359778"
  },
  {
    "text": "So yeah, so this\ndiagonal plus rank 1. So when you see this, you\nshould be able to look at that",
    "start": "1359778",
    "end": "1365290"
  },
  {
    "text": "and say to someone, I can\nsolve that in linear time. And they would say,\nwow, how do you do that?",
    "start": "1365290",
    "end": "1372040"
  },
  {
    "text": "I thought Newton-- I'm using\nSGD because it's too big, or I don't-- blah, blah, blah.",
    "start": "1372040",
    "end": "1377410"
  },
  {
    "text": "And you're like, no, no. They go, but I have\na million variables. And you're like, too bad. I can do it on my phone.",
    "start": "1377410",
    "end": "1384400"
  },
  {
    "text": "Everyone following this? So these are extremely\ngood things to know about.",
    "start": "1384400",
    "end": "1389769"
  },
  {
    "text": "Because, I don't know,\nevery day there's people who don't know these\nthings making the wrong--",
    "start": "1389770",
    "end": "1395300"
  },
  {
    "text": "arriving somewhere and\nsaying, oh, I tried NumPy and it was too\nslow or something. Yes?",
    "start": "1395300",
    "end": "1400580"
  },
  {
    "text": "I'm sorry, what is the\npoint of the F matrix? Because I don't see it\nin the reduced problem.",
    "start": "1400580",
    "end": "1406610"
  },
  {
    "text": "Oh, it is here. What I've written here, this is\nactually-- this is literally--",
    "start": "1406610",
    "end": "1412910"
  },
  {
    "text": "it's the original f of then\ncapital F z plus x hat.",
    "start": "1412910",
    "end": "1419030"
  },
  {
    "text": "So I'm just writing it out\nin its more natural form. But that's exactly what it is.",
    "start": "1419030",
    "end": "1424280"
  },
  {
    "text": "In fact, you can sort\nof see that, right? Here with this one. The F has an I stacked\non top of a minus 1's.",
    "start": "1424280",
    "end": "1432320"
  },
  {
    "text": "Here's the minus 1's, and\nhere's the I. That's an n by 1 by n minus 1 I--",
    "start": "1432320",
    "end": "1438559"
  },
  {
    "text": "n minus 1 by n minus 1, yeah. OK. OK, cool? This is-- OK.",
    "start": "1438560",
    "end": "1444789"
  },
  {
    "text": " So the Newton step--",
    "start": "1444790",
    "end": "1451160"
  },
  {
    "text": "you can also directly get-- you can actually directly\nget the Newton step",
    "start": "1451160",
    "end": "1456710"
  },
  {
    "text": "from the KKT system not\nby doing this reduction. And so to do that,\nyou form this system.",
    "start": "1456710",
    "end": "1463105"
  },
  {
    "text": "And this is just literally--\nthis is the definition of the Newton step. And it's lots of cool\ninterpretations of it.",
    "start": "1463105",
    "end": "1469019"
  },
  {
    "text": "One is this. You'd say, well, I want to\nminimize this smooth function subject to Ax equals b.",
    "start": "1469020",
    "end": "1474740"
  },
  {
    "text": "And someone says, cool,\nplease take your function f and make a second-order\nTaylor expansion, which",
    "start": "1474740",
    "end": "1481700"
  },
  {
    "text": "is going to be a constant\nplus a linear term plus a quadratic,\nconvex quadratic. And then you say you want to\nminimize that subject to Ax",
    "start": "1481700",
    "end": "1489500"
  },
  {
    "text": "equals b. But that matches something\nyou have a solver for. It's called linear-- it's\ncalled-- you just form a KKT",
    "start": "1489500",
    "end": "1494630"
  },
  {
    "text": "system and solve it. So that's one interpretation\nof what this is. ",
    "start": "1494630",
    "end": "1500860"
  },
  {
    "text": "Another interpretation\nis pretty cool, is that the so-called--\nthis Newton step",
    "start": "1500860",
    "end": "1506200"
  },
  {
    "text": "here actually comes\nfrom linearizing the optimality condition. So one of the optimality\nconditions is already linear.",
    "start": "1506200",
    "end": "1514360"
  },
  {
    "text": "This is already-- so no\nlinearization is needed here. It's already just a set\nof linear equations.",
    "start": "1514360",
    "end": "1520870"
  },
  {
    "text": "This one is going to be-- this one is going to\nhave to be approximated.",
    "start": "1520870",
    "end": "1527679"
  },
  {
    "text": "So we'll approximate this\nby its second-order-- sorry, its first-order expansion\nof the gradient, which",
    "start": "1527680",
    "end": "1534903"
  },
  {
    "text": "is, of course, related to\nthe second-order expansion of the function. And we get something like this. So you can say that the Newton\nstep are something like this.",
    "start": "1534903",
    "end": "1543310"
  },
  {
    "text": "It's a delta x\nand a w for which,",
    "start": "1543310",
    "end": "1552790"
  },
  {
    "text": "if you were to take that step\nand if your original function was quadratic, that\nwould actually just",
    "start": "1552790",
    "end": "1557919"
  },
  {
    "text": "be the optimal solution. That would just be the solution. Everybody got this? So this is the Newton step.",
    "start": "1557920",
    "end": "1567080"
  },
  {
    "text": "OK. The Newton decrement--\nwe saw this before in the\nunconstrained case, right?",
    "start": "1567080",
    "end": "1573510"
  },
  {
    "text": "And it turns out you can write\nit lots of different ways. But it is the norm of the\nNewton step in the metric given",
    "start": "1573510",
    "end": "1583710"
  },
  {
    "text": "by the Hessian. But maybe a better way is this. It basically says it's\nthe 1/2 lambda squared",
    "start": "1583710",
    "end": "1591000"
  },
  {
    "text": "is actually the\npredicted decrease in f if you took the Newton step and\nif the function were convex--",
    "start": "1591000",
    "end": "1597840"
  },
  {
    "text": "sorry, quadratic, so\nthat it was exact. So it's actually, as\na practical matter,",
    "start": "1597840",
    "end": "1603490"
  },
  {
    "text": "a very good approximation of\nhow far from suboptimal you are.",
    "start": "1603490",
    "end": "1609420"
  },
  {
    "text": "OK. So this is the Newton decrement. And now we get Newton's method.",
    "start": "1609420",
    "end": "1614710"
  },
  {
    "text": "So here it is. You start with something\nthat's feasible. So Ax equals b and\nx is in the domain.",
    "start": "1614710",
    "end": "1621960"
  },
  {
    "text": "And you compute the\nNewton step and decrement. You stop. Otherwise, you do a line\nsearch by backtracking",
    "start": "1621960",
    "end": "1628380"
  },
  {
    "text": "and then you update. Actually, you might ask,\nhow is this algorithm different from Newton's method\nwithout equality constraints?",
    "start": "1628380",
    "end": "1637260"
  },
  {
    "text": "Reasonable question. The answer is, it's not. They're identical. It's just the same. What's different is, for\nexample, when you compute",
    "start": "1637260",
    "end": "1645870"
  },
  {
    "text": "the Newton step, if you\nhave equality constraints, you're actually solving that\nKKT system, not just with",
    "start": "1645870",
    "end": "1652290"
  },
  {
    "text": "the Hessian. But it's just exactly the same. So that's the idea.",
    "start": "1652290",
    "end": "1657809"
  },
  {
    "text": "And this is the so-called\nfeasible descent method. What happens is xk-- of course, x0 0 is\nfeasible because you",
    "start": "1657810",
    "end": "1664080"
  },
  {
    "text": "have to start it with something\nthat satisfies Ax equals b and it's in the domain. You're always in the domain,\nalways have Ax equals b.",
    "start": "1664080",
    "end": "1672750"
  },
  {
    "text": "And in fact, not only that, but\nthe function value goes down. And this algorithm\nis affine invariant.",
    "start": "1672750",
    "end": "1678549"
  },
  {
    "text": "So if you change coordinates\nor rescale the variables, at least to first-- it doesn't make any\ndifference at all.",
    "start": "1678550",
    "end": "1685110"
  },
  {
    "text": "You get exactly the same number\nof iterates-- same iterates. OK. So that's Newton's method.",
    "start": "1685110",
    "end": "1691830"
  },
  {
    "text": "What's cool is that we\ndon't need any more-- we don't need no\nmore analysis of it. And the reason is this.",
    "start": "1691830",
    "end": "1697500"
  },
  {
    "text": "It turns out if you apply\nNewton's method to this eliminate-- where you eliminate\nthe equality constraints",
    "start": "1697500",
    "end": "1704310"
  },
  {
    "text": "like this, if you\nwork out what it is, it turns out it is\nidentical to just running",
    "start": "1704310",
    "end": "1710159"
  },
  {
    "text": "Newton's method. So it's just-- it's actually\njust exactly the same thing.",
    "start": "1710160",
    "end": "1718930"
  },
  {
    "text": "Everything is the\nsame, like absolute-- the iterates are identical,\nthe primal iterates, the dual iterates.",
    "start": "1718930",
    "end": "1724240"
  },
  {
    "text": "The stopping-- the\nlambda is the same. So it basically-- that\nmeans we can actually",
    "start": "1724240",
    "end": "1730600"
  },
  {
    "text": "pull in the proof of convergence\nfrom whatever last lecture. So that's all we need.",
    "start": "1730600",
    "end": "1737440"
  },
  {
    "text": "OK. So we don't ever have-- we're not going to talk about\ndoes it converge and all that? The answer is yeah, it does.",
    "start": "1737440",
    "end": "1744340"
  },
  {
    "text": "And we'll see-- then\nyou'd say, well, what would be the difference\nbetween these two? And this gets you\ninto the usual thing.",
    "start": "1744340",
    "end": "1752590"
  },
  {
    "text": "In this case you say,\nwell, I'm eliminating. And so I'm solving-- let's say you have 1,000\nvariables and 100 constraints.",
    "start": "1752590",
    "end": "1759160"
  },
  {
    "text": "Then someone would say,\nwhat are you doing? You'd say, well, I'm solving-- I've eliminated the\nequality constraints,",
    "start": "1759160",
    "end": "1766059"
  },
  {
    "text": "so I'm solving a problem\nwith 900 variables. Everybody following this? You'd say, well,\nwhat are you doing?",
    "start": "1766060",
    "end": "1771600"
  },
  {
    "text": "And you go, yeah? I'm solving a problem\nwith 1,100 variables. Everybody got this?",
    "start": "1771600",
    "end": "1778410"
  },
  {
    "text": "So that's if you\nsolve the KKT system because it's a 1,000 by 1,000\nblock stacked on top of a 100",
    "start": "1778410",
    "end": "1785820"
  },
  {
    "text": "by 1,000 A matrix. So someone else\nwould look at them",
    "start": "1785820",
    "end": "1792000"
  },
  {
    "text": "and say, but wait a minute,\nlast time I checked, 1,100 cubed is bigger\nthan 900 cubed.",
    "start": "1792000",
    "end": "1798420"
  },
  {
    "text": "And you'd say, it\nis, you're right. But what we'll find is actually\nthat it's almost always better",
    "start": "1798420",
    "end": "1805049"
  },
  {
    "text": "to solve the bigger system. So this is the other thing. If someone comes up to\nyou-- if the matrices have",
    "start": "1805050",
    "end": "1811050"
  },
  {
    "text": "no structure at\nall, obviously you'd prefer to solve a smaller\nsystem of equations.",
    "start": "1811050",
    "end": "1816760"
  },
  {
    "text": "But when there's structure,\nthis is, generally speaking, completely false.",
    "start": "1816760",
    "end": "1823260"
  },
  {
    "text": "And it's not intuitive.  OK.",
    "start": "1823260",
    "end": "1829190"
  },
  {
    "text": "Aha. So now we're getting to\nsomething which is related to something you'll\ndo in your homework,",
    "start": "1829190",
    "end": "1835149"
  },
  {
    "text": "is the so-called\ninfeasible Newton-- infeasible start Newton method.",
    "start": "1835150",
    "end": "1841419"
  },
  {
    "text": "And what it does is it's\nactually a simple extension. Before, we assumed\nthat x was feasible,",
    "start": "1841420",
    "end": "1850100"
  },
  {
    "text": "and so Ax minus b was 0. Then this just says, well,\nsuppose you're not 0.",
    "start": "1850100",
    "end": "1856179"
  },
  {
    "text": "What is the Newton step? And it's defined by this here.",
    "start": "1856180",
    "end": "1862000"
  },
  {
    "text": "And you can write out all\nsorts of cool things about it. It's linearizing the-- if\nyou write the residual as--",
    "start": "1862000",
    "end": "1870610"
  },
  {
    "text": "well, two things here. We can have the-- if you write out\nthis thing, if you",
    "start": "1870610",
    "end": "1876790"
  },
  {
    "text": "have Ax equals-- if Ax\nequals b, and this residual should be about 0,\nyou get these things.",
    "start": "1876790",
    "end": "1882310"
  },
  {
    "text": "And the solution of this is\ncalled the infeasible Newton step. And so this is\nbasically 90%, 95%",
    "start": "1882310",
    "end": "1891250"
  },
  {
    "text": "of what people actually use. Or actually, since you've\nbeen solving lots of problems, it's what [? EKOS ?] and\nall sorts of other things",
    "start": "1891250",
    "end": "1898420"
  },
  {
    "text": "have been using on your behalf. You didn't know it, but\nthat's what it was doing.",
    "start": "1898420",
    "end": "1905380"
  },
  {
    "text": "You'll know all the details by\nmaybe next lecture or something like that. OK, so this is the infeasible\nstart Newton method.",
    "start": "1905380",
    "end": "1913120"
  },
  {
    "text": "And there's some very\ninteresting things about it. First, if you take a step\nof size 1, you will--",
    "start": "1913120",
    "end": "1920460"
  },
  {
    "text": "then the constraints will--\nthen the equality constraints will hold. You will be feasible if\nyou take a step of 1.",
    "start": "1920460",
    "end": "1926260"
  },
  {
    "text": "And you'll always be\nfeasible thereafter. Because once you take a step\nof 1, and once x is feasible,",
    "start": "1926260",
    "end": "1934450"
  },
  {
    "text": "this is 0. And this just-- it\nactually becomes, except for the line search\nand the stopping criterion,",
    "start": "1934450",
    "end": "1941770"
  },
  {
    "text": "it's basically identical\nto just Newton's method with equality constraints. That's why it's called\ninfeasible start.",
    "start": "1941770",
    "end": "1948750"
  },
  {
    "text": "We'll see why you\nmight want to use this. There are some very good\nreasons I'll get to in a minute. Yeah?",
    "start": "1948750",
    "end": "1954659"
  },
  {
    "text": "So you said that once\nit takes a step of 1, it becomes the [INAUDIBLE]\nexcept for the stopping criterion and stuff like that.",
    "start": "1954660",
    "end": "1960900"
  },
  {
    "text": "Would do in practice\ntake a step of 1 and then change back\nto the original-- No. You may not be allowed\nto take a step of 1.",
    "start": "1960900",
    "end": "1967200"
  },
  {
    "text": "And it's a good--\nso actually, what I'm about to say is highly\nrelevant for the homework you're going to do in\nthe next couple of days.",
    "start": "1967200",
    "end": "1974130"
  },
  {
    "text": "You may not be\nallowed-- you may not be able to take-- because,\nfor example, a step of 1 might take you outside\nthe domain of f.",
    "start": "1974130",
    "end": "1982285"
  },
  {
    "text": "Then so you may not--\nthat's one reason. We'll look at the line search\nfor the infeasible start Newton",
    "start": "1982285",
    "end": "1988090"
  },
  {
    "text": "method actually works with-- instead of working with\nthe function value, it actually works with\nthe norm of the residual.",
    "start": "1988090",
    "end": "1995860"
  },
  {
    "text": "So it actually works with\nthe norm of the residual. So you look at-- I'll call it z, right?",
    "start": "1995860",
    "end": "2004410"
  },
  {
    "text": "You would look at this. z is going to be like\nx stacked on top of nu.",
    "start": "2004410",
    "end": "2011280"
  },
  {
    "text": "So in fact, people call z\nthe primal dual variable because it's basically, top\nof it is the primal variable",
    "start": "2011280",
    "end": "2017520"
  },
  {
    "text": "and the bottom is\nthe dual variable. So OK.",
    "start": "2017520",
    "end": "2023100"
  },
  {
    "text": "And I think-- let's see. So here's the infeasible\nstart Newton method. You will implement this,\nso I would strongly",
    "start": "2023100",
    "end": "2029550"
  },
  {
    "text": "recommend listening. You also want to\nlook at the book and stuff like that to read\na couple of pages about this.",
    "start": "2029550",
    "end": "2035830"
  },
  {
    "text": "So here, the initial point\ndoes not have to satisfy Ax equals b, period.",
    "start": "2035830",
    "end": "2042800"
  },
  {
    "text": "It does not have to. It must be in the\ndomain of your function. So if your function\nwas something",
    "start": "2042800",
    "end": "2049629"
  },
  {
    "text": "like minus the sum of the\nlogs of the components, someone want to suggest a\nreasonable starting point?",
    "start": "2049630",
    "end": "2056050"
  },
  {
    "start": "2056050",
    "end": "2061460"
  },
  {
    "text": "If I say, give me\na function, give me a number that's in the\ndomain of negative log x,",
    "start": "2061460",
    "end": "2067820"
  },
  {
    "text": "just what's the first one\nthat comes to your head? 1. There you go, 1. Good, I like it. OK. The truth is you could have\nsaid any positive number,",
    "start": "2067820",
    "end": "2074690"
  },
  {
    "text": "and I couldn't have\nsaid anything about it. So OK. OK, so that's the\ninitialization.",
    "start": "2074690",
    "end": "2080789"
  },
  {
    "text": "So you compute the primal\nand dual Newton steps. The backtracking\nsearch is actually on the norm of the residual.",
    "start": "2080790",
    "end": "2088669"
  },
  {
    "text": "And we have to look\nat this equation pretty-- we have to look\nat this sort of pseudocode pretty carefully here.",
    "start": "2088670",
    "end": "2094879"
  },
  {
    "text": "This says that if\nthis norm does not go down by some\namount like that,",
    "start": "2094880",
    "end": "2100730"
  },
  {
    "text": "then you have to\nmultiply t by beta. Beta is this number.",
    "start": "2100730",
    "end": "2105980"
  },
  {
    "text": "Like, typically it's a 1/2\nor something like that. Actually, people use\nall sorts of things. But they would never--\nyou could use 1/2, 1/4.",
    "start": "2105980",
    "end": "2111890"
  },
  {
    "text": "Some people use 0.1. It doesn't matter, it turns out. OK. Very important thing here\nis one of the requirements",
    "start": "2111890",
    "end": "2120230"
  },
  {
    "text": "is that x plus tx Newton has\nto be in the domain of f.",
    "start": "2120230",
    "end": "2126390"
  },
  {
    "text": "So let's suppose the domain\nof f was r plus plus to the n. That's what it's going to be\nfor your homework problem.",
    "start": "2126390",
    "end": "2134300"
  },
  {
    "text": "You calculate a Newton step. Here's what you do.",
    "start": "2134300",
    "end": "2140020"
  },
  {
    "text": "You don't just take\nx plus t delta x and then evaluate the\ngradient and Hessian.",
    "start": "2140020",
    "end": "2146390"
  },
  {
    "text": "Because if x is\noutside the domain-- so an entry of x is\nnegative, for example, or 0,",
    "start": "2146390",
    "end": "2153080"
  },
  {
    "text": "then the gradient-- actually, you're really\ngoing to be messed up, especially if you had formulas\nlike the gradient is 1 over x--",
    "start": "2153080",
    "end": "2160160"
  },
  {
    "text": "minus 1 over x-- and the Hessian is\n1 over x squared. Unfortunately, those things will\nevaluate correctly to a number",
    "start": "2160160",
    "end": "2167870"
  },
  {
    "text": "even if x is negative. Everybody following this? But nothing good is going\nto happen after that.",
    "start": "2167870",
    "end": "2173840"
  },
  {
    "text": "Absolutely nothing good\nis going to happen. Your algorithm will not work. It's complete-- nothing\nwill happen, right?",
    "start": "2173840",
    "end": "2180630"
  },
  {
    "text": "So that's why I almost wish\nthat they had in math libraries you could simply say the\ndomain of-- actually,",
    "start": "2180630",
    "end": "2188250"
  },
  {
    "text": "no, you'll evaluate-- if you\nwere to evaluate-- so these are interesting because you actually\ndon't evaluate f anywhere here.",
    "start": "2188250",
    "end": "2193950"
  },
  {
    "text": "But if you did, you'd know you\nwere in trouble because you'd get an error saying,\nhey, you're trying",
    "start": "2193950",
    "end": "2199051"
  },
  {
    "text": "to evaluate the log of\na negative number or 0. And that's not going to\ngo very well unless you",
    "start": "2199051",
    "end": "2205802"
  },
  {
    "text": "have a complex library or\nsomething like that, right? OK. So the way to do\nthis, then-- so what",
    "start": "2205802",
    "end": "2210950"
  },
  {
    "text": "that means is if this\nis not in the domain, this isn't even defined, this\njust means it's infinity. So you would take a t, check--",
    "start": "2210950",
    "end": "2218720"
  },
  {
    "text": "check if x plus t\ndelta x is positive. If it is not positive,\ndon't evaluate this.",
    "start": "2218720",
    "end": "2227930"
  },
  {
    "text": "Just times t times equals beta. And you keep doing that\nuntil you're actually",
    "start": "2227930",
    "end": "2233630"
  },
  {
    "text": "in the domain, which\nmeans it's positive, then you can evaluate this. So I don't know\nif that was clear,",
    "start": "2233630",
    "end": "2238820"
  },
  {
    "text": "but maybe to those working\non this already, it is. So OK.",
    "start": "2238820",
    "end": "2244430"
  },
  {
    "text": "So I think that was\nenough of a hint. But it's an important one. So OK.",
    "start": "2244430",
    "end": "2252020"
  },
  {
    "text": "So this is it. And the basic idea of this is\nthat the directional derivative",
    "start": "2252020",
    "end": "2259370"
  },
  {
    "text": "is, in fact, also equal to\nminus the norm of the residual. So what that says is if\nyou're doing a line search,",
    "start": "2259370",
    "end": "2266430"
  },
  {
    "text": "it's pretty cool. It looks like this. This is t. This is the norm-- I'm going to plot here\nthe norm of the residual.",
    "start": "2266430",
    "end": "2274950"
  },
  {
    "text": "So at t equals 0, it's\nat your current point. And what it says is that\nthe slope of this curve",
    "start": "2274950",
    "end": "2280170"
  },
  {
    "text": "is exactly minus 1. So if you're going to make that\nstep backtracking or Armijo",
    "start": "2280170",
    "end": "2287550"
  },
  {
    "text": "step or whatever you call it,\nit's going to look like this. What it really-- it's just\ngoing to look like that.",
    "start": "2287550",
    "end": "2293039"
  },
  {
    "text": "And so you will\ndegrade this by alpha. And you will take the first\ntime you get something",
    "start": "2293040",
    "end": "2299130"
  },
  {
    "text": "that is below that curve. And you absolutely have to\nbecause if t is small enough, it's got to happen. So OK, yeah.",
    "start": "2299130",
    "end": "2307140"
  },
  {
    "text": "You actually did this, even\nthough yours isn't quite-- there's some questions as to\nwhether or not it's working.",
    "start": "2307140",
    "end": "2313800"
  },
  {
    "text": "I think I made this\nerror, probably. OK. Well, sorry. That's what you\nget for doing it-- so we should have done--",
    "start": "2313800",
    "end": "2319890"
  },
  {
    "text": "yeah, anyway, that's a timing. That's right. That's a good experience. It's a good life\nexperience to have.",
    "start": "2319890",
    "end": "2325530"
  },
  {
    "text": "Everybody should have--\nyou should actually at once in their life write\na Newton thing",
    "start": "2325530",
    "end": "2331228"
  },
  {
    "text": "or something like that\nwhere they were not-- they were too cavalier about domains,\nand nothing-- and it was bad.",
    "start": "2331228",
    "end": "2337260"
  },
  {
    "text": "So OK. All right. So you'll be implementing\nthis for the homework.",
    "start": "2337260",
    "end": "2344880"
  },
  {
    "text": "OK. Let's talk about how you\nsolve the KKT system.",
    "start": "2344880",
    "end": "2350910"
  },
  {
    "text": "Well, one obvious thing, which\nis, by the way, not bad at all, is actually just doing LDL\ntranspose factorization.",
    "start": "2350910",
    "end": "2357930"
  },
  {
    "text": "So here's good news about\nLDL transpose factorization, is if you use a good\nsparse package, then--",
    "start": "2357930",
    "end": "2365670"
  },
  {
    "text": "I mean, I'm assuming\nH and A are sparse. Then it's actually going\nto exploit all the--",
    "start": "2365670",
    "end": "2371369"
  },
  {
    "text": "it's going to exploit a\nlot of structure there, more even than you know about. So I mean, you could do\nthis for a control problem,",
    "start": "2371370",
    "end": "2379530"
  },
  {
    "text": "signal processing-- all sorts\nof stuff you could just do this. You can do elimination.",
    "start": "2379530",
    "end": "2386460"
  },
  {
    "text": "This is often done. If H is diagonal, so if the\nHessian is diagonal-- like,",
    "start": "2386460",
    "end": "2392430"
  },
  {
    "text": "for example, if the function\nis separable, it's separable but they're coupled by\nthe equality constraints,",
    "start": "2392430",
    "end": "2399150"
  },
  {
    "text": "then H will be diagonal. And in that point,\nthat's tempting enough",
    "start": "2399150",
    "end": "2405269"
  },
  {
    "text": "for you to do elimination on it. So you would eliminate\nbecause we know the inverse. H could be banded-- same story.",
    "start": "2405270",
    "end": "2411809"
  },
  {
    "text": "So I mean, here's an\nexample of a banded one. Optimal control with\na constraint that--",
    "start": "2411810",
    "end": "2420180"
  },
  {
    "text": "some constraints that\ncouple states far apart. For example, you're\nmaking a tour. You say, make me a\nlittle drone tour.",
    "start": "2420180",
    "end": "2426840"
  },
  {
    "text": "And it's got to start and\nend at the same place. Well, OK. So when I hear that, when\nI hear optimal control,",
    "start": "2426840",
    "end": "2434590"
  },
  {
    "text": "I visualize immediately\na banded system. I know immediately\nthe banded system is going to drop down somewhere\ndeep in the linear algebra.",
    "start": "2434590",
    "end": "2441730"
  },
  {
    "text": "I'm going to solve with banded\nsystem, can do that super fast. But when I say\nsomething like, you have to be back at\nthe same place, that",
    "start": "2441730",
    "end": "2448390"
  },
  {
    "text": "actually couples the\nbeginning position and the final position. And that's actually\none of these-- that's going to be an\nequality constraint that",
    "start": "2448390",
    "end": "2454717"
  },
  {
    "text": "breaks the bandedness. But I'll also note that\nI can do that super fast.",
    "start": "2454717",
    "end": "2461930"
  },
  {
    "text": "So in other words,\nthat's just a nonproblem. OK. All right. So to eliminate here H\ninverse-- and usually,",
    "start": "2461930",
    "end": "2471369"
  },
  {
    "text": "this is done when H is\nsomething like diagonal. So you would just do a\nblock elimination here.",
    "start": "2471370",
    "end": "2478870"
  },
  {
    "text": "And you'd end up with\nsomething looking like that. So you would first-- you\nwould form this matrix, then",
    "start": "2478870",
    "end": "2488080"
  },
  {
    "text": "factorize it, then solve. And H inverse, you\nshouldn't even worry about,",
    "start": "2488080",
    "end": "2494263"
  },
  {
    "text": "because it's diagonal. So it's kind of a silly thing. And then you'd end up\nwith something like that.",
    "start": "2494263",
    "end": "2500910"
  },
  {
    "text": "So this is-- once you form-- this would allow\nyou to solve for w.",
    "start": "2500910",
    "end": "2506100"
  },
  {
    "text": "Once you solve for w, you\nsolve for v over here, right? And that's cheap. Well, it's silly.",
    "start": "2506100",
    "end": "2511290"
  },
  {
    "text": "H is diagonal, so that\nwould be-- that's the idea. OK. ",
    "start": "2511290",
    "end": "2517400"
  },
  {
    "text": "All right. OK, so we're going to look at--",
    "start": "2517400",
    "end": "2522590"
  },
  {
    "text": "so actually, what's weird\nabout all these things is that they're all\nkind of the same, all these different-- so now you\nhave lots of different methods,",
    "start": "2522590",
    "end": "2529420"
  },
  {
    "text": "right? By the way, this is shockingly\nclose to what you will be solving in your homework.",
    "start": "2529420",
    "end": "2535550"
  },
  {
    "text": "So again, you might\nwant to listen up as to how all this works. So we now have all\nsorts of different ways.",
    "start": "2535550",
    "end": "2544370"
  },
  {
    "text": "So you could say, yeah,\nI could solve that. It's smooth problem.",
    "start": "2544370",
    "end": "2550269"
  },
  {
    "text": "And you can say, oh, I\ncould solve the dual here. And in this case,\nyou could actually-- you could reconstruct the\nsolution of the primal",
    "start": "2550270",
    "end": "2558100"
  },
  {
    "text": "from the dual. It's not always true, but it\nis if the objective is strictly convex, which it is here.",
    "start": "2558100",
    "end": "2563980"
  },
  {
    "text": "So you could say--\nyou could do this. All right. So let's just do\nthe simple thing.",
    "start": "2563980",
    "end": "2570220"
  },
  {
    "text": "Actually, here's\nwhat's interesting. They all have different\ninitialization requirements.",
    "start": "2570220",
    "end": "2575349"
  },
  {
    "text": "And actually, that's\nactually what's going to actually be the-- you're going to see all these\nmethods are going to turn out",
    "start": "2575350",
    "end": "2581588"
  },
  {
    "text": "to be essentially the same. They're not the same. But in spirit, the work you\ndo per iteration, if you know",
    "start": "2581588",
    "end": "2587420"
  },
  {
    "text": "what you're doing, is the same. So there's just no difference\nbetween any of them. So-- well, except\nthe initialization.",
    "start": "2587420",
    "end": "2593160"
  },
  {
    "text": "So if we just do Newton's method\nwith equality constraints, that says someone has got to come\nup with an x that is positive,",
    "start": "2593160",
    "end": "2601260"
  },
  {
    "text": "strictly positive, and\nsatisfies Ax equals b. If these are just general A's--\nif this is just a general A",
    "start": "2601260",
    "end": "2607880"
  },
  {
    "text": "and b, how would\nyou find such an x? I guess you'd have to\nsolve-- in general,",
    "start": "2607880",
    "end": "2613860"
  },
  {
    "text": "you'd have to solve\nan LP to do that. And this is bad because this\nis maybe simpler than an LP.",
    "start": "2613860",
    "end": "2620870"
  },
  {
    "text": "And it's a general rule\nthat you should not have a subroutine that's more\ncomplicated than just solving",
    "start": "2620870",
    "end": "2627170"
  },
  {
    "text": "the problem itself. So on the other hand, sometimes\nit is easy to come up with a--",
    "start": "2627170",
    "end": "2634078"
  },
  {
    "text": "even though there's an\nequality constraint, sometimes it's easy to do. How about resource\nallocation where--",
    "start": "2634078",
    "end": "2639950"
  },
  {
    "text": "here's the constraint. Ready? The sum of the x's is equal-- I'm going to make my budget b.",
    "start": "2639950",
    "end": "2645708"
  },
  {
    "text": "There you go. Sum of the x's is equal to b. Can someone suggest-- and\nsuppose that all of the--",
    "start": "2645708",
    "end": "2652520"
  },
  {
    "text": "well, let's see. Suppose all of the functions\nthat I want to minimize-- or actually, you would call this\nlog utility, which is actually",
    "start": "2652520",
    "end": "2659569"
  },
  {
    "text": "widely used. So this would be log utility\nallocation of resources or something like that.",
    "start": "2659570",
    "end": "2666920"
  },
  {
    "text": "Someone got a suggestion for-- can you think of\na positive vector that-- so b is a scalar\nfor us now, right?",
    "start": "2666920",
    "end": "2674130"
  },
  {
    "text": "It's sum xi equals b. That's the budget. So I'm deciding how much I/O\nbandwidth or something like",
    "start": "2674130",
    "end": "2680580"
  },
  {
    "text": "that a bunch of processes get. So somebody-- how\nwould you-- what's",
    "start": "2680580",
    "end": "2686190"
  },
  {
    "text": "an initial resource allocation? What? b divided by n. Perfect. Uniform. Yeah, you just do\neach xi is b over n,",
    "start": "2686190",
    "end": "2693720"
  },
  {
    "text": "and you put a comment in the\ncode that says we initialize with uniform allocation.",
    "start": "2693720",
    "end": "2699900"
  },
  {
    "text": "Done. So then you can do this. OK. All right. So here's what happens\nif you run this.",
    "start": "2699900",
    "end": "2705185"
  },
  {
    "text": "I think this is just run from\nseveral different initial conditions. And you see your\nclassic Newton thing.",
    "start": "2705185",
    "end": "2710609"
  },
  {
    "text": "This is what you're looking for. This is a log plot,\nand this is linear. This is a semi-log plot.",
    "start": "2710610",
    "end": "2716470"
  },
  {
    "text": "And you're looking\nfor this acceleration. That's when you declare success. That's Newton's-- that's how\nyou know you actually have",
    "start": "2716470",
    "end": "2724980"
  },
  {
    "text": "an implementation\nof Newton's method. OK. So OK. And you have that.",
    "start": "2724980",
    "end": "2730650"
  },
  {
    "text": "OK, that's one.  We could actually apply\nNewton's method to the dual.",
    "start": "2730650",
    "end": "2738940"
  },
  {
    "text": "And let's actually see. I think-- let's see. Did I say what the-- oh, I\ndid say what the numbers were.",
    "start": "2738940",
    "end": "2744610"
  },
  {
    "text": "Aha. OK. So here, I said that there's\n100 equality constraints and 500 variables, right?",
    "start": "2744610",
    "end": "2750970"
  },
  {
    "text": "So in this problem,\nyeah, this is--",
    "start": "2750970",
    "end": "2756099"
  },
  {
    "text": "we have basically 500 variables. But then someone says, oh, no,\nno, I learned about this dual.",
    "start": "2756100",
    "end": "2763467"
  },
  {
    "text": "It's really awesome. Plus, you can-- it\nsounds fancy, and so you can impress your friends if\nthey're gullible or whatever.",
    "start": "2763467",
    "end": "2768849"
  },
  {
    "text": "And you can say, oh, no, no,\nno, to solve that resource allocation-- I'm going to solve the\ndual resource allocation.",
    "start": "2768850",
    "end": "2774513"
  },
  {
    "text": "And if you say it the right way,\nit sounds really sophisticated. And then someone would say-- you could even then say things\nlike, actually, what I'm doing,",
    "start": "2774513",
    "end": "2781120"
  },
  {
    "text": "mine is a price discovery method\nbecause the dual variables are, in fact, precisely--",
    "start": "2781120",
    "end": "2788200"
  },
  {
    "text": "in a resource allocation,\nthey are precisely the marginal utilities\nof the things.",
    "start": "2788200",
    "end": "2793630"
  },
  {
    "text": "So you'd say, oh,\nno, I'm using-- I'm solving the dual problem. And you could blabber on and on. And you'd say, also, plus\nI only have 100 variables.",
    "start": "2793630",
    "end": "2802430"
  },
  {
    "text": "Whoever was solving the\nNewton thing, they had 500. And last time I checked,\n100 is smaller than 500.",
    "start": "2802430",
    "end": "2807980"
  },
  {
    "text": "So should be-- I should be a factor\nof 5 cubed better.",
    "start": "2807980",
    "end": "2813500"
  },
  {
    "text": "I should be 100x faster. Everybody following this? This is what someone might say. And actually, people\nsay this all the time.",
    "start": "2813500",
    "end": "2819252"
  },
  {
    "text": "It's completely\nwrong, of course. So OK.",
    "start": "2819252",
    "end": "2824660"
  },
  {
    "text": "So we'll solve this method. And then you get\ndown, and you say, oh, look, you started from a\ncouple of different things. Oh, by the way,\nthis requires-- you",
    "start": "2824660",
    "end": "2830888"
  },
  {
    "text": "have to have something\nfeasible for the dual. And that's basically,\nI have to find a vector with A\ntranspose nu positive.",
    "start": "2830888",
    "end": "2838100"
  },
  {
    "text": "Once again, how do you find\nsuch a-- well, it depends. In some cases, it's easy. In the simple resource\nallocation, it's easy.",
    "start": "2838100",
    "end": "2844790"
  },
  {
    "text": "In others, it's not. OK. Everybody following this?",
    "start": "2844790",
    "end": "2849890"
  },
  {
    "text": "Then they'd say, oh, look,\nI had nine iterations. That was fewer than you\ndid with Newton's method.",
    "start": "2849890",
    "end": "2856700"
  },
  {
    "text": "Newton's method\ntook 14 iterations. Plus, you have 500 variables. I have 100 variables,\nand I did 9.",
    "start": "2856700",
    "end": "2862760"
  },
  {
    "text": "Everybody following this? So I have heard people\nsay this to each other or put this in slides.",
    "start": "2862760",
    "end": "2869320"
  },
  {
    "text": "It turns out it's all\nso wrong and so stupid, it's not even funny. So OK. So we'll get to that.",
    "start": "2869320",
    "end": "2876460"
  },
  {
    "text": "That's a fun part in the end. Then you'd say, well, there's\nthe infeasible start Newton method.",
    "start": "2876460",
    "end": "2881550"
  },
  {
    "text": "This is the only one that\nhas an extremely simple initialization.",
    "start": "2881550",
    "end": "2886809"
  },
  {
    "text": "You don't have to\nsatisfy Ax equals b. You only have to satisfy that\nyou're in the domain of f. f is the sum of negative logs.",
    "start": "2886810",
    "end": "2894070"
  },
  {
    "text": "The domain is r\nplus plus to the n. So you initialize with 1's or\nyour favorite other positive",
    "start": "2894070",
    "end": "2900340"
  },
  {
    "text": "vector, and you're done. And you'd say, look,\nhere's what it is. And they'd say, oh,\nyou're doing 22 steps.",
    "start": "2900340",
    "end": "2906430"
  },
  {
    "text": "That's-- anyway, this\nis all just silly. So OK. Let's look a little bit more\nclosely at what happens.",
    "start": "2906430",
    "end": "2914080"
  },
  {
    "text": "Oh, by the way, these methods\nwould be wildly different if you used naive\nlinear algebra.",
    "start": "2914080",
    "end": "2921340"
  },
  {
    "text": "If you used naive\nnumerical linear algebra, they would be wild-- if you\nactually formed a KKT matrix",
    "start": "2921340",
    "end": "2926410"
  },
  {
    "text": "or whatever, the full\nthing, and solved it, this would be different.",
    "start": "2926410",
    "end": "2932079"
  },
  {
    "text": "But if you actually do the\nanalysis to figure out-- assuming you actually know\nhow to use linear algebra,",
    "start": "2932080",
    "end": "2937660"
  },
  {
    "text": "then it turns out,\nwell, it's not shocking. They're all actually\nidentical in complexity.",
    "start": "2937660",
    "end": "2943870"
  },
  {
    "text": "We'll see how that works. So to use Newton's method, you\nuse block elimination on this.",
    "start": "2943870",
    "end": "2950230"
  },
  {
    "text": "And you would end\nup solving something that looks like A times\nthe diagonal matrix times A transpose.",
    "start": "2950230",
    "end": "2956470"
  },
  {
    "text": "That's what you'd-- you'd form\nand solve that system, OK?",
    "start": "2956470",
    "end": "2962920"
  },
  {
    "text": "OK. Notice that this\nis the smaller one. That's the 100 by\n100 linear system.",
    "start": "2962920",
    "end": "2969860"
  },
  {
    "text": "If you wanted to\ndo the dual, you'd actually have to work\nout what is the Hessian",
    "start": "2969860",
    "end": "2975590"
  },
  {
    "text": "of the dual objective. And it turns out that looks--\nit's going to be identical. You're going to solve something\nthat looks like A times",
    "start": "2975590",
    "end": "2982880"
  },
  {
    "text": "the diagonal matrix,\npositive matrix, times A transpose\ntimes delta nu. And this is just the\ndetail of what it is.",
    "start": "2982880",
    "end": "2990350"
  },
  {
    "text": "So once again, you\nwould solve a 100 by 100 set of linear equations.",
    "start": "2990350",
    "end": "2997730"
  },
  {
    "text": "OK. If you use block elimination,\nyou end up with this. Oh, hey, wait a minute. That's identical to this.",
    "start": "2997730",
    "end": "3003340"
  },
  {
    "text": "The only thing different is\nthat this is 0 because you are assuming x is feasible. And down here, we put in--",
    "start": "3003340",
    "end": "3009430"
  },
  {
    "text": "we actually are putting\nin the residual. So this need not be 0.",
    "start": "3009430",
    "end": "3015370"
  },
  {
    "text": "If you are not-- if you're not yet feasible,\nthen b minus Ax is not 0.",
    "start": "3015370",
    "end": "3020829"
  },
  {
    "text": "And this is going\nto actually reduce to solving this,\nexactly the same system, but with a different\nright-hand side.",
    "start": "3020830",
    "end": "3028000"
  },
  {
    "text": "That's the difference. So the conclusion\nis that basically,",
    "start": "3028000",
    "end": "3034720"
  },
  {
    "text": "in all three of these methods\nthat look wildly different-- and someone could\ntry to argue they",
    "start": "3034720",
    "end": "3040780"
  },
  {
    "text": "were wildly different, right? It's all the same. By the way, this is the\nsame generally for duality,",
    "start": "3040780",
    "end": "3048370"
  },
  {
    "text": "that if someone says, oh-- you know, especially for\nNewton's method and things",
    "start": "3048370",
    "end": "3054160"
  },
  {
    "text": "like that, they\nsay, well, no, I'm going to use this to solve\nthe dual or something. For methods like this,\nthey're no different if you",
    "start": "3054160",
    "end": "3060190"
  },
  {
    "text": "use smart linear algebra. If you use naive linear\nalgebra, they're very different. There are reasons to solve\na dual instead of a primal,",
    "start": "3060190",
    "end": "3067160"
  },
  {
    "text": "but it's much more advanced. It has to do with if you want a\ndistributed method or something like that.",
    "start": "3067160",
    "end": "3072430"
  },
  {
    "text": "Then it can start\nmaking a difference. But for a lot of these,\nthere's no difference at all.",
    "start": "3072430",
    "end": "3079235"
  },
  {
    "text": "OK.  Let's look at-- this is actually\na real and cool example.",
    "start": "3079235",
    "end": "3087800"
  },
  {
    "text": "So let me just show\nyou what it is. It's, I guess, flow\nutility maximization.",
    "start": "3087800",
    "end": "3096850"
  },
  {
    "text": "So it looks like this. I have a graph. So I have some vertices.",
    "start": "3096850",
    "end": "3105160"
  },
  {
    "text": "And I can-- I'll\nmake it directed.  So here's my graph.",
    "start": "3105160",
    "end": "3111712"
  },
  {
    "text": "I mean, this doesn't\nmatter, right? But I'm just going to draw\na picture of it like this. There you go.",
    "start": "3111712",
    "end": "3118359"
  },
  {
    "text": "OK, there's my graph. And then what I\nimagine is that there's each xi is going to\nrepresent a flow on an edge.",
    "start": "3118360",
    "end": "3125019"
  },
  {
    "text": "So there's a flow here. By the way, x can be negative,\nas those of you in EE",
    "start": "3125020",
    "end": "3132160"
  },
  {
    "text": "would know. This is called the\nreference direction. X negative says simply that the\nflow of whatever it really is,",
    "start": "3132160",
    "end": "3139240"
  },
  {
    "text": "like, for example,\nelectricity or current-- charge, I should say--",
    "start": "3139240",
    "end": "3144982"
  },
  {
    "text": "it means if it's\nnegative, it means it goes in the opposite\ndirection of what's",
    "start": "3144982",
    "end": "3150550"
  },
  {
    "text": "called the reference direction. Everybody got this, right? And this comes up\nin a lot of things.",
    "start": "3150550",
    "end": "3158090"
  },
  {
    "text": "Ax equals b is actually-- Ax equals 0 says that you\nhave flow conservation if A",
    "start": "3158090",
    "end": "3165730"
  },
  {
    "text": "is the incidence matrix. That would be flow conservation. And in fact, you would call--",
    "start": "3165730",
    "end": "3171940"
  },
  {
    "text": "you'd call an x\nwith Ax equals 0, that's called a\ncirculation because it's",
    "start": "3171940",
    "end": "3177309"
  },
  {
    "text": "something where, at each\nnode, you have conservation. And so an x that\nsatisfies that is",
    "start": "3177310",
    "end": "3184359"
  },
  {
    "text": "called a circulation\nbecause-- here's an example of a circulation. I have a flow of 1, 1, 1, 1.",
    "start": "3184360",
    "end": "3190720"
  },
  {
    "text": "It's just-- it's\na flow in a circle here, which would be-- that\nwould be a circulation.",
    "start": "3190720",
    "end": "3196670"
  },
  {
    "text": "When you have b, what\nit corresponds to is that you actually have someone\nis injecting at each node b.",
    "start": "3196670",
    "end": "3206450"
  },
  {
    "text": "And again, this could either be\ninjecting or pulling stuff out. Depends on what it is.",
    "start": "3206450",
    "end": "3213079"
  },
  {
    "text": "So this is what a network\nwould look like for that. Here, I'll put another\nedge here just for fun.",
    "start": "3213080",
    "end": "3219410"
  },
  {
    "text": "So this is my network. And this would have\nlots of examples. Oh, some people call it a\nsingle commodity flow problem.",
    "start": "3219410",
    "end": "3228049"
  },
  {
    "text": "\"Single commodity\" means it's\na single commodity, which is whatever is moving across here.",
    "start": "3228050",
    "end": "3233090"
  },
  {
    "text": "It could be charge in\nan electrical circuit. This could be a\npower grid, where we are not taking into\naccount transmission losses.",
    "start": "3233090",
    "end": "3241369"
  },
  {
    "text": "So if someone is pumping\npower in, that's a generator. If they're pulling it\noff, that is a load.",
    "start": "3241370",
    "end": "3250549"
  },
  {
    "text": "And then this is the power\nflows on all these transmission lines. This is neglecting losses in\nthe transmission line, which",
    "start": "3250550",
    "end": "3259550"
  },
  {
    "text": "is actually not a bad\nthing to-- it's a perfectly good model for some of this. So this is what this is.",
    "start": "3259550",
    "end": "3265240"
  },
  {
    "text": "But this could be\nlots of other stuff. This could be flows of\nmoney in a banking system,",
    "start": "3265240",
    "end": "3271559"
  },
  {
    "text": "could be all sorts of stuff-- flow of mass or something\nlike that, movement of a drug",
    "start": "3271560",
    "end": "3279970"
  },
  {
    "text": "from different\ncompartments in a person. That's another one. But OK.",
    "start": "3279970",
    "end": "3286400"
  },
  {
    "text": "All right. So this is network\nflow optimization. OK. So the matrix A is\nsuper duper sparse",
    "start": "3286400",
    "end": "3295779"
  },
  {
    "text": "because actually, each\ncolumn has at most--",
    "start": "3295780",
    "end": "3300850"
  },
  {
    "text": "it has exactly two non-zeros. It's got a plus 1 and a minus 1.",
    "start": "3300850",
    "end": "3307029"
  },
  {
    "text": "Because each column\ncorresponds to one edge here. And one edge goes\nbetween two vertices,",
    "start": "3307030",
    "end": "3312110"
  },
  {
    "text": "so this is exactly\ntwo non-zeros. So A is--",
    "start": "3312110",
    "end": "3317140"
  },
  {
    "text": "I guess A tilde because we're\ngoing to reduce it in a minute. Yeah, so A tilde\nis going to be--",
    "start": "3317140",
    "end": "3324820"
  },
  {
    "text": "the A tilde is the so-called\nnode incidence matrix. That's not full rank.",
    "start": "3324820",
    "end": "3330430"
  },
  {
    "text": "So you'd remove one row\nand refer to-- by the way, you do that and you refer to\none of the nodes as ground.",
    "start": "3330430",
    "end": "3338050"
  },
  {
    "text": "Now I'm speaking EE dialect,\nbut that's what you do. So OK.",
    "start": "3338050",
    "end": "3344500"
  },
  {
    "text": "Let's see. How do I know that\nthis is not full rank?",
    "start": "3344500",
    "end": "3350750"
  },
  {
    "text": "I mean, how do I know that\nif I write a matrix that looks like this, here--",
    "start": "3350750",
    "end": "3357344"
  },
  {
    "text": " if I make a matrix\nthat looks like that,",
    "start": "3357344",
    "end": "3365340"
  },
  {
    "text": "how do I know it's\nnot full rank? [INAUDIBLE] it becomes\na reduced form.",
    "start": "3365340",
    "end": "3372080"
  },
  {
    "text": "Yeah. So you-- if you just add\nup all the rows, you get 0.",
    "start": "3372080",
    "end": "3377240"
  },
  {
    "text": "So that means 1\ntranspose A is 0.",
    "start": "3377240",
    "end": "3382430"
  },
  {
    "text": "And 1 is not-- the\n1's vector is not 0, so that means it's\nnot full rank. So anyway.",
    "start": "3382430",
    "end": "3389190"
  },
  {
    "text": "I won't go into the details. This is all-- this is just\nvery good stuff to have seen. Actually, how many people\nhave seen stuff like this",
    "start": "3389190",
    "end": "3396215"
  },
  {
    "text": "in some class somewhere? OK, good. What context?",
    "start": "3396215",
    "end": "3402470"
  },
  {
    "text": "Flux balance. Oh, flux balance in biology. OK, good. Good. Yeah.",
    "start": "3402470",
    "end": "3407510"
  },
  {
    "text": "OK, you, too? Yeah. Linear programming class. Yeah, OK. Linear programming, good,\ngood, and flux balance.",
    "start": "3407510",
    "end": "3416150"
  },
  {
    "text": "OK. All right. So let's look at the KKT system\nfor this specific problem.",
    "start": "3416150",
    "end": "3422299"
  },
  {
    "text": "Oh, so you're looking\nat this and you'd say, I don't know, how easy--",
    "start": "3422300",
    "end": "3427790"
  },
  {
    "text": "OK, California grid\nhas about 3,100x-- nah, it's about 6,000\ncome to think of it.",
    "start": "3427790",
    "end": "3433970"
  },
  {
    "text": "Sorry, there's about\n3,100 generators, and there's about maybe 6,000\ntransmission lines, roughly,",
    "start": "3433970",
    "end": "3439670"
  },
  {
    "text": "in California. So the question would be-- that's actually not-- that\nproblem doesn't scare me,",
    "start": "3439670",
    "end": "3446720"
  },
  {
    "text": "because even if you\nwere really dumb and just did it with\ndents and everything, it would be-- it would\nstill be just fine, right?",
    "start": "3446720",
    "end": "3453290"
  },
  {
    "text": "But if I took like-- I don't know. If I took the US\nWestern grid, then it would not be the case anymore.",
    "start": "3453290",
    "end": "3460490"
  },
  {
    "text": "We would start getting\n50,000, 100,000-- we'd be getting some\nbig problems there.",
    "start": "3460490",
    "end": "3467630"
  },
  {
    "text": "But when you see this, you\nshould probably-- lots of signs should be blinking.",
    "start": "3467630",
    "end": "3473720"
  },
  {
    "text": "Number one, your\nobjective is diagonal. So-- sorry, the\nHessian of your--",
    "start": "3473720",
    "end": "3479520"
  },
  {
    "text": "your objective is\nseparable, which means your Hessian is diagonal. So that should just-- the\nfirst time you see this,",
    "start": "3479520",
    "end": "3485740"
  },
  {
    "text": "you should just\nsay, OK, I'm primed. I'm ready to do some\nactual linear algebra here.",
    "start": "3485740",
    "end": "3493589"
  },
  {
    "text": "Then you see Ax equals b,\nand you say, what is that? And you say, oh, that's\nthis node incidence matrix. Every column has exactly\ntwo non-zero entries.",
    "start": "3493590",
    "end": "3501450"
  },
  {
    "text": "Then your sparsity\npart of your brain should be like now\nblinking, right? And so at this point, you\nshould actually depending on,",
    "start": "3501450",
    "end": "3508620"
  },
  {
    "text": "I don't know-- you\ncould say to whoever asked you to solve this\nproblem, you could say, well, we're going\nto be able to solve",
    "start": "3508620",
    "end": "3515010"
  },
  {
    "text": "that shockingly efficiently. And they'd say, yeah, but I\nhave like a million x's and 10",
    "start": "3515010",
    "end": "3520660"
  },
  {
    "text": "million-- I have a million nodes\nin my power network, and I have 10 million edges.",
    "start": "3520660",
    "end": "3526650"
  },
  {
    "text": "Really? And you go, Yeah. They'll say, are you going\nto use a gradient method? And you're like, no, dude,\nwe're going full Newton on this.",
    "start": "3526650",
    "end": "3534740"
  },
  {
    "text": "And they're like\nhow, could you do-- you see what I'm saying, right? There are still people\nstuck in this mindset. Actually, I'd say the vast\nmajority would not know this.",
    "start": "3534740",
    "end": "3542570"
  },
  {
    "text": "So anyway. And you'd say, yeah. They'd say, well,\nhow can you do that? You can't even form the Hessian.",
    "start": "3542570",
    "end": "3547730"
  },
  {
    "text": "And they're right. You'd say, yeah, you're right. I'm not going to\nform the Hessian. I'm going to solve a KKT system.",
    "start": "3547730",
    "end": "3553352"
  },
  {
    "text": "I'm going to exploit\nthe structure. And I'll be able to do it fast. And that's how. OK.",
    "start": "3553352",
    "end": "3558485"
  },
  {
    "text": " So here it is.",
    "start": "3558485",
    "end": "3563580"
  },
  {
    "text": "That's our diagonal\npart of the Hessian. We'll do some elimination here.",
    "start": "3563580",
    "end": "3570210"
  },
  {
    "text": "And you'll end up computing-- forming this matrix here. By the way, this matrix\nis, if you've heard of it,",
    "start": "3570210",
    "end": "3576510"
  },
  {
    "text": "it's a Laplacian matrix. So in circuits, this\nwould be literally the conductance matrix.",
    "start": "3576510",
    "end": "3582450"
  },
  {
    "text": "Again, that's how you\nwould say it in EE. But in other fields,\nit's got other names.",
    "start": "3582450",
    "end": "3587700"
  },
  {
    "text": "So this is a conductance\nmatrix or a Laplacian. \"Laplacian\" means the\ndiagonals are positive,",
    "start": "3587700",
    "end": "3594930"
  },
  {
    "text": "the offdiagonals\nare nonpositive, and the sum of each\nrow is going to be 0.",
    "start": "3594930",
    "end": "3603660"
  },
  {
    "text": "That's if we don't do the\nreduced one, but that's fine. Actually, if we do the\nreduced one, that's fine, too. So everybody got that?",
    "start": "3603660",
    "end": "3610710"
  },
  {
    "text": "So this is-- oh, by the\nway, this thing here, solving so-called Laplacian\nequations like this,",
    "start": "3610710",
    "end": "3618990"
  },
  {
    "text": "that's like a\nwhole subindustry-- mostly of computer--\ntheoretical computer science.",
    "start": "3618990",
    "end": "3624329"
  },
  {
    "text": "So if you type that into Google,\nyou'll find lots of stuff--",
    "start": "3624330",
    "end": "3630120"
  },
  {
    "text": "mostly, to be perfectly bluntly\nhonest about it, completely and utterly useless. There are very good methods\nto do it that are not those.",
    "start": "3630120",
    "end": "3637710"
  },
  {
    "text": "So OK.  OK. So let's see.",
    "start": "3637710",
    "end": "3643029"
  },
  {
    "text": "You can even say this thing,\nthis Laplacian matrix, is actually-- we know the\nsparsity pattern of it",
    "start": "3643030",
    "end": "3649060"
  },
  {
    "text": "immediately, which\nis really cool. It's basically this. ",
    "start": "3649060",
    "end": "3655780"
  },
  {
    "text": "It's basically nodes by nodes. So if I had a million nodes,\nthis would be a million",
    "start": "3655780",
    "end": "3660849"
  },
  {
    "text": "by million matrix. But what it says is\nthat nodes i and j,",
    "start": "3660850",
    "end": "3667390"
  },
  {
    "text": "only if they're\nconnected by an arc would it be true that\nthis matrix here, the Schur complement, is\ngoing to have a non-zero.",
    "start": "3667390",
    "end": "3675820"
  },
  {
    "text": "So if I told you\nmy original power network had 10 million nodes-- a million nodes,\n10 million edges--",
    "start": "3675820",
    "end": "3683800"
  },
  {
    "text": "and suppose I told\nyou that there weren't weird things where there's not\none thing with 50,000 edges",
    "start": "3683800",
    "end": "3690760"
  },
  {
    "text": "adjacent to it. If I said, look, the degree of\na node varies between 2 and 50,",
    "start": "3690760",
    "end": "3700300"
  },
  {
    "text": "Then that tells you that\nthe number of non-zeros in this matrix, per row,\nis between 10 and 50.",
    "start": "3700300",
    "end": "3706960"
  },
  {
    "text": "Everybody following this? And that's going to qualify\nas a sparse matrix, OK?",
    "start": "3706960",
    "end": "3712359"
  },
  {
    "text": "Everybody following this? Right? So what it means is that you can\nsolve network flow optimization",
    "start": "3712360",
    "end": "3721990"
  },
  {
    "text": "problems for, I don't know,\nvery large problems if you want.",
    "start": "3721990",
    "end": "3728630"
  },
  {
    "text": "And if you know how to do it. OK? So right. Did all that make sense?",
    "start": "3728630",
    "end": "3735470"
  },
  {
    "text": "Yeah.  OK. We'll look at something-- this\nis a little bit more esoteric,",
    "start": "3735470",
    "end": "3742369"
  },
  {
    "text": "but just to show\nyou that the ideas-- I mean, you just use these\nideas whenever you can.",
    "start": "3742370",
    "end": "3748210"
  },
  {
    "text": "So on any specific problem,\nyou work out what they are. So here's one where our variable\nis actually a matrix capital",
    "start": "3748210",
    "end": "3754990"
  },
  {
    "text": "X, which is-- it's n by n, my matrix.",
    "start": "3754990",
    "end": "3760840"
  },
  {
    "text": "So one option for this would be\nto string out all the entries",
    "start": "3760840",
    "end": "3766120"
  },
  {
    "text": "of X. X is symmetric. So there would be n n\nplus 1 over 2 entries if I vectorized\nX. Because that's",
    "start": "3766120",
    "end": "3773490"
  },
  {
    "text": "the number of distinct\nelements in a symmetric matrix. So then I could\ncompute the Hessian,",
    "start": "3773490",
    "end": "3780930"
  },
  {
    "text": "which would be n n plus 1 over\n2 by n n plus 1 over 2 matrix.",
    "start": "3780930",
    "end": "3787020"
  },
  {
    "text": "Everybody following this? OK. So that's got n to the\nfourth entries in it, right?",
    "start": "3787020",
    "end": "3793560"
  },
  {
    "text": "Then if I just naively-- oh, then I stack A on top of\nit, I would form a KKT system.",
    "start": "3793560",
    "end": "3803790"
  },
  {
    "text": "Anyway, I'd end up\nwith a giant matrix that was basically n to the\nfourth by n to the fourth.",
    "start": "3803790",
    "end": "3809040"
  },
  {
    "text": "Everybody following this? So you know,\nsomething n equals 200 is going to be a\nkind of a showstopper",
    "start": "3809040",
    "end": "3815400"
  },
  {
    "text": "or something like that, because\nit's just going to be too big. But in theory, it would work.",
    "start": "3815400",
    "end": "3821650"
  },
  {
    "text": "You could actually do this,\nrun this, and then reassemble X into a matrix and all that. So it turns out we'll do\na lot better than that.",
    "start": "3821650",
    "end": "3829733"
  },
  {
    "text": "By the way, each\nNewton step there would cost you n to the sixth. Here's why.",
    "start": "3829733",
    "end": "3835760"
  },
  {
    "text": "The number of variables\nis really n squared. Because I used n\nto say X is n by n.",
    "start": "3835760",
    "end": "3841040"
  },
  {
    "text": "It's n squared over 2 roughly. Then if I form this\nmatrix, my storage is going to be n to the fourth.",
    "start": "3841040",
    "end": "3846800"
  },
  {
    "text": "And the computation per\niteration is going to be-- what would normally be-- it's\nthe size of the matrix cubed. But if that's n squared, that's\ngoing to be n to the sixth.",
    "start": "3846800",
    "end": "3854570"
  },
  {
    "text": "Everybody following this? So OK, that would be bad. By the way, you might ask\nwho uses this kind of stuff.",
    "start": "3854570",
    "end": "3861290"
  },
  {
    "text": "This comes up-- ooh, I could\ngive an interpretation of that in probability, in\nstochastic processes. I could-- in control, things\nlike this come up all the time.",
    "start": "3861290",
    "end": "3869930"
  },
  {
    "text": "But it comes up. So things like this,\npeople actually do solve. OK. Now we'll do it the right way.",
    "start": "3869930",
    "end": "3877099"
  },
  {
    "text": "So the right way is to write\ndown the optimality conditions here for this problem.",
    "start": "3877100",
    "end": "3882410"
  },
  {
    "text": "Oh, and the truth\nis you don't want to calculate the Hessian\nof the determinant",
    "start": "3882410",
    "end": "3889250"
  },
  {
    "text": "of the inverse of a matrix. I mean, I can tell\nyou what it is,",
    "start": "3889250",
    "end": "3894980"
  },
  {
    "text": "but you do not want\nto write it out. Actually, you can.",
    "start": "3894980",
    "end": "3900590"
  },
  {
    "text": "You can write it out as an\nabstract linear operator pretty easily. So let's look how this works.",
    "start": "3900590",
    "end": "3906150"
  },
  {
    "text": "So the conditions are that-- I mean, it's got to\nbe positive definite. Otherwise, this makes no sense.",
    "start": "3906150",
    "end": "3912890"
  },
  {
    "text": "And then the gradient of\nthis, I'm not too afraid of, because that's minus X inverse.",
    "start": "3912890",
    "end": "3918590"
  },
  {
    "text": "So that's the gradient. The Hessian is not\ntoo bad, either. But OK.",
    "start": "3918590",
    "end": "3924950"
  },
  {
    "text": "So I write this out like this. And so these are the equations\nI really want to solve.",
    "start": "3924950",
    "end": "3930450"
  },
  {
    "text": "And in fact, what\nthe Newton method is doing is it's actually\njust linearizing--",
    "start": "3930450",
    "end": "3938225"
  },
  {
    "text": "these are linear. They don't need\nto be linearized. These are not linear,\nbecause of this,",
    "start": "3938225",
    "end": "3943579"
  },
  {
    "text": "and you just linearize that. When you linearize this,\nyou end up with that. That's the derivative of\nthe inverse of a matrix.",
    "start": "3943580",
    "end": "3950539"
  },
  {
    "text": "And it makes sense\nbecause minus 1 over X, If you take the derivative\nof that, is 1 over X squared.",
    "start": "3950540",
    "end": "3957740"
  },
  {
    "text": "That's for scalars. For matrices, it's hardly--",
    "start": "3957740",
    "end": "3963472"
  },
  {
    "text": "then the only question\nis, how do you say X-- 1 over X squared in matrices?",
    "start": "3963472",
    "end": "3968717"
  },
  {
    "text": "And there weren't too many ways,\nbut this is the way to do it. OK? So you get that.",
    "start": "3968717",
    "end": "3973950"
  },
  {
    "text": "So we have to solve\nthese equations here. And remember, we're going to try\nto find this matrix, delta X.",
    "start": "3973950",
    "end": "3982800"
  },
  {
    "text": "And we're going to try to find\nthese dual variables, w, j.",
    "start": "3982800",
    "end": "3987930"
  },
  {
    "text": "OK. So this is the idea. And you have a\nbunch of variables.",
    "start": "3987930",
    "end": "3994830"
  },
  {
    "text": "You have basically\nn squared variables. And all we're doing\nis-- like, this is just block elimination\nbut doing it manually",
    "start": "3994830",
    "end": "4001610"
  },
  {
    "text": "for this particular problem. So what you do is you eliminate\ndelta X from the first equation",
    "start": "4001610",
    "end": "4009920"
  },
  {
    "text": "by multiplying. You go back over here. You multiply this equation by\nX on the left and the right.",
    "start": "4009920",
    "end": "4017720"
  },
  {
    "text": "And you'll get like\nan X and an X-- X and X over here. This will become X.\nAnd that will become--",
    "start": "4017720",
    "end": "4023920"
  },
  {
    "text": "this will become\ndelta X over here. And you end up with\nthis equation here.",
    "start": "4023920",
    "end": "4029859"
  },
  {
    "text": "Then you substitute that\nin the second equation, and you get this system\nof equations here.",
    "start": "4029860",
    "end": "4035440"
  },
  {
    "text": "And that's a dense\npositive definite system of linear equations,\nwhich you then solve. But if you work out the whole\nthing, what's going to happen",
    "start": "4035440",
    "end": "4043359"
  },
  {
    "text": "is you have different-- you\nhave various things here. p is small.",
    "start": "4043360",
    "end": "4048730"
  },
  {
    "text": "But the main thing\nthat's going to cost you is going to be the piece like p\nsquared n squared or p n cubed,",
    "start": "4048730",
    "end": "4056349"
  },
  {
    "text": "this kind of thing. And notice that the entire\nthing is now quadratic in n,",
    "start": "4056350",
    "end": "4063400"
  },
  {
    "text": "whereas the super naive method\nwas the sixth power in n.",
    "start": "4063400",
    "end": "4069039"
  },
  {
    "text": "So that's the same as\ngoing from n cubed to n, going from cubic to linear.",
    "start": "4069040",
    "end": "4076330"
  },
  {
    "text": "And that's what's\nhappened here, once again, except the\nnumber of variables is like n squared over 2.",
    "start": "4076330",
    "end": "4081830"
  },
  {
    "text": "So that's the same thing. It went from n to the\nsixth to n squared. That's a big difference, right?",
    "start": "4081830",
    "end": "4089480"
  },
  {
    "text": "OK. So and-- oh, yes,\nyou didn't know it, but this has been\ndone on your behalf.",
    "start": "4089480",
    "end": "4095059"
  },
  {
    "text": "Any time you ever\nsolved an SDP, this is what was going\non in the solver.",
    "start": "4095060",
    "end": "4101210"
  },
  {
    "text": "You don't need to\nknow it, but that's-- which is kind of the point. OK. So this finishes up the equality\nconstrained minimization.",
    "start": "4101210",
    "end": "4112729"
  },
  {
    "text": "You will do a homework\nproblem on it. Oh, I was going to\nsay it's worthwhile--",
    "start": "4112729",
    "end": "4120605"
  },
  {
    "text": "spend some time on it. But I would hope if\nyou-- at some point, if you've been working on\nit for two, three hours",
    "start": "4120605",
    "end": "4126350"
  },
  {
    "text": "and still debugging, you\nshould have the sense to just step away\nfrom the keyboard,",
    "start": "4126350",
    "end": "4131538"
  },
  {
    "text": "hold your hands up where\nother people can see it, and then just walk away. I just say this because I've\neven had people come and say,",
    "start": "4131538",
    "end": "4140170"
  },
  {
    "text": "I've been working on this for\nhours, we'll look at the code together, and I can't\nfigure out what's wrong.",
    "start": "4140170",
    "end": "4146085"
  },
  {
    "text": "So this is just a little--\nthis is just slightly over the threshold where you're\nnot writing a 10-line script.",
    "start": "4146085",
    "end": "4153411"
  },
  {
    "text": "Actually, in the\nend, it's probably going to be like 15 lines. But it's just entirely possible\nthat, I don't know, something--",
    "start": "4153412",
    "end": "4160359"
  },
  {
    "text": "So I'm just saying,\nby all means try. But somewhere around the\nsecond or third hour,",
    "start": "4160359",
    "end": "4167950"
  },
  {
    "text": "feel free to quit. Just because there's\nno real good reason",
    "start": "4167950",
    "end": "4173770"
  },
  {
    "text": "to sit there\ndebugging something. And what you're looking\nfor is you're looking",
    "start": "4173770",
    "end": "4178778"
  },
  {
    "text": "for that quadratic convergence. When you see an error\ncurve that goes like this and then accelerates,\nthen you're done.",
    "start": "4178779",
    "end": "4186818"
  },
  {
    "text": "Maybe. You're already seeing\nthat, but there's some other suspicious things\nin your implementation, right? So yeah.",
    "start": "4186819",
    "end": "4192910"
  },
  {
    "text": "OK, everybody cool? OK. Now we're going to go\non to the last section.",
    "start": "4192910",
    "end": "4200330"
  },
  {
    "text": "So actually, we're\nbuilding a hierarchy. And we went from\nthe very low level.",
    "start": "4200330",
    "end": "4205340"
  },
  {
    "text": "Low level is linear algebra. Interestingly,\nlinear algebra is how",
    "start": "4205340",
    "end": "4210770"
  },
  {
    "text": "you solve quadratic\noptimization-- sorry, equality constrained quadratic\nminimization problems.",
    "start": "4210770",
    "end": "4217070"
  },
  {
    "text": "So linear algebra, you could\njust call it minimizing-- it's effective\nmethods for solving--",
    "start": "4217070",
    "end": "4225320"
  },
  {
    "text": "for minimizing quadratic--\nconvex quadratic subject to linear equality constraints.",
    "start": "4225320",
    "end": "4231330"
  },
  {
    "text": "Then you say, well, now I\nknow-- now I'm showing you a method for minimizing smooth\nfunctions subject to equality",
    "start": "4231330",
    "end": "4236560"
  },
  {
    "text": "constraints. Someone says, how\ndo you do that? You go, I approximate the\nsmooth function as a quadratic in 22 steps.",
    "start": "4236560",
    "end": "4243260"
  },
  {
    "text": "That's what I do. And they go, well,\nthen what happens? And you go, well,\neach step is basically minimizing a quadratic\nfunction that's",
    "start": "4243260",
    "end": "4248690"
  },
  {
    "text": "under equality constraint,\nwhich is linear algebra. Everybody following this? OK, so-- all right.",
    "start": "4248690",
    "end": "4254690"
  },
  {
    "text": "Next in the hierarchy is we're\ngoing to handle inequality constraints.",
    "start": "4254690",
    "end": "4259870"
  },
  {
    "text": "So that's the next one. It's not as straightforward\nas equality constraints, but we're going to add\ninequality constraints",
    "start": "4259870",
    "end": "4266829"
  },
  {
    "text": "and show how to\nreduce them to solving a sequence of smooth\nequality constraint",
    "start": "4266830",
    "end": "4273940"
  },
  {
    "text": "minimization problems. That's the overall program.",
    "start": "4273940",
    "end": "4279620"
  },
  {
    "text": "OK.  So first, let's talk\nabout what we're",
    "start": "4279620",
    "end": "4284740"
  },
  {
    "text": "going to be looking at here. So we're going to\nminimize-- we're going to solve a problem\nwhere all of the functions",
    "start": "4284740",
    "end": "4291190"
  },
  {
    "text": "here are smooth. Now, by the way, that's\nalready done for you",
    "start": "4291190",
    "end": "4296380"
  },
  {
    "text": "by something like CVX pi\nor something like that because it's compiled to a\nproblem that looks like that.",
    "start": "4296380",
    "end": "4302440"
  },
  {
    "text": "So for example, if I said,\nminimize the L infinity",
    "start": "4302440",
    "end": "4307510"
  },
  {
    "text": "norm of Ax minus b or\nsomething like that, that does not satisfy--",
    "start": "4307510",
    "end": "4312760"
  },
  {
    "text": "that doesn't work here. Oh, subject to x non-negative. The x non-negative works,\nthe objective does not.",
    "start": "4312760",
    "end": "4319239"
  },
  {
    "text": "So you would transform\nthat to an LP. And an LP satisfies\nthis, for sure,",
    "start": "4319240",
    "end": "4324280"
  },
  {
    "text": "because all the fi's,\nthey're all affine. And affine is, well,\nreally very smooth.",
    "start": "4324280",
    "end": "4330940"
  },
  {
    "text": "So OK. So this is the idea. Now, we'll assume that the\nproblem is strictly feasible.",
    "start": "4330940",
    "end": "4338210"
  },
  {
    "text": "And so that's fine. And that means that\nstrong duality holds, and there's a dual optimum\nand all that kind of stuff.",
    "start": "4338210",
    "end": "4345710"
  },
  {
    "text": "OK. So examples would be things\nlike LP, QP, QCQP, GP is one.",
    "start": "4345710",
    "end": "4351290"
  },
  {
    "text": "Because GP, all the fi's are\nlog sum exp of an affine thing.",
    "start": "4351290",
    "end": "4356510"
  },
  {
    "text": "That's what a GP is. OK, here's one, is\nentropy maximization with",
    "start": "4356510",
    "end": "4361700"
  },
  {
    "text": "linear inequality constraints. So I mean, here\nwould be a fun one. x represents a\nprobability distribution",
    "start": "4361700",
    "end": "4368990"
  },
  {
    "text": "on some finite number of things. This basically--\nthis says minimize-- this is negative entropy.",
    "start": "4368990",
    "end": "4374510"
  },
  {
    "text": "This says, maximize the entropy\nover all distributions that satisfy linear\nequality constraints",
    "start": "4374510",
    "end": "4381373"
  },
  {
    "text": "and linear inequality\nconstraints. Now, a linear constraint\non a probability vector",
    "start": "4381373",
    "end": "4386720"
  },
  {
    "text": "is actually precisely the same\nas just an expectation of any of an event, like period.",
    "start": "4386720",
    "end": "4393290"
  },
  {
    "text": "It's the expectation of--\nsorry, a function on the event space, which is these n things.",
    "start": "4393290",
    "end": "4399907"
  },
  {
    "text": "So this would be\nsomething like this. You say, well, I have a\ndiscrete random variable,",
    "start": "4399908",
    "end": "4407710"
  },
  {
    "text": "and I have a whole bunch of-- I can tell you about\nthe probability that it's in this set,\nprobability in that set.",
    "start": "4407710",
    "end": "4414550"
  },
  {
    "text": "I can give you ranges on those. The probability that it's\nbetween here and here is-- it's between 0.1 and 0.2,\nor something like that right.",
    "start": "4414550",
    "end": "4424390"
  },
  {
    "text": "Anyway, then this says,\nfind me the maximum entropy distribution that satisfies\nthose constraints.",
    "start": "4424390",
    "end": "4430930"
  },
  {
    "text": "So OK. All right. OK.",
    "start": "4430930",
    "end": "4436420"
  },
  {
    "text": "So I think I already\nmentioned this sort of stuff. We'll get to things\nlike SDPs and SOCPs.",
    "start": "4436420",
    "end": "4441640"
  },
  {
    "text": "They're actually better\nhandled as problems with generalized inequalities. So we'll handle that later.",
    "start": "4441640",
    "end": "4448239"
  },
  {
    "text": "OK. So everything is going to\nhinge around this thing called the log barrier.",
    "start": "4448240",
    "end": "4453340"
  },
  {
    "text": "You've seen it before. And we'll talk more\nabout this on Thursday.",
    "start": "4453340",
    "end": "4460630"
  },
  {
    "text": "But the story is actually\nkind of interesting. And it goes like this, right?",
    "start": "4460630",
    "end": "4467920"
  },
  {
    "text": "You say, I have\nthese constraints and I don't know how\nto handle constraints.",
    "start": "4467920",
    "end": "4473260"
  },
  {
    "text": "And a constraint\nlooks like this. If this is fi of x, here's 0.",
    "start": "4473260",
    "end": "4479290"
  },
  {
    "text": "And basically, there's a\nfunction that looks like this. And this says, if you're\nless than or equal to 0,",
    "start": "4479290",
    "end": "4486789"
  },
  {
    "text": "I will assess you a charge of 0. If you're positive,\nI will assess a charge of plus infinity.",
    "start": "4486790",
    "end": "4492790"
  },
  {
    "text": "So this is the semantics\nof a constraint. This is actually why people\ncall it a hard constraint.",
    "start": "4492790",
    "end": "4499000"
  },
  {
    "text": "What we're going to\ndo here is actually-- it's this exact\nsame story as dual--",
    "start": "4499000",
    "end": "4505150"
  },
  {
    "text": "Lagrangian dual--\nLagrangian duality, I don't know if you remember,\nbut the story started this way. And then someone said, I'm going\nto approximate this function.",
    "start": "4505150",
    "end": "4513679"
  },
  {
    "text": "And you go, yeah,\ngo ahead, please. And they say, yeah,\nhere's my approximation.",
    "start": "4513680",
    "end": "4519420"
  },
  {
    "text": "I mean, this is the beginning\nof the story of Lagrangian duality, to which a\nnormal person would say,",
    "start": "4519420",
    "end": "4527489"
  },
  {
    "text": "this is not a particularly\ngood approximation of this. I mean, no one could\nsay that, right?",
    "start": "4527490",
    "end": "4534420"
  },
  {
    "text": "Now, that story had\na happy ending right because it went on and\non, and it turned out for convex things,\nit turns out if you",
    "start": "4534420",
    "end": "4540510"
  },
  {
    "text": "get these slopes\njust right, then it actually is a\ngood approximation. Actually, it's an infinitely\ngood approximation.",
    "start": "4540510",
    "end": "4547600"
  },
  {
    "text": "Everybody-- and then there\nwas a lot of other stuff, like these were the prices\nand all that kind of stuff. That's fine.",
    "start": "4547600",
    "end": "4552660"
  },
  {
    "text": "OK. This story, actually,\nis way better.",
    "start": "4552660",
    "end": "4558090"
  },
  {
    "text": "What you're going to--\nthis story starts this way. And we'll continue the\ndiscussion on Friday. This story starts this way.",
    "start": "4558090",
    "end": "4563175"
  },
  {
    "text": "Someone says, well, I'd like\nto apply Newton's method, but I have this\nfunction that's 0",
    "start": "4563175",
    "end": "4568800"
  },
  {
    "text": "and then goes up\nto plus infinity. So it's not continuous, it's\nnot differentiable, it's not-- and so there's a lot\nof things you could do.",
    "start": "4568800",
    "end": "4575640"
  },
  {
    "text": "One thing you can actually do\nis just apply Newton's method to this. How well would that work? ",
    "start": "4575640",
    "end": "4583701"
  },
  {
    "text": "Not well. Not well, right? In fact, it wouldn't\nwork at all, right? And someone would say, well,\nit didn't work because you",
    "start": "4583701",
    "end": "4590322"
  },
  {
    "text": "need a smooth function. So you'd say, fine, I'll\njust make a smooth function.",
    "start": "4590322",
    "end": "4596070"
  },
  {
    "text": "I'll make a function that\nlooks like this, roughly.",
    "start": "4596070",
    "end": "4601780"
  },
  {
    "text": "It's a barrier function. That's a thing, right? It's smooth, it goes\nto infinity, and so on.",
    "start": "4601780",
    "end": "4608630"
  },
  {
    "text": "I can apply Newton's\nmethod to that. And so that's actually how\nwe're going to start here.",
    "start": "4608630",
    "end": "4614540"
  },
  {
    "text": "So we'll quit. I think we'll quit for today,\nand then we'll continue this. Good luck on implementing\nyour Newton solver.",
    "start": "4614540",
    "end": "4622770"
  },
  {
    "start": "4622770",
    "end": "4627000"
  }
]