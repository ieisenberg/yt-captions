[
  {
    "start": "0",
    "end": "197000"
  },
  {
    "start": "0",
    "end": "920"
  },
  {
    "text": "OK.",
    "start": "920",
    "end": "1730"
  },
  {
    "text": "So that was with\nsingle variable.",
    "start": "1730",
    "end": "5240"
  },
  {
    "text": "And usually when we\ndo classification,",
    "start": "5240",
    "end": "7100"
  },
  {
    "text": "we got more than one variable.",
    "start": "7100",
    "end": "8750"
  },
  {
    "text": "So now we're going to go\nto multivariate Gaussians.",
    "start": "8750",
    "end": "13219"
  },
  {
    "text": "So here's a picture\nof a Gaussian density,",
    "start": "13220",
    "end": "16129"
  },
  {
    "text": "when we've got two variables.",
    "start": "16129",
    "end": "17869"
  },
  {
    "text": "So it's one of these fancy\nlooking three-dimensional plots.",
    "start": "17870",
    "end": "22665"
  },
  {
    "text": "It's beautiful.",
    "start": "22665",
    "end": "23290"
  },
  {
    "text": "Beautiful, isn't it?",
    "start": "23290",
    "end": "26360"
  },
  {
    "text": "The height's color coded.",
    "start": "26360",
    "end": "29060"
  },
  {
    "text": "Could you make a\nplot like this, Rob?",
    "start": "29060",
    "end": "31880"
  },
  {
    "text": "I could try, and then\nyou'd criticize it.",
    "start": "31880",
    "end": "34686"
  },
  {
    "text": "[LAUGHS]",
    "start": "34686",
    "end": "35540"
  },
  {
    "text": "You do criticize it actually.",
    "start": "35540",
    "end": "38740"
  },
  {
    "text": "The beautiful thing is\nthat R made this plot,",
    "start": "38740",
    "end": "41060"
  },
  {
    "text": "with very little work from us.",
    "start": "41060",
    "end": "44270"
  },
  {
    "text": "So there's two\nvariables x1 and x2.",
    "start": "44270",
    "end": "47340"
  },
  {
    "text": "And you can see the\nGaussian density",
    "start": "47340",
    "end": "49040"
  },
  {
    "text": "in two dimensions looks\nlike a bell function.",
    "start": "49040",
    "end": "52670"
  },
  {
    "text": "And the left-hand\ncase is the case",
    "start": "52670",
    "end": "55579"
  },
  {
    "text": "when the two variables\nare uncorrelated,",
    "start": "55580",
    "end": "58110"
  },
  {
    "text": "so it's just really like a bell.",
    "start": "58110",
    "end": "60470"
  },
  {
    "text": "And the right-hand case is\nwhen there's correlation,",
    "start": "60470",
    "end": "63030"
  },
  {
    "text": "so now it's like\na stretched bell.",
    "start": "63030",
    "end": "65119"
  },
  {
    "text": "You can see this positive\ncorrelation between x1 and x2.",
    "start": "65120",
    "end": "69190"
  },
  {
    "text": "So those are pictures\nof the densities.",
    "start": "69190",
    "end": "71650"
  },
  {
    "text": "The formula for the density\ndoesn't look nearly as pretty.",
    "start": "71650",
    "end": "74270"
  },
  {
    "text": "And here it is over here.",
    "start": "74270",
    "end": "76299"
  },
  {
    "text": "And it's somewhat complex.",
    "start": "76300",
    "end": "79130"
  },
  {
    "text": "We should go over this\nin great detail now,",
    "start": "79130",
    "end": "81939"
  },
  {
    "text": "excruciating detail please.",
    "start": "81940",
    "end": "85270"
  },
  {
    "text": "I have a feeling\nthat's a hint not",
    "start": "85270",
    "end": "87039"
  },
  {
    "text": "to go into it in great detail.",
    "start": "87040",
    "end": "89380"
  },
  {
    "text": "Anyway, this is just a\ngeneralization of the formula",
    "start": "89380",
    "end": "92210"
  },
  {
    "text": "that we had for a\nsingle variable.",
    "start": "92210",
    "end": "93710"
  },
  {
    "text": "This is called the\ncovariance matrix.",
    "start": "93710",
    "end": "95799"
  },
  {
    "text": "And if you stare\nat this formula,",
    "start": "95800",
    "end": "98507"
  },
  {
    "text": "and you stare at the\nprevious formula,",
    "start": "98507",
    "end": "100090"
  },
  {
    "text": "you'll see that there's somewhat\nsimilarities, especially",
    "start": "100090",
    "end": "102465"
  },
  {
    "text": "if a bit of vector algebra.",
    "start": "102465",
    "end": "104655"
  },
  {
    "start": "104655",
    "end": "107250"
  },
  {
    "text": "If you go through\nthe simplifications,",
    "start": "107250",
    "end": "110220"
  },
  {
    "text": "if you know your\nlinear algebra, you",
    "start": "110220",
    "end": "113430"
  },
  {
    "text": "can form it, and\ndo the cancellation",
    "start": "113430",
    "end": "115380"
  },
  {
    "text": "similar to what we did before.",
    "start": "115380",
    "end": "117399"
  },
  {
    "text": "You can form the\ndiscriminant function,",
    "start": "117400",
    "end": "119770"
  },
  {
    "text": "which is given over here.",
    "start": "119770",
    "end": "122979"
  },
  {
    "text": "And it looks complex.",
    "start": "122980",
    "end": "125380"
  },
  {
    "text": "But important thing to\nnote is that it's again",
    "start": "125380",
    "end": "127979"
  },
  {
    "text": "linear in x, yes,\nx alone multiplied",
    "start": "127980",
    "end": "131460"
  },
  {
    "text": "by a coefficient vector.",
    "start": "131460",
    "end": "133620"
  },
  {
    "text": "And these are all\njust constants,",
    "start": "133620",
    "end": "136060"
  },
  {
    "text": "so this, again, is\na linear function.",
    "start": "136060",
    "end": "139470"
  },
  {
    "text": "If the maths is beyond you,\ndon't let it bother you,",
    "start": "139470",
    "end": "142827"
  },
  {
    "text": "this is a linear function.",
    "start": "142827",
    "end": "143909"
  },
  {
    "text": "And we'll make that\nmore clear a little",
    "start": "143910",
    "end": "145440"
  },
  {
    "text": "bit later on as well, that\nthis is a linear function.",
    "start": "145440",
    "end": "147648"
  },
  {
    "start": "147648",
    "end": "150230"
  },
  {
    "text": "In fact, we make it clear right\nhere, it might look complex,",
    "start": "150230",
    "end": "153739"
  },
  {
    "text": "but this can just be written\nin this form over here,",
    "start": "153740",
    "end": "158120"
  },
  {
    "text": "it's a linear function.",
    "start": "158120",
    "end": "159390"
  },
  {
    "text": "So the Ck0, so this is\na function for class k,",
    "start": "159390",
    "end": "163670"
  },
  {
    "text": "Ck0 is built up of all\nthese pieces over here.",
    "start": "163670",
    "end": "168599"
  },
  {
    "text": "And then each of the\ncoefficients of x1, x2, to xp,",
    "start": "168600",
    "end": "172730"
  },
  {
    "text": "come from this part over here.",
    "start": "172730",
    "end": "174950"
  },
  {
    "text": "Remember, x is a\nvector in this case,",
    "start": "174950",
    "end": "177530"
  },
  {
    "text": "and so you can expand\nthis expression",
    "start": "177530",
    "end": "179810"
  },
  {
    "text": "and get this term over here.",
    "start": "179810",
    "end": "182599"
  },
  {
    "text": "And I think I forgot\nto mention it before,",
    "start": "182600",
    "end": "185452"
  },
  {
    "text": "but the idea of the\ndiscriminant function",
    "start": "185452",
    "end": "187159"
  },
  {
    "text": "is, you compute one of these\nfor each of the classes,",
    "start": "187160",
    "end": "189980"
  },
  {
    "text": "and then you classify it to the\nclass for which it's largest,",
    "start": "189980",
    "end": "193340"
  },
  {
    "text": "you pick the discriminant\nfunction that's largest.",
    "start": "193340",
    "end": "195504"
  },
  {
    "start": "195505",
    "end": "198900"
  },
  {
    "start": "197000",
    "end": "327000"
  },
  {
    "text": "Well, we can draw\nother nice pictures",
    "start": "198900",
    "end": "200760"
  },
  {
    "text": "for discriminant analysis,\nsimilar to the one-dimensional",
    "start": "200760",
    "end": "205019"
  },
  {
    "text": "picture we drew before.",
    "start": "205020",
    "end": "206760"
  },
  {
    "text": "So here we've got two\nvariables and three classes.",
    "start": "206760",
    "end": "211140"
  },
  {
    "text": "And instead of showing those\ndensity plots, what we do",
    "start": "211140",
    "end": "216210"
  },
  {
    "text": "is show contours in this\ncase of the density.",
    "start": "216210",
    "end": "219820"
  },
  {
    "text": "So here's the blue class,\nand we showing the contour",
    "start": "219820",
    "end": "222780"
  },
  {
    "text": "of a particular level of\nprobability for the blue class,",
    "start": "222780",
    "end": "227940"
  },
  {
    "text": "for the green class, and\nfor the orange class.",
    "start": "227940",
    "end": "231600"
  },
  {
    "text": "And lo and behold, and\nthe decision boundary",
    "start": "231600",
    "end": "236280"
  },
  {
    "text": "is the dotted line here.",
    "start": "236280",
    "end": "239010"
  },
  {
    "text": "And it's really very\npretty, it shows you",
    "start": "239010",
    "end": "243480"
  },
  {
    "text": "where you classify to\nblue versus orange,",
    "start": "243480",
    "end": "246569"
  },
  {
    "text": "and it's exactly where\nthe line goes exactly",
    "start": "246570",
    "end": "250530"
  },
  {
    "text": "through the points where\nthe contours are cut.",
    "start": "250530",
    "end": "253470"
  },
  {
    "text": "Both for the orange to\nblue, for the blue to green,",
    "start": "253470",
    "end": "257640"
  },
  {
    "text": "and for the green to orange.",
    "start": "257640",
    "end": "259890"
  },
  {
    "text": "And you can see they all\nmeet in the middle over here,",
    "start": "259890",
    "end": "263190"
  },
  {
    "text": "right in the center over here.",
    "start": "263190",
    "end": "265750"
  },
  {
    "text": "And so if you knew\nthe true densities,",
    "start": "265750",
    "end": "268960"
  },
  {
    "text": "it would tell you exactly, and\nin this case, the Gaussian,",
    "start": "268960",
    "end": "272620"
  },
  {
    "text": "you get the exact\ndecision boundaries.",
    "start": "272620",
    "end": "275360"
  },
  {
    "text": "Again, these are called the\nBayes decision boundaries,",
    "start": "275360",
    "end": "277780"
  },
  {
    "text": "these are the true\ndecision boundaries.",
    "start": "277780",
    "end": "280150"
  },
  {
    "text": "Now, of course, we\ndon't, but we go ahead",
    "start": "280150",
    "end": "283419"
  },
  {
    "text": "and estimate the parameters\nfor the Gaussians",
    "start": "283420",
    "end": "286060"
  },
  {
    "text": "in each of the\nclass, using formulas",
    "start": "286060",
    "end": "288160"
  },
  {
    "text": "similar to what we had\nbefore, but appropriate",
    "start": "288160",
    "end": "290170"
  },
  {
    "text": "for this multivariate case.",
    "start": "290170",
    "end": "291770"
  },
  {
    "text": "So in this case, we to\nget the mean for x1 and x2",
    "start": "291770",
    "end": "296080"
  },
  {
    "text": "for the blue class.",
    "start": "296080",
    "end": "297729"
  },
  {
    "text": "It's about there\nfor the green class,",
    "start": "297730",
    "end": "299680"
  },
  {
    "text": "with data points about there,\nfor the orange class, say there.",
    "start": "299680",
    "end": "303820"
  },
  {
    "text": "And then we plug them\ninto the formula.",
    "start": "303820",
    "end": "306620"
  },
  {
    "text": "Instead of getting the dotted\nlines, we get the solid lines.",
    "start": "306620",
    "end": "311750"
  },
  {
    "text": "And in this case,\nit's remarkably close.",
    "start": "311750",
    "end": "314650"
  },
  {
    "text": "Now these data were actually\ngenerated from a Gaussian",
    "start": "314650",
    "end": "317290"
  },
  {
    "text": "distribution, so it's not too\nsurprising that we got close.",
    "start": "317290",
    "end": "320450"
  },
  {
    "text": "But with relatively\nfew points, we",
    "start": "320450",
    "end": "322240"
  },
  {
    "text": "get decision\nboundaries that look",
    "start": "322240",
    "end": "323830"
  },
  {
    "text": "pretty close to the real ones.",
    "start": "323830",
    "end": "326960"
  },
  {
    "text": "You cannot learn about\ndiscriminant analysis without",
    "start": "326960",
    "end": "329630"
  },
  {
    "start": "327000",
    "end": "430000"
  },
  {
    "text": "seeing Fisher's iris data, it's\nmaybe of the most famous data",
    "start": "329630",
    "end": "333380"
  },
  {
    "text": "sets around.",
    "start": "333380",
    "end": "336380"
  },
  {
    "text": "It studies three\nspecies of Iris.",
    "start": "336380",
    "end": "340070"
  },
  {
    "text": "These species are setosa,\nversicolor, and virginica.",
    "start": "340070",
    "end": "344570"
  },
  {
    "text": "And they color-coded in\nthis scatter plot matrix,",
    "start": "344570",
    "end": "347660"
  },
  {
    "text": "again with, three\ncolors in this case.",
    "start": "347660",
    "end": "350210"
  },
  {
    "text": "And there are four\nvariables that",
    "start": "350210",
    "end": "352264"
  },
  {
    "text": "are going to be used to\ntry and automatically",
    "start": "352265",
    "end": "354140"
  },
  {
    "text": "classify to these three classes,\nsepal length, sepal width,",
    "start": "354140",
    "end": "361410"
  },
  {
    "text": "petal length and petal width.",
    "start": "361410",
    "end": "363380"
  },
  {
    "text": "So these are aspects\nof the flower.",
    "start": "363380",
    "end": "367480"
  },
  {
    "text": "And there's 50\nsamples in each class.",
    "start": "367480",
    "end": "371743"
  },
  {
    "text": "Now, if you look at\nthese scatter plots,",
    "start": "371743",
    "end": "373410"
  },
  {
    "text": "you can see that there's\nsome really good separation.",
    "start": "373410",
    "end": "376450"
  },
  {
    "text": "For example, in this plot of\npetal length and petal width,",
    "start": "376450",
    "end": "379050"
  },
  {
    "text": "the blue class really\nstands out as being",
    "start": "379050",
    "end": "381810"
  },
  {
    "text": "different from the\nother two classes.",
    "start": "381810",
    "end": "384450"
  },
  {
    "text": "They seem to be more confused\nin some of the plots.",
    "start": "384450",
    "end": "387790"
  },
  {
    "text": "And in some, there's\nslightly more separation.",
    "start": "387790",
    "end": "392230"
  },
  {
    "text": "But the idea here is to use\nall these variables together",
    "start": "392230",
    "end": "395190"
  },
  {
    "text": "to come up with a discriminant\nrule and classify.",
    "start": "395190",
    "end": "400350"
  },
  {
    "text": "And so this was a\nmotivating example",
    "start": "400350",
    "end": "402540"
  },
  {
    "text": "that Fisher used in\nhis first description",
    "start": "402540",
    "end": "406620"
  },
  {
    "text": "of linear discriminant analysis.",
    "start": "406620",
    "end": "408850"
  },
  {
    "text": "And in fact, it's often known\nas Fisher's linear discriminant",
    "start": "408850",
    "end": "412440"
  },
  {
    "text": "analysis.",
    "start": "412440",
    "end": "415050"
  },
  {
    "text": "Importantly, what comes\nwith discriminant analysis",
    "start": "415050",
    "end": "417900"
  },
  {
    "text": "is a nice plot.",
    "start": "417900",
    "end": "420410"
  },
  {
    "text": "In this previous picture,\nthere's four variables.",
    "start": "420410",
    "end": "423730"
  },
  {
    "text": "So we showed a scatter plot\nmatrix of each variable",
    "start": "423730",
    "end": "426150"
  },
  {
    "text": "against the rest.",
    "start": "426150",
    "end": "427419"
  },
  {
    "text": "But it turns out that\nthere's a single plot that",
    "start": "427420",
    "end": "430410"
  },
  {
    "start": "430000",
    "end": "660000"
  },
  {
    "text": "captures the classification\ninformation for all",
    "start": "430410",
    "end": "433590"
  },
  {
    "text": "of these variables.",
    "start": "433590",
    "end": "434970"
  },
  {
    "text": "And here it is.",
    "start": "434970",
    "end": "437040"
  },
  {
    "text": "And I've got\ndiscriminant variable 1",
    "start": "437040",
    "end": "440100"
  },
  {
    "text": "and discriminant variable\n2 as the horizontal",
    "start": "440100",
    "end": "442500"
  },
  {
    "text": "and vertical axes.",
    "start": "442500",
    "end": "444420"
  },
  {
    "text": "And it turns out these\nare linear combinations",
    "start": "444420",
    "end": "446910"
  },
  {
    "text": "of the original variables.",
    "start": "446910",
    "end": "449680"
  },
  {
    "text": "But they specialized\nin new combinations.",
    "start": "449680",
    "end": "452470"
  },
  {
    "text": "And when you plot the\nvariables against these two,",
    "start": "452470",
    "end": "454990"
  },
  {
    "text": "you see really good separation.",
    "start": "454990",
    "end": "458199"
  },
  {
    "text": "And these arise from\nactually performing",
    "start": "458200",
    "end": "460720"
  },
  {
    "text": "the linear\ndiscriminant analysis.",
    "start": "460720",
    "end": "463660"
  },
  {
    "text": "Because you got\nthree classes, what",
    "start": "463660",
    "end": "465970"
  },
  {
    "text": "Fisher's linear discriminant\nanalysis is really doing,",
    "start": "465970",
    "end": "468850"
  },
  {
    "text": "or Gaussian LDA is really doing,\nis it's measuring which centroid",
    "start": "468850",
    "end": "474070"
  },
  {
    "text": "is the closest.",
    "start": "474070",
    "end": "477150"
  },
  {
    "text": "But it's measuring\nit in a distance,",
    "start": "477150",
    "end": "479520"
  },
  {
    "text": "where it takes into account the\ncovariance of the variables.",
    "start": "479520",
    "end": "484470"
  },
  {
    "text": "But ignoring the\ncovariance for a moment.",
    "start": "484470",
    "end": "486560"
  },
  {
    "start": "486560",
    "end": "489820"
  },
  {
    "text": "The three centroids actually\nlie in a plane, a subspace",
    "start": "489820",
    "end": "495550"
  },
  {
    "text": "of the four-dimensional space.",
    "start": "495550",
    "end": "497889"
  },
  {
    "text": "And it's really distance,\nso if you have three points,",
    "start": "497890",
    "end": "500620"
  },
  {
    "text": "they span a\ntwo-dimensional subspace.",
    "start": "500620",
    "end": "503440"
  },
  {
    "text": "And that's essentially\nwhat we plotting here,",
    "start": "503440",
    "end": "505360"
  },
  {
    "text": "is the two-dimensional subspace.",
    "start": "505360",
    "end": "507189"
  },
  {
    "text": "So seeing which class\nis closest really",
    "start": "507190",
    "end": "510310"
  },
  {
    "text": "amounts to distance\nin that subspace.",
    "start": "510310",
    "end": "513200"
  },
  {
    "text": "And that leads to these\nnice, low-dimensional plots.",
    "start": "513200",
    "end": "516559"
  },
  {
    "text": "And so we have three\nclasses here, we",
    "start": "516559",
    "end": "518775"
  },
  {
    "text": "can make a two-dimensional\nplot, and it",
    "start": "518775",
    "end": "520400"
  },
  {
    "text": "captures exactly what's\nimportant in terms",
    "start": "520400",
    "end": "524360"
  },
  {
    "text": "of the classification.",
    "start": "524360",
    "end": "526519"
  },
  {
    "text": "And when we have more\nthan three classes,",
    "start": "526520",
    "end": "528650"
  },
  {
    "text": "we can still find two\ndimensional plots.",
    "start": "528650",
    "end": "531510"
  },
  {
    "text": "But in that case, it doesn't\ncapture all the information,",
    "start": "531510",
    "end": "533990"
  },
  {
    "text": "the two-dimensional\nplot, but we can",
    "start": "533990",
    "end": "535490"
  },
  {
    "text": "find the best\ntwo-dimensional plot",
    "start": "535490",
    "end": "536959"
  },
  {
    "text": "for visualizing the\ndiscriminant rule.",
    "start": "536960",
    "end": "539630"
  },
  {
    "text": "And that's another\nimportant reason",
    "start": "539630",
    "end": "541520"
  },
  {
    "text": "why linear discriminant\nanalysis is",
    "start": "541520",
    "end": "543590"
  },
  {
    "text": "very popular for\nmulti-class classification.",
    "start": "543590",
    "end": "546487"
  },
  {
    "text": "Keep in mind this case,\nwe had only four features,",
    "start": "546487",
    "end": "548570"
  },
  {
    "text": "four variables.",
    "start": "548570",
    "end": "549360"
  },
  {
    "text": "And this analysis was a\nvery attractive method.",
    "start": "549360",
    "end": "551880"
  },
  {
    "text": "But imagine we had\n4,000 features,",
    "start": "551880",
    "end": "554570"
  },
  {
    "text": "then an ingredient in what we\njust did was the covariance",
    "start": "554570",
    "end": "557528"
  },
  {
    "text": "matrix, we had to plug in an\nestimate of the covariance",
    "start": "557528",
    "end": "559820"
  },
  {
    "text": "matrix.",
    "start": "559820",
    "end": "560880"
  },
  {
    "text": "If we had 4,000 features, that\ncovariance matrix would be",
    "start": "560880",
    "end": "563270"
  },
  {
    "text": "of size 4,000 by 4,000.",
    "start": "563270",
    "end": "565760"
  },
  {
    "text": "That's a great point, Rob.",
    "start": "565760",
    "end": "567410"
  },
  {
    "text": "And we just can't carry\nout discriminant analysis",
    "start": "567410",
    "end": "572720"
  },
  {
    "text": "without other modifications\nif the number of variables",
    "start": "572720",
    "end": "575660"
  },
  {
    "text": "are very large.",
    "start": "575660",
    "end": "576329"
  },
  {
    "text": "And we're going to talk about\nsome ways of doing that.",
    "start": "576330",
    "end": "578580"
  },
  {
    "text": "And later on in the class,\nwe'll talk about even more ways.",
    "start": "578580",
    "end": "581080"
  },
  {
    "start": "581080",
    "end": "585200"
  },
  {
    "text": "We've talked about discriminant\nfunctions, which is telling you",
    "start": "585200",
    "end": "588200"
  },
  {
    "text": "how you classify.",
    "start": "588200",
    "end": "589460"
  },
  {
    "text": "But it turns out that you can\nturn these into probabilities",
    "start": "589460",
    "end": "592130"
  },
  {
    "text": "very easily.",
    "start": "592130",
    "end": "593360"
  },
  {
    "text": "And here's the\nexpression over here.",
    "start": "593360",
    "end": "596720"
  },
  {
    "text": "And so remember, we\ngot the dk's by doing",
    "start": "596720",
    "end": "599720"
  },
  {
    "text": "a lot of simplification.",
    "start": "599720",
    "end": "601199"
  },
  {
    "text": "Well, it turns out that\nthose simplifications",
    "start": "601200",
    "end": "603680"
  },
  {
    "text": "and cancellations hold exactly\nfor computing the probabilities.",
    "start": "603680",
    "end": "607970"
  },
  {
    "text": "So in other words,\nthose expressions",
    "start": "607970",
    "end": "609889"
  },
  {
    "text": "we had earlier, for\ncomputing the probabilities,",
    "start": "609890",
    "end": "614340"
  },
  {
    "text": "let's go back to it.",
    "start": "614340",
    "end": "615540"
  },
  {
    "text": "For example, this\nexpression here that's",
    "start": "615540",
    "end": "617720"
  },
  {
    "text": "used in computing,\nthe probabilities,",
    "start": "617720",
    "end": "622189"
  },
  {
    "text": "this is the expression for\nthe single variable case",
    "start": "622190",
    "end": "625310"
  },
  {
    "text": "with all these\nconstants in that,",
    "start": "625310",
    "end": "627770"
  },
  {
    "text": "all the cancellation\nhappens, and we",
    "start": "627770",
    "end": "630650"
  },
  {
    "text": "get to this nice, simple\nexpression over here,",
    "start": "630650",
    "end": "634110"
  },
  {
    "text": "which just involves the\ndiscriminant functions.",
    "start": "634110",
    "end": "637260"
  },
  {
    "text": "So you see, you can actually\nshow that as well, it's",
    "start": "637260",
    "end": "640200"
  },
  {
    "text": "not that hard to show.",
    "start": "640200",
    "end": "642060"
  },
  {
    "text": "So can not only does\ndiscriminant analysis",
    "start": "642060",
    "end": "646740"
  },
  {
    "text": "give us a classifier, it also\ngives us the probabilities.",
    "start": "646740",
    "end": "649690"
  },
  {
    "start": "649690",
    "end": "652280"
  },
  {
    "text": "When k is two, we'll\nclassify it to class 2,",
    "start": "652280",
    "end": "654820"
  },
  {
    "text": "if these probabilities\nare bigger than 0.5,",
    "start": "654820",
    "end": "657380"
  },
  {
    "text": "else to class one, just\nlike in logistic regression.",
    "start": "657380",
    "end": "661530"
  },
  {
    "start": "660000",
    "end": "825000"
  },
  {
    "text": "Here's the credit data.",
    "start": "661530",
    "end": "664860"
  },
  {
    "text": "And we produce, in this case,\na misclassification table.",
    "start": "664860",
    "end": "670769"
  },
  {
    "text": "So this table, along\nthe horizontal,",
    "start": "670770",
    "end": "676680"
  },
  {
    "text": "it's got the true status.",
    "start": "676680",
    "end": "678580"
  },
  {
    "text": "So it's a true default\nstatus, which is no or yes.",
    "start": "678580",
    "end": "683610"
  },
  {
    "text": "You can see we had 10,000\nsamples in the credit data.",
    "start": "683610",
    "end": "687510"
  },
  {
    "text": "And in the vertical, we've got\nthe predicted status, no or yes.",
    "start": "687510",
    "end": "692910"
  },
  {
    "text": "So of course, we'd\nlike everything",
    "start": "692910",
    "end": "694920"
  },
  {
    "text": "to lie on the diagonal.",
    "start": "694920",
    "end": "697079"
  },
  {
    "text": "It turns out we've\ngot a lot of the no's",
    "start": "697080",
    "end": "698910"
  },
  {
    "text": "right, and not that\nmany of the yes's right.",
    "start": "698910",
    "end": "705379"
  },
  {
    "text": "So on the diagonal is\nwhat you got correct,",
    "start": "705380",
    "end": "707300"
  },
  {
    "text": "on the off diagonals is\nwhat you got incorrect.",
    "start": "707300",
    "end": "710300"
  },
  {
    "text": "Nevertheless,\noverall, we make 2.75%",
    "start": "710300",
    "end": "713420"
  },
  {
    "text": "misclassification errors here.",
    "start": "713420",
    "end": "716250"
  },
  {
    "text": "So this is called\na confusion matrix,",
    "start": "716250",
    "end": "719370"
  },
  {
    "text": "it tells you how well you did.",
    "start": "719370",
    "end": "722040"
  },
  {
    "text": "Now, there's some caveats,\nthis is training error,",
    "start": "722040",
    "end": "725899"
  },
  {
    "text": "we fit the rule to\nthese data, and now",
    "start": "725900",
    "end": "728010"
  },
  {
    "text": "we see how well it\nperforms on these data.",
    "start": "728010",
    "end": "730100"
  },
  {
    "text": "So we may be overfitting.",
    "start": "730100",
    "end": "732600"
  },
  {
    "text": "Well, we've got 10,000\ntraining samples here,",
    "start": "732600",
    "end": "735600"
  },
  {
    "text": "and we've only fit a\nhandful of parameters,",
    "start": "735600",
    "end": "739060"
  },
  {
    "text": "so it's very unlikely in this\ncase that we're overfitting.",
    "start": "739060",
    "end": "741870"
  },
  {
    "text": "For small training sets,\nthat would be an issue,",
    "start": "741870",
    "end": "743938"
  },
  {
    "text": "and you'd need to have\na separate test set.",
    "start": "743938",
    "end": "745730"
  },
  {
    "start": "745730",
    "end": "753690"
  },
  {
    "text": "Another thing to note is\nthat, although 2.75% seems",
    "start": "753690",
    "end": "759930"
  },
  {
    "text": "really good misclassification\nrate, if we just use",
    "start": "759930",
    "end": "763320"
  },
  {
    "text": "a very naive classification\nrule, and said,",
    "start": "763320",
    "end": "765760"
  },
  {
    "text": "I'm always classified\nto the largest class,",
    "start": "765760",
    "end": "769150"
  },
  {
    "text": "in other words classify\naccording to the prior,",
    "start": "769150",
    "end": "772440"
  },
  {
    "text": "we'd only make 3.333%\nerrors, because there's",
    "start": "772440",
    "end": "778110"
  },
  {
    "text": "a predominance of, of\nno's in this data set.",
    "start": "778110",
    "end": "781529"
  },
  {
    "text": "The total number of no's\nis 333 out of 10,000.",
    "start": "781530",
    "end": "788250"
  },
  {
    "text": "So this we call the null rate.",
    "start": "788250",
    "end": "790920"
  },
  {
    "text": "And so you always bear\nin mind the null rate",
    "start": "790920",
    "end": "795899"
  },
  {
    "text": "when getting excited about a\nmisclassification error rate.",
    "start": "795900",
    "end": "799040"
  },
  {
    "start": "799040",
    "end": "801662"
  },
  {
    "text": "The other thing to\nlook at, though,",
    "start": "801662",
    "end": "803120"
  },
  {
    "text": "is you can break the errors\ninto different kinds.",
    "start": "803120",
    "end": "806529"
  },
  {
    "text": "So of the true no's,\nwe make 0.2% errors,",
    "start": "806530",
    "end": "811630"
  },
  {
    "text": "so we hardly ever\nmisclassify a no.",
    "start": "811630",
    "end": "814360"
  },
  {
    "text": "But of the yes's, we make\na whopping 75.7% errors.",
    "start": "814360",
    "end": "819709"
  },
  {
    "text": "So the errors are very\nlopsided in this case,",
    "start": "819710",
    "end": "822790"
  },
  {
    "text": "and that's maybe not\nsuch a good idea.",
    "start": "822790",
    "end": "827550"
  },
  {
    "start": "825000",
    "end": "904000"
  },
  {
    "text": "So we break down these\nerrors into finer categories.",
    "start": "827550",
    "end": "833670"
  },
  {
    "text": "So we call the\nfalse positive rate,",
    "start": "833670",
    "end": "835790"
  },
  {
    "text": "the fraction of\nnegative examples",
    "start": "835790",
    "end": "838459"
  },
  {
    "text": "that are classified as positive,\nso they false positives.",
    "start": "838460",
    "end": "841860"
  },
  {
    "text": "In that case, that was\nthe 0.2% in this case.",
    "start": "841860",
    "end": "845630"
  },
  {
    "text": "And the false\nnegative rate, that's",
    "start": "845630",
    "end": "848090"
  },
  {
    "text": "the fraction of\npositive examples",
    "start": "848090",
    "end": "849650"
  },
  {
    "text": "that are classified as negative,\nthe 75.7% in this case.",
    "start": "849650",
    "end": "855300"
  },
  {
    "text": "Now we produce the\nclassification table",
    "start": "855300",
    "end": "857490"
  },
  {
    "text": "by intuitively correct,\nclassifying default",
    "start": "857490",
    "end": "861560"
  },
  {
    "text": "is yes if the probability of\ndefault was bigger than 0.5.",
    "start": "861560",
    "end": "868330"
  },
  {
    "text": "But it gave us\nthis very lopsided,",
    "start": "868330",
    "end": "871450"
  },
  {
    "text": "false positive and\nfalse negative rate.",
    "start": "871450",
    "end": "875440"
  },
  {
    "text": "In some cases, especially\nfor these kinds",
    "start": "875440",
    "end": "877510"
  },
  {
    "text": "of screening\nexamples, you may want",
    "start": "877510",
    "end": "881350"
  },
  {
    "text": "to change the false positive\nand false negative rates",
    "start": "881350",
    "end": "885160"
  },
  {
    "text": "and skew them to one\nside or the other.",
    "start": "885160",
    "end": "887589"
  },
  {
    "text": "And you can do this by\nchanging this threshold.",
    "start": "887590",
    "end": "890770"
  },
  {
    "text": "Instead of classifying,\nin this case, to default,",
    "start": "890770",
    "end": "893780"
  },
  {
    "text": "if it's bigger than 0.5,\nyou can change a threshold,",
    "start": "893780",
    "end": "896560"
  },
  {
    "text": "and maybe make the threshold\nin this case smaller,",
    "start": "896560",
    "end": "899050"
  },
  {
    "text": "so that you can catch more of\nthe high risk cases for default.",
    "start": "899050",
    "end": "905200"
  },
  {
    "start": "904000",
    "end": "1063000"
  },
  {
    "text": "And so here we've done it as\na function of the threshold,",
    "start": "905200",
    "end": "909640"
  },
  {
    "text": "and we've just looked at\ndecreasing thresholds.",
    "start": "909640",
    "end": "914050"
  },
  {
    "text": "And so in this plot we've got,\nin black, the overall error",
    "start": "914050",
    "end": "917830"
  },
  {
    "text": "rate, in orange, we've got\nthe false positive rate,",
    "start": "917830",
    "end": "923830"
  },
  {
    "text": "and in blue, we've got\nthe false negative rate.",
    "start": "923830",
    "end": "926650"
  },
  {
    "text": "And so, as we decrease\nthe threshold,",
    "start": "926650",
    "end": "930100"
  },
  {
    "text": "the false positive rate\nincreases, because now we're",
    "start": "930100",
    "end": "934060"
  },
  {
    "text": "going to classify more and\nmore negatives as positive.",
    "start": "934060",
    "end": "937360"
  },
  {
    "text": "But it increases very slowly.",
    "start": "937360",
    "end": "939470"
  },
  {
    "text": "You'll see for a long part\nof the threshold going down,",
    "start": "939470",
    "end": "943990"
  },
  {
    "text": "the decrease in threshold,\nthe false positive rate",
    "start": "943990",
    "end": "947470"
  },
  {
    "text": "doesn't increase very fast.",
    "start": "947470",
    "end": "949540"
  },
  {
    "text": "Of course, the false negative\nrate increases as we do that.",
    "start": "949540",
    "end": "952660"
  },
  {
    "text": "And so even at 0.1,\nthe false positive rate",
    "start": "952660",
    "end": "962740"
  },
  {
    "text": "hasn't increased a huge\namount, and the false negative",
    "start": "962740",
    "end": "966220"
  },
  {
    "text": "hasn't increased a huge\namount, but we've changed",
    "start": "966220",
    "end": "969160"
  },
  {
    "text": "the balance of classification.",
    "start": "969160",
    "end": "972300"
  },
  {
    "text": "And so you can\nchange the threshold.",
    "start": "972300",
    "end": "974610"
  },
  {
    "text": "Well, you can capture\nthat change in threshold",
    "start": "974610",
    "end": "977550"
  },
  {
    "text": "in what's known as an ROC curve.",
    "start": "977550",
    "end": "980910"
  },
  {
    "text": "So what this shows is\nthe two error rates,",
    "start": "980910",
    "end": "986459"
  },
  {
    "text": "in this case, false positive\nrate and true positive rate,",
    "start": "986460",
    "end": "992040"
  },
  {
    "text": "as we change the threshold.",
    "start": "992040",
    "end": "994350"
  },
  {
    "text": "And we'd like the false\npositive rate to be low,",
    "start": "994350",
    "end": "1000860"
  },
  {
    "text": "and the true positive\nrate to be high.",
    "start": "1000860",
    "end": "1003290"
  },
  {
    "text": "And so this ROC curve traces\nout as we change the threshold.",
    "start": "1003290",
    "end": "1008480"
  },
  {
    "text": "And the 45-degree line is\nthe no information line.",
    "start": "1008480",
    "end": "1014399"
  },
  {
    "text": "And what you'd like, is\nyou'd like this curve",
    "start": "1014400",
    "end": "1017390"
  },
  {
    "text": "to be right up as\nfar as possible",
    "start": "1017390",
    "end": "1019580"
  },
  {
    "text": "into the top left-hand corner.",
    "start": "1019580",
    "end": "1021530"
  },
  {
    "text": "So if you had a true\npositive rate of 1",
    "start": "1021530",
    "end": "1024800"
  },
  {
    "text": "and a false positive\nrate of zero.",
    "start": "1024800",
    "end": "1027131"
  },
  {
    "text": "If you flipped a coin, you'd\nbe on the straight line.",
    "start": "1027131",
    "end": "1029339"
  },
  {
    "text": "You'd be on the straight line.",
    "start": "1029339",
    "end": "1030890"
  },
  {
    "text": "And so this is a\nsingle curve that",
    "start": "1030890",
    "end": "1034939"
  },
  {
    "text": "captures the behavior of\nthe classification rule",
    "start": "1034940",
    "end": "1038539"
  },
  {
    "text": "for all possible thresholds.",
    "start": "1038540",
    "end": "1040849"
  },
  {
    "text": "And you can compare\ndifferent classifiers",
    "start": "1040849",
    "end": "1042800"
  },
  {
    "text": "by comparing their ROC curves.",
    "start": "1042800",
    "end": "1045470"
  },
  {
    "text": "And to summarize it even more,\nwe sometimes use the area",
    "start": "1045470",
    "end": "1051230"
  },
  {
    "text": "under the curve, or\nAUC, which captures",
    "start": "1051230",
    "end": "1054350"
  },
  {
    "text": "the extent to which you're\nup in the Northwest corner,",
    "start": "1054350",
    "end": "1057890"
  },
  {
    "text": "and higher AUC is good.",
    "start": "1057890",
    "end": "1061050"
  },
  {
    "start": "1061050",
    "end": "1063000"
  }
]