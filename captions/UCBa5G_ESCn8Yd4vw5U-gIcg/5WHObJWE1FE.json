[
  {
    "start": "0",
    "end": "42000"
  },
  {
    "start": "0",
    "end": "5270"
  },
  {
    "text": "Hello.",
    "start": "5270",
    "end": "5770"
  },
  {
    "text": "This is your\nEmbedded EthiCS team.",
    "start": "5770",
    "end": "7550"
  },
  {
    "text": "In this video, we will be\ndiscussing the AI alignment",
    "start": "7550",
    "end": "10900"
  },
  {
    "text": "problem and go over two ways\nin which these problems are",
    "start": "10900",
    "end": "13900"
  },
  {
    "text": "instantiated-- reward hacking\nand negative side effects.",
    "start": "13900",
    "end": "17710"
  },
  {
    "text": "After watching\nthis mini-lecture,",
    "start": "17710",
    "end": "19262"
  },
  {
    "text": "you should be better prepared\nto answer problem five",
    "start": "19263",
    "end": "21430"
  },
  {
    "text": "in the homework assignment.",
    "start": "21430",
    "end": "24110"
  },
  {
    "text": "In this video, we'll define\nthe AI alignment problem",
    "start": "24110",
    "end": "26720"
  },
  {
    "text": "and go over two problems of\nAI alignment-- reward hacking",
    "start": "26720",
    "end": "29660"
  },
  {
    "text": "and negative side effects.",
    "start": "29660",
    "end": "31070"
  },
  {
    "text": "We'll discuss these\ntwo problems and give",
    "start": "31070",
    "end": "32900"
  },
  {
    "text": "some examples to help you\nidentify them in the future.",
    "start": "32900",
    "end": "35510"
  },
  {
    "text": "We'll also discuss the\nethical implications",
    "start": "35510",
    "end": "38239"
  },
  {
    "text": "of the AI alignment problem.",
    "start": "38240",
    "end": "42090"
  },
  {
    "start": "42000",
    "end": "198000"
  },
  {
    "text": "Let's begin by talking about\nthe AI alignment problem.",
    "start": "42090",
    "end": "46860"
  },
  {
    "text": "The goal of AI alignment is\nto ensure that AI is properly",
    "start": "46860",
    "end": "50540"
  },
  {
    "text": "aligned with human interests.",
    "start": "50540",
    "end": "52520"
  },
  {
    "text": "AI misalignment occurs\nwhen an AI system is not",
    "start": "52520",
    "end": "55580"
  },
  {
    "text": "able to achieve this.",
    "start": "55580",
    "end": "57450"
  },
  {
    "text": "So how do we define what\nalignment looks like?",
    "start": "57450",
    "end": "61590"
  },
  {
    "text": "The first approach\ncould be the agent",
    "start": "61590",
    "end": "63960"
  },
  {
    "text": "does what I instructed to do.",
    "start": "63960",
    "end": "65580"
  },
  {
    "text": "It's simple.",
    "start": "65580",
    "end": "66670"
  },
  {
    "text": "I give it a set of instructions,\nand they follow it.",
    "start": "66670",
    "end": "69580"
  },
  {
    "text": "But in reality, it\nis more complicated.",
    "start": "69580",
    "end": "72210"
  },
  {
    "text": "Think about large models,\nlike large language models.",
    "start": "72210",
    "end": "76200"
  },
  {
    "text": "It's not possible for us to\ntake such a literal approach",
    "start": "76200",
    "end": "79170"
  },
  {
    "text": "because there are so many\nparameters, contingencies,",
    "start": "79170",
    "end": "81990"
  },
  {
    "text": "possibilities that we cannot\ngive an instruction for all",
    "start": "81990",
    "end": "85409"
  },
  {
    "text": "of them.",
    "start": "85410",
    "end": "86290"
  },
  {
    "text": "This approach also\nruns into issues",
    "start": "86290",
    "end": "88230"
  },
  {
    "text": "of reward hacking, which we'll\ntalk about later in this video.",
    "start": "88230",
    "end": "92660"
  },
  {
    "text": "Then, what about if the agent\ndoes what I intended to do?",
    "start": "92660",
    "end": "96000"
  },
  {
    "text": "Suppose our development\nin AI is advanced enough",
    "start": "96000",
    "end": "99740"
  },
  {
    "text": "for our models to\nunderstand the intentions",
    "start": "99740",
    "end": "102259"
  },
  {
    "text": "behind our instructions.",
    "start": "102260",
    "end": "103790"
  },
  {
    "text": "Say, they grasp\nour human language",
    "start": "103790",
    "end": "105830"
  },
  {
    "text": "or cultures and practices.",
    "start": "105830",
    "end": "107270"
  },
  {
    "text": "That sounds convincing, but\nagain, we run into a problem.",
    "start": "107270",
    "end": "111180"
  },
  {
    "text": "What if our intentions\nare irrational?",
    "start": "111180",
    "end": "113720"
  },
  {
    "text": "Misinformed?",
    "start": "113720",
    "end": "114740"
  },
  {
    "text": "Should we still\npermit these models",
    "start": "114740",
    "end": "116450"
  },
  {
    "text": "to operate according\nto our intentions?",
    "start": "116450",
    "end": "120009"
  },
  {
    "text": "OK.",
    "start": "120010",
    "end": "120510"
  },
  {
    "text": "Then let's say we\nwant our agent to do",
    "start": "120510",
    "end": "122610"
  },
  {
    "text": "what I want it to do if I\nwere rational and informed.",
    "start": "122610",
    "end": "127080"
  },
  {
    "text": "This way, we avoid lapses\nin judgment or errors",
    "start": "127080",
    "end": "130380"
  },
  {
    "text": "from limited information.",
    "start": "130380",
    "end": "132130"
  },
  {
    "text": "But this doesn't\nprevent us from wanting",
    "start": "132130",
    "end": "134070"
  },
  {
    "text": "unethical or harmful things.",
    "start": "134070",
    "end": "136320"
  },
  {
    "text": "Depending on our\nnotion of rationality,",
    "start": "136320",
    "end": "138820"
  },
  {
    "text": "which we won't get into here,\nand however informed we are,",
    "start": "138820",
    "end": "141585"
  },
  {
    "text": "we can still arrive at desires\nthat seem to be nevertheless",
    "start": "141585",
    "end": "144660"
  },
  {
    "text": "morally reprehensible.",
    "start": "144660",
    "end": "148070"
  },
  {
    "text": "Now, we finally arrive\nat the values approach.",
    "start": "148070",
    "end": "150800"
  },
  {
    "text": "We design our AI models to do\nwhat it morally ought to do,",
    "start": "150800",
    "end": "154400"
  },
  {
    "text": "as defined by the individual\nor our broader society.",
    "start": "154400",
    "end": "157879"
  },
  {
    "text": "Values indicate our judgment\nof what's good or bad",
    "start": "157880",
    "end": "161480"
  },
  {
    "text": "and should be morally\npraised or reprehended.",
    "start": "161480",
    "end": "164300"
  },
  {
    "text": "With a values-based approach, we\ncan avoid all the difficulties",
    "start": "164300",
    "end": "167810"
  },
  {
    "text": "we encountered with our previous\nconceptions of alignment.",
    "start": "167810",
    "end": "170900"
  },
  {
    "text": "Additionally, we can think\nbeyond the simple calculation",
    "start": "170900",
    "end": "174260"
  },
  {
    "text": "of maximizing good\nand think about how",
    "start": "174260",
    "end": "176540"
  },
  {
    "text": "our AI models can promote our\nnotions of justice and rights.",
    "start": "176540",
    "end": "180349"
  },
  {
    "text": "Importantly, though the\nvalues-based approach is not",
    "start": "180350",
    "end": "183530"
  },
  {
    "text": "the end all be all, there\ncan be many criticisms",
    "start": "183530",
    "end": "186980"
  },
  {
    "text": "of the values-based approach.",
    "start": "186980",
    "end": "188510"
  },
  {
    "text": "Similarly to how you walk\nthrough the other definitions",
    "start": "188510",
    "end": "191030"
  },
  {
    "text": "of alignment, try\nto think about what",
    "start": "191030",
    "end": "193340"
  },
  {
    "text": "can be some potential\npushback against",
    "start": "193340",
    "end": "195230"
  },
  {
    "text": "the values-based approach.",
    "start": "195230",
    "end": "198780"
  },
  {
    "text": "How we decide which values\nto align with our AI models",
    "start": "198780",
    "end": "201980"
  },
  {
    "text": "can be a bit tricky, and\nthere is no consensus",
    "start": "201980",
    "end": "205069"
  },
  {
    "text": "of which approach is best.",
    "start": "205070",
    "end": "206810"
  },
  {
    "text": "Values are often specific\nto certain use cases",
    "start": "206810",
    "end": "209690"
  },
  {
    "text": "and communities.",
    "start": "209690",
    "end": "210810"
  },
  {
    "text": "So determining which\nvalues to prioritize often",
    "start": "210810",
    "end": "213380"
  },
  {
    "text": "requires being sensitive\nto various cultural norms",
    "start": "213380",
    "end": "215870"
  },
  {
    "text": "and values that\nyour users may hold.",
    "start": "215870",
    "end": "218510"
  },
  {
    "text": "Here, we'll share three\npossible frameworks",
    "start": "218510",
    "end": "221330"
  },
  {
    "text": "rooted in philosophy\nand ethics you",
    "start": "221330",
    "end": "223070"
  },
  {
    "text": "could draw on to better\nalign AI models with values.",
    "start": "223070",
    "end": "226830"
  },
  {
    "text": "The first principle\nis selecting values",
    "start": "226830",
    "end": "230360"
  },
  {
    "text": "that are aligned with\nglobal public morality",
    "start": "230360",
    "end": "232400"
  },
  {
    "text": "and previously\ncodified human rights.",
    "start": "232400",
    "end": "234860"
  },
  {
    "text": "Even though which\nvalues are important",
    "start": "234860",
    "end": "236840"
  },
  {
    "text": "can vary among\ndifferent communities,",
    "start": "236840",
    "end": "238640"
  },
  {
    "text": "there are certain\nprinciples of justice",
    "start": "238640",
    "end": "240470"
  },
  {
    "text": "that are supported by\nthe majority of people--",
    "start": "240470",
    "end": "242690"
  },
  {
    "text": "for example, basic human\nrights, such as the belief",
    "start": "242690",
    "end": "245510"
  },
  {
    "text": "that all individuals should be\ngiven food, water, education,",
    "start": "245510",
    "end": "248870"
  },
  {
    "text": "and protection from\nphysical violence.",
    "start": "248870",
    "end": "251420"
  },
  {
    "text": "Oftentimes, these\nhave already been",
    "start": "251420",
    "end": "253040"
  },
  {
    "text": "implemented into regulations\nby government organizations.",
    "start": "253040",
    "end": "257680"
  },
  {
    "text": "The second is choosing values\nbehind a veil of ignorance.",
    "start": "257680",
    "end": "260759"
  },
  {
    "text": "The veil of ignorance\nis a thought experiment",
    "start": "260760",
    "end": "263220"
  },
  {
    "text": "introduced by the\nphilosopher John Rawls",
    "start": "263220",
    "end": "265320"
  },
  {
    "text": "that asks people to consider\na device that prevents them",
    "start": "265320",
    "end": "268560"
  },
  {
    "text": "from knowing their own\nparticular moral beliefs",
    "start": "268560",
    "end": "270810"
  },
  {
    "text": "or the position they\nwill occupy in society.",
    "start": "270810",
    "end": "273370"
  },
  {
    "text": "So using the veil\nof ignorance, we",
    "start": "273370",
    "end": "275160"
  },
  {
    "text": "might ask what\nprinciples would people",
    "start": "275160",
    "end": "277110"
  },
  {
    "text": "choose to regulate an AI\nsystem if they did not",
    "start": "277110",
    "end": "280169"
  },
  {
    "text": "know who they were or what\nbelief system they ascribe to.",
    "start": "280170",
    "end": "283680"
  },
  {
    "text": "In other words, what\nprinciples or values",
    "start": "283680",
    "end": "285810"
  },
  {
    "text": "might people select if they did\nnot know for certain how the AI",
    "start": "285810",
    "end": "289139"
  },
  {
    "text": "system would impact them?",
    "start": "289140",
    "end": "290640"
  },
  {
    "text": "This principle assumes that\npeople are risk-averse.",
    "start": "290640",
    "end": "294630"
  },
  {
    "text": "Finally, the third principle\nis using social choice theory",
    "start": "294630",
    "end": "297900"
  },
  {
    "text": "to combine different\nviewpoints to ultimately inform",
    "start": "297900",
    "end": "300449"
  },
  {
    "text": "the alignment of an AI model.",
    "start": "300450",
    "end": "302250"
  },
  {
    "text": "One way of doing this is through\nusing democratic processes",
    "start": "302250",
    "end": "305760"
  },
  {
    "text": "such as voting, discussion,\nand civic engagement",
    "start": "305760",
    "end": "308160"
  },
  {
    "text": "to arrive at values.",
    "start": "308160",
    "end": "309540"
  },
  {
    "text": "The other is by combining\nindividual preferences",
    "start": "309540",
    "end": "311970"
  },
  {
    "text": "into a single ranking.",
    "start": "311970",
    "end": "313470"
  },
  {
    "text": "And again, these are\nnot the only frameworks",
    "start": "313470",
    "end": "315720"
  },
  {
    "text": "that would be appropriate to\nalign an AI model with values,",
    "start": "315720",
    "end": "319680"
  },
  {
    "text": "but they should give\nyou a starting point.",
    "start": "319680",
    "end": "322370"
  },
  {
    "text": "Now, let's take a look at those\nthree principles in practice",
    "start": "322370",
    "end": "325630"
  },
  {
    "text": "to help make those definitions\na bit more concrete.",
    "start": "325630",
    "end": "328810"
  },
  {
    "text": "Consider self-driving cars.",
    "start": "328810",
    "end": "330520"
  },
  {
    "text": "If we are aligning values with\nthe global public morality",
    "start": "330520",
    "end": "333639"
  },
  {
    "text": "and human rights\nframework, we might",
    "start": "333640",
    "end": "335410"
  },
  {
    "text": "consider existing regulations\nset by government entities.",
    "start": "335410",
    "end": "338680"
  },
  {
    "text": "For example, the State\nof California Department",
    "start": "338680",
    "end": "341470"
  },
  {
    "text": "of Motor Vehicles has a\nset of standards defining",
    "start": "341470",
    "end": "344920"
  },
  {
    "text": "specific terms related\nto autonomous vehicles,",
    "start": "344920",
    "end": "347620"
  },
  {
    "text": "requirements for\ntesting permits,",
    "start": "347620",
    "end": "350110"
  },
  {
    "text": "and requirements\nfor test drivers.",
    "start": "350110",
    "end": "353050"
  },
  {
    "text": "If we are selecting values\nusing Rawls' veil of ignorance",
    "start": "353050",
    "end": "356080"
  },
  {
    "text": "thought experiment, we might\nconsider who's at greatest risk",
    "start": "356080",
    "end": "359110"
  },
  {
    "text": "to prioritize the\nleast well-off.",
    "start": "359110",
    "end": "361150"
  },
  {
    "text": "For example, pedestrians\nwith darker skin",
    "start": "361150",
    "end": "363729"
  },
  {
    "text": "might be more likely to get\nhit by a self-driving car",
    "start": "363730",
    "end": "366040"
  },
  {
    "text": "than white pedestrians.",
    "start": "366040",
    "end": "367340"
  },
  {
    "text": "So maybe this\ninforms the values we",
    "start": "367340",
    "end": "369100"
  },
  {
    "text": "select for how to test AI\nmodels in the real world.",
    "start": "369100",
    "end": "374230"
  },
  {
    "text": "Finally, if we are using\nthe social choice theory,",
    "start": "374230",
    "end": "377230"
  },
  {
    "text": "we might involve\ndifferent stakeholders",
    "start": "377230",
    "end": "378940"
  },
  {
    "text": "in collectively determining\nhow research deployment",
    "start": "378940",
    "end": "381580"
  },
  {
    "text": "and oversight of autonomous\nvehicles is conducted.",
    "start": "381580",
    "end": "386680"
  },
  {
    "text": "The alignment problem has\nimportant implications",
    "start": "386680",
    "end": "388780"
  },
  {
    "text": "for real-life society.",
    "start": "388780",
    "end": "390050"
  },
  {
    "text": "As systems are misaligned with\ntheir users and societies,",
    "start": "390050",
    "end": "393009"
  },
  {
    "text": "goals can cause\nsignificant harm.",
    "start": "393010",
    "end": "395210"
  },
  {
    "text": "Let's look at some\nmore examples.",
    "start": "395210",
    "end": "397520"
  },
  {
    "text": "The first is Tay.",
    "start": "397520",
    "end": "399340"
  },
  {
    "text": "Tay was a Microsoft AI\nchatbot launched on Twitter",
    "start": "399340",
    "end": "403120"
  },
  {
    "text": "in March of 2016.",
    "start": "403120",
    "end": "404860"
  },
  {
    "text": "In less than a day,\nit was taken down",
    "start": "404860",
    "end": "406840"
  },
  {
    "text": "because it was generating\ntweets and replies that were",
    "start": "406840",
    "end": "409120"
  },
  {
    "text": "considered racist and sexist.",
    "start": "409120",
    "end": "411130"
  },
  {
    "text": "The bot's behavior\nwasn't necessarily",
    "start": "411130",
    "end": "413320"
  },
  {
    "text": "due to a programming error.",
    "start": "413320",
    "end": "414583"
  },
  {
    "text": "Instead, it was because\nthe developers had not",
    "start": "414583",
    "end": "416500"
  },
  {
    "text": "given the bot an understanding\nof appropriate human behavior.",
    "start": "416500",
    "end": "419800"
  },
  {
    "text": "In the absence of\nthat, the bot began",
    "start": "419800",
    "end": "421539"
  },
  {
    "text": "to mimic the harmful behavior it\nsaw among other Twitter users.",
    "start": "421540",
    "end": "425630"
  },
  {
    "text": "We also see AI misalignment\nin medical applications of AI.",
    "start": "425630",
    "end": "429970"
  },
  {
    "text": "For example, one\nalgorithm used in the US",
    "start": "429970",
    "end": "433420"
  },
  {
    "text": "to identify patients who\nmight need additional care",
    "start": "433420",
    "end": "436360"
  },
  {
    "text": "uses cost as a measure\nof health care need.",
    "start": "436360",
    "end": "439129"
  },
  {
    "text": "However, because of unequal\naccess to health care,",
    "start": "439130",
    "end": "441850"
  },
  {
    "text": "typically, less money is spent\non care for Black patients",
    "start": "441850",
    "end": "445510"
  },
  {
    "text": "compared to white patients.",
    "start": "445510",
    "end": "447240"
  },
  {
    "text": "Thus, this leads the algorithm\nto prioritize white patients",
    "start": "447240",
    "end": "450349"
  },
  {
    "text": "over sicker Black patients.",
    "start": "450350",
    "end": "452240"
  },
  {
    "text": "As another example\nin this space,",
    "start": "452240",
    "end": "454009"
  },
  {
    "text": "during the height of\nthe COVID-19 pandemic,",
    "start": "454010",
    "end": "456110"
  },
  {
    "text": "Facebook tried to promote\nvaccine-related content",
    "start": "456110",
    "end": "458360"
  },
  {
    "text": "from government\nagencies to encourage",
    "start": "458360",
    "end": "460430"
  },
  {
    "text": "people to get vaccinated.",
    "start": "460430",
    "end": "461930"
  },
  {
    "text": "Potentially, the intended values\nhere align with society's goals",
    "start": "461930",
    "end": "465530"
  },
  {
    "text": "to stop the spread of the\ndisease by getting more people",
    "start": "465530",
    "end": "467960"
  },
  {
    "text": "vaccinated.",
    "start": "467960",
    "end": "468949"
  },
  {
    "text": "Yet these posts from\nofficial sources",
    "start": "468950",
    "end": "471470"
  },
  {
    "text": "ended up being flooded\nwith critical comments,",
    "start": "471470",
    "end": "473700"
  },
  {
    "text": "including misinformation.",
    "start": "473700",
    "end": "475490"
  },
  {
    "text": "And as posts with\nantivaccine comments",
    "start": "475490",
    "end": "478160"
  },
  {
    "text": "became more visible\nto Facebook users,",
    "start": "478160",
    "end": "480200"
  },
  {
    "text": "it may have undermined\nvaccine uptake.",
    "start": "480200",
    "end": "482240"
  },
  {
    "start": "482240",
    "end": "484919"
  },
  {
    "start": "484000",
    "end": "611000"
  },
  {
    "text": "So recall in the\nmountain car assignment,",
    "start": "484920",
    "end": "487620"
  },
  {
    "text": "you learned about safe\nexploration and reinforcement",
    "start": "487620",
    "end": "490410"
  },
  {
    "text": "learning as one example\nof a problem in AI safety.",
    "start": "490410",
    "end": "493950"
  },
  {
    "text": "Two other problems\nin AI safety which",
    "start": "493950",
    "end": "496350"
  },
  {
    "text": "are also examples of\nthe AI alignment problem",
    "start": "496350",
    "end": "499020"
  },
  {
    "text": "are reward hacking and\nnegative side effects.",
    "start": "499020",
    "end": "501720"
  },
  {
    "text": "While these relate to AI safety\nand reinforcement learning,",
    "start": "501720",
    "end": "504930"
  },
  {
    "text": "they are also relevant to\nother types of AI algorithms,",
    "start": "504930",
    "end": "507690"
  },
  {
    "text": "such as large language models,\nevolutionary algorithms,",
    "start": "507690",
    "end": "510660"
  },
  {
    "text": "and genetic algorithms.",
    "start": "510660",
    "end": "512320"
  },
  {
    "text": "So in this video, we'll\ntalk about them broadly",
    "start": "512320",
    "end": "514530"
  },
  {
    "text": "rather than a specific\ntype of algorithm.",
    "start": "514530",
    "end": "517080"
  },
  {
    "text": "Let's begin by discussing\nreward hacking.",
    "start": "517080",
    "end": "519044"
  },
  {
    "start": "519045",
    "end": "522360"
  },
  {
    "text": "Reward hacking\noccurs when an agent",
    "start": "522360",
    "end": "524370"
  },
  {
    "text": "games its reward function.",
    "start": "524370",
    "end": "526050"
  },
  {
    "text": "By doing this, the agent\ndiscovers a clever or easy",
    "start": "526050",
    "end": "529649"
  },
  {
    "text": "solution that still formally\nsatisfies the qualifications",
    "start": "529650",
    "end": "532860"
  },
  {
    "text": "to acquire rewards and\nis able to maximize",
    "start": "532860",
    "end": "536070"
  },
  {
    "text": "the rewards it receives.",
    "start": "536070",
    "end": "537840"
  },
  {
    "text": "However, the solution\nthey've discovered",
    "start": "537840",
    "end": "539910"
  },
  {
    "text": "might not align with the spirit\nof the designer's intent.",
    "start": "539910",
    "end": "542850"
  },
  {
    "text": "In other words, the agent\noptimizes the formal objective",
    "start": "542850",
    "end": "546240"
  },
  {
    "text": "function but doesn't\nlearn the outcome intended",
    "start": "546240",
    "end": "548550"
  },
  {
    "text": "by the programmer or designer.",
    "start": "548550",
    "end": "550500"
  },
  {
    "text": "For example, if we\nreward a cleaning robot",
    "start": "550500",
    "end": "553140"
  },
  {
    "text": "for picking up messes,\none way in which",
    "start": "553140",
    "end": "555365"
  },
  {
    "text": "it might gain its\nreward function",
    "start": "555365",
    "end": "556740"
  },
  {
    "text": "is by hiding messes behind\nfurniture or under the rug.",
    "start": "556740",
    "end": "559830"
  },
  {
    "text": "Another way is by\nbringing in more trash",
    "start": "559830",
    "end": "561750"
  },
  {
    "text": "and starting over once it's done\nto keep receiving the rewards.",
    "start": "561750",
    "end": "565890"
  },
  {
    "text": "Let's consider two\nexamples of reward hacking.",
    "start": "565890",
    "end": "568670"
  },
  {
    "text": "In the first, a\nreinforcement learning",
    "start": "568670",
    "end": "570800"
  },
  {
    "text": "agent that was designed to move\na block to a certain position",
    "start": "570800",
    "end": "574370"
  },
  {
    "text": "on the table learned to move\nthe table rather than the block.",
    "start": "574370",
    "end": "577760"
  },
  {
    "text": "In the second, ChatGPT made up\nfake cases related to a prompt",
    "start": "577760",
    "end": "582260"
  },
  {
    "text": "when it was asked to\ndeliver cases by a lawyer.",
    "start": "582260",
    "end": "586550"
  },
  {
    "text": "Reward hacking arises\nfrom misspecified rewards",
    "start": "586550",
    "end": "590240"
  },
  {
    "text": "when important\naspects of the reward",
    "start": "590240",
    "end": "592220"
  },
  {
    "text": "have been left out in\nthe design process,",
    "start": "592220",
    "end": "594500"
  },
  {
    "text": "leading to poor agent behavior.",
    "start": "594500",
    "end": "597120"
  },
  {
    "text": "One way to mitigate\nreward hacking",
    "start": "597120",
    "end": "599120"
  },
  {
    "text": "is to anticipate and penalize\na possible misbehavior",
    "start": "599120",
    "end": "602210"
  },
  {
    "text": "in advance, but some things\nwill be missed by human error.",
    "start": "602210",
    "end": "605690"
  },
  {
    "text": "Addressing these\nlimitations is still",
    "start": "605690",
    "end": "607490"
  },
  {
    "text": "an open problem in AI research.",
    "start": "607490",
    "end": "610700"
  },
  {
    "text": "Now we'll discuss\nnegative side effects.",
    "start": "610700",
    "end": "614440"
  },
  {
    "start": "611000",
    "end": "693000"
  },
  {
    "text": "Negative side effects arise\nwhen an agent's behavior,",
    "start": "614440",
    "end": "617320"
  },
  {
    "text": "while pursuing its goals,\nends up conflicting",
    "start": "617320",
    "end": "620530"
  },
  {
    "text": "with broader societal values.",
    "start": "620530",
    "end": "622460"
  },
  {
    "text": "Going back to the example\nof a cleaning robot,",
    "start": "622460",
    "end": "624610"
  },
  {
    "text": "the robot might\nknock over a vase",
    "start": "624610",
    "end": "626470"
  },
  {
    "text": "or push people and\npets out of the way",
    "start": "626470",
    "end": "628720"
  },
  {
    "text": "because it can clean\nfaster by doing so.",
    "start": "628720",
    "end": "631970"
  },
  {
    "text": "Some examples of\nnegative side effects",
    "start": "631970",
    "end": "634430"
  },
  {
    "text": "include an autonomous\nagent that splashes water",
    "start": "634430",
    "end": "637910"
  },
  {
    "text": "on nearby pedestrians\nas it rolls by",
    "start": "637910",
    "end": "640279"
  },
  {
    "text": "or an AI system that\ncompletely displaces workers",
    "start": "640280",
    "end": "643430"
  },
  {
    "text": "in a particular industry.",
    "start": "643430",
    "end": "646779"
  },
  {
    "text": "Negative side effects occur\nbecause the agent's model",
    "start": "646780",
    "end": "649720"
  },
  {
    "text": "and objective function\nfocus on some aspects",
    "start": "649720",
    "end": "652089"
  },
  {
    "text": "of the environment over others.",
    "start": "652090",
    "end": "653840"
  },
  {
    "text": "This can happen either\nbecause of misalignment,",
    "start": "653840",
    "end": "656560"
  },
  {
    "text": "distributional\nshifts, or the agent",
    "start": "656560",
    "end": "658779"
  },
  {
    "text": "having incomplete knowledge.",
    "start": "658780",
    "end": "660550"
  },
  {
    "text": "Misaligned systems are more\nlikely to produce negative side",
    "start": "660550",
    "end": "663820"
  },
  {
    "text": "effects because they are not\naligned with users' intentions",
    "start": "663820",
    "end": "666820"
  },
  {
    "text": "and goals.",
    "start": "666820",
    "end": "667970"
  },
  {
    "text": "However, negative side effects\ncan occur even in contexts",
    "start": "667970",
    "end": "671709"
  },
  {
    "text": "where the agent optimizes\nvalues that align",
    "start": "671710",
    "end": "673900"
  },
  {
    "text": "with users' objectives.",
    "start": "673900",
    "end": "675350"
  },
  {
    "text": "For example, if an AI system is\ndeployed in an environment that",
    "start": "675350",
    "end": "678730"
  },
  {
    "text": "is different from the\none it was tested in",
    "start": "678730",
    "end": "680889"
  },
  {
    "text": "and does not have enough\ninformation about how",
    "start": "680890",
    "end": "683110"
  },
  {
    "text": "to respond to a new scenario,\nnegative side effects",
    "start": "683110",
    "end": "686110"
  },
  {
    "text": "may occur.",
    "start": "686110",
    "end": "687959"
  },
  {
    "start": "687960",
    "end": "693000"
  }
]