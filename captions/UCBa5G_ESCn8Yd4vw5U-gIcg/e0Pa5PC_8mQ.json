[
  {
    "start": "0",
    "end": "185000"
  },
  {
    "text": "Hello, welcome to the section on probabilistic classification.",
    "start": "5000",
    "end": "9790"
  },
  {
    "text": "So- so far we've",
    "start": "10400",
    "end": "17520"
  },
  {
    "text": "talked about classifiers that given a record,",
    "start": "17520",
    "end": "22725"
  },
  {
    "text": "u return for us a prediction of which class the target variable lies in.",
    "start": "22725",
    "end": "30855"
  },
  {
    "text": "And now we want to talk in this section about an extension of that idea.",
    "start": "30855",
    "end": "37380"
  },
  {
    "text": "Instead of having a classifier that simply, uh, predicts one value.",
    "start": "37380",
    "end": "42750"
  },
  {
    "text": "We want to think about a classifier that can make more general types of predictions.",
    "start": "42750",
    "end": "48035"
  },
  {
    "text": "So if we have a, uh, classifier that predicts one value, let's say call it v-hat, um,",
    "start": "48035",
    "end": "54640"
  },
  {
    "text": "that would be called a- a point classifier or a point predictor, it makes just one guess.",
    "start": "54640",
    "end": "60365"
  },
  {
    "text": "Um, a classifier that produces more than a single guess,",
    "start": "60365",
    "end": "66174"
  },
  {
    "text": "might, for example, produce a list of guesses or an ordered list of guesses.",
    "start": "66174",
    "end": "72305"
  },
  {
    "text": "Um, so it says here are the top three things, so for example, we've got a camera,",
    "start": "72305",
    "end": "78645"
  },
  {
    "text": "it's sitting on a car, it's looking at all the things out on the street, and it says, well, you know,",
    "start": "78645",
    "end": "84170"
  },
  {
    "text": "that could be a pedestrian or it could be a person riding a scooter, or it could be a, uh,",
    "start": "84170",
    "end": "92240"
  },
  {
    "text": "a person, uh, ah, who is, uh, riding a bicycle. And those would be the- the best guess",
    "start": "92240",
    "end": "101030"
  },
  {
    "text": "is that the predictor makes it knows it's not a car, but it- it can't determine exactly which one of those three things it is.",
    "start": "101030",
    "end": "109940"
  },
  {
    "text": "And it may rank those guesses. Now it says most likely to be a,",
    "start": "109940",
    "end": "116365"
  },
  {
    "text": "uh, uh pedestrian, second choice is scooters, third choice is bicyclist.",
    "start": "116365",
    "end": "123549"
  },
  {
    "text": "Um, another, uh, possible output of a predictor or a",
    "start": "124580",
    "end": "130550"
  },
  {
    "text": "classifier would be a probability distribution on possible targets.",
    "start": "130550",
    "end": "136905"
  },
  {
    "text": "So with probability 0.9, it's a pedestrian. With probability 0.05, somebody on a scooter,",
    "start": "136905",
    "end": "143129"
  },
  {
    "text": "with probability 0.05. it's, uh, somebody on a bicycle.",
    "start": "143130",
    "end": "148390"
  },
  {
    "text": "Um, and, um, uh, these are very common.",
    "start": "148390",
    "end": "153765"
  },
  {
    "text": "Um, in fact, probabilistic classification is probably the most common type of classifier that people use today.",
    "start": "153765",
    "end": "162004"
  },
  {
    "text": "Um, uh, and we also of course, are quite familiar with the idea of probabilistic, uh,",
    "start": "162005",
    "end": "169040"
  },
  {
    "text": "predictions from weather forecasts. Uh, if you look at your weather app on your phone, uh,",
    "start": "169040",
    "end": "175959"
  },
  {
    "text": "it will tell you the chance of rain today is 10%, uh, for example.",
    "start": "175960",
    "end": "182370"
  },
  {
    "start": "185000",
    "end": "301000"
  },
  {
    "text": "So let us consider a list classifier. Um, so this is a classifier that produces an ordered list,",
    "start": "187750",
    "end": "194690"
  },
  {
    "text": "uh, we might call these v_top, v_2nd, and v_3rd as its best,",
    "start": "194690",
    "end": "201295"
  },
  {
    "text": "um, first, second, and third best, uh, guesses for what the true y should be,",
    "start": "201295",
    "end": "210140"
  },
  {
    "text": "what the true v should be. And of course, it may produce three guesses.",
    "start": "210140",
    "end": "215270"
  },
  {
    "text": "It may produce a top 10, or it could be a variable number. Sometimes it will know,",
    "start": "215270",
    "end": "221569"
  },
  {
    "text": "with a- with this very strong certainty that it's, uh, pedestrian and others, it may have more uncertainty,",
    "start": "221570",
    "end": "227660"
  },
  {
    "text": "and so it produces two or three or ten guesses. And so, uh, some predictors do produce",
    "start": "227660",
    "end": "234739"
  },
  {
    "text": "a variable number of guesses and some predictors always produce a fixed number. Um, and so if we're trying to,",
    "start": "234740",
    "end": "244020"
  },
  {
    "text": "uh, uh, uh, evaluate such a predictor, well, we'd be obviously happy as if the true v is the- the first choice of the predictor,",
    "start": "244020",
    "end": "254885"
  },
  {
    "text": "um, and a little bit less happy if it's the second choice, and so on.",
    "start": "254885",
    "end": "259949"
  },
  {
    "text": "Um, one place you very often see, um, list predictors is in a recommendation system.",
    "start": "260650",
    "end": "268669"
  },
  {
    "text": "So, um, Amazon has a predictor, for example, that looks at, uh, you know,",
    "start": "268670",
    "end": "276485"
  },
  {
    "text": "your recent browsing history and looks at your recent purchases, and tries to figure out what you're most likely to be",
    "start": "276485",
    "end": "284420"
  },
  {
    "text": "interested in for your next purchase and show you those things, and so it will figure out your top 10 list",
    "start": "284420",
    "end": "292460"
  },
  {
    "text": "of things you're most likely to buy or most likely be interested in and show you those. Uh, one way to- to make a list classifier,",
    "start": "292460",
    "end": "306540"
  },
  {
    "start": "301000",
    "end": "365000"
  },
  {
    "text": "um, is to, uh, use, uh, on embedding that we've seen so far.",
    "start": "306540",
    "end": "312995"
  },
  {
    "text": "So for example, we've used nearest neighbor on embedding to construct a classifier,",
    "start": "312995",
    "end": "319385"
  },
  {
    "text": "we have a prediction algorithm that produces a y-hat,",
    "start": "319385",
    "end": "324795"
  },
  {
    "text": "we have a bunch of different representatives embedded in R_m Psi_1 through Psi_k,",
    "start": "324795",
    "end": "331000"
  },
  {
    "text": "and our predictor so far has always just returned for us the representative which is closest to y-hat,",
    "start": "331000",
    "end": "340605"
  },
  {
    "text": "but it could equally, well, the return for us, the three closest representatives to y-hat,",
    "start": "340605",
    "end": "347705"
  },
  {
    "text": "and those would be the three, um, our best predictions, and the closest one would be the top guess,",
    "start": "347705",
    "end": "354980"
  },
  {
    "text": "the second closest one would be the second guess, and so on.",
    "start": "354980",
    "end": "358830"
  },
  {
    "start": "365000",
    "end": "557000"
  },
  {
    "text": "So we're going to spend some time in the next few sections talking about probabilistic methods,",
    "start": "367130",
    "end": "375180"
  },
  {
    "text": "and so we really need to start looking at probability in earnest.",
    "start": "375180",
    "end": "380289"
  },
  {
    "text": "So this is where the, uh, co-requisite in this class of knowing about",
    "start": "380289",
    "end": "386169"
  },
  {
    "text": "probability is going to start to come into play. And we're not going to use a great deal of probability,",
    "start": "386170",
    "end": "393159"
  },
  {
    "text": "but we will certainly use some. And so just a brief review,",
    "start": "393160",
    "end": "398670"
  },
  {
    "text": "right here or probability, uh, probability distribution on our target set V, it's a function.",
    "start": "398670",
    "end": "406650"
  },
  {
    "text": "Uh, for each element of our target set, it gives us a real number, and those real numbers have to be non-negative and add up to one,",
    "start": "406650",
    "end": "415080"
  },
  {
    "text": "and those are the probabilities associated with those elements of the target set V.",
    "start": "415080",
    "end": "422039"
  },
  {
    "text": "So p of v is the probability of the value V. Um,",
    "start": "422040",
    "end": "430120"
  },
  {
    "text": "so for example, if the target set is rain and shine, we might have p of rain is 0.15 and p of shine is 0.85.",
    "start": "430120",
    "end": "439055"
  },
  {
    "text": "If our predictor is predicting a probability that it will rain of 15% and the probability that it will not rain of 85%.",
    "start": "439055",
    "end": "449604"
  },
  {
    "text": "Um, if we have, uh, uh, a probability distribution like this,",
    "start": "449605",
    "end": "457250"
  },
  {
    "text": "well, we have, uh, numbered the entries in the target set V as v_1 through v_K,",
    "start": "457250",
    "end": "466135"
  },
  {
    "text": "K categories or K classes, or will then we've also, I've got,",
    "start": "466135",
    "end": "474070"
  },
  {
    "text": "uh, p v_i, which is the probability of v_i we can think about as",
    "start": "474440",
    "end": "479840"
  },
  {
    "text": "the ith entry in a probability distribution vector.",
    "start": "479840",
    "end": "484885"
  },
  {
    "text": "And so our probability distribution assigns a number to each of those K classes,",
    "start": "484885",
    "end": "491690"
  },
  {
    "text": "and so we've actually got K numbers, which form give us a K-dimensional vector.",
    "start": "491690",
    "end": "498395"
  },
  {
    "text": "And so in vector notation, we might say that p is greater than or equal to 0, and what that means is that,",
    "start": "498395",
    "end": "504740"
  },
  {
    "text": "that inequality should be interpreted element-wise, all of the entries p_i of p as a vector are non-negative,",
    "start": "504740",
    "end": "514349"
  },
  {
    "text": "and the fact that the- because this is a probability distribution, the entries have to sum up to 1.",
    "start": "514350",
    "end": "521074"
  },
  {
    "text": "We can express that as 1 transpose p is equal to 1, where the one on the left there is a vector of all 1s,",
    "start": "521075",
    "end": "530558"
  },
  {
    "text": "and so that's simply saying that the entries have to sum up to 1. So we can either think about our probability distributions as",
    "start": "530559",
    "end": "537769"
  },
  {
    "text": "functions which map script v to the reals, or we can think about them as K-dimensional vectors.",
    "start": "537770",
    "end": "545810"
  },
  {
    "text": "Both of those are convenient, um, and we will make use of both choices of notation.",
    "start": "545810",
    "end": "554670"
  },
  {
    "text": "A probabilistic classifier produces a probability distribution p-hat on V, given u.",
    "start": "559430",
    "end": "566640"
  },
  {
    "text": "So we have a classifier, it takes in the input u, and it returns for us a prediction,",
    "start": "566640",
    "end": "574440"
  },
  {
    "text": "and we see that prediction being y-hat, um,",
    "start": "574440",
    "end": "579840"
  },
  {
    "text": "embedded value of p, or instead of it being V-hat,",
    "start": "579840",
    "end": "586364"
  },
  {
    "text": "a prediction of which class the target variable lies in.",
    "start": "586364",
    "end": "593865"
  },
  {
    "text": "It produces as p-hat, and so the hat here indicates prediction. P-hat is a probability distribution on",
    "start": "593865",
    "end": "602655"
  },
  {
    "text": "V. So it's a number associated with each of the different classes, which I think- which, uh,",
    "start": "602655",
    "end": "608954"
  },
  {
    "text": "informs us how likely the classifier",
    "start": "608955",
    "end": "614070"
  },
  {
    "text": "thinks each one of the different classes is to correspond to that particular u.",
    "start": "614070",
    "end": "621615"
  },
  {
    "text": "Otherwise, this is p-hat is G of u. This is capital G,",
    "start": "621615",
    "end": "627915"
  },
  {
    "text": "so the capital G is used in the same way we used it before,",
    "start": "627915",
    "end": "633360"
  },
  {
    "text": "in that we use letter G to indicate, uh, predictor that's taking input x and producing an output y,",
    "start": "633360",
    "end": "642225"
  },
  {
    "text": "and a big G to use- to indicate, uh, a predictor that's un-embedded,",
    "start": "642225",
    "end": "649575"
  },
  {
    "text": "that's composed with the embedding and un-embedding operations to, uh, take an input u and produce an output V. And here,",
    "start": "649575",
    "end": "659730"
  },
  {
    "text": "because we're dealing with probabilistic classification, we're using G to indicate a predictor",
    "start": "659730",
    "end": "666735"
  },
  {
    "text": "that takes in an input u and returns for us an output,",
    "start": "666735",
    "end": "671940"
  },
  {
    "text": "which is a probability distribution on V. So when you read this,",
    "start": "671940",
    "end": "678930"
  },
  {
    "text": "you should keep in mind that we've got something a little tricky going on here.",
    "start": "678930",
    "end": "684765"
  },
  {
    "text": "G is a function and it's returning a distribution which is itself a function.",
    "start": "684765",
    "end": "690285"
  },
  {
    "text": "So we've a function that returns a function. We can call the function.",
    "start": "690285",
    "end": "696959"
  },
  {
    "text": "So we get p-hat is g of u, and then I can take p-hat and evaluate it at a particular class VI.",
    "start": "696960",
    "end": "704700"
  },
  {
    "text": "And I can take p-hat of VI. And that will give us the probability that V is VI when the independent variable is U.",
    "start": "704700",
    "end": "713710"
  },
  {
    "text": "Of course, I could just write that all once. So I could say G of u evaluated at VI,",
    "start": "713840",
    "end": "721214"
  },
  {
    "text": "which I write as G parentheses, u parentheses VI. Um, and so here,",
    "start": "721215",
    "end": "728850"
  },
  {
    "text": "um, of course this is totally fine. Uh, we're used to, uh, uh, functions",
    "start": "728850",
    "end": "735090"
  },
  {
    "text": "returning concrete objects such as numbers and vectors and matrices,",
    "start": "735090",
    "end": "740205"
  },
  {
    "text": "and here we've got a function that's returning a more abstract object, which is a function.",
    "start": "740205",
    "end": "746190"
  },
  {
    "text": "Um, and, uh, modern programming languages such as Julia can happily do this.",
    "start": "746190",
    "end": "752460"
  },
  {
    "text": "Uh, we've seen this already when we pass around loss functions in Julia.",
    "start": "752460",
    "end": "758325"
  },
  {
    "text": "And so here, we're going to pass around probability distributions which are just functions.",
    "start": "758325",
    "end": "763450"
  },
  {
    "text": "Um, now in some sense,",
    "start": "766280",
    "end": "772950"
  },
  {
    "start": "768000",
    "end": "949000"
  },
  {
    "text": "the point classifier and probabilistic classifiers are related to each other.",
    "start": "772950",
    "end": "779160"
  },
  {
    "text": "Um, we can say, well, for example, a point classifier as a special case of a probabilistic classifier.",
    "start": "779160",
    "end": "785070"
  },
  {
    "text": "It's a probabilistic classifier that's saying the probability of a particular target is 100%,",
    "start": "785070",
    "end": "791865"
  },
  {
    "text": "and the probability of all the other classes is 0. So we could construct such a thing, we would have p-hat of V being 1.",
    "start": "791865",
    "end": "801480"
  },
  {
    "text": "If V is p-hat and 0 otherwise. Um, and that would be a distribution that corresponds to",
    "start": "801480",
    "end": "809745"
  },
  {
    "text": "100%  certainty that the guess is V-hat and 0% certainty,",
    "start": "809745",
    "end": "816525"
  },
  {
    "text": "for the others, 0% probability for the others. Um, and typically this is not what you want, of course.",
    "start": "816525",
    "end": "822930"
  },
  {
    "text": "Yes, it's a probabilistic classify. You've taken a point classifier and constructed a probabilistic classifier but you haven't gotten any more information out of this.",
    "start": "822930",
    "end": "831670"
  },
  {
    "text": "You can also go the other way. Um, you can, if you've got a probabilistic classifier,",
    "start": "834830",
    "end": "840615"
  },
  {
    "text": "you can construct a point classifier. So what you do is you say, well, my probabilistic classifier gave me a probability distribution, p-hat.",
    "start": "840615",
    "end": "848595"
  },
  {
    "text": "Let me look, ah, at the roles of the V's for the one that's most likely the one which maximizes p-hat of V.",
    "start": "848595",
    "end": "856920"
  },
  {
    "text": "And that would be a way of translating a probabilistic prediction into a deterministic prediction,",
    "start": "856920",
    "end": "867524"
  },
  {
    "text": "into a very specific prediction which is actionable. Um, ah, so for example,",
    "start": "867525",
    "end": "874274"
  },
  {
    "text": "if you, ah, at a predictor of the weather and it's turning into the probability of rain.",
    "start": "874275",
    "end": "882435"
  },
  {
    "text": "Some point you have to make that actionable. You have to decide am I going to carry an umbrella outside or not?",
    "start": "882435",
    "end": "889695"
  },
  {
    "text": "And so one way to do that, it would be to say, is it more likely that it's going to rain",
    "start": "889695",
    "end": "895004"
  },
  {
    "text": "or that it's going to shine, and it's not going to rain. And if it's more likely that it's going to rain,",
    "start": "895005",
    "end": "900195"
  },
  {
    "text": "then you carry an umbrella. If it's more likely that it's going to be sunny, then you don't carry your umbrella.",
    "start": "900195",
    "end": "905355"
  },
  {
    "text": "And of course, there are many ways of translating probabili- probability distributions into actions.",
    "start": "905355",
    "end": "912870"
  },
  {
    "text": "And this is just one way of doing it. This particular method of translating",
    "start": "912870",
    "end": "920490"
  },
  {
    "text": "a probabilistic prediction into a point prediction is called maximum likelihood.",
    "start": "920490",
    "end": "926685"
  },
  {
    "text": "We look for the class that's most likely that it has highest probability.",
    "start": "926685",
    "end": "935290"
  },
  {
    "text": "We can also generate a list classifier from a probabilistic classifier simply by giving the value sorted by a probability.",
    "start": "939110",
    "end": "948399"
  },
  {
    "start": "949000",
    "end": "1026000"
  },
  {
    "text": "There are several different ways of constructing a probabilistic classifier. Um, all of the ones that we've seen so far for point classifiers can be extended.",
    "start": "951470",
    "end": "962460"
  },
  {
    "text": "So one can have a tree based -tree based probabilistic classifier, which is a decision tree,",
    "start": "962460",
    "end": "968205"
  },
  {
    "text": "it has nodes labeled by feature and threshold value, and the leaves contain distributions rather than point predictions.",
    "start": "968205",
    "end": "977204"
  },
  {
    "text": "You can have a nearest neighbor probabilistic classifier. And the way that works is then you have, uh, uh, predicted.",
    "start": "977204",
    "end": "986894"
  },
  {
    "text": "Uh, if you have a query point x, then you look for the k nearest neighbors 2x.",
    "start": "986895",
    "end": "994605"
  },
  {
    "text": "So the x sub i's in your data set which are the closest to x,",
    "start": "994605",
    "end": "999959"
  },
  {
    "text": "and then amongst those, you look at the distribution of VIs that you have.",
    "start": "999960",
    "end": "1005615"
  },
  {
    "text": "So if three of them are true and five of them are false, then one would predict a probability of three-fifths,",
    "start": "1005615",
    "end": "1012395"
  },
  {
    "text": "for the corresponding target variable at the point x. And you can also do probabilistic classifiers which are based",
    "start": "1012395",
    "end": "1020149"
  },
  {
    "text": "on linear predictors or based on neural network predictors, and we will come to those shortly.",
    "start": "1020150",
    "end": "1026400"
  },
  {
    "start": "1026000",
    "end": "1153000"
  },
  {
    "text": "So here we have a nearest neighbor classifier. Um, we've embedded the u's in r2 to give us two-dimensional xs.",
    "start": "1027520",
    "end": "1039305"
  },
  {
    "text": "Given a query point, uh, we would look at the corresponding point in the plane.",
    "start": "1039305",
    "end": "1044525"
  },
  {
    "text": "And we would like to predict either red or blue for the corresponding class.",
    "start": "1044525",
    "end": "1051200"
  },
  {
    "text": "The way we do it is here, we look at the k nearest neighbors of a query point.",
    "start": "1051200",
    "end": "1061355"
  },
  {
    "text": "So here k is being chosen to be 8. So for any given query point we look at the eight nearest neighbors.",
    "start": "1061355",
    "end": "1066890"
  },
  {
    "text": "Let's look at an example. Um, so we'll have a query point,",
    "start": "1066890",
    "end": "1077120"
  },
  {
    "text": "[NOISE] say right here. And we'll draw a circle that contains eight neighbors,",
    "start": "1077120",
    "end": "1086660"
  },
  {
    "text": "which I would guess is something like that,",
    "start": "1086660",
    "end": "1092660"
  },
  {
    "text": "and then of those eight neighbors, we will count how many of them are red and how many of them are blue,",
    "start": "1092660",
    "end": "1098405"
  },
  {
    "text": "and that will give us the prediction probability distribution, the empirical distribution that we're seeing amongst the nearest eight neighbors.",
    "start": "1098405",
    "end": "1108080"
  },
  {
    "text": "And so here the colors in this plot are chosen accordingly.",
    "start": "1108080",
    "end": "1113195"
  },
  {
    "text": "So if the prediction is blue, 100%, red, 0%,",
    "start": "1113195",
    "end": "1120220"
  },
  {
    "text": "then that's this color which is dark blue. Um, if the prediction is red, 100%, blue, 0%,",
    "start": "1120220",
    "end": "1129395"
  },
  {
    "text": "that's this orange color, and of course, um, the colors in between vary.",
    "start": "1129395",
    "end": "1135600"
  },
  {
    "text": "And maybe we're only predicting one probability here because once we put our prediction probability for blue,",
    "start": "1135600",
    "end": "1142000"
  },
  {
    "text": "the prediction probability for red is 1 minus the probability for blue.",
    "start": "1142000",
    "end": "1152495"
  },
  {
    "text": "So now we want to consider, well,",
    "start": "1152495",
    "end": "1157955"
  },
  {
    "start": "1153000",
    "end": "1301000"
  },
  {
    "text": "suppose we've got a probabilistic classifier or a list classifier, how are we going to evaluate how good they are?",
    "start": "1157955",
    "end": "1164750"
  },
  {
    "text": "Um, and there are several different ways of doing this. It's not quite as simple as it is for a point classifier.",
    "start": "1164750",
    "end": "1174335"
  },
  {
    "text": "Uh, if we try to judge a list classifier, uh, then uh,",
    "start": "1174335",
    "end": "1180154"
  },
  {
    "text": "we may look at the error rate as a function of the list rank.",
    "start": "1180155",
    "end": "1186965"
  },
  {
    "text": "So, for example, we look- we have a test data se- set, we have a true value of v and we have our predictor is giving us- is giving us,",
    "start": "1186965",
    "end": "1198600"
  },
  {
    "text": "uh, v_top v_2nd and v_3rd as a- as its guesses.",
    "start": "1198730",
    "end": "1204740"
  },
  {
    "text": "And- and then we ask ourselves, well, then how many of our- o- of the te- how many points in the test set did",
    "start": "1204740",
    "end": "1213290"
  },
  {
    "text": "the predictor get v_top to be the actual true v?",
    "start": "1213290",
    "end": "1220805"
  },
  {
    "text": "And we might find that's 68% of the samples. The v_top was actually the true v. And then we could say, well,",
    "start": "1220805",
    "end": "1229730"
  },
  {
    "text": "how many of the test set points was the true value among the top two guesses?",
    "start": "1229730",
    "end": "1236615"
  },
  {
    "text": "And that might be- has to be higher than 68%, that might be 79%, and amongst the top three guesses it might be 85%.",
    "start": "1236615",
    "end": "1244415"
  },
  {
    "text": "And so, if you want just one number, you might decide, well, I'm really concerned that it be in the top 3.",
    "start": "1244415",
    "end": "1252230"
  },
  {
    "text": "As long as it's in the top 3 I'm happy, and, uh, in which case 85% might be, uh, reasonable accuracy.",
    "start": "1252230",
    "end": "1260645"
  },
  {
    "text": "Another way to do it might be to score the test data set.",
    "start": "1260645",
    "end": "1267800"
  },
  {
    "text": "So for every, uh, data point v_i in the test data set,",
    "start": "1267800",
    "end": "1275270"
  },
  {
    "text": "we look whether it was the top guess, in which case it gets three points.",
    "start": "1275270",
    "end": "1281435"
  },
  {
    "text": "The second best guess in which case it gets two points, also a best guess in which case it gets one point.",
    "start": "1281435",
    "end": "1288170"
  },
  {
    "text": "And then we add up the scores over the entire test set, and that's a measure of how good our job,",
    "start": "1288170",
    "end": "1293210"
  },
  {
    "text": "our list classifier there.",
    "start": "1293210",
    "end": "1295950"
  },
  {
    "start": "1301000",
    "end": "1403000"
  },
  {
    "text": "Now to judge our probabilistic classifier, that's a- a reasonably subtle idea.",
    "start": "1302020",
    "end": "1309080"
  },
  {
    "text": "Um, what do we have? Well, we have a data set with u's and v's in it,",
    "start": "1309080",
    "end": "1315910"
  },
  {
    "text": "and then for each one of those data points, we have a prediction p-hat.",
    "start": "1315910",
    "end": "1322500"
  },
  {
    "text": "And so, um, uh, so for each data point, uh,",
    "start": "1322500",
    "end": "1328140"
  },
  {
    "text": "we have a true value, the v, and we have a prediction which is",
    "start": "1328140",
    "end": "1333770"
  },
  {
    "text": "a probability distribution p-hat over the set of possible v's.",
    "start": "1333770",
    "end": "1339020"
  },
  {
    "text": "And one way to say whether this is a- a good prediction or not would be to say,",
    "start": "1339020",
    "end": "1344690"
  },
  {
    "text": "\"Let's look at the p-hat that we got and see whether it gives a high probability to the actual value that happened,",
    "start": "1344690",
    "end": "1353765"
  },
  {
    "text": "v.\" So, for example, if we've got a- a prediction of rain and, um,",
    "start": "1353765",
    "end": "1361610"
  },
  {
    "text": "we look back at historical data and we look at the predictions of rain based on the weather the day before and",
    "start": "1361610",
    "end": "1368480"
  },
  {
    "text": "we know from whether it did rain or not, well, what we'd like to see is that on days when it rained,",
    "start": "1368480",
    "end": "1374645"
  },
  {
    "text": "the probability predicted for rain was large, and on days when it didn't rain,",
    "start": "1374645",
    "end": "1380405"
  },
  {
    "text": "we'd like the probability predicted for rain to be small. And if we see something like that, we might say,",
    "start": "1380405",
    "end": "1386240"
  },
  {
    "text": "\"Well, that's a- a good predictor.\" Now we go to make this idea formal.",
    "start": "1386240",
    "end": "1393395"
  },
  {
    "text": "Um, uh, the most common way of formalizing this idea is to use what's called a log-likelihood.",
    "start": "1393395",
    "end": "1401010"
  },
  {
    "start": "1403000",
    "end": "1604000"
  },
  {
    "text": "So we have a data set u_1 to u_n, v_1 to v_n.",
    "start": "1404350",
    "end": "1409655"
  },
  {
    "text": "And, um, we're going to have a prediction at each of those u_i's,",
    "start": "1409655",
    "end": "1416495"
  },
  {
    "text": "which we'll call p-hat_i. It's the output of our predictor when we feed u_i in.",
    "start": "1416495",
    "end": "1422645"
  },
  {
    "text": "And it's a probability distribution over the v's. Now, if this were the true distribution of-",
    "start": "1422645",
    "end": "1433985"
  },
  {
    "text": "so p-hat_i was the true probability distribution that the v_i's- that v_i was generated according to,",
    "start": "1433985",
    "end": "1442025"
  },
  {
    "text": "then we could ask ourselves, what is the probability of seeing that sequence v_1 through v_n?",
    "start": "1442025",
    "end": "1449360"
  },
  {
    "text": "So if i is 1, the probability of getting the particular v that we got",
    "start": "1449360",
    "end": "1456279"
  },
  {
    "text": "is given by evaluating p-hat_1 at that particular v, which is just v_1.",
    "start": "1456280",
    "end": "1462190"
  },
  {
    "text": "Similarly for v- for i is equal to 2, we have to evaluate p-hat_2 at v_2.",
    "start": "1462190",
    "end": "1469870"
  },
  {
    "text": "And those give us the probabilities of getting those outcomes under that distribution.",
    "start": "1469870",
    "end": "1475770"
  },
  {
    "text": "If we make the probabilistic assumption that each of those data points is independent,",
    "start": "1475770",
    "end": "1484460"
  },
  {
    "text": "then the pro- probability of getting the entire data set is the product of the individual probabilities.",
    "start": "1484460",
    "end": "1491929"
  },
  {
    "text": "And so the probability of seeing that entire data set under those distributions,",
    "start": "1491930",
    "end": "1496955"
  },
  {
    "text": "p-hat _1 through p-hat_n is simply the product from i is 1 to n of p-hat_i evaluated at v_i.",
    "start": "1496955",
    "end": "1504720"
  },
  {
    "text": "So this is just a probability. It says well, you've got some distribution, and what's the probability of getting that particular outcome?",
    "start": "1505390",
    "end": "1515720"
  },
  {
    "text": "And we can look at it as, well, just the probability of getting the outcome, but we can also look at it another way and look at it as",
    "start": "1515720",
    "end": "1524210"
  },
  {
    "text": "a measure of the distributions p-hat_1 through p-hat_n.",
    "start": "1524210",
    "end": "1530720"
  },
  {
    "text": "And when we do that, instead of calling it the probability of getting v's,",
    "start": "1530720",
    "end": "1536000"
  },
  {
    "text": "we call it the likelihood of p-hats being correct.",
    "start": "1536000",
    "end": "1541235"
  },
  {
    "text": "So the likelihood is simply another name for the probability,",
    "start": "1541235",
    "end": "1546500"
  },
  {
    "text": "but we call it the probability when we're looking at the probability of outcomes,",
    "start": "1546500",
    "end": "1551675"
  },
  {
    "text": "and we look- call it the likelihood when we're talking about the likelihood of probability distributions.",
    "start": "1551675",
    "end": "1558005"
  },
  {
    "text": "And so we'd like this quantity, the probability or the likelihood to be large.",
    "start": "1558005",
    "end": "1565370"
  },
  {
    "text": "And this is a fundamental measure of how well the predicted distribution matches the data.",
    "start": "1565370",
    "end": "1572540"
  },
  {
    "text": "Just as in our rain versus sun example, if the probability of rain is large on days when it did rain,",
    "start": "1572540",
    "end": "1580070"
  },
  {
    "text": "that's a good predictor. So we can compare two different classifiers by looking at their associated likelihoods,",
    "start": "1580070",
    "end": "1590000"
  },
  {
    "text": "which one has the largest probability value, their- the- the largest likelihood.",
    "start": "1590000",
    "end": "1595505"
  },
  {
    "text": "The one with the largest likelihood we would consider the better classifier.",
    "start": "1595505",
    "end": "1601080"
  },
  {
    "start": "1604000",
    "end": "1915000"
  },
  {
    "text": "Now it's actually more convenient to work with log probabilities rather than purely probabilities.",
    "start": "1605080",
    "end": "1613370"
  },
  {
    "text": "Now one reason for that is that the likelihood is the product and when we take the log, we're going to get a sum.",
    "start": "1613370",
    "end": "1618470"
  },
  {
    "text": "Uh, there are some other reasons for that as well, which we will see. Um, so what we work with is actually the negative log-likelihood.",
    "start": "1618470",
    "end": "1627500"
  },
  {
    "text": "That's simply the negative log of the probability of getting v_1 through v_n under our predicted distributions p-hat_1 through p-hat_n.",
    "start": "1627500",
    "end": "1637505"
  },
  {
    "text": "That's the negative log of the product of the p-hat_i's evaluated to the v_ i's,",
    "start": "1637505",
    "end": "1643490"
  },
  {
    "text": "which is negative of the sum of the logs of the p_i's to v_i's.",
    "start": "1643490",
    "end": "1650480"
  },
  {
    "text": "Now, the negative log-likelihood is actually a positive quantity, um,",
    "start": "1650480",
    "end": "1655880"
  },
  {
    "text": "and we would like it to be small, eh, if the negative log is small that means the probabi- probability itself is large.",
    "start": "1655880",
    "end": "1662735"
  },
  {
    "text": "Um, one other things about this is that this is- um,",
    "start": "1662735",
    "end": "1668330"
  },
  {
    "text": "if we just look at the log of the probabilities, then that gets smaller the more data points we have.",
    "start": "1668330",
    "end": "1675005"
  },
  {
    "text": "And so we need to normalize in some way, and the way we do that is we look at the average negative log-likelihood.",
    "start": "1675005",
    "end": "1681905"
  },
  {
    "text": "So that's here, L is minus 1 on n. The sum from i is 1 to n of the log of p-hat_i of v_i.",
    "start": "1681905",
    "end": "1690289"
  },
  {
    "text": "And this is a quantity that we can compare, um, the effectiveness of different classifiers on different size data sets.",
    "start": "1690290",
    "end": "1702510"
  },
  {
    "text": "So now we have uh, a very nice performance metric,",
    "start": "1711580",
    "end": "1718745"
  },
  {
    "text": "which is this average negative log likelihood. Now just as we did when we were looking at loss functions,",
    "start": "1718745",
    "end": "1726860"
  },
  {
    "text": "when we were looking at the square loss and the absolute loss, we asked ourselves the question, if we just looked at constant classifiers,",
    "start": "1726860",
    "end": "1734600"
  },
  {
    "text": "which would be the best constant classifier,",
    "start": "1734600",
    "end": "1739289"
  },
  {
    "text": "which minimizes the average value of those losses? Here we have a particular performance metric,",
    "start": "1739780",
    "end": "1747155"
  },
  {
    "text": "the average negative log likelihood. And we can ask ourselves the question, which is the best constant predictor,",
    "start": "1747155",
    "end": "1758135"
  },
  {
    "text": "the best constant probabilistic classifier that minimizes that particular performance metric?",
    "start": "1758135",
    "end": "1767850"
  },
  {
    "text": "So how is this going to work? Well, we're going to have uh,",
    "start": "1768970",
    "end": "1774470"
  },
  {
    "text": "a dataset which is just V1 through VN,",
    "start": "1774470",
    "end": "1779784"
  },
  {
    "text": "a bunch of classes. And we're going to look for a classifier that doesn't depend on the use,",
    "start": "1779785",
    "end": "1786850"
  },
  {
    "text": "so the use may not even exist. All we're going to do is try to predict to a distribution P-hat.",
    "start": "1786850",
    "end": "1794070"
  },
  {
    "text": "Um, and that distribution P-hat has to be a probability distribution on script V. So what do we do?",
    "start": "1794500",
    "end": "1804830"
  },
  {
    "text": "We're going to choose script- we're going to choose P-hat to minimize the average negative log likelihood minus 1 on N.",
    "start": "1804830",
    "end": "1812495"
  },
  {
    "text": "The sum from I is 1 to N of log of P-hat of VI. And we can choose any distribution P-hat we want.",
    "start": "1812495",
    "end": "1821150"
  },
  {
    "text": "Of course it has to be a probability distribution. So it has to be non-negative at every point V,",
    "start": "1821150",
    "end": "1827165"
  },
  {
    "text": "and it has to sum to one. Now, it turns out that the optimal constant probabilistic classifier is very nice,",
    "start": "1827165",
    "end": "1842120"
  },
  {
    "text": "it's a very sensible quantity, is actually the empirical distribution of the data.",
    "start": "1842120",
    "end": "1847575"
  },
  {
    "text": "So the empirical distribution of the data, we simply count up for each V in script V,",
    "start": "1847575",
    "end": "1856105"
  },
  {
    "text": "the fraction of the data which are equal to that particular class.",
    "start": "1856105",
    "end": "1861945"
  },
  {
    "text": "So it's simply the empirical distribution. It's simply the counts divided by",
    "start": "1861945",
    "end": "1867020"
  },
  {
    "text": "the total number of data elements we have. We'll call that Q. And then the P-hat that minimizes the average negative log likelihood is Q.",
    "start": "1867020",
    "end": "1880140"
  },
  {
    "text": "And this is- this is very nice. This is simp- this is just like when we talked about minimizing the square loss.",
    "start": "1881380",
    "end": "1889760"
  },
  {
    "text": "We found that the best constant predictor is the mean. And we talked about minimizing the absolute loss.",
    "start": "1889760",
    "end": "1896570"
  },
  {
    "text": "We found the best constant predictor is the median. Now, we're predicting probabilities and when we think about",
    "start": "1896570",
    "end": "1904715"
  },
  {
    "text": "minimizing the average negative log likelihood, the best constant predictor is the empirical distribution.",
    "start": "1904715",
    "end": "1912720"
  },
  {
    "start": "1915000",
    "end": "2258000"
  },
  {
    "text": "Now, the negative log likelihood can be computed in a particular way.",
    "start": "1917530",
    "end": "1925625"
  },
  {
    "text": "So we have the negative log likelihood is minus one on N. The sum from I is one to N of the log of P-hat VI.",
    "start": "1925625",
    "end": "1934190"
  },
  {
    "text": "Now, each of the VI's lives in one of the classes. Now, remember what the classes are.",
    "start": "1934190",
    "end": "1940760"
  },
  {
    "text": "We use this notation, V is V1 up to VK.",
    "start": "1940760",
    "end": "1947554"
  },
  {
    "text": "Notice those are subscripts to indicate the classes. And we've got the data elements which are the superscript I from I is one up to N. Now,",
    "start": "1947555",
    "end": "1958820"
  },
  {
    "text": "if I compute this quantity, the sum from I is 1 up to N of the log of P-hat of VI.",
    "start": "1958820",
    "end": "1969335"
  },
  {
    "text": "Well, I can split that sum up. I can split it up into those I's for which VI is say V1.",
    "start": "1969335",
    "end": "1980130"
  },
  {
    "text": "And then I can take the log of P-hat of VI, but all of those VI's of V1,",
    "start": "1980130",
    "end": "1986860"
  },
  {
    "text": "so I can just write that V1, and then I can do the next category, which is actually VI is V2 of the log of P-hat of V2 and so on,",
    "start": "1986860",
    "end": "2000985"
  },
  {
    "text": "up to K categories. Sum from uh, FI such that VI is VK of the log of P-hat of VK.",
    "start": "2000985",
    "end": "2014330"
  },
  {
    "text": "So by splitting up like this- I just split them up into- into uh, categories.",
    "start": "2015060",
    "end": "2023185"
  },
  {
    "text": "Notice that this in- inside here- inside each of these sums,",
    "start": "2023185",
    "end": "2028345"
  },
  {
    "text": "the- the terms don't depend on I.",
    "start": "2028345",
    "end": "2037405"
  },
  {
    "text": "So I've really got just a certain number of terms that are all the same and the number that I've got, well,",
    "start": "2037405",
    "end": "2045340"
  },
  {
    "text": "it just the- it's just the number of data points for which VI is V1 in this sum,",
    "start": "2045340",
    "end": "2051714"
  },
  {
    "text": "the number of data points which VI is V2 in this sum, and so on. So when I work out this sum and taking into account the factor of N, well,",
    "start": "2051715",
    "end": "2062320"
  },
  {
    "text": "then the fraction of terms is QVJ.",
    "start": "2062320",
    "end": "2068260"
  },
  {
    "text": "Those are then- those are- that's the fraction of the terms in this sum for which VI is equal to category VJ.",
    "start": "2068260",
    "end": "2076944"
  },
  {
    "text": "And for those- each one of those terms, the quantity log P-hat of VI,",
    "start": "2076945",
    "end": "2083485"
  },
  {
    "text": "is just log P-hat of the corresponding class VJ.",
    "start": "2083485",
    "end": "2089030"
  },
  {
    "text": "So I end up with the sum over the categories rather than a sum over the data points.",
    "start": "2089340",
    "end": "2094570"
  },
  {
    "text": "And I have that the average neg log- negative log likelihood is the sum over the categories of",
    "start": "2094570",
    "end": "2102760"
  },
  {
    "text": "the empirical distribution of each of the categories times the log of the probability of that category.",
    "start": "2102760",
    "end": "2113000"
  },
  {
    "text": "This quantity is called the cross entropy, of P-hat relative to Q,",
    "start": "2113430",
    "end": "2121120"
  },
  {
    "text": "where P-hat is the distribution that is being evaluated.",
    "start": "2121120",
    "end": "2128650"
  },
  {
    "text": "It's the distribution that's produced by the predictor. And Q is the underlying true empirical distribution of the V's.",
    "start": "2128650",
    "end": "2139840"
  },
  {
    "text": "Uh, we might write this as H. Oops. [NOISE]",
    "start": "2139840",
    "end": "2153630"
  },
  {
    "text": "H of q, p hat is equal to minus the sum from j is 1 up to k",
    "start": "2155290",
    "end": "2164375"
  },
  {
    "text": "of q_j log p hat_j using our vector notation.",
    "start": "2164375",
    "end": "2173480"
  },
  {
    "text": "Uh, there's also a- a- a related quantity when p hat and q are the same distribution.",
    "start": "2173480",
    "end": "2182825"
  },
  {
    "text": "So we'll call them both p, that quantity is called h of p. That's the sum",
    "start": "2182825",
    "end": "2191210"
  },
  {
    "text": "over k of p_k log p_k negative p_k log p_k.",
    "start": "2191210",
    "end": "2198724"
  },
  {
    "text": "That's called the entropy of a probability distribution. And both of these quantities,",
    "start": "2198725",
    "end": "2204140"
  },
  {
    "text": "the cross entropy and the entropy are- um, [NOISE] are very important, mathematically,",
    "start": "2204140",
    "end": "2211190"
  },
  {
    "text": "very important practically, um, they're used, uh, very widely in coding theory and in information theory,",
    "start": "2211190",
    "end": "2219635"
  },
  {
    "text": "and in machine learning. Um, the fundamental properties of probability distributions.",
    "start": "2219635",
    "end": "2226520"
  },
  {
    "text": "We don't need much of the theory of that here or in fact any of the theory of it here, we just want to be able to determine which",
    "start": "2226520",
    "end": "2234350"
  },
  {
    "text": "is the p hat that minimizes the average negative log likelihood.",
    "start": "2234350",
    "end": "2240410"
  },
  {
    "text": "Another way to say it is, if I've got a q, which p hat minimizes H of q,",
    "start": "2240410",
    "end": "2248675"
  },
  {
    "text": "p hat because H of q, p hat, the cross entropy, is the average negative log likelihood.",
    "start": "2248675",
    "end": "2254840"
  },
  {
    "text": "[NOISE] Uh,",
    "start": "2254840",
    "end": "2261590"
  },
  {
    "text": "there's a convenient quantity to use for this,",
    "start": "2261590",
    "end": "2266855"
  },
  {
    "text": "and that's called the Kullback-Leibler divergence of q and p. So we've got H of q,",
    "start": "2266855",
    "end": "2274160"
  },
  {
    "text": "p. We'd like to minimize that over p. But actually,",
    "start": "2274160",
    "end": "2279305"
  },
  {
    "text": "instead of looking at H of q, p, we're going to look at this thing called the Kullback-Leibler divergence,",
    "start": "2279305",
    "end": "2284450"
  },
  {
    "text": "d_KL of q_p, and that's H of q, p minus H of q minus the entropy of Q.",
    "start": "2284450",
    "end": "2292865"
  },
  {
    "text": "This is convenient for, uh, the following reason. We want to minimize H of q,",
    "start": "2292865",
    "end": "2299180"
  },
  {
    "text": "p over p. Um, but actually, well,",
    "start": "2299180",
    "end": "2306109"
  },
  {
    "text": "H of q, p is just d_KL of q,",
    "start": "2306110",
    "end": "2313810"
  },
  {
    "text": "p plus H of q. If I'm minimizing this quantity over p, well,",
    "start": "2313810",
    "end": "2321380"
  },
  {
    "text": "this a- additional term here doesn't matter because it doesn't depend on p. It just depends on q, it's just a constant.",
    "start": "2321380",
    "end": "2327875"
  },
  {
    "text": "So we're just shifting the objective function in our optimization problem.",
    "start": "2327875",
    "end": "2332990"
  },
  {
    "text": "Um, this quantity, the Kullback-Leibler divergence,",
    "start": "2332990",
    "end": "2339575"
  },
  {
    "text": "uh, has a very nice property that it's non-negative. One way to think about it is that it's a measure of",
    "start": "2339575",
    "end": "2347510"
  },
  {
    "text": "how similar two distributions- probability distributions p and q are.",
    "start": "2347510",
    "end": "2353780"
  },
  {
    "text": "And when they are the same, well, then d q, p is 0.",
    "start": "2353780",
    "end": "2360800"
  },
  {
    "text": "Because if we look back in our definition of entropy,",
    "start": "2360800",
    "end": "2366630"
  },
  {
    "text": "H of p is the cross entropy of p with",
    "start": "2367150",
    "end": "2373760"
  },
  {
    "text": "itself and so when q is equal to p,",
    "start": "2373760",
    "end": "2381455"
  },
  {
    "text": "we clearly that have the Kullback-Leibler divergence of q and p is 0.",
    "start": "2381455",
    "end": "2387380"
  },
  {
    "text": "And when q and p are not equal, well then the Kullback-Leibler divergence is always non-negative.",
    "start": "2387380",
    "end": "2394680"
  },
  {
    "text": "Er, this is, uh, easy to see, uh, if I just look at what the sum is,",
    "start": "2394750",
    "end": "2400040"
  },
  {
    "text": "d_KL of q_p is the sum over j of q_j log p_j on q_j.",
    "start": "2400040",
    "end": "2406340"
  },
  {
    "text": "So here we're using our vector notation, p_j is simply a shorthand for p of v_j.",
    "start": "2406340",
    "end": "2414904"
  },
  {
    "text": "Um, and so here I've used the fact that I've",
    "start": "2414905",
    "end": "2420125"
  },
  {
    "text": "subtracted one log from another and that's simply the log of the ratio.",
    "start": "2420125",
    "end": "2427490"
  },
  {
    "text": "Um, now the log of p_j over q_j, uh, has a nice bound.",
    "start": "2427490",
    "end": "2434540"
  },
  {
    "text": "Now the log of the quantity log, this is one, and then the logarithm looks like that.",
    "start": "2434540",
    "end": "2447120"
  },
  {
    "text": "And, uh, I can compare that with, uh, the function x minus 1.",
    "start": "2447120",
    "end": "2454654"
  },
  {
    "text": "Function x minus 1 is the tangent right there, and so the log of x is less than or equal to x minus 1.",
    "start": "2454655",
    "end": "2465330"
  },
  {
    "text": "This implies that this sum is less than or equal to- is greater than or equal to this sum.",
    "start": "2466420",
    "end": "2474125"
  },
  {
    "text": "And this sum we can expand as being equal to the sum over j of p_j,",
    "start": "2474125",
    "end": "2482620"
  },
  {
    "text": "um, minus the sum over j of q_j.",
    "start": "2482620",
    "end": "2489215"
  },
  {
    "text": "And both of those terms are equal to 1 and so because both p and q are distributions,",
    "start": "2489215",
    "end": "2496160"
  },
  {
    "text": "and so the overall sum adds up to 0. Hence the Kullback-Leibler divergence of p and q is greater than or equal to 0.",
    "start": "2496160",
    "end": "2509390"
  },
  {
    "text": "Here we used the fact that q was non-zero and when we did this division.",
    "start": "2509390",
    "end": "2514924"
  },
  {
    "text": "Um, uh, but, uh, in fact this proof can be shown to hold even if some of the q's are equal to 0.",
    "start": "2514925",
    "end": "2523500"
  },
  {
    "text": "And that tells us the best constant predictor. Um, the best, uh,",
    "start": "2529450",
    "end": "2535234"
  },
  {
    "text": "constant predictor is the- given by the p hat that minimizes the average negative log likelihood.",
    "start": "2535235",
    "end": "2543470"
  },
  {
    "text": "The average negative log likelihood is equal to the cross entropy",
    "start": "2543470",
    "end": "2549170"
  },
  {
    "text": "and so we would like to minimize H of q_p hat with respect to p hat.",
    "start": "2549170",
    "end": "2555755"
  },
  {
    "text": "Uh, we can, uh, make this term 0 by setting p hat equal to q.",
    "start": "2555755",
    "end": "2563705"
  },
  {
    "text": "And of course this is just a constant term. And so when p hat is q,",
    "start": "2563705",
    "end": "2569765"
  },
  {
    "text": "um, that's the best constant predictor, best constant predictor has",
    "start": "2569765",
    "end": "2576064"
  },
  {
    "text": "a probability distribution e- equal to the empirical distribution. The resulting, uh, cross entropy is the entropy of q.",
    "start": "2576064",
    "end": "2587280"
  },
  {
    "start": "2589000",
    "end": "2624000"
  },
  {
    "text": "So let's summarize. Uh, a point classifier makes a single guess of v given u,",
    "start": "2591400",
    "end": "2598310"
  },
  {
    "text": "whereas a probabilistic classifier guesses a probability distribution on the target set- on the target set of classes v given to u.",
    "start": "2598310",
    "end": "2608765"
  },
  {
    "text": "And we judge a probabilistic classifier by its average log likelihood on test data.",
    "start": "2608765",
    "end": "2617430"
  }
]