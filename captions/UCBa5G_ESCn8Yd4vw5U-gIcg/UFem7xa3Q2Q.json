[
  {
    "start": "0",
    "end": "5500"
  },
  {
    "text": "In theoretical physics, to\nget this kind of audience you have to win the\nNobel Prize or something. But of course, I've been\nworking on ML recently.",
    "start": "5500",
    "end": "12730"
  },
  {
    "text": "And it's been much\nmore exciting. There's a huge\namount of interest. So to some extent, part of\nwhat I'll be talking about,",
    "start": "12730",
    "end": "20320"
  },
  {
    "text": "maybe an implicit\ntheme, will be sort of why there's so\nmuch excitement and why you might expect\nthat excitement to continue.",
    "start": "20320",
    "end": "29780"
  },
  {
    "text": "So an outline of my talk\nis that I'll, first, start by discussing motivations\nfor language modeling.",
    "start": "29780",
    "end": "37510"
  },
  {
    "text": "I'm sure you're all\nvery well motivated because this an NLP class. And I'll also talk about sort\nof orders of magnitude of data",
    "start": "37510",
    "end": "45250"
  },
  {
    "text": "and compute that go into\ncontemporary language modeling. And that will kind\nof set the stage",
    "start": "45250",
    "end": "51280"
  },
  {
    "text": "for talking about scaling laws\nfor neural language modeling.",
    "start": "51280",
    "end": "56500"
  },
  {
    "text": "And a further realization\nthat these scaling laws",
    "start": "56500",
    "end": "61640"
  },
  {
    "text": "seem to be quite universal\nfor generative models and maybe for sort of machine\nlearning, more generally.",
    "start": "61640",
    "end": "67700"
  },
  {
    "text": "And then finally,\nafter discussing that, I'll talk about what\nhappens when we actually",
    "start": "67700",
    "end": "72980"
  },
  {
    "text": "do scale up language models. I'll talk about the GPT-3 model. And if there's time, I'll\ntalk about some lessons",
    "start": "72980",
    "end": "79400"
  },
  {
    "text": "from all of these\nideas for research, which I imagine many of you\nare excited to be involved",
    "start": "79400",
    "end": "85700"
  },
  {
    "text": "with soon. So I'll start by\ntalking about why we do language modeling and\nFermi estimates for language",
    "start": "85700",
    "end": "92520"
  },
  {
    "text": "modeling. By Fermi estimates, I mean,\nquestions like estimating. Is it either a million or\n100,000 or 10,000 piano tuners",
    "start": "92520",
    "end": "102979"
  },
  {
    "text": "in Chicago. Fermi famously asked\nthis kind of question. And there are a lot\nof estimates like this that we can kind of do in\na back of the envelope way",
    "start": "102980",
    "end": "108957"
  },
  {
    "text": "to really get a sense\nfor what's going on.",
    "start": "108957",
    "end": "114140"
  },
  {
    "text": "So but before going into that,\nwhy should you study language? This is sort of my motivation. You might have all sorts\nof other motivations.",
    "start": "114140",
    "end": "120737"
  },
  {
    "text": "Language is obviously\na very fascinating intellectual creation\nby our species.",
    "start": "120737",
    "end": "127612"
  },
  {
    "text": "But I think another reason why\nit's a particularly exciting for AI is that language is, in\nsome sense, our species' best",
    "start": "127612",
    "end": "133970"
  },
  {
    "text": "attempt to encode everything\nabout the world in as a efficient and compressed\nway as possible.",
    "start": "133970",
    "end": "140250"
  },
  {
    "text": "And that means that it's\nvery yielding to an AI.",
    "start": "140250",
    "end": "145400"
  },
  {
    "text": "There isn't a lot of noise. There's a lot of signal. There's, of course,\na huge quantity",
    "start": "145400",
    "end": "150620"
  },
  {
    "text": "of writing freely\navailable on the internet. And there are also a\nhuge number of books.",
    "start": "150620",
    "end": "156950"
  },
  {
    "text": "For example, I think,\nvery roughly speaking in this sort of\nFermi estimate level, there's something\nlike 10 million books",
    "start": "156950",
    "end": "163520"
  },
  {
    "text": "in the Library of Congress. And very, very roughly,\nthat might mean there's",
    "start": "163520",
    "end": "168799"
  },
  {
    "text": "something like a trillion\nwords out there in books. And then there's actually much\nmore language information out",
    "start": "168800",
    "end": "174860"
  },
  {
    "text": "on the internet. And so there's,\ntherefore, a lot of data for AI models to learn from.",
    "start": "174860",
    "end": "181580"
  },
  {
    "text": "And then a third reason, at\nleast, to some extent for me and maybe for many of you,\nis that if you're actually",
    "start": "181580",
    "end": "187400"
  },
  {
    "text": "able to get an AI that kind\nof knows quote, unquote, \"understands\" language, then\nyou can communicate with it",
    "start": "187400",
    "end": "195170"
  },
  {
    "text": "in a kind of natural way. You can ask it about anything. And you can get a lot of\nintuition from the responses",
    "start": "195170",
    "end": "201380"
  },
  {
    "text": "and behaviors, and all sorts of\ndifferent kinds of evaluations, you can perform on such a model.",
    "start": "201380",
    "end": "207830"
  },
  {
    "text": "If you compare it to sort\nof ancient history of AI like excitement about\nclassifying images",
    "start": "207830",
    "end": "214430"
  },
  {
    "text": "from, I don't know, AlexNet\n10 years ago in sort of the distant past\nand from say, AlphaGo,",
    "start": "214430",
    "end": "222050"
  },
  {
    "text": "again, in the distant\npast, five years ago, there's a lot more\nintuition you can get. And that you can use that\nto sort of understand",
    "start": "222050",
    "end": "229280"
  },
  {
    "text": "what these models know\nand don't know and can do. And you can also\nthink about this in terms of how to make\nthese models aligned",
    "start": "229280",
    "end": "238519"
  },
  {
    "text": "with what humans prefer. There's a lot of work on trying\nto understand language model",
    "start": "238520",
    "end": "245629"
  },
  {
    "text": "bias, racism, other such issues. And there's really a lot that\nyou can kind of explore and dig",
    "start": "245630",
    "end": "251060"
  },
  {
    "text": "into. So I imagine this is all\nvery basic for everyone here, but just so we're\non the same page,",
    "start": "251060",
    "end": "258140"
  },
  {
    "text": "if you're doing kind of\ncontemporary neural network based machine learning,\nthe ingredients that you need to get started\nare really surprisingly simple.",
    "start": "258140",
    "end": "265970"
  },
  {
    "text": "You need some kind of model\nto parametarize a function. You need a data set.",
    "start": "265970",
    "end": "272540"
  },
  {
    "text": "You need some computers\nwith plenty of computation. You need a loss function and you\nneed some choice of optimizer.",
    "start": "272540",
    "end": "280940"
  },
  {
    "text": "And basically, for pretty\nmuch everything in this talk, I'll be thinking about\nlanguage modeling",
    "start": "280940",
    "end": "287480"
  },
  {
    "text": "as a task, where the\nloss function is simply to predict the next word in some\nsentence or paragraph or book.",
    "start": "287480",
    "end": "296660"
  },
  {
    "text": "And so that's how,\nbasically, all of the models that I'll be\ntalking about are trained. They have a loss function,\nwhich incentivizes",
    "start": "296660",
    "end": "302705"
  },
  {
    "text": "them to predict the\ncorrect probability distribution for the next word.",
    "start": "302705",
    "end": "308570"
  },
  {
    "text": "But what about these other\ningredients like the models that we use, the data\nsets that we use,",
    "start": "308570",
    "end": "314060"
  },
  {
    "text": "and how much\ncomputation do we use. What are those sort of\norder of magnitude figures?",
    "start": "314060",
    "end": "322050"
  },
  {
    "text": "So one way to\nthink about this is sort of how much\nlanguage do we consume",
    "start": "322050",
    "end": "327210"
  },
  {
    "text": "as a person for comparison? So you can imagine that if you\nwere a very voracious reader,",
    "start": "327210",
    "end": "333569"
  },
  {
    "text": "maybe you would read\na long book every day. And you'd spend your life\ndoing that, maybe you'd",
    "start": "333570",
    "end": "340050"
  },
  {
    "text": "live for 70 years. If you did that, you'd\nend up reading something like 2 billion words\nover your lifetime.",
    "start": "340050",
    "end": "348060"
  },
  {
    "text": "For comparison, a canonical\nlarge language model GPT-3",
    "start": "348060",
    "end": "353460"
  },
  {
    "text": "was trained for on the\norder of 200 billion words. So that's about 100\ntimes more language data",
    "start": "353460",
    "end": "361950"
  },
  {
    "text": "than maybe you'd\nsee in your lifetime if you kind of tried really\nhard to attend to written text.",
    "start": "361950",
    "end": "369767"
  },
  {
    "text": "There are other data\nsets, of course, that are much, much bigger\nthan GPT-3's training set.",
    "start": "369768",
    "end": "377850"
  },
  {
    "text": "There's Common Crawl,\nwhich is a sort of snapshot of the internet that anyone\ncan go out and download",
    "start": "377850",
    "end": "384240"
  },
  {
    "text": "if you like. And this has very roughly on\nthe order of 10 to the 15 words.",
    "start": "384240",
    "end": "391300"
  },
  {
    "text": "I said earlier that\nthe Library of Congress has something like\nmaybe 10 million books.",
    "start": "391300",
    "end": "397509"
  },
  {
    "text": "Each book is maybe\n100,000 words. So the Library of\nCongress, in total, maybe has something\nlike a trillion words.",
    "start": "397510",
    "end": "405250"
  },
  {
    "text": "And as another sort of\nsmaller data set example, English Wikipedia is\nvery roughly of order",
    "start": "405250",
    "end": "412840"
  },
  {
    "text": "3 billion words. So maybe if you spent your\nwhole life reading Wikipedia, you could just barely do it\nif that was your mission.",
    "start": "412840",
    "end": "420460"
  },
  {
    "text": " So what about the\nactual neural networks",
    "start": "420460",
    "end": "427009"
  },
  {
    "text": "that I'll be talking\nabout, that we currently seem to be using fairly\neffectively to model language?",
    "start": "427010",
    "end": "434250"
  },
  {
    "text": "So I'll be talking about\ntransformer language models, so-called decoder-only\ntransformer language models",
    "start": "434250",
    "end": "440630"
  },
  {
    "text": "of which GPT-3 is an example. And just to sort of prep\nthings, these models",
    "start": "440630",
    "end": "446810"
  },
  {
    "text": "have, with kind of the standard\nway that they're set up, a number of parameters,\nwhich is something",
    "start": "446810",
    "end": "452240"
  },
  {
    "text": "like 12 times the number\nof layers in the network. So GPT-3 has 96 layers.",
    "start": "452240",
    "end": "458840"
  },
  {
    "text": "You can make deeper or\nshallower such networks, times the sort of activation\ndimension squared.",
    "start": "458840",
    "end": "466225"
  },
  {
    "text": " So d model, this\nd model parameter is just the dimension\nof the vector space",
    "start": "466225",
    "end": "474710"
  },
  {
    "text": "that each token\noccupies, or word if you were to use words\nas tokens when you run",
    "start": "474710",
    "end": "482150"
  },
  {
    "text": "this model on language data. And so this gives you\nsome sense for where",
    "start": "482150",
    "end": "488510"
  },
  {
    "text": "parameter counts come from. I think d model for GPT-3 is of\norder 10,000 and n layer is 96.",
    "start": "488510",
    "end": "494659"
  },
  {
    "text": "And that's how you get\nroughly 200 billion parameters in that model and other\nmodels scale similarly.",
    "start": "494660",
    "end": "502070"
  },
  {
    "text": "Now, how much computation\ndo you actually do when you train\nthis kind of model?",
    "start": "502070",
    "end": "508850"
  },
  {
    "text": "Well, it turns out that\ndifferent neural network architectures have different\nproperties with respect",
    "start": "508850",
    "end": "514520"
  },
  {
    "text": "to this question, but\ntransformers are actually quite simple. In a forward pass\nof a transformer,",
    "start": "514520",
    "end": "522890"
  },
  {
    "text": "every parameter on every\ntoken performs roughly one add and one multiply.",
    "start": "522890",
    "end": "529160"
  },
  {
    "text": "And then about twice this\nin the backward pass. And so that gives us\na very simple formula",
    "start": "529160",
    "end": "534980"
  },
  {
    "text": "that the number\nof floating point operations that a model like\nthis performs during training",
    "start": "534980",
    "end": "540680"
  },
  {
    "text": "is 6, which is 2 times 1 plus 2,\ntimes the number of parameters",
    "start": "540680",
    "end": "547010"
  },
  {
    "text": "in the model, times the number\nof tokens, that's what d is, sort of the size of the data\nset in tokens that you process.",
    "start": "547010",
    "end": "554888"
  },
  {
    "text": "And one other point\nthat sort of I'll make while kind of going\nover these estimates",
    "start": "554888",
    "end": "562010"
  },
  {
    "text": "is that you might\nwonder whether or not there's a lot of\ncomputation involved",
    "start": "562010",
    "end": "567110"
  },
  {
    "text": "in processing long sequences. There's a famous point that\ndense attention in transformer",
    "start": "567110",
    "end": "575209"
  },
  {
    "text": "models is n squared with\nrespect to context length, and that's absolutely true.",
    "start": "575210",
    "end": "581160"
  },
  {
    "text": "However, if you actually work\nout the sort of coefficients, the ratio of the\namount of computation",
    "start": "581160",
    "end": "588019"
  },
  {
    "text": "you do in a forward pass or\nduring training in the context direction versus in\nthe direction of sort",
    "start": "588020",
    "end": "595399"
  },
  {
    "text": "of moving up the\nlayers of the model is roughly n context\nover 12 times d model.",
    "start": "595400",
    "end": "602240"
  },
  {
    "text": "So I note this\njust because if you think, which I'll kind\nof suggest that this",
    "start": "602240",
    "end": "609740"
  },
  {
    "text": "is a likely direction for\nthe world to be heading, that models might\ncontinue to get bigger, then d model for GPT-3\nis already 10,000.",
    "start": "609740",
    "end": "618649"
  },
  {
    "text": "So the denominator here\nis order of 100,000. And so actually, even if\nyou have quite long contexts",
    "start": "618650",
    "end": "624710"
  },
  {
    "text": "with the sort of dumbest\npossible dense attention, the amount of\ncompute you actually do in the context direction\nis not always so much.",
    "start": "624710",
    "end": "631730"
  },
  {
    "text": " So what about actually numerical\nvalues for this compute?",
    "start": "631730",
    "end": "640290"
  },
  {
    "text": "So the largest\nmodels that we have so far if we're in kind\nof Fermi estimate mode, we can round up and\nsay, they have say",
    "start": "640290",
    "end": "646279"
  },
  {
    "text": "order of a trillion parameters. If you have a model with a\ntrillion parameters, then",
    "start": "646280",
    "end": "654168"
  },
  {
    "text": "what kind of hardware are\nyou going to run it on? Well, you might run it on an\nA100 GPU, at least, this year.",
    "start": "654168",
    "end": "660380"
  },
  {
    "text": "And A100 GPUs perform\nabout 3 times 10 to the 14 floating point\noperations per second",
    "start": "660380",
    "end": "667640"
  },
  {
    "text": "or 2 times 10 to the 19 floating\npoint operations per day.",
    "start": "667640",
    "end": "672750"
  },
  {
    "text": "This means that it's sort of\nconvenient to sometimes use units of petaflop-days, which\nis 10 to the 15 floating point",
    "start": "672750",
    "end": "679520"
  },
  {
    "text": "operations per\nsecond times a day. And that means that's\nabout three A100 days.",
    "start": "679520",
    "end": "685550"
  },
  {
    "text": "And that's about 8.6\ntimes 10 to the 19 or order 10 to the 20 floating\npoint operations in a day.",
    "start": "685550",
    "end": "694650"
  },
  {
    "text": "So how does sort of the compute\navailable on hardware compared to the compute that we do when\nwe train these gigantic models?",
    "start": "694650",
    "end": "702620"
  },
  {
    "text": "Well, if we have a model\nwith a trillion parameters and we train it for\n300 billion tokens,",
    "start": "702620",
    "end": "709940"
  },
  {
    "text": "then we get 6 times 10 to the\n12, times 3 times 10 to the 11.",
    "start": "709940",
    "end": "715040"
  },
  {
    "text": "And so we get on the order of\n10 to the 24 floating point operations to train a trillion\nparameter model on one",
    "start": "715040",
    "end": "724880"
  },
  {
    "text": "of these large data sets. So these numbers involved-- I mean, I think the\nthing that I find most amazing about this is\nthat I still remember taking",
    "start": "724880",
    "end": "732530"
  },
  {
    "text": "chemistry in high school. And in chemistry, you learn that\nsort of a macroscopic amount of stuff is sort of an\nAvogadro's number of atoms,",
    "start": "732530",
    "end": "741510"
  },
  {
    "text": "which is like 6\ntimes 10 to the 23. So somehow, we're actually\nable to build computers",
    "start": "741510",
    "end": "746975"
  },
  {
    "text": "that working together, do\nmore than an Avogadro's number of computations to train\nthese neural models.",
    "start": "746975",
    "end": "753020"
  },
  {
    "text": "So anyway, I find these\nnumbers kind of mind boggling and also useful to just sort of\nhave in the back of your head",
    "start": "753020",
    "end": "758540"
  },
  {
    "text": "to understand what's going on. So with that prelude, unless\nthere are any questions,",
    "start": "758540",
    "end": "765830"
  },
  {
    "text": "I'll start talking about\nscaling laws for these kinds of language models.",
    "start": "765830",
    "end": "771060"
  },
  {
    "text": " So what I'll\nbasically be arguing",
    "start": "771060",
    "end": "777870"
  },
  {
    "text": "is that there are\nvery surprisingly precise empirical scaling laws\nfor the performance of machine",
    "start": "777870",
    "end": "788250"
  },
  {
    "text": "learning systems,\nmachine learning models as a function of kind of\ngross macroscopic inputs",
    "start": "788250",
    "end": "794279"
  },
  {
    "text": "like how many parameters\ndoes the model have, how big is the data\nset, and how much",
    "start": "794280",
    "end": "799620"
  },
  {
    "text": "compute is used for training. And I'll also make the\npoint that if you're",
    "start": "799620",
    "end": "806279"
  },
  {
    "text": "sort of in an airplane\nat 30,000 feet, looking down on what's\ngoing on in the field, a lot of the other\ndetails in these systems",
    "start": "806280",
    "end": "813450"
  },
  {
    "text": "don't matter all that much. Or at least, they don't\nmatter as much as you might have expected that they would.",
    "start": "813450",
    "end": "819060"
  },
  {
    "text": "Very often, they\njust change some kind of constant pre-factor in these\nkinds of scaling laws, which",
    "start": "819060",
    "end": "826170"
  },
  {
    "text": "give you kind of the big picture\nof what's changing as you really increase these inputs.",
    "start": "826170",
    "end": "832170"
  },
  {
    "text": "And one way of sort of turning\nthis into sort of a theme,",
    "start": "832170",
    "end": "837514"
  },
  {
    "text": "what do you learn from it,\nhow do you summarize it, is that getting these\nmodels to perform better",
    "start": "837515",
    "end": "842790"
  },
  {
    "text": "is to a large extent about\nkind of avoiding bottlenecks. It's avoiding being\nblocked by something.",
    "start": "842790",
    "end": "849270"
  },
  {
    "text": "And there are a lot of things\nthat can block improvements in performance.",
    "start": "849270",
    "end": "854940"
  },
  {
    "text": "The most obvious one, which is\nwhat scaling laws are studying is you can not have enough data.",
    "start": "854940",
    "end": "860580"
  },
  {
    "text": "You can not have a\nlarge enough model. You can not have\nenough computation to train that model.",
    "start": "860580",
    "end": "866170"
  },
  {
    "text": "And then there are also a lot\nof other literal bottlenecks that you can think\nabout, many of which",
    "start": "866170",
    "end": "872010"
  },
  {
    "text": "involve sort of bad\ninformation propagation through the network. So I guess, one way\nthat I would summarize",
    "start": "872010",
    "end": "877350"
  },
  {
    "text": "a lot of the most\nhighly cited papers in machine learning\nin the last 10 years, papers like about\nthings like resnets,",
    "start": "877350",
    "end": "883890"
  },
  {
    "text": "and layer norm, batch\nnorm, things like that, is that they're sort of\nalleviating bottlenecks",
    "start": "883890",
    "end": "890399"
  },
  {
    "text": "where information wasn't\npropagating nicely through your network. And the sort of simplest\npossible picture",
    "start": "890400",
    "end": "897450"
  },
  {
    "text": "to sort of illustrate\nthis, which perhaps, is a cartoon of what's\ngoing on, something",
    "start": "897450",
    "end": "902580"
  },
  {
    "text": "that I'll talk about\nlater on with LSTMs is that if you take a matrix-- I mean, neural\nnetworks are really",
    "start": "902580",
    "end": "908400"
  },
  {
    "text": "just fancy systems that do a\nlot of matrix multiplication. If you take a matrix\nand you multiply it a large number of\ntimes, then very roughly",
    "start": "908400",
    "end": "916860"
  },
  {
    "text": "speaking, what you end\nup with is a projection onto its largest eigenspace.",
    "start": "916860",
    "end": "924390"
  },
  {
    "text": "And so very roughly speaking,\nif you have a deep network and you sort of don't\nset it up correctly,",
    "start": "924390",
    "end": "930329"
  },
  {
    "text": "it's very easy to\nbe in a situation where you lose signal\nor lose information",
    "start": "930330",
    "end": "935800"
  },
  {
    "text": "and you get like\na literal model. But anyway, that's sort of\nthe philosophy that, at least,",
    "start": "935800",
    "end": "942360"
  },
  {
    "text": "at zeroth order,\nyou might sort of reach from thinking about\nsome of these results.",
    "start": "942360",
    "end": "948639"
  },
  {
    "text": "So this slide is\nreally about the kind of core results for scaling\nlaws for language models",
    "start": "948640",
    "end": "955600"
  },
  {
    "text": "and so I'll explain\nit in some detail. So I'm actually going to start\nwith the plot on the far right,",
    "start": "955600",
    "end": "963850"
  },
  {
    "text": "which is about scaling\nlaws with respect to the number of parameters\nin a neural network.",
    "start": "963850",
    "end": "971530"
  },
  {
    "text": "And so what we did\nto generate this plot was get a very\nlarge data set such",
    "start": "971530",
    "end": "979380"
  },
  {
    "text": "that we weren't worried about\nmodels overfitting at all, and train all of our models\nfor a very long time,",
    "start": "979380",
    "end": "987209"
  },
  {
    "text": "so that they were\nessentially at convergence. So in other words,\ntraining time or compute",
    "start": "987210",
    "end": "992240"
  },
  {
    "text": "was not a constraint\non performance. And then plot the resulting\ntest loss of language models,",
    "start": "992240",
    "end": "999110"
  },
  {
    "text": "trained to predict\nthe next word, as a function of parameter\ncount on a nice log scale.",
    "start": "999110",
    "end": "1005900"
  },
  {
    "text": "And so what you see is that\nthere's this power law, which is a straight line\non a log-log plot of the loss as a\nfunction of the parameter",
    "start": "1005900",
    "end": "1013899"
  },
  {
    "text": "count of these models. In the middle plot,\nwe do the same thing,",
    "start": "1013900",
    "end": "1019000"
  },
  {
    "text": "but switched the role\nof the amount of data that we have with\nparameter count. So we train a model\nthat's very large,",
    "start": "1019000",
    "end": "1026470"
  },
  {
    "text": "maybe one of the largest models\non the plot on the right. So that model size is not\na constraint on performance",
    "start": "1026470",
    "end": "1033490"
  },
  {
    "text": "on data sets of various sizes. And we apply early stopping. So we measure the test loss\nat the point where the test",
    "start": "1033490",
    "end": "1040060"
  },
  {
    "text": "loss is at its minimum during,\notherwise, pretty naive straightforward training. And we find, again, a very\nclear power law for loss",
    "start": "1040060",
    "end": "1048790"
  },
  {
    "text": "as a function of data set size. And then the most complicated\nplot is the one on the left.",
    "start": "1048790",
    "end": "1055450"
  },
  {
    "text": "So on the left, we plot\nall of the learning curves",
    "start": "1055450",
    "end": "1061860"
  },
  {
    "text": "for many different models. We provide these models\nwith plenty of data,",
    "start": "1061860",
    "end": "1067139"
  },
  {
    "text": "so they're not overfitting. They're in the\nunderparameterized regime. But we train all of these\ndifferent model sizes",
    "start": "1067140",
    "end": "1075570"
  },
  {
    "text": "for a very, very long time. And we measure, on the\nx-axis, not the number",
    "start": "1075570",
    "end": "1081630"
  },
  {
    "text": "of training steps\nor training tokens, but the amount of\ncompute that has been used, so far, during training.",
    "start": "1081630",
    "end": "1087840"
  },
  {
    "text": "And as a consequence of one\nof the formulas that I wrote on a couple of slides ago,\nthat compute is 6 times",
    "start": "1087840",
    "end": "1095160"
  },
  {
    "text": "parameter count times the\namount of training data. If you take the\nlogarithm of both sides,",
    "start": "1095160",
    "end": "1100400"
  },
  {
    "text": "the log of parameters times\ndata is log of parameters plus log of data. So what that means is\nthat learning curves",
    "start": "1100400",
    "end": "1107240"
  },
  {
    "text": "for models of different\nsizes are just shifted over left and right by\nconstant amounts with the largest models on\nthe sort of the far right",
    "start": "1107240",
    "end": "1115220"
  },
  {
    "text": "of this curve, and the\nsmallest models on the left. So we have the learning\ncurves for all of these models",
    "start": "1115220",
    "end": "1121580"
  },
  {
    "text": "all put together. And so a question\nyou can ask is sort of what is the best loss\nyou can get for any given",
    "start": "1121580",
    "end": "1127760"
  },
  {
    "text": "amount of training compute,\nwhere you're allowing yourself to choose the model\nthat does best for that amount of\ntraining compute?",
    "start": "1127760",
    "end": "1134160"
  },
  {
    "text": "And that's what sort\nof the heavy black line and the orange fit\nare picking out.",
    "start": "1134160",
    "end": "1139970"
  },
  {
    "text": "I mean, formally, you\ncould call this the thing like the convex hull\nof all of these curves.",
    "start": "1139970",
    "end": "1145580"
  },
  {
    "text": "And that, again,\nsomewhat surprisingly, seems to obey a\nvery nice power law",
    "start": "1145580",
    "end": "1151970"
  },
  {
    "text": "fit over many, many orders\nof magnitude in computation.",
    "start": "1151970",
    "end": "1157159"
  },
  {
    "text": "And it's crucial for\nall of these experiments that you're only\nlimiting performance with one thing at a time.",
    "start": "1157160",
    "end": "1163700"
  },
  {
    "text": "On the far right, you have\nplenty of data and compute, but you're limiting the\nnumber of parameters. In the middle, you're\nlimiting the amount of data,",
    "start": "1163700",
    "end": "1169670"
  },
  {
    "text": "but you have a big model. On the left, you're looking\nat training compute,",
    "start": "1169670",
    "end": "1174800"
  },
  {
    "text": "but you have all sorts\nof different model sizes and again, plenty of data. So in other words, in\neach of these cases,",
    "start": "1174800",
    "end": "1180570"
  },
  {
    "text": "there's one of these\nparameters that's bottlenecking performance. And otherwise, you have\nplenty of resources.",
    "start": "1180570",
    "end": "1188588"
  },
  {
    "text": "There's a question? ",
    "start": "1188588",
    "end": "1196270"
  },
  {
    "text": "[INAUDIBLE] the opposite. I hope that's not true. So there's a minus\nsign in the exponent.",
    "start": "1196270",
    "end": "1203620"
  },
  {
    "text": "I'm not sure if you're looking\nat the lines or the function.",
    "start": "1203620",
    "end": "1209260"
  },
  {
    "text": "I'm looking at the axes, on\nthe bottom axis on the right, it's the-- oh, I see. OK.",
    "start": "1209260",
    "end": "1214480"
  },
  {
    "text": "Yeah. It's just a log scale plot. But yeah, please\nask any questions.",
    "start": "1214480",
    "end": "1222330"
  },
  {
    "text": "Great. And then the x-axis\non this compute plot is this petaflops day unit.",
    "start": "1222330",
    "end": "1227880"
  },
  {
    "text": "That's why it's\nactually a small number. Any other questions about--",
    "start": "1227880",
    "end": "1233590"
  },
  {
    "text": "anything about this plot?  Cool.",
    "start": "1233590",
    "end": "1238950"
  },
  {
    "text": "So there's another\nthing that you can do that's kind of\ninteresting with the plot on the left, which\nis you can ask",
    "start": "1238950",
    "end": "1248190"
  },
  {
    "text": "for any given\nquantity of compute that you have available. If someone kindly donates\nto you some number of A100s",
    "start": "1248190",
    "end": "1256170"
  },
  {
    "text": "to use for a few\nweeks and you want to use it to train the best\npossible language model you can.",
    "start": "1256170",
    "end": "1261850"
  },
  {
    "text": "And so you can ask, based\non this plot in the left, how should I allocate\nthe computation that",
    "start": "1261850",
    "end": "1267930"
  },
  {
    "text": "was given to me in terms\nof making a bigger model or training longer?",
    "start": "1267930",
    "end": "1274659"
  },
  {
    "text": "And it turns out there's\nsort of a simplified cartoon for the answer that\nwe found with our language",
    "start": "1274660",
    "end": "1280960"
  },
  {
    "text": "data, which was that you\nmostly want to allocate most of your compute, basically,\n2/3 on a geometric scale",
    "start": "1280960",
    "end": "1289180"
  },
  {
    "text": "to making models bigger. And you can allocate\nabout a third to training for\nlonger on more data.",
    "start": "1289180",
    "end": "1298330"
  },
  {
    "text": "And so this, at least for us,\nwasn't an obvious conclusion. It suggests that a lot of the\ngains that you're going to get,",
    "start": "1298330",
    "end": "1307390"
  },
  {
    "text": "if you want to get better\nperformance with a fixed amount of compute,\na fixed budget, is going to come from\nmaking your models bigger.",
    "start": "1307390",
    "end": "1314770"
  },
  {
    "text": "And it turns out\nthat in practice, I won't go into it in detail,\nyou can, to some extent, just make your batch size\nbigger during training.",
    "start": "1314770",
    "end": "1322180"
  },
  {
    "text": "And that means that the\ntotal number of serial steps that you train for doesn't\nhave to increase all that much.",
    "start": "1322180",
    "end": "1327820"
  },
  {
    "text": "You don't necessarily have\nto train for vastly longer. You seemingly just need\nlargely a bigger model.",
    "start": "1327820",
    "end": "1335990"
  },
  {
    "text": "And that's something that you\nread off from this compute plot that I showed you. [INAUDIBLE]",
    "start": "1335990",
    "end": "1342960"
  },
  {
    "text": "Sure. So that's a general question.",
    "start": "1342960",
    "end": "1349350"
  },
  {
    "text": "The way that you get\nthis graph is you basically do an\nanalysis where you",
    "start": "1349350",
    "end": "1357029"
  },
  {
    "text": "look at any given point\nfor compute and you look up and you pick out the\nblue curve that's",
    "start": "1357030",
    "end": "1363539"
  },
  {
    "text": "closest to the black line. And that gives you a model\nsize and an amount of training.",
    "start": "1363540",
    "end": "1369300"
  },
  {
    "text": "And so you can do that for\nall of these different points",
    "start": "1369300",
    "end": "1374820"
  },
  {
    "text": "on the x-axis. And then for any given\npoint in this x-axis, that tells you a model size.",
    "start": "1374820",
    "end": "1379960"
  },
  {
    "text": "You learn model\nsize as a function of your compute budget. And then conversely,\nyou also learn",
    "start": "1379960",
    "end": "1386140"
  },
  {
    "text": "an amount of training, which\nis sort of a data set size. And so that's the explanation\nfor the sort of million",
    "start": "1386140",
    "end": "1394320"
  },
  {
    "text": "x model size versus\n1,000 x in data.",
    "start": "1394320",
    "end": "1400325"
  },
  {
    "text": "I probably won't try to explain\nthe batch size question, but it's basically based on\nsome empirical analysis where",
    "start": "1400326",
    "end": "1405335"
  },
  {
    "text": "you ask how big can you\nmake your batch size, how far can you push data\nparallelism without seeing",
    "start": "1405335",
    "end": "1412169"
  },
  {
    "text": "diminishing returns. And that's sort of the\nrough answer from that. Question? [INAUDIBLE]",
    "start": "1412170",
    "end": "1419960"
  },
  {
    "text": " They're all trained\nfrom scratch. So this is always\nalmost everything that I'll talk about in this\ntalk is training from scratch.",
    "start": "1419960",
    "end": "1428929"
  },
  {
    "text": "Any other questions?  OK.",
    "start": "1428930",
    "end": "1434500"
  },
  {
    "text": "And then another point that-- I mean, I don't want\nto overemphasize. But like I said, from a\nsort of very zeroth order",
    "start": "1434500",
    "end": "1441430"
  },
  {
    "text": "naive perspective, is that\nfor some of these results, architecture isn't the\nmost crucial thing.",
    "start": "1441430",
    "end": "1447740"
  },
  {
    "text": "So I think one of the biggest\nadvances in machine learning in the last five\nor ten years has",
    "start": "1447740",
    "end": "1454059"
  },
  {
    "text": "been the development of\nthe transformer models that I'm talking about. But of course, you can\ndo language modeling",
    "start": "1454060",
    "end": "1459970"
  },
  {
    "text": "with a recurrent model\nthat reads words in order. And of course, LSTMs\nor stacked LSTMs",
    "start": "1459970",
    "end": "1466570"
  },
  {
    "text": "are sort of the\nstandard way to do that. And you can compare\nwhat you actually",
    "start": "1466570",
    "end": "1472419"
  },
  {
    "text": "get if you study LSTMs\nversus transformers. And at zeroth order, it doesn't\nseem like LSTMs are so bad.",
    "start": "1472420",
    "end": "1480215"
  },
  {
    "text": "It looks like, as\nyou make them bigger, they are scaling\nup quite nicely. But there's basically a constant\noffset, where transformers",
    "start": "1480215",
    "end": "1487390"
  },
  {
    "text": "are something like\nfive or ten times more efficient for a given\nmodel size than LSTMs.",
    "start": "1487390",
    "end": "1493240"
  },
  {
    "text": "And so I think this is a very,\nvery convincing plot that tells you that transformers\nare, in fact, better.",
    "start": "1493240",
    "end": "1498820"
  },
  {
    "text": "But you don't necessarily\nneed a transformer to see that making models bigger\nis giving you [INAUDIBLE]..",
    "start": "1498820",
    "end": "1504850"
  },
  {
    "text": "And really, the sort of\nmore interesting limitation of LSTMs, that I'll also talk\nabout a little more later,",
    "start": "1504850",
    "end": "1511630"
  },
  {
    "text": "is if we plot something else. So if we look at\n1,000 tokens, which",
    "start": "1511630",
    "end": "1517480"
  },
  {
    "text": "is something like\n600 words of context, we can look at what the\nloss is as a function",
    "start": "1517480",
    "end": "1523600"
  },
  {
    "text": "of the position in the context. Because if you've read\nmore of a document already, you're going to be\nbetter at predicting",
    "start": "1523600",
    "end": "1530275"
  },
  {
    "text": "what the next word\nis because you have more context available. And they are very smooth. It turns out, also\npower law curves",
    "start": "1530275",
    "end": "1536350"
  },
  {
    "text": "for the loss as a function\nof context position. But the thing that you notice\nis that the red lines are LSTMs",
    "start": "1536350",
    "end": "1546220"
  },
  {
    "text": "and the blue lines\nare transformers. And LSTMs tend to sort\nof plateau in performance",
    "start": "1546220",
    "end": "1551649"
  },
  {
    "text": "after on the order\nof 100 tokens. And this is sort of\nanother bottleneck",
    "start": "1551650",
    "end": "1557409"
  },
  {
    "text": "in a different direction. This is the famous\nfact that transformers are much better at learning\nlong context information.",
    "start": "1557410",
    "end": "1564970"
  },
  {
    "text": "And this is obviously\na limitation of LSTMs. But sort of the basic\nparameter scaling law",
    "start": "1564970",
    "end": "1571090"
  },
  {
    "text": "seems like it holds\nfor many architectures. And then there are much more\nrefined questions you can ask.",
    "start": "1571090",
    "end": "1576100"
  },
  {
    "text": "I won't go into too\nmuch detail on this. But there are all sorts\nof hyperparameters in transformer models.",
    "start": "1576100",
    "end": "1581520"
  },
  {
    "text": "And you might ask, how\nmuch does it matter if I really optimize those? Do I get qualitatively\ndifferent behavior",
    "start": "1581520",
    "end": "1587049"
  },
  {
    "text": "if I optimize those better? And what all of\nthese plots show is that for various different\nkinds of hyperparameters",
    "start": "1587050",
    "end": "1593620"
  },
  {
    "text": "and transformer models,\nthere's some broad basin, where you get quite good performance.",
    "start": "1593620",
    "end": "1598630"
  },
  {
    "text": "I mean, maybe a factor of\nthree in either direction, where performance doesn't\nchange all that much. Of course, you might\nwant to optimize that.",
    "start": "1598630",
    "end": "1605200"
  },
  {
    "text": "I'm not saying you shouldn't. But kind of qualitatively, it's\nnot an enormous difference.",
    "start": "1605200",
    "end": "1613320"
  },
  {
    "text": "So I think this is also\na place where it's-- so I'm going to tell\nyou, in a few slides,",
    "start": "1613320",
    "end": "1619470"
  },
  {
    "text": "that a lot of these features\nare true more generally beyond language.",
    "start": "1619470",
    "end": "1624480"
  },
  {
    "text": "And they really sort\nof say that much of what's going on when machines\nlearn is quite universal.",
    "start": "1624480",
    "end": "1631320"
  },
  {
    "text": "But there are features\nthat are non-universal. So this is kind of a nicer plot\nof loss versus token index.",
    "start": "1631320",
    "end": "1642309"
  },
  {
    "text": "And I've included\nsome power law fits, which are dotted lines, which\nshow that this performance is",
    "start": "1642310",
    "end": "1651450"
  },
  {
    "text": "also highly predictable. That just says the obvious\nthat when you read more, you understand.",
    "start": "1651450",
    "end": "1657420"
  },
  {
    "text": "It's easier for you to\npredict what's coming next. But you can train\nmodels on images.",
    "start": "1657420",
    "end": "1663030"
  },
  {
    "text": "I'll briefly talk\nabout that later. You can train models,\nidentically, on images.",
    "start": "1663030",
    "end": "1668100"
  },
  {
    "text": "And there, you see\nperformance as a function of context position. It's very different. So here, you have a model\nthat reads pixels row by row.",
    "start": "1668100",
    "end": "1677850"
  },
  {
    "text": "And as you might\nexpect, there's usually much more non-trivial\nstuff going on in the middle of an image\nrather than in the background.",
    "start": "1677850",
    "end": "1685050"
  },
  {
    "text": "And that's represented by the\nfact that models do much worse, their loss is higher\nin the center of images",
    "start": "1685050",
    "end": "1690390"
  },
  {
    "text": "as compared to near the edges. So while some properties of\ntransformers and language",
    "start": "1690390",
    "end": "1696690"
  },
  {
    "text": "models are universal and I'll\ntalk about those later on, there are features\nof language data",
    "start": "1696690",
    "end": "1701820"
  },
  {
    "text": "that are totally different\nfrom other data distributions. And this is a very\nstark example of that.",
    "start": "1701820",
    "end": "1707100"
  },
  {
    "text": " But generally, the\nfact that there",
    "start": "1707100",
    "end": "1712180"
  },
  {
    "text": "are these kinds of\nnice patterns lurking whenever you optimize a model,\nI think that is very common.",
    "start": "1712180",
    "end": "1719510"
  },
  {
    "text": "So any questions about this? Yeah.",
    "start": "1719510",
    "end": "1725049"
  },
  {
    "text": "Do you mind explaining what\nit means to have a loss on the first token\nversus the 1,000th token?",
    "start": "1725050",
    "end": "1734320"
  },
  {
    "text": "Yeah. So if you imagine you have\n1,000 words extracted randomly from a book, then\nthe very first thing",
    "start": "1734320",
    "end": "1741693"
  },
  {
    "text": "you can ask the\nmodel to do is try to predict the very first word. Then you ask it to predict the\nsecond word, the third word,",
    "start": "1741693",
    "end": "1747940"
  },
  {
    "text": "et cetera. The very first word,\nbasically, all the model can possibly do is predict\nthe unigram distribution",
    "start": "1747940",
    "end": "1755470"
  },
  {
    "text": "for its training set. It just doesn't\nhave any information to go on, otherwise, to\npredict what's happening.",
    "start": "1755470",
    "end": "1760640"
  },
  {
    "text": "And so that's why its\nloss is very high. But by the time you get\nto the end of the passage, you've read a lot of\nsome little short story.",
    "start": "1760640",
    "end": "1768169"
  },
  {
    "text": "And you know a lot about\nwhat's going to happen. You know what kinds of words\nare likely to come next. You know about the author's\nstyle and vocabulary.",
    "start": "1768170",
    "end": "1775720"
  },
  {
    "text": "You know about what\ncharacters exist, et cetera. And so your model has gotten\nmuch, much better at prediction",
    "start": "1775720",
    "end": "1782049"
  },
  {
    "text": "by the end of the context. And so literally,\nto make this plot, you take maybe 1,000,\n10,000 different passages",
    "start": "1782050",
    "end": "1789429"
  },
  {
    "text": "with 1,000 words in them. You compute the model's\nloss on all of the words in the passage, and\nthen you take the mean",
    "start": "1789430",
    "end": "1795820"
  },
  {
    "text": "and you get some\nnice plot like this. Yeah. Because the\ncomputational complexity",
    "start": "1795820",
    "end": "1804550"
  },
  {
    "text": "is quadratic with\nrespect to token of x, would that mean that\nessentially for token",
    "start": "1804550",
    "end": "1810790"
  },
  {
    "text": "of x, if you think about it,\nif you were looking at compute, then it would go from\n0 to like 10 to the 6.",
    "start": "1810790",
    "end": "1818740"
  },
  {
    "text": "So you have significantly\ngreater compute",
    "start": "1818740",
    "end": "1824290"
  },
  {
    "text": "for a given test loss as\nyou increase token of x. So it's true that if\nyou make the context",
    "start": "1824290",
    "end": "1831279"
  },
  {
    "text": "length longer and longer,\nyou will spend somewhat more compute. But the fraction of\nthe amount of compute",
    "start": "1831280",
    "end": "1838060"
  },
  {
    "text": "you spend near the last\ntoken isn't nearly so stark. Most of the compute happens\nin the matrix multiplies",
    "start": "1838060",
    "end": "1847180"
  },
  {
    "text": "for the NLP feed forward\npart of the transformer. And also the matrix multiplies\nto make the keys, inquiries,",
    "start": "1847180",
    "end": "1855850"
  },
  {
    "text": "and values, et cetera. That's actually in most of all. It depends on the\nmodel hyperparameters.",
    "start": "1855850",
    "end": "1862160"
  },
  {
    "text": "But in many models, especially\nmodels that are large, that's actually the\npredominant compute.",
    "start": "1862160",
    "end": "1867250"
  },
  {
    "text": "And so actually, the\namount of compute you do for the last\ntoken, the first token might only differ\nby a few percent. So for GPT-3, I\nthink it's literally",
    "start": "1867250",
    "end": "1874420"
  },
  {
    "text": "like 1% or 2% difference. So matrix multiply dominates\nthe attention in terms of--",
    "start": "1874420",
    "end": "1879519"
  },
  {
    "text": "Yeah, yeah, yeah. So I mean, the formula for that\nwas this one that I briefly mentioned here. So basically, how\nmuch compute you",
    "start": "1879520",
    "end": "1886270"
  },
  {
    "text": "do in the context\ndirection divided by the amount of compute you\ndo in the matrix multiply",
    "start": "1886270",
    "end": "1891460"
  },
  {
    "text": "direction is this. So if d model is very small,\nlike if d model is 128",
    "start": "1891460",
    "end": "1896920"
  },
  {
    "text": "and context is 1,000,\nthen it's basically 50-50.",
    "start": "1896920",
    "end": "1902050"
  },
  {
    "text": "But if d model is 10,000\nand context is 2000, then it's like 2%.",
    "start": "1902050",
    "end": "1908040"
  },
  {
    "text": "So if the models\nkeep getting bigger, then that means that if\nyou're willing to pay a fractional cost,\nthen you can keep",
    "start": "1908040",
    "end": "1914520"
  },
  {
    "text": "making context length longer\nand pay a fixed fractional cost. And of course, if you\nuse something fancier",
    "start": "1914520",
    "end": "1920130"
  },
  {
    "text": "than dense attention, you also\nget extra wins on top of that. Any other questions?",
    "start": "1920130",
    "end": "1925200"
  },
  {
    "text": " Cool.",
    "start": "1925200",
    "end": "1931010"
  },
  {
    "text": "So both of these, the\nleft and the right,",
    "start": "1931010",
    "end": "1937670"
  },
  {
    "text": "show you samples from\na transformer model.",
    "start": "1937670",
    "end": "1944060"
  },
  {
    "text": "Very roughly speaking,\nthey're identical kinds of transformer models, which\njust with some slightly different hyperparameters.",
    "start": "1944060",
    "end": "1950040"
  },
  {
    "text": "But they're trained on very\ndifferent data distributions. The one on the left is,\nobviously, this is GPT-3.",
    "start": "1950040",
    "end": "1955760"
  },
  {
    "text": "The one on the right is IGBT. It's a model that's trained\nto predict pixels row by row.",
    "start": "1955760",
    "end": "1962150"
  },
  {
    "text": "And so what happened\nhere was that we took the top half of an\nimage and then generated",
    "start": "1962150",
    "end": "1967700"
  },
  {
    "text": "all of the rows beneath. And so the same kind\nof model architecture,",
    "start": "1967700",
    "end": "1972793"
  },
  {
    "text": "but just trained\non different data distributions is sort of\nable to effectively learn very impressive, generative\ncapabilities in both cases.",
    "start": "1972793",
    "end": "1984510"
  },
  {
    "text": "And so this is sort\nof a qualitative hint at sort of the possibility\nthat what's going on here",
    "start": "1984510",
    "end": "1989750"
  },
  {
    "text": "is quite universal. And so another way of\nintroducing it to say,",
    "start": "1989750",
    "end": "1996200"
  },
  {
    "text": "you might have some questions\nafter the last few slides, are the scaling laws I'm\ntalking about really specific",
    "start": "1996200",
    "end": "2001713"
  },
  {
    "text": "to language, are they a\nfeature of the kinds of data that language is. And you might ask, do these\nscaling laws really continue?",
    "start": "2001713",
    "end": "2009190"
  },
  {
    "text": "You showed that they're true\nover many orders of magnitude, but do they break\ndown eventually? And in what way?",
    "start": "2009190",
    "end": "2014350"
  },
  {
    "text": "And then another\nquestion you might ask is, what do they imply for\nother kinds of evaluations.",
    "start": "2014350",
    "end": "2020740"
  },
  {
    "text": "You probably don't just\nwant to generate raw samples from either of these\nkinds of models. You might want to use them for\nsome other more specific task.",
    "start": "2020740",
    "end": "2029240"
  },
  {
    "text": "And so the question of\nwhether or not the test loss, the training loss that\nyou've optimized as that",
    "start": "2029240",
    "end": "2036370"
  },
  {
    "text": "goes down in a\npredictable way, does that also imply that other\nthings, other capabilities of the model are improving?",
    "start": "2036370",
    "end": "2043270"
  },
  {
    "text": "So I'll be talking\nabout these questions. ",
    "start": "2043270",
    "end": "2052190"
  },
  {
    "text": "So this plot contains kind of\na lot of compressed information all at once or the set of plots. So this is the result\nof what happens",
    "start": "2052190",
    "end": "2060109"
  },
  {
    "text": "if you train the same\nkind of transfer models on sort of five different\ndata distributions.",
    "start": "2060110",
    "end": "2067710"
  },
  {
    "text": "So text, language,\nwe already saw. But you can try video,\nwhere you predict",
    "start": "2067710",
    "end": "2072739"
  },
  {
    "text": "every pixel in a video in\nthis sort of rectangular prism of video pixels.",
    "start": "2072739",
    "end": "2079399"
  },
  {
    "text": "Images, the sort\nof synthetically generated DeepMind\nmath data set,",
    "start": "2079400",
    "end": "2085070"
  },
  {
    "text": "where you're trying to predict\nthe answer to math problems. There's a multimodal data\nset, where you have image text",
    "start": "2085070",
    "end": "2092690"
  },
  {
    "text": "pairs in either direction. And in all cases,\nthe x-axis is compute",
    "start": "2092690",
    "end": "2099470"
  },
  {
    "text": "and the y-axis is\nthe appropriate test loss for that class of\nmodels minus a constant.",
    "start": "2099470",
    "end": "2108215"
  },
  {
    "text": " So that's the one complication\nthat I've added here.",
    "start": "2108215",
    "end": "2115270"
  },
  {
    "text": " So the claim is\nthat these dashed",
    "start": "2115270",
    "end": "2121220"
  },
  {
    "text": "lines in terms of\nthe original loss are a power law, like\nthe power laws that we",
    "start": "2121220",
    "end": "2131150"
  },
  {
    "text": "saw on a much earlier slide,\nplus one constant term.",
    "start": "2131150",
    "end": "2138000"
  },
  {
    "text": "And if you subtract\noff that constant term, and you make a log-log\nplot once again, then you, once again, get these\nvery, very nice straight lines.",
    "start": "2138000",
    "end": "2146390"
  },
  {
    "text": "And so this compute\nscaling law generalizes to all these other\ndata distributions. And the other scaling\nlaws also generalize.",
    "start": "2146390",
    "end": "2153740"
  },
  {
    "text": "I just haven't plotted them. So the claim of this\nslide is that scaling laws",
    "start": "2153740",
    "end": "2159660"
  },
  {
    "text": "do generalize to all of these\nother data distributions when you train the same\nbasic kind of model on them.",
    "start": "2159660",
    "end": "2167460"
  },
  {
    "text": "And furthermore, there's sort\nof an intellectually, slightly interesting point, which\nis that if you really",
    "start": "2167460",
    "end": "2174480"
  },
  {
    "text": "believe that these\ndashed lines are true,",
    "start": "2174480",
    "end": "2180450"
  },
  {
    "text": "if you think that they're a\nreal feature of what's going on and they continue out very,\nvery, very far, then if you",
    "start": "2180450",
    "end": "2187890"
  },
  {
    "text": "think that the loss is a\nconstant plus a power law, then you can interpret\nthe constant term",
    "start": "2187890",
    "end": "2194640"
  },
  {
    "text": "as the entropy of the\nunderlying data distribution. And you can interpret\nthe power law",
    "start": "2194640",
    "end": "2201180"
  },
  {
    "text": "as something like\nthe KL divergence between the true data\ndistribution and the model",
    "start": "2201180",
    "end": "2206460"
  },
  {
    "text": "that you have. So that's a lot. The important summary at\nzeroth order to remember",
    "start": "2206460",
    "end": "2215940"
  },
  {
    "text": "is that I'm telling you that\nthe kinds of scaling laws I presented for\nlanguage generalize to all of these other domains.",
    "start": "2215940",
    "end": "2222030"
  },
  {
    "text": "There's also some other\ninteresting features here. The reason why I used compute\nto illustrate that the scaling",
    "start": "2222030",
    "end": "2230710"
  },
  {
    "text": "law is generalize is because\nyou can ask another question now",
    "start": "2230710",
    "end": "2235900"
  },
  {
    "text": "that puts all of the different\ndata distributions on one plot. It wouldn't have made any\nsense to combine the five",
    "start": "2235900",
    "end": "2242994"
  },
  {
    "text": "plots on the last\nslide into one plot because the test loss, when\nyou're predicting a word,",
    "start": "2242995",
    "end": "2248290"
  },
  {
    "text": "is not in any way\ncomparable to the test loss when you're predicting a pixel. It doesn't really make sense. They don't have the same units.",
    "start": "2248290",
    "end": "2254778"
  },
  {
    "text": "It doesn't make sense\nto put them together. But something that does\nmake sense to put together is what the optimal\nmodel size is",
    "start": "2254778",
    "end": "2262000"
  },
  {
    "text": "as a function of your\ncomputational budget. And so in the same way\nthat we did for language,",
    "start": "2262000",
    "end": "2268540"
  },
  {
    "text": "you can go here and you can\nask for any given amount of compute, like 10 to\nthe minus 2 petaflop days,",
    "start": "2268540",
    "end": "2273700"
  },
  {
    "text": "what is the best model size. You can do that for\nall of these plots, combine that\ninformation together,",
    "start": "2273700",
    "end": "2278980"
  },
  {
    "text": "and you find something kind\nof surprising, which is that, again, roughly\nspeaking, if you're",
    "start": "2278980",
    "end": "2284800"
  },
  {
    "text": "sort of willing to allow a\nlittle bit of wiggle room, all of these different\nkinds of models",
    "start": "2284800",
    "end": "2290619"
  },
  {
    "text": "seem to be on the same\ntrajectory for optimal model size versus compute.",
    "start": "2290620",
    "end": "2296170"
  },
  {
    "text": "There's some kind of universal\nfit of how much bigger you should make your\nmodel if you're going to model any of these\ndata distributions",
    "start": "2296170",
    "end": "2303558"
  },
  {
    "text": "with some given\namount of compute. ",
    "start": "2303558",
    "end": "2308630"
  },
  {
    "text": "So what about other\nkinds of tasks? Well, one of the most classic\ntasks that you can ask about",
    "start": "2308630",
    "end": "2315380"
  },
  {
    "text": "in ML is image classification. And so the models that we\nwere training on images",
    "start": "2315380",
    "end": "2322490"
  },
  {
    "text": "and that I've shown you\nplots, their training loss, these models are sort\nof tiny little images",
    "start": "2322490",
    "end": "2329780"
  },
  {
    "text": "predicted pixel by pixel. In particular, there\nare 32 by 32 images. So we can look at the\n32 by 32 pixel version",
    "start": "2329780",
    "end": "2337400"
  },
  {
    "text": "of ImageNet classification. And the models that\nI was discussing",
    "start": "2337400",
    "end": "2342700"
  },
  {
    "text": "are generative models\nthat predict pixels. But you can chop\noff their heads, add a classification\nhead in its place,",
    "start": "2342700",
    "end": "2351430"
  },
  {
    "text": "and try to predict ImageNet\nand train on ImageNet. And the orange curve\nthat I've shown you here",
    "start": "2351430",
    "end": "2357317"
  },
  {
    "text": "is what happens if you\njust take a randomly initialized model with that\narchitecture and train it.",
    "start": "2357318",
    "end": "2362860"
  },
  {
    "text": "You get very good\nperformance up to a point and then performance\nplateaus because you're",
    "start": "2362860",
    "end": "2368135"
  },
  {
    "text": "being limited by the\nfact that ImageNet is, from this point of\nview, a small data set. However, if you take these\npre-trained models that",
    "start": "2368135",
    "end": "2376160"
  },
  {
    "text": "have been trained,\ngeneratively, to draw pixels, they sort of use the\nfeatures-- presumably,",
    "start": "2376160",
    "end": "2382850"
  },
  {
    "text": "they're using the features they\nlearned from image generation for classification.",
    "start": "2382850",
    "end": "2388190"
  },
  {
    "text": "And so you get some\nnice trend for the error rate in classification as\na function of model size.",
    "start": "2388190",
    "end": "2396619"
  },
  {
    "text": "So this is saying that\nin this particular case, we actually do fine tuning\nthe pre-training that you did",
    "start": "2396620",
    "end": "2403790"
  },
  {
    "text": "and the sort of trends you\nsaw really kind of transfer into trends in something\nelse you might care about",
    "start": "2403790",
    "end": "2409160"
  },
  {
    "text": "like image classification. We can ask the same\nkinds of questions",
    "start": "2409160",
    "end": "2415460"
  },
  {
    "text": "about language models. In particular, does this\nsteady improvement in language",
    "start": "2415460",
    "end": "2421010"
  },
  {
    "text": "modeling as a function\nof scale, does that translate into\nbetter performance?",
    "start": "2421010",
    "end": "2427730"
  },
  {
    "text": "And this is sort of an\ninteresting subject by itself, and you can ask what happens\nif we scale language models.",
    "start": "2427730",
    "end": "2434099"
  },
  {
    "text": "And so this is sort of\nthis exact same plot that you've seen a couple of\ntimes now for language models,",
    "start": "2434100",
    "end": "2439160"
  },
  {
    "text": "but it just increased\nfrom sort of original work that we did out to this\nyellow line, which is GPT-3.",
    "start": "2439160",
    "end": "2447290"
  },
  {
    "text": "And you see that basically,\nthis sort of trends continue. Possibly, GPT-3 is sort of\nmissing the trend a little bit.",
    "start": "2447290",
    "end": "2454555"
  },
  {
    "text": "I can't really,\nhonestly tell you whether that's because\nGPT-3 wasn't well optimized or if it's because\nthere's some bending",
    "start": "2454555",
    "end": "2463430"
  },
  {
    "text": "in this curve, where we're\nhitting some irreducible loss. That irreducible loss\nwould be something",
    "start": "2463430",
    "end": "2468440"
  },
  {
    "text": "like the entropy of the sort\nof language data set itself.",
    "start": "2468440",
    "end": "2474980"
  },
  {
    "text": "But those sort of\ntrends continue. And what's now I think\npretty well known",
    "start": "2474980",
    "end": "2481450"
  },
  {
    "text": "is that if you train fairly\nlarge language models, then they can sort of exhibit\nin-context learning.",
    "start": "2481450",
    "end": "2487460"
  },
  {
    "text": "So the kind of learning\nthat I'm talking about is that you give these models\nan example of many arithmetic",
    "start": "2487460",
    "end": "2495520"
  },
  {
    "text": "problems or many anagrams\nor whatnot or translation",
    "start": "2495520",
    "end": "2501040"
  },
  {
    "text": "tasks for individual words. Then early on, in the\nsequence of the top,",
    "start": "2501040",
    "end": "2506320"
  },
  {
    "text": "they might not be very\ngood at doing the task, but they figure out what\nthe pattern is in the task,",
    "start": "2506320",
    "end": "2512230"
  },
  {
    "text": "and they learn to do it. And in particular,\nyou can plot that. So you can ask for, say,\nlike one of these anagram",
    "start": "2512230",
    "end": "2518619"
  },
  {
    "text": "tasks, what is the performance\nof the model as a function of how many examples of the\ntask gets seen in the context.",
    "start": "2518620",
    "end": "2527510"
  },
  {
    "text": "So this is kind of\nsimilar to the loss as a function of\ncontext position, but it's now an accuracy\ndoing an actual task",
    "start": "2527510",
    "end": "2534309"
  },
  {
    "text": "like unscrambling the\nletters in a word. And you see, probably,\nmost importantly,",
    "start": "2534310",
    "end": "2540110"
  },
  {
    "text": "that if you give\nmore examples, you get significantly\nbetter performance starting from very, very poor\nperformance to pretty good.",
    "start": "2540110",
    "end": "2547990"
  },
  {
    "text": "And also, you see that\nlarger models do this better. You also finally see that\ngiving a natural language",
    "start": "2547990",
    "end": "2553930"
  },
  {
    "text": "prompt with some instructions\nhelps significantly in the regime where you\nhave very few examples.",
    "start": "2553930",
    "end": "2560515"
  },
  {
    "text": "This is in-context learning. You can call this a\nkind of meta learning.",
    "start": "2560515",
    "end": "2565960"
  },
  {
    "text": "And it just emerges\nautomatically from training large\nlanguage models",
    "start": "2565960",
    "end": "2571640"
  },
  {
    "text": "without any particular attempt\nto get this kind of behavior.",
    "start": "2571640",
    "end": "2577269"
  },
  {
    "text": "And you can also ask sort\nof about downstream tasks that you actually care about.",
    "start": "2577270",
    "end": "2583490"
  },
  {
    "text": "So there is accuracy\nat doing arithmetic as a function of model size,\na bunch of different kinds",
    "start": "2583490",
    "end": "2588940"
  },
  {
    "text": "of arithmetic problems. There is just some data set\nof analogies from a test",
    "start": "2588940",
    "end": "2596950"
  },
  {
    "text": "that American\ncollege students take to go to college, the SATs.",
    "start": "2596950",
    "end": "2602950"
  },
  {
    "text": "And if you care, the sort of\naverage score of that year's",
    "start": "2602950",
    "end": "2608800"
  },
  {
    "text": "test was, I think, 58% or so. So the largest model is\ndoing a little bit better",
    "start": "2608800",
    "end": "2613960"
  },
  {
    "text": "than the average American\nhigh school student. The trivia QA, which\nis sort of just knowing",
    "start": "2613960",
    "end": "2621010"
  },
  {
    "text": "trivia and Winograd schemas or\nproblems like if a tree falls",
    "start": "2621010",
    "end": "2627280"
  },
  {
    "text": "on your roof and you got it\nfixed, what did you get fixed? Did you get the tree\nfixed or your roof?",
    "start": "2627280",
    "end": "2633582"
  },
  {
    "text": "It's a measure of\ncommon sense reasoning. And models are also\ngetting better at this. And I think the other\ninteresting thing that's",
    "start": "2633582",
    "end": "2639460"
  },
  {
    "text": "very often emphasized is that\nclearly, trivia performance is improving very smoothly.",
    "start": "2639460",
    "end": "2644980"
  },
  {
    "text": "As you make models\nbigger, the models are just remembering\nmore and more trivia. Winograd schemas are also\nimproving fairly smoothly.",
    "start": "2644980",
    "end": "2653707"
  },
  {
    "text": "But then there are\nexamples like arithmetic, where models are very poor,\nand then they sort of suddenly get pretty good.",
    "start": "2653707",
    "end": "2659840"
  },
  {
    "text": "And so these kind\nof sudden grok sort of models sort of\nsuddenly kind of like gets",
    "start": "2659840",
    "end": "2665305"
  },
  {
    "text": "what it's supposed\nto do for arithmetic are pretty interesting. I mean, there are all\nsorts of other kind",
    "start": "2665305",
    "end": "2670510"
  },
  {
    "text": "of interesting things\nif you dig into these specific capabilities. Yeah.",
    "start": "2670510",
    "end": "2676340"
  },
  {
    "text": "Why bigger models do\nbetter in-context learning?",
    "start": "2676340",
    "end": "2681620"
  },
  {
    "text": "I mean, I guess the sort\nof dumb zeroth order point is that larger models\nare just getting much",
    "start": "2681620",
    "end": "2688550"
  },
  {
    "text": "better and better at\npredicting the next word given more and more context. So I think there's a\nvery tight connection",
    "start": "2688550",
    "end": "2695750"
  },
  {
    "text": "between a plot like\nthis and these sort of in-context learning plots.",
    "start": "2695750",
    "end": "2701630"
  },
  {
    "text": "Basically, the more\ninformation you're getting. I mean, all of these\nmodels, probably, know the unigram distribution\nof words and tokens pretty well.",
    "start": "2701630",
    "end": "2710930"
  },
  {
    "text": "But the bigger model is\ngetting much, much, much more information from its context\nthan the smaller models.",
    "start": "2710930",
    "end": "2716990"
  },
  {
    "text": "And at a certain\npoint, I mean, it depends on your\ntraining distribution and all sorts of other things.",
    "start": "2716990",
    "end": "2723200"
  },
  {
    "text": "One of the things\nthat we do is when we see several examples of\nsomething happening in a text, we guess that that's what\nwe're going to see next.",
    "start": "2723200",
    "end": "2731660"
  },
  {
    "text": "And that's really\nprobably embedded in a ton of text that's\nout there on the internet",
    "start": "2731660",
    "end": "2737060"
  },
  {
    "text": "and in books. And models have to decrease\ntheir loss somehow. That's a pattern in the text.",
    "start": "2737060",
    "end": "2743030"
  },
  {
    "text": "It's a pattern that\nmodels eventually learn, and they seemingly\napply this knowledge. I think there are other\npeople, of course,",
    "start": "2743030",
    "end": "2749690"
  },
  {
    "text": "who've kind of worked on this\nquestion more specifically. And I have more\nspecific theories, but I think in like kind of\nan intuitive sense, that's",
    "start": "2749690",
    "end": "2755630"
  },
  {
    "text": "how I would think about it. ",
    "start": "2755630",
    "end": "2762789"
  },
  {
    "text": "I guess, one final\nevaluation you can ask, can people tell that text\nwritten by a language model",
    "start": "2762790",
    "end": "2769360"
  },
  {
    "text": "was written by a language\nmodel or that it's a human? This is an evaluation where we\nlooked at short news articles.",
    "start": "2769360",
    "end": "2776980"
  },
  {
    "text": "There's, I think, two\nor three paragraphs and generated equivalent\nnews articles from GPT-3.",
    "start": "2776980",
    "end": "2782500"
  },
  {
    "text": "And by the time you get to\nsort of the largest models, people are approaching\nchance accuracy at being able to\ntell the difference.",
    "start": "2782500",
    "end": "2788750"
  },
  {
    "text": "This sort of has a lot\nof implications, both-- I mean, it's interesting\nand surprising as a statement about\nlanguage modeling,",
    "start": "2788750",
    "end": "2795460"
  },
  {
    "text": "but it's also somewhat scary. It means that these\nlanguage models are-- it's very difficult\nto tell that you're",
    "start": "2795460",
    "end": "2802750"
  },
  {
    "text": "talking to a language\nmodel if you don't have a very long conversation. ",
    "start": "2802750",
    "end": "2809470"
  },
  {
    "text": "Yeah. All right. So I am wondering if-- OK, for so this specific\nstudy, for example,",
    "start": "2809470",
    "end": "2815740"
  },
  {
    "text": "because with larger models, they\nare more prone to memorization. So have you examined like\nif the generated articles",
    "start": "2815740",
    "end": "2823660"
  },
  {
    "text": "are original enough and are\nnot part of the training set? That's a great question. I actually don't know the\nanswer to that question",
    "start": "2823660",
    "end": "2830590"
  },
  {
    "text": "for this particular analysis\noff the top of my head. I believe that these\nare not memorized.",
    "start": "2830590",
    "end": "2839470"
  },
  {
    "text": "One simple thing you can do,\nat least, for some things that occur frequently is\nlike you can look at the distribution of the loss\nfor a model on its own samples.",
    "start": "2839470",
    "end": "2848860"
  },
  {
    "text": "And at least, for things that\nare very clearly memorized,",
    "start": "2848860",
    "end": "2854350"
  },
  {
    "text": "usually, of course, probably\nthey occur frequently in the training set,\nbut also the loss tends to be like much, much\nlower on memorized samples.",
    "start": "2854350",
    "end": "2861430"
  },
  {
    "text": "Because I mean, you\ncan sort of intuitively understand this\nbecause if there's 100 words that are exactly\nverbatim sampled out,",
    "start": "2861430",
    "end": "2868960"
  },
  {
    "text": "and you're sampling at sort\nof temperature equals 1, then all of the next\nword predictions have to be extremely,\nextremely confident.",
    "start": "2868960",
    "end": "2875497"
  },
  {
    "text": "And that means the loss\nhas to be super low. So I mean, just\ninformally, something that I've done to just get rid\nof memorized samples is compute",
    "start": "2875497",
    "end": "2883280"
  },
  {
    "text": "the loss. And usually, you'll just\nsee a pretty clear bimodal, where it'll will be a few\nmemorized examples and then things that aren't.",
    "start": "2883280",
    "end": "2889299"
  },
  {
    "text": "That's a simple thing\nyou can do to check. You can also, of course,\ndo deduplication, I don't remember off\nthe top of my head",
    "start": "2889300",
    "end": "2895090"
  },
  {
    "text": "what deduplication\nwas done here, though.  On the downstream\ntasks such as this.",
    "start": "2895090",
    "end": "2901997"
  },
  {
    "text": "Does the one [INAUDIBLE]\nscaling laws. Did you look at\n[INAUDIBLE] context and adversarial context.",
    "start": "2901997",
    "end": "2909630"
  },
  {
    "text": "I don't think I have\nanything particularly clear to say about that.",
    "start": "2909630",
    "end": "2915510"
  },
  {
    "text": "I mean, these evals,\nI think, are not adversarial in the\nsense that they're just few shot evaluations\nwith some fixed data set.",
    "start": "2915510",
    "end": "2924640"
  },
  {
    "text": "There are a large number\nof different kinds of adversarial\ndata sets out there for reasoning, for commonsense\nknowledge, for truthfulness.",
    "start": "2924640",
    "end": "2934590"
  },
  {
    "text": "I mean, there's like\nfor example truthful QA. Famously, as an\nexample, where there aren't any trends like\nthis and arguably,",
    "start": "2934590",
    "end": "2940950"
  },
  {
    "text": "the trends go downward. Though, it depends on your\ntraining distribution. And some models\nactually do improve.",
    "start": "2940950",
    "end": "2946260"
  },
  {
    "text": "So I think that's a\ncomplicated question. I think it's hard\nto find examples, where the trends go down.",
    "start": "2946260",
    "end": "2951900"
  },
  {
    "text": "I don't think it's easy,\nbut these do exist. Any other questions?",
    "start": "2951900",
    "end": "2958240"
  },
  {
    "start": "2958240",
    "end": "2963810"
  },
  {
    "text": "Great.  So I guess, I'll sort of end\nby summarizing some lessons",
    "start": "2963810",
    "end": "2973099"
  },
  {
    "text": "that you might draw kind of\npretty practically for research from this. And then I can either\nopen it up for questions",
    "start": "2973100",
    "end": "2980210"
  },
  {
    "text": "or I can always talk\ninfinitely long. I've been a professor for\nlike 10 years of my life,",
    "start": "2980210",
    "end": "2986150"
  },
  {
    "text": "so I can just talk forever. But I'll sort of end after\ntalking about some lessons.",
    "start": "2986150",
    "end": "2993140"
  },
  {
    "text": "So I think one lesson that\nkind of I draw from this is that kind of scanning over\nsome of the important inputs",
    "start": "2993140",
    "end": "2999470"
  },
  {
    "text": "to your training process is\njust a pretty useful thing to do when you're\ndoing ML research.",
    "start": "2999470",
    "end": "3004780"
  },
  {
    "text": "And it's sort of\ntypically very cheap. It's cheap because\ngenerally, most things",
    "start": "3004780",
    "end": "3010840"
  },
  {
    "text": "vary in important way\non a log scale or sort of on a geometric scale. However, you want to say it.",
    "start": "3010840",
    "end": "3017200"
  },
  {
    "text": "And that means that if\nyou're training with the data set of size d, maybe you\nshould also train with d over 2",
    "start": "3017200",
    "end": "3022990"
  },
  {
    "text": "and d over 4 and d over\n8 or something like that. And if you sum that\ngeometric series, you get 2d.",
    "start": "3022990",
    "end": "3028480"
  },
  {
    "text": "So you sort of-- I mean, you made your training\nprocess twice as expensive in some sense, but it's\nnot really a big change",
    "start": "3028480",
    "end": "3036550"
  },
  {
    "text": "in what you have to do. But you can often\nlike learn a lot about what's going on by\ndoing these kinds of scans.",
    "start": "3036550",
    "end": "3042720"
  },
  {
    "text": "And so I mean, this is\nan example of some data that I didn't show\nearlier, but something",
    "start": "3042720",
    "end": "3047867"
  },
  {
    "text": "you might wonder\nabout is what happens if you scan over a\ndata set size and model size at the same time.",
    "start": "3047867",
    "end": "3053510"
  },
  {
    "text": "And it turns out there's\nsome very simple trends that you can model, in\nthat case too that tell you about things like overfitting.",
    "start": "3053510",
    "end": "3059890"
  },
  {
    "text": "And I mean, if you\ncare about overfitting, then this tells\nyou about something like how big you have to make\nyour data set for a given model",
    "start": "3059890",
    "end": "3065342"
  },
  {
    "text": "size to avoid overfitting\nbeing a significant problem. So that you can answer all\nkinds of questions like that.",
    "start": "3065342",
    "end": "3070790"
  },
  {
    "text": "And I, at least, find that\nthis is kind of useful, and it's nice for learning\nthings about behavior.",
    "start": "3070790",
    "end": "3079310"
  },
  {
    "text": "And I think alongside that, I\nthink, this is sort of a joke. This isn't real.",
    "start": "3079310",
    "end": "3085210"
  },
  {
    "text": "This is sort of making fun\nof a large number of machine learning papers\nthat you might see. I think a lot of\nmachine learning papers",
    "start": "3085210",
    "end": "3091869"
  },
  {
    "text": "have tables like this. And it's sort of hard to tell\nfrom like this kind of table. Obviously, I'm making fun.",
    "start": "3091870",
    "end": "3097420"
  },
  {
    "text": "But I think it's\nnot so unrealistic. Like did the technique that\nwent into our model really improve on other\nthings that happened?",
    "start": "3097420",
    "end": "3105020"
  },
  {
    "text": "And I think that this\nkind of plot, at least, for me, is a much more\nconvincing statement, that like well, clearly, transformers\nare just better than LSTMs.",
    "start": "3105020",
    "end": "3113350"
  },
  {
    "text": "So the slogan here is sort\nof success for new techniques",
    "start": "3113350",
    "end": "3118360"
  },
  {
    "text": "if your goal is to sort\nof improve a model-- if that is your goal, then I\nthink, it's at least, to me,",
    "start": "3118360",
    "end": "3126280"
  },
  {
    "text": "much more convincing\nand kind of clear what's going on if\nyou see these trends. Maybe I have another\nslide making fun of this.",
    "start": "3126280",
    "end": "3132599"
  },
  {
    "text": "Yeah. So I mean, I think this\nis a thing that I actually see very often in research,\nis that you come up",
    "start": "3132600",
    "end": "3139150"
  },
  {
    "text": "with some new\nidea, and you see-- like you first do the\ncheapest, easiest experiment,",
    "start": "3139150",
    "end": "3146800"
  },
  {
    "text": "and you see, wow, my new\nidea improved performance. I'm really excited. Everyone should adopt this.",
    "start": "3146800",
    "end": "3154120"
  },
  {
    "text": "But then you make\nsome plot like this and you sort of say\noh, OK, I guess, it doesn't really\nmatter that much at all.",
    "start": "3154120",
    "end": "3160385"
  },
  {
    "text": "And I think this is\nactually a common-- I mean, I think we all\nhave all sorts of ideas. I mean, people fall asleep at\nnight and they can't sleep,",
    "start": "3160385",
    "end": "3168940"
  },
  {
    "text": "and then they wake up and\nthey have ideas and like, oh, I'm going to go try this. We all do it.",
    "start": "3168940",
    "end": "3174280"
  },
  {
    "text": "But oftentimes, they don't work. And I think this is sort\nof useful for understanding whether your idea\nreally, really works.",
    "start": "3174280",
    "end": "3180670"
  },
  {
    "text": "And I mean, if all\nyou're ever going to do is train this model,\nthen your idea did work. But I think that like there's\nsort of an expectation",
    "start": "3180670",
    "end": "3190420"
  },
  {
    "text": "that probably people will be\nusing bigger computers to train larger models in the future.",
    "start": "3190420",
    "end": "3195530"
  },
  {
    "text": "And so the idea\nis that are really going to have a huge impact\nor ones that sort of point in the opposite direction. I've even seen ideas\nwhere on small models,",
    "start": "3195530",
    "end": "3202518"
  },
  {
    "text": "they make no difference at all. But on larger models,\nthey do better. ",
    "start": "3202518",
    "end": "3208480"
  },
  {
    "text": "And so these kinds of\ntrends, I think, are useful. And they're certainly\nuseful to think about.",
    "start": "3208480",
    "end": "3213970"
  },
  {
    "text": "Another point that\nI find useful--",
    "start": "3213970",
    "end": "3219010"
  },
  {
    "text": "I think, it's not\nsort of obvious and maybe you shouldn't\ntrust it completely",
    "start": "3219010",
    "end": "3224050"
  },
  {
    "text": "is that I tend to think-- I mean, because I've sort of\nswallowed my own Kool-Aid,",
    "start": "3224050",
    "end": "3229420"
  },
  {
    "text": "that if something\nworks, then it should scale fairly predictably. That's not always true, but for\nthings that you can measure,",
    "start": "3229420",
    "end": "3237700"
  },
  {
    "text": "that are very close to\nyour optimization target, if sort of your training\nprocess, your hyperparameters,",
    "start": "3237700",
    "end": "3245349"
  },
  {
    "text": "et cetera are all\nkind of set up well, then I tend to think\nthat you should see some kind of predictable trend.",
    "start": "3245350",
    "end": "3251960"
  },
  {
    "text": "And if that trend goes away,\nthen, I mean, maybe that's just",
    "start": "3251960",
    "end": "3257260"
  },
  {
    "text": "exactly what's\ntrue, but I think, often, it means that\nthere's something broken about what's going on. Maybe your numerics are broken\nand you need higher precision",
    "start": "3257260",
    "end": "3264760"
  },
  {
    "text": "in some part of your model. Maybe there's some bottleneck\nyou hadn't thought of. So I mean, this\nis also an example",
    "start": "3264760",
    "end": "3271470"
  },
  {
    "text": "that predictable\nscaling kind of can be found all over the place. So I just think this\nis sort of neat.",
    "start": "3271470",
    "end": "3279190"
  },
  {
    "text": "So if you just train these\nextremely naive, very stupid, multimodal models where you\nuse a decoder-only transformer",
    "start": "3279190",
    "end": "3285720"
  },
  {
    "text": "to either model the\ntext based on the image or model the image\nbased on the text, then you can measure a sort of\nempirical mutual information",
    "start": "3285720",
    "end": "3294480"
  },
  {
    "text": "between the image and the text. How much information\ndid the image give you about the words\nin the sense of sort",
    "start": "3294480",
    "end": "3301957"
  },
  {
    "text": "of Shannon information and/or\nconversely how much information did the text give\nyou about the image?",
    "start": "3301957",
    "end": "3307740"
  },
  {
    "text": "And this is also a place where-- I mean, this is very close\nto the optimization target. The whole point\nof the multi-model",
    "start": "3307740",
    "end": "3313777"
  },
  {
    "text": "is to get this information. And you see that there's some\npredictable scaling going on,",
    "start": "3313777",
    "end": "3319680"
  },
  {
    "text": "where larger models are getting\nmore information about one part of the distribution\nfor the other.",
    "start": "3319680",
    "end": "3327960"
  },
  {
    "text": "But I think this is\nsort of a general thing that you should expect\nin model training.",
    "start": "3327960",
    "end": "3336850"
  },
  {
    "text": "And so maybe to\nsort of summarize, maybe even bigger\npicture implications,",
    "start": "3336850",
    "end": "3343000"
  },
  {
    "text": "I think, that these\nkinds of results suggest that it may not be\nthe best or the smartest",
    "start": "3343000",
    "end": "3349960"
  },
  {
    "text": "or the most interesting way\nto make better ML models. Maybe it won't be the way\nthat happens in the future.",
    "start": "3349960",
    "end": "3356420"
  },
  {
    "text": "But at least, I think these\nresults kind of suggest that there aren't any really\nhard conceptual barriers preventing people from\ntraining significantly more",
    "start": "3356420",
    "end": "3364630"
  },
  {
    "text": "powerful models of all\nkinds, including, of course, language models in AI research.",
    "start": "3364630",
    "end": "3373780"
  },
  {
    "text": "I think, certainly,\nmy perspective, originally, as a physicist, sort\nof coming to machine learning",
    "start": "3373780",
    "end": "3382760"
  },
  {
    "text": "sort of fresh, new way five\nyears ago, is that, I mean, this is sort of one\nset of abstractions",
    "start": "3382760",
    "end": "3389260"
  },
  {
    "text": "for thinking about kind of\nwhat's going on in AI research, that if you're going to be\ntraining fairly large models",
    "start": "3389260",
    "end": "3397733"
  },
  {
    "text": "and you want them to do well,\nthat's the thing that you're going to do, then you\nprobably want your models",
    "start": "3397733",
    "end": "3403299"
  },
  {
    "text": "to sort of be scaling well in\nterms of their performance. And I think this framework of\nmaybe there's a bottleneck,",
    "start": "3403300",
    "end": "3409460"
  },
  {
    "text": "but if you remove\nthe bottleneck, then you'll just continue\nto see further progress I found useful.",
    "start": "3409460",
    "end": "3417309"
  },
  {
    "text": "I think, another point that-- well, maybe I'll make\nthis point at the end.",
    "start": "3417310",
    "end": "3422600"
  },
  {
    "text": "Another point is that,\nyeah, scaling laws are just sort of all over the place. And they can help\nyou to sort of maybe",
    "start": "3422600",
    "end": "3428829"
  },
  {
    "text": "organize your research a bit. And then, I mean, maybe the most\ninteresting point conceptually",
    "start": "3428830",
    "end": "3434200"
  },
  {
    "text": "though, is that it seems like if\nyou believe this kind of story that it seems like\nmany domains of ML",
    "start": "3434200",
    "end": "3442090"
  },
  {
    "text": "are kind of surprisingly\nsimple and universal, things that you might\nnot have thought are the same or more similar\nthan they are different.",
    "start": "3442090",
    "end": "3450800"
  },
  {
    "text": "And of course, this is\nalso a fascinating thing to try to understand. So I mean, I was a theoretical\nphysicist for most of my life,",
    "start": "3450800",
    "end": "3458470"
  },
  {
    "text": "so I mostly tried to like\nunderstand things that seemed extremely\nesoteric and weird",
    "start": "3458470",
    "end": "3463720"
  },
  {
    "text": "and why would anyone\ncare about them. This is a thing that I think,\nprobably, everyone in this room",
    "start": "3463720",
    "end": "3470290"
  },
  {
    "text": "kind of cares about, like\ncan AI models write, can they communicate in language. And these kinds of trends\nare really, really nice.",
    "start": "3470290",
    "end": "3478462"
  },
  {
    "text": "They're the kind\nof trends that you might see in a very\ncontrolled physics experiment or something.",
    "start": "3478462",
    "end": "3484223"
  },
  {
    "text": "And yet, they're coming\nout of something very, very noisy and random\nlike predicting language data on the internet.",
    "start": "3484223",
    "end": "3490390"
  },
  {
    "text": "So I think it's very\ninteresting to think about why are these kinds of trends true?",
    "start": "3490390",
    "end": "3496300"
  },
  {
    "text": "What is the underlying\ntheory or science here that makes these trends true?",
    "start": "3496300",
    "end": "3501890"
  },
  {
    "text": "Can we predict it? Can we refine those predictions? Can we understand why, when\nthis does and doesn't occur?",
    "start": "3501890",
    "end": "3508100"
  },
  {
    "text": "Another question is sort of\nthis is some exponent here. This is a straight line,\nbut the straight line represents a power law\nwith a particular exponent.",
    "start": "3508100",
    "end": "3514990"
  },
  {
    "text": "Why that exponent? For language, it's\nlike 0.08 or so. Why 0.08 and not\n0.2 or 0.4 or 0.001?",
    "start": "3514990",
    "end": "3523693"
  },
  {
    "text": "I think there are all\nsorts of questions here. When you see data that\nhas a very clear trend, it's very interesting\nto understand",
    "start": "3523693",
    "end": "3529090"
  },
  {
    "text": "to try to think about why is\nsomething so simple happening.",
    "start": "3529090",
    "end": "3534280"
  },
  {
    "text": "And I'll sort of\nleave you with that. ",
    "start": "3534280",
    "end": "3546650"
  },
  {
    "text": "I'll ask some questions. Yeah. Have you thought any about\nhow these scaling laws",
    "start": "3546650",
    "end": "3554380"
  },
  {
    "text": "probably for like human being? So I mean, your\npicture is essentially make everything bigger, avoid\nbottlenecks, and we'll be good.",
    "start": "3554380",
    "end": "3563530"
  },
  {
    "text": "Whereas I guess, human beings. So you're good on the\nnumber of parameters,",
    "start": "3563530",
    "end": "3569940"
  },
  {
    "text": "but there are still\nseveral orders of magnitude headroom there. But it seems like you're not\nvery good about [INAUDIBLE]..",
    "start": "3569940",
    "end": "3580320"
  },
  {
    "text": "So human use is\nvery constrained, both in the [INAUDIBLE]\nbecause of the slow processing,",
    "start": "3580320",
    "end": "3586750"
  },
  {
    "text": "but also because\nwith the [INAUDIBLE] trying to [INAUDIBLE] using\nmost of their parameters most of the time.",
    "start": "3586750",
    "end": "3594150"
  },
  {
    "text": "And there is a little bit\ncomplex with, I guess, if we get a ton of live video\ndata coming at us all the time.",
    "start": "3594150",
    "end": "3601029"
  },
  {
    "text": "But certainly, if\nyou are thinking about the amount of\nlanguage data we get,",
    "start": "3601030",
    "end": "3609630"
  },
  {
    "text": "the fully competent\nlanguage users are sort of three orders of\nmagnitude down [INAUDIBLE]..",
    "start": "3609630",
    "end": "3615230"
  },
  {
    "text": "GPT-3 is now [INAUDIBLE]. But yet something good seems\nto happen in human abilities",
    "start": "3615230",
    "end": "3621300"
  },
  {
    "text": "to learn. Any thoughts on that? I mean-- I think it's a\nfantastic question.",
    "start": "3621300",
    "end": "3627070"
  },
  {
    "text": "I don't have anything to say\nthat isn't quite speculative. So I mean, I don't have any\ngood answer to the question.",
    "start": "3627070",
    "end": "3632950"
  },
  {
    "text": "I think it's a great question. I guess one thing that\nseems like it's true is that sort of the factor\nof 1,000 you mentioned",
    "start": "3632950",
    "end": "3638102"
  },
  {
    "text": "seems pretty common. I mean, my impression\nis that AlphaGo probably plays like 1,000\ntimes more games",
    "start": "3638102",
    "end": "3644520"
  },
  {
    "text": "when it trains than\nlike a Go master does.",
    "start": "3644520",
    "end": "3650110"
  },
  {
    "text": "I think this is a\npretty common factor to see in a lot of\nlike ML contexts. But I have no idea why it is.",
    "start": "3650110",
    "end": "3656280"
  },
  {
    "text": "I don't know if it's\nthat evolution optimized us to learn fast, if we have\nsome hard coded information, if the sort of multimodal\ninputs that we have help a lot.",
    "start": "3656280",
    "end": "3664880"
  },
  {
    "text": "You might imagine that when you\nhave a system that's already pretty smart, reinforcement\nlearning or active learning",
    "start": "3664880",
    "end": "3670377"
  },
  {
    "text": "of some form becomes\nmore and more important because when these language\nmodels or a person,",
    "start": "3670377",
    "end": "3675510"
  },
  {
    "text": "like if I read a\nphysics textbook, I don't really learn a\nlot in a certain sense",
    "start": "3675510",
    "end": "3681420"
  },
  {
    "text": "because I already\nlearned physics. And I think the same is\nprobably true for these models. So as the models get\nsmarter, this sort",
    "start": "3681420",
    "end": "3687690"
  },
  {
    "text": "of very dumb next\nword prediction task is giving you less\nand less information. But you might expect to get\nmore and more information if you",
    "start": "3687690",
    "end": "3695130"
  },
  {
    "text": "did something more active. I can continue to speculate,\nbut I don't I don't really",
    "start": "3695130",
    "end": "3700559"
  },
  {
    "text": "know anything about-- I don't have anything sort of\nwell-established to tell you.",
    "start": "3700560",
    "end": "3707352"
  },
  {
    "text": "It's a great question. Fair enough. And in case of the\ntransformers and LSTMs, they always [INAUDIBLE]\nhave that at some point.",
    "start": "3707353",
    "end": "3713440"
  },
  {
    "text": "Yeah. But it's really quite down. The transformers\nare a bit better, but they are sort of similar.",
    "start": "3713440",
    "end": "3719110"
  },
  {
    "text": "It seems like even the ability\nto learn relative to the amount",
    "start": "3719110",
    "end": "3725480"
  },
  {
    "text": "that they compute seems\non a very different place on the graph. Yeah, I think\nthat's absolutely--",
    "start": "3725480",
    "end": "3730799"
  },
  {
    "text": "I think it just-- I mean, the sample efficiency\nof these models is not similar. I mean, another way\nof saying is just",
    "start": "3730800",
    "end": "3735840"
  },
  {
    "text": "that if you got into AI research\nto understand the human brain, it's very unclear whether we're\nmaking any progress on that.",
    "start": "3735840",
    "end": "3743460"
  },
  {
    "text": "But if we just want\nto sort of-- yeah, for a lot of these\ntasks that doesn't--",
    "start": "3743460",
    "end": "3748470"
  },
  {
    "text": "we don't seem to have\nto solve the brain to solve AI, surprisingly.",
    "start": "3748470",
    "end": "3754039"
  },
  {
    "text": "Other questions? Any other questions? Yeah. You talk about\nscaling the model.",
    "start": "3754040",
    "end": "3759840"
  },
  {
    "text": "But that also\nimplies that you have to scale much data\non your [INAUDIBLE] in terms of having\na [INAUDIBLE]..",
    "start": "3759840",
    "end": "3766596"
  },
  {
    "text": " Can you [INAUDIBLE]\non this because I",
    "start": "3766596",
    "end": "3772940"
  },
  {
    "text": "worry that when we have\nlarger and larger models, are we going to have like\ngood [INAUDIBLE] data",
    "start": "3772940",
    "end": "3779850"
  },
  {
    "text": "that we can use\nthroughout pre-training? Because like if we\nscrape the Internet there's a lot of things on there\nthat we probably don't want",
    "start": "3779850",
    "end": "3786660"
  },
  {
    "text": "and don't like probably\n[INAUDIBLE] that for model if training a model trying to\npredict human-like languages.",
    "start": "3786660",
    "end": "3795540"
  },
  {
    "text": "And also, there's also like\nthings that we probably have never really [INAUDIBLE].",
    "start": "3795540",
    "end": "3801262"
  },
  {
    "text": "Like knowledge actually\nwould have that, probably, aren't necessarily\nbeing written for. And things like [INAUDIBLE].",
    "start": "3801262",
    "end": "3808510"
  },
  {
    "text": "Sure. Sure. These are all great questions. So I mean, sort of\nearly on, I commented",
    "start": "3808510",
    "end": "3815400"
  },
  {
    "text": "on some sources of data. I mean, you're certainly\ncorrect about quality. I think in terms of\nquantity, I mean,",
    "start": "3815400",
    "end": "3821790"
  },
  {
    "text": "I don't think anyone has like a\ndigitized Library of Congress. But I think if you did, that\nwould be like, I don't know,",
    "start": "3821790",
    "end": "3828510"
  },
  {
    "text": "maybe 10x bigger than the\ntraining set for GPT-3. So there's a sense in\nwhich there's probably quite a lot of still quite high\nquality data that isn't in use.",
    "start": "3828510",
    "end": "3837690"
  },
  {
    "text": "I don't know whether\nit will ever be in use. It's a complicated question. And then if you are\nwilling to sort take",
    "start": "3837690",
    "end": "3843210"
  },
  {
    "text": "all of this garbage\non the internet or try to filter\nthat garbage down,",
    "start": "3843210",
    "end": "3848730"
  },
  {
    "text": "I think I don't know how\naccurate this estimate is, but in order of\nmagnitude level, you can get something like\n10 to 15 words, which",
    "start": "3848730",
    "end": "3855510"
  },
  {
    "text": "is 1,000 times bigger. And of course, if\nyou find any kind of intelligent way of filtering,\nthen if you can filter down",
    "start": "3855510",
    "end": "3862859"
  },
  {
    "text": "to 0.1% of that and take\nthe 0.1% that's best, then you do still\nhave a lot of data. So I think for\nlanguage modeling,",
    "start": "3862860",
    "end": "3868445"
  },
  {
    "text": "there's definitely\nstill some headroom. But this is certainly\na constraint.",
    "start": "3868445",
    "end": "3876232"
  },
  {
    "text": "And there are other kinds\nof data distributions where you'll run out sooner. I mean, in terms of--",
    "start": "3876232",
    "end": "3883700"
  },
  {
    "text": "yeah, I mean, of course, there\nare all sorts of other things one can explore. One can explore\nmultimodal models.",
    "start": "3883700",
    "end": "3888779"
  },
  {
    "text": "One can switch to a different\nkind of loss function that is more interactive or\nactually accomplishing a task.",
    "start": "3888780",
    "end": "3896430"
  },
  {
    "text": "But I think for pure\nlanguage modeling, it seems like there's\nat least some room left. And if you think that your\nmodel size increases sort of--",
    "start": "3896430",
    "end": "3904093"
  },
  {
    "text": "if you think you can\nincrease your model size by a factor of 100\nand increase your data set size by a factor of 10,\nwhich is sort of like roughly",
    "start": "3904093",
    "end": "3913579"
  },
  {
    "text": "what this is saying. If you believe that,\nthen you can still scale up your model size\na lot and have probably",
    "start": "3913580",
    "end": "3919670"
  },
  {
    "text": "plenty of data. But yeah, I mean, you\ncouldn't sort of do this stuff",
    "start": "3919670",
    "end": "3925580"
  },
  {
    "text": "without the internet. Yeah. ",
    "start": "3925580",
    "end": "3931678"
  },
  {
    "text": "Did you want-- Sure, yeah. In terms of bottlenecks\nfor [INAUDIBLE] task",
    "start": "3931678",
    "end": "3938690"
  },
  {
    "text": "over time, are you more\noptimistic about the hardware",
    "start": "3938690",
    "end": "3944119"
  },
  {
    "text": "system for enabling\nmuch larger models on the same architectures\nor improvement",
    "start": "3944120",
    "end": "3949650"
  },
  {
    "text": "or architectural improvements\nlike the LSTM and transformers?",
    "start": "3949650",
    "end": "3955579"
  },
  {
    "text": "I guess, I mean, I think I'm\nsort of optimistic about both. I think that my understanding\nin sort of the zeroth order",
    "start": "3955580",
    "end": "3962775"
  },
  {
    "text": "understanding of the\nhardware situation is that like connecting together\nGPUs and GPU-like objects,",
    "start": "3962775",
    "end": "3969349"
  },
  {
    "text": "works pretty well. And that like\ninterconnection speeds",
    "start": "3969350",
    "end": "3974420"
  },
  {
    "text": "are increasing and can\nincrease pretty easily. So I think that\nyou don't need one chip to run your entire model.",
    "start": "3974420",
    "end": "3980420"
  },
  {
    "text": "You can distribute\nyour model over many, many, many accelerators. And I think you can\ndo that if you're",
    "start": "3980420",
    "end": "3987980"
  },
  {
    "text": "willing to pay for those\naccelerators, et cetera, then I think you can do that. Architectural\nimprovements, I think,",
    "start": "3987980",
    "end": "3995210"
  },
  {
    "text": "I would say sort of typically,\nI haven't been super excited about architectural\nimprovements.",
    "start": "3995210",
    "end": "4001720"
  },
  {
    "text": "But I think there\nwill continue to be architectural improvements. I think that sort of\nwhenever you do something",
    "start": "4001720",
    "end": "4008050"
  },
  {
    "text": "for the first time or even\njust like whenever you train a really big model\nfor the first time, you sort of don't do it\nin the best possible way.",
    "start": "4008050",
    "end": "4014200"
  },
  {
    "text": "And there's a lot of all\nsorts of different kinds of improvements. Maybe there are sort of\nnon-incremental improvements",
    "start": "4014200",
    "end": "4020117"
  },
  {
    "text": "that will look like big jumps. So yeah, I think,\nthere will be both. So yeah, I mean, there's a\nsense in which if all you did",
    "start": "4020117",
    "end": "4028150"
  },
  {
    "text": "was look at this plot and\njust try to continue it, that might be an\nunderestimate of progress",
    "start": "4028150",
    "end": "4033460"
  },
  {
    "text": "that the field is going\nto make because there will be improvements in\narchitecture and algorithms",
    "start": "4033460",
    "end": "4040234"
  },
  {
    "text": "and things like that. ",
    "start": "4040235",
    "end": "4045840"
  },
  {
    "text": "So does [INAUDIBLE]\non the data set because that's like\ncontinuing the question",
    "start": "4045840",
    "end": "4053000"
  },
  {
    "text": "from the previous\nquestion about the data. We can theoretically\nhave like a data",
    "start": "4053000",
    "end": "4058930"
  },
  {
    "text": "set is similar [INAUDIBLE]\njust like how we do data with written\ncommunication for like maybe",
    "start": "4058930",
    "end": "4064430"
  },
  {
    "text": "big [INAUDIBLE] steps\n[INAUDIBLE] somehow. And then we can, theoretically,\nhave increasing count of data that's scaling\nwell and won't fall apart",
    "start": "4064430",
    "end": "4073244"
  },
  {
    "text": "or [INAUDIBLE] by this type\nof like almost synthetic data. I think that's a great question.",
    "start": "4073245",
    "end": "4079920"
  },
  {
    "text": "I think the sort\nof simplest version of this-- well, a\nsimple version of it that I think is probably\nimportant and increasingly",
    "start": "4079920",
    "end": "4086420"
  },
  {
    "text": "important to sort of just\nreinforcement learning. Reinforcement learning\nin a certain sense is a situation, where you\ngenerate your own data.",
    "start": "4086420",
    "end": "4092270"
  },
  {
    "text": "Because if you have a\nlanguage model doing RL, then it writes\nsomething and then you're training on that data. So I definitely do\nthink that will sort of",
    "start": "4092270",
    "end": "4100240"
  },
  {
    "text": "augment data and mean\nthat there will be other avenues for improvement.",
    "start": "4100240",
    "end": "4105409"
  },
  {
    "text": "Literal data augmentation itself\nalso seems plausible to me. I think it's not happening a\nlot because there still is more",
    "start": "4105410",
    "end": "4111620"
  },
  {
    "text": "just language data out there. ",
    "start": "4111620",
    "end": "4116689"
  },
  {
    "text": "Yeah. So I have two questions. First one's kind of quick. Just coming from a non-computer\nscience background,",
    "start": "4116689",
    "end": "4124384"
  },
  {
    "text": "I was interested in how-- what about, especially\nwith the way we feel [INAUDIBLE] really\nexcited about the physics.",
    "start": "4124385",
    "end": "4132350"
  },
  {
    "text": "And the second part\nis, in your research before this, I'm\nsure you dealt a lot",
    "start": "4132350",
    "end": "4137750"
  },
  {
    "text": "with different types\nof scaling laws and that kind of\nstuff [INAUDIBLE]",
    "start": "4137750",
    "end": "4143156"
  },
  {
    "text": "[LAUGHING] How do I turn [INAUDIBLE]\nthis type of stuff?",
    "start": "4143157",
    "end": "4149568"
  },
  {
    "text": "Are there any findings that you\nfound particularly surprising, or unexpected?",
    "start": "4149569",
    "end": "4154759"
  },
  {
    "text": "Just with your past experiences. At least when I look\nat this, a lot of it looks like [INAUDIBLE] For you,\nother than this, or other stuff",
    "start": "4154760",
    "end": "4166460"
  },
  {
    "text": "that you're doing, with your\nresearch stuff right now, is there anything that\nis kind of like this",
    "start": "4166460",
    "end": "4172910"
  },
  {
    "text": "is [INAUDIBLE] Or just\nparticularly surprising?",
    "start": "4172910",
    "end": "4178700"
  },
  {
    "text": "I think to me, the\nmost surprising thing of these sorts of\nresults was probably",
    "start": "4178700",
    "end": "4186410"
  },
  {
    "text": "that there is a very, very\nprecise trend it seems. I mean, I think this is\nsort of an unusual thing.",
    "start": "4186410",
    "end": "4195170"
  },
  {
    "text": "And I think when I\nsaw that, I thought it was a really big deal. I think that usually it's not\ntrue in most many things you",
    "start": "4195170",
    "end": "4204570"
  },
  {
    "text": "plot. I mean, obviously there\nare other plots that don't show this kind of trend,\neven if they're reasonable. I mean, I don't know, there's\nsort of a trend in TriviaQA,",
    "start": "4204570",
    "end": "4212230"
  },
  {
    "text": "but I don't really\nknow what that means. But I think the fact\nthat there's something seemingly very precise is--",
    "start": "4212230",
    "end": "4219380"
  },
  {
    "text": "I view that as a very\nintriguing entry point to try to dig into something,\nbecause it means that there's",
    "start": "4219380",
    "end": "4225890"
  },
  {
    "text": "probably some deeper reason. And then the fact that\nit seems fairly universal across data distributions, again\nsuggests something like that.",
    "start": "4225890",
    "end": "4235010"
  },
  {
    "text": "Yeah, the main difference\nbetween data distributions is these exponents in the\nscaling laws are different.",
    "start": "4235010",
    "end": "4240559"
  },
  {
    "text": "In terms of coming\nfrom physics, I mean, I think I got into\na lot of this stuff",
    "start": "4240560",
    "end": "4246620"
  },
  {
    "text": "partly because I'm\nfairly mercurial, and I was interested, and\na lot of other friends I had were interested.",
    "start": "4246620",
    "end": "4252170"
  },
  {
    "text": "And so we sort of studied\nit, and it went from there. I had friends, et cetera.",
    "start": "4252170",
    "end": "4258495"
  },
  {
    "text": "But I mean, from\nanother point of view, I think I got involved in\nit for really weird reasons. Perhaps in the sense\nthat I just know",
    "start": "4258495",
    "end": "4265040"
  },
  {
    "text": "a lot of people who were already\nin sort of, I don't know, 2015, talking about things\nlike, wow, how much better",
    "start": "4265040",
    "end": "4273200"
  },
  {
    "text": "is AI going to get? What are the implications\ngoing to be for the world? Is this going to keep\nimproving at a drastic clip?",
    "start": "4273200",
    "end": "4280909"
  },
  {
    "text": "What are we going to\ndo to sort of make sure that these models are\naligned with human values, to use the kind of usual sort\nof phrase that's now used?",
    "start": "4280910",
    "end": "4290180"
  },
  {
    "text": "And I thought these people\nwere weird and crazy, even though they\nwere friends of mine. And I said, \"Oh,\nthis is really dumb,",
    "start": "4290180",
    "end": "4297150"
  },
  {
    "text": "I don't think that these\nAI models are really something to worry about.\" But I was still interested,\nand sort of I was like, well,",
    "start": "4297150",
    "end": "4305690"
  },
  {
    "text": "smart people I know think that\nAI is improving very rapidly, and that might have\na lot of impacts,",
    "start": "4305690",
    "end": "4313260"
  },
  {
    "text": "and might require a lot of\nsort of caution and thought and work to sort\nof make it safe.",
    "start": "4313260",
    "end": "4318340"
  },
  {
    "text": "And so that was actually\na significant motivation for me getting involved. It was a mixture of there being\na lot of potentially really",
    "start": "4318340",
    "end": "4326460"
  },
  {
    "text": "intellectually interesting\nquestions, liking to sort of switch\nfields every few years,",
    "start": "4326460",
    "end": "4332400"
  },
  {
    "text": "and friends of mine being\nvery kind of concerned about this question. And yeah.",
    "start": "4332400",
    "end": "4339150"
  },
  {
    "text": "That was sort of what\nbrought me in, yeah. Out of everything you've\nseen and studied here,",
    "start": "4339150",
    "end": "4345350"
  },
  {
    "text": "[INAUDIBLE] scale [INAUDIBLE]\nWhat factor have you found",
    "start": "4345350",
    "end": "4351200"
  },
  {
    "text": "has the most potential\nto throw off this scaling law from its natural force?",
    "start": "4351200",
    "end": "4358610"
  },
  {
    "text": "I mean, if we go back to sort\nof very basic ML ingredients of,",
    "start": "4358610",
    "end": "4364670"
  },
  {
    "text": "what are these things-- there's a sense in which\nthis is all you're doing, you choose one of each\nof these five things.",
    "start": "4364670",
    "end": "4372170"
  },
  {
    "text": "I would guess that\nwhat the objective is, is most likely to\nsort of change things, in the sense that\npredicting the next word",
    "start": "4372170",
    "end": "4379010"
  },
  {
    "text": "is really sort of one of the\nlaziest, sort of dumbest things you can do. And I mean, there are\nall sorts of things.",
    "start": "4379010",
    "end": "4387849"
  },
  {
    "text": "So it's really just\nchosen because you want to be able to\ncompute, you wanted to be able to do\nbackprop, and so you want to be able to get\nsome differentiable thing.",
    "start": "4387850",
    "end": "4394923"
  },
  {
    "text": "You want to be able\nto get a lot of data for which you can compute\nthis differentiable thing. And so that's the game\nthat you're playing.",
    "start": "4394923",
    "end": "4402889"
  },
  {
    "text": "But I think that you could\nhave other objectives through reinforcement\nlearning, or some other kind",
    "start": "4402890",
    "end": "4409790"
  },
  {
    "text": "of active learning, whatever. I mean, some combination\nof such things. And I would just guess\nthat generally, performance",
    "start": "4409790",
    "end": "4418550"
  },
  {
    "text": "will change a lot more. If you're expecting\nsort of these trends to be very different,\nI would guess they're different if you\nhave a different objective.",
    "start": "4418550",
    "end": "4425667"
  },
  {
    "text": "I think changing the data\ndistribution or the model might also change\nthings but, but I",
    "start": "4425667",
    "end": "4431150"
  },
  {
    "text": "think that the lesson\nthat I personally draw from something like this\nis that even if you found",
    "start": "4431150",
    "end": "4436385"
  },
  {
    "text": "a really revolutionary\nchange that was much better\nthan transformers,",
    "start": "4436385",
    "end": "4441530"
  },
  {
    "text": "it might be kind of equivalent\nto making transformers 10 times bigger. But I'm not sure if that\nwould be as big of a deal",
    "start": "4441530",
    "end": "4449000"
  },
  {
    "text": "as changing the loss. Changing what the objective is. But that's just my\nguess, I have no idea.",
    "start": "4449000",
    "end": "4455090"
  },
  {
    "text": "And of course, this\nparadigm, I think I was trying to be polite. I usually have a picture\nof a grilled cheese",
    "start": "4455090",
    "end": "4461960"
  },
  {
    "text": "here to emphasize sort of\nhow simple and silly this is, rather than this sort of\nvery sophisticated palette",
    "start": "4461960",
    "end": "4469190"
  },
  {
    "text": "of spices. And I mean, maybe\nsomeone will say this isn't the right set\nof ingredients from which",
    "start": "4469190",
    "end": "4475850"
  },
  {
    "text": "to think about\nthings, and there's a different thing you\nshould do, and maybe that will make a big\ndifference as well. But that's sort of\nan unknown unknown.",
    "start": "4475850",
    "end": "4484390"
  },
  {
    "start": "4484390",
    "end": "4489003"
  }
]