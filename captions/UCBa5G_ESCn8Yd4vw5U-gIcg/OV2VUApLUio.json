[
  {
    "start": "0",
    "end": "5760"
  },
  {
    "text": "OK. Great. Welcome to the class. Super, super exciting week with\nsome really exciting topics",
    "start": "5760",
    "end": "15210"
  },
  {
    "text": "for us to cover. So now for the topic for today. We are going to talk\nabout recommender systems,",
    "start": "15210",
    "end": "23009"
  },
  {
    "text": "and we are also going to talk\nabout how can graph learning be applied to recommender systems.",
    "start": "23010",
    "end": "29640"
  },
  {
    "text": "And before I start,\nwe are going to look even what is a recommender\nsystem task, how it's defined,",
    "start": "29640",
    "end": "37020"
  },
  {
    "text": "and how do we evaluate it. But the main motivation\nfor recommendation",
    "start": "37020",
    "end": "42390"
  },
  {
    "text": "is that in this-- mainly in the online world,\nwhere you can basically have huge inventory of things,\nand the inventory of things",
    "start": "42390",
    "end": "51510"
  },
  {
    "text": "that you have is not limited\nby some physical store something that user can\nnavigate or a library.",
    "start": "51510",
    "end": "57810"
  },
  {
    "text": "But you can essentially\nhave unlimited inventory. The problem then\nbecomes, how do people",
    "start": "57810",
    "end": "63060"
  },
  {
    "text": "find things they are looking\nfor or things that they like? And usually, when you say,\nI want people to find,",
    "start": "63060",
    "end": "70060"
  },
  {
    "text": "one option is that\nthey explicitly tell you what they want. And you give it to them. That is a search\nengine-type metaphor.",
    "start": "70060",
    "end": "78280"
  },
  {
    "text": "But another type of\na metaphor is also that you want to see\nwhat things the user has",
    "start": "78280",
    "end": "84310"
  },
  {
    "text": "consumed in the past. And based on that, you want\nto recommend new things that the user was not aware, but\nmight be suitable for the user.",
    "start": "84310",
    "end": "92380"
  },
  {
    "text": "And you can think of anything\nfrom movies in Netflix, to products in Amazon,\nto music, to videos,",
    "start": "92380",
    "end": "99100"
  },
  {
    "text": "to pins, and many, many\nother things, right? And the point of this is\nthat we want these things",
    "start": "99100",
    "end": "105367"
  },
  {
    "text": "to be personalized, right? They should be different\nfor every human because different\nhumans are different.",
    "start": "105367",
    "end": "111430"
  },
  {
    "text": "They have different needs,\ndifferent tastes, and so on. So we want to suggest a small\nnumber of interesting items",
    "start": "111430",
    "end": "118360"
  },
  {
    "text": "to each user. And this is critical from user\nsatisfaction point of view",
    "start": "118360",
    "end": "125830"
  },
  {
    "text": "and the utility they get out of\nthat platform service, whatever",
    "start": "125830",
    "end": "133120"
  },
  {
    "text": "it is. OK. And the way we can model the\nrecommender systems as a graph, we can model them as a\nbipartite graph, where",
    "start": "133120",
    "end": "140590"
  },
  {
    "text": "we have users on one side and\nwe have items on the other side. And what you can also do--",
    "start": "140590",
    "end": "146140"
  },
  {
    "text": "many times do-- and\nbasically all big, especially e-commerce companies\nhave huge efforts",
    "start": "146140",
    "end": "153040"
  },
  {
    "text": "that they build a knowledge\ngraph on this side, right? You can think about here how\nthe knowledge graph can extend.",
    "start": "153040",
    "end": "160120"
  },
  {
    "text": "If these are movies,\nwhat are properties of the movies, categories,\nactors, genres, and so on?",
    "start": "160120",
    "end": "166310"
  },
  {
    "text": "And you can have a knowledge\ngraph on this side. What you can also do on the\nuser side is that it's not",
    "start": "166310",
    "end": "171970"
  },
  {
    "text": "only that you know user and\nthe items the user interacted, let's say, purchased or\nbrowsed, but also, you",
    "start": "171970",
    "end": "180850"
  },
  {
    "text": "have other information about the\nuser other behavior on the site that you can enrich with the\ngraph on the left-hand side,",
    "start": "180850",
    "end": "191709"
  },
  {
    "text": "right? But the most simple\nrepresentation is that you have\nusers and items.",
    "start": "191710",
    "end": "198280"
  },
  {
    "text": "And an edge means\nthat a user has let's say consumed\nthat given item, right?",
    "start": "198280",
    "end": "204940"
  },
  {
    "text": "So the edge indicates some\nuser-item interaction, maybe a click, maybe a\npurchase, maybe a review.",
    "start": "204940",
    "end": "212110"
  },
  {
    "text": "And it's often associated\nwith a timestamp. Because this interaction\nbetween a user and an item",
    "start": "212110",
    "end": "217150"
  },
  {
    "text": "happened at a particular\npoint in time. So the most general way to write\nout the recommender system task",
    "start": "217150",
    "end": "226330"
  },
  {
    "text": "is that we have some users\nand we have some items and we have some of these\ninformation, these interactions",
    "start": "226330",
    "end": "231970"
  },
  {
    "text": "known from the past. And the task is that we\nwant to predict new items",
    "start": "231970",
    "end": "237250"
  },
  {
    "text": "each user is going to interact\nwith in the future, right? And we can think of this as a\nlink prediction problem, right?",
    "start": "237250",
    "end": "244750"
  },
  {
    "text": "Where we want to predict new\nuser-item interaction edges based on the past edges, right?",
    "start": "244750",
    "end": "251680"
  },
  {
    "text": "So for a given user u\nand for a given item v, we need to get a real-valued\nscore that would say,",
    "start": "251680",
    "end": "258910"
  },
  {
    "text": "what is the utility of this\nuser with respect to that item. And item can be\nsomething very abstract.",
    "start": "258910",
    "end": "266289"
  },
  {
    "text": "It can be a product, it can be\na movie, it can be a restaurant, it can be a dish\nat a restaurant,",
    "start": "266290",
    "end": "271555"
  },
  {
    "text": "it can be a movie director,\nit can be a taxi ride,",
    "start": "271555",
    "end": "276639"
  },
  {
    "text": "it can be a decision whether\nyou want to order food, or you want to take a taxi ride,\nor you want some other service.",
    "start": "276640",
    "end": "282500"
  },
  {
    "text": "So a lot of different things\nyou can think about something recommending\nsomething to the user and trying to anticipate\nwhat the user is going to do.",
    "start": "282500",
    "end": "291070"
  },
  {
    "text": "A modern recommender\nsystem is a big system.",
    "start": "291070",
    "end": "297070"
  },
  {
    "text": "It has many components. And one of the main problems\nis that like, in reality,",
    "start": "297070",
    "end": "302560"
  },
  {
    "text": "we cannot evaluate\nthis function f, this function for every\nuser item pair, right?",
    "start": "302560",
    "end": "309010"
  },
  {
    "text": "You cannot show up to\nthe Amazon web page, and in 50 milliseconds, score\nhow likely are you to be",
    "start": "309010",
    "end": "315850"
  },
  {
    "text": "interested at this time on\nTuesday at 3:09 PM for each",
    "start": "315850",
    "end": "320920"
  },
  {
    "text": "of the 100 million products\nthey have or how many even more, and then show you top 20, or\nsomehow decide what to show",
    "start": "320920",
    "end": "328000"
  },
  {
    "text": "you, right? So, usually, the way\nthis works is that you have a two-stage process. So you have what they\ncall candidate generation",
    "start": "328000",
    "end": "335050"
  },
  {
    "text": "and then you have a ranking. And a candidate\ngeneration, the idea is that basically you\nuse the user as a query",
    "start": "335050",
    "end": "341440"
  },
  {
    "text": "into your database of items\nand you retrieve the closest,",
    "start": "341440",
    "end": "346750"
  },
  {
    "text": "the most relevant items. Maybe 100, maybe 1,000 of them. And then you come with\na big fat ranking stage",
    "start": "346750",
    "end": "356020"
  },
  {
    "text": "that now scores every of\nthose 1,000 items with respect to the user.",
    "start": "356020",
    "end": "361030"
  },
  {
    "text": "And in a recommender\nsystem, usually, you would have a lot of different\ncandidate-generated hours",
    "start": "361030",
    "end": "366510"
  },
  {
    "text": "that look at various aspects. And then you'd have, let's\nsay, one scoring function.",
    "start": "366510",
    "end": "373170"
  },
  {
    "text": "So, in the embedding world,\nwhen we work with embeddings, the idea is that\nyou somehow embed",
    "start": "373170",
    "end": "380099"
  },
  {
    "text": "a user and the embedding\nof the user is your query. And then you have a K-NN means\nk nearest neighbor engine",
    "start": "380100",
    "end": "387780"
  },
  {
    "text": "where you basically take\nthis embedding of the query. And you say what are hundreds\nor 1,000 nearest item embeddings",
    "start": "387780",
    "end": "395940"
  },
  {
    "text": "to the embedding of the user? And you retrieve those. So this is a high-dimensional\nsearch engine.",
    "start": "395940",
    "end": "402060"
  },
  {
    "text": "Actually, the way this\nworks, it's interesting. The state-of-the-art\nmethods for retrieval,",
    "start": "402060",
    "end": "410010"
  },
  {
    "text": "this nearest neighbor engines\nare graph-based called H and S is the method. Hierarchical small-world\nnetworks is the method, right?",
    "start": "410010",
    "end": "417090"
  },
  {
    "text": "But you retrieve here from\nmillions, tens of millions, billions of items. You have your candidates.",
    "start": "417090",
    "end": "423030"
  },
  {
    "text": "And now, you are scoring these\n1,000 candidates with respect to the user, and then\nreturn, let's say, top 10.",
    "start": "423030",
    "end": "430940"
  },
  {
    "text": "And really in reality,\nyou never return top 10. Because out of the highest\nranking ones you're like,",
    "start": "430940",
    "end": "437240"
  },
  {
    "text": "which ones has the\nuser seen yesterday? Those I throw away. I want some diversity. So I'll show five\nexamples of this type",
    "start": "437240",
    "end": "446480"
  },
  {
    "text": "and I'll show another five\nexamples of that type. And somebody is paying\nme to show that item.",
    "start": "446480",
    "end": "451979"
  },
  {
    "text": "So it's an ad. And I'll throw that in there. So there's a lot of\nlogic that goes on",
    "start": "451980",
    "end": "458840"
  },
  {
    "text": "in here from use scoring\nto actually user seeing it. But if you are interested\nmore in this next quarter,",
    "start": "458840",
    "end": "467539"
  },
  {
    "text": "I'll be teaching\nCS 246 and we'll talk more in depth about\nrecommender systems.",
    "start": "467540",
    "end": "472970"
  },
  {
    "text": "But it's a super\nfascinating area. OK. So search. And then I recommend.",
    "start": "472970",
    "end": "478310"
  },
  {
    "text": "OK? So the point is\nthat for each user, you want to recommend in this\nK items where K is, let's say,",
    "start": "478310",
    "end": "486030"
  },
  {
    "text": "10 or 100, right? And the recommendation\nto be effective, K needs to be much smaller\nthan the total number of items",
    "start": "486030",
    "end": "493340"
  },
  {
    "text": "I have in my database, right? So usually, we recommend, let's\nsay, 10 items to the user. And then see what the user\ndoes with the 10 items.",
    "start": "493340",
    "end": "500569"
  },
  {
    "text": "And as soon as the user scrolls,\nwe recommend the next 10. And the user scrolls\nand the next 10. Because we want to\nbe very real-time.",
    "start": "500570",
    "end": "507470"
  },
  {
    "text": "We want to be very\nresponsive to the user. It makes no sense to use yes,\nyour engagement from last week",
    "start": "507470",
    "end": "514849"
  },
  {
    "text": "to materialize\nrecommendations for next week. It's stale. So we want to be\nresponsive, right?",
    "start": "514850",
    "end": "521450"
  },
  {
    "text": "And the goal is to include as\nmany of these positive items as possible into the\ntop K recommended items.",
    "start": "521450",
    "end": "528480"
  },
  {
    "text": "So what do I mean here is\nthat a positive item is item that the user will interact\nwith in the future, right?",
    "start": "528480",
    "end": "535220"
  },
  {
    "text": "So the idea is that\nbasically, I want to predict what\nitems you are going to interact with next week.",
    "start": "535220",
    "end": "541590"
  },
  {
    "text": "And I want today to guess\nas many of those items as possible. The evaluation metric that\npeople would talk about",
    "start": "541590",
    "end": "549680"
  },
  {
    "text": "is they would call it Recall@K\nor they would call hits at K. So out of top K items\nthat you recommended,",
    "start": "549680",
    "end": "556790"
  },
  {
    "text": "how many has the user\nactually interacted with in the next time period? So the way you can think of\nthis as that for every user,",
    "start": "556790",
    "end": "563900"
  },
  {
    "text": "P sub u is a set\nof positive items that the user will\ninteract in the future. Right now, of course,\nyou can say, OK,",
    "start": "563900",
    "end": "571470"
  },
  {
    "text": "how do I know what the user\nwill interact in the future? The way you know it\nis that you pretend",
    "start": "571470",
    "end": "576710"
  },
  {
    "text": "that today is February 1. And then you look what the user\ndoes in the week of February 1st to February 2nd, right?",
    "start": "576710",
    "end": "583100"
  },
  {
    "text": "So you go into the\nhistory and you hide the future from\nyourself and see can you predict that future.",
    "start": "583100",
    "end": "588570"
  },
  {
    "text": "That's the way you know what\nthe user will do, right? And let R sub u be the set of\nitems recommended by our model,",
    "start": "588570",
    "end": "596490"
  },
  {
    "text": "right? So the top k recommendations. And items that the user has\nalready interacted with,",
    "start": "596490",
    "end": "605589"
  },
  {
    "text": "we want to exclude. So the point is that we\nwant to maximize the size of the intersection, right?",
    "start": "605590",
    "end": "611700"
  },
  {
    "text": "We want out of the items\nthat we recommend as many of them as possible\nto be the ones",
    "start": "611700",
    "end": "617670"
  },
  {
    "text": "that the user has interacted\nwith in the future. So this is called the\nRecall@K. And it's simply",
    "start": "617670",
    "end": "624863"
  },
  {
    "text": "the intersection\nbetween these two sets divided by the\nfraction of positive items.",
    "start": "624863",
    "end": "631320"
  },
  {
    "text": "And the higher value indicates\nthat more positive items are recommended to the user.",
    "start": "631320",
    "end": "637440"
  },
  {
    "text": "And the final recall is\ncomputed by averaging the recall values across all users, right?",
    "start": "637440",
    "end": "642690"
  },
  {
    "text": "So the unit of\nrecommendation is a user. I'm recommending it to a user.",
    "start": "642690",
    "end": "648420"
  },
  {
    "text": "I'm recommending\nit in a session. OK? So this is what I\nwant to say from what",
    "start": "648420",
    "end": "655110"
  },
  {
    "text": "is the goal of a\nrecommender system. So now, let's talk about\nembedding-based methods for recommender systems.",
    "start": "655110",
    "end": "661560"
  },
  {
    "text": "The notation I'm going\nto use is the following U is a set of users,\nV is a set of items,",
    "start": "661560",
    "end": "669440"
  },
  {
    "text": "and E is a set of observed\nuser item interactions. So in some sense, a set of\nedges between users and items",
    "start": "669440",
    "end": "678020"
  },
  {
    "text": "where an edge means that user U\ninteracted with item V. So now,",
    "start": "678020",
    "end": "683540"
  },
  {
    "text": "what is an interaction? In this simplified case,\nlet's assume an interaction",
    "start": "683540",
    "end": "688550"
  },
  {
    "text": "is I bought an item. I liked an item. Something of that sort. Some positive action\nthat I did with the item.",
    "start": "688550",
    "end": "695750"
  },
  {
    "text": "I watched the movie\nand so on, right? And then yeah, this\nis domain-specific.",
    "start": "695750",
    "end": "701810"
  },
  {
    "text": "And very important\nhow you define this positive\ninteraction, right? With the movie, you\nusually say, oh, you",
    "start": "701810",
    "end": "707120"
  },
  {
    "text": "watched 80% of the\nmovie, and so on, OK? So that's what we say.",
    "start": "707120",
    "end": "714350"
  },
  {
    "text": "And now what is the\nscoring function? The scoring function is we\nwant to get the top K items.",
    "start": "714350",
    "end": "719720"
  },
  {
    "text": "So we need a score function\nthat scores a user comma item and gives me a score.",
    "start": "719720",
    "end": "725340"
  },
  {
    "text": "So I'll write it this way. Score between a\nuser and an item. And I'm going to return the\nK items with the largest",
    "start": "725340",
    "end": "734759"
  },
  {
    "text": "scores for a given user, right? Again, excluding\nalready interacted items",
    "start": "734760",
    "end": "740178"
  },
  {
    "text": "at the time of\nrecommendation, right? So the idea is that\nfor a given user, I'm going to score\nall possible items.",
    "start": "740178",
    "end": "746250"
  },
  {
    "text": "Here are their scores. And then I'm going to\nreturn a top K, where",
    "start": "746250",
    "end": "751320"
  },
  {
    "text": "K is some hyperparameter. OK. That's what's\nconceptually happening.",
    "start": "751320",
    "end": "758160"
  },
  {
    "text": "And a method that\nworks really, really well are based on embeddings.",
    "start": "758160",
    "end": "764280"
  },
  {
    "text": "And the reason these\nmethods work really well is because, usually, you have\na lot of this interaction data.",
    "start": "764280",
    "end": "770759"
  },
  {
    "text": "And because you have a lot\nof these interaction data, you can really learn\nthis optimal embeddings",
    "start": "770760",
    "end": "775860"
  },
  {
    "text": "about of users and\nitems so you can then learn basically their\ndifferent aspects",
    "start": "775860",
    "end": "781770"
  },
  {
    "text": "and make good recommendations. So for our case,\nfor every user, we are going to have a\nD-dimensional embedding.",
    "start": "781770",
    "end": "788700"
  },
  {
    "text": "And for every item,\nwe are going to have a D-dimensional embedding. And then I'm going\nto have some scoring",
    "start": "788700",
    "end": "794070"
  },
  {
    "text": "function f parameterized\nby these parameters theta. That is going to take embedding\nof the user, embedding",
    "start": "794070",
    "end": "800400"
  },
  {
    "text": "of the item, and will\nreturn me a score, OK? And that will be, in some\nsense, a utility that",
    "start": "800400",
    "end": "808200"
  },
  {
    "text": "of a user with respect\nto a given item. And this means that\nembedding-based methods",
    "start": "808200",
    "end": "814830"
  },
  {
    "text": "are going to have three\ntypes of parameters that I need to\nlearn or estimate. I need an encoder to\ngenerate user embeddings,",
    "start": "814830",
    "end": "822520"
  },
  {
    "text": "I need a set of\nparameters or an encoder to generate item\nembeddings, and then I also need parameters theta\nof this scoring function.",
    "start": "822520",
    "end": "830613"
  },
  {
    "text": "So I can learn a\nscoring function. I learn the embeddings of\nusers and embeddings of items. And the training\nobjective is that I",
    "start": "830613",
    "end": "837400"
  },
  {
    "text": "want to optimize the model\nparameters to achieve high Recall@K on seen user-item\ninteractions, on the training",
    "start": "837400",
    "end": "845410"
  },
  {
    "text": "user-item interactions, right? And the hope is that\nthis objective would also lead to high Recall@K for the\nunseen future interactions.",
    "start": "845410",
    "end": "854770"
  },
  {
    "text": "That's what we are\ntrying to achieve. There is one problem\nwith this Recall@K.",
    "start": "854770",
    "end": "861580"
  },
  {
    "text": "The problem is that this\noriginal objective function is non-differentiable, right?",
    "start": "861580",
    "end": "866709"
  },
  {
    "text": "I cannot apply gradient-based\nlearning to it. I cannot apply\noptimization methods to it. So that's a problem.",
    "start": "866710",
    "end": "873100"
  },
  {
    "text": "So what you do in practice\nis that people have developed to surrogate loss functions\nthat are widely used",
    "start": "873100",
    "end": "881500"
  },
  {
    "text": "and enable efficient\ngradient-based learning, right? Basically, I can do\nback propagation. I can use SGD, Adam,\nand so on, right?",
    "start": "881500",
    "end": "889090"
  },
  {
    "text": "And there are two ways\nhow people do this. One is called a binary loss,\nand the other one that's better is called Bayesian Personalized\nRanking loss, or BPR.",
    "start": "889090",
    "end": "897580"
  },
  {
    "text": "So I'll explain this one\nquickly and what the problem is with binary loss, and\nthen I'll show you the BPR.",
    "start": "897580",
    "end": "905470"
  },
  {
    "text": "The point is that these\nsurrogate losses are differentiable, and\nthey, in some sense,",
    "start": "905470",
    "end": "911290"
  },
  {
    "text": "should align well with\nthe original training objective of that or\ntraining goal of optimizing",
    "start": "911290",
    "end": "916930"
  },
  {
    "text": "for equal at K. So what is a binary loss? A binary loss, we define\npositive and negative edges.",
    "start": "916930",
    "end": "925100"
  },
  {
    "text": "A set of positive edges E are\nthe observed training user-item interaction.",
    "start": "925100",
    "end": "930300"
  },
  {
    "text": "So this is what\nthe user is going to interact with in the future. And then we have a\nset of negative edges",
    "start": "930300",
    "end": "935930"
  },
  {
    "text": "which are interactions\nthat are not going to happen in the future.",
    "start": "935930",
    "end": "941269"
  },
  {
    "text": "Now, how do you select? This becomes very\nimportant in practice.",
    "start": "941270",
    "end": "947270"
  },
  {
    "text": "Because you won't be able to use\nall non-existent interactions",
    "start": "947270",
    "end": "952490"
  },
  {
    "text": "as negatives. And you have to be careful how\nyou select negative examples,",
    "start": "952490",
    "end": "957629"
  },
  {
    "text": "right? If I'm really excited\nabout baking, then if you pick 100 random Amazon\nproducts as negative examples,",
    "start": "957630",
    "end": "966500"
  },
  {
    "text": "probably none of them\nwill be about baking. There'll be a car,\nthere'll be a toy, there'll be I know\nsome dress, and so on.",
    "start": "966500",
    "end": "973310"
  },
  {
    "text": "It'd be very easy to say,\noh, yeah, [? Huri's ?] not interested in that, he\nreally is into baking, right?",
    "start": "973310",
    "end": "979040"
  },
  {
    "text": "It will be too easy. So there is, as I'll\nshow you, towards the end for some, let's say,\ncommercial recommender systems",
    "start": "979040",
    "end": "988190"
  },
  {
    "text": "that are deployed. There is a lot of thinking that\ngoes into this negative example",
    "start": "988190",
    "end": "994110"
  },
  {
    "text": "selection. Now that I defined\nE and E negative, I'll just define a\nsigmoid function.",
    "start": "994110",
    "end": "999769"
  },
  {
    "text": "A sigmoid function\nis a function that takes an input on the\nreal axis and squishes it",
    "start": "999770",
    "end": "1005270"
  },
  {
    "text": "between 0 and 1. So why do we like\nsigmoid function? Because now if things\nare on 0 and 1,",
    "start": "1005270",
    "end": "1010880"
  },
  {
    "text": "I can interpret this\nas a probability. OK. So that's all. It takes a real\naxis and squishes it",
    "start": "1010880",
    "end": "1017089"
  },
  {
    "text": "in a bijective way to an\ninterval 0 in one, right?",
    "start": "1017090",
    "end": "1022460"
  },
  {
    "text": "So maps real-valued\nscores into something that can be interpreted as a\nbinary likelihood score, right?",
    "start": "1022460",
    "end": "1028550"
  },
  {
    "text": "As a probability. So a binary loss is\nthe following, right? It's a binary classification\nof positive negative edges",
    "start": "1028550",
    "end": "1036500"
  },
  {
    "text": "using my scoring function f. So it's basically a\ncross-entropy loss. That's another way, right? So I'm saying, here\nare my positive edges.",
    "start": "1036500",
    "end": "1044119"
  },
  {
    "text": "It's a sum over\nmy positive edges. And it's the log\nsigmoid of the score.",
    "start": "1044119",
    "end": "1050360"
  },
  {
    "text": "So it's basically the\nlog probability, right? I'm just multiplying or summing\nup this log probabilities",
    "start": "1050360",
    "end": "1056750"
  },
  {
    "text": "for edges that actually occur. So I will want this to be\nas small as possible, right?",
    "start": "1056750",
    "end": "1064060"
  },
  {
    "text": "And then I go here over\nthe negative edges. So the edges that\ndid not appear.",
    "start": "1064060",
    "end": "1070000"
  },
  {
    "text": "What I want to\nmaximize here, I want to do 1 minus the probability\nof an edge, right?",
    "start": "1070000",
    "end": "1076540"
  },
  {
    "text": "So really I want these scores\nto be as small as possible",
    "start": "1076540",
    "end": "1082090"
  },
  {
    "text": "so that if it's small\nas negative as possible, then a sigmoid of that\nwill be close to 0.",
    "start": "1082090",
    "end": "1089200"
  },
  {
    "text": "1 minus something close\nto 0 will be close to 1. Log of 1 is 0, right? So that's what I want.",
    "start": "1089200",
    "end": "1095289"
  },
  {
    "text": "I want these log terms to\nbe as small as possible. And I want these terms\nto be as big as possible,",
    "start": "1095290",
    "end": "1103720"
  },
  {
    "text": "as close to 1 as possible. While here, I want this as\nclose to 0 as possible, right?",
    "start": "1103720",
    "end": "1110230"
  },
  {
    "text": "And that means that\nthis binary loss pushes the scores\nof positive examples",
    "start": "1110230",
    "end": "1116350"
  },
  {
    "text": "higher than those of negative\nexamples or negative edges. And this aligns well with\nour recall metric, right?",
    "start": "1116350",
    "end": "1123529"
  },
  {
    "text": "We want the interactions\nin the future to get high score and\nthe negative interactions",
    "start": "1123530",
    "end": "1129350"
  },
  {
    "text": "to get low score. One detail is that usually\nduring training, these terms,",
    "start": "1129350",
    "end": "1136690"
  },
  {
    "text": "you don't really go over all\nthe positive edges and all the negative edges, but you\ncreate mini-batches, right?",
    "start": "1136690",
    "end": "1144460"
  },
  {
    "text": "And you can basically\napproximate this loss by using mini-batches of\npositive and negative examples.",
    "start": "1144460",
    "end": "1150970"
  },
  {
    "text": "And many times,\npeople would sometimes change only maybe have\none positive example",
    "start": "1150970",
    "end": "1157210"
  },
  {
    "text": "in a mini-batch and a\nlot of negative examples. And sometimes because\nsampling negative examples",
    "start": "1157210",
    "end": "1162460"
  },
  {
    "text": "can take a lot of\ntime, you would even use the same set of\nnegative examples across a few mini batches\nand then refresh them.",
    "start": "1162460",
    "end": "1170330"
  },
  {
    "text": "So there's a lot of\ntricks that people do to make this work\nfor hundreds of millions or billions of users or items.",
    "start": "1170330",
    "end": "1177610"
  },
  {
    "text": "The issue with\nthis binary loss is that it scores\nall positive edges",
    "start": "1177610",
    "end": "1185080"
  },
  {
    "text": "and tries to push them high. And it pushes all the\nnegative edges low, right?",
    "start": "1185080",
    "end": "1190510"
  },
  {
    "text": "What do I mean by here? This is overall edges UV,\nand this is overall edges UV,",
    "start": "1190510",
    "end": "1197810"
  },
  {
    "text": "right? So the problem is it tries to\nput all positive edges high and all negative edges low.",
    "start": "1197810",
    "end": "1205100"
  },
  {
    "text": "But this would unnecessarily\npenalize a model predictions even if the training\nrecall metric is perfect.",
    "start": "1205100",
    "end": "1212880"
  },
  {
    "text": "OK. So let me show you an example\nof what I mean by this. What I mean is the following.",
    "start": "1212880",
    "end": "1219330"
  },
  {
    "text": "Imagine, I have two\nusers and two items. And imagine user 0 likes item\n0 and user 1 likes item 1.",
    "start": "1219330",
    "end": "1229040"
  },
  {
    "text": "And imagine my\nscoring function would say there is a score for on\nthis edge and a score of 2",
    "start": "1229040",
    "end": "1234860"
  },
  {
    "text": "on the other edge. OK. So here, right?",
    "start": "1234860",
    "end": "1240500"
  },
  {
    "text": "So it means that\nthe pink edge is ranked higher than the\nedge between 1 and item 0.",
    "start": "1240500",
    "end": "1246230"
  },
  {
    "text": "And for the user 0, the edge\nbetween user zero and item 0 has score 1.",
    "start": "1246230",
    "end": "1251270"
  },
  {
    "text": "And the other edge to the\nitem 1 has a score of minus 1. So this would be\na perfect recall.",
    "start": "1251270",
    "end": "1258289"
  },
  {
    "text": "But if you look at what\nthe binary loss would do, it would penalize\nyou because you",
    "start": "1258290",
    "end": "1264980"
  },
  {
    "text": "have ranked this negative\nedge, you gave it a score of 2, while that positive\nedge has a score of 1.",
    "start": "1264980",
    "end": "1271430"
  },
  {
    "text": "So you would receive\npenalty, right? So the problem is that\nbinary loss would still",
    "start": "1271430",
    "end": "1277910"
  },
  {
    "text": "penalize the model because\nthe negative edge has a higher",
    "start": "1277910",
    "end": "1283160"
  },
  {
    "text": "score than this positive\nedge, even though they come from different users. And if you look at\neach user individually,",
    "start": "1283160",
    "end": "1289909"
  },
  {
    "text": "the ranking of the edges\nis actually correct. OK. So that's the problem\nwith the binary loss.",
    "start": "1289910",
    "end": "1296630"
  },
  {
    "text": "So the insight is\nthat the binary loss is non-personalized in a sense\nthat positive negative edges",
    "start": "1296630",
    "end": "1302750"
  },
  {
    "text": "are considered across\nall users at once, right? But the recall\nmetric that I defined",
    "start": "1302750",
    "end": "1309770"
  },
  {
    "text": "is inherently personalized. It's per user recall, right? And in some sense, this\nnon-personalized binary loss",
    "start": "1309770",
    "end": "1321230"
  },
  {
    "text": "is overly stringent for\nmeasuring personalized recall metric.",
    "start": "1321230",
    "end": "1327080"
  },
  {
    "text": "So a suitable alternative\nto this that makes this fix is a loss function that\nbasically is personalized.",
    "start": "1327080",
    "end": "1336412"
  },
  {
    "text": "What do I mean by this? Is that for each user,\nwe want the scores of the positive items\nto be above the scores",
    "start": "1336412",
    "end": "1342410"
  },
  {
    "text": "of the negative items\nfor each user separately, regardless of what is the\nmagnitude of the scores for one",
    "start": "1342410",
    "end": "1350000"
  },
  {
    "text": "user versus the\nother user, right?  We don't care about the\nordering of scores across users.",
    "start": "1350000",
    "end": "1357920"
  },
  {
    "text": "And the BPR, the Bayesian\nPersonalized Ranking loss achieves this. So let me show you\nwhat this loss is.",
    "start": "1357920",
    "end": "1366110"
  },
  {
    "text": "So one thing, here's a note. This BPR was invented\nback in 2009.",
    "start": "1366110",
    "end": "1375250"
  },
  {
    "text": "And at that point\nin time, they called it Bayesian because they had\nsome Bayesian priors as part",
    "start": "1375250",
    "end": "1382210"
  },
  {
    "text": "of their model. But it turns out that the\nkey innovation was not the Bayesian priors, but was\nthis personalized ranking.",
    "start": "1382210",
    "end": "1390100"
  },
  {
    "text": "And people still call it\nBayesian personalized ranking, even though it's\nthe personalized ranking that is important here.",
    "start": "1390100",
    "end": "1396110"
  },
  {
    "text": "OK. So just to explain\nwhy Bayesian is there even though it shouldn't be.",
    "start": "1396110",
    "end": "1401740"
  },
  {
    "text": "So the point being\nis that this loss is personalized surrogate\nloss that aligns well with this Recall@K. And the\nway it works is the following.",
    "start": "1401740",
    "end": "1410049"
  },
  {
    "text": "Is that for every\nuser, we will now define a set of positive\nand negative edges, right?",
    "start": "1410050",
    "end": "1415510"
  },
  {
    "text": "So rather than saying here are\nall the positive edges and here are all the negative\nedges, now we are going to have positive and\nnegative set for every user.",
    "start": "1415510",
    "end": "1423800"
  },
  {
    "text": "So now, notice that I have\na set of positive edges that is a function of the user\nand a set of negative edges",
    "start": "1423800",
    "end": "1430389"
  },
  {
    "text": "that is a function of the user. OK. Before we just said\nthere are negative edges, now, this negative\nset will be different",
    "start": "1430390",
    "end": "1437270"
  },
  {
    "text": "depending on what user\nI'm thinking about, OK. And notice here, when\ni give the definitions,",
    "start": "1437270",
    "end": "1444050"
  },
  {
    "text": "I always condition on this user. I say these are the positive\ninteractions of this given user",
    "start": "1444050",
    "end": "1450259"
  },
  {
    "text": "and these are the\nnon-interactions or negative edges of this given user. OK.",
    "start": "1450260",
    "end": "1455660"
  },
  {
    "text": "So the training objective then\nis that for each user u star, we want scores of the\nrooted positive edges.",
    "start": "1455660",
    "end": "1463820"
  },
  {
    "text": "Rooted in a sense that left-hand\nside of the edge is fixed, the user is fixed, to\nbe higher than those",
    "start": "1463820",
    "end": "1470360"
  },
  {
    "text": "of the rooted negative edges. And the way you write this down\nis here, where I'm saying I'm",
    "start": "1470360",
    "end": "1478760"
  },
  {
    "text": "going over every positive\nedge and every negative edge",
    "start": "1478760",
    "end": "1483800"
  },
  {
    "text": "for a given user. And then here I'm saying\nfor a positive edge,",
    "start": "1483800",
    "end": "1489350"
  },
  {
    "text": "it has to be ranked higher than\nall the negative edges, right? Because I take a\npositive edge, and then",
    "start": "1489350",
    "end": "1496820"
  },
  {
    "text": "I go over all the\nnegative edges. And here I take the\ndifference in ranking. This is the positive and\nthis is a negative, right?",
    "start": "1496820",
    "end": "1504020"
  },
  {
    "text": "So now, if the score\nof the positive guy is always bigger than the\nscore of any negative edge",
    "start": "1504020",
    "end": "1516350"
  },
  {
    "text": "for a given user, then this\ndifference will be positive, then this sigmoid will give\nme something that is above 0.",
    "start": "1516350",
    "end": "1525110"
  },
  {
    "text": "And things will\nthings will work out. OK.",
    "start": "1525110",
    "end": "1530420"
  },
  {
    "text": "So the final BPR loss will\nbe 1 over number of users.",
    "start": "1530420",
    "end": "1539370"
  },
  {
    "text": "And then you are\nsumming this loss over all the users, where\nthe loss is defined here.",
    "start": "1539370",
    "end": "1544990"
  },
  {
    "text": "And then, again,\nthe point here is that you can approximate this,\nagain, using mini batches,",
    "start": "1544990",
    "end": "1552179"
  },
  {
    "text": "and so on. But the key is that\nyou are now comparing the difference between\na positive interaction",
    "start": "1552180",
    "end": "1558390"
  },
  {
    "text": "and a negative 1 for\na fixed user, OK? And only then, you\nsum up or average over",
    "start": "1558390",
    "end": "1566070"
  },
  {
    "text": "all the users you have. OK. Now, one thing about this\nBPR loss that I want to say",
    "start": "1566070",
    "end": "1573550"
  },
  {
    "text": "is that, in practice, you want\nto do this over mini batches. So this means that we sample a\nsubset of users U for a given",
    "start": "1573550",
    "end": "1581980"
  },
  {
    "text": "for a given mini batch. And then for each user, we\nsample one positive item.",
    "start": "1581980",
    "end": "1590740"
  },
  {
    "text": "For each user in\nthe mini batch, we sample one positive item\nand a set of negative items.",
    "start": "1590740",
    "end": "1599169"
  },
  {
    "text": "OK. And then in the\nmini batch, the loss would be computed as follows.",
    "start": "1599170",
    "end": "1605380"
  },
  {
    "text": "So I average over all the\nusers in the mini batch. I go over the negative edges.",
    "start": "1605380",
    "end": "1612310"
  },
  {
    "text": "And I'm saying I have this\ndifference between the users positive item.",
    "start": "1612310",
    "end": "1617620"
  },
  {
    "text": "There is only one and a set of\nnegative items for that fixed user. OK.",
    "start": "1617620",
    "end": "1622690"
  },
  {
    "text": "So now, I have one\npositive and a set of negatives for every\nuser in that mini batch.",
    "start": "1622690",
    "end": "1629809"
  },
  {
    "text": "And in the next mini batch,\nI may have the same user.",
    "start": "1629810",
    "end": "1637310"
  },
  {
    "text": "But I'll have a\ndifferent positive item and a different set\nof negative items. That's the idea.",
    "start": "1637310",
    "end": "1643500"
  },
  {
    "text": "So let me summarize what\nwe have learned so far. We have learned\nthat this Recall@K",
    "start": "1643500",
    "end": "1649460"
  },
  {
    "text": "is a metric for\npersonalized recommendation. And we talked that\nbasically the way",
    "start": "1649460",
    "end": "1657380"
  },
  {
    "text": "you could build a\nrecommender system is that basically\nthat you would learn three types of parameters. This could be shallow\nembeddings, a user encoder,",
    "start": "1657380",
    "end": "1664429"
  },
  {
    "text": "item encoder, and\na scoring function. And I showed you the surrogate\nloss functions either a binary",
    "start": "1664430",
    "end": "1671899"
  },
  {
    "text": "or this personalized\nranking loss that basically allow you now\nto optimize the loss in order",
    "start": "1671900",
    "end": "1678320"
  },
  {
    "text": "for you to learn\nthese parameters. And generally in this\nrecommender system benchmarks,",
    "start": "1678320",
    "end": "1684840"
  },
  {
    "text": "so this is benchmarks, the\nembedding-based models have achieved a state-of-the-art\nperformance.",
    "start": "1684840",
    "end": "1691420"
  },
  {
    "text": "So now I want to\nexplain why do they work so well in this benchmarks.",
    "start": "1691420",
    "end": "1698309"
  },
  {
    "text": "Question? Why are you always sampling\nmany more negative edges than positive? Why am I always sampling\nmany more negative edges",
    "start": "1698310",
    "end": "1705810"
  },
  {
    "text": "than positive edges? Because the problem is\nreally imbalanced, right?",
    "start": "1705810",
    "end": "1711990"
  },
  {
    "text": " In a sense that I know Netflix\nhas hundreds of thousands",
    "start": "1711990",
    "end": "1720120"
  },
  {
    "text": "of movies. Amazon has, I don't\nknow, a billion products. And you only interact\nwith a very small number",
    "start": "1720120",
    "end": "1726210"
  },
  {
    "text": "of [INAUDIBLE]. So it's not a 50/50. It's very tiny.",
    "start": "1726210",
    "end": "1731560"
  },
  {
    "text": "So you need a lot\nof negative examples to push away the positive 1.",
    "start": "1731560",
    "end": "1738490"
  },
  {
    "text": "What's the issue\nempirically with the non-personalized losses? Is it a wasted compute or is it\nthat the best performance you",
    "start": "1738490",
    "end": "1746770"
  },
  {
    "text": "could get is not\ngoing to be as good with the binary versus BPR? Good question. So what is the problem\nwith the binary loss?",
    "start": "1746770",
    "end": "1754450"
  },
  {
    "text": "In practice, what happens\nis that the binary loss-- I mean, in some sense,\nit's harder to optimize",
    "start": "1754450",
    "end": "1762910"
  },
  {
    "text": "and it leads the model focused\non the wrong things, right?",
    "start": "1762910",
    "end": "1769010"
  },
  {
    "text": "Yes. If you'd have an all powerful\nmodel, then you'd say, yeah, I'll use binary\nbecause it forces my model",
    "start": "1769010",
    "end": "1775630"
  },
  {
    "text": "to even learn more. But in reality, your data is\nnoisy, your model is imperfect.",
    "start": "1775630",
    "end": "1780940"
  },
  {
    "text": "So now you're trying\nto force model to learn something that you\ndon't really care about. So it's better to have\na personalized loss",
    "start": "1780940",
    "end": "1788170"
  },
  {
    "text": "because it really tells\nthe model what you really care about. And the model is going\nto use the modeling power",
    "start": "1788170",
    "end": "1793870"
  },
  {
    "text": "to separate out items\nfor a given user rather than make items\ncomparable across users.",
    "start": "1793870",
    "end": "1800600"
  },
  {
    "text": "That's the key why one\nis better than the other. But a good point. Yeah, thank you. Yes.",
    "start": "1800600",
    "end": "1806030"
  },
  {
    "text": "For the negative\nedge choices, do you want the negative edges to be\ncloser to your required sample?",
    "start": "1806030",
    "end": "1815120"
  },
  {
    "text": "Or do you want it\nto be very distant? Right. Yeah, good question. How do I select negative edges?",
    "start": "1815120",
    "end": "1820250"
  },
  {
    "text": "I have slides at the\nend where I'll show you how to do this properly. But the idea is the following.",
    "start": "1820250",
    "end": "1827330"
  },
  {
    "text": "The idea is that if you pick\nrandom things, and my baking example right,\nthen it becomes too",
    "start": "1827330",
    "end": "1834470"
  },
  {
    "text": "easy for the model to learn. Because it's really\nsaying, oh, here is baking.",
    "start": "1834470",
    "end": "1839750"
  },
  {
    "text": "And there's everything\nelse, right? But if you think about\nthese candidates generators,",
    "start": "1839750",
    "end": "1844760"
  },
  {
    "text": "they really need to pick the\nnot out of all baking items, but maybe I like\nwhatever making cookies",
    "start": "1844760",
    "end": "1852080"
  },
  {
    "text": "that look cats and dogs, right? So you want to recommend me\nI know cooking stuff that",
    "start": "1852080",
    "end": "1857270"
  },
  {
    "text": "makes cats and dogs. I'm making this up as I go. So let's see where it takes me. But what I'm trying\nto say is it's",
    "start": "1857270",
    "end": "1863750"
  },
  {
    "text": "important to force\nthe model to really give you those good items in\nthe top 100 out of a billion.",
    "start": "1863750",
    "end": "1869450"
  },
  {
    "text": "So you are really learning\nto push the good stuff all the way to the top and all the\nirrelevant stuff to the bottom.",
    "start": "1869450",
    "end": "1875640"
  },
  {
    "text": "So the point is if you\nhave random negatives, it's too easy for\nthe model to learn and it's too much\nnoise at the top.",
    "start": "1875640",
    "end": "1881750"
  },
  {
    "text": "So it never learns. So what you can\nthen do is to start introducing what is called\nhard negative examples, right?",
    "start": "1881750",
    "end": "1887960"
  },
  {
    "text": "So baking stuff that\nis not cats and dogs. So that you really learn how to\npersonalize baking stuff for me",
    "start": "1887960",
    "end": "1895850"
  },
  {
    "text": "because I'm in a very I know\nspecific type of baking, right? So that's what I mean by that.",
    "start": "1895850",
    "end": "1902240"
  },
  {
    "text": "And I'll show you\nsomething that is called distance-based\nsampling, where you want",
    "start": "1902240",
    "end": "1907490"
  },
  {
    "text": "to sample negative examples. You want to make sure\nyou sample them evenly across the embedding distances.",
    "start": "1907490",
    "end": "1914000"
  },
  {
    "text": "Because if you\nsample randomly, you are like sampling from far away. And it's easy.",
    "start": "1914000",
    "end": "1919190"
  },
  {
    "text": "If you sample too\nclose, then you confuse the model\nbecause you give it points that are very\nsimilar, but you",
    "start": "1919190",
    "end": "1924920"
  },
  {
    "text": "don't tell it that those\nall these other things are irrelevant. So you have to sample across\nthe range of distances.",
    "start": "1924920",
    "end": "1931909"
  },
  {
    "text": "That's the intuition. I'll make this more\nprecise at the end. Yes, go ahead. I was wondering, is the\nuse always a single user?",
    "start": "1931910",
    "end": "1939230"
  },
  {
    "text": "Or let's say, a few users\nliving in the same household, they may buy similar items.",
    "start": "1939230",
    "end": "1944809"
  },
  {
    "text": "Is it possible that\nyou would consider a subset of the nodes in\nseveral one single nodes? Good question.",
    "start": "1944810",
    "end": "1950480"
  },
  {
    "text": "So your question is\nabout what about people who share passwords, right? That's basically. Or people that use the\nsame account either Amazon,",
    "start": "1950480",
    "end": "1957799"
  },
  {
    "text": "or Netflix, and so on. So the way you would\ndo this is you'd try to model that heterogeneity\nseparately outside.",
    "start": "1957800",
    "end": "1966110"
  },
  {
    "text": "You didn't try to identify\nthat the user is heterogeneous and split the user\ninto a couple of nodes.",
    "start": "1966110",
    "end": "1972020"
  },
  {
    "text": "You'd like maybe\nvery quickly try to identify what identity of\nthe user you are working with.",
    "start": "1972020",
    "end": "1979650"
  },
  {
    "text": "It's a very hard, I\nwould say, use case specific problem if\nbehind the same identity,",
    "start": "1979650",
    "end": "1985850"
  },
  {
    "text": "there's actually\nmultiple individuals or multiple different tastes. OK.",
    "start": "1985850",
    "end": "1991395"
  },
  {
    "text": "Good.  Thank you for the question. Let me continue.",
    "start": "1991395",
    "end": "1997460"
  },
  {
    "text": "And please ask me more, right? So why do embedding\nsystems work?",
    "start": "1997460",
    "end": "2002860"
  },
  {
    "text": "And that is this super\npowerful intuition for recommender systems that's\ncalled collaborative filtering.",
    "start": "2002860",
    "end": "2010770"
  },
  {
    "text": "And basically, the\ncollaborative filtering, the idea is that if you\nrecommend items for a user",
    "start": "2010770",
    "end": "2019260"
  },
  {
    "text": "by collecting preferences\nof other similar users, you will do really,\nreally well, right?",
    "start": "2019260",
    "end": "2025440"
  },
  {
    "text": "In a sense that\nsimilar users tend to prefer similar items, right? If you and I watch 20 movies\nin common on Netflix, then",
    "start": "2025440",
    "end": "2035970"
  },
  {
    "text": "probably the 21st movie,\nwe will watch will also be very similar, right? So if I watch the\nmovies and you watch 21,",
    "start": "2035970",
    "end": "2043170"
  },
  {
    "text": "and all the first\n20 are the same, then a very good\nrecommendation is to say whatever is your 21st\nmovie, that's my 21st movie.",
    "start": "2043170",
    "end": "2051540"
  },
  {
    "text": "That's the idea of\ncollaborative filtering where you can basically\nsay two users are",
    "start": "2051540",
    "end": "2056610"
  },
  {
    "text": "similar if they interacted\nwith a lot of same items. And then you just look\nwhat the other user",
    "start": "2056610",
    "end": "2063750"
  },
  {
    "text": "has interacted with that\nthe first one hasn't yet. And that's a very\ngood recommendation",
    "start": "2063750",
    "end": "2070649"
  },
  {
    "text": "that you would\ncome up with here. And these embedding-based\ngraph-based methods, do exactly that, right?",
    "start": "2070650",
    "end": "2077940"
  },
  {
    "text": "Because these two users have\ninteracted with similar items, they will have\nsimilar embeddings.",
    "start": "2077940",
    "end": "2083580"
  },
  {
    "text": "So whenever this particular\nuser and that particular item",
    "start": "2083580",
    "end": "2089370"
  },
  {
    "text": "will have a high score, because\nthis is a positive edge, so it means the user you and\nthis particular item will also",
    "start": "2089370",
    "end": "2095609"
  },
  {
    "text": "likely have a high score. That's this collaborative\nfiltering intuition, right?",
    "start": "2095610",
    "end": "2103190"
  },
  {
    "text": "So the key here is, how\ndo we capture similarity between users and items?",
    "start": "2103190",
    "end": "2108859"
  },
  {
    "text": "And embedding-based\nmodels basically allow you to capture\nthis really well. Because low-dimensional\nembeddings are basically forced",
    "start": "2108860",
    "end": "2118730"
  },
  {
    "text": "to generalize in a sense that\nthey cannot simply capture all the user-item\ninteraction data, right?",
    "start": "2118730",
    "end": "2125150"
  },
  {
    "text": "And the embeddings are forced to\nlearn this implicit similarity between users and items.",
    "start": "2125150",
    "end": "2131059"
  },
  {
    "text": "And this allows the models\nto make effective prediction on unseen user item\ninteractions, right?",
    "start": "2131060",
    "end": "2138260"
  },
  {
    "text": "If you are embedding\ndimensionality is too large, then basically, you can\njust memorize the edges.",
    "start": "2138260",
    "end": "2143870"
  },
  {
    "text": "And you don't want to do that. So what you do is you make\nthe embedding dimensionality a bit smaller to force the model\nto identify these similarities",
    "start": "2143870",
    "end": "2152360"
  },
  {
    "text": "between users and items. So what I want to tell in the\nremaining of the lecture is",
    "start": "2152360",
    "end": "2161089"
  },
  {
    "text": "actually show you a\nfew GNN-based methods for recommender systems, right?",
    "start": "2161090",
    "end": "2166589"
  },
  {
    "text": "What we learned so\nfar was can just do shallow embeddings of\nusers and shallow embeddings of items.",
    "start": "2166590",
    "end": "2172170"
  },
  {
    "text": "The problem with\nthat is, for example, is that it's transductive. And that you need to\nlearn these embeddings.",
    "start": "2172170",
    "end": "2180600"
  },
  {
    "text": "I'm now going to show you\nthree methods, the neural graph",
    "start": "2180600",
    "end": "2185730"
  },
  {
    "text": "collaborative filtering,\nthe LightGCN, which is a simplification,\nand then I'll",
    "start": "2185730",
    "end": "2191520"
  },
  {
    "text": "tell you about this\npaper called PinSAGE that use that showed\nhow to use GNNs",
    "start": "2191520",
    "end": "2197370"
  },
  {
    "text": "to make high-quality\nembeddings for recommendations. And these are the first\ntwo are research papers.",
    "start": "2197370",
    "end": "2204990"
  },
  {
    "text": "This is also a research paper,\nbut it's a deployed system. So it's a lot of practical\ntips on how to make things work",
    "start": "2204990",
    "end": "2213330"
  },
  {
    "text": "and what really matters. OK. So let's start with neural\ncollaborative filtering.",
    "start": "2213330",
    "end": "2219010"
  },
  {
    "text": "So the collaborative\nfiltering is",
    "start": "2219010",
    "end": "2224520"
  },
  {
    "text": "based on this shallow encoders,\nwhich means shallow embeddings. We have no user\nno item features.",
    "start": "2224520",
    "end": "2230970"
  },
  {
    "text": "And basically, for\nevery user and item, we prepare a shallow\nlearnable embedding, right?",
    "start": "2230970",
    "end": "2238890"
  },
  {
    "text": "We just directly\nlearn the embedding. And usually, the\nscoring function, people just use the dot product.",
    "start": "2238890",
    "end": "2245130"
  },
  {
    "text": "Usually, the scoring\nfunction is just the dot product between\nthe embedding of the user and the embedding of the item.",
    "start": "2245130",
    "end": "2250470"
  },
  {
    "text": "Because it's then easy to\ndo the nearest neighbor search if you have a very\nsimple scoring function.",
    "start": "2250470",
    "end": "2256589"
  },
  {
    "text": "OK, so that's the way\npeople usually do it. But the problem with\nthe shallow encoders",
    "start": "2256590",
    "end": "2263190"
  },
  {
    "text": "is that this does not explicitly\ncapture the graph structure. And the graph structure is\nonly implicitly captured",
    "start": "2263190",
    "end": "2269880"
  },
  {
    "text": "as part of the\ntraining objective. So the researchers\nin the paper say",
    "start": "2269880",
    "end": "2275850"
  },
  {
    "text": "that only the first-order\ngraph structure, the edges is captured in\nthe training objective.",
    "start": "2275850",
    "end": "2281280"
  },
  {
    "text": "And the question is, could you\ncapture a bit more of the graph structure explicitly beyond\njust individual edges?",
    "start": "2281280",
    "end": "2290940"
  },
  {
    "text": "So the motivation is that we\nwant a model that explicitly captures the graph structure\nbeyond just the training",
    "start": "2290940",
    "end": "2298260"
  },
  {
    "text": "objective. And that captures bigger\nregion of the graph",
    "start": "2298260",
    "end": "2303270"
  },
  {
    "text": "rather than just each\nedge individually. And here, the GNNs are a\nnatural approach to this.",
    "start": "2303270",
    "end": "2309460"
  },
  {
    "text": "And as I said, I'll talk about\nthe neural graph collaborative filtering and then a\nmethod called LightGCN.",
    "start": "2309460",
    "end": "2318270"
  },
  {
    "text": "That's a simplified version\nof the first method. But the neural collaborative\nfiltering explicitly",
    "start": "2318270",
    "end": "2325660"
  },
  {
    "text": "incorporates higher\norder graph structure when generating user\nand item embeddings. And the key idea is to use GNN\nto generate graph-aware user",
    "start": "2325660",
    "end": "2334780"
  },
  {
    "text": "item embeddings. In the initial step, we\nlearn shallow embeddings of users and items.",
    "start": "2334780",
    "end": "2341410"
  },
  {
    "text": "This is what I showed\nyou to do so far. And then what this\npaper proposes",
    "start": "2341410",
    "end": "2346990"
  },
  {
    "text": "is to use a GNN to\npropagate the embeddings and make them more graph aware.",
    "start": "2346990",
    "end": "2353240"
  },
  {
    "text": "So what this means\nis that use now a GNN to enrich an embedding\nof every user and item",
    "start": "2353240",
    "end": "2359050"
  },
  {
    "text": "through the embeddings of\nnearby users and items. And you smooth these embeddings\nacross the graph a bit.",
    "start": "2359050",
    "end": "2366220"
  },
  {
    "text": "You enrich them, and you\nget better performance for that reason. OK.",
    "start": "2366220",
    "end": "2371750"
  },
  {
    "text": "So the way this works is\nwe are given user item bipartite graph.",
    "start": "2371750",
    "end": "2379840"
  },
  {
    "text": "We prepare shallow learnable\nembedding for every node, and then we use\na multi-layer GNN",
    "start": "2379840",
    "end": "2385120"
  },
  {
    "text": "to propagate embeddings\nacross this bipartite graph.",
    "start": "2385120",
    "end": "2390460"
  },
  {
    "text": "And then, in this\nsense, final embeddings are explicitly graph aware.",
    "start": "2390460",
    "end": "2396099"
  },
  {
    "text": "There are two kinds of\nlearnable parameters that are jointly learned. The shallow user item\nembeddings, as well as",
    "start": "2396100",
    "end": "2403630"
  },
  {
    "text": "the parameters of\nthe GNN, right? So here, the way\nwe would do it, we would not first train\nthe shallow embeddings",
    "start": "2403630",
    "end": "2411820"
  },
  {
    "text": "and then the GNN. But we would do it one\nafter the other, right? Like we would basically do\nthem in a coordinate descent",
    "start": "2411820",
    "end": "2418780"
  },
  {
    "text": "type of way so that we\nlearn both at the same time. OK.",
    "start": "2418780",
    "end": "2424839"
  },
  {
    "text": "Now, about the initial\nnode embeddings, we set a shallow\nlearnable embeddings",
    "start": "2424840",
    "end": "2430810"
  },
  {
    "text": "that basically are initial\nnode features or node",
    "start": "2430810",
    "end": "2436000"
  },
  {
    "text": "embeddings for every user. For every item, We\ninitialize this embedding.",
    "start": "2436000",
    "end": "2443180"
  },
  {
    "text": "And then we iteratively\nupdate node embeddings using these neighborhood embeddings.",
    "start": "2443180",
    "end": "2449320"
  },
  {
    "text": "So we say the\nembedding of a given node at the next iteration is\na combination of the embedding",
    "start": "2449320",
    "end": "2456960"
  },
  {
    "text": "of the same node at the previous\niteration plus aggregation of the embeddings of neighbors.",
    "start": "2456960",
    "end": "2463560"
  },
  {
    "text": "And here, we are\ncombining from items",
    "start": "2463560",
    "end": "2471360"
  },
  {
    "text": "and we are combining\nfrom users, right? And different\narchitectural choices",
    "start": "2471360",
    "end": "2478799"
  },
  {
    "text": "are possible for the\naggregation and combination. Aggregation could be mean. And combine can simply\nbe like concatenation",
    "start": "2478800",
    "end": "2486809"
  },
  {
    "text": "and some linear layer and\nthen a non-linearity, right?",
    "start": "2486810",
    "end": "2492330"
  },
  {
    "text": "So there's a lot\nflexibility how exactly",
    "start": "2492330",
    "end": "2498540"
  },
  {
    "text": "you implement this part. OK. And then what are the final\nembeddings in the score",
    "start": "2498540",
    "end": "2505000"
  },
  {
    "text": "function? After K rounds of this\nneighborhood aggregation, we get the final\nembedding H of U of K.",
    "start": "2505000",
    "end": "2515380"
  },
  {
    "text": "And also the H of the item at K. And then what do we do I say\nthe embedding of the user",
    "start": "2515380",
    "end": "2522040"
  },
  {
    "text": "is simply the embedding\nat the last iteration. And depending of the\nitem is the embedding at the last iteration.",
    "start": "2522040",
    "end": "2528070"
  },
  {
    "text": "And then score function. What people like to use\nis just the dot product between the\nembedding of the user",
    "start": "2528070",
    "end": "2533890"
  },
  {
    "text": "and the embedding of the item. So to summarize, why this neural\ngraph collaborative filtering",
    "start": "2533890",
    "end": "2541960"
  },
  {
    "text": "does? It's a conventional\ncollaborative filtering method\nuses a shallow user item embeddings and\ndoesn't explicitly",
    "start": "2541960",
    "end": "2550480"
  },
  {
    "text": "model the graph structure. So the NGCF uses a GNN to\npropagate shallow embeddings",
    "start": "2550480",
    "end": "2557170"
  },
  {
    "text": "across the graphs. And embeddings are explicitly\naware of higher-order graph",
    "start": "2557170",
    "end": "2563390"
  },
  {
    "text": "structure because\nof this propagation. That's the key\ninnovation here that you",
    "start": "2563390",
    "end": "2569359"
  },
  {
    "text": "get the benefit of\nshallow embeddings, but also some smoothness\nor propagation",
    "start": "2569360",
    "end": "2575150"
  },
  {
    "text": "of the shallow embeddings\nacross the graph using this learnable GNN.",
    "start": "2575150",
    "end": "2580250"
  },
  {
    "text": "OK, that's the idea here. So that's the first method.",
    "start": "2580250",
    "end": "2587848"
  },
  {
    "text": "I don't know. Do people have any questions? Or shall I show you the\nsimplified version of this?",
    "start": "2587848",
    "end": "2594860"
  },
  {
    "text": " OK, I'll show you a simplified.",
    "start": "2594860",
    "end": "2602050"
  },
  {
    "text": "And then please\nask me a question. OK. So the next method I want\nto show is called LightGCN.",
    "start": "2602050",
    "end": "2610869"
  },
  {
    "text": "And it was also developed with\nrecommender systems in mind.",
    "start": "2610870",
    "end": "2616730"
  },
  {
    "text": "So here is how to\nthink of this, right? The neural graph collaborative\nfiltering jointly",
    "start": "2616730",
    "end": "2624550"
  },
  {
    "text": "learns two kinds of parameters,\nthe shallow user item embeddings and the\nGNN parameters.",
    "start": "2624550",
    "end": "2631480"
  },
  {
    "text": "The observation is that these\nshallow learnable embeddings are already quite\nexpressive, right?",
    "start": "2631480",
    "end": "2638589"
  },
  {
    "text": "There learned for every user\nand every item separately. And if you look at where\nare the free parameters,",
    "start": "2638590",
    "end": "2647320"
  },
  {
    "text": "most of the free parameters\nare in the shallow embeddings where the number of\nitems nodes users",
    "start": "2647320",
    "end": "2653829"
  },
  {
    "text": "is much bigger\nthan the embedding dimensionality, right? Because the amount of parameters\nin the shallow embedding",
    "start": "2653830",
    "end": "2660789"
  },
  {
    "text": "is N times D. And the GNN\nwill have this squared number of parameters, right?",
    "start": "2660790",
    "end": "2668320"
  },
  {
    "text": "Or that order, right? So that is much more modeling\npower here than here. So what the intuition is that\nmaybe these GNN parameters are",
    "start": "2668320",
    "end": "2678700"
  },
  {
    "text": "not so essential in this. So the question is, can\nwe simplify the GNN part",
    "start": "2678700",
    "end": "2686320"
  },
  {
    "text": "and maybe make the model more\nefficient, easier to learn?",
    "start": "2686320",
    "end": "2691870"
  },
  {
    "text": "And the answer is yes. And what people have also\nfound is that, actually, it",
    "start": "2691870",
    "end": "2697880"
  },
  {
    "text": "gives you a slight boost\nin performance, again, on benchmarks as well.",
    "start": "2697880",
    "end": "2703590"
  },
  {
    "text": "So let's take a look\nat how does this work. The idea goes as follows.",
    "start": "2703590",
    "end": "2710140"
  },
  {
    "text": "We are going to have\nan adjacency matrix for the bipartite graph. And I'm going to use\nthe matrix formulation",
    "start": "2710140",
    "end": "2717119"
  },
  {
    "text": "of the GCN of the graph\nconvolutional neural network, right? This is lecture 4 or\n5, something like that,",
    "start": "2717120",
    "end": "2724830"
  },
  {
    "text": "right? [? Jichuan ?]\nwas talking about this. Matrix formulation of a GCN.",
    "start": "2724830",
    "end": "2730260"
  },
  {
    "text": "And then we are actually\ngoing to simplify the GCN by removing non-linearity.",
    "start": "2730260",
    "end": "2736800"
  },
  {
    "text": "And this is similar to\nthis method that is called SGC that is for scalable GNNs.",
    "start": "2736800",
    "end": "2744000"
  },
  {
    "text": "And I think this lecture will\ncome next week or in two weeks. OK, so we are going\nto learn about this.",
    "start": "2744000",
    "end": "2750930"
  },
  {
    "text": "And the two ideas are related. So let me now tell you\nabout this LightGCN.",
    "start": "2750930",
    "end": "2758040"
  },
  {
    "text": "So we are going to take\nthe adjacency matrix of an undirected\nbipartite graph.",
    "start": "2758040",
    "end": "2763500"
  },
  {
    "text": "So the way the adjacency\nmatrix of a bipartite graph looks like, there\nare two blocks of 0s",
    "start": "2763500",
    "end": "2769090"
  },
  {
    "text": "because items don't\nlink to items and users don't link to users. But there are non-zero\nentries here and here.",
    "start": "2769090",
    "end": "2776500"
  },
  {
    "text": "And one is the transposed\nversion of the other because the graph is undirected.",
    "start": "2776500",
    "end": "2783520"
  },
  {
    "text": "OK. Here RUV is 1 If the user U\ninteracts with item v and 0",
    "start": "2783520",
    "end": "2790270"
  },
  {
    "text": "otherwise. And then we also have\nan embedding matrix that has the embeddings\nof all the users",
    "start": "2790270",
    "end": "2796210"
  },
  {
    "text": "and the embeddings\nof all the items. OK. That's the setting.",
    "start": "2796210",
    "end": "2802980"
  },
  {
    "text": "So now, try to remind you what\nwas the matrix formulation",
    "start": "2802980",
    "end": "2809810"
  },
  {
    "text": "of a GCN. And the key concept here is this\nnotion of a diffusion matrix.",
    "start": "2809810",
    "end": "2816350"
  },
  {
    "text": "And another connection between\nthe lecture today and what we are going to talk about\nnow is the connection",
    "start": "2816350",
    "end": "2821450"
  },
  {
    "text": "to this correct\nand smooth method. That was I know. Lecture 9 or something,\nwhere we talked",
    "start": "2821450",
    "end": "2827870"
  },
  {
    "text": "about this semi-supervised\ncorrect and smooth method that was propagating the\nlabels across the graph.",
    "start": "2827870",
    "end": "2835100"
  },
  {
    "text": "And the key concept\nthere was this notion of diffusion matrix, where we\nhave D to be a degree matrix.",
    "start": "2835100",
    "end": "2842760"
  },
  {
    "text": "So this is a\ndiagonal matrix that has degrees of nodes\non the diagonal. And then we define this A\ntilde as a normalized adjacency",
    "start": "2842760",
    "end": "2853250"
  },
  {
    "text": "matrix to be the degree matrix\ntimes the adjacency matrix,",
    "start": "2853250",
    "end": "2859850"
  },
  {
    "text": "times the properly\nprocessed degree matrix. And then let E of K be the\nembedding matrix at the K",
    "start": "2859850",
    "end": "2868800"
  },
  {
    "text": "layer. And again, the embedding\nmatrix is simply each row stores an\nembedding of a proper node.",
    "start": "2868800",
    "end": "2877730"
  },
  {
    "text": "The difference here\nbetween the GCN, the original GCN formulation\nis that here there",
    "start": "2877730",
    "end": "2885080"
  },
  {
    "text": "is no self-connections, right? In the GCN, we added a\nself-loop to every node. Here, we don't.",
    "start": "2885080",
    "end": "2891080"
  },
  {
    "text": "OK. And then each layer\nof GCN's segregation",
    "start": "2891080",
    "end": "2896250"
  },
  {
    "text": "can be written in\nthe matrix form. This was that lecture\nthat I was referring",
    "start": "2896250",
    "end": "2901680"
  },
  {
    "text": "to, where basically you can\ntake this diffusion matrix embeddings at a layer\nK, the weight matrix",
    "start": "2901680",
    "end": "2911010"
  },
  {
    "text": "to transform the\nembeddings, send it through a non-linearity,\nand you get embeddings at the next layer.",
    "start": "2911010",
    "end": "2917080"
  },
  {
    "text": "So that's a matrix version of\na graph neural network, where",
    "start": "2917080",
    "end": "2923490"
  },
  {
    "text": "you work with the\nentire graph at once, and, you propagate step by\nstep from all the nodes using--",
    "start": "2923490",
    "end": "2930000"
  },
  {
    "text": "Here basically what happens\nis neighborhood aggregation, right? Because you are taking\nan adjacency matrix",
    "start": "2930000",
    "end": "2936240"
  },
  {
    "text": "and multiplying it with\nthe embedding matrix. And these are the learnable\nlinear transformations.",
    "start": "2936240",
    "end": "2946170"
  },
  {
    "text": "So that's the idea here. So what does now LightGCN does?",
    "start": "2946170",
    "end": "2951400"
  },
  {
    "text": "LightGCN says, if you are\niterating this equation up there, and you remove\nthe non-linearity, right?",
    "start": "2951400",
    "end": "2957680"
  },
  {
    "text": "I had a reLU here. Now, I removed it. And now my iteration\nsays that EK plus 1",
    "start": "2957680",
    "end": "2964870"
  },
  {
    "text": "equals A tilde times\nAK times WK, right? Then if I take this recursion\nand expand it, then basically,",
    "start": "2964870",
    "end": "2973630"
  },
  {
    "text": "I say E of K is A tilde. EK minus 1 times W. K minus 1.",
    "start": "2973630",
    "end": "2979089"
  },
  {
    "text": "So I can take this EK and\nthrow it into that EK. So here is what I get, right?",
    "start": "2979090",
    "end": "2985180"
  },
  {
    "text": "And now I can expand this. And I get A tilde times A tilde\ntimes EK minus 2W, K minus 2W,",
    "start": "2985180",
    "end": "2995102"
  },
  {
    "text": "K minus 1. OK. And you can see now\nthat I can take this one and further unfold my recursion.",
    "start": "2995102",
    "end": "3004980"
  },
  {
    "text": "OK. And at the end, when\nI have unrolled this all the way to the 0,\nthis is what I will get.",
    "start": "3004980",
    "end": "3011560"
  },
  {
    "text": "OK. I'll get this multiplication\nof A tilde K times. Then I'll get the E0,\nthe initial embeddings.",
    "start": "3011560",
    "end": "3020380"
  },
  {
    "text": "And then I'll get this\nparameter matrices multiplied. K multiplied together.",
    "start": "3020380",
    "end": "3026800"
  },
  {
    "text": "OK. But what's the point? The point is that\nthese A tildes-- this is just A tilde raised\nto the power of capital K.",
    "start": "3026800",
    "end": "3034269"
  },
  {
    "text": "This 0 are the embeddings. And all these parameter\nmatrices don't really",
    "start": "3034270",
    "end": "3040210"
  },
  {
    "text": "matter because I can just\ncollapse them together into a single parameter matrix.",
    "start": "3040210",
    "end": "3046420"
  },
  {
    "text": "OK. So what this simplifies\nto, it simplifies",
    "start": "3046420",
    "end": "3051710"
  },
  {
    "text": "to the following thing. It says that the embedding\nmatrix at layer K is simply the A tilde\nraised to the power K times",
    "start": "3051710",
    "end": "3061250"
  },
  {
    "text": "the initial embeddings times\nparameter matrix W. OK.",
    "start": "3061250",
    "end": "3066410"
  },
  {
    "text": "And this basically means that\nI'm diffusing node embeddings along the graph in a similar\nway that what correct and smooth",
    "start": "3066410",
    "end": "3074569"
  },
  {
    "text": "did using the soft\nlabels in the graph. So what is the algorithm?",
    "start": "3074570",
    "end": "3080890"
  },
  {
    "text": "Algorithm that these\npeople are proposing is that you are simply\nrunning this iteration. You take E multiply\nit with A tilde",
    "start": "3080891",
    "end": "3088010"
  },
  {
    "text": "to get new E. Put it\nhere, A tilde, and so on. So each matrix multiplication\ndiffuses the current embeddings",
    "start": "3088010",
    "end": "3096440"
  },
  {
    "text": "to one hot neighbors. And note that A to the\nK is a dense matrix.",
    "start": "3096440",
    "end": "3104759"
  },
  {
    "text": "So if I run this\nas an iteration, this A raised to\nthe power of K never",
    "start": "3104760",
    "end": "3110700"
  },
  {
    "text": "really gets materialized, right? So I don't have to\nmaterialize this A tilde raised to the power of K\nbecause this will become a very",
    "start": "3110700",
    "end": "3117839"
  },
  {
    "text": "dense matrix. Because every power creates,\nconnects nodes that are two",
    "start": "3117840",
    "end": "3123000"
  },
  {
    "text": "hops away. This was something I explained\nin the first second lecture if you guys remember.",
    "start": "3123000",
    "end": "3128250"
  },
  {
    "text": "OK. So this is basically iterative\nmatrix-vector product is",
    "start": "3128250",
    "end": "3133830"
  },
  {
    "text": "useful to compute\nthis expression, but much more scalable\nbecause I'm just",
    "start": "3133830",
    "end": "3139079"
  },
  {
    "text": "doing this multiplication\nseveral times. OK. So this is the basic\nversion of this algorithm.",
    "start": "3139080",
    "end": "3148540"
  },
  {
    "text": "What people do in reality\nis they call what they say multi-scale diffusion, right?",
    "start": "3148540",
    "end": "3155680"
  },
  {
    "text": "So what they say is\nthey say, basically,",
    "start": "3155680",
    "end": "3161579"
  },
  {
    "text": "let's weigh\ncontribution from nodes that are farther out less than\nfrom the nodes that are close.",
    "start": "3161580",
    "end": "3169380"
  },
  {
    "text": "OK. So the way they do it is they\ncome up with these weights, alpha. Alpha 0, alpha 1,\nalpha 2, and so on.",
    "start": "3169380",
    "end": "3176950"
  },
  {
    "text": "And this is now multi-scale\nbecause these neighbors at different layers\ncontribute different amount.",
    "start": "3176950",
    "end": "3187369"
  },
  {
    "text": "And this includes embeddings\ndiffused, a multi-hop scales.",
    "start": "3187370",
    "end": "3192780"
  },
  {
    "text": "And this alpha here is how\nyou can write it out, right?",
    "start": "3192780",
    "end": "3198890"
  },
  {
    "text": "Alpha 0 times-- E0 equals\nalpha 0 to A tilde 0.",
    "start": "3198890",
    "end": "3204289"
  },
  {
    "text": "And E0. X as a self-connection\nthat is omitted",
    "start": "3204290",
    "end": "3209900"
  },
  {
    "text": "in the definition of A tilde. And the coefficients,\nthis alpha 0 to alpha K",
    "start": "3209900",
    "end": "3217339"
  },
  {
    "text": "are hyperparameters. And what do people like to\ndo, for example, just as a way",
    "start": "3217340",
    "end": "3222770"
  },
  {
    "text": "to set these alpha parameters. LightGCN uses this\nuniform coefficient",
    "start": "3222770",
    "end": "3228920"
  },
  {
    "text": "where basically they say alpha\nK is just 1 minus k plus 1. So here, they would\nactually set the alpha",
    "start": "3228920",
    "end": "3240260"
  },
  {
    "text": "to be the same for all\nalpha 0 to alpha capital K.",
    "start": "3240260",
    "end": "3247440"
  },
  {
    "text": "OK. So how does this work? Let me try to summarize this.",
    "start": "3247440",
    "end": "3252960"
  },
  {
    "text": "So, basically, we are going\nto iteratively diffuse embedding matrix E\nusing this A tilde",
    "start": "3252960",
    "end": "3258890"
  },
  {
    "text": "matrix that I discussed, right? So from little K going\nfrom 0 to K minus 1,",
    "start": "3258890",
    "end": "3265309"
  },
  {
    "text": "I'm going to take the embedding,\nthe current embedding matrix.",
    "start": "3265310",
    "end": "3271640"
  },
  {
    "text": "And then basically, I'm\ngoing to multiply it with this normalized\nadjacency matrix",
    "start": "3271640",
    "end": "3276770"
  },
  {
    "text": "L A tilde with self-loop omitted\nto get the next level embedding",
    "start": "3276770",
    "end": "3283090"
  },
  {
    "text": "matrix. And then I'm going to put\nthis here and multiply again. And I'm going to iterate this\nfor capital K number of steps.",
    "start": "3283090",
    "end": "3294560"
  },
  {
    "text": "Yes. Two questions. The first one is,\nif you are Amazon and you have a billion products,\nhow are you doing this?",
    "start": "3294560",
    "end": "3302610"
  },
  {
    "text": "I mean, are you doing\nthis matrix multiplication all in memory? And then the second one\nis, is this related?",
    "start": "3302610",
    "end": "3309150"
  },
  {
    "text": "Is this like a generalization\nof the [? Katz ?] matrix to the normalized\nadjacency matrix?",
    "start": "3309150",
    "end": "3314880"
  },
  {
    "text": "Two good questions. So the first question\nwas about scale.",
    "start": "3314880",
    "end": "3320460"
  },
  {
    "text": "Yeah, multiplying big\nmatrices is not a problem. I don't know how to say.",
    "start": "3320460",
    "end": "3326140"
  },
  {
    "text": "If you take my\nclass in the spring, you will see you can multiply\nreally, really huge matrices",
    "start": "3326140",
    "end": "3334290"
  },
  {
    "text": "if need be. You can even use\nMapReduce and you can multiply the matrix\ndescribing the entire web",
    "start": "3334290",
    "end": "3341220"
  },
  {
    "text": "and things like that. So this multiplication,\nyes, it's non-trivial. But you can do.",
    "start": "3341220",
    "end": "3346950"
  },
  {
    "text": "What becomes a bottleneck\nis storing these embeddings.",
    "start": "3346950",
    "end": "3352290"
  },
  {
    "text": "But you can distribute\nthem across machines. You can distribute parts of\nthis matrix across machines.",
    "start": "3352290",
    "end": "3358349"
  },
  {
    "text": "You need some engineering,\nbut it's very solvable. And then the second\nquestion was about cuts.",
    "start": "3358350",
    "end": "3365880"
  },
  {
    "text": "Is this similar to cuts? Yes, it's similar, right? It's similar in a\nsense that in cuts, we",
    "start": "3365880",
    "end": "3372360"
  },
  {
    "text": "had this alphas as well. But they were decaying\nexponentially, right? And then there was this closed\nforum that we discussed.",
    "start": "3372360",
    "end": "3379590"
  },
  {
    "text": "Here these alphas are-- these people decided\nto choose it this way.",
    "start": "3379590",
    "end": "3389260"
  },
  {
    "text": "It's a good question. It's a good question. What would you do if you do not\nLightGCN, but you do a cut GCN?",
    "start": "3389260",
    "end": "3399480"
  },
  {
    "text": "It'd be cool to at least spend\nsome time on Google Scholar and see if anyone\nhas tried to do that. But it's an\ninteresting question.",
    "start": "3399480",
    "end": "3406030"
  },
  {
    "text": "So what the authors\npropose here is that basically you store all\nthese intermediate embedding",
    "start": "3406030",
    "end": "3412140"
  },
  {
    "text": "matrices. And at the end, you just\naverage them together. This is this uniform\nalpha, right?",
    "start": "3412140",
    "end": "3417540"
  },
  {
    "text": "You just average them together. And that gives you your\nfinal embedding matrix.",
    "start": "3417540",
    "end": "3423640"
  },
  {
    "text": "OK? So that's the overview\nof this method, where rather than iterating\nand taking the last one,",
    "start": "3423640",
    "end": "3433080"
  },
  {
    "text": "you remember the\nintermediate ones as well and average them up into your\nfinal embedding for the users",
    "start": "3433080",
    "end": "3441150"
  },
  {
    "text": "and for the items. And then, as I said,\nthe score function",
    "start": "3441150",
    "end": "3447030"
  },
  {
    "text": "is simply a dot product. You take a user,\nyou take an item, you dot product the two vectors,\nand that will be your score.",
    "start": "3447030",
    "end": "3457510"
  },
  {
    "text": "OK. So that's about this LightGCN. That is amazingly simple.",
    "start": "3457510",
    "end": "3465380"
  },
  {
    "text": "You only take this\nadjacency matrix. You pre-process it\nusing the degree matrix.",
    "start": "3465380",
    "end": "3471790"
  },
  {
    "text": " And then you run the\niteration, right?",
    "start": "3471790",
    "end": "3479080"
  },
  {
    "text": "So the overview is you are\ngiven the adjacency matrix.",
    "start": "3479080",
    "end": "3484990"
  },
  {
    "text": "You take this initial\nlearnable embedding matrix E. So I use the shallow\nembedding matrix E",
    "start": "3484990",
    "end": "3492880"
  },
  {
    "text": "that I learned ahead of time. And then basically,\nall I need to do is take the adjacency matrix,\nnormalize it to this a tilde,",
    "start": "3492880",
    "end": "3500770"
  },
  {
    "text": "and then multiply this learned\nembedding matrix with itself a couple of times, and then\naverage the results, right?",
    "start": "3500770",
    "end": "3506830"
  },
  {
    "text": "So the LightGCN in this respect\nis a post-processing step",
    "start": "3506830",
    "end": "3512140"
  },
  {
    "text": "where in the first step, I learn\nthe shallow embeddings using the BPR loss or whatever.",
    "start": "3512140",
    "end": "3518770"
  },
  {
    "text": "And then I propagate\nthese embeddings",
    "start": "3518770",
    "end": "3523930"
  },
  {
    "text": "or multiply them several times\nwith the normalized adjacency matrix, average those\nresulting embedding",
    "start": "3523930",
    "end": "3530980"
  },
  {
    "text": "matrices together to get a\nbetter version of embeddings. So in this LightGCN,\nthere is no learning",
    "start": "3530980",
    "end": "3538540"
  },
  {
    "text": "after I learned the\ninitial embeddings. OK. So that's the key idea here.",
    "start": "3538540",
    "end": "3547779"
  },
  {
    "text": "Right. And why does this\nsimple diffusion",
    "start": "3547780",
    "end": "3553900"
  },
  {
    "text": "propagation works so well? The answer is that\ndiffusion directly encourages the embeddings\nof similar users items",
    "start": "3553900",
    "end": "3561670"
  },
  {
    "text": "to be similar, right? So similar users share many\ncommon neighbors, items, and in",
    "start": "3561670",
    "end": "3568600"
  },
  {
    "text": "this respect, expected to have\nsimilar future preferences. Which means that when you are\naggregating these embeddings",
    "start": "3568600",
    "end": "3575620"
  },
  {
    "text": "from similar users and averaging\nthem, you are, in some sense, amplifying this\nsimilarity between users.",
    "start": "3575620",
    "end": "3584140"
  },
  {
    "text": "And you could make\nthe same argument for items is the two\nitems are similar if they",
    "start": "3584140",
    "end": "3589900"
  },
  {
    "text": "have a lot of users in common\nwho interact with them.",
    "start": "3589900",
    "end": "3596619"
  },
  {
    "text": "So that's the idea here. ",
    "start": "3596620",
    "end": "3602360"
  },
  {
    "text": "I maybe want to just\nmake a quick comparison to situate the LightGCN, and\nthe original GCN, and also",
    "start": "3602360",
    "end": "3612350"
  },
  {
    "text": "the correct and\nsmooth method, and say what the differences were. Is that the embedding\npropagation in GCN is close.",
    "start": "3612350",
    "end": "3619520"
  },
  {
    "text": "In LightGCN is closely related\nto these other two methods. And in GCN and correct and\nsmooth, the way we did it we",
    "start": "3619520",
    "end": "3628730"
  },
  {
    "text": "said that in terms of\npropagation when we sum over the neighbors, we normalize\nthe embeddings by the 1",
    "start": "3628730",
    "end": "3638780"
  },
  {
    "text": "over the square root of the\nproduct, of the degrees,",
    "start": "3638780",
    "end": "3644540"
  },
  {
    "text": "of nodes u and node v.",
    "start": "3644540",
    "end": "3651410"
  },
  {
    "text": "And we had the self-loop to\nbe as part of this iteration.",
    "start": "3651410",
    "end": "3656539"
  },
  {
    "text": "The LightGCN uses\nthe same equation, except that the self-loop is\nnot added in the neighborhood",
    "start": "3656540",
    "end": "3663869"
  },
  {
    "text": "definition. And final embedding\ntakes the average",
    "start": "3663870",
    "end": "3668970"
  },
  {
    "text": "from embeddings from\nall layers, right? So the final embedding is the\nsum of the individual layer",
    "start": "3668970",
    "end": "3676530"
  },
  {
    "text": "embeddings, rather than\nsaying the final embedding is the final layer\nembedding, right?",
    "start": "3676530",
    "end": "3682260"
  },
  {
    "text": "So h of v is not. H of v at capital K.\nBut is a sum of h of v",
    "start": "3682260",
    "end": "3690300"
  },
  {
    "text": "for k is going from\nup to capital K. So those are the two key,\nthe two key differences.",
    "start": "3690300",
    "end": "3698799"
  },
  {
    "text": "So with this, let\nme just summarize",
    "start": "3698800",
    "end": "3705800"
  },
  {
    "text": "what this LightGCN did is. It took this neural graph\nconvolutional filtering",
    "start": "3705800",
    "end": "3711650"
  },
  {
    "text": "by basically removing the\nlearning parameters of the GNN. And now, all the\nlearnable parameters",
    "start": "3711650",
    "end": "3719600"
  },
  {
    "text": "are in the shallow\ninput node embeddings.",
    "start": "3719600",
    "end": "3725120"
  },
  {
    "text": "And the diffusion\npropagation only involves matrix-vector\nmultiplication.",
    "start": "3725120",
    "end": "3730910"
  },
  {
    "text": "And the simplification\nleads to a bit better empirical performance.",
    "start": "3730910",
    "end": "3736230"
  },
  {
    "text": "So that's what I wanted to\nsay about the core two methods",
    "start": "3736230",
    "end": "3743060"
  },
  {
    "text": "that we wanted to\ntalk about today. I'm now happy to take questions.",
    "start": "3743060",
    "end": "3750930"
  },
  {
    "text": "I see your hands. And what I'll do\nafter the questions, I'll tell you about an\nindustrial deployment of GNNs",
    "start": "3750930",
    "end": "3759170"
  },
  {
    "text": "for recommendation systems. It's called PinSAGE. And this runs and was\ndeveloped at Pinterest",
    "start": "3759170",
    "end": "3766980"
  },
  {
    "text": "to a huge success I would say. So I can say more. Yes. Is there an intuition why\nthe graph-aware embeddings",
    "start": "3766980",
    "end": "3774240"
  },
  {
    "text": "are better than the\none-hop embeddings? Because I think the\nintuition you provided, I didn't fully understand where\nthe benefit is given that it's",
    "start": "3774240",
    "end": "3782940"
  },
  {
    "text": "like a bipartite graph. It seems like even with\none-hop embeddings, you're getting like\nembeddings where",
    "start": "3782940",
    "end": "3788370"
  },
  {
    "text": "the nodes are similar if\ntheir neighbors are similar. So is there an intuition for\nthe type of graph-aware feature",
    "start": "3788370",
    "end": "3793920"
  },
  {
    "text": "that you might be interested in? Yes, good question. So why does this help?",
    "start": "3793920",
    "end": "3799020"
  },
  {
    "text": "And the reason it helps is\nbecause it smooths explicitly",
    "start": "3799020",
    "end": "3804120"
  },
  {
    "text": "the embeddings across\nthe graph a bit, right? And your objective function\ndoesn't enforce any smoothness,",
    "start": "3804120",
    "end": "3813550"
  },
  {
    "text": "right? It doesn't explicitly\nsay, hey, you have to be similar because\nyou are connected, right? Yes, there is some\nterm in there.",
    "start": "3813550",
    "end": "3820800"
  },
  {
    "text": "But there is a lot of\nother terms as well. While these smoothing\ntype things.",
    "start": "3820800",
    "end": "3827289"
  },
  {
    "text": "they really force\nembeddings to be more",
    "start": "3827290",
    "end": "3832770"
  },
  {
    "text": "similar across connections. So I would say in some\nsense, it smooths the data,",
    "start": "3832770",
    "end": "3838090"
  },
  {
    "text": "it may be denoise the data bit. And that's why you get added,\nwhy you get added benefit.",
    "start": "3838090",
    "end": "3844530"
  },
  {
    "text": "It might be also that\nsome users are very noisy. But you are still just training\nthem based on their own data.",
    "start": "3844530",
    "end": "3850380"
  },
  {
    "text": "But now because it takes some\nother user, the information it stabilizes the user itself.",
    "start": "3850380",
    "end": "3857710"
  },
  {
    "text": "So it averages the embeddings\na bit across the graph, makes them more\nsmooth, and actually",
    "start": "3857710",
    "end": "3862769"
  },
  {
    "text": "makes them a bit more\nstable because of that. Yes. This idea of summing all\nyour H vectors of all layers,",
    "start": "3862770",
    "end": "3872940"
  },
  {
    "text": "is this specific to\nthis type of problem? Or can that also\nbe like transferred",
    "start": "3872940",
    "end": "3878130"
  },
  {
    "text": "to previous prediction tasks? Yeah. Because it doesn't\nseem specific to this--",
    "start": "3878130",
    "end": "3884840"
  },
  {
    "text": "Great question. Great question. So the question, I'm repeating\nbecause I don't think people",
    "start": "3884840",
    "end": "3889850"
  },
  {
    "text": "online are hearing. The question was, this averaging\nof embeddings across layers",
    "start": "3889850",
    "end": "3896600"
  },
  {
    "text": "is different than what\nwe've been doing so far. Because we always\nsaid, oh, the embedding is the final layer\nembedding, right?",
    "start": "3896600",
    "end": "3903860"
  },
  {
    "text": "And you are saying, is\nthis a general approach? Because it doesn't\nlook like it's",
    "start": "3903860",
    "end": "3909350"
  },
  {
    "text": "related to recommender systems. I would say, yes,\nyou are correct. You could try this\nstrategy elsewhere as well.",
    "start": "3909350",
    "end": "3916319"
  },
  {
    "text": "Originally, it's been developed\nin the context of recommender systems. And that's also where it gives\nprobably the best performance.",
    "start": "3916320",
    "end": "3926890"
  },
  {
    "text": "But yeah, it's a\nmore general idea. So that's a good observation.",
    "start": "3926890",
    "end": "3932120"
  },
  {
    "text": "Anything else? ",
    "start": "3932120",
    "end": "3937880"
  },
  {
    "text": "OK. So let me tell you about\nthis PinSAGE or Pinterest",
    "start": "3937880",
    "end": "3945430"
  },
  {
    "text": "recommender system\nthat's graph-based. What is the idea? The idea is that\ngiven a query image,",
    "start": "3945430",
    "end": "3953530"
  },
  {
    "text": "you want to recommend related\nimages or related pins. And the way you\nwant to do this is",
    "start": "3953530",
    "end": "3960940"
  },
  {
    "text": "that you want to create this\nembedding of a pin, right? And a pin is not just the\nimage, it includes a description",
    "start": "3960940",
    "end": "3970900"
  },
  {
    "text": "like you see here and it also\nincludes the graph information.",
    "start": "3970900",
    "end": "3976450"
  },
  {
    "text": "And for a while, this was the\nlargest industrial deployment of GNNs that got hugely adopted\nacross all kinds of products",
    "start": "3976450",
    "end": "3986770"
  },
  {
    "text": "and services across Pinterest. And it allows you to basically\nboth do fresh content.",
    "start": "3986770",
    "end": "3995620"
  },
  {
    "text": "So as a new pin is\ncreated, it's basically embedded in this\npin embedding space.",
    "start": "3995620",
    "end": "4003600"
  },
  {
    "text": "And then these embeddings\nare consumed down the line. And the core of this approach\nis a graph convolutional neural",
    "start": "4003600",
    "end": "4013530"
  },
  {
    "text": "network. And the goal is to\ncreate embeddings of nodes in a large-scale\nPinterest graph.",
    "start": "4013530",
    "end": "4020170"
  },
  {
    "text": "And I'm going to explain with\ntens, hundreds of billions of objects. And the key idea here was\nto really borrow information",
    "start": "4020170",
    "end": "4028020"
  },
  {
    "text": "from nearby from nearby nodes. What happens if you only\nuse visual information",
    "start": "4028020",
    "end": "4035640"
  },
  {
    "text": "to train your models then you\nmake mistakes like this, right? You confuse a steel fence\nfor a rail of a bed.",
    "start": "4035640",
    "end": "4044520"
  },
  {
    "text": "You confuse ground\nmeat and soil. You confuse a rug\nand a tapestry.",
    "start": "4044520",
    "end": "4051839"
  },
  {
    "text": "So you create these things\nthat look visually the same, but are semantically\ncompletely different.",
    "start": "4051840",
    "end": "4057759"
  },
  {
    "text": "So it is essential to have a\nhigh-quality embedding that you",
    "start": "4057760",
    "end": "4063220"
  },
  {
    "text": "can then use for a\nlot of different tasks from search, to ads,\nto content safety,",
    "start": "4063220",
    "end": "4069700"
  },
  {
    "text": "and so on, and so forth. So now, how was\nthis built, right?",
    "start": "4069700",
    "end": "4074710"
  },
  {
    "text": "What is the key component\nof what Pinterest users do is that you can think of\nit as they are looking",
    "start": "4074710",
    "end": "4082150"
  },
  {
    "text": "at these images and\nthey are collecting them in different boards. So they are creating\nthis bipartite graph",
    "start": "4082150",
    "end": "4088930"
  },
  {
    "text": "between pins and boards. And board is just a\ncollection of images, right?",
    "start": "4088930",
    "end": "4094119"
  },
  {
    "text": "So maybe somebody is\ninterested in men's style, how to dress well, and\nthey are browsing images",
    "start": "4094120",
    "end": "4100568"
  },
  {
    "text": "and say, oh, I like this\none, I like that one. Then they add them together. And then some other user\nmight be also collecting",
    "start": "4100569",
    "end": "4108310"
  },
  {
    "text": "how men can dress well. And somebody else\nmight be into plants. So they are collecting different\narrangements of plants.",
    "start": "4108310",
    "end": "4115870"
  },
  {
    "text": "And somebody might be in\narchitecture, and so on, and so forth. So we have billions\nof these boards",
    "start": "4115870",
    "end": "4121960"
  },
  {
    "text": "where people curate content. And especially if you are\ndoing home remodeling, if you want to figure out\nhow to throw a birthday",
    "start": "4121960",
    "end": "4129880"
  },
  {
    "text": "party or something, usually,\npeople create a board and then browse and say,\noh, these are ideas I like.",
    "start": "4129880",
    "end": "4135189"
  },
  {
    "text": "It's almost like\nwindow shopping. OK. So in this graph, we\nhave two types of nodes.",
    "start": "4135189",
    "end": "4140949"
  },
  {
    "text": "We have the pins and we\nhave these boards, right? And the link is that a\npin belongs to a board",
    "start": "4140950",
    "end": "4146020"
  },
  {
    "text": "and a same pin can belong\nto multiple boards. And the graph has tens of\nbillions of nodes and edges.",
    "start": "4146020",
    "end": "4152620"
  },
  {
    "text": "And the way we create\nan embedding of a node is that basically,\nespecially of a pin,",
    "start": "4152620",
    "end": "4158770"
  },
  {
    "text": "is that basically we have a\nGCN across this graph, right? At the bottom\nlayer, we are going",
    "start": "4158770",
    "end": "4164830"
  },
  {
    "text": "to have embeddings or\nfeatures of individual pins.",
    "start": "4164830",
    "end": "4169899"
  },
  {
    "text": "This would be images, and so on. But then we have a GCN\nthat aggregates this. So what this means is that\nthe embedding of a given pin",
    "start": "4169899",
    "end": "4177939"
  },
  {
    "text": "will depend on the\nembeddings of other pins that it shares boards with.",
    "start": "4177939",
    "end": "4186068"
  },
  {
    "text": "And now, you can basically\nbackpropagate into this GCN",
    "start": "4186069",
    "end": "4191380"
  },
  {
    "text": "to learn how to optimally\ncollect information from neighbors to create an\nembedding of a given object.",
    "start": "4191380",
    "end": "4198310"
  },
  {
    "text": "And why does this help\nis because a garden fence is going to be pinned\nwith other garden fences.",
    "start": "4198310",
    "end": "4206650"
  },
  {
    "text": "Some of those garden\nfences will really look like a garden fence. So even the garden\nfence that seems",
    "start": "4206650",
    "end": "4212080"
  },
  {
    "text": "to look like a bad railing\nwill collect embeddings of other garden fences\nand say, hey, actually,",
    "start": "4212080",
    "end": "4218800"
  },
  {
    "text": "I'm not a bad rail. I'm a garden fence, right? And a bad will say,\nwhat am I pinned with?",
    "start": "4218800",
    "end": "4224380"
  },
  {
    "text": "And it will be pinned\nwith good-looking bads. So that bad will say, hey,\nI'm truly a bad, right?",
    "start": "4224380",
    "end": "4230980"
  },
  {
    "text": "So that's the intuition. What's happening here?",
    "start": "4230980",
    "end": "4236679"
  },
  {
    "text": "And what's resting\nin this work is that in addition to the\nmodel, the innovation,",
    "start": "4236680",
    "end": "4244230"
  },
  {
    "text": "the paper introduces\nseveral methods to scale up GCN to this billion-node\nscale and to the production",
    "start": "4244230",
    "end": "4251250"
  },
  {
    "text": "workloads. And I'll give you\na few examples. One was across negative\nsamples that there",
    "start": "4251250",
    "end": "4257220"
  },
  {
    "text": "was a question\nearlier, especially around hard negatives\ncurriculum learning",
    "start": "4257220",
    "end": "4262650"
  },
  {
    "text": "and also mini batch\ntraining of GCNs, right? Usually, people would have this\nmatrix formulation of a GNN.",
    "start": "4262650",
    "end": "4271710"
  },
  {
    "text": "This was one of the first\npapers that said, hey, let's do the mini batch\nwith the explicit sampling",
    "start": "4271710",
    "end": "4278190"
  },
  {
    "text": "of the computation graphs\nand learning over that. So the way this\nsystem was supervised",
    "start": "4278190",
    "end": "4284700"
  },
  {
    "text": "was supervised, in some sense,\nlink prediction task where the idea is that I\nwant to recommend",
    "start": "4284700",
    "end": "4290880"
  },
  {
    "text": "related pins to the user. So I have a query. I have a successful\nrecommendation and an unsuccessful\nrecommendation.",
    "start": "4290880",
    "end": "4297630"
  },
  {
    "text": "And the objective function\nwas that the distance between the query and the\npositive recommendation",
    "start": "4297630",
    "end": "4304320"
  },
  {
    "text": "has to be smaller than\nthe query and the sweater. So the GNNs will go this way.",
    "start": "4304320",
    "end": "4311280"
  },
  {
    "text": "And then here is where\nthe score gets computed. And then back propagation\ngoes through this two tower",
    "start": "4311280",
    "end": "4318420"
  },
  {
    "text": "architecture. In terms of training data,\nthis was a few years back.",
    "start": "4318420",
    "end": "4325079"
  },
  {
    "text": "But even then they were training\non over a billion pairs.",
    "start": "4325080",
    "end": "4331470"
  },
  {
    "text": "And here are some examples of\npositive training examples,",
    "start": "4331470",
    "end": "4336870"
  },
  {
    "text": "right? And what's interesting\nis that sometimes, they might be quite different,\nbut they are related, right?",
    "start": "4336870",
    "end": "4343929"
  },
  {
    "text": "This is about sea. This is sailors. Here is Navy. Even though this is something\nto put on the door or something,",
    "start": "4343930",
    "end": "4350260"
  },
  {
    "text": "and here's a mug, and\nso on, and so forth. So they have a lot of examples\nof positive training pairs.",
    "start": "4350260",
    "end": "4359060"
  },
  {
    "text": "One innovation to\nmake this scale is these sharing of\nnegative examples.",
    "start": "4359060",
    "end": "4364160"
  },
  {
    "text": "That in a BPR loss for\nevery user in a batch, we sample one positive\ninteraction and then",
    "start": "4364160",
    "end": "4371570"
  },
  {
    "text": "a set of negative interactions. And using more negative\nsamples per user",
    "start": "4371570",
    "end": "4378410"
  },
  {
    "text": "improves recommendation\nperformance, but also slows down training. Because you have to create these\nnegative examples over and over",
    "start": "4378410",
    "end": "4385160"
  },
  {
    "text": "again. So right. If you want to have some\nnumber of positive users,",
    "start": "4385160",
    "end": "4391070"
  },
  {
    "text": "then for each one, you\nneed to create some number of negative examples. And this can become\nquite expensive.",
    "start": "4391070",
    "end": "4398510"
  },
  {
    "text": "OK. So what is the idea? The idea is that we can\nshare the negative examples",
    "start": "4398510",
    "end": "4406100"
  },
  {
    "text": "across all users\nin the mini batch. So the idea is that while\npositive examples are",
    "start": "4406100",
    "end": "4412750"
  },
  {
    "text": "user-specific, now,\nthe negative examples are the same for all the\nusers in the mini batch.",
    "start": "4412750",
    "end": "4418960"
  },
  {
    "text": "This way, we only need\nto generate whatever",
    "start": "4418960",
    "end": "4425680"
  },
  {
    "text": "is the number of\nnegative examples embeddings for negative nodes. And we don't have to resample\nnegative examples every time",
    "start": "4425680",
    "end": "4433900"
  },
  {
    "text": "for each user. And this saves a lot of\nnode embedding generation,",
    "start": "4433900",
    "end": "4439900"
  },
  {
    "text": "a lot of computation\nby basically the factor of the size of your batch.",
    "start": "4439900",
    "end": "4446020"
  },
  {
    "text": "And empirically, the\nperformance stays about similar. But you have just sped up your\ncomputation by a big amount.",
    "start": "4446020",
    "end": "4457120"
  },
  {
    "text": "So that's the first. And then the second one is\nthis idea of hard negatives.",
    "start": "4457120",
    "end": "4464610"
  },
  {
    "text": "And this goes to the questions\nI was getting earlier, right? That you basically in this\nindustrial recommender systems,",
    "start": "4464610",
    "end": "4469963"
  },
  {
    "text": "you need to make these extremely\nfine grained predictions, right? You have billions of items. But then you only care\nabout top 10, top 100.",
    "start": "4469963",
    "end": "4477570"
  },
  {
    "text": "So the issue is that shared\nnegative examples in principle",
    "start": "4477570",
    "end": "4483210"
  },
  {
    "text": "are randomly sampled\nfrom all users. So most of them are\neasy negatives, right?",
    "start": "4483210",
    "end": "4488639"
  },
  {
    "text": "It's so easy to recognize\nwhat's a positive example versus negative examples. So what you want to do is create\nthis notion of a hard negative.",
    "start": "4488640",
    "end": "4497100"
  },
  {
    "text": "And let me give\nyou an idea, right? If this is a source,\nthis is a query,",
    "start": "4497100",
    "end": "4502560"
  },
  {
    "text": "and that's a positive\nexample, right? Then if I just\npick a random pin,",
    "start": "4502560",
    "end": "4507780"
  },
  {
    "text": "it's some random cottage\nsomewhere in the wood. It's very easy to say\nthis is a positive pair",
    "start": "4507780",
    "end": "4513940"
  },
  {
    "text": "and that's a negative pair. But this is an example of a\nhard negative example, right?",
    "start": "4513940",
    "end": "4519849"
  },
  {
    "text": "So these are two. Thank you very much you\nare special type of cards.",
    "start": "4519850",
    "end": "4525400"
  },
  {
    "text": "And this looks like\na card, but it's not. If you see, this is something\nyou can put on a desk",
    "start": "4525400",
    "end": "4532390"
  },
  {
    "text": "and it says happy\nbirthday, right? So this is a hard\nnegative example. And if I just have\nthese random negatives,",
    "start": "4532390",
    "end": "4540130"
  },
  {
    "text": "I'll be learning to make\ndistinction between these too and that too. And that's very easy. So I'll never learn to the\nlevel to say, oh, actually,",
    "start": "4540130",
    "end": "4547090"
  },
  {
    "text": "this is not related. This is very different. OK. So you have this notion\nof hard negative examples.",
    "start": "4547090",
    "end": "4554950"
  },
  {
    "text": "And the key insight is to apply\nthis technique of curriculum learning where you make negative\nexamples gradually harder",
    "start": "4554950",
    "end": "4564369"
  },
  {
    "text": "and harder as the process goes. So that the model\nfocuses on hard",
    "start": "4564370",
    "end": "4569560"
  },
  {
    "text": "to distinguish\nparts of the space and really learns this\nfine-grained distinctions.",
    "start": "4569560",
    "end": "4574849"
  },
  {
    "text": " There are a lot\nof heuristics how you set hard negative examples.",
    "start": "4574850",
    "end": "4581753"
  },
  {
    "text": "At some point in time,\nthey were saying, oh, let's look at something\nthat is ranked at rank 2,000",
    "start": "4581753",
    "end": "4586820"
  },
  {
    "text": "to 5,000. And maybe say that those\nare negative examples. But the correct\nway to do this is",
    "start": "4586820",
    "end": "4592785"
  },
  {
    "text": "this distance-based sampling. So here is the key plot\nthat I want to show to you.",
    "start": "4592785",
    "end": "4598789"
  },
  {
    "text": "So this is the distance\nbetween examples, right? ",
    "start": "4598790",
    "end": "4605170"
  },
  {
    "text": "So this is distance zero. And this is distance infinity. Think of it that way, right?",
    "start": "4605170",
    "end": "4611680"
  },
  {
    "text": "If you do uniform sampling,\nthen basically, most of the pins",
    "start": "4611680",
    "end": "4617260"
  },
  {
    "text": "are unrelated, so they\nwill be very far away. So your positive\nexample is close. But then this exists this gap.",
    "start": "4617260",
    "end": "4624369"
  },
  {
    "text": "And then there are\nyour negative examples. So if you do these hard negative\nexamples that I showed you,",
    "start": "4624370",
    "end": "4631360"
  },
  {
    "text": "then you sample\nfrom this region. So you have a positive example,\nthis region, and that region.",
    "start": "4631360",
    "end": "4636429"
  },
  {
    "text": "Again, it's not good. But if you use this\ndistance weight sampling, then you sample your negatives\nalong the entire distance",
    "start": "4636430",
    "end": "4644330"
  },
  {
    "text": "spectrum. And that really makes\nthe model learn best because you give it examples\nof all kinds of hardnesses",
    "start": "4644330",
    "end": "4651520"
  },
  {
    "text": "rather than giving it super\neasy or super hard examples. And if you do this, then you\nget really, really good results,",
    "start": "4651520",
    "end": "4659570"
  },
  {
    "text": "right? So here's a query pin. And this is the visual\nembeddings nearest neighbors. And here are the graph-based\nnearest neighbors.",
    "start": "4659570",
    "end": "4669080"
  },
  {
    "text": "And you can see how\nbasically-- this is not bad. But this is definitely\nbetter, right?",
    "start": "4669080",
    "end": "4674890"
  },
  {
    "text": "And you get some mistakes. So my last slide\nand then finish.",
    "start": "4674890",
    "end": "4681190"
  },
  {
    "text": "So this PinSAGE uses GNN to\ngenerate high-quality user and item embeddings that\ncapture rich node attributes,",
    "start": "4681190",
    "end": "4689530"
  },
  {
    "text": "as well as the graph structure. The key innovation was\nscaling and the notion",
    "start": "4689530",
    "end": "4696310"
  },
  {
    "text": "of negative sampling. And if you uncover the\ntopic of this lecture",
    "start": "4696310",
    "end": "4706540"
  },
  {
    "text": "is, how do we scale\nGNNs up to large scale? And I'm going to talk\nabout this either I",
    "start": "4706540",
    "end": "4713080"
  },
  {
    "text": "think in the next\none or two lectures. So this is what's yet coming. But I hope you\nliked the lecture.",
    "start": "4713080",
    "end": "4719080"
  },
  {
    "text": "And thank you so much. ",
    "start": "4719080",
    "end": "4726000"
  }
]