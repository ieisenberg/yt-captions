[
  {
    "start": "0",
    "end": "134000"
  },
  {
    "text": "PAX HEHMEYER: Jeannette Bohg\nis an assistant professor of computer science for\nrobotics here at Stanford.",
    "start": "9490",
    "end": "14889"
  },
  {
    "text": "She directs the\ninteractive perception in the robot learning lab. Let me just-- and her research\nis focused on robotic grasping",
    "start": "14890",
    "end": "24490"
  },
  {
    "text": "and manipulation. She has received\nseveral early career and best paper awards,\nmost notably the 2019 IEEE.",
    "start": "24490",
    "end": "30850"
  },
  {
    "text": "I think that's the way you say\nit, Robotics and Automation Society Early Career Award\nand the 2020 Robotics Science",
    "start": "30850",
    "end": "36910"
  },
  {
    "text": "and Systems Early Career Award. It's always really\ninteresting talking to her, and we have a link below\nto another discussion",
    "start": "36910",
    "end": "43570"
  },
  {
    "text": "we did with her and two other\nroboticists here at Stanford. Because I hear so much about-- we all hear so much about\nthe incredible advances",
    "start": "43570",
    "end": "51070"
  },
  {
    "text": "in artificial intelligence\nand computers, and incredible complex\nproblems they're solving.",
    "start": "51070",
    "end": "57370"
  },
  {
    "text": "And then, you talk\nto a roboticist and you find out\nthat there's still some very basic things that\nare a challenge for robots. And in particular, manipulating,\ngrabbing, and moving",
    "start": "57370",
    "end": "65410"
  },
  {
    "text": "basic everyday objects. And so, this is a\nbig part of what Jeanette's research focuses on.",
    "start": "65410",
    "end": "71890"
  },
  {
    "text": "And she'll be talking\ntoday about how her successes in this\nfield and her failures",
    "start": "71890",
    "end": "76960"
  },
  {
    "text": "have informed her approach to\nthis topic and her thinking about the future of it.",
    "start": "76960",
    "end": "82970"
  },
  {
    "text": "And so with that, I'm excited\nto hand it over to Jeanette. One other quick note,\njust a reminder,",
    "start": "82970",
    "end": "88110"
  },
  {
    "text": "we will have a Q&A at the end. So please, submit those\nquestions in the Q&A box.",
    "start": "88110",
    "end": "93280"
  },
  {
    "text": "Jeannette will be also\nsharing her screen. So your media player may\nexpand, you can resize it. And she will be\nsharing a few videos.",
    "start": "93280",
    "end": "99610"
  },
  {
    "text": "And it's unfortunately just\nthe nature of our platform that those videos will\nbe a little bit choppy. It's not anything on\nyour end or anything",
    "start": "99610",
    "end": "105730"
  },
  {
    "text": "we can troubleshoot on our end. But you should still be able\nto see the basic robotic motion",
    "start": "105730",
    "end": "111430"
  },
  {
    "text": "and Jeanette will be able to\nget her basic points across. But that's just a note for you.",
    "start": "111430",
    "end": "116600"
  },
  {
    "text": "So with that,\nJeanette, I'm going to hand the reins over to you.",
    "start": "116600",
    "end": "123922"
  },
  {
    "text": "JEANNETTE BOHG:\nThank you very much Pax for the really\nnice introduction and for kind of giving\na preview of what",
    "start": "123922",
    "end": "129849"
  },
  {
    "text": "I'm going to talk about. So let me just quickly\nshare my screen. So I hope everyone\nis going to see that.",
    "start": "129850",
    "end": "136269"
  },
  {
    "start": "134000",
    "end": "210000"
  },
  {
    "text": "So today, I really want\nto talk about what I've learned from my successes\nand failures in my work",
    "start": "136270",
    "end": "143110"
  },
  {
    "text": "and how that informs\nmy current research and what I think is an\nimportant in robotic grasping",
    "start": "143110",
    "end": "148390"
  },
  {
    "text": "and manipulation. So I really uncovered\nsome principles there",
    "start": "148390",
    "end": "154209"
  },
  {
    "text": "that I think will bring us\ncloser to autonomous robots. Yeah, so with that, I\njust want to mention",
    "start": "154210",
    "end": "161620"
  },
  {
    "text": "again, my research is on robotic\nmanipulation and grasping. And it's really\ndriven by the puzzle",
    "start": "161620",
    "end": "168340"
  },
  {
    "text": "of why humans can effortlessly\nmanipulate any kind of object",
    "start": "168340",
    "end": "173739"
  },
  {
    "text": "while it is really\nhard to reproduce the skill on a robot that has a\ncompletely different embodiment",
    "start": "173740",
    "end": "180430"
  },
  {
    "text": "from a person. And so, in my first\nPhD research work that I've done kind\nof starting 2008, 9,",
    "start": "180430",
    "end": "188620"
  },
  {
    "text": "and then published in\nthis paper in 2010, I try to address this\nparticular challenge",
    "start": "188620",
    "end": "194920"
  },
  {
    "text": "by looking into finding\nsuitable grasping points for any kind of object.",
    "start": "194920",
    "end": "200590"
  },
  {
    "text": "And we assume that these\nobjects are presented to the robot as\n2D images and it's",
    "start": "200590",
    "end": "206080"
  },
  {
    "text": "supposed to kind of find\nthese points in these images. And so we formalized\nthis problem",
    "start": "206080",
    "end": "212599"
  },
  {
    "text": "of grasp point\ndetection in images as a classification\nproblem, as put forward",
    "start": "212600",
    "end": "217640"
  },
  {
    "text": "by Saxena et al in 2006. So imagine you\nhave this robot who",
    "start": "217640",
    "end": "223370"
  },
  {
    "text": "wants to pick up this\ncup from the table, and it sees this cup\nthrough a camera.",
    "start": "223370",
    "end": "228799"
  },
  {
    "text": "And we compute a\nper pixel feature and compute the probability\nthat this pixel represents",
    "start": "228800",
    "end": "237170"
  },
  {
    "text": "a good grasping point. So y equals, 1 here means\ngood grasping point. And how do we train\nthis simple classifier?",
    "start": "237170",
    "end": "245330"
  },
  {
    "text": "We use the same database\nthat was put forward by Saxena et al and use\nsupervised learning to train,",
    "start": "245330",
    "end": "251000"
  },
  {
    "text": "in this case, an SDM. Remember, this was\n2006 pre-deep learning.",
    "start": "251000",
    "end": "256380"
  },
  {
    "start": "256000",
    "end": "340000"
  },
  {
    "text": "OK. So the contribution that I\nmade in my particular work",
    "start": "256380",
    "end": "261859"
  },
  {
    "text": "was on the feature\nrepresentation. I proposed to use something\ncalled shape context that",
    "start": "261860",
    "end": "267200"
  },
  {
    "text": "captures the shape of an entire\nobject relative to a pixel in that particular image.",
    "start": "267200",
    "end": "272820"
  },
  {
    "text": "So really, here is\nhow this looks like and specifically, we have this\nparticular histogram feature",
    "start": "272820",
    "end": "280250"
  },
  {
    "text": "here. And we showed that this\nreally indeed outperforms the previous feature\nformulation by a large margin",
    "start": "280250",
    "end": "288020"
  },
  {
    "text": "for many different objects. OK. So this is really just a quick\nwalk-through sort of method.",
    "start": "288020",
    "end": "295800"
  },
  {
    "text": "And, so now, given this\nfeature of representation, we have an idea of what pixel in\nthe RGB image of this cup here,",
    "start": "295800",
    "end": "304070"
  },
  {
    "text": "for example, represents\na good grasping point. But of course, grasping\nis a spatial process.",
    "start": "304070",
    "end": "311030"
  },
  {
    "text": "A 3D process really. And if we need to infer an\norientation and a position",
    "start": "311030",
    "end": "317750"
  },
  {
    "text": "of the hand that is\nsomehow linked to that, to the grasping point.",
    "start": "317750",
    "end": "323710"
  },
  {
    "text": "And the details here\nare not very important. But we basically use local\n3D information around",
    "start": "323710",
    "end": "330220"
  },
  {
    "text": "that 2D grasping\npoint we detected in the image to infer\neventually, a lack of the hand",
    "start": "330220",
    "end": "336760"
  },
  {
    "text": "pose for the robot. OK. So here are two\nexamples of this work,",
    "start": "336760",
    "end": "345850"
  },
  {
    "start": "340000",
    "end": "446000"
  },
  {
    "text": "of this method that I just\ntalked about in action. And in both of these\nvideos, the robot",
    "start": "345850",
    "end": "351910"
  },
  {
    "text": "is going to successfully\ngrasp an object. So this is quite\na vintage video. Remember, this was in 2009.",
    "start": "351910",
    "end": "359020"
  },
  {
    "text": "So here, the robot that I used\nwith this three-fingered hand is basically going to\ngrasp this particular box",
    "start": "359020",
    "end": "369830"
  },
  {
    "text": "and will be\nsuccessfully lifting it. And the second video here,\nit's also going to be success--",
    "start": "369830",
    "end": "376450"
  },
  {
    "text": "the robot is also going to\nbe successfully grasping. But maybe also pay\nattention to the shadows",
    "start": "376450",
    "end": "382330"
  },
  {
    "text": "here in the background. So I hope you see that despite\nthe video being a bit choppy,",
    "start": "382330",
    "end": "388490"
  },
  {
    "text": "this was me basically an\nabsolute joy that this actually",
    "start": "388490",
    "end": "394880"
  },
  {
    "text": "worked. And so you can infer from\nhow happy I am back in 2009 that this actually worked.",
    "start": "394880",
    "end": "401090"
  },
  {
    "text": "That this method actually\nwas really hard to make work and that it didn't\nnecessarily work that often.",
    "start": "401090",
    "end": "407970"
  },
  {
    "text": "And so, here, I also want to\nshare with you a few failures. This first one here, for\nexample, is more of an,",
    "start": "407970",
    "end": "416660"
  },
  {
    "text": "I would say an accidental grasp. This object, the spray bottle,\neventually ends up in the hand",
    "start": "416660",
    "end": "422300"
  },
  {
    "text": "but it's really more\nlike a coincidence. And the second\none, you see, this",
    "start": "422300",
    "end": "428610"
  },
  {
    "text": "is maybe a little hard\nto see due to the quality of the video. But there was a\ncup on this table and the robot tries to\ngrasp it, and it's going",
    "start": "428610",
    "end": "436130"
  },
  {
    "text": "to slip off the rim of the cup. And so, you see it could\nactually really need some tactile feedback\nto understand that this",
    "start": "436130",
    "end": "443449"
  },
  {
    "text": "is actually not going to work. OK. So what are the insights that\nI gained from this project?",
    "start": "443450",
    "end": "453419"
  },
  {
    "start": "446000",
    "end": "567000"
  },
  {
    "text": "So the work, the earlier\nwork that I based my work on by Saxena, has\nsuddenly kickstarted",
    "start": "453420",
    "end": "458599"
  },
  {
    "text": "the entire field of learning\nto grasp for robots. And it departed\nfrom prior work that",
    "start": "458600",
    "end": "465139"
  },
  {
    "text": "made many, many\nassumptions about what is known about the object.",
    "start": "465140",
    "end": "470490"
  },
  {
    "text": "It also did not infer precise\ncontact points but only",
    "start": "470490",
    "end": "477500"
  },
  {
    "text": "hand pulses. And today, we actually\nfind many interesting works that basically follow\nthis overall approach.",
    "start": "477500",
    "end": "485900"
  },
  {
    "text": "Like, here for example, the\nGoogle Iron Farm or Dex-Net by Ken Goldberg's lab.",
    "start": "485900",
    "end": "492419"
  },
  {
    "text": "So these approaches\nhave basically nowadays better data, better\nlearning approaches, and so on.",
    "start": "492420",
    "end": "498270"
  },
  {
    "text": "And if you want to\nknow more about this, you can look at these surveys. One written by myself in\n2012 and a newer one in 2019.",
    "start": "498270",
    "end": "507200"
  },
  {
    "text": "So I made contributions\nto this field throughout the years\nbut this first approach",
    "start": "507200",
    "end": "513979"
  },
  {
    "text": "that I just showed you was\nactually extremely brittle and failed often.",
    "start": "513980",
    "end": "519620"
  },
  {
    "text": "And here's what I learned. So 2D grasping points really\ndo not carry enough information",
    "start": "519620",
    "end": "526370"
  },
  {
    "text": "to be successful in grasping. They still have\nto be transformed to a three-dimensional\nhand pose or a position",
    "start": "526370",
    "end": "533570"
  },
  {
    "text": "and an orientation of the hand. And this conversion\ncan be very brittle.",
    "start": "533570",
    "end": "539149"
  },
  {
    "text": "Also, open loop control of\nthe robot arm does not work. So not taking any\nneed, for example,",
    "start": "539150",
    "end": "545240"
  },
  {
    "text": "tactile feedback in\naccount to determine if things are actually\ngoing well does not work.",
    "start": "545240",
    "end": "550400"
  },
  {
    "text": "And also, the approach that\nI showed you completely ignores the\nenvironmental context",
    "start": "550400",
    "end": "556610"
  },
  {
    "text": "and requires careful\npost-processing of any movement of the robot for\ncollision checking and so on.",
    "start": "556610",
    "end": "563330"
  },
  {
    "start": "567000",
    "end": "649000"
  },
  {
    "text": "So for me personally,\nthe real value of the first ever, this first\nPhD project that I've done,",
    "start": "568070",
    "end": "574880"
  },
  {
    "text": "was not actually in the\nscientific contributions, although there was one. But the real value\nfor me personally",
    "start": "574880",
    "end": "581930"
  },
  {
    "text": "was in the lesson I\nlearned on what works and especially what does not\nwork in robotic grasping.",
    "start": "581930",
    "end": "588470"
  },
  {
    "text": "So the first thing I learned\nis that grasp representation really matter and matter a\nlot, and it should be spatial,",
    "start": "588470",
    "end": "596510"
  },
  {
    "text": "so three dimensional. Continuous multimodal feedback\nand constant replanning",
    "start": "596510",
    "end": "602330"
  },
  {
    "text": "is crucial in\nrobotic manipulation. And maybe to many of\nyour work with robots,",
    "start": "602330",
    "end": "608990"
  },
  {
    "text": "this may be obvious,\nbut it wasn't obvious for me back in 2009.",
    "start": "608990",
    "end": "614000"
  },
  {
    "text": "And it's not necessarily\nobvious on how to actually do it in real time.",
    "start": "614000",
    "end": "621770"
  },
  {
    "text": "And then, a last point,\ninstead of avoiding collision with everything else\nbut the target object,",
    "start": "621770",
    "end": "629600"
  },
  {
    "text": "it really helps to reward\nand also assess people if we exploit contact\nwith the environment",
    "start": "629600",
    "end": "636020"
  },
  {
    "text": "and I will explain it a little\nbit later on what I mean. But let's first start\nwith the first point.",
    "start": "636020",
    "end": "642110"
  },
  {
    "text": "So how do we get spatial grasp\nrepresentation instead of just like one point in the image?",
    "start": "642110",
    "end": "649760"
  },
  {
    "start": "649000",
    "end": "742000"
  },
  {
    "text": "So let's get rid\nof this 2D grasping point detected in RGB images\nthat this prior work has done.",
    "start": "649760",
    "end": "659139"
  },
  {
    "text": "And the reason why we want\nthat is because a good grasp actually depends on both\nthe shape and the form",
    "start": "659140",
    "end": "668260"
  },
  {
    "text": "of the object but also\non the capabilities and kinematics of the gripper.",
    "start": "668260",
    "end": "673480"
  },
  {
    "text": "And so, in my lab,\nwe proposed a method called UniGrasp for grasping\nany object with any gripper.",
    "start": "673480",
    "end": "682480"
  },
  {
    "text": "And so, to achieve\nthis, UniGrasp selects sets of contact points.",
    "start": "682480",
    "end": "690040"
  },
  {
    "text": "And as input, it takes the\nobject point clouds record it with some 3D camera and a\nspecification of the kinematics",
    "start": "690040",
    "end": "699820"
  },
  {
    "text": "and geometry of the robot hand. And then, this modeling\nUniGrasp sequentially",
    "start": "699820",
    "end": "705340"
  },
  {
    "text": "outputs contact points. And these contact points are\nboth reachable by the hand,",
    "start": "705340",
    "end": "711760"
  },
  {
    "text": "basically contact points\nof each of the fingers, and they are also in something\ncalled force closure, which",
    "start": "711760",
    "end": "717550"
  },
  {
    "text": "you can just think about a\ngrasp actually being stable and the hand being able to\nbalance the object in the hand",
    "start": "717550",
    "end": "725530"
  },
  {
    "text": "without dropping it. So UniGrasp was able to\nextract gripper geometry",
    "start": "725530",
    "end": "731110"
  },
  {
    "text": "and kinematic features\nand concatenate them with object features\nin this encoding stage in the first\nstage of this model.",
    "start": "731110",
    "end": "738250"
  },
  {
    "text": "And I will explain\nthis model first. So we use something called\na UDF file of the gripper.",
    "start": "738250",
    "end": "746860"
  },
  {
    "start": "742000",
    "end": "789000"
  },
  {
    "text": "So it's basically a description\nof the kinematics and geometry of the robotic hand or\nof a robot in general.",
    "start": "746860",
    "end": "752780"
  },
  {
    "text": "And using this description,\nwe generate the point cloud of the gripper in a particular\njoint configuration.",
    "start": "752780",
    "end": "760250"
  },
  {
    "text": "And we use these point clouds as\ninput to train an autoencoder,",
    "start": "760250",
    "end": "766010"
  },
  {
    "text": "to learn a low dimensional\nfeature representation that",
    "start": "766010",
    "end": "771080"
  },
  {
    "text": "encodes the geometry\nof the robotic hands in specific joint configuration.",
    "start": "771080",
    "end": "777170"
  },
  {
    "text": "And we then used the encoder to\ncompute features and different with different hand\nconfigurations, so",
    "start": "777170",
    "end": "784250"
  },
  {
    "text": "for example, different\nopenings and concatenate them.",
    "start": "784250",
    "end": "789270"
  },
  {
    "start": "789000",
    "end": "850000"
  },
  {
    "text": "And so, this here,\nwhat you see here is a visualization of\nthe embedding space. So you first compute the\nfeatures of two input point",
    "start": "789270",
    "end": "798454"
  },
  {
    "text": "here, on the left and on the\nright, and circle them red. And we interpolate\nthese two features",
    "start": "798455",
    "end": "804200"
  },
  {
    "text": "and decode these\ninterpolated features into group of point\nclouds that you actually",
    "start": "804200",
    "end": "810860"
  },
  {
    "text": "see here in the middle. You can see that the top row\nshows prismatic joint movement.",
    "start": "810860",
    "end": "818010"
  },
  {
    "text": "So basically, an opening\nand closing of the fingers. And the middle row represents\na revelou joint movement",
    "start": "818010",
    "end": "825839"
  },
  {
    "text": "so basically it's\nchanging the angle. And the bottom row shows\nthat geometry changes",
    "start": "825840",
    "end": "831230"
  },
  {
    "text": "between two types of different\n3D fingers robotic hands. So what the slide\nbasically shows",
    "start": "831230",
    "end": "838529"
  },
  {
    "text": "is that this latent space that\nrepresents gripper geometry and kinematics can be\nmeaningfully interpolated",
    "start": "838530",
    "end": "846990"
  },
  {
    "text": "and also generalized\nover different hands. OK. So in addition to the gripper\nfeature that I just showed you,",
    "start": "846990",
    "end": "855490"
  },
  {
    "text": "we also extract an object\nfeature using a model called PointNet++ and concatenate\nit with the gripper feature.",
    "start": "855490",
    "end": "865800"
  },
  {
    "text": "And then, the\nconcatenated feature will be fed into a point\nset selection network",
    "start": "865800",
    "end": "870990"
  },
  {
    "text": "to sequentially output contact\npoints for the robotic hand. And so, we propose a\nnew model for solving",
    "start": "870990",
    "end": "878220"
  },
  {
    "text": "the difficult\ncombinatorical problem of finding a set\nof contact points among the object point clouds.",
    "start": "878220",
    "end": "884339"
  },
  {
    "text": "And I don't have time to explain\nthis very complex model here but I just going to walk\nyou through a more abstract",
    "start": "884340",
    "end": "892800"
  },
  {
    "text": "representation of this model. So given an n fingered gripper,\nthe networks has n stages.",
    "start": "892800",
    "end": "902140"
  },
  {
    "text": "So in stage one, the model\nestimates the probability for each point in the\nobject point cloud,",
    "start": "902140",
    "end": "907410"
  },
  {
    "text": "if the point belongs to a\nvalid set of contact points.",
    "start": "907410",
    "end": "912819"
  },
  {
    "text": "And after ranking\nthese points, the model selects the top key points\nthat are shown here in blue.",
    "start": "912820",
    "end": "918990"
  },
  {
    "text": "And then, conditioned on\none of these points, one of these blue points\nselected in the first stage,",
    "start": "918990",
    "end": "925110"
  },
  {
    "text": "the model now predicts\nthe probability of each point in the point cloud\nto form a valid set of contacts",
    "start": "925110",
    "end": "930630"
  },
  {
    "text": "with the first point. And these points are\nthen ranked again and the top key\npoints are selected.",
    "start": "930630",
    "end": "937740"
  },
  {
    "text": "And then, finally,\nin stage three, we select the\nthird contact point for this three-fingered\nhand here.",
    "start": "937740",
    "end": "942990"
  },
  {
    "text": "In this case, conditioned on\nthe first two contact points. And so, the network is\ntrying to output points that",
    "start": "942990",
    "end": "949080"
  },
  {
    "text": "are in first closures so\nproduce a stable grasp and are also reachable by\nthe hand in the first place.",
    "start": "949080",
    "end": "956880"
  },
  {
    "text": "And before executing\nthe grasp, we still can check kinematic\nfeasibility for the end.",
    "start": "956880",
    "end": "962630"
  },
  {
    "start": "962000",
    "end": "1020000"
  },
  {
    "text": "OK. So here is how this actually\nlooks like for a toy example",
    "start": "962630",
    "end": "967810"
  },
  {
    "text": "to visualize how a model\npredicts reasonable contact points that are dependent\non the gripper geometry",
    "start": "967810",
    "end": "974620"
  },
  {
    "text": "and of the kinematics as well. So in black here,\nyou see the object",
    "start": "974620",
    "end": "980140"
  },
  {
    "text": "and in blue, the gripper. And the first gripper\nhas a very small opening",
    "start": "980140",
    "end": "987100"
  },
  {
    "text": "of the two fingers. So the model, our\nmodel, UniGrasp, generates contact points at\nthe thin part of the object.",
    "start": "987100",
    "end": "995800"
  },
  {
    "text": "And here, the second hand, this\none has the same shape but it",
    "start": "995800",
    "end": "1001500"
  },
  {
    "text": "cannot fully close the fingers\nbut instead it can open very widely.",
    "start": "1001500",
    "end": "1006910"
  },
  {
    "text": "So the model outputs\ncontact points that are on a thicker\npart of the object.",
    "start": "1006910",
    "end": "1012460"
  },
  {
    "text": "And finally, for a\nthree-fingered hand, the model also again, outputs\nreasonable contact points",
    "start": "1012460",
    "end": "1017585"
  },
  {
    "text": "for each of the fingers. So we used our model,\nUniGrasp, to grasp objects",
    "start": "1017585",
    "end": "1027039"
  },
  {
    "start": "1020000",
    "end": "1142000"
  },
  {
    "text": "shown in the middle here with\nfive different robotic hands",
    "start": "1027040",
    "end": "1032170"
  },
  {
    "text": "that you see here\nat the bottom, that are really quite different. And here, I want to show\nyou a bunch of videos",
    "start": "1032170",
    "end": "1039640"
  },
  {
    "text": "that I hope are\nOK, visible to you. So we basically test our model\nby grasping novel objects",
    "start": "1039640",
    "end": "1047619"
  },
  {
    "text": "that this model hasn't\nbeen trained on, with a known gripper.",
    "start": "1047619",
    "end": "1052750"
  },
  {
    "text": "And here, known gripper means\nthat we have trained our model with that gripper. We test when using this,\nwhat is called the Robotiq-3F",
    "start": "1052750",
    "end": "1061570"
  },
  {
    "text": "gripper and our model\nachieves 95% success rate on these 65 trials that we\nhave done on real hardware.",
    "start": "1061570",
    "end": "1069220"
  },
  {
    "text": "But even more interesting\nwith this model is something that no other model\nhas actually done is we wanted to see if we can\ngrasp objects, novel objects,",
    "start": "1069220",
    "end": "1080950"
  },
  {
    "text": "with novel grippers that the\nmodel hasn't been trained on.",
    "start": "1080950",
    "end": "1086179"
  },
  {
    "text": "So here, we see\nnew robotic hands.",
    "start": "1086180",
    "end": "1093490"
  },
  {
    "text": "Actually, it's a hand that\nwe have trained our model on except we have removed\nin our description",
    "start": "1093490",
    "end": "1100809"
  },
  {
    "text": "file one of the fingers. So it's now only a\ntwo-fingered gripper and our model achieves\n93% grasp success rate",
    "start": "1100810",
    "end": "1108820"
  },
  {
    "text": "on 60 trials, which\nis really good. And here's a very interesting\nanthropomorphic hand",
    "start": "1108820",
    "end": "1114340"
  },
  {
    "text": "with four fingers, and it's\ncalled the allegro hand. And our model achieves\n90% success rate",
    "start": "1114340",
    "end": "1122559"
  },
  {
    "text": "while our baseline\nmethod achieves only 40%.",
    "start": "1122560",
    "end": "1127630"
  },
  {
    "text": "And of course, there are\nsome failure cases as well. And these are typically due\nto imprecise perception,",
    "start": "1127630",
    "end": "1135210"
  },
  {
    "text": "low friction, so slippery\nobjects, object deformations, and premature contact\nwith the object.",
    "start": "1135210",
    "end": "1142679"
  },
  {
    "start": "1142000",
    "end": "1185000"
  },
  {
    "text": "And so, just to summarize\nthis particular work, the main idea is here\nthat we can feed a model",
    "start": "1142680",
    "end": "1150419"
  },
  {
    "text": "with an object and a hand\ndescription, something that hasn't been previously\ndone, to compute contact",
    "start": "1150420",
    "end": "1156900"
  },
  {
    "text": "points for the fingers instead\nof like one 2D grasping point. And the algorithmic\ncontribution here",
    "start": "1156900",
    "end": "1163530"
  },
  {
    "text": "was a point set selection\nnetwork that I really briefly explained. And the result is a model that\noutputs a valid grasp also",
    "start": "1163530",
    "end": "1172590"
  },
  {
    "text": "for novel robotic hands,\nwhich is really relevant for, for example, if you think\nin manufacturing, when",
    "start": "1172590",
    "end": "1180450"
  },
  {
    "text": "a robot gets a new hand, right? You may not want to\nretrain your model.",
    "start": "1180450",
    "end": "1186049"
  },
  {
    "start": "1185000",
    "end": "1295000"
  },
  {
    "text": "OK. So this was giving you an idea\nof how we've removed that one",
    "start": "1186050",
    "end": "1192970"
  },
  {
    "text": "assumption from this old model\nto make it basically better and in terms of introducing a\nspatial grasp representation.",
    "start": "1192970",
    "end": "1201160"
  },
  {
    "text": "And I want to come to\nthe next point, which is the importance of\ncontinuous feedback",
    "start": "1201160",
    "end": "1207790"
  },
  {
    "text": "during actually executing\na grasp and the ability to replan.",
    "start": "1207790",
    "end": "1213150"
  },
  {
    "text": "So what-- I don't know,\nfor those of you who",
    "start": "1213150",
    "end": "1218520"
  },
  {
    "text": "have worked with robots, it's\nprobably relatively clear that you have to close something\ncalled the perception action",
    "start": "1218520",
    "end": "1225570"
  },
  {
    "text": "loop around high dimensional\nsense of the data that comes for example, from cameras,\nor from tactile sensors.",
    "start": "1225570",
    "end": "1233640"
  },
  {
    "text": "So it means that\nbasically, the robot should be able to continuously\ntake feedback into account and react to it.",
    "start": "1233640",
    "end": "1240419"
  },
  {
    "text": "But how to do that concretely\non a high dimensional system is an open question. And so I started focusing\ndirectly on this time,",
    "start": "1240420",
    "end": "1249660"
  },
  {
    "text": "on this point, using\nthis particular robot. So here's an example\nfor some behavior",
    "start": "1249660",
    "end": "1254820"
  },
  {
    "text": "that the resulting system\nthat we built produces. And you see here that this\nrobot is called Apollo,",
    "start": "1254820",
    "end": "1264480"
  },
  {
    "text": "it has to grasp\nthis pringles box without knocking anything over. And my colleague here,\nJermaine Price, he",
    "start": "1264480",
    "end": "1271260"
  },
  {
    "text": "makes it really hard for\nApollo by moving around these objects in the\nenvironment and it has to react in millisecond.",
    "start": "1271260",
    "end": "1278070"
  },
  {
    "text": "Apollo, the robot,\nhas to keep track of the environment in real\ntime and provide the feedback to a fast motion\ngeneration mechanism that",
    "start": "1278070",
    "end": "1285779"
  },
  {
    "text": "drives the behavior\nand allows it to react. So this is really a system\nwhere real-time perception meets",
    "start": "1285780",
    "end": "1293390"
  },
  {
    "text": "reactive motion generation. And so, here is a\nvery simplified figure of how the architecture of such\na system of a robotic system",
    "start": "1293390",
    "end": "1301610"
  },
  {
    "start": "1295000",
    "end": "1509000"
  },
  {
    "text": "that is operating under the\nhood actually looks like. And I have no time to explain\neach box but each of them",
    "start": "1301610",
    "end": "1308660"
  },
  {
    "text": "makes a contribution to either\nrobust real-time visual robot arm or object tracking, or on\nline trajectory generation.",
    "start": "1308660",
    "end": "1317630"
  },
  {
    "text": "What I want you to\npay attention to is how these\nreal-time perception methods and reactive\nmotion generation methods",
    "start": "1317630",
    "end": "1324049"
  },
  {
    "text": "are actually connected. And so, in our\nsystem, everything is connected in loops\nof different frequencies",
    "start": "1324050",
    "end": "1331880"
  },
  {
    "text": "where sensory data\nfeeds into controllers or motion optimization methods.",
    "start": "1331880",
    "end": "1339630"
  },
  {
    "text": "Also, all of these things\ncome at different rates. So the camera runs at 30\nhertz, the haptic sensor",
    "start": "1339630",
    "end": "1348020"
  },
  {
    "text": "runs at kilohertz. So all of this has to be\nprocessed asynchronously. And notice that this is not\na hierarchy or something,",
    "start": "1348020",
    "end": "1354550"
  },
  {
    "text": "it's really a bunch of loops. So it's important, one\nthing important to note",
    "start": "1354550",
    "end": "1360480"
  },
  {
    "text": "is that it doesn't\nmatter how amazing each of these components are. If you put them together\nin the wrong way,",
    "start": "1360480",
    "end": "1368040"
  },
  {
    "text": "your system will not work well. And in this paper, we showed\nexactly this empirically.",
    "start": "1368040",
    "end": "1375790"
  },
  {
    "text": "So let me walk you through\none loop in the system. So we start with the\nraw sensory data.",
    "start": "1375790",
    "end": "1382590"
  },
  {
    "text": "That what you see is basically\na point cloud from the top and what you don't\nreally see here",
    "start": "1382590",
    "end": "1388080"
  },
  {
    "text": "is that there's also\nsome haptic data and feedback on where the joints\nof the robot currently are.",
    "start": "1388080",
    "end": "1394120"
  },
  {
    "text": "So RGB in depth data come in\nat 30 hertz and proprioceptive data from the robot\nat a kilohertz.",
    "start": "1394120",
    "end": "1399730"
  },
  {
    "text": "So and then, using real time\nvisual tracking methods, this data is processed to infer\nthe object pose, the robot",
    "start": "1399730",
    "end": "1405210"
  },
  {
    "text": "arm pose, and obstacles\nin the environment that you see here in these\nkind of rainbow-colored little",
    "start": "1405210",
    "end": "1410640"
  },
  {
    "text": "boxes. And given this information,\nwe compute a potential field",
    "start": "1410640",
    "end": "1416280"
  },
  {
    "text": "where the target object provides\nan attractive potential, and the obstacles basically\nprovide a repulsive potential",
    "start": "1416280",
    "end": "1426600"
  },
  {
    "text": "as shown here with\nthe red arrows. So this feedback is used\nby local controllers to compute the\noptimal action that",
    "start": "1426600",
    "end": "1432930"
  },
  {
    "text": "bring the arm closer to the\nobject, to the target object, and push it away\nfrom the obstacles.",
    "start": "1432930",
    "end": "1439860"
  },
  {
    "text": "And then, we also use\nonline trajectory optimizes that optimize actions over\ntime horizon of two seconds",
    "start": "1439860",
    "end": "1446280"
  },
  {
    "text": "and produce what we call\nacceleration policy. So in this case, it's called\nRiemannian Motion Policies,",
    "start": "1446280",
    "end": "1453480"
  },
  {
    "text": "that are tracked with\nsome low level controller. And this unknown trajectory\noptimizes much slower than",
    "start": "1453480",
    "end": "1459910"
  },
  {
    "text": "the very basic feedback\ncontrollers and therefore",
    "start": "1459910",
    "end": "1465720"
  },
  {
    "text": "cannot react as quickly to\nchanges but it's certainly less susceptible to local minimum.",
    "start": "1465720",
    "end": "1471360"
  },
  {
    "text": "And we fused these\ntwo types of policies to obtain a fused policy that\nis updated at a kilohertz",
    "start": "1471360",
    "end": "1476580"
  },
  {
    "text": "and that you see here in yellow. OK. So in this paper, we compare\nthree system architectures.",
    "start": "1476580",
    "end": "1484100"
  },
  {
    "text": "We have a standard sense plan\narchitecture that sensors first then plans and then acts without\ntaking feedback into account.",
    "start": "1484100",
    "end": "1492960"
  },
  {
    "text": "And then, we have a\nreactive architecture that does not look ahead, for\ncomputing the next best action",
    "start": "1492960",
    "end": "1498110"
  },
  {
    "text": "but runs extremely\nfast at a kilohertz. And we have the\nproposed architecture that runs a number of loops\nasynchronously including",
    "start": "1498110",
    "end": "1505790"
  },
  {
    "text": "an unknown planning,\nit looks ahead for optimizing the next action. And so, we evaluated these\ndifferent system architecture",
    "start": "1505790",
    "end": "1513320"
  },
  {
    "start": "1509000",
    "end": "1649000"
  },
  {
    "text": "that all consist of the\nsame components in four different scenarios.",
    "start": "1513320",
    "end": "1519230"
  },
  {
    "text": "And I can only, in\nthe interest of time, only show you just one example.",
    "start": "1519230",
    "end": "1526260"
  },
  {
    "text": "So here, what you see is just\nan overview of the setup. The robot basically\nhas to again, pick",
    "start": "1526260",
    "end": "1533780"
  },
  {
    "text": "and place this box here,\nthis cylindrical chips box,",
    "start": "1533780",
    "end": "1540110"
  },
  {
    "text": "pick it up and bring\nit to the other side of the table on the right. And what happens\nhere specifically,",
    "start": "1540110",
    "end": "1546179"
  },
  {
    "text": "is that this cardboard box\nis being pushed in the way. And yeah, basically, the\nrobot has to avoid it.",
    "start": "1546180",
    "end": "1553500"
  },
  {
    "text": "And at the bottom right corner,\nyou see what the robot sees. So these kind of voxels\nand the colorful voxels",
    "start": "1553500",
    "end": "1561590"
  },
  {
    "text": "down there is what the\nrobot basically has available as information OK.",
    "start": "1561590",
    "end": "1567650"
  },
  {
    "text": "So here, first, is this\nsense, plan, act architecture that basically senses once,\nplans once, and then executes.",
    "start": "1567650",
    "end": "1575390"
  },
  {
    "text": "And it doesn't take\nfeedback into account. So it doesn't realize\nthere is a box in the way. So just going to basically\ncollide with it, right?",
    "start": "1575390",
    "end": "1583910"
  },
  {
    "text": "And will just go through. This of course is\nOK in this scenario because the box is light.",
    "start": "1583910",
    "end": "1589370"
  },
  {
    "text": "But of course, you want\nto definitely avoid that. And so here in the middle,\nwe have the system that",
    "start": "1589370",
    "end": "1596660"
  },
  {
    "text": "uses a reactive controller. And you see that it manages to\nnot really collide with this",
    "start": "1596660",
    "end": "1604909"
  },
  {
    "text": "box but it also cannot really\nfind a way to get to the other",
    "start": "1604910",
    "end": "1610640"
  },
  {
    "text": "side, although there is, right? And it does recover once the box\nis removed but it's basically",
    "start": "1610640",
    "end": "1617090"
  },
  {
    "text": "stuck in a local minimum. And so, the proposed\nmethod can cope with this.",
    "start": "1617090",
    "end": "1624450"
  },
  {
    "text": "So even though the\nbox gets in the way, it is able to look ahead in time\nand find this narrow passage",
    "start": "1624450",
    "end": "1633559"
  },
  {
    "text": "between the box and\nits own body to get the box to the other side. So this combination of a long\nhorizon trajectory optimizer",
    "start": "1633560",
    "end": "1642650"
  },
  {
    "text": "with a fast local controller\nis really the best option that we have here.",
    "start": "1642650",
    "end": "1649990"
  },
  {
    "start": "1649000",
    "end": "1688000"
  },
  {
    "text": "OK. So just to summarize this\nparticular part of the talk, the main idea of this work\nwas that system architectures",
    "start": "1649990",
    "end": "1659940"
  },
  {
    "text": "really have to consist of\nthese interlocked perception action loops that connect\nsensory feedback with motion",
    "start": "1659940",
    "end": "1668159"
  },
  {
    "text": "controllers. And then, the contribution\nthat we made here was that we really\nevaluated the influence",
    "start": "1668160",
    "end": "1675270"
  },
  {
    "text": "of system architecture and\nperformance of the system. And the result was a system\nthat can perform, successfully",
    "start": "1675270",
    "end": "1684150"
  },
  {
    "text": "manipulate objects in uncertain\nand dynamic environments. All right. So with that, I want to come--",
    "start": "1684150",
    "end": "1693760"
  },
  {
    "start": "1688000",
    "end": "1716000"
  },
  {
    "text": "so that part, I basically\nemphasized how important it is that a robot is\nable to take continuously",
    "start": "1693760",
    "end": "1699100"
  },
  {
    "text": "feedback into account\nand be able to replan. And the last point that I took\naway from my initial PhD work",
    "start": "1699100",
    "end": "1707500"
  },
  {
    "text": "was that a robot really benefits\nfrom actually exploiting the environment rather\nthan only avoiding it.",
    "start": "1707500",
    "end": "1717020"
  },
  {
    "text": "So let me explain\nwhat that means. So it turns out that humans\nare actually not avoiding",
    "start": "1717020",
    "end": "1726940"
  },
  {
    "text": "the environment, we're\nmanipulating objects, which is something that we always\ntry to make robots do",
    "start": "1726940",
    "end": "1733960"
  },
  {
    "text": "or in, for example,\nautonomous driving, it's very important to avoid\nthe environment, right? But manipulation, it's\nactually detrimental.",
    "start": "1733960",
    "end": "1741200"
  },
  {
    "text": "So here's an example from\nJulia Child chopping potatoes that was introduced\nto the community",
    "start": "1741200",
    "end": "1746740"
  },
  {
    "text": "by this Professor,\nMatt Mason from CMU. And for example,\nyou see how she's",
    "start": "1746740",
    "end": "1751840"
  },
  {
    "text": "using her knuckles as a\nconstraint to guide the knife. And this is just\none of the things",
    "start": "1751840",
    "end": "1758380"
  },
  {
    "text": "that she does to\nexploit the environment. And so there are other\nexamples of smart design.",
    "start": "1758380",
    "end": "1764519"
  },
  {
    "text": "So for example,\nthis ATM here, this has this funnel\naround the card slot",
    "start": "1764520",
    "end": "1770500"
  },
  {
    "text": "to make it easier for\npeople to insert the card. And so autonomously\nlearning robot skills,",
    "start": "1770500",
    "end": "1780200"
  },
  {
    "start": "1774000",
    "end": "1797000"
  },
  {
    "text": "any kind of manipulation\nskills, is really challenging as you see here. So these are examples\nof a contact-reach task",
    "start": "1780200",
    "end": "1786210"
  },
  {
    "text": "like peg insertion\nor a wrench task, or inserting a battery into\na phone at the very right.",
    "start": "1786210",
    "end": "1792860"
  },
  {
    "text": "And they require\nquite some accuracy and have a very small\nmargin of error.",
    "start": "1792860",
    "end": "1798140"
  },
  {
    "text": "And in fact, there\nare many examples where it is shown that getting\nin touch with the environment",
    "start": "1798140",
    "end": "1804480"
  },
  {
    "text": "instead of-- and exploiting it, makes\ngrasping more robust.",
    "start": "1804480",
    "end": "1810480"
  },
  {
    "text": "So here's an example for peg\ninsertion that was really hard. But when we put\nthis kind of bracket",
    "start": "1810480",
    "end": "1816200"
  },
  {
    "text": "there, this fixture as\nwe call it and read, suddenly, this becomes easy.",
    "start": "1816200",
    "end": "1821540"
  },
  {
    "text": "Because the fixture, it blocks\nthe wrong motion of the robot. It really acts\nlike a funnel that",
    "start": "1821540",
    "end": "1828320"
  },
  {
    "text": "restricts the motion\nof the robot and guides the robot towards\na successful goal.",
    "start": "1828320",
    "end": "1833090"
  },
  {
    "text": "So other people have\nalso recognized this. That these environmental\nconstraints can be exploited,",
    "start": "1835830",
    "end": "1844370"
  },
  {
    "text": "and here are some examples. However, the majority of\nthe works in this area,",
    "start": "1844370",
    "end": "1850020"
  },
  {
    "text": "they are often designed\nhow the environment can be exploited and\nthereby determined the behavior ahead of time.",
    "start": "1850020",
    "end": "1857140"
  },
  {
    "text": "And what we've\nwondered in our work is that if a robot can\ndiscover how to best place",
    "start": "1857140",
    "end": "1862500"
  },
  {
    "text": "these kind of fixtures itself. So this is really the\nquestion we ask in this work,",
    "start": "1862500",
    "end": "1869490"
  },
  {
    "text": "can a robot\nautonomously discover how to ease manipulation\nskills learning problems",
    "start": "1869490",
    "end": "1875160"
  },
  {
    "text": "through the optimal\nplacement of such a fixture of the red fixture\nthat you've seen in this video",
    "start": "1875160",
    "end": "1880620"
  },
  {
    "text": "before? So our approach consists\nof an inner and outer loop that each run\nreinforcement learning.",
    "start": "1880620",
    "end": "1888080"
  },
  {
    "start": "1881000",
    "end": "2088000"
  },
  {
    "text": "And so the outer loop learns\nto place a physical fixture",
    "start": "1888080",
    "end": "1894110"
  },
  {
    "text": "that you see here. So you see like\nthis blue robot has",
    "start": "1894110",
    "end": "1899360"
  },
  {
    "text": "this red fixture\nin the hand, and it tries to learn how to place it\nbest so that the other robot",
    "start": "1899360",
    "end": "1904700"
  },
  {
    "text": "can do the insertion task. So the inner loop learns\nthe manipulation skill",
    "start": "1904700",
    "end": "1910789"
  },
  {
    "text": "given this fixture pose. And after a fixed\nnumber of iteration,",
    "start": "1910790",
    "end": "1916220"
  },
  {
    "text": "this inner loop returns\nthe achieved reward to the outer loop.",
    "start": "1916220",
    "end": "1921870"
  },
  {
    "text": "And so, the higher the reward\nachieved by the inner loop, the more reward the fixture\npose receives in the outer loop.",
    "start": "1921870",
    "end": "1930630"
  },
  {
    "text": "So we formulate this\nouter loop problem as a contextual bandit problem.",
    "start": "1930630",
    "end": "1936050"
  },
  {
    "text": "And the goal is to learn\na good policy for choosing an action given the context.",
    "start": "1936050",
    "end": "1941570"
  },
  {
    "text": "And the context here\nis really a depth image of this manipulation\ntest of the object that",
    "start": "1941570",
    "end": "1947029"
  },
  {
    "text": "is being manipulated. So in the outer\nloop, we are learning",
    "start": "1947030",
    "end": "1953570"
  },
  {
    "text": "Qw that approximates the\ntrue Q function, which",
    "start": "1953570",
    "end": "1959509"
  },
  {
    "text": "outputs the expected reward\nafter taking an action a. And the context is the\ndepth image here represented",
    "start": "1959510",
    "end": "1967220"
  },
  {
    "text": "by this gray image and\nit's processed by CNN.",
    "start": "1967220",
    "end": "1969755"
  },
  {
    "text": "And so basically, the action is\nplacing this fixture in black",
    "start": "1973250",
    "end": "1978680"
  },
  {
    "text": "here and the state is this sf. And at test time we use\nan algorithm called QT up",
    "start": "1978680",
    "end": "1986880"
  },
  {
    "text": "to find an optimal action. So we take this Qw and we find\nthe maximum, the action af that",
    "start": "1986880",
    "end": "1993600"
  },
  {
    "text": "maximizes the expected reward. And that's basically our policy.",
    "start": "1993600",
    "end": "1998785"
  },
  {
    "text": "So there is like a\nparticular challenge here in learning this Qw\nthat approximates the true Q",
    "start": "1998785",
    "end": "2004400"
  },
  {
    "text": "function. And that is that the true\nQ function is actually discontinuous.",
    "start": "2004400",
    "end": "2009840"
  },
  {
    "text": "So let me explain. So for training, so basically\nyou have here on the y-axis the",
    "start": "2009840",
    "end": "2018380"
  },
  {
    "text": "expected reward, so\nyou want that to be high for particular\ngood actions.",
    "start": "2018380",
    "end": "2024950"
  },
  {
    "text": "And on the x-axis, you\nhave particular options for a fixture pose.",
    "start": "2024950",
    "end": "2030169"
  },
  {
    "text": "And you see that\non the very left, the fixture, this black bracket,\nis basically covering the hole.",
    "start": "2030170",
    "end": "2038253"
  },
  {
    "text": "And of course, that means that\nthe other robot cannot even insert anything into that hole.",
    "start": "2038253",
    "end": "2043920"
  },
  {
    "text": "So that should give\nvery low reward. And then, as soon\nas you perfectly align the bracket\nwith that hole, as you",
    "start": "2043920",
    "end": "2051109"
  },
  {
    "text": "see in this middle\noption, suddenly, the expected reward\nis really high. And then, the\nfurther we remove it,",
    "start": "2051110",
    "end": "2057830"
  },
  {
    "text": "the reward kind of\ngracefully degrades. But the problem is there is\nthis discontinuity there.",
    "start": "2057830",
    "end": "2064850"
  },
  {
    "text": "So for training, we proposed\na new smooth zooming algorithm that we show is\ncapable of learning this discontinuous function.",
    "start": "2064850",
    "end": "2071419"
  },
  {
    "text": "And I'm not going to\ngo into detail of this. Feel free to look at the\npaper if you're interested.",
    "start": "2071420",
    "end": "2077520"
  },
  {
    "text": "So for the inner\nloop, I also mentioned that we use\nreinforcement learning, but there we use\nmodel-free RL that",
    "start": "2077520",
    "end": "2084020"
  },
  {
    "text": "learns a good policy for\nthe specific task given a particular bracket.",
    "start": "2084020",
    "end": "2089850"
  },
  {
    "start": "2088000",
    "end": "2143000"
  },
  {
    "text": "And so, what we found is\nthat learning is really significantly faster with\na good fixture placement,",
    "start": "2089850",
    "end": "2096629"
  },
  {
    "text": "with a good fixture placement. So here, what you see is\nthat, you see basically",
    "start": "2096630",
    "end": "2103290"
  },
  {
    "text": "the success rate on the top\ngiven different conditions. So if you have an optimal\nfixture placement in orange,",
    "start": "2103290",
    "end": "2111150"
  },
  {
    "text": "the success rate becomes\nreally fast, really high. And if we have a suboptimal\nfixture placement in purple,",
    "start": "2111150",
    "end": "2119640"
  },
  {
    "text": "the robot has a really hard\ntime actually learning it. Without a fixtures\nit takes even longer.",
    "start": "2119640",
    "end": "2125620"
  },
  {
    "text": "So I think what\nis happening here is that this particular\nred fixture there is really",
    "start": "2125620",
    "end": "2132390"
  },
  {
    "text": "constraining the\nextra space that doesn't have to be explored\nso it makes learning really",
    "start": "2132390",
    "end": "2137670"
  },
  {
    "text": "faster. And for the other tasks\nwe explored in this paper, the picture is kind of similar.",
    "start": "2137670",
    "end": "2143590"
  },
  {
    "text": "OK. So putting it all together,\none thing to remember here",
    "start": "2143590",
    "end": "2148620"
  },
  {
    "text": "is basically the faster\nthe inner loop robot learns how to do the peg insertion\ntask in this particular case,",
    "start": "2148620",
    "end": "2155080"
  },
  {
    "text": "the higher is the reward\nfor the outer loop robot and so it notices which fixture\nplacement is actually the best.",
    "start": "2155080",
    "end": "2163530"
  },
  {
    "text": "And here's a visualization of\nthis Qw function for fixture pose selection.",
    "start": "2163530",
    "end": "2169200"
  },
  {
    "text": "And maybe I don't go\ninto detail of this one but it basically learns\nsomething meaningful.",
    "start": "2169200",
    "end": "2175560"
  },
  {
    "start": "2174000",
    "end": "2263000"
  },
  {
    "text": "And so here is, here's an\nexample of how this actually looks like on a real robot.",
    "start": "2175560",
    "end": "2181500"
  },
  {
    "text": "So first, the robot is basically\ntrying to figure out like, OK, which one of these\nplacements of the fixture",
    "start": "2181500",
    "end": "2187349"
  },
  {
    "text": "gives me the highest\nexpected reward. And once you have found\nit, the other robot",
    "start": "2187350",
    "end": "2193559"
  },
  {
    "text": "is basically doing\nthe manipulation task given this pose. And you saw that it wasn't\nreally fast at doing that.",
    "start": "2193560",
    "end": "2200430"
  },
  {
    "text": "And here was another\ntest of wrench tasks. We also saw the other robot\nwas holding the read block,",
    "start": "2200430",
    "end": "2205530"
  },
  {
    "text": "finding a good position to\nsupport the second robot to do the actual wrench task.",
    "start": "2205530",
    "end": "2211230"
  },
  {
    "text": "And here is the task of-- and it's called shallow\ndepth insertion. It's basically how to\nautomatically insert",
    "start": "2211230",
    "end": "2217480"
  },
  {
    "text": "a battery into your phone. And you see that this is\nquite a complex manipulation",
    "start": "2217480",
    "end": "2223860"
  },
  {
    "text": "for the robot holding the\nbattery, the yellow part. And so, a correct\nfixture placement",
    "start": "2223860",
    "end": "2229890"
  },
  {
    "text": "is really making\nit easier for it. And one cool thing\nabout this approach",
    "start": "2229890",
    "end": "2236630"
  },
  {
    "text": "is that we actually trained\nall of this in simulation, and then, we directly\ntest the resulting policy",
    "start": "2236630",
    "end": "2243529"
  },
  {
    "text": "in the real world. And additionally, because\nof just physical fixture,",
    "start": "2243530",
    "end": "2250670"
  },
  {
    "text": "the learned policy also\ngeneralizes to new pack shape and the robot doesn't\neven know that these",
    "start": "2250670",
    "end": "2255900"
  },
  {
    "text": "have different shapes. But the fixture\nplacement is just kind of generalizing to\nthese different shapes.",
    "start": "2255900",
    "end": "2263990"
  },
  {
    "text": "Yeah, so just to summarize\nthis particular part, the main idea of\nthis research project",
    "start": "2263990",
    "end": "2271340"
  },
  {
    "text": "was to learn, to\nlet the robot learn how to alter the environment,\nto help scaffold manipulation",
    "start": "2271340",
    "end": "2279050"
  },
  {
    "text": "learning for another robot,\nto help it to learn faster. And we made an\nalgorithmic contribution",
    "start": "2279050",
    "end": "2284480"
  },
  {
    "text": "that I didn't explain\nmuch in detail here. But the result was more robust\nmanipulation and learning",
    "start": "2284480",
    "end": "2290099"
  },
  {
    "text": "is sped up dramatically once\nyou find the right fixture placement.",
    "start": "2290100",
    "end": "2295410"
  },
  {
    "start": "2294000",
    "end": "2358000"
  },
  {
    "text": "OK. So with this, I want\nto conclude my talk. And I showed you some\nresults from my first project",
    "start": "2295410",
    "end": "2300650"
  },
  {
    "text": "I've done during my PhD. And I gave you three\nlessons that I personally took away from observing the\nlimitations of this first work.",
    "start": "2300650",
    "end": "2308690"
  },
  {
    "text": "And I showed you three works\nthat address these challenges. So first, I showed you some\nwork that in first contact",
    "start": "2308690",
    "end": "2315109"
  },
  {
    "text": "points instead of these\n2D grasping points,",
    "start": "2315110",
    "end": "2320510"
  },
  {
    "text": "and that work allowed\nyou to infer really a spatial grasping pose of the\nhand and even with novel hands",
    "start": "2320510",
    "end": "2328760"
  },
  {
    "text": "And I showed you\na concrete example for a system architecture\nthat consists of interlocked\nperception action loops",
    "start": "2328760",
    "end": "2335750"
  },
  {
    "text": "and can deal with dynamically\nchanging and uncertain environments by allowing\ncontinuous feedback",
    "start": "2335750",
    "end": "2342500"
  },
  {
    "text": "and free planning. And finally, I also\nshowed you work in which a robot\ncan learn to alter",
    "start": "2342500",
    "end": "2348020"
  },
  {
    "text": "the environment to\nscaffold manipulation and where the robot holding\nthis yellow part here",
    "start": "2348020",
    "end": "2353269"
  },
  {
    "text": "can really exploit\nthe environment to do more robust manipulation.",
    "start": "2353270",
    "end": "2358839"
  },
  {
    "start": "2358000",
    "end": "2418000"
  },
  {
    "text": "OK. So yeah. So after all of these\nworks, of course, we are looking at more\ninteresting research questions",
    "start": "2358840",
    "end": "2366060"
  },
  {
    "text": "like for example, how can we\ninclude more sensing modalities like vision, touch, motion,\nlanguage, and even sound?",
    "start": "2366060",
    "end": "2376619"
  },
  {
    "text": "And we are looking at more\ndifficult manipulation tasks. So for example, long-horizon\ntask or manipulating deformable",
    "start": "2376620",
    "end": "2383850"
  },
  {
    "text": "object like rope and laundry. And we are also looking at how\nmultiple robots can actually",
    "start": "2383850",
    "end": "2390060"
  },
  {
    "text": "coordinate with each other\nin a good way for example, for conveyor belt operations or\nmulti-drone delivery systems.",
    "start": "2390060",
    "end": "2399270"
  },
  {
    "text": "And with that, I thank you\nvery much for your attention. And I thank, of\ncourse, all my students",
    "start": "2399270",
    "end": "2404420"
  },
  {
    "text": "and my funding\nsources for providing the means to do this research.",
    "start": "2404420",
    "end": "2409619"
  },
  {
    "text": "Thank you very much. PAX HEHMEYER: Thank\nyou Jeannette. That is wonderful. And really appreciated\nthat and we",
    "start": "2409620",
    "end": "2416420"
  },
  {
    "text": "have a lot of great questions. So with that, I want to jump\ninto the Q&A. And we have--",
    "start": "2416420",
    "end": "2423202"
  },
  {
    "start": "2418000",
    "end": "2477000"
  },
  {
    "text": "so, and Jeannette, we have\nkind of an interesting mix of both very specific and\nsome general questions. So I thought I would start\nmaybe with a general one that",
    "start": "2423202",
    "end": "2430000"
  },
  {
    "text": "builds off your last point\naround how robots are using,",
    "start": "2430000",
    "end": "2435250"
  },
  {
    "text": "can use and create environmental\nconstraints to scaffold the learning for another robot. And so, what do you-- this\nis kind of a big question.",
    "start": "2435250",
    "end": "2443510"
  },
  {
    "text": "So in terms of the\nbuilt environment, how do you see it changing in\norder to accommodate robots? Or will robots continue\nto sort of learn",
    "start": "2443510",
    "end": "2451390"
  },
  {
    "text": "to work within the\nhuman built environment? One example that I came\nacross when preparing for this",
    "start": "2451390",
    "end": "2456520"
  },
  {
    "text": "was that apparently, brick\nlaying is very hard for robots to do. And one of the\nreasons is bricks are kind of built for\nbeing manipulated",
    "start": "2456520",
    "end": "2463690"
  },
  {
    "text": "by people and not robots. So where do you see the\nfuture of kind of built-in",
    "start": "2463690",
    "end": "2469030"
  },
  {
    "text": "environmental constraints aiding\nrobots versus humans and that whole-- I guess, it's a\nvery big question.",
    "start": "2469030",
    "end": "2475100"
  },
  {
    "text": "But I'm curious to\nhear your thoughts. JEANNETTE BOHG: Yeah. It's a very, very good question.",
    "start": "2475100",
    "end": "2480280"
  },
  {
    "text": "So in fact, yeah,\nthere are kind of like two current\nfacts on this I guess.",
    "start": "2480280",
    "end": "2486810"
  },
  {
    "text": "So first of all, robots\nare very incapable",
    "start": "2486810",
    "end": "2492310"
  },
  {
    "text": "and unstructured environments\nat the moment, right? It's very hard to\nmake them work. So where robots are\ncurrently working really well",
    "start": "2492310",
    "end": "2500500"
  },
  {
    "text": "are environments\nthat are actually built for them in a way, right? And so in factories where\nassembly lines are basically",
    "start": "2500500",
    "end": "2511829"
  },
  {
    "text": "met, like, basically, where the\nrobots are working on assembly lines, they are typically\nstructured for them",
    "start": "2511830",
    "end": "2517660"
  },
  {
    "text": "so that they have an\neasier time manipulating and where uncertainty is also\nreduced as much as possible.",
    "start": "2517660",
    "end": "2524420"
  },
  {
    "text": "And so, I think it's\nbasically a requirement right",
    "start": "2524420",
    "end": "2530650"
  },
  {
    "text": "now to design the environment,\nsuch that a robot works reasonably well.",
    "start": "2530650",
    "end": "2536660"
  },
  {
    "text": "And so what we really-- so the second-- that's one fact\nof the current kind of state",
    "start": "2536660",
    "end": "2542750"
  },
  {
    "text": "of things. And the second one is that\npeople are also exploiting",
    "start": "2542750",
    "end": "2547990"
  },
  {
    "text": "environmental constraint. And in fact, there are\nreally good designs of products and of the home or\nof just everyday environments",
    "start": "2547990",
    "end": "2555100"
  },
  {
    "text": "that make it easy for\npeople to manipulate them. Just take this one example I\nhad like right with the ATM",
    "start": "2555100",
    "end": "2561339"
  },
  {
    "text": "for example. But just think of doorknobs\nand of door handles, and you know your stove, and\nhow they are designed, right?",
    "start": "2561340",
    "end": "2568870"
  },
  {
    "text": "They are all made\nsomehow people. And so I think it's very\nviable to also consider",
    "start": "2568870",
    "end": "2576220"
  },
  {
    "text": "designing the\nenvironment for robots to make it easier for\nthem but also maybe to change the hardware of\nrobots to make it easier",
    "start": "2576220",
    "end": "2583930"
  },
  {
    "text": "for them to manipulate\nthings that are already in our environment.",
    "start": "2583930",
    "end": "2589270"
  },
  {
    "text": "And in our work, we were\nreally interested in if the environment is maybe\nnot as friendly to a robot",
    "start": "2589270",
    "end": "2597040"
  },
  {
    "text": "right now, how can a\nrobot basically change",
    "start": "2597040",
    "end": "2602290"
  },
  {
    "text": "it autonomously to help itself\nor maybe help another robot? And so this work that I showed\nis really just the beginning",
    "start": "2602290",
    "end": "2610119"
  },
  {
    "text": "of it and it still has-- there, the robot only\nhas very few choices",
    "start": "2610120",
    "end": "2615160"
  },
  {
    "text": "of what to actually\ndo and it can't really pick the shape of the\nenvironmental constraints,",
    "start": "2615160",
    "end": "2620559"
  },
  {
    "text": "for example, we\ngive it one shape. And so, they have really\ninteresting questions there that to explore and how to\nexpand this towards this vision",
    "start": "2620560",
    "end": "2631060"
  },
  {
    "text": "that I just outlined, that\na robot can actually help itself to manipulate better. So if you want to\nwork on that idea,",
    "start": "2631060",
    "end": "2636598"
  },
  {
    "text": "you should come to the course\nand explore that research project.",
    "start": "2636598",
    "end": "2641625"
  },
  {
    "text": "PAX HEHMEYER: That's fantastic. Yeah, that's very interesting. We have a couple\nof other, I don't",
    "start": "2641625",
    "end": "2646740"
  },
  {
    "text": "know if this is a good\nsegue, but the question just popped up. And this is again, maybe\na high-level question.",
    "start": "2646740",
    "end": "2654270"
  },
  {
    "text": "Have there been\nbiologically inspired solutions for object grasping?",
    "start": "2654270",
    "end": "2659387"
  },
  {
    "text": "So I don't know quite how\nwe would interpret that, whether it's sort of\ninspirations from nature",
    "start": "2659387",
    "end": "2664530"
  },
  {
    "text": "or whether trying to reverse\nengineer how humans do it. But I guess I'll leave it there.",
    "start": "2664530",
    "end": "2670964"
  },
  {
    "text": "Have there been any biologically\nsolutions for object. JEANNETTE BOHG: Yeah, for sure. Yeah.",
    "start": "2670965",
    "end": "2678180"
  },
  {
    "text": "I think especially when\nit comes to hand design, many, many hands are actually\nbiologically inspired",
    "start": "2678180",
    "end": "2684569"
  },
  {
    "text": "and are anthropomorphic, right? And I showed two examples\nin this presentation of two hands that are really\nlooking like human hands, maybe",
    "start": "2684570",
    "end": "2692099"
  },
  {
    "text": "have one finger, only\nfour fingers instead of 5. So they are definitely\nthis but it turns out",
    "start": "2692100",
    "end": "2698790"
  },
  {
    "text": "that sometimes non-biological\nsolutions are actually",
    "start": "2698790",
    "end": "2703890"
  },
  {
    "text": "working much better. So for example,\none of our partners who we're doing research\nwith has actually",
    "start": "2703890",
    "end": "2711780"
  },
  {
    "text": "developed a particular\nend-effector for a two-fingered gripper\nthat is more like a hook,",
    "start": "2711780",
    "end": "2720030"
  },
  {
    "text": "and it really eases\neverything suddenly, right? So there is a value of also\njust thinking outside of what we",
    "start": "2720030",
    "end": "2728280"
  },
  {
    "text": "know or what we see. And it's sometimes\nreally hard to decide.",
    "start": "2728280",
    "end": "2734520"
  },
  {
    "text": "Like, oh, is the\nbiological-inspired solution here actually good?",
    "start": "2734520",
    "end": "2739770"
  },
  {
    "text": "Or can we think outside of\nthe box because it's a robot and we have the\nability to design. And so, you know, sometimes\nyou want something",
    "start": "2739770",
    "end": "2746550"
  },
  {
    "text": "that is not\nbiologically-inspired, but this idea of exploiting\nenvironmental constraints is actually inspired\nby observing",
    "start": "2746550",
    "end": "2755099"
  },
  {
    "text": "how people manipulate it. So you could think of it\nas biologically-inspired.",
    "start": "2755100",
    "end": "2760440"
  },
  {
    "text": "PAX HEHMEYER: So\nstaying on this topic because it seems like we're\ngetting some more interesting questions around this.",
    "start": "2760440",
    "end": "2766800"
  },
  {
    "text": "And I'm going to-- I'm going to read\nthe whole question, and I don't\nunderstand part of it. But can you speak\nto the modeling",
    "start": "2766800",
    "end": "2772740"
  },
  {
    "text": "the environment with the\nminimal sparse spatial matrix. So basically, can\nyou speak to how a robot can reduce\ncomplexity in the environment",
    "start": "2772740",
    "end": "2779760"
  },
  {
    "text": "to just the relevant\ninfo for the task, right? How, can, or does a robot\ndeal, and maybe this",
    "start": "2779760",
    "end": "2786599"
  },
  {
    "text": "is beyond the current research,\ndeal with a complex environment and figure out, I\ndon't need to deal with all this noise\nover here, I just",
    "start": "2786600",
    "end": "2793680"
  },
  {
    "text": "want to deal with this\nright in front of me. JEANNETTE BOHG: Yeah. Oh, yeah. This is like a\nvery hard question.",
    "start": "2793680",
    "end": "2800690"
  },
  {
    "text": "And yeah, that's like an\nongoing research question and there are many different\nways on how to do that.",
    "start": "2800690",
    "end": "2806490"
  },
  {
    "text": "For example, there is a\nbiologically-inspired approach that is called attention.",
    "start": "2806490",
    "end": "2811980"
  },
  {
    "text": "So people are actually\nalso doing that, right? Like, they focus on parts\nthat are important to the task",
    "start": "2811980",
    "end": "2819392"
  },
  {
    "text": "that they are doing right\nnow and they kind of like ignore even unconsciously\nthings that are not important.",
    "start": "2819393",
    "end": "2828040"
  },
  {
    "text": "And so there are\nalso ways on how to implement that on a robot so\nthat it, for example, visually",
    "start": "2828040",
    "end": "2834390"
  },
  {
    "text": "only pays attention to things\nthat are somehow salient and pop out or that are\nprobably relevant for the task",
    "start": "2834390",
    "end": "2843180"
  },
  {
    "text": "that it's doing. And we have done work\non this actually,",
    "start": "2843180",
    "end": "2848310"
  },
  {
    "text": "on training a model that is task\ndependent, is paying attention",
    "start": "2848310",
    "end": "2855540"
  },
  {
    "text": "to certain things in\nthe task-dependent way. And as training data, we\nused eye tracking or we",
    "start": "2855540",
    "end": "2864000"
  },
  {
    "text": "track basically where\npeople are searching when they are supposed to find\na particular object in an image.",
    "start": "2864000",
    "end": "2870210"
  },
  {
    "text": "And so, we just use\nthis eye tracking data and trained a model for\nrobots basically on this.",
    "start": "2870210",
    "end": "2878530"
  },
  {
    "text": "Another way on how to\nhave a robot compress all the sensory\ninformation that comes in",
    "start": "2878530",
    "end": "2886079"
  },
  {
    "text": "is by learning low\ndimensional representation of this very high\ndimensional sensory data",
    "start": "2886080",
    "end": "2892110"
  },
  {
    "text": "in the context of a task. So that's actually what\nwe've done in the work that I haven't presented here\nat all, where a robot is also",
    "start": "2892110",
    "end": "2901050"
  },
  {
    "text": "supposed to do a\nmanipulation task and we learn from high\ndimensional images",
    "start": "2901050",
    "end": "2909480"
  },
  {
    "text": "and tactile data to\ncompress it into just like 124 dimensional vector.",
    "start": "2909480",
    "end": "2915810"
  },
  {
    "text": "And we showed that\nthis is good enough. We're actually done feeding\nit into a learned policy",
    "start": "2915810",
    "end": "2921150"
  },
  {
    "text": "to do the task. And so that's really learned\nin the context of the task as well.",
    "start": "2921150",
    "end": "2927779"
  },
  {
    "text": "And somewhat the\nlimitation there I see is that these\nrepresentations,",
    "start": "2927780",
    "end": "2934500"
  },
  {
    "text": "they don't necessarily then\ngeneralize to other tasks, right? And so I think what we\nhaven't yet figured out",
    "start": "2934500",
    "end": "2941550"
  },
  {
    "text": "is how to compress\ninformation for, automatically",
    "start": "2941550",
    "end": "2947280"
  },
  {
    "text": "for any kind of task, right? So we can do it pretty\nwell for one task but then that kind of\ncompressed information",
    "start": "2947280",
    "end": "2953850"
  },
  {
    "text": "doesn't necessarily work very\nwell for another manipulation task. Also, another interesting\nresearch question.",
    "start": "2953850",
    "end": "2961069"
  },
  {
    "text": "Yeah. PAX HEHMEYER: I think\nthis is building on that question is can you-- you talked about human\nscanning the environment.",
    "start": "2961070",
    "end": "2968460"
  },
  {
    "text": "Can you talk about\nvision-based tactile sensing and non-vision\nsensing approaches,",
    "start": "2968460",
    "end": "2974829"
  },
  {
    "text": "and what you see and the\ndifferences of those, if there are opportunities in one\napproach over the other,",
    "start": "2974830",
    "end": "2980800"
  },
  {
    "text": "trade offs, those\nkinds of things. JEANNETTE BOHG: Yeah. So yeah, in fact,\nI've been working",
    "start": "2980800",
    "end": "2986799"
  },
  {
    "text": "with many different kinds\nof sensing modalities. And vision or just like\nRGB, like, just color images",
    "start": "2986800",
    "end": "2994630"
  },
  {
    "text": "and video are very important. But I also worked a lot\nwith just depth images so",
    "start": "2994630",
    "end": "3000480"
  },
  {
    "text": "where you can directly perceive\n3D structure of the world. And this is, in fact, very\nimportant for robot decision",
    "start": "3000480",
    "end": "3007950"
  },
  {
    "text": "making because they are\nmoving in this 3D world. So they really have\nto understand how far",
    "start": "3007950",
    "end": "3013080"
  },
  {
    "text": "away something is,\nwhere a robot hand is relative to this object, what\nthe shape really of this object",
    "start": "3013080",
    "end": "3019890"
  },
  {
    "text": "is? The three dimensional shape. So that's certainly\nreally important. But then what is specifically\nimportant for manipulation",
    "start": "3019890",
    "end": "3028620"
  },
  {
    "text": "is also the sense of touch. And that-- like, especially\nonce an object is in the hand",
    "start": "3028620",
    "end": "3038342"
  },
  {
    "text": "or close to the hand,\nit's very important to use that kind of\nsense because there are a lot of occlusion just\nby the hand of the object.",
    "start": "3038342",
    "end": "3046420"
  },
  {
    "text": "And so, it's important to use\nthat other sense that is not affected by this occlusion. But it's very different\nfrom visual data.",
    "start": "3046420",
    "end": "3055589"
  },
  {
    "text": "And also, the hardware\nfor tactile sensing",
    "start": "3055590",
    "end": "3061740"
  },
  {
    "text": "is actually not as\nadvanced maybe as cameras. So there are many\ndifferent solutions, unclear what is the best\none, and there are not",
    "start": "3061740",
    "end": "3069240"
  },
  {
    "text": "like good, necessarily\ngood architectures to process this data.",
    "start": "3069240",
    "end": "3074740"
  },
  {
    "text": "So this is also very open\nspace but it's undoubtedly, it's very important for a robot\nto have that sense of touch",
    "start": "3074740",
    "end": "3081029"
  },
  {
    "text": "but it's also like\nanother open area that is very exciting to look at. And another interesting modality\nthat I haven't also shown",
    "start": "3081030",
    "end": "3089579"
  },
  {
    "text": "is the modality of sound. So how-- if a robot, for\nexample, taps on an object",
    "start": "3089580",
    "end": "3096390"
  },
  {
    "text": "and it makes a sound\nor maybe with a tool or something like that, can\nit infer material properties,",
    "start": "3096390",
    "end": "3102970"
  },
  {
    "text": "it can, turns out. And if it knows that, it\ncan actually maybe adjust",
    "start": "3102970",
    "end": "3109440"
  },
  {
    "text": "its manipulation\nstrategy because, oh, it's figured out it's\nglass so it's more slippery. Or maybe it's something\nrougher, so it's not a slippery",
    "start": "3109440",
    "end": "3118560"
  },
  {
    "text": "so it can somehow\ngrasp it differently. So this is another\ninteresting modality",
    "start": "3118560",
    "end": "3125490"
  },
  {
    "text": "that is actually quite\nunder-explored in robotics as well. PAX HEHMEYER: That's really\ninteresting and exciting.",
    "start": "3125490",
    "end": "3133740"
  },
  {
    "text": "I think-- Yeah, it'd be\ninteresting to see how sound is, figures into the future\nrobots in terms of figuring out",
    "start": "3133740",
    "end": "3141750"
  },
  {
    "text": "their environment. I wanted to come back to\nmaybe some questions around,",
    "start": "3141750",
    "end": "3147680"
  },
  {
    "text": "I think it was your UniGrasp\nmodel and around the point",
    "start": "3147680",
    "end": "3152750"
  },
  {
    "text": "clouds. So two questions around that. One very specific\nthat just came in is, do you see any\napplications for--",
    "start": "3152750",
    "end": "3161570"
  },
  {
    "text": "If I understand,\nthe point clouds were used to just configure\nthe grippers themselves. Do you see any applications\nfor configuring the whole arm,",
    "start": "3161570",
    "end": "3168859"
  },
  {
    "text": "using those models to configure\nthe whole arm or other parts of the robot? JEANNETTE BOHG: Yeah.",
    "start": "3168860",
    "end": "3174130"
  },
  {
    "text": "I think I see that. Although I would say\nin that second part, the robot arm actually had\nto be moved in such a way",
    "start": "3179470",
    "end": "3186790"
  },
  {
    "text": "that none of its part are\ncolliding with the environment, while the hand is actually\ncolliding with things.",
    "start": "3186790",
    "end": "3194280"
  },
  {
    "text": "So it's a little bit of a\ndifferent problem or the hand and the rest of the arm actually\nhave slightly different goals.",
    "start": "3194280",
    "end": "3200950"
  },
  {
    "text": "One wants to get in\ntouch with things, the other one\nreally doesn't want to get in touch with things.",
    "start": "3200950",
    "end": "3206767"
  },
  {
    "text": "So it's slightly different. So I personally would\nsolve the problem",
    "start": "3206767",
    "end": "3212020"
  },
  {
    "text": "of what the arm is\ndoing differently from what the hand is doing. Just because we can\nmodel it pretty well,",
    "start": "3212020",
    "end": "3219670"
  },
  {
    "text": "how to avoid collisions\nwith an arm while what the fine sensory motor\ncontrol of fingers",
    "start": "3219670",
    "end": "3225079"
  },
  {
    "text": "is like a different\nsort of problem that we don't have as\ngood models for it as",
    "start": "3225080",
    "end": "3230290"
  },
  {
    "text": "could cost functions\nto optimize and so on. Yeah. But I think you could do\nit but I would actually",
    "start": "3230290",
    "end": "3235660"
  },
  {
    "text": "make a different choice. PAX HEHMEYER: It's interesting. And maybe a follow\nup on that is someone that had asked why you need such\nhigh resolution on those point",
    "start": "3235660",
    "end": "3242710"
  },
  {
    "text": "clouds if there are\nso few states of-- the small number of states\nfor the actual gripper.",
    "start": "3242710",
    "end": "3250260"
  },
  {
    "text": "JEANNETTE BOHG: I see. I see.",
    "start": "3250260",
    "end": "3255600"
  },
  {
    "text": "So I think that maybe\nit refers to how the gripper is represented. Although, maybe it's just\nthe two-fingered gripper",
    "start": "3255600",
    "end": "3261690"
  },
  {
    "text": "and it has really just\none degree of freedom can open and close to fingers. Yeah, that's true.",
    "start": "3261690",
    "end": "3268210"
  },
  {
    "text": "But there are, especially\nfor the object, an object may have\na very complex shape",
    "start": "3268210",
    "end": "3274440"
  },
  {
    "text": "and even with just a simple\ngripper that can just open and close to fingers, there\nare still infinitely many ways",
    "start": "3274440",
    "end": "3281850"
  },
  {
    "text": "to grasp that complex object. And so, you want\nto really capture especially the\nshape of the object",
    "start": "3281850",
    "end": "3288420"
  },
  {
    "text": "to match the shape\nof the hand to it. And so, that's why I would\nsay you need this point cloud.",
    "start": "3288420",
    "end": "3297930"
  },
  {
    "text": "And maybe you can\ndown and sample it. But I think the shape,\nthe complementary shape",
    "start": "3297930",
    "end": "3303269"
  },
  {
    "text": "of the gripper too in these\ndifferent configurations to the object is actually\nquite important to capture.",
    "start": "3303270",
    "end": "3309270"
  },
  {
    "text": "PAX HEHMEYER: It's great. We're just about\nout of time here. So maybe-- I got a lot of\nreally wonderful questions.",
    "start": "3309270",
    "end": "3316470"
  },
  {
    "text": "We didn't get to them all. But thank you all\nfor participating. Jeannette, is there\nanything you would want to add or anything\nthat you're particularly",
    "start": "3316470",
    "end": "3323250"
  },
  {
    "text": "excited about in terms of\nnext steps or the course that you'd want to\nleave everyone with? JEANNETTE BOHG: Oh, I'm excited\nabout so many different things",
    "start": "3323250",
    "end": "3331170"
  },
  {
    "text": "that I already mentioned. Like, equipping robots\nwith the ability to understand language,\nthat people talking to them",
    "start": "3331170",
    "end": "3339390"
  },
  {
    "text": "and telling them\nwhat to do and then being able to do these things\nor using sound for example.",
    "start": "3339390",
    "end": "3345190"
  },
  {
    "text": "So all of these different\nthings I'm excited about. And I'm excited\nabout exploring ideas",
    "start": "3345190",
    "end": "3350910"
  },
  {
    "text": "that weren't even in my\nhead, and this course it's like a great opportunity\nto explore this.",
    "start": "3350910",
    "end": "3356605"
  },
  {
    "text": "And thank you very\nmuch, everyone, for all these great questions\nthat I saw and we couldn't even",
    "start": "3356605",
    "end": "3363940"
  },
  {
    "text": "get to answering. PAX HEHMEYER: Yeah. Well, thank you, Jeannette. Thank you everyone\nwho joined us.",
    "start": "3363940",
    "end": "3370170"
  },
  {
    "text": "And we really\nappreciate your time, and we hope you\nenjoyed the session.",
    "start": "3370170",
    "end": "3374900"
  }
]