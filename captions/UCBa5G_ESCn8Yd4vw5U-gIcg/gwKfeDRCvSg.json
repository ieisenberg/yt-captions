[
  {
    "start": "0",
    "end": "5710"
  },
  {
    "text": "OK, so let's get started. So I think the last time\nwhat we were left was on--",
    "start": "5710",
    "end": "17110"
  },
  {
    "text": "I think we covered the\nweaker generalization bond. And then, today we\nare going to provide",
    "start": "17110",
    "end": "24460"
  },
  {
    "text": "a stronger generalization\nbound for the neural network.",
    "start": "24460",
    "end": "30070"
  },
  {
    "text": "Let me just double\ncheck whether I-- sorry. Somehow I got confused\nwhere I'm left.",
    "start": "30070",
    "end": "39350"
  },
  {
    "text": "OK, cool, cool. Yeah. Ah, yeah. Yeah. So last time what we did was\nthat we had this generalization",
    "start": "39350",
    "end": "49250"
  },
  {
    "text": "bound of the form that\nyou have something like something in the square\nroot of n in the denominator.",
    "start": "49250",
    "end": "56469"
  },
  {
    "start": "56470",
    "end": "61650"
  },
  {
    "text": "And today we are going to\nremove that square root of n, not exactly by just\nimproving the bound.",
    "start": "61650",
    "end": "67829"
  },
  {
    "text": "We also have to somewhat\nchange the hypothesis class. So that's the first\npart of the lecture.",
    "start": "67830",
    "end": "74340"
  },
  {
    "text": "And then we're going\nto talk about-- today we're going to\ntalk about stronger. So first we have a\nstronger version,",
    "start": "74340",
    "end": "79650"
  },
  {
    "text": "and then we talk about some\nconnections to kernel method.",
    "start": "79650",
    "end": "85425"
  },
  {
    "text": " And then we will talk\nabout even stronger bound",
    "start": "85425",
    "end": "92440"
  },
  {
    "text": "for multiple layer networks. And that requires\nsome preparation",
    "start": "92440",
    "end": "97469"
  },
  {
    "text": "with some techniques. And we'll talk about\nthose techniques if we have time today. Otherwise, we'll talk\nabout those next week.",
    "start": "97470",
    "end": "105630"
  },
  {
    "text": "OK, so just to briefly\nreview the setup. So the setup was that we\nhave some theta, which",
    "start": "105630",
    "end": "111719"
  },
  {
    "text": "consists of two layers. And the first layer\nis, I think this",
    "start": "111720",
    "end": "119010"
  },
  {
    "text": "is called second layer,\nwhich is the vector w. And the first layer\nis a matrix that maps",
    "start": "119010",
    "end": "124020"
  },
  {
    "text": "dimension d to dimension n. And our model is something\nlike w transpose phi of Ux.",
    "start": "124020",
    "end": "133310"
  },
  {
    "text": "And the phi is\nelement-wise ReLU. ",
    "start": "133310",
    "end": "141480"
  },
  {
    "text": "And so, last time\nwhat we had was that we have this\ngeneralization bound of the form",
    "start": "141480",
    "end": "149710"
  },
  {
    "text": "that this Rademacher\ncomplexity of H is bounded by something\nlike times 2 times",
    "start": "149710",
    "end": "157390"
  },
  {
    "text": "Bw, Bu times C, square root of\nn over n, where H is defined",
    "start": "157390",
    "end": "164360"
  },
  {
    "text": "to be something like where\nyou restricted 2 norm of w",
    "start": "164360",
    "end": "172780"
  },
  {
    "text": "to be Bw. And you restricted the\nmax 2 norm of Ui to be Bu.",
    "start": "172780",
    "end": "181710"
  },
  {
    "text": "And that's the hypothesis class. And in some sense,\nif you, I guess,",
    "start": "181710",
    "end": "186799"
  },
  {
    "text": "we discussed this a\nlittle bit in the class. So and also I think somebody\nasked this question.",
    "start": "186800",
    "end": "193270"
  },
  {
    "text": "In some sense, there is\na scaling invariance. Because you can-- so\nalpha times w over alpha U",
    "start": "193270",
    "end": "199755"
  },
  {
    "text": "would be the same model\nas w times U, right? So just because you can scale\nthe first layer by alpha",
    "start": "199755",
    "end": "207694"
  },
  {
    "text": "and you downscale the second\nlayer by 1 over alpha if alpha is bigger than 0. So that means that you can also\nchange this bound a little bit",
    "start": "207695",
    "end": "215660"
  },
  {
    "text": "and rewrite it as something\nlike, basically you can say, roughly speaking the\ngeneralization error,",
    "start": "215660",
    "end": "223360"
  },
  {
    "text": "it is something like bounded\nby O of square root n over n.",
    "start": "223360",
    "end": "228430"
  },
  {
    "text": "Sorry, this is square\nroot n, over square root n times the norm of\nw times the max i--",
    "start": "228430",
    "end": "238660"
  },
  {
    "text": "the max of the 2 norm of U. This\nis kind of the intuitive way to think about this. ",
    "start": "238660",
    "end": "246153"
  },
  {
    "text": "So today we're going to have\na stronger bound that doesn't have the square root of n here. But we will have some\nslightly different terms",
    "start": "246153",
    "end": "251537"
  },
  {
    "text": "here in terms of how do you\nmeasure the complexity of w and the complexity of U. OK? ",
    "start": "251537",
    "end": "262750"
  },
  {
    "text": "So here is a refined bound. Let me define that. Let me state the theorem first.",
    "start": "262750",
    "end": "269009"
  },
  {
    "text": "So the theorem is that we define\nthis complex measure that's called C of theta.",
    "start": "269010",
    "end": "275340"
  },
  {
    "text": "This complex measure\nis defined to be the sum of the absolute value,\nwj, times the 2 norm of uj.",
    "start": "275340",
    "end": "286539"
  },
  {
    "text": "And you take a sum over j. And correspondingly, given\nthis complex measure,",
    "start": "286540",
    "end": "292950"
  },
  {
    "text": "you can define the corresponding\nhypothesis class, which is the family of functions\nwith bounded complexity,",
    "start": "292950",
    "end": "300420"
  },
  {
    "text": "something like bounded\nby B. And also, we assume that the norm of xi\nis less than C for every i.",
    "start": "300420",
    "end": "313850"
  },
  {
    "text": "Know that here actually we have\na strong assumption of data. Because before we assumed\nthe average of the norm",
    "start": "313850",
    "end": "318990"
  },
  {
    "text": "is less than C, or the\naverage of the squared norm is less than C squared.",
    "start": "318990",
    "end": "324669"
  },
  {
    "text": "Now we assume each data point\nis less than C. This is just the technicality in some sense.",
    "start": "324670",
    "end": "331260"
  },
  {
    "text": "And with all of this, then we\ncan prove that the Rademacher complexity of H is\nbounded by 2 times",
    "start": "331260",
    "end": "339100"
  },
  {
    "text": "B times C over square root of n. ",
    "start": "339100",
    "end": "351962"
  },
  {
    "text": "OK. So maybe let me first start\nwith some interpretation of this theorem and see why this\nis an interesting one to prove,",
    "start": "351962",
    "end": "358260"
  },
  {
    "text": "and then I'll write the proof. So a few remarks. The first one is that why this\nis better than before, right?",
    "start": "358260",
    "end": "366060"
  },
  {
    "text": "So I'm claiming that\nthis is strictly better than before, at least\nin the following sense.",
    "start": "366060",
    "end": "379210"
  },
  {
    "text": "So if you really, I guess,\nso the way that I compare",
    "start": "379210",
    "end": "384850"
  },
  {
    "text": "them is the following. So before what we had was\nthat this generalization is",
    "start": "384850",
    "end": "391602"
  },
  {
    "text": "something like square root of\nn over square root of n times this complexity is\nsomething like 2 norm",
    "start": "391602",
    "end": "396740"
  },
  {
    "text": "of w times this one,\nas I already said. That's kind of the intuitive\nway of thinking about it.",
    "start": "396740",
    "end": "402800"
  },
  {
    "text": "If you assume the\nC is a constant. C is just something\nabout the data, which then will change as you\nchange the hypothesis class.",
    "start": "402800",
    "end": "410530"
  },
  {
    "text": "It's really something\nlike a constant. And now, you can basically\nthink of this new bound",
    "start": "410530",
    "end": "417400"
  },
  {
    "text": "as peak O of 1 over square\nroot of n times B. What is the capital B?",
    "start": "417400",
    "end": "422500"
  },
  {
    "text": "The capital B is basically\nsum of wj times u2 2 not.",
    "start": "422500",
    "end": "428970"
  },
  {
    "text": "So basically, the way\nI'm comparing them is that I'm comparing\nthese two quantities.",
    "start": "428970",
    "end": "434070"
  },
  {
    "text": "And the claim that the\nsecond quantity is strictly smaller than the first quality. And the reason is just that if\nyou do some simple inequality,",
    "start": "434070",
    "end": "445330"
  },
  {
    "text": "you can see that\nthis is less than-- ",
    "start": "445330",
    "end": "451500"
  },
  {
    "text": "so first Cauchy-Schwarz you\nsay this is less than sum of wj squared 1/2, and sum of uj 2\nnorm square to the power 1/2.",
    "start": "451500",
    "end": "463229"
  },
  {
    "text": "And then the first term\nbecomes 2 norm of w. And the second term is you can\nbond each of them by the max.",
    "start": "463230",
    "end": "471060"
  },
  {
    "text": "So you get n times max\nj, Uj 2 norm square 1/2.",
    "start": "471060",
    "end": "479550"
  },
  {
    "text": "So what you got is square root n\ntimes w 2 norm times max j Uj 2",
    "start": "479550",
    "end": "487479"
  },
  {
    "text": "norm. So this is, in this sense, this\nis a strictly better bound.",
    "start": "487480",
    "end": "494400"
  },
  {
    "text": "And there could be the\nsame if your wj and Uj makes all of these\ninequalities exactly equal.",
    "start": "494400",
    "end": "500500"
  },
  {
    "text": "But in other cases, it won't. And in some sense, you can\nsee one of the intuition here is that this\nnew complex measure--",
    "start": "500500",
    "end": "507690"
  },
  {
    "text": "so another thing is that\nthis new complex measure-- ",
    "start": "507690",
    "end": "515059"
  },
  {
    "text": "C theta captures the\nscaling environments better. ",
    "start": "515059",
    "end": "528150"
  },
  {
    "text": "What do I mean by that? So what I mean is that-- so what I mentioned, I\nmentioned the following scaling",
    "start": "528150",
    "end": "534150"
  },
  {
    "text": "environments. So if you have w and U, this\nis equivalent to alpha dot w",
    "start": "534150",
    "end": "541600"
  },
  {
    "text": "over alpha U. This is\nbecause the ReLU is scaling environments,\nand you can do this.",
    "start": "541600",
    "end": "548163"
  },
  {
    "text": "But actually, you have a lot\nof more scaling environments. You can scale actually each\npair of neurons in this way.",
    "start": "548163",
    "end": "554520"
  },
  {
    "text": "So what you really have is that\nthis is actually equivalent to-- you can scale each\nwi to something like wj.",
    "start": "554520",
    "end": "563186"
  },
  {
    "text": "You scale each of these-- ",
    "start": "563186",
    "end": "568410"
  },
  {
    "text": "the j's coordinate by alpha j. And you scale correspondingly\nof 1 over alpha j times Uj.",
    "start": "568410",
    "end": "574080"
  },
  {
    "text": "And you do this for different\nscalar, for every wj and Uj. And this is still\nthe same, right?",
    "start": "574080",
    "end": "581370"
  },
  {
    "text": "But just because the\nsum of wj ReLU Uj",
    "start": "581370",
    "end": "588660"
  },
  {
    "text": "transpose x is the same as alpha\nj, wj phi of 1 over alpha j Uj",
    "start": "588660",
    "end": "597310"
  },
  {
    "text": "x, for any Uj-- for any scaling that\nis positive, right?",
    "start": "597310",
    "end": "603290"
  },
  {
    "text": "So and you can see\nthat if you consider these kind of environments,\nstill this complex measure is the same, right?",
    "start": "603290",
    "end": "609209"
  },
  {
    "text": "So the complexity measure\nis, it's really the invariant",
    "start": "609210",
    "end": "616720"
  },
  {
    "text": "to the scaling here. Because if you change\nwj and Uj accordingly, you don't change the complexity.",
    "start": "616720",
    "end": "622090"
  },
  {
    "text": "Which to some extent seems to\nbe a good thing to have, right? ",
    "start": "622090",
    "end": "629160"
  },
  {
    "text": "So but before the\ncomplex dimension doesn't have this\nproperty, so if you look at this complex dimension.",
    "start": "629160",
    "end": "637009"
  },
  {
    "text": "If you scale each of the wj by\na different scalar and you scale each of the unit by\ndifferent scalar,",
    "start": "637010",
    "end": "642290"
  },
  {
    "text": "you wouldn't-- this\nnumber would change.  [INAUDIBLE]",
    "start": "642290",
    "end": "649129"
  },
  {
    "text": " Sure. [INAUDIBLE] two\nnormal [INAUDIBLE]..",
    "start": "649129",
    "end": "657430"
  },
  {
    "text": "Right. [INAUDIBLE] Right. So you are saying that this? Yes.",
    "start": "657430",
    "end": "662850"
  },
  {
    "text": "So yes. So this one, you do make\na stronger assumption. [INAUDIBLE]",
    "start": "662850",
    "end": "669400"
  },
  {
    "text": "Sorry? Can you say again? [INAUDIBLE] ",
    "start": "669400",
    "end": "675720"
  },
  {
    "text": "By C, yes. [INAUDIBLE] Sorry, what was the question? Maybe I didn't answer.",
    "start": "675720",
    "end": "680877"
  },
  {
    "text": " I was thinking you write\nthat norm [INAUDIBLE]",
    "start": "680877",
    "end": "685974"
  },
  {
    "text": "as ex or [INAUDIBLE]? ",
    "start": "685974",
    "end": "693160"
  },
  {
    "text": "So but I think,\nI'm guessing what you are saying is that before\nthe condition was something",
    "start": "693160",
    "end": "700870"
  },
  {
    "text": "like-- [INAUDIBLE] I think it's 1 over n\ntimes sum of xi 2 norm",
    "start": "700870",
    "end": "708030"
  },
  {
    "text": "squared square root\nis less than C. That was in the previous\ntheorem, I think.",
    "start": "708030",
    "end": "714730"
  },
  {
    "text": "Or something like maybe-- that\nwas, or in the previous theorem we did like this. Yeah.",
    "start": "714730",
    "end": "720910"
  },
  {
    "text": "It's less than C squared. Right. So indeed, this is-- so the\nnew condition is stronger",
    "start": "720910",
    "end": "727870"
  },
  {
    "text": "than the old one, because\nthis one implies the old one. Correct. So yeah. So I'm assuming that suppose\nyou say this is not a problem,",
    "start": "727870",
    "end": "736180"
  },
  {
    "text": "you just live with the\nstronger assumption. Then outer bound\nis strictly better. In some sense, this\nassumption x actually",
    "start": "736180",
    "end": "742420"
  },
  {
    "text": "is a little bit less\nimportant to some extent. Because, for example,\nif any way your data",
    "start": "742420",
    "end": "748893"
  },
  {
    "text": "satisfies the\nstronger assumption, then it's less important. So yeah.",
    "start": "748893",
    "end": "755693"
  },
  {
    "text": "But you are right that the\ndata assumption is a little bit different. But I don't think it\nmatters that much. ",
    "start": "755693",
    "end": "764000"
  },
  {
    "text": "So I guess [INAUDIBLE]?  Right.",
    "start": "764000",
    "end": "769519"
  },
  {
    "text": "That's true. That is definitely true. Or you can choose the\nright C. So but I guess,",
    "start": "769520",
    "end": "776089"
  },
  {
    "text": "I think the question was\nmore about comparing the two theorems. If you normalize here, maybe\nyou should normalize there.",
    "start": "776090",
    "end": "782100"
  },
  {
    "text": "So what's the fair comparison? ",
    "start": "782100",
    "end": "787459"
  },
  {
    "text": "Cool. So this is one thing about\nthis complex measure. And sometimes,\nthis complex matter",
    "start": "787460",
    "end": "794753"
  },
  {
    "text": "is a little bit more\nenvironment to-- at least of the trivial environments\nin the neural network. So and also, the bond is better.",
    "start": "794753",
    "end": "801440"
  },
  {
    "text": "And also, another thing\nthat we have about-- nice thing about this is that--",
    "start": "801440",
    "end": "807050"
  },
  {
    "text": "about this theorem, is that if\nyou have n goes to infinity,",
    "start": "807050",
    "end": "812089"
  },
  {
    "text": "at least you get a stronger\nor equivalent theorem.",
    "start": "812090",
    "end": "819290"
  },
  {
    "text": "So the theorem its stronger.",
    "start": "819290",
    "end": "824880"
  },
  {
    "text": "So what do I mean by that? So let me explain this. So suppose you look at a\ndependency on m, right?",
    "start": "824880",
    "end": "831240"
  },
  {
    "text": "So this whole theorem depends\non m implicitly somewhere. I didn't specify that. But now let's make\nit more explicit.",
    "start": "831240",
    "end": "837270"
  },
  {
    "text": "Let's say, Hm is\nthis complexity. And the same thing so\nwhere you have m neurons.",
    "start": "837270",
    "end": "846540"
  },
  {
    "text": "And also C theta is\nless than B. All right. So for every m our\ntheorem applies.",
    "start": "846540",
    "end": "852240"
  },
  {
    "text": "So but now I'm just\nmaking a dependency on m a little more explicit. And you know that Hm is\na subset of Hm plus 1.",
    "start": "852240",
    "end": "862690"
  },
  {
    "text": "In what sense? In the sense that if you have\na function that is in Hm, you can always add a fake\nneuron, or 0, dummy neuron,",
    "start": "862690",
    "end": "870970"
  },
  {
    "text": "to make it Hm plus 1. Just so any f theta in Hm,\nyou can add a dummy neuron.",
    "start": "870970",
    "end": "882840"
  },
  {
    "text": "So meaning that you make w plus\n1, 0, and the U m plus 1, 0.",
    "start": "882840",
    "end": "889530"
  },
  {
    "text": "And then you can\nextend its function so that it becomes in Hm plus 1.",
    "start": "889530",
    "end": "895770"
  },
  {
    "text": "So Hm plus 1 is\nalways a strong-- it's a bigger family\nof functions than Hm.",
    "start": "895770",
    "end": "900870"
  },
  {
    "text": "So that's why you have a-- but the bond will depend on m. You have the same Rademacher\ncomplexity for every m.",
    "start": "900870",
    "end": "909209"
  },
  {
    "text": "So in some sense, you're bond\nwould be stronger for bigger m.",
    "start": "909210",
    "end": "914490"
  },
  {
    "text": "So the strongest\ntheorem would be you just applied for H infinity.",
    "start": "914490",
    "end": "920370"
  },
  {
    "text": "So and that's actually, in some\nsense, the fundamental reason why later you will\nsee that you're",
    "start": "920370",
    "end": "927000"
  },
  {
    "text": "going to have a\ngeneralization bound, at least the generalization\nbound that is decreasing as m goes to infinity.",
    "start": "927000",
    "end": "933640"
  },
  {
    "text": "So and that's\nanother nice property of this complexity measure.",
    "start": "933640",
    "end": "940350"
  },
  {
    "text": "And also, another small\nremark is that there is something called path-norm.",
    "start": "940350",
    "end": "945840"
  },
  {
    "text": "If you don't-- haven't heard of\nit, it probably doesn't matter. This is a complex measure\nthat people proposed.",
    "start": "945840",
    "end": "955649"
  },
  {
    "text": "And people evaluate that-- people found that\nthis is correlated with the real generalization\nbound empirically.",
    "start": "955650",
    "end": "964350"
  },
  {
    "text": "And this is very closely related\nto the definition of C theta here. So in some sense, what you\nsee, that the path-norm",
    "start": "964350",
    "end": "972930"
  },
  {
    "text": "is trying to say that you look\nat all the path from the input to the output. And you look at the total\nnorms of all the paths.",
    "start": "972930",
    "end": "981720"
  },
  {
    "text": "And in some sense, this\nis kind of like that. It's not exactly\nthe same depending on which version\nof the path-norm.",
    "start": "981720",
    "end": "988200"
  },
  {
    "text": "But the way you\nthink about this is that you look at the input x. And so, this is wj.",
    "start": "988200",
    "end": "996050"
  },
  {
    "text": "And this thing is Uj. So in some sense,\nyou look at it--",
    "start": "996050",
    "end": "1001706"
  },
  {
    "text": "so every path matters. So that's why you look\nat wj times Uj first, and then you take the sum.",
    "start": "1001706",
    "end": "1007000"
  },
  {
    "text": "Instead of that you\nlook at each layer first and then you multiply. ",
    "start": "1007000",
    "end": "1012590"
  },
  {
    "text": "Yeah. If you haven't heard of the\npath-norm, what I said probably wouldn't make that much sense.",
    "start": "1012590",
    "end": "1019451"
  },
  {
    "text": "But if you have heard\nof it, probably you can see the connection there. This is not super important. This is just something people\nhave empirically studied.",
    "start": "1019452",
    "end": "1026183"
  },
  {
    "text": " All right, so we'll talk\nabout more implications",
    "start": "1026184",
    "end": "1031390"
  },
  {
    "text": "of the theorem later. But before that,\nlet me prove it.",
    "start": "1031390",
    "end": "1037480"
  },
  {
    "text": "Any questions so far? ",
    "start": "1037480",
    "end": "1042819"
  },
  {
    "text": "So how do we prove this? So you can see that one of\nthe main point in the proof is that you want to change\nthe scaling in the right way,",
    "start": "1042819",
    "end": "1050142"
  },
  {
    "text": "because you want to capture\nthe scaling environments. You don't want to peel off. So before what we did was that\nwe tried to remove the w first.",
    "start": "1050142",
    "end": "1056970"
  },
  {
    "text": "And then we removed the U.\nYou have a sup over w and U, and somehow remove each\nof them sequentially.",
    "start": "1056970",
    "end": "1063630"
  },
  {
    "text": "And now, the thing is that\nyou still do the same thing. But you want to remove\nthem sequentially as well.",
    "start": "1063630",
    "end": "1072120"
  },
  {
    "text": "But you want to first rescale\nthings first and then remove them so that you can eventually\nget the right scaling",
    "start": "1072120",
    "end": "1079020"
  },
  {
    "text": "environment. I'm not sure whether\nthis makes sense. You will see more\nclearly in the proof.",
    "start": "1079020",
    "end": "1086640"
  },
  {
    "text": "So first of all, let's define\nthis vector U. Let's define U bar to be the normalized\nversion for U. So and then,",
    "start": "1086640",
    "end": "1094799"
  },
  {
    "text": "let's start with the derivation. So what we have is that\nthe Rademacher complexity",
    "start": "1094800",
    "end": "1101220"
  },
  {
    "text": "is something like this. I put my 1 over in front\njust to make it easier.",
    "start": "1101220",
    "end": "1107050"
  },
  {
    "text": "So you have-- this\nis the definition. ",
    "start": "1107050",
    "end": "1112790"
  },
  {
    "text": "And I guess, we do\nthe usual thing.",
    "start": "1112790",
    "end": "1119530"
  },
  {
    "text": "The first steps. The first two steps I'm just\nplugging in a definition--",
    "start": "1119530",
    "end": "1125970"
  },
  {
    "start": "1125970",
    "end": "1133530"
  },
  {
    "text": "xi.  And now, we want to\nfirst rescale w and U",
    "start": "1133530",
    "end": "1141480"
  },
  {
    "text": "before we take the sup. So what we do is that we\nread this as wj, Uj 2 norm.",
    "start": "1141480",
    "end": "1158600"
  },
  {
    "text": "So and then we insert the phi,\nwe use Uj bar transpose xi,",
    "start": "1158600",
    "end": "1164630"
  },
  {
    "text": "right? So in some sense, you put a\nnorm of Uj outside of the phi.",
    "start": "1164630",
    "end": "1171340"
  },
  {
    "text": "The norm of Uj is\na positive number. So you can put it out\noutside of the phi. And sorry, I have a little\nbit trouble reading this.",
    "start": "1171340",
    "end": "1187669"
  },
  {
    "text": "But I think I can\nremember what-- ",
    "start": "1187670",
    "end": "1194740"
  },
  {
    "text": "oh. OK. There's a page segment\nso that I couldn't read what my notes were.",
    "start": "1194740",
    "end": "1201000"
  },
  {
    "text": "Anyway, so you rearrange\nthis a little bit. So in some sense, we\ntreat this wj times",
    "start": "1201000",
    "end": "1207900"
  },
  {
    "text": "Uj 2 norm as our old wj. And we want to kind\nof remove that first. That's kind of--\nand also, you can",
    "start": "1207900",
    "end": "1213570"
  },
  {
    "text": "see that this one is\nsomething that shows up in the complexity measure. The complexity measure\nis basically the sum",
    "start": "1213570",
    "end": "1219120"
  },
  {
    "text": "of this is less than B, right? The complex measure is really\njust the sum of wj, Uj 2 norm,",
    "start": "1219120",
    "end": "1224335"
  },
  {
    "text": "right? So you have sup over theta. And you, I guess\nwe rewrite this.",
    "start": "1224335",
    "end": "1231809"
  },
  {
    "text": "You change the order\nof the summation so that it's clearer that this\ntimes sum of i from 1 to n,",
    "start": "1231810",
    "end": "1245000"
  },
  {
    "text": "Uj, phi of Uj bar transpose xi. ",
    "start": "1245000",
    "end": "1252590"
  },
  {
    "text": "And here, I guess let's specify\nwhat the concern of theta is. The concern of theta\nis that C theta is less than B. Which\nmeans that this wj Uj 2",
    "start": "1252590",
    "end": "1261830"
  },
  {
    "text": "norm is less than B, right? So the constraint is really just\nsaying that the sum of wj Uj 2",
    "start": "1261830",
    "end": "1269539"
  },
  {
    "text": "is less than B.",
    "start": "1269540",
    "end": "1274710"
  },
  {
    "text": "And now, you can see that\nthe sum of these quantities is less than B. But we care\nabout the weighted sub.",
    "start": "1274710",
    "end": "1280179"
  },
  {
    "text": "So we weight each of the\nquantities by something. And then you take the sub. Where's the sigma that\ndropped out of that last one?",
    "start": "1280180",
    "end": "1287860"
  },
  {
    "text": "What is it? Oh, sorry. Yeah. My bad. There's a sigma here. Sorry.",
    "start": "1287860",
    "end": "1294180"
  },
  {
    "text": "This is the bad-- this is the problem when\nyou draft things on the fly.",
    "start": "1294180",
    "end": "1300420"
  },
  {
    "text": "Just this particular line, I\ncouldn't read it from my notes. So I'm improvising. OK.",
    "start": "1300420",
    "end": "1307440"
  },
  {
    "text": "Thanks. So and then, let's-- OK. So we know that\nthe sum of wj Uj 2",
    "start": "1307440",
    "end": "1314460"
  },
  {
    "text": "is less than B. So\nthat means that you can use an inequality here. So you say that you,\nso you, I guess,",
    "start": "1314460",
    "end": "1327130"
  },
  {
    "text": "maybe let me just have a-- so this is basically\nyou are applying this.",
    "start": "1327130",
    "end": "1332680"
  },
  {
    "text": "ai bi is less than--\nyou know the sub in ai times the max of bi.",
    "start": "1332680",
    "end": "1340230"
  },
  {
    "text": "This is what we applied. Actually, I probably\nshould use j just to be more consistent with--",
    "start": "1340230",
    "end": "1346039"
  },
  {
    "text": "so this is j from 1 to n, and\nthis is j from 1 to n, aj, bj.",
    "start": "1346040",
    "end": "1353330"
  },
  {
    "text": "And aj corresponds\nto wj Uj 2 norm.",
    "start": "1353330",
    "end": "1358659"
  },
  {
    "text": "And bj corresponds to\nthis quality, right? It's abstractly what I'm doing.",
    "start": "1358660",
    "end": "1363750"
  },
  {
    "text": "So if you live in this, then\nyou get basically the sum of aj,",
    "start": "1363750",
    "end": "1372010"
  },
  {
    "text": "the sum of wj Uj 2 norm, j from\n1 to n, times the max over j.",
    "start": "1372010",
    "end": "1380675"
  },
  {
    "start": "1380675",
    "end": "1391650"
  },
  {
    "text": "Right. So in some sense, this is\nhow the inequality writes inner the product\na and b is less",
    "start": "1391650",
    "end": "1396980"
  },
  {
    "text": "than the 1 norm of a and\nthe infinity norm of b.",
    "start": "1396980",
    "end": "1402049"
  },
  {
    "text": "And then, this\nquantity, now we got-- this separate quantity. This is less than B, right? So then, this is less than\n1 over n times sigma times",
    "start": "1402050",
    "end": "1413390"
  },
  {
    "text": "b times sup over theta max\nj, sum of sigma i phi Uj",
    "start": "1413390",
    "end": "1423010"
  },
  {
    "text": "bar transpose xi. ",
    "start": "1423010",
    "end": "1429380"
  },
  {
    "text": "And now, this-- if\nyou carefully compare this with what we\nhad before, this should look somewhat\nsimilar-- familiar.",
    "start": "1429380",
    "end": "1435680"
  },
  {
    "text": "Because in some sense, we\nachieved almost the same thing as we have done before.",
    "start": "1435680",
    "end": "1440690"
  },
  {
    "text": "We removed the influence of w. And we only have\nsomething about U. And here, what you have about U\ndoesn't have the scale anymore.",
    "start": "1440690",
    "end": "1448460"
  },
  {
    "text": "You only have Uj bar. So basically, now what you can\ndo is that you can say that--",
    "start": "1448460",
    "end": "1458429"
  },
  {
    "text": "so from here it's\nbasically the same thing as the previous proof. Let me try to\nrepeat a few steps.",
    "start": "1458430",
    "end": "1464930"
  },
  {
    "text": "So I guess one\nthing you can do is that you realize\nthat this max over j is not doing really much.",
    "start": "1464930",
    "end": "1470370"
  },
  {
    "text": "So what you can do is\nthat you can replace this by max over U bar, where\nthe norm of U bar is 1,",
    "start": "1470370",
    "end": "1477620"
  },
  {
    "text": "and some sigma i phi\nU bar transpose xi.",
    "start": "1477620",
    "end": "1486690"
  },
  {
    "text": "So that's one thing we can do. [INAUDIBLE]  Sure.",
    "start": "1486690",
    "end": "1492480"
  },
  {
    "text": "That's good point. So I should have absolute value. So I think I should\nhave it here.",
    "start": "1492480",
    "end": "1500000"
  },
  {
    "text": "And I should have it here. And I still should have it here. Thanks.",
    "start": "1500000",
    "end": "1505740"
  },
  {
    "text": "Yeah. Thanks for catching all of this. So and then, this I guess, you\nprobably also remember, there's",
    "start": "1505740",
    "end": "1514159"
  },
  {
    "text": "a step I skipped before where\nI remove the x value by paying a factor of 2. So you can do it-- you can--",
    "start": "1514160",
    "end": "1520890"
  },
  {
    "text": "this is less than this sup. ",
    "start": "1520890",
    "end": "1536730"
  },
  {
    "text": "And there was-- all of this\nare almost the same as-- it's exactly the same as before.",
    "start": "1536730",
    "end": "1542080"
  },
  {
    "text": "And now, you can\npeel off the-- you can remove the phi by the\nLipschitz compensation",
    "start": "1542080",
    "end": "1549720"
  },
  {
    "text": "lemma or the Talagrand lemma. So you can get rid of the phi.",
    "start": "1549720",
    "end": "1556160"
  },
  {
    "start": "1556160",
    "end": "1561500"
  },
  {
    "text": "So this is Talagrand lemma. And then, you can--",
    "start": "1561500",
    "end": "1566690"
  },
  {
    "text": "then this becomes the\ncomplexity of the linear model. And you do some things.",
    "start": "1566690",
    "end": "1572750"
  },
  {
    "text": "And then you can get\nthe same thing, 2 B, C over square root of\n2, where the C comes",
    "start": "1572750",
    "end": "1578470"
  },
  {
    "text": "from the norm of the xi. ",
    "start": "1578470",
    "end": "1588900"
  },
  {
    "text": "So basically, from\nafter here, these are--",
    "start": "1588900",
    "end": "1598390"
  },
  {
    "text": "so this part, the\nsame as before. ",
    "start": "1598390",
    "end": "1605620"
  },
  {
    "text": "I guess there is a\nsmall difference, which is that the U bar now is\nnormalized to non-one.",
    "start": "1605620",
    "end": "1614420"
  },
  {
    "text": "So you just have a-- so that's why you\ndon't catch up. So before, if you look\nat the proof before,",
    "start": "1614420",
    "end": "1620850"
  },
  {
    "text": "what happens is that you don't\nhave the-- you have some other control of U bar-- control of the U. You know\nthat norm of U is less than BU.",
    "start": "1620850",
    "end": "1628649"
  },
  {
    "text": "And now you know the norm\nof U bar is less than 1. So that's why you don't have the\nBU show up in the final bond.",
    "start": "1628650",
    "end": "1634590"
  },
  {
    "text": "Because the norm of\nU bar is less than 1. So in some sense, this is\njust almost the same proof.",
    "start": "1634590",
    "end": "1640140"
  },
  {
    "text": "Which the only difference\nis that you somehow remove the scaling of U first. You put the scaling\nof U actively in the w",
    "start": "1640140",
    "end": "1648120"
  },
  {
    "text": "so that you can organize\nthis a little bit better. ",
    "start": "1648120",
    "end": "1657182"
  },
  {
    "text": "Any questions? OK. Cool. Great. So I think next, let\nme talk about some",
    "start": "1657182",
    "end": "1664750"
  },
  {
    "text": "of the kind of the implications\nof the theory here.",
    "start": "1664750",
    "end": "1671290"
  },
  {
    "text": "Some of them are\nkind of interesting. So I think one thing is that\nif you believe in a theory,",
    "start": "1671290",
    "end": "1676570"
  },
  {
    "text": "then what directly we\nshould do is that-- this is not what\npeople do in practice, but I would argue this\nis also close to what",
    "start": "1676570",
    "end": "1683470"
  },
  {
    "text": "people do in the practice. But if you just believe in\na theory, what you probably would do is you want to define\nthe following max margin",
    "start": "1683470",
    "end": "1691480"
  },
  {
    "text": "solution. You want to do the max margin\non the minimum norm solution. So I guess you can\ndo maybe either do",
    "start": "1691480",
    "end": "1698830"
  },
  {
    "text": "problem one, where you minimize\nthe complexity of C theta. ",
    "start": "1698830",
    "end": "1709950"
  },
  {
    "text": "And with the constraint that\nthe margin is larger than 1. ",
    "start": "1709950",
    "end": "1716833"
  },
  {
    "text": "Then why we care about the\nmargin, recall that all of this depends on the\nmargin eventually. Because eventually, your\ngeneralization error",
    "start": "1716833",
    "end": "1722610"
  },
  {
    "text": "will be the complexity\nover the margin. Or alternatively, I think\nthese are exactly equivalent.",
    "start": "1722610",
    "end": "1732760"
  },
  {
    "text": "So you can say that you\nmaximize the margin. ",
    "start": "1732760",
    "end": "1738870"
  },
  {
    "text": "And with the constraint that\nyour complexity is less than 1.",
    "start": "1738870",
    "end": "1747309"
  },
  {
    "text": "So let's call this program two. And we can probably define\nthis to be gamma star-- ",
    "start": "1747310",
    "end": "1753259"
  },
  {
    "text": "that I probably don't\nhave to define now. So we can do these\ntwo programs, right?. So and these two\nprograms, the reason",
    "start": "1753260",
    "end": "1760070"
  },
  {
    "text": "why you want these programs\nis because your generalization error bound will be something\nlike the generalization error",
    "start": "1760070",
    "end": "1773080"
  },
  {
    "text": "will be something like L theta,\nwill be less than L theta hat,",
    "start": "1773080",
    "end": "1782370"
  },
  {
    "text": "will be less than C\ntheta hat over gamma mean theta hat over square root\nof 10 plus low order terms,",
    "start": "1782370",
    "end": "1791530"
  },
  {
    "text": "right?  But this is using the general\nmachinery that we had, right?",
    "start": "1791530",
    "end": "1798179"
  },
  {
    "text": "So you have 1 over\nsquare root of 10. So you basically have the\nRademacher complexity.",
    "start": "1798180",
    "end": "1803889"
  },
  {
    "text": "This is the\nRademacher complexity. And sorry. I mean, this part is the\nRademacher complexity.",
    "start": "1803890",
    "end": "1811669"
  },
  {
    "text": "This corresponds to the\nRademacher complexity of H, right? And this is the margin.",
    "start": "1811670",
    "end": "1818570"
  },
  {
    "text": "So that's what we got\nfrom the margin theory.",
    "start": "1818570",
    "end": "1823899"
  },
  {
    "text": "[INAUDIBLE]  Is there any [INAUDIBLE]\ndifference between [INAUDIBLE]??",
    "start": "1823900",
    "end": "1832700"
  },
  {
    "text": "It just seems like [INAUDIBLE]. ",
    "start": "1832700",
    "end": "1845889"
  },
  {
    "text": "I think, depending on-- I think you are basically right.",
    "start": "1845890",
    "end": "1853330"
  },
  {
    "text": "But I think I would say\nthis is already something, we already achieved something.",
    "start": "1853330",
    "end": "1858340"
  },
  {
    "text": "Because this-- I think\nmaybe the right way to think about this is that you\ncompare this to in two things,",
    "start": "1858340",
    "end": "1870480"
  },
  {
    "text": "so in the very idealistic way. So for all the\nwj's are the same, all the Uj's are the\nsame, then these two",
    "start": "1870480",
    "end": "1876450"
  },
  {
    "text": "bounds are just the same. So then you are right. You are just voting-- you are just changing on form\nof a bound and nothing really",
    "start": "1876450",
    "end": "1882185"
  },
  {
    "text": "changed, right? But you somehow fold the\nsquare root n somewhere. But the thing is that\nthis is not tight always.",
    "start": "1882185",
    "end": "1890340"
  },
  {
    "text": "And you probably shouldn't\nexpect it to be tight. It shouldn't be the case that\nall the wj's and Uj's are",
    "start": "1890340",
    "end": "1895410"
  },
  {
    "text": "the same. You probably should\nhave a decaying wj. As you have more\nand more neurons, you're going to have\nsmaller and smaller wj.",
    "start": "1895410",
    "end": "1901963"
  },
  {
    "text": " [] There's no way that this\nis tight for all n, right?",
    "start": "1901963",
    "end": "1908280"
  },
  {
    "text": "So it can be tight for\nall the-- for one n. But if you had more neurons\nit wouldn't be tight.",
    "start": "1908280",
    "end": "1913410"
  },
  {
    "text": "So the typical thing\nwould be that as you have more and more neurons,\nthese neurons probably should",
    "start": "1913410",
    "end": "1920520"
  },
  {
    "text": "have smaller and smaller norm. Because they are capturing\nmore and more kind of complex subtleties in\nyour function, in your ground",
    "start": "1920520",
    "end": "1929100"
  },
  {
    "text": "choose function. So basically I'm saying that\nthis inequality wouldn't be tight in the idealist--\nin the ground truth function,",
    "start": "1929100",
    "end": "1938830"
  },
  {
    "text": "for example. ",
    "start": "1938830",
    "end": "1944049"
  },
  {
    "text": "Right. So yeah. But from a very\ntechnical point of view,",
    "start": "1944050",
    "end": "1949830"
  },
  {
    "text": "I think that's we only\ndid a very small trick to change the form. Yeah.",
    "start": "1949830",
    "end": "1955210"
  },
  {
    "text": "So this [INAUDIBLE] the other\nproblems [INAUDIBLE] that",
    "start": "1955210",
    "end": "1961270"
  },
  {
    "text": "[INAUDIBLE]? ",
    "start": "1961270",
    "end": "1967110"
  },
  {
    "text": "Yeah, I think you can say\nthat in some sense, yes. ",
    "start": "1967110",
    "end": "1973639"
  },
  {
    "text": "Or at least that the\nother bond would be-- yeah, I guess depending on\nhow you think about this.",
    "start": "1973640",
    "end": "1979020"
  },
  {
    "text": "Yeah. But I think the way\nI think about this that is really just--",
    "start": "1979020",
    "end": "1984270"
  },
  {
    "text": "the way I think about it\nis that these two bounds are exactly the same when all\nthe wj's and Uj's are the same.",
    "start": "1984270",
    "end": "1991210"
  },
  {
    "text": "They are all, for example,\nconstant or something like that. Or maybe all 1 over\nsquare root of n. So then you don't get\nanything from this, right?",
    "start": "1991210",
    "end": "1998850"
  },
  {
    "text": "So but it would\nbe much different if you want to find a function\nwhere your wj and Uj goes",
    "start": "1998850",
    "end": "2006260"
  },
  {
    "text": "to 0 gradually as you add\nmore and more neurons. OK? ",
    "start": "2006260",
    "end": "2017067"
  },
  {
    "text": "So going back to the\ntransition bound. So I think the transition\nbond in some sense motivates the use of this\nkind of max norm solution",
    "start": "2017067",
    "end": "2023082"
  },
  {
    "text": "or the minimum norm solution. Just because eventually\nyour Rademacher complexity depends on the\ncomplexity of the model.",
    "start": "2023082",
    "end": "2030980"
  },
  {
    "text": "And you also have the margin\nterm from the margin part, from the last part.",
    "start": "2030980",
    "end": "2037190"
  },
  {
    "text": "So and one of the\ninteresting thing is that this quantity, if you\nthink about this quantity,",
    "start": "2037190",
    "end": "2042770"
  },
  {
    "text": "and this quantity\nyou can show this is not increasing as\nn goes to infinity.",
    "start": "2042770",
    "end": "2049230"
  },
  {
    "text": "So and the reason is\nactually pretty simple. So but maybe let me\nwrite it down just to be clear about\nwhat I really mean.",
    "start": "2049230",
    "end": "2057949"
  },
  {
    "text": "So let's use, say, the\nhat n to be the minimizer",
    "start": "2057949",
    "end": "2066875"
  },
  {
    "text": "of, say, program one. So and n is to index which--\nhow many neurons you are using.",
    "start": "2066875",
    "end": "2076370"
  },
  {
    "text": "So for every n you\nhave a minimizer. And you can define\ngamma n star to be",
    "start": "2076370",
    "end": "2084919"
  },
  {
    "text": "the solution of the-- to be\nthe corresponding margin. You can define\ngamma n star to be the margin with the constraints\nthat C theta is less than hat.",
    "start": "2084920",
    "end": "2095750"
  },
  {
    "text": "So let's say define\nthis to become n star. So I think I want to\ndefine this to be true.",
    "start": "2095750",
    "end": "2105950"
  },
  {
    "text": "So let's mostly use\n2 as our main thing.",
    "start": "2105950",
    "end": "2111300"
  },
  {
    "text": "This is a little bit typo here. ",
    "start": "2111300",
    "end": "2117910"
  },
  {
    "text": "So suppose you solve\nthis problem 2, and you get this\nmaximizer solution. So and then, so\nyour bound-- so this",
    "start": "2117910",
    "end": "2125350"
  },
  {
    "text": "means that the bound is C theta\nhat n over gamma min theta hat",
    "start": "2125350",
    "end": "2136750"
  },
  {
    "text": "m over square root of n. And because we normalize\nthe C, the complexity to be 1, so this is really\n1 over gamma m star times",
    "start": "2136750",
    "end": "2145270"
  },
  {
    "text": "square root of n, right? This is the\ngeneralization bound. So basically whether this\nbond is better or not",
    "start": "2145270",
    "end": "2151210"
  },
  {
    "text": "depends on whether gamma n star\nis increasing or decreasing.",
    "start": "2151210",
    "end": "2157430"
  },
  {
    "text": "And interestingly, the\ngamma m star is increasing.",
    "start": "2157430",
    "end": "2162760"
  },
  {
    "start": "2162760",
    "end": "2168770"
  },
  {
    "text": "And this is in some sense\nalmost by definition. Why? This is because if you think\nabout what that-- the gamma",
    "start": "2168770",
    "end": "2175550"
  },
  {
    "text": "star, m star means, it means\nthat the maximum margin you can achieve when you\nrestrict your complexity to be less than 1, right?",
    "start": "2175550",
    "end": "2183339"
  },
  {
    "text": "and also use n neurons. And the thing is that when you\nhave more neurons, at least",
    "start": "2183340",
    "end": "2191480"
  },
  {
    "text": "you would achieve\nthe same margin. You shouldn't be worse. Just because the only-- so with more neurons,\nyou never get worse.",
    "start": "2191480",
    "end": "2199444"
  },
  {
    "text": " Can at least achieve\nthe same margin by adding just a dummy neuron\nas exactly the same argument",
    "start": "2199445",
    "end": "2208000"
  },
  {
    "text": "as I had.  At least achieve\nthe same margin.",
    "start": "2208000",
    "end": "2216235"
  },
  {
    "text": " Because you just\nadd a dummy neuron,",
    "start": "2216235",
    "end": "2221390"
  },
  {
    "text": "and it doesn't change\nthe functionality, it doesn't change\nthe complexity, it doesn't change the margin,\nit just everything is the same.",
    "start": "2221390",
    "end": "2227600"
  },
  {
    "text": "But having more neurons give\nyou additional flexibility. You could possibly change\nyour neurons a little bit more",
    "start": "2227600",
    "end": "2233930"
  },
  {
    "text": "cleverly instead of just\nadding a dummy neuron. That's why you margin-- adding one more neuron\nwill potentially",
    "start": "2233930",
    "end": "2239810"
  },
  {
    "text": "make your margin bigger. So at least, you never\nget in the margin smaller",
    "start": "2239810",
    "end": "2245420"
  },
  {
    "text": "by adding neurons. So that means that this bond can\ndecrease as n goes to infinity.",
    "start": "2245420",
    "end": "2254460"
  },
  {
    "text": "At least it's not increasing\nas n goes to infinity. So in some sense,\nthis is the nice thing",
    "start": "2254460",
    "end": "2261090"
  },
  {
    "text": "about this compared to\nother bonds, where you have explicit dependency on n. If you have an explicit\ndependency on n,",
    "start": "2261090",
    "end": "2266730"
  },
  {
    "text": "at least if you\njust look at it, you wouldn't be able to argue\nthat this bond is better.",
    "start": "2266730",
    "end": "2271859"
  },
  {
    "text": "So now you can say this bond is\nbetter as n going to infinity. Of course, this\ndoesn't really say--",
    "start": "2271860",
    "end": "2278730"
  },
  {
    "text": "this doesn't address\neverything, because this is just upper bound. It's not like you are saying\nthat the actual generalization",
    "start": "2278730",
    "end": "2285270"
  },
  {
    "text": "error is decreasing\nas n goes to infinity. That would be the ideal\ntheorem of the proof, ri ght?",
    "start": "2285270",
    "end": "2290430"
  },
  {
    "text": "That would match exactly the\nplots I showed last time, where you have more neurons and\nyour accuracy is improving",
    "start": "2290430",
    "end": "2299289"
  },
  {
    "text": "or your error is\ndecreasing, right? So here you're only talking\nabout bounds, right? So if the bound is\nloose, then it's",
    "start": "2299290",
    "end": "2306420"
  },
  {
    "text": "unclear whether this\ndecreasing in m thing is really a big deal.",
    "start": "2306420",
    "end": "2312570"
  },
  {
    "text": "And that's indeed true. So but I think this\nis a, in some sense, this is a starting point, right?",
    "start": "2312570",
    "end": "2317710"
  },
  {
    "text": "So if your bond is\nincreasing the m, that is completely useless. Your bound is decreasing\nm, that doesn't really",
    "start": "2317710",
    "end": "2323490"
  },
  {
    "text": "mean that it's super powerful. But at least that's a\ngood sign to have, right? That's a good thing to have.",
    "start": "2323490",
    "end": "2330250"
  },
  {
    "text": "And in some sense,\nit's really hard to capture the exact test error.",
    "start": "2330250",
    "end": "2338710"
  },
  {
    "text": "So if you really want to\nsay that the exact test error of the\ngeneralization error is decreasing in m, that's\nbasically the only thing you",
    "start": "2338710",
    "end": "2345480"
  },
  {
    "text": "can do is with linear model. At least so far, the\nonly technique I know is that you just\nliterally compute",
    "start": "2345480",
    "end": "2351630"
  },
  {
    "text": "exactly what the test error is. On linear model you can do\nthe analytical derivation using linear algebra\nto simplify them.",
    "start": "2351630",
    "end": "2358620"
  },
  {
    "text": "And in certain cases, you\ncan show indeed the error",
    "start": "2358620",
    "end": "2363660"
  },
  {
    "text": "is decreasing as n\ngoes to infinity. This is actually a\npretty popular direction",
    "start": "2363660",
    "end": "2373800"
  },
  {
    "text": "in the last few years. People have done this for\nvarious kind of linear models.",
    "start": "2373800",
    "end": "2379080"
  },
  {
    "text": "But basically, only\nrestricted to linear models. Right?  So here, we want to work\nwith neural network.",
    "start": "2379080",
    "end": "2385860"
  },
  {
    "text": "So we have to somehow live\nwith the weaker result. You only say that the\nbond is decreasing",
    "start": "2385860",
    "end": "2392099"
  },
  {
    "text": "but not actual\nerror is decreasing.  So I guess, the next\nthing I want to say",
    "start": "2392100",
    "end": "2399320"
  },
  {
    "text": "is that is actually-- another thing that this is\ndifferent from this program",
    "start": "2399320",
    "end": "2405467"
  },
  {
    "text": "well too, they are still\ndifferent from what you do in practice. You'll probably don't do\nexactly this complexity measure.",
    "start": "2405467",
    "end": "2411710"
  },
  {
    "text": "Nobody regularize it like that. Probably somebody tried. It probably wouldn't\nmake a difference.",
    "start": "2411710",
    "end": "2417230"
  },
  {
    "text": "And here, what I'm going to\nsay is that actually it's interesting that this\ncomplex measure is definitely",
    "start": "2417230",
    "end": "2425740"
  },
  {
    "text": "different from L2\ncomplex measure, right? But once you minimize\nthe complex measure,",
    "start": "2425740",
    "end": "2432430"
  },
  {
    "text": "you get the same effect\nas minimizing the L2. Or minimizing the L2 is the\nsame as minimizing this.",
    "start": "2432430",
    "end": "2438490"
  },
  {
    "text": "Maybe let me just clarify\nwhat does that mean. So basically, my main point here\nis that if you maximize margin,",
    "start": "2438490",
    "end": "2454599"
  },
  {
    "text": "sorry. You can-- so can be\ndone by minimizing",
    "start": "2454600",
    "end": "2461190"
  },
  {
    "text": "the cross-entropy loss, the\none with L2 regularization.",
    "start": "2461190",
    "end": "2472167"
  },
  {
    "text": "So here I have two things. One is I'm using\ncross-entropy loss. And the other is I'm\nusing L2 regularization. I'll do one of them as that.",
    "start": "2472167",
    "end": "2480119"
  },
  {
    "text": "So the first, I'm going\nto do first use L2 regularization instead of the\ncomplex measure I defined. And I'm going to say that it's\nactually doing the same thing.",
    "start": "2480120",
    "end": "2487500"
  },
  {
    "text": "So here is this first lemma. So suppose you consider the\none we have considered, right?",
    "start": "2487500",
    "end": "2495170"
  },
  {
    "text": "So let's call this J1,\nwhich is you minimize the complexity\nwith the constraint",
    "start": "2495170",
    "end": "2500450"
  },
  {
    "text": "that the margin\nis larger than 1. By the way, I keep\nchanging the-- sometimes",
    "start": "2500450",
    "end": "2506100"
  },
  {
    "text": "I'm minimizing the complexity\nwith the margin, sorry. And sometimes I'm minimizing\nthe margin with the complexity,",
    "start": "2506100",
    "end": "2511680"
  },
  {
    "text": "so that's the one. So somehow, I probably should\nmake them all consistent.",
    "start": "2511680",
    "end": "2516895"
  },
  {
    "text": "But just in my mind they\nare always the same. So sometimes I forgot to-- sorry. I should probably just keep\na single version of it.",
    "start": "2516895",
    "end": "2524730"
  },
  {
    "text": "But they are the same. They are just\nequivalents because--",
    "start": "2524730",
    "end": "2530400"
  },
  {
    "text": "yeah. So anyway, so here,\nI am minimizing the complexity with the\nmargins larger than 1.",
    "start": "2530400",
    "end": "2536670"
  },
  {
    "text": "And I'm claiming\nthat if you look at another one, which is\nyou minimize the L2 norm,",
    "start": "2536670",
    "end": "2544819"
  },
  {
    "text": "and with the constraint that\nthe margin is larger than 1.",
    "start": "2544820",
    "end": "2550250"
  },
  {
    "text": "So these two are the same. ",
    "start": "2550250",
    "end": "2557320"
  },
  {
    "text": "So obviously, these two\nfunctions are not the same. There's two complex\nmeasure are not the same. But if you minimize\nthe complexity,",
    "start": "2557320",
    "end": "2563190"
  },
  {
    "text": "the extreme point\nactually turns out to be the same, which\nis kind of interesting.",
    "start": "2563190",
    "end": "2569230"
  },
  {
    "text": "And the proof is like follows. So I think at least\none thing you know",
    "start": "2569230",
    "end": "2577880"
  },
  {
    "text": "is that the L2\nregularization, what is that?",
    "start": "2577880",
    "end": "2583849"
  },
  {
    "text": "This is L2 regularization is\nthe sum of the squares of all the parameters, which\nis sum of wj squared",
    "start": "2583850",
    "end": "2590480"
  },
  {
    "text": "plus sum of Uj 2 norm square. And you can show\nthat this is larger",
    "start": "2590480",
    "end": "2596880"
  },
  {
    "text": "than the complex\nmeasure we have defined, because you can use the am, gm. So you can say this is wj\nsquared plus Uj 2 norm squared.",
    "start": "2596880",
    "end": "2605540"
  },
  {
    "text": "And you use-- I think this is called AMG,\nI mean, inequality of--",
    "start": "2605540",
    "end": "2610980"
  },
  {
    "text": "for me, everything\nis Cauchy-Schwarz, so JC inequality. So anyway, so you get wj\ntimes U2 2 norm times 2.",
    "start": "2610980",
    "end": "2621320"
  },
  {
    "text": "And you cancel these 2. So this is B theta.",
    "start": "2621320",
    "end": "2626480"
  },
  {
    "text": "So you are minimizing--\nso in J2, the program J2, you are minimizing a\nlarger complex dimension.",
    "start": "2626480",
    "end": "2634470"
  },
  {
    "text": "But I guess the\nintuition is that even though you are minimizing\na larger complex measure, but when you--",
    "start": "2634470",
    "end": "2642380"
  },
  {
    "text": "the extremal point actually will\nmake these two things the same. So the intuition is that the\nextremal point should satisfy--",
    "start": "2642380",
    "end": "2653875"
  },
  {
    "start": "2653875",
    "end": "2660030"
  },
  {
    "text": "should satisfy wj\nis equal to Uj even when you are minimizing the\nL2 regularization, right?",
    "start": "2660030",
    "end": "2667690"
  },
  {
    "text": "So and if that's the--\nsuppose that's the case, then you can believe that\nthese two things are the same. Because when I'm minimizing\nthe L2 regularization,",
    "start": "2667690",
    "end": "2674980"
  },
  {
    "text": "if the extremal point\nis-- satisfies this, then for this case, if this\nis true, then C theta",
    "start": "2674980",
    "end": "2680290"
  },
  {
    "text": "is the same as the L2. So then, you are not really\ndoing anything different.",
    "start": "2680290",
    "end": "2685970"
  },
  {
    "text": "So that's kind of the intuition. If you really want to prove\nthis kind of formally, I guess the simplest way to\nprove this is the following.",
    "start": "2685970",
    "end": "2694400"
  },
  {
    "text": "So you say that, I\nguess this implies",
    "start": "2694400",
    "end": "2700299"
  },
  {
    "text": "that J2 is larger than J1. And you want to\nuse the intuition to show that J1 is\nbigger than J2 instead--",
    "start": "2700300",
    "end": "2708640"
  },
  {
    "text": "it's larger than J2 as well. So what we do is that\nwe say, let theta",
    "start": "2708640",
    "end": "2714526"
  },
  {
    "text": "be the minimizer of 1, of J1.",
    "start": "2714526",
    "end": "2723420"
  },
  {
    "text": "Maybe let's call this-- I think let's call this\nmaybe 3 and this is 4.",
    "start": "2723420",
    "end": "2732599"
  },
  {
    "text": "So is that a good number? That's probably\nnot a good number. Let's call this P1 and P2.",
    "start": "2732600",
    "end": "2741940"
  },
  {
    "text": "So minimizer of P1. And then, what you\ndo is you construct.",
    "start": "2741940",
    "end": "2748050"
  },
  {
    "text": "So you get a theta that is the\nminimizer of the first one. And you want to construct\na theta prime which",
    "start": "2748050",
    "end": "2754069"
  },
  {
    "text": "is very good on the fact-- in\nterms of the second program.",
    "start": "2754070",
    "end": "2762380"
  },
  {
    "text": "So you construct a theta prime. And what you do is\nthat you say, I'm going to take wj prime to be\nthe renormalized version of wj.",
    "start": "2762380",
    "end": "2775730"
  },
  {
    "text": "And Uj prime again, to be the\nrenormalized version of Uj. ",
    "start": "2775730",
    "end": "2785550"
  },
  {
    "text": "And then, you can verify that\nbecause I'm just changing scaling, Uj times phi of wj\ntranspose x-- is the same as--",
    "start": "2785550",
    "end": "2796520"
  },
  {
    "text": "sorry. wj times v\nof Uj transposed are actually the same as before. ",
    "start": "2796520",
    "end": "2805360"
  },
  {
    "text": "And also, wj in terms of\nthe complexity measure, they are also the same after\ndoing this transformation.",
    "start": "2805360",
    "end": "2814530"
  },
  {
    "start": "2814530",
    "end": "2819760"
  },
  {
    "text": "And this means that C theta\nis the same as C theta prime.",
    "start": "2819760",
    "end": "2825910"
  },
  {
    "text": "And f theta is the\nsame as f theta prime. So the functionality and\nthe complexity measure",
    "start": "2825910",
    "end": "2831280"
  },
  {
    "text": "didn't change. And what's interesting\nis that for theta prime,",
    "start": "2831280",
    "end": "2837100"
  },
  {
    "text": "C theta prime is also\nequal to the L2 norm.",
    "start": "2837100",
    "end": "2843654"
  },
  {
    "text": "Because my construction-- OK-- why I'm doing this construction? I'm doing this construction\nbecause I wanted wj it",
    "start": "2843654",
    "end": "2852350"
  },
  {
    "text": "to be equal to the norm of Uj. This is why I\nchose this scaling.",
    "start": "2852350",
    "end": "2858410"
  },
  {
    "text": "Anyway, I think this\nshould be like this. ",
    "start": "2858410",
    "end": "2867320"
  },
  {
    "text": "Oh, sorry. No. Am I right? Oh, no. It's like this. So we can verify wj is\nthe same as Uj 2, this.",
    "start": "2867320",
    "end": "2877780"
  },
  {
    "text": "So because this is actually\nmy design in some sense.",
    "start": "2877780",
    "end": "2882880"
  },
  {
    "text": "You can verify this. But this is-- if this is true,\nI should change my designs to make it true.",
    "start": "2882880",
    "end": "2888640"
  },
  {
    "text": "But that's the point. So that means that--",
    "start": "2888640",
    "end": "2894009"
  },
  {
    "text": "so what does this mean? This means that theta prime\nsatisfies constraint of--",
    "start": "2894010",
    "end": "2906550"
  },
  {
    "text": "so all of this means that\ntheta prime constrains of p2. So that means that C\ntheta prime is less than--",
    "start": "2906550",
    "end": "2917023"
  },
  {
    "text": "or is bigger than J2, right? ",
    "start": "2917023",
    "end": "2923069"
  },
  {
    "text": "And C theta prime is equal\nto, n theta to the prime",
    "start": "2923070",
    "end": "2934310"
  },
  {
    "text": "is equals 1/2 theta prime\nsquared, is equal to 1/2-- ",
    "start": "2934310",
    "end": "2942850"
  },
  {
    "text": "sorry. C theta prime is equals to--",
    "start": "2942850",
    "end": "2949260"
  },
  {
    "text": "OK. What is this equal to? This is equal to--",
    "start": "2949260",
    "end": "2954730"
  },
  {
    "text": "let's see what's going on here. Then I want to show that theta\nthe prime is equal to J1.",
    "start": "2954730",
    "end": "2962080"
  },
  {
    "text": "This is because-- all right.",
    "start": "2962080",
    "end": "2971260"
  },
  {
    "text": "This is just because the problem\nis equal to C theta, which is equal to J1, OK?",
    "start": "2971260",
    "end": "2978297"
  },
  {
    "text": "I didn't change the\ncomplexity measure because I'm just rescaling. So that's J1 is bigger than J2.",
    "start": "2978297",
    "end": "2985990"
  },
  {
    "text": "And before you got\nJ2 is bigger than J1. So that's why J2\nand J1 are the same.",
    "start": "2985990",
    "end": "2993350"
  },
  {
    "text": "Yeah. Actually I was a little\nhesitating whether I should show this proof\nor the more intuitive--",
    "start": "2993350",
    "end": "2999807"
  },
  {
    "text": "another version which is\nactually in the lecture notes. In the lecture notes there's\na-- it's a different way to prove the same thing.",
    "start": "2999808",
    "end": "3005910"
  },
  {
    "text": "At the end of it, everything\nis relatively simple. It's nothing really hard. So this proof is\nvery easy to verify.",
    "start": "3005910",
    "end": "3012810"
  },
  {
    "text": "And the other proof is kind\nof in some sense carries the intuition. And intuition is\nreally just what I said, at the extremal\npoint anyway wj",
    "start": "3012810",
    "end": "3021450"
  },
  {
    "text": "and Uj 2 norm has to be the\nsame so these two complexity measures are not different. So that's the manual intuition.",
    "start": "3021450",
    "end": "3029566"
  },
  {
    "text": "[INAUDIBLE] ",
    "start": "3029566",
    "end": "3035620"
  },
  {
    "text": "Theta prime satisfies\nthe constraint p2. So the constraint is only\nabout the margin, right?",
    "start": "3035620",
    "end": "3040660"
  },
  {
    "text": "So the margin is only about the\nfunctionality of this model, right? So if you predict\nthe same thing,",
    "start": "3040660",
    "end": "3047140"
  },
  {
    "text": "your margin will\nbe the same, right? So theta prime and theta\nhave the same functionality",
    "start": "3047140",
    "end": "3053570"
  },
  {
    "text": "because you only rebalance\nthe scale, right? You just multiply wj by\nsomething and divide Uj",
    "start": "3053570",
    "end": "3062200"
  },
  {
    "text": "by something else. So the functionality\nis maintained. So that's why the\nmargin is the same. ",
    "start": "3062200",
    "end": "3070290"
  },
  {
    "text": "In the first order proof\nproof, why is [INAUDIBLE]??",
    "start": "3070290",
    "end": "3075660"
  },
  {
    "text": "In the why there is no-- In the first order proof\nwhen you just-- you pull out the sum? Here? When you need it?",
    "start": "3075660",
    "end": "3081248"
  },
  {
    "text": "Yeah. So here, this is the equality? No, the line below it.",
    "start": "3081248",
    "end": "3087809"
  },
  {
    "text": "Oh, sorry. Sorry, sorry, sorry. This, not that. Why this is equality\nand this is inequality. I got it.",
    "start": "3087810",
    "end": "3093230"
  },
  {
    "text": " OK, cool.",
    "start": "3093230",
    "end": "3098480"
  },
  {
    "text": " Great.",
    "start": "3098480",
    "end": "3105119"
  },
  {
    "text": "So the first thing,\nso the first lemma we have shown,\nwhat we have done. We basically are saying that\nif you minimize the L2 norm,",
    "start": "3105120",
    "end": "3110615"
  },
  {
    "text": "it's the same as minimizing\nthis complex measure, OK? And we also wanted to\ndo the cross-entropy.",
    "start": "3110615",
    "end": "3118390"
  },
  {
    "text": "And this is something I\nam not going to prove. But I'm just going\nto state the lemma. And if you're interested, you\ncan read a paper about it.",
    "start": "3118390",
    "end": "3127423"
  },
  {
    "text": "The proof is actually\nrelatively simple. But I think we won't probably\nhave time today to do that.",
    "start": "3127423",
    "end": "3135150"
  },
  {
    "text": "So the lemma 2 is\nthat if you consider a regularized cross\nentropy loss, and something",
    "start": "3135150",
    "end": "3151730"
  },
  {
    "text": "like L hat lambda theta, which\nis equal to 1 over n times--",
    "start": "3151730",
    "end": "3157710"
  },
  {
    "text": "I guess in this lecture this\nis the first time I have ever talked about what is\ncross-entropy loss. But I assume that you somewhat\nknow what they are, right?",
    "start": "3157710",
    "end": "3165600"
  },
  {
    "text": "This is the loss for\nlogistic regression where you have yi\ntimes f theta xi.",
    "start": "3165600",
    "end": "3174380"
  },
  {
    "text": "So this is the input. And the loss is the log of 1. So the loss in some\nsense is really log",
    "start": "3174380",
    "end": "3180609"
  },
  {
    "text": "1 plus exponential minus t. This is the logistic loss. And you add some lambda\ntimes L2 regularization.",
    "start": "3180610",
    "end": "3193190"
  },
  {
    "text": "Suppose you do this. And let's say, let theta\nlambda hat be the minimizer.",
    "start": "3193190",
    "end": "3201270"
  },
  {
    "text": " I'm going to claim that\nfor small enough lambda",
    "start": "3201270",
    "end": "3206570"
  },
  {
    "text": "theta hat lambda is basically\ndoing the same thing as the max margin solution.",
    "start": "3206570",
    "end": "3211910"
  },
  {
    "text": "But there is a\nsmall thing that I have to deal with, which is\nthat what is the norm, right?",
    "start": "3211910",
    "end": "3217789"
  },
  {
    "text": "Because the max\nmargin thing is-- you need a norm. You need to basically--\nyou need to cover",
    "start": "3217790",
    "end": "3223100"
  },
  {
    "text": "the ratio between the\nmargin and the norm. So that's why my\nstatement is that--",
    "start": "3223100",
    "end": "3228350"
  },
  {
    "start": "3228350",
    "end": "3245770"
  },
  {
    "text": "again, I don't know why-- OK, here. So then, my statement\nis something about this.",
    "start": "3245770",
    "end": "3252559"
  },
  {
    "text": "It's like this. So basically, you can\nsay that if lambda goes to 0 for small\nenough lambda,",
    "start": "3252560",
    "end": "3258740"
  },
  {
    "text": "then the norm versus the\nmargin will go to J1.",
    "start": "3258740",
    "end": "3270205"
  },
  {
    "text": "J1, which was defined\nto be the max--",
    "start": "3270205",
    "end": "3276635"
  },
  {
    "text": "the minimum norm\nsolution, right?  This is just I'm\nrecalling the definition.",
    "start": "3276635",
    "end": "3287910"
  },
  {
    "text": "So basically, you are converging\nto the max margin solution, or the minimum norm\nsolution up to a scaling.",
    "start": "3287910",
    "end": "3293319"
  },
  {
    "text": "Because you are\nlooking at a ratio. So this, when you have a\nvery small theta, you--",
    "start": "3293320",
    "end": "3302340"
  },
  {
    "text": "sorry. When you have this\nvery small lambda, your normal theta would be\nsomething actually pretty big. That's because your\norganization is too weak.",
    "start": "3302340",
    "end": "3308855"
  },
  {
    "text": "So you are not going to\nget very big norm solution. But if you normalize the\nnorm with the margin, then you found that this is\nactually the max norm solution.",
    "start": "3308855",
    "end": "3317510"
  },
  {
    "text": "I'm not going to prove this. I guess if you're\ninterested, this is a theorem 4.2 of a paper I\nwrote with two collaborators.",
    "start": "3317510",
    "end": "3330420"
  },
  {
    "text": "And actually, this theorem\nis actually very simple. ",
    "start": "3330420",
    "end": "3335579"
  },
  {
    "text": "And actually, it works for not\nonly the L2 regularization, it works for almost all\nhomogeneous-- or almost",
    "start": "3335580",
    "end": "3342329"
  },
  {
    "text": "all regularizations\nyou can think of. So the gist is basically saying\nthat if you care about the max",
    "start": "3342330",
    "end": "3348930"
  },
  {
    "text": "margin solution with respect\nto certain complex measure, so the complex\nmeasure could be L2.",
    "start": "3348930",
    "end": "3354510"
  },
  {
    "text": "In this case, it could\nbe something else. Like here, it could\nbe anything, right? So one way to achieve\nit is that you just",
    "start": "3354510",
    "end": "3361650"
  },
  {
    "text": "add a very weak regularization\nin the cross-entropy loss. And that will give you\na max margin solution.",
    "start": "3361650",
    "end": "3367297"
  },
  {
    "text": "OK. ",
    "start": "3367297",
    "end": "3377850"
  },
  {
    "text": "Any questions? ",
    "start": "3377850",
    "end": "3382859"
  },
  {
    "text": "[INAUDIBLE]  Yeah. [INAUDIBLE] Yeah.",
    "start": "3382860",
    "end": "3387940"
  },
  {
    "text": "So the general kind of\ngist is that suppose you care about the max\nmargin solution, right? But max margin solution requires\na complex measure, right?",
    "start": "3387940",
    "end": "3395610"
  },
  {
    "text": "You need to say I'm\nminimizing the norm-- such a norm with the\nmargins larger than 1, or I'm maximizing the margin\nwith some constraints, right?",
    "start": "3395610",
    "end": "3402160"
  },
  {
    "text": "There's a norm or there's\na complex measure. So if you want to get\na max margin solution, you just put a complex\nmeasure in the--",
    "start": "3402160",
    "end": "3409472"
  },
  {
    "text": "at here, right, so and\nwith a small enough lambda.",
    "start": "3409472",
    "end": "3414849"
  },
  {
    "text": "Then you have a\ncross-entropy loss. And then the solution, this way\nwill give you the max margin",
    "start": "3414850",
    "end": "3420930"
  },
  {
    "text": "solution. Of course, you can look\nfor the max margin solution just directly by\nsolving the program.",
    "start": "3420930",
    "end": "3427430"
  },
  {
    "text": "But you can also do it this way. And this is something that\nseems to be more typical.",
    "start": "3427430",
    "end": "3435040"
  },
  {
    "text": "At least, this is what people\ndo empirically all the time in some sense, right? In some sense, this\nis just linking",
    "start": "3435040",
    "end": "3441640"
  },
  {
    "text": "what people do empirically with\nthe max margin solution, which is not what people do, not\ntypically in deep learning.",
    "start": "3441640",
    "end": "3448190"
  },
  {
    "text": " But there is a-- but the caveat here, you\ncare about the broader",
    "start": "3448190",
    "end": "3456203"
  },
  {
    "text": "interpretation,\nthe caveat here is you need the\nlongitude very small. So basically, this\nis saying that if you use a very small lambda, you\nget a maximum resolution.",
    "start": "3456203",
    "end": "3463140"
  },
  {
    "text": "But empirically, actually, you\ndon't use that small lambda. You actually use\nsomething bigger",
    "start": "3463140",
    "end": "3469530"
  },
  {
    "text": "than this infinitesimal\nsmall lambda. So empirically, you\nprobably wouldn't get exactly maximum resolution.",
    "start": "3469530",
    "end": "3475860"
  },
  {
    "text": "You're going to get something\nsimilar to it, but not exactly the same. And actually, it's\nkind of interesting",
    "start": "3475860",
    "end": "3482280"
  },
  {
    "text": "that, I guess,\nprobably for CS 239, you have learned a\nmax margin solution.",
    "start": "3482280",
    "end": "3488790"
  },
  {
    "text": "So it sounds like before\ndeep learning that's the right thing to do, right?",
    "start": "3488790",
    "end": "3494190"
  },
  {
    "text": "But even if you look at\nlinear models, it's never-- at least I haven't seen.",
    "start": "3494190",
    "end": "3500190"
  },
  {
    "text": "I'm not a practitioner. I do a lot of theory. But when I do\nexperiments, I've never",
    "start": "3500190",
    "end": "3505200"
  },
  {
    "text": "seen that max margin solution\nis the best for linear model. Somehow it's like when you\nuse a very small lambda,",
    "start": "3505200",
    "end": "3512918"
  },
  {
    "text": "you do get max margin solution. But if you use bigger lambda,\nsometimes it's a little better. So I think max margin\nsolution in some sense",
    "start": "3512918",
    "end": "3519750"
  },
  {
    "text": "is just a theoretical\napproximation of what people really do in practice. All right.",
    "start": "3519750",
    "end": "3525165"
  },
  {
    "start": "3525165",
    "end": "3531380"
  },
  {
    "text": "So let me see.  So the next part I'm trying\nto connect this deep learning",
    "start": "3531380",
    "end": "3539599"
  },
  {
    "text": "thing, this deep\nlearning not very deep, like two-layer network thing\nwith the so-called L1 SVM.",
    "start": "3539600",
    "end": "3545550"
  },
  {
    "start": "3545550",
    "end": "3550690"
  },
  {
    "text": "This is also kind of like-- I think people-- the exact\nthing is in my paper as well.",
    "start": "3550690",
    "end": "3559000"
  },
  {
    "text": "But it's only three\nparagraphs in the appendix. And we are not\nreally inventing it.",
    "start": "3559000",
    "end": "3565869"
  },
  {
    "text": "We just in some\nsense said something that people already\nknew implicitly.",
    "start": "3565870",
    "end": "3571015"
  },
  {
    "text": "We thought that it's\nuseful to write it down. ",
    "start": "3571015",
    "end": "3576400"
  },
  {
    "text": "So the general thing is\nthat you want to say-- what we claim is the following. So we want to claim that\nthis is-- we are doing--",
    "start": "3576400",
    "end": "3583505"
  },
  {
    "text": "what neural network is doing\nwith this two-layer network, and the max margin\nsolution, it's really just doing\nsomething like L1 SVM",
    "start": "3583505",
    "end": "3590740"
  },
  {
    "text": "in some kind of feature space. So but let me explain.",
    "start": "3590740",
    "end": "3598180"
  },
  {
    "text": "I guess I haven't\ndefined what L1 SVM is. So you're probably\nfamiliar with SVM. That's the so-called\nL2 version of the SVM.",
    "start": "3598180",
    "end": "3604490"
  },
  {
    "text": "So here you are going to have\na slightly different version of SVM. So the idea is that\nfirst of all, let's look",
    "start": "3604490",
    "end": "3614830"
  },
  {
    "text": "at infinite number of neurons. Because we have claimed that\nmore neurons is always better.",
    "start": "3614830",
    "end": "3620230"
  },
  {
    "text": "So why not think about\ninfinite number of neurons? And let's see what\ninfinite number of neurons will do for us, right?",
    "start": "3620230",
    "end": "3626190"
  },
  {
    "text": "So you look at the\nmax margin, where you have infinite\nnumber of neurons, this is the largest\npossible margin you can achieve with even\ninfinite number of neurons.",
    "start": "3626190",
    "end": "3634650"
  },
  {
    "text": "And suppose that\nthis is achieved",
    "start": "3634650",
    "end": "3642930"
  },
  {
    "text": "by U1, so on, so forth.",
    "start": "3642930",
    "end": "3650010"
  },
  {
    "text": "You need infinite number\nof neurons, probably. So many neurons. Actually, you can achieve this\nwithout an infinite number",
    "start": "3650010",
    "end": "3657849"
  },
  {
    "text": "of neurons. You can achieve it with,\nI think n plus 1 neurons, any number of data points.",
    "start": "3657850",
    "end": "3663850"
  },
  {
    "text": "So but let's say you have\ninfinite number of neurons. Just like basically,\ninfinite is not",
    "start": "3663850",
    "end": "3669510"
  },
  {
    "text": "very different from\nn plus 1 neurons. As long as you have more\nthan n plus 1 neurons, you don't really get\nanything more from this.",
    "start": "3669510",
    "end": "3676800"
  },
  {
    "text": "So and again, U bar is\nthe normalization of U.",
    "start": "3676800",
    "end": "3685080"
  },
  {
    "text": "So I think we have kind\nof played with this a lot of times. This is equivalent to\nw1 U1 2 norm, U1 bar,",
    "start": "3685080",
    "end": "3698237"
  },
  {
    "text": "so on and so forth, right? ",
    "start": "3698237",
    "end": "3707670"
  },
  {
    "text": "This is what we have. Let's call this theta-- ",
    "start": "3707670",
    "end": "3713073"
  },
  {
    "text": "I think I'll call it\ntheta tilde in this case, and I call this one w1 tilde.",
    "start": "3713073",
    "end": "3718200"
  },
  {
    "text": "So we have then this\nrescaling a lot of times. And we know that if\nyou rescale this,",
    "start": "3718200",
    "end": "3723690"
  },
  {
    "text": "you don't change\nthe complex measure. And then here, the complex\nmeasure is the wj Uj 2 norm.",
    "start": "3723690",
    "end": "3731540"
  },
  {
    "text": "And this is just the wj\ntilde absolute value. So this is the 1 norm of wj--",
    "start": "3731540",
    "end": "3738580"
  },
  {
    "text": "w tilde. So that's where the 1\nnorm comes into play. So basically, the\nidea is that after you",
    "start": "3738580",
    "end": "3746160"
  },
  {
    "text": "change this viewpoint,\nbasically you just view w1 and w1 tilde\nas the variable.",
    "start": "3746160",
    "end": "3751920"
  },
  {
    "text": "And then, you are\ndoing some kind of sparse linear\nregression or sparse SVM.",
    "start": "3751920",
    "end": "3758609"
  },
  {
    "text": "So formally, what you can\ndo is that you can pretend",
    "start": "3758610",
    "end": "3768470"
  },
  {
    "text": "so every U in the sphere--",
    "start": "3768470",
    "end": "3773859"
  },
  {
    "text": "on the sphere Sd minus 1 is\nthe d-dimensional sphere. ",
    "start": "3773860",
    "end": "3782830"
  },
  {
    "text": "So you pretend every\nU in the sphere shows up in the\ncollection of U bars.",
    "start": "3782830",
    "end": "3790690"
  },
  {
    "text": " Once you pretend that every U--",
    "start": "3790690",
    "end": "3797299"
  },
  {
    "text": "why this is possible? This is just because adding more\nneurons is never a bad thing. But you add a lot of neurons\nto the Ui's, and you just",
    "start": "3797300",
    "end": "3805590"
  },
  {
    "text": "set those corresponding-- this is just because you\ncan add neurons at Uj and 0.",
    "start": "3805590",
    "end": "3814470"
  },
  {
    "text": "You just add this things,\nit never changes anything. If you don't see any\nneurons in this collection,",
    "start": "3814470",
    "end": "3821680"
  },
  {
    "text": "you just add this neuron\ninto the collection. And you add 0 as\nthe coefficient.",
    "start": "3821680",
    "end": "3826880"
  },
  {
    "text": "It doesn't change\nthe functionality. It doesn't change\nthe complex measure. So that's why you can pretend\nthat the collection of u1",
    "start": "3826880",
    "end": "3833790"
  },
  {
    "text": "up to Un, and I guess\nthere is also more-- you have infinite of this.",
    "start": "3833790",
    "end": "3840980"
  },
  {
    "text": "It's really just a collection\nof all the possible unit norm vector on the sphere.",
    "start": "3840980",
    "end": "3847190"
  },
  {
    "text": "That doesn't really\nchange anything. And once you have that, then-- so once you pretend\nthat this Uj bar is just",
    "start": "3847190",
    "end": "3855920"
  },
  {
    "text": "equal to Sd minus\n1, then you can take a continuous perspective. You can say that this f\ntheta tilde x is really",
    "start": "3855920",
    "end": "3864079"
  },
  {
    "text": "something like sum of-- if you write this discrete\nversion, you get this.",
    "start": "3864080",
    "end": "3869280"
  },
  {
    "text": "But you can say this is-- you can think of this as a\ncontinuous version, where",
    "start": "3869280",
    "end": "3876150"
  },
  {
    "text": "for every U bar you have a w. And you are integrating\nover all the U bar.",
    "start": "3876150",
    "end": "3881339"
  },
  {
    "start": "3881340",
    "end": "3887600"
  },
  {
    "text": "I'm not sure whether\nthis make any sense. This is the simplest\nway that I came up with",
    "start": "3887600",
    "end": "3894350"
  },
  {
    "text": "to define this without\ntalking about what's of--",
    "start": "3894350",
    "end": "3901151"
  },
  {
    "text": "I think this is one way that\nI came up to explain this without too much jargon. But I, of course, I\ndon't know whether this",
    "start": "3901151",
    "end": "3908180"
  },
  {
    "text": "works for everyone. Again, in the\nlecture notes, there is a slightly different way\nto introduce this, which",
    "start": "3908180",
    "end": "3915410"
  },
  {
    "text": "requires a bit more jargon. I don't know. Any questions? ",
    "start": "3915410",
    "end": "3922420"
  },
  {
    "text": "[INAUDIBLE] Sorry. My bad. Why did I write sigma here? Phi.",
    "start": "3922420",
    "end": "3929410"
  },
  {
    "text": "Yeah. All right, sorry. OK. ",
    "start": "3929410",
    "end": "3943043"
  },
  {
    "text": "Yeah, feel free. So [INAUDIBLE] number\nof neurons [INAUDIBLE]??",
    "start": "3943043",
    "end": "3952589"
  },
  {
    "text": "Right, OK, yeah. Can I have uncountable-- So the question is whether\nwe can have uncountable for the number of neurons?",
    "start": "3952590",
    "end": "3957600"
  },
  {
    "start": "3957600",
    "end": "3962910"
  },
  {
    "text": "Here, this is not super-- this is just a concept, for\nexample, the same question--",
    "start": "3962910",
    "end": "3972010"
  },
  {
    "text": "you could ask the same question\nabout why you can-- when we define the\nintegral, actually,",
    "start": "3972010",
    "end": "3977500"
  },
  {
    "text": "you are using the countable\nnumber of discretizations. And you take the limit,\nand you can still get something uncountable.",
    "start": "3977500",
    "end": "3983470"
  },
  {
    "text": "So this is kind\nof the same thing. And also, in some sense,\neventually this thing--",
    "start": "3983470",
    "end": "3991079"
  },
  {
    "text": "it's just a language,\nin some sense. It's not really like you\nimplement the integral in practice.",
    "start": "3991080",
    "end": "3999060"
  },
  {
    "text": "So yeah. That make some sense? OK. So and basically, I\nthink if you see this,",
    "start": "3999060",
    "end": "4007635"
  },
  {
    "text": "this is kind of like--\nwhat you can-- the way you can view this is that\nyou can say this is w tilde inner product with a phi.",
    "start": "4007635",
    "end": "4014270"
  },
  {
    "text": "And a phi acts as a\nuniversal feature map.",
    "start": "4014270",
    "end": "4020002"
  },
  {
    "text": "So basically, you think\nof this-- each of these is a feature. This is a feature. And this is like the coefficient\nin front of the feature.",
    "start": "4020002",
    "end": "4029560"
  },
  {
    "text": "And this feature is-- now the feature is-- the\ndifference between all--",
    "start": "4029560",
    "end": "4035613"
  },
  {
    "text": "the difference here\nis that this feature is a predefined feature. It's no longer\nsomething learned.",
    "start": "4035613",
    "end": "4041150"
  },
  {
    "text": "Because you have all\nthe possible U bar in this world in\nyour feature set. So basically, you can view\nthis phi acts as really just",
    "start": "4041150",
    "end": "4050960"
  },
  {
    "text": "this gigantic feature--  phi feature vector, where you\nhave all the possible U bars",
    "start": "4050960",
    "end": "4059245"
  },
  {
    "text": "in your feature set. So and w tilde is\nthe coefficient",
    "start": "4059245",
    "end": "4064660"
  },
  {
    "text": "in front of feature. So basically, if you take\nC as doing that, and this",
    "start": "4064660",
    "end": "4070930"
  },
  {
    "text": "is the feature in\nthe kernel part, and this is the theta,\nor the weight vector,",
    "start": "4070930",
    "end": "4077350"
  },
  {
    "text": "or the parameters in\nfront of features. So now it's a linear\nfunction in the features.",
    "start": "4077350",
    "end": "4082744"
  },
  {
    "text": " So but the thing is that the\ncomplexity measure corresponds",
    "start": "4082745",
    "end": "4090280"
  },
  {
    "text": "to, as we argued, corresponds to\nw tilde 1 norm but not 2 norm. So this is why the max\nmargin with the C theta",
    "start": "4090280",
    "end": "4105560"
  },
  {
    "text": "is less than 1\ncorresponds to max margin",
    "start": "4105560",
    "end": "4111799"
  },
  {
    "text": "with this L1 norm max margin. So basically, you\ncan think of this",
    "start": "4111800",
    "end": "4118109"
  },
  {
    "text": "as the-- so the\ncorresponding questions you are maximizing the margin. The margin is the same.",
    "start": "4118109",
    "end": "4123729"
  },
  {
    "text": "So w tilde phi xi, take the mean\nover i, and max over w tilde.",
    "start": "4123729",
    "end": "4131299"
  },
  {
    "text": "And then with the constraint\nthat the 1 norm is less than 1. And this is called L1\nSVM with feature phi.",
    "start": "4131300",
    "end": "4142715"
  },
  {
    "text": " So and the difference\nfrom the SVM you learned in the,\nfor example, CS 239",
    "start": "4142716",
    "end": "4149339"
  },
  {
    "text": "would be that this is\n1 norm but not 2 norm. So it's not doing just\na simple kernel SVM.",
    "start": "4149340",
    "end": "4157318"
  },
  {
    "text": "It's doing something\ndifferent from that. And the interesting thing is\nthat the L1SVM is actually not",
    "start": "4157319",
    "end": "4164759"
  },
  {
    "text": "implementable with infinite\nnumber of features. ",
    "start": "4164760",
    "end": "4175290"
  },
  {
    "text": "It's not implementable. So when you take CS\n239, one of the message",
    "start": "4175290",
    "end": "4181469"
  },
  {
    "text": "we had is that when you\nuse the kernel trick, you can actually work with\neven dimensional feature.",
    "start": "4181470",
    "end": "4187889"
  },
  {
    "text": "Because you can change-- everything depends on the\nkernel, the inner product of the feature.",
    "start": "4187890",
    "end": "4193180"
  },
  {
    "text": "So you don't really care about\ndimensionality of the features. So you can work with\ninfinite dimensional feature. But here, you don't have\nthat kernel trick anymore.",
    "start": "4193180",
    "end": "4200500"
  },
  {
    "text": "So if you have\nthe L1 constraint, the kernel trick doesn't apply. The final solution is not just\na function of the inner product",
    "start": "4200500",
    "end": "4208290"
  },
  {
    "text": "of the features. So you cannot apply\nthe kernel trick. So that's why you cannot\nimplement it with the kernel",
    "start": "4208290",
    "end": "4219840"
  },
  {
    "text": "trick. So this part is purely\nfor understanding. This is saying that,\nOK, neural network",
    "start": "4219840",
    "end": "4225990"
  },
  {
    "text": "is doing something more than\nwhat you can do with kernel. Because now you are\neffectively doing a L1 version of the kernel\nproblem, which was not",
    "start": "4225990",
    "end": "4234239"
  },
  {
    "text": "able to do-- which\nis something you are not able to do with\nthe standard kernel trick. You have to use\nthe neural network",
    "start": "4234240",
    "end": "4240237"
  },
  {
    "text": "to achieve the same thing.  So did we prove that l1 SVM\nis not implementable or is",
    "start": "4240237",
    "end": "4248285"
  },
  {
    "text": "that just sort of an effect? Yeah, we didn't prove that\nL1 SVM is not implementable.",
    "start": "4248286",
    "end": "4254320"
  },
  {
    "text": "But I think the-- how do I say this? ",
    "start": "4254320",
    "end": "4259590"
  },
  {
    "text": "I guess, how do you prove\nthat it's not implementable? You have to have a--",
    "start": "4259590",
    "end": "4265170"
  },
  {
    "text": "you have to say what do\nyou mean by implementation. So this is just really\njust-- we don't know. Maybe the easiest\nway to say is just,",
    "start": "4265170",
    "end": "4270582"
  },
  {
    "text": "we don't know how\nto implement it. But it sounds like very\nunlikely to be able to be done.",
    "start": "4270582",
    "end": "4276780"
  },
  {
    "text": " Also on the other side, on the\nflip side, for neural networks",
    "start": "4276780",
    "end": "4283360"
  },
  {
    "text": "we are saying that\nyou can implement it. So basically, here is saying\nthat you can effectively",
    "start": "4283360",
    "end": "4288969"
  },
  {
    "text": "use neural network to\nimplement this L1 SVM. But the caveat is that you\nstill don't know whether you",
    "start": "4288970",
    "end": "4295000"
  },
  {
    "text": "can optimize the network. So it's not an\nend to end result. It's saying that if you\nassume you can optimize your neural network efficiently\nand up to global minimum,",
    "start": "4295000",
    "end": "4304240"
  },
  {
    "text": "then you can solve the L1 SVM. But there is a caveat about\nwhether you can really computationally\nsolve neural network.",
    "start": "4304240",
    "end": "4311800"
  },
  {
    "text": "That's something we\ndon't know how to do. We don't know how to\nprove theoretically.",
    "start": "4311800",
    "end": "4317730"
  },
  {
    "text": "Empirically it sounds like\ntrue, you can do it just by gradient descent. ",
    "start": "4317730",
    "end": "4338295"
  },
  {
    "text": "OK. So I think this is\nall I wanted to say about the two-layer network.",
    "start": "4338295",
    "end": "4346810"
  },
  {
    "text": "Next we are going\nto talk about-- our goal would\nbe-- the next goal would be to prove something\nabout multiple-layer network.",
    "start": "4346810",
    "end": "4354370"
  },
  {
    "text": "And we need more tools. So my plan is to spend the next\n10 minutes to talk about some",
    "start": "4354370",
    "end": "4360070"
  },
  {
    "text": "of the tools. And we need to continue about\nthe tools in the next lecture.",
    "start": "4360070",
    "end": "4367130"
  },
  {
    "text": "And then we can talk\nabout how to have better bounds for multi-layer network. ",
    "start": "4367130",
    "end": "4374210"
  },
  {
    "text": "But if there's any questions,\nI can talk about that. I can answer any\nquestions first.",
    "start": "4374210",
    "end": "4380489"
  },
  {
    "text": "It's a little bit awkward. I thought I have 20 minutes. But there is only 10 minutes.",
    "start": "4380490",
    "end": "4386085"
  },
  {
    "text": "But still, I think it's OK. We can start with\nthe simple thing.",
    "start": "4386085",
    "end": "4391320"
  },
  {
    "text": "But it will be kind of like a-- at least for the moment it will\nbe a quite different mindset.",
    "start": "4391320",
    "end": "4396640"
  },
  {
    "text": "We are thinking about\nthe tools again. OK. ",
    "start": "4396640",
    "end": "4402610"
  },
  {
    "text": "So now we are talking--\nwe are getting back to how do we bound\nRademacher complexity.",
    "start": "4402610",
    "end": "4409260"
  },
  {
    "text": " And we are talking about\nthe different type of tools.",
    "start": "4409260",
    "end": "4416670"
  },
  {
    "text": "And let's recall, maybe--",
    "start": "4416670",
    "end": "4421840"
  },
  {
    "text": "OK. I guess before doing\nthat, maybe let's think about a function\nspace view of the Rademacher",
    "start": "4421840",
    "end": "4433199"
  },
  {
    "text": "complexity. So maybe let me write down the\nRademacher complexity first.",
    "start": "4433200",
    "end": "4438480"
  },
  {
    "text": "So this is something like if\nyou have a function of class f, a Rademacher complexity is--",
    "start": "4438480",
    "end": "4446290"
  },
  {
    "text": "this is the empirical\nRademacher complexity. ",
    "start": "4446290",
    "end": "4454510"
  },
  {
    "text": "So if f is equal to Z1 up to Zn. ",
    "start": "4454510",
    "end": "4464440"
  },
  {
    "text": "And let's think about-- let's define the following set\nQ. This is a set of vectors.",
    "start": "4464440",
    "end": "4472180"
  },
  {
    "text": "And the vectors are the outputs\nof f on these endpoints.",
    "start": "4472180",
    "end": "4478510"
  },
  {
    "text": " So for every\nfunction, you're going to have a vector, n\ndimensional vector.",
    "start": "4478510",
    "end": "4486340"
  },
  {
    "text": "And so this is basically the\nset of outputs of f on the data",
    "start": "4486340",
    "end": "4496710"
  },
  {
    "text": "points Z1 up to Zn, right? These are all the\npossible outputs you can get from applying\nf on this set of points.",
    "start": "4496710",
    "end": "4508100"
  },
  {
    "text": "And they are vectors. And then, you can rewrite this\nas the Rademacher complexity",
    "start": "4508100",
    "end": "4514820"
  },
  {
    "text": "as the following. So you can think of\nthis as you are looking at all the possible\nvectors V in Q.",
    "start": "4514820",
    "end": "4523270"
  },
  {
    "text": "And you look at inner product\nof sigma with v, right?",
    "start": "4523270",
    "end": "4528580"
  },
  {
    "text": "Just because this\n1 over n sigma v is really just the\nsum of sigma i vi,",
    "start": "4528580",
    "end": "4534909"
  },
  {
    "text": "which is the sum\nof sigma i f Zi.",
    "start": "4534910",
    "end": "4540640"
  },
  {
    "text": "This is just a rewriting. So the point here is\nthat this RSF only",
    "start": "4540640",
    "end": "4554699"
  },
  {
    "text": "depends on Q. It\ndepends on the outputs. But for example, but\nnot as opposed to the--",
    "start": "4554700",
    "end": "4564930"
  },
  {
    "text": "for example, the\nparameterization of f. Let me explain what I mean here.",
    "start": "4564930",
    "end": "4570610"
  },
  {
    "text": "So for example,\nlet's suppose you have function of\nclass F, which is F of x is equal to something\nlike sum of theta i xi.",
    "start": "4570610",
    "end": "4579400"
  },
  {
    "text": "Where theta is in dimension d. And suppose you have\nanother function class,",
    "start": "4579400",
    "end": "4584935"
  },
  {
    "text": "f prime, which is of\nthe following form, say something like\nsum of theta i.",
    "start": "4584935",
    "end": "4592210"
  },
  {
    "text": "I'm writing something, where\ntheta is in d and w also",
    "start": "4592210",
    "end": "4597650"
  },
  {
    "text": "is entirely in d. This is just a weird example\njust to demonstrate a point. So suppose you have\nthis two function class.",
    "start": "4597650",
    "end": "4606402"
  },
  {
    "text": "And they have different\nparameterization. They have different\nparameter space even, right? So one has d-dimensional\nparameter space,",
    "start": "4606402",
    "end": "4612100"
  },
  {
    "text": "and the other has 2d\ndimensional parameter space. But these two functions\nhave the same Q--",
    "start": "4612100",
    "end": "4618054"
  },
  {
    "text": " corresponding Q. Because they\nare the family of outputs",
    "start": "4618055",
    "end": "4628220"
  },
  {
    "text": "are the same. Because in some sense,\nyou can have a one to one match between one--",
    "start": "4628220",
    "end": "4634430"
  },
  {
    "text": "a function in capital F and\na function in capital F prime because they are just-- ",
    "start": "4634430",
    "end": "4642300"
  },
  {
    "text": "for every possible outputs that\ncan be output by the function F, you can also find the\none that can be output",
    "start": "4642300",
    "end": "4649410"
  },
  {
    "text": "by some function in F prime. So they have this\ndifferent parameterization.",
    "start": "4649410",
    "end": "4655380"
  },
  {
    "text": "But they have the same\nfunctionality in some sense. Or it's the same\nfamily of functions.",
    "start": "4655380",
    "end": "4661540"
  },
  {
    "text": "And they have the\nsame Q. And then that means they have the\nsame Rademacher complexity. So I guess, I'm just trying\nto reinforce this idea",
    "start": "4661540",
    "end": "4669050"
  },
  {
    "text": "that the only thing\nthat matters is the outputs of the functions,\nbut not how the functions are",
    "start": "4669050",
    "end": "4674840"
  },
  {
    "text": "represented or parameterized. And this would be useful\nas a general thing.",
    "start": "4674840",
    "end": "4684770"
  },
  {
    "text": "It's kind of like a\nchange of mindset. So before you are talking\nabout the parameters, right? So what are the parameters of F?",
    "start": "4684770",
    "end": "4690580"
  },
  {
    "text": "How do you discretize\nyour parameters? From now on, we are going\nto get rid of-- we are not going to think about the\nparameters that much.",
    "start": "4690580",
    "end": "4697027"
  },
  {
    "text": "We are more thinking about\nthe outputs of the functions. ",
    "start": "4697027",
    "end": "4703480"
  },
  {
    "text": "And there's a so-called\nMassart's lemma, which is actually one\nof the things you are",
    "start": "4703480",
    "end": "4708700"
  },
  {
    "text": "asked to prove in the homework. So this lemma is saying\nthat if this Q satisfies",
    "start": "4708700",
    "end": "4716910"
  },
  {
    "text": "that Q is, first of\nall, so I guess maybe",
    "start": "4716910",
    "end": "4726340"
  },
  {
    "text": "let's say for every\nvector V in Q, the two number of V over square\nroot of n is less than n.",
    "start": "4726340",
    "end": "4738130"
  },
  {
    "text": "So this set of Q contains only\nbounded vectors in this sense.",
    "start": "4738130",
    "end": "4744190"
  },
  {
    "text": "By the way, from\nnow on, we're going to see these things very often. Just because you want to\nnormalize your vector.",
    "start": "4744190",
    "end": "4752829"
  },
  {
    "text": "You measure the vector\nby the normalized norm. So the norm itself\ndoesn't matter that much.",
    "start": "4752830",
    "end": "4759190"
  },
  {
    "text": "You want to normalize the norm\nby the dimension of the vector.",
    "start": "4759190",
    "end": "4764260"
  },
  {
    "text": "[INAUDIBLE] ",
    "start": "4764260",
    "end": "4769850"
  },
  {
    "text": "Right. So this is like the range\nfrom theta [INAUDIBLE]?? ",
    "start": "4769850",
    "end": "4778650"
  },
  {
    "text": "Right. That's right. But I think this is actually\na very good question, which I probably should have\ntalked about earlier.",
    "start": "4778650",
    "end": "4786880"
  },
  {
    "text": "So I think probably I\nmentioned this a little bit at some point. So one of the nice thing about\nempirical Rademacher complexity",
    "start": "4786880",
    "end": "4795780"
  },
  {
    "text": "is that now you\nare in this mindset that your Zi's are fixed.",
    "start": "4795780",
    "end": "4800952"
  },
  {
    "text": "So you don't have any\nrandomness in Zi's. They are just the endpoints\nfixed there forever.",
    "start": "4800952",
    "end": "4807280"
  },
  {
    "text": "And of course, the functions\ncan be changed when you have a family of functions. But you don't have\na changing Zi's.",
    "start": "4807280",
    "end": "4815800"
  },
  {
    "text": "So that simplifies things\na lot in some sense. And so, in some\nsense, you can think of this even in\nsome sense, you can",
    "start": "4815800",
    "end": "4823360"
  },
  {
    "text": "think of the functions--\nthe family of functions are functions that map Zi's to\nreal numbers, but not functions",
    "start": "4823360",
    "end": "4829510"
  },
  {
    "text": "that maps Rd to real numbers. You forget about\nany other points.",
    "start": "4829510",
    "end": "4835420"
  },
  {
    "text": "This will just have endpoints. And all your functions\ncan be just represented as n numbers, which are the\noutputs of these endpoints.",
    "start": "4835420",
    "end": "4845380"
  },
  {
    "text": "There's no other point\nyou have to care about. That's kind of the\nbeauty of the Rademacher complexity in some sense.",
    "start": "4845380",
    "end": "4851320"
  },
  {
    "text": "That's why it's powerful. Because you, before the Z is\nthe source of the randomness.",
    "start": "4851320",
    "end": "4856360"
  },
  {
    "text": "But now the randomness\ncome from the sigma. So that's why you can fix Zi's.",
    "start": "4856360",
    "end": "4862880"
  },
  {
    "text": "So is this going to be a\nstatement about if you have-- as long as you have\nmore than [INAUDIBLE]??",
    "start": "4862880",
    "end": "4869560"
  },
  {
    "start": "4869560",
    "end": "4877090"
  },
  {
    "text": "As long as-- You have [INAUDIBLE]? Because then you\ncan [INAUDIBLE]??",
    "start": "4877090",
    "end": "4883902"
  },
  {
    "start": "4883902",
    "end": "4892210"
  },
  {
    "text": "I think you are-- the exact same problem\nis not what you said,",
    "start": "4892210",
    "end": "4898300"
  },
  {
    "text": "but you are in the\nright direction. So basically, the\nRademacher complexity depends on how\ncomplex this set Q is.",
    "start": "4898300",
    "end": "4908740"
  },
  {
    "text": "That's what I'm going to say. And actually, the next\ntime you can see that--",
    "start": "4908740",
    "end": "4915040"
  },
  {
    "text": "I think we have actually\nmentioned this before. So if Q is not very complex.",
    "start": "4915040",
    "end": "4920320"
  },
  {
    "text": "For example, if Q\nis a finite set, then you have good\nRademacher complexity.",
    "start": "4920320",
    "end": "4926930"
  },
  {
    "text": "And of course, how do you\nmeasure the complexity of Q, that's a little bit of a--",
    "start": "4926930",
    "end": "4933142"
  },
  {
    "text": "it's a question that\nwe have to study. But there are, for example,\nif the Q is finite, then you have a bond\non Rademacher complex.",
    "start": "4933142",
    "end": "4939558"
  },
  {
    "text": "That's what I'm going to write. So suppose, so you\nneed two things.",
    "start": "4939558",
    "end": "4945197"
  },
  {
    "text": "One is that Q is finite,\nand the other thing is that Q is roughly bonded. And there's two\nthings that you have",
    "start": "4945197",
    "end": "4951410"
  },
  {
    "text": "that this expectation\nover sigma of this thing,",
    "start": "4951410",
    "end": "4958320"
  },
  {
    "text": "which is equivalent to\nRademacher complexity, is bonded by square root\n2 n square log Q over n.",
    "start": "4958320",
    "end": "4969937"
  },
  {
    "text": " So the sign of Q come into play.",
    "start": "4969937",
    "end": "4975170"
  },
  {
    "text": "And I guess as a\ncorollary, I think",
    "start": "4975170",
    "end": "4980386"
  },
  {
    "text": "this corollary is\nsomething I have presented before with other proof.",
    "start": "4980387",
    "end": "4985540"
  },
  {
    "text": "If F satisfies that this\nfunction is bonded on this Zi's",
    "start": "4985540",
    "end": "4995410"
  },
  {
    "text": "in the following sense,\nthe average output is bounded by m square, the\naverage output square is bonded",
    "start": "4995410",
    "end": "5003140"
  },
  {
    "text": "by m square, then the\nRademacher complexity of F",
    "start": "5003140",
    "end": "5008780"
  },
  {
    "text": "is bounded by 2m\nsquare log F over n.",
    "start": "5008780",
    "end": "5015552"
  },
  {
    "text": "OK. ",
    "start": "5015552",
    "end": "5022750"
  },
  {
    "text": "So that's the\nrelatively easy thing where you have finite\nhypothesis class. OK. ",
    "start": "5022750",
    "end": "5034469"
  },
  {
    "text": "So and this is a\nhomework question. I thought I made the\nhomework question. I think there's a hint, which\nis actually pretty important,",
    "start": "5034470",
    "end": "5040990"
  },
  {
    "text": "which is you should\nconsider using something about the moment\ngenerating function, which",
    "start": "5040990",
    "end": "5046000"
  },
  {
    "text": "will make the math easier. Actually, there are\ntwo ways to prove it. The other way is that you do\nthis quantization plus union",
    "start": "5046000",
    "end": "5052720"
  },
  {
    "text": "bound. And that will give you a-- you will have a relative\nhard time to do that, just because the constants\nare so hard to make--",
    "start": "5052720",
    "end": "5060640"
  },
  {
    "text": "you can work out a similar\nbound, but just a little messy. The moment generating\nfunction is very really cool.",
    "start": "5060640",
    "end": "5067060"
  },
  {
    "text": "The proof is\nactually pretty short if you use the right way--\nyou use it in the right way. OK. So I guess, let me just\nbriefly give a quick overview",
    "start": "5067060",
    "end": "5077560"
  },
  {
    "text": "of what we're going to do next. So that you can appreciate why\nI'm setting up the things here.",
    "start": "5077560",
    "end": "5082900"
  },
  {
    "text": "So the next thing is that\nwhat if Q is not finite?",
    "start": "5082900",
    "end": "5090610"
  },
  {
    "text": "What do we do, right? So and our answer would be\nthat you do some discretization",
    "start": "5090610",
    "end": "5099623"
  },
  {
    "text": "and class union bound. ",
    "start": "5099623",
    "end": "5104660"
  },
  {
    "text": "So basically you have some\nepsilon covering stuff, and you have a union bound.",
    "start": "5104660",
    "end": "5111440"
  },
  {
    "text": "Or maybe I should say\nI have discretization to reduce to the finite case.",
    "start": "5111440",
    "end": "5117989"
  },
  {
    "text": "That's basically the idea. And you probably have seen\nthis idea before, right? You have seen it in\nthe third lecture",
    "start": "5117990",
    "end": "5124780"
  },
  {
    "text": "maybe when we talk about\ninfinite hypothesis class. But here, the difference\nis that here you",
    "start": "5124780",
    "end": "5130690"
  },
  {
    "text": "are discretizing the output\nspace, the set Q which is",
    "start": "5130690",
    "end": "5137570"
  },
  {
    "text": "a set of n-dimensional vectors. So before you were discretizing\nthe parameter space.",
    "start": "5137570",
    "end": "5146440"
  },
  {
    "text": "You already have a\nd-dimensional parameter space. You discretize that. But here you are doing a more\nand sometimes fundamental",
    "start": "5146440",
    "end": "5152500"
  },
  {
    "text": "discretization. Because any way the\nparameters are argued is probably not the most\nimportant thing, right?",
    "start": "5152500",
    "end": "5157636"
  },
  {
    "text": "What's really important\nis that what's the functionality of\nthis family of functions. So now you are discretizing\nin the right space,",
    "start": "5157637",
    "end": "5165909"
  },
  {
    "text": "your more fundamental space. And this is the\nspace of the outputs. So what we will\ndo is, we're going",
    "start": "5165910",
    "end": "5171880"
  },
  {
    "text": "to discuss a few techniques\nto discretize this Q",
    "start": "5171880",
    "end": "5177010"
  },
  {
    "text": "and what kind of\ndiscretization you really need, so and so forth. So and there's\nactually some kind",
    "start": "5177010",
    "end": "5183520"
  },
  {
    "text": "of a pretty deep theorem, which\nis called the Dudley chaining",
    "start": "5183520",
    "end": "5190430"
  },
  {
    "text": "theorem. Which actually requires you to\ndiscretize in your nested way.",
    "start": "5190430",
    "end": "5196780"
  },
  {
    "text": "You have a hierarchical\ndiscretization, so that you can have\nthe best discretization. So this is something beyond\nwhat we have done before.",
    "start": "5196780",
    "end": "5205570"
  },
  {
    "text": "Even you don't care about the\ndifference between the output space and parameter space. Here you can discretize in a\nmuch more efficient fashion.",
    "start": "5205570",
    "end": "5215230"
  },
  {
    "text": "So that's what we're\ngoing to do next. And then we're going to use this\nfor the multi-layer network.",
    "start": "5215230",
    "end": "5223400"
  },
  {
    "text": "Sounds good. I think that's all for today. ",
    "start": "5223400",
    "end": "5231000"
  }
]