[
  {
    "start": "0",
    "end": "16000"
  },
  {
    "start": "0",
    "end": "5660"
  },
  {
    "text": "OK, let's get started. So today we're going\nto be covering a fun",
    "start": "5660",
    "end": "11585"
  },
  {
    "text": "topic because a lot of\nwhat we'll be covering is stuff that's\nextremely recent. Now, topic one is\nreally the question",
    "start": "11585",
    "end": "18620"
  },
  {
    "start": "16000",
    "end": "48000"
  },
  {
    "text": "that we're going to be focusing\non for the lecture today is, how should we be defining\ntasks for good meta-learning",
    "start": "18620",
    "end": "26509"
  },
  {
    "text": "performance? We've talked a lot about\ndifferent meta-learning algorithms, and we've\ntalked about one way",
    "start": "26510",
    "end": "32270"
  },
  {
    "text": "to construct tasks. We've also looked at\nsome case studies. But in this lecture, we're\ngoing to talk about some",
    "start": "32270",
    "end": "38180"
  },
  {
    "text": "of the different failure\nmodes that can come up, or some of the situations\nwhere maybe it's pretty hard to construct\ntasks and some of the things",
    "start": "38180",
    "end": "45289"
  },
  {
    "text": "that you might be able to\ndo in those situations. So in particular,\nthe plan for today",
    "start": "45290",
    "end": "50570"
  },
  {
    "start": "48000",
    "end": "127000"
  },
  {
    "text": "is to very briefly\nrecap the three different meta-learning\napproaches that we've talked about as well as the\nkind of ways to construct tasks",
    "start": "50570",
    "end": "59800"
  },
  {
    "text": "that we've talked about. Then we're going to talk about\nmemorization in meta-learning, and when it arises, a\npotential solution to that.",
    "start": "59800",
    "end": "66810"
  },
  {
    "text": "And then lastly, we'll\ntalk about meta-learning when you don't have any\ntasks that are provided",
    "start": "66810",
    "end": "72110"
  },
  {
    "text": "and ways in which the algorithm\nmight be able to automatically come up with tasks that it\nshould solve in a way that",
    "start": "72110",
    "end": "78259"
  },
  {
    "text": "can allow it to still\nbe able to quickly adapt to human-defined\ndownstream tasks.",
    "start": "78260",
    "end": "84900"
  },
  {
    "text": "As I mentioned, these\ntopics are really new. They're really at the\nbleeding edge of research. And so just as a disclaimer\nor a warning, potentially",
    "start": "84900",
    "end": "93480"
  },
  {
    "text": "a lot of the things that we're\ngoing to be covering today are really at the\ncusp of what we know",
    "start": "93480",
    "end": "99995"
  },
  {
    "text": "and what we've\nexperimented with. And so I encourage\nyou to ask questions. But for some of those questions,\nI may not know the answer.",
    "start": "99995",
    "end": "107297"
  },
  {
    "text": "They may be still open\nproblems and open questions in the field. With that said, the goals\nby the end of the lecture",
    "start": "107297",
    "end": "114649"
  },
  {
    "text": "is to help you understand\nhow and when memorization in meta-learning may occur\nand understanding techniques",
    "start": "114650",
    "end": "121609"
  },
  {
    "text": "for constructing\ntasks automatically.",
    "start": "121610",
    "end": "126890"
  },
  {
    "text": "OK, so let's get started. So first let's recap\nthe different approaches",
    "start": "126890",
    "end": "132170"
  },
  {
    "start": "127000",
    "end": "219000"
  },
  {
    "text": "that we talked about. So I think a couple of\nweeks ago at this point, we talked about\nblack-box meta-learning,",
    "start": "132170",
    "end": "138540"
  },
  {
    "text": "where you have some\nneural network f, that takes as input\na data, it also takes",
    "start": "138540",
    "end": "144530"
  },
  {
    "text": "as input a new data point,\nand makes a prediction for that new data point.",
    "start": "144530",
    "end": "150370"
  },
  {
    "text": "So the key idea is to\nkind of parameterize a learner as a neural network. This approach is\nvery expressive,",
    "start": "150370",
    "end": "156110"
  },
  {
    "text": "but it's also a very challenging\noptimization problem. We also talked about\noptimization-based",
    "start": "156110",
    "end": "161810"
  },
  {
    "text": "meta-learning\nalgorithms, which instead of using a neural network\nto parameterize the learner, they use an\noptimization process.",
    "start": "161810",
    "end": "168050"
  },
  {
    "text": "And any free parameters of\nthat optimization process like the initial parameters,\nor the inner learning rate",
    "start": "168050",
    "end": "175280"
  },
  {
    "text": "can be meta-learned. So the key idea is to kind of\nsimply embed this optimization",
    "start": "175280",
    "end": "181400"
  },
  {
    "text": "inside the learning\nprocess and optimize end to end with respect\nto those parameters.",
    "start": "181400",
    "end": "187545"
  },
  {
    "text": "And this is what\nyou'll be implementing as part of Homework 2\nthat came out today. The benefit of this\nis that you have",
    "start": "187545",
    "end": "193580"
  },
  {
    "text": "the structure of\noptimization embedded into the meta-learning process. The downside is\nthat it typically",
    "start": "193580",
    "end": "199640"
  },
  {
    "text": "requires a second order\noptimization, which can be a bit more heavy or\ncomputationally intensive.",
    "start": "199640",
    "end": "207470"
  },
  {
    "text": "The good news, though, is that\nthrough various deep learning libraries that we\nhave can typically handle the more arduous\nparts of the second order",
    "start": "207470",
    "end": "216140"
  },
  {
    "text": "optimization for you. And then lastly,\nlast week we also",
    "start": "216140",
    "end": "221439"
  },
  {
    "start": "219000",
    "end": "299000"
  },
  {
    "text": "talked about a third class of\nmeta-learning algorithms, which I referred to as non-parametric\nmeta-learning algorithms.",
    "start": "221440",
    "end": "227140"
  },
  {
    "text": "And the way that these\nmeta-learning algorithms approach the problem\nis they first embed the test example and\nall of your training examples,",
    "start": "227140",
    "end": "236260"
  },
  {
    "text": "and then they do some\nsort of nearest neighbor by comparing your test\nembedding to all of the training",
    "start": "236260",
    "end": "241989"
  },
  {
    "text": "embeddings, find the one that's\nmost similar, such as this one right here, and\nuse that to predict",
    "start": "241990",
    "end": "248680"
  },
  {
    "text": "the corresponding label. So really, the key\nidea here is to use a non-parametric learner, which\nis basically everything shown",
    "start": "248680",
    "end": "255850"
  },
  {
    "text": "in green here, with a\nparametric embedding or distance function, which are each of\nthe arrows shown in blue.",
    "start": "255850",
    "end": "263590"
  },
  {
    "text": "For example, the\nnon-parametric learner may be something like K\nnearest neighbors to examples or prototypes, and then\nthe parametric embedding",
    "start": "263590",
    "end": "270880"
  },
  {
    "text": "is what the meta-learner\nis learning in order to make this non-parametric\nlearner successful with only",
    "start": "270880",
    "end": "276780"
  },
  {
    "text": "a few examples.  The benefits of this is is\nthat it's easy to optimize",
    "start": "276780",
    "end": "282580"
  },
  {
    "text": "and it's computationally\nvery fast. This is entirely a\nfeed forward process.",
    "start": "282580",
    "end": "288190"
  },
  {
    "text": "The downside is that\nit, so far, at least, has been largely restricted\nto classification problems. ",
    "start": "288190",
    "end": "295280"
  },
  {
    "text": "OK, so that's kind\nof a brief recap of the three\nmeta-learning approaches that we've talked about. And then we've also talked\nabout different ways",
    "start": "295280",
    "end": "301940"
  },
  {
    "start": "299000",
    "end": "341000"
  },
  {
    "text": "to construct tasks. So one of the canonical\nways to construct tasks",
    "start": "301940",
    "end": "307040"
  },
  {
    "text": "is to use labeled images\nfrom previous classes in order to allow you to\nsolve N-way classification",
    "start": "307040",
    "end": "313340"
  },
  {
    "text": "problems with a small\nnumber of examples. But we also looked\nat case studies that allow you to adapt\nto regional differences",
    "start": "313340",
    "end": "320600"
  },
  {
    "text": "using labeled data\nfrom other regions. And also things like\nfew-shot imitation learning,",
    "start": "320600",
    "end": "326360"
  },
  {
    "text": "where you use demonstrations\nfrom previous tasks. The essence here, though, is\nthat you construct a task using",
    "start": "326360",
    "end": "332870"
  },
  {
    "text": "labeled data from tasks that\nare representative of the kinds of tasks that you'll\nbe seeing at test time.",
    "start": "332870",
    "end": "338180"
  },
  {
    "text": " OK so that's a brief recap. Now let's talk a bit\nabout some challenges that",
    "start": "338180",
    "end": "345627"
  },
  {
    "start": "341000",
    "end": "456000"
  },
  {
    "text": "come up when we construct\ntasks for meta-learning. ",
    "start": "345627",
    "end": "351130"
  },
  {
    "text": "So, in the example of\nfew-shot image classification, the tasks that we've\nlook at looked something",
    "start": "351130",
    "end": "359259"
  },
  {
    "text": "like this, where we constructed\na training set and some test examples.",
    "start": "359260",
    "end": "365110"
  },
  {
    "text": "And we constructed this\ntraining set and test set for basically each of the\ntasks in our training set.",
    "start": "365110",
    "end": "371780"
  },
  {
    "text": "Now, one of the\nthings that we did when we constructed these tasks\nis we randomly assigned labels",
    "start": "371780",
    "end": "378610"
  },
  {
    "text": "to each class. So, for example, in this first\ntask, the class of mushroom",
    "start": "378610",
    "end": "384760"
  },
  {
    "text": "may get a label of one,\nwhereas in this third task, the label of mushroom\nmight get a label--",
    "start": "384760",
    "end": "389980"
  },
  {
    "text": "sorry. The class of a mushroom\nmay get a label of 0. And so we're randomly assigning\ndifferent class labels",
    "start": "389980",
    "end": "397169"
  },
  {
    "text": "to image classes for each task. And this means that\nthe tasks are what",
    "start": "397170",
    "end": "402450"
  },
  {
    "text": "I will call mutually exclusive. And what I mean by\nthat is that there isn't just a single\nfunction that",
    "start": "402450",
    "end": "408730"
  },
  {
    "text": "can solve all of these tasks. And that's because we're\nassigning different labels",
    "start": "408730",
    "end": "414070"
  },
  {
    "text": "to different classes. So we don't have\na single function that will both assign a 0 to a\nmushroom and a 1 to a mushroom.",
    "start": "414070",
    "end": "420010"
  },
  {
    "text": "And likewise for\nclasses like pianos, that also have different\nlabels for different tasks. ",
    "start": "420010",
    "end": "426360"
  },
  {
    "text": "OK. So one byproduct of this way\nthat we're constructing tasks",
    "start": "426360",
    "end": "431900"
  },
  {
    "text": "is that the algorithm must\nuse the training data in order to figure out what the\nlabel ordering should be.",
    "start": "431900",
    "end": "438198"
  },
  {
    "text": "So it actually has to look\nat this training data, look at of what ID was\nassigned to mushroom, what",
    "start": "438198",
    "end": "443449"
  },
  {
    "text": "ID was assigned to piano,\nand so forth, in order to make accurate predictions\non the test examples.",
    "start": "443450",
    "end": "449130"
  },
  {
    "text": " OK, so now with\nthis in mind, this is kind of the\ncanonical way that we",
    "start": "449130",
    "end": "455430"
  },
  {
    "text": "construct tasks, what happens if\nthe label order is consistent?",
    "start": "455430",
    "end": "461669"
  },
  {
    "start": "456000",
    "end": "925000"
  },
  {
    "text": "So in particular,\nsay that we construct a task like this, where we\nalways assigned the label",
    "start": "461670",
    "end": "468199"
  },
  {
    "text": "1 to mushroom, the label 2 to\nthis kind of dog, the label",
    "start": "468200",
    "end": "473240"
  },
  {
    "text": "4 to carousel, the label 1\nto landscape, and so forth. ",
    "start": "473240",
    "end": "480320"
  },
  {
    "text": "So based on this, I guess\nI want to run a little poll to see what people think.",
    "start": "480320",
    "end": "486610"
  },
  {
    "text": "And in particular,\nthe question will be, what happens when the\nlabel order is consistent?",
    "start": "486610",
    "end": "493100"
  },
  {
    "text": "So if we meta-train a\nmodel on the task like this with a consistent\nlabel ordering, and then test them on new\ntasks with held-out classes,",
    "start": "493100",
    "end": "502689"
  },
  {
    "text": "what do you think will happen? So a logical and--",
    "start": "502690",
    "end": "509800"
  },
  {
    "text": "I'll also give you a\nminute to think about it. And the different\noptions are, will it",
    "start": "509800",
    "end": "514823"
  },
  {
    "text": "do much better than the\nshuffled label ordering, slightly better than\nthe shuffled label ordering, the same as before,\nslightly worse, or much worse?",
    "start": "514824",
    "end": "523539"
  },
  {
    "start": "523539",
    "end": "532420"
  },
  {
    "text": "So we have some votes coming in. ",
    "start": "532420",
    "end": "542930"
  },
  {
    "text": "Mentioned in the chat\nthat he accidentally did this in Homework 1. ",
    "start": "542930",
    "end": "558190"
  },
  {
    "text": "So I'll wait a little\nbit longer for a few more votes to trickle in. ",
    "start": "558190",
    "end": "576120"
  },
  {
    "text": "OK, so we'll end the poll there\nand I'll share the results.",
    "start": "576120",
    "end": "583740"
  },
  {
    "text": "So most of you think that\nit will do much worse. Although, it's a bit--",
    "start": "583740",
    "end": "589063"
  },
  {
    "text": "different people think\ndifferent things. So I'm curious why people\nput the answer that they put.",
    "start": "589063",
    "end": "596790"
  },
  {
    "text": "So did anyone say that\nthey think it performed-- I guess does anyone want to\nshare their reasoning for why",
    "start": "596790",
    "end": "604740"
  },
  {
    "text": "they think the\nanswer that they put?  You can raise your hand.",
    "start": "604740",
    "end": "610400"
  },
  {
    "start": "610400",
    "end": "617890"
  },
  {
    "text": "So I put much worse. And the idea there\nwas that, if there",
    "start": "617890",
    "end": "623589"
  },
  {
    "text": "is an ordering of\nclasses, and if you use the model like an LSTM,\nthen you could see maybe",
    "start": "623590",
    "end": "629700"
  },
  {
    "text": "unit learning features are\nspecific to a certain class. And then when they shuffle\nthe ordering at test time,",
    "start": "629700",
    "end": "635440"
  },
  {
    "text": "then they'll do worse.",
    "start": "635440",
    "end": "642020"
  },
  {
    "text": "Yeah, so the idea there\nis that the tasks that will be given at\ntest time will have",
    "start": "642020",
    "end": "647570"
  },
  {
    "text": "a different-- will\nhave new classes and so you won't necessarily\nknow the ordering. And so if we're\nlearning kind of things",
    "start": "647570",
    "end": "653480"
  },
  {
    "text": "that are specific\nto these training classes with this\nlabel order, maybe it won't generalize to that\nkind of new test task.",
    "start": "653480",
    "end": "663610"
  },
  {
    "text": "Any other opinions or thoughts\nthat people want to share? ",
    "start": "663610",
    "end": "670200"
  },
  {
    "text": "Does this make it easier or\nmore possible to shuffle? So, for example, if we shuffle\n[INAUDIBLE] we can still have",
    "start": "670200",
    "end": "677005"
  },
  {
    "text": "all dogs T2-- Sorry, I mean, I guess it is\nnot doing in the second T2.",
    "start": "677005",
    "end": "685890"
  },
  {
    "text": "But [INAUDIBLE] it\nenables us to shuffle. So I thought maybe\nit will do better. ",
    "start": "685890",
    "end": "695560"
  },
  {
    "text": "I guess it's not clear\nexactly what you mean by, it enables us to shuffle. So if you're training\nwith these classes that",
    "start": "695560",
    "end": "702220"
  },
  {
    "text": "have a consistent label\nordering during training, what do you think that that\nmakes it easier to shuffle?",
    "start": "702220",
    "end": "707864"
  },
  {
    "text": "Just trying to understand\nyour reasoning? In fact, maybe I didn't\nunderstand the idea of order",
    "start": "707864",
    "end": "714490"
  },
  {
    "text": "being consistent label ordering. From what I thought, I\nthought all carousels are 2, all dogs are 2, and\nsomething like that.",
    "start": "714490",
    "end": "722360"
  },
  {
    "text": "And so if you see a dog, that\nmodel is able to predict a 2.",
    "start": "722360",
    "end": "727453"
  },
  {
    "text": "But it looks more that--\nit's something that's more to do with the\norder of the images than what's in the images? Maybe I didn't really\nunderstand the question. OK.",
    "start": "727453",
    "end": "732860"
  },
  {
    "text": " And do you want to\nshare your thoughts?",
    "start": "732860",
    "end": "740519"
  },
  {
    "text": "Yeah. So I put much worse. And that's because\nthe model will",
    "start": "740520",
    "end": "746274"
  },
  {
    "text": "learn what class it belongs. So for example, in this\ncase, it will learn",
    "start": "746274",
    "end": "754290"
  },
  {
    "text": "that a dog is always class 2. So during the training, it will\nalways classify it correctly.",
    "start": "754290",
    "end": "761420"
  },
  {
    "text": "But for held-out classes, it\nwon't be able to generalize. It will be more like\nsupervised, I guess.",
    "start": "761420",
    "end": "769190"
  },
  {
    "text": "Yeah, it will be more\nlike supervised use case meta-learning.",
    "start": "769190",
    "end": "774920"
  },
  {
    "text": "Yeah, so just to\nsummarize, it may just learn to kind of do a supervised\nregression to the labels",
    "start": "774920",
    "end": "780440"
  },
  {
    "text": "rather than actually\nlearning to adapt to the data set, which will--",
    "start": "780440",
    "end": "786626"
  },
  {
    "text": "if it's more like\nsupervised learning, then it may not be able to\nquickly adapt to new examples.",
    "start": "786627",
    "end": "791730"
  },
  {
    "text": "And then I see some\nmore hands raised. You want to share your thoughts? Yeah, I just wanted to\nask a clarifying question.",
    "start": "791730",
    "end": "798410"
  },
  {
    "text": "So we've mentioned the\norder a couple times. But in this case, the\nalgorithm doesn't necessarily",
    "start": "798410",
    "end": "803632"
  },
  {
    "text": "learn the order, right? It's more about not\nbeing able to learn how to learn instead of learning\nthe supervised learning task,",
    "start": "803632",
    "end": "812240"
  },
  {
    "text": "is that a good summary? Yes, I guess what\nI meant by ordering is basically that the assignment\nof class IDs to classes",
    "start": "812240",
    "end": "823620"
  },
  {
    "text": "is consistent. Yeah, so maybe ordering\nisn't quite the right way",
    "start": "823620",
    "end": "829980"
  },
  {
    "text": "to describe that. OK, that makes sense. Thanks.",
    "start": "829980",
    "end": "835090"
  },
  {
    "text": "OK. Cool. And there are a couple\nmore hands raised.",
    "start": "835090",
    "end": "840400"
  },
  {
    "text": "Do you want to share something? Yeah, sure. I put much worse. I think it might be a problem\ndepending on whether it's",
    "start": "840400",
    "end": "846850"
  },
  {
    "text": "slightly worse or much worse. But my thinking was that if the\nordering is consistent, then",
    "start": "846850",
    "end": "853180"
  },
  {
    "text": "at meta test time,\nthe model can adjust-- well, during meta-training,\nif the labels are consistent,",
    "start": "853180",
    "end": "861640"
  },
  {
    "text": "then the model will learn to\nignore the label in D train.",
    "start": "861640",
    "end": "868600"
  },
  {
    "text": "It no longer needs\nthe y in D train because it can memorize\nall of the classes",
    "start": "868600",
    "end": "874810"
  },
  {
    "text": "in the meta-training set. Yeah, exactly.",
    "start": "874810",
    "end": "880420"
  },
  {
    "text": "And so in some ways, the outline\nof the lecture sort of maybe gave it away, what the\nanswer was in some regard.",
    "start": "880420",
    "end": "888240"
  },
  {
    "text": "But when you have this\nconsistent assignment of labels to classes, and the tasks become\nnot mutually exclusive such",
    "start": "888240",
    "end": "895200"
  },
  {
    "text": "that a single function\ncan solve all those tasks, and this means that your\nmeta-learned model can simply",
    "start": "895200",
    "end": "901540"
  },
  {
    "text": "learn to classify the inputs\nirrespective of the training data that's passed in. So you can basically\ncompletely ignore",
    "start": "901540",
    "end": "908620"
  },
  {
    "text": "the training data sets, just\nlearn to classify everything in the test sets per task.",
    "start": "908620",
    "end": "915150"
  },
  {
    "text": "And then when you give it\na new training data set, it won't be able\nto actually process that because it didn't learn\nto process the training",
    "start": "915150",
    "end": "922620"
  },
  {
    "text": "sets during meta-training. And so in particular,\nwhat can happen",
    "start": "922620",
    "end": "927710"
  },
  {
    "start": "925000",
    "end": "1611000"
  },
  {
    "text": "is, if you have a\nblack box model, it can simply just ignore\nall of the training inputs",
    "start": "927710",
    "end": "934130"
  },
  {
    "text": "here and simply just\nclassify the last input that's provided to the model.",
    "start": "934130",
    "end": "940690"
  },
  {
    "text": "And likewise with an\noptimization-based model, it can also just\nignore the dataset and learn a set of\ninitial parameters",
    "start": "940690",
    "end": "948010"
  },
  {
    "text": "that somehow renders the\noptimization in the inner loop kind of irrelevant.",
    "start": "948010",
    "end": "953740"
  },
  {
    "text": "For example, if it can already\npredict all of the examples correctly just by\nlooking at the image",
    "start": "953740",
    "end": "959715"
  },
  {
    "text": "and not by looking\nat the labels, then the inner\ngradient will be 0. And that won't have any\nimpact on the network.",
    "start": "959715",
    "end": "967940"
  },
  {
    "text": "And that will effectively\nlead it to ignore the training set also.",
    "start": "967940",
    "end": "973640"
  },
  {
    "text": "So in particular, now if\nyou meta-train a model",
    "start": "973640",
    "end": "980400"
  },
  {
    "text": "with this label assignment\nthat's consistent, and then evaluate it to a new\ntask with held-out classes,",
    "start": "980400",
    "end": "989339"
  },
  {
    "text": "with classes like\nthis, for example, it doesn't know what\nthe labels should",
    "start": "989340",
    "end": "995100"
  },
  {
    "text": "be for each of these classes. It doesn't know what\nthe assignment should be, if it should be zero,\none, two, three, or four for each of these classes.",
    "start": "995100",
    "end": "1002265"
  },
  {
    "text": "And because it's learned to\nignore each of these training data sets, it's also going\nto ignore this training data set of the test\ntask, and it won't",
    "start": "1002265",
    "end": "1008840"
  },
  {
    "text": "be able to effectively make\npredictions on the test examples. And if you actually run\nMAML on these tasks,",
    "start": "1008840",
    "end": "1017690"
  },
  {
    "text": "you'll get performance\nthat looks like this. So in this case, this is a\nnon-mutually exclusive version",
    "start": "1017690",
    "end": "1023060"
  },
  {
    "text": "of Omniglot, where you apply\nthe same label ID to each class",
    "start": "1023060",
    "end": "1029390"
  },
  {
    "text": "consistently across\nall the tasks. And you see performance drops\nfrom performance roughly",
    "start": "1029390",
    "end": "1034400"
  },
  {
    "text": "in the 90s to accuracy numbers\nthat are around 7.8% and 50.7%",
    "start": "1034400",
    "end": "1041630"
  },
  {
    "text": "in these different cases. So you see a pretty\nsubstantial drop in performance",
    "start": "1041630",
    "end": "1047779"
  },
  {
    "text": "when you make the label\nassignment consistent.",
    "start": "1047780",
    "end": "1053710"
  },
  {
    "text": "Now, this seems\nlike it shouldn't-- if you think about how\nyou construct tasks, it seems like this--",
    "start": "1053710",
    "end": "1059680"
  },
  {
    "text": "ideally, it shouldn't make\nthis huge of a difference depending on how you\nassign the labels.",
    "start": "1059680",
    "end": "1067340"
  },
  {
    "text": "So maybe let's kind of\ndig into this problem. So asking, does\nthis memorization",
    "start": "1067340",
    "end": "1073220"
  },
  {
    "text": "occur in non-parametric\nmodels as well? I'll say that it occurs in\nblack box models for sure,",
    "start": "1073220",
    "end": "1080180"
  },
  {
    "text": "and I'll show some\nresults on that. With regard to\nnon-parametric models,",
    "start": "1080180",
    "end": "1087020"
  },
  {
    "text": "I think that it\nprobably won't occur because non-parametric\nmodels basically",
    "start": "1087020",
    "end": "1093350"
  },
  {
    "text": "use the label ordering\nin a very specific way. So you basically force the model\nto use the labels in a way that",
    "start": "1093350",
    "end": "1104450"
  },
  {
    "text": "looks at similarity. And there's kind of no way--",
    "start": "1104450",
    "end": "1110660"
  },
  {
    "text": "in the course of\nthis model, it's probably a lot harder to\ncompletely ignore the training data than in the\nother kinds of models",
    "start": "1110660",
    "end": "1117930"
  },
  {
    "text": "because your\npredictions have to-- because you literally\nhave to actually use",
    "start": "1117930",
    "end": "1123649"
  },
  {
    "text": "this similarity in order to\noutput the correct label. Otherwise you have no other way\nto kind of directly fast track",
    "start": "1123650",
    "end": "1130160"
  },
  {
    "text": "from x test to y test other\nthan going through the labels in your training set.",
    "start": "1130160",
    "end": "1135395"
  },
  {
    "text": "So I think that this problem-- I haven't experimented\nwith it in practice, but I'm pretty sure that\nthis problem won't come up",
    "start": "1135395",
    "end": "1141435"
  },
  {
    "text": "for non-parametric approaches.  OK, great.",
    "start": "1141435",
    "end": "1152420"
  },
  {
    "text": "So this seems like a bit\nof a problem, potentially.",
    "start": "1152420",
    "end": "1157890"
  },
  {
    "text": "And it's worth mentioning\nthat potentially for image classification, we\ncan just shuffle the labels and we'll still be fine.",
    "start": "1157890",
    "end": "1164100"
  },
  {
    "text": "Although it is potentially still\nperhaps somewhat disturbing that this issue comes up if\nyou don't shuffle the labels.",
    "start": "1164100",
    "end": "1170880"
  },
  {
    "text": "And maybe it's\nalso not a problem if we see the same image\nclasses as training.",
    "start": "1170880",
    "end": "1176340"
  },
  {
    "text": "If we don't see\nheld-out image classes, then this model will be fine\nbecause it doesn't actually",
    "start": "1176340",
    "end": "1181500"
  },
  {
    "text": "need to use the training data. But it is a problem if we\nwant to be able to adapt with data to new tasks.",
    "start": "1181500",
    "end": "1190220"
  },
  {
    "text": "OK, so it looks like\nthere are some questions and I'll take those\nquestions now. So I think-- can I assume that\nyour hand is up from before?",
    "start": "1190220",
    "end": "1197390"
  },
  {
    "text": " Not sure if I\npronounced that right. ",
    "start": "1197390",
    "end": "1208640"
  },
  {
    "text": "Do you have a question? No, actually. I was raising my hand for\nthe previous question.",
    "start": "1208640",
    "end": "1214600"
  },
  {
    "text": "That's fine. OK. ",
    "start": "1214600",
    "end": "1219640"
  },
  {
    "text": "I was still a little\nconfused about why we were able to say that the\nmodel ignores the training",
    "start": "1219640",
    "end": "1227110"
  },
  {
    "text": "data set in meta-train. Wouldn't it still need to\nlook at the training set",
    "start": "1227110",
    "end": "1233200"
  },
  {
    "text": "in order to correctly classify\nthe new task set images? ",
    "start": "1233200",
    "end": "1240970"
  },
  {
    "text": "Yeah, so basically\nwhat happens is-- I guess-- so I'll get into\nthis a little bit later,",
    "start": "1240970",
    "end": "1247990"
  },
  {
    "text": "but it is possible that it will\nstill learn to pay attention to this data set here.",
    "start": "1247990",
    "end": "1255050"
  },
  {
    "text": "But I guess the important\nthing to point out is that if piano always\nhas a label of 4,",
    "start": "1255050",
    "end": "1261160"
  },
  {
    "text": "then it doesn't\nneed to figure out-- it doesn't need to look\nat these training examples to tell that the label\nshould be a 4 for this image.",
    "start": "1261160",
    "end": "1268860"
  },
  {
    "text": " And basically, if there is\nkind of a consistent assignment",
    "start": "1268860",
    "end": "1275169"
  },
  {
    "text": "of labels to image\nclasses, then it could just kind of memorize\nthat consistent assignment",
    "start": "1275170",
    "end": "1282720"
  },
  {
    "text": "and make predictions based\nonly on the test example, and not based on\nthe training set.",
    "start": "1282720",
    "end": "1289850"
  },
  {
    "text": "OK, thank you. ",
    "start": "1289850",
    "end": "1296050"
  },
  {
    "text": "I'm just trying reconcile\nthis in Homework 1 where we were asked not to\nshuffle the examples, right?",
    "start": "1296050",
    "end": "1302730"
  },
  {
    "text": "So the input to the LSTM\nfor a two-shot problem would look like 0,\n1, 2, 3, 0, 1, 2, 3,",
    "start": "1302730",
    "end": "1309010"
  },
  {
    "text": "or something like that. So is that why we were\nexpecting a lower performance",
    "start": "1309010",
    "end": "1315221"
  },
  {
    "text": "than what we would have\nwhen we compared it to state of the art or-- So to clarify in Homework\n1, you were still",
    "start": "1315221",
    "end": "1322470"
  },
  {
    "text": "shuffling the assignment\nof labels to classes because you would basically\nrandomly sample N-classes",
    "start": "1322470",
    "end": "1328830"
  },
  {
    "text": "and randomly assign\nN-labels to those classes. What you weren't\nshuffling was necessarily the order in which you\npassed these examples",
    "start": "1328830",
    "end": "1335970"
  },
  {
    "text": "into the network. So basically, the labels\nhere would be shuffled and you'd have\ndifferent assignments",
    "start": "1335970",
    "end": "1341160"
  },
  {
    "text": "for different classes. But once you determine\nthat assignment of labels",
    "start": "1341160",
    "end": "1347460"
  },
  {
    "text": "to classes for\nthat task, then you fix the order of the\nexamples that you pass in.",
    "start": "1347460",
    "end": "1358680"
  },
  {
    "text": "OK. And there's also a few\nquestions in the chat.",
    "start": "1358680",
    "end": "1364420"
  },
  {
    "text": "So [AUDIO OUT],, asking,\nthat it should always predict 4 for piano and\nget a high accuracy rate.",
    "start": "1364420",
    "end": "1370900"
  },
  {
    "text": "So it's worth mentioning\nthat when this happens, the model will actually\ndo great in terms",
    "start": "1370900",
    "end": "1377170"
  },
  {
    "text": "of meta-training performance. It will be able to\nvery accurately predict that the piano is a 4, that\nthis particular kind of dog",
    "start": "1377170",
    "end": "1385299"
  },
  {
    "text": "is a 2 and so forth. Where performance\nwill drop is when you're given new tasks\nat meta-test time that",
    "start": "1385300",
    "end": "1391580"
  },
  {
    "text": "have new image\nclasses, and the model doesn't know the\nassignment, doesn't know what label those new\nimage classes correspond to.",
    "start": "1391580",
    "end": "1398780"
  },
  {
    "text": " And then there's also a\nquestion in the chat about,",
    "start": "1398780",
    "end": "1405390"
  },
  {
    "text": "do we have a smaller\nperformance gap under 5 way setting compared\nto a 20 way setting?",
    "start": "1405390",
    "end": "1410490"
  },
  {
    "text": "I can't remember exactly. I think I might be\nshowing results later for the 5 way setting.",
    "start": "1410490",
    "end": "1416138"
  },
  {
    "text": "And we might see that\nlater in the lecture.  Do you have a question?",
    "start": "1416138",
    "end": "1424000"
  },
  {
    "text": "Sorry, I-- go back\nto [INAUDIBLE].. ",
    "start": "1424000",
    "end": "1430905"
  },
  {
    "text": "Yeah, [INAUDIBLE]\nso we are saying that we fix the labels\nif we have N classes.",
    "start": "1430905",
    "end": "1437490"
  },
  {
    "text": "So we have fixed the\nlabels to, like, 0 to 9. And now if this sample--",
    "start": "1437490",
    "end": "1442565"
  },
  {
    "text": "if we are doing the\nT-shot training, then we sample T\nclasses out of it.",
    "start": "1442565",
    "end": "1448450"
  },
  {
    "text": "So how would that perform? Mapping-- because\nif we have T classes and we have 4, 7, and 9 then\nhow will that [INAUDIBLE] affect",
    "start": "1448450",
    "end": "1456460"
  },
  {
    "text": "that kind of work?  So I guess one thing\nthat I'll mention here",
    "start": "1456460",
    "end": "1463050"
  },
  {
    "text": "is that, there are still-- so in this example, you're\ndoing 5 way classification.",
    "start": "1463050",
    "end": "1470070"
  },
  {
    "text": "You have more than\nfive classes total in your meta-training set. But you're going to be\nassigning multiple examples",
    "start": "1470070",
    "end": "1475410"
  },
  {
    "text": "to the same label. For example, the label of 4 is\nboth being assigned to piano and also being\nassigned to carousel.",
    "start": "1475410",
    "end": "1482130"
  },
  {
    "text": "And so basically, the\nclassification task now becomes, like, is it a piano\nand a carousel, or a carousel,",
    "start": "1482130",
    "end": "1488280"
  },
  {
    "text": "or is it some\nbarrels, or a singer, or is it this kind of dog\nor this tank, and so forth.",
    "start": "1488280",
    "end": "1496350"
  },
  {
    "text": "You could also have something\nwhere you basically assign-- you have N possible labels, N\npossible meta-training classes,",
    "start": "1496350",
    "end": "1505679"
  },
  {
    "text": "and assign a different label\nfor piano and for carousel. You'll also run into\nthe same problem",
    "start": "1505680",
    "end": "1510870"
  },
  {
    "text": "there because you'll still-- at meta-test time,\nyou'll still be giving it",
    "start": "1510870",
    "end": "1516660"
  },
  {
    "text": "kind of new classes\nand it won't figure out how to make a prediction\nfor those classes.",
    "start": "1516660",
    "end": "1523090"
  },
  {
    "text": "I had another question. The last slide where you showed\nthat it won't be a problem? Can you go to that slide?",
    "start": "1523090",
    "end": "1528500"
  },
  {
    "text": " This slide? Next slide, there is a example.",
    "start": "1528500",
    "end": "1533922"
  },
  {
    "text": " Oh, yeah. So I didn't get how\nthe second problem no,",
    "start": "1533922",
    "end": "1540544"
  },
  {
    "text": "if we see the same image\nclasses as training?  Yeah, so this\nbasically corresponds--",
    "start": "1540545",
    "end": "1548590"
  },
  {
    "text": "what it is essentially\nreverting to is just a supervised\nmeta-image classifier that ignores the\ntraining examples",
    "start": "1548590",
    "end": "1554530"
  },
  {
    "text": "and just classifies\nthe test examples. And so if you see the\nsame image classes,",
    "start": "1554530",
    "end": "1559750"
  },
  {
    "text": "if piano is one of the\nmeta-training classes and you also see pianos at meta\ntest time, then this is fine.",
    "start": "1559750",
    "end": "1565659"
  },
  {
    "text": "Because you'll get good\naccuracy on things like pianos. The issue is if you see\nnew image classes than what",
    "start": "1565660",
    "end": "1573580"
  },
  {
    "text": "you saw during\ntraining, it won't be able to figure out\nwhat the correct label is for that class.",
    "start": "1573580",
    "end": "1578740"
  },
  {
    "text": " Meta-learning does\nnot have much benefit. It's on this [INAUDIBLE] of\nthe non-specific learning.",
    "start": "1578740",
    "end": "1586900"
  },
  {
    "text": " Yes, there are\nsome nuances there.",
    "start": "1586900",
    "end": "1593080"
  },
  {
    "text": "Like, maybe you\nwant to classify-- maybe different tasks\ncorrespond to classifying pianos in different regions of\nthe world or something",
    "start": "1593080",
    "end": "1599497"
  },
  {
    "text": "and they look different. But yeah, in general,\nthis is a setting",
    "start": "1599497",
    "end": "1604720"
  },
  {
    "text": "that you wouldn't want to\napply meta-learning to anyway. Thank you.",
    "start": "1604720",
    "end": "1611360"
  },
  {
    "start": "1611000",
    "end": "1801000"
  },
  {
    "text": "OK. So let's look at a\ncouple other examples where this might come up\nand potentially convince you",
    "start": "1611360",
    "end": "1619730"
  },
  {
    "text": "that this is maybe a problem\nthat's a bit more concerning. So we haven't really talked\nabout reinforcement learning",
    "start": "1619730",
    "end": "1626210"
  },
  {
    "text": "yet, but say we\nhave a robot and we want to be able to train\nacross a wide range of tasks and then be able to quickly\nlearn how to do a new task.",
    "start": "1626210",
    "end": "1634110"
  },
  {
    "text": "And here are some of\nthe training tasks. These are actually\nreal training tasks from this benchmark\ncalled Meta-World.",
    "start": "1634110",
    "end": "1641090"
  },
  {
    "text": "And different tasks--\nlike, one task corresponds to closing a drawer, one task\ncorresponds to using a hammer",
    "start": "1641090",
    "end": "1647490"
  },
  {
    "text": "to hit a nail. Another task corresponds\nto stacking an object. And maybe when you tell,\nfor these sets of tasks,",
    "start": "1647490",
    "end": "1654150"
  },
  {
    "text": "you tell it both, like,\nthis language description of what you want it\nto do, and you also",
    "start": "1654150",
    "end": "1659190"
  },
  {
    "text": "allow it to collect\nsome data for that task. And you want to be\nable to quickly learn",
    "start": "1659190",
    "end": "1665430"
  },
  {
    "text": "from a very small amount of\ndata how to do that task. Now, the tricky part\nis, if you give it",
    "start": "1665430",
    "end": "1671550"
  },
  {
    "text": "both the data and\nthis description, it may just memorize a policy\nfrom each of these descriptions",
    "start": "1671550",
    "end": "1679230"
  },
  {
    "text": "to the correct actions for\nthat task and ignores the data and doesn't actually\nleverage the data",
    "start": "1679230",
    "end": "1684930"
  },
  {
    "text": "in order to quickly adapt. Then when you give it a new\ntask like closing a box, it doesn't know how to\ninterpret close box.",
    "start": "1684930",
    "end": "1692040"
  },
  {
    "text": "And it previously\nwas ignoring the data when it was learning how to\nsolve each of these tasks.",
    "start": "1692040",
    "end": "1697810"
  },
  {
    "text": "So it's stuck. It doesn't know\nhow to do the task and it also doesn't know how\nto quickly adapt from data.",
    "start": "1697810",
    "end": "1704669"
  },
  {
    "text": "So this is kind of another\npotentially more real problem where memorization would\nreally bite in some way.",
    "start": "1704670",
    "end": "1713289"
  },
  {
    "text": " Essentially, if you tell the\nrobot what the task goal is,",
    "start": "1713290",
    "end": "1719250"
  },
  {
    "text": "then the robot can just\nignore any trial data that you allow it to use. ",
    "start": "1719250",
    "end": "1725990"
  },
  {
    "text": "And then lastly, one\nmore example of this, is say you want to predict\nthe pose of objects,",
    "start": "1725990",
    "end": "1734182"
  },
  {
    "text": "and specifically you want\nto predict the orientation. So you want to predict the\nangle at which the object is",
    "start": "1734182",
    "end": "1739240"
  },
  {
    "text": "facing you. And you have data for\ndifferent kinds of objects.",
    "start": "1739240",
    "end": "1746580"
  },
  {
    "text": "For example, this couch is at 45\ndegrees here, 108 degrees here. You kind of do\nthe standard thing",
    "start": "1746580",
    "end": "1751970"
  },
  {
    "text": "of separating out\ntraining datasets and test datasets for each task.",
    "start": "1751970",
    "end": "1757200"
  },
  {
    "text": "Now again, what the model\ncould learn to do here is memorize the\ncanonical orientation",
    "start": "1757200",
    "end": "1763070"
  },
  {
    "text": "for each of your\ntraining objects and use that to learn a\nclassifier for each of the test examples and effectively learn\nto ignore all of the training",
    "start": "1763070",
    "end": "1770750"
  },
  {
    "text": "examples. And then when it's\ngiven a new object, it doesn't know what the\ncanonical orientation",
    "start": "1770750",
    "end": "1776270"
  },
  {
    "text": "for that object is and\nit hasn't figured out how to use the training\nexamples to infer the canonical orientation.",
    "start": "1776270",
    "end": "1782850"
  },
  {
    "text": "So when you're given\na chair, for example, It isn't able to figure out what\nthe orientation of that chair is from a small number\nof training examples.",
    "start": "1782850",
    "end": "1789810"
  },
  {
    "text": " OK, great.",
    "start": "1789810",
    "end": "1796730"
  },
  {
    "text": "So essentially, the model can\nmemorize economical orientation",
    "start": "1796730",
    "end": "1802020"
  },
  {
    "start": "1801000",
    "end": "1900000"
  },
  {
    "text": "of the training examples. And I'd like to address the\nquestion in the chat, which is, can this be considered\nas some form of over fitting,",
    "start": "1802020",
    "end": "1808570"
  },
  {
    "text": "as the meta-train and\nmeta test performance will be quite different? And that's kind\nof exactly right.",
    "start": "1808570",
    "end": "1814590"
  },
  {
    "text": "You can essentially view this\nas a form of meta overfitting.",
    "start": "1814590",
    "end": "1820380"
  },
  {
    "text": "And maybe I'll actually\nskip forward to one slide to describe that and\nthen we'll get to what",
    "start": "1820380",
    "end": "1826559"
  },
  {
    "text": "we can do about it in a minute. So if we kind of compare\nmeta-learning and supervised",
    "start": "1826560",
    "end": "1832410"
  },
  {
    "text": "learning, we'll refer to this\nproblem as meta overfitting where it's essentially\nmemorizing training functions",
    "start": "1832410",
    "end": "1840195"
  },
  {
    "text": "f corresponding to each of\nthe meta-training tasks. ",
    "start": "1840195",
    "end": "1846620"
  },
  {
    "text": "And this is somewhat analogous\nto standard overfitting except in standard overfitting\nin supervised learning,",
    "start": "1846620",
    "end": "1852140"
  },
  {
    "text": "you're memorizing the labels\nfor training data points. ",
    "start": "1852140",
    "end": "1857610"
  },
  {
    "text": "So you can view it as like\na meta-learning analog of standard overfitting, where\ninstead of memorizing training",
    "start": "1857610",
    "end": "1862650"
  },
  {
    "text": "data points, you're memorizing\nthe training data functions. ",
    "start": "1862650",
    "end": "1869150"
  },
  {
    "text": "You're essentially memorizing\nthe functions that solve each of the training tasks. And then when you try to\ngeneralize to a new task that",
    "start": "1869150",
    "end": "1875120"
  },
  {
    "text": "requires a new function,\nyou are able to figure that out from data just like how,\nin standard overfitting,",
    "start": "1875120",
    "end": "1881120"
  },
  {
    "text": "you're not able to figure out\nthe labels for new examples if you simply memorized\neach of the labels",
    "start": "1881120",
    "end": "1886280"
  },
  {
    "text": "in your training set.  OK.",
    "start": "1886280",
    "end": "1892870"
  },
  {
    "text": "So now let's talk about a\npotential solution to this. ",
    "start": "1892870",
    "end": "1900900"
  },
  {
    "start": "1900000",
    "end": "2031000"
  },
  {
    "text": "So to kind of recap where we're\nat, if the tasks are mutually exclusive, then a single\nfunction cannot solve all",
    "start": "1900900",
    "end": "1908270"
  },
  {
    "text": "of the tasks. This may be due to\nlabel shuffling. It may be due to\nhiding some information from the meta-learner.",
    "start": "1908270",
    "end": "1914167"
  },
  {
    "text": "But if the tasks are\nnot mutually exclusive, a single function can\nsolve all of the tasks. And when we're in\nthis situation,",
    "start": "1914167",
    "end": "1920440"
  },
  {
    "text": "there's actually\nmultiple solutions to the meta-learning problem. So if your meta-learner\nis something",
    "start": "1920440",
    "end": "1926880"
  },
  {
    "text": "that takes as input a training\ndataset and a test input, then one solution is\nto essentially memorize",
    "start": "1926880",
    "end": "1933600"
  },
  {
    "text": "information in theta and\nignore the training examples. For example, in the\npose prediction example,",
    "start": "1933600",
    "end": "1939350"
  },
  {
    "text": "it could memorize\nthe information about the canonical poses\nof all of the training objects in theta and then\nignore the training dataset.",
    "start": "1939350",
    "end": "1948840"
  },
  {
    "text": "And then another solution\nis to carry new information about the canonical pose\nin your parameter's theta",
    "start": "1948840",
    "end": "1955440"
  },
  {
    "text": "and instead infer that\ninformation from the training examples. ",
    "start": "1955440",
    "end": "1962250"
  },
  {
    "text": "And both of these solutions\nto the meta-training problem will achieve good accuracy\non the meta-training set.",
    "start": "1962250",
    "end": "1969750"
  },
  {
    "text": "However, only this\nexample will be able to generalize to new tasks. ",
    "start": "1969750",
    "end": "1976317"
  },
  {
    "text": "And it's also worth mentioning\nthat there isn't just two solutions. There actually is essentially\nan entire spectrum of solutions",
    "start": "1976317",
    "end": "1983179"
  },
  {
    "text": "based on how information flows. So this is kind of\none extreme where",
    "start": "1983180",
    "end": "1988547"
  },
  {
    "text": "all the information\nabout the canonical pose comes from the\ntraining data set. This is another extreme where\nall of it comes from theta.",
    "start": "1988547",
    "end": "1994580"
  },
  {
    "text": "But you may have kind of a\ncontinuous spectrum of where information is flowing\nfrom, whether more",
    "start": "1994580",
    "end": "1999711"
  },
  {
    "text": "of it is flowing from\ntheta or more of it is flowing from the\ntraining dataset. ",
    "start": "1999712",
    "end": "2005950"
  },
  {
    "text": "OK. Now, the cool part about\nthis is this actually suggests a potential solution.",
    "start": "2005950",
    "end": "2011340"
  },
  {
    "text": "Which is that, if we can\ncontrol how information flows, we can encourage it\nto essentially try",
    "start": "2011340",
    "end": "2018800"
  },
  {
    "text": "to gather more information from\nthe training samples and less information from theta.",
    "start": "2018800",
    "end": "2025850"
  },
  {
    "text": " OK.",
    "start": "2025850",
    "end": "2031720"
  },
  {
    "start": "2031000",
    "end": "2549000"
  },
  {
    "text": "So with that in\nmind, we'll develop a meta-regularization method to\ntry to mitigate this problem.",
    "start": "2031720",
    "end": "2039320"
  },
  {
    "text": "And in particular, one\noption that we might do is try to maximize\nthe mutual information",
    "start": "2039320",
    "end": "2045100"
  },
  {
    "text": "between the predicted label\nand the training dataset given your test example.",
    "start": "2045100",
    "end": "2051679"
  },
  {
    "text": "And if we're maximizing\nthis information flow, this will encourage it\nto acquire information from D train rather\nthan from theta.",
    "start": "2051679",
    "end": "2060949"
  },
  {
    "text": "Unfortunately, at least\nin our experiences, it was a little bit difficult\nto try to actually quantify",
    "start": "2060949",
    "end": "2066730"
  },
  {
    "text": "and optimize this term. So what we did\ninstead is we tried to minimize both the\nmeta-training loss as well",
    "start": "2066730",
    "end": "2074770"
  },
  {
    "text": "as the information in theta. And this second term\nwill essentially",
    "start": "2074770",
    "end": "2080379"
  },
  {
    "text": "be a regularization\nterm that encourages it to store less information\nin theta so that it then",
    "start": "2080380",
    "end": "2087610"
  },
  {
    "text": "is encouraged to-- as a byproduct, acquire more\ninformation from D train.",
    "start": "2087610",
    "end": "2095199"
  },
  {
    "text": "And there's actually a\npretty nice way to do this. So the meta-training\nloss just looks like the standard\nmeta-training losses",
    "start": "2095199",
    "end": "2101380"
  },
  {
    "text": "that we've been\nlooking at before. And then there's a way\nto minimize a bound",
    "start": "2101380",
    "end": "2106420"
  },
  {
    "text": "on the information in\ntheta by essentially placing a distribution\non our weight's theta.",
    "start": "2106420",
    "end": "2113720"
  },
  {
    "text": "So we're going to have a\ndistribution with mean theta mu and variance theta\nsigma, and we're",
    "start": "2113720",
    "end": "2119140"
  },
  {
    "text": "going to basically\nminimize the KL divergence between this\ndistribution on our weights",
    "start": "2119140",
    "end": "2125170"
  },
  {
    "text": "and some prior distribution. And specifically\nwhat this corresponds",
    "start": "2125170",
    "end": "2130180"
  },
  {
    "text": "to is basically just adding\nnoise to our weights. So instead of having a kind of\ndeterministic weight vector,",
    "start": "2130180",
    "end": "2137920"
  },
  {
    "text": "we're going to add noise\naccording to the variance here and encourage that\nnoise to not go",
    "start": "2137920",
    "end": "2145319"
  },
  {
    "text": "to 0 by enforcing this prior. And then the kind\nof noisy weights that are sampled from\nthis distribution",
    "start": "2145320",
    "end": "2152010"
  },
  {
    "text": "will be used in our\nmeta-training loss. Now, of course, if you only\nhave the meta-training loss,",
    "start": "2152010",
    "end": "2157540"
  },
  {
    "text": "then you wouldn't-- and\nyou only have this term, then it's used to store no\ninformation or a minimal amount",
    "start": "2157540",
    "end": "2163059"
  },
  {
    "text": "of information in your weights. So it is really a\nregularization term that's trying to encourage\nit to lead to the solution",
    "start": "2163060",
    "end": "2169185"
  },
  {
    "text": "rather than the solution that\nignores the training examples. ",
    "start": "2169185",
    "end": "2175350"
  },
  {
    "text": "OK. So kind of in\nessence, this places precedence on using information\nfrom the training example",
    "start": "2175350",
    "end": "2182260"
  },
  {
    "text": "over storing\ninformation in theta. And if you're interested in\nlearning more about how this",
    "start": "2182260",
    "end": "2188790"
  },
  {
    "text": "is a bound on\ninformation in theta, you can look at some work\nby Alex Alemi as an example.",
    "start": "2188790",
    "end": "2198080"
  },
  {
    "text": "So Alemi et al, let\nme give you the paper maybe from a few years ago\nthat looks at basically how",
    "start": "2198080",
    "end": "2205809"
  },
  {
    "text": "this is a bound on\ninformation in data and has some kind of\ndescription about how",
    "start": "2205810",
    "end": "2211930"
  },
  {
    "text": "you could optimize this. ",
    "start": "2211930",
    "end": "2216982"
  },
  {
    "text": "OK, and then\n[AUDIO OUT] is asking, is there a common choice\nfor the prior over theta? Maybe standard normal?",
    "start": "2216982",
    "end": "2222418"
  },
  {
    "text": "Yeah, that's right. So typically, the kind of most\ncommon choice for p of theta",
    "start": "2222418",
    "end": "2227859"
  },
  {
    "text": "is just a standard normal\nGaussian distribution with mean 0 and variance 1.",
    "start": "2227860",
    "end": "2233559"
  },
  {
    "text": " And the reason why this\nis really easy to optimize",
    "start": "2233560",
    "end": "2239360"
  },
  {
    "text": "is that you can\nactually write down the KL divergence for two\nGaussians in closed form as a function of the\nmean and the variance,",
    "start": "2239360",
    "end": "2246560"
  },
  {
    "text": "which makes it pretty\neasy to optimize this. And then, I guess there's other\nthings that we'll actually",
    "start": "2246560",
    "end": "2253357"
  },
  {
    "text": "cover next lecture that\nwill also make this easier to optimize that essentially\nallow you to differentiate through the samples\nfrom a Gaussian.",
    "start": "2253357",
    "end": "2260840"
  },
  {
    "text": "But we won't get\ninto that today. OK. Do you have a question?",
    "start": "2260840",
    "end": "2267030"
  },
  {
    "text": "Yeah, I have a question. Like for [INAUDIBLE]\nmemorization is like the\nzero-shot [INAUDIBLE] on this meta-training\ndata will be",
    "start": "2267030",
    "end": "2273320"
  },
  {
    "text": "quite high if the [INAUDIBLE]\nhas memorized a lot.",
    "start": "2273320",
    "end": "2278820"
  },
  {
    "text": "Yeah, exactly. So if it's memorized a lot,\nthen the zero-shot performance will be great for\nthe training classes.",
    "start": "2278820",
    "end": "2285210"
  },
  {
    "text": "So can you not just make a\nloss and maximize this loss because we know\nexplicitly we don't",
    "start": "2285210",
    "end": "2291789"
  },
  {
    "text": "want zero-shot\nperformance to be high. Yeah, so that's actually\na reasonable idea.",
    "start": "2291790",
    "end": "2298320"
  },
  {
    "text": "And I think that\nthere's a paper that maybe tried to do this,\nwhere you basically try to have minimal\npre-update performance,",
    "start": "2298320",
    "end": "2308130"
  },
  {
    "text": "zero-shot performance,\nand great post update, few-shot performance.",
    "start": "2308130",
    "end": "2313870"
  },
  {
    "text": "The tricky part is\nthat it turns out that the model can\nactually still achieve",
    "start": "2313870",
    "end": "2321210"
  },
  {
    "text": "this while also memorizing. Basically, even if it memorizes\nthe canonical pose information",
    "start": "2321210",
    "end": "2329820"
  },
  {
    "text": "in theta, it may distort in\na way that isn't accessible",
    "start": "2329820",
    "end": "2337030"
  },
  {
    "text": "or it may just kind of shuffle\nthe last part, the last layer of the network or\nsomething in some way such that it kind of looks\nlike it isn't memorizing,",
    "start": "2337030",
    "end": "2346020"
  },
  {
    "text": "but it actually is. It's maybe-- I'm not sure if\nthat was the best explanation.",
    "start": "2346020",
    "end": "2352480"
  },
  {
    "text": "But essentially, even if the\nkind of pre-update performance looks really\nterrible, that doesn't",
    "start": "2352480",
    "end": "2357558"
  },
  {
    "text": "mean that it isn't memorizing\nthe information in the network. It could just be that it's kind\nof disguising that information",
    "start": "2357558",
    "end": "2362683"
  },
  {
    "text": "and then exposing it\nafter a gradient update or after you're actually\ngiving it test examples.",
    "start": "2362683",
    "end": "2368619"
  },
  {
    "text": " OK, [AUDIO OUT] is\nasking, is this KL term",
    "start": "2368620",
    "end": "2375358"
  },
  {
    "text": "more like a Bayesian method\nwhere we are learning a distribution over data? Yeah, that's exactly right.",
    "start": "2375358",
    "end": "2380490"
  },
  {
    "text": "So another name for this\nis Bayes by Backprop.",
    "start": "2380490",
    "end": "2386070"
  },
  {
    "text": "And I do feel like there's a\nlot of different names for this. Whoops. ",
    "start": "2386070",
    "end": "2392730"
  },
  {
    "text": "And it's actually a common-- it's worth mentioning\nthis is actually",
    "start": "2392730",
    "end": "2398110"
  },
  {
    "text": "a common approach for\nBayesian regularization on neural networks in general.",
    "start": "2398110",
    "end": "2403570"
  },
  {
    "text": "Although, in general,\nit hasn't been that popular for regularizing\nneural networks for supervised learning.",
    "start": "2403570",
    "end": "2408650"
  },
  {
    "start": "2408650",
    "end": "2414890"
  },
  {
    "text": "Yeah, I wanted to ask,\nby assuming a prior, and using this [INAUDIBLE]\nloss to make sure",
    "start": "2414890",
    "end": "2421640"
  },
  {
    "text": "that it follows this\nparticular distribution, how are we making sure that\nit's not going to memorize?",
    "start": "2421640",
    "end": "2427880"
  },
  {
    "text": "It can still memorize\ninformation, right, even if it follows\nthat same distribution?",
    "start": "2427880",
    "end": "2434910"
  },
  {
    "text": "So it makes it harder\nto memorize information. Or it discourages it from\nmemorizing information",
    "start": "2434910",
    "end": "2440180"
  },
  {
    "text": "because you're basically\nadding noise to the weights and then using the\nbasically weight sampled",
    "start": "2440180",
    "end": "2448190"
  },
  {
    "text": "from this distribution\nover weights when making predictions. Because you're\nadding noise, that",
    "start": "2448190",
    "end": "2453350"
  },
  {
    "text": "makes it harder to store\ninformation in the weights. And as you make it harder\nto store information,",
    "start": "2453350",
    "end": "2460160"
  },
  {
    "text": "that is encouraging\nit to use information from other means if\npossible, and will implicitly",
    "start": "2460160",
    "end": "2466760"
  },
  {
    "text": "encourage it to use information\nfrom the training examples instead. It's worth mentioning that\npart of the reason why-- well,",
    "start": "2466760",
    "end": "2475310"
  },
  {
    "text": "also I'll get into how well\nthis works on the next slide. But part of the reason why\nwe might expect this to work",
    "start": "2475310",
    "end": "2481130"
  },
  {
    "text": "is that both of\nthese two solutions achieve a very good\nmeta-training loss.",
    "start": "2481130",
    "end": "2487141"
  },
  {
    "text": "And we're simply\njust trying to tip the balance towards\nthis latter solution",
    "start": "2487142",
    "end": "2493280"
  },
  {
    "text": "rather than kind of completely\nchanging what it does. And so we really only need a\nfairly weak regularization term",
    "start": "2493280",
    "end": "2499280"
  },
  {
    "text": "to try to tip the balance\ntowards the solution. And it turns out that\nthings like adding noise to the weights to decrease the\namount of information in theta",
    "start": "2499280",
    "end": "2507260"
  },
  {
    "text": "is enough to tip that balance\ntowards this latter solution. ",
    "start": "2507260",
    "end": "2517857"
  },
  {
    "text": "Okay, and then someone's\nasking, is the theta here the pre-update or\nthe post-update? So theta is either the\npre-update weights in MAML",
    "start": "2517857",
    "end": "2525250"
  },
  {
    "text": "or they are the meta parameters\nin a black box, black box meta-learning model.",
    "start": "2525250",
    "end": "2530470"
  },
  {
    "text": " OK, and then it's\nalso worth mentioning",
    "start": "2530470",
    "end": "2537120"
  },
  {
    "text": "that you can combine this with\nyour favorite meta-learning algorithm. You can combine it both\nwith black box methods",
    "start": "2537120",
    "end": "2543810"
  },
  {
    "text": "and with\noptimization-based methods. ",
    "start": "2543810",
    "end": "2549640"
  },
  {
    "start": "2549000",
    "end": "2755000"
  },
  {
    "text": "So I'll just quickly\ngo over the results and then I'll answer\na few more questions. So for non-mutually exclusive\nOmniglot, which is basically",
    "start": "2549640",
    "end": "2558615"
  },
  {
    "text": "Omniglot where you have a\nconsistent assignment of labels to classes, again we see\nthat MAML's is quite poorly.",
    "start": "2558615",
    "end": "2566610"
  },
  {
    "text": "There's this other method called\ntask agnostic meta-learning for few-shot learning that\nactually does something similar",
    "start": "2566610",
    "end": "2572279"
  },
  {
    "text": "to what was suggested\nbefore, where you encouraged the pre-update update\nweights of MAML to--",
    "start": "2572280",
    "end": "2578023"
  },
  {
    "text": "I believe they're\nencouraging it to have high entropy, which would\nimplicitly be low performance.",
    "start": "2578023",
    "end": "2583557"
  },
  {
    "text": "This also doesn't perform well. It's essentially able\nto disguise the fact that it's memorizing.",
    "start": "2583558",
    "end": "2588960"
  },
  {
    "text": "Whereas if you apply this\nmeta-regularization to MAML and specifically apply\nthe meta-regularization",
    "start": "2588960",
    "end": "2594960"
  },
  {
    "text": "on the weights, then you\nsee a really dramatic change in the performance\nbecause it's essentially",
    "start": "2594960",
    "end": "2600990"
  },
  {
    "text": "tipping the balance towards\nthe solution that's actually using the training examples. ",
    "start": "2600990",
    "end": "2609880"
  },
  {
    "text": "And then on the post-prediction\ntask that I mentioned, we see also kind of a pretty\nbig improvement in terms of mean",
    "start": "2609880",
    "end": "2617320"
  },
  {
    "text": "squared error,\nfrom 5.39 to 2.26. Also, if you combine\nthe meta-regularization",
    "start": "2617320",
    "end": "2622359"
  },
  {
    "text": "to conditional neural processes,\nwhich is a kind of black box meta-learner, you also see\na pretty big improvement",
    "start": "2622360",
    "end": "2628420"
  },
  {
    "text": "in performance. It's also not quite as simple\nas standard regularization.",
    "start": "2628420",
    "end": "2633640"
  },
  {
    "text": "So if you apply just,\nlike, weight decay or Bayes by Backprop to conditional\nneural processes, to say,",
    "start": "2633640",
    "end": "2641200"
  },
  {
    "text": "all of the weights\nin the model, not just the ones that correspond\nto the meta-parameters,",
    "start": "2641200",
    "end": "2646240"
  },
  {
    "text": "then you don't see as\nbig of an improvement compared to the\nmeta-regularization that I just",
    "start": "2646240",
    "end": "2652630"
  },
  {
    "text": "described.  OK, you have a question?",
    "start": "2652630",
    "end": "2658570"
  },
  {
    "start": "2658570",
    "end": "2663800"
  },
  {
    "text": "I'm sorry. I wanted to ask, does dropout\nhelp with meta-learning? Just like, it's also adding\nnoise to your parameters?",
    "start": "2663800",
    "end": "2671840"
  },
  {
    "text": "Yeah, so dropout is\nactually adding noise to the activations, not to\nthe parameters themselves.",
    "start": "2671840",
    "end": "2678650"
  },
  {
    "text": "If you actually\nlook at this paper, \"Meta-Learning\nWithout Memorization,\" there are experiments where\nyou add noise to the activation",
    "start": "2678650",
    "end": "2685250"
  },
  {
    "text": "in the form of an information\nbottleneck rather than adding noise to the weights. And that performs well in some\ncases, but not in all cases,",
    "start": "2685250",
    "end": "2693315"
  },
  {
    "text": "and there's an\nexplanation in the paper for why we might expect\nadding noise on the weights to work better. Yeah, thanks.",
    "start": "2693315",
    "end": "2698392"
  },
  {
    "start": "2698392",
    "end": "2703760"
  },
  {
    "text": "Yes, so when I say that this\ntheta contains information,",
    "start": "2703760",
    "end": "2709450"
  },
  {
    "text": "it is possible for us to predict\nsome of the task's results",
    "start": "2709450",
    "end": "2716450"
  },
  {
    "text": "based on this theta value? Can we train a network\nto do the task?",
    "start": "2716450",
    "end": "2722609"
  },
  {
    "text": " So you're asking, can you\npredict the performance",
    "start": "2722610",
    "end": "2730029"
  },
  {
    "text": "from the weights of the\nnetwork or something else? Yes, yes, from the weights. Because you mentioned\nthat this theta might",
    "start": "2730030",
    "end": "2736490"
  },
  {
    "text": "contain some information. So from this theta, we might\nbe able to memorize something",
    "start": "2736490",
    "end": "2743870"
  },
  {
    "text": "and we might be able to perform\nthe task from this theta value. ",
    "start": "2743870",
    "end": "2756410"
  },
  {
    "text": "Let's see. So you're saying that you\nwant to predict, basically,",
    "start": "2756410",
    "end": "2764460"
  },
  {
    "text": "from the mean of the\nparameters or the variance? No, no, not for this\nKL divergence term.",
    "start": "2764460",
    "end": "2771240"
  },
  {
    "text": "So this is a kind of\nproblem of meta overfitting.",
    "start": "2771240",
    "end": "2777130"
  },
  {
    "text": "So you mentioned that this theta\nmay store some information.",
    "start": "2777130",
    "end": "2784599"
  },
  {
    "text": "And I'm just-- Yeah, yeah, I see. I see what you're asking. Yeah so, you could\nbasically try to understand if the information about the\ncanonical poses and theta.",
    "start": "2784600",
    "end": "2792442"
  },
  {
    "text": "Maybe a little bit\nlike training a model to predict the canonical\nposes from theta or something like that.",
    "start": "2792443",
    "end": "2798550"
  },
  {
    "text": "That's a good question. So I think that in\nthe general case, you may not\nnecessarily know what",
    "start": "2798550",
    "end": "2805600"
  },
  {
    "text": "the thing that it'll memorize\nis, like the canonical pose. Although in some\ncases, you might.",
    "start": "2805600",
    "end": "2812360"
  },
  {
    "text": "I think you could\npotentially try to train a model to\npredict a canonical pose and then you could use\nan adversarial objective",
    "start": "2812360",
    "end": "2819590"
  },
  {
    "text": "to encourage it not to be able\nto contain that specific amount of information. That would a\nreasonable approach,",
    "start": "2819590",
    "end": "2826760"
  },
  {
    "text": "and you can potentially also\ndo this on the activations rather than the weights or both.",
    "start": "2826760",
    "end": "2833000"
  },
  {
    "text": "We haven't experimented\nwith that explicitly, but it seems like a pretty\nreasonable approach for--",
    "start": "2833000",
    "end": "2841682"
  },
  {
    "text": "reasonable thing to investigate. Yeah, thank you. ",
    "start": "2841682",
    "end": "2850470"
  },
  {
    "text": "Yeah, so I was thinking\nabout data augmentation. That during\nmeta-training, if you",
    "start": "2850470",
    "end": "2856920"
  },
  {
    "text": "want to add random noise\nor adversarial noise to the Gauss\ndistribution, I mean,",
    "start": "2856920",
    "end": "2863069"
  },
  {
    "text": "would that help\nsolve this problem because now we are forcing\nit to look at the data",
    "start": "2863070",
    "end": "2870060"
  },
  {
    "text": "and not memorize? Yeah, so there is actually\na paper, a very recent paper",
    "start": "2870060",
    "end": "2875670"
  },
  {
    "start": "2873000",
    "end": "3033000"
  },
  {
    "text": "that I think came out on Arxiv. I think it's going to\nbe at NeurIPS this year,",
    "start": "2875670",
    "end": "2881610"
  },
  {
    "text": "on trying to use data\naugmentation to mitigate this memorization problem.",
    "start": "2881610",
    "end": "2887740"
  },
  {
    "text": "And so I'd refer\nyou to that paper if you're kind of interested\nin looking into that.",
    "start": "2887740",
    "end": "2893890"
  },
  {
    "text": "But kind of the short\nstory is that they were able to get pretty\npromising results by using",
    "start": "2893890",
    "end": "2899520"
  },
  {
    "text": "different forms of\ndata augmentation to solve the problem. OK, thanks.",
    "start": "2899520",
    "end": "2905248"
  },
  {
    "text": " OK, and then there\nare a couple questions in the chat that I\nwant to try to get to.",
    "start": "2905248",
    "end": "2911860"
  },
  {
    "text": "So [AUDIO OUT] is\nasking, can you elaborate on what theta\nmean and theta variance is? Are they the mean and variance\nof all dimensions of theta?",
    "start": "2911860",
    "end": "2919480"
  },
  {
    "text": "Yes, so that's correct. Well, they're basically\nthe mean and variance of the meta-parameters.",
    "start": "2919480",
    "end": "2926049"
  },
  {
    "text": "They are quite high dimensional\nif theta is high dimensional. In principle, you\ncould potentially",
    "start": "2926050",
    "end": "2931329"
  },
  {
    "text": "apply them to only a subset\nof the meta parameters. And I think that's actually in\nthe MAML results that I showed,",
    "start": "2931330",
    "end": "2938290"
  },
  {
    "text": "we found it most effective to\napply them most predominantly to the encoder of the network\nto encourage the later layers",
    "start": "2938290",
    "end": "2947800"
  },
  {
    "text": "to still adapt with data.  Yeah, but kind of\nyou can think of them",
    "start": "2947800",
    "end": "2956270"
  },
  {
    "text": "essentially as having\nthe dimensionality of the parameters and being a\nfixed vector, one for the mean,",
    "start": "2956270",
    "end": "2963317"
  },
  {
    "text": "and one fixed vector\nfor the variance. ",
    "start": "2963317",
    "end": "2970401"
  },
  {
    "text": "[INAUDIBLE] is asking,\nis there a concept of dropout for the\nthetas in meta-learning? It sounds like applying\nGaussian noise would be equivalent to that.",
    "start": "2970402",
    "end": "2978160"
  },
  {
    "text": "I think dropout would be\nprobably a little bit trickier to implement\nbecause, well, it may",
    "start": "2978160",
    "end": "2984247"
  },
  {
    "text": "be a little bit trickier because\nonce you drop-- when you drop out a weight value to 0, it may\nbe difficult for the network",
    "start": "2984247",
    "end": "2990850"
  },
  {
    "text": "to compensate for that. But I'm not sure. I haven't thought\nabout that too much. When you have noise, though,\nit's a little bit easier",
    "start": "2990850",
    "end": "2997648"
  },
  {
    "text": "to differentiate through. And then [AUDIO OUT]\nis asking, does applying meta-regularization\nto shuffled Omniglot",
    "start": "2997648",
    "end": "3004480"
  },
  {
    "text": "improve performance compared to\nthe non-regularized Omniglot? So if you're in the kind of\nstandard Omniglot setting",
    "start": "3004480",
    "end": "3011440"
  },
  {
    "text": "and you apply this,\nI think that it leads to roughly similar\nperformance, maybe slightly better performance empirically.",
    "start": "3011440",
    "end": "3018934"
  },
  {
    "text": " OK, for the sake\nof time, I'm going",
    "start": "3018934",
    "end": "3024930"
  },
  {
    "text": "to keep on going because\nI think that we should try to get to the second part.",
    "start": "3024930",
    "end": "3031530"
  },
  {
    "text": " I don't think we have time\nto go over this slide.",
    "start": "3031530",
    "end": "3037102"
  },
  {
    "start": "3033000",
    "end": "3069000"
  },
  {
    "text": "But just one quick\nthing worth mentioning is that you can actually\ntheoretically analyze this approach and\nkind of actually",
    "start": "3037102",
    "end": "3044030"
  },
  {
    "text": "derive a generalization bound on\nthe performance that basically looks something like the\nerror on the meta-training set",
    "start": "3044030",
    "end": "3051320"
  },
  {
    "text": "plus the\nmeta-regularization term. And so this allows you\nto actually recover",
    "start": "3051320",
    "end": "3056690"
  },
  {
    "text": "the meta-regularized objective\nfrom this PAC Bayes bound, is what it's called.",
    "start": "3056690",
    "end": "3063358"
  },
  {
    "text": "If you're interested in that,\nI'll let you look at the slides or look at the paper offline.",
    "start": "3063358",
    "end": "3069349"
  },
  {
    "start": "3069000",
    "end": "3143000"
  },
  {
    "text": "And then to summarize\nthe memorization problem, we already talked about how\nit's essentially analogous, it's",
    "start": "3069350",
    "end": "3076860"
  },
  {
    "text": "kind of a meta-learning analog\nof standard overfitting, whereas instead of memorizing\nthe training data points,",
    "start": "3076860",
    "end": "3082820"
  },
  {
    "text": "we're memorizing the\ntraining functions for each of the tasks. And the solution to it, or\nat least one solution to it,",
    "start": "3082820",
    "end": "3090360"
  },
  {
    "text": "there are potentially other\nsolutions that could maybe be even more effective, is\nto control information flow.",
    "start": "3090360",
    "end": "3097410"
  },
  {
    "text": "And this is in contrast\nto standard regularization in supervised learning\nwhere you try to regularize",
    "start": "3097410",
    "end": "3104110"
  },
  {
    "text": "the hypothesis class. Meta-regularization\ncan also be viewed as trying to regularize\nthe description",
    "start": "3104110",
    "end": "3110550"
  },
  {
    "text": "length of the\nmeta-parameters, which is somewhat analogous\nto regularizing the hypothesis class.",
    "start": "3110550",
    "end": "3116700"
  },
  {
    "text": "Although it's worth noting\nin practice that things like Bayesian regularization\nand placing",
    "start": "3116700",
    "end": "3121800"
  },
  {
    "text": "a distribution\nover the parameters hasn't been nearly as successful\nin supervised learning",
    "start": "3121800",
    "end": "3127200"
  },
  {
    "text": "as the results that\nI showed previously when you have the memorization\nproblem in meta-learning.",
    "start": "3127200",
    "end": "3132470"
  },
  {
    "text": " OK, so now let's talk a\nbit about meta-learning",
    "start": "3132470",
    "end": "3139100"
  },
  {
    "text": "when you don't have\ntasks provided. So we talked a bit\nabout how tasks",
    "start": "3139100",
    "end": "3146380"
  },
  {
    "start": "3143000",
    "end": "3599000"
  },
  {
    "text": "can come from different sources\ndepending on the application domain that you're in.",
    "start": "3146380",
    "end": "3152750"
  },
  {
    "text": "But in a lot of\napplication areas, we may not even have data\nfrom other tasks or data",
    "start": "3152750",
    "end": "3159190"
  },
  {
    "text": "from other domains\nthat may allow us to help solve the problem. So what I want to talk about\nfor the rest of the lecture is,",
    "start": "3159190",
    "end": "3166660"
  },
  {
    "text": "what if we only\nhave unlabeled data? Can essentially the algorithm\npropose tasks itself",
    "start": "3166660",
    "end": "3174410"
  },
  {
    "text": "from unlabeled data that\nallows it to adapt to new tasks",
    "start": "3174410",
    "end": "3182690"
  },
  {
    "text": "at meta-test time? In particular, our goal will\nbe to do few-shot meta-learning",
    "start": "3182690",
    "end": "3188100"
  },
  {
    "text": "from unlabeled images\nor unlabeled text so that we can then actually\nbe able to solve few-shot image",
    "start": "3188100",
    "end": "3196660"
  },
  {
    "text": "classification problems or\nfew-shot text classification problems that are defined\nby humans at meta-test time.",
    "start": "3196660",
    "end": "3204420"
  },
  {
    "text": "So kind of the problem setup\nwill be during meta-training,",
    "start": "3204420",
    "end": "3210140"
  },
  {
    "text": "we'll just have basically\na bunch of unlabeled data,",
    "start": "3210140",
    "end": "3217760"
  },
  {
    "text": "and then at meta-test time, we\nwill have a training dataset",
    "start": "3217760",
    "end": "3227830"
  },
  {
    "text": "that's labeled. And also some test\ninputs that you want to make predictions for.",
    "start": "3227830",
    "end": "3234490"
  },
  {
    "text": "So meta-test time will\nlook identical to what we had before. What's different is that\nduring meta-training time,",
    "start": "3234490",
    "end": "3239980"
  },
  {
    "text": "we just have\nunlabeled data, and we need to be able\nto construct tasks from that unlabeled data\nthat will prepare us for what",
    "start": "3239980",
    "end": "3247017"
  },
  {
    "text": "happens at meta-test time. ",
    "start": "3247017",
    "end": "3253730"
  },
  {
    "text": "OK, so first, kind\nof a general recipe for what this class of\napproaches have typically done.",
    "start": "3253730",
    "end": "3261040"
  },
  {
    "text": "Is first, given\nsome unlabeled data, you want to propose tasks\nfrom that unlabeled data.",
    "start": "3261040",
    "end": "3266758"
  },
  {
    "text": "And once you propose\ntasks from that, you can then run\nmeta-learning on those tasks. So pretty simple, right?",
    "start": "3266758",
    "end": "3273799"
  },
  {
    "text": "Obviously, the hard part\nis the task proposing part. And that's what we'll talk about\nprimarily on the next slides.",
    "start": "3273800",
    "end": "3281932"
  },
  {
    "text": "And so really, the goal of\nunsupervised meta-learning methods are to automatically\nconstruct tasks from the unlabelled data.",
    "start": "3281932",
    "end": "3287380"
  },
  {
    "text": " OK, so I have another\nquestion for all of you, which",
    "start": "3287380",
    "end": "3294690"
  },
  {
    "text": "is, if you were to propose\ntasks from unlabeled data,",
    "start": "3294690",
    "end": "3301290"
  },
  {
    "text": "what do you want that\ndistribution over tasks to look like? Do you have any thoughts on how\nyou might try to propose tasks",
    "start": "3301290",
    "end": "3309750"
  },
  {
    "text": "in a way that prepares\nyou for the tasks that you might see\nat meta-test time? ",
    "start": "3309750",
    "end": "3321190"
  },
  {
    "text": "Have a Gaussian distribution. A Gaussian distribution. So what do you mean by\na Gaussian distribution?",
    "start": "3321190",
    "end": "3328640"
  },
  {
    "text": "Unevenly distributed\nwith a mean at which-- so let's say we have a mean\naverage, we have a task",
    "start": "3328640",
    "end": "3335119"
  },
  {
    "text": "and then there's some variance\nand tasks at each variance. ",
    "start": "3335120",
    "end": "3340660"
  },
  {
    "text": "Yeah, in evenly\ndistribution, that seemed like a\nreasonable suggestion.",
    "start": "3340660",
    "end": "3348070"
  },
  {
    "text": "I'm not sure how to specify\nGaussian distribution over tasks if the\ntask is-- well, yeah,",
    "start": "3348070",
    "end": "3353680"
  },
  {
    "text": "that seems very reasonable.  Do you have thoughts\non what the mean",
    "start": "3353680",
    "end": "3359450"
  },
  {
    "text": "and variance of that Gaussian\ndistribution would be? ",
    "start": "3359450",
    "end": "3366579"
  },
  {
    "text": "No. OK, cool. That's a good start.",
    "start": "3366580",
    "end": "3373490"
  },
  {
    "text": "[AUDIO OUT] Do\nyou have thoughts? It seems like proposing tasks\nis almost similar to coming up",
    "start": "3373490",
    "end": "3379670"
  },
  {
    "text": "with labels for the unlabelled\ndata set in such a way that you want to\nget the labels that",
    "start": "3379670",
    "end": "3385039"
  },
  {
    "text": "helps you learn the tests\nbetter as good as possible. So you want to\ncome up with labels",
    "start": "3385040",
    "end": "3391820"
  },
  {
    "text": "that are most similar to your\ntest task, if that makes sense.",
    "start": "3391820",
    "end": "3398790"
  },
  {
    "text": "Yeah. So if you had a way to\ncome up with labels for all of your training data set,\nthen one thing you could do is",
    "start": "3398790",
    "end": "3404930"
  },
  {
    "text": "construct kind of tasks that are\nfrom that kind of labelled data",
    "start": "3404930",
    "end": "3412980"
  },
  {
    "text": "set similar to how\nwe typically do, and you want those labels to\nbe reflective of the kinds of labels that we might\nsee at test times.",
    "start": "3412980",
    "end": "3420550"
  },
  {
    "text": "So it's a good thought.  Yeah, so for a\nclassification problem,",
    "start": "3420550",
    "end": "3427280"
  },
  {
    "text": "for example, we can look\nat a data distribution in a different space, like\nin space or something,",
    "start": "3427280",
    "end": "3434320"
  },
  {
    "text": "and we could cluster\ndifferent distribution and each could correspond to\na different class, something",
    "start": "3434320",
    "end": "3441490"
  },
  {
    "text": "like that.  Yeah, so you could basically\ntry to have different kinds",
    "start": "3441490",
    "end": "3449920"
  },
  {
    "text": "of tasks, potentially. Is that what you're suggesting? Yeah.",
    "start": "3449920",
    "end": "3455700"
  },
  {
    "text": "Cool. Other thoughts from [AUDIO OUT]",
    "start": "3455700",
    "end": "3463260"
  },
  {
    "text": "So you might want to find\na network set of tasks. Like, coming up with a data set.",
    "start": "3463260",
    "end": "3470124"
  },
  {
    "text": "So it's like partitioning, like\nto have each set [INAUDIBLE] as soon as possible.",
    "start": "3470124",
    "end": "3475910"
  },
  {
    "text": "But each task internally\ncontains examples of meta-based supervision.",
    "start": "3475910",
    "end": "3481385"
  },
  {
    "text": "Yeah, so this is also similar I\nthink to what maybe [AUDIO OUT] was suggesting, is that\nit seems like we probably want to use all of our data\nset and somewhat evenly use",
    "start": "3481385",
    "end": "3489380"
  },
  {
    "text": "our data set rather\nthan focusing on different parts\nof the data set or skewing our\ndistribution in some way.",
    "start": "3489380",
    "end": "3496937"
  },
  {
    "text": "There are also some\nsuggestions in the chat, such as using self supervision\nsignals to propose tasks",
    "start": "3496937",
    "end": "3503150"
  },
  {
    "text": "and to partition the unlabeled\ndata into clean clusters, maybe where different\nclusters are different tasks.",
    "start": "3503150",
    "end": "3510930"
  },
  {
    "text": "So thinking about this\nproblem, the two things that I came up with for this\nslide is a diversity of tasks.",
    "start": "3510930",
    "end": "3519560"
  },
  {
    "text": "And I think this is\nwhat you all meant by trying to have more\nevenly distributed tasks.",
    "start": "3519560",
    "end": "3526070"
  },
  {
    "text": "Essentially, if you\nhave more evenly distributed tasks or\nmore diverse tasks, you're more likely to\ncover the test task.",
    "start": "3526070",
    "end": "3532650"
  },
  {
    "text": "So the test task is more likely\nto be in that set of tasks if you have a\ndiverse set of tasks.",
    "start": "3532650",
    "end": "3537960"
  },
  {
    "text": "Now, the extreme,\nthough, if you have too diverse of a\nset of tasks, then",
    "start": "3537960",
    "end": "3544220"
  },
  {
    "text": "you may not be\nable to actually-- your meta-learner\nmay not actually be able to pick up on any\nstructure in that set of tasks.",
    "start": "3544220",
    "end": "3550580"
  },
  {
    "text": "And you also need\nyour task to be structured so that few-shot\nmeta-learning is even possible.",
    "start": "3550580",
    "end": "3556869"
  },
  {
    "text": "For example, at the\nextreme where you just sample completely random\ntasks from your data, there won't be any\nstructure to pick up on",
    "start": "3556870",
    "end": "3564380"
  },
  {
    "text": "in order to learn efficiently\nwith a small number of examples. And so we're going to try to\naim for both diverse tasks",
    "start": "3564380",
    "end": "3570020"
  },
  {
    "text": "and structured tasks.  OK, so we'll talk first about\nhow you might construct tasks",
    "start": "3570020",
    "end": "3577540"
  },
  {
    "text": "from unlabeled\nimage data and then we'll talk about constructing\ntasks from text data. ",
    "start": "3577540",
    "end": "3584740"
  },
  {
    "text": "So first, let's\ntalk about labeling from unlabelled-- meta-learning\nfrom unlabeled images. And the approach\nhere is actually",
    "start": "3584740",
    "end": "3591390"
  },
  {
    "text": "going to be quite similar\nto the approach suggested in the chat, which is to\npartition unlabeled data",
    "start": "3591390",
    "end": "3596670"
  },
  {
    "text": "into clean clusters. And in particular,\nwhat we're going to do is we're first going to\nrun an unsupervised learning",
    "start": "3596670",
    "end": "3604190"
  },
  {
    "text": "algorithm to get\nsome embedding space, this is because working in\nthe original image space is quite challenging.",
    "start": "3604190",
    "end": "3610830"
  },
  {
    "text": "So this will allow us to embed\nimages in our meta-training set",
    "start": "3610830",
    "end": "3616070"
  },
  {
    "text": "to a lower dimensional embedding\nspace shown here, visualized",
    "start": "3616070",
    "end": "3621260"
  },
  {
    "text": "in 2D for easy visualization. And then what we'll\ndo is we'll run",
    "start": "3621260",
    "end": "3626480"
  },
  {
    "text": "clustering in this latent space\njust using k-means clustering.",
    "start": "3626480",
    "end": "3632630"
  },
  {
    "text": "And we'll propose tasks\nas classification tasks of classifying between\nimages in different clusters.",
    "start": "3632630",
    "end": "3641920"
  },
  {
    "text": "And so concretely what\nthis can look like is we'll cluster, actually,\nmultiple times. So one run of k-means\nclustering may give us",
    "start": "3641920",
    "end": "3648490"
  },
  {
    "text": "something like this. Another run may give\nus something like this. And then to construct\na task, we'll",
    "start": "3648490",
    "end": "3655900"
  },
  {
    "text": "sample two clusters,\nmaybe the red cluster and the blue\ncluster, and the task will be to classify images\nfrom one cluster versus images",
    "start": "3655900",
    "end": "3664450"
  },
  {
    "text": "in another cluster. So we'll construct our training\ndata set as these two examples right here and we'll\nconstruct our test example",
    "start": "3664450",
    "end": "3672820"
  },
  {
    "text": "as these two images right here. So this will give us a kind of a\ntwo way one-shot classification",
    "start": "3672820",
    "end": "3680579"
  },
  {
    "text": "task. And we can then construct\nanother task, for example, maybe by looking at\nthis purple cluster",
    "start": "3680580",
    "end": "3686170"
  },
  {
    "text": "versus this green cluster. And we can also construct\nN-way classification tasks",
    "start": "3686170",
    "end": "3691390"
  },
  {
    "text": "by sampling and\ndestroying clusters.  And then, of\ncourse, once we have",
    "start": "3691390",
    "end": "3697799"
  },
  {
    "text": "these tasks of discriminating\nbetween different clusters, we can then run our favorite\nmeta-learning algorithm.",
    "start": "3697800",
    "end": "3705292"
  },
  {
    "text": " OK, and then the\nresult of this is",
    "start": "3705292",
    "end": "3710460"
  },
  {
    "text": "kind of a representation that's\nsuitable for really quickly learning downstream\ntasks because we're training it to be\nable to distinguish",
    "start": "3710460",
    "end": "3717539"
  },
  {
    "text": "between different clusters\ngiven a small data set. ",
    "start": "3717540",
    "end": "3725710"
  },
  {
    "text": "OK, so this is kind\nof the first approach. How do you get the\nembedding space?",
    "start": "3725710",
    "end": "3730750"
  },
  {
    "text": "For this, you can\nuse kind of existing unsupervised representation\nlearning methods out of the box",
    "start": "3730750",
    "end": "3736480"
  },
  {
    "text": "like BiGAN or DeepCluster. There are also more recent\napproaches than this as well.",
    "start": "3736480",
    "end": "3742760"
  },
  {
    "text": "W1K means clustering and\nproposed cluster discrimination tasks. We refer to this\nmethod as clustering",
    "start": "3742760",
    "end": "3749170"
  },
  {
    "text": "to automatically construct tasks\nfor unsupervised meta-learning, or CACTUS. And then we run meta-learning\non top of those tasks.",
    "start": "3749170",
    "end": "3759140"
  },
  {
    "text": "So if you're interested in\nwhat a MAML on top of a CACTUS looks like, you get\nsomething like this.",
    "start": "3759140",
    "end": "3765350"
  },
  {
    "text": "And if you look at\nminiImageNet, if you run MAML with kind\nof full labels",
    "start": "3765350",
    "end": "3771310"
  },
  {
    "text": "on the meta-training set,\nyou get 62% accuracy. If you compare two\ndifferent approaches",
    "start": "3771310",
    "end": "3777670"
  },
  {
    "text": "of learning on top of the\ninitial representation space, you get lower accuracy, but you\nstill do somewhat reasonably.",
    "start": "3777670",
    "end": "3787470"
  },
  {
    "text": "If you run MAML on top\nof the clustered tasks, you get five-shot\naccuracy around 51%.",
    "start": "3787470",
    "end": "3795150"
  },
  {
    "text": "And if you use, actually,\nthe deep cluster latent space instead, you get 50-- almost 54% accuracy on five\nway five-shot miniImageNet.",
    "start": "3795150",
    "end": "3805830"
  },
  {
    "text": "And this is without any labels\nin the meta-training set. Just using the unlabeled\nimages in the miniImageNet",
    "start": "3805830",
    "end": "3811290"
  },
  {
    "text": "meta-training set. So this is pretty cool. We can actually\nstart to approach",
    "start": "3811290",
    "end": "3817400"
  },
  {
    "text": "the performance of\na fully supervised meta-learning algorithm. And you really see kind of\nthe same trend of results",
    "start": "3817400",
    "end": "3825259"
  },
  {
    "text": "for four different embedding\nmethods for datasets other than miniImageNet, like\nOmniglot, CelebA, and MNIST.",
    "start": "3825260",
    "end": "3831440"
  },
  {
    "text": "Also for prototypical\nnetworks and for test tasks that have larger data sets\nthat are larger than five-shot,",
    "start": "3831440",
    "end": "3837880"
  },
  {
    "text": "for example.  OK, so that's the first\napproach that we can take.",
    "start": "3837880",
    "end": "3844360"
  },
  {
    "text": "Now, another thing\nworth considering is maybe, in some domains, we\nhave some knowledge about what",
    "start": "3844360",
    "end": "3850270"
  },
  {
    "text": "might make a good task. So, for example, in the realm\nof image classification,",
    "start": "3850270",
    "end": "3856760"
  },
  {
    "text": "we know that if we do\ncertain things to the image, the label won't change.",
    "start": "3856760",
    "end": "3861877"
  },
  {
    "text": "So if you drop out\nsome of the pixels, or you translate the\nimage a little bit, or if you reflect the image from\nleft to right, then typically,",
    "start": "3861877",
    "end": "3870089"
  },
  {
    "text": "the label of that\nimage won't change. For example, if you\ntake this Omniglot digit",
    "start": "3870090",
    "end": "3875700"
  },
  {
    "text": "and translate it a\nlittle bit and drop out some of the pixels, it\nstill has the same identity.",
    "start": "3875700",
    "end": "3882350"
  },
  {
    "text": "Likewise, if you take an\nimage from miniImageNet and you reflect it left,\nright, then this image",
    "start": "3882350",
    "end": "3887830"
  },
  {
    "text": "still has the same class\nas the image before. And so we can use\nthis domain knowledge",
    "start": "3887830",
    "end": "3894280"
  },
  {
    "text": "to develop a way\nto propose tasks by simply using augmented\nversions of our examples",
    "start": "3894280",
    "end": "3904290"
  },
  {
    "text": "as additional examples\nfrom that class. So in particular, what this\nlooks like is for each task",
    "start": "3904290",
    "end": "3911450"
  },
  {
    "text": "that we want to construct,\nwe randomly sample N images and assign the labels 1\nto N. So, for example,",
    "start": "3911450",
    "end": "3918330"
  },
  {
    "text": "maybe we sample\nthese three images and assign them\ncorresponding labels.",
    "start": "3918330",
    "end": "3923450"
  },
  {
    "text": "And then for each of\nthese data points, we augment the image\nusing our domain knowledge",
    "start": "3923450",
    "end": "3929270"
  },
  {
    "text": "and assign them the same\ncorresponding labels. And this top row can correspond\nto the training data set",
    "start": "3929270",
    "end": "3935359"
  },
  {
    "text": "for that example,\nand the bottom row can correspond to the test set.",
    "start": "3935360",
    "end": "3941230"
  },
  {
    "text": "So we're basically using\nour domain knowledge about how examples about\nintercost variation",
    "start": "3941230",
    "end": "3947620"
  },
  {
    "text": "within a class and using that\nto construct new examples",
    "start": "3947620",
    "end": "3952660"
  },
  {
    "text": "about how the models should\nbe able to generalize after training on\nthese examples. ",
    "start": "3952660",
    "end": "3961490"
  },
  {
    "text": "So this is another way\nto construct tasks. And again, you can also, once\nyou construct tasks using",
    "start": "3961490",
    "end": "3967560"
  },
  {
    "text": "your unlabeled data\nset, you can then apply this with a\nmeta-learning algorithm. It's worth mentioning that\nevery once in a while,",
    "start": "3967560",
    "end": "3973320"
  },
  {
    "text": "you will sample images that\nhave the same underlying class. And in practice,\nthat's often OK.",
    "start": "3973320",
    "end": "3983130"
  },
  {
    "text": "It will give you kind\nof more diverse tasks and some of the tasks that\nyou're giving your meta-learner won't reflect the kinds of\ntasks you see at meta-test time.",
    "start": "3983130",
    "end": "3990359"
  },
  {
    "text": "What's most important is\nthat the meta-test task is within the convex hall of the\ntasks that you're training on.",
    "start": "3990360",
    "end": "3996450"
  },
  {
    "text": "[AUDIO OUT] is asking,\nis this similar to self supervised learning?",
    "start": "3996450",
    "end": "4001770"
  },
  {
    "text": "Yes. So this sort of\ndata augmentation has also been pretty successful\nin the realm of self supervised",
    "start": "4001770",
    "end": "4007611"
  },
  {
    "text": "learning as well.  And so if you take\nthis approach,",
    "start": "4007612",
    "end": "4013120"
  },
  {
    "text": "and for Omniglot, use\ntranslation and pixel dropout for augmentation,\nand for miniImageNet, you use a mixture of\ntranslation, rotation, and sear",
    "start": "4013120",
    "end": "4020859"
  },
  {
    "text": "based on this kind of\nauto-augment approach in the paper by Cubuk et al. We see results that\nlooks like this.",
    "start": "4020860",
    "end": "4028330"
  },
  {
    "text": "To summarize, it is\nreally outstanding Omniglot performance. The performance actually almost\napproaches the fully supervised",
    "start": "4028330",
    "end": "4036850"
  },
  {
    "text": "performance. So in the 5 way 5-shot setting,\nit's like, 95% versus 98%.",
    "start": "4036850",
    "end": "4042400"
  },
  {
    "text": "On the 20 way 5-shot\nsetting, it's 92% versus 96%.",
    "start": "4042400",
    "end": "4047569"
  },
  {
    "text": "This is because, in Omniglot,\nwe have pretty good domain knowledge about the\nimages of Omniglot.",
    "start": "4047570",
    "end": "4054430"
  },
  {
    "text": "And then in miniImageNet,\nthe performance is slightly below\nthat of CACTUS. This is probably because we\ndon't have quite as good domain",
    "start": "4054430",
    "end": "4061820"
  },
  {
    "text": "knowledge about all of\nthe kinds of variation that natural images\nhave within the class.",
    "start": "4061820",
    "end": "4067250"
  },
  {
    "text": "And it's hard to construct forms\nof argumentation that cover all of that interclass variation.",
    "start": "4067250",
    "end": "4073262"
  },
  {
    "start": "4073262",
    "end": "4079257"
  },
  {
    "text": "You have a question?  Yeah. Do you think you can go back to\nthe first slide with the title,",
    "start": "4079257",
    "end": "4088060"
  },
  {
    "text": "can we meta-learn with\nonly unlabelled images? Yeah.",
    "start": "4088060",
    "end": "4093730"
  },
  {
    "text": "So I'm not sure if\nI'm interpreting this the wrong way, but\nit seems like this is just",
    "start": "4093730",
    "end": "4100388"
  },
  {
    "text": "kind of teaching--\nor it's learning how to classify images\nthat are visually similar",
    "start": "4100389",
    "end": "4106210"
  },
  {
    "text": "based on the tasks\nthat are created.",
    "start": "4106210",
    "end": "4111350"
  },
  {
    "text": "So how are we supposed\nto gain an image classification of,\nsay, classifying a fruit as a fruit versus a\njellyfish as a jellyfish out",
    "start": "4111350",
    "end": "4119439"
  },
  {
    "text": "of this? Yeah, so I guess\nit's worth mentioning",
    "start": "4119439",
    "end": "4125880"
  },
  {
    "text": "that the goal of\nthis kind of approach isn't to get a\nclassifier out of it.",
    "start": "4125880",
    "end": "4132680"
  },
  {
    "text": "The goal is to be able to do\nfew-shot classification at test time.",
    "start": "4132680",
    "end": "4137899"
  },
  {
    "text": "And so these tasks don't\nhave to be perfect. And you don't have\nto perfectly match",
    "start": "4137899",
    "end": "4143318"
  },
  {
    "text": "all the tasks you're going to\nbe seeing at meta-test time. You simply want to cover a\npretty diverse range of tasks",
    "start": "4143319",
    "end": "4148568"
  },
  {
    "text": "that you might potentially\nsee at meta-test time. ",
    "start": "4148569",
    "end": "4153880"
  },
  {
    "text": "And if it learns to generally\ndistinguish things--",
    "start": "4153880",
    "end": "4158889"
  },
  {
    "text": "distinguish a diverse range\nof things, then maybe-- from a small amount of\nexamples of those things, then at test time, it may\nbe able to also distinguish",
    "start": "4158890",
    "end": "4166899"
  },
  {
    "text": "things in the\nmeta-test tasks based on a small number of examples\nfor your meta-test task.",
    "start": "4166899",
    "end": "4174790"
  },
  {
    "text": "So at test time--\nsorry, go ahead. Oh, go ahead. So at test time, would we\nbe giving examples that",
    "start": "4174790",
    "end": "4181350"
  },
  {
    "text": "are all within the same class? That, say, we would\nwant to classify,",
    "start": "4181350",
    "end": "4186609"
  },
  {
    "text": "or would it also be a\nwide variety of things? Yeah, so at test time.",
    "start": "4186609",
    "end": "4191689"
  },
  {
    "text": "Maybe you want to kind\nof be able to classify between your two\npets or something, and so you would\ngive it five images",
    "start": "4191689",
    "end": "4197020"
  },
  {
    "text": "of one pet, five\nimages of another pet, and then adopt\nthe model that was meta-trained on these examples\nto your classification problem.",
    "start": "4197020",
    "end": "4207580"
  },
  {
    "text": "Got it. Thanks. It's also worth mentioning\nthat this approach is probably",
    "start": "4207580",
    "end": "4213489"
  },
  {
    "text": "by no means perfect and\nthere's probably a lot that we could do to\nimprove upon this. It's also very challenging to,\nin a completely unsupervised",
    "start": "4213490",
    "end": "4220930"
  },
  {
    "text": "way, distinguish\ndifferent classes. I think the power\nof this though,",
    "start": "4220930",
    "end": "4226130"
  },
  {
    "text": "is that these tasks\ndon't necessarily need to exactly correspond\nto true semantically",
    "start": "4226130",
    "end": "4232540"
  },
  {
    "text": "meaningful tasks as long as\nyou have essentially a means to cover a broad range\nof structured tasks.",
    "start": "4232540",
    "end": "4239469"
  },
  {
    "start": "4239470",
    "end": "4244790"
  },
  {
    "text": "Yeah, so [AUDIO OUT]\nthese proposed tasks are designed to help to train\nthe image encoder, which",
    "start": "4244790",
    "end": "4250070"
  },
  {
    "text": "will be used at test time. So these tasks will\nbe used to train kind of a meta-learning\nalgorithm,",
    "start": "4250070",
    "end": "4255380"
  },
  {
    "text": "such as a set of initial\nparameters with MAML. And then at test time, you\nwant to run gradient descent,",
    "start": "4255380",
    "end": "4261800"
  },
  {
    "text": "or run your kind of prototypical\nnetworks or something on a small data set and be able\nto transfer to that new task.",
    "start": "4261800",
    "end": "4272400"
  },
  {
    "start": "4272400",
    "end": "4280570"
  },
  {
    "text": "Yeah, so I was just wondering,\nif I understand it correctly, the CACTUS approach,\nwhich was the clustering,",
    "start": "4280570",
    "end": "4286060"
  },
  {
    "text": "is an alternative to the one\nwhere you are incorporating domain knowledge, right?",
    "start": "4286060",
    "end": "4291080"
  },
  {
    "text": "They are two\ndifferent approaches? Yeah, these are basically\ntwo different approaches. Yeah, right. So in CACTUS, you just\ndon't incorporate any domain",
    "start": "4291080",
    "end": "4297935"
  },
  {
    "text": "knowledge and you use\n[INAUDIBLE] to define task. But in this, guess,\nUMTRE, you call it,",
    "start": "4297935",
    "end": "4304990"
  },
  {
    "text": "you just augment existing\ndata, like images, and you use that\nas your test cog.",
    "start": "4304990",
    "end": "4311530"
  },
  {
    "text": "If that's the case,\nthen in this UMTRE case, what happens if you, let's say,\nsample three or four same breed",
    "start": "4311530",
    "end": "4320004"
  },
  {
    "text": "dog images in your N.\nWouldn't they then consider",
    "start": "4320004",
    "end": "4326230"
  },
  {
    "text": "this, like, distinct classes? Yeah, so that's kind\nof one of the failure modes of this kind\nof UMTRE approach,",
    "start": "4326230",
    "end": "4335090"
  },
  {
    "text": "is that, with some\nprobability, you will be sampling images\nfrom the same class. Luckily, if you\nhave a large number",
    "start": "4335090",
    "end": "4341420"
  },
  {
    "text": "of classes in your\nmeta-training set, that will happen with\nrelatively low probability.",
    "start": "4341420",
    "end": "4347400"
  },
  {
    "text": "And again, even if\nsome of your tasks aren't representative\nof the kinds of things that you see at\nmeta-test time, that's",
    "start": "4347400",
    "end": "4353780"
  },
  {
    "text": "OK as long as the\npredominant part",
    "start": "4353780",
    "end": "4359900"
  },
  {
    "text": "of your meta-training tasks that\nare proposed by the algorithm do cover the meta-test tasks.",
    "start": "4359900",
    "end": "4367500"
  },
  {
    "text": "But this is a downside\nof this approach. Cool, thanks. ",
    "start": "4367500",
    "end": "4375989"
  },
  {
    "text": "OK. And I was briefly talking\nabout meta-learning with only unlabeled text. So the first option\nhere is to formulate it",
    "start": "4375990",
    "end": "4382770"
  },
  {
    "text": "as a language modeling problem. This is an example-- this is what we saw when we\nlooked at GPT-3 for example.",
    "start": "4382770",
    "end": "4390150"
  },
  {
    "text": "Where D train is a\nsequence of characters, D test is the following\nsequence of characters, and we saw with\nhow, basically, you",
    "start": "4390150",
    "end": "4396960"
  },
  {
    "text": "can use a lot of\nunlabeled text data to train a model such\nthat it can learn things",
    "start": "4396960",
    "end": "4402449"
  },
  {
    "text": "like simple math problems,\nspelling, correction, and translating\nbetween languages.",
    "start": "4402450",
    "end": "4408489"
  },
  {
    "text": "So this is one\nsuccessful approach for using only unlabeled data.",
    "start": "4408490",
    "end": "4415949"
  },
  {
    "text": "When might we not want\nto use this option? So I think that\nthis sort of option is a bit harder to combine\nwith optimization-based",
    "start": "4415950",
    "end": "4423150"
  },
  {
    "text": "meta-learning because\nyou'd have to somehow adapt your model with gradient\ndescent on the unlabeled text",
    "start": "4423150",
    "end": "4430260"
  },
  {
    "text": "that you're given. And I also think\nthat it may be harder to apply the\nclassification tasks",
    "start": "4430260",
    "end": "4436940"
  },
  {
    "text": "because the model is just\noutputting kind of text rather than classification\nlabels, such as labels",
    "start": "4436940",
    "end": "4444590"
  },
  {
    "text": "for sentiment classification\nor political bias. And so I think that you could\napply it to classification",
    "start": "4444590",
    "end": "4451828"
  },
  {
    "text": "tasks with a bit of\nwork, but it's probably a bit harder to do\nbecause you have to process the natural language\noutput that it produces.",
    "start": "4451828",
    "end": "4457490"
  },
  {
    "text": " OK. So another option that was\nactually recently proposed",
    "start": "4457490",
    "end": "4464410"
  },
  {
    "text": "by Bansal et al,\nactually came up on Arxiv within the\npast month or two,",
    "start": "4464410",
    "end": "4470469"
  },
  {
    "text": "is to construct tasks\nby masking out words. So different tasks\nwill basically",
    "start": "4470470",
    "end": "4476990"
  },
  {
    "text": "correspond to classifying\nthe word that was masked out and trying to figure\nout what that word was.",
    "start": "4476990",
    "end": "4483720"
  },
  {
    "text": "Specifically, what this\nlooks like is for each task, you can sample a subset\nof n unique words,",
    "start": "4483720",
    "end": "4490559"
  },
  {
    "text": "assign each of those unique\nwords a different label ID. So, for example, maybe you\nsample two words, democratic,",
    "start": "4490560",
    "end": "4497790"
  },
  {
    "text": "and capital. You assign them the ID of 1\nand 2 or the label of 1 and 2.",
    "start": "4497790",
    "end": "4504360"
  },
  {
    "text": "And then you sample\nsentences with each of those words\nmasking out that word",
    "start": "4504360",
    "end": "4511350"
  },
  {
    "text": "and construct the training\nset and the test set with the masked sentences and\nthe corresponding word IDs",
    "start": "4511350",
    "end": "4517800"
  },
  {
    "text": "as the label. So as a concrete\nexample, your support set might look like this, where\nthis is kind of democratic",
    "start": "4517800",
    "end": "4525210"
  },
  {
    "text": "and capital again, and\nthe word that was selected is masked out.",
    "start": "4525210",
    "end": "4531060"
  },
  {
    "text": "And the model needs to predict\nwhich word was masked out.",
    "start": "4531060",
    "end": "4537160"
  },
  {
    "text": "So this has kind of\nboth inputs and labels. And then the test data set\ncorresponds to a new sentence",
    "start": "4537160",
    "end": "4545520"
  },
  {
    "text": "with that word masked out\nand the corresponding label. ",
    "start": "4545520",
    "end": "4552130"
  },
  {
    "text": "This is a pretty\nsimple way to construct a large number of\nclassification tasks from text,",
    "start": "4552130",
    "end": "4560680"
  },
  {
    "text": "and it works pretty well. So they can have,\nactually, a lot of results.",
    "start": "4560680",
    "end": "4566103"
  },
  {
    "text": "Everything to the\nleft of this bar is kind of using an entirely\nunsupervised pretraining approach.",
    "start": "4566103",
    "end": "4571347"
  },
  {
    "text": "And then everything on the\nright also incorporates some supervised tasks. If we first look at that\nentirely unsupervised",
    "start": "4571347",
    "end": "4578719"
  },
  {
    "text": "pretraining results, BERT\nis, like, a standard self supervised approach where you--",
    "start": "4578720",
    "end": "4584150"
  },
  {
    "text": "it's actually quite\nsimilar to the masked-- to the method proposed here.",
    "start": "4584150",
    "end": "4589760"
  },
  {
    "text": "Whereas SMLNT is the proposed\nunsupervised meta-learning algorithm that we\njust went over.",
    "start": "4589760",
    "end": "4596090"
  },
  {
    "text": "Once we see that, in general,\nthese approaches tend to perform pretty\nsimilarly, except for, perhaps, on the\ndisaster data set",
    "start": "4596090",
    "end": "4602000"
  },
  {
    "text": "where the unsupervised\nmeta-learning approach does a lot better. And then in this more\nsupervised setting,",
    "start": "4602000",
    "end": "4609530"
  },
  {
    "text": "they compared two multi-task\nlearning approaches where you train on a number\nof supervised learning tasks",
    "start": "4609530",
    "end": "4615530"
  },
  {
    "text": "and then fine tune\non a new task. They also compared\na LEOPARD, which is an optimization based\nmodel learner based",
    "start": "4615530",
    "end": "4620870"
  },
  {
    "text": "on MAML, where this\nis trained also only on the supervised tasks. And then they compared to\ntheir hybrid method, which does",
    "start": "4620870",
    "end": "4628010"
  },
  {
    "text": "unsupervised learning\non their proposed tasks, but also integrates the\nsupervised learning tasks",
    "start": "4628010",
    "end": "4633230"
  },
  {
    "text": "into the meta-learner. And overall, they find that\nthis meta-learning approach that kind of incorporates\nboth unsupervised",
    "start": "4633230",
    "end": "4639755"
  },
  {
    "text": "meta-learning tasks and\nsupervised meta-learning tasks is able to do\nsignificantly better",
    "start": "4639755",
    "end": "4645410"
  },
  {
    "text": "than these other approaches\nthat don't leverage the unsupervised tasks or don't\nleverage the supervised tasks.",
    "start": "4645410",
    "end": "4652430"
  },
  {
    "text": " OK, so we're\nrunning out of time,",
    "start": "4652430",
    "end": "4658620"
  },
  {
    "text": "so we'll skip this last part.  And then, yeah, to recap, we've\ntalked about memorization,",
    "start": "4658620",
    "end": "4667190"
  },
  {
    "text": "we talked about different\nways to propose tasks from unlabeled data. This gives us a class of\nunsupervised meta-learning",
    "start": "4667190",
    "end": "4673130"
  },
  {
    "text": "algorithms. And yeah, I covered\nthe goals of trying to talk about how and when\nmemorization will occur",
    "start": "4673130",
    "end": "4680869"
  },
  {
    "text": "and also techniques\nfor constructing tasks. I have a question regarding this\nlanguage classification task",
    "start": "4680870",
    "end": "4688160"
  },
  {
    "text": "as few-shot data mining\nthe class using-- like, sampling different\nwords and masking them.",
    "start": "4688160",
    "end": "4695120"
  },
  {
    "text": "So do you think it would matter\nif the classification task can be made harder in the sense--",
    "start": "4695120",
    "end": "4702020"
  },
  {
    "text": "instead of sampling words\nlike democratic and capital, let's say we sample words\nlike democratic and republic.",
    "start": "4702020",
    "end": "4708330"
  },
  {
    "text": "And because those two\nwords can fill the context, but the model need to\nknow some other knowledge",
    "start": "4708330",
    "end": "4714570"
  },
  {
    "text": "as well to find out whether\nthe democratic word would be the best choice\nor the republic word.",
    "start": "4714570",
    "end": "4721350"
  },
  {
    "text": "Yeah, that's a great idea. I think that if you were to\ncompletely randomly sample words, then you'll probably\nget words that are very",
    "start": "4721350",
    "end": "4728159"
  },
  {
    "text": "easy to distinguish between. Like, if you\nsampled, for example, between democratic and with,\nor rival, or something, or even",
    "start": "4728160",
    "end": "4739199"
  },
  {
    "text": "words that are nouns\nversus adjectives, as in this example too.",
    "start": "4739200",
    "end": "4744250"
  },
  {
    "text": "So yeah, I think that some sort\nof way to encourage the method to find harder pairs of words\nand harder sets of words",
    "start": "4744250",
    "end": "4753150"
  },
  {
    "text": "would probably,\nactually, lead to better performance from these methods. OK.",
    "start": "4753150",
    "end": "4758540"
  },
  {
    "text": " One more note on\nthat is you may also want to make it not too hard.",
    "start": "4758540",
    "end": "4764890"
  },
  {
    "text": "So if, for example, the\ntwo words that you pick are, like, purple\nand blue, and there are many situations where\nyou can't necessarily",
    "start": "4764890",
    "end": "4772890"
  },
  {
    "text": "distinguish when-- maybe she has a purple water\nbottle or a blue water bottle. You can't really know which\none is the right color,",
    "start": "4772890",
    "end": "4781100"
  },
  {
    "text": "then you're going to be giving\nit a task that are impossible and that may lead to over\nfitting and worse performance.",
    "start": "4781100",
    "end": "4786580"
  },
  {
    "text": "So there's a bit of a\nbalance to strike there. Yeah, so just to\nfollow up on that. So would you consider this to be\na possible project exploration",
    "start": "4786580",
    "end": "4795900"
  },
  {
    "text": "idea for this class? Yeah, I think that would be a\ngreat direction for a project. OK. Thank you. ",
    "start": "4795900",
    "end": "4804070"
  },
  {
    "text": "Hi. I had a quick question about\nthe loss class theta information",
    "start": "4804070",
    "end": "4809270"
  },
  {
    "text": "flow. The slide that you had that on. I'm curious the beta itself, if\nyou tried decaying it and not",
    "start": "4809270",
    "end": "4816860"
  },
  {
    "text": "keeping it constant,\nwhat would happen? Yeah, so beta is a\nhyper parameter here",
    "start": "4816860",
    "end": "4823590"
  },
  {
    "text": "that is actually\npretty important because you need to\nmake sure that it's strong enough, but also not 0.",
    "start": "4823590",
    "end": "4830990"
  },
  {
    "text": "It could make sense to decay\nit if it's having trouble finding a good solution\nto this problem.",
    "start": "4830990",
    "end": "4836890"
  },
  {
    "text": "So, for example, you may want to\nstart it as being high and then decay it to be lower. And if you start it\nas high, basically, it",
    "start": "4836890",
    "end": "4843500"
  },
  {
    "text": "will encourage it to really use\nanything that it can in D train and then if you\ndecrease it over time,",
    "start": "4843500",
    "end": "4848929"
  },
  {
    "text": "then it can store more\ninformation in theta. So that seems like it could be a\nreasonable approach, especially",
    "start": "4848930",
    "end": "4854180"
  },
  {
    "text": "if finding a single\nvalue of beta is hard. OK, thank you. ",
    "start": "4854180",
    "end": "4861250"
  },
  {
    "text": "I have a question about\nI think your final slide with all the results. And because in that\nslide, it says,",
    "start": "4861250",
    "end": "4868250"
  },
  {
    "text": "like, only for a disaster task\nthe unsupervised learning works better. So is there any\nspecific characteristics",
    "start": "4868250",
    "end": "4875500"
  },
  {
    "text": "and is there any reasoning\nwhy this specific task unsupervised-- works better unsupervised?",
    "start": "4875500",
    "end": "4880986"
  },
  {
    "text": "Because the intuition, I think,\nis that supervised is always better. It just needs some more\ninformation, right?",
    "start": "4880986",
    "end": "4887016"
  },
  {
    "text": "Or is that not true? Yeah. So first, it's worth\nmentioning that you also see improvement on the\npolitical bias task",
    "start": "4887016",
    "end": "4893060"
  },
  {
    "text": "when you have a larger\nnumber of shots. And also, I actually\ncut off this table. There's actually more data\nsets that they tested on too.",
    "start": "4893060",
    "end": "4900240"
  },
  {
    "text": "I just couldn't fit\nit on the slide. And I think that there\nwere also some more where the unsupervised method\nwas doing better.",
    "start": "4900240",
    "end": "4906530"
  },
  {
    "text": "I don't quite understand why\nthe fully unsupervised method is doing better in this\ncase than the one that",
    "start": "4906530",
    "end": "4913400"
  },
  {
    "text": "incorporates supervised tasks. My intuition would be that\nincorporating supervised tasks",
    "start": "4913400",
    "end": "4918710"
  },
  {
    "text": "should always help. It's possible that\nmaybe they have",
    "start": "4918710",
    "end": "4923900"
  },
  {
    "text": "an explanation in the paper. I didn't see it when\nI went over the paper, but I may have missed it.",
    "start": "4923900",
    "end": "4929500"
  },
  {
    "text": "OK, I'll take a look\nat the paper, then. Thanks. I guess one thing that I\ncould mention is that maybe--",
    "start": "4929500",
    "end": "4934610"
  },
  {
    "text": "I can't remember exactly\nwhat supervised tasks that they were\nusing to incorporate in the meta-training,\nbut maybe those tasks",
    "start": "4934610",
    "end": "4940007"
  },
  {
    "text": "were quite different\nfrom the kinds of tasks that you see in this test task. And maybe the unsupervised tasks\nare just a little bit closer",
    "start": "4940007",
    "end": "4947420"
  },
  {
    "text": "to the kinds of tasks\nthat you see at test time. Right.",
    "start": "4947420",
    "end": "4953500"
  },
  {
    "text": "It might also be harder to\nget unsupervised task, right?",
    "start": "4953500",
    "end": "4958800"
  },
  {
    "text": "Yeah. Yeah, OK. ",
    "start": "4958800",
    "end": "4966210"
  },
  {
    "text": "Hi, I have a\nquestion on slide 16. ",
    "start": "4966210",
    "end": "4972380"
  },
  {
    "text": "So where you have all the\ncell phone objects and angles. ",
    "start": "4972380",
    "end": "4978860"
  },
  {
    "text": "So for this type\nof tasks, the label is the angle of\nthe object, right? And for the model to infer\nfrom what's the angle",
    "start": "4978860",
    "end": "4987080"
  },
  {
    "text": "is the task set, it needs\nto know the relationship between labels.",
    "start": "4987080",
    "end": "4992870"
  },
  {
    "text": "Instead of regular\nclassification tasks, each label is independent. So how does the model\nlearn that from this?",
    "start": "4992870",
    "end": "5001870"
  },
  {
    "text": "Yeah, so you need\nto cut more than-- well, I guess it does\nneed to understand, like--",
    "start": "5001870",
    "end": "5010350"
  },
  {
    "text": " you need to kind of look\nat two images, maybe",
    "start": "5010350",
    "end": "5015740"
  },
  {
    "text": "two of the images in the\ntraining example, or one image here and one image here\nto be able to compare the orientations.",
    "start": "5015740",
    "end": "5022280"
  },
  {
    "text": "The methods like\nprototypical networks, like, already build\nthis in to some degree because you're kind\nof comparing them.",
    "start": "5022280",
    "end": "5027830"
  },
  {
    "text": "Although it's also\na regression task, so it'd be a bit\ntricky to apply things like prototypical\nnetworks to it.",
    "start": "5027830",
    "end": "5034970"
  },
  {
    "text": "In general, though models like\nblack box models and MAML do",
    "start": "5034970",
    "end": "5040920"
  },
  {
    "text": "incorporate information\nfrom multiple examples. MAML does it by\naveraging the gradient and then black box models\ndo it by processing all",
    "start": "5040920",
    "end": "5048550"
  },
  {
    "text": "of the examples it has input. And so they have the capacity\nto look at multiple examples",
    "start": "5048550",
    "end": "5055150"
  },
  {
    "text": "jointly to some degree. And that's kind of one way that\nallows them to solve this task.",
    "start": "5055150",
    "end": "5063090"
  },
  {
    "text": "Does that answer your question? So you're saying this\nis a regression task,",
    "start": "5063090",
    "end": "5068449"
  },
  {
    "text": "it has to have 360\nway classification? We treated it as\na regression task. You could also treat it as a\n360 way classification task.",
    "start": "5068450",
    "end": "5077067"
  },
  {
    "text": "OK. So do you also apply--",
    "start": "5077067",
    "end": "5082187"
  },
  {
    "text": "another question\nwould be, do you also apply a uniform or a\nglobal angle measurement",
    "start": "5082187",
    "end": "5088570"
  },
  {
    "text": "across different images? Because you can\ndefine the angle, so it depends on how you\naim your axis, right?",
    "start": "5088570",
    "end": "5097100"
  },
  {
    "text": "Yeah, that's what I meant by\nthe canonical orientation. That's kind of the base axes.",
    "start": "5097100",
    "end": "5103010"
  },
  {
    "text": "However, with different\nobject shapes, it's hard to determine-- like,\nbasically, each object has",
    "start": "5103010",
    "end": "5108969"
  },
  {
    "text": "to have its own\ncanonical orientation. It's difficult to align\nobjects in a way that",
    "start": "5108970",
    "end": "5115060"
  },
  {
    "text": "allows you to have the same\ncanonical orientation for all the objects. OK.",
    "start": "5115060",
    "end": "5120370"
  },
  {
    "text": "Cool. Cool. Got it. Thank you. ",
    "start": "5120370",
    "end": "5126630"
  },
  {
    "text": "I have a question regarding\nthe task [INAUDIBLE]..",
    "start": "5126630",
    "end": "5131790"
  },
  {
    "text": "So in the task [INAUDIBLE],,\nis there a word combining the [INAUDIBLE]",
    "start": "5131790",
    "end": "5161630"
  },
  {
    "text": "Yeah, so I don't know of any\nmethods that combine them. One kind of naive\nway to combine them would just be to train on both\ntasks, both sets of tasks.",
    "start": "5161630",
    "end": "5169693"
  },
  {
    "text": "But there may be more\ninteresting sophisticated ways to try to combine them as well.",
    "start": "5169693",
    "end": "5175170"
  },
  {
    "text": "And then one question I have\nregarding the last [INAUDIBLE] classification.",
    "start": "5175170",
    "end": "5180630"
  },
  {
    "text": "So if you look at\nthe [INAUDIBLE]",
    "start": "5180630",
    "end": "5210170"
  },
  {
    "text": "So BERT is a really\nstrong baseline here and it does perform pretty well. Although, I will say that kind\nof, even though the performance",
    "start": "5210170",
    "end": "5217647"
  },
  {
    "text": "is-- you see only a small\nimprovement in some cases, like from 49% to 52%, there\nare other settings where you",
    "start": "5217647",
    "end": "5223909"
  },
  {
    "text": "see a much larger improvement. For example, from,\nlike, 42% to 56%,",
    "start": "5223910",
    "end": "5229910"
  },
  {
    "text": "that's a kind of 14%\nabsolute improvement. Or 38% to 63%. ",
    "start": "5229910",
    "end": "5237340"
  },
  {
    "text": "Yeah, there are other\nexamples as well. So, like, 54% to 61%. There are examples where you\nsee a pretty big improvement.",
    "start": "5237340",
    "end": "5243620"
  },
  {
    "text": "50% to 70% is another. But yeah, BERT is a\npretty strong baseline",
    "start": "5243620",
    "end": "5252470"
  },
  {
    "text": "and has also been heavily-- ",
    "start": "5252470",
    "end": "5260000"
  }
]