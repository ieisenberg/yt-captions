[
  {
    "text": "Welcome to the section on fitting classifiers by empirical risk minimization.",
    "start": "4610",
    "end": "11250"
  },
  {
    "text": "[NOISE] Now, this section is very much",
    "start": "11250",
    "end": "18665"
  },
  {
    "text": "parallel to our earlier section on regression by empirical risk minimization.",
    "start": "18665",
    "end": "26725"
  },
  {
    "text": "We have to really discuss two things in order to do classification. And the first is, how do you embed categorical variables v?",
    "start": "26725",
    "end": "35690"
  },
  {
    "text": "So we have a script v. It's a, it's a finite set of possible values of classes that,",
    "start": "35690",
    "end": "44614"
  },
  {
    "text": "uh, our target variable v can take. And we wanna take the target variable v and embed it into a vector y in R_m.",
    "start": "44615",
    "end": "56865"
  },
  {
    "text": "And we're gonna let y be Psi of v. And, um,",
    "start": "56865",
    "end": "62280"
  },
  {
    "text": "of course, Psi of v only takes k values as well, because v only takes k values.",
    "start": "62280",
    "end": "67700"
  },
  {
    "text": "[NOISE] And so we have, um, Psi- we have capital K vectors,",
    "start": "67700",
    "end": "73035"
  },
  {
    "text": "Psi 1 through Psi capital K. Each of those are the images under the map Psi of the classes v_1 to v_k.",
    "start": "73035",
    "end": "82140"
  },
  {
    "text": "[NOISE] And we call these vectors Psi_i the representative of v_i.",
    "start": "82140",
    "end": "91050"
  },
  {
    "text": "And the set of all representatives Psi_1 through Psi_K is called the constellation of representatives.",
    "start": "91050",
    "end": "99555"
  },
  {
    "text": "Now, here are some examples, um, we might, um, embed true to 1 and false to minus 1.",
    "start": "99555",
    "end": "108345"
  },
  {
    "text": "We might embed true to 1 and false to 0. Uh, those are mappings from, er, uh, uh,",
    "start": "108345",
    "end": "116295"
  },
  {
    "text": "the, uh, of two element set of categories into the real numbers.",
    "start": "116295",
    "end": "122729"
  },
  {
    "text": "Uh, we might, uh, also map the three element set, yes, maybe, no, to the real numbers 1,",
    "start": "122730",
    "end": "131955"
  },
  {
    "text": "0, and minus 1. Um, another thing we might do is do a,",
    "start": "131955",
    "end": "137785"
  },
  {
    "text": "a one-hot embedding where we have, say, three categories: apple, orange, and banana.",
    "start": "137785",
    "end": "147680"
  },
  {
    "text": "And, um, we map apple to E_1, orange to E_2,",
    "start": "147680",
    "end": "153719"
  },
  {
    "text": "and banana to E_3, but E_1, and E_2, and E_3 are the canonical unit variables- unit vectors in R_3.",
    "start": "153720",
    "end": "163079"
  },
  {
    "text": "Um, now, we might also take three categories and map them directly to the reals.",
    "start": "163700",
    "end": "170435"
  },
  {
    "text": "Um, if we bought, say, three horses: Horse 3, Horse 1, and Horse 2, we might map them just to 3, 1, and 2.",
    "start": "170435",
    "end": "177995"
  },
  {
    "text": "Um, well, we might want to do really complicated things like Word2vec, which maps, er, a large number of words to,",
    "start": "177995",
    "end": "185780"
  },
  {
    "text": "uh, vectors in R_300. [NOISE] So the one-hot embedding,",
    "start": "185780",
    "end": "196065"
  },
  {
    "text": "um, takes K classes and maps into R_K. So a Psi of v_i is just e_i.",
    "start": "196065",
    "end": "204045"
  },
  {
    "text": "Um, we could do it to Booleans if, um, [NOISE] we have, er,",
    "start": "204045",
    "end": "210010"
  },
  {
    "text": "Booleans where the first representative maps to 1, 0 and the second representative maps to 0, 1.",
    "start": "210010",
    "end": "218519"
  },
  {
    "text": "Um, there's another version of the one-hot embedding called the reduced one-hot embedding, um, which, er,",
    "start": "218750",
    "end": "226620"
  },
  {
    "text": "maps one of the classes to 0 and the other K minus 1 class is mapped to the unit vectors in R_K minus 1.",
    "start": "226620",
    "end": "237569"
  },
  {
    "text": "Um, so if we had Booleans,",
    "start": "237880",
    "end": "243089"
  },
  {
    "text": "in that case, that reduces the mapping, er, er, [NOISE] ah, from the,",
    "start": "243089",
    "end": "250560"
  },
  {
    "text": "the one-hot embedding is- maps the Booleans to 1, 0 and 0, 1. And the reduced one-hot embedding would map the Booleans to 0 and 1;",
    "start": "250560",
    "end": "260530"
  },
  {
    "text": "one-dimensional embedding rather than a two-dimensional embedding.",
    "start": "260530",
    "end": "265980"
  },
  {
    "text": "Um, we might also have, ah, yes, maybe no, and the default be maybe, in which case,",
    "start": "266080",
    "end": "273530"
  },
  {
    "text": "uh, that's three categories we would map it to R_2 by the one-hot embedding- the reduced one-hot embedding.",
    "start": "273530",
    "end": "281840"
  },
  {
    "text": "And we put maybe as the default. So maybe would map to the origin, yes might map to 1,",
    "start": "281840",
    "end": "286970"
  },
  {
    "text": "0, and no might map to 0, 1.",
    "start": "286970",
    "end": "290880"
  },
  {
    "text": "Now, once we've, er, embedded,",
    "start": "294160",
    "end": "300275"
  },
  {
    "text": "we are going to have to do something we didn't have to do, uh, in regression and that is unembed.",
    "start": "300275",
    "end": "307610"
  },
  {
    "text": "Ah, and in particular, we've got to unembed, er, points which are not mapped to by our embedding map.",
    "start": "307610",
    "end": "316705"
  },
  {
    "text": "And, um, so, ah, we will see how to do that.",
    "start": "316705",
    "end": "323025"
  },
  {
    "text": "Um, [NOISE], uh, let's look at how this works from the, uh, overall, er, predictor point of view.",
    "start": "323025",
    "end": "331860"
  },
  {
    "text": "Uh, so we start off, we embed our raw input u to a feature vector x by the map Phi,",
    "start": "331860",
    "end": "341450"
  },
  {
    "text": "the feature vector x is now R_d. Ah, and we go to raw output, which is in our data set which is now a",
    "start": "341450",
    "end": "347995"
  },
  {
    "text": "v, which is a categorical. We map it to Psi of v,",
    "start": "347995",
    "end": "353760"
  },
  {
    "text": "which is y which lives in R_m. We use these data points we've got from n,",
    "start": "353760",
    "end": "361919"
  },
  {
    "text": "u_i, v_i pairs to n, x_i, y_i pairs. And we use those to create a predictor G,",
    "start": "361920",
    "end": "368870"
  },
  {
    "text": "which maps R_d to R_m. And for any x, it will give us a y hat,",
    "start": "368870",
    "end": "374645"
  },
  {
    "text": "which lives in R_m. And- now what are we gonna do with y hat?",
    "start": "374645",
    "end": "380449"
  },
  {
    "text": "Well, what we hope is that, uh, um, if y hat is G of x,",
    "start": "380450",
    "end": "386025"
  },
  {
    "text": "then y is somehow close to Psi of v, where v is one of the categories.",
    "start": "386025",
    "end": "393360"
  },
  {
    "text": "Um, and close here would usually mean in the norm.",
    "start": "393760",
    "end": "398820"
  },
  {
    "text": "So I've got the, ah, the norm of, ah, y hat minus Psi of v is",
    "start": "398820",
    "end": "408539"
  },
  {
    "text": "small for one of the Vs. And to get the prediction, we've got to un-embed.",
    "start": "408540",
    "end": "414620"
  },
  {
    "text": "We've got to pick, ah, a v hat, which is, um, [NOISE] er,",
    "start": "414620",
    "end": "421680"
  },
  {
    "text": "corresponding to the y hat. And we're gonna define a map Psi dagger,",
    "start": "421760",
    "end": "428190"
  },
  {
    "text": "which takes vectors in R_m and maps them back into the target set script B, the unembedding function.",
    "start": "428190",
    "end": "438030"
  },
  {
    "text": "So once we've got a Psi dagger, well, we can use it in order to construct our final classifier.",
    "start": "438030",
    "end": "443419"
  },
  {
    "text": "Our final classifier has to take a u and give us back a v hat. And the way it's gonna work is we're gonna take u,",
    "start": "443420",
    "end": "451090"
  },
  {
    "text": "pump it through our embedding Phi to give us an x, pump that through G to give us a prediction y hat,",
    "start": "451090",
    "end": "459785"
  },
  {
    "text": "and then pump that through the unembedded Psi dagger to give us a v hat. And then we'll call that map from u to v hat capital G. Um,",
    "start": "459785",
    "end": "471960"
  },
  {
    "text": "G is the composition of Psi dagger with G with Phi. In other words, we embed,",
    "start": "471960",
    "end": "477395"
  },
  {
    "text": "we predict, we unembed. [NOISE]",
    "start": "477395",
    "end": "483229"
  },
  {
    "text": "The usual un-embedding that people choose is the nearest neighbor un-embedding.",
    "start": "483230",
    "end": "491425"
  },
  {
    "text": "And what it does is it says when you've gotta y hat, pick out all the categories in script v,",
    "start": "491425",
    "end": "499240"
  },
  {
    "text": "the one, such that psi of b is that the closest to y hat.",
    "start": "499240",
    "end": "504865"
  },
  {
    "text": "So minimize over all little v's in script v, the norm of y hat minus psi of v. And that is called psi dagger of y hat.",
    "start": "504865",
    "end": "516835"
  },
  {
    "text": "And if it so happens that there are two vectors v,",
    "start": "516835",
    "end": "522745"
  },
  {
    "text": "two classes for which their embedded points are equally close to y hat,",
    "start": "522745",
    "end": "528820"
  },
  {
    "text": "well then you can break ties anyway you want to. Another way to say it is what we're doing is we're choosing the target variable,",
    "start": "528820",
    "end": "538045"
  },
  {
    "text": "target category, um, which is associated with the nearest representative to y hat.",
    "start": "538045",
    "end": "545240"
  },
  {
    "text": "So here's the- the simplest possible example. If we've embedded the Booleans true and false to 1 and minus 1,",
    "start": "547680",
    "end": "556775"
  },
  {
    "text": "so those are our representatives, psi_1 and psi_2. Well, then we would un-embed by mapping any positive number y hat to true,",
    "start": "556775",
    "end": "569005"
  },
  {
    "text": "and any negative number y hat to false.",
    "start": "569005",
    "end": "574090"
  },
  {
    "text": "Because if we've got two representatives, which we have; minus 1 and 1, well,",
    "start": "574090",
    "end": "579670"
  },
  {
    "text": "if y hat is positive, then the closest representative is 1, and if y hat is negative then the closest representative is minus 1.",
    "start": "579670",
    "end": "587930"
  },
  {
    "text": "Um, suppose we've got one-hot. So let's look at the r2 case.",
    "start": "590160",
    "end": "596980"
  },
  {
    "text": "Um, so here, we have y_1,",
    "start": "596980",
    "end": "605678"
  },
  {
    "text": "y_2, two components of y. We have 1, 0 and 0, 1.",
    "start": "605679",
    "end": "613435"
  },
  {
    "text": "This will be psi_1 and this will be psi_2. Those are the embeddings of our two categories in the case of the Booleans.",
    "start": "613435",
    "end": "623320"
  },
  {
    "text": "So this would be psi of true and this would be psi of false.",
    "start": "623320",
    "end": "632800"
  },
  {
    "text": "And then we develop a predictor, and that predictor gives us back at y hat,",
    "start": "632800",
    "end": "638920"
  },
  {
    "text": "which is anywhere in the two-dimensional planes.",
    "start": "638920",
    "end": "644604"
  },
  {
    "text": "And then what do we do? Well we will say, [BACKGROUND]",
    "start": "644604",
    "end": "654460"
  },
  {
    "text": "if you're on this side of the plane, if one hat's over there,",
    "start": "654460",
    "end": "659815"
  },
  {
    "text": "then corresponding v hat will be psi_1. And er, if you're on this side of the plane,",
    "start": "659815",
    "end": "671829"
  },
  {
    "text": "then the corresponding v hat will be psi_2.",
    "start": "671830",
    "end": "676840"
  },
  {
    "text": "And to be precise, there will be no psi_1 and psi_2 but v_1 and v_2,",
    "start": "676840",
    "end": "682645"
  },
  {
    "text": "the corresponding targets true and false. Um, and so here, it's clear.",
    "start": "682645",
    "end": "688870"
  },
  {
    "text": "What you do is you look at the two components of y hat,",
    "start": "688870",
    "end": "695650"
  },
  {
    "text": "y hat_1, and y hat_2. And if y hat_1 is larger then you're on this side of",
    "start": "695650",
    "end": "700750"
  },
  {
    "text": "the plane- you're on this side of the plane.",
    "start": "700750",
    "end": "708460"
  },
  {
    "text": "And if, uh, y_2 is larger, well then you're on this side, um, of the boundary between the two,",
    "start": "708460",
    "end": "715615"
  },
  {
    "text": "we- between the- which separates the region which belongs to psi_1 and the region that belongs to psi_2.",
    "start": "715615",
    "end": "723220"
  },
  {
    "text": "Uh, let's put it another way. If, uh, y hat_1 is greater than y hat_2, um, er,",
    "start": "723220",
    "end": "735889"
  },
  {
    "text": "then the arg mean over v of the norm of psi v",
    "start": "736170",
    "end": "746214"
  },
  {
    "text": "minus y hat is, uh, is v_1.",
    "start": "746215",
    "end": "759800"
  },
  {
    "text": "Otherwise, it's v_2. Uh, this is true more generally, um, uh,",
    "start": "762870",
    "end": "770190"
  },
  {
    "text": "if we want to find out which is- what is the solution in general to this problem of unembedding,",
    "start": "770190",
    "end": "779500"
  },
  {
    "text": "what is psi dagger when we've used to one-hot embedding, um, it turns out that this is exactly what we think it is.",
    "start": "779500",
    "end": "787255"
  },
  {
    "text": "We look at the largest component of the vector y hat and,",
    "start": "787255",
    "end": "793210"
  },
  {
    "text": "uh, we return the corresponding target vector.",
    "start": "793210",
    "end": "799210"
  },
  {
    "text": "So if y hat_3 is the largest component of y hat,",
    "start": "799210",
    "end": "805165"
  },
  {
    "text": "then unembedding psi dagger would return v_3.",
    "start": "805165",
    "end": "810610"
  },
  {
    "text": "Um, and the reason for that, we can just work it out algebraically what it is. Um, if I look at what, um,",
    "start": "810610",
    "end": "817165"
  },
  {
    "text": "uh, what y hat minus ei is, um,",
    "start": "817165",
    "end": "826100"
  },
  {
    "text": "well, that's the two norm squared of that quantity, is the two norm of y hat plus 1, uh,",
    "start": "826230",
    "end": "835030"
  },
  {
    "text": "minus 2 y hat transpose ei, and y hat transpose ei is just y hat_i.",
    "start": "835030",
    "end": "843775"
  },
  {
    "text": "It's just the ith component of y hat. And so picking i to",
    "start": "843775",
    "end": "852175"
  },
  {
    "text": "minimize that quantity is precisely the same as picking i to maximize y hat_i.",
    "start": "852175",
    "end": "859660"
  },
  {
    "text": "Um, suppose we've embedded",
    "start": "859660",
    "end": "868899"
  },
  {
    "text": "with the reduced one-hot embedding. We've used the reduced one-hot embedding to embed yes,",
    "start": "868900",
    "end": "874810"
  },
  {
    "text": "maybe, no as 1,0, 0,0, or 0,1. Well, then we un-embed in exactly the same way for any point y hat in the r2 plane,",
    "start": "874810",
    "end": "888310"
  },
  {
    "text": "we pick the closest representative. And that tells us which target variable we're predicting.",
    "start": "888310",
    "end": "895630"
  },
  {
    "text": "So if y hat is over here in this region,",
    "start": "895630",
    "end": "901705"
  },
  {
    "text": "it's- I'm going to return yes. Yes is psi dagger of any point in that region.",
    "start": "901705",
    "end": "909790"
  },
  {
    "text": "No is psi dagger of any point in this region, and maybe a psi dagger of any point in the remaining region.",
    "start": "909790",
    "end": "917695"
  },
  {
    "text": "And that's nicely expressed in the real cases construction there.",
    "start": "917695",
    "end": "923605"
  },
  {
    "text": "And again, you can choose any value you like on the boundary.",
    "start": "923605",
    "end": "927199"
  },
  {
    "text": "So more generally, we might, um, have, ah, an embedding of, ah,",
    "start": "932240",
    "end": "940230"
  },
  {
    "text": "ah, categories into R_m. And, ah, that gives us a whole bunch of different representatives,",
    "start": "940230",
    "end": "947579"
  },
  {
    "text": "here represented by these blue dots.",
    "start": "947580",
    "end": "950860"
  },
  {
    "text": "And for any given point y hat in R_m,",
    "start": "952870",
    "end": "958325"
  },
  {
    "text": "we want to find the co- closest representative. And the way we do that is by the Voronoi diagram that we saw earlier.",
    "start": "958325",
    "end": "966175"
  },
  {
    "text": "And so in particular, ah,",
    "start": "966175",
    "end": "968920"
  },
  {
    "text": "this region right here is the set of",
    "start": "971550",
    "end": "978459"
  },
  {
    "text": "points for which the closest blue dot is- that one in the middle.",
    "start": "978460",
    "end": "985440"
  },
  {
    "text": "And so that means that if y hat is in that region, I will return the corresponding representative to that blue dot right there.",
    "start": "985440",
    "end": "995550"
  },
  {
    "text": "And as we've seen when we looked at the nearest neighbor classifier, the, ah,",
    "start": "995550",
    "end": "1000635"
  },
  {
    "text": "regions which, ah, correspond to each target variable,",
    "start": "1000635",
    "end": "1007325"
  },
  {
    "text": "with each representative are polyhedra. [NOISE]",
    "start": "1007325",
    "end": "1021920"
  },
  {
    "text": "Now suppose we are using a parameterized predictor. So g is a function of a parameter Theta, and g is-,",
    "start": "1021920",
    "end": "1029720"
  },
  {
    "text": "ah, might be Theta transpose times x if it's a linear predictor or might be something more complicated.",
    "start": "1029720",
    "end": "1035464"
  },
  {
    "text": "Well, then theta is the parameter we choose. And then once we've got a predictor g Theta,",
    "start": "1035465",
    "end": "1042545"
  },
  {
    "text": "we can- we compose it with the embedding Psi and the un-embedding- the embedding- this should be the embedding Phi.",
    "start": "1042545",
    "end": "1051470"
  },
  {
    "text": "Let's just see that we use that same notation here. This- that should be Phi.",
    "start": "1051470",
    "end": "1058130"
  },
  {
    "text": "[NOISE] So this should say",
    "start": "1058130",
    "end": "1065050"
  },
  {
    "text": "Psi Dagger g Theta Phi of u.",
    "start": "1065050",
    "end": "1073550"
  },
  {
    "text": "Not Psi of u. And that's the classifier that gives us v hat. So what do we do?",
    "start": "1075070",
    "end": "1083060"
  },
  {
    "text": "We choose Theta using ERM and the training data set, and we validate the predictor by looking at the performance metric, which might be,",
    "start": "1083060",
    "end": "1092000"
  },
  {
    "text": "ah, the error rate on a test data set. Of course, the performance metric might be the Neyman-Pearson error measure as well.",
    "start": "1092000",
    "end": "1103350"
  },
  {
    "text": "And so when we're doing a parameterized predictors for classification, we might use a tree-based predictor, and has,",
    "start": "1104890",
    "end": "1112850"
  },
  {
    "text": "ah, got a name, it's very commonly used, it's called a classification tree. And then Theta encodes the tree,",
    "start": "1112850",
    "end": "1119150"
  },
  {
    "text": "tells us when to split at each node, and the threshold, and leaf values, and initially,",
    "start": "1119150",
    "end": "1124910"
  },
  {
    "text": "it has a corresponding value of y hat. And that leads immediately to the corresponding target because we",
    "start": "1124910",
    "end": "1131299"
  },
  {
    "text": "can un-embed the y hat to the v hat. Or we might use a neural network, um, in which case,",
    "start": "1131300",
    "end": "1139850"
  },
  {
    "text": "Theta gives us the bias or the offset and the weights in the different layers, and then y hat is the output- the last layer.",
    "start": "1139850",
    "end": "1146270"
  },
  {
    "text": "And again, we are un-embed. We might use a linear predictor. Theta is a d by m matrix,",
    "start": "1146270",
    "end": "1153950"
  },
  {
    "text": "and y hat is just Theta transpose times x.",
    "start": "1153950",
    "end": "1158610"
  },
  {
    "text": "Now, the other major piece that we need a classification is a loss function.",
    "start": "1161080",
    "end": "1168409"
  },
  {
    "text": "And, ah, it turns out that the loss function, ah, that we would use for classification is slightly different",
    "start": "1168410",
    "end": "1176540"
  },
  {
    "text": "from the typical loss function we would use for regression. Ah, and we're gonna see in the next section, ah,",
    "start": "1176540",
    "end": "1182674"
  },
  {
    "text": "several different loss functions, um, for classifiers. Ah, for the moment,",
    "start": "1182675",
    "end": "1188179"
  },
  {
    "text": "I want to consider the simplest, ah, ah, loss function. And that's- um, ah,",
    "start": "1188180",
    "end": "1195270"
  },
  {
    "text": "for example, the square loss. Now the way it works is just like it did in regression,",
    "start": "1196660",
    "end": "1204334"
  },
  {
    "text": "is that we have a loss function. This is not quite right. It should be the loss function maps R_m cross R_m to R,",
    "start": "1204334",
    "end": "1215760"
  },
  {
    "text": "and is a function of y hat and y. And it tells us how much prediction y hat bothers us when",
    "start": "1215980",
    "end": "1224570"
  },
  {
    "text": "the true observed value of y is what it is.",
    "start": "1224570",
    "end": "1229894"
  },
  {
    "text": "And it's going to be, of course, one of their representatives. Those are the only possible values for y because",
    "start": "1229895",
    "end": "1236990"
  },
  {
    "text": "the only possible values for v are v_1 through v_k, and when we embed those,",
    "start": "1236990",
    "end": "1242180"
  },
  {
    "text": "we get Psi 1 to psi K. As a result, we can think about loss functions slightly differently.",
    "start": "1242180",
    "end": "1248660"
  },
  {
    "text": "Rather than thinking about a loss function as taking a y hat and y,",
    "start": "1248660",
    "end": "1255729"
  },
  {
    "text": "we can say, \"Well, really what we have is we have K different loss functions.\" I mean, the first one is the loss function,",
    "start": "1255729",
    "end": "1264065"
  },
  {
    "text": "which tells us how much loss y hat causes us when the target variable is v_1,",
    "start": "1264065",
    "end": "1273784"
  },
  {
    "text": "that's l of y-hat Psi 1, or when the target variable is v_2 and then we get a different loss.",
    "start": "1273785",
    "end": "1280159"
  },
  {
    "text": "And we'll think of that as a totally different loss function. And we might even param- we might even use this kind of notation,",
    "start": "1280160",
    "end": "1287179"
  },
  {
    "text": "ah, l_j of y hat to say, \"Well, actually,",
    "start": "1287180",
    "end": "1292700"
  },
  {
    "text": "we've got a different loss function for j is 1 up to capital",
    "start": "1292700",
    "end": "1300260"
  },
  {
    "text": "K.\" And this loss function only depends on y hat because the dependency on y is taken care of by j.",
    "start": "1300260",
    "end": "1310610"
  },
  {
    "text": "What it is, is it's how much we dislike predicting y hat in that particular case where the target variable is v_j,",
    "start": "1310610",
    "end": "1318515"
  },
  {
    "text": "or correspondingly, when y is Psi j. And typically, what you want is you want that the loss function l of y hat Psi j,",
    "start": "1318515",
    "end": "1329525"
  },
  {
    "text": "basing on negative number, and it's small when y hat is close to Psi j.",
    "start": "1329525",
    "end": "1335945"
  },
  {
    "text": "So very commonly, we might use the square loss, um,",
    "start": "1335945",
    "end": "1341029"
  },
  {
    "text": "which is the two norm of y hat minus Psi j. Um, and we'll see other loss functions which work much",
    "start": "1341030",
    "end": "1348680"
  },
  {
    "text": "better for classification in the next section. [NOISE] So how do we think",
    "start": "1348680",
    "end": "1354950"
  },
  {
    "text": "about two different loss functions when we're using the square loss in the Boolean case? Well, the translated versions of the quadratic function.",
    "start": "1354950",
    "end": "1363335"
  },
  {
    "text": "This is on the- on the right here is l of",
    "start": "1363335",
    "end": "1368600"
  },
  {
    "text": "y hat y when y is Psi, ah, 2.",
    "start": "1368600",
    "end": "1377169"
  },
  {
    "text": "And Psi 2 is just 1, and this is l. On the left here,",
    "start": "1377170",
    "end": "1382340"
  },
  {
    "text": "y-hat and y when y is Psi 1,",
    "start": "1382340",
    "end": "1387620"
  },
  {
    "text": "which is just minus 1. And we can see that the one on the right is a quadratic centered at 1,",
    "start": "1387620",
    "end": "1394775"
  },
  {
    "text": "and the one on the left is a quadratic centered at minus 1.",
    "start": "1394775",
    "end": "1399930"
  },
  {
    "text": "So we're given, uh, a training data set; x_i, y_i pass of i 1 up to n. And",
    "start": "1404930",
    "end": "1413070"
  },
  {
    "text": "we're gonna parameterize predictor g Theta. And empirical risk minimization simply says, well,",
    "start": "1413070",
    "end": "1419700"
  },
  {
    "text": "what we do is we minimize the average loss function over Theta,",
    "start": "1419700",
    "end": "1426559"
  },
  {
    "text": "where the loss is evaluated at y hat i and y_i,",
    "start": "1426560",
    "end": "1432980"
  },
  {
    "text": "and y hat i is just our prediction evaluated at x_i. So ERM chooses Theta to minimize the loss.",
    "start": "1432980",
    "end": "1442035"
  },
  {
    "text": "Regularized ERM chooses the Theta to minimize the empirical risk,",
    "start": "1442035",
    "end": "1447735"
  },
  {
    "text": "plus Lambda times the regularization function. And Lambda here is a positive regularization hyper-parameter.",
    "start": "1447735",
    "end": "1455680"
  },
  {
    "text": "And in most cases, we gonna need to do numerical optimization to find Theta, there won't be an explicit formula.",
    "start": "1455780",
    "end": "1463060"
  },
  {
    "text": "Uh, and the least squares case is quite interesting. Let's consider what that is in the Boolean case.",
    "start": "1465500",
    "end": "1472809"
  },
  {
    "text": "So in the Boolean case, let's have u be, uh,",
    "start": "1473900",
    "end": "1481620"
  },
  {
    "text": "one dimensional, just to make it easy for me to draw.",
    "start": "1481620",
    "end": "1487184"
  },
  {
    "text": "And let's be v, have v, be either true or false,",
    "start": "1487185",
    "end": "1492750"
  },
  {
    "text": "and we'll embed true to 1. So this will be Psi of true,",
    "start": "1492750",
    "end": "1500039"
  },
  {
    "text": "and this will be Psi of false.",
    "start": "1500040",
    "end": "1506230"
  },
  {
    "text": "And at each of our data points, will give us a value of u and a corresponding value of,",
    "start": "1506870",
    "end": "1516990"
  },
  {
    "text": "um, of v, which will embed to give us,",
    "start": "1516990",
    "end": "1523110"
  },
  {
    "text": "uh, a 1 or a minus 1. So we might have a bunch of data points here, and a bunch of data points here,",
    "start": "1523110",
    "end": "1529679"
  },
  {
    "text": "[NOISE] and that's our data points.",
    "start": "1529680",
    "end": "1534705"
  },
  {
    "text": "And then what we're gonna do is we're gonna fit that with least squares. So we're gonna say, use linear predictor,",
    "start": "1534705",
    "end": "1542370"
  },
  {
    "text": "y hat is Theta transpose x, and that will fit a straight line right there.",
    "start": "1542370",
    "end": "1550419"
  },
  {
    "text": "Now, because we're un-embedding using Psi dagger.",
    "start": "1551240",
    "end": "1557415"
  },
  {
    "text": "Psi dagger here, remember, is the inverse- is the, uh, un-embedding map corresponding to the one hot- embedding.",
    "start": "1557415",
    "end": "1568230"
  },
  {
    "text": "And so it's going to say, well, whenever y hat is positive, your prediction should be true.",
    "start": "1568230",
    "end": "1576299"
  },
  {
    "text": "And whenever y hat is negative, your prediction should be false. And if, uh,",
    "start": "1576300",
    "end": "1584330"
  },
  {
    "text": "line of the- the plot of our predictor Theta transpose x happens to be like this,",
    "start": "1584330",
    "end": "1589784"
  },
  {
    "text": "that means that whenever, uh, u is over to the right here, I predict true.",
    "start": "1589785",
    "end": "1598575"
  },
  {
    "text": "And whenever u is over to the left here, I predict false.",
    "start": "1598575",
    "end": "1604510"
  },
  {
    "text": "And if we look at our data, we can see, well, in my simple sketch there didn't seem to do too badly, um,",
    "start": "1604910",
    "end": "1612720"
  },
  {
    "text": "although it's an astonishing idea that we can fit a function that only takes values 1 and minus 1 with a straight line.",
    "start": "1612720",
    "end": "1621760"
  },
  {
    "text": "This is called a least squares classifier. We're using a square loss,",
    "start": "1622940",
    "end": "1628680"
  },
  {
    "text": "we might be using a square regularizer, and we just solve the RERM problem. And this is the case where we can solve it exactly using least squares.",
    "start": "1628680",
    "end": "1637360"
  },
  {
    "text": "As an example, in two-dimensions when we see it, it kinda works.",
    "start": "1640100",
    "end": "1645615"
  },
  {
    "text": "And so here I've got u is in R_2. It's embedded in R_3 as 1, u_1, u_2.",
    "start": "1645615",
    "end": "1653310"
  },
  {
    "text": "Remember, we've put the constant feature and we've embedded minus 1,",
    "start": "1653310",
    "end": "1658380"
  },
  {
    "text": "1 just directly as y is minus 1 or y is 1 as our two representatives;",
    "start": "1658380",
    "end": "1665160"
  },
  {
    "text": "Psi 1 is minus 1, Psi 2 is 1. And we use a square loss and a square regularizer,",
    "start": "1665160",
    "end": "1671775"
  },
  {
    "text": "and these are our predictor- this is our predictor. The shading here, shows which points u_1,",
    "start": "1671775",
    "end": "1679695"
  },
  {
    "text": "u_2 map to, uh, 1, and which points map to minus 1.",
    "start": "1679695",
    "end": "1688529"
  },
  {
    "text": "And of course, really what's going on is that this here,",
    "start": "1688530",
    "end": "1693735"
  },
  {
    "text": "this boundary, that's the boundary where y hat is 0,",
    "start": "1693735",
    "end": "1701010"
  },
  {
    "text": "and y hat is a linear function of u_1, u_2. Y hat, we know what it must look like.",
    "start": "1701010",
    "end": "1711360"
  },
  {
    "text": "Y hat must be equal to Theta 1 plus Theta 2 u_1 plus Theta 3 u_2.",
    "start": "1711360",
    "end": "1720315"
  },
  {
    "text": "And this line right here is the u_1s and u_2s,",
    "start": "1720315",
    "end": "1725610"
  },
  {
    "text": "which result in y hat being 0. Over on this side, we have y hat positive,",
    "start": "1725610",
    "end": "1732905"
  },
  {
    "text": "and over on this side we have y hat negative, and our un-embedding maps that to 1 and minus 1 to give us a classifier.",
    "start": "1732905",
    "end": "1746140"
  },
  {
    "text": "Now if we have the Neyman-Pearson metric, so the Neyman-Pearson metric, remember what it is.",
    "start": "1755360",
    "end": "1762705"
  },
  {
    "text": "It says we're going to consider a weighted sum of the different errors,",
    "start": "1762705",
    "end": "1768015"
  },
  {
    "text": "where E_j is the rate of mistaking v_j for some other class.",
    "start": "1768015",
    "end": "1773355"
  },
  {
    "text": "And Kappa here is going to be the weight. So Kappa J is how much we care about mistaking v_j,",
    "start": "1773355",
    "end": "1779970"
  },
  {
    "text": "about predicting the answer in the true case where the true target variable is v_j wrong.",
    "start": "1779970",
    "end": "1786880"
  },
  {
    "text": "Now what we do is we scale the losses by Kappa.",
    "start": "1787130",
    "end": "1793065"
  },
  {
    "text": "So suppose we have loss functions, we've seen already the square loss,",
    "start": "1793065",
    "end": "1799650"
  },
  {
    "text": "which we'll call l tilde y hat. Well, then l tilde y hat Psi j,",
    "start": "1799650",
    "end": "1808200"
  },
  {
    "text": "for example, might be the square loss. It might be the square of y hat minus Psi j,",
    "start": "1808200",
    "end": "1816780"
  },
  {
    "text": "or it might be the norm of y hat minus Psi j squared. Well, then we'll construct our loss function from these unweighted losses by",
    "start": "1816780",
    "end": "1825270"
  },
  {
    "text": "taking a weighted combination of these losses. So now, will use a loss function,",
    "start": "1825270",
    "end": "1832305"
  },
  {
    "text": "l of y Psi j, which is Kappa j times l tilde of y and Psi j.",
    "start": "1832305",
    "end": "1840790"
  },
  {
    "text": "So let's look at, uh, an example. Suppose that Psi 1 is minus 1 and Psi 2 is 1,",
    "start": "1845630",
    "end": "1852285"
  },
  {
    "text": "we gonna care about the Neyman-Pearson metric. And because this is Boolean,",
    "start": "1852285",
    "end": "1857295"
  },
  {
    "text": "there are two types of errors we can make; the false negatives and the false positives.",
    "start": "1857295",
    "end": "1862350"
  },
  {
    "text": "And our Neyman-Pearson metric is gonna be Kappa times the false negative rate plus the false positive rate.",
    "start": "1862350",
    "end": "1869565"
  },
  {
    "text": "Kappa is some number posi- uh, greater than 0. Um, notice here that we've only got one scalar Kappa, uh,",
    "start": "1869565",
    "end": "1877020"
  },
  {
    "text": "whereas on the previous slide we had the number of scalar Kappa is equal to number of categories,",
    "start": "1877020",
    "end": "1882420"
  },
  {
    "text": "and so we really- this should have Kappa 1 times E_fn, plus Kappa 2 times E_fp.",
    "start": "1882420",
    "end": "1888705"
  },
  {
    "text": "And here, of course, Kappa is simply Kappa 1 divided by Kappa 2, if you like, because, um,",
    "start": "1888705",
    "end": "1896890"
  },
  {
    "text": "I can set one of the Kappas to 1 and all that matters is",
    "start": "1897350",
    "end": "1902669"
  },
  {
    "text": "the ratio of the two Kappas in terms of affecting which prediction I get.",
    "start": "1902670",
    "end": "1909030"
  },
  {
    "text": "Now, in our loss function, we're gonna use the square loss but we're going to scale it. And so we'll use y hat minus y squared when y is minus 1.",
    "start": "1909030",
    "end": "1919060"
  },
  {
    "text": "And we use Kappa times y hat minus y squared when y is 1.",
    "start": "1919060",
    "end": "1924200"
  },
  {
    "text": "And that will give us a weight which is greater when, uh,",
    "start": "1924200",
    "end": "1930529"
  },
  {
    "text": "when the true y is 1, and so we want to make sure that we",
    "start": "1930530",
    "end": "1938955"
  },
  {
    "text": "are penalizing here false negatives more than false positives.",
    "start": "1938955",
    "end": "1944620"
  },
  {
    "text": "Here's an example. So square loss, sum of squares regularizer. Um, now, this on the left here is the ROC curves,",
    "start": "1947540",
    "end": "1960225"
  },
  {
    "text": "and they correspond to minimizing the weighted sum Kappa E_fn plus E_fp.",
    "start": "1960225",
    "end": "1969990"
  },
  {
    "text": "And these are the two losses, the training losses in blue and the test losses in red.",
    "start": "1969990",
    "end": "1979660"
  },
  {
    "text": "On the right hand plot over here, we see the minimum error classifier, which is given by when Kappa is 1,",
    "start": "1979730",
    "end": "1986745"
  },
  {
    "text": "and that would correspond to, uh, this point down here.",
    "start": "1986745",
    "end": "1994990"
  },
  {
    "text": "Now, if we pick Kappa is 0.4, well, then we're very concerned to make sure that",
    "start": "1997220",
    "end": "2004340"
  },
  {
    "text": "we don't mistake any red points for blue points, and we end up with a classifier that has moved the boundary in this direction.",
    "start": "2004340",
    "end": "2013530"
  },
  {
    "text": "Conversely, if we pick Kappa is 4, well, then we're very concerned to make sure we don't mistake any blue points as red points,",
    "start": "2013530",
    "end": "2023545"
  },
  {
    "text": "and so we moved the boundary in this direction. And by varying Kappa,",
    "start": "2023545",
    "end": "2029120"
  },
  {
    "text": "we will vary our preference for what type of errors we're willing to tolerate.",
    "start": "2029120",
    "end": "2036180"
  },
  {
    "text": "So let's summarize. A classifier is a predictor when the raw output is categorical.",
    "start": "2041950",
    "end": "2049615"
  },
  {
    "text": "We have categories v_1 through v_K. When K is 2 it's called a Boolean classifier.",
    "start": "2049615",
    "end": "2056014"
  },
  {
    "text": "When K is greater than 2 it's called a multi-class classifier. Now we might have various error rates summarized in a confusion matrix.",
    "start": "2056015",
    "end": "2065359"
  },
  {
    "text": "Now, when we're fitting a classifier to training data by an ERM or a regularized ERM,",
    "start": "2065360",
    "end": "2072605"
  },
  {
    "text": "we embed the raw output v into R_m using Psi.",
    "start": "2072605",
    "end": "2078000"
  },
  {
    "text": "The vectors Psi 1 through Psi K are then the embeddings of our targets v_1 through v_K,",
    "start": "2078670",
    "end": "2088319"
  },
  {
    "text": "which now use ERM to build a predictor for y.",
    "start": "2088660",
    "end": "2094085"
  },
  {
    "text": "We un-embed to get y. Um, we- our predictor will give us a y hat,",
    "start": "2094085",
    "end": "2101405"
  },
  {
    "text": "which we can un-embed to get, uh, a class prediction v hat. We do this by nearest neighbor un-embedding.",
    "start": "2101405",
    "end": "2109710"
  },
  {
    "text": "Next section we will talk about special loss functions for classifiers.",
    "start": "2110260",
    "end": "2115710"
  }
]