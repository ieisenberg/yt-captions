[
  {
    "start": "0",
    "end": "1040"
  },
  {
    "text": "So here we are going to do\nour final session in the model",
    "start": "1040",
    "end": "4670"
  },
  {
    "text": "selection chapter.",
    "start": "4670",
    "end": "7520"
  },
  {
    "text": "And we're going to look at\nridge regression and lasso.",
    "start": "7520",
    "end": "10730"
  },
  {
    "text": "And once again, we're\nusing our R Markdown",
    "start": "10730",
    "end": "13250"
  },
  {
    "text": "in RStudio that will help\nus create a nice document",
    "start": "13250",
    "end": "16970"
  },
  {
    "text": "at the end of the session.",
    "start": "16970",
    "end": "18390"
  },
  {
    "text": "And as we've seen, it's just\nas simple as having a script,",
    "start": "18390",
    "end": "22890"
  },
  {
    "text": "but it's better because\nyou get nice annotation",
    "start": "22890",
    "end": "25099"
  },
  {
    "text": "at the end of the day.",
    "start": "25100",
    "end": "26980"
  },
  {
    "text": "So let's have a\nlook at the screen.",
    "start": "26980",
    "end": "30310"
  },
  {
    "text": "And we see our\nheading, we've got",
    "start": "30310",
    "end": "32800"
  },
  {
    "text": "ridge regression and the lasso.",
    "start": "32800",
    "end": "34750"
  },
  {
    "text": "And for this, we're going\nto use the package GLMnet.",
    "start": "34750",
    "end": "38290"
  },
  {
    "text": "GLMnet is a package that\nI actually manage on CRAN,",
    "start": "38290",
    "end": "43060"
  },
  {
    "text": "and it's been created by\nmyself, Jerome Friedman and Rob",
    "start": "43060",
    "end": "47860"
  },
  {
    "text": "Tibshirani for fitting\nlasso models, ridge models,",
    "start": "47860",
    "end": "51010"
  },
  {
    "text": "and a whole class of models\nin between, elastic net, which",
    "start": "51010",
    "end": "54969"
  },
  {
    "text": "we won't cover in this class.",
    "start": "54970",
    "end": "57490"
  },
  {
    "text": "And for a lot of\ndifferent loss functions.",
    "start": "57490",
    "end": "59690"
  },
  {
    "text": "So we can do\nlogistic regressions,",
    "start": "59690",
    "end": "61840"
  },
  {
    "text": "ordinary regressions, and\na variety of other models.",
    "start": "61840",
    "end": "66060"
  },
  {
    "text": "We won't be using\nordinary regression yet.",
    "start": "66060",
    "end": "69250"
  },
  {
    "text": "So we'll run library GLMnet.",
    "start": "69250",
    "end": "72920"
  },
  {
    "text": "And GLMnet doesn't use\na formula language.",
    "start": "72920",
    "end": "78350"
  },
  {
    "text": "So we require to\ngive it a matrix",
    "start": "78350",
    "end": "81020"
  },
  {
    "text": "x of predictors and a\nresponse vector in this case.",
    "start": "81020",
    "end": "85079"
  },
  {
    "text": "So we create those\nby using methods,",
    "start": "85080",
    "end": "89930"
  },
  {
    "text": "by this time we know how.",
    "start": "89930",
    "end": "91290"
  },
  {
    "text": "So we make an x and a y,\nwhich is the predictor",
    "start": "91290",
    "end": "95000"
  },
  {
    "text": "matrix and the response.",
    "start": "95000",
    "end": "97190"
  },
  {
    "text": "And we'll first foot a\nridge regression model.",
    "start": "97190",
    "end": "101070"
  },
  {
    "text": "So GLMnet's got\nan alpha argument.",
    "start": "101070",
    "end": "104840"
  },
  {
    "text": "And alpha equals 1 is lasso,\nand alpha equals 0 is ridge.",
    "start": "104840",
    "end": "109570"
  },
  {
    "text": "And if you look\non the help file,",
    "start": "109570",
    "end": "111470"
  },
  {
    "text": "you'll see instructions\non how to on some",
    "start": "111470",
    "end": "114260"
  },
  {
    "text": "of the other arguments.",
    "start": "114260",
    "end": "115680"
  },
  {
    "text": "And for alphas in\nbetween 0 and 1,",
    "start": "115680",
    "end": "117920"
  },
  {
    "text": "you get what's called\nelastic net models which",
    "start": "117920",
    "end": "120229"
  },
  {
    "text": "are in between ridge and lasso.",
    "start": "120230",
    "end": "122150"
  },
  {
    "text": "But for us now, we'll just\nfocus on alpha equals 0,",
    "start": "122150",
    "end": "125390"
  },
  {
    "text": "which is ridge.",
    "start": "125390",
    "end": "126680"
  },
  {
    "text": "And so we do that, and it\ncomes back very quickly.",
    "start": "126680",
    "end": "133860"
  },
  {
    "text": "GLMnet's is extremely fast,\nand this isn't a big data set,",
    "start": "133860",
    "end": "136400"
  },
  {
    "text": "so it's especially fast.",
    "start": "136400",
    "end": "138230"
  },
  {
    "text": "Let's use the plot method\nfor a GLMnet object.",
    "start": "138230",
    "end": "141980"
  },
  {
    "text": "And this is interesting,\nit makes a plot",
    "start": "141980",
    "end": "147700"
  },
  {
    "text": "as a function of log of lambda.",
    "start": "147700",
    "end": "150849"
  },
  {
    "text": "And what it's plotting\nare the coefficients.",
    "start": "150850",
    "end": "153940"
  },
  {
    "text": "So if you go back to the\nlectures on ridge regression",
    "start": "153940",
    "end": "159610"
  },
  {
    "text": "and lasso, you'll\nsee ridge regression",
    "start": "159610",
    "end": "165320"
  },
  {
    "text": "is penalized by the sum of\nsquares of the coefficients.",
    "start": "165320",
    "end": "168580"
  },
  {
    "text": "So a penalty's put on the sum\nof squares of the coefficients,",
    "start": "168580",
    "end": "171490"
  },
  {
    "text": "and that's controlled\nby a parameter, lambda.",
    "start": "171490",
    "end": "174650"
  },
  {
    "text": "So the criterion\nfor ridge regression",
    "start": "174650",
    "end": "179420"
  },
  {
    "text": "is residual sum\nof squares, which",
    "start": "179420",
    "end": "185270"
  },
  {
    "text": "is the usual for linear\nregression, plus lambda,",
    "start": "185270",
    "end": "189770"
  },
  {
    "text": "times the summation, j equals\n1 to p of beta j squared.",
    "start": "189770",
    "end": "197980"
  },
  {
    "text": "So it's trying to minimize\nthe residual sum of squares.",
    "start": "197980",
    "end": "202400"
  },
  {
    "text": "But it's been modified by a\npenalty on the sum of squares",
    "start": "202400",
    "end": "206750"
  },
  {
    "text": "of the coefficients.",
    "start": "206750",
    "end": "208310"
  },
  {
    "text": "So if lambda is\nbig, you're going",
    "start": "208310",
    "end": "210380"
  },
  {
    "text": "to want the sum of squares of\nthe coefficients to be small,",
    "start": "210380",
    "end": "213790"
  },
  {
    "text": "so that will shrink the\ncoefficients towards zero.",
    "start": "213790",
    "end": "216799"
  },
  {
    "text": "And as lambda gets very big,\nthe coefficients will all be 0.",
    "start": "216800",
    "end": "221270"
  },
  {
    "text": "And so what GLMnet\ndoes is it actually",
    "start": "221270",
    "end": "224570"
  },
  {
    "text": "develops a whole path of models\non a grid of values of lambda,",
    "start": "224570",
    "end": "228230"
  },
  {
    "text": "quite a fine grid, about\n100 values of lambda.",
    "start": "228230",
    "end": "232010"
  },
  {
    "text": "And so we see on the log scale,\nwhen log of lambda is 12,",
    "start": "232010",
    "end": "236540"
  },
  {
    "text": "all the coefficients\nare essentially 0.",
    "start": "236540",
    "end": "239360"
  },
  {
    "text": "Then as we relax lambda, the\ncoefficients grow away from 0",
    "start": "239360",
    "end": "243200"
  },
  {
    "text": "in a nice, smooth\nway, and the sum",
    "start": "243200",
    "end": "248540"
  },
  {
    "text": "of squares of the coefficients\nis getting bigger and bigger,",
    "start": "248540",
    "end": "251360"
  },
  {
    "text": "until we reach a point where\nlambda is effectively 0",
    "start": "251360",
    "end": "255950"
  },
  {
    "text": "and the coefficients\nare unregularized.",
    "start": "255950",
    "end": "258445"
  },
  {
    "text": "And so these would\nbe the coefficients",
    "start": "258446",
    "end": "260028"
  },
  {
    "text": "that you get from an\nordinary least squares fit",
    "start": "260029",
    "end": "263240"
  },
  {
    "text": "of these variables.",
    "start": "263240",
    "end": "264440"
  },
  {
    "text": "So GLMnet will create the\nwhole path of variables.",
    "start": "264440",
    "end": "267560"
  },
  {
    "text": "So unlike subset and\nforward stepwise regression,",
    "start": "267560",
    "end": "271800"
  },
  {
    "text": "which controls the\ncomplexity of a model",
    "start": "271800",
    "end": "274250"
  },
  {
    "text": "by restricting the\nnumber of variables,",
    "start": "274250",
    "end": "277010"
  },
  {
    "text": "ridge regression keeps all\nthe variables in and shrinks",
    "start": "277010",
    "end": "280130"
  },
  {
    "text": "the coefficients towards 0.",
    "start": "280130",
    "end": "283030"
  },
  {
    "text": "So It gives a\nwhole path, we need",
    "start": "283030",
    "end": "291180"
  },
  {
    "text": "to pick a value along the path.",
    "start": "291180",
    "end": "294370"
  },
  {
    "text": "And GLMnet's got a\nbuilt-in function",
    "start": "294370",
    "end": "296970"
  },
  {
    "text": "called cv dot glmnet that'll\ndo cross-validation for you.",
    "start": "296970",
    "end": "301570"
  },
  {
    "text": "k-fold cross-validation.",
    "start": "301570",
    "end": "303340"
  },
  {
    "text": "And so we'll run cv dot ridge,\nit's done by default 10-fold",
    "start": "303340",
    "end": "307169"
  },
  {
    "text": "cross-validation.",
    "start": "307170",
    "end": "308380"
  },
  {
    "text": "And then we can do a plot,\nthere's a plot method for that.",
    "start": "308380",
    "end": "311730"
  },
  {
    "text": "And it gives you a nice plot of\nthe cross-validated mean-squared",
    "start": "311730",
    "end": "316710"
  },
  {
    "text": "error.",
    "start": "316710",
    "end": "317699"
  },
  {
    "text": "And there it goes.",
    "start": "317700",
    "end": "319510"
  },
  {
    "text": "And you see it dips down.",
    "start": "319510",
    "end": "321330"
  },
  {
    "text": "In the beginning, the mean\nsquared error is very high,",
    "start": "321330",
    "end": "323889"
  },
  {
    "text": "the coefficients are\nrestricted to be too small.",
    "start": "323890",
    "end": "326340"
  },
  {
    "text": "And then at some\npoint, it levels off.",
    "start": "326340",
    "end": "329610"
  },
  {
    "text": "This seems to indicate that the\nfull model is doing a good job.",
    "start": "329610",
    "end": "334530"
  },
  {
    "text": "And there's two vertical lines,\nthe one is at the minimum,",
    "start": "334530",
    "end": "337740"
  },
  {
    "text": "and the other vertical line\nis at one standard error",
    "start": "337740",
    "end": "341160"
  },
  {
    "text": "of the minimum, within\none standard error.",
    "start": "341160",
    "end": "344110"
  },
  {
    "text": "So it's a slightly more\nrestricted model that",
    "start": "344110",
    "end": "347340"
  },
  {
    "text": "does almost as well\nas the minimum,",
    "start": "347340",
    "end": "349460"
  },
  {
    "text": "and sometimes we'll go for that.",
    "start": "349460",
    "end": "351380"
  },
  {
    "text": "And at the top here, it's\nindicating that at all stages,",
    "start": "351380",
    "end": "354140"
  },
  {
    "text": "there's all 20\nvariables in the model.",
    "start": "354140",
    "end": "356130"
  },
  {
    "text": "So that's our 19 variables\nplus the intercept.",
    "start": "356130",
    "end": "360340"
  },
  {
    "text": "So that's ridge regression using\nGLMnet Let's use a lasso model.",
    "start": "360340",
    "end": "366340"
  },
  {
    "text": "Now, if you recall,\nthe lasso was",
    "start": "366340",
    "end": "370310"
  },
  {
    "text": "similar to ridge regression,\nthe only thing that",
    "start": "370310",
    "end": "372770"
  },
  {
    "text": "was different was the penalty.",
    "start": "372770",
    "end": "374033"
  },
  {
    "start": "374033",
    "end": "376580"
  },
  {
    "text": "Let's get the pen on here.",
    "start": "376580",
    "end": "378900"
  },
  {
    "text": "So for lasso, we\nminimize the residual sum",
    "start": "378900",
    "end": "381870"
  },
  {
    "text": "of squares plus lambda.",
    "start": "381870",
    "end": "384210"
  },
  {
    "text": "And it's a very subtle change,\nwe got j equals 1 to p.",
    "start": "384210",
    "end": "389580"
  },
  {
    "text": "Instead of the sum of\nsquares of the coefficients,",
    "start": "389580",
    "end": "392229"
  },
  {
    "text": "we penalize the absolute\nvalues of the coefficients.",
    "start": "392230",
    "end": "396410"
  },
  {
    "text": "It's also controlling the\nsize of the coefficients,",
    "start": "396410",
    "end": "399590"
  },
  {
    "text": "and it seems like absolute\nvalue and sum of squares",
    "start": "399590",
    "end": "403400"
  },
  {
    "text": "would be rather similar.",
    "start": "403400",
    "end": "405389"
  },
  {
    "text": "But the lasso's got a\nsomewhat magical quality.",
    "start": "405390",
    "end": "408830"
  },
  {
    "text": "And by penalizing\nthe absolute values,",
    "start": "408830",
    "end": "411354"
  },
  {
    "text": "that's actually\ngoing to restrict",
    "start": "411355",
    "end": "412730"
  },
  {
    "text": "some of the coefficients\nto be exactly 0.",
    "start": "412730",
    "end": "415180"
  },
  {
    "start": "415180",
    "end": "419250"
  },
  {
    "text": "Oh, we got to get back\nto our controller.",
    "start": "419250",
    "end": "422310"
  },
  {
    "text": "So let's run that.",
    "start": "422310",
    "end": "424530"
  },
  {
    "text": "Again, it fits the whole path.",
    "start": "424530",
    "end": "426670"
  },
  {
    "text": "And when we make the plot\nagainst the log of lambda,",
    "start": "426670",
    "end": "430410"
  },
  {
    "text": "indeed, what you see is that\ninitially, all the coefficients",
    "start": "430410",
    "end": "435030"
  },
  {
    "text": "are 0.",
    "start": "435030",
    "end": "435900"
  },
  {
    "text": "And then the first coefficient\nseems to be a purple,",
    "start": "435900",
    "end": "440310"
  },
  {
    "text": "there's a pair of them there.",
    "start": "440310",
    "end": "441820"
  },
  {
    "text": "And then the blue ones\nbeing 0 up to this point,",
    "start": "441820",
    "end": "444070"
  },
  {
    "text": "then it jumps in, then\nthis red one jumps in.",
    "start": "444070",
    "end": "446410"
  },
  {
    "text": "So you see the\ncoefficients jump in,",
    "start": "446410",
    "end": "449070"
  },
  {
    "text": "having been 0 for the\nlength of the path.",
    "start": "449070",
    "end": "451870"
  },
  {
    "text": "And at the top of\nthe plot, we actually",
    "start": "451870",
    "end": "453600"
  },
  {
    "text": "see, as a function\nof lambda as well,",
    "start": "453600",
    "end": "456040"
  },
  {
    "text": "how many non-zero\nvariables are in the model.",
    "start": "456040",
    "end": "459140"
  },
  {
    "text": "So as we saw in\nthe lectures, lasso",
    "start": "459140",
    "end": "464450"
  },
  {
    "text": "is doing shrinkage and\nvariable selection.",
    "start": "464450",
    "end": "469180"
  },
  {
    "text": "When we fit the\nlasso, using GLMnet,",
    "start": "469180",
    "end": "471490"
  },
  {
    "text": "we didn't tell at alpha,\nbecause the default for alpha",
    "start": "471490",
    "end": "474009"
  },
  {
    "text": "is 1, which gives you the lasso.",
    "start": "474010",
    "end": "475620"
  },
  {
    "start": "475620",
    "end": "479870"
  },
  {
    "text": "We plotted the lasso\nagainst xvar equals lambda.",
    "start": "479870",
    "end": "484770"
  },
  {
    "text": "If you look at the\nhelp file for the plot,",
    "start": "484770",
    "end": "486590"
  },
  {
    "text": "you'll see that there's\nvarious choices.",
    "start": "486590",
    "end": "489270"
  },
  {
    "text": "And one of them is a\nreally interesting choice.",
    "start": "489270",
    "end": "491699"
  },
  {
    "text": "And it's deviance.",
    "start": "491700",
    "end": "493980"
  },
  {
    "text": "And what it means is, a\npercentage of deviance",
    "start": "493980",
    "end": "498140"
  },
  {
    "text": "explained, or percentage\nof, in the case",
    "start": "498140",
    "end": "501620"
  },
  {
    "text": "of regression your sum\nof square's explained,",
    "start": "501620",
    "end": "503540"
  },
  {
    "text": "which is the r squared.",
    "start": "503540",
    "end": "505140"
  },
  {
    "text": "So if we make that plot,\nit's changed the orientation.",
    "start": "505140",
    "end": "510590"
  },
  {
    "text": "And at the bottom of here\nit says fraction of deviance",
    "start": "510590",
    "end": "513789"
  },
  {
    "text": "explained, so that's\nlike the r squared.",
    "start": "513789",
    "end": "516890"
  },
  {
    "text": "And what you find out here is\nthat a lot of the r-squared",
    "start": "516890",
    "end": "521860"
  },
  {
    "text": "was explained for quite\nheavily-shrunk coefficients.",
    "start": "521860",
    "end": "526839"
  },
  {
    "text": "And towards the end, with\na relatively small increase",
    "start": "526840",
    "end": "530590"
  },
  {
    "text": "in r squared, from\nbetween 0.4 and 0.5,",
    "start": "530590",
    "end": "533600"
  },
  {
    "text": "the coefficients\ngrow very large.",
    "start": "533600",
    "end": "535850"
  },
  {
    "text": "And so this may be an indication\nthat at the end of the path",
    "start": "535850",
    "end": "539290"
  },
  {
    "text": "it's overfitting.",
    "start": "539290",
    "end": "540730"
  },
  {
    "text": "So these different ways of\nplotting the coefficients",
    "start": "540730",
    "end": "545740"
  },
  {
    "text": "give you different information\nabout the coefficients",
    "start": "545740",
    "end": "548620"
  },
  {
    "text": "and about the\nnature of the path.",
    "start": "548620",
    "end": "552660"
  },
  {
    "text": "We can use cross-validation\nagain and plot it,",
    "start": "552660",
    "end": "557310"
  },
  {
    "text": "and see here's cross\nvalidation for the lasso.",
    "start": "557310",
    "end": "560460"
  },
  {
    "text": "And it's telling us that the\nminimum cross-validation error",
    "start": "560460",
    "end": "565110"
  },
  {
    "text": "is for size 15.",
    "start": "565110",
    "end": "568440"
  },
  {
    "text": "And within one standard error,\nwe have a model of size about 6.",
    "start": "568440",
    "end": "573190"
  },
  {
    "text": "And it's somewhat\nflat in between,",
    "start": "573190",
    "end": "576780"
  },
  {
    "text": "and so we may make our\nchoice based on that.",
    "start": "576780",
    "end": "580560"
  },
  {
    "text": "There's a coefficient\nfunction extractive",
    "start": "580560",
    "end": "583560"
  },
  {
    "text": "that works on a\ncross-validation object.",
    "start": "583560",
    "end": "586560"
  },
  {
    "text": "And remember the\nfit dot lasso will",
    "start": "586560",
    "end": "591510"
  },
  {
    "text": "have the whole path\nof coefficients,",
    "start": "591510",
    "end": "593110"
  },
  {
    "text": "So it's got roughly 100\ncoefficient vectors,",
    "start": "593110",
    "end": "596310"
  },
  {
    "text": "depending on each index by\ndifferent values of lambda.",
    "start": "596310",
    "end": "600010"
  },
  {
    "text": "But if we extract the\ncoefficient from the cv object,",
    "start": "600010",
    "end": "602910"
  },
  {
    "text": "it will pick the coefficient\nvector corresponding",
    "start": "602910",
    "end": "605459"
  },
  {
    "text": "to the best model, which in\nthis case was the model of size,",
    "start": "605460",
    "end": "615760"
  },
  {
    "text": "well, let's see what size.",
    "start": "615760",
    "end": "617180"
  },
  {
    "text": "1, 2, 3, 4, 5, so I guess it's\npicked this model over here,",
    "start": "617180",
    "end": "624750"
  },
  {
    "text": "so it's within one standard\nerror of the minimum.",
    "start": "624750",
    "end": "628530"
  },
  {
    "text": "So it's airing on the\nmore parsimonious side,",
    "start": "628530",
    "end": "632240"
  },
  {
    "text": "we know that the\ncross-validation error",
    "start": "632240",
    "end": "634740"
  },
  {
    "text": "is measured with some variance,\nand so that's the model that's",
    "start": "634740",
    "end": "639990"
  },
  {
    "text": "actually chosen.",
    "start": "639990",
    "end": "640745"
  },
  {
    "start": "640745",
    "end": "645820"
  },
  {
    "text": "We will end this session by\nusing our earlier validation",
    "start": "645820",
    "end": "652380"
  },
  {
    "text": "training division to\nuse a validation set",
    "start": "652380",
    "end": "656340"
  },
  {
    "text": "to select the lasso model.",
    "start": "656340",
    "end": "659880"
  },
  {
    "text": "And that's also easy to do.",
    "start": "659880",
    "end": "662150"
  },
  {
    "text": "So we've still got our\nvariable train hanging around,",
    "start": "662150",
    "end": "665190"
  },
  {
    "text": "index variable.",
    "start": "665190",
    "end": "666550"
  },
  {
    "text": "So we'll fit the\npath of lasso models,",
    "start": "666550",
    "end": "669390"
  },
  {
    "text": "using x train and y train.",
    "start": "669390",
    "end": "673130"
  },
  {
    "text": "And you can do a\nsummary of the fit.",
    "start": "673130",
    "end": "676715"
  },
  {
    "text": "And it actually tells us.",
    "start": "676715",
    "end": "680320"
  },
  {
    "text": "the summary of a GLMnet fit for\neach of the models in the path.",
    "start": "680320",
    "end": "685480"
  },
  {
    "text": "It gives you the\ndegrees of freedom,",
    "start": "685480",
    "end": "686980"
  },
  {
    "text": "which is the number of non-zero\ncoefficients, the percentage",
    "start": "686980",
    "end": "690550"
  },
  {
    "text": "deviance explained,\nwhich is r squared",
    "start": "690550",
    "end": "692950"
  },
  {
    "text": "for generalized linear models.",
    "start": "692950",
    "end": "694815"
  },
  {
    "text": "But this case, it's\nsquared error loss,",
    "start": "694815",
    "end": "696440"
  },
  {
    "text": "so that's just r squared on\nthe squared error loss scale,",
    "start": "696440",
    "end": "699430"
  },
  {
    "text": "and the lambda value that\ncorresponded to that fit.",
    "start": "699430",
    "end": "703740"
  },
  {
    "text": "And it actually tries to\nfit 100 values of lambda.",
    "start": "703740",
    "end": "707860"
  },
  {
    "text": "But if it reaches a point\nwhere nothing much is changing,",
    "start": "707860",
    "end": "710399"
  },
  {
    "text": "like you can see nothing\nmuch is changing here,",
    "start": "710400",
    "end": "712358"
  },
  {
    "text": "it stops, because it's not\nreally making any progress.",
    "start": "712358",
    "end": "715730"
  },
  {
    "text": "So as lambda is\ngetting smaller here,",
    "start": "715730",
    "end": "717339"
  },
  {
    "text": "the model is really\nnot changing,",
    "start": "717340",
    "end": "718730"
  },
  {
    "text": "it's pretty much at the\nordinary least squares fit,",
    "start": "718730",
    "end": "724120"
  },
  {
    "text": "unpenalised model.",
    "start": "724120",
    "end": "727870"
  },
  {
    "text": "So now we can make predictions\non our left out data.",
    "start": "727870",
    "end": "731770"
  },
  {
    "text": "So that's indexes x by minus\ntrain, much like we did before.",
    "start": "731770",
    "end": "737050"
  },
  {
    "text": "And if we do dim\nof predict, there's",
    "start": "737050",
    "end": "738940"
  },
  {
    "text": "83 observations in\nthe validation set.",
    "start": "738940",
    "end": "742980"
  },
  {
    "text": "And we've got 89\nvalues of lambda,",
    "start": "742980",
    "end": "745570"
  },
  {
    "text": "so there's going to be\n89 different columns",
    "start": "745570",
    "end": "748050"
  },
  {
    "text": "in this prediction matrix.",
    "start": "748050",
    "end": "751380"
  },
  {
    "text": "Here's an interesting\nlittle command here.",
    "start": "751380",
    "end": "756120"
  },
  {
    "text": "We want to compute our\nsum of squared errors.",
    "start": "756120",
    "end": "759140"
  },
  {
    "text": "Well, y minus train is\na vector of length 83.",
    "start": "759140",
    "end": "763280"
  },
  {
    "text": "But pred is a matrix 83 by 89.",
    "start": "763280",
    "end": "767130"
  },
  {
    "text": "And here we happily say\ny minus train minus pred.",
    "start": "767130",
    "end": "770900"
  },
  {
    "text": "Well, what it does is, it\nrecycles this vector 89 times,",
    "start": "770900",
    "end": "775910"
  },
  {
    "text": "and it does it column\nwise, so that it",
    "start": "775910",
    "end": "778730"
  },
  {
    "text": "can compute this difference.",
    "start": "778730",
    "end": "780120"
  },
  {
    "text": "So the result of this here\nis a matrix that's 83 by 89,",
    "start": "780120",
    "end": "784670"
  },
  {
    "text": "even though we only\ngave y as a vector,",
    "start": "784670",
    "end": "786950"
  },
  {
    "text": "it recycled it, which is a\ntrick, but it's a handy trick.",
    "start": "786950",
    "end": "791180"
  },
  {
    "text": "We square those\nerrors, we then use",
    "start": "791180",
    "end": "794060"
  },
  {
    "text": "a ply to compute the\nmeans in each column",
    "start": "794060",
    "end": "797480"
  },
  {
    "text": "of the squared errors,\nand we put that in,",
    "start": "797480",
    "end": "800940"
  },
  {
    "text": "and then we take\nthe square root.",
    "start": "800940",
    "end": "802470"
  },
  {
    "text": "So there's a whole bunch of\ncommands all in one there.",
    "start": "802470",
    "end": "804720"
  },
  {
    "text": "And now we can plot that\nas a function of lambda.",
    "start": "804720",
    "end": "811639"
  },
  {
    "text": "And we see a nice\nvalidation curve,",
    "start": "811640",
    "end": "816470"
  },
  {
    "text": "it goes down, and\nthen climbs up again.",
    "start": "816470",
    "end": "819000"
  },
  {
    "text": "This will be overfitting,\nthis is underfitting,",
    "start": "819000",
    "end": "822100"
  },
  {
    "text": "and this seems to be the\nsweet spot over here.",
    "start": "822100",
    "end": "824670"
  },
  {
    "text": "And we can extract\nthe best lambda.",
    "start": "824670",
    "end": "828120"
  },
  {
    "text": "And I do this by indexing.",
    "start": "828120",
    "end": "834330"
  },
  {
    "text": "There's a component on the\nGLMnet object called lambda,",
    "start": "834330",
    "end": "839240"
  },
  {
    "text": "so this extracts that component.",
    "start": "839240",
    "end": "841540"
  },
  {
    "text": "And then I index it by order\nof rmse, so that's our root",
    "start": "841540",
    "end": "845940"
  },
  {
    "text": "mean squared error.",
    "start": "845940",
    "end": "848070"
  },
  {
    "text": "Order puts them in\nascending order,",
    "start": "848070",
    "end": "851740"
  },
  {
    "text": "and so we want the index of the\nfirst, or the smallest value,",
    "start": "851740",
    "end": "856770"
  },
  {
    "text": "and that will pick\nup the best lambda.",
    "start": "856770",
    "end": "859350"
  },
  {
    "text": "So that's a way of\ndoing that, and we",
    "start": "859350",
    "end": "862110"
  },
  {
    "text": "find the best lambda is 19.98.",
    "start": "862110",
    "end": "865740"
  },
  {
    "text": "And so now we can look at the\ncoefficients corresponding",
    "start": "865740",
    "end": "869220"
  },
  {
    "text": "to that variable,\nand that will give us",
    "start": "869220",
    "end": "871050"
  },
  {
    "text": "a subset of the coefficients.",
    "start": "871050",
    "end": "873390"
  },
  {
    "text": "And the dots correspond\nto the 0's, so those",
    "start": "873390",
    "end": "876140"
  },
  {
    "text": "are the ones that are missing.",
    "start": "876140",
    "end": "877390"
  },
  {
    "text": "And so they're not\nmissing, but 0's.",
    "start": "877390",
    "end": "880320"
  },
  {
    "text": "This is actually\nprinted out in what's",
    "start": "880320",
    "end": "882330"
  },
  {
    "text": "known as a sparse\nmatrix format, which",
    "start": "882330",
    "end": "886940"
  },
  {
    "text": "means only the non-zero\nvalues are actually recorded.",
    "start": "886940",
    "end": "892100"
  },
  {
    "text": "So there's our\ncoefficient vector.",
    "start": "892100",
    "end": "894630"
  },
  {
    "text": "And that's the end\nof the session.",
    "start": "894630",
    "end": "897510"
  },
  {
    "text": "So to summarize, in\nthe last few sessions,",
    "start": "897510",
    "end": "904480"
  },
  {
    "text": "we've looked at model\nselection, we've",
    "start": "904480",
    "end": "907959"
  },
  {
    "text": "looked at a variety\nof different methods,",
    "start": "907960",
    "end": "910010"
  },
  {
    "text": "best subset, forward, stepwise,\nridge regression, lasso.",
    "start": "910010",
    "end": "913810"
  },
  {
    "text": "We've used validation\nsets to select models,",
    "start": "913810",
    "end": "917529"
  },
  {
    "text": "we've used cp to select\nmodels, and we've also",
    "start": "917530",
    "end": "920110"
  },
  {
    "text": "used k-fold cross-validation.",
    "start": "920110",
    "end": "922730"
  },
  {
    "text": "We've also seen how to use R\nMarkdown to actually produce",
    "start": "922730",
    "end": "928190"
  },
  {
    "text": "a nice recording of\neverything we've done.",
    "start": "928190",
    "end": "933140"
  },
  {
    "text": "And we hit the Knit HTML at\nthe top of the page here.",
    "start": "933140",
    "end": "938310"
  },
  {
    "text": "And this is for the entire set\nof sessions on model selection.",
    "start": "938310",
    "end": "942990"
  },
  {
    "text": "And you get a really nice\nsummary of everything",
    "start": "942990",
    "end": "945830"
  },
  {
    "text": "you've done, all the\noutput, all the plots,",
    "start": "945830",
    "end": "948170"
  },
  {
    "text": "nicely formatted, and it\nmakes for a good report.",
    "start": "948170",
    "end": "952730"
  },
  {
    "text": "Think about if you're doing\na data analysis for a client,",
    "start": "952730",
    "end": "956839"
  },
  {
    "text": "this is a really\nnice way of showing",
    "start": "956840",
    "end": "960770"
  },
  {
    "text": "the output, it\nshows what you did,",
    "start": "960770",
    "end": "962390"
  },
  {
    "text": "and the results of what you did.",
    "start": "962390",
    "end": "965060"
  },
  {
    "text": "And so that's available,\nit's a web page,",
    "start": "965060",
    "end": "968010"
  },
  {
    "text": "and so you can distribute\nthe link to that web page,",
    "start": "968010",
    "end": "971450"
  },
  {
    "text": "you can put it in a blog, you\ncan do what you like with it.",
    "start": "971450",
    "end": "976990"
  },
  {
    "text": "What we didn't cover was\nprincipal component regression",
    "start": "976990",
    "end": "981880"
  },
  {
    "text": "and partial least squares.",
    "start": "981880",
    "end": "983590"
  },
  {
    "text": "There's a lab in the\nend of the chapter,",
    "start": "983590",
    "end": "987040"
  },
  {
    "text": "but by now you'll have\nlearnt enough tools,",
    "start": "987040",
    "end": "989509"
  },
  {
    "text": "you'll be able to go through\nthat lab on your own.",
    "start": "989510",
    "end": "993000"
  },
  {
    "start": "993000",
    "end": "995000"
  }
]