[
  {
    "start": "0",
    "end": "10190"
  },
  {
    "text": "Thank you so much for\nthis introduction. Thank you all for\ncoming it's always great to be here at\nStanford to learn",
    "start": "10190",
    "end": "15980"
  },
  {
    "text": "about all the exciting research\nthat is happening here. I actually didn't come\nall the way from Zurich. I'm spending some\ntime in Cambridge",
    "start": "15980",
    "end": "22238"
  },
  {
    "text": "right now in Boston\nin the East Coast. I'd love to share\nsome work that we've",
    "start": "22238",
    "end": "27740"
  },
  {
    "text": "been doing in the group around\nsafe and efficient learning in the real world. And, of course,\nthis is joint work",
    "start": "27740",
    "end": "34100"
  },
  {
    "text": "with a fabulous group\nof PhD students, postdocs, and senior\ncollaborators who I'll acknowledge as we go along.",
    "start": "34100",
    "end": "41630"
  },
  {
    "text": "You probably all\nagree that these are really exciting times\nfor machine learning and artificial intelligence,\nfar beyond classifying images,",
    "start": "41630",
    "end": "48770"
  },
  {
    "text": "translating languages. These models now really exhibit\na pretty impressive level of generality. You can ask them to compose\npoems about and rap songs",
    "start": "48770",
    "end": "57920"
  },
  {
    "text": "around submodular optimization. There's examples of creating\nphotorealistic videos",
    "start": "57920",
    "end": "63644"
  },
  {
    "text": "from textual descriptions\nand the like. So this is certainly\nsuper exciting. What does it take to bring these\nadvances to the physical world,",
    "start": "63644",
    "end": "73219"
  },
  {
    "text": "applications like human-robot\ninteraction, advancing recommendations in health care,\nscientific discovery, precision",
    "start": "73220",
    "end": "78860"
  },
  {
    "text": "farming, and the likes? So as soon as you talk\nabout the real world, stakes are high right.",
    "start": "78860",
    "end": "84050"
  },
  {
    "text": "Failures are much\nmore problematic. And so what we've been\nreally interested in",
    "start": "84050",
    "end": "89510"
  },
  {
    "text": "is how can one enable\nlearning agents to actually learn by\ninteracting in the real world.",
    "start": "89510",
    "end": "97310"
  },
  {
    "text": "And I want to share\nsome of the thoughts that we've been\nexploring in this space. Now, the central paradigm\nin machine learning",
    "start": "97310",
    "end": "103280"
  },
  {
    "text": "in these contexts is\nreinforcement learning, which, of course, this is an\nabstract model of an agent that situated in an environment which\nit can affect by carrying out",
    "start": "103280",
    "end": "110540"
  },
  {
    "text": "actions that cause the\nenvironment to change its state in a Markovian\nfashion and the agent to derive some reward.",
    "start": "110540",
    "end": "116060"
  },
  {
    "text": "And, of course, the same model\nhas been studied in control. So adaptive control,\napproximate dynamic programming,",
    "start": "116060",
    "end": "122420"
  },
  {
    "text": "and the likes. And a key challenge\nin this domain is that the agent doesn't\nknow how the world works,",
    "start": "122420",
    "end": "128100"
  },
  {
    "text": "so it faces the dilemma\nof trading exploration, so experimenting with actions\nto learn about the consequences",
    "start": "128100",
    "end": "133230"
  },
  {
    "text": "and exploitation using what it\nlearned in order to do well. And so we've also seen these\npretty remarkable advances",
    "start": "133230",
    "end": "140700"
  },
  {
    "text": "a few years back in\ngameplay, but also more recently in challenging\nrobotics applications.",
    "start": "140700",
    "end": "146820"
  },
  {
    "text": "Our colleagues in\nZurich, for example, use them to have robots go\nhike or fly drones faster",
    "start": "146820",
    "end": "152490"
  },
  {
    "text": "than human pilots and so on. So it's certainly pretty\nexciting advances in this space.",
    "start": "152490",
    "end": "158269"
  },
  {
    "text": "Now, RL is portrayed\nin this cartoon that the agent goes out\nand learns by interacting with the real world.",
    "start": "158270",
    "end": "163909"
  },
  {
    "text": "But, of course, in truth,\nthe agent lives in a matrix. So the agent really needs\nan accurate simulator that",
    "start": "163910",
    "end": "171349"
  },
  {
    "text": "describes how the world works. And a lot of\napplications, like those",
    "start": "171350",
    "end": "176780"
  },
  {
    "text": "that I've mentioned\nbefore, that's the best approximate simulators\nof how the real world would work right.",
    "start": "176780",
    "end": "181970"
  },
  {
    "text": "And applications like\nscientific discovery, you do quite a bit of work\naround self-driving labs and autonomous\nexperimentation and so.",
    "start": "181970",
    "end": "187400"
  },
  {
    "text": "I mean, that's the whole\npurpose of science-- to learn how the world works. And so the question is, now,\nif you go to these domains,",
    "start": "187400",
    "end": "195230"
  },
  {
    "text": "then exploration is costly. You need to carry\nout experiments.",
    "start": "195230",
    "end": "200340"
  },
  {
    "text": "So you really want to be\nsample-efficient and potentially dangerous. And so we've been\nreally interested in how",
    "start": "200340",
    "end": "206239"
  },
  {
    "text": "can learning agents learn by\ninteracting with the real world efficiently and safely.",
    "start": "206240",
    "end": "211810"
  },
  {
    "text": "And that's what I want\nto talk a little bit about in this presentation. So I start with a\nline of work that I've",
    "start": "211810",
    "end": "217000"
  },
  {
    "text": "been pursuing now for a couple\nof years around Safe Bayesian Optimization and\nthen build on that to talk about some recent\ndirections that we're exploring.",
    "start": "217000",
    "end": "224500"
  },
  {
    "text": "And so let me start with\nthe motivating example. So this is a collaboration\nwith the Paul Scherrer Institute, a large research\nfacility in Switzerland.",
    "start": "224500",
    "end": "232152"
  },
  {
    "text": "This is the SwissFEL\nFree Electron Laser. It's a 700-meter long linear\naccelerator that generates X-ray",
    "start": "232152",
    "end": "237400"
  },
  {
    "text": "pulses of extremely\nshort duration, so femtosecond duration,\none millionth of a billionth of a second.",
    "start": "237400",
    "end": "242470"
  },
  {
    "text": "So you can use this\nto image very fast processes like molecules\nturning each other in context of a chemical reaction, super\nimportant applications from drug",
    "start": "242470",
    "end": "250360"
  },
  {
    "text": "design to materials\ndiscovery to many others. It's also a pretty\ncomplicated device. So the beam properties are\ndependent on carefully arranged",
    "start": "250360",
    "end": "258327"
  },
  {
    "text": "set of magnets that partly\nactually are physically moved around. The beam properties\nvary quite substantially",
    "start": "258327",
    "end": "264563"
  },
  {
    "text": "based on subtle environmental\nconditions like temperature, humidity-- have to be adapted\nbased on target application",
    "start": "264563",
    "end": "270970"
  },
  {
    "text": "requirements and so on. So it's not just that you set\nthis up once, and you're done. But you have to\nconstantly adapt it.",
    "start": "270970",
    "end": "277050"
  },
  {
    "text": "It's also safety-critical. So if you're not\ncareful what you do, you can basically damage the\nmachine, damage the magnets",
    "start": "277050",
    "end": "283169"
  },
  {
    "text": "and destroy it. So you really have\nto be careful. But fortunately,\nof course, there's monitors all over this device.",
    "start": "283170",
    "end": "288480"
  },
  {
    "text": "So that monitor\nhow close you get in terms of violating\nsome of the constraints.",
    "start": "288480",
    "end": "294860"
  },
  {
    "text": "There's another question is-- so maybe the other\nthing I want to mention is, in principle, we can\nsimulate the underlying physics,",
    "start": "294860",
    "end": "300440"
  },
  {
    "text": "but the simulations\nare extremely expensive at this\nlevel of accuracy. And given the fact\nthat you really need to adapt to different\nenvironmental conditions",
    "start": "300440",
    "end": "307395"
  },
  {
    "text": "and so on, you can't really run\nand prepare this in simulation. So essentially, you have to do\nthis tuning and experimentation",
    "start": "307395",
    "end": "314060"
  },
  {
    "text": "on the device\nwhile it's running. And so one way to think about\nthis very abstractly as sort",
    "start": "314060",
    "end": "319660"
  },
  {
    "text": "of trying to optimize a black\nbox, like this electron laser, by injecting some tuning\nparameters-- delta t over time--",
    "start": "319660",
    "end": "326140"
  },
  {
    "text": "and obtaining some measurements\nabout the rewards namely beam intensity, say. So other targets you care\nabout, as well as how close you",
    "start": "326140",
    "end": "334360"
  },
  {
    "text": "are to violating some\nof the constraints through these beam\nloss monitors. And given us information,\nyou'd like to close the loop.",
    "start": "334360",
    "end": "341000"
  },
  {
    "text": "The problem is, you don't know\nthe reward nor the constraints ahead of time.",
    "start": "341000",
    "end": "346290"
  },
  {
    "text": "You need to take your\nobservations into account in order to drive\nexperimentation. So in a way, what\nyou want to do is",
    "start": "346290",
    "end": "352490"
  },
  {
    "text": "you want to solve this nonlinear\nconstraint optimization problem, optimizing\nthis unknown reward subject to unknown constraints\nwhile maintaining feasibility",
    "start": "352490",
    "end": "359630"
  },
  {
    "text": "every step along the way. And, of course, in general,\nthis is a completely ill-posed problem. Without any prior\nassumptions, this is hopeless.",
    "start": "359630",
    "end": "367879"
  },
  {
    "text": "So one way to impose\nprior assumptions is to take a Bayesian\nperspective on this",
    "start": "367880",
    "end": "372900"
  },
  {
    "text": "and try to model uncertainty\nin the unknown objectives and the unknown constraints. And that's exactly philosophy\nbehind Bayesian optimization,",
    "start": "372900",
    "end": "379849"
  },
  {
    "text": "where one endows the unknown\nreward and constraint with a stochastic process\nprior, which one can then",
    "start": "379850",
    "end": "385640"
  },
  {
    "text": "use in order to\nguide experimentation while being cautious. And so the first result we\nhad about this a while back",
    "start": "385640",
    "end": "393140"
  },
  {
    "text": "is basically\noperationalizing this idea. So we model the reward\nand constraints in a way as samples of the\nprobabilistic model.",
    "start": "393140",
    "end": "399690"
  },
  {
    "text": "So a classical default\nin Bayesian optimization is Gaussian process models\nthat we analyze quite a bit.",
    "start": "399690",
    "end": "405550"
  },
  {
    "text": "You can also use Bayesian\nneural network models, and we'll talk a little bit\nmore about new network-based approaches later.",
    "start": "405550",
    "end": "411479"
  },
  {
    "text": "And then you can use\nthe uncertainty two on one side reason about\nconstraint satisfaction. So for example, if you\nuse a pessimistic estimate",
    "start": "411480",
    "end": "418470"
  },
  {
    "text": "of your constraints, the\nlower confidence bound about the constraints,\nyou can statistically",
    "start": "418470",
    "end": "424290"
  },
  {
    "text": "inner-approximate\nthe feasible region that holds with high probability\nas long as these models are",
    "start": "424290",
    "end": "430860"
  },
  {
    "text": "well-calibrated. And similarly, you can use\nyour confidence estimates about the rewards in\norder to, for example,",
    "start": "430860",
    "end": "436710"
  },
  {
    "text": "reason about what are subsets\nof the parameter space that are plausibly\noptimal, where",
    "start": "436710",
    "end": "442410"
  },
  {
    "text": "the optimistic estimate exceeds\nthe best pessimistic lower bound.",
    "start": "442410",
    "end": "447930"
  },
  {
    "text": "And so a natural\nalgorithm would be to basically try to explore\nwithin this plausibly optimal",
    "start": "447930",
    "end": "454199"
  },
  {
    "text": "region but being constrained\nin this red set, this feasible region. The problem is that,\nthen, you quickly",
    "start": "454200",
    "end": "459660"
  },
  {
    "text": "get stuck in suboptimal\nsolutions, where you also have to be care about is in a\nway to try to gather information",
    "start": "459660",
    "end": "465030"
  },
  {
    "text": "about what is feasible\nin the first place, and that's captured using\nthis notion of expanders. Of course, everything\nmathematically",
    "start": "465030",
    "end": "470725"
  },
  {
    "text": "defined in the paper,\nwhere you have an incentive to actually try to\nexplore the boundary",
    "start": "470725",
    "end": "475860"
  },
  {
    "text": "and discover new\nregions in the space that you can statistically\ncertify to be safe.",
    "start": "475860",
    "end": "481960"
  },
  {
    "text": "And eventually, you might find\nbetter solutions this way. That's a cartoon picture. Of course, the details\nare in the paper.",
    "start": "481960",
    "end": "487539"
  },
  {
    "text": "But in order to make\nideas like this work, you really need these models\nto be well-calibrated.",
    "start": "487540",
    "end": "492670"
  },
  {
    "text": "So you need to make sure that\nthis true unknown objective and true unknown constraint\nare actually contained within--",
    "start": "492670",
    "end": "499355"
  },
  {
    "text": "the credible sets of\nthe Bayesian model is also similarly frequentist\nperspectives on this.",
    "start": "499355",
    "end": "504770"
  },
  {
    "text": "And so now, of course,\nif you were a Bayesian, you had a perfectly\nwell-calibrated model. You could just take a\npoint, take your mean,",
    "start": "504770",
    "end": "511180"
  },
  {
    "text": "add some multiple times\nyour standard deviation, and enumerate your percentiles\non that marginal distribution.",
    "start": "511180",
    "end": "516309"
  },
  {
    "text": "But, of course, your model\nmight be miscalibrated. The true objective might lie\noutside of these credible sets.",
    "start": "516309",
    "end": "521799"
  },
  {
    "text": "And moreover, you want to ensure\nthat these bounds hold actually everywhere in the domain\nand uniformly over time.",
    "start": "521799",
    "end": "528010"
  },
  {
    "text": "And that's constructing\nmodels that produce these kind of\nwell-calibrated confidence sets that hold reliably\nuniformly over time, any time",
    "start": "528010",
    "end": "536150"
  },
  {
    "text": "valid. That's a very interesting and\nimportant theoretical question. And we've been\nspending quite a bit of time looking at\nthese types of problems,",
    "start": "536150",
    "end": "543769"
  },
  {
    "text": "and turns out it's actually\npossible to calibrate these uncertainty sets. So you need to, anyway, take\ninto account how, in a way,",
    "start": "543770",
    "end": "552440"
  },
  {
    "text": "the mismatch between\nthe underlying objective and these\nconfidence sets, as well as some notion of\ncapacity that, in a way,",
    "start": "552440",
    "end": "559100"
  },
  {
    "text": "you can use in order to get\nuniform bounds over the domain. So there's quite a bit of\nwork on the statistical side",
    "start": "559100",
    "end": "565040"
  },
  {
    "text": "that I'm not going\nto dwell on too much in this talk on trying\nto really understand in what circumstances.",
    "start": "565040",
    "end": "570199"
  },
  {
    "text": "You can analyze this. So the first works were\naround Gaussian process models and can extend\nthis to some extent to neural-network based\nmodels and other models",
    "start": "570200",
    "end": "576628"
  },
  {
    "text": "as well, and also other\ntypes of likelihoods. And now, under these types\nof regularity conditions,",
    "start": "576628",
    "end": "581900"
  },
  {
    "text": "regularity assumptions,\none can actually analyze this algorithm that\nI've described to you before and establish bounds\non how many samples",
    "start": "581900",
    "end": "588990"
  },
  {
    "text": "you need in order to find\na near-optimal reachable solution while\nensuring constraint satisfaction, with\nhigh probability",
    "start": "588990",
    "end": "594920"
  },
  {
    "text": "every step along the way. And so variants\nof this algorithm,",
    "start": "594920",
    "end": "600000"
  },
  {
    "text": "we actually got to deploy\non this real device. So here is a picture\nof my students",
    "start": "600000",
    "end": "605250"
  },
  {
    "text": "Johannes and Moynier, together\nwith our collaborator, Nicole, sitting in the operating\nroom of this machine running",
    "start": "605250",
    "end": "611820"
  },
  {
    "text": "the code in the background. And they would do\nexperiments, where they would start with\nparameter settings that",
    "start": "611820",
    "end": "617790"
  },
  {
    "text": "were chosen by the\ndomain experts, detune them so that the whole\nsystem would perform badly,",
    "start": "617790",
    "end": "622920"
  },
  {
    "text": "and then try to recover\nusing different strategies. And so there's local\nsearch-based strategies",
    "start": "622920",
    "end": "628649"
  },
  {
    "text": "that are used in this domain. And you can actually\ndo quite a bit better by the more global optimization,\nlet's say, Bayesian optimization",
    "start": "628650",
    "end": "634328"
  },
  {
    "text": "affords, and the\nsame time, actually have these safety guarantees.",
    "start": "634328",
    "end": "639330"
  },
  {
    "text": "And this is an early work. There's been now quite\na bit of applications of the safe Bayesian\noptimization machinery.",
    "start": "639330",
    "end": "644970"
  },
  {
    "text": "So we did another collaboration\naround large-scale scientific instruments. This is a high-intensity\nproton accelerator operated",
    "start": "644970",
    "end": "651900"
  },
  {
    "text": "by that same research center\nthat's even more safety-critical in some sense, so\nyou really have to be careful\nabout what happens.",
    "start": "651900",
    "end": "657380"
  },
  {
    "text": "And here, the\nsystem really allows you to actually find much better\nsolutions without generating beam interruptions.",
    "start": "657380",
    "end": "664720"
  },
  {
    "text": "We've been using this\nto tune gain parameters for high-precision motors used\nfor industrial manufacturing",
    "start": "664720",
    "end": "671590"
  },
  {
    "text": "tasks to control gate\nparameters for quadruped robots and the likes.",
    "start": "671590",
    "end": "677170"
  },
  {
    "text": "And so now, what I want to do\nin the remainder of this talk is to tell you about some\nideas towards scaling",
    "start": "677170",
    "end": "683740"
  },
  {
    "text": "these ideas to richer and\nmore complex applications. And so one, first topic that I\nwant to talk a little bit about",
    "start": "683740",
    "end": "690940"
  },
  {
    "text": "is where does the\nprior come from. So in Bayesian optimization,\nreally the key requirements",
    "start": "690940",
    "end": "698700"
  },
  {
    "text": "is you need a\nprobabilistic model that encodes your prior assumptions\nabout how the unknown objective",
    "start": "698700",
    "end": "705420"
  },
  {
    "text": "and rewards looks like. And if there's\none thing to learn from all the advances\nin deep learning",
    "start": "705420",
    "end": "711029"
  },
  {
    "text": "and now also in\ngenerative AI, it's really that if you have\nenough data learning good representations,\nyou can often",
    "start": "711030",
    "end": "717660"
  },
  {
    "text": "do better than hand-designed\nrepresentations. And so in addition, once you\nlearn good representations",
    "start": "717660",
    "end": "724230"
  },
  {
    "text": "from one task, often, you\ncan adapt it to fine-tune it to related tasks. For example, take image\nclassification models trained",
    "start": "724230",
    "end": "731279"
  },
  {
    "text": "on natural images and just\nusing a little bit of, say, medical data, you can adapt\nthe model in a very simple",
    "start": "731280",
    "end": "737790"
  },
  {
    "text": "efficient manner. So the question is,\ncan you do something like this for sequential\ndecision-making tasks like Bayesian optimization?",
    "start": "737790",
    "end": "745720"
  },
  {
    "text": "And so that's a very\nnatural perspective on this through the lens\nof Bayesian meta-learning.",
    "start": "745720",
    "end": "750790"
  },
  {
    "text": "There's some really interesting\nwork that's happening here at Stanford in this\ndomain, where you,",
    "start": "750790",
    "end": "758050"
  },
  {
    "text": "instead of starting with\na hand-designed prior distribution, maybe a Gaussian\nprocess prior with a fixed",
    "start": "758050",
    "end": "764200"
  },
  {
    "text": "kernel that you choose to step\nup one level in the Bayesian",
    "start": "764200",
    "end": "769390"
  },
  {
    "text": "hierarchy, to take a collection\nof tasks from related domains",
    "start": "769390",
    "end": "774490"
  },
  {
    "text": "and then try to-- starting with the\nhyper-prior on the parameters",
    "start": "774490",
    "end": "781420"
  },
  {
    "text": "of your prior distribution. Apply Bayesian inference\nor approximations to obtain a hyper-posterior\nthat, in a way,",
    "start": "781420",
    "end": "787720"
  },
  {
    "text": "absorbs all the information\nfrom related tasks and that you can hopefully\nadapt in a very sample efficient manner to new tasks.",
    "start": "787720",
    "end": "795730"
  },
  {
    "text": "So that's a classical idea. The question is how to get this\nto work with modern models.",
    "start": "795730",
    "end": "801209"
  },
  {
    "text": "And so the canonical approach\nof how to address this is to basically start with the\nhyper-prior on the parameters",
    "start": "801210",
    "end": "809710"
  },
  {
    "text": "of your prior distribution. So you can do this, for example,\nfor shared linear regression.",
    "start": "809710",
    "end": "815850"
  },
  {
    "text": "Maybe you have linear models. You have a common set of\ncoefficients across all tasks, and then you learn how to\ndeviate across different tasks",
    "start": "815850",
    "end": "823140"
  },
  {
    "text": "that you want to fit. The question is, how to do in\nthis basically non-parametric setting involving neural nets. And the default choice that's\noften done in the literature",
    "start": "823140",
    "end": "831090"
  },
  {
    "text": "is to start with a very\nuninformed isotropic prior. Just put a Gaussian maybe on\nthe coefficients, the weights",
    "start": "831090",
    "end": "837360"
  },
  {
    "text": "of the neural network. The problem with this is,\nit's very hard to reason about what kind of\nprior assumption",
    "start": "837360",
    "end": "842899"
  },
  {
    "text": "this really imposes on\nthis function space, and you also get often very\npoor empirical side effects if you do something like this.",
    "start": "842900",
    "end": "849170"
  },
  {
    "text": "And so what we've been exploring\nthe alternative is to, in a way, encode prior\nassumptions directly",
    "start": "849170",
    "end": "854690"
  },
  {
    "text": "on the space of functions. And the motivation, in a way,\nis the Gaussian processes",
    "start": "854690",
    "end": "860120"
  },
  {
    "text": "behave in a very\ninterpretable fashion. So what typically happens\nis close to your data, you're confident.",
    "start": "860120",
    "end": "865880"
  },
  {
    "text": "And as you go away from your\ndata, your uncertainty goes up. The question is,\nhow can I express these types of prior assumptions\nin this context of Bayesian",
    "start": "865880",
    "end": "873890"
  },
  {
    "text": "meta-learning with\nneural network models. And so that suggests a\nvery natural perspective",
    "start": "873890",
    "end": "880850"
  },
  {
    "text": "towards meta-learning,\nbasically trying to learn a model of this\nunderlying stochastic process",
    "start": "880850",
    "end": "887210"
  },
  {
    "text": "directly that governs the\nspace of reward function and constraints.",
    "start": "887210",
    "end": "892589"
  },
  {
    "text": "Now, that seems like\nan even harder problem. You start with the\nproblem of specifying a prior over large but\nfinite set of parameters.",
    "start": "892590",
    "end": "900608"
  },
  {
    "text": "Now, you want to learn\na prior distribution over entire functions. But fortunately,\nstochastic processes",
    "start": "900608",
    "end": "909420"
  },
  {
    "text": "are fully defined\non their marginals. So you really only need\nto characterize them where you use them, in some sense.",
    "start": "909420",
    "end": "915720"
  },
  {
    "text": "And moreover, you\ndon't actually need to learn the full underlying\nprior distribution over functions. So it turns out that\nof all the standard",
    "start": "915720",
    "end": "922560"
  },
  {
    "text": "approximate inference techniques\nfor Bayesian deep learning, all they really need is\nwhat's called a score. So the gradient of the log\ndensity of this stochastic",
    "start": "922560",
    "end": "930510"
  },
  {
    "text": "process prior on a set of\npoints where you evaluate them. So as long as you can get\nyour hands on that score,",
    "start": "930510",
    "end": "936360"
  },
  {
    "text": "you're in business. And that's the perspective\nthat we've been exploring. So essentially, what you want\nto do is to take a bag of tasks.",
    "start": "936360",
    "end": "945800"
  },
  {
    "text": "Maybe observations in\npast tuning sessions of this free electron laser to\nstay with this running example.",
    "start": "945800",
    "end": "951470"
  },
  {
    "text": "And then try to learn\nsomething about this underlying stochastic process prior.",
    "start": "951470",
    "end": "956860"
  },
  {
    "text": "And so we said, we\nreally need to control the marginals of this\nunderlying stochastic process.",
    "start": "956860",
    "end": "962077"
  },
  {
    "text": "So the one dimensional\nmarginals, of course, are just the function responses,\nsay, at a particular point x. So that's one example.",
    "start": "962077",
    "end": "967899"
  },
  {
    "text": "And, of course, we can look\nat high-dimensional marginals, like a two-dimensional\nexample of x point, basically the function\nresponse at points x1 and x2,",
    "start": "967900",
    "end": "975040"
  },
  {
    "text": "and how they're\njointly distributed. For Gaussian processes,\nall these marginals are joint Gaussian.",
    "start": "975040",
    "end": "980571"
  },
  {
    "text": "That's the\ncharacterizing property. But in general, of\ncourse, they need not be. And so the question\nis, how can we learn a flexible\nmodel that, in a way,",
    "start": "980572",
    "end": "988300"
  },
  {
    "text": "allows us to predict\nenough about the score where we want to use these\nmodels to make predictions?",
    "start": "988300",
    "end": "996370"
  },
  {
    "text": "And so to this end,\nwe use a very flexible neural architecture, basically\na transformer-based model, that directly seeks to predict\nthe score of where we use it",
    "start": "996370",
    "end": "1005430"
  },
  {
    "text": "for the stochastic\nprocess prior. And in order to\npredict the score, we basically use score matching.",
    "start": "1005430",
    "end": "1010620"
  },
  {
    "text": "So the key machinery used\nbehind diffusion models. And so that actually\nworks really quite well.",
    "start": "1010620",
    "end": "1017930"
  },
  {
    "text": "And actually, a\npaper last year, we do a fairly extensive empirical\nstudy across the whole range of different data sets,\nablations against a whole range",
    "start": "1017930",
    "end": "1025011"
  },
  {
    "text": "of different\nmeta-learning, and not meta-learning-based approaches. Some of them really designed\nfor Bayesian meta-learning and uncertainty\nquantification, some",
    "start": "1025012",
    "end": "1031270"
  },
  {
    "text": "for constructing point\nestimates and really stress-test them in\nthis regime, where you have very few\nexamples per task,",
    "start": "1031270",
    "end": "1036819"
  },
  {
    "text": "and possibly also only very few\ntasks, this really challenging domain, where you really\nneed to extract as many bits",
    "start": "1036819",
    "end": "1043540"
  },
  {
    "text": "as you can from\nrelated observations. So there's one data set. It's about this free electron\nlaser example that I mentioned.",
    "start": "1043540",
    "end": "1050590"
  },
  {
    "text": "So here, tasks correspond to\ndifferent tuning sessions. There's a medical time\nseries, data set we look at,",
    "start": "1050590",
    "end": "1056830"
  },
  {
    "text": "so each task is a patient\nmonitored in the intensive care unit. You want to learn a joint\nmodel that anyway explains",
    "start": "1056830",
    "end": "1063159"
  },
  {
    "text": "all the data, but personalize\nit with respect to each patient. There's a whole\nbunch of other data sets we look at in\nthis context as well.",
    "start": "1063160",
    "end": "1069560"
  },
  {
    "text": "And this model performs\nreally quite well in terms of the\nflexibility, both in terms",
    "start": "1069560",
    "end": "1074980"
  },
  {
    "text": "of getting high accuracy and\nalso getting low calibration error. Now, of course, lots of\ninteresting theoretical",
    "start": "1074980",
    "end": "1082320"
  },
  {
    "text": "questions of what\none can do about meta-learning\nprobabilistic models, in particular, for\nsequential decision-making.",
    "start": "1082320",
    "end": "1088230"
  },
  {
    "text": "They don't get IID data. In general, it's a\nvery hard problem. I just want to give a quick\nshout-out to my student,",
    "start": "1088230",
    "end": "1093270"
  },
  {
    "text": "Parnian, who's been looking\nat very good questions in more parametric regimes\nof this question.",
    "start": "1093270",
    "end": "1099910"
  },
  {
    "text": "So analyzing this algorithm\nend-to-end, I think, is quite challenging.",
    "start": "1099910",
    "end": "1105060"
  },
  {
    "text": "Now, one thing I\nwant to talk about is how does this interface with\ntasks where safety is important.",
    "start": "1105060",
    "end": "1111000"
  },
  {
    "text": "By the way, you really not just\nwant to get good prediction, but you really want\nto make sure that you produce conservative\nuncertainty estimates.",
    "start": "1111000",
    "end": "1119980"
  },
  {
    "text": "And so there, what\nyou've been looking at is basically encoding a\nlittle bit more structure",
    "start": "1119980",
    "end": "1125559"
  },
  {
    "text": "in terms of this hyper-prior. And so one way to do so is\nto use a Gaussian process",
    "start": "1125560",
    "end": "1130690"
  },
  {
    "text": "as sort of a\nhyper-prior and then basically shaping it through\nkey hyper-parameters.",
    "start": "1130690",
    "end": "1135830"
  },
  {
    "text": "So one key parameter\nis basically a scale parameter, the variance\nof the function responses.",
    "start": "1135830",
    "end": "1141250"
  },
  {
    "text": "Another key parameter, of\ncourse, is the length scale. And if you have\nlarge length scales,",
    "start": "1141250",
    "end": "1146710"
  },
  {
    "text": "single observations, in a way,\nreduce the uncertainty a lot, associated with getting\na lot of information.",
    "start": "1146710",
    "end": "1153759"
  },
  {
    "text": "But, of course, might be\nunsafe if you're, in a way, predicts too large\nreduction of uncertainty.",
    "start": "1153760",
    "end": "1161560"
  },
  {
    "text": "In contrast, small-length scales\nare much more conservative, but, of course, might lead\nto a more robust behavior.",
    "start": "1161560",
    "end": "1170590"
  },
  {
    "text": "And so you can look at\nthis, basically, phase diagram here in\nsome sense, where",
    "start": "1170590",
    "end": "1176590"
  },
  {
    "text": "you compare these\nhyper-parameters along length scales and the variance.",
    "start": "1176590",
    "end": "1181640"
  },
  {
    "text": "And in a way, if you\ndecrease length scale and increase the\nvariance parameter,",
    "start": "1181640",
    "end": "1187580"
  },
  {
    "text": "the model becomes\nmore conservative, but, of course,\ncorrespondingly slower.",
    "start": "1187580",
    "end": "1193040"
  },
  {
    "text": "If you increase the length\nscale but decrease the variance, you get unsafe behavior. And so the question\nis, of course, how",
    "start": "1193040",
    "end": "1199160"
  },
  {
    "text": "do you find the sweet spot? And so ideally, what\nyou would like to do is you'd like to find such\nparameter settings such",
    "start": "1199160",
    "end": "1205040"
  },
  {
    "text": "that, in a way, you\nensure constraint satisfaction during learning,\nwhile at the same time,",
    "start": "1205040",
    "end": "1211370"
  },
  {
    "text": "minimizing regret. So maximizing performance\nat the corresponding task that you would want to run. And, of course, that\nyou can't really",
    "start": "1211370",
    "end": "1217682"
  },
  {
    "text": "do with in the absence of\nactually collecting data and deploying the algorithm. And so what is shown\nin this CoRL paper",
    "start": "1217682",
    "end": "1224300"
  },
  {
    "text": "is that it's possible\nto find computationally efficient proxies\nfor these metrics",
    "start": "1224300",
    "end": "1229580"
  },
  {
    "text": "that you actually care about. In particular, what you can look\nat is on related tasks, what's",
    "start": "1229580",
    "end": "1234650"
  },
  {
    "text": "the calibration of the\nmodel so, in a way, the coverage of the\ncorresponding confidence",
    "start": "1234650",
    "end": "1241070"
  },
  {
    "text": "sets that you get\nout of this model. At the same time, you can\nalso look at, in a way, the efficiency of the prior,\nso the amount of reduction",
    "start": "1241070",
    "end": "1247250"
  },
  {
    "text": "in variance. And so these are very correlated\nacross these different tasks. And you can use\nthis idea in order",
    "start": "1247250",
    "end": "1253947"
  },
  {
    "text": "to, basically, do some sort\nof frontier search algorithm to, anyway, solve this\nconstraint optimization problem that you,\nin a way, maximize",
    "start": "1253947",
    "end": "1261680"
  },
  {
    "text": "the informativeness\nof the prior, while ensuring calibration\non past data sets.",
    "start": "1261680",
    "end": "1270160"
  },
  {
    "text": "And so that's, in a\nway, it's basically a very compact parameterization\nof this hyper-prior that",
    "start": "1270160",
    "end": "1276009"
  },
  {
    "text": "shapes your stochastic process\npredictions that you can then navigate efficiently\nexperimentally.",
    "start": "1276010",
    "end": "1281919"
  },
  {
    "text": "We've done a fairly extensive\nstudy including experiments on hardware, experiments\nwhere basically using",
    "start": "1281920",
    "end": "1289870"
  },
  {
    "text": "these meta-learning\nideas not only you get conservative predictions,\nwhich is what you would want,",
    "start": "1289870",
    "end": "1295270"
  },
  {
    "text": "but you get substantial\nacceleration in terms of performance. So if you don't use\nmeta-learning, in a way,",
    "start": "1295270",
    "end": "1301450"
  },
  {
    "text": "you take much, much\nlonger in order to find good parameter\nsettings, then, using this correspondingly\noptimized prior.",
    "start": "1301450",
    "end": "1310042"
  },
  {
    "text": "So this is some of\nthe work that we wanted to talk about\nmeta-learning, so reusing",
    "start": "1310042",
    "end": "1315310"
  },
  {
    "text": "information from related tasks. And now, the question is,\nhow can we make use of it",
    "start": "1315310",
    "end": "1321670"
  },
  {
    "text": "in tasks beyond\nBayesian optimization? What I want to talk\na little bit about",
    "start": "1321670",
    "end": "1326770"
  },
  {
    "text": "is how to lift ideas from\nBayesian optimization to learning-based control, in\nparticular, Bayesian model-based",
    "start": "1326770",
    "end": "1334310"
  },
  {
    "text": "reinforcement learning. And so Bayesian optimization\nis really a black-box method.",
    "start": "1334310",
    "end": "1340370"
  },
  {
    "text": "They have this black box. You put in a parameter. You measure something\nas an output. But instead of\ndynamical systems,",
    "start": "1340370",
    "end": "1346820"
  },
  {
    "text": "you really would want to\nmake use of observations of how the system\nactually works, state observations along the way.",
    "start": "1346820",
    "end": "1353039"
  },
  {
    "text": "And so one way to\nthink about this is to basically model an unknown\ndynamical system governed",
    "start": "1353040",
    "end": "1359220"
  },
  {
    "text": "by basically a Markov\ndecision transition kernel. And the illustrations\nhere, we'll actually talk about\ndeterministic systems,",
    "start": "1359220",
    "end": "1365520"
  },
  {
    "text": "but you can scale these ideas\nto stochastic systems as well. And what you're going to\ndo is you're basically",
    "start": "1365520",
    "end": "1371040"
  },
  {
    "text": "quantifying uncertainty\nin the dynamics of this dynamical system.",
    "start": "1371040",
    "end": "1378510"
  },
  {
    "text": "So these confidence sets\nthat we talked about before, they basically govern\nthe transition kernel,",
    "start": "1378510",
    "end": "1383910"
  },
  {
    "text": "our epistemic uncertainty about\nthe next state, as t plus 1, that you would get to carry\nif you carry out action",
    "start": "1383910",
    "end": "1390750"
  },
  {
    "text": "\"at\" in status. Now, I can think about this way. Suppose you want to start\nfrom some starting state,",
    "start": "1390750",
    "end": "1396655"
  },
  {
    "text": "maybe go to some goal state,\nwhile avoiding unsafe states. What you can do is\nyou can basically propagate the\nepistemic uncertainty",
    "start": "1396655",
    "end": "1403220"
  },
  {
    "text": "in this unknown dynamics model. But since you're uncertain\nabout where you end up,",
    "start": "1403220",
    "end": "1411603"
  },
  {
    "text": "you don't really know\nwhere you actually will up when you\ncarry out this policy. But, of course, once you deploy\nit, you collect information.",
    "start": "1411603",
    "end": "1419270"
  },
  {
    "text": "Your uncertainty is\ngoing to contract. And you can replan and\nhopefully be less conservative",
    "start": "1419270",
    "end": "1426350"
  },
  {
    "text": "and obtain improved behavior. And so what we can basically do\nis use these confidence sets,",
    "start": "1426350",
    "end": "1431659"
  },
  {
    "text": "both for reasoning\nabout exploration. So we might want to think about\nplanning an action sequence that",
    "start": "1431660",
    "end": "1437270"
  },
  {
    "text": "will plausibly allow you to\nachieve the task, while always",
    "start": "1437270",
    "end": "1442280"
  },
  {
    "text": "going to avoid\nvisiting unsafe states, with respect to the\nconfidence estimates afforded",
    "start": "1442280",
    "end": "1447860"
  },
  {
    "text": "by this probabilistic model. So now, the question is,\nhow to make this use, make this work with algorithms\nfor reinforcement learning",
    "start": "1447860",
    "end": "1456820"
  },
  {
    "text": "and scale them to complex tasks\nand complex control policies?",
    "start": "1456820",
    "end": "1462220"
  },
  {
    "text": "And so first, I'll just talk\nabout optimistic exploration. So how should we go about,\nin a way, using reinforcement",
    "start": "1462220",
    "end": "1469419"
  },
  {
    "text": "learning for\nintrospective planning but being very careful\nabout interactions with the real environment? So learn good\nbehavior very quickly.",
    "start": "1469420",
    "end": "1477980"
  },
  {
    "text": "And so the protocol\nwe're going to look at is basically this\nmodel-based RL paradigm",
    "start": "1477980",
    "end": "1483929"
  },
  {
    "text": "in the episodic setting. The heavy rounds, we\ncarry out an episode. We have a current\npolicy that we interact",
    "start": "1483930",
    "end": "1489720"
  },
  {
    "text": "with the real environments. We deploy it. We're going to collect\ndata, and we use this data in order to refine our\nestimates of the dynamics model.",
    "start": "1489720",
    "end": "1500040"
  },
  {
    "text": "And then we use the\nepistemic uncertainty around this transition model\nin order to replan our policy.",
    "start": "1500040",
    "end": "1505710"
  },
  {
    "text": "And for latter, that is a\npurely introspective task.",
    "start": "1505710",
    "end": "1510919"
  },
  {
    "text": "So we basically\nuse the estimates, including the epistemic\nestimates of the uncertainty,",
    "start": "1510920",
    "end": "1516470"
  },
  {
    "text": "in order to basically\nplan a new policy. And this is where we want to\nmake use of state-of-the-art",
    "start": "1516470",
    "end": "1523490"
  },
  {
    "text": "techniques in model-free\nreinforcement learning policy gradient techniques and so on. So in a way, we want to\nestimate a simulator, where",
    "start": "1523490",
    "end": "1532040"
  },
  {
    "text": "we can spend computation,\nwhatever you want, in order to find a\ngood policy, but then be very careful when deploying\nit on the real system.",
    "start": "1532040",
    "end": "1538280"
  },
  {
    "text": " So how would we do this?",
    "start": "1538280",
    "end": "1544090"
  },
  {
    "text": "So one way of how one can\nthink about doing this in model-based RL\nis to basically try to optimize our objective,\nand we discount it returns.",
    "start": "1544090",
    "end": "1551880"
  },
  {
    "text": "I understand that's approximate\ndynamic programming formulation. But the problem is we don't\nknow with the transition kernel.",
    "start": "1551880",
    "end": "1558059"
  },
  {
    "text": "So typically, what\nyou would assume, you can actually at least sample\nfrom the transition kernel. We don't know what the\ntransition model is.",
    "start": "1558060",
    "end": "1565610"
  },
  {
    "text": "So what one can do\nis one can consider an optimistic transition kernel.",
    "start": "1565610",
    "end": "1571220"
  },
  {
    "text": "There's a set of plausible\ntransition models, dynamics models. And try to optimize the policy\nthat under its most plausible",
    "start": "1571220",
    "end": "1578960"
  },
  {
    "text": "realization, you do well.  So now, the problem is, solving\nthis optimization problem",
    "start": "1578960",
    "end": "1587580"
  },
  {
    "text": "is really difficult, in general,\nif you have complex things like Gaussian\nprocess-based dynamics",
    "start": "1587580",
    "end": "1592950"
  },
  {
    "text": "models, neural-network-based\ndynamics models and so on. You have a very complex\nnon-convex constraint set",
    "start": "1592950",
    "end": "1599310"
  },
  {
    "text": "that you have to optimize over. And so as a consequence\nstate-of-the-art techniques that",
    "start": "1599310",
    "end": "1607049"
  },
  {
    "text": "use epistemic uncertainty about\nthe unknown transition kernel typically just\naverage out over it,",
    "start": "1607050",
    "end": "1613710"
  },
  {
    "text": "optimize over an\nexpectation, for example. So this is at the heart\nof some of the PILCO method as a classical\nmethod for model-based RL",
    "start": "1613710",
    "end": "1619649"
  },
  {
    "text": "with Gaussian processes, the\nPETS method using neural network ensembles and so on. That's basically just\naverage over your models,",
    "start": "1619650",
    "end": "1627270"
  },
  {
    "text": "basically, certainty equivalent\ncontrol in some sense. ",
    "start": "1627270",
    "end": "1632470"
  },
  {
    "text": "And so what we figured\nout is this a way of actually carrying out\noptimistic exploration.",
    "start": "1632470",
    "end": "1641590"
  },
  {
    "text": "So propagating the uncertainty\nin the dynamics model, but reducing this\nproblem to just a standard approximate dynamic\nprogramming problem to which one",
    "start": "1641590",
    "end": "1648940"
  },
  {
    "text": "can apply standard\nmodel-free policy gradient algorithms for\nintrospective planning.",
    "start": "1648940",
    "end": "1656710"
  },
  {
    "text": "And let me just explain\nhow that roughly works. So here's a simple\none-dimensional cartoon.",
    "start": "1656710",
    "end": "1663370"
  },
  {
    "text": "So suppose we start in our\nstarting state, maybe a 0, and we want to reach\na sparse reward state.",
    "start": "1663370",
    "end": "1670900"
  },
  {
    "text": "So the y-axis is the\none-dimensional state space in this cartoon. The x-axis is time.",
    "start": "1670900",
    "end": "1676793"
  },
  {
    "text": "And so now, for illustration\nagain, let's just look at deterministic dynamics. So the only uncertainty\nis of an epistemic nature",
    "start": "1676793",
    "end": "1683810"
  },
  {
    "text": "that we have lack of information\nabout the consequences of our actions. So it means that if we\nconsider a candidate policy,",
    "start": "1683810",
    "end": "1690799"
  },
  {
    "text": "pi, that is going to take us\nto some nominal next-state, predicted next-state.",
    "start": "1690800",
    "end": "1697409"
  },
  {
    "text": "Then we'll have some\nepistemic uncertainty of where we would really end up.",
    "start": "1697410",
    "end": "1703260"
  },
  {
    "text": "And so in this formulation,\nI talked about before. Now, you also have to do this\nfor all plausible dynamics",
    "start": "1703260",
    "end": "1709409"
  },
  {
    "text": "that you would unroll for it. So instead, what\nwe're going to do is we'll let the agent\ncontrol its luck.",
    "start": "1709410",
    "end": "1717380"
  },
  {
    "text": "So we'll let the\nagent choose where within this one-step confidence\nsets it wants to end up with.",
    "start": "1717380",
    "end": "1726400"
  },
  {
    "text": "So you can choose where within\nthese plausible next-states it will be. Now, this gives us an\noptimistic next-state,",
    "start": "1726400",
    "end": "1734650"
  },
  {
    "text": "and basically using\nthese uncertainty sets, where you essentially\ndescribe rectangular uncertainty",
    "start": "1734650",
    "end": "1740950"
  },
  {
    "text": "sets. You maybe have a nominal\nnext-state prediction and some sort of\ncoordinate-wise uncertainty",
    "start": "1740950",
    "end": "1747760"
  },
  {
    "text": "estimates for the next-state. You can basically\nreparameterize this choice as a new dynamics model,\nf tilde, that, in a way,",
    "start": "1747760",
    "end": "1755559"
  },
  {
    "text": "takes this control action\nthat lets the agent control its luck into account.",
    "start": "1755560",
    "end": "1761240"
  },
  {
    "text": "And so now, we can keep\nunrolling this further, get another\noptimistic next-state, unroll it further, get\nanother optimistic next-state,",
    "start": "1761240",
    "end": "1767660"
  },
  {
    "text": "and then finally reach\nthis high-reward state. And so what's nice\nabout this is, this is basically just a\nstandard approximate dynamic",
    "start": "1767660",
    "end": "1774680"
  },
  {
    "text": "programming problem to which\none can apply standard policy gradient techniques in order\nto solve it and basically",
    "start": "1774680",
    "end": "1782832"
  },
  {
    "text": "throw the hammer at it. That's the matrix. That's the learned\nsimulator, in a way, that you have to optimize. That, in a way, incorporates the\nepistemic uncertainty about it.",
    "start": "1782832",
    "end": "1792080"
  },
  {
    "text": "So here is a simple\ncartoon illustration on the inverted\npendulum task, as you",
    "start": "1792080",
    "end": "1797150"
  },
  {
    "text": "want to swing up the pendulum. So it start at the beginning. So the agent knows\nnothing about physics.",
    "start": "1797150",
    "end": "1804200"
  },
  {
    "text": "It only has some vague\nsmoothness assumptions about the dynamics. And so the first\noptimistic trajectory just",
    "start": "1804200",
    "end": "1809420"
  },
  {
    "text": "believes if I just push\nit in one direction, I can swing it up. And, of course, that's\nnot how the world works.",
    "start": "1809420",
    "end": "1815270"
  },
  {
    "text": "So if you actually\ncarry out that policy, the pendulum just\nswings down a little bit in the swung-down position.",
    "start": "1815270",
    "end": "1823930"
  },
  {
    "text": "Then, of course, you\nupdate your model. You quickly learn\nthat you actually have to swing back\nin one direction",
    "start": "1823930",
    "end": "1829288"
  },
  {
    "text": "in order to swing it up\non the other direction. That still doesn't\nquite work, but now you collect relevant data over a\nlarge part of the state space.",
    "start": "1829288",
    "end": "1837540"
  },
  {
    "text": "Condition on it. Now, just a little\nlater, basically you learned how to swing it up.",
    "start": "1837540",
    "end": "1843059"
  },
  {
    "text": "But it's slightly overshoots\nbecause you haven't collected any data on the top. And then just a little\nlater, it basically",
    "start": "1843060",
    "end": "1848935"
  },
  {
    "text": "has found an optimal policy,\nwith the optimistic estimate corresponds to the\nactual one observed.",
    "start": "1848935",
    "end": "1855540"
  },
  {
    "text": "So now, that's, of course,\na very simple task, but apply this to standard\ndeep RL benchmark tasks.",
    "start": "1855540",
    "end": "1862650"
  },
  {
    "text": "So here's comparisons against\nthis greedy, the certainty equivalent method,\njust averaging out over the uncertainty in\nthe transition model,",
    "start": "1862650",
    "end": "1869039"
  },
  {
    "text": "also comparing against\nThompson sampling. So posterior sampling,\nwhere you basically sample one of the models\nfrom the posterior.",
    "start": "1869040",
    "end": "1874770"
  },
  {
    "text": "And here's this\noptimistic exploration, which already in this setting\ndoes maybe a little bit better, does, of course, a lot of\nvariability when training",
    "start": "1874770",
    "end": "1881730"
  },
  {
    "text": "these reinforcement\nlearning agents. You can make the problem harder\nby increasing action penalties,",
    "start": "1881730",
    "end": "1887790"
  },
  {
    "text": "typically would want\nto encourage the agent to use as little\nforce as possible.",
    "start": "1887790",
    "end": "1893420"
  },
  {
    "text": "And so if you do that,\nexploration actually becomes harder. And why is that? Well, standard policy\ngradients, if you",
    "start": "1893420",
    "end": "1899360"
  },
  {
    "text": "penalize using large force, will\nessentially learn to do nothing. It will basically\nlearn to stand still.",
    "start": "1899360",
    "end": "1905830"
  },
  {
    "text": "But with the three\nparameterization, you learned that\nthere is actually a chance of getting further\nif you just stick with it.",
    "start": "1905830",
    "end": "1913627"
  },
  {
    "text": "And eventually, you really\nneed this optimistic in order to reach sparse reward states.",
    "start": "1913627",
    "end": "1918860"
  },
  {
    "text": "And here's has just a little\ncartoon illustration of this. So if you don't use\naction penalties and just use policy\ngradient techniques,",
    "start": "1918860",
    "end": "1925520"
  },
  {
    "text": "you just happen to learn how\nto move forward by vigorously waving with your extremities. So it's probably not\nexploration you would",
    "start": "1925520",
    "end": "1931610"
  },
  {
    "text": "want to do in a real robot. And so if you regularize\nby essentially controlling",
    "start": "1931610",
    "end": "1937730"
  },
  {
    "text": "these action penalties,\nthen the behaviors are going to be a little\nbit more constrained.",
    "start": "1937730",
    "end": "1943100"
  },
  {
    "text": "But, of course,\nexploration becomes harder. And you can do\nsimilar ideas once you have constraints as well.",
    "start": "1943100",
    "end": "1948600"
  },
  {
    "text": "So this was only about\nefficient exploration. You can also be a bit more\ncareful about reasoning, about constraint satisfaction.",
    "start": "1948600",
    "end": "1954110"
  },
  {
    "text": "So for example, one\ncan do the same idea in constraint MDPs, which is one\nway of not just having a reward,",
    "start": "1954110",
    "end": "1960680"
  },
  {
    "text": "but also having a cost that\nmaybe quantifies the probability of visiting unsafe states.",
    "start": "1960680",
    "end": "1967310"
  },
  {
    "text": "That's one formalism\nthat's been looked at in this context, where\nyou want to ensure constraint",
    "start": "1967310",
    "end": "1974510"
  },
  {
    "text": "satisfaction. And now, again, the trouble\nis that in standard constraint MDPs, you need to know what\nthe transition model is.",
    "start": "1974510",
    "end": "1980220"
  },
  {
    "text": "So you maybe need to be\nable to sample from it. But you can use that same idea. But now, you can be optimistic\nwith respect to exploration,",
    "start": "1980220",
    "end": "1988590"
  },
  {
    "text": "but be pessimistic with respect\nto constraint satisfaction. And use similar\nreparameterization ideas,",
    "start": "1988590",
    "end": "1996330"
  },
  {
    "text": "in a way, to turn this into a\nproblem to which one can apply the hammer standard\ndeep RL policy",
    "start": "1996330",
    "end": "2005269"
  },
  {
    "text": "gradient-based techniques for\nconstraint policy optimization. And so there are\nsome experiments",
    "start": "2005270",
    "end": "2010820"
  },
  {
    "text": "that we describe in the ICLR\npaper on learning to control-- I mean, this is relatively\nsimple robotics tasks in this",
    "start": "2010820",
    "end": "2018620"
  },
  {
    "text": "safety-gym benchmark suite-- a point mass, some simple\ncar model, a simple quadruped-- in\norder to solve tasks",
    "start": "2018620",
    "end": "2024770"
  },
  {
    "text": "like reaching a goal,\ntouching buttons, pushing a box to a goal, and the\nlikes from first-person view.",
    "start": "2024770",
    "end": "2031033"
  },
  {
    "text": "So that's a\nforward-facing camera. You try to learn\ndirectly from pixels. And now, basically, we\nuse a states-based model",
    "start": "2031033",
    "end": "2039810"
  },
  {
    "text": "to account for the\npartial observability and do uncertainty\nquantification around that.",
    "start": "2039810",
    "end": "2044980"
  },
  {
    "text": "But otherwise, the ideas\nare basically the same, that you can apply them to the\nconstraint setting as well.",
    "start": "2044980",
    "end": "2050649"
  },
  {
    "text": "And here's just\nexamples of the types of observations that\nthe agents get to see and the tasks that it's\ngoing to carry out.",
    "start": "2050650",
    "end": "2056746"
  },
  {
    "text": "And so there's a\nbunch of experiments, where we compare this algorithm\nis optimistic, pessimistic",
    "start": "2056747",
    "end": "2062710"
  },
  {
    "text": "algorithm, with a bunch\nof other model-based and model-free\ndeep RL algorithms,",
    "start": "2062710",
    "end": "2068110"
  },
  {
    "text": "both along the final objective\nachieved at the task, how",
    "start": "2068110",
    "end": "2074770"
  },
  {
    "text": "the fraction of cases where\nyou achieve constraint satisfaction at the end of\nthe task, where you satisfy,",
    "start": "2074770",
    "end": "2080830"
  },
  {
    "text": "really find a safe\npolicy in the end, as well as safety\nduring training. This is a very difficult task.",
    "start": "2080830",
    "end": "2087369"
  },
  {
    "text": "You really learn from scratch\nby observation only from pixels. So essentially,\nsafe exploration is",
    "start": "2087370",
    "end": "2092560"
  },
  {
    "text": "very difficult in that\nsetting, but at least you still want to learn\nas carefully as you can.",
    "start": "2092560",
    "end": "2098500"
  },
  {
    "text": "And so basically,\nacross this comparison, this approach leads to\ngood task completion,",
    "start": "2098500",
    "end": "2107390"
  },
  {
    "text": "is the only one that's actually\nable to safely complete all these tasks, and also does\npretty well, even not perfect",
    "start": "2107390",
    "end": "2114380"
  },
  {
    "text": "in terms of the safe\nexploration objective. There's also comparisons\nagainst model-free techniques",
    "start": "2114380",
    "end": "2120350"
  },
  {
    "text": "that are given a lot more\nenvironment interaction, so orders of magnitude more\nenvironment interactions.",
    "start": "2120350",
    "end": "2126049"
  },
  {
    "text": "And you find similar\nperformance in the end. But, of course, much\nmore efficiently in terms of much\nfewer interactions",
    "start": "2126050",
    "end": "2132349"
  },
  {
    "text": "with the actual environment. So that brings me to the\nlast part of the talk",
    "start": "2132350",
    "end": "2140520"
  },
  {
    "text": "that I want to do in\norder to combine some of the ideas we've\nbeen discussing",
    "start": "2140520",
    "end": "2145560"
  },
  {
    "text": "throughout this presentation. And so here's a\nsimple illustration",
    "start": "2145560",
    "end": "2151020"
  },
  {
    "text": "on the task of\ntaking this race car. And what you want to\ndo is you basically want to reverse-park it.",
    "start": "2151020",
    "end": "2157050"
  },
  {
    "text": "It's not a particularly complex\ntask, but it's a nice example. It's non-trivial in the\nsense that there is slippage,",
    "start": "2157050",
    "end": "2165839"
  },
  {
    "text": "and there's properties\nof the tires and so on. If you want to really carefully\nmodel this, it's challenging.",
    "start": "2165840",
    "end": "2174329"
  },
  {
    "text": "And so we can first just run\nthis basic model-based RL algorithm that I've\ndescribed to you before.",
    "start": "2174330",
    "end": "2180420"
  },
  {
    "text": "And so after a couple episodes-- so initially, just\nlearns, collects data.",
    "start": "2180420",
    "end": "2187890"
  },
  {
    "text": "Again, it uses a completely\nuninformed dynamics model, so knows nothing about physics. It knows nothing\nabout the environment.",
    "start": "2187890",
    "end": "2193539"
  },
  {
    "text": "But after about\n20 episodes or so, it roughly is able\nto learn the task. So certainly,\nmodel-free RL algorithms",
    "start": "2193540",
    "end": "2201849"
  },
  {
    "text": "will use many orders of\nmagnitude more samples in order to get there. So it's relatively\nsimple efficient",
    "start": "2201850",
    "end": "2209070"
  },
  {
    "text": "still to learn from\ncompletely from scratch. Now, of course, maybe a\nmuch more standard way",
    "start": "2209070",
    "end": "2214290"
  },
  {
    "text": "of trying to solve that problem\nfrom controls engineering would be to maybe derive a first\nprinciple physics-based model.",
    "start": "2214290",
    "end": "2221490"
  },
  {
    "text": "Do sys id to estimate\nthe parameters. Then derive a\nfeedback controller and deploy it on the system.",
    "start": "2221490",
    "end": "2227130"
  },
  {
    "text": "That's maybe a much\nmore canonical strawman that one should compare with. And so now, of course, if\nyou're careful with modeling,",
    "start": "2227130",
    "end": "2235230"
  },
  {
    "text": "this works perfect. If you use a very simple\nmodel-- so here, is basically, we just use a bicycle model.",
    "start": "2235230",
    "end": "2241437"
  },
  {
    "text": "Then there is, of course,\na sim-to-real gap. So you won't exactly\nsolve the task. You'll get pretty close, but\nyou won't exactly solve it.",
    "start": "2241437",
    "end": "2247530"
  },
  {
    "text": "Of course, you could\nspend more work to model tires, model\nslippage and so on right.",
    "start": "2247530",
    "end": "2253500"
  },
  {
    "text": "But instead, we want\nto be more data-driven. And so the question we\nasked is, how can we get around the\nsim-to-real gap in a more",
    "start": "2253500",
    "end": "2260440"
  },
  {
    "text": "data-efficient fashion? And so what we've been\nexploring is basically, through this lens of\nBayesian meta-learning.",
    "start": "2260440",
    "end": "2269050"
  },
  {
    "text": "But instead of starting\nwith a data set that's collected from a bunch\nof related tasks,",
    "start": "2269050",
    "end": "2275590"
  },
  {
    "text": "we want to replace\nit with a simulator. So let's just use a very simple\nfirst-principle simulator,",
    "start": "2275590",
    "end": "2282279"
  },
  {
    "text": "just the bicycle model with a\nbunch of different parameters. And in a way, describe the\nunderlying stochastic process,",
    "start": "2282280",
    "end": "2288970"
  },
  {
    "text": "the prior assumptions\nof the dynamics model, through, in a way,\nprior distribution",
    "start": "2288970",
    "end": "2294700"
  },
  {
    "text": "over these different parameters\nthat govern the simulator.",
    "start": "2294700",
    "end": "2299900"
  },
  {
    "text": "And so now, what you want\nto do is you basically want to learn a model in a\ncompletely data-driven fashion",
    "start": "2299900",
    "end": "2309660"
  },
  {
    "text": "however bias it and\nregularize it towards behaving",
    "start": "2309660",
    "end": "2314940"
  },
  {
    "text": "like the simulator does. That's the idea. We want to distill the simulator\ninto a neural network prior.",
    "start": "2314940",
    "end": "2323430"
  },
  {
    "text": "That's the idea behind it. And so what you're\nbasically going to do is use the same idea that we've\ndiscussed before, to essentially",
    "start": "2323430",
    "end": "2330457"
  },
  {
    "text": "estimate the score of this\nunderlying stochastic process, but now, purely\nbased on simulations.",
    "start": "2330457",
    "end": "2336450"
  },
  {
    "text": "And if you do this, then\nalready in the first episode, you at least get off\nclose to the target. ",
    "start": "2336450",
    "end": "2345520"
  },
  {
    "text": "And then after-- he\nstill gets at least going in the right direction. And after about 10 episodes\nnow, it has solved the task.",
    "start": "2345520",
    "end": "2353082"
  },
  {
    "text": "It refines it a little bit\nfurther during the next episodes and so on. So it already starts off much\nbetter, much more informed",
    "start": "2353082",
    "end": "2359710"
  },
  {
    "text": "by the underlying physics,\neven though the model is just the neural network ensemble. It's basically just a\nfully integrated model.",
    "start": "2359710",
    "end": "2366070"
  },
  {
    "text": "All we do is we regularize it\na little bit towards behaving like what the simulator does. ",
    "start": "2366070",
    "end": "2373443"
  },
  {
    "text": "So here's some\nquantitative comparisons. So what this plot shows\nis negative log likelihood",
    "start": "2373443",
    "end": "2379070"
  },
  {
    "text": "on a holdout data set\ncollected on hardware to predict\nnext-states, basically,",
    "start": "2379070",
    "end": "2387170"
  },
  {
    "text": "from current status and actions. We compare on one\ninsight an approach that basically just does\nsystem identification",
    "start": "2387170",
    "end": "2393380"
  },
  {
    "text": "to just fit the model\nto data on the simulator and then evaluating\nthat predictive model",
    "start": "2393380",
    "end": "2399259"
  },
  {
    "text": "on the holdout data. Here's the uninformed\nneural network model",
    "start": "2399260",
    "end": "2407330"
  },
  {
    "text": "that I had discussed before,\nwhere initially, of course, you start off much worse,\nbut eventually, you",
    "start": "2407330",
    "end": "2413450"
  },
  {
    "text": "find better solutions. Another natural\nthing one can do is to do a gray-box model,\nwhere you basically",
    "start": "2413450",
    "end": "2420529"
  },
  {
    "text": "estimate a physics-informed,\nmaybe mean, and then learn some dynamics, some deviations\nfrom it using neural network.",
    "start": "2420530",
    "end": "2428470"
  },
  {
    "text": "And that already starts off at\na better place, but in the event it converges to\nthe same solution.",
    "start": "2428470",
    "end": "2433960"
  },
  {
    "text": "And here's this approach that's\nfully data-driven and just regularized towards the\nneural network prior.",
    "start": "2433960",
    "end": "2440270"
  },
  {
    "text": "And not only does it start\noff at a better spot, but it also converges to a\nbetter solution in the end. And here's the\nexperiment of that,",
    "start": "2440270",
    "end": "2446380"
  },
  {
    "text": "is a quantitative version of\nthe videos I showed before. The x-axis is the number\nof episodes to carry out, and the y-axis is basically\nthe achieved performance",
    "start": "2446380",
    "end": "2453490"
  },
  {
    "text": "at the task. And the green one is\nbasically this model-based RL",
    "start": "2453490",
    "end": "2459849"
  },
  {
    "text": "from scratch, with an\nuninformed prior knowing nothing about physics. And the blue one is\nbasically that regularized",
    "start": "2459850",
    "end": "2465880"
  },
  {
    "text": "towards this bicycle model. It's this very simple\nfirst-principle model. And again, it starts\noff with a better spot",
    "start": "2465880",
    "end": "2471790"
  },
  {
    "text": "and more quickly\nfinds a good solution. So that's basically what\nI wanted to talk about.",
    "start": "2471790",
    "end": "2480540"
  },
  {
    "text": "So I think there's a lot\nof exciting open challenges still to be solved, then\ntrying to develop agents",
    "start": "2480540",
    "end": "2489480"
  },
  {
    "text": "that can really safely and\nefficiently learn by interacting with the real world.",
    "start": "2489480",
    "end": "2494550"
  },
  {
    "text": "And I think one of the key\nchallenges in this domain is really enabling models to\nlearn to know what they're not,",
    "start": "2494550",
    "end": "2500670"
  },
  {
    "text": "what they don't know. I think that's also one\nof the big frontiers currently in foundation models.",
    "start": "2500670",
    "end": "2506190"
  },
  {
    "text": "How can one, in\na way, figure out the boundary of where the\nlatent representations still",
    "start": "2506190",
    "end": "2512430"
  },
  {
    "text": "are meaningful and\nwhere they are not? And I think that's a key\nchallenge to work on. And so one thing I just\nwant to briefly mention.",
    "start": "2512430",
    "end": "2519990"
  },
  {
    "text": "So in Switzerland, our\nnext-generation supercomputer in Lugano is coming\nonline in the near future,",
    "start": "2519990",
    "end": "2527520"
  },
  {
    "text": "with the sizable number\nof top-line GPUs. And so we are hoping\nto get a chance",
    "start": "2527520",
    "end": "2533130"
  },
  {
    "text": "to try out some of these ideas\nand scale them to larger scale settings in this context.",
    "start": "2533130",
    "end": "2539040"
  },
  {
    "text": "So here's just some of\nthe references in case you want to read up more. I guess it's being recorded.",
    "start": "2539040",
    "end": "2544260"
  },
  {
    "text": "And I'd like to end by thanking\nall the collaborators who",
    "start": "2544260",
    "end": "2549750"
  },
  {
    "text": "got a chance to work\nwith in this research, thank the funding agencies. And thank all of you\nfor your attention.",
    "start": "2549750",
    "end": "2554760"
  },
  {
    "text": "Thank you very much. [APPLAUSE] ",
    "start": "2554760",
    "end": "2566082"
  },
  {
    "text": "Thank you for the talk. I'm curious what\nyour thoughts are on maybe an alternative\napproach in imitation learning.",
    "start": "2566082",
    "end": "2571450"
  },
  {
    "text": "Would you argue that we\nlose the safety guarantees that you're presenting here? Or I'm just curious.",
    "start": "2571450",
    "end": "2576550"
  },
  {
    "text": "Your short take. ",
    "start": "2576550",
    "end": "2587640"
  },
  {
    "text": "One potential\nfundamental limitation in imitation\nlearning, of course, you can only do as well\nas what you get to see.",
    "start": "2587640",
    "end": "2595200"
  },
  {
    "text": "Now, of course, there's\nvarious different ways of starting with\nimitating something",
    "start": "2595200",
    "end": "2601590"
  },
  {
    "text": "and then trying to\nimprove upon that, where there's various\ndifferent ways of doing so. And I think that's probably\nexactly the way to go.",
    "start": "2601590",
    "end": "2607950"
  },
  {
    "text": "And in some sense, yeah,\nI think there's lots of ways of interfacing this.",
    "start": "2607950",
    "end": "2613320"
  },
  {
    "text": "So for example, one can-- using imitation learning, try\nto find an informed latent space",
    "start": "2613320",
    "end": "2618720"
  },
  {
    "text": "and then apply all the methods\non that informed latent space, as opposed to maybe\nin a generic state space.",
    "start": "2618720",
    "end": "2627330"
  },
  {
    "text": "So that's one potential\nway to go there. I think there's many\nother approaches",
    "start": "2627330",
    "end": "2633090"
  },
  {
    "text": "to interface that I think\nit's quite complementary. Yeah. Please.",
    "start": "2633090",
    "end": "2639400"
  },
  {
    "text": "The situation in which you\nwere waiting the type learning from the simulator. Is it possible to take it\nto a more granular level",
    "start": "2639400",
    "end": "2646510"
  },
  {
    "text": "like the action space, like\nin the case of the pendulum? One action.",
    "start": "2646510",
    "end": "2652000"
  },
  {
    "text": "And the case of like\nthe multi-wheeled robot. Each of those motors would\nrepresent a separate action.",
    "start": "2652000",
    "end": "2658359"
  },
  {
    "text": "So is it possible to maybe\nlearn the uncertainty for each of those individual, like\nactions that you could take,",
    "start": "2658360",
    "end": "2666369"
  },
  {
    "text": "and then possibly weight them\nseparately rather than weighting the entire learning\nfrom the simulator?",
    "start": "2666370",
    "end": "2673090"
  },
  {
    "text": "Has a separate-- That's a good question. ",
    "start": "2673090",
    "end": "2678819"
  },
  {
    "text": "Of course, if you\nhave multiple actions. So of course, in\nprinciple, you can just feed them into\nsome black box that predicts what's going to happen\nand apply the approach as is.",
    "start": "2678820",
    "end": "2687079"
  },
  {
    "text": "Question is maybe, is there\nsome more fine granular way to go about this?",
    "start": "2687080",
    "end": "2692290"
  },
  {
    "text": "And so we're going a little\nbit in this direction. So one thing to think\nabout this, in a way, is through the lens\nof causal discovery,",
    "start": "2692290",
    "end": "2698990"
  },
  {
    "text": "where you really try to\nthink about causal effects of individual actions\nthat you might apply",
    "start": "2698990",
    "end": "2704930"
  },
  {
    "text": "and how they might, in\na localized fashion, affect the state space. | we've been doing some work\nin model-based causal Bayesian",
    "start": "2704930",
    "end": "2711350"
  },
  {
    "text": "optimization that goes\nin this direction. I think there's a lot\nmore to do in this space. Thanks a lot for the question.",
    "start": "2711350",
    "end": "2716690"
  },
  {
    "text": " Any other questions?",
    "start": "2716690",
    "end": "2723090"
  },
  {
    "text": "Then let's go for pizza. OK, yeah. Then, thank you very much again. [APPLAUSE]",
    "start": "2723090",
    "end": "2729710"
  },
  {
    "start": "2729710",
    "end": "2732000"
  }
]