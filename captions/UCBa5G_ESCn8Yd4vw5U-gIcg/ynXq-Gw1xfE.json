[
  {
    "start": "0",
    "end": "1110"
  },
  {
    "text": "So as I mentioned\nearlier, we're talking",
    "start": "1110",
    "end": "4680"
  },
  {
    "text": "about best subset selection\nhere for least squares.",
    "start": "4680",
    "end": "7210"
  },
  {
    "text": "But we can just as well do\nthis for logistic regression",
    "start": "7210",
    "end": "9750"
  },
  {
    "text": "or for any other type of model.",
    "start": "9750",
    "end": "12750"
  },
  {
    "text": "And in particular,\nwhen we're talking",
    "start": "12750",
    "end": "14309"
  },
  {
    "text": "about other types of\nmodels, we don't usually",
    "start": "14310",
    "end": "16350"
  },
  {
    "text": "talk about residual\nsum of squares,",
    "start": "16350",
    "end": "17970"
  },
  {
    "text": "instead, we talk\nabout something that's",
    "start": "17970",
    "end": "19595"
  },
  {
    "text": "called the deviance, which\nis negative 2 times the log",
    "start": "19595",
    "end": "22410"
  },
  {
    "text": "likelihood.",
    "start": "22410",
    "end": "23520"
  },
  {
    "text": "And in the case\nof least squares,",
    "start": "23520",
    "end": "28110"
  },
  {
    "text": "the deviance and the residual\nsum of squares are equivalent.",
    "start": "28110",
    "end": "31630"
  },
  {
    "text": "But for other model\ntypes, the deviance",
    "start": "31630",
    "end": "33750"
  },
  {
    "text": "is really just a generalization\nof residual sum of squares.",
    "start": "33750",
    "end": "36493"
  },
  {
    "text": "But here we're going to talk\nabout residual sum of squares",
    "start": "36493",
    "end": "38910"
  },
  {
    "text": "for simplicity.",
    "start": "38910",
    "end": "39970"
  },
  {
    "start": "39970",
    "end": "44280"
  },
  {
    "text": "So in that figure we were\nlooking at a couple of slides",
    "start": "44280",
    "end": "47760"
  },
  {
    "text": "ago, with the credit\ndata, we saw that there",
    "start": "47760",
    "end": "49769"
  },
  {
    "text": "were all of those gray dots.",
    "start": "49770",
    "end": "51078"
  },
  {
    "text": "And I mentioned that\nthere's actually",
    "start": "51078",
    "end": "52620"
  },
  {
    "text": "2 to the 10 gray\ndots in that picture.",
    "start": "52620",
    "end": "56910"
  },
  {
    "text": "And the idea is that if I\nhave access to p predictors,",
    "start": "56910",
    "end": "60690"
  },
  {
    "text": "and I want to consider\nevery possible submodel,",
    "start": "60690",
    "end": "63149"
  },
  {
    "text": "I'm actually looking\nat 2 to the p subsets.",
    "start": "63150",
    "end": "67750"
  },
  {
    "text": "And that is an\nabsolutely huge number.",
    "start": "67750",
    "end": "70130"
  },
  {
    "text": "So just to put this in\nperspective, 2 to the ten,",
    "start": "70130",
    "end": "74140"
  },
  {
    "text": "that's 1,000.",
    "start": "74140",
    "end": "76150"
  },
  {
    "text": "So that's OK, fitting 1,000\nmodels if you've got a decent",
    "start": "76150",
    "end": "79540"
  },
  {
    "text": "computer is no problem.",
    "start": "79540",
    "end": "81550"
  },
  {
    "text": "But 2 to the 40 is\na really big number.",
    "start": "81550",
    "end": "90470"
  },
  {
    "text": "And so the idea is\nthat when p is large,",
    "start": "90470",
    "end": "95627"
  },
  {
    "text": "we're really not going\nto be able to do best",
    "start": "95627",
    "end": "97460"
  },
  {
    "text": "subset selection.",
    "start": "97460",
    "end": "98479"
  },
  {
    "text": "And 40 predictors isn't\nlarge, I work on data",
    "start": "98480",
    "end": "101780"
  },
  {
    "text": "where there's easily\ntens or hundreds",
    "start": "101780",
    "end": "103580"
  },
  {
    "text": "of thousands of predictors.",
    "start": "103580",
    "end": "104850"
  },
  {
    "text": "And so best sub\nselection just does not",
    "start": "104850",
    "end": "106670"
  },
  {
    "text": "scale for many of\nthe types of problems",
    "start": "106670",
    "end": "108710"
  },
  {
    "text": "that people are\ninterested in today.",
    "start": "108710",
    "end": "110960"
  },
  {
    "text": "I think the limit\nfor most packages",
    "start": "110960",
    "end": "112729"
  },
  {
    "text": "that do subset selection\nis about 30 or 40.",
    "start": "112730",
    "end": "115230"
  },
  {
    "text": "And beyond that,\nthey just say, I",
    "start": "115230",
    "end": "116990"
  },
  {
    "text": "can't look at all\nthe combinations.",
    "start": "116990",
    "end": "117970"
  },
  {
    "text": "Right.",
    "start": "117970",
    "end": "118470"
  },
  {
    "text": "So putting aside\ncomputational concerns,",
    "start": "118470",
    "end": "120980"
  },
  {
    "text": "though, best subset\nselection also",
    "start": "120980",
    "end": "122990"
  },
  {
    "text": "suffers from\nstatistical problems.",
    "start": "122990",
    "end": "124740"
  },
  {
    "text": "And the idea is, if I'm\nconsidering, gosh, what is this,",
    "start": "124740",
    "end": "128000"
  },
  {
    "text": "this is a trillion models,\nif I have 40 predictors",
    "start": "128000",
    "end": "130100"
  },
  {
    "text": "and I'm considering\na trillion models,",
    "start": "130100",
    "end": "133670"
  },
  {
    "text": "I'm going to overfit the data.",
    "start": "133670",
    "end": "135209"
  },
  {
    "text": "I'm just looking\nat so many models,",
    "start": "135210",
    "end": "136883"
  },
  {
    "text": "I'm going to choose a model that\nlooks really great on this data",
    "start": "136883",
    "end": "139550"
  },
  {
    "text": "by chance, but it's\nnot going to look great",
    "start": "139550",
    "end": "141513"
  },
  {
    "text": "on an independent\ntest set that I didn't",
    "start": "141513",
    "end": "143180"
  },
  {
    "text": "use in training the model.",
    "start": "143180",
    "end": "145140"
  },
  {
    "text": "And so I'm going to have a\nproblem with overfitting.",
    "start": "145140",
    "end": "148100"
  },
  {
    "text": "And so for these two reasons,\ncomputational and statistical,",
    "start": "148100",
    "end": "151790"
  },
  {
    "text": "best subset selection\nisn't really great",
    "start": "151790",
    "end": "153799"
  },
  {
    "text": "unless p is extremely small.",
    "start": "153800",
    "end": "155690"
  },
  {
    "text": "And in this setting, we can\nuse stepwise methods, which",
    "start": "155690",
    "end": "159290"
  },
  {
    "text": "are the same idea as\nbest subset selection,",
    "start": "159290",
    "end": "161569"
  },
  {
    "text": "but they look at a more\nrestricted set of models.",
    "start": "161570",
    "end": "163860"
  },
  {
    "text": "So instead of looking\nat 2 to the p models,",
    "start": "163860",
    "end": "166220"
  },
  {
    "text": "they look at more like P squared\nmodels and of course 2p is much",
    "start": "166220",
    "end": "170240"
  },
  {
    "text": "more than p squared for any\np of any reasonable size.",
    "start": "170240",
    "end": "174390"
  },
  {
    "text": "So at this point\nthat Danielle made",
    "start": "174390",
    "end": "175850"
  },
  {
    "text": "is actually pretty\nsomewhat counterintuitive.",
    "start": "175850",
    "end": "177660"
  },
  {
    "text": "Especially people\nin computer science",
    "start": "177660",
    "end": "179202"
  },
  {
    "text": "might think it's always best to\ndo the most exact computation",
    "start": "179202",
    "end": "182060"
  },
  {
    "text": "you can.",
    "start": "182060",
    "end": "182580"
  },
  {
    "text": "If you can do the\noptimization over all models,",
    "start": "182580",
    "end": "184880"
  },
  {
    "text": "it's usually felt to be better.",
    "start": "184880",
    "end": "186300"
  },
  {
    "text": "But Danielle says something\nthat's actually quite somewhat",
    "start": "186300",
    "end": "188960"
  },
  {
    "text": "counterintuitive, that actually\nto avoid fitting too hard,",
    "start": "188960",
    "end": "192530"
  },
  {
    "text": "it's good to actually not\nlook at all possible models,",
    "start": "192530",
    "end": "194780"
  },
  {
    "text": "but only a subset of the models.",
    "start": "194780",
    "end": "196440"
  },
  {
    "text": "So forward stepwise regression,\nwhich we'll look at stepwise",
    "start": "196440",
    "end": "199250"
  },
  {
    "text": "and forward stepwise\nin particular,",
    "start": "199250",
    "end": "200960"
  },
  {
    "text": "doesn't try to look at\nall possible models,",
    "start": "200960",
    "end": "202940"
  },
  {
    "text": "despite the fact that, in\ncases when p is less than 40,",
    "start": "202940",
    "end": "208140"
  },
  {
    "text": "so you can look at\nall possible models,",
    "start": "208140",
    "end": "210000"
  },
  {
    "text": "it may not be the\nbest thing to do that.",
    "start": "210000",
    "end": "212080"
  },
  {
    "text": "So forward stepwise\non purpose looks only",
    "start": "212080",
    "end": "215890"
  },
  {
    "text": "at a subset of the models,\nand that can actually",
    "start": "215890",
    "end": "217890"
  },
  {
    "text": "be helpful to [INAUDIBLE].",
    "start": "217890",
    "end": "218972"
  },
  {
    "text": "So, Rob best subset\nselection, what",
    "start": "218973",
    "end": "222583"
  },
  {
    "text": "would be your limit where if\np is larger than that number,",
    "start": "222583",
    "end": "225000"
  },
  {
    "text": "you wouldn't want to\ngo beyond best subset.",
    "start": "225000",
    "end": "227860"
  },
  {
    "text": "I wouldn't want to use\nthe best subset, maybe not",
    "start": "227860",
    "end": "230110"
  },
  {
    "text": "beyond 10 or 20.",
    "start": "230110",
    "end": "232210"
  },
  {
    "text": "I don't think I would\nprobably even use it for 20.",
    "start": "232210",
    "end": "234490"
  },
  {
    "text": "I think I would use\nbest subset if I've",
    "start": "234490",
    "end": "236170"
  },
  {
    "text": "got a handful of predictors.",
    "start": "236170",
    "end": "237365"
  },
  {
    "text": "And if I've got 10,\nI wouldn't be using",
    "start": "237365",
    "end": "238990"
  },
  {
    "text": "that anymore, most likely.",
    "start": "238990",
    "end": "240160"
  },
  {
    "text": "So the point is,\nit's not always best",
    "start": "240160",
    "end": "241360"
  },
  {
    "text": "to do a full search,\neven when you can do it,",
    "start": "241360",
    "end": "243070"
  },
  {
    "text": "because you pay a\nprice in variance.",
    "start": "243070",
    "end": "244605"
  },
  {
    "start": "244605",
    "end": "247819"
  },
  {
    "text": "So now we're going to talk about\nthese two stepwise methods,",
    "start": "247820",
    "end": "251360"
  },
  {
    "text": "forward and backward, stepwise.",
    "start": "251360",
    "end": "252870"
  },
  {
    "text": "And these are really\npretty similar,",
    "start": "252870",
    "end": "255667"
  },
  {
    "text": "but there's just one\nfundamental difference, which",
    "start": "255667",
    "end": "257750"
  },
  {
    "text": "is whether you're starting with\na model with no predictors,",
    "start": "257750",
    "end": "260450"
  },
  {
    "text": "or you're starting with a\nmodel with all the predictors.",
    "start": "260450",
    "end": "263210"
  },
  {
    "text": "So in forward\nstepwise selection,",
    "start": "263210",
    "end": "265190"
  },
  {
    "text": "we start with a model that\ncontains no predictors.",
    "start": "265190",
    "end": "267570"
  },
  {
    "text": "This is just a model\nwith only the intercept,",
    "start": "267570",
    "end": "269520"
  },
  {
    "text": "it's what we were calling the\nnull model M zero earlier.",
    "start": "269520",
    "end": "273056"
  },
  {
    "text": "And then we're going\nto add predictors",
    "start": "273057",
    "end": "274640"
  },
  {
    "text": "to the model, one at a time,\nuntil all of the predictors",
    "start": "274640",
    "end": "277940"
  },
  {
    "text": "are in the model.",
    "start": "277940",
    "end": "279290"
  },
  {
    "text": "So this sounds like\nbest subset selection,",
    "start": "279290",
    "end": "281480"
  },
  {
    "text": "but there's actually a really\nmajor difference, which",
    "start": "281480",
    "end": "284030"
  },
  {
    "text": "is that at each step,\nwe're not looking",
    "start": "284030",
    "end": "285920"
  },
  {
    "text": "at every single possible model\nin the universe that contains",
    "start": "285920",
    "end": "289580"
  },
  {
    "text": "K predictors, we're just\nlooking at the models that",
    "start": "289580",
    "end": "292939"
  },
  {
    "text": "contain the K minus 1\npredictors that we already",
    "start": "292940",
    "end": "295580"
  },
  {
    "text": "chose in the previous\nstep, plus one more.",
    "start": "295580",
    "end": "298189"
  },
  {
    "text": "So at the K step, we're looking\nat a much more restricted set",
    "start": "298190",
    "end": "302510"
  },
  {
    "text": "of models than we are in\nbest subset selection.",
    "start": "302510",
    "end": "306140"
  },
  {
    "text": "In particular, at\neach step, we're",
    "start": "306140",
    "end": "307590"
  },
  {
    "text": "just going to choose the\nvariable that gives the biggest",
    "start": "307590",
    "end": "310440"
  },
  {
    "text": "improvement to the model we\njust had a moment earlier.",
    "start": "310440",
    "end": "314970"
  },
  {
    "text": "So the idea behind forward\nstepwise selection,",
    "start": "314970",
    "end": "317520"
  },
  {
    "text": "is that once again, we start\nwith this model M zero,",
    "start": "317520",
    "end": "320110"
  },
  {
    "text": "it's the null model, and it\njust contains an intercept.",
    "start": "320110",
    "end": "322900"
  },
  {
    "text": "so you're just going to predict\nthe mean for every observation.",
    "start": "322900",
    "end": "327250"
  },
  {
    "text": "And then we're going to\ntry to create a model, M1.",
    "start": "327250",
    "end": "329950"
  },
  {
    "text": "And the model M one is going to\njust contain one more predictor",
    "start": "329950",
    "end": "333400"
  },
  {
    "text": "than M0.",
    "start": "333400",
    "end": "334449"
  },
  {
    "text": "So we're just going\nto take M0, and we're",
    "start": "334450",
    "end": "336490"
  },
  {
    "text": "going to look for\nthe best predictor",
    "start": "336490",
    "end": "338229"
  },
  {
    "text": "to add that's going to lead to\nthe smallest RSS or the largest",
    "start": "338230",
    "end": "341290"
  },
  {
    "text": "R squared.",
    "start": "341290",
    "end": "342810"
  },
  {
    "text": "So that gives us M1.",
    "start": "342810",
    "end": "344700"
  },
  {
    "text": "And now in order to get\nM2, we're going to take M1,",
    "start": "344700",
    "end": "348480"
  },
  {
    "text": "and we're going\nto consider adding",
    "start": "348480",
    "end": "353460"
  },
  {
    "text": "all p minus 1 possible\npredictors to M1,",
    "start": "353460",
    "end": "356130"
  },
  {
    "text": "and we're going to see which\nof those P minus 1 predictors",
    "start": "356130",
    "end": "358620"
  },
  {
    "text": "gives us the best model M2.",
    "start": "358620",
    "end": "361229"
  },
  {
    "text": "And to get M3, we look\nat M2, we consider",
    "start": "361230",
    "end": "363900"
  },
  {
    "text": "adding all p minus 2\npredictors that aren't in M2.",
    "start": "363900",
    "end": "366880"
  },
  {
    "text": "we choose the best one, and\nthat gives us M3, and so on.",
    "start": "366880",
    "end": "370830"
  },
  {
    "text": "So the really key thing\nhere is that in this step,",
    "start": "370830",
    "end": "374099"
  },
  {
    "text": "this is different from what we\nwere doing in the best subset",
    "start": "374100",
    "end": "377610"
  },
  {
    "text": "selection case,\nbecause here we're",
    "start": "377610",
    "end": "380099"
  },
  {
    "text": "not looking at every possible\nmodel containing K predictors.",
    "start": "380100",
    "end": "383490"
  },
  {
    "text": "Instead, we're just looking\nat every possible model that",
    "start": "383490",
    "end": "387360"
  },
  {
    "text": "contains one more\npredictor than Mk minus 1,",
    "start": "387360",
    "end": "390659"
  },
  {
    "text": "where we're going\nto take the model",
    "start": "390660",
    "end": "392132"
  },
  {
    "text": "we just got and we're just\ngoing to add one predictor to it",
    "start": "392132",
    "end": "394590"
  },
  {
    "text": "to get a slightly bigger model.",
    "start": "394590",
    "end": "397610"
  },
  {
    "text": "So just like in best\nsubset selection,",
    "start": "397610",
    "end": "399740"
  },
  {
    "text": "we're going to get p plus\n1 models from M0 to Mp,",
    "start": "399740",
    "end": "404150"
  },
  {
    "text": "but the difference is that\nthese models are nested.",
    "start": "404150",
    "end": "407150"
  },
  {
    "text": "So M1 contains the predictor\nand M0 plus one more.",
    "start": "407150",
    "end": "412580"
  },
  {
    "text": "M2 contains M1 plus\none more predictor,",
    "start": "412580",
    "end": "415580"
  },
  {
    "text": "M3 contains the predictors and\nM2 plus one more, and so on.",
    "start": "415580",
    "end": "419020"
  },
  {
    "text": "These models are nested in a way\nthat wasn't the case for a best",
    "start": "419020",
    "end": "421910"
  },
  {
    "text": "subset selection.",
    "start": "421910",
    "end": "423940"
  },
  {
    "text": "So after we get these\nP plus 1 models,",
    "start": "423940",
    "end": "426415"
  },
  {
    "text": "we're going to\nchoose among them.",
    "start": "426415",
    "end": "427790"
  },
  {
    "text": "And as I mentioned\nearlier, we can't",
    "start": "427790",
    "end": "429610"
  },
  {
    "text": "use RSS or R squared to choose\namong these p plus 1 models,",
    "start": "429610",
    "end": "433090"
  },
  {
    "text": "because they all\nhave different sizes.",
    "start": "433090",
    "end": "434900"
  },
  {
    "text": "And in a few minutes, we'll\ntalk about some approaches",
    "start": "434900",
    "end": "437229"
  },
  {
    "text": "like cross validation\nand AIC and BIC",
    "start": "437230",
    "end": "439930"
  },
  {
    "text": "to choose among these\np plus 1 models.",
    "start": "439930",
    "end": "441975"
  },
  {
    "start": "441975",
    "end": "445730"
  },
  {
    "text": "So we've said that\nforward stepwise selection",
    "start": "445730",
    "end": "451550"
  },
  {
    "text": "has a computational advantage\nover best subset selection.",
    "start": "451550",
    "end": "454860"
  },
  {
    "text": "And just to really\nreiterate why that's true,",
    "start": "454860",
    "end": "457310"
  },
  {
    "text": "in best subset selection,\nwe're considering 2",
    "start": "457310",
    "end": "460840"
  },
  {
    "text": "to the p models, which as\nwe said, is like a trillion",
    "start": "460840",
    "end": "463780"
  },
  {
    "text": "when P equals 40.",
    "start": "463780",
    "end": "465910"
  },
  {
    "text": "But in contrast, in\nforward stepwise selection,",
    "start": "465910",
    "end": "471320"
  },
  {
    "text": "for M0, we're just\nconsidering one model.",
    "start": "471320",
    "end": "476410"
  },
  {
    "text": "And to get M1, we're considering\nadding P different predictors,",
    "start": "476410",
    "end": "479410"
  },
  {
    "text": "so we're considering p models.",
    "start": "479410",
    "end": "482050"
  },
  {
    "text": "For M2, we're considering adding\np minus 1 additional predictors,",
    "start": "482050",
    "end": "485349"
  },
  {
    "text": "so that's p minus 1\nmodels, and so on.",
    "start": "485350",
    "end": "489250"
  },
  {
    "text": "And so actually, when we do\nforward stepwise selection,",
    "start": "489250",
    "end": "492280"
  },
  {
    "text": "we're considering\naround p squared models.",
    "start": "492280",
    "end": "496130"
  },
  {
    "text": "And so p squared is much\nless than 2 to the p.",
    "start": "496130",
    "end": "498270"
  },
  {
    "text": "So we're considering\nfar fewer predictors",
    "start": "498270",
    "end": "500060"
  },
  {
    "text": "when we do forward stepwise.",
    "start": "500060",
    "end": "501610"
  },
  {
    "text": "And here's another picture\nto help see what's going on.",
    "start": "501610",
    "end": "504620"
  },
  {
    "text": "And see if I can draw it. but\nif we think of the RSS picture",
    "start": "504620",
    "end": "509180"
  },
  {
    "text": "as a function of model\nsize, remember this is RSS.",
    "start": "509180",
    "end": "512299"
  },
  {
    "text": "And for A subset,\nremember of course",
    "start": "512299",
    "end": "516830"
  },
  {
    "text": "we said the curve keeps going\ndown, it might flatten out.",
    "start": "516830",
    "end": "521214"
  },
  {
    "start": "521214",
    "end": "523760"
  },
  {
    "text": "But so this is for best\nsubset, we saw that before.",
    "start": "523760",
    "end": "528955"
  },
  {
    "start": "528955",
    "end": "531478"
  },
  {
    "text": "For forward stepwise, it's also\ngoing to have the same shape,",
    "start": "531478",
    "end": "534020"
  },
  {
    "text": "but it's going to\nbe above that curve.",
    "start": "534020",
    "end": "540272"
  },
  {
    "text": "So start in the same place,\nbecause for one variable,",
    "start": "540272",
    "end": "542480"
  },
  {
    "text": "it's going to pick the\nsame best variable,",
    "start": "542480",
    "end": "545480"
  },
  {
    "text": "but it's going to be\ntypically above that curve--",
    "start": "545480",
    "end": "549300"
  },
  {
    "text": "I'm not drawing it very well.",
    "start": "549300",
    "end": "550760"
  },
  {
    "text": "--until the very end.",
    "start": "550760",
    "end": "552960"
  },
  {
    "text": "These are meant to\njoin at the end.",
    "start": "552960",
    "end": "554750"
  },
  {
    "text": "So the point is that\nforward stepwise is not",
    "start": "554750",
    "end": "557480"
  },
  {
    "text": "doing a search among\nall possible models.",
    "start": "557480",
    "end": "559380"
  },
  {
    "text": "So for a given model\nsize, it's going",
    "start": "559380",
    "end": "561710"
  },
  {
    "text": "to have an RSS\nthat typically will",
    "start": "561710",
    "end": "564380"
  },
  {
    "text": "be above that for best subset.",
    "start": "564380",
    "end": "567385"
  },
  {
    "start": "567385",
    "end": "573300"
  },
  {
    "text": "That relates to the fact that\nforward stepwise isn't actually",
    "start": "573300",
    "end": "576420"
  },
  {
    "text": "guaranteed to find the best\npossible model out of all two",
    "start": "576420",
    "end": "578940"
  },
  {
    "text": "to the p models that\nbest subset considers.",
    "start": "578940",
    "end": "583090"
  },
  {
    "text": "So if you look at Rob's\npicture right here,",
    "start": "583090",
    "end": "586410"
  },
  {
    "text": "the forward stepwise\nand best subset curves",
    "start": "586410",
    "end": "589500"
  },
  {
    "text": "have the same RSS,\nbecause they each contain",
    "start": "589500",
    "end": "593670"
  },
  {
    "text": "just the null model.",
    "start": "593670",
    "end": "594910"
  },
  {
    "text": "And then over here, these\ntwo models are the same,",
    "start": "594910",
    "end": "598138"
  },
  {
    "text": "because forward stepwise\nand best subset are each",
    "start": "598138",
    "end": "600180"
  },
  {
    "text": "considering the model\nwith all p predictors,",
    "start": "600180",
    "end": "602110"
  },
  {
    "text": "so those are the same.",
    "start": "602110",
    "end": "603279"
  },
  {
    "text": "But in between there's this\ngap, and the reason for the gap,",
    "start": "603280",
    "end": "606120"
  },
  {
    "text": "is because best subset selection\nis going to find the best",
    "start": "606120",
    "end": "609120"
  },
  {
    "text": "model with, let's\nsay k predictors,",
    "start": "609120",
    "end": "611190"
  },
  {
    "text": "but forward stepwise might not.",
    "start": "611190",
    "end": "612935"
  },
  {
    "text": "And the reason\nthat it might not,",
    "start": "612935",
    "end": "614310"
  },
  {
    "text": "is because it could be that\nthe best model containing",
    "start": "614310",
    "end": "617370"
  },
  {
    "text": "k predictors is not a superset\nof the best model containing",
    "start": "617370",
    "end": "621390"
  },
  {
    "text": "k minus 1 predictors.",
    "start": "621390",
    "end": "623370"
  },
  {
    "text": "So we can actually\nsee an example of that",
    "start": "623370",
    "end": "625410"
  },
  {
    "text": "if we look a little more\ncarefully at the credit data.",
    "start": "625410",
    "end": "627910"
  },
  {
    "text": "So this table shows\nthe results that you",
    "start": "627910",
    "end": "631750"
  },
  {
    "text": "get if you look for M0, M1, M2,\nM3 and M4 on the credit data.",
    "start": "631750",
    "end": "638610"
  },
  {
    "text": "So this is M1, M2, M3, and M4.",
    "start": "638610",
    "end": "644720"
  },
  {
    "text": "And so if we do a\nbest subset selection,",
    "start": "644720",
    "end": "647209"
  },
  {
    "text": "then M1 just contains\nthe rating variable.",
    "start": "647210",
    "end": "652410"
  },
  {
    "text": "And if we do forward\nstepwise, M1 just",
    "start": "652410",
    "end": "654290"
  },
  {
    "text": "contains the rating variable.",
    "start": "654290",
    "end": "655769"
  },
  {
    "text": "So far so good.",
    "start": "655770",
    "end": "657350"
  },
  {
    "text": "If we look at M2,\nand the best subsets,",
    "start": "657350",
    "end": "659750"
  },
  {
    "text": "M2 contains rating and income.",
    "start": "659750",
    "end": "661520"
  },
  {
    "text": "And same with forward\nstepwise is M2.",
    "start": "661520",
    "end": "664610"
  },
  {
    "text": "If we look M3, best subset\nhas rating income student,",
    "start": "664610",
    "end": "667760"
  },
  {
    "text": "and same thing with forward\nstepwise, rating income student.",
    "start": "667760",
    "end": "670820"
  },
  {
    "text": "But when we get to\nM4, things suddenly",
    "start": "670820",
    "end": "673160"
  },
  {
    "text": "change, because now in the\ncontext of a model with four",
    "start": "673160",
    "end": "675589"
  },
  {
    "text": "variables, the\nbest that we can do",
    "start": "675590",
    "end": "677660"
  },
  {
    "text": "is cards income,\nstudent, and limit.",
    "start": "677660",
    "end": "681190"
  },
  {
    "text": "But that is not a model that\nforward stepwise can give us.",
    "start": "681190",
    "end": "684100"
  },
  {
    "text": "Because remember,\nforward stepwise",
    "start": "684100",
    "end": "686110"
  },
  {
    "text": "can only give us a model\nthat contains rating, income,",
    "start": "686110",
    "end": "689570"
  },
  {
    "text": "student, and one other variable.",
    "start": "689570",
    "end": "691410"
  },
  {
    "text": "So forward stepwise is going to\ngive us rating, income, student,",
    "start": "691410",
    "end": "694449"
  },
  {
    "text": "and it's going to add in\nlimit when we ask for the best",
    "start": "694450",
    "end": "697060"
  },
  {
    "text": "model with four variables.",
    "start": "697060",
    "end": "698810"
  },
  {
    "text": "And in contrast, best subset\nisn't going to have rating,",
    "start": "698810",
    "end": "702130"
  },
  {
    "text": "it's instead going to have\ncards, and it's lost rating.",
    "start": "702130",
    "end": "705610"
  },
  {
    "text": "And so this M4, the\nbest subset, gives",
    "start": "705610",
    "end": "708190"
  },
  {
    "text": "us is going to have a smaller\nresidual sum of squares.",
    "start": "708190",
    "end": "710922"
  },
  {
    "text": "It's a different model\nthan we would have",
    "start": "710922",
    "end": "712630"
  },
  {
    "text": "gotten with forward stepwise.",
    "start": "712630",
    "end": "714980"
  },
  {
    "text": "But just because best subset has\na better model on the training",
    "start": "714980",
    "end": "718633"
  },
  {
    "text": "data doesn't mean\nthat it's really",
    "start": "718633",
    "end": "720050"
  },
  {
    "text": "going to be a\nbetter model overall",
    "start": "720050",
    "end": "721550"
  },
  {
    "text": "in the context of\ntest data, which",
    "start": "721550",
    "end": "723440"
  },
  {
    "text": "is what we really care about.",
    "start": "723440",
    "end": "724790"
  },
  {
    "text": "Well, my point\nabout that, so you",
    "start": "724790",
    "end": "726170"
  },
  {
    "text": "might wonder why this happens.",
    "start": "726170",
    "end": "727350"
  },
  {
    "text": "Well, it only happens\nwhen there's correlation",
    "start": "727350",
    "end": "729266"
  },
  {
    "text": "between the features.",
    "start": "729267",
    "end": "730280"
  },
  {
    "text": "It's pretty easy to show\nthat if the variables had",
    "start": "730280",
    "end": "732363"
  },
  {
    "text": "no correlation, then the\nvariables chosen by the two",
    "start": "732363",
    "end": "736190"
  },
  {
    "text": "methods would be\nexactly the same.",
    "start": "736190",
    "end": "737700"
  },
  {
    "text": "But because of the correlation\nbetween the features, which",
    "start": "737700",
    "end": "740117"
  },
  {
    "text": "is typically there, you can\nget a discrepancy between best",
    "start": "740117",
    "end": "742610"
  },
  {
    "text": "subset and forward stepwise.",
    "start": "742610",
    "end": "745029"
  },
  {
    "start": "745030",
    "end": "746000"
  }
]