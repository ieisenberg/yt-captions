[
  {
    "start": "0",
    "end": "61000"
  },
  {
    "text": " So for this week, we are going\nto be talking about a Bayesian",
    "start": "0",
    "end": "8860"
  },
  {
    "text": "perspective on meta-learning. And today's lecture is going\nto be a little bit different than a lot of the lectures we've\nhad so far because we're really",
    "start": "8860",
    "end": "17980"
  },
  {
    "text": "not going to be\ntalking that much about meta-learning in\nthis lecture, we're going to be talking about doing\napproximate Bayesian inference",
    "start": "17980",
    "end": "26530"
  },
  {
    "text": "via variational inference. And the reason why we're\ngoing to talk about that today is that it will be a lot\nof really useful background",
    "start": "26530",
    "end": "33340"
  },
  {
    "text": "knowledge for understanding\nBayesian meta-learning algorithms. And it's also pretty\nuseful outside of meta-learning\nalgorithms as well.",
    "start": "33340",
    "end": "40280"
  },
  {
    "text": "And so we'll see a\nlittle bit of motivation for why these things matter\nin the context of learning,",
    "start": "40280",
    "end": "45758"
  },
  {
    "text": "and we'll talk a lot about\nBayesian meta-learning on Wednesday. But today we're going to\ngo over some kind of really",
    "start": "45758",
    "end": "53800"
  },
  {
    "text": "fundamentals on how to do\nBayesian inference with more complex distribution classes.",
    "start": "53800",
    "end": "59860"
  },
  {
    "text": " Awesome. So the plan for today\nis to talk about a class",
    "start": "59860",
    "end": "65500"
  },
  {
    "start": "61000",
    "end": "120000"
  },
  {
    "text": "of probabilistic models\ncalled latent variable models. Then we're going to talk\nabout how to actually train",
    "start": "65500",
    "end": "71380"
  },
  {
    "text": "these latent variable models\nwith variational inference. Then we'll talk about\nsomething called",
    "start": "71380",
    "end": "76390"
  },
  {
    "text": "amortized variational\ninference, which will address a key\nshortcoming of trying to do non-amortized\nvariational inference.",
    "start": "76390",
    "end": "84369"
  },
  {
    "text": "And then we'll cover a couple\nexample latent variable models. So the goals of today will be\nto understand how these latent",
    "start": "84370",
    "end": "91630"
  },
  {
    "text": "variable models work, including\nin deep learning settings, and also understand how to use\nvariational inference in order",
    "start": "91630",
    "end": "98228"
  },
  {
    "text": "to train these models.  Also, some of the stuff\nwill be useful for doing",
    "start": "98228",
    "end": "106030"
  },
  {
    "text": "the optional homework 4. It will be really themed about-- one of the questions\nwill be themed",
    "start": "106030",
    "end": "112030"
  },
  {
    "text": "on Bayesian meta-learning. But one of the derivations\nin that homework will draw upon some of the\nideas that we talk about today.",
    "start": "112030",
    "end": "118630"
  },
  {
    "text": " Cool. So let's get started by talking\nabout probabilistic models.",
    "start": "118630",
    "end": "125840"
  },
  {
    "text": "So in probabilistic\nmodeling, we want to model some distribution. So for example, we may\nhave some distribution p",
    "start": "125840",
    "end": "132740"
  },
  {
    "text": "of x where we have samples\nfrom that distribution. We have some\nexample data points.",
    "start": "132740",
    "end": "138200"
  },
  {
    "text": "And we want to formulate\na probabilistic model that can generate those data points\nor evaluate the likelihood",
    "start": "138200",
    "end": "145280"
  },
  {
    "text": "of those data points. So for example, we might\ntry to fit a Gaussian model to these data\npoints, and that",
    "start": "145280",
    "end": "150829"
  },
  {
    "text": "would allow us to generate\nother data points like that or generally model the\ndistribution over those data",
    "start": "150830",
    "end": "156680"
  },
  {
    "text": "points. Similarly, we might have\na conditional probability distribution that we want to\nmodel where this won't be--",
    "start": "156680",
    "end": "164425"
  },
  {
    "text": "over the examples,\nwe'll actually be conditioning on\none variable in order to try to formulate\na probabilistic model",
    "start": "164425",
    "end": "171860"
  },
  {
    "text": "of another random variable. And we've already seen examples\nof conditional probabilistic",
    "start": "171860",
    "end": "177740"
  },
  {
    "text": "models in this course. For example, trying to\npredict a distribution over the labels given the input.",
    "start": "177740",
    "end": "183800"
  },
  {
    "text": "And most commonly, you'll view-- you'll be looking at\nthings like probability values over a discrete\ncategorical distribution",
    "start": "183800",
    "end": "191750"
  },
  {
    "text": "or outputting the mean\nand variance of a Gaussian distribution.",
    "start": "191750",
    "end": "198709"
  },
  {
    "text": "And so instead of actually\nliterally outputting the label y, we often actually\noutput the values",
    "start": "198710",
    "end": "203750"
  },
  {
    "text": "of a probability distribution. And so that's what\nkind of the logits are.",
    "start": "203750",
    "end": "209600"
  },
  {
    "text": "But of course, it doesn't\nhave to necessarily be a categorical distribution\nor a Gaussian distribution. Those are extremely\ncommon in the literature.",
    "start": "209600",
    "end": "216769"
  },
  {
    "text": "But there are all sorts\nof other distributions that we might want to\nformulate, and there's",
    "start": "216770",
    "end": "222140"
  },
  {
    "text": "things that are\nmuch more expressive than categorical or\nGaussian distributions.",
    "start": "222140",
    "end": "228409"
  },
  {
    "text": "Now when we want to go about\ntraining a probabilistic model, we will formulate our model,\nso it could be p theta of x,",
    "start": "228410",
    "end": "238040"
  },
  {
    "text": "it could be p\ntheta of y given x. And we have data from\nthat we're assuming",
    "start": "238040",
    "end": "246140"
  },
  {
    "text": "to be from that\ndistribution that we want to be able to model. And typically, we will formulate\na maximum likelihood objective",
    "start": "246140",
    "end": "252709"
  },
  {
    "text": "where our goal is to find the\nmodel for which it maximizes",
    "start": "252710",
    "end": "258049"
  },
  {
    "text": "the likelihood of the data. And this will be kind\nof arguably the best fit for the data that\nwe have observed.",
    "start": "258050",
    "end": "265910"
  },
  {
    "text": " So this sort of\nobjective is very easy",
    "start": "265910",
    "end": "271770"
  },
  {
    "text": "to evaluate and differentiate\nfor categorical distributions and Gaussian distributions,\nwhich is one of the reasons",
    "start": "271770",
    "end": "277350"
  },
  {
    "text": "why we often use categorical\nand Gaussian distributions. And this maximum\nlikelihood objective",
    "start": "277350",
    "end": "285120"
  },
  {
    "text": "corresponds to the cross-entropy\nloss and the mean squared error loss that we all use and like.",
    "start": "285120",
    "end": "292319"
  },
  {
    "text": " So really the goal\nof this lecture is to try to go beyond\ncategorical or Gaussian",
    "start": "292320",
    "end": "299460"
  },
  {
    "text": "distributions and try\nto model and train more complex distributions. ",
    "start": "299460",
    "end": "307900"
  },
  {
    "text": "Cool. So why might we want to\ntrain something more complex? So maybe we want to instead of\ntraining a normal distribution,",
    "start": "307900",
    "end": "314033"
  },
  {
    "text": "we want to train a\nparanormal distribution, or instead of generating\na label maybe,",
    "start": "314033",
    "end": "320949"
  },
  {
    "text": "we want to generate\nan image or we want to generate text or\nvideo or something like that.",
    "start": "320950",
    "end": "327007"
  },
  {
    "text": "And here's an example of\na video generative model. So we might want to generate\nvideo of an HD video of riding",
    "start": "327007",
    "end": "333639"
  },
  {
    "text": "a horse in the park at sunrise. And we don't just want\nit to give us one video, we want to give us kind\nof a range of videos",
    "start": "333640",
    "end": "340449"
  },
  {
    "text": "that capture that description. Another example of a\nmore complex scenario,",
    "start": "340450",
    "end": "345490"
  },
  {
    "text": "we might want a more\ncomplex distribution is we want to represent\nuncertainty over labels.",
    "start": "345490",
    "end": "350560"
  },
  {
    "text": "Maybe there's some ambiguity\narising from limited data or some partial observability. And it may be that the true\ndistribution over the labels",
    "start": "350560",
    "end": "358449"
  },
  {
    "text": "may not be a unimodal\ndistribution. It may be a multimodal\ndistribution that's difficult to capture\nwith a Gaussian distribution",
    "start": "358450",
    "end": "364480"
  },
  {
    "text": "or a categorical distribution. And second, we may also want\nto represent uncertainty",
    "start": "364480",
    "end": "371080"
  },
  {
    "text": "over functions rather than\njust uncertainty over examples or over labels.",
    "start": "371080",
    "end": "377633"
  },
  {
    "text": "Now what do we mean by that? Well, in meta-learning,\nwe have been representing",
    "start": "377633",
    "end": "384849"
  },
  {
    "text": "essentially point estimates of\ndistributions over functions. So we've often been\ntrying to estimate",
    "start": "384850",
    "end": "390430"
  },
  {
    "text": "what are our task\nspecific parameters given a small training data set\nand our meta parameters.",
    "start": "390430",
    "end": "395979"
  },
  {
    "text": "And typically, we just\noutput a single set of task-specific\nparameters phi i. But there are cases\nwhere we might actually",
    "start": "395980",
    "end": "403210"
  },
  {
    "text": "want to fully represent\nthe distribution over task-specific parameters.",
    "start": "403210",
    "end": "408850"
  },
  {
    "text": "So for example, you might have\na few shot learning problem where there's some ambiguity.",
    "start": "408850",
    "end": "415480"
  },
  {
    "text": "So say for example, this is\nlike a small training data set or a small support\nset for a classifier.",
    "start": "415480",
    "end": "422198"
  },
  {
    "text": "We have some positive\nexamples on the left, some negative\nexamples on the right. And then say I gave you\nthis example right here.",
    "start": "422198",
    "end": "432720"
  },
  {
    "text": "If you look at the attributes\nof the positive examples and the negative\nexamples, you might notice that everyone in the\npositive examples is smiling,",
    "start": "432720",
    "end": "440370"
  },
  {
    "text": "and everyone in the positive\nexamples is also wearing a hat. And this person is not smiling,\nand they're wearing a hat.",
    "start": "440370",
    "end": "447150"
  },
  {
    "text": "And so it's inherently\nambiguous what the correct label should be. Is the classifier\nsupposed to pay attention",
    "start": "447150",
    "end": "452542"
  },
  {
    "text": "to the facial expression? Is it supposed to pay\nattention to whether they're wearing a hat?",
    "start": "452542",
    "end": "458650"
  },
  {
    "text": "And there might be another\nexample like this one where it's also somewhat\nambiguous with respect to age.",
    "start": "458650",
    "end": "463932"
  },
  {
    "text": "Maybe the classifier\nis supposed to also be looking at the\nage of the person.",
    "start": "463933",
    "end": "469140"
  },
  {
    "text": "So this sort of\nambiguity can come up and we may want to not just\noutput a single classifier,",
    "start": "469140",
    "end": "474870"
  },
  {
    "text": "but actually output\nmultiple classifiers that might kind of generate\nhypotheses underlying",
    "start": "474870",
    "end": "481320"
  },
  {
    "text": "the data that we see. So yeah. Can we learn to generate\nhypotheses about the underlying",
    "start": "481320",
    "end": "488620"
  },
  {
    "text": "function so that we can\nsample from this distribution over task-specific parameters?",
    "start": "488620",
    "end": "495129"
  },
  {
    "text": "And this ability to\nreason about ambiguity might be important in a\nfew different settings.",
    "start": "495130",
    "end": "500169"
  },
  {
    "text": "It might be important in really\nsafety critical settings. So if you want to do something\nlike medical imaging, then it'll be useful\nto know if your class--",
    "start": "500170",
    "end": "507430"
  },
  {
    "text": "if you're pretty confident\nabout the function that you have or not.",
    "start": "507430",
    "end": "512950"
  },
  {
    "text": "It can also be important for\nactive learning settings. So if you want to figure out-- if you want to generate--\nlike should you generate more",
    "start": "512950",
    "end": "520030"
  },
  {
    "text": "examples or more data points? Basically, can the\nalgorithm tell you,",
    "start": "520030",
    "end": "526090"
  },
  {
    "text": "yes, I need more\nlabels or no, I'm actually pretty confident\nabout this example? And there's actually\na number of works",
    "start": "526090",
    "end": "532630"
  },
  {
    "text": "that have looked\nat active learning with meta-learning algorithms. And it can also be useful in\nmeta reinforcement learning",
    "start": "532630",
    "end": "540190"
  },
  {
    "text": "as well. If you want to figure out how\nto explore a new environment with a small amount of data,\nreasoning about the uncertainty",
    "start": "540190",
    "end": "546820"
  },
  {
    "text": "that you have can\nhelp you figure out what parts of the\nenvironment to explore and what parts of\nthe environment",
    "start": "546820",
    "end": "552134"
  },
  {
    "text": "are already-- kind of you're\nalready sufficiently confident. OK. ",
    "start": "552134",
    "end": "557819"
  },
  {
    "text": "So those past few\nslides are a couple of motivation for why\nwe might want to train",
    "start": "557820",
    "end": "563850"
  },
  {
    "text": "more complex distributions. And then the rest\nof this lecture will really be on actually\ntrying to do that.",
    "start": "563850",
    "end": "572610"
  },
  {
    "start": "572000",
    "end": "756000"
  },
  {
    "text": "Cool. So let's start by looking at\na couple examples of latent",
    "start": "572610",
    "end": "578130"
  },
  {
    "text": "variable models. And some of them might\nstart with examples that you've seen before.",
    "start": "578130",
    "end": "583810"
  },
  {
    "text": "So say you have\nexamples of some data points that look\nlike this, and you want to fit a model\nto these data points.",
    "start": "583810",
    "end": "592962"
  },
  {
    "text": "If you just fit a\nGaussian to this, you probably wouldn't get a\nvery good fit to this data.",
    "start": "592962",
    "end": "597995"
  },
  {
    "text": "And so what you\nmight use instead is something called a mixture\nmodel where you try to fit--",
    "start": "597995",
    "end": "603700"
  },
  {
    "text": "you have a few\ndifferent components. And each component is one\ncomponent of your mixture.",
    "start": "603700",
    "end": "608875"
  },
  {
    "text": "And this would be an example\nof something like a Gaussian mixture model where the\nunderlying data points,",
    "start": "608875",
    "end": "614700"
  },
  {
    "text": "you don't know the underlying\nstructure in which data points correspond to which\nmixture component,",
    "start": "614700",
    "end": "620880"
  },
  {
    "text": "but through the\nmodeling process you want to model both the mixture\ncomponents and the parameters of each mixture component.",
    "start": "620880",
    "end": "627518"
  },
  {
    "text": "And so mathematically, the way\nthat you would write this down is something like this where\nyour mixture component is z",
    "start": "627518",
    "end": "634140"
  },
  {
    "text": "and your parameters\nof each mixture",
    "start": "634140",
    "end": "639420"
  },
  {
    "text": "corresponds to the\np of x given z. And in this particular case of\na Gaussian mixture model, p of z",
    "start": "639420",
    "end": "646050"
  },
  {
    "text": "would be a categorical\ndistribution. And then p of x given z would\nbe a conditional Gaussian distribution.",
    "start": "646050",
    "end": "651779"
  },
  {
    "text": " Cool. So you may also have a\nconditional distribution",
    "start": "651780",
    "end": "658910"
  },
  {
    "text": "that you want to model also\nwith something like a mixture. And to do that, you\ncan basically just take",
    "start": "658910",
    "end": "664520"
  },
  {
    "text": "the same Gaussian mixture\nmodel and condition everything on the variable\nthat you're conditioning.",
    "start": "664520",
    "end": "670770"
  },
  {
    "text": "So this would now be instead\nof p of z you could condition your mixture element on x. So you'd have p of z given x.",
    "start": "670770",
    "end": "677300"
  },
  {
    "text": "And instead of having\nyour conditional Gaussian distribution only\nbe conditioned on z, you would also condition\nit on the variable",
    "start": "677300",
    "end": "684470"
  },
  {
    "text": "that you're conditioning\non, which in this case is x. And this is referred\nto something--",
    "start": "684470",
    "end": "689600"
  },
  {
    "text": "It's really basically, just a\nconditional Gaussian mixture model, but it has this fancy\nname of a mixture density",
    "start": "689600",
    "end": "694670"
  },
  {
    "text": "network. And so as an example of\nwhat this might look like is if you want to\nclassify or if you",
    "start": "694670",
    "end": "700340"
  },
  {
    "text": "want to regress to the length\nof a paper given the title. Then instead of just outputting\nthe mean length that you think",
    "start": "700340",
    "end": "709190"
  },
  {
    "text": "or the mean and variance\nof the length of the paper, you would actually output\nmultiple mean and variances",
    "start": "709190",
    "end": "714230"
  },
  {
    "text": "and the weight of the\ncorresponding Gaussian.",
    "start": "714230",
    "end": "721829"
  },
  {
    "text": "And so typically for a\nstandard regression problem, we may even only output the\nmean just a single value",
    "start": "721830",
    "end": "727610"
  },
  {
    "text": "corresponding to the\nlength of the paper. And this sort of\nmodel will actually represent kind of a much\nmore rich notion of what",
    "start": "727610",
    "end": "735830"
  },
  {
    "text": "you think the label might be. And instead of outputting\njust one number, you would output\nn times 3 numbers",
    "start": "735830",
    "end": "743570"
  },
  {
    "text": "where n is the number of mixture\ncomponents in your mixture model. ",
    "start": "743570",
    "end": "750253"
  },
  {
    "text": "So these are a couple of\nexamples of latent variable models. Now we can look at\nthe more general case.",
    "start": "750253",
    "end": "757500"
  },
  {
    "text": "So in general, our goal\nwith latent variable models is to model a fairly\ncomplicated distribution,",
    "start": "757500",
    "end": "765380"
  },
  {
    "text": "such as the distribution here. And the way that we do that\nis first we formulate a latent",
    "start": "765380",
    "end": "771620"
  },
  {
    "text": "variable z. And it's called latent\nbecause we don't observe z. We only observe x. We don't observe z",
    "start": "771620",
    "end": "778190"
  },
  {
    "text": "And then we sample from\nthat latent variable. This is going to be a\nrelatively simple distribution,",
    "start": "778190",
    "end": "784200"
  },
  {
    "text": "such as a Gaussian. And then we'll pass a\nsample from that variable",
    "start": "784200",
    "end": "789589"
  },
  {
    "text": "into a neural network\nthat will also be a pretty simple\ndistribution as well. So this will also be a\nGaussian distribution.",
    "start": "789590",
    "end": "797090"
  },
  {
    "text": "But the parameters of\nthis conditional Gaussian distribution will\nbe fairly complex.",
    "start": "797090",
    "end": "804800"
  },
  {
    "text": "And by essentially kind of\ntaking this easy distribution, and this easy distribution,\nwe can kind of",
    "start": "804800",
    "end": "811070"
  },
  {
    "text": "compose them to formulate a\nmore complicated distribution. Basically, we're going to be\nkind of transforming samples",
    "start": "811070",
    "end": "818695"
  },
  {
    "text": "from a Gaussian distribution\nwith a neural network, to then give us\nsamples from this more",
    "start": "818695",
    "end": "823940"
  },
  {
    "text": "complicated distribution. And so the kind of\ngeneral form of this",
    "start": "823940",
    "end": "831438"
  },
  {
    "text": "is this pretty simple\nequation right here, where we have our distribution\nover our latent variable",
    "start": "831438",
    "end": "837959"
  },
  {
    "text": "and our conditional distribution\nof our example given our latent variable. ",
    "start": "837960",
    "end": "845540"
  },
  {
    "text": "And so in both of\nthese cases, p of z is going to be an easy or\nsimple distribution and p",
    "start": "845540",
    "end": "851420"
  },
  {
    "text": "of x given z is also going to\nbe a simple distribution class.",
    "start": "851420",
    "end": "856558"
  },
  {
    "text": "And so really the\nkey idea is that we can represent this more complex\ndistribution by composing",
    "start": "856558",
    "end": "862269"
  },
  {
    "text": "to more simple distributions. Now it's worth mentioning\nthat the p of x given z,",
    "start": "862270",
    "end": "873709"
  },
  {
    "text": "I'm calling this a\nsimple distribution. But the function itself\ncan be very complex.",
    "start": "873710",
    "end": "878757"
  },
  {
    "text": "The function itself\ncan be represented by a neural network, which\nis a pretty complex function. But the distribution\nclass is really",
    "start": "878757",
    "end": "886970"
  },
  {
    "text": "going to be limited by the\noutput distribution, which is a Gaussian distribution,\nand your neural network",
    "start": "886970",
    "end": "892880"
  },
  {
    "text": "is going to be outputting\nthe mean and variance of that Gaussian distribution. Yeah?",
    "start": "892880",
    "end": "898250"
  },
  {
    "text": "So your x given z is\nalways going to be like-- just that's always a\nGaussian distribution.",
    "start": "898250",
    "end": "905150"
  },
  {
    "text": "So in order to get that\ncomplicated distribution, you kind of just\nmap the different x to like these differences\nthat you've got,",
    "start": "905150",
    "end": "914130"
  },
  {
    "text": "the corresponding p's in the\nmore complex distribution in terms of composing\nthe distributions?",
    "start": "914130",
    "end": "919147"
  },
  {
    "text": " Yeah. So you can basically think of\nlike, the neural network will",
    "start": "919147",
    "end": "925940"
  },
  {
    "text": "take kind of one slice of\nthis and then kind of map it onto a particular slice\nof the top distribution.",
    "start": "925940",
    "end": "934220"
  },
  {
    "text": "And it may give you-- it's not just going to\ngive you a single value.",
    "start": "934220",
    "end": "939740"
  },
  {
    "text": "It'll still give you a\ndistribution over that. But it might be-- it's going to be a fairly\nsimple distribution.",
    "start": "939740",
    "end": "945220"
  },
  {
    "text": "And so you can imagine like\nmapping to a small Gaussian right here, for example. And so yeah, you can think of\nit as taking samples from this",
    "start": "945220",
    "end": "952069"
  },
  {
    "text": "and mapping it to samples\ninto that function. Yeah? WIth this setup, you won't\nget any arbitrary distribution",
    "start": "952070",
    "end": "959230"
  },
  {
    "text": "over p of x? So the question is,\nwith this set up, can you get any arbitrary\ndistribution over p of x?",
    "start": "959230",
    "end": "965730"
  },
  {
    "text": " In general, yes. I don't know of--",
    "start": "965730",
    "end": "971279"
  },
  {
    "text": "I can't think of\nany distribution where you wouldn't be\nable to represent it. It does mean that there\nare some distributions that",
    "start": "971280",
    "end": "976980"
  },
  {
    "text": "will be harder to\nrepresent than others. If you have an extremely\ndiscontinuous representation, it's hard for--",
    "start": "976980",
    "end": "982980"
  },
  {
    "text": "it was hard to\nalso just optimize for those distributions\nin the first place. But this does give you really\nconsiderable expressive power",
    "start": "982980",
    "end": "991170"
  },
  {
    "text": "over distributions.  The other thing that\nI'll note here is that--",
    "start": "991170",
    "end": "997210"
  },
  {
    "text": "note that there's only one\nneural network in this picture, which is the P of x given z. Typically, p of z is going to\nbe a Gaussian distribution.",
    "start": "997210",
    "end": "1005880"
  },
  {
    "text": "And you don't even actually\nhave to learn the parameters of this Gaussian distribution. You can just set it to be like\na standard normal distribution",
    "start": "1005880",
    "end": "1013590"
  },
  {
    "text": "with 0 mean and unit variance. And that you don't\nlose any generality by doing that because\nthe neural network",
    "start": "1013590",
    "end": "1020210"
  },
  {
    "text": "you can very easily take\nthat and transform it into whatever it wants. And so you can think\nof the first layer",
    "start": "1020210",
    "end": "1026744"
  },
  {
    "text": "as like picking a different\nmean and variance for that. And so that means that\nyou don't actually have to learn any\naspect of p of z,",
    "start": "1026744",
    "end": "1033000"
  },
  {
    "text": "you only have to\nlearn p of x given z. ",
    "start": "1033000",
    "end": "1041433"
  },
  {
    "text": "OK. So now I have two\nquestions for you. And in honor of Halloween,\nI have a little bit of candy",
    "start": "1041434",
    "end": "1047020"
  },
  {
    "text": "for people who ask questions\nor who answer questions. So the first question is, once\nyou train this neural network,",
    "start": "1047020",
    "end": "1056919"
  },
  {
    "text": "how do you generate\na sample from p of x? ",
    "start": "1056920",
    "end": "1064200"
  },
  {
    "text": "Yeah? Just from sample from p of\nz toward through the network",
    "start": "1064200",
    "end": "1069690"
  },
  {
    "text": "and that your-- you should learn probably. You should produce p of\nx or something probably.",
    "start": "1069690",
    "end": "1075400"
  },
  {
    "text": "Yeah. So basically, you can\nsample from p of z then pass that through\nyour neural network, and then your neural\nnetwork will give you",
    "start": "1075400",
    "end": "1082470"
  },
  {
    "text": "a mean and variance. And then you just\nsample from the Gaussian with that mean and variance. And that will then\ngive you a sample",
    "start": "1082470",
    "end": "1088350"
  },
  {
    "text": "from your estimated p of x. So I'm not very\ngood at throwing. ",
    "start": "1088350",
    "end": "1095789"
  },
  {
    "text": "It's only one seat off. Cool. And then the second\nquestion, which",
    "start": "1095790",
    "end": "1100980"
  },
  {
    "text": "is a little bit harder than\nthe first question is, now we have this model\nthat we've trained,",
    "start": "1100980",
    "end": "1108510"
  },
  {
    "text": "how do we evaluate the\nlikelihood of a given sample? So we just talked about\nhow do we sample something.",
    "start": "1108510",
    "end": "1115429"
  },
  {
    "text": "Now instead of trying\nto sample something, we're actually just given an x. And how do we evaluate\np of x for that given x?",
    "start": "1115430",
    "end": "1123840"
  },
  {
    "text": " I mean, p of x these are--",
    "start": "1123840",
    "end": "1131900"
  },
  {
    "text": "probably you'd get some\nblocks to integrate over several projections.",
    "start": "1131900",
    "end": "1139620"
  },
  {
    "text": "Yeah. Do you want to--  so you're saying that we could--",
    "start": "1139620",
    "end": "1145390"
  },
  {
    "text": "if x is our example,\nwe can then integrate? ",
    "start": "1145390",
    "end": "1151600"
  },
  {
    "text": "For more p of x that we\ncreated or we draw a sample.",
    "start": "1151600",
    "end": "1157500"
  },
  {
    "text": "So you say we can draw a sample\nfrom z, and then integrate? Yeah.",
    "start": "1157500",
    "end": "1163190"
  },
  {
    "text": "Normally, we just give an\nintegral function from px to z. But you just\nenforce a bunch of z",
    "start": "1163190",
    "end": "1168620"
  },
  {
    "text": "and you just\nintegrate the course. Yeah. So what you could do is we\nhave our expression over here.",
    "start": "1168620",
    "end": "1174240"
  },
  {
    "text": "And so we could sample\na bunch of z's and use that to estimate this\nintegral on the left. And that would give us an\nestimate for what p of x is.",
    "start": "1174240",
    "end": "1183510"
  },
  {
    "text": "Of course, I will-- ",
    "start": "1183510",
    "end": "1190080"
  },
  {
    "text": "You're good. OK. I don't want to disincentivize\nincorrect answers. ",
    "start": "1190080",
    "end": "1197680"
  },
  {
    "text": "Cool. So you can estimate p of x. And one thing that's important\nthat's worth noting here",
    "start": "1197680",
    "end": "1204700"
  },
  {
    "text": "is that actually evaluating\np of x like analytically is extremely difficult because\nof this integral right here.",
    "start": "1204700",
    "end": "1213230"
  },
  {
    "text": "And that's going to make\ntraining these models also pretty difficult. Question?",
    "start": "1213230",
    "end": "1218340"
  },
  {
    "text": "So we know that\nmean squared error is equivalent to\nlikelihood, right? So can we sample that?",
    "start": "1218340",
    "end": "1225320"
  },
  {
    "start": "1220000",
    "end": "1332000"
  },
  {
    "text": "So can we sample-- let's say, you're\ntrying sampling from z and getting off with x and then\ndoing mean squared error over",
    "start": "1225320",
    "end": "1232520"
  },
  {
    "text": "x, is it possible? So you're asking-- we know\nthat mean squared error is equivalent to log likelihood.",
    "start": "1232520",
    "end": "1240264"
  },
  {
    "text": "It's equivalent\nto log likelihood for I think unit variance,\nif you don't basically",
    "start": "1240265",
    "end": "1245708"
  },
  {
    "text": "learn the variance. And you're saying, if we\nsample something from our model",
    "start": "1245708",
    "end": "1251180"
  },
  {
    "text": "and then measure\nmean squared error, would that give us\nlikelihood, you're saying? ",
    "start": "1251180",
    "end": "1256730"
  },
  {
    "text": "That would not. In general, if you\nonly sample one thing,",
    "start": "1256730",
    "end": "1263180"
  },
  {
    "text": "you're not going to get a\nvery good estimate of it, so you might need to\nsample much more than that. ",
    "start": "1263180",
    "end": "1270809"
  },
  {
    "text": "Yeah. ",
    "start": "1270810",
    "end": "1276817"
  },
  {
    "text": "So generally, if you're\nnot learning the variance, you might be able to\ndo something like that. But really the\ncorrect thing to do",
    "start": "1276817",
    "end": "1283970"
  },
  {
    "text": "is to try to\nevaluate or estimate that integral right there. And you might get a\nform that-- if you don't learn the variance\nof certain things,",
    "start": "1283970",
    "end": "1290592"
  },
  {
    "text": "you might get a form that\nlooks like mean squared error. Yeah? After multiple axis of\nthe radius [INAUDIBLE]",
    "start": "1290592",
    "end": "1298710"
  },
  {
    "text": "everything like that? Yeah. So once you actually-- so\nlog likelihood corresponds",
    "start": "1298710",
    "end": "1304740"
  },
  {
    "text": "to mean squared error if\nyou have unit variance. And if you learn the variance\nthat it ends up being",
    "start": "1304740",
    "end": "1309780"
  },
  {
    "text": "a weighted mean squared error. And so there is basically\nthat sort of equivalence.",
    "start": "1309780",
    "end": "1316230"
  },
  {
    "text": "Once you actually\nstart to write out that equation for\nGaussian distributions,",
    "start": "1316230",
    "end": "1321540"
  },
  {
    "text": "you will get\nsomething that looks like a weighted\nmean squared error. ",
    "start": "1321540",
    "end": "1330440"
  },
  {
    "text": "Cool. So this gets into how we can\ntrain latent variable models.",
    "start": "1330440",
    "end": "1336050"
  },
  {
    "start": "1332000",
    "end": "1434000"
  },
  {
    "text": "And we know that we have\nthis likelihood function. And so we know that--",
    "start": "1336050",
    "end": "1342840"
  },
  {
    "text": "sorry, we know that p\nof x is the integral of p of x given z times p of z.",
    "start": "1342840",
    "end": "1348030"
  },
  {
    "text": "And so if we wanted to go\nabout training these models, we could basically plug-in\nthis p of x into our equation",
    "start": "1348030",
    "end": "1358810"
  },
  {
    "text": "right there. And so our objective\nwould be kind of summing over our data\npoints or averaging over",
    "start": "1358810",
    "end": "1365740"
  },
  {
    "text": "our data points of log\nof the integral of p",
    "start": "1365740",
    "end": "1371320"
  },
  {
    "text": "of x given z times p of z dz.",
    "start": "1371320",
    "end": "1377200"
  },
  {
    "text": "And so we could try to go about\noptimizing for this equation",
    "start": "1377200",
    "end": "1384549"
  },
  {
    "text": "right here. But unfortunately,\ngenerally, you need to sample a lot of z's in\norder to actually accurately",
    "start": "1384550",
    "end": "1394330"
  },
  {
    "text": "estimate this integral. And so in general, this\nintegral isn't going to be particularly tractable.",
    "start": "1394330",
    "end": "1402370"
  },
  {
    "text": "If we needed to optimize-- if\nyou need to every single time we take a gradient step\nthat we need to sample a ton of different z's,\nthen we wouldn't",
    "start": "1402370",
    "end": "1410590"
  },
  {
    "text": "be able to optimize this\nparticularly easily. And so that's where\nall these kind",
    "start": "1410590",
    "end": "1418760"
  },
  {
    "text": "of different techniques for\ntraining latent variable models come in. They want to basically\ntry to estimate",
    "start": "1418760",
    "end": "1425735"
  },
  {
    "text": "the gradient of this\nobjective without having to evaluate the integral. ",
    "start": "1425735",
    "end": "1434880"
  },
  {
    "start": "1434000",
    "end": "1548000"
  },
  {
    "text": "So there's a number\nof different flavors of latent variable models in\nthe deep learning literature.",
    "start": "1434880",
    "end": "1440420"
  },
  {
    "text": "And they basically all just\ncome down to different ways to train these kinds of\nlatent variable models.",
    "start": "1440420",
    "end": "1445712"
  },
  {
    "text": "And so you may have\nheard of things like generative\nadversarial networks, variational autoencoders,\nnormalizing flow",
    "start": "1445712",
    "end": "1451460"
  },
  {
    "text": "models, and diffusion models. And all of these models\nare just different examples",
    "start": "1451460",
    "end": "1457130"
  },
  {
    "text": "of different latent\nvariable models. They all have some\nsort of latent variable",
    "start": "1457130",
    "end": "1462590"
  },
  {
    "text": "that's typically-- I guess actually in\nall of these cases, I think it's typically a\nGaussian latent variable,",
    "start": "1462590",
    "end": "1469340"
  },
  {
    "text": "is then transformed\nwith the neural network into the original problem into\nthe space of the examples.",
    "start": "1469340",
    "end": "1478587"
  },
  {
    "text": "There's one class\nof generative models that does not use\nlatent variables, and this is\nautoregressive models. You saw these in the generative\npre-training lecture.",
    "start": "1478587",
    "end": "1487020"
  },
  {
    "text": "But basically, everything\nelse that I know of uses a form of latent variable. ",
    "start": "1487020",
    "end": "1493760"
  },
  {
    "text": "And really the way that these\ndifferent latent variable models differ is how\nthey are trained.",
    "start": "1493760",
    "end": "1499700"
  },
  {
    "text": "And they have\ndifferent pros and cons in terms of the ability\nto optimize them",
    "start": "1499700",
    "end": "1506420"
  },
  {
    "text": "and the ability to evaluate\nlikelihoods of samples and so forth. In this lecture and\nalso on Wednesday,",
    "start": "1506420",
    "end": "1513757"
  },
  {
    "text": "we're really going\nto focus on methods that use variational\ninference to train them,",
    "start": "1513757",
    "end": "1519029"
  },
  {
    "text": "primarily because it has\na number of benefits, and it also is what's been used\nmost for latent variable models",
    "start": "1519030",
    "end": "1528870"
  },
  {
    "text": "over parameters\nin meta-learning. However, there's also lots\nof hybrids of these models",
    "start": "1528870",
    "end": "1535674"
  },
  {
    "text": "as well where variational\ninference becomes quite useful. Like there are papers that\ncombine VAEs and GANs. There are variational\ndiffusion models.",
    "start": "1535675",
    "end": "1542323"
  },
  {
    "text": "And there's also connections\nbetween variational inference and normalizing\nflow models as well. ",
    "start": "1542323",
    "end": "1549090"
  },
  {
    "start": "1548000",
    "end": "1669000"
  },
  {
    "text": "Yeah? What is like a good example\nof latent variables?",
    "start": "1549090",
    "end": "1555389"
  },
  {
    "text": "What is a good example\nof latent variables? That's a great question. So in general, oftentimes when\nyou train the latent variable",
    "start": "1555390",
    "end": "1564929"
  },
  {
    "text": "model, the latent\nvariables don't necessarily have any particular\ninterpretable meaning to them.",
    "start": "1564930",
    "end": "1572910"
  },
  {
    "text": "In the case of the mixture\nmodels that we saw, the latent variable corresponded\nto the different mixture components.",
    "start": "1572910",
    "end": "1578159"
  },
  {
    "text": "And in some cases, that\nmight have a natural kind of interpretable meaning. If your data naturally has\nthose different clusters, then",
    "start": "1578160",
    "end": "1585150"
  },
  {
    "text": "basically, the\nlatent variable will correspond to the\nidentity of that cluster.",
    "start": "1585150",
    "end": "1591040"
  },
  {
    "text": "And in this example, the\nface attribute example that we saw different\nlatent variables. If you were to try to\ngenerate images like that,",
    "start": "1591040",
    "end": "1599648"
  },
  {
    "text": "different latent\nvariables might correspond to different attributes. And if you're trying\nto train a classifier",
    "start": "1599648",
    "end": "1605610"
  },
  {
    "text": "and basically generate\nthese different classifiers, then different latent\nvariables would correspond to different\ncombinations of attributes",
    "start": "1605610",
    "end": "1611323"
  },
  {
    "text": "that that classifier\nis paying attention to. So ideally, that's\nthe kind of thing",
    "start": "1611323",
    "end": "1617309"
  },
  {
    "text": "that we want these latent\nvariables to correspond to. There's a question of whether\nthe model will actually",
    "start": "1617310",
    "end": "1622980"
  },
  {
    "text": "learn those things in practice. And it can use the latent\nvariables to really correspond",
    "start": "1622980",
    "end": "1629760"
  },
  {
    "text": "to whatever it wants. The other thing that I'll\nmention on this topic is that, especially in the\ncontext of conditional latent",
    "start": "1629760",
    "end": "1637019"
  },
  {
    "text": "variable models-- for example, if you're\ntraining something to predict y given x where\nz is your latent variable,",
    "start": "1637020",
    "end": "1646650"
  },
  {
    "text": "then the latent\nvariable is basically the only source of randomness or\nthe only source of information",
    "start": "1646650",
    "end": "1653100"
  },
  {
    "text": "other than your input x. And so z or the\nlatent variable needs",
    "start": "1653100",
    "end": "1658409"
  },
  {
    "text": "to capture things in y\nthat can't be found in x. ",
    "start": "1658410",
    "end": "1668440"
  },
  {
    "text": "Cool. So now let's try to talk\nabout how we can actually train these models,\nlike how do we train",
    "start": "1668440",
    "end": "1675130"
  },
  {
    "start": "1669000",
    "end": "1713000"
  },
  {
    "text": "these models to generate\npretty pictures or parameters or whatever.",
    "start": "1675130",
    "end": "1680240"
  },
  {
    "text": "And so an outline of what\nwe're going to talk about is, first, we're going to\nformulate a bound on the log",
    "start": "1680240",
    "end": "1686350"
  },
  {
    "text": "likelihood objective\nthat will allow us to get away from computing\nthat annoying integral.",
    "start": "1686350",
    "end": "1693460"
  },
  {
    "text": "Then we're going to check\nhow tight the bound is. And then we're going to move\ntowards amortized variational",
    "start": "1693460",
    "end": "1700320"
  },
  {
    "text": "inference, which is kind of an\nimportant practical step that's important in deep\nlatent variable models.",
    "start": "1700320",
    "end": "1705909"
  },
  {
    "text": "And lastly, we'll talk about\nhow to actually optimize the bound that we had. ",
    "start": "1705910",
    "end": "1714220"
  },
  {
    "start": "1713000",
    "end": "2304000"
  },
  {
    "text": "Cool. So we want to go about\noptimizing this objective, and we're going to formulate\na slightly different objective",
    "start": "1714220",
    "end": "1721970"
  },
  {
    "text": "called the expected\nlog likelihood. And on this slide, I'm not\ngoing to justify this objective.",
    "start": "1721970",
    "end": "1727770"
  },
  {
    "text": "This is mostly for\njust trying to build some intuition for\nwhat we're going to be doing on the next slide.",
    "start": "1727770",
    "end": "1735420"
  },
  {
    "text": "And in particular,\nwith this kind of expected log likelihood,\nit looks a lot like this.",
    "start": "1735420",
    "end": "1741810"
  },
  {
    "text": "But we've now\nreplaced the integral with an expectation over the\nlatent variable given the input",
    "start": "1741810",
    "end": "1751670"
  },
  {
    "text": "x. And the intuition here is\nthat essentially, we don't--",
    "start": "1751670",
    "end": "1757795"
  },
  {
    "text": "when we have this\nintegral, we're kind of considering all\npossible values of z. And really the ones that\nare relevant for optimizing",
    "start": "1757795",
    "end": "1766760"
  },
  {
    "text": "this function are the z's that\nare most likely for a given xi. We don't want to have to\nconsider all possible z's.",
    "start": "1766760",
    "end": "1775435"
  },
  {
    "text": "We want to consider\nthe ones that are actually going to be most\nlikely corresponding to an xi.",
    "start": "1775435",
    "end": "1780620"
  },
  {
    "text": "Of course, there isn't just\na single z for a given xi.",
    "start": "1780620",
    "end": "1785870"
  },
  {
    "text": "There may actually be\nmultiple possible values of your latent variable. And so we're going to be\nusing this distribution of z",
    "start": "1785870",
    "end": "1792799"
  },
  {
    "text": "given xi, which is often\nreferred to as the posterior distribution.",
    "start": "1792800",
    "end": "1798870"
  },
  {
    "text": "And once we can kind of consider\nthese possible values of z",
    "start": "1798870",
    "end": "1804500"
  },
  {
    "text": "and then maximize for this kind\nof joint likelihood for that z",
    "start": "1804500",
    "end": "1810200"
  },
  {
    "text": "and our sample x, this\nobjective becomes a lot easier to optimize because we can\nestimate that expectation using",
    "start": "1810200",
    "end": "1818475"
  },
  {
    "text": "samples. ",
    "start": "1818475",
    "end": "1823510"
  },
  {
    "text": "And so once we have\nthis objective, it's pretty nice,\nalthough there's still this question of how do we\nactually estimate z given xi.",
    "start": "1823510",
    "end": "1832820"
  },
  {
    "text": "That's not something that\nwe're really given a priori. ",
    "start": "1832820",
    "end": "1838549"
  },
  {
    "text": "And so if you think\nabout first what z given xi looks like maybe\nfor this particular value of x,",
    "start": "1838550",
    "end": "1846990"
  },
  {
    "text": "the corresponding z might have\nthis distribution right here.",
    "start": "1846990",
    "end": "1852567"
  },
  {
    "text": "What we're going to\ntry to do is we're going to try to estimate\nthis distribution of z given xi with something\nfairly simple.",
    "start": "1852567",
    "end": "1861390"
  },
  {
    "text": "So we'll estimate it\nwith what I'll call qi. And basically for every\nsingle data point,",
    "start": "1861390",
    "end": "1868350"
  },
  {
    "text": "we're going to try to estimate\nwhat we think the latent variable is, and that will be\nrepresented by this q variable",
    "start": "1868350",
    "end": "1876180"
  },
  {
    "text": "here. So instead of actually\nhaving the true z of xi, we're going to estimate it\nwith something that looks",
    "start": "1876180",
    "end": "1883020"
  },
  {
    "text": "like a Gaussian distribution. Now, this estimate isn't\ngoing to be perfect.",
    "start": "1883020",
    "end": "1888419"
  },
  {
    "text": "And as you saw in the slide,\nthese two distributions weren't exactly the same.",
    "start": "1888420",
    "end": "1893760"
  },
  {
    "text": "But once we have\nthis estimate, it will help us with\noptimizing the objective.",
    "start": "1893760",
    "end": "1899252"
  },
  {
    "text": " So the thing that's nice\nabout actually trying",
    "start": "1899252",
    "end": "1906320"
  },
  {
    "text": "to approximate z given\nxi with something like qi is that once we have this\nestimate of the latent",
    "start": "1906320",
    "end": "1915980"
  },
  {
    "text": "variable for a\nparticular data point, it means that we can formulate\na bound on our objective.",
    "start": "1915980",
    "end": "1923960"
  },
  {
    "text": "And the bound is not too\ndifficult to formulate.",
    "start": "1923960",
    "end": "1931167"
  },
  {
    "text": " So we want to formulate a bound\non p of xi or log p of xi.",
    "start": "1931167",
    "end": "1945780"
  },
  {
    "text": "So this is the log likelihood\nfor a single data point. In reality, we would actually\nbe summing over all of our data",
    "start": "1945780",
    "end": "1951480"
  },
  {
    "text": "points, but I'm just going to\nremove the sums for simplicity of notation. And like we talked about before,\nthis is equal to the integral",
    "start": "1951480",
    "end": "1960640"
  },
  {
    "text": "over--  sorry, this is equal to the\nlog of the integral of p",
    "start": "1960640",
    "end": "1970460"
  },
  {
    "text": "of xi given z times p of z dz. ",
    "start": "1970460",
    "end": "1979420"
  },
  {
    "text": "Now there's a question of where\nthis qi will come into play.",
    "start": "1979420",
    "end": "1984700"
  },
  {
    "text": "And one thing that\nyou could do whenever you want to introduce new\ndistributions or new variables",
    "start": "1984700",
    "end": "1992850"
  },
  {
    "text": "into your equation is\nbasically take your expression and multiply it by--",
    "start": "1992850",
    "end": "2000529"
  },
  {
    "text": "sorry, qi of z\ndivided by qi of z.",
    "start": "2000530",
    "end": "2006838"
  },
  {
    "text": "And this is OK to do\nbecause this is always going to be equal to 1. And so we're just\nmultiplying the inside by 1.",
    "start": "2006838",
    "end": "2014330"
  },
  {
    "text": "And once we have this,\nwe can then formulate-- this as an expectation. So this is equal to the log\nof the expectation of qi--",
    "start": "2014330",
    "end": "2025910"
  },
  {
    "text": "of z sample from qi of\neverything on the inside. So we get p of xi given z times\np of z divided by qi of z.",
    "start": "2025910",
    "end": "2037310"
  },
  {
    "text": " And this is a much\nnicer form than what",
    "start": "2037310",
    "end": "2042680"
  },
  {
    "text": "we had before because we\nknow how to sample from qi.",
    "start": "2042680",
    "end": "2048500"
  },
  {
    "text": "qi is Gaussian distribution that\nwe're estimating right there.",
    "start": "2048500",
    "end": "2054090"
  },
  {
    "text": "And so we now can-- yeah, we now basically\nhave something",
    "start": "2054090",
    "end": "2059310"
  },
  {
    "text": "that we can sample\nfrom instead of trying to evaluate the integral. ",
    "start": "2059310",
    "end": "2065210"
  },
  {
    "text": "So all I've done here\nis-- this is just algebra up until this point.",
    "start": "2065210",
    "end": "2070419"
  },
  {
    "text": "We actually haven't made\nany approximations yet. And it's worth noting\nthat we can actually-- this kind of holds true\nbasically for any qi.",
    "start": "2070420",
    "end": "2078940"
  },
  {
    "text": "We aren't making any\nassumptions on how good qi is. ",
    "start": "2078940",
    "end": "2086169"
  },
  {
    "text": "Cool. And then from here,\nwhat we can do is--",
    "start": "2086170",
    "end": "2091300"
  },
  {
    "text": "it's annoying to have an\nexpectation inside of a log because we want the expectation\nto be on the outside",
    "start": "2091300",
    "end": "2096799"
  },
  {
    "text": "so that we can sample\nand estimate, sample mini batches rather than sampling\na ton of different z.",
    "start": "2096800",
    "end": "2102760"
  },
  {
    "text": "And this is where Jensen's\ninequality can come in, which we saw previously. And in this case,\nJensen's inequality",
    "start": "2102760",
    "end": "2109270"
  },
  {
    "text": "will actually work in our\nfavor rather than hurting us. So Jensen's inequality\nsays that log",
    "start": "2109270",
    "end": "2124030"
  },
  {
    "text": "of expectation of\nsome variable y is greater than or equal to the\nexpectation of the log of y.",
    "start": "2124030",
    "end": "2134500"
  },
  {
    "text": "I should also mention actually\nthat Jensen's inequality is actually-- it's actually a lot\nmore general than this.",
    "start": "2134500",
    "end": "2141040"
  },
  {
    "text": "It actually holds for\nany concave function. So log is an example of\none concave function, but you can plug-in any other\nconcave function other than log",
    "start": "2141040",
    "end": "2150700"
  },
  {
    "text": "and also show the same bound. Also if you have a convex\nfunction, it's also the same.",
    "start": "2150700",
    "end": "2156130"
  },
  {
    "text": "You just need to flip the\nsign of the inequality. And so with Jensen's\ninequality, we",
    "start": "2156130",
    "end": "2162850"
  },
  {
    "text": "can now formulate a lower\nbound on our objective",
    "start": "2162850",
    "end": "2168130"
  },
  {
    "text": "by basically just\nswapping the order of the expectation and the log. ",
    "start": "2168130",
    "end": "2176840"
  },
  {
    "text": "And because these\nare products, we can then start to actually\nwrite these out as a sum of logs",
    "start": "2176840",
    "end": "2185090"
  },
  {
    "text": "rather than a log\nof the product. So log of p of xi given\nz plus log of p of z",
    "start": "2185090",
    "end": "2196370"
  },
  {
    "text": "minus log of qi of z. ",
    "start": "2196370",
    "end": "2206510"
  },
  {
    "text": "And this is basically\nin our objective.",
    "start": "2206510",
    "end": "2212340"
  },
  {
    "text": "So this is something\nthat we can now optimize. So we'll sample a z\nfrom the distribution qi",
    "start": "2212340",
    "end": "2221600"
  },
  {
    "text": "for that data point and\nmaximize the likelihood of the data point of xi\nconditioned on that value of z.",
    "start": "2221600",
    "end": "2232400"
  },
  {
    "text": "We'll also optimize for log\nof p of z for this value.",
    "start": "2232400",
    "end": "2240930"
  },
  {
    "text": "And then lastly, we'll\nalso get this last term. And actually, this last\nterm with the expectation",
    "start": "2240930",
    "end": "2247730"
  },
  {
    "text": "corresponds to the\nentropy of q of z.",
    "start": "2247730",
    "end": "2253234"
  },
  {
    "text": " Now this whole objective is\ncalled the evidence lower bound",
    "start": "2253235",
    "end": "2265400"
  },
  {
    "text": "or the ELBO. ",
    "start": "2265400",
    "end": "2270470"
  },
  {
    "text": "And let's try to\ntalk a little bit through some of the intuition\nbehind this objective. So this is basically\njust writing out",
    "start": "2270470",
    "end": "2278910"
  },
  {
    "text": "what I wrote out on\nthe board and also kind",
    "start": "2278910",
    "end": "2285299"
  },
  {
    "text": "of writing it out in terms\nof with Jensen's inequality. So this is the final bound.",
    "start": "2285300",
    "end": "2292220"
  },
  {
    "text": "And once you maximize\nthis objective, that means that\nyou're also maximizing our original objective,\nwhich is the log likelihood.",
    "start": "2292220",
    "end": "2300243"
  },
  {
    "text": " So to talk a little bit\nabout the intuition,",
    "start": "2300243",
    "end": "2307539"
  },
  {
    "start": "2304000",
    "end": "2515000"
  },
  {
    "text": "I think it's helpful to first\ntalk about some quantities like entropy and KL divergence.",
    "start": "2307540",
    "end": "2313780"
  },
  {
    "text": "So entropy is\nsomething that we've kind of talked about before, but\nwe haven't formally gone over.",
    "start": "2313780",
    "end": "2321460"
  },
  {
    "text": "And it's defined as the\nnegative log probability",
    "start": "2321460",
    "end": "2327640"
  },
  {
    "text": "under basically the\nnegative expected log probability, which is equal\nto the negative integral of p",
    "start": "2327640",
    "end": "2332920"
  },
  {
    "text": "of x log p of x. And there's a couple\nof different intuitions behind what entropy\ncorresponds to.",
    "start": "2332920",
    "end": "2340650"
  },
  {
    "text": "The first is basically\nhow random a variable is.",
    "start": "2340650",
    "end": "2346119"
  },
  {
    "text": "And so for example, if\nyou think about a coin flip like a Bernoulli variable,\nif the weight of that coin",
    "start": "2346120",
    "end": "2354330"
  },
  {
    "text": "is 0.5, that means\nthat it's more random and the entropy measures that. Whereas if it's the kind of\nprobability of heads is 1,",
    "start": "2354330",
    "end": "2362560"
  },
  {
    "text": "then it has zero entropy because\nit doesn't have any randomness. So you can essentially\nthink of entropy as measuring how much randomness\ndoes this variable have.",
    "start": "2362560",
    "end": "2371940"
  },
  {
    "text": "And the other way\nto look at it is thinking about how large is the\nlog probability of the variable",
    "start": "2371940",
    "end": "2378090"
  },
  {
    "text": "in expectation under itself. And so if you have two different\ndistributions, in expectation,",
    "start": "2378090",
    "end": "2386150"
  },
  {
    "text": "this one has high\nlog probabilities. And this one has a lot\nof low log probabilities.",
    "start": "2386150",
    "end": "2393810"
  },
  {
    "text": "And if you have a small log-- sorry, this one has\nhigh probability. This one has low probabilities.",
    "start": "2393810",
    "end": "2399530"
  },
  {
    "text": "And something with\nlow probabilities will have-- the log of that\nwill be a large negative number.",
    "start": "2399530",
    "end": "2404690"
  },
  {
    "text": "Whereas the log of something\nwith a higher probability will be a small negative number.",
    "start": "2404690",
    "end": "2409800"
  },
  {
    "text": "And so the thing that has\nthe large negative numbers will have a large entropy.",
    "start": "2409800",
    "end": "2416150"
  },
  {
    "text": "And the one that has these\nhigh probabilities, which",
    "start": "2416150",
    "end": "2422240"
  },
  {
    "text": "correspond to the\nsmall log probabilities will have a low entropy. ",
    "start": "2422240",
    "end": "2431570"
  },
  {
    "text": "And so if we think\nabout the objective that we just talked\nabout right here,",
    "start": "2431570",
    "end": "2437220"
  },
  {
    "text": "which is the\nevidence lower bound, it has really two\nmain terms in it.",
    "start": "2437220",
    "end": "2443840"
  },
  {
    "text": "And the first\nterm-- so if we try to plot as a function\nof z, p of xi, z,",
    "start": "2443840",
    "end": "2449630"
  },
  {
    "text": "and we draw a sample\nfrom q, you can think of the first part or\nbasically the first two terms",
    "start": "2449630",
    "end": "2456650"
  },
  {
    "text": "here as saying, OK, for a given\nz, my probability of x comma z",
    "start": "2456650",
    "end": "2462890"
  },
  {
    "text": "should be high. And so that's going to basically\npick something like that.",
    "start": "2462890",
    "end": "2468320"
  },
  {
    "text": "It tries to pick a z that\nit has high p of x comma z.",
    "start": "2468320",
    "end": "2473477"
  },
  {
    "text": "And then what the\nsecond term is going to try to do is it's\ngoing to try to maximize the entropy of q of z.",
    "start": "2473477",
    "end": "2480100"
  },
  {
    "text": "And so it's going to try to\nbasically make the distribution as wide as possible.",
    "start": "2480100",
    "end": "2485390"
  },
  {
    "text": "And so overall what this\nobjective is going to do is it has really these\ntwo parts one that's maximizing probability\nand one that's",
    "start": "2485390",
    "end": "2491110"
  },
  {
    "text": "trying to make that distribution\nas wide as possible, such that you're matching the\nunderlying distribution.",
    "start": "2491110",
    "end": "2498070"
  },
  {
    "start": "2498070",
    "end": "2503740"
  },
  {
    "text": "Any questions on entropy\nor this intuition? ",
    "start": "2503740",
    "end": "2516180"
  },
  {
    "start": "2515000",
    "end": "2638000"
  },
  {
    "text": "Cool. So the second thing\nthat we can look at is what's called\nthe KL divergence.",
    "start": "2516180",
    "end": "2523090"
  },
  {
    "text": "And this is also something that\nI think we've seen briefly. And the KL divergence\nbetween the two distributions",
    "start": "2523090",
    "end": "2530140"
  },
  {
    "text": "is defined as the\nexpectation under one of those distributions\nof the log of the ratio of the first\ndistribution divided",
    "start": "2530140",
    "end": "2536950"
  },
  {
    "text": "by the second distribution. And you can kind of\nequivalently write this out",
    "start": "2536950",
    "end": "2544210"
  },
  {
    "text": "as the negative probability\nof one distribution under the other\nminus the entropy",
    "start": "2544210",
    "end": "2550390"
  },
  {
    "text": "of that other distribution. And again, there's\ntwo different ways to intuitively look\nat the KL divergence.",
    "start": "2550390",
    "end": "2557180"
  },
  {
    "text": "The first is thinking\nabout how different are these two distributions,\nhow different is q from p.",
    "start": "2557180",
    "end": "2563510"
  },
  {
    "text": "And you can see that, for\nexample, when q equals p, the KL divergence will be 0. Because when q equals p, that\nratio on the left will be 1,",
    "start": "2563510",
    "end": "2572000"
  },
  {
    "text": "and then the log of 1 is 0. And so this will be 0.",
    "start": "2572000",
    "end": "2577160"
  },
  {
    "text": "And as they become\nmore and more different the KL divergence will grow. ",
    "start": "2577160",
    "end": "2584130"
  },
  {
    "text": "And then the second way that\nwe can look at it, especially looking at this\nequation right here is basically how small is\nthe expected log probability",
    "start": "2584130",
    "end": "2594119"
  },
  {
    "text": "of one distribution under\nthe other distribution minus the entropy.",
    "start": "2594120",
    "end": "2600810"
  },
  {
    "text": "And so this ends up giving us\na similar picture to before, where if we think about the\nprobability of one distribution",
    "start": "2600810",
    "end": "2609060"
  },
  {
    "text": "under the other, that will\nbasically give us the kind of-- ",
    "start": "2609060",
    "end": "2616680"
  },
  {
    "text": "maximizing that will\nmaximize or minimizing that will maximize this first part. And then the entry people\nwill also try to maximize it.",
    "start": "2616680",
    "end": "2625000"
  },
  {
    "text": "And so if you want to try to\nminimize the KL divergence, this is going to try to\nmake the distributions as",
    "start": "2625000",
    "end": "2630592"
  },
  {
    "text": "similar as possible. ",
    "start": "2630592",
    "end": "2639880"
  },
  {
    "text": "So up until this\npoint, we've kind of derived an objective\nfor optimizing",
    "start": "2639880",
    "end": "2648150"
  },
  {
    "text": "these latent variable models. And the way that we\ndid that is we still",
    "start": "2648150",
    "end": "2654960"
  },
  {
    "text": "have our p of z\nterm, which is just can be like a\nGaussian distribution like a standard\nnormal distribution.",
    "start": "2654960",
    "end": "2660780"
  },
  {
    "text": "And we have our neural network\nthat's kind of mapping from a z to an xi. And then we additionally\nintroduced this third thing",
    "start": "2660780",
    "end": "2670350"
  },
  {
    "text": "which is this kind of\ndistribution that's trying to approximate the\nposterior that is trying",
    "start": "2670350",
    "end": "2677640"
  },
  {
    "text": "to approximate p of z given xi. ",
    "start": "2677640",
    "end": "2684640"
  },
  {
    "text": "And so then the next question\nis, we bounded this objective,",
    "start": "2684640",
    "end": "2691589"
  },
  {
    "text": "is this bound actually tight? And in which cases, is\nthis bound actually tight? ",
    "start": "2691590",
    "end": "2699620"
  },
  {
    "text": "And part of this relates to the\nquestion of the choice of qi",
    "start": "2699620",
    "end": "2705150"
  },
  {
    "text": "and what makes it qi. And really the\nintuition is that we want qi to be as close as\npossible to p of z given xi.",
    "start": "2705150",
    "end": "2714030"
  },
  {
    "text": "We want to basically\nbe able to tell us the distribution over the latent\nvariable for a given example.",
    "start": "2714030",
    "end": "2720435"
  },
  {
    "text": " Yeah?",
    "start": "2720435",
    "end": "2725890"
  },
  {
    "text": "Could you explain more\nabout the [INAUDIBLE] because like where the\nequality would come from?",
    "start": "2725890",
    "end": "2731619"
  },
  {
    "text": " You're asking, can I tell\nyou when would qi not",
    "start": "2731620",
    "end": "2738880"
  },
  {
    "text": "be equal to this? [INAUDIBLE] equality\nwas identity, like where",
    "start": "2738880",
    "end": "2746880"
  },
  {
    "text": "in when you go from the equality\nto use like Jensen's inequality",
    "start": "2746880",
    "end": "2754484"
  },
  {
    "text": "to get to like the third line. Where is equality-- why\nwould it be equal, I guess?",
    "start": "2754485",
    "end": "2760410"
  },
  {
    "text": "Because it'd be [INAUDIBLE]. Yeah. So you're a little\nbit ahead of me. So basically, if qi is\nequal to p of z given xi,",
    "start": "2760410",
    "end": "2768390"
  },
  {
    "text": "it turns out the\nbound will be tight. And in particular, if\ntheir KL divergence",
    "start": "2768390",
    "end": "2777460"
  },
  {
    "text": "is minimized, if the\nKL divergence is zero, then the bound will be tight. And there is-- I guess,\nthere's a question",
    "start": "2777460",
    "end": "2784372"
  },
  {
    "text": "of-- do I want to go on\nthis on the whiteboard or on the slides? ",
    "start": "2784373",
    "end": "2790350"
  },
  {
    "text": "I feel like it'd be\njust a little bit faster to go over on the\nslides and that, it might be nice to do that.",
    "start": "2790350",
    "end": "2795940"
  },
  {
    "text": "So to answer your\nquestion, we can basically",
    "start": "2795940",
    "end": "2801150"
  },
  {
    "text": "look at the KL divergence\nbetween qi and p of z given xi and derive what\nthis actually corresponds to.",
    "start": "2801150",
    "end": "2809550"
  },
  {
    "text": "And we can do this by just\nfirst writing out the equation for KL divergence, which\nis the expectation of q,",
    "start": "2809550",
    "end": "2817920"
  },
  {
    "text": "of log q over p. And with Bayes' rule--",
    "start": "2817920",
    "end": "2823980"
  },
  {
    "text": "or if we multiply the top\nand bottom by p of xi, we get an expression\nthat looks like this.",
    "start": "2823980",
    "end": "2829110"
  },
  {
    "text": " And then if we\nkind of expand out",
    "start": "2829110",
    "end": "2835214"
  },
  {
    "text": "the log of the product\nto the sum of logs, we get the negative log\nof the bottom, which",
    "start": "2835215",
    "end": "2843960"
  },
  {
    "text": "is p of xi given z and p of z\nplus the log of the top, which",
    "start": "2843960",
    "end": "2850080"
  },
  {
    "text": "is the log of qi of z\nand the log of p of x.",
    "start": "2850080",
    "end": "2857620"
  },
  {
    "text": "And this starts to\nlook pretty familiar. So the first three--",
    "start": "2857620",
    "end": "2863860"
  },
  {
    "text": "or basically, this is equal\nto the entropy and everything",
    "start": "2863860",
    "end": "2870190"
  },
  {
    "text": "before this last part is exactly\nequal to the evidence lower bound.",
    "start": "2870190",
    "end": "2877220"
  },
  {
    "text": "And so what this\nmeans is that if you",
    "start": "2877220",
    "end": "2883859"
  },
  {
    "text": "set the KL divergence\nequal to zero, if we set qi of z equal\nto p of z given xi,",
    "start": "2883860",
    "end": "2891030"
  },
  {
    "text": "then Li is equal to\nthe log probability. ",
    "start": "2891030",
    "end": "2898470"
  },
  {
    "text": "And so that means that\nbasically the bound is tight-- the loss function or\nthe evidence lower bound",
    "start": "2898470",
    "end": "2904790"
  },
  {
    "text": "is exactly equal to\nthe log likelihood when the KL divergence between\nqi of z and p of z given xi",
    "start": "2904790",
    "end": "2911480"
  },
  {
    "text": "is zero. ",
    "start": "2911480",
    "end": "2917840"
  },
  {
    "text": "Cool. And also because the\nKL divergence is always non-negative, this also\nkind of is another way",
    "start": "2917840",
    "end": "2924260"
  },
  {
    "text": "to derive the bound on p of xi. ",
    "start": "2924260",
    "end": "2931270"
  },
  {
    "text": "And so this bound\nessentially, instead of using Jensen's inequality,\nit's just relying on the fact",
    "start": "2931270",
    "end": "2936310"
  },
  {
    "text": "that the KL divergences\nis non-negative. ",
    "start": "2936310",
    "end": "2945040"
  },
  {
    "text": "Cool. So if the KL divergence\nis zero, then that means our evidence\nlower bound is tight, and that means that we are--",
    "start": "2945040",
    "end": "2951090"
  },
  {
    "text": "if we basically exactly\nrepresent p of z given xi, then we're going to be exactly\nmaximizing the likelihood.",
    "start": "2951090",
    "end": "2957119"
  },
  {
    "text": " The second thing to\nnote here is that--",
    "start": "2957120",
    "end": "2964075"
  },
  {
    "text": "so far we've talked\nabout how we're going to be optimizing\nthis neural network. The other question is,\nhow do we optimize for qi?",
    "start": "2964075",
    "end": "2971140"
  },
  {
    "text": "And if we optimize\nfor qi with respect to the same lower bound,\nthen what that's going to do",
    "start": "2971140",
    "end": "2980740"
  },
  {
    "text": "is that's going to be\nminimizing the KL divergence.",
    "start": "2980740",
    "end": "2986683"
  },
  {
    "text": "I guess I can write\nthis out right here. So we saw that the KL\ndivergence between qi of z",
    "start": "2986683",
    "end": "2999830"
  },
  {
    "text": "and p of z given xi is\nequal to our last function",
    "start": "2999830",
    "end": "3007030"
  },
  {
    "text": "plus log p of xi. ",
    "start": "3007030",
    "end": "3016320"
  },
  {
    "text": "Actually, sorry, negative. Yeah. And so basically, if we decide\nto maximize Li with respect",
    "start": "3016320",
    "end": "3030880"
  },
  {
    "text": "to qi, this doesn't\ndepend on qi at all.",
    "start": "3030880",
    "end": "3038730"
  },
  {
    "text": "And so that means\nthat if we maximize L,",
    "start": "3038730",
    "end": "3044310"
  },
  {
    "text": "this corresponds to\nminimizing this left hand side, which means that we're\nminimizing the KL divergence.",
    "start": "3044310",
    "end": "3049589"
  },
  {
    "text": "And that's a really good\nthing because if we're minimizing the divergence\nwith respect to q,",
    "start": "3049590",
    "end": "3054720"
  },
  {
    "text": "that means that we're\nmaking the bound tighter. And so this really justifies\noptimizing this evidence lower",
    "start": "3054720",
    "end": "3062775"
  },
  {
    "text": "bound both with respect to\ntheta as well as with respect to q because it means that we're\ngoing to be optimizing a bound",
    "start": "3062775",
    "end": "3068730"
  },
  {
    "text": "and we're going to be making\nthat bound tighter as well. ",
    "start": "3068730",
    "end": "3079070"
  },
  {
    "text": "Cool. And so what are\noptimisation objective will look like is basically, we\nhave this single objective Li",
    "start": "3079070",
    "end": "3086780"
  },
  {
    "text": "and we're going to maximize\nit both with respect to our model parameters\ntheta and with respect to what's called the\nvariational distribution qi.",
    "start": "3086780",
    "end": "3094335"
  },
  {
    "start": "3094335",
    "end": "3101510"
  },
  {
    "text": "All right. So then there's a\nquestion of actually kind of walking through what\nthe algorithm looks like.",
    "start": "3101510",
    "end": "3107620"
  },
  {
    "text": "So once we have this\nobjective, the first step will be to sample an example\nxi or a mini batch of examples.",
    "start": "3107620",
    "end": "3118550"
  },
  {
    "text": "Then we want to compute\nthe gradient with respect to theta of Li.",
    "start": "3118550",
    "end": "3127490"
  },
  {
    "text": "And the way that\nwe'll do-- actually, does anyone want\nto actually say how",
    "start": "3127490",
    "end": "3134500"
  },
  {
    "text": "do we compute the gradient of\nthis with respect to theta,",
    "start": "3134500",
    "end": "3143000"
  },
  {
    "text": "or what the first step is? ",
    "start": "3143000",
    "end": "3148660"
  },
  {
    "text": "So you can see theta\nappears just right here, and so that's the only\nplace where theta appears.",
    "start": "3148660",
    "end": "3156690"
  },
  {
    "text": "Yeah? I think we take\nthe gradient inside because the function\nwill behave positive.",
    "start": "3156690",
    "end": "3161810"
  },
  {
    "text": "So if we just take the\nindividual p of x given z, which we already\nknow in the formula,",
    "start": "3161810",
    "end": "3168300"
  },
  {
    "text": "it's like the normal thing. So you're saying that we could\ntake the gradient inside. So we sampled an xi.",
    "start": "3168300",
    "end": "3176120"
  },
  {
    "text": "But to evaluate the\ngradient of this, we need both an xi and a z.",
    "start": "3176120",
    "end": "3181160"
  },
  {
    "text": "So where do we get the z from? I guess we can make a list\nof probabilities for z",
    "start": "3181160",
    "end": "3187040"
  },
  {
    "text": "based on the values\nthat for next cycle to most possible-- the\nmost likely z given",
    "start": "3187040",
    "end": "3193609"
  },
  {
    "text": "x because your sampled\nx [INAUDIBLE] of the z's most likely or they could\nsolve probably for [INAUDIBLE]..",
    "start": "3193610",
    "end": "3200060"
  },
  {
    "text": "Yeah. So what we'll do is-- I'm not fully sure if this\nis what you're suggesting,",
    "start": "3200060",
    "end": "3205700"
  },
  {
    "text": "but what we can do is-- we have qi. And so we can sample a z\nor multiple z's from qi.",
    "start": "3205700",
    "end": "3215830"
  },
  {
    "text": "And then once we have those z's,\nthen we can use that to kind of pass that into here and\ncompute the gradient of log p",
    "start": "3215830",
    "end": "3225490"
  },
  {
    "text": "theta using those z's. And so yeah, we'll basically\nfirst sample z from qi.",
    "start": "3225490",
    "end": "3239630"
  },
  {
    "text": " And then once we have the\nz, we can then basically",
    "start": "3239630",
    "end": "3247670"
  },
  {
    "text": "kind of compute the gradient of\nlog p theta of xi given that z.",
    "start": "3247670",
    "end": "3258390"
  },
  {
    "text": "In practice-- well, I'm only\nwriting a single sample here.",
    "start": "3258390",
    "end": "3263640"
  },
  {
    "text": "Typically, if you want to get\nan extremely accurate gradient, then you would sample\nmultiple z's from here.",
    "start": "3263640",
    "end": "3269980"
  },
  {
    "text": "But in practice, actually,\njust sampling one often works just fine. ",
    "start": "3269980",
    "end": "3278130"
  },
  {
    "text": "Cool. And I feel like I should-- No. Don't worry about it.",
    "start": "3278130",
    "end": "3284490"
  },
  {
    "text": "It's also pretty far. So I'm not sure how\nwell I can throw. Yeah? Using this one z, does they're\nsort of implicitly assume",
    "start": "3284490",
    "end": "3292770"
  },
  {
    "text": "that our data is well behaved? Does it implicitly assume\nthat our data is what, sorry?",
    "start": "3292770",
    "end": "3297869"
  },
  {
    "text": "Well behaved. Otherwise, it would\nbe very noisy.",
    "start": "3297870",
    "end": "3303030"
  },
  {
    "text": "So the question is, by\nonly sampling one z, does that implicitly assume\nthat our data is well behaved?",
    "start": "3303030",
    "end": "3308714"
  },
  {
    "text": " I'm not sure what\nwell behaved means.",
    "start": "3308715",
    "end": "3314039"
  },
  {
    "text": " When the expectation would\neventually correct itself.",
    "start": "3314040",
    "end": "3320535"
  },
  {
    "text": " Right. So I think that one of the\nreasons why it works well",
    "start": "3320535",
    "end": "3328859"
  },
  {
    "text": "is we're modeling qi as\na Gaussian distribution. And Gaussian distributions are\nfairly simple and unimodal.",
    "start": "3328860",
    "end": "3338390"
  },
  {
    "text": "And so I think that\nfor that reason-- because Gaussian distributions\nare well behaved perhaps,",
    "start": "3338390",
    "end": "3345510"
  },
  {
    "text": "using just one sample is OK.  Yeah.",
    "start": "3345510",
    "end": "3350790"
  },
  {
    "text": "I'm not sure if I have a\nbetter answer than that. Let's say we have beta\ndistribution instead",
    "start": "3350790",
    "end": "3355870"
  },
  {
    "text": "of a Gaussian,\nwould it still work?",
    "start": "3355870",
    "end": "3360940"
  },
  {
    "text": "If we had a beta distribution\nrather than a Gaussian. Well, I guess, we\nhaven't yet gotten",
    "start": "3360940",
    "end": "3368710"
  },
  {
    "text": "to the training the qi part. But it'd be harder to\ntrain a beta distribution.",
    "start": "3368710",
    "end": "3376750"
  },
  {
    "text": "You can measure the entropy\nof a Gaussian distribution in closed form with respect to\nthe mean and variance of that",
    "start": "3376750",
    "end": "3382210"
  },
  {
    "text": "so that becomes a lot easier,\nspecifically with respect to this step, I'm not sure.",
    "start": "3382210",
    "end": "3388430"
  },
  {
    "text": "I've never tried it. In part because of\nthe complexity there. But I could see\nit being possibly",
    "start": "3388430",
    "end": "3394863"
  },
  {
    "text": "needing more samples\nin that case.  Yeah?",
    "start": "3394863",
    "end": "3400285"
  },
  {
    "text": "Do we have one qi\nper xi or do we have multiple choice for each?",
    "start": "3400285",
    "end": "3406130"
  },
  {
    "text": "Yeah, good question. So far, we have one\nqi for every xi.",
    "start": "3406130",
    "end": "3411530"
  },
  {
    "text": "Then when [INAUDIBLE] qi,\nzu, and x, or something?",
    "start": "3411530",
    "end": "3416980"
  },
  {
    "text": "We will do that soon. It's a great idea. Was there a question?",
    "start": "3416980",
    "end": "3423430"
  },
  {
    "text": "After selection,\nsampling just once really works well when training. For prediction, do we\nstill sample multiple z's?",
    "start": "3423430",
    "end": "3431119"
  },
  {
    "text": "Right. So the question\nis, for prediction, do we still sample multiple z's? So it depends on\nhow you're using it.",
    "start": "3431120",
    "end": "3440682"
  },
  {
    "text": "If you're just generating\nfrom the model, you're actually not going to\nbe sampling from q at all. You'll be sampling\nfrom p of z, and then",
    "start": "3440682",
    "end": "3447190"
  },
  {
    "text": "passing that through\nyour neural network. If you do want to get\na really good like--",
    "start": "3447190",
    "end": "3453708"
  },
  {
    "text": "if you do want to get a better\nestimate, one thing you can do is sample and then take--",
    "start": "3453708",
    "end": "3459676"
  },
  {
    "text": "basically, sample and then take\nthe max or something like that and sample multiple times. So there are things that\nyou can do to sample more",
    "start": "3459676",
    "end": "3467020"
  },
  {
    "text": "than once at inference time. It kind of depends on how\nyou're using it, though. In the case of\nvariational autoencoders,",
    "start": "3467020",
    "end": "3472480"
  },
  {
    "text": "you actually very rarely\nuse q at test time. You often just throw\nit away at test time,",
    "start": "3472480",
    "end": "3477770"
  },
  {
    "text": "and only just use it\nto generate samples. But there are other cases\nwhere you may actually be using q at test time.",
    "start": "3477770",
    "end": "3483160"
  },
  {
    "text": " Cool.",
    "start": "3483160",
    "end": "3488660"
  },
  {
    "text": "So we got through\nstep two, which is how to compute the gradient.",
    "start": "3488660",
    "end": "3494730"
  },
  {
    "text": "And then the last thing\nthat we want to do is to update qi with respect\nto the loss function.",
    "start": "3494730",
    "end": "3508240"
  },
  {
    "text": "And the way that\nwe'll do this is, if qi is a Gaussian\ndistribution,",
    "start": "3508240",
    "end": "3513640"
  },
  {
    "text": "then we can set\nbasically qi to be a Gaussian distribution\nwith a mean of mu i",
    "start": "3513640",
    "end": "3520860"
  },
  {
    "text": "and a variance of sigma i. And then basically compute the\ngradient of your loss function",
    "start": "3520860",
    "end": "3531240"
  },
  {
    "text": "with respect to mu i,\nand also the gradient with respect to sigma i.",
    "start": "3531240",
    "end": "3536820"
  },
  {
    "text": "And then use that to\nupdate mu i and sigma i. ",
    "start": "3536820",
    "end": "3545460"
  },
  {
    "text": "And so this is basically just\nwhat's written on the slide. So we estimate the gradient.",
    "start": "3545460",
    "end": "3552630"
  },
  {
    "text": "It's an estimate if you\nonly take one sample. Although even if you\ntake a few samples,",
    "start": "3552630",
    "end": "3557640"
  },
  {
    "text": "it's still an\nestimate unless you take an infinite number\nof samples, and then we'll update qi.",
    "start": "3557640",
    "end": "3563340"
  },
  {
    "text": "And the way that\nwe do that is we'll evaluate the\ngradient of the mean and the gradient\nof the variance. ",
    "start": "3563340",
    "end": "3572520"
  },
  {
    "text": "Cool. So this already sort of came up. But there's a little\nbit of a problem",
    "start": "3572520",
    "end": "3577920"
  },
  {
    "text": "here, which is that\ntypically in deep learning, we have a large\nnumber of samples.",
    "start": "3577920",
    "end": "3585099"
  },
  {
    "text": "And that means that it may be\nsomewhat impractical to fit a qi for every single example.",
    "start": "3585100",
    "end": "3594390"
  },
  {
    "text": "And so one question is, when\nwe have this kind of model, what is the total\nnumber of parameters?",
    "start": "3594390",
    "end": "3603430"
  },
  {
    "text": "Yeah? So first you have\nall the thetas, and then you also have a\nset of big and ready sets",
    "start": "3603430",
    "end": "3611080"
  },
  {
    "text": "of the means, variants\nof mu i and sigma i?",
    "start": "3611080",
    "end": "3616150"
  },
  {
    "text": "Yeah, exactly. So we will have\nall of our theta, and we'll have one of those.",
    "start": "3616150",
    "end": "3621200"
  },
  {
    "text": "And then for the number\nof examples that we have, we'll have 2 times the\ndimensionality of-- or well,",
    "start": "3621200",
    "end": "3628869"
  },
  {
    "text": "n times the dimensionality\nof mu and n times the dimensionality of sigma.",
    "start": "3628870",
    "end": "3634950"
  },
  {
    "text": "And so this is a pretty\nlarge number of parameters. So to solve this\nproblem, what we can do,",
    "start": "3634950",
    "end": "3641748"
  },
  {
    "text": "which was actually\nalready suggested, which is that instead\nof having a single qi for every single data point,\nwe can train a neural network",
    "start": "3641748",
    "end": "3649160"
  },
  {
    "text": "to predict the z given xi. And so instead of only\nhaving a neural network that",
    "start": "3649160",
    "end": "3657010"
  },
  {
    "text": "predicts x given z, we're going\nto have a second network, which is often called an inference\nnetwork, to give you",
    "start": "3657010",
    "end": "3664720"
  },
  {
    "text": "a distribution over z given x. ",
    "start": "3664720",
    "end": "3670680"
  },
  {
    "text": "And this neural\nnetwork will give-- it'll take us input x, and it'll\noutput a mean and a variance.",
    "start": "3670680",
    "end": "3678539"
  },
  {
    "text": "And so q will again be\na Gaussian distribution. But it'll be a conditional\nGaussian distribution.",
    "start": "3678540",
    "end": "3683820"
  },
  {
    "text": "And this neural\nnetwork is outputting the mean and the variance, and\nthen the normal distribution is parameterized by that\nmean in that variance.",
    "start": "3683820",
    "end": "3690750"
  },
  {
    "text": " Cool. So this is where we get to\namortize variational inference.",
    "start": "3690750",
    "end": "3697630"
  },
  {
    "text": "And the reason why it's called\namortize variational inference is that we're\nessentially amortizing the inference process\nusing this neural network",
    "start": "3697630",
    "end": "3706780"
  },
  {
    "text": "rather than just doing inference\nfor every single data point individually. ",
    "start": "3706780",
    "end": "3715280"
  },
  {
    "text": "And so the way this\nworks is we again will formulate the\nsame exact objective,",
    "start": "3715280",
    "end": "3721990"
  },
  {
    "text": "except instead of being qi\nof z, we'll just replace that with qi phi of z given xi.",
    "start": "3721990",
    "end": "3728079"
  },
  {
    "text": " And this works\nout with the bound because we mentioned\nthat this works really",
    "start": "3728080",
    "end": "3735860"
  },
  {
    "text": "for any value of q. And so it can-- q can be conditioned on xi.",
    "start": "3735860",
    "end": "3741287"
  },
  {
    "text": "It can really be\nconditioned on whatever you want it to be conditioned on. And so it's natural for\nit to be conditioned on xi",
    "start": "3741287",
    "end": "3748460"
  },
  {
    "text": "because we want it to be very\nsimilar to p of z given xi. ",
    "start": "3748460",
    "end": "3757230"
  },
  {
    "text": "Cool. And then the algorithm ends\nup looking the same as before, except now instead of sampling\nfrom qi or sorry, qi over z,",
    "start": "3757230",
    "end": "3770900"
  },
  {
    "text": "we're now going to\nbe sampling over this conditional distribution\nthat's conditioned on xi. ",
    "start": "3770900",
    "end": "3778130"
  },
  {
    "text": "And then instead of updating\neach of the individual qi's, we're going to be\nupdating the parameters",
    "start": "3778130",
    "end": "3784520"
  },
  {
    "text": "of our inference network,\nwhich I'm using phi to denote. Cool. ",
    "start": "3784520",
    "end": "3793170"
  },
  {
    "text": "So that's all\nwritten right here. And we can take a gradient\ndescent step on phi.",
    "start": "3793170",
    "end": "3798630"
  },
  {
    "text": "Now there's one more\nproblem that we have to fix, which is actually,\nhow do we end up",
    "start": "3798630",
    "end": "3804630"
  },
  {
    "text": "calculating this\ngradient right here? And the reason why\nthis is a problem",
    "start": "3804630",
    "end": "3812220"
  },
  {
    "text": "is that if we look\nat our objective, there is a few different\nplaces where phi will come up.",
    "start": "3812220",
    "end": "3820450"
  },
  {
    "text": "So phi will come up right\nhere and phi will also come up right here.",
    "start": "3820450",
    "end": "3827160"
  },
  {
    "text": "And this part isn't\ntoo hard to calculate. You can look up-- there's\na closed form equation",
    "start": "3827160",
    "end": "3833760"
  },
  {
    "text": "of the entropy of a Gaussian\nthat you can look up. But this one ends up being\na little bit more difficult",
    "start": "3833760",
    "end": "3840500"
  },
  {
    "text": "because here you actually\nhave to differentiate through the sampling\nprocess into phi. ",
    "start": "3840500",
    "end": "3848990"
  },
  {
    "text": "So this part is easy. The second part is\na little bit harder.",
    "start": "3848990",
    "end": "3854430"
  },
  {
    "text": "And in terms of going\ninto this, we'll",
    "start": "3854430",
    "end": "3859960"
  },
  {
    "text": "refer to this inside part. It doesn't really matter\nwhat this inside part is. We'll refer to it\nas r of xi given z.",
    "start": "3859960",
    "end": "3868900"
  },
  {
    "text": "And then our goal is\nto be able to compute the gradient with respect to\nphi of this expectation of q",
    "start": "3868900",
    "end": "3877990"
  },
  {
    "text": "phi of r of xi comma z. So this is our goal.",
    "start": "3877990",
    "end": "3883580"
  },
  {
    "text": "This is what we want\nto be able to compute.  And q is going to be outputting\na Gaussian distribution.",
    "start": "3883580",
    "end": "3893440"
  },
  {
    "text": "Now this is where what's\ncalled the reparameterization trick comes in.",
    "start": "3893440",
    "end": "3900200"
  },
  {
    "text": "And so in particular, q\nof phi of z given x, this",
    "start": "3900200",
    "end": "3909780"
  },
  {
    "text": "is defined to be a\nGaussian distribution of some neural\nnetwork that takes",
    "start": "3909780",
    "end": "3915150"
  },
  {
    "text": "as input x and the variance\nthat takes input x.",
    "start": "3915150",
    "end": "3921539"
  },
  {
    "text": "And here we're going to be\nsampling in this expectation",
    "start": "3921540",
    "end": "3930480"
  },
  {
    "text": "a z from this, our\nq distribution.",
    "start": "3930480",
    "end": "3940170"
  },
  {
    "text": "And we want to be\nable to differentiate through the sampling process. ",
    "start": "3940170",
    "end": "3945890"
  },
  {
    "text": "And the really cool\nthing about what's called the reparameterization\ntrick is we can rewrite this equation,\nthe sampling process",
    "start": "3945890",
    "end": "3955310"
  },
  {
    "text": "as the equation mu of x plus\nepsilon times sigma of x",
    "start": "3955310",
    "end": "3964380"
  },
  {
    "text": "where epsilon is a Gaussian--",
    "start": "3964380",
    "end": "3970440"
  },
  {
    "text": "epsilon is a variable that's-- epsilon is value\nthat is sampled from a standard normal distribution.",
    "start": "3970440",
    "end": "3977170"
  },
  {
    "text": "And so this equation should\nbe fairly straightforward. So we're just saying that\na sample from a Gaussian distribution equals the\nmean plus some noise",
    "start": "3977170",
    "end": "3984840"
  },
  {
    "text": "times the variance. And the really cool\nthing about this equation is that epsilon is completely\nindependent of phi.",
    "start": "3984840",
    "end": "3997390"
  },
  {
    "text": "Whereas the mu and sigma\nare parameterized by phi. And that means that\nwe can basically",
    "start": "3997390",
    "end": "4004200"
  },
  {
    "text": "generate a sample\nthis way, where we generate a\nsample from epsilon and then plug it\ninto this equation.",
    "start": "4004200",
    "end": "4010290"
  },
  {
    "text": "And then that gives us a sample. And we can differentiate\nthrough this equation into phi in order to update phi\nthrough the sampling process.",
    "start": "4010290",
    "end": "4019275"
  },
  {
    "text": " And so more concretely,\nthis is the equation",
    "start": "4019275",
    "end": "4027890"
  },
  {
    "text": "that I wrote up on the board,\nz equals mu plus epsilon times sigma. We can first sample sigma--",
    "start": "4027890",
    "end": "4034250"
  },
  {
    "text": "or sorry, we can\nfirst sample epsilon in a way that's completely\nindependent of the inference",
    "start": "4034250",
    "end": "4039470"
  },
  {
    "text": "that we're parameters,\nmultiply that by sigma, and add in the mean.",
    "start": "4039470",
    "end": "4045370"
  },
  {
    "text": "And then in order to compute\nthe gradient of this expression,",
    "start": "4045370",
    "end": "4050990"
  },
  {
    "text": "now we no longer actually\nhave to sample from q. We can instead sample from the\nstandard normal distribution.",
    "start": "4050990",
    "end": "4058069"
  },
  {
    "text": "And this equation is much\neasier to differentiate through in comparison to\nthe first equation.",
    "start": "4058070",
    "end": "4063790"
  },
  {
    "text": " And so basically to estimate\nthe gradient of phi,",
    "start": "4063790",
    "end": "4073010"
  },
  {
    "text": "we first sample a\nbunch of samples from the standard\nnormal distribution. A single sample actually also\ncan work well in this case.",
    "start": "4073010",
    "end": "4081140"
  },
  {
    "text": "And then the gradient\nwill be approximately equal to the\ngradient of phi of r",
    "start": "4081140",
    "end": "4092360"
  },
  {
    "text": "where we plug-in this equation\nfor our sample, for z.",
    "start": "4092360",
    "end": "4100710"
  },
  {
    "text": "Cool. Any questions on how that works? I should mention,\nit's really important",
    "start": "4100710",
    "end": "4106278"
  },
  {
    "text": "that this is independent of phi. Yeah? The epsilon is actually\none, always of identity",
    "start": "4106279",
    "end": "4113855"
  },
  {
    "text": "by information that's on there? Right. So if this is vector value,\nthen this will be identity.",
    "start": "4113855",
    "end": "4120799"
  },
  {
    "text": " Yeah? The solution to the amortization\nthat we're doing really--",
    "start": "4120800",
    "end": "4130460"
  },
  {
    "text": "usually, when someone\nlose something where you can't get over\nsomething which is really good,",
    "start": "4130460",
    "end": "4137240"
  },
  {
    "text": "what's a good thing that\nwe're doing in getting over? Do we have to use\nany parameters? Yeah.",
    "start": "4137240",
    "end": "4142430"
  },
  {
    "text": "So the question is,\nare we losing anything by doing amortization?",
    "start": "4142430",
    "end": "4148818"
  },
  {
    "text": " It's a good question. ",
    "start": "4148819",
    "end": "4157080"
  },
  {
    "text": "I think that the only thing\nthat we may be really losing in this case is that you\nessentially have something",
    "start": "4157080",
    "end": "4165960"
  },
  {
    "text": "that may be slightly-- in practice, I don't think\nthis is really the case. But when you actually\nseparately represent each qi,",
    "start": "4165960",
    "end": "4173970"
  },
  {
    "text": "you can really represent\nlike just a Gaussian for all of them. And you have a lot\nof expressive power.",
    "start": "4173970",
    "end": "4180256"
  },
  {
    "text": "Whereas when you have a\nsingle neural network that has to output, a\ndifferent z for a given x,",
    "start": "4180257",
    "end": "4188099"
  },
  {
    "text": "it has to be able to look at\nx and then tell you what z is. And it may be somewhat\ndifficult to do",
    "start": "4188100",
    "end": "4193710"
  },
  {
    "text": "that with a single function. It may also be that xi-- there are two xi's that\nlook very different, but actually have very\ndifferent latent variables.",
    "start": "4193710",
    "end": "4200683"
  },
  {
    "text": "And in cases like\nthat, it may be especially difficult for the\ninference network to do that.",
    "start": "4200683",
    "end": "4209640"
  },
  {
    "text": "So yeah. But really just being able to\nrepresent q of z given x like,",
    "start": "4209640",
    "end": "4216557"
  },
  {
    "text": "you have to be able\nto represent that. But I think that's really the\nonly thing that you're losing. ",
    "start": "4216557",
    "end": "4226625"
  },
  {
    "text": "I guess I could also\nmention that there are some works that\nkind of amortize and then run some additional\ngradient steps on z.",
    "start": "4226625",
    "end": "4233920"
  },
  {
    "text": "And there are kind of\nmore hybrid methods as well that you can consider.",
    "start": "4233920",
    "end": "4239215"
  },
  {
    "text": " Cool. So for the\nreparameterization trick,",
    "start": "4239215",
    "end": "4245760"
  },
  {
    "text": "it's really like super\neasy to implement. And it also gives you a\npretty low variance estimate",
    "start": "4245760",
    "end": "4252900"
  },
  {
    "text": "of your gradient. It only works for continuous\nlatent variables one,",
    "start": "4252900",
    "end": "4259199"
  },
  {
    "text": "specifically in this case,\nGaussian random variables. If you are really excited\nabout having discrete latent",
    "start": "4259200",
    "end": "4265470"
  },
  {
    "text": "variables, then\nthere are techniques for optimizing for those. So there's something called\nthe straight-through estimator",
    "start": "4265470",
    "end": "4271710"
  },
  {
    "text": "and vector quantization that\nhas been used in conjunction with variational autoencoders. We won't go into\nit here, but it's",
    "start": "4271710",
    "end": "4277837"
  },
  {
    "text": "something that's been pretty\nsuccessful with discrete latent variables. And you can also use something\ncalled the reinforced trick,",
    "start": "4277837",
    "end": "4284230"
  },
  {
    "text": "which is also policy\ngradients, which are often used in reinforcement\nlearning, but also can be used to estimate\ngradients here.",
    "start": "4284230",
    "end": "4292200"
  },
  {
    "text": "But for continuous\nlatent variables, the reparameterization trick is\nfar better than these options.",
    "start": "4292200",
    "end": "4298949"
  },
  {
    "start": "4298950",
    "end": "4303970"
  },
  {
    "text": "Cool.  In the last five\nminutes, I'd like",
    "start": "4303970",
    "end": "4309360"
  },
  {
    "text": "to also go over\nanother interpretation of the objective.",
    "start": "4309360",
    "end": "4314969"
  },
  {
    "text": "So we've talked\nabout the objective as basically kind\nof an entropy term",
    "start": "4314970",
    "end": "4321000"
  },
  {
    "text": "and like a likelihood term. And alternatively, there's a\ndifferent way to look at it.",
    "start": "4321000",
    "end": "4329230"
  },
  {
    "text": "And the other way that\nyou can look at it",
    "start": "4329230",
    "end": "4336108"
  },
  {
    "text": "is, if you combine this\nterm and this term, you end up getting q log--",
    "start": "4336108",
    "end": "4343320"
  },
  {
    "text": "it's expectation of q log p\nminus expectation of q log q.",
    "start": "4343320",
    "end": "4348780"
  },
  {
    "text": "And these two terms\ncombined actually correspond to the KL\ndivergence, specifically",
    "start": "4348780",
    "end": "4360030"
  },
  {
    "text": "the KL divergence between\nq of z given x and p of z.",
    "start": "4360030",
    "end": "4370449"
  },
  {
    "text": "And then you're just left\nwith-- you also still have this first term, which is\nthe expectation of z sampled",
    "start": "4370450",
    "end": "4378400"
  },
  {
    "text": "from q of log p\ntheta of xi given z.",
    "start": "4378400",
    "end": "4387880"
  },
  {
    "text": "And this is basically a separate\nway to look at the equation.",
    "start": "4387880",
    "end": "4397580"
  },
  {
    "text": "And actually, sorry, this\nshould be a minus sign. ",
    "start": "4397580",
    "end": "4405100"
  },
  {
    "text": "And put it this\nway, you can sort of think of this from the\nstandpoint of an autoencoder",
    "start": "4405100",
    "end": "4412480"
  },
  {
    "text": "where your q network is encoding\nyour example into this latent",
    "start": "4412480",
    "end": "4418690"
  },
  {
    "text": "space. And your model p of x given z\nis decoding from your latent",
    "start": "4418690",
    "end": "4425139"
  },
  {
    "text": "variable back into your space. So if you think of\nthe autoencoders",
    "start": "4425140",
    "end": "4430210"
  },
  {
    "text": "that we saw before, then you can\nthink of the encoder as q of z",
    "start": "4430210",
    "end": "4435820"
  },
  {
    "text": "given x and the decoder\nas p of x given z. And then from this standpoint,\nthis term right here you",
    "start": "4435820",
    "end": "4445030"
  },
  {
    "text": "first encode, and then you\nevaluate the likelihood under your decoder. And this basically corresponds\nto reconstruction error,",
    "start": "4445030",
    "end": "4451900"
  },
  {
    "text": "like how good are\nyou at reconstructing the example after you've\nencoded it and decoded it.",
    "start": "4451900",
    "end": "4457360"
  },
  {
    "text": "And the second\nterm is telling you that once you encode your\nexample into your z space,",
    "start": "4457360",
    "end": "4465460"
  },
  {
    "text": "this should look like a\nGaussian distribution.",
    "start": "4465460",
    "end": "4470810"
  },
  {
    "text": "So your z's basically should\nbe Gaussian distributed. And this is going to\nthen basically add noise",
    "start": "4470810",
    "end": "4481750"
  },
  {
    "text": "into this variable right here. And that will kind\nof force the model to not be able to\nrepresent the identity",
    "start": "4481750",
    "end": "4487420"
  },
  {
    "text": "function through your\nencoder and decoder. ",
    "start": "4487420",
    "end": "4495030"
  },
  {
    "text": "So the KL divergence also has\na convenient analytical form for Gaussian distributions. ",
    "start": "4495030",
    "end": "4503520"
  },
  {
    "text": "Yeah. So you can basically think\nof this as taking an x, outputting a mean\ninvariance over z,",
    "start": "4503520",
    "end": "4510079"
  },
  {
    "text": "then adding noise or\nsampling from q in order to generate a z sample.",
    "start": "4510080",
    "end": "4516440"
  },
  {
    "text": "And then decoding that z\nsample with your p of theta to generate an x.",
    "start": "4516440",
    "end": "4522949"
  },
  {
    "text": " Cool. And this basically corresponds\nto the variational autoencoder.",
    "start": "4522950",
    "end": "4530800"
  },
  {
    "text": "And you can basically kind\nof sample a z from a Gaussian space, like a low-dimensional\nGaussian space,",
    "start": "4530800",
    "end": "4536950"
  },
  {
    "text": "and then have your\ndecoder or your model p of x given z then\ngenerate some cool looking",
    "start": "4536950",
    "end": "4544300"
  },
  {
    "text": "faces for you. Yeah? You're saying the KL term is\nsort of regularizing in there?",
    "start": "4544300",
    "end": "4549670"
  },
  {
    "text": "Yeah, exactly. So you can think of\nthe KL term as acting as a regularizer\non the autoencoder so that you're not only\noptimizing for reconstruction,",
    "start": "4549670",
    "end": "4556929"
  },
  {
    "text": "you're also encouraging\nit to kind of regularize the information content in z.",
    "start": "4556930",
    "end": "4562450"
  },
  {
    "start": "4562450",
    "end": "4568130"
  },
  {
    "text": "Cool. Yeah? I'm just trying to see,\nwhat does variational mean?",
    "start": "4568130",
    "end": "4574160"
  },
  {
    "text": "Yeah. What does variational mean? So I don't know if it has\nany meaning in English.",
    "start": "4574160",
    "end": "4583570"
  },
  {
    "text": "But the distribution\nq of z, this",
    "start": "4583570",
    "end": "4590739"
  },
  {
    "text": "is often referred to as the\nvariational distribution maybe in the sense that it's\nnot the true distribution.",
    "start": "4590740",
    "end": "4600010"
  },
  {
    "text": "It's not the distribution\ngiven by your model. It's not like the true\ndistribution, for example. ",
    "start": "4600010",
    "end": "4614010"
  },
  {
    "text": "Yeah? Can it come from\ncalcuation of variations? Yes, that's possible. ",
    "start": "4614010",
    "end": "4622695"
  },
  {
    "text": "Cool.  And then when you want to sample\nfrom a variational autoencoder,",
    "start": "4622695",
    "end": "4628060"
  },
  {
    "text": "you can first sample z\nand then sample x given z. ",
    "start": "4628060",
    "end": "4636355"
  },
  {
    "text": "Lastly, if you want to\ndo conditional models, you can also condition\non something we primarily",
    "start": "4636355",
    "end": "4642310"
  },
  {
    "text": "didn't-- we talked about\nunconditional models to start, but you could also basically\ncondition your inference network and condition your\nmodel on something like a class",
    "start": "4642310",
    "end": "4652630"
  },
  {
    "text": "variable to generate\nimages of that class, or text and so forth.",
    "start": "4652630",
    "end": "4657969"
  },
  {
    "text": "And so really\neverything is the same. We're just conditioning\non some input value. ",
    "start": "4657970",
    "end": "4666010"
  },
  {
    "text": "Cool. So that's it. To summarize, we talked\nabout latent variable models, how to train them with\nvariational inference",
    "start": "4666010",
    "end": "4672663"
  },
  {
    "text": "and amortized\nvariational inference. And we learned\nabout various things like the\nreparameterization trick",
    "start": "4672663",
    "end": "4677935"
  },
  {
    "text": "to optimize these\nkinds of models. Yeah, and as a reminder,\nhomework 3 is due on Friday.",
    "start": "4677935",
    "end": "4684690"
  },
  {
    "text": "And I'll see you\nfolks on Wednesday. ",
    "start": "4684690",
    "end": "4691000"
  }
]