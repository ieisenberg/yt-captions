[
  {
    "start": "0",
    "end": "5640"
  },
  {
    "text": "Hi, everybody. We're going to go ahead and\nget started because we're going to be having a guest lecture\ntoday, which will start at 1:45.",
    "start": "5640",
    "end": "11280"
  },
  {
    "text": "So welcome back. Just in terms of where we are,\na few different quick logistics",
    "start": "11280",
    "end": "16680"
  },
  {
    "text": "things. The midterm, as everybody\nprobably knows, is on Wednesday. It'll be in class. You're allowed to have one\nside of a normal sheet of paper",
    "start": "16680",
    "end": "26130"
  },
  {
    "text": "in terms of your sheet of notes. All the material\nthrough today is going to be eligible for the exam.",
    "start": "26130",
    "end": "32340"
  },
  {
    "text": "That was also in the Ed post. And you can see the Ed post\nfor any additional information around midterms and prior exams.",
    "start": "32340",
    "end": "39910"
  },
  {
    "text": "Because Homework 2 was only due\non Friday, and a lot of people used late days\nthrough yesterday,",
    "start": "39910",
    "end": "45562"
  },
  {
    "text": "we won't be able to grade\nit in time for the midterm, but we will release solutions. So those will be available\nby the end of today.",
    "start": "45562",
    "end": "53820"
  },
  {
    "text": "All right. So let's start with a quick\nrefresher understanding. This is on the poles, and then\nI'll do a quick recap of RLHF",
    "start": "53820",
    "end": "62010"
  },
  {
    "text": "before we dive into\nour guest lecture. ",
    "start": "62010",
    "end": "73088"
  },
  {
    "text": "This will be a good reminder\nof some of the ideas that will be relevant to\ntoday's lecture, as well. ",
    "start": "73088",
    "end": "152830"
  },
  {
    "text": "All right. We have pretty good consensus on\nthe first one that this is true. The Bradley Terry model\nexpresses the probability",
    "start": "152830",
    "end": "157882"
  },
  {
    "text": "that someone will select one\noption over another option. So this is true.",
    "start": "157882",
    "end": "163170"
  },
  {
    "text": "And we have pretty\ngood consensus that the last one is false. In RLHF, we do not update the\nmodel after each PPO rollout.",
    "start": "163170",
    "end": "170635"
  },
  {
    "text": "There's a little bit of\ndisagreement, particularly, about these two. So why don't you\nturn to a neighbor and quickly see if\nyou can resolve those?",
    "start": "170635",
    "end": "178035"
  },
  {
    "start": "178035",
    "end": "214872"
  },
  {
    "text": "And as a hint, it's\nuseful to think about whether things can\nchange based on whether or not it's\npositive or negative.",
    "start": "214872",
    "end": "220070"
  },
  {
    "start": "220070",
    "end": "255630"
  },
  {
    "text": "All right. I hope everyone got a chance to\nthink about that for a second. So the second one is true. The third one is also true.",
    "start": "255630",
    "end": "263280"
  },
  {
    "text": "Somebody want to say why\nthe fourth one is false? It's false.",
    "start": "263280",
    "end": "268560"
  },
  {
    "text": "This one is false. ",
    "start": "268560",
    "end": "273990"
  },
  {
    "text": "No. Yeah. If you to play by a negative\nconstant, it's both references. Yeah, exactly.",
    "start": "273990",
    "end": "279070"
  },
  {
    "text": "So remind me of your name. So that is exactly right. So if you multiply by\nnegative, of course,",
    "start": "279070",
    "end": "284690"
  },
  {
    "text": "that's exactly flipping\nall the rewards. And so in general, that will\nnot preserve preferences.",
    "start": "284690",
    "end": "289930"
  },
  {
    "text": "You can shift it\nby any constant. And if you go\nthrough the math, you can see that the\nexponentials will all cancel.",
    "start": "289930",
    "end": "295070"
  },
  {
    "text": "So that part is true. ",
    "start": "295070",
    "end": "303150"
  },
  {
    "text": "OK, great. So what we talked\nabout last time was maximum entropy inverse\nreinforcement learning, and we started\ntalking about RLHF,",
    "start": "303150",
    "end": "309550"
  },
  {
    "text": "including how you could use the\nBradley Terry model for Markov decision processes. I'm going to do a really\nquick discussion of RLHF",
    "start": "309550",
    "end": "317080"
  },
  {
    "text": "with respect to\nlarge language models before we get into our\nguest lecture today. And then on Wednesday\nis the midterm.",
    "start": "317080",
    "end": "324400"
  },
  {
    "text": "So as we talked about\nlast week, while you could do imitation\nlearning, where",
    "start": "324400",
    "end": "329883"
  },
  {
    "text": "you get sort of\nfull trajectories, and you want to\nimitate those, that is less information than you\nmight be able to get if you",
    "start": "329883",
    "end": "336490"
  },
  {
    "text": "got pairwise preferences. And we talked about how\npairwise preferences might be an interesting intermediary\npoint between humans having",
    "start": "336490",
    "end": "343659"
  },
  {
    "text": "to label, like they do in DAGGER\nat every step of what someone should do, or provide really\ndense rewards versus just",
    "start": "343660",
    "end": "350860"
  },
  {
    "text": "providing demonstrations. And so this sort of has\nmotivated a long line of work,",
    "start": "350860",
    "end": "356150"
  },
  {
    "text": "including preference\nlearning recently. We saw how you could learn the\nparameters of a Bradley Terry",
    "start": "356150",
    "end": "362270"
  },
  {
    "text": "model. As we saw just now, these\nare not unique, in general. You can do translations\nof the rewards",
    "start": "362270",
    "end": "368780"
  },
  {
    "text": "and you will preserve the\nresulting preferences. You can maximize this\nwith cross entropy.",
    "start": "368780",
    "end": "374760"
  },
  {
    "text": "And last time, we\nsaw how you could do this for trajectories,\nas well as for bandit like problems where you only\nhave a finite set of actions.",
    "start": "374760",
    "end": "382820"
  },
  {
    "text": "In Homework 3, you're going\nto be implementing both DPO and RLHF for Markov\ndecision processes.",
    "start": "382820",
    "end": "389669"
  },
  {
    "text": "So you get a chance to play with\nthis where you're using rollouts from MuJoCo like problems.",
    "start": "389670",
    "end": "396190"
  },
  {
    "text": "But before we go on\nto our guest lecture, I wanted to just\nbriefly go through",
    "start": "396190",
    "end": "401389"
  },
  {
    "text": "how you go from doing this\napproach to learning reward",
    "start": "401390",
    "end": "406760"
  },
  {
    "text": "models all the way to ChatGPT. And so for this,\nI'm going to draw upon some of Tatsu Hashimoto's\nreally nice lecture notes",
    "start": "406760",
    "end": "413960"
  },
  {
    "text": "from an NLP class. So recall from the start of the\nreinforcement learning course,",
    "start": "413960",
    "end": "419730"
  },
  {
    "text": "we looked at this sort\nof pipeline from ChatGPT. And here, we had the\ndemonstration data,",
    "start": "419730",
    "end": "424790"
  },
  {
    "text": "collecting the comparison data,\nand then optimizing a policy. So now we've seen how those\nlast two steps happen.",
    "start": "424790",
    "end": "432510"
  },
  {
    "text": "So in particular, you can\ngenerate pairwise preferences, or in fact, you can\ngenerate full rankings,",
    "start": "432510",
    "end": "438260"
  },
  {
    "text": "and then use that to\nlearn a reward model. And so while we thought before\nabout different ways of doing",
    "start": "438260",
    "end": "444139"
  },
  {
    "text": "this, as a particular\nexample involving language, you might say someone might\nprefer an earthquake hit San Francisco.",
    "start": "444140",
    "end": "449640"
  },
  {
    "text": "There was minor property damage\nbut no injuries versus a 4.2 magnitude earthquake hit\nSan Francisco, resulting",
    "start": "449640",
    "end": "455780"
  },
  {
    "text": "in massive damage, versus\nBarry has good weather, but it sometimes has\nwildfires and earthquakes.",
    "start": "455780",
    "end": "462590"
  },
  {
    "text": "So you can see in\nthis case that these are places where\nsomeone might be able to provide different\nrankings in response to prompts.",
    "start": "462590",
    "end": "468330"
  },
  {
    "text": "So now you can think of the\ncontext as being a prompt, and the output as being\nall the actions or all",
    "start": "468330",
    "end": "474038"
  },
  {
    "text": "the different\nresponses you can have, and people are\ngoing to rank them. ",
    "start": "474038",
    "end": "479600"
  },
  {
    "text": "Now, sort of building on\nthat, before you actually do PPO or something,\nyou may want to try to check the quality\nof your reward model.",
    "start": "479600",
    "end": "487169"
  },
  {
    "text": "And this is something\nthat you'll also think about for Homework 3. So in general, depending on\nthe amount of data you have",
    "start": "487170",
    "end": "493280"
  },
  {
    "text": "and the complexity\nof your reward model, you're going to be able to do\na better or worse job of being able to try to capture the\nunderlying latent reward",
    "start": "493280",
    "end": "500060"
  },
  {
    "text": "model of people. So in this case, this is looking\nat different model sizes. And these are big models.",
    "start": "500060",
    "end": "507095"
  },
  {
    "text": "A lot of the models that people\nhave thought about historically are things like linear models\nor then neural network models.",
    "start": "507095",
    "end": "513179"
  },
  {
    "text": "But these can be\nextremely large models. They can be on the same order\nas large language models. It's not uncommon to see 7\nbillion parameter reward models.",
    "start": "513179",
    "end": "522150"
  },
  {
    "text": "And what they're looking at\nhere is validation accuracy. And so what you can\nsee here is, when you start to get enough data,\nand you have a big enough model,",
    "start": "522150",
    "end": "529890"
  },
  {
    "text": "then you can start to capture\nreally complex reward models. And so that's a useful thing to\nthink about when you're thinking",
    "start": "529890",
    "end": "535970"
  },
  {
    "text": "about your projects or you're\nthinking about homeworks of what is the complexity\nwe need in order to start to capture human preferences.",
    "start": "535970",
    "end": "544070"
  },
  {
    "text": "And then once you\nhave that, now we have everything we need\nto do that pipeline. So if you've gotten\na lot of preferences,",
    "start": "544070",
    "end": "551110"
  },
  {
    "text": "now, again, the question is,\nhow many of those preferences do you need. It might be a lot. So if you look\nback here, this is",
    "start": "551110",
    "end": "558640"
  },
  {
    "text": "quite a lot of preference data. Now, it's not the\nsame amount of data that we would generally need\nto be using to train an LLM,",
    "start": "558640",
    "end": "565750"
  },
  {
    "text": "but it's not like\none or two either. And in fact, there is a lot\nof ongoing interesting work in trying to think\nabout how do we",
    "start": "565750",
    "end": "572110"
  },
  {
    "text": "reduce the amount of\nonline preference data that we need in\norder to train these.",
    "start": "572110",
    "end": "577787"
  },
  {
    "text": "By online, I just\nmean additional data compared to the historical. So in reinforcement learning\nfrom human feedback, what",
    "start": "577787",
    "end": "583390"
  },
  {
    "text": "we can do is, once we've had\nthat learned reward model, now you can use that with PPO.",
    "start": "583390",
    "end": "589030"
  },
  {
    "text": "And one of the\nimportant things to note here is that, just like how we\nsaw for PPO before, in general,",
    "start": "589030",
    "end": "595459"
  },
  {
    "text": "we're going to need some sort\nof reference decision policy that maybe we've used\nfrom behavior cloning",
    "start": "595460",
    "end": "602110"
  },
  {
    "text": "or supervised fine tuning. And we want to regularize so\nwe don't get too far from that when we're doing PPO.",
    "start": "602110",
    "end": "608810"
  },
  {
    "text": "And so that sort\nof divergence is going to be just as\nimportant as what we've seen in the previous work.",
    "start": "608810",
    "end": "615825"
  },
  {
    "text": "And one of the things\nthat's been noted is that, perhaps\nnot surprisingly, given the huge success of\nChatGPT, this type of approach",
    "start": "615825",
    "end": "623000"
  },
  {
    "text": "can make a significant\ndifference. So by leveraging\nrewards and doing RLHF, there really was a substantial\ngain over previous approaches,",
    "start": "623000",
    "end": "631590"
  },
  {
    "text": "even when you fix\nfor the model size. So that suggests that changing\nthe optimization function",
    "start": "631590",
    "end": "637470"
  },
  {
    "text": "we're using and using\nthe reward functions really can lead to substantial\ngains in performance.",
    "start": "637470",
    "end": "644310"
  },
  {
    "text": "So I think something that's\nimportant to notice here is, well, what are we doing\nthe reinforcement learning over",
    "start": "644310",
    "end": "650490"
  },
  {
    "text": "and how are we training\nthe reward model. In comparison to what\nwe've talked about mostly in this class, this\nis really where",
    "start": "650490",
    "end": "656340"
  },
  {
    "text": "you're trying to\ndo something almost like Meta reinforcement learning\nor multi-task reinforcement learning.",
    "start": "656340",
    "end": "661420"
  },
  {
    "text": "So instead of training\nan agent to do one task, like do a backflip\nor solve a Gridworld,",
    "start": "661420",
    "end": "667030"
  },
  {
    "text": "we're really trying to train\na large language model here to do any possible task\nthe user might want.",
    "start": "667030",
    "end": "672240"
  },
  {
    "text": "And so then when\nwe're collecting data and we're doing comparisons, you\nmight have an enormous number",
    "start": "672240",
    "end": "677970"
  },
  {
    "text": "of different tasks. So writing a thank you\nletter, to making a website, to lots of different\nthings, all things",
    "start": "677970",
    "end": "684510"
  },
  {
    "text": "that used to be previously\nconsidered different tasks will likely be involved in this. ",
    "start": "684510",
    "end": "691840"
  },
  {
    "text": "So another thing that I\nthink it's useful to note is that this is a comparison\nfrom 2023, also, from Stanford.",
    "start": "691840",
    "end": "699147"
  },
  {
    "text": "There's also been a\nlot of other work. This is a very\nimportant ongoing area to understand how good\nthese approaches are.",
    "start": "699147",
    "end": "705550"
  },
  {
    "text": "And one thing that's useful\nto know is that best of n is an alternative where\nyou could, for example, use",
    "start": "705550",
    "end": "712630"
  },
  {
    "text": "your reward model,\njust generate n samples from your\noriginal model, and then just use your reward\nmodel to pick the best one,",
    "start": "712630",
    "end": "719829"
  },
  {
    "text": "according to your reward model. So that doesn't use any\nreinforcement learning. It doesn't use PPO.",
    "start": "719830",
    "end": "725540"
  },
  {
    "text": "It's just using\nyour reward model as sort of an external\nexpert to try to pick among all of your generations.",
    "start": "725540",
    "end": "732430"
  },
  {
    "text": "And what you can see here is\nthat also does pretty well, relative to PPO.",
    "start": "732430",
    "end": "737980"
  },
  {
    "text": "Now, in general, it\ndoesn't do quite as well, but I think it's really\nuseful to think about some of these alternative baselines,\nparticularly depending on",
    "start": "737980",
    "end": "744705"
  },
  {
    "text": "whether or not you have\naccess to actually training the model again versus you\nmight have access to being able to train a reward model.",
    "start": "744705",
    "end": "751579"
  },
  {
    "text": "And you might have access\nto an off the shelf LLM, and you might be able\nto combine these. It's a very active,\nongoing area to figure out",
    "start": "751580",
    "end": "758172"
  },
  {
    "text": "what's the best way to train and\nrefine these sorts of models. ",
    "start": "758172",
    "end": "763890"
  },
  {
    "text": "All right. So that was a five\nminute overview of how people use\nRLHF to train ChatGPT.",
    "start": "763890",
    "end": "770670"
  },
  {
    "text": "And now I'm really excited\nto have our guest lecture on direct preference\noptimization. ",
    "start": "770670",
    "end": "777560"
  },
  {
    "text": "Yay. All right. OK, well, I'm super delighted\nto have Raphael, Archit and Eric",
    "start": "777560",
    "end": "783080"
  },
  {
    "text": "here today to talk about\ndirect preference optimization. I really appreciate\nyou guys coming. I know you guys have done\nthis rodeo before at NeurIPS",
    "start": "783080",
    "end": "790910"
  },
  {
    "text": "in terms of bouncing\nbetween three people. So for those of you\nthat don't know, direct preference optimization\ngot outstanding best paper",
    "start": "790910",
    "end": "798260"
  },
  {
    "text": "runner up at NeurIPS\nthis year, which is the premier machine\nlearning conference. It's also had a huge impact\nalready, really broadly,",
    "start": "798260",
    "end": "808040"
  },
  {
    "text": "on the LLM community as\nan alternative to RLHF. So I think it's\nextremely exciting.",
    "start": "808040",
    "end": "814020"
  },
  {
    "text": "You guys will get to do, to my\nknowledge, the first homework that's incorporating RLHF and\nDPO, which will be really great.",
    "start": "814020",
    "end": "820715"
  },
  {
    "text": "And what they're going to\ntalk about today is this. And they also just\nhad a new paper drop on Archive just\na few days ago talking",
    "start": "820715",
    "end": "827750"
  },
  {
    "text": "about some extensions. And so I think it was timely\nthat they could all be here. Thanks so much. OK.",
    "start": "827750",
    "end": "834529"
  },
  {
    "text": "Well, yeah, thanks so\nmuch, Emma, for having us.",
    "start": "834530",
    "end": "839700"
  },
  {
    "text": "It's funny when you talk\nabout the impact of the paper. You sort of want to say RL,\nbut I guess it's LLM community.",
    "start": "839700",
    "end": "846930"
  },
  {
    "text": "Or what even is the\ncommunity anymore? It's hard to draw the\nboundaries between things, but I think it's\nso cool to see how",
    "start": "846930",
    "end": "853290"
  },
  {
    "text": "the boundaries are\nkind of breaking down between these areas. So yeah, as Emma\nsaid, we're going",
    "start": "853290",
    "end": "858779"
  },
  {
    "text": "to talk a bit\nabout RLHF and DPO. And we have a little\nbit of background that I'll do to set\nthings up for these guys",
    "start": "858780",
    "end": "865890"
  },
  {
    "text": "to bring things home. And some of this\nis probably going to be review from things\nEmma has already covered. But just to make sure\nwe're all on the same page,",
    "start": "865890",
    "end": "873720"
  },
  {
    "text": "we are, in fact, talking about\nthis setting of reinforcement learning from human feedback.",
    "start": "873720",
    "end": "879850"
  },
  {
    "text": "And as a small piece of\nbackground or setup here,",
    "start": "879850",
    "end": "884970"
  },
  {
    "text": "why are we talking about RLHF? Why are we doing RL\non language models? Why are we talking about it now?",
    "start": "884970",
    "end": "891190"
  },
  {
    "text": "People did not start doing RL on\nlanguage models a few years ago when ChatGPT came out. People have been doing RL on\nlanguage models for a long time.",
    "start": "891190",
    "end": "898660"
  },
  {
    "text": "But this sort of ChatGPT\nmoment, so to speak, is something that I think\nreally brought these RL methods",
    "start": "898660",
    "end": "905700"
  },
  {
    "text": "to language models into the\nforefront of people's minds, because there was sort of a\nsense in which things really started working for\nthe first time in a way",
    "start": "905700",
    "end": "912330"
  },
  {
    "text": "that maybe they didn't before. And a lot of this\ncomes from being able to start from a really\nstrong pre-trained model that",
    "start": "912330",
    "end": "919110"
  },
  {
    "text": "already has a lot of\ninteresting skills and pre learned behaviors\nthat we can fine tune.",
    "start": "919110",
    "end": "926230"
  },
  {
    "text": "And so we don't have\nto start from scratch when we're doing RL, typically,\non these language models. And that makes it a lot\nmore kind of accessible",
    "start": "926230",
    "end": "932408"
  },
  {
    "text": "to get some benefit\nfrom these algorithms. OK, so RLHF. We have this we have\nthis three stage",
    "start": "932408",
    "end": "938530"
  },
  {
    "text": "pipeline that is the thing that\nhas sort of been popularized by ChatGPT. So in this first stage,\nI think Emma actually",
    "start": "938530",
    "end": "945640"
  },
  {
    "text": "showed this same figure in\nher slide just a minute ago. So this isn't totally new. In this first stage, there's\nreally a step 0 here,",
    "start": "945640",
    "end": "953440"
  },
  {
    "text": "which is do the\nunsupervised pre-training. This is when we just\nfit a big generative model of a ton of text. This is, again, where we learn,\nmeta learn, in some sense,",
    "start": "953440",
    "end": "961779"
  },
  {
    "text": "some skills we're\ngoing to select from. And then we're going to\ncollect some supervised demos, so from humans.",
    "start": "961780",
    "end": "967910"
  },
  {
    "text": "We'll have some\ndata set of prompts, explain the moon landing\nto a six-year-old. And a human is\ngoing to write sort of a sensible, good\ndemonstration response",
    "start": "967910",
    "end": "976420"
  },
  {
    "text": "to this prompt. And we're just going to do\nsupervised fine tuning here. And this is going to actually\nserve as that reference policy",
    "start": "976420",
    "end": "982630"
  },
  {
    "text": "that Emma was talking\nabout a few minutes ago, the thing that we're going\nto constrain our model to,",
    "start": "982630",
    "end": "987760"
  },
  {
    "text": "to make learning\na little easier, and also, to avoid\nover optimization",
    "start": "987760",
    "end": "992800"
  },
  {
    "text": "of our approximate\nproxy reward function that we're learning from. In the second stage, that's when\nwe do the learning of the reward",
    "start": "992800",
    "end": "999290"
  },
  {
    "text": "model. So here's when we\ncollect preference data. So we're going to\nsample responses, typically, from the supervised,\nfine tuned model that we",
    "start": "999290",
    "end": "1006010"
  },
  {
    "text": "learned in the first stage. And we're going to ask a human\nor opt in a collection of humans to provide ranking annotations\nover multiple draws",
    "start": "1006010",
    "end": "1013330"
  },
  {
    "text": "from that supervised,\nfine tuned model. And we're going to use\nthose preferences to learn a reward model, so a mapping\nfrom a prompt, a dialogue",
    "start": "1013330",
    "end": "1021610"
  },
  {
    "text": "history, and a potential\nresponse to a scalar reward. And then in the\nthird stage, we're going to do policy learning.",
    "start": "1021610",
    "end": "1027140"
  },
  {
    "text": "So we're going to try to fine\ntune that supervised, fine tuned model to generate responses\nthat receive high reward",
    "start": "1027140",
    "end": "1033939"
  },
  {
    "text": "from that reward model. OK, so the first step is\npretty straightforward. Supervised fine tuning.",
    "start": "1033940",
    "end": "1039630"
  },
  {
    "text": "We don't really need to\ntalk about it very much. And again, Emma already\ncovered some of these things, so hopefully, this\nis mostly review.",
    "start": "1039630",
    "end": "1045240"
  },
  {
    "text": "But of course, ask questions\nif anything seems funny. Like I said, the feedback here,\nthe thing we're going to do",
    "start": "1045240",
    "end": "1052165"
  },
  {
    "text": "is we're going to\nget preferences over responses from our model. OK? So we're going to end up with\nsome data set of a prompt.",
    "start": "1052165",
    "end": "1058710"
  },
  {
    "text": "And this could be\nsingle prompt, or it could be an entire dialogue\nhistory, so multiple turns and then the most\nrecent user message.",
    "start": "1058710",
    "end": "1066480"
  },
  {
    "text": "And then this is typically\ngoing to be only two responses that we're going to do give\na binary preference over.",
    "start": "1066480",
    "end": "1071940"
  },
  {
    "text": "You can do rankings\nover a more responses, but the returns can be a\nplateau relatively quickly.",
    "start": "1071940",
    "end": "1078767"
  },
  {
    "text": "And it's typically maybe better\nto have more prompts and fewer responses per prompt. I think it's worth\nmentioning briefly,",
    "start": "1078767",
    "end": "1084380"
  },
  {
    "text": "why are we talking about\npreferences over responses, instead of directly asking\nfor reward annotations.",
    "start": "1084380",
    "end": "1091880"
  },
  {
    "text": "You could take your\nprompt and your response and just ask the human, 1 to 10,\nhow good of a response is this.",
    "start": "1091880",
    "end": "1099620"
  },
  {
    "text": "And there are a couple\nof reasons for this. First of all, actually,\nin another set of slides, we have an example\nof this, which",
    "start": "1099620",
    "end": "1105410"
  },
  {
    "text": "I think makes it quite clear. I don't think we have\nthem in this deck. But if you take two\ndifferent humans and you say,",
    "start": "1105410",
    "end": "1112783"
  },
  {
    "text": "here's a prompt\nthat says write me a recipe for making\na really good cake, and you ask two\ndifferent humans, you have two different\nresponses from your model.",
    "start": "1112783",
    "end": "1119790"
  },
  {
    "text": "And for human A,\nyou say, what reward do you give to this\nresponse and that response, and another human, you\nask the same question,",
    "start": "1119790",
    "end": "1126170"
  },
  {
    "text": "you can end up with the\nsame ranking over responses, but actually a lot\nof disagreement in the actual rewards\nthat you're giving.",
    "start": "1126170",
    "end": "1132812"
  },
  {
    "text": "So people are not\nreally calibrated to each other in terms\nof the absolute rewards that they're going\nto be assigning.",
    "start": "1132812",
    "end": "1137930"
  },
  {
    "text": "And it's also just more\ncognitively difficult to assign this absolute\nnumber in contrast",
    "start": "1137930",
    "end": "1144710"
  },
  {
    "text": "to anchoring to one response,\nand then just making a decision about, is another\nthing better or worse.",
    "start": "1144710",
    "end": "1150500"
  },
  {
    "text": "So in some sense, I think\ngathering preferences as opposed to asking humans\nto write high quality",
    "start": "1150500",
    "end": "1156230"
  },
  {
    "text": "demonstrations, or asking\nhumans to assign directly the reward itself\nis sort of a way to get higher return of\nannotation information per unit",
    "start": "1156230",
    "end": "1166259"
  },
  {
    "text": "of cognitive effort\nof the human labeler. So we're going to get\nthese preferences. And now, we just have\nthis Bradley Terry model,",
    "start": "1166260",
    "end": "1172920"
  },
  {
    "text": "which is a very simple model of\ndiscrete choice in humans, which relates a scoring function,\nor in case, a reward function,",
    "start": "1172920",
    "end": "1180149"
  },
  {
    "text": "so that's this r of\nx and a response y here, to a\nprobabilistic decision",
    "start": "1180150",
    "end": "1186350"
  },
  {
    "text": "over two discrete choices. So here are our\ndiscrete choices. We have the thing that was\nlabeled preferred in the data",
    "start": "1186350",
    "end": "1193232"
  },
  {
    "text": "set, and the thing\nthat was labeled this preferred in the data set. And we wanted to train a model,\nsome probabilistic model, again,",
    "start": "1193232",
    "end": "1200040"
  },
  {
    "text": "our reward model, to maximize\nthe likelihood of this observed data. And we need to decide\non some model that",
    "start": "1200040",
    "end": "1206809"
  },
  {
    "text": "relates a scoring\nfunction to these choices in order to do\nmaximum likelihood. And that is this Bradley Terry\nmodel, which we can then simply",
    "start": "1206810",
    "end": "1214010"
  },
  {
    "text": "do a maximum likelihood in,\nor use this negative log likelihood of loss.",
    "start": "1214010",
    "end": "1220310"
  },
  {
    "text": "So we're using this Bradley\nTerry conceptual model of choices, and this turns into\na maximum likelihood of loss.",
    "start": "1220310",
    "end": "1226710"
  },
  {
    "text": "So we're simply solving a\nbinary classification problem. So we have a binary\nclassifier here, where the logic\nis just our reward",
    "start": "1226710",
    "end": "1233549"
  },
  {
    "text": "model, the difference\nin the reward we're assigning to\nthe chosen response minus the dispreferred\nor the rejected response.",
    "start": "1233550",
    "end": "1241302"
  },
  {
    "text": "We're treating that as a\nlogit of a binary classifier and doing maximum likelihood. So once we do that,\nwe get a reward model.",
    "start": "1241302",
    "end": "1247799"
  },
  {
    "text": "We finished step two now. And now, we need to\njust find a policy that actually optimizes this reward.",
    "start": "1247800",
    "end": "1253530"
  },
  {
    "text": "And really, this is the\nRL bit, so to speak. And here, we want to\nlearn, again, pi theta.",
    "start": "1253530",
    "end": "1261007"
  },
  {
    "text": "This is our policy\nthat we're actually fine tuning that we're\nactually learning here. And the objective\nhere is, we have",
    "start": "1261007",
    "end": "1267207"
  },
  {
    "text": "prompts where we have some data\nset of prompts or conversation histories and\nexpectation for responses",
    "start": "1267207",
    "end": "1273270"
  },
  {
    "text": "sampled from our policy. We want to achieve high reward,\nbut that's not the full story here.",
    "start": "1273270",
    "end": "1279210"
  },
  {
    "text": "If we just optimize to\nmaximize the reward here,",
    "start": "1279210",
    "end": "1284649"
  },
  {
    "text": "what can happen? I'm not sure if you've\ntalked about this already. OK.",
    "start": "1284650",
    "end": "1291040"
  },
  {
    "text": "Perfect. Anybody have any worries about\njust optimizing this objective, or are we good?",
    "start": "1291040",
    "end": "1296929"
  },
  {
    "text": "Because this is OK. We could tell if you haven't. You can forget the\nrest of the objectives. ",
    "start": "1296930",
    "end": "1304269"
  },
  {
    "text": "Perfect. OK, so one thing that can\nhappen here is, you remember, this is not a true\nreward function.",
    "start": "1304270",
    "end": "1310029"
  },
  {
    "text": "This is something we learned\nfrom a finite data set. And so there's going to be some\ndistribution, in which this",
    "start": "1310030",
    "end": "1316539"
  },
  {
    "text": "gives us accurate or\nmeaningful rewards. And there's going to be,\noutside of that distribution, there's no guarantee\nthis thing is going",
    "start": "1316540",
    "end": "1321730"
  },
  {
    "text": "to generalize meaningfully. So what we typically\nend up doing is we actually have an\nadditional constraint, a KL",
    "start": "1321730",
    "end": "1327760"
  },
  {
    "text": "penalty from our starting model,\nthe SFT model, or a reference model to say, I want\nyou to maximize rewards,",
    "start": "1327760",
    "end": "1333612"
  },
  {
    "text": "but I don't want you drift too\nfar from the starting model. Because again, our\nreward model was",
    "start": "1333612",
    "end": "1338679"
  },
  {
    "text": "trained on preferences\nover samples from that reference model. So if we drift far from\nthe reference model,",
    "start": "1338680",
    "end": "1344240"
  },
  {
    "text": "we're sort of out of\ndistribution for the data that our reward\nmodel was trained on. So we basically can start\ngetting bogus reward scores",
    "start": "1344240",
    "end": "1350440"
  },
  {
    "text": "if our policy changes too much\nfrom that reference model.",
    "start": "1350440",
    "end": "1355600"
  },
  {
    "text": "Yeah. Is the reference\nmodel ever changing? It depends on the algorithm. I think in the original\ncanonical version of RLF,",
    "start": "1355600",
    "end": "1363992"
  },
  {
    "text": "no, it was a fixed\nreference model. But there have\nbeen a lot of works since then showing ways to\nupdate the reference model and use a moving reference\nmodel over time, yeah.",
    "start": "1363992",
    "end": "1371452"
  },
  {
    "text": "Yeah. The original data is coming\nfrom that reference model. You mean that both\nthose yw and yl,",
    "start": "1371452",
    "end": "1377850"
  },
  {
    "text": "they are both coming\nfrom the same model? Again, in the sort of original\ncanonical form of RLHF, yes.",
    "start": "1377850",
    "end": "1383220"
  },
  {
    "text": "Since then, people have\nproposed a wide variety of different sampling\nschemes, a way to select what pair\nof responses do",
    "start": "1383220",
    "end": "1390277"
  },
  {
    "text": "you show the human to\nget a preference over. But in the original\nvanilla version of RLHF, yeah, you typically\nsample two responses",
    "start": "1390277",
    "end": "1395730"
  },
  {
    "text": "from the reference model,\nget a preference over them, and use that to learn\nyour word model. I've heard that in\npractice, but the responses",
    "start": "1395730",
    "end": "1403242"
  },
  {
    "text": "come from the same model\nwith different temperatures, or different models. Does that, in theory, mean that\nwe're doing something wrong?",
    "start": "1403242",
    "end": "1412320"
  },
  {
    "text": "Well, I'm not\nsure, in theory, it means you're doing\nsomething wrong. I think one way to think\nabout this is, again, we",
    "start": "1412320",
    "end": "1419610"
  },
  {
    "text": "want our reward model to perform\nwell across the state action space.",
    "start": "1419610",
    "end": "1424620"
  },
  {
    "text": "So if you think of our state\nspace being, or our context space being the\nconversational history so far,",
    "start": "1424620",
    "end": "1430200"
  },
  {
    "text": "and our action space\nbeing the response, you want to have good\ncoverage over this space",
    "start": "1430200",
    "end": "1435649"
  },
  {
    "text": "so that you're going to get\nmeaningful rewards out when you actually update your policy. And so in principle,\nyeah, we would",
    "start": "1435650",
    "end": "1443180"
  },
  {
    "text": "like to be able to\ncover this space, assuming we have a model\nthat has high enough capacity to model all of it. We'd like to cover as much\nof this space as possible.",
    "start": "1443180",
    "end": "1449528"
  },
  {
    "text": "So yeah, a more diverse\npreference data set is very helpful. And there's some\ntrade offs between, we want to concentrate\nour preference",
    "start": "1449528",
    "end": "1455882"
  },
  {
    "text": "data set on the things\nthat are high quality, but also, make sure we\ndo cover a wide variety so we don't overestimate\nrewards for bad stuff.",
    "start": "1455882",
    "end": "1465160"
  },
  {
    "text": "OK, one more, and then I'll\nhand it off to these guys. So [INAUDIBLE] learning,\nwe know that even",
    "start": "1465160",
    "end": "1470710"
  },
  {
    "text": "if you have limited\ndata set, but if you have a large enough\nnetwork, then if you train",
    "start": "1470710",
    "end": "1477100"
  },
  {
    "text": "it to near zero error on\nthe limited train data set, it can't generalize well on\nthe test data set, as well.",
    "start": "1477100",
    "end": "1483610"
  },
  {
    "text": "Why is that not applicable here,\nas well for the reward model? Well, it is applicable\nin the sense",
    "start": "1483610",
    "end": "1490030"
  },
  {
    "text": "that the same sort of phenomena,\nlike double descent and things like this are still\napplicable in this case.",
    "start": "1490030",
    "end": "1495800"
  },
  {
    "text": "So you will get better\nperformance, typically, from using a larger\nreward model. But there are limits to this.",
    "start": "1495800",
    "end": "1501693"
  },
  {
    "text": "There's only a certain\namount of information content in a finite data\nset of preferences. And so the extent to which\nyou can push that model",
    "start": "1501693",
    "end": "1508690"
  },
  {
    "text": "to generalize to new things-- my preference data set only\nhas questions about what types of pets someone likes.",
    "start": "1508690",
    "end": "1514350"
  },
  {
    "text": "It's just not going to tell you\nanything about quantum field theory, no matter how\nbig you make your model.",
    "start": "1514350",
    "end": "1520809"
  },
  {
    "text": "So there are limits. But yes, you would expect\nsome level of generalization.",
    "start": "1520810",
    "end": "1525890"
  },
  {
    "text": "OK, cool. So that is a primer on RLHF. And, basically, unfortunately,\nwhat we end up with is this--",
    "start": "1525890",
    "end": "1533060"
  },
  {
    "text": "if we're doing PPO,\nfor example, this ends up being really,\nreally, really complicated in the policy learning stage. So there are a lot of\nmoving pieces here,",
    "start": "1533060",
    "end": "1539150"
  },
  {
    "text": "and I guess you all will\nhave the distinct pleasure of implementing this\nfor your homework.",
    "start": "1539150",
    "end": "1545090"
  },
  {
    "text": "Congratulations. But there are a lot\nof moving pieces here, and that was one of the\nmotivating reasons for why",
    "start": "1545090",
    "end": "1553130"
  },
  {
    "text": "DPO came to be,\nbasically, was that. PPO, turns out it was\na little bit tricky to get it to work for\na particular problem",
    "start": "1553130",
    "end": "1558950"
  },
  {
    "text": "that Rafael was initiating\nsome research on. So anyway, that's the\nbackground on RLHF.",
    "start": "1558950",
    "end": "1565575"
  },
  {
    "text": "And I'm going to\nleave it to Archit now to give an overview of PPO.",
    "start": "1565575",
    "end": "1570760"
  },
  {
    "text": "All right. Thanks, Eric. Is this working? OK, cool. All right, who's ready\nfor the fun math stuff?",
    "start": "1570760",
    "end": "1579020"
  },
  {
    "text": "So you saw the\nscary picture here. And really, the question\nwe wanted to start with",
    "start": "1579020",
    "end": "1584299"
  },
  {
    "text": "is, do we need to\ndo all this just to fine tune our model\naccording to human preferences. And unsurprisingly,\nthe answer is",
    "start": "1584300",
    "end": "1590810"
  },
  {
    "text": "going to be no, so be\nprepared for the ride. And yeah, we saw this\nobjective earlier.",
    "start": "1590810",
    "end": "1597050"
  },
  {
    "text": "And before we go\ninto the math, I want to just give a high\nlevel picture of what is going to happen here.",
    "start": "1597050",
    "end": "1604040"
  },
  {
    "text": "We had some reward function,\nwhich kind of told us what humans like and\nhumans do not like. And right now, we're\nparameterizing that",
    "start": "1604040",
    "end": "1610790"
  },
  {
    "text": "as a separate network,\nsaying that this will give us a score for which answer is\ngood and which answer is bad.",
    "start": "1610790",
    "end": "1615910"
  },
  {
    "text": "Now, really, can we\nleverage the idea that our language models\nhave these probabilities",
    "start": "1615910",
    "end": "1623120"
  },
  {
    "text": "over completions? And the completions right now\nrepresent any distribution over the internet.",
    "start": "1623120",
    "end": "1628260"
  },
  {
    "text": "But can we like overload\nit somehow to basically represent, can we only\nput probability on things",
    "start": "1628260",
    "end": "1633510"
  },
  {
    "text": "that humans like? And that's roughly the idea\nwe're going to try to exploit, is that there's, essentially,\na mapping between the language",
    "start": "1633510",
    "end": "1640350"
  },
  {
    "text": "model and the\nreward model itself, one-on-one mapping\nthat you can use to directly train the policy\non preferences themselves.",
    "start": "1640350",
    "end": "1647740"
  },
  {
    "text": "And towards the\nend of this, what you're going to have is a\ndistribution over responses that are not just arbitrary text\nresponses on the internet,",
    "start": "1647740",
    "end": "1655059"
  },
  {
    "text": "but responses that humans like. And that's where direct\npreference optimization will come in.",
    "start": "1655060",
    "end": "1660690"
  },
  {
    "text": "How do we do that? That's where the\nmath is going to be. So we saw the RLHF\nobjective, which is, essentially, we want to\nmaximize the expected reward",
    "start": "1660690",
    "end": "1667890"
  },
  {
    "text": "over completions, and we have a\nKL constraint to the reference distribution.",
    "start": "1667890",
    "end": "1673770"
  },
  {
    "text": "For now, we're just using\nany reward function. The math is going to hold\nfor any reward function, but in general, it's the\nlearned reward function.",
    "start": "1673770",
    "end": "1680910"
  },
  {
    "text": "Now, I don't know if this was\ncovered in the class or not, but it turns out that this\nequation or this problem",
    "start": "1680910",
    "end": "1687030"
  },
  {
    "text": "has a closed form solution.  OK, great.",
    "start": "1687030",
    "end": "1692485"
  },
  {
    "text": "I'm not going to derive it. Maybe I'll leave it as\nan exercise for people. But it's a fun derivation,\nand it's not too hard.",
    "start": "1692485",
    "end": "1699500"
  },
  {
    "text": "So hopefully, you should\nfind the time to do it. But really, if you've ever\nheard of Boltzmann distribution",
    "start": "1699500",
    "end": "1706210"
  },
  {
    "text": "or something of this form,\nthis is really just it. And this is not what\nwe're contributing.",
    "start": "1706210",
    "end": "1711679"
  },
  {
    "text": "This is a known result for a\nwhile and it's very intuitive. It might look\nscary for a second, but really, what it's saying\nis that we have the reference",
    "start": "1711680",
    "end": "1718000"
  },
  {
    "text": "distribution that\nwe started with, and we had some reward function. And really, what\nwe're doing is we're upgrading the responses by\nthe exponentiated reward.",
    "start": "1718000",
    "end": "1725200"
  },
  {
    "text": "So things which\nhave a higher reward will have a higher\nprobability according to the exponentiated reward. Now, if you just look\nat this is very simple,",
    "start": "1725200",
    "end": "1732515"
  },
  {
    "text": "but this won't be a\nprobability distribution. And the thing on\nthe left hand side is a probability distribution. So we normalize it by\nthis partition function,",
    "start": "1732515",
    "end": "1739789"
  },
  {
    "text": "which is the Z of x. Think of is as summing over\nevery completion for a given",
    "start": "1739790",
    "end": "1745570"
  },
  {
    "text": "question x. Now, you can imagine\nthat that's a very, very interactable quantity.",
    "start": "1745570",
    "end": "1751340"
  },
  {
    "text": "If I start computing\nevery sentence and try to measure\nthe probability, and then multiply it by\nan exponential reward,",
    "start": "1751340",
    "end": "1757050"
  },
  {
    "text": "that's basically not tractable. So this equation by\nitself is not very useful.",
    "start": "1757050",
    "end": "1765899"
  },
  {
    "text": "And I went over this. This is exactly the definition\nof the partition function. We're summing over\nevery response y.",
    "start": "1765900",
    "end": "1772503"
  },
  {
    "text": "The pi ref is the\ndistribution we started with and the exponentiated\nreward, and beta is the temperature term trading\nof the reward and the KL",
    "start": "1772503",
    "end": "1780180"
  },
  {
    "text": "constraint. So this is intractable. We'll hold on to the partition\nfunction for a second",
    "start": "1780180",
    "end": "1787544"
  },
  {
    "text": "and we'll see what\nhappens to it. But really, this result\nis a relationship between pi star\nand the reward r.",
    "start": "1787545",
    "end": "1792780"
  },
  {
    "text": "But now we can do a\nlittle bit of algebra and shuffle it around and\nrewrite the reward in terms",
    "start": "1792780",
    "end": "1798210"
  },
  {
    "text": "of the optimal policy itself. So what does this equation say? We're writing the reward in\nterms of the beta log ratio,",
    "start": "1798210",
    "end": "1806110"
  },
  {
    "text": "where the ratio is between\nthe optimal policy pi star and the reference\ndistribution we started with.",
    "start": "1806110",
    "end": "1811360"
  },
  {
    "text": "And then there's this pesky\npartition function that just continues to stay on there. I'm going to try to develop\nsome intuition here.",
    "start": "1811360",
    "end": "1818890"
  },
  {
    "text": "This is important. What it is saying is\nthat, if an optimal policy ways puts more\nprobability distribution",
    "start": "1818890",
    "end": "1824279"
  },
  {
    "text": "on a response than a\nreference distribution, the reward is higher. Does that come through?",
    "start": "1824280",
    "end": "1830110"
  },
  {
    "text": "And if a probability is lower,\nthen the reward is lower. And this is intuitively correct. This is how our reward\nfunction should also be.",
    "start": "1830110",
    "end": "1837220"
  },
  {
    "text": "If a response is\npreferred, then it should have a higher\nprobability and a higher reward. So you can see we are\nstarting to develop",
    "start": "1837220",
    "end": "1842720"
  },
  {
    "text": "a relationship between a reward\nfunction and the probability distribution itself. ",
    "start": "1842720",
    "end": "1848630"
  },
  {
    "text": "Cool. But the main problem here\nis that this is, by itself,",
    "start": "1848630",
    "end": "1853887"
  },
  {
    "text": "not very practical because the\npartition function, as we said, is just completely intractable. So maybe let's go\nback to what we",
    "start": "1853887",
    "end": "1860720"
  },
  {
    "text": "were doing in the RLHF process. The high level idea\nis that we have",
    "start": "1860720",
    "end": "1867476"
  },
  {
    "text": "a loss function and\na reward function, and we're going to use\nthis transformation. And once we plug\nit all together, we're going to get\na loss function",
    "start": "1867477",
    "end": "1873577"
  },
  {
    "text": "on the policies themselves. And if we go back to our loss\nfunction for the reward bit,",
    "start": "1873577",
    "end": "1879679"
  },
  {
    "text": "if you remember, the\nlogit is the difference between the rewards of\nthe preferred response and the dispreferred response.",
    "start": "1879680",
    "end": "1886260"
  },
  {
    "text": "This is what Eric covered\njust a little bit back. Now, if you look at\nthis, this difference is not going to depend on the\ninput itself or the partition",
    "start": "1886260",
    "end": "1895190"
  },
  {
    "text": "function itself if we\nlook at it explicitly. So this is exactly what\nis going to happen here,",
    "start": "1895190",
    "end": "1900740"
  },
  {
    "text": "is we're going to take the\nequation that we took earlier, express the reward in terms of\nthe policy we're going to learn,",
    "start": "1900740",
    "end": "1906050"
  },
  {
    "text": "and we're going to plug it\ninto the reward modeling loss. And once you compute\nthis difference,",
    "start": "1906050",
    "end": "1913440"
  },
  {
    "text": "this partition function is going\nto cancel out because it only depends on the input\nx, and it does not",
    "start": "1913440",
    "end": "1918860"
  },
  {
    "text": "depend on the output\nwe're computing it over. And when we do this, we get our\nfinal beautiful loss function,",
    "start": "1918860",
    "end": "1928620"
  },
  {
    "text": "which we called the\nDPO loss function. And really, is just a\nreward modeling loss.",
    "start": "1928620",
    "end": "1934140"
  },
  {
    "text": "And let's take a second\nto see what it is doing. What we're trying to do is, we\nhave a preferred response yw",
    "start": "1934140",
    "end": "1940820"
  },
  {
    "text": "and a disprefereed response\nyl for a given question x. And we're trying to\nmaximize this difference.",
    "start": "1940820",
    "end": "1949430"
  },
  {
    "text": "That's how we would\nminimize this loss. And maximizing this\ndifference means that our log probability\non the preferred response",
    "start": "1949430",
    "end": "1956390"
  },
  {
    "text": "should be higher\nthan the probability that the reference\ndistribution puts on it. And the log probability\nof the preferred response",
    "start": "1956390",
    "end": "1963150"
  },
  {
    "text": "should be lower\nthan the probability that the reference\ndistribution puts on it.",
    "start": "1963150",
    "end": "1968190"
  },
  {
    "text": "Does this make\nintuitive sense why this would change the\nprobabilities in the right way? ",
    "start": "1968190",
    "end": "1977835"
  },
  {
    "text": "Cool. And yeah, the log partition\nfunction basically just cancels out.",
    "start": "1977835",
    "end": "1983180"
  },
  {
    "text": " You can think of this as\nbeing a benefit to the fact",
    "start": "1983180",
    "end": "1989309"
  },
  {
    "text": "that you can shift the\nrewards by a constant. So often, that's considered\nnot a good thing. But here, they're leveraging it\nbecause you can just cancel out",
    "start": "1989310",
    "end": "1995845"
  },
  {
    "text": "the partition function. Yeah. All right. I'll hand it to Rafael, and\nyou can go over the results.",
    "start": "1995845",
    "end": "2002150"
  },
  {
    "text": "All right. Can you guys hear me? So this is sort of like the\nfirst control experiment",
    "start": "2002150",
    "end": "2008660"
  },
  {
    "text": "we ran on this project. And basically, we took\nthis IMDb reviewed data set, which is sort of\nlike movie reviews,",
    "start": "2008660",
    "end": "2015420"
  },
  {
    "text": "and we wanted to train the\nmodel to generate positive movie reviews. So we use the pre-trained\nsentiment classifier",
    "start": "2015420",
    "end": "2022527"
  },
  {
    "text": "as a gold reward function. In this case, we do know we have\naccess to the underlying reward score.",
    "start": "2022527",
    "end": "2028190"
  },
  {
    "text": "And then we generated a bunch\nof data from the base model, which was pre-trained\nFT, ranked it based on the\nsentiment classifier,",
    "start": "2028190",
    "end": "2034970"
  },
  {
    "text": "and created synthetic\npreferences. And then, basically, we just\ntook a bunch of baselines",
    "start": "2034970",
    "end": "2040550"
  },
  {
    "text": "across that data. And we fundamentally were\ninterested in comparing to what degree is DPO\nan actual good optimizer",
    "start": "2040550",
    "end": "2048469"
  },
  {
    "text": "of the core objective. Essentially, there's\nthis reward KL trade off underlying all of this.",
    "start": "2048469",
    "end": "2054629"
  },
  {
    "text": "And we basically wanted to\nsee how good of a Pareto curve can we extract from that\nto see, essentially,",
    "start": "2054630",
    "end": "2063429"
  },
  {
    "text": "DPO's optimal trade off here\nin this simple rule problem. Define Pareto curve.",
    "start": "2063429",
    "end": "2069370"
  },
  {
    "text": "I see. Yeah. Well, Pareto curve is a general\nconcept in economics and sort of decision analysis\nand things like that",
    "start": "2069370",
    "end": "2076070"
  },
  {
    "text": "where we have trade offs\nbetween several things, for example, in this\ncase, reward versus KL.",
    "start": "2076070",
    "end": "2082320"
  },
  {
    "text": "And we're interested\nin the optimal trade off that we can get. And we say, for\nexample, one method Pareto dominates, another\nmethod if, essentially, we",
    "start": "2082320",
    "end": "2090020"
  },
  {
    "text": "can get something, get\nmore without giving up on something else. So in this case,\nfor the same KL,",
    "start": "2090020",
    "end": "2096500"
  },
  {
    "text": "we can get more reward using\nDPO than another method.",
    "start": "2096500",
    "end": "2101550"
  },
  {
    "text": "And we actually played quite\na bit with the baselines here. I probably spent a\ncouple of months trying",
    "start": "2101550",
    "end": "2106860"
  },
  {
    "text": "to push these PPO numbers. And essentially, it works.",
    "start": "2106860",
    "end": "2112320"
  },
  {
    "text": "PPO kind of works, and you\nget some results there, but it can't quite catch\nup with the DPO objective.",
    "start": "2112320",
    "end": "2120670"
  },
  {
    "text": "And what I kind of wanted\nto include this curve here in this talk is, essentially,\nI think, even now,",
    "start": "2120670",
    "end": "2126609"
  },
  {
    "text": "basically, almost all\nof the RLHF papers that you read are actually doing\nevaluation potentially wrong.",
    "start": "2126610",
    "end": "2133140"
  },
  {
    "text": "Because you go read these\npapers and you kind of get the win rate, or you get\nthe comparisons, et cetera.",
    "start": "2133140",
    "end": "2138760"
  },
  {
    "text": "But none of them really\nplot these curves. For none of them, you\nreally don't know where along this trade off you are.",
    "start": "2138760",
    "end": "2145120"
  },
  {
    "text": "And that number in and\nof itself doesn't really tell you much because it's\na question of optimization.",
    "start": "2145120",
    "end": "2152550"
  },
  {
    "text": "And you don't know how well\nthat optimization worked or didn't work\njust by extracting one position on this curve.",
    "start": "2152550",
    "end": "2159330"
  },
  {
    "text": "So I think that's quite\nan important point that the community is still\nnot quite making as much.",
    "start": "2159330",
    "end": "2164440"
  },
  {
    "text": "But I think when any of\nthese new things come up, I think this is the fundamental\nquestion that should be asked.",
    "start": "2164440",
    "end": "2171060"
  },
  {
    "text": "Do you think it was because the\nreward model is misspecified, or which part do\nyou think it's--",
    "start": "2171060",
    "end": "2176970"
  },
  {
    "text": "where is RLHF\nreally in this case? Do you think the core\nmodel isn't that good, or the PPO optimization\nisn't that good?",
    "start": "2176970",
    "end": "2183299"
  },
  {
    "text": "So basically, if you\nlook at the purple thing, that's kind of like the\nout-of-the-box PPO, TRL.",
    "start": "2183300",
    "end": "2190569"
  },
  {
    "text": "And if you look at some of\nour implementation things, they do a lot better.",
    "start": "2190570",
    "end": "2197190"
  },
  {
    "text": "So the core difference there,\nand surprising to me, people have written like numerous\npapers about the same thing now.",
    "start": "2197190",
    "end": "2202380"
  },
  {
    "text": "And to me, it was\nsort of a footnote. How we got this to\nwork better, we just sampled more answers per prompt.",
    "start": "2202380",
    "end": "2207900"
  },
  {
    "text": "And essentially, that was\na question of variance. And in the RLHF setting,\nthe variance problem",
    "start": "2207900",
    "end": "2213540"
  },
  {
    "text": "is even higher because\nof the constant shift. So we actually did some\nanalysis around this when we writing the paper.",
    "start": "2213540",
    "end": "2218583"
  },
  {
    "text": "About 60% of the reward\nscores are noise, essentially. Sort a signal to noise in\nregular PPO is about 40%.",
    "start": "2218583",
    "end": "2225010"
  },
  {
    "text": "And when you mix in\nthe whole process, the variance like\ncompletely explodes. So it's a very sparse\nsignal to learn from there.",
    "start": "2225010",
    "end": "2231900"
  },
  {
    "text": " I'm just sorry. This is just a picture\nquestion, but I'm",
    "start": "2231900",
    "end": "2237059"
  },
  {
    "text": "having a hard time knowing\nhow do we read the graph. Is it better to\nhave higher reward,",
    "start": "2237060",
    "end": "2242369"
  },
  {
    "text": "or what is this graph actually\ntelling us about each metric? Yeah, so obviously, it's\nbetter to have higher reward.",
    "start": "2242370",
    "end": "2247890"
  },
  {
    "text": "This is a core concept of\nreinforcement learning. You want to maximize reward. But essentially,\nfrom the RLHF setup,",
    "start": "2247890",
    "end": "2253440"
  },
  {
    "text": "we maximize the reward\nsubject to a KL constraint, subject to some KL cost. And what this graph is\nsaying, basically, it's",
    "start": "2253440",
    "end": "2259980"
  },
  {
    "text": "plotting it for a\nlevel of KL using each of these baselines,\nhow much reward can I get. And you want that\nto be, basically,",
    "start": "2259980",
    "end": "2266748"
  },
  {
    "text": "as I said prior,\noptimal in the sense that you want to get the most\nreward for a certain level of KL. And the other point\nI made is, basically,",
    "start": "2266748",
    "end": "2273000"
  },
  {
    "text": "people compare only win rates\nor, essentially, the reward, but they don't\ntell you about KL. So you can compare,\nfor example--",
    "start": "2273000",
    "end": "2279210"
  },
  {
    "text": "oops, sorry. You can compare this DPO\npoint to this PPO point,",
    "start": "2279210",
    "end": "2284770"
  },
  {
    "text": "and this PPO point\nwill appear better because it has more reward. But fundamentally, as an\noptimization algorithm,",
    "start": "2284770",
    "end": "2290430"
  },
  {
    "text": "that's not the case.  You said our model\nis interpretable.",
    "start": "2290430",
    "end": "2297160"
  },
  {
    "text": "Or can you maybe explain\nwhat's going on under the hood? Or it's just optimization math?",
    "start": "2297160",
    "end": "2302930"
  },
  {
    "text": "What do you mean by interpret? So if you provide feedback,\nthe behavior will change. Can you explain\nthe whole process,",
    "start": "2302930",
    "end": "2308970"
  },
  {
    "text": "or you can not explain\nthe whole situation? If you put in noisy data, for\nexample, can you debug it?",
    "start": "2308970",
    "end": "2316940"
  },
  {
    "text": "You spend the whole process? Yeah, I think that's a\nmore complicated question than it seems on the surface.",
    "start": "2316940",
    "end": "2322830"
  },
  {
    "text": "There's whole lines of\nresearch on, basically, average of noisy feedback, average\nof multimodal feedback,",
    "start": "2322830",
    "end": "2328210"
  },
  {
    "text": "plurality of alignment. So it's not quite like an answer\nI can give in one sentence.",
    "start": "2328210",
    "end": "2333750"
  },
  {
    "text": "It's a lot. Yeah. Can you explain again\nwhy we have a bad signal to noise ratio in normal PPO?",
    "start": "2333750",
    "end": "2341250"
  },
  {
    "text": "It's a long question. There's a whole\nsection in the paper. It's about half a page. ",
    "start": "2341250",
    "end": "2349230"
  },
  {
    "text": "But essentially, by sampling\nmore answers per response, it kind of goes away.",
    "start": "2349230",
    "end": "2356070"
  },
  {
    "text": "Can you explain what the reward\nmeans for sentiment generation? It was basically the\nsentiment of the sentence.",
    "start": "2356070",
    "end": "2362680"
  },
  {
    "text": "And 1 is very good sentiment. 0 is very bad sentiment. So like movie reviews.",
    "start": "2362680",
    "end": "2370080"
  },
  {
    "text": "This one's hopefully\na one sentencer. For our appreciation of the\ngraph, about what KL divergence",
    "start": "2370080",
    "end": "2376050"
  },
  {
    "text": "trade off would you choose\nin a real model here? Is it that you might\nchoose something like 10",
    "start": "2376050",
    "end": "2381598"
  },
  {
    "text": "so we're really in that region? Or is it somewhere much farther? It's very much model\nand data dependent.",
    "start": "2381598",
    "end": "2387520"
  },
  {
    "text": "OK. Yeah, this graph means\nabsolutely nothing in a summarization set. Gotcha. Yeah.",
    "start": "2387520",
    "end": "2392660"
  },
  {
    "text": " I think it's very hard\nto choose a specific KL. But usually, what\npeople do is measure",
    "start": "2392660",
    "end": "2398880"
  },
  {
    "text": "performance on other\nbenchmarks they care about. And usually, if they find\nthat, if the KL is smaller,",
    "start": "2398880",
    "end": "2404770"
  },
  {
    "text": "the performance on other\nbenchmarks is preserved. So you typically try to err\non the side of lower KL.",
    "start": "2404770",
    "end": "2412829"
  },
  {
    "text": "And there's no specific number. But wherever you find your\nMMLE performance is great,",
    "start": "2412830",
    "end": "2419090"
  },
  {
    "text": "that's where you stop. OK, in the interest of time, we\nhad a bunch of other experiments",
    "start": "2419090",
    "end": "2425170"
  },
  {
    "text": "in the paper, which show\nbasically how DPO works. But I think, really, the\ntestament to the algorithm",
    "start": "2425170",
    "end": "2431080"
  },
  {
    "text": "is it's kind of\nbeen widely adopted more in the community\nand larger scale.",
    "start": "2431080",
    "end": "2436450"
  },
  {
    "text": "This was maybe a\nlittle outdated. I haven't looked\nat this recently. But couple of months\nago, this was basically",
    "start": "2436450",
    "end": "2442060"
  },
  {
    "text": "the open LLM leaderboard\non Hugging Face, basically, the leaderboard\nof open language models. And I think 9 out of the top 10\nmodels were trained with DPO.",
    "start": "2442060",
    "end": "2450859"
  },
  {
    "text": "And this is the open\nsource community. And since then, even\ninstitutions have taken this up.",
    "start": "2450860",
    "end": "2457210"
  },
  {
    "text": "In particular, this is\ntaken from the Mistral paper that basically used\nDPO exclusively as their RLHF algorithm.",
    "start": "2457210",
    "end": "2463960"
  },
  {
    "text": "And as you know, basically, some\nof the strong Mistral models are somewhat competitive\nwith GPT 4, for example.",
    "start": "2463960",
    "end": "2469935"
  },
  {
    "text": "So we do definitely\nhave evidence that this works at\nvery large scales. And from basically\nlast week, we w",
    "start": "2469935",
    "end": "2475300"
  },
  {
    "text": "know even LLaMa3 is using DPO\nas part of its optimization pipeline. Interestingly enough, they're\nactually using it with mixed",
    "start": "2475300",
    "end": "2481587"
  },
  {
    "text": "with other things. So basically, the TLDR is this\nkind of algorithm sort of works.",
    "start": "2481587",
    "end": "2487390"
  },
  {
    "text": "And we're seeing\nit taken up more and being used for\nmore and more things.",
    "start": "2487390",
    "end": "2492910"
  },
  {
    "text": "So this is kind of\nwhere the paper ends. Since then, there's been\nlike a ton of other works that we have done and\nother people have done.",
    "start": "2492910",
    "end": "2499690"
  },
  {
    "text": "Thought a lot about what to talk\nabout this from those works. For example, I\nheard you guys have",
    "start": "2499690",
    "end": "2505600"
  },
  {
    "text": "learned inverse max entropy,\ninverse reinforcement learning. You can actually derive DPO as\na inverse Q-learning algorithm",
    "start": "2505600",
    "end": "2512530"
  },
  {
    "text": "in a max entropy RL setting. Sounds actually trivial,\nbut it is possible. And that paper is called Your\nLanguage Model is Secretly",
    "start": "2512530",
    "end": "2521410"
  },
  {
    "text": "a Q Function. So for example, you can do that. I heard you're going to use\nRLHF on control problems.",
    "start": "2521410",
    "end": "2530950"
  },
  {
    "text": "I don't know. I haven't talked with the TAs. But actually, DPO does\nnot work for control",
    "start": "2530950",
    "end": "2537100"
  },
  {
    "text": "under the classical\nformulation, and you need formulation of\npreferences under regret,",
    "start": "2537100",
    "end": "2542200"
  },
  {
    "text": "rather than the\nreward functions. So hoping they've taken\nthat into account. That's a whole\nseparate other work.",
    "start": "2542200",
    "end": "2547490"
  },
  {
    "text": "But I guess what I\ndecided to focus on is this sort of DPO\nversus PPO debate, which",
    "start": "2547490",
    "end": "2552670"
  },
  {
    "text": "is going to be raging a lot on\nin the community, in industry, very much on Twitter.",
    "start": "2552670",
    "end": "2559420"
  },
  {
    "text": "And I kind of want to give\nyou my perspective for this. And I don't want to\nsound egocentric, but I think pretty much\nthe entire debate is wrong.",
    "start": "2559420",
    "end": "2566325"
  },
  {
    "text": " Let's skip that for now.",
    "start": "2566325",
    "end": "2571990"
  },
  {
    "text": "But basically,\nthere's two things. DPO fits this implicit reward\nfunction, which Archit showed.",
    "start": "2571990",
    "end": "2577059"
  },
  {
    "text": "You can think about this as\nfitting a particular reward model. And there are two\nquestions there.",
    "start": "2577060",
    "end": "2582349"
  },
  {
    "text": "The first question is, is\nthis implicit reward function as good as an explicitly\nparameterized reward function.",
    "start": "2582350",
    "end": "2588280"
  },
  {
    "text": "A similar question is, for\nthis implicit reward model the DPO fits, you\ncan analytically extract the optimal policy.",
    "start": "2588280",
    "end": "2595329"
  },
  {
    "text": "So basically, what I can do\nis I can get the DPO policy, or I can take the DPO\nimplicit reward function,",
    "start": "2595330",
    "end": "2602570"
  },
  {
    "text": "put it into PPO, and run\nthat optimization loop. Under perfect optimization,\nabsolutely perfect optimization,",
    "start": "2602570",
    "end": "2608780"
  },
  {
    "text": "I'll get back the DPO policy\ndirectly if my PPO is perfect. But that is rarely the case\nwith any sort of machine",
    "start": "2608780",
    "end": "2614973"
  },
  {
    "text": "learning optimization. So we get something\nthat's suboptimal. And this suboptimality\ninduces some sort",
    "start": "2614973",
    "end": "2620049"
  },
  {
    "text": "of regularization effect\nthat makes my model stronger. So these are the two\nbig questions, I think,",
    "start": "2620050",
    "end": "2626780"
  },
  {
    "text": "in this debate. So they've been kind\nof tackled recently.",
    "start": "2626780",
    "end": "2632092"
  },
  {
    "text": "There's this thing\ncalled came out, Reward Bench, which\nis a large scale evaluation of reward models.",
    "start": "2632092",
    "end": "2638350"
  },
  {
    "text": "And it has DPO is both a\ngenerative and a reward model, discriminative model. You can evaluate DPO\nmodels as rewards.",
    "start": "2638350",
    "end": "2645579"
  },
  {
    "text": "And basically, on\nseveral scores here, we have this chat, safety,\nreasoning, type of task.",
    "start": "2645580",
    "end": "2652279"
  },
  {
    "text": "So this, for example,\nshows scoring reward, scoring preferences based\non dialogue and chat.",
    "start": "2652280",
    "end": "2659200"
  },
  {
    "text": "You can see the top four\nmodels are all DPO models and outperform, for example,\nproprietary models, much bigger",
    "start": "2659200",
    "end": "2665580"
  },
  {
    "text": "and sort of closed source ones. And on reasoning, the top model\nis this proprietary Cohere",
    "start": "2665580",
    "end": "2671250"
  },
  {
    "text": "model. But the next five\nare all DPO models.",
    "start": "2671250",
    "end": "2677301"
  },
  {
    "text": "And obviously, there's\nalways more work to be done, more\nresearch to be done. But in my mind,\nthis sort of work",
    "start": "2677301",
    "end": "2683700"
  },
  {
    "text": "solidified this, that the DPO\nimplicit reward is about as good as the classic RLHF reward. We're not losing generality.",
    "start": "2683700",
    "end": "2689828"
  },
  {
    "text": "We're not losing\ncapability for considering this implicit model versus an\nexplicit parameterized one.",
    "start": "2689828",
    "end": "2695190"
  },
  {
    "text": "So the other big\nquestion is then, does using a weaker\noptimizer, such as PPO, provide a better solution, gives\nyou some sort of regularization.",
    "start": "2695190",
    "end": "2704040"
  },
  {
    "text": "And basically, started to\nlook more into this recently.",
    "start": "2704040",
    "end": "2710310"
  },
  {
    "text": "Some of the first\nfeedback we got on DPO was, someone tried to train like\na very large scale DPO model.",
    "start": "2710310",
    "end": "2716350"
  },
  {
    "text": "And what they said\nwas, it does well, and then it becomes\nmore and more verbose,",
    "start": "2716350",
    "end": "2721414"
  },
  {
    "text": "and then starts\nspeaking more and more. And at some point,\nit reaches a point where it just won't stop and\njust kind of goes off the rails.",
    "start": "2721415",
    "end": "2728535"
  },
  {
    "text": "It just can't stop talking. And we looked at this on two\ndata sets, one on summarization, one on dialogue.",
    "start": "2728535",
    "end": "2734420"
  },
  {
    "text": "And what you can see\nhere is the distribution of lengths of answers.",
    "start": "2734420",
    "end": "2739880"
  },
  {
    "text": "And the blue distribution\nis the preferred answer, and the red distribution\nis the dispreferred answer. So we can see there's a very\nslight bias towards longer",
    "start": "2739880",
    "end": "2748240"
  },
  {
    "text": "responses. People have biases. They prefer more\nverbose answers. They prefer more like\nverbose, longer summaries,",
    "start": "2748240",
    "end": "2755060"
  },
  {
    "text": "et cetera, et cetera. But once we train with\nDPO, under every column is a separate level\nof regularization.",
    "start": "2755060",
    "end": "2760870"
  },
  {
    "text": "Under any level\nof regularization, this is blown way\nout of proportion. It's not only DPO is\nallocating probability mass",
    "start": "2760870",
    "end": "2768730"
  },
  {
    "text": "within the distribution. Basically, this green\nhistogram is the DPO length.",
    "start": "2768730",
    "end": "2774380"
  },
  {
    "text": "It's pushing things way\nout of distribution. And you see, now we have\nanswers which are significantly",
    "start": "2774380",
    "end": "2779680"
  },
  {
    "text": "outside of the distribution\nthat's covered in our data set. So what is happening there? And there is this concept\nof reward hacking.",
    "start": "2779680",
    "end": "2785963"
  },
  {
    "text": "I don't know if you've\ncovered reward hacking. But there's a very famous paper\nfrom OpenAI called Scaling Laws",
    "start": "2785963",
    "end": "2793240"
  },
  {
    "text": "for Reward Model Optimization. And what they did there is\nessentially the sentiment experiment, but a larger scale.",
    "start": "2793240",
    "end": "2798640"
  },
  {
    "text": "They got some real\nhuman preferences. They trained a reward model, a\nvery good, very strong reward model.",
    "start": "2798640",
    "end": "2804069"
  },
  {
    "text": "And then they use\nthat reward model to annotate some synthetic\ndata, synthetic preferences. And then they repeated\nthe whole RLHF process",
    "start": "2804070",
    "end": "2810760"
  },
  {
    "text": "on top of the\nsynthetic preferences. And this is what\nthey discovered.",
    "start": "2810760",
    "end": "2815800"
  },
  {
    "text": "So basically, what\nthis graph is, is the same graph I showed\nearlier for sentiment, except that the x-axis\nis a KL constraint,",
    "start": "2815800",
    "end": "2823570"
  },
  {
    "text": "and the y-axis is rewards. And these things, the\ndashed things you see",
    "start": "2823570",
    "end": "2828880"
  },
  {
    "text": "are the learned reward\nfunctions in PPO, basically, the expected reward\nfrom your model training.",
    "start": "2828880",
    "end": "2834130"
  },
  {
    "text": "And the solid lines are the\nactual gold reward models. So what you're seeing from\na reinforcement learning",
    "start": "2834130",
    "end": "2840400"
  },
  {
    "text": "perspective, it looks like the\nmodels are doing really well. It's maximizing\nreward quite a bit. But actually, its quality is\neither stagnating or going down.",
    "start": "2840400",
    "end": "2848540"
  },
  {
    "text": "And this concept\nof reward hacking has become quite\nprominent since then, both for practical purposes,\nbut for example, the AI safety",
    "start": "2848540",
    "end": "2856568"
  },
  {
    "text": "community is very\nworried about this, the whole like paper clipping\nthing, if you've heard about it, and the way that, basically,\nthe model can find a way",
    "start": "2856568",
    "end": "2862978"
  },
  {
    "text": "to exploit these\nreward functions, such that it thinks it's\ndoing something good while it's actually\ndoing something very bad.",
    "start": "2862978",
    "end": "2869320"
  },
  {
    "text": "And basically, these\nthings are well understood. This paper has something\nlike 200 citations. A ton of work has been done\non mitigating these things.",
    "start": "2869320",
    "end": "2876950"
  },
  {
    "text": "And the thinking there\nis, in classical RLHF, I'm learning a reward function. I have a proxy reward,\nand I'm continuously",
    "start": "2876950",
    "end": "2883842"
  },
  {
    "text": "querying that reward with new\ndata, which might make it out of distribution, which\nmight kick it off course, et cetera, et cetera.",
    "start": "2883842",
    "end": "2889549"
  },
  {
    "text": "So it's not surprising\nthat this happens. I think by and large, the\ncommunity has not realized yet",
    "start": "2889550",
    "end": "2895750"
  },
  {
    "text": "that this happens\nin direct alignment, as well, because, A, there's\nno proxy reward function. You're directly optimizing\nthe model on the data.",
    "start": "2895750",
    "end": "2902000"
  },
  {
    "text": "And B, there's no new data. There's no synthetic\ndata being sampled. It's all within the data set.",
    "start": "2902000",
    "end": "2907257"
  },
  {
    "text": "But what we have discovered,\nand essentially, this is a new result that we are\ncurrently still developing, is that, actually,\nreward hacking seems",
    "start": "2907257",
    "end": "2913690"
  },
  {
    "text": "to be quite prominent\nin DPO, and actually, all of the DPO variants, things\nlike IPO and slick, as well,",
    "start": "2913690",
    "end": "2919470"
  },
  {
    "text": "do this. Have you heard of those? And actually, it might even\nbe more prominent than PPO,",
    "start": "2919470",
    "end": "2926470"
  },
  {
    "text": "because PPO is a\nweaker optimizer. So you have to push\nreally hard to really hit",
    "start": "2926470",
    "end": "2932470"
  },
  {
    "text": "those tail of the\nreward function. But DPO gives you the exactly\noptimal analytical function.",
    "start": "2932470",
    "end": "2937820"
  },
  {
    "text": "So in a sense, it sort of\nalmost hacks in an absolute way.",
    "start": "2937820",
    "end": "2943900"
  },
  {
    "text": "So yeah, this is\ncurrently, I think, part of the dialogue\nand the research",
    "start": "2943900",
    "end": "2949630"
  },
  {
    "text": "that the community is not\nquite figuring out yet. And that's my goal\nto put these things out that this same reward\nhacking phenomena, very",
    "start": "2949630",
    "end": "2956690"
  },
  {
    "text": "surprisingly, because\nit sort of goes against all the intuition\nwe've had from before, happens in these sort\nof algorithms, as well.",
    "start": "2956690",
    "end": "2962470"
  },
  {
    "start": "2962470",
    "end": "2967990"
  },
  {
    "text": "Right. So it's kind of the\nsame type of plot you see on the left, the\nx-axis, the KL divergence.",
    "start": "2967990",
    "end": "2973150"
  },
  {
    "text": "And y-axis here is GPT 4\nwin rate, so basically, judgments by GPT 4. And each checkpoint, each data\nis like a different checkpoint",
    "start": "2973150",
    "end": "2979710"
  },
  {
    "text": "evaluated to train with DPO. And kind of similar\nto before, you see that, basically, it's\ndifferent model sizes.",
    "start": "2979710",
    "end": "2986490"
  },
  {
    "text": "And these are different data,\nbut what I'm pointing out here is the pattern, this\ncomb shaped pattern.",
    "start": "2986490",
    "end": "2992250"
  },
  {
    "text": "You kind of see, the more you\ntrain, sort of like the higher KL you go. Actually, your performance\ndoesn't improve. It goes down.",
    "start": "2992250",
    "end": "2997680"
  },
  {
    "text": "So it's the same reward\nhacking phenomenon. The theory tells you that\nthis thing should be monotone.",
    "start": "2997680",
    "end": "3003110"
  },
  {
    "text": "You give up some KL,\nyou get some reward. But that's not the case. And kind of the\npoint here is this",
    "start": "3003110",
    "end": "3008840"
  },
  {
    "text": "seems to be more\nprevalent in this-- ",
    "start": "3008840",
    "end": "3017390"
  },
  {
    "text": "Technically, the\nDPO reward function is just as good as any\nother reward function. But if you're\noptimizing it too much,",
    "start": "3017390",
    "end": "3024450"
  },
  {
    "text": "we might be in this\nreward hacking phenomenon. And this is where,\npotentially, a PPO optimization could be more stable or could\nbe more beneficial because it's",
    "start": "3024450",
    "end": "3032390"
  },
  {
    "text": "a weaker optimizer,\nessentially, from a [INAUDIBLE]. So yeah, I think\nthis is sort of where",
    "start": "3032390",
    "end": "3038369"
  },
  {
    "text": "we are with these type\nof algorithms right now. And I think there's kind of\nexciting work to be done again.",
    "start": "3038370",
    "end": "3046140"
  },
  {
    "text": "In conclusion, yeah,\nwe saw of these things. But I think it's\nkind of interesting",
    "start": "3046140",
    "end": "3052510"
  },
  {
    "text": "what the next steps are. A ton of work has gone\ninto making RLHF robust.",
    "start": "3052510",
    "end": "3060100"
  },
  {
    "text": "Basically, now we're showing\nthat these alignment algorithms are very prone to\nreward hacking, as well. So I think a lot\nof work will need",
    "start": "3060100",
    "end": "3066160"
  },
  {
    "text": "to be done to make direct\nalignment algorithms robust, as well. There's a lot more interest,\nas Professor Brusco",
    "start": "3066160",
    "end": "3072609"
  },
  {
    "text": "mentioned, on the online\nfine tuning algorithms. How do we elicit preferences? How do we actually fine tune\nthese things efficiently?",
    "start": "3072610",
    "end": "3080120"
  },
  {
    "text": "There's been explosion of RLHF\nacross modalities, not just language models. We've done vision\nlanguage models.",
    "start": "3080120",
    "end": "3085333"
  },
  {
    "text": "We've done diffusion models. In particular, Stable\nDiffusion 3, for example, is also trained with DPO. We've done text to image.",
    "start": "3085333",
    "end": "3091460"
  },
  {
    "text": "There's text to video\nwork being done. Potentially, speech and music is\nour next frontier to be tackled.",
    "start": "3091460",
    "end": "3097570"
  },
  {
    "text": "In a couple of weeks,\nwe'll be releasing a paper on protein\nsynthesis with feedback",
    "start": "3097570",
    "end": "3103270"
  },
  {
    "text": "and actively working\non things like robot safety for things like\nlarge scale robotics foundation models.",
    "start": "3103270",
    "end": "3109160"
  },
  {
    "text": "We're trying to do\nmulti-turn interactions, which classical RLHF cannot do\nand things like agents to use.",
    "start": "3109160",
    "end": "3116360"
  },
  {
    "text": "And all those things\nare, basically, things that are in the pipeline\nand we're looking into.",
    "start": "3116360",
    "end": "3121550"
  },
  {
    "text": "So I think there's kind of\na lot of exciting things that are happening\nin this field, and still, it's\nbeen on for a while. But I think only now,\nwe're just starting",
    "start": "3121550",
    "end": "3128098"
  },
  {
    "text": "to get deeper into understand\na lot of the finer points of these alignment algorithms.",
    "start": "3128098",
    "end": "3134510"
  },
  {
    "text": "I'm sorry if we run a\nlittle bit over time. Yeah. ",
    "start": "3134510",
    "end": "3144448"
  },
  {
    "text": "That's great. You got some time for questions. So I want to see\nif you haven any. ",
    "start": "3144448",
    "end": "3151830"
  },
  {
    "text": "Isn't the reward\nhacking implicitly used by the Bradley\nTerry model itself?",
    "start": "3151830",
    "end": "3158190"
  },
  {
    "text": "It's a sort of work\nin and of itself. It's a finite data\ntype of issue. If you have uniform data\ncoverage over everything,",
    "start": "3158190",
    "end": "3166450"
  },
  {
    "text": "reward hacking will go away. But it is fundamentally\na finite data thing. Because you have ratios\nor exponentiated ratios",
    "start": "3166450",
    "end": "3175080"
  },
  {
    "text": "in the reward formulation, and\nyou're using that everywhere, because your model will\ntry to maximize that,",
    "start": "3175080",
    "end": "3182560"
  },
  {
    "text": "it will essentially\ntry to skew that ratio.",
    "start": "3182560",
    "end": "3187860"
  },
  {
    "text": "I'm wondering, if you\nhad some other reward function, then maybe--",
    "start": "3187860",
    "end": "3193583"
  },
  {
    "text": "Yeah it could still happens. So if you use hinge\nobjective, it still happens. If you use a square type\nobjective, it still happens.",
    "start": "3193583",
    "end": "3200108"
  },
  {
    "text": "And basically think\nabout why this happens. So if you think about\nit as, you see that you have cheetah running along.",
    "start": "3200108",
    "end": "3205980"
  },
  {
    "text": "And basically, imagine\nyou have cheetah, and your target's running\nat a target speed of 10. And you run a target speed of\n8, that's better than running",
    "start": "3205980",
    "end": "3213829"
  },
  {
    "text": "a target speed of 7. Learning speed of\n9 is running better than a target speed\nof 8, et cetera.",
    "start": "3213830",
    "end": "3218920"
  },
  {
    "text": "Then you think\nabout, well, probably running a target speed of 11 is\nbetter than target speed of 10. But you've never seen anything\nrunning a target speed of 11,",
    "start": "3218920",
    "end": "3225350"
  },
  {
    "text": "so you're just extrapolating\nin a way that's just wrong. It's basically\nlike this picture.",
    "start": "3225350",
    "end": "3230810"
  },
  {
    "text": "Right? We think long things\nare better, so longer thing is always better, too. Is there a question there?",
    "start": "3230810",
    "end": "3237680"
  },
  {
    "text": "Yeah, it's kind of\na niche question. But I'm kind of wondering, so\nwhat if for a particular prompt,",
    "start": "3237680",
    "end": "3243520"
  },
  {
    "text": "all of the samples aren't\nthat great, but obviously, whoever's ranking them\nhas to rank all of them",
    "start": "3243520",
    "end": "3249790"
  },
  {
    "text": "and doesn't have any way of\nindicating that even the best sample isn't that great? I was wondering\nif there is a way",
    "start": "3249790",
    "end": "3255250"
  },
  {
    "text": "to account for that, any sort of\nweighting that could be applied to the rankings that would\nindicate that the rankings are",
    "start": "3255250",
    "end": "3263080"
  },
  {
    "text": "more or less confident overall. No, I don't have the means.",
    "start": "3263080",
    "end": "3269799"
  },
  {
    "text": "Feel free to interject, but\nthat's a great question. I think general problem around--",
    "start": "3269800",
    "end": "3274810"
  },
  {
    "text": "this is almost like the\nexploration problem in RL, is if you do not like ever\nsee good trajectories,",
    "start": "3274810",
    "end": "3282200"
  },
  {
    "text": "what are you going\nto learn without it. I don't have any easy\nanswers, frankly. But I think some\nthings that work",
    "start": "3282200",
    "end": "3288160"
  },
  {
    "text": "is, there's other forms\nof feedback, as well. So this is like\ncomparative feedback, where you're comparing two things.",
    "start": "3288160",
    "end": "3294320"
  },
  {
    "text": "But you can give\nthumbs up, thumbs down. And then if all of\nthem are just bad, you can indicate, optimize\nin a different way,",
    "start": "3294320",
    "end": "3301600"
  },
  {
    "text": "such that you're down weighting\nmost of the responses.",
    "start": "3301600",
    "end": "3308350"
  },
  {
    "text": "But yeah, this is a good\nopen problem to look at. I think the exclamation\npointed to it,",
    "start": "3308350",
    "end": "3313540"
  },
  {
    "text": "in that one thing that people\nask a lot is, how can PPO work. Because with PPO, you get\nto sample from your policy",
    "start": "3313540",
    "end": "3320050"
  },
  {
    "text": "during training so\nyou can explore, and that has to be helpful. Right? DPO is just from your\nfixed preference data set,",
    "start": "3320050",
    "end": "3326470"
  },
  {
    "text": "and you're never\nsampling during training. But I think your question\nactually points out the fact that, in some sense,\nbecause we have",
    "start": "3326470",
    "end": "3333070"
  },
  {
    "text": "this issue of we're optimizing\nonly a proxy reward, we don't get to optimize\nthe real reward. The important\nexploration is actually",
    "start": "3333070",
    "end": "3339670"
  },
  {
    "text": "the exploration we do when\nwe gather the data that we're getting preferences over that\nwe're going to learn our reward function from.",
    "start": "3339670",
    "end": "3345170"
  },
  {
    "text": "Because if we do good\nexploration at policy training time, but we sample\nsome great trajectory that our reward model doesn't\ncorrectly label as good,",
    "start": "3345170",
    "end": "3352270"
  },
  {
    "text": "it doesn't help us. So yeah, in that\nsense, it's basically an exploration problem,\nand it's very important.",
    "start": "3352270",
    "end": "3357468"
  },
  {
    "text": "That's why I think\nthese sort of multi-term or an iterative process\ncould be really helpful.",
    "start": "3357468",
    "end": "3363850"
  },
  {
    "text": "Let's go to the middle. Yeah. Do you think a similar\nidea could be applied",
    "start": "3363850",
    "end": "3369220"
  },
  {
    "text": "for, say, multi-step\nkind of reward, in which you get a reward\nafter multiple steps,",
    "start": "3369220",
    "end": "3374560"
  },
  {
    "text": "and you have a preference\nat the final step? But the reward\nfunction was explicitly",
    "start": "3374560",
    "end": "3381490"
  },
  {
    "text": "comparing two preferences\nbetween exactly two.",
    "start": "3381490",
    "end": "3387820"
  },
  {
    "text": "Can you repeat that? I just didn't quite\ncatch the question. I think if you have a multi-step\nkind of reasoning process",
    "start": "3387820",
    "end": "3394119"
  },
  {
    "text": "and a reward, which\ncomes at the end of that, would this idea apply?",
    "start": "3394120",
    "end": "3400570"
  },
  {
    "text": "Yeah, it does work. As I said, you can think of\nthis as a Q-learning problem,",
    "start": "3400570",
    "end": "3406030"
  },
  {
    "text": "actually. That is, however,\nnot trivial to show. But it does work.",
    "start": "3406030",
    "end": "3411529"
  },
  {
    "text": "If you have a problem\nwhere, basically, you have a sparse reward\nat the end, the model does end up doing some\nsort of credit assignment",
    "start": "3411530",
    "end": "3418190"
  },
  {
    "text": "on the intermediate tokens. If you think of this\nas a per token MDP, you will end up\nwith something that",
    "start": "3418190",
    "end": "3423650"
  },
  {
    "text": "does something interesting\nfor those intermediate steps. It's not doing\nexplicit bootstrapping,",
    "start": "3423650",
    "end": "3428900"
  },
  {
    "text": "obviously, but you do\nend up with some sort of credit assignment. And there are either\nseveral results now",
    "start": "3428900",
    "end": "3435920"
  },
  {
    "text": "showing that, if you have\nsequence level rewards, you can end up doing something\ninteresting, even though you",
    "start": "3435920",
    "end": "3444890"
  },
  {
    "text": "don't have this\nintermediate rewards. Do you have a question next? ",
    "start": "3444890",
    "end": "3455620"
  },
  {
    "text": "Yeah. Can you go a few\nslides back, when you talk about the synthetic data?",
    "start": "3455620",
    "end": "3460720"
  },
  {
    "text": "Yeah, there. Can you just explain,\nagain, what the difference is between the\nreal and synthetic,",
    "start": "3460720",
    "end": "3466390"
  },
  {
    "text": "what they're doing\nin both cases? Yeah, it's part of the same\nsort of sentiment problem",
    "start": "3466390",
    "end": "3471430"
  },
  {
    "text": "I was talking about before. They had real human data and\ntrain a reward function on this. So they want to be able\nto measure the real reward",
    "start": "3471430",
    "end": "3478060"
  },
  {
    "text": "function. So they get this\ngoal reward model, which is trained on this\nreal human comparisons,",
    "start": "3478060",
    "end": "3483640"
  },
  {
    "text": "and they generate data\nfrom their base model, and rank that data using\nthis code reward function.",
    "start": "3483640",
    "end": "3489145"
  },
  {
    "text": "So essentially,\nthey have access. They can query the\ngoal reward function and know what the actual score\nof these synthetic generations",
    "start": "3489145",
    "end": "3495790"
  },
  {
    "text": "is. So basically, they can\nessentially create these graphs. So the reason we're\ngetting reward hacking here",
    "start": "3495790",
    "end": "3502060"
  },
  {
    "text": "is because we're not using\nthe actual reward function? We're using this\nsynthetic reward function?",
    "start": "3502060",
    "end": "3507940"
  },
  {
    "text": "If you train on\nany reward function with a finite amount\nof data, and if you do,",
    "start": "3507940",
    "end": "3513130"
  },
  {
    "text": "in the limit of infinite\ndata, you would probably not see this phenomena. But because you're\ntraining on finite data,",
    "start": "3513130",
    "end": "3518872"
  },
  {
    "text": "there will be errors\noutside the distribution that it's trained on. And some errors would\nlike skew positive",
    "start": "3518872",
    "end": "3524450"
  },
  {
    "text": "or overestimate the reward. Some would skew,\nunderestimate the reward. But because you're\noptimizing against that,",
    "start": "3524450",
    "end": "3531150"
  },
  {
    "text": "you'll end up giving\nresponses where the errors are skewing positive.",
    "start": "3531150",
    "end": "3536660"
  },
  {
    "text": "So that's why you start\nseeing phenomena where your learned reward\nis increasing,",
    "start": "3536660",
    "end": "3542300"
  },
  {
    "text": "but your true reward\nis actually decreasing. If you guys think\nback to DAGGER, where they saw this propagating\nherds and supervised learning,",
    "start": "3542300",
    "end": "3550200"
  },
  {
    "text": "it's the same thing. Here's also another interesting\ntidbit of information. All these checkpoints,\neven the very high KL ones",
    "start": "3550200",
    "end": "3558290"
  },
  {
    "text": "that have quite low\nsuccess rates, actually have very low losses\nand very high accuracies",
    "start": "3558290",
    "end": "3564859"
  },
  {
    "text": "as reward functions. So basically, the quality\nof the reward function is not necessarily\nconnected to the performance",
    "start": "3564860",
    "end": "3572180"
  },
  {
    "text": "of the downstream policy, which\nis quite a surprising result, I would say.",
    "start": "3572180",
    "end": "3577540"
  },
  {
    "text": "You had a question? Yeah. So back to the pairwise\ncomparison you did,",
    "start": "3577540",
    "end": "3584060"
  },
  {
    "text": "so if your objects that you are\ncomparing cannot perfectly do this kind of pairwise\ncomparison, so for example, say,",
    "start": "3584060",
    "end": "3591940"
  },
  {
    "text": "the game rock, paper, scissors. So rock is bigger, much\npreferred against scissors.",
    "start": "3591940",
    "end": "3598690"
  },
  {
    "text": "But it's not a perfect\npartially ordered set, then what can we do?",
    "start": "3598690",
    "end": "3604230"
  },
  {
    "text": "If the reward function\nis not transitive, it's a great question. Yeah so there's an interesting\noutcropping of work",
    "start": "3604230",
    "end": "3611760"
  },
  {
    "text": "that is basically trying\nto get away from the reward maximization framework and\nthink of this as a game, where,",
    "start": "3611760",
    "end": "3620280"
  },
  {
    "text": "instead of saying I want\nto generate responses that are the highest\nreward responses,",
    "start": "3620280",
    "end": "3625720"
  },
  {
    "text": "we should think of this at\nthe policy optimization level, and I should search for a policy\nwhere the average win rate--",
    "start": "3625720",
    "end": "3631470"
  },
  {
    "text": "if I take the\nexpectation of the win rate of sampling an action from\nthe policy that I'm optimizing,",
    "start": "3631470",
    "end": "3637980"
  },
  {
    "text": "and then I have some\ncomparison or adversary policy, I'm going to sample an action\nfrom that adversary policy,",
    "start": "3637980",
    "end": "3643720"
  },
  {
    "text": "what is the expected\nwin rate of the action sampled from my policy\ncompared to the action sampled from the adversary?",
    "start": "3643720",
    "end": "3649510"
  },
  {
    "text": "And so now we have to pick\nlike an adversary policy class, which kind of makes\nsense in your rock, paper,",
    "start": "3649510",
    "end": "3656230"
  },
  {
    "text": "scissors example. Because yeah, there's not like\nan optimal action to take here. It depends on what the\npolicy of your adversary",
    "start": "3656230",
    "end": "3661900"
  },
  {
    "text": "is here to know what's\ngood and what's bad. So in this case, it exactly\ndoes address this issue of,",
    "start": "3661900",
    "end": "3667089"
  },
  {
    "text": "if you have only a\npartial ordering, you can't necessarily compare\nall pairs of responses. We can still use\nthat kind of data.",
    "start": "3667090",
    "end": "3673640"
  },
  {
    "text": "We don't have to be\nbottlenecked by fitting a reward function first. So there are methods like a--",
    "start": "3673640",
    "end": "3681710"
  },
  {
    "text": "Nash policy? Yeah, like direct Nash\noptimization, or Nash learning from human feedback are\nthis other kind of newer,",
    "start": "3681710",
    "end": "3689990"
  },
  {
    "text": "I guess, family of algorithms\nare really interesting. Well interestingly\nenough, rock, paper, scissors doesn't actually have a\ndeterministic Nash fixed point.",
    "start": "3689990",
    "end": "3697210"
  },
  {
    "text": "It has a stochastic\none, but stochastic ones are just like equal\nprobability over everything. That's related to some\ndeeper results that actually",
    "start": "3697210",
    "end": "3703570"
  },
  {
    "text": "say examples like this of\nplurality of preferences are actually unsatisfiable. So you cannot actually\ntheoretically train,",
    "start": "3703570",
    "end": "3709720"
  },
  {
    "text": "and in practice, train a model\nthat will satisfy that set of preferences. I think it's good.",
    "start": "3709720",
    "end": "3715290"
  },
  {
    "text": "That's one way\nthat is motivated, too, is if you have different\ndistributions of populations with different preferences,\neven if each of them",
    "start": "3715290",
    "end": "3721410"
  },
  {
    "text": "are internally consistent\nwith transitivity. They may not be across. ",
    "start": "3721410",
    "end": "3728820"
  },
  {
    "text": "Yeah. So assuming that reward hacking\nis not happening, what in DPO",
    "start": "3728820",
    "end": "3736950"
  },
  {
    "text": "prevents it from taking large\nsteps in the accusation?",
    "start": "3736950",
    "end": "3745040"
  },
  {
    "text": "What do you mean by that? Assuming we're hacking is not\nhappening, is there something",
    "start": "3745040",
    "end": "3750319"
  },
  {
    "text": "that DPO is doing that's\npreventing it from taking too large of a step?",
    "start": "3750320",
    "end": "3756609"
  },
  {
    "text": "Yeah, I think the\nKL regularization, if you look at the beta term,\nif the beta term is higher,",
    "start": "3756610",
    "end": "3763821"
  },
  {
    "text": "the sigmoid essentially\nsaturates after a point. Right? And if the beta\nterm is higher, you",
    "start": "3763822",
    "end": "3769069"
  },
  {
    "text": "have to increase the differences\nless to satisfy the loss. So roughly, the beta\ncontrols how quickly",
    "start": "3769070",
    "end": "3777350"
  },
  {
    "text": "you change the loss function. But there are other\nparameters, as well, like learning rates and so on,\nwhich also change affect this.",
    "start": "3777350",
    "end": "3783990"
  },
  {
    "text": "So yeah. ",
    "start": "3783990",
    "end": "3789760"
  },
  {
    "text": "Yeah. For this reward\nhacking problem, one",
    "start": "3789760",
    "end": "3795280"
  },
  {
    "text": "of the methods\nthat people usually try to use to address this\nissue is using ensemble models. Right?",
    "start": "3795280",
    "end": "3800829"
  },
  {
    "text": "Is that something\nthat could be done with direct methods like\nPPO, an ensemble of DPOs",
    "start": "3800830",
    "end": "3806829"
  },
  {
    "text": "or something like that? You could. The problem with\nthat is then you have to keep all\nthe models around.",
    "start": "3806830",
    "end": "3812650"
  },
  {
    "text": "But there's smarter\nassembling things you can do. ",
    "start": "3812650",
    "end": "3818359"
  },
  {
    "text": "You don't have to have complete\ncopies of your entire model to have an ensemble,\nfor example. So you can ensemble sub\npieces of your model,",
    "start": "3818360",
    "end": "3826910"
  },
  {
    "text": "or even represent your reward\nmodel as a distribution, instead of a single scalar.",
    "start": "3826910",
    "end": "3832910"
  },
  {
    "text": "And this starts tying back\ninto these situations where we have a variety of\npreferences in our data",
    "start": "3832910",
    "end": "3839660"
  },
  {
    "text": "that aren't always\nconsistent with each other. One way of modeling this\ndata better is to say,",
    "start": "3839660",
    "end": "3845210"
  },
  {
    "text": "I have a sort of a\nnon deterministic, or I have a multi-modal\nreward function instead.",
    "start": "3845210",
    "end": "3850940"
  },
  {
    "text": "And if you have a way\nof representing this with this generative\nmodel architecture,",
    "start": "3850940",
    "end": "3856230"
  },
  {
    "text": "then you can still just stick\nthis into a DPO looking loss. ",
    "start": "3856230",
    "end": "3865180"
  },
  {
    "text": "Yeah, your answer already\nkind of answered my question. But I just wanted\nto ask, in general,",
    "start": "3865180",
    "end": "3871600"
  },
  {
    "text": "what were the promising\ndirections for addressing reward hacking in DPO? ",
    "start": "3871600",
    "end": "3879060"
  },
  {
    "text": "Well, there's a number of reward\nhacking works on classical RLHF. There was a huge\nnumber of those.",
    "start": "3879060",
    "end": "3885119"
  },
  {
    "text": "Some of those transfer\npretty straightforwardly. Here's something I'm\nkind of excited about.",
    "start": "3885120",
    "end": "3890550"
  },
  {
    "text": "And interestingly enough,\nthat came from the open source community in a way that they\ndidn't actually understand",
    "start": "3890550",
    "end": "3895560"
  },
  {
    "text": "what they were doing. They kind of like stumbled\nacross this very randomly by a very questionable group\nof researchers on Twitter.",
    "start": "3895560",
    "end": "3904042"
  },
  {
    "text": "What they discovered\nis, basically, if you just take a bunch of\nrandom RLHF models and you just,",
    "start": "3904042",
    "end": "3909510"
  },
  {
    "text": "literally, weight average\nthem, they just become better. Take the weights,\ntake the average,",
    "start": "3909510",
    "end": "3915030"
  },
  {
    "text": "and it just becomes better. And it turns out there's a\nton of work on this from 2018",
    "start": "3915030",
    "end": "3920940"
  },
  {
    "text": "around the optimization\nlandscape of these things. And they very randomly\nstumbled across it.",
    "start": "3920940",
    "end": "3928369"
  },
  {
    "text": "Now it seems to work\nnow, but there's a paper called WARM,\nWeight Averaging Reward",
    "start": "3928370",
    "end": "3933569"
  },
  {
    "text": "Models, which kind of makes\nthat point for reward models. So if you train an\nensemble of reward models, you don't keep the ensemble,\nbut you average them,",
    "start": "3933570",
    "end": "3940080"
  },
  {
    "text": "weight average the ensemble. That significantly improves your\nrobustness as a reward function.",
    "start": "3940080",
    "end": "3946072"
  },
  {
    "text": "And the same seems to be\nactually happening with DPO. If you train ensemble\nof DPO models, or already pre-trained\nDPO models,",
    "start": "3946072",
    "end": "3951780"
  },
  {
    "text": "and you weight\naverage them, that seems to actually significantly\nimprove the robustness, as well. And Twitter randomly\nstumbled across this",
    "start": "3951780",
    "end": "3959309"
  },
  {
    "text": "without really understanding it. But it seemed to work for them. And it turns out\nthere's a really sort",
    "start": "3959310",
    "end": "3964740"
  },
  {
    "text": "of deep reasons behind this. So that's one thing\nI'm kind of sad about.",
    "start": "3964740",
    "end": "3970410"
  },
  {
    "text": "And actually, after\nwe get this paper out, we have right now something on\nthe order of 400 checkpoints",
    "start": "3970410",
    "end": "3975480"
  },
  {
    "text": "or something. The next thing we're\nprobably going to do is try to see how much\nrobustness we can squeeze out from some sort of--",
    "start": "3975480",
    "end": "3981690"
  },
  {
    "text": "and people do smart things\nnow, like evolutionary merge and things like that,\nhow much robustness we can squeeze from some sort of\nevoluntionary merging strategy.",
    "start": "3981690",
    "end": "3989075"
  },
  {
    "text": " I'll give you one thing that's\nsort of interesting, also,",
    "start": "3989075",
    "end": "3994330"
  },
  {
    "text": "is that we're starting with this\nKL penalized reward maximization objective.",
    "start": "3994330",
    "end": "3999740"
  },
  {
    "text": "And that was the\noriginal policy learning objective is maximize rewards\nsubject to this KL penalty. And the intuition\nis that, yeah, we",
    "start": "3999740",
    "end": "4007150"
  },
  {
    "text": "want to keep the KL\nsmall so we don't over optimize our reward function. But this is kind of a crude way\nof encoding this desideratum,",
    "start": "4007150",
    "end": "4018560"
  },
  {
    "text": "basically. And something that might be\ncloser to what we really want is to say, well, the places\nwhere my reward model has",
    "start": "4018560",
    "end": "4027560"
  },
  {
    "text": "high uncertainty, those\nare the places where I want to be conservative.",
    "start": "4027560",
    "end": "4033960"
  },
  {
    "text": "But if I have something\nthat's out of distribution, but my reward model\nis really confident,",
    "start": "4033960",
    "end": "4039050"
  },
  {
    "text": "or I have low uncertainty over\nwhat the reward should be,",
    "start": "4039050",
    "end": "4044720"
  },
  {
    "text": "or basically, the lower\npercentiles of reward are still quite high,\nthen it's OK to change",
    "start": "4044720",
    "end": "4050280"
  },
  {
    "text": "my model a lot in these places. And so I think one direction\nthat I think is also interesting here is getting away from\nthe KL regularized policy",
    "start": "4050280",
    "end": "4057990"
  },
  {
    "text": "optimization objective,\nwhich is nice because it gives us this\none-to-oneness from policies to reward models.",
    "start": "4057990",
    "end": "4064050"
  },
  {
    "text": "But also, I think it's possible\nthis is a bit too crude and it leaves some\nperformance on the table,",
    "start": "4064050",
    "end": "4070320"
  },
  {
    "text": "because we're over\nconstraining our policy. Another quick question,\na quick point.",
    "start": "4070320",
    "end": "4075742"
  },
  {
    "text": "As you said, you can think\nabout all these algorithms as Q-functions, essentially,\nan interesting framework. I think that's kind\nof interesting.",
    "start": "4075742",
    "end": "4081838"
  },
  {
    "text": "What we're pursuing\nis, initially, DQNs were really hard to get working. Right? And there's, after\na couple of years,",
    "start": "4081838",
    "end": "4088005"
  },
  {
    "text": "they would work great\nbecause a lot of tricks were used to make them\nstable, to make them perform, and make them not\nbootstrap, and make them",
    "start": "4088005",
    "end": "4093720"
  },
  {
    "text": "not overfit, et cetera. And I think a lot\nof these things could potentially transfer\nfrom that to the LLMs. And particularly, the\nweight averaging thing",
    "start": "4093720",
    "end": "4099990"
  },
  {
    "text": "is very much, I think, also\ninspired by results in DQNs, where you have a\ntarget function.",
    "start": "4099990",
    "end": "4105960"
  },
  {
    "text": "I don't know if you guys did\nthe DQN homework already. We have a target\nQ function, which is actually weight average, so\nsome sort of Polyak averaging.",
    "start": "4105960",
    "end": "4113578"
  },
  {
    "text": "So it's kind of like staggered. And that seems, for example,\nto improve stability a lot. I think a similar result holds\nfor LLM cues, so to speak.",
    "start": "4113578",
    "end": "4121484"
  },
  {
    "text": "But again, it's still sort of\nin the pipeline of experiments to do. ",
    "start": "4121485",
    "end": "4129109"
  },
  {
    "text": "Yeah. I'm not sure if this was\nalready touched upon, but are there any\nrisks with overfitting? And is there certain domains,\nlike medical domains,",
    "start": "4129109",
    "end": "4137899"
  },
  {
    "text": "where there's very,\nvery small data sets? Is there a scope for this\nkind of work on those?",
    "start": "4137899",
    "end": "4143308"
  },
  {
    "text": "This is essentially an\noverfitting problem. You have limited data coverage\nextrapolated in the wrong way. It's a little bit\nmore trickier, though.",
    "start": "4143308",
    "end": "4150479"
  },
  {
    "text": "People have actually found\nthat, in DPO and other settings, this overfitting is\nsomewhat beneficial. So you can do multiple\nepochs on small data sets.",
    "start": "4150479",
    "end": "4159540"
  },
  {
    "text": "And for some of our\nexperiments, you can get that very tiny\npreference data sets, as well, and it still sort of works.",
    "start": "4159540",
    "end": "4165799"
  },
  {
    "text": "But people do multiple epochs,\nand they're very clearly overfitting, but the performance\nstill keeps on improving.",
    "start": "4165800",
    "end": "4171620"
  },
  {
    "text": "But again, a lot depends upon\nhow you evaluate these models. And you're probably\nlosing somewhere else.",
    "start": "4171620",
    "end": "4178068"
  },
  {
    "text": "So it really depends upon\nhow you're going to use this. One thing to keep in\nmind is that we've been talking a lot about reward\nover optimization, or reward",
    "start": "4178069",
    "end": "4184399"
  },
  {
    "text": "hacking, which is\nthis discrepancy between the proxy reward that\nwe're actually optimizing against, the thing we\nlearned from feedback,",
    "start": "4184399",
    "end": "4191040"
  },
  {
    "text": "and the true reward that we\ndon't actually get to observe. But there's another discrepancy\nthat we haven't really talked about that Archit\njust mentioned, which",
    "start": "4191040",
    "end": "4197937"
  },
  {
    "text": "is that, when we evaluate\nthese models, in practice, we're actually typically not\nevaluating average reward.",
    "start": "4197937",
    "end": "4204030"
  },
  {
    "text": "We're typically\nevaluating something more like a win rate,\nwhich is comparing to some baseline\npolicy or something.",
    "start": "4204030",
    "end": "4211150"
  },
  {
    "text": "So the setup is more like\nalmost a satisficing rather than a maximizing kind of situation.",
    "start": "4211150",
    "end": "4217120"
  },
  {
    "text": "And so that's another layer of\ndisconnect between the thing that we're using as our training\nobjective and the thing that",
    "start": "4217120",
    "end": "4225570"
  },
  {
    "text": "is actually providing\nutility for the human, or the person who's actually\nbuilding this thing. So there's another layer of\nwhere we can get overfitting,",
    "start": "4225570",
    "end": "4233760"
  },
  {
    "text": "in a sense, to the objective. ",
    "start": "4233760",
    "end": "4239530"
  },
  {
    "text": "Yeah. So my understanding is\nit's kind of two stages. The first one is the\nnormal supervised training of the language model.",
    "start": "4239530",
    "end": "4245180"
  },
  {
    "text": "And then the second stage\nis the DPO training. And then you use\nthe KL divergence as a means to make\nsure you're not",
    "start": "4245180",
    "end": "4251260"
  },
  {
    "text": "moving too far away from your\noriginal supervised model. It seems like two stages. Is it possible to combine\nthis preference learning",
    "start": "4251260",
    "end": "4258850"
  },
  {
    "text": "during the normal\nsupervised training, so as you're training\nthe model from the start,",
    "start": "4258850",
    "end": "4264230"
  },
  {
    "text": "you're also digging into these? Because it seems like you're\nusing KL as kind of a proxy for making sure doesn't\nmove too far away.",
    "start": "4264230",
    "end": "4269719"
  },
  {
    "text": "But if you do them\nat the same time, maybe it'll help address that. There are a few works\nthat have tried.",
    "start": "4269720",
    "end": "4276889"
  },
  {
    "text": "So you're talking about merging\nthe supervised instruction tuning and the\npreference tuning part? Yeah, because they're\none after another.",
    "start": "4276890",
    "end": "4282820"
  },
  {
    "text": "Yes. Yeah, so there's a few works\nthat have tried to do that. I think it's still an\nactive area of research. But the general idea\nwhy, so maybe it's",
    "start": "4282820",
    "end": "4291872"
  },
  {
    "text": "useful to understand why\nwe even do instruction tuning before doing RLHF. It's that when you start\nwith a pre-trained model,",
    "start": "4291872",
    "end": "4298530"
  },
  {
    "text": "it will give you gibberish\nresponses, which are not even aligned with the\ninstruction you're giving.",
    "start": "4298530",
    "end": "4303750"
  },
  {
    "text": "So the instruction\ntuning sort of helps us generate the\nright preference data set, where you're starting to\nfollow the question being asked.",
    "start": "4303750",
    "end": "4311030"
  },
  {
    "text": "So typically, in a very\ntypical RLHF pipeline, when you don't even have\na preference data set to begin with, that's why you\ndo the instruction tuning bit.",
    "start": "4311030",
    "end": "4319160"
  },
  {
    "text": "But if you have preference\ndata sets already, people are coming\nup with methods where you can both\ncombine instruction tuning",
    "start": "4319160",
    "end": "4325880"
  },
  {
    "text": "and preference learning bit into\nthe same optimization algorithm. They're not very different. They're usually some elements\nof the loss functions",
    "start": "4325880",
    "end": "4332870"
  },
  {
    "text": "you already see. But it's still somewhat of\nan active area of research. Yeah.",
    "start": "4332870",
    "end": "4337910"
  },
  {
    "text": "Can you do maybe a DAGGER\nesque kind of thing where you train the model\nand then you do fine tuning?",
    "start": "4337910",
    "end": "4343619"
  },
  {
    "text": "There's methods which\ndo that, as well. Yeah. In my personal experience,\nthey didn't work very well.",
    "start": "4343620",
    "end": "4349470"
  },
  {
    "text": "But there are papers that\nclaim that works really well. ",
    "start": "4349470",
    "end": "4355215"
  },
  {
    "text": "You've also have\nproblems trying to get DAGGER [INAUDIBLE] to work. It doesn't mean it's impossible. Yeah, it doesn't mean you can't.",
    "start": "4355215",
    "end": "4360433"
  },
  {
    "text": "There's a lot of details\nthat go into these. Yeah. And also, personally,\nI'm somewhat",
    "start": "4360433",
    "end": "4366040"
  },
  {
    "text": "suspicious of these things\nbecause the optimization landscape is so-- basically, as you're seeing\nfrom this optimization thing,",
    "start": "4366040",
    "end": "4372440"
  },
  {
    "text": "the optimization landscape\nis so complicated and can form so many\ndifferent pitfalls,",
    "start": "4372440",
    "end": "4377469"
  },
  {
    "text": "that I think trying to\ncombine this and navigate that in a single shot\noptimization direction is pretty",
    "start": "4377470",
    "end": "4383350"
  },
  {
    "text": "hard. Probably not impossible,\nbut pretty hard. To me, it's not really clear\nwhat the benefits from that are.",
    "start": "4383350",
    "end": "4388897"
  },
  {
    "text": "But again, I also think\nit kind of goes back to the exploration\nquestion, which I think how Archit framed it there is--",
    "start": "4388897",
    "end": "4394764"
  },
  {
    "text": " on the first day, OpenAI\nsaid, let there be NSFT model.",
    "start": "4394765",
    "end": "4403610"
  },
  {
    "text": "And there was no preference\ndata you said yet. And so in order to actually\nget the preferences, you needed a source\nof exploration",
    "start": "4403610",
    "end": "4410920"
  },
  {
    "text": "to get the trajectories\nto get preferences over. So you had to do one\nand then the other. And so I think that's\nthe original way",
    "start": "4410920",
    "end": "4418346"
  },
  {
    "text": "to think about that. But now, if we're trying to do\nthis in a single offline stage, well, now we're sort of stuck.",
    "start": "4418347",
    "end": "4423402"
  },
  {
    "text": "We're just stuck\nwith whatever data we have in terms\nof the exploration. And there's only\ngoing to be so much can do when you have\npurely offline data,",
    "start": "4423402",
    "end": "4430200"
  },
  {
    "text": "so if you're doing\nthis iteratively. But being able to\nsample and then get new preferences over those\nsamples is useful to do.",
    "start": "4430200",
    "end": "4437110"
  },
  {
    "start": "4437110",
    "end": "4442139"
  },
  {
    "text": "So you mentioned the\ndiscrepancy where we're training to maximize\nthe reward function, but then during\nevaluation, we're",
    "start": "4442140",
    "end": "4448350"
  },
  {
    "text": "evaluating based on win rate. So could we just use a\ndifferent objective function to optimize directly\nfor win rate?",
    "start": "4448350",
    "end": "4455112"
  },
  {
    "text": "Is that possible? Yeah. These NASH algorithms\nbasically are doing that. So, instead of deriving this\nas we have some reward function",
    "start": "4455112",
    "end": "4462900"
  },
  {
    "text": "and we're maximizing\nreward, it's like, literally, I have\nsome either baseline policy",
    "start": "4462900",
    "end": "4468090"
  },
  {
    "text": "and I want to-- if I can only evaluate the\npreference function, not the reward function,\nso a function",
    "start": "4468090",
    "end": "4473670"
  },
  {
    "text": "that takes two responses and\nsays which one is better, one objective I\ncould come up with is, the expectation under\nmy policy of that preference",
    "start": "4473670",
    "end": "4482910"
  },
  {
    "text": "function computed on one\nresponse from my policy, and one response from\nthis baseline policy,",
    "start": "4482910",
    "end": "4488170"
  },
  {
    "text": "or one response from an\nadversarial best, worst case, best adversary policy.",
    "start": "4488170",
    "end": "4494940"
  },
  {
    "text": "And so now, you're basically\nexplicitly optimizing for either average\ncase or worst case",
    "start": "4494940",
    "end": "4500340"
  },
  {
    "text": "win rate against some\nreference or comparison. So how does that compare to PPO?",
    "start": "4500340",
    "end": "4505390"
  },
  {
    "text": "It depends on who you ask. The papers introducing these\nmethods show improvements.",
    "start": "4505390",
    "end": "4512660"
  },
  {
    "text": "I think one of the ways that\nit's helpful is that you're not, again, going through\na reward function.",
    "start": "4512660",
    "end": "4519500"
  },
  {
    "text": "And so you're not requiring-- you're not explicitly\ntraining to have some complete total ordering\nover all of your responses.",
    "start": "4519500",
    "end": "4527630"
  },
  {
    "text": "And so this can be helpful. It's not as constraining\nof a sort of framework",
    "start": "4527630",
    "end": "4534280"
  },
  {
    "text": "to think about. At the same time, any\npolicy we end up with, compared to some\nreference model,",
    "start": "4534280",
    "end": "4539440"
  },
  {
    "text": "we can interpret as\na reward function. So I'm not exactly sure how\nto think about the advantages",
    "start": "4539440",
    "end": "4545370"
  },
  {
    "text": "there. But yes, if you look at the\nexperiments in the papers, they will say, yeah,\nwe have improvements in win rate, which\nkind of makes sense.",
    "start": "4545370",
    "end": "4550900"
  },
  {
    "text": "Right? You're evalling with\nwin rate, and now we're training for win rate\ninstead of training for reward maximization. It's not that surprising.",
    "start": "4550900",
    "end": "4556842"
  },
  {
    "text": "You can see improvements. There's also another point that,\nif you do consider the Bradley Terry model to be true that\nthis is your preference model,",
    "start": "4556842",
    "end": "4563310"
  },
  {
    "text": "that this is the data\ngeneration model, maximizing reward and maximizing\nprobability of win rate are actually identical.",
    "start": "4563310",
    "end": "4569710"
  },
  {
    "text": "So as I said, this reward\nmaximization thing, because of the Fried parameter,\nhas very high variance. So what OpenAI does\nin other papers,",
    "start": "4569710",
    "end": "4576590"
  },
  {
    "text": "but it's always like a footnote\nin a 100 page paper is, they actually normalize\nthe reward functions.",
    "start": "4576590",
    "end": "4582250"
  },
  {
    "text": "So they subtract\nsome human baseline. So the reward of the human\ncompletion or the human data",
    "start": "4582250",
    "end": "4588310"
  },
  {
    "text": "is zero. And what this gives you is,\nessentially, actually, the log probability. Then the reward function\nthey optimize with PPO",
    "start": "4588310",
    "end": "4594849"
  },
  {
    "text": "is the log probability. The generation is preferred\nover the human generation",
    "start": "4594850",
    "end": "4600070"
  },
  {
    "text": "under Bradley Terry. So these things are\nvery tightly coupled. And the normalization\npart, from our perspective,",
    "start": "4600070",
    "end": "4606170"
  },
  {
    "text": "actually doesn't change\nthe optimal policy. In things I've seen\nin experiments,",
    "start": "4606170",
    "end": "4612190"
  },
  {
    "text": "it actually significantly\nreduces the variance, which is the intuition there. But actually, there's\na very direct way",
    "start": "4612190",
    "end": "4617350"
  },
  {
    "text": "to tie that with, essentially,\nmaximizing probability of winning, essentially.",
    "start": "4617350",
    "end": "4623790"
  },
  {
    "text": "It's like a baseline. Yeah, it's exactly a\nbaseline, essentially. And this baseline\nactually works.",
    "start": "4623790",
    "end": "4630040"
  },
  {
    "text": "The variance actually\nsignificantly goes down. Why don't we do one\nmore from somebody who",
    "start": "4630040",
    "end": "4636420"
  },
  {
    "text": "hasn't asked a question yet? ",
    "start": "4636420",
    "end": "4642890"
  },
  {
    "text": "So are those DPO applicable to\nmulti-objective correlation?",
    "start": "4642890",
    "end": "4649510"
  },
  {
    "text": " There's a paper\ncalled MODPO, which",
    "start": "4649510",
    "end": "4655170"
  },
  {
    "text": "stands for multi-objective DPO. And yeah, you can basically--",
    "start": "4655170",
    "end": "4662480"
  },
  {
    "text": "yes, you can do\nDPO in this setting where you basically\ncondition on a scalarization",
    "start": "4662480",
    "end": "4668640"
  },
  {
    "text": "of your multiple objectives,\nlike a particular weighting. You don't have to learn\nany reward function,",
    "start": "4668640",
    "end": "4674610"
  },
  {
    "text": "or you have to learn\nn minus 1 reward? No, on of you guys,\ncorrect me if I'm wrong.",
    "start": "4674610",
    "end": "4680590"
  },
  {
    "text": "I think you're still\nlearning, basically, a weighting conditioned policy,\nwhere you can pick the mixture.",
    "start": "4680590",
    "end": "4686210"
  },
  {
    "text": "You have all of your\ndifferent objectives, and you can pick like what\nweighting over these objectives",
    "start": "4686210",
    "end": "4691320"
  },
  {
    "text": "you want to use, which\npolicy do you actually want to end up with.",
    "start": "4691320",
    "end": "4696830"
  },
  {
    "text": "How do you trade these off? And you don't have to retrain\nfor every single different scalarization.",
    "start": "4696830",
    "end": "4702350"
  },
  {
    "text": "There are others that\ndo this with uncertainty over the reward model, as well.",
    "start": "4702350",
    "end": "4708938"
  },
  {
    "text": "Now let's thank\nour speakers again. Thank you very\nmuch for having us. Thank you.",
    "start": "4708938",
    "end": "4714400"
  },
  {
    "text": "Thanks so much for coming. All right. Good luck with the\nmidterm, everybody. See you Wednesday. ",
    "start": "4714400",
    "end": "4724000"
  }
]