[
  {
    "text": "So, um, the third topic I wanna discuss is",
    "start": "4130",
    "end": "8880"
  },
  {
    "text": "scaling up graph neural networks by simplifying their architecture.",
    "start": "8880",
    "end": "13140"
  },
  {
    "text": "Um, and this is kind of an orthogonal approach,",
    "start": "13140",
    "end": "15820"
  },
  {
    "text": "uh, to the first two approaches.",
    "start": "15820",
    "end": "17520"
  },
  {
    "text": "And here is how we are going to do this.",
    "start": "17520",
    "end": "19770"
  },
  {
    "text": "We will start from a graph convolutional,",
    "start": "19770",
    "end": "21869"
  },
  {
    "text": "uh, uh, network, uh,",
    "start": "21869",
    "end": "24210"
  },
  {
    "text": "just as an example architecture,",
    "start": "24210",
    "end": "26099"
  },
  {
    "text": "and we are going to simplify it by removing",
    "start": "26100",
    "end": "28335"
  },
  {
    "text": "the non-linear activation from the GCN, right?",
    "start": "28335",
    "end": "31410"
  },
  {
    "text": "And actually, there's been a paper, uh,",
    "start": "31410",
    "end": "33210"
  },
  {
    "text": "two years ago or a year and a half ago that demonstrates that",
    "start": "33210",
    "end": "37500"
  },
  {
    "text": "the performance on- on benchmarks is not too much lower because you have,",
    "start": "37500",
    "end": "42660"
  },
  {
    "text": "uh, removed the non-linearity.",
    "start": "42660",
    "end": "44920"
  },
  {
    "text": "Uh, and this means that now,",
    "start": "44920",
    "end": "46589"
  },
  {
    "text": "this simplified GCN architecture turns out to be an extremely scalable,",
    "start": "46590",
    "end": "50840"
  },
  {
    "text": "uh, model that we can train very fast.",
    "start": "50840",
    "end": "53225"
  },
  {
    "text": "So basically, the idea here is you are going to simplify",
    "start": "53225",
    "end": "57050"
  },
  {
    "text": "the expressive power of the graph neural network so that you can train it faster.",
    "start": "57050",
    "end": "61445"
  },
  {
    "text": "Of course, this kind of deb- defeats the purpose of deep-learning a bit",
    "start": "61445",
    "end": "65390"
  },
  {
    "text": "because the point of deep learning or representation learning is a lot of data over,",
    "start": "65390",
    "end": "70250"
  },
  {
    "text": "uh, over complex models that can model complex, um, uh, representations.",
    "start": "70250",
    "end": "75770"
  },
  {
    "text": "Here, we are saying, simplify the model so it's fast to run on big data,",
    "start": "75770",
    "end": "79909"
  },
  {
    "text": "and of course, there is a trade off there.",
    "start": "79910",
    "end": "82015"
  },
  {
    "text": "So let me, uh, introduce the, uh,",
    "start": "82015",
    "end": "85229"
  },
  {
    "text": "GCN, uh, and then we'll continue from there, right?",
    "start": "85230",
    "end": "88350"
  },
  {
    "text": "So GCN takes in, uh, a graph,",
    "start": "88350",
    "end": "90360"
  },
  {
    "text": "uh, with, uh, node features.",
    "start": "90360",
    "end": "93135"
  },
  {
    "text": "And let's assume that every node includes a self-loop.",
    "start": "93135",
    "end": "96155"
  },
  {
    "text": "Uh, this would be, uh, convenient for,",
    "start": "96155",
    "end": "98840"
  },
  {
    "text": "uh, mathematical notation later.",
    "start": "98840",
    "end": "100715"
  },
  {
    "text": "And let's think of this as a full batch implementation, right?",
    "start": "100715",
    "end": "103520"
  },
  {
    "text": "So we basically set that the node embeddings",
    "start": "103520",
    "end": "105950"
  },
  {
    "text": "at layer- at layer 0 to be simply node features,",
    "start": "105950",
    "end": "108685"
  },
  {
    "text": "and then we are going to iterate for K,",
    "start": "108685",
    "end": "110990"
  },
  {
    "text": "uh, K layers, uh, where at, uh,",
    "start": "110990",
    "end": "113375"
  },
  {
    "text": "every- every node at layer k plus 1 is going",
    "start": "113375",
    "end": "116840"
  },
  {
    "text": "to take the embeddings of its neighbors from the previous layers,",
    "start": "116840",
    "end": "120890"
  },
  {
    "text": "sum them up, um,",
    "start": "120890",
    "end": "123350"
  },
  {
    "text": "and divide by the number of layers- number of neighbors, uh,",
    "start": "123350",
    "end": "127070"
  },
  {
    "text": "um, transform by this learned matrix and pass through a non-linearity.",
    "start": "127070",
    "end": "131825"
  },
  {
    "text": "And then we're going to run this, uh,",
    "start": "131825",
    "end": "133910"
  },
  {
    "text": "recursion for k iterations and whatever we end up in the end with,",
    "start": "133910",
    "end": "138950"
  },
  {
    "text": "whatever embedding we end up at the end,",
    "start": "138950",
    "end": "140825"
  },
  {
    "text": "that is what we call, uh, final embedding.",
    "start": "140825",
    "end": "144040"
  },
  {
    "text": "Now, what is- benefit of GCN is that it is so simple.",
    "start": "144040",
    "end": "148069"
  },
  {
    "text": "We can very nicely write it into the- in the matrix form.",
    "start": "148070",
    "end": "151970"
  },
  {
    "text": "So the way we are going to write it in the matrix form is that we are going to take",
    "start": "151970",
    "end": "155450"
  },
  {
    "text": "these embeddings and we are going to stack them into an embedding matrix.",
    "start": "155450",
    "end": "160010"
  },
  {
    "text": "And then A is our adjacency matrix where every node also has a self-loop.",
    "start": "160010",
    "end": "165580"
  },
  {
    "text": "Then the way you can write the sum over the neighbors of a given node- sum",
    "start": "165580",
    "end": "171900"
  },
  {
    "text": "of the embeddings of a- of a given no- of the neighbors of a given node,",
    "start": "171900",
    "end": "177840"
  },
  {
    "text": "you can simply write this as a- as a product between the adjacency matrix A",
    "start": "177840",
    "end": "182394"
  },
  {
    "text": "and the- and the embedding matrix H. And, uh,",
    "start": "182395",
    "end": "187585"
  },
  {
    "text": "what this means is that now,",
    "start": "187585",
    "end": "189445"
  },
  {
    "text": "we can also define this notion of D to be",
    "start": "189445",
    "end": "193810"
  },
  {
    "text": "a diagonal matrix where we- it is all of 0 only on the diagonal,",
    "start": "193810",
    "end": "198640"
  },
  {
    "text": "we have the degree of every node.",
    "start": "198640",
    "end": "201685"
  },
  {
    "text": "And then the inverse of D,",
    "start": "201685",
    "end": "204099"
  },
  {
    "text": "D to the minus 1 is the inverse of",
    "start": "204100",
    "end": "206320"
  },
  {
    "text": "the diagonal matrix which is just you put 1 over the degree,",
    "start": "206320",
    "end": "210460"
  },
  {
    "text": "uh, on the- on the diagonal entry for every node.",
    "start": "210460",
    "end": "213740"
  },
  {
    "text": "So now, the way you can write a summation over the neighbors",
    "start": "213740",
    "end": "217070"
  },
  {
    "text": "divided by the degree of that node is simply, uh,",
    "start": "217070",
    "end": "221500"
  },
  {
    "text": "the inverse of the diagonal matrix,",
    "start": "221500",
    "end": "223870"
  },
  {
    "text": "so one over the degree times the adjacency matrix A times the hidden- uh, uh,",
    "start": "223870",
    "end": "230360"
  },
  {
    "text": "embeddings matrix H. So this means that now,",
    "start": "230360",
    "end": "233030"
  },
  {
    "text": "given H at level l,",
    "start": "233030",
    "end": "234680"
  },
  {
    "text": "if we multiply it by A and multiply it by D to the minus 1,",
    "start": "234680",
    "end": "239150"
  },
  {
    "text": "we get the matrix of node embeddings at level h plus 1.",
    "start": "239150",
    "end": "242269"
  },
  {
    "text": "So basically, what is elegant here is that you can rewrite",
    "start": "242270",
    "end": "245750"
  },
  {
    "text": "this iteration just as a product of three matrices,",
    "start": "245750",
    "end": "250140"
  },
  {
    "text": "and of course, in the GCN,",
    "start": "250140",
    "end": "251840"
  },
  {
    "text": "we also have a ReLU non-linearity,",
    "start": "251840",
    "end": "254840"
  },
  {
    "text": "um, uh, and, uh, transformation here.",
    "start": "254840",
    "end": "258370"
  },
  {
    "text": "So going back to the GCN,",
    "start": "258370",
    "end": "261150"
  },
  {
    "text": "here is the node-based, uh, uh,",
    "start": "261150",
    "end": "263470"
  },
  {
    "text": "formulation of the GCN,",
    "start": "263470",
    "end": "265080"
  },
  {
    "text": "if you write it in the matrix form,",
    "start": "265080",
    "end": "267229"
  },
  {
    "text": "you can write it that this is",
    "start": "267230",
    "end": "268670"
  },
  {
    "text": "a non-linearity times this A tilde that is simply degree times, uh,",
    "start": "268670",
    "end": "274515"
  },
  {
    "text": "A, um, times the previous layer embeddings times,",
    "start": "274515",
    "end": "280065"
  },
  {
    "text": "uh, the W matrix which is the transformation matrix.",
    "start": "280065",
    "end": "283880"
  },
  {
    "text": "So basically, the equation- this equation,",
    "start": "283880",
    "end": "288195"
  },
  {
    "text": "uh, that is based on the network and the following matrix equation, they are equivalent.",
    "start": "288195",
    "end": "293029"
  },
  {
    "text": "So if you've- if you've computed this product of these matrices,",
    "start": "293029",
    "end": "296525"
  },
  {
    "text": "you have just computed that the k plus 1 layer",
    "start": "296525",
    "end": "298755"
  },
  {
    "text": "embedding for all the nodes of the network, and now,",
    "start": "298755",
    "end": "301100"
  },
  {
    "text": "you can kind of iterate this capital K times to compute the k layers.",
    "start": "301100",
    "end": "306290"
  },
  {
    "text": "So, um, this is what a matrix formulation of our GCN looks like.",
    "start": "306290",
    "end": "311810"
  },
  {
    "text": "So let's now go and simplify the GCN.",
    "start": "311810",
    "end": "314840"
  },
  {
    "text": "Let's assume and go and remove this ReLU, uh, non-linearity.",
    "start": "314840",
    "end": "320010"
  },
  {
    "text": "So let's- lets say,",
    "start": "320010",
    "end": "321270"
  },
  {
    "text": "what would happen if the GCN would- would be governed by the following equation, right?",
    "start": "321270",
    "end": "325610"
  },
  {
    "text": "So going back, this is the equation with a non-linearity, now,",
    "start": "325610",
    "end": "330180"
  },
  {
    "text": "we decided- now we decided to drop the non-linearity,",
    "start": "330180",
    "end": "333419"
  },
  {
    "text": "so here is our, you know,",
    "start": "333420",
    "end": "335250"
  },
  {
    "text": "simplified the GCN equation.",
    "start": "335250",
    "end": "337650"
  },
  {
    "text": "So now, let's go and unroll this- uh,",
    "start": "337650",
    "end": "341955"
  },
  {
    "text": "this iteration, let's- let's unroll this recursion, right?",
    "start": "341955",
    "end": "345720"
  },
  {
    "text": "So we say, ah, here is the final layer embeddings for the nodes,",
    "start": "345720",
    "end": "350000"
  },
  {
    "text": "uh, they depend on the layer k minus 1 embeddings of the nodes.",
    "start": "350000",
    "end": "353780"
  },
  {
    "text": "So let's now take this H_k minus 1 and insert it, uh,",
    "start": "353780",
    "end": "358280"
  },
  {
    "text": "with the way we compute, uh,",
    "start": "358280",
    "end": "360290"
  },
  {
    "text": "H_k to the minus- H to the layer k minus 1.",
    "start": "360290",
    "end": "364160"
  },
  {
    "text": "And you know, I take this part and just I insert it here.",
    "start": "364160",
    "end": "367375"
  },
  {
    "text": "And now, I notice this depends on H_k minus 2.",
    "start": "367375",
    "end": "371100"
  },
  {
    "text": "So again, I can go take H_k minus 2 and again expand it.",
    "start": "371100",
    "end": "375640"
  },
  {
    "text": "And if I do this all the way down to the 0 layer,",
    "start": "375640",
    "end": "379655"
  },
  {
    "text": "then, um, I- I know what to do, right?",
    "start": "379655",
    "end": "382310"
  },
  {
    "text": "Like H0 is simply the vector of,",
    "start": "382310",
    "end": "386165"
  },
  {
    "text": "uh- of, uh, node features X.",
    "start": "386165",
    "end": "388765"
  },
  {
    "text": "These A tildes just kind of get multiplied together.",
    "start": "388765",
    "end": "392100"
  },
  {
    "text": "So this is A tilde raised to the power k. And this here at the end, these, uh,",
    "start": "392100",
    "end": "397515"
  },
  {
    "text": "parameter matrices, it is just a product of parameter matrices that is still a matrix.",
    "start": "397515",
    "end": "403295"
  },
  {
    "text": "So I can rewrite this, uh,",
    "start": "403295",
    "end": "405750"
  },
  {
    "text": "recursive equa- equation if I expand it in the following way,",
    "start": "405750",
    "end": "409490"
  },
  {
    "text": "and then I realized that a product of",
    "start": "409490",
    "end": "412880"
  },
  {
    "text": "parameter matrices is just a parameter matrix, right?",
    "start": "412880",
    "end": "416340"
  },
  {
    "text": "So basically, I have just rewritten the K layer",
    "start": "416340",
    "end": "419810"
  },
  {
    "text": "GCN into this very simple equation that is non-recursive.",
    "start": "419810",
    "end": "423830"
  },
  {
    "text": "It's A tilde raised to the power k",
    "start": "423830",
    "end": "426574"
  },
  {
    "text": "times the feature vector times the transformation matrix.",
    "start": "426575",
    "end": "430410"
  },
  {
    "text": "So what is important?",
    "start": "430410",
    "end": "432570"
  },
  {
    "text": "What do you need to, uh,",
    "start": "432570",
    "end": "433785"
  },
  {
    "text": "remember here about A tilde raised to the power k?",
    "start": "433785",
    "end": "437285"
  },
  {
    "text": "Remember in I think Lecture 1 or 2,",
    "start": "437285",
    "end": "439760"
  },
  {
    "text": "we talked about what does powering the adjacency matrix to the- to the kth power mean?",
    "start": "439760",
    "end": "445400"
  },
  {
    "text": "It means we are counting paths,",
    "start": "445400",
    "end": "447620"
  },
  {
    "text": "it means we are connecting nodes that are neighbors of-",
    "start": "447620",
    "end": "450245"
  },
  {
    "text": "that are neighbors- neighbors of neighbors and so on.",
    "start": "450245",
    "end": "453110"
  },
  {
    "text": "So basically, what this means is that this til- A tilde to the k really",
    "start": "453110",
    "end": "457099"
  },
  {
    "text": "connects the target node to its neighbors, neighbors of neighbors,",
    "start": "457100",
    "end": "460970"
  },
  {
    "text": "neighbors of neighbors of neighbors,",
    "start": "460970",
    "end": "462470"
  },
  {
    "text": "and so on, um,",
    "start": "462470",
    "end": "463910"
  },
  {
    "text": "one-hop farther out in the network as we increase,",
    "start": "463910",
    "end": "468285"
  },
  {
    "text": "uh, uh, K, um,",
    "start": "468285",
    "end": "470085"
  },
  {
    "text": "and that's very interesting.",
    "start": "470085",
    "end": "471840"
  },
  {
    "text": "So now, why did- what can we conclude?",
    "start": "471840",
    "end": "474885"
  },
  {
    "text": "We can conclude that removing the non-linearity significantly simplifies the GCN.",
    "start": "474885",
    "end": "480210"
  },
  {
    "text": "Uh, notice also that, um,",
    "start": "480210",
    "end": "482955"
  },
  {
    "text": "these A tilde to the k times x does not",
    "start": "482955",
    "end": "485669"
  },
  {
    "text": "include- contain any learnable parameters so we can",
    "start": "485670",
    "end": "489050"
  },
  {
    "text": "actually pre-compute this on the- on the CPU even before we start, uh, training, right?",
    "start": "489050",
    "end": "495150"
  },
  {
    "text": "And this can be very efficiently computed because all I have to do is,",
    "start": "495150",
    "end": "499160"
  },
  {
    "text": "um, multiply the mat- uh,",
    "start": "499160",
    "end": "502275"
  },
  {
    "text": "A tilde with, uh,",
    "start": "502275",
    "end": "504389"
  },
  {
    "text": "x, uh, kind of with itself, uh,",
    "start": "504390",
    "end": "506940"
  },
  {
    "text": "multiple times, and I will be able to get to the A tilde raised to the power K times x.",
    "start": "506940",
    "end": "511980"
  },
  {
    "text": "So computing this part is very easy,",
    "start": "511980",
    "end": "514260"
  },
  {
    "text": "it does not depend on any, um,",
    "start": "514260",
    "end": "517835"
  },
  {
    "text": "model parameters so I can just pre-compute it even before I start learning.",
    "start": "517835",
    "end": "522445"
  },
  {
    "text": "So now, um, how about,",
    "start": "522445",
    "end": "525615"
  },
  {
    "text": "uh- so what we have learned is that this, uh,",
    "start": "525615",
    "end": "528740"
  },
  {
    "text": "A_k times x could be pre-computed and let's call this X tilde.",
    "start": "528740",
    "end": "534295"
  },
  {
    "text": "So now, uh, the simplified GCN,",
    "start": "534295",
    "end": "536930"
  },
  {
    "text": "the final embedding layer is simply this X tilde times the parameter matrix,",
    "start": "536930",
    "end": "541310"
  },
  {
    "text": "and this is just a linear transformation of the pre-computed matrix, right?",
    "start": "541310",
    "end": "545650"
  },
  {
    "text": "So the way I can- I can think of this,",
    "start": "545650",
    "end": "547840"
  },
  {
    "text": "this is basically just a pre-computed feature vector for node,",
    "start": "547840",
    "end": "551260"
  },
  {
    "text": "uh, v, uh, times its- uh,",
    "start": "551260",
    "end": "554355"
  },
  {
    "text": "times the matrix- learned matrix W. So embedding of node,",
    "start": "554355",
    "end": "558514"
  },
  {
    "text": "uh, v only depends on its own pre-processed, uh, features,",
    "start": "558515",
    "end": "562940"
  },
  {
    "text": "right, where I had- I say this X tilde is really",
    "start": "562940",
    "end": "566155"
  },
  {
    "text": "X tilde computed by A- A tilde raised to the power K times X, right?",
    "start": "566155",
    "end": "571935"
  },
  {
    "text": "But this is a matrix that has one row for every node,",
    "start": "571935",
    "end": "576160"
  },
  {
    "text": "so if I say what is the final layer embedding of a given node?",
    "start": "576160",
    "end": "579259"
  },
  {
    "text": "Is simply the- the corresponding row in",
    "start": "579260",
    "end": "582150"
  },
  {
    "text": "the matrix times the learned parameter matrix, uh,",
    "start": "582150",
    "end": "585600"
  },
  {
    "text": "W. But what is important to notice here is that once this X tilde has been computed,",
    "start": "585600",
    "end": "591470"
  },
  {
    "text": "then learn- then, uh, um,",
    "start": "591470",
    "end": "593805"
  },
  {
    "text": "the embedding of a given node only depends on",
    "start": "593805",
    "end": "596645"
  },
  {
    "text": "a given row of the X tilde that is fixed and constant.",
    "start": "596645",
    "end": "600065"
  },
  {
    "text": "And, uh, the only thing that changes that is learnable is W. So what this means is",
    "start": "600065",
    "end": "606000"
  },
  {
    "text": "that embe- embeddings of M nodes can be generated in linear ti- in the time linear,",
    "start": "606000",
    "end": "612220"
  },
  {
    "text": "um, with M because for a given, uh, node,",
    "start": "612220",
    "end": "615754"
  },
  {
    "text": "its final embedding only depends on its own,",
    "start": "615755",
    "end": "619555"
  },
  {
    "text": "uh, row in the matrix, uh, X tilde.",
    "start": "619555",
    "end": "622485"
  },
  {
    "text": "So I can easily sample a mini batch of nodes,",
    "start": "622485",
    "end": "625890"
  },
  {
    "text": "I sample a set of rows from the matrix X,",
    "start": "625890",
    "end": "630490"
  },
  {
    "text": "and then I multiply that with W to get the final layer embeddings of those,",
    "start": "630490",
    "end": "635200"
  },
  {
    "text": "uh, nodes, uh, in the mini-batch.",
    "start": "635200",
    "end": "637985"
  },
  {
    "text": "So, um, and of course this would be super fast because there is no dependencies between",
    "start": "637985",
    "end": "643790"
  },
  {
    "text": "the nodes or all the dependencies are already captured in this matrix, uh, X tilde.",
    "start": "643790",
    "end": "649430"
  },
  {
    "text": "So in summary, uh, Simplified GCN consists of two steps,",
    "start": "649430",
    "end": "654334"
  },
  {
    "text": "the pre-processing step where- uh,",
    "start": "654334",
    "end": "657035"
  },
  {
    "text": "where a pre- where we pre-compute this X tilde to be simply the adjacency matrix A,",
    "start": "657035",
    "end": "665509"
  },
  {
    "text": "um, with, uh, one over the degree of the node on the diagonal.",
    "start": "665510",
    "end": "669350"
  },
  {
    "text": "We call this A tilde,",
    "start": "669350",
    "end": "670714"
  },
  {
    "text": "we raise this to the Kth power so we multiply it with itself,",
    "start": "670715",
    "end": "674515"
  },
  {
    "text": "K step, K times,",
    "start": "674515",
    "end": "676230"
  },
  {
    "text": "and then we multiply this with the original features of the nodes x.",
    "start": "676230",
    "end": "681829"
  },
  {
    "text": "And all of this can be done on a CPU even before we start training.",
    "start": "681830",
    "end": "686420"
  },
  {
    "text": "So now what this means is that we have this matrix, uh, uh, uh,",
    "start": "686420",
    "end": "690920"
  },
  {
    "text": "X, uh, X tilde that has one row for every node, um,",
    "start": "690920",
    "end": "695930"
  },
  {
    "text": "and that's all- all we need for",
    "start": "695930",
    "end": "698690"
  },
  {
    "text": "the training step where basically for each mini-batch we are going to sample uh,",
    "start": "698690",
    "end": "703070"
  },
  {
    "text": "M nodes at random,",
    "start": "703070",
    "end": "705065"
  },
  {
    "text": "and then we simply compute their embeddings by taking,",
    "start": "705065",
    "end": "708905"
  },
  {
    "text": "uh, W and, uh,",
    "start": "708905",
    "end": "710225"
  },
  {
    "text": "multiplying it with uh, with uh,",
    "start": "710225",
    "end": "712399"
  },
  {
    "text": "uh, with the corresponding, uh, uh,",
    "start": "712400",
    "end": "715490"
  },
  {
    "text": "entry in the- in the matrix, uh,",
    "start": "715490",
    "end": "718385"
  },
  {
    "text": "X tilde that corresponds to that,",
    "start": "718385",
    "end": "720560"
  },
  {
    "text": "uh, node, uh, um, uh, uh,",
    "start": "720560",
    "end": "723390"
  },
  {
    "text": "v. And we simply compute the final layer embeddings of all the nodes in the mini-batch,",
    "start": "723390",
    "end": "728090"
  },
  {
    "text": "um, and then, you know,",
    "start": "728090",
    "end": "729470"
  },
  {
    "text": "we can use these embeddings to make predictions,",
    "start": "729470",
    "end": "731480"
  },
  {
    "text": "compute the loss, uh,",
    "start": "731480",
    "end": "732785"
  },
  {
    "text": "and then update the matrix- the parameter matrix,",
    "start": "732785",
    "end": "736639"
  },
  {
    "text": "uh, W. So, um,",
    "start": "736640",
    "end": "739700"
  },
  {
    "text": "the- the good point here is that now the embedding of every node,",
    "start": "739700",
    "end": "743540"
  },
  {
    "text": "uh, computation of it is independent from the other nodes.",
    "start": "743540",
    "end": "747185"
  },
  {
    "text": "It is a simple, uh,",
    "start": "747185",
    "end": "748550"
  },
  {
    "text": "matrix times vector product where",
    "start": "748550",
    "end": "751310"
  },
  {
    "text": "the matrix W is what we are trying to learn and this can be done,",
    "start": "751310",
    "end": "754760"
  },
  {
    "text": "uh, super, super fast.",
    "start": "754760",
    "end": "756515"
  },
  {
    "text": "So, um, now let's,",
    "start": "756515",
    "end": "758900"
  },
  {
    "text": "uh, uh, summarize and kind of, uh,",
    "start": "758900",
    "end": "761195"
  },
  {
    "text": "compare, uh, uh, Cluster-GCN with,",
    "start": "761195",
    "end": "764090"
  },
  {
    "text": "uh, uh, other methods we have learned, uh, today.",
    "start": "764090",
    "end": "767330"
  },
  {
    "text": "The simplified GCN generates node embeddings much more, uh, efficiently.",
    "start": "767330",
    "end": "772070"
  },
  {
    "text": "There is no need to create the giant computation graph,",
    "start": "772070",
    "end": "775040"
  },
  {
    "text": "there is no need to do graph sampling,",
    "start": "775040",
    "end": "777063"
  },
  {
    "text": "it is all very, very simple, right?",
    "start": "777064",
    "end": "779450"
  },
  {
    "text": "You just do those matrix products and then the learning step is also very easy.",
    "start": "779450",
    "end": "785240"
  },
  {
    "text": "So, um, it seems great, but what do we lose?",
    "start": "785240",
    "end": "789080"
  },
  {
    "text": "Um, right, compared to Cluster-GCN,",
    "start": "789080",
    "end": "792200"
  },
  {
    "text": "mini-batch of nodes in a simplified GCN can be sampled completely at random from,",
    "start": "792200",
    "end": "797960"
  },
  {
    "text": "uh, the entire set of nodes.",
    "start": "797960",
    "end": "799625"
  },
  {
    "text": "As I said, there is no need to do group- uh,",
    "start": "799625",
    "end": "802325"
  },
  {
    "text": "to do node groups to do the,",
    "start": "802325",
    "end": "804260"
  },
  {
    "text": "uh, like in Cluster-GCN,",
    "start": "804260",
    "end": "805970"
  },
  {
    "text": "no need to do the induced subgraph, uh, nothing like this.",
    "start": "805970",
    "end": "809120"
  },
  {
    "text": "So this me- this means that, uh,",
    "start": "809120",
    "end": "810860"
  },
  {
    "text": "our- the training is very stable, um,",
    "start": "810860",
    "end": "814250"
  },
  {
    "text": "and the- the variance of the gradients,",
    "start": "814250",
    "end": "816589"
  },
  {
    "text": "uh, is much more under control.",
    "start": "816589",
    "end": "818615"
  },
  {
    "text": "But, right, what is the price?",
    "start": "818615",
    "end": "821240"
  },
  {
    "text": "The price is that this model is far,",
    "start": "821240",
    "end": "823670"
  },
  {
    "text": "far, uh, less expressive.",
    "start": "823670",
    "end": "825930"
  },
  {
    "text": "Meaning, compared to the original graph neural network models,",
    "start": "825930",
    "end": "829450"
  },
  {
    "text": "simplified GCN is far less",
    "start": "829450",
    "end": "831790"
  },
  {
    "text": "expressive because it lacked- it doesn't have the non-linearity,",
    "start": "831790",
    "end": "835899"
  },
  {
    "text": "uh, in generating, uh, node embedding.",
    "start": "835900",
    "end": "838465"
  },
  {
    "text": "So it means that the theory that we discussed",
    "start": "838465",
    "end": "841330"
  },
  {
    "text": "about computation graphs, Weisfeiler-Lehman isomorphism test,",
    "start": "841330",
    "end": "847155"
  },
  {
    "text": "keeping the structure of the underlying subgraphs, uh,",
    "start": "847155",
    "end": "850775"
  },
  {
    "text": "through that injective mapping,",
    "start": "850775",
    "end": "852650"
  },
  {
    "text": "all that is out of the window because we don't have the non-linearity anymore.",
    "start": "852650",
    "end": "857390"
  },
  {
    "text": "So that really makes a huge difference in the expressive power, uh, of the model.",
    "start": "857390",
    "end": "862595"
  },
  {
    "text": "Right? But, you know,",
    "start": "862595",
    "end": "864740"
  },
  {
    "text": "in many real-world cases,",
    "start": "864740",
    "end": "866990"
  },
  {
    "text": "a simplified GCN, uh,",
    "start": "866990",
    "end": "869149"
  },
  {
    "text": "tends to work well, um,",
    "start": "869150",
    "end": "871205"
  },
  {
    "text": "and tends to kind of work just slightly worse than the original graph neural networks,",
    "start": "871205",
    "end": "876530"
  },
  {
    "text": "even though being far less, uh,",
    "start": "876530",
    "end": "878510"
  },
  {
    "text": "expressive in, uh, theory.",
    "start": "878510",
    "end": "880850"
  },
  {
    "text": "So the question is, uh, why is that?",
    "start": "880850",
    "end": "884345"
  },
  {
    "text": "And the reason for that is something that is called graph homophily,",
    "start": "884345",
    "end": "888875"
  },
  {
    "text": "which basically means, uh,",
    "start": "888875",
    "end": "890540"
  },
  {
    "text": "this is a concept from social science, uh,",
    "start": "890540",
    "end": "893584"
  },
  {
    "text": "generally people like to call it, uh,",
    "start": "893585",
    "end": "896420"
  },
  {
    "text": "birds of feather, uh, sto- stick together.",
    "start": "896420",
    "end": "899615"
  },
  {
    "text": "So or birds of feather flock together.",
    "start": "899615",
    "end": "901970"
  },
  {
    "text": "So basically the idea is that similar people tends to connect with each other, right?",
    "start": "901970",
    "end": "907040"
  },
  {
    "text": "So that basically, you know,",
    "start": "907040",
    "end": "909005"
  },
  {
    "text": "computer scientists know each other,",
    "start": "909005",
    "end": "911285"
  },
  {
    "text": "uh, people who study music know each other.",
    "start": "911285",
    "end": "913370"
  },
  {
    "text": "So essentially, the idea is that you have",
    "start": "913370",
    "end": "914810"
  },
  {
    "text": "these social communities that are tightly compact where people share",
    "start": "914810",
    "end": "918740"
  },
  {
    "text": "properties and attributes just because it is easier",
    "start": "918740",
    "end": "921545"
  },
  {
    "text": "to connect to people who have something in common with you,",
    "start": "921545",
    "end": "924740"
  },
  {
    "text": "with whom you share some, uh, interest, right?",
    "start": "924740",
    "end": "927529"
  },
  {
    "text": "So basically what this means in,",
    "start": "927530",
    "end": "929090"
  },
  {
    "text": "let's say networks, both in social, biological,",
    "start": "929090",
    "end": "932825"
  },
  {
    "text": "knowledge graphs is that nodes connected by edges tend to",
    "start": "932825",
    "end": "936320"
  },
  {
    "text": "share the same labels they tend to have similar features.",
    "start": "936320",
    "end": "940655"
  },
  {
    "text": "Right? In citation networks,",
    "start": "940655",
    "end": "943025"
  },
  {
    "text": "papers from the same- same area tend to cite each other.",
    "start": "943025",
    "end": "945935"
  },
  {
    "text": "In movie recommendation, it's like people are interested in",
    "start": "945935",
    "end": "949490"
  },
  {
    "text": "given genres and you watch multiple movies from the same genre.",
    "start": "949490",
    "end": "953480"
  },
  {
    "text": "So these movies are kind of similar to each other.",
    "start": "953480",
    "end": "956329"
  },
  {
    "text": "So, um, and, you know,",
    "start": "956330",
    "end": "958370"
  },
  {
    "text": "why- why is this important for a simplified GCN?",
    "start": "958370",
    "end": "961100"
  },
  {
    "text": "Because, um, the t- the three- pre-processing step of",
    "start": "961100",
    "end": "965560"
  },
  {
    "text": "a simplified GCN is simply feature aggregation over a k-hop, uh, neighborhood, right?",
    "start": "965560",
    "end": "971830"
  },
  {
    "text": "So the pre-processing features obtained by- is- are",
    "start": "971830",
    "end": "975160"
  },
  {
    "text": "obtained by iteratively averaging, um, um, uh,",
    "start": "975160",
    "end": "978639"
  },
  {
    "text": "features of the neighbors and features of the neighbors of neighbors",
    "start": "978640",
    "end": "982835"
  },
  {
    "text": "without learned transformation and will- without any non-linearity.",
    "start": "982835",
    "end": "987455"
  },
  {
    "text": "So, as a result,",
    "start": "987455",
    "end": "988790"
  },
  {
    "text": "nodes connected by edges tend to have similar pre-processing,",
    "start": "988790",
    "end": "992480"
  },
  {
    "text": "uh, features, and now with labels, um,",
    "start": "992480",
    "end": "995389"
  },
  {
    "text": "are also clustered kind of across the homophilis parts of the network,",
    "start": "995390",
    "end": "1001060"
  },
  {
    "text": "um, if the labels kind of cluster a- across the network,",
    "start": "1001060",
    "end": "1004210"
  },
  {
    "text": "then simplified GCN will work, uh, really well.",
    "start": "1004210",
    "end": "1007660"
  },
  {
    "text": "So, uh, basically, the- the- when does the simplified GCN work?",
    "start": "1007660",
    "end": "1012610"
  },
  {
    "text": "The premise is that the model uses the pre-process node features to make prediction.",
    "start": "1012610",
    "end": "1016810"
  },
  {
    "text": "Nodes connected by edges tend to get",
    "start": "1016810",
    "end": "1019900"
  },
  {
    "text": "similar pre-processed features because it's all",
    "start": "1019900",
    "end": "1022450"
  },
  {
    "text": "about feature averaging across local neighborhoods.",
    "start": "1022450",
    "end": "1025540"
  },
  {
    "text": "So if nodes connected by edges tend to be in the same class,",
    "start": "1025540",
    "end": "1029560"
  },
  {
    "text": "tend to have the same label, then simplified GCN,",
    "start": "1029560",
    "end": "1033025"
  },
  {
    "text": "uh, is going to, uh,",
    "start": "1033025",
    "end": "1034540"
  },
  {
    "text": "make very accurate, uh,",
    "start": "1034540",
    "end": "1036520"
  },
  {
    "text": "predictions, uh, so basically if the graph has this kind of homophily structure.",
    "start": "1036520",
    "end": "1041020"
  },
  {
    "text": "Now, if the graph doesn't have this homophily structure,",
    "start": "1041020",
    "end": "1043944"
  },
  {
    "text": "then the simplified GCN is going to fail,",
    "start": "1043945",
    "end": "1046855"
  },
  {
    "text": "uh, quite, uh, quite bad.",
    "start": "1046855",
    "end": "1048910"
  },
  {
    "text": "So that's kind of the intuition,",
    "start": "1048910",
    "end": "1050590"
  },
  {
    "text": "and of course, ahead of time,",
    "start": "1050590",
    "end": "1052000"
  },
  {
    "text": "we generally don't know whether labels are clustered together",
    "start": "1052000",
    "end": "1055690"
  },
  {
    "text": "or they're kind of the- kind of sparkled, uh, across the network.",
    "start": "1055690",
    "end": "1059904"
  },
  {
    "text": "So to summarize, simplified GCN removes non-linearity in GCN and then reduces,",
    "start": "1059905",
    "end": "1066309"
  },
  {
    "text": "uh, the- uh, to simple, uh,",
    "start": "1066309",
    "end": "1068620"
  },
  {
    "text": "pre-processing of the node features and graph adjacency matrix.",
    "start": "1068620",
    "end": "1072205"
  },
  {
    "text": "When these pre-processed features are obtained on the CPU, a very scalable,",
    "start": "1072205",
    "end": "1077199"
  },
  {
    "text": "simple mini-batch- batch gra- stochastic gradient descent",
    "start": "1077199",
    "end": "1080860"
  },
  {
    "text": "can be directly applied to optimize the parameters.",
    "start": "1080860",
    "end": "1084595"
  },
  {
    "text": "Um, simplified GCN works surprisingly well in- in many benchmarks.",
    "start": "1084595",
    "end": "1090370"
  },
  {
    "text": "Uh, the reason for that being is that those benchmarks are easy.",
    "start": "1090370",
    "end": "1094315"
  },
  {
    "text": "Um, uh, nodes of similar label tend to link to each other.",
    "start": "1094315",
    "end": "1098230"
  },
  {
    "text": "They tend to be part,",
    "start": "1098230",
    "end": "1099415"
  },
  {
    "text": "uh, of the same network,",
    "start": "1099415",
    "end": "1100975"
  },
  {
    "text": "and meaning that just simple averaging of their features, um, uh,",
    "start": "1100975",
    "end": "1105385"
  },
  {
    "text": "without any nonlinearities and without any weight or different weighing of them,",
    "start": "1105385",
    "end": "1110290"
  },
  {
    "text": "um, gives you, uh,",
    "start": "1110290",
    "end": "1111700"
  },
  {
    "text": "good performance, uh, in practice.",
    "start": "1111700",
    "end": "1114799"
  }
]