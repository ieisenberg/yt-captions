[
  {
    "start": "0",
    "end": "65000"
  },
  {
    "text": "Hello. Welcome to the section on non-quadratic losses. So far, we've talked mainly about regression,",
    "start": "6050",
    "end": "15180"
  },
  {
    "text": "that is the case when the target variable is a real number or a real vector.",
    "start": "15180",
    "end": "23670"
  },
  {
    "text": "Uh, we've seen several different predictors for such things, including things such as neural networks and simple linear predictors,",
    "start": "23670",
    "end": "33320"
  },
  {
    "text": "as well as more complicated things such as trees. And we've seen quadratic losses and a few other example loss functions.",
    "start": "33320",
    "end": "43495"
  },
  {
    "text": "And so today, I want to go through a few more different non-quadratic loss functions and talk about",
    "start": "43495",
    "end": "50765"
  },
  {
    "text": "how the resulting predictors behave when we use those as losses and empirical risk minimization.",
    "start": "50765",
    "end": "61710"
  },
  {
    "text": "So when we're solving the empirical risk minimization problem,",
    "start": "67280",
    "end": "72290"
  },
  {
    "text": "the empirical risk, the average loss, is simply 1 on n, the sum from i is 1 up to n or the loss evaluated at y hat i,",
    "start": "72290",
    "end": "82430"
  },
  {
    "text": "y_i, where y hat i is the predicted value at position x_i.",
    "start": "82430",
    "end": "89100"
  },
  {
    "text": "So y hat i is g Theta of x_i. Now, very often, the loss function,",
    "start": "89100",
    "end": "96734"
  },
  {
    "text": "l of y hat y, the intention of it is it- that it's measuring the discrepancy between",
    "start": "96734",
    "end": "103850"
  },
  {
    "text": "y hat and y and why- how much is the deviation between y hat and y?",
    "start": "103850",
    "end": "109810"
  },
  {
    "text": "And so as a result, it's very common that the, uh, loss function looks like a penalty function,",
    "start": "109810",
    "end": "118325"
  },
  {
    "text": "p of y hat minus y. Here, p can take several different forms.",
    "start": "118325",
    "end": "127010"
  },
  {
    "text": "Uh, we've seen, for example, the square penalty, r squared, where the resulting loss function is therefore the square of y hat minus y.",
    "start": "127010",
    "end": "139200"
  },
  {
    "text": "Here, r, y hat minus y is the prediction error or the residual.",
    "start": "139390",
    "end": "146125"
  },
  {
    "text": "Um, if you've got a scalar y, then r is greater than 0 means that our predictor is overestimating,",
    "start": "146125",
    "end": "152705"
  },
  {
    "text": "and r is less than 0, it means that our predictor is underestimating. And not all loss functions have this form.",
    "start": "152705",
    "end": "160890"
  },
  {
    "text": "We've seen, for example, the, uh, percentage error which had a form which cannot",
    "start": "160890",
    "end": "168349"
  },
  {
    "text": "be expressed as the penalty function of y hat minus y.",
    "start": "168350",
    "end": "173710"
  },
  {
    "text": "So if we have a penalty function, it tells us about how much we object to different values of prediction error.",
    "start": "178220",
    "end": "185810"
  },
  {
    "text": "Um, very often, p of 0 is 0, and p of r is greater than or equal to 0 for every other r. Um,",
    "start": "185810",
    "end": "195569"
  },
  {
    "text": "very often, p is symmetric, p of minus r is p of r. In which case, we really are only determining the penalty based on",
    "start": "195950",
    "end": "204320"
  },
  {
    "text": "the magnitude or that is the absolute value of the prediction error. Sometimes, p is asymmetric,",
    "start": "204320",
    "end": "211590"
  },
  {
    "text": "p of minus r is not p of r. In which case, we get a different amount of penalty",
    "start": "211590",
    "end": "218900"
  },
  {
    "text": "depending on whether we're overestimating or underestimating. Uh, let's look at",
    "start": "218900",
    "end": "232830"
  },
  {
    "start": "227000",
    "end": "292000"
  },
  {
    "text": "two common penalty functions, here is the square penalty function versus the absolute value penalty function.",
    "start": "232830",
    "end": "242194"
  },
  {
    "text": "So when we're using the square penalty, that's p of r is r squared.",
    "start": "242195",
    "end": "249835"
  },
  {
    "text": "Well, when the prediction error is very small, well, then the penalty is really very- very small, small squared.",
    "start": "249835",
    "end": "258729"
  },
  {
    "text": "Um, when the prediction error is large, well then, the penalty is extremely large, large squared.",
    "start": "258729",
    "end": "267380"
  },
  {
    "text": "If you compare this with the absolute penalty, we'll see that the absolute penalty responds less.",
    "start": "267960",
    "end": "275925"
  },
  {
    "text": "For small prediction errors, the penalty is larger than the square penalty,",
    "start": "275925",
    "end": "281765"
  },
  {
    "text": "and for large prediction errors, the penalty is smaller than the square penalty.",
    "start": "281765",
    "end": "289330"
  },
  {
    "start": "292000",
    "end": "379000"
  },
  {
    "text": "Now, here's another one we've seen before when we were looking at quantiles. This is the tilted absolute penalty function,",
    "start": "298660",
    "end": "305790"
  },
  {
    "text": "uh, it is parameterized by tau. It's an absolute value but it's tilted.",
    "start": "305790",
    "end": "311790"
  },
  {
    "text": "And so when r is negative, the penalty is minus tau times r, and when r is positive,",
    "start": "311790",
    "end": "321090"
  },
  {
    "text": "it's 1 minus tau times r. And tau is somewhere between zero and one.",
    "start": "321090",
    "end": "328324"
  },
  {
    "text": "When tau is a half, the tilted absolute penalty function isn't tilted,",
    "start": "328325",
    "end": "334550"
  },
  {
    "text": "becomes the same as one-half the absolute value of r. So we have the same penalty for underestimating as overestimating.",
    "start": "334550",
    "end": "342475"
  },
  {
    "text": "When tau is greater than a half, well then, it's worse to underestimate than to overestimate.",
    "start": "342475",
    "end": "349130"
  },
  {
    "text": "Remember, the residual is y hat minus y, so the residual is positive means that y hat is greater than y.",
    "start": "349130",
    "end": "359730"
  },
  {
    "text": "And so when tau is greater than a half, well then, our penalty is less for positive r and for negative r. And similarly,",
    "start": "359730",
    "end": "370615"
  },
  {
    "text": "when tau is less than a half, it's worse to overestimate than to underestimate.",
    "start": "370615",
    "end": "376680"
  },
  {
    "start": "379000",
    "end": "429000"
  },
  {
    "text": "Now, the penalty function expresses how you feel about large and small prediction errors,",
    "start": "381110",
    "end": "390444"
  },
  {
    "text": "about positive and negative prediction errors. And as a result, they drive",
    "start": "390445",
    "end": "395635"
  },
  {
    "text": "the predictor to prefer certain- making certain types of predictions.",
    "start": "395635",
    "end": "401230"
  },
  {
    "text": "Um, in other words, the choice of the penalty function is going to",
    "start": "401230",
    "end": "407155"
  },
  {
    "text": "change the shape of the histogram of prediction errors.",
    "start": "407155",
    "end": "413480"
  },
  {
    "text": "Remember how we construct a histogram? We look at all the residuals, and we divide them up into bins,",
    "start": "413630",
    "end": "420370"
  },
  {
    "text": "and we display this as a bar graph which shows the distribution of residuals.",
    "start": "420370",
    "end": "426380"
  },
  {
    "start": "429000",
    "end": "605000"
  },
  {
    "text": "So here's an example. Here, we constructed random data, 300 data records,",
    "start": "429260",
    "end": "436715"
  },
  {
    "text": "each one of which has a scalar y and a 30 dimensional view,",
    "start": "436715",
    "end": "442764"
  },
  {
    "text": "and then we append a constant 1 so that x stops with 1 and then has 30 random numbers afterwards.",
    "start": "442765",
    "end": "451445"
  },
  {
    "text": "And we use a 50/50 test train split here. And, uh, here, the residual is y hat minus y,",
    "start": "451445",
    "end": "461370"
  },
  {
    "text": "so the ith residual is Theta transpose x_i minus y_i.",
    "start": "461370",
    "end": "466620"
  },
  {
    "text": "Now, we've plotted here the histogram of the residuals for",
    "start": "466620",
    "end": "471725"
  },
  {
    "text": "the- the predictor that minimizes the empirical risk.",
    "start": "471725",
    "end": "477305"
  },
  {
    "text": "Now, we've done this in two cases. The first case is the top case where we're using a square penalty,",
    "start": "477305",
    "end": "486689"
  },
  {
    "text": "and so this curve here is our penalty function.",
    "start": "487060",
    "end": "494715"
  },
  {
    "text": "Now, we can say we've got some distribution of residuals, and if we look at that in the test data,",
    "start": "494715",
    "end": "503404"
  },
  {
    "text": "we get a fairly similar distribution of residuals as we'd expect.",
    "start": "503404",
    "end": "510490"
  },
  {
    "text": "Now, we also try this with the loss function being the tilted penalty of the residual.",
    "start": "510520",
    "end": "522269"
  },
  {
    "text": "So now, it's a loss of y hat comma y is the tilted penalty function of y hat minus y. And what do we see-",
    "start": "522270",
    "end": "534270"
  },
  {
    "text": "We see that on the right-hand side of this plot,",
    "start": "542130",
    "end": "548440"
  },
  {
    "text": "there's many fewer residuals, many fewer data points for which the residual is positive.",
    "start": "548440",
    "end": "559224"
  },
  {
    "text": "Then there are data points for which the residual is negative because",
    "start": "559224",
    "end": "565120"
  },
  {
    "text": "the penalty is much greater for producing a positive residual and in other words,",
    "start": "565120",
    "end": "572290"
  },
  {
    "text": "for overestimating, than for underestimating. And so our predictor-a predictor that is chosen by",
    "start": "572290",
    "end": "580390"
  },
  {
    "text": "empirical risk minimization turns out to be a predictor that prefers to underestimate than to overestimate.",
    "start": "580390",
    "end": "589240"
  },
  {
    "text": "And that extends also to the test data. As much as we might expect that",
    "start": "589240",
    "end": "594430"
  },
  {
    "text": "there's far fewer data points on the right-hand side of the test data, block test residual histogram than",
    "start": "594430",
    "end": "602140"
  },
  {
    "text": "there are on the left-hand side. Now I want to switch to another type of error that",
    "start": "602140",
    "end": "610480"
  },
  {
    "start": "605000",
    "end": "746000"
  },
  {
    "text": "we might want to shape and that's the case of outliers, and this is called the best-fitting.",
    "start": "610480",
    "end": "617995"
  },
  {
    "text": "So, in some applications you have a few data points that are just wrong- that are just way of.",
    "start": "617995",
    "end": "625720"
  },
  {
    "text": "Sometimes this occurs because when the data was entered or transcribed,",
    "start": "625720",
    "end": "630745"
  },
  {
    "text": "it was entered incorrectly. There's an error in the decimal point position or somebody just made a typo.",
    "start": "630745",
    "end": "638095"
  },
  {
    "text": "Other times they're caused by sensor failures or various types of anomalies that occurred when the data was collected.",
    "start": "638095",
    "end": "651865"
  },
  {
    "text": "And these uh, points are called outliers. And a consequence of having outliers,",
    "start": "651865",
    "end": "659020"
  },
  {
    "text": "even if you've got only a very few of them, is that the ERM can pick a very poor predictor.",
    "start": "659020",
    "end": "665545"
  },
  {
    "text": "Um, now there are several methods for removing outliers.",
    "start": "665545",
    "end": "671829"
  },
  {
    "text": "Um, ah here's one. Um, create a predictor just based on the entire data set.",
    "start": "671830",
    "end": "679600"
  },
  {
    "text": "And now look through your data points and for each data point,",
    "start": "679600",
    "end": "684850"
  },
  {
    "text": "compute the residual, compute the prediction error. And those data points that have very large prediction errors,",
    "start": "684850",
    "end": "693325"
  },
  {
    "text": "mark them as outliers, and remove them from the data set. And then re-fit and if you've still got poor prediction,",
    "start": "693325",
    "end": "703000"
  },
  {
    "text": "so a large empirical risk, go through and do it again.",
    "start": "703000",
    "end": "707990"
  },
  {
    "text": "This method can work well. It's not easy to implement because one",
    "start": "708960",
    "end": "716230"
  },
  {
    "text": "has to decide what it means to have a large prediction error. And it's certainly possible to choose that poorly and not be aware of that, um.",
    "start": "716230",
    "end": "730940"
  },
  {
    "text": "Another way of, uh, removing outliers or at least handling outliers and that is to use",
    "start": "730950",
    "end": "736449"
  },
  {
    "text": "a penalty function that's less sensitive to outlier data points.",
    "start": "736450",
    "end": "741775"
  },
  {
    "text": "And we're going to look at that in this section. These kind of penalty functions are called robust.",
    "start": "741775",
    "end": "750475"
  },
  {
    "start": "746000",
    "end": "832000"
  },
  {
    "text": "The robust penalty function is one that has low sensitivity to outliers.",
    "start": "750475",
    "end": "758029"
  },
  {
    "text": "And the idea is- is that the way you make a penalty function robust is you make sure that it grows more",
    "start": "758310",
    "end": "764620"
  },
  {
    "text": "slowly for large prediction errors in particular than the square penalty.",
    "start": "764620",
    "end": "771650"
  },
  {
    "text": "And so that means that the predictor",
    "start": "771810",
    "end": "778840"
  },
  {
    "text": "will not be predisposed to avoiding those large prediction errors,",
    "start": "778840",
    "end": "785785"
  },
  {
    "text": "which presumably occur with the outliers. So they respond a bit more gracefully to the outliers.",
    "start": "785785",
    "end": "793090"
  },
  {
    "text": "So instead of the error, in the- in other words,",
    "start": "793090",
    "end": "798700"
  },
  {
    "text": "instead of the empirical risk being dominated by the loss and a few very large values of y.",
    "start": "798700",
    "end": "810805"
  },
  {
    "text": "Because the loss function doesn't grow so rapidly with y hat minus y.",
    "start": "810805",
    "end": "820645"
  },
  {
    "text": "Those aren't the values that dominate the empirical risk any more. And we end it with a robust predictor that fits most of the data reasonably well.",
    "start": "820645",
    "end": "830720"
  },
  {
    "start": "832000",
    "end": "945000"
  },
  {
    "text": "And the most famous robust penalty function is called a Huber penalty function.",
    "start": "832080",
    "end": "838075"
  },
  {
    "text": "This is named after a Swiss statistician named Peter Huber and it looks like this.",
    "start": "838075",
    "end": "845020"
  },
  {
    "text": "Um, between, ah, it is a function which is defined in two pieces.",
    "start": "845020",
    "end": "854350"
  },
  {
    "text": "Ah, first it's defined for small r and then separately it's",
    "start": "854350",
    "end": "859810"
  },
  {
    "text": "defined for large r. And so the transition value will be, um, Alpha.",
    "start": "859810",
    "end": "865765"
  },
  {
    "text": "And so for r with absolute value less than Alpha, it's just the quadratic function.",
    "start": "865765",
    "end": "871850"
  },
  {
    "text": "It's just r squared. And for r, greater than Alpha, it's a linear function.",
    "start": "873360",
    "end": "887839"
  },
  {
    "text": "And the slope of that and the intercept of that linear function I've chosen so that they meet up very",
    "start": "891300",
    "end": "897250"
  },
  {
    "text": "nicely with the quadratic at Alpha and here Alpha is 1.",
    "start": "897250",
    "end": "903970"
  },
  {
    "text": "And so here we've got a function which behaves exactly like the quadratic.",
    "start": "903970",
    "end": "909745"
  },
  {
    "text": "When r is small, when the residual is small, it will penalize the residual exactly the same amount as the quadratic penalty would.",
    "start": "909745",
    "end": "918565"
  },
  {
    "text": "But when the residual gets large, it penalizes much less than the quadratic penalty would.",
    "start": "918565",
    "end": "930160"
  },
  {
    "text": "And there's an explicit formula here for the Huber penalty. It's r squared when r is small and some linear function of r when r is large.",
    "start": "930160",
    "end": "939790"
  },
  {
    "text": "Um, let's look at what it does.",
    "start": "939790",
    "end": "947920"
  },
  {
    "text": "So here, what we have is- we have a simple scalar ERM problem.",
    "start": "947920",
    "end": "955959"
  },
  {
    "text": "We have a bunch of data points. And what's plotted here is on this horizontal axis we have u and on this vertical axis",
    "start": "955960",
    "end": "970300"
  },
  {
    "text": "we have v and then have the embedding that x is equal to 1",
    "start": "970300",
    "end": "977140"
  },
  {
    "text": "and u and y is equal to v. So we're gonna use a linear predictor here.",
    "start": "977140",
    "end": "984685"
  },
  {
    "text": "So we're going to use y hat is equal to Theta transpose 1 u.",
    "start": "984685",
    "end": "992350"
  },
  {
    "text": "And we're fitting a straight line to a bunch of data. And we have, um,",
    "start": "992350",
    "end": "1000899"
  },
  {
    "text": "a whole bunch of data points sitting in the middle here.",
    "start": "1000900",
    "end": "1005110"
  },
  {
    "text": "And then we have, out here and out here, some data points that are outliers.",
    "start": "1006260",
    "end": "1014295"
  },
  {
    "text": "And as a result, if we use the square penalty function,",
    "start": "1014295",
    "end": "1019334"
  },
  {
    "text": "so we solve this problem using least squares. We end up with a predictor,",
    "start": "1019335",
    "end": "1026079"
  },
  {
    "text": "which is that straight line there, which as we can see, is doing rather poorly at fitting this data set.",
    "start": "1027080",
    "end": "1035625"
  },
  {
    "text": "And we can think about it as these data points are pulling the line down,",
    "start": "1035625",
    "end": "1042584"
  },
  {
    "text": "and these data points are pulling the line up and we're twisting our predictor.",
    "start": "1042585",
    "end": "1051340"
  },
  {
    "text": "And as a result, even though we got many more data points that are not",
    "start": "1053870",
    "end": "1059205"
  },
  {
    "text": "outliers than data points that are outliers,",
    "start": "1059205",
    "end": "1064414"
  },
  {
    "text": "what matters is the square of the distance between the predicted value and the actual data point.",
    "start": "1064415",
    "end": "1074045"
  },
  {
    "text": "And so it's the square of the distance between a data point down here and the predicted value up here.",
    "start": "1074045",
    "end": "1081285"
  },
  {
    "text": "That distance squared is how much that data point is going to contribute to the empirical risk.",
    "start": "1081285",
    "end": "1089684"
  },
  {
    "text": "These data points contribute those distance squared.",
    "start": "1089685",
    "end": "1098140"
  },
  {
    "text": "And so even though there are many more data points that are true data points,",
    "start": "1098140",
    "end": "1107200"
  },
  {
    "text": "the distance of the outliers from the predictor is so large that by the time you square the magnitude of those corresponding residuals,",
    "start": "1107200",
    "end": "1116455"
  },
  {
    "text": "the resulting terms in the empirical risk and up swamping the true loss terms in the empirical risk.",
    "start": "1116455",
    "end": "1127610"
  },
  {
    "text": "The Huber, if we solve the same problem, but instead of minimizing the empirical risk with a square loss,",
    "start": "1127670",
    "end": "1138160"
  },
  {
    "text": "we minimize the empirical risk with a Huber loss. We end up with a predictor that looks like this.",
    "start": "1138160",
    "end": "1145460"
  },
  {
    "text": "It's still being influenced by the outliers, but as we can see, it's influenced a lot less.",
    "start": "1147200",
    "end": "1153215"
  },
  {
    "text": "And it's doing a reasonable job at the rest of the data as a result.",
    "start": "1153215",
    "end": "1159260"
  },
  {
    "start": "1164000",
    "end": "1261000"
  },
  {
    "text": "Now you can take this further. You can say, okay, well, I'd like, ah,",
    "start": "1165830",
    "end": "1171390"
  },
  {
    "text": "a penalty function which is even less sensitive to outliers than the Huber penalty.",
    "start": "1171390",
    "end": "1178140"
  },
  {
    "text": "Here's a penalty that, uh, we might call the log Huber. Again, within a particular region here Alpha is 1.",
    "start": "1178140",
    "end": "1189510"
  },
  {
    "text": "So between minus 1 and 1, when the residual is between minus 1 and 1, we got a quadratic function.",
    "start": "1189510",
    "end": "1195495"
  },
  {
    "text": "And when the residual is larger than that in absolute value, we've got a logarithmic function.",
    "start": "1195495",
    "end": "1202539"
  },
  {
    "text": "And we can choose the terms in that logarithmic function such that the- um,",
    "start": "1204320",
    "end": "1213509"
  },
  {
    "text": "it joins up nicely with the quadratic with the slope at that point.",
    "start": "1213510",
    "end": "1219270"
  },
  {
    "text": "Notice that even though it says log y squared there, of course log of y squared is just 2 log y, um.",
    "start": "1219270",
    "end": "1227470"
  },
  {
    "text": "And, um, so we got, ah, something which is quadratic for a small y and logarithmic for large y.",
    "start": "1228290",
    "end": "1235410"
  },
  {
    "text": "And that means that when we compare this with the quadratic loss at large R,",
    "start": "1235410",
    "end": "1242085"
  },
  {
    "text": "we're really discounting a lot, very large residuals.",
    "start": "1242085",
    "end": "1248804"
  },
  {
    "text": "We have a diminishing penalty at large residuals or at large y effectively.",
    "start": "1248805",
    "end": "1259180"
  },
  {
    "start": "1261000",
    "end": "1467000"
  },
  {
    "text": "And so if we can- if we apply the log Huber loss function,",
    "start": "1261380",
    "end": "1266640"
  },
  {
    "text": "to our same data problem, to our same data feeding problem.",
    "start": "1266640",
    "end": "1271420"
  },
  {
    "text": "We have here a fit which really passe-",
    "start": "1272750",
    "end": "1278760"
  },
  {
    "text": "passes straight through the middle of the true data points,",
    "start": "1278760",
    "end": "1284250"
  },
  {
    "text": "and effectively ignores the outliers.",
    "start": "1284250",
    "end": "1288640"
  },
  {
    "text": "We can see this in terms of the error histograms as well.",
    "start": "1293630",
    "end": "1298905"
  },
  {
    "text": "Here, um, the top row of these- these plots show the results from applying the square loss function.",
    "start": "1298905",
    "end": "1310320"
  },
  {
    "text": "Here is our predictor. We can see we've got a residual of training errors.",
    "start": "1310320",
    "end": "1317970"
  },
  {
    "text": "So these are the training residuals which are spread out between minus 2 and 2. And then we see very similar things in test that we do on train as we should.",
    "start": "1317970",
    "end": "1328170"
  },
  {
    "text": "Uh, in this plot on the left, some of the data points some marked blue and some of them are marked red.",
    "start": "1328260",
    "end": "1334505"
  },
  {
    "text": "The blue ones are the training points, and the red ones are the test points.",
    "start": "1334505",
    "end": "1340975"
  },
  {
    "text": "Here's the Huber. The fit that results from using the Huber loss function.",
    "start": "1340975",
    "end": "1348555"
  },
  {
    "text": "And we can see that what's happened is that the residuals have split up, right?",
    "start": "1348555",
    "end": "1355020"
  },
  {
    "text": "There's a whole bunch of residuals that are now very close to zero. Those are the true data points.",
    "start": "1355020",
    "end": "1361094"
  },
  {
    "text": "And we can really clearly see the outliers. And the same thing we see ever on test.",
    "start": "1361094",
    "end": "1370320"
  },
  {
    "text": "When we apply the log Huber. We can see that the training residuals near",
    "start": "1370320",
    "end": "1375900"
  },
  {
    "text": "zero get even smaller and get even closer to 0. We've only got two bars rather than four bars,",
    "start": "1375900",
    "end": "1381780"
  },
  {
    "text": "um, in the middle, and the same thing in test.",
    "start": "1381780",
    "end": "1387040"
  },
  {
    "text": "So one could from either the Huber or the log Huber,",
    "start": "1387080",
    "end": "1393059"
  },
  {
    "text": "immediately identify the residuals.",
    "start": "1393060",
    "end": "1398115"
  },
  {
    "text": "And we could simply say, these data points here, we're gonna remove from our dataset,",
    "start": "1398115",
    "end": "1404175"
  },
  {
    "text": "and then we're gonna fit again. And of course, when we fit again, having removed those data points and we can use",
    "start": "1404175",
    "end": "1409799"
  },
  {
    "text": "anyone of these loss functions, and it would be fine. It's also important to notice here that,",
    "start": "1409800",
    "end": "1417930"
  },
  {
    "text": "when you look at plots like the ones on the left, it's tempting to think, well, why do we need a machine like this in order to be able to identify outliers?",
    "start": "1417930",
    "end": "1427110"
  },
  {
    "text": "And the answer is, because here x and y are one dimensional.",
    "start": "1427110",
    "end": "1432159"
  },
  {
    "text": "If, uh, x is in a million dimensions and y is in 100 dimensions,",
    "start": "1432160",
    "end": "1438720"
  },
  {
    "text": "well and suddenly we can't make these kind of plots anymore, and we can't see which data points are outliers and which ones are not.",
    "start": "1438720",
    "end": "1446755"
  },
  {
    "text": "Nonetheless, we can still plot the residuals or at least the norm of",
    "start": "1446755",
    "end": "1452790"
  },
  {
    "text": "the residuals and the histograms of the norms of the residuals and be able to identify outliers that way.",
    "start": "1452790",
    "end": "1461500"
  },
  {
    "start": "1467000",
    "end": "1507000"
  },
  {
    "text": "So the next topic I want to address is quantile regression. The idea here is that we're going to do ERM or Regularized ERM.",
    "start": "1467000",
    "end": "1474945"
  },
  {
    "text": "And we're gonna use a loss function which is generated by the tilted penalty function.",
    "start": "1474945",
    "end": "1481139"
  },
  {
    "text": "And that's called quantile regression. And the intuition is that when Tau is greater than a half,",
    "start": "1481140",
    "end": "1489960"
  },
  {
    "text": "that's gonna make it worse to underestimate, and so we're going to end up with predictions that are high.",
    "start": "1489960",
    "end": "1499095"
  },
  {
    "text": "When Tau is less than a half it makes it worse to overestimate, so we're gonna end with predictions that are low.",
    "start": "1499095",
    "end": "1504520"
  },
  {
    "start": "1507000",
    "end": "2114000"
  },
  {
    "text": "Now, we can be explicit as to exactly how high or low these predictions are.",
    "start": "1508010",
    "end": "1516975"
  },
  {
    "text": "And the way this works is that we're going to assume the predictor has",
    "start": "1516975",
    "end": "1522554"
  },
  {
    "text": "the form Theta_1 plus g Theta tilde of x.",
    "start": "1522555",
    "end": "1532140"
  },
  {
    "text": "And that means that, say Theta_1 might correspond to the- a linear predictor where x_1 is 1.",
    "start": "1532140",
    "end": "1541575"
  },
  {
    "text": "And so the resulting prediction has a Theta_1 term in it. And we're gonna assume Theta_1 is not regularized.",
    "start": "1541575",
    "end": "1549510"
  },
  {
    "text": "The other components of Theta might be regularized and g tilde of Theta of x,",
    "start": "1549510",
    "end": "1555165"
  },
  {
    "text": "it may not be a linear predictor, it could, it's an arbitrary predictor.",
    "start": "1555165",
    "end": "1560320"
  },
  {
    "text": "In other words, that the regularizer r of Theta does not depend on Theta_1.",
    "start": "1560570",
    "end": "1566144"
  },
  {
    "text": "So we might have ridge regression where we're using the square regularizer.",
    "start": "1566145",
    "end": "1571695"
  },
  {
    "text": "And so r of Theta is Theta_2 squared plus Theta_3 squared all the way up to Theta_d squared.",
    "start": "1571695",
    "end": "1578620"
  },
  {
    "text": "Now, if you do this, then it turns out that on a training set,",
    "start": "1580490",
    "end": "1585630"
  },
  {
    "text": "when you're using the regularized ERM predictor. The 1 minus Tau quantile of residuals is 0.",
    "start": "1585630",
    "end": "1594570"
  },
  {
    "text": "In other words, the fraction of data for which we overestimate is Tau.",
    "start": "1594570",
    "end": "1602590"
  },
  {
    "text": "And that's why it's called quantile regression.",
    "start": "1602750",
    "end": "1608050"
  },
  {
    "text": "If the predictor generalizes and we would expect to see the same thing in the test data that we see in the training data.",
    "start": "1609620",
    "end": "1618705"
  },
  {
    "text": "Notice that in the training data, this is exact, apart from the possibility that there'll not be",
    "start": "1618705",
    "end": "1625275"
  },
  {
    "text": "data points with repeated residual values.",
    "start": "1625275",
    "end": "1630280"
  },
  {
    "text": "We can create predictors for many different Taus, which give many different quantile estimates for a particular x.",
    "start": "1631370",
    "end": "1640000"
  },
  {
    "text": "Let's look at why we get this phenomena. Remember what we saw in this section on constant predictors.",
    "start": "1641930",
    "end": "1649304"
  },
  {
    "text": "We saw that if we minimize- [NOISE] that if we minimize the empirical risk 1 on n,",
    "start": "1649305",
    "end": "1661980"
  },
  {
    "text": "the sum from i is 1 up to n of the tilted loss of Theta minus y_i.",
    "start": "1661980",
    "end": "1673259"
  },
  {
    "text": "That the resulting Theta has the property that the Theta is",
    "start": "1673260",
    "end": "1679320"
  },
  {
    "text": "the Tau quantile of the y_i's.",
    "start": "1679320",
    "end": "1685929"
  },
  {
    "text": "Now, here we're minimizing something different.",
    "start": "1687650",
    "end": "1693285"
  },
  {
    "text": "We're minimizing 1 on n, the sum over i is 1 n up to n of p Tau",
    "start": "1693285",
    "end": "1702105"
  },
  {
    "text": "of g Theta of x_i minus y_i.",
    "start": "1702105",
    "end": "1711070"
  },
  {
    "text": "So that's not quite the same, but let's split it up by using the property of g Theta.",
    "start": "1713510",
    "end": "1720855"
  },
  {
    "text": "And that is the g Theta is Theta_1 plus g tilde Theta of x_i minus y_i.",
    "start": "1720855",
    "end": "1730210"
  },
  {
    "text": "Now we're going to minimize this over Theta, and that means our problem is minimize over Theta L of Theta.",
    "start": "1730910",
    "end": "1742480"
  },
  {
    "text": "Now, Theta has multiple components, Theta_1 through Theta_d. And we can regard this minimization as equivalent to,",
    "start": "1742480",
    "end": "1750440"
  },
  {
    "text": "it is equivalent to, minimum over Theta_1, minimum over the rest of Theta, L of Theta.",
    "start": "1750440",
    "end": "1759750"
  },
  {
    "text": "And once we've done this part of the minimization here, what's left is a minimization that looks exactly like the one we had before.",
    "start": "1760700",
    "end": "1774570"
  },
  {
    "text": "It looks like this, where now we have",
    "start": "1774570",
    "end": "1780285"
  },
  {
    "text": "Theta_1 plus a term g th- Theta tilde x_i minus y_i,",
    "start": "1780285",
    "end": "1788340"
  },
  {
    "text": "which doesn't depend on Theta_1, right? As a result, Theta_1 is going to turn out to be",
    "start": "1788340",
    "end": "1796250"
  },
  {
    "text": "the Tau quantile of y_i minus g Theta tilde of x_i.",
    "start": "1796250",
    "end": "1803160"
  },
  {
    "text": "We can say that another way. The fraction of the data points for which y_i minus g Theta tilde",
    "start": "1804950",
    "end": "1814020"
  },
  {
    "text": "of x_i is less than or equal to 0- is less than or equal to Theta_1, is around Tau.",
    "start": "1814020",
    "end": "1821470"
  },
  {
    "text": "Subtracting Theta_1 from both sides gives us the residual.",
    "start": "1823730",
    "end": "1828989"
  },
  {
    "text": "And so the fraction of i for which the residual is greater than or equal to 0 is around tau.",
    "start": "1828989",
    "end": "1837225"
  },
  {
    "text": "The fraction of data points for which we overestimate is around Tau.",
    "start": "1837225",
    "end": "1842530"
  },
  {
    "text": "Here's an example. Here we have again some random data,",
    "start": "1849020",
    "end": "1854820"
  },
  {
    "text": "and we've solved ERM for, where we're using a loss function,",
    "start": "1854820",
    "end": "1862350"
  },
  {
    "text": "which is a tilted loss function, and we can see exactly what we expect to see.",
    "start": "1862350",
    "end": "1868545"
  },
  {
    "text": "When Tau is 0.1, 90% of the data has a residual which is less than 0.",
    "start": "1868545",
    "end": "1875565"
  },
  {
    "text": "When Tau is 0.9, 10% of the data has a residual which is less than 0.",
    "start": "1875565",
    "end": "1882825"
  },
  {
    "text": "And when Tau is a half, about half of the data is one side to half the data as the other side.",
    "start": "1882825",
    "end": "1888880"
  },
  {
    "text": "Let's look at an example of ERM where we're trying to fit data.",
    "start": "1891470",
    "end": "1901740"
  },
  {
    "text": "Here we have a bunch of data points. Again, similar one-dimensional u, one-dimensional v,",
    "start": "1901740",
    "end": "1909390"
  },
  {
    "text": "and we're going to embed by x is equal to 1u,",
    "start": "1909390",
    "end": "1914910"
  },
  {
    "text": "so we'll have a constant term and, er, a linear term in our predictor.",
    "start": "1914910",
    "end": "1922530"
  },
  {
    "text": "We've got blue points here, which are training points, and red points here, which are test points. Um, and we're going to fit a straight line prediction model using the tilted loss.",
    "start": "1922530",
    "end": "1934260"
  },
  {
    "text": "That's three different values of Tau. And these are the resulting predictors.",
    "start": "1934260",
    "end": "1943660"
  },
  {
    "text": "Uh, this is Tau is 0.1, the predictor for which our prediction at any given value of u,",
    "start": "1946220",
    "end": "1957480"
  },
  {
    "text": "let me pick a value of u, say, I pick point 3, the prediction is somewhere around one,",
    "start": "1957480",
    "end": "1964005"
  },
  {
    "text": "which is clearly above most of the data.",
    "start": "1964005",
    "end": "1969675"
  },
  {
    "text": "The prediction- the predictor prefers to overestimate than to underestimate.",
    "start": "1969675",
    "end": "1978390"
  },
  {
    "text": "When Tau is a half, we get a predictor that's somewhere in the middle,",
    "start": "1978390",
    "end": "1984019"
  },
  {
    "text": "and when Tau is 0.1, we get a predictor that prefers to underestimate than to overestimate.",
    "start": "1984020",
    "end": "1990840"
  },
  {
    "text": "Another way to view this data is to plot",
    "start": "1992510",
    "end": "1998460"
  },
  {
    "text": "the predicted value and the true value for the data points on one plot.",
    "start": "1999590",
    "end": "2008450"
  },
  {
    "text": "So here, we have the true value of v, or of y,",
    "start": "2008450",
    "end": "2013773"
  },
  {
    "text": "and here we have the predicted value v-hat on the vertical axis,",
    "start": "2013774",
    "end": "2019100"
  },
  {
    "text": "so each of these three plots. Now, in the ideal world, v-hat would be equal to v,",
    "start": "2019100",
    "end": "2025145"
  },
  {
    "text": "our predictor would produce perfect predictions, but, of course,",
    "start": "2025145",
    "end": "2030575"
  },
  {
    "text": "we know that's not very likely, particularly with this dataset, and so, um, we see that v-hat and v are spread out.",
    "start": "2030575",
    "end": "2042815"
  },
  {
    "text": "So in the ideal world, we'd have v-hat and v would be a bunch of data points that would live on the diagonal,",
    "start": "2042815",
    "end": "2049940"
  },
  {
    "text": "and that would be the perfect predictor. If you go to predictor that tends to overestimate, well then,",
    "start": "2049940",
    "end": "2058429"
  },
  {
    "text": "the predictor tends to produce v-hats that are greater than v, and that means it tends to produce points that are in this half of the plane.",
    "start": "2058430",
    "end": "2068585"
  },
  {
    "text": "And if you've got a predictor that prefers to underestimate, it tends to produce v-hats less than v,",
    "start": "2068585",
    "end": "2074225"
  },
  {
    "text": "and we end up with points in this half of the plane. And we can, um,",
    "start": "2074225",
    "end": "2083659"
  },
  {
    "text": "look at, er, er, if this is training data, we can look at our data and count and we would find",
    "start": "2083660",
    "end": "2091068"
  },
  {
    "text": "that 90% of the points are above the line here and 90% of the points are below the line over here.",
    "start": "2091069",
    "end": "2101340"
  },
  {
    "text": "And, er, with, er, er,",
    "start": "2101710",
    "end": "2107585"
  },
  {
    "text": "Tau is a half, well then we should see exactly half of the points on one side of the line and half for the points on the other side of the line.",
    "start": "2107585",
    "end": "2114945"
  },
  {
    "start": "2114000",
    "end": "2276000"
  },
  {
    "text": "So there's another way of plotting this data and that is to look at the cumulative distribution of the residuals.",
    "start": "2114945",
    "end": "2123565"
  },
  {
    "text": "So for any given q, we are going to plot q here on the horizontal axis,",
    "start": "2123565",
    "end": "2136820"
  },
  {
    "text": "and then on the vertical axis, we will plot the number of data points for which the residual is less than q.",
    "start": "2136820",
    "end": "2149070"
  },
  {
    "text": "And in particular, I've got three plots here, one shows the residuals when we've computed the predictor using Tau as 0.1,",
    "start": "2149200",
    "end": "2161029"
  },
  {
    "text": "another when we've computed the predictor using Tau as 0.5, and a third using the- showing the residuals when Tau is 0.9.",
    "start": "2161030",
    "end": "2169345"
  },
  {
    "text": "And if we look at this plot, and we look at q is equal to 0,",
    "start": "2169345",
    "end": "2176379"
  },
  {
    "text": "this- the green predictor with Tau as a half shows us that",
    "start": "2176379",
    "end": "2182285"
  },
  {
    "text": "exactly one-half of the training data points",
    "start": "2182285",
    "end": "2187369"
  },
  {
    "text": "are less than ze- have a residual less than 0. When Tau is 0.1, we see exactly fraction 0.9,",
    "start": "2187370",
    "end": "2197539"
  },
  {
    "text": "so exactly 90% of the data points have a residual less than 0.",
    "start": "2197540",
    "end": "2205355"
  },
  {
    "text": "When Tau is 0.9, we see exactly 10% of the data points have a residual less than 0.",
    "start": "2205355",
    "end": "2216710"
  },
  {
    "text": "And so, here we're saying that the, er,",
    "start": "2216710",
    "end": "2222380"
  },
  {
    "text": "quantile regression is giving us exactly the quantiles we expected it to on the training data.",
    "start": "2222380",
    "end": "2230900"
  },
  {
    "text": "On the test data, let's have a look how well we did. Um, at, er, er, q is 0,",
    "start": "2230900",
    "end": "2239030"
  },
  {
    "text": "er, we're getting almost exactly 0.1. When Tau is 0.9, er, we're getting 0.45,",
    "start": "2239030",
    "end": "2248870"
  },
  {
    "text": "so we're getting 45% of the data have a residual less than 0  when Tau is a half,",
    "start": "2248870",
    "end": "2255050"
  },
  {
    "text": "and we're getting 85% of the data have a residual less than zero when Tau is 0.1.",
    "start": "2255050",
    "end": "2260960"
  },
  {
    "text": "See, even on the training data, we're getting quantiles for the residual that match pretty much what we expected to get.",
    "start": "2260960",
    "end": "2269790"
  },
  {
    "start": "2276000",
    "end": "2347000"
  },
  {
    "text": "Let's summarize. Loss function is often expressed as a penalty function of the residual.",
    "start": "2277270",
    "end": "2283865"
  },
  {
    "text": "Residual is r equals y-hat minus y. And the loss function expresses how much we object to different values of the residual.",
    "start": "2283865",
    "end": "2294170"
  },
  {
    "text": "Different choices of loss function give us different ERM predictors. And in particular, there are two types that are very important.",
    "start": "2294170",
    "end": "2302299"
  },
  {
    "text": "One is robust fitting. When you're fitting data with outliers, there's a penalty function which increases more slowly than the square penalty function,",
    "start": "2302300",
    "end": "2312695"
  },
  {
    "text": "and that gives you robustness to the outliers. If we're interested in producing predictors that prefer to",
    "start": "2312695",
    "end": "2321860"
  },
  {
    "text": "overestimate or underestimate the data,",
    "start": "2321860",
    "end": "2327245"
  },
  {
    "text": "then you can do quantile rejection- regression. And in quantile regression,",
    "start": "2327245",
    "end": "2332569"
  },
  {
    "text": "you fit data which gives an- in such a way that you get a specific fraction of over-estimates.",
    "start": "2332569",
    "end": "2339260"
  },
  {
    "text": "And you can choose that fraction by choosing Tau.",
    "start": "2339260",
    "end": "2342630"
  }
]