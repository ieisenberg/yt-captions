[
  {
    "start": "0",
    "end": "81000"
  },
  {
    "text": "Hello and welcome to the section on Boolean classification.",
    "start": "5930",
    "end": "11590"
  },
  {
    "text": "So we've already seen the core idea. We have a target variable which is categorical,",
    "start": "13520",
    "end": "22994"
  },
  {
    "text": "which we embed as representatives in Euclidean space,",
    "start": "22995",
    "end": "30360"
  },
  {
    "text": "and in the Boolean classification case, we will embed them in a one-dimensional Euclidean space,",
    "start": "30360",
    "end": "36150"
  },
  {
    "text": "the reals, as plus or minus 1, so true might embed to plus 1 and false to minus 1.",
    "start": "36150",
    "end": "43260"
  },
  {
    "text": "And then we use regularized empirical risk minimization to fit",
    "start": "43280",
    "end": "48350"
  },
  {
    "text": "with various loss functions and regularizers.",
    "start": "48350",
    "end": "53519"
  },
  {
    "text": "And then we use the Neyman-Pearson metric without a choice",
    "start": "53520",
    "end": "59000"
  },
  {
    "text": "of kappa to express our preference for false negatives and false positives,",
    "start": "59000",
    "end": "65045"
  },
  {
    "text": "and we can validate using that as our performance metric, and of course when kappa is 1 this reduces to",
    "start": "65045",
    "end": "73820"
  },
  {
    "text": "the sum of the false-negative rate and the false positive rate, which is the error rate.",
    "start": "73820",
    "end": "79470"
  },
  {
    "start": "81000",
    "end": "407000"
  },
  {
    "text": "Now I want to look now in more detail at possible loss functions that one uses for Boolean classification.",
    "start": "83880",
    "end": "92005"
  },
  {
    "text": "Now, here y can only take values minus 1 or 1.",
    "start": "92005",
    "end": "98020"
  },
  {
    "text": "And so really there are only two scalar loss functions.",
    "start": "98020",
    "end": "105585"
  },
  {
    "text": "So normally, when we think about a loss function, we have a function which is a function of y and y hat,",
    "start": "105585",
    "end": "115975"
  },
  {
    "text": "and the loss function is going to be small or 0 and y is equal to y hat,",
    "start": "115975",
    "end": "123600"
  },
  {
    "text": "so along this line, and it's going to be increasing as we move away from that line.",
    "start": "123600",
    "end": "134800"
  },
  {
    "text": "But in our case, y only takes two values,",
    "start": "134800",
    "end": "139965"
  },
  {
    "text": "and so there's y equal to 1 and y equal to minus 1.",
    "start": "139965",
    "end": "146250"
  },
  {
    "text": "So I've got a function which in theory is a function of the entire plane,",
    "start": "146250",
    "end": "151700"
  },
  {
    "text": "the loss function l of y, y hat, and y. But really it just takes two possible values for y,",
    "start": "151700",
    "end": "160230"
  },
  {
    "text": "and so I really only evaluate the function along this line and along that line.",
    "start": "160230",
    "end": "167855"
  },
  {
    "text": "And along that line the function is l of y hat 1,",
    "start": "167855",
    "end": "173885"
  },
  {
    "text": "and along this line, it's l of y hat minus 1.",
    "start": "173885",
    "end": "179580"
  },
  {
    "text": "Now we might call them something else. We might call this equal to l_a of y hat,",
    "start": "179900",
    "end": "191160"
  },
  {
    "text": "and this l_b of y hat just to be explicit there.",
    "start": "191160",
    "end": "196170"
  },
  {
    "text": "Even though we write them with two arguments, they really are both functions on one-dimensional space,",
    "start": "196170",
    "end": "204845"
  },
  {
    "text": "the space of y hat. And when we look at what this function l_a or l y hat is,",
    "start": "204845",
    "end": "212175"
  },
  {
    "text": "it tells us how much y hat irritates us when y is 1,",
    "start": "212175",
    "end": "217425"
  },
  {
    "text": "and the other one l-b tells us how much y hat irritates us when y is minus 1, and we would look for this function here to be",
    "start": "217425",
    "end": "227135"
  },
  {
    "text": "small at y hat is 1, and increasing away.",
    "start": "227135",
    "end": "236500"
  },
  {
    "text": "Similarly this function should be small at y hat is minus 1 and increasing away,",
    "start": "236500",
    "end": "244410"
  },
  {
    "text": "and that sounds reasonable based on our intuition for loss functions in two variables.",
    "start": "245510",
    "end": "254534"
  },
  {
    "text": "But actually that's not quite what we want for Boolean classification, and the reason is,",
    "start": "254535",
    "end": "261829"
  },
  {
    "text": "is that if y is 1,",
    "start": "261830",
    "end": "267000"
  },
  {
    "text": "then we would like the y hat that un-embeds to give us 1 also.",
    "start": "267000",
    "end": "276850"
  },
  {
    "text": "And that means that any y hat which is positive should give us a though loss.",
    "start": "277160",
    "end": "288560"
  },
  {
    "text": "Similarly, when y is minus 1, we'd like a y hat that unembeds to minus 1.",
    "start": "288560",
    "end": "295895"
  },
  {
    "text": "And so any y hat that's negative should give us a small loss,",
    "start": "295895",
    "end": "301425"
  },
  {
    "text": "and so what we really want is we want this l_a function over here, we want it to be small when y hat is positive,",
    "start": "301425",
    "end": "311170"
  },
  {
    "text": "we want it to be small here and growing in this direction,",
    "start": "311170",
    "end": "317505"
  },
  {
    "text": "and we would let this function to be small here and growing in that direction.",
    "start": "317505",
    "end": "325380"
  },
  {
    "text": "And so we've got to look at two functions that behave in the opposite way, l_y hat 1,",
    "start": "328910",
    "end": "336845"
  },
  {
    "text": "we'd like to be large when y hat is negative and small and when y hat is positive.",
    "start": "336845",
    "end": "346424"
  },
  {
    "text": "L of y hat minus 1, we'd like to be small when y hat is negative and large when y hat is positive.",
    "start": "346424",
    "end": "355620"
  },
  {
    "text": "Now the way we're gonna get this is we're going to define a penalty function p and we'll have l of y hat minus 1 be just p of y hat,",
    "start": "357280",
    "end": "367090"
  },
  {
    "text": "and we'll have l of y hat 1, be p of minus y hat.",
    "start": "367090",
    "end": "373350"
  },
  {
    "text": "Now in order to get the property that false positives and false negatives irritate us different amounts,",
    "start": "373350",
    "end": "384585"
  },
  {
    "text": "we're gonna scale one of those. So in fact, we'll have l of y hat 1 be p of y hat,",
    "start": "384585",
    "end": "393180"
  },
  {
    "text": "and l of y hat 1 be kappa times p of minus y-hat.",
    "start": "393180",
    "end": "400300"
  },
  {
    "start": "407000",
    "end": "592000"
  },
  {
    "text": "Let's look at favorite loss to date, the Square loss function.",
    "start": "408770",
    "end": "415470"
  },
  {
    "text": "Now if we had a loss function,",
    "start": "416030",
    "end": "421925"
  },
  {
    "text": "which was 1 whenever",
    "start": "421925",
    "end": "427319"
  },
  {
    "text": "we chose the wrong answer and 0 whenever we chose the right answer,",
    "start": "427320",
    "end": "433710"
  },
  {
    "text": "well, that would be this loss function right here,",
    "start": "433710",
    "end": "439750"
  },
  {
    "text": "and the square loss function is this purple blue curve here.",
    "start": "441170",
    "end": "449530"
  },
  {
    "text": "That's for the case when y is minus 1. Here, I've scaled on the right-hand side everything by kappa.",
    "start": "452090",
    "end": "460270"
  },
  {
    "text": "So here kappa is 2, and so I've got a loss function which is 2,",
    "start": "460270",
    "end": "467440"
  },
  {
    "text": "when I've got an ideal loss function which is 2,",
    "start": "467440",
    "end": "473715"
  },
  {
    "text": "when y hat is negative and 0 when y hat is positive, and I've got the square loss function here.",
    "start": "473715",
    "end": "487210"
  },
  {
    "text": "And what it does satisfy some properties that we like. In particular, when y ha- when y is minus 1,",
    "start": "488810",
    "end": "498675"
  },
  {
    "text": "the loss is large when y hat is positive, over here.",
    "start": "498675",
    "end": "505960"
  },
  {
    "text": "Use a different color there, and similarly,",
    "start": "509330",
    "end": "520019"
  },
  {
    "text": "when y is 1, the loss is large when y hat is negative,",
    "start": "520020",
    "end": "527405"
  },
  {
    "text": "which is what it should be. And then we have the part where the square loss lets us down.",
    "start": "527405",
    "end": "535940"
  },
  {
    "text": "The square loss also becomes large over here.",
    "start": "535940",
    "end": "541040"
  },
  {
    "text": "where it should in fact be small, where we'd like a loss to be small because any y hat,",
    "start": "541040",
    "end": "550435"
  },
  {
    "text": "which is negative, should- and that's a small loss when the true y is minus 1.",
    "start": "550435",
    "end": "559260"
  },
  {
    "text": "Because any y hat that's negative, it's gonna unembed to a y hat value of minus 1,",
    "start": "559260",
    "end": "568255"
  },
  {
    "text": "which is exactly why, and the same over here when y is 1,",
    "start": "568255",
    "end": "575384"
  },
  {
    "text": "the square loss grows again and that's not the property that we want.",
    "start": "575385",
    "end": "582670"
  },
  {
    "text": "On the plus side, of course, the resulting ERM problem is a least squares problem and so it's easy to solve.",
    "start": "582860",
    "end": "589770"
  },
  {
    "start": "592000",
    "end": "701000"
  },
  {
    "text": "Now, there's another loss function that we might consider, and that's the Neyman-Pearson loss.",
    "start": "595080",
    "end": "600760"
  },
  {
    "text": "And here it is. This is our ideal loss function. So this is the loss function which",
    "start": "600760",
    "end": "607120"
  },
  {
    "text": "is 1 when y hat is going to embed to the wrong value,",
    "start": "607120",
    "end": "615550"
  },
  {
    "text": "and 0 when y hat is going to embed it to the right value.",
    "start": "615550",
    "end": "620740"
  },
  {
    "text": "And we scale that by Kappa when y is 1.",
    "start": "620740",
    "end": "625760"
  },
  {
    "text": "And as a result, the Neyman-Pearson loss is the loss such that",
    "start": "625830",
    "end": "634030"
  },
  {
    "text": "when we evaluate the empirical risk,",
    "start": "634030",
    "end": "640300"
  },
  {
    "text": "well, what's gonna happen? We're gonna score one point for every time y is minus 1 and y hat is 1,",
    "start": "640300",
    "end": "652584"
  },
  {
    "text": "and we're gonna score Kappa points every time y is 1 and y hat is minus 1.",
    "start": "652585",
    "end": "660385"
  },
  {
    "text": "And so it will equal to Kappa times the false negative rate plus the false positive rate,",
    "start": "660385",
    "end": "669205"
  },
  {
    "text": "which is exactly the Neyman-Pearson performance metric that we defined before.",
    "start": "669205",
    "end": "675215"
  },
  {
    "text": "And so this is the ideal loss function for any particular Kappa.",
    "start": "675215",
    "end": "682645"
  },
  {
    "text": "Use this as a loss function and do empirical risk minimization with this- with this loss function,",
    "start": "682645",
    "end": "692500"
  },
  {
    "text": "and the empirical risk will be exactly our desired performance metric.",
    "start": "692500",
    "end": "700100"
  },
  {
    "start": "701000",
    "end": "805000"
  },
  {
    "text": "That's a good thing, but the unfortunate thing is that it's actually very",
    "start": "702600",
    "end": "707800"
  },
  {
    "text": "hard to minimize empirical risk with the Neyman-Pearson loss. And the reason for that is twofold.",
    "start": "707800",
    "end": "715345"
  },
  {
    "text": "One is that it has discontinuities at the origin,",
    "start": "715345",
    "end": "725350"
  },
  {
    "text": "and those discontinuities make many algorithms,",
    "start": "725350",
    "end": "731800"
  },
  {
    "text": "uh, for solving empirical risk minimization not work well.",
    "start": "731800",
    "end": "737450"
  },
  {
    "text": "The other is that the derivative is 0 everywhere else.",
    "start": "737610",
    "end": "746329"
  },
  {
    "text": "And that's one of the features that makes empirical risk minimization algorithms not work well.",
    "start": "747510",
    "end": "752959"
  },
  {
    "text": "Now, if the derivative is 0, then if we've got a particular predictor and we change its parameters slightly,",
    "start": "752960",
    "end": "762719"
  },
  {
    "text": "then what's going to happen is that the value of the empirical risk is probably not gonna change,",
    "start": "762719",
    "end": "771265"
  },
  {
    "text": "because we're gonna have, uh, we're gonna move our resulting y hats,",
    "start": "771265",
    "end": "776695"
  },
  {
    "text": "say from here to here, and in both cases we are gonna have exactly the same empirical risk.",
    "start": "776695",
    "end": "786025"
  },
  {
    "text": "And so it's hard to have an algorithm which is based upon small changes to Theta and searching over small changes with Theta.",
    "start": "786025",
    "end": "796720"
  },
  {
    "text": "Hard- it's hard to have such an algorithm work when one has a loss function which has zero derivatives everywhere.",
    "start": "796720",
    "end": "802645"
  },
  {
    "text": "Um, so instead we",
    "start": "802645",
    "end": "811000"
  },
  {
    "start": "805000",
    "end": "1101000"
  },
  {
    "text": "don't use the- the Neyman-Pearson loss. And in fact, we get better performance if we use different loss functions.",
    "start": "811000",
    "end": "818095"
  },
  {
    "text": "And those different loss functions not only give us better performance but they're actually also easier to minimize.",
    "start": "818095",
    "end": "824305"
  },
  {
    "text": "If the loss functions are convex, well, we've already discussed how convex optimization problems are simpler to",
    "start": "824305",
    "end": "831760"
  },
  {
    "text": "solve than problems where the objective function isn't convex.",
    "start": "831760",
    "end": "838059"
  },
  {
    "text": "Um, of course, we need to regularize it to be convex too. Well, then there's a very efficient solution.",
    "start": "838059",
    "end": "844075"
  },
  {
    "text": "Uh, but even if the, uh, loss function isn't convex, it's still very useful in practice and very commonly used.",
    "start": "844075",
    "end": "852410"
  },
  {
    "text": "So here's a loss. This is a function called the sigmoid function.",
    "start": "854220",
    "end": "860274"
  },
  {
    "text": "This is 1 on 1 plus e to the minus y hat. That's, uh, this function right here.",
    "start": "860275",
    "end": "869709"
  },
  {
    "text": "And this function is of course the flip of it. So we've replaced y hat by minus y hat and scaled it by Kappa.",
    "start": "869710",
    "end": "878680"
  },
  {
    "text": "And you can see that this is a nice smooth approximation of the Neyman-Pearson loss.",
    "start": "878680",
    "end": "886800"
  },
  {
    "text": "We've smoothed it out and that's dealt with both of our problems. It's removed the discontinuity,",
    "start": "886800",
    "end": "892949"
  },
  {
    "text": "and it's removed the 0 derivative problem.",
    "start": "892950",
    "end": "900775"
  },
  {
    "text": "So this is a nice differentiable approximation of the Neyman-Pearson loss. It's not convex, but nonetheless, it can work well.",
    "start": "900775",
    "end": "910070"
  },
  {
    "text": "But here is one of the most commonly used loss functions for Boolean classification.",
    "start": "912210",
    "end": "918759"
  },
  {
    "text": "This is the logistic loss function. This is the log of 1 plus e_y hat for the case when y is minus 1.",
    "start": "918759",
    "end": "931315"
  },
  {
    "text": "And on the case when y is 1, we flip it and we scale by Kappa.",
    "start": "931315",
    "end": "935870"
  },
  {
    "text": "And you can see that, well, it's not the perfect approximation to Neyman-Pearson,",
    "start": "938400",
    "end": "945280"
  },
  {
    "text": "but it does grow in the right direction. It's small on the left-hand side, and it grows as we move to the right.",
    "start": "945280",
    "end": "952705"
  },
  {
    "text": "And, uh, the thing it doesn't do is it doesn't flatten out. All right? Instead, it keeps on growing.",
    "start": "952705",
    "end": "960310"
  },
  {
    "text": "Whereas there's no reason in terms of a performance metric to penalize",
    "start": "960310",
    "end": "966925"
  },
  {
    "text": "a y hat which is very large over one that's not very large.",
    "start": "966925",
    "end": "976360"
  },
  {
    "text": "As long as it's greater than 0, it would unembed to 1 no matter what.",
    "start": "976360",
    "end": "982310"
  },
  {
    "text": "Um, but this is convex as a result of it not, um, flattening out, and so it's both differentiable and convex.",
    "start": "983490",
    "end": "994075"
  },
  {
    "text": "Um, and this is very commonly used and works very well in practice.",
    "start": "994075",
    "end": "999740"
  },
  {
    "text": "The other very commonly used loss function for Boolean classification is the hinge loss.",
    "start": "1001910",
    "end": "1008730"
  },
  {
    "text": "And this is the positive part of 1 plus y hat when y is minus 1.",
    "start": "1008730",
    "end": "1015180"
  },
  {
    "text": "And again, the flip and scale when y is minus 1.",
    "start": "1015180",
    "end": "1020640"
  },
  {
    "text": "And so it's a function which is 0 when y hat is less than minus 1, and then increases linearly. It is convex.",
    "start": "1020640",
    "end": "1029714"
  },
  {
    "text": "It's not- it's differentiable almost everywhere, except at this one point right here,",
    "start": "1029715",
    "end": "1034839"
  },
  {
    "text": "um, and it is very, very commonly used as well.",
    "start": "1034850",
    "end": "1040800"
  },
  {
    "text": "This is the other most common choice for Boolean classification.",
    "start": "1040800",
    "end": "1047410"
  },
  {
    "text": "And we might do, um, we might make up our own loss function. So here's one.",
    "start": "1049400",
    "end": "1056595"
  },
  {
    "text": "Here's one we made up, which we've called the Hubristic loss, which is a cross between the Huber and the logistic.",
    "start": "1056595",
    "end": "1064035"
  },
  {
    "text": "So it behaves, uh, uh, it is constant in 0 when y hat is less than minus 1,",
    "start": "1064035",
    "end": "1074775"
  },
  {
    "text": "it grows like a quadratic here to, uh, approximate that, uh,",
    "start": "1074775",
    "end": "1082065"
  },
  {
    "text": "change in value from 0-1, and then it continues growing linearly at the same rate in order",
    "start": "1082065",
    "end": "1089520"
  },
  {
    "text": "to have a smooth derivative at the origin.",
    "start": "1089520",
    "end": "1095050"
  },
  {
    "start": "1101000",
    "end": "1253000"
  },
  {
    "text": "Now, several of these classifiers are- are extremely popular, um,",
    "start": "1103280",
    "end": "1108660"
  },
  {
    "text": "if we're using the square loss and a square regularizer that will be called the least squares classifier.",
    "start": "1108660",
    "end": "1114540"
  },
  {
    "text": "If we use the logistic loss and any regularizer, then that's called logistic regression.",
    "start": "1114540",
    "end": "1121140"
  },
  {
    "text": "And so that would be called, for example, logistic regression with an L_1 regularizer.",
    "start": "1121140",
    "end": "1127000"
  },
  {
    "text": "Um, if we use the hinge loss with the square regularizer,",
    "start": "1127220",
    "end": "1132240"
  },
  {
    "text": "that has a rather bizarre name of the support vector machine.",
    "start": "1132240",
    "end": "1137325"
  },
  {
    "text": "And what that means is it's regularized empirical risk minimization with that particular loss function and that particular regularizer.",
    "start": "1137325",
    "end": "1145780"
  },
  {
    "text": "All right. Here's an example. Uh, on the left here we have the, uh,",
    "start": "1147230",
    "end": "1155460"
  },
  {
    "text": "a classifier that's generated using the logistic loss.",
    "start": "1155460",
    "end": "1161325"
  },
  {
    "text": "There's no test set here, and there's no regularization here. We've got a lot of data points and there's no real benefit in either of those.",
    "start": "1161325",
    "end": "1172304"
  },
  {
    "text": "Um, and this is just, of course, a toy example. And you can see the lo- logistic loss, um, gives us,",
    "start": "1172305",
    "end": "1181529"
  },
  {
    "text": "uh, an error of 16% on this particular data set. On the right here we see exactly the same data set,",
    "start": "1181530",
    "end": "1190170"
  },
  {
    "text": "only the classifier has been computed with the squared loss, and we can see very similar performance.",
    "start": "1190170",
    "end": "1198044"
  },
  {
    "text": "Uh, in fact, the only difference is that this has slightly smaller error rate,",
    "start": "1198045",
    "end": "1206905"
  },
  {
    "text": "15% as opposed to 16%. Um, but that's in no way a-",
    "start": "1206905",
    "end": "1213430"
  },
  {
    "text": "a general conclusion, just a particular feature of this data set.",
    "start": "1213430",
    "end": "1218995"
  },
  {
    "text": "Um, and in general, one expects quite similar performance",
    "start": "1218995",
    "end": "1227305"
  },
  {
    "text": "between the different losses that one might use for Boolean classification.",
    "start": "1227305",
    "end": "1232440"
  },
  {
    "text": "Um, uh, sometimes the logistic loss or the hinge loss do better than squared loss,",
    "start": "1232440",
    "end": "1242625"
  },
  {
    "text": "um, but as usual, the only real way to determine that is through validation.",
    "start": "1242625",
    "end": "1249860"
  },
  {
    "text": "Uh, this is the support vector machine. This is the hinge loss combined with the square regularizer.",
    "start": "1252830",
    "end": "1262559"
  },
  {
    "start": "1253000",
    "end": "1616000"
  },
  {
    "text": "And it has a- a peculiar feature that is worth observing.",
    "start": "1262560",
    "end": "1269565"
  },
  {
    "text": "Um, and it only happens in this somewhat artificial case. And so it never really happens in practice.",
    "start": "1269565",
    "end": "1276315"
  },
  {
    "text": "But it, it is worth at least discussing here. And that is that if you have two, a dataset with two classes are perfectly separable.",
    "start": "1276315",
    "end": "1289409"
  },
  {
    "text": "So here we have a dataset where all of the blue points are up here,",
    "start": "1289410",
    "end": "1296335"
  },
  {
    "text": "and all of the red points are down here, and so our classifier, our predictor is doing perfectly.",
    "start": "1296335",
    "end": "1304080"
  },
  {
    "text": "It's getting zero training risk.",
    "start": "1304080",
    "end": "1310260"
  },
  {
    "text": "And um, but it has an additional feature, and that is the- the- that's this line, and this line.",
    "start": "1310260",
    "end": "1324790"
  },
  {
    "text": "And those lines, ah, the lines in x space with theta transpose x is plus or minus one.",
    "start": "1326050",
    "end": "1336740"
  },
  {
    "text": "So theta transpose x is 1, maybe here, and theta transpose x is minus 1,",
    "start": "1336740",
    "end": "1343500"
  },
  {
    "text": "maybe here or they may be the other way round, um, depending on which points are 1 and which points are minus 1.",
    "start": "1343500",
    "end": "1352528"
  },
  {
    "text": "Now, why should that be the case? Now, the reason that's the case is as follows.",
    "start": "1352529",
    "end": "1360030"
  },
  {
    "text": "Is that the theta transpose x is one line is perpendicular to the direction the theta points.",
    "start": "1360030",
    "end": "1368340"
  },
  {
    "text": "And so theta here is a vector and it points in this direction, in which case that's theta transpose x is 1 right there.",
    "start": "1368340",
    "end": "1377205"
  },
  {
    "text": "Now, what happens if I make theta larger? Well, that means that theta transpose x is 1,",
    "start": "1377205",
    "end": "1386159"
  },
  {
    "text": "is going to move in a bit. And similarly, theta transpose x is minus 1 is also going to",
    "start": "1386160",
    "end": "1391200"
  },
  {
    "text": "move in a bit towards the origin. And um, now if I evaluate ah,",
    "start": "1391200",
    "end": "1403799"
  },
  {
    "text": "ah the predictor at my data-points, well the predictors ah at the data-points gives me y hat and y hat is theta transpose x.",
    "start": "1403800",
    "end": "1416220"
  },
  {
    "text": "So all of these points have a theta transpose x value,",
    "start": "1416220",
    "end": "1421590"
  },
  {
    "text": "have a y hat value and which is greater than 1 and",
    "start": "1421590",
    "end": "1427650"
  },
  {
    "text": "all of these points have a y hat value,",
    "start": "1427650",
    "end": "1433890"
  },
  {
    "text": "which is less than minus 1. And so, because if I look at this loss function here, right,",
    "start": "1433890",
    "end": "1443265"
  },
  {
    "text": "all of the blue points which have a y hat value of greater than 1, are going to give me 0",
    "start": "1443265",
    "end": "1450539"
  },
  {
    "text": "in my loss, and then if we look at this plot here for the loss,",
    "start": "1450539",
    "end": "1456990"
  },
  {
    "text": "for the red points, all of my red data points have a y hat value of less than minus 1,",
    "start": "1456990",
    "end": "1464715"
  },
  {
    "text": "and so they're also going to give me 0 loss. And so as long as theta is sufficiently large and pointing vaguely in this direction,",
    "start": "1464715",
    "end": "1475030"
  },
  {
    "text": "I will end up with exactly zero empirical risk. Now, what's happening, is that here we've got the square regularizer.",
    "start": "1475520",
    "end": "1487695"
  },
  {
    "text": "And so that's a penalty on the size of theta. And so by minimizing the empirical risk plus lambda times the norm of theta,",
    "start": "1487695",
    "end": "1501735"
  },
  {
    "text": "we're going to try and make theta small. And so the best that can happen is that we make theta sufficiently- well.",
    "start": "1501735",
    "end": "1512340"
  },
  {
    "text": "So depending on the value of lambda, right, for some values of lambda,",
    "start": "1512340",
    "end": "1518100"
  },
  {
    "text": "we will find that the right thing to do is to make theta as small as we can,",
    "start": "1518100",
    "end": "1525294"
  },
  {
    "text": "but still keep the loss, the total loss 0.",
    "start": "1525295",
    "end": "1532529"
  },
  {
    "text": "And that's exactly what's happened here. Is that when we make theta small,",
    "start": "1532530",
    "end": "1538740"
  },
  {
    "text": "the smallest we can make theta, and yet have a loss of 0 is when the theta transpose x is one line has moved up,",
    "start": "1538740",
    "end": "1549390"
  },
  {
    "text": "went up against this- this one data point right there. It's a blue data point.",
    "start": "1549390",
    "end": "1554535"
  },
  {
    "text": "And the corresponding theta transpose x is minus one line has moved up against these data points which are right here on the boundary.",
    "start": "1554535",
    "end": "1563440"
  },
  {
    "text": "And so the theta vector,",
    "start": "1564740",
    "end": "1569830"
  },
  {
    "text": "it is a normal vector to the boundary between the two classes,",
    "start": "1570830",
    "end": "1582225"
  },
  {
    "text": "and its magnitude is equal to 1 over half the margin.",
    "start": "1582225",
    "end": "1591130"
  },
  {
    "text": "So that's the margin half width and the dish- that distance is equal to 1 over the norm of theta.",
    "start": "1591230",
    "end": "1601960"
  },
  {
    "text": "Again, this is a very peculiar special case that you never see in practice because",
    "start": "1602810",
    "end": "1608025"
  },
  {
    "text": "you're- you don't have interesting problems where the class is perfectly separate.",
    "start": "1608025",
    "end": "1615430"
  },
  {
    "text": "Let's look now at a more practical example. Uh, this example is the Australian weather,",
    "start": "1616100",
    "end": "1626340"
  },
  {
    "text": "where what we have is we have data collected at many different weather stations across Australia.",
    "start": "1626340",
    "end": "1635400"
  },
  {
    "text": "And at each of those weather stations, we've collected ah,",
    "start": "1635400",
    "end": "1640000"
  },
  {
    "text": "attributes of the weather, such as temperature, pressure, humidity, wind, and so on.",
    "start": "1640880",
    "end": "1648720"
  },
  {
    "text": "And this data was collected every 10 years from 2007 through 2017,",
    "start": "1648720",
    "end": "1654929"
  },
  {
    "text": "and consists of 142,000 or so records.",
    "start": "1654930",
    "end": "1661290"
  },
  {
    "text": "And the objective we have today is that given this data,",
    "start": "1661290",
    "end": "1667095"
  },
  {
    "text": "predict whether or not it will rain tomorrow. And so included in this data is rainfall.",
    "start": "1667095",
    "end": "1676980"
  },
  {
    "text": "And the target variable is whether or not it rained the next day.",
    "start": "1676980",
    "end": "1682990"
  },
  {
    "text": "Um, now, each, if we look at all of these data records, there turns out there were quite a few ah,",
    "start": "1683390",
    "end": "1690120"
  },
  {
    "text": "ah, missing entries in the data. And so what we've done, is we've gone through each of the records of the data,",
    "start": "1690120",
    "end": "1696179"
  },
  {
    "text": "and any record that has any field missing at all has been removed. And so once one does that,",
    "start": "1696180",
    "end": "1702120"
  },
  {
    "text": "one only ends up with 112,000 or so records, which is still a substantial amount of data.",
    "start": "1702120",
    "end": "1708690"
  },
  {
    "text": "Ah, we will see later in the class how to ah, fill in ah, missing values in- in the dataset.",
    "start": "1708690",
    "end": "1718695"
  },
  {
    "text": "But for now we've just removed them. And this data, of course, we didn't collect it ourselves,",
    "start": "1718695",
    "end": "1724049"
  },
  {
    "text": "this data's from Kaggle, there's a link here and you can go there and get the data yourself and play with it.",
    "start": "1724050",
    "end": "1736870"
  },
  {
    "start": "1740000",
    "end": "1844000"
  },
  {
    "text": "Uh, the- the fields in the data in each record, ah, there's a bunch of numeric fields,",
    "start": "1741110",
    "end": "1747390"
  },
  {
    "text": "those are here listed. Now, the way this data was collected,",
    "start": "1747390",
    "end": "1753915"
  },
  {
    "text": "is that it's daily data. And so at each of the different sites,",
    "start": "1753915",
    "end": "1759930"
  },
  {
    "text": "there are 44 different sites, there's a record collected each day,",
    "start": "1759930",
    "end": "1765510"
  },
  {
    "text": "and that measures attributes of the day's weather at that site. So for example the minimum temperature",
    "start": "1765510",
    "end": "1772080"
  },
  {
    "text": "and the maximum temperature or the minimum temperature, the maximum temperature over that day and the day starts at 9:00 AM and ends at 9:00 AM.",
    "start": "1772080",
    "end": "1779115"
  },
  {
    "text": "Similarly, we have rainfall, which is total rainfall over the day, the wind gust speed,",
    "start": "1779115",
    "end": "1785264"
  },
  {
    "text": "the wind speed at 9:00 AM, the wind speed at 3:00 PM. The humidity at 9:00 AM and the humidity at 3:00 PM,",
    "start": "1785265",
    "end": "1791174"
  },
  {
    "text": "the pressure at 9:00 AM and the pressure at 3:00 PM, the temperature at 9:00 AM and the temperature at 3:00 PM.",
    "start": "1791175",
    "end": "1797275"
  },
  {
    "text": "These are all numeric fields because they're all in different units and we will embed them and standardize them.",
    "start": "1797275",
    "end": "1804840"
  },
  {
    "text": "We also have categorical fields, we have the location.",
    "start": "1804890",
    "end": "1810015"
  },
  {
    "text": "As I said, there were 44 possible locations ah. Each of the wind speeds has an associated wind direction and",
    "start": "1810015",
    "end": "1819210"
  },
  {
    "text": "those 16 compass points, things like east, southeast, um and we",
    "start": "1819210",
    "end": "1829170"
  },
  {
    "text": "also have a field which is whether or not it rained today at that location, which is either yes or no.",
    "start": "1829170",
    "end": "1835005"
  },
  {
    "text": "And then there's one more final field in the data and that is the ah, date on which the record was taken.",
    "start": "1835005",
    "end": "1843820"
  },
  {
    "start": "1844000",
    "end": "2000000"
  },
  {
    "text": "So let's take a look at some of this data. Because there are over 100,000 records,",
    "start": "1844010",
    "end": "1849540"
  },
  {
    "text": "we're only looking at a small 2% random sample of the data just to get a feel for it,",
    "start": "1849540",
    "end": "1854910"
  },
  {
    "text": "and just looking at a couple of the features here. So here we have the minimum temperature plotted on",
    "start": "1854910",
    "end": "1860760"
  },
  {
    "text": "this axis and the maximum temperature plotted on this axis. And we've plotted in blue those points for which there was rain",
    "start": "1860760",
    "end": "1870150"
  },
  {
    "text": "the next day and red those points in which there was no rain the next day.",
    "start": "1870150",
    "end": "1876000"
  },
  {
    "text": "And you can see that there is some indication here, um over here, that's,",
    "start": "1876000",
    "end": "1883095"
  },
  {
    "text": "ah, over here, we're getting much more chance of rain the next day than over here.",
    "start": "1883095",
    "end": "1891825"
  },
  {
    "text": "And and so the data does indicate that although by no means is there a clear separation.",
    "start": "1891825",
    "end": "1898799"
  },
  {
    "text": "Over here we look to the 3:00 PM temperature and the 3:00 PM pressure.",
    "start": "1898800",
    "end": "1904904"
  },
  {
    "text": "And we can see again that when both of those are small, there's a chance of rain tomorrow,",
    "start": "1904904",
    "end": "1911445"
  },
  {
    "text": "when both of those are large, there's much less chance of rain tomorrow.",
    "start": "1911445",
    "end": "1916390"
  },
  {
    "text": "The way we embedded the data, well, we have 12 numerical fields and those are just embedded as they are by the identity map.",
    "start": "1921680",
    "end": "1930404"
  },
  {
    "text": "Each one of those wind speeds is associated with the wind direction, uh, which we embed as one-hot.",
    "start": "1930405",
    "end": "1938230"
  },
  {
    "text": "Whether there's rain today, that's simply a Boolean categorical, so we embedded it as minus 101.",
    "start": "1938540",
    "end": "1944475"
  },
  {
    "text": "Uh, it turns out that the date and the location fields actually did not improve the,",
    "start": "1944475",
    "end": "1950490"
  },
  {
    "text": "uh, performance after validation, and so we simply removed those.",
    "start": "1950490",
    "end": "1955810"
  },
  {
    "text": "We standardize, we add the constant feature. Uh, how does this work out?",
    "start": "1956240",
    "end": "1961350"
  },
  {
    "text": "Well, there are 12 numerical fields, there are 3 times 16, uh,",
    "start": "1961350",
    "end": "1967200"
  },
  {
    "text": "one-hots and so that's, ah, 48 plus 12 that's 60.",
    "start": "1967200",
    "end": "1974025"
  },
  {
    "text": "And then there's rain today, so that's 61, and then there's the constant feature and that gives us 62 dimensional x.",
    "start": "1974025",
    "end": "1983260"
  },
  {
    "text": "The target variable is y, which is simply a Boolean categorical whether it rains tomorrow or not,",
    "start": "1983630",
    "end": "1990870"
  },
  {
    "text": "and that's, ah- ah, I simply embedded it as minus 101.",
    "start": "1990870",
    "end": "1997059"
  },
  {
    "start": "2000000",
    "end": "2090000"
  },
  {
    "text": "Now here we're gonna use the logistic loss, which we've seen before, uh, when y is minus one,",
    "start": "2000460",
    "end": "2007280"
  },
  {
    "text": "the loss of y hat is log 1 plus e to the y hat, and similarly, when y is 1,",
    "start": "2007280",
    "end": "2014030"
  },
  {
    "text": "it's Kappa times log 1 plus e to the minus y hat. And we use a linear predictor and we square regularization.",
    "start": "2014030",
    "end": "2023730"
  },
  {
    "text": "Now, we randomly split into an 80/20 training and test sets.",
    "start": "2025930",
    "end": "2031505"
  },
  {
    "text": "It turns out when you do this that, um, ah,",
    "start": "2031505",
    "end": "2036330"
  },
  {
    "text": "the training error and the test error are",
    "start": "2037270",
    "end": "2042380"
  },
  {
    "text": "to the training empirical risk and the test empirical risk are extremely close,",
    "start": "2042380",
    "end": "2048679"
  },
  {
    "text": "and as a result, one only needs a very small amount to a regularization.",
    "start": "2048680",
    "end": "2054050"
  },
  {
    "text": "And there's no point sweeping over regularization because there's no dip in,",
    "start": "2054050",
    "end": "2061399"
  },
  {
    "text": "ah, in the test loss to be gained. And so as it does we just fixed the regularization amount to a very small fixed amount,",
    "start": "2061400",
    "end": "2071934"
  },
  {
    "text": "something like 10 to the minus 5 for Lambda. And so here we plotted the operating characteristic,",
    "start": "2071935",
    "end": "2078980"
  },
  {
    "text": "the ROC curve, and, um, you can see that there's a red curve and",
    "start": "2078980",
    "end": "2085370"
  },
  {
    "text": "a blue curve and both of them are pretty much on top of each other. The minimum probability of error point on the ROC curve is right about here.",
    "start": "2085370",
    "end": "2096620"
  },
  {
    "start": "2090000",
    "end": "2191000"
  },
  {
    "text": "That's the minimum probability of error classify. And it achieves a false negative rate",
    "start": "2096620",
    "end": "2107405"
  },
  {
    "text": "of about 0.08 and a false positive rate of about 0.08, so a total probability of error of about 16%, uh, which is pretty good.",
    "start": "2107405",
    "end": "2118369"
  },
  {
    "text": "Uh, it's worth comparing also with this point down here.",
    "start": "2118370",
    "end": "2124850"
  },
  {
    "text": "That's a classifier which achieves an error rate of 22%,",
    "start": "2124850",
    "end": "2131555"
  },
  {
    "text": "um, and the way it does that is by always predicting that it will not rain tomorrow.",
    "start": "2131555",
    "end": "2138425"
  },
  {
    "text": "And that's because the data itself contains about 22% days with rainfall and the remaining 78% of days have no rain.",
    "start": "2138425",
    "end": "2151940"
  },
  {
    "text": "Um, and so, uh, this is a very simple predictor, which is doing,",
    "start": "2151940",
    "end": "2158300"
  },
  {
    "text": "uh, reasonably well over the prediction with an error rate of, uh, 22%.",
    "start": "2158300",
    "end": "2164525"
  },
  {
    "text": "Um, of course, this is very trivial, and where that's really the baseline which we have to do better,",
    "start": "2164525",
    "end": "2170105"
  },
  {
    "text": "uh, which we have to improve upon. And so we're achieving 16% which is a significant improvement over 22%.",
    "start": "2170105",
    "end": "2178895"
  },
  {
    "text": "Uh it would be nice to do better than that, um, but with the- the linear predictor that we're using,",
    "start": "2178895",
    "end": "2184310"
  },
  {
    "text": "uh, this seems to be the best achievable.",
    "start": "2184310",
    "end": "2187890"
  },
  {
    "start": "2191000",
    "end": "2447000"
  },
  {
    "text": "Now, if we look at",
    "start": "2192220",
    "end": "2197945"
  },
  {
    "text": "the minimum error predictor and we look at its,",
    "start": "2197945",
    "end": "2204740"
  },
  {
    "text": "uh, parameter vector Theta, and then we can plot here the components of Theta,",
    "start": "2204740",
    "end": "2211690"
  },
  {
    "text": "Theta i against i. Uh, this says absolute value Theta of i is not- of course,",
    "start": "2211690",
    "end": "2218170"
  },
  {
    "text": "the absolute value is just Theta i. Um, and,",
    "start": "2218170",
    "end": "2223180"
  },
  {
    "text": "ah, what we see is some large values and some small values.",
    "start": "2223180",
    "end": "2228800"
  },
  {
    "text": "These large values here don't matter because there are 16 of them, and, uh, those,",
    "start": "2228800",
    "end": "2238100"
  },
  {
    "text": "uh, 16 entries of x are the one-hot embedded values",
    "start": "2238100",
    "end": "2243170"
  },
  {
    "text": "for WindDir at 9:00 A.M. And because those columns,",
    "start": "2243170",
    "end": "2248494"
  },
  {
    "text": "ah, are one-hot embedded, they, uh, sum up to 1, and so this is just equivalent to a constant term in the predictor.",
    "start": "2248495",
    "end": "2259849"
  },
  {
    "text": "And that's just the way the- the training worked out. We could simply subtract off this term and add it to the Theta 1 term,",
    "start": "2259850",
    "end": "2268925"
  },
  {
    "text": "and, uh, it would give us an equivalent predictor.",
    "start": "2268925",
    "end": "2274370"
  },
  {
    "text": "So those are the significant thing. Uh, some interesting things are important,",
    "start": "2274370",
    "end": "2279380"
  },
  {
    "text": "uh, this and this. Uh, this is the pressure at 3:00 P.M and this is the pressure",
    "start": "2279380",
    "end": "2292460"
  },
  {
    "text": "at 9:00 A.M. And so we've got a predictor that",
    "start": "2292460",
    "end": "2298640"
  },
  {
    "text": "depends very strongly on the difference between the pressure at 9:00 A.M and the pressure at 3:00 P.M. Um,",
    "start": "2298640",
    "end": "2306530"
  },
  {
    "text": "and so if the pressure is falling, so at 3:00 P.M pressure is much less than 9:00 A.M pressure,",
    "start": "2306530",
    "end": "2313505"
  },
  {
    "text": "then that is a strong predictor of rainfall. And of course, this is a very well known phenomena in",
    "start": "2313505",
    "end": "2321500"
  },
  {
    "text": "the weather that rapidly falling pressure indicates that the storm is coming. Um, the other features here that are significant are,",
    "start": "2321500",
    "end": "2332540"
  },
  {
    "text": "um, this is, of course, just the constant term, so it's not really a feature. Um, this one right here is the WindGustSpeed,",
    "start": "2332540",
    "end": "2342270"
  },
  {
    "text": "this one here is humidity at 3:00 P.M,",
    "start": "2345280",
    "end": "2350850"
  },
  {
    "text": "and these two are MinTemp and MaxTemp.",
    "start": "2352300",
    "end": "2357890"
  },
  {
    "text": "Since those are the significant features, we can simply say let's retrain our predictor using only those six features,",
    "start": "2357890",
    "end": "2366155"
  },
  {
    "text": "the MinTemp, the MaxTemp, the WindGustSpeed, humidity at 3:00 PM, pressure at 9:00 A.M, and pressure at 3:00 P.M. And if you do that,",
    "start": "2366155",
    "end": "2375095"
  },
  {
    "text": "then you end up with a classifier that achieves a probability of error on the test set of about 16.5%,",
    "start": "2375095",
    "end": "2381965"
  },
  {
    "text": "which is basically the same as the classifier we had with all of these features.",
    "start": "2381965",
    "end": "2388550"
  },
  {
    "text": "And so the rest of these features are irrelevant at predicting rainfall,",
    "start": "2388550",
    "end": "2396109"
  },
  {
    "text": "at least with the current linear predictor. So it's possible that one could do better,",
    "start": "2396109",
    "end": "2402799"
  },
  {
    "text": "maybe by using a more complicated predictor such as a neural network predictor. Maybe by using some more sophisticated feature engineering, combining features, uh,",
    "start": "2402800",
    "end": "2412880"
  },
  {
    "text": "and this is simply, of course, ah, a first foray into designing a predictor to predict rainfall.",
    "start": "2412880",
    "end": "2420420"
  },
  {
    "text": "Okay, so next section, we will talk about classification where there are more than two classes,",
    "start": "2422710",
    "end": "2433489"
  },
  {
    "text": "multi class classification, and we will see the appropriate form of losses to use in that case.",
    "start": "2433489",
    "end": "2441780"
  }
]