[
  {
    "start": "0",
    "end": "4472"
  },
  {
    "text": "CHRIS POTTS: Hello, everyone.",
    "start": "4472",
    "end": "5680"
  },
  {
    "text": "I'm Chris Potts.",
    "start": "5680",
    "end": "6346"
  },
  {
    "text": "Welcome to our unit\non contextual word",
    "start": "6347",
    "end": "8430"
  },
  {
    "text": "representations.",
    "start": "8430",
    "end": "9300"
  },
  {
    "text": "I thought I'd kick this off with\nsome high level guiding ideas.",
    "start": "9300",
    "end": "12653"
  },
  {
    "text": "The first thing I\nwanted to say is",
    "start": "12653",
    "end": "14070"
  },
  {
    "text": "that in previous\niterations of this course,",
    "start": "14070",
    "end": "16049"
  },
  {
    "text": "we've spent about two weeks\nfocused on static vector",
    "start": "16050",
    "end": "19380"
  },
  {
    "text": "representations of words\nand, in some cases, phrases",
    "start": "19380",
    "end": "22590"
  },
  {
    "text": "and full sentences.",
    "start": "22590",
    "end": "23880"
  },
  {
    "text": "For this iteration\nof the course,",
    "start": "23880",
    "end": "25380"
  },
  {
    "text": "we're going to go directly\nto contextual word",
    "start": "25380",
    "end": "27720"
  },
  {
    "text": "representations, which have\nproven so powerful for today's",
    "start": "27720",
    "end": "31380"
  },
  {
    "text": "NLP research and technologies.",
    "start": "31380",
    "end": "33239"
  },
  {
    "text": "But I did want to offer a brief\noverview of the history that",
    "start": "33240",
    "end": "36360"
  },
  {
    "text": "leads to contextual\nrepresentations.",
    "start": "36360",
    "end": "38890"
  },
  {
    "text": "So let's rewind the\nclock back to what",
    "start": "38890",
    "end": "40710"
  },
  {
    "text": "I've called feature-based\nclassical lexical",
    "start": "40710",
    "end": "43829"
  },
  {
    "text": "representations.",
    "start": "43830",
    "end": "44820"
  },
  {
    "text": "And a hallmark of\nthese representations",
    "start": "44820",
    "end": "46860"
  },
  {
    "text": "is that they are sparse.",
    "start": "46860",
    "end": "48360"
  },
  {
    "text": "Typically, what\nwe're thinking of",
    "start": "48360",
    "end": "49770"
  },
  {
    "text": "is very long vectors where the\ncolumn dimensions represent",
    "start": "49770",
    "end": "53730"
  },
  {
    "text": "the output of usually\nhand-built feature functions",
    "start": "53730",
    "end": "56336"
  },
  {
    "text": "that we've written that\nmight capture things",
    "start": "56337",
    "end": "58170"
  },
  {
    "text": "like the binary sentiment\nclassification of a word",
    "start": "58170",
    "end": "61679"
  },
  {
    "text": "or which part of speech it\ntends to have most dominantly",
    "start": "61680",
    "end": "65650"
  },
  {
    "text": "or whether or not it ends in\na suffix like ING for English",
    "start": "65650",
    "end": "69640"
  },
  {
    "text": "and so forth and so on.",
    "start": "69640",
    "end": "71090"
  },
  {
    "text": "So we write a lot of\nthese feature functions.",
    "start": "71090",
    "end": "73009"
  },
  {
    "text": "And as a result, we\nhave vectors that",
    "start": "73010",
    "end": "74860"
  },
  {
    "text": "are mostly zeros\nbecause most words don't",
    "start": "74860",
    "end": "76990"
  },
  {
    "text": "have these properties.",
    "start": "76990",
    "end": "78009"
  },
  {
    "text": "But the few ones carry\nimportant information.",
    "start": "78010",
    "end": "81600"
  },
  {
    "text": "In the next phase, we\nhave count-based methods.",
    "start": "81600",
    "end": "84330"
  },
  {
    "text": "This is the introduction of\nthe distributional hypothesis.",
    "start": "84330",
    "end": "87660"
  },
  {
    "text": "I'm thinking of methods like\nPointwise Mutual Information",
    "start": "87660",
    "end": "90540"
  },
  {
    "text": "and TF-IDF, Term\nFrequency-Inverse Document",
    "start": "90540",
    "end": "93780"
  },
  {
    "text": "Frequency, a classic from\ninformation retrieval",
    "start": "93780",
    "end": "96690"
  },
  {
    "text": "that we'll visit a bit\nlater in the course.",
    "start": "96690",
    "end": "99520"
  },
  {
    "text": "For these methods, we begin\nfrom some kind of count matrix.",
    "start": "99520",
    "end": "102509"
  },
  {
    "text": "So, typically, for PMI, it\nwould be a word-by-word matrix,",
    "start": "102510",
    "end": "105750"
  },
  {
    "text": "where the cells in that matrix\ncapture the number of time",
    "start": "105750",
    "end": "108960"
  },
  {
    "text": "so that each word co-occurs with\nthe other words in our corpus.",
    "start": "108960",
    "end": "112650"
  },
  {
    "text": "For TF-IDF, it would probably\nbe a word-by-document matrix.",
    "start": "112650",
    "end": "116160"
  },
  {
    "text": "And now we're capturing\nwhich words appear",
    "start": "116160",
    "end": "118860"
  },
  {
    "text": "and with what frequency in all\nthe documents in our corpus.",
    "start": "118860",
    "end": "122610"
  },
  {
    "text": "And the idea behind\nthese methods",
    "start": "122610",
    "end": "124470"
  },
  {
    "text": "is that we take PMI or TF-IDF,\nand we massage those counts",
    "start": "124470",
    "end": "128610"
  },
  {
    "text": "in a way that leads to\nbetter representations.",
    "start": "128610",
    "end": "131910"
  },
  {
    "text": "And that's coming purely from\ndistributional information.",
    "start": "131910",
    "end": "134520"
  },
  {
    "text": "We typically don't write\nhand-built feature functions",
    "start": "134520",
    "end": "137130"
  },
  {
    "text": "in this mode, but these vectors\ntend to be pretty sparse.",
    "start": "137130",
    "end": "142070"
  },
  {
    "text": "The next phase is what I've\ncalled classical dimensionality",
    "start": "142070",
    "end": "144770"
  },
  {
    "text": "reduction.",
    "start": "144770",
    "end": "145280"
  },
  {
    "text": "This is the introduction\nof dense representations.",
    "start": "145280",
    "end": "147830"
  },
  {
    "text": "I have in mind methods\nlike PCA, Principal",
    "start": "147830",
    "end": "151010"
  },
  {
    "text": "Components Analysis; SVD, that's\nSingular Value Decomposition;",
    "start": "151010",
    "end": "155060"
  },
  {
    "text": "LDA is Latent\nDirichlet Allocation.",
    "start": "155060",
    "end": "157340"
  },
  {
    "text": "There's a whole family of these.",
    "start": "157340",
    "end": "158989"
  },
  {
    "text": "And, typically, what\nwe're doing at this phase",
    "start": "158990",
    "end": "160940"
  },
  {
    "text": "is taking maybe the\noutput of representations",
    "start": "160940",
    "end": "163520"
  },
  {
    "text": "built in the mode of step 2\nthere and compressing them.",
    "start": "163520",
    "end": "167690"
  },
  {
    "text": "And in compressing\nthem, we typically",
    "start": "167690",
    "end": "169790"
  },
  {
    "text": "get denser, more\ninformative representations",
    "start": "169790",
    "end": "173150"
  },
  {
    "text": "that can also\ncapture higher order",
    "start": "173150",
    "end": "174950"
  },
  {
    "text": "notions of distributional\nsimilarity and co-occurrence",
    "start": "174950",
    "end": "178370"
  },
  {
    "text": "and so forth.",
    "start": "178370",
    "end": "179069"
  },
  {
    "text": "And that proved\nincredibly powerful.",
    "start": "179070",
    "end": "182157"
  },
  {
    "text": "And then the fourth\nphase, which might",
    "start": "182157",
    "end": "183739"
  },
  {
    "text": "be the final phase for this\nstatic vector representation",
    "start": "183740",
    "end": "186800"
  },
  {
    "text": "approach, we have\nwhat I've called",
    "start": "186800",
    "end": "188420"
  },
  {
    "text": "learned dimensionality\nreduction approaches.",
    "start": "188420",
    "end": "190760"
  },
  {
    "text": "Here again, we have\ndense representations.",
    "start": "190760",
    "end": "193250"
  },
  {
    "text": "These could be the\noutput of an autoencoder",
    "start": "193250",
    "end": "195800"
  },
  {
    "text": "or a classical method\nlike word2vec or GloVe.",
    "start": "195800",
    "end": "199100"
  },
  {
    "text": "And what we do at this\nphase is really essentially",
    "start": "199100",
    "end": "202010"
  },
  {
    "text": "combine the count-based\nmethods from step 2",
    "start": "202010",
    "end": "205580"
  },
  {
    "text": "in this history with the\ndimensionality reduction",
    "start": "205580",
    "end": "209060"
  },
  {
    "text": "that we see from methods\nlike SVD and PCA.",
    "start": "209060",
    "end": "212780"
  },
  {
    "text": "And that leads us to very\npowerful typically learned",
    "start": "212780",
    "end": "216440"
  },
  {
    "text": "representations that have\na tremendous capacity",
    "start": "216440",
    "end": "219380"
  },
  {
    "text": "to capture higher order\nnotions of co-occurrence.",
    "start": "219380",
    "end": "223130"
  },
  {
    "text": "That's a very fast overview.",
    "start": "223130",
    "end": "224750"
  },
  {
    "text": "If you would like to get\nhands on with these methods",
    "start": "224750",
    "end": "228170"
  },
  {
    "text": "and really deeply\nunderstand them,",
    "start": "228170",
    "end": "230150"
  },
  {
    "text": "then check out this page\nthat I've linked to here",
    "start": "230150",
    "end": "232819"
  },
  {
    "text": "from the course website.",
    "start": "232820",
    "end": "234200"
  },
  {
    "text": "It links to a lot of notebooks\nand some older videos that",
    "start": "234200",
    "end": "237349"
  },
  {
    "text": "could help you with a\nrefresher on all these methods",
    "start": "237350",
    "end": "240050"
  },
  {
    "text": "or to just get up to\nspeed where you're",
    "start": "240050",
    "end": "242010"
  },
  {
    "text": "ready to think directly about\ncontextual representations.",
    "start": "242010",
    "end": "247569"
  },
  {
    "text": "The representations that we get\nfrom these contextual models",
    "start": "247570",
    "end": "250690"
  },
  {
    "text": "that will be the\nfocus for this unit",
    "start": "250690",
    "end": "252410"
  },
  {
    "text": "and really for the\nentire course really",
    "start": "252410",
    "end": "254800"
  },
  {
    "text": "resonate with me as a linguist.",
    "start": "254800",
    "end": "256660"
  },
  {
    "text": "And so I thought I would\njust pause here and run",
    "start": "256660",
    "end": "259329"
  },
  {
    "text": "through some\nexamples that I think",
    "start": "259329",
    "end": "261040"
  },
  {
    "text": "lead from the linguistics\nangle to the conclusion",
    "start": "261040",
    "end": "263980"
  },
  {
    "text": "that word representations are\nhighly sensitive to the context",
    "start": "263980",
    "end": "267760"
  },
  {
    "text": "in which they're used.",
    "start": "267760",
    "end": "269020"
  },
  {
    "text": "Let's start with a\nsimple case like the vase",
    "start": "269020",
    "end": "271389"
  },
  {
    "text": "broke, and our focus will be\non that English verb \"break.\"",
    "start": "271390",
    "end": "274960"
  },
  {
    "text": "Here, the sense of break\nis something like shatter.",
    "start": "274960",
    "end": "278919"
  },
  {
    "text": "For dawn broke, though, we\nhave a superficially similar",
    "start": "278920",
    "end": "281920"
  },
  {
    "text": "looking sentence, subject-verb.",
    "start": "281920",
    "end": "284140"
  },
  {
    "text": "But now the sense of\nbreak is more like begin.",
    "start": "284140",
    "end": "287890"
  },
  {
    "text": "And that's presumably\nconditioned",
    "start": "287890",
    "end": "289780"
  },
  {
    "text": "by what we know\nabout the subject",
    "start": "289780",
    "end": "291820"
  },
  {
    "text": "and how that operates on the\nsense of the verb \"break\"",
    "start": "291820",
    "end": "294850"
  },
  {
    "text": "for a stereotypical reading.",
    "start": "294850",
    "end": "296770"
  },
  {
    "text": "The news broke-- again,\nsuperficially similar sentence.",
    "start": "296770",
    "end": "299949"
  },
  {
    "text": "The news is the subject.",
    "start": "299950",
    "end": "301430"
  },
  {
    "text": "But now it seems to be\nconditioning a reading that",
    "start": "301430",
    "end": "303759"
  },
  {
    "text": "is more like was announced\nor appeared or was published,",
    "start": "303760",
    "end": "308210"
  },
  {
    "text": "very different sense yet again.",
    "start": "308210",
    "end": "310100"
  },
  {
    "text": "For Sandy broke the record, we\nhave our first transitive case.",
    "start": "310100",
    "end": "313550"
  },
  {
    "text": "And now the sense of\nbreak is something",
    "start": "313550",
    "end": "315590"
  },
  {
    "text": "like surpassed the\nprevious world record.",
    "start": "315590",
    "end": "318510"
  },
  {
    "text": "But for Sandy broke the law,\na different sense yet again.",
    "start": "318510",
    "end": "321710"
  },
  {
    "text": "This is something\nlike a transgression.",
    "start": "321710",
    "end": "323660"
  },
  {
    "text": "Again, conditioned in this\ncase by the direct object.",
    "start": "323660",
    "end": "326810"
  },
  {
    "text": "The burglar broke\ninto the house--",
    "start": "326810",
    "end": "328790"
  },
  {
    "text": "now we have break appearing\nwith a particle \"into.\"",
    "start": "328790",
    "end": "331800"
  },
  {
    "text": "And, in this case, it\nmeans like enter forcibly",
    "start": "331800",
    "end": "334430"
  },
  {
    "text": "without permission.",
    "start": "334430",
    "end": "335630"
  },
  {
    "text": "But the newscaster broke into\nthe movie broadcast means",
    "start": "335630",
    "end": "338450"
  },
  {
    "text": "something more like interrupt,\na related but interestingly",
    "start": "338450",
    "end": "341690"
  },
  {
    "text": "distinct meaning that is\ncoming from the same break",
    "start": "341690",
    "end": "344690"
  },
  {
    "text": "plus particle construction.",
    "start": "344690",
    "end": "347480"
  },
  {
    "text": "And then in we broke even--",
    "start": "347480",
    "end": "349430"
  },
  {
    "text": "just for another surprise,\nthis is an entirely new sense.",
    "start": "349430",
    "end": "352460"
  },
  {
    "text": "Break plus even means something\nlike we lost the same amount",
    "start": "352460",
    "end": "355970"
  },
  {
    "text": "as we gained.",
    "start": "355970",
    "end": "357570"
  },
  {
    "text": "And this is just a sample of\nthe many, many senses that break",
    "start": "357570",
    "end": "362090"
  },
  {
    "text": "can take on in English.",
    "start": "362090",
    "end": "363590"
  },
  {
    "text": "And what we see\nin these patterns",
    "start": "363590",
    "end": "365480"
  },
  {
    "text": "is that the sense it\ndoes have in context",
    "start": "365480",
    "end": "368120"
  },
  {
    "text": "is driven by the\nsurrounding words",
    "start": "368120",
    "end": "370520"
  },
  {
    "text": "transparently but maybe\nalso by things that",
    "start": "370520",
    "end": "372860"
  },
  {
    "text": "are happening in the context.",
    "start": "372860",
    "end": "375439"
  },
  {
    "text": "Similar things arise for\nadjectives like \"flat,\"",
    "start": "375440",
    "end": "379640"
  },
  {
    "text": "as in flat beer, flat note,\nflat tire, flat surface.",
    "start": "379640",
    "end": "384080"
  },
  {
    "text": "That's the same\nadjective \"flat.\"",
    "start": "384080",
    "end": "385879"
  },
  {
    "text": "But depending on what\nnoun it modifies,",
    "start": "385880",
    "end": "388040"
  },
  {
    "text": "you get very different senses.",
    "start": "388040",
    "end": "390020"
  },
  {
    "text": "It can happen with a verb-- to\nthrow a party, throw a fight,",
    "start": "390020",
    "end": "393050"
  },
  {
    "text": "throw a ball, throw a fit.",
    "start": "393050",
    "end": "394849"
  },
  {
    "text": "There might be some\nmetaphorical sense",
    "start": "394850",
    "end": "396770"
  },
  {
    "text": "in which all of these\nsenses are related,",
    "start": "396770",
    "end": "399240"
  },
  {
    "text": "but they are also\nclearly distinct.",
    "start": "399240",
    "end": "401419"
  },
  {
    "text": "And they're being\ndriven by something",
    "start": "401420",
    "end": "403730"
  },
  {
    "text": "about how this direct object\ninteracts with the verb.",
    "start": "403730",
    "end": "407630"
  },
  {
    "text": "And we should extend\nthis beyond just",
    "start": "407630",
    "end": "410000"
  },
  {
    "text": "the simple\nmorphosyntactic context.",
    "start": "410000",
    "end": "412880"
  },
  {
    "text": "Let's think about\nambiguity resolution",
    "start": "412880",
    "end": "415010"
  },
  {
    "text": "coming from the things\nwe say to each other.",
    "start": "415010",
    "end": "417860"
  },
  {
    "text": "For the sentence \"A\ncrane caught a fish,\"",
    "start": "417860",
    "end": "420080"
  },
  {
    "text": "you probably infer that\nthere, the sense of crane",
    "start": "420080",
    "end": "422689"
  },
  {
    "text": "is for a bird.",
    "start": "422690",
    "end": "424190"
  },
  {
    "text": "It's not determined\nby that, but that's",
    "start": "424190",
    "end": "426380"
  },
  {
    "text": "the most likely inference.",
    "start": "426380",
    "end": "427850"
  },
  {
    "text": "Whereas, \"A crane picked\nup the steel beam,\"",
    "start": "427850",
    "end": "431090"
  },
  {
    "text": "you probably infer,\nin that case,",
    "start": "431090",
    "end": "432919"
  },
  {
    "text": "that a crane is a machine.",
    "start": "432920",
    "end": "435170"
  },
  {
    "text": "And then correspondingly\nfor \"I saw",
    "start": "435170",
    "end": "437240"
  },
  {
    "text": "a crane,\" you might not,\nabsent more context,",
    "start": "437240",
    "end": "440330"
  },
  {
    "text": "know whether I mean\na bird or a machine.",
    "start": "440330",
    "end": "443159"
  },
  {
    "text": "But if we do embed this\nsentence in larger context,",
    "start": "443160",
    "end": "446090"
  },
  {
    "text": "you'll begin to make\ninferences about whether it",
    "start": "446090",
    "end": "448820"
  },
  {
    "text": "was a bird or a machine.",
    "start": "448820",
    "end": "450570"
  },
  {
    "text": "And so that shows you\nthat even things that",
    "start": "450570",
    "end": "452570"
  },
  {
    "text": "are happening ambiently in\nthe context of utterance",
    "start": "452570",
    "end": "455060"
  },
  {
    "text": "can impact what sense crane\nhas in these sentences.",
    "start": "455060",
    "end": "461530"
  },
  {
    "text": "\"Are there any typos?",
    "start": "461530",
    "end": "462760"
  },
  {
    "text": "I didn't see any.\"",
    "start": "462760",
    "end": "463780"
  },
  {
    "text": "The second sentence\nis clearly elliptical.",
    "start": "463780",
    "end": "465790"
  },
  {
    "text": "And in this case,\nwe probably infer",
    "start": "465790",
    "end": "467590"
  },
  {
    "text": "that \"I didn't see any\"\nmeans I didn't see any typos.",
    "start": "467590",
    "end": "470740"
  },
  {
    "text": "Contrast that with, \"Are\nthere any bookstores downtown?",
    "start": "470740",
    "end": "473620"
  },
  {
    "text": "I didn't see any.\"",
    "start": "473620",
    "end": "474699"
  },
  {
    "text": "It's an identical\nsecond sentence.",
    "start": "474700",
    "end": "477130"
  },
  {
    "text": "But now, the inference\nis that \"any\" means",
    "start": "477130",
    "end": "479140"
  },
  {
    "text": "something like any bookstores.",
    "start": "479140",
    "end": "481750"
  },
  {
    "text": "And that is again showing that\nthe senses that these words",
    "start": "481750",
    "end": "485080"
  },
  {
    "text": "take on in context\ncould be driven",
    "start": "485080",
    "end": "486849"
  },
  {
    "text": "by everything that is happening\nin the surrounding sentence",
    "start": "486850",
    "end": "490270"
  },
  {
    "text": "and also in the surrounding\ndiscourse on out extending",
    "start": "490270",
    "end": "493810"
  },
  {
    "text": "into things about world\nknowledge, and so forth.",
    "start": "493810",
    "end": "497300"
  },
  {
    "text": "And so that for me really shows\nthat the static word vector",
    "start": "497300",
    "end": "500830"
  },
  {
    "text": "representation approach\nwas never really going",
    "start": "500830",
    "end": "503860"
  },
  {
    "text": "to work out because it\ninsists that \"broke,\"",
    "start": "503860",
    "end": "507310"
  },
  {
    "text": "in all those examples in one,\ncorrespond to a single vector",
    "start": "507310",
    "end": "511389"
  },
  {
    "text": "\"flat\" into a--",
    "start": "511390",
    "end": "513219"
  },
  {
    "text": "has to be a single vector\nand so forth and so on.",
    "start": "513220",
    "end": "516379"
  },
  {
    "text": "And what we actually see\nin how language works is",
    "start": "516380",
    "end": "519370"
  },
  {
    "text": "much more malleability for\nindividual word meanings.",
    "start": "519370",
    "end": "522950"
  },
  {
    "text": "And that's precisely what\ncontextual word representations",
    "start": "522950",
    "end": "526790"
  },
  {
    "text": "allow us to capture.",
    "start": "526790",
    "end": "527975"
  },
  {
    "start": "527975",
    "end": "530500"
  },
  {
    "text": "I thought I would\npause here just",
    "start": "530500",
    "end": "532000"
  },
  {
    "text": "to offer a brief history of\nwhere these ideas come from,",
    "start": "532000",
    "end": "534950"
  },
  {
    "text": "because it's a very\nrecent history.",
    "start": "534950",
    "end": "536930"
  },
  {
    "text": "Things are moving fast, but\nit's also interesting to track.",
    "start": "536930",
    "end": "539649"
  },
  {
    "text": "I think a classic paper.",
    "start": "539650",
    "end": "541180"
  },
  {
    "text": "Maybe the starting point for\nthis is Dai and Lei, 2015.",
    "start": "541180",
    "end": "544990"
  },
  {
    "text": "They really showed the\nvalue of language model",
    "start": "544990",
    "end": "547540"
  },
  {
    "text": "style pretraining\nfor downstream tasks.",
    "start": "547540",
    "end": "549880"
  },
  {
    "text": "And the paper is\nfascinating to look at.",
    "start": "549880",
    "end": "552010"
  },
  {
    "text": "It's a pre-transformer paper.",
    "start": "552010",
    "end": "554170"
  },
  {
    "text": "A lot of the things that they do\nlook complicated in retrospect.",
    "start": "554170",
    "end": "557500"
  },
  {
    "text": "And some of the simpler\nideas that they offer,",
    "start": "557500",
    "end": "559840"
  },
  {
    "text": "we can now see in retrospect,\nare incredibly powerful.",
    "start": "559840",
    "end": "563710"
  },
  {
    "text": "We fast forward a little bit\nto August 2017, McCann et al.",
    "start": "563710",
    "end": "567610"
  },
  {
    "text": "This is the CoVe paper.",
    "start": "567610",
    "end": "569269"
  },
  {
    "text": "And what they showed is that\npretrained bidirectional LSTMs",
    "start": "569270",
    "end": "573280"
  },
  {
    "text": "for machine translation\ncould offer us",
    "start": "573280",
    "end": "576250"
  },
  {
    "text": "sequence representations\nthat were a useful starting",
    "start": "576250",
    "end": "579550"
  },
  {
    "text": "point for many other\ndownstream tasks outside of MT.",
    "start": "579550",
    "end": "584320"
  },
  {
    "text": "And that kind of\nbegins this move",
    "start": "584320",
    "end": "586750"
  },
  {
    "text": "toward pretraining for\ncontextual representations.",
    "start": "586750",
    "end": "589420"
  },
  {
    "text": "It really takes off with\nELMo in February 2018.",
    "start": "589420",
    "end": "593570"
  },
  {
    "text": "They were-- that team was\nreally the first to show",
    "start": "593570",
    "end": "595880"
  },
  {
    "text": "that very large\nscale pretraining",
    "start": "595880",
    "end": "598160"
  },
  {
    "text": "of bidirectional LSTMs could\nlead to rich multipurpose",
    "start": "598160",
    "end": "602060"
  },
  {
    "text": "representations that\nwere easily adapted",
    "start": "602060",
    "end": "604640"
  },
  {
    "text": "to lots of downstream\ntasks via fine tuning.",
    "start": "604640",
    "end": "608840"
  },
  {
    "text": "In June 2018, we get GPT.",
    "start": "608840",
    "end": "611810"
  },
  {
    "text": "And then in October 2018,\nthe BERT era truly begins.",
    "start": "611810",
    "end": "616310"
  },
  {
    "text": "Devlin et al, 2019, is when\nthe paper was published.",
    "start": "616310",
    "end": "619820"
  },
  {
    "text": "But the work\nappeared before that",
    "start": "619820",
    "end": "621560"
  },
  {
    "text": "and had already had\ntremendous influence",
    "start": "621560",
    "end": "623750"
  },
  {
    "text": "by the time it was actually\nofficially published.",
    "start": "623750",
    "end": "626300"
  },
  {
    "text": "That BERT model is really\nthe cornerstone of so much",
    "start": "626300",
    "end": "629690"
  },
  {
    "text": "that we'll discuss in\nthis unit and beyond.",
    "start": "629690",
    "end": "634110"
  },
  {
    "text": "There's a parallel\ndimension to all this,",
    "start": "634110",
    "end": "636600"
  },
  {
    "text": "a kind of parallel\njourney that I",
    "start": "636600",
    "end": "638160"
  },
  {
    "text": "thought I would talk a little\nbit about as another guiding",
    "start": "638160",
    "end": "640990"
  },
  {
    "text": "idea.",
    "start": "640990",
    "end": "641490"
  },
  {
    "text": "I've put this under the\nheading of model structure",
    "start": "641490",
    "end": "644130"
  },
  {
    "text": "and linguistic structure.",
    "start": "644130",
    "end": "645540"
  },
  {
    "text": "And this is related\nalso to this idea",
    "start": "645540",
    "end": "648300"
  },
  {
    "text": "of what kinds of\nstructural biases",
    "start": "648300",
    "end": "651420"
  },
  {
    "text": "we build into our\nmodel architectures.",
    "start": "651420",
    "end": "654149"
  },
  {
    "text": "In the upper left, I\nhave a simple model",
    "start": "654150",
    "end": "656580"
  },
  {
    "text": "that's reviewed in those\nbackground materials, where",
    "start": "656580",
    "end": "658890"
  },
  {
    "text": "you can imagine that each\nword in the sentence \"The rock",
    "start": "658890",
    "end": "662310"
  },
  {
    "text": "rules\" is looked up\nin like a GloVe space",
    "start": "662310",
    "end": "664950"
  },
  {
    "text": "or a word2vec space.",
    "start": "664950",
    "end": "667110"
  },
  {
    "text": "And then what this\nmodel does is simply add",
    "start": "667110",
    "end": "669269"
  },
  {
    "text": "those representations together\nto get a representation",
    "start": "669270",
    "end": "672600"
  },
  {
    "text": "for the entire sentence.",
    "start": "672600",
    "end": "674470"
  },
  {
    "text": "This is a very high bias\nmodel, in particular,",
    "start": "674470",
    "end": "678180"
  },
  {
    "text": "because we have to\ndecide ahead of time",
    "start": "678180",
    "end": "680520"
  },
  {
    "text": "that the way those\nrepresentations will combine",
    "start": "680520",
    "end": "683190"
  },
  {
    "text": "will be via addition.",
    "start": "683190",
    "end": "685350"
  },
  {
    "text": "And you have to think, even if\nthat's approximately correct,",
    "start": "685350",
    "end": "688769"
  },
  {
    "text": "it's only going\nto be approximate.",
    "start": "688770",
    "end": "690300"
  },
  {
    "text": "It would be a kind\nof miner miracle",
    "start": "690300",
    "end": "692310"
  },
  {
    "text": "if addition turned out to\nbe actually the optimal way",
    "start": "692310",
    "end": "696070"
  },
  {
    "text": "to combine those word meanings.",
    "start": "696070",
    "end": "698080"
  },
  {
    "text": "And you could see\nthat as giving rise",
    "start": "698080",
    "end": "699850"
  },
  {
    "text": "to the models that\nare on the right here.",
    "start": "699850",
    "end": "701589"
  },
  {
    "text": "This is a kind of\nrecurrent neural network.",
    "start": "701590",
    "end": "704260"
  },
  {
    "text": "Again, you could imagine that we\nlook up each one of those words",
    "start": "704260",
    "end": "707380"
  },
  {
    "text": "in a static vector\nrepresentation",
    "start": "707380",
    "end": "709870"
  },
  {
    "text": "space like GloVe.",
    "start": "709870",
    "end": "711490"
  },
  {
    "text": "But now we feed them\ninto this RNN process",
    "start": "711490",
    "end": "714640"
  },
  {
    "text": "that actually processes\nthem with a bunch",
    "start": "714640",
    "end": "717610"
  },
  {
    "text": "of new neural\nnetwork parameters.",
    "start": "717610",
    "end": "720339"
  },
  {
    "text": "And in that way, it\ncould be trained.",
    "start": "720340",
    "end": "723410"
  },
  {
    "text": "It could be taught\nto combine those word",
    "start": "723410",
    "end": "725769"
  },
  {
    "text": "meanings in an optimal way.",
    "start": "725770",
    "end": "727780"
  },
  {
    "text": "It could be the case\nthat the optimal way",
    "start": "727780",
    "end": "730330"
  },
  {
    "text": "to combine word meanings\nis with addition.",
    "start": "730330",
    "end": "732910"
  },
  {
    "text": "And the models that we\nhave over here on the right",
    "start": "732910",
    "end": "735310"
  },
  {
    "text": "are certainly powerful enough to\nlearn addition of those vectors",
    "start": "735310",
    "end": "739060"
  },
  {
    "text": "if it's correct.",
    "start": "739060",
    "end": "740180"
  },
  {
    "text": "But that is very\nunlikely to happen.",
    "start": "740180",
    "end": "742120"
  },
  {
    "text": "Probably, the model will learn\na much more complicated function",
    "start": "742120",
    "end": "746050"
  },
  {
    "text": "that might be much more nuanced.",
    "start": "746050",
    "end": "747769"
  },
  {
    "text": "And so in that way, we've\nreleased some of the biases",
    "start": "747770",
    "end": "751450"
  },
  {
    "text": "that we introduced over here.",
    "start": "751450",
    "end": "753130"
  },
  {
    "text": "And we're going\nto allow ourselves",
    "start": "753130",
    "end": "754570"
  },
  {
    "text": "to learn in a more free\nform way from data about how",
    "start": "754570",
    "end": "757900"
  },
  {
    "text": "to optimally combine words.",
    "start": "757900",
    "end": "760180"
  },
  {
    "text": "There's another\ndimension to this.",
    "start": "760180",
    "end": "761620"
  },
  {
    "text": "If you go down on\nthe left here, this",
    "start": "761620",
    "end": "763162"
  },
  {
    "text": "is a tree structured\nneural network.",
    "start": "763162",
    "end": "765200"
  },
  {
    "text": "In this case, we\ndecide ahead of time",
    "start": "765200",
    "end": "766870"
  },
  {
    "text": "that we know the\nconstituent structure.",
    "start": "766870",
    "end": "769240"
  },
  {
    "text": "And then we might have a bunch\nof neural network parameters",
    "start": "769240",
    "end": "771820"
  },
  {
    "text": "that combine the child nodes\nto create representations",
    "start": "771820",
    "end": "775330"
  },
  {
    "text": "for the parents in\na recursive fashion,",
    "start": "775330",
    "end": "777520"
  },
  {
    "text": "as we move up the tree.",
    "start": "777520",
    "end": "779290"
  },
  {
    "text": "That is very\npowerful in the sense",
    "start": "779290",
    "end": "781720"
  },
  {
    "text": "that we could have lots\nof different functions",
    "start": "781720",
    "end": "784300"
  },
  {
    "text": "that get learned\nfor how to combine",
    "start": "784300",
    "end": "786310"
  },
  {
    "text": "child nodes into their parents.",
    "start": "786310",
    "end": "788290"
  },
  {
    "text": "But this model is\nkind of high bias",
    "start": "788290",
    "end": "790449"
  },
  {
    "text": "because it decides\nahead of time about how",
    "start": "790450",
    "end": "793000"
  },
  {
    "text": "the constituent\nstructure should look.",
    "start": "793000",
    "end": "794680"
  },
  {
    "text": "You have to believe\nor no a priori",
    "start": "794680",
    "end": "797920"
  },
  {
    "text": "that The Rock forms a\nconstituent in a way",
    "start": "797920",
    "end": "800139"
  },
  {
    "text": "that Rock rules simply does not.",
    "start": "800140",
    "end": "802840"
  },
  {
    "text": "Whereas, the models up here\ntake a more free form approach.",
    "start": "802840",
    "end": "807430"
  },
  {
    "text": "Down in the right-hand corner,\nI have the kind of models",
    "start": "807430",
    "end": "810700"
  },
  {
    "text": "that we saw in the lead\nup to the transformer.",
    "start": "810700",
    "end": "813740"
  },
  {
    "text": "This could be like\na bidirectional RNN,",
    "start": "813740",
    "end": "816760"
  },
  {
    "text": "where, again, we could look up\nthe words in a static vector",
    "start": "816760",
    "end": "819460"
  },
  {
    "text": "representation space.",
    "start": "819460",
    "end": "820780"
  },
  {
    "text": "But now we have information\nflowing left to right",
    "start": "820780",
    "end": "823420"
  },
  {
    "text": "in these hidden representations.",
    "start": "823420",
    "end": "825160"
  },
  {
    "text": "And we might have added a\nbunch of attention mechanisms",
    "start": "825160",
    "end": "828519"
  },
  {
    "text": "on top that essentially\nconnect every other hidden unit",
    "start": "828520",
    "end": "831910"
  },
  {
    "text": "to every other hidden unit\nin this representation.",
    "start": "831910",
    "end": "835160"
  },
  {
    "text": "So whereas this\nhad a presumption",
    "start": "835160",
    "end": "836709"
  },
  {
    "text": "that we would process\nleft to right,",
    "start": "836710",
    "end": "838580"
  },
  {
    "text": "and this one had an assumption\nthat we would process",
    "start": "838580",
    "end": "840940"
  },
  {
    "text": "by constituent structure,\nthis model down here",
    "start": "840940",
    "end": "843760"
  },
  {
    "text": "says essentially anything goes.",
    "start": "843760",
    "end": "846070"
  },
  {
    "text": "And I think it's fair\nto say that a lesson",
    "start": "846070",
    "end": "848260"
  },
  {
    "text": "of the transformer\nera is that anything",
    "start": "848260",
    "end": "850780"
  },
  {
    "text": "goes given sufficient data\nis the most powerful mode",
    "start": "850780",
    "end": "854560"
  },
  {
    "text": "to be in.",
    "start": "854560",
    "end": "855430"
  },
  {
    "text": "And that really is a kind of\ninsight behind the transformer",
    "start": "855430",
    "end": "859120"
  },
  {
    "text": "architecture.",
    "start": "859120",
    "end": "861760"
  },
  {
    "text": "The attention mechanisms\nthat I mentioned",
    "start": "861760",
    "end": "863950"
  },
  {
    "text": "there are really important.",
    "start": "863950",
    "end": "865272"
  },
  {
    "text": "And this is also part\nof the journey that",
    "start": "865272",
    "end": "866980"
  },
  {
    "text": "leads us to the transformer and\nmight harbor most of its power.",
    "start": "866980",
    "end": "871209"
  },
  {
    "text": "Let me give you a simple\nexample of how attention worked,",
    "start": "871210",
    "end": "875170"
  },
  {
    "text": "especially in the lead\nup to transformers.",
    "start": "875170",
    "end": "877810"
  },
  {
    "text": "Here I have a model that you\nmight think of as an RNN.",
    "start": "877810",
    "end": "880270"
  },
  {
    "text": "Maybe we're processing left to\nright for really not so good.",
    "start": "880270",
    "end": "884110"
  },
  {
    "text": "We look up those words in\nsome kind of static vector",
    "start": "884110",
    "end": "887380"
  },
  {
    "text": "representation space.",
    "start": "887380",
    "end": "888820"
  },
  {
    "text": "And then we have our\nleft to right process",
    "start": "888820",
    "end": "890770"
  },
  {
    "text": "that leads to these\nhidden representations.",
    "start": "890770",
    "end": "893260"
  },
  {
    "text": "Suppose now that I want to\ntrain a classifier on top",
    "start": "893260",
    "end": "896980"
  },
  {
    "text": "of this final output state.",
    "start": "896980",
    "end": "899410"
  },
  {
    "text": "Well, I might worry in\ndoing that that there'll",
    "start": "899410",
    "end": "903040"
  },
  {
    "text": "be a lot of information\nabout the words",
    "start": "903040",
    "end": "904839"
  },
  {
    "text": "that are late in the sequence\nbut not enough information",
    "start": "904840",
    "end": "907750"
  },
  {
    "text": "in this representation\nhere about the words that",
    "start": "907750",
    "end": "910570"
  },
  {
    "text": "were earlier in the sequence.",
    "start": "910570",
    "end": "912530"
  },
  {
    "text": "And so attention\nemerges as a kind of way",
    "start": "912530",
    "end": "914800"
  },
  {
    "text": "to remind ourselves\nin late states what",
    "start": "914800",
    "end": "918399"
  },
  {
    "text": "was in earlier ones.",
    "start": "918400",
    "end": "920050"
  },
  {
    "text": "We could do that with\na scoring function.",
    "start": "920050",
    "end": "922290"
  },
  {
    "text": "And here what I've depicted is\na simple dot product scoring",
    "start": "922290",
    "end": "925130"
  },
  {
    "text": "function, exactly\nthe sort that we",
    "start": "925130",
    "end": "927260"
  },
  {
    "text": "get from the\ntransformer in essence.",
    "start": "927260",
    "end": "929330"
  },
  {
    "text": "And what it's doing is taking\nthe dot product of our target",
    "start": "929330",
    "end": "932180"
  },
  {
    "text": "representation with all of\nthe previous hidden states.",
    "start": "932180",
    "end": "936680"
  },
  {
    "text": "We softmax normalize those.",
    "start": "936680",
    "end": "939110"
  },
  {
    "text": "And then we kind of bring\nthose in to the representation",
    "start": "939110",
    "end": "942769"
  },
  {
    "text": "that we are targeting to\nget a context vector here.",
    "start": "942770",
    "end": "946010"
  },
  {
    "text": "We could take the\naverage of all of them.",
    "start": "946010",
    "end": "948410"
  },
  {
    "text": "Then, finally, we get this\nattention combination h here.",
    "start": "948410",
    "end": "952399"
  },
  {
    "text": "And that could be a kind of\nneural network parameterized",
    "start": "952400",
    "end": "955430"
  },
  {
    "text": "function that takes\nin this representation",
    "start": "955430",
    "end": "958100"
  },
  {
    "text": "plus the attention\nrepresentation",
    "start": "958100",
    "end": "960440"
  },
  {
    "text": "that we created\nhere and feeds that",
    "start": "960440",
    "end": "962720"
  },
  {
    "text": "through some parameters\nand a nonlinearity.",
    "start": "962720",
    "end": "966110"
  },
  {
    "text": "And that finally gives\nus the representation",
    "start": "966110",
    "end": "969079"
  },
  {
    "text": "that we feed into the\nclassifier that we",
    "start": "969080",
    "end": "971150"
  },
  {
    "text": "wanted to fit originally.",
    "start": "971150",
    "end": "973370"
  },
  {
    "text": "And the idea here is that now\nour classification decision",
    "start": "973370",
    "end": "976910"
  },
  {
    "text": "is based indeed on this\nrepresentation at the end,",
    "start": "976910",
    "end": "980540"
  },
  {
    "text": "but now infused with a lot of\ninformation about how similar",
    "start": "980540",
    "end": "984430"
  },
  {
    "text": "that representation is to\nthe ones that preceded it.",
    "start": "984430",
    "end": "987550"
  },
  {
    "text": "And that is the essential idea\nbehind dot product attention,",
    "start": "987550",
    "end": "991480"
  },
  {
    "text": "which will be the beating\nheart, so to speak,",
    "start": "991480",
    "end": "994430"
  },
  {
    "text": "of the transformer.",
    "start": "994430",
    "end": "997250"
  },
  {
    "text": "Another idea that has\nproved so powerful",
    "start": "997250",
    "end": "1000040"
  },
  {
    "text": "is a notion of subword modeling.",
    "start": "1000040",
    "end": "1002027"
  },
  {
    "text": "And I thought I would take\nyou on a brief journey of how",
    "start": "1002027",
    "end": "1004360"
  },
  {
    "text": "we arrived in the current\nphase for this kind",
    "start": "1004360",
    "end": "1007240"
  },
  {
    "text": "of subword modeling beginning\nwith ELMo, because what",
    "start": "1007240",
    "end": "1010600"
  },
  {
    "text": "ELMo did is truly fascinating.",
    "start": "1010600",
    "end": "1013060"
  },
  {
    "text": "The ELMo word\nrepresentation space",
    "start": "1013060",
    "end": "1015730"
  },
  {
    "text": "begins with character\nlevel representations.",
    "start": "1015730",
    "end": "1019029"
  },
  {
    "text": "And then it has a bunch of\nfilters on top of those.",
    "start": "1019030",
    "end": "1022390"
  },
  {
    "text": "And then it has a bunch of\ndifferent convolutional layers",
    "start": "1022390",
    "end": "1026589"
  },
  {
    "text": "that we then do max-pooling\nover to get a representation",
    "start": "1026589",
    "end": "1030400"
  },
  {
    "text": "for the entire sequence.",
    "start": "1030400",
    "end": "1032140"
  },
  {
    "text": "We do that at\ndifferent layers here.",
    "start": "1032140",
    "end": "1034189"
  },
  {
    "text": "And so those get concatenated\nup into these max-pooling",
    "start": "1034190",
    "end": "1037240"
  },
  {
    "text": "representations at the top.",
    "start": "1037240",
    "end": "1039130"
  },
  {
    "text": "And those form the basis\nfor word representations.",
    "start": "1039130",
    "end": "1042490"
  },
  {
    "text": "And the idea is that this gives\nus whole word vectors that",
    "start": "1042490",
    "end": "1047140"
  },
  {
    "text": "nonetheless have\nlots of information",
    "start": "1047140",
    "end": "1049390"
  },
  {
    "text": "about the subword parts, all\nthe way down to characters",
    "start": "1049390",
    "end": "1053230"
  },
  {
    "text": "but including all of\nthese convolutions",
    "start": "1053230",
    "end": "1055270"
  },
  {
    "text": "of different lengths that\ncapture different notions",
    "start": "1055270",
    "end": "1057560"
  },
  {
    "text": "of subword within that space.",
    "start": "1057560",
    "end": "1059900"
  },
  {
    "text": "Incredibly visionary,\nI would say.",
    "start": "1059900",
    "end": "1062780"
  },
  {
    "text": "One thing I should note, though,\nis that the ELMo vocabulary",
    "start": "1062780",
    "end": "1065570"
  },
  {
    "text": "has about 100,000\nwords in it, which",
    "start": "1065570",
    "end": "1068240"
  },
  {
    "text": "is an enormous vocabulary.",
    "start": "1068240",
    "end": "1070100"
  },
  {
    "text": "And even still, if you\ndeal with real text,",
    "start": "1070100",
    "end": "1072830"
  },
  {
    "text": "you will find that you are\nmostly encountering words that",
    "start": "1072830",
    "end": "1076010"
  },
  {
    "text": "are not in that vocabulary.",
    "start": "1076010",
    "end": "1077990"
  },
  {
    "text": "And even if you double\nit to 200,000 or 300,000,",
    "start": "1077990",
    "end": "1082340"
  },
  {
    "text": "now you're getting a really\nlarge embedding space,",
    "start": "1082340",
    "end": "1084590"
  },
  {
    "text": "you will still mostly encounter\nwords that are unked out--",
    "start": "1084590",
    "end": "1089330"
  },
  {
    "text": "that is unknown to your model.",
    "start": "1089330",
    "end": "1091309"
  },
  {
    "text": "And that's an incredibly\nlimiting factor",
    "start": "1091310",
    "end": "1093650"
  },
  {
    "text": "for this whole word approach.",
    "start": "1093650",
    "end": "1095600"
  },
  {
    "text": "But we see in here the\nessence of the idea",
    "start": "1095600",
    "end": "1097850"
  },
  {
    "text": "that we should model subwords.",
    "start": "1097850",
    "end": "1101240"
  },
  {
    "text": "A big change happened\nfor the transformer",
    "start": "1101240",
    "end": "1103910"
  },
  {
    "text": "when we got in\nparallel this notion",
    "start": "1103910",
    "end": "1106220"
  },
  {
    "text": "of wordpiece tokenization.",
    "start": "1106220",
    "end": "1108280"
  },
  {
    "text": "Here I'm going to give\nyou a feel for that",
    "start": "1108280",
    "end": "1110030"
  },
  {
    "text": "by looking at the BERT\ntokenizer, that gets loaded in",
    "start": "1110030",
    "end": "1113580"
  },
  {
    "text": "and sent in cell 2 here.",
    "start": "1113580",
    "end": "1115399"
  },
  {
    "text": "And then when we call the\ntokenizer on the sentence",
    "start": "1115400",
    "end": "1117920"
  },
  {
    "text": "\"This isn't too\nsurprising,\" we get",
    "start": "1117920",
    "end": "1120140"
  },
  {
    "text": "things that look mostly\nlike whole words,",
    "start": "1120140",
    "end": "1122480"
  },
  {
    "text": "especially if you're used to\nNLP, where the kind of suffix",
    "start": "1122480",
    "end": "1125809"
  },
  {
    "text": "for \"isn't\" has\nbeen broken apart,",
    "start": "1125810",
    "end": "1127460"
  },
  {
    "text": "and so is the punctuation.",
    "start": "1127460",
    "end": "1129870"
  },
  {
    "text": "But when we call the tokenizer\non the sequence \"Encode me!\"",
    "start": "1129870",
    "end": "1133350"
  },
  {
    "text": "with an exclamation mark, notice\nthat the word \"encode\" has",
    "start": "1133350",
    "end": "1136620"
  },
  {
    "text": "been split apart into two words,\n\"en\" and then \"code\" with those",
    "start": "1136620",
    "end": "1141780"
  },
  {
    "text": "markers there, indicating that\nthat is a word internal piece.",
    "start": "1141780",
    "end": "1145540"
  },
  {
    "text": "And if you tokenize a\nword like \"snuffleupagus,\"",
    "start": "1145540",
    "end": "1149250"
  },
  {
    "text": "you get a sequence of\n1, 2, 3, 4, 5, 6 parts",
    "start": "1149250",
    "end": "1154380"
  },
  {
    "text": "to that single\nword that came in.",
    "start": "1154380",
    "end": "1157460"
  },
  {
    "text": "The effect here is\nthat we can have",
    "start": "1157460",
    "end": "1159320"
  },
  {
    "text": "a vanishingly small vocabulary.",
    "start": "1159320",
    "end": "1161509"
  },
  {
    "text": "There are under 30,000 words in\nthis BERT tokenization space.",
    "start": "1161510",
    "end": "1165660"
  },
  {
    "text": "So a very small embedding space.",
    "start": "1165660",
    "end": "1167690"
  },
  {
    "text": "But nonetheless,\nwhen we encounter",
    "start": "1167690",
    "end": "1169669"
  },
  {
    "text": "words that are\noutside of that space,",
    "start": "1169670",
    "end": "1171920"
  },
  {
    "text": "they don't get UNKed\nout, but rather we",
    "start": "1171920",
    "end": "1174710"
  },
  {
    "text": "analyze them into subword\npieces that we do have",
    "start": "1174710",
    "end": "1177950"
  },
  {
    "text": "embedding representations for.",
    "start": "1177950",
    "end": "1180830"
  },
  {
    "text": "Incredibly powerful.",
    "start": "1180830",
    "end": "1182570"
  },
  {
    "text": "And in the context of\na contextual model,",
    "start": "1182570",
    "end": "1185419"
  },
  {
    "text": "we might have some hope that\nfor cases like 'en' 'code' being",
    "start": "1185420",
    "end": "1189200"
  },
  {
    "text": "split into two tokens, the\nmodel will learn internally that",
    "start": "1189200",
    "end": "1192799"
  },
  {
    "text": "in some sense, those\nform a coherent piece--",
    "start": "1192800",
    "end": "1195890"
  },
  {
    "text": "a word.",
    "start": "1195890",
    "end": "1196580"
  },
  {
    "text": "But we don't need that\ndirectly reflected",
    "start": "1196580",
    "end": "1199159"
  },
  {
    "text": "in the tokenizer's vocabulary.",
    "start": "1199160",
    "end": "1202670"
  },
  {
    "text": "Incredible idea.",
    "start": "1202670",
    "end": "1205190"
  },
  {
    "text": "A related idea for\nthe transformer",
    "start": "1205190",
    "end": "1207019"
  },
  {
    "text": "that is so\nfoundational and that I",
    "start": "1207020",
    "end": "1208580"
  },
  {
    "text": "think the field is\nstill figuring out",
    "start": "1208580",
    "end": "1211070"
  },
  {
    "text": "is propositional encoding.",
    "start": "1211070",
    "end": "1212929"
  },
  {
    "text": "When we talk about\nthe transformer,",
    "start": "1212930",
    "end": "1214640"
  },
  {
    "text": "you will see that\nit has almost no way",
    "start": "1214640",
    "end": "1216500"
  },
  {
    "text": "of keeping track of the\norder of words in a sequence.",
    "start": "1216500",
    "end": "1219330"
  },
  {
    "text": "It's mostly a bunch of\ncolumns of different things",
    "start": "1219330",
    "end": "1222450"
  },
  {
    "text": "that happen independently\nwith some attention mechanisms",
    "start": "1222450",
    "end": "1225690"
  },
  {
    "text": "bringing them together.",
    "start": "1225690",
    "end": "1227350"
  },
  {
    "text": "So to capture word\norder, we typically",
    "start": "1227350",
    "end": "1230070"
  },
  {
    "text": "have something like a\npositional encoding.",
    "start": "1230070",
    "end": "1232529"
  },
  {
    "text": "The most heavy handed way to\ndo that is to simply have,",
    "start": "1232530",
    "end": "1236100"
  },
  {
    "text": "in addition to your\nword embedding space,",
    "start": "1236100",
    "end": "1238500"
  },
  {
    "text": "a propositional\nembedding space that",
    "start": "1238500",
    "end": "1240750"
  },
  {
    "text": "simply records where individual\nwords appear in the sequence.",
    "start": "1240750",
    "end": "1244990"
  },
  {
    "text": "So here \"The\" is paired\nwith 1 because it's",
    "start": "1244990",
    "end": "1247140"
  },
  {
    "text": "at the start of the sequence.",
    "start": "1247140",
    "end": "1248590"
  },
  {
    "text": "But if \"The\" was\nin position 4, it",
    "start": "1248590",
    "end": "1251220"
  },
  {
    "text": "would be this fixed\nvector here combined",
    "start": "1251220",
    "end": "1254039"
  },
  {
    "text": "with the embedding\nfor position 4.",
    "start": "1254040",
    "end": "1256530"
  },
  {
    "text": "And those get added\ntogether into what",
    "start": "1256530",
    "end": "1258480"
  },
  {
    "text": "you might think of as the basis\nfor contextual representation.",
    "start": "1258480",
    "end": "1263690"
  },
  {
    "text": "This has proved\neffective, but it",
    "start": "1263690",
    "end": "1265220"
  },
  {
    "text": "has many limitations\nthat we're going to talk",
    "start": "1265220",
    "end": "1267230"
  },
  {
    "text": "about later in the unit.",
    "start": "1267230",
    "end": "1268610"
  },
  {
    "text": "And we're going to explore\nways to capture what's",
    "start": "1268610",
    "end": "1270980"
  },
  {
    "text": "good about\npropositional encoding",
    "start": "1270980",
    "end": "1272690"
  },
  {
    "text": "while also overcoming\nsome of the problems",
    "start": "1272690",
    "end": "1274940"
  },
  {
    "text": "that it introduces.",
    "start": "1274940",
    "end": "1277659"
  },
  {
    "text": "And then, of course,\none of the major guiding",
    "start": "1277660",
    "end": "1280720"
  },
  {
    "text": "ideas behind all\nof this is simply",
    "start": "1280720",
    "end": "1283659"
  },
  {
    "text": "massive scale pretraining.",
    "start": "1283660",
    "end": "1285520"
  },
  {
    "text": "This is an idea\nthat was unlocked",
    "start": "1285520",
    "end": "1287590"
  },
  {
    "text": "by the distributional\nhypothesis, which",
    "start": "1287590",
    "end": "1290260"
  },
  {
    "text": "had the insight\nthat we don't need",
    "start": "1290260",
    "end": "1291730"
  },
  {
    "text": "to write hand-built\nfeature functions,",
    "start": "1291730",
    "end": "1293350"
  },
  {
    "text": "but rather we can just\nrely on unlabeled corpora",
    "start": "1293350",
    "end": "1297039"
  },
  {
    "text": "and keep track of which words\nare appearing with which",
    "start": "1297040",
    "end": "1299890"
  },
  {
    "text": "other words.",
    "start": "1299890",
    "end": "1301070"
  },
  {
    "text": "It really comes into its\nown in the neural era",
    "start": "1301070",
    "end": "1304179"
  },
  {
    "text": "with models like word2vec\nand GloVe, as I mentioned.",
    "start": "1304180",
    "end": "1308230"
  },
  {
    "text": "Following that, we\nget the ELMo paper.",
    "start": "1308230",
    "end": "1310630"
  },
  {
    "text": "And I mentioned before that that\nwas the eye-opening moment when",
    "start": "1310630",
    "end": "1313360"
  },
  {
    "text": "we saw that we could learn\ncontextual representations",
    "start": "1313360",
    "end": "1316840"
  },
  {
    "text": "at scale and have that\ntransfer into tasks",
    "start": "1316840",
    "end": "1320440"
  },
  {
    "text": "we wanted to fine tune for.",
    "start": "1320440",
    "end": "1322659"
  },
  {
    "text": "And, of course, you\nget the GPT paper.",
    "start": "1322660",
    "end": "1324970"
  },
  {
    "text": "And then BERT\nlaunches the BERT era.",
    "start": "1324970",
    "end": "1328510"
  },
  {
    "text": "And then you get, at the end\nof this little history here,",
    "start": "1328510",
    "end": "1331750"
  },
  {
    "text": "the GPT 3 paper, which\napplied this massive scale",
    "start": "1331750",
    "end": "1335170"
  },
  {
    "text": "idea at a level\nthat was previously",
    "start": "1335170",
    "end": "1337930"
  },
  {
    "text": "unimagined and unimaginable.",
    "start": "1337930",
    "end": "1340120"
  },
  {
    "text": "And that really did introduce\na phase change in research",
    "start": "1340120",
    "end": "1343450"
  },
  {
    "text": "as we started to deal with\nthese truly massive models",
    "start": "1343450",
    "end": "1346840"
  },
  {
    "text": "and ask them to\nlearn in context.",
    "start": "1346840",
    "end": "1349429"
  },
  {
    "text": "That is just from prompts we\noffer them how to perform tasks",
    "start": "1349430",
    "end": "1352900"
  },
  {
    "text": "and so forth.",
    "start": "1352900",
    "end": "1355160"
  },
  {
    "text": "And then related\nto this, of course,",
    "start": "1355160",
    "end": "1357170"
  },
  {
    "text": "is the idea of fine tuning\nand its corresponding notion",
    "start": "1357170",
    "end": "1360760"
  },
  {
    "text": "of pretraining.",
    "start": "1360760",
    "end": "1361780"
  },
  {
    "text": "Here's a brief review\nof these ideas.",
    "start": "1361780",
    "end": "1364300"
  },
  {
    "text": "In 2016 to 2018, the notion\nof pretraining that we had",
    "start": "1364300",
    "end": "1369040"
  },
  {
    "text": "was essentially that we\nwould feed static word",
    "start": "1369040",
    "end": "1371230"
  },
  {
    "text": "representations into\nvariants of RNNs.",
    "start": "1371230",
    "end": "1374740"
  },
  {
    "text": "And then the model\nwould be fine tuned,",
    "start": "1374740",
    "end": "1377170"
  },
  {
    "text": "and it would learn\na bunch of stuff.",
    "start": "1377170",
    "end": "1379210"
  },
  {
    "text": "When we fast forward to\nthe BERT era in 2018,",
    "start": "1379210",
    "end": "1382720"
  },
  {
    "text": "we start to get fine tuning\nof contextual models.",
    "start": "1382720",
    "end": "1385870"
  },
  {
    "text": "And here is just a\nbit of code of a sort",
    "start": "1385870",
    "end": "1388540"
  },
  {
    "text": "that you will write\nin this course,",
    "start": "1388540",
    "end": "1390130"
  },
  {
    "text": "where we read in\nBERT representations",
    "start": "1390130",
    "end": "1392920"
  },
  {
    "text": "and fine tune them essentially\nto be a classifier.",
    "start": "1392920",
    "end": "1397180"
  },
  {
    "text": "That started in 2018,\nand I think it continues.",
    "start": "1397180",
    "end": "1399910"
  },
  {
    "text": "We might be headed into\nan era in which most",
    "start": "1399910",
    "end": "1402580"
  },
  {
    "text": "of the fine tuning that happens\nis on these massive language",
    "start": "1402580",
    "end": "1406120"
  },
  {
    "text": "models that we mostly\ndon't have access to.",
    "start": "1406120",
    "end": "1408470"
  },
  {
    "text": "So we can't write code as\nin the BERT code there,",
    "start": "1408470",
    "end": "1411770"
  },
  {
    "text": "but rather we just call\nan API and some kind",
    "start": "1411770",
    "end": "1414670"
  },
  {
    "text": "of partially understood and\npartially known to us, fine",
    "start": "1414670",
    "end": "1418210"
  },
  {
    "text": "tuning process, fine tune\nsome of the parameters for one",
    "start": "1418210",
    "end": "1421570"
  },
  {
    "text": "of these large models to\ndo what we want to do.",
    "start": "1421570",
    "end": "1424190"
  },
  {
    "text": "I'm hoping that we still see\na lot more of this custom",
    "start": "1424190",
    "end": "1426789"
  },
  {
    "text": "code being written.",
    "start": "1426790",
    "end": "1427690"
  },
  {
    "text": "It's very powerful analytically\nand technologically.",
    "start": "1427690",
    "end": "1431110"
  },
  {
    "text": "But this is surely part of your\nfuture as an NLPer as well.",
    "start": "1431110",
    "end": "1437549"
  },
  {
    "start": "1437550",
    "end": "1442000"
  }
]