[
  {
    "start": "0",
    "end": "192000"
  },
  {
    "text": "So this lecture is going to be on reinforcement learning. Um, I will, in the interest of time,",
    "start": "4190",
    "end": "10379"
  },
  {
    "text": "skip the, the quiz. So, so the way to think about how reinforcement learning fits into what we've done so far is,",
    "start": "10380",
    "end": "18170"
  },
  {
    "text": "you remember this class has this picture, right? So we talk about different models and we talk about different algorithms,",
    "start": "18170",
    "end": "26039"
  },
  {
    "text": "inference algorithms to be able to predict using these models and answer queries, and then we have learning which is,",
    "start": "26040",
    "end": "33480"
  },
  {
    "text": "how do you actually learn these models, right? So every type of model we go through, we have to kind of check the boxes for each of these [NOISE] pieces.",
    "start": "33480",
    "end": "41490"
  },
  {
    "text": "So last lecture, we talked about Markov decision processes.",
    "start": "41490",
    "end": "47105"
  },
  {
    "text": "This is a kind of a modeling framework, allows you to define models. For example, for crossing volcanoes or playing dice games or tram, taking trams.",
    "start": "47105",
    "end": "57079"
  },
  {
    "text": "Um, what about inference? So what do we have here last time? We had value iteration and which allows you to compute the optimal policy and policy,",
    "start": "57080",
    "end": "66950"
  },
  {
    "text": "uh, evaluation which eva- allows you to estimate the value of,",
    "start": "66950",
    "end": "72960"
  },
  {
    "text": "uh, a particular policy. So these are algorithms that, um, will operate on MDP, right?",
    "start": "72960",
    "end": "80015"
  },
  {
    "text": "And we sort of looked at these algorithms last time. So this lecture is gonna be about learning.",
    "start": "80015",
    "end": "86865"
  },
  {
    "text": "Uh, I'll just put RL for now. RL is not an algorithm, it's a kind of, uh, refers to the family of algorithms that fits in, uh, this week.",
    "start": "86865",
    "end": "95955"
  },
  {
    "text": "Um, but that's the way you should think about it. RL allows you to, um, either explicitly or implicitly estimate MDPs.",
    "start": "95955",
    "end": "103145"
  },
  {
    "text": "And then once you have that, you can do all these, um, uh, inference algorithms to, uh,",
    "start": "103145",
    "end": "108665"
  },
  {
    "text": "figure, uh, what the optimal policy is. Okay? [NOISE] So just to review.",
    "start": "108665",
    "end": "115220"
  },
  {
    "text": "Um, so what is the MDP? Um, the clearest way- remember to think about it is- it's,",
    "start": "115220",
    "end": "122509"
  },
  {
    "text": "um, in terms of a graph. So you have a set of states. So in this dice game, we have in and end.",
    "start": "122510",
    "end": "129304"
  },
  {
    "text": "So we have a set of states. From every state, you have a set of actions coming out.",
    "start": "129305",
    "end": "135035"
  },
  {
    "text": "So in this case, uh, stay and quit. Um, the actions take you to chance nodes, uh,",
    "start": "135035",
    "end": "143695"
  },
  {
    "text": "where the- uh, you don't get to control what happens, but nature does and there's randomness.",
    "start": "143695",
    "end": "150190"
  },
  {
    "text": "So out of these chance nodes are transitions. Each transition takes you into a state,",
    "start": "150190",
    "end": "156180"
  },
  {
    "text": "it has some probability associated with it. So two-thirds in this case. It also has some reward associated with it which you pick up along the way.",
    "start": "156180",
    "end": "163820"
  },
  {
    "text": "So naturally, this has to be one-third, four and remember last time,",
    "start": "163820",
    "end": "169020"
  },
  {
    "text": "this was probability 1:10. Okay. So, um, and then there is, you know, uh,",
    "start": "169020",
    "end": "177644"
  },
  {
    "text": "the discount factor which Gamma, which is a number between 0 and 1 tells you how much you value the future.",
    "start": "177645",
    "end": "183830"
  },
  {
    "text": "Uh, for default, you can think about it as 1, uh, for simplicity. Okay. So this is a Markov decision process.",
    "start": "183830",
    "end": "191835"
  },
  {
    "text": "Um, and what do you do with one of these things? [NOISE] We, um, have a notion of a policy and a policy,",
    "start": "191835",
    "end": "202304"
  },
  {
    "start": "192000",
    "end": "648000"
  },
  {
    "text": "um, [NOISE] see, I'll write it over here. So a policy denoted Pi. Uh, let me use green.",
    "start": "202304",
    "end": "211025"
  },
  {
    "text": "Um, so a policy, Pi, uh, is a mapping from states to action.",
    "start": "211025",
    "end": "218600"
  },
  {
    "text": "It tells you a policy when you apply it, it says, \"When I land here, where should I go? Should I do stay or quit?\"",
    "start": "218600",
    "end": "224295"
  },
  {
    "text": "If I land, well, I mean this is kind of a simple MDP. Otherwise, there'd usually be more states and for every state,",
    "start": "224295",
    "end": "229939"
  },
  {
    "text": "blue circle will tell you where to go. Um, and when you run a policy, uh, what happens?",
    "start": "229939",
    "end": "236909"
  },
  {
    "text": "Uh, you get a path, um, which I'm going to call an episode. So what do you do?",
    "start": "236910",
    "end": "244010"
  },
  {
    "text": "You start in state S_0, that's- that will be in. In this particular example, um,",
    "start": "244010",
    "end": "250000"
  },
  {
    "text": "you take an action a_1, let's say stay. Uh, you get some reward,",
    "start": "250000",
    "end": "255060"
  },
  {
    "text": "in this case it will be 4. You end up in a new state, um, oops, S_1.",
    "start": "255060",
    "end": "260609"
  },
  {
    "text": "And suppose you go back to end and, uh, then you take another action, maybe it's stay, reward is 4 again and,",
    "start": "260610",
    "end": "268290"
  },
  {
    "text": "and so on, right? So this sequence is a path or in RL speak,",
    "start": "268290",
    "end": "275040"
  },
  {
    "text": "it's, uh, an episode. Um, let's see. So let me- let me erase this comment.",
    "start": "275040",
    "end": "284070"
  },
  {
    "text": "Uh, so this is an episode. Um, and until you hit the end state.",
    "start": "284070",
    "end": "292245"
  },
  {
    "text": "Um, and, uh, what happens out of the episode, you can look at a utility.",
    "start": "292245",
    "end": "299134"
  },
  {
    "text": "We're gonna denote U which is the discounted sum of rewards along the way, right?",
    "start": "299135",
    "end": "304670"
  },
  {
    "text": "So if you, um, you know, stayed three times and then went there, you would have, uh,",
    "start": "304670",
    "end": "311090"
  },
  {
    "text": "a utility of 4 plus 4 plus 4 plus 4, so that'll be 16.",
    "start": "311090",
    "end": "316610"
  },
  {
    "text": "Okay? So the last lecture, we didn't really work with,",
    "start": "316610",
    "end": "321920"
  },
  {
    "text": "um, the episodes into utility, um, because we were able to define a set of recurrences that,",
    "start": "321920",
    "end": "328625"
  },
  {
    "text": "uh, computed the expected utility. So, uh, remember that we want to- you know,",
    "start": "328625",
    "end": "334650"
  },
  {
    "text": "we don't know what's going to happen. So, uh, there's a distribution, and in order to optimize something, we have to turn it to a number,",
    "start": "334650",
    "end": "340870"
  },
  {
    "text": "that's what expectation does. Um, so there's two, uh, concepts that we had from last time.",
    "start": "340870",
    "end": "347080"
  },
  {
    "text": "One is the value function of a particular policy. So V_Pi of S is the expected utility if you follow Pi from S. What does that mean?",
    "start": "347080",
    "end": "355419"
  },
  {
    "text": "That means, if you take a particular S, let's take, uh, n, and I put you there,",
    "start": "355420",
    "end": "361510"
  },
  {
    "text": "and you run the policy, so stay and you traverse this graph, um, you will have different utilities coming out and the average of",
    "start": "361510",
    "end": "369140"
  },
  {
    "text": "those is going to be V_Pi of S. Similarly, there's a Q value,",
    "start": "369140",
    "end": "374395"
  },
  {
    "text": "um, expect the utility, if you first take an action from a state S and then follow Pi.",
    "start": "374395",
    "end": "379759"
  },
  {
    "text": "So what does that mean? That means if I put you on one of these, uh, red chance nodes and you basically play out the game,",
    "start": "379760",
    "end": "386360"
  },
  {
    "text": "um, and average the resulting utilities that you get, what number do you get? Okay? [NOISE] Um, and we saw recurrences that related these two.",
    "start": "386360",
    "end": "398580"
  },
  {
    "text": "So V_Pi of S is, um, you, recurrence, the name of the game",
    "start": "398580",
    "end": "404090"
  },
  {
    "text": "is to kind of delegate to some kind of simpler problems. So you first, uh, look up what you're supposed to do in s, that's",
    "start": "404090",
    "end": "410330"
  },
  {
    "text": "Pi S [NOISE] and that takes you to a chance node which is s, Pi S of S, and then you say, \"Hey,",
    "start": "410330",
    "end": "416610"
  },
  {
    "text": "how much, um, utility am I going to get from that node?\" And similarly from the, the chance nodes,",
    "start": "416610",
    "end": "422509"
  },
  {
    "text": "you have to look at all the possible successors, the probability of going into that successor, um,",
    "start": "422510",
    "end": "429039"
  },
  {
    "text": "of the immediate reward that you get along the edge plus the discounted, um,",
    "start": "429040",
    "end": "434405"
  },
  {
    "text": "reward of the kind of a future when you end up in, um, S-prime.",
    "start": "434405",
    "end": "442530"
  },
  {
    "text": "Okay. So any questions about this? This is kind of review of, uh, Markov decision processes from, um, last time.",
    "start": "442530",
    "end": "450639"
  },
  {
    "text": "Okay. So now we're about to do something different. Okay. So, um, if you say goodbye to the transition and rewards,",
    "start": "456980",
    "end": "465770"
  },
  {
    "text": "that's called reinforcement learning. So remember Markov decision processes. I give you everything here and you just have to find the optimal policy.",
    "start": "465770",
    "end": "474290"
  },
  {
    "text": "And now, I'm gonna make life difficult by not even telling you,",
    "start": "474290",
    "end": "479310"
  },
  {
    "text": "um, what rewards and what are transitions you have to get. Okay. So just to get a, kind of flavor of what that's like.",
    "start": "479310",
    "end": "487460"
  },
  {
    "text": "Um, let's play a game. So, um, I'm going to need a volunteer. I'll, I'll give you the game,",
    "start": "487460",
    "end": "493310"
  },
  {
    "text": "but this volunteer, you have to have a lot of, uh, grit and, uh, persistence, because this is not gonna be [NOISE] an easy game.",
    "start": "493310",
    "end": "499970"
  },
  {
    "text": "You have to be one of those people that even though you're losing a lot, uh, you're still gonna not give up.",
    "start": "499970",
    "end": "505470"
  },
  {
    "text": "Okay. So here's how the game works. Um, so for each round, r equals, uh, 1, 2, 3,",
    "start": "505470",
    "end": "510990"
  },
  {
    "text": "4, 5, 6, and so on. You're just going to choose A or B, um, red pill or blue pill, I guess.",
    "start": "510990",
    "end": "517514"
  },
  {
    "text": "Um, and you, you move to a new state. So the state is here and you get some rewards which I'm gonna show here.",
    "start": "517515",
    "end": "526145"
  },
  {
    "text": "Okay. And the state is 5, 0, that's the initial state. Okay. So everything clear about the rules of the game?",
    "start": "526145",
    "end": "534260"
  },
  {
    "text": "[LAUGHTER] That's reinforcement learning, right? [LAUGHTER] We don't know anything about how.",
    "start": "534260",
    "end": "539764"
  },
  {
    "text": "Okay. So any volunteers. Um, how about you in the front? Okay. Okay.",
    "start": "539765",
    "end": "545720"
  },
  {
    "text": "Okay. Let me, let me fix that. A. A, A, [LAUGHTER] [NOISE] [LAUGHTER] B, B, A, [LAUGHTER] A.",
    "start": "545720",
    "end": "549120"
  },
  {
    "text": "It's a MDP, so, uh, in that case that helps. B, B, B, B,",
    "start": "561350",
    "end": "571290"
  },
  {
    "text": "B, just infinitely click B with an A, I guess. [LAUGHTER] It's like I'm losing a point every time.",
    "start": "571290",
    "end": "580470"
  },
  {
    "text": "I warned you. [LAUGHTER] Okay. A, A, A, A, B, A,",
    "start": "580470",
    "end": "590689"
  },
  {
    "text": "A, A, A, A, A. [LAUGHTER]",
    "start": "590690",
    "end": "600915"
  },
  {
    "text": "Okay. [APPLAUSE] I'm glad this worked because last time it took a lot longer [LAUGHTER].",
    "start": "600915",
    "end": "608310"
  },
  {
    "text": "Um, but, you know, so what did you have to do? I mean you don't know what to try so you try A and B.",
    "start": "608310",
    "end": "614355"
  },
  {
    "text": "And then hopefully you're building an MDP in your head, right? Yeah, right? [LAUGHTER] Okay. Just smile and nod.",
    "start": "614355",
    "end": "620490"
  },
  {
    "text": "Um, and you have to figure out how the game works, right? So maybe you noticed that hey, A is, you know, decrementing and B isn't going up but then there's this other bit that gets flipped.",
    "start": "620490",
    "end": "629835"
  },
  {
    "text": "So, um, okay you figure this out, and in the process you're also trying to maximize reward which, uh,",
    "start": "629835",
    "end": "635700"
  },
  {
    "text": "apparently I guess wasn't - doesn't come until the very end because, um, it's a cruel game.",
    "start": "635700",
    "end": "641339"
  },
  {
    "text": "[LAUGHTER]. Okay. So how do we get an algorithm to kind of do this and how do we think about, uh, us doing this?",
    "start": "641340",
    "end": "649455"
  },
  {
    "start": "648000",
    "end": "697000"
  },
  {
    "text": "So just to kind of make the contrast between MDPs and reinforcement learning sharper,",
    "start": "649455",
    "end": "655590"
  },
  {
    "text": "so Markov decision process is a offline thing, right? So you already have a mental model of how the work- world works.",
    "start": "655590",
    "end": "662070"
  },
  {
    "text": "That's the MDP, that's all the rewards and the transitions and the states and actions. And you have to find a policy to collect maximum rewards.",
    "start": "662070",
    "end": "669480"
  },
  {
    "text": "You have it all in your head, so you just kind of think really hard about, you know, what is the best thing.",
    "start": "669480",
    "end": "674730"
  },
  {
    "text": "It's like \"Oh, if I do this action then I'll go here\" and, you know, look at the probabilities, take the max of whatever.",
    "start": "674730",
    "end": "680130"
  },
  {
    "text": "So reinforcement learning is very different. You don't know how the world works. So you can't just sit there and think because",
    "start": "680130",
    "end": "685290"
  },
  {
    "text": "thinking isn't going to help you figure out how the world works. Um, so you have to just go out and perform actions in the world, right?",
    "start": "685290",
    "end": "691785"
  },
  {
    "text": "And in doing so you - hopefully you'll learn something but also you'll, um, you'll get some rewards.",
    "start": "691785",
    "end": "697950"
  },
  {
    "text": "Okay so-so to maybe formalize the, um, the paradigm of RL.",
    "start": "697950",
    "end": "703680"
  },
  {
    "text": "So you can think about it as an agent. That's, uh, that's you. Uh, and do you have the environment,",
    "start": "703680",
    "end": "710025"
  },
  {
    "text": "which is everything else that's not an agent. The agent takes actions. So that sends action to the environment and the",
    "start": "710025",
    "end": "716700"
  },
  {
    "text": "environment just send you back rewards and a new state. And you keep on doing this.",
    "start": "716700",
    "end": "721995"
  },
  {
    "text": "Um, so what you have to do is figure out first of all how to - am I going to act.",
    "start": "721995",
    "end": "728460"
  },
  {
    "text": "If I'm in a particular state S_t minus 1, what actions should I choose, okay? So that's one, um, one question.",
    "start": "728460",
    "end": "736530"
  },
  {
    "text": "And then you're gonna get this reward and observe a new state. How -what, what should I do to update my mental model of the world, okay?",
    "start": "736530",
    "end": "744900"
  },
  {
    "text": "So these are the main two questions. I'm going to talk first about how to update the parameters and then",
    "start": "744900",
    "end": "750990"
  },
  {
    "text": "later in the lecture I'm going to come back to how do you actually go and, you know, explore it.",
    "start": "750990",
    "end": "756160"
  },
  {
    "start": "756000",
    "end": "795000"
  },
  {
    "text": "Okay. So I'm not going to say much here but, you know, in the context of volcano crossing, um,",
    "start": "756260",
    "end": "764040"
  },
  {
    "text": "just to kind of think through things, every time you play the game, right? You're gonna get some utility.",
    "start": "764040",
    "end": "769830"
  },
  {
    "text": "So you take -so this is the episode over here. So a r s, you're gonna -sometimes you fall into a pit.",
    "start": "769830",
    "end": "776460"
  },
  {
    "text": "Sometimes you go to a hut. Um, and based on these experiences, um,",
    "start": "776460",
    "end": "782400"
  },
  {
    "text": "if I didn't -hadn't told you what any of the actions do and what's a slip probability or anything,",
    "start": "782400",
    "end": "788579"
  },
  {
    "text": "how would you kind of go about, um, kinda solving this problem? That's a -that's a question.",
    "start": "788580",
    "end": "794770"
  },
  {
    "start": "795000",
    "end": "812000"
  },
  {
    "text": "Okay so there's a bunch of algorithms. I think there's gonna be 1, 2, 3, 4.",
    "start": "795080",
    "end": "801930"
  },
  {
    "text": "At least four algorithms that we're going to talk about with different characteristics. But they're all going to kind of build onto each other in some way.",
    "start": "801930",
    "end": "808575"
  },
  {
    "text": "So first class of algorithms is Monte Carlo methods, right? So, um, okay.",
    "start": "808575",
    "end": "816525"
  },
  {
    "start": "812000",
    "end": "969000"
  },
  {
    "text": "So whenever you're doing RL or any sort of learning, uh, the first thing you get is you just have data.",
    "start": "816525",
    "end": "824399"
  },
  {
    "text": "Let's, let's suppose that you run even a random policy, you're just gonna -because in the beginning you don't know any better,",
    "start": "824400",
    "end": "830339"
  },
  {
    "text": "so you're just going to try random actions and, uh, but in the process you're gonna see \"Hey, I tried this action and it led to this reward and so on\".",
    "start": "830340",
    "end": "838170"
  },
  {
    "text": "So in a concrete example just to make, uh, things a little bit more crisp, it's gonna look something like in, uh,",
    "start": "838170",
    "end": "845625"
  },
  {
    "text": "and then you take, uh, you know you did, um, let's see. Let me try to color coordinate this a little bit.",
    "start": "845625",
    "end": "852750"
  },
  {
    "text": "Um, so you're in n,",
    "start": "852750",
    "end": "858195"
  },
  {
    "text": "you do, um, stay. And then you get a reward of 4 and then you're back in n,",
    "start": "858195",
    "end": "864420"
  },
  {
    "text": "you do a stay, and then you get 4 and then maybe you're done, you're out.",
    "start": "864420",
    "end": "870644"
  },
  {
    "text": "Okay. So this is an example episode just to make things concrete. So this is s_0,",
    "start": "870645",
    "end": "876329"
  },
  {
    "text": "a_1, r_1, s_2, s_1. I keep on incrementing too quickly.",
    "start": "876330",
    "end": "883005"
  },
  {
    "text": "Um, a_2, r_2, s_3, okay? Okay so what should you do here?",
    "start": "883005",
    "end": "890740"
  },
  {
    "text": "Alright so, um, any ideas? Model-based Monte Carlo.",
    "start": "890870",
    "end": "898170"
  },
  {
    "text": "So if you have MDP you would be done. But we don't have MDP, we have data. So what can we do?",
    "start": "898170",
    "end": "904440"
  },
  {
    "text": "[NOISE] Yeah.",
    "start": "904440",
    "end": "910110"
  },
  {
    "text": "[inaudible]. Yeah. Let's try to build a MDP from that data. Okay. So, um, the key idea is estimate the MDP.",
    "start": "910110",
    "end": "919845"
  },
  {
    "text": "Um, so intuitively, we just need to figure out what",
    "start": "919845",
    "end": "925470"
  },
  {
    "text": "the transitions and rewards are and then we're done, right? Um, so how do you do the transitions? Um, so the transition says if I'm in state S and I take action A, what will happen?",
    "start": "925470",
    "end": "936585"
  },
  {
    "text": "I don't know what will happen, but let's see in the data what will happen. So I can look at the number of times I went into a",
    "start": "936585",
    "end": "942870"
  },
  {
    "text": "particular S prime and then divide it over the number of times I attempted any- this action from",
    "start": "942870",
    "end": "950010"
  },
  {
    "text": "that state at all and just take the ratio, okay? And for the rewards, um,",
    "start": "950010",
    "end": "955995"
  },
  {
    "text": "this is actually fairly, you know, easy, when I - because when I observe a reward,",
    "start": "955995",
    "end": "961305"
  },
  {
    "text": "um, from S, A and S prime. I just write it down and say that's the reward, okay?",
    "start": "961305",
    "end": "968955"
  },
  {
    "text": "Okay. So on the concrete example what does this look like? So remember now, here's the MDP graph. I don't know what the -the, uh,",
    "start": "968955",
    "end": "976095"
  },
  {
    "start": "969000",
    "end": "1181000"
  },
  {
    "text": "transition distribution or the rewards are. Um, so let's suppose I get this trajectory.",
    "start": "976095",
    "end": "984315"
  },
  {
    "text": "What should I do? So I get stay, stay, stay, stay, and I'm out, okay?",
    "start": "984315",
    "end": "989865"
  },
  {
    "text": "So first I, I can write down the rewards of 4 here, and then I can, um,",
    "start": "989865",
    "end": "997485"
  },
  {
    "text": "estimate the probability of, you know, transitioning.",
    "start": "997485",
    "end": "1002510"
  },
  {
    "text": "So three out of four times I went back to in. One out of four times I went to end. So I'm gonna estimate as three-fourths, one-fourths.",
    "start": "1002510",
    "end": "1009110"
  },
  {
    "text": "Okay. But then suppose I get a new data point. So I have stay, stay,",
    "start": "1009110",
    "end": "1015470"
  },
  {
    "text": "end. So what do I do? I can add to these counts, um. So everything is kind of cumulative.",
    "start": "1015470",
    "end": "1021890"
  },
  {
    "text": "So two more times, I'm sorry one more time I went into in and another time I went to end, so this becomes four out of six, three out of six.",
    "start": "1021890",
    "end": "1029375"
  },
  {
    "text": "And suppose I see another time when I just go into end, so I'm just going to increment, uh, this counter and now it's three out of seven and four out of seven, okay?",
    "start": "1029375",
    "end": "1038689"
  },
  {
    "text": "So pretty, um, pretty simple. Okay so for reasons I'm not going to get into,",
    "start": "1038690",
    "end": "1044885"
  },
  {
    "text": "this process actually, you know, converges to the -if you do this kind of, uh, you know,",
    "start": "1044885",
    "end": "1049970"
  },
  {
    "text": "a million times, you'll get pretty, um, accurate. Yeah, question?",
    "start": "1049970",
    "end": "1056280"
  },
  {
    "text": "Yes, the question is, you don't know the rewards or the transitions, uh, but yes you do know the set of,",
    "start": "1061910",
    "end": "1069450"
  },
  {
    "text": "ah, states and the actions. Set of states, I guess, you don't have to know them all in advance,",
    "start": "1069450",
    "end": "1074909"
  },
  {
    "text": "but you just observe them as they come. The actions, you need to know because you- you are an agent and you need to play the game.",
    "start": "1074910",
    "end": "1081524"
  },
  {
    "text": "Yeah, good question. Okay. So, yeah. Does this work with variable costs?",
    "start": "1081525",
    "end": "1087360"
  },
  {
    "text": "Like, there is a probabilit- or variable reward around it. There's a probability you get some rewards for probability [inaudible].",
    "start": "1087360",
    "end": "1093540"
  },
  {
    "text": "Yeah. So the question is, does this work with variable, uh, rewards. Um, and if the reward is not a function of, um,",
    "start": "1093540",
    "end": "1102794"
  },
  {
    "text": "sas prime, you would just take the average of the rewards that you see.",
    "start": "1102795",
    "end": "1107865"
  },
  {
    "text": "Yeah. Okay. So- so what do you do with this?",
    "start": "1107865",
    "end": "1113700"
  },
  {
    "text": "So after you estimate the MDP, so all you need is the transitions and rewards. Um, then now we have MDP.",
    "start": "1113700",
    "end": "1120914"
  },
  {
    "text": "It might- it may not be the exact right MDP because this is estimated from data so it's not gonna match it exactly,",
    "start": "1120915",
    "end": "1127033"
  },
  {
    "text": "um, but nonetheless, we already have these tools from last time. You can do value iteration to compute, um,",
    "start": "1127034",
    "end": "1133385"
  },
  {
    "text": "the optimal policy on it and then you just, you know, you're done, you run it. On- in practice, you would probably kind of interleave",
    "start": "1133385",
    "end": "1140520"
  },
  {
    "text": "the learning and the- the optimization but, uh, for simplicity we can think about it as a two-stage where you gather a bunch of data,",
    "start": "1140520",
    "end": "1147120"
  },
  {
    "text": "you estimate the MDP and then you are off. Okay. There's one problem here.",
    "start": "1147120",
    "end": "1153375"
  },
  {
    "text": "Does anyone know what the problem might be? You can actually see it by looking on the slide. Yeah.",
    "start": "1153375",
    "end": "1166860"
  },
  {
    "text": "Well, with your based policy of all this thing, you'll never explore the quick branch of the world.",
    "start": "1166860",
    "end": "1171945"
  },
  {
    "text": "Yeah, yeah. You didn't explore this at all, so you actually don't know how much reward is here. Maybe it's like, uh, you know, 100, right?",
    "start": "1171945",
    "end": "1179640"
  },
  {
    "text": "So- so this is this problem, this kind of actually a pretty big problem that unless you have a policy that,",
    "start": "1179640",
    "end": "1188220"
  },
  {
    "start": "1181000",
    "end": "1250000"
  },
  {
    "text": "uh, actually goes and covers all the- the states, you just won't know, right?",
    "start": "1188220",
    "end": "1194010"
  },
  {
    "text": "And this is kind of natural because there can always be, you know, a lot of reward hiding under a kind of",
    "start": "1194010",
    "end": "1199110"
  },
  {
    "text": "one state but unless you see it you- you don't- you just don't know. Um, okay.",
    "start": "1199110",
    "end": "1204390"
  },
  {
    "text": "So this is a kind of key idea, key challenge I would say, in reinforcement learning is exploration.",
    "start": "1204390",
    "end": "1211080"
  },
  {
    "text": "So you need to be able to explore, um, the state space. This is different from normal machine learning where data just",
    "start": "1211080",
    "end": "1217830"
  },
  {
    "text": "comes in passively and you learn on your nice function and then you're- you're done. Here, you actually have to figure out how to get the data,",
    "start": "1217830",
    "end": "1224985"
  },
  {
    "text": "and that's- that's kind of one of the, the key challenges of RL.",
    "start": "1224985",
    "end": "1229720"
  },
  {
    "text": "So we're gonna go back to this- this problem, and I'm not really gonna, uh,",
    "start": "1230480",
    "end": "1236280"
  },
  {
    "text": "try to solve it now. Um, for now you can just think about Pi as a random policy because a random policy eventually will just,",
    "start": "1236280",
    "end": "1243044"
  },
  {
    "text": "you know, hit everything for, you know, finite, uh, small, uh, state spaces.",
    "start": "1243045",
    "end": "1249225"
  },
  {
    "text": "Okay. So, um, okay. So that's basically end of the first algorithm.",
    "start": "1249225",
    "end": "1257565"
  },
  {
    "start": "1250000",
    "end": "1392000"
  },
  {
    "text": "Let me just write this over here. So algorithms, we have model-based, um, Monte Carlo.",
    "start": "1257565",
    "end": "1267510"
  },
  {
    "text": "And the model-based is referring to the fact that we're estimating a model the- in particular the MDP.",
    "start": "1267510",
    "end": "1274725"
  },
  {
    "text": "The Monte Carlo part is just referring to the fact that we're using samples, uh,",
    "start": "1274725",
    "end": "1280980"
  },
  {
    "text": "to estimate, um, a model or you're basically applying a policy multiple times and then estimating,",
    "start": "1280980",
    "end": "1287520"
  },
  {
    "text": "uh, the model based on averages. Okay. So- so now,",
    "start": "1287520",
    "end": "1293295"
  },
  {
    "text": "I'm going to present a- a different algorithm and it's called, uh, model-free Monte Carlo.",
    "start": "1293295",
    "end": "1300795"
  },
  {
    "text": "And you might from the name guess what we might want to do is maybe we don't have to estimate this model, okay?",
    "start": "1300795",
    "end": "1311309"
  },
  {
    "text": "And why- why is that? Well, what do we do with this model? Um, what we did was we, you know, uh,",
    "start": "1311310",
    "end": "1320070"
  },
  {
    "text": "presumably use value iteration to, um, you know, compute the optimal policy.",
    "start": "1320070",
    "end": "1325620"
  },
  {
    "text": "And the- remember this, uh, recurrence, um, for computing Q_opt, um,",
    "start": "1325620",
    "end": "1331815"
  },
  {
    "text": "it's in terms of T and reward, but at the end of the day all you need is Q_opt.",
    "start": "1331815",
    "end": "1337304"
  },
  {
    "text": "If I told you, um, Q_opt (s, a) which is, um, what is Q_opt (s, a)?",
    "start": "1337305",
    "end": "1342480"
  },
  {
    "text": "It's the, um, the maximum possible utility I could get if I'm in,",
    "start": "1342480",
    "end": "1347790"
  },
  {
    "text": "chance node sa and I follow the optimal policy. So clearly if I knew that, then I would just produce the optimal policy and I'd be done,",
    "start": "1347790",
    "end": "1355365"
  },
  {
    "text": "I don't even need to know- understand the- the rewards and transitions. Okay. So with that, uh,",
    "start": "1355365",
    "end": "1362205"
  },
  {
    "text": "insight is model-free learning, which is that we're just going to try to estimate Q_opt,",
    "start": "1362205",
    "end": "1368670"
  },
  {
    "text": "um, you know, directly. Um, sometimes it can be a little bit confusing what is meant by model-free.",
    "start": "1368670",
    "end": "1375030"
  },
  {
    "text": "So Q_opt itself you can think about as a- as a model, but in the context of MDPs in reinforcement learning,",
    "start": "1375030",
    "end": "1382230"
  },
  {
    "text": "generally people when they say model-free refers to the fact that there's no MDP model,",
    "start": "1382230",
    "end": "1387360"
  },
  {
    "text": "not that there is no, um, model in general. Okay. So, um, so we're not gonna get to Q_opt, uh, yet.",
    "start": "1387360",
    "end": "1398085"
  },
  {
    "start": "1392000",
    "end": "1712000"
  },
  {
    "text": "Um, that will come later in the lecture. So let's warm up a little bit. Um, so here's our data staring at us.",
    "start": "1398085",
    "end": "1405885"
  },
  {
    "text": "Um, remember- let's, let's look at a related quantity, so Q Pi.",
    "start": "1405885",
    "end": "1411150"
  },
  {
    "text": "Remember what Q Pi is. Q Pi (s, a) is an expected utility if we start at",
    "start": "1411150",
    "end": "1416370"
  },
  {
    "text": "s and you first take action a and then follow policy Pi, right?",
    "start": "1416370",
    "end": "1421590"
  },
  {
    "text": "So in, um, in- I guess another way to write this is,",
    "start": "1421590",
    "end": "1427245"
  },
  {
    "text": "um, if you are at a particular, uh, time step t, you can define u_t as",
    "start": "1427245",
    "end": "1434550"
  },
  {
    "text": "the- the discounted sum of the rewards from that point on, which is, you know, the reward immediately that you will get plus",
    "start": "1434550",
    "end": "1441390"
  },
  {
    "text": "the discounted part in the non- next time step plus, you know, a square discounted and then,",
    "start": "1441390",
    "end": "1446625"
  },
  {
    "text": "uh, two time steps in the future and so on. And, um, what you can do is you can try to estimate Q Pi from this utility.",
    "start": "1446625",
    "end": "1458685"
  },
  {
    "text": "Right? So this is the utility, uh, that you get out to predict your time steps. So suppose you do the following.",
    "start": "1458685",
    "end": "1465885"
  },
  {
    "text": "So suppose you average the utilities that you get only on",
    "start": "1465885",
    "end": "1471480"
  },
  {
    "text": "the time steps where I was in a particular state s and I took an action a.",
    "start": "1471480",
    "end": "1477945"
  },
  {
    "text": "Okay. So you have a- let's suppose you have a bunch of episodes, right? So, um, here pictorially, um,",
    "start": "1477945",
    "end": "1483660"
  },
  {
    "text": "uh, let's see. [NOISE] Here's another way to think about it.",
    "start": "1483660",
    "end": "1490110"
  },
  {
    "text": "So I get a bunch of episodes. I'm gonna do- do some abstract, um, drawing here.",
    "start": "1490110",
    "end": "1495434"
  },
  {
    "text": "Um, so every time you have you know, s, a shows up here, maybe it shows up here, maybe it shows up here,",
    "start": "1495435",
    "end": "1501260"
  },
  {
    "text": "maybe it shows up here, you're going to look at how much reward do I get from that point on? How much reward do I get from here on?",
    "start": "1501260",
    "end": "1508220"
  },
  {
    "text": "How much reward do I get from here on? And, um, average them, right? So there's a kind of, a technicality",
    "start": "1508220",
    "end": "1515910"
  },
  {
    "text": "which is that if s, a appears here and it also appears, uh, after it then I'm not going to count that because I'm",
    "start": "1515910",
    "end": "1521850"
  },
  {
    "text": "kind of- if I do both I'm kind of double counting. Um, in fact it works both ways, but just,",
    "start": "1521850",
    "end": "1528990"
  },
  {
    "text": "conceptually it's easier to think about just taking of, uh, an s, a, uh, of the same you don't kind of go back to the same position.",
    "start": "1528990",
    "end": "1538570"
  },
  {
    "text": "Okay, so let's do that on a concrete example. So Q-pi, let's just write it.",
    "start": "1538610",
    "end": "1544230"
  },
  {
    "text": "Q-pi s, a is a thing where we're trying to estimate and this is,",
    "start": "1544230",
    "end": "1549960"
  },
  {
    "text": "uh, a value associated with every chance node s, a. So in particular, I've drawn it here.",
    "start": "1549960",
    "end": "1555180"
  },
  {
    "text": "I need a value here and, uh, a value here. Okay? So suppose I get some data,",
    "start": "1555180",
    "end": "1562169"
  },
  {
    "text": "I stay and then I got- go to the end. Uh, so what's my utility here? It's not a trick question.",
    "start": "1562170",
    "end": "1573570"
  },
  {
    "text": "4. 4, yes. Um, sum of 4 is 4. Okay, so now I can say, \"Okay it's 4.\"",
    "start": "1573570",
    "end": "1580320"
  },
  {
    "text": "And that's my best guess so far. I mean, I haven't seen anything else, maybe it's 4. Um, so what happens if I play the game again and I get 4, 4?",
    "start": "1580320",
    "end": "1589320"
  },
  {
    "text": "So what's the utility here? 8. 8? So then I update this to the average of 4 and 8,",
    "start": "1589320",
    "end": "1595860"
  },
  {
    "text": "do it again, I get 16 then I average, uh, in the 16. Okay? And, um, and again, you know,",
    "start": "1595860",
    "end": "1605595"
  },
  {
    "text": "I'm using stays so I don't learn anything about this,  in practice you would actually go explore this and figure out how much utility you're seeing there.",
    "start": "1605595",
    "end": "1612690"
  },
  {
    "text": "So in particular, notice I'm not updating the rewards nor the transitions because I'm model-free,",
    "start": "1612690",
    "end": "1618434"
  },
  {
    "text": "I just care about the Q values that I get which are the values that sit at the nodes not on the edges.",
    "start": "1618435",
    "end": "1624519"
  },
  {
    "text": "Okay, so one caveat is that we are estimating Q-pi not Q-opt.",
    "start": "1625820",
    "end": "1632264"
  },
  {
    "text": "We'll revisit this, um, later. Um, and another, uh,",
    "start": "1632265",
    "end": "1637809"
  },
  {
    "text": "thing to kind of note is the difference between what is called On-policy and Off-policy.",
    "start": "1637810",
    "end": "1645225"
  },
  {
    "text": "Okay? So in reinforcement learning, you're always following some policy to get around the world right?",
    "start": "1645225",
    "end": "1654350"
  },
  {
    "text": "Um, and that's generally called the exploration-policy or the control policy um,",
    "start": "1654350",
    "end": "1659700"
  },
  {
    "text": "and then there's usually some other thing that you're trying to estimate, usually the- the value of",
    "start": "1659700",
    "end": "1664950"
  },
  {
    "text": "a particular policy and that policy could be the same or it could be different. So On-policy means that, uh,",
    "start": "1664950",
    "end": "1671430"
  },
  {
    "text": "we're estimating the value of the policy that we're following, the data-generating policy.",
    "start": "1671430",
    "end": "1676800"
  },
  {
    "text": "Off-policy means that we're not. Okay? So um, so in particular is,",
    "start": "1676800",
    "end": "1684390"
  },
  {
    "text": "uh, model-free Monte Carlo, um, On-policy or Off-policy?",
    "start": "1684390",
    "end": "1689620"
  },
  {
    "text": "It's On-policy because I'm estimating Q-pi not Q-opt.",
    "start": "1690200",
    "end": "1695534"
  },
  {
    "text": "Okay? That's On-policy. Um, and Off-policy ,",
    "start": "1695535",
    "end": "1701880"
  },
  {
    "text": "uh, what about model-based Monte Carlo? [NOISE] I mean it's a little bit of a slightly weird question,",
    "start": "1701880",
    "end": "1709350"
  },
  {
    "text": "but in model-based Monte Carlo,",
    "start": "1709350",
    "end": "1714630"
  },
  {
    "start": "1712000",
    "end": "1815000"
  },
  {
    "text": "we're following some policy, maybe even a random policy, but we're estimating the transition then rewards,",
    "start": "1714630",
    "end": "1720150"
  },
  {
    "text": "and from that we can compute the- the optimal policy. So you can- you can think about is, um,",
    "start": "1720150",
    "end": "1726405"
  },
  {
    "text": "Off-policy but, you know, that's maybe not, uh, completely standard.",
    "start": "1726405",
    "end": "1732645"
  },
  {
    "text": "Okay. So any questions about what model-free Monte Carlo is doing?",
    "start": "1732645",
    "end": "1737730"
  },
  {
    "text": "So let me just actually write. So what is model-based Monte Carlo is doing, it's trying to estimate the, uh,",
    "start": "1737730",
    "end": "1744000"
  },
  {
    "text": "the transition and rewards and model-free Monte Carlo is trying to estimate,",
    "start": "1744000",
    "end": "1749850"
  },
  {
    "text": "uh, the, um, Q-pi. Um, okay? And just as- as a note,",
    "start": "1749850",
    "end": "1756990"
  },
  {
    "text": "I put Hats on, uh, any letter that is supposed to be a quantity that is estimated from data and that's what, you know,",
    "start": "1756990",
    "end": "1764220"
  },
  {
    "text": "I guess statisticians do, um, to differentiate them between whenever I Q-pi,",
    "start": "1764220",
    "end": "1769950"
  },
  {
    "text": "that's the true, uh, value of that, you know, policy which, you know, I don't have.",
    "start": "1769950",
    "end": "1776799"
  },
  {
    "text": "Okay, any questions about model-free Monte Carlo? Both of these algorithms are pretty simple, right?",
    "start": "1777890",
    "end": "1784620"
  },
  {
    "text": "You just, you know, you look at the data and you take averages. Yeah.",
    "start": "1784620",
    "end": "1789930"
  },
  {
    "text": "So model free is not trying to optimize [inaudible] policy. So the question is is model-free,",
    "start": "1789930",
    "end": "1797370"
  },
  {
    "text": "uh, making changes to a policy or is it a fixed policy? So- so this version I've given you is only for a fixed policy.",
    "start": "1797370",
    "end": "1804090"
  },
  {
    "text": "The general idea of model-free as we'll see later, uh, you can also optimize the policy.",
    "start": "1804090",
    "end": "1810460"
  },
  {
    "text": "Okay. So- so now what we're gonna do is we're gonna,",
    "start": "1812150",
    "end": "1817515"
  },
  {
    "start": "1815000",
    "end": "1900000"
  },
  {
    "text": "uh, do theme and variations on, uh, model-free Monte Carlo. Actually where it's going to be the same algorithm but I just wanted to interpret",
    "start": "1817515",
    "end": "1825690"
  },
  {
    "text": "it in kind of slightly different ways that'll help us, um, generalize it in the future. Yeah.",
    "start": "1825690",
    "end": "1832395"
  },
  {
    "text": "Are there certain problems where model-free does better than model base? Are there certain problems where model-free is better than model base?",
    "start": "1832395",
    "end": "1839684"
  },
  {
    "text": "So this is actually a really interesting question, right? So, um, you can show that if your model is correct,",
    "start": "1839685",
    "end": "1847725"
  },
  {
    "text": "if your model of the world is correct, model-based is kind of the way to go because there'll be more sample efficient,",
    "start": "1847725",
    "end": "1853380"
  },
  {
    "text": "meaning that you need fewer, uh, data points. But it's really hard to get the model correct in the real world.",
    "start": "1853380",
    "end": "1860625"
  },
  {
    "text": "So recently, especially with, you know, deep reinforcement learning, people have gone a lot of mileage by just going model-free because then, um,",
    "start": "1860625",
    "end": "1869970"
  },
  {
    "text": "jumping ahead a little bit, you can model this as a kind of a deep neural network and that gives you extraordinary flexibility and power without having to solve the hard problem of,",
    "start": "1869970",
    "end": "1878294"
  },
  {
    "text": "you know, constructing the MDP. Okay. So- so there's kind of three ways you can think about this.",
    "start": "1878295",
    "end": "1888330"
  },
  {
    "text": "So the first, we already talked about it, is, you know, this average idea. So we're just looking at the utilities that you",
    "start": "1888330",
    "end": "1895050"
  },
  {
    "text": "see whenever you encounter an s and a, and you just average them. Okay. So here is an equivalent formulation.",
    "start": "1895050",
    "end": "1903300"
  },
  {
    "text": "Um, and the way it works is that for every,",
    "start": "1903300",
    "end": "1908560"
  },
  {
    "text": "um, s, a, u that you see, so every time you see a particular s, a, u, s, a, u, s, a,",
    "start": "1908570",
    "end": "1914160"
  },
  {
    "text": "u and so on, I am going to perform the following update on.",
    "start": "1914160",
    "end": "1919950"
  },
  {
    "text": "So I'm gonna take my existing value and I'm going to do a- what- what we call a convex combination.",
    "start": "1919950",
    "end": "1925410"
  },
  {
    "text": "So, you know, 1 minus eta and eta sum to 1. So it's, you know, a kind of balancing between two things.",
    "start": "1925410",
    "end": "1931575"
  },
  {
    "text": "Balancing between the old value that I had and the- the new utility that I saw.",
    "start": "1931575",
    "end": "1937274"
  },
  {
    "text": "Okay? And the eta is set to be 1 over 1 plus the number of updates. Okay? So let me do a concrete example.",
    "start": "1937275",
    "end": "1943860"
  },
  {
    "text": "I think you'll make this very clear what's- what's going on. So suppose my data looks like this.",
    "start": "1943860",
    "end": "1950130"
  },
  {
    "text": "So I get, uh, 4, um, and then a 1 and a 1. Um, so these are the utilities, right?",
    "start": "1950130",
    "end": "1957120"
  },
  {
    "text": "That's- that's a U here. I'm ignoring the s and a, I'm just assume that there are some- something.",
    "start": "1957120",
    "end": "1962835"
  },
  {
    "text": "Okay, so first, uh, let's assume that Q-pi is 0, okay? So the first time I do, um, uh,",
    "start": "1962835",
    "end": "1971054"
  },
  {
    "text": "let's see, number of updates, I haven't done anything so it's 1, um, 1 minus 0.",
    "start": "1971055",
    "end": "1976230"
  },
  {
    "text": "So 0 times 0 plus 1 times 4 which is the first view that comes in.",
    "start": "1976230",
    "end": "1982605"
  },
  {
    "text": "Um, okay, so this is 4, okay? So then what about the next data point that comes in?",
    "start": "1982605",
    "end": "1989310"
  },
  {
    "text": "So I'm gonna to take, um, one-half now times 4 plus one-half times 1,",
    "start": "1989310",
    "end": "1996150"
  },
  {
    "text": "which is the new value that comes in. And that is, I'm gonna to write it as 4 plus 1 over 2, okay?",
    "start": "1996150",
    "end": "2004250"
  },
  {
    "text": "So now- okay just to keep track of things, this results in this,",
    "start": "2004250",
    "end": "2009790"
  },
  {
    "text": "this results in this, and then now, um, I'm running out of space but hopefully we can- so now on the third one,",
    "start": "2009790",
    "end": "2017049"
  },
  {
    "text": "I do, um, uh, two-thirds, so I have 4 plus 1 over 2 times two-thirds plus,",
    "start": "2017050",
    "end": "2026855"
  },
  {
    "text": "um, actually I- I guess I should do two-thirds to be consistent.",
    "start": "2026855",
    "end": "2033200"
  },
  {
    "text": "Two-thirds times 4 plus 1 over 2 which is the previous value that's sitting in Q-pi plus one-third times 1,",
    "start": "2033200",
    "end": "2041150"
  },
  {
    "text": "which is a new value, and that gives me, um, 4 plus 1 plus 1 over 3, right?",
    "start": "2041150",
    "end": "2048590"
  },
  {
    "text": "So you can see what's going on here is that, you know, each, uh, each time I have this, you know,",
    "start": "2048590",
    "end": "2055895"
  },
  {
    "text": "sum over all the tools I've seen over the number of times it occurs and this eta is set so that next time I kind",
    "start": "2055895",
    "end": "2063830"
  },
  {
    "text": "of cancel out the old uh, count and I add the new count to the denominator and it kind of all works out so that at",
    "start": "2063830",
    "end": "2070940"
  },
  {
    "text": "every time-step what actually is in Q-pi is just a plain average over all of the numbers I've seen before.",
    "start": "2070940",
    "end": "2080070"
  },
  {
    "text": "All right, this is just kind of an algebraic trick to, um, get this original formulation,",
    "start": "2081150",
    "end": "2087639"
  },
  {
    "text": "which is a notion of average, into this formulation which is a notion of, um, kind of you're trying to, um,",
    "start": "2087640",
    "end": "2095240"
  },
  {
    "text": "take a little bit of the old thing and add a little bit of a new thing.",
    "start": "2095240",
    "end": "2100350"
  },
  {
    "text": "Okay. So [NOISE], um, I guess I'm going to call this, uh,",
    "start": "2103020",
    "end": "2111415"
  },
  {
    "text": "I guess, um, combination I guess.",
    "start": "2111415",
    "end": "2116650"
  },
  {
    "text": "So the- that's the second interpretation. There's a third interpretation here which,",
    "start": "2116650",
    "end": "2123650"
  },
  {
    "start": "2120000",
    "end": "2254000"
  },
  {
    "text": "uh, you can think about is, uh, in terms of stochastic gradient descent. So this is actually a kind of a,",
    "start": "2123650",
    "end": "2129010"
  },
  {
    "text": "uh, simple algebraic manipulation. So if you look at this expression, what is this?",
    "start": "2129010",
    "end": "2134140"
  },
  {
    "text": "So you have 1 times Q Pi, so I'm gonna pull it out and put it down here",
    "start": "2134140",
    "end": "2139795"
  },
  {
    "text": "and then I'm gonna have minus eta times Q Pi, that's this thing and then I also have a eta, a u,",
    "start": "2139795",
    "end": "2146724"
  },
  {
    "text": "so I'm going to put kind of minus a- u here and this is, uh, inside this parenthesis.",
    "start": "2146724",
    "end": "2152500"
  },
  {
    "text": "So if you just, you know, do the algebra you can see that these two, you know, are equivalent.",
    "start": "2152500",
    "end": "2159265"
  },
  {
    "text": "Uh, so what's the point of this? Right, so, um, where have you kind of seen this, uh,",
    "start": "2159265",
    "end": "2167245"
  },
  {
    "text": "before, something like, maybe not,",
    "start": "2167245",
    "end": "2173140"
  },
  {
    "text": "not this exact expression but something like that [NOISE]. Any ideas? Yeah, when you look down at a stochastic gradient descent in the context of,",
    "start": "2173140",
    "end": "2182560"
  },
  {
    "text": "uh, the square loss for linear regression. Right, so remember, uh,",
    "start": "2182560",
    "end": "2188440"
  },
  {
    "text": "we had these updates that all looked like kind of prediction minus target which was, you know, the residual and that was used to kind of update.",
    "start": "2188440",
    "end": "2196705"
  },
  {
    "text": "So one way to interpret this is, uh, this is kind of implicitly trying to do",
    "start": "2196705",
    "end": "2203050"
  },
  {
    "text": "stochastic gradient descent on the objective which is a squared, uh, loss on, uh,",
    "start": "2203050",
    "end": "2209425"
  },
  {
    "text": "the, the Q Pi value that you, you, you're trying to set and,",
    "start": "2209425",
    "end": "2216415"
  },
  {
    "text": "uh, u which is the new piece of data that you got. So think about in regression this is the y,",
    "start": "2216415",
    "end": "2221950"
  },
  {
    "text": "this is, uh, y, you know, the- what the output is and you- this is the model that's trying to predict it and you want those to be close to each other.",
    "start": "2221950",
    "end": "2231109"
  },
  {
    "text": "Okay? So, so those are kind of three views on basically,",
    "start": "2233900",
    "end": "2240089"
  },
  {
    "text": "uh, this idea of averaging or incremental updates.",
    "start": "2240090",
    "end": "2245440"
  },
  {
    "text": "Okay. So it'll become clear why, you know, I, I did this isn't just to, you know,",
    "start": "2245550",
    "end": "2251875"
  },
  {
    "text": "have fun. Uh, okay. So now let's, uh, see an example of model- free Monte Carlo in action on this,",
    "start": "2251875",
    "end": "2259930"
  },
  {
    "start": "2254000",
    "end": "2584000"
  },
  {
    "text": "ah, the volcano games. So remember here we have this, uh, you know, volcanic example and, uh,",
    "start": "2259930",
    "end": "2266560"
  },
  {
    "text": "I'm going to, uh, set the number of episodes to let's say 1,000, let's see what happens.",
    "start": "2266560",
    "end": "2273640"
  },
  {
    "text": "Uh, so here, okay. So what does this kind of, uh, uh, grid-like structure, a grid of triangles denote?",
    "start": "2273640",
    "end": "2281349"
  },
  {
    "text": "So this remember is a state, this is 2, 1. So what I am doing here is dividing",
    "start": "2281350",
    "end": "2287770"
  },
  {
    "text": "into four pieces which correspond to the four different action, so this triangle is 2, 1 north,",
    "start": "2287770",
    "end": "2293079"
  },
  {
    "text": "this triangle is 2, 1 east and so on. Okay. And a number here is the Q Pi or value that I'm estimating along the way.",
    "start": "2293080",
    "end": "2303790"
  },
  {
    "text": "Okay, so the, the policy I'm using, uh, is a complete random,",
    "start": "2303790",
    "end": "2309190"
  },
  {
    "text": "uh, just move randomly, uh, and I run this 1,000 times and we see that the average utility is,",
    "start": "2309190",
    "end": "2319105"
  },
  {
    "text": "uh you know, minus 18 which is, uh, obviously not great.",
    "start": "2319105",
    "end": "2324234"
  },
  {
    "text": "Okay. Uh, but this is an estimate of how well the random policy is doing.",
    "start": "2324235",
    "end": "2330490"
  },
  {
    "text": "So, you know, as advertised, you know, random policy you would expect to fall into a volcano quite often.",
    "start": "2330490",
    "end": "2335710"
  },
  {
    "text": "Uh, okay. Uh, and you can run this and sometimes you get slightly different results but,",
    "start": "2335710",
    "end": "2342985"
  },
  {
    "text": "you know, it's pretty much stable around minus 19, minus 18.",
    "start": "2342985",
    "end": "2347150"
  },
  {
    "text": "Okay. Any questions about this before we move on to, uh, different algorithms?",
    "start": "2349380",
    "end": "2356724"
  },
  {
    "text": "Okay. So model-based Monte Carlo we're estimating the MDP, model-free Monte Carlo we're just estimating the Q values of a particular policy for now.",
    "start": "2356725",
    "end": "2367880"
  },
  {
    "text": "Okay. So, so let's revisit what model-free Monte Carlo is doing.",
    "start": "2368790",
    "end": "2379300"
  },
  {
    "text": "So if you use the policy Pi equals stay for the dice game,",
    "start": "2379300",
    "end": "2384550"
  },
  {
    "text": "um, you know, you might get a bunch of different, uh, trajectories that come out.",
    "start": "2384550",
    "end": "2390115"
  },
  {
    "text": "These are possible episodes and in each episode you have a utility, you know, associated with it.",
    "start": "2390115",
    "end": "2395200"
  },
  {
    "text": "Uh, and what model free Monte Carlo is doing is it's using these utilities,",
    "start": "2395200",
    "end": "2401410"
  },
  {
    "text": "uh, to kind of update, uh, towards, uh, update u Q Pi.",
    "start": "2401410",
    "end": "2408410"
  },
  {
    "text": "Right, so in particular like for example this you're saying, okay, I'm in,",
    "start": "2408410",
    "end": "2413765"
  },
  {
    "text": "I'm in, uh, the in-state and I, you know, take an action and stay,",
    "start": "2413765",
    "end": "2419079"
  },
  {
    "text": "when you're- what will happen? Well, in this case I got, you know, 16 and, uh, this case I've got 12.",
    "start": "2419080",
    "end": "2424690"
  },
  {
    "text": "And notice that there's quite a bit of variance. So on average, this actually does the right thing.",
    "start": "2424690",
    "end": "2430165"
  },
  {
    "text": "Right? So, um, just by definition, this is our unbiased, you know,",
    "start": "2430165",
    "end": "2435819"
  },
  {
    "text": "estimate, if you do this a million times and average you're just going to get the right value which is, uh, 12 in this case.",
    "start": "2435820",
    "end": "2442090"
  },
  {
    "text": "But the variance is here, so if you, for example if you only do this a few times,",
    "start": "2442090",
    "end": "2447520"
  },
  {
    "text": "you're not going to get 12, you might get something, you know, sort of related. Uh, so how can we kind of counteract,",
    "start": "2447520",
    "end": "2453505"
  },
  {
    "text": "uh, this, this variance? So the key idea, uh, behind what we're going to call bootstrapping is,",
    "start": "2453505",
    "end": "2460224"
  },
  {
    "text": "is that, you know, we actually have, you know, some more information here.",
    "start": "2460225",
    "end": "2466450"
  },
  {
    "text": "So we have this Q Pi that we're estimating along the way.",
    "start": "2466450",
    "end": "2471700"
  },
  {
    "text": "Right? So, so this view is saying, okay, we're trying to estimate Q Pi, um,",
    "start": "2471700",
    "end": "2476965"
  },
  {
    "text": "and then we're going to try to basically regress it against, you know, this data that we're seeing but, you know,",
    "start": "2476965",
    "end": "2483099"
  },
  {
    "text": "can we actually use Q Pi itself to, uh, help, you know, reduce the variance?",
    "start": "2483100",
    "end": "2489850"
  },
  {
    "text": "So, so the idea here is, uh, um, I'm going to look at all the cases where,",
    "start": "2489850",
    "end": "2499270"
  },
  {
    "text": "you know, I started in and I take stay, I get a 4. Okay? So I'm going to say,",
    "start": "2499270",
    "end": "2505885"
  },
  {
    "text": "I get a 4 but then after that point I'm actually just going to substitute this 11 in.",
    "start": "2505885",
    "end": "2514975"
  },
  {
    "text": "Okay? This is kind of weird, right, because normally I would just see, okay, what would happen?",
    "start": "2514975",
    "end": "2520165"
  },
  {
    "text": "But what happens is kind of random. On average it's going to be right but, you know, on any given case,",
    "start": "2520165",
    "end": "2526060"
  },
  {
    "text": "I'm gonna get, like, you know, 24 or something. And the, the hope here is that by using",
    "start": "2526060",
    "end": "2532060"
  },
  {
    "text": "my current estimate which isn't going to be right because if I were, if it were right I would be done but hopefully it's kind of somewhat",
    "start": "2532060",
    "end": "2539080"
  },
  {
    "text": "right and that will, you know, be, you know, better than using the, the kind of the raw,",
    "start": "2539080",
    "end": "2545845"
  },
  {
    "text": "rollout value. Yeah, question. You, you would update your current estimate at the end of each episode, correct?",
    "start": "2545845",
    "end": "2554590"
  },
  {
    "text": "Uh, yeah. So the question is, would you update the current estimate, um, after each episode?",
    "start": "2554590",
    "end": "2561100"
  },
  {
    "text": "Yeah. So all of these algorithms, I haven't been explicit about it, is that you've seen an episode, you update,",
    "start": "2561100",
    "end": "2567760"
  },
  {
    "text": "uh, after you see it and then you get a new episode and so on. Yeah. Sometimes you would even update before you're done with the episode, uh.",
    "start": "2567760",
    "end": "2575710"
  },
  {
    "text": "[NOISE] Okay. So, uh, let me show this, uh,",
    "start": "2575710",
    "end": "2581559"
  },
  {
    "text": "what, um, this algorithm. So this is a new algorithm, it's called SARSA.",
    "start": "2581560",
    "end": "2590650"
  },
  {
    "start": "2584000",
    "end": "2733000"
  },
  {
    "text": "Does anyone know why it's called SARSA? [inaudible]. Oh, yeah, right. So if you look at this,",
    "start": "2590650",
    "end": "2597910"
  },
  {
    "text": "it's spelled SARSA and that's literally the reason why it's called SARSA. Uh, so what does this algorithm say?",
    "start": "2597910",
    "end": "2604270"
  },
  {
    "text": "So you're in a state s, you took action a, you got a reward, and then you ended up in state s prime and then you took another action a prime.",
    "start": "2604270",
    "end": "2611724"
  },
  {
    "text": "So for every kind of quintuple that you see, you're going to perform this update. Okay, so what is this update doing?",
    "start": "2611725",
    "end": "2618434"
  },
  {
    "text": "So this is the convex combination, uh, remember that we saw from before, um,",
    "start": "2618435",
    "end": "2624005"
  },
  {
    "text": "where you take a part of the old value and then you, uh, try to merge them with the new value.",
    "start": "2624005",
    "end": "2630505"
  },
  {
    "text": "So what is the new value here? This is looking at just the immediate reward, not the full utility,",
    "start": "2630505",
    "end": "2636460"
  },
  {
    "text": "just the immediate reward which is this 4 here and you're adding the discount which is 1 for now,",
    "start": "2636460",
    "end": "2641470"
  },
  {
    "text": "um, of your estimate. And remember, what is the estimate trying to do? Estimate is trying to be the expectation of rewards that you will get in the future.",
    "start": "2641470",
    "end": "2653545"
  },
  {
    "text": "So if this were actually a q pi and not a q pi hat, then this will actually just be strictly better because that would be,",
    "start": "2653545",
    "end": "2660625"
  },
  {
    "text": "uh, just reducing the variance. Uh, but, you know, of course this is not exactly right,",
    "start": "2660625",
    "end": "2666415"
  },
  {
    "text": "there's bias so it's 11, not 12 but the hope is that, you know, this is not biased by, you know, too much.",
    "start": "2666415",
    "end": "2674089"
  },
  {
    "text": "Okay? So these would be the kind of the, the values that you will be updating rather than these kind of raw values here.",
    "start": "2674580",
    "end": "2684380"
  },
  {
    "text": "Okay. So just to kind of compare them, well, okay. Okay, any questions about what SARSA is doing before we move on?",
    "start": "2685020",
    "end": "2695450"
  },
  {
    "text": "So maybe I'll write something to try to be helpful here.",
    "start": "2699700",
    "end": "2705589"
  },
  {
    "text": "So Q pi model-free Monte Carlo estimates Q pi based on u,",
    "start": "2705590",
    "end": "2711365"
  },
  {
    "text": "and SARSA is still Q pi hat, but it's based on reward plus,",
    "start": "2711365",
    "end": "2716450"
  },
  {
    "text": "uh, essentially Q pi hat. I mean this is not like a valid expression, but hopefully, it's some symbols that will evoke, uh,",
    "start": "2716450",
    "end": "2723890"
  },
  {
    "text": "the right memories, um, okay?",
    "start": "2723890",
    "end": "2730565"
  },
  {
    "text": "So let's discuss, um, the differences.",
    "start": "2730565",
    "end": "2735770"
  },
  {
    "start": "2733000",
    "end": "2823000"
  },
  {
    "text": "So this is- this- whenever people say, kind of, bootstrapping, um, in the context of reinforcement learning,",
    "start": "2735770",
    "end": "2741694"
  },
  {
    "text": "this is kinda what they mean, is that instead of using u as its prediction target,",
    "start": "2741695",
    "end": "2747605"
  },
  {
    "text": "you're using r plus Q pi, and this is kind of you're pulling up yourself from",
    "start": "2747605",
    "end": "2752630"
  },
  {
    "text": "your bootstraps because you're trying to estimate q pi, but you don't know q pi, but you're using Q pi to estimate it.",
    "start": "2752630",
    "end": "2758840"
  },
  {
    "text": "Okay. So u is based on one path, um, er, in SARSA,",
    "start": "2758840",
    "end": "2764930"
  },
  {
    "text": "you're based on the estimate which is based on all your previous kind of experiences, um,",
    "start": "2764930",
    "end": "2770720"
  },
  {
    "text": "which means that this is unbiased, uh, model for your Monte Carlo is biased,",
    "start": "2770720",
    "end": "2776480"
  },
  {
    "text": "but SARSA is biased. Monte Carlo has large variance. SARSA has, you know, smaller variance.",
    "start": "2776480",
    "end": "2783230"
  },
  {
    "text": "Um, and one, I guess, uh, consequence of the way the algorithm is set up is that model-free Monte Carlo,",
    "start": "2783230",
    "end": "2789724"
  },
  {
    "text": "you have to kind of roll out the entire game. Basically, play the game or the MDP until you reach the terminal state,",
    "start": "2789725",
    "end": "2797465"
  },
  {
    "text": "and then you can- now you have your u to update, whereas, uh, SARSA when- or any sort of bootstrapping algorithm,",
    "start": "2797465",
    "end": "2804365"
  },
  {
    "text": "you can just immediately update because all you need to do is you need to see, this is like a very local window of S-A-R-S-A,",
    "start": "2804365",
    "end": "2813140"
  },
  {
    "text": "and then you can just update, and that can happen, kind of, you know, anywhere. You don't have to wait until the very end to get the value.",
    "start": "2813140",
    "end": "2820410"
  },
  {
    "text": "Okay. So just as a quick sanity check. Um, which of the following algorithms allows you to estimate Q opt,",
    "start": "2822940",
    "end": "2834815"
  },
  {
    "start": "2823000",
    "end": "2934000"
  },
  {
    "text": "so model-based Monte Carlo, model-free Monte Carlo, or SARSA?",
    "start": "2834815",
    "end": "2841860"
  },
  {
    "text": "Okay. So I'll give you maybe ten seconds to ponder this.",
    "start": "2842020",
    "end": "2848090"
  },
  {
    "text": "[NOISE] Okay?",
    "start": "2848090",
    "end": "2857150"
  },
  {
    "text": "How many of you more- need more time?",
    "start": "2857150",
    "end": "2859500"
  },
  {
    "text": "Okay. Let's, uh, get a report. I think I didn't reset it from last year,",
    "start": "2865270",
    "end": "2871280"
  },
  {
    "text": "so this includes last year's, uh, participants. Um, so model-based Monte Carlo,",
    "start": "2871280",
    "end": "2877595"
  },
  {
    "text": "uh, allows you to get Q opt, right? Because once you have the MDP, you can get whatever you want. You can get Q opt.",
    "start": "2877595",
    "end": "2883835"
  },
  {
    "text": "Model-free Monte Carlo, um, estimates Q Pi; it doesn't estimate Q opt and, um,",
    "start": "2883835",
    "end": "2890690"
  },
  {
    "text": "SARSA also estimates Q Pi, but it doesn't estimate Q opt, okay?",
    "start": "2890690",
    "end": "2897300"
  },
  {
    "text": "All right. So, so that's, uh, kind of a problem. I mean, these algorithms are fine for, uh,",
    "start": "2897610",
    "end": "2906950"
  },
  {
    "text": "estimating the value of a policy, um, but you really want the optimal policy, right?",
    "start": "2906950",
    "end": "2914975"
  },
  {
    "text": "In fact, these can be used to improve the policy as well because you can, um, do something called policy improvement,",
    "start": "2914975",
    "end": "2922580"
  },
  {
    "text": "which I didn't talk about. Once you have the Q values, you can define a new policy based on the Q values.",
    "start": "2922580",
    "end": "2928235"
  },
  {
    "text": "Um, but there's actually a kind of a more direct way to do this, okay? So, so here's the kind of the way mental framework you should have in your head.",
    "start": "2928235",
    "end": "2937474"
  },
  {
    "start": "2934000",
    "end": "2989000"
  },
  {
    "text": "So there's two values: Q Pi and Q opt. So in MDPs, we saw that policy evaluation allows you to get Q Pi;",
    "start": "2937475",
    "end": "2945065"
  },
  {
    "text": "value iteration get- allows you to get Q opt. And now, we're doing reinforcement learning, and we saw model-free Monte Carlo and SARSA allow you to get Q Pi.",
    "start": "2945065",
    "end": "2954380"
  },
  {
    "text": "And now we need, I'm going to show you a new algorithm called Q-learning, that allows you to get Q opt.",
    "start": "2954380",
    "end": "2962040"
  },
  {
    "text": "So this gives you Q opt, and it's based on reward, uh,",
    "start": "2966280",
    "end": "2972185"
  },
  {
    "text": "plus, uh, Q opt, kind of. Okay. So this is going to be very similar to SARSA,",
    "start": "2972185",
    "end": "2981020"
  },
  {
    "text": "and it's only going to differ by, essentially, as you might guess, the same difference between policy evaluation and value iteration.",
    "start": "2981020",
    "end": "2989090"
  },
  {
    "start": "2989000",
    "end": "3374000"
  },
  {
    "text": "Okay. So it's helpful to go back to kind of the MDP recurrences.",
    "start": "2989090",
    "end": "2994610"
  },
  {
    "text": "So even though MDP recurrences can only apply when you know the MDP. For deriving reinforcement learning algorithms, um,",
    "start": "2994610",
    "end": "3001440"
  },
  {
    "text": "it's- they can kind of give you inspiration for the actual algorithm. Okay. So remember Q opt,",
    "start": "3001440",
    "end": "3008070"
  },
  {
    "text": "what is a Q opt? Q opt is considering all possible successors of probability immediate reward plus,",
    "start": "3008070",
    "end": "3014049"
  },
  {
    "text": "uh, future, um, returns. Okay. So the Q-learning is, it's actually a really kind of clever idea, um,",
    "start": "3014050",
    "end": "3021849"
  },
  {
    "text": "and it's- it could also be called SARS, SARS, I guess, um, but maybe you don't want to call it that,",
    "start": "3021850",
    "end": "3028490"
  },
  {
    "text": "and what it does is as follows. So this has the same form,",
    "start": "3028620",
    "end": "3034270"
  },
  {
    "text": "the convex combination of the old, uh, value, uh, and the new value, right?",
    "start": "3034270",
    "end": "3041380"
  },
  {
    "text": "So what is the new value? Um, so if you look at Q opt,",
    "start": "3041380",
    "end": "3047080"
  },
  {
    "text": "Q opt is looking at different successors reward plus V opt.",
    "start": "3047080",
    "end": "3052630"
  },
  {
    "text": "What we're gonna do is, well, we don't have all- we're not gonna be able to sum over all our successors because we're in our reinforcement learning setting,",
    "start": "3052630",
    "end": "3059680"
  },
  {
    "text": "and we only saw one particular successor. So let's just use that as a successor. So on that successor,",
    "start": "3059680",
    "end": "3065200"
  },
  {
    "text": "we're going to get the reward. So R is a stand-in for the actual reward of, I mean,",
    "start": "3065200",
    "end": "3071035"
  },
  {
    "text": "is the stand-in for the reward, the reward function, and then you have Gamma times.",
    "start": "3071035",
    "end": "3077170"
  },
  {
    "text": "And then V opt, I am going to replace it with, uh, the, our estimate of what V opt is,",
    "start": "3077170",
    "end": "3084955"
  },
  {
    "text": "and what should the estimate of V opt be?",
    "start": "3084955",
    "end": "3088370"
  },
  {
    "text": "So what relates V opt to Q opt? Yeah?",
    "start": "3092640",
    "end": "3098740"
  },
  {
    "text": "I think the a that maximizes Q opt but [inaudible] V opt. Yeah. Exactly. So if you,",
    "start": "3098740",
    "end": "3105760"
  },
  {
    "text": "define V opt to be the max over all possible actions of Q opt of s in that particular action,",
    "start": "3105760",
    "end": "3112795"
  },
  {
    "text": "then this is V opt, right? So Q is saying, I'm at a chance node, um,",
    "start": "3112795",
    "end": "3119424"
  },
  {
    "text": "how much, what is the optimal utility I can get provided I took an action? Clearly, the best thing to do if you're at",
    "start": "3119425",
    "end": "3126250"
  },
  {
    "text": "a state is just choose the action that gives you the maximum of Q value that you get into, okay?",
    "start": "3126250",
    "end": "3133750"
  },
  {
    "text": "So that's just Q-learning, so let's put it side-by-side with SARSA. Okay. So SARSA, these two are very similar, right?",
    "start": "3133750",
    "end": "3144099"
  },
  {
    "text": "So SARSA, remember updates against r plus Q Pi? And now we're updating against r plus this max over Q opt, okay?",
    "start": "3144100",
    "end": "3157704"
  },
  {
    "text": "And you can see that SARSA requires knowing what action I'm gonna take next,",
    "start": "3157705",
    "end": "3163165"
  },
  {
    "text": "um, kind of a one-step look ahead, a prime and that plugs i- into here, whereas Q-learning,",
    "start": "3163165",
    "end": "3169300"
  },
  {
    "text": "it doesn't matter what a you took because I'm just gonna take the one that maximizes, right?",
    "start": "3169300",
    "end": "3176470"
  },
  {
    "text": "So you can see why SARSA is estimating the value of policy because, you know, what a prime, uh,",
    "start": "3176470",
    "end": "3182785"
  },
  {
    "text": "shows up here is a function of a policy. And here, um, I'm kind of",
    "start": "3182785",
    "end": "3188185"
  },
  {
    "text": "insulated from that because I'm just taking the maximum over all actions. This is the same intuition as for value iteration versus policy evaluation, okay?",
    "start": "3188185",
    "end": "3200575"
  },
  {
    "text": "I'll pause here. Any questions? Q-learning versus SARSA.",
    "start": "3200575",
    "end": "3204950"
  },
  {
    "text": "So is Q-learning on-policy or off-policy?",
    "start": "3206430",
    "end": "3210740"
  },
  {
    "text": "It's off-policy because I'm following whatever policy I'm following, and I get to estimate the value of",
    "start": "3212220",
    "end": "3219970"
  },
  {
    "text": "the optimal policy which is probably not the one I'm following, at least, in the beginning.",
    "start": "3219970",
    "end": "3224630"
  },
  {
    "text": "Okay. So let's look at the example here. So here's SARSA and run it for 1,000 iterations.",
    "start": "3225390",
    "end": "3232165"
  },
  {
    "text": "And like model-free Monte Carlo, um, this, um, I'm estimated that an average- the average utility I'm getting is minus 20,",
    "start": "3232165",
    "end": "3241645"
  },
  {
    "text": "and in particular, the values I'm getting are all very negative because this is Q Pi. This is a policy I'm following,",
    "start": "3241645",
    "end": "3248200"
  },
  {
    "text": "which is the random policy. Um, if I replace this with q, what happens?",
    "start": "3248200",
    "end": "3255460"
  },
  {
    "text": "So first, notice that the average utility is still minus 19 because I actually haven't changed my exploration policy.",
    "start": "3255460",
    "end": "3263050"
  },
  {
    "text": "I'm still doing random exploration. Um, well, yeah. I'm still doing random exploration.",
    "start": "3263050",
    "end": "3269695"
  },
  {
    "text": "But notice that the value, the Q opt values are all around,",
    "start": "3269695",
    "end": "3275055"
  },
  {
    "text": "you know, 20, right? And this is because the optimum policy, remember, is just to- and this is,",
    "start": "3275055",
    "end": "3280740"
  },
  {
    "text": "uh, slip probability is 0. So optimal policy is just to go down here and get your 20, okay?",
    "start": "3280740",
    "end": "3286690"
  },
  {
    "text": "And Q- and I- I guess it's kind of interesting that Q-learning, I'm just blindly following the policy running, you know, off,",
    "start": "3286690",
    "end": "3294370"
  },
  {
    "text": "off the cliff into the volcano all the time but, you know, I'm learning something, and I'm learning how to behave optimally,",
    "start": "3294370",
    "end": "3302635"
  },
  {
    "text": "even though I'm not behaving optimally, and that's, uh, the kind of hallmark of off-policy learning.",
    "start": "3302635",
    "end": "3309590"
  },
  {
    "text": "Okay. So, any questions about these four algorithms?",
    "start": "3313220",
    "end": "3318970"
  },
  {
    "text": "So model-based Monte Carlo, estimate MDP, model-free Monte Carlo, um, estimate, ah,",
    "start": "3319340",
    "end": "3326204"
  },
  {
    "text": "the Q value of this policy based on, um, the actual returns that you get,",
    "start": "3326205",
    "end": "3331680"
  },
  {
    "text": "the actual sum of the, ah, rewards. SARSA is bootstrapping estimating the same thing but with kind of a one-step look ahead.",
    "start": "3331680",
    "end": "3340125"
  },
  {
    "text": "And Q learning is like SARSA except for I'm estimating the optimal instead of, um, fixed policy Pi. Yeah.",
    "start": "3340125",
    "end": "3348060"
  },
  {
    "text": "Is SARSA on-policy or off policy? SARSA is on-policy because I'm estimating Q Pi. All right.",
    "start": "3348060",
    "end": "3359655"
  },
  {
    "text": "Okay so now let's talk about encountering the unknown. So these are the algorithms.",
    "start": "3359655",
    "end": "3365115"
  },
  {
    "text": "So at this point if I just hand you some data, um, if I told you here's a fixed policy,",
    "start": "3365115",
    "end": "3370484"
  },
  {
    "text": "here's some data, you can actually estimate all these quantities. Um, but now there's a question of exploration which we saw was really important,",
    "start": "3370485",
    "end": "3379500"
  },
  {
    "start": "3374000",
    "end": "3556000"
  },
  {
    "text": "because if you don't even, even see all the states, how can you possibly act optimally?",
    "start": "3379500",
    "end": "3385005"
  },
  {
    "text": "So, um, so which exploration policy should you use?",
    "start": "3385005",
    "end": "3390285"
  },
  {
    "text": "So here are kind of two extremes. So the first extreme is, um,",
    "start": "3390285",
    "end": "3395520"
  },
  {
    "text": "let's just set the exploration policy. So, so imagine we're doing Q learning now.",
    "start": "3395520",
    "end": "3401490"
  },
  {
    "text": "So you have this Q_opt estimate. So it's not a true Q_opt but you have an estimate of Q_opt.",
    "start": "3401490",
    "end": "3407010"
  },
  {
    "text": "Um, the naive thing to do is just take a- use that Q_opt, figure out which action is best and just always do that action.",
    "start": "3407010",
    "end": "3415619"
  },
  {
    "text": "Okay. So what happens when you do this is, um, you, ah, don't do very well.",
    "start": "3415620",
    "end": "3425160"
  },
  {
    "text": "So why don't you do very well? Because initially while you explore randomly and soon you find the 2.",
    "start": "3425160",
    "end": "3434760"
  },
  {
    "text": "And once you've found that 2, you say, \"Ah, well, 2 is better than 0, 0, 0. So I'm just gonna keep on going down to the 2 which is you know,",
    "start": "3434760",
    "end": "3442319"
  },
  {
    "text": "all exploitation, no exploration. Right? You don't realize you that there's all this other stuff over here.",
    "start": "3442320",
    "end": "3451170"
  },
  {
    "text": "Um, so in the other direction, we have no exploitation, all exploration.",
    "start": "3451170",
    "end": "3459420"
  },
  {
    "text": "Um, here, ah, you kind of have the opposite setup where I'm,",
    "start": "3459420",
    "end": "3466680"
  },
  {
    "text": "I'm running Q learning, right? So as we saw before, I'm actually able to estimate the, uh, the, the Q_opt values.",
    "start": "3466680",
    "end": "3473505"
  },
  {
    "text": "So I learn a lot. But the average utility which is the actual utility I'm getting by playing this game is pretty bad.",
    "start": "3473505",
    "end": "3482595"
  },
  {
    "text": "In particular, it's the, the utility you get from just, you know, moving randomly.",
    "start": "3482595",
    "end": "3488710"
  },
  {
    "text": "So kinda what you really want to do is, uh, balance you know, exploration and exploitation.",
    "start": "3488930",
    "end": "3496349"
  },
  {
    "text": "So just kind of a, kind of an aside or a commentary is that I really feel reinforcement learning kind of captures,",
    "start": "3496350",
    "end": "3504764"
  },
  {
    "text": "ah, life pretty well. Um, uh, because in life there's, you know, you don't know what's going on.",
    "start": "3504764",
    "end": "3511665"
  },
  {
    "text": "Um, you want to get rewards, you know, you want to do well. Um, and, ah, but at the same,",
    "start": "3511665",
    "end": "3518130"
  },
  {
    "text": "time you have to, um, kind of learn about how the world works so that you can kind of improve your policy.",
    "start": "3518130",
    "end": "3524910"
  },
  {
    "text": "So if you think about going to in restaurants or finding the shortest path better way to get to, um,",
    "start": "3524910",
    "end": "3530880"
  },
  {
    "text": "to school or to work, or in research even when you are trying to figure out, um,",
    "start": "3530880",
    "end": "3536670"
  },
  {
    "text": "a problem you can work on the thing that you know how to do and will definitely work or, you know,",
    "start": "3536670",
    "end": "3542265"
  },
  {
    "text": "do you try to do something new in hopes of you learning something but maybe it won't get you as high reward.",
    "start": "3542265",
    "end": "3548130"
  },
  {
    "text": "So, um, hopefully reinforcement learning is, um, I know, it's kind of a metaphor for life in the US.",
    "start": "3548130",
    "end": "3554640"
  },
  {
    "text": "Um, okay so, ah,",
    "start": "3554640",
    "end": "3560085"
  },
  {
    "start": "3556000",
    "end": "3599000"
  },
  {
    "text": "back to concrete stuff. Um, so here's one way you can balance,",
    "start": "3560085",
    "end": "3565365"
  },
  {
    "text": "um, exploration and exploitation, right? So it's called the Epsilon-greedy policy.",
    "start": "3565365",
    "end": "3570450"
  },
  {
    "text": "And this assumes that you're doing something like Q learning. So you have these Q_opt values and ideas that, you know,",
    "start": "3570450",
    "end": "3577110"
  },
  {
    "text": "with probability of 1 minus Epsilon where Epsilon is, you know, let's say like 0.1, you're usually gonna give exploit.",
    "start": "3577110",
    "end": "3584580"
  },
  {
    "text": "We're just gonna do, give you- give it all you have. Um, and then, um,",
    "start": "3584580",
    "end": "3591089"
  },
  {
    "text": "once in a while, you're also gonna do something random. Okay. So this is actually not a bad policy to act in life.",
    "start": "3591090",
    "end": "3597839"
  },
  {
    "text": "So once in a while, maybe you should just do something random and kind of see what happens. Um, so if you do this,",
    "start": "3597840",
    "end": "3603480"
  },
  {
    "text": "um, what, what do you get? Okay, so what I've done here is, uh,",
    "start": "3603480",
    "end": "3610320"
  },
  {
    "text": "I've set Epsilon to be starting with one. So one is, ah, all exploration.",
    "start": "3610320",
    "end": "3617730"
  },
  {
    "text": "And then I'm going to change the value, ah, a third of the way into 0.5.",
    "start": "3617730",
    "end": "3622740"
  },
  {
    "text": "And then I'm gonna, two-thirds the way I'm gonna change it to 0. Okay. So if I do this then I actually estimate the values,",
    "start": "3622740",
    "end": "3631109"
  },
  {
    "text": "ah, really really well. Um, and also I get utility which is, you know,",
    "start": "3631110",
    "end": "3636839"
  },
  {
    "text": "pretty good, you know 32. Um, okay.",
    "start": "3636840",
    "end": "3642750"
  },
  {
    "text": "And this is also kind of something that happens, uh, as you get older,",
    "start": "3642750",
    "end": "3647760"
  },
  {
    "text": "you tend to, um, [NOISE] explore less and exploit more. Um, it just happens.",
    "start": "3647760",
    "end": "3653685"
  },
  {
    "text": "Um, okay. All right. So that was exploration. So let's put some stuff on the board here.",
    "start": "3653685",
    "end": "3661935"
  },
  {
    "text": "Um, do I need this anymore? Maybe [NOISE]. Okay. Um, okay.",
    "start": "3661935",
    "end": "3672240"
  },
  {
    "text": "So covering the unknown, so we talked about, you know, exploration, um, you know, Epsilon-greedy.",
    "start": "3672240",
    "end": "3681790"
  },
  {
    "text": "Um, and there's other ways to do this. Um, Epsilon-greedy is just kind of the simplest thing that actually, you know,",
    "start": "3681860",
    "end": "3688980"
  },
  {
    "text": "works remarkably, you know, well, um, even in the stabilized systems.",
    "start": "3688980",
    "end": "3694065"
  },
  {
    "text": "So the other problem now I'm gonna talk about is, you know, generalization.",
    "start": "3694065",
    "end": "3700545"
  },
  {
    "text": "Uh, so remember when we say exploration. Well, if you don't see a particular state, then you don't know what to do with this.",
    "start": "3700545",
    "end": "3707130"
  },
  {
    "text": "I mean you think about it for a moment, that's kind of unreasonable because, you know, in life you're never gonna be in the exact same, you know, situation.",
    "start": "3707130",
    "end": "3715035"
  },
  {
    "text": "And yet we are [NOISE] we need to be able to act properly right. So general problem is that a state-space that you,",
    "start": "3715035",
    "end": "3722205"
  },
  {
    "text": "you might deal with in a kind of a real, ah, world situation is enormous. And there's no way you're going to go and track down every possible state.",
    "start": "3722205",
    "end": "3730200"
  },
  {
    "text": "Okay. So this state space is actually not that enormous, um, but this is the biggest state space I could draw on the- on the screen.",
    "start": "3730200",
    "end": "3737460"
  },
  {
    "text": "Um, and you can see that this, you know, the average utility is, you know, pretty bad here. Okay. So what can we do about this?",
    "start": "3737460",
    "end": "3745155"
  },
  {
    "text": "So, um, I guess let's talk about a large state space.",
    "start": "3745155",
    "end": "3750910"
  },
  {
    "text": "So this is the problem. So now this is where",
    "start": "3751040",
    "end": "3757664"
  },
  {
    "text": "the second- the third interpretation of model-free Monte Carlo will come in handy.",
    "start": "3757665",
    "end": "3762870"
  },
  {
    "text": "So let's take a look at Q learning. Okay. So in the context of,",
    "start": "3762870",
    "end": "3769050"
  },
  {
    "text": "ah, SGD, looks like this. Right. So it's a kind of a gradient step where you take the old value",
    "start": "3769050",
    "end": "3776040"
  },
  {
    "text": "and you minus eta and something that kind of looks like, ah, it could be a gradient,",
    "start": "3776040",
    "end": "3781050"
  },
  {
    "text": "which is the residual here. Um, so one thing to note is that under the,",
    "start": "3781050",
    "end": "3788745"
  },
  {
    "text": "the kind of formulations of Q learning that I've talked about so far, this is what we call a kind of rote learning.",
    "start": "3788745",
    "end": "3796065"
  },
  {
    "text": "Right. Um, which if we were, you know, two weeks ago, we already said this is,",
    "start": "3796065",
    "end": "3801345"
  },
  {
    "text": "you know, kind of ridiculous because it's, uh, not really learning or generalizing at all.",
    "start": "3801345",
    "end": "3806955"
  },
  {
    "text": "Um, right now it's basically for every single state and action I have a value. If I have a different state and action, completely different value.",
    "start": "3806955",
    "end": "3814680"
  },
  {
    "text": "I don't- I don't- there's no kind of, ah, sharing of information. And naturally, if I do that, I can't generalize between states and actions. Um, okay.",
    "start": "3814680",
    "end": "3823620"
  },
  {
    "text": "So here's the key idea that will allow us to, um, actually overcome this. So it's called function approximation in the context of reinforcement learning.",
    "start": "3823620",
    "end": "3833385"
  },
  {
    "text": "Uh, in normal machine learning, it's just called normal machine learning. Um, so the way it works is this,",
    "start": "3833385",
    "end": "3841770"
  },
  {
    "text": "uh, so we're going to define this Q_opt s, a.",
    "start": "3841770",
    "end": "3847035"
  },
  {
    "text": "It's not going to be a lookup table, it's going to depend on some parameters here w. And I'm gonna define",
    "start": "3847035",
    "end": "3852840"
  },
  {
    "text": "this function to be w dot Phi s, a. Okay. So I'm gonna define this feature vector very similar to how we did it",
    "start": "3852840",
    "end": "3861810"
  },
  {
    "text": "in kind of machine- in the machine learning section except for instead of s, a we had x. And now the weights are going to be kind of, you know, the same.",
    "start": "3861810",
    "end": "3870160"
  },
  {
    "text": "Okay. So what kind of features might you have? Ah, you might have for example, um, features on, you know, actions.",
    "start": "3870160",
    "end": "3877190"
  },
  {
    "text": "So these are indicator features that say, \"Hey, maybe it's better to go east then to go west or maybe it's better to be in the fifth,",
    "start": "3877190",
    "end": "3885390"
  },
  {
    "text": "ah, row or as it's good to be in a six column and, you know, things like that.\" So, um, you have a smaller set of features and you try to",
    "start": "3885390",
    "end": "3894150"
  },
  {
    "text": "use that to kind of generalize across all the different states that you might see.",
    "start": "3894150",
    "end": "3899160"
  },
  {
    "text": "So what this looks like is now with the features is",
    "start": "3899160",
    "end": "3904980"
  },
  {
    "text": "actually the same as before except for,",
    "start": "3904980",
    "end": "3910564"
  },
  {
    "text": "um, now we have something that really looks like, uh, you know, the machine learning lectures,",
    "start": "3910564",
    "end": "3915815"
  },
  {
    "text": "is that you take your weight vector and you do, um, an update of the residual times the feature vector.",
    "start": "3915815",
    "end": "3926175"
  },
  {
    "text": "Okay. So how many of you this looks familiar from linear regression?",
    "start": "3926175",
    "end": "3933180"
  },
  {
    "text": "Okay. All right. So, so just to contrast, so before we were just updating the Q_opt values,",
    "start": "3933180",
    "end": "3940920"
  },
  {
    "text": "um, but the residual is exactly the same and there's nothing over here. And now what we're doing is we're updating not the Q values,",
    "start": "3940920",
    "end": "3948030"
  },
  {
    "text": "we're updating the weights. The residual is the same and the thing that connects the, the,",
    "start": "3948030",
    "end": "3954605"
  },
  {
    "text": "the Q values with the, the residual width, the, the weights is, ah, the kind of the feature vector.",
    "start": "3954605",
    "end": "3960694"
  },
  {
    "text": "Okay. As a sanity check, this has the same dimension. This is a vector. This is a scalar.",
    "start": "3960695",
    "end": "3966240"
  },
  {
    "text": "This is a vector which has the same dimensionality s, a; w. Okay.",
    "start": "3966240",
    "end": "3971850"
  },
  {
    "text": "And if you want to derive this, um, you can actually think about the implied objective function as,",
    "start": "3971850",
    "end": "3978690"
  },
  {
    "text": "ah, simply, you know, linear regression. You have a model that's trying to predict a value,",
    "start": "3978690",
    "end": "3985530"
  },
  {
    "text": "um, from an input, um, s, a. So s, a is like x and Q_opt is like kind of y.",
    "start": "3985530",
    "end": "3993000"
  },
  {
    "text": "And then your regre- sorry. This target is like, uh, the y that you're trying to predict and you're just trying to make this prediction close",
    "start": "3993000",
    "end": "4001069"
  },
  {
    "text": "to the target. Yeah, question.",
    "start": "4001070",
    "end": "4007470"
  },
  {
    "text": "Is the eta, you said that [inaudible] [NOISE] Yeah. So a good question. So what is this eta now?",
    "start": "4009040",
    "end": "4015060"
  },
  {
    "text": "Uh, is it the same as before? So when we first started talking about these algorithms,",
    "start": "4015060",
    "end": "4021464"
  },
  {
    "text": "eta was supposed to be one over the number of updates and so on. But once you get into the SGD form like this",
    "start": "4021465",
    "end": "4030105"
  },
  {
    "text": "then now this just behaves as a step size and you can tune it to your heart's content.",
    "start": "4030105",
    "end": "4035260"
  },
  {
    "text": "All right. So that's all I will say about these two challenges.",
    "start": "4037310",
    "end": "4044925"
  },
  {
    "text": "One is how do you do exploration? You can use Epsilon-greedy which allows you to kind of balance exploration with",
    "start": "4044925",
    "end": "4053609"
  },
  {
    "text": "exploitation and then the second thing is that for large state spaces,",
    "start": "4053610",
    "end": "4059070"
  },
  {
    "text": "Epsilon-greedy isn't going to cut it because you're not going to see all the states even if you try really",
    "start": "4059070",
    "end": "4065070"
  },
  {
    "text": "hard and you need something like function approximation to tell you about new states that you fundamentally haven't seen before.",
    "start": "4065070",
    "end": "4072220"
  },
  {
    "text": "Okay. So summary so far, online learning. We're in an online setting.",
    "start": "4073370",
    "end": "4079079"
  },
  {
    "text": "This is the game of reinforcement learning. You have to learn and take actions in real world. One of the key challenges is the exploration-exploitation trade-off.",
    "start": "4079080",
    "end": "4087974"
  },
  {
    "text": "We saw, um, four algorithms, there's kind of two key ideas here.",
    "start": "4087975",
    "end": "4093930"
  },
  {
    "text": "One is Monte Carlo which is that from data alone, you can basically use averages to estimate",
    "start": "4093930",
    "end": "4100500"
  },
  {
    "text": "quantities that you care about, for example, transitions, rewards, and Q values. And the second key idea is this bootstrapping which shows",
    "start": "4100500",
    "end": "4107250"
  },
  {
    "text": "up in SARSA and Q-learning which is that you're updating towards a target that depends on your estimate of what you're trying to predict.",
    "start": "4107250",
    "end": "4117015"
  },
  {
    "text": "Um, not just the kind of raw data that you see.",
    "start": "4117015",
    "end": "4121660"
  },
  {
    "text": "Okay. So now I'm gonna maybe step back a little bit and talk about reinforcement learning in the context of some kinda other things.",
    "start": "4123140",
    "end": "4132810"
  },
  {
    "text": "So there's kind of two things that happen when we went from",
    "start": "4132810",
    "end": "4138330"
  },
  {
    "text": "binary classification which was two weeks ago to reinforcement learning now and it's worth kind of decoupling these two things.",
    "start": "4138330",
    "end": "4145605"
  },
  {
    "text": "One is state and one is feedback. So the idea about partial feedback is that you can only learn about actions you take.",
    "start": "4145605",
    "end": "4156884"
  },
  {
    "text": "Right. I mean this is kinda obvious in reinforcement learning. If you don't, don't, quit in this game,",
    "start": "4156885",
    "end": "4163444"
  },
  {
    "text": "you never know how much money you'll get.",
    "start": "4163445",
    "end": "4168484"
  },
  {
    "text": "And the other idea is the notion of state which is that new rewards depend on your previous actions.",
    "start": "4168485",
    "end": "4179040"
  },
  {
    "text": "So if you're going through a volcano, you have to, ah, there's a kind of a different situation depending on where you are in, in the map.",
    "start": "4179040",
    "end": "4188580"
  },
  {
    "text": "Um, and there's actually kind of- so, so this is kind of you can draw a two-by-two grid where you go",
    "start": "4188580",
    "end": "4195420"
  },
  {
    "text": "from supervised learning which is stateless and full feedback. So there is no state, every iteration you just get a new example, ah,",
    "start": "4195420",
    "end": "4203550"
  },
  {
    "text": "and that doesn't have, you know, there's no dependency and in terms of prediction on the previous examples.",
    "start": "4203550",
    "end": "4209489"
  },
  {
    "text": "Um, and full feedback in because in supervised learning,",
    "start": "4209490",
    "end": "4215040"
  },
  {
    "text": "you're told which is the correct label. Even if there might be 1,000 labels for example in image classification,",
    "start": "4215040",
    "end": "4222570"
  },
  {
    "text": "you're just told which ones are the correct label. Ah, and now in reinforcement learning, both of those are made harder.",
    "start": "4222570",
    "end": "4229650"
  },
  {
    "text": "There is two other interesting points. So what is called multi-armed bandits is kind of a,",
    "start": "4229650",
    "end": "4236760"
  },
  {
    "text": "you can think about as a warm up to reinforcement learning where there's partial feedback, but there's no state which makes it easier.",
    "start": "4236760",
    "end": "4243225"
  },
  {
    "text": "And there's also, you can get full feedback but there are states. So instruction prediction. For example in machine translation,",
    "start": "4243225",
    "end": "4249570"
  },
  {
    "text": "you're told what the translation output should be, but clearly though actions depend on previous actions because,",
    "start": "4249570",
    "end": "4258795"
  },
  {
    "text": "you know, you can't just translate words in isolation essentially.",
    "start": "4258795",
    "end": "4265030"
  },
  {
    "text": "Um, okay, So one of the things I'll just mention very briefly is, you know, this is deep reinforcement learning has been very popular in recent years.",
    "start": "4265310",
    "end": "4276150"
  },
  {
    "text": "So reinforcement learning, there was kind of a lot of interest in the kind of '90s where a lot of the algorithms were kind of,",
    "start": "4276150",
    "end": "4283469"
  },
  {
    "text": "ah, in theory were kind of developed. And then there was a period where kind of not that much, not as much",
    "start": "4283470",
    "end": "4289829"
  },
  {
    "text": "happened and since I guess 2013, there has been a revival of reinforcement of research.",
    "start": "4289830",
    "end": "4297270"
  },
  {
    "text": "A lot of it's due to I guess at the DeepMind where they",
    "start": "4297270",
    "end": "4303035"
  },
  {
    "text": "published a paper showing how they can do- use raw reinforced learning to play Atari.",
    "start": "4303035",
    "end": "4309270"
  },
  {
    "text": "So this will be talked about more in a section this Friday. But the basic idea of deep reinforcement learning just to",
    "start": "4309270",
    "end": "4317490"
  },
  {
    "text": "kind of demystify things is that you are using a neural network for Q_opt. Essentially that's what it is.",
    "start": "4317490",
    "end": "4324660"
  },
  {
    "text": "And there's also a lot of tricks to make",
    "start": "4324660",
    "end": "4329810"
  },
  {
    "text": "this kind of work which are necessary when you're dealing with enormous state spaces.",
    "start": "4329810",
    "end": "4335030"
  },
  {
    "text": "So one of the things that's different about deep reinforcement learning is that people are much more ambitious about handling problems where the state spaces are kinda enormous.",
    "start": "4335030",
    "end": "4342824"
  },
  {
    "text": "So for this, the state is just the, you know, the pixels, right, so there's, you know, a huge number of pixels and whereas before people were kind",
    "start": "4342825",
    "end": "4352890"
  },
  {
    "text": "of in what is known as a tabular case which the number of states you can kind of enumerate.",
    "start": "4352890",
    "end": "4358784"
  },
  {
    "text": "So, um, there's a lot of details here to care about. One general comment is that reinforcement learning is, it's really hard,",
    "start": "4358785",
    "end": "4369330"
  },
  {
    "text": "right, because of the statefulness and also the delayed feedback. So just when you're maybe thinking about final projects, I mean,",
    "start": "4369330",
    "end": "4377070"
  },
  {
    "text": "it's a really cool area, but don't underestimate how much work and compute you need to do.",
    "start": "4377070",
    "end": "4383219"
  },
  {
    "text": "Some other things I won't have time to talk about is so far we've talked about methods that are trying to estimate the Q function.",
    "start": "4383220",
    "end": "4390990"
  },
  {
    "text": "There's also a way to even do without the Q function and just try to estimate the policy directly that's called,",
    "start": "4390990",
    "end": "4397290"
  },
  {
    "text": "um, methods like policy gradient. There's also methods like actor critic that try to",
    "start": "4397290",
    "end": "4402570"
  },
  {
    "text": "combine of these value based methods and policy-based methods. These are used in DeepMind's",
    "start": "4402570",
    "end": "4411495"
  },
  {
    "text": "AlphaGo, and AlphaZero programs for crushing humans at Go.",
    "start": "4411495",
    "end": "4418290"
  },
  {
    "text": "This will actually will be deferred to next week's section because this is in the context of games.",
    "start": "4418290",
    "end": "4425565"
  },
  {
    "text": "There's a bunch of other applications. You can fly helicopters, play backgammon, this is actually one of the early examples TD-Gammon was one of the early examples in",
    "start": "4425565",
    "end": "4434460"
  },
  {
    "text": "the early '90s of kind of one of the success stories of using reinforcement learning in particular,",
    "start": "4434460",
    "end": "4440475"
  },
  {
    "text": "you know, self play. For non-games, reinforcement learning can be used to kind of do",
    "start": "4440475",
    "end": "4446565"
  },
  {
    "text": "elevator scheduling and managing data centers and so on. Okay. So that concludes this section on",
    "start": "4446565",
    "end": "4454140"
  },
  {
    "text": "Markov decision processes which we- the idea is we are playing against nature.",
    "start": "4454140",
    "end": "4460290"
  },
  {
    "text": "So nature is kinda random but kind of neutral. Next time, we're going to play against an opponent where they're out to get us.",
    "start": "4460290",
    "end": "4469650"
  },
  {
    "text": "So we'll see about that.",
    "start": "4469650",
    "end": "4472030"
  }
]