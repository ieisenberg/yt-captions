[
  {
    "text": "Okay. Welcome back everyone [NOISE] to Lecture 12 of CS229.",
    "start": "4130",
    "end": "9540"
  },
  {
    "text": "The topics for today are bias-variance trade-off,",
    "start": "9540",
    "end": "14745"
  },
  {
    "text": "model selection and cross-validation, and regularization. The three are- are kind of somewhat related to each other.",
    "start": "14745",
    "end": "22110"
  },
  {
    "text": "And I would say bias-variance trade-off is probably one of the most important,",
    "start": "22110",
    "end": "28650"
  },
  {
    "text": "uh, topics that you need to take away from this course. It- it- it's at the heart of machine learning and the concept that kind of,",
    "start": "28650",
    "end": "35700"
  },
  {
    "text": "uh, is unique to machine learning and distinguishes it from, uh, other fields. All right. So a quick recap of what we covered,",
    "start": "35700",
    "end": "42375"
  },
  {
    "text": "uh, over the last, uh, two classes. So the last two classes we covered neural networks and deep learning.",
    "start": "42375",
    "end": "48290"
  },
  {
    "text": "And the- the main takeaways from the last, uh, two classes was the neural networks are basically a composition of simple building blocks,",
    "start": "48290",
    "end": "58850"
  },
  {
    "text": "where the simple build- building blocks are non-linear models that we've, uh, seen in the past.",
    "start": "58850",
    "end": "64070"
  },
  {
    "text": "So we- we, um, we take the output of one- one, um, simple model and feed that as input to,",
    "start": "64070",
    "end": "71414"
  },
  {
    "text": "you know, another simple model and so on. And the crucial, um,",
    "start": "71415",
    "end": "77039"
  },
  {
    "text": "the crucial thing while composing them and composing them is having non-linearities, right?",
    "start": "77040",
    "end": "82380"
  },
  {
    "text": "And non-linearities are crucial, uh, because if there were no non-linearities then the entire network can-",
    "start": "82380",
    "end": "88060"
  },
  {
    "text": "could be represented as just a single linear, uh, single linear layer with one, uh, one matrix.",
    "start": "88060",
    "end": "95020"
  },
  {
    "text": "And also, um, neural networks and deep learning, they are- they are,",
    "start": "95020",
    "end": "100395"
  },
  {
    "text": "um, non-convex, and it's important that we initialize the parameters randomly.",
    "start": "100395",
    "end": "106789"
  },
  {
    "text": "Whereas in the previous models that we saw, the simpler models, they were mostly convex and initialization did not matter.",
    "start": "106790",
    "end": "114905"
  },
  {
    "text": "But, uh, for neural networks, for example, initializing all your parameters to zero will not work, uh,",
    "start": "114905",
    "end": "121985"
  },
  {
    "text": "because, uh, because of the, uh, symmetry, uh, symmetry properties that we, uh, uh, that we discussed.",
    "start": "121985",
    "end": "128565"
  },
  {
    "text": "And then, uh, the approach for training neural networks was basically backpropagation.",
    "start": "128565",
    "end": "134030"
  },
  {
    "text": "Backpropagation is just a fancy name for the chain rule of multivariate calculus. And what we saw was, um,",
    "start": "134030",
    "end": "141765"
  },
  {
    "text": "we- we calculate the gradients of the final loss with respect to every parameter at all layers.",
    "start": "141765",
    "end": "148280"
  },
  {
    "text": "And in order to calculate those, um, those, uh, gradients, we use the multivariate, um, chain rule.",
    "start": "148280",
    "end": "155530"
  },
  {
    "text": "So the last layer was, you know, a- a linear- a linear model that we've, uh, uh, you know, very similar to GLMs.",
    "start": "155530",
    "end": "162695"
  },
  {
    "text": "And then from- from, uh, beyond the last layer as we- as we compute",
    "start": "162695",
    "end": "167840"
  },
  {
    "text": "the gradients of parameters from earlier layers, we see- we saw that you- we encounter a- a chain of very simple Jacobians.",
    "start": "167840",
    "end": "176030"
  },
  {
    "text": "They're either diagonal, uh, Jacobians where each diagonal entry is the derivative of the non-linearity,",
    "start": "176030",
    "end": "182590"
  },
  {
    "text": "or it's going to be the weight matrix. So the weight matrix itself will be, uh, the Jacobian.",
    "start": "182590",
    "end": "187665"
  },
  {
    "text": "Right? And, um, we- we- we construct the simple chain of",
    "start": "187665",
    "end": "193489"
  },
  {
    "text": "Jacobians until we arrive at the layer at which we are calculating the gradient from.",
    "start": "193490",
    "end": "199130"
  },
  {
    "text": "And then from- from the, um, the branch that's specific to that layer, uh,",
    "start": "199130",
    "end": "204599"
  },
  {
    "text": "it was pretty- pretty straightforward calculus. Uh, that- the notation was a little heavy, but, um,",
    "start": "204600",
    "end": "211800"
  },
  {
    "text": "it's- it's pretty much, you know, there's nothing- nothing, uh, fancy going on there. It's just simple- simple, uh,",
    "start": "211800",
    "end": "217935"
  },
  {
    "text": "gradients that we need to, uh, that we need to take. And to the end of,",
    "start": "217935",
    "end": "224310"
  },
  {
    "text": "uh, of, uh, the last lecture, we saw this, um, or we just discussed this theorem which is, you know,",
    "start": "224310",
    "end": "230959"
  },
  {
    "text": "it's not gonna be in your- in your, um, uh, syllabus and we're not gonna test you on that theorem, uh,",
    "start": "230960",
    "end": "236310"
  },
  {
    "text": "but it's a- it's a good kind of segue into bias-variance, uh, because the universal approximation theorem",
    "start": "236310",
    "end": "243890"
  },
  {
    "text": "suggests that neural networks can be very expressive. So it can represent any possible- any- any, um,",
    "start": "243890",
    "end": "250879"
  },
  {
    "text": "smooth and differentiable function on a given, uh, input range.",
    "start": "250880",
    "end": "256299"
  },
  {
    "text": "And that- that kind of, uh, brings us into the bias-variance and asking the question,",
    "start": "256300",
    "end": "263705"
  },
  {
    "text": "Is that a good thing or a bad thing? You know, having- having expressive models, is that, um, is that good or is that bad?",
    "start": "263705",
    "end": "270350"
  },
  {
    "text": "So that brings us to bias-variance. [NOISE]",
    "start": "270350",
    "end": "282680"
  },
  {
    "text": "Any questions on neural networks or deep learning before we, uh, jump into, uh, bias-variance?",
    "start": "282680",
    "end": "288040"
  },
  {
    "text": "Any remaining questions? Okay? So, uh, bias-variance.",
    "start": "288040",
    "end": "293340"
  },
  {
    "text": "So in- in your- in your homework 1, in- in the last question, where we- we- we saw feature maps, right?",
    "start": "293340",
    "end": "299819"
  },
  {
    "text": "So in feature maps, we, uh, supposing we had a data, uh, a dataset that looked like this.",
    "start": "299820",
    "end": "305730"
  },
  {
    "text": "[NOISE] I'm plotting the same dataset again here and here.",
    "start": "305730",
    "end": "313575"
  },
  {
    "text": "It's the same dataset. [NOISE] Right? Now, um,",
    "start": "313575",
    "end": "319065"
  },
  {
    "text": "if we fit a linear model where the feature map is just, uh, a polynomial of degree 1,",
    "start": "319065",
    "end": "326055"
  },
  {
    "text": "so k equals 1, we get a straight line fit through the data.",
    "start": "326055",
    "end": "333840"
  },
  {
    "text": "Right? And if we try with, uh, k equals 2,",
    "start": "334010",
    "end": "339060"
  },
  {
    "text": "where the- the polynomial degree, uh, is 2, we get say, a quadratic fit.",
    "start": "339060",
    "end": "346015"
  },
  {
    "text": "Similarly, if we try with say, k equals 20, right,",
    "start": "346015",
    "end": "351500"
  },
  {
    "text": "we would get a fit that might look something like this.",
    "start": "351500",
    "end": "355410"
  },
  {
    "text": "Right? Now, we could have done something very similar with say a classification problem.",
    "start": "358340",
    "end": "367195"
  },
  {
    "text": "Right? Supposing the x's here [NOISE] mark one class and o's mark another.",
    "start": "367195",
    "end": "377890"
  },
  {
    "text": "[NOISE] Let's say there's one x here and one o here.",
    "start": "377890",
    "end": "384240"
  },
  {
    "text": "[NOISE]",
    "start": "384240",
    "end": "399479"
  },
  {
    "text": "Supposing we had a dataset like this, and this is the third, um,",
    "start": "399480",
    "end": "404695"
  },
  {
    "text": "I'm just copying the same dataset in three different places. [NOISE]",
    "start": "404695",
    "end": "419819"
  },
  {
    "text": "Here the, um, x's, uh, mark one class and the always mark another class.",
    "start": "419820",
    "end": "425160"
  },
  {
    "text": "Now, if we try to fit, say, a logistic regression with, uh, polynomial features,",
    "start": "425160",
    "end": "430580"
  },
  {
    "text": "say with k equals 1, you would again get a straight-line fit that tries to separate the two classes.",
    "start": "430580",
    "end": "436685"
  },
  {
    "text": "If we try the logistic regression with k equals 2, you would get a quadratic fit.",
    "start": "436685",
    "end": "442050"
  },
  {
    "text": "Right? And let's say we tried it with k equals, you know, maybe 20,",
    "start": "442870",
    "end": "448125"
  },
  {
    "text": "right, and we- we would probably get something that would potentially look like this.",
    "start": "448125",
    "end": "454760"
  },
  {
    "text": "Right? So, um, loosely speaking,",
    "start": "456260",
    "end": "462235"
  },
  {
    "text": "we can call these models as being underfit,",
    "start": "462235",
    "end": "466999"
  },
  {
    "text": "these models as overfit.",
    "start": "468260",
    "end": "472720"
  },
  {
    "text": "Right? So in machine learning, we are given a training set,",
    "start": "473870",
    "end": "479014"
  },
  {
    "text": "but the- the- the, uh, error that we actually care about is",
    "start": "479015",
    "end": "484483"
  },
  {
    "text": "not how well the model performs on the given dataset, but how well does the model perform in general on data that it has not seen before.",
    "start": "484484",
    "end": "493055"
  },
  {
    "text": "Uh, you can call it, loosely speaking, the test set, but in general, you know, you- you call it generalization error.",
    "start": "493055",
    "end": "499220"
  },
  {
    "text": "So generalization error. [NOISE]",
    "start": "499220",
    "end": "507694"
  },
  {
    "text": "Generalization error is the error or the loss of the costs that a model incurs when it is",
    "start": "507695",
    "end": "516099"
  },
  {
    "text": "tested against possibly all the infinite data out there in the world from whose distribution we got the training set, right?",
    "start": "516100",
    "end": "525589"
  },
  {
    "text": "In machine learning, what we care about is that our model needs to have a low generalization error.",
    "start": "526500",
    "end": "532915"
  },
  {
    "text": "But the- the fundamental problem is that what we have access to is just a finite sample from the- from the,",
    "start": "532915",
    "end": "541764"
  },
  {
    "text": "you know, the- the infinitely many possible examples from the data distribution.",
    "start": "541765",
    "end": "547840"
  },
  {
    "text": "We have a finite sample from uh, the data generating distribution. And we need- using this finite sample,",
    "start": "547840",
    "end": "554019"
  },
  {
    "text": "we need to build a model that has good error rate or low error when it is tested against all the possible,",
    "start": "554020",
    "end": "561250"
  },
  {
    "text": "you know, infinitely many examples in our data distribution. So this um, this concept of under-fitting and over-fitting is um,",
    "start": "561250",
    "end": "574855"
  },
  {
    "text": "you can- you can think of under-fitting and over-fitting as corresponding to different uh,",
    "start": "574855",
    "end": "583269"
  },
  {
    "text": "um, components of the generalization error. So loosely speaking, generalization error, which means taking the model that we obtain and",
    "start": "583270",
    "end": "590785"
  },
  {
    "text": "testing it against an example that we have not seen before. And when we test it against an example that we've not seen it before,",
    "start": "590785",
    "end": "597894"
  },
  {
    "text": "there's gonna be some error. It's never going to be perfectly right and that error can be broken down into sub-components.",
    "start": "597895",
    "end": "605845"
  },
  {
    "text": "So the generalization error can be broken down into two parts. A component called the bias and the component called the variance.",
    "start": "605845",
    "end": "617060"
  },
  {
    "text": "And loosely speaking uh, bias is the component due to what you can call as- this",
    "start": "621030",
    "end": "631180"
  },
  {
    "text": "is the component of generalization error.",
    "start": "631180",
    "end": "638185"
  },
  {
    "text": "I'm going to write it as GE, component of generalized error",
    "start": "638185",
    "end": "645080"
  },
  {
    "text": "due to expressivity handicap,",
    "start": "647010",
    "end": "654140"
  },
  {
    "text": "and variance is the component of generalization error due",
    "start": "658530",
    "end": "667240"
  },
  {
    "text": "to finite sample",
    "start": "667240",
    "end": "673944"
  },
  {
    "text": "of training set, right?",
    "start": "673944",
    "end": "681204"
  },
  {
    "text": "And the generalization error that is incurred by this model,",
    "start": "681205",
    "end": "686905"
  },
  {
    "text": "you expect it to be pretty high. And the generalization error incurred by this model is also expected to be pretty high,",
    "start": "686905",
    "end": "693985"
  },
  {
    "text": "but they are high for fundamentally very different reasons. The reason why this model,",
    "start": "693985",
    "end": "701350"
  },
  {
    "text": "or these kinds of models are expected to high- have high generalization error is because they",
    "start": "701350",
    "end": "708880"
  },
  {
    "text": "are linear models and they're kind of hand- handicap and cannot capture this quadratic relation between- between uh,",
    "start": "708880",
    "end": "716365"
  },
  {
    "text": "the x's and y's. Similarly, this model is extremely expressive.",
    "start": "716365",
    "end": "723970"
  },
  {
    "text": "It can twist and turn in wild ways. But it- it- it cannot generalize well because it",
    "start": "723970",
    "end": "730660"
  },
  {
    "text": "just- it does not have enough- enough data to kind of uh, fit it- fit it well rather.",
    "start": "730660",
    "end": "737215"
  },
  {
    "text": "So it does extremely well on the given training set. And in order to do well on the given training set.",
    "start": "737215",
    "end": "743305"
  },
  {
    "text": "Now we're kind of losing the big picture of wanting to generalize well on unseen data.",
    "start": "743305",
    "end": "748795"
  },
  {
    "text": "And we do overly well on the training set. And this is due to the fact that we just don't have enough training data, right?",
    "start": "748795",
    "end": "755574"
  },
  {
    "text": "If- if- if we had a lot more number of training data, then even though we had uh, higher expressive model, it wouldn't have over-fit the data like this, right?",
    "start": "755575",
    "end": "765100"
  },
  {
    "text": "And we saw some amount of that in uh, your homework question as well. Where if- if you start with a small data-set where the number of examples is very small,",
    "start": "765100",
    "end": "775720"
  },
  {
    "text": "then the higher-order polynomials can over-fit it by- by forming highly regularly hypothesis functions.",
    "start": "775720",
    "end": "783250"
  },
  {
    "text": "Whereas if you had a larger data-set, even polynomials of higher degree can do a pretty good job of- of fitting the data.",
    "start": "783250",
    "end": "793279"
  },
  {
    "text": "So loosely speaking, bias is the generalization error of a model",
    "start": "793740",
    "end": "801640"
  },
  {
    "text": "due to it being kind of handicapped in its expressivity. And it just cannot even do a good job of fitting the training set itself very well.",
    "start": "801640",
    "end": "810985"
  },
  {
    "text": "And variance is- is- is due to the fact that your model was highly expressive,",
    "start": "810985",
    "end": "818170"
  },
  {
    "text": "but it just did not have enough data for- for it to- for it to generalize well.",
    "start": "818170",
    "end": "824860"
  },
  {
    "text": "And these concepts, um, these definitions are very loose.",
    "start": "824980",
    "end": "831270"
  },
  {
    "text": "These are not standard definitions, but this is a good- good kind of intuition to have of- of- of what bias and variance are.",
    "start": "831270",
    "end": "839800"
  },
  {
    "text": "In fact, when we are using the squared error as the loss function.",
    "start": "839800",
    "end": "848899"
  },
  {
    "text": "In the setting of the squared error and where",
    "start": "852540",
    "end": "859195"
  },
  {
    "text": "the model is- and we assume that our data comes like this.",
    "start": "859195",
    "end": "865810"
  },
  {
    "text": "Y equals F of x plus epsilon,",
    "start": "865810",
    "end": "870970"
  },
  {
    "text": "where F is some function, some hypothesis, right? If- if this is the true data generating distribution where the expectation of epsilon",
    "start": "870970",
    "end": "882325"
  },
  {
    "text": "is 0 and the variance of epsilon is sigma square.",
    "start": "882325",
    "end": "890060"
  },
  {
    "text": "For- in- in this particular setting, the generalization error or the test error,",
    "start": "890070",
    "end": "898399"
  },
  {
    "text": "and just also the generalization error is equal to",
    "start": "900660",
    "end": "907435"
  },
  {
    "text": "the expected y minus",
    "start": "907435",
    "end": "914019"
  },
  {
    "text": "F hat x squared.",
    "start": "914019",
    "end": "922105"
  },
  {
    "text": "Now, let's- let's analyze this for a moment. So F hat that means suffix it with n. F hat n is the- is",
    "start": "922105",
    "end": "932140"
  },
  {
    "text": "the model that we get by fitting on n training examples.",
    "start": "932140",
    "end": "940225"
  },
  {
    "text": "And n training examples were obtained with this data-generating process,",
    "start": "940225",
    "end": "946029"
  },
  {
    "text": "which means the n training examples, grouped in with y^ i equals F of x^ i plus epsilon i, right?",
    "start": "946030",
    "end": "957730"
  },
  {
    "text": "So the n training examples from which the F hat uh, was obtained had noise embedded in them.",
    "start": "957730",
    "end": "964975"
  },
  {
    "text": "And therefore, F hat n is a random variable. It's a random function- it's a random variable because it's a function",
    "start": "964975",
    "end": "973885"
  },
  {
    "text": "or inputs that were random, right?",
    "start": "973885",
    "end": "980050"
  },
  {
    "text": "So F hat n is- it accepts an input as x. Now x is not random,",
    "start": "980050",
    "end": "986260"
  },
  {
    "text": "but the way F hat n was constructed was using n training examples and the n training examples had noise in them.",
    "start": "986260",
    "end": "995080"
  },
  {
    "text": "And therefore, because the n training examples had noise in them, F hat n is random.",
    "start": "995080",
    "end": "1001330"
  },
  {
    "text": "It is random, but it accepts an input which is deterministic. But the way F hat n itself was constructed was, it's a random variable.",
    "start": "1001400",
    "end": "1011130"
  },
  {
    "text": "And what is this expression? What's this expression calculating?",
    "start": "1011130",
    "end": "1017595"
  },
  {
    "text": "It is calculating over here, x and y are a pair from the test- test set",
    "start": "1017595",
    "end": "1025015"
  },
  {
    "text": "or on an example that the model has not seen before.",
    "start": "1025015",
    "end": "1030265"
  },
  {
    "text": "So on an example that the model has not seen before, what is the prediction of our learnt model on the new input?",
    "start": "1030265",
    "end": "1038204"
  },
  {
    "text": "And this is the error of that model with the new output.",
    "start": "1038205",
    "end": "1044625"
  },
  {
    "text": "So this is generalization error and this expectation is over the noise that is in the test example and noise in the training set.",
    "start": "1044625",
    "end": "1057620"
  },
  {
    "text": "So this is the expectation on all the noise- noise variables, both in the training set and the test set.",
    "start": "1057620",
    "end": "1064540"
  },
  {
    "text": "So their expectation is over",
    "start": "1064540",
    "end": "1073540"
  },
  {
    "text": "all epsilon in the training set and test example.",
    "start": "1073540",
    "end": "1084520"
  },
  {
    "text": "And in the case of the squared error, we can see that this can be with",
    "start": "1087080",
    "end": "1092409"
  },
  {
    "text": "some pretty simple- pretty simple uh, algebra.",
    "start": "1092410",
    "end": "1101010"
  },
  {
    "text": "You can show that this will be equal to sigma square plus expectation of",
    "start": "1101010",
    "end": "1110780"
  },
  {
    "text": "F of x minus F hat x squared",
    "start": "1111030",
    "end": "1119020"
  },
  {
    "text": "plus variance of F hat x.",
    "start": "1119020",
    "end": "1128955"
  },
  {
    "text": "So what here? F is the true function that we don't have access to,",
    "start": "1128955",
    "end": "1134485"
  },
  {
    "text": "and f-hat- [NOISE] f-hat_n",
    "start": "1134485",
    "end": "1140850"
  },
  {
    "text": "is the model that we obtained by training on the finite, uh, uh, training, uh- training data,",
    "start": "1140850",
    "end": "1146370"
  },
  {
    "text": "and we can decompose this- this error on a new test example as into three components.",
    "start": "1146370",
    "end": "1154410"
  },
  {
    "text": "Okay? This component is called irreducible error,",
    "start": "1154410",
    "end": "1159790"
  },
  {
    "text": "and this component is called",
    "start": "1164180",
    "end": "1169380"
  },
  {
    "text": "the bias squared- bias squared,",
    "start": "1169380",
    "end": "1175995"
  },
  {
    "text": "and this component is called the variance.",
    "start": "1175995",
    "end": "1180220"
  },
  {
    "text": "Right? So what's happening here? So what- what this is telling us is the test error of",
    "start": "1182630",
    "end": "1191880"
  },
  {
    "text": "any model can be broken down into- into these three components.",
    "start": "1191880",
    "end": "1197235"
  },
  {
    "text": "Now, all of these three components are non-negative because this is- this is, uh, Sigma squared,",
    "start": "1197235",
    "end": "1203610"
  },
  {
    "text": "that's the variance of the noise, that is non-negative, this is the bias squared,",
    "start": "1203610",
    "end": "1208664"
  },
  {
    "text": "so that is non-negative, and this is the variance of something and that is non-negative. So the test error or the expected test error can be broken down into three components.",
    "start": "1208665",
    "end": "1218625"
  },
  {
    "text": "Now, each of these- each of these three errors is,",
    "start": "1218625",
    "end": "1224610"
  },
  {
    "text": "uh- is a- is a fundamentally very different kind of error compared to the other two.",
    "start": "1224610",
    "end": "1230085"
  },
  {
    "text": "So irreducible error tells us that no matter what kind of model we choose,",
    "start": "1230085",
    "end": "1236070"
  },
  {
    "text": "no matter how big training- uh, how big a training set we start with, we can never do better than irreducible error, and why is that?",
    "start": "1236070",
    "end": "1245055"
  },
  {
    "text": "That's because fundamentally, data is noisy, which means, for the same given value of x,",
    "start": "1245055",
    "end": "1251235"
  },
  {
    "text": "for different examples, y can be different because of the noise term, which means at test time,",
    "start": "1251235",
    "end": "1257385"
  },
  {
    "text": "when we're given a value of x on- on which we need to make a prediction, the y-value could be multiple possible y-",
    "start": "1257385",
    "end": "1264465"
  },
  {
    "text": "y values and there's just no way we can- we can, uh, you know, get the right-, uh, get the right prediction all the time",
    "start": "1264465",
    "end": "1270600"
  },
  {
    "text": "because our function f is a deterministic function, right? And that noise is essentially what's captured in this irreducible error, right?",
    "start": "1270600",
    "end": "1277980"
  },
  {
    "text": "So irreducible error is because the data is noisy and for given value of x, there is no single right answer, right?",
    "start": "1277980",
    "end": "1284580"
  },
  {
    "text": "That's irreducible no matter what kind of model you choose, you choose a neural network, you choose a linear regression,",
    "start": "1284580",
    "end": "1289679"
  },
  {
    "text": "you choose any fancy model you want, you can never do better than irreducible error. Any questions on this list?",
    "start": "1289680",
    "end": "1295560"
  },
  {
    "text": "[inaudible].",
    "start": "1295560",
    "end": "1319770"
  },
  {
    "text": "So the question was, uh, why is this irreducible error? Because by increasing k,",
    "start": "1319770",
    "end": "1324795"
  },
  {
    "text": "we- we actually made it go through all the points, right? [inaudible]. So- um, and- and the question is,",
    "start": "1324795",
    "end": "1332505"
  },
  {
    "text": "you know, isn't Epsilon 0 in this case? Um, the- the, uh, answer to that is, here,",
    "start": "1332505",
    "end": "1338790"
  },
  {
    "text": "we are talking about test error, which means now we're measuring how well this model",
    "start": "1338790",
    "end": "1343830"
  },
  {
    "text": "performs on a test point that was not included in the training set. Right? So this is generalization error,",
    "start": "1343830",
    "end": "1350700"
  },
  {
    "text": "which means we are measuring how well the model performs on unseen data, right? Because doing well on the given training set is an easy thing.",
    "start": "1350700",
    "end": "1358290"
  },
  {
    "text": "You know, it's- there's- there's- you know, you don't need so much of theory to do well on just the given training set that you have.",
    "start": "1358290",
    "end": "1364590"
  },
  {
    "text": "In fact, if you're just given a training set and you want to do well on the training set, just memorize the training set, right?",
    "start": "1364590",
    "end": "1370425"
  },
  {
    "text": "The whole point of machine learning is to start with a limited set of training data and still do well on generalization error, right?",
    "start": "1370425",
    "end": "1379125"
  },
  {
    "text": "So generalization error is, when you are given a new example that you've never seen before,",
    "start": "1379125",
    "end": "1386325"
  },
  {
    "text": "the expected error can be decomposed into an irreducible error component,",
    "start": "1386325",
    "end": "1394620"
  },
  {
    "text": "which is due to the fact that the data is noisy, and then there are two more components here.",
    "start": "1394620",
    "end": "1401519"
  },
  {
    "text": "Now what are these two components? So the first one is the expectation of fx minus f hat_n x.",
    "start": "1401520",
    "end": "1409680"
  },
  {
    "text": "So what does this mean? There is a true underlying signal that we don't have access to, right,",
    "start": "1409680",
    "end": "1417419"
  },
  {
    "text": "and the true underlying signal will be different from the prediction made by our model, and in general,",
    "start": "1417420",
    "end": "1424830"
  },
  {
    "text": "what is the expectation of this- of this- uh, of the difference between- of- of",
    "start": "1424830",
    "end": "1431025"
  },
  {
    "text": "the difference between the true underlying signal and our, uh- uh, and the prediction that our model makes in general.",
    "start": "1431025",
    "end": "1437745"
  },
  {
    "text": "And if we take the expectation across all the- all- all the possible different training sets that we get and the expectation across the noise in the,",
    "start": "1437745",
    "end": "1447705"
  },
  {
    "text": "uh, test example, that gives us how- how systematically wrong are we.",
    "start": "1447705",
    "end": "1454485"
  },
  {
    "text": "All right.. What's the expected- expected difference between the true signal and,",
    "start": "1454485",
    "end": "1459920"
  },
  {
    "text": "uh- and and- and the predicted, uh- predicted value, and this is the square of the bias, right?",
    "start": "1459920",
    "end": "1467190"
  },
  {
    "text": "And finally, we have a- a third part called the- the variance of, uh, f hat_n x.",
    "start": "1467270",
    "end": "1474315"
  },
  {
    "text": "Now, what is- what is the meaning of the variance of f hat_n x? It means if we were to repeat this experiment with a new set of n examples, right?",
    "start": "1474315",
    "end": "1485610"
  },
  {
    "text": "I don't need to repeat the experiment of refitting on the same dataset. Repeat this experiment by collecting a new set of",
    "start": "1485610",
    "end": "1492029"
  },
  {
    "text": "n examples from the same data generating distribution. F hat_n x will be a different,",
    "start": "1492030",
    "end": "1498840"
  },
  {
    "text": "uh- uh, the prediction on the test set will be a different value, right? And now, if we were to keep repeating this process over and over",
    "start": "1498840",
    "end": "1506070"
  },
  {
    "text": "by collecting a new set of n examples and making a prediction on this, uh, test set, you know,",
    "start": "1506070",
    "end": "1511784"
  },
  {
    "text": "I repeat again, a new set of  n examples, fit the model, make a prediction on, uh, the new test set, uh, x.",
    "start": "1511785",
    "end": "1519105"
  },
  {
    "text": "All these different predictions on the new test set is going to have some variance in it, right,",
    "start": "1519105",
    "end": "1524700"
  },
  {
    "text": "and this is the variance of the prediction made on a new test example when we are changing the training set to be,",
    "start": "1524700",
    "end": "1534450"
  },
  {
    "text": "uh, uh, resampled every time, right? And this variance, you can kind of see,",
    "start": "1534450",
    "end": "1540765"
  },
  {
    "text": "was completely unrelated to what the right answer for f of- of- for the new test example was, right?",
    "start": "1540765",
    "end": "1547125"
  },
  {
    "text": "It's just how- how sensitive to noise we are in the- for- for the noise in the training set,",
    "start": "1547125",
    "end": "1554414"
  },
  {
    "text": "and- and that's captured by variance. Yes, question. [inaudible]",
    "start": "1554415",
    "end": "1581700"
  },
  {
    "text": "So the question is, uh, because, uh, we- we said variance is- is due to,",
    "start": "1581700",
    "end": "1586920"
  },
  {
    "text": "uh, the limitation of having a finite sample, uh, test set. Now, you can imagine, uh,",
    "start": "1586920",
    "end": "1592065"
  },
  {
    "text": "if your population is- has, you know, uh- theoretically, it has an infinite number of- number of- of points,",
    "start": "1592065",
    "end": "1599370"
  },
  {
    "text": "infinite number of examples, if you were to take the entire infinite set of, uh, attaining population and fit your model,",
    "start": "1599370",
    "end": "1606765"
  },
  {
    "text": "and were- were to repeat it by taking the- the entire infinite set again and fit your model,",
    "start": "1606765",
    "end": "1611790"
  },
  {
    "text": "you'll always get the same model, right? And the- the variance that we get from experiment to experiment,",
    "start": "1611790",
    "end": "1621120"
  },
  {
    "text": "where the experiment includes collecting your n training, uh, examples, the- the- the- the variance that you get from experiment to",
    "start": "1621120",
    "end": "1628679"
  },
  {
    "text": "experiment is due to the fact that you're collecting a small number of finite, uh, examples.",
    "start": "1628680",
    "end": "1634080"
  },
  {
    "text": "If you were to collect more number of examples in each experiment, then the variance will come down, and we'll go a little bit deeper into this, uh, shortly.",
    "start": "1634080",
    "end": "1641955"
  },
  {
    "text": "So coming back to your question about the polynomial case, um, in- in our, uh, homework, uh,",
    "start": "1641955",
    "end": "1647940"
  },
  {
    "text": "question 5, what we saw was, with, uh, k equals 20, it was a pretty, uh, wiggly function.",
    "start": "1647940",
    "end": "1655169"
  },
  {
    "text": "Now, if we were to repeat the k equals 20 case with a dataset of say, uh,",
    "start": "1655170",
    "end": "1660210"
  },
  {
    "text": "10,000 examples, then what you'll see is that it- it will do, uh, you know, fairly well,",
    "start": "1660210",
    "end": "1665730"
  },
  {
    "text": "it'll fit it fairly well. You'll resample a new set of, uh, what was the number I said?",
    "start": "1665730",
    "end": "1671595"
  },
  {
    "text": "20,000 or whatever, you know, uh, 10,000 examples, you repeat it, it will be slightly different, right?",
    "start": "1671595",
    "end": "1676775"
  },
  {
    "text": "And you repeat it again, you'll get a new hypothesis, which is, again, very slightly different, and if you were to repeat this experiment with a new number, say, you know,",
    "start": "1676775",
    "end": "1684020"
  },
  {
    "text": "10 million examples, then the- the- the, the variance from ex- experiment to experiment is going to come down even further, right?",
    "start": "1684020",
    "end": "1691890"
  },
  {
    "text": "The more number of examples you have, they may be noisy, right? You know, you take 10,000 examples.",
    "start": "1691890",
    "end": "1697575"
  },
  {
    "text": "Each of those examples has noise in them. But the variance in the fitted model from experiment to experiment is going to come down,",
    "start": "1697575",
    "end": "1703365"
  },
  {
    "text": "the more number of examples you have. [inaudible]. We- we- uh, we will come to that.",
    "start": "1703365",
    "end": "1711299"
  },
  {
    "text": "So the question is- is it- uh, is the relation linear or quadratic? We- we'll come to that shortly.",
    "start": "1711300",
    "end": "1716924"
  },
  {
    "text": "All right. So this is in the context of machine learning, right? Uh, however, uh- however,",
    "start": "1716925",
    "end": "1724769"
  },
  {
    "text": "I think understanding bias and variance from a more classical statistis- statistics,",
    "start": "1724770",
    "end": "1730184"
  },
  {
    "text": "uh, setting is a little more intuitive then understanding this. So- so let's have a look at bias-variance from a more classical statistics setting,",
    "start": "1730184",
    "end": "1742085"
  },
  {
    "text": "because here we were focused on prediction, you know, how well does the model- uh, or what does the model do when it's presented with an x that has- it has not seen before, right?",
    "start": "1742085",
    "end": "1751490"
  },
  {
    "text": "All right. So bias variance in a more, um.",
    "start": "1751490",
    "end": "1758370"
  },
  {
    "text": "[NOISE]",
    "start": "1758370",
    "end": "1769530"
  },
  {
    "text": "So, let's assume there is a data generating distribution. Um, so data x, y are pairs, um,",
    "start": "1769530",
    "end": "1779850"
  },
  {
    "text": "come from some- some distribution, [NOISE] that's parameterized by Theta, right?",
    "start": "1779850",
    "end": "1787575"
  },
  {
    "text": "And what we do is from this, uh, data generating distribution,",
    "start": "1787575",
    "end": "1792885"
  },
  {
    "text": "we collect [NOISE] n, um, examples.",
    "start": "1792885",
    "end": "1806909"
  },
  {
    "text": "So each of these is a pair, x1, y1,",
    "start": "1806910",
    "end": "1811920"
  },
  {
    "text": "until xn, yn, right?",
    "start": "1811920",
    "end": "1817365"
  },
  {
    "text": "We- we sample x and y pairs from the data generating distribution and we can- uh,",
    "start": "1817365",
    "end": "1823860"
  },
  {
    "text": "sample n such, uh, um, n such pairs, and we run it through a statistical model.",
    "start": "1823860",
    "end": "1833790"
  },
  {
    "text": "[NOISE] Right?",
    "start": "1833790",
    "end": "1841695"
  },
  {
    "text": "And by a statistical model, um, what- what I mean is say- the MLE estimator.",
    "start": "1841695",
    "end": "1848265"
  },
  {
    "text": "You know it? And what we get out is a Theta hat.",
    "start": "1848265",
    "end": "1855525"
  },
  {
    "text": "Okay? This is what we did for example in- in linear regression. You know, if- if- if we assume a linear relation between x and y where the,",
    "start": "1855525",
    "end": "1863655"
  },
  {
    "text": "uh, Theta captures the, uh, linear, uh, coefficients, then this is just the normal equation. Right? But in general,",
    "start": "1863655",
    "end": "1869310"
  },
  {
    "text": "it is some- some statistical model. If- if- if this is, you know, x is, um,",
    "start": "1869310",
    "end": "1874650"
  },
  {
    "text": "x is some features and y is 0, 1, then, you know, think of this as logistic regression. And once you- once you fit the model,",
    "start": "1874650",
    "end": "1881400"
  },
  {
    "text": "you get the, uh, the estimated parameters. Right? So now this is random.",
    "start": "1881400",
    "end": "1890220"
  },
  {
    "text": "Random because there were random samples from our data generating distribution, right?",
    "start": "1890220",
    "end": "1895380"
  },
  {
    "text": "And this is not random. This is, you know, deterministic.",
    "start": "1895380",
    "end": "1901800"
  },
  {
    "text": "[NOISE] Right?",
    "start": "1901800",
    "end": "1906965"
  },
  {
    "text": "And what happens when you feed a random variable into a deterministic function, the output will be? Random.",
    "start": "1906965",
    "end": "1912620"
  },
  {
    "text": "Random. Right? So this is also random. [NOISE] And the way it is random,",
    "start": "1912620",
    "end": "1921270"
  },
  {
    "text": "um, we can- we can visualize it- it.",
    "start": "1921270",
    "end": "1925600"
  },
  {
    "text": "So let's assume, um, we have four different statistical models, [NOISE] right?",
    "start": "1927950",
    "end": "1935350"
  },
  {
    "text": "So this is Theta_1, and this is Theta_d.",
    "start": "1939920",
    "end": "1945820"
  },
  {
    "text": "Theta_d, and this is our parameter space, Theta_1. So over here, we were looking at the data space. [NOISE] Right?",
    "start": "1946400",
    "end": "1954795"
  },
  {
    "text": "So here, this was x and this was y, and here this was x_1 and this was x_d.",
    "start": "1954795",
    "end": "1964635"
  },
  {
    "text": "Right? [NOISE] Over here, we are in the parameter space, right?",
    "start": "1964635",
    "end": "1971430"
  },
  {
    "text": "That's- that's, uh, uh, so it's a switch from that view. Theta_1, Theta_d, Theta_1, and Theta_d.",
    "start": "1971430",
    "end": "1982965"
  },
  {
    "text": "Now, the- the- the data that we sample from came from some true distribution,",
    "start": "1982965",
    "end": "1988049"
  },
  {
    "text": "let's call it Theta star, right? So Theta star is some unknown- [NOISE] unknown constant that,",
    "start": "1988050",
    "end": "1995940"
  },
  {
    "text": "you know, we don't have access to. So let's, you know, assume Theta star lives here. So this is Theta star.",
    "start": "1995940",
    "end": "2002390"
  },
  {
    "text": "[NOISE] Drawing it in the same position in all the four. Theta star.",
    "start": "2002390",
    "end": "2010205"
  },
  {
    "text": "This is Theta star. One- yeah. So- so this is- this is basically in the same position in all the four, um, four.",
    "start": "2010205",
    "end": "2018785"
  },
  {
    "text": "And let's- let's assume we have four different statistical models,",
    "start": "2018785",
    "end": "2024140"
  },
  {
    "text": "four different estimators of our parameter, right? And we start an experiment,",
    "start": "2024140",
    "end": "2030710"
  },
  {
    "text": "we sample n examples from this distribution. Here we are assuming we have access to, you know,",
    "start": "2030710",
    "end": "2037535"
  },
  {
    "text": "or potentially sampling an infinite number of points and assume sampling is cheap, right? Sample n examples where n is some fixed number,",
    "start": "2037535",
    "end": "2044885"
  },
  {
    "text": "run it through the stat- statistical model. Run it through each of the four models.",
    "start": "2044885",
    "end": "2050284"
  },
  {
    "text": "So we have- [NOISE] so let's call this model A, model B,",
    "start": "2050285",
    "end": "2056059"
  },
  {
    "text": "model C and model D. And these four plots correspond to A,",
    "start": "2056060",
    "end": "2062149"
  },
  {
    "text": "B, C, and D. Right? And for a given sample of, uh,",
    "start": "2062150",
    "end": "2069095"
  },
  {
    "text": "n examples, we get some estimate, Theta hat, right? So maybe, you know,",
    "start": "2069095",
    "end": "2075875"
  },
  {
    "text": "this was the, uh, estimate by A and by B, maybe this was the estimate.",
    "start": "2075875",
    "end": "2083909"
  },
  {
    "text": "Right? And maybe from C, this was the estimate.",
    "start": "2084010",
    "end": "2089480"
  },
  {
    "text": "And maybe from D, maybe this was the estimate. Right? So unfortunately I do not have colored pens here, but, um,",
    "start": "2089480",
    "end": "2099440"
  },
  {
    "text": "maybe I'll highlight this with- with, uh, the true parameter and- and, um,",
    "start": "2099440",
    "end": "2107990"
  },
  {
    "text": "I- I make it a little bigger as a block because I don't have a different colored pen today.",
    "start": "2107990",
    "end": "2113670"
  },
  {
    "text": "Right? And- and supposing we repeat the experiment,",
    "start": "2120940",
    "end": "2128450"
  },
  {
    "text": "get a new set of n examples, fit it on the four different models,",
    "start": "2128450",
    "end": "2133505"
  },
  {
    "text": "and because the new set of examples had, you know, noise in them, the estimator Theta is going to be different again.",
    "start": "2133505",
    "end": "2140734"
  },
  {
    "text": "Right? And in this case, let's say the new e- estimate was here,",
    "start": "2140735",
    "end": "2147665"
  },
  {
    "text": "and in this case, the new estimate let's say it was- was here,",
    "start": "2147665",
    "end": "2152690"
  },
  {
    "text": "and our- in this case, let's say it was here, [NOISE] and in this case, let's say it was here.",
    "start": "2152690",
    "end": "2158720"
  },
  {
    "text": "Right? And then we repeat it. Set of n, run it, fit it through four different models and plot the estimated values.",
    "start": "2158720",
    "end": "2167210"
  },
  {
    "text": "Right? So let's say it was here, and let's say we keep repeating it. [NOISE]",
    "start": "2167210",
    "end": "2194589"
  },
  {
    "text": "So each, each dot in these plots corresponds to one experiment, right?",
    "start": "2194590",
    "end": "2201190"
  },
  {
    "text": "It is the output of one experiment, right? The number of dots is not the same as the number of examples,",
    "start": "2201190",
    "end": "2206500"
  },
  {
    "text": "so the number of the- each dot corresponds to one experiment. And what we see is, um,",
    "start": "2206500",
    "end": "2213880"
  },
  {
    "text": "we might get plots like these, and what we would call- what we see here is,",
    "start": "2213880",
    "end": "2219565"
  },
  {
    "text": "in- in these- in these two examples, the- the variance of the expected, um,",
    "start": "2219565",
    "end": "2227140"
  },
  {
    "text": "th- the variance of the estimator is pretty small because they are- they are- because",
    "start": "2227140",
    "end": "2233349"
  },
  {
    "text": "they're pretty tightly concentrated in one region, right?",
    "start": "2233350",
    "end": "2240265"
  },
  {
    "text": "Whereas in these two cases, the variance is pretty large, right? So you would call these two as low variance,",
    "start": "2240265",
    "end": "2250640"
  },
  {
    "text": "and these two as high variance.",
    "start": "2253110",
    "end": "2256610"
  },
  {
    "text": "For a low variance estimator, and high variance estimator, right?",
    "start": "2258630",
    "end": "2269185"
  },
  {
    "text": "And similarly, what we see is over here, the distribution of Theta hat.",
    "start": "2269185",
    "end": "2275610"
  },
  {
    "text": "So you can- you can think of the dots that we obtained by, you know, repeating this process as samples from Theta hat,",
    "start": "2275610",
    "end": "2285115"
  },
  {
    "text": "the distribution of Theta hat, and that's also called the sampling distribution, right?",
    "start": "2285115",
    "end": "2295165"
  },
  {
    "text": "So these- each of these dots have samples from the corresponding sampling distributions, right? And what we see is",
    "start": "2295165",
    "end": "2302185"
  },
  {
    "text": "the sampling distribution is kind of centered on the true value, right?",
    "start": "2302185",
    "end": "2307240"
  },
  {
    "text": "So th- so the center of the sampling distribution coincides with the true value in both the cases, right? So this is, so- uh,",
    "start": "2307240",
    "end": "2314740"
  },
  {
    "text": "this is therefore called a low bias estimator,",
    "start": "2314740",
    "end": "2321559"
  },
  {
    "text": "and this, the center of the sampling distribution is- has moved away from the true parameter.",
    "start": "2324120",
    "end": "2332994"
  },
  {
    "text": "So these, you would call them as high variance estimat- high bias estimators.",
    "start": "2332995",
    "end": "2337460"
  },
  {
    "text": "So what we see is that bias and variance are a fundamentally different things, right?",
    "start": "2342390",
    "end": "2348865"
  },
  {
    "text": "Having one, each- having- having low variance or high variance says nothing about whether the model",
    "start": "2348865",
    "end": "2354640"
  },
  {
    "text": "has high bi- high bias or low bias, okay? The way to think about, um, high bias is that the estimator has,",
    "start": "2354640",
    "end": "2363369"
  },
  {
    "text": "uh, has a fundamental systematic error in estimating parameters.",
    "start": "2363370",
    "end": "2370330"
  },
  {
    "text": "In that, no matter how many times you- you sample different datasets and fit them,",
    "start": "2370330",
    "end": "2375775"
  },
  {
    "text": "in this case, they are, you know, fundamentally kind of, there's a systematic bias, say in this case, to be closer to 0, right?",
    "start": "2375775",
    "end": "2384820"
  },
  {
    "text": "And that has got nothing to do with whether they have low variance or high variance.",
    "start": "2384820",
    "end": "2389810"
  },
  {
    "text": "Similarly, this model has low variance.",
    "start": "2389910",
    "end": "2396174"
  },
  {
    "text": "These- these- these models have low variance, which means no matter, um, no matter how much noise, uh,",
    "start": "2396175",
    "end": "2403795"
  },
  {
    "text": "no matter the noise that you get from, you know, different, uh, training sets, the estimated value is somewhat, you know,",
    "start": "2403795",
    "end": "2409825"
  },
  {
    "text": "tightly, um, tightly grouped in some region, right? So the- the- um,",
    "start": "2409825",
    "end": "2416210"
  },
  {
    "text": "it is less swayed by the noise in the training set. That's one way to think of it, right?",
    "start": "2416520",
    "end": "2422035"
  },
  {
    "text": "In this case, it may have a systematic error. In this case, it may not have a systematic error, but these estimators are less swayed by the noise in the training set.",
    "start": "2422035",
    "end": "2430885"
  },
  {
    "text": "Whereas these- these two models are pretty swa- uh, uh, they are sensitive to noise in- in the training set.",
    "start": "2430885",
    "end": "2437950"
  },
  {
    "text": "So, um, depending on the specific set of N examples,",
    "start": "2437950",
    "end": "2443515"
  },
  {
    "text": "you know, the- the variance in the estimated values are pretty large, right?",
    "start": "2443515",
    "end": "2448914"
  },
  {
    "text": "Any questions on this? Yes. [inaudible].",
    "start": "2448915",
    "end": "2454360"
  },
  {
    "text": "Yes, so I'll come to the trade off, why we call it a trade off. Um, what we- what we then notice is,",
    "start": "2454360",
    "end": "2464065"
  },
  {
    "text": "instead of N examples, let's say we collect a much larger set of,",
    "start": "2464065",
    "end": "2470050"
  },
  {
    "text": "say N. Which is a much larger training set,",
    "start": "2470050",
    "end": "2475770"
  },
  {
    "text": "say a thousand times the size, right? So x^1, y^1, all the",
    "start": "2475770",
    "end": "2482190"
  },
  {
    "text": "way till x^N, y^N.",
    "start": "2482190",
    "end": "2490810"
  },
  {
    "text": "If you increase your dataset by a- a- a large fraction, uh- uh, by a large amount, what we observe is that in all the four cases,",
    "start": "2490810",
    "end": "2500484"
  },
  {
    "text": "the variance will come down. Right? So- so with a large- larger dataset,",
    "start": "2500485",
    "end": "2509500"
  },
  {
    "text": "so this is a, b,",
    "start": "2509500",
    "end": "2515090"
  },
  {
    "text": "c, d. Let's say these were the true values.",
    "start": "2515090",
    "end": "2520550"
  },
  {
    "text": "Yes. So instead of n number of samples, we are repeating this- this, uh,",
    "start": "2528390",
    "end": "2535270"
  },
  {
    "text": "cycle of experiments with N examples in each experiment. What we see is that the variance is gonna come down,",
    "start": "2535270",
    "end": "2545690"
  },
  {
    "text": "and in this case- All",
    "start": "2545700",
    "end": "2552020"
  },
  {
    "text": "right, so this is with N examples and this was with n examples.",
    "start": "2562230",
    "end": "2569150"
  },
  {
    "text": "So what we see is that this is still has some bias left,",
    "start": "2574260",
    "end": "2579445"
  },
  {
    "text": "but the variance has reduced much more, right? And in this case, in all the cases, the variance has come down, right?",
    "start": "2579445",
    "end": "2585835"
  },
  {
    "text": "But the, the- um, these two still remain unbiased. These two still remain biased.",
    "start": "2585835",
    "end": "2593570"
  },
  {
    "text": "And as n is increasing, we see that the bias is also kind of reducing as n increased from here to here.",
    "start": "2594690",
    "end": "2606640"
  },
  {
    "text": "And that's a pretty, [NOISE] excuse me. That's a pretty common [NOISE] phenomenon as well. As you increase- as you increase n your- your, um,",
    "start": "2606640",
    "end": "2619135"
  },
  {
    "text": "variants will almost always come down, and the, um- um,",
    "start": "2619135",
    "end": "2625180"
  },
  {
    "text": "bias also generally also, um, comes down. So-",
    "start": "2625180",
    "end": "2630260"
  },
  {
    "text": "so in- in a- in a classical statistical setting, the expected difference between Theta hat- so the expectation of",
    "start": "2640680",
    "end": "2648460"
  },
  {
    "text": "Theta hat is gonna be them- the- the center of your sampling distribution.",
    "start": "2648460",
    "end": "2653994"
  },
  {
    "text": "Right? So in- in, um, in classical statistical setting,",
    "start": "2653995",
    "end": "2660385"
  },
  {
    "text": "the expectation of Theta hat minus Theta star is called the bias. If the- the mean of the sampling distribution is centered exactly at Theta star,",
    "start": "2660385",
    "end": "2669820"
  },
  {
    "text": "then the bias will be 0, right? And the variance of Theta hat is called the variance, right?",
    "start": "2669820",
    "end": "2682915"
  },
  {
    "text": "And if the- as",
    "start": "2682915",
    "end": "2690685"
  },
  {
    "text": "n tends to infinity,",
    "start": "2690685",
    "end": "2694670"
  },
  {
    "text": "so if the bias tends to 0 as n tends to infinity,",
    "start": "2697530",
    "end": "2703555"
  },
  {
    "text": "then this is called a consistent estimator.",
    "start": "2703555",
    "end": "2707720"
  },
  {
    "text": "Right. So what does it mean? As- as, um- as the number of examples, um,",
    "start": "2712690",
    "end": "2721310"
  },
  {
    "text": "goes to, um- goes to infinity, the variance, you know, will generally come down.",
    "start": "2721310",
    "end": "2726740"
  },
  {
    "text": "But also if the bias also goes to 0, as n tends to infinity, then it's called a consistent estimator.",
    "start": "2726740",
    "end": "2732905"
  },
  {
    "text": "For example, um, in- in case of Gaussians, the maximum likelihood estimator for the variance, um,",
    "start": "2732905",
    "end": "2741695"
  },
  {
    "text": "so Theta Sigma square hat is equal to 1 over n,",
    "start": "2741695",
    "end": "2748850"
  },
  {
    "text": "equals 1 to n x^i minus Mu hat i square.",
    "start": "2748850",
    "end": "2758360"
  },
  {
    "text": "So this is the- if you recall, in case of the Gaussians, the maximum likelihood estimator for the variance parameter is this,",
    "start": "2758360",
    "end": "2767434"
  },
  {
    "text": "where Mu hat is the estimated, uh, mean. This is a biased estimator.",
    "start": "2767435",
    "end": "2775445"
  },
  {
    "text": "This will always systematically underestimate the bias, right? But it's also consistent, which means,",
    "start": "2775445",
    "end": "2781880"
  },
  {
    "text": "as n tends to infinity, the bias vanishes. Question? Yes. Do you mean x hat or Mu hat?",
    "start": "2781880",
    "end": "2789560"
  },
  {
    "text": "Yeah, x^i minus Mu hat. Oh, so- sorry, there's no i there.",
    "start": "2789560",
    "end": "2795030"
  },
  {
    "text": "So can you say why it's underestimating? I won't go into details as to why it's underestimating,",
    "start": "2795730",
    "end": "2802760"
  },
  {
    "text": "but, uh, the- the- the general idea is that, um, so Sigma hat square,",
    "start": "2802760",
    "end": "2810860"
  },
  {
    "text": "if it were to be 1 over n equals 1 to n x^i minus Mu.",
    "start": "2810860",
    "end": "2819905"
  },
  {
    "text": "So if this- if you use the true Mu value, then it's an unbiased estimator, but if you use the estimated Mu hat,",
    "start": "2819905",
    "end": "2825845"
  },
  {
    "text": "then this is a biased estimator. And then there are ways to, you know, um- if you use n minus 1,",
    "start": "2825845",
    "end": "2831650"
  },
  {
    "text": "there is a correction, uh, you can apply to make it unbiased. [NOISE] So this would be,",
    "start": "2831650",
    "end": "2839735"
  },
  {
    "text": "uh, an unbiased, uh, estimator. So this is unbiased. This is biased. [NOISE] And this is again unbiased.",
    "start": "2839735",
    "end": "2849110"
  },
  {
    "text": "[NOISE] And MLE will use this one.",
    "start": "2849110",
    "end": "2854825"
  },
  {
    "text": "If you- if you perform MLE on- on a Gaussian, you will get this, but that is biased. But it so happens that even though it is biased,",
    "start": "2854825",
    "end": "2861320"
  },
  {
    "text": "as n tends to infinity, the bias vanishes. So it is still a consistent estimator, right? If your bias vanishes as the number of examples goes to infinity,",
    "start": "2861320",
    "end": "2869120"
  },
  {
    "text": "then it's consistent. Yes, question? [inaudible] This one?",
    "start": "2869120",
    "end": "2874355"
  },
  {
    "text": "Yeah, like you model the second and called it MLE, is there a name for the last one? So this one is- is, uh- it's- it's generally called the,",
    "start": "2874355",
    "end": "2880580"
  },
  {
    "text": "uh, unbiased esti- estimator, where you apply this n- you know,",
    "start": "2880580",
    "end": "2887075"
  },
  {
    "text": "n by n minus 1 correction to get n minus 1 in place of n, and that becomes a- a bias.",
    "start": "2887075",
    "end": "2893674"
  },
  {
    "text": "But yeah, so, uh, a consistent estimator is one that eliminates the bias as- as the number of examples goes to infinity.",
    "start": "2893675",
    "end": "2901745"
  },
  {
    "text": "And you can also see, um- so that the limit as n tends to infinity,",
    "start": "2901745",
    "end": "2911278"
  },
  {
    "text": "variance of Theta hat. Um, the rate at which this goes to",
    "start": "2912100",
    "end": "2919970"
  },
  {
    "text": "zero is the- is also called the statistical efficiency.",
    "start": "2919970",
    "end": "2926520"
  },
  {
    "text": "All right? So how many examples does your estimator need to be really confident of the estimated,",
    "start": "2929620",
    "end": "2937880"
  },
  {
    "text": "uh- estimated, uh, parameter? Yes, question? In- in context of this, like,",
    "start": "2937880",
    "end": "2943685"
  },
  {
    "text": "biased variance idea, what exactly is it that MLE does? Because [inaudible] MLE have returned to that second one,",
    "start": "2943685",
    "end": "2953180"
  },
  {
    "text": "the n minus 1. This one? Yes. So the MLE is not always an unbiased estimator.",
    "start": "2953180",
    "end": "2959164"
  },
  {
    "text": "So what does the MLE do? MLE performs maximum likelihood. So write out the- the likelihood, take the gradient,",
    "start": "2959165",
    "end": "2966140"
  },
  {
    "text": "set it equal to 0, or do gradient descent and you- you end up with some parameter estimate. But, uh- so- so bias means that- that",
    "start": "2966140",
    "end": "2974660"
  },
  {
    "text": "the answer is consistently wrong. Is it centered around the wrong, uh- Yes, that's correct. That's correct.",
    "start": "2974660",
    "end": "2980599"
  },
  {
    "text": "-estimate. Yes. So why would that be my [inaudible] So the 1 over n- n minus 1,",
    "start": "2980600",
    "end": "2987095"
  },
  {
    "text": "that would be centered around the true, uh, Sigma squared. So shouldn't- wouldn't that be maximum likelihood?",
    "start": "2987095",
    "end": "2996830"
  },
  {
    "text": "So the question is, why isn't this the maximum likelihood estimate? Yeah. So maximum likelihood is, uh,",
    "start": "2996830",
    "end": "3002980"
  },
  {
    "text": "a recipe or a procedure for you to follow to construct an estimator, right? For- in case of Gaussian,",
    "start": "3002980",
    "end": "3009640"
  },
  {
    "text": "the maximum likelihood estimator for Mu is unbiased, but if you follow the same maximum likelihood recipe",
    "start": "3009640",
    "end": "3016135"
  },
  {
    "text": "and construct an estimator for Sigma, it is going to be biased. And there is no- there is no, um,",
    "start": "3016135",
    "end": "3023140"
  },
  {
    "text": "fundamental reason to tell when MLE is biased or unbiased. Sometimes it's biased, sometimes it's unbiased.",
    "start": "3023140",
    "end": "3030055"
  },
  {
    "text": "And how did you come up with that? With this one? The next one. This one. So that is,",
    "start": "3030055",
    "end": "3035725"
  },
  {
    "text": "uh- I can give you more details about this if you post it on Piazza, but that's not relevant to the discussion right now.",
    "start": "3035725",
    "end": "3041740"
  },
  {
    "text": "Uh, but I'm happy to give you more, uh, links to why this is, uh, unbiased. Anyway, that's- that's- the- the whole point of this was",
    "start": "3041740",
    "end": "3049810"
  },
  {
    "text": "to give you an example of a consistent estimator, right? As n tends to infinity, the, um,",
    "start": "3049810",
    "end": "3056529"
  },
  {
    "text": "the bias, uh, vanishes, right? And the rate at which, um, the variance reduces as you increase,",
    "start": "3056530",
    "end": "3064435"
  },
  {
    "text": "um- as you increase the number of examples is called the statistical efficiency.",
    "start": "3064435",
    "end": "3069610"
  },
  {
    "text": "And that could be something like at the rate of 1 over n or 1 over square root n. In different scenarios,",
    "start": "3069610",
    "end": "3076930"
  },
  {
    "text": "it's- it's, uh- you get different rates, right? So in the context- in the context of classical statistics,",
    "start": "3076930",
    "end": "3086740"
  },
  {
    "text": "this is bias and variance, because in statistics, we are interested in estimating parameters, right?",
    "start": "3086740",
    "end": "3093955"
  },
  {
    "text": "And- and our goal is to estimate a parameter and call it a day. Whereas, in machine learning, our goal was to make predictions on unseen examples.",
    "start": "3093955",
    "end": "3103045"
  },
  {
    "text": "In machine learning, our goal is generalization error, which means we are interested in how well",
    "start": "3103045",
    "end": "3110395"
  },
  {
    "text": "the error or how low the error of our model is on unseen examples, right? And- and so in- in machine learning,",
    "start": "3110395",
    "end": "3117400"
  },
  {
    "text": "we are now interested in this- in this- in the squared errors on- on seen examples, right?",
    "start": "3117400",
    "end": "3124045"
  },
  {
    "text": "And [NOISE] this was a good visualization for the statistical setting, uh,",
    "start": "3124045",
    "end": "3133434"
  },
  {
    "text": "where you- where you have a true unknown parameter and, you know, the estimator gives you a- uh, you know, a- a sampling distribution either centered on it or away from it,",
    "start": "3133435",
    "end": "3141895"
  },
  {
    "text": "either with the low variance or high variance. Similarly, we can also kind of,",
    "start": "3141895",
    "end": "3147775"
  },
  {
    "text": "uh, try to visualize, uh, this one to get a good sense of what bias and variance means in case of the squared error for a prediction problem.",
    "start": "3147775",
    "end": "3156700"
  },
  {
    "text": "So I'm just going to use the space right on top here. Maybe I'll push it up in case you want to",
    "start": "3156700",
    "end": "3163430"
  },
  {
    "text": "refer to it in the meantime, right?",
    "start": "3165210",
    "end": "3172089"
  },
  {
    "text": "[NOISE]",
    "start": "3172090",
    "end": "3178845"
  },
  {
    "text": "So to kind of understand- understand this, so assume we have a function.",
    "start": "3178845",
    "end": "3186420"
  },
  {
    "text": "So this is x, this is y. And let's say this is f of x.",
    "start": "3186420",
    "end": "3197710"
  },
  {
    "text": "All right. And the training data that we get are basically",
    "start": "3197750",
    "end": "3203000"
  },
  {
    "text": "samples of some x calculate the f of x,",
    "start": "3203000",
    "end": "3208335"
  },
  {
    "text": "and add some noise. So this could be, you know, similarly, you know, sample some other x, calculate f of x and add some noise.",
    "start": "3208335",
    "end": "3217450"
  },
  {
    "text": "So our training set might look like this.",
    "start": "3218030",
    "end": "3221710"
  },
  {
    "text": "Because that's our data generating distribution. Now pick some x- some x side. Choose some x side.",
    "start": "3224420",
    "end": "3230880"
  },
  {
    "text": "Evaluate f of x, add some noise. The noise is mean 0 so the noise could be positive or negative.",
    "start": "3230880",
    "end": "3236880"
  },
  {
    "text": "And that's the observed y, y^i. All right? And now say this was the- uh, the true distribution.",
    "start": "3236880",
    "end": "3249780"
  },
  {
    "text": "So let me instead make this a dotted line, because in general, we don't have access to y hat.",
    "start": "3249780",
    "end": "3258615"
  },
  {
    "text": "It's invisible to us. We don't know what a- what a f of x truly is, right?",
    "start": "3258615",
    "end": "3271395"
  },
  {
    "text": "And we take these this training set",
    "start": "3271395",
    "end": "3277390"
  },
  {
    "text": "and construct an estimator, say, using maximum likelihood, say, you know,",
    "start": "3282080",
    "end": "3288435"
  },
  {
    "text": "linear regression with a polynomial set of features. And let's say the- the resulting model that is specific to this training set",
    "start": "3288435",
    "end": "3296520"
  },
  {
    "text": "was something like this.",
    "start": "3296520",
    "end": "3302505"
  },
  {
    "text": "So the dotted line is the true f of x. And this line is f hat n of x, right?",
    "start": "3302505",
    "end": "3312870"
  },
  {
    "text": "So for a given x,",
    "start": "3312870",
    "end": "3318945"
  },
  {
    "text": "let's call it x star, test example, right? So for a given test example,",
    "start": "3318945",
    "end": "3327160"
  },
  {
    "text": "consider this vertical slice.",
    "start": "3327340",
    "end": "3332890"
  },
  {
    "text": "The true distribution of y is centered around the dotted line.",
    "start": "3332890",
    "end": "3341380"
  },
  {
    "text": "So this is- this is",
    "start": "3341600",
    "end": "3350760"
  },
  {
    "text": "the distribution of y equals f of x plus Epsilon, right?",
    "start": "3350760",
    "end": "3359385"
  },
  {
    "text": "And if this were to have been in our training set, then the corresponding y would have been a sample from this distribution, right?",
    "start": "3359385",
    "end": "3372750"
  },
  {
    "text": "Is- is that clear? If- if- if the distribution of y's for",
    "start": "3372750",
    "end": "3379260"
  },
  {
    "text": "this x star is the corresponding y-value that you might observe, right?",
    "start": "3379260",
    "end": "3385500"
  },
  {
    "text": "And if- if- if this- this x star was in our training set, then the corresponding y would have been a sample from this distribution, right?",
    "start": "3385500",
    "end": "3393810"
  },
  {
    "text": "And that distribution has a variance of Sigma squared.",
    "start": "3393810",
    "end": "3398290"
  },
  {
    "text": "This has variance Sigma squared, right?",
    "start": "3399290",
    "end": "3404685"
  },
  {
    "text": "And that's the variance of epsilon, right? Now, let me draw this part a little bigger over here.",
    "start": "3404685",
    "end": "3414585"
  },
  {
    "text": "So I'm just gonna focus on- this is f of",
    "start": "3414585",
    "end": "3423660"
  },
  {
    "text": "x and it has a distribution",
    "start": "3423660",
    "end": "3431189"
  },
  {
    "text": "for y and you are interested in x star, right?",
    "start": "3431189",
    "end": "3441680"
  },
  {
    "text": "Now, if we take n examples from our training set and construct an estimator,",
    "start": "3441680",
    "end": "3448204"
  },
  {
    "text": "that estimator, say estimator 1 for- from experiment 1 might go like this.",
    "start": "3448205",
    "end": "3456615"
  },
  {
    "text": "So this is from experiment 1.",
    "start": "3456615",
    "end": "3464020"
  },
  {
    "text": "We collect another set of n examples, fit our model from those n examples, and plot it.",
    "start": "3464540",
    "end": "3472965"
  },
  {
    "text": "That might look like- I don't know, this. This is experiment 2, right?",
    "start": "3472965",
    "end": "3482145"
  },
  {
    "text": "What are we- in- in- in this- in this example, each experiment resulted in a dot, right?",
    "start": "3482145",
    "end": "3488535"
  },
  {
    "text": "In this- in this setting, each experiment results in the corresponding, um, hypothesis that looks like that, and so on.",
    "start": "3488535",
    "end": "3496829"
  },
  {
    "text": "So as you keep repeating n, we will get different hypotheses.",
    "start": "3496830",
    "end": "3503825"
  },
  {
    "text": "And what this is measuring is that the expected difference between the point where the dotted line,",
    "start": "3503825",
    "end": "3512789"
  },
  {
    "text": "so this- this point over here is f of x.",
    "start": "3512790",
    "end": "3521040"
  },
  {
    "text": "And the points where the different hypotheses are crossing this line, each one of them are samples from f hat n of x.",
    "start": "3521040",
    "end": "3534850"
  },
  {
    "text": "This looks a little messy because unfortunately, I don't have different colors, but let me repeat what's- what's happening here.",
    "start": "3535460",
    "end": "3541755"
  },
  {
    "text": "So this- this vertical line is like the slice that's of interest to us because we are measuring the- what- what's happening at- when we predict that x star.",
    "start": "3541755",
    "end": "3550964"
  },
  {
    "text": "The dotted line that passes through it is the- the true function that we do not have access to,",
    "start": "3550965",
    "end": "3558255"
  },
  {
    "text": "which is why it's a- a dotted line. And this distribution represents the y-values that- from which we,",
    "start": "3558255",
    "end": "3569310"
  },
  {
    "text": "you know, from which observations are made for the, uh, um, for this specific, uh, x star.",
    "start": "3569310",
    "end": "3576464"
  },
  {
    "text": "For example, uh, y in the- in the test error would be a sample from,",
    "start": "3576465",
    "end": "3582585"
  },
  {
    "text": "uh, from the- from the y- corresponding y distribution or y given x distribution.",
    "start": "3582585",
    "end": "3587730"
  },
  {
    "text": "And because we are using different number of training examples or different sets of training examples to fit our model,",
    "start": "3587730",
    "end": "3595800"
  },
  {
    "text": "we get different hypotheses, uh, that are kind of near to the true dotted line.",
    "start": "3595800",
    "end": "3602640"
  },
  {
    "text": "So this- to the- to the true f of x, right? And these points where the different hypotheses cross",
    "start": "3602640",
    "end": "3611715"
  },
  {
    "text": "the vertical line are the different values of f hat n of x. Does that make sense?",
    "start": "3611715",
    "end": "3619125"
  },
  {
    "text": "Question. Yes. Question. So you're saying that we don't have access to the dotted line? Yeah. Do you usually use the average prediction you estimated?",
    "start": "3619125",
    "end": "3626040"
  },
  {
    "text": "Because how else do you- We don't know. So- so the question is, how do we know- how do we calculate the dotted line?",
    "start": "3626040",
    "end": "3631500"
  },
  {
    "text": "We don't know. So in this expression, um, that- that's a very good question. So the test error,",
    "start": "3631500",
    "end": "3636660"
  },
  {
    "text": "the expected test error, can be decomposed like this. And this is a mathematical expression, right?",
    "start": "3636660",
    "end": "3642675"
  },
  {
    "text": "We cannot compute these components, right? You, er, you know, you- you- you mentally,",
    "start": "3642675",
    "end": "3647699"
  },
  {
    "text": "you kind of break it down into these subcomponents. But it's very hard to actually",
    "start": "3647699",
    "end": "3653340"
  },
  {
    "text": "compute the two different components because we don't know what f of x is exactly, right?",
    "start": "3653340",
    "end": "3659415"
  },
  {
    "text": "And if we knew what f of x is then we could compute it, but then why even do machine learning? Because we already have access to f of x.",
    "start": "3659415",
    "end": "3665460"
  },
  {
    "text": "Right, but I've seen that every time we are in an experiment, we wanna measure our equipment or we wanna measure some in",
    "start": "3665460",
    "end": "3672810"
  },
  {
    "text": "the effect of different circumstances then we could always know the truth and as a result, we use that to measure the effect directly.",
    "start": "3672810",
    "end": "3679405"
  },
  {
    "text": "I'm just saying that that there is something unusual but generally speaking we don't have access to that. We don't yes, the, you know,",
    "start": "3679405",
    "end": "3685580"
  },
  {
    "text": "we don't know what f of x and that's- that's the, ah, uh, fundamental assumption. Next question? Based on the same question, if you want to like systematically know, er,",
    "start": "3685580",
    "end": "3694145"
  },
  {
    "text": "after repeating many experiments that- whether you are underfitting or overfitting can you redo,",
    "start": "3694145",
    "end": "3700640"
  },
  {
    "text": "let's say 1,000 examples, 1,000 different connections of fitting and then see on average,",
    "start": "3700640",
    "end": "3706715"
  },
  {
    "text": "if you take the average perimeter and then from that average perimeter this certain perimeter is",
    "start": "3706715",
    "end": "3712175"
  },
  {
    "text": "less than and then use that average of the same experiment as the true estimate?",
    "start": "3712175",
    "end": "3718730"
  },
  {
    "text": "So the question is, can we repeat the- repeat the experiment multiple times to guess whether we are systematically underestimating or overestimating?",
    "start": "3718730",
    "end": "3727985"
  },
  {
    "text": "From experimentally, it's very hard to- to calculate because we don't have access to the true y distribution.",
    "start": "3727985",
    "end": "3734869"
  },
  {
    "text": "If we had access to the true y distribution, if we could take an infinite number of samples from the true y given x,",
    "start": "3734870",
    "end": "3740795"
  },
  {
    "text": "then we could do such a comparison. But in- in general, it's- it's, er, hard to do. Anyway, so moving on.",
    "start": "3740795",
    "end": "3747099"
  },
  {
    "text": "So the- the way to think of, um, to think of this test error is that,",
    "start": "3747100",
    "end": "3754799"
  },
  {
    "text": "let's say we end up with this hypothesis and our prediction is over here, right?",
    "start": "3754800",
    "end": "3761180"
  },
  {
    "text": "And let's say the test example was also sampled from this y distribution. Say, maybe we sampled this value, right?",
    "start": "3761180",
    "end": "3769865"
  },
  {
    "text": "So this squared error- so the expectation of the square of this distance can be broken down into",
    "start": "3769865",
    "end": "3777110"
  },
  {
    "text": "the variance of- of the y- y given x distribution itself plus the systematic bias of,",
    "start": "3777110",
    "end": "3785510"
  },
  {
    "text": "er, whether our distribution of Fn, is it going to systematically be below f",
    "start": "3785510",
    "end": "3793730"
  },
  {
    "text": "star- below the dotted line or is it going to be above the line? The square of that plus the variance of f hat n x,",
    "start": "3793730",
    "end": "3801440"
  },
  {
    "text": "which means in general, do the different, er, how- how spread apart are the different hypotheses,",
    "start": "3801440",
    "end": "3809285"
  },
  {
    "text": "of the value of the hypothesis at that line? It's a little hard to visualize because, you know, er,",
    "start": "3809285",
    "end": "3815705"
  },
  {
    "text": "I'm using black color everywhere, but, er, but maybe you can sit down and kind of write it out yourself on your own book and- and,",
    "start": "3815705",
    "end": "3823235"
  },
  {
    "text": "um, understand this, right? So the square error- so the squared error is",
    "start": "3823235",
    "end": "3829940"
  },
  {
    "text": "the only error that allows for such a clean decomposition for other areas, for example, er, the logistic regression loss,",
    "start": "3829940",
    "end": "3837545"
  },
  {
    "text": "there is no well-accepted, you know, decomposition in this way. But there are a few papers which kind of unify this,",
    "start": "3837545",
    "end": "3845435"
  },
  {
    "text": "um, to other kinds of losses as- as well. And- and, um, I- I can share those links if anybody's interested.",
    "start": "3845435",
    "end": "3851570"
  },
  {
    "text": "But the- the idea here is that squared error can be decomposed into",
    "start": "3851570",
    "end": "3856850"
  },
  {
    "text": "these parts and you can think of this decomposition visually like this.",
    "start": "3856850",
    "end": "3862655"
  },
  {
    "text": "Unfortunately, this figure is a mess. And also kind of see the similarities with",
    "start": "3862655",
    "end": "3869750"
  },
  {
    "text": "the more classical statistical setting where the visualization is much easier, okay? What- what's happening over here in case of prediction is",
    "start": "3869750",
    "end": "3879260"
  },
  {
    "text": "actually very similar to what's happening over here if you were to kind of put them all in one- one single axis.",
    "start": "3879260",
    "end": "3886220"
  },
  {
    "text": "Maybe, maybe you can, er, do that on your own as an exercise. Right? So that's- that's biased various, er, decomposition.",
    "start": "3886220",
    "end": "3898190"
  },
  {
    "text": "The general takeaway from bias various- variance decomposition is that",
    "start": "3898190",
    "end": "3906050"
  },
  {
    "text": "underfitting roughly means you",
    "start": "3906050",
    "end": "3912620"
  },
  {
    "text": "have high bias, right?",
    "start": "3912620",
    "end": "3917930"
  },
  {
    "text": "Overfitting approximately means you have",
    "start": "3917930",
    "end": "3923420"
  },
  {
    "text": "high variance and these are approximate terms.",
    "start": "3923420",
    "end": "3931714"
  },
  {
    "text": "So, you know, you can have models that are- that have high bias and high variance which means your model can simultaneously",
    "start": "3931715",
    "end": "3939020"
  },
  {
    "text": "be underfitting and overfitting at the same time, right? It could mean that your model has fit to the noise of your data,",
    "start": "3939020",
    "end": "3945485"
  },
  {
    "text": "but just hasn't fit to the signal at all. So your model could theoretically be underfitting and overfitting at the same time, right?",
    "start": "3945485",
    "end": "3952714"
  },
  {
    "text": "And- and this is- this is just a- a- a heuristic or a rule of thumb to talk about how,",
    "start": "3952715",
    "end": "3961565"
  },
  {
    "text": "you know, how our model is- is performing question. Yes, question? You said that a model can be certainly",
    "start": "3961565",
    "end": "3968150"
  },
  {
    "text": "overfitting and underfitting at the same time if it's too organized. Don't we just call that overfitting?",
    "start": "3968150",
    "end": "3974025"
  },
  {
    "text": "So, er, overfitting, er, if it's overfitting and underfitting at the same time, don't call it just overfitting.",
    "start": "3974025",
    "end": "3980140"
  },
  {
    "text": "Um, no, no- if it's underfitting and overfitting at the same time, one example could be, let's say you're-",
    "start": "3980140",
    "end": "3987140"
  },
  {
    "text": "Is that the k equals 20 case? So in the k equals 20 case, it is overfitting and not underfitting, right?",
    "start": "3987140",
    "end": "3994250"
  },
  {
    "text": "So if you have data that is pretty noisy, right?",
    "start": "3994250",
    "end": "3999290"
  },
  {
    "text": "You could have, theoretically, you could have a model that might give you a hypothesis.",
    "start": "3999290",
    "end": "4005950"
  },
  {
    "text": "And if you take a different sample, the hypothesis might look very different. And it still doesn't fit your data at all.",
    "start": "4005950",
    "end": "4011650"
  },
  {
    "text": "It's just capturing the noise, but maybe hasn't captured the signal. So it's weighs a lot depending on the sample that you get.",
    "start": "4011650",
    "end": "4018745"
  },
  {
    "text": "But still it doesn't go through it so that it- it is kind of overfitting and underfitting at the same time.",
    "start": "4018745",
    "end": "4026569"
  },
  {
    "text": "But these are just heuristics, right? So what- what you, um, what you want to do in general,",
    "start": "4030690",
    "end": "4039010"
  },
  {
    "text": "um, what- what we've discussed so far is- is- is mostly for you to build a mental model of what's going on under the covers, okay?",
    "start": "4039010",
    "end": "4048160"
  },
  {
    "text": "But what you actually do in practice, the actions that you take, is what we're going to discuss next, and that's called cross-validation.",
    "start": "4048160",
    "end": "4055359"
  },
  {
    "text": "[NOISE]",
    "start": "4055360",
    "end": "4101680"
  },
  {
    "text": "So remember, our goal",
    "start": "4101680",
    "end": "4104930"
  },
  {
    "text": "is to do well on generalization, generalization error, right?",
    "start": "4106920",
    "end": "4114890"
  },
  {
    "text": "Of course in- in- in- in cases when the generalization error is measured as- as say the uh,",
    "start": "4119220",
    "end": "4126279"
  },
  {
    "text": "squared loss or the squared error. We can decompose it into uh, into bias and variance.",
    "start": "4126280",
    "end": "4132324"
  },
  {
    "text": "But in general, what we care about is that we just want to have low generalization error, right?",
    "start": "4132325",
    "end": "4137859"
  },
  {
    "text": "And the irreducible error is like a lower bound on how low the generalization error can be.",
    "start": "4137860",
    "end": "4144880"
  },
  {
    "text": "But you're- you're, um, but we- we- we still want to do as well as we can.",
    "start": "4144880",
    "end": "4153984"
  },
  {
    "text": "And the idea of measuring how well we are doing on generalization error.",
    "start": "4153985",
    "end": "4161545"
  },
  {
    "text": "So the idea behind how we do it is something very simple, we call it cross validation.",
    "start": "4161545",
    "end": "4167300"
  },
  {
    "text": "What that means is when you're given a dataset, we don't use our complete dataset to fit our model.",
    "start": "4169560",
    "end": "4177400"
  },
  {
    "text": "Instead, what we do is split it into a training set,",
    "start": "4177400",
    "end": "4186969"
  },
  {
    "text": "a dev set, and a test set, right? So you split it generally into three parts.",
    "start": "4186970",
    "end": "4192549"
  },
  {
    "text": "Into train, call it validation or dev set,",
    "start": "4192550",
    "end": "4199915"
  },
  {
    "text": "they are synonymous and a test set. And historically there have been um, you know,",
    "start": "4199915",
    "end": "4208120"
  },
  {
    "text": "um, a rule of thumbs that, you know this needs to be, you know, 70% and 20%,",
    "start": "4208120",
    "end": "4215349"
  },
  {
    "text": "10% or, you know, some people do it 60%, 20%,",
    "start": "4215350",
    "end": "4221320"
  },
  {
    "text": "20%, or 80%, 10%, 10%.",
    "start": "4221320",
    "end": "4226494"
  },
  {
    "text": "There is no single right answer. All right? And you split it into three different,",
    "start": "4226495",
    "end": "4232855"
  },
  {
    "text": "uh, three different uh, um, splits. And you fit your model on only the training set.",
    "start": "4232855",
    "end": "4241360"
  },
  {
    "text": "And you- you, uh, when you- when you do maximum likelihood,",
    "start": "4241360",
    "end": "4246670"
  },
  {
    "text": "you are doing it only on- on the training set, which means you're doing it on some fraction of the data.",
    "start": "4246670",
    "end": "4252175"
  },
  {
    "text": "And then you need to- you need to make uh, a decision of whether you want to increase your feature map size,",
    "start": "4252175",
    "end": "4260215"
  },
  {
    "text": "reduce your feature map size. Should you use logistic regression versus, you know, SVMs? Do you want to use neural networks that are all of these decisions that you need to make.",
    "start": "4260215",
    "end": "4268900"
  },
  {
    "text": "And broadly speaking, we're going to call them hyperparameters. Do you want to increase the number of layers?",
    "start": "4268900",
    "end": "4274240"
  },
  {
    "text": "Do you want to add regularization? We're gonna talk about regularization next, right? All these decisions, we call them hyperparameters.",
    "start": "4274240",
    "end": "4280824"
  },
  {
    "text": "And the way you go about deciding on what hyperparameters you want,",
    "start": "4280825",
    "end": "4286180"
  },
  {
    "text": "is by not measuring how well your model is doing on the training set.",
    "start": "4286180",
    "end": "4291400"
  },
  {
    "text": "But use the fitted model and make predictions on the validation or dev set and see how well your model is performing in those predictions, right?",
    "start": "4291400",
    "end": "4301990"
  },
  {
    "text": "You would expect you're training- your training error to be overoptimistic because the objective is to minimize the training error.",
    "start": "4301990",
    "end": "4310840"
  },
  {
    "text": "So there is- there is a systematic bias to um, um, your- that your training error will be uh,",
    "start": "4310840",
    "end": "4318295"
  },
  {
    "text": "lower than the generalization error. So to get a better estimate of how well we're doing on our- on our goal- our goal,",
    "start": "4318295",
    "end": "4326170"
  },
  {
    "text": "which again is to do well on generalization error, is to measure- measure it, you know, have a holdout set,",
    "start": "4326170",
    "end": "4332965"
  },
  {
    "text": "and pretend it is, you know, unseen examples that you're encountering in- in",
    "start": "4332965",
    "end": "4338500"
  },
  {
    "text": "production and see how well the model does on- on those examples. And it is common practice to repeat this cycle a multiple number of times.",
    "start": "4338500",
    "end": "4353275"
  },
  {
    "text": "So you- you- you take the training set- training data that you have. Start with some model,",
    "start": "4353275",
    "end": "4359775"
  },
  {
    "text": "say logistic regression, see how well it did on your validation set. You know, maybe you're happy,",
    "start": "4359775",
    "end": "4365969"
  },
  {
    "text": "maybe or not, very likely you will not be happy the first time. And say you- you go about increasing your feature map size.",
    "start": "4365970",
    "end": "4373695"
  },
  {
    "text": "You start with k equal to 1, you set it to k equal to 10, right?",
    "start": "4373695",
    "end": "4378775"
  },
  {
    "text": "And you fit your model again and measure the performance on the validation set with a new model which has you know,",
    "start": "4378775",
    "end": "4386755"
  },
  {
    "text": "a higher uh, a feature map. And if it has over-fit, your validation error is gonna go up.",
    "start": "4386755",
    "end": "4392219"
  },
  {
    "text": "Even though your training error came down, your validation error is going to go up if it has over-fit, right?",
    "start": "4392220",
    "end": "4397350"
  },
  {
    "text": "And you realize that you over-fit, and you- you know, take back your decision of going with k equals 10 and",
    "start": "4397350",
    "end": "4405010"
  },
  {
    "text": "maybe try k equals 2 or maybe k equals to 5, and so on. And the decision could be things like the size of your feature map.",
    "start": "4405010",
    "end": "4413830"
  },
  {
    "text": "It could be things like use neural networks versus logistic regression. It could be the number of layers in the neural network.",
    "start": "4413830",
    "end": "4421060"
  },
  {
    "text": "So no matter what your hyperparameter decision is, the process of cycling between the training set and a validation set is essential, right?",
    "start": "4421060",
    "end": "4432474"
  },
  {
    "text": "And this is- this is one of those things that is so pervasive and so",
    "start": "4432475",
    "end": "4437815"
  },
  {
    "text": "universal for all of machine learning that it is something you should- you should spend some time thinking about.",
    "start": "4437815",
    "end": "4443500"
  },
  {
    "text": "And- and it's- it's always good practice to follow. And that- that process of having",
    "start": "4443500",
    "end": "4451600"
  },
  {
    "text": "a holdout cross validation set against which you are measuring the quality of your hyperparameter tuning",
    "start": "4451600",
    "end": "4459100"
  },
  {
    "text": "is so universal that it works for any kind of hyper parameter in general, right? So it could be- it could be something as simple as,",
    "start": "4459100",
    "end": "4468025"
  },
  {
    "text": "you know, what is the regularization parameter? What is you know, choice of different algorithms.",
    "start": "4468025",
    "end": "4473515"
  },
  {
    "text": "All those- all those kinds of choices come under the same umbrella of being evaluated using this cross-validation approach, right?",
    "start": "4473515",
    "end": "4483895"
  },
  {
    "text": "Now, why do we have this test set? We- we- we had this uh, holdout validation set.",
    "start": "4483895",
    "end": "4490030"
  },
  {
    "text": "But why do we still have this test set? The reason is because just in the same way",
    "start": "4490030",
    "end": "4495880"
  },
  {
    "text": "how the training set gave us an over-optimistic estimate of our model's performance after",
    "start": "4495880",
    "end": "4503650"
  },
  {
    "text": "repeatedly performing hyperparameter tuning to do well- to do better on the validation set.",
    "start": "4503650",
    "end": "4511719"
  },
  {
    "text": "We've also kind of over-fit on our validation set as well, right? By repeating this cycle,",
    "start": "4511720",
    "end": "4517630"
  },
  {
    "text": "we are over-fitting on the validation set more and more, even though we do not explicitly minimize our training loss on this, right?",
    "start": "4517630",
    "end": "4526030"
  },
  {
    "text": "One way to think about it is the- when you obtain a dataset for the first time,",
    "start": "4526030",
    "end": "4533184"
  },
  {
    "text": "before you evaluate your model or fit your model on it, think of it as fresh. And every time you fit a model and look at its outcome,",
    "start": "4533185",
    "end": "4540655"
  },
  {
    "text": "or every time you use it as a validation set and look at the outcome.",
    "start": "4540655",
    "end": "4546520"
  },
  {
    "text": "Mentally, you need to think of that dataset as rotting. Every time you look at it, it rots a little bit. And- and the more number of times you- you run your model on",
    "start": "4546520",
    "end": "4555070"
  },
  {
    "text": "that dataset or you train your dataset on that model and look at the outcome, that dataset is rotting, right?",
    "start": "4555070",
    "end": "4560710"
  },
  {
    "text": "[NOISE] And to- to- as a way to kind of counter that rot effect,",
    "start": "4560710",
    "end": "4569844"
  },
  {
    "text": "you have this test set that you always keep away all the way until the very end.",
    "start": "4569845",
    "end": "4575545"
  },
  {
    "text": "All right? You may be going through this cycle, you know, a lot of number of times.",
    "start": "4575545",
    "end": "4581605"
  },
  {
    "text": "But all this while you need to keep your test set away and just not even look at your test set, right?",
    "start": "4581605",
    "end": "4586855"
  },
  {
    "text": "In the end, once you are satisfied with your performance, you know, in order to get a realistic expectation of your generalization error,",
    "start": "4586855",
    "end": "4597849"
  },
  {
    "text": "only for that purpose all the way- all- at the very end. You measured your- the- the performance of the model and the test set.",
    "start": "4597850",
    "end": "4605635"
  },
  {
    "text": "And that gives you- you know, a rough approximate of how well it is going to do in- in generalization error, right?",
    "start": "4605635",
    "end": "4611935"
  },
  {
    "text": "So the purpose of the training set and the validation set. So the purpose of the training set is to do well on",
    "start": "4611935",
    "end": "4620740"
  },
  {
    "text": "training set, slash minimize loss.",
    "start": "4620740",
    "end": "4627740"
  },
  {
    "text": "The purpose of the cross validation set, is to do well for generalization error- do well in generalization error.",
    "start": "4629880",
    "end": "4639770"
  },
  {
    "text": "The purpose of the test set, which strictly speaking, you need to evaluate your test set,",
    "start": "4644580",
    "end": "4651625"
  },
  {
    "text": "only ones all the way at the end, and never again is to give you- get",
    "start": "4651625",
    "end": "4658929"
  },
  {
    "text": "an estimate on",
    "start": "4658930",
    "end": "4667480"
  },
  {
    "text": "generalization error, right?",
    "start": "4667480",
    "end": "4675070"
  },
  {
    "text": "And in practice, because going through the cycle, you know, rots your validation set.",
    "start": "4675990",
    "end": "4683590"
  },
  {
    "text": "And over time, if you- if you repeat this- this cycle too many times,",
    "start": "4683590",
    "end": "4689860"
  },
  {
    "text": "your validation set is no longer, you know, a good sample of your uh,",
    "start": "4689860",
    "end": "4694915"
  },
  {
    "text": "of- of generalization performance. It is sometimes quite common to have multiple levels of validation sets, right?",
    "start": "4694915",
    "end": "4704080"
  },
  {
    "text": "You can have multiple validation sets where, you know, once you kind of uh, uh,",
    "start": "4704080",
    "end": "4709690"
  },
  {
    "text": "repeat the cycle with one validation set and you feel that you kind of, you know,",
    "start": "4709690",
    "end": "4714880"
  },
  {
    "text": "over fit on this validation set for your hyperparameters, it's quite common to discard a validation",
    "start": "4714880",
    "end": "4720190"
  },
  {
    "text": "set and move on to the next validation set, right? And that's- that's common too. And you wanna keep your test set all the way to the very end,",
    "start": "4720190",
    "end": "4727210"
  },
  {
    "text": "and- and the purpose of the test set is to give you an estimate of how well your generalization error will be at the end of the cycle. Yes, question.",
    "start": "4727210",
    "end": "4736030"
  },
  {
    "text": "[inaudible]",
    "start": "4736030",
    "end": "4746994"
  },
  {
    "text": "Yeah, so the question is, how do we know that we'll fit on our uh, validation set?",
    "start": "4746995",
    "end": "4752095"
  },
  {
    "text": "One uh, there is no- there's no clear answer to know how whether we would overfit on our validation set or not.",
    "start": "4752095",
    "end": "4761289"
  },
  {
    "text": "Uh, the- the one way you can- you know, check whether you will overfit on your validation set is",
    "start": "4761290",
    "end": "4768070"
  },
  {
    "text": "to- is to have yet another validation set and- and you're making your decisions again,",
    "start": "4768070",
    "end": "4775510"
  },
  {
    "text": "one validation set, but you could just measure the gap between the moderate performance on the validation set with which",
    "start": "4775510",
    "end": "4781990"
  },
  {
    "text": "you're making decisions versus some other validation set which you are using only for the purpose of detecting whether you fit on the first validation set.",
    "start": "4781990",
    "end": "4790630"
  },
  {
    "text": "[BACKGROUND] Yeah, so in general- in general, there is a when- when does the cycle end?",
    "start": "4790630",
    "end": "4797020"
  },
  {
    "text": "There is no well-defined answer for that, and-and uh, it- it is something that you- you use your judgment to say,",
    "start": "4797020",
    "end": "4804880"
  },
  {
    "text": "you know, you've done your best. Yes question.",
    "start": "4804880",
    "end": "4810550"
  },
  {
    "text": "[inaudible]",
    "start": "4810550",
    "end": "4816610"
  },
  {
    "text": "Yeah, that's a very good question. You know, you go through all of this. Finally, we measure our test set.",
    "start": "4816610",
    "end": "4822460"
  },
  {
    "text": "Uh, what do we do? The performance is really bad. And uh, strictly speaking, you know,",
    "start": "4822460",
    "end": "4830949"
  },
  {
    "text": "you need to use your test set only for the purpose of just getting an estimate of how well you're gonna do. Uh, but you know, it's a good question.",
    "start": "4830950",
    "end": "4837849"
  },
  {
    "text": "What you do if- if you know, if you don't do well on your test set and the, um, um, you know, in real life,",
    "start": "4837850",
    "end": "4844494"
  },
  {
    "text": "if you're trying to take something into production, you still want to do something about it. And for those purposes,",
    "start": "4844495",
    "end": "4851199"
  },
  {
    "text": "it's always good to have, you know, yet another test that you- you haven't looked at.",
    "start": "4851200",
    "end": "4856570"
  },
  {
    "text": "um, otherwise, you can always go through this and- and kind of you know, check your test set- test set performance again.",
    "start": "4856570",
    "end": "4863950"
  },
  {
    "text": "But in a way you're now starting to rock your test set now. Yeah, and- and- and that's like a fundamental paradox for which there's no right answer.",
    "start": "4863950",
    "end": "4873204"
  },
  {
    "text": "Next question. [BACKGROUND]",
    "start": "4873205",
    "end": "4899800"
  },
  {
    "text": "Yeah, so the question is, uh, can we fit the model on- on the 80% or some percent,",
    "start": "4899800",
    "end": "4907360"
  },
  {
    "text": "go through the cycle, measure it on the test set, now you're happy with everything, now,",
    "start": "4907360",
    "end": "4912415"
  },
  {
    "text": "stick to the same set of hyperparameters that you- that you used and refit the model on the full- full training set.",
    "start": "4912415",
    "end": "4921715"
  },
  {
    "text": "You can do that. If your dataset is, is reasonably large, then you know that might work well.",
    "start": "4921715",
    "end": "4927925"
  },
  {
    "text": "But- sometimes if your model is pretty sensitive to small variations in your training data,",
    "start": "4927925",
    "end": "4936295"
  },
  {
    "text": "then you may want to just hold onto the model that you have, which happens to be you know, working reasonably well, rather than- you know,",
    "start": "4936295",
    "end": "4943780"
  },
  {
    "text": "risking it and fitting it on the full dataset. Especially if your- if your model is kind of sensitive to noise in the dataset.",
    "start": "4943780",
    "end": "4951340"
  },
  {
    "text": "But you know uh, you can certainly do that in most cases. Uh, in- in practice,",
    "start": "4951340",
    "end": "4958239"
  },
  {
    "text": "people generally do not do that, you- you know uh, well, let me take that back, It depends on how big your dataset is, right?",
    "start": "4958240",
    "end": "4966610"
  },
  {
    "text": "If your dataset is reasonably large, say you have a million examples and you have some- you know, test set that you've kept apart, and you repeat this.",
    "start": "4966610",
    "end": "4974845"
  },
  {
    "text": "Let's say you kept away 1,000 examples, in your test set, and you repeat it, then there's not much value when you already, you know, you have like 999,000 um,",
    "start": "4974845",
    "end": "4985045"
  },
  {
    "text": "and you know there's little value in adding another 1,000 to refit it. But if you're in a smaller a regime where let's",
    "start": "4985045",
    "end": "4991870"
  },
  {
    "text": "say you have 100 examples and you've kept away 20, then it might be worth- worth doing it, good question.",
    "start": "4991870",
    "end": "4997615"
  },
  {
    "text": "All right, so this is- this is, uh, this kind of cross-validation is also called- is called hold-out cross validation.",
    "start": "4997615",
    "end": "5006284"
  },
  {
    "text": "Where you take a fixed validation and test set and keep it away and fit- fit your model only on the training set.",
    "start": "5006285",
    "end": "5015825"
  },
  {
    "text": "So this is called hold-out cross validation. There is another kind of cross validation technique called k-fold cross-validation.",
    "start": "5015825",
    "end": "5024820"
  },
  {
    "text": "And this is more common when you have small datasets.",
    "start": "5031880",
    "end": "5037929"
  },
  {
    "text": "So the idea behind k-fold cross-validation is to take your training set, right?",
    "start": "5039680",
    "end": "5048120"
  },
  {
    "text": "So x_1, y_1 till x_n y_n,",
    "start": "5048120",
    "end": "5057150"
  },
  {
    "text": "right, And split it into K folds,",
    "start": "5057150",
    "end": "5063195"
  },
  {
    "text": "In this case, k is 1, 2, 3, 4, right. And what you do is once you split it into K folds,",
    "start": "5063195",
    "end": "5072795"
  },
  {
    "text": "you fit k different models. Where for each model,",
    "start": "5072795",
    "end": "5078060"
  },
  {
    "text": "you take one of the folds as the validation set. Use the remaining folds- aggregate the remaining folds as your training set, right?",
    "start": "5078060",
    "end": "5088050"
  },
  {
    "text": "So for Model 1,",
    "start": "5088050",
    "end": "5091869"
  },
  {
    "text": "fold 1 is the validation set and fold 2 to n is the test set- is the training set.",
    "start": "5095000",
    "end": "5107010"
  },
  {
    "text": "There is no- you can- you can also have a separate test set that you don't uh, touch at all.",
    "start": "5107010",
    "end": "5114735"
  },
  {
    "text": "But it's quite common to use the full training set to perform uh, uh, k-fold cross-validation, right?",
    "start": "5114735",
    "end": "5120240"
  },
  {
    "text": "And for model 2, fold 2 is the validation set,",
    "start": "5120240",
    "end": "5130035"
  },
  {
    "text": "and fold 1, 3 to N,",
    "start": "5130035",
    "end": "5135075"
  },
  {
    "text": "is training set, right? So one model time,",
    "start": "5135075",
    "end": "5140489"
  },
  {
    "text": "pick a fold, make that your validation set. Use the remaining fold as your training set, right? And using the corresponding model,",
    "start": "5140490",
    "end": "5148919"
  },
  {
    "text": "make predictions on the corresponding validation set, So from model 1-model 1,",
    "start": "5148919",
    "end": "5157469"
  },
  {
    "text": "we use for model one we- this is the training set, right?",
    "start": "5157470",
    "end": "5163095"
  },
  {
    "text": "Using the training set, make predictions on it and get your predictions here,",
    "start": "5163095",
    "end": "5167920"
  },
  {
    "text": "Y hat from model 1.",
    "start": "5168140",
    "end": "5172720"
  },
  {
    "text": "And similarly for fold 2, take model 2.",
    "start": "5174050",
    "end": "5179744"
  },
  {
    "text": "Model 2 was trained on, these going to Model 2,",
    "start": "5179745",
    "end": "5186310"
  },
  {
    "text": "and using model 2 make predictions y hat from right,",
    "start": "5186770",
    "end": "5198030"
  },
  {
    "text": "and similarly for each model, make the prediction into the corresponding fold. And you're gonna get your full set of predictions of",
    "start": "5198030",
    "end": "5206100"
  },
  {
    "text": "y hats and your full true label wise. And using these two, you can measure, uh,",
    "start": "5206100",
    "end": "5213030"
  },
  {
    "text": "you know, you may have some kind of- some kind of uh, um, evaluation metrics such as, you know,",
    "start": "5213030",
    "end": "5219465"
  },
  {
    "text": "maybe accuracy may be squared, error may be area under the ROC curve. Um, and you can use these- you know,",
    "start": "5219465",
    "end": "5226350"
  },
  {
    "text": "the- the true labels and the predictions for the full dataset and see how well your model is doing.",
    "start": "5226350",
    "end": "5231600"
  },
  {
    "text": "Yes question. [BACKGROUND]",
    "start": "5231600",
    "end": "5241245"
  },
  {
    "text": "Training set. Yes. Right, so for model- model 2 make predictions on fold 2,",
    "start": "5241245",
    "end": "5248445"
  },
  {
    "text": "but model 2's training set is fold 1, fold 3 and all the way clear, right?",
    "start": "5248445",
    "end": "5253605"
  },
  {
    "text": "Everything except the corresponding fold is- is part of the training set, right? And this is called K-fold cross-validation.",
    "start": "5253605",
    "end": "5259800"
  },
  {
    "text": "And this is more commonly done when you're mod- your model is not very computationally expensive and your data's not very big.",
    "start": "5259800",
    "end": "5267600"
  },
  {
    "text": "And in fact, you can even take this to the limit where K equals number of examples, right?",
    "start": "5267600",
    "end": "5274695"
  },
  {
    "text": "So you have one model, per example, for which that example becomes the validation example,",
    "start": "5274695",
    "end": "5280620"
  },
  {
    "text": "and all the rest of the examples are part of the training set for that model. And that's al- also called leave-one-out cross validation,",
    "start": "5280620",
    "end": "5288730"
  },
  {
    "text": "cross-validation where K equals n. Yes, question.",
    "start": "5291290",
    "end": "5298725"
  },
  {
    "text": "In this case there is no distinction between validation set and test set, right? [inaudible] your. Yeah, so the question is- is there",
    "start": "5298725",
    "end": "5304830"
  },
  {
    "text": "a di- uh- uh- uh a difference between validation set and - and, uh, test set. And um- you can still have a separate test set that is not part of this K- fold process.",
    "start": "5304830",
    "end": "5316665"
  },
  {
    "text": "You can still have a- a- um- um- um, a test set and that k-means error. But most- most of the times when you",
    "start": "5316665",
    "end": "5325230"
  },
  {
    "text": "do K- fold cross-validation or leave-one-out cross-validation, you're doing it because your data set is really small.",
    "start": "5325230",
    "end": "5332010"
  },
  {
    "text": "Let's say you have just 20 examples. So let's say you're- you have a data set of some very rare disease, you know, for which the number of patients is just a very small fraction.",
    "start": "5332010",
    "end": "5340200"
  },
  {
    "text": "And you want to build a model in that. In those cases, doing this would be a bad idea because, you know, you have only 20 examples you don't want to lose,",
    "start": "5340200",
    "end": "5347280"
  },
  {
    "text": "let's say 4 of those 20, just for the purpose of cross-validation. Uh- and in those cases,",
    "start": "5347280",
    "end": "5352530"
  },
  {
    "text": "you would do leave-one-out or- or, um, K-fold cross validation because, you know,",
    "start": "5352530",
    "end": "5357810"
  },
  {
    "text": "you want to kind of use your- all the data that you have to build a model in some way and not leave some expensive data.",
    "start": "5357810",
    "end": "5364395"
  },
  {
    "text": "In those cases, you generally would not have a separate test set. And um- you would just uh- add- add- and you do cross validation like this.",
    "start": "5364395",
    "end": "5373305"
  },
  {
    "text": "And then you could either just use the ensemble of all the K models and average their predictions at test time.",
    "start": "5373305",
    "end": "5380680"
  },
  {
    "text": "Or you can use this- use this procedure only for the purpose",
    "start": "5380680",
    "end": "5385790"
  },
  {
    "text": "of deciding the hyper-parameters and take those hyper-parameters and just refit using the entire data set.",
    "start": "5385790",
    "end": "5391645"
  },
  {
    "text": "And- and both- both of those are- are commonly done.",
    "start": "5391645",
    "end": "5396810"
  },
  {
    "text": "So, what's validation error of your test error, right? In that case. Yeah, in that case you- you can think of",
    "start": "5396810",
    "end": "5402090"
  },
  {
    "text": "your validation error and test error to be the same. Uh- though this process, you know, um- yeah yo- you.",
    "start": "5402090",
    "end": "5409185"
  },
  {
    "text": "This- this cycle does not fit well with this procedure.",
    "start": "5409185",
    "end": "5415680"
  },
  {
    "text": "Yeah. Right? Any- any questions on this,",
    "start": "5415680",
    "end": "5421020"
  },
  {
    "text": "before we move on to regularization.",
    "start": "5421020",
    "end": "5424120"
  },
  {
    "text": "All right.",
    "start": "5430850",
    "end": "5433270"
  },
  {
    "text": "Yes question. [inaudible]",
    "start": "5445010",
    "end": "5455909"
  },
  {
    "text": "So is the- you know, um, is there a good reason not to uh, uh, sample from your um, um, training set itself?",
    "start": "5455910",
    "end": "5466815"
  },
  {
    "text": "Uh- um. So there is- there's a process where you take your data set and, you know, sample from it and fit a model and, you know, do it again.",
    "start": "5466815",
    "end": "5475080"
  },
  {
    "text": "And that, that's called bootstrap. However, bootstrap is generally used for um,",
    "start": "5475080",
    "end": "5482325"
  },
  {
    "text": "bootstrap is- is generally used to obtain um- what -what we saw previously as",
    "start": "5482325",
    "end": "5487440"
  },
  {
    "text": "the sampling distribution for getting an -an- uh- a distribution over your parameters, right?",
    "start": "5487440",
    "end": "5493095"
  },
  {
    "text": "And uh, it's not commonly used in machine learning where the goal is to do prediction on unseen examples.",
    "start": "5493095",
    "end": "5501480"
  },
  {
    "text": "Uh- it's, it's more of a technique to get um- uncertainty estimates or confidence intervals on your model parameters.",
    "start": "5501480",
    "end": "5510584"
  },
  {
    "text": "And that's more commonly used in- in- in more classical statistics settings where you want to get confidence intervals on your parameters.",
    "start": "5510584",
    "end": "5517830"
  },
  {
    "text": "Um, it doesn't- it doesn't help us a lot in terms of getting generalization error estimates.",
    "start": "5517830",
    "end": "5527365"
  },
  {
    "text": "All right.",
    "start": "5527365",
    "end": "5533925"
  },
  {
    "text": "Regularization. So first let's,",
    "start": "5533925",
    "end": "5543929"
  },
  {
    "text": "uh- we gonna have a quick, um- a",
    "start": "5543930",
    "end": "5554280"
  },
  {
    "text": "quick motivation for regularization. An-n-n-d, and in- in this lecture we will cover regularization from a Bayesian perspective.",
    "start": "5554280",
    "end": "5562200"
  },
  {
    "text": "Um. So if you remember, um- in the case of uh- K equals 20 in linear regression, right?",
    "start": "5562200",
    "end": "5572400"
  },
  {
    "text": "x and y, where K is- was the degree of polynomials. You got a hyper, there's this function that is very wiggly.",
    "start": "5574220",
    "end": "5583005"
  },
  {
    "text": "Right? And if you look at the coefficients of the,",
    "start": "5583005",
    "end": "5591344"
  },
  {
    "text": "uh- uh- the resulting Theta vector that corresponds to this very wiggly hypothesis,",
    "start": "5591345",
    "end": "5597045"
  },
  {
    "text": "you will notice that in order to get these very sharp wiggles,",
    "start": "5597045",
    "end": "5602354"
  },
  {
    "text": "the Theta- the magnitude of Theta i- Theta i should be large.",
    "start": "5602354",
    "end": "5608535"
  },
  {
    "text": "Right? If all your theta i values are small, then the resulting hypothesis,",
    "start": "5608535",
    "end": "5614625"
  },
  {
    "text": "even for K equals 20 will be something that is much smoother. Right? So it is these large values of Theta that",
    "start": "5614625",
    "end": "5622830"
  },
  {
    "text": "makes for these highly wiggly turns in- in the hypothesis function.",
    "start": "5622830",
    "end": "5628335"
  },
  {
    "text": "And the general intuition is that um- having something so wiggly is bad.",
    "start": "5628335",
    "end": "5635010"
  },
  {
    "text": "You know, that's the general intuition, because it's very unlikely that uh- you know,",
    "start": "5635010",
    "end": "5640200"
  },
  {
    "text": "x's that are, that are so nearby would have very, you know, would have y values that are differing so much,",
    "start": "5640200",
    "end": "5647355"
  },
  {
    "text": "it's more likely that you're gonna have models where your hypothesis is much smoother, where nearby values of x have nearby values of y.",
    "start": "5647355",
    "end": "5656039"
  },
  {
    "text": "And using this intuition, we want to come up with ways in which",
    "start": "5656040",
    "end": "5661724"
  },
  {
    "text": "the- our estimator will output small values of Theta.",
    "start": "5661725",
    "end": "5666870"
  },
  {
    "text": "You still want to have the flexibility of, you know, K equals 20 and have- have the ability to fit our Theta if necessary.",
    "start": "5666870",
    "end": "5676500"
  },
  {
    "text": "But at the same time, we also want to keep the magnitude of each of the Theta i to also",
    "start": "5676500",
    "end": "5681720"
  },
  {
    "text": "be kind of as small as possible because that encourages smoother hypotheses. Right? So the- the- the technique",
    "start": "5681720",
    "end": "5691335"
  },
  {
    "text": "of encouraging",
    "start": "5691335",
    "end": "5702180"
  },
  {
    "text": "small values of theta is,",
    "start": "5702180",
    "end": "5709740"
  },
  {
    "text": "you know, think of it as regularization. You know, how can we kind of,",
    "start": "5709740",
    "end": "5718155"
  },
  {
    "text": "um- not let the noise in the data sway our fit hypothesis to this extent and- and yet,",
    "start": "5718155",
    "end": "5727934"
  },
  {
    "text": "you know, still be flexible to some degree, but encourage smoothness in the resulting hypothesis.",
    "start": "5727935",
    "end": "5734280"
  },
  {
    "text": "That's the goal of regularization. And the goal of regularization is almost actually, you know,",
    "start": "5734280",
    "end": "5740640"
  },
  {
    "text": "stated in- in- in the statement itself that we want small values of Theta. right? Yes question?",
    "start": "5740640",
    "end": "5747255"
  },
  {
    "text": "Why would you want to introduce the- are you saying the normal Theta or just the number of dimensions- divisions to reduce the dimensions there?",
    "start": "5747255",
    "end": "5757245"
  },
  {
    "text": "So the- in- in regularization, we reduce the- the values of- taken on by each component.",
    "start": "5757245",
    "end": "5764460"
  },
  {
    "text": "So regularization. Um. So yeah, so we want to make the values of theta small.",
    "start": "5764460",
    "end": "5770280"
  },
  {
    "text": "[inaudible]",
    "start": "5770280",
    "end": "5777800"
  },
  {
    "text": "Yeah. Then we solve them making the dimensions of theta itself smaller. [inaudible]",
    "start": "5777800",
    "end": "5784370"
  },
  {
    "text": "Yeah. So that problem would be solved by reducing the- Yeah, so- so we could- so the question is,",
    "start": "5784370",
    "end": "5790920"
  },
  {
    "text": "we can solve this by reducing the number of Thetas that we have, right?",
    "start": "5790920",
    "end": "5797760"
  },
  {
    "text": "And, you know, the- so wha- what- why are we trying to reduce the value, the- the magnitude of Theta rather than just reducing the dimensionality of Theta, right?",
    "start": "5797760",
    "end": "5806190"
  },
  {
    "text": "And tha- that's- that's a valid approach too, and we'll see that one of the regularization techniques, you know, achieves that goal.",
    "start": "5806190",
    "end": "5812114"
  },
  {
    "text": "Um, so- so- so the intuition there is if- if your Theta_i equals 0 for some i,",
    "start": "5812115",
    "end": "5817680"
  },
  {
    "text": "then you implicitly kind of reduce the number of Thetas. And- and we- we- we're going to cover all these, uh, shortly.",
    "start": "5817680",
    "end": "5823380"
  },
  {
    "text": "Right? So in general, the- the intuition is that smaller values of Theta_i.",
    "start": "5823380",
    "end": "5828664"
  },
  {
    "text": "So this is bad. We want Theta_i's to be- [NOISE] to be small.",
    "start": "5828665",
    "end": "5837435"
  },
  {
    "text": "So, uh, one way to do it is to- is to penalize large values of Theta.",
    "start": "5837435",
    "end": "5843330"
  },
  {
    "text": "Okay? And what that means is, uh, in case of linear regression, if our cost function, J of Theta,",
    "start": "5843330",
    "end": "5849540"
  },
  {
    "text": "it was i equals 1 to n,",
    "start": "5849540",
    "end": "5854500"
  },
  {
    "text": "y^i minus h Theta of x^i squared.",
    "start": "5860210",
    "end": "5868230"
  },
  {
    "text": "So this was standard linear regression where h Theta of x was,",
    "start": "5868230",
    "end": "5873730"
  },
  {
    "text": "um, [NOISE] was just Theta transpose x. Right? So we take this standard loss of linear regression and kind of add on",
    "start": "5874580",
    "end": "5887490"
  },
  {
    "text": "this extra constraint by penalizing- [NOISE] by penalizing the norm of the Theta vector.",
    "start": "5887490",
    "end": "5900690"
  },
  {
    "text": "The idea here is that this extra term in the loss function",
    "start": "5900690",
    "end": "5906675"
  },
  {
    "text": "will penalize the model for selecting Theta I's that are very large and therefore,",
    "start": "5906675",
    "end": "5912974"
  },
  {
    "text": "give us very wiggly hypotheses. Okay? And these themes- um,",
    "start": "5912975",
    "end": "5921710"
  },
  {
    "text": "and- and- what- so what is the value of- of-of- of Lambda in this case? So Lambda is some coefficient or weighting factor of",
    "start": "5921710",
    "end": "5929699"
  },
  {
    "text": "how much we care about the original objective versus how much we care about having small Thetas.",
    "start": "5929700",
    "end": "5935100"
  },
  {
    "text": "So if Lambda is very big, then the- the model will focus more on",
    "start": "5935100",
    "end": "5941970"
  },
  {
    "text": "making the values of Theta small and might not even fit our- uh, fit the data at all.",
    "start": "5941970",
    "end": "5948014"
  },
  {
    "text": "If Lambda is 0, then the model will just fit our data as the,",
    "start": "5948015",
    "end": "5955350"
  },
  {
    "text": "uh, original objective without any regularization, right? So Lambda acts as this- this, uh, um,",
    "start": "5955350",
    "end": "5961695"
  },
  {
    "text": "relative weighing factor between the original data fitting objective and the regularization objective.",
    "start": "5961695",
    "end": "5968040"
  },
  {
    "text": "And the way we go about, you know, tuning the value of Lambda is through?",
    "start": "5968040",
    "end": "5974140"
  },
  {
    "text": "Cross-validation, exactly. So use cross-validation to figure out what",
    "start": "5974660",
    "end": "5980340"
  },
  {
    "text": "Lambda value works well for- on your validation set.",
    "start": "5980340",
    "end": "5986010"
  },
  {
    "text": "Okay? And- um, and refit the model. So this- this is, um,",
    "start": "5986010",
    "end": "5992039"
  },
  {
    "text": "um, loosely speaking, this is a regularization. A few things to- um,",
    "start": "5992040",
    "end": "5998860"
  },
  {
    "text": "few things to observe, uh, is that the norm that we used here was the 2-norm.",
    "start": "5999350",
    "end": "6005840"
  },
  {
    "text": "You can also use [NOISE] the 1-norm.",
    "start": "6005840",
    "end": "6012125"
  },
  {
    "text": "Right? And the 1-norm is basically- uh, so this is basically [NOISE] Lambda times i equals 1 to d Theta i.",
    "start": "6012125",
    "end": "6022505"
  },
  {
    "text": "Okay? It's just the sum of the absolute values. Yes, question? Why not learn Lambda using MLE? So the question is, why not learn Lambda using MLE?",
    "start": "6022505",
    "end": "6031700"
  },
  {
    "text": "And the- so- so by definition, MLE will- um, will- will give you a zero Lambda I think.",
    "start": "6031700",
    "end": "6041450"
  },
  {
    "text": "Granted because our goal is to, uh, minimize this loss, this is always non-negative.",
    "start": "6041450",
    "end": "6047585"
  },
  {
    "text": "So the way to minimize this is to just set Lambda to 0. So MLE will give you Lambda equals 0. Yes, question?",
    "start": "6047585",
    "end": "6055310"
  },
  {
    "text": "So it is- it's still not clear to me why that's- uh, why- why do we- why that- that is- uh,",
    "start": "6055310",
    "end": "6062180"
  },
  {
    "text": "like, why that is a correct one to have to minimize the norm of Theta? How- how is that- so am I correct to say that we want to",
    "start": "6062180",
    "end": "6069920"
  },
  {
    "text": "minimize the norm of Theta [NOISE] because we want to not overfit? I mean, reducing Theta reduces overfitting?",
    "start": "6069920",
    "end": "6077360"
  },
  {
    "text": "Yeah. So- so the question is, why- why does reducing the norm of Theta, um, help- um, um,",
    "start": "6077360",
    "end": "6082700"
  },
  {
    "text": "help or fight overfitting? So the norm of the Theta, if you remember, um, this is equal to sum over i equals 1 to d Theta_i squared, right?",
    "start": "6082700",
    "end": "6095120"
  },
  {
    "text": "So the- the idea that if you want to have small values of Theta and you want, you know, all your values of Theta to be small,",
    "start": "6095120",
    "end": "6102455"
  },
  {
    "text": "sum over the squared values. And, you know, that's- that's the- um, uh, that's the general idea, right?",
    "start": "6102455",
    "end": "6109969"
  },
  {
    "text": "So, you know, if- if- if all your Thetas are- are reasonably small, then you get, uh, uh, a smoother function.",
    "start": "6109970",
    "end": "6116045"
  },
  {
    "text": "So, you know, penalized- penalize those values. Okay? So, um, however,",
    "start": "6116045",
    "end": "6122240"
  },
  {
    "text": "this seems a little arbitrary. Uh, I mean, you know, it- it seems a little hacky. We just- we knew- we- we use our intuition to that large values of Theta,",
    "start": "6122240",
    "end": "6131195"
  },
  {
    "text": "kind of, costs overfitting so let's directly penalize it, which- which seems kind of a little hacky. But however, regularization has a very- um,",
    "start": "6131195",
    "end": "6140495"
  },
  {
    "text": "very nice Bayesian interpretation, right? If you remember- [NOISE]",
    "start": "6140495",
    "end": "6155510"
  },
  {
    "text": "if you remember, in the- in the Bayesian setting, we had Theta come from some prior distribution.",
    "start": "6155510",
    "end": "6161510"
  },
  {
    "text": "[NOISE] Right? And your data, I'm just going to call it S for both x and y's.",
    "start": "6161510",
    "end": "6168065"
  },
  {
    "text": "So S, uh, given Theta is something which is called the likelihood.",
    "start": "6168065",
    "end": "6173449"
  },
  {
    "text": "[NOISE] And we construct the posterior,",
    "start": "6173450",
    "end": "6178670"
  },
  {
    "text": "[NOISE] it's called the posterior.",
    "start": "6178670",
    "end": "6187400"
  },
  {
    "text": "[NOISE] Right? And in the Bayesian setting, all we did was construct the posterior distribution.",
    "start": "6187400",
    "end": "6194645"
  },
  {
    "text": "So p of Theta given S is equal to P of S",
    "start": "6194645",
    "end": "6200960"
  },
  {
    "text": "given Theta times P of Theta over P of S. Right?",
    "start": "6200960",
    "end": "6208055"
  },
  {
    "text": "And this gives us a full distribution over Theta, and we use this, uh,",
    "start": "6208055",
    "end": "6214655"
  },
  {
    "text": "distribution into test time to construct the posterior predictor distribution of- for constructing P of Y star given X star,",
    "start": "6214655",
    "end": "6225515"
  },
  {
    "text": "comma, X, y was equal to the expectation of Theta,",
    "start": "6225515",
    "end": "6231680"
  },
  {
    "text": "let's call it Theta hat sample from P of Theta given S of P of y given- y",
    "start": "6231680",
    "end": "6240650"
  },
  {
    "text": "star given X star times P,",
    "start": "6240650",
    "end": "6248219"
  },
  {
    "text": "yeah, Theta hat. Okay? So we- we constructed",
    "start": "6248320",
    "end": "6253880"
  },
  {
    "text": "the posterior predictive distribution from- from the posterior distribution. [NOISE] However, only in simple models is this integral tractable.",
    "start": "6253880",
    "end": "6264530"
  },
  {
    "text": "In most cases, this integ- this integral is not tractable. It's very hard to perform this integration to",
    "start": "6264530",
    "end": "6270920"
  },
  {
    "text": "construct a posterior predictive distribution, let alone the posterior predictive distribution,",
    "start": "6270920",
    "end": "6275990"
  },
  {
    "text": "it's very hard to construct even the posterior distribution with the parameters itself. In simple models, it's possible.",
    "start": "6275990",
    "end": "6281255"
  },
  {
    "text": "We saw that in Bayesian linear regression, it's possible. But in a lot of other models, constructing this posterior is extremely hard.",
    "start": "6281255",
    "end": "6288905"
  },
  {
    "text": "So what's done in practice is,",
    "start": "6288905",
    "end": "6294065"
  },
  {
    "text": "and this is something that you have on PS2 as well, is something that's called MAP or",
    "start": "6294065",
    "end": "6303179"
  },
  {
    "text": "maximum a posteriori parameter estimate.",
    "start": "6303790",
    "end": "6320855"
  },
  {
    "text": "Okay? So what we instead do is take our poste- posterior distribution.",
    "start": "6320855",
    "end": "6328160"
  },
  {
    "text": "[NOISE] All right? This is now a distribution over your parameters.",
    "start": "6328160",
    "end": "6335224"
  },
  {
    "text": "It is not a single point estimate, it's a full distribution over all the infinitely many possible parameters.",
    "start": "6335225",
    "end": "6341240"
  },
  {
    "text": "And out of this, [NOISE] we extract just one parameter setting called MAP,",
    "start": "6341240",
    "end": "6349310"
  },
  {
    "text": "which is arg max. [NOISE] So Theta hat MAP",
    "start": "6349310",
    "end": "6360665"
  },
  {
    "text": "is the arg max of Theta of- uh, from the posterior. What is the arg max of- so if you have",
    "start": "6360665",
    "end": "6370160"
  },
  {
    "text": "a distribution over this Theta,",
    "start": "6370160",
    "end": "6376160"
  },
  {
    "text": "this is P of Theta given S, what we want is to find a point which maximizes P of Theta given S. Okay?",
    "start": "6376160",
    "end": "6388340"
  },
  {
    "text": "So we want this to be our Theta-hat MAP. So the MAP estimate is the mode of the posterior distribution, right?",
    "start": "6388340",
    "end": "6399680"
  },
  {
    "text": "The value of the, uh, parameter that has the highest density or the probability in the posterior distribution.",
    "start": "6399680",
    "end": "6406670"
  },
  {
    "text": "And- and now, once we, once we obtain this,",
    "start": "6406670",
    "end": "6412085"
  },
  {
    "text": "we're going to switch back to the frequentist setting and make our prediction using h_Theta hat_MAP on unseen examples, X star.",
    "start": "6412085",
    "end": "6425030"
  },
  {
    "text": "Right? So this is called a MAP estimation.",
    "start": "6425030",
    "end": "6430380"
  },
  {
    "text": "And wha- what we're going to see now is that regularization is",
    "start": "6430380",
    "end": "6437409"
  },
  {
    "text": "essentially doing MAP estimation with particular choices for the prior distribution, right?",
    "start": "6437410",
    "end": "6444980"
  },
  {
    "text": "Well, let's- let's see what- what exactly we mean by that? [NOISE]",
    "start": "6444980",
    "end": "6462151"
  },
  {
    "text": "[inaudible] So what we were doing before was not taking the mean of this distribution.",
    "start": "6462151",
    "end": "6469565"
  },
  {
    "text": "We were holding onto the full distribution to construct the posterior predictive distribution, right?",
    "start": "6469565",
    "end": "6476125"
  },
  {
    "text": "We have a holding onto the full distribution to construct the nes- uh, the- the posterior predictive distribution, right?",
    "start": "6476125",
    "end": "6482290"
  },
  {
    "text": "Now, we are not holding onto the full distribution. We're gonna just pick one value of Theta and switch back to",
    "start": "6482290",
    "end": "6489260"
  },
  {
    "text": "the frequent setting of using that as the estimated value. Yes, question?",
    "start": "6489260",
    "end": "6494329"
  },
  {
    "text": "Is that similar to MLE? It's very similar to MLE, and that's what we're gonna see and that's what you're gonna show in your homework as well.",
    "start": "6494330",
    "end": "6499775"
  },
  {
    "text": "It's- it's- it's- so what we want to do is Theta hat equals arg max.",
    "start": "6499775",
    "end": "6507635"
  },
  {
    "text": "I'm actually probably doing some of your homework here for you, so, you know, pay attention.",
    "start": "6507635",
    "end": "6513755"
  },
  {
    "text": "P of Theta given S, right? And this is equal to arg max of P of",
    "start": "6513755",
    "end": "6524840"
  },
  {
    "text": "S given Theta times P of Theta over P of S, right?",
    "start": "6524840",
    "end": "6532369"
  },
  {
    "text": "But P of S has no Theta term in here, so this is just arg max P of S given Theta times P of Theta.",
    "start": "6532370",
    "end": "6543304"
  },
  {
    "text": "And you can apply the log here because log is monotonically increasing and you get",
    "start": "6543305",
    "end": "6551070"
  },
  {
    "text": "arg max Theta log P of S given Theta",
    "start": "6551500",
    "end": "6557630"
  },
  {
    "text": "minus log p of Theta, right?",
    "start": "6557630",
    "end": "6563420"
  },
  {
    "text": "So this is the MAP estimate. And now, what we are seeing here is that you're trying to find the value of",
    "start": "6563420",
    "end": "6569420"
  },
  {
    "text": "Theta that satisfies this times this, right? You're starting this- balance out two different competing weights.",
    "start": "6569420",
    "end": "6577340"
  },
  {
    "text": "If P of Theta is Gaussian normal with some covariance or Sigma square,",
    "start": "6577340",
    "end": "6586730"
  },
  {
    "text": "then it is going to be your original likelihood term.",
    "start": "6586730",
    "end": "6592430"
  },
  {
    "text": "So this is your likelihood.",
    "start": "6592430",
    "end": "6594630"
  },
  {
    "text": "And this in case of P of Theta to be Gaussian,",
    "start": "6600250",
    "end": "6606590"
  },
  {
    "text": "as we've seen before, will turn out to be some constant times Lambda squared.",
    "start": "6606590",
    "end": "6617675"
  },
  {
    "text": "Remember, the log likelihood of a Gaussian. exb minus half, .",
    "start": "6617675",
    "end": "6641300"
  },
  {
    "text": "If it's mean zero, then there's just the square over here, and you take the log of this. I'm just going to cancel out everything except the squared,",
    "start": "6641300",
    "end": "6649505"
  },
  {
    "text": "and that's exactly what you see here. All right. So having a Gaussian prior on your parameters and",
    "start": "6649505",
    "end": "6657350"
  },
  {
    "text": "performing MAP estimation is effectively the same as adding a reg- a squared reg- uh,",
    "start": "6657350",
    "end": "6663620"
  },
  {
    "text": "uh, an L2 regularization on your- on your- on your,",
    "start": "6663620",
    "end": "6669470"
  },
  {
    "text": "uh, uh, MLE estimate. So this could be, in case of regressions, there's just the square loss, okay?",
    "start": "6669470",
    "end": "6677239"
  },
  {
    "text": "And there is also, um, you can, you know, do a quick visualization of this. So if this is your Theta and this is 0,",
    "start": "6677240",
    "end": "6686360"
  },
  {
    "text": "then your prior is some distribution that's- so this is prior.",
    "start": "6686360",
    "end": "6695130"
  },
  {
    "text": "And over here, let's say this is your- um,",
    "start": "6698740",
    "end": "6703650"
  },
  {
    "text": "um, let's call this likelihood, okay?",
    "start": "6705550",
    "end": "6712445"
  },
  {
    "text": "And this is your MLE estimate. So MLE maximizes the likelihood, right?",
    "start": "6712445",
    "end": "6719570"
  },
  {
    "text": "So assume this just peaked over here. And instead what you're trying to maximize is the product of these two, right?",
    "start": "6719570",
    "end": "6726695"
  },
  {
    "text": "The- the prior times the likelihood. And how- how does the product of two functions look, right?",
    "start": "6726695",
    "end": "6732500"
  },
  {
    "text": "So if either one of them is very close to 0, then the product is 0, right?",
    "start": "6732500",
    "end": "6738140"
  },
  {
    "text": "So over here, the prior is close to 0, so the product will be 0.",
    "start": "6738140",
    "end": "6743989"
  },
  {
    "text": "Over here, the likelihood is 0, so the product is 0. So instead, what we get is- so this",
    "start": "6743990",
    "end": "6753065"
  },
  {
    "text": "is prior times likelihood, right?",
    "start": "6753065",
    "end": "6762260"
  },
  {
    "text": "There's a prior times likelihood because in any place where either, uh, the prior or the likelihood is 0,",
    "start": "6762260",
    "end": "6767869"
  },
  {
    "text": "the product will be very close to 0. So only in- in the region where both of them are kind of non-zero, that's where the maximum will be observed.",
    "start": "6767870",
    "end": "6774770"
  },
  {
    "text": "And MAP estimation, you get us this one here. So Theta hat MAP, right?",
    "start": "6774770",
    "end": "6783560"
  },
  {
    "text": "So what we see is that MAP- so there is this kind of, um,",
    "start": "6783560",
    "end": "6789305"
  },
  {
    "text": "there is these conflicting forces between the prior trying to pull the parameter estimate close to 0,",
    "start": "6789305",
    "end": "6797240"
  },
  {
    "text": "and MLE trying to estimate, it'll pull it towards what the data tells us,",
    "start": "6797240",
    "end": "6802400"
  },
  {
    "text": "the, uh, the estimate is. And MAP is kind of trying to,",
    "start": "6802400",
    "end": "6807545"
  },
  {
    "text": "you know, find the right balance between the prior and the likelihood, and it's gonna therefore shrink your MLE close to 0,",
    "start": "6807545",
    "end": "6815945"
  },
  {
    "text": "right? Yes? Question?",
    "start": "6815945",
    "end": "6822860"
  },
  {
    "text": "When we do Bayesian statistics, when we are making a decision about how our class priors are going to be like,",
    "start": "6822860",
    "end": "6829760"
  },
  {
    "text": "do we just choose the kind of distribution [inaudible] or do we also choose the mean of that distribution?",
    "start": "6829760",
    "end": "6835639"
  },
  {
    "text": "In Bayesian statistics, you choose a prior for your parameters, you choose the family and the parameter. [inaudible].",
    "start": "6835640",
    "end": "6851210"
  },
  {
    "text": "So the- the- the initial belief, uh, so- uh, is basically captured in the parameters of your Theta,",
    "start": "6851210",
    "end": "6857930"
  },
  {
    "text": "uh, the- the Theta prior. So if you have a strong belief that your parameters are close to 0,",
    "start": "6857930",
    "end": "6864079"
  },
  {
    "text": "then you will assign a very small variance, which means your prior is peaked.",
    "start": "6864080",
    "end": "6870005"
  },
  {
    "text": "And if your prior is peaked at 0, then this pull is even stronger, and your MAP will be even closer to 0.",
    "start": "6870005",
    "end": "6875720"
  },
  {
    "text": "Uh, and sometimes it's common to use something what is called as a flat prior, which means your prior is just flat,",
    "start": "6875720",
    "end": "6882695"
  },
  {
    "text": "which means your likelihood multiplied by the prior is going to hold the same shape. So that's when you use, uh,",
    "start": "6882695",
    "end": "6888200"
  },
  {
    "text": "something that's called a flat prior where you don't want to do anything at all. But, uh, in that case, you know, you don't have the regularization interpretation,",
    "start": "6888200",
    "end": "6895760"
  },
  {
    "text": "but the regularization interpretation, your prior needs to be a Gaussian, and this Lambda is- is gonna,",
    "start": "6895760",
    "end": "6902570"
  },
  {
    "text": "you know, involve terms of the, you know, Sigma square and- and few other things which you do in your homework.",
    "start": "6902570",
    "end": "6908255"
  },
  {
    "text": "But the general idea is this, you know, um, you know, this- this acts as a regularization term because of the squared error that comes out here.",
    "start": "6908255",
    "end": "6917010"
  },
  {
    "text": "So how confident we are [inaudible]? Yeah. So how confident we are depends on what kind of,",
    "start": "6917230",
    "end": "6922970"
  },
  {
    "text": "uh, ah, variance we assigned to the prior? All right. Uh, if there are any other questions,",
    "start": "6922970",
    "end": "6929330"
  },
  {
    "text": "feel free to walk up to- walk up here, and, uh, I can answer that. And that's all for today. Thanks.",
    "start": "6929330",
    "end": "6935280"
  }
]