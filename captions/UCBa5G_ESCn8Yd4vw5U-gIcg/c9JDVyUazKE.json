[
  {
    "start": "0",
    "end": "12000"
  },
  {
    "text": "Welcome to the lecture on Principal Component Analysis.",
    "start": "5180",
    "end": "10299"
  },
  {
    "start": "12000",
    "end": "486000"
  },
  {
    "text": "Principal component analysis is a- a type of",
    "start": "12470",
    "end": "18960"
  },
  {
    "text": "unsupervised learning where we use a particular data model.",
    "start": "18960",
    "end": "26775"
  },
  {
    "text": "And, uh, in order to do that, we need the following idea,",
    "start": "26775",
    "end": "33075"
  },
  {
    "text": "which is the idea of distance to a subspace. So here, we're going to have a- a set of d-dimensional vectors, theta_1 to theta_r.",
    "start": "33075",
    "end": "47055"
  },
  {
    "text": "And w- when we take all possible linear combinations of those vectors,",
    "start": "47055",
    "end": "55100"
  },
  {
    "text": "we get a subspace. So in other words, I'm looking at combinations of the form, uh,",
    "start": "55100",
    "end": "65125"
  },
  {
    "text": "x is equal to a_1 theta_1 plus",
    "start": "65125",
    "end": "70950"
  },
  {
    "text": "a_2 theta_2 all the way up to a_d theta_d.",
    "start": "70950",
    "end": "78715"
  },
  {
    "text": "And I'm allowed to pick the As any way I like. And if I look at all such possible vectors x I can construct, that's a subspace.",
    "start": "78715",
    "end": "88370"
  },
  {
    "text": "And it's a d-dimensional, and it's a r-dimensional subspace of R_d.",
    "start": "88370",
    "end": "94994"
  },
  {
    "text": "Um, and we might write that, uh, as theta_a,",
    "start": "94995",
    "end": "104155"
  },
  {
    "text": "where theta is a matrix whose columns are the individual vectors,",
    "start": "104155",
    "end": "114150"
  },
  {
    "text": "theta_i and a is a vector a_1,",
    "start": "114150",
    "end": "119820"
  },
  {
    "text": "a_2, all the way up to a_r of the coefficients.",
    "start": "119820",
    "end": "125270"
  },
  {
    "text": "And so the matrix theta times the vector a is equal to",
    "start": "125270",
    "end": "131224"
  },
  {
    "text": "the linear combination of the columns of theta and the coefficients in that linear combination of the a Is.",
    "start": "131225",
    "end": "140580"
  },
  {
    "text": "And so here, we have, uh,",
    "start": "140860",
    "end": "146560"
  },
  {
    "text": "d by r matrix, and I guess I labeled that incorrectly, so let me fix that.",
    "start": "146560",
    "end": "152620"
  },
  {
    "text": "That is theta_r not theta_d.",
    "start": "152620",
    "end": "155870"
  },
  {
    "text": "So we have a d by r matrix theta,",
    "start": "158140",
    "end": "164375"
  },
  {
    "text": "and that is the matrix that describes our subspace faster. It defines the subspace.",
    "start": "164375",
    "end": "170599"
  },
  {
    "text": "And we can pick any point in the subspace by picking the vector a.",
    "start": "170600",
    "end": "177270"
  },
  {
    "text": "Now if we wanted to say I've got another, uh, vector x,",
    "start": "177340",
    "end": "182405"
  },
  {
    "text": "and I'd like to figure out how far is it from the subspace,",
    "start": "182405",
    "end": "189800"
  },
  {
    "text": "let's draw a little picture for that. So here is my subspace,",
    "start": "189800",
    "end": "197050"
  },
  {
    "text": "here is my vector x.",
    "start": "197210",
    "end": "204090"
  },
  {
    "text": "And this distance here is what I'd like to know. Um, then, well, what is that?",
    "start": "204090",
    "end": "212495"
  },
  {
    "text": "Well, every point in my subspace here, S has the form theta times a,",
    "start": "212495",
    "end": "219860"
  },
  {
    "text": "and so out of all such points, theta times a, I want to fi- find the one that's closest to x,",
    "start": "219860",
    "end": "227750"
  },
  {
    "text": "and that's this optimization problem right here. Minimize over a x minus theta_a.",
    "start": "227750",
    "end": "236349"
  },
  {
    "text": "And the norm of x minus theta_a, is the distance,",
    "start": "236360",
    "end": "242685"
  },
  {
    "text": "um, between x and a particular point theta a within this subspace.",
    "start": "242685",
    "end": "248560"
  },
  {
    "text": "Now this is, uh, if I square that objective function, so I look at this distance squared while minimizing the distance and",
    "start": "248560",
    "end": "255260"
  },
  {
    "text": "minimizing the distance squared are the same problem, and so, uh, this becomes the least-squares problem.",
    "start": "255260",
    "end": "264895"
  },
  {
    "text": "And we know what the solution is to least-squares problems. It gives the optimal a as theta dagger times x.",
    "start": "264895",
    "end": "273590"
  },
  {
    "text": "And theta dagger is theta transpose, theta inverse, theta transpose, that matrix.",
    "start": "273590",
    "end": "283425"
  },
  {
    "text": "And this works if theta has linearly independent columns,",
    "start": "283425",
    "end": "289985"
  },
  {
    "text": "in which case theta transpose theta is invertible and theta dagger is theta transpose,",
    "start": "289985",
    "end": "295979"
  },
  {
    "text": "theta inverse, theta transpose. So that tells us what a is,",
    "start": "295980",
    "end": "301155"
  },
  {
    "text": "and once we know what a is, well, then we know theta_a is this point right here and which is the,",
    "start": "301155",
    "end": "312509"
  },
  {
    "text": "uh, the best, the closest point in the subspace to my given point x over here.",
    "start": "312510",
    "end": "319820"
  },
  {
    "text": "Um, and we might call this thing x-hat.",
    "start": "319820",
    "end": "329190"
  },
  {
    "text": "Um, that's the closest point in S to x. We will call it the projection of x onto S. x-hat is theta times a,",
    "start": "329190",
    "end": "339470"
  },
  {
    "text": "which is just theta times theta transpose theta inverse times theta times x.",
    "start": "339470",
    "end": "344480"
  },
  {
    "text": "So here, we've simply substituted in for a. Um,",
    "start": "344480",
    "end": "352755"
  },
  {
    "text": "and I don't know what that transpose is doing there, um, because that transpose- this should actually read x-hat",
    "start": "352755",
    "end": "363794"
  },
  {
    "text": "is equal to theta theta transpose theta inverse theta transpose x.",
    "start": "363795",
    "end": "371360"
  },
  {
    "text": "Somehow that transpose got misplaced. And so if I want to know what the distance is between x and the subspace,",
    "start": "371360",
    "end": "381770"
  },
  {
    "text": "well, that's x minus x-hat. The norm of x minus x-hat. And x-hat is this thing here,",
    "start": "381770",
    "end": "389265"
  },
  {
    "text": "so substituting x-hat in gives me this rather unpleasant expression.",
    "start": "389265",
    "end": "395640"
  },
  {
    "text": "But really, if you look at that unpleasant expression, it's really just- uh, let's call this matrix W. It's the norm of W times x and W is this matrix.",
    "start": "395640",
    "end": "410615"
  },
  {
    "text": "Identity minus theta theta transpose theta inverse theta transpose.",
    "start": "410615",
    "end": "419020"
  },
  {
    "text": "Yeah. So here's the picture I just drew. This is the case when r, of course is 1, when we've got a one-dimensional subspace,",
    "start": "423770",
    "end": "432190"
  },
  {
    "text": "that's s. Um, and then the subspace is a line,",
    "start": "432190",
    "end": "438045"
  },
  {
    "text": "there's a theta is a- a matrix which has only one column, which you might as well think about as a vector.",
    "start": "438045",
    "end": "445120"
  },
  {
    "text": "But it certainly makes sense and the, uh, uh, this,",
    "start": "445120",
    "end": "451905"
  },
  {
    "text": "uh, theta transpose theta inverse theta transpose gives us an a,",
    "start": "451905",
    "end": "460255"
  },
  {
    "text": "which is a scalar. And then if I multiply it by theta, I get x-hat.",
    "start": "460255",
    "end": "465440"
  },
  {
    "text": "And that's, uh, uh, this- this thing here, in this case it will be a 2 by 2 matrix because theta is a 2 by 1 vector.",
    "start": "469340",
    "end": "480980"
  },
  {
    "text": "So what's our data model? Now we've got the idea of distance.",
    "start": "485630",
    "end": "491280"
  },
  {
    "start": "486000",
    "end": "641000"
  },
  {
    "text": "Um, the data model is that x should be near to a linear combination of the vectors theta_1 to theta_r and R_d.",
    "start": "491280",
    "end": "499300"
  },
  {
    "text": "In other words, it's near to a subspace. The parameter that defines the subspace is this matrix theta,",
    "start": "499300",
    "end": "509060"
  },
  {
    "text": "which is a d by R matrix. So we believe that there is a subspace and for such a subspace,",
    "start": "509060",
    "end": "520133"
  },
  {
    "text": "uh, x, all of the data points x more close to it. Um, now there are quantity R,",
    "start": "520134",
    "end": "530555"
  },
  {
    "text": "the number, the dimension of the subspace is also called the rank of the model.",
    "start": "530555",
    "end": "536660"
  },
  {
    "text": "Um, and the vectors theta_1 through theta_R are called the principal components or the archetypes.",
    "start": "536660",
    "end": "546900"
  },
  {
    "text": "So this is called Principal Component Analysis. This is called a PCA model.",
    "start": "546980",
    "end": "553760"
  },
  {
    "text": "But some people also call it a low rank model, and our loss function or implausibility is simply the distance between x and S squared.",
    "start": "553760",
    "end": "564779"
  },
  {
    "text": "And it will be convenient to assume that Theta has orthonormal columns.",
    "start": "570470",
    "end": "577350"
  },
  {
    "text": "Orthonormal columns means that Theta transpose Theta is the identity. Of course any subspace that,",
    "start": "577350",
    "end": "585015"
  },
  {
    "text": "um, I want to describe, I can describe it by a set of vectors and I can choose the- uh,",
    "start": "585015",
    "end": "595170"
  },
  {
    "text": "the vectors that I'm using to describe this subspace to be orthonormal. And- uh, and so there's no loss of generality in- in making this assumption.",
    "start": "595170",
    "end": "604904"
  },
  {
    "text": "If you have a Theta that doesn't have orthonormal columns, you can compute the orthonormal- uh,",
    "start": "604905",
    "end": "613485"
  },
  {
    "text": "set of orthonormal columns which, um, defines the same subspace by the QR factorization of the matrix Theta.",
    "start": "613485",
    "end": "622140"
  },
  {
    "text": "So you take the QR factorization of the matrix Theta and you get rid of Theta and you replace it with Q.",
    "start": "622140",
    "end": "628155"
  },
  {
    "text": "And then the, uh, set of linear combinations of the columns of Q is exactly the same subspace,",
    "start": "628155",
    "end": "635070"
  },
  {
    "text": "as the set of linear combinations of the columns of the original Theta, but the columns of Q are orthonormal.",
    "start": "635070",
    "end": "642820"
  },
  {
    "start": "641000",
    "end": "799000"
  },
  {
    "text": "So when Theta, Theta transpose is the identity, the loss function is the distance between x and S squared,",
    "start": "642890",
    "end": "654135"
  },
  {
    "text": "which has this nice matrix expression. It's the norm of WX squared where",
    "start": "654135",
    "end": "661649"
  },
  {
    "text": "the matrix W is identity minus Theta,",
    "start": "661650",
    "end": "667590"
  },
  {
    "text": "Theta transpose Theta inverse Theta transpose. Because Theta transpose Theta is the identity,",
    "start": "667590",
    "end": "673965"
  },
  {
    "text": "that's just identity minus Theta, Theta transpose. Now computing this norm times x,",
    "start": "673965",
    "end": "683730"
  },
  {
    "text": "well, we have that's equal to the norm of x minus Theta, Theta transpose x norm squared,",
    "start": "683730",
    "end": "692144"
  },
  {
    "text": "which is x norm squared plus the norm of Theta,",
    "start": "692145",
    "end": "700065"
  },
  {
    "text": "Theta transpose x norm squared minus twice the cross term.",
    "start": "700065",
    "end": "705610"
  },
  {
    "text": "We can simplify that. Um, in particular, uh,",
    "start": "708850",
    "end": "714160"
  },
  {
    "text": "the- the norm of Theta times",
    "start": "714780",
    "end": "719940"
  },
  {
    "text": "any vector v squared is v transpose Theta transpose Theta v,",
    "start": "719940",
    "end": "728130"
  },
  {
    "text": "which is just equal to the norm of v squared. And so uh, that tells us that this term",
    "start": "728130",
    "end": "742290"
  },
  {
    "text": "here is equal to the norm of",
    "start": "742290",
    "end": "747630"
  },
  {
    "text": "Theta transpose x squared because it's Theta times Theta transpose x.",
    "start": "747630",
    "end": "753405"
  },
  {
    "text": "And, uh, as we've just seen, norm of Theta v is the norm of v. And this here is minus 2,",
    "start": "753405",
    "end": "765480"
  },
  {
    "text": "the norm of Theta transpose x squared because it's x transpose Theta,",
    "start": "765480",
    "end": "771089"
  },
  {
    "text": "Theta transpose x. Um, and those nicely add up, uh,",
    "start": "771090",
    "end": "779380"
  },
  {
    "text": "to give us minus norm of Theta transpose x squared,",
    "start": "780530",
    "end": "786510"
  },
  {
    "text": "which is a very, uh, convenient expression right here for the loss function.",
    "start": "786510",
    "end": "794410"
  },
  {
    "start": "799000",
    "end": "903000"
  },
  {
    "text": "So now the empirical risk, that's the average on the loss function.",
    "start": "801530",
    "end": "806835"
  },
  {
    "text": "The average of the loss function on the dataset, simply 1 over n sum over i is 1 to n and the distance from x_i to S squared.",
    "start": "806835",
    "end": "816119"
  },
  {
    "text": "And the PCA data model says we should choose orthonormal Theta_1 through Theta_r to minimize this empirical risk.",
    "start": "816119",
    "end": "826200"
  },
  {
    "text": "So we get a bunch of x's, a bunch of data. We then, uh, pick",
    "start": "826200",
    "end": "833580"
  },
  {
    "text": "the Theta that minimizes the empirical risk and then we can use that model,",
    "start": "833580",
    "end": "838680"
  },
  {
    "text": "for example, to do imputation. So this is the case when r is 1.",
    "start": "838680",
    "end": "847880"
  },
  {
    "text": "It has a nice interpretation geometrically, we have all these data points shown in green.",
    "start": "847880",
    "end": "854180"
  },
  {
    "text": "Here, we're trying to choose a one-dimensional subspace, which is just a line. And the loss function is the distance between the point and the line,",
    "start": "854180",
    "end": "863165"
  },
  {
    "text": "these red distances here. And the sum of the squared- sum of the squares of those distances that's the- uh, er,",
    "start": "863165",
    "end": "874430"
  },
  {
    "text": "that's the empirical risk scaled by n. So we're trying to find the subspace that best fits the data.",
    "start": "874960",
    "end": "886230"
  },
  {
    "text": "Um, and we're measuring the quality of fit by the normal distance between a point and a subspace,",
    "start": "886230",
    "end": "895710"
  },
  {
    "text": "in this case a point and a line. Uh, we can, er,",
    "start": "895710",
    "end": "906550"
  },
  {
    "start": "903000",
    "end": "995000"
  },
  {
    "text": "look at this in matrix notation. So let's express the empirical risk in matrix notation.",
    "start": "906550",
    "end": "914025"
  },
  {
    "text": "As before, we construct the data matrix through the x's. This is just the same one data matrix we used in regression.",
    "start": "914025",
    "end": "921043"
  },
  {
    "text": "It's an by d matrix, each row of which is a corresponding,",
    "start": "921044",
    "end": "927795"
  },
  {
    "text": "uh, data element x_i. So the ith row of the matrix X is X,",
    "start": "927795",
    "end": "933615"
  },
  {
    "text": "is little x_i transpose. So the empirical PCA loss is then expressible",
    "start": "933615",
    "end": "943440"
  },
  {
    "text": "in terms of the matrix by noticing that the sum of the norms of x_i squared is the Frobenius norm of the matrix x squared.",
    "start": "943440",
    "end": "957750"
  },
  {
    "text": "And that if I want to compute Theta transpose times x_i, well, I can compute the matrix x times Theta,",
    "start": "957750",
    "end": "966240"
  },
  {
    "text": "and that will give me a matrix whose, uh, ith, uh,",
    "start": "966240",
    "end": "972645"
  },
  {
    "text": "row is precisely Theta transpose x_i, or, uh, transposed.",
    "start": "972645",
    "end": "983020"
  },
  {
    "text": "Notice that here, this should be a factor of n there.",
    "start": "983120",
    "end": "991600"
  },
  {
    "text": "Now in order to fit the PCA model,",
    "start": "994340",
    "end": "999570"
  },
  {
    "start": "995000",
    "end": "1106000"
  },
  {
    "text": "we start off with, uh, n data points x_1 through x_n,",
    "start": "999570",
    "end": "1005330"
  },
  {
    "text": "and we want to minimize the empirical risk. We've got the additional constraint that we want to",
    "start": "1005330",
    "end": "1012125"
  },
  {
    "text": "find the Theta that minimizes the empirical risk, but also should satisfy Theta transpose Theta is equal to the identity.",
    "start": "1012125",
    "end": "1020629"
  },
  {
    "text": "In other words, the columns of the matrix Theta should be orthonormal. Uh, because the empirical risk is",
    "start": "1020630",
    "end": "1029089"
  },
  {
    "text": "the norm of X minus the norm of X Theta.",
    "start": "1029090",
    "end": "1039240"
  },
  {
    "text": "Minimizing this quantity over Theta is the same as simply",
    "start": "1041260",
    "end": "1047645"
  },
  {
    "text": "maximizing minus- is simply maximizing the norm of X Theta_F squared.",
    "start": "1047645",
    "end": "1055655"
  },
  {
    "text": "Um, and of course, the minus change the minimum- changes the minimum to a maximum.",
    "start": "1055655",
    "end": "1061669"
  },
  {
    "text": "And it turns out that there's- there's an exact algorithm for doing this. Um, so it's not a heuristic.",
    "start": "1061670",
    "end": "1068885"
  },
  {
    "text": "Um, er, uh, these algorithms are called a singular value decomposition or the eigenvalue decomposition.",
    "start": "1068885",
    "end": "1076279"
  },
  {
    "text": "Uh, in this class, we're not going to go into the details of how those work, uh,",
    "start": "1076280",
    "end": "1082580"
  },
  {
    "text": "but it's worth knowing that they exist, that their complexity takes is of the order of nd squared,",
    "start": "1082580",
    "end": "1090740"
  },
  {
    "text": "where d is the dimension of X and n is the number of data points. Uh, and there are, uh, uh, more efficient methods when r is much smaller than d.",
    "start": "1090740",
    "end": "1101670"
  },
  {
    "text": "What do we do when we've got such a data model? One thing we can do is imputation and the idea is straightforward.",
    "start": "1105620",
    "end": "1112635"
  },
  {
    "start": "1106000",
    "end": "1252000"
  },
  {
    "text": "We fitted a subspace, here it is.",
    "start": "1112635",
    "end": "1117720"
  },
  {
    "text": "We've got, uh, a data vector with some missing entries,",
    "start": "1117720",
    "end": "1124125"
  },
  {
    "text": "so suppose for example we only know x_2, then we know [NOISE] that the true x lies somewhere on that line,",
    "start": "1124125",
    "end": "1133965"
  },
  {
    "text": "and we pick this point right here to fill in the remaining x_1 entry,",
    "start": "1133965",
    "end": "1142470"
  },
  {
    "text": "and that corresponds to exactly what it corresponded to.",
    "start": "1142470",
    "end": "1147809"
  },
  {
    "text": "In our previous session on imputation, we minimize the loss function over",
    "start": "1147810",
    "end": "1155715"
  },
  {
    "text": "the unknown components of x with the components of x that are known fixed.",
    "start": "1155715",
    "end": "1163034"
  },
  {
    "text": "Now in order to find the, uh, intersection point right here,",
    "start": "1163035",
    "end": "1171760"
  },
  {
    "text": "the way we do that is we have to find the a.",
    "start": "1171760",
    "end": "1177290"
  },
  {
    "text": "Remember a parameterizes the subspace, so we have to find the a corresponding to that intersection point,",
    "start": "1177290",
    "end": "1184885"
  },
  {
    "text": "that corresponds to minimizing this objective function. So here, we're looking for the uh, uh,",
    "start": "1184885",
    "end": "1196470"
  },
  {
    "text": "the point a that minimizes the norm of the difference between x minus c to a.",
    "start": "1196470",
    "end": "1205320"
  },
  {
    "text": "That's the point in the subspace, uh, which is as close as possible to x.",
    "start": "1205320",
    "end": "1210975"
  },
  {
    "text": "However we don't use the entire norm because some components of x are unknown, and as a result,",
    "start": "1210975",
    "end": "1217485"
  },
  {
    "text": "we simply use only those entries of x which are known and that gives us the sum",
    "start": "1217485",
    "end": "1224429"
  },
  {
    "text": "over all i in the known set of x_i minus Theta a_i squared,",
    "start": "1224430",
    "end": "1233925"
  },
  {
    "text": "and that finds exactly this point right here, and if we look at the corresponding x-hat which is Theta a,",
    "start": "1233925",
    "end": "1243540"
  },
  {
    "text": "then we've got get the ith component of it that will give us the missing entries of x-hat.",
    "start": "1243540",
    "end": "1251385"
  },
  {
    "text": "Now when we're fitting our model, we start off with data points X_1 to X_n and we would",
    "start": "1251385",
    "end": "1260670"
  },
  {
    "start": "1252000",
    "end": "1656000"
  },
  {
    "text": "like to choose the best Theta so that we're minimizing the empirical risk.",
    "start": "1260670",
    "end": "1267255"
  },
  {
    "text": "Um, and the way we do that is we, uh, look at the distance between X and S,",
    "start": "1267255",
    "end": "1274049"
  },
  {
    "text": "where we've parameterized the subspaces S by Theta.",
    "start": "1274050",
    "end": "1279195"
  },
  {
    "text": "In order to compute that distance, in turn we have to compute the optimal a.",
    "start": "1279195",
    "end": "1284805"
  },
  {
    "text": "The optimal a tells us which point in the subspace is the closest to X,",
    "start": "1284805",
    "end": "1289875"
  },
  {
    "text": "and there's a different a for each of the different Xs. The computation that goes into that is this computation right here.",
    "start": "1289875",
    "end": "1300674"
  },
  {
    "text": "We find the a that minimizes x_i minus Theta a,",
    "start": "1300675",
    "end": "1306045"
  },
  {
    "text": "and that we know is a_i is Theta transpose Theta inverse,",
    "start": "1306045",
    "end": "1315075"
  },
  {
    "text": "Theta transpose x_i, and if we are restricting ourselves to only looking at Theta with orthonormal columns,",
    "start": "1315075",
    "end": "1324269"
  },
  {
    "text": "then that's just Theta transpose x_i, because Theta transpose Theta is the identity,",
    "start": "1324270",
    "end": "1331215"
  },
  {
    "text": "and we can write this in a convenient way. We can have a matrix A, [NOISE] where the ith row is the corresponding ith a transposed,",
    "start": "1331215",
    "end": "1344039"
  },
  {
    "text": "[NOISE] and so now the matrix A will be an n by r matrix,",
    "start": "1344040",
    "end": "1354040"
  },
  {
    "text": "and that A is going to be equal to x Theta. Uh, let me erase that and write it correctly,",
    "start": "1354200",
    "end": "1361919"
  },
  {
    "text": "[NOISE] and that just says A is equal to x Theta,",
    "start": "1361919",
    "end": "1367470"
  },
  {
    "text": "simply says equivalently that a transpose is equal to Theta transpose X transpose,",
    "start": "1367470",
    "end": "1373980"
  },
  {
    "text": "[NOISE]  and therefore that the ith row of A,",
    "start": "1373980",
    "end": "1379730"
  },
  {
    "text": "a_i is equal to Theta transpose x_i. It's simply writing this as a matrix equation.",
    "start": "1379730",
    "end": "1386730"
  },
  {
    "text": "Now, uh, once we've got the a,",
    "start": "1387260",
    "end": "1392385"
  },
  {
    "text": "we can correspondingly work out the x tilde,- the corresponding closest point.",
    "start": "1392385",
    "end": "1398110"
  },
  {
    "text": "There's our subspace, it's our point. There's the closest point.",
    "start": "1398110",
    "end": "1405890"
  },
  {
    "text": "This is x_i, and this is x-tilde_i,",
    "start": "1405890",
    "end": "1411550"
  },
  {
    "text": "and x-tilde_i, well that's what a_i tells us. X-tilde_i is just, um, Theta times a_i.",
    "start": "1411550",
    "end": "1424169"
  },
  {
    "text": "Now we can write that as a matrix equation as well in the same way we'll write x-tilde,",
    "start": "1424170",
    "end": "1431805"
  },
  {
    "text": "having its components x-tilde_1 transpose up to x-tilde_n transpose the n rows of x-tilde,",
    "start": "1431805",
    "end": "1441809"
  },
  {
    "text": "and then the equation- this equation here corresponds exactly to",
    "start": "1441810",
    "end": "1447030"
  },
  {
    "text": "the matrix equation x-tilde is equal to A Theta transpose.",
    "start": "1447030",
    "end": "1453190"
  },
  {
    "text": "So our empirical risk",
    "start": "1454430",
    "end": "1460155"
  },
  {
    "text": "is the average of the distance between x_i and x-tilde_i.",
    "start": "1460155",
    "end": "1466830"
  },
  {
    "text": "If we forget about this- the factor of 1i_n, then that becomes the Frobenius norm of x minus x-tilde squared,",
    "start": "1466830",
    "end": "1477540"
  },
  {
    "text": "each row of x being the corresponding x_i and each row of x-tilde being their corresponding x-tilde_i,",
    "start": "1477540",
    "end": "1485175"
  },
  {
    "text": "and so the Frobenius norm just gives us the sum of the squares of the differences in the norms of the vector paths.",
    "start": "1485175",
    "end": "1491620"
  },
  {
    "text": "We could write this like this. Since x-tilde is, uh, uh,",
    "start": "1492110",
    "end": "1497295"
  },
  {
    "text": "is A Theta transpose and A is x Theta,",
    "start": "1497295",
    "end": "1507360"
  },
  {
    "text": "and just substituting those two in gives us x-tilde is x Theta Theta transpose.",
    "start": "1507360",
    "end": "1512924"
  },
  {
    "text": "Another thing we could do is we can simply say x-tilde is A Theta transpose,",
    "start": "1512925",
    "end": "1519270"
  },
  {
    "text": "and so let's look at this expression directly, and that says that, well we- we start off with x,",
    "start": "1519270",
    "end": "1527054"
  },
  {
    "text": "and our job is to find both in A and a Theta.",
    "start": "1527055",
    "end": "1532920"
  },
  {
    "text": "Of course, once you know Theta, well then A is given to you.",
    "start": "1532920",
    "end": "1538815"
  },
  {
    "text": "A is Theta transpose x. Conversely, um,",
    "start": "1538815",
    "end": "1545129"
  },
  {
    "text": "once you know A, well then x-tilde is given to you, but here, we're simply saying, \"Let's try and find simultaneously both A and Theta transpose.\"",
    "start": "1545130",
    "end": "1555360"
  },
  {
    "text": "And the ex- the idea here is that this is a matrix factorization.",
    "start": "1555360",
    "end": "1561420"
  },
  {
    "text": "We want to find a matrix A which has to have the dimensions n by r and the matrix",
    "start": "1561420",
    "end": "1568290"
  },
  {
    "text": "Theta transpose which has to have di- dimensions r by d. I guess this should say d here,",
    "start": "1568290",
    "end": "1575430"
  },
  {
    "text": "so that x is approximately A Theta transpose, and the dimensions look like this.",
    "start": "1575430",
    "end": "1582795"
  },
  {
    "text": "The dimensions are, there is",
    "start": "1582795",
    "end": "1588045"
  },
  {
    "text": "A and that has to be approximately equal.",
    "start": "1588045",
    "end": "1594370"
  },
  {
    "text": "Sorry, that's x, and",
    "start": "1595760",
    "end": "1601400"
  },
  {
    "text": "it has to be approximately equal to A Theta transpose,",
    "start": "1601400",
    "end": "1609900"
  },
  {
    "text": "where this is n and this is d. This is therefore n and r,",
    "start": "1609900",
    "end": "1614985"
  },
  {
    "text": "and this is therefore r and d. Now in general,",
    "start": "1614985",
    "end": "1620010"
  },
  {
    "text": "if r is small, one will not be able to find exactly a pair of matrices",
    "start": "1620010",
    "end": "1626640"
  },
  {
    "text": "A and Theta transpose such that x is A times Theta transpose,",
    "start": "1626640",
    "end": "1632204"
  },
  {
    "text": "um, and so this is an approximate matrix factorization problem,",
    "start": "1632204",
    "end": "1637995"
  },
  {
    "text": "and what PCA is doing is finding the closest matrix 2x that is a product of an n by r and an r by d matrix.",
    "start": "1637995",
    "end": "1651040"
  },
  {
    "start": "1656000",
    "end": "1757000"
  },
  {
    "text": "Now, the mapping a is Theta transpose can be thought of as an embedding.",
    "start": "1656000",
    "end": "1662640"
  },
  {
    "text": "It's taking an x and giving a vector a. The vector x has dimension d,",
    "start": "1662640",
    "end": "1668355"
  },
  {
    "text": "and the vector a has dimension r. So this is an embedding which is taking x in a which is- may have a large dimension and giving us ah,",
    "start": "1668355",
    "end": "1677705"
  },
  {
    "text": "an a, which would normally have a much smaller dimension r. Would normally be much smaller than d. And so it's a dimension reduction.",
    "start": "1677705",
    "end": "1686890"
  },
  {
    "text": "We can think about a as a compressed feature vector when x is the original feature vector.",
    "start": "1686890",
    "end": "1694840"
  },
  {
    "text": "Um, now, when we've done feature engineering in the past,",
    "start": "1694850",
    "end": "1701220"
  },
  {
    "text": "we've done things like constructing products and applying non-linear maps to x to construct features.",
    "start": "1701220",
    "end": "1709395"
  },
  {
    "text": "But here, the embedding is based on the data set, the embedding is being learned.",
    "start": "1709395",
    "end": "1715260"
  },
  {
    "text": "And so this is a learned linear embedding from the d dimensional space of x's to the r dimensional space of a's.",
    "start": "1715260",
    "end": "1725409"
  },
  {
    "text": "Now one of the nice things about this embedding is that it- approximately preserves distances.",
    "start": "1725930",
    "end": "1734490"
  },
  {
    "text": "So that points that are far apart in our original d dimensional space,",
    "start": "1734490",
    "end": "1740640"
  },
  {
    "text": "are also far apart in our r dimensional space. And points that are close, are also close.",
    "start": "1740640",
    "end": "1745740"
  },
  {
    "text": "But because r is less than d, it cannot do that exactly. And so it does that as well as it can.",
    "start": "1745740",
    "end": "1753705"
  },
  {
    "text": "Let's have a look at this. So this property that the distances are almost preserved",
    "start": "1753705",
    "end": "1761070"
  },
  {
    "start": "1757000",
    "end": "1945000"
  },
  {
    "text": "is called the approximate isometry property of PCA. And isometry, say a map from R to the p to R to the q,",
    "start": "1761070",
    "end": "1771090"
  },
  {
    "text": "a map is called an isometry if it preserves distances, which means that if I've got two, ah, I've got two vectors, x and x tilde.",
    "start": "1771090",
    "end": "1777645"
  },
  {
    "text": "I map them both, both under F. I get F of x and F of x tilde.",
    "start": "1777645",
    "end": "1782669"
  },
  {
    "text": "And the distance between F of x and F of x tilde, is approximately the same as the distance between x and x tilde.",
    "start": "1782670",
    "end": "1789075"
  },
  {
    "text": "The most well-known simple example of this is F of x is Q times x,",
    "start": "1789075",
    "end": "1795269"
  },
  {
    "text": "where Q is a matrix which is orthonormal columns, Q transpose Q is the identity.",
    "start": "1795270",
    "end": "1800909"
  },
  {
    "text": "Then, ah, ah, we can see that norm of Q-",
    "start": "1800910",
    "end": "1807780"
  },
  {
    "text": "[NOISE]",
    "start": "1807780",
    "end": "1812910"
  },
  {
    "text": "of Qx minus Qx tilde squared.",
    "start": "1812910",
    "end": "1818670"
  },
  {
    "text": "Well that's equal to Q times the norm of x minus x tilde squared,",
    "start": "1818670",
    "end": "1826260"
  },
  {
    "text": "which is equal to x minus x tilde transpose Q transpose Q,",
    "start": "1826260",
    "end": "1834255"
  },
  {
    "text": "x minus x tilde. And Q transpose Q is the identity, since this is equal to the norm of x minus x tilde squared.",
    "start": "1834255",
    "end": "1844210"
  },
  {
    "text": "Now, what we're doing in PCA, our loss function is the norm of x squared minus the norm of Theta transpose x squared.",
    "start": "1844490",
    "end": "1856605"
  },
  {
    "text": "And the-the, ah, the Theta transpose x,",
    "start": "1856605",
    "end": "1862679"
  },
  {
    "text": "well that's precisely a. So this is the norm of x squared minus the norm of a squared.",
    "start": "1862680",
    "end": "1870480"
  },
  {
    "text": "Once you know x, well then you know a. And so what we're doing is we're trying to make norm of x",
    "start": "1870480",
    "end": "1877919"
  },
  {
    "text": "squared approximately equal to the norm of a squared. In other words, this means that the embedding is",
    "start": "1877920",
    "end": "1884730"
  },
  {
    "text": "going to be nice as an approximate isometry. The smaller we can make the loss function, the closer to an isometry it'll be.",
    "start": "1884730",
    "end": "1891750"
  },
  {
    "text": "In particular, we'll see that ah, ah, often we choose r to be 2 or 3",
    "start": "1891750",
    "end": "1899325"
  },
  {
    "text": "simply so that we can visualize the data effectively, we'll be able to, by picking r as 2,",
    "start": "1899325",
    "end": "1906554"
  },
  {
    "text": "we can plot all of our data points in the plane. Each xi gets mapped to an a i.",
    "start": "1906555",
    "end": "1913230"
  },
  {
    "text": "And these a i are two-dimensional vectors. And often by doing that, the PCA embedding picks for us and shows for us a map of our data in two dimensions.",
    "start": "1913230",
    "end": "1926145"
  },
  {
    "text": "Where points that are close are similar, and points that are far apart are dissimilar.",
    "start": "1926145",
    "end": "1933460"
  },
  {
    "text": "We'll see an example of that. So the example we're going to look at is called Latent semantic indexing.",
    "start": "1933710",
    "end": "1942640"
  },
  {
    "text": "The idea here is that we have a corpus.",
    "start": "1943160",
    "end": "1948750"
  },
  {
    "start": "1945000",
    "end": "2213000"
  },
  {
    "text": "A body of documents, a sele- a collection of documents. Each of our records ui,",
    "start": "1948750",
    "end": "1954810"
  },
  {
    "text": "will be a document. In that corpus of documents we can look at all the words and count up the number of different words that are in there,",
    "start": "1954810",
    "end": "1962835"
  },
  {
    "text": "and we'll call that number d, the number of unique words in all the documents. Now, whenever we've got ah, ah,",
    "start": "1962835",
    "end": "1972350"
  },
  {
    "text": "d we can number all of the words that like show up anywhere in our ah,",
    "start": "1972350",
    "end": "1978215"
  },
  {
    "text": "corpus of documents from 1 through d. And that means- that gives us a very natural way of embedding documents.",
    "start": "1978215",
    "end": "1987029"
  },
  {
    "text": "Um, we can look at a document, um, and ah, we can set x1 to be the number of times word 1 occurs in that document.",
    "start": "1987030",
    "end": "1998715"
  },
  {
    "text": "x2 to be the number of times where 2 occurs in the document and so on. So we'll have a histogram of the word occurrences for each document.",
    "start": "1998715",
    "end": "2009665"
  },
  {
    "text": "And that's a very reasonable embedding um, ah, from ah, which maps a document to a d-dimensional vector.",
    "start": "2009665",
    "end": "2020390"
  },
  {
    "text": "Ah, in fact, we don't tend to use that particular embedding, but we use a very similar embedding.",
    "start": "2020390",
    "end": "2026539"
  },
  {
    "text": "An embedding which has the same idea where x- the jth component of x is",
    "start": "2026540",
    "end": "2032750"
  },
  {
    "text": "approximately equal to the number of times the jth unique word occurs in that document.",
    "start": "2032750",
    "end": "2040100"
  },
  {
    "text": "And is larger the more often the word occurs, but it's not quite that. Um, and let's see why not.",
    "start": "2040100",
    "end": "2049230"
  },
  {
    "text": "Now, ah, we're going to use two- in order to construct this particular embedding,",
    "start": "2050170",
    "end": "2055954"
  },
  {
    "text": "we're going to use two quantities. The first is called the term frequency of word j.",
    "start": "2055955",
    "end": "2062510"
  },
  {
    "text": "And so that- what that is, is that if you give me a particular document, I count up the number of occurrences of word j in that document,",
    "start": "2062510",
    "end": "2072050"
  },
  {
    "text": "and I divide it by the total number of words in that document. Um, so that tells me what fraction of words in the document are word j.",
    "start": "2072050",
    "end": "2081889"
  },
  {
    "text": "That's called a term frequency of word j in document u.",
    "start": "2081890",
    "end": "2087125"
  },
  {
    "text": "Now I can also look at a different quantity, the document frequency of word j.",
    "start": "2087125",
    "end": "2093169"
  },
  {
    "text": "And that is, if I look at the entire er, corpus of documents,",
    "start": "2093170",
    "end": "2098315"
  },
  {
    "text": "and I look at the number of documents in which that word occurs, and I divide by the now total number of documents I have.",
    "start": "2098315",
    "end": "2105515"
  },
  {
    "text": "Um, now, why is this a good idea? Er, if we just kept a histogram of the term frequencies,",
    "start": "2105515",
    "end": "2114650"
  },
  {
    "text": "that would certainly make a vector which would have large entries to words that show up a lot in that document.",
    "start": "2114650",
    "end": "2122630"
  },
  {
    "text": "And small entries for words that don't show up a lot. Trouble is is that some of the large entries would be kind of meaningless.",
    "start": "2122630",
    "end": "2129290"
  },
  {
    "text": "They'd be words like the, if, and, but. And ah, those words are words show up a lot in all documents.",
    "start": "2129290",
    "end": "2137720"
  },
  {
    "text": "And so as a result, we would like to scale in some way, to de-emphasize words which are popular words in all the documents.",
    "start": "2137720",
    "end": "2147424"
  },
  {
    "text": "And that's what we do with the document frequency. And so this thing is called a TFIDF embedding,",
    "start": "2147425",
    "end": "2154790"
  },
  {
    "text": "the Term Frequency Inverse Document Frequency embedding. And it's not quite the ratio of the term frequency to the document frequency,",
    "start": "2154790",
    "end": "2164329"
  },
  {
    "text": "but it's the term frequency multiplied by the log of one over the document frequency.",
    "start": "2164330",
    "end": "2171200"
  },
  {
    "text": "And this kind of does the following. If a word j occurs very often in a document,",
    "start": "2171200",
    "end": "2181789"
  },
  {
    "text": "and it doesn't occur very often in all documents,",
    "start": "2181789",
    "end": "2188000"
  },
  {
    "text": "then the TFIDF embedding will be large.",
    "start": "2188000",
    "end": "2193490"
  },
  {
    "text": "And here where ah, um,",
    "start": "2193490",
    "end": "2199430"
  },
  {
    "text": "have a way of discounting the occurrence of very common words such as the.",
    "start": "2199430",
    "end": "2207890"
  },
  {
    "text": "[NOISE] So let's look at a specific example.",
    "start": "2207890",
    "end": "2215630"
  },
  {
    "start": "2213000",
    "end": "2274000"
  },
  {
    "text": "Uh, here we're going to have two texts, The Critique of Pure Reason by Immanuel Kant and The Problems of Philosophy by Bertrand Russell.",
    "start": "2215630",
    "end": "2224375"
  },
  {
    "text": "Uh, these are both very famous philosophical works, uh, both in a specific area of philosophy and they're famous,",
    "start": "2224375",
    "end": "2232910"
  },
  {
    "text": "uh, in part because they take opposite- opposing views. Um, and these we got- the way we've",
    "start": "2232910",
    "end": "2240260"
  },
  {
    "text": "analyzed these is we've taken 50 excerpts from each of these books. Each excerpt has about 3,000 characters.",
    "start": "2240260",
    "end": "2247920"
  },
  {
    "text": "Uh, for each, excerpt we've taken- we split them into words, removed any punctuation or capitalization,",
    "start": "2247920",
    "end": "2255520"
  },
  {
    "text": "and it gives us uh, a total of the whole corpus of 3,566 unique words.",
    "start": "2255520",
    "end": "2263955"
  },
  {
    "text": "We embed these using TFIDF embedding, we standardize and we apply PCA.",
    "start": "2263955",
    "end": "2271440"
  },
  {
    "text": "So here, uh, is an example of 1,000 characters of Kant.",
    "start": "2273940",
    "end": "2279815"
  },
  {
    "start": "2274000",
    "end": "2284000"
  },
  {
    "text": "Now, you can read this and get the sense of what it sounds like, and 1,000 characters of Russell.",
    "start": "2279815",
    "end": "2287460"
  },
  {
    "start": "2284000",
    "end": "2304000"
  },
  {
    "text": "Now, um, just based on 1,000 characters, of course, you can't glean too much from the meaning.",
    "start": "2287500",
    "end": "2293765"
  },
  {
    "text": "And the question is, can we learn something that would enable us to uh,",
    "start": "2293765",
    "end": "2299434"
  },
  {
    "text": "distinguish these two documents? And so here, this is the embedding that PCA comes up with.",
    "start": "2299435",
    "end": "2309080"
  },
  {
    "start": "2304000",
    "end": "2704000"
  },
  {
    "text": "We've picked r as 2. We've got a data matrix X, which is 100 by 2,262.",
    "start": "2309080",
    "end": "2317810"
  },
  {
    "text": "Um, and, ah, this is,",
    "start": "2317810",
    "end": "2323795"
  },
  {
    "text": "uh, 100 different excerpts, 50 from Russell, 50 from Kant.",
    "start": "2323795",
    "end": "2330530"
  },
  {
    "text": "Now, each data point, each vector x i gets mapped to an ai.",
    "start": "2330530",
    "end": "2338045"
  },
  {
    "text": "That ai of course is just Theta transpose times x i.",
    "start": "2338045",
    "end": "2343190"
  },
  {
    "text": "And because we picked r as 2, each document corresponds to a vector in the plane,",
    "start": "2343190",
    "end": "2349130"
  },
  {
    "text": "and so we can plot all our documents. And here they are, colored the blue ones corresponding to Kant,",
    "start": "2349130",
    "end": "2355609"
  },
  {
    "text": "the red ones correspond to Russell. And you can see they're quite well separated. And that's one of the things PCA does for us,",
    "start": "2355610",
    "end": "2362840"
  },
  {
    "text": "is it shows us the structure. It's given us an embedding where it has",
    "start": "2362840",
    "end": "2370220"
  },
  {
    "text": "tried to spread out the features as much as possible.",
    "start": "2370220",
    "end": "2375930"
  },
  {
    "text": "Now, we have, uh,",
    "start": "2379600",
    "end": "2385130"
  },
  {
    "text": "a Theta, which is, uh, uh, uh, a vector, uh, which is a matrix, which is, um,",
    "start": "2385130",
    "end": "2395420"
  },
  {
    "text": "uh, d by 2, as two columns, Theta 1 and theta 2.",
    "start": "2395420",
    "end": "2402349"
  },
  {
    "text": "Um, and the way we should think about that, is that these are our basis vectors for our subspace S,",
    "start": "2402350",
    "end": "2412445"
  },
  {
    "text": "each entry of theta 1 corresponds to a particular word, same for each entry of Theta 2.",
    "start": "2412445",
    "end": "2420710"
  },
  {
    "text": "And so, here we can take all of our words and plot for each word.",
    "start": "2420710",
    "end": "2428405"
  },
  {
    "text": "Theta 1 and theta 2, the corresponding entries of the Theta 1 i and Theta 2 i,",
    "start": "2428405",
    "end": "2436760"
  },
  {
    "text": "the corresponding entries are Theta 1 and Theta 2 for word i. Now what does this mean?",
    "start": "2436760",
    "end": "2444065"
  },
  {
    "text": "For example, let's pick a word. Let's pick this word down here, which is representation.",
    "start": "2444065",
    "end": "2451759"
  },
  {
    "text": "Now, the word representation here has a Theta 2, which is negative,",
    "start": "2451760",
    "end": "2457744"
  },
  {
    "text": "and a Theta 1 which is quite small, close to 0. And it means that if we have a document in which the word representation shows up a",
    "start": "2457744",
    "end": "2468545"
  },
  {
    "text": "significant number of times, then the corresponding imbedded vector a,",
    "start": "2468545",
    "end": "2474950"
  },
  {
    "text": "will have an a2, which is shifted negative by those words,",
    "start": "2474950",
    "end": "2481025"
  },
  {
    "text": "and an a1 that isn't really changed. Similarly, if I look at a word over here,",
    "start": "2481025",
    "end": "2488180"
  },
  {
    "text": "this word is about, that's got a positive Theta 2,",
    "start": "2488180",
    "end": "2493744"
  },
  {
    "text": "and a negative Theta 1. And as a result, if the word about occurs significantly in a particular document,",
    "start": "2493745",
    "end": "2505100"
  },
  {
    "text": "then the corresponding embedded a will be moved. So here's the origin,",
    "start": "2505100",
    "end": "2511354"
  },
  {
    "text": "will be moved in this direction. So now, we can look back at our texts.",
    "start": "2511354",
    "end": "2521615"
  },
  {
    "text": "And we can say that because Kant's documents are down here,",
    "start": "2521615",
    "end": "2529085"
  },
  {
    "text": "and Russell's documents are up here, that suggest that,",
    "start": "2529085",
    "end": "2537950"
  },
  {
    "text": "the words- these words over here, are words that Kant tends to use,",
    "start": "2537950",
    "end": "2546424"
  },
  {
    "text": "and these words over here are words that Russell tends to use. Uh, let's uh, look at some of these.",
    "start": "2546424",
    "end": "2556145"
  },
  {
    "text": "I happen to know that there's transcendental over here and conception over here.",
    "start": "2556145",
    "end": "2563360"
  },
  {
    "text": "If we look at, uh, Kant, there,",
    "start": "2563360",
    "end": "2569240"
  },
  {
    "text": "we can see in the middle transcendental conception, right there.",
    "start": "2569240",
    "end": "2576110"
  },
  {
    "text": "And so, the reason why these documents split up like this,",
    "start": "2576110",
    "end": "2585980"
  },
  {
    "text": "and then we can distinguish Russell from Kant, is they tend to use different words with different frequencies.",
    "start": "2585980",
    "end": "2594395"
  },
  {
    "text": "And that shows up and is detected by PCA.",
    "start": "2594395",
    "end": "2600440"
  },
  {
    "text": "Now, of course, it doesn't always happen that a two-dimensional embedding will split your documents so nicely like this.",
    "start": "2600440",
    "end": "2609605"
  },
  {
    "text": "But nonetheless, this gives us a great choice for an embedding which will",
    "start": "2609605",
    "end": "2615320"
  },
  {
    "text": "enable methods such as classification or regression,",
    "start": "2615320",
    "end": "2621635"
  },
  {
    "text": "which we learned earlier in the class to do a good job. And uh, we're seeing that here.",
    "start": "2621635",
    "end": "2632270"
  },
  {
    "text": "Uh, we may want or need to use an R as higher dimension, which means we won't be able to visualize it,",
    "start": "2632270",
    "end": "2637535"
  },
  {
    "text": "but it will still make our classification and our regression methods work well. [NOISE] Now,",
    "start": "2637535",
    "end": "2650030"
  },
  {
    "text": "I think that brings us to the end of the class. Officially, we have one more lecture scheduled for next week,",
    "start": "2650030",
    "end": "2658895"
  },
  {
    "text": "but I think there is no need to try to fit one more topic in- in one lecture.",
    "start": "2658895",
    "end": "2665465"
  },
  {
    "text": "And so we're going to stop here. I know it's been uh, a very challenging quarter uh, and, uh,",
    "start": "2665465",
    "end": "2673430"
  },
  {
    "text": "I appreciate you all working hard on this class during such difficult times.",
    "start": "2673430",
    "end": "2679790"
  },
  {
    "text": "Uh, despite the continuing challenges we are facing in spring 2020,",
    "start": "2679790",
    "end": "2686870"
  },
  {
    "text": "I hope you've had uh, a productive, uh, quarter, and I hope this class has gone well for you,",
    "start": "2686870",
    "end": "2694685"
  },
  {
    "text": "and I wish you all the best for the summer.",
    "start": "2694685",
    "end": "2698280"
  }
]