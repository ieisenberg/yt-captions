[
  {
    "start": "0",
    "end": "24000"
  },
  {
    "start": "0",
    "end": "4228"
  },
  {
    "text": "CHRISTOPHER POTTS:\nWelcome, everyone.",
    "start": "4228",
    "end": "5770"
  },
  {
    "text": "This is part two in our\nseries on grounded language",
    "start": "5770",
    "end": "7895"
  },
  {
    "text": "understanding.",
    "start": "7895",
    "end": "8830"
  },
  {
    "text": "Our task for this unit is\nessentially a natural language",
    "start": "8830",
    "end": "11520"
  },
  {
    "text": "generation task.",
    "start": "11520",
    "end": "12810"
  },
  {
    "text": "And I've called those\nspeakers, the idea",
    "start": "12810",
    "end": "14670"
  },
  {
    "text": "is that speakers go\nfrom the world, that",
    "start": "14670",
    "end": "16442"
  },
  {
    "text": "is some non-linguistic\nthing that they're",
    "start": "16442",
    "end": "18150"
  },
  {
    "text": "trying to communicate\nabout, into language.",
    "start": "18150",
    "end": "20850"
  },
  {
    "text": "Those are really the central\nagents that will be explored.",
    "start": "20850",
    "end": "24390"
  },
  {
    "start": "24000",
    "end": "71000"
  },
  {
    "text": "To ground all this, we're\ngoing to have a simple task.",
    "start": "24390",
    "end": "26640"
  },
  {
    "text": "I'm going to start with the\nmost basic version of the task",
    "start": "26640",
    "end": "29057"
  },
  {
    "text": "that we'll ultimately tackle\nin our assignment and bake-off.",
    "start": "29057",
    "end": "31769"
  },
  {
    "text": "And that is color reference.",
    "start": "31770",
    "end": "34090"
  },
  {
    "text": "So these are examples taken from\na corpus that was originally",
    "start": "34090",
    "end": "37320"
  },
  {
    "text": "collected by Randall\nMunroe of XKCD fame,",
    "start": "37320",
    "end": "40260"
  },
  {
    "text": "and processed into an NLP task\nby McMahan and Stone 2015.",
    "start": "40260",
    "end": "44280"
  },
  {
    "text": "And it's a simple\nformulation in that",
    "start": "44280",
    "end": "45934"
  },
  {
    "text": "the state of the world we\nwant to communicate about",
    "start": "45935",
    "end": "48060"
  },
  {
    "text": "is a color patch.",
    "start": "48060",
    "end": "49530"
  },
  {
    "text": "And the task is simply\nto produce descriptions",
    "start": "49530",
    "end": "51652"
  },
  {
    "text": "of those color patches.",
    "start": "51652",
    "end": "52610"
  },
  {
    "text": "Now I've given\nsome examples here.",
    "start": "52610",
    "end": "54210"
  },
  {
    "text": "And you can see that they\nrange from simple one word",
    "start": "54210",
    "end": "56520"
  },
  {
    "text": "descriptions, all\nthe way up to things",
    "start": "56520",
    "end": "59550"
  },
  {
    "text": "that are kind of complicated\nboth cognitively and",
    "start": "59550",
    "end": "61845"
  },
  {
    "text": "linguistically, and I\nthink point to the idea",
    "start": "61845",
    "end": "63720"
  },
  {
    "text": "that even though this is a\nsimple and constrained domain,",
    "start": "63720",
    "end": "66810"
  },
  {
    "text": "it's a pretty cognitively and\nlinguistically interesting one.",
    "start": "66810",
    "end": "71820"
  },
  {
    "start": "71000",
    "end": "244000"
  },
  {
    "text": "So our speakers, at least\nour baseline speakers,",
    "start": "71820",
    "end": "74990"
  },
  {
    "text": "are standard versions of\nencoder decoder models.",
    "start": "74990",
    "end": "78380"
  },
  {
    "text": "We're going to have for this\ninitial formulation a very",
    "start": "78380",
    "end": "80810"
  },
  {
    "text": "simple encoder.",
    "start": "80810",
    "end": "81740"
  },
  {
    "text": "The task of the\nencoder is simply",
    "start": "81740",
    "end": "83210"
  },
  {
    "text": "to take a color\nrepresentation, which",
    "start": "83210",
    "end": "85640"
  },
  {
    "text": "is going to be a list of floats\nembedded in some embedding",
    "start": "85640",
    "end": "89360"
  },
  {
    "text": "space, and then learn some\nhidden representation for that",
    "start": "89360",
    "end": "92810"
  },
  {
    "text": "color.",
    "start": "92810",
    "end": "93350"
  },
  {
    "text": "And that's all that\nneeds to happen.",
    "start": "93350",
    "end": "94850"
  },
  {
    "text": "So it's just one step.",
    "start": "94850",
    "end": "96500"
  },
  {
    "text": "The decoder is where the\nspeaking part happens.",
    "start": "96500",
    "end": "98910"
  },
  {
    "text": "So the initial token produced\nby the decoder, by the speaker,",
    "start": "98910",
    "end": "102020"
  },
  {
    "text": "is always the start\ntoken which is looked up",
    "start": "102020",
    "end": "104180"
  },
  {
    "text": "in an embedding space.",
    "start": "104180",
    "end": "106010"
  },
  {
    "text": "And then we get our first\ndecoder hidden state,",
    "start": "106010",
    "end": "108230"
  },
  {
    "text": "which is created from\nthe color representation",
    "start": "108230",
    "end": "111738"
  },
  {
    "text": "of the initial hidden\nstate in the sequence we're",
    "start": "111738",
    "end": "113780"
  },
  {
    "text": "going to build, together\nwith an embedding.",
    "start": "113780",
    "end": "115823"
  },
  {
    "text": "And both of those have\nweight transformations.",
    "start": "115823",
    "end": "117740"
  },
  {
    "text": "And it's an additive\ncombination of them",
    "start": "117740",
    "end": "119750"
  },
  {
    "text": "that delivers this\nvalue h1 here.",
    "start": "119750",
    "end": "123100"
  },
  {
    "text": "Then we use some\nsoftmax parameters",
    "start": "123100",
    "end": "124810"
  },
  {
    "text": "to make a prediction\nabout the next token.",
    "start": "124810",
    "end": "126740"
  },
  {
    "text": "Here we've predicted \"dark.\"",
    "start": "126740",
    "end": "128770"
  },
  {
    "text": "And we get our error signal\nby comparing that prediction",
    "start": "128770",
    "end": "131680"
  },
  {
    "text": "with the actual token that\noccurred in our training data.",
    "start": "131680",
    "end": "134390"
  },
  {
    "text": "In this case, it was\nthe word \"light.\"",
    "start": "134390",
    "end": "136030"
  },
  {
    "text": "So since we made a\nwrong prediction,",
    "start": "136030",
    "end": "137530"
  },
  {
    "text": "we're going to get a substantive\nerror signal that will then,",
    "start": "137530",
    "end": "140830"
  },
  {
    "text": "we hope, update the\nweight parameters",
    "start": "140830",
    "end": "142510"
  },
  {
    "text": "throughout this\nmodel in a way that",
    "start": "142510",
    "end": "144159"
  },
  {
    "text": "leads them to produce better\ngenerations the next time.",
    "start": "144160",
    "end": "147570"
  },
  {
    "text": "In a little more detail,\njust as a reminder.",
    "start": "147570",
    "end": "149700"
  },
  {
    "text": "So we have an embedding\nfor that start token,",
    "start": "149700",
    "end": "151810"
  },
  {
    "text": "and indeed for all tokens.",
    "start": "151810",
    "end": "153690"
  },
  {
    "text": "The hidden state is derived\nfrom the embedding via a weight",
    "start": "153690",
    "end": "157140"
  },
  {
    "text": "transformation, and the\ncolor representation, which",
    "start": "157140",
    "end": "160170"
  },
  {
    "text": "is state h0 in the recurrence\nthat we're building.",
    "start": "160170",
    "end": "163200"
  },
  {
    "text": "And that too has\na transformation",
    "start": "163200",
    "end": "164610"
  },
  {
    "text": "applied to it, to travel\nthrough the hidden layer.",
    "start": "164610",
    "end": "167340"
  },
  {
    "text": "That gives us the state H1.",
    "start": "167340",
    "end": "168840"
  },
  {
    "text": "And then we have\nsoftmax parameters",
    "start": "168840",
    "end": "170790"
  },
  {
    "text": "on top of that h1 that\nmake a prediction.",
    "start": "170790",
    "end": "173969"
  },
  {
    "text": "The prediction that they\nmake is a prediction",
    "start": "173970",
    "end": "175890"
  },
  {
    "text": "over the entire vocabulary.",
    "start": "175890",
    "end": "178080"
  },
  {
    "text": "And the probability\nof the actual token",
    "start": "178080",
    "end": "180120"
  },
  {
    "text": "gives us our error signals.",
    "start": "180120",
    "end": "181290"
  },
  {
    "text": "So the probability of\n\"light\" is the error signal",
    "start": "181290",
    "end": "183332"
  },
  {
    "text": "that we'll use here to\nupdate the model parameters.",
    "start": "183332",
    "end": "187370"
  },
  {
    "text": "And then we begin with\nthe next timestep.",
    "start": "187370",
    "end": "189680"
  },
  {
    "text": "I've called this\nteacher forcing,",
    "start": "189680",
    "end": "191372"
  },
  {
    "text": "because in the standard mode,\nwhich is the teacher forcing",
    "start": "191373",
    "end": "193790"
  },
  {
    "text": "mode, even though we\npredicted 'dark' at timestep 1",
    "start": "193790",
    "end": "197599"
  },
  {
    "text": "we're going to have\nas our second token,",
    "start": "197600",
    "end": "199850"
  },
  {
    "text": "the token \"light\" which is the\nactual token in the underlying",
    "start": "199850",
    "end": "203390"
  },
  {
    "text": "training data.",
    "start": "203390",
    "end": "204020"
  },
  {
    "text": "And we'll proceed as though\nwe did not make a mistake.",
    "start": "204020",
    "end": "206550"
  },
  {
    "text": "So again, we do an\nembedding look up,",
    "start": "206550",
    "end": "208370"
  },
  {
    "text": "we get our second hidden\nstate for the decoder, that's",
    "start": "208370",
    "end": "210860"
  },
  {
    "text": "a combination of\nthe embedding x 37",
    "start": "210860",
    "end": "213320"
  },
  {
    "text": "and the previous student state,\nand we make another prediction.",
    "start": "213320",
    "end": "215980"
  },
  {
    "text": "And in this case, our\nprediction is \"blue\"",
    "start": "215980",
    "end": "217730"
  },
  {
    "text": "and that's the actual token.",
    "start": "217730",
    "end": "219290"
  },
  {
    "text": "And life is good\nfor a little bit.",
    "start": "219290",
    "end": "221000"
  },
  {
    "text": "And then we proceed\nwith a third timestep.",
    "start": "221000",
    "end": "223400"
  },
  {
    "text": "The actual token is \"blue\" h3.",
    "start": "223400",
    "end": "226709"
  },
  {
    "text": "We predict \"green.\"",
    "start": "226710",
    "end": "227860"
  },
  {
    "text": "And in this case, we\nshould have predicted",
    "start": "227860",
    "end": "229610"
  },
  {
    "text": "the stop token which\nwould cause us to stop",
    "start": "229610",
    "end": "231530"
  },
  {
    "text": "processing the sequence.",
    "start": "231530",
    "end": "232990"
  },
  {
    "text": "We're just going to get an error\nsignal as we steadily would",
    "start": "232990",
    "end": "235490"
  },
  {
    "text": "and propagate that back down\nthrough the model in hopes",
    "start": "235490",
    "end": "237920"
  },
  {
    "text": "that the next time,\nwhen we want to stop,",
    "start": "237920",
    "end": "240110"
  },
  {
    "text": "we'll actually produce this stop\ntoken that I've given up here.",
    "start": "240110",
    "end": "245040"
  },
  {
    "start": "244000",
    "end": "278000"
  },
  {
    "text": "At prediction time, of course\nthe sequence is not given.",
    "start": "245040",
    "end": "248060"
  },
  {
    "text": "That doesn't change the\nencoder because the color",
    "start": "248060",
    "end": "250558"
  },
  {
    "text": "representation is part\nof the model inputs.",
    "start": "250558",
    "end": "252350"
  },
  {
    "text": "So then we have\nto decode and just",
    "start": "252350",
    "end": "253910"
  },
  {
    "text": "describe without any feedback.",
    "start": "253910",
    "end": "255940"
  },
  {
    "text": "So we proceed as we did before\nand we predict \"dark\" here.",
    "start": "255940",
    "end": "259519"
  },
  {
    "text": "And then \"dark\" has to become\nthe token at the next timestep",
    "start": "259519",
    "end": "262542"
  },
  {
    "text": "because we don't know\nwhat the ground truth is.",
    "start": "262542",
    "end": "264500"
  },
  {
    "text": "And we proceed as\nbefore and say \"blue.\"",
    "start": "264500",
    "end": "267200"
  },
  {
    "text": "And then that becomes\nthe third time step,",
    "start": "267200",
    "end": "269270"
  },
  {
    "text": "and with luck, there\nin that third position,",
    "start": "269270",
    "end": "271550"
  },
  {
    "text": "we predict the stop token.",
    "start": "271550",
    "end": "273169"
  },
  {
    "text": "And the decoding\nprocess is completed.",
    "start": "273170",
    "end": "276770"
  },
  {
    "text": "That is the fundamental model.",
    "start": "276770",
    "end": "278503"
  },
  {
    "start": "278000",
    "end": "357000"
  },
  {
    "text": "Even though it's\nsimple, it admits",
    "start": "278503",
    "end": "279919"
  },
  {
    "text": "of many interesting\nmodifications.",
    "start": "279920",
    "end": "281930"
  },
  {
    "text": "Let me just mention\na few of them.",
    "start": "281930",
    "end": "284070"
  },
  {
    "text": "First, the encoder and\nthe decoder, of course,",
    "start": "284070",
    "end": "286022"
  },
  {
    "text": "could have many\nmore hidden layers.",
    "start": "286022",
    "end": "287480"
  },
  {
    "text": "Mine just had one.",
    "start": "287480",
    "end": "288680"
  },
  {
    "text": "But they could be\nvery deep networks.",
    "start": "288680",
    "end": "290750"
  },
  {
    "text": "We would expect that\nthe layer counts",
    "start": "290750",
    "end": "292940"
  },
  {
    "text": "for the encoder and\nthe decoder match",
    "start": "292940",
    "end": "294780"
  },
  {
    "text": "so that you have this even\nhandoff from encoder to decoder",
    "start": "294780",
    "end": "297650"
  },
  {
    "text": "across all the hidden layers.",
    "start": "297650",
    "end": "299570"
  },
  {
    "text": "But even that's not\na hard constraint.",
    "start": "299570",
    "end": "301160"
  },
  {
    "text": "I can imagine that some\npooling or copying could",
    "start": "301160",
    "end": "303692"
  },
  {
    "text": "accommodate different numbers\nof layers and these two",
    "start": "303692",
    "end": "305900"
  },
  {
    "text": "components.",
    "start": "305900",
    "end": "308430"
  },
  {
    "text": "It's very common at\npresent for researchers",
    "start": "308430",
    "end": "310520"
  },
  {
    "text": "to tie the embedding and\nclassifier parameters, right?",
    "start": "310520",
    "end": "313305"
  },
  {
    "text": "The embedding gives\nus a representation",
    "start": "313305",
    "end": "314930"
  },
  {
    "text": "for every vocabulary item.",
    "start": "314930",
    "end": "316490"
  },
  {
    "text": "And the transpose\nof that can serve",
    "start": "316490",
    "end": "318530"
  },
  {
    "text": "as the set of parameters\nfor a softmax classifier",
    "start": "318530",
    "end": "322760"
  },
  {
    "text": "when we predict tokens.",
    "start": "322760",
    "end": "324530"
  },
  {
    "text": "And tying those weights seems\nto be very productive in terms",
    "start": "324530",
    "end": "327410"
  },
  {
    "text": "of optimization effectiveness.",
    "start": "327410",
    "end": "329380"
  },
  {
    "text": "So you might consider that.",
    "start": "329380",
    "end": "331280"
  },
  {
    "text": "And finally, during\ntraining, we might",
    "start": "331280",
    "end": "333139"
  },
  {
    "text": "drop that teacher\nforcing assumption,",
    "start": "333140",
    "end": "335150"
  },
  {
    "text": "which would mean that in a\nsmall percentage of cases,",
    "start": "335150",
    "end": "337940"
  },
  {
    "text": "we would allow the model to just\nproceed as though its predicted",
    "start": "337940",
    "end": "341300"
  },
  {
    "text": "token was the correct token\nfor the next time step,",
    "start": "341300",
    "end": "344150"
  },
  {
    "text": "even if that was a faulty\nassumption, on the idea",
    "start": "344150",
    "end": "347165"
  },
  {
    "text": "that that might help the\nmodel explore a wider",
    "start": "347165",
    "end": "349820"
  },
  {
    "text": "range of the space\nand inject generations",
    "start": "349820",
    "end": "353240"
  },
  {
    "text": "with some helpful diversity.",
    "start": "353240",
    "end": "355395"
  },
  {
    "text": "And then there's one\nother modification",
    "start": "355395",
    "end": "357020"
  },
  {
    "start": "357000",
    "end": "407000"
  },
  {
    "text": "that I want to\nmention, because you'll",
    "start": "357020",
    "end": "358039"
  },
  {
    "text": "see this as part of the\nhomework and the system",
    "start": "358040",
    "end": "359900"
  },
  {
    "text": "that you're developing.",
    "start": "359900",
    "end": "360858"
  },
  {
    "text": "So we found that\nin Monroe et al.",
    "start": "360858",
    "end": "362810"
  },
  {
    "text": "2016, it was helpful\nto remind the decoder",
    "start": "362810",
    "end": "366230"
  },
  {
    "text": "at each one of its\ntimesteps about what",
    "start": "366230",
    "end": "368450"
  },
  {
    "text": "it was trying to describe.",
    "start": "368450",
    "end": "369780"
  },
  {
    "text": "So in more detail, we had\nHSV color representations",
    "start": "369780",
    "end": "373040"
  },
  {
    "text": "as our inputs.",
    "start": "373040",
    "end": "373860"
  },
  {
    "text": "We did a Fourier transform\nto get an embedding.",
    "start": "373860",
    "end": "376460"
  },
  {
    "text": "And that was processed\ninto a hidden state.",
    "start": "376460",
    "end": "378830"
  },
  {
    "text": "And then during\ndecoding, we appended",
    "start": "378830",
    "end": "380960"
  },
  {
    "text": "to each one of the embeddings\nthe Fourier transformation",
    "start": "380960",
    "end": "384229"
  },
  {
    "text": "representation of the color\nas a kind of informal reminder",
    "start": "384230",
    "end": "387350"
  },
  {
    "text": "at each timestep about what\nthe input was actually like,",
    "start": "387350",
    "end": "390440"
  },
  {
    "text": "on the assumption that for\nlong sequences where we get all",
    "start": "390440",
    "end": "393110"
  },
  {
    "text": "the way down to the\nend, the model might",
    "start": "393110",
    "end": "395030"
  },
  {
    "text": "have a hazy memory of what\nit's trying to describe.",
    "start": "395030",
    "end": "397639"
  },
  {
    "text": "And this functions as a kind\nof reminder at that point.",
    "start": "397640",
    "end": "400898"
  },
  {
    "text": "And that proved to\nbe very effective,",
    "start": "400898",
    "end": "402440"
  },
  {
    "text": "and I'll encourage you to\nexplore that in the homework.",
    "start": "402440",
    "end": "405850"
  },
  {
    "text": "And then I hope you can see\nthat even though this task",
    "start": "405850",
    "end": "408100"
  },
  {
    "start": "407000",
    "end": "465000"
  },
  {
    "text": "formulation is simple, it's\nan instance of a wide range",
    "start": "408100",
    "end": "411457"
  },
  {
    "text": "of tasks that we might explore\nunder the heading of grounding.",
    "start": "411457",
    "end": "414040"
  },
  {
    "text": "After all, for\ngrounding in this sense,",
    "start": "414040",
    "end": "416020"
  },
  {
    "text": "we just need some non-linguistic\nrepresentation coming in,",
    "start": "416020",
    "end": "419259"
  },
  {
    "text": "and the ideas that will\ngenerate language in response",
    "start": "419260",
    "end": "421840"
  },
  {
    "text": "to that input.",
    "start": "421840",
    "end": "423169"
  },
  {
    "text": "So image captioning is\nan instance of this.",
    "start": "423170",
    "end": "425650"
  },
  {
    "text": "Scene description, of\ncourse, is another instance.",
    "start": "425650",
    "end": "428259"
  },
  {
    "text": "Visual question answering\nis a slight modification",
    "start": "428260",
    "end": "430810"
  },
  {
    "text": "where the input is\nnot just an image,",
    "start": "430810",
    "end": "432850"
  },
  {
    "text": "but also a question\ntext and the idea",
    "start": "432850",
    "end": "434740"
  },
  {
    "text": "is that you want to produce\nan answer to that question",
    "start": "434740",
    "end": "437050"
  },
  {
    "text": "relative to the image input.",
    "start": "437050",
    "end": "439419"
  },
  {
    "text": "And then instruction giving\nwould be a more general form",
    "start": "439420",
    "end": "441850"
  },
  {
    "text": "where the input is some\nkind of state description",
    "start": "441850",
    "end": "444190"
  },
  {
    "text": "and the idea is that we want to\noffer a complicated instruction",
    "start": "444190",
    "end": "446950"
  },
  {
    "text": "on that basis.",
    "start": "446950",
    "end": "448147"
  },
  {
    "text": "And I think we can think\nof many others that",
    "start": "448147",
    "end": "449980"
  },
  {
    "text": "would fit into this\nmold and benefit",
    "start": "449980",
    "end": "452050"
  },
  {
    "text": "not only from the encoder\ndecoder architecture,",
    "start": "452050",
    "end": "454400"
  },
  {
    "text": "but also from\nconceptualization explicitly",
    "start": "454400",
    "end": "457270"
  },
  {
    "text": "as grounded natural\nlanguage generation tasks.",
    "start": "457270",
    "end": "460889"
  },
  {
    "start": "460890",
    "end": "465000"
  }
]