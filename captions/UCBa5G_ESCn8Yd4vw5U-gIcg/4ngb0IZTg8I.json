[
  {
    "start": "0",
    "end": "5485"
  },
  {
    "text": "Hey, everybody. Welcome back. We're going to be talking more\nabout policy gradient methods today, and then starting to\ntalk about imitation learning.",
    "start": "5485",
    "end": "11450"
  },
  {
    "text": "But we'll do a quick refresh\nyour understanding to start. ",
    "start": "11450",
    "end": "63470"
  },
  {
    "text": "I think everyone agrees\nthat it will not necessarily converge to a global optima. So that's great. There's some differing opinions\nabout some of the other ones.",
    "start": "63470",
    "end": "71189"
  },
  {
    "text": "So maybe turn to a\nneighbor and talk about whether a baseline term\ncan help to reduce the variance and whether or not after\none step of policy gradient,",
    "start": "71190",
    "end": "78420"
  },
  {
    "text": "the resulting policy\ncan get worse. ",
    "start": "78420",
    "end": "85330"
  },
  {
    "text": "[SIDE CONVERSATIONS] ",
    "start": "85330",
    "end": "199905"
  },
  {
    "text": "OK, great. All right, so I think a lot of\npeople remembered from last time that, in general,\na baseline term",
    "start": "199905",
    "end": "205920"
  },
  {
    "text": "does help with the variance. And that's one of the\nreasons we are adding it.",
    "start": "205920",
    "end": "212010"
  },
  {
    "text": "You can initialize it with-- you can't initialize it\nwith a deterministic policy. Does somebody want to say why?",
    "start": "212010",
    "end": "219040"
  },
  {
    "text": "What's the problem with\ndeterministic policies? [INAUDIBLE] Yes. And remind me of your name.",
    "start": "219040",
    "end": "224713"
  },
  {
    "text": "[MUTED] Potentially, there's an action\nthat the policy will never take. So it's not able to reach\na local minima or optima.",
    "start": "224713",
    "end": "232110"
  },
  {
    "text": "Yeah, so if you're. It's like [MUTED] said. So if you are only taking\nactions deterministically, you're never going to know\nabout what other actions are",
    "start": "232110",
    "end": "238620"
  },
  {
    "text": "in that state. So if your current\npolicy is suboptimal, you won't get to\nthe optimal state. And then, the last\none is something we're",
    "start": "238620",
    "end": "244710"
  },
  {
    "text": "going to talk more about today. So in general, it can get worse. This is true.",
    "start": "244710",
    "end": "251075"
  },
  {
    "text": "In general, we're not guaranteed\nto have monotonic improvement. We would like to have\nmonotonic improvement. But in general, policy gradient\ndoesn't guarantee that.",
    "start": "251075",
    "end": "258799"
  },
  {
    "text": "But last term with\nPPO, we saw things that are trying to get\nmore towards that kind of monotonic improvement.",
    "start": "258800",
    "end": "265150"
  },
  {
    "text": "OK, great. So what we're going\nto be doing today is we're going to talk a\nlittle bit more about PPO",
    "start": "265150",
    "end": "272530"
  },
  {
    "text": "and some of the theoretical\nunderpinnings of it, as well as another\nfeature about it that we didn't talk\nabout last time.",
    "start": "272530",
    "end": "277880"
  },
  {
    "text": "And then, we're going\nto start talking about imitation learning. ",
    "start": "277880",
    "end": "283500"
  },
  {
    "text": "So first, and all\nof you guys are going to be implementing PPO\nas part of your homework,",
    "start": "283500",
    "end": "288570"
  },
  {
    "text": "as well as reinforce. So you'll get a chance\nto practice with this. We're first going to talk\nabout Generalized Advantage",
    "start": "288570",
    "end": "294210"
  },
  {
    "text": "Estimation. So first, let's just refresh our\nmemory of some of the challenges",
    "start": "294210",
    "end": "300120"
  },
  {
    "text": "with policy gradients\nthat motivated PPO and a whole bunch\nof other research on better policy gradient\nmethods beyond reinforce.",
    "start": "300120",
    "end": "308760"
  },
  {
    "text": "So in general, remember,\nwe're using theta to parameterize\nthe policy space. And we're just going to do\nstochastic gradient descent",
    "start": "308760",
    "end": "314730"
  },
  {
    "text": "to try to get to a good value,\na policy with a good value.",
    "start": "314730",
    "end": "320250"
  },
  {
    "text": "The challenge is that, in\ngeneral, when we did reinforce, the sample efficiency was poor. We had to run, get\ndata from one policy,",
    "start": "320250",
    "end": "327819"
  },
  {
    "text": "take a single gradient\nstep, and then get more data from the new policy. And as we were\njust discussing, I",
    "start": "327820",
    "end": "333780"
  },
  {
    "text": "think [MUTED] was mentioning\nthis or maybe [MUTED] that the distance in\nthe parameter space is generally not equal to the\ndistance in the action space,",
    "start": "333780",
    "end": "341710"
  },
  {
    "text": "so sort of the policy space. So when you make a small\nchange in the parameters, it might really change the\ntype of actions you take.",
    "start": "341710",
    "end": "347794"
  },
  {
    "text": " So in proximal\npolicy optimization,",
    "start": "347795",
    "end": "354320"
  },
  {
    "text": "we saw two different\nways to try to make it so that we could\nessentially take bigger",
    "start": "354320",
    "end": "359410"
  },
  {
    "text": "steps in between each run\nof when we execute a policy, but do so in a way that\nwould try to encourage",
    "start": "359410",
    "end": "367090"
  },
  {
    "text": "monotonic improvement. And so we saw this\nbound, and we're going to come back to\nthat very shortly, which",
    "start": "367090",
    "end": "372910"
  },
  {
    "text": "looked at how we could try to\napproximate the performance of a new policy under--",
    "start": "372910",
    "end": "378550"
  },
  {
    "text": "only using the data\nthat we have right now. So that's an instance of\noff-policy estimation.",
    "start": "378550",
    "end": "384010"
  },
  {
    "text": "And the bound showed\nthat this relates to the KL divergence between\nthe actual actions taken",
    "start": "384010",
    "end": "389889"
  },
  {
    "text": "under the new policy\nversus the old policy. And in PPO, you could either do\nthis adaptive KL penalty, which",
    "start": "389890",
    "end": "397060"
  },
  {
    "text": "says don't go too far from\nyour previous policy in terms of the actual actions it takes,\nor a clipped objective, which is",
    "start": "397060",
    "end": "404170"
  },
  {
    "text": "going to do something similar. ",
    "start": "404170",
    "end": "409390"
  },
  {
    "text": "All right, so one\nthing that you probably noticed, and particular, if\nyou started implementing this already, is that we\ntalked last time a lot",
    "start": "409390",
    "end": "415400"
  },
  {
    "text": "about using the\nadvantage function. So we talked about how\nwe are going to be doing and, in general, for\nsort of policy gradient,",
    "start": "415400",
    "end": "421800"
  },
  {
    "text": "we're often going to want\nan advantage function. And you might wonder what we're\ngoing to plug in for that. There are a lot of different\nchoices for what the advantage",
    "start": "421800",
    "end": "428780"
  },
  {
    "text": "function could be. So what we're going\nto talk about today is a particular choice\nthat was used in PPO",
    "start": "428780",
    "end": "434360"
  },
  {
    "text": "and that can be pretty powerful. So let's go back to last\nlecture before we introduce PPO",
    "start": "434360",
    "end": "439900"
  },
  {
    "text": "and talk about the\nN-step estimators. So in general, in\nclass, since the first-- probably since the second\nlecture or so, we've been",
    "start": "439900",
    "end": "447160"
  },
  {
    "text": "talking-- second or third, I guess. Probably third, lecture 3. We were talking\nabout this trade-off between methods that bootstrap\nand use the Markov property,",
    "start": "447160",
    "end": "456350"
  },
  {
    "text": "such as temporal difference\nlearning, and methods that don't leverage anything\nabout the Markov property, like Monte Carlo.",
    "start": "456350",
    "end": "462370"
  },
  {
    "text": "So in particular, we've\ntalked about cases where this is a temporal\ndifference estimate,",
    "start": "462370",
    "end": "467805"
  },
  {
    "text": "where you just have the\nimmediate reward plus gamma times the-- you\nimmediately bootstrap, you plug in your\nestimate of the value",
    "start": "467805",
    "end": "473740"
  },
  {
    "text": "of the next state versus\nones like A infinity here, where basically we\nhave a Monte Carlo estimate.",
    "start": "473740",
    "end": "480348"
  },
  {
    "text": "Obviously, you can't\nactually go out to infinity. You'd have to have\nepisodic cases. But we, generally, have been\nfocusing on episodic cases",
    "start": "480348",
    "end": "487970"
  },
  {
    "text": "so far in the class. Minus the value. So the blue there is\njust us subtracting off",
    "start": "487970",
    "end": "493270"
  },
  {
    "text": "our current value. So we have this\nadvantage estimate. So we talked before\nabout the trade-offs",
    "start": "493270",
    "end": "499760"
  },
  {
    "text": "between these different\nestimates and how some of them would have higher\nbias and some of them would have higher variance.",
    "start": "499760",
    "end": "505610"
  },
  {
    "text": "And what I'm going\nto talk about now is sort of a way\nto use these to try to get to a new form of\nadvantage estimation.",
    "start": "505610",
    "end": "512700"
  },
  {
    "text": "And it also involves\na technique that comes up a lot in\nreinforcement learning. So it's a useful\nthing to be aware of.",
    "start": "512700",
    "end": "517729"
  },
  {
    "text": "So what we're going\nto do is we're going to define something\ncalled delta Vt. Let me just highlight this.",
    "start": "517730",
    "end": "524390"
  },
  {
    "text": "And what this is is it's just\nessentially here, the TD backup.",
    "start": "524390",
    "end": "529890"
  },
  {
    "text": "So this is just what\nwe've often seen where we have our\nimmediate reward plus gamma times V of the next state.",
    "start": "529890",
    "end": "536300"
  },
  {
    "text": "So notice here that\nwe've got a time step t. So that's going to be important.",
    "start": "536300",
    "end": "541760"
  },
  {
    "text": "And then, this is just the\nvalue, our current estimate of the value of the state.",
    "start": "541760",
    "end": "547160"
  },
  {
    "text": "So this should look like normal. This is the same\nadvantage as up here.",
    "start": "547160",
    "end": "553170"
  },
  {
    "text": "But note, I could plug\nin different ts here. So we've defined this\nnew thing called delta.",
    "start": "553170",
    "end": "560130"
  },
  {
    "text": "So when we use this\nnew delta, then we would say that our\nadvantage with--",
    "start": "560130",
    "end": "566785"
  },
  {
    "text": "the advantage\nfunction we've seen before where we use\na TD estimate is just exactly equal to delta Vt.",
    "start": "566785",
    "end": "572750"
  },
  {
    "text": "Why is it V? Well, V is specifying\nhere what value function we're going to plug in.",
    "start": "572750",
    "end": "578810"
  },
  {
    "text": "And this is just\nexactly equal to this. So same as what we saw before. Now, the next thing\nthat we can say",
    "start": "578810",
    "end": "585020"
  },
  {
    "text": "is, well, actually, what\nis the advantage for if we use this two-step estimate.",
    "start": "585020",
    "end": "590900"
  },
  {
    "text": "That is this. So we've got this expression. And that is exactly equal to\ndelta Vt plus delta Vt plus 1.",
    "start": "590900",
    "end": "602210"
  },
  {
    "text": "So I'm just going\nto write that out, so we can see it for one\nsecond for why that's true. So we had rt plus gamma\nV st plus 1 minus V",
    "start": "602210",
    "end": "613940"
  },
  {
    "text": "of st. That's this term. Plus gamma r of t plus 1.",
    "start": "613940",
    "end": "621120"
  },
  {
    "text": "Because notice here,\nthis is t plus 1.",
    "start": "621120",
    "end": "627440"
  },
  {
    "text": "Plus gamma V of st plus\n2 minus V of st plus 1.",
    "start": "627440",
    "end": "633910"
  },
  {
    "text": " So when we do in this way,\nwhat will end up canceling here",
    "start": "633910",
    "end": "643040"
  },
  {
    "text": "is the V of st plus 1. Let me just-- so now, we have\na gamma here in the front.",
    "start": "643040",
    "end": "649360"
  },
  {
    "text": "So we have this term\ncancels with this term. Oops, sorry. Let me do that.",
    "start": "649360",
    "end": "654449"
  },
  {
    "text": "This term cancels this term. ",
    "start": "654450",
    "end": "659730"
  },
  {
    "text": "Let me just be a\nlittle careful here. We're going to have this\nterm cancels with this term.",
    "start": "659730",
    "end": "667650"
  },
  {
    "text": "Good. We're going to get the two\nrewards, rt and rt plus 1. The second one, so\nthat's here and here.",
    "start": "667650",
    "end": "674140"
  },
  {
    "text": "And then, we have gamma\nsquared times V of st plus 2.",
    "start": "674140",
    "end": "679970"
  },
  {
    "text": "So I just wrote out exactly\nwhat the definition was of delta and delta t plus 1. And essentially, the\nimportant thing to see here",
    "start": "679970",
    "end": "686230"
  },
  {
    "text": "is that one of the\nterms canceled. And so that's why\nwe ended up getting exactly the same expression\nfor a 2 as we had before.",
    "start": "686230",
    "end": "696800"
  },
  {
    "text": "And you can repeat this. And what will happen is all\nof those intermediate terms,",
    "start": "696800",
    "end": "704340"
  },
  {
    "text": "these things where\nyou were bootstrapping will cancel along the way.",
    "start": "704340",
    "end": "710120"
  },
  {
    "text": "So this is why it's\ncalled a telescoping sum. Because here, we're adding\nsomething that at the next round",
    "start": "710120",
    "end": "717350"
  },
  {
    "text": "we're going to\nsubtract from this-- the next one. And so those are\ngoing to cancel, and so you just end up\ngetting all the discounted sum",
    "start": "717350",
    "end": "724070"
  },
  {
    "text": "of rewards plus the last term. Who's seen telescoping\nsums before?",
    "start": "724070",
    "end": "730892"
  },
  {
    "text": "OK, so maybe about\nhalf of people here. So for some of you,\nthis is really familiar. For some of you,\nthis might be new.",
    "start": "730892",
    "end": "736478"
  },
  {
    "text": "It's a useful technique\nto know about, because it comes up in a lot\nof the reinforcement learning proofs.",
    "start": "736478",
    "end": "742089"
  },
  {
    "text": "Yeah. Can I ask, just in comparing\nthis to the equation for a hat",
    "start": "742090",
    "end": "749110"
  },
  {
    "text": "sub t 2 above-- Yeah. --are we basically saying\nthat gamma squared V st plus 2",
    "start": "749110",
    "end": "755560"
  },
  {
    "text": "is equal to gamma. V st plus 1? No, we're just saying--\nwe're just actually literally",
    "start": "755560",
    "end": "760690"
  },
  {
    "text": "canceling it. Good question. So when we write\nout this expression, there's a gamma in front.",
    "start": "760690",
    "end": "766310"
  },
  {
    "text": "And because there's\na t plus 1 here, this will become s of t plus 2. This will become s of t plus 1.",
    "start": "766310",
    "end": "772610"
  },
  {
    "text": "And there's a gamma. So this will be a gamma, gamma\ntimes a minus V of st plus 1.",
    "start": "772610",
    "end": "777830"
  },
  {
    "text": "And that will cancel\nwith the V of st plus 1 in the previous one. OK, got it.",
    "start": "777830",
    "end": "784529"
  },
  {
    "text": "Yeah. Yeah. So they had 2t on top\nand on the bottom.",
    "start": "784530",
    "end": "789943"
  },
  {
    "text": "Are they equivalent, though? They're identical. Identical? OK. Yeah. Yeah, that was a good question. So yeah, I've written\nin this notation",
    "start": "789943",
    "end": "796830"
  },
  {
    "text": "now with the delta\nnotation, but this is exactly equal to\nthis, which is the same. So these are identical.",
    "start": "796830",
    "end": "804230"
  },
  {
    "text": "Ooh! Yeah, so thanks. So there's a typo there. That might be the\nquestion, yeah.",
    "start": "804230",
    "end": "810605"
  },
  {
    "text": "Let me just highlight that. So this should be t. ",
    "start": "810605",
    "end": "821320"
  },
  {
    "text": "Yeah, in general, we're\nalways bootstrapping with the final time step. Thanks for catching that.",
    "start": "821320",
    "end": "827755"
  },
  {
    "text": "Yeah, so these all end up\nbeing exactly equivalent. And we've just rewritten it in\nterms of this delta notation using a telescoping sum.",
    "start": "827755",
    "end": "834390"
  },
  {
    "text": "So these are just different\nend step estimators. We haven't done anything\nnew yet in terms-- I mean, we've rewritten things,\nbut we haven't introduced",
    "start": "834390",
    "end": "840920"
  },
  {
    "text": "any new type of estimator. These are just different\nadvantage functions. And as you might\nimagine, the first one",
    "start": "840920",
    "end": "845959"
  },
  {
    "text": "is going to be low\nvariance, high bias. The last one is going to\nbe low bias, high variance.",
    "start": "845960",
    "end": "854170"
  },
  {
    "text": "So Generalized\nAdvantage Estimation involves taking a\nweighted combination",
    "start": "854170",
    "end": "859780"
  },
  {
    "text": "of k-step estimators. So we had here, this was just\nlots of different estimators.",
    "start": "859780",
    "end": "867576"
  },
  {
    "text": "And you might say, well,\nhow do I pick among them? I'm not sure how I'm\ngoing to pick among them. I'm just going to take a\nweighted combination of all",
    "start": "867577",
    "end": "873910"
  },
  {
    "text": "of them. And in particular,\nyou could just take an average\nweighted combination.",
    "start": "873910",
    "end": "879370"
  },
  {
    "text": "So let's just step\nthrough this a little bit, just to see how we do this. So what this is saying here is\nI'm going to take the 1-step",
    "start": "879370",
    "end": "889180"
  },
  {
    "text": "advantage estimator\nplus lambda-- I've introduced a new\nparameter here, lambda. Times the 2-step 1 times lambda\nsquared plus the 3-step 1.",
    "start": "889180",
    "end": "896245"
  },
  {
    "text": "I'm just saying like,\nOK, well, why don't you use all of my estimators? And I'm going to weigh\nmy different estimators.",
    "start": "896245",
    "end": "902380"
  },
  {
    "text": "So now, what I'm going\nto do is I'm going to-- I've next just written\nthis in the delta notation that we saw on the\nprevious slide.",
    "start": "902380",
    "end": "910615"
  },
  {
    "text": "And now, what I'm going to see\nis that some of these terms appear a lot of times. So there's this.",
    "start": "910615",
    "end": "915640"
  },
  {
    "text": "This term appears in all of the\nterms, in all of the advantages.",
    "start": "915640",
    "end": "921400"
  },
  {
    "text": "The second one only appears in\nthe second to the last one, et cetera.",
    "start": "921400",
    "end": "926700"
  },
  {
    "text": "So I'm going to collect terms. So I'm going to write\nthis as follows. ",
    "start": "926700",
    "end": "935839"
  },
  {
    "text": "And this was introduced in\na previous paper to PPO, and then PPO builds on it.",
    "start": "935840",
    "end": "940870"
  },
  {
    "start": "940870",
    "end": "947730"
  },
  {
    "text": "And just notice what\nI've done there. I've noticed that\nI had this term. ",
    "start": "947730",
    "end": "959857"
  },
  {
    "text": "So I'm just taking\nall of those terms, and I'm noticing how many\nlambdas I had in front of them. ",
    "start": "959857",
    "end": "966526"
  },
  {
    "text": "And then, I'm going to\nhave delta t plus 1 V",
    "start": "966526",
    "end": "971960"
  },
  {
    "text": "times lambda times 1 plus--",
    "start": "971960",
    "end": "978080"
  },
  {
    "text": "squared. Or I'll write it differently. ",
    "start": "978080",
    "end": "985860"
  },
  {
    "text": "So this term is going to start\nwith lambda plus lambda squared,",
    "start": "985860",
    "end": "991269"
  },
  {
    "text": "because it's in the\nsecond through all the rest of the terms. Plus-- OK.",
    "start": "991270",
    "end": "1005090"
  },
  {
    "text": "All right, so I'm just\nrearranging the sums. And now, when I look at\nthis, I realize that I've got a geometric series.",
    "start": "1005090",
    "end": "1011420"
  },
  {
    "text": "So this is just going\nto be equal to 1 minus lambda times lambda t V\ndivided by 1 minus lambda plus--",
    "start": "1011420",
    "end": "1021940"
  },
  {
    "text": "let me just make\nsure I've got it. There was-- I'll put it\non the next page just",
    "start": "1021940",
    "end": "1028530"
  },
  {
    "text": "to make sure I made a-- there\nshould have been a gamma here. Let me just put gamma.",
    "start": "1028530",
    "end": "1033900"
  },
  {
    "start": "1033900",
    "end": "1041990"
  },
  {
    "text": "I'll write out cleanly\nin the next slide, so this will be clear. So we also had a gamma\nhere from before.",
    "start": "1041990",
    "end": "1050240"
  },
  {
    "text": "So gamma squared. And then what you\ndo is you realize this is a geometric series that\ngoes to 1 over 1 minus lambda.",
    "start": "1050240",
    "end": "1058309"
  },
  {
    "text": "And then, this is gamma times\nlambda 1 over 1 minus lambda. And this is gamma squared\ntimes lambda squared.",
    "start": "1058310",
    "end": "1064010"
  },
  {
    "text": "I'm just using the fact that\nthis is a geometric series. It's fine if you\nhaven't seen this. If you've done real analysis,\nyou've seen this before.",
    "start": "1064010",
    "end": "1072140"
  },
  {
    "text": "And that means that\nthe term below just looks like the following.",
    "start": "1072140",
    "end": "1077330"
  },
  {
    "text": "And this was introduced\nby a previous paper. And the idea there\nis to say, well, why don't we just take kind\nof a weighted average of all",
    "start": "1077330",
    "end": "1083660"
  },
  {
    "text": "these different terms that have\ndifferent biases and variances, and then we can\nre-express it compactly.",
    "start": "1083660",
    "end": "1089348"
  },
  {
    "text": "We don't actually have to\ncompute all of the advantages separately and track them. We just are going to keep\ntrack of these deltas.",
    "start": "1089348",
    "end": "1095540"
  },
  {
    "text": "And these deltas are pretty easy\nto keep track of, because those are just like these one-step\ndifferences between--",
    "start": "1095540",
    "end": "1101190"
  },
  {
    "text": "so just remind ourselves\nwhat the deltas look like. The deltas are pretty\neasy to keep track of,",
    "start": "1101190",
    "end": "1106340"
  },
  {
    "text": "because they're\njust the difference between your previous\nestimate and your new reward plus gamma V of st plus 1.",
    "start": "1106340",
    "end": "1113000"
  },
  {
    "text": "So you can just keep\ntrack of those over time, and then you're\njust weighing them.",
    "start": "1113000",
    "end": "1119130"
  },
  {
    "text": "And our derivation\njust followed it. And so then you\njust sum these up, and you essentially\nhave different weights.",
    "start": "1119130",
    "end": "1124472"
  },
  {
    "text": " All right, so let's\nthink about what this means in terms\nof bias and variance,",
    "start": "1124472",
    "end": "1131940"
  },
  {
    "text": "as we often like to in terms\nof the estimators we're using. So this is trying to be an\nestimate of the advantage. And we'll do a trickier\nunderstanding now about",
    "start": "1131940",
    "end": "1139310"
  },
  {
    "text": "how different choices-- so this is a discount vector.",
    "start": "1139310",
    "end": "1145350"
  },
  {
    "text": "So this is the discount factor,\ncomma, your choice of lambda, which is how much\nyou're weighting",
    "start": "1145350",
    "end": "1150830"
  },
  {
    "text": "earlier ones versus later ones. So GAE is generally a function\nof these two hyperparameters.",
    "start": "1150830",
    "end": "1157070"
  },
  {
    "text": "And let's think for\na second about what this does for bias and variance\nand how it relates to t.",
    "start": "1157070",
    "end": "1162330"
  },
  {
    "start": "1162330",
    "end": "1220980"
  },
  {
    "text": "Can you select multiple or no? You can? OK, good.",
    "start": "1220980",
    "end": "1226130"
  },
  {
    "text": "I was going to say, otherwise,\nTA that's helping make these, I can just check with them. ",
    "start": "1226130",
    "end": "1241546"
  },
  {
    "text": "And feel free to go\nto the previous slide to look at the definitions. ",
    "start": "1241546",
    "end": "1253885"
  },
  {
    "text": "And the reason these\nall are really important is because if you get better\nestimates of the advantage, you're going to get better\nestimates of the gradient.",
    "start": "1253885",
    "end": "1259340"
  },
  {
    "text": "If you get better\nestimates of the gradient, you can hopefully\nuse less data to get to that really good policy.",
    "start": "1259340",
    "end": "1265450"
  },
  {
    "text": "So that's why people spend quite\na lot of effort thinking about-- with using deep neural networks,\nboth either for the advantage",
    "start": "1265450",
    "end": "1271840"
  },
  {
    "text": "or for the policy,\nhow can we really quickly get good estimates? Is [INAUDIBLE] comma\n0 even defined?",
    "start": "1271840",
    "end": "1279640"
  },
  {
    "text": "[INAUDIBLE] 0 to 0. There's a 0-- is this defined?",
    "start": "1279640",
    "end": "1285220"
  },
  {
    "text": "Yeah, because in the\nfirst term, there will be 0 to 0 for the x term. 0 to 0.",
    "start": "1285220",
    "end": "1290810"
  },
  {
    "text": "There shouldn't be to the x-- oh, you mean here? ",
    "start": "1290810",
    "end": "1297730"
  },
  {
    "text": "In that case, you\nwould just plug 0 in up here, and then\nthat would disappear.",
    "start": "1297730",
    "end": "1303452"
  },
  {
    "text": "And you would just\nget this number. ",
    "start": "1303452",
    "end": "1316813"
  },
  {
    "text": "All right, why don't you\nturn to your neighbor and see if you got\nthe same answer? ",
    "start": "1316813",
    "end": "1323770"
  },
  {
    "text": "Because the definition\nof 0 to 0 is 1. Yeah, it's easier just\nto look at this one.",
    "start": "1323770",
    "end": "1331915"
  },
  {
    "text": "[SIDE CONVERSATIONS] ",
    "start": "1331915",
    "end": "1453830"
  },
  {
    "text": "OK, so thanks for\nthe good question. I'll make sure to\nclarify the notation.",
    "start": "1453830",
    "end": "1459270"
  },
  {
    "text": "If lambda is equal to-- whoopsie. I'll make sure to clarify that--",
    "start": "1459270",
    "end": "1466020"
  },
  {
    "text": "let's just say if lambda is\nequal to 0, look at first line.",
    "start": "1466020",
    "end": "1473545"
  },
  {
    "text": " OK. ",
    "start": "1473545",
    "end": "1479745"
  },
  {
    "text": "And I'll make sure to clarify\nthat in next year's slides. So in that case,\neverything drops off.",
    "start": "1479745",
    "end": "1486169"
  },
  {
    "text": "So if lambda is equal to 0, all\nthe other estimators go away. Basically, you have no weight on\nall of the advantage estimators",
    "start": "1486170",
    "end": "1494919"
  },
  {
    "text": "that are 2 or more. And so it just becomes\nthe first term. And the first term\nis the TD estimator.",
    "start": "1494920",
    "end": "1500240"
  },
  {
    "text": "Yes? When lambda is equal to 1,\nshouldn't the entire thing becomes 0?",
    "start": "1500240",
    "end": "1506685"
  },
  {
    "text": "If what? If lambda is 1. If lambda is 1. Yeah, so if the lambda is\n1, well, then you're also--",
    "start": "1506686",
    "end": "1513070"
  },
  {
    "text": "you're summing from an\ninfinite number of terms, too. But yes-- well, so this is true.",
    "start": "1513070",
    "end": "1521150"
  },
  {
    "text": "So b is true. And the second one, we'll\nsee on the next slide. It's a little weird to write\ndown in this fractional infinite",
    "start": "1521150",
    "end": "1529179"
  },
  {
    "text": "horizon, because you\ncan't ever do Monte Carlo returns with infinite horizon. But it's a good-- you\nguys have good questions.",
    "start": "1529180",
    "end": "1536210"
  },
  {
    "text": "I'll make sure to clarify\nwhy what happens in the age equals infinity [INAUDIBLE]\nand lambda equals 0 cases.",
    "start": "1536210",
    "end": "1544280"
  },
  {
    "text": "Just so that the\ninfinities are clear. But this is certainly false,\nbecause this is not TD 0,",
    "start": "1544280",
    "end": "1550492"
  },
  {
    "text": "because we'd have a whole\nbunch of terms here. And there'd be this weird\nwaiting in that case. And then, this-- because then\nyou have to weigh how much",
    "start": "1550493",
    "end": "1559802"
  },
  {
    "text": "is this term versus the\ninfinity of the other-- like, the 0 versus the infinity\nof the other term. So I'll make sure\nto clarify that.",
    "start": "1559802",
    "end": "1566480"
  },
  {
    "text": "D is also true, because\nonce this is a TD estimate, then we generally\nknow TD estimates have",
    "start": "1566480",
    "end": "1572390"
  },
  {
    "text": "higher bias and lower variance. So sort of true.",
    "start": "1572390",
    "end": "1579780"
  },
  {
    "text": "Now, note in general,\nyou would think, therefore, you want to put\nlambda somewhere in the middle.",
    "start": "1579780",
    "end": "1585800"
  },
  {
    "text": "Because it will be balancing\nbetween bias and variance. But what they do in PPO\nis a little bit different,",
    "start": "1585800",
    "end": "1591757"
  },
  {
    "text": "but it's related to this. So this is what the Generalized\nAdvantage Estimation is. You do this like\nexponential weighting",
    "start": "1591758",
    "end": "1598670"
  },
  {
    "text": "over lots of different\nadvantage estimators, but without actually\nhaving to have separate copies and memory of\nall the advantage estimators.",
    "start": "1598670",
    "end": "1605460"
  },
  {
    "text": "So that's why this is nice. So what we're going to do now\nis see what we actually-- what",
    "start": "1605460",
    "end": "1612830"
  },
  {
    "text": "they actually did in PPO. Which is instead of\ndoing all of these, we're just going to\ndo a finite number.",
    "start": "1612830",
    "end": "1619830"
  },
  {
    "text": "So what they're going to\ndo is a truncated version,",
    "start": "1619830",
    "end": "1624870"
  },
  {
    "text": "where they use this, but they\nonly go up to a certain point.",
    "start": "1624870",
    "end": "1630150"
  },
  {
    "text": "So they're not going to\ngo out for to forever. There's multiple\nbenefits to this, including the fact that they're\ngoing to be in episodic domains.",
    "start": "1630150",
    "end": "1637480"
  },
  {
    "text": "And what this means\nis that, let's say, your horizon is very\nlong, but not infinity. So your horizon might be\nsomething like 2,000 steps",
    "start": "1637480",
    "end": "1644340"
  },
  {
    "text": "for your Mont Car or\nsomething like that. You might pick t\nequal to be 200.",
    "start": "1644340",
    "end": "1649779"
  },
  {
    "text": "And what that would mean is--\nso remember the benefit-- one of the benefits of temporal\ndifference learning compared to Monte Carlo is\nthat you can update",
    "start": "1649780",
    "end": "1655950"
  },
  {
    "text": "your estimate after every step. The problem with the advantage\nestimator that is defined here",
    "start": "1655950",
    "end": "1661889"
  },
  {
    "text": "is you still have to wait\ntill the very end to update your estimator, because you need\nyour advantage near infinity,",
    "start": "1661890",
    "end": "1670038"
  },
  {
    "text": "and then you're going\nto weigh all of them. So you don't actually want\nto do that in practice. So one thing that PPO\nproposes is to say, well,",
    "start": "1670038",
    "end": "1676902"
  },
  {
    "text": "why don't we just do\na truncated version? And that means every\nT-steps, like big T-steps. So let's say t is 200.",
    "start": "1676902",
    "end": "1682830"
  },
  {
    "text": "Every 200 time steps,\nyou can compute this. You compute your new sort\nof weighted average--",
    "start": "1682830",
    "end": "1691440"
  },
  {
    "text": "advantage estimator,\nand then update. So you can think of\nthe big T here is",
    "start": "1691440",
    "end": "1698230"
  },
  {
    "text": "determining how\nlong you have to go before you can make an update.",
    "start": "1698230",
    "end": "1703299"
  },
  {
    "text": "So that's what they do in PPO. They use this truncated\nGeneralized Advantage Estimation in order to\nget better estimators.",
    "start": "1703300",
    "end": "1713860"
  },
  {
    "text": "OK. Anybody have any questions about\nthat before we move on to-- going back to this question\nof monotonic improvement?",
    "start": "1713860",
    "end": "1720070"
  },
  {
    "start": "1720070",
    "end": "1725200"
  },
  {
    "text": "OK, so now, let's go on to\nanother important feature of PPO, which is it's\nreally sort of in some ways",
    "start": "1725200",
    "end": "1730660"
  },
  {
    "text": "going backwards. But I wanted to make sure\nto go through the algorithm for a last time, so that\nyou guys could start",
    "start": "1730660",
    "end": "1735670"
  },
  {
    "text": "working on implementation. But I think, and\nas in many papers, the theory is a\nlittle bit decoupled",
    "start": "1735670",
    "end": "1741910"
  },
  {
    "text": "from what's actually done, but\nit sort of serves as motivation. So I think it's\nuseful to go back",
    "start": "1741910",
    "end": "1748000"
  },
  {
    "text": "to the bound that\nwas proposed there that helped inspire\ntheir algorithm and think about what it\nactually implies about what",
    "start": "1748000",
    "end": "1754720"
  },
  {
    "text": "happens when we do updates. So remember that-- and as\nyou are proving right now",
    "start": "1754720",
    "end": "1760030"
  },
  {
    "text": "for Homework 2, remember\nthat what we do in-- what they were thinking\nof doing in PPO",
    "start": "1760030",
    "end": "1767110"
  },
  {
    "text": "was to say we want to be able to\nuse our old data from a policy pi to estimate the performance\nof policy pi prime.",
    "start": "1767110",
    "end": "1774460"
  },
  {
    "text": "But the problem is\nthat in general, that's going to induce a\ndifferent state distribution. And so we played this\napproximation and said,",
    "start": "1774460",
    "end": "1781970"
  },
  {
    "text": "let's just ignore the difference\nin the state distributions. And that's great, because now we\ncan use our old data to estimate",
    "start": "1781970",
    "end": "1789280"
  },
  {
    "text": "the value of our new policy. Only our old data. Because we always know what\nthe actual policy parameter is,",
    "start": "1789280",
    "end": "1795400"
  },
  {
    "text": "but we don't actually have\nto gather new data from it. And we called this\nsort of L pi pi prime, because pi prime is\nhere, but everything else",
    "start": "1795400",
    "end": "1803290"
  },
  {
    "text": "is being used by pi. And what was proven was that\nif your two policies have",
    "start": "1803290",
    "end": "1810250"
  },
  {
    "text": "a close KL-divergence in\nterms of the actual actions they take, then you get\nthis bound on performance.",
    "start": "1810250",
    "end": "1817940"
  },
  {
    "text": "OK, so it said this\napproximation is not too bad. And in particular,\nwe get this thing",
    "start": "1817940",
    "end": "1825179"
  },
  {
    "text": "of this monotonic\nimprovement theory, saying that the\nvalue here-- so I'll just write down that J of\npi is equal to V of pi.",
    "start": "1825180",
    "end": "1835090"
  },
  {
    "text": "Some people use J.\nSome people use V. We mostly use V in the class. OK, so the value of\nyour new policy pi",
    "start": "1835090",
    "end": "1841710"
  },
  {
    "text": "prime minus the value\nof the old policy is greater than or\nequal to this term",
    "start": "1841710",
    "end": "1847410"
  },
  {
    "text": "that we had on the\nprevious slide, so this L term, this whole thing. ",
    "start": "1847410",
    "end": "1854700"
  },
  {
    "text": "I'll just draw [INAUDIBLE].  Minus this sort of error\nthat we get from the fact",
    "start": "1854700",
    "end": "1863360"
  },
  {
    "text": "that we are approximating the\nstate distribution by something that's not true. So we have this nice bound.",
    "start": "1863360",
    "end": "1871010"
  },
  {
    "text": "And now, what we're\ngoing to go through now is to show why if we maximize\nwith respect to the right hand",
    "start": "1871010",
    "end": "1877880"
  },
  {
    "text": "side, that we are guaranteed\nto improve over pi.",
    "start": "1877880",
    "end": "1883230"
  },
  {
    "text": "That shouldn't necessarily--\nwell, I'll ask first. So who has seen this\nsort of majorize, maximize algorithm before?",
    "start": "1883230",
    "end": "1889970"
  },
  {
    "text": "I wouldn't expect you to. [INAUDIBLE] So this kind of goes back--",
    "start": "1889970",
    "end": "1896240"
  },
  {
    "text": "I think we've seen ideas related\nto this in policy improvement",
    "start": "1896240",
    "end": "1901730"
  },
  {
    "text": "from the very beginning. But this is different, because\nwe've got these bound sets. So what this is saying\nis this is a lower bound.",
    "start": "1901730",
    "end": "1908630"
  },
  {
    "text": "This says that the difference\nbetween these two policies is at least as big as\nthis term minus this term.",
    "start": "1908630",
    "end": "1915950"
  },
  {
    "text": "But it shouldn't-- and what\nwe're actually going to propose to do is to say,\nall right, well, if we try to pick a pi prime\nthat maximizes this lower bound,",
    "start": "1915950",
    "end": "1925010"
  },
  {
    "text": "does that actually mean that\nwe're going to be guaranteed to improve over pi?",
    "start": "1925010",
    "end": "1930058"
  },
  {
    "text": "And it shouldn't\nnecessarily be immediately obvious that would be true. But it's going to turn\nout that that's the case.",
    "start": "1930058",
    "end": "1936299"
  },
  {
    "text": "So let's just go\nthrough the proof for that, which is pretty cool. All right, so we're going to\nprove that if you do this,",
    "start": "1936300",
    "end": "1942840"
  },
  {
    "text": "if what you try to do\nis pick a policy, pi k plus 1, which is the\nargmax of this lower bound,",
    "start": "1942840",
    "end": "1950110"
  },
  {
    "text": "that you will, in fact, get\na new policy that's either at a local optima or\nis actually better",
    "start": "1950110",
    "end": "1956490"
  },
  {
    "text": "than your previous policy. So that's the idea of\nwhat we're trying to do. So note a few things.",
    "start": "1956490",
    "end": "1964090"
  },
  {
    "text": "So pi-- so we're going to\nassume that we have some pi K. That's our previous policy.",
    "start": "1964090",
    "end": "1970890"
  },
  {
    "text": "And that it was feasible. ",
    "start": "1970890",
    "end": "1976810"
  },
  {
    "text": "So it's a well-defined policy. Sums to 1. It satisfies all of\nthose constraints.",
    "start": "1976810",
    "end": "1983540"
  },
  {
    "text": "OK, so now, let's just\nwrite it in terms of--",
    "start": "1983540",
    "end": "1995000"
  },
  {
    "text": "OK, so now, recall that-- ",
    "start": "1995000",
    "end": "2000030"
  },
  {
    "text": "let's just do something\na little silly, but it's going to be useful. OK, so we're going to look at\nwhat L pi of pi k of pi k is.",
    "start": "2000030",
    "end": "2009372"
  },
  {
    "text": "That's this term. Let's just see what that\nis if we just plug in, if we try to evaluate\nwhat that term-- what",
    "start": "2009372",
    "end": "2014850"
  },
  {
    "text": "that sort of expression\nis when we plug in the same policy\nas what we actually used to gather our data.",
    "start": "2014850",
    "end": "2020370"
  },
  {
    "text": "All right, so remember that\nwould just be equal to 1 over 1 minus gamma, expected value\nover s according to d pi k.",
    "start": "2020370",
    "end": "2028990"
  },
  {
    "text": "Just writing down the\ndefinition of what L is. ",
    "start": "2028990",
    "end": "2035410"
  },
  {
    "text": "And this is going to be pi\nk of a given s divided by pi k of a given s times A of pi k.",
    "start": "2035410",
    "end": "2044065"
  },
  {
    "text": " All right, well,\nthis is just one.",
    "start": "2044065",
    "end": "2050760"
  },
  {
    "text": "So this cancels. But the important\nthing to remember here",
    "start": "2050760",
    "end": "2056989"
  },
  {
    "text": "is that the advantage function\nof a policy with respect to itself is 0.",
    "start": "2056989",
    "end": "2062364"
  },
  {
    "text": " So if I take actions according\nto the current policy",
    "start": "2062364",
    "end": "2069199"
  },
  {
    "text": "and compare what the value is\nto taking actions according to that current policy,\nand then acting according",
    "start": "2069199",
    "end": "2076339"
  },
  {
    "text": "to the current policy,\nminus f first taking actions according to the current\npolicy, that difference is 0.",
    "start": "2076340",
    "end": "2083879"
  },
  {
    "text": "So that's the-- I can just write that\nout, too, in case. So just remember,\nwhat we have here is",
    "start": "2083880",
    "end": "2090290"
  },
  {
    "text": "we're going to have Q pi k\nof s, a minus V pi k of s.",
    "start": "2090290",
    "end": "2102250"
  },
  {
    "text": "But notice what we have here\nis that what are we taking-- what's the distribution? We're taking these actions.",
    "start": "2102250",
    "end": "2107820"
  },
  {
    "text": "It's exactly pi k. So Q pi k, if you first follow--",
    "start": "2107820",
    "end": "2114430"
  },
  {
    "text": "like, I can just write that\nout just in case it's helpful. So this is like sum over A pi\nk of a given s, Q pi k s, a,",
    "start": "2114430",
    "end": "2127299"
  },
  {
    "text": "which is just equal to V pi k. It's like if you start\ntaking this action",
    "start": "2127300",
    "end": "2132862"
  },
  {
    "text": "and you follow the\npolicy, and then you follow the policy from all\nfuture time steps versus if you just follow the policy\nfrom now till forever,",
    "start": "2132862",
    "end": "2139550"
  },
  {
    "text": "that's exactly the same. So that means that this is 0.",
    "start": "2139550",
    "end": "2144560"
  },
  {
    "text": "And that's good. And that means that because-- if we think back to\nwhat this looks like, that says that the difference\nbetween the value of the policy",
    "start": "2144560",
    "end": "2152390"
  },
  {
    "text": "and the policy itself is 0. So this bound is tight\nif you are evaluating it",
    "start": "2152390",
    "end": "2157910"
  },
  {
    "text": "with respect to itself. There is no difference\nbetween the value of the policy and the\npolicy itself, because the--",
    "start": "2157910",
    "end": "2163055"
  },
  {
    "text": "oh, I'll say the next thing. So then, because also D KL of\npi k over pi k is equal to 0.",
    "start": "2163055",
    "end": "2175560"
  },
  {
    "text": "There is no KL. The KL-divergence between a\ndistribution and itself is 0.",
    "start": "2175560",
    "end": "2180680"
  },
  {
    "text": " All right, so now, let\nme just label these two.",
    "start": "2180680",
    "end": "2186630"
  },
  {
    "text": "So let's call this\nterm 1 and this term 2.",
    "start": "2186630",
    "end": "2193925"
  },
  {
    "text": "So what we have here is we\nhave that term 1 is 0 for pi k",
    "start": "2193926",
    "end": "2199440"
  },
  {
    "text": "and term 2 is 0 for pi k. All right, so that\nmeans 1 minus 2",
    "start": "2199440",
    "end": "2207030"
  },
  {
    "text": "has to be at least\nas great as 0. Does somebody want\nto say why that is?",
    "start": "2207030",
    "end": "2214080"
  },
  {
    "text": "I made it-- that's\nnot immediately obvious from these steps yet. You have to make one more step. And it has to do\nwith the argmax.",
    "start": "2214080",
    "end": "2220230"
  },
  {
    "text": "Anybody see why that\nis and want to share? ",
    "start": "2220230",
    "end": "2225819"
  },
  {
    "text": "Why is 1 minus 2 always have to\nbe greater than or equal to 0? ",
    "start": "2225820",
    "end": "2234349"
  },
  {
    "text": "Given the argmax. [INAUDIBLE] because we know of\nthe policy that's going to be 0.",
    "start": "2234350",
    "end": "2242660"
  },
  {
    "text": "Achieve 0 for [INAUDIBLE] Exactly. Exactly what [MUTED] said. Yeah. So pi k is an existence\nproof, that there",
    "start": "2242660",
    "end": "2248510"
  },
  {
    "text": "exists at least one policy for\nwhich the right hand side is 0. We're taking an argmax over\nthe whole policy space.",
    "start": "2248510",
    "end": "2254820"
  },
  {
    "text": "That means the argmax has\nto have value at least 0, hopefully better than. And so that is exactly why.",
    "start": "2254820",
    "end": "2261869"
  },
  {
    "text": "So because argmax is at\nleast as good as pi k",
    "start": "2261870",
    "end": "2276050"
  },
  {
    "text": "because we're trying\nto maximize that. OK, so what that means\nthen is that-- so remember,",
    "start": "2276050",
    "end": "2281480"
  },
  {
    "text": "all of this term here\non the right hand side was what we had here.",
    "start": "2281480",
    "end": "2287240"
  },
  {
    "text": "So this whole-- so we had J\npi k plus 1 minus J of pi k",
    "start": "2287240",
    "end": "2296460"
  },
  {
    "text": "is greater than or equal to term\n1 minus term 2, which we just",
    "start": "2296460",
    "end": "2303430"
  },
  {
    "text": "showed is greater\nthan or equal to 0. So what we just proved is that\nby maximizing with respect",
    "start": "2303430",
    "end": "2308890"
  },
  {
    "text": "to our lower bound, we got a\nnew policy that was at least as good as the old policy.",
    "start": "2308890",
    "end": "2314470"
  },
  {
    "text": "Which is really cool. So that means that\nusing a lower bound on the gap between the\nperformance of the policies",
    "start": "2314470",
    "end": "2323170"
  },
  {
    "text": "is sufficient to allow us to\nmake monotonic improvement. So that's super elegant. So now we could have something\nif we actually did this--",
    "start": "2323170",
    "end": "2330100"
  },
  {
    "text": "most policies do not\ndo this, and we'll talk about that in a second. But if you actually\ndid this, you would get monotonic improvement.",
    "start": "2330100",
    "end": "2336770"
  },
  {
    "text": "And there's certainly\na number of domains where it'd be really cool to\nget monotonic improvement. So I think I've mentioned\neducation before,",
    "start": "2336770",
    "end": "2342770"
  },
  {
    "text": "but you could imagine\nhealth care as well, like there are a lot of\ncases if you're doing stuff in the intensive\ncare unit, et cetera,",
    "start": "2342770",
    "end": "2348200"
  },
  {
    "text": "you people might be kind\nof worried about doing random exploration\nor epsilon-greedy. But if you could,\nsay, we're, only",
    "start": "2348200",
    "end": "2354310"
  },
  {
    "text": "going to improve when we\nknow that the new policy is at least as good as\nthe old policy, that's",
    "start": "2354310",
    "end": "2359830"
  },
  {
    "text": "likely to be a scenario\nthat's much more palatable. ",
    "start": "2359830",
    "end": "2365930"
  },
  {
    "text": "All right, so I wrote this\nout a little bit more here. And one of the elegant\nthings about this is that we can\nrestrict ourselves",
    "start": "2365930",
    "end": "2373990"
  },
  {
    "text": "to parameterize policies. This doesn't mean we have to\nhave completely-- we can think",
    "start": "2373990",
    "end": "2379870"
  },
  {
    "text": "about any sort of policy class. And as long as we initialize--\nso our initial policy is in that class.",
    "start": "2379870",
    "end": "2385490"
  },
  {
    "text": "It could be a Gaussian. It could be a deep\nneural network. Then, you will-- and then,\nyou keep doing argmax",
    "start": "2385490",
    "end": "2393040"
  },
  {
    "text": "over your policy class. You'll get this\nmonotonic improvement. So it's really nice.",
    "start": "2393040",
    "end": "2398210"
  },
  {
    "text": "It's really elegant that you\ncould do it in this case. But unfortunately, like many\nbeautiful theory things,",
    "start": "2398210",
    "end": "2405230"
  },
  {
    "text": "it has some limitations. So if you look at the\nactual-- so C is a constant. And we haven't went through\nwhat the constant is in class,",
    "start": "2405230",
    "end": "2411650"
  },
  {
    "text": "but you're welcome to\nlook it up in the paper. When gamma is near 1, and\nwhat gamma near 1 means",
    "start": "2411650",
    "end": "2417250"
  },
  {
    "text": "is that we care almost as much\nabout long horizon rewards as we do about\nimmediate rewards.",
    "start": "2417250",
    "end": "2422859"
  },
  {
    "text": "When it is close to 1,\ngamma is pretty large. And so what that means is that,\nin general, that second term can",
    "start": "2422860",
    "end": "2430930"
  },
  {
    "text": "make you be very conservative. So why is that? Well, that means you've got--\nif C is really large, that means",
    "start": "2430930",
    "end": "2437530"
  },
  {
    "text": "that if your new policy\ntakes actions that are quite different than your old\npolicy, you're going",
    "start": "2437530",
    "end": "2442780"
  },
  {
    "text": "to have a really big penalty. So what that basically does\nis it shrinks your step size. It says this is going to be\na term that is weighed a lot.",
    "start": "2442780",
    "end": "2452269"
  },
  {
    "text": "And unless you only\nmake very small changes, you could get a big penalty. Essentially, because you're\nsaying, I'm really not sure.",
    "start": "2452270",
    "end": "2458750"
  },
  {
    "text": "It might be that when\nI change my policy, I end up with very different\nstate distributions. And I don't know what the\nrewards would be there.",
    "start": "2458750",
    "end": "2465638"
  },
  {
    "text": "So what that means is that\nin practice, if you actually try to use this\nequation directly, just straight from the theory,\nthe step sizes are too small.",
    "start": "2465638",
    "end": "2473095"
  },
  {
    "text": "Now, when people say\nthey're too small, that doesn't mean that there's\nanything wrong with them. It just means it's going\nto take way too long.",
    "start": "2473095",
    "end": "2479450"
  },
  {
    "text": "It just means that\npeople are impatient, but they're impatient or were\nbeing very sample inefficient.",
    "start": "2479450",
    "end": "2486390"
  },
  {
    "text": "So it means that\nthis is reasonable. It will hold. You will get\nmonotonic improvement. It's just going to take a\nreally, really long time.",
    "start": "2486390",
    "end": "2493260"
  },
  {
    "text": "And it's not going to be\nfeasible for a lot of things, or it's not going\nto be practical. And so that is\nwhat sort of helped",
    "start": "2493260",
    "end": "2498530"
  },
  {
    "text": "motivate why you\nmight want to tune the KL penalty, which\nwe saw last time, where you sort increase or\ndecrease how much you care",
    "start": "2498530",
    "end": "2504859"
  },
  {
    "text": "about this penalty,\nor use a trust region, or use the clipping.",
    "start": "2504860",
    "end": "2510240"
  },
  {
    "text": "And so that's why we see a\ndifference between what's formally guaranteed by if\nyou were to just directly use",
    "start": "2510240",
    "end": "2515640"
  },
  {
    "text": "this lower bound versus what's\nactually done in practice. But I think in terms\nof the take-homes",
    "start": "2515640",
    "end": "2522640"
  },
  {
    "text": "from this part on\npolicy gradient and PPO is that\nit's really useful to know that you don't just\nhave to take one gradient step.",
    "start": "2522640",
    "end": "2530960"
  },
  {
    "text": "You can be much\nmore data efficient. You can play this\ntrick of pretending there's no change in\nthe state distribution",
    "start": "2530960",
    "end": "2537309"
  },
  {
    "text": "in order to take\nseveral gradient steps and that you can do\nthat while still trying to maybe approximately get\nmonotonic improvement--",
    "start": "2537310",
    "end": "2546170"
  },
  {
    "text": "PPO does not guarantee\nmonotonic improvement, but it can be pretty close-- by thinking explicitly\nabout these lower bounds,",
    "start": "2546170",
    "end": "2553540"
  },
  {
    "text": "and how much your\nperformance might change, and how much essentially\nyour state distribution might change, so that when\nyou're not confident",
    "start": "2553540",
    "end": "2560290"
  },
  {
    "text": "in these approximations. It also uses generalized\nadvantage estimation,",
    "start": "2560290",
    "end": "2565580"
  },
  {
    "text": "which can be helpful. And as I mentioned before,\nit's extremely popular. You can use it in\nmany, many places,",
    "start": "2565580",
    "end": "2571490"
  },
  {
    "text": "in part, because also you\ndon't need your reward function to be differentiable. So people have used\nit in lots of domains.",
    "start": "2571490",
    "end": "2577200"
  },
  {
    "text": " And the other thing\nthat I think is just",
    "start": "2577200",
    "end": "2583090"
  },
  {
    "text": "useful to remember when we\nthink about policy gradients is that you can also use them\nwith actor-critic methods.",
    "start": "2583090",
    "end": "2588710"
  },
  {
    "text": "So you can have\ndeep neural networks to approximate your\nvalue function,",
    "start": "2588710",
    "end": "2593980"
  },
  {
    "text": "and then use that for\nyour advantage estimation and combine them. And so that's what\nmost people do, is that they have some sort\nof critic, a.k.a., your value",
    "start": "2593980",
    "end": "2601480"
  },
  {
    "text": "function estimate, and a policy. And these are only to--",
    "start": "2601480",
    "end": "2606580"
  },
  {
    "text": "reinforce and PPO are, of\ncourse, not the only policy gradient algorithms, but they\nare the backbone to-- well,",
    "start": "2606580",
    "end": "2612550"
  },
  {
    "text": "they're still used\nempirically a lot. And then also\nthey're the backbone to many of the other ones. So if you read other\npapers, they'll",
    "start": "2612550",
    "end": "2618890"
  },
  {
    "text": "be really useful baselines\nthat you often see or that people are building on. All right, we're now going to\ngo into imitation learning.",
    "start": "2618890",
    "end": "2626230"
  },
  {
    "text": "But does anybody have any\nquestions before we start there? ",
    "start": "2626230",
    "end": "2634040"
  },
  {
    "text": "Yeah. On slide 22, just a general\nquestion-- or, I guess,",
    "start": "2634040",
    "end": "2640060"
  },
  {
    "text": "sorry, the one before this. Yeah, so this-- does that mean\nwhen the policy is more myopic",
    "start": "2640060",
    "end": "2648380"
  },
  {
    "text": "and gamma near 0, then your\nstep size will be like--",
    "start": "2648380",
    "end": "2655769"
  },
  {
    "text": "you'll be able to improve\nmore to a greater extent?",
    "start": "2655770",
    "end": "2661400"
  },
  {
    "text": "Yeah, that's a great question. So like, is the converse good? So if gamma is near\n0, is this practical?",
    "start": "2661400",
    "end": "2669880"
  },
  {
    "text": "I don't actually know\noff the top of my head what C looks like for gamma\nequals 0 or not 0, but near 0.",
    "start": "2669880",
    "end": "2676530"
  },
  {
    "text": "So I don't think anybody\nuses this in practice. I think they always use the\nclipping or the KL trust region.",
    "start": "2676530",
    "end": "2683850"
  },
  {
    "text": "So my guess is that it's\nstill not practical. Oftentimes, the C constants will\noften be a function of V max,",
    "start": "2683850",
    "end": "2690750"
  },
  {
    "text": "like your maximum value, often\nscaled by 1 over 1 minus gamma.",
    "start": "2690750",
    "end": "2696510"
  },
  {
    "text": "So it can really be quite\nenormous in many cases. So it might be that here\nit was particularly--",
    "start": "2696510",
    "end": "2703728"
  },
  {
    "text": "they might be interested in\ncases where your horizon is pretty large or where you-- I think one thing here,\ntoo, is that if we're",
    "start": "2703728",
    "end": "2709380"
  },
  {
    "text": "in the episodic case, there's\nnot really a good reason to think that the\ndiscount factor shouldn't be near 1, because you probably\nactually do just care about all",
    "start": "2709380",
    "end": "2716280"
  },
  {
    "text": "the rewards. So they're probably mostly\ninterested in domains, where they didn't think\nit was reasonable. But yeah, that's\na good question.",
    "start": "2716280",
    "end": "2722120"
  },
  {
    "text": " All right, let's talk\nabout imitation learning.",
    "start": "2722120",
    "end": "2730560"
  },
  {
    "text": "So as we've said before, in\ngeneral, in computer science, we like to try to\nreduce things if we can.",
    "start": "2730560",
    "end": "2736183"
  },
  {
    "text": "We like to reduce\nthem to other problems that we know how to solve. And so imitation\nlearning is going to be our attempt to try to do\nthat, at least in certain ways,",
    "start": "2736183",
    "end": "2743920"
  },
  {
    "text": "for all of\nreinforcement learning. And some of these slides come\nfrom some of my colleagues",
    "start": "2743920",
    "end": "2749910"
  },
  {
    "text": "at Berkeley and at CMU. So in general, we're going to\nnow be thinking about the case",
    "start": "2749910",
    "end": "2755370"
  },
  {
    "text": "where we're not going to\nbe gathering data online. So we saw in PPO that we tried\nto reuse our data little bit",
    "start": "2755370",
    "end": "2761160"
  },
  {
    "text": "more to take bigger steps. But one thing you\nmight wonder is, well, why do I need any\nmore data at all? Couldn't I just gather some\ndata, and then just use that?",
    "start": "2761160",
    "end": "2768510"
  },
  {
    "text": "And maybe I don't need to\ngather any new online data. And we'll see more ideas\nabout that shortly.",
    "start": "2768510",
    "end": "2775770"
  },
  {
    "text": "But one case where you might\nthink that would be reasonable is, what if you have\ngreat demonstrations?",
    "start": "2775770",
    "end": "2781020"
  },
  {
    "text": "So you have instances of doctors\nmaking really good decisions in the intensive care unit, or\nyou have people flying planes,",
    "start": "2781020",
    "end": "2787920"
  },
  {
    "text": "or you have people driving cars. Why couldn't we just use those\nexamples to directly learn",
    "start": "2787920",
    "end": "2793530"
  },
  {
    "text": "decision policies? And so the hope would\nbe there is if we just have those recordings, any time\nsomeone's driving like a Tesla",
    "start": "2793530",
    "end": "2801270"
  },
  {
    "text": "or someone's\ndriving an airplane, could we just get those sort of\nstate action pairs and tuples",
    "start": "2801270",
    "end": "2809740"
  },
  {
    "text": "and use that information to\ntry to learn a policy directly? Now, one thing you\ncould do instead",
    "start": "2809740",
    "end": "2816220"
  },
  {
    "text": "is to say like, well, you'd\nhave a human in the loop, but that's going to\nbe pretty expensive. And so the hope would be\nthat instead we could just",
    "start": "2816220",
    "end": "2822910"
  },
  {
    "text": "use the demonstrations\npeople are already doing and that might be much\nmore reasonable, too, in terms of people's time.",
    "start": "2822910",
    "end": "2828450"
  },
  {
    "text": " So one thing in\nthis case would be,",
    "start": "2828450",
    "end": "2835345"
  },
  {
    "text": "all right, now,\nmaybe we're going to try to just look\ndirectly at demonstrations, and that means we're not\ngoing to need to have anybody",
    "start": "2835345",
    "end": "2840740"
  },
  {
    "text": "to label things. This is an example from trying\nto understand what the reward",
    "start": "2840740",
    "end": "2845960"
  },
  {
    "text": "function might be for driving. So I guess, I should say,\nin addition to the fact that we often have\ndata about people doing",
    "start": "2845960",
    "end": "2852640"
  },
  {
    "text": "these sorts of complex tasks\nthat we'd like to imitate, it also might be in\nthose tasks that it's really hard for someone to\nwrite down a reward function.",
    "start": "2852640",
    "end": "2859540"
  },
  {
    "text": "Like, maybe in this\nsorts of setting, you want to avoid the\nwater unless it's really, really steep or really\ngravelly, in which case",
    "start": "2859540",
    "end": "2866110"
  },
  {
    "text": "maybe your truck or a train\ncan go into the water. Or maybe like in general,\nyou want to avoid trees.",
    "start": "2866110",
    "end": "2871190"
  },
  {
    "text": "But again, if it's really slippy\nand muddy, it's actually better. And so it might just be that\nit's really hard for people",
    "start": "2871190",
    "end": "2876640"
  },
  {
    "text": "to write down a reward\nfunction in this case. But they could drive it and\nindicate that implicit reward",
    "start": "2876640",
    "end": "2881710"
  },
  {
    "text": "function. And so again, that might\nbe easier to gather.",
    "start": "2881710",
    "end": "2886900"
  },
  {
    "text": "This comes up in a lot\nof different cases. And people have thought\nabout it certainly a lot for manipulating heavy machinery\nor manipulating cars or things",
    "start": "2886900",
    "end": "2895220"
  },
  {
    "text": "like that. But for things like driving\nand parking and stuff,",
    "start": "2895220",
    "end": "2900900"
  },
  {
    "text": "those are a lot of cases\nwhere people provide those sorts of\ndemonstrations where it might be hard to specify\nthat reward function.",
    "start": "2900900",
    "end": "2906205"
  },
  {
    "text": " So the idea from learning\nfrom demonstrations",
    "start": "2906205",
    "end": "2912410"
  },
  {
    "text": "is that you're going to get a\nnumber of expert demonstrations. So experts will\ndemonstrate things,",
    "start": "2912410",
    "end": "2918872"
  },
  {
    "text": "whether they're\nflying a helicopter or manipulating something\nwith a robotic arm or stuff like through teleoperation.",
    "start": "2918872",
    "end": "2925290"
  },
  {
    "text": "Dorsa Sadigh's group\ndoes a lot of this. And it will give you a sequence\nof states and actions-- not rewards.",
    "start": "2925290",
    "end": "2931140"
  },
  {
    "text": "So you just are going to have\ntrajectories of state action",
    "start": "2931140",
    "end": "2937065"
  },
  {
    "text": "s-prime.  So we're not going to\nhave any rewards anymore.",
    "start": "2937065",
    "end": "2942480"
  },
  {
    "text": "Everything's just going to\nbe implicit in this case. And now, we're going to assume\nthat it's easier for people",
    "start": "2942480",
    "end": "2950370"
  },
  {
    "text": "to do this. So they're just\ngoing to, hopefully, be able to provide\nthese demonstrations or they maybe already have.",
    "start": "2950370",
    "end": "2957530"
  },
  {
    "text": "So what's the setup\nfor the rest of today? The setup is that we still have\na state space and an action",
    "start": "2957530",
    "end": "2962540"
  },
  {
    "text": "space. We're going to assume there's\nsome transition model. And there's a reward function,\nbut we don't know it.",
    "start": "2962540",
    "end": "2969840"
  },
  {
    "text": "So there might be a reward\nfunction, but we don't know. So there's nothing\nexplicit here. There's no explicit rewards.",
    "start": "2969840",
    "end": "2976460"
  },
  {
    "text": "And that we have these\nset of demonstrations. In behavior cloning,\nwhat we're going to do",
    "start": "2976460",
    "end": "2981700"
  },
  {
    "text": "is just reduce this\nto supervised learning and try to learn a mapping\nfrom states to actions.",
    "start": "2981700",
    "end": "2987030"
  },
  {
    "text": "I'm just going to try\nto clone the behavior. And then, we're going to also\nsee some about, can we actually",
    "start": "2987030",
    "end": "2993550"
  },
  {
    "text": "recover the reward function\nthat people might be using to generate their behavior? And then if we have\nthat, can we actually",
    "start": "2993550",
    "end": "3000059"
  },
  {
    "text": "try to get a new\ngood decision policy? But the first one is just to\ntry to directly learn a policy.",
    "start": "3000060",
    "end": "3007015"
  },
  {
    "text": "So this is called\nbehavior cloning. And essentially, once\nyou decide to do this,",
    "start": "3007016",
    "end": "3013320"
  },
  {
    "text": "this is just off-the-shelf\nsupervised learning. So now, you treat it as you have\na sequence of states and actions",
    "start": "3013320",
    "end": "3021630"
  },
  {
    "text": "from your expert trajectories. And you can use whatever\ntools and supervised",
    "start": "3021630",
    "end": "3027840"
  },
  {
    "text": "learning you want. So just anything\ncan be done there. It's just to reduce.",
    "start": "3027840",
    "end": "3033700"
  },
  {
    "text": "This is strictly now made into\na supervised learning problem. And there were some\nreally early successes.",
    "start": "3033700",
    "end": "3040250"
  },
  {
    "text": "So like ALVINN from a very long\ntime ago, and then in 1993, learning to fly in\na flight simulator,",
    "start": "3040250",
    "end": "3045280"
  },
  {
    "text": "really early on in the history\nof reinforcement learning or the modern history of\nreinforcement learning,",
    "start": "3045280",
    "end": "3050290"
  },
  {
    "text": "people thought like, could\nwe just reduce this problem? And we'll see in a second what's\none of the challenges that comes up when we do this.",
    "start": "3050290",
    "end": "3057010"
  },
  {
    "text": "But it certainly can\nbe really helpful. So it's kind of fun\nto look at ALVINN. This was-- yeah, late '80s.",
    "start": "3057010",
    "end": "3063030"
  },
  {
    "text": "But I think this must\nhave been kind of amazing. They were already\nthinking about cars then. They're already thinking about\nnot-so-deep neural networks,",
    "start": "3063030",
    "end": "3069724"
  },
  {
    "text": "but they were thinking\nabout neural networks. I think this came out of\nCMU, if I remember right. And they had this tiny\n30-by-32 video input.",
    "start": "3069725",
    "end": "3077750"
  },
  {
    "text": "And they used this rangefinder. And so they were trying to use\nnot-so-deep neural networks",
    "start": "3077750",
    "end": "3083770"
  },
  {
    "text": "to do behavior cloning for\ndriving in the late '80s, which is pretty awesome.",
    "start": "3083770",
    "end": "3089616"
  },
  {
    "text": "So it can be done pretty well. In reality, this is something\nthat still people try a lot.",
    "start": "3089616",
    "end": "3096480"
  },
  {
    "text": "It's a really good baseline\nto try if you have good data. And I'll talk about\nsome of the challenges",
    "start": "3096480",
    "end": "3101720"
  },
  {
    "text": "with doing behavior cloning. But I think one thing now is\nlike if you have a lot of data, like a lot, a lot, a\nlot of demonstrations--",
    "start": "3101720",
    "end": "3107760"
  },
  {
    "text": "like imagine you have all\nthe data from all the pilots, like you have their\nactual what they're doing, all of the different sort of\ninput actions they're doing,",
    "start": "3107760",
    "end": "3114720"
  },
  {
    "text": "and you have that for, I\ndon't know, all of United or something like that. So if you have an\nenormous amount of data",
    "start": "3114720",
    "end": "3120230"
  },
  {
    "text": "and you have a\npretty sophisticated supervised learning\ntechnique, it can work really\nwell, particularly",
    "start": "3120230",
    "end": "3127339"
  },
  {
    "text": "if you use behavioral cloning\nwith an RNN or something that takes a track of the history.",
    "start": "3127340",
    "end": "3132650"
  },
  {
    "text": "So while what I wrote here\ninvolved just states, and then like the last state,\nlike a Markov assumption,",
    "start": "3132650",
    "end": "3139380"
  },
  {
    "text": "like the state and the action,\nyou don't have to do that. You could also say I could\nhave my state, action, state,",
    "start": "3139380",
    "end": "3146029"
  },
  {
    "text": "and then go from there to a1,\nor state, action, state, action,",
    "start": "3146030",
    "end": "3152970"
  },
  {
    "text": "state.  So you, in general,\ncould use something",
    "start": "3152970",
    "end": "3159250"
  },
  {
    "text": "that is like a recurrent neural\nnetwork or anything that keeps track of long term histories.",
    "start": "3159250",
    "end": "3164270"
  },
  {
    "text": "It does not have to be\na Markov representation. And that often can\nwork very well.",
    "start": "3164270",
    "end": "3170140"
  },
  {
    "text": "Again, it depends a\nlot on your domain. I think that there's a nice\npaper a few years ago in CORL,",
    "start": "3170140",
    "end": "3175430"
  },
  {
    "text": "which is one of the robotics\nlearning conferences, where they looked at what were some of the\nimportant factors when you're",
    "start": "3175430",
    "end": "3180970"
  },
  {
    "text": "doing offline learning and\nfor robot applications.",
    "start": "3180970",
    "end": "3186369"
  },
  {
    "text": "So it doesn't always work well,\nbut it can work really well, particularly if you\nuse the history.",
    "start": "3186370",
    "end": "3192310"
  },
  {
    "text": "What domains might have\na history [INAUDIBLE]? Because imagine if you're flying\nor driving, really what matters",
    "start": "3192310",
    "end": "3200015"
  },
  {
    "text": "is just the current moment. So when is actually helpful? I actually would debate that. So I think even--\nit's a great question.",
    "start": "3200015",
    "end": "3205527"
  },
  {
    "text": "I think maybe it\npartly depends on how you're thinking\nof the state space. But I think if your state, say,\nfor-- let's say I'm driving.",
    "start": "3205527",
    "end": "3211990"
  },
  {
    "text": "If my state is just\nmy immediate position, that's probably not enough. I probably need at\nleast my last few",
    "start": "3211990",
    "end": "3217960"
  },
  {
    "text": "to get to velocity\nand acceleration. So you might already be\nthinking, oh, in my state, they already have those. If that's the case,\nif your state already",
    "start": "3217960",
    "end": "3224610"
  },
  {
    "text": "incorporates like something\nabout the first or second order derivatives, that's\nprobably OK in some cases.",
    "start": "3224610",
    "end": "3230350"
  },
  {
    "text": "But in other cases, if it's\njust your immediate sensors, then you want the longer\nhistory to capture that.",
    "start": "3230350",
    "end": "3237070"
  },
  {
    "text": "And same for planes and stuff. Yeah, it's a good question. So this is always just a\nreally good thing to try.",
    "start": "3237070",
    "end": "3244180"
  },
  {
    "text": "It's a really natural baseline. It's generally\nreally easy to do. People often report it\nin offline [INAUDIBLE].",
    "start": "3244180",
    "end": "3249886"
  },
  {
    "text": "It's extensively used. It does not always work. Let's see why it might not work.",
    "start": "3249886",
    "end": "3255170"
  },
  {
    "text": "And I think one of the\nthemes that you're seeing now with the policy\ngradient work right now is this challenge of what\nare the states you reach",
    "start": "3255170",
    "end": "3262530"
  },
  {
    "text": "and how, when you use\ndifferent policies, you're going to end up\nat different states.",
    "start": "3262530",
    "end": "3267930"
  },
  {
    "text": "In general, that's\nthe definition. If your policies don't ever\nreach any different states and they never take\ndifferent actions,",
    "start": "3267930",
    "end": "3273610"
  },
  {
    "text": "they're the same policy. They generate the\nsame trajectories. So DAGGER was a paper in 2008.",
    "start": "3273610",
    "end": "3280990"
  },
  {
    "text": "I'm trying to remember. It came out in the first decade\nof the 2000s to try to address",
    "start": "3280990",
    "end": "3288510"
  },
  {
    "text": "some of the challenges\nwith behavioral cloning. And I think what\nthey were noticing is",
    "start": "3288510",
    "end": "3294720"
  },
  {
    "text": "this challenge of if\nyou do behavior cloning, sometimes things go badly. And essentially, that's\nbecause the decisions",
    "start": "3294720",
    "end": "3302380"
  },
  {
    "text": "that you make over time\ncan have cascading effects. So let's see what\nthat might look like.",
    "start": "3302380",
    "end": "3309050"
  },
  {
    "text": "So if you do something like\nsupervised learning, you--",
    "start": "3309050",
    "end": "3316470"
  },
  {
    "text": "and this is what we do\nwhen we would be reducing our problem to imitation. In behavior cloning,\nwe just reduce it",
    "start": "3316470",
    "end": "3322030"
  },
  {
    "text": "to supervised learning. In general, we assume that our\npairs, our data points, our x,",
    "start": "3322030",
    "end": "3327120"
  },
  {
    "text": "y pairs in supervised\nlearning are IID. So they're are Independent\nand Identically Distributed.",
    "start": "3327120",
    "end": "3332339"
  },
  {
    "text": "And they're ignoring\ntemporal structure, because they just assume\nthey're totally independent. But in our case, they're\nabsolutely related.",
    "start": "3332340",
    "end": "3340240"
  },
  {
    "text": "In fact, if you assume a Markov\nstructure, then what happens is you have s0, a0, s1.",
    "start": "3340240",
    "end": "3346590"
  },
  {
    "text": "And so whatever you\ndid here exactly helps determine what is\nthe next state you do. So they're not-- the\nstates are definitely",
    "start": "3346590",
    "end": "3352700"
  },
  {
    "text": "not independent at all of\nyour different time points. So one of the\nchallenges with that",
    "start": "3352700",
    "end": "3358040"
  },
  {
    "text": "is that if you have\nindependent and time errors-- and generally,\nthat's not too bad.",
    "start": "3358040",
    "end": "3363410"
  },
  {
    "text": "And that's what most of our\nsupervised learning guarantees are for, is that you assume\nyour data is all IID, and then you can think\nabout how much sort",
    "start": "3363410",
    "end": "3371119"
  },
  {
    "text": "of error in your estimates,\nwhat sort of error you get. So in general, if you\nhave an error at time",
    "start": "3371120",
    "end": "3378180"
  },
  {
    "text": "t with probability less\nthan or equal to epsilon and you have T\ndecisions-- so let's",
    "start": "3378180",
    "end": "3383670"
  },
  {
    "text": "assume we have T decisions,\nthen your expected number of total errors, if all\nyour decisions are independent,",
    "start": "3383670",
    "end": "3390020"
  },
  {
    "text": "is just epsilon times T\nbecause they're all IID.",
    "start": "3390020",
    "end": "3395360"
  },
  {
    "text": "And that's not too terrible. But that's not what we\nnormally have in-- oh,",
    "start": "3395360",
    "end": "3401690"
  },
  {
    "text": "I see what happened there. OK.  OK, let's think\nabout something else.",
    "start": "3401690",
    "end": "3407350"
  },
  {
    "text": "I'll add a different\npicture later. Let's think of a racetrack.",
    "start": "3407350",
    "end": "3412810"
  },
  {
    "text": "All right, so in this\ncase, you have a racetrack. And your car is driving, except\nfor your supervised learning",
    "start": "3412810",
    "end": "3420049"
  },
  {
    "text": "thing isn't perfect and\nso it makes a small error. So what you actually do--\nyou should maybe-- maybe you actually should\nhave went this way,",
    "start": "3420050",
    "end": "3427500"
  },
  {
    "text": "but you went to the black part. And now, you, again, make\na little bit of an error.",
    "start": "3427500",
    "end": "3433650"
  },
  {
    "text": "And now, you're off the track. And now, this is really\ntricky, because you",
    "start": "3433650",
    "end": "3439050"
  },
  {
    "text": "may have almost no\ndata in the part of-- because your humans never\ndecided to drive off the track.",
    "start": "3439050",
    "end": "3444943"
  },
  {
    "text": "And so now, you're\nin part of the region where you have very little\ndata and very little coverage, and you're even more\nlikely to make mistakes.",
    "start": "3444943",
    "end": "3450870"
  },
  {
    "text": "And so what you can\nsee in this case is that if you make\nsmall mistakes early on, those can compound and get you\ninto parts of the state space",
    "start": "3450870",
    "end": "3457320"
  },
  {
    "text": "where you have\neven less coverage and you generally have\neven less accuracy. And so in general, you can\nactually do much worse.",
    "start": "3457320",
    "end": "3465220"
  },
  {
    "text": "And this is because you have\na data distribution mismatch. If the policy that\nyou compute gives you",
    "start": "3465220",
    "end": "3471340"
  },
  {
    "text": "a different distribution\nbetween train and test, then you don't necessarily\nhave the same guarantees.",
    "start": "3471340",
    "end": "3477220"
  },
  {
    "text": "And we're going to get a\ndifferent distribution here, because the policy we were\nusing to gather the data",
    "start": "3477220",
    "end": "3483340"
  },
  {
    "text": "is not exactly the same\nas the policy we get now. And we saw that in PPO, too--\nthat when the policy changed,",
    "start": "3483340",
    "end": "3488638"
  },
  {
    "text": "we're going to get to\ndifferent states and actions. What's causing our\npolicy to change here is the fact that we can't\nperfectly imitate the expert.",
    "start": "3488638",
    "end": "3496000"
  },
  {
    "text": "OK, so let's just see\nwhat that looks like. So in this case, we had--",
    "start": "3496000",
    "end": "3501680"
  },
  {
    "text": "in our training set,\nwe had pi star, which we assume to be our expert. And we were generating\nstates from pi star.",
    "start": "3501680",
    "end": "3508790"
  },
  {
    "text": "In our test set, we\nhave learned a policy by trying to match the\nstate action pairs we",
    "start": "3508790",
    "end": "3515119"
  },
  {
    "text": "saw in our training\nset and we're getting a different\ndistribution of states. ",
    "start": "3515120",
    "end": "3521254"
  },
  {
    "text": "In general, this is\ngoing to be different. And we're going to get\nworse errors in this case. ",
    "start": "3521254",
    "end": "3528030"
  },
  {
    "text": "Sorry about the-- I see\nwhat happened in this case. So I'll just draw it. So what can happen\nin this case is let's",
    "start": "3528030",
    "end": "3534360"
  },
  {
    "text": "say this is the\nerror you make now, and then you can\nmake another error.",
    "start": "3534360",
    "end": "3540070"
  },
  {
    "text": "And it keeps compounding. So if you make an error at\ntime step t with probability E,",
    "start": "3540070",
    "end": "3547200"
  },
  {
    "text": "essentially what\ncan happen there is that you may then make errors\non the remaining time steps.",
    "start": "3547200",
    "end": "3554790"
  },
  {
    "text": "So could cause you to get\ninto parts of the state action space for which you\nmake lots of errors,",
    "start": "3554790",
    "end": "3561900"
  },
  {
    "text": "and then you incur lots of\nregret or costs through the end. So in general, and I'm not going\nto step through all of the proof",
    "start": "3561900",
    "end": "3568950"
  },
  {
    "text": "today, the error can actually\ncompound instead of linearly with the number of time steps. It can compound\nquadratically, which",
    "start": "3568950",
    "end": "3575810"
  },
  {
    "text": "means that essentially\nyour performance is much worse than supervised\nlearning would predict. Supervised learning\nsaid, oh, I've",
    "start": "3575810",
    "end": "3581520"
  },
  {
    "text": "got an epsilon optimal or\nepsilon accurate policy. Great.",
    "start": "3581520",
    "end": "3586950"
  },
  {
    "text": "And what this says is because\nall of those decisions are being made across\nan entire trajectory,",
    "start": "3586950",
    "end": "3592070"
  },
  {
    "text": "you can actually end\nup with epsilon times T squared errors instead\nof epsilon times T.",
    "start": "3592070",
    "end": "3597290"
  },
  {
    "text": "So this is what\nmotivated DAGGER. DAGGER said, OK,\nwhat's the problem?",
    "start": "3597290",
    "end": "3602760"
  },
  {
    "text": "The problem that's happening\nhere that we'd like to address is whenever we make mistakes,\nwe go into a different part",
    "start": "3602760",
    "end": "3608150"
  },
  {
    "text": "of the state space. Once we're there, we maybe\nhave very little guarantees that we're going to do\nanything reasonable.",
    "start": "3608150",
    "end": "3613380"
  },
  {
    "text": "So essentially, what\nwe want to try to do is figure out how we\nmight correct or adjust",
    "start": "3613380",
    "end": "3618740"
  },
  {
    "text": "to those states that\nwe reach that weren't in our original training set. So the idea in this\ncase is that your-- this",
    "start": "3618740",
    "end": "3625970"
  },
  {
    "text": "is going to be an\niterative approach. So you get a data set where\nyou take a current policy",
    "start": "3625970",
    "end": "3632359"
  },
  {
    "text": "and you execute it\nin the environment. So it's like you drive your\nrace car around a track.",
    "start": "3632360",
    "end": "3637940"
  },
  {
    "text": "And hopefully, it's similar\nto what the expert would have done, but probably not perfect. And then what you do is\nyou go to your expert,",
    "start": "3637940",
    "end": "3644630"
  },
  {
    "text": "and you say, OK,\nthis is what I did when I went around that track. What should I have done there? They're like a coach. And so then what the coach\ndoes or the expert does,",
    "start": "3644630",
    "end": "3651500"
  },
  {
    "text": "they say, ah, in\neach of those states, this is what you\nshould have done. So it would say, hey,\nif you went like this,",
    "start": "3651500",
    "end": "3660059"
  },
  {
    "text": "and then you did this and did\nall these other crazy things after that, it would have said,\nOK, no, first of all, here,",
    "start": "3660060",
    "end": "3667270"
  },
  {
    "text": "you should have gone here. And then once you\nreached here, you should have went down to try\nto get back onto the road.",
    "start": "3667270",
    "end": "3676140"
  },
  {
    "text": "So essentially, what\nyou're having a human do is you're having them label\nat every time point, at every state in\nthat trajectory what",
    "start": "3676140",
    "end": "3681930"
  },
  {
    "text": "they would have done. And when you do that, that\ngives you a new set of data",
    "start": "3681930",
    "end": "3687270"
  },
  {
    "text": "to learn from. So it's like your\nexpert pilot gives you feedback on every place\nyou made a mistake",
    "start": "3687270",
    "end": "3692640"
  },
  {
    "text": "when you just did\nyour last flight run, and then you integrate that. You're like, oh, OK, when I'm\nfeeling this form of lift,",
    "start": "3692640",
    "end": "3699460"
  },
  {
    "text": "next time, I got to do this. So it gives you a\nwhole bunch more data, and then we aggregate that data,\nthat's why it's called DAGGER.",
    "start": "3699460",
    "end": "3705970"
  },
  {
    "text": "So we're aggregating the data\nsets of the old data we had and the new data that we\njust got from our expert.",
    "start": "3705970",
    "end": "3713070"
  },
  {
    "text": "We then do behavior\ncloning again on our new data set,\nwhich now includes more of the states\nin the environment.",
    "start": "3713070",
    "end": "3721340"
  },
  {
    "text": "And then we repeat. And I think part of the\nmotivation for this. And this is why I said behavior\ncloning can work really",
    "start": "3721340",
    "end": "3727420"
  },
  {
    "text": "well when you have enough data,\nis that the problem that's happening here is that we're\nassuming we don't kind of have",
    "start": "3727420",
    "end": "3733540"
  },
  {
    "text": "full coverage over\nthe whole domain of what the expert would do\nat any place inside of the, say, race car track.",
    "start": "3733540",
    "end": "3739539"
  },
  {
    "text": "And what this is\nallowing us to do is to better figure out\nover the whole space what",
    "start": "3739540",
    "end": "3745300"
  },
  {
    "text": "the expert would do, and\nmake better decisions, and correct, in case,\nwe end up in those.",
    "start": "3745300",
    "end": "3751070"
  },
  {
    "text": "So in DAGGER, we do this\nover and over and over again. And there's some nice\ntheoretical guarantees of what you'll converge\nto when you do this.",
    "start": "3751070",
    "end": "3757859"
  },
  {
    "text": "And what they did is\nto show this for things like driving, driving\nin a simulated domain, like a Mario Kart\nor a video game,",
    "start": "3757860",
    "end": "3764450"
  },
  {
    "text": "and show that they could\nlearn quickly how to get a very good policy\nthat didn't suffer from these kind of\ncompounding errors.",
    "start": "3764450",
    "end": "3772400"
  },
  {
    "text": "Can anybody think\nof what a limitation might be of doing this\nover behavior cloning?",
    "start": "3772400",
    "end": "3777570"
  },
  {
    "text": "Yeah. [INAUDIBLE] Exactly. It's super expensive. Yeah. So you have to--",
    "start": "3777570",
    "end": "3784210"
  },
  {
    "text": "basically, it's like\nyou have to have that coach, or your teacher,\nor your expert with you the whole learning.",
    "start": "3784210",
    "end": "3789277"
  },
  {
    "text": "So the nice thing about behavior\ncloning is you get data once, the data might\nalready be available, and then you can\njust learn from it.",
    "start": "3789277",
    "end": "3795470"
  },
  {
    "text": "Here you have to have\nconstant supervision. Now, in some cases, that\nmight be reasonable.",
    "start": "3795470",
    "end": "3800690"
  },
  {
    "text": "But in most settings, that's\ngoing to be really useful. So this is very\nhuman in the loop.",
    "start": "3800690",
    "end": "3807350"
  },
  {
    "text": "Human has to supervise. ",
    "start": "3807350",
    "end": "3813250"
  },
  {
    "text": "And so I think\nfor those reasons, that's one of the\nreasons that in robotics and some other areas,\npeople certainly",
    "start": "3813250",
    "end": "3819700"
  },
  {
    "text": "have built a lot on DAGGER. But I don't think it's as\npopular as behavior cloning, because it really does require\na lot more work of the human.",
    "start": "3819700",
    "end": "3828307"
  },
  {
    "text": "All right, so a second\nthing you might want to do is learn a reward.",
    "start": "3828308",
    "end": "3833770"
  },
  {
    "text": "So you might say,\nall right, there's-- I'd to actually figure\nout what the reward is. You might want this\nfor several reasons.",
    "start": "3833770",
    "end": "3839250"
  },
  {
    "text": "You might want to\nlearn the reward, because you want to\nunderstand something about human decision-making.",
    "start": "3839250",
    "end": "3844275"
  },
  {
    "text": "Like, you might\nsay, all right, I want to understand how\nsurgeons are making trade-offs when they're dealing with really\ncomplicated situations of like,",
    "start": "3844275",
    "end": "3851340"
  },
  {
    "text": "how do I trade off time or\nrisk or things like that. And maybe it's really\nhard, or just their time is really valuable to ask\nthem lots of questions.",
    "start": "3851340",
    "end": "3859020"
  },
  {
    "text": "But you really would like to\nunderstand that preference structure. So that's one goal. And another is that\nyou might want to use",
    "start": "3859020",
    "end": "3865610"
  },
  {
    "text": "that then to learn a policy. You might say like, if I can\nextract that from the data, then I can learn a\npolicy from that.",
    "start": "3865610",
    "end": "3871160"
  },
  {
    "text": "And you'll see\nthat in Homework 3, because we're going to be\ndoing RLHF as part of that. We're going to try to\nlearn from preferences.",
    "start": "3871160",
    "end": "3877670"
  },
  {
    "text": "So there's lots of\nreasons you might want to be able to\nlearn a reward function. So in this case, we're going\nto be in a similar setting.",
    "start": "3877670",
    "end": "3883500"
  },
  {
    "text": "We're going to still have a\nstate space, and action space, and a transition model, but\nstill no reward function. It's still going to have\nsome expert demonstrations.",
    "start": "3883500",
    "end": "3890467"
  },
  {
    "text": "And what we want to do is\ninfer the reward function the expert was using implicitly\nto make their decisions.",
    "start": "3890467",
    "end": "3898050"
  },
  {
    "text": " And what we're going\nto assume for now",
    "start": "3898050",
    "end": "3903490"
  },
  {
    "text": "is that the teacher's\npolicy is optimal. So you can call it expert--",
    "start": "3903490",
    "end": "3908980"
  },
  {
    "text": "the teacher, the expert's\npolicy is optimal. So let's think about what\nwe can infer from that.",
    "start": "3908980",
    "end": "3916960"
  },
  {
    "text": "So if you see someone's\ndemonstrations, and you know that\ntheir optimal--",
    "start": "3916960",
    "end": "3922620"
  },
  {
    "text": "so teacher, I'll use teacher,\nequals to expert for this thing.",
    "start": "3922620",
    "end": "3928560"
  },
  {
    "text": "If you see these, and\nyou know it's optimal, is there a single unique R that\nmakes teacher's policy optimal?",
    "start": "3928560",
    "end": "3934860"
  },
  {
    "text": "Are there many? Does it depend on the\nMarkov decision process? Are you not sure? ",
    "start": "3934860",
    "end": "3941910"
  },
  {
    "text": "Now, remember, we know that\nthe actual policy is optimal. ",
    "start": "3941910",
    "end": "3982990"
  },
  {
    "text": "If you think there are many, I'd\nlike you to give me an example. A simple one, which would\nmake things optimal.",
    "start": "3982990",
    "end": "3989525"
  },
  {
    "text": "I mean, not in the thing,\nbut I'll ask in a second. ",
    "start": "3989525",
    "end": "4003950"
  },
  {
    "text": "All right, why don't we\njust do a quick check. And talk to a neighbor,\nand see what you find. ",
    "start": "4003950",
    "end": "4012072"
  },
  {
    "text": "[SIDE CONVERSATIONS] ",
    "start": "4012072",
    "end": "4138240"
  },
  {
    "text": "OK, good. So almost everybody said the\nanswer is B, which is true. There is many. Does anybody want to tell\nme kind of a silly one",
    "start": "4138240",
    "end": "4146120"
  },
  {
    "text": "that any policy\nis optimal under? Yeah. ",
    "start": "4146120",
    "end": "4154189"
  },
  {
    "text": "To scale by a constant factor. Yeah, that's great. And I was hearing\nthat, too, over there. So if you scale-- if you take a reward\nfunction and you multiply it",
    "start": "4154189",
    "end": "4161070"
  },
  {
    "text": "by a positive constant, then\nthat can't change the policy. 0 works, too.",
    "start": "4161071",
    "end": "4166370"
  },
  {
    "text": "So you can just use 0,\nand any policy is optimal if you never get reward.",
    "start": "4166370",
    "end": "4172580"
  },
  {
    "text": "So I bring this up\nnot to trivialize it, but just to highlight that\nthis is a huge identifiability",
    "start": "4172580",
    "end": "4179660"
  },
  {
    "text": "problem. There is not a\nsingle R. Even if you know that the\ndemonstrations are expert,",
    "start": "4179660",
    "end": "4184759"
  },
  {
    "text": "there's not a single\nreward function that's compatible with them. So that's a problem. And that's something\nto keep in mind when",
    "start": "4184760",
    "end": "4190408"
  },
  {
    "text": "we start getting\ninto RLHF and DPO shortly, that this\nis either you need",
    "start": "4190408",
    "end": "4197245"
  },
  {
    "text": "to be making other\nsorts of assumptions to constrain your reward\nfunction or, in general, we're going to have to\nmake additional choices",
    "start": "4197245",
    "end": "4203160"
  },
  {
    "text": "or constraints,\nbecause otherwise this is not identifiable problem. OK, great.",
    "start": "4203160",
    "end": "4208640"
  },
  {
    "text": " So one thing some\npeople do to try",
    "start": "4208640",
    "end": "4213980"
  },
  {
    "text": "to think about how we might\ndo this is to think about-- [INAUDIBLE]",
    "start": "4213980",
    "end": "4219860"
  },
  {
    "text": "What happened? I was editing two\nsets of slides. And I think the other\none is now well updated.",
    "start": "4219860",
    "end": "4225070"
  },
  {
    "text": "But this one was\nnot, unfortunately. In any case, we talked\nbriefly about value function",
    "start": "4225070",
    "end": "4230880"
  },
  {
    "text": "approximation through\ndeep Q-learning. Deep Q-learning\nnaturally implies that we would use a\ndeep neural network,",
    "start": "4230880",
    "end": "4236410"
  },
  {
    "text": "but you could use a\nlinear value function just like a very shallow network. The idea here--\nand this is all--",
    "start": "4236410",
    "end": "4243270"
  },
  {
    "text": "this work predated\ndeep Q-learning-- is to think about, generally,\nwhere your reward is linear over the features.",
    "start": "4243270",
    "end": "4248650"
  },
  {
    "text": "So your reward of s. So here, we're just doing\nreward respect to states is w.",
    "start": "4248650",
    "end": "4254535"
  },
  {
    "text": "W is just going to\nbe a feature vector. W is just going to be a vector. X of s. And x of s here is just\na feature representation.",
    "start": "4254535",
    "end": "4262120"
  },
  {
    "text": "So this is just features are xs.",
    "start": "4262120",
    "end": "4268040"
  },
  {
    "text": "So that, for example,\ncould be like if I'm a robot, if this is\nmy current location, what's the distance to that wall?",
    "start": "4268040",
    "end": "4273720"
  },
  {
    "text": "What's the distance to that\nwall, that wall, and this wall? That would be a set of features. And then, I could have a\nweighted combination of those",
    "start": "4273720",
    "end": "4279600"
  },
  {
    "text": "to give me the reward\nof me standing here. And the goal is to identify\nthe weight vector w given",
    "start": "4279600",
    "end": "4285739"
  },
  {
    "text": "a set of demonstrations. So in that case, you can also\nexpress the resulting value",
    "start": "4285740",
    "end": "4292520"
  },
  {
    "text": "function for a policy\nas a combination of these weighted features. ",
    "start": "4292520",
    "end": "4299110"
  },
  {
    "text": "And let me just write it out. So let me just write\nit out, particularly, because we didn't do\nit in class very much.",
    "start": "4299110",
    "end": "4305277"
  },
  {
    "text": "I'm going to write out\nwhat that looks like. So it's the states we\nreach under this policy.",
    "start": "4305277",
    "end": "4310560"
  },
  {
    "text": "This time step t equals 0 to\ninfinity gamma T of our weight",
    "start": "4310560",
    "end": "4316430"
  },
  {
    "text": "vector, it's unknown, times\nour feature representation for that time step\ngiven we start at s0.",
    "start": "4316430",
    "end": "4322730"
  },
  {
    "text": "But note here, that\nw is always the same, so we can just take this out.",
    "start": "4322730",
    "end": "4329190"
  },
  {
    "text": "So we have wT, the\nexpected value of s pi. And this should start to look\nsomewhat familiar, because it's",
    "start": "4329190",
    "end": "4336690"
  },
  {
    "text": "going to look like these\nweird discounted features that we've seen sort of before.",
    "start": "4336690",
    "end": "4343320"
  },
  {
    "text": "So we can also call\nthis wT mu of pi",
    "start": "4343320",
    "end": "4348380"
  },
  {
    "text": "where this is the state\ndistribution under discounted",
    "start": "4348380",
    "end": "4357409"
  },
  {
    "text": "state distribution under pi. OK, we've seen this before where\nwe go sort of back and forth",
    "start": "4357410",
    "end": "4363969"
  },
  {
    "text": "between thinking of\nthere being time steps and thinking of us as\nsort of saying, well, over all time, how much\ntime do we spend in each",
    "start": "4363970",
    "end": "4369440"
  },
  {
    "text": "of the different states? So in particular\nhere, I've defined mu",
    "start": "4369440",
    "end": "4376159"
  },
  {
    "text": "to just be the discounted\nweighted frequency of state features starting in\na particular state.",
    "start": "4376160",
    "end": "4382320"
  },
  {
    "text": "So why have I done this? Well, I've done this to say we\ncan relate what the value is to just a linear combination\nunder this linear reward",
    "start": "4382320",
    "end": "4389940"
  },
  {
    "text": "function, a linear combination\nof my weight feature, which I don't know, times my\nfeature distribution.",
    "start": "4389940",
    "end": "4396870"
  },
  {
    "text": "And that's good, because\nI have access to features. I have access to\ntrajectories that were",
    "start": "4396870",
    "end": "4403170"
  },
  {
    "text": "demonstrated by my experts. And I can use that to extract\nthe features of those states and compute something like mu.",
    "start": "4403170",
    "end": "4408815"
  },
  {
    "text": " But we don't know what\nw is yet, so let's think",
    "start": "4408815",
    "end": "4415450"
  },
  {
    "text": "of what we could do. So the goal here is that we want\nto identify the weight vector w",
    "start": "4415450",
    "end": "4422750"
  },
  {
    "text": "given a set of demonstrations. We've just seen that we can\nrewrite the value of a policy pi",
    "start": "4422750",
    "end": "4428610"
  },
  {
    "text": "if these rewards are linear\nas wT mu of pi, where it's",
    "start": "4428610",
    "end": "4434719"
  },
  {
    "text": "the discounted state frequency. All right, so what we\nknow is that V star",
    "start": "4434720",
    "end": "4442400"
  },
  {
    "text": "for the optimal policy is\ngreater than or equal to V pi for any other policy.",
    "start": "4442400",
    "end": "4448880"
  },
  {
    "text": "And that means that\nw pi of mu pi star",
    "start": "4448880",
    "end": "4457040"
  },
  {
    "text": "has to be greater than or equal\nto w pi of mu pi for all pi",
    "start": "4457040",
    "end": "4466780"
  },
  {
    "text": "where this is observed. So [INAUDIBLE] experts.",
    "start": "4466780",
    "end": "4475520"
  },
  {
    "text": "So what it means is that\nif I pick any other policy and I generate what\nare the state features",
    "start": "4475520",
    "end": "4482600"
  },
  {
    "text": "you'd get under running\nthat policy in the world, that distribution\nof features has",
    "start": "4482600",
    "end": "4488000"
  },
  {
    "text": "to have lower reward than\nthe features I've actually observed in my\ndata, because I've",
    "start": "4488000",
    "end": "4493580"
  },
  {
    "text": "assumed my expert is optimal. So my experts\ndemonstrated things. It's optimal. And when they\ndemonstrated things,",
    "start": "4493580",
    "end": "4499850"
  },
  {
    "text": "like let's say they're\ncontrolling a robot and the robot spends\nall this time over in this part of the room.",
    "start": "4499850",
    "end": "4505769"
  },
  {
    "text": "And if they spend time\nover this part of the room, then all my features are\ngoing to come from over here. And that means that any\nother policy that I use,",
    "start": "4505770",
    "end": "4516010"
  },
  {
    "text": "its features have\nto have a lower weight if they don't match what\nthe features are of the expert.",
    "start": "4516010",
    "end": "4521205"
  },
  {
    "text": " Regardless of what w is, right?",
    "start": "4521205",
    "end": "4527300"
  },
  {
    "text": "Because this has to hold. So this-- for the w that we\npick, this has to be true.",
    "start": "4527300",
    "end": "4534620"
  },
  {
    "text": "So you can rewrite that as\nsaying that the value of V star",
    "start": "4534620",
    "end": "4541060"
  },
  {
    "text": "has to be greater than or\nequal to V, which means that the value under-- ",
    "start": "4541060",
    "end": "4547980"
  },
  {
    "text": "yeah, we can just write\nit down in terms of this and the resulting frequency. So therefore, the\nexpert's demonstrations",
    "start": "4547980",
    "end": "4554579"
  },
  {
    "text": "are from the optimal policy to\nidentify sufficient to find a w star such that this holds.",
    "start": "4554580",
    "end": "4559800"
  },
  {
    "text": "So we know this has to be true\nunder the true expert that-- under the true w, it\nhas to be that features",
    "start": "4559800",
    "end": "4567090"
  },
  {
    "text": "we get under the\nexpert policy have to have a higher reward\nthan the features we",
    "start": "4567090",
    "end": "4572280"
  },
  {
    "text": "get under any other policy. So this gives us a constraint.",
    "start": "4572280",
    "end": "4577710"
  },
  {
    "text": "It says when we are searching\nfor what w is, because remember, w determines our reward\nfunction, this has to hold.",
    "start": "4577710",
    "end": "4585790"
  },
  {
    "text": "[INAUDIBLE]  And then what we can do is--",
    "start": "4585790",
    "end": "4593880"
  },
  {
    "text": "so it's sufficient\nto say, well, what would be one thing we could\ndo to be optimal if we wanted to get to a policy?",
    "start": "4593880",
    "end": "4599800"
  },
  {
    "text": "Well, we just need to match\nthe features of the expert. We need a policy that induces\na same distribution of states",
    "start": "4599800",
    "end": "4607260"
  },
  {
    "text": "as the expert. So in general, if you have a\npolicy such that the features",
    "start": "4607260",
    "end": "4613860"
  },
  {
    "text": "you generate under that policy\nare really close to the features you get under pi star, then\nfor all w with w infinity",
    "start": "4613860",
    "end": "4622909"
  },
  {
    "text": "less equal to 1-- this is\nusing Holder's inequality. You're guaranteed that\nthe reward of this policy",
    "start": "4622910",
    "end": "4628520"
  },
  {
    "text": "is very close to the reward\nof the optimal policy or your expert policy.",
    "start": "4628520",
    "end": "4635120"
  },
  {
    "text": "And all of this\nis just to say you can reduce the problem of\nreward learning and policy",
    "start": "4635120",
    "end": "4641750"
  },
  {
    "text": "learning, in this case,\nto feature matching. That's kind of the high\nlevel idea, is to say, in the case where you don't\nobserve the reward directly,",
    "start": "4641750",
    "end": "4649080"
  },
  {
    "text": "but you have access to optimal\ndemonstrations by an expert, all you need to do is to\nfind a policy and a reward",
    "start": "4649080",
    "end": "4656330"
  },
  {
    "text": "function that allows you\nto match those features. Because those are the features\nthat we know have high reward.",
    "start": "4656330",
    "end": "4663285"
  },
  {
    "text": " Now, as we've\nalready talked about,",
    "start": "4663285",
    "end": "4669489"
  },
  {
    "text": "there is still an infinite\nnumber of reward functions with the same optimal policy. So even when we think about\nthis mapping to features,",
    "start": "4669490",
    "end": "4675400"
  },
  {
    "text": "it doesn't solve the\nissue we just identified. And there are many\nstochastic policies that",
    "start": "4675400",
    "end": "4681390"
  },
  {
    "text": "can match the feature counts. So I haven't told you anything\nyet to solve that big problem.",
    "start": "4681390",
    "end": "4686659"
  },
  {
    "text": "I've just told you sort of\nanother way to think about it. And so there's this\nquestion of how do we pick among all\nthese different options.",
    "start": "4686660",
    "end": "4693960"
  },
  {
    "text": "So there's a number of\ndifferent ways to do this. Some of the largest and most\ninfluential ideas are these two.",
    "start": "4693960",
    "end": "4700539"
  },
  {
    "text": "Maximum entropy inverse\nreinforcement learning and GAIL.",
    "start": "4700540",
    "end": "4705930"
  },
  {
    "text": "And what we'll do\nnext time is to talk about maximum entropy\ninverse reinforcement learning, which has been\nvery, very influential.",
    "start": "4705930",
    "end": "4712860"
  },
  {
    "text": "So this is in 2008. So we'll pick up on that\non Thursday-- on Wednesday. Thanks.",
    "start": "4712860",
    "end": "4718440"
  },
  {
    "start": "4718440",
    "end": "4723000"
  }
]