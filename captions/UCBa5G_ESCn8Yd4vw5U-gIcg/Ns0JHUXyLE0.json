[
  {
    "start": "0",
    "end": "15000"
  },
  {
    "start": "0",
    "end": "4790"
  },
  {
    "text": "CHRISTOPHER POTTS:\nWelcome back, everyone.",
    "start": "4790",
    "end": "6540"
  },
  {
    "text": "This is part 6 in our\nseries on Contextual Word",
    "start": "6540",
    "end": "8540"
  },
  {
    "text": "Representations.",
    "start": "8540",
    "end": "9380"
  },
  {
    "text": "We're going to be talking\nabout practical fine tuning.",
    "start": "9380",
    "end": "11630"
  },
  {
    "text": "It's time to get hands-on\nwith these parameters we've",
    "start": "11630",
    "end": "14270"
  },
  {
    "text": "been talking about.",
    "start": "14270",
    "end": "15570"
  },
  {
    "start": "15000",
    "end": "68000"
  },
  {
    "text": "So here's the guiding idea.",
    "start": "15570",
    "end": "17360"
  },
  {
    "text": "Your existing architecture, say,\nfor the current original system",
    "start": "17360",
    "end": "20120"
  },
  {
    "text": "and bake off\nprobably can benefit",
    "start": "20120",
    "end": "22250"
  },
  {
    "text": "from contextual representation.",
    "start": "22250",
    "end": "24110"
  },
  {
    "text": "We've seen that in\nmany, many contexts",
    "start": "24110",
    "end": "26130"
  },
  {
    "text": "and I know you\nknow these things.",
    "start": "26130",
    "end": "27980"
  },
  {
    "text": "The notebook fine\ntuning shows you",
    "start": "27980",
    "end": "29810"
  },
  {
    "text": "how to bring in transformer\nrepresentations in two ways.",
    "start": "29810",
    "end": "32520"
  },
  {
    "text": "First, with simple\nfeaturization, and then",
    "start": "32520",
    "end": "35150"
  },
  {
    "text": "with full-on fine tuning.",
    "start": "35150",
    "end": "36320"
  },
  {
    "text": "And I'm going to talk about both\nof those in this screencast.",
    "start": "36320",
    "end": "39739"
  },
  {
    "text": "The heart of this idea is\nthat by extending existing",
    "start": "39740",
    "end": "42530"
  },
  {
    "text": "PyTorch modules from the\ncourse code distribution,",
    "start": "42530",
    "end": "46129"
  },
  {
    "text": "you can very easily create\ncustomized fine tuning models",
    "start": "46130",
    "end": "48980"
  },
  {
    "text": "with just a few lines of code.",
    "start": "48980",
    "end": "50645"
  },
  {
    "text": "And that should be really\nempowering in terms",
    "start": "50645",
    "end": "52520"
  },
  {
    "text": "of exploring lots\nof different designs",
    "start": "52520",
    "end": "54500"
  },
  {
    "text": "and seeing how best to use these\nparameters for your problem.",
    "start": "54500",
    "end": "57583"
  },
  {
    "text": "I just want to\nmention that really",
    "start": "57583",
    "end": "59000"
  },
  {
    "text": "and truly, this is only possible\nbecause of the amazing work",
    "start": "59000",
    "end": "62492"
  },
  {
    "text": "that the Hugging\nFace team has done",
    "start": "62492",
    "end": "63950"
  },
  {
    "text": "to make these parameters\naccessible to all of us.",
    "start": "63950",
    "end": "67900"
  },
  {
    "text": "So let's start with\nsimple featurization.",
    "start": "67900",
    "end": "69700"
  },
  {
    "start": "68000",
    "end": "121000"
  },
  {
    "text": "And I actually want to\nrewind to our discussion",
    "start": "69700",
    "end": "71860"
  },
  {
    "text": "of recurrent neural networks\nand think about how we represent",
    "start": "71860",
    "end": "75010"
  },
  {
    "text": "examples for those models.",
    "start": "75010",
    "end": "76870"
  },
  {
    "text": "In the standard mode, we have\nas our examples lists of tokens",
    "start": "76870",
    "end": "80290"
  },
  {
    "text": "here.",
    "start": "80290",
    "end": "81400"
  },
  {
    "text": "We convert those into\nlists of indices,",
    "start": "81400",
    "end": "83590"
  },
  {
    "text": "and those indices\nhelp us look up",
    "start": "83590",
    "end": "85570"
  },
  {
    "text": "vector representations of those\nwords in some fixed embedding",
    "start": "85570",
    "end": "88930"
  },
  {
    "text": "space.",
    "start": "88930",
    "end": "89560"
  },
  {
    "text": "And the result of that is that\neach example is represented",
    "start": "89560",
    "end": "92799"
  },
  {
    "text": "by a list of vectors.",
    "start": "92800",
    "end": "94320"
  },
  {
    "text": "That's important\nto keep in mind.",
    "start": "94320",
    "end": "96100"
  },
  {
    "text": "We tend to think of\nthe model as taking",
    "start": "96100",
    "end": "98140"
  },
  {
    "text": "as its inputs lists of tokens\nand having an embedding.",
    "start": "98140",
    "end": "101882"
  },
  {
    "text": "But from the point of\nview of the model itself,",
    "start": "101882",
    "end": "103840"
  },
  {
    "text": "it really wants to process\nas inputs lists of vectors.",
    "start": "103840",
    "end": "107350"
  },
  {
    "text": "And that's the empowering\nidea because if we use it",
    "start": "107350",
    "end": "110080"
  },
  {
    "text": "in fixed embedding, of course,\nthen these two occurrences of A",
    "start": "110080",
    "end": "113050"
  },
  {
    "text": "will be the same\nvector, and these two",
    "start": "113050",
    "end": "114970"
  },
  {
    "text": "occurrences of B across examples\nwill be the same vector.",
    "start": "114970",
    "end": "118515"
  },
  {
    "text": "But the model\ndoesn't really care",
    "start": "118515",
    "end": "119890"
  },
  {
    "text": "that there's a same vector.",
    "start": "119890",
    "end": "121210"
  },
  {
    "start": "121000",
    "end": "216000"
  },
  {
    "text": "We could, if we wanted\nto, convert directly",
    "start": "121210",
    "end": "123490"
  },
  {
    "text": "from token sequences into\nlists of vectors using",
    "start": "123490",
    "end": "127119"
  },
  {
    "text": "a device like a BERT model.",
    "start": "127120",
    "end": "128869"
  },
  {
    "text": "And that would allow that\nA in the first position",
    "start": "128870",
    "end": "131290"
  },
  {
    "text": "and A in the third\ncould correspond",
    "start": "131290",
    "end": "133030"
  },
  {
    "text": "to different vectors, or B\nacross these two examples",
    "start": "133030",
    "end": "136120"
  },
  {
    "text": "might correspond to\ndifferent vectors.",
    "start": "136120",
    "end": "137739"
  },
  {
    "text": "That would be the\ncontextual representation",
    "start": "137740",
    "end": "139960"
  },
  {
    "text": "part of these models.",
    "start": "139960",
    "end": "141685"
  },
  {
    "text": "And again, from the\npoint of view of the RNN,",
    "start": "141685",
    "end": "143560"
  },
  {
    "text": "we can feed these indirectly.",
    "start": "143560",
    "end": "145090"
  },
  {
    "text": "That's straight forward.",
    "start": "145090",
    "end": "146319"
  },
  {
    "text": "This is a complete\nrecipe for doing",
    "start": "146320",
    "end": "148120"
  },
  {
    "text": "that using the SST\ncode and the PyTorch",
    "start": "148120",
    "end": "151450"
  },
  {
    "text": "modules from the course\ncode distribution.",
    "start": "151450",
    "end": "153500"
  },
  {
    "text": "So you can see that beyond\nthe setup stuff which we've",
    "start": "153500",
    "end": "156430"
  },
  {
    "text": "done a few times, the\nfeature function is just",
    "start": "156430",
    "end": "159579"
  },
  {
    "text": "going to use BERT functionality\nto look up the example's",
    "start": "159580",
    "end": "162820"
  },
  {
    "text": "indices and then convert them\ninto vector representations.",
    "start": "162820",
    "end": "165700"
  },
  {
    "text": "And here, as a\nsummary, we're going",
    "start": "165700",
    "end": "167440"
  },
  {
    "text": "to use the representation\nabove the class token.",
    "start": "167440",
    "end": "170740"
  },
  {
    "text": "But lots of things are\npossible at that point.",
    "start": "170740",
    "end": "173440"
  },
  {
    "text": "And then when we have\nour model wrapper here,",
    "start": "173440",
    "end": "175330"
  },
  {
    "text": "we set up a Torch\nRNN Classifier.",
    "start": "175330",
    "end": "177058"
  },
  {
    "text": "And there're just\ntwo things of note.",
    "start": "177058",
    "end": "178600"
  },
  {
    "text": "First, we say use\nembedding equals false,",
    "start": "178600",
    "end": "181180"
  },
  {
    "text": "because we're going to\nfeed vectors in directly.",
    "start": "181180",
    "end": "183219"
  },
  {
    "text": "There's no embedding\ninvolved here.",
    "start": "183220",
    "end": "185290"
  },
  {
    "text": "And we also don't need\nto have a vocabulary.",
    "start": "185290",
    "end": "187209"
  },
  {
    "text": "You could specify\none, but it's not",
    "start": "187210",
    "end": "188770"
  },
  {
    "text": "involved because\nfundamentally, again,",
    "start": "188770",
    "end": "190960"
  },
  {
    "text": "the model deals\ndirectly with vectors.",
    "start": "190960",
    "end": "194200"
  },
  {
    "text": "And then at SST\nexperiment, you, again,",
    "start": "194200",
    "end": "196090"
  },
  {
    "text": "say vectorized equals false.",
    "start": "196090",
    "end": "197769"
  },
  {
    "text": "And that is a complete\nrecipe for bringing",
    "start": "197770",
    "end": "200560"
  },
  {
    "text": "in BERT representations\nwith the standard RNN.",
    "start": "200560",
    "end": "204239"
  },
  {
    "text": "This isn't quite\nfine tuning though,",
    "start": "204240",
    "end": "206170"
  },
  {
    "text": "so let's think about how\nwe might get added benefits",
    "start": "206170",
    "end": "208360"
  },
  {
    "text": "from actually updating those\nBERT parameters as opposed",
    "start": "208360",
    "end": "211000"
  },
  {
    "text": "to just using them as\nfrozen representations",
    "start": "211000",
    "end": "213760"
  },
  {
    "text": "inputs to another model.",
    "start": "213760",
    "end": "216599"
  },
  {
    "start": "216000",
    "end": "353000"
  },
  {
    "text": "What I'd encourage\nyou to do is think",
    "start": "216600",
    "end": "218250"
  },
  {
    "text": "about subclassing the\nPyTorch modules that",
    "start": "218250",
    "end": "220650"
  },
  {
    "text": "are included in our\ncourse code distribution.",
    "start": "220650",
    "end": "222750"
  },
  {
    "text": "Because then, you will be able\nto write code, just oriented",
    "start": "222750",
    "end": "226380"
  },
  {
    "text": "toward your model\narchitecture, and a lot",
    "start": "226380",
    "end": "228420"
  },
  {
    "text": "of the details of optimization\nand data processing",
    "start": "228420",
    "end": "230880"
  },
  {
    "text": "will be handled for you.",
    "start": "230880",
    "end": "232590"
  },
  {
    "text": "This is, I hope, a\npowerful example of that.",
    "start": "232590",
    "end": "234550"
  },
  {
    "text": "It comes from the tutorial\nPyTorch models notebook.",
    "start": "234550",
    "end": "237720"
  },
  {
    "text": "It's a Torch Softmax\nClassifier, and the only thing",
    "start": "237720",
    "end": "240570"
  },
  {
    "text": "we have to do is rewrite this\nbuild_graph function to specify",
    "start": "240570",
    "end": "243990"
  },
  {
    "text": "one single dense layer.",
    "start": "243990",
    "end": "246180"
  },
  {
    "text": "We are using as our base\nclass the Torch Shallow Neural",
    "start": "246180",
    "end": "248700"
  },
  {
    "text": "Classifier which\nhandles everything else",
    "start": "248700",
    "end": "250709"
  },
  {
    "text": "about setting up this\nmodel and optimizing it.",
    "start": "250710",
    "end": "253870"
  },
  {
    "text": "If we wanted to go in\nthe other direction",
    "start": "253870",
    "end": "255730"
  },
  {
    "text": "and instead fit a\nreally deep model,",
    "start": "255730",
    "end": "257260"
  },
  {
    "text": "we could, again, begin from\nTorch Shallow Neural Classifier",
    "start": "257260",
    "end": "260528"
  },
  {
    "text": "and rewrite the build_graph\nfunction so that it just",
    "start": "260529",
    "end": "262750"
  },
  {
    "text": "has more layers, essentially.",
    "start": "262750",
    "end": "264250"
  },
  {
    "text": "And then what's happening\nin this init method",
    "start": "264250",
    "end": "266380"
  },
  {
    "text": "is we're just giving\nthe user access",
    "start": "266380",
    "end": "268150"
  },
  {
    "text": "to the various hyperparameters\nthat they could",
    "start": "268150",
    "end": "270280"
  },
  {
    "text": "choose to set up this model.",
    "start": "270280",
    "end": "273200"
  },
  {
    "text": "Finally, here's a\nmore involved example.",
    "start": "273200",
    "end": "275260"
  },
  {
    "text": "This one, we start with a\nPyTorch NN module, kind of all",
    "start": "275260",
    "end": "278710"
  },
  {
    "text": "the way down at the base here.",
    "start": "278710",
    "end": "280120"
  },
  {
    "text": "This is a Torch Linear\nRegression model.",
    "start": "280120",
    "end": "282560"
  },
  {
    "text": "We set up the weight\nparameters here",
    "start": "282560",
    "end": "284080"
  },
  {
    "text": "and then we have this\nsingle forward pass,",
    "start": "284080",
    "end": "285860"
  },
  {
    "text": "which corresponds\nto the structure",
    "start": "285860",
    "end": "287289"
  },
  {
    "text": "of a simple linear regression.",
    "start": "287290",
    "end": "289850"
  },
  {
    "text": "Now, for the actual\ninterface, we",
    "start": "289850",
    "end": "292085"
  },
  {
    "text": "need to do a little\nbit more work here.",
    "start": "292085",
    "end": "293710"
  },
  {
    "text": "So we set up the\nloss so that it's",
    "start": "293710",
    "end": "296169"
  },
  {
    "text": "appropriate for our\nregression model.",
    "start": "296170",
    "end": "298190"
  },
  {
    "text": "So most of the classifiers we've\nbeen looking at up until now.",
    "start": "298190",
    "end": "301270"
  },
  {
    "text": "build_graph just\nuses the NN module",
    "start": "301270",
    "end": "303340"
  },
  {
    "text": "that I showed you a second ago.",
    "start": "303340",
    "end": "305025"
  },
  {
    "text": "We need to do a little bit\nof work in build_dataset",
    "start": "305025",
    "end": "307150"
  },
  {
    "text": "and rewrite that so we\nprocess linear regression",
    "start": "307150",
    "end": "309940"
  },
  {
    "text": "data correctly.",
    "start": "309940",
    "end": "311463"
  },
  {
    "text": "And then we do need to\nrewrite the predict and score",
    "start": "311463",
    "end": "313630"
  },
  {
    "text": "functions to be kind of good\ncitizens of the code base",
    "start": "313630",
    "end": "316240"
  },
  {
    "text": "and allow for hyperparameter\noptimization and cross",
    "start": "316240",
    "end": "319120"
  },
  {
    "text": "validation and so forth.",
    "start": "319120",
    "end": "320122"
  },
  {
    "text": "But that's, again,\nstraightforward,",
    "start": "320122",
    "end": "321580"
  },
  {
    "text": "and fundamentally for predict\nwe're actually making use",
    "start": "321580",
    "end": "324069"
  },
  {
    "text": "of the base class's _predict\nmethod for the heavy lifting",
    "start": "324070",
    "end": "327580"
  },
  {
    "text": "there.",
    "start": "327580",
    "end": "328139"
  },
  {
    "text": "And then score, of course, is\njust moving us out of the mode",
    "start": "328140",
    "end": "330640"
  },
  {
    "text": "of evaluating classifiers and\ninto the mode of evaluating",
    "start": "330640",
    "end": "333850"
  },
  {
    "text": "regression models.",
    "start": "333850",
    "end": "335107"
  },
  {
    "text": "That's all you need to do.",
    "start": "335107",
    "end": "336190"
  },
  {
    "text": "And again, conspicuously\nabsent from this",
    "start": "336190",
    "end": "338020"
  },
  {
    "text": "is most of the aspects\nof data processing",
    "start": "338020",
    "end": "340449"
  },
  {
    "text": "and all of the details\nof optimization.",
    "start": "340450",
    "end": "342550"
  },
  {
    "text": "The base class Torch\nmodel base here",
    "start": "342550",
    "end": "345400"
  },
  {
    "text": "has a very full\nfeatured fit method",
    "start": "345400",
    "end": "347590"
  },
  {
    "text": "that you can use to\noptimize these models",
    "start": "347590",
    "end": "349330"
  },
  {
    "text": "and do hyperparameter\nexploration.",
    "start": "349330",
    "end": "352280"
  },
  {
    "text": "And that brings us to the\nstar of the show, which",
    "start": "352280",
    "end": "354710"
  },
  {
    "start": "353000",
    "end": "459000"
  },
  {
    "text": "would be BERT fine tuning\nwith Hugging Face parameters.",
    "start": "354710",
    "end": "357590"
  },
  {
    "text": "Here we'll start with\na PyTorch nn.module.",
    "start": "357590",
    "end": "360860"
  },
  {
    "text": "We load in a BERT module\nas we've done before,",
    "start": "360860",
    "end": "364039"
  },
  {
    "text": "and make sure to set it to\ntrain so that it can be updated.",
    "start": "364040",
    "end": "367228"
  },
  {
    "text": "And then the new\nparameters here are really",
    "start": "367228",
    "end": "369020"
  },
  {
    "text": "just this classifier\nlayer, a dense layer",
    "start": "369020",
    "end": "371132"
  },
  {
    "text": "that's going to be oriented\ntoward the classification",
    "start": "371132",
    "end": "373340"
  },
  {
    "text": "structure that we want\nour model to have.",
    "start": "373340",
    "end": "376490"
  },
  {
    "text": "The forward method\ncalls the forward method",
    "start": "376490",
    "end": "378530"
  },
  {
    "text": "of the BERT model, and you get\na bunch of representations.",
    "start": "378530",
    "end": "380990"
  },
  {
    "text": "There are a lot of options here.",
    "start": "380990",
    "end": "382830"
  },
  {
    "text": "What I've decided to do is just\nuse the Hugging Face Pooler",
    "start": "382830",
    "end": "385460"
  },
  {
    "text": "Output, which is some\nparameters on top",
    "start": "385460",
    "end": "387470"
  },
  {
    "text": "of the class token as the\ninput to the classifier.",
    "start": "387470",
    "end": "391490"
  },
  {
    "text": "When we optimize this model,\nwith luck in a productive way,",
    "start": "391490",
    "end": "394889"
  },
  {
    "text": "not only will these classifier\nparameters be updated, but also",
    "start": "394890",
    "end": "397820"
  },
  {
    "text": "all the parameters\nof this BERT model",
    "start": "397820",
    "end": "399470"
  },
  {
    "text": "that you loaded\nin in train mode.",
    "start": "399470",
    "end": "402170"
  },
  {
    "text": "The interface is a\nlittle bit involved here.",
    "start": "402170",
    "end": "404460"
  },
  {
    "text": "So what we do is\nprovide the user",
    "start": "404460",
    "end": "406280"
  },
  {
    "text": "with some flexibility\nabout what choices to make.",
    "start": "406280",
    "end": "409558"
  },
  {
    "text": "build_graph, again just\nloads in the module",
    "start": "409558",
    "end": "411350"
  },
  {
    "text": "that I showed you\njust a second ago.",
    "start": "411350",
    "end": "413240"
  },
  {
    "text": "And then build_dataset\nis a bit involved.",
    "start": "413240",
    "end": "415800"
  },
  {
    "text": "But what we do fundamentally is\nuse the BERT tokenizer to batch",
    "start": "415800",
    "end": "419750"
  },
  {
    "text": "encode our data.",
    "start": "419750",
    "end": "421130"
  },
  {
    "text": "And then we do a little bit\nof processing on the output",
    "start": "421130",
    "end": "423620"
  },
  {
    "text": "labels to make sure PyTorch\ncan make sense of them.",
    "start": "423620",
    "end": "426440"
  },
  {
    "text": "That's really it.",
    "start": "426440",
    "end": "427160"
  },
  {
    "text": "In the heart of this, it's\njust that we're, again,",
    "start": "427160",
    "end": "429243"
  },
  {
    "text": "using Hugging Face\nfunctionality to represent",
    "start": "429243",
    "end": "431750"
  },
  {
    "text": "our data to the BERT model.",
    "start": "431750",
    "end": "433790"
  },
  {
    "text": "And then this is the\nreally interesting part.",
    "start": "433790",
    "end": "436400"
  },
  {
    "text": "Calling the forward\nmethod and then fitting",
    "start": "436400",
    "end": "438530"
  },
  {
    "text": "the classifier on top is\npretty much all you need to do.",
    "start": "438530",
    "end": "441690"
  },
  {
    "text": "And of course, that opens\nup a world of options.",
    "start": "441690",
    "end": "443840"
  },
  {
    "text": "Reps here has lots\nof other things",
    "start": "443840",
    "end": "445430"
  },
  {
    "text": "that you could use as the\ninput to this classifier layer.",
    "start": "445430",
    "end": "448992"
  },
  {
    "text": "And many of them\nactually might be",
    "start": "448993",
    "end": "450410"
  },
  {
    "text": "more productive than the simple\napproach that I've taken here.",
    "start": "450410",
    "end": "454300"
  },
  {
    "start": "454300",
    "end": "458198"
  }
]