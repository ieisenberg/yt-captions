[
  {
    "start": "0",
    "end": "4468"
  },
  {
    "text": "CHRISTOPHER POTTS:\nWelcome, everyone.",
    "start": "4468",
    "end": "6010"
  },
  {
    "text": "This is the first\nscreencast in our series",
    "start": "6010",
    "end": "7760"
  },
  {
    "text": "on in-context learning.",
    "start": "7760",
    "end": "8940"
  },
  {
    "text": "This series is a kind\nof companion to the one",
    "start": "8940",
    "end": "10950"
  },
  {
    "text": "that we did on\ninformation retrieval.",
    "start": "10950",
    "end": "12870"
  },
  {
    "text": "The two series come\ntogether to help you",
    "start": "12870",
    "end": "14969"
  },
  {
    "text": "with Homework 2 and\nBakeoff 2, which",
    "start": "14970",
    "end": "16770"
  },
  {
    "text": "is focused on few-shot open\ndomain question answering",
    "start": "16770",
    "end": "20130"
  },
  {
    "text": "with frozen retrievers and\nfrozen large language models.",
    "start": "20130",
    "end": "24329"
  },
  {
    "text": "To start this series,\nI thought we would just",
    "start": "24330",
    "end": "26220"
  },
  {
    "text": "reflect a bit on the\norigins of the idea",
    "start": "26220",
    "end": "29009"
  },
  {
    "text": "of in-context learning, which\nis really a story of how NLP got",
    "start": "29010",
    "end": "33330"
  },
  {
    "text": "to this strange and exciting\nand chaotic moment for the field",
    "start": "33330",
    "end": "38100"
  },
  {
    "text": "and maybe also for the\nsociety more broadly.",
    "start": "38100",
    "end": "41340"
  },
  {
    "text": "All credit to the ChomskyBot\nfor bringing us to this moment.",
    "start": "41340",
    "end": "45630"
  },
  {
    "text": "I'm only joking.",
    "start": "45630",
    "end": "46810"
  },
  {
    "text": "The ChomskyBot is a very simple\npattern-based language model.",
    "start": "46810",
    "end": "51990"
  },
  {
    "text": "It's been around since\nthe '90s, I believe.",
    "start": "51990",
    "end": "55320"
  },
  {
    "text": "And with very simple\nmechanisms, it",
    "start": "55320",
    "end": "57690"
  },
  {
    "text": "produces prose that is\nroughly in the style",
    "start": "57690",
    "end": "60360"
  },
  {
    "text": "of the political philosopher and\nsometime linguist Noam Chomsky.",
    "start": "60360",
    "end": "64510"
  },
  {
    "text": "It produces prose that\ndelights and maybe informs us,",
    "start": "64510",
    "end": "68860"
  },
  {
    "text": "and the underlying\nmechanisms are very simple.",
    "start": "68860",
    "end": "71110"
  },
  {
    "text": "And I think that's a\nnice reminder about what",
    "start": "71110",
    "end": "73960"
  },
  {
    "text": "all of these large\nlanguage models",
    "start": "73960",
    "end": "75430"
  },
  {
    "text": "might be doing even\nin the present day.",
    "start": "75430",
    "end": "78350"
  },
  {
    "text": "But I'm only joking.",
    "start": "78350",
    "end": "79790"
  },
  {
    "text": "Although it's only\npartly a joke.",
    "start": "79790",
    "end": "81470"
  },
  {
    "text": "I think when we think\nabout precedents",
    "start": "81470",
    "end": "83360"
  },
  {
    "text": "for in-context\nlearning, it is worth",
    "start": "83360",
    "end": "85460"
  },
  {
    "text": "mentioning that in the\npre-deep learning era,",
    "start": "85460",
    "end": "88490"
  },
  {
    "text": "n-gram based language models,\nvery sparse large language",
    "start": "88490",
    "end": "91970"
  },
  {
    "text": "models, were often\ntruly massive.",
    "start": "91970",
    "end": "94520"
  },
  {
    "text": "For example, Brants et al.",
    "start": "94520",
    "end": "95869"
  },
  {
    "text": "2007 used a 300 billion\nparameter language model",
    "start": "95870",
    "end": "100820"
  },
  {
    "text": "trained on two\ntrillion tokens of text",
    "start": "100820",
    "end": "103490"
  },
  {
    "text": "to help with\nmachine translation.",
    "start": "103490",
    "end": "105619"
  },
  {
    "text": "That is a very large\nand very powerful",
    "start": "105620",
    "end": "108080"
  },
  {
    "text": "mechanism with a\ndifferent character",
    "start": "108080",
    "end": "109880"
  },
  {
    "text": "from the large language\nmodels of today.",
    "start": "109880",
    "end": "112070"
  },
  {
    "text": "But it is nonetheless\nworth noting",
    "start": "112070",
    "end": "114020"
  },
  {
    "text": "that they played\nan important role",
    "start": "114020",
    "end": "115820"
  },
  {
    "text": "in a lot of different\nfields way back when.",
    "start": "115820",
    "end": "119330"
  },
  {
    "text": "I think for in-context\nlearning as we know it now,",
    "start": "119330",
    "end": "123140"
  },
  {
    "text": "the earliest paper, as far as\nI know, is the decaNLP paper.",
    "start": "123140",
    "end": "127700"
  },
  {
    "text": "This is McCann et al.",
    "start": "127700",
    "end": "128750"
  },
  {
    "text": "2018.",
    "start": "128750",
    "end": "129649"
  },
  {
    "text": "They do multitask training\nwith task instructions that",
    "start": "129650",
    "end": "133370"
  },
  {
    "text": "are natural language questions.",
    "start": "133370",
    "end": "135260"
  },
  {
    "text": "And that does seem like\nthe origin of the idea",
    "start": "135260",
    "end": "138209"
  },
  {
    "text": "that with free form natural\nlanguage instructions,",
    "start": "138210",
    "end": "140430"
  },
  {
    "text": "we could essentially\nend up with artifacts",
    "start": "140430",
    "end": "142859"
  },
  {
    "text": "that could do multiple\nthings guided solely by text.",
    "start": "142860",
    "end": "148200"
  },
  {
    "text": "And then it's worth noting\nalso that in the GPT paper,",
    "start": "148200",
    "end": "151590"
  },
  {
    "text": "Radford et al.",
    "start": "151590",
    "end": "152370"
  },
  {
    "text": "2018, you can find buried in\nthere some tentative proposals",
    "start": "152370",
    "end": "156299"
  },
  {
    "text": "to do prompt-based\nexperiments with that model.",
    "start": "156300",
    "end": "161400"
  },
  {
    "text": "But the real origins of the\nideas, again, as far as I know,",
    "start": "161400",
    "end": "165780"
  },
  {
    "text": "are Radford et al.",
    "start": "165780",
    "end": "167000"
  },
  {
    "text": "2019.",
    "start": "167000",
    "end": "167870"
  },
  {
    "text": "This is the GPT-2 paper.",
    "start": "167870",
    "end": "170569"
  },
  {
    "text": "And let me just show you some\nsnippets from this paper.",
    "start": "170570",
    "end": "173060"
  },
  {
    "text": "It's really inspiring\nhow much they did.",
    "start": "173060",
    "end": "175250"
  },
  {
    "text": "They say at the\nstart, we demonstrate",
    "start": "175250",
    "end": "177170"
  },
  {
    "text": "language models can\nperform downstream tasks",
    "start": "177170",
    "end": "179540"
  },
  {
    "text": "in a zero-shot setting without\nany parameter or architecture",
    "start": "179540",
    "end": "183260"
  },
  {
    "text": "modification.",
    "start": "183260",
    "end": "184110"
  },
  {
    "text": "So there you see this idea\nof using frozen models,",
    "start": "184110",
    "end": "187370"
  },
  {
    "text": "prompting them and\nseeing if they will",
    "start": "187370",
    "end": "189409"
  },
  {
    "text": "produce interesting behaviors.",
    "start": "189410",
    "end": "191540"
  },
  {
    "text": "They looked at a bunch\nof different tasks.",
    "start": "191540",
    "end": "194120"
  },
  {
    "text": "For summarization, they say, to\ninduce summarization behavior,",
    "start": "194120",
    "end": "197450"
  },
  {
    "text": "we add the text TL;DR after\nthe article and generate 100",
    "start": "197450",
    "end": "201319"
  },
  {
    "text": "tokens.",
    "start": "201320",
    "end": "202130"
  },
  {
    "text": "This is mind-blowing.",
    "start": "202130",
    "end": "203240"
  },
  {
    "text": "I remember when I first\nheard about this idea,",
    "start": "203240",
    "end": "206570"
  },
  {
    "text": "I had such a cognitive bias\nagainst in-context learning",
    "start": "206570",
    "end": "210200"
  },
  {
    "text": "of this sort being successful\nthat I assumed what they were",
    "start": "210200",
    "end": "213319"
  },
  {
    "text": "trying to say to us is that\nthey had trained that token",
    "start": "213320",
    "end": "217900"
  },
  {
    "text": "in a task-specific way to do\nsummarization and then just",
    "start": "217900",
    "end": "220879"
  },
  {
    "text": "kind of giving it\na colorful name.",
    "start": "220880",
    "end": "223090"
  },
  {
    "text": "But no, they really meant it.",
    "start": "223090",
    "end": "225010"
  },
  {
    "text": "They simply prompt the\nmodel with this token,",
    "start": "225010",
    "end": "227349"
  },
  {
    "text": "and look at what comes out.",
    "start": "227350",
    "end": "230630"
  },
  {
    "text": "For translation,\nthey say, we test",
    "start": "230630",
    "end": "232370"
  },
  {
    "text": "whether GPT-2 has begun\nto learn how to translate",
    "start": "232370",
    "end": "235069"
  },
  {
    "text": "from one language to another.",
    "start": "235070",
    "end": "236780"
  },
  {
    "text": "In order to help it infer\nthat this is the desired task,",
    "start": "236780",
    "end": "239720"
  },
  {
    "text": "we condition the language\nmodel on a context of example",
    "start": "239720",
    "end": "242570"
  },
  {
    "text": "pairs of the format\nEnglish sentence",
    "start": "242570",
    "end": "244490"
  },
  {
    "text": "equals French sentence.",
    "start": "244490",
    "end": "246140"
  },
  {
    "text": "And then after a final prompt\nof English sentence equals,",
    "start": "246140",
    "end": "249230"
  },
  {
    "text": "we sample from the model\nwith greedy decoding",
    "start": "249230",
    "end": "251540"
  },
  {
    "text": "and use the first generated\nsentence as the translation.",
    "start": "251540",
    "end": "254569"
  },
  {
    "text": "Incredible.",
    "start": "254570",
    "end": "255440"
  },
  {
    "text": "And what you see emerging there\nis this idea of demonstrations,",
    "start": "255440",
    "end": "259047"
  },
  {
    "text": "including in the prompts.",
    "start": "259047",
    "end": "260088"
  },
  {
    "text": "Some examples of the\nbehavior that you",
    "start": "260089",
    "end": "261919"
  },
  {
    "text": "want as a way of\ncoaxing the model",
    "start": "261920",
    "end": "263930"
  },
  {
    "text": "to do what you\nwould like it to do.",
    "start": "263930",
    "end": "265970"
  },
  {
    "text": "Here's a similar example.",
    "start": "265970",
    "end": "267620"
  },
  {
    "text": "They say, similar\nto translation,",
    "start": "267620",
    "end": "269300"
  },
  {
    "text": "the context of\nthe language model",
    "start": "269300",
    "end": "270800"
  },
  {
    "text": "is seeded with example\nquestion-answer pairs, which",
    "start": "270800",
    "end": "273830"
  },
  {
    "text": "helps the model infer the short\nanswer style of the data set.",
    "start": "273830",
    "end": "277199"
  },
  {
    "text": "So that's for QA.",
    "start": "277200",
    "end": "278210"
  },
  {
    "text": "And again, they started\nto see that demonstrations",
    "start": "278210",
    "end": "280819"
  },
  {
    "text": "could help the model see\nwhat the implicit task",
    "start": "280820",
    "end": "283730"
  },
  {
    "text": "instruction was.",
    "start": "283730",
    "end": "285470"
  },
  {
    "text": "And they also in\nthe paper evaluate",
    "start": "285470",
    "end": "287860"
  },
  {
    "text": "a bunch of other things.",
    "start": "287860",
    "end": "288860"
  },
  {
    "text": "Text completion,\nWinograd schemas,",
    "start": "288860",
    "end": "291509"
  },
  {
    "text": "and reading comprehension,\nand maybe others.",
    "start": "291510",
    "end": "293850"
  },
  {
    "text": "It's a very impressive\nand thorough exploration.",
    "start": "293850",
    "end": "295920"
  },
  {
    "text": "Very open about the benefits\nand limitations of the methods.",
    "start": "295920",
    "end": "299850"
  },
  {
    "text": "Very impressive\nand creative paper.",
    "start": "299850",
    "end": "303830"
  },
  {
    "text": "That was the beginning of the\nidea in terms of research.",
    "start": "303830",
    "end": "309360"
  },
  {
    "text": "The cultural moment certainly\narrives with the GPT-3 paper.",
    "start": "309360",
    "end": "313439"
  },
  {
    "text": "Brown et al.",
    "start": "313440",
    "end": "314250"
  },
  {
    "text": "2020, which is also\nimpressive in its own ways.",
    "start": "314250",
    "end": "317165"
  },
  {
    "text": "And here, I'm just going\nto quote from the abstract",
    "start": "317165",
    "end": "319290"
  },
  {
    "text": "and we can linger a\nbit over what it says.",
    "start": "319290",
    "end": "322030"
  },
  {
    "text": "They start, we show that scaling\nup language models greatly",
    "start": "322030",
    "end": "325380"
  },
  {
    "text": "improves task-agnostic\nfew-shot performance,",
    "start": "325380",
    "end": "328290"
  },
  {
    "text": "sometimes even reaching\ncompetitiveness with prior",
    "start": "328290",
    "end": "331410"
  },
  {
    "text": "state-of-the-art\nfine-tuning approaches.",
    "start": "331410",
    "end": "333840"
  },
  {
    "text": "We could quibble\nwith whether or not",
    "start": "333840",
    "end": "335639"
  },
  {
    "text": "they actually saw\ncompetitiveness in that sense.",
    "start": "335640",
    "end": "338140"
  },
  {
    "text": "But it is absolutely\ntrue that they",
    "start": "338140",
    "end": "340470"
  },
  {
    "text": "got very impressive\nbehaviors out of their model",
    "start": "340470",
    "end": "342930"
  },
  {
    "text": "in this task-agnostic\nfew-shot setting.",
    "start": "342930",
    "end": "346600"
  },
  {
    "text": "Specifically, we train GPT-3, an\nautoregressive language model,",
    "start": "346600",
    "end": "350980"
  },
  {
    "text": "with 175 billion\nparameters, 10x more",
    "start": "350980",
    "end": "354400"
  },
  {
    "text": "than any previous\nnon-sparse language model,",
    "start": "354400",
    "end": "356919"
  },
  {
    "text": "and test its performance\nin the few-shot setting.",
    "start": "356920",
    "end": "359500"
  },
  {
    "text": "There are two things I\nreally like about this part.",
    "start": "359500",
    "end": "361720"
  },
  {
    "text": "First, 175 billion\nparameters is indeed",
    "start": "361720",
    "end": "365290"
  },
  {
    "text": "incredibly ambitious and\nimpressive, even today,",
    "start": "365290",
    "end": "368530"
  },
  {
    "text": "to say nothing of back in 2020.",
    "start": "368530",
    "end": "370780"
  },
  {
    "text": "And I also really love that they\nmentioned non-sparse language",
    "start": "370780",
    "end": "374630"
  },
  {
    "text": "model.",
    "start": "374630",
    "end": "375130"
  },
  {
    "text": "A nod to those\nn-gram based models",
    "start": "375130",
    "end": "377350"
  },
  {
    "text": "that I mentioned before, which\nwere often truly massive.",
    "start": "377350",
    "end": "381580"
  },
  {
    "text": "For all tasks, GPT-3 is applied\nwithout any gradient updates",
    "start": "381580",
    "end": "385659"
  },
  {
    "text": "or fine-tuning, with tasks\nand few-shot demonstrations",
    "start": "385660",
    "end": "388900"
  },
  {
    "text": "specified purely via text\ninteraction with the model.",
    "start": "388900",
    "end": "392080"
  },
  {
    "text": "That's nice.",
    "start": "392080",
    "end": "392949"
  },
  {
    "text": "You might think in\nretrospect that they're kind",
    "start": "392950",
    "end": "395400"
  },
  {
    "text": "of repeating themselves here.",
    "start": "395400",
    "end": "397156"
  },
  {
    "text": "They've already\nestablished that these",
    "start": "397157",
    "end": "398740"
  },
  {
    "text": "are going to be frozen models.",
    "start": "398740",
    "end": "399990"
  },
  {
    "text": "But I think it's\nnecessary for them",
    "start": "399990",
    "end": "402039"
  },
  {
    "text": "to do that because this was\nsuch an unfamiliar idea.",
    "start": "402040",
    "end": "404530"
  },
  {
    "text": "And I can imagine, again,\nbeing a reader of this paper",
    "start": "404530",
    "end": "407870"
  },
  {
    "text": "and assuming that\nthey can't really",
    "start": "407870",
    "end": "409610"
  },
  {
    "text": "mean they're just using frozen\nmodels for all these tasks.",
    "start": "409610",
    "end": "412099"
  },
  {
    "text": "Surely there is some\nfine tuning somewhere.",
    "start": "412100",
    "end": "414080"
  },
  {
    "text": "And so they're emphasizing\nthat, in fact, the model",
    "start": "414080",
    "end": "416960"
  },
  {
    "text": "is entirely frozen.",
    "start": "416960",
    "end": "419449"
  },
  {
    "text": "GPT-3 achieves\nstrong performance",
    "start": "419450",
    "end": "421700"
  },
  {
    "text": "on many NLP data sets,\nincluding translation, question",
    "start": "421700",
    "end": "425090"
  },
  {
    "text": "answering, and cloze tasks.",
    "start": "425090",
    "end": "426800"
  },
  {
    "text": "As well as several tasks that\nrequire on-the-fly reasoning",
    "start": "426800",
    "end": "429949"
  },
  {
    "text": "or domain adaptation, such\nas unscrambling words, using",
    "start": "429950",
    "end": "433550"
  },
  {
    "text": "a novel word in a\nsentence, or performing",
    "start": "433550",
    "end": "435349"
  },
  {
    "text": "three-digit arithmetic.",
    "start": "435350",
    "end": "436920"
  },
  {
    "text": "I love this.",
    "start": "436920",
    "end": "437600"
  },
  {
    "text": "A real diversity of tasks.",
    "start": "437600",
    "end": "439160"
  },
  {
    "text": "And what I think you\ncan see them doing",
    "start": "439160",
    "end": "441110"
  },
  {
    "text": "is really trying to\npush the limits of what",
    "start": "441110",
    "end": "443180"
  },
  {
    "text": "would be possible in this mode.",
    "start": "443180",
    "end": "446259"
  },
  {
    "text": "At the same time,\nwe also identify",
    "start": "446260",
    "end": "448120"
  },
  {
    "text": "some data sets where GPT-3's\nfew-shot learning still",
    "start": "448120",
    "end": "451360"
  },
  {
    "text": "struggles, as well as some\ndata sets where GPT-3 faces",
    "start": "451360",
    "end": "454479"
  },
  {
    "text": "methodological issues related to\ntraining on large web corpora.",
    "start": "454480",
    "end": "458410"
  },
  {
    "text": "I also love this sentence.",
    "start": "458410",
    "end": "459700"
  },
  {
    "text": "It's again, very open\nabout what they achieved",
    "start": "459700",
    "end": "462010"
  },
  {
    "text": "and where the limitations are.",
    "start": "462010",
    "end": "463510"
  },
  {
    "text": "They're acknowledging that\nthey found some tasks that",
    "start": "463510",
    "end": "465760"
  },
  {
    "text": "are still hard for the model.",
    "start": "465760",
    "end": "467170"
  },
  {
    "text": "And they also\nacknowledge in the paper",
    "start": "467170",
    "end": "469000"
  },
  {
    "text": "that they had some\nsort of minor slip-ups",
    "start": "469000",
    "end": "471550"
  },
  {
    "text": "where they intended to make\nsure they hadn't trained on data",
    "start": "471550",
    "end": "474639"
  },
  {
    "text": "that was relevant for the test\ntask that they were performing,",
    "start": "474640",
    "end": "477970"
  },
  {
    "text": "and in fact, they had not\nquite gotten that right.",
    "start": "477970",
    "end": "480610"
  },
  {
    "text": "And so they're being\nvery open about that",
    "start": "480610",
    "end": "482650"
  },
  {
    "text": "and exploring how hard it is\nto get that right at the scale",
    "start": "482650",
    "end": "486370"
  },
  {
    "text": "that they're operating at.",
    "start": "486370",
    "end": "487820"
  },
  {
    "text": "So just like the GPT-2 paper,\na wonderfully open and thorough",
    "start": "487820",
    "end": "492340"
  },
  {
    "text": "exploration of the ideas.",
    "start": "492340",
    "end": "495960"
  },
  {
    "start": "495960",
    "end": "501000"
  }
]