[
  {
    "text": "OK, in this next\nexample, we're going",
    "start": "0",
    "end": "2029"
  },
  {
    "text": "to look at the IMDb Document\nClassification problem.",
    "start": "2029",
    "end": "5100"
  },
  {
    "text": "So here the observations\nare actually documents--",
    "start": "5100",
    "end": "10130"
  },
  {
    "text": "short documents, which each\none is a review of a movie.",
    "start": "10130",
    "end": "14090"
  },
  {
    "text": "So we've got sentences in\nit and words and so on.",
    "start": "14090",
    "end": "17400"
  },
  {
    "text": "And there's a\nsequence to these--",
    "start": "17400",
    "end": "19490"
  },
  {
    "text": "the sequential data.",
    "start": "19490",
    "end": "21690"
  },
  {
    "text": "But the first model\nwe're going to fit",
    "start": "21690",
    "end": "23510"
  },
  {
    "text": "is going to use a bag\nof words representation.",
    "start": "23510",
    "end": "26280"
  },
  {
    "text": "And so in this case, we\nrestrict the dictionary",
    "start": "26280",
    "end": "28970"
  },
  {
    "text": "to 10,000 words in the\nEnglish dictionary.",
    "start": "28970",
    "end": "32570"
  },
  {
    "text": "And these are somehow the\ntop important 10,000 words.",
    "start": "32570",
    "end": "37160"
  },
  {
    "text": "And we're going to score our\ndocument for how many times",
    "start": "37160",
    "end": "40040"
  },
  {
    "text": "each word appeared\nin the document.",
    "start": "40040",
    "end": "41930"
  },
  {
    "text": "That means we'll have\nan observation vector",
    "start": "41930",
    "end": "44300"
  },
  {
    "text": "of length 10,000, mostly 0.",
    "start": "44300",
    "end": "46970"
  },
  {
    "text": "And to make this\neasy for this lab,",
    "start": "46970",
    "end": "50400"
  },
  {
    "text": "we actually\npre-prepared the data.",
    "start": "50400",
    "end": "53130"
  },
  {
    "text": "And that's where\nwe're going to start.",
    "start": "53130",
    "end": "55760"
  },
  {
    "text": "And Jonathan will tell\nus what we do next.",
    "start": "55760",
    "end": "58789"
  },
  {
    "text": "Thank you, Trevor.",
    "start": "58790",
    "end": "59540"
  },
  {
    "text": "And we also compare that to\nat least one sequential model.",
    "start": "59540",
    "end": "63019"
  },
  {
    "text": "There are various sequential\nmodels one could apply to this.",
    "start": "63019",
    "end": "65518"
  },
  {
    "text": "But we're just going to try\nand contrast the bag of words",
    "start": "65519",
    "end": "67979"
  },
  {
    "text": "to the sequential one here.",
    "start": "67980",
    "end": "71190"
  },
  {
    "text": "So the general structure\nwill follow a similar pattern",
    "start": "71190",
    "end": "76680"
  },
  {
    "text": "to what we've seen--",
    "start": "76680",
    "end": "77910"
  },
  {
    "text": "to the examples\nwe've seen so far.",
    "start": "77910",
    "end": "80410"
  },
  {
    "text": "So we're going to skip to\nwhere we specify our model.",
    "start": "80410",
    "end": "86490"
  },
  {
    "text": "And it's going to be a\npretty simple model with one",
    "start": "86490",
    "end": "89700"
  },
  {
    "text": "hidden layer here.",
    "start": "89700",
    "end": "90630"
  },
  {
    "text": "And our network specification\nis very similar to the Hitters",
    "start": "90630",
    "end": "97740"
  },
  {
    "text": "data.",
    "start": "97740",
    "end": "98890"
  },
  {
    "text": "Recall, we have to call\nthis super function.",
    "start": "98890",
    "end": "101260"
  },
  {
    "text": "So that Torch knows--",
    "start": "101260",
    "end": "103440"
  },
  {
    "text": "well, it needs to do this\nin order to be ready to fit.",
    "start": "103440",
    "end": "108690"
  },
  {
    "text": "So the rest of the specifying\nthe model and fitting",
    "start": "108690",
    "end": "111150"
  },
  {
    "text": "is very similar to the\nHitters and the MNIST",
    "start": "111150",
    "end": "116280"
  },
  {
    "text": "as well as the CIFAR data.",
    "start": "116280",
    "end": "118110"
  },
  {
    "text": "So, Jonathan, in this case,\nwould input size be 10,000",
    "start": "118110",
    "end": "123090"
  },
  {
    "text": "and it's going to\n16 hidden units?",
    "start": "123090",
    "end": "125939"
  },
  {
    "text": "Yes, that's right.",
    "start": "125940",
    "end": "128460"
  },
  {
    "text": "Thank you.",
    "start": "128460",
    "end": "129060"
  },
  {
    "text": "Yes, so this is where we\ntell what the input size is.",
    "start": "129060",
    "end": "134220"
  },
  {
    "text": "And it says, well, this\nis the the x train tensor.",
    "start": "134220",
    "end": "140970"
  },
  {
    "text": "And this is the number\nof columns of it--",
    "start": "140970",
    "end": "143340"
  },
  {
    "text": "10,000.",
    "start": "143340",
    "end": "145230"
  },
  {
    "text": "OK, and you can see\nthe summary here.",
    "start": "145230",
    "end": "149282"
  },
  {
    "text": "There are three stop\nwords that are included--",
    "start": "149283",
    "end": "151200"
  },
  {
    "text": "Three stop words.",
    "start": "151200",
    "end": "151907"
  },
  {
    "text": "Yes, but morally\n10,000 features here.",
    "start": "151908",
    "end": "156660"
  },
  {
    "text": "OK, so let's just\nspecify our model again.",
    "start": "156660",
    "end": "161760"
  },
  {
    "text": "Instead of regression,\nit's binary classification.",
    "start": "161760",
    "end": "164400"
  },
  {
    "text": "And we can make a similar\nplot to the other examples",
    "start": "164400",
    "end": "168329"
  },
  {
    "text": "and track--",
    "start": "168330",
    "end": "169350"
  },
  {
    "text": "There were 25,000 training\nexamples and 25,000 in test.",
    "start": "169350",
    "end": "174150"
  },
  {
    "text": "Yes.",
    "start": "174150",
    "end": "175349"
  },
  {
    "text": "And we get-- it achieves\nan accuracy of about 85%.",
    "start": "175350",
    "end": "179490"
  },
  {
    "text": "And since this is a relatively\nsimple model, the bag of words",
    "start": "179490",
    "end": "182520"
  },
  {
    "text": "model, we can use other methods,\nlike other linear methods",
    "start": "182520",
    "end": "186420"
  },
  {
    "text": "or regularized methods,\nlike the Lasso.",
    "start": "186420",
    "end": "189090"
  },
  {
    "text": "So the response was sentiment,\nwhich was positive or negative.",
    "start": "189090",
    "end": "193256"
  },
  {
    "text": "Yes, whether someone\nlikes the movie or not.",
    "start": "193257",
    "end": "195090"
  },
  {
    "text": "That's right.",
    "start": "195090",
    "end": "197440"
  },
  {
    "text": "OK, so we're going to compare\nthis to logistic regression",
    "start": "197440",
    "end": "200980"
  },
  {
    "text": "with the LASSO.",
    "start": "200980",
    "end": "201659"
  },
  {
    "start": "201660",
    "end": "206470"
  },
  {
    "text": "And I guess a note here--",
    "start": "206470",
    "end": "208420"
  },
  {
    "text": "we see we're going to use\nsparse data representations",
    "start": "208420",
    "end": "212620"
  },
  {
    "text": "for the training and\ntest because we have,",
    "start": "212620",
    "end": "215349"
  },
  {
    "text": "well, 25,000 cases in\neach training and test",
    "start": "215350",
    "end": "218360"
  },
  {
    "text": "and 10,000 features.",
    "start": "218360",
    "end": "219640"
  },
  {
    "text": "If those were full matrices,\nthose would be pretty big.",
    "start": "219640",
    "end": "222190"
  },
  {
    "text": "But, of course, most\nof the data matrix",
    "start": "222190",
    "end": "224440"
  },
  {
    "text": "is going to be zeros because\nthe documents are all short.",
    "start": "224440",
    "end": "227860"
  },
  {
    "text": "And so most of the\nwords in our dictionary",
    "start": "227860",
    "end": "230470"
  },
  {
    "text": "aren't going to be used.",
    "start": "230470",
    "end": "231610"
  },
  {
    "text": "Yeah, so if we didn't\nuse sparse matrices here,",
    "start": "231610",
    "end": "233620"
  },
  {
    "text": "this would be really very\nmemory intensive to load",
    "start": "233620",
    "end": "238180"
  },
  {
    "text": "and very expensive to fit.",
    "start": "238180",
    "end": "241150"
  },
  {
    "text": "This is sort of ideal\nproblem for Lasso",
    "start": "241150",
    "end": "243280"
  },
  {
    "text": "as well when you've got\nvery wide data sets--",
    "start": "243280",
    "end": "245830"
  },
  {
    "text": "in this case, 10,000.",
    "start": "245830",
    "end": "247048"
  },
  {
    "text": "And you don't expect\nall the features",
    "start": "247048",
    "end": "248590"
  },
  {
    "text": "to be important for this task.",
    "start": "248590",
    "end": "250120"
  },
  {
    "text": "Yes.",
    "start": "250120",
    "end": "251830"
  },
  {
    "text": "So let's see how\nwell the Lasso does.",
    "start": "251830",
    "end": "255160"
  },
  {
    "text": "We're going to-- what we'll do--\nwe'll compute the solution path",
    "start": "255160",
    "end": "262390"
  },
  {
    "text": "and evaluate test data\nalong the solution path",
    "start": "262390",
    "end": "264910"
  },
  {
    "text": "and compare that\nto a the test score",
    "start": "264910",
    "end": "270410"
  },
  {
    "text": "for the neural network\nas a function of Epoch.",
    "start": "270410",
    "end": "273590"
  },
  {
    "text": "So I'm just going to\nskip to the plot here.",
    "start": "273590",
    "end": "280720"
  },
  {
    "text": "So we compare training and\ntest hair, black and blue,",
    "start": "280720",
    "end": "289260"
  },
  {
    "text": "between the two methods.",
    "start": "289260",
    "end": "290460"
  },
  {
    "text": "And so the x-axes\nhere are different.",
    "start": "290460",
    "end": "292979"
  },
  {
    "text": "On the left hand side, this\nis the lambda parameter",
    "start": "292980",
    "end": "295530"
  },
  {
    "text": "for the lasso.",
    "start": "295530",
    "end": "296490"
  },
  {
    "text": "And we see going from a very\nsparse solution to a denser",
    "start": "296490",
    "end": "300780"
  },
  {
    "text": "solution, it has an\naccuracy in the low 80s,",
    "start": "300780",
    "end": "305230"
  },
  {
    "text": "which is quite similar to\nthe right hand side here.",
    "start": "305230",
    "end": "307810"
  },
  {
    "text": "This is our single hidden\nlayer neural network.",
    "start": "307810",
    "end": "311410"
  },
  {
    "text": "And what this example\nshows is that sometimes",
    "start": "311410",
    "end": "314820"
  },
  {
    "text": "the neural network is\nnot necessary to get",
    "start": "314820",
    "end": "318690"
  },
  {
    "text": "a good performance.",
    "start": "318690",
    "end": "319560"
  },
  {
    "text": "In this case, we\nhave about the same.",
    "start": "319560",
    "end": "323320"
  },
  {
    "text": "And I should say at the\ndense end, it's about 83%.",
    "start": "323320",
    "end": "326370"
  },
  {
    "text": "I think it peaks\nhere at about 88%.",
    "start": "326370",
    "end": "328543"
  },
  {
    "text": "So it does slightly\nbetter on test data.",
    "start": "328543",
    "end": "330210"
  },
  {
    "text": "But that's not that\nbig of an improvement.",
    "start": "330210",
    "end": "332383"
  },
  {
    "text": "But it's still slightly better.",
    "start": "332383",
    "end": "333675"
  },
  {
    "start": "333675",
    "end": "336599"
  },
  {
    "text": "So we're going to compare\nthese bag of word models",
    "start": "336600",
    "end": "339990"
  },
  {
    "text": "to a simple--",
    "start": "339990",
    "end": "342210"
  },
  {
    "text": "well, a sequential model for\ndocument for the IMDb data.",
    "start": "342210",
    "end": "347169"
  },
  {
    "text": "So we're going to use this\nLSTM, so Long Short-Term Memory,",
    "start": "347170",
    "end": "351780"
  },
  {
    "text": "recurrent neural network.",
    "start": "351780",
    "end": "353070"
  },
  {
    "text": "And you can read\nabout the structure",
    "start": "353070",
    "end": "355200"
  },
  {
    "text": "of that in the textbook.",
    "start": "355200",
    "end": "357120"
  },
  {
    "text": "We'll see in order to specify,\nit is actually pretty simple.",
    "start": "357120",
    "end": "360580"
  },
  {
    "text": "We're going to combine\nthat with this embedding",
    "start": "360580",
    "end": "364500"
  },
  {
    "text": "representation of the words.",
    "start": "364500",
    "end": "368370"
  },
  {
    "start": "368370",
    "end": "373470"
  },
  {
    "text": "And we'll ultimately want\nto compare the accuracy.",
    "start": "373470",
    "end": "377800"
  },
  {
    "text": "So, again, we have to\nconstruct our data loader",
    "start": "377800",
    "end": "381449"
  },
  {
    "text": "for all of these different\nviews of the IMDb data.",
    "start": "381450",
    "end": "384135"
  },
  {
    "text": "Actually, there's a\ndifferent form of the way",
    "start": "384135",
    "end": "386010"
  },
  {
    "text": "that the data is represented.",
    "start": "386010",
    "end": "388750"
  },
  {
    "text": "For the sequential model,\nit's important to have",
    "start": "388750",
    "end": "390750"
  },
  {
    "text": "a sequential representation.",
    "start": "390750",
    "end": "392350"
  },
  {
    "text": "So each-- rather than\njust reducing the document",
    "start": "392350",
    "end": "396220"
  },
  {
    "text": "to the number of times\neach word appears,",
    "start": "396220",
    "end": "399670"
  },
  {
    "text": "we actually have to track\nthe order of the words.",
    "start": "399670",
    "end": "403990"
  },
  {
    "text": "So in order to set\nthe data up here,",
    "start": "403990",
    "end": "406030"
  },
  {
    "text": "the documents were restricted\nof length 500 words.",
    "start": "406030",
    "end": "410660"
  },
  {
    "text": "So if a document had\nless than 500 words,",
    "start": "410660",
    "end": "413050"
  },
  {
    "text": "padding was used,\njust with blanks.",
    "start": "413050",
    "end": "416050"
  },
  {
    "text": "And if it had more,\nit was truncated",
    "start": "416050",
    "end": "418389"
  },
  {
    "text": "to use the first 500 words.",
    "start": "418390",
    "end": "419995"
  },
  {
    "text": "OK.",
    "start": "419995",
    "end": "422229"
  },
  {
    "text": "And so a document now\nis represented as a 500",
    "start": "422230",
    "end": "427120"
  },
  {
    "text": "by 10,000 one-hot matrix\nthat sort of indicates",
    "start": "427120",
    "end": "431860"
  },
  {
    "text": "which word appeared\nat which time",
    "start": "431860",
    "end": "434229"
  },
  {
    "text": "in the sequence of length 500.",
    "start": "434230",
    "end": "436810"
  },
  {
    "text": "So this embedding layer is\ngoing to take these 10,000 words",
    "start": "436810",
    "end": "442240"
  },
  {
    "text": "and reduce that to 32.",
    "start": "442240",
    "end": "444550"
  },
  {
    "text": "So then we'll have\nsomething like a 500",
    "start": "444550",
    "end": "446500"
  },
  {
    "text": "by 32 representation\nof a document",
    "start": "446500",
    "end": "449170"
  },
  {
    "text": "as we move along the sequence.",
    "start": "449170",
    "end": "452080"
  },
  {
    "text": "And then we use\nthis LSTM model that",
    "start": "452080",
    "end": "455650"
  },
  {
    "text": "sort of tries to fit a\nsequential model or some kind",
    "start": "455650",
    "end": "459400"
  },
  {
    "text": "of time series model to the\ndata using 32 units, which",
    "start": "459400",
    "end": "464440"
  },
  {
    "text": "we've reduced using the\nembedding layer to reduce it",
    "start": "464440",
    "end": "467410"
  },
  {
    "text": "to.",
    "start": "467410",
    "end": "469070"
  },
  {
    "text": "So that's a rather\ncomplicated thing to describe.",
    "start": "469070",
    "end": "472390"
  },
  {
    "text": "But, actually, it surprisingly\neasy to specify this in code.",
    "start": "472390",
    "end": "477160"
  },
  {
    "text": "OK.",
    "start": "477160",
    "end": "479410"
  },
  {
    "text": "So this is how we're\ngoing to specify",
    "start": "479410",
    "end": "482350"
  },
  {
    "text": "both the embedding and the\nLSTM model in the network.",
    "start": "482350",
    "end": "487760"
  },
  {
    "text": "So input sizes is,\nagain, about 10,000.",
    "start": "487760",
    "end": "490720"
  },
  {
    "text": "So we're going to ask\nfor an embedding layer",
    "start": "490720",
    "end": "493210"
  },
  {
    "text": "of 10,000 input and 32 output.",
    "start": "493210",
    "end": "496490"
  },
  {
    "text": "So this actually is going\nto incur roughly 10,000",
    "start": "496490",
    "end": "500889"
  },
  {
    "text": "times 32 parameters.",
    "start": "500890",
    "end": "502550"
  },
  {
    "text": "So that's going to be about\n320,000 parameters just",
    "start": "502550",
    "end": "505389"
  },
  {
    "text": "for this.",
    "start": "505390",
    "end": "506470"
  },
  {
    "text": "And then we're\ngoing to have this--",
    "start": "506470",
    "end": "508540"
  },
  {
    "text": "once we've reduced\nthe 32 dimension,",
    "start": "508540",
    "end": "510310"
  },
  {
    "text": "we're going to use this\nsequential model, the LSTM",
    "start": "510310",
    "end": "513219"
  },
  {
    "text": "model, to take account\nof the temporal aspect.",
    "start": "513220",
    "end": "518360"
  },
  {
    "text": "And then, finally, we\ntake those 32 units.",
    "start": "518360",
    "end": "520740"
  },
  {
    "text": "And we map to one again because\nwe have a binary classification",
    "start": "520740",
    "end": "523469"
  },
  {
    "text": "task.",
    "start": "523470",
    "end": "524219"
  },
  {
    "text": "And, well, the\nrest of the fitting",
    "start": "524220",
    "end": "526139"
  },
  {
    "text": "is-- once you specify\nthe form of the network",
    "start": "526140",
    "end": "528540"
  },
  {
    "text": "is, again, similar to\nall the other examples.",
    "start": "528540",
    "end": "530820"
  },
  {
    "text": "Let's just jump\nahead and take a look",
    "start": "530820",
    "end": "533430"
  },
  {
    "text": "at how this model performs.",
    "start": "533430",
    "end": "536760"
  },
  {
    "start": "536760",
    "end": "539550"
  },
  {
    "text": "In the end, it gets about\ntest accuracy of 85%.",
    "start": "539550",
    "end": "542769"
  },
  {
    "text": "So pretty comparable\nto the bag of words",
    "start": "542770",
    "end": "544710"
  },
  {
    "text": "and to the Lasso in this case.",
    "start": "544710",
    "end": "547360"
  },
  {
    "text": "So, I guess I see a few\nthings with this example, one",
    "start": "547360",
    "end": "549899"
  },
  {
    "text": "that the torch and\nother packages have--",
    "start": "549900",
    "end": "553320"
  },
  {
    "text": "it's easy to specify\ncomplex models--",
    "start": "553320",
    "end": "555840"
  },
  {
    "text": "this embedding in recurrent\nneural network model.",
    "start": "555840",
    "end": "559320"
  },
  {
    "text": "But they don't\nalways perform better",
    "start": "559320",
    "end": "561630"
  },
  {
    "text": "than linear models or\nother simpler models.",
    "start": "561630",
    "end": "565790"
  },
  {
    "start": "565790",
    "end": "571000"
  }
]