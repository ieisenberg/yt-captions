[
  {
    "start": "0",
    "end": "5720"
  },
  {
    "text": "So today, we'll be talking\nabout model-based reinforcement learning and how it can be\nused for multi-task learning.",
    "start": "5720",
    "end": "12870"
  },
  {
    "text": "So for today, first,\nwe're going to recap goal-conditioned reinforcement\nlearning and relabeling.",
    "start": "12870",
    "end": "18920"
  },
  {
    "text": "Then we'll talk about\nmodel-based reinforcement learning and how it can be\nused for multi-task learning.",
    "start": "18920",
    "end": "25060"
  },
  {
    "text": "And then, we'll talk\nabout model-based RL when you have higher-dimensional\nobservations such as images.",
    "start": "25060",
    "end": "31800"
  },
  {
    "text": "And the goals for today will be\nto allow you to understand how to use and implement model-based\nreinforcement learning methods,",
    "start": "31800",
    "end": "39539"
  },
  {
    "text": "how to use-- how model-based\nreinforcement learning can be used for\nmulti-task learning,",
    "start": "39540",
    "end": "45720"
  },
  {
    "text": "and challenges and\nstrategies for model-based RL when you have high-dimensional\nobservation spaces.",
    "start": "45720",
    "end": "51150"
  },
  {
    "text": " So let's start with the recap.",
    "start": "51150",
    "end": "57890"
  },
  {
    "text": "Last Monday, before the\nproject presentations or before the proposal\npresentations,",
    "start": "57890",
    "end": "62970"
  },
  {
    "text": "we talked about-- first, briefly overviewing\nreinforcement learning.",
    "start": "62970",
    "end": "69380"
  },
  {
    "text": "And in particular,\nCarl introduced model-free\nreinforcement learning methods, including policy\ngradients and Q-learning.",
    "start": "69380",
    "end": "77780"
  },
  {
    "text": " Carl talked about how\nreinforcement learning",
    "start": "77780",
    "end": "83160"
  },
  {
    "text": "algorithms, in general, follow\nthis structure where you first generate samples by\nrunning the policy,",
    "start": "83160",
    "end": "89970"
  },
  {
    "text": "fit some model to estimate the\nreturn that those samples get, and then use that model\nto improve your policy.",
    "start": "89970",
    "end": "97960"
  },
  {
    "text": "We saw two different\nways for doing this. The first was using\npolicy gradients,",
    "start": "97960",
    "end": "103960"
  },
  {
    "text": "which fits some kind\nof a model to estimate the return by simply\nlooking at Monte Carlo",
    "start": "103960",
    "end": "111160"
  },
  {
    "text": "rollouts of your policy and\ncomputing the discounted return from that rollout.",
    "start": "111160",
    "end": "117370"
  },
  {
    "text": "And the second approach\nis to use Q-learning, where you fit a Q-function\nusing the Bellman equation",
    "start": "117370",
    "end": "123730"
  },
  {
    "text": "to estimate your return. Both of these are\ntwo ways that you can use to estimate your return--",
    "start": "123730",
    "end": "129479"
  },
  {
    "text": "one where you just average\nover the trajectories and one where you actually fit\nsomething like a neural network.",
    "start": "129479",
    "end": "135799"
  },
  {
    "text": "And then, once you have\nthis model of the return, you can either use it to\ndirectly update your policy using a policy gradient or\nuse basically your Q function.",
    "start": "135800",
    "end": "147870"
  },
  {
    "text": "Take the maximum over\nactions to get the best action for your policy. ",
    "start": "147870",
    "end": "156270"
  },
  {
    "text": "OK, so then these are two\nmodel-free reinforcement learning methods. And then, later in\nthis lecture, once we",
    "start": "156270",
    "end": "162210"
  },
  {
    "text": "get into the core\nof the lecture, we'll focus on model-based\nreinforcement learning methods. ",
    "start": "162210",
    "end": "168350"
  },
  {
    "text": "We also talked about what is\na reinforcement learning task? We talked about how\nbasically a task corresponds",
    "start": "168350",
    "end": "173810"
  },
  {
    "text": "to a Markov decision process\nwith a state space and action space, an initial state\ndistribution, transition",
    "start": "173810",
    "end": "180050"
  },
  {
    "text": "dynamics, and a reward\nwhere all of these might vary across\ndifferent tasks. ",
    "start": "180050",
    "end": "187270"
  },
  {
    "text": "And you can alternatively view\na multi-task reinforcement learning problem as something\nthat tries to integrate a task",
    "start": "187270",
    "end": "195519"
  },
  {
    "text": "identifier into your\nrepresentation of the state where this s bar is\nthe original state.",
    "start": "195520",
    "end": "203300"
  },
  {
    "text": "So this just puts the\nmulti-task-- the set of MDPs into a single MDP where you\ncan then condition your policy",
    "start": "203300",
    "end": "211540"
  },
  {
    "text": "and condition your Q-function\non the task identifier in order to get a\nmulti-task policy or a multi-task Q-function.",
    "start": "211540",
    "end": "220370"
  },
  {
    "text": "What does the i correspond to? This can correspond\nto a range of things. It can correspond to a\none-hot task identifier,",
    "start": "220370",
    "end": "228440"
  },
  {
    "text": "which is the simplest case. It can correspond to a language\ndescription of the task. This would correspond to an\ninstruction-following setting.",
    "start": "228440",
    "end": "236330"
  },
  {
    "text": "And it can also correspond\nto a desired goal state. So you could have a\nmulti-task RL problem",
    "start": "236330",
    "end": "243490"
  },
  {
    "text": "where different tasks\ncorrespond to trying to reach different states,\ndifferent goal states.",
    "start": "243490",
    "end": "248950"
  },
  {
    "text": "And this is a special case\nof multi-task reinforcement learning that's called goal\nconditioned RL in the sense",
    "start": "248950",
    "end": "255130"
  },
  {
    "text": "that you are going\nto be conditioning your policy and your\nQ-function on the goal",
    "start": "255130",
    "end": "260979"
  },
  {
    "text": "that you want to reach. And you're going to be training\nthis policy and this Q-function to reach goals.",
    "start": "260980",
    "end": "266860"
  },
  {
    "start": "266860",
    "end": "272680"
  },
  {
    "text": "OK, with this, we can look at\nan example of a multi-task RL problem. So this is the example\nthat Carl gave last week",
    "start": "272680",
    "end": "279010"
  },
  {
    "text": "where you have a hockey player. Maybe there's a-- the hockey\nplayer has a teammate as well as someone on the other team\nthat's defending the goal.",
    "start": "279010",
    "end": "286180"
  },
  {
    "text": "And you have two tasks. One task is to pass\nit to the teammate. And another task\nis to shoot goals.",
    "start": "286180",
    "end": "293009"
  },
  {
    "text": "And in this example,\nif you think about the case of a\nsetting where maybe you were trying to shoot a\ngoal, but you accidentally",
    "start": "293010",
    "end": "300060"
  },
  {
    "text": "performed a great\npass to your teammate. In this case, it makes sense\nto actually share that data",
    "start": "300060",
    "end": "307860"
  },
  {
    "text": "and not just use that data for\nthe task of shooting the goal but also use that data for\ntraining your passing policy.",
    "start": "307860",
    "end": "317190"
  },
  {
    "text": "So to do this, you can store\nyour experience as normal, but also relabel your\nexperience and relabel it",
    "start": "317190",
    "end": "325110"
  },
  {
    "text": "with kind of the ID of task\n1, for example, if you're",
    "start": "325110",
    "end": "332409"
  },
  {
    "text": "originally trying to do task 2. So this actually\nshould be task 1.",
    "start": "332410",
    "end": "339060"
  },
  {
    "text": "Say that-- OK. In retrospect,\nlet's say that I was pretending to perform task 1.",
    "start": "339060",
    "end": "344430"
  },
  {
    "text": "I was pretending to pass. I did a great job at passing. You can store that with-- store that experience with\nthe corresponding task",
    "start": "344430",
    "end": "350850"
  },
  {
    "text": "ID and the reward. And allow yourself to learn\nwith that data for both",
    "start": "350850",
    "end": "356370"
  },
  {
    "text": "this new task as well\nas the original task that you were attempting.",
    "start": "356370",
    "end": "361460"
  },
  {
    "text": "This is what's called\nhindsight relabelling. ",
    "start": "361460",
    "end": "367280"
  },
  {
    "text": "It's also referred to hindsight\nexperience replay or HER.",
    "start": "367280",
    "end": "372340"
  },
  {
    "text": "And the way this works\nas an algorithm is you collect data that has the\noriginal states, the actions,",
    "start": "372340",
    "end": "378789"
  },
  {
    "text": "the goal state that you were\nattempting, and the rewards, using some policy. You then store that data\nin your replay buffer.",
    "start": "378790",
    "end": "386360"
  },
  {
    "text": "And then, when you want to\nperform hindsight relabeling, you relabel your experience\nin Dk using the last state",
    "start": "386360",
    "end": "394940"
  },
  {
    "text": "that you reached as the goal. So instead of storing\nit with the goal state that you are attempting, you\ncan replace that goal state",
    "start": "394940",
    "end": "403040"
  },
  {
    "text": "with the state that you\nreached in practice. And this gives you an example\nof a success for that state.",
    "start": "403040",
    "end": "410000"
  },
  {
    "text": "And then, you also\nset the reward to be something like\nthe negative distance between the current state\nand that new goal state.",
    "start": "410000",
    "end": "416240"
  },
  {
    "text": " Once you store this relabeled\ndata in your replay buffer,",
    "start": "416240",
    "end": "421920"
  },
  {
    "text": "you can then update your\npolicy using all of the data that you have, including\nthe relabeled data.",
    "start": "421920",
    "end": "430650"
  },
  {
    "text": "And then, you can iteratively\napply this process. There may be other relabelling\nstrategies as well.",
    "start": "430650",
    "end": "437230"
  },
  {
    "text": "So, in this case, we\nrelabeled with the final state that we reached as the goal. But you could also use any\nstate from the trajectory.",
    "start": "437230",
    "end": "446270"
  },
  {
    "text": "This will allow you to\nalleviate exploration challenges by trying to target the way that\nyou share data across tasks.",
    "start": "446270",
    "end": "452430"
  },
  {
    "text": " OK, is there any\nquestions on this process? ",
    "start": "452430",
    "end": "461360"
  },
  {
    "text": "Hello. Yeah, go ahead. So from this slide, it seems\nthat the final state s of big T",
    "start": "461360",
    "end": "471440"
  },
  {
    "text": "is like your target state. But how can you make\nsure that's the case? Yeah, so basically, when\nyou collected the data,",
    "start": "471440",
    "end": "480680"
  },
  {
    "text": "your goal state was randomly\nsampled, for example, and stored in your data.",
    "start": "480680",
    "end": "485930"
  },
  {
    "text": "And this is saying,\nin retrospect, let's say that my\ngoal was actually the state that I reached.",
    "start": "485930",
    "end": "491840"
  },
  {
    "text": "And store the data\nwith that kind of state that I reached as the goal.",
    "start": "491840",
    "end": "497400"
  },
  {
    "text": "OK. I see. OK. Thanks. ",
    "start": "497400",
    "end": "503130"
  },
  {
    "text": "Any other questions?  So this is the\nalgorithm that you'll",
    "start": "503130",
    "end": "508410"
  },
  {
    "text": "be implementing in homework 3. I have one question. Yeah, go ahead.",
    "start": "508410",
    "end": "514320"
  },
  {
    "text": "That last state that\nis reached is not a good state for the task. Let's say in the case\nof example of the hockey",
    "start": "514320",
    "end": "520860"
  },
  {
    "text": "player, the [INAUDIBLE]\nscores an own goal. So that's not a\ngood state to be in.",
    "start": "520860",
    "end": "528950"
  },
  {
    "text": "But we do not want to reward-- train on that particular kind of\nstrategy because that would be",
    "start": "528950",
    "end": "534980"
  },
  {
    "text": "like [INAUDIBLE] so how do we-- So if that's a goal that\nyou don't want to reach,",
    "start": "534980",
    "end": "541580"
  },
  {
    "text": "note that in this case, we're\nstill passing in the goal to our policy. So we're training our policy\nwith a given S and Sg.",
    "start": "541580",
    "end": "553620"
  },
  {
    "text": "And so, essentially, what\nthis is going to be doing is it's going to be training\nit such that if you wanted to score a goal on\nyourself, for example,",
    "start": "553620",
    "end": "561270"
  },
  {
    "text": "with that particular\ngoal state, then that's the way that you\nshould-- that's kind",
    "start": "561270",
    "end": "566280"
  },
  {
    "text": "of an example of how\nyou could do that. This won't necessarily\nharm the other policies.",
    "start": "566280",
    "end": "573959"
  },
  {
    "text": "It won't tell the other\npolicies for other goals to score a goal on yourself.",
    "start": "573960",
    "end": "580890"
  },
  {
    "text": "It's only going to be rewarding\nthe policy with that goal to--",
    "start": "580890",
    "end": "587473"
  },
  {
    "text": "that that's actually\na good thing to do. With that goal state,\nthat's a good thing to do. So, basically, if ST is\nscoring goal on yourself,",
    "start": "587473",
    "end": "597990"
  },
  {
    "text": "then you'll set that to\nbe the goal state here. And you'll give it a high\nreward for doing that.",
    "start": "597990",
    "end": "603810"
  },
  {
    "text": "But, of course, only\nfor this policy. ",
    "start": "603810",
    "end": "609500"
  },
  {
    "text": "OK. Thank you. ",
    "start": "609500",
    "end": "617050"
  },
  {
    "text": "Do you have a question? Sorry. I was muted. Yeah, I'm listening\n[INAUDIBLE] we",
    "start": "617050",
    "end": "624120"
  },
  {
    "text": "want to be selective\nin terms of what we add to the replay buffer. So is this strictly\ndriven by the reward?",
    "start": "624120",
    "end": "632115"
  },
  {
    "text": "Like at the end of the\nepisode, we look at the reward, and then we think that, oh\nyeah, this is a good result.",
    "start": "632115",
    "end": "640170"
  },
  {
    "text": "And that's what I will\nadd to the replay buffer. Yeah, so the intuition\nhere is that if you choose",
    "start": "640170",
    "end": "647430"
  },
  {
    "text": "the last state or one of the\nstates in the trajectory, that's an example of something\nthat you achieved on accident.",
    "start": "647430",
    "end": "653190"
  },
  {
    "text": "And you can use that kind\nof accidental success as data for--",
    "start": "653190",
    "end": "660240"
  },
  {
    "text": "as if you did want\nto accomplish that. If those kinds of\ngoals are things",
    "start": "660240",
    "end": "665672"
  },
  {
    "text": "that you don't care about, like\nscoring a goal on yourself, for example, you could\nchoose to only relabel",
    "start": "665672",
    "end": "671610"
  },
  {
    "text": "within the set of goals that\nyou care about, for example.  There's also kind of a\ngeneralization of this",
    "start": "671610",
    "end": "678450"
  },
  {
    "text": "where you could, instead of-- in the setting where\nyou're goal-conditioned, if you're in a\nmulti-task setting,",
    "start": "678450",
    "end": "683917"
  },
  {
    "text": "you could see if the\nexperience you collected had a high reward\nfor any of the tasks.",
    "start": "683917",
    "end": "689130"
  },
  {
    "text": "And relabel for tasks that\nit has high rewards for. And this is kind\nof a heuristic that",
    "start": "689130",
    "end": "695400"
  },
  {
    "text": "allows you to store data that is\nsuccessful for different tasks that you weren't\ntrying to accomplish.",
    "start": "695400",
    "end": "701970"
  },
  {
    "text": "You could also store\ndata in other ways. This isn't the only way\nto relabel your data.",
    "start": "701970",
    "end": "708048"
  },
  {
    "text": "This is just one\nheuristic that tends to work well because it stores\nsuccesses of these other tasks.",
    "start": "708048",
    "end": "716790"
  },
  {
    "text": "Does that answer your question? Yeah, [INAUDIBLE] Thanks. ",
    "start": "716790",
    "end": "723550"
  },
  {
    "text": "Yeah, so [MUTED] is asking, this\nallows us to gain a lot of data for potential successes, right?",
    "start": "723550",
    "end": "728920"
  },
  {
    "text": "And that's exactly right.  [MUTED] is asking-- so are we\nadding more tasks by adding",
    "start": "728920",
    "end": "735170"
  },
  {
    "text": "different goal states? And that's also kind\nof generally correct.",
    "start": "735170",
    "end": "742130"
  },
  {
    "text": "Basically, each goal\nstate corresponds to a different task. And each time, you add-- each time, you kind of\nexperience reaching a new goal",
    "start": "742130",
    "end": "749120"
  },
  {
    "text": "state that you hadn't\nseen in your data before that may\nactually correspond to adding the new task. ",
    "start": "749120",
    "end": "756122"
  },
  {
    "text": "[MUTED] is asking, does\nrelabelling significantly slow down the training process? It will increase your\nreplay buffer size.",
    "start": "756122",
    "end": "763430"
  },
  {
    "text": "But it won't\nnecessarily slow down the process of\nlearning in comparison",
    "start": "763430",
    "end": "771400"
  },
  {
    "text": "to goal-conditioned learning. And, actually,\ntypically, it makes--",
    "start": "771400",
    "end": "776589"
  },
  {
    "text": "typically it makes learning\nfaster and more efficient because it actually\nhelps alleviate",
    "start": "776590",
    "end": "782470"
  },
  {
    "text": "exploration challenges by\nstoring data of successes. And then [MUTED] is\nasking, shouldn't we",
    "start": "782470",
    "end": "789630"
  },
  {
    "text": "constrain that if ST is one\nof the goal states for one of the tasks that we\nwant to perform, and then only perform this augmentation?",
    "start": "789630",
    "end": "796520"
  },
  {
    "text": "Yeah, so if you have a set\nof tasks that you really-- that are the ones that\nyou care most about,",
    "start": "796520",
    "end": "803100"
  },
  {
    "text": "then you could choose to relabel\nonly for those sets of tasks. So basically, have\na rule here that",
    "start": "803100",
    "end": "809130"
  },
  {
    "text": "says if ST is in my\nset of goal states.",
    "start": "809130",
    "end": "815415"
  },
  {
    "text": " One of the reasons that we\nmay not want to do this--",
    "start": "815415",
    "end": "821090"
  },
  {
    "text": "I think this could be a very\nreasonable way to approach things. And one reason where\nit may make sense",
    "start": "821090",
    "end": "826339"
  },
  {
    "text": "to actually relabel\nwith any possible state is that it may actually help the\nmodel learn about general ways",
    "start": "826340",
    "end": "834770"
  },
  {
    "text": "to reach states. And that general\nlearning process about how to reach\ndifferent states",
    "start": "834770",
    "end": "840560"
  },
  {
    "text": "may be useful for the\nweights of the model to learn how to accomplish\ndifferent kinds of goals.",
    "start": "840560",
    "end": "846370"
  },
  {
    "text": " OK.",
    "start": "846370",
    "end": "852390"
  },
  {
    "text": "Great. And then, also, with regard\nto-- going back to the question about it slowing\ndown training, you'll also get an opportunity\nto experiment",
    "start": "852390",
    "end": "858290"
  },
  {
    "text": "with this in your homework. So you'll be able to\nunderstand exactly what the ramifications are.",
    "start": "858290",
    "end": "863660"
  },
  {
    "text": " OK.",
    "start": "863660",
    "end": "869180"
  },
  {
    "text": "And then, how does this\nactually work in practice? So the 2017 paper that\nintroduced this concept that",
    "start": "869180",
    "end": "877760"
  },
  {
    "text": "was actually also somewhat\nintroduced in this 1993 paper by Leslie Kaelbling,\nit experimented--",
    "start": "877760",
    "end": "883310"
  },
  {
    "text": "the 2017 paper experimented\nwith the simulated robotic manipulation task where\ntasks corresponded",
    "start": "883310",
    "end": "889130"
  },
  {
    "text": "to pushing, sliding,\nand pick-and-place of objects on a table.",
    "start": "889130",
    "end": "895310"
  },
  {
    "text": "And what they find is that\nwhen you use relabeling, which is in the blue and\nthe red curve here,",
    "start": "895310",
    "end": "900680"
  },
  {
    "text": "they have two different\nversions of it. They find that you're able to\nget pretty substantial benefits",
    "start": "900680",
    "end": "905690"
  },
  {
    "text": "over either count-based\nexploration or just a vanilla version of the algorithm,\nwhich is kind of evidence",
    "start": "905690",
    "end": "913130"
  },
  {
    "text": "for how well or how this\ncan alleviate challenges with exploration. ",
    "start": "913130",
    "end": "921840"
  },
  {
    "text": "OK. Great. So that's a brief recap of what\nwas covered on Monday last week",
    "start": "921840",
    "end": "927948"
  },
  {
    "text": "with regard to\nreinforcement learning and goal-conditioned\nreinforcement learning. Now we'll get to the kind\nof core part of this lecture",
    "start": "927948",
    "end": "934470"
  },
  {
    "text": "on model-based\nreinforcement learning and how it can be used\nfor multi-task learning. ",
    "start": "934470",
    "end": "940949"
  },
  {
    "text": "So the main idea behind\nmodel-based reinforcement learning is that you're\ngoing to learn a model",
    "start": "940950",
    "end": "947459"
  },
  {
    "text": "of the environment dynamics. And the reason why\nyou might do this",
    "start": "947460",
    "end": "953649"
  },
  {
    "text": "is that it often leads\nto better data efficiency because if you learn a\nmodel of the environment,",
    "start": "953650",
    "end": "959500"
  },
  {
    "text": "you can then actually leverage\nthat model when figuring out what to do rather\nthan collecting data",
    "start": "959500",
    "end": "964660"
  },
  {
    "text": "from the real environment. And this allows the model\nto be easily reused.",
    "start": "964660",
    "end": "970860"
  },
  {
    "text": "And as we'll talk about\nin a couple of slides, the model can actually be\nreused across different tasks",
    "start": "970860",
    "end": "975899"
  },
  {
    "text": "in many different contexts.  The way that it looks like\nin the context of this loop",
    "start": "975900",
    "end": "983490"
  },
  {
    "text": "that we talked about before\nis when we're fitting a model, we are estimating the\ntransition dynamics",
    "start": "983490",
    "end": "989970"
  },
  {
    "text": "using some neural network. In the slides,\nI'll denote this as a deterministic\nneural network that",
    "start": "989970",
    "end": "995579"
  },
  {
    "text": "predicts s prime from s and a. But in practice, this\nnetwork may actually be stochastic in order\nto capture stochasticity",
    "start": "995580",
    "end": "1003920"
  },
  {
    "text": "in the real world. And this just comes down to\na supervised learning problem",
    "start": "1003920",
    "end": "1010680"
  },
  {
    "text": "where you want to\nminimize the error between your predicted next\nstate and the true next state.",
    "start": "1010680",
    "end": "1016920"
  },
  {
    "text": " Then once you learn\nthis model, you",
    "start": "1016920",
    "end": "1023700"
  },
  {
    "text": "can then optimize for\nbehavior or for a policy using that model.",
    "start": "1023700",
    "end": "1028750"
  },
  {
    "text": "And we'll talk about\nexactly how that happens in a couple of slides.",
    "start": "1028750",
    "end": "1033819"
  },
  {
    "text": "But at a high level,\nwe're going to be fitting a model of the environment\nand the dynamics and then using that model\nto improve the policy.",
    "start": "1033819",
    "end": "1039810"
  },
  {
    "text": " OK, so what does this have to\ndo with multi-task reinforcement",
    "start": "1039810",
    "end": "1045310"
  },
  {
    "text": "learning? Before, we talked about\nmulti-task reinforcement learning as having potentially\ndifferent dynamics,",
    "start": "1045310",
    "end": "1051545"
  },
  {
    "text": "different reward functions,\ndifferent states, and different action spaces. And one observation here\nis that in many situations,",
    "start": "1051545",
    "end": "1059909"
  },
  {
    "text": "the only thing that\nvaries across tasks is just the reward function.",
    "start": "1059910",
    "end": "1065118"
  },
  {
    "text": "And in particular,\ntoday, we're going to focus exactly\non the special case where the state space, the\naction space, the initial state",
    "start": "1065118",
    "end": "1070920"
  },
  {
    "text": "distribution, and the dynamics\nare the same across all of the tasks. And only the reward function\nvaries with the task.",
    "start": "1070920",
    "end": "1082360"
  },
  {
    "text": "And a few examples of\nwhere this may come up-- one example is robotics,\nwhere the laws of physics",
    "start": "1082360",
    "end": "1088570"
  },
  {
    "text": "are staying constant\nacross tasks. But you want the robot to\ncomplete different objectives.",
    "start": "1088570",
    "end": "1094042"
  },
  {
    "text": "So maybe you want\nit to do laundry. Maybe you want it to be able\nto cook for you and so forth. In this case, the dynamics\nwill often be the same.",
    "start": "1094042",
    "end": "1101242"
  },
  {
    "text": "And the only thing\nthat's different is the reward\nfunction that you're giving it for doing\nthese different tasks.",
    "start": "1101242",
    "end": "1108120"
  },
  {
    "text": "Another example is in\na task-driven dialogue system where maybe the user\nthat the dialogue system is",
    "start": "1108120",
    "end": "1116269"
  },
  {
    "text": "interacting with is\nconstant, and the language is also the same. But the user has\ndifferent objectives",
    "start": "1116270",
    "end": "1122540"
  },
  {
    "text": "that it wants this dialogue\nagent to be able to complete in different scenarios. Maybe in one scenario, the\nuser wants the dialogue agent",
    "start": "1122540",
    "end": "1131929"
  },
  {
    "text": "to order food. In another scenario, it wants\nit to provide emotional support, and so forth. ",
    "start": "1131930",
    "end": "1139820"
  },
  {
    "text": "And then, one third example\nwhere this special case also holds is in autonomous driving.",
    "start": "1139820",
    "end": "1147560"
  },
  {
    "text": "For example, maybe\nthe rules of the road stay constant over time. You're always in\nthe same country",
    "start": "1147560",
    "end": "1152890"
  },
  {
    "text": "or in the same kind of\nregion of the world. But what might be varying is\ndifferent user preferences.",
    "start": "1152890",
    "end": "1159770"
  },
  {
    "text": "So different tasks\nwould correspond to driving for different people. And one person may want\nthe car to go very fast.",
    "start": "1159770",
    "end": "1166470"
  },
  {
    "text": "Another person may want the\ncar to drive very smoothly. ",
    "start": "1166470",
    "end": "1172350"
  },
  {
    "text": "OK. So this is the special case\nthat we'll focus on today. And in future\nlectures, we will also",
    "start": "1172350",
    "end": "1177679"
  },
  {
    "text": "be considering scenarios\nwhere things like the dynamics may be changing over time. ",
    "start": "1177680",
    "end": "1185170"
  },
  {
    "text": "Now, one thing that's nice\nabout this special case is that now,\nestimating the model",
    "start": "1185170",
    "end": "1190650"
  },
  {
    "text": "is just a single-task problem. You just need to learn a\nsingle model of the world",
    "start": "1190650",
    "end": "1196950"
  },
  {
    "text": "because that's constant, and\nonly the reward is changing. So that means that we\ncan kind of readily",
    "start": "1196950",
    "end": "1202530"
  },
  {
    "text": "apply several standard\nmodel-based single-task learning methods for these\nmulti-task reinforcement",
    "start": "1202530",
    "end": "1212365"
  },
  {
    "text": "learning cases.  OK. So how do these--",
    "start": "1212365",
    "end": "1220380"
  },
  {
    "text": "how do model-based\nRL algorithms work? And how do they optimize\nthese objectives? So the first approach\nthat we'll consider",
    "start": "1220380",
    "end": "1226440"
  },
  {
    "text": "is optimizing over the\nactions using the model. There's a few different\nways that we could do this.",
    "start": "1226440",
    "end": "1233535"
  },
  {
    "text": "This is also referred to\nas planning in the sense that you're planning\na course of action",
    "start": "1233535",
    "end": "1239630"
  },
  {
    "text": "using the model and the task\nthat you want to complete. ",
    "start": "1239630",
    "end": "1245320"
  },
  {
    "text": "The first way that\nyou could imagine doing this to optimize\nover these actions",
    "start": "1245320",
    "end": "1251250"
  },
  {
    "text": "is just with backpropagation. So you could take your reward\nfunction and backpropagate gradients through your model\ninto your sequence of actions.",
    "start": "1251250",
    "end": "1260730"
  },
  {
    "text": "And optimize over a sequence of\nactions with gradient descent that lead to a high\nsum of rewards.",
    "start": "1260730",
    "end": "1267960"
  },
  {
    "text": " So this could be with a\ngradient-based optimization.",
    "start": "1267960",
    "end": "1274850"
  },
  {
    "text": "And the way that\nthis algorithm works is you first run some\npolicy to collect data.",
    "start": "1274850",
    "end": "1281010"
  },
  {
    "text": "You then learn a model to\nminimize your model error.",
    "start": "1281010",
    "end": "1286800"
  },
  {
    "text": "And then, you backpropagate\nthrough that model to choose a sequence of actions\na1 through aT, for example.",
    "start": "1286800",
    "end": "1297095"
  },
  {
    "text": " And then you execute\nthose actions. ",
    "start": "1297095",
    "end": "1305840"
  },
  {
    "text": "And you can backpropagate\nto just choose those actions directly and\nexecute those actions directly.",
    "start": "1305840",
    "end": "1312160"
  },
  {
    "text": "Or you could pass those\ngradients into a policy and optimize over the\nparameters of this policy.",
    "start": "1312160",
    "end": "1317480"
  },
  {
    "text": " Any questions on how this\nworks at a high level?",
    "start": "1317480",
    "end": "1323200"
  },
  {
    "start": "1323200",
    "end": "1333179"
  },
  {
    "text": "[INAUDIBLE] a question\nabout steps two and three. So we're using the\nsame trajectories",
    "start": "1333180",
    "end": "1339540"
  },
  {
    "text": "to minimize the model\nof the environment as well as the policy?",
    "start": "1339540",
    "end": "1347019"
  },
  {
    "text": "Yeah, this is a good question. So we will-- ",
    "start": "1347020",
    "end": "1352540"
  },
  {
    "text": "I guess-- so in this\ncase, we're using-- this is going to be using\nyour data in this step.",
    "start": "1352540",
    "end": "1361450"
  },
  {
    "text": "It depends on how you optimized. But you don't actually\nnecessarily need data for this step.",
    "start": "1361450",
    "end": "1367040"
  },
  {
    "text": "So, for example, say\nthe case that you're not-- you're just optimizing\nover a sequence of actions a1",
    "start": "1367040",
    "end": "1374080"
  },
  {
    "text": "through T. What you can do\nis you can just randomly initialize these actions.",
    "start": "1374080",
    "end": "1379240"
  },
  {
    "text": "And then pass them\nthrough the model. And kind of iteratively-- maybe this is kind of\nthe first iteration--",
    "start": "1379240",
    "end": "1386290"
  },
  {
    "text": "iteratively update and optimize\nthe sequence of actions such that it maximizes the\nreward or the sum of rewards.",
    "start": "1386290",
    "end": "1395101"
  },
  {
    "text": "And to do this optimization,\nyou don't actually need to optimize it over data\nbecause you're optimizing over the action sequences.",
    "start": "1395102",
    "end": "1401900"
  },
  {
    "text": "And you can actually\ngenerate kind of fake data from the model itself. ",
    "start": "1401900",
    "end": "1410500"
  },
  {
    "text": "OK. [MUTED] is asking, does it\nrequire a continuous action space for\nbackpropagation to work?",
    "start": "1410500",
    "end": "1416800"
  },
  {
    "text": "The short answer is yes. Although, there are\nalso gradient estimators",
    "start": "1416800",
    "end": "1422700"
  },
  {
    "text": "that can approximate gradients\nthrough discrete things. This includes things like the\nstraight-through estimator,",
    "start": "1422700",
    "end": "1429030"
  },
  {
    "text": "Gumbel-Softmax, et cetera. ",
    "start": "1429030",
    "end": "1435040"
  },
  {
    "text": "Do you have a question? Yes. So I just want to clarify\nthat in step three,",
    "start": "1435040",
    "end": "1441720"
  },
  {
    "text": "we do not run simulations over-- run simulations that interact\nwith the environment.",
    "start": "1441720",
    "end": "1448980"
  },
  {
    "text": "We only use these\nlearned f actions. Correct. So step three isn't involving\nthe environment in any way.",
    "start": "1448980",
    "end": "1456540"
  },
  {
    "text": "You're just kind of involving\nyour learned model, which can be viewed as a learned\nmodel of the environment, which",
    "start": "1456540",
    "end": "1461640"
  },
  {
    "text": "you're using to optimize\nit for your actions. Thank you. ",
    "start": "1461640",
    "end": "1470690"
  },
  {
    "text": "OK.  Then another approach\nthat we could use,",
    "start": "1470690",
    "end": "1477010"
  },
  {
    "text": "instead of a gradient-based\noptimization over actions, is to use sampling, which will\nessentially just correspond",
    "start": "1477010",
    "end": "1484960"
  },
  {
    "text": "to a gradient-free\noptimization and can actually work surprisingly\nwell in some cases.",
    "start": "1484960",
    "end": "1492100"
  },
  {
    "text": "And intuition for why\ngradient-free optimization might make sense is that\nthe sequence of actions",
    "start": "1492100",
    "end": "1498820"
  },
  {
    "text": "may be fairly\nlow-dimensional, certainly lower-dimensional than the size\nof a neural network parameter",
    "start": "1498820",
    "end": "1505480"
  },
  {
    "text": "vector. And the way that this works\nis the first two steps",
    "start": "1505480",
    "end": "1510539"
  },
  {
    "text": "are the same. You collect some data. You train a model. And the third step is\na little bit different.",
    "start": "1510540",
    "end": "1517630"
  },
  {
    "text": "So instead of doing a\ngradient-based optimization, you instead iteratively sample\na large number of action",
    "start": "1517630",
    "end": "1524399"
  },
  {
    "text": "sequences, run those action\nsequences through the model, and either pick the model--",
    "start": "1524400",
    "end": "1531120"
  },
  {
    "text": "either pick the action sequence\nthat led to the highest reward or take the top k%\nof action sequences.",
    "start": "1531120",
    "end": "1541410"
  },
  {
    "text": "And then kind of\niteratively refine that set of action sequences.",
    "start": "1541410",
    "end": "1548970"
  },
  {
    "text": "Where, for example, you\ncan pick the top k%, fit a new distribution\nto that top k%,",
    "start": "1548970",
    "end": "1555940"
  },
  {
    "text": "resample from that\ndistribution, and allow yourself to iteratively get\na better and better sequence",
    "start": "1555940",
    "end": "1562855"
  },
  {
    "text": "of actions.  So there's a variety of\ngradient-free optimization",
    "start": "1562855",
    "end": "1567990"
  },
  {
    "text": "approaches that you can use. One is what's called random\nshooting, which is simply",
    "start": "1567990",
    "end": "1575190"
  },
  {
    "text": "where you just randomly sample\na bunch of action sequences and kind of shoot\nthem out, which",
    "start": "1575190",
    "end": "1581480"
  },
  {
    "text": "is why it's called shooting. And then pick the one\nthat works the most. Another is called\nthe cross entropy",
    "start": "1581480",
    "end": "1587450"
  },
  {
    "text": "method, which may sound\na little bit confusing.",
    "start": "1587450",
    "end": "1593510"
  },
  {
    "text": "It doesn't really\nhave much to do with the cross-entropy loss,\nwhich is the iterative approach",
    "start": "1593510",
    "end": "1600160"
  },
  {
    "text": "that I mentioned before. And there's also things\nlike evolutionary methods,",
    "start": "1600160",
    "end": "1611700"
  },
  {
    "text": "which are other forms of\ngradient-free optimizations. One thing that is\nnice about this",
    "start": "1611700",
    "end": "1618440"
  },
  {
    "text": "is it's pretty easy\nto apply these methods to discrete action spaces. Although it also actually\nworks pretty well",
    "start": "1618440",
    "end": "1624720"
  },
  {
    "text": "for short-horizon\ncontinuous action spaces. ",
    "start": "1624720",
    "end": "1632200"
  },
  {
    "text": "OK. So now, this is kind of the one.",
    "start": "1632200",
    "end": "1639200"
  },
  {
    "text": "How can this approach fail? Does anyone have thoughts\non what might go wrong",
    "start": "1639200",
    "end": "1644780"
  },
  {
    "text": "with this approach? Feel free to either raise\nyour hand or answer in chat. ",
    "start": "1644780",
    "end": "1653260"
  },
  {
    "text": "Any guesses?  Yeah, so there's one comment\nfrom [MUTED] which says",
    "start": "1653260",
    "end": "1659650"
  },
  {
    "text": "that there's no exploration. So the model might always choose\nfrom a small set of actions",
    "start": "1659650",
    "end": "1665200"
  },
  {
    "text": "that may not be very good-- mentions that it ignores future\nrewards after the horizon.",
    "start": "1665200",
    "end": "1672315"
  },
  {
    "text": "So yeah, each of\nthese approaches we're only optimizing\nfor a fixed horizon of--",
    "start": "1672315",
    "end": "1678519"
  },
  {
    "text": "kind of a fixed horizon\nfrom 1 to T, for example. And if you have a very\nlong-horizon problem,",
    "start": "1678520",
    "end": "1683840"
  },
  {
    "text": "this may not work well. [MUTED] is mentioning that\nif the model is inaccurate, you're going to have\ncompounding errors because you",
    "start": "1683840",
    "end": "1690830"
  },
  {
    "text": "need to predict multiple\ntimesteps into the future. And if it starts-- if you get a small\nerror from the model",
    "start": "1690830",
    "end": "1697375"
  },
  {
    "text": "and it starts to\nkind of fall off the manifold of what\nit's seen before, then it's going to make\nbigger and bigger errors",
    "start": "1697375",
    "end": "1702860"
  },
  {
    "text": "and be actually quite inaccurate\nas you have longer horizons. ",
    "start": "1702860",
    "end": "1709039"
  },
  {
    "text": "[MUTED] is hypothesizing\nthat maybe there are problems with inaccuracies in the data. ",
    "start": "1709040",
    "end": "1716039"
  },
  {
    "text": "Cool. So all of these are--\nall these good answers. ",
    "start": "1716040",
    "end": "1722590"
  },
  {
    "text": "The main thing that I\nactually have on the slides is Jerry's point, which is\nthat the action optimization--",
    "start": "1722590",
    "end": "1730685"
  },
  {
    "text": "if you have an inaccurate\nmodel, especially when you have compounding errors over time,\nthis optimization over actions",
    "start": "1730685",
    "end": "1737440"
  },
  {
    "text": "will exploit any\nimprecision in the model. So if the model is erroneously\noptimistic about one set",
    "start": "1737440",
    "end": "1744670"
  },
  {
    "text": "of action sequences\nleading to high reward, then it's going to\ntry to actually find",
    "start": "1744670",
    "end": "1750399"
  },
  {
    "text": "the actions in which\nthe model was inaccurate because the model is telling you\nthat that leads to high reward.",
    "start": "1750400",
    "end": "1756399"
  },
  {
    "text": " Now, one really easy way\nto fix this to some degree",
    "start": "1756400",
    "end": "1763600"
  },
  {
    "text": "is to refit them\nall using new data. So this will specifically\nhappen when--",
    "start": "1763600",
    "end": "1769870"
  },
  {
    "text": "the model, specifically,\nwill be imprecise when it's off of\nthe manifold of data that it's seen before,\nduring training.",
    "start": "1769870",
    "end": "1777445"
  },
  {
    "text": "And what we can\ndo is we can then collect new data from the action\nsequences that we optimized,",
    "start": "1777445",
    "end": "1783769"
  },
  {
    "text": "which will precisely\nbe in the situations where our model\nmay be inaccurate. And use that data\nto retrain our model",
    "start": "1783770",
    "end": "1789830"
  },
  {
    "text": "to be accurate in those regions. So we can basically just add a\nfourth step to this algorithm",
    "start": "1789830",
    "end": "1797090"
  },
  {
    "text": "where we actually execute\nthe planned actions and then append the visiting\ndata to our dataset.",
    "start": "1797090",
    "end": "1803330"
  },
  {
    "text": "And then retrain our model\nusing all of the data that we've seen so far. ",
    "start": "1803330",
    "end": "1812200"
  },
  {
    "text": "So this is kind of the full\nversion one of the algorithm.",
    "start": "1812200",
    "end": "1818023"
  },
  {
    "text": " Even in this case, retraining\nthe model using new data",
    "start": "1818024",
    "end": "1827380"
  },
  {
    "text": "will definitely help with\nthese sorts of imprecisions. But in the general case,\nlearning a good global model",
    "start": "1827380",
    "end": "1833770"
  },
  {
    "text": "is pretty difficult. There are a few different ways\nfor tackling this problem.",
    "start": "1833770",
    "end": "1841130"
  },
  {
    "text": "One additional way\nthat we can use to tackle this problem is\nwhenever we find ourselves",
    "start": "1841130",
    "end": "1846250"
  },
  {
    "text": "in a new state, instead of\njust keep on executing the plan that you had come on-- that\nyou had derived before,",
    "start": "1846250",
    "end": "1853360"
  },
  {
    "text": "actually, replan using your\nmodel at that new state. And this can allow you to\npotentially kind of correct",
    "start": "1853360",
    "end": "1859390"
  },
  {
    "text": "for errors that you made\nat that previous timestep. And this is what's called\nmodel-predictive control",
    "start": "1859390",
    "end": "1865809"
  },
  {
    "text": "or MPC, which is a\nbit of a fancy name. But the gist of what it\nmeans is to plan and then",
    "start": "1865810",
    "end": "1872200"
  },
  {
    "text": "replan using your model. So the way this looks\nlike is you first run",
    "start": "1872200",
    "end": "1877840"
  },
  {
    "text": "some initial policy\nto collect data, learn a model using that data,\nuse your model to optimize",
    "start": "1877840",
    "end": "1883360"
  },
  {
    "text": "different action sequence. All of these first three\nsteps are the same as before. Now, what we're\ngoing to do is we're",
    "start": "1883360",
    "end": "1889360"
  },
  {
    "text": "going to execute the\nfirst planned action. So, in this case, maybe\nwe're at timestep t.",
    "start": "1889360",
    "end": "1896020"
  },
  {
    "text": "We optimized for\nan action sequence from at to at plus\nH. And here, we're",
    "start": "1896020",
    "end": "1904900"
  },
  {
    "text": "going to execute only at,\nobserve the next state,",
    "start": "1904900",
    "end": "1910330"
  },
  {
    "text": "append this to our\ndata, and then we're actually going to kind of\nreplan at the next time step. That will be at--\nkind of replanning",
    "start": "1910330",
    "end": "1917890"
  },
  {
    "text": "at plus 1 through at plus 1\nplus H. I'll write it down here.",
    "start": "1917890",
    "end": "1925400"
  },
  {
    "text": " And then, again, executing\nthe first planned action.",
    "start": "1925400",
    "end": "1932570"
  },
  {
    "text": " And, of course,\nyou can also still have this outer loop\nthat retrains your model",
    "start": "1932570",
    "end": "1939230"
  },
  {
    "text": "as you see more and more data. And essentially, what\nthis allows you to do",
    "start": "1939230",
    "end": "1945130"
  },
  {
    "text": "is you start making a wrong\nturn and see yourself in a state that you didn't\nexpect to be in, then",
    "start": "1945130",
    "end": "1951220"
  },
  {
    "text": "you can correct yourself\nat the next timestep. And this can actually help\na lot with model errors,",
    "start": "1951220",
    "end": "1958300"
  },
  {
    "text": "including compounding errors. The downside is\nthat it's actually pretty compute-intensive because\nyou need to run the planning",
    "start": "1958300",
    "end": "1965410"
  },
  {
    "text": "process much more\nfrequently than just at the very beginning\nof the trajectory.",
    "start": "1965410",
    "end": "1975019"
  },
  {
    "text": "So this is one approach that\ncan help with model errors. There's a number\nof other approaches as well that we won't\nhave time to get into.",
    "start": "1975020",
    "end": "1981130"
  },
  {
    "text": "This includes things like\nuncertainty estimation, where you try to measure the\nuncertainty of your model",
    "start": "1981130",
    "end": "1987700"
  },
  {
    "text": "and be conservative with regard\nto your model's uncertainty. There's also approaches that\ntry to use only a local model",
    "start": "1987700",
    "end": "1994600"
  },
  {
    "text": "and constrain the action\noptimization to be close to where the\nmodel is accurate.",
    "start": "1994600",
    "end": "2001071"
  },
  {
    "text": " OK.",
    "start": "2001071",
    "end": "2006970"
  },
  {
    "text": "So that's the gist of how\nmodel-based reinforcement learning algorithms work. What does this have to do\nwith multi-task reinforcement",
    "start": "2006970",
    "end": "2012910"
  },
  {
    "text": "learning?  How you actually apply this\nto a multi-task RL problem",
    "start": "2012910",
    "end": "2017980"
  },
  {
    "text": "depends on the form of\nyour reward function. So if you know exactly--",
    "start": "2017980",
    "end": "2023800"
  },
  {
    "text": "if you know what the\nreward function is, if you know how to\nevaluate it, then",
    "start": "2023800",
    "end": "2030730"
  },
  {
    "text": "you can just learn a single\nmodel of the dynamics. And then, when you run the\nplanning procedure to optimize",
    "start": "2030730",
    "end": "2035770"
  },
  {
    "text": "over your actions, just whenever\nyou do that planning process, you consider the reward\nfunction that you care about",
    "start": "2035770",
    "end": "2042669"
  },
  {
    "text": "at test time. And plan with respect\nto that reward function. ",
    "start": "2042670",
    "end": "2048649"
  },
  {
    "text": "So, for example, this is\nactually a recent paper from a year ago where\nreward functions",
    "start": "2048650",
    "end": "2055629"
  },
  {
    "text": "correspond to different\npencil trajectories. And it's basically able to\nreplan with a single model",
    "start": "2055630",
    "end": "2062510"
  },
  {
    "text": "to write different digits. So here is like a digit-- I think a 5. I think there's a 4 here,\na 7 here, and so forth.",
    "start": "2062510",
    "end": "2071119"
  },
  {
    "text": "So it can use its model\nand plan with regard to these different tasks of\nwriting different digits.",
    "start": "2071120",
    "end": "2079330"
  },
  {
    "text": "Another example is\nthe different tasks they correspond to trying\nto manipulate balls",
    "start": "2079330",
    "end": "2085780"
  },
  {
    "text": "in the palm of a hand. I think they're called\nBaoding balls in this case.",
    "start": "2085780",
    "end": "2091090"
  },
  {
    "text": "And, for example,\nthis first task may be to rotate them\ncounterclockwise.",
    "start": "2091090",
    "end": "2096940"
  },
  {
    "text": "A second task might\nbe to simply just put this ball in a certain position\nas indicated by the red dot.",
    "start": "2096940",
    "end": "2103900"
  },
  {
    "text": "It's a little bit hard to see. And another task might\nbe to rotate the Baoding balls clockwise, which\nthe model is also",
    "start": "2103900",
    "end": "2111662"
  },
  {
    "text": "able to figure out how to do.  And one of the pretty\ncool things about this--",
    "start": "2111662",
    "end": "2119220"
  },
  {
    "text": "actually, before I go\ninto that, one caveat here is that the reward\nfunction will actually change how you collect the data.",
    "start": "2119220",
    "end": "2125619"
  },
  {
    "text": "So if you have that outer loop\nthat's actually collecting-- that's telling you\nto collect data--",
    "start": "2125620",
    "end": "2131175"
  },
  {
    "text": "basically collecting data based\non the actions that you select, then you'll want\nto be collecting data for these different\nreward functions",
    "start": "2131175",
    "end": "2138180"
  },
  {
    "text": "rather than just collecting data\nfor a single reward function and trying to generalize\nto a new reward function.",
    "start": "2138180",
    "end": "2143460"
  },
  {
    "text": " One last note is that these\nalgorithms are actually",
    "start": "2143460",
    "end": "2150620"
  },
  {
    "text": "quite sample-efficient. And they're actually\nefficient enough to be able to train for\nthis task on a real robot.",
    "start": "2150620",
    "end": "2158750"
  },
  {
    "text": "I think that it took on the\norder of a couple of hours to learn how to\nmanipulate these Baoding",
    "start": "2158750",
    "end": "2165630"
  },
  {
    "text": "balls in the real world. ",
    "start": "2165630",
    "end": "2172319"
  },
  {
    "text": "OK. So that's the case where we\nknow the reward function. In this case, the\nreward function was just, for example, the\nposition between the goal",
    "start": "2172320",
    "end": "2180058"
  },
  {
    "text": "position-- the negative\ndistance between the goal position of each of the\nballs and the true position of the balls or the distance\nbetween the trajectory that was",
    "start": "2180058",
    "end": "2189660"
  },
  {
    "text": "drawn and the goal trajectory. But in other cases,\nyou don't necessarily",
    "start": "2189660",
    "end": "2195570"
  },
  {
    "text": "know exactly what the form\nof the reward function is for each of the tasks. And so you might need to\nactually learn a model",
    "start": "2195570",
    "end": "2202720"
  },
  {
    "text": "of the reward function,\nwhich you can just learn with multi-task reinforcement\nlearning or sorry with multi-task--",
    "start": "2202720",
    "end": "2208640"
  },
  {
    "text": "it's the standard\nmulti-task learning. Or you could also use\nmeta-learning to meta-learn",
    "start": "2208640",
    "end": "2214990"
  },
  {
    "text": "a reward function with\na little bit of data with a few examples\nof what that task is.",
    "start": "2214990",
    "end": "2220789"
  },
  {
    "text": "And then, once you get this\nkind of parametric form of the reward, you\ncan use that to plan.",
    "start": "2220790",
    "end": "2226360"
  },
  {
    "text": "So as an example of\nwhat this looks like, say you want the robot\nto put this pencil case",
    "start": "2226360",
    "end": "2235060"
  },
  {
    "text": "above or behind the notebook. We could give it a\ntraining dataset that",
    "start": "2235060",
    "end": "2240130"
  },
  {
    "text": "has a few positive examples. And then use this dataset to\nacquire a success classifier",
    "start": "2240130",
    "end": "2249097"
  },
  {
    "text": "or basically a binary\nreward function that's 1 when it's behind\nand 0 otherwise.",
    "start": "2249097",
    "end": "2255910"
  },
  {
    "text": "And then, once you\nadapt your model using the training\ndataset, you can then",
    "start": "2255910",
    "end": "2261630"
  },
  {
    "text": "plan using your model\nto maximize reward. And this is what kind of\nthe executed trajectory",
    "start": "2261630",
    "end": "2267900"
  },
  {
    "text": "looks like after\nyou plan with regard to the task of\npushing the pencil",
    "start": "2267900",
    "end": "2273000"
  },
  {
    "text": "case behind the notebook.  OK.",
    "start": "2273000",
    "end": "2278840"
  },
  {
    "text": "So any questions\non model-based RL for these multi-task\nlearning problems? ",
    "start": "2278840",
    "end": "2291550"
  },
  {
    "text": "So can you just go\nover what you mean by the form of the rewards?",
    "start": "2291550",
    "end": "2297100"
  },
  {
    "text": "Yeah, so what I mean by\nthat is, do you basically have an equation that\ntells you like what",
    "start": "2297100",
    "end": "2304450"
  },
  {
    "text": "r is as a function of s and a? And so in the case of writing--",
    "start": "2304450",
    "end": "2311395"
  },
  {
    "text": "in the case of writing\na digit or trying to manipulate a ball\nto a certain location,",
    "start": "2311395",
    "end": "2317260"
  },
  {
    "text": "the reward function\nmay kind of be something that equals\nthe negative distance",
    "start": "2317260",
    "end": "2323140"
  },
  {
    "text": "between the state and the\ngoal state for that ball.",
    "start": "2323140",
    "end": "2329380"
  },
  {
    "text": "And if you have this\nequation, then you can just learn a model and plan\nwith respect to this equation.",
    "start": "2329380",
    "end": "2337280"
  },
  {
    "text": "If you don't have this\nequation, for example, if your states are images\nand your reward is like 1",
    "start": "2337280",
    "end": "2344650"
  },
  {
    "text": "if you were successful and\n0 if you weren't successful, then you need to actually\nbuild a optimizer with actions.",
    "start": "2344650",
    "end": "2352720"
  },
  {
    "text": "You need some equation\nthat gives you the reward from the states. And if you don't\nhave that equation,",
    "start": "2352720",
    "end": "2358550"
  },
  {
    "text": "then you can basically\ntrain a neural network to tell you what\nthe reward is given a state and action and your\ntask identifier or your training",
    "start": "2358550",
    "end": "2367510"
  },
  {
    "text": "data. OK. So it's just\nbasically whether you have an explicit formulation\nof the reward versus having",
    "start": "2367510",
    "end": "2373372"
  },
  {
    "text": "to learn it. OK. Thank you. Exactly. Yeah.  [MUTED] asking, do we\nneed to do reward shaping?",
    "start": "2373372",
    "end": "2381760"
  },
  {
    "text": "This is a good question. So model-based reinforcement\nlearning methods typically",
    "start": "2381760",
    "end": "2388059"
  },
  {
    "text": "work better for\nshorter-horizon problems because if you have a\nvery long-horizon problem",
    "start": "2388060",
    "end": "2394390"
  },
  {
    "text": "and a sparse reward, then you\nneed to roll out your model for a long period of time. And it may be difficult.\nYour model probably",
    "start": "2394390",
    "end": "2400840"
  },
  {
    "text": "won't be accurate for\nthat long of a horizon. If you, instead, have\na long-horizon problem with a very shaped reward,\nthen model-based planning",
    "start": "2400840",
    "end": "2407500"
  },
  {
    "text": "will be feasible. Or if you have a short-horizon\nproblem with a sparse reward. It will also be feasible.",
    "start": "2407500",
    "end": "2413325"
  },
  {
    "text": "So this example\non the side right here is an example of a sparse\nreward where you just get 1",
    "start": "2413325",
    "end": "2418660"
  },
  {
    "text": "for success and 0 otherwise. But it's a fairly short\nhorizon because the robot only needs to move its arm\ndown and move the pencil",
    "start": "2418660",
    "end": "2426700"
  },
  {
    "text": "case to the left.  Yeah, and then\n[MUTED] is asking,",
    "start": "2426700",
    "end": "2432870"
  },
  {
    "text": "can you explain\nagain what do you mean by the reward will\nchange how you collect data So the reason why this\nis, is basically here.",
    "start": "2432870",
    "end": "2441550"
  },
  {
    "text": "So when you are optimizing\nover your action sequences",
    "start": "2441550",
    "end": "2447090"
  },
  {
    "text": "to choose actions, you're\ngoing to be optimizing for your action sequences with\nregard to your reward function.",
    "start": "2447090",
    "end": "2455970"
  },
  {
    "text": "And then once you actually\ntry to solve that task, you're going to be adding this\ndata to your dataset that's",
    "start": "2455970",
    "end": "2462570"
  },
  {
    "text": "being used to train your model. And so if one task is like\nmove an object over here, another task is to move\nan object over here,",
    "start": "2462570",
    "end": "2468660"
  },
  {
    "text": "and you only train with\nregard to that first task, you're not going to\nsee data of trying to move the object to\nthe second position,",
    "start": "2468660",
    "end": "2475230"
  },
  {
    "text": "and this will make it\nharder to actually perform that second task, because\nthe model won't be very accurate for those states.",
    "start": "2475230",
    "end": "2481490"
  },
  {
    "text": " [MUTED] is asking, the\nassumption here is that",
    "start": "2481490",
    "end": "2488310"
  },
  {
    "text": "the meta learning model\naccuracy is 100 %. It doesn't need to\nbe exactly 100 %--",
    "start": "2488310",
    "end": "2494260"
  },
  {
    "text": "oh, I think you're referring\nto this point right here-- but it does need to\nbe fairly accurate.",
    "start": "2494260",
    "end": "2500880"
  },
  {
    "text": "If it starts to make\na lot of mistakes, then your model is going to kind\nof exploit those mistakes just",
    "start": "2500880",
    "end": "2509670"
  },
  {
    "text": "like how it exploits errors\nin the dynamics model. ",
    "start": "2509670",
    "end": "2514987"
  },
  {
    "text": "[MUTED] is asking, is\nlearning the reward function a supervised learning problem?",
    "start": "2514987",
    "end": "2520270"
  },
  {
    "text": "Yes. So in this case,\nthis is basically a multitask supervised\nlearning problem",
    "start": "2520270",
    "end": "2526090"
  },
  {
    "text": "because you're assuming you\nhave some labels of the reward function, and you want to be\nable to predict those labels.",
    "start": "2526090",
    "end": "2532122"
  },
  {
    "text": "But it's a multitask\nproblem in the sense that you probably are\npredicting the rewards for multiple different tasks.",
    "start": "2532122",
    "end": "2538000"
  },
  {
    "text": "And here it's\nbasically equivalent to a meta-learning problem\nin the supervised case. ",
    "start": "2538000",
    "end": "2545610"
  },
  {
    "text": "And then [MUTED] is\nasking, is the outcome of this equivalent of training\na joint MDP across these tasks?",
    "start": "2545610",
    "end": "2552750"
  },
  {
    "text": "I'm not sure exactly what you\nmean by training a joint MDP, because the MDP is\nthe task itself.",
    "start": "2552750",
    "end": "2558900"
  },
  {
    "text": "But this is one approach for\nkind of solving a policy,",
    "start": "2558900",
    "end": "2564869"
  },
  {
    "text": "or solving for behaviors\nacross all of these tasks. ",
    "start": "2564870",
    "end": "2573950"
  },
  {
    "text": "OK. So I think those are\nall the questions. Now that we've kind of gone\nover the basics of model",
    "start": "2573950",
    "end": "2581660"
  },
  {
    "text": "based reinforcement learning\nfor multitask learning, let's talk about\nthe scenario where",
    "start": "2581660",
    "end": "2586730"
  },
  {
    "text": "we have image observations,\nbecause things get a bit harder in that case. ",
    "start": "2586730",
    "end": "2593640"
  },
  {
    "text": "So for example, we can take\nthis graphical model here, that's similar to the\none that the Carl showed",
    "start": "2593640",
    "end": "2599600"
  },
  {
    "text": "on Monday last week\nwhere you can directly observe the observations\nO, and you can't",
    "start": "2599600",
    "end": "2606380"
  },
  {
    "text": "observe the underlying states. So as a concrete example,\nsay you want your robot",
    "start": "2606380",
    "end": "2613240"
  },
  {
    "text": "to be able to use the spatula\nto lift up this object and put it into the bowl. And the only thing that you have\naccess to is this camera image.",
    "start": "2613240",
    "end": "2623589"
  },
  {
    "text": "And the robot can't\ndirectly access for example, the\nposition of the bowl or the position of the object. ",
    "start": "2623590",
    "end": "2632130"
  },
  {
    "text": "Also when you have images,\nyou don't necessarily have a direct reward signal.",
    "start": "2632130",
    "end": "2637230"
  },
  {
    "text": "You can train a\nclassifier, but you don't know the formula\nfor the reward function.",
    "start": "2637230",
    "end": "2642990"
  },
  {
    "text": " So as I mentioned, you can\nlearn an image classifier",
    "start": "2642990",
    "end": "2650460"
  },
  {
    "text": "to get the reward function. Another thing that\nyou could do is provide an image\nof the goal, so you can provide an image\nthat looks like this",
    "start": "2650460",
    "end": "2657630"
  },
  {
    "text": "and try to reach that image. ",
    "start": "2657630",
    "end": "2663080"
  },
  {
    "text": "So these are the\napproaches or options that we can use for\nspecifying the reward.",
    "start": "2663080",
    "end": "2668240"
  },
  {
    "text": "There are a few\ndifferent approaches for model based RL when you\nhave these kinds of image observations.",
    "start": "2668240",
    "end": "2674520"
  },
  {
    "text": "One is to learn a model in some\nlatent representation space, another is to learn a model\ndirectly of the observations,",
    "start": "2674520",
    "end": "2682070"
  },
  {
    "text": "and the third is\nto learn a model of some alternative quantities. And we'll go over each of these\napproaches for the remaining",
    "start": "2682070",
    "end": "2690560"
  },
  {
    "text": "time of the lecture. So let's first talk\nabout learning models",
    "start": "2690560",
    "end": "2696140"
  },
  {
    "text": "in latent space. The key idea here\nis that we're going",
    "start": "2696140",
    "end": "2701460"
  },
  {
    "text": "to learn an embedding that\nkind of embeds our observations",
    "start": "2701460",
    "end": "2706930"
  },
  {
    "text": "into some usually lower\ndimensional space, and then just treat that lower\ndimensional space as our state",
    "start": "2706930",
    "end": "2713360"
  },
  {
    "text": "space. So for example, if this is\nkind of our original graphical",
    "start": "2713360",
    "end": "2718500"
  },
  {
    "text": "model, we can basically\njust try to learn an encoder that maps us\nback from the observations",
    "start": "2718500",
    "end": "2724458"
  },
  {
    "text": "back to our states. ",
    "start": "2724458",
    "end": "2730105"
  },
  {
    "text": "And then once you\nlearn this embedding, you can just do the\nsame sort of model based RL in that latent space.",
    "start": "2730105",
    "end": "2736840"
  },
  {
    "text": "So it sounds pretty simple. There are a couple of\npapers that do this, or there were a couple papers\nthat did this back, like four",
    "start": "2736840",
    "end": "2743620"
  },
  {
    "text": "or five years ago. They're actually still fairly\nrepresentative of the kinds of latent space models\nthat people use today,",
    "start": "2743620",
    "end": "2750170"
  },
  {
    "text": "so we'll talk about these. I can also talk about\nextensions on these and more recent approaches if\npeople are interested.",
    "start": "2750170",
    "end": "2758910"
  },
  {
    "text": "And the algorithm looks\nsomething like this. So you run some policy\nto collect data, you then learn a latent\nembedding of your observation",
    "start": "2758910",
    "end": "2767400"
  },
  {
    "text": "space that maps from o to s, and\na model in that latent space.",
    "start": "2767400",
    "end": "2774339"
  },
  {
    "text": "Then you can use this\nmodel to optimize over a set of action sequences, just\nlike before, and then execute",
    "start": "2774340",
    "end": "2782770"
  },
  {
    "text": "your planned actions and append\nthe visited observation action, next observation tuples\nto your data set.",
    "start": "2782770",
    "end": "2788350"
  },
  {
    "text": " Cool, so there are a\ncouple of questions",
    "start": "2788350",
    "end": "2793760"
  },
  {
    "text": "that arise from this algorithm. This is basically the\nsame as before with this additional step of\nembedding the observation",
    "start": "2793760",
    "end": "2800240"
  },
  {
    "text": "space. The first question\nthat comes up is, what is the reward function for\noptimizing for your actions?",
    "start": "2800240",
    "end": "2806480"
  },
  {
    "text": " The reward function\nthat we'll use and that is most commonly\nused as just given the goal",
    "start": "2806480",
    "end": "2816390"
  },
  {
    "text": "state that you're given, just\nmeasure the negative distance between your\nembedded observation",
    "start": "2816390",
    "end": "2822360"
  },
  {
    "text": "and your embedded\ngoal observation. So this is a pretty\nsimple approach.",
    "start": "2822360",
    "end": "2827420"
  },
  {
    "text": "Again like I mentioned, you\ncould also use something like a classifier, or if\nyou do have observations of the rewards, you can\nlearn a model of your rewards",
    "start": "2827420",
    "end": "2836780"
  },
  {
    "text": "on top of your learned\nobservation space. So you could learn\nsomething like this.",
    "start": "2836780",
    "end": "2844450"
  },
  {
    "text": "But in practice,\noften approaches use something like\nthis because getting reward functions is actually\npretty tricky in practice.",
    "start": "2844450",
    "end": "2852240"
  },
  {
    "text": " So [MUTED] asking, in\nthe spatula example",
    "start": "2852240",
    "end": "2858410"
  },
  {
    "text": "would we be learning a model\nthat maps images to the spatula orientation and location?",
    "start": "2858410",
    "end": "2864359"
  },
  {
    "text": "That would roughly be the\ngoal, although we also want it to model things\nlike the object in the bowl",
    "start": "2864360",
    "end": "2870300"
  },
  {
    "text": "and not just the spatula. In practice, I guess\nwe'll talk about how",
    "start": "2870300",
    "end": "2875859"
  },
  {
    "text": "we go about learning\nthis embedding space on the next slide. ",
    "start": "2875860",
    "end": "2881408"
  },
  {
    "text": "One critical assumption\nwith this reward function is that this distance\nin this latent space is actually an accurate metric.",
    "start": "2881408",
    "end": "2888730"
  },
  {
    "text": "This assumption is actually\npretty tricky in practice, because even if this\nrepresentation space is",
    "start": "2888730",
    "end": "2897280"
  },
  {
    "text": "useful for modeling\nthe dynamics, distances of that space may\nnot actually be meaningful.",
    "start": "2897280",
    "end": "2903640"
  },
  {
    "text": "Yeah, this is something\nlike the L2 metric, you could also use other\nmetrics in this space as well.",
    "start": "2903640",
    "end": "2910080"
  },
  {
    "start": "2910080",
    "end": "2915910"
  },
  {
    "text": "OK, so now how do we go\nabout actually optimizing for the latent embedding space? ",
    "start": "2915910",
    "end": "2923099"
  },
  {
    "text": "What this first paper did\nis they learned an embedding and a model jointly.",
    "start": "2923100",
    "end": "2928170"
  },
  {
    "text": "So they basically\ntook something that looks like a\nvariational autoencoder, except over two timesteps\nand jointly learned",
    "start": "2928170",
    "end": "2935570"
  },
  {
    "text": "how to reconstruct images. Basically jointly learned\nkind of g of o mapping to s,",
    "start": "2935570",
    "end": "2945170"
  },
  {
    "text": "also they learned a decoder\nthat maps from s back",
    "start": "2945170",
    "end": "2952250"
  },
  {
    "text": "to the original space,\nand then they finally also learned something that\nmaps from s and a to s prime.",
    "start": "2952250",
    "end": "2961940"
  },
  {
    "text": "This is the gist of the model. It's a little bit more\ncomplicated than that because it's actually\nkind of building",
    "start": "2961940",
    "end": "2969339"
  },
  {
    "text": "upon variational\ninference, but if you're interested in the details you\ncan take a look at the paper. ",
    "start": "2969340",
    "end": "2976090"
  },
  {
    "text": "And in this case,\nthey were trying to reach different goal states.",
    "start": "2976090",
    "end": "2982450"
  },
  {
    "text": "So as an example,\nthey would give it a goal state that\nlooks like this where the arm is kind of\nreached out to the top right.",
    "start": "2982450",
    "end": "2991860"
  },
  {
    "text": "And then here's an example\nof a trajectory that's trying to reach that state.",
    "start": "2991860",
    "end": "2998690"
  },
  {
    "text": "And you can see\nboth the trajectory of the arm on the left\nand also the prediction",
    "start": "2998690",
    "end": "3005650"
  },
  {
    "text": "from the previous timestep. So these are kind of\nreconstructed images of the state. Now if you give it a\ndifferent task, which",
    "start": "3005650",
    "end": "3013060"
  },
  {
    "text": "is to kind of curl\nthe arm upward, you can also figure\nout how to reach this.",
    "start": "3013060",
    "end": "3020210"
  },
  {
    "text": "And then finally, there's a\nthird example where the arm is kind of reaching downward. It's given this goal\nstate, and it's trying",
    "start": "3020210",
    "end": "3026630"
  },
  {
    "text": "to plan to reach the state. ",
    "start": "3026630",
    "end": "3032860"
  },
  {
    "text": "And all of this was\nactually learned with only around\n300 trials, which",
    "start": "3032860",
    "end": "3038619"
  },
  {
    "text": "would correspond to about 25\nminutes of robot time per task, which is quite efficient.",
    "start": "3038620",
    "end": "3045174"
  },
  {
    "text": " And then another way to optimize\nfor a latent embedding space is",
    "start": "3045174",
    "end": "3053310"
  },
  {
    "text": "to try to incorporate a\nlittle bit more structure into the process. And so in this\nwork, it was trying",
    "start": "3053310",
    "end": "3060180"
  },
  {
    "text": "to optimize first\nlatent embedding space where the latent space\ncorresponded to points in 2D.",
    "start": "3060180",
    "end": "3068240"
  },
  {
    "text": "And as a result of kind\nof imposing the structure on the latent space,\nit led to kind",
    "start": "3068240",
    "end": "3074390"
  },
  {
    "text": "of features that tracked\nthe objects through time.",
    "start": "3074390",
    "end": "3081328"
  },
  {
    "text": "And specifically what\nit looks like is it's an autoencoder where the goal\nis to reconstruct images.",
    "start": "3081328",
    "end": "3089870"
  },
  {
    "text": "We wanted to impose\nthis 2D structure onto the latent representation.",
    "start": "3089870",
    "end": "3095130"
  },
  {
    "text": "So to do that, on top of\nthe last convolutional layer",
    "start": "3095130",
    "end": "3101089"
  },
  {
    "text": "there is a spatial softmax,\nwhich essentially exponentiates each of the activations\nand then normalizes over",
    "start": "3101090",
    "end": "3111500"
  },
  {
    "text": "the activations within\nthe spatial dimensions. And so each of these\ndifferent feature maps",
    "start": "3111500",
    "end": "3116630"
  },
  {
    "text": "here correspond to\ndifferent distributions over the spatial extent\nof the feature map.",
    "start": "3116630",
    "end": "3123140"
  },
  {
    "text": "And then once you have this\ndistribution over locations, essentially, you can then\ncompute the expected location",
    "start": "3123140",
    "end": "3130609"
  },
  {
    "text": "of each of those distributions\nto give you essentially expected x, y-coordinates for\neach of these feature maps.",
    "start": "3130610",
    "end": "3140070"
  },
  {
    "text": "So then as a result, this is\nkind of a 32 dimensional vector where you get two dimensions\nfor each of these feature maps,",
    "start": "3140070",
    "end": "3147200"
  },
  {
    "text": "one for the x and one\nfor the y-coordinate. And then you get something\nthat looks like this.",
    "start": "3147200",
    "end": "3153600"
  },
  {
    "text": "So this is visualizing only\ntwo of the feature points, but one of the feature points\ntracks the finger of the robot,",
    "start": "3153600",
    "end": "3159845"
  },
  {
    "text": "and another one of\nthem tracks the object. ",
    "start": "3159845",
    "end": "3166130"
  },
  {
    "text": "I also mentioned that\nthe embedding is smooth, so there's actually an\nadditional loss on top of this reconstruction loss\nthat encourages the feature",
    "start": "3166130",
    "end": "3173540"
  },
  {
    "text": "points to-- that penalizes the acceleration\nof the feature point",
    "start": "3173540",
    "end": "3180160"
  },
  {
    "text": "so you can encourage it to-- maybe these feature points\nare kind of indicated by xy--",
    "start": "3180160",
    "end": "3187369"
  },
  {
    "text": "you could encourage the-- maybe you have\nsomething like this.",
    "start": "3187370",
    "end": "3193914"
  },
  {
    "text": "This is the velocity of\nthe point, essentially, the difference between the\npoint's position at t plus 1",
    "start": "3193915",
    "end": "3201369"
  },
  {
    "text": "and the point at t. And if you want to\npenalize the acceleration, you can do something\nlike this where",
    "start": "3201370",
    "end": "3207790"
  },
  {
    "text": "you compare the velocity\nat this timestep to the velocity at the\nprevious time step.",
    "start": "3207790",
    "end": "3213680"
  },
  {
    "text": "This gives you something\nlike the acceleration, and you can penalize this\nby basically just putting",
    "start": "3213680",
    "end": "3218960"
  },
  {
    "text": "in L2 loss on this to give\nyou points that are encouraged to be smooth over time.",
    "start": "3218960",
    "end": "3224150"
  },
  {
    "text": " OK, so this was the approach\nfor learning the feature space,",
    "start": "3224150",
    "end": "3230930"
  },
  {
    "text": "and then this is what the\nlearning process looks like. So in this case, the\ngoal is to put the black",
    "start": "3230930",
    "end": "3237260"
  },
  {
    "text": "somewhere over here. And the policy is\ninitialized to something that knows how to move its arm\nto the right position, but not",
    "start": "3237260",
    "end": "3243702"
  },
  {
    "text": "necessarily the block. And over time, it is\nable to figure out",
    "start": "3243702",
    "end": "3249890"
  },
  {
    "text": "how to put the block in\nthe correct position. And they have drawn the\nblue square in slightly",
    "start": "3249890",
    "end": "3255690"
  },
  {
    "text": "the wrong position,\nbut yeah, it's able to figure out how to put\nit in the goal position, which was indicated by a goal image.",
    "start": "3255690",
    "end": "3263670"
  },
  {
    "text": " Here is the final\npolicy that it learns.",
    "start": "3263670",
    "end": "3268812"
  },
  {
    "text": " And then we can also\ndo it for other tasks.",
    "start": "3268812",
    "end": "3274020"
  },
  {
    "text": "So in this case, this\nis the special task that we were\nmotivated-- that I gave with the initial\nmotivation, and it",
    "start": "3274020",
    "end": "3280240"
  },
  {
    "text": "can use this task\nto learn how to lift the object into the goal, into\nthe bowl given the goal image,",
    "start": "3280240",
    "end": "3286880"
  },
  {
    "text": "and also in this case, given\nthe target configuration of the arm. ",
    "start": "3286880",
    "end": "3295372"
  },
  {
    "text": "Lastly, you can also actually\nvisualize the feature points. So each of these circles is\nthe visualized feature point",
    "start": "3295372",
    "end": "3301490"
  },
  {
    "text": "for the image, and the x's is\nthe kind of the encoded feature points of the goal image.",
    "start": "3301490",
    "end": "3308850"
  },
  {
    "text": "And you get\ntrajectories that look like this, where the final\npolicy is trying to match--",
    "start": "3308850",
    "end": "3314186"
  },
  {
    "text": "is trying to minimize the\ndistance between the goal feature points and the\nencoded feature points.",
    "start": "3314186",
    "end": "3322130"
  },
  {
    "start": "3322130",
    "end": "3327160"
  },
  {
    "text": "And these skills\nwere learned actually individually in\nthis case, and they",
    "start": "3327160",
    "end": "3332890"
  },
  {
    "text": "were learned in about 11\nminutes of robot time per task with the caveat that each\nof the representations",
    "start": "3332890",
    "end": "3339200"
  },
  {
    "text": "were actually learned per\ntask and per environment. ",
    "start": "3339200",
    "end": "3346790"
  },
  {
    "text": "OK, so any questions on kind\nof learning in latent space?",
    "start": "3346790",
    "end": "3354602"
  },
  {
    "text": "Feel free to raise your hand.  And looking at the\nquestions in the chat,",
    "start": "3354602",
    "end": "3360550"
  },
  {
    "text": "I think answered most of them. For the most recent\none, this does require that you have a goal\nimage from the same orientation",
    "start": "3360550",
    "end": "3367599"
  },
  {
    "text": "as your observations, otherwise\nit may not work very well. ",
    "start": "3367600",
    "end": "3376400"
  },
  {
    "text": "OK. So now one thought exercise-- feel free to answer in chat\nor by raising your hand--",
    "start": "3376400",
    "end": "3382150"
  },
  {
    "text": "is both of these\ntwo approaches we're learning the embedding\nspace with an autoencoder,",
    "start": "3382150",
    "end": "3388000"
  },
  {
    "text": "by reconstructing the image. And why not just\nlearn the embedding,",
    "start": "3388000",
    "end": "3393280"
  },
  {
    "text": "why not just learn g of\no to s by simply trying to minimize the model error of\nthe model in that latent space?",
    "start": "3393280",
    "end": "3404240"
  },
  {
    "text": "So say you were just trying\nto minimize this with regard",
    "start": "3404240",
    "end": "3410600"
  },
  {
    "text": "to the parameters of g, and\nwhere you basically just",
    "start": "3410600",
    "end": "3416550"
  },
  {
    "text": "set s to be the output of g. So what would\nhappen in this case,",
    "start": "3416550",
    "end": "3421580"
  },
  {
    "text": "if you just tried to optimize\nthis embedding with regard",
    "start": "3421580",
    "end": "3427020"
  },
  {
    "text": "to the model error\nin your latent space? ",
    "start": "3427020",
    "end": "3432550"
  },
  {
    "text": "So [MUTED] said, a long\ntime before convergence, bad embedding would\nresult in a bad model,",
    "start": "3432550",
    "end": "3437680"
  },
  {
    "text": "and the model would be largely\ndependent on embedding. [MUTED] suggested\nthat maybe there's a trivial solution\nwhen the embedding is",
    "start": "3437680",
    "end": "3443950"
  },
  {
    "text": "just constant, and [MUTED]\nalso suggested that. So that's right.",
    "start": "3443950",
    "end": "3448960"
  },
  {
    "text": "So there's actually\na trivial solution to this, which is that if you\noptimize it only with respect",
    "start": "3448960",
    "end": "3454030"
  },
  {
    "text": "to the model error, then\nyour encoding can just always output 0 deterministically,\nfor example.",
    "start": "3454030",
    "end": "3460480"
  },
  {
    "text": "And then your model just\nhas to always predict 0, and this wouldn't lead to a very\nuseful embedding space, right?",
    "start": "3460480",
    "end": "3471360"
  },
  {
    "text": "So this is why it's important\nto have some sort of objective to prevent this\ndegenerate solution.",
    "start": "3471360",
    "end": "3477913"
  },
  {
    "text": "So you need to do\nsomething-- it doesn't need to be\nreconstruction-- but you need something that prevents\nthe embedding from collapsing.",
    "start": "3477913",
    "end": "3486856"
  },
  {
    "text": " OK, so to summarize, if\nyou learn in latent space,",
    "start": "3486856",
    "end": "3494890"
  },
  {
    "text": "we found that you can learn\ncomplex skills, visual skills from images very efficiently.",
    "start": "3494890",
    "end": "3501797"
  },
  {
    "text": "Things like structure\nrepresentations that also enable this\neffective learning, the downside is that\nreconstruction of objectives",
    "start": "3501798",
    "end": "3508230"
  },
  {
    "text": "may not recover the\nright representation. So they are of course\nbetter than not having the\nreconstruction objective,",
    "start": "3508230",
    "end": "3515100"
  },
  {
    "text": "but for example in the work that\nI showed with the spatula, one",
    "start": "3515100",
    "end": "3520860"
  },
  {
    "text": "of the tasks that I also wanted\nthe robot to be able to do was to place a ping pong\nball in the correct position.",
    "start": "3520860",
    "end": "3530100"
  },
  {
    "text": "So this is kind of a low\ndimensional image of the robot with the ping pong ball here.",
    "start": "3530100",
    "end": "3535500"
  },
  {
    "text": "And if you trained an\nautoencoder on this data, what you got for the\nreconstruction was",
    "start": "3535500",
    "end": "3540750"
  },
  {
    "text": "something that looks like this. So it essentially learned\nto erase the ping pong ball because it was a very small\nobject that didn't actually",
    "start": "3540750",
    "end": "3550119"
  },
  {
    "text": "lead to a high\nreconstruction error if it didn't reconstruct it.",
    "start": "3550120",
    "end": "3555272"
  },
  {
    "text": "And of course the ping\npong ball is like, in this case, exactly what you\nwanted to know for the task.",
    "start": "3555272",
    "end": "3560637"
  },
  {
    "text": "And this is an example of where\na reconstruction objective may not recover the\nright representation.",
    "start": "3560637",
    "end": "3567180"
  },
  {
    "text": "It may not actually\nfocus on things that are relevant for the task. ",
    "start": "3567180",
    "end": "3572609"
  },
  {
    "text": "So this suggests that we\nneed potentially better unsupervised representation\nlearning methods. There has been a\nlot of development",
    "start": "3572610",
    "end": "3578240"
  },
  {
    "text": "on this in kind of\nthe last few years, but there's still\nmore work to be",
    "start": "3578240",
    "end": "3584930"
  },
  {
    "text": "seen on whether\nthese new methods are effective for their\nrepresentations for these kinds of tasks.",
    "start": "3584930",
    "end": "3590270"
  },
  {
    "text": " Do you have a question?",
    "start": "3590270",
    "end": "3596430"
  },
  {
    "text": "Yeah, would you be able to give\nsome examples of latent space,",
    "start": "3596430",
    "end": "3602520"
  },
  {
    "text": "I guess something in latent\nspace, not just a picture? Yeah, so you can also--\ninstead of having an image,",
    "start": "3602520",
    "end": "3612660"
  },
  {
    "text": "you can also have other sensors. So maybe-- in robotics\nyou have audio,",
    "start": "3612660",
    "end": "3618840"
  },
  {
    "text": "you could have tactile\nsensors for example, and you may want to learn kind\nof a latent representation",
    "start": "3618840",
    "end": "3624270"
  },
  {
    "text": "of those sensor readings. Beyond robotics, in\nautonomous driving",
    "start": "3624270",
    "end": "3630430"
  },
  {
    "text": "you may have things like\nLiDAR readings for example. If you're in a\nsetting like language,",
    "start": "3630430",
    "end": "3636569"
  },
  {
    "text": "language models work pretty\nwell so you could probably just model directly in\nthe space of language.",
    "start": "3636570",
    "end": "3643650"
  },
  {
    "text": "Yeah, those are a few examples. Thanks. ",
    "start": "3643650",
    "end": "3649240"
  },
  {
    "text": "Great. And then [MUTED] is\nasking, what happens if you don't learn\nlatent space and just feed the observations into\nthe dynamics model directly?",
    "start": "3649240",
    "end": "3658950"
  },
  {
    "text": "And this is actually exactly\nwhat we'll talk about next, so we could actually\nskip these slides. We'll maybe get\nback to it later,",
    "start": "3658950",
    "end": "3666190"
  },
  {
    "text": "but what happens if\nwe just model directly in observation space? And this is actually something\nthat we can absolutely do.",
    "start": "3666190",
    "end": "3675090"
  },
  {
    "text": "So we can just kind of\ntake the MPC algorithm that we did before, replace\nall of the s's with o's, that's",
    "start": "3675090",
    "end": "3681180"
  },
  {
    "text": "exactly what I did\nhere, and train them all to operate directly on\nyour raw observations.",
    "start": "3681180",
    "end": "3686790"
  },
  {
    "text": " So what this might look like\nin the context of robotics",
    "start": "3686790",
    "end": "3694950"
  },
  {
    "text": "is you collect some data,\nso you can collect data that looks something like this.",
    "start": "3694950",
    "end": "3701160"
  },
  {
    "text": "In this case, this is\nactually just a random policy for the most part.",
    "start": "3701160",
    "end": "3707839"
  },
  {
    "text": "Then you can train your model\nto minimize prediction error.",
    "start": "3707840",
    "end": "3713140"
  },
  {
    "text": "This actually corresponds to\ntraining a video prediction model, and you can\nget predictions",
    "start": "3713140",
    "end": "3718450"
  },
  {
    "text": "that look something like this. That's predicting\nwhat the future will look like based on the\nactions the robot takes,",
    "start": "3718450",
    "end": "3725410"
  },
  {
    "text": "or basically what its future\nsensor readings will show based on the actions it takes.",
    "start": "3725410",
    "end": "3731400"
  },
  {
    "text": "You can also apply\nthis to the towel data that I showed before\nto get predictions that look like this.",
    "start": "3731400",
    "end": "3736530"
  },
  {
    "text": "One of the things that's\nnice about operating directly on observations is\nthat it means that you don't have to prescribe\na certain representation",
    "start": "3736530",
    "end": "3743310"
  },
  {
    "text": "of your state. And for example, with\ndeformable objects it's actually quite hard to\nthink about how you would",
    "start": "3743310",
    "end": "3749278"
  },
  {
    "text": "represent a towel, for example. And if you essentially\njust operate directly",
    "start": "3749278",
    "end": "3754360"
  },
  {
    "text": "on your raw sensor\nobservations, then you can train models like this.",
    "start": "3754360",
    "end": "3760663"
  },
  {
    "text": "And then once you\nhave this model, you can then use them all\nto optimize for your action sequence with respect\nto some reward function.",
    "start": "3760663",
    "end": "3767630"
  },
  {
    "text": "Again, your reward function\ncould be a classifier or it would be some other\nmetric that operates on your predicted images.",
    "start": "3767630",
    "end": "3773480"
  },
  {
    "text": " There's a couple\nof details here.",
    "start": "3773480",
    "end": "3778530"
  },
  {
    "text": "So how do you predict video? This corresponds to\na video picture model like I said before.",
    "start": "3778530",
    "end": "3784049"
  },
  {
    "text": "There's a number of\ndifferent models out there, some of the important\nelements of this is to have a deep\nrecurrent model that",
    "start": "3784050",
    "end": "3791240"
  },
  {
    "text": "predicts multiple\nframes into the future. And it's also important\nfor it to be conditioned",
    "start": "3791240",
    "end": "3796520"
  },
  {
    "text": "on the actions that you\ntake so that it predicts the future based on the planned\nactions that you want to take.",
    "start": "3796520",
    "end": "3803180"
  },
  {
    "text": "And often these models\nare also stochastic, this is because there's often\na lot of uncertainty about what",
    "start": "3803180",
    "end": "3809690"
  },
  {
    "text": "the future will look like,\nand you want your model to be able to capture that\nuncertainty rather than simply",
    "start": "3809690",
    "end": "3816367"
  },
  {
    "text": "blurring out-- rather than\nsimply kind of blurring the things that it's\nnot certain about.",
    "start": "3816367",
    "end": "3822097"
  },
  {
    "text": "If you want an example of what\nkind of the latest and greatest models look like,\nthis is a paper from late last year\nfrom Ruben Villegas.",
    "start": "3822097",
    "end": "3830706"
  },
  {
    "text": "The left is showing the\npredictions from the model and the right is\nshowing the ground truth on each of these pairs.",
    "start": "3830707",
    "end": "3836180"
  },
  {
    "text": "And you see that\nthey can actually predict pretty well\nhow the robot will move",
    "start": "3836180",
    "end": "3843140"
  },
  {
    "text": "and how the objects will move in\nresponse to the robot grasping",
    "start": "3843140",
    "end": "3848720"
  },
  {
    "text": "the shorts, or\ngrasping the towel and moving it to the\ndesired location. ",
    "start": "3848720",
    "end": "3857840"
  },
  {
    "text": "OK. So that's how you predict video. How do you plan? Basically plan the same\nexact way as before.",
    "start": "3857840",
    "end": "3863640"
  },
  {
    "text": "For example, you could use\na gradient free optimization where you sample a large\nnumber of action sequences,",
    "start": "3863640",
    "end": "3869430"
  },
  {
    "text": "including these two\naction sequences. Predict the future for\neach action sequence, like as the\npredictions shown here,",
    "start": "3869430",
    "end": "3876170"
  },
  {
    "text": "and then pick the future\nthat is the most successful according to your\nreward function,",
    "start": "3876170",
    "end": "3882920"
  },
  {
    "text": "and execute the\ncorresponding action.  And then you can also repeat.",
    "start": "3882920",
    "end": "3890460"
  },
  {
    "text": "If you want to replan, you can\nalso kind of repeat these steps at each timestep. or every couple of timesteps.",
    "start": "3890460",
    "end": "3896460"
  },
  {
    "start": "3896460",
    "end": "3901690"
  },
  {
    "text": "OK. Just essentially kind of\nMPC but in the visual space or in the kind of raw sensor\nspace of these robots.",
    "start": "3901690",
    "end": "3909505"
  },
  {
    "text": " So how it works is you\nmight specify a goal.",
    "start": "3909505",
    "end": "3916102"
  },
  {
    "text": "Like I mentioned before, there's\na number of different ways to specify the goal. In some of our recent work, one\nway that we specify the goal",
    "start": "3916102",
    "end": "3924200"
  },
  {
    "text": "is specifying where we want\npixels to move in the image. For example, specifying\nthat we want this red pixel",
    "start": "3924200",
    "end": "3930470"
  },
  {
    "text": "to move up to where\nthis green pixel is, and that we don't want\nthis pixel here to move.",
    "start": "3930470",
    "end": "3937250"
  },
  {
    "text": "Then we can run kind of visual\nMPC with regard to this goal,",
    "start": "3937250",
    "end": "3943880"
  },
  {
    "text": "so we get a prediction\nlike this that we think will accomplish the goal. And then we can execute\nthose actions on the robot.",
    "start": "3943880",
    "end": "3953440"
  },
  {
    "text": "This is with a model that isn't\nquite as precise as the latest models that I mentioned before,\nbut it can still approximately",
    "start": "3953440",
    "end": "3960550"
  },
  {
    "text": "accomplish the task.  And of course, one of the nice\nthings about these methods",
    "start": "3960550",
    "end": "3967290"
  },
  {
    "text": "is that with just a single\nvideo prediction model, you can accomplish not just one\ntask but many different tasks,",
    "start": "3967290",
    "end": "3974340"
  },
  {
    "text": "assuming that kind of the\ndynamics of individual space and of the world are constant\nacross these different tasks.",
    "start": "3974340",
    "end": "3981940"
  },
  {
    "text": "So things like\npicking up an object, moving a sleeve towards\nthe edge of the shirt,",
    "start": "3981940",
    "end": "3988380"
  },
  {
    "text": "picking up an apple and\nputting it on a plate, you basically just\nneed to kind of pass in the task you want it to\ndo and some specification",
    "start": "3988380",
    "end": "3997050"
  },
  {
    "text": "on images that\nallows it to optimize with regard to that\ntask, and then it can plan to accomplish the\ntask using its video prediction",
    "start": "3997050",
    "end": "4006210"
  },
  {
    "text": "model. So here are a few more examples. ",
    "start": "4006210",
    "end": "4012509"
  },
  {
    "text": "OK, so the benefits of modeling\ndirectly in image space is that this sort of approach\ncan scale to real images.",
    "start": "4012510",
    "end": "4019300"
  },
  {
    "text": "It also involves fairly\nlimited human involvement, so once you start\njust collecting data,",
    "start": "4019300",
    "end": "4025410"
  },
  {
    "text": "training the model is actually\nfully self supervised. This is also true for the\nlatent space methods as well.",
    "start": "4025410",
    "end": "4034240"
  },
  {
    "text": "And actually, unlike the\nlatent space methods, for the most part we were able\nto accomplish a wider variety",
    "start": "4034240",
    "end": "4039660"
  },
  {
    "text": "of tasks with a single model. Some of the downsides is\nthat these images still",
    "start": "4039660",
    "end": "4044880"
  },
  {
    "text": "do have limited\nbackground variability, and you have a lot of\nvariability in the background and the objects in these models\ncan have trouble capturing",
    "start": "4044880",
    "end": "4052380"
  },
  {
    "text": "all of that diversity. And the skills here are\nstill relatively simple,",
    "start": "4052380",
    "end": "4058420"
  },
  {
    "text": "that's kind of a\nshortcoming-- could be a shortcoming of a\nnumber of different things, including the\ncapacity of the model",
    "start": "4058420",
    "end": "4064178"
  },
  {
    "text": "to model complex\ninteractions, and the ability to specify those tasks, and\nthe power of the planner",
    "start": "4064178",
    "end": "4072609"
  },
  {
    "text": "to be able to optimize for\nthe right action sequence to accomplish those tasks.",
    "start": "4072610",
    "end": "4077852"
  },
  {
    "text": "These methods are also\nquite compute intensive, because you have to actually\nplan using a video prediction model, which is quite large.",
    "start": "4077852",
    "end": "4083650"
  },
  {
    "text": " OK. Any questions on\nmodeling directly",
    "start": "4083650",
    "end": "4088890"
  },
  {
    "text": "in your observation space? ",
    "start": "4088890",
    "end": "4101500"
  },
  {
    "text": "OK. So the last set of approaches\nthat you can consider is predicting something other\nthan some latent representation",
    "start": "4101500",
    "end": "4110960"
  },
  {
    "text": "around that image, predicting\nkind of alternative qualities. And what this looks like is in\ndifferent scenarios you could",
    "start": "4110960",
    "end": "4117759"
  },
  {
    "text": "ask, if I take a certain\nsequence of actions, will I successfully\ngrasp an object,",
    "start": "4117760",
    "end": "4123189"
  },
  {
    "text": "or will I collide\nwith an object, or if I take a\nsequence of actions,",
    "start": "4123189",
    "end": "4128240"
  },
  {
    "text": "what will my health or my\ndamage be, for example? And what this is\ndoing is you basically",
    "start": "4128240",
    "end": "4135109"
  },
  {
    "text": "consider different possible\nthings that the agent could do,",
    "start": "4135109",
    "end": "4140359"
  },
  {
    "text": "and then you predict the\nanswer to this question. You predict, as a\nresult of my actions,",
    "start": "4140359",
    "end": "4146600"
  },
  {
    "text": "what is the outcome in terms\nof these specific metrics? ",
    "start": "4146600",
    "end": "4152939"
  },
  {
    "text": "This actually has a pretty\nclose connection to Q-learning, so if you actually\nset the reward to be the probability\nof the event happening,",
    "start": "4152939",
    "end": "4159660"
  },
  {
    "text": "then this actually corresponds\nvery closely to Q-learning.",
    "start": "4159660",
    "end": "4164818"
  },
  {
    "text": "And this can be used for things\nlike grasping, and avoiding collisions, and so\nforth, and allows",
    "start": "4164819",
    "end": "4170640"
  },
  {
    "text": "you to only predict\nthe quantities that are relevant to the task. The downside, of course, is\nthat you need to manually pick",
    "start": "4170640",
    "end": "4177089"
  },
  {
    "text": "these quantities,\nand you must be able to directly observe them. In grasping, you can observe\nwhen you've got something",
    "start": "4177090",
    "end": "4183909"
  },
  {
    "text": "by looking at your fingers\nand seeing if your fingers are closed or not. If your fingers aren't\nall the way closed,",
    "start": "4183910",
    "end": "4190180"
  },
  {
    "text": "that suggests that you're\nprobably grasping something, and so forth. But in a variety of situations\nlike, for example the spatula",
    "start": "4190180",
    "end": "4197810"
  },
  {
    "text": "task, you can't directly\nobserve whether an object is-- like you can't directly\nobserve a binary indicator for",
    "start": "4197810",
    "end": "4204230"
  },
  {
    "text": "whether or not the\nobject is in the bowl.",
    "start": "4204230",
    "end": "4209920"
  },
  {
    "text": "Once you learn a\nmodel like this, it's actually quite simple\nto actually perform the task. So for grasping for example,\nif you have something",
    "start": "4209920",
    "end": "4217300"
  },
  {
    "text": "it tells you given a\nsequence of actions whether or not you'll\nsuccessfully grasp, then you can then optimize\nover a set of actions that",
    "start": "4217300",
    "end": "4224380"
  },
  {
    "text": "lead to successful grasping. ",
    "start": "4224380",
    "end": "4232910"
  },
  {
    "text": "OK. So those are kind of the three\ncore approaches for model based reinforcement\nlearning when you have",
    "start": "4232910",
    "end": "4239780"
  },
  {
    "text": "high dimensional observations.  So just to wrap\nup, I'll talk a bit",
    "start": "4239780",
    "end": "4248199"
  },
  {
    "text": "about some of the takeaways\nbetween model based methods and model-free methods. So today we talked a\nlot about models, Monday",
    "start": "4248200",
    "end": "4255640"
  },
  {
    "text": "last week we talked a lot\nabout model-free methods. What are kind of the trade\noffs when considering these kinds of methods?",
    "start": "4255640",
    "end": "4262120"
  },
  {
    "text": "So the benefit of\nmodel based methods is that it's easy to collect\ndata in a very scalable way",
    "start": "4262120",
    "end": "4267574"
  },
  {
    "text": "because you can train a model\nwith data that doesn't even have rewards labeled. It's also easy to transfer\nmodels across reward functions,",
    "start": "4267575",
    "end": "4276070"
  },
  {
    "text": "and they typically\nrequire a smaller quantity of data that's supervised\nwith a reward in practice.",
    "start": "4276070",
    "end": "4283430"
  },
  {
    "text": "The downside is that models\ndon't optimize directly for task performance,\nand so there's",
    "start": "4283430",
    "end": "4289010"
  },
  {
    "text": "a bit of a mismatch\nbetween the objective, and sometimes the models\nmay miss quantities that are important for the task.",
    "start": "4289010",
    "end": "4295430"
  },
  {
    "text": "And sometimes the\nmodel is harder to learn than the policy. So for example, training\na video prediction model",
    "start": "4295430",
    "end": "4301010"
  },
  {
    "text": "may be more difficult\nthan just predicting what the correct action is. ",
    "start": "4301010",
    "end": "4306980"
  },
  {
    "text": "And then lastly, you often\nneed some assumptions in order to learn more complex\nskills, because you",
    "start": "4306980",
    "end": "4312002"
  },
  {
    "text": "need to be able to\nfigure out-- you need to be able to\nlearn a model that",
    "start": "4312002",
    "end": "4317710"
  },
  {
    "text": "is kind of suitable for those\nmore challenging interactions. For example, if you have a\nlot of kind of harsh contact",
    "start": "4317710",
    "end": "4324250"
  },
  {
    "text": "dynamics, then\nlearning a model that can accurately predict\nthe contact dynamics can be challenging.",
    "start": "4324250",
    "end": "4330880"
  },
  {
    "text": "For model-free\nmethods, the upside is that they make very\nlittle assumptions beyond having access\nto a reward function,",
    "start": "4330880",
    "end": "4337340"
  },
  {
    "text": "although having access\nto a reward function can be a substantial assumption\nin some applications.",
    "start": "4337340",
    "end": "4343970"
  },
  {
    "text": "They're very effective for\nlearning very complex policies or learning very complex\ntasks, because they just",
    "start": "4343970",
    "end": "4349340"
  },
  {
    "text": "have to learn a\npolicy they don't have to learn how the world works. The downside is that they\ntend to require a lot of data,",
    "start": "4349340",
    "end": "4355489"
  },
  {
    "text": "they're slower to learn,\nand it can involve a harder optimization problem in the\nmultitask setting because you",
    "start": "4355490",
    "end": "4363349"
  },
  {
    "text": "aren't just training for a\nsingle model of how things work, so you're trying to\noptimize for many different Q functions or--",
    "start": "4363350",
    "end": "4369438"
  },
  {
    "text": "basically the Q-function\nfor many different tasks or the policy for\nmany different tasks.",
    "start": "4369438",
    "end": "4375570"
  },
  {
    "text": "And ultimately, it's likely\nthat we probably want elements both of model-based learning\nand of model-free learning.",
    "start": "4375570",
    "end": "4381432"
  },
  {
    "text": "And they kind of\nboth have benefits in different scenarios. ",
    "start": "4381432",
    "end": "4386850"
  },
  {
    "text": "OK. So that's it for today.",
    "start": "4386850",
    "end": "4391920"
  },
  {
    "text": "We kind of went over the basics\nof model-based reinforcement learning, including\nhow to implement these kinds of algorithms,\nhow model based",
    "start": "4391920",
    "end": "4398570"
  },
  {
    "text": "RL can be used for\nmultitask problems, and challenges and\nstrategies for handling",
    "start": "4398570",
    "end": "4403639"
  },
  {
    "text": "high dimensional state\nspaces with model-based RL. Next time we'll talk about\nmeta-reinforcement learning.",
    "start": "4403640",
    "end": "4410906"
  },
  {
    "text": "So, so far we've talked\nabout different ways to solve multitask problems,\nand for the next three lectures we'll focus on solving\nmeta-reinforcement learning",
    "start": "4410907",
    "end": "4419310"
  },
  {
    "text": "problems where you care\nabout being to quickly learn a new task, rather\nthan just learning a set of training tasks.",
    "start": "4419310",
    "end": "4426969"
  },
  {
    "text": "Hi. I was wondering if you\ncould go back to the part where you talk about learning\nmapping from the image",
    "start": "4426970",
    "end": "4435730"
  },
  {
    "text": "to a physically meaningful\nlatent representation? Yeah.",
    "start": "4435730",
    "end": "4442110"
  },
  {
    "text": "I think that one on the right. And I guess I was a bit confused\nabout the whole spatial softmax",
    "start": "4442110",
    "end": "4450440"
  },
  {
    "text": "stuff going on and how\nyou ended up with this. I thought was really cool that\nyou end up with the latent",
    "start": "4450440",
    "end": "4455846"
  },
  {
    "text": "representation that actually\ncorresponded to the points, but I wanted to understand a\nbit better how that worked.",
    "start": "4455847",
    "end": "4461290"
  },
  {
    "text": "Yeah. So the way that it works\nis we kind of assume that-- well, I guess first we\nbasically have this operation",
    "start": "4461290",
    "end": "4468460"
  },
  {
    "text": "inside the neural network. But I'll first kind of emphasize\nthat this autoencoder is trained end to end with\nregard to the reconstruction",
    "start": "4468460",
    "end": "4475510"
  },
  {
    "text": "objective, so it's\nbasically optimizing for a set of 2D points,\nsuch that it can reconstruct",
    "start": "4475510",
    "end": "4481730"
  },
  {
    "text": "the image effectively. Now the special part\nabout this autoencoder is this operation\nwhere we're essentially",
    "start": "4481730",
    "end": "4487969"
  },
  {
    "text": "converting these feature\nmaps to these 2D points. And the way that we're\nconstraining this",
    "start": "4487970",
    "end": "4493699"
  },
  {
    "text": "is we basically take a softmax\nover the extent of each of the feature maps.",
    "start": "4493700",
    "end": "4499910"
  },
  {
    "text": "This gives us a distribution\nover each of the feature maps, and then we can take the\nexpectation, which is written",
    "start": "4499910",
    "end": "4505970"
  },
  {
    "text": "by this equation right here. We'll take the expectation\nover the kind of x extent,",
    "start": "4505970",
    "end": "4511550"
  },
  {
    "text": "which is shown here,\nwhich is basically computing the average--",
    "start": "4511550",
    "end": "4517720"
  },
  {
    "text": "sorry, so sij is basically\nthe probability distribution.",
    "start": "4517720",
    "end": "4522760"
  },
  {
    "text": "s is the probability\nvalue for pixel i of j, and this is computing\nthe average over i",
    "start": "4522760",
    "end": "4530230"
  },
  {
    "text": "and the average over j. And the result of\nthis is basically kind of the expected\nvalue of that distribution",
    "start": "4530230",
    "end": "4537580"
  },
  {
    "text": "over coordinates, which\nwill give you a kind of a 2D coordinate in this space.",
    "start": "4537580",
    "end": "4543730"
  },
  {
    "text": "And both of these\noperations, the softmax and this expectation, are\ndifferentiable operations",
    "start": "4543730",
    "end": "4550989"
  },
  {
    "text": "which allow us to be\nable to back propagate the reconstruction\nobjective into the earlier",
    "start": "4550990",
    "end": "4556420"
  },
  {
    "text": "layers of the network. OK. So each of the 16 distributions\ncould be its own object.",
    "start": "4556420",
    "end": "4563670"
  },
  {
    "text": "It's like a probability\ndistribution over where that object is? Yeah. So it can basically correspond\nto probability distributions",
    "start": "4563670",
    "end": "4569698"
  },
  {
    "text": "over the location of\ndifferent objects. We give it 16 so that it\ncan kind of choose how",
    "start": "4569698",
    "end": "4575310"
  },
  {
    "text": "it wants to use that itself. And based on how\nit's initialized, it may be easier for\nsome distributions",
    "start": "4575310",
    "end": "4580469"
  },
  {
    "text": "to represent some\nobjects than others. You may not have 16 objects\nin practice, of course.",
    "start": "4580470",
    "end": "4585880"
  },
  {
    "text": "So here we only have like two\nprimary objects of interest. ",
    "start": "4585880",
    "end": "4591449"
  },
  {
    "text": "Yeah, and this is what comes out\nof this kind of architecture.",
    "start": "4591450",
    "end": "4596620"
  },
  {
    "text": "OK. And you get the\nfull autoencoder-- like you train this on\nits own before using it in any decision making?",
    "start": "4596620",
    "end": "4602969"
  },
  {
    "text": "Like just the\nautoencoder training. Yeah, exactly. OK. Thank you, that helped a lot.",
    "start": "4602970",
    "end": "4608950"
  },
  {
    "text": "Great.  Yes, so I have a question on\ndirectly modeling observation",
    "start": "4608950",
    "end": "4618739"
  },
  {
    "text": "space. So I'm wondering so how can you\ndefine the objective using only",
    "start": "4618740",
    "end": "4624470"
  },
  {
    "text": "the observations? Yeah, so the model\nlearning part is just",
    "start": "4624470",
    "end": "4632010"
  },
  {
    "text": "kind of a generative model. I assume that you mean the\nobjective for the planning process? Yes, the planning process.",
    "start": "4632010",
    "end": "4637260"
  },
  {
    "text": "Yeah. So there's a few\ndifferent ways to do this.",
    "start": "4637260",
    "end": "4642642"
  },
  {
    "text": "I think I probably don't\nhave a backup slide on it. Yeah, so there's a few different\nways that you can do it.",
    "start": "4642642",
    "end": "4648760"
  },
  {
    "text": "One is to train a classifier. Actually, the classifier is\nwhat was done in this earlier",
    "start": "4648760",
    "end": "4655748"
  },
  {
    "text": "example, I didn't actually tell\nyou that it was using images. But this example right\nhere was actually",
    "start": "4655748",
    "end": "4661500"
  },
  {
    "text": "training using a\nbinary classifier. And we evaluated that\nbinary classifier on the predicted images.",
    "start": "4661500",
    "end": "4666525"
  },
  {
    "text": " And that's kind of one pretty\neffective way for doing this.",
    "start": "4666525",
    "end": "4673110"
  },
  {
    "text": "In these cases, we're\nactually doing it a little bit differently. So we're telling it to move a\npixel to a certain position,",
    "start": "4673110",
    "end": "4680070"
  },
  {
    "text": "and then you need to actually\nevaluate for a video prediction where the pixels moved.",
    "start": "4680070",
    "end": "4686639"
  },
  {
    "text": "And the way that we\ndid it in this case is we actually used a\nvideo prediction model that isn't trying to directly\noutput pixel values,",
    "start": "4686640",
    "end": "4694200"
  },
  {
    "text": "but it's actually outputting\nhow pixels will move, and it's doing that in\na differentiable way.",
    "start": "4694200",
    "end": "4700800"
  },
  {
    "text": "And this allows us\nto kind of implicitly use the model's\npredictions about how",
    "start": "4700800",
    "end": "4706950"
  },
  {
    "text": "pixels move in the\nobjective itself, actually. So the way that it works is--",
    "start": "4706950",
    "end": "4713177"
  },
  {
    "text": "I didn't really want to\ngo into these full details in the lecture, but\nthe way that it works is it predicts\nthese kernels that",
    "start": "4713177",
    "end": "4719460"
  },
  {
    "text": "predict how images will\nmove, these kernels are applied to the image to\npredict a transformed image.",
    "start": "4719460",
    "end": "4725708"
  },
  {
    "text": "And then these different\ntransformed images are composed to produce\nthe image itself,",
    "start": "4725708",
    "end": "4731730"
  },
  {
    "text": "and then this whole\narchitecture is trained end to end with regard\nto the reconstruction error of this frame.",
    "start": "4731730",
    "end": "4737100"
  },
  {
    "text": "And then when we're\nplanning for moving pixels, we actually look at\nthe kernels itself",
    "start": "4737100",
    "end": "4742890"
  },
  {
    "text": "and how it predicts that\npixels will move in the future, and we use that to\nevaluate the reward.",
    "start": "4742890",
    "end": "4752460"
  },
  {
    "text": "OK, so from my understanding\nthat this kind of definition of the goals still is same\nkind of use as the latent",
    "start": "4752460",
    "end": "4761670"
  },
  {
    "text": "space for example, when\ntraining a classifier, and I can view the last\nlayer before the output",
    "start": "4761670",
    "end": "4767849"
  },
  {
    "text": "as some kind of latent space. So you can view it like that.",
    "start": "4767850",
    "end": "4777170"
  },
  {
    "text": "The last layer isn't being used\nin any way of the classifier, so we're only using kind of\nthe output of the classifier",
    "start": "4777170",
    "end": "4786860"
  },
  {
    "text": "when we're planning,\nwe're just using whether it predicts\nthat this was a success, or whether it predicts\nthat it was a failure.",
    "start": "4786860",
    "end": "4793160"
  },
  {
    "text": "And then the planning is\ndone in the image space, and the producing images are\npassing through the classifier.",
    "start": "4793160",
    "end": "4799490"
  },
  {
    "text": "Yeah, so for example when\nyou optimize the model",
    "start": "4799490",
    "end": "4805620"
  },
  {
    "text": "in the observation space, when\nyou use the images directly.",
    "start": "4805620",
    "end": "4811170"
  },
  {
    "text": "So for example, if you're\nstill using L2 loss. So I kind of think\nthat this kind of loss",
    "start": "4811170",
    "end": "4817260"
  },
  {
    "text": "may not characterize\nthe situation very well, for example. Yeah. ",
    "start": "4817260",
    "end": "4824460"
  },
  {
    "text": "So for the model training,\na lot of the models I showed were actually deterministic\nand optimized with regard",
    "start": "4824460",
    "end": "4830280"
  },
  {
    "text": "to an L2 loss. But a lot of the kind of\nmore recent models that",
    "start": "4830280",
    "end": "4835350"
  },
  {
    "text": "are very accurate don't\nuse L2 loss directly. So this model for example,\nuses latent variables,",
    "start": "4835350",
    "end": "4842530"
  },
  {
    "text": "like a VAE style model to\ncapture its uncertainty and what will happen\nin the future.",
    "start": "4842530",
    "end": "4850020"
  },
  {
    "text": "These latent variables aren't\nequivalent to a latent space. They aren't capturing kind\nof exactly what the state is,",
    "start": "4850020",
    "end": "4857625"
  },
  {
    "text": "but they are able\nto capture kind of the uncertainty\nabout the future.",
    "start": "4857625",
    "end": "4865270"
  },
  {
    "text": "So you can use like a\nVAE style loss, which is kind of L2 reconstruction--\nor something like L2",
    "start": "4865270",
    "end": "4871110"
  },
  {
    "text": "reconstruction plus a KL term. There are also models that use\nGAN style losses to optimize",
    "start": "4871110",
    "end": "4876719"
  },
  {
    "text": "the model predictions,\nand there are also methods that use kind\nof optimized likelihood",
    "start": "4876720",
    "end": "4882630"
  },
  {
    "text": "for example. Thank you so much. Great.",
    "start": "4882630",
    "end": "4888080"
  },
  {
    "text": "And then [MUTED]\nasking in the chat, explaining why hindsight\nexperience replay works well.",
    "start": "4888080",
    "end": "4893820"
  },
  {
    "text": "So let's go back to\nthe beginning here.",
    "start": "4893820",
    "end": "4900400"
  },
  {
    "text": "And actually, I like doing\nit using this example. So one of the reasons\nwhy it works well,",
    "start": "4900400",
    "end": "4906437"
  },
  {
    "text": "or kind of the\nintuition, is that if you have a lot of different tasks\nthat you want to accomplish,",
    "start": "4906437",
    "end": "4913210"
  },
  {
    "text": "a lot of different goals\nthat you want to figure out how to reach, a standard\napproach would just",
    "start": "4913210",
    "end": "4918580"
  },
  {
    "text": "keep the datas completely\nseparate for these different goals and basically practice\nthis task of passing a lot,",
    "start": "4918580",
    "end": "4925465"
  },
  {
    "text": "practice this task of\nshooting a lot, and so forth, and keep the data\ncompletely separate.",
    "start": "4925465",
    "end": "4930550"
  },
  {
    "text": "However, when you\nattempt one task, you often end up kind of\nreaching a different task,",
    "start": "4930550",
    "end": "4936250"
  },
  {
    "text": "or reaching a different\ngoal on accident. And you can use those\naccidental successes to actually",
    "start": "4936250",
    "end": "4941980"
  },
  {
    "text": "help train the other tasks. And by using those, you\ncan actually kind of share",
    "start": "4941980",
    "end": "4947650"
  },
  {
    "text": "data across tasks. And when you're trying\nto reach a large number of different goals,\nthis process allows",
    "start": "4947650",
    "end": "4953890"
  },
  {
    "text": "you to be significantly\nmore efficient, basically by sharing data across\nall of your tasks,",
    "start": "4953890",
    "end": "4959590"
  },
  {
    "text": "than if you were to try to\nattempt the task individually. Another reason for\nwhy it works is",
    "start": "4959590",
    "end": "4965500"
  },
  {
    "text": "that if one task is\neasier than another task, they can actually help provide\na little bit of a curriculum",
    "start": "4965500",
    "end": "4972670"
  },
  {
    "text": "because practicing\nthat task will give you some reward signal for\nthe correct solution. And that will allow you to--",
    "start": "4972670",
    "end": "4978700"
  },
  {
    "text": " getting some kind\nof signal for that",
    "start": "4978700",
    "end": "4984040"
  },
  {
    "text": "will actually allow you to\nexplore in different ways because you'll know\nhow to solve that. And then when you\nsolve that task,",
    "start": "4984040",
    "end": "4989770"
  },
  {
    "text": "you will accidentally solve\nother tasks, and so forth. So kind of the intuition is\nthat it allows you to share data",
    "start": "4989770",
    "end": "4997110"
  },
  {
    "text": "across different\ngoals, and it gives you kind of a signal for-- ",
    "start": "4997110",
    "end": "5002630"
  },
  {
    "text": "it gives you kind\nof a reward signal for things that you accomplished\non accident when trying",
    "start": "5002630",
    "end": "5008630"
  },
  {
    "text": "to solve a different task. Oh, so there's\none more question. So while choosing\ntasks, would this",
    "start": "5008630",
    "end": "5014890"
  },
  {
    "text": "also mean that the\nentropy of the task goals for the tasks\nwe choose should be high to maximize this?",
    "start": "5014890",
    "end": "5021550"
  },
  {
    "text": " Right. So when you're kind of\ngoing about collecting data,",
    "start": "5021550",
    "end": "5028470"
  },
  {
    "text": "you need to choose some\ndistribution over goals that you want to pass your\npolicy to try to accomplish.",
    "start": "5028470",
    "end": "5035593"
  },
  {
    "text": "There are some\napproaches that basically try to reach a much\nwider range of goals",
    "start": "5035593",
    "end": "5041849"
  },
  {
    "text": "in order to help explore\nrather than just reaching a very narrow set of goals. And this sort of kind of\nmaximum entropy over goals",
    "start": "5041850",
    "end": "5049470"
  },
  {
    "text": "can be an effective technique\nfor trying to explore. ",
    "start": "5049470",
    "end": "5055400"
  },
  {
    "text": "So one example paper that\nyou could look at that does something like\nthat is called Skew-Fit",
    "start": "5055400",
    "end": "5063800"
  },
  {
    "text": "and it's by Vitchyr Pong et al. ",
    "start": "5063800",
    "end": "5070540"
  },
  {
    "text": "OK, great. I'll see everyone. ",
    "start": "5070540",
    "end": "5079000"
  }
]