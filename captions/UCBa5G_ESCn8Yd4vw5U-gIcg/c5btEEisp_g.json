[
  {
    "start": "0",
    "end": "6000"
  },
  {
    "start": "0",
    "end": "5225"
  },
  {
    "text": "Hi. In this module, I'm going\nto briefly introduce the idea of differentiable\nprogramming.",
    "start": "5225",
    "end": "10740"
  },
  {
    "start": "6000",
    "end": "47000"
  },
  {
    "text": "And differentiable\nprogramming kind of just runs off with ideas\nof computation graphs,",
    "start": "10740",
    "end": "15930"
  },
  {
    "text": "some back propagation\nthat we developed for simple neural networks. There's really\nenough to say here,",
    "start": "15930",
    "end": "21810"
  },
  {
    "text": "to fill up an entire\ncourse at least. So I'm going to try to keep\nthings pretty high level",
    "start": "21810",
    "end": "27640"
  },
  {
    "text": "but I will try to highlight\nthe power of composition. So differentiable programming\nis closely related",
    "start": "27640",
    "end": "34855"
  },
  {
    "text": "to deep learning. I've adopted the former\nterm as an attempt to be more precise in\nterms of highlighting",
    "start": "34855",
    "end": "42190"
  },
  {
    "text": "the mechanics of writing\nmodels as you would code. ",
    "start": "42190",
    "end": "48280"
  },
  {
    "start": "47000",
    "end": "84000"
  },
  {
    "text": "So if you look around\nat deep learning today, there's some pretty\ncomplex models which have many layers,\nretention mechanisms,",
    "start": "48280",
    "end": "55379"
  },
  {
    "text": "residual connections\nto name a few. And this could be quite\noverwhelming at first glance,",
    "start": "55380",
    "end": "61770"
  },
  {
    "text": "but when you look\ncloser, you notice that these complex\nmodels are actually composed of functions and\nthese functions themselves are",
    "start": "61770",
    "end": "69750"
  },
  {
    "text": "composed of smaller functions. So this is the programming part\nof differentiable programming",
    "start": "69750",
    "end": "75130"
  },
  {
    "text": "which allows you to build\nup an increasingly more sophisticated model\nwithout losing track of what's going on.",
    "start": "75130",
    "end": "81570"
  },
  {
    "text": " So let's begin with our familiar\nexample, the three layer neural",
    "start": "81570",
    "end": "88390"
  },
  {
    "start": "84000",
    "end": "263000"
  },
  {
    "text": "network. So remember that in a\nthree layer neural network, we start with our\nfeature vector.",
    "start": "88390",
    "end": "95740"
  },
  {
    "text": "In this case, it's a\nsix dimensional vector. And we left multiply\nby a matrix.",
    "start": "95740",
    "end": "101740"
  },
  {
    "text": "I've drawn some lines here to\nhelp us interpret this matrix as a set of rows where each row\ncorresponds to a hidden unit.",
    "start": "101740",
    "end": "110710"
  },
  {
    "text": "And I'm going to\ntake the dot product of each row with\nthe input vector to produce a hidden\nvector of dimension 4.",
    "start": "110710",
    "end": "120760"
  },
  {
    "text": "I'm going to add a\nbias term and then I'm going to apply an\nactivation function.",
    "start": "120760",
    "end": "125920"
  },
  {
    "text": "Element-wise, for example,\nthe ReLU or logistic. Now I have a vector and now I\ncan do the same thing again.",
    "start": "125920",
    "end": "135939"
  },
  {
    "text": "I apply a matrix,\nadd a bias term, apply an activation function.",
    "start": "135940",
    "end": "141700"
  },
  {
    "text": "Apply a matrix which happens to\nbe a vector, so I get a scalar and I add a simple\nscalar bias term.",
    "start": "141700",
    "end": "148160"
  },
  {
    "text": "And I get a score which\nthen I can happily drive regression or take the\nsign to drive classification.",
    "start": "148160",
    "end": "157250"
  },
  {
    "text": "So what I want to do now is to\nfactor out this kind of complex",
    "start": "157250",
    "end": "164140"
  },
  {
    "text": "looking expression into a\nreusable component which I'm going to call FeedForward.",
    "start": "164140",
    "end": "169750"
  },
  {
    "text": "But we're going to see a\nlot of these box diagrams which are going to represent\nfunctions that we can reuse",
    "start": "169750",
    "end": "176110"
  },
  {
    "text": "and have a nice interpretation. So the FeedForward function\ntakes in an input vector x",
    "start": "176110",
    "end": "182500"
  },
  {
    "text": "and produces an output\nvector which could be of a different dimensionality.",
    "start": "182500",
    "end": "189590"
  },
  {
    "text": "And the way to interpret\nwhat people are doing is performing one\nstep of processing.",
    "start": "189590",
    "end": "195260"
  },
  {
    "text": "In particular what\nthat processing is, is taking this input vector,\nmultiplying it by a matrix,",
    "start": "195260",
    "end": "202850"
  },
  {
    "text": "adding a bias term and applying\nan activation function, OK?",
    "start": "202850",
    "end": "207960"
  },
  {
    "text": "So this is a function\nor a program, but unlike normal\nprogramming it's",
    "start": "207960",
    "end": "213750"
  },
  {
    "text": "under specified because\nthe red numbers here are parameters which are\nprivate to this function which",
    "start": "213750",
    "end": "221040"
  },
  {
    "text": "are going to be set and tuned\nlater via back propagation.",
    "start": "221040",
    "end": "226890"
  },
  {
    "text": "So now we can write our\nthree layer neural network using FeedForward\nand the way I'm",
    "start": "226890",
    "end": "232200"
  },
  {
    "text": "going to do this is\nscore is equal to you take x or distribute phi of x.",
    "start": "232200",
    "end": "242100"
  },
  {
    "text": "And you apply FeedForward,\nFeedForward, FeedForward. And you can write\nthis as FeedForward",
    "start": "242100",
    "end": "249299"
  },
  {
    "text": "cubed as to be more compact. So this is a very compact way\nof writing something that would",
    "start": "249300",
    "end": "256458"
  },
  {
    "text": "otherwise be quite complicated. ",
    "start": "256459",
    "end": "263420"
  },
  {
    "start": "263000",
    "end": "318000"
  },
  {
    "text": "So now let's suppose you want\nto do image classification. We need some way of\nrepresenting images.",
    "start": "263420",
    "end": "270940"
  },
  {
    "text": "So the FeedForward\nfunction that we just introduced, takes\na vector as input and we can represent an\nimage as a long vector",
    "start": "270940",
    "end": "279180"
  },
  {
    "text": "by, for example,\nadding all the rows. But then we would\nhave this huge matrix",
    "start": "279180",
    "end": "285479"
  },
  {
    "text": "that we would need to be able to\ntransform this vector resulting in a lot of parameters which\nmay make life difficult.",
    "start": "285480",
    "end": "294630"
  },
  {
    "text": "And the problem here is\nthat we're not really using the spatial\nstructure of images.",
    "start": "294630",
    "end": "302020"
  },
  {
    "text": "For example, if I just permuted\nall the elements of this vector and retrain, I would basically\nI would get the identical model.",
    "start": "302020",
    "end": "311760"
  },
  {
    "text": "So it's kind of not\npaying attention to which pixels are close by.",
    "start": "311760",
    "end": "317740"
  },
  {
    "text": "To fix this problem,\nwe introduce convolutional neural\nnetworks which",
    "start": "317740",
    "end": "322780"
  },
  {
    "start": "318000",
    "end": "629000"
  },
  {
    "text": "is a refinement of a fully\nconnected neural network. So here is an example\nof ConvNet in action.",
    "start": "322780",
    "end": "331680"
  },
  {
    "text": "So here's a car and\nyou can see that it goes through a number of\nlayers and over time it",
    "start": "331680",
    "end": "339449"
  },
  {
    "text": "computes increasingly abstract\nrepresentations of the image. And at the end, you get\na vector representing",
    "start": "339450",
    "end": "347310"
  },
  {
    "text": "the probabilities of the\ndifferent object categories. So if you want to\nplay with ConvNets,",
    "start": "347310",
    "end": "353669"
  },
  {
    "text": "you can actually click\nhere for Andrej Karpathy's excellent demo where you can\nactually create and train",
    "start": "353670",
    "end": "360000"
  },
  {
    "text": "ConvNets in your browser. So another comment is\nthat ConvNets, we're",
    "start": "360000",
    "end": "366060"
  },
  {
    "text": "going to introduce\nthem for 2D images but they can also be applied\nto text or sequences which",
    "start": "366060",
    "end": "373259"
  },
  {
    "text": "are 1D or videos\nwhich are [INAUDIBLE].. ",
    "start": "373260",
    "end": "378760"
  },
  {
    "text": "So ConvNets have two\nbasic building blocks. We're not going to go\nthrough the details.",
    "start": "378760",
    "end": "385030"
  },
  {
    "text": "You can take CS231 if you want\nto learn all about ConvNets. But instead I'm going to\nfocus on the interface",
    "start": "385030",
    "end": "391870"
  },
  {
    "text": "and show how these\nmodules compare. So the first is Conv.",
    "start": "391870",
    "end": "398640"
  },
  {
    "text": "And so Conv takes an\nimage and the image is going to be represented\nas a volume which",
    "start": "398640",
    "end": "404970"
  },
  {
    "text": "is a collection of matrices,\none for each channel, red, green, blue.",
    "start": "404970",
    "end": "410820"
  },
  {
    "text": "Each matrix has the\nsame dimensionality as the image, height by width.",
    "start": "410820",
    "end": "417870"
  },
  {
    "text": "And what the Conv\nis going to do is it's going to compute\nanother volume of a slightly",
    "start": "417870",
    "end": "423560"
  },
  {
    "text": "different size, usually the\nheight and width of this volume is going to be equal or\nmaybe slightly smaller",
    "start": "423560",
    "end": "430100"
  },
  {
    "text": "than the input volume and\nthe number of channels is going to be\nsomewhat different.",
    "start": "430100",
    "end": "436580"
  },
  {
    "text": "The way that Conv is going\nto compute this volume is via a sequence of\nfilters, and intuitively",
    "start": "436580",
    "end": "443650"
  },
  {
    "text": "what it's going to\ndo is try to detect local patterns with [AUDIO OUT]\nSo here is one filter",
    "start": "443650",
    "end": "450730"
  },
  {
    "text": "and how it works is\nI'm going to slide",
    "start": "450730",
    "end": "456010"
  },
  {
    "text": "this filter across the image. And if I put the\nfilter here, I'm",
    "start": "456010",
    "end": "462070"
  },
  {
    "text": "going to kind of align it\nup with the first pixels on the image.",
    "start": "462070",
    "end": "468340"
  },
  {
    "text": "I'm going to compute the dot\nproduct between the eight numbers here and the--",
    "start": "468340",
    "end": "473820"
  },
  {
    "text": "actually, sorry, 12 numbers\nhere and the 12 numbers here. I got a single number which I'm\ngoing to write into this entry.",
    "start": "473820",
    "end": "480340"
  },
  {
    "text": "I slide the filter\nover a little bit. I'm going to write into\nthe second entry and so on.",
    "start": "480340",
    "end": "486550"
  },
  {
    "text": "And then for the\nsecond filter, I'm going to use to fill up\nthe second output channel. So the number of filters is\nthe number of output channels.",
    "start": "486550",
    "end": "494440"
  },
  {
    "text": "OK. So that's all I'm going\nto say about Conv. The second operation\nis MaxPool which",
    "start": "494440",
    "end": "501750"
  },
  {
    "text": "again takes an input\nvolume and then it produces a smaller\noutput volume.",
    "start": "501750",
    "end": "508169"
  },
  {
    "text": "It's going to have the\nsame number of channels. And for every slice\nthrough the matrix, it's going to slide a\nlittle max operation",
    "start": "508170",
    "end": "518969"
  },
  {
    "text": "over every 2x2 or 3x3 region. So the max over\nthese four numbers",
    "start": "518970",
    "end": "525240"
  },
  {
    "text": "is going to be used to build\nthis [INAUDIBLE] and so on.",
    "start": "525240",
    "end": "530550"
  },
  {
    "text": "OK. That's all I'm going\nto say about MaxPool. If you want to go\ninto the details,",
    "start": "530550",
    "end": "536670"
  },
  {
    "text": "you can check out this demo\nor you can learn more in 231.",
    "start": "536670",
    "end": "542750"
  },
  {
    "text": "But again, I want to highlight\nthat there's these two modules. One for detecting patterns\nand one for aggregating,",
    "start": "542750",
    "end": "549050"
  },
  {
    "text": "to kind of reduce\nthe dimensionality. And with these two functions\nalong with FeedForward,",
    "start": "549050",
    "end": "556639"
  },
  {
    "text": "now we can define AlexNet which\nwas the seminal CNN from 2012",
    "start": "556640",
    "end": "563510"
  },
  {
    "text": "that won the\nImageNet competition and really transformed\n[INAUDIBLE]",
    "start": "563510",
    "end": "568580"
  },
  {
    "text": "So how this works is I'm going\nto start with my input image,",
    "start": "568580",
    "end": "573650"
  },
  {
    "text": "apply a convolutional\nlayer, apply MaxPool, apply another convolutional\nlayer, apply MaxPool.",
    "start": "573650",
    "end": "579949"
  },
  {
    "text": "Apply three more convolutional\nlayers, apply MaxPool, and then apply three\nlayers of FeedForward.",
    "start": "579950",
    "end": "588310"
  },
  {
    "text": "OK. So in one line I have AlexNet. Now of course, I've under\nspecified a couple of things",
    "start": "588310",
    "end": "596130"
  },
  {
    "text": "here. One is I haven't\nspecified the parameters.",
    "start": "596130",
    "end": "601930"
  },
  {
    "text": "Those are to be learned. And each of these functions\nholds a private set",
    "start": "601930",
    "end": "608079"
  },
  {
    "text": "of parameters that\nneed to be learned. The second thing is I\nalso haven't specified the hyperparameters which\nis the number of channels,",
    "start": "608080",
    "end": "615250"
  },
  {
    "text": "the filter sizes, and so\non, which are actually pretty important for\ngetting a good performance. But I just wanted to highlight\nthe overarching structure",
    "start": "615250",
    "end": "623300"
  },
  {
    "text": "and the idea that\nyou can compose in a fairly effortless way.",
    "start": "623300",
    "end": "629519"
  },
  {
    "start": "629000",
    "end": "711000"
  },
  {
    "text": "So now let's turn our attention\nto natural language processing. So here is a motivating example.",
    "start": "629520",
    "end": "635330"
  },
  {
    "text": "Suppose we want to build a\nquestion answering system. We have a paragraph.",
    "start": "635330",
    "end": "641060"
  },
  {
    "text": "It's from Wikipedia. We have a question and we\nwant to select the answer",
    "start": "641060",
    "end": "647000"
  },
  {
    "text": "from that passage,\nfrom the paragraph. So this happens to be from\nthe SQuAD question answering",
    "start": "647000",
    "end": "653130"
  },
  {
    "text": "benchmark. Let's just read this. In meteorology,\nprecipitation is any product",
    "start": "653130",
    "end": "659400"
  },
  {
    "text": "of the condensation\nof atmosphere water vapor that falls under gravity. And the question is, what\ncauses precipitation to fall,",
    "start": "659400",
    "end": "667950"
  },
  {
    "text": "and the answer is gravity. So to do question\nanswering, you have",
    "start": "667950",
    "end": "673709"
  },
  {
    "text": "to do a fair amount\nof processing. So you somehow have to relate\nthe question with the paragraph",
    "start": "673710",
    "end": "682140"
  },
  {
    "text": "but it's not an exact match. Some of the words match like\nprecipitation but some of them",
    "start": "682140",
    "end": "687360"
  },
  {
    "text": "are kind of more subtle,\nlike causes is somehow related to product.",
    "start": "687360",
    "end": "692730"
  },
  {
    "text": "And also the fact that\nsome words are ambiguous, like product can be--",
    "start": "692730",
    "end": "699620"
  },
  {
    "text": "multiplication or output. So there's a lot of processing\nthat needs to happen",
    "start": "699620",
    "end": "708290"
  },
  {
    "text": "and it's hard to kind\nof specify in advance. ",
    "start": "708290",
    "end": "714290"
  },
  {
    "start": "711000",
    "end": "781000"
  },
  {
    "text": "So first things first. So words are discrete\nobjects and neural networks speak vectors.",
    "start": "714290",
    "end": "720630"
  },
  {
    "text": "So whenever you're doing\nNLP with neural nets, you first have to embed words,\nor more generally, tokens.",
    "start": "720630",
    "end": "731730"
  },
  {
    "text": "So we're going to define\nan EmbedToken function that takes a word or a token x\nand maps it into a vector.",
    "start": "731730",
    "end": "740040"
  },
  {
    "text": "And all this function\nis going to do is it's going to look up\nvector in a dictionary that has a static set of\nvectors associated",
    "start": "740040",
    "end": "748770"
  },
  {
    "text": "with particular tokens. So this is fine if you\nhave a sequence of words",
    "start": "748770",
    "end": "759980"
  },
  {
    "text": "then you can just embed\neach word into a vector to get a sequence of vectors.",
    "start": "759980",
    "end": "766250"
  },
  {
    "text": "There's one problem which is\nthat the meaning of the words and tokens depends on context.",
    "start": "766250",
    "end": "773510"
  },
  {
    "text": "So this representation\nof the sentence is not going to be a\nparticularly sophisticated one.",
    "start": "773510",
    "end": "781870"
  },
  {
    "start": "781000",
    "end": "857000"
  },
  {
    "text": "So what we're going\nto do is going to define an abstract function.",
    "start": "781870",
    "end": "788830"
  },
  {
    "text": "Borrowing terminology\nfrom programming, an abstract function\nis something that has an interface but\nnot an implementation.",
    "start": "788830",
    "end": "796370"
  },
  {
    "text": "So a SequenceModel is\ngoing to be something that takes a sequence\nof input vectors",
    "start": "796370",
    "end": "801760"
  },
  {
    "text": "and produces a corresponding\nsequence of output vectors where each vector\nin this sequence",
    "start": "801760",
    "end": "808480"
  },
  {
    "text": "is a process with respect\nto the other elements. So in other words, I want to\ncontextualize these vectors",
    "start": "808480",
    "end": "818940"
  },
  {
    "text": "using the sequence models. I'm going to talk about two\nimplementations of the sequence",
    "start": "818940",
    "end": "824230"
  },
  {
    "text": "models. One is recurrent neural networks\nand one is transformers.",
    "start": "824230",
    "end": "829900"
  },
  {
    "text": "So historically\nrecurrent neural networks have been around\nsince the early '90s,",
    "start": "829900",
    "end": "835990"
  },
  {
    "text": "and since 2011 or\nso, it became really kind of the dominant paradigm\nfor doing deep learning NLP.",
    "start": "835990",
    "end": "844930"
  },
  {
    "text": "Transformers, who\ncame out in 2017, and really has kind\nof started, I guess,",
    "start": "844930",
    "end": "852160"
  },
  {
    "text": "transformed the landscape\nof deep learning and NLP.",
    "start": "852160",
    "end": "858350"
  },
  {
    "start": "857000",
    "end": "1058000"
  },
  {
    "text": "So an RNN, or a\nrecurrent neural network, can be thought of as reading\na sentence left to right.",
    "start": "858350",
    "end": "865565"
  },
  {
    "text": "That's the kind of intuitive\nway to think about it. So we have a word which gets\nmapped into a vector that",
    "start": "865565",
    "end": "874040"
  },
  {
    "text": "produces some hidden state. And then I'm going to read\na second input vector,",
    "start": "874040",
    "end": "881270"
  },
  {
    "text": "and I'm going to update\nthis hidden state along with this thing that I just\nread into a new hidden state.",
    "start": "881270",
    "end": "889399"
  },
  {
    "text": "And then I'm going to\nread another input vector, update its state and\nrepeating and again, OK?",
    "start": "889400",
    "end": "898760"
  },
  {
    "text": "So at the end of the day,\nI have the sequence model because that maps input sequence\ninto an output sequence.",
    "start": "898760",
    "end": "907310"
  },
  {
    "text": "And I notice that each\nvector here now depends on not just the input\nvector but [INAUDIBLE]",
    "start": "907310",
    "end": "919840"
  },
  {
    "text": "So if you look at h3, h3 depends\non x3, x2, and x1 following",
    "start": "919840",
    "end": "925320"
  },
  {
    "text": "this computation map. So the intuition again\nis reading left-to-right,",
    "start": "925320",
    "end": "930730"
  },
  {
    "text": "updating the hidden\nstate as you go along. It's kind of like a memory.",
    "start": "930730",
    "end": "936490"
  },
  {
    "text": "One thing I haven't\nspecified is what this function that takes an\nold hidden state, an input,",
    "start": "936490",
    "end": "944200"
  },
  {
    "text": "and updates the hidden state. So I'm going to do that next.",
    "start": "944200",
    "end": "949690"
  },
  {
    "text": "There's two types\nof implementations I'm going to talk about. One is a simple RNN.",
    "start": "949690",
    "end": "955750"
  },
  {
    "text": "So the contract here is I'm\ngoing to have an old hidden state, an input, and\nwe're going to want",
    "start": "955750",
    "end": "962350"
  },
  {
    "text": "to generate a new hidden state\nof the same dimensionality. And the way a simple RNN works\nis I take a hidden state,",
    "start": "962350",
    "end": "971590"
  },
  {
    "text": "multiply by a matrix, take the\ninput, multiply by the matrix,",
    "start": "971590",
    "end": "979690"
  },
  {
    "text": "and I add these two and I\napply an activation function. So it's fairly simple.",
    "start": "979690",
    "end": "986040"
  },
  {
    "text": "And one other way\nto think about this is that this is really\nthe FeedForward function applied to concatenation\nof h and x.",
    "start": "986040",
    "end": "995930"
  },
  {
    "text": "OK, so one problem\nwith a simple RNN is that it suffers from the\nvanishing gradient problem.",
    "start": "995930",
    "end": "1003340"
  },
  {
    "text": "If you have long sequences then\nthe gradients start vanishing. So LSTMs, or long\nshort term memory,",
    "start": "1003340",
    "end": "1012130"
  },
  {
    "text": "were developed to\nsolve this problem. And the way that this works\nis the interface is the same",
    "start": "1012130",
    "end": "1020230"
  },
  {
    "text": "and the implementation\nis rather involving, that I'm not going to explain.",
    "start": "1020230",
    "end": "1028040"
  },
  {
    "text": "But intuitively,\nyou should black box this and think about\nLSTMs as just a way",
    "start": "1028040",
    "end": "1033319"
  },
  {
    "text": "to update the hidden\nstate given a new input but without forgetting the past.",
    "start": "1033319",
    "end": "1039959"
  },
  {
    "text": "Remember up here\nfor a simple RNN, we can think of it as this\nFeedForward on x and h",
    "start": "1039960",
    "end": "1048600"
  },
  {
    "text": "which are treated\nkind of equally. LSTMs kind of privilege\nh and make sure that h doesn't get forgotten\nwhile going through this arrow.",
    "start": "1048600",
    "end": "1056269"
  },
  {
    "text": "OK, so now we have our\nsequenced model on RNN",
    "start": "1056270",
    "end": "1064730"
  },
  {
    "start": "1058000",
    "end": "1173000"
  },
  {
    "text": "which produces a\nsequence of vectors, and the number of vectors\ndepends on how long the input",
    "start": "1064730",
    "end": "1071990"
  },
  {
    "text": "sequence is. So suppose we want\nto do classification, we need to somehow collapse\nthat into a single vector.",
    "start": "1071990",
    "end": "1078330"
  },
  {
    "text": "So I'm going to define this\nfunction Collapse which takes a sequence of vectors\nand returns a single vector.",
    "start": "1078330",
    "end": "1086620"
  },
  {
    "text": "So you can intuitively\nthink about this as summarizing the\ncollection of vectors as one.",
    "start": "1086620",
    "end": "1092039"
  },
  {
    "text": "There's three common\nthings you can do. You can just simply\ntake the first vector,",
    "start": "1092040",
    "end": "1098370"
  },
  {
    "text": "you can take the last\nvector, or you can take the average of all vectors. So if you're doing\ntext classification,",
    "start": "1098370",
    "end": "1104940"
  },
  {
    "text": "you probably want to pick\nthe average to not privilege any individual word. But as we'll see later if you're\ntrying to do language modeling,",
    "start": "1104940",
    "end": "1111990"
  },
  {
    "text": "you want to take the last. So here is an example\ntext classification model",
    "start": "1111990",
    "end": "1117640"
  },
  {
    "text": "that we can develop. The score for, let's say,\nbinary classification",
    "start": "1117640",
    "end": "1124149"
  },
  {
    "text": "is going to be equal to taking\nthe input sequence of tokens.",
    "start": "1124150",
    "end": "1130900"
  },
  {
    "text": "You embed all the tokens\ninto a sequence of vectors, and now you can apply\na sequence model,",
    "start": "1130900",
    "end": "1139370"
  },
  {
    "text": "for example, a sequence RNN. And you can do this three\ntimes, that gives you depth",
    "start": "1139370",
    "end": "1145160"
  },
  {
    "text": "just like we talked about\nwith FeedForward networks. And now you can collapse\nthat into a single vector,",
    "start": "1145160",
    "end": "1151909"
  },
  {
    "text": "take the dot product\nto get a number out. So these types of functions\nwhere the input and output have",
    "start": "1151910",
    "end": "1161940"
  },
  {
    "text": "the same type signature are\nreally handy because then you can compose them with\neach other and get",
    "start": "1161940",
    "end": "1168150"
  },
  {
    "text": "multiple steps of computation. ",
    "start": "1168150",
    "end": "1174179"
  },
  {
    "start": "1173000",
    "end": "1199000"
  },
  {
    "text": "So recurrent neural networks\nwork generally fairly well,",
    "start": "1174180",
    "end": "1180900"
  },
  {
    "text": "but they suffer from one problem\nis that they're fairly local.",
    "start": "1180900",
    "end": "1186430"
  },
  {
    "start": "1186430",
    "end": "1195392"
  },
  {
    "text": "And so one problem\nthat-- so is a problem that we're going to try to\naddress with transformers.",
    "start": "1195392",
    "end": "1202110"
  },
  {
    "start": "1199000",
    "end": "1577000"
  },
  {
    "text": "So introducing transformers\nis fairly involved.",
    "start": "1202110",
    "end": "1207510"
  },
  {
    "text": "So I'm going to step through,\nintroduce a few things before actually defining it.",
    "start": "1207510",
    "end": "1212590"
  },
  {
    "text": "So the core part\nof a transformer is the attention mechanism.",
    "start": "1212590",
    "end": "1219860"
  },
  {
    "text": "And the attention\nmechanism takes in a collection of input\nvectors and a query vector",
    "start": "1219860",
    "end": "1227690"
  },
  {
    "text": "and it outputs a single vector. And intuitively what the\nattention is doing is",
    "start": "1227690",
    "end": "1233000"
  },
  {
    "text": "it's going to process y by\ncomparing it to each of these",
    "start": "1233000",
    "end": "1238190"
  },
  {
    "text": "x's. OK. So mathematically\nwhat this is doing is you start with\nthe query vector.",
    "start": "1238190",
    "end": "1248520"
  },
  {
    "text": "I'm going to multiply a matrix\nto reduce its dimensionality, in this case from 6 to 3.",
    "start": "1248520",
    "end": "1255730"
  },
  {
    "text": "I'm also going to take\nthe x transpose which is each row here is one\nof the input vectors.",
    "start": "1255730",
    "end": "1265450"
  },
  {
    "text": "x1, x2, x3, x4. I'm going to reduce its\ndimensionality to also 3",
    "start": "1265450",
    "end": "1272889"
  },
  {
    "text": "dimensions. And now I can take the\ndot product between these",
    "start": "1272890",
    "end": "1278360"
  },
  {
    "text": "x's and y's. So that's going to give me a\nfour-dimensional vector of dot",
    "start": "1278360",
    "end": "1285280"
  },
  {
    "text": "products intuitively measuring\nthe similarity between the x's",
    "start": "1285280",
    "end": "1292360"
  },
  {
    "text": "and the y. So now I can take\nthose scores and I",
    "start": "1292360",
    "end": "1297679"
  },
  {
    "text": "can turn them into probabilities\nby taking a softmax. So a softmax\nexponentiates the scores",
    "start": "1297680",
    "end": "1306440"
  },
  {
    "text": "and normalizes that into a\nprobability distribution. So now I have a\ndistribution over the input",
    "start": "1306440",
    "end": "1313370"
  },
  {
    "text": "vectors, x1, x2, x3, x4. It's a four-dimensional vector. I can use those\nprobabilities, those weights,",
    "start": "1313370",
    "end": "1322309"
  },
  {
    "text": "when I multiply\nby x to take away the combination of\nthe columns of x here.",
    "start": "1322310",
    "end": "1329730"
  },
  {
    "text": "So for intuition,\nif one of the inputs",
    "start": "1329730",
    "end": "1335240"
  },
  {
    "text": "has a very high probability,\nlet's say it's 0, 0, 1, 0, then I'm just going to pick\nout the third input vector.",
    "start": "1335240",
    "end": "1346320"
  },
  {
    "text": "So in general, this\nis a distribution so this is kind of\nsoftly picking out which",
    "start": "1346320",
    "end": "1352100"
  },
  {
    "text": "input vector is similar to y. OK, so then finally I'm going\nto reduce the dimensionality",
    "start": "1352100",
    "end": "1358809"
  },
  {
    "text": "to some four-dimensional object. So similarity can be\na multifaceted thing.",
    "start": "1358810",
    "end": "1368580"
  },
  {
    "text": "So one thing that\nthe transformer does is it allows us to use\nmultiple attention heads.",
    "start": "1368580",
    "end": "1376190"
  },
  {
    "text": "So I'm going to repeat\nthis process again, taking the query vector,\ntaking the input vector,",
    "start": "1376190",
    "end": "1382730"
  },
  {
    "text": "comparing them in\nthe distribution over the input vectors. And using that distribution,\nre-weight the input vector.",
    "start": "1382730",
    "end": "1389200"
  },
  {
    "text": "So I'm selecting out softly the\ninput vector and I multiply it by a matrix to reduce\nthe dimensionality.",
    "start": "1389200",
    "end": "1396440"
  },
  {
    "text": "I've done this twice,\nbut in general you can do this any, 4 or 16.",
    "start": "1396440",
    "end": "1402909"
  },
  {
    "text": "So now I concatenate\nthese vectors. So I have a\nfour-dimensional vector from this computation,\nfour-dimensional vector",
    "start": "1402910",
    "end": "1409242"
  },
  {
    "text": "from this computation. I can concatenate them into\nan eight-dimensional vector. And now I can reduce\nthe dimensionality back",
    "start": "1409243",
    "end": "1416340"
  },
  {
    "text": "to the original\ndimensionality of the inputs.",
    "start": "1416340",
    "end": "1421610"
  },
  {
    "text": "OK. So that was a kind of a\nvery involved process, but at the end of the day\nyou can think about this",
    "start": "1421610",
    "end": "1429190"
  },
  {
    "text": "as taking y, comparing it with\nthe x's, and selecting out the one that's most\nsimilar and doing",
    "start": "1429190",
    "end": "1438610"
  },
  {
    "text": "some dimensionality\nreduction in the process. OK.",
    "start": "1438610",
    "end": "1443720"
  },
  {
    "text": "So that's attention. The transformer uses something\ncalled self attention, which",
    "start": "1443720",
    "end": "1451168"
  },
  {
    "text": "means that the query vector is\nactually going to be the input vectors themselves. So if self attention takes\na sequence of input vectors",
    "start": "1451168",
    "end": "1462440"
  },
  {
    "text": "and then it's going to output\nthe same sequence of output vectors where the\nfirst vector is,",
    "start": "1462440",
    "end": "1470180"
  },
  {
    "text": "I'm going to stick x1 into\nthe query vector for y and compute the attention,\nand then x2 and x3 and x4.",
    "start": "1470180",
    "end": "1480799"
  },
  {
    "text": "So each of these vectors is\ncomparing a particular input vector with the rest\nof the input vectors",
    "start": "1480800",
    "end": "1487610"
  },
  {
    "text": "and doing some processing. So in other words,\nI've basically",
    "start": "1487610",
    "end": "1495500"
  },
  {
    "text": "generated a sequence of vectors\nwhere all of the objects, all n",
    "start": "1495500",
    "end": "1501170"
  },
  {
    "text": "squared of the objects I've\nallowed them to communicate with each other directly.",
    "start": "1501170",
    "end": "1508940"
  },
  {
    "text": "So in contrast with\nthe RNN, you have representations that have to\nkind of proceed step by step.",
    "start": "1508940",
    "end": "1517870"
  },
  {
    "text": "And the number of\nsteps is the length of a sequence which causes\nthese long chains which",
    "start": "1517870",
    "end": "1524560"
  },
  {
    "text": "prevents kind of\nfast propagation, whereas attention\nsolves this problem.",
    "start": "1524560",
    "end": "1532120"
  },
  {
    "text": "So one kind of side\ncomment is that I'm speaking very vaguely and\nintuitively about these things,",
    "start": "1532120",
    "end": "1541990"
  },
  {
    "text": "trying to provide as much\nintuition as possible. And you can't really be more\nprecise because I'm, again, not",
    "start": "1541990",
    "end": "1549600"
  },
  {
    "text": "specifying the\nactual computation, I'm only specifying kind of the\nscope of possible computations",
    "start": "1549600",
    "end": "1556200"
  },
  {
    "text": "that can be done once\nthe parameters are learned from data.",
    "start": "1556200",
    "end": "1562780"
  },
  {
    "text": "OK. So that's an\nattention mechanism. You can think about this as a\nsequence model that just takes",
    "start": "1562780",
    "end": "1569880"
  },
  {
    "text": "input sequence and\ncontextualizes the input vectors into output vectors.",
    "start": "1569880",
    "end": "1578110"
  },
  {
    "start": "1577000",
    "end": "1718000"
  },
  {
    "text": "So there's two\nother pieces I need to talk about before I can\nfully define the transformer.",
    "start": "1578110",
    "end": "1584340"
  },
  {
    "text": "Layer normalization and\nresidual connections. So these are really kind\nof technical devices",
    "start": "1584340",
    "end": "1589500"
  },
  {
    "text": "to make the final neural\nnetwork easier to train. I'm going to package them up\ninto something called AddNorm",
    "start": "1589500",
    "end": "1597150"
  },
  {
    "text": "and it also has a type\nsignature of a sequence model where I have input\nsequence of vectors",
    "start": "1597150",
    "end": "1603870"
  },
  {
    "text": "and I spit out the corresponding\nset of contextualized vectors. And the intuition\nbehind this is I'm",
    "start": "1603870",
    "end": "1610500"
  },
  {
    "text": "going to apply f to x safely. So let me explain\nwhat that means.",
    "start": "1610500",
    "end": "1616870"
  },
  {
    "text": "So AddNorm of f\nof x is equal to-- I'm first going to take\nx and apply f to it.",
    "start": "1616870",
    "end": "1623799"
  },
  {
    "text": "OK. So why is that not good enough? Well, remember that these\nfunctions are under specified.",
    "start": "1623800",
    "end": "1635039"
  },
  {
    "text": "So at the beginning\nof training, they're basically not doing anything.",
    "start": "1635040",
    "end": "1640290"
  },
  {
    "text": "So they're basically\nkind of junk. And if this is\njunk then anything that I build on top of it is\nalso going to be pretty junky.",
    "start": "1640290",
    "end": "1648380"
  },
  {
    "text": "So what I want to do is\nadd a residual connection. So a residual connection\nis a kind of escape hatch",
    "start": "1648380",
    "end": "1654289"
  },
  {
    "text": "that allows x to be\npropagated through verbatim. So that means if f is\njunked, at least I have x.",
    "start": "1654290",
    "end": "1663800"
  },
  {
    "text": "So then I'm going to\nadd a LayerNorm function on top of this.",
    "start": "1663800",
    "end": "1669950"
  },
  {
    "text": "So layer normalization\nis just a way to make sure that this vector\nis not too big or not too small",
    "start": "1669950",
    "end": "1679490"
  },
  {
    "text": "because big vectors\nand small vectors result in exploding gradients\nor vanishing gradients",
    "start": "1679490",
    "end": "1685880"
  },
  {
    "text": "which stalls training or\nmakes training diverge.",
    "start": "1685880",
    "end": "1690950"
  },
  {
    "text": "So specifically what\nLayerNorm does on a single vector\nis that it treats these as a set of\nelements and it",
    "start": "1690950",
    "end": "1698250"
  },
  {
    "text": "subtracts the mean\nof those elements and divides by the\nstandard deviation to kind of standardize the\nmagnitude of the vectors.",
    "start": "1698250",
    "end": "1707450"
  },
  {
    "text": " OK. So in summary, AddNorm\nwith a particular function",
    "start": "1707450",
    "end": "1715110"
  },
  {
    "text": "is just applying f to x safely. OK. So now I'm finally ready to\ndefine the TransformerBlock.",
    "start": "1715110",
    "end": "1724570"
  },
  {
    "start": "1718000",
    "end": "1890000"
  },
  {
    "text": "And this is, again,\na sequence model that takes a sequence\nof input vectors and spits out a contextualized\nset of output vectors,",
    "start": "1724570",
    "end": "1732430"
  },
  {
    "text": "and this is just intuitively\nprocessing each xi in context.",
    "start": "1732430",
    "end": "1740190"
  },
  {
    "text": "So there's only one line here. We've done actually\nmost of the hard work. So the transformer block\non a sequence of vectors,",
    "start": "1740190",
    "end": "1748760"
  },
  {
    "text": "is going to be x and\nyou apply attention that allows all the vectors\nto talk to each other,",
    "start": "1748760",
    "end": "1758400"
  },
  {
    "text": "and then you want to normalize\nand to do this safely.",
    "start": "1758400",
    "end": "1763710"
  },
  {
    "text": "And finally you\napply FeedForward to each individual resulting\nvector independently,",
    "start": "1763710",
    "end": "1770520"
  },
  {
    "text": "and then you also want to\nnormalize and do this safely.",
    "start": "1770520",
    "end": "1775630"
  },
  {
    "text": "So that's it for a\nTransformerBlock. ",
    "start": "1775630",
    "end": "1783500"
  },
  {
    "text": "So now we have enough that\nwe can actually build up to BERT which was this\ncomplicated thing that I",
    "start": "1783500",
    "end": "1788890"
  },
  {
    "text": "mentioned at the beginning. So BERT is this large\nunsupervised pretrained model which came out in 2018 which has\nreally kind of transformed NLP.",
    "start": "1788890",
    "end": "1799690"
  },
  {
    "text": "Before there were a lot of\nspecialized architectures for different tasks, but BERT\nwas a single model architecture",
    "start": "1799690",
    "end": "1806590"
  },
  {
    "text": "that worked well\nacross the many tasks. So this is the way it works\nfor question answering.",
    "start": "1806590",
    "end": "1815080"
  },
  {
    "text": "You take the question,\nyou concatenate it",
    "start": "1815080",
    "end": "1820740"
  },
  {
    "text": "with the paragraph, that gives\nyou just a sequence of tokens.",
    "start": "1820740",
    "end": "1827380"
  },
  {
    "text": "And what BERT does on\na sequence of tokens is it's going to\nembed the tokens,",
    "start": "1827380",
    "end": "1835390"
  },
  {
    "text": "and then it's just going to\napply the TransformerBlock 24",
    "start": "1835390",
    "end": "1840670"
  },
  {
    "text": "times. So again, the nice thing about\nhaving a TransformerBlock where",
    "start": "1840670",
    "end": "1846290"
  },
  {
    "text": "the input and output have the\nsame dimensionality and type is that you can just\nkind of lay it on and get",
    "start": "1846290",
    "end": "1852560"
  },
  {
    "text": "much deeper networks. OK. So at the end of the\nday, BERT gives you",
    "start": "1852560",
    "end": "1857600"
  },
  {
    "text": "a sequence of vectors\nwhich are highly contextualized and\nnuanced and contains",
    "start": "1857600",
    "end": "1863720"
  },
  {
    "text": "a lot of rich information\nabout the sentence. And from there, you can either\nuse it to drive classification",
    "start": "1863720",
    "end": "1872929"
  },
  {
    "text": "of, let's say, binary\nclassification directly by collapsing the\nvectors into one vector,",
    "start": "1872930",
    "end": "1878690"
  },
  {
    "text": "or you can use it to select\nout an answer to the question.",
    "start": "1878690",
    "end": "1885379"
  },
  {
    "text": "And I'm not going to go into\ndetails of how that works. ",
    "start": "1885380",
    "end": "1891100"
  },
  {
    "start": "1890000",
    "end": "1956000"
  },
  {
    "text": "So so far we've talked about\nhow to design functions that can process a sentence, a\nsequence of tokens or vectors,",
    "start": "1891100",
    "end": "1898840"
  },
  {
    "text": "but we can also\ngenerate new sequences. And the basic building\nblock for generation",
    "start": "1898840",
    "end": "1905170"
  },
  {
    "text": "is, I'm going to call\nit GenerateToken. ",
    "start": "1905170",
    "end": "1910225"
  },
  {
    "text": "And you take a vector x\nand you generate token y.",
    "start": "1910225",
    "end": "1916020"
  },
  {
    "text": "And this is kind of the\nreverse of EmbedToken which takes a token\nand produces a vector.",
    "start": "1916020",
    "end": "1924149"
  },
  {
    "text": "And the way GenerateToken\nis going to work is that it's going to actually\nuse this as a subroutine.",
    "start": "1924150",
    "end": "1930120"
  },
  {
    "text": "It's going to look at all\nthe possible candidate words that one could generate.",
    "start": "1930120",
    "end": "1935700"
  },
  {
    "text": "It's going to embed those to\nand take the dot product of x to get some sort of\nsimilarity between the vector",
    "start": "1935700",
    "end": "1942840"
  },
  {
    "text": "and a potential\ncandidate generation. Now we have some scores. We apply the softmax\nto get a distribution",
    "start": "1942840",
    "end": "1949860"
  },
  {
    "text": "over possible words and\nthen we can generate from that probability function. ",
    "start": "1949860",
    "end": "1957570"
  },
  {
    "start": "1956000",
    "end": "2026000"
  },
  {
    "text": "So here building on\ntop of GenerateToken, we can do language\nmodeling where",
    "start": "1957570",
    "end": "1964610"
  },
  {
    "text": "the input is a sequence\nof words and the output is",
    "start": "1964610",
    "end": "1969679"
  },
  {
    "text": "the next word. So this is actually\nfairly simple",
    "start": "1969680",
    "end": "1975200"
  },
  {
    "text": "since we already have\nessentially all the tools. So language modeling\nof x is you take",
    "start": "1975200",
    "end": "1981620"
  },
  {
    "text": "x, you embed them into tokens. The crucial step is\nthat you stick it through a sequence model.",
    "start": "1981620",
    "end": "1988429"
  },
  {
    "text": "Remember a sequence model\ndoes this fancy stuff and turns this sequence of\nkind of primitive vectors",
    "start": "1988430",
    "end": "1997790"
  },
  {
    "text": "into contextualized vectors\nwhich contain more information",
    "start": "1997790",
    "end": "2003010"
  },
  {
    "text": "and then it collapses them. And this time,\nyou generally want to use the last vector\nthat's closest to the word",
    "start": "2003010",
    "end": "2013480"
  },
  {
    "text": "that you want to generate next. And then that gives\nyou just one vector and you can use that\nto generate a token.",
    "start": "2013480",
    "end": "2021220"
  },
  {
    "text": " OK. Finally we can take\nlanguage models",
    "start": "2021220",
    "end": "2030010"
  },
  {
    "start": "2026000",
    "end": "2122000"
  },
  {
    "text": "and we can build on\ntop of them to create what is known as a\nsequence-to-sequence model.",
    "start": "2030010",
    "end": "2036920"
  },
  {
    "text": "So this is perhaps one of my\nkind of favorite interfaces because it's so versatile.",
    "start": "2036920",
    "end": "2042970"
  },
  {
    "text": "So the basic idea\nis that you have input, which is a\nsequence, and you",
    "start": "2042970",
    "end": "2048010"
  },
  {
    "text": "are trying to generate another\nsequence, which is the output. And sequences are very--",
    "start": "2048010",
    "end": "2053138"
  },
  {
    "text": "in general, you can use\nsequences to encode basically any sort of discrete output.",
    "start": "2053139",
    "end": "2059623"
  },
  {
    "text": "And the way we're\ngoing to do that is just using a language model.",
    "start": "2059623",
    "end": "2066060"
  },
  {
    "text": "So remember, a language\nmodel takes the sequence and predicts the next token. So I can start out with x and\nI can query the language model",
    "start": "2066060",
    "end": "2076849"
  },
  {
    "text": "to generate the next token. And then I can feed this\ntoken, attach this token to the history, query\nthe language model again",
    "start": "2076850",
    "end": "2084888"
  },
  {
    "text": "to generate the next token,\nand so on and so forth until I'm done.",
    "start": "2084889",
    "end": "2093179"
  },
  {
    "text": "So this is by and\nlarge how a lot of the state of the art\nmethods for, for example,",
    "start": "2093179",
    "end": "2099660"
  },
  {
    "text": "machine translation works. We're generating a\ntranslated sentence, given the input sentence\nor a document summarization",
    "start": "2099660",
    "end": "2107220"
  },
  {
    "text": "or semantic parsing. Each of these\nsequence can be framed as sequence-to-sequence\ntasks based",
    "start": "2107220",
    "end": "2113970"
  },
  {
    "text": "on, usually these\ndays, basically BERT and Transformers.",
    "start": "2113970",
    "end": "2120150"
  },
  {
    "text": " OK so that was a really quick\nand high level whirlwind",
    "start": "2120150",
    "end": "2126220"
  },
  {
    "start": "2122000",
    "end": "2262000"
  },
  {
    "text": "tour of different types\nof differentiable programs from deep learning. So we started with--",
    "start": "2126220",
    "end": "2133540"
  },
  {
    "text": "Now in hindsight, it seems kind\nof very simple, FeedForward networks.",
    "start": "2133540",
    "end": "2140350"
  },
  {
    "text": "And we looked at\nimages and looked at convolutional\nneural networks which were built on Conv\nlayers and MaxPool layers",
    "start": "2140350",
    "end": "2148510"
  },
  {
    "text": "and also FeedForward. So the nice thing about\npackaging this in a module is that now this actually\nis used in transformers",
    "start": "2148510",
    "end": "2155710"
  },
  {
    "text": "and different places as well.  For text and sequences,\nwe first have",
    "start": "2155710",
    "end": "2164730"
  },
  {
    "text": "to embed them into a\nsequence of vectors and then we have\nkind of two choices.",
    "start": "2164730",
    "end": "2171280"
  },
  {
    "text": "We can either use\nrecurrent neural networks, or we can use transformers\nwhich are based on attention.",
    "start": "2171280",
    "end": "2182660"
  },
  {
    "text": "We can use sequence\nmodels, collapse it into a vector to drive\nclassification decisions,",
    "start": "2182660",
    "end": "2189410"
  },
  {
    "text": "or we can use them to generate\nnew sequences as well.",
    "start": "2189410",
    "end": "2195119"
  },
  {
    "text": "So there are many details\nthat are glossed over. In particular, some\nof the architectures",
    "start": "2195120",
    "end": "2200540"
  },
  {
    "text": "have been simplified\nso I encourage you to consult the\noriginal source if you want kind of the actual,\nthe full gory details.",
    "start": "2200540",
    "end": "2209420"
  },
  {
    "text": "Another thing I\nhaven't talked about is learning any of these models. It's going to be using some\nvariant of stochastic gradient",
    "start": "2209420",
    "end": "2215820"
  },
  {
    "text": "descent, but there's\noften various tricks that are needed to get it to work.",
    "start": "2215820",
    "end": "2221880"
  },
  {
    "text": "But maybe the final\nthing I'll leave you with is the idea that all of\ndifferentiable programming--",
    "start": "2221880",
    "end": "2230220"
  },
  {
    "text": "which is that all of\nthese complex models are built out of modules. And even if you kind\nof don't understand",
    "start": "2230220",
    "end": "2236660"
  },
  {
    "text": "or I didn't explain\nthe details, I think it's really\nimportant to pay attention to the type signature\nof these functions,",
    "start": "2236660",
    "end": "2246980"
  },
  {
    "text": "as well as with an intuitive\nidea of what each of these are doing.",
    "start": "2246980",
    "end": "2253210"
  },
  {
    "text": "OK. So that ends this module. Thanks for listening. ",
    "start": "2253210",
    "end": "2262000"
  }
]