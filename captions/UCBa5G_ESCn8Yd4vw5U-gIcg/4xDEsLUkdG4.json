[
  {
    "start": "0",
    "end": "5850"
  },
  {
    "text": "OK. So I guess let's get started.  So today this\nlecture, we are going",
    "start": "5850",
    "end": "12260"
  },
  {
    "text": "to discuss a few small\nstuff that are remained--",
    "start": "12260",
    "end": "19490"
  },
  {
    "text": "that are kind of left\nfrom previous lectures, and then we're going to move\non to unsupervised learning.",
    "start": "19490",
    "end": "26630"
  },
  {
    "text": "So I guess the first thing\nis recall that last time,",
    "start": "26630",
    "end": "32790"
  },
  {
    "text": "we talked about implicit\nregularization of the noise. ",
    "start": "32790",
    "end": "42910"
  },
  {
    "text": "And we mentioned that\nin certain cases,",
    "start": "42910",
    "end": "48739"
  },
  {
    "text": "you can prove that a regulizer\nprefers the noise, noisy GD.",
    "start": "48740",
    "end": "55100"
  },
  {
    "text": "Noisy GD prefers smaller\nof this quantity, R theta,",
    "start": "55100",
    "end": "63789"
  },
  {
    "text": "which is defined to be something\nlike the trace of the Hessian. ",
    "start": "63790",
    "end": "71520"
  },
  {
    "text": "And in the first\npart of this lecture, I'm going to spend\nprobably 10 to 15 minutes",
    "start": "71520",
    "end": "78020"
  },
  {
    "text": "to briefly discuss why\nthis is a reasonable thing to try to minimize, or\ntry to kind of regularize.",
    "start": "78020",
    "end": "85280"
  },
  {
    "text": "So why the trace of the Hessian\nis some meaningful quantity, or-- but this part wouldn't be\nexactly kind of rigorous,",
    "start": "85280",
    "end": "93380"
  },
  {
    "text": "because you have to do some\napproximations and so forth. But I'm just going to do some\nkind of a somewhat heuristic",
    "start": "93380",
    "end": "99950"
  },
  {
    "text": "derivation to justify why\nsomething like the Hessian would be useful for\nus to regularize.",
    "start": "99950",
    "end": "107240"
  },
  {
    "text": "So I guess the thing\nis that how do we-- what is the Hessian?",
    "start": "107240",
    "end": "112729"
  },
  {
    "text": "What is the Hessian? Maybe, actually, I\nshould write l hat. This is the empirical-- the\nHessian on the empirical loss.",
    "start": "112730",
    "end": "119340"
  },
  {
    "text": "So maybe for\nsimplicity, let's only",
    "start": "119340",
    "end": "125149"
  },
  {
    "text": "consider the one data point. ",
    "start": "125150",
    "end": "132849"
  },
  {
    "text": "And let's say, suppose f-- let's denote f to\nbe f comma theta.",
    "start": "132850",
    "end": "139900"
  },
  {
    "text": "This is the-- I guess maybe I should say f\ntheta of x be the model output.",
    "start": "139900",
    "end": "148165"
  },
  {
    "text": " And let lfy be\nthe loss function.",
    "start": "148165",
    "end": "159879"
  },
  {
    "start": "159880",
    "end": "166320"
  },
  {
    "text": "Then what you can do is\nthat you can compute. So then l theta, in this\ncase, is just l of f theta xy.",
    "start": "166320",
    "end": "176310"
  },
  {
    "text": "So in this case, then we can\ncompute what the Hessian is. So the Hessian--\nmaybe let's call",
    "start": "176310",
    "end": "182280"
  },
  {
    "text": "it l hat just to be consistent\nin terms of notation. The Hessian is the\ngradient of the gradient.",
    "start": "182280",
    "end": "189670"
  },
  {
    "text": "Now what's the gradient? If you use Chain\nrule, what you got is that you get the partial l\nor partial f times partial f",
    "start": "189670",
    "end": "198870"
  },
  {
    "text": "over partial theta. So this is number. ",
    "start": "198870",
    "end": "206150"
  },
  {
    "text": "The directive is back to-- so this is a scalar function.\nl is a function of f,",
    "start": "206150",
    "end": "211280"
  },
  {
    "text": "but this is a very simple\nfunction because l is a scalar, f is a scalar. So this is a scalar and times\nthe gradient of f theta at x.",
    "start": "211280",
    "end": "222280"
  },
  {
    "text": "And now you are\ntaking a gradient of a product of two quantities. One is a scalar and the\nother is a gradient.",
    "start": "222280",
    "end": "228500"
  },
  {
    "text": "And then you can do chain rule. What you got is that\nyou first, for example, do the gradient with\nrespect to your first part.",
    "start": "228500",
    "end": "235280"
  },
  {
    "text": "What you get is--  let me see what you got from the\nfirst one, is that you have l,",
    "start": "235280",
    "end": "243563"
  },
  {
    "text": "the second order derivative\nwith respect to f. And then you do the chain\nrule, you get this f theta x,",
    "start": "243563",
    "end": "250239"
  },
  {
    "text": "this is with respect with\ntheta, times gradient f theta x transposed. And this is in some sense the--",
    "start": "250240",
    "end": "258100"
  },
  {
    "text": "so this part is the gradient of\nthis and this part is copying. I guess this is copying\nfrom here in some sense.",
    "start": "258100",
    "end": "267330"
  },
  {
    "text": "But I guess this is something\nthat you can verify offline, if you do all the-- if you look at all\nthe coordinates",
    "start": "267330",
    "end": "273660"
  },
  {
    "text": "and do all the calculations. And then you can also do the\nchain rule for the other part.",
    "start": "273660",
    "end": "279940"
  },
  {
    "text": "So what you can get is\nthe bl over df times",
    "start": "279940",
    "end": "285570"
  },
  {
    "text": "the second order derivative\nof the model theta at x.",
    "start": "285570",
    "end": "292040"
  },
  {
    "text": "The matrix of\ndimension p by p, if p is the number of parameters.",
    "start": "292040",
    "end": "297600"
  },
  {
    "text": " And this is a scalar,\nthis is a scalar.",
    "start": "297600",
    "end": "303110"
  },
  {
    "text": "And this is a vector,\nthis is a vector. So the whole thing\nis a p by p matrix.",
    "start": "303110",
    "end": "309370"
  },
  {
    "text": "So we'll have--\nsuppose your loss-- so this is a general formula,\nwhich is just rigorously true.",
    "start": "309370",
    "end": "314820"
  },
  {
    "text": "And suppose the loss function is\nlf y is equal to, for example,",
    "start": "314820",
    "end": "328500"
  },
  {
    "text": "1/2 times y minus f squared,\nthen this formula becomes--",
    "start": "328500",
    "end": "334730"
  },
  {
    "text": "what is the second order\nderivative of this loss function with respect to f? Its function is a\nquadratic function of f.",
    "start": "334730",
    "end": "342420"
  },
  {
    "text": "The loss function is a quadratic\nfunction with respect to f, and the leading\nterm is f squared. So the loss-- the second order\nderivative with respect to f",
    "start": "342420",
    "end": "349840"
  },
  {
    "text": "is 1. So this is equal to 1\ntimes gradient theta-- of f times the gradient\nof f theta transposed.",
    "start": "349840",
    "end": "358080"
  },
  {
    "text": "And then this first order\nderivative will be-- respect to f will be f minus y\ntimes the Hessian of theta x.",
    "start": "358080",
    "end": "368880"
  },
  {
    "text": "So this decomposition\nis often called-- so what you can see is\nthat this is a convex.",
    "start": "368880",
    "end": "375680"
  },
  {
    "text": "Term. This is a PSD term, sorry. This is PSD because it is the\nother product of a rank one",
    "start": "375680",
    "end": "385250"
  },
  {
    "text": "matrix, and this is non-active. And this is not necessarily PSD.",
    "start": "385250",
    "end": "390260"
  },
  {
    "text": " So the Hessian may not be\nPSD in general, of course,",
    "start": "390260",
    "end": "397560"
  },
  {
    "text": "right, because you have\na non-convex function, but one of the terms is PSD. ",
    "start": "397560",
    "end": "403830"
  },
  {
    "text": "And in general, if\nyou have a convex loss function in the first term,\nin general, this is called--",
    "start": "403830",
    "end": "411220"
  },
  {
    "text": "this is called-- I don't know why\nthis is called this, but it's called\nGauss-Newton decomposition.",
    "start": "411220",
    "end": "417860"
  },
  {
    "text": "I think it must have\nsomething to do with these two famous people at some point. But it's called\nGauss-Newton decomposition.",
    "start": "417860",
    "end": "424970"
  },
  {
    "text": "And in general, the\nfirst term, this",
    "start": "424970",
    "end": "430810"
  },
  {
    "text": "is always positive for\nconvex loss function.",
    "start": "430810",
    "end": "437250"
  },
  {
    "text": "By loss function, I really\nmean literally the top, the either quadratic loss,\nor cross-entropy loss.",
    "start": "437250",
    "end": "447237"
  },
  {
    "text": "They're all convex, right? So this term, in\nalmost all cases where we study the first\nterm, this is non-zero.",
    "start": "447237",
    "end": "456000"
  },
  {
    "text": "So the first term is-- this is PSD. ",
    "start": "456000",
    "end": "464570"
  },
  {
    "text": "PSD. And empirically, people\nfound that the second term",
    "start": "464570",
    "end": "469660"
  },
  {
    "text": "in most of the cases is small. So there could be\nmultiple reasons for this. So empirically, the second\nterm, f minus y, this",
    "start": "469660",
    "end": "485990"
  },
  {
    "text": "is generally smaller.  And while the reason\ncould be that at least",
    "start": "485990",
    "end": "494360"
  },
  {
    "text": "when you are at a global\nminimum, this term is 0. So when theta is at a\nglobal min, meaning f theta",
    "start": "494360",
    "end": "510830"
  },
  {
    "text": "x is equal to 1, right? So global min would\nfit the data exactly. So in this case, then this\nterm is actually literally 0.",
    "start": "510830",
    "end": "518000"
  },
  {
    "text": "So f minus y 0. So this could be one reason why\nempirically, the second term",
    "start": "518000",
    "end": "525640"
  },
  {
    "text": "is relatively small. Of course, this is\nnot always true. It's not always true you\ncan fill data at any point.",
    "start": "525640",
    "end": "530830"
  },
  {
    "text": "But somehow, people found that\nthe second term is somewhat",
    "start": "530830",
    "end": "536170"
  },
  {
    "text": "smaller than first term. So if you don't care\nabout anything super-- if you don't care about a\nvery nuanced quantities,",
    "start": "536170",
    "end": "544510"
  },
  {
    "text": "about a Hessian,\nthen the first term is the reasonable\napproximation for the Hessian. Of course, in certain cases\nyou do care about nuances.",
    "start": "544510",
    "end": "551743"
  },
  {
    "text": "For example, when you care about\nwhether this function is convex or not, you should\ntalk about-- even any non-negative eigenvalue\nwould make it non-convex.",
    "start": "551743",
    "end": "559180"
  },
  {
    "text": "So then the second\nterm becomes important. But if you just have a\nchoice, then the second term",
    "start": "559180",
    "end": "564339"
  },
  {
    "text": "is not that important. So that's the rough intuition.",
    "start": "564340",
    "end": "569800"
  },
  {
    "text": "And now suppose ignoring\nthe second term. this is a big\nassumption, but suppose",
    "start": "569800",
    "end": "575500"
  },
  {
    "text": "ignoring the second term. Then we can see what's\nthe trace of the Hessian.",
    "start": "575500",
    "end": "581910"
  },
  {
    "text": "Second term. So the trace of the\nHessian for whatever it is. For example, you can ignore it\njust because it's empirically",
    "start": "581910",
    "end": "588530"
  },
  {
    "text": "small, or you can\nignore it because you are at a global minimum. But suppose you ignore\nthe second term. Then the trace of\nthe Hessian, f theta",
    "start": "588530",
    "end": "598500"
  },
  {
    "text": "is approximately equal\nto this derivative is equal to y, which is probably\n1 if you have square loss,",
    "start": "598500",
    "end": "605910"
  },
  {
    "text": "times the trace of\nthis transpose which",
    "start": "605910",
    "end": "618529"
  },
  {
    "text": "is equal to some scalar times\nthe true norm of the gradient.",
    "start": "618530",
    "end": "624565"
  },
  {
    "text": " So you can see\nthat by minimizing",
    "start": "624565",
    "end": "629630"
  },
  {
    "text": "the trace of the Hessian,\nyou are minimizing the l2 norm of the Lipschitzness\nwith respect to the parameter.",
    "start": "629630",
    "end": "640110"
  },
  {
    "text": "So minimizing the\ntrace of the Hessian,",
    "start": "640110",
    "end": "646899"
  },
  {
    "text": "is somewhat similar\nheuristically, minimizing Lipschitzness of the\nmodel with respectful to theta.",
    "start": "646900",
    "end": "661350"
  },
  {
    "text": " And I think-- so\nwhy is minimizing",
    "start": "661350",
    "end": "667753"
  },
  {
    "text": "the Lipschitzness of the\nmodel with respect to theta is useful? Actually, first of all,\nthis is indeed useful.",
    "start": "667753",
    "end": "673010"
  },
  {
    "text": "If you just expressly\nminimize this, people have found that\nempirically this is useful. And why this is useful?",
    "start": "673010",
    "end": "679670"
  },
  {
    "text": "If you allow some\nheuristics, you can also say that this is\nvery similar to minimizing",
    "start": "679670",
    "end": "686250"
  },
  {
    "text": "the Lipschitzness\nof the model output",
    "start": "686250",
    "end": "694190"
  },
  {
    "text": "with respect to the\nhidden variables. ",
    "start": "694190",
    "end": "702080"
  },
  {
    "text": "I think this is something\nthat we discussed probably a few weeks\nago when we talked",
    "start": "702080",
    "end": "707183"
  },
  {
    "text": "about the all-layer margins. So recall that if you have\nan original theta, which consists of, for example,\na bunch of layers,",
    "start": "707183",
    "end": "716260"
  },
  {
    "text": "suppose you have a deep\nnetwork with a lot of weights, and then the derivative\nof the model with respect",
    "start": "716260",
    "end": "721730"
  },
  {
    "text": "to some layer i, this is\nequal to the derivative",
    "start": "721730",
    "end": "727760"
  },
  {
    "text": "of the model with respect to the\nlayer above it times the layer",
    "start": "727760",
    "end": "736120"
  },
  {
    "text": "mc. So this is hi plus 1 times\nhi minus 1 transposed. So this is the so-called--",
    "start": "736120",
    "end": "742400"
  },
  {
    "text": "OK, I guess now I remember\nthe it's called Hebbian rule. But it's actually just\nliterally a simple chain rule.",
    "start": "742400",
    "end": "752320"
  },
  {
    "text": "In a narrow sense, this\nis called Hebbian rule. But technically, it's\njust really a chain rule.",
    "start": "752320",
    "end": "757560"
  },
  {
    "text": "You want to take the derivative\nwith respect to a parameter, and the parameter kind of\ncomes into play that depends",
    "start": "757560",
    "end": "763000"
  },
  {
    "text": "on hn as 1 and hi plus 1. This is hi. ",
    "start": "763000",
    "end": "769850"
  },
  {
    "text": "So here, hi plus\n1 is w times hi.",
    "start": "769850",
    "end": "778000"
  },
  {
    "text": "So this is the i-th layer and\nthis is the pre-activation",
    "start": "778000",
    "end": "787170"
  },
  {
    "text": "of a plus 1 layer. I guess maybe technically\nI should call this hi prime",
    "start": "787170",
    "end": "793550"
  },
  {
    "text": "just so that I can distinguish\nit from post-calibration. But I guess you get the point. The point is that if\nyou take the derivative",
    "start": "793550",
    "end": "800660"
  },
  {
    "text": "with the respective\nparameter, it's actually very closely related\nto the derivative, with respect",
    "start": "800660",
    "end": "805790"
  },
  {
    "text": "to a hidden variable, and a\nnorm of the hidden variable, hi, all right? So this means that\nEuclidean norm.",
    "start": "805790",
    "end": "814230"
  },
  {
    "text": "If you read the matrix and\nit's a Frobenius norm of this is equal to true norm times hi.",
    "start": "814230",
    "end": "822610"
  },
  {
    "text": "True norm. So minimizing the Lipschitzness\nwith respect to the parameters",
    "start": "822610",
    "end": "830650"
  },
  {
    "text": "is similar to minimizing the\nLipschitzness with respect to hidden variable. I think this is something we\nhave discussed before when we",
    "start": "830650",
    "end": "838418"
  },
  {
    "text": "do the all-layer margin, right? So then we talk\nabout the derivative of the hidden\nvariable, then this is kind of like all-layer margin.",
    "start": "838418",
    "end": "844020"
  },
  {
    "text": " I guess you are maximizing\nall-layer margin,",
    "start": "844020",
    "end": "849070"
  },
  {
    "text": "because all-layer\nmargin is bigger if you have more\nLipschitz model. You have a larger\nall-layer margin.",
    "start": "849070",
    "end": "855100"
  },
  {
    "text": "So none of these steps\ncan be made 100% rigorous. Some of the intermediate\nequations that I've written",
    "start": "855100",
    "end": "862449"
  },
  {
    "text": "are exactly true. But I don't think\nall of these steps can be made completely rigorous. But sometimes, this is probably\nthe nature of the networks",
    "start": "862450",
    "end": "870040"
  },
  {
    "text": "where you cannot be 100% precise\njust because things don't match exactly. But I think the\nintuition is really",
    "start": "870040",
    "end": "877330"
  },
  {
    "text": "just that the Hessian relates to\nthe Lipschitzness of the model",
    "start": "877330",
    "end": "883173"
  },
  {
    "text": "with respect to the parameter,\nand the Lipschitzness of the model with\nrespect to parameter relates to the\nLipschitzness of the model",
    "start": "883173",
    "end": "888860"
  },
  {
    "text": "with hidden variables,\nwhich is kind of like captured all-layer margin. ",
    "start": "888860",
    "end": "897640"
  },
  {
    "text": "Any questions? ",
    "start": "897640",
    "end": "923772"
  },
  {
    "text": "OK. So this is the\nfirst thing about--",
    "start": "923772",
    "end": "929560"
  },
  {
    "text": "this is the remaining\nsteps, the remaining remarks from the last lecture about\nthe implicit regularization",
    "start": "929560",
    "end": "936310"
  },
  {
    "text": "of the noise. And there's another\nthing I want to discuss, which is something I--",
    "start": "936310",
    "end": "942040"
  },
  {
    "text": "sometimes it's my omission. I forgot to provide a proof\nfor one of the theorems that we discussed I\nthink two weeks ago",
    "start": "942040",
    "end": "949270"
  },
  {
    "text": "about the implicit recognition\neffect in the classification case. I think there, we only--",
    "start": "949270",
    "end": "955269"
  },
  {
    "text": "basically, at the\nend of the lecture, we only were able\nto kind of provide a theorem and the\nbasic intuition.",
    "start": "955270",
    "end": "963460"
  },
  {
    "text": "But we weren't able to\nreally show the proof. The proof is very simple\nand short, just one page.",
    "start": "963460",
    "end": "968740"
  },
  {
    "text": "I think it's a very nice proof. So I really want\nto show it to you. So maybe let's discuss\nthat in the next part.",
    "start": "968740",
    "end": "975950"
  },
  {
    "text": "So I'll remind you what\nthe theorem was about. So guys, just this is I\nthink two lectures ago.",
    "start": "975950",
    "end": "986230"
  },
  {
    "text": " So two lectures ago, we\nshowed the following theorem.",
    "start": "986230",
    "end": "993420"
  },
  {
    "text": "The theorem was\nsomething like suppose maybe the context is\nthat we have linear model",
    "start": "993420",
    "end": "1002590"
  },
  {
    "text": "classification and we\nhave a gradient flow.",
    "start": "1002590",
    "end": "1009590"
  },
  {
    "text": "We have infinitesimal\nlearning rate, and want to understand\nwhat's the implicit bias",
    "start": "1009590",
    "end": "1016639"
  },
  {
    "text": "of the algorithm in this case. And the theorem that we had was\nthat gradient flow converges",
    "start": "1016640",
    "end": "1027199"
  },
  {
    "text": "to the direction of the max\nmargin solution in the sense",
    "start": "1027200",
    "end": "1043189"
  },
  {
    "text": "that so the margin\nof your intuition",
    "start": "1043190",
    "end": "1051379"
  },
  {
    "text": "is converging to the max margin\nsolution as t goes to infinity. ",
    "start": "1051380",
    "end": "1058810"
  },
  {
    "text": "So here, wt is iterate of\ngradient descent at time",
    "start": "1058810",
    "end": "1068580"
  },
  {
    "text": "t of gradient flow at time t.",
    "start": "1068580",
    "end": "1073855"
  },
  {
    "text": "And gamma is the\nnormalized margin. ",
    "start": "1073855",
    "end": "1082670"
  },
  {
    "text": "And gamma bar is the\nmax normalized margin.",
    "start": "1082670",
    "end": "1087770"
  },
  {
    "text": " I think at the end\nof the lecture,",
    "start": "1087770",
    "end": "1093140"
  },
  {
    "text": "I think I discussed\nthe intuition. The main intuition is that the\ncross entropy loss is really--",
    "start": "1093140",
    "end": "1100190"
  },
  {
    "text": "basically, you can\ndo an approximation. And in certain cases,\nthe cross entropy loss is an approximation\nof the max margin.",
    "start": "1100190",
    "end": "1109790"
  },
  {
    "text": "So the main intuition, if\nyou recall that lecture--",
    "start": "1109790",
    "end": "1115720"
  },
  {
    "text": "I will just very, very\nbriefly summarize this. So the main intuition\nis that if you do a bunch of\nheuristic calculations,",
    "start": "1115720",
    "end": "1122800"
  },
  {
    "text": "you can find out that\nthe log of the loss is approximately equal to--",
    "start": "1122800",
    "end": "1129055"
  },
  {
    "text": " let's see.",
    "start": "1129055",
    "end": "1135970"
  },
  {
    "text": "So the log of loss is\napproximately equals to minus",
    "start": "1135970",
    "end": "1146870"
  },
  {
    "text": "times the norm of w\ntimes the gamma of w. So basically,\nminimizing the loss",
    "start": "1146870",
    "end": "1153290"
  },
  {
    "text": "is kind of either you want\nto make the norm of w bigger, or you want to make\nthe margin bigger.",
    "start": "1153290",
    "end": "1159049"
  },
  {
    "text": "So we did this very\nheuristic simplification.",
    "start": "1159050",
    "end": "1165200"
  },
  {
    "text": "Give you this. So that's why if you want\nto minimize the loss, in some sense, you are either\ntrying to make the norm bigger,",
    "start": "1165200",
    "end": "1170990"
  },
  {
    "text": "or you are trying to\nmake the margin bigger. And it turns out\nthat you can actually control both of these\ntwo forces, these two",
    "start": "1170990",
    "end": "1178399"
  },
  {
    "text": "kind of tendencies. And if it's actually true\nthat the norm is growing",
    "start": "1178400",
    "end": "1184490"
  },
  {
    "text": "and the margin is also\ngrowing, both of them are trying to be big. And you can show that the\nnorm grows to infinity",
    "start": "1184490",
    "end": "1191120"
  },
  {
    "text": "and the margin grows to the\nmoderate-- the largest margin. So that's the thing we're\ngoing to prove in this theorem.",
    "start": "1191120",
    "end": "1198820"
  },
  {
    "text": "Any questions so far?  And then one of the key things\nthat we discussed at that point",
    "start": "1198820",
    "end": "1206520"
  },
  {
    "text": "was that the log sum exponential\nand one of the key techniques is that log sum exponential is\nkind of like the same as max",
    "start": "1206520",
    "end": "1216240"
  },
  {
    "text": "if your input has a large scale. So today, I'm going to provide\na formal proof for this theorem,",
    "start": "1216240",
    "end": "1224770"
  },
  {
    "text": "which is actually pretty-- in my opinion, it's\nvery elegant and simple. And we only prove it for--",
    "start": "1224770",
    "end": "1231740"
  },
  {
    "text": "prove for only the case\nwhen the loss function is minus exponential t,\nthe exponential loss.",
    "start": "1231740",
    "end": "1240920"
  },
  {
    "text": "Recall that in that\nlecture, we also discussed that the logistic\nloss, even though it's",
    "start": "1240920",
    "end": "1246470"
  },
  {
    "text": "called logistic\nloss, is actually very close to exponential loss. ",
    "start": "1246470",
    "end": "1253440"
  },
  {
    "text": "So we only deal with\nexponential loss, which is almost the\nsame as the logistical. So the main feature is\nthat as t goes to infinity,",
    "start": "1253440",
    "end": "1260780"
  },
  {
    "text": "the loss goes to 0. So the t is supposed\nto be the margin, and when the margin\nis very, very big,",
    "start": "1260780",
    "end": "1266930"
  },
  {
    "text": "your loss is very small. And the idea is that we can\nconsider the smooth margin.",
    "start": "1266930",
    "end": "1277710"
  },
  {
    "text": "So the smooth margin\nis defined to be-- I think in the lecture, we\ndefined the smooth margin",
    "start": "1277710",
    "end": "1283290"
  },
  {
    "text": "to be-- let me find out the-- ",
    "start": "1283290",
    "end": "1291970"
  },
  {
    "text": "OK, so I'm looking at-- ",
    "start": "1291970",
    "end": "1300430"
  },
  {
    "text": "So the smooth margin is\ndefined to be the following. So consider the smooth margin. ",
    "start": "1300430",
    "end": "1312310"
  },
  {
    "text": "So the smooth margin\nis defined to be the log of the empirical\nloss over the true norm of w.",
    "start": "1312310",
    "end": "1321059"
  },
  {
    "text": "So recall that we\nhave tried to-- I'm sorry, minus log.",
    "start": "1321060",
    "end": "1326670"
  },
  {
    "text": "So we have established\nthis equation last time during the intuition,\nand that actually",
    "start": "1326670",
    "end": "1332840"
  },
  {
    "text": "motivates the use of\nthe smooth margin. You can see that the\nsmooth margin is basically",
    "start": "1332840",
    "end": "1337910"
  },
  {
    "text": "supposed to be approximately\nequals to the margin, gamma of w, if this\napproximation is true.",
    "start": "1337910",
    "end": "1344420"
  },
  {
    "text": "But it's not exactly\nequal to that just because this is only\napproximately equals 2.",
    "start": "1344420",
    "end": "1349460"
  },
  {
    "text": "So that's why we work with\nthis smoother version, which is, in some sense, almost\nthe same as the margin,",
    "start": "1349460",
    "end": "1357440"
  },
  {
    "text": "but just more kind of closer\nto the loss function, l hat.",
    "start": "1357440",
    "end": "1362460"
  },
  {
    "text": "And if you work with\nthe smooth margin, you can show that the\nsmooth margin is actually-- I guess we have proved\nthis in the last lecture.",
    "start": "1362460",
    "end": "1369570"
  },
  {
    "text": "So the margin is actually\nbigger than the smooth margin. ",
    "start": "1369570",
    "end": "1377799"
  },
  {
    "text": "So I guess maybe let's just\nwrite out exactly what this is. This is minus log\nsum of n exponential",
    "start": "1377800",
    "end": "1386580"
  },
  {
    "text": "minus yi times w transposed\nxi the norm of w.",
    "start": "1386580",
    "end": "1393875"
  },
  {
    "text": " And you can show that the margin\nis larger than a smooth margin.",
    "start": "1393875",
    "end": "1401900"
  },
  {
    "text": "It's because we can replace each\nof these terms by the maximum-- by the minimum, right? This is just because\nyi w transposed",
    "start": "1401900",
    "end": "1410460"
  },
  {
    "text": "xi is less than gamma\nw times the norm of w.",
    "start": "1410460",
    "end": "1417830"
  },
  {
    "text": "Sorry, this is not n. So the margin is supposed to be\nsomething close to the margin,",
    "start": "1417830",
    "end": "1424370"
  },
  {
    "text": "but smaller. So that's why it\nsuffices to show",
    "start": "1424370",
    "end": "1431500"
  },
  {
    "text": "that the smooth margin, gamma to\nthe w, converges to gamma bar.",
    "start": "1431500",
    "end": "1440235"
  },
  {
    "text": "wt converges to gamma bar. This is because you\nhave the sandwich thing.",
    "start": "1440235",
    "end": "1447490"
  },
  {
    "text": "So you know that gamma w\nis always then gamma bar. So basically it's the smooth\nmargin is sandwiched between--",
    "start": "1447490",
    "end": "1456302"
  },
  {
    "text": "so if the smooth margin\nconverges to gamma bar, then gamma w has to\nconverge to gamma bar, because there is no way for\ngamma w to go beyond gamma bar.",
    "start": "1456302",
    "end": "1464440"
  },
  {
    "text": " So now basically this is\nwhat we're going to do.",
    "start": "1464440",
    "end": "1470810"
  },
  {
    "text": "We're going to prove\nthat even the smaller value, the smooth margin, is\ngoing to converge to gamma bar.",
    "start": "1470810",
    "end": "1476490"
  },
  {
    "text": "So then the larger value will\nalso converge to gamma bar. And the proof is actually\nalso pretty simple.",
    "start": "1476490",
    "end": "1482669"
  },
  {
    "text": "So we basically show that-- we'll show a gradient\nflow will increase",
    "start": "1482670",
    "end": "1497800"
  },
  {
    "text": "this quantity, the log of\nthe wt, log of the log-- the log loss, because it\ndecreases intuitively,",
    "start": "1497800",
    "end": "1511139"
  },
  {
    "text": "because it decreases l hat wt. So let's do this\nformally, concretely.",
    "start": "1511140",
    "end": "1519980"
  },
  {
    "text": "Because I think-- no. The statement\nitself, it increases the minus of the log loss.",
    "start": "1519980",
    "end": "1526130"
  },
  {
    "text": "That's kind of like almost\nobvious because the loss itself is going to increase. But how much it increases\nrequires some kind",
    "start": "1526130",
    "end": "1532850"
  },
  {
    "text": "of mathematical derivation. So concretely, recall\nthat the change in w",
    "start": "1532850",
    "end": "1540510"
  },
  {
    "text": "is minus gradient lwt. This is the definition\nof the gradient flow.",
    "start": "1540510",
    "end": "1547800"
  },
  {
    "text": "Then what you have is\nthat the derivative with respect to\nt of the change--",
    "start": "1547800",
    "end": "1558380"
  },
  {
    "text": "the changes in the log minus\nthe log loss is equal to--",
    "start": "1558380",
    "end": "1564170"
  },
  {
    "text": "so how does this change? You take the chain rule, right? So you first look at how\ndoes the loss depend on w.",
    "start": "1564170",
    "end": "1572445"
  },
  {
    "text": " And then you look at\nhow does w change,",
    "start": "1572445",
    "end": "1581140"
  },
  {
    "text": "and how does the\nloss depends on w? So does the derivative-- so then\nthis is a derivative of this,",
    "start": "1581140",
    "end": "1588200"
  },
  {
    "text": "which is minus--",
    "start": "1588200",
    "end": "1593522"
  },
  {
    "text": "you use the chain rule again. So you get l of wt is the\nchain rule for the log. And above, you get a gradient\nof l hat wt and then fw dot t.",
    "start": "1593522",
    "end": "1610799"
  },
  {
    "text": "And recall w dot t is really the\ngradient of the loss function. So basically, up to a sign.",
    "start": "1610800",
    "end": "1616330"
  },
  {
    "text": "So basically, you get\nthe gradient of the loss function 2 norm\nsquared over l of wt.",
    "start": "1616330",
    "end": "1626110"
  },
  {
    "text": "And this is bigger than 0. So this shows that\nthis minus log loss is going to increase\nas t goes to infinity.",
    "start": "1626110",
    "end": "1635170"
  },
  {
    "text": "But the important\nthing is how fast it increases is this quantity. This is something\nwe're going to use. This whole thing is\nincreasing is not surprising",
    "start": "1635170",
    "end": "1642490"
  },
  {
    "text": "because the loss is decreasing. But we also want to know\nhow fast this is increasing. And by the way,\nyou can actually--",
    "start": "1642490",
    "end": "1648910"
  },
  {
    "text": "I think it's useful to\nuse this, because we're going to compare it with.",
    "start": "1648910",
    "end": "1655802"
  },
  {
    "text": "You can also write this\nas this, equals to this, just because the nabla l hat\nis just equal to w dot t, OK?",
    "start": "1655802",
    "end": "1663090"
  },
  {
    "text": " So now with this,\nwhat we can do is",
    "start": "1663090",
    "end": "1671260"
  },
  {
    "text": "that we can control\nwhat, eventually, after t step, what\nhappens with the log loss.",
    "start": "1671260",
    "end": "1678400"
  },
  {
    "text": "So what you get is\nthat minus log l hat wt",
    "start": "1678400",
    "end": "1684980"
  },
  {
    "text": "is equal to minus log l hat\nw0 plus the integral between 0",
    "start": "1684980",
    "end": "1692750"
  },
  {
    "text": "and t of the derivative\nof this quantity. ",
    "start": "1692750",
    "end": "1706800"
  },
  {
    "text": "And this is going to be\nusing the equation above. You got that this log w\n0 plus the integral of w",
    "start": "1706800",
    "end": "1716870"
  },
  {
    "text": "dot t 2 norm\nsquared over lwt dt. OK? ",
    "start": "1716870",
    "end": "1727070"
  },
  {
    "text": "So we basically\nnow know how fast-- how large is the\nlog loss, right?",
    "start": "1727070",
    "end": "1732770"
  },
  {
    "text": "So recall that what we care\nabout is this quantity. What we care about is-- ",
    "start": "1732770",
    "end": "1740320"
  },
  {
    "text": "what we care about is this\nand how does this goes to-- how does this go to gamma\nbar as t goes to infinity?",
    "start": "1740320",
    "end": "1748060"
  },
  {
    "text": "And we have already\ndealt with the numerator, and we just have to--",
    "start": "1748060",
    "end": "1754000"
  },
  {
    "text": "we know how does\nthis-- we somewhat know how does this\nchange for the numerator. And we have to--",
    "start": "1754000",
    "end": "1760029"
  },
  {
    "text": "again, another thing\nis that we have to try to understand\nthe denominator, right?",
    "start": "1760030",
    "end": "1767960"
  },
  {
    "text": "So the denominator, you\nhave to normalize this by the norm of w. ",
    "start": "1767960",
    "end": "1773637"
  },
  {
    "text": "So basically, next\nthing is that we're going to go with this\nterm and compare it with the normalizer norm of w.",
    "start": "1773637",
    "end": "1779419"
  },
  {
    "text": "So what you do is that you\nlook at the w dot t squared.",
    "start": "1779420",
    "end": "1785690"
  },
  {
    "text": "This is bigger than\nw dot t times w star.",
    "start": "1785690",
    "end": "1792669"
  },
  {
    "text": "Recall w star is the direction\nof max margin solution.",
    "start": "1792670",
    "end": "1800430"
  },
  {
    "start": "1800430",
    "end": "1805517"
  },
  {
    "text": "This is just by\nCauchy-Schwarz, right? So the inner product\nof two vectors is less than the norm\nof one vector times",
    "start": "1805517",
    "end": "1811679"
  },
  {
    "text": "the norm of the other vector. And the norm of the double\nstar is assumed to be 1. So then we plug in the\ndefinition of the w",
    "start": "1811680",
    "end": "1821460"
  },
  {
    "text": "dot minus gradient l wt w star.",
    "start": "1821460",
    "end": "1829029"
  },
  {
    "text": "And then we plug in the true\ndefinition of the nabla l. So we plug in the\nderivation for the nabla l.",
    "start": "1829030",
    "end": "1837680"
  },
  {
    "text": "So this equals to yi times\nthe exponential minus yi xw",
    "start": "1837680",
    "end": "1846200"
  },
  {
    "text": "transposed xi times\nxi and times w star.",
    "start": "1846200",
    "end": "1851779"
  },
  {
    "text": " And then this is a vector, this\nis a scalar, this is a scalar.",
    "start": "1851780",
    "end": "1857880"
  },
  {
    "text": "So basically, you can just\ntake any part of these two and matched by the scalar. So this will be equal to--",
    "start": "1857880",
    "end": "1866039"
  },
  {
    "text": "I guess there's no minus here\nbecause there's another minus in the gradient, which cancels. So then this is equal\nto sum of yi times",
    "start": "1866040",
    "end": "1875730"
  },
  {
    "text": "exponential minus\nyi w transposed xi",
    "start": "1875730",
    "end": "1881070"
  },
  {
    "text": "times w star times xi. ",
    "start": "1881070",
    "end": "1887410"
  },
  {
    "text": "I guess maybe let's write\nthis w star transposed xi.",
    "start": "1887410",
    "end": "1892640"
  },
  {
    "start": "1892640",
    "end": "1898480"
  },
  {
    "text": "And this, we can see that this\nis the margin of the max margin",
    "start": "1898480",
    "end": "1903760"
  },
  {
    "text": "because w star is the\nmax margin solution. So this is always bigger\nthan the max margin.",
    "start": "1903760",
    "end": "1910320"
  },
  {
    "text": "So this is larger\nthan gamma bar times--",
    "start": "1910320",
    "end": "1915654"
  },
  {
    "text": " I guess let me finish--",
    "start": "1915655",
    "end": "1923000"
  },
  {
    "text": "let me explain, because\nyi w star transposed",
    "start": "1923000",
    "end": "1932500"
  },
  {
    "text": "xi is bigger than gamma bar. This is just because gamma\nbar is the margin of w star.",
    "start": "1932500",
    "end": "1938649"
  },
  {
    "text": "So that's why every\ndata point has a bigger margin than the\nmargin of the data set.",
    "start": "1938650",
    "end": "1944460"
  },
  {
    "text": "Gamma bar is essentially minimum\nover all data sets, right? And then this is equal to\ngamma bar times the loss.",
    "start": "1944460",
    "end": "1951510"
  },
  {
    "text": " So with this, then we can\nproceed by dealing with--",
    "start": "1951510",
    "end": "1958310"
  },
  {
    "text": "we can proceed by\ndealing with this term to further lower bounds how--",
    "start": "1958310",
    "end": "1963830"
  },
  {
    "text": "control how fast you grow. So with this, you get log l\nhat wt is large than minus log",
    "start": "1963830",
    "end": "1972390"
  },
  {
    "text": "l hat w0 plus--",
    "start": "1972390",
    "end": "1979710"
  },
  {
    "start": "1979710",
    "end": "1985659"
  },
  {
    "text": "so maybe just one\nmore before I use. This let me just\ntry to interpret what this is really doing.",
    "start": "1985660",
    "end": "1990730"
  },
  {
    "text": "So this-- let's see. ",
    "start": "1990730",
    "end": "2009470"
  },
  {
    "text": "So in some sense, as a remark,\nwhat this is really doing is that so in wt,\nwe show that wt--",
    "start": "2009470",
    "end": "2016340"
  },
  {
    "text": "so we show that wt is\ncorrelated with w star.",
    "start": "2016340",
    "end": "2026985"
  },
  {
    "text": "That's what we are showing. So we are showing that the w\ndot t times w star is bigger",
    "start": "2026985",
    "end": "2033270"
  },
  {
    "text": "than a non-negative quantity. Now how correlated\nis this depends on-- and the correlation depends\non gamma bar and the loss.",
    "start": "2033270",
    "end": "2044940"
  },
  {
    "text": " So in some sense, the-- and\nbecause you are correlated with",
    "start": "2044940",
    "end": "2051730"
  },
  {
    "text": "the w star, it means\nthat you cannot-- w dot t itself\ncannot be too small.",
    "start": "2051730",
    "end": "2058419"
  },
  {
    "text": "And so this is another\nthing we got right. So w dot t is not too small,\nat least compared to the loss.",
    "start": "2058420",
    "end": "2068989"
  },
  {
    "text": " So what is he saying is that if\nthe loss is not too small, then",
    "start": "2068989",
    "end": "2076340"
  },
  {
    "text": "you have to make some\nchanges in your w. And if you have to make\nsome changes in your w,",
    "start": "2076340",
    "end": "2082032"
  },
  {
    "text": "then you have to\nmake some changes in the log of the l hat wt. So basically, if the\nloss is not small,",
    "start": "2082032",
    "end": "2088070"
  },
  {
    "text": "then your log of the\nloss needs to increase. The minus log of the\nloss needs to increase.",
    "start": "2088070",
    "end": "2093149"
  },
  {
    "text": "So it's a little\ncounterintuitive in some sense, but I guess--",
    "start": "2093150",
    "end": "2098180"
  },
  {
    "text": "so what we do next is that this\ncontrol this additional terms that are circled here.",
    "start": "2098180",
    "end": "2103530"
  },
  {
    "text": "So this term, if you\nuse the equation we got,",
    "start": "2103530",
    "end": "2113770"
  },
  {
    "text": "we got this is larger\nthan gamma bar times",
    "start": "2113770",
    "end": "2120530"
  },
  {
    "text": "you cancel out one of the law-- you can use this for one of\nthe-- there is a power of 2",
    "start": "2120530",
    "end": "2126470"
  },
  {
    "text": "here. You can use the\nequation-- maybe let's get this equation one for one\nof these occurrences of w dot t.",
    "start": "2126470",
    "end": "2134260"
  },
  {
    "text": "So then you get-- you\nare left with one, and then you got the\ngamma bar and I hat. l",
    "start": "2134260",
    "end": "2140325"
  },
  {
    "text": "hat got canceled with the\ndenominator and gamma bar is put in the front,\nso we get this.",
    "start": "2140325",
    "end": "2145740"
  },
  {
    "text": " So basically, I'm applying\nequation one for one",
    "start": "2145740",
    "end": "2152579"
  },
  {
    "text": "of the w do t true norm. And then you can use a\ntriangle inequality--",
    "start": "2152580",
    "end": "2158460"
  },
  {
    "text": "this is by one-- and use the triangle\ninequality to say that this is larger\nthan the integral of w",
    "start": "2158460",
    "end": "2165795"
  },
  {
    "text": "dot t true norm squared dt-- get rid of that.",
    "start": "2165795",
    "end": "2172079"
  },
  {
    "text": "This is replacing the\nintegral with the norm, and got gamma bar\ntimes a norm of wt.",
    "start": "2172080",
    "end": "2181270"
  },
  {
    "text": "So I guess next, you're going\nto see why we care about all of this. Because we care about\nthis because now you",
    "start": "2181270",
    "end": "2187180"
  },
  {
    "text": "can control how fast\nl hat, this log loss",
    "start": "2187180",
    "end": "2193250"
  },
  {
    "text": "is improving compared to\nhow fast the norm of w",
    "start": "2193250",
    "end": "2200490"
  },
  {
    "text": "is improving. ",
    "start": "2200490",
    "end": "2207099"
  },
  {
    "text": "And this is what we\nreally care about because fundamentally, we care\nabout the ratio between them. This is the definition\nof the soft margin,",
    "start": "2207100",
    "end": "2214080"
  },
  {
    "text": "or the smooth margin.  So this means that the ratio\nis getting closer to gamma bar.",
    "start": "2214080",
    "end": "2227450"
  },
  {
    "text": "So this term is a\nconstant and this term is something that becomes closer\nto 0 as t goes to infinity.",
    "start": "2227450",
    "end": "2237150"
  },
  {
    "text": "So wt goes to infinity\nas t goes to 0--",
    "start": "2237150",
    "end": "2243150"
  },
  {
    "text": "as t goes to infinity. That's why this term\nhere converge to 0",
    "start": "2243150",
    "end": "2251440"
  },
  {
    "text": "as t goes to infinity. So that's why, if you take the\nlimit, when t goes to infinity,",
    "start": "2251440",
    "end": "2260432"
  },
  {
    "text": "we got this smooth margin.",
    "start": "2260432",
    "end": "2265450"
  },
  {
    "text": "So we call that this\nratio is the smooth margin is converging to gamma bar.",
    "start": "2265450",
    "end": "2272099"
  },
  {
    "text": "So in other words, the limit\nt to infinity gamma tilde wt",
    "start": "2272100",
    "end": "2279050"
  },
  {
    "text": "is equal to gamma bar.  Maybe here you\nonly get negative,",
    "start": "2279050",
    "end": "2285620"
  },
  {
    "text": "and then you use the\nother way to show that. And you also know that-- OK, so we also know gamma bar is\nlarger than the margin of any w",
    "start": "2285620",
    "end": "2298460"
  },
  {
    "text": "because gamma bar is\nthe max margin, which is larger than wt.",
    "start": "2298460",
    "end": "2304040"
  },
  {
    "text": "And then you can show\nthat the limit is actually equal to gamma bar exactly.",
    "start": "2304040",
    "end": "2310080"
  },
  {
    "text": "So we're good. ",
    "start": "2310080",
    "end": "2316436"
  },
  {
    "text": "Any questions? ",
    "start": "2316436",
    "end": "2328182"
  },
  {
    "text": "OK. So I guess with this, we\nbasically concluded our section",
    "start": "2328182",
    "end": "2334630"
  },
  {
    "text": "about implicit regularization. So I guess just to very\nquickly briefly wrap up,",
    "start": "2334630",
    "end": "2341480"
  },
  {
    "text": "so this is the\nend of the section about implicit\nregularization, and we",
    "start": "2341480",
    "end": "2348310"
  },
  {
    "text": "have talked about a bunch of\nthings like initialization. So a small initialization\nprefers a certain kind",
    "start": "2348310",
    "end": "2355100"
  },
  {
    "text": "of solution, typically\na small norm solution--",
    "start": "2355100",
    "end": "2360140"
  },
  {
    "text": "prefers small norm solution. ",
    "start": "2360140",
    "end": "2369078"
  },
  {
    "text": "And we are-- actually,\nin one of the cases, we also show that you\ncan have interpolation between small initialization\nand large initialization.",
    "start": "2369078",
    "end": "2376060"
  },
  {
    "text": "So in that case, you can\nshow the implicit bias for any initialization. And we also talk about the\nclassification problem,",
    "start": "2376060",
    "end": "2386500"
  },
  {
    "text": "so where you got the max margin. So this is where you get\nthe max margin solution.",
    "start": "2386500",
    "end": "2392400"
  },
  {
    "text": "And we also talk\nabout a lot the noise.",
    "start": "2392400",
    "end": "2397450"
  },
  {
    "text": "So in all these cases, it's\nkind of like you have something in your optimizer that is only\ndesigned for optimizing faster",
    "start": "2397450",
    "end": "2405299"
  },
  {
    "text": "in some sense, but\nsomehow, as a side effect, you get implicit\nregularization effect. OK.",
    "start": "2405300",
    "end": "2410480"
  },
  {
    "start": "2410480",
    "end": "2421359"
  },
  {
    "text": "So any questions?",
    "start": "2421360",
    "end": "2426780"
  },
  {
    "start": "2426780",
    "end": "2451805"
  },
  {
    "text": "OK. So if there's no\nquestions, let me move on to the final part\nof this lecture--",
    "start": "2451805",
    "end": "2458680"
  },
  {
    "text": "of this course,\nwhich is more about unsupervised\nlearning reputation, and so on and so forth.",
    "start": "2458680",
    "end": "2463898"
  },
  {
    "text": " So in this lecture and the last\ntwo lectures, you still have--",
    "start": "2463898",
    "end": "2470530"
  },
  {
    "text": "so basically, in the\nnext 2.5 lectures, we're going to talk about\nunsupervised learning.",
    "start": "2470530",
    "end": "2477789"
  },
  {
    "text": "There are not that\nmany theoretical work about unsupervised learning.",
    "start": "2477790",
    "end": "2483280"
  },
  {
    "text": "Of course, there are a lot of\nvery amazing empirical works these days, but not that\nmany are theoretical work.",
    "start": "2483280",
    "end": "2492352"
  },
  {
    "text": "So what I'm going\nto do is that I'm going to start with somewhat\nkind of classical approach a little bit.",
    "start": "2492352",
    "end": "2497809"
  },
  {
    "text": "So for this lecture and the\nbeginning of the next lecture, or maybe a good portion\nof the next lecture,",
    "start": "2497810",
    "end": "2504080"
  },
  {
    "text": "I'm going to talk about\nthe classical approach-- I mean, a classical\ntheoretical approach.",
    "start": "2504080",
    "end": "2510850"
  },
  {
    "text": "So there are many,\nmany approaches before, like, for example, the most\nempirically, probably--",
    "start": "2510850",
    "end": "2517190"
  },
  {
    "text": "before deep learning, the\nbest empirical approach would be probably you do\nlatent variable models with EM,",
    "start": "2517190",
    "end": "2524240"
  },
  {
    "text": "expectation-maximization. But I'm going to for\nthose kind EM algorithms,",
    "start": "2524240",
    "end": "2529650"
  },
  {
    "text": "there are very little\ntheoretical analysis. And even their analysis, it's\nkind of like special case, and it's not clear\nwhether they can",
    "start": "2529650",
    "end": "2535578"
  },
  {
    "text": "be extended to a complex case. So what I'm going to talk\nabout is a different line",
    "start": "2535578",
    "end": "2540590"
  },
  {
    "text": "of research, which uses the\nso-called moment method. So these kinds of\nmethods don't necessarily",
    "start": "2540590",
    "end": "2547240"
  },
  {
    "text": "work very well empirically,\nbut they have very good--",
    "start": "2547240",
    "end": "2555160"
  },
  {
    "text": "you can analyze them\nin a very clean way. And these kinds of\nmathematical techniques",
    "start": "2555160",
    "end": "2560650"
  },
  {
    "text": "are also useful for\nmany other cases. So I think it's worth\nspending one lecture to talk about this approach.",
    "start": "2560650",
    "end": "2566799"
  },
  {
    "text": "And it used to be the case\nthat actually, around probably 2012, 2013, at\nthat point, I think",
    "start": "2566800",
    "end": "2572832"
  },
  {
    "text": "the community, the\ntheoretical community, thought that this\nmight be the new thing. This could be the new thing\nthat you can both analyze",
    "start": "2572832",
    "end": "2579970"
  },
  {
    "text": "and empirically work. It turns out that the analysis\npart developed-- got developed",
    "start": "2579970",
    "end": "2586810"
  },
  {
    "text": "very well, but\nthe empirical part is doing OK, but not good enough\nto replace the EM algorithms.",
    "start": "2586810",
    "end": "2594069"
  },
  {
    "text": "At least not enough to\nreplace them completely. And then I'm going\nto talk about some",
    "start": "2594070",
    "end": "2600490"
  },
  {
    "text": "of the more modern work\nwith deep learning--",
    "start": "2600490",
    "end": "2607317"
  },
  {
    "text": "with deep learning,\nlike, for example, self-training or\ncontrastive learning.",
    "start": "2607317",
    "end": "2614549"
  },
  {
    "text": "So these are basically\nanalyses in the last one or two years about some of the new\nalgorithms in deep learning.",
    "start": "2614550",
    "end": "2621870"
  },
  {
    "text": " So I'm going to spend\nprobably the last lecture-- and the last 1.5\nlectures on this.",
    "start": "2621870",
    "end": "2629260"
  },
  {
    "text": "OK, so that's the plan\nfor the next 2.5 lectures. And so today, I'm going to talk\nabout a classical approach,",
    "start": "2629260",
    "end": "2637275"
  },
  {
    "text": "right?  And by the way, another\nkind of general comment",
    "start": "2637275",
    "end": "2643430"
  },
  {
    "text": "is that in my opinion\nthis unsupervised learning seems to be the core\nfor many things, right? So this also relates\nto, for example,",
    "start": "2643430",
    "end": "2652420"
  },
  {
    "text": "semi-supervised learning, where\nyou have some unlabeled data together with labeled data. And this also relates to\nunsupervised domain adaptation.",
    "start": "2652420",
    "end": "2662040"
  },
  {
    "text": "And my personal\nopinion is that all of these questions, what really\nyou care about is really--",
    "start": "2662040",
    "end": "2669160"
  },
  {
    "text": "in both of these questions,\nwhat you really care about is how do you leverage\nunlabeled data. So in some sense,\nthey all reduce",
    "start": "2669160",
    "end": "2674785"
  },
  {
    "text": "this to unsupervised\nlearning, in my opinion. ",
    "start": "2674785",
    "end": "2681410"
  },
  {
    "text": "So now let's get into\nsomething more concrete. ",
    "start": "2681410",
    "end": "2686859"
  },
  {
    "text": "So let's say-- let's\nhave some setup. So this is the setup.",
    "start": "2686860",
    "end": "2694400"
  },
  {
    "text": "This is with latent\nvariable models-- latent variable models. ",
    "start": "2694400",
    "end": "2702280"
  },
  {
    "text": "So we are interested in those\nconflicting variable models, especially in a\nclassical approach.",
    "start": "2702280",
    "end": "2707390"
  },
  {
    "text": "So the formulation is that you\nhave a distribution, p theta,",
    "start": "2707390",
    "end": "2714400"
  },
  {
    "text": "parameterized by theta. How it's parameterized\nby theta, that would be--",
    "start": "2714400",
    "end": "2720040"
  },
  {
    "text": "there are many\ndifferent ways, which I'm going to introduce\na few of them. But every parameter decides\nthe distribution, p of theta.",
    "start": "2720040",
    "end": "2726850"
  },
  {
    "text": "And then you are given\nunlabeled examples.",
    "start": "2726850",
    "end": "2731890"
  },
  {
    "text": "There's no labels anywhere. So you're given\nexamples x1 up to xn.",
    "start": "2731890",
    "end": "2737539"
  },
  {
    "text": "They are sampled iid from\nthis distribution, p theta. And your goal is\nto recover theta--",
    "start": "2737540",
    "end": "2747550"
  },
  {
    "text": " or learn theta from the data.",
    "start": "2747550",
    "end": "2753460"
  },
  {
    "text": "From the data.  So that's the formulation.",
    "start": "2753460",
    "end": "2759049"
  },
  {
    "text": "And p theta can be described\nas a latent variable model,",
    "start": "2759050",
    "end": "2764490"
  },
  {
    "text": "or can be, or typically is\ndescribed by a latent variable",
    "start": "2764490",
    "end": "2769735"
  },
  {
    "text": "model.  So everything that describes a\ngenerative model in some sense.",
    "start": "2769735",
    "end": "2776450"
  },
  {
    "text": "So for example, I assume you\nsomehow know roughly speaking what latent variable\nmodel is from CS29,",
    "start": "2776450",
    "end": "2784520"
  },
  {
    "text": "but let me give some examples. For example,\nmixture of Gaussian. This is one of probably--",
    "start": "2784520",
    "end": "2791950"
  },
  {
    "text": "the most studied executions\nin machine learning.",
    "start": "2791950",
    "end": "2798119"
  },
  {
    "text": "So the assumption is that\nin the most general sense, the theta is-- so the parameters describe\na bunch of things.",
    "start": "2798120",
    "end": "2807020"
  },
  {
    "text": "So let me write it down first. So you have a bunch of vectors,\nk vectors, and a probability--",
    "start": "2807020",
    "end": "2814390"
  },
  {
    "text": "a bunch of probability numbers. So each of these\nmu i in dimension d",
    "start": "2814390",
    "end": "2820840"
  },
  {
    "text": "is the mean of the component.",
    "start": "2820840",
    "end": "2827540"
  },
  {
    "text": "And p1 up to pk, this\nis a probability vector",
    "start": "2827540",
    "end": "2837240"
  },
  {
    "text": "in the simplex, right? Let's call it theta k a\nsimplex in k dimension, which",
    "start": "2837240",
    "end": "2842880"
  },
  {
    "text": "is basically a set of\nvectors with norm one--",
    "start": "2842880",
    "end": "2848250"
  },
  {
    "text": "sorry, norm equals to 1\nnon-negative in dimension k,",
    "start": "2848250",
    "end": "2854635"
  },
  {
    "text": "right? So p1 up to pk is a probability\nvector over k items.",
    "start": "2854635",
    "end": "2860215"
  },
  {
    "start": "2860215",
    "end": "2869265"
  },
  {
    "text": "And given these parameters,\nwhat's the model? How do you generate? So this is my parameter, and\nhow do you generate data?",
    "start": "2869265",
    "end": "2875200"
  },
  {
    "text": "So it's a mixture of functions. So intuitively, you just\nwant to model the case",
    "start": "2875200",
    "end": "2880410"
  },
  {
    "text": "where you have, for example,\nsomething like this. You have several clusters of\ndata, something like this.",
    "start": "2880410",
    "end": "2888599"
  },
  {
    "text": "I guess you don't see\nthe color in the data, you just see the raw inputs.",
    "start": "2888600",
    "end": "2894260"
  },
  {
    "text": "The color is just to indicate\nwhich Gaussian it comes from. So mathematically, you say\nthat you sample x from p theta",
    "start": "2894260",
    "end": "2903560"
  },
  {
    "text": "by your first sample\nsum i, the cluster id from a categorical\ndistribution defined by p.",
    "start": "2903560",
    "end": "2913400"
  },
  {
    "text": "So i is between-- i can take values\nfrom 1 to k And then",
    "start": "2913400",
    "end": "2919910"
  },
  {
    "text": "given the id, the cluster\nid, you sample a Gaussian with mu sub i, and then some\ncovariant, let's say, identity.",
    "start": "2919910",
    "end": "2928430"
  },
  {
    "text": "So actually, the\ncovariance can also be a parameter\nyou want to learn. But here, for\nsimplicity, I just assume",
    "start": "2928430",
    "end": "2933830"
  },
  {
    "text": "all the Gaussians have\nthe same covariance just to make everything easier.",
    "start": "2933830",
    "end": "2941540"
  },
  {
    "text": "So this is the latent\nvariable model, so where i is the latent variable.",
    "start": "2941540",
    "end": "2946990"
  },
  {
    "text": "This is something you\ndon't observe in data. You only observe x.",
    "start": "2946990",
    "end": "2953630"
  },
  {
    "text": "But given the latent variable,\nyou can generate the data. So basically, there\nare two parts,",
    "start": "2953630",
    "end": "2958682"
  },
  {
    "text": "where you first\ngenerate a variable and then generate\ndata under the hood.",
    "start": "2958682",
    "end": "2965168"
  },
  {
    "text": "And then in the\nother approach, which I'm going to define probably\nmostly when I'm use it-- I'm going to use it.",
    "start": "2965168",
    "end": "2970380"
  },
  {
    "text": "So HMM, the hidden Markov model.",
    "start": "2970380",
    "end": "2975660"
  },
  {
    "text": "If you take some NLP\nclass, probably you have seen these kinds of things. Or ICA, independent\ncomponent analysis.",
    "start": "2975660",
    "end": "2982799"
  },
  {
    "text": "This is also something, I\nthink, covered in CS229.",
    "start": "2982800",
    "end": "2991767"
  },
  {
    "text": "And so there are\nmany, many other kind of latent viable models,\nBayes nets and so forth.",
    "start": "2991767",
    "end": "2997730"
  },
  {
    "text": "So this is the final question\nwe're going to study. And now let's talk\nabout the approach. So the approach-- maybe\nbefore that, any questions?",
    "start": "2997730",
    "end": "3005860"
  },
  {
    "text": " OK. So the approach\nwe're going to study",
    "start": "3005860",
    "end": "3012720"
  },
  {
    "text": "is the so-called\nmoment method, which is actually pretty powerful. As an approach, there\nare some drawbacks,",
    "start": "3012720",
    "end": "3020500"
  },
  {
    "text": "which make it empirically\nless appealing. But the approach\nitself, if you don't",
    "start": "3020500",
    "end": "3025530"
  },
  {
    "text": "have a certain kind\nof aspects, then it's actually pretty powerful. And this is called\nMoment method.",
    "start": "3025530",
    "end": "3031650"
  },
  {
    "text": "I think this method is\nproposed by, actually, an economist, or actually\na few economists,",
    "start": "3031650",
    "end": "3039600"
  },
  {
    "text": "to understand data\nfrom economy-- from economists-- from,\nI think, I don't know.",
    "start": "3039600",
    "end": "3049860"
  },
  {
    "text": "I don't know, some kind of-- so the original\nsource is definitely not machine learning.",
    "start": "3049860",
    "end": "3055410"
  },
  {
    "text": "But then people use\nthis for machine learning these days,\nwith, actually, a pretty complicated approach.",
    "start": "3055410",
    "end": "3063160"
  },
  {
    "text": "Actually, even though I think-- actually, I think I misspoke.",
    "start": "3063160",
    "end": "3068730"
  },
  {
    "text": "The very original proposal\nof this moment method actually probably dates\nback to 19th centuries",
    "start": "3068730",
    "end": "3075990"
  },
  {
    "text": "by some statisticians. And then actually\nsome economists got--",
    "start": "3075990",
    "end": "3082980"
  },
  {
    "text": "even got the Nobel\nPrize by generalizing this modern methods\nto something like what",
    "start": "3082980",
    "end": "3088590"
  },
  {
    "text": "we are discussing right now. Anyway, so let's see\nhow does this work.",
    "start": "3088590",
    "end": "3097880"
  },
  {
    "text": "So I'm going to just only-- I'm going to walk you\nthrough this kind of method by showing examples.",
    "start": "3097880",
    "end": "3104000"
  },
  {
    "text": "So let's do the first example. ",
    "start": "3104000",
    "end": "3111650"
  },
  {
    "text": "So first example,\nlet's talk about of mixture of two Gaussians.",
    "start": "3111650",
    "end": "3118019"
  },
  {
    "text": " So you just have two Gaussians.",
    "start": "3118020",
    "end": "3123570"
  },
  {
    "text": "And I think that-- and also\nlet's say k is 2, right? And then let's also assume\np1 and p2 are just a half.",
    "start": "3123570",
    "end": "3135500"
  },
  {
    "text": "So these two quotients\nhave the same probability. So they have the same\nmarginal density.",
    "start": "3135500",
    "end": "3141109"
  },
  {
    "text": "And also, with the\nloss of generality, we can assume the min is--\nthe average of the min is 0.",
    "start": "3141110",
    "end": "3148790"
  },
  {
    "text": "So basically, they are just\nsymmetric around origin. This is, in some\nsense, [INAUDIBLE]",
    "start": "3148790",
    "end": "3156440"
  },
  {
    "text": "because which point you\nchoose at the origin wouldn't really\nmatter that much. ",
    "start": "3156440",
    "end": "3163070"
  },
  {
    "text": "So then mu 1, you\ncan write mu 1. ",
    "start": "3163070",
    "end": "3168960"
  },
  {
    "text": "So let mu to be equal\nto mu 1, and then mu 2",
    "start": "3168960",
    "end": "3175290"
  },
  {
    "text": "is equal to minus mu. So basically, we only want\nto learn one parameter vector, which is\nmu, and the data",
    "start": "3175290",
    "end": "3182640"
  },
  {
    "text": "comes from this mixture\nof two Gaussians. One Gaussian is min mu\nand covariance entity.",
    "start": "3182640",
    "end": "3189380"
  },
  {
    "text": "Another Gaussian is min\nminus mu covariance entity. ",
    "start": "3189380",
    "end": "3196570"
  },
  {
    "text": "And the moment method-- ",
    "start": "3196570",
    "end": "3202875"
  },
  {
    "text": "so the general approach for the\nmoment method is the following. So first, you estimate moment\nof x using empirical samples.",
    "start": "3202875",
    "end": "3217705"
  },
  {
    "text": " I'm going to define what\nexactly a moment really means.",
    "start": "3217705",
    "end": "3223410"
  },
  {
    "text": "Moment really means the-- I guess depending on whether\nyou have any background-- I could always define\nwhat moment really means.",
    "start": "3223410",
    "end": "3230130"
  },
  {
    "text": "And then what you do is\nyou recover parameters",
    "start": "3230130",
    "end": "3238869"
  },
  {
    "text": "from moment of x. And by moment, we really\nmean something like this.",
    "start": "3238870",
    "end": "3244869"
  },
  {
    "text": "So the first moment, this means\nthe average of x of the data.",
    "start": "3244870",
    "end": "3252965"
  },
  {
    "text": " So the first moment-- and let's try to do this\nfor this particular example.",
    "start": "3252965",
    "end": "3260460"
  },
  {
    "text": "So if you do the first\nmoment, then the first moment is the expectation of x.",
    "start": "3260460",
    "end": "3266869"
  },
  {
    "text": "And what is expectation of x? There are two cases. One case is that you have a\nlatent variable, which is 1,",
    "start": "3266870",
    "end": "3272960"
  },
  {
    "text": "and the other case is\nthe latent variable is 2. So you can look at the\nexpectation of x for both of the two Gaussians, right?",
    "start": "3272960",
    "end": "3278597"
  },
  {
    "text": "So with half the chance you\ncome from the first Gaussian, and that's the case when i is 1.",
    "start": "3278597",
    "end": "3283705"
  },
  {
    "text": "There's half the chance you\ncome from the second Gaussian.",
    "start": "3283705",
    "end": "3288820"
  },
  {
    "text": "And when it comes from the\nfirst Gaussian, the min is mu. So that's the definition,\nso you get a half times mu.",
    "start": "3288820",
    "end": "3296829"
  },
  {
    "text": "When you come from the second\nGaussian, the min is minus mu, so you get minus mu, which is 0.",
    "start": "3296830",
    "end": "3302920"
  },
  {
    "text": "So this means that\nthere is no information",
    "start": "3302920",
    "end": "3308510"
  },
  {
    "text": "about mu from the first moment. ",
    "start": "3308510",
    "end": "3317310"
  },
  {
    "text": "Not so good. So this is not our plan. Our plan is to recover\nmu from the moments. But from the first moment, we\ncannot really get anything.",
    "start": "3317310",
    "end": "3325620"
  },
  {
    "text": "So then what you do is you\ngo to the second moment. So the second moment\nis the expectation,",
    "start": "3325620",
    "end": "3336000"
  },
  {
    "text": "let's call it called M2. Maybe I should\ncall it M1 as well. So a second moment\nis M2, is defined",
    "start": "3336000",
    "end": "3342119"
  },
  {
    "text": "to be the expectation\nof the ultra",
    "start": "3342120",
    "end": "3348350"
  },
  {
    "text": "paradox of x with x itself. There's this expectation\nof x and x transposed.",
    "start": "3348350",
    "end": "3355319"
  },
  {
    "text": "So why is this called\nthe second moment? This is really-- this\nis a matrix, basically.",
    "start": "3355320",
    "end": "3362406"
  },
  {
    "text": "Basically, you can see that\nM2ij is the expectation of xixj.",
    "start": "3362406",
    "end": "3370670"
  },
  {
    "text": "So basically, this expectation\nof the product of two coordinates of the data.",
    "start": "3370670",
    "end": "3379670"
  },
  {
    "text": "And you organize all\nof this into a matrix and you call it M2. ",
    "start": "3379670",
    "end": "3388340"
  },
  {
    "text": "And if you compute the second\nmoment, then you can see, actually, mu is--",
    "start": "3388340",
    "end": "3394040"
  },
  {
    "text": "you can kind of see mu from it. So how do I compute\nthe second moment? Again, the same thing\nwith half of chance,",
    "start": "3394040",
    "end": "3400940"
  },
  {
    "text": "your x from the same-- from the first Gaussian,\nwith the half of chance your x comes from\nthe second Gaussian.",
    "start": "3400940",
    "end": "3407380"
  },
  {
    "text": " And when it comes from\nthe first Gaussian,",
    "start": "3407380",
    "end": "3413490"
  },
  {
    "text": "so what's the covariance-- what's the second moment of\nx under the first Gaussian?",
    "start": "3413490",
    "end": "3419319"
  },
  {
    "text": "So this requires a little\nbit of calculation. So let's do that here. So suppose x come\nfrom a Gaussian",
    "start": "3419320",
    "end": "3427900"
  },
  {
    "text": "with min mu and\ncovariance entity. What is the second moment?",
    "start": "3427900",
    "end": "3433484"
  },
  {
    "text": " Maybe let's have a\ndifferent letter for it so that we don't call it x.",
    "start": "3433485",
    "end": "3438520"
  },
  {
    "text": "Let's call it z. ",
    "start": "3438520",
    "end": "3448210"
  },
  {
    "text": "So how do you compute this? So there are several ways. One way is that\nyou just literally",
    "start": "3448210",
    "end": "3453760"
  },
  {
    "text": "look at each of the\ncoordinates and try to compute expectations. That's perfectly fine. So here I'm going\nto be a little lazy.",
    "start": "3453760",
    "end": "3460130"
  },
  {
    "text": "I'm going to write that\nthis is equal to expectation of z times expectation\nof z transposed",
    "start": "3460130",
    "end": "3466240"
  },
  {
    "text": "plus the covariance of z. Because covariance of z is\nequal to the second moment",
    "start": "3466240",
    "end": "3473600"
  },
  {
    "text": "minus the ultra\nproduct of the min. And the min is mu.",
    "start": "3473600",
    "end": "3480410"
  },
  {
    "text": "So the mu-mu transposed, and\ncovariance is an identity. So that's where we got mu-mu\ntransposed plus identity.",
    "start": "3480410",
    "end": "3488150"
  },
  {
    "text": "And then for the second-- so basically, you get\na half times mu-mu transposed plus identity.",
    "start": "3488150",
    "end": "3494690"
  },
  {
    "text": "And then for the\nsecond Gaussian, actually, the\nmoment is the same, just because mu and minus mu\nis the same if you square it.",
    "start": "3494690",
    "end": "3504770"
  },
  {
    "text": "So you get a half\ntimes mu-mu transposed. So eventually, you get mu-mu\ntransposed plus identity.",
    "start": "3504770",
    "end": "3511440"
  },
  {
    "text": "OK? ",
    "start": "3511440",
    "end": "3516950"
  },
  {
    "text": "So now it looks good, because\nat least mu seems to come-- mu can be, in some sense, read\nout from the moment, right?",
    "start": "3516950",
    "end": "3525980"
  },
  {
    "text": "So if you get the second\nmoment, you subtract i, you can recover mu, right?",
    "start": "3525980",
    "end": "3531320"
  },
  {
    "text": "So basically, what\nyou do is you say-- first, you estimate M-- but you don't necessarily\nknow M2 exactly, right?",
    "start": "3531320",
    "end": "3537440"
  },
  {
    "text": "So you estimate M2 by\nthe empirical samples. ",
    "start": "3537440",
    "end": "3546059"
  },
  {
    "text": "So what's the empirical samples? So you define this\nempirical moment as the empirical second moment.",
    "start": "3546060",
    "end": "3554185"
  },
  {
    "text": " And then you recover\nmu from M2 hat",
    "start": "3554185",
    "end": "3566539"
  },
  {
    "text": "by pretending M2\nis the same as-- M2 hat is the same as M2.",
    "start": "3566540",
    "end": "3573259"
  },
  {
    "text": "So for example, you\ncan recover mu by-- how do you do this? One way to do it is you\ncan subtract i from M2 hat",
    "start": "3573260",
    "end": "3580609"
  },
  {
    "text": "and then try to take\nthe square root of it. And here, I'm going to do one.",
    "start": "3580610",
    "end": "3588440"
  },
  {
    "text": "So basically, the\nkey thing is that-- so how do we recover?",
    "start": "3588440",
    "end": "3595010"
  },
  {
    "text": "So let's do a warm up. So I guess in some sense,\nto recover it from M2 hat,",
    "start": "3595010",
    "end": "3601628"
  },
  {
    "text": "the first thing you\nwant to make sure is that you can\nrecover it from M2. So this is kind of\nlike a premises,",
    "start": "3601628",
    "end": "3610265"
  },
  {
    "text": "can you recover\nmu from M2, right?",
    "start": "3610266",
    "end": "3616590"
  },
  {
    "text": "And we have argued that\nthis is actually true because you can just\nsubtract i from M2 and then take the square root.",
    "start": "3616590",
    "end": "3623190"
  },
  {
    "text": "There's another way\nto do it, which is-- so another way, which\nis the spectral method.",
    "start": "3623190",
    "end": "3632030"
  },
  {
    "text": "I'm going to introduce\nthis here because it's",
    "start": "3632030",
    "end": "3637080"
  },
  {
    "text": "going to be useful\nfor the future cases. So how do we recover mu from\nmu-mu transposed plus identity?",
    "start": "3637080",
    "end": "3645720"
  },
  {
    "text": "What you do is you take\nthe top eigenvector of M2",
    "start": "3645720",
    "end": "3655650"
  },
  {
    "text": "is, actually, equal to\nmu over the norm of mu.",
    "start": "3655650",
    "end": "3661450"
  },
  {
    "text": "Let's got this mu bar. So the top eigenvector\nof M2 is actually exactly in the direction of mu bar.",
    "start": "3661450",
    "end": "3669150"
  },
  {
    "text": " And this is something-- and\nalso the eigenvalue is--",
    "start": "3669150",
    "end": "3678360"
  },
  {
    "text": "the top eigenvalue is mu 2\nnorm squared plus identity.",
    "start": "3678360",
    "end": "3684690"
  },
  {
    "text": "And this is something you\ncan verify relatively easily. So because eigenvector\nof mu-mu transpose",
    "start": "3684690",
    "end": "3693510"
  },
  {
    "text": "is mu bar, and then\neigenvector of mu-mu transpose",
    "start": "3693510",
    "end": "3701895"
  },
  {
    "text": "plus identity is the same. ",
    "start": "3701895",
    "end": "3706900"
  },
  {
    "text": "This is just because if you\nadd identity to any matrix, you don't change\nthe eigensystem.",
    "start": "3706900",
    "end": "3714100"
  },
  {
    "text": "You don't change\nthe eigenvectors. You only change the eigenvalue.",
    "start": "3714100",
    "end": "3719290"
  },
  {
    "text": "The eigenvalue got\nincrement by one.",
    "start": "3719290",
    "end": "3725510"
  },
  {
    "start": "3725510",
    "end": "3731150"
  },
  {
    "text": "That's what happens when you\nadd identity to any matrix. ",
    "start": "3731150",
    "end": "3738950"
  },
  {
    "text": "So you can see that from\nM2, you can recover mu, either using a simple\nsubtraction and square root,",
    "start": "3738950",
    "end": "3745970"
  },
  {
    "text": "or you can do this\neigendecomposition. And this is the case--\nactually, this, actually,",
    "start": "3745970",
    "end": "3753290"
  },
  {
    "text": "also corresponds to\nthe infinite data case. Because when you\nhave infinite data,",
    "start": "3753290",
    "end": "3758330"
  },
  {
    "text": "you can literally compute M2. Because the average\nwill be exactly equal to the population.",
    "start": "3758330",
    "end": "3764130"
  },
  {
    "text": "So now, the question\nbecomes, what if you don't have infinite data? You don't have M2,\nyou only have M2 hat.",
    "start": "3764130",
    "end": "3769790"
  },
  {
    "text": "So basically, you need-- recover from M2 hat, basically,\nusing the same algorithm--",
    "start": "3769790",
    "end": "3780070"
  },
  {
    "text": "using the same\nalgorithm, on M2 hat.",
    "start": "3780070",
    "end": "3785820"
  },
  {
    "text": "So basically you just use\nthe same eigendecomposition on M2 hat, and you\nneed this algorithm",
    "start": "3785820",
    "end": "3795570"
  },
  {
    "text": "to be robust to errors. ",
    "start": "3795570",
    "end": "3803440"
  },
  {
    "text": "Robust to errors in\nthe sense that if you have two matrices,\nM2 and M2 hat,",
    "start": "3803440",
    "end": "3808810"
  },
  {
    "text": "that are similar, then\napplying this algorithm will give you similar answers.",
    "start": "3808810",
    "end": "3814820"
  },
  {
    "text": "So if that's the case, then\nyou get similar answers as if you computed on M2. So you got to an approximate\nestimate for mu, right?",
    "start": "3814820",
    "end": "3825580"
  },
  {
    "text": "And it turns out that\nthis robustness thing is often OK, at least\nin a qualitative sense.",
    "start": "3825580",
    "end": "3834357"
  },
  {
    "text": "For most of the algorithms\nwe're going to discuss, they are robust to some errors.",
    "start": "3834357",
    "end": "3839415"
  },
  {
    "text": " So actually, the most\nimportant thing would be this.",
    "start": "3839415",
    "end": "3845910"
  },
  {
    "text": "So we're going to focus mostly\non the infinite data case. So we're going to focus\non infinite data case",
    "start": "3845910",
    "end": "3860599"
  },
  {
    "text": "because most of the\nalgorithms is robust to ours. So the algorithm analysis\npart is important",
    "start": "3860600",
    "end": "3867170"
  },
  {
    "text": "if you really publish the\npaper, but for the core idea, you don't have to really\ndo the algorithm analysis,",
    "start": "3867170",
    "end": "3872810"
  },
  {
    "text": "because most of the algorithms\nare reasonably robust. ",
    "start": "3872810",
    "end": "3894290"
  },
  {
    "text": "Any questions so far?  So basically we have\ncompleted our discussion",
    "start": "3894290",
    "end": "3901540"
  },
  {
    "text": "about this mixture\nof two Gaussians. And now let's deal with a\nmixture of three Gaussians.",
    "start": "3901540",
    "end": "3907760"
  },
  {
    "text": "And you can see that the point\nwill be that you cannot just only use the first\nmoment and second moment.",
    "start": "3907760",
    "end": "3913600"
  },
  {
    "text": "You have to actually\ngo to the third moment, and it will make things a\nlittle bit more complicated.",
    "start": "3913600",
    "end": "3921380"
  },
  {
    "text": "So maybe the general\napproach is to-- ",
    "start": "3921380",
    "end": "3927960"
  },
  {
    "text": "is that you compute M1, which\nis the expectation of x, M2,",
    "start": "3927960",
    "end": "3936020"
  },
  {
    "text": "which is the expectation\nof xx prime, and M3. What is M3? What's the third moment?",
    "start": "3936020",
    "end": "3941660"
  },
  {
    "text": " M3 is the expectation\nof x tensor x tensor x.",
    "start": "3941660",
    "end": "3949307"
  },
  {
    "text": "If you are not familiar\nwith this notation, so x tensor x tensor x, this\nis the third-level tensor",
    "start": "3949308",
    "end": "3956340"
  },
  {
    "text": "of dimension d by d by d. And so let's say\nthis is called T.",
    "start": "3956340",
    "end": "3962609"
  },
  {
    "text": "So then T is the\nthird-level tensor, and the ijk entry\nof this tensor is",
    "start": "3962610",
    "end": "3968730"
  },
  {
    "text": "equal to xi times xj times xk. ",
    "start": "3968730",
    "end": "3975220"
  },
  {
    "text": "So in some sense,\nif you do the-- so x tensor x is basically just\na rewriting of xx transposed,",
    "start": "3975220",
    "end": "3984450"
  },
  {
    "text": "and x tensor x tensor\nx is defined like this. ",
    "start": "3984450",
    "end": "3993250"
  },
  {
    "text": "And you can also have\na tensor, b tensor c. So suppose T prime is equal\nto a tensor b tensor c.",
    "start": "3993250",
    "end": "4002160"
  },
  {
    "text": "Then T prime ijk is equal to-- definition would be\nai times bj times ck.",
    "start": "4002160",
    "end": "4009150"
  },
  {
    "text": " So that's why in this\nsense, if you look at M3,",
    "start": "4009150",
    "end": "4018230"
  },
  {
    "text": "ijk entry then this is the\nentry of the ijk entry, which",
    "start": "4018230",
    "end": "4028359"
  },
  {
    "text": "is expectation of xi\ntimes xj times xk.",
    "start": "4028360",
    "end": "4034760"
  },
  {
    "text": "So basically every entry of\nthis third-order tensor M3 is the expectation of the\nproduct of three coordinates",
    "start": "4034760",
    "end": "4044540"
  },
  {
    "text": "of the data.  And you can do this\neven for M4, or--",
    "start": "4044540",
    "end": "4051260"
  },
  {
    "text": "M4 and M5 so on and so forth. And then you design an\nalgorithm and let's just",
    "start": "4051260",
    "end": "4062630"
  },
  {
    "text": "call it A, script A, that\ntakes in the moment and outputs",
    "start": "4062630",
    "end": "4081210"
  },
  {
    "text": "theta. So you want to recover from\nthe moment the parameter theta. And then if you can do\nthis, then the last step",
    "start": "4081210",
    "end": "4089040"
  },
  {
    "text": "will be that you have to\nshow A is robust to errors,",
    "start": "4089040",
    "end": "4098240"
  },
  {
    "text": "and then apply A to\nthe empirical moment. ",
    "start": "4098240",
    "end": "4109068"
  },
  {
    "text": "And how many-- what is\nthe order of the moment? So this is in reality.",
    "start": "4109069",
    "end": "4114080"
  },
  {
    "text": "So this is the final algorithm. So apply A to the\nempirical moment, that's the final algorithm.",
    "start": "4114080",
    "end": "4119420"
  },
  {
    "text": "All the previous\nsteps are the process of designing the algorithm. ",
    "start": "4119420",
    "end": "4127568"
  },
  {
    "text": "So basically, what is the order\nof the moment you have to use, right? So do you need third-order\nmoment, fourth-order moment?",
    "start": "4127569",
    "end": "4134299"
  },
  {
    "text": "That depends on from\nhow many moments you can recover the parameter theta.",
    "start": "4134300",
    "end": "4139937"
  },
  {
    "text": "If, from the first\nmoment, second moment you can recover, then sure,\ntwo moments are fine. If you need three or\nmore moments to recover,",
    "start": "4139937",
    "end": "4147259"
  },
  {
    "text": "then you need M3. Otherwise, you\nprobably even need M4. In fact, in some cases\nindeed we need M4.",
    "start": "4147260",
    "end": "4153318"
  },
  {
    "text": "I guess in-- yeah, I think\neven in a case that we're going to discuss, we need M4\nfor the first tensor--",
    "start": "4153319",
    "end": "4159979"
  },
  {
    "text": "the first tensor. Any questions? ",
    "start": "4159979",
    "end": "4170002"
  },
  {
    "text": "OK. So I think I have only-- less than-- about--",
    "start": "4170002",
    "end": "4175939"
  },
  {
    "text": "oh, I have about 15 minutes. So I'm going to show that--",
    "start": "4175939",
    "end": "4185949"
  },
  {
    "text": "let's talk about mixture\nof high Gaussians, and I'm going to show\nyou that you actually",
    "start": "4185950",
    "end": "4191350"
  },
  {
    "text": "need at least the third moment\nwhen the number of components",
    "start": "4191350",
    "end": "4199330"
  },
  {
    "text": "is not just two. And this is very typical. In most of the cases, you\nneed at least a third moment.",
    "start": "4199330",
    "end": "4205570"
  },
  {
    "text": "Actually, it's not very\neasy to find a case where second moment suffices. I have to think about which\ncase second moment suffices",
    "start": "4205570",
    "end": "4213730"
  },
  {
    "text": "for when I found this\ntwo-component mix of Gaussians. In almost all other cases\nthen you need a third moment.",
    "start": "4213730",
    "end": "4223370"
  },
  {
    "text": "So let's assume-- again,\nlet's make it simpler. So let's assume that this\nis a mixture of Gaussians",
    "start": "4223370",
    "end": "4231199"
  },
  {
    "text": "with a uniform mixture. So all the components have\n1/k probability to show up.",
    "start": "4231200",
    "end": "4237469"
  },
  {
    "text": "So basically, you would\nsample i uniformly from k",
    "start": "4237470",
    "end": "4242670"
  },
  {
    "text": "and then you generate\nx from Gaussian with min mu i and\ncovariance identity.",
    "start": "4242670",
    "end": "4250440"
  },
  {
    "text": "This is a generative\nmodel for our data. And alternatively,\nyou can probably write x is sampled from\nthis, the average of this k",
    "start": "4250440",
    "end": "4263210"
  },
  {
    "text": "distributions. And in all the follow ups, we\nare going to only do a and b.",
    "start": "4263210",
    "end": "4271900"
  },
  {
    "text": "So we only do a\nand b, the step a and step b for all\nexamples in the SQL,",
    "start": "4271900",
    "end": "4282520"
  },
  {
    "text": "even including examples\nin the next lecture. The robustness, you\ncan do that, but it requires too much mathematical\njargon, which is not",
    "start": "4282520",
    "end": "4289389"
  },
  {
    "text": "really needed for this course. And A and B is really the gist--\nis really the core thing that",
    "start": "4289390",
    "end": "4296080"
  },
  {
    "text": "enables this. ",
    "start": "4296080",
    "end": "4302010"
  },
  {
    "text": "So now let's try to\ncompute the moment and see which moment is\nenough for us to recover.",
    "start": "4302010",
    "end": "4307460"
  },
  {
    "text": "Again, let's compute\nthe first moment. So this is-- we have\nk possible cases.",
    "start": "4307460",
    "end": "4316614"
  },
  {
    "text": "So each case arises with x\nprobability k-- probability 1/k.",
    "start": "4316615",
    "end": "4322909"
  },
  {
    "text": "So each cluster shows\nup with probability 1/k. So condition on the\ncluster i, your min",
    "start": "4322910",
    "end": "4329350"
  },
  {
    "text": "is mu i into some of them-- this is 1/k times\nthe sum of mu y.",
    "start": "4329350",
    "end": "4338800"
  },
  {
    "text": "So clearly from the first-order\nmoment, the first moment, you only know the\naverage of the min. You probably wouldn't be able\nto recover each of the mins.",
    "start": "4338800",
    "end": "4346140"
  },
  {
    "text": "That sounds reasonable. And now let's look\nat a second moment.",
    "start": "4346140",
    "end": "4355320"
  },
  {
    "text": "The second moment is-- ",
    "start": "4355320",
    "end": "4363560"
  },
  {
    "text": "I guess we still do this\ntotal law of expectation. Your condition on the hidden\nvariable i, the latent variable",
    "start": "4363560",
    "end": "4370670"
  },
  {
    "text": "i, and you can set the\nmoment for that Gaussian. And we have shown that for\nevery Gaussian, the moment is--",
    "start": "4370670",
    "end": "4379850"
  },
  {
    "text": "second moment is mu i, mu\ni transposed plus identity into the sum of i to k, right?",
    "start": "4379850",
    "end": "4386245"
  },
  {
    "text": " And then this is\n1/k times sum of--",
    "start": "4386245",
    "end": "4394000"
  },
  {
    "text": "basically, this is the average\nof the outer product of mu i mu i transposed plus identity.",
    "start": "4394000",
    "end": "4401870"
  },
  {
    "text": "So the question becomes-- suppose you just want\nto use the first moment and the second moment.",
    "start": "4401870",
    "end": "4408300"
  },
  {
    "text": "The question becomes, can\nwe recover from M1 and M2?",
    "start": "4408300",
    "end": "4416520"
  },
  {
    "text": " Or maybe more specifically,\nfrom the average of mu i",
    "start": "4416520",
    "end": "4428050"
  },
  {
    "text": "and the average of\nmu i mu i transposed. ",
    "start": "4428050",
    "end": "4437160"
  },
  {
    "text": "And the claim is that this is\nnot possible, at least when",
    "start": "4437160",
    "end": "4444925"
  },
  {
    "text": "k is larger than 3. So there are two arguments.",
    "start": "4444925",
    "end": "4453100"
  },
  {
    "text": "I guess there is one argument. The argument is the following. The reason why this\nis not possible",
    "start": "4453100",
    "end": "4458440"
  },
  {
    "text": "is that these are just not\nenough information for you to recover, in some sense. So there's-- so you're\nstill missing some kind",
    "start": "4458440",
    "end": "4466420"
  },
  {
    "text": "of rotations-- missing rotation and\nlikely information.",
    "start": "4466420",
    "end": "4474889"
  },
  {
    "text": "What does that really mean? Let me specify. So suppose you-- just\nto make the discussion",
    "start": "4474890",
    "end": "4482940"
  },
  {
    "text": "easier, let's define u to\nbe this collection of mins,",
    "start": "4482940",
    "end": "4488380"
  },
  {
    "text": "mu1 up to mu k, which\nis in dimension d by k.",
    "start": "4488380",
    "end": "4493875"
  },
  {
    "text": "So this is the matrix\nyou want to recover.  And I'm claiming\nthat there are going",
    "start": "4493875",
    "end": "4501490"
  },
  {
    "text": "to exist two sets of mus\nthat have the same average--",
    "start": "4501490",
    "end": "4509280"
  },
  {
    "text": "the same qualities here,\nthese two quantities. Both of M1 and M2\nare the same, even though the mus are different.",
    "start": "4509280",
    "end": "4515400"
  },
  {
    "text": "I'm going to construct\nsuch a situation. How do I do that? I'm going to take a rotation\nmatrix R, in dimension k by k.",
    "start": "4515400",
    "end": "4531400"
  },
  {
    "text": "I'm going to rotate-- so basically, I'm going to\nconsider u versus u times R.",
    "start": "4531400",
    "end": "4539500"
  },
  {
    "text": "If you rotate on\nthe right-hand side, you got a different\ntype of set of means.",
    "start": "4539500",
    "end": "4544639"
  },
  {
    "text": "So I'm going to claim\nthat u and u times R have the same statistics,\nhave these same two quantities.",
    "start": "4544640",
    "end": "4551020"
  },
  {
    "text": " So first thing is\nthat if you look at the average of the outer\nproduct, mu i mu i transposed--",
    "start": "4551020",
    "end": "4558750"
  },
  {
    "text": " sorry, it's 1/k, then\nthis 1/k times uu",
    "start": "4558750",
    "end": "4566240"
  },
  {
    "text": "transposed in our\nsimplified notation. And this is equal to 1/k\ntimes uR times uR transposed.",
    "start": "4566240",
    "end": "4575655"
  },
  {
    "text": "This is just because RR\ntransposed is equal to entity. That's the definition\nof rotation. ",
    "start": "4575655",
    "end": "4583950"
  },
  {
    "text": "So that means that u and\nUR not distinguishable from this quantity, from\nthe average of the product",
    "start": "4583950",
    "end": "4589869"
  },
  {
    "text": "of the mu i mu i transposed. So now let's look at\nthe first-order moment.",
    "start": "4589870",
    "end": "4598610"
  },
  {
    "text": "So to make the first-order\nmoment also not distinguishable, I also\nhave to take another-- take in addition R, such that\nR times L1 vector is still",
    "start": "4598610",
    "end": "4613639"
  },
  {
    "text": "equal to L1 vector. So you want a rotation\nsuch that you don't-- you want a rotation,\nbut you don't",
    "start": "4613640",
    "end": "4619340"
  },
  {
    "text": "want to rotate the\ndirection of L1 vector. That's easy. You have so many rotations.",
    "start": "4619340",
    "end": "4624800"
  },
  {
    "text": "You can-- you just want\nto say, I am going-- it's like if you\nhave a globe, you have one direction,\nwhich you don't change.",
    "start": "4624800",
    "end": "4631369"
  },
  {
    "text": "But you can rotate still in\nother dimensions directions. There are still k\nminus 2 directions,",
    "start": "4631370",
    "end": "4636440"
  },
  {
    "text": "because the dimension is k here. So you still have a lot\nof degrees of freedom to choose many different\nRs that satisfies this,",
    "start": "4636440",
    "end": "4642440"
  },
  {
    "text": "as long as k is somewhat\nbig, like larger than 3. And then suppose\nyou satisfy this.",
    "start": "4642440",
    "end": "4648300"
  },
  {
    "text": "Then uR times L1 vector. Maybe let's write this.",
    "start": "4648300",
    "end": "4653679"
  },
  {
    "text": "So 1/k times sum of mu i.",
    "start": "4653680",
    "end": "4659590"
  },
  {
    "text": "This is one 1/k times\nmu times L1 vector.",
    "start": "4659590",
    "end": "4666250"
  },
  {
    "text": "I'm claiming that this is equal\nto 1/k times u times R times L1 vector, just because I\ndesigned R like this.",
    "start": "4666250",
    "end": "4675430"
  },
  {
    "text": "So that's why-- from this\naverage column statistic,",
    "start": "4675430",
    "end": "4682760"
  },
  {
    "text": "or from this quantity, you still\ndon't distinguish u and uR.",
    "start": "4682760",
    "end": "4688489"
  },
  {
    "text": "So u and uR are\nnot distinguishable",
    "start": "4688490",
    "end": "4701660"
  },
  {
    "text": "because they exactly match the\nfirst moment and second moment. So that's why we need to\ngo to M3, to distinguish--",
    "start": "4701660",
    "end": "4713900"
  },
  {
    "text": "to uniquely identify\nthe columns of u. OK.",
    "start": "4713900",
    "end": "4720310"
  },
  {
    "text": "I think we are\nfive minutes early, but I think the next\nthing would be--",
    "start": "4720310",
    "end": "4725927"
  },
  {
    "text": "probably takes much\nmore than five minutes, so I guess I would\njust stop here to see whether\nthere's any questions.",
    "start": "4725928",
    "end": "4732190"
  },
  {
    "text": "And tomorrow-- next\nlecture, we will continue with solving\nthis question with M3.",
    "start": "4732190",
    "end": "4738789"
  },
  {
    "text": " Any questions? Is there a [INAUDIBLE]?",
    "start": "4738790",
    "end": "4745560"
  },
  {
    "text": " Yeah. So the question is, how do you\ninfer the number of Gaussians?",
    "start": "4745560",
    "end": "4753300"
  },
  {
    "text": "So first of all, indeed you are\nright that in the formulation right now, I am\nassuming I know exactly",
    "start": "4753300",
    "end": "4759500"
  },
  {
    "text": "the number of Gaussians. I'm even assuming that I\nknow all the probabilities",
    "start": "4759500",
    "end": "4764840"
  },
  {
    "text": "for each Gaussian, right? p1 up to pk are\nexactly just 1/k. And the question is, how do you\nkind of infer enough Gaussians.",
    "start": "4764840",
    "end": "4771860"
  },
  {
    "text": "Maybe also another question is,\nhow do we infer the p1 to pk? So there are ways.",
    "start": "4771860",
    "end": "4777695"
  },
  {
    "text": " Of course, there\nare ways, depending",
    "start": "4777695",
    "end": "4782708"
  },
  {
    "text": "on-- there are\nvarious ways depending on what assumptions you make. But definitely it's possible.",
    "start": "4782708",
    "end": "4788179"
  },
  {
    "text": "For example, one somewhat-- ",
    "start": "4788180",
    "end": "4793949"
  },
  {
    "text": "one way that would\nwork in certain cases is that you can infer\nthe number of Gaussians by looking at the\nrank of this matrix.",
    "start": "4793950",
    "end": "4805080"
  },
  {
    "text": "Suppose you believe that all the\nmu i's are not like degenerate.",
    "start": "4805080",
    "end": "4810240"
  },
  {
    "text": "These are not-- they're\nall in general positions. So then the rank of this matrix\nwill be k, especially when",
    "start": "4810240",
    "end": "4818250"
  },
  {
    "text": "k is less than b, right? So then you can infer\nnumber of Gaussians, k,",
    "start": "4818250",
    "end": "4823469"
  },
  {
    "text": "by looking at the\nrank of this matrix. But I'm not saying that that's\nactually really a great method",
    "start": "4823470",
    "end": "4829890"
  },
  {
    "text": "because empirically, we\ngot into other issues because maybe your condition\nis not exactly matched and so forth.",
    "start": "4829890",
    "end": "4835810"
  },
  {
    "text": "So there are many other ways. And empirically,\nthe most typical way",
    "start": "4835810",
    "end": "4842112"
  },
  {
    "text": "to estimate the\nnumber of Gaussians is using\nnon-parametric-based methods, which I guess is not\nsomething we will cover here.",
    "start": "4842112",
    "end": "4849809"
  },
  {
    "text": "So for the theoretical\nsetup, we are mostly interested in\na clean setting where you know everything, and\nit's still an open question",
    "start": "4849810",
    "end": "4857100"
  },
  {
    "text": "to recover the mu i's,\neven with a knowledge of the number of Gaussians.",
    "start": "4857100",
    "end": "4862122"
  },
  {
    "start": "4862122",
    "end": "4870384"
  },
  {
    "text": "[INAUDIBLE] ",
    "start": "4870384",
    "end": "4878050"
  },
  {
    "text": "Right. So as long as that\nhappens, it wouldn't work. So that's why it's\nprobably not a great idea. But actually,\ntypically, if you really",
    "start": "4878050",
    "end": "4884790"
  },
  {
    "text": "got high dimensional\ndata, the mu i's typically they are independent.",
    "start": "4884790",
    "end": "4892340"
  },
  {
    "text": "But they could have\nsome kind of-- one of them could live\nin approximately the subspace of the others.",
    "start": "4892340",
    "end": "4898385"
  },
  {
    "text": "So then it becomes tricky\nbecause whether you are robust to errors\nso on and so forth. Yes. So I think loosely\nspeaking, it's reasonable.",
    "start": "4898385",
    "end": "4906352"
  },
  {
    "text": "But if you really look at the\ndetails, it's not that great. So that's why you need\nother methods sometimes.",
    "start": "4906352",
    "end": "4911725"
  },
  {
    "start": "4911725",
    "end": "4918030"
  },
  {
    "text": "I guess if there's\nno other questions, I'll see you next Monday.",
    "start": "4918030",
    "end": "4923329"
  },
  {
    "start": "4923330",
    "end": "4928000"
  }
]