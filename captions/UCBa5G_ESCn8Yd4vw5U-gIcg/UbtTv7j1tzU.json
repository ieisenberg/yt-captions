[
  {
    "text": "So I guess the\nlast time, Masha talked about deep\nlearning, introduced",
    "start": "5080",
    "end": "12190"
  },
  {
    "text": "deep learning neural networks. Today, we're going to talk\nabout back propagation, which is probably the most important\nthing in deep learning,",
    "start": "12190",
    "end": "18560"
  },
  {
    "text": "like how do you\ncomplete a gradient and implement this algorithm? Of course, there\nare many other kind of decisions you have to\nmake in deep learning.",
    "start": "18560",
    "end": "26650"
  },
  {
    "text": "And also, there are\nthis back propagation. This computing gradient becomes\nstandardized these days.",
    "start": "26650",
    "end": "34600"
  },
  {
    "text": "You don't necessarily\nhave to implement your own gradient\ncomputation process, where you can just order--",
    "start": "34600",
    "end": "40800"
  },
  {
    "text": "The gradient is done by the\nso-called auto differentiation algorithm.",
    "start": "40800",
    "end": "46489"
  },
  {
    "text": "But this is actually the\nonly algorithmic part in deep learning. So that's why we still teach it. And also, in some sense,\nit's still important",
    "start": "46489",
    "end": "52780"
  },
  {
    "text": "because this idea of\ncomputing gradient automatically actually has many\nimplications in other areas.",
    "start": "52780",
    "end": "61380"
  },
  {
    "text": "For example, suppose\nyou want to study the so-called meta-learning. I'm not sure whether you\nheard of this word, which",
    "start": "61380",
    "end": "67780"
  },
  {
    "text": "is basically built\nupon this idea that you can do all the\ndifferentiation for almost any computation you want.",
    "start": "67780",
    "end": "74460"
  },
  {
    "text": "And these same ideas also\nshows up in other cases. And also, I know that some\nof the previous courses,",
    "start": "74460",
    "end": "80670"
  },
  {
    "text": "where it also covered\nthe back propagation. I think CS221 does cover back\npropagation in some sense.",
    "start": "80670",
    "end": "87900"
  },
  {
    "text": "So what I'm going to do\nis that, so in the past, in the last four years\nwhen I teach this,",
    "start": "87900",
    "end": "94570"
  },
  {
    "text": "I have a lot of derivations\nwith all of these indices. And you computed the\ngradient of-- you",
    "start": "94570",
    "end": "99578"
  },
  {
    "text": "do the chain rule in\na very detailed way. And this year, I'm going to try\na slightly different approach,",
    "start": "99579",
    "end": "105439"
  },
  {
    "text": "where I'm going to make\nthis a little bit more packaged in some sense,\ndivide it into submodules.",
    "start": "105439",
    "end": "110850"
  },
  {
    "text": "And in some sense, it's\na little more abstract. In some other\nsenses, it's cleaner",
    "start": "110850",
    "end": "118770"
  },
  {
    "text": "because you don't have to deal\nwith all the small indices, like all the indices. Sometimes, there are five\nindices you have to keep track.",
    "start": "118770",
    "end": "124460"
  },
  {
    "text": "And now everything\nis in vector form. And this is not entirely new.",
    "start": "124460",
    "end": "129670"
  },
  {
    "text": "It's not like I'm just\ndoing an experiment because, in the last time, I\ndid both of the two versions. I have the very\nfine-grain derivations.",
    "start": "129670",
    "end": "136410"
  },
  {
    "text": "And I also have the\nvectorized form. And I think I got\nsome feedback from you",
    "start": "136410",
    "end": "141519"
  },
  {
    "text": "that the fine-grained\nderivation is not that useful because either\nyou feel like it's messy,",
    "start": "141520",
    "end": "147950"
  },
  {
    "text": "and I'm doing all of this\ncomputation on the board, and you know how to do\nit, and it's very messy or, if you haven't seen it\nbefore, it's hard to follow.",
    "start": "147950",
    "end": "156269"
  },
  {
    "text": "So if you haven't\nseen all of them, if you haven't seen\nback propagation I taught even for a\nvery, very simple case",
    "start": "156269",
    "end": "164319"
  },
  {
    "text": "and you want to do the\nvery low-level stuff, probably take a look at\nlecture notes afterwards.",
    "start": "164319",
    "end": "170409"
  },
  {
    "text": "It's independent. What I teach here doesn't\ndepend on anything. It doesn't require\nany background.",
    "start": "170409",
    "end": "176690"
  },
  {
    "text": "But it's more on a little more\nabstract level to some extent. I would say it's more\non the vectorized level.",
    "start": "176690",
    "end": "183800"
  },
  {
    "text": "So everything is\ntreated as vectors. So if you want to do the\nmore low-level details, you can take a look at\nthe lecture notes, which",
    "start": "183800",
    "end": "190849"
  },
  {
    "text": "I think that those part is\na little easier and mostly covered by some of the\nother courses as well.",
    "start": "190850",
    "end": "196090"
  },
  {
    "text": "So that's why I'm\nmaking this decision. Don't worry if you\ndon't see any-- if this, my comments,\ndoesn't make",
    "start": "196090",
    "end": "201930"
  },
  {
    "text": "sense here, just if you haven't\nseen any back propagation. So I hope, at least,\nthis is still--",
    "start": "201930",
    "end": "208850"
  },
  {
    "text": "the materials I'm\ngoing to cover today still is completely\nfine if you don't have-- if you\nhaven't seen anything",
    "start": "208850",
    "end": "214300"
  },
  {
    "text": "about back propagation.",
    "start": "214300",
    "end": "219700"
  },
  {
    "text": "So let's get into\nthe more details. So I guess, so basically, the\nso-called back propagation,",
    "start": "219700",
    "end": "225319"
  },
  {
    "text": "this is just a word\nthat describes-- it's a terminology in\nsome sense, back prop,",
    "start": "225319",
    "end": "233879"
  },
  {
    "text": "that is a technique for us to\ncompute a gradient of the loss",
    "start": "233879",
    "end": "240340"
  },
  {
    "text": "function for neural network. So recall that last time, I\nthink we talked about this.",
    "start": "240340",
    "end": "246670"
  },
  {
    "text": "You have a loss function. For example, you have a\nloss function, J of theta, which is-- maybe let's\njust look at one example, J",
    "start": "246670",
    "end": "253069"
  },
  {
    "text": "subscript little j. This is the loss\nfunction of Jth example.",
    "start": "253069",
    "end": "261000"
  },
  {
    "text": "And this is something like\nyj minus h theta xj squared.",
    "start": "261000",
    "end": "267000"
  },
  {
    "text": "So this loss function can\nbe other loss function. Here, I'm just only using the\nsquare loss for simplicity.",
    "start": "267000",
    "end": "273710"
  },
  {
    "text": "And this is the neural network. And recall that we talk about\nSGD, stochastic gradient",
    "start": "273710",
    "end": "282180"
  },
  {
    "text": "descent. And when you deal\nwith neural networks, you always have to\ncompute this gradient. So the algorithm is\nsomething like this,",
    "start": "282180",
    "end": "291000"
  },
  {
    "text": "minus alpha times J.\nThe gradient of the loss function on some\nparticular example, jk,",
    "start": "291000",
    "end": "297560"
  },
  {
    "text": "may be a random example\nat the parameter theta.",
    "start": "297560",
    "end": "302660"
  },
  {
    "text": "So basically, today,\nwhat we are going to do is that-- so\nbasically, today, how to compute this, the gradient.",
    "start": "302660",
    "end": "315160"
  },
  {
    "text": "So this is the main thing today. And once you know how to\ncompute this gradient, you can implement the algorithm.",
    "start": "315160",
    "end": "321400"
  },
  {
    "text": "And there is a generic way\nto compute the gradient. So what I'm going\nto do is then I'm going to start with a\nvery generic theorem.",
    "start": "321400",
    "end": "328620"
  },
  {
    "text": "And the theorem, I'm\nnot going to prove it. You don't have to\nknow how to prove it.",
    "start": "328620",
    "end": "334460"
  },
  {
    "text": "But I think it's good to know\nthe existence of the theorem. So the theorem is saying that--",
    "start": "334460",
    "end": "339960"
  },
  {
    "text": "I'm going to write\ndown the statement . But the theorem is\nbasically saying that if this loss function, J--",
    "start": "339960",
    "end": "345100"
  },
  {
    "text": "by the way, I'm going\nto focus on only one particular example today\nbecause, if you know how to compute a gradient\nfor one example, then",
    "start": "345100",
    "end": "351840"
  },
  {
    "text": "you know how to compute a\ngradient for all the examples. They're all the same. So the point-- so on the theorem\nI'm going to write down today",
    "start": "351840",
    "end": "357990"
  },
  {
    "text": "is that if you know how\nto compute the loss itself in an efficient way, then\nin almost all situations,",
    "start": "357990",
    "end": "365120"
  },
  {
    "text": "you know how to compute\nthe gradient of the loss. And in almost same\namount of time. It's very-- in some sense, if\nyou heard of it the first time,",
    "start": "365120",
    "end": "373870"
  },
  {
    "text": "it's a little bit striking. So let me write\ndown the theorem. So I need to first specify some\nof the minor details, which",
    "start": "373870",
    "end": "382090"
  },
  {
    "text": "don't matter that much. So anyway, this\ntheorem is going to be stated somewhat informally.",
    "start": "382091",
    "end": "388770"
  },
  {
    "text": "So let me define\nthis so-called notion of differentiable circuits\nor differentiable networks.",
    "start": "388770",
    "end": "408030"
  },
  {
    "text": "And let's say this\ndifferentiable circuits or differentiable networks\nare compositions of sequence",
    "start": "408030",
    "end": "418910"
  },
  {
    "text": "of, let's say, arithmetic.",
    "start": "418910",
    "end": "428349"
  },
  {
    "text": "And arithmetic. am I\nspelling it correctly?",
    "start": "428349",
    "end": "436620"
  },
  {
    "text": "Sorry. Operations and\nelementary functions. By the way, my definition here\nis a little bit hand wavey,",
    "start": "436620",
    "end": "444770"
  },
  {
    "text": "as you can see. But the point is you\nwill see the point. The point is not exactly\nabout the details.",
    "start": "444770",
    "end": "449879"
  },
  {
    "text": "It's more about the\nform of the theorem. So by arithmetic operations,\nelementary functions, I mean that, for example,\nthings like maybe addition,",
    "start": "449879",
    "end": "461650"
  },
  {
    "text": "subtraction, product, division. And some of the elementary\nfunctions, you can handle.",
    "start": "461650",
    "end": "469090"
  },
  {
    "text": "Actually, most of the elementary\nfunctions, you can handle. So maybe cosine,\nsine, so and so forth,",
    "start": "469090",
    "end": "476720"
  },
  {
    "text": "exponential log,\nlog, logarithmic, maybe ReLU maybe sigmoid.",
    "start": "476720",
    "end": "482060"
  },
  {
    "text": "There are many, many\nof these functions. And suppose you have--\nthis is my definition",
    "start": "482060",
    "end": "488470"
  },
  {
    "text": "of differentiable circuits or\ndifferentiable neural networks, differentiable networks. And you can see a neural\nnetwork is often one of this",
    "start": "488470",
    "end": "494789"
  },
  {
    "text": "because, when you\nhave a neural network, you have a lot of\nmatrix multiplications. And well, no matter how many\nmatrix multiplication you have,",
    "start": "494790",
    "end": "502199"
  },
  {
    "text": "it's really just some\ncomplex compositions of all of these operations.",
    "start": "502199",
    "end": "508580"
  },
  {
    "text": "So in some sense,\neverything you compute are some combinations\nof these things, no matter if it's by neural\nnetwork or something else.",
    "start": "508580",
    "end": "517050"
  },
  {
    "text": "But I'm going to\ninsist that they are differentiable because\nI'm going to have-- I'm going to differentiate\nthe output of this network.",
    "start": "517050",
    "end": "523959"
  },
  {
    "text": "So here is my theorem. So maybe I should do--",
    "start": "523959",
    "end": "529680"
  },
  {
    "text": "I think a blue color\nis a little better?",
    "start": "529680",
    "end": "535730"
  },
  {
    "text": "Is that right? Yeah, and this is\ninformally stated.",
    "start": "535730",
    "end": "542230"
  },
  {
    "text": "It's not very far from\nthe formal version. I guess just the\nformal version requires some minor details about\nthe differentiablilty,",
    "start": "542230",
    "end": "550600"
  },
  {
    "text": "so on and so forth. So the claim is that suppose you\nhave a differentiable circuit",
    "start": "550600",
    "end": "560910"
  },
  {
    "text": "of, say, size N. N means how\nmany basic operations you are",
    "start": "560910",
    "end": "575170"
  },
  {
    "text": "using. So this is my size. The size means the number\nof basic operations.",
    "start": "575170",
    "end": "581130"
  },
  {
    "text": "And suppose this circuit\ncomputes, computes",
    "start": "581130",
    "end": "586690"
  },
  {
    "text": "of a real value of the function.",
    "start": "586690",
    "end": "591730"
  },
  {
    "text": "So I'm going to stress this. This is a real value of\nfunction f that maps, let's say,",
    "start": "591730",
    "end": "601290"
  },
  {
    "text": "l dimension to one dimension. I'm going to stress\nthat this theorem only",
    "start": "601290",
    "end": "608180"
  },
  {
    "text": "works when you have one output. So the function can\nonly have one output. But they can have\nmultiple inputs.",
    "start": "608180",
    "end": "615890"
  },
  {
    "text": "So then, suppose you\nhave such a circuit. Then the gradient\nof this function",
    "start": "615890",
    "end": "623329"
  },
  {
    "text": "that you are computing,\nthe gradient,",
    "start": "623329",
    "end": "630209"
  },
  {
    "text": "so what is the gradient? The gradient at any\npoint is a vector because you have io out--",
    "start": "630210",
    "end": "635490"
  },
  {
    "text": "inputs. The gradient is l dimensional. So the gradient is l\ndimensional vector.",
    "start": "635490",
    "end": "643680"
  },
  {
    "text": "The gradient at this\nparticular point, let's say x. x is just the abstract point.",
    "start": "643680",
    "end": "649269"
  },
  {
    "text": "And this can be computed",
    "start": "649269",
    "end": "665079"
  },
  {
    "text": "in time of N. I think.",
    "start": "665079",
    "end": "671310"
  },
  {
    "text": "I guess, technically, this is\nO of N plus D. But I guess-- I'm not sure why on my notes.",
    "start": "671310",
    "end": "681579"
  },
  {
    "text": "This is a typo, I think. Oh, I see.",
    "start": "681579",
    "end": "688370"
  },
  {
    "text": "So I guess time of N by\nour circuit of size O of N.",
    "start": "688370",
    "end": "701329"
  },
  {
    "text": "And here, I have an implicit\nassumption because implicitly",
    "start": "701329",
    "end": "706690"
  },
  {
    "text": "assuming N is bigger than d\nbecause if N is not less--",
    "start": "706690",
    "end": "712670"
  },
  {
    "text": "it's less than d, it's\na little bit-- sorry, N is bigger than l, because\nif you have a circuit,",
    "start": "712670",
    "end": "719240"
  },
  {
    "text": "the circuit, if it-- most of the circuits should\nread like all the inputs.",
    "start": "719240",
    "end": "724750"
  },
  {
    "text": "So you probably\nneed at least N-- l time to read all the inputs. So that's why I'm assuming\nN is bigger than l.",
    "start": "724750",
    "end": "730600"
  },
  {
    "text": "If N is not bigger\nthan l, then this has to be slightly changed. But the message doesn't change.",
    "start": "730600",
    "end": "736680"
  },
  {
    "text": "So anyway, what's\nthe main message? The main message is that if\nyou can compute a function f",
    "start": "736680",
    "end": "743089"
  },
  {
    "text": "by a circuit of size\nN, then its gradient can also be computed in\na similar amount of time.",
    "start": "743089",
    "end": "750000"
  },
  {
    "text": "And you just pay a\nconstant vector more. And I think this constant is\nliterally-- in most of cases, this constant is just 1.",
    "start": "750000",
    "end": "756690"
  },
  {
    "text": "Of course, when you really talk\nabout the absolute constant, it has to depend on how-- some of the small details\nlike, how do you really",
    "start": "756690",
    "end": "762250"
  },
  {
    "text": "implement this in the computer? So that's why I'm\nhiding the concept. But in some sense, you can\nview these concepts, even 1.",
    "start": "762250",
    "end": "773100"
  },
  {
    "text": "So I guess-- sorry, I\nthink the-- one of are the concepts I think, also\ndepends on how you count,",
    "start": "773100",
    "end": "779899"
  },
  {
    "text": "right? So whether you\nassume you know you are to compute a function f. So maybe, so\nbasically, the constant",
    "start": "779899",
    "end": "787250"
  },
  {
    "text": "would be 2 if you\nhave to compute f and the gradient of f.",
    "start": "787250",
    "end": "793240"
  },
  {
    "text": "So basically, there\nis a-- so if you know the function f already,\nthen this constant would be 1. If you don't know\nthe function f,",
    "start": "793240",
    "end": "798990"
  },
  {
    "text": "then you have to first evaluate\nf and then do the gradient. And then, this\nconstant will be 2. I'll discuss this a\nlittle bit later as well.",
    "start": "798990",
    "end": "806060"
  },
  {
    "text": "But anyway, for the moment,\nyou can just think of this-- you almost have the\nsame amount of time. You can only use the same amount\nof time to compute a gradient.",
    "start": "806060",
    "end": "815000"
  },
  {
    "text": "So gradient is never much\ndifficult, more difficult, than computing loss itself.",
    "start": "815000",
    "end": "820930"
  },
  {
    "text": "And this is very general\nbecause it doesn't have to be a neural network. It could be almost\nanything like this.",
    "start": "820930",
    "end": "827920"
  },
  {
    "text": "And if you want to\ninstantiate this theorem",
    "start": "827920",
    "end": "836579"
  },
  {
    "text": "for neural networks of\nthe laws of networks, then f in this theorem\nwould corresponds",
    "start": "836579",
    "end": "842019"
  },
  {
    "text": "to the loss function on a\nparticular example, J, Jth. example. And theta is the\nvariable x here.",
    "start": "842019",
    "end": "850570"
  },
  {
    "text": "And the gradient of f\ncorresponds to the gradient of the loss here.",
    "start": "850570",
    "end": "857580"
  },
  {
    "text": "And what is l? So l is the number of\ninputs to this function f.",
    "start": "857580",
    "end": "862980"
  },
  {
    "text": "So here, what is the input? What's the variable\nI care about? The variable I care about\nis the parameter theta.",
    "start": "862980",
    "end": "869470"
  },
  {
    "text": "So l is going to-- equals to, corresponds to,\nthe number of parameters.",
    "start": "869470",
    "end": "877899"
  },
  {
    "text": "And N, so what is the time to\ncompute this loss function?",
    "start": "877899",
    "end": "885700"
  },
  {
    "text": "The time to compute\na loss function is the same as the\nnumber of parameters. So if you think about\nyou have a neural network",
    "start": "885700",
    "end": "892800"
  },
  {
    "text": "of a million parameters\nand how much time you have to use to compute\nthe final loss function,",
    "start": "892800",
    "end": "898690"
  },
  {
    "text": "where you basically\nhave to give your input to the neural network, and\ngo through the entire neural network, and do all\nof these operations.",
    "start": "898690",
    "end": "905660"
  },
  {
    "text": "And basically,\neventually, the amount of time you have\nto spend is also similar to the\nnumber of parameters",
    "start": "905660",
    "end": "912480"
  },
  {
    "text": "for evaluating a loss. N is the time to\nevaluate the loss. So that means that the time\nfor the gradient, so computing",
    "start": "912480",
    "end": "921769"
  },
  {
    "text": "gradient, also takes-- according to the theorem,\nthis also takes O of N time.",
    "start": "921769",
    "end": "928529"
  },
  {
    "text": "So basically, you only have to\ntake O of number of parameters time to compute the gradient.",
    "start": "928529",
    "end": "936180"
  },
  {
    "text": "Any questions so far? Would you mind explaining\nwhy l is the level of prop?",
    "start": "936180",
    "end": "945420"
  },
  {
    "text": "Because it seems like l is the\ndimension of the input already. So they sound pretty\ndifferent constantly.",
    "start": "945420",
    "end": "952470"
  },
  {
    "text": "So here, my-- that\ndepends on how you might apply this\ntheorem to this setting. So here, I'm going to\nview this as a function",
    "start": "952470",
    "end": "959790"
  },
  {
    "text": "of the parameters. So the parameter is my\ninput to this function. I'm going to differentiate with\nrespect to the inputs, right?",
    "start": "959790",
    "end": "966810"
  },
  {
    "text": "So the theorem is\na generic theorem. So there are some function. And you differentiate\nwith respect",
    "start": "966810",
    "end": "972250"
  },
  {
    "text": "to the input of the function. It depends on how\nyou use this theorem. So if you use this\ntheorem, you say",
    "start": "972250",
    "end": "977420"
  },
  {
    "text": "the theta corresponds to x\nthere, so then, what is l? l is always dimension\nof the x right there.",
    "start": "977420",
    "end": "984579"
  },
  {
    "text": "So l is dimension\nof the theta here. Thanks. Yeah, any other questions?",
    "start": "984579",
    "end": "994690"
  },
  {
    "text": "So I guess the plan for the\nnext-- rest of the lecture is that we're going\nto show you how",
    "start": "994690",
    "end": "1000259"
  },
  {
    "text": "this works for neural networks. I'm not going to\nshow you how this works for our general circuits.",
    "start": "1000259",
    "end": "1005759"
  },
  {
    "text": "It's actually not\nmuch more different if you do it for the\ngeneral circuits. But it's just like,\nthe whole thing",
    "start": "1005759",
    "end": "1011310"
  },
  {
    "text": "is you need to have\na lot of jargon to really prove this\nwith the general circuits",
    "start": "1011310",
    "end": "1017379"
  },
  {
    "text": "because you have\nto consider how-- all the generalities. So basically, I'm going to\nshow the concrete example",
    "start": "1017380",
    "end": "1022831"
  },
  {
    "text": "for neural networks. How do you compute a gradient? And when you see the\nconcrete examples",
    "start": "1022831",
    "end": "1028140"
  },
  {
    "text": "for the neural\nnetworks, you see how you do it for the general\ncircuits, which I will probably",
    "start": "1028140",
    "end": "1034538"
  },
  {
    "text": "discuss if I have\ntime at the end. And another thing, before I\ntalk about the theorem here,",
    "start": "1034539",
    "end": "1043740"
  },
  {
    "text": "I guess I'd like to\nsay that this is also, this theorem is also,\nthe basis for many of the so-called\nsecond order method.",
    "start": "1043740",
    "end": "1051750"
  },
  {
    "text": "So basis.",
    "start": "1051750",
    "end": "1061170"
  },
  {
    "text": "And also, for example,\nfor meta-learning. I guess we haven't\nintroduced meta-learning.",
    "start": "1061170",
    "end": "1067080"
  },
  {
    "text": "But I think the point-- the\npart that is relevant is that--",
    "start": "1067080",
    "end": "1073769"
  },
  {
    "text": "so actually, I'm\nnot going to talk about what meta-learning\nin this course, just because it requires\nmore backgrounds.",
    "start": "1073770",
    "end": "1084390"
  },
  {
    "text": "But the general idea is that\nyou can use this theorem twice so that you\ncan do something",
    "start": "1084390",
    "end": "1091020"
  },
  {
    "text": "about the second\norder derivative. But there's a specific\nsetting you can do.",
    "start": "1091020",
    "end": "1097509"
  },
  {
    "text": "So basically, I think-- let me just give you a\nsense of what I mean here. This is not comprehensive\nbecause, sometimes, you're",
    "start": "1097510",
    "end": "1105830"
  },
  {
    "text": "going to use this theorem\nin different ways. But this is one way to use\nthis theorem in a clever way to get something more\nthan what it offers.",
    "start": "1105830",
    "end": "1112840"
  },
  {
    "text": "So for example, suppose\nin the same setting,",
    "start": "1112840",
    "end": "1121440"
  },
  {
    "text": "you can claim that for any\nvector V of dimension l,",
    "start": "1121440",
    "end": "1128620"
  },
  {
    "text": "this is the same as the\ninput dimension of x, then this, the Hessian, the\nsecond derivative, which",
    "start": "1128620",
    "end": "1139610"
  },
  {
    "text": "is a matrix, this is I by l\nmatrix times this vector V,",
    "start": "1139610",
    "end": "1146640"
  },
  {
    "text": "can be computed \nin",
    "start": "1146640",
    "end": "1153169"
  },
  {
    "text": "O of N plus l time. And as I said, N\nis bigger than l.",
    "start": "1153169",
    "end": "1159330"
  },
  {
    "text": "So this is still O of N time. So you can see that this is--",
    "start": "1159330",
    "end": "1165049"
  },
  {
    "text": "now it's more kind of a little\nbit more magical, right, because, if you want\nto compute this matrix,",
    "start": "1165049",
    "end": "1171309"
  },
  {
    "text": "typically, it takes-- even just to\nmemorize this matrix, even you just want to\ncompute each entry--",
    "start": "1171309",
    "end": "1176799"
  },
  {
    "text": "suppose each entry of this\nmatrix takes O of 1 time, then you already spend l squared\ntime to compute this matrix.",
    "start": "1176799",
    "end": "1184490"
  },
  {
    "text": "So certainly, if you wanted\nto get O of N plus l time, your algorithm cannot just first\ncompute this matrix and then",
    "start": "1184490",
    "end": "1191850"
  },
  {
    "text": "take the matrix vector product. That would be too\ninefficient because computing this matrix itself will\ntake l squared time.",
    "start": "1191850",
    "end": "1198770"
  },
  {
    "text": "So however, if you\ncompute that whole thing without going through--\nwithout doing the matrix first,",
    "start": "1198770",
    "end": "1204120"
  },
  {
    "text": "and then take the\nmatrix vector product and compute the whole\nthing altogether, then there is a\nway to speed it up.",
    "start": "1204120",
    "end": "1209409"
  },
  {
    "text": "And you can do it\nin O of N time. Well, N is bigger\nthan l, basically. So basically, you\njust take O of N time.",
    "start": "1209409",
    "end": "1215370"
  },
  {
    "text": "If we're going to do\nthat often, then why can't we just use\nNewton's method?",
    "start": "1215370",
    "end": "1222429"
  },
  {
    "text": "Is that [INAUDIBLE]? Right. So Newton's method,\ndepending on which Newton method you\nare talking about, the vanilla Newton's method\nrequires this matrix.",
    "start": "1222429",
    "end": "1230039"
  },
  {
    "text": "Then it's l squared time. So we only know how\nto do matrix vector-- Hessian vector product.",
    "start": "1230039",
    "end": "1237809"
  },
  {
    "text": "So that's why, I\nthink, at least there are a bunch of papers\nin the last few years, which tries to implement\nthe original Newton's method",
    "start": "1237810",
    "end": "1245990"
  },
  {
    "text": "without computing the Hessian. So they try to use\nthe Hessian vector product to have a\ndifferent way to implement",
    "start": "1245990",
    "end": "1251500"
  },
  {
    "text": "the original Newton's method. And that was\nsomewhat successful. I think you can have\nsome new algorithms that",
    "start": "1251500",
    "end": "1259669"
  },
  {
    "text": "tries to only use the\nHessian vector product to implement approximate\nversion of the Newton's method.",
    "start": "1259669",
    "end": "1266289"
  },
  {
    "text": "Yeah, but it's not trivial. It's pretty nontrivial.",
    "start": "1266289",
    "end": "1271909"
  },
  {
    "text": "You cannot do it just\nin the most trivial way. Right.",
    "start": "1271910",
    "end": "1278559"
  },
  {
    "text": "And actually, this corollary is\nactually pretty easy to prove.",
    "start": "1278559",
    "end": "1283980"
  },
  {
    "text": "And when you prove it, you\ncan see how it's actually-- actually, the proof also tells\nyou how to implement this.",
    "start": "1283980",
    "end": "1292300"
  },
  {
    "text": "So basically, the\nway to prove it is that you just say, I'm\ngoing to define a new function,",
    "start": "1292300",
    "end": "1299460"
  },
  {
    "text": "g of x. This new function is going to\nbe the gradient times v. So this",
    "start": "1299460",
    "end": "1307940"
  },
  {
    "text": "is a real valued\nfunction because this is a function that maps\nI dimensional vector",
    "start": "1307940",
    "end": "1316299"
  },
  {
    "text": "to a real vector. The output is real\nvector, even though you-- there are some-- sorry, the\noutput is a real valued scalar,",
    "start": "1316299",
    "end": "1323870"
  },
  {
    "text": "right? Even though these are vectors,\nyou take any part of that, it becomes scalar.",
    "start": "1323870",
    "end": "1329100"
  },
  {
    "text": "And then, so if you use the\ntheorem, so ",
    "start": "1329100",
    "end": "1338020"
  },
  {
    "text": "by the theorem, you know that g of x can\nbe computed in ",
    "start": "1338020",
    "end": "1348659"
  },
  {
    "text": "O of N time, just because then the\ngradient can be computed.",
    "start": "1348659",
    "end": "1355620"
  },
  {
    "text": "So that's why,\nafter the gradient, you take the inner product. You spend another O of N-- actually, O of N\nplus l time, just",
    "start": "1355620",
    "end": "1362190"
  },
  {
    "text": "to be precise, because\nyou compute the gradient,",
    "start": "1362190",
    "end": "1367789"
  },
  {
    "text": "it takes O of N time. And then, computing the\ninner product is l time.",
    "start": "1367789",
    "end": "1372880"
  },
  {
    "text": "You get O of N plus l time. And then, you use the\ntheorem again on g.",
    "start": "1372880",
    "end": "1382060"
  },
  {
    "text": "So before, you are\nusing the ",
    "start": "1382060",
    "end": "1391250"
  },
  {
    "text": "theorem on f. And you say the nabla\nf can be computed. Now you are going to\nuse the theorem on g.",
    "start": "1391250",
    "end": "1396370"
  },
  {
    "text": "So you view g as the new f. So you view the f as the-- what's the right way to say it?",
    "start": "1396370",
    "end": "1402370"
  },
  {
    "text": "So this g will be the f\nin the theorem, right? Then you verify whether g\nsatisfies the condition.",
    "start": "1402370",
    "end": "1408320"
  },
  {
    "text": "That's true because g can be\ncomputed in O of N plus l time.",
    "start": "1408320",
    "end": "1413440"
  },
  {
    "text": "And then, that\nmeans the nabla of g can also be computed\nin O of N plus l time.",
    "start": "1413440",
    "end": "1420720"
  },
  {
    "text": "All right. So this means that nabla g\nalso computed, can be computed,",
    "start": "1420720",
    "end": "1432230"
  },
  {
    "text": "in O of N plus l time because\nyou can do this twice, right?",
    "start": "1432230",
    "end": "1438610"
  },
  {
    "text": "And what is the nabla g? The gradient of g of x, if\nyou do the computation, what's",
    "start": "1438610",
    "end": "1447860"
  },
  {
    "text": "the gradient of this? It's really just, you take\nanother gradient here. I guess this probably\nrequires a little bit--",
    "start": "1447860",
    "end": "1453410"
  },
  {
    "text": "you can verify this using\nchain rules offline. But the gradient of g\nis really the gradient",
    "start": "1453410",
    "end": "1460309"
  },
  {
    "text": "of the inner product\nof f x and v. And this is actually the Hessian\nof f x times v. Any questions?",
    "start": "1460309",
    "end": "1485110"
  },
  {
    "text": "And you can do these\nfor many other things where, for example,\nif you define any functions of\nthe gradient, you",
    "start": "1485110",
    "end": "1491520"
  },
  {
    "text": "can still take the derivative. For example, I think\nin meta-learning, sometimes you have to have\nmaybe some special function",
    "start": "1491520",
    "end": "1498059"
  },
  {
    "text": "of the gradient. And then, you have to take\nanother gradient of it. And that's sometimes a\nsubmodule used in meta-learning.",
    "start": "1498059",
    "end": "1504600"
  },
  {
    "text": "And then, you can\napply this again. You can say, OK, because\nthe gradient is computable,",
    "start": "1504600",
    "end": "1510800"
  },
  {
    "text": "in O of N time this\nfunction is probably just a very simple function. So the whole thing can be\ncomputable in O of N time.",
    "start": "1510800",
    "end": "1515980"
  },
  {
    "text": "And then, you can take\nanother derivative again. So that's when--",
    "start": "1515980",
    "end": "1521840"
  },
  {
    "text": "I'm not going to go\ninto more details. But that's how people are using\nit in meta-learning as well.",
    "start": "1521840",
    "end": "1527148"
  },
  {
    "text": "So anyway, so this part\nis advanced material. We are not going to test it in\nany of the exams or homeworks.",
    "start": "1527149",
    "end": "1533450"
  },
  {
    "text": "But just, I feel like this is\ngood to know to some extent, just because this is\nused in many other more",
    "start": "1533450",
    "end": "1540710"
  },
  {
    "text": "advanced situations. Any other questions?",
    "start": "1540710",
    "end": "1549460"
  },
  {
    "text": "Cool. So I guess now, next,\nwhat I'm going to do is that I'm going to\ndiscuss a concrete case",
    "start": "1549460",
    "end": "1558210"
  },
  {
    "text": "with a neural network. I'm going to start with\ntwo neural networks. And then, I'm going to talk\nabout deep neural nets.",
    "start": "1558210",
    "end": "1564750"
  },
  {
    "text": "And basically, I'm going to\napply the auto differentiation.",
    "start": "1564750",
    "end": "1570230"
  },
  {
    "text": "The proof of the theorem\nis, basically, you have to give auto\ndifferentiation algorithm, also called back propagation.",
    "start": "1570230",
    "end": "1576370"
  },
  {
    "text": "So basically, I'm going to\napply the back propagation to the specific instance,\nneural networks.",
    "start": "1576370",
    "end": "1583260"
  },
  {
    "text": "OK. So I guess, let me set up some--",
    "start": "1583260",
    "end": "1595470"
  },
  {
    "text": "so before talking\nabout neural networks, maybe let me briefly\ndiscuss a preliminary.",
    "start": "1595470",
    "end": "1603380"
  },
  {
    "text": "This is the chain rule. I'm somewhat assuming\nthat this is covered",
    "start": "1603380",
    "end": "1609500"
  },
  {
    "text": "in most of the calculus class. But I'm going to have\nto review this in here,",
    "start": "1609500",
    "end": "1614640"
  },
  {
    "text": "partly because I'm going to have\npotentially different notations from what you-- not exactly the same\nnotations from what",
    "start": "1614640",
    "end": "1621039"
  },
  {
    "text": "you learned from\nthe calculus course, just because different calculus\nbook has different notations. So for the purpose\nof this course,",
    "start": "1621039",
    "end": "1629030"
  },
  {
    "text": "what I'm going to do is\nthat suppose I have a-- suppose J is a function\nof theta 1 up to theta p.",
    "start": "1629030",
    "end": "1644309"
  },
  {
    "text": "I'm trying to use as close\nnotations as our final use case, even though this part\nis supposed to be abstract.",
    "start": "1644309",
    "end": "1652049"
  },
  {
    "text": "So all the notations\nare just some symbols. I'm trying to use\nsimilar notations, just to avoid too many confusions.",
    "start": "1652050",
    "end": "1660240"
  },
  {
    "text": "So suppose you have\nJ is a function of a bunch of parameters,\na bunch of variables.",
    "start": "1660240",
    "end": "1666470"
  },
  {
    "text": "And then, suppose you have\nsome intermediate variables. Suppose there is a--",
    "start": "1666470",
    "end": "1674010"
  },
  {
    "text": "so how does this function work? This function works by--",
    "start": "1674010",
    "end": "1679559"
  },
  {
    "text": "I guess, maybe let me just-- this function is defined\nin this following form.",
    "start": "1679559",
    "end": "1684720"
  },
  {
    "text": "So you have some\nintermediate variables.",
    "start": "1684720",
    "end": "1692840"
  },
  {
    "text": "So maybe I say\nthere are j1, which is a function of the inputs,\ntheta 1 up to theta p.",
    "start": "1692840",
    "end": "1702809"
  },
  {
    "text": "And also, you have up\nto jk, which are j-- some functions of this.",
    "start": "1702809",
    "end": "1711590"
  },
  {
    "text": "So I guess I'm not sure whether\nthese formulas make any sense. So I'm using the ji as\nboth the function or--",
    "start": "1711590",
    "end": "1721870"
  },
  {
    "text": "and the variable of the\noutput of the function. I think this is pretty typical\nin most of the math books.",
    "start": "1721870",
    "end": "1728860"
  },
  {
    "text": "So you can view j as a variable. And then, this j is a function\nof the theta 1 up to theta p. And which function it is?",
    "start": "1728860",
    "end": "1734860"
  },
  {
    "text": "You still use j1 to\ndescribe that function. So does it make sense?",
    "start": "1734860",
    "end": "1741799"
  },
  {
    "text": "Yeah, and then, just because I\ndon't really necessarily care about exactly what the\nfunction is, so I'm just",
    "start": "1741799",
    "end": "1747540"
  },
  {
    "text": "going to give a name. And this name is just called\nthe same thing as the variable. And then, J is a function of\nthese intermediate variables.",
    "start": "1747540",
    "end": "1757559"
  },
  {
    "text": "And once you have\nthis two-layer set up,",
    "start": "1757559",
    "end": "1764278"
  },
  {
    "text": "then suppose you care\nabout the derivative. The chain rule is\nsaying that if you",
    "start": "1764279",
    "end": "1770800"
  },
  {
    "text": "care about the partial\nderivative of the final output with respect to some\ninput theta i, then now",
    "start": "1770800",
    "end": "1779220"
  },
  {
    "text": "how do you compute\nthis derivative? You can do it with\nthe chain rule. So what you do is you say\nyou are going to-- you numerate over all\nintermediate variables.",
    "start": "1779220",
    "end": "1787000"
  },
  {
    "text": "So you take the sum\nover j from 1 to k. And for each\nintermediate variables, you first compute the\nderivative, which is back",
    "start": "1787000",
    "end": "1793870"
  },
  {
    "text": "to the intermediate variables. And then, you multiply\nthis derivative with the derivative of\nintermediate variables",
    "start": "1793870",
    "end": "1800460"
  },
  {
    "text": "with respect to the variables\nyou care about, theta i. And just to clarify\ndimension, so this",
    "start": "1800460",
    "end": "1807158"
  },
  {
    "text": "is in R. And everything is in-- everything is a scalar so far.",
    "start": "1807159",
    "end": "1815460"
  },
  {
    "text": "So this is the chain rule, just\nin the language of this course.",
    "start": "1815460",
    "end": "1822658"
  },
  {
    "text": "Any questions? So now I'm going to talk\nabout a concrete case.",
    "start": "1822659",
    "end": "1830090"
  },
  {
    "text": "And maybe, let me also\ndefine some notations.",
    "start": "1830090",
    "end": "1841850"
  },
  {
    "text": "So maybe one important\nthing to note is that every time, at least\nin the context of this course,",
    "start": "1841850",
    "end": "1847870"
  },
  {
    "text": "every time you have this\npartial derivative thing, we insist that this quantity,\nthe quantity on top,",
    "start": "1847870",
    "end": "1854860"
  },
  {
    "text": "is a real valued function. So we are not considering\nanything about multivariate outputs, not because you\ncannot consider them.",
    "start": "1854860",
    "end": "1862960"
  },
  {
    "text": "It's more because, typically\nin machine learning, if you are taking derivatives\nof a multioutput function,",
    "start": "1862960",
    "end": "1871179"
  },
  {
    "text": "multivariable function,\ncomputationally, it's a problem.",
    "start": "1871180",
    "end": "1880929"
  },
  {
    "text": "So in some sense, I think\nin machine learning, people try to avoid that. And at least on the\nalgorithm level.",
    "start": "1880929",
    "end": "1886549"
  },
  {
    "text": "Of course, in\nanalysis, probably you have to think about that notion. But on an algorithm level,\nlike 90% of the time,",
    "start": "1886550",
    "end": "1894408"
  },
  {
    "text": "you always try to\ntake the derivative of a real valued function. You never try to take\nderivatives of multioutputs,",
    "start": "1894409",
    "end": "1902850"
  },
  {
    "text": "multivariable functions. And in this course, we only\nhave to think about the case where this is a\nreal value function.",
    "start": "1902850",
    "end": "1909889"
  },
  {
    "text": "So that's why in the\nnotation, so the notation here is that, so suppose J is a\nreal valued function, valued",
    "start": "1909890",
    "end": "1921630"
  },
  {
    "text": "variable of function. Then of course, it's clear\nwhat this would mean.",
    "start": "1921630",
    "end": "1929149"
  },
  {
    "text": "So if you take this with-- back\nto some variable theta, that just means the partial\nderivative, that's easy.",
    "start": "1929150",
    "end": "1934269"
  },
  {
    "text": "But what if you have a vector? So suppose if A-- so this is easy.",
    "start": "1934269",
    "end": "1939750"
  },
  {
    "text": "This is just-- so if theta\nis a real valued vector, this is just the partial\nderivative, which is easy.",
    "start": "1939750",
    "end": "1947600"
  },
  {
    "text": "And what if A is-- is it the green\none it's not great? Maybe I'll use the black one.",
    "start": "1947600",
    "end": "1953549"
  },
  {
    "text": "So if A is some\nvariable in dimension d,",
    "start": "1953549",
    "end": "1960010"
  },
  {
    "text": "then this would be\nalso in dimension d.",
    "start": "1960010",
    "end": "1966909"
  },
  {
    "text": "That's our notation. I'm just clarifying a\ndimensionality, right? And if-- so\nbasically, this would",
    "start": "1966909",
    "end": "1973419"
  },
  {
    "text": "be just a collection of\nscalars, the partial derivative",
    "start": "1973419",
    "end": "1981009"
  },
  {
    "text": "of J with respect to A1 up\nto the partial derivative J with respect to Ad,\nwhich is dimension d.",
    "start": "1981010",
    "end": "1988250"
  },
  {
    "text": "And we are also going to\nget into the situation",
    "start": "1988250",
    "end": "1993950"
  },
  {
    "text": "where A is a matrix. Maybe A is a matrix of d1, d2.",
    "start": "1993950",
    "end": "1999940"
  },
  {
    "text": "And now, what does\nthis notation mean becomes a little bit\ntricky because, sometimes in different literature,\nthere are a little bit",
    "start": "1999940",
    "end": "2007180"
  },
  {
    "text": "different conventions. For this course, what\nwe're going to do",
    "start": "2007180",
    "end": "2013200"
  },
  {
    "text": "is just that this has\nthe same shape as A. So this is also just all\nthe partial derivatives.",
    "start": "2013200",
    "end": "2026029"
  },
  {
    "text": "And this is the same as this,\nhas exactly the same shape as a itself.",
    "start": "2026029",
    "end": "2031059"
  },
  {
    "text": "So basically, in\nthis course notation, these kind of derivatives or\ngradient, whatever you call it,",
    "start": "2031059",
    "end": "2038070"
  },
  {
    "text": "right, so they always\nhave the same shape as the original variable.",
    "start": "2038070",
    "end": "2043420"
  },
  {
    "text": "All right. OK.",
    "start": "2043420",
    "end": "2050320"
  },
  {
    "text": "So \nI think I need to--",
    "start": "2050320",
    "end": "2058669"
  },
  {
    "text": "so now let me talk about\nthe two layer network.",
    "start": "2058669",
    "end": "2070419"
  },
  {
    "text": "This is just a review. I think Masha has\ntalked about this. So you have something like--",
    "start": "2070419",
    "end": "2077220"
  },
  {
    "text": "so if you-- the loss function\nis evaluated by the following. So if you evaluate\na loss function,",
    "start": "2077220",
    "end": "2084369"
  },
  {
    "text": "what do you have to do? You have to first compute\nthe output of the model.",
    "start": "2084370",
    "end": "2090220"
  },
  {
    "text": "And then, you compare\nit with the label. And you compute the loss. So basically, the computation\nto evaluate a loss function",
    "start": "2090220",
    "end": "2096378"
  },
  {
    "text": "is the sequence of operations. So you first compute the\nso-called intermediate layers",
    "start": "2096379",
    "end": "2102020"
  },
  {
    "text": "by doing something like this. This is the first layer of\nweights times the input,",
    "start": "2102020",
    "end": "2107109"
  },
  {
    "text": "the data, times-- plus some bias variables. And so let's say this\nis of some dimension m.",
    "start": "2107109",
    "end": "2115339"
  },
  {
    "text": "And then, you apply\nsome entrywise ReLU. And you still maintain\nthe same dimension.",
    "start": "2115340",
    "end": "2124920"
  },
  {
    "text": "And then, you apply\nanother layer. Maybe, let's say--\nanother layer, let's call it O. The output\nof the nonlayer is called O.",
    "start": "2124920",
    "end": "2132230"
  },
  {
    "text": "So you apply W to a plus b2. And this layer becomes the\nout-- this is the output layer.",
    "start": "2132230",
    "end": "2139420"
  },
  {
    "text": "So you want to make\nthis one-dimensional. So you get R1. And often, sometimes, this\nis called h theta of x.",
    "start": "2139420",
    "end": "2145810"
  },
  {
    "text": "This is the model output. And then, you compute the loss. The loss is something like",
    "start": "2145810",
    "end": "2151660"
  },
  {
    "text": "By the way, I'm here only\nhaving one example, x and y. X is the input. y is the output.",
    "start": "2151660",
    "end": "2160760"
  },
  {
    "text": "OK? So that's just a quick review\nof the two neural networks. I think I'm quite\nsure that we are using",
    "start": "2160760",
    "end": "2166440"
  },
  {
    "text": "the same notation as Masha. Hopefully. [CHUCKLES] So now the question\nis, how do I compute",
    "start": "2166440",
    "end": "2173160"
  },
  {
    "text": "the gradient with respect\nto W, W1, W2, b1, b2? [INAUDIBLE] for\nthe-- in general,",
    "start": "2173160",
    "end": "2180328"
  },
  {
    "text": "how many of the samples will\nbe using the gradient for?",
    "start": "2180329",
    "end": "2188240"
  },
  {
    "text": "For all of them? Or it will be just the value of\na gradient for a few samples,",
    "start": "2188240",
    "end": "2195569"
  },
  {
    "text": "like in batches in\nthe [INAUDIBLE]?? Right. So that's another layer. So that's when you-- that's about how you implement\nthis algorithm, right?",
    "start": "2195569",
    "end": "2202400"
  },
  {
    "text": "So if every time, you just\nselect, take one example, you just have to do\nit for one example. If every time, you\ntake a batch, then you have to do it for a batch.",
    "start": "2202400",
    "end": "2209430"
  },
  {
    "text": "But for the purpose\nof this lecture-- this is a good question. But for the purpose\nof this lecture,",
    "start": "2209430",
    "end": "2215088"
  },
  {
    "text": "we only care about one example\nbecause, eventually, you just do all the-- for all--\neven you have 10 examples, do all of them the same way.",
    "start": "2215089",
    "end": "2221290"
  },
  {
    "text": "Yeah. Yeah, you just repeat\nthe calculation. Of course you can\nparallelize all the gradients",
    "start": "2221290",
    "end": "2226920"
  },
  {
    "text": "of all the examples. But the method is the same. [INAUDIBLE]",
    "start": "2226920",
    "end": "2233029"
  },
  {
    "text": "So how do I compute\na gradient of this?",
    "start": "2233030",
    "end": "2238040"
  },
  {
    "text": "The gradient of the loss with\nrespect to the parameters.",
    "start": "2238040",
    "end": "2256220"
  },
  {
    "text": "So what I'm going to do\nis that I'm going to-- so go ahead.",
    "start": "2256220",
    "end": "2262630"
  },
  {
    "text": "Yeah, sorry. Just a question. Why is h theta of x equal to 0?",
    "start": "2262630",
    "end": "2268359"
  },
  {
    "text": "Oh, no. Yeah, this is a\nvariable called O. [CHUCKLES] I think I probably should--",
    "start": "2268359",
    "end": "2274240"
  },
  {
    "text": "how do I-- yeah, I\nshould change the notes. Yeah, this is fine,\nthe LaTeX, right?",
    "start": "2274240",
    "end": "2280460"
  },
  {
    "text": "But how do I write this so\nthat it does not look like a 0? [SIDE CONVERSATION]",
    "start": "2280460",
    "end": "2287930"
  },
  {
    "text": "I think it's OK to-- let's see. So is there any chance\nthat I can do it on the fly to change the notation?",
    "start": "2287930",
    "end": "2294260"
  },
  {
    "text": "I think I should be able to. What will it let\nme change it to? Sigma [INAUDIBLE]",
    "start": "2294260",
    "end": "2301940"
  },
  {
    "text": "Sigma is going to be used. It's going to be used\nfor some other things. Tau. OK.",
    "start": "2301940",
    "end": "2308359"
  },
  {
    "text": "But just, this will make a--\nintroduce some inconsistency with the notes because,\nin the notes, this is O.",
    "start": "2308359",
    "end": "2314269"
  },
  {
    "text": "[CHUCKLES] Just to let you know. OK. But I probably should change\nthat in the notes as well.",
    "start": "2314270",
    "end": "2320040"
  },
  {
    "text": "And also, you see that this\none doesn't show up that often.",
    "start": "2320040",
    "end": "2325380"
  },
  {
    "text": "[CHUCKLES] Hopefully. OK. So what I'm going\nto do is that I'm going to try to use some\nchain rule with this, right?",
    "start": "2325380",
    "end": "2335089"
  },
  {
    "text": "So how do you use chain rule? So for example, if you just look\nat one entry of this matrix,",
    "start": "2335090",
    "end": "2340970"
  },
  {
    "text": "this is a matrix. You look at 1 and 2. You can use the\nchain rule to derive the derivative of that entry.",
    "start": "2340970",
    "end": "2346650"
  },
  {
    "text": "So that's the typical\nway that we do it. It's going to be a\nvery complex formula. But you can still do it. So you use the chain\nrule multiple times",
    "start": "2346650",
    "end": "2353089"
  },
  {
    "text": "and then compute the\nderivative of that entry. And then eventually, you\nget a lot of formulas.",
    "start": "2353089",
    "end": "2359030"
  },
  {
    "text": "And then, you try to\nregroup those formulas into a nice form. So that's actually written\nin the lecture notes.",
    "start": "2359030",
    "end": "2364750"
  },
  {
    "text": "If you're interested,\nyou can look at them. It's pretty much a\nbrute force computation.",
    "start": "2364750",
    "end": "2370210"
  },
  {
    "text": "Of course, if you do that\nbrute force computation multiple times, then you\nget a little bit kind of like experience.",
    "start": "2370210",
    "end": "2375330"
  },
  {
    "text": "And you can do it\nfaster in the future. So what I'm going\nto do is that I'm going to try to keep everything\nas a vectorized notation.",
    "start": "2375330",
    "end": "2383470"
  },
  {
    "text": "And still, I'm going\nto do chain rule. But I'm going to do a more\nvectorized of the chain rule.",
    "start": "2383470",
    "end": "2391279"
  },
  {
    "text": "So because of that, I'm going to\nhave this kind of abstraction.",
    "start": "2391279",
    "end": "2400510"
  },
  {
    "text": "So this is-- in some sense,\nI call this chain rule for matrix multiplication. I guess there's no\nunique name for this.",
    "start": "2400510",
    "end": "2406940"
  },
  {
    "text": "This is just invented by me. But this is really\na simple fact.",
    "start": "2406940",
    "end": "2414640"
  },
  {
    "text": "So I'm going to suppose z is\nequal to W times u plus b,",
    "start": "2414640",
    "end": "2425200"
  },
  {
    "text": "where, let's say, so W\nis a matrix of dimension. I say m by d.",
    "start": "2425200",
    "end": "2431569"
  },
  {
    "text": "And u is a vector\nof dimension d. And b is a vector\nof dimension l.",
    "start": "2431569",
    "end": "2437190"
  },
  {
    "text": "We don't have to\nreally necessarily care about the dimensions because\nat least as long as they match, then it's fine.",
    "start": "2437190",
    "end": "2443050"
  },
  {
    "text": "And then, I have some\nfunction applied on top of z.",
    "start": "2443050",
    "end": "2449660"
  },
  {
    "text": "So this is my abstraction. So you can see this is\na little bit like this because if you map this z to\nthis z, and this W1 to this W,",
    "start": "2449660",
    "end": "2458839"
  },
  {
    "text": "and x to u, and b1 to b,\nthen it's kind of like that.",
    "start": "2458840",
    "end": "2465280"
  },
  {
    "text": "It's a abstraction of\na part of the problem. And then, I'm going\nto claim that then,",
    "start": "2465280",
    "end": "2474619"
  },
  {
    "text": "the derivative with\nrespect to the W, which is what I care about, at least\nif you map it back to here,",
    "start": "2474620",
    "end": "2480460"
  },
  {
    "text": "we care about derivative\nwith respect to W, so then this is going\nto be equals to what?",
    "start": "2480460",
    "end": "2486280"
  },
  {
    "text": "It's going to equal to a\nderivative with respect to jz times u transpose.",
    "start": "2486280",
    "end": "2497810"
  },
  {
    "text": "And maybe, just to make\nsure you are convinced that a notation-- the dimensions\nmatch, so this is supposed",
    "start": "2497810",
    "end": "2505380"
  },
  {
    "text": "to be in R to the m by d because\nthe derivative, as I said,",
    "start": "2505380",
    "end": "2511509"
  },
  {
    "text": "should have the same dimension\nas the original variable W. So W is the N by d.",
    "start": "2511510",
    "end": "2516530"
  },
  {
    "text": "Then this is m by d. And this is in Rm because\nz is m dimensional vector.",
    "start": "2516530",
    "end": "2525500"
  },
  {
    "text": "So that's why this is in R. And this is in R1 times d\nbecause u is of dimension d.",
    "start": "2525500",
    "end": "2537950"
  },
  {
    "text": "And u transpose is\nof dimension 1 by d. I guess all the vectors\nare column vectors. So if it's R, m dimensional\nvector, it's really m by 1.",
    "start": "2537950",
    "end": "2546430"
  },
  {
    "text": "So all the vectors in this\ncourse is column vectors. So that's why this whole\nthing kind of match",
    "start": "2546430",
    "end": "2554640"
  },
  {
    "text": "dimension because this\nis a column vector. This is a row vector. You take the outer product\nof them, you get a matrix.",
    "start": "2554640",
    "end": "2559730"
  },
  {
    "text": "Actually, this is\na rank 1 matrix. That's actually an\ninteresting observation. The gradient with respect to the\nweight matrix, for one example,",
    "start": "2559730",
    "end": "2567160"
  },
  {
    "text": "is always a rank 1 matrix. Oh, but for one example,\nnot for all the examples. Not for the full gradient.",
    "start": "2567160",
    "end": "2574530"
  },
  {
    "text": "If you just have one\nexample, the gradient for the weight matrix is\ntypically rank 1 matrix.",
    "start": "2574530",
    "end": "2580369"
  },
  {
    "text": "And if you look at-- and also, we know the\ngradient with respect",
    "start": "2580369",
    "end": "2587510"
  },
  {
    "text": "to b is going to be equals to\ngradient with respect to z.",
    "start": "2587510",
    "end": "2593910"
  },
  {
    "text": "So this doesn't solve everything\nbecause you still don't know what this quantity is.",
    "start": "2593910",
    "end": "2598920"
  },
  {
    "text": "This quantity is unknown here. And this quantity, the same\nquantity here, is unknown. But at least they\nsolve the local part.",
    "start": "2598920",
    "end": "2605308"
  },
  {
    "text": "It's like chain\nrule, where it says that if you want to derive-- take the derivative\nwith respect to W, then you only have to know\nthe derivative with respect",
    "start": "2605309",
    "end": "2613349"
  },
  {
    "text": "to intermediate variable z. And then next, I'm\ngoing to talk about how to take the derivative\nwith respect to z.",
    "start": "2613349",
    "end": "2620420"
  },
  {
    "text": "But this is a\ndecomposition, right? So it says that if\nyou want to know the derivative\nwith respect to W,",
    "start": "2620420",
    "end": "2626309"
  },
  {
    "text": "then you only have to know the\nderivative with respect to z.",
    "start": "2626309",
    "end": "2632579"
  },
  {
    "text": "Can you explain again\nwhy [INAUDIBLE]?? This is just what\na formula tells me. But here, I'm verifying\nthat it does make sense",
    "start": "2632579",
    "end": "2639830"
  },
  {
    "text": "because this is a 1 by d matrix, And this is m dimensional vector\nor m times 1 dimensional vector",
    "start": "2639830",
    "end": "2647828"
  },
  {
    "text": "because I view all the\nvectors as column vectors. So that's why m by 1 times Now why is it transpose?",
    "start": "2647829",
    "end": "2655930"
  },
  {
    "text": "That's just because-- [INAUDIBLE] you\nwant the dimensions to match [INAUDIBLE]. Oh, I think the\nfundamental reason",
    "start": "2655930",
    "end": "2662700"
  },
  {
    "text": "is you do the calculation,\nit's exactly this. But it just happens to match. It has to match if the\ncalculation is correct.",
    "start": "2662700",
    "end": "2672078"
  },
  {
    "text": "And also, maybe that's a\nreasonable way to memorize it. [CHUCKLES] You have to make\nthe dimension match.",
    "start": "2672079",
    "end": "2677280"
  },
  {
    "text": "So that's why it's the case. So how do you prove this?",
    "start": "2677280",
    "end": "2683180"
  },
  {
    "text": "So proving this, you\nhave to go low level. You have to do it\nfor every entry. So I'm going to show it once.",
    "start": "2683180",
    "end": "2688609"
  },
  {
    "text": "And then later,\nI'm going to have more abstractions like this. And then, I'm going\nto prove it for you. So for this one, I'm\ngoing to do a quick proof.",
    "start": "2688609",
    "end": "2696410"
  },
  {
    "text": "It's really just a derivation. So what you do is you\njust use the chain rule.",
    "start": "2696410",
    "end": "2714510"
  },
  {
    "text": "So what you do is you look\nat the derivative, derivative with respect to any entry, Wij.",
    "start": "2714510",
    "end": "2722610"
  },
  {
    "text": "And how do you do this? You say-- you use the most\nbasic version of the chain rule. You loop over all the possible\nintermediate variables.",
    "start": "2722610",
    "end": "2730230"
  },
  {
    "text": "So what are the\nintermediate variables? These are z's, right? It sounds like it's\nvery natural to use z as the intermediate variables.",
    "start": "2730230",
    "end": "2735609"
  },
  {
    "text": "So I'm going to loop over\nall possible intermediate variables, k from 1 to m. And each of the zi is one of the\nintermediate variables, or zk.",
    "start": "2735609",
    "end": "2746830"
  },
  {
    "text": "So dj over dzk and\nthen dzk over dWij.",
    "start": "2746830",
    "end": "2752079"
  },
  {
    "text": "All right? And then, I'm going to\nplug-in the definition of zk.",
    "start": "2752079",
    "end": "2764850"
  },
  {
    "text": "So then, what's the\ndefinition of zk? And zk is is defined by\nthis matrix multiplication.",
    "start": "2764850",
    "end": "2770500"
  },
  {
    "text": "What is the kth\ndimension of here? It's the kth dimension of this. The case dimension\nof this is going",
    "start": "2770500",
    "end": "2775620"
  },
  {
    "text": "to be something like\nthe definition of matrix multiplication, Wk1 times u1\nplus Wk2 times u2, up, dot,",
    "start": "2775620",
    "end": "2785200"
  },
  {
    "text": "dot until Wkd times ud\nplus this b, the bk.",
    "start": "2785200",
    "end": "2796640"
  },
  {
    "text": "And you look at the part of the\nderivative of this with respect to Wij.",
    "start": "2796640",
    "end": "2801839"
  },
  {
    "text": "So how to do this. It's like-- so first of all,\nno, to make this non-zero,",
    "start": "2801839",
    "end": "2807430"
  },
  {
    "text": "you have to make sure that\nthis variable at least show up in the top. If the variable doesn't\neven show up on top, there's no partial derivative.",
    "start": "2807430",
    "end": "2813460"
  },
  {
    "text": "The partial\nderivative will be 0. So this is only non-zero\nonly if this Wij",
    "start": "2813460",
    "end": "2822690"
  },
  {
    "text": "does show up in the top. So when Wij shows up on the\ntop, only if k is equal to i.",
    "start": "2822690",
    "end": "2829930"
  },
  {
    "text": "So because only Wk something\nshow up on the top and Wij. So only i and j are the\nsame. r-- i and k are same,",
    "start": "2829930",
    "end": "2838650"
  },
  {
    "text": "then Wij can show up on the top. So that's why you only have\nto care about those cases",
    "start": "2838650",
    "end": "2844230"
  },
  {
    "text": "where k is equal to i. So you just-- the\nentire sum is gone.",
    "start": "2844230",
    "end": "2849289"
  },
  {
    "text": "So you only have\nto care about zi.",
    "start": "2849290",
    "end": "2854660"
  },
  {
    "text": "And then, so you have-- maybe I'll just do it slowly. So this is just Wi1 times u1\nplus to Wid times ud plus bi",
    "start": "2854660",
    "end": "2869410"
  },
  {
    "text": "or dWij. So Wij only show up once\nhere on the top linearly.",
    "start": "2869410",
    "end": "2877308"
  },
  {
    "text": "So Wij show up here on\nthe top in this term",
    "start": "2877309",
    "end": "2882480"
  },
  {
    "text": "somewhere in the middle. So there is a middle term, which\njust look like Wij times uj.",
    "start": "2882480",
    "end": "2891060"
  },
  {
    "text": "That's the term that's\nWi-- where Wij shows up. And the derivative of this with\nrespect to Wij is equal to uj.",
    "start": "2891060",
    "end": "2898020"
  },
  {
    "text": "So that's why this is\nequal to, sorry, times uj.",
    "start": "2898020",
    "end": "2918780"
  },
  {
    "text": "So that's my dj over dWij. And then, I have to group all\nof these into a matrix form.",
    "start": "2918780",
    "end": "2926220"
  },
  {
    "text": "So if you verified\nthat, so basically, you group all of these\nentries into a matrix form,",
    "start": "2926220",
    "end": "2931650"
  },
  {
    "text": "it will be like this. So you're going to get\nall of this, dj over dzi,",
    "start": "2931650",
    "end": "2937890"
  },
  {
    "text": "into this dj over dz term. And then, this uj term will be\ngrouped into this u transpose.",
    "start": "2937890",
    "end": "2951040"
  },
  {
    "text": "So I guess I'm using\na very simple fact. The simple fact is that if\nsomething, maybe let's say xij,",
    "start": "2951040",
    "end": "2961059"
  },
  {
    "text": "is equal to ai times\nbj, then the matrix x is equal to a times b transpose.",
    "start": "2961060",
    "end": "2968568"
  },
  {
    "text": "That's the simple\nfact I'm using. So if some-- if the\nentry of a matrix is equal to some ai\ntimes bj, then you",
    "start": "2968569",
    "end": "2976890"
  },
  {
    "text": "can write it as matrix\nform, where then this matrix x is equal to a times b\ntranspose. a is a vector.",
    "start": "2976890",
    "end": "2985970"
  },
  {
    "text": "b is a vector.",
    "start": "2985970",
    "end": "3000660"
  },
  {
    "text": "Any questions?",
    "start": "3000660",
    "end": "3008000"
  },
  {
    "text": "OK. So now I'm going to\napply this abstraction,",
    "start": "3008000",
    "end": "3014210"
  },
  {
    "text": "this so-called vectorized\nform of the chain rule. So my problem\nhere, so what I got",
    "start": "3014210",
    "end": "3022160"
  },
  {
    "text": "is that, so what's the mapping? The mapping is that z maps to z.",
    "start": "3022160",
    "end": "3027578"
  },
  {
    "text": "And W1 maps to W. x maps to u.",
    "start": "3027579",
    "end": "3034420"
  },
  {
    "text": "And J maps to J. So that's\nhow I use this abstraction.",
    "start": "3034420",
    "end": "3040400"
  },
  {
    "text": "And after I use this\nabstraction, what I got is that I\ngot W1j dj over dW1",
    "start": "3040400",
    "end": "3049859"
  },
  {
    "text": "is equals to dj over dz times u\ntranspose will be x transpose.",
    "start": "3049859",
    "end": "3061759"
  },
  {
    "text": "x transpose. So of course, this is\nnot done because we",
    "start": "3061760",
    "end": "3070490"
  },
  {
    "text": "want to compute this. But we have a reduction\nin some sense. So we did some partial work.",
    "start": "3070490",
    "end": "3075940"
  },
  {
    "text": "Our goal is to compute this. But now we said\nthat you don't have to compute this, dj over dz.",
    "start": "3075940",
    "end": "3081440"
  },
  {
    "text": "And next, I'm going to show\nyou how to compute dj over dz. So it's like you are\npeeling off every--",
    "start": "3081440",
    "end": "3086798"
  },
  {
    "text": "a layer by layer in some sense. Of course, you can also\ndo the same thing for b. I guess the b is always easier.",
    "start": "3086799",
    "end": "3093578"
  },
  {
    "text": "So this will be just this.",
    "start": "3093579",
    "end": "3101809"
  },
  {
    "text": "So next question is,\nhow do you do the-- let's see whether I should--\nmaybe I should use a new board.",
    "start": "3101809",
    "end": "3128578"
  },
  {
    "text": "So next, I'm going\nto compute this.",
    "start": "3128579",
    "end": "3140340"
  },
  {
    "text": "From-- the z is this\nz So how do I do this?",
    "start": "3140340",
    "end": "3146140"
  },
  {
    "text": "I'm going to have\nanother abstraction. So this is the abstract problem.",
    "start": "3146140",
    "end": "3156109"
  },
  {
    "text": "So note that my z, the\nrelations between J and z",
    "start": "3156109",
    "end": "3163660"
  },
  {
    "text": "is through this a and W2. So there are some complicated\ndependencies between J and z.",
    "start": "3163660",
    "end": "3170019"
  },
  {
    "text": "So I'm going to\nabstract one part of it, just to make our\nderivation clean.",
    "start": "3170020",
    "end": "3175319"
  },
  {
    "text": "So that abstraction would\nbe that you think of--",
    "start": "3175319",
    "end": "3183220"
  },
  {
    "text": "suppose you have a variable\na, which is sigma of z. And then, J is a function of a.",
    "start": "3183220",
    "end": "3188809"
  },
  {
    "text": "And sigma is this entrywise,\nsigma is entrywise,",
    "start": "3188809",
    "end": "3203650"
  },
  {
    "text": "activation function. Sigma is the s, basically.",
    "start": "3203650",
    "end": "3210849"
  },
  {
    "text": "So I'm going to claim\nthat in this case, you can-- you know that your\ntarget, the question you care",
    "start": "3210849",
    "end": "3217960"
  },
  {
    "text": "about, dj over dz, is\ngoing to be equals to dj over da times sigma prime z.",
    "start": "3217960",
    "end": "3234099"
  },
  {
    "text": "And this is a-- so I guess, let me\nexplain the notation here.",
    "start": "3234099",
    "end": "3239839"
  },
  {
    "text": "So this is our m\ndimensional vector. This is also m\ndimensional vector.",
    "start": "3239839",
    "end": "3246058"
  },
  {
    "text": "Let's say in this\nabstract in z, a, they're are all m\ndimensional vector.",
    "start": "3246059",
    "end": "3251740"
  },
  {
    "text": "So then, these are all\nm dimensional vector. And this is the so-called\nentrywise, entrywise, product.",
    "start": "3251740",
    "end": "3260480"
  },
  {
    "text": "So I'm taking two m\ndimensional vectors. I take entrywise product. And that gets the\nderivative I care about.",
    "start": "3260480",
    "end": "3271940"
  },
  {
    "text": "And then, you can see\nthat the only thing you have to care-- you\nhave to compute next is, what is dj over da?",
    "start": "3271940",
    "end": "3279780"
  },
  {
    "text": "Because dj over da is still-- I know.",
    "start": "3279780",
    "end": "3285099"
  },
  {
    "text": "I'm not going to do\nthis proof for this. It's actually even easier\nthan the other one. You just have to expand\nand do the chain rule.",
    "start": "3285099",
    "end": "3296568"
  },
  {
    "text": "So now, next, let me try to--",
    "start": "3296569",
    "end": "3313190"
  },
  {
    "text": "so now I'm trying to\ndeal with dj over da. So how do I deal\nwith dj over da?",
    "start": "3313190",
    "end": "3319849"
  },
  {
    "text": "I'm going to, again, abstractify\nthis part of computation in some abstract form.",
    "start": "3319849",
    "end": "3325560"
  },
  {
    "text": "So my abstraction is\nthat, so the abstraction",
    "start": "3325560",
    "end": "3332049"
  },
  {
    "text": "is that I guess\nI'm going to have",
    "start": "3332049",
    "end": "3337859"
  },
  {
    "text": "a is equals to W\ntimes u plus b and J",
    "start": "3337859",
    "end": "3342880"
  },
  {
    "text": "is equals to a function of a.",
    "start": "3342880",
    "end": "3349038"
  },
  {
    "text": "So why this is a\nreasonable abstraction? Because a maps to this a. W maps to this W2.",
    "start": "3349039",
    "end": "3355548"
  },
  {
    "text": "And this u maps to-- wait, am I doing something? Sorry. My bad.",
    "start": "3355549",
    "end": "3362240"
  },
  {
    "text": "I have to use a different-- maybe, let's call this tau.",
    "start": "3362240",
    "end": "3367359"
  },
  {
    "text": "That's a perfect\nplace to call it. So tau-- so why this is\na useful abstraction?",
    "start": "3367359",
    "end": "3374600"
  },
  {
    "text": "This is because the\nmapping in my mind is that tau means the tau above.",
    "start": "3374600",
    "end": "3383519"
  },
  {
    "text": "And the W here\nmeans the W2 above.",
    "start": "3383520",
    "end": "3390799"
  },
  {
    "text": "And b here means the b2 above. And J means the J.",
    "start": "3390799",
    "end": "3401030"
  },
  {
    "text": "So note that the difference\nhere is that this W now means the second layer.",
    "start": "3401030",
    "end": "3410380"
  },
  {
    "text": "And if you make this mapping,\nso then what you care about is that you care about\ndj over du because u--",
    "start": "3410380",
    "end": "3418490"
  },
  {
    "text": "oh, I guess I didn't say\nwhat u corresponds to. So u corresponds to a.",
    "start": "3418490",
    "end": "3423619"
  },
  {
    "text": "So u corresponds to\na because a is what",
    "start": "3423619",
    "end": "3430279"
  },
  {
    "text": "is multiplied with the matrix. So dj over du, so that's\nwhat I care about.",
    "start": "3430280",
    "end": "3438200"
  },
  {
    "text": "So you can see that even\nthough this abstraction is very similar to\nthis abstraction, the difference is that here\nI care about dj over du.",
    "start": "3438200",
    "end": "3445650"
  },
  {
    "text": "I care about the\nderivative with respect to the input of the\nmatrix multiplication. And before, I care about\nthe derivative with respect",
    "start": "3445650",
    "end": "3452490"
  },
  {
    "text": "to the matrices, the matrix\nin the matrix multiplication.",
    "start": "3452490",
    "end": "3457609"
  },
  {
    "text": "All right. So that's why it's\na bit different. And you're going to have a\ndifferent formula for it, of course, because you\nare taking the derivative",
    "start": "3457609",
    "end": "3464250"
  },
  {
    "text": "with respect to-- y is in respect to W. And\nthe other is in respect to u. All right.",
    "start": "3464250",
    "end": "3470770"
  },
  {
    "text": "So what's the formula for this? The formula for this is, if you\nwrite it in the matrix form,",
    "start": "3470770",
    "end": "3480800"
  },
  {
    "text": "W transpose times dj over dv.",
    "start": "3480800",
    "end": "3488589"
  },
  {
    "text": "d, sorry, tau.",
    "start": "3488589",
    "end": "3495180"
  },
  {
    "text": "So I guess if you check the\ndimensionality, then this one--",
    "start": "3495180",
    "end": "3500960"
  },
  {
    "text": "I don't know what\nthe dimension here. So I guess, let me\nspecify a dimension. So W is maybe something like--",
    "start": "3500960",
    "end": "3506330"
  },
  {
    "text": "I don't know. Let me come up with the-- maybe R times m.",
    "start": "3506330",
    "end": "3512058"
  },
  {
    "text": "And then, u is of dimension m. And tau then has to\nbe in dimension m--",
    "start": "3512059",
    "end": "3519059"
  },
  {
    "text": "sorry, dimension R. Sorry. So then, this is in dimension\nR. And this double transpose",
    "start": "3519059",
    "end": "3526380"
  },
  {
    "text": "is in dimension m\nby R. And that's why they can be\nmultiplied together.",
    "start": "3526380",
    "end": "3537210"
  },
  {
    "text": "Of course, I guess\nanother thing is that if you don't\nwant to remember all of these equations--\nyou want to remember this. So first of all, you don't\nto remember all of them.",
    "start": "3537210",
    "end": "3543670"
  },
  {
    "text": "Second, if you want\nto remember them and you want to\ncheat a little bit, you can just view\nthem as scalars.",
    "start": "3543670",
    "end": "3550170"
  },
  {
    "text": "And you can see then,\nso for example, this one would make a lot of sense\nif they are scalars.",
    "start": "3550170",
    "end": "3555619"
  },
  {
    "text": "That's just the\ntrivial chain rule. And this one makes\na lot of sense if they are all\nscalars because you",
    "start": "3555619",
    "end": "3560849"
  },
  {
    "text": "want to take the derivative\nwith u, with respect to u, then W have to show\nup as the coefficient.",
    "start": "3560849",
    "end": "3567369"
  },
  {
    "text": "So everything makes a lot of\nsense if they are scalars. And the only tricky thing is\nthat if they are matrices, then you have to figure out, what's\nthe right transpose if you",
    "start": "3567369",
    "end": "3576640"
  },
  {
    "text": "left multiply, right\nmultiply, so on and so forth?",
    "start": "3576640",
    "end": "3595000"
  },
  {
    "text": "So once I have\nthis thing, then I can apply this to the\nspecial case above.",
    "start": "3595000",
    "end": "3601529"
  },
  {
    "text": "If you apply it, then what you\nget is that with this mapping,",
    "start": "3601530",
    "end": "3608940"
  },
  {
    "text": "then you got dj over da is\nequals to W2 transpose times dj",
    "start": "3608940",
    "end": "3618270"
  },
  {
    "text": "over d tau.",
    "start": "3618270",
    "end": "3625990"
  },
  {
    "text": "That's because-- I'm just\nreplacing the note here. I'm just applying this\ngeneral thing to this case.",
    "start": "3625990",
    "end": "3633960"
  },
  {
    "text": "And now you see that there's\nonly one thing that is missing. What is dj over d tau?",
    "start": "3633960",
    "end": "3639430"
  },
  {
    "text": "And that's trivial\nbecause dj over d tau is just-- this is really\njust the very last thing.",
    "start": "3639430",
    "end": "3644558"
  },
  {
    "text": "J is just a y minus tau squared. So dj over d tau,\neveryone can compute.",
    "start": "3644559",
    "end": "3649578"
  },
  {
    "text": "This is just, I think,\nminus y minus tau.",
    "start": "3649579",
    "end": "3659910"
  },
  {
    "text": "So what do you really do? Eventually, then what\nyou really do eventually is that you first compute this.",
    "start": "3659910",
    "end": "3666088"
  },
  {
    "text": "Maybe this is step one. And then, this is step two. You compute dj over da.",
    "start": "3666089",
    "end": "3673309"
  },
  {
    "text": "Then where dj over da is used. And then, dj over\nda is used here.",
    "start": "3673309",
    "end": "3680289"
  },
  {
    "text": "So then, you do this step three. And then you get dj over dz.",
    "start": "3680289",
    "end": "3688309"
  },
  {
    "text": "dj over dz, they used it here. This is 4. Wait.",
    "start": "3688309",
    "end": "3694069"
  },
  {
    "text": "Maybe 4 is here. Then you get the derivative\nwith respect to W1.",
    "start": "3694069",
    "end": "3699940"
  },
  {
    "text": "So when you really implement\nit, you have to do it backward. When you do the\nderivation, it's reason--",
    "start": "3699940",
    "end": "3705260"
  },
  {
    "text": "we can do anyway. But I guess I'm doing it\nin this way from W to--",
    "start": "3705260",
    "end": "3712030"
  },
  {
    "text": "from the lower layer\nto the top layer. Sorry, from the first\nlayer to the second layer. But when you do\nthe implementation,",
    "start": "3712030",
    "end": "3717740"
  },
  {
    "text": "you have to first compute this,\nand this, and one, two, three,",
    "start": "3717740",
    "end": "3723290"
  },
  {
    "text": "four. I guess I didn't compute\nthe derivative with respect to all the parameters.",
    "start": "3723290",
    "end": "3730170"
  },
  {
    "text": "So I'm going to compute\nderivative with respect to W and b1. What if you want to compute the\nderivative with respect to W2?",
    "start": "3730170",
    "end": "3738020"
  },
  {
    "text": "Maybe that's a good question\nto see whether it's somehow you digest this. What if you want to\ndo the dj over dW2?",
    "start": "3738020",
    "end": "3757550"
  },
  {
    "text": "The chain rule states the\nderivative is with respect",
    "start": "3757550",
    "end": "3763190"
  },
  {
    "text": "to tau and then times the\nderivative of tau with respect to W2 and--",
    "start": "3763190",
    "end": "3768380"
  },
  {
    "text": "And for example, which\nabstraction do you want to use? I have three things,\none, two, and three. Which one do I use?",
    "start": "3768380",
    "end": "3780720"
  },
  {
    "text": "Yeah, I think your\nanswer is correct. So I guess, I think you only use\nthis because w2 is the matrix.",
    "start": "3780720",
    "end": "3793390"
  },
  {
    "text": "You care about derivative\nwith respect to matrix, then you want to use this. So basically, so \nif",
    "start": "3793390",
    "end": "3806880"
  },
  {
    "text": "you care about this, then you view this\nW2 is the W here.",
    "start": "3806880",
    "end": "3814770"
  },
  {
    "text": "So I guess, then you\nneed a different mapping. So I guess you need to\nuse the first abstraction,",
    "start": "3814770",
    "end": "3820289"
  },
  {
    "text": "the first dilemma. And then, you're going to\nsay W2 corresponds to W.",
    "start": "3820289",
    "end": "3829049"
  },
  {
    "text": "And the u now corresponds to\nwhat? u corresponds to the a here.",
    "start": "3829050",
    "end": "3838710"
  },
  {
    "text": "And b1, b2 corresponds to the b. And z corresponds to--",
    "start": "3838710",
    "end": "3845270"
  },
  {
    "text": "sorry, not z. So the tau corresponds to z.",
    "start": "3845270",
    "end": "3851549"
  },
  {
    "text": "So this is in the--\nthe right-hand side is in the abstraction. And the left-hand side is in\nthe real case I care about.",
    "start": "3851549",
    "end": "3859050"
  },
  {
    "text": "So this means that this\nis dj over d tau times--",
    "start": "3859050",
    "end": "3869349"
  },
  {
    "text": "I had u transpose here. Now I had a transpose here. So I'm going to have\na transpose here.",
    "start": "3869349",
    "end": "3880510"
  },
  {
    "text": "And I also know dj\nover db2 is going to be equals to dj over d tau.",
    "start": "3880510",
    "end": "3896150"
  },
  {
    "text": "Make sense?",
    "start": "3896150",
    "end": "3901829"
  },
  {
    "text": "So now if you have all\nof this, then how do we-- you-- I guess this is still\na little bit complicated.",
    "start": "3901830",
    "end": "3909130"
  },
  {
    "text": "But it's a little\nbetter than before because everything is\nnow vectorized notation.",
    "start": "3909130",
    "end": "3914630"
  },
  {
    "text": "And now, what if you do\nit for multiple layers? You'll see that everything\njust stays just the same. So you basically are\njust going to repeat it,",
    "start": "3914630",
    "end": "3920808"
  },
  {
    "text": "use these two Lemmas,\nso the three Lemmas for the three abstractions. I guess maybe I should\nnumber them in some way.",
    "start": "3920809",
    "end": "3928890"
  },
  {
    "text": "Maybe, let's call this Lemma 1.",
    "start": "3928890",
    "end": "3935880"
  },
  {
    "text": "And then here,\nI'm using Lemma 1.",
    "start": "3935880",
    "end": "3941859"
  },
  {
    "text": "And maybe, let's\ncall this Lemma 2.",
    "start": "3941859",
    "end": "3948500"
  },
  {
    "text": "And this is Lemma 3.",
    "start": "3948500",
    "end": "3954000"
  },
  {
    "text": "So for the deep\nneural networks, I'm just going to use Lemma",
    "start": "3954000",
    "end": "3959520"
  },
  {
    "text": "And I'm going to\nget the gradient. So that's the last thing in\nthis course-- in this lecture.",
    "start": "3959520",
    "end": "3974480"
  },
  {
    "text": "So let me first recall\nthe deep networks.",
    "start": "3974480",
    "end": "3989029"
  },
  {
    "text": "So suppose you have multiple\nlayers of neural networks.",
    "start": "3989029",
    "end": "3995000"
  },
  {
    "text": "So then, I guess using the\nsame notation as last lecture, the first layer is this.",
    "start": "3995000",
    "end": "4002329"
  },
  {
    "text": "The second layer is\nsome relu times z1.",
    "start": "4002329",
    "end": "4011700"
  },
  {
    "text": "Sorry, this is still\nthe first layer. This is activation. And then, you do\nthe second layer.",
    "start": "4011700",
    "end": "4019260"
  },
  {
    "text": "You have W2, a1\nplus b2, dot, dot.",
    "start": "4019260",
    "end": "4026819"
  },
  {
    "text": "Then you get the r minus 1\nlayer, which is relu of z r",
    "start": "4026819",
    "end": "4037361"
  },
  {
    "text": "minus 1. And then, I say you\nhave the r'th layer, zr,",
    "start": "4037361",
    "end": "4047079"
  },
  {
    "text": "which is equals to Wr\ntimes a r minus 1 plus br.",
    "start": "4047080",
    "end": "4057240"
  },
  {
    "text": "And then finally,\nyou have a loss. Like I say, I'll just\nwrite the loss here,",
    "start": "4057240",
    "end": "4063210"
  },
  {
    "text": "given that the loss will be-- J will be 1/2 times\ny minus zr squared.",
    "start": "4063210",
    "end": "4073260"
  },
  {
    "text": "So that's my computation of\nthe loss function or sequence",
    "start": "4073260",
    "end": "4078990"
  },
  {
    "text": "of matrix multiplication and\nthe activation functions.",
    "start": "4078990",
    "end": "4091290"
  },
  {
    "text": "So now I'm going to try to\ncompute the derivatives, the partial gradient.",
    "start": "4091290",
    "end": "4097568"
  },
  {
    "text": "So first of all, I'm\ngoing to compute dj",
    "start": "4097569",
    "end": "4103679"
  },
  {
    "text": "over dWk for some kth layer.",
    "start": "4103680",
    "end": "4110409"
  },
  {
    "text": "So how do you do that?",
    "start": "4110409",
    "end": "4115548"
  },
  {
    "text": "Can you raise that\na bit [INAUDIBLE]??",
    "start": "4115549",
    "end": "4121829"
  },
  {
    "text": "[CHUCKLES] Yeah, just a little bit awkward.",
    "start": "4121830",
    "end": "4128100"
  },
  {
    "text": "I guess maybe it's\nOK to ignore all-- you all know this is deep net?",
    "start": "4128100",
    "end": "4135150"
  },
  {
    "text": "So is that better? Yeah, nothing different. This is the same\nthing as in last time.",
    "start": "4135150",
    "end": "4142528"
  },
  {
    "text": "I'm just recalling\nthe notations. So I want to compute a\nderivative with respect",
    "start": "4142529",
    "end": "4154639"
  },
  {
    "text": "to kth matrix. So maybe I say, a derivative\nwith respect to W2, I say. Suppose k is 2.",
    "start": "4154640",
    "end": "4161068"
  },
  {
    "text": "Now how do you do it? So if I'm going to do\nthis with respect to W2, then the thing is that\nyou want to use the lemma.",
    "start": "4161069",
    "end": "4169588"
  },
  {
    "text": "So the lemma-- lemma 1, I guess. Maybe that's the most relevant\nbecause lemma 1 is trying",
    "start": "4169589",
    "end": "4175338"
  },
  {
    "text": "to take derivative\nwith respect to some W. So you just do some\npattern matching. You want to apply lemma 1.",
    "start": "4175339",
    "end": "4181969"
  },
  {
    "text": "So what's the abstraction here? The abstraction here\nis that if you--",
    "start": "4181970",
    "end": "4187028"
  },
  {
    "text": "so how do you do the\npattern matching? So I guess you say that\nwhat's the definition of z--",
    "start": "4187029",
    "end": "4192238"
  },
  {
    "text": "Wk? Wk is involved in\nthis computation in the following way. So Wk is involved in\nthe following way,",
    "start": "4192239",
    "end": "4198730"
  },
  {
    "text": "zk is equals to Wk times\na k minus 1 plus bk.",
    "start": "4198730",
    "end": "4208000"
  },
  {
    "text": "This is how Wk is involved. And then you say, I don't\ncare about what happens next.",
    "start": "4208000",
    "end": "4213159"
  },
  {
    "text": "I just abstractify the rest. So then, this is\npretty much the same as the setting, the lemma 1.",
    "start": "4213159",
    "end": "4222600"
  },
  {
    "text": "So then, using lemma 1-- oh, you can-- yeah, using this--",
    "start": "4222600",
    "end": "4231690"
  },
  {
    "text": "actually, you can call it-- you don't have to\ncall it the lemma. You can call it a\nformula or something. So lemma 1 tells\nyou that if you take",
    "start": "4231690",
    "end": "4238829"
  },
  {
    "text": "the derivative with\nrespect to the matrix, it's equals to the derivatives\nwith respect to zk.",
    "start": "4238830",
    "end": "4249350"
  },
  {
    "text": "zk is basically the output\nof the matrix multiplication. So times the input of the\nmatrix multiplication which",
    "start": "4249350",
    "end": "4258540"
  },
  {
    "text": "is a k minus 1 here transposed.",
    "start": "4258540",
    "end": "4305350"
  },
  {
    "text": "So that means that I only\nhave to take the derivative with respect to zk.",
    "start": "4305350",
    "end": "4311840"
  },
  {
    "text": "zk is basically some of these\nintermediate variables, right? So how do you take the\nderivative with respect to zk?",
    "start": "4311840",
    "end": "4319550"
  },
  {
    "text": "So you need to think\nabout, how is zk involved in this computation? So zk is involved.",
    "start": "4319550",
    "end": "4325870"
  },
  {
    "text": "So I care about this.",
    "start": "4325870",
    "end": "4331330"
  },
  {
    "text": "And zk is involved\nin the following way. So I'm not sure.",
    "start": "4331330",
    "end": "4343330"
  },
  {
    "text": "So zk is equals to relu-- sorry, zk is involved\ndirectly in the following way.",
    "start": "4343330",
    "end": "4354050"
  },
  {
    "text": "So ak is equals to relu of zk. And then J, the loss,\nis a function of ak.",
    "start": "4354050",
    "end": "4363469"
  },
  {
    "text": "That's the part that zk is\ndirectly involved because you first-- it goes to the relu.",
    "start": "4363469",
    "end": "4369380"
  },
  {
    "text": "You get a. And then, you do some\ncomputation to get J. And then, you can use\nthe so-called lemma 2.",
    "start": "4369380",
    "end": "4380550"
  },
  {
    "text": "So the lemma 2 will tell you\nthat, what is the dj over dzk? So it's going to be something\nabout dj over dak times",
    "start": "4380550",
    "end": "4394820"
  },
  {
    "text": "the relu prime times of zk. In fact, this is lemma 2.",
    "start": "4394820",
    "end": "4423870"
  },
  {
    "text": "And the third thing is,\nhow do you deal with ak? Well, what is the derivative\nwith respect to ak?",
    "start": "4423870",
    "end": "4430210"
  },
  {
    "text": "So if I don't know derivative\nwith respect to ak, then you have to see,\nagain, how ak is involved",
    "start": "4430210",
    "end": "4435870"
  },
  {
    "text": "in this whole computation. So how does-- how\nis ak involved? So ak is involved because\nak-- if you-- you only",
    "start": "4435870",
    "end": "4442562"
  },
  {
    "text": "use a to compute z again. You use a to compute z k plus 1.",
    "start": "4442562",
    "end": "4448860"
  },
  {
    "text": "So if you look at that\npart, so basically,",
    "start": "4448860",
    "end": "4453928"
  },
  {
    "text": "z k plus 1 is equals to\nsome WK plus 1 times ak plus",
    "start": "4453929",
    "end": "4462219"
  },
  {
    "text": "b b plus 1. This is the first\ntime the ak is used",
    "start": "4462219",
    "end": "4468989"
  },
  {
    "text": "and the only time ak is used. And then, you say\nthe rest of the thing is abstracted as\na general thing.",
    "start": "4468989",
    "end": "4477620"
  },
  {
    "text": "And then, you say\nI'm going to use-- now I'm going to use lemma",
    "start": "4477620",
    "end": "4483100"
  },
  {
    "text": "about matrix multiplication. Lemma 3 is also about\nmatrix multiplication.",
    "start": "4483100",
    "end": "4490980"
  },
  {
    "text": "But it's trying to take\nthe derivative with respect to the input to the\nmatrix multiplication.",
    "start": "4490980",
    "end": "4497040"
  },
  {
    "text": "So a is the input. So using lemma 3, I'm going\nto say, I guess, maybe--",
    "start": "4497040",
    "end": "4503360"
  },
  {
    "text": "I'm not sure whether I should\nalso make an explicit map. So if you care about\nthe explicit map,",
    "start": "4503360",
    "end": "4508850"
  },
  {
    "text": "then it means that ak\ncorresponds to the u there in Laman 3.",
    "start": "4508850",
    "end": "4517179"
  },
  {
    "text": "So WK plus 1\ncorresponds to W there. And bk plus 1 corresponds\nto the b there.",
    "start": "4517179",
    "end": "4526460"
  },
  {
    "text": "And J corresponds to J. I guess\nzk plus 1 corresponds to tau.",
    "start": "4526460",
    "end": "4536860"
  },
  {
    "text": "So I guess if you just\nkeep pattern matching, I think it's a bit\ndifficult to see the map.",
    "start": "4536860",
    "end": "4542350"
  },
  {
    "text": "Actually, probably\nthe right thing-- the right way to\nthink about it is that you really think about\nthe rows of these things.",
    "start": "4542350",
    "end": "4547929"
  },
  {
    "text": "So z k plus 1 is the output\nof the matrix multiplication. ak is the input to the\nmatrix multiplication.",
    "start": "4547929",
    "end": "4554080"
  },
  {
    "text": "And WK is the matrix\nin the multiplication. So that's how you easily\nmap the rows of them.",
    "start": "4554080",
    "end": "4562409"
  },
  {
    "text": "And then, so you\nget this thing ",
    "start": "4562410",
    "end": "4572380"
  },
  {
    "text": "is equals to WK plus 1 transpose.",
    "start": "4572380",
    "end": "4596400"
  },
  {
    "text": "And then-- oh, sorry,\nI think I have some-- is this-- this should be z.",
    "start": "4596400",
    "end": "4608100"
  },
  {
    "text": "So then, you can see that\nif you look at these two-- sorry, this is by lemma 3.",
    "start": "4608100",
    "end": "4613159"
  },
  {
    "text": "So if you look at this\nformula and this formula,",
    "start": "4613160",
    "end": "4619830"
  },
  {
    "text": "basically, this is like a\nrecursive in some sense.",
    "start": "4619830",
    "end": "4624980"
  },
  {
    "text": "So here, you are saying\nthat from dj, dak, you can compute dj, dzk.",
    "start": "4624980",
    "end": "4632880"
  },
  {
    "text": "And here from d-- yeah, basically, you\ncan just recursively use",
    "start": "4632880",
    "end": "4640340"
  },
  {
    "text": "these two to get all of them. So I'll just make it explicit.",
    "start": "4640340",
    "end": "4657040"
  },
  {
    "text": "So basically, with all of\nthis formula, you are-- basically, you already\ncompute everything. It's just, I'm going\nto reorganize this",
    "start": "4657040",
    "end": "4663190"
  },
  {
    "text": "to make it a little clearer. So what you do is\nthat you say, you",
    "start": "4663190",
    "end": "4671550"
  },
  {
    "text": "start from the last layer,\nthe last layer here.",
    "start": "4671550",
    "end": "4677290"
  },
  {
    "text": "So you first compute--",
    "start": "4677290",
    "end": "4683949"
  },
  {
    "text": "I guess, maybe let me just\ndescribe the final algorithm.",
    "start": "4683949",
    "end": "4694620"
  },
  {
    "text": "So you first compute, there\nis so-called forward pass. So you compute all the\nvalues of all the variables.",
    "start": "4694620",
    "end": "4707770"
  },
  {
    "text": "So in some sense, I've\nalready assumed that it's computed implicitly before.",
    "start": "4707770",
    "end": "4712780"
  },
  {
    "text": "So basically, compute\nall the z1, a1, z2, a2, so on and so forth.",
    "start": "4712780",
    "end": "4724850"
  },
  {
    "text": "So just by computing all of this\nnetwork, evaluating the loss, you get everything. So you get all the variables.",
    "start": "4724850",
    "end": "4731460"
  },
  {
    "text": "And then in the\nso-called backward pass, you compute a gradient.",
    "start": "4731460",
    "end": "4738860"
  },
  {
    "text": "And the way you do it is\nthat pretty much the same as the two layer network. You start with the last one.",
    "start": "4738860",
    "end": "4745948"
  },
  {
    "text": "So you first can compute\nthis with the last layer.",
    "start": "4745949",
    "end": "4753210"
  },
  {
    "text": "And this is trivial because\nJ depends on zr, just in a very trivial way. So this is just\nminus y minus zr.",
    "start": "4753210",
    "end": "4761530"
  },
  {
    "text": "All right. So this is our starting point. And now you recursively\nuse both of these",
    "start": "4761530",
    "end": "4768678"
  },
  {
    "text": "to get all of the dz\nof d-- dz over da. So you already have dj over dz--",
    "start": "4768679",
    "end": "4774730"
  },
  {
    "text": "sorry, dz over dzr. And then, you can\nuse that to compute",
    "start": "4774730",
    "end": "4782090"
  },
  {
    "text": "the previous one using this. So from this, you can\nget dj over da r minus 1",
    "start": "4782090",
    "end": "4792719"
  },
  {
    "text": "using this formula\nbecause you get, this is W. If I can do\nit on the fly correctly,",
    "start": "4792719",
    "end": "4799580"
  },
  {
    "text": "I guess this is ar\ntranspose dj of dzr.",
    "start": "4799580",
    "end": "4809909"
  },
  {
    "text": "So you remove the index by 1. And you get to a. And then, you can use the a to\ncompute the z, the dz over da",
    "start": "4809909",
    "end": "4819579"
  },
  {
    "text": "to compute the dz over-- dj over dz. So this is number 1.",
    "start": "4819580",
    "end": "4824698"
  },
  {
    "text": "This is number 2, number 3. You get z r minus 1, which\nis equals to minus 1.",
    "start": "4824699",
    "end": "4850120"
  },
  {
    "text": "So basically, after\ndoing this iteration, you get from r to r minus 1.",
    "start": "4850120",
    "end": "4855420"
  },
  {
    "text": "And then, you repeat. So then, you can repeat. You can say--",
    "start": "4855420",
    "end": "4861199"
  },
  {
    "text": "I guess, maybe I should\nnumber them by-- so maybe I should number this one by 1.",
    "start": "4861199",
    "end": "4866429"
  },
  {
    "text": "This will be 2, 2.1, 2.2, 3.1.",
    "start": "4866430",
    "end": "4872280"
  },
  {
    "text": "In the first round, you get a\nr minus 2 using this equation.",
    "start": "4872280",
    "end": "4881510"
  },
  {
    "text": "Maybe, let's say this\nequation is called 2. Let's call this 1.",
    "start": "4881510",
    "end": "4887120"
  },
  {
    "text": "So you're using 2.",
    "start": "4887120",
    "end": "4892449"
  },
  {
    "text": "And then, you get a,\nz version using 1.",
    "start": "4892449",
    "end": "4914300"
  },
  {
    "text": "And you do this repeatedly. So you get everything\nabout a and z. And after you get everything\nabout a and z, so basically,",
    "start": "4914300",
    "end": "4933090"
  },
  {
    "text": "after obtaining all the-- everything, like\nthis, it's pretty easy",
    "start": "4933090",
    "end": "4944198"
  },
  {
    "text": "to get the gradient\nwith respect to W because you can just\nsay a is using this.",
    "start": "4944199",
    "end": "4955510"
  },
  {
    "text": "Maybe, let's call this 3. You can say this is equals to--",
    "start": "4955510",
    "end": "4991790"
  },
  {
    "text": "so I guess you can see that\nI do need a forward pass because, in my\nbackward pass, I do",
    "start": "4991790",
    "end": "4998010"
  },
  {
    "text": "require a bunch of quantities. For example, it does require, I\nknow the quantity z, r minus 1.",
    "start": "4998010",
    "end": "5003969"
  },
  {
    "text": "So I have to save all the easy\nquantities, a, and r, and all of this in my memory.",
    "start": "5003970",
    "end": "5009330"
  },
  {
    "text": "And then in the backward\npass, I'm going to use them. And why this is\ncalled backward pass?",
    "start": "5009330",
    "end": "5015160"
  },
  {
    "text": "In some sense, if you think\nabout the computational flow, I guess this is probably my\nlast point I can make today.",
    "start": "5015160",
    "end": "5022120"
  },
  {
    "text": "So this is-- the reason why\nit's called back propagation is because you can view this as--",
    "start": "5022120",
    "end": "5030020"
  },
  {
    "text": "you can see that you are-- the way that you\nchange the index is you start from r and r minus 1.",
    "start": "5030020",
    "end": "5035610"
  },
  {
    "text": "And then, you do r\nminus 2, r minus 3. So you do this in\na backward way. So in some sense, if you\ndraw a computational graph",
    "start": "5035610",
    "end": "5045389"
  },
  {
    "text": "or flow of the computation,\nI'm not being very formal here, but if you draw\nit in some sense, then you can view\nthis whole computation",
    "start": "5045389",
    "end": "5051679"
  },
  {
    "text": "as you start with x. And then, you have some\nmatrix multiplication. You need to use the\nweight, W1 and b1.",
    "start": "5051679",
    "end": "5059519"
  },
  {
    "text": "And you do some\nmatrix multiplication. And you get z1.",
    "start": "5059519",
    "end": "5065850"
  },
  {
    "text": "And then, you do another\nmatrix multiplication. Maybe you do another a. Maybe this is as-- I say this.",
    "start": "5065850",
    "end": "5072928"
  },
  {
    "text": "You get z1. And you get the activation.",
    "start": "5072929",
    "end": "5078370"
  },
  {
    "text": "See, activation. And you get a1. And then, you do a\nmatrix multiplication,",
    "start": "5078370",
    "end": "5085199"
  },
  {
    "text": "which is also W2, b2.",
    "start": "5085199",
    "end": "5091179"
  },
  {
    "text": "And then, you get z2.",
    "start": "5091179",
    "end": "5101370"
  },
  {
    "text": "And you do this repeatedly. And you finally get zr. And you get the loss, J. So\nthis is like the forward pass.",
    "start": "5101370",
    "end": "5112280"
  },
  {
    "text": "And if you think about\nhow to conceptually-- how do you organize\nthe information",
    "start": "5112280",
    "end": "5118000"
  },
  {
    "text": "in this backward algorithm? In some sense, what\nyou do is that I'm just trying to visualize this.",
    "start": "5118000",
    "end": "5123620"
  },
  {
    "text": "But of course, everything, all\nof this, will be in computer. So in some sense, what you\ndo is that you first compute,",
    "start": "5123620",
    "end": "5136219"
  },
  {
    "text": "you first compute, a\nderivative with respect to this variable, zr.",
    "start": "5136219",
    "end": "5144550"
  },
  {
    "text": "And then you say, OK,\nhow did you get the z? You get the z by some\nmatrix multiplication.",
    "start": "5144550",
    "end": "5150449"
  },
  {
    "text": "So let me say this is\nmy matrix summation multiplication, where the input\nof the z is ar minus 1, right?",
    "start": "5150449",
    "end": "5159480"
  },
  {
    "text": "And so then, you take this. And sometimes from this,\nyou compute the derivative",
    "start": "5159480",
    "end": "5167099"
  },
  {
    "text": "with respect to an minus 1.",
    "start": "5167100",
    "end": "5172260"
  },
  {
    "text": "That's my step 2.1.",
    "start": "5172260",
    "end": "5177510"
  },
  {
    "text": "And then, you just keep\ndoing this backward fashion. So you figure out where a come\nfrom, a r minus 1 come from.",
    "start": "5177510",
    "end": "5187409"
  },
  {
    "text": "a r minus 1 come\nfrom z r minus 1. So that's why you\ndo a backward pass.",
    "start": "5187409",
    "end": "5192580"
  },
  {
    "text": "You say this is\ncoming from a minus 1.",
    "start": "5192580",
    "end": "5198449"
  },
  {
    "text": "That's my step 2.-- sorry, I'm not remembering\nthese correctly. So this is 2.2.",
    "start": "5198449",
    "end": "5204719"
  },
  {
    "text": "That's my step, 2.2.",
    "start": "5204719",
    "end": "5210710"
  },
  {
    "text": "And I keep doing this. Eventually, eventually,\nI get maybe z1.",
    "start": "5210710",
    "end": "5218869"
  },
  {
    "text": "So that's the so-called\nbackward, back propagation in some sense. And in the middle, you can\nalso compute the derivative",
    "start": "5218870",
    "end": "5225580"
  },
  {
    "text": "with respect to the W.\nSo because the derivative with respect to W is\nequals to something",
    "start": "5225580",
    "end": "5231530"
  },
  {
    "text": "about the derivative\nwith respect to z, So basically, once\nyou get this quantity,",
    "start": "5231530",
    "end": "5238178"
  },
  {
    "text": "you know you can start\nfrom this quantity to get the derivative with\nrespect to WR minus 1,",
    "start": "5238179",
    "end": "5246240"
  },
  {
    "text": "I think, r minus r, r minus 1. And then, every time you\nget one more dj over dz,",
    "start": "5246240",
    "end": "5254740"
  },
  {
    "text": "then you can get\none more dj over dW. So from this, you can\nget the dj over dW1.",
    "start": "5254740",
    "end": "5263389"
  },
  {
    "text": "So that's why it's\ncalled back propagation. And maybe just to say one-- I know we are\nrunning out of time.",
    "start": "5263390",
    "end": "5269600"
  },
  {
    "text": "Just one more word about this. So this is a very sequential\ncomputational graph.",
    "start": "5269600",
    "end": "5276080"
  },
  {
    "text": "But actually, if you have a more\ncomplex computational graph, so which is not as\nsequential as this,",
    "start": "5276080",
    "end": "5282619"
  },
  {
    "text": "you can do almost\nthe same thing. Basically, you just\nwrite all this graph. And then, you figure out how\nto do the back propagation.",
    "start": "5282619",
    "end": "5289810"
  },
  {
    "text": "And the general way to\ndo the back propagation is exactly the same. You just run a graph backward. The only thing you\nhave to figure out",
    "start": "5289810",
    "end": "5296550"
  },
  {
    "text": "is that, how does this-- what's this relationship? Basically, this arrow.",
    "start": "5296550",
    "end": "5302280"
  },
  {
    "text": "So how does the-- the derivative with\nrespect to a depends on z. So what's the\nderivative with respect",
    "start": "5302280",
    "end": "5307840"
  },
  {
    "text": "to the input of this\nmodule, of this-- you view this matrix\nmultiplication as a generic module, let's say.",
    "start": "5307840",
    "end": "5315559"
  },
  {
    "text": "Right? And the only thing\nyou have to figure out is that, how do you write a\nso-called backward function? This backward function takes\nin the derivative with respect",
    "start": "5315560",
    "end": "5323360"
  },
  {
    "text": "to the output of this\nmodule and output the derivative with respect\nto input of this module.",
    "start": "5323360",
    "end": "5329659"
  },
  {
    "text": "And that's the so-called\nbackward function. For example, if\nyou write PyTorch, if you need to\nwrite a new module, basically you have to\nimplement the forward function",
    "start": "5329659",
    "end": "5336429"
  },
  {
    "text": "and the backward function. The forward function is,\nhow do you get z from a? And the backward\nfunction is, how do you get the derivative with respect\nto a given a hypothetical",
    "start": "5336430",
    "end": "5348158"
  },
  {
    "text": "derivative with respect to z? By hypothetical, I\nmean just some vectors. You take in some vectors.",
    "start": "5348159",
    "end": "5353690"
  },
  {
    "text": "And then, you get\nsome vector result. And this is the\nfunction you have to implement for the module. And once you have this\nmodule, then you can--",
    "start": "5353690",
    "end": "5359580"
  },
  {
    "text": "then just, everything\nelse is systematic. Yeah, I think we\nshould stop here.",
    "start": "5359580",
    "end": "5365260"
  },
  {
    "text": "OK. Thanks.",
    "start": "5365260",
    "end": "5366260"
  }
]