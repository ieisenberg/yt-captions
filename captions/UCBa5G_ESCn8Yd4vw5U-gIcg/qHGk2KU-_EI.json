[
  {
    "start": "0",
    "end": "180000"
  },
  {
    "text": "Hello, today we are going to discuss ERM for probabilistic classification.",
    "start": "6290",
    "end": "12910"
  },
  {
    "text": "So the key idea is that we have a probabilistic classifier,",
    "start": "15590",
    "end": "21105"
  },
  {
    "text": "G sub Theta which depends on a parameter Theta. It takes as input u,",
    "start": "21105",
    "end": "30215"
  },
  {
    "text": "and it returns to us a probability distribution over the target set.",
    "start": "30215",
    "end": "36985"
  },
  {
    "text": "And we're gonna choose the Theta by ERM, or regularized ERM as we usually do.",
    "start": "36985",
    "end": "44440"
  },
  {
    "text": "And we want to judge the performance of the probabilistic classifier using",
    "start": "44440",
    "end": "49760"
  },
  {
    "text": "the average negative log likelihood on our test dataset.",
    "start": "49760",
    "end": "55739"
  },
  {
    "text": "And now our dataset, remember what it consists of, it consists of n points u_1 through u_n and v_1 through v_n.",
    "start": "57980",
    "end": "69915"
  },
  {
    "text": "And we're produ- going to produce, uh, corresponding distributions,",
    "start": "69915",
    "end": "75840"
  },
  {
    "text": "p-hat 1 through p-hat n, which are our predictions at each of those data points, u_1 through u_n.",
    "start": "75840",
    "end": "87105"
  },
  {
    "text": "And then we're gonna to compare our predictions, p-hat_i, with the true value v_i.",
    "start": "87105",
    "end": "96210"
  },
  {
    "text": "So in order to do that, we need a loss function, and the loss function has to be able to compare two things,",
    "start": "96210",
    "end": "102050"
  },
  {
    "text": "a p-hat and a v. And the p-hat is a distribution on our target set script v,",
    "start": "102050",
    "end": "111435"
  },
  {
    "text": "and the v is actually an element of script b.",
    "start": "111435",
    "end": "117070"
  },
  {
    "text": "And so, um- so we're comparing things that aren't really like.",
    "start": "117070",
    "end": "122295"
  },
  {
    "text": "Um, one thing to notice here is that p-hat is actually a probability distribution, is a function.",
    "start": "122295",
    "end": "130114"
  },
  {
    "text": "And so we're feeding in to this function l, another function, and a v,",
    "start": "130115",
    "end": "138290"
  },
  {
    "text": "which is just, uh, one of our possible targets. And so once we've got such a thing,",
    "start": "138290",
    "end": "145170"
  },
  {
    "text": "uh, we are, uh, going to look at the average loss 1 on n,",
    "start": "145170",
    "end": "151939"
  },
  {
    "text": "sum from i is 1 to n, the loss of p-hat_i and v_i. And we're gonna choose the Theta to minimize the average loss.",
    "start": "151940",
    "end": "160805"
  },
  {
    "text": "And if we're doing regularized ERM, then we'll have our regularization, uh,",
    "start": "160805",
    "end": "167160"
  },
  {
    "text": "function r of Theta, and we'll minimize the average loss plus lambda times r of Theta.",
    "start": "167160",
    "end": "176130"
  },
  {
    "start": "180000",
    "end": "337000"
  },
  {
    "text": "Now the particular, uh, uh,",
    "start": "182660",
    "end": "187740"
  },
  {
    "text": "loss of interest is going to be this thing, l_ce.",
    "start": "187740",
    "end": "193350"
  },
  {
    "text": "Um, and what it is, is it's the negative log likelihood of v under the distribution p-hat.",
    "start": "193350",
    "end": "203004"
  },
  {
    "text": "And so remember that we computed the negative log-likelihood of our entire set of p's and v's.",
    "start": "203005",
    "end": "213375"
  },
  {
    "text": "Uh, assuming the- all of the v's were independent. Um, and so here we're just looking at one of those for a single data point,",
    "start": "213375",
    "end": "221365"
  },
  {
    "text": "and we're looking at the negative log-likelihood of getting that particular v under a particular distribution p-hat.",
    "start": "221365",
    "end": "229855"
  },
  {
    "text": "And this is called l_ce, the ce stands for cross entropy. So this is also called the cross entropy loss.",
    "start": "229855",
    "end": "238765"
  },
  {
    "text": "And it's just the negative log of the probability of the outcome v. When you look at this,",
    "start": "238765",
    "end": "245150"
  },
  {
    "text": "it's important to realize that it really is a function of two things. It's a function of p-hat,",
    "start": "245150",
    "end": "250610"
  },
  {
    "text": "and it's a function of v. And that function is, of course, a composition operation composed with a negative log operation.",
    "start": "250610",
    "end": "259415"
  },
  {
    "text": "We're taking p-hat and v and we're evaluating p-hat at the point v. Um,",
    "start": "259415",
    "end": "267390"
  },
  {
    "text": "and of course we can do that because p-hat is a function. Um, now because, uh,",
    "start": "267390",
    "end": "275115"
  },
  {
    "text": "p-hat is, uh, less than or equal to 1. Well, the negative log of it must be greater than or equal to 0.",
    "start": "275115",
    "end": "283159"
  },
  {
    "text": "And the only way you can have the- the - the negative log equal to 0 is if, uh,",
    "start": "283160",
    "end": "289765"
  },
  {
    "text": "p-hat is equal to 1, that is means we would- that would mean we will be completely certain about the outcome.",
    "start": "289765",
    "end": "296310"
  },
  {
    "text": "In other words, the probability of getting that particular V, uh, would be, uh, would be 1.",
    "start": "296310",
    "end": "302370"
  },
  {
    "text": "And the probability of getting any other v, therefore would be 0. So that's when our prediction is that",
    "start": "302370",
    "end": "310160"
  },
  {
    "text": "the true v is actually the v we're evaluating at and with 100% probability.",
    "start": "310160",
    "end": "316150"
  },
  {
    "text": "Um, and so we want the negative log-likelihood, uh,",
    "start": "316150",
    "end": "322850"
  },
  {
    "text": "to be small so that, uh, it's much as- as close as possible,",
    "start": "322850",
    "end": "328549"
  },
  {
    "text": "we're getting p-hat of v close to 1. Now, um,",
    "start": "328549",
    "end": "332800"
  },
  {
    "start": "337000",
    "end": "584000"
  },
  {
    "text": "this cross entropy loss, it's a- is a loss function,",
    "start": "341660",
    "end": "347694"
  },
  {
    "text": "but it's a loss function which is aimed specifically at probabilistic prediction, um, uh,",
    "start": "347695",
    "end": "354530"
  },
  {
    "text": "and this is a little different from when we did ERM for either deterministic classification or for regression.",
    "start": "354530",
    "end": "362845"
  },
  {
    "text": "Because there we had a loss function which was a function of y-hat and y,",
    "start": "362845",
    "end": "367984"
  },
  {
    "text": "and, um, it was comparing y-hat and y, the predicted value y over the actual value y.",
    "start": "367984",
    "end": "375415"
  },
  {
    "text": "And those are both same kinda quantities. They were both vectors in r,",
    "start": "375415",
    "end": "380889"
  },
  {
    "text": "m. And here, this is a little different in two ways.",
    "start": "380890",
    "end": "386195"
  },
  {
    "text": "First of all, our prediction isn't a y-hat, but it's a p-hat. So it's a function, it's a probability distribution.",
    "start": "386195",
    "end": "392895"
  },
  {
    "text": "And the second is, is that- the- the- the second argument of the loss function isn't an embedded v,",
    "start": "392895",
    "end": "400190"
  },
  {
    "text": "it's actually the raw target v. Um, and of course,",
    "start": "400190",
    "end": "405390"
  },
  {
    "text": "the raw target v is simply one of the, um, one of the original target set script v. Um,",
    "start": "405390",
    "end": "413580"
  },
  {
    "text": "and we don't need to embed it into, uh, uh, into rm in order to be able to evaluate a loss,",
    "start": "413580",
    "end": "420510"
  },
  {
    "text": "and so we don't have to do that. So we can certainly compute the empirical risk",
    "start": "420510",
    "end": "426229"
  },
  {
    "text": "on the entire dataset now that we've got a loss function. We simply compute 1 on n,",
    "start": "426230",
    "end": "432155"
  },
  {
    "text": "the sum from i is 1 to n of l_ce, of, uh, the p-hat_i, that's with v_i.",
    "start": "432155",
    "end": "441245"
  },
  {
    "text": "And p-hat_i is the prediction at the ith value of u,",
    "start": "441245",
    "end": "446505"
  },
  {
    "text": "and v_i is the true value of the target variable in the ith datapoint.",
    "start": "446505",
    "end": "453455"
  },
  {
    "text": "And p-hat_i is itself generated by the predictor. It's G of u_i.",
    "start": "453455",
    "end": "459600"
  },
  {
    "text": "Um, one nice thing about this is that this is actually our performance metric.",
    "start": "459700",
    "end": "466030"
  },
  {
    "text": "And so, uh, remember that one of the things we did in regression and deterministic classification was that we had something we really cared about,",
    "start": "466030",
    "end": "476185"
  },
  {
    "text": "such as the Neyman-Pearson Loss, but we couldn't actually minimize that.",
    "start": "476185",
    "end": "481860"
  },
  {
    "text": "And so we used a proxy loss. We used a loss function which was a replacement for the true quantity of interest,",
    "start": "481860",
    "end": "489325"
  },
  {
    "text": "the true performance metric. And here we don't need to do that because the average value",
    "start": "489325",
    "end": "495129"
  },
  {
    "text": "of the cross entropy loss is the average negative log-likelihood, which is a performance metric that we care about.",
    "start": "495130",
    "end": "503520"
  },
  {
    "text": "It is the probability of seeing those data points v_i under p-hat,",
    "start": "503520",
    "end": "511780"
  },
  {
    "text": "or it is the likelihood of the joint distribution, p-hat 1 through p-hat n. Um, uh,",
    "start": "511780",
    "end": "521990"
  },
  {
    "text": "the, uh, now remember we've seen already that,",
    "start": "521990",
    "end": "527060"
  },
  {
    "text": "uh, when you have a constant predictor, that means p-hat_i doesn't depend on i. Um, then you can work out what this quantity is,",
    "start": "527060",
    "end": "534710"
  },
  {
    "text": "and that is the cross entropy. So the average cross entropy loss is the cross entropy when we had the constant.",
    "start": "534710",
    "end": "543740"
  },
  {
    "text": "So when we're interpreting this, uh, we think about it as a measure of implausibility.",
    "start": "547340",
    "end": "555570"
  },
  {
    "text": "The cross entropy loss of p-hat and v is large when v is implausible and, uh, distribution p-hat.",
    "start": "555570",
    "end": "566279"
  },
  {
    "text": "And it's small when v is likely under distribution p-hat. Uh, and so people have names for this kind of thing,",
    "start": "566280",
    "end": "574250"
  },
  {
    "text": "people might call it surprise or perplexity.",
    "start": "574250",
    "end": "577710"
  },
  {
    "start": "584000",
    "end": "959000"
  },
  {
    "text": "Now, um, we would like to be able to do,",
    "start": "588270",
    "end": "596095"
  },
  {
    "text": "um, predictors, to use predictors which generate vectors, right?",
    "start": "596095",
    "end": "602290"
  },
  {
    "text": "So we've certainly seen that if we've got a tree, then we can simply label the leaves of the tree with probabilities,",
    "start": "602290",
    "end": "613180"
  },
  {
    "text": "probability distributions, and that's enough, we can simply evaluate the output of that predictor in the- uh,",
    "start": "613180",
    "end": "620560"
  },
  {
    "text": "uh, in the loss function.",
    "start": "620560",
    "end": "626515"
  },
  {
    "text": "Um, uh, however, if we've got,",
    "start": "626515",
    "end": "633070"
  },
  {
    "text": "say, a linear predictor, um, or, uh, uh,",
    "start": "633070",
    "end": "641950"
  },
  {
    "text": "um, certain other types of predictors, then the predictor produces a- a vector.",
    "start": "641950",
    "end": "649345"
  },
  {
    "text": "And, um, of course, we can't just use that vector as a probability distribution,",
    "start": "649345",
    "end": "656334"
  },
  {
    "text": "it might not be non-negative, it might not sum up to 1. Um, and so, um, we need a way around that.",
    "start": "656335",
    "end": "665154"
  },
  {
    "text": "If we were doing point classification, what we would do is we would unembed a vector y hat in R_K,",
    "start": "665155",
    "end": "671139"
  },
  {
    "text": "um, by, uh, just, uh, picking the corresponding target to be v_i where i is",
    "start": "671140",
    "end": "682060"
  },
  {
    "text": "the index of the representative Psi i which is the closest of the representatives to y hat.",
    "start": "682060",
    "end": "692695"
  },
  {
    "text": "Um, and that is great, it maps the vector y hat in R_K into the target set script v. Um,",
    "start": "692695",
    "end": "704080"
  },
  {
    "text": "that's not quite what we want to do here, we want to unembed and map y hat into",
    "start": "704080",
    "end": "713245"
  },
  {
    "text": "a probability distribution on the target set script V. Um,",
    "start": "713245",
    "end": "721180"
  },
  {
    "text": "and there's a- a- a way of doing this which is very common, and that's the following on embedding,",
    "start": "721180",
    "end": "727149"
  },
  {
    "text": "it's called the logistic map or the soft-argmax function.",
    "start": "727150",
    "end": "732745"
  },
  {
    "text": "Um, and what we do is we define p hat of v_k to be the",
    "start": "732745",
    "end": "739765"
  },
  {
    "text": "exponential of y hat k divided by the sum of the exponentials of y hat j. Um,",
    "start": "739765",
    "end": "750010"
  },
  {
    "text": "and this is a map which for each k gives us p hat of v_k,",
    "start": "750010",
    "end": "756085"
  },
  {
    "text": "it maps a vector y hat in R_K to a probability distribution.",
    "start": "756085",
    "end": "761290"
  },
  {
    "text": "And you can see that if you sum from k is 1 up to capital K of p hat v_k, you get 1.",
    "start": "761290",
    "end": "769045"
  },
  {
    "text": "And you can see that because it's the ratio of an exponential and the sum of exponentials,",
    "start": "769045",
    "end": "774144"
  },
  {
    "text": "it's a non-negative number. So it certainly satisfies the two fundamental requirements for a probability distribution.",
    "start": "774145",
    "end": "783370"
  },
  {
    "text": "And this map Sigma, it has lots of names.",
    "start": "783370",
    "end": "789115"
  },
  {
    "text": "I guess the community cannot agree as to what the right thing to think of it, right way to name it is,",
    "start": "789115",
    "end": "795010"
  },
  {
    "text": "and people call it the logistic map, people call it an activation function, and they use it as an activation function in the neural network.",
    "start": "795010",
    "end": "803710"
  },
  {
    "text": "People call it the inverse link function, the softarg max function,",
    "start": "803710",
    "end": "808975"
  },
  {
    "text": "the normalized exponential, or the softmax function.",
    "start": "808975",
    "end": "813800"
  },
  {
    "text": "Um, let- let's take a look at it slightly more closely.",
    "start": "815340",
    "end": "823255"
  },
  {
    "text": "Um, so, uh, what does it do? Well, the exponential is mapping the real line to the non-negative half of the real line.",
    "start": "823255",
    "end": "836379"
  },
  {
    "text": "And so, um, what the exponentials are doing is we're placing all of the entries,",
    "start": "836380",
    "end": "842170"
  },
  {
    "text": "all the components of y hat with non-negative numbers.",
    "start": "842170",
    "end": "848440"
  },
  {
    "text": "And then the division, all that's doing is normalizing it,",
    "start": "848440",
    "end": "853450"
  },
  {
    "text": "it's arranging for the components of p hat to sum up to 1.",
    "start": "853450",
    "end": "860695"
  },
  {
    "text": "Um, and so we can see that we are getting a probability distribution.",
    "start": "860695",
    "end": "866724"
  },
  {
    "text": "Uh, you can also see that, for example, if you take y hat, your predicted output of your predictor,",
    "start": "866724",
    "end": "876445"
  },
  {
    "text": "and add a constant to each entry, but then it doesn't affect p hat.",
    "start": "876445",
    "end": "882290"
  },
  {
    "text": "Um, you can also see that if you increase one of the components, say k, uh, if you can increase y hat k,",
    "start": "882360",
    "end": "890274"
  },
  {
    "text": "then that will increase the probability at p hat v_k and decrease all the others",
    "start": "890275",
    "end": "897730"
  },
  {
    "text": "because the others have to be chosen to sum up to 1. And so it is, um,",
    "start": "897730",
    "end": "903460"
  },
  {
    "text": "larger y hats correspond to larger probabilities.",
    "start": "903460",
    "end": "908420"
  },
  {
    "text": "Um, but it can never equal either 0 or 1. Um, uh, p hat v_k is close to 0 when y hat k is much smaller than all the others.",
    "start": "908550",
    "end": "923334"
  },
  {
    "text": "And p hat v_k is close to 1 when y hat k is much larger than all the other components of y hat.",
    "start": "923335",
    "end": "930530"
  },
  {
    "text": "There's one more special case which is when y hat is 0. And when y hat is 0, well then,",
    "start": "931860",
    "end": "937990"
  },
  {
    "text": "you end up with p hat of v_k is the uniform distribution, y over k. And in fact, if y hat is, uh,",
    "start": "937990",
    "end": "946360"
  },
  {
    "text": "a vector with all of its entries equal, then you'll also end up with the uniform distribution.",
    "start": "946360",
    "end": "954050"
  },
  {
    "start": "959000",
    "end": "1124000"
  },
  {
    "text": "So now, we can do ERM with logistic un-embedding.",
    "start": "959580",
    "end": "965080"
  },
  {
    "text": "Um, um, let's just compare with what",
    "start": "965080",
    "end": "970900"
  },
  {
    "text": "we would have done in the deterministic case.",
    "start": "970900",
    "end": "976375"
  },
  {
    "text": "In deterministic classification, we take our u's and our v's, we embed them as x's and y's,",
    "start": "976375",
    "end": "982433"
  },
  {
    "text": "and then we use ERM to minimize, uh, the loss evaluated at g of x_i and y_i,",
    "start": "982434",
    "end": "992275"
  },
  {
    "text": "the average loss over all of the i's. And then we get the resulting predictor,",
    "start": "992275",
    "end": "998380"
  },
  {
    "text": "um, uh, by composing it with the embedding and the unembedding. So you give me a, uh,",
    "start": "998380",
    "end": "1005279"
  },
  {
    "text": "particular u, this should say u, so this should say v hat is",
    "start": "1005280",
    "end": "1012885"
  },
  {
    "text": "Psi dagger g Theta Phi of u.",
    "start": "1012885",
    "end": "1021090"
  },
  {
    "text": "Uh, then I- I compose, I take u,",
    "start": "1021090",
    "end": "1026670"
  },
  {
    "text": "I apply the embedding trick to Phi of u, I apply that predictor g Theta to that, and then I unembed,",
    "start": "1026670",
    "end": "1032579"
  },
  {
    "text": "uh, the resulting output of G Theta to give me a v hat. Uh, and same here, this quantity here should be a",
    "start": "1032580",
    "end": "1040170"
  },
  {
    "text": "u. Um, for probabilistic classification, we don't embed v,",
    "start": "1040170",
    "end": "1047865"
  },
  {
    "text": "we embed, uh, uh, u using Phi.",
    "start": "1047865",
    "end": "1052965"
  },
  {
    "text": "And, uh- and then what we do is we minimize the average loss- the average cross entropy loss.",
    "start": "1052965",
    "end": "1063060"
  },
  {
    "text": "And here are the two entries in the cross entropy loss. One of them is simply v, the true v, and the other one is the unembed value of the output of the predictor.",
    "start": "1063060",
    "end": "1075870"
  },
  {
    "text": "So g Theta of x_i is going to produce a y hat, and Sigma of g Theta of x_i is going to produce a p hat.",
    "start": "1075870",
    "end": "1085860"
  },
  {
    "text": "And so the resulting predictor is given by, uh, uh, Sigma composed with g Theta,",
    "start": "1085860",
    "end": "1093390"
  },
  {
    "text": "composed with Phi of u. So the role of the unembedding here is a little different.",
    "start": "1093390",
    "end": "1103260"
  },
  {
    "text": "All right, the unembedding lives inside the loss function in probabilistic classification.",
    "start": "1103260",
    "end": "1110330"
  },
  {
    "text": "And its role is to take those vectors that come out of our linear predictor,",
    "start": "1110330",
    "end": "1116149"
  },
  {
    "text": "g Theta, and turn them into probability distributions.",
    "start": "1116150",
    "end": "1120539"
  },
  {
    "start": "1124000",
    "end": "1304000"
  },
  {
    "text": "Let's take a closer look at our loss function. Um, let's suppose that, uh,",
    "start": "1125440",
    "end": "1132410"
  },
  {
    "text": "we're unembeddings in the logistic map, the logistic unembedding.",
    "start": "1132410",
    "end": "1137510"
  },
  {
    "text": "Where- so we got a predictor that producers a y-hat and we pump it through the function Sigma to get a probability distribution p-hat.",
    "start": "1137510",
    "end": "1148054"
  },
  {
    "text": "And now let's look at the cross-entropy loss and say we've got such a p-hat,",
    "start": "1148055",
    "end": "1154100"
  },
  {
    "text": "and let's evaluate what the cross-entropy loss at p-hat v_k is.",
    "start": "1154100",
    "end": "1160610"
  },
  {
    "text": "So we've got a particular v, say, v_k, that we're evaluating at.",
    "start": "1160610",
    "end": "1165815"
  },
  {
    "text": "So what is that? Well, the cross-entropy loss just says evaluate",
    "start": "1165815",
    "end": "1171740"
  },
  {
    "text": "the probability distribution p hat at the point v_k.",
    "start": "1171740",
    "end": "1177140"
  },
  {
    "text": "So it's just the Kth entry of that probability distribution. That's just the negative log of, uh,",
    "start": "1177140",
    "end": "1184409"
  },
  {
    "text": "the exponential of y-hat k divided by the sum of the exponentials of all the entries.",
    "start": "1184570",
    "end": "1191000"
  },
  {
    "text": "And that's the cross-entropy loss. We could simplify that slightly. Um, it's going to be minus y-hat k plus the log of the sum in the denominator.",
    "start": "1191000",
    "end": "1203495"
  },
  {
    "text": "And this is an expression that we've seen before.",
    "start": "1203495",
    "end": "1208650"
  },
  {
    "text": "This is simply the logistic loss when k_i is 1. And so the logistic loss,",
    "start": "1210670",
    "end": "1219754"
  },
  {
    "text": "which we used for deterministic classification,",
    "start": "1219754",
    "end": "1226410"
  },
  {
    "text": "is exactly the same as the cross-entropy loss when you use the logistic unembedding.",
    "start": "1226870",
    "end": "1236405"
  },
  {
    "text": "And so if we're computing one, we're computing the other, since we're doing exactly the same thing.",
    "start": "1236405",
    "end": "1242795"
  },
  {
    "text": "And so when we look at the empirical risk that we are using, uh,",
    "start": "1242795",
    "end": "1249590"
  },
  {
    "text": "for, uh, logistic regression in the deterministic case, that's exactly the average negative log likelihood.",
    "start": "1249590",
    "end": "1258500"
  },
  {
    "text": "And we can simply use, uh, the y-hats that come out of logistic regression,",
    "start": "1258500",
    "end": "1265099"
  },
  {
    "text": "those predictions are vectors. But instead of unembedding them using the nearest neighbor map,",
    "start": "1265099",
    "end": "1271775"
  },
  {
    "text": "we unembed them using the logistic unembedding, p-hat is Sigma y-hat.",
    "start": "1271775",
    "end": "1278585"
  },
  {
    "text": "And that way, instead of getting a deterministic classifier,",
    "start": "1278585",
    "end": "1283760"
  },
  {
    "text": "we get a probabilistic classifier. So that's nice. It says that, in some sense,",
    "start": "1283760",
    "end": "1291320"
  },
  {
    "text": "what we've been doing when we were doing logistic regression is actually computing a probabilistic classifier all along.",
    "start": "1291320",
    "end": "1299390"
  },
  {
    "text": "And now we know how to compute the probabilities. Um, now,",
    "start": "1299390",
    "end": "1310880"
  },
  {
    "start": "1304000",
    "end": "1407000"
  },
  {
    "text": "we can - we can interpret this. If we, uh, uh, look at what p-hat is,",
    "start": "1310880",
    "end": "1317600"
  },
  {
    "text": "it's, um- well, it's Sigma y-hat. And let's use this in vector notation.",
    "start": "1317600",
    "end": "1324050"
  },
  {
    "text": "X of y-hat here means the element-wise exponential of the vector y-hat,",
    "start": "1324050",
    "end": "1329810"
  },
  {
    "text": "and 1 transpose is doing the sum for us. 1 transpose x by y-hat is summing over all of the exponentials.",
    "start": "1329810",
    "end": "1337279"
  },
  {
    "text": "And y-hat here is Theta transpose x when we're doing logistic regression and we're using a linear predictor.",
    "start": "1337280",
    "end": "1344435"
  },
  {
    "text": "The next one is one is the constant feature, and we're standardizing all the other fea- features.",
    "start": "1344435",
    "end": "1351960"
  },
  {
    "text": "Um, and that means that the first raw of Theta, Theta 1 transpose,",
    "start": "1352270",
    "end": "1357560"
  },
  {
    "text": "is y-hat when x2- 2d is 0.",
    "start": "1357560",
    "end": "1363485"
  },
  {
    "text": "So all of the non-constant features are taking their mean value.",
    "start": "1363485",
    "end": "1369755"
  },
  {
    "text": "And the corresponding distribution is Sigma of Theta 1,",
    "start": "1369755",
    "end": "1375110"
  },
  {
    "text": "um, then Theta_ij gives the effect of x_i on p-hat_j.",
    "start": "1375110",
    "end": "1385520"
  },
  {
    "text": "And so that tells us in particular that if we, uh,",
    "start": "1385520",
    "end": "1390665"
  },
  {
    "text": "have a very large component of Theta_ij, then x_i is going to be a significant effect on the probability distribution p-hat_j.",
    "start": "1390665",
    "end": "1404160"
  },
  {
    "text": "Now, we can also do exactly the same for Boolean classification.",
    "start": "1409600",
    "end": "1415025"
  },
  {
    "text": "We can do Boolean probabilistic classification and we could just use the methods we've seen so far.",
    "start": "1415025",
    "end": "1421455"
  },
  {
    "text": "Um, uh, but there's one special wrinkle that people like to do for Boolean classification,",
    "start": "1421455",
    "end": "1427205"
  },
  {
    "text": "um, and that is, take advantage of the fact that, when you've got a probability distribution over two quantities,",
    "start": "1427205",
    "end": "1433745"
  },
  {
    "text": "you only actually need to specify that by one number rather than by two numbers, because the other one has to be 1 minus it.",
    "start": "1433745",
    "end": "1441170"
  },
  {
    "text": "So if v is- the target set is v_1, v_2, um, we're gonna guess p-hat is g of u. Um,",
    "start": "1441170",
    "end": "1450049"
  },
  {
    "text": "er, instead of, uh, just giving p-hat comp- uh, compound of v_1 and p-hat of v_2,",
    "start": "1450050",
    "end": "1457535"
  },
  {
    "text": "we can just give one of them since they must sum up to 1. And so we might give p-hat of v_2,",
    "start": "1457535",
    "end": "1466460"
  },
  {
    "text": "which is the probability that v is v_2, and just define p-hat of v_1 as 1 minus p-hat of v_2.",
    "start": "1466460",
    "end": "1473460"
  },
  {
    "text": "Now, if we're only going to, uh, need one number, one probability,",
    "start": "1478030",
    "end": "1485660"
  },
  {
    "text": "then we only need one component of y-hat. So if we're using, uh,",
    "start": "1485660",
    "end": "1491450"
  },
  {
    "text": "a predictor which is generating a vector in R k, but we're reducing ourselves to,",
    "start": "1491450",
    "end": "1497870"
  },
  {
    "text": "instead of needing two probabilities, needing one probability, we can just pick k is equal to 1 or m is equal to 1.",
    "start": "1497870",
    "end": "1505715"
  },
  {
    "text": "And then we've got a scalar produced by our predictor, a y-hat, and we want to generate,",
    "start": "1505715",
    "end": "1512210"
  },
  {
    "text": "uh, a probability out of that scalar. So that y-hat could be any real number.",
    "start": "1512210",
    "end": "1518630"
  },
  {
    "text": "What do we do? We take 1 on1 plus e to the minus y-hat.",
    "start": "1518630",
    "end": "1524045"
  },
  {
    "text": "This is also called the sigmoid function, and we will denote this by also Sigma.",
    "start": "1524045",
    "end": "1531140"
  },
  {
    "text": "And it's giving us a number which is between 0 and 1. It's, uh, uh, 0 as y-hat gets large and negative,",
    "start": "1531140",
    "end": "1543634"
  },
  {
    "text": "and it becomes 1 as y-hat becomes large and positive. Um, and so we're saying, okay, well,",
    "start": "1543634",
    "end": "1552409"
  },
  {
    "text": "now we take y-hat, we take Sigma of y-hat, that gives us a number between 0 and 1,",
    "start": "1552410",
    "end": "1557600"
  },
  {
    "text": "which we'll use as our probable- prediction probability for target value v_1.",
    "start": "1557600",
    "end": "1567390"
  },
  {
    "text": "And to get the prediction probability of target value v_2, we'll just take 1 minus Sigma of y-hat.",
    "start": "1567400",
    "end": "1574020"
  },
  {
    "text": "And so we've mapped a real number to a distribution on v, which is exactly what we needed to do.",
    "start": "1575170",
    "end": "1581195"
  },
  {
    "text": "Um, and so this is called a sigmoid function. Um, the inverse map,",
    "start": "1581195",
    "end": "1586895"
  },
  {
    "text": "which allows you to map from the probabilities back to the y-hat, we don't need to do that for,",
    "start": "1586895",
    "end": "1593480"
  },
  {
    "text": "er, classification, but that- it has a name. It's called a log-odds- log-odds or the logit function.",
    "start": "1593480",
    "end": "1601140"
  },
  {
    "text": "And here's a- a plot of the sigmoid. You can see it goes between 0 at minus infinity and to 1 at- plus infinity,",
    "start": "1602050",
    "end": "1611959"
  },
  {
    "text": "and it's half when y is 0. You can also see the symmetry. Um, that symmetry is that if I take it and rotate it about this,",
    "start": "1611959",
    "end": "1620870"
  },
  {
    "text": "uh, center point right here, then, uh, I get the same function.",
    "start": "1620870",
    "end": "1626120"
  },
  {
    "text": "And that's expressed neatly here, that Sigma of minus y-hat is 1 minus Sigma of y-hat.",
    "start": "1626120",
    "end": "1635160"
  },
  {
    "text": "So let's compute exactly as we did in the multiclass case.",
    "start": "1640160",
    "end": "1645585"
  },
  {
    "text": "Let's compute what happens if you use the sigmoid unembedding and you evaluate the cross-entropy loss.",
    "start": "1645585",
    "end": "1653730"
  },
  {
    "text": "So we've got the cross-entropy loss is the negative log of the probability at v_i.",
    "start": "1653730",
    "end": "1661880"
  },
  {
    "text": "And well, we know what Sigma of y hat i is.",
    "start": "1661880",
    "end": "1667775"
  },
  {
    "text": "If, uh, uh, uh, and, um, we need to, um,",
    "start": "1667775",
    "end": "1677890"
  },
  {
    "text": "it's either going to get- it's gonna give us two numbers as our distribution,",
    "start": "1677890",
    "end": "1682965"
  },
  {
    "text": "it'll give us Sigma y hat i, um, for the probability that v is v_1,",
    "start": "1682965",
    "end": "1691679"
  },
  {
    "text": "and it'll give us one minus Sigma y hat i for the probability that v is v_2.",
    "start": "1691680",
    "end": "1697830"
  },
  {
    "text": "And so we're- when we're computing the cross-entropy loss, we need to evaluate this probability at v_i,",
    "start": "1697830",
    "end": "1704490"
  },
  {
    "text": "and so if v_i is v_1, well then we get the first case,",
    "start": "1704490",
    "end": "1709740"
  },
  {
    "text": "minus log Sigma y hat i. If v_i is v_2,",
    "start": "1709740",
    "end": "1714990"
  },
  {
    "text": "then we get the second case. And now, we go through and do the algebra, and we see, well,",
    "start": "1714990",
    "end": "1721170"
  },
  {
    "text": "log of Sigma is- uh, sorry, negative log of Sigma is log of 1 plus e to the minus y hat,",
    "start": "1721170",
    "end": "1729240"
  },
  {
    "text": "and this is also something that we've seen before. This is the Boolean logistic loss.",
    "start": "1729240",
    "end": "1739065"
  },
  {
    "text": "And so when we were doing Boolean classification in the deterministic case,",
    "start": "1739065",
    "end": "1744914"
  },
  {
    "text": "we were also behind the scenes doing probabilistic classification.",
    "start": "1744915",
    "end": "1750855"
  },
  {
    "text": "And the way one gets the probabilities is by taking the y hat that comes out of the predictor and feeding it into the sigmoid function.",
    "start": "1750855",
    "end": "1760390"
  },
  {
    "start": "1763000",
    "end": "2138000"
  },
  {
    "text": "And this is the corresponding empirical risk minimization problem.",
    "start": "1765320",
    "end": "1770715"
  },
  {
    "text": "One on n times the average of- sorry, the loss, the empirical risk is the average of the cross-entropy loss,",
    "start": "1770715",
    "end": "1778544"
  },
  {
    "text": "and the cross-entropy loss splits into two cases, the cases where v_i is v_1 and the cases where v_i is v_2.",
    "start": "1778545",
    "end": "1787935"
  },
  {
    "text": "And we choose Theta to minimize the empirical risk as before. And once we've got Theta,",
    "start": "1787935",
    "end": "1793500"
  },
  {
    "text": "Theta transpose x is y hat, and then Sigma Theta transpose x is the probability that v is v_1 at x,",
    "start": "1793500",
    "end": "1802050"
  },
  {
    "text": "and 1 minus Sigma Theta transpose x is the probability that v is v_2 at x.",
    "start": "1802050",
    "end": "1808150"
  },
  {
    "text": "And this is simply expressing the same thing in a more convenient notation,",
    "start": "1814430",
    "end": "1821100"
  },
  {
    "text": "um, where we've used the vector notation here.",
    "start": "1821100",
    "end": "1825400"
  },
  {
    "text": "Let's look at a couple of examples. Here, we have, um,",
    "start": "1829430",
    "end": "1835395"
  },
  {
    "text": "a two class problem, three different two class problems,",
    "start": "1835395",
    "end": "1840764"
  },
  {
    "text": "we've done empirical risk minimization. And here, we've plotted the data points,",
    "start": "1840764",
    "end": "1847725"
  },
  {
    "text": "the red and the blue data points, there's no test or train set here, we're just using one dataset, and there's no regularization.",
    "start": "1847725",
    "end": "1855330"
  },
  {
    "text": "And we've plotted here in color, in the shading, the corresponding probability distribution,",
    "start": "1855330",
    "end": "1865605"
  },
  {
    "text": "Sigma of Theta transpose x. And you can see that it does what we expect it to do.",
    "start": "1865605",
    "end": "1873075"
  },
  {
    "text": "In the middle, here, along this line, this uncertainty and the probability is half.",
    "start": "1873075",
    "end": "1881865"
  },
  {
    "text": "As we move this way, the probability of red becomes 1. As we move this way,",
    "start": "1881865",
    "end": "1887760"
  },
  {
    "text": "the probability of red becomes 0 or equivalently, the probability of blue becomes 1.",
    "start": "1887760",
    "end": "1893054"
  },
  {
    "text": "Um, you can also see that here, in this example in the middle,",
    "start": "1893055",
    "end": "1899235"
  },
  {
    "text": "we've got data points where the blue and the red overlap considerably.",
    "start": "1899235",
    "end": "1905145"
  },
  {
    "text": "And as a result, the distribution goes quite smoothly and",
    "start": "1905145",
    "end": "1913710"
  },
  {
    "text": "slowly between being quite certain over here that it's red and being quite certain over here that it's blue,",
    "start": "1913710",
    "end": "1922410"
  },
  {
    "text": "and then probabilities that are somewhere between 0 and 1 in this middle region.",
    "start": "1922410",
    "end": "1930010"
  },
  {
    "text": "And there's this wide band of uncertainty where the predictor cannot make a very definite prediction as",
    "start": "1930080",
    "end": "1938430"
  },
  {
    "text": "to whether the target variable is going to be red or blue.",
    "start": "1938430",
    "end": "1946620"
  },
  {
    "text": "Here's a case where it's much more separated and, um, there is a narrow band of uncertainty and then very definite blue,",
    "start": "1946620",
    "end": "1956174"
  },
  {
    "text": "very definite predictions of red. And this is the advantage of using a probabilistic predictor",
    "start": "1956175",
    "end": "1965265"
  },
  {
    "text": "over deterministic predictor is that the probabilistic predictor,",
    "start": "1965265",
    "end": "1971430"
  },
  {
    "text": "when it is giving you the prediction of probability which is close to a half, well then, you can say, \"Well,",
    "start": "1971430",
    "end": "1977280"
  },
  {
    "text": "we're in some region where we can't make very certain predictions,\" and we're on the boundary and the reality is it could be either a blue or a red point.",
    "start": "1977280",
    "end": "1986340"
  },
  {
    "text": "You can also see this in the matrix in the vector Theta.",
    "start": "1986340",
    "end": "1992520"
  },
  {
    "text": "So here, the norm of Theta is rather small, and here, the norm of Theta is much larger.",
    "start": "1992520",
    "end": "1998850"
  },
  {
    "text": "When the norm of Theta is large, well then, Theta transpose x varies rapidly with x,",
    "start": "1998850",
    "end": "2006530"
  },
  {
    "text": "and so Sigma of Theta transpose x varies rapidly with x, and the probability changes quickly as we move in this direction.",
    "start": "2006530",
    "end": "2016260"
  },
  {
    "text": "Over here, because theta has a small norm, then the probability changes slowly as we move in the corresponding direction.",
    "start": "2016570",
    "end": "2026280"
  },
  {
    "text": "Here's a three class case, this is the Iris dataset. And remember what we had where in this Iris dataset,",
    "start": "2032050",
    "end": "2042335"
  },
  {
    "text": "when we look at these particular two components of the data, we find that the red points are well separated from the green and the blues,",
    "start": "2042335",
    "end": "2050569"
  },
  {
    "text": "but the green and the blues overlap considerably, and it's hard to tell a green from a blue. Um, so on the left-hand plot,",
    "start": "2050570",
    "end": "2057725"
  },
  {
    "text": "we're seeing multiclass logistic regression, we're unembedding using the nearest neighbor unembedding,",
    "start": "2057725",
    "end": "2065705"
  },
  {
    "text": "and this is deterministic classification. And in the right-hand plot, we're unembedding using the logistic map in a Sigma of Theta transpose x.",
    "start": "2065705",
    "end": "2076820"
  },
  {
    "text": "And we're getting out of that a three-dimensional probability vector which gives us the probability of red,",
    "start": "2076820",
    "end": "2082849"
  },
  {
    "text": "the probability of green, and the probability of blue. And what's showing up here is this- is the level of certainty of prediction.",
    "start": "2082850",
    "end": "2093274"
  },
  {
    "text": "Over here, red is really very certain. Then when we transition across this line,",
    "start": "2093275",
    "end": "2102690"
  },
  {
    "text": "we end up in, uh, a region where there's significant uncertainty.",
    "start": "2103120",
    "end": "2109400"
  },
  {
    "text": "So over here, our prediction is 1, 0, 0 or close to that,",
    "start": "2109400",
    "end": "2118205"
  },
  {
    "text": "and over here, our prediction is 0, a half, a half,",
    "start": "2118205",
    "end": "2125645"
  },
  {
    "text": "and there's much more uncertainty. We know it's not red but we can't quite tell whether it's green or whether it's blue.",
    "start": "2125645",
    "end": "2133710"
  },
  {
    "text": "So let's summarize. We use the average log likelihood on the test data to judge our probabilistic classifier.",
    "start": "2139600",
    "end": "2150005"
  },
  {
    "text": "This is our performance metric and it happens to equal the empirical risk when we're using the cross-entropy loss.",
    "start": "2150005",
    "end": "2157470"
  },
  {
    "text": "And there's a loss- so there's a loss function which exactly equals to our preferred performance matrix.",
    "start": "2157470",
    "end": "2163110"
  },
  {
    "text": "Um, and once we've- if we're using a linear predictor, then we can unembed that prediction into a distribution using the logistic unembedding,",
    "start": "2163110",
    "end": "2174395"
  },
  {
    "text": "and it gives us probability distributions. And the deterministic methods that we've seen for",
    "start": "2174395",
    "end": "2181400"
  },
  {
    "text": "classification using either Boolean logistic loss or multiclass logistic loss",
    "start": "2181400",
    "end": "2187430"
  },
  {
    "text": "with the one-hot embedding are exactly the same ERM optimization",
    "start": "2187430",
    "end": "2195470"
  },
  {
    "text": "that we do when we do probabilistic classification.",
    "start": "2195470",
    "end": "2200555"
  },
  {
    "text": "But to construct the probabilities, instead of unembedding using the nearest neighbor map to get the deterministic prediction,",
    "start": "2200555",
    "end": "2208670"
  },
  {
    "text": "we unembed using Sigma to get the probabilistic prediction.",
    "start": "2208670",
    "end": "2213569"
  }
]