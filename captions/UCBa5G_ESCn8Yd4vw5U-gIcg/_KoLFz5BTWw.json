[
  {
    "start": "0",
    "end": "5570"
  },
  {
    "text": "Hi, everyone. Happy Wednesday. A couple of class reminders.",
    "start": "5570",
    "end": "10610"
  },
  {
    "text": "Homework 3 is due tonight. Optional homework\n4 is out today.",
    "start": "10610",
    "end": "16400"
  },
  {
    "text": "And also, hopefully\nyou've been getting started on your projects. The project milestone is due\nin two weeks on Wednesday.",
    "start": "16400",
    "end": "25250"
  },
  {
    "text": " Again, similar to\nthe proposals, we're going to be grading\nthem pretty lightly.",
    "start": "25250",
    "end": "31992"
  },
  {
    "text": "Really the goal is to\nbe able to give you feedback on your\nproject and, yeah, help",
    "start": "31992",
    "end": "37240"
  },
  {
    "text": "you keep on track in\nterms of making progress towards the end.",
    "start": "37240",
    "end": "42835"
  },
  {
    "text": " So for today, we're going to\nrecap quickly Monday's lecture",
    "start": "42835",
    "end": "49390"
  },
  {
    "text": "on the basics of meta-RL. Then we're going to talk a lot\nabout learning how to explore.",
    "start": "49390",
    "end": "55030"
  },
  {
    "text": "This will be first just talking\nabout end-to-end optimization of exploration strategies,\nand then talking",
    "start": "55030",
    "end": "61990"
  },
  {
    "text": "about some alternative\nstrategies that address some of the shortcomings\nof end-to-end optimization,",
    "start": "61990",
    "end": "67120"
  },
  {
    "text": "and then talking\nabout an approach that tries to get kind of\nthe best of both worlds.",
    "start": "67120",
    "end": "73930"
  },
  {
    "text": "Meta-RL and learning to explore\nis the focus of homework 4. And so, this lecture will\nbe probably super useful",
    "start": "73930",
    "end": "81490"
  },
  {
    "text": "for that homework. And then, the goals\nof the lecture are to understand the challenges\nof end-to-end optimization",
    "start": "81490",
    "end": "88630"
  },
  {
    "text": "of exploration, understand\nthe basics of using some alternative strategies, and\nthen also be able to understand",
    "start": "88630",
    "end": "95620"
  },
  {
    "text": "and implement the last approach\nthat decouples exploration and exploitation.",
    "start": "95620",
    "end": "101712"
  },
  {
    "text": "Cool. So to recap meta-RL and what\nwe talked about last time,",
    "start": "101712",
    "end": "107409"
  },
  {
    "text": "we want to be able to learn how\nto solve a maze, for example, or solve a new task quickly.",
    "start": "107410",
    "end": "113870"
  },
  {
    "text": "And the way that\nwe do this is we learn how to learn\nmany different mazes or many different\nmeta-training tasks.",
    "start": "113870",
    "end": "119799"
  },
  {
    "text": "And then, use this to try to\nquickly solve a new task where we explore in a new maze.",
    "start": "119800",
    "end": "126280"
  },
  {
    "text": "This exploration constitutes\nour training data set. And then we use\nthis to solve a task",
    "start": "126280",
    "end": "132250"
  },
  {
    "text": "and acquire a policy\nthat can go straight to the goal of the maze.",
    "start": "132250",
    "end": "137336"
  },
  {
    "text": "We also talked about the\nproblem of meta-RL kind of more abstractly,\nwhere we want",
    "start": "137336",
    "end": "142840"
  },
  {
    "text": "to be able to take some\nexperience in a task and use that to\nform a policy that",
    "start": "142840",
    "end": "150910"
  },
  {
    "text": "maps from states to actions\nthat can maximize reward for that given task.",
    "start": "150910",
    "end": "156820"
  },
  {
    "text": "And the way that\nwe do this is we collect the data\nset of data sets, one data set for each\ntask, or one replay buffer",
    "start": "156820",
    "end": "163420"
  },
  {
    "text": "for each task. And we use this to learn\nhow to quickly solve",
    "start": "163420",
    "end": "168610"
  },
  {
    "text": "one of the tasks in\nour task distribution so that we can hopefully\ngeneralize to a new task.",
    "start": "168610",
    "end": "174490"
  },
  {
    "text": "And so, there are two aspects\nof the meta-RL problem. One is picking a\ngood function f that",
    "start": "174490",
    "end": "181030"
  },
  {
    "text": "can learn how to\nadapt from data, and the other is\nfiguring out how",
    "start": "181030",
    "end": "186370"
  },
  {
    "text": "to actually go about collecting\nand exploring in the task. And the second part is\nessentially the problem",
    "start": "186370",
    "end": "193510"
  },
  {
    "text": "of learning how to explore. And this is exactly what we'll\nbe focusing on in this lecture.",
    "start": "193510",
    "end": "200420"
  },
  {
    "text": "We sort of swept this\nunder the rug on Monday. And there are actually\ncircumstances where exploration",
    "start": "200420",
    "end": "206290"
  },
  {
    "text": "is quite non-trivial,\nand you want to learn a more sophisticated\nexploration strategy where",
    "start": "206290",
    "end": "212728"
  },
  {
    "text": "you're actually optimizing\nfor the exploration strategy rather than something\nmore arbitrary.",
    "start": "212728",
    "end": "219190"
  },
  {
    "text": "Cool. And then, in today's\nlecture, we're going to be focusing on\nblack box meta-RL methods. Last time, we also talked about\noptimization-based meta-RL.",
    "start": "219190",
    "end": "226310"
  },
  {
    "text": "But here, we're\nreally going to be focusing on black box methods. And the way that we saw black\nbox methods working is you",
    "start": "226310",
    "end": "234950"
  },
  {
    "text": "have, for example, a\nrecurrent neural network that takes as input\nthe experience so far",
    "start": "234950",
    "end": "240200"
  },
  {
    "text": "and uses that to infer\nwhat a good action is for solving the task. And you can think of D\ntrain as essentially all",
    "start": "240200",
    "end": "247217"
  },
  {
    "text": "of your experience up until\nthe current time step. And your query set as\nthe current time step. And you're going\nto be optimizing",
    "start": "247217",
    "end": "252950"
  },
  {
    "text": "this network for many different\nsizes of support sets.",
    "start": "252950",
    "end": "259976"
  },
  {
    "text": "For the purpose of\nthis lecture, we're going to call our\ninitial episodes exploration episodes and\nour later episodes execution",
    "start": "259977",
    "end": "269420"
  },
  {
    "text": "episodes. They don't necessarily need\nto have a very hard breakdown",
    "start": "269420",
    "end": "275060"
  },
  {
    "text": "of one episode for exploration\nand one episode for execution. But it'll be helpful, just\nin terms of understanding--",
    "start": "275060",
    "end": "284060"
  },
  {
    "text": "it'll be helpful to\nkind of break these into two different\nparts of the problem for understanding some of\nthe challenges that come up",
    "start": "284060",
    "end": "289598"
  },
  {
    "text": "when you learn how to explore. Also, sometimes in the\nexploration episode,",
    "start": "289598",
    "end": "295190"
  },
  {
    "text": "sometimes you\nactually don't really care about how the\nreward that the policy is achieving during the\nexploration episode.",
    "start": "295190",
    "end": "300470"
  },
  {
    "text": "Oftentimes you only care about\nthat during the execution episode. Although in some\ncircumstances, you might also want your\nexploration episode",
    "start": "300470",
    "end": "307460"
  },
  {
    "text": "not to jump off a\ncliff, for example.  In terms of black\nbox approaches,",
    "start": "307460",
    "end": "313550"
  },
  {
    "text": "they're very general\nand expressive. There's a variety of design\nchoices in the architecture. But they can be\ndifficult to optimize,",
    "start": "313550",
    "end": "320300"
  },
  {
    "text": "like we talked a little\nbit about on Monday. We also talked about\nhow these methods will inherit the sample\nefficiency of their outer RL",
    "start": "320300",
    "end": "327650"
  },
  {
    "text": "optimizer. Cool. So that's a recap of\nsome of the concepts",
    "start": "327650",
    "end": "335669"
  },
  {
    "text": "that we covered on Monday. Now we're going to be talking\nabout learning how to explore.",
    "start": "335670",
    "end": "342660"
  },
  {
    "text": "And I'd like to start this by\ntaking a step back and thinking about reinforcement\nlearning in general.",
    "start": "342660",
    "end": "348750"
  },
  {
    "text": "And oftentimes, in\nreinforcement learning we'll use a number of different\nexploration strategies.",
    "start": "348750",
    "end": "355068"
  },
  {
    "text": "Maybe we will try to maximize\nthe entropy of our policy and do more random things. There's also\nexploration strategies",
    "start": "355068",
    "end": "360893"
  },
  {
    "text": "known as Epsilon-Greedy,\nwhere with some probability you take a random\naction, and with the rest",
    "start": "360893",
    "end": "366264"
  },
  {
    "text": "of your probability\nyou take the action according to your policy. And these exploration\nstrategies are very naive.",
    "start": "366265",
    "end": "374160"
  },
  {
    "text": "And we're actually using the\nsame exploration strategies for solving a wide\nrange of problems.",
    "start": "374160",
    "end": "380177"
  },
  {
    "text": "And if you take a step\nback, it seems a little bit silly to be thinking about\nusing the same exploration approach for learning\nto navigate environment",
    "start": "380177",
    "end": "388565"
  },
  {
    "text": "and the same\nexploration approach for making recommendations\nto users for learning a policy for computer\nsystem caching",
    "start": "388565",
    "end": "394770"
  },
  {
    "text": "or physically operating a robot. And essentially, in\nreinforcement learning, the way that we approach\nexploration is we",
    "start": "394770",
    "end": "401260"
  },
  {
    "text": "have a single strategy\nlike Epsilon-Greedy and we apply that to all\nof these kinds of problems. And that seems a\nlittle bit silly.",
    "start": "401260",
    "end": "407190"
  },
  {
    "text": "Because you would\nexpect that in a lot of these different\napplications, you",
    "start": "407190",
    "end": "412230"
  },
  {
    "text": "want strategies\nthat are targeting aspects of that problem.",
    "start": "412230",
    "end": "417850"
  },
  {
    "text": "So for example,\nin navigation, we want exploration\nstrategies that try to reach different parts\nof the environment.",
    "start": "417850",
    "end": "423000"
  },
  {
    "text": "In recommender\nsystems, we may want to have approaches\nthat are targeted",
    "start": "423000",
    "end": "428490"
  },
  {
    "text": "towards particular users\nand treat different users differently. In terms of physically\noperating a tool or a machine,",
    "start": "428490",
    "end": "435420"
  },
  {
    "text": "there are probably certain\nthings that we want to try and certain things that\nare irrelevant and won't be helpful.",
    "start": "435420",
    "end": "440580"
  },
  {
    "text": "Like walking to\nanother room probably won't be helpful in\nterms of exploration for that particular\ntool or machine.",
    "start": "440580",
    "end": "448560"
  },
  {
    "text": "And so, in some ways, the\nidea behind meta-reinforcement learning is trying to learn\nexploration strategies based",
    "start": "448560",
    "end": "456870"
  },
  {
    "text": "on other tasks in that\nparticular domain in order to be able to be\nmuch more targeted--",
    "start": "456870",
    "end": "464241"
  },
  {
    "text": "in order to explore in\na much more targeted way for a particular domain. ",
    "start": "464242",
    "end": "470439"
  },
  {
    "text": "Yeah. And so, if these algorithms\nare very effective, we may be able to get\nexploration strategies that are very targeted\nand that are more",
    "start": "470440",
    "end": "477330"
  },
  {
    "text": "domain-specific than\nthings like Epsilon-Greedy. ",
    "start": "477330",
    "end": "486070"
  },
  {
    "text": "OK, so in terms of the\nalgorithms that we'll think about, I want\nto think about them",
    "start": "486070",
    "end": "491200"
  },
  {
    "text": "in the context of an example. And I think that this\nshould help give across some of the intuition\nbehind the algorithms.",
    "start": "491200",
    "end": "496930"
  },
  {
    "text": "And the example that we'll\nconsider is as follows. So we have some agent.",
    "start": "496930",
    "end": "502330"
  },
  {
    "text": "This is what's choosing to take\nactions in the environment. And there are a number\nof different hallways",
    "start": "502330",
    "end": "508810"
  },
  {
    "text": "in the environment. And different tasks\ncorrespond to navigating to the end of one\nof the hallways",
    "start": "508810",
    "end": "515469"
  },
  {
    "text": "and receiving a reward at\nthe end of the hallway. And so, for one\ntask, maybe the goal",
    "start": "515470",
    "end": "521890"
  },
  {
    "text": "is to go to the end of hallway\ntwo and collect a reward. For another task, the\ngoal is to go to hallway k",
    "start": "521890",
    "end": "527410"
  },
  {
    "text": "and collect a\nreward, and so forth. So this is the basic\nversion of the setup.",
    "start": "527410",
    "end": "533843"
  },
  {
    "text": "And if you were to\nlearn how to explore, you would imagine that\nyou might want to explore the ends of different hallways.",
    "start": "533843",
    "end": "540160"
  },
  {
    "text": "We're going to add one more\nbit to this environment, which is that on the\nground there's also some paper that has instructions\non a hallway that is--",
    "start": "540160",
    "end": "549522"
  },
  {
    "text": "essentially where the robot\nshould go for the current task. ",
    "start": "549522",
    "end": "555170"
  },
  {
    "text": "Cool. And so, this is also\nkind of illustrated in a much more basic\nway right here as well.",
    "start": "555170",
    "end": "564270"
  },
  {
    "text": "Does anyone have\nthoughts on what some strategies are for\nexploring and learning",
    "start": "564270",
    "end": "569360"
  },
  {
    "text": "a new task? ",
    "start": "569360",
    "end": "576329"
  },
  {
    "text": "Yeah. [INAUDIBLE]  Yeah. So one thing that\nyou could do is you could try to find the\nsheet of paper on the ground,",
    "start": "576330",
    "end": "583572"
  },
  {
    "text": "read, it use that to\nfigure out where to go. And then, if you\ncan't effectively read what's on the\npaper, then you",
    "start": "583572",
    "end": "589589"
  },
  {
    "text": "could just go there directly. And then, another\nstrategy would be, if you don't know\nhow to read, you",
    "start": "589590",
    "end": "595830"
  },
  {
    "text": "could try to basically just\ngo down all of the hallways until you find the hallway\nthat gives you a reward. ",
    "start": "595830",
    "end": "602680"
  },
  {
    "text": "So these are two\neffective strategies. And those are, I think, the\nonly two effective strategies",
    "start": "602680",
    "end": "607800"
  },
  {
    "text": "for solving this problem. If you don't learn to go\nto the end of the hallways or you don't learn how to\nread, then you probably",
    "start": "607800",
    "end": "613860"
  },
  {
    "text": "won't be able to learn the task.  OK, so we'll be considering\nthis example throughout.",
    "start": "613860",
    "end": "620440"
  },
  {
    "text": "And it'll be useful\nfor understanding different approaches.",
    "start": "620440",
    "end": "625540"
  },
  {
    "text": "So now, the first approach\nfor learning how to explore is just to try to optimize\nfor exploration and task",
    "start": "625540",
    "end": "633550"
  },
  {
    "text": "execution end-to-end with\nrespect to the reward of solving the task.",
    "start": "633550",
    "end": "639040"
  },
  {
    "text": "This is essentially what we\nsaw in the lecture on Monday, where we trained a big\nrecurrent neural network",
    "start": "639040",
    "end": "644709"
  },
  {
    "text": "to be able to do\nboth exploration and execution for the task. And we could train\nit with something",
    "start": "644710",
    "end": "650080"
  },
  {
    "text": "like key learning or something\nlike policy gradients. If we train it with\npolicy gradients,",
    "start": "650080",
    "end": "655089"
  },
  {
    "text": "we'll get an objective that\nlooks something like this. And essentially, what this is\ngoing to be trying to doing is",
    "start": "655090",
    "end": "662410"
  },
  {
    "text": "your policy is going\nto need to both explore as well as use the\nexploration data in order",
    "start": "662410",
    "end": "667810"
  },
  {
    "text": "to solve the task. And so, if we think about\nthe meta-training process,",
    "start": "667810",
    "end": "674260"
  },
  {
    "text": "it's going to be collecting\nepisodes and trying to learn how to explore and solve tasks.",
    "start": "674260",
    "end": "679840"
  },
  {
    "text": "And so, say, during\nmeta-training, maybe it collects a- it's trying to\nlearn how to explore and solve",
    "start": "679840",
    "end": "688779"
  },
  {
    "text": "the task distribution. Maybe it collects\na trajectory that looks like this\nthat goes to the--",
    "start": "688780",
    "end": "695500"
  },
  {
    "text": "maybe even the correct\nhallway and gets a reward. An episode that looks like\nthis is great in the sense",
    "start": "695500",
    "end": "703150"
  },
  {
    "text": "that it's going to get positive\nreward for the current task. ",
    "start": "703150",
    "end": "708970"
  },
  {
    "text": "But it's not actually going to\nbe able to learn exploration from this trajectory\nbecause it hasn't--",
    "start": "708970",
    "end": "715510"
  },
  {
    "text": "D train isn't really\ngoing to say anything about what task is or\nisn't the case until it",
    "start": "715510",
    "end": "722680"
  },
  {
    "text": "gets to the very end. And so, as a result,\neven though it's",
    "start": "722680",
    "end": "728830"
  },
  {
    "text": "giving some positive\nreward for the task, it's going to be\nfairly difficult to use",
    "start": "728830",
    "end": "734620"
  },
  {
    "text": "a trajectory like this to\nfigure out how to both explore and solve the task. ",
    "start": "734620",
    "end": "741600"
  },
  {
    "text": "Now, another trajectory\nthat we might consider is maybe something\nlike this, where the agent goes to one\nend of one hallway",
    "start": "741600",
    "end": "749070"
  },
  {
    "text": "and then goes to this one. This trajectory, it will\nget positive reward again.",
    "start": "749070",
    "end": "757380"
  },
  {
    "text": "And it's actually also going\nto get positive reward for-- ",
    "start": "757380",
    "end": "763027"
  },
  {
    "text": "it's going to get\npositive reward that will help it learn and\nexplore some strategy of going to different hallways.",
    "start": "763027",
    "end": "768720"
  },
  {
    "text": "And so, in general,\nthis trajectory will probably be\nmuch more helpful than the green\ntrajectory, because it",
    "start": "768720",
    "end": "773802"
  },
  {
    "text": "can learn an exploration\nstrategy, which is to if you see a negative\nreward then you should go to a different hallway.",
    "start": "773802",
    "end": "779490"
  },
  {
    "text": "So this will be helpful. Although it's going to\nbe helpful for learning",
    "start": "779490",
    "end": "784770"
  },
  {
    "text": "a suboptimal strategy. Because the optimal\nstrategy would be to read the\ninstructions first and then go to the correct hallway.",
    "start": "784770",
    "end": "791550"
  },
  {
    "text": " And then, lastly, say that\nyou have a trajectory that",
    "start": "791550",
    "end": "796870"
  },
  {
    "text": "looks like this, where\nthe agent actually does go and reads the instructions. And then, maybe after it\nreads the instructions",
    "start": "796870",
    "end": "803410"
  },
  {
    "text": "it does some random stuff.  An episode like this\nis good in the sense",
    "start": "803410",
    "end": "810350"
  },
  {
    "text": "that it is actually showing\nsome exploratory behavior that's informative for the task.",
    "start": "810350",
    "end": "815430"
  },
  {
    "text": "However, it's not going to be\nable to use this experience in a meaningful way. Because it's still going\nto get a reward of zero",
    "start": "815430",
    "end": "821270"
  },
  {
    "text": "for this experience. And so, in this last case, it\nhas good exploratory behavior.",
    "start": "821270",
    "end": "827900"
  },
  {
    "text": "But that is not going\nto get any reward for this exploratory behavior. And as a result,\nthe only-- in terms",
    "start": "827900",
    "end": "834980"
  },
  {
    "text": "of learning the\noptimal strategy, the only kinds of\ntrajectories that will be helpful for learning\nis if it actually kind of first",
    "start": "834980",
    "end": "842240"
  },
  {
    "text": "goes to the instructions\nand then goes to the correct hallway. ",
    "start": "842240",
    "end": "850060"
  },
  {
    "text": "And so, this illustrates\nthe challenge of learning exploration and\nexploitation end-to-end.",
    "start": "850060",
    "end": "855253"
  },
  {
    "text": "Which is that if you're\ntrying to learn both of them at the same time, you\nreally need trajectories that do both exploration\nand execution of the task",
    "start": "855253",
    "end": "864080"
  },
  {
    "text": "in order to get good signal\nfor how to do exploration and execution together. And if you have a trajectory\nthat does only one of them,",
    "start": "864080",
    "end": "871940"
  },
  {
    "text": "that isn't going to\nhelp you very well to essentially learn\nhow to solve tasks",
    "start": "871940",
    "end": "877850"
  },
  {
    "text": "in your task distribution. Any questions about this?",
    "start": "877850",
    "end": "883500"
  },
  {
    "start": "883500",
    "end": "889640"
  },
  {
    "text": "Yeah. It might be silly,\nbut why can't you just reward the agent for\nlooking at the instruction regardless of what\nthey do after that?",
    "start": "889640",
    "end": "896520"
  },
  {
    "text": "Yeah. The question is,\ncan you just reward the agent for looking\nat the instructions regardless of what\nit does after that.",
    "start": "896520",
    "end": "903200"
  },
  {
    "text": "And that's something that\nyou can definitely do. If you know what a good\nexploration strategy is, then you could essentially\nchange the rewards",
    "start": "903200",
    "end": "910160"
  },
  {
    "text": "during meta-training to\nencourage it to kind of shape its exploration behavior. And that will encourage it.",
    "start": "910160",
    "end": "916100"
  },
  {
    "text": "That will make the\nproblem much easier, and that will encourage it\nto actually do that behavior.",
    "start": "916100",
    "end": "922574"
  },
  {
    "text": "Note that you do need\nto have some sort of domain-specific information\nin order to do that. And you need to know\nwhat a good exploration",
    "start": "922575",
    "end": "928190"
  },
  {
    "text": "strategy is in order to provide\nthat kind of reward shaping.",
    "start": "928190",
    "end": "933230"
  },
  {
    "text": "If you can do that, then that\nwill help solve the problem. If you don't know\nthat information, then you're still stuck with\nthe challenge of this sort",
    "start": "933230",
    "end": "943260"
  },
  {
    "text": "of end-to-end optimization. ",
    "start": "943260",
    "end": "949910"
  },
  {
    "text": "Cool. So to summarize kind of\nthe end-to-end approach",
    "start": "949910",
    "end": "955389"
  },
  {
    "text": "is very simple. And it in principle will\nlead to the optimal strategy if you give it enough\ndata, and it eventually",
    "start": "955390",
    "end": "962680"
  },
  {
    "text": "explores the correct exploration\nand execution strategy. The downside is that it's a\nvery challenging optimization",
    "start": "962680",
    "end": "970960"
  },
  {
    "text": "process, especially when\nexploration is difficult, because it needs to\nkind of figure out",
    "start": "970960",
    "end": "976912"
  },
  {
    "text": "that exploration\nstrategy in conjunction with solving the task. ",
    "start": "976913",
    "end": "984370"
  },
  {
    "text": "We can also look\nat another example of kind of a hard\nexploration meta-RL problem.",
    "start": "984370",
    "end": "990800"
  },
  {
    "text": "And we can see how this problem\narises in that problem as well. So say that we want our\nrobot to be able to cook us",
    "start": "990800",
    "end": "997630"
  },
  {
    "text": "a meal, which would be nice. And the meta-training\ntasks correspond to cooking",
    "start": "997630",
    "end": "1003630"
  },
  {
    "text": "in a number of\nprevious kitchens. And then, our goal\nis for the robot to be able to cook us a\nmeal in our kitchen, which",
    "start": "1003630",
    "end": "1010680"
  },
  {
    "text": "is a new kitchen.  In order to cook, you need to\nfind ingredients for the meal.",
    "start": "1010680",
    "end": "1019371"
  },
  {
    "text": "And then you also need\nto use those ingredients to make the meal. And so, you can\nthink of exploration",
    "start": "1019372",
    "end": "1025109"
  },
  {
    "text": "as finding the ingredients and\nexecution is actually using those ingredients to cook. And one of the things that's\ndifficult about end-to-end",
    "start": "1025109",
    "end": "1032550"
  },
  {
    "text": "optimization is you\nhave a chicken and egg problem between\nlearning how to explore and learning how\nto solve the task.",
    "start": "1032550",
    "end": "1039099"
  },
  {
    "text": "And if you say, for\nexample, you haven't yet learned how to explore. You don't know how\nto find ingredients.",
    "start": "1039099",
    "end": "1045900"
  },
  {
    "text": "Then if you don't\nhave ingredients, then you can't\nlearn how to cook. And likewise, if you haven't\nyet learned how to cook,",
    "start": "1045900",
    "end": "1054120"
  },
  {
    "text": "then you're not going\nto get any reward for finding the ingredients. And the second example\nis just like this case,",
    "start": "1054120",
    "end": "1060840"
  },
  {
    "text": "where if you haven't yet learned\nhow to read instructions and go to the correct goal,\nthen you're not going to get any reward for\nlooking at the instructions.",
    "start": "1060840",
    "end": "1067950"
  },
  {
    "text": " And so, essentially, you\nhave this coupling problem where learning how to explore\nand learning how to solve",
    "start": "1067950",
    "end": "1075580"
  },
  {
    "text": "tasks depend on one another. And if you try to optimize\nfor these end-to-end,",
    "start": "1075580",
    "end": "1083049"
  },
  {
    "text": "this can lead to\npoor local optimum like the approach of\nexploring different hallways.",
    "start": "1083050",
    "end": "1090602"
  },
  {
    "text": "It can also lead to very\npoor sample efficiency because you need a\nlot more data in order to happen upon a strategy that\nexplores and solves the task.",
    "start": "1090602",
    "end": "1098200"
  },
  {
    "text": " Cool. So that's the gist of why\nend-to-end training is hard.",
    "start": "1098200",
    "end": "1107235"
  },
  {
    "text": " Kind of in the rest\nof this lecture,",
    "start": "1107235",
    "end": "1112810"
  },
  {
    "text": "we'll talk about alternative\nstrategies that essentially try to bypass the difficulty of\nthis end-to-end optimization.",
    "start": "1112810",
    "end": "1124440"
  },
  {
    "text": "Do folks understand why\nend-to-end optimization is hard? Cool. I see nods.",
    "start": "1124440",
    "end": "1129895"
  },
  {
    "text": "OK.  So the second solution\nthat we will look at",
    "start": "1129895",
    "end": "1139220"
  },
  {
    "text": "is to try to leverage\nalternative exploration strategies. And instead of trying to\nexplicitly learn an exploration",
    "start": "1139220",
    "end": "1145909"
  },
  {
    "text": "strategy that is kind of\nthe best approach for trying",
    "start": "1145910",
    "end": "1151868"
  },
  {
    "text": "to figure out what\nthe task is, we're going to leverage alternative\nstrategies that might help us figure out what the task is.",
    "start": "1151868",
    "end": "1157040"
  },
  {
    "text": "They might be\nsuboptimal strategies. But they're going\nto be easier to use and optimize for than\nthe end-to-end approach.",
    "start": "1157040",
    "end": "1162860"
  },
  {
    "text": " Cool. So the first thing\nthat we'll consider",
    "start": "1162860",
    "end": "1167960"
  },
  {
    "text": "is to use posterior\nsampling, which is also known as Thompson sampling.",
    "start": "1167960",
    "end": "1174020"
  },
  {
    "text": "And at a high level, the way\nthat this is going to work is--",
    "start": "1174020",
    "end": "1179540"
  },
  {
    "text": "I'll erase these here. The way this is going\nto work is we're",
    "start": "1179540",
    "end": "1185060"
  },
  {
    "text": "going to learn how to\nsolve each of the tasks.",
    "start": "1185060",
    "end": "1191490"
  },
  {
    "text": "So we'll learn how to navigate\nto the end of hallways. And then, we're going to\nmaintain a belief about what",
    "start": "1191490",
    "end": "1198140"
  },
  {
    "text": "we think the task is and\nsample from kind of our belief or sample from our distribution\nof what we think the task is.",
    "start": "1198140",
    "end": "1205700"
  },
  {
    "text": "And so, what we\ncan do is, we can say that we have learned\nthat the tasks correspond",
    "start": "1205700",
    "end": "1211250"
  },
  {
    "text": "going to different hallways. Then we will start by kind\nof having a fairly uniform",
    "start": "1211250",
    "end": "1218120"
  },
  {
    "text": "prior over tasks. We'll sample from this\nto pick a particular task",
    "start": "1218120",
    "end": "1223610"
  },
  {
    "text": "that we might want to do. So maybe we'll sample\ntask two to start.",
    "start": "1223610",
    "end": "1228919"
  },
  {
    "text": "Then we will execute our\npolicy for that task, which will go to there.",
    "start": "1228920",
    "end": "1235490"
  },
  {
    "text": "We'll then see that we get-- we don't get a positive\nreward from this. Then we will update\nour kind of belief",
    "start": "1235490",
    "end": "1243620"
  },
  {
    "text": "over the task given\nthe experience that we've seen so far.",
    "start": "1243620",
    "end": "1249920"
  },
  {
    "text": "We'll sample a new task\nfrom our updated belief. This new distribution is\ngoing to be everything but this task right here.",
    "start": "1249920",
    "end": "1257150"
  },
  {
    "text": "And we'll then, again, execute-- pick a different task\nand execute for that.",
    "start": "1257150",
    "end": "1264150"
  },
  {
    "text": "So maybe we'll sample this task. And then we will get a\npositive reward here.",
    "start": "1264150",
    "end": "1270500"
  },
  {
    "text": "We'll then, again,\nupdate our belief over the task given the\nexperience we have so far.",
    "start": "1270500",
    "end": "1277800"
  },
  {
    "text": "And once we see a\npositive reward here, our distribution over\nwhat we think the task is is going to be much narrower.",
    "start": "1277800",
    "end": "1282920"
  },
  {
    "text": "We'll know that the task\nis to go to this hallway. And that we should kind of from\nthere on forward just always",
    "start": "1282920",
    "end": "1289490"
  },
  {
    "text": "go to this hallway. And so, this is referred to as\nposterior sampling in the sense",
    "start": "1289490",
    "end": "1295790"
  },
  {
    "text": "that you kind of maintain a\nkind of posterior distribution over what you think the task is. And then you act according\nto that distribution.",
    "start": "1295790",
    "end": "1306020"
  },
  {
    "text": "And so, more specifically\nwhat this would look like is, you first learn how to\nsolve and collect data",
    "start": "1306020",
    "end": "1312290"
  },
  {
    "text": "for all of the training tasks. This will be independent\nof learning how to explore.",
    "start": "1312290",
    "end": "1321320"
  },
  {
    "text": "Then you'll form a\nrepresentation of each task. This will be captured\nby this variable z.",
    "start": "1321320",
    "end": "1329270"
  },
  {
    "text": "And we'll learn a\npolicy conditioned on that representation\nof the task. ",
    "start": "1329270",
    "end": "1336279"
  },
  {
    "text": "And so, for example, this z,\nit could be a one hot vector of task 1, task 2, task 3. It could also be a more\ncontinuous representation.",
    "start": "1336280",
    "end": "1345549"
  },
  {
    "text": "And then, once we learn\nthis representation of task, we can then learn how\nto infer a distribution",
    "start": "1345550",
    "end": "1352899"
  },
  {
    "text": "over the current task. And so, we'll learn both kind\nof the marginal distribution over the task, which is p of\nz, as well as a distribution",
    "start": "1352900",
    "end": "1361570"
  },
  {
    "text": "q that is trying to\nunderstand the distribution of the task given the\nevidence I've seen so far.",
    "start": "1361570",
    "end": "1367660"
  },
  {
    "text": " And then, lastly, once we have\na policy conditioned on our task",
    "start": "1367660",
    "end": "1375570"
  },
  {
    "text": "representation as well\nas a kind of distribution over what we think\nthe task is, then we",
    "start": "1375570",
    "end": "1381920"
  },
  {
    "text": "can do this approach\nwhere we alternate between sampling from\nour current distribution over the task and sampling\nfrom the policy for that task.",
    "start": "1381920",
    "end": "1392202"
  },
  {
    "text": "And this is what's known\nas posterior sampling. Yeah. I'm wondering what-- step one\nseems like a really tall ask",
    "start": "1392202",
    "end": "1399810"
  },
  {
    "text": "of whatever model we have. To be able to solve a quadratic\nproblem for training tasks just seems pretty infeasible\nin most cases, right?",
    "start": "1399810",
    "end": "1411090"
  },
  {
    "text": "Yeah. So there's a few different ways\nthat you can approach that.",
    "start": "1411090",
    "end": "1416895"
  },
  {
    "text": "I guess I can talk concretely\nabout how we're essentially going to do task-- step one and two.",
    "start": "1416895",
    "end": "1421970"
  },
  {
    "text": "And in particular, I mean,\nin some ways, you have to--",
    "start": "1421970",
    "end": "1429840"
  },
  {
    "text": "if you aren't able to solve\nthe training tasks at all, then you're kind of out of luck. One of the things\nthat's nice is if you",
    "start": "1429840",
    "end": "1436350"
  },
  {
    "text": "do have a shaped\nreward for your tasks, then it will be easier\nto learn those tasks.",
    "start": "1436350",
    "end": "1443382"
  },
  {
    "text": "Whereas oftentimes you might\nnot have a shaped reward for the exploration strategy.",
    "start": "1443382",
    "end": "1449710"
  },
  {
    "text": "And so, the way that\nthis can work in practice is, you could\nessentially do this",
    "start": "1449710",
    "end": "1457679"
  },
  {
    "text": "with a kind of a particular\nform of a black box architecture, where you\nlearn a policy that is--",
    "start": "1457680",
    "end": "1465018"
  },
  {
    "text": "still takes the kind\nof standard form of conditioning on the current\nstate, and your training data, and outputs an action.",
    "start": "1465018",
    "end": "1471960"
  },
  {
    "text": "One thing that will make this\nstep easier is, in this case, we're going to use an\noff policy approach",
    "start": "1471960",
    "end": "1478980"
  },
  {
    "text": "to learning the training tasks. And so, and we'll\nmaintain a replay buffer,",
    "start": "1478980",
    "end": "1485070"
  },
  {
    "text": "a data set for\neach of our tasks. And once we do that and we\nsample D train from that replay",
    "start": "1485070",
    "end": "1492210"
  },
  {
    "text": "buffer, that buffer will\ncontain some trajectories that hopefully will eventually\nhave actually figured out--",
    "start": "1492210",
    "end": "1497460"
  },
  {
    "text": "or some trajectories that\ndo actually solve the task. And if you condition on that\ntrajectory in D train, then",
    "start": "1497460",
    "end": "1504360"
  },
  {
    "text": "it's pretty easy to\nidentify what the task is from that trajectory.",
    "start": "1504360",
    "end": "1510460"
  },
  {
    "text": "And so, I guess the short\nanswer to your question is that, you do need to be\nable to solve the training",
    "start": "1510460",
    "end": "1517470"
  },
  {
    "text": "tasks in some sense. And once you do solve\nthe training tasks,",
    "start": "1517470",
    "end": "1522930"
  },
  {
    "text": "it isn't too hard to learn\nan architecture like this. ",
    "start": "1522930",
    "end": "1534060"
  },
  {
    "text": "Cool. So in terms of these\nfirst two steps, we'll learn this\npolicy like this.",
    "start": "1534060",
    "end": "1539187"
  },
  {
    "text": "One thing that's important\nabout this architecture is that we're going to be\nhaving this intermediate",
    "start": "1539187",
    "end": "1544510"
  },
  {
    "text": "representation z that only takes\nas input the training data.",
    "start": "1544510",
    "end": "1550330"
  },
  {
    "text": "And as a result, this\nmeans that the zi is going to be capturing\ninformation about the task--",
    "start": "1550330",
    "end": "1556660"
  },
  {
    "text": "and information\nabout the task that's needed in order to learn\nthe policy in the critic.",
    "start": "1556660",
    "end": "1563470"
  },
  {
    "text": "Yeah. So essentially we\nhave to optimize [INAUDIBLE] most\nevidence in this case",
    "start": "1563470",
    "end": "1570235"
  },
  {
    "text": "is a trajectory for reward? ",
    "start": "1570235",
    "end": "1577536"
  },
  {
    "text": "You're maybe a little bit--\nmaybe one or two steps ahead. I guess, I won't actually be\nformulating an evidence lower",
    "start": "1577536",
    "end": "1583750"
  },
  {
    "text": "bound, although you can\nactually connect this to a variational approach. You can think about the\nevidence as maximizing reward.",
    "start": "1583750",
    "end": "1592780"
  },
  {
    "text": " I can maybe also\nget back to that after we finish kind of\nexplaining the details",
    "start": "1592780",
    "end": "1599858"
  },
  {
    "text": "of the last two steps.  OK, so now, we've gotten\nsteps one and two.",
    "start": "1599858",
    "end": "1611950"
  },
  {
    "text": "We have a representation\nof the task. And we also have a\ntask condition policy.",
    "start": "1611950",
    "end": "1618970"
  },
  {
    "text": "One thing we do not yet\nhave is this distribution over the task. And so, we need to essentially\nimpose a distribution somehow",
    "start": "1618970",
    "end": "1627129"
  },
  {
    "text": "on our task variable. And the way that we\ncan do this is actually",
    "start": "1627130",
    "end": "1635571"
  },
  {
    "text": "quite similar to\nsome of the things that we saw earlier in\nBayesian meta-learning. Where we instead of having a\nmodel deterministically output",
    "start": "1635572",
    "end": "1644140"
  },
  {
    "text": "z, it's going to output a\nmean and a variance over z. And you'll include a\nterm in your objective",
    "start": "1644140",
    "end": "1651640"
  },
  {
    "text": "that encourages that\nmean and variance to be-- essentially, the\ndistribution over that",
    "start": "1651640",
    "end": "1657909"
  },
  {
    "text": "variable to be a\nGaussian distribution. And so, you can do this\nagain with the KL divergence",
    "start": "1657910",
    "end": "1664270"
  },
  {
    "text": "term that encourages the\ndistribution over your task to follow a standard\nGaussian distribution.",
    "start": "1664270",
    "end": "1671500"
  },
  {
    "text": " And then, the complete\nobjective of what you're going to\nbe doing here is,",
    "start": "1671500",
    "end": "1678130"
  },
  {
    "text": "you'll have one model that takes\nas input D train and outputs",
    "start": "1678130",
    "end": "1687230"
  },
  {
    "text": "kind of a mean and a variance\nover your task variable.",
    "start": "1687230",
    "end": "1692809"
  },
  {
    "text": "So this will be kind of\nq of z given D train.",
    "start": "1692810",
    "end": "1698740"
  },
  {
    "text": "And you'll also\nhave another model that takes as input a particular\nsample z and outputs--",
    "start": "1698740",
    "end": "1708010"
  },
  {
    "text": "and gives you a policy\nfor that particular task. And the objective for\nlearning these two models",
    "start": "1708010",
    "end": "1717280"
  },
  {
    "text": "is to maximize reward\nacross all of the tasks and also to encourage\nthis to actually represent",
    "start": "1717280",
    "end": "1724539"
  },
  {
    "text": "a distribution rather than\nbeing a deterministic model. ",
    "start": "1724540",
    "end": "1731210"
  },
  {
    "text": "And this will give you-- yeah. This will give you essentially\nexactly what we wanted here,",
    "start": "1731210",
    "end": "1737340"
  },
  {
    "text": "which was a posterior\ndistribution over the task given the data\nset as well as a policy that",
    "start": "1737340",
    "end": "1742880"
  },
  {
    "text": "solves the task. Yeah. Is that problem\nwhy you have-- it gets penalized, like,\nit's further-- it",
    "start": "1742880",
    "end": "1749960"
  },
  {
    "text": "veers away from the normal? Yeah, exactly. So the KL divergence\nterm is going",
    "start": "1749960",
    "end": "1756057"
  },
  {
    "text": "to encourage-- is\ngoing to penalize it if it's further away from\nthe normal distribution. One of the things that's\nreally important about having",
    "start": "1756057",
    "end": "1762440"
  },
  {
    "text": "this term is that-- one thing this model could\ndo is, it could just set-- it's going to say-- it\ncould say that, oh, I just",
    "start": "1762440",
    "end": "1769370"
  },
  {
    "text": "want it to be deterministic,\nand set the noise to be zero, for example. If it does this, then\nthis is a problem.",
    "start": "1769370",
    "end": "1774410"
  },
  {
    "text": "Because then, when you try\nto go and sample a task, it's just going to give you\na single number rather than",
    "start": "1774410",
    "end": "1780020"
  },
  {
    "text": "a distribution over\nwhat the task might be.  It's also important to do\nsomething like this because we",
    "start": "1780020",
    "end": "1786260"
  },
  {
    "text": "also want to start with\nsome prior distribution over our tasks. And by regularizing the\ndistribution to this zero--",
    "start": "1786260",
    "end": "1794539"
  },
  {
    "text": "to this standard\nGaussian distribution, this prior distribution will\nnow just be a standard Gaussian",
    "start": "1794540",
    "end": "1800780"
  },
  {
    "text": "distribution. And so, when we don't have any\nevidence about the training data set, we can sample\nfrom the standard Gaussian.",
    "start": "1800780",
    "end": "1806550"
  },
  {
    "text": "And that will represent\na reasonable distribution over our tasks. ",
    "start": "1806550",
    "end": "1816789"
  },
  {
    "text": "You can also think\nabout this second term as what's known as an\ninformation bottleneck.",
    "start": "1816790",
    "end": "1823890"
  },
  {
    "text": "It's going to\nessentially encourage this task variable to carry less\ninformation about the training",
    "start": "1823890",
    "end": "1830370"
  },
  {
    "text": "data and only carry the\ninformation that's needed. ",
    "start": "1830370",
    "end": "1835530"
  },
  {
    "text": "But you can also\njust think about it as essentially just imposing\na distribution over your task variable rather than having\nit be a deterministic model.",
    "start": "1835530",
    "end": "1842865"
  },
  {
    "start": "1842865",
    "end": "1848480"
  },
  {
    "text": "Cool. And then, getting back to the\nevidence lower bound question,",
    "start": "1848480",
    "end": "1853540"
  },
  {
    "text": "I think that there is a way to\nformulate this as optimizing an evidence lower bound.",
    "start": "1853540",
    "end": "1859120"
  },
  {
    "text": "It looks a lot like a\nvariational autoencoder objective, as you might notice,\nwhere your z corresponds",
    "start": "1859120",
    "end": "1867760"
  },
  {
    "text": "to your latent variable. And then you're trying\nto maximize reward. So your evidence would be--",
    "start": "1867760",
    "end": "1874690"
  },
  {
    "text": "it's a little bit\nmore complicated to actually derive the\nevidence lower bound because you need to actually\nthink about reward--",
    "start": "1874690",
    "end": "1880480"
  },
  {
    "text": "maximizing reward as actually\nmaximizing likelihood in a graphical model. And there's actually\na lot more formalism if you want to try to do that.",
    "start": "1880480",
    "end": "1886660"
  },
  {
    "text": "So it gets fairly complicated. And it's kind ofbeyond the\nscope of what we'll cover today.",
    "start": "1886660",
    "end": "1893860"
  },
  {
    "text": "Yeah. [INAUDIBLE] this for a maze, for\nexample, as we've seen before,",
    "start": "1893860",
    "end": "1899009"
  },
  {
    "text": "is this assuming that we have\naccess to [INAUDIBLE] the best of all possible positions\nof the variable,",
    "start": "1899010",
    "end": "1904530"
  },
  {
    "text": "or is this open to\ngeneralize across positions in the [INAUDIBLE]? ",
    "start": "1904530",
    "end": "1912530"
  },
  {
    "text": "Yeah. So you're asking basically,\ndoes this assume access to kind of some low dimensional\nrepresentation of the task,",
    "start": "1912530",
    "end": "1921309"
  },
  {
    "text": "or are you hoping to\ntry to generalize? Because it is a discrete\ndistribution over processes, essentially.",
    "start": "1921310",
    "end": "1926470"
  },
  {
    "text": "Yeah, exactly. So by conditioning it on--",
    "start": "1926470",
    "end": "1931810"
  },
  {
    "text": "by trying to infer z,\nthis latent variable based on the training\ndata, we are hoping to generalize potentially.",
    "start": "1931810",
    "end": "1937690"
  },
  {
    "text": "And this is going to\nkind of represent-- z is going to be a\ncontinuous variable. And so, it's going to represent\na more continuous distribution",
    "start": "1937690",
    "end": "1943900"
  },
  {
    "text": "over tasks. And so, in principle,\nthis should be able to generalize to\nnew tasks if you have--",
    "start": "1943900",
    "end": "1951676"
  },
  {
    "text": "if your task distribution\nis sampled densely enough. If you have enough\ntraining tasks, basically.",
    "start": "1951676",
    "end": "1958300"
  },
  {
    "text": "And yeah. And so, in this approach,\nkind of when I was initially explaining this, I was talking\nabout these kind of task IDs",
    "start": "1958300",
    "end": "1965050"
  },
  {
    "text": "potentially. But by actually using\nthis task variable, you should actually\nbe able to generalize. Whereas if you only\nuse the task IDs,",
    "start": "1965050",
    "end": "1971980"
  },
  {
    "text": "you wouldn't be able to\ngeneralize to new tasks. ",
    "start": "1971980",
    "end": "1983009"
  },
  {
    "text": "OK, so to summarize, we're\nlearning a representation of the task, a policy\nthat conditions on that",
    "start": "1983010",
    "end": "1988950"
  },
  {
    "text": "task, and then doing this form\nof posterior sampling in order",
    "start": "1988950",
    "end": "1994559"
  },
  {
    "text": "to solve the task. As an example of what this\nlooks like in an example that's",
    "start": "1994560",
    "end": "2000919"
  },
  {
    "text": "actually fairly similar\nto the mazes is that, say different tasks correspond\nto these blue circles.",
    "start": "2000920",
    "end": "2007490"
  },
  {
    "text": "And the correct task is\nthis dark blue circle. Then what it will actually\ndo in practice is,",
    "start": "2007490",
    "end": "2014809"
  },
  {
    "text": "you first sample\nfrom z and then run your policy for that sample.",
    "start": "2014810",
    "end": "2020760"
  },
  {
    "text": "And then, once you\ndo that four times, you'll get these\nfour trajectories. And you see that it's sampling\nfrom the distribution of tasks.",
    "start": "2020760",
    "end": "2029330"
  },
  {
    "text": "Then, once you have\nthis experience, you can then condition your\nestimate of your distribution",
    "start": "2029330",
    "end": "2034370"
  },
  {
    "text": "of z on that experience. And then, you'll collect these\nlighter purple trajectories",
    "start": "2034370",
    "end": "2040040"
  },
  {
    "text": "that explore parts of the space\nthat are not previously covered from your data.",
    "start": "2040040",
    "end": "2047120"
  },
  {
    "text": "And then, you'll notice\nthat one of the trajectories here actually does\ngo to the goal and would get a positive reward.",
    "start": "2047120",
    "end": "2053330"
  },
  {
    "text": "And so, once you have\nthis experience, then it can essentially, for the\nfuture trajectories shown in orange, it'll now just\ngo directly to the goal",
    "start": "2053330",
    "end": "2061030"
  },
  {
    "text": "because it's figured\nout what the task is. ",
    "start": "2061030",
    "end": "2068590"
  },
  {
    "text": "Yeah. So in the middle column,\nwhy does conditioning on your past\ntrajectories make you",
    "start": "2068590",
    "end": "2076174"
  },
  {
    "text": "sample tasks over a distribution\nthat you don't already have an example of? Yeah.",
    "start": "2076174",
    "end": "2081340"
  },
  {
    "text": "So you're asking,\nwhy does conditioning on your past\nexperience encourage you to visit tasks that\nyou haven't visited before?",
    "start": "2081340",
    "end": "2089109"
  },
  {
    "text": "Yeah. And so, the reason\nwhy that's the case is that when you train a model\nto infer the task from data,",
    "start": "2089110",
    "end": "2096849"
  },
  {
    "text": "if it was given data that\nis doing other tasks,",
    "start": "2096850",
    "end": "2103360"
  },
  {
    "text": "then that's going to help it-- then basically\nthat's going to help",
    "start": "2103360",
    "end": "2109390"
  },
  {
    "text": "it understand what the task is. And because we're\ntraining this encoder",
    "start": "2109390",
    "end": "2115990"
  },
  {
    "text": "both for the KL\ndivergence term but also for the reward, when\nyou essentially--",
    "start": "2115990",
    "end": "2123628"
  },
  {
    "text": "say that when you're\noptimizing this, you're optimizing for hallway 4. Then if you condition this\non data from hallway 1,",
    "start": "2123628",
    "end": "2133510"
  },
  {
    "text": "then it doesn't know\nexactly what the task is. But it knows basically that\nit wants to maximize reward,",
    "start": "2133510",
    "end": "2140050"
  },
  {
    "text": "it shouldn't go to hallway 1. OK. So it's pushing down\nthe beliefs of the ones it's already visited, because\nit knows that's not a problem.",
    "start": "2140050",
    "end": "2147375"
  },
  {
    "text": "And basically, that the policy\nwon't get reward for that. And so, it should push\ndown those probabilities.",
    "start": "2147375",
    "end": "2154917"
  },
  {
    "text": "Good question. Yeah. This seems like maybe sort\nof a cross-entropy method. Because there, when you're kind\nof pushing up probabilities,",
    "start": "2154917",
    "end": "2161852"
  },
  {
    "text": "like, [INAUDIBLE]? ",
    "start": "2161852",
    "end": "2168280"
  },
  {
    "text": "The question is, it seems kind\nof like cross-entropy method. One thing that's different\nfrom cross-entropy method",
    "start": "2168280",
    "end": "2174520"
  },
  {
    "text": "is that if you're getting\nzero reward for these, cross-entropy method is going\nto try to take the best ones.",
    "start": "2174520",
    "end": "2180670"
  },
  {
    "text": "And there aren't any\ngood ones in the sample. And so, it probably\nwon't do very well if you-- it's not\ngoing to know that it",
    "start": "2180670",
    "end": "2186790"
  },
  {
    "text": "should try other things. The last step is a lot\nlike cross-entropy method. Where once you do actually\nhave some good samples,",
    "start": "2186790",
    "end": "2194140"
  },
  {
    "text": "it's going to sample\nmore from that. But this is actually\nmore powerful than a cross-entropy method.",
    "start": "2194140",
    "end": "2199207"
  },
  {
    "text": "Because it has this\nability to reason about uncertainty over the\ntask even given negative data.",
    "start": "2199207",
    "end": "2206846"
  },
  {
    "text": "I guess you could use\nthis for planning phase, it would be a lot more\ncomputationally [INAUDIBLE]??",
    "start": "2206846",
    "end": "2212660"
  },
  {
    "text": " So the question is, can\nyou use this for planning? ",
    "start": "2212660",
    "end": "2220120"
  },
  {
    "text": "Being able to estimate\nthis posterior is difficult in general. And so, if you can represent\nthat posterior during planning,",
    "start": "2220120",
    "end": "2227380"
  },
  {
    "text": "then you should\ndefinitely use it. In practice, having\naccess to it-- you have to learn\nit in some way.",
    "start": "2227380",
    "end": "2233810"
  },
  {
    "text": "And this is one way to learn it. But in planning, we often\ndon't have access to it. Yeah. So the p(z) you just\nassume that it's normal,",
    "start": "2233810",
    "end": "2240460"
  },
  {
    "text": "or is it uniform across\nall of the platforms? Yeah. So p(z) is going\nto be our prior.",
    "start": "2240460",
    "end": "2247250"
  },
  {
    "text": "And so, if we use\na KL divergence to a standard Gaussian\ndistribution, then p of z will just be the\nstandard Gaussian.",
    "start": "2247250",
    "end": "2252568"
  },
  {
    "start": "2252568",
    "end": "2258492"
  },
  {
    "text": "Cool. So now a question for\nyou, in what situations might this approach do poorly?",
    "start": "2258492",
    "end": "2266920"
  },
  {
    "text": "Yeah. When you [INAUDIBLE]? ",
    "start": "2266920",
    "end": "2272540"
  },
  {
    "text": "Yeah. So if your posterior\nis not very good, then this is going\nto do very poorly.",
    "start": "2272540",
    "end": "2279410"
  },
  {
    "text": "What if you have a\nvery good posterior? Are there scenarios where you\nhave a great posterior that's very accurate, but this still\nmight be a bad strategy?",
    "start": "2279410",
    "end": "2287390"
  },
  {
    "text": "Yeah. [INAUDIBLE]  You have very sparse rewards. ",
    "start": "2287390",
    "end": "2295970"
  },
  {
    "text": "Yes. Although, I guess, it kind of--\nif you have a couple of tasks and you can explore\nthose, then sparse rewards",
    "start": "2295970",
    "end": "2302240"
  },
  {
    "text": "will still be OK.  Any thoughts on task\ndistributions or problems",
    "start": "2302240",
    "end": "2309790"
  },
  {
    "text": "where this would be\nvery inefficient? ",
    "start": "2309790",
    "end": "2316790"
  },
  {
    "text": "Yeah. If you're calculating\nmore information from observing the\ntrajectories, which",
    "start": "2316790",
    "end": "2322910"
  },
  {
    "text": "makes the prior distribution\nand the posterior distribution have the same kind\nof information.",
    "start": "2322910",
    "end": "2331440"
  },
  {
    "text": "So if you don't gain\na lot of information from the trajectories.",
    "start": "2331440",
    "end": "2337377"
  },
  {
    "text": "And in particular, if you\ndon't get a lot of information from which trajectories? ",
    "start": "2337377",
    "end": "2347150"
  },
  {
    "text": "Yeah. What if you have two\nsolutions, and it encounters",
    "start": "2347150",
    "end": "2352220"
  },
  {
    "text": "one, which has a\nsomewhat decent reward, but the other one\nhas a better reward.",
    "start": "2352220",
    "end": "2357980"
  },
  {
    "text": "If you're unlucky, the\n[INAUDIBLE] thing [INAUDIBLE].. ",
    "start": "2357980",
    "end": "2369760"
  },
  {
    "text": "So it might omit this\nGaussian from the reward and never see the [INAUDIBLE] Yeah.",
    "start": "2369760",
    "end": "2375260"
  },
  {
    "text": "So if you have a\nmultimodal distribution, it might be difficult to kind\nof represent the multiple modes.",
    "start": "2375260",
    "end": "2382040"
  },
  {
    "text": "And it might just pick one of\nthem, which would do poorly. That's good. Any other thoughts? ",
    "start": "2382040",
    "end": "2390800"
  },
  {
    "text": "Any thoughts on this example\nand how well posterior sampling",
    "start": "2390800",
    "end": "2396710"
  },
  {
    "text": "would do here? ",
    "start": "2396710",
    "end": "2402740"
  },
  {
    "text": "Yeah. Does it really help us in\nthis example, [INAUDIBLE]..",
    "start": "2402740",
    "end": "2409880"
  },
  {
    "text": "So if you have a trajectory\nthat did learn how to read, but then it didn't\nget any reward for it, the posterior\n[INAUDIBLE] wouldn't",
    "start": "2409880",
    "end": "2416300"
  },
  {
    "text": "update the belief [INAUDIBLE]. Yeah. So one thing that it's not\ngoing to do in this example",
    "start": "2416300",
    "end": "2421730"
  },
  {
    "text": "is, it's not going\nto learn how to read. It's just going to\ntry all the tasks. And so, this will be a\nsuboptimal exploration strategy",
    "start": "2421730",
    "end": "2430100"
  },
  {
    "text": "because it takes a lot longer\nto try the tasks exhaustively than to just read them\nand then figure out",
    "start": "2430100",
    "end": "2435500"
  },
  {
    "text": "what the task is from there. And one scenario in which\nthis is particularly bad",
    "start": "2435500",
    "end": "2441710"
  },
  {
    "text": "is if your hallways\nare super long. Then exploring all\nthe hallways is going to take a lot\nof time in comparison",
    "start": "2441710",
    "end": "2448310"
  },
  {
    "text": "to just trying to\nread the instructions. And the reason why it\ndoesn't read the instructions is that reading instructions\nisn't actually part",
    "start": "2448310",
    "end": "2455540"
  },
  {
    "text": "of solving any of the tasks. It's just going\nto-- it just tries to iteratively solve the tasks. It doesn't try to\nactually see if there's",
    "start": "2455540",
    "end": "2462212"
  },
  {
    "text": "any information in the\nenvironment that will actually help it solve-- help it understand\nwhat the task is.",
    "start": "2462212",
    "end": "2467360"
  },
  {
    "text": "That's kind of independent\nfrom actually solving the task. ",
    "start": "2467360",
    "end": "2473592"
  },
  {
    "text": "I have a question. Yeah. One more question. But if you were to to\nread, wouldn't that",
    "start": "2473592",
    "end": "2478770"
  },
  {
    "text": "give us repeat information\nabout like, [INAUDIBLE]?? ",
    "start": "2478770",
    "end": "2488720"
  },
  {
    "text": "If the information\nis being retained somewhere in our trajectory, but\nit's not retained [INAUDIBLE]?? ",
    "start": "2488720",
    "end": "2494620"
  },
  {
    "text": "Yeah. So if the instructions are\nactually part of the trajectory and you actually\nsee that, then it",
    "start": "2494620",
    "end": "2500160"
  },
  {
    "text": "will actually learn to kind\nof collapse its posterior down to the correct task. But it's not going to\nactually go and see that.",
    "start": "2500160",
    "end": "2505800"
  },
  {
    "text": "It's not going to\ngo out of its way to go see the instructions if\nthat's not part of actually getting reward for the task.",
    "start": "2505800",
    "end": "2512160"
  },
  {
    "text": "And so, yeah, it's not\ngoing to essentially go out of its way to find that\ninformation because it's just",
    "start": "2512160",
    "end": "2519990"
  },
  {
    "text": "sampling from tasks and sampling\nfrom the policy for that task. ",
    "start": "2519990",
    "end": "2525890"
  },
  {
    "text": "And so, if for example\nyou have super-- your goals are really far\naway, and these instructions",
    "start": "2525890",
    "end": "2531920"
  },
  {
    "text": "or a sign on the wall tells\nyou exactly what to do, then it's not going to actually\ngo seek out that information.",
    "start": "2531920",
    "end": "2540360"
  },
  {
    "text": "Yeah. Does it have to learn how to\nread, or does it already know?",
    "start": "2540360",
    "end": "2546695"
  },
  {
    "text": "The question is, does it\nhave to learn how to read, or does it already\nknow how to read? I guess there are different\nvariants of the problem.",
    "start": "2546695",
    "end": "2553560"
  },
  {
    "text": "You could have one\nwhere if it's goes here, it's told directly one,\ntwo, three, four, five.",
    "start": "2553560",
    "end": "2559500"
  },
  {
    "text": "Or something where it goes\nhere, and it gets an image, and it has to learn\nhow to read the text and solve a math problem and\nfigure out what the number is.",
    "start": "2559500",
    "end": "2566910"
  },
  {
    "text": "In both of those cases, it's\nnot going to actually go here.",
    "start": "2566910",
    "end": "2572250"
  },
  {
    "text": "It's going-- posterior\nsampling approaches will just try to sample tasks\nand solve tasks.",
    "start": "2572250",
    "end": "2579210"
  },
  {
    "text": "And so, the reason\nwhy I bring this up is that this approach\nis a lot better",
    "start": "2579210",
    "end": "2584250"
  },
  {
    "text": "than end-to-end\noptimization in that it's a lot easier to\nultimately figure out",
    "start": "2584250",
    "end": "2590040"
  },
  {
    "text": "how to solve the tasks. The downside is\nthat it may end up",
    "start": "2590040",
    "end": "2595650"
  },
  {
    "text": "on site with\nexploration strategies that are very suboptimal. So in some cases, the\nstrategy will be fine.",
    "start": "2595650",
    "end": "2601720"
  },
  {
    "text": "In other cases, it might\nbe arbitrarily bad,",
    "start": "2601720",
    "end": "2608099"
  },
  {
    "text": "like if the hallways are\ninfinitely long, for example. ",
    "start": "2608100",
    "end": "2614307"
  },
  {
    "text": "Another example of something\nthat would be better than this approach\nright here is instead kind of going to each of those,\nif it first went here and then",
    "start": "2614308",
    "end": "2621782"
  },
  {
    "text": "walked around the circle, then\nit could actually figure out what the task is in\njust a single episode rather than requiring a\nlot of episodes to explore.",
    "start": "2621782",
    "end": "2630993"
  },
  {
    "text": "And that's even without a sign. ",
    "start": "2630993",
    "end": "2639740"
  },
  {
    "text": "OK, so this was our first\nkind of alternative approach to an end-to-end optimization. And this is not end-to-end,\nbecause we are not",
    "start": "2639740",
    "end": "2647109"
  },
  {
    "text": "learning our exploration\nstrategy end-to-end. We're actually using\nthis different strategy for exploring. And then, we're\nlearning how to solve",
    "start": "2647110",
    "end": "2653500"
  },
  {
    "text": "the task based on information we\ninferred from that exploration.",
    "start": "2653500",
    "end": "2658960"
  },
  {
    "text": " One second alternative\napproach that I'll talk about",
    "start": "2658960",
    "end": "2665380"
  },
  {
    "text": "is to instead of doing any\nsort of posterior sampling,",
    "start": "2665380",
    "end": "2673509"
  },
  {
    "text": "we're going to try to explore in\na way that tells us information about the dynamics\nand the reward.",
    "start": "2673510",
    "end": "2679630"
  },
  {
    "text": "And the way that we can do\nthis is, if we learn a dynamics model and a reward function for\nall of the tasks conditioned",
    "start": "2679630",
    "end": "2688240"
  },
  {
    "text": "on our training data,\nthen what we could do is, we could try to train\nan exploration strategy such",
    "start": "2688240",
    "end": "2694630"
  },
  {
    "text": "that this model is accurate.  And so, once we\nhave this model, we",
    "start": "2694630",
    "end": "2702960"
  },
  {
    "text": "can then kind of train\nan exploration policy that gets high reward if\nthis model has low error.",
    "start": "2702960",
    "end": "2710095"
  },
  {
    "text": " And so, what an\napproach like this would look like\nis for each task,",
    "start": "2710095",
    "end": "2716650"
  },
  {
    "text": "you collect some data with your\ncurrent exploration policy. Then you also collect data\nwith an execution policy,",
    "start": "2716650",
    "end": "2724750"
  },
  {
    "text": "a separate policy that tries\nto solve the task based on their collected data, and\nthen, train your exploration",
    "start": "2724750",
    "end": "2732820"
  },
  {
    "text": "policy with respect\nto the reward of kind of the negative error\nof your dynamics model.",
    "start": "2732820",
    "end": "2739990"
  },
  {
    "text": "And then, train your\nexecution policy with respect",
    "start": "2739990",
    "end": "2745000"
  },
  {
    "text": "to the goal of\nsolving the tasks. And so, note here the\nexploration policy",
    "start": "2745000",
    "end": "2750520"
  },
  {
    "text": "and execution policy have\ndifferent reward functions. They're optimizing\nfor different things. ",
    "start": "2750520",
    "end": "2756850"
  },
  {
    "text": "And this is essentially--\nthis exploration policy is going to try to find things\nthat allow it to predict",
    "start": "2756850",
    "end": "2764500"
  },
  {
    "text": "the reward and the dynamics. And so, in this example\nright here, the dynamics",
    "start": "2764500",
    "end": "2770980"
  },
  {
    "text": "are all the same across\nall of the tasks. I guess, actually, no. That's not quite right.",
    "start": "2770980",
    "end": "2776412"
  },
  {
    "text": "So the dynamics are actually\na little bit different. So the dynamics are different\nhere at the instructions.",
    "start": "2776413",
    "end": "2781960"
  },
  {
    "text": "Because if you move on\nto the instructions, you're going to see something\ndifferent for different tasks because the instructions are\ndifferent for different tasks.",
    "start": "2781960",
    "end": "2788348"
  },
  {
    "text": "And then, you also\nhave different rewards. And so, what this approach\nis going to do is, it's going to try to learn\na model of the dynamics",
    "start": "2788348",
    "end": "2794213"
  },
  {
    "text": "and the reward and then seek\nout parts of the state space that allow it to\ndifferentiate different tasks.",
    "start": "2794213",
    "end": "2801770"
  },
  {
    "text": "And so, then what it's\ngoing to do in this case is, the exploration policy, once\nyou learn a model based on data,",
    "start": "2801770",
    "end": "2808060"
  },
  {
    "text": "the exploration\npolicy will actually be encouraged to\nseek out this state, because that will help\nit actually differentiate",
    "start": "2808060",
    "end": "2816549"
  },
  {
    "text": "what the reward is going to be. ",
    "start": "2816550",
    "end": "2821630"
  },
  {
    "text": "Yeah. And so, if you also\nlook at the example that we looked at before, where\nwe have these different goals,",
    "start": "2821630",
    "end": "2829180"
  },
  {
    "text": "then it will actually\nlearn a policy that looks like this,\nwhere it's essentially",
    "start": "2829180",
    "end": "2834520"
  },
  {
    "text": "trying to learn how\nto kind of estimate the reward of the task.",
    "start": "2834520",
    "end": "2840430"
  },
  {
    "text": "And a good way to explore to\nfigure out what the reward is, is to actually go\naround the circle",
    "start": "2840430",
    "end": "2846640"
  },
  {
    "text": "rather than sampling\nfrom the posterior. ",
    "start": "2846640",
    "end": "2857440"
  },
  {
    "text": "Cool. Any questions on how this works? ",
    "start": "2857440",
    "end": "2868210"
  },
  {
    "text": "Is it possible [INAUDIBLE]? So like, in this\ncase, it [INAUDIBLE]??",
    "start": "2868210",
    "end": "2874600"
  },
  {
    "text": "Yeah. So this example is\ndifferent than this one. They're related. But in this case, if you can't\nactually go around the circle",
    "start": "2874600",
    "end": "2881910"
  },
  {
    "text": "because the dynamics\ndon't let you do that,",
    "start": "2881910",
    "end": "2887700"
  },
  {
    "text": "then you can't\ntake this approach. So the examples are a\nlittle bit different here.",
    "start": "2887700",
    "end": "2893040"
  },
  {
    "start": "2893040",
    "end": "2900468"
  },
  {
    "text": "And so, to summarize\nwhat's happening here, we are going to learn to predict\nthe dynamics and the reward",
    "start": "2900468",
    "end": "2907030"
  },
  {
    "text": "given the state action\nand our experience. This is just trained with mean\nsquared error or something",
    "start": "2907030",
    "end": "2913240"
  },
  {
    "text": "like that. Then we'll learn an\nexploration policy that gets--",
    "start": "2913240",
    "end": "2922900"
  },
  {
    "text": "it gets high reward if\nthe data that it collected helps this model predict well.",
    "start": "2922900",
    "end": "2928300"
  },
  {
    "text": "So it's good if f is able\nto accurately predict",
    "start": "2928300",
    "end": "2937483"
  },
  {
    "text": "the dynamics and the reward. It also gets negative\nreward if it's inaccurate. And then you'll also train--",
    "start": "2937483",
    "end": "2944410"
  },
  {
    "text": "this is just-- this can be\ndone completely separately from actually solving the tasks. ",
    "start": "2944410",
    "end": "2951190"
  },
  {
    "text": "And then, this execution\npolicy can in many ways just be trained after\nthe fact in order",
    "start": "2951190",
    "end": "2956650"
  },
  {
    "text": "to try to actually solve the\ntasks given the training data.",
    "start": "2956650",
    "end": "2963230"
  },
  {
    "text": "And this is just going\nto be trying to maximize",
    "start": "2963230",
    "end": "2968320"
  },
  {
    "text": "expected reward of this policy. ",
    "start": "2968320",
    "end": "2975339"
  },
  {
    "text": "And so, this is essentially\ndecoupling exploration from execution, because you\ncan first just train this model",
    "start": "2975340",
    "end": "2981150"
  },
  {
    "text": "and explore so that\nthis model is accurate. And then, separately\nuse your experience",
    "start": "2981150",
    "end": "2987558"
  },
  {
    "text": "to actually figure out\nhow to solve the tasks. ",
    "start": "2987558",
    "end": "2994880"
  },
  {
    "text": "Yeah. [INAUDIBLE] just put it\n[INAUDIBLE] exploration?",
    "start": "2994880",
    "end": "3004040"
  },
  {
    "text": "Yeah. Great question. So the exploration\nreward, it's essentially",
    "start": "3004040",
    "end": "3009970"
  },
  {
    "text": "set to be the negative\nerror of this model. And the intuition is that\nif this model is accurate,",
    "start": "3009970",
    "end": "3019570"
  },
  {
    "text": "that means that you've been\nable to distinguish information about the task. You've been able to predict\nthe dynamics or the reward",
    "start": "3019570",
    "end": "3025210"
  },
  {
    "text": "for that task. And so, this is\ngoing to be rewarded in cases where you've learned\ninformation that helps",
    "start": "3025210",
    "end": "3032740"
  },
  {
    "text": "it distinguish different tasks. And you're going to\nget a negative reward if this model is\ninaccurate, If it's not",
    "start": "3032740",
    "end": "3039828"
  },
  {
    "text": "able to predict the\ndynamics and the reward for the current task. ",
    "start": "3039828",
    "end": "3047860"
  },
  {
    "text": "Yeah. What is-- where do you\nget the state action pairs to evaluate [INAUDIBLE]?",
    "start": "3047860",
    "end": "3056083"
  },
  {
    "text": "Is it from like, set over Q?  Ah, great question.",
    "start": "3056083",
    "end": "3061359"
  },
  {
    "text": "So yeah. So essentially, this reward is-- ",
    "start": "3061360",
    "end": "3067202"
  },
  {
    "text": "I'll write it out right here. So ooh, not that.",
    "start": "3067202",
    "end": "3073589"
  },
  {
    "text": " So you're going to\nsample your training data",
    "start": "3073590",
    "end": "3080540"
  },
  {
    "text": "from your exploration policy. And then, the reward is going\nto be evaluated on this training",
    "start": "3080540",
    "end": "3088539"
  },
  {
    "text": "data. So the reward for exploration\nwill be kind of for k",
    "start": "3088540",
    "end": "3096830"
  },
  {
    "text": "the state action pairs in this\ncollective experience, what",
    "start": "3096830",
    "end": "3102470"
  },
  {
    "text": "is the negative error, which is\nf of s a minus r and s prime.",
    "start": "3102470",
    "end": "3118982"
  },
  {
    "text": " So this is the kind of full\nequation for the reward.",
    "start": "3118982",
    "end": "3125420"
  },
  {
    "text": "Yeah. I guess follow\n[INAUDIBLE] policy, this is a classified\nstate [INAUDIBLE]??",
    "start": "3125420",
    "end": "3131699"
  },
  {
    "text": " Cool.",
    "start": "3131699",
    "end": "3137175"
  },
  {
    "text": "Good question. So one thing that-- one issue\nthat might come up is if you-- ",
    "start": "3137175",
    "end": "3143820"
  },
  {
    "text": "right. Great. So if you only evaluate\nthis on the support set, then it might just stay in place\nand not actually be able to--",
    "start": "3143820",
    "end": "3152560"
  },
  {
    "text": " which is actually bad.",
    "start": "3152560",
    "end": "3157740"
  },
  {
    "text": "The-- and so, yeah.",
    "start": "3157740",
    "end": "3166938"
  },
  {
    "text": "That's interesting. I actually-- I can't\nremember actually how they dealt with that problem.",
    "start": "3166938",
    "end": "3172290"
  },
  {
    "text": "I'm guessing that actually it's\nprobably a mix of D train and-- ",
    "start": "3172290",
    "end": "3180490"
  },
  {
    "text": "sorry. Yes. So this model is conditioned\non your training experience.",
    "start": "3180490",
    "end": "3189820"
  },
  {
    "text": "And so, you want-- right. So you want your model to be\nable to predict well based",
    "start": "3189820",
    "end": "3195720"
  },
  {
    "text": "on your training experience. And then, this\nexpectation, I think, is actually going\nto be with respect",
    "start": "3195720",
    "end": "3202770"
  },
  {
    "text": "to states that are sampled\nfrom a policy that's solving the task,\nor something that's",
    "start": "3202770",
    "end": "3208920"
  },
  {
    "text": "actually more broad than\njust the training experience. Yeah. That's a great correction. ",
    "start": "3208920",
    "end": "3215970"
  },
  {
    "text": "So I'll just name that di. ",
    "start": "3215970",
    "end": "3223530"
  },
  {
    "text": "But it's very important that\nthese are the same thing. Because this is\nactually what the model is using to make predictions.",
    "start": "3223530",
    "end": "3230100"
  },
  {
    "text": "Yeah. Follow-up question about\nyour exploration rewards.",
    "start": "3230100",
    "end": "3235140"
  },
  {
    "text": "I don't think it's a good\nenough pool for training the explore prediction\npolicy because it would be successful\nto train reward model.",
    "start": "3235140",
    "end": "3244530"
  },
  {
    "text": "So basically, I'm\nthinking, I'm going to introduce another\nexploration reward, which",
    "start": "3244530",
    "end": "3251280"
  },
  {
    "text": "reward the space that kind of\nsurprises the reward model. I think that's a better one,\nbetter replacement phase.",
    "start": "3251280",
    "end": "3261370"
  },
  {
    "text": "So yeah. One downside to\nthis is that you do have to train a dynamics model. And training this\nmight be expensive.",
    "start": "3261370",
    "end": "3269130"
  },
  {
    "text": "You're suggesting\nthat instead of trying to minimize model\nerror, you'll try to learn things that are\nsurprising to the model,",
    "start": "3269130",
    "end": "3275490"
  },
  {
    "text": "that it can't model accurately? ",
    "start": "3275490",
    "end": "3282310"
  },
  {
    "text": "I think that that's a\nreasonable approach. That is actually--\npeople have done that in standard\nreinforcement learning.",
    "start": "3282310",
    "end": "3288187"
  },
  {
    "text": "I'm not sure if that\nwould necessarily learn exploration\nstrategies that are targeted for particular tasks. And one thing that's\nnice about this objective",
    "start": "3288187",
    "end": "3294960"
  },
  {
    "text": "is it is going to try to\ndiscover the things that are different among\ndifferent tasks and use that to solve the task.",
    "start": "3294960",
    "end": "3300569"
  },
  {
    "text": "And that's a good\nstrategy, because if you learn about everything\nthat's interesting, there might be a\nlot of things that",
    "start": "3300570",
    "end": "3306480"
  },
  {
    "text": "are interesting and\nsurprising but aren't relevant to the task. And essentially,\nwhat we want to do",
    "start": "3306480",
    "end": "3312750"
  },
  {
    "text": "when learning\nexploration strategies is to find things that help\nus figure out what the task is",
    "start": "3312750",
    "end": "3318600"
  },
  {
    "text": "and ignore things that are\nirrelevant to the task. ",
    "start": "3318600",
    "end": "3324900"
  },
  {
    "text": "So that said, one thing\nthat is potentially bad about this\napproach is if there",
    "start": "3324900",
    "end": "3330870"
  },
  {
    "text": "are parts of the dynamics that\nare different across tasks but irrelevant to\nsolving the task, then it will actually\ntry to seek those out.",
    "start": "3330870",
    "end": "3339570"
  },
  {
    "text": " And so, yeah,\nessentially, if there",
    "start": "3339570",
    "end": "3345210"
  },
  {
    "text": "are a lot of tasks with\nirrelevant distractors, also if you're in a very\ncomplex high-dimensional state",
    "start": "3345210",
    "end": "3350670"
  },
  {
    "text": "space where learning\na model is difficult, this sort of approach\nwill be less satisfying.",
    "start": "3350670",
    "end": "3355860"
  },
  {
    "start": "3355860",
    "end": "3363120"
  },
  {
    "text": "Cool. So overall, these approaches\nare generally a lot easier to optimize than the\nend-to-end optimization",
    "start": "3363120",
    "end": "3370720"
  },
  {
    "text": "because the exploration\nstrategy is-- the objective for the\nexploration strategy is more decoupled from actually\nlearning to solve the task.",
    "start": "3370720",
    "end": "3380140"
  },
  {
    "text": "They're not optimizing\nthe same objective. A lot of them are based\non principled strategies",
    "start": "3380140",
    "end": "3385720"
  },
  {
    "text": "for exploration. But the downside is\nthat these approaches",
    "start": "3385720",
    "end": "3391780"
  },
  {
    "text": "can be arbitrarily suboptimal. So for example, in\nposterior sampling,",
    "start": "3391780",
    "end": "3397870"
  },
  {
    "text": "we saw that it might-- it won't\nlearn how to read instructions. Likewise for the\nsecond approach,",
    "start": "3397870",
    "end": "3403839"
  },
  {
    "text": "if there are aspects\nof the dynamics that are different across tasks\nthat are very interesting but are completely irrelevant\nto solving the task, like maybe",
    "start": "3403840",
    "end": "3411370"
  },
  {
    "text": "there are, I don't know, lots of\npaintings on the wall that vary across different tasks,\nthen this approach",
    "start": "3411370",
    "end": "3417579"
  },
  {
    "text": "is going to try to explore\nall of those details, which will not be very useful\nand will be very--",
    "start": "3417580",
    "end": "3424290"
  },
  {
    "text": "it might spend a lot of time\ndoing that rather than actually learning to differentiate\nthe things that are relevant for solving the task.",
    "start": "3424290",
    "end": "3429640"
  },
  {
    "text": " Cool. And so, what I'd\nlike to get to now",
    "start": "3429640",
    "end": "3438383"
  },
  {
    "text": "is an approach\nthat actually tries to decouple these two things\nwithout sacrificing optimality.",
    "start": "3438383",
    "end": "3445080"
  },
  {
    "text": "And it turns out\nthis is actually possible to essentially get\nthe best of both worlds.",
    "start": "3445080",
    "end": "3450660"
  },
  {
    "text": "Where we're able\nto explore in a way that is only\nrelevant to the task,",
    "start": "3450660",
    "end": "3457920"
  },
  {
    "text": "and also do so in a way that's\nvery efficient without having",
    "start": "3457920",
    "end": "3464128"
  },
  {
    "text": "this coupling problem. ",
    "start": "3464128",
    "end": "3469240"
  },
  {
    "text": "Cool. So the last solution\nthat we talked about was to try to\npredict the dynamics and explore in a way that\nallows you to better predict",
    "start": "3469240",
    "end": "3476120"
  },
  {
    "text": "the dynamics. But do we really have\nto learn a full dynamics model and a full reward model?",
    "start": "3476120",
    "end": "3484790"
  },
  {
    "text": "The idea that we're going to\nlook at here, or the almost final idea, is to label\neach of your training tasks",
    "start": "3484790",
    "end": "3492440"
  },
  {
    "text": "with a unique identifier. This could just be a\none hot identifier, like you just enumerate\nall of your training tasks.",
    "start": "3492440",
    "end": "3501049"
  },
  {
    "text": "And then, you're\ninstead of trying to predict the dynamics\nand the reward, you're going to try to\npredict this identifier.",
    "start": "3501050",
    "end": "3506720"
  },
  {
    "text": " And so, what this will look\nlike in this example is,",
    "start": "3506720",
    "end": "3518730"
  },
  {
    "text": "instead of predicting\ndynamics and reward, we're just going\nto try to predict what the task identifier is.",
    "start": "3518730",
    "end": "3525420"
  },
  {
    "text": "And this model will be\naccurate, essentially, when it finds things that are\ndifferent among the tasks.",
    "start": "3525420",
    "end": "3532420"
  },
  {
    "text": "And so, if the\ninstructions are something that's different in the\ndifferent environments, then this will encourage\nthe exploration model",
    "start": "3532420",
    "end": "3538859"
  },
  {
    "text": "to find the thing that helps it\ndifferentiate between the tasks very quickly.",
    "start": "3538860",
    "end": "3546010"
  },
  {
    "text": "And so, that's what the\nexploration strategy",
    "start": "3546010",
    "end": "3553875"
  },
  {
    "text": "will look like. And so, concretely, we're\ngoing to train this task identification model.",
    "start": "3553875",
    "end": "3561750"
  },
  {
    "text": "I guess I named it q.  And also train\nexploration policy",
    "start": "3561750",
    "end": "3568770"
  },
  {
    "text": "that tries to explore\nif q is accurate.",
    "start": "3568770",
    "end": "3574990"
  },
  {
    "text": "And so, its exploration\npolicy is trained such that when you sample data\nfrom your exploration policy,",
    "start": "3574990",
    "end": "3581430"
  },
  {
    "text": "you're able to accurately\npredict the task. And then, we'll separately\ntrain an execution policy",
    "start": "3581430",
    "end": "3589170"
  },
  {
    "text": "that is just conditioned\non the task identifier. ",
    "start": "3589170",
    "end": "3595161"
  },
  {
    "text": "And this last part\nshould be really easy. Because this amounts\nto solving the task independent of figuring out\nhow to explore for those tasks.",
    "start": "3595162",
    "end": "3603270"
  },
  {
    "text": "And so, this is\ncompletely decoupled. And then, once you do this,\nthen at meta-test time,",
    "start": "3603270",
    "end": "3608450"
  },
  {
    "text": "you can explore\nby collecting data from your exploration policy.",
    "start": "3608450",
    "end": "3614090"
  },
  {
    "text": "Then infer what you\nthink the task ID is. And then condition\nyour execution policy",
    "start": "3614090",
    "end": "3620630"
  },
  {
    "text": "on this task identifier. ",
    "start": "3620630",
    "end": "3627410"
  },
  {
    "text": "And so, one of the things that's\nreally nice about this is now you no longer have to model\nthe dynamics and the rewards.",
    "start": "3627410",
    "end": "3632599"
  },
  {
    "text": " And it should\nactually encourage it to find things\nthat are different",
    "start": "3632600",
    "end": "3639380"
  },
  {
    "text": "about different environments. And so, going back to\nour objective right here,",
    "start": "3639380",
    "end": "3646970"
  },
  {
    "text": "this will now be\nsomething that's trying to predict just the task\nidentifier from your training",
    "start": "3646970",
    "end": "3654560"
  },
  {
    "text": "data. And this is going to be compared\nto the true task identifier.",
    "start": "3654560",
    "end": "3664890"
  },
  {
    "text": "And then, the reward for\nyour exploration policy will be how well it's able to\npredict the task identifier.",
    "start": "3664890",
    "end": "3671915"
  },
  {
    "text": " And then, so we have\nthese three models.",
    "start": "3671916",
    "end": "3678825"
  },
  {
    "text": "We have something\nthat takes as input--  we have our exploration\npolicy which",
    "start": "3678825",
    "end": "3685940"
  },
  {
    "text": "then outputs your trained data. We also have a model that takes\ntraining data and predicts mu.",
    "start": "3685940",
    "end": "3693650"
  },
  {
    "text": "And then lastly, a model\nthat takes mu and predicts",
    "start": "3693650",
    "end": "3699470"
  },
  {
    "text": "or solves the task from there. ",
    "start": "3699470",
    "end": "3707350"
  },
  {
    "text": "OK, now one-- I guess, could\nanyone spot any downsides with this approach? ",
    "start": "3707350",
    "end": "3717690"
  },
  {
    "text": "Yeah. For example, to\nclassify that these are close to each other but\n[INAUDIBLE] not very close.",
    "start": "3717690",
    "end": "3724805"
  },
  {
    "text": " Yeah. So if your task identifier is--\nif two of your task identifiers",
    "start": "3724805",
    "end": "3730950"
  },
  {
    "text": "are very close in value, but in\nreality they're very different, then this is a problem. If you use a one hot\nidentifier, this will be better.",
    "start": "3730950",
    "end": "3737655"
  },
  {
    "text": "And then, instead of\nusing mean squared error you could do cross-entropy loss. And that will\nresolve that issue.",
    "start": "3737655",
    "end": "3743820"
  },
  {
    "text": " Any other thoughts? ",
    "start": "3743820",
    "end": "3750100"
  },
  {
    "text": "Yeah. I have a follow-up\nquestion on that. Does the task identifier\nassume that you've seen that task before?",
    "start": "3750100",
    "end": "3756025"
  },
  {
    "text": " Because if it's a one\nhot vector, [INAUDIBLE]??",
    "start": "3756025",
    "end": "3762070"
  },
  {
    "text": "Yeah. So this is essentially\nthe downside, which is that if you give it\na new task identifier, if you",
    "start": "3762070",
    "end": "3769710"
  },
  {
    "text": "have a new task,\nfor example, then it's not going to be able to--",
    "start": "3769710",
    "end": "3775140"
  },
  {
    "text": "if this is your one hot vectors\nfor all of your training tasks, then it's not going to know how\nto predict a one hot identifier",
    "start": "3775140",
    "end": "3782010"
  },
  {
    "text": "for the new task. And so, it won't generalize\nwell to new tasks.",
    "start": "3782010",
    "end": "3788430"
  },
  {
    "text": "Because you're just using-- especially if you're\nusing one hot identifiers.",
    "start": "3788430",
    "end": "3793650"
  },
  {
    "text": "Yeah. I don't really understand\nhow this solves this problem. Because don't we still need\nto have trajectories with,",
    "start": "3793650",
    "end": "3799562"
  },
  {
    "text": "in this example,\nlearn how to read, and then go and find the correct\nhallway in order to distinguish",
    "start": "3799562",
    "end": "3804750"
  },
  {
    "text": "between the different tasks. And that was the\nsame problem that we had in the previous lecture\nwith the posterior samples,",
    "start": "3804750",
    "end": "3812870"
  },
  {
    "text": "we still needed those\ntrajectories [INAUDIBLE]?? Yeah, great question. So the question is,\nit seems like maybe we",
    "start": "3812870",
    "end": "3819559"
  },
  {
    "text": "still need trajectories that go\nread and then solve the task. And maybe this doesn't\nsolve the problem.",
    "start": "3819560",
    "end": "3826099"
  },
  {
    "text": "The reason why this can\nsolve the problem is that for different\ntasks, there are",
    "start": "3826100",
    "end": "3831110"
  },
  {
    "text": "going to be-- the instructions\nare going to look different. And what this exploration\npolicy is going to try to do",
    "start": "3831110",
    "end": "3838790"
  },
  {
    "text": "is, it's going to try to\nbe able to discern the task identity from its interactions.",
    "start": "3838790",
    "end": "3846020"
  },
  {
    "text": "And because the\ninstructions look different for different tasks,\nit can discern",
    "start": "3846020",
    "end": "3851210"
  },
  {
    "text": "what task it's in by just\nwalking to the instructions and stopping there. But without the reward\nfeedback, how does it know",
    "start": "3851210",
    "end": "3858940"
  },
  {
    "text": "which task that corresponds to? Yeah. So the question is,\nwithout reward feedback, how does it know which\ntask this corresponds to?",
    "start": "3858940",
    "end": "3867010"
  },
  {
    "text": "By enumerating your\ntask identifiers, this reward doesn't depend on\nthe task reward in any way.",
    "start": "3867010",
    "end": "3874720"
  },
  {
    "text": "It just is looking\nat whether you're able to predict the task\nthat you're currently in-- the environment\nthat you're in.",
    "start": "3874720",
    "end": "3880754"
  },
  {
    "text": "OK. And so, it will essentially\nget-- this exploration policy will get a reward just for\nbeing able to predict a task",
    "start": "3880754",
    "end": "3886930"
  },
  {
    "text": "identifier completely\nseparate from actually being able to solve the\ntask from that identifier. OK.",
    "start": "3886930",
    "end": "3892017"
  },
  {
    "text": "So that goes back to\nwhat others were saying, where you have to know before\nwhat all your task identifiers",
    "start": "3892017",
    "end": "3897490"
  },
  {
    "text": "are. Yeah. So this is going to assume\nthat you have task identifiers that you've been able to\nenumerate all of your training",
    "start": "3897490",
    "end": "3903190"
  },
  {
    "text": "tasks and you know which\ntraining task you're currently exploring in. Yeah. If you use a non-one\nhot task identifier,",
    "start": "3903190",
    "end": "3909536"
  },
  {
    "text": "does that problem go away? Yeah. So if you use a non-one hot\ntask identifier, is this better?",
    "start": "3909536",
    "end": "3918160"
  },
  {
    "text": "It is better if you can. In the next couple\nof slides, we'll talk about an approach that\nwill still work even when you",
    "start": "3918160",
    "end": "3924915"
  },
  {
    "text": "have one hot task identifiers. But it definitely\ngets a lot better if you have more informative\ntask identifiers.",
    "start": "3924915",
    "end": "3931329"
  },
  {
    "text": "Yeah. How do we get such\n[INAUDIBLE] in practice?",
    "start": "3931330",
    "end": "3936539"
  },
  {
    "text": "Yeah. So how do you get these task\nidentifiers in practice? It kind of depends\non the domain. But if for example, we're\nin the kitchen example,",
    "start": "3936540",
    "end": "3943809"
  },
  {
    "text": "where the goal is to cook a\nmeal in different kitchens. And you need to find ingredients\nin different kitchens.",
    "start": "3943810",
    "end": "3949030"
  },
  {
    "text": "Then basically,\nwhenever you collect experience in a kitchen, you\ntell it, you're in kitchen one. And if you're in a\ndifferent kitchen you say,",
    "start": "3949030",
    "end": "3955690"
  },
  {
    "text": "you're in kitchen two. Or a different kitchen,\nyou're in kitchen three. And you just need to\nmake sure that you are consistent saying\nyou're in kitchen three",
    "start": "3955690",
    "end": "3962170"
  },
  {
    "text": "when you're actually\nin kitchen three. [INAUDIBLE] Sorry, what?",
    "start": "3962170",
    "end": "3969255"
  },
  {
    "text": "By just having\n[INAUDIBLE] telling it [INAUDIBLE] kitchen,\nisn't that still one hot? Oh, sorry.",
    "start": "3969255",
    "end": "3974317"
  },
  {
    "text": "I think I misunderstood\nyour question. You're asking how do you\nget non-one hot identifiers? Yeah.",
    "start": "3974317",
    "end": "3979350"
  },
  {
    "text": "So getting non-one hot\nidentifiers is harder. You would need to say--",
    "start": "3979350",
    "end": "3984355"
  },
  {
    "text": "you need to be able\nto give it features that distinguish the\nkitchens, for example, that identify where the--",
    "start": "3984355",
    "end": "3990900"
  },
  {
    "text": "what cabinet are\nthe ingredients in, and what's other information\nabout the environment.",
    "start": "3990900",
    "end": "3998920"
  },
  {
    "text": "Yeah. [INAUDIBLE]  Yes. And so, that's what we'll do on\nthe next slide, is essentially",
    "start": "3998920",
    "end": "4005990"
  },
  {
    "text": "try to learn a better\ntask identifier that isn't one hot from the\none hot task identifier.",
    "start": "4005990",
    "end": "4011840"
  },
  {
    "text": "Yeah. [INAUDIBLE] to learn\nthat one hot identifier, would you need to\nlearn a linear--",
    "start": "4011840",
    "end": "4017260"
  },
  {
    "text": "you would have some kind\nof multilayer [INAUDIBLE] prediction [INAUDIBLE]? ",
    "start": "4017260",
    "end": "4026820"
  },
  {
    "text": "Yeah. Maybe I'll just explain-- maybe I'll talk about\nhow we solve this.",
    "start": "4026820",
    "end": "4031950"
  },
  {
    "text": "So essentially, what\nwe're going to do is we're still going to\nassume that we have these one hot identifiers.",
    "start": "4031950",
    "end": "4037610"
  },
  {
    "text": "And we're going to try to learn\nour representation-- we're basically going to introduce\nan additional model that",
    "start": "4037610",
    "end": "4046070"
  },
  {
    "text": "goes from one hot identifier\nto our task representation. And so, I'm going\nto erase this stuff.",
    "start": "4046070",
    "end": "4058550"
  },
  {
    "start": "4058550",
    "end": "4064140"
  },
  {
    "text": "So we're going to have one\nmodel that goes from our task",
    "start": "4064140",
    "end": "4070519"
  },
  {
    "text": "identifier, or one hot task\nidentifier to a better task representation.",
    "start": "4070520",
    "end": "4076100"
  },
  {
    "text": "We're also going to\nhave a model that goes from this task\nrepresentation to actually",
    "start": "4076100",
    "end": "4082010"
  },
  {
    "text": "solving the task. So this is our execution policy.",
    "start": "4082010",
    "end": "4087870"
  },
  {
    "text": "And then, we're also going to\nhave our exploration policy.",
    "start": "4087870",
    "end": "4097149"
  },
  {
    "text": "And this is going to\nspit out some data that it explored with.",
    "start": "4097149",
    "end": "4103210"
  },
  {
    "text": "And then, we'll\nhave one final model that can try to predict the\ntask from this training data.",
    "start": "4103210",
    "end": "4108886"
  },
  {
    "text": "And instead of\npredicting the one hot, it's just going to\ntry to predict our z. ",
    "start": "4108887",
    "end": "4116859"
  },
  {
    "text": "So this will\nessentially have-- this is like four different models.",
    "start": "4116859",
    "end": "4122120"
  },
  {
    "text": "This is what we'll\nrefer to as q, because it's trying to infer\nthe task from the data. This is just our\nexecution policy.",
    "start": "4122120",
    "end": "4128290"
  },
  {
    "text": "This is our exploration policy. And then, this model right\nhere we'll call capital F.",
    "start": "4128290",
    "end": "4138700"
  },
  {
    "text": "If we can learn\nthese four things, then what we can do at meta-test\ntime is, we will then--",
    "start": "4138700",
    "end": "4149649"
  },
  {
    "text": "we'll take our\nexploration policy, which will give us data.",
    "start": "4149649",
    "end": "4155500"
  },
  {
    "text": "We'll predict z from that data. And then, we'll plug this\ninto our execution policy.",
    "start": "4155500",
    "end": "4165909"
  },
  {
    "text": "And so, by the test time,\nwe're going to throw away this encoder part. This will just be used during\nmeta-training to learn a better",
    "start": "4165910",
    "end": "4172870"
  },
  {
    "text": "task representation. And once we have\nthat, we'll then be able to explore, collect\nthe training data set, get a good task\nrepresentation, and pass that",
    "start": "4172870",
    "end": "4180100"
  },
  {
    "text": "to our execution policy.  So this is kind of a sketch of\nhow things are going to work.",
    "start": "4180100",
    "end": "4186179"
  },
  {
    "text": " And so, in particular, there's\ntwo aspects of meta-training.",
    "start": "4186179",
    "end": "4193889"
  },
  {
    "text": "The first part will be learning\nthis first model right here, which is going to both learn how\nto solve the tasks conditioned",
    "start": "4193890",
    "end": "4200610"
  },
  {
    "text": "on the task ID and also\nlearn a good representation of the task in the process.",
    "start": "4200610",
    "end": "4208270"
  },
  {
    "text": "And so, this will\nessentially just train a policy condition on one hots. We're going to bottleneck\nthe representation",
    "start": "4208270",
    "end": "4214800"
  },
  {
    "text": "on this intermediate variable z. The MDP identifier could\nbe a big one hot vector.",
    "start": "4214800",
    "end": "4220590"
  },
  {
    "text": "It could also be some other\ndescriptive information about the environment. ",
    "start": "4220590",
    "end": "4228300"
  },
  {
    "text": "And then, this is just doing\nstandard one hot conditioned",
    "start": "4228300",
    "end": "4233309"
  },
  {
    "text": "reinforcement learning. And then, for the\nsecond stage, we're going to learn how to\nexplore by trying to recover",
    "start": "4233310",
    "end": "4239730"
  },
  {
    "text": "and be able to predict our z-- our task representation.",
    "start": "4239730",
    "end": "4245170"
  },
  {
    "text": "And so, this will be trying to\ntrain exploration policy that collects data that\nallows you to predict",
    "start": "4245170",
    "end": "4250410"
  },
  {
    "text": "what the task variable is. ",
    "start": "4250410",
    "end": "4257520"
  },
  {
    "text": "Cool. And so, in this first\nstep, like I mentioned, this is basically\njust going to be",
    "start": "4257520",
    "end": "4264679"
  },
  {
    "text": "trying to maximize the\ntask reward with respect to your execution policy. We're also going to\nimpose a distribution on z",
    "start": "4264680",
    "end": "4274190"
  },
  {
    "text": "like we did before. This is just going\nto encourage it to throw information and throw\nout information in the one",
    "start": "4274190",
    "end": "4280895"
  },
  {
    "text": "hot identifier and learn\na representation that's more compressed.",
    "start": "4280895",
    "end": "4286220"
  },
  {
    "text": "And then, for the\nsecond step we need to train our exploration policy\nin order to predict the task",
    "start": "4286220",
    "end": "4293780"
  },
  {
    "text": "representation.  And while you can\ndo these separately,",
    "start": "4293780",
    "end": "4303639"
  },
  {
    "text": "you can also just train\nthem all at once as well. In practice, it will\nprobably learn the first step before it learns\nthe second step.",
    "start": "4303640",
    "end": "4311510"
  },
  {
    "text": "Yeah. [INAUDIBLE] term involved\nin the distribution there, would it just learn\nto [INAUDIBLE]",
    "start": "4311510",
    "end": "4319870"
  },
  {
    "text": "was the one hot\nencoding for the task, if it's not for two [INAUDIBLE]?",
    "start": "4319870",
    "end": "4324980"
  },
  {
    "text": "Yeah, exactly. It could learn an\nidentity mapping. Yeah. So this could just learn\nan identity mapping.",
    "start": "4324980",
    "end": "4330380"
  },
  {
    "text": "And by adding the KL\ndivergence and adding noise, we're going to encourage it to\nnot learn the identity mapping and try to compress\nthat representation.",
    "start": "4330380",
    "end": "4337065"
  },
  {
    "start": "4337065",
    "end": "4344630"
  },
  {
    "text": "Cool. So this is also a\ndecoupled approach in that we're\ntraining the execution policy and the\nexploration policy",
    "start": "4344630",
    "end": "4351310"
  },
  {
    "text": "with different objectives. And we're first going\nto-- we can first train the execution policy\nand then train the exploration",
    "start": "4351310",
    "end": "4356950"
  },
  {
    "text": "policy.  One more detail in terms of\ntraining this exploration",
    "start": "4356950",
    "end": "4364270"
  },
  {
    "text": "policy is, you could use a\nreward similar to the reward that we talked about before,\nwhich would be to essentially",
    "start": "4364270",
    "end": "4375130"
  },
  {
    "text": "be able--  simply try to be\nable to predict z",
    "start": "4375130",
    "end": "4383320"
  },
  {
    "text": "from your training experience. One detail is that\nyou could do this",
    "start": "4383320",
    "end": "4389650"
  },
  {
    "text": "at the very end of the episode. But it would be nice if our\nexploration policy got rewards",
    "start": "4389650",
    "end": "4394659"
  },
  {
    "text": "at every single time step. And so, what you can\ndo is you can actually give it this reward based on all\nthe data it's collected so far",
    "start": "4394660",
    "end": "4404860"
  },
  {
    "text": "and do that at every\nsingle time step. Now, the downside to\nthat is if you do it-- if you do this for the\ndata collected so far,",
    "start": "4404860",
    "end": "4411085"
  },
  {
    "text": "then you're going to\nstart double counting. Because if it had great-- if it explored well\nat time step one but then did random\nstuff after that,",
    "start": "4411085",
    "end": "4417640"
  },
  {
    "text": "then you're still\ngoing to be giving it positive reward for that. And so, what we'll\ndo in practice is we're going to actually\ntry to measure this per step",
    "start": "4417640",
    "end": "4428620"
  },
  {
    "text": "information gain about z. And the way that we can do this\nis have the reward correspond",
    "start": "4428620",
    "end": "4436269"
  },
  {
    "text": "to the prediction error-- comparing the\nprediction error up",
    "start": "4436270",
    "end": "4441520"
  },
  {
    "text": "until the current time\nstep and the prediction error up until the\nprevious time step. And this is going to get\nessentially higher reward",
    "start": "4441520",
    "end": "4449260"
  },
  {
    "text": "if it is able to effectively\nlower the prediction error from the previous time\nstep to the current time step.",
    "start": "4449260",
    "end": "4454750"
  },
  {
    "text": " And this will essentially give\nit a more dense reward to say,",
    "start": "4454750",
    "end": "4460390"
  },
  {
    "text": "at this current time step,\ndid you actually help me gain information about\nwhat the task variable is.",
    "start": "4460390",
    "end": "4466825"
  },
  {
    "text": " Yeah. Is that like adding\na baseline for it?",
    "start": "4466825",
    "end": "4475433"
  },
  {
    "text": "The question was, is this\nlike adding a baseline? It's not quite like\nadding a baseline",
    "start": "4475434",
    "end": "4480780"
  },
  {
    "text": "because it's not a constant\nthat you're adding.",
    "start": "4480780",
    "end": "4486570"
  },
  {
    "text": "So it is essentially just trying\nto prevent double counting. And it's just looking at--",
    "start": "4486570",
    "end": "4492580"
  },
  {
    "text": "you can't actually formalize\nit as information gain. ",
    "start": "4492580",
    "end": "4502200"
  },
  {
    "text": "So this last approach is\nreferred to as a DREAM through a somewhat\nmangled acronym.",
    "start": "4502200",
    "end": "4509315"
  },
  {
    "start": "4509315",
    "end": "4515927"
  },
  {
    "text": "In terms of trying to get\nthe best of both worlds from this approach, we'd love\nto get both a good exploration",
    "start": "4515928",
    "end": "4526190"
  },
  {
    "text": "strategy as well as\nefficiency from learning. We're running low\non time, so I'll go through this pretty quickly.",
    "start": "4526190",
    "end": "4532160"
  },
  {
    "text": "But essentially, you can\nshow that the DREAM objective is consistent with\nend-to-end optimization. And what I mean by this\nis that in the limit",
    "start": "4532160",
    "end": "4540860"
  },
  {
    "text": "as your meta-training data\nkind of goes to infinity,",
    "start": "4540860",
    "end": "4548190"
  },
  {
    "text": "then the expected\nreward of DREAM",
    "start": "4548190",
    "end": "4554600"
  },
  {
    "text": "is going to be essentially\nequal to the expected reward",
    "start": "4554600",
    "end": "4560420"
  },
  {
    "text": "of the end-to-end approach. So essentially, as you get more\nand more meta-training data,",
    "start": "4560420",
    "end": "4567708"
  },
  {
    "text": "the solutions will be the same. You'll get the optimal\nsolution in both cases. ",
    "start": "4567708",
    "end": "4574250"
  },
  {
    "text": "This is under somewhat\nmild assumptions. And so, yeah,\nessentially it means",
    "start": "4574250",
    "end": "4580490"
  },
  {
    "text": "in principle you can recover\nthe optimal strategy. You can also do some theoretical\nanalysis on efficiency as well.",
    "start": "4580490",
    "end": "4589909"
  },
  {
    "text": "Doing analysis on efficiency\nis difficult in general. And so, if you look at a\nmore bandit-like setting,",
    "start": "4589910",
    "end": "4597852"
  },
  {
    "text": "I think I don't have time\nto explain the setup. But essentially, you can show\nthat the sample efficiency",
    "start": "4597852",
    "end": "4602930"
  },
  {
    "text": "of an end-to-end\napproach in orange is very terrible as you make\nthe exploration more difficult.",
    "start": "4602930",
    "end": "4610520"
  },
  {
    "text": "Whereas the sample complexity\nof a DREAM-like approach is much nicer as the exploration\nbecomes more difficult.",
    "start": "4610520",
    "end": "4618722"
  },
  {
    "text": "The dashed lines are showing\nthe theoretical results. And the solid lines are\nshowing the empirical results.",
    "start": "4618722",
    "end": "4624620"
  },
  {
    "text": "And so, you see that the theory\ndoes seem to be indicative. Although it's\nactually, in practice it seems to be an even\nbigger difference.",
    "start": "4624620",
    "end": "4631190"
  },
  {
    "text": " Cool. So to summarize, the end-to-end\napproaches will in principle",
    "start": "4631190",
    "end": "4637480"
  },
  {
    "text": "lead to the optimal strategy. In practice, it's a pretty\nchallenging optimization. The alternative strategies\nare a lot easier to optimize.",
    "start": "4637480",
    "end": "4644710"
  },
  {
    "text": "And many of them are\nfairly principled. But they can also be suboptimal\nby an arbitrarily large amount.",
    "start": "4644710",
    "end": "4651280"
  },
  {
    "text": "And the decoupled\napproach that we saw last will lead to the\noptimal strategy in principle. It's also easy to optimize.",
    "start": "4651280",
    "end": "4657530"
  },
  {
    "text": "So we really get the\nbest of both worlds. It does have this\nrequirement of being able to assign a one\nhot identifier to all",
    "start": "4657530",
    "end": "4664510"
  },
  {
    "text": "of your tasks. In practice, this\nis fairly mild. But it is one thing that\nit is using in order",
    "start": "4664510",
    "end": "4670000"
  },
  {
    "text": "to get the best of both worlds.  We can also empirically compare\nthese different approaches.",
    "start": "4670000",
    "end": "4678340"
  },
  {
    "text": "In this example,\ndifferent tasks correspond to going to different objects. And you need-- the agent\nstarts behind the wall.",
    "start": "4678340",
    "end": "4686560"
  },
  {
    "text": "It needs to-- the optimal\nthing is to go around the wall to read the sign to figure out\nthe color of the correct object",
    "start": "4686560",
    "end": "4691960"
  },
  {
    "text": "and then go to the\ncorrect object. All of it's from\npixel observations",
    "start": "4691960",
    "end": "4697120"
  },
  {
    "text": "and sparse rewards. So it's a pretty\nchallenging problem. And if we look at\nthe average return",
    "start": "4697120",
    "end": "4704080"
  },
  {
    "text": "as a function of\nmeta-training experience, we see that the end-to-end\napproaches shown",
    "start": "4704080",
    "end": "4709570"
  },
  {
    "text": "in yellow, green, and red all\ndo very poorly because it's really hard to learn exploration\nand execution together.",
    "start": "4709570",
    "end": "4718420"
  },
  {
    "text": "We see that using something\nlike posterior sampling is able to do better\nthan these approaches.",
    "start": "4718420",
    "end": "4727000"
  },
  {
    "text": "But it cannot learn the\noptimal exploration strategy because it's going to try to\ngo to objects and get negative rewards when you go\nto the wrong object.",
    "start": "4727000",
    "end": "4734170"
  },
  {
    "text": "And then lastly, the last\napproach that we talked about can efficiently learn\na good approach.",
    "start": "4734170",
    "end": "4740560"
  },
  {
    "text": "And it also starts to\napproach the optimal reward for the task.",
    "start": "4740560",
    "end": "4747520"
  },
  {
    "text": "And you can actually look\nat what the behavior does. And you can see, this is\nthe exploration episode. And it learns to go around the\nwall and then look at the sign.",
    "start": "4747520",
    "end": "4757690"
  },
  {
    "text": "And the execution\npolicy uses this-- the exploration\nepisode infers what",
    "start": "4757690",
    "end": "4763330"
  },
  {
    "text": "the task is and then goes to-- it goes to the right object. ",
    "start": "4763330",
    "end": "4770380"
  },
  {
    "text": "Cool. So that was the\nmain plan for today. A quick roadmap for\nthe remaining lectures.",
    "start": "4770380",
    "end": "4777270"
  },
  {
    "text": "Next week, we'll be talking\nabout offline multitask RL and Hierarchical RL.",
    "start": "4777270",
    "end": "4783360"
  },
  {
    "text": "In the following week, we'll\nhave some really awesome guest lectures. And the week after that we'll be\ntalking about lifelong learning",
    "start": "4783360",
    "end": "4789719"
  },
  {
    "text": "and frontiers. You can essentially\nthink of next week as the final reinforcement\nlearning topics,",
    "start": "4789720",
    "end": "4795120"
  },
  {
    "text": "the following week as some\nlectures on multitask NLP and learned optimizers,\nand then the last week",
    "start": "4795120",
    "end": "4801750"
  },
  {
    "text": "about learning\ntasks sequentially, and then some open\nchallenges in the field. ",
    "start": "4801750",
    "end": "4807720"
  },
  {
    "text": "The next week and the last\nweek are lectures that will be taught by Karl.",
    "start": "4807720",
    "end": "4814770"
  },
  {
    "text": "And the week in between\nare guest lectures. And so, this is my last\nlecture for the course.",
    "start": "4814770",
    "end": "4820357"
  },
  {
    "text": "I just want to thank\neveryone for being kind of an engaging student group. And yeah. I really enjoyed\nteaching everyone.",
    "start": "4820357",
    "end": "4827670"
  },
  {
    "text": "[APPLAUSE] ",
    "start": "4827670",
    "end": "4835135"
  }
]