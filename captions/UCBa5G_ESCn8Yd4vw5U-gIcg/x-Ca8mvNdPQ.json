[
  {
    "start": "0",
    "end": "5690"
  },
  {
    "text": "A couple of logistics. First, the project\nposter session is on Wednesday next week.",
    "start": "5690",
    "end": "11660"
  },
  {
    "text": "We'll be posting\ndetails on Ed very soon. It'll actually be broken up. Because we have so many\nstudents in the class,",
    "start": "11660",
    "end": "16910"
  },
  {
    "text": "it'll actually be broken up\ninto two different sessions, and you'll be assigned to\none of the two sessions.",
    "start": "16910",
    "end": "22022"
  },
  {
    "text": "And so, it'll be\na bunch of details on that coming soon on Ed. The final project report is\ndue in two weeks on Monday.",
    "start": "22022",
    "end": "31588"
  },
  {
    "text": "And then a couple of\nother logistical items from the high-resolution\nfeedback. First, the\nhigh-resolution feedback",
    "start": "31588",
    "end": "38630"
  },
  {
    "text": "from before\nThanksgiving break was about kind of the timing of\nhomework 3 and homework 4,",
    "start": "38630",
    "end": "43940"
  },
  {
    "text": "and so we will revisit that\ntiming for the next offering. And then another\nthing that has come up",
    "start": "43940",
    "end": "50300"
  },
  {
    "text": "a couple of different times\nwas asking for solutions to the homeworks. And we've had a lot of\ndiscussion around it,",
    "start": "50300",
    "end": "58700"
  },
  {
    "text": "and it's something that I\nthink is always a little bit tricky to do.",
    "start": "58700",
    "end": "64010"
  },
  {
    "text": "In particular, the\nthing that's tricky is that we reuse the\nassignments over time,",
    "start": "64010",
    "end": "71600"
  },
  {
    "text": "over different\nquarters, and we do that so that we can have\na very high standard and a high quality\nof the assignments.",
    "start": "71600",
    "end": "78180"
  },
  {
    "text": "But it also means\nthat if we post the solutions this\nquarter, then it",
    "start": "78180",
    "end": "83905"
  },
  {
    "text": "could be that those solutions\nstay floating around, and other students use those\nsolutions for future quarters.",
    "start": "83905",
    "end": "89299"
  },
  {
    "text": "And we have seen in the\npast cases where students have actually posted their\nown solutions online, and students have\ncopied those solutions.",
    "start": "89300",
    "end": "95990"
  },
  {
    "text": "And so, we don't\nwant to incentivize that sort of cheating,\nand so, as a result, we don't post solutions,\nwhich is somewhat unfortunate.",
    "start": "95990",
    "end": "102659"
  },
  {
    "text": "But if you have questions\nabout mistakes on the homework, either about your\ngrade or anything,",
    "start": "102660",
    "end": "108530"
  },
  {
    "text": "really feel free to make a post\non Ed or come to office hours, and we're really happy to\nwalk through the solution",
    "start": "108530",
    "end": "115790"
  },
  {
    "text": "to the homework. And hopefully, that's just\nas good as having access to the solution.",
    "start": "115790",
    "end": "122060"
  },
  {
    "text": "So my plan for today\nis really to talk about kind of the most\nrecent trends and research",
    "start": "122060",
    "end": "129250"
  },
  {
    "text": "that I find really exciting. And so, this is, I\nthink, a pretty fun lecture because, yeah, I\nbasically will talk about some",
    "start": "129250",
    "end": "135962"
  },
  {
    "text": "of the things that I\nthink are really cool that are on the edge of research. This will be including\nmeta learning",
    "start": "135962",
    "end": "141340"
  },
  {
    "text": "for adapting to\ndistribution shift, including adapting\nwith unlabeled examples and adapting with kind of\nlocal descriptions of an edit",
    "start": "141340",
    "end": "149500"
  },
  {
    "text": "that you want to\nmake to a model. And then, I'll also be talking\nabout how we can meta-learn things that are more general\nthan some of the things",
    "start": "149500",
    "end": "157498"
  },
  {
    "text": "that we've seen in\nthe past, and this includes trying to meta-learn\na generic optimizer that can be used for any\ndifferent problem,",
    "start": "157498",
    "end": "164290"
  },
  {
    "text": "as well as trying to meta-learn\nsymmetries in neural network architectures.",
    "start": "164290",
    "end": "169510"
  },
  {
    "text": "And at the very\nend, I'll talk a bit about what I think are some\noutstanding open problems and challenges in\nthe field and, yeah,",
    "start": "169510",
    "end": "179560"
  },
  {
    "text": "how we might start to\nmove towards addressing those challenges.",
    "start": "179560",
    "end": "185210"
  },
  {
    "text": "Cool. So let's first talk a\nlittle bit about adapting to distribution shift, and\nfirst is why this actually",
    "start": "185210",
    "end": "192330"
  },
  {
    "text": "matters at all. I think distribution shift is\na really fascinating problem, and I think that it's really\nan important problem because",
    "start": "192330",
    "end": "202260"
  },
  {
    "text": "of kind of the\nnature of reality. So the current paradigm in\nmachine learning research",
    "start": "202260",
    "end": "207600"
  },
  {
    "text": "is to take a data set, then\ntrain a model on that data set and then evaluate that\nmodel on some held-out data",
    "start": "207600",
    "end": "217470"
  },
  {
    "text": "or something like that. But in reality,\nthings are constantly changing in the world. And so, we see stocks,\nor supply and demand,",
    "start": "217470",
    "end": "224580"
  },
  {
    "text": "or you deploy your system in\na different part of the world. And as a result, the data that\nthe system is often deployed on",
    "start": "224580",
    "end": "231030"
  },
  {
    "text": "is actually different\nfrom the data that it saw during training. And I think we need\nalgorithms that",
    "start": "231030",
    "end": "236849"
  },
  {
    "text": "can handle the fact\nthat the world is changing rather than just being\ndeployed as a static system.",
    "start": "236850",
    "end": "242392"
  },
  {
    "text": "And I also think that\nmeta-learning is actually possibly a very\nuseful tool for trying to address this challenge\nbecause meta-learning",
    "start": "242392",
    "end": "249030"
  },
  {
    "text": "can train systems that\ncan adapt very quickly. And so, we'll see that a little\nbit in some of the upcoming",
    "start": "249030",
    "end": "255780"
  },
  {
    "text": "slides. Now, if this is kind\nof a current reality",
    "start": "255780",
    "end": "261870"
  },
  {
    "text": "that that's faced,\nyou might ask, well, what does industry do? So when actually people use\nmachine learning in practice,",
    "start": "261870",
    "end": "267932"
  },
  {
    "text": "how do they cope with the fact\nthat the world is constantly changing? And you could ask Chip Huyen,\nwho works on machine learning",
    "start": "267932",
    "end": "275670"
  },
  {
    "text": "systems, and she says that\nmachine learning systems degrade quickly over time\nbecause of concept drift",
    "start": "275670",
    "end": "282930"
  },
  {
    "text": "when they're in production. And for online machine\nlearning systems, typically, you just want\nto update them as fast",
    "start": "282930",
    "end": "289050"
  },
  {
    "text": "as humanly possible in\norder to kind of address the fact that they\ndegrade quickly",
    "start": "289050",
    "end": "294090"
  },
  {
    "text": "because of concept drift. And so, I think this is\nactually an interesting thing",
    "start": "294090",
    "end": "303070"
  },
  {
    "text": "to know about because it means\nthat the way that we typically develop machine learning systems\nis under this kind of train",
    "start": "303070",
    "end": "309400"
  },
  {
    "text": "test paradigm and actually,\nthat the way they're being used in practice is\nthis continual training setting, which is a bit\ndifferent from the way",
    "start": "309400",
    "end": "316030"
  },
  {
    "text": "that they were\noriginally intended. And so, maybe that\nmeans if we want to build better machine\nlearning systems and to actually\ndo research that's",
    "start": "316030",
    "end": "324670"
  },
  {
    "text": "relevant to the\nreal world, maybe we should take into\naccount the fact that they need to be updated\nconstantly over time.",
    "start": "324670",
    "end": "331030"
  },
  {
    "text": " Cool. And so, fine-tuning is a\nreally reliable and performant",
    "start": "331030",
    "end": "338710"
  },
  {
    "text": "approach for trying to update\nmodels quickly over time, but it also has\nsome limitations.",
    "start": "338710",
    "end": "345349"
  },
  {
    "text": "So first, it requires you\nto collect labeled data from the new part of\nthe data distribution,",
    "start": "345350",
    "end": "351250"
  },
  {
    "text": "and this can be expensive. It can take time. It may not be available.",
    "start": "351250",
    "end": "356650"
  },
  {
    "text": "Can also be computationally\nexpensive, and especially as you're trying to train\nlarger and larger models,",
    "start": "356650",
    "end": "362380"
  },
  {
    "text": "the amount of\ncompute you'll need to fine-tune that model\nwill, at some point, become somewhat impractical.",
    "start": "362380",
    "end": "369880"
  },
  {
    "text": "And, in general, it's\na fairly blunt tool. And if you only want to make a\nvery small change to your model because you notice an error\nor notice one small thing that",
    "start": "369880",
    "end": "377530"
  },
  {
    "text": "changed, fine-tuning isn't\nnecessarily the best tool to make a very precise and\nsmall change to your model.",
    "start": "377530",
    "end": "385820"
  },
  {
    "text": "And so, yeah, what\nI'll talk about next is how we might be able\nto leverage meta-learning to actually address\nsome of these weaknesses",
    "start": "385820",
    "end": "392290"
  },
  {
    "text": "of fine-tuning. Cool. So first, we'll focus\non domain shift,",
    "start": "392290",
    "end": "398540"
  },
  {
    "text": "and this is something-- a kind\nof distribution shift that we saw in the domain adaptation and\ndomain generalization lectures.",
    "start": "398540",
    "end": "405308"
  },
  {
    "text": "And then, after we talk\nabout domain shift, we'll talk about a more general\nkind of distribution shift that's called concept shift.",
    "start": "405308",
    "end": "413100"
  },
  {
    "text": "And so, to recap,\nin domain shift, you have some sort of\ncategorical domain variable,",
    "start": "413100",
    "end": "419775"
  },
  {
    "text": "and this could correspond\nto different users, to different locations,\ndifferent times of day,",
    "start": "419775",
    "end": "426120"
  },
  {
    "text": "and oftentimes, this sort\nof domain information can be derived from\nmetadata that already exists",
    "start": "426120",
    "end": "433289"
  },
  {
    "text": "in your training data set. And then, what\ndomain shift looks",
    "start": "433290",
    "end": "439125"
  },
  {
    "text": "like is you assume that\nyour training data is from this distribution\nright here and your test data is from\nthe same distribution,",
    "start": "439125",
    "end": "446670"
  },
  {
    "text": "but where the only\nthing that has changed is the distribution of this\nunderlying domain variable.",
    "start": "446670",
    "end": "453790"
  },
  {
    "text": "And this kind of distribution\nshift is fairly general, and this is the kind of\nthe shift that we'll look",
    "start": "453790",
    "end": "460050"
  },
  {
    "text": "at in this first part, but there\nare also some things that it cannot capture. ",
    "start": "460050",
    "end": "467090"
  },
  {
    "text": "Now, one approach, I guess--\nso we saw some approaches for handling domain shift in\nmy lecture on domain adaptation",
    "start": "467090",
    "end": "475910"
  },
  {
    "text": "and Huashi's lecture on\ndomain generalization. One approach that we didn't\nreally talk too much about is called distributionally\nrobust optimization.",
    "start": "475910",
    "end": "484699"
  },
  {
    "text": "And the way that this\napproach works is you try to form some adversarial\ndistribution over your domain",
    "start": "484700",
    "end": "492260"
  },
  {
    "text": "variable and then optimize for\nthe worst-case distribution.",
    "start": "492260",
    "end": "497900"
  },
  {
    "text": "So, in particular,\nyou can formulate kind of a adversarial\noptimization, where you are trying to\nfind a model that",
    "start": "497900",
    "end": "504860"
  },
  {
    "text": "does well under the worst-case\ndistribution over domains.",
    "start": "504860",
    "end": "510139"
  },
  {
    "text": "And if you have a\ncategorical domain variable, this is actually fairly\nsimple to evaluate. You simply pick the domain that\nthe model is doing the worst",
    "start": "510140",
    "end": "517270"
  },
  {
    "text": "on in your training\ndata set and then optimize for the\nperformance on that domain,",
    "start": "517270",
    "end": "524530"
  },
  {
    "text": "and then iterate that process. And this sort of\nadversarial optimization",
    "start": "524530",
    "end": "529980"
  },
  {
    "text": "can enable a robustness\nto different domains. It can also be less pessimistic\nthan adversarial robustness,",
    "start": "529980",
    "end": "537672"
  },
  {
    "text": "which is another\nkind of robustness that you might have heard about. But it can often sacrifice\nthe average performance",
    "start": "537672",
    "end": "545519"
  },
  {
    "text": "or the empirical performance\nbecause it's actually quite pessimistic about the\ndata that it will be seeing",
    "start": "545520",
    "end": "550680"
  },
  {
    "text": "or the domain that it will\nbe seeing at test time. And so, what we're\ngoing to try to do",
    "start": "550680",
    "end": "556079"
  },
  {
    "text": "is actually kind of formulate\nan alternative paradigm that",
    "start": "556080",
    "end": "561540"
  },
  {
    "text": "is a little bit less\npessimistic but still allows us to be kind of fairly robust. And the way that\nwe're going to be",
    "start": "561540",
    "end": "567750"
  },
  {
    "text": "doing that is\ninstead of optimizing for the worst-case\ndomain, we're going to try to actually optimize\nfor domain performance",
    "start": "567750",
    "end": "575370"
  },
  {
    "text": "after having adapted a small\namount to each given domain. ",
    "start": "575370",
    "end": "582040"
  },
  {
    "text": "And so, specifically,\nwhat I mean by that is we're going to assume\nthat we have unlabeled data",
    "start": "582040",
    "end": "589080"
  },
  {
    "text": "from the test domain. This could be the unlabeled\ndata from a new user,",
    "start": "589080",
    "end": "594640"
  },
  {
    "text": "from a new time of\nday, or a new place. And so, if you want to, for\nexample, recognize handwriting,",
    "start": "594640",
    "end": "601780"
  },
  {
    "text": "this is from the Federated\nextended MNIST data set. This is all from one user. You might have this\nbatch of unlabeled data",
    "start": "601780",
    "end": "608080"
  },
  {
    "text": "from that one user. And then, your goal is to\ntake this unlabeled data, adapt your model to that\nuser, and then infer",
    "start": "608080",
    "end": "616029"
  },
  {
    "text": "the labels for all the\nexamples in that batch.",
    "start": "616030",
    "end": "621980"
  },
  {
    "text": "And so, this is going to be\ndifferent from other approaches in a few different ways. First, unlike\nfine-tuning, this only",
    "start": "621980",
    "end": "628520"
  },
  {
    "text": "needs unlabeled data in order\nto adapt the model rather than labeled data.",
    "start": "628520",
    "end": "634070"
  },
  {
    "text": "Second, unlike things\nlike domain generalization and domain adaptation,\nwe're not going to be assuming access\nto this unlabeled data",
    "start": "634070",
    "end": "641780"
  },
  {
    "text": "during training time. We're actually just\ngoing to adapt the model on the fly at test time.",
    "start": "641780",
    "end": "647089"
  },
  {
    "text": " You really-- the main\nassumption that this is--",
    "start": "647090",
    "end": "652868"
  },
  {
    "text": "I guess the main two assumptions\nthat it's going to be making is that first, you have\ndomain labels in your training data set, and\nsecond, at test time,",
    "start": "652868",
    "end": "660470"
  },
  {
    "text": "you have a batch of test\ninputs from the same group or from the same domain\navailable at once",
    "start": "660470",
    "end": "666830"
  },
  {
    "text": "rather than just having\na single example. This is a little bit different\nfrom the standard machine learning setting, where you just\nassume that you have just one",
    "start": "666830",
    "end": "674270"
  },
  {
    "text": "input at a time at test time. So this is the problem setting. ",
    "start": "674270",
    "end": "680570"
  },
  {
    "text": "And we kind of refer\nto this problem setting as adaptive risk\nminimization in the sense that you are--",
    "start": "680570",
    "end": "685940"
  },
  {
    "text": "instead of just evaluating\nrisk directly on the examples that you're given, you\nactually have an opportunity to adapt with a small\namount of examples.",
    "start": "685940",
    "end": "695140"
  },
  {
    "text": "And then there's the question\nof actually how to optimize for this and how\nto actually develop a method that can allow you to\nadapt with just unlabeled data",
    "start": "695140",
    "end": "702400"
  },
  {
    "text": "from a new part of\nthe distribution. And so to do this,\nwe could actually train a model for\nfew-shot adaptation",
    "start": "702400",
    "end": "709027"
  },
  {
    "text": "to the different domains\nin the training data set, and this will just use\nbasically the same meta-learning",
    "start": "709027",
    "end": "714290"
  },
  {
    "text": "algorithms that you saw in the\nearlier parts of the course.",
    "start": "714290",
    "end": "719365"
  },
  {
    "text": "The key thing that's\ndifferent, though, is instead of doing\nfew-shot labeled adaptation,",
    "start": "719365",
    "end": "725690"
  },
  {
    "text": "we're only given unlabeled data. And so, we want to\nbe able to adapt with these unlabeled examples\nrather than a few labeled",
    "start": "725690",
    "end": "731895"
  },
  {
    "text": "examples.  And so there's a few different\nways that you can do this.",
    "start": "731895",
    "end": "738139"
  },
  {
    "text": "One is you could use\nthe MAML algorithm. But because you can't compute\na standard cross-entropy loss",
    "start": "738140",
    "end": "745360"
  },
  {
    "text": "function, you can meta-learn\na loss function and use--",
    "start": "745360",
    "end": "750385"
  },
  {
    "text": "meta-learn that\nloss function such that when you adapt\nwith that loss function on the\nunlabeled data, you",
    "start": "750385",
    "end": "756010"
  },
  {
    "text": "do well on each of the\ndomains in the training set. And so what this\nlooks like is you",
    "start": "756010",
    "end": "762010"
  },
  {
    "text": "are going to be meta-learning\nboth the initial parameters, as well as the parameters\nof some loss function,",
    "start": "762010",
    "end": "769300"
  },
  {
    "text": "also represented by\na neural network. You, in the inner\nloop, use that loss",
    "start": "769300",
    "end": "774970"
  },
  {
    "text": "function to update the network\nfrom the initial parameters to get a new network, and\nthen optimize this whole thing",
    "start": "774970",
    "end": "782170"
  },
  {
    "text": "end-to-end such that you\nget good performance on each of the training domains.",
    "start": "782170",
    "end": "787950"
  },
  {
    "text": "So that's one way to\nadapt with unlabeled data. Alternatively, you could\nalso use a black box approach and simply basically pass in\nall of the unlabeled examples",
    "start": "787950",
    "end": "797180"
  },
  {
    "text": "into a neural network\nthat produces some context and use that context to make\npredictions for your examples.",
    "start": "797180",
    "end": "803972"
  },
  {
    "text": "And this sort of\nblack box approach is actually quite\nstraightforward because the only thing you need\nto do to modify a black box",
    "start": "803972",
    "end": "813110"
  },
  {
    "text": "approach to work\nwithout labels is to just remove the labels when\nyou pass the data as input. ",
    "start": "813110",
    "end": "819908"
  },
  {
    "text": "There's a few\ndifferent architectures that you could use for this\nsort of black box approach.",
    "start": "819908",
    "end": "826010"
  },
  {
    "text": "One is the architecture\nthat's shown here, where you just kind of\npredict this context",
    "start": "826010",
    "end": "831290"
  },
  {
    "text": "and pass that into the network. Alternatively, this\ncontext could actually be computed in a different way.",
    "start": "831290",
    "end": "838400"
  },
  {
    "text": "One really simple way\nto compute this context would just have the\ncontext correspond to batch form statistics.",
    "start": "838400",
    "end": "843589"
  },
  {
    "text": "And this would mean\nthat you're basically passing all of your\nexamples in as a batch,",
    "start": "843590",
    "end": "850130"
  },
  {
    "text": "and instead of using the\nbatch form statistics from the training\ndata set, you're going to be computing them\non the fly at test time",
    "start": "850130",
    "end": "859129"
  },
  {
    "text": "using all of your examples.  Cool.",
    "start": "859130",
    "end": "864490"
  },
  {
    "text": "Any questions on how this works? Yeah. How do we train a loss\nfunction and the parameters",
    "start": "864490",
    "end": "871180"
  },
  {
    "text": "at the same time because is\nthere like a loss for the loss function? Yeah, so the question\nis, how do we",
    "start": "871180",
    "end": "876580"
  },
  {
    "text": "train the initialization and the\nloss function at the same time? One really critical\nthing is that we're only",
    "start": "876580",
    "end": "881950"
  },
  {
    "text": "going to be training\nthe inner loss function, and the outer loss function\nis going to stay the same.",
    "start": "881950",
    "end": "887180"
  },
  {
    "text": "And so if your inner\nloop update is something",
    "start": "887180",
    "end": "892450"
  },
  {
    "text": "like theta minus alpha grad\nL, the parameters of L,",
    "start": "892450",
    "end": "897760"
  },
  {
    "text": "this neural network\nis parameterized by phi that takes as input\nthe unlabeled examples.",
    "start": "897760",
    "end": "903400"
  },
  {
    "text": " I guess, in this case, there's\nonly three unlabeled examples.",
    "start": "903400",
    "end": "910640"
  },
  {
    "text": "This is the inner loop\nupdate, and then we're going to be evaluating this in\nterms of how well these updated",
    "start": "910640",
    "end": "918250"
  },
  {
    "text": "parameters do on the\nlabeled examples, which",
    "start": "918250",
    "end": "923800"
  },
  {
    "text": "will be x1 to 3, and the\ncorresponding labels. And this outer loss\nfunction will correspond",
    "start": "923800",
    "end": "929920"
  },
  {
    "text": "to just a standard\ncross-entropy loss function, and we could optimize\nthis entire thing",
    "start": "929920",
    "end": "936220"
  },
  {
    "text": "over both the initial\nparameters and the loss function parameters.",
    "start": "936220",
    "end": "942450"
  },
  {
    "text": "Cool. Any other questions? ",
    "start": "942450",
    "end": "951820"
  },
  {
    "text": "Cool. So there's a lot of different\nexperiments in the paper, but I'll mention just\na couple of them.",
    "start": "951820",
    "end": "956920"
  },
  {
    "text": "The first is the Federated\nExtended MNIST example that I showed on\nthe previous slide. And in this case,\nthe experiments",
    "start": "956920",
    "end": "963750"
  },
  {
    "text": "are all trying to\nadapt to new users that have different handwriting,\nonly with unlabeled data",
    "start": "963750",
    "end": "969660"
  },
  {
    "text": "available at test\ntime from those users. The experiments compare to a\nnumber of different methods.",
    "start": "969660",
    "end": "978040"
  },
  {
    "text": "One of them is\njust standard ERM. We could also compare to the\ngroup robustness approach",
    "start": "978040",
    "end": "983339"
  },
  {
    "text": "that I mentioned before. We can compare it to domain\nadversarial training, which was covered in Huashi's\nlecture and also compared it",
    "start": "983340",
    "end": "993089"
  },
  {
    "text": "to a really simple upweighting\napproach that will just upweight the data\nfrom each of the users to be a uniform distribution\nso that you're sampling data",
    "start": "993090",
    "end": "1002540"
  },
  {
    "text": "from them equally.  Cool. And then, we can\nlook at performance,",
    "start": "1002540",
    "end": "1008912"
  },
  {
    "text": "both in terms of\nthe average test performance on these new users\nas well as the performance",
    "start": "1008912",
    "end": "1014300"
  },
  {
    "text": "on the worst user. And this sort of\nworst-case performance is useful because it will\ngive you a notion of fairness.",
    "start": "1014300",
    "end": "1020870"
  },
  {
    "text": "Are you doing really terribly\non some users and much better on other users? And it will also\ngive you a notion",
    "start": "1020870",
    "end": "1026300"
  },
  {
    "text": "of robustness, such that if\nyour distribution changes more towards those worst-case\nusers, what will",
    "start": "1026300",
    "end": "1032390"
  },
  {
    "text": "your performance look like? And if we look at\nthe numbers, first, in terms of average\naccuracy, we see",
    "start": "1032390",
    "end": "1038599"
  },
  {
    "text": "that domain adversarial neural\nnetworks actually gives you somewhat better performance than\nempirical risk minimization,",
    "start": "1038599",
    "end": "1045500"
  },
  {
    "text": "but that we can get around\na 5% improvement by actually adopting, in this case,\nwith a context variable,",
    "start": "1045500",
    "end": "1052280"
  },
  {
    "text": "in comparison to just trying to\ntrain a single neural network. Then, in terms of\nworst-case accuracy,",
    "start": "1052280",
    "end": "1058370"
  },
  {
    "text": "we also see around a 5%\nimprovement, as well, over the best existing\nmethod, in this case,",
    "start": "1058370",
    "end": "1065850"
  },
  {
    "text": "which corresponded to either\nupweighting or the domain adversarial training.",
    "start": "1065850",
    "end": "1071890"
  },
  {
    "text": "Now, we can actually\nqualitatively look at what it's doing here. And so here's one\nexample where--",
    "start": "1071890",
    "end": "1079150"
  },
  {
    "text": "this is an image that\nthe system is given, and if you ask the neural\nnetwork trained with ERM,",
    "start": "1079150",
    "end": "1085090"
  },
  {
    "text": "it will tell you\nthat this is a 2. And similarly, if you ask\nkind of the adaptive risk",
    "start": "1085090",
    "end": "1094240"
  },
  {
    "text": "minimization, the meta-learned\nmodel that used a context variable, if we give\nit these two examples and ask it what the label is,\nit will also say it's a 2.",
    "start": "1094240",
    "end": "1104140"
  },
  {
    "text": "But if you actually give it a\nlittle bit even more context, if you give it all of\nthese unlabeled examples,",
    "start": "1104140",
    "end": "1110173"
  },
  {
    "text": "it can correctly figure\nout that this actually corresponds to a lowercase a. And the reason why\nit can do that is it",
    "start": "1110173",
    "end": "1115990"
  },
  {
    "text": "can look at other examples,\nfor example, this example, and realize that this\nparticular user often",
    "start": "1115990",
    "end": "1121360"
  },
  {
    "text": "writes their 2 without a\nloop, and therefore, this must be a lowercase\na rather than a 2.",
    "start": "1121360",
    "end": "1128287"
  },
  {
    "text": "And so essentially,\nwhat the system is doing is it's adapting to\nthis particular user, adapting the model\nto the user, such",
    "start": "1128287",
    "end": "1134440"
  },
  {
    "text": "that it can more accurately\nmake predictions for that user. ",
    "start": "1134440",
    "end": "1142740"
  },
  {
    "text": "Cool. One other experiment\nthat I'll mention was looking at adapting to\ndifferent image corruptions.",
    "start": "1142740",
    "end": "1148840"
  },
  {
    "text": "And so this is using the CIFAR-C\nand TinyImageNet-C data sets, was trained using 56\ndifferent corruptions,",
    "start": "1148840",
    "end": "1154800"
  },
  {
    "text": "and then testing its ability\nto adapt to images that",
    "start": "1154800",
    "end": "1159900"
  },
  {
    "text": "were corrupted with 22\nother corruptions that weren't seen during training.",
    "start": "1159900",
    "end": "1166110"
  },
  {
    "text": "And again, here, we see that\nthe domain adversarial neural",
    "start": "1166110",
    "end": "1171130"
  },
  {
    "text": "network approach is one of\nthe best-performing prior approaches, although\nit actually really",
    "start": "1171130",
    "end": "1178360"
  },
  {
    "text": "is performing somewhat similarly\nto standard empirical risk minimization. And in contrast, by actually\nadapting and training",
    "start": "1178360",
    "end": "1186970"
  },
  {
    "text": "for fast adaptation\nwith unlabeled data, we're able to get\naround a 2% to 3% improvement in average\naccuracy and a 7% to 9%",
    "start": "1186970",
    "end": "1194559"
  },
  {
    "text": "improvement in worst\ngroup accuracy. Yeah.",
    "start": "1194560",
    "end": "1200620"
  },
  {
    "text": "[INAUDIBLE] when\nyou say corruptions,",
    "start": "1200620",
    "end": "1209050"
  },
  {
    "text": "are there still similar\nkind of corruptions let's say if I test like\nImageNet [INAUDIBLE]",
    "start": "1209050",
    "end": "1217403"
  },
  {
    "text": "is it able to handle a very\ndifferent kind of distribution shift? Yeah, that's a great question. So first, in terms\nof the corruptions,",
    "start": "1217403",
    "end": "1225790"
  },
  {
    "text": "we construct more corruptions\nby also considering the severity of the corruption, as well.",
    "start": "1225790",
    "end": "1231170"
  },
  {
    "text": "And so, I think that's how we\nget to 78 corruptions here. And then, in terms of\nbeing able to handle very different corruptions,\nI think that it may still",
    "start": "1231170",
    "end": "1240970"
  },
  {
    "text": "help somewhat.  But I do think that the\nimprovements will be less",
    "start": "1240970",
    "end": "1247270"
  },
  {
    "text": "than the improvements that\nyou get from corruptions that are a little bit more like\nthe one seen during training. And that's because we're\ntraining for adaptation",
    "start": "1247270",
    "end": "1254710"
  },
  {
    "text": "performance on a\nset of corruptions, and if you give\nit something-- it has to adapt to something\nthat's very different from that,",
    "start": "1254710",
    "end": "1261490"
  },
  {
    "text": "it won't necessarily generalize. ",
    "start": "1261490",
    "end": "1267270"
  },
  {
    "text": "Yeah. Does this loopback [INAUDIBLE]\non higher-severity corruptions, as well?",
    "start": "1267270",
    "end": "1272790"
  },
  {
    "text": "Was there an\nexperiment [INAUDIBLE] We didn't specifically\nhave any experiment",
    "start": "1272790",
    "end": "1278460"
  },
  {
    "text": "that was looking\nat how performance does as the severity changes.",
    "start": "1278460",
    "end": "1284159"
  },
  {
    "text": "My guess would be\nthat it does help more when the severity\nincreases because ERM",
    "start": "1284160",
    "end": "1292080"
  },
  {
    "text": "does much better than these\nwhen there's no corruption.",
    "start": "1292080",
    "end": "1298967"
  },
  {
    "text": "And I don't think these\nmethods would help when there's no corruption, of course. So my guess is that\nit would be better",
    "start": "1298968",
    "end": "1304590"
  },
  {
    "text": "with more severe corruptions,\nbut it's also possible that you might see\na bit of a fall-off if it's just too corrupted\nto be able to get any signal.",
    "start": "1304590",
    "end": "1311730"
  },
  {
    "text": " Cool. So the takeaway\nhere is that we can",
    "start": "1311730",
    "end": "1318180"
  },
  {
    "text": "use meta-learning to adapt\nto distribution shift, and specifically to adapt to\ndifferent kinds of domains",
    "start": "1318180",
    "end": "1327630"
  },
  {
    "text": "only using unlabeled data. Now, what about other forms\nof distribution shift?",
    "start": "1327630",
    "end": "1335030"
  },
  {
    "text": "Specifically, next, we'll look\nat a kind of distribution shift that's called concept\ndrift or concept shift.",
    "start": "1335030",
    "end": "1342110"
  },
  {
    "text": "And this is when basically,\nkind of the entire P of y given",
    "start": "1342110",
    "end": "1348350"
  },
  {
    "text": "x distribution can change. And actually, I guess\nin a lot of the examples",
    "start": "1348350",
    "end": "1353840"
  },
  {
    "text": "that we'll be looking at, P of\nx is not going to be changing. Only P of y given x is\ngoing to be changing.",
    "start": "1353840",
    "end": "1359750"
  },
  {
    "text": " And I should note that when\nP of y given x is changing,",
    "start": "1359750",
    "end": "1368580"
  },
  {
    "text": "you're going to need\nsome sort of supervision in order to adapt to\nthe distribution shift",
    "start": "1368580",
    "end": "1373860"
  },
  {
    "text": "because P of y\ngiven x could change in a lot of different\nkinds of ways.",
    "start": "1373860",
    "end": "1380238"
  },
  {
    "text": "And specifically,\nin this case, we'll look at handling this\nkind of distribution shift",
    "start": "1380238",
    "end": "1385320"
  },
  {
    "text": "in the context of\nlanguage models, although a lot of the methods\nthat we'll be looking at are applicable beyond\nlanguage models, as well.",
    "start": "1385320",
    "end": "1394139"
  },
  {
    "text": "And as a motivating example,\nif you take a version of GPT-3 that was accessed earlier\nthis year, and you ask it,",
    "start": "1394140",
    "end": "1403559"
  },
  {
    "text": "what is the largest\nEnglish-speaking EU nation? It will tell you the answer\nis the United Kingdom,",
    "start": "1403560",
    "end": "1410670"
  },
  {
    "text": "but, of course, the UK\nis no longer in the EU, and so this answer\nisn't correct, and the answer\nshould be Ireland.",
    "start": "1410670",
    "end": "1416940"
  },
  {
    "text": "If you ask it who is\nAlgeria's current president, it will also give you an\nanswer that is out of date.",
    "start": "1416940",
    "end": "1424410"
  },
  {
    "text": "And likewise, if you\nask it the club team that Lionel Messi plays for,\nit will give you something",
    "start": "1424410",
    "end": "1430260"
  },
  {
    "text": "that's out of date, as well. And so, there's a\nquestion of what should we",
    "start": "1430260",
    "end": "1436397"
  },
  {
    "text": "do in this scenario. What should we do to\ntry to actually update something like GPT-3\nin order to allow",
    "start": "1436397",
    "end": "1444520"
  },
  {
    "text": "it to make correct predictions\non things about the world that are changing?",
    "start": "1444520",
    "end": "1450315"
  },
  {
    "text": "Unfortunately, it\nwould be really expensive to have to retrain\nGPT-3 or even fine-tune GPT-3.",
    "start": "1450315",
    "end": "1456940"
  },
  {
    "text": "It would probably\nbe fairly expensive. And so, what we'd\nlike to be able to do is try to figure out how to\nkeep these large models up",
    "start": "1456940",
    "end": "1463030"
  },
  {
    "text": "to date without\nhaving to fine-tune or without having to completely\nretrain the entire model.",
    "start": "1463030",
    "end": "1469840"
  },
  {
    "text": " And so this is where the\nkind of framework of editing",
    "start": "1469840",
    "end": "1476490"
  },
  {
    "text": "can come in. What would be really nice is if\nwe could take our base model.",
    "start": "1476490",
    "end": "1481680"
  },
  {
    "text": "If you ask it who is the\nprime minister of the UK, it will tell you the answer\nis Theresa May, which was, of course, correct\nbefore, but that's",
    "start": "1481680",
    "end": "1488550"
  },
  {
    "text": "also like three prime\nministers ago now. And what would be really nice\nis if we could take this example",
    "start": "1488550",
    "end": "1496427"
  },
  {
    "text": "and tell it that the answer\nshould be Rishi Sunak, and pass that to a model\neditor, and then get",
    "start": "1496427",
    "end": "1504990"
  },
  {
    "text": "an edited model that can give\nus the correct answer, including a correct answer to\nrephrasings of the questions",
    "start": "1504990",
    "end": "1511530"
  },
  {
    "text": "or other things that are in\nscope like, is Rishi Sunak the prime minister of the UK? ",
    "start": "1511530",
    "end": "1519490"
  },
  {
    "text": "Furthermore, we\ndon't just want it to give us the correct answer\nfor inputs that are in scope.",
    "start": "1519490",
    "end": "1525070"
  },
  {
    "text": "We also want to be able to ask\nit things that are out of scope and unrelated, like who does\nMessi play for, and still have",
    "start": "1525070",
    "end": "1533170"
  },
  {
    "text": "it give us the correct answer. So we don't want to\nhave it affect things that are unrelated to the edit. ",
    "start": "1533170",
    "end": "1541809"
  },
  {
    "text": "And so specifically, to try\nto scope out this problem a little bit more,\nmaybe your edit example",
    "start": "1541810",
    "end": "1546880"
  },
  {
    "text": "is, who is the prime\nminister of the UK? Our scope is the space of inputs\nthat are related to that input,",
    "start": "1546880",
    "end": "1558670"
  },
  {
    "text": "and that would include\nrephrasings of the question. And there are also examples\nthat are out of scope, like why is the sky blue?",
    "start": "1558670",
    "end": "1565029"
  },
  {
    "text": " It's also worth mentioning that\nthere are some examples that",
    "start": "1565030",
    "end": "1570060"
  },
  {
    "text": "are in-scope and\nout-of-scope that are going to be much harder, like\nwhere is Rishi Sunak the PM? This is going to be\nin-scope, but it's",
    "start": "1570060",
    "end": "1576390"
  },
  {
    "text": "going to be a little bit further\nand more difficult than just a rephrasing of the question.",
    "start": "1576390",
    "end": "1581550"
  },
  {
    "text": "And likewise, there are\nthings that seemingly are somewhat related, but\nshouldn't be changed when",
    "start": "1581550",
    "end": "1586890"
  },
  {
    "text": "we make an edit to the model. ",
    "start": "1586890",
    "end": "1592640"
  },
  {
    "text": "And these are actually really\nthe most challenging cases, and it's important to-- when we actually get to\nsome of the experiments,",
    "start": "1592640",
    "end": "1598450"
  },
  {
    "text": "we're actually\ngoing to be trying to focus on evaluating\non these examples rather than the easy examples,\nlike why is the sky blue?",
    "start": "1598450",
    "end": "1605350"
  },
  {
    "text": "Yeah. [INAUDIBLE] ",
    "start": "1605350",
    "end": "1612965"
  },
  {
    "text": "Yeah, so what about\nprompts that aren't in the form of the question? ",
    "start": "1612965",
    "end": "1621080"
  },
  {
    "text": "I guess-- so, in\nprinciple, yes, you could have some of those things\nbe in-scope or out-of-scope,",
    "start": "1621080",
    "end": "1626300"
  },
  {
    "text": "and you don't just\nhave to have this for a question-answering model. You could have it be for\njust general language models.",
    "start": "1626300",
    "end": "1634100"
  },
  {
    "text": "And if you have a model that\ncan do both question answering and continuations,\nthen you could",
    "start": "1634100",
    "end": "1640669"
  },
  {
    "text": "have those kinds of things be\npart of this picture, as well.",
    "start": "1640670",
    "end": "1646040"
  },
  {
    "text": "Yeah. What about [INAUDIBLE]",
    "start": "1646040",
    "end": "1652790"
  },
  {
    "text": "Yeah. So that would also\nbe in-scope, as well. And you could also\nimagine, I mean, getting along the lines\nof other kinds of inputs.",
    "start": "1652790",
    "end": "1660722"
  },
  {
    "text": "You could also have something\nlike a picture of Theresa May and say, is this person\nthe prime minister? And, in principle,\nthat would also",
    "start": "1660722",
    "end": "1666277"
  },
  {
    "text": "be in-scope, as well, if\nyou're considering models that can take that sort of input. ",
    "start": "1666277",
    "end": "1674370"
  },
  {
    "text": "Cool. So this is what we'd\nlike to be able to do. And we can formulate this\nas a meta-learning problem.",
    "start": "1674370",
    "end": "1683770"
  },
  {
    "text": "And so, there was\nthis really cool paper by Sinitsin et al. in 2020\nthat framed this problem",
    "start": "1683770",
    "end": "1691289"
  },
  {
    "text": "as a meta-learning problem\nof trying to actually train a neural network that could be\nedited in very specific ways.",
    "start": "1691290",
    "end": "1698490"
  },
  {
    "text": "Unfortunately-- and we'll\ntalk about the formulation from that paper. Unfortunately, this kind of\nspecific method in the ICLR",
    "start": "1698490",
    "end": "1706260"
  },
  {
    "text": "2020 paper wasn't particularly\nscalable to large language models, and so, in terms of the\nmethods that we'll be covering,",
    "start": "1706260",
    "end": "1712680"
  },
  {
    "text": "we'll be covering some\nmore recent work that is much more scalable. But this was the\nwork that introduced",
    "start": "1712680",
    "end": "1717990"
  },
  {
    "text": "the paradigm of doing this\nin the meta-learning setting.",
    "start": "1717990",
    "end": "1723429"
  },
  {
    "text": "And so, to frame this as\na meta-learning problem, we are going to assume\nthat we have the ability",
    "start": "1723430",
    "end": "1729930"
  },
  {
    "text": "to collect a data set\nthat shows us examples of how editing should be done.",
    "start": "1729930",
    "end": "1735830"
  },
  {
    "text": "And in particular, this data\nset will contain, first, a descriptor of the edit.",
    "start": "1735830",
    "end": "1742760"
  },
  {
    "text": "This could be an input-output\npair, like who is the UK PM? And the answer is Rishi Sunak.",
    "start": "1742760",
    "end": "1747800"
  },
  {
    "text": " It will also include\nan example of something",
    "start": "1747800",
    "end": "1753020"
  },
  {
    "text": "that's out of scope,\nand this is going to try to enforce the fact\nthat the model shouldn't change",
    "start": "1753020",
    "end": "1761059"
  },
  {
    "text": "on these out-of-scope examples. And it'll also include examples\nof in-scope input-output pairs.",
    "start": "1761060",
    "end": "1768380"
  },
  {
    "text": "And so, in particular,\nif it's given an example like the\nprime minister of the UK is currently who?",
    "start": "1768380",
    "end": "1773570"
  },
  {
    "text": "It'll be told what the new\nanswer to that question should be.",
    "start": "1773570",
    "end": "1779280"
  },
  {
    "text": "So this edit data\nset will essentially be something that we'll use\nto teach a model editor how",
    "start": "1779280",
    "end": "1785150"
  },
  {
    "text": "to edit a model. Collecting this edit\ndata set is going to be a nontrivial effort,\nalthough, in general, if we",
    "start": "1785150",
    "end": "1794503"
  },
  {
    "text": "collect a general\nenough edit data set that has examples of lots\nof different kinds of things that we might want\nto do to edit models,",
    "start": "1794503",
    "end": "1801650"
  },
  {
    "text": "then we only need\nto train a single-- we only need to use it\nonce and collect it once",
    "start": "1801650",
    "end": "1807290"
  },
  {
    "text": "in order to train a single model\neditor for making all sorts of edits to a large model.",
    "start": "1807290",
    "end": "1813125"
  },
  {
    "text": " Cool. And then once we\nhave the data set,",
    "start": "1813125",
    "end": "1819260"
  },
  {
    "text": "there are a few different\nways that we could train one of these model editors. The first approach\nis going to try",
    "start": "1819260",
    "end": "1826600"
  },
  {
    "text": "to actually change the\ngradients and actually update the weights of the model. And so, what we'll do is we'll\ntake as input the gradient that",
    "start": "1826600",
    "end": "1835150"
  },
  {
    "text": "corresponds to the gradient\nof trying to fine-tune the model on that\none edit example,",
    "start": "1835150",
    "end": "1840550"
  },
  {
    "text": "and we'll try to\ntransform that fine-tuning gradient into a better\nupdate for the model.",
    "start": "1840550",
    "end": "1847520"
  },
  {
    "text": "And so, this is kind of\ncaptured by this picture, where if you have your\npre-trained model, and you want to\nmake an edit to it,",
    "start": "1847520",
    "end": "1853960"
  },
  {
    "text": "then you compute what\nthe gradient would be if you were to\nfine-tune on that edit",
    "start": "1853960",
    "end": "1859770"
  },
  {
    "text": "and then pass that gradient\ninto this Model Editor that will then give you\nan updated gradient",
    "start": "1859770",
    "end": "1867210"
  },
  {
    "text": "that you'll actually\nuse to edit the model. And once you apply this kind of\nrevised version of the gradient",
    "start": "1867210",
    "end": "1876020"
  },
  {
    "text": "to the model, the\ngoal will be for it to give you the correct answer\nfor these edits and also,",
    "start": "1876020",
    "end": "1883380"
  },
  {
    "text": "of course, not affect the\nmodel output on other examples.",
    "start": "1883380",
    "end": "1888500"
  },
  {
    "text": " And so, really, the\nmain component of this",
    "start": "1888500",
    "end": "1893980"
  },
  {
    "text": "is just training this single\nModel Editor right here in the middle\nthat's going to take as input a gradient\nand output a modified",
    "start": "1893980",
    "end": "1902470"
  },
  {
    "text": "version of that gradient. And the way that this is trained\nis using the edit data set.",
    "start": "1902470",
    "end": "1908320"
  },
  {
    "text": "It will be trained to output\nthe correct prediction on the in-scope\ngeneralization example,",
    "start": "1908320",
    "end": "1914920"
  },
  {
    "text": "and it will be told\nthat it should maintain the same distribution\nof outputs,",
    "start": "1914920",
    "end": "1920170"
  },
  {
    "text": "given the out-of-scope\nexample as the original model.",
    "start": "1920170",
    "end": "1926750"
  },
  {
    "text": "So there's two terms\nin the loss function. Yeah. [INAUDIBLE]",
    "start": "1926750",
    "end": "1932020"
  },
  {
    "text": " Oh, there's a question.",
    "start": "1932020",
    "end": "1937030"
  },
  {
    "text": "So the question was, isn't this\nmore expensive than fine-tuning because you're still\ncomputing the gradients?",
    "start": "1937030",
    "end": "1944040"
  },
  {
    "text": "So yeah, you do need to\nstill compute the gradient. The one big thing\nis that we're only",
    "start": "1944040",
    "end": "1950610"
  },
  {
    "text": "going to be applying\none update to the model. So we're going to be outputting\na modified gradient, such",
    "start": "1950610",
    "end": "1956279"
  },
  {
    "text": "that just one step of\nthat gradient, one update, gives you a good model.",
    "start": "1956280",
    "end": "1961690"
  },
  {
    "text": "And so, it will require fewer\nupdate steps than fine-tuning. And second, it's also\njust going to work better",
    "start": "1961690",
    "end": "1967320"
  },
  {
    "text": "than fine-tuning. And so, by actually\ntraining it to do this,",
    "start": "1967320",
    "end": "1973090"
  },
  {
    "text": "it will give you something\nthat more effectively makes",
    "start": "1973090",
    "end": "1979230"
  },
  {
    "text": "targeted edits without\naffecting out-of-scope examples. [INAUDIBLE]",
    "start": "1979230",
    "end": "1986301"
  },
  {
    "start": "1986301",
    "end": "1992740"
  },
  {
    "text": "So the question is, is\nthere any analysis on what it's doing to the gradient?",
    "start": "1992740",
    "end": "1998620"
  },
  {
    "text": "Unfortunately, it's a little\nbit difficult to analyze. I would guess that\nit is probably",
    "start": "1998620",
    "end": "2004680"
  },
  {
    "text": "increasing the scale\nto some degree, but it's certainly\ndoing more than that.",
    "start": "2004680",
    "end": "2010230"
  },
  {
    "text": "And generally, it's hard\nto interpret weights of neural networks,\nand it's also hard to interpret gradients.",
    "start": "2010230",
    "end": "2015720"
  },
  {
    "text": "So yeah, we weren't\nreally sure how to analyze it in particular.",
    "start": "2015720",
    "end": "2021460"
  },
  {
    "text": "One thing that I will mention\nthat we did in this work is that the gradient\nof a single example",
    "start": "2021460",
    "end": "2030480"
  },
  {
    "text": "is actually a rank 1 matrix. So if you have a\nweight matrix, and you compute the gradient\nof that, so say",
    "start": "2030480",
    "end": "2038550"
  },
  {
    "text": "you have a one-weight\nmatrix right here. This is layer a and layer b.",
    "start": "2038550",
    "end": "2045810"
  },
  {
    "text": "The gradient of\nthis weight matrix corresponds to the outer product\nof the forward activations.",
    "start": "2045810",
    "end": "2056949"
  },
  {
    "text": "So-- this isn't a\ngradient notation but the forward activations\nand the gradient coming",
    "start": "2056949",
    "end": "2064829"
  },
  {
    "text": "backward from b. So it's basically\nsomething like dL db.",
    "start": "2064830",
    "end": "2071879"
  },
  {
    "text": "So this is a vector,\nthis is a vector, and it's the outer product\nof those two things. And so, it's actually\na rank 1 matrix.",
    "start": "2071880",
    "end": "2077349"
  },
  {
    "text": "And one thing that's cool\nthat this Model Editor called MEND does is it\nactually decomposes--",
    "start": "2077350",
    "end": "2083789"
  },
  {
    "text": "instead of passing in\nthe weight matrices, it passes in the two rank 1\ncomponents into the network",
    "start": "2083790",
    "end": "2090570"
  },
  {
    "text": "and outputs a rank 1 or a\nlow-rank update to the model. And as a result,\nthis ends up being--",
    "start": "2090570",
    "end": "2098265"
  },
  {
    "text": " the dimensionality of the\ninputs and outputs of this Model Editor are much smaller.",
    "start": "2098265",
    "end": "2105880"
  },
  {
    "text": "And so, in\nparticular, if this is ha and hb are the kind of the\nsize of these two activations,",
    "start": "2105880",
    "end": "2116460"
  },
  {
    "text": "then this weight\nmatrix is ha times hb,",
    "start": "2116460",
    "end": "2121890"
  },
  {
    "text": "whereas the\ndimensionality of the two rank 1 terms is ha plus hb.",
    "start": "2121890",
    "end": "2129890"
  },
  {
    "text": "And this means that the\ndimensionality of the input is going to be\nmuch, much smaller",
    "start": "2129890",
    "end": "2137660"
  },
  {
    "text": "than if you were to input\nand output the entire weight matrix.",
    "start": "2137660",
    "end": "2142700"
  },
  {
    "start": "2142700",
    "end": "2147920"
  },
  {
    "text": "Cool. So this is the first\napproach to model editing. That's actually to try\nto update the weights or change the weights\nof the model directly.",
    "start": "2147920",
    "end": "2154403"
  },
  {
    "text": " Before we go over\nsome of the results, I'm also going to\nshow a second approach",
    "start": "2154403",
    "end": "2160619"
  },
  {
    "text": "to model editing that\ntries not to update the weights of the model.",
    "start": "2160620",
    "end": "2165780"
  },
  {
    "text": "And the motivation\nfor the second one is that this approach\nends up being somewhat",
    "start": "2165780",
    "end": "2171840"
  },
  {
    "text": "difficult to scale to\nlarge numbers of edits because it's actually fairly\ndifficult to figure out",
    "start": "2171840",
    "end": "2179310"
  },
  {
    "text": "how to change the weights\nin a way that will make these sorts of targeted edits.",
    "start": "2179310",
    "end": "2186120"
  },
  {
    "text": "And so, the second\napproach is going to try to take a more\nsemi-parametric approach.",
    "start": "2186120",
    "end": "2193440"
  },
  {
    "text": "And so, we'll have our\nbase language model, and instead of updating the\nweights of this language model, it's actually going\nto be fully frozen.",
    "start": "2193440",
    "end": "2200060"
  },
  {
    "text": "And instead, we're\ngoing to try to form a wrapper around\nthat language model such that the wrapped\nlanguage model gives you",
    "start": "2200060",
    "end": "2207410"
  },
  {
    "text": "the behavior that you want\nafter you've applied the edits.",
    "start": "2207410",
    "end": "2212950"
  },
  {
    "text": "And so, this approach will have\na few different components. The first will be\na memory of edits",
    "start": "2212950",
    "end": "2219840"
  },
  {
    "text": "that stores all the edits that\nyou want to apply to the model. The second will be\na scope classifier",
    "start": "2219840",
    "end": "2225240"
  },
  {
    "text": "that will classify whether\nor not an edit and an input are within scope.",
    "start": "2225240",
    "end": "2230773"
  },
  {
    "text": "This is actually going to be\nsomewhat similar to something like matching networks\nor prototypical networks. We're actually going\nto be comparing",
    "start": "2230773",
    "end": "2236940"
  },
  {
    "text": "the edits that you want\nto make and the inputs and seeing if they're\nsimilar or not.",
    "start": "2236940",
    "end": "2243690"
  },
  {
    "text": "And then, additionally, you can\ntrain a counterfactual model that will make\npredictions if the edit is",
    "start": "2243690",
    "end": "2249950"
  },
  {
    "text": "being applied to the model. And so the way this\nworks at test time is,",
    "start": "2249950",
    "end": "2255780"
  },
  {
    "text": "first, you can populate the\nedit memory with the edits that you want to\nmake to your model.",
    "start": "2255780",
    "end": "2261990"
  },
  {
    "text": "This is the one\nslide that I didn't update with the correct\nPM because the PM is changing really quickly.",
    "start": "2261990",
    "end": "2267795"
  },
  {
    "start": "2267795",
    "end": "2273299"
  },
  {
    "text": "Then, when we get a new test\ninput, what we're going to do is we're going to first compare\nthat test input to the edits",
    "start": "2273300",
    "end": "2280050"
  },
  {
    "text": "in our memory bank. The scope classifier will tell\nus that this edit is in-scope.",
    "start": "2280050",
    "end": "2287800"
  },
  {
    "text": "And then, because there\nis an edit in-scope, we'll then pass the edit\nas well as the input",
    "start": "2287800",
    "end": "2295630"
  },
  {
    "text": "to the counterfactual model,\nwhich will give us the revised prediction for the input.",
    "start": "2295630",
    "end": "2302170"
  },
  {
    "text": "Alternatively, if we get an\nexample that's out of scope, then we'll just pass that\ndirectly-- out of scope",
    "start": "2302170",
    "end": "2307863"
  },
  {
    "text": "for all the edits according\nto the scope classifier, then we'll just pass that\ndirectly to the base model.",
    "start": "2307863",
    "end": "2314250"
  },
  {
    "text": "And so, really, there's\njust two components that we're training here. First is a scope classifier\nthat will tell us whether or not",
    "start": "2314250",
    "end": "2320540"
  },
  {
    "text": "an example and an edit are-- whether an example is\nin-scope for a given edit.",
    "start": "2320540",
    "end": "2325589"
  },
  {
    "text": "And then the second is\nthe counterfactual model, which will train to make\npredictions after applying",
    "start": "2325590",
    "end": "2331520"
  },
  {
    "text": "the edit for a given example. Yeah. [INAUDIBLE]",
    "start": "2331520",
    "end": "2338210"
  },
  {
    "start": "2338210",
    "end": "2343775"
  },
  {
    "text": "Yeah, so one thing\nthat you could do instead of having\na counterfactual model is essentially prepend the\nedit to the prompt of the base",
    "start": "2343775",
    "end": "2351150"
  },
  {
    "text": "model. And one potential advantage\nthat this might have is that it will leverage kind\nof the power of this really",
    "start": "2351150",
    "end": "2357450"
  },
  {
    "text": "large base model. In practice, we found that that\ndidn't seem to work that well,",
    "start": "2357450",
    "end": "2365070"
  },
  {
    "text": "but perhaps you could\nfine-tune the base model in order to be good at that\nsort of prompted behavior.",
    "start": "2365070",
    "end": "2373140"
  },
  {
    "text": "And the counterfactual model\nseemed to do reasonably well,",
    "start": "2373140",
    "end": "2378487"
  },
  {
    "text": "but it's something that-- I do think that trying to\nleverage the base model more",
    "start": "2378488",
    "end": "2383940"
  },
  {
    "text": "I think would be kind of\nan interesting direction for future work.",
    "start": "2383940",
    "end": "2389030"
  },
  {
    "text": "Yeah. [INAUDIBLE] and you\nneed a new model and say that the edit\nsize becomes larger",
    "start": "2389030",
    "end": "2398059"
  },
  {
    "text": "than the model suffice. Yeah, so the question\nis this scalable,",
    "start": "2398060",
    "end": "2403190"
  },
  {
    "text": "especially as you want to\napply a large number of edits, and do you need the\ncounterfactual model to be large?",
    "start": "2403190",
    "end": "2408930"
  },
  {
    "text": "So first, yeah. So this will-- as the\nnumber of edits increases,",
    "start": "2408930",
    "end": "2415860"
  },
  {
    "text": "you will still need to store\nall of those edits in memory. Fortunately, you can apply the\nscope classifier in parallel",
    "start": "2415860",
    "end": "2421790"
  },
  {
    "text": "to all of those edits,\nand also in practice. For the edits, we\nactually only need",
    "start": "2421790",
    "end": "2428060"
  },
  {
    "text": "to store the embeddings\nof the edits. We can just cache\nthat computation. So there is things that you can\ndo to make it computationally",
    "start": "2428060",
    "end": "2435200"
  },
  {
    "text": "fairly cheap. The other thing\nthat you could do is if you do start to accumulate\na very large number of edits,",
    "start": "2435200",
    "end": "2442898"
  },
  {
    "text": "at that point, it\nmay just make sense to fine-tune your model\nbecause at that point,",
    "start": "2442898",
    "end": "2448099"
  },
  {
    "text": "you do need to actually\nchange the model fairly significantly and hopefully,\nthat you don't accumulate edits",
    "start": "2448100",
    "end": "2454190"
  },
  {
    "text": "that fast over time. And then, in terms of\nthe counterfactual model,",
    "start": "2454190",
    "end": "2459470"
  },
  {
    "text": "in practice, we do\nkind of pretrain it with a BERT style model. And so, it won't be necessarily\nas large as the base model,",
    "start": "2459470",
    "end": "2467870"
  },
  {
    "text": "but it could still\nbe somewhat large. And with a BERT\nstyle pretraining,",
    "start": "2467870",
    "end": "2473270"
  },
  {
    "text": "we found that it could work well\neven with edit data sets that",
    "start": "2473270",
    "end": "2479420"
  },
  {
    "text": "are far less in size than\nthe data set that the base",
    "start": "2479420",
    "end": "2484994"
  },
  {
    "text": "model was trained on.  Yeah. So with this type\nof computation,",
    "start": "2484995",
    "end": "2494290"
  },
  {
    "text": "if we see something that\nis within the scope, we just completely ignore\nthe base model, right?",
    "start": "2494290",
    "end": "2500463"
  },
  {
    "text": "Is it possible to\ncreate something that's like a\ncontainer in which,",
    "start": "2500463",
    "end": "2506436"
  },
  {
    "text": "instead of just sending it to a\nseparate counterfactual model, like the counterfactual\nmodel just edits the output of the base model?",
    "start": "2506437",
    "end": "2512530"
  },
  {
    "text": "Kind of like with the edits. Yeah. So one suggestion\nthat came up before is we could possibly prepend\nthe edit to the base model's",
    "start": "2512530",
    "end": "2521170"
  },
  {
    "text": "prompt. Alternatively, you could\nkind of make a prediction from the base model\nand then pass that",
    "start": "2521170",
    "end": "2526420"
  },
  {
    "text": "into the counterfactual model. And that might\nstill be leveraging all the power of\nthis base model,",
    "start": "2526420",
    "end": "2532060"
  },
  {
    "text": "and that it would\nonly be editing the output of this base model.",
    "start": "2532060",
    "end": "2537500"
  },
  {
    "text": "And so, I think that for\nthings like question-answering, that isn't necessarily\nthat important,",
    "start": "2537500",
    "end": "2543580"
  },
  {
    "text": "but I think that if you're in\na setting where the task is to write long-form text,\nthen something like that",
    "start": "2543580",
    "end": "2549670"
  },
  {
    "text": "may actually make a lot more\nsense because generating long-form text is much harder\nthan answering a question.",
    "start": "2549670",
    "end": "2556095"
  },
  {
    "text": "And also, it may\nbe that you only need to edit a small part\nof that long-form text. And so, yeah, I think that\ngenerally, different directions",
    "start": "2556095",
    "end": "2562420"
  },
  {
    "text": "for leveraging the base\nmodel are interesting, and one way to do\nthat possibly may be to take the output\nof the base model",
    "start": "2562420",
    "end": "2569260"
  },
  {
    "text": "and pass it into the\ncounterfactual model. One other small thing\nthat I'll mention that's kind of a benefit of\nhaving the tube be decoupled",
    "start": "2569260",
    "end": "2577280"
  },
  {
    "text": "is that by having\nthem be decoupled, this does mean that you can\nactually plug in any base model",
    "start": "2577280",
    "end": "2583670"
  },
  {
    "text": "into this architecture. And we found\nexperimentally that even if you train this whole\nsystem with one base model,",
    "start": "2583670",
    "end": "2592250"
  },
  {
    "text": "you can kind of apply it to\nlots of different base models. And that means that even if\nyou then retrain a base model,",
    "start": "2592250",
    "end": "2599690"
  },
  {
    "text": "at some point, you could\nstill reuse the editor across multiple\ndifferent base models. ",
    "start": "2599690",
    "end": "2607660"
  },
  {
    "text": "Cool. So in terms of the\nexperiments, I'll highlight just two experiments. The first was on\nquestion-answering, just",
    "start": "2607660",
    "end": "2614018"
  },
  {
    "text": "like all the examples we've\nbeen looking at so far. And there were two metrics\nthat we looked at in order",
    "start": "2614018",
    "end": "2620079"
  },
  {
    "text": "to evaluate how good the\neditor was performing. The first was edit success,\nwhich corresponds to accuracy",
    "start": "2620080",
    "end": "2627430"
  },
  {
    "text": "on in-scope examples.  And then the second\nwas drawdown,",
    "start": "2627430",
    "end": "2633790"
  },
  {
    "text": "which corresponds to how\nmuch is the accuracy dropping on examples that are out\nof scope from the edit.",
    "start": "2633790",
    "end": "2641869"
  },
  {
    "text": "In order to combine these\ntwo things into one metric, we can just measure edit\nsuccess minus drawdown.",
    "start": "2641870",
    "end": "2648290"
  },
  {
    "text": "And so, the best that you\ncould do is edit success of 1 and a drawdown of 0. And so, then subtracting\nthe two would give you 1.",
    "start": "2648290",
    "end": "2656480"
  },
  {
    "text": "And then, as this\nscore goes down, that means that either your\nedit success is dropping or your drawdown is increasing.",
    "start": "2656480",
    "end": "2664440"
  },
  {
    "text": "Cool. And then, we're going\nto evaluate this as we increase the\nnumber of edits that we're going to be\napplying at test time.",
    "start": "2664440",
    "end": "2671030"
  },
  {
    "text": "And we'll be, in this case,\nediting a T5-Large model on the type of\nquestion-answering things that we've been seeing so far.",
    "start": "2671030",
    "end": "2676570"
  },
  {
    "text": "Cool.  And so, if we compare the three\ndifferent editing approaches--",
    "start": "2676570",
    "end": "2683680"
  },
  {
    "text": "I guess I didn't include\nfine-tuning on here, although fine-tuning\ngenerally kind of does worse than all three\nof these methods.",
    "start": "2683680",
    "end": "2689830"
  },
  {
    "text": " First, we see that\nwith a single edit,",
    "start": "2689830",
    "end": "2694839"
  },
  {
    "text": "both MEND and SERAC\nare able to do-- both the approach\nthat was editing",
    "start": "2694840",
    "end": "2699880"
  },
  {
    "text": "the weights and the\nsemi-parametric approach are able to do fairly well. But as you increase\nthe number of edits,",
    "start": "2699880",
    "end": "2707954"
  },
  {
    "text": "really, these\napproaches that try to edit the weights of the\nmodel are much less successful",
    "start": "2707955",
    "end": "2713260"
  },
  {
    "text": "at doing so. And really, only SERAC\nis able to achieve",
    "start": "2713260",
    "end": "2718809"
  },
  {
    "text": "a high edit success\nand a low drawdown as you increase the\nnumber of edits. Cool. And then, one other\nexperiment that I'll mention",
    "start": "2718810",
    "end": "2726490"
  },
  {
    "text": "is you don't necessarily have\nto just edit factual knowledge and so forth.",
    "start": "2726490",
    "end": "2733293"
  },
  {
    "text": "And so, one\nexperiment that we did was editing the\nsentiment of a model.",
    "start": "2733293",
    "end": "2739940"
  },
  {
    "text": "And so, we trained it\nto edit the sentiment on a variety of\ndifferent topics. And then, at test\ntime, if we ask",
    "start": "2739940",
    "end": "2748420"
  },
  {
    "text": "one of Facebook's\nprior public models, what do you think\nabout vaccines? It gives you a model that is\nvery negative about vaccines.",
    "start": "2748420",
    "end": "2756529"
  },
  {
    "text": "So everything highlighted in\nred is things that are negative. And then, if you actually\ntry to edit this model",
    "start": "2756530",
    "end": "2763660"
  },
  {
    "text": "with the latter SERAC approach,\nyou can get something that is--",
    "start": "2763660",
    "end": "2769347"
  },
  {
    "text": "it's because we're telling\nit to be more positive, and so as a result, you\nsee that it's much more positive about vaccines.",
    "start": "2769347",
    "end": "2775460"
  },
  {
    "text": "And it's not just positive\nin a superficial way. It also says things like\nI think they're a good way to prevent infectious diseases.",
    "start": "2775460",
    "end": "2781773"
  },
  {
    "text": "I think it's good\nfor people to be informed about the risk of\ntheir health, and so forth. And so this sort of\nediting approach, I think,",
    "start": "2781773",
    "end": "2789720"
  },
  {
    "text": "is actually fairly general. And as long as you\nhave a data set that can tell it how you\nwanted to make edits,",
    "start": "2789720",
    "end": "2797850"
  },
  {
    "text": "then you could apply it in\na variety of different ways to these very large models. ",
    "start": "2797850",
    "end": "2805890"
  },
  {
    "text": "Cool. So the takeaways\nof this first part is we can use meta-learning\nto enable adaptation",
    "start": "2805890",
    "end": "2811380"
  },
  {
    "text": "and fine-tuning of\nthese large models, either with only\nunlabeled target data or with a high-level description\nor a single example of a change",
    "start": "2811380",
    "end": "2818910"
  },
  {
    "text": "that you want to make. And these are two things\nthat are, I think, very difficult to do just\nwith vanilla fine-tuning.",
    "start": "2818910",
    "end": "2825583"
  },
  {
    "text": " Cool. Now, I also want to\ntalk a little bit",
    "start": "2825583",
    "end": "2831740"
  },
  {
    "text": "about meta-learning across more\ngeneral task distributions. And we're a little\nbit low time, so it's",
    "start": "2831740",
    "end": "2838040"
  },
  {
    "text": "possible I might just\ncover the first thing and skip the second thing.",
    "start": "2838040",
    "end": "2843710"
  },
  {
    "text": "This work is really\nabout trying to push the boundaries of\nmeta-learning and trying to see if we can\nget something that's",
    "start": "2843710",
    "end": "2849860"
  },
  {
    "text": "much more general\nthan the priors that we've been\nlooking at previously.",
    "start": "2849860",
    "end": "2854930"
  },
  {
    "text": "And this is a\npaper that came out on trying to train\ngeneric optimizers.",
    "start": "2854930",
    "end": "2862990"
  },
  {
    "text": "And it's a really cool\npaper, and really, the goal of their work was to try\nto get an optimizer that",
    "start": "2862990",
    "end": "2868420"
  },
  {
    "text": "works well for any problem\nand any architecture without having to tune\nthe hyperparameters",
    "start": "2868420",
    "end": "2875950"
  },
  {
    "text": "of that optimizer, and so,\nwithout having to tune learning rates or without having to\ntune momentum or anything",
    "start": "2875950",
    "end": "2882130"
  },
  {
    "text": "else of the optimizer. And we'll get to the\nexperiments at the end,",
    "start": "2882130",
    "end": "2887360"
  },
  {
    "text": "but the short version\nis they largely achieve this goal, which I think\nis really exciting.",
    "start": "2887360",
    "end": "2893540"
  },
  {
    "text": " So it has four\ncentral components.",
    "start": "2893540",
    "end": "2899270"
  },
  {
    "text": "The first is a neural\nnetwork architecture that predicts weight updates. The second is a algorithm for\ntraining that neural network.",
    "start": "2899270",
    "end": "2908620"
  },
  {
    "text": "The third is a very\nlarge and broad set of optimization tasks.",
    "start": "2908620",
    "end": "2914415"
  },
  {
    "text": "This is really\nimportant in order to get the generic part of this.",
    "start": "2914415",
    "end": "2919690"
  },
  {
    "text": "And then, the last\nis a lot of compute.  Cool.",
    "start": "2919690",
    "end": "2925180"
  },
  {
    "text": "So let's step through each of\nthose different components. So the first is\nthe architecture.",
    "start": "2925180",
    "end": "2930250"
  },
  {
    "text": "The architecture is actually\nsomewhat complicated, but once you break it\ndown, it's fairly simple.",
    "start": "2930250",
    "end": "2935390"
  },
  {
    "text": "So first, for every\nsingle weight, for every single parameter\nin the network that you're",
    "start": "2935390",
    "end": "2942230"
  },
  {
    "text": "optimizing, there is a\nreally small neural network. It's really a tiny,\nfully connected network",
    "start": "2942230",
    "end": "2947810"
  },
  {
    "text": "with two hidden layers\nthat each have four units, and there's one of these\nfully connected networks",
    "start": "2947810",
    "end": "2954730"
  },
  {
    "text": "for every single\nparameter in your network. And this is the\nnetwork that's going",
    "start": "2954730",
    "end": "2960530"
  },
  {
    "text": "to be outputting the parameter\nupdate for that weight. Now, where do the weights\nof this network come from?",
    "start": "2960530",
    "end": "2968050"
  },
  {
    "text": "So there is a separate\nneural network, that's an LSTM, that is\nacting over the parameters",
    "start": "2968050",
    "end": "2975870"
  },
  {
    "text": "in the weight tensor. And this is going\nto be generating the weights of all of the MLPs\nfor each of the parameters.",
    "start": "2975870",
    "end": "2986630"
  },
  {
    "text": "And so, this LSTM-- so for every weight or, I\nguess, for every tensor,",
    "start": "2986630",
    "end": "2993610"
  },
  {
    "text": "so for every weight\nmatrix, for example, you're going to have an LSTM\nthat acts over those weights.",
    "start": "2993610",
    "end": "2999280"
  },
  {
    "text": "And that LSTM is going to\nbe outputting the weight",
    "start": "2999280",
    "end": "3005730"
  },
  {
    "text": "matrices of these\nreally tiny MLPs for all of the parameters\nin that tensor. ",
    "start": "3005730",
    "end": "3014690"
  },
  {
    "text": "The input features of\nthis LSTM correspond to a bunch of different things. It includes the\nmean and variance",
    "start": "3014690",
    "end": "3020810"
  },
  {
    "text": "of each of the parameter\nvalues in that tensor. A moving average of the gradient\nand the squared gradient,",
    "start": "3020810",
    "end": "3027050"
  },
  {
    "text": "that's information\nthat Adam uses. Also, things that indicate the\nfraction of training that's",
    "start": "3027050",
    "end": "3032300"
  },
  {
    "text": "been completed, as well. So the input is all of\nthose different features,",
    "start": "3032300",
    "end": "3038160"
  },
  {
    "text": "the output is the\nweight matrices of the small MLP for\neach of the parameters.",
    "start": "3038160",
    "end": "3045060"
  },
  {
    "text": " The last thing is this\nglobal aggregation.",
    "start": "3045060",
    "end": "3051240"
  },
  {
    "text": "The LSTM is also going to\noutput a global context. This global context\nvector is going",
    "start": "3051240",
    "end": "3057690"
  },
  {
    "text": "to be pooled across\nthe weight tensors and then reinput into each LSTM.",
    "start": "3057690",
    "end": "3063869"
  },
  {
    "text": "And so, this will just help\nthe LSTMs basically stay on the same page. ",
    "start": "3063870",
    "end": "3070720"
  },
  {
    "text": "Cool. So that's the architecture. How do we actually\ntrain this architecture?",
    "start": "3070720",
    "end": "3077789"
  },
  {
    "text": "So the meta objective\nis the training loss",
    "start": "3077790",
    "end": "3083780"
  },
  {
    "text": "at the end of the\noptimization for each task. And so, basically, you'll\nhave some initial parameters",
    "start": "3083780",
    "end": "3091990"
  },
  {
    "text": "for the task. You'll then use\nthis architecture to update the weights many\ntimes up until, I think,",
    "start": "3091990",
    "end": "3100810"
  },
  {
    "text": "around 200,000 steps. And then, you'll get\nthis final parameter,",
    "start": "3100810",
    "end": "3106270"
  },
  {
    "text": "and the meta loss\nwill be how good is this final parameter at the\ntask that it is trying to do?",
    "start": "3106270",
    "end": "3117000"
  },
  {
    "text": "And the objective will\nbe this final loss, but, of course,\naveraged over all of the different\noptimization tasks",
    "start": "3117000",
    "end": "3123857"
  },
  {
    "text": "that it's being meta-trained on. ",
    "start": "3123857",
    "end": "3129940"
  },
  {
    "text": "Now, the meta-optimizer\nis going to be using evolution strategies. This is the approach that\nYunho covered in his lecture",
    "start": "3129940",
    "end": "3137309"
  },
  {
    "text": "using full rollouts. They're not going to be\ndoing any sort of truncation. And so this is going to be a\npretty expensive optimization.",
    "start": "3137310",
    "end": "3145710"
  },
  {
    "text": "So they'll take this\nfinal loss and then update the weights of the LSTM\nthat are used to make",
    "start": "3145710",
    "end": "3151918"
  },
  {
    "text": "all of these different updates.  Then, they also\nfound a curriculum",
    "start": "3151918",
    "end": "3158300"
  },
  {
    "text": "to be extremely helpful. And what they did is\nthey first started with optimization problems with\na smaller number of training",
    "start": "3158300",
    "end": "3165170"
  },
  {
    "text": "steps and with a\nsmaller problem size and then gradually moved\ntowards longer training",
    "start": "3165170",
    "end": "3171500"
  },
  {
    "text": "times and larger problems. How do you measure problem size?",
    "start": "3171500",
    "end": "3176810"
  },
  {
    "text": "They estimate the problem size. This was just the time\nrequired to do one forward pass in the network. And so this is an indication\nof how big is the network",
    "start": "3176810",
    "end": "3184048"
  },
  {
    "text": "or how complicated\nis the network. ",
    "start": "3184048",
    "end": "3189690"
  },
  {
    "text": "Cool. And then, lastly,\ntasks and compute. So they constructed on the\norder of millions of tasks",
    "start": "3189690",
    "end": "3198839"
  },
  {
    "text": "with lots of different\narchitectures, including MLPs, ConvNets,\ntransformers, RNNs,",
    "start": "3198840",
    "end": "3204300"
  },
  {
    "text": "autoencoders, and also\nother learned optimizers. For each model family, they\nvaried the training data set,",
    "start": "3204300",
    "end": "3211400"
  },
  {
    "text": "loss function,\ninitialization strategy, and various hyperparameters,\nlike the hidden layer size,",
    "start": "3211400",
    "end": "3216480"
  },
  {
    "text": "the depth, the activation\nfunctions, and so forth. And they also did various forms\nof task augmentations, where",
    "start": "3216480",
    "end": "3225230"
  },
  {
    "text": "they did things like\nreparametizing the weight tensors, for example,\ndividing the weight",
    "start": "3225230",
    "end": "3231650"
  },
  {
    "text": "tensor by a certain value,\nhaving that be the weight tensor, and then having\na constant be multiplied",
    "start": "3231650",
    "end": "3238490"
  },
  {
    "text": "after that, delaying\nthe gradients, changing the floating\npoint position, and other forms of\ntask augmentation.",
    "start": "3238490",
    "end": "3247660"
  },
  {
    "text": "And then, computationally,\nas we mentioned on the previous slide,\nwe're doing full rollouts, and we are optimizing this\nwith evolutionary strategies.",
    "start": "3247660",
    "end": "3256900"
  },
  {
    "text": "And so, they estimated that it\ntook around 4,000 TPU months to meta-train the optimizer,\nwhich is rather expensive.",
    "start": "3256900",
    "end": "3267549"
  },
  {
    "text": "So let's get to\nsome of the results. Their first evaluation was on\na set of 83 canonical tasks,",
    "start": "3267550",
    "end": "3276820"
  },
  {
    "text": "and this is measured in\nterms of the number of update steps required to get\nthe same performance",
    "start": "3276820",
    "end": "3283930"
  },
  {
    "text": "as a version of Adam where\nthey tuned the learning rate of Adam.",
    "start": "3283930",
    "end": "3289490"
  },
  {
    "text": "And so the red line shows this\nversatile load optimizer, VeLO.",
    "start": "3289490",
    "end": "3295930"
  },
  {
    "text": "And first, we can see\nthat on 50% of the tasks,",
    "start": "3295930",
    "end": "3301540"
  },
  {
    "text": "VeLO is more than four times\nfaster than the learning rate tuned Adam.",
    "start": "3301540",
    "end": "3306590"
  },
  {
    "text": "And on basically\nall of the tasks, it is actually the same\nspeed or faster than Adam",
    "start": "3306590",
    "end": "3313190"
  },
  {
    "text": "in terms of the number\nof update steps. And they also compare,\nnot just to Adam but also",
    "start": "3313190",
    "end": "3320059"
  },
  {
    "text": "these other optimizers,\nincluding optimizers for which they tune\nthe hyperparameters,",
    "start": "3320060",
    "end": "3325370"
  },
  {
    "text": "like OptList and Shampoo\nand also prior methods",
    "start": "3325370",
    "end": "3333590"
  },
  {
    "text": "that are hyperparameter-free,\nfor which there didn't need to be any tuning.",
    "start": "3333590",
    "end": "3339190"
  },
  {
    "text": "So this result, in my\nmind, is really impressive and pretty exciting\nthat you could actually,",
    "start": "3339190",
    "end": "3345300"
  },
  {
    "text": "on such a large number of\ntasks, outperform Adam.",
    "start": "3345300",
    "end": "3350820"
  },
  {
    "text": "What I think is even\nmore interesting and cool is actually looking\nat how well it's able to optimize things that\nwere out of distribution.",
    "start": "3350820",
    "end": "3357870"
  },
  {
    "text": "So we looked at a lot of\ndifferent out-of-distribution tasks. Here, they compared to\na few different methods,",
    "start": "3357870",
    "end": "3363330"
  },
  {
    "text": "although I generally tend to\nfocus on the comparison to Adam because I think that's\nwhat people use the most,",
    "start": "3363330",
    "end": "3369660"
  },
  {
    "text": "at least in terms of research. And they evaluate\nit on NERF training. NERF is a very\ndifferent kind of--",
    "start": "3369660",
    "end": "3378377"
  },
  {
    "text": "I mean, it's not necessarily a\ndifferent kind of architecture, but it's a very different\nkind of optimization problem",
    "start": "3378377",
    "end": "3384020"
  },
  {
    "text": "than what it saw\nduring training. And we can see that it is-- despite the fact that this is\nan out-of-distribution task,",
    "start": "3384020",
    "end": "3390710"
  },
  {
    "text": "or at least seemingly\nout of distribution, the optimizer is able to\noptimize faster than Adam,",
    "start": "3390710",
    "end": "3397079"
  },
  {
    "text": "so you can compare the red\ncurve and the blue curve. The blue curve is actually\na tuned version of Adam, where they tried--",
    "start": "3397080",
    "end": "3403280"
  },
  {
    "text": "this is kind of the best out\nof 14 different hyperparameter settings of Adam. They also looked at an\nMLP mixer architecture,",
    "start": "3403280",
    "end": "3410090"
  },
  {
    "text": "which is just a different\narchitecture than anything that's seen during training. And we also see\nimprovements over Adam,",
    "start": "3410090",
    "end": "3417510"
  },
  {
    "text": "so red is faster than blue.  They also looked at\nobject detection.",
    "start": "3417510",
    "end": "3423770"
  },
  {
    "text": "So this is this kind\nof RCNN architecture, as well as the\ndecision transformer,",
    "start": "3423770",
    "end": "3429830"
  },
  {
    "text": "and we see that it's better\nthan SGD on object detection and kind of similar to\na decision transformer",
    "start": "3429830",
    "end": "3436880"
  },
  {
    "text": "or similar to Adam on\na decision transformer. ",
    "start": "3436880",
    "end": "3442890"
  },
  {
    "text": "Cool. Any questions about the results? Yeah. [INAUDIBLE]",
    "start": "3442890",
    "end": "3448780"
  },
  {
    "text": " For the object detection? Yes.",
    "start": "3448780",
    "end": "3453869"
  },
  {
    "text": "I'm not fully sure. I would guess that\nthey chose to use SGD because that's the default\narchitecture that's",
    "start": "3453870",
    "end": "3459450"
  },
  {
    "text": "used for this or the default\noptimizer that's used for this,",
    "start": "3459450",
    "end": "3464920"
  },
  {
    "text": "but I'm not fully sure. ",
    "start": "3464920",
    "end": "3471890"
  },
  {
    "text": "Cool. So they also had a\nreally long list of-- they also included a\nnumber of experiments",
    "start": "3471890",
    "end": "3478279"
  },
  {
    "text": "that it didn't work in,\nalthough, in general, I",
    "start": "3478280",
    "end": "3483760"
  },
  {
    "text": "felt like the results\nwere really impressive. The first caveat here\nis that it's compute.",
    "start": "3483760",
    "end": "3491870"
  },
  {
    "text": "Even though it's faster in\nterms of the number of updates, there is a 10x computational\noverhead over Adam.",
    "start": "3491870",
    "end": "3500400"
  },
  {
    "text": "And so, it's more expensive\nto run the LSTM compared to running Adam.",
    "start": "3500400",
    "end": "3506030"
  },
  {
    "text": "That said, compute\nis often dominated more by gradient computation\nthan the optimizer computation,",
    "start": "3506030",
    "end": "3512580"
  },
  {
    "text": "and so this may not\nbe that big of a deal. You may still get\nwall clock speed-ups.",
    "start": "3512580",
    "end": "3518060"
  },
  {
    "text": "And second, they\nhad some experiments that were looking at the\nperformance with varying batch sizes.",
    "start": "3518060",
    "end": "3523140"
  },
  {
    "text": "And you can see that\nit generally does well with very large\nbatches, and this",
    "start": "3523140",
    "end": "3529190"
  },
  {
    "text": "means that if you're worried\nabout wall clock time, you can still get actually\na much better wall clock",
    "start": "3529190",
    "end": "3535970"
  },
  {
    "text": "time than Adam by just using a\nlarger batch size because this means that when you have\na larger batch size,",
    "start": "3535970",
    "end": "3542583"
  },
  {
    "text": "your compute will be\neven more dominated by the gradient computation. ",
    "start": "3542583",
    "end": "3551010"
  },
  {
    "text": "They also found\nthat the optimizer doesn't do well at optimizing\nvery large models, specifically",
    "start": "3551010",
    "end": "3557390"
  },
  {
    "text": "models that had more than\n500 million parameters. And so, if you're\nin the business of training large\nlanguage models,",
    "start": "3557390",
    "end": "3563180"
  },
  {
    "text": "this may not be the\noptimizer for you. They also found that it\nhad worse performance",
    "start": "3563180",
    "end": "3569089"
  },
  {
    "text": "if you required more\nthan 200,000 update stops because they only trained\nfor up to 200,000 update steps.",
    "start": "3569090",
    "end": "3578809"
  },
  {
    "text": "And lastly, they found that\nin reinforcement learning settings, specifically\npolicy gradient and evolutionary\nstrategy optimizations,",
    "start": "3578810",
    "end": "3586520"
  },
  {
    "text": "it was much worse than Adam. This isn't too surprising\nbecause it wasn't trained on these kinds of\ntasks, and the gradients",
    "start": "3586520",
    "end": "3592820"
  },
  {
    "text": "that you get from\npolicy gradient and ES on reinforcement\nlearning problems is going to be very different\nthan the gradients you",
    "start": "3592820",
    "end": "3598407"
  },
  {
    "text": "typically get from\ngradient descent. I guess maybe the other\nconsideration here is whether it's worth spending\n4,000 TPU months, I think was,",
    "start": "3598407",
    "end": "3609890"
  },
  {
    "text": "on training this optimizer. In principle, if it\nkind of speeds up",
    "start": "3609890",
    "end": "3616700"
  },
  {
    "text": "a lot of future\nexperiments by being a single generic\noptimizer, then I",
    "start": "3616700",
    "end": "3621980"
  },
  {
    "text": "think that there is probably\na case for it being worth it, but there's always--",
    "start": "3621980",
    "end": "3627200"
  },
  {
    "text": "it's always, I think,\nworth considering weighing computational\ncosts and so forth.",
    "start": "3627200",
    "end": "3632360"
  },
  {
    "text": " Cool.",
    "start": "3632360",
    "end": "3637480"
  },
  {
    "text": "So the takeaway here is that\nmeta-learning can produce generic optimizers, which is\nreally, in my mind, the first--",
    "start": "3637480",
    "end": "3645525"
  },
  {
    "text": "we've seen some\nresults that have moved towards more optimizers\nthat generalize well, but we certainly\nhaven't seen something",
    "start": "3645525",
    "end": "3652030"
  },
  {
    "text": "that's kind of truly\ngiving something generic at this level before.",
    "start": "3652030",
    "end": "3658000"
  },
  {
    "text": " Cool. ",
    "start": "3658000",
    "end": "3667098"
  },
  {
    "text": "We're a little bit low on time. I think I'll just briefly\nmention some of this work",
    "start": "3667098",
    "end": "3674190"
  },
  {
    "text": "so that we still have\ntime to go through some of the open challenges. So the second kind\nof thing that I",
    "start": "3674190",
    "end": "3681710"
  },
  {
    "text": "think you can meta-learn over,\nin addition to the optimizer, is the architecture\nand the symmetries that",
    "start": "3681710",
    "end": "3691369"
  },
  {
    "text": "are built into an architecture. One example of the symmetries\nbuilt into an architecture",
    "start": "3691370",
    "end": "3696410"
  },
  {
    "text": "is convolutions, which\ngives you 2D equivariants. And things like\nconvolutions are great when",
    "start": "3696410",
    "end": "3703569"
  },
  {
    "text": "we know the structure and\nwe know how to build it in. But there are also\nscenarios where we don't know what the\nstructure should be",
    "start": "3703570",
    "end": "3709540"
  },
  {
    "text": "or we don't know\nhow to build it in. And so, there's this\nquestion of whether we",
    "start": "3709540",
    "end": "3714720"
  },
  {
    "text": "might be able to use\nmeta-learning to recover the equivariant or invariant\nstructure underlying a data",
    "start": "3714720",
    "end": "3721590"
  },
  {
    "text": "set. For example, could you\nrecover convolutions if you're given translational\nequivariant data?",
    "start": "3721590",
    "end": "3727980"
  },
  {
    "start": "3727980",
    "end": "3733594"
  },
  {
    "text": "You might ask if MAML\ncan already do this, and MAML can learn some\nvariant initial features,",
    "start": "3733594",
    "end": "3738970"
  },
  {
    "text": "but that equivariance\nmay not be preserved through the gradient update. And so, really, the\ngoal here is to try",
    "start": "3738970",
    "end": "3745790"
  },
  {
    "text": "to decompose the weights\ninto an equivariant structure and the corresponding\nparameters, such",
    "start": "3745790",
    "end": "3751160"
  },
  {
    "text": "that in the inner loop, you\ncan update only the parameters and try to retain\nthe equivariance",
    "start": "3751160",
    "end": "3757340"
  },
  {
    "text": "and keep that fixed. And so, we can walk through\nan example of convolutions",
    "start": "3757340",
    "end": "3763450"
  },
  {
    "text": "where if you look\nat a 1D convolution, you get something like\nthis, and you can actually",
    "start": "3763450",
    "end": "3769680"
  },
  {
    "text": "represent a 1D convolution\nas a fully connected layer. And if you do that, it looks\nlike this, where the weights",
    "start": "3769680",
    "end": "3775200"
  },
  {
    "text": "are copied multiple times. And from this\nperspective, you can",
    "start": "3775200",
    "end": "3782710"
  },
  {
    "text": "think about trying to\nreparametrize this weight matrix into, first, the\nunderlying filter parameters,",
    "start": "3782710",
    "end": "3789910"
  },
  {
    "text": "a, b, and c, and\nsecond, a matrix",
    "start": "3789910",
    "end": "3794920"
  },
  {
    "text": "that indicates how the\nweights should be shared or how the weights\nshould be tied. ",
    "start": "3794920",
    "end": "3800850"
  },
  {
    "text": "And kind of the\nproduct of this sharing matrix and the underlying\nfilter parameters will give you a\nweight vector that you",
    "start": "3800850",
    "end": "3807210"
  },
  {
    "text": "could reshape into the\ncorresponding fully connected weight matrix. And you can kind of\nintuitively think",
    "start": "3807210",
    "end": "3813330"
  },
  {
    "text": "of the sharing matrix is\ncapturing the symmetries and the vector v is capturing\nthe underlying shared",
    "start": "3813330",
    "end": "3819630"
  },
  {
    "text": "parameters. Now, this was an example\nfor convolutions, but it turns out that\nthis sort of decomposition",
    "start": "3819630",
    "end": "3827070"
  },
  {
    "text": "can directly represent this\ndecoupled equivariant sharing pattern and filter parameters\nfor a wide range of symmetries,",
    "start": "3827070",
    "end": "3836400"
  },
  {
    "text": "specifically for kind\nof all G-convolutions for a finite group G.\nThere are some symmetries",
    "start": "3836400",
    "end": "3842850"
  },
  {
    "text": "that this can't represent,\nbut we won't have time to get into that here. ",
    "start": "3842850",
    "end": "3849350"
  },
  {
    "text": "Cool. And so, once you have this\ndecomposition of the weight matrix, you can basically,\nin the outer loop,",
    "start": "3849350",
    "end": "3855080"
  },
  {
    "text": "learn the equivariance\nand possibly the initial parameters,\nand, in the inner loop, only update the parameters while\nkeeping that sharing matrix",
    "start": "3855080",
    "end": "3862550"
  },
  {
    "text": "fixed. And so, if you do\nthis, you can try",
    "start": "3862550",
    "end": "3870190"
  },
  {
    "text": "to actually see if\nyou can recover things like convolutions from\ntranslational equivariant data. And so, specifically,\nhere, we're",
    "start": "3870190",
    "end": "3876130"
  },
  {
    "text": "going to try to recover 1D\nconvolutions by giving it examples of random functions\nthat have this 1D translation",
    "start": "3876130",
    "end": "3887799"
  },
  {
    "text": "equivariance. And if you do this,\nof course, this is showing the mean squared\nerror on random held-out tasks,",
    "start": "3887800",
    "end": "3898180"
  },
  {
    "text": "and this is if you do MAML\nwith a fully-connected network, with a locally-connected\nnetwork, or with convolutions.",
    "start": "3898180",
    "end": "3904960"
  },
  {
    "text": "And, of course,\nconvolutions does the best because that reflects\nthe underlying symmetry of the tasks.",
    "start": "3904960",
    "end": "3910819"
  },
  {
    "text": "And if you use this approach\nwith a fully connected network, it's actually able\nto do just as well",
    "start": "3910820",
    "end": "3916540"
  },
  {
    "text": "as convolutions because it's\nable to recover the structure. And so, specifically, if you\nlook at the recovered weight",
    "start": "3916540",
    "end": "3923740"
  },
  {
    "text": "matrix from this\nsort of approach, where it's meta-learning\nthe sharing matrix,",
    "start": "3923740",
    "end": "3931330"
  },
  {
    "text": "you see that it actually gives\nyou exactly the structure of a 1D convolution.",
    "start": "3931330",
    "end": "3936410"
  },
  {
    "text": "Cool. And then, there's a\nquestion of, well, maybe you can also do better than\nconvolutions in scenarios where the structure\nisn't exactly",
    "start": "3936410",
    "end": "3942880"
  },
  {
    "text": "translational\nequivariance, and so, you can give it problems that have\npartial translation symmetry. So here, k is\ngoing to correspond",
    "start": "3942880",
    "end": "3950770"
  },
  {
    "text": "to the rank of a\nlocally-connected layer, and so k equals 1 corresponds\nexactly to convolutions,",
    "start": "3950770",
    "end": "3956050"
  },
  {
    "text": "where k equals 2 and k\nequals 3 has less symmetry. And here, we see\nthat this approach",
    "start": "3956050",
    "end": "3963490"
  },
  {
    "text": "is able to recover\na solution that's much better than convolutions\nby actually recovering something",
    "start": "3963490",
    "end": "3968860"
  },
  {
    "text": "closer to the true\nstructure in the data. And likewise, you\ncan try to actually",
    "start": "3968860",
    "end": "3975280"
  },
  {
    "text": "give it more symmetries\nthan just translation, like rotation and reflection. Here, we actually started\nwith a convolutional network",
    "start": "3975280",
    "end": "3982210"
  },
  {
    "text": "and tried to learn\nsymmetry on top of that. And you can get,\nagain, something that's better than\nconvolutions by recovering",
    "start": "3982210",
    "end": "3988510"
  },
  {
    "text": "that sort of structure. These are all fairly\nsimple problems,",
    "start": "3988510",
    "end": "3994029"
  },
  {
    "text": "but I think that it's still\nconceptually pretty cool to think about how you\nmight try to meta-learn",
    "start": "3994030",
    "end": "4001965"
  },
  {
    "text": "these kinds of\narchitectures from data. And maybe if you were\nable to kind of-- maybe",
    "start": "4001965",
    "end": "4009550"
  },
  {
    "text": "if you did scale up\nthese kinds of approaches with more compute,\nthen we might be able to actually do this\non a much larger scale,",
    "start": "4009550",
    "end": "4016870"
  },
  {
    "text": "rather than these simple\n1D convolution problems. ",
    "start": "4016870",
    "end": "4023290"
  },
  {
    "text": "Cool. So the takeaway is that we saw\nhow meta-learning can produce a generic optimizer, and we\nsaw some preliminary evidence",
    "start": "4023290",
    "end": "4029320"
  },
  {
    "text": "of meta-learning being able\nto capture equivariances in neural network architectures.",
    "start": "4029320",
    "end": "4036045"
  },
  {
    "text": " Cool.",
    "start": "4036045",
    "end": "4041480"
  },
  {
    "text": "I'd like to finish by talking\nabout some open challenges. So I'll group these into a\nfew different categories.",
    "start": "4041480",
    "end": "4051450"
  },
  {
    "text": "So the first category, I\nthink, of open challenges is trying to address some\nof the problem assumptions",
    "start": "4051450",
    "end": "4057680"
  },
  {
    "text": "that we often make in\nmeta-learning problems and multi-task\nlearning problems.",
    "start": "4057680",
    "end": "4064190"
  },
  {
    "text": "And the first is generalization. So this actually kind\nof came up earlier when we were\ntalking about trying",
    "start": "4064190",
    "end": "4069290"
  },
  {
    "text": "to generalize the\ncorruptions that are very different\nfrom the corruptions that we saw during training. ",
    "start": "4069290",
    "end": "4075859"
  },
  {
    "text": "And one specific\ninstantiation of this is like a long-tailed\nproblem, where you want to try to\nbe able to generalize",
    "start": "4075860",
    "end": "4081680"
  },
  {
    "text": "to the tail of the distribution. In this case, the--",
    "start": "4081680",
    "end": "4087194"
  },
  {
    "text": "in few-shot learning,\nin principle, we should be able to\nkind of meta-train on this part of the\ndistribution and then",
    "start": "4087195",
    "end": "4092400"
  },
  {
    "text": "do few-shot learning on the\ntail of the distribution. But these few-shot tasks on\nthe tail of the distribution",
    "start": "4092400",
    "end": "4099630"
  },
  {
    "text": "may be from a different\ndistribution than the task that we saw during training.",
    "start": "4099630",
    "end": "4105778"
  },
  {
    "text": "And so, there are some examples\nof generalization to the tails of distributions in\ndermatological diseases",
    "start": "4105779",
    "end": "4114490"
  },
  {
    "text": "and also kind of the adaptive\nrisk minimization example we saw before. But it's still, I\nthink, an open challenge",
    "start": "4114490",
    "end": "4120979"
  },
  {
    "text": "to really truly\ntackle these kinds of long-tailed situations. And I think that there\nmay be perhaps some hints",
    "start": "4120979",
    "end": "4127699"
  },
  {
    "text": "that come from some of\nthe robustness literature and perhaps trying to\ncombine those sorts of ideas",
    "start": "4127700",
    "end": "4134540"
  },
  {
    "text": "with things like meta-learning. ",
    "start": "4134540",
    "end": "4141028"
  },
  {
    "text": "Another thing that's-- another\nchallenge that comes up is so far, we've really been\nlooking at meta-training over",
    "start": "4141029",
    "end": "4149000"
  },
  {
    "text": "tasks that are from\na single modality. And in contrast, when people\nleverage previous experience,",
    "start": "4149000",
    "end": "4158568"
  },
  {
    "text": "we have not just one\nmodality of data, but we have lots of modalities,\nlike tactile feedback,",
    "start": "4158569",
    "end": "4164750"
  },
  {
    "text": "and language, and\nsocial cues, and we're able to merge all of that prior\nexperience into the knowledge",
    "start": "4164750",
    "end": "4172850"
  },
  {
    "text": "that we have and\nleverage, for example, cues from language that\nwe've read about when making",
    "start": "4172850",
    "end": "4178490"
  },
  {
    "text": "visual decisions, and so forth. And so, I think it'd be\nreally interesting to try",
    "start": "4178490",
    "end": "4184000"
  },
  {
    "text": "to learn priors across multiple\ndifferent data modalities, rather than trying to learn\na prior that works well",
    "start": "4184000",
    "end": "4190389"
  },
  {
    "text": "for a single modality. And there are some challenges\nand opportunities here. Different modalities often\nhave different dimensionalities",
    "start": "4190390",
    "end": "4196960"
  },
  {
    "text": "or different units,\nbut they can also carry complementary\nforms of information,",
    "start": "4196960",
    "end": "4203500"
  },
  {
    "text": "and that might be\nquite useful when trying to actually leverage\nthat prior information.",
    "start": "4203500",
    "end": "4209346"
  },
  {
    "text": "There's some kind of,\nI think, initial works in this direction,\nincluding the Flamingo paper that Eric covered\nin his lecture.",
    "start": "4209347",
    "end": "4215770"
  },
  {
    "text": "Also, the Gato paper that tried\nto train a single agent on lots of different modalities.",
    "start": "4215770",
    "end": "4221500"
  },
  {
    "text": "But I think that there's\nstill really a long way to go in terms of\ntrying to capture",
    "start": "4221500",
    "end": "4226510"
  },
  {
    "text": "all of this rich\nprior information when learning new tasks. ",
    "start": "4226510",
    "end": "4233700"
  },
  {
    "text": "And then, lastly,\nbeyond generalization and multimodality,\none question that I think has come up quite\na bit in this course",
    "start": "4233700",
    "end": "4240420"
  },
  {
    "text": "is actually trying to understand\nwhen multi-task learning might help versus when you might\njust be better off trying",
    "start": "4240420",
    "end": "4246840"
  },
  {
    "text": "to learn a specialized model. And so, there's all\nsorts of questions around algorithm selection\nand model selection",
    "start": "4246840",
    "end": "4253050"
  },
  {
    "text": "that I think are\nstill fairly wide open to explore in future work. ",
    "start": "4253050",
    "end": "4261750"
  },
  {
    "text": "Beyond building\nbetter algorithms, I think that also a\nbig part of research",
    "start": "4261750",
    "end": "4268590"
  },
  {
    "text": "and a big part of research\nthat fuels progress in machine learning AI\nis better benchmarks.",
    "start": "4268590",
    "end": "4275340"
  },
  {
    "text": "And I think that\nwe need benchmarks that challenge current\nalgorithms to find common structure. And arguably, the kind\nof learned optimizer",
    "start": "4275340",
    "end": "4283920"
  },
  {
    "text": "work that we just\ntalked about was fueled by a lot of compute, of\ncourse, but also, a benchmark",
    "start": "4283920",
    "end": "4290190"
  },
  {
    "text": "that had a much broader set\nof optimization tasks than was previously considered.",
    "start": "4290190",
    "end": "4296290"
  },
  {
    "text": "And we also want\nbenchmarks that reflect real-world problems that will\nhave an impact on the world.",
    "start": "4296290",
    "end": "4303020"
  },
  {
    "text": "And so, we've seen some steps\ntowards some good benchmarks over the past few years\nin various domains,",
    "start": "4303020",
    "end": "4310360"
  },
  {
    "text": "although I think that there's\nstill really a lot of progress that can be done here. And I especially think\nthat a lot of benchmarks",
    "start": "4310360",
    "end": "4317110"
  },
  {
    "text": "primarily look at computer\nvision or NLP domains, and there's all sorts of domains\noutside of images and text",
    "start": "4317110",
    "end": "4324970"
  },
  {
    "text": "that are extremely\nuseful, impactful, including things like\nsatellite imagery or molecules.",
    "start": "4324970",
    "end": "4331080"
  },
  {
    "text": "And I think we often\ndon't see benchmarks that do a good job of covering\nthose kinds of problems,",
    "start": "4331080",
    "end": "4336500"
  },
  {
    "text": "as well. ",
    "start": "4336500",
    "end": "4341670"
  },
  {
    "text": "Cool. And then, lastly, beyond\naddressing problem assumptions",
    "start": "4341670",
    "end": "4346910"
  },
  {
    "text": "and benchmarks is\nalso just trying to improve the core algorithms.",
    "start": "4346910",
    "end": "4352159"
  },
  {
    "text": "We saw in the learned\noptimizer work that bi-level optimization\ncan be very expensive,",
    "start": "4352160",
    "end": "4358350"
  },
  {
    "text": "and so perhaps there are\nways to actually make that cheaper and\napproaches, both in terms",
    "start": "4358350",
    "end": "4365360"
  },
  {
    "text": "of reducing computation,\nreducing memory, that make it possible to\ndo things like that",
    "start": "4365360",
    "end": "4370520"
  },
  {
    "text": "with much less compute. And also, I think that\nthere's always a lot of room",
    "start": "4370520",
    "end": "4376579"
  },
  {
    "text": "to develop a better\ntheoretical understanding of these different kinds of\napproaches, which may also eventually then kind\nof fuel advances",
    "start": "4376580",
    "end": "4384470"
  },
  {
    "text": "in the algorithms\nthemselves, as well. ",
    "start": "4384470",
    "end": "4389550"
  },
  {
    "text": "And then, beyond\nthese, also, I'm sure that you found challenges\nin your own homeworks",
    "start": "4389550",
    "end": "4395300"
  },
  {
    "text": "and final project. And so, this list\ncertainly isn't complete. It's just some of\nthe things that I",
    "start": "4395300",
    "end": "4400880"
  },
  {
    "text": "think are particularly\ninteresting directions for future investigation.",
    "start": "4400880",
    "end": "4407840"
  },
  {
    "text": "And then lastly, I'd like to\nend on a slightly kind of bigger picture for you, which is\nthat I think that still,",
    "start": "4407840",
    "end": "4416250"
  },
  {
    "text": "if you think about some\nof the biggest milestones, or at least what a lot of\npeople think of as the biggest",
    "start": "4416250",
    "end": "4421880"
  },
  {
    "text": "milestones in AI\nresearch, you think of things like IBM\nWatson, or the AlphaGo",
    "start": "4421880",
    "end": "4428660"
  },
  {
    "text": "system that was able to beat a\nchampion in AlphaGo or in Go.",
    "start": "4428660",
    "end": "4436370"
  },
  {
    "text": "And all of these\nsystems are really specialized for one particular\napplication, or game,",
    "start": "4436370",
    "end": "4441590"
  },
  {
    "text": "or problem. And even things like\nGPT-3 are really, I think, pushing the boundaries\nin terms of generality,",
    "start": "4441590",
    "end": "4447080"
  },
  {
    "text": "but they're still specialized\nto this domain of text. And, in contrast, we know that\nhumans are much more general.",
    "start": "4447080",
    "end": "4456739"
  },
  {
    "text": "They don't limit\nthemselves to language. They don't limit themselves\nto playing the game of Go",
    "start": "4456740",
    "end": "4462950"
  },
  {
    "text": "from day one. And so, I think that the-- yeah, we still have,\nI think, a long way",
    "start": "4462950",
    "end": "4469400"
  },
  {
    "text": "to go in terms of the field\nof artificial intelligence, of moving towards things\nthat are more general,",
    "start": "4469400",
    "end": "4474620"
  },
  {
    "text": "and have broader\nnotions of intelligence. And in this course,\nwe've covered,",
    "start": "4474620",
    "end": "4481310"
  },
  {
    "text": "I think, a few different\nthings that are, I think, steps towards building\nmore general systems, like learning multiple\ntasks in a single model,",
    "start": "4481310",
    "end": "4488510"
  },
  {
    "text": "leveraging previous experience\nwith learning new things, leveraging unlabeled prior data\nor data from different domains,",
    "start": "4488510",
    "end": "4497270"
  },
  {
    "text": "as well as trying to learn\ncontinuously over time, rather than just trying\nto learn one thing.",
    "start": "4497270",
    "end": "4504300"
  },
  {
    "text": "And so, there's, of course,\nI think, bits and pieces that",
    "start": "4504300",
    "end": "4509420"
  },
  {
    "text": "are still missing towards\ngetting artificial intelligence",
    "start": "4509420",
    "end": "4515030"
  },
  {
    "text": "that can operate at the\nsame level that humans can,",
    "start": "4515030",
    "end": "4521190"
  },
  {
    "text": "but I think that hopefully,\nthrough this course, you're much better\nequipped at trying to understand what might\nbe missing for building",
    "start": "4521190",
    "end": "4528172"
  },
  {
    "text": "these kinds of systems.  Cool.",
    "start": "4528172",
    "end": "4533600"
  },
  {
    "text": "That's it. I'd like to thank all of you\nfor a really awesome quarter, but thank you, everyone,\nfor being so engaged",
    "start": "4533600",
    "end": "4539930"
  },
  {
    "text": "in the lectures and,\nyeah, for a great quarter. ",
    "start": "4539930",
    "end": "4548000"
  }
]