[
  {
    "text": "So today we are going to discuss, uh, heterogeneous graphs and, uh, knowledge graph embeddings.",
    "start": "4010",
    "end": "11190"
  },
  {
    "text": "And in particular, we will be focusing on methods for, uh, knowledge graph completion.",
    "start": "11190",
    "end": "17025"
  },
  {
    "text": "So the topic for today is about, uh, first to talk about,",
    "start": "17025",
    "end": "22420"
  },
  {
    "text": "um, uh, a concept called heterogeneous graphs. And, uh, basically so far in our course,",
    "start": "22420",
    "end": "28770"
  },
  {
    "text": "we had been handling graphs that have only one edge type and only one node type.",
    "start": "28770",
    "end": "34200"
  },
  {
    "text": "We just said there is nodes and edges and you know nodes are connected with edges. So now the question would be,",
    "start": "34200",
    "end": "40289"
  },
  {
    "text": "how do we handle graphs, directed- undirected graphs that have multiple types of, uh,",
    "start": "40290",
    "end": "46490"
  },
  {
    "text": "connections, multiple types of nodes, multiple types of links between them. Um, and this is the notion of heterogeneous graphs.",
    "start": "46490",
    "end": "54219"
  },
  {
    "text": "Uh, heterogeneous in a sense that they contain heterogeneous sets of entities and heterogeneous set of, uh, edges.",
    "start": "54220",
    "end": "61655"
  },
  {
    "text": "Uh, and what we are going to talk today about in the first part is to discuss about, uh, relational GCN that takes care about different,",
    "start": "61655",
    "end": "69730"
  },
  {
    "text": "uh, edge types, different relation types. And then we are going to, uh, talk about more specifically about knowledge graphs,",
    "start": "69730",
    "end": "77015"
  },
  {
    "text": "which are a class of heterogeneous graphs. Um, how do we do knowledge graph embedding?",
    "start": "77015",
    "end": "82325"
  },
  {
    "text": "And also how do we do, uh, a very common task which is called knowledge graph completion.",
    "start": "82325",
    "end": "88240"
  },
  {
    "text": "So let's start with, uh, heterogeneous graphs and discuss relational GCNs.",
    "start": "88240",
    "end": "94950"
  },
  {
    "text": "So relational graph convolutional, uh, neural network. So a heterogeneous graph is,",
    "start": "94950",
    "end": "101990"
  },
  {
    "text": "uh, defined by a quadruple, is defined by a set of nodes, uh,",
    "start": "101990",
    "end": "107060"
  },
  {
    "text": "we'll call them V. Uh, nodes could have, uh, different types. Uh, it is defined by a set of edges that connect",
    "start": "107060",
    "end": "114650"
  },
  {
    "text": "different nodes and each edge is a- has a different, uh, uh, relation type.",
    "start": "114650",
    "end": "120670"
  },
  {
    "text": "So this means that edge is now a triple that says node i is connected to node j,",
    "start": "120670",
    "end": "126455"
  },
  {
    "text": "via relation, uh, R. And then also each node, um, is of a different type and I can get the type of the node by,",
    "start": "126455",
    "end": "136050"
  },
  {
    "text": "uh, by looking into this, uh, uh, function or set, uh, T. So basically I have nodes and edges where every node is labeled by a type,",
    "start": "136050",
    "end": "145220"
  },
  {
    "text": "and every relation, um, is labeled, uh, by a type. So what would be some examples of heterogeneous graphs?",
    "start": "145220",
    "end": "152990"
  },
  {
    "text": "Uh, one very common example is in biomedical, um, uh, field where you can basically take different biomedical entities, um,",
    "start": "152990",
    "end": "161715"
  },
  {
    "text": "and then relationships between them, and represent this as a heterogeneous graph. So for example, in- in this case,",
    "start": "161715",
    "end": "168545"
  },
  {
    "text": "I would have different, uh, node types. I would have diseases, I would have drugs, I would have proteins.",
    "start": "168545",
    "end": "174724"
  },
  {
    "text": "I would have different edge types which is different types of relations between these, uh, different types of entities.",
    "start": "174725",
    "end": "181250"
  },
  {
    "text": "And then you know, an example of the node here could be the- could be the disease, uh, migraine.",
    "start": "181250",
    "end": "186580"
  },
  {
    "text": "And, uh, then I also have, uh, different, uh, edge for example, it would be that, uh,",
    "start": "186580",
    "end": "192030"
  },
  {
    "text": "a particular drug treats a particular, uh, disease. And then, um, you know,",
    "start": "192030",
    "end": "197280"
  },
  {
    "text": "how can this- and- and then I can have different types of relations between them, like causes, uh, uh,",
    "start": "197280",
    "end": "203510"
  },
  {
    "text": "and so on and so forth. So that is one example, uh, of a-, uh, of a heterogeneous graph,",
    "start": "203510",
    "end": "209250"
  },
  {
    "text": "the different types of entities, uh, and different, uh, types of, um, uh,",
    "start": "209250",
    "end": "214739"
  },
  {
    "text": "uh, relationships between them. You can also think of event graphs as another example of a heterogenous graph,",
    "start": "214740",
    "end": "221425"
  },
  {
    "text": "where again, I could have different types of nodes, different types of entities. Uh, like for example,",
    "start": "221425",
    "end": "227120"
  },
  {
    "text": "a node type could be a flight. I could have different, uh, edge types, it would be a destination of a flight, um,",
    "start": "227120",
    "end": "233670"
  },
  {
    "text": "and then I could have, you know, um, the destination, different airports like the SFO, uh,",
    "start": "233670",
    "end": "239810"
  },
  {
    "text": "and create different types of, uh, relationships, uh, between these, uh, different entities.",
    "start": "239810",
    "end": "245245"
  },
  {
    "text": "This is, uh, another example of a heterogeneous graph. So now if we are given these type of a graph where we",
    "start": "245245",
    "end": "252710"
  },
  {
    "text": "have different types of nodes and different types of relations between them, then we would like to be able for our model to handle this heterogeneity,",
    "start": "252710",
    "end": "261260"
  },
  {
    "text": "in some sense, to take advantage of it, to learn that, uh, that you know, treats, uh,",
    "start": "261260",
    "end": "266780"
  },
  {
    "text": "a given drug treating a disease is a different type of relationship, than a given protein interacting with another, uh, protein.",
    "start": "266780",
    "end": "273425"
  },
  {
    "text": "So, um, wha- how are we going to do this? Is we are actually going to extend our, um, GCN.",
    "start": "273425",
    "end": "279560"
  },
  {
    "text": "So, um, um, uh, graph convolutional, uh, neural network, uh, to be able to handle different types of, uh, relationships.",
    "start": "279560",
    "end": "288380"
  },
  {
    "text": "We will call this our relational, uh, GCN. Um, and we will,",
    "start": "288380",
    "end": "293745"
  },
  {
    "text": "uh, so far, right, we define the, uh, GCN on one type of a relation and one type of a node type,",
    "start": "293745",
    "end": "300590"
  },
  {
    "text": "so on simple graphs, and now we would like to expand it to, uh, more complex heterogeneous graphs.",
    "start": "300590",
    "end": "307205"
  },
  {
    "text": "Um, the way I want to explain this is first to remind you about the GCN and then it will become very natural, how do we extend it?",
    "start": "307205",
    "end": "314909"
  },
  {
    "text": "So, uh, the first question when you think about a graph neural network is, you know, how do we- how does a GCN, uh,",
    "start": "314910",
    "end": "321694"
  },
  {
    "text": "operate and how does it update a representation of a given, uh, target node, uh, in the graph?",
    "start": "321695",
    "end": "327430"
  },
  {
    "text": "And we have discussed these through this notion of, uh, uh, message passing and- and computational graph,",
    "start": "327430",
    "end": "333080"
  },
  {
    "text": "where for a given target node, let's say A, here we see its corresponding, uh, computation graph,",
    "start": "333080",
    "end": "338479"
  },
  {
    "text": "um, where every node gets information from its neighbors. Notice that perhaps from the previous classes we talked about undirected graphs,",
    "start": "338480",
    "end": "347915"
  },
  {
    "text": "here now the graph is directed, so every node only, uh, gathers information along the direction of the edges.",
    "start": "347915",
    "end": "355805"
  },
  {
    "text": "So A gets information from B, C, and D. But for example, uh, node B, even though, um,",
    "start": "355805",
    "end": "361770"
  },
  {
    "text": "uh, it has two edges adjacent to it, it only gathers information from C because C points to it and B points to A.",
    "start": "361770",
    "end": "369710"
  },
  {
    "text": "So that's why, for example here, uh, the graph structure- the- the computation graph structure,",
    "start": "369710",
    "end": "375810"
  },
  {
    "text": "uh, is like this. So this is now how you can, uh, take into consideration, uh, relations, uh, of edges.",
    "start": "375810",
    "end": "383215"
  },
  {
    "text": "Now, uh, that we have, uh, re- reminded ourselves of, uh, uh, uh, wha- how to, uh,",
    "start": "383215",
    "end": "390525"
  },
  {
    "text": "unfold, uh, the graph neural network over a directed graph, we then talked about what does define a single layer of a graph neural network?",
    "start": "390525",
    "end": "398600"
  },
  {
    "text": "And we defined this notion of a message, where every me- every node, uh, computes, uh, the message coming,",
    "start": "398600",
    "end": "405645"
  },
  {
    "text": "uh, uh, from its, uh, children and this message, uh, depends, uh, both on its, uh,",
    "start": "405645",
    "end": "411300"
  },
  {
    "text": "uh, on its message from the previous level, um, as well as, uh, the- the neighbors, uh,",
    "start": "411300",
    "end": "417705"
  },
  {
    "text": "that- that it has at the previous level. And then the way the aggregation of these messages happens is",
    "start": "417705",
    "end": "424070"
  },
  {
    "text": "that- that the node V we will basically take these transformed messages, uh, from each of the children and aggregate it in some way.",
    "start": "424070",
    "end": "432275"
  },
  {
    "text": "Uh, combine it with its own message and produce the message, uh, or the embedding of the node, uh, at the next level.",
    "start": "432275",
    "end": "440170"
  },
  {
    "text": "And in order to add expressivity in terms of feature transformations,",
    "start": "440170",
    "end": "445490"
  },
  {
    "text": "not in terms of capturing the graph structure, but in terms of capturing the feature, uh, transformations, uh, we can then also add the,",
    "start": "445490",
    "end": "453780"
  },
  {
    "text": "uh, nonlinearity activation functions like sigmoid, uh, ReLU, uh, and so on.",
    "start": "453780",
    "end": "459720"
  },
  {
    "text": "Um, and this is how we think of, uh, graph neural networks. Now, going back to the GCN,",
    "start": "459720",
    "end": "467105"
  },
  {
    "text": "and a- and a single layer of GCN is defined, uh, by the following formula, right?",
    "start": "467105",
    "end": "472160"
  },
  {
    "text": "It's basically a node goes over its neighbors u takes the previous layer representations of the nodes,",
    "start": "472160",
    "end": "478985"
  },
  {
    "text": "normalizes it by the- by the, uh, parent's n-degrees, sums them up,",
    "start": "478985",
    "end": "484760"
  },
  {
    "text": "transforms them, and sends them through a non-linearity. So, uh, what does this mean is that, uh,",
    "start": "484760",
    "end": "489965"
  },
  {
    "text": "the way you can think of this is that the aggregation of a- of a GCN is,",
    "start": "489965",
    "end": "496230"
  },
  {
    "text": "uh, uh, average pooling type of operator, because of a summation and normalization here.",
    "start": "496230",
    "end": "502100"
  },
  {
    "text": "And then the message transformation is simple, uh, linear, uh, transformation.",
    "start": "502100",
    "end": "507810"
  },
  {
    "text": "So now what happens if we have multiple relation types, right?",
    "start": "507810",
    "end": "513130"
  },
  {
    "text": "What if I have a graph that has multiple types of relations? Er, the way I will denote this is that I will use edges of different colors,",
    "start": "513130",
    "end": "522250"
  },
  {
    "text": "um, and, you know, here I labeled them as r_1, r_2, r_3. Where it basically says, aha,",
    "start": "522250",
    "end": "527560"
  },
  {
    "text": "that A and B are connected according to the relation r_1, while, you know, uh, B and C, er,",
    "start": "527560",
    "end": "533890"
  },
  {
    "text": "are connected by, er, a relation, er, r_3. So that's, er, the way, er,",
    "start": "533890",
    "end": "539440"
  },
  {
    "text": "we are going to think of the input graph now. And the way we can now generalize a",
    "start": "539440",
    "end": "545379"
  },
  {
    "text": "GCN to the- this kind of heterogeneous graph is to use different network transformations,",
    "start": "545380",
    "end": "552490"
  },
  {
    "text": "different neural network, uh, weights, different neural network parameters for different, er, relation types.",
    "start": "552490",
    "end": "558865"
  },
  {
    "text": "So it means that when we are doing relation transformation, when we are doing message transformation,",
    "start": "558865",
    "end": "564100"
  },
  {
    "text": "we are not going to apply the same matrix W for every incoming edge,",
    "start": "564100",
    "end": "570894"
  },
  {
    "text": "but we are going to apply a different matrix W, um, depending on whether- um,",
    "start": "570895",
    "end": "577570"
  },
  {
    "text": "er, what is the kind of the relation that we are considering. So if we have three different relation types,",
    "start": "577570",
    "end": "584125"
  },
  {
    "text": "we are going to use three different types of, uh, W matrices; one for relation 1, one for relation 2, and one for, uh, relation 3.",
    "start": "584125",
    "end": "592240"
  },
  {
    "text": "So now, if I look at how does the, uh, neural network, uh, computation graph for, uh,",
    "start": "592240",
    "end": "599845"
  },
  {
    "text": "the node A look like, what is different now is that these transformation- message transformation operators,",
    "start": "599845",
    "end": "606280"
  },
  {
    "text": "these Ws, now have a color associated with them. So it means that the message coming from, let's say,",
    "start": "606280",
    "end": "612550"
  },
  {
    "text": "node B and the message coming from node D, uh, towards node A will be transformed using the same operator, the same W,",
    "start": "612550",
    "end": "620275"
  },
  {
    "text": "because the message is traveling along the relation type, uh, uh, r_1.",
    "start": "620275",
    "end": "625435"
  },
  {
    "text": "While, for example, the message from node C that, uh, is connected to node A with, er, relation r_2 will be transformed with a different transformation operator, the red one.",
    "start": "625435",
    "end": "636130"
  },
  {
    "text": "The red one is, er, designed for, er, r_2. So here, um, you see- uh,",
    "start": "636130",
    "end": "642550"
  },
  {
    "text": "you see the difference, right? Then, for example, um, this means that now what- how we are going to write this up is that we have-",
    "start": "642550",
    "end": "649570"
  },
  {
    "text": "we have these transformation matrices W that are also indexed, uh, by the relation type, right?",
    "start": "649570",
    "end": "655960"
  },
  {
    "text": "So in a relational GCN, the way we write out the message propagation and",
    "start": "655960",
    "end": "661870"
  },
  {
    "text": "aggregation is the following: we basically say, um, to compute the message of node v at level l plus 1,",
    "start": "661870",
    "end": "669595"
  },
  {
    "text": "we are going to sum up over all the relation types. For every relation type,",
    "start": "669595",
    "end": "675970"
  },
  {
    "text": "you are going to look at who are the neighbors, uh, of, uh, node v that- that are connected to node v according to this relation type",
    "start": "675970",
    "end": "684955"
  },
  {
    "text": "r. Then we are going to take the transformation matrix specific to that,",
    "start": "684955",
    "end": "690865"
  },
  {
    "text": "er, relation r to take the, uh, messages from the- uh, from the neighbors that are- er,",
    "start": "690865",
    "end": "697435"
  },
  {
    "text": "neighbors u that are connected to v according to- to this relation type r,",
    "start": "697435",
    "end": "702460"
  },
  {
    "text": "and we are are also going then to do the me- the embedding of the node v from previous layer,",
    "start": "702460",
    "end": "707920"
  },
  {
    "text": "er, and, um, er, er transform it. Uh, the- the important difference here is that Ws,",
    "start": "707920",
    "end": "714940"
  },
  {
    "text": "before, they used to be only differ layer by layer, now what happens here is that we have W for every layer,",
    "start": "714940",
    "end": "723279"
  },
  {
    "text": "for every, uh, relation. So, uh, another thing I did not explain is what is this C, uh,",
    "start": "723280",
    "end": "730675"
  },
  {
    "text": "er, sub v, r. This is simply a normalized node, er, in degree compared to that, er, relation.",
    "start": "730675",
    "end": "737470"
  },
  {
    "text": "So it's basically the number of incoming, er, er, relations of a given type r to the node, er,",
    "start": "737470",
    "end": "743620"
  },
  {
    "text": "v. This is still a- a graph neural network. It is still- I can write it out in",
    "start": "743620",
    "end": "750160"
  },
  {
    "text": "this formalism of a message transformation and aggregation, where basically each neighbor of a given- uh,",
    "start": "750160",
    "end": "757464"
  },
  {
    "text": "of a given node v transforms its- er, its previous layer embedding according to the matrix W,",
    "start": "757465",
    "end": "765280"
  },
  {
    "text": "but now W changes for every, er, re- based on the type of the relation between,",
    "start": "765280",
    "end": "770470"
  },
  {
    "text": "uh, you and me and, er, you know, here we transform the- the message,",
    "start": "770470",
    "end": "775480"
  },
  {
    "text": "uh, from node v, from the previous layer to, the, uh, embedding from the previous layer into the message, uh,",
    "start": "775480",
    "end": "782320"
  },
  {
    "text": "for the- from- for the- from the previous layer, and then the aggregation is still,",
    "start": "782320",
    "end": "787570"
  },
  {
    "text": "uh, simply a summation, right? We take these, um, transformed messages fro- based on the embeddings from the neighbors from previous layer,",
    "start": "787570",
    "end": "796900"
  },
  {
    "text": "where the transformation is relation-specific, as well as node's own embedding from the previous layer uh, transformed.",
    "start": "796900",
    "end": "805690"
  },
  {
    "text": "We add all these together to get the embedding, uh, at the next layer. So this is called,",
    "start": "805690",
    "end": "811825"
  },
  {
    "text": "a relational, er, GCN. So, um, basically, a relational graph neural",
    "start": "811825",
    "end": "817750"
  },
  {
    "text": "network or a relational graph convolutional neural network, where the main difference is that now we have different message transformation,",
    "start": "817750",
    "end": "826870"
  },
  {
    "text": "er, operators based on the type of the relationship between a pair of nodes.",
    "start": "826870",
    "end": "832480"
  },
  {
    "text": "And this is denoted by this, uh, subscript, uh, r that denotes, er, the relation type.",
    "start": "832480",
    "end": "838675"
  },
  {
    "text": "So, um, this is how we defined a RGCN.",
    "start": "838675",
    "end": "844615"
  },
  {
    "text": "Um, the point is now, right? We said that for every relation r,",
    "start": "844615",
    "end": "849820"
  },
  {
    "text": "we need L- capital L matrices, meaning, um, these transformation matrices are different for",
    "start": "849820",
    "end": "856030"
  },
  {
    "text": "every layer of the neural network- of the graph neural network, and they are different for every relation.",
    "start": "856030",
    "end": "862780"
  },
  {
    "text": "So the problem then becomes if you ask what is the size of this matrix? The size of the matrix W is simply the embedding dimension, er,",
    "start": "862780",
    "end": "872230"
  },
  {
    "text": "on the- on the lower layer, um, times the embedding dimension at the- at the upper layer, right?",
    "start": "872230",
    "end": "878350"
  },
  {
    "text": "So it's basically d, er, ^l and d, er, ^l plus 1 are the embedding dimension at",
    "start": "878350",
    "end": "884110"
  },
  {
    "text": "layer l and the embedding dimension at layer, er, l plus 1. And these can quickly be a few hundred- you know,",
    "start": "884110",
    "end": "890740"
  },
  {
    "text": "these d's are in the order of a few hundred, maybe up to a- up to 1,000, right?",
    "start": "890740",
    "end": "896319"
  },
  {
    "text": "So now the problem is that I have one such matrix per every layer and I have one such matrix per every relation type.",
    "start": "896320",
    "end": "905019"
  },
  {
    "text": "And because, uh, heterogeneous graphs, and in particular, knowledge graphs, can have hundreds,",
    "start": "905020",
    "end": "911050"
  },
  {
    "text": "can have thousands of different, er, relation types, then you get- uh,",
    "start": "911050",
    "end": "916615"
  },
  {
    "text": "you can get tens of thousands of these different, er, matrices. And each matrix can ha- is- is dense and can have quite big size.",
    "start": "916615",
    "end": "926185"
  },
  {
    "text": "And the problem then becomes that the number of parameters of this model- of this RGCN model, um,",
    "start": "926185",
    "end": "932875"
  },
  {
    "text": "tends to explode because, uh, it grows with the number of different relation types,",
    "start": "932875",
    "end": "938350"
  },
  {
    "text": "and there can be thousands of different relation types. And one problem is that the model becomes too big, um, to train,",
    "start": "938350",
    "end": "945400"
  },
  {
    "text": "and then the other problem is that the model has too many parameters, um, and overfitting can quickly become an issue.",
    "start": "945400",
    "end": "952480"
  },
  {
    "text": "So what I wanna discuss next is, er, two approaches, how to reduce the number of parameters of this RGCN-style model,",
    "start": "952480",
    "end": "962950"
  },
  {
    "text": "um, using two techniques. One technique will be to use, uh, block diagonal matrices, uh,",
    "start": "962950",
    "end": "969055"
  },
  {
    "text": "and the other one will be to use basis or, uh, dictionary learning as it is called.",
    "start": "969055",
    "end": "974350"
  },
  {
    "text": "So let me first talk about, uh, use of block diagonal, er, matrices.",
    "start": "974350",
    "end": "979930"
  },
  {
    "text": "So the key insight is that we wanna make these Ws- W matrices,",
    "start": "979930",
    "end": "985420"
  },
  {
    "text": "the transformation matrices, we wanna make them sparse. And one way to make them sparse is to enforce, to have this block diagonal structure.",
    "start": "985420",
    "end": "994569"
  },
  {
    "text": "So basically, non-zero elements are only along the specific blocks, uh, of this bigger,",
    "start": "994570",
    "end": "1000810"
  },
  {
    "text": "er, matrix, er, W. Uh, if you think about this, this will now reduce the number of non-zero, uh,",
    "start": "1000810",
    "end": "1009209"
  },
  {
    "text": "elements- the number of parameters in each W you have to estimate, because you only have to estimate that the blocks,",
    "start": "1009210",
    "end": "1015675"
  },
  {
    "text": "uh, these two green blocks, and you can basically ignore or assumed these, uh, empty parts of the matrix are 0.",
    "start": "1015675",
    "end": "1022889"
  },
  {
    "text": "So if you assume, for example that W is composed from B, uh, low dimensional,",
    "start": "1022890",
    "end": "1029505"
  },
  {
    "text": "uh, matrices, low-dimensional blocks, then the number of parameters is going to decrease,",
    "start": "1029505",
    "end": "1035744"
  },
  {
    "text": "uh, by a factor B. So if you said B by 10, uh, you have just reduced the number of free parameters that you need to learn,",
    "start": "1035744",
    "end": "1043664"
  },
  {
    "text": "uh, or estimate by a factor of 10. Of course, what do you lose?",
    "start": "1043665",
    "end": "1049350"
  },
  {
    "text": "What you lose is now that if you think about this as a transformation matrix, what you lose is that, um,",
    "start": "1049350",
    "end": "1056070"
  },
  {
    "text": "embedding dimensions that are far apart from each other, they cannot interact with each other, right?",
    "start": "1056070",
    "end": "1061830"
  },
  {
    "text": "So this means that, for example here, only embedding dimensions 1 and 2 can interact with each other,",
    "start": "1061830",
    "end": "1067380"
  },
  {
    "text": "but not, um, let's say 2 and 3 because they are in different blocks. So it may require several layers of propagations",
    "start": "1067380",
    "end": "1075434"
  },
  {
    "text": "and- and different structures of block diagonal matrices to be able for, a embedding dimension 3 to kind of talk to embedding dimension 2.",
    "start": "1075435",
    "end": "1085590"
  },
  {
    "text": "Um, so basically what this means is that only nearby neurons or neurons in the same block can talk to each other and exchange information with each other.",
    "start": "1085590",
    "end": "1094470"
  },
  {
    "text": "So perhaps your GN- GCN, GNN may need to be a bit deeper, but you have reduced, uh,",
    "start": "1094470",
    "end": "1100590"
  },
  {
    "text": "the number of parameters, uh, significantly, which will lead to, uh, faster training, perhaps more robust model,",
    "start": "1100590",
    "end": "1107205"
  },
  {
    "text": "less overfitting, um, and so on. So that's, uh, the technique of,",
    "start": "1107205",
    "end": "1112545"
  },
  {
    "text": "uh, block diagonal matrices, uh, where basically you reduce the dimensionality of",
    "start": "1112545",
    "end": "1117780"
  },
  {
    "text": "each W by assuming a block diagonal structure.",
    "start": "1117780",
    "end": "1122889"
  },
  {
    "text": "Uh, second idea how to, uh, make this, uh, RGCN more scalable and its learning more tractable is to- to",
    "start": "1123080",
    "end": "1132539"
  },
  {
    "text": "build on the key insight which is we wanna share weights across different relations, right?",
    "start": "1132540",
    "end": "1138210"
  },
  {
    "text": "We don't wanna consider relations as independent from each other, but we want relations to kind of share weights,",
    "start": "1138210",
    "end": "1146205"
  },
  {
    "text": "uh, to also share some information. And the way you can achieve this very",
    "start": "1146205",
    "end": "1151440"
  },
  {
    "text": "elegantly is that you will represent the weight matrix, the transformation matrix W for each relation as a linear combination of basis,",
    "start": "1151440",
    "end": "1162835"
  },
  {
    "text": "uh, transformations or as of basis matrices. And you call these basis matrices as your dictionary.",
    "start": "1162835",
    "end": "1170075"
  },
  {
    "text": "So this is also called dictionary learning. So let me now, uh, explain. The idea is that now your, uh, W,",
    "start": "1170075",
    "end": "1177570"
  },
  {
    "text": "uh, r is simply, uh, weighted summation over this dictionary of matrices V,",
    "start": "1177570",
    "end": "1186255"
  },
  {
    "text": "um, that- and I have B- B of them, right? So basically the idea is that I'll have a dictionary of matrices V.",
    "start": "1186255",
    "end": "1193424"
  },
  {
    "text": "Uh, and then to come up with a- with a, um, transformation matrix for, um,",
    "start": "1193425",
    "end": "1199350"
  },
  {
    "text": "node W, then basically what I have to do is, uh, have these, uh, weight- weight- weight scores,",
    "start": "1199350",
    "end": "1207135"
  },
  {
    "text": "uh, important weights, uh, a, that say how important is a given matrix,",
    "start": "1207135",
    "end": "1213940"
  },
  {
    "text": "uh, V_b, for a given relation R, right? And then the point is that these, uh,",
    "start": "1214520",
    "end": "1220020"
  },
  {
    "text": "matrices V are shared across all the relations. So I can think of matrices V as my basis matrices or as my, uh, dictionary.",
    "start": "1220020",
    "end": "1228600"
  },
  {
    "text": "And then these, uh, weights, uh, A, those are importance weights,",
    "start": "1228600",
    "end": "1233895"
  },
  {
    "text": "uh, for each matrix. So basically I'll say, the transformation matrix for, uh, a given, um,",
    "start": "1233895",
    "end": "1239820"
  },
  {
    "text": "relation R is some kind of linear combination of these basis matrices where these linear combination weights are learned for every,",
    "start": "1239820",
    "end": "1248925"
  },
  {
    "text": "uh, for every, uh, relation, right? So this means that now every relation only needs to",
    "start": "1248925",
    "end": "1254970"
  },
  {
    "text": "learn these importance, uh, weights, um, which is just capital V number of scalars rather than,",
    "start": "1254970",
    "end": "1262785"
  },
  {
    "text": "uh, uh, a- a large number of different, uh, transformation matrices. So now basically, every relation",
    "start": "1262785",
    "end": "1269400"
  },
  {
    "text": "specific transformation matrix is simply a linear combination, uh, from this dictionary of, uh, matrices, uh,",
    "start": "1269400",
    "end": "1276900"
  },
  {
    "text": "V. And that's, uh, an elegant way how to reduce, uh, the number of parameters because b can be relatively small,",
    "start": "1276900",
    "end": "1284625"
  },
  {
    "text": "let's say, I don't know, 100, uh, or 10 but, maybe you still have, um, uh, 1,000 different, uh, relation types.",
    "start": "1284625",
    "end": "1292980"
  },
  {
    "text": "And you basically say, I will represent every of the 1,000 relations as a linear combination from my dictionary of 10, uh, weight matrices.",
    "start": "1292980",
    "end": "1302310"
  },
  {
    "text": "And that's a very elegant way how to reduce, uh, the number of, uh, parameters.",
    "start": "1302310",
    "end": "1307830"
  },
  {
    "text": "So now that we have talked about this, uh, scalability issue of, uh,",
    "start": "1307830",
    "end": "1313919"
  },
  {
    "text": "uh, RGCN with two different approaches, one being, um, the block diagonal structure of the transformation matrix W,",
    "start": "1313920",
    "end": "1323475"
  },
  {
    "text": "the other one being that we cast learning of W as a dictionary learning problem,",
    "start": "1323475",
    "end": "1329850"
  },
  {
    "text": "which basically means we assume there is an underlying set of 10- let's say, 10 different basis matrices which we call dictionary,",
    "start": "1329850",
    "end": "1337050"
  },
  {
    "text": "and then every, uh, uh, relation specific transformation matrix is simply a linear combination of the, uh,",
    "start": "1337050",
    "end": "1345045"
  },
  {
    "text": "of the matrices coming from the dictionary also reduces the number of parameters makes the method more robust and, uh, speeds it up.",
    "start": "1345045",
    "end": "1354930"
  },
  {
    "text": "So now that we have RGCM, uh, what we want to do is also talk briefly about,",
    "start": "1354930",
    "end": "1361559"
  },
  {
    "text": "how do you define various prediction tasks on the, um, on the graph- on the heterogeneous graph?",
    "start": "1361560",
    "end": "1369405"
  },
  {
    "text": "So in terms of node classification or anti-classification, um, it is, uh, it is, uh,",
    "start": "1369405",
    "end": "1375465"
  },
  {
    "text": "it is all- all, uh, kind of as it used to be, right? So, uh, RGCN will compute the representation or an embedding for every node,",
    "start": "1375465",
    "end": "1384285"
  },
  {
    "text": "uh, for every node, um, then- then based on that embedding, we can then create a prediction head, uh, for example,",
    "start": "1384285",
    "end": "1391320"
  },
  {
    "text": "if you want to classify nodes into k, uh, different classes, uh, then the final prediction head will simply be",
    "start": "1391320",
    "end": "1398414"
  },
  {
    "text": "a linear transform that will basically take the embedding, multiply it with the weight vector,",
    "start": "1398415",
    "end": "1404610"
  },
  {
    "text": "uh, pass it through a non, uh, non-linearity like a sigmoid and we can interpret that as a probability,",
    "start": "1404610",
    "end": "1411059"
  },
  {
    "text": "uh, of that given class. So basically we will have a k head output rather than, uh, um, h, and then have a softmax on top of it,",
    "start": "1411060",
    "end": "1419535"
  },
  {
    "text": "so that basically we interpret that as a probability that, uh, that a given node is of the kth,",
    "start": "1419535",
    "end": "1426375"
  },
  {
    "text": "uh, given type or kth head. That's how we think of node classification.",
    "start": "1426375",
    "end": "1432304"
  },
  {
    "text": "For link prediction, things get a bit more tricky because now links have different, ah,",
    "start": "1432305",
    "end": "1439015"
  },
  {
    "text": "types and we don't want to simply split links, uh, randomly as we did it so far, uh,",
    "start": "1439015",
    "end": "1445750"
  },
  {
    "text": "because some types might be very common and other types might be very uncommon, right?",
    "start": "1445750",
    "end": "1451600"
  },
  {
    "text": "Some relation types might be very common, some relation types might be very uncommon. So what we would like to do is for every relation type,",
    "start": "1451600",
    "end": "1459850"
  },
  {
    "text": "we would like to split it between, ah, training message edges, uh,",
    "start": "1459850",
    "end": "1465414"
  },
  {
    "text": "training supervision edges, validation edges, and test edges. And then we would like to do this for every",
    "start": "1465415",
    "end": "1472120"
  },
  {
    "text": "of the relation types and then kind of merge, uh, all of the message edges for the- for the training will, er,",
    "start": "1472120",
    "end": "1479664"
  },
  {
    "text": "take all the training supervision edges for each separate relation type into",
    "start": "1479665",
    "end": "1484960"
  },
  {
    "text": "the training supervision edges and then same for validation edges and for test edges, right?",
    "start": "1484960",
    "end": "1489970"
  },
  {
    "text": "So kind of the point is that we wanna independently split edges from",
    "start": "1489970",
    "end": "1496405"
  },
  {
    "text": "each relation type and then merge it together, uh, these splits. And the reason we wanna do this is because this means that even for a very infrequent,",
    "start": "1496405",
    "end": "1506184"
  },
  {
    "text": "ah, very rare, uh, relation type, some of its- some of the instances of it will be- will be in the training,",
    "start": "1506185",
    "end": "1513159"
  },
  {
    "text": "some of instances will be in the validation, and some will be in the test set. Because the point would be if you just blindly split the edges,",
    "start": "1513160",
    "end": "1521500"
  },
  {
    "text": "just by chance it can happen that a very rare edge type does not appear in your,",
    "start": "1521500",
    "end": "1526990"
  },
  {
    "text": "ah, validation set because it's just too rare and by chance none- none of the edges, uh, landed there.",
    "start": "1526990",
    "end": "1533080"
  },
  {
    "text": "So that's why we wanna do this, what is called stratification, where we- where we independently split, uh,",
    "start": "1533080",
    "end": "1539575"
  },
  {
    "text": "each relation- edges of each relation type separately, and then merge it all, ah, together.",
    "start": "1539575",
    "end": "1545350"
  },
  {
    "text": "Um, so we have these four different edge buckets, training message as edges,",
    "start": "1545350",
    "end": "1551034"
  },
  {
    "text": "training supervision edges, validation edges, and test edges. We do the splitting for each relation type separately",
    "start": "1551035",
    "end": "1557740"
  },
  {
    "text": "and then merge it all together into the four sets. And then, you know, everything still, uh,",
    "start": "1557740",
    "end": "1563590"
  },
  {
    "text": "applies as we talked about, ah, link prediction. So to tell more or give you more details about how you formalize link prediction,",
    "start": "1563590",
    "end": "1572140"
  },
  {
    "text": "you look at heterogeneous graphs. Imagine, you know, I wanna be able to predict, um, whether there is an edge or what's the probability of an edge between nodes E and A,",
    "start": "1572140",
    "end": "1581799"
  },
  {
    "text": "um, and that- that edge is of, ah, type 3- of relation type 3.",
    "start": "1581800",
    "end": "1586870"
  },
  {
    "text": "Um, so imagine that this edge is a training supervision edge, right? And let's say that all edge- all other edges are,",
    "start": "1586870",
    "end": "1594370"
  },
  {
    "text": "uh, training message edges. So the way I would now, uh, use the RGCN to score these edges,",
    "start": "1594370",
    "end": "1601045"
  },
  {
    "text": "I would take the final, uh, layer embedding of node D. I would take the final layer embedding of node A.",
    "start": "1601045",
    "end": "1607795"
  },
  {
    "text": "And then I would have a relation specific scoring function f, ah, that would basically, um,",
    "start": "1607795",
    "end": "1614320"
  },
  {
    "text": "take these two embeddings and transform them into a real- ah, into a real value.",
    "start": "1614320",
    "end": "1619825"
  },
  {
    "text": "So one app- approach to do this would be to use this kind of, ah, uh, linear- uh,",
    "start": "1619825",
    "end": "1626695"
  },
  {
    "text": "bi-linear form, where basically I take one embedding, I have the transformation matrix in-between and another embedding,",
    "start": "1626695",
    "end": "1633160"
  },
  {
    "text": "ah, so that at the end, basically this takes, take the embedding of node D, transform it,",
    "start": "1633160",
    "end": "1638590"
  },
  {
    "text": "and then dot-product it with, uh, embedding of node A.",
    "start": "1638590",
    "end": "1643600"
  },
  {
    "text": "And I can interpret these, perhaps send it through some sigmoid or something like that. I can interpret this simply as the probability that- ah,",
    "start": "1643600",
    "end": "1652300"
  },
  {
    "text": "there is an edge of relation type, let's say r_1 between nodes E and A. So that would be one way, uh,",
    "start": "1652300",
    "end": "1659020"
  },
  {
    "text": "to- to actually formalize and, uh, instantiate, ah, this, ah, problem.",
    "start": "1659020",
    "end": "1666175"
  },
  {
    "text": "Now, um, how exactly am I thinking about these during training? So let's assume again that this is the edge,",
    "start": "1666175",
    "end": "1673345"
  },
  {
    "text": "that is a supervision edge. Um, and let's think that, ah, all other, er,",
    "start": "1673345",
    "end": "1678940"
  },
  {
    "text": "edges in the graph are- are the training message passing, ah, edges.",
    "start": "1678940",
    "end": "1684879"
  },
  {
    "text": "So we wanna use the training message-passing edges to predict, ah, the likelihood or the existence of this,",
    "start": "1684880",
    "end": "1692514"
  },
  {
    "text": "ah, training, ah, supervision edge of interest. What we also have to do in link prediction is that we have to create negative instances.",
    "start": "1692515",
    "end": "1702420"
  },
  {
    "text": "We have to create, ah, negative edges. So the way we create negative edges is by perturbing the supervision edge.",
    "start": "1702420",
    "end": "1710650"
  },
  {
    "text": "So for example, if this is the supervision edge, then one way how we can do- do it is that we corrupt the tail of it, right?",
    "start": "1710650",
    "end": "1718540"
  },
  {
    "text": "So we maintain the head, we maintain node E, but we pick some other node,",
    "start": "1718540",
    "end": "1724149"
  },
  {
    "text": "ah, that node E is not connected to with the relation of type r_3.",
    "start": "1724150",
    "end": "1730450"
  },
  {
    "text": "So in our case, we could, for example, create an edge from E to B or from,",
    "start": "1730450",
    "end": "1736284"
  },
  {
    "text": "uh, E to D. So these could be our negative edges. What is important when you create negative edges,",
    "start": "1736285",
    "end": "1742810"
  },
  {
    "text": "the negative edges should not belong to training message edges or training supervision edges.",
    "start": "1742810",
    "end": "1748165"
  },
  {
    "text": "So for example, edge to the node C cannot be a negative edge because",
    "start": "1748165",
    "end": "1753385"
  },
  {
    "text": "node E is already connected to the node C with relation type 3.",
    "start": "1753385",
    "end": "1758680"
  },
  {
    "text": "So this is not a negative- uh, a negative edge because the edge already exists and it's of the same type as here.",
    "start": "1758680",
    "end": "1764590"
  },
  {
    "text": "So we don't wanna create this, uh, contradiction. So we have to be a bit careful when sampling these, uh, negative edges.",
    "start": "1764590",
    "end": "1772870"
  },
  {
    "text": "Now that we have, uh, created a negative edge by perturbing the tail,",
    "start": "1772870",
    "end": "1779680"
  },
  {
    "text": "uh, so the end point of the- of the supervision edge. We can now use,",
    "start": "1779680",
    "end": "1785575"
  },
  {
    "text": "uh, the GNN model, the RGCN, to score- to score the positive as well as the negative edges.",
    "start": "1785575",
    "end": "1791740"
  },
  {
    "text": "And the- you know, the loss function you would wanna optimize is a standard, uh, cross entropy loss where basically we want to",
    "start": "1791740",
    "end": "1798610"
  },
  {
    "text": "maximize the scores- the score of the training supervision edge, we would like to maximize the scores of this guy,",
    "start": "1798610",
    "end": "1804355"
  },
  {
    "text": "and we would like to minimize the score of negative edges, like for example, E to B or E to D. So that's how we- we would write down,",
    "start": "1804355",
    "end": "1813730"
  },
  {
    "text": "ah, the penalty, uh, as I show it here. Um, and then, er, use the optimizer stochastic gradient descent to",
    "start": "1813730",
    "end": "1821470"
  },
  {
    "text": "optimize the parameters of RGCN to basically assign high probability, high score to the training supervision edges and low score,",
    "start": "1821470",
    "end": "1829210"
  },
  {
    "text": "uh, to negative edges. Now that we have the model trained, now assume we move to the validation time.",
    "start": "1829210",
    "end": "1837265"
  },
  {
    "text": "We wanna validate, uh, the our- the- the performance of our models.",
    "start": "1837265",
    "end": "1843190"
  },
  {
    "text": "So let's assume now that the validation edge that I'm interested in is this edge between node E,",
    "start": "1843190",
    "end": "1849235"
  },
  {
    "text": "uh, and node D. And I'm interested whether it's of type r_3. Then in this case,",
    "start": "1849235",
    "end": "1855415"
  },
  {
    "text": "training message edges and training supervision- supervision edges basically means, in- in my example,",
    "start": "1855415",
    "end": "1861309"
  },
  {
    "text": "all existing edges of the graph are used, ah, for the message propagation and then at the validation time,",
    "start": "1861310",
    "end": "1868929"
  },
  {
    "text": "I'm basically using all these solid edges to do the message propagation. And I'm trying to score the value of this particular, uh,",
    "start": "1868930",
    "end": "1878305"
  },
  {
    "text": "edge from node E to node D, right? And again, the intuition here is that the score of",
    "start": "1878305",
    "end": "1885790"
  },
  {
    "text": "this edge should be higher than the score of, let's say all other, ah,",
    "start": "1885790",
    "end": "1891190"
  },
  {
    "text": "edges that are- that are in some sense negative or don't exist in the data set yet from node D. So for example,",
    "start": "1891190",
    "end": "1899230"
  },
  {
    "text": "this would mean that the score of this node edge from E to D has to be higher than the one from E to B in our case.",
    "start": "1899230",
    "end": "1907840"
  },
  {
    "text": "Um, and- and I cannot consider these other edges because they are already used as a message passing engages in my,",
    "start": "1907840",
    "end": "1916164"
  },
  {
    "text": "uh, validation, uh, RGCN. So notice that it is important that these sets of edges that are independent from each other,",
    "start": "1916165",
    "end": "1923875"
  },
  {
    "text": "uh, when we score them. So how would I evaluate this?",
    "start": "1923875",
    "end": "1929095"
  },
  {
    "text": "Right, I would get- use the RGCN to calculate the score of edge ED,",
    "start": "1929095",
    "end": "1934330"
  },
  {
    "text": "uh, according to the relation type 3. I would then calculate the score of all the negative edges.",
    "start": "1934330",
    "end": "1939655"
  },
  {
    "text": "So in my case, only two possible negative edges of type r_3 are E to B and E to F, right?",
    "start": "1939655",
    "end": "1947755"
  },
  {
    "text": "I cannot do A and C because they are already connected according to the relation 3.",
    "start": "1947755",
    "end": "1952945"
  },
  {
    "text": "So that would be kind of a contradiction. So I have only two negative edges. Ah, and the- the- the goal then is basically to obtain a score for all these three edges.",
    "start": "1952945",
    "end": "1964690"
  },
  {
    "text": "We rank them and hopefully the- the rank, the output score of the edge, ah,",
    "start": "1964690",
    "end": "1971155"
  },
  {
    "text": "ED will be higher than the output score of EF and BF.",
    "start": "1971155",
    "end": "1976595"
  },
  {
    "text": "And then how do I usually evaluate the calculation matrix? How do I usually, um, evaluate?",
    "start": "1976595",
    "end": "1983745"
  },
  {
    "text": "Um, I can do either what is called hits, which would be, how often was the correct positive edge ranked among",
    "start": "1983745",
    "end": "1990880"
  },
  {
    "text": "the top K of my predicted edges or I can do a reciprocal rank which is 1 over the rank of the-",
    "start": "1990880",
    "end": "1998875"
  },
  {
    "text": "of the- of the positive edge ranked among all other, uh, negative edges. And then you can do mean reciprocal rank,",
    "start": "1998875",
    "end": "2005835"
  },
  {
    "text": "and the higher the mean reciprocal rank, the better or high- the higher the hits score, the better.",
    "start": "2005835",
    "end": "2013710"
  },
  {
    "text": "So let me summarize. We talked about relational GCN, which is a graph neural network for heterogeneous graphs.",
    "start": "2013710",
    "end": "2022020"
  },
  {
    "text": "We talked about how to define it. We talked about how to have a relation type specific transformation functions.",
    "start": "2022020",
    "end": "2029280"
  },
  {
    "text": "Uh, we discussed about, uh, how to do entity classification as well as link prediction tasks.",
    "start": "2029280",
    "end": "2035640"
  },
  {
    "text": "We also discussed how- discussed how to make a relational GCN more scalable through, uh,",
    "start": "2035640",
    "end": "2041070"
  },
  {
    "text": "making matrices W, block diagonal or using it as a- as a linear combination of basis transformations.",
    "start": "2041070",
    "end": "2050129"
  },
  {
    "text": "So this kind of dictionary learning approach. And of course, you can take out GCN and extend it in any way you like, right?",
    "start": "2050130",
    "end": "2058080"
  },
  {
    "text": "You can- you could think that you have- you know, you could have RGNN, you could have RGraphSAGE,",
    "start": "2058080",
    "end": "2063540"
  },
  {
    "text": "you could have R graph attention networks. So all these, ah,",
    "start": "2063540",
    "end": "2068609"
  },
  {
    "text": "basic fundamental GNN architectures that we already discussed.",
    "start": "2068610",
    "end": "2073695"
  },
  {
    "text": "It's kind of natural to extend them to the- to this, er, multi-relational case, ah, by- by",
    "start": "2073695",
    "end": "2081149"
  },
  {
    "text": "basically adding relation specific, uh, transformations. So it's kind of natural that- how you can play with this model and make it more expressive,",
    "start": "2081150",
    "end": "2089700"
  },
  {
    "text": "and richer and so on.",
    "start": "2089700",
    "end": "2091929"
  }
]