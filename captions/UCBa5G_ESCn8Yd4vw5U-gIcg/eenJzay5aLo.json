[
  {
    "start": "0",
    "end": "5490"
  },
  {
    "text": "All right, welcome back. Welcome to the last\nlecture for CS234. What we'll do today is we'll\ndo a review and a wrap up.",
    "start": "5490",
    "end": "13140"
  },
  {
    "text": "And we're also going to\ndiscuss the quiz a little bit. But before we get\nstarted, I just wanted to remind\nus where we are.",
    "start": "13140",
    "end": "19590"
  },
  {
    "text": "So last time, we did the quiz. Today, we have a review of the\ncourse and looking forward.",
    "start": "19590",
    "end": "25793"
  },
  {
    "text": "So what we're going\nto do today is we're going to do a\ncombination of the quiz recap. And then looking\nforward to reviewing",
    "start": "25793",
    "end": "32978"
  },
  {
    "text": "some of the things\nwe've done in the class, as well as looking forward. So we're going to\njump into the quiz.",
    "start": "32978",
    "end": "38489"
  },
  {
    "text": "The quiz, we'll have back to\nyou guys within about a day. And we're just going to\nstep through some of it,",
    "start": "38490",
    "end": "44550"
  },
  {
    "text": "because I think it's\na nice summary of some of the different aspects. So we'll go to the quiz.",
    "start": "44550",
    "end": "51510"
  },
  {
    "text": "I'm going to start\non question three. So the quiz, as everybody\nknows, was comprehensive.",
    "start": "51510",
    "end": "59100"
  },
  {
    "text": "It covered the entire course. We're not finished\ngrading it yet. But we noticed that\nthere were some problems",
    "start": "59100",
    "end": "65059"
  },
  {
    "text": "that people had more\nchallenges with than others. One quick clarification is that\nwhen we said a justification",
    "start": "65060",
    "end": "74780"
  },
  {
    "text": "for your choice, we expected\nyou to put something different than the choice itself. So we wanted you\nto actually provide",
    "start": "74780",
    "end": "81890"
  },
  {
    "text": "an explanation or rationale for\nwhy you picked what you did. So let's just step\nthrough the quiz.",
    "start": "81890",
    "end": "87620"
  },
  {
    "text": "And inside of the solutions,\nwe'll also do that. You might have noticed that\nthe second question was identical to the midterm.",
    "start": "87620",
    "end": "93210"
  },
  {
    "text": "So it was a good chance to\nrefresh if you hadn't remembered that from the midterm. The third one was\nslightly tricky.",
    "start": "93210",
    "end": "100440"
  },
  {
    "text": "So I want to just make\nsure to go through it. It's a nice way to review PPO. So the third\nquestion really asks",
    "start": "100440",
    "end": "107060"
  },
  {
    "text": "you to think about proximal\npolicy optimization, which was something that you implemented. And one thing that might\nhave been slightly confusing,",
    "start": "107060",
    "end": "114420"
  },
  {
    "text": "or a good thing to\nrefresh, is that we really emphasized in class that PPO\nallowed us to use data and make",
    "start": "114420",
    "end": "120090"
  },
  {
    "text": "multiple gradient steps. And when it made\nmultiple gradient steps, those would be off policy.",
    "start": "120090",
    "end": "126160"
  },
  {
    "text": "But the very first step that\nPPO makes is always on policy. So this is true.",
    "start": "126160",
    "end": "135000"
  },
  {
    "text": "Because if you've\njust gotten the data, and then you're doing a\npolicy gradient step on that, that part is\nconsidered on policy.",
    "start": "135000",
    "end": "141730"
  },
  {
    "text": "After that, you are trying\nto take a further step. So if you had sort of a\none dimensional policy,",
    "start": "141730",
    "end": "147550"
  },
  {
    "text": "your first step is\ngoing to be on policy. And then any further\nsteps you take are now going to be off policy,\nusing data that you collected",
    "start": "147550",
    "end": "154560"
  },
  {
    "text": "from the previous round. So I know that was often a good\nthing to make sure to refresh.",
    "start": "154560",
    "end": "160420"
  },
  {
    "text": "And the second part was, we do\nnot have any guarantees on B.",
    "start": "160420",
    "end": "165780"
  },
  {
    "text": "And the third part is true. And we want to emphasize here\nthat we're only doing importance sampling over the actions.",
    "start": "165780",
    "end": "172560"
  },
  {
    "text": "What PPO does and what some\nof the other algorithms it was inspired by do\nis that they don't",
    "start": "172560",
    "end": "177930"
  },
  {
    "text": "try to directly handle the\nstate distribution mismatch. And instead, they try to\ncreate a new policy that's",
    "start": "177930",
    "end": "183840"
  },
  {
    "text": "close enough that they\nhope that the fact that you're going to be\nvisiting different states under a new policy, it's only\ngoing to be slightly observed.",
    "start": "183840",
    "end": "193792"
  },
  {
    "text": "And the last is that you can\nuse lots of different types of advantage estimators. And so D is not true.",
    "start": "193792",
    "end": "200000"
  },
  {
    "text": "You could use generalized\nadvantage estimation, but you could also use\nother methods as well.",
    "start": "200000",
    "end": "206260"
  },
  {
    "text": "And throughout this, if\nanybody has any questions, feel free to ask me. So the fourth question was\ngiven by our guest lecturer.",
    "start": "206260",
    "end": "214640"
  },
  {
    "text": "For those of you that had a\nchance to attend or to watch it later, he talked a lot--",
    "start": "214640",
    "end": "220879"
  },
  {
    "text": "Dan talked a lot about thinking\nabout the alignment problem and thinking about what\nthings are important for that.",
    "start": "220880",
    "end": "227380"
  },
  {
    "text": "The first part is not true. It's generally hard\nto think about. There's different ways\nto think about autonomy,",
    "start": "227380",
    "end": "233959"
  },
  {
    "text": "but that was not what\nwe were focused on. The second one is true. So one of the things Dan talked\nabout a lot in his lecture",
    "start": "233960",
    "end": "240550"
  },
  {
    "text": "was the fact that, often, when\nwe think about preferences and alignment, we often\nare focusing on people's",
    "start": "240550",
    "end": "246070"
  },
  {
    "text": "individual preferences. Like someone says,\nthey like option one instead of option two. But that focuses,\nreally, on the utility",
    "start": "246070",
    "end": "253600"
  },
  {
    "text": "to a single\nindividual as opposed to the implications for\nthe broader society. And so the second one is true\nbecause, as Dan brought up,",
    "start": "253600",
    "end": "261320"
  },
  {
    "text": "moral theories give us a way to\nthink about more broad benefits to society and to\ncollections of individuals",
    "start": "261320",
    "end": "267070"
  },
  {
    "text": "instead of to individuals. He also talked a lot about\nhow autonomy is often",
    "start": "267070",
    "end": "273050"
  },
  {
    "text": "a core principle when\nwe think about the value of different\ndecisions we can make.",
    "start": "273050",
    "end": "279300"
  },
  {
    "text": "And so the idea of an AI\nagent to allowing people",
    "start": "279300",
    "end": "286669"
  },
  {
    "text": "to have some autonomy would\nsay that an AI agent that thinks about someone's\nsuboptimal decision--",
    "start": "286670",
    "end": "294000"
  },
  {
    "text": "so it might be that\nsomebody really wants to do something that we\nknow is not very good for them. That an agent that\nis aligned and allows",
    "start": "294000",
    "end": "300770"
  },
  {
    "text": "that person autonomy would\nstill support that because-- in the interest of upweighting\nthe degree of autonomy.",
    "start": "300770",
    "end": "309290"
  },
  {
    "text": "And the last one is\nalso true, because you",
    "start": "309290",
    "end": "314390"
  },
  {
    "text": "could think of this as\na form of paternalism. So if the agent\ndecides, well, it's not really a good\nidea for you to smoke.",
    "start": "314390",
    "end": "320130"
  },
  {
    "text": "And so I'm not going to tell you\nwhere you can buy cigarettes. That may or may not be true.",
    "start": "320130",
    "end": "325200"
  },
  {
    "text": "Of course, we know that\nsmoking is causally associated with lung cancer. So you could imagine,\nin that case,",
    "start": "325200",
    "end": "331336"
  },
  {
    "text": "it's not in their best interest. But that would be considered\na form of paternalism. And so that would\nundermine user autonomy.",
    "start": "331337",
    "end": "338070"
  },
  {
    "text": "Yeah, I don't know if I agree\nwith even the explanation because it seems to me that\nbest interest and suboptimal",
    "start": "338070",
    "end": "345020"
  },
  {
    "text": "decisions are definitionally-- well, best interests entail\nlike optimal decisions.",
    "start": "345020",
    "end": "352200"
  },
  {
    "text": "And you're saying we\nshould let the user make a suboptimal decisions. I don't see how those are--",
    "start": "352200",
    "end": "358400"
  },
  {
    "text": "it is in fact in the\nbest interest of the user to make decisions,\nthen those decisions",
    "start": "358400",
    "end": "363440"
  },
  {
    "text": "are no longer suboptimal. We should be a contradiction,\nit seems to be. I don't see how that follows.",
    "start": "363440",
    "end": "368610"
  },
  {
    "text": "It's an interesting question. So I think what it says here is\nthat there is different notions of what the objective is.",
    "start": "368610",
    "end": "374132"
  },
  {
    "text": "And there are different\nnotions of what is considered optimal or not optimal. So there might be some cases\nwhere, for general population,",
    "start": "374132",
    "end": "381545"
  },
  {
    "text": "or even for humans in general,\nthere is one part of your reward function that says, this\nparticular decision, like smoking cigarettes, is not\nconsidered to be optimal because",
    "start": "381545",
    "end": "389180"
  },
  {
    "text": "of long-term health outcomes. However, you might have another\npart of your reward function that talks about the\nimportance of user autonomy.",
    "start": "389180",
    "end": "395880"
  },
  {
    "text": "And so if you value user\nautonomy higher than, say,",
    "start": "395880",
    "end": "401420"
  },
  {
    "text": "someone's health, perhaps,\nin this particular instance-- for that particular\nconstraint, then in that case,",
    "start": "401420",
    "end": "406500"
  },
  {
    "text": "you might say, well, if I'm\nsupporting that person-- so if the user's best interest--\nif the best interest of the user",
    "start": "406500",
    "end": "413210"
  },
  {
    "text": "is to more value their ability\nto have autonomy, then for them to make this particular\nhealth decision,",
    "start": "413210",
    "end": "419583"
  },
  {
    "text": "then you would give\nthem the information about where cigarettes are. Yeah.",
    "start": "419583",
    "end": "425180"
  },
  {
    "text": "So just the first clause,\nsince it is in the user's best interest, I thought\nit's really hard",
    "start": "425180",
    "end": "432230"
  },
  {
    "text": "to generalize about what a\nsingle user's best interest was. And so that was not a true\nstatement by the start.",
    "start": "432230",
    "end": "439550"
  },
  {
    "text": "Because maybe some people aren't\nbest making their own decisions about things. And so I wasn't sure how\nconfidently you could say that.",
    "start": "439550",
    "end": "446532"
  },
  {
    "text": "Yeah, it's an\ninteresting question. So what Dan argued is that\nit is generally a principle",
    "start": "446532",
    "end": "453440"
  },
  {
    "text": "that everyone needs\nsome amount of autonomy. And so if you go\nwith that argument, then you would\nsay, if we believe",
    "start": "453440",
    "end": "460850"
  },
  {
    "text": "that it is important for all\nof us to have some autonomy, then under that,\nthat should also allow us the freedom to make\nbad decisions some of the time.",
    "start": "460850",
    "end": "468000"
  },
  {
    "text": "And in that case, an LLM\nthat is supporting us also needs to be able to respect\nthose bad decisions.",
    "start": "468000",
    "end": "473585"
  },
  {
    "text": "And you could\ndisagree with this. You could disagree\nwith that as a premise, in terms of the type of theories\nthat promote that everyone",
    "start": "473585",
    "end": "480470"
  },
  {
    "text": "should have autonomy. And we give different\npeople in society, different amounts of autonomy. Children generally\nhave less than adults.",
    "start": "480470",
    "end": "487190"
  },
  {
    "text": "But if you assume that that's\nthe case-- as long as you assume that it's always important\nfor every individual",
    "start": "487190",
    "end": "493850"
  },
  {
    "text": "to have some amount\nof autonomy, that would include allowing them to\nmake bad decisions sometimes.",
    "start": "493850",
    "end": "498889"
  },
  {
    "text": "If our justification\nis like, that's not something you can assume. How would that be?",
    "start": "498890",
    "end": "504302"
  },
  {
    "text": "I don't know how that\nwould [INAUDIBLE]. I didn't grade this\nparticular question. But you can definitely see\nwhat they say in terms of that.",
    "start": "504303",
    "end": "510310"
  },
  {
    "text": "Yeah, we will look at everyone's\njustification in terms of that. Good questions.",
    "start": "510310",
    "end": "515461"
  },
  {
    "text": " The next one that I\nwanted to go through",
    "start": "515461",
    "end": "521820"
  },
  {
    "text": "was Monte Carlo tree search. So this is another\none that people--",
    "start": "521820",
    "end": "527430"
  },
  {
    "text": "there was a little\nbit of differences over in terms of whether\nthis was something people had",
    "start": "527430",
    "end": "532620"
  },
  {
    "text": "some questions on. So the first one is true.",
    "start": "532620",
    "end": "537690"
  },
  {
    "text": "So Monte Carlo tree\nsearch, the MCTS, the M and the Monte Carlo tree\nsearch stands for Monte, not",
    "start": "537690",
    "end": "543450"
  },
  {
    "text": "for Markov. And the way that we\ndescribed it in class, you can use it in both.",
    "start": "543450",
    "end": "549310"
  },
  {
    "text": "So what we do in Monte\nCarlo tree search is we sample from the dynamics\nmodel to get to a next state.",
    "start": "549310",
    "end": "555040"
  },
  {
    "text": "And as long as we\ncan sample from that, whether that's a Markov\nmodel, or if you required all of the history so far in\nthe tree to make that dynamics,",
    "start": "555040",
    "end": "562270"
  },
  {
    "text": "that would be OK. So there's not something\ninherent in Monte Carlo tree search that means you always\nhave to have a Markov system.",
    "start": "562270",
    "end": "570840"
  },
  {
    "text": "The second is true. So the way that Monte Carlo\ntree search uses sampling is it samples the next state.",
    "start": "570840",
    "end": "577523"
  },
  {
    "text": "And what it does\nthere is it means that instead of having to\nenumerate all possible states, you could just sample\na subset of them",
    "start": "577523",
    "end": "583890"
  },
  {
    "text": "and still get an accurate\nestimation of the expectation.",
    "start": "583890",
    "end": "589590"
  },
  {
    "text": "And the fourth is false. So this is not true\nbecause in this case,",
    "start": "589590",
    "end": "597790"
  },
  {
    "text": "like in a lot of settings,\nincluding AlphaGo, the reward model is known.",
    "start": "597790",
    "end": "602860"
  },
  {
    "text": "So we're not trying to\nlearn the reward model. But upper confidence\nbounds are still useful because they allow us to\nprioritize among the actions.",
    "start": "602860",
    "end": "610810"
  },
  {
    "text": "Because ultimately, we want to\nbe thinking about taking maxes. And so in these cases,\nupper confidence bounds,",
    "start": "610810",
    "end": "617380"
  },
  {
    "text": "like upper confidence\nbound trees, are using UCB to\nactively think of how",
    "start": "617380",
    "end": "623910"
  },
  {
    "text": "we're expanding out the tree. And then the fourth\none is also true, because that's exactly what\nAlphaZero does, is they",
    "start": "623910",
    "end": "632790"
  },
  {
    "text": "use self-play to\nimprove the network, to predict values and\naction probabilities.",
    "start": "632790",
    "end": "638530"
  },
  {
    "text": "Yeah? The fourth on is false? It's false. It's false? It's false.",
    "start": "638530",
    "end": "643760"
  },
  {
    "text": "Yeah. Yeah? I'm a bit confused\nabout the false wording. It says Monte Carlo tree\nsearch is used in AlphaZero.",
    "start": "643760",
    "end": "651760"
  },
  {
    "text": "But doesn't AlphaZero use a\ndifferent variant of a tree search that is not Monte Carlo?",
    "start": "651760",
    "end": "657460"
  },
  {
    "text": "I am a bit confused. AlphaZero uses Monte\nCarlo tree search. It uses Monte Carlo tree\nsearch with self-play",
    "start": "657460",
    "end": "662950"
  },
  {
    "text": "to train a network that predicts\nvalues and action probabilities. That's part of what it does. Well, wasn't there\na different thing",
    "start": "662950",
    "end": "669190"
  },
  {
    "text": "that had the confidence for-- Upper confidence trees?",
    "start": "669190",
    "end": "674750"
  },
  {
    "text": "Yeah. Yeah. Well, it uses a particular\nform of upper confidence trees as well.",
    "start": "674750",
    "end": "679850"
  },
  {
    "text": "So upper confidence trees is a\ntype of Monte Carlo tree search. It's like Monte Carlo tree\nsearch is a superset of UCT.",
    "start": "679850",
    "end": "685454"
  },
  {
    "start": "685455",
    "end": "697700"
  },
  {
    "text": "Let's go through these two,\nbecause all of these are true. And so I think this is a useful\none to go through as well.",
    "start": "697700",
    "end": "705270"
  },
  {
    "text": "So ChatGPT did learn\nrewards from humans,",
    "start": "705270",
    "end": "710970"
  },
  {
    "text": "preparing preferences\nover prompt-output pairs. And then use PPO to\ntrain a better prompt. So in fact, we did a ranking\nwhere they did do this.",
    "start": "710970",
    "end": "720260"
  },
  {
    "text": "In general, for long horizon\nproblems and really large action spaces, that\nwould be somewhere",
    "start": "720260",
    "end": "727272"
  },
  {
    "text": "where forward search would\nbe really expensive to do. So using something like\nAlphaZero, which essentially builds a subset of\nthe tree search,",
    "start": "727272",
    "end": "733190"
  },
  {
    "text": "it can be really helpful. In PAC, probably\napproximately correct methods.",
    "start": "733190",
    "end": "738360"
  },
  {
    "text": "We are guaranteed to learn\nan epsilon optimal policy. But the epsilon\nmight be non-zero, which means we're not guaranteed\nto learn an optimal policy.",
    "start": "738360",
    "end": "745580"
  },
  {
    "text": "So if you want to get a-- if\nyou're OK with your kitchen being slightly\nmessy, which I am,",
    "start": "745580",
    "end": "750680"
  },
  {
    "text": "then it would be OK to\nuse a PAC RL algorithm. That would make a finite\nnumber of mistakes. But most of the time, would\nkeep your kitchen pretty neat.",
    "start": "750680",
    "end": "758550"
  },
  {
    "text": "But maybe not perfectly neat. And then offline RL\nmay be particularly beneficial for health care\nand other high-stake settings",
    "start": "758550",
    "end": "765060"
  },
  {
    "text": "where online exploration might\nbe risky or very expensive. So in this case, all\nof these were true.",
    "start": "765060",
    "end": "771680"
  },
  {
    "text": " And then the next one I\nwas going to talk about",
    "start": "771680",
    "end": "777350"
  },
  {
    "text": "is 9, where we go through some\nof the theoretical properties. So in this case, what we\nhave is that optimism--",
    "start": "777350",
    "end": "789259"
  },
  {
    "text": "in this case, the\nfirst one is not true,",
    "start": "789260",
    "end": "794300"
  },
  {
    "text": "because we don't have\nany guarantees in general for the REINFORCE algorithm. The second one is true for\nthe reasons we just said.",
    "start": "794300",
    "end": "801720"
  },
  {
    "text": "A PAC algorithm guarantees\nyour epsilon optimal. So this is not\nnecessarily fully optimal.",
    "start": "801720",
    "end": "808459"
  },
  {
    "text": "In the third case,\nit is not guaranteed to have sublinear regret.",
    "start": "808460",
    "end": "814340"
  },
  {
    "text": "So this is false. And the reason,\nagain, for this is that if you just get an\nepsilon optimal policy,",
    "start": "814340",
    "end": "819780"
  },
  {
    "text": "you might make epsilon\nmistakes for the rest of time. So epsilon times t, which would\nstill give you a linear regret.",
    "start": "819780",
    "end": "826610"
  },
  {
    "text": "The fourth is also false. So in general, you can think\nof just minimizing regret. Which is the difference\nbetween your policy",
    "start": "826610",
    "end": "832550"
  },
  {
    "text": "and the optimal policy. Or maximizing expected\ncumulative rewards. And they're just the same thing.",
    "start": "832550",
    "end": "837570"
  },
  {
    "text": "One, you just subtract\nfrom the other. Can either maximize\ncumulative regret or minimize-- maximize\ncumulative reward",
    "start": "837570",
    "end": "843180"
  },
  {
    "text": "or minimize regret. And then in the fourth case,\nthis will not necessarily",
    "start": "843180",
    "end": "854010"
  },
  {
    "text": "be a PAC algorithm. So the only one in this\ncase is only B is true.",
    "start": "854010",
    "end": "861570"
  },
  {
    "text": "And the reason for\nthis is a PAC algorithm has to make a finite\nnumber of mistakes. So it normally has\nto be polynomial.",
    "start": "861570",
    "end": "868560"
  },
  {
    "text": "And so even this would\nsay this algorithm would be consistently converging\nto the optimal policy. But you don't necessarily\nknow how long it would take.",
    "start": "868560",
    "end": "875760"
  },
  {
    "text": "So it could be very expensive. And then the final\nquestion has us",
    "start": "875760",
    "end": "881940"
  },
  {
    "text": "think about which algorithms\ncould generate the observed",
    "start": "881940",
    "end": "887000"
  },
  {
    "text": "reward. Raise your hand if anybody\nwants me to go through that? I'm happy to, and\nstep through it. Or otherwise, we'll move\non to the next slide.",
    "start": "887000",
    "end": "894720"
  },
  {
    "text": "Allow the people to just\nthink about how different algorithms would run,\nand whether or not they could generate\nthis observed data.",
    "start": "894720",
    "end": "900925"
  },
  {
    "start": "900925",
    "end": "906815"
  },
  {
    "text": "Well, we'll release\nthe solutions for the quiz over the next day. And we'll also\nrelease the grades.",
    "start": "906815",
    "end": "912650"
  },
  {
    "text": " Yeah? So go back [INAUDIBLE]\nmean that we're only",
    "start": "912650",
    "end": "920660"
  },
  {
    "text": "allowed to meet a finite\nnumber of mistakes? Or is it that your\n[INAUDIBLE] mistakes",
    "start": "920660",
    "end": "927139"
  },
  {
    "text": "should be within some budget? Finite number of mistakes. So a great question.",
    "start": "927140",
    "end": "932399"
  },
  {
    "text": "So in terms of PAC,\nwhat we normally require is that the\nnumber of mistakes made-- well, with high probability\non all but a finite number,",
    "start": "932400",
    "end": "940410"
  },
  {
    "text": "you will be epsilon optimal. And that finite number needs\nto be a polynomial function of your problem parameters,\nincluding like 1 over epsilon,",
    "start": "940410",
    "end": "947192"
  },
  {
    "text": "the size of your state space,\nthe size of your action space, et cetera. ",
    "start": "947192",
    "end": "952963"
  },
  {
    "text": "It doesn't always tell you\nwhen those mistakes will occur. They may be at the beginning,\nor they may be later. I was thinking if you could\nhave some sort of [INAUDIBLE]",
    "start": "952963",
    "end": "962590"
  },
  {
    "text": "mistakes that it would still\nbe fine as long as there's some [INAUDIBLE] some\nform [INAUDIBLE] sequence",
    "start": "962590",
    "end": "971290"
  },
  {
    "text": "[INAUDIBLE]. Yeah. I mean, you certainly\ncould do that. It wouldn't be PAC,\nunless you could guarantee",
    "start": "971290",
    "end": "976720"
  },
  {
    "text": "with high probability that\ntotal number of mistakes would be small. Yeah, it's a good question. One thing some of the work\nthat we have done in the past",
    "start": "976720",
    "end": "983713"
  },
  {
    "text": "is-- you may or\nmay not know what epsilon you want to\ncommit to in advance. And so we've also\ndeveloped algorithms where you could think\nof this occurring",
    "start": "983713",
    "end": "990640"
  },
  {
    "text": "for different epsilons. Maybe as you have different\namounts of budget, you might want to be\nable to pick epsilon. If you get a lot of data,\nmaybe you can get more optimal.",
    "start": "990640",
    "end": "998546"
  },
  {
    "text": "Good question. Anybody else have\nquestions about the quiz? ",
    "start": "998546",
    "end": "1005262"
  },
  {
    "text": "All right, well, feel free\nto-- we have normal office hours this week, so feel free\nto come to our office hours.",
    "start": "1005262",
    "end": "1010420"
  },
  {
    "text": "Next week, we will not\nhave office hours anymore. But if you have any\nquestions about the quiz or about your projects,\nfeel free to come see us.",
    "start": "1010420",
    "end": "1017282"
  },
  {
    "start": "1017282",
    "end": "1023000"
  },
  {
    "text": "So I think it's always\nexciting to go back to the beginning of the\nquarter and think back",
    "start": "1023000",
    "end": "1028240"
  },
  {
    "text": "of all the things\nthat we've covered, as well as looking forward\nin terms of the field. So when we very started\nthe very first lecture--",
    "start": "1028240",
    "end": "1034819"
  },
  {
    "text": "the slide might look\nsomewhat familiar-- we talked about how reinforcement\nlearning is fundamentally the question of learning\nthrough experience to make",
    "start": "1034819",
    "end": "1042280"
  },
  {
    "text": "good decisions in order to\noptimize our long-term reward. And so that's really\nthe central question",
    "start": "1042280",
    "end": "1047410"
  },
  {
    "text": "that it tries to\nstart to answer. And we talked about there being\na number of different learning",
    "start": "1047410",
    "end": "1053010"
  },
  {
    "text": "objectives in the course. And so what I hope that\npeople will walk away with in this class\nis to understand",
    "start": "1053010",
    "end": "1058650"
  },
  {
    "text": "what are the key features\nof reinforcement learning. And how does that change\ncompared to supervised learning,",
    "start": "1058650",
    "end": "1063690"
  },
  {
    "text": "and AI planning, a lot of other\nareas, or unsupervised learning? To understand how if you're\ngiven an application problem,",
    "start": "1063690",
    "end": "1071049"
  },
  {
    "text": "whether and how you should\nuse RL for it, what algorithms might be appropriate. To be able to implement\nin code RL algorithms.",
    "start": "1071050",
    "end": "1077679"
  },
  {
    "text": "And you have had lots\nof practice with that. And then to understand how\nwe would compare and contrast about what it means to\nhave a good RL algorithm.",
    "start": "1077680",
    "end": "1084760"
  },
  {
    "text": "And what are the ways we should\nevaluate algorithms themselves as a way to help us understand\nif we're making progress.",
    "start": "1084760",
    "end": "1091150"
  },
  {
    "text": "So thinking about\nthings like regret, sample complexity,\ncomputational complexity, empirical performance.",
    "start": "1091150",
    "end": "1096520"
  },
  {
    "text": "Does it converge to\nthe optimal policy? Does it converge at all? And then also to understand\nthe exploration-exploitation",
    "start": "1096520",
    "end": "1103560"
  },
  {
    "text": "challenge in terms\nof data collection. And these sort of\nfundamental challenges we have between the\ndata that we gather,",
    "start": "1103560",
    "end": "1110558"
  },
  {
    "text": "allowing us to learn things\nabout the environment and about different decision\npolicies, versus using that information to actually\nobtain high rewards.",
    "start": "1110558",
    "end": "1119350"
  },
  {
    "text": "And so throughout this,\nyou've had a chance to think about this on\nthe quiz, on the midterm, on the homeworks. And then now also in\nyour final project.",
    "start": "1119350",
    "end": "1127770"
  },
  {
    "text": "So what I'd like to\ndo now is to, again, revisit the second question. Because I think, really, as\nyou go forward, this will be--",
    "start": "1127770",
    "end": "1134919"
  },
  {
    "text": "when you use\nreinforcement learning, this is going to be\none of the things that you constantly have to do. Which is to decide for any\nnew problem you're looking at,",
    "start": "1134920",
    "end": "1141340"
  },
  {
    "text": "is it appropriate to think about\nreinforcement learning as a tool to help you solve that problem?",
    "start": "1141340",
    "end": "1146850"
  },
  {
    "text": "And so I think to do that,\nit's helpful to go back to the motivating domains\nfrom the first lecture.",
    "start": "1146850",
    "end": "1151980"
  },
  {
    "text": " So three of the\ndomains-- we talked about a number of different\ndomains throughout the class.",
    "start": "1151980",
    "end": "1158720"
  },
  {
    "text": "But here are three\nof the domains that we talked about\non the first lecture. So the first one is AlphaTensor.",
    "start": "1158720",
    "end": "1165170"
  },
  {
    "text": "This is AlphaTensor. ",
    "start": "1165170",
    "end": "1170910"
  },
  {
    "text": "And in AlphaTensor, the\nidea was to figure out a more effective algorithm for\nlearning to multiply matrices.",
    "start": "1170910",
    "end": "1177390"
  },
  {
    "text": "And the amazingly\nbeautiful thing they did in that case\nis that they actually are doing reinforcement learning\nto learn algorithms, which",
    "start": "1177390",
    "end": "1184890"
  },
  {
    "text": "I still think is\nreally incredible. It's extremely creative. And so what they want to think\nabout in this case is if you",
    "start": "1184890",
    "end": "1191399"
  },
  {
    "text": "want to multiply two matrices--\nthis is just two by two, but they go beyond that-- what is the way we\nshould operationalize",
    "start": "1191400",
    "end": "1198720"
  },
  {
    "text": "that so that we can think about\nthe particular products and sums that we're doing\nin order to reduce",
    "start": "1198720",
    "end": "1203940"
  },
  {
    "text": "the amount of\ncomputational complexity we need to accomplish\nthis in a correct way?",
    "start": "1203940",
    "end": "1209310"
  },
  {
    "text": "And what the\nresearchers at DeepMind were doing when they\nwere thinking about this is they were thinking\nabout a common task that",
    "start": "1209310",
    "end": "1215277"
  },
  {
    "text": "comes up everywhere. We multiply matrices all the\ntime for almost all of AI and machine learning.",
    "start": "1215277",
    "end": "1221790"
  },
  {
    "text": "And so we're really relying\nunderneath those constantly that cost that we're doing. And so they're thinking\nabout, can we essentially",
    "start": "1221790",
    "end": "1228120"
  },
  {
    "text": "invent better algorithms for\ndoing some of these really basic substructures? So I think that was\nreally exciting.",
    "start": "1228120",
    "end": "1234760"
  },
  {
    "text": "And I think this is\none of the domains that now you have\nsome of the tools to be able to do the\nsame types of algorithms",
    "start": "1234760",
    "end": "1240850"
  },
  {
    "text": "is what they use to solve this. Bless you. So what I'm going\nto do right now is I'm going to\nrevisit these three.",
    "start": "1240850",
    "end": "1246200"
  },
  {
    "text": "And then I'm going to ask\nyou to think about how given what you know now, how\nyou would formulate them. And some of them, I've talked\nabout a little bit more",
    "start": "1246200",
    "end": "1252820"
  },
  {
    "text": "or a little bit less. But I'll first just give\nyou a quick refresher so you can think about,\ngiven what you know,",
    "start": "1252820",
    "end": "1257860"
  },
  {
    "text": "you might formulate this. So the second one\nwas plasma control.",
    "start": "1257860",
    "end": "1263350"
  },
  {
    "text": "And this is much\nmore of a controls like, more like the\nMuJoCo type of task that we saw where they're\ntrying to manipulate and control",
    "start": "1263350",
    "end": "1271390"
  },
  {
    "text": "these different plasma. And they want to think\nabout a control policy to allow you to\nachieve different types",
    "start": "1271390",
    "end": "1276550"
  },
  {
    "text": "of configurations. And then the third one\nwas thinking about, how do we figure out who to\ntest given finite resources?",
    "start": "1276550",
    "end": "1285530"
  },
  {
    "text": "So this is for COVID testing. So if you have a bunch of\npeople coming off an airplane",
    "start": "1285530",
    "end": "1291309"
  },
  {
    "text": "and you have a finite\nnumber of tests, who can you test to\nbetter understand",
    "start": "1291310",
    "end": "1296740"
  },
  {
    "text": "who might be sick and\nrestrict the flow-- restrict the spread of COVID. And this is a process\nthat's happening",
    "start": "1296740",
    "end": "1304030"
  },
  {
    "text": "every day as people\nare flying into Greece, into different airports. And then you would send\noff those samples to labs.",
    "start": "1304030",
    "end": "1310870"
  },
  {
    "text": "And then a few days later,\nyou would get the results. And those people that\nyou asked to test could come out of quarantine.",
    "start": "1310870",
    "end": "1317470"
  },
  {
    "text": "And so what I'd\nlike you to do now, and I've posted a poll for this,\nis to think about the following. So I'll label this as well.",
    "start": "1317470",
    "end": "1323360"
  },
  {
    "text": "This is the AlphaTensor.  This is plasma, and\nthis is COVID testing.",
    "start": "1323360",
    "end": "1333880"
  },
  {
    "text": "If you can go on to the poll\nand say which domain are you choosing-- is it a bandit? Is it a multi-step RL problem?",
    "start": "1333880",
    "end": "1339890"
  },
  {
    "text": "What type of problem is this? What setting are we in? Is the problem an offline\nsetting or an online setting,",
    "start": "1339890",
    "end": "1345580"
  },
  {
    "text": "or some combination? What do you think the state\naction rewards might be? And what algorithms do\nyou think you would use",
    "start": "1345580",
    "end": "1353270"
  },
  {
    "text": "to try to tackle this problem?  And we'll take a few\nminutes to go through that.",
    "start": "1353270",
    "end": "1358975"
  },
  {
    "start": "1358975",
    "end": "1534330"
  },
  {
    "text": "I will give a few\nmore minutes, and then share some of our thoughts. ",
    "start": "1534330",
    "end": "1568748"
  },
  {
    "text": "And I think one good thing\nto think about in this too, is like, are there\nproblems with distribution shift that might come up?",
    "start": "1568748",
    "end": "1574769"
  },
  {
    "text": "Are there cases where we'd want\nto be conservative with respect to the results that\nare being generalized?",
    "start": "1574770",
    "end": "1580570"
  },
  {
    "text": "Or do we not have to worry\ntoo much about distribution shift in these cases?",
    "start": "1580570",
    "end": "1586289"
  },
  {
    "text": "Could there be unsafe states, or\ntoo risky, or things like that? ",
    "start": "1586290",
    "end": "1598740"
  },
  {
    "text": "So I think we actually\nhave a nice breakdown. Raise your hand\nif you did plasma.",
    "start": "1598740",
    "end": "1604980"
  },
  {
    "text": "Is there someone else here? So we do have another\nperson doing plasma, but they are remote then. Raise your hand\nif you did COVID.",
    "start": "1604980",
    "end": "1612500"
  },
  {
    "text": "OK, maybe you want\nto go near those guys so you guys can all\ncompare your answers. And then did you guys\nboth do AlphaTensor?",
    "start": "1612500",
    "end": "1619640"
  },
  {
    "text": "OK, perfect. So why don't we take a minute\nand talk to your neighbor. And I'll also come around\nand see if you guys came up with the same formulation.",
    "start": "1619640",
    "end": "1625679"
  },
  {
    "start": "1625680",
    "end": "1647094"
  },
  {
    "text": "[SIDE CONVERSATIONS] ",
    "start": "1647094",
    "end": "1718929"
  },
  {
    "text": "You can't be like, plasma, stop. Yeah, exactly. So this research or anything\nof that sort is ultimately--",
    "start": "1718930",
    "end": "1724880"
  },
  {
    "text": "[INTERPOSING VOICES] Yeah. So I think it would\nbe a-- it's like you-- [INTERPOSING VOICES]",
    "start": "1724880",
    "end": "1732019"
  },
  {
    "start": "1732020",
    "end": "1804920"
  },
  {
    "text": "I'm not sure how to-- like, what I put\ndown it was like, total number of test\ncases of the country.",
    "start": "1804920",
    "end": "1813300"
  },
  {
    "text": "But that doesn't represent\nthe people being tested. It's like [INAUDIBLE]. ",
    "start": "1813300",
    "end": "1827051"
  },
  {
    "text": "But there has to be some\nway to include the effect. That would be the hope.",
    "start": "1827051",
    "end": "1832260"
  },
  {
    "text": "Yeah, unless you really proxy\nit, and you could say like, it's the [INAUDIBLE]\nnumber of tests,",
    "start": "1832260",
    "end": "1837620"
  },
  {
    "text": "a fixed number of tests, and\nthen this number of [INAUDIBLE]. ",
    "start": "1837620",
    "end": "1880670"
  },
  {
    "text": "I would think it was closer\nto offline, because there's this batch setting with delay.",
    "start": "1880670",
    "end": "1886700"
  },
  {
    "text": "You have to make\ndecisions for [INAUDIBLE]. ",
    "start": "1886700",
    "end": "1918117"
  },
  {
    "text": "Well, then maybe\nlike-- so what do you think in terms of algorithms? [INTERPOSING VOICES]",
    "start": "1918117",
    "end": "1924033"
  },
  {
    "start": "1924033",
    "end": "1955257"
  },
  {
    "text": "That's right. Exactly. So you get Thompson sampling,\nthese samples from the prior.",
    "start": "1955258",
    "end": "1960330"
  },
  {
    "text": "So then-- yeah. This is a nice motivation\nfor why [INAUDIBLE].",
    "start": "1960330",
    "end": "1966760"
  },
  {
    "start": "1966760",
    "end": "1975787"
  },
  {
    "text": "Yeah, because they really want-- I mean, you kind of\nget some exploration that they have\nfeatures of people.",
    "start": "1975787",
    "end": "1981030"
  },
  {
    "text": "So you get-- it's not like\neveryone would necessarily get the same. But in general, if there\nwere two people that--",
    "start": "1981030",
    "end": "1986792"
  },
  {
    "text": "[INTERPOSING VOICES] ",
    "start": "1986792",
    "end": "1994375"
  },
  {
    "text": "I feel like Thompson sampling\nis kind of a cool way to do it without having\nto think about it. [INTERPOSING VOICES]",
    "start": "1994375",
    "end": "2000710"
  },
  {
    "start": "2000710",
    "end": "2046328"
  },
  {
    "text": "Because if someone's\ngoing to go on a farm, maybe it doesn't matter as much. But if they're going\nto go to Seoul, then I don't think any\nof that [INAUDIBLE].",
    "start": "2046328",
    "end": "2054198"
  },
  {
    "text": " Let's come back together.",
    "start": "2054198",
    "end": "2060310"
  },
  {
    "text": "I really like these domains. I think that they're\nreally interesting domains to think about, what is\nthe implications for how",
    "start": "2060310",
    "end": "2067480"
  },
  {
    "text": "we model these. And the choices that\nwe have to make. And what are the\nalgorithms that would work? So let's go through\nsome of them, because I know that a lot of\npeople pick different ones.",
    "start": "2067480",
    "end": "2074643"
  },
  {
    "text": "So AlphaTensor--\nbecause I think nobody-- because I know there's\nsome people who are watching online remotely,\nwe have more answers as well.",
    "start": "2074643",
    "end": "2082989"
  },
  {
    "text": "So I think it's\nreally interesting to see the perspective. I'm not sure that-- there was very few\npeople that mentioned",
    "start": "2082989",
    "end": "2088840"
  },
  {
    "text": "Monte Carlo tree search. But actually for AlphaTensor,\nit is something-- I guess maybe the alpha\nshould hint at that--",
    "start": "2088840",
    "end": "2095690"
  },
  {
    "text": "like AlphaZero, et cetera. It is something where they're\nusing reinforcement learning",
    "start": "2095690",
    "end": "2101110"
  },
  {
    "text": "and policy networks, et cetera. But they're also combining it\nwith AlphaZero-like technology.",
    "start": "2101110",
    "end": "2107690"
  },
  {
    "text": "So they use Monte Carlo\ntree search in this case. So they have Monte\nCarlo tree search.",
    "start": "2107690",
    "end": "2113440"
  },
  {
    "text": "Carlo tree search. So this is a reinforcement\nlearning problem.",
    "start": "2113440",
    "end": "2118700"
  },
  {
    "text": "It's multi-step,\nbecause the idea is that you want to\ntake a series of steps until you solve the\nmultiplication problem.",
    "start": "2118700",
    "end": "2124830"
  },
  {
    "text": "And what the steps\nare, in this case, is you could think of it as\nlike algorithmic steps. So like, which parts of your--",
    "start": "2124830",
    "end": "2131690"
  },
  {
    "text": "if we go back to here. Let me just make this\nbig for a second.",
    "start": "2131690",
    "end": "2137250"
  },
  {
    "text": "If you think of this, when\nyou do matrix multiplication, you have A1 times B1, and A2\nplus A2 times B3, A1 times",
    "start": "2137250",
    "end": "2143960"
  },
  {
    "text": "B2, A2 plus B4, et cetera. You can think of there's\nall these different products and sums. And you could do them\nin different orders.",
    "start": "2143960",
    "end": "2150170"
  },
  {
    "text": "And you can kind\nof refactor that. So when you think about all\nthe operations you could do,",
    "start": "2150170",
    "end": "2156440"
  },
  {
    "text": "you're going to have to do a\nseries of operations such that-- remember, what they're\ntrying to learn here is not how to multiply\ntwo particular ones.",
    "start": "2156440",
    "end": "2163410"
  },
  {
    "text": "But they're trying to\nlearn the algorithm that will always correctly solve in\nthe minimum number of steps.",
    "start": "2163410",
    "end": "2168960"
  },
  {
    "text": "So here, their reward\nfunction is the number of steps, the number\nof computations that you have to do.",
    "start": "2168960",
    "end": "2174510"
  },
  {
    "text": "And of course, it\nhas to be correct. And to me, one of the\nbrilliance of their ideas is, how do you make\nsure that you're only",
    "start": "2174510",
    "end": "2180240"
  },
  {
    "text": "searching within the space\nof correct algorithms? And so there's some\nreally nice properties for this particular problem\nthat allowed them to do that.",
    "start": "2180240",
    "end": "2187540"
  },
  {
    "text": "Some other people had also\nnoticed this in the past. And then what they\nsaid is, oh, given that we have that-- given\nthat we have a way to verify",
    "start": "2187540",
    "end": "2195810"
  },
  {
    "text": "and only search in the\nalgorithms that are correct, now what we can do is just\noptimize for length. And so the way that they\ndo that, in this case,",
    "start": "2195810",
    "end": "2202290"
  },
  {
    "text": "is they're going to-- very similar in certain\nways to AlphaZero-- be able to search through\nusing policy networks and value",
    "start": "2202290",
    "end": "2212120"
  },
  {
    "text": "networks. So you can see here, they\nhave a neural network with both a policy\nhead and a value head. Similar to what we\nsaw for AlphaZero.",
    "start": "2212120",
    "end": "2218670"
  },
  {
    "text": "But they are going to\ndo this forward search. Now, one of the interesting\nthings about this is that compared to what we\nsaw for AlphaGo, in AlphaGo--",
    "start": "2218670",
    "end": "2227430"
  },
  {
    "text": "because I saw some of-- we\ntalked about this with some of you and saw that\nin your notes--",
    "start": "2227430",
    "end": "2233190"
  },
  {
    "text": "at runtime, they're not\ngoing to do search anymore. What they're going\nto do at this point is they're just trying to find\nthe best possible algorithm.",
    "start": "2233190",
    "end": "2240312"
  },
  {
    "text": "And then in the\nfuture, they're not going to do any additional\nMonte Carlo tree search, unlike what we do with plain Go.",
    "start": "2240312",
    "end": "2246582"
  },
  {
    "text": "Because the assumption\nis, at that point, they have the algorithm. And they'll just apply\nit to multiplying. So they don't continue to\ndo Monte Carlo tree search",
    "start": "2246582",
    "end": "2254250"
  },
  {
    "text": "kind of at runtime. This is all something done just\nto find that best algorithm. So this is a case where we would\nhave Monte Carlo tree search,",
    "start": "2254250",
    "end": "2262890"
  },
  {
    "text": "and we would also\nhave policy networks. Policy and value network.",
    "start": "2262890",
    "end": "2269369"
  },
  {
    "text": "And where they're sharing-- again, this is a\nsingle neural network. So you can get shared\nrepresentations here,",
    "start": "2269370",
    "end": "2274840"
  },
  {
    "text": "very similar to AlphaZero. And then they can\nplay in this case.",
    "start": "2274840",
    "end": "2280830"
  },
  {
    "text": "Yeah? How do they overcome\ndistribution shifts? How do they overcome\ndistribution shifts?",
    "start": "2280830",
    "end": "2287180"
  },
  {
    "text": "[INAUDIBLE]  So they are trying to have--",
    "start": "2287180",
    "end": "2293030"
  },
  {
    "text": "so all of the algorithms they\nsearch through are correct. So there's no distribution\nshift in that.",
    "start": "2293030",
    "end": "2299549"
  },
  {
    "text": "They will always be correct\nfor a future problem. It's just that it may or\nmay not be that they found",
    "start": "2299550",
    "end": "2305360"
  },
  {
    "text": "the very most optimal one. So there's not the same\nproblem that you might write into different states.",
    "start": "2305360",
    "end": "2311900"
  },
  {
    "text": "So the nice thing here is it's\njust a series of operations. It may be that the\nsearch is stuff that they",
    "start": "2311900",
    "end": "2317210"
  },
  {
    "text": "didn't find the optimal one. So there might be still better\nalgorithms that are shorter. I don't think they prove this as\na lower bound, to my knowledge.",
    "start": "2317210",
    "end": "2323570"
  },
  {
    "text": "And so there's not--\nit's a great question. There's not going to\nbe a problem with like, when you deploy this on a\nnew matrix multiplication",
    "start": "2323570",
    "end": "2330380"
  },
  {
    "text": "that you might get\nsomething wrong. It's just that it may or may\nnot be the most optimal way to multiply that\nparticular two matrices.",
    "start": "2330380",
    "end": "2338569"
  },
  {
    "text": "So I think there, that's the\ncleverness of having the policy network be-- or having\nthe space they search over",
    "start": "2338570",
    "end": "2346069"
  },
  {
    "text": "always has fine correctness. Yeah? Were they able to\ninterpret the algorithm or learn something [INAUDIBLE]?",
    "start": "2346070",
    "end": "2354560"
  },
  {
    "text": "Yeah, it's a great question. So was there some kind\nof high-level insight-- and in particular,\nhigh-level insight you could translate\nto other problems?",
    "start": "2354560",
    "end": "2361000"
  },
  {
    "text": "Not that I remember. I think that they--",
    "start": "2361000",
    "end": "2366510"
  },
  {
    "text": "I don't remember there being\nany sort of particular like, aha moment, now, this means for all\nthese other types of problems,",
    "start": "2366510",
    "end": "2371643"
  },
  {
    "text": "we can do this. It'd be interesting to\ngo back to the paper and see if there was anything\nthat I missed in that case.",
    "start": "2371643",
    "end": "2377589"
  },
  {
    "text": "So I think what they found in\nthis case, to what I remember, is that they relearned a couple\ndifferent well-known algorithms",
    "start": "2377590",
    "end": "2383930"
  },
  {
    "text": "for trying to-- during the search process, they\nlearned a couple algorithms",
    "start": "2383930",
    "end": "2389000"
  },
  {
    "text": "that are known to be\ngood and more effective. And then found some others that\nhadn't been discovered before.",
    "start": "2389000",
    "end": "2394383"
  },
  {
    "text": "And so I think there's also\nan interesting question, because there may\nbe other utility functions for downstream\nuse of these algorithms.",
    "start": "2394383",
    "end": "2400260"
  },
  {
    "text": "And so in that case, you\nmight want these approaches to provide you a set of\nsolutions, a set of algorithms.",
    "start": "2400260",
    "end": "2406019"
  },
  {
    "text": "And then people could pick which\nones they thought were best. All right, so this is a\nmulti-step RL problem.",
    "start": "2406020",
    "end": "2415460"
  },
  {
    "text": "And here, the\nstate of the system would essentially be, what are\nthe operations you have so far?",
    "start": "2415460",
    "end": "2422480"
  },
  {
    "text": "So what are the operations that\nyou've done on the input two matrices specified as tensors?",
    "start": "2422480",
    "end": "2429917"
  },
  {
    "text": "And then how far do you\nneed to go until you can get the complete solution?",
    "start": "2429917",
    "end": "2435200"
  },
  {
    "text": "And the reward in\nthis case, assuming that you've conditioned\neverything on being correct, this is just length. ",
    "start": "2435200",
    "end": "2443610"
  },
  {
    "text": "So next, let's go to\nLearning Plasma Control for Fusion Science. And I think this is a\nreally interesting one.",
    "start": "2443610",
    "end": "2449620"
  },
  {
    "text": "I appreciated that I\nsaw for a lot of people saying like, we don't want to\ndo epsilon greedy on real RL",
    "start": "2449620",
    "end": "2456480"
  },
  {
    "text": "with plasma. That's probably a bad idea\nfor all of our health. So this need to be like\nsome form of offline phase.",
    "start": "2456480",
    "end": "2463510"
  },
  {
    "text": "And that's exactly right. That's certainly what\nthey did in this case. I think it's interesting to\nthink about how it's represented",
    "start": "2463510",
    "end": "2471240"
  },
  {
    "text": "and the different\ntypes of controls you'd be applying in this\ncase, which generally will be real valued.",
    "start": "2471240",
    "end": "2476577"
  },
  {
    "text": "So it's a very different\nproblem than AlphaTensor. Let's look at what\ntheir architecture was. So in this case, one thing\nthat they also really emphasize",
    "start": "2476577",
    "end": "2484950"
  },
  {
    "text": "in this is that they had to\nspend quite a long time-- they had to think\nreally carefully about what is the objective.",
    "start": "2484950",
    "end": "2490390"
  },
  {
    "text": "So this is an interesting one. It's not just minimize\nthe number of computations to solve two matrices.",
    "start": "2490390",
    "end": "2496450"
  },
  {
    "text": "It's saying, we want to be\nable to manipulate plasma into particular configurations.",
    "start": "2496450",
    "end": "2501690"
  },
  {
    "text": "And so you could\nimagine, in this case, you might have lots of\ndifferent reward functions. And you want to be\nable to quickly learn",
    "start": "2501690",
    "end": "2506937"
  },
  {
    "text": "policies for those. So what they do to ameliorate\nthe offline safety issue",
    "start": "2506937",
    "end": "2513099"
  },
  {
    "text": "is they build a simulator. And I was just coming\nto someone that-- on a recent panel I\nwas on, I was talking",
    "start": "2513100",
    "end": "2519520"
  },
  {
    "text": "to a mechanical\nengineer that said that's one of the reasons they\nwere really interested in AI and machine learning, is\nthey like to make simulators",
    "start": "2519520",
    "end": "2525040"
  },
  {
    "text": "a really computationally\nexpensive physical processes. And so they here\nhave a simulator",
    "start": "2525040",
    "end": "2530500"
  },
  {
    "text": "that is fairly high\nfidelity, but not perfect. And is high fidelity\nenough that they think it'll be useful,\nbut low fidelity enough",
    "start": "2530500",
    "end": "2536470"
  },
  {
    "text": "that you could do\noptimization over it. So what they're going to do in\nthis case is they are solving the offline case by\nconstructing here--",
    "start": "2536470",
    "end": "2544340"
  },
  {
    "text": "not necessarily from data or\nmaybe from a physics model-- a simulator. So we're going to\ndo model-based RL.",
    "start": "2544340",
    "end": "2549928"
  },
  {
    "text": "Model-based in the\nsense that we have to have a model or a simulator. But then what they're going to\ndo is an actor critic method.",
    "start": "2549928",
    "end": "2556340"
  },
  {
    "text": "So they are going\nto do actor critic, in this case, where they\nhave a control policy.",
    "start": "2556340",
    "end": "2562453"
  },
  {
    "text": "And they also are\ngoing to learn-- so we have the actor here. And they're also going\nto be learning a critic.",
    "start": "2562453",
    "end": "2567940"
  },
  {
    "text": "So I thought this was pretty\ninteresting for why they took this particular architecture.",
    "start": "2567940",
    "end": "2572990"
  },
  {
    "text": "I'm just going to read you a\nlittle bit about that part. Let me go down there. So a couple of things.",
    "start": "2572990",
    "end": "2578420"
  },
  {
    "text": "So I thought one of the things\nis they use an actor critic method that is related\nto something else we saw, but not exactly the same.",
    "start": "2578420",
    "end": "2584240"
  },
  {
    "text": "It's called MPO. And I'll write that\nout in a second. But one of the things that\nI thought was interesting",
    "start": "2584240",
    "end": "2589780"
  },
  {
    "text": "is they said, in our\nsimulating period, we can do sort of\nwhatever we want,",
    "start": "2589780",
    "end": "2595110"
  },
  {
    "text": "and we can have a really\ncomplicated critic when. We are deploying this,\nit has to be real-time.",
    "start": "2595110",
    "end": "2600660"
  },
  {
    "text": "So some other people, I think\nit was-- brought this up when we were chatting about it. This is like self-driving cars.",
    "start": "2600660",
    "end": "2606445"
  },
  {
    "text": "And you have to have\nreally fast controllers. You can't do Monte\nCarlo tree search and wait for us to decide--",
    "start": "2606445",
    "end": "2611472"
  },
  {
    "text": "like, the plasma's\ngoing to do something. And so either you're\ncontrolling it, or it is doing something\nelse if you're not",
    "start": "2611472",
    "end": "2616829"
  },
  {
    "text": "making an active control. And so they needed an\nactor, a.k.a. a policy, that is really\ncomputationally fast.",
    "start": "2616830",
    "end": "2622779"
  },
  {
    "text": "And so what they said is that,\ninside of their actor critic architecture, one\nof the reasons they wanted to do that\nduring the training",
    "start": "2622780",
    "end": "2628800"
  },
  {
    "text": "is they could\nrequire their actor to be pretty low-dimensional. And so have a\npretty small network",
    "start": "2628800",
    "end": "2634109"
  },
  {
    "text": "to specify the actor\nor the control policy, which is what they're\ngoing to eventually deploy.",
    "start": "2634110",
    "end": "2639360"
  },
  {
    "text": "But they could have a\nreally complicated critic. And so they can\nleverage the fact that in the offline\nsetting, they can really,",
    "start": "2639360",
    "end": "2646000"
  },
  {
    "text": "in a complicated\nway, many parameters, specify their value function. Because this is all offline. And so this is of a nice,\ninteresting asymmetry",
    "start": "2646000",
    "end": "2653190"
  },
  {
    "text": "between computational\nefficiency, and what are the affordances you\nhave offline compared to online.",
    "start": "2653190",
    "end": "2658970"
  },
  {
    "text": "So they have a very\ncomplicated critic. And they have a\nvery simple actor. And so then they\ntrain the actor to try",
    "start": "2658970",
    "end": "2665140"
  },
  {
    "text": "to find a good point in that\npolicy space using their really complicated critic.",
    "start": "2665140",
    "end": "2670150"
  },
  {
    "text": "And so they said,\nthe representation of the control policy\nvector is restricted, as it must run on TBC\nwith real-time guarantees.",
    "start": "2670150",
    "end": "2676670"
  },
  {
    "text": "But the critic is unrestricted. So I thought that was pretty\ninteresting that they had this. Now, another thing--\nand this came up",
    "start": "2676670",
    "end": "2682810"
  },
  {
    "text": "in some conversations--\nis, as you might imagine, if we go from offline to online,\nthere is always the problem",
    "start": "2682810",
    "end": "2689290"
  },
  {
    "text": "that it might not translate. And again, we're\ndealing with plasma. So we want to have some\nsort of safety guarantees.",
    "start": "2689290",
    "end": "2695180"
  },
  {
    "text": "So here, the ideas\nwe've talked about before about having more trusted\nregions or having pessimism",
    "start": "2695180",
    "end": "2702070"
  },
  {
    "text": "come up. And the way that\nthey handle this is by putting it inside\nof the reward function. So they essentially\ndefine areas which they",
    "start": "2702070",
    "end": "2708820"
  },
  {
    "text": "think could cause bad outcomes. And then they put that\ninside their reward function to lead to a policy\nthat veers away from that area.",
    "start": "2708820",
    "end": "2717640"
  },
  {
    "text": "And I think, again, that's\na pretty common idea that if you have safety-- this comes up in robotics\nand other ones, too.",
    "start": "2717640",
    "end": "2723770"
  },
  {
    "text": "Claire Tomlin up at\nBerkeley does this, too. A number of others.",
    "start": "2723770",
    "end": "2729050"
  },
  {
    "text": "You put that inside of\nthe reward function, so the resulting\npolicy avoid those. And so here, they're doing that\nnot necessarily because reaching",
    "start": "2729050",
    "end": "2735850"
  },
  {
    "text": "that particular\npart would be bad, but because you're\ngetting close to a part where it might be\nunsafe, or where you don't trust your simulator.",
    "start": "2735850",
    "end": "2744069"
  },
  {
    "text": "So let's go back to here. So in this case,\nit's an actor critic.",
    "start": "2744070",
    "end": "2749360"
  },
  {
    "text": "Actor critic. This is complicated. ",
    "start": "2749360",
    "end": "2756119"
  },
  {
    "text": "This is simple.  It has to be simple for speed.",
    "start": "2756120",
    "end": "2762580"
  },
  {
    "text": "And we all do this\nwith a simulator. We put penalties and the\nreward to avoid inaccuracies",
    "start": "2762580",
    "end": "2782510"
  },
  {
    "text": "in simulator, or\nunsafe outcomes.",
    "start": "2782510",
    "end": "2788530"
  },
  {
    "text": " And so this is very similar to\nthis pessimism over the places",
    "start": "2788530",
    "end": "2794040"
  },
  {
    "text": "where we're uncertain, whether\nbecause of data sparsity, or because of known\nproblems in our simulator.",
    "start": "2794040",
    "end": "2800285"
  },
  {
    "text": "[INTERPOSING VOICES] ",
    "start": "2800285",
    "end": "2809997"
  },
  {
    "text": "Or how do you double check that? I assume they really don't\nwant to be making [INAUDIBLE]. So a great question.",
    "start": "2809997",
    "end": "2815260"
  },
  {
    "text": "So my guess, in this\ncase, is that it ends up making you just\npretty conservative. I think of just how far away--\nno, I assume in this case,",
    "start": "2815260",
    "end": "2822540"
  },
  {
    "text": "maybe because some of the\nphysics simulators that they have access to, that they could\nplay with some of saying like,",
    "start": "2822540",
    "end": "2828520"
  },
  {
    "text": "if you-- how negative do you need\nto make some of these? Or how out of bounds? Or how hard of a\nconstraint is that?",
    "start": "2828520",
    "end": "2834210"
  },
  {
    "text": "So that you could be very\nconfident that before you deploy this, you make sure\nthat this doesn't reach there. At least in the simulator,\nyou could see whether or not",
    "start": "2834210",
    "end": "2841320"
  },
  {
    "text": "you're violating\nthose constraints. Or if you have these\npenalties, if you're sufficient not to\nreach parts of the area",
    "start": "2841320",
    "end": "2846750"
  },
  {
    "text": "that you think you\nmight want to avoid. Whether that will translate\nto your real system is an important question.",
    "start": "2846750",
    "end": "2853450"
  },
  {
    "text": "So, yeah, it's a great\nissue of how you-- no, I think it also introduces\nthe really interesting question of whether you can verify.",
    "start": "2853450",
    "end": "2859535"
  },
  {
    "text": "So there are other methods. This is not some-- most of what\nwe've talked about is not those, but where you could\nverify that you're not going to reach unsafe regions.",
    "start": "2859535",
    "end": "2865820"
  },
  {
    "text": "And this would certainly be an\narea you might want to do that. The third one was efficient and\ntargeted COVID-19 border testing",
    "start": "2865820",
    "end": "2873490"
  },
  {
    "text": "for-- I should have also mentioned--\nso this is also a multi-step RL",
    "start": "2873490",
    "end": "2878700"
  },
  {
    "text": "problem. So absolutely, the\ncontrols you're doing affect the next state.",
    "start": "2878700",
    "end": "2883810"
  },
  {
    "text": "And that's the whole point. And then you want to\nmanipulate the plasma into a particular occasion. So it's definitely\na multi-step system.",
    "start": "2883810",
    "end": "2889069"
  },
  {
    "text": " This one is thinking\nabout how do you",
    "start": "2889070",
    "end": "2894610"
  },
  {
    "text": "do efficient and targeted\nCOVID-19 border testing? And even though it's\nvia RL, it really is a bandit problem\nin this case.",
    "start": "2894610",
    "end": "2901430"
  },
  {
    "text": "So it's a repeated\nbandit problem. It's a batch bandit\nwith delayed outcomes.",
    "start": "2901430",
    "end": "2910890"
  },
  {
    "text": " So let's make this\na little bit bigger. So again, remember to think\nback what happens in this case.",
    "start": "2910890",
    "end": "2918910"
  },
  {
    "text": "People come in. Greece has some information\nabout those individuals before they show up.",
    "start": "2918910",
    "end": "2923920"
  },
  {
    "text": "We have finite numbers of\ntests we can run and process. We have a policy for each\nindividual coming off",
    "start": "2923920",
    "end": "2930682"
  },
  {
    "text": "that plane, whether\nor not they're going to be given no\ntest or they're tested.",
    "start": "2930683",
    "end": "2935700"
  },
  {
    "text": "You get the results\n24 hours later. And you use that to\nupdate your policy. So I think this is a really nice\nexample of this batch bandit",
    "start": "2935700",
    "end": "2942540"
  },
  {
    "text": "process. Who you test today\ndoes not affect who arrives tomorrow on a plane. So it's a bandit problem.",
    "start": "2942540",
    "end": "2948810"
  },
  {
    "text": "But we have this\ndelayed outcome problem that you don't observe the\noutcomes of who you just tested for a while.",
    "start": "2948810",
    "end": "2954910"
  },
  {
    "text": "Which means that algorithms\nlike Thompson sampling may be helpful.",
    "start": "2954910",
    "end": "2960128"
  },
  {
    "text": "And then in addition, some of\nthe other really big challenges in this case is that you\nhave a lot of constraints. ",
    "start": "2960128",
    "end": "2968700"
  },
  {
    "text": "You have constraints\nfor multiple reasons. So we have constraints over\nthe number of tests we can run. You also can have\ndifferent constraints",
    "start": "2968700",
    "end": "2975030"
  },
  {
    "text": "depending on where you're\narriving in Greece, and where you can send things. So there are different\ntesting sites which might have\ndifferent capacities.",
    "start": "2975030",
    "end": "2981550"
  },
  {
    "text": "And in some cases,\nalso, you might have-- I don't think they dealt\nwith in this paper, but sometimes you might have\nfairness constraints, too.",
    "start": "2981550",
    "end": "2987339"
  },
  {
    "text": "Like, maybe it's best\nto test all the women, but maybe that's\nconsidered unfair. And so you may have a number\nof different constraints",
    "start": "2987340",
    "end": "2994200"
  },
  {
    "text": "that you can think of as\nrestricting your policy class. So it's a pretty interesting\ninteraction problem here.",
    "start": "2994200",
    "end": "3001230"
  },
  {
    "text": "And also because of the\nfact that it's budgeted, it means that a lot of your\noutcomes are coupled in a way that they might not be.",
    "start": "3001230",
    "end": "3007670"
  },
  {
    "text": "So, for example, if\nyou give me a test-- if we only have one test\nthat we can do in this room and you give me the test, then\nyou can't give it to any of you.",
    "start": "3007670",
    "end": "3014960"
  },
  {
    "text": "And so there's this interaction,\ntoo, in terms of the data that we get to\nobserve for the right.",
    "start": "3014960",
    "end": "3022535"
  },
  {
    "text": "So I think this is a\nreally interesting case. And it is really\ninteresting that it ended up having a significant benefit.",
    "start": "3022535",
    "end": "3028339"
  },
  {
    "text": "One of the things,\ntoo, that's interesting about this is how we\ndefine the reward. One thing that we were talking\nabout in our smaller groups",
    "start": "3028340",
    "end": "3034740"
  },
  {
    "text": "is that, really, would like to\nunderstand how this is impacting downstream COVID outcomes. And you can measure\nthose, but you",
    "start": "3034740",
    "end": "3040980"
  },
  {
    "text": "can measure those really late. You can use those\nas a way to evaluate how effective the\noverall program was, but not necessarily a reward\nyou can use to optimize.",
    "start": "3040980",
    "end": "3048640"
  },
  {
    "text": "And that's often a\nreally common challenge. The rewards you get\nimmediately that you could use to change\nyour policy may",
    "start": "3048640",
    "end": "3054089"
  },
  {
    "text": "be different than the downstream\noutcome you care about. And on Friday, I was at an\nexperimentation workshop",
    "start": "3054090",
    "end": "3059910"
  },
  {
    "text": "at the business school here,\nand I was giving a talk. And I was really excited\nand interested to see how many other people\nwere also thinking",
    "start": "3059910",
    "end": "3066119"
  },
  {
    "text": "of this challenge of\nshort-term outcomes versus long-term rewards\nthat you really care about.",
    "start": "3066120",
    "end": "3071580"
  },
  {
    "text": "And I think this comes\nup a lot in advertising, and other areas, too. Companies like\nNetflix, and Spotify,",
    "start": "3071580",
    "end": "3076853"
  },
  {
    "text": "and others we're talking\nabout this common challenge where you have to make\npolicy decisions--",
    "start": "3076853",
    "end": "3081900"
  },
  {
    "text": "or update your policy\nway before you can maybe observe those outcomes. And so if you have to\nwait a really long time,",
    "start": "3081900",
    "end": "3088119"
  },
  {
    "text": "it limits how quickly\nyou can experiment. And so in this case,\ntoo, you might really care about these\ndownstream ones.",
    "start": "3088120",
    "end": "3093910"
  },
  {
    "text": "But one of the\npoints of this paper was to argue, looking\nat that lagged information was allowing people\nto make not as good decisions.",
    "start": "3093910",
    "end": "3100990"
  },
  {
    "text": "And so you need these sort\nof shorter term outcomes. So do we have any\nquestions about this one? ",
    "start": "3100990",
    "end": "3109087"
  },
  {
    "text": "So I encourage you\nto-- if you haven't read any of those\npapers, they're really beautiful papers, if you\nwant to read any of them or all.",
    "start": "3109087",
    "end": "3115020"
  },
  {
    "text": "And then just finally, if you\nremember all the way back, we talked about ChatGPT at the\nvery beginning of the class. And I think you should\nfeel excited now",
    "start": "3115020",
    "end": "3121319"
  },
  {
    "text": "that you really understand\nthis whole pipeline of what's possible. The first is sort of training\na supervised policy, which",
    "start": "3121320",
    "end": "3127170"
  },
  {
    "text": "we could think of\nas behavior cloning. The second is doing direct\npreference elicitation.",
    "start": "3127170",
    "end": "3132940"
  },
  {
    "text": "We did it with two pairs,\nand then doing PPO. And we also, of course,\ndid DPO as well. So I think now, even though we\ndidn't do with large language",
    "start": "3132940",
    "end": "3140220"
  },
  {
    "text": "models, you really have a\nsense of the whole process you could use if you were to\ntrain large language models",
    "start": "3140220",
    "end": "3145320"
  },
  {
    "text": "and do the fine tuning.  So now we're just going to wrap\nup with some of the main ideas,",
    "start": "3145320",
    "end": "3153260"
  },
  {
    "text": "and then looking forward. So if we think about the main\ncharacteristics of reinforcement learning, this idea of learning\ndirectly from data to make",
    "start": "3153260",
    "end": "3159910"
  },
  {
    "text": "good decisions. We've been thinking a lot\nabout optimization, delayed consequences, exploration\nand generalization.",
    "start": "3159910",
    "end": "3166460"
  },
  {
    "text": "And I think a key\nthing just to remember, if you didn't remember\nanything else from this class, is that one of the big\ndifferences of reinforcement",
    "start": "3166460",
    "end": "3172630"
  },
  {
    "text": "learning is that, in general,\nthe actions impact the data distribution. Certainly, of the\nrewards we observe.",
    "start": "3172630",
    "end": "3179150"
  },
  {
    "text": "But often, also of the\nstates we get to reach. And that's just very, very\ndifferent than supervised learning or\nunsupervised learning,",
    "start": "3179150",
    "end": "3185770"
  },
  {
    "text": "where the data we get doesn't-- you always see the\nlabel, or you just",
    "start": "3185770",
    "end": "3191170"
  },
  {
    "text": "have a static-generated\ndistribution of data. So this is both a huge\nopportunity and a huge challenge",
    "start": "3191170",
    "end": "3198430"
  },
  {
    "text": "because we have to think a lot\nmore about distribution shift. ",
    "start": "3198430",
    "end": "3203920"
  },
  {
    "text": "So in terms of the\nstandard settings we've seen, we've\ntalked about bandits, where the next state is\nindependent of the prior state",
    "start": "3203920",
    "end": "3210820"
  },
  {
    "text": "and action. As well as general\ndecision processes, where the next\nstate might depend",
    "start": "3210820",
    "end": "3216130"
  },
  {
    "text": "on all the previous\nactions and states. Or it might be\nMarkov, and it only depends on the\nimmediate state and",
    "start": "3216130",
    "end": "3221980"
  },
  {
    "text": "the immediate previous action. We've also talked a lot about\nthe online/offline settings,",
    "start": "3221980",
    "end": "3227319"
  },
  {
    "text": "where either you have\nhistorical data only and you're trying to learn\nbetter policies from that. Or where you can actually\nactively gather your own data.",
    "start": "3227320",
    "end": "3235244"
  },
  {
    "text": "And I will highlight\nthere that I think many real-world settings\nare often between these two.",
    "start": "3235245",
    "end": "3241840"
  },
  {
    "text": "Many. ",
    "start": "3241840",
    "end": "3247790"
  },
  {
    "text": "So in many cases, you might have\na large pool of offline data, and then you might be\nable to get a small amount of new online data.",
    "start": "3247790",
    "end": "3253730"
  },
  {
    "text": "This comes up in robotics. It comes up in some of our work. We often call this sort\nof experimental design. So that you might\nhave offline data,",
    "start": "3253730",
    "end": "3260330"
  },
  {
    "text": "and then you can\ndesign an experiment to gather a small amount of data\nto try to learn a good decision policy.",
    "start": "3260330",
    "end": "3266000"
  },
  {
    "text": "So I think, in\ngeneral, we can think of this as an entire spectrum\nbetween these two extremes. ",
    "start": "3266000",
    "end": "3272870"
  },
  {
    "text": "Now, what are some of the\ncore ideas we've seen? Well, of course, we've seen\na lot of different ideas. But I think it's nice\nto pop up a level",
    "start": "3272870",
    "end": "3278960"
  },
  {
    "text": "and think about\nthe common themes. And Chelsea Finn,\nwho teaches Deep RL, also had a really\nnice slide on this.",
    "start": "3278960",
    "end": "3285180"
  },
  {
    "text": "So I found that my thoughts were\naligning with a number of hers as well. So one thing is just to be\nreally familiar with the fact",
    "start": "3285180",
    "end": "3293119"
  },
  {
    "text": "that when we have\nfunction approximation-- which we're almost always\ngoing to need because we want to handle hard problems--",
    "start": "3293120",
    "end": "3298980"
  },
  {
    "text": "hard, complex problems. And we want to do\noff-policy learning that, honestly, we\noften want to do,",
    "start": "3298980",
    "end": "3304320"
  },
  {
    "text": "whether we're online or offline. And just remember,\noff-policy learning just means that we want to take\nsome data that was generated",
    "start": "3304320",
    "end": "3311400"
  },
  {
    "text": "from one decision policy and\nuse it to think about how another one might work. Whether in terms\nof gradient steps,",
    "start": "3311400",
    "end": "3317660"
  },
  {
    "text": "or in terms of fully\noffline learning. And this is generally\njust really hard.",
    "start": "3317660",
    "end": "3322740"
  },
  {
    "text": "So you could argue\nthat a huge number of papers in reinforcement\nlearning just think about this problem.",
    "start": "3322740",
    "end": "3328470"
  },
  {
    "text": "It's just incredibly hard. And the reason is that\nwhenever we have a new policy, we're going to get a new\ndistribution over state action",
    "start": "3328470",
    "end": "3335160"
  },
  {
    "text": "rewards. And that means that it may\nnot match our current data. We have a data\ndistribution shift.",
    "start": "3335160",
    "end": "3341730"
  },
  {
    "text": "And the reason we\nwant to do this-- the reason we want to\nhave use the offline data is because we want\nto be data efficient.",
    "start": "3341730",
    "end": "3349100"
  },
  {
    "text": "And this is true even\nif you can be online. Because as we saw\nfor things like PPO,",
    "start": "3349100",
    "end": "3354930"
  },
  {
    "text": "if you follow the theory,\nor if you follow this, you often have to really\nbe incredibly conservative,",
    "start": "3354930",
    "end": "3360860"
  },
  {
    "text": "or just have bad performance\nfor a very long time. But the problem is that when we\ncombine these two, in general,",
    "start": "3360860",
    "end": "3367830"
  },
  {
    "text": "we're going to be doing\ngeneralization or extrapolation. And whenever we do that,\nwe need to be worried that,",
    "start": "3367830",
    "end": "3373920"
  },
  {
    "text": "like the values that\nour predictions of how good a policy will be, will not\nmatch its actual performance.",
    "start": "3373920",
    "end": "3379760"
  },
  {
    "text": "And so over and\nover and over again, we've seen, how do we\ntry to mitigate this",
    "start": "3379760",
    "end": "3384770"
  },
  {
    "text": "in different types of methods? So in PPO, the way we\ncontrol this-- and this is",
    "start": "3384770",
    "end": "3389900"
  },
  {
    "text": "an online method-- is we\ncontrol it with clipping. We just can't take too big of\na step inside of our gradients.",
    "start": "3389900",
    "end": "3396110"
  },
  {
    "text": "And that allows us to make\nsure that we are limiting this extrapolation problem. In the DAGGER case,\nwe mitigated this",
    "start": "3396110",
    "end": "3403160"
  },
  {
    "text": "by getting more expert labels. We knew that there could\nbe a data distribution shift when we started to follow\nour behavior clone policy.",
    "start": "3403160",
    "end": "3411099"
  },
  {
    "text": "And so we just try\nto get more labels when we get into states\nwhere we make decisions different than the expert.",
    "start": "3411100",
    "end": "3416470"
  },
  {
    "text": "So we can cover the\ndistribution of states we reach under the learned policy.",
    "start": "3416470",
    "end": "3421950"
  },
  {
    "text": "And things like\npessimistic Q Learning, which came from my lab. CQL, which came from Berkeley.",
    "start": "3421950",
    "end": "3427037"
  },
  {
    "text": "And MOPO, which came from\nother colleagues of mine here at Stanford, all introduce\npessimism into offline RL.",
    "start": "3427037",
    "end": "3432750"
  },
  {
    "text": "Again, exactly to limit\nthis extrapolation problem where you're overly optimistic\nabout what will happen.",
    "start": "3432750",
    "end": "3440105"
  },
  {
    "text": "So I don't think\nyou should think of these as being the only\nways to solve this problem. I think what they should\ninspire you to is to think, wow,",
    "start": "3440105",
    "end": "3447247"
  },
  {
    "text": "this is a problem\nthat comes up really throughout all of\nreinforcement learning. And we have some methods\nfor trying to handle this.",
    "start": "3447247",
    "end": "3453118"
  },
  {
    "text": "But this is certainly\nnot a solved problem.  Some of the other\ncore ideas that we",
    "start": "3453118",
    "end": "3459610"
  },
  {
    "text": "saw a lot was this\nidea of there's different ways we could\nthink about the main objects in reinforcement learning.",
    "start": "3459610",
    "end": "3465360"
  },
  {
    "text": "So we had this sort of\nmodels, values, and policies. Sometimes people ask me like,\ndo we really need all of these?",
    "start": "3465360",
    "end": "3470680"
  },
  {
    "text": "Or are these all useful ideas? I think some of the application\nareas we were just going through illustrate why these\nmight all be useful ideas.",
    "start": "3470680",
    "end": "3478130"
  },
  {
    "text": "So models are often easier\nways to represent uncertainty.",
    "start": "3478130",
    "end": "3483339"
  },
  {
    "text": "So if we only have\nfinite data and we're training something about a\nvalue, or a model, or a policy,",
    "start": "3483340",
    "end": "3488750"
  },
  {
    "text": "often, it might\nbe easiest for us to represent that\nuncertainty with a model. So we have an idea\nof why that might be.",
    "start": "3488750",
    "end": "3495290"
  },
  {
    "text": "Why might it be\neasier to represent uncertainty for a model rather\nthan a Q function or a policy? ",
    "start": "3495290",
    "end": "3503480"
  },
  {
    "text": "You could disagree with me, too. But I can give you why I think\nthis might be the easiest. ",
    "start": "3503480",
    "end": "3512502"
  },
  {
    "text": "We're building just like a\ndynamics model or a reward model. Why wouldn't that\nbe an easier place for us to represent\nour uncertainty about?",
    "start": "3512502",
    "end": "3521240"
  },
  {
    "text": "How the world works\ncompared to trying to represent our\nuncertainty over the Q function or the policy. ",
    "start": "3521240",
    "end": "3530890"
  },
  {
    "text": "Isn't it just because when\nyou're making-- when you're just like [INAUDIBLE] uncertainty in\nyour policy, there's uncertainty",
    "start": "3530890",
    "end": "3538520"
  },
  {
    "text": "both [INAUDIBLE], but\nalso uncertainty about given your assumption\nof the world of what you think the best action is.",
    "start": "3538520",
    "end": "3544010"
  },
  {
    "text": "So you're just dealing\nwith a joint uncertainty, whereas the model of the\nworld is kind of like-- it's a more specified problem,\nlike one source of uncertainty.",
    "start": "3544010",
    "end": "3551955"
  },
  {
    "text": "Yeah, I think that's great. That's a great intuition. That's what I was\ngoing for here. So to repeat what I said here,\nwhen you think about policy",
    "start": "3551955",
    "end": "3559250"
  },
  {
    "text": "uncertainty, there's-- that kind of\ncombines and wraps up this idea of uncertainty\nover how the world works,",
    "start": "3559250",
    "end": "3564620"
  },
  {
    "text": "and uncertainty\nover what you should do to make good decisions\ngiven that world. And same for the Q function.",
    "start": "3564620",
    "end": "3571035"
  },
  {
    "text": "And there are ways to directly\nrepresent your uncertainty over the policies\nand the Q functions. But models, it's a\nprediction problem.",
    "start": "3571035",
    "end": "3578520"
  },
  {
    "text": "And so we have lots of tools\nfrom supervised learning, and from statistics\nand data science to think about modeling\nour uncertainty",
    "start": "3578520",
    "end": "3585110"
  },
  {
    "text": "when it's just a\nprediction problem. Like, what state\nwill happen next? Or what reward will\nI get in this state?",
    "start": "3585110",
    "end": "3591420"
  },
  {
    "text": "There's no planning or\ndecision making yet. It's just prediction. And so it's sort of\na nice place for us",
    "start": "3591420",
    "end": "3596820"
  },
  {
    "text": "to reduce or leverage the\nbeautiful history of work in all the other fields of\nhow we can do this easily,",
    "start": "3596820",
    "end": "3602730"
  },
  {
    "text": "instead of then having to\npropagate that through. So I think, often,\nthis is an easier place",
    "start": "3602730",
    "end": "3608580"
  },
  {
    "text": "to represent our uncertainty. Of course, there's\nno free lunch. If we have it there,\nwe want to think about our uncertainty over\npolicies and value functions,",
    "start": "3608580",
    "end": "3616630"
  },
  {
    "text": "we still have to propagate it. But it may be easier for us\nto represent that and drive ourselves towards it. They're also really useful for\nthings like Monte Carlo tree",
    "start": "3616630",
    "end": "3623760"
  },
  {
    "text": "search. You can use models for\nsimulators, or for plasma. You may be able to use\nthese ones as a place",
    "start": "3623760",
    "end": "3630510"
  },
  {
    "text": "to think about risky domains,\nor to be very data efficient.",
    "start": "3630510",
    "end": "3635760"
  },
  {
    "text": "The Q function, in some ways, is\nkind of the central part of RL. In the sense that\nit just summarizes",
    "start": "3635760",
    "end": "3641310"
  },
  {
    "text": "the performance of your policy. And you can use it\noften to directly act, because you just take an argmax\nwith respect to the Q function.",
    "start": "3641310",
    "end": "3648430"
  },
  {
    "text": "So it's a good way to\nsummarize how good things are. And policies are just\nultimately what we want to have.",
    "start": "3648430",
    "end": "3656520"
  },
  {
    "text": "We want to have good\ndecision making. We often want to know\nexactly how good that is. And that's maybe\nwhere the Q function",
    "start": "3656520",
    "end": "3662700"
  },
  {
    "text": "is one particular nice thing. But ultimately, we\nwant to try to make good decisions in the world.",
    "start": "3662700",
    "end": "3667762"
  },
  {
    "text": "I think another thing\nthat's come up repeatedly is this question of computation\nversus data efficiency.",
    "start": "3667762",
    "end": "3674155"
  },
  {
    "text": "And I think one thing that\nit's really useful to remember is that in some cases,\nthey are the same. So in this class, I've\noften talked as if they're",
    "start": "3674155",
    "end": "3682110"
  },
  {
    "text": "sort of totally different. But in many situations,\nif you have a simulator,",
    "start": "3682110",
    "end": "3687730"
  },
  {
    "text": "data is the same as computation. You're either using\nyour computation to maybe do more planning, or\ntry to get to a better policy",
    "start": "3687730",
    "end": "3694980"
  },
  {
    "text": "before you simulate\nthe next step. Or you're just\nsimulating more steps. And so I think when\nyou look at papers,",
    "start": "3694980",
    "end": "3701950"
  },
  {
    "text": "if they have a simulated\ndomain and they're trying to do something really\nfancy in the back, it's useful to remind yourself\nthat if it was a real problem",
    "start": "3701950",
    "end": "3709200"
  },
  {
    "text": "you want to solve, you\ncould either take that same computation and just have\nmaybe 10x more samples.",
    "start": "3709200",
    "end": "3714450"
  },
  {
    "text": "Or you can do 10x more\ncomputation between each sample. Now, in some other cases, we\nreally do have limited data.",
    "start": "3714450",
    "end": "3723960"
  },
  {
    "text": "We just fortunately do not have\n7 billion people with COVID.",
    "start": "3723960",
    "end": "3729190"
  },
  {
    "text": "There's just a finite\nnumber of people. And there's a finite\nnumber of students. And so sometimes you really\nwant to be data efficient.",
    "start": "3729190",
    "end": "3735690"
  },
  {
    "text": "When you do that,\nit's often trading off for computational cost. So we're going to try\nto squeeze everything",
    "start": "3735690",
    "end": "3741600"
  },
  {
    "text": "we can out of the data. And when we do\nthat, we often are going to rely on methods that\nare much more computationally intensive.",
    "start": "3741600",
    "end": "3747450"
  },
  {
    "text": "And also, as you've\nseen in some cases, you have real\nconstraints on this. Like in plasma.",
    "start": "3747450",
    "end": "3752560"
  },
  {
    "text": "Like in self-driving cars. Like in robotics. There are sometimes\ncases where you have to have fast computation,\nbecause otherwise, there is",
    "start": "3752560",
    "end": "3761370"
  },
  {
    "text": "a default. There's kind\nof a hidden action, which is, you have to make a\ndecision at every time point. If you're not doing\nsomething optimal,",
    "start": "3761370",
    "end": "3767200"
  },
  {
    "text": "something else is happening. There's some default action\nthat's always occurring. ",
    "start": "3767200",
    "end": "3773490"
  },
  {
    "text": "Now, what are some of\nthe open challenges? I think there's a lot\nof open challenges. I think RL is a\nfascinating area.",
    "start": "3773490",
    "end": "3780950"
  },
  {
    "text": "But RL has not yet had\nthe applicational impact that we've seen in some other\nareas of AI and engineering.",
    "start": "3780950",
    "end": "3789430"
  },
  {
    "text": "And I think this is for\na number of reasons. But one of them\nis that you really want methods that are\noff-the-shelf and robust and",
    "start": "3789430",
    "end": "3797200"
  },
  {
    "text": "reliable. And many RL algorithms\nhave hyperparameters. You have to pick\nthe learning rate.",
    "start": "3797200",
    "end": "3803770"
  },
  {
    "text": "You have to pick--\nsome of these are the same as normal machine\nlearning, and others of them are different. And one of the\nchallenges here is",
    "start": "3803770",
    "end": "3809890"
  },
  {
    "text": "if you're online, even though\nin our world, like when we're doing a homework, you\nmight be able to try it",
    "start": "3809890",
    "end": "3815110"
  },
  {
    "text": "with different hyperparameters. In a real-world setting,\nlike for health care or for customers, you would\njust have that one trajectory.",
    "start": "3815110",
    "end": "3822320"
  },
  {
    "text": "And so in that case,\nor one deployment, you can't optimize\nthose parameters. And so I think that\nthere's this real need",
    "start": "3822320",
    "end": "3828940"
  },
  {
    "text": "for automatic hyperparameter\ntuning, model selection. By that I mean, how do you\nfigure out what architecture",
    "start": "3828940",
    "end": "3834700"
  },
  {
    "text": "you use. How do you even write\ndown the problem? And generally robust methods,\nmodel selection terms",
    "start": "3834700",
    "end": "3841310"
  },
  {
    "text": "like the size of your\nneural network, et cetera. And just general sort\nof robust guarantees that we're not going\nto suddenly have one",
    "start": "3841310",
    "end": "3847700"
  },
  {
    "text": "run where your\nperformance is really bad. The other is that\nwe often need things",
    "start": "3847700",
    "end": "3853760"
  },
  {
    "text": "that are going to\nbe able to span this data versus\ncomputation efficiency. And we don't normally\nhave very good ways",
    "start": "3853760",
    "end": "3860750"
  },
  {
    "text": "to allow a practitioner to say\nlike, OK, well, this is how much I care about this or that. It'd be really nice\nif we could have",
    "start": "3860750",
    "end": "3866300"
  },
  {
    "text": "sort of like Pareto frontiers. And you could say, well, if this\nis computation and this is data,",
    "start": "3866300",
    "end": "3873060"
  },
  {
    "text": "you might say, OK, I\nwant to have things that are always somehow\noptimally trading off between those two.",
    "start": "3873060",
    "end": "3878628"
  },
  {
    "text": "And depending on my\napplication area, I can pick where I want\nto be on this curve. And I also think this\nhybrid offline-online case",
    "start": "3878628",
    "end": "3885140"
  },
  {
    "text": "is a really important one. Where many\norganizations might be willing to do a little bit of\nadditional data collection,",
    "start": "3885140",
    "end": "3890940"
  },
  {
    "text": "but not fully online learning. I think there's also some\njust really big questions for reinforcement learning.",
    "start": "3890940",
    "end": "3897319"
  },
  {
    "text": "We focused a lot on the Markov\ndecision process formulation. That's where it comes\nout of the 1950s.",
    "start": "3897320",
    "end": "3902840"
  },
  {
    "text": "And Bellman, that's\nhow I learned about it, many people learned it. And it has some really nice\nintellectual properties.",
    "start": "3902840",
    "end": "3908839"
  },
  {
    "text": "But it is not clear that\nthis is the right way to solve data-driven\ndecision making. This is one framework.",
    "start": "3908840",
    "end": "3915460"
  },
  {
    "text": "So I had a professor when\nI was a grad student, who said that the whole world\nis a multi-agent partially",
    "start": "3915460",
    "end": "3924623"
  },
  {
    "text": "observable Markov\ndecision process where you're doing learning. But it doesn't mean you\nwant to solve it like that.",
    "start": "3924623",
    "end": "3929860"
  },
  {
    "text": "And so while, in\nmany times, we might be able to model things in\nthese kind of stochastic Markov",
    "start": "3929860",
    "end": "3935680"
  },
  {
    "text": "decision process ways,\nthat may or may not be the most efficient way\nto represent the problem. It's just like how\nyou could always",
    "start": "3935680",
    "end": "3941738"
  },
  {
    "text": "represent a bandit as a\nreally complicated RL problem. But if your next states are\nindependent of your previous",
    "start": "3941738",
    "end": "3947350"
  },
  {
    "text": "one, why would you do that? So I think there's some\nreal questions over like, are there better formulations?",
    "start": "3947350",
    "end": "3954168"
  },
  {
    "text": "I think a second thing is that,\nhistorically in reinforcement learning-- and even throughout\nmost of this class-- we focused on, I'm going\nto learn from this one task",
    "start": "3954168",
    "end": "3961480"
  },
  {
    "text": "from scratch. But of course, that's\nnot what humans do. We constantly are building\non our prior experience.",
    "start": "3961480",
    "end": "3967039"
  },
  {
    "text": "We are sort of imperfect\nagents for learning across many, many, many tasks. And what we've seen\nfrom generative AI--",
    "start": "3967040",
    "end": "3973630"
  },
  {
    "text": "sort of large language\nmodels, et cetera, is that doing many, many tasks\nmight be really powerful.",
    "start": "3973630",
    "end": "3979990"
  },
  {
    "text": "And that's been relatively\nunderstudied in the RL setting. And it might be\nmuch more effective.",
    "start": "3979990",
    "end": "3985070"
  },
  {
    "text": "We've seen even in like\nAlphaZero, and AlphaTensor, and others, that these\nshared representations can have huge benefits.",
    "start": "3985070",
    "end": "3991717"
  },
  {
    "text": "And so those might be\nreally productive ways to think about accelerating\nthe speed of decision making and learning good\ndata driven policies.",
    "start": "3991717",
    "end": "3999760"
  },
  {
    "text": "I think a third thing is\nthinking about alternative forms of feedback. Assuming you get\nsingle scalar rewards,",
    "start": "3999760",
    "end": "4006569"
  },
  {
    "text": "it's pretty limiting,\nparticularly now that we have\nlarge language models. You could imagine having\nreally rich feedback",
    "start": "4006570",
    "end": "4012570"
  },
  {
    "text": "or really sparse feedback. Like, thumbs up, thumbs\ndown, or preference pairs. Or really detailed examples\nabout how something is wrong",
    "start": "4012570",
    "end": "4019360"
  },
  {
    "text": "or what your preferences are. And now that we can start\nto have language as rewards,",
    "start": "4019360",
    "end": "4024770"
  },
  {
    "text": "I think that's a much\nricher opportunity. And people are starting\nto explore this already.",
    "start": "4024770",
    "end": "4030250"
  },
  {
    "text": "Another sort of just\nwhat settings we're in. Most of this class, we thought\nabout stochastic settings.",
    "start": "4030250",
    "end": "4036369"
  },
  {
    "text": "Take an action from a state. You get to some next state\ngenerated sort of stochastically",
    "start": "4036370",
    "end": "4041530"
  },
  {
    "text": "from some indifferent process. But that's not very common\nin real-world settings.",
    "start": "4041530",
    "end": "4048603"
  },
  {
    "text": "In many real-world\nsettings, there are other stakeholders\nor multi agents that might be adversarial,\nor might be cooperative.",
    "start": "4048603",
    "end": "4055595"
  },
  {
    "text": "You might have a\nteacher that's helping the agent learn something. Or you might have\nan adversary that's competing with that agent.",
    "start": "4055595",
    "end": "4061310"
  },
  {
    "text": "And so those settings are also\nreally important to consider. And I think another question,\ntoo, is, throughout this class,",
    "start": "4061310",
    "end": "4068783"
  },
  {
    "text": "we've been thinking about\nintegrating, and doing learning, and planning, and decision\nmaking all at once, everything. And that's wonderful\nand elegant.",
    "start": "4068783",
    "end": "4075810"
  },
  {
    "text": "But there are many\napproximations to this. So in some other fields, they\noften do system identification.",
    "start": "4075810",
    "end": "4081690"
  },
  {
    "text": "Like you might learn how the\nMarkov decision process works. You learn your dynamics model,\nyou learn your word model.",
    "start": "4081690",
    "end": "4086700"
  },
  {
    "text": "You stop, you plant. And so while this\noffers some flexibility,",
    "start": "4086700",
    "end": "4092290"
  },
  {
    "text": "it also introduces\na lot of complexity. And again, in some\nareas, there might be some really good\nalternatives to this.",
    "start": "4092290",
    "end": "4099114"
  },
  {
    "text": "And finally, this is\none that's perhaps closest to my heart,\nwhich is, I think that there's just\nan enormous room to do better data-driven\ndecision making",
    "start": "4099115",
    "end": "4107910"
  },
  {
    "text": "in domains that could benefit. So I think there are\nlots of application areas we've talked about in class.",
    "start": "4107910",
    "end": "4112960"
  },
  {
    "text": "But there's so\nmany areas where I think our society could benefit\nfrom better decision making. And so it'd be incredible\nto see more of that impact,",
    "start": "4112960",
    "end": "4119735"
  },
  {
    "text": "whether it's from\nthe frameworks we've covered in class or from others. And I think one of\nthe wonderful things is that you guys are\nvery well equipped now",
    "start": "4119736",
    "end": "4126449"
  },
  {
    "text": "to go out and start\nanswering these questions, or other ones that you\nthink are important.",
    "start": "4126450",
    "end": "4132609"
  },
  {
    "text": "All right, I'll just close\nwith two more slides. One is that if you like\nreinforcement learning, there is a lot of\npeople at Stanford",
    "start": "4132609",
    "end": "4138149"
  },
  {
    "text": "who think about\nreinforcement learning. There are lots of classes. There's at least another five.",
    "start": "4138149",
    "end": "4143710"
  },
  {
    "text": "So there's Deep RL with Chelsea. There's Decision Making\nunder Uncertainty with Mykel. Mykel and I both\noffer advanced courses",
    "start": "4143710",
    "end": "4150180"
  },
  {
    "text": "in needed decision making or RL. And Ben Ryan Roy often\nalso offers an advanced RL,",
    "start": "4150180",
    "end": "4155609"
  },
  {
    "text": "or bandit class. So there's lots of\nplaces to learn more. And finally, thanks for\nbeing part of the course.",
    "start": "4155609",
    "end": "4161380"
  },
  {
    "text": "It's great to get\nto meet everyone. And we're really excited to\nsee your posters on Wednesday. Thanks.",
    "start": "4161380",
    "end": "4166730"
  },
  {
    "start": "4166730",
    "end": "4173000"
  }
]