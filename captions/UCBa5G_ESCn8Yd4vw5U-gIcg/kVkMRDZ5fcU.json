[
  {
    "start": "0",
    "end": "6430"
  },
  {
    "text": "OK, I guess let's get started. So in this lecture,\nwhat we're going to do",
    "start": "6430",
    "end": "11830"
  },
  {
    "text": "is that at the\nbeginning we're going to talk about deep\nlearning, especially",
    "start": "11830",
    "end": "17352"
  },
  {
    "text": "some of the challenges\nin deep learning theory. ",
    "start": "17352",
    "end": "23640"
  },
  {
    "text": "And then in the next\nprobably 5 to 10 lectures, we are going to\ndiscuss different aspects",
    "start": "23640",
    "end": "30442"
  },
  {
    "text": "about deep learning, I guess. You will see like we're going to\ntalk optimization, [INAUDIBLE] so on, so forth.",
    "start": "30442",
    "end": "37296"
  },
  {
    "text": "So basically, in\ndeep learning theory, there are different aspects,\nfor example, optimization, which we spend probably\ntwo lectures on later.",
    "start": "37297",
    "end": "43470"
  },
  {
    "text": "And generalization\nis another question which we probably will\ntalk about for probably",
    "start": "43470",
    "end": "50070"
  },
  {
    "text": "more than three lectures. And at end of the\ncourse, we are going to talk about some other\nslightly different topics.",
    "start": "50070",
    "end": "58302"
  },
  {
    "text": "So in some sense,\nyou can view this as kind of like an outline\nfor the next five weeks. ",
    "start": "58302",
    "end": "66697"
  },
  {
    "text": "So to talk about\ndeep learning theory, I think it's probably useful\nto somewhat kind of summarize",
    "start": "66697",
    "end": "72000"
  },
  {
    "text": "the classical machine learning\ntheory, which I actually didn't really talk about\nthat much from a bird's eye",
    "start": "72000",
    "end": "79890"
  },
  {
    "text": "view that much in the\nbeginning of the course because I felt that\nif you give too",
    "start": "79890",
    "end": "85313"
  },
  {
    "text": "much information\nat the beginning, it's probably a\nlittle bit too much. So but now I'm going\nto have a higher level",
    "start": "85313",
    "end": "92250"
  },
  {
    "text": "view about what\nclassical machine learning theory do in terms\nof different kind of aspects",
    "start": "92250",
    "end": "99570"
  },
  {
    "text": "or different topics. So I guess in the more classical\nmachine learning theory,",
    "start": "99570",
    "end": "105570"
  },
  {
    "text": "there are several things. So one thing is called\napproximation theory.",
    "start": "105570",
    "end": "111180"
  },
  {
    "start": "111180",
    "end": "117560"
  },
  {
    "text": "So in some sense, this--\nand another keyword is called expressivity or\nrepresentational power.",
    "start": "117560",
    "end": "125170"
  },
  {
    "text": "If you see these\nkind of things, you know that's\nrepresentational power. So you know that they are\nall about the same thing.",
    "start": "125170",
    "end": "132470"
  },
  {
    "text": "So what they are doing\nis really caring about, basically, you want\nto bound L theta",
    "start": "132470",
    "end": "139340"
  },
  {
    "text": "star, which is the best\nmodel in your family.",
    "start": "139340",
    "end": "146870"
  },
  {
    "text": "So so far, until this week, we\nalways talk about excess risk.",
    "start": "146870",
    "end": "152870"
  },
  {
    "text": "We compare it with the\nbest model in the class, and we say that if you can get\nthe best model in the class,",
    "start": "152870",
    "end": "158090"
  },
  {
    "text": "then you are done. But actually it's not\ndone, because maybe you are using the wrong\nhypothesis class.",
    "start": "158090",
    "end": "165840"
  },
  {
    "text": "So your best hypothesis\nclass in the family hypothesis-- the best hypothesis\nin the hypothesis class",
    "start": "165840",
    "end": "172160"
  },
  {
    "text": "is probably not great, right? So approximation\ntheory is basically trying to deal with this, right?",
    "start": "172160",
    "end": "178430"
  },
  {
    "text": "You are trying to understand\nwhether your hypothesis class is powerful enough to express\nthe functions you care about.",
    "start": "178430",
    "end": "186690"
  },
  {
    "text": "So for example, a kind of\ntrivial case, for example, suppose you have some data like\nthis and something like this--",
    "start": "186690",
    "end": "196910"
  },
  {
    "text": "some positive data,\nsome negative data. Here, you know that if\nyou use linear model, then the best linear model is\nnot going to do great, right?",
    "start": "196910",
    "end": "204650"
  },
  {
    "text": "Because if you probably\nfind the best linear model, you probably would do\nsomething like this. I don't know. ",
    "start": "204650",
    "end": "212060"
  },
  {
    "text": "So in this case, you can say\nthat L theta star wouldn't be great if you\nchoose your capital",
    "start": "212060",
    "end": "217550"
  },
  {
    "text": "theta to be linear family. And then you can study what\nhypothesis class can contain",
    "start": "217550",
    "end": "224750"
  },
  {
    "text": "a good classifier even you\nhave access to population data, so on, so forth, right?",
    "start": "224750",
    "end": "230959"
  },
  {
    "text": "So in some sense, this\nis trying to understand how good can a hypothesis class\nH approximate the ground truth",
    "start": "230960",
    "end": "238443"
  },
  {
    "text": "label function, right?  So that's one type of question.",
    "start": "238443",
    "end": "246410"
  },
  {
    "text": "And another type of question\nis what we discussed already, which is about the statistical\naspect of sometimes people call",
    "start": "246410",
    "end": "257000"
  },
  {
    "text": "it generalization theory. ",
    "start": "257000",
    "end": "262099"
  },
  {
    "text": "So this is about\nthe excess risk, as we discussed in the\nlast several weeks.",
    "start": "262100",
    "end": "268160"
  },
  {
    "text": "So you are trying to bound\nfrom above the difference between your learned hypothesis\nfrom the best hypothesis, theta",
    "start": "268160",
    "end": "278610"
  },
  {
    "text": "star, right?  And what we have done was\nsomething like you bound this",
    "start": "278610",
    "end": "288560"
  },
  {
    "text": "by L theta hat minus L hat theta\nhat plus L theta star minus L",
    "start": "288560",
    "end": "299630"
  },
  {
    "text": "hat theta star.  And people have called this\nthe generalization error.",
    "start": "299630",
    "end": "308120"
  },
  {
    "text": " The generalization\nerror is the difference",
    "start": "308120",
    "end": "315250"
  },
  {
    "text": "between the population\nloss and empirical loss on the learned type\nparameter, right?",
    "start": "315250",
    "end": "320830"
  },
  {
    "text": " So this is the\ngeneralization loss basically the difference between training\nloss and test loss, right,",
    "start": "320830",
    "end": "329610"
  },
  {
    "text": "on the learned\nparameters theta hat. If, say, the hat is ERM, then\nthis is talking about ERM,",
    "start": "329610",
    "end": "334920"
  },
  {
    "text": "but maybe in other cases you\nare using some other algorithm to find theta hat that you\nwant the generalization",
    "start": "334920",
    "end": "340605"
  },
  {
    "text": "error for that theta hat. And this term, as we argued,\nthe second term is always small.",
    "start": "340605",
    "end": "346230"
  },
  {
    "text": "Just no matter what hypothesis\nclass you use, basically, as long as your loss\nfunction is bounded and this term is\nalways something",
    "start": "346230",
    "end": "352290"
  },
  {
    "text": "like 1 over square root of n. So basically, that's why we\ndon't care about this term that much.",
    "start": "352290",
    "end": "358200"
  },
  {
    "text": "OK, so what we have\ndone was something",
    "start": "358200",
    "end": "364110"
  },
  {
    "text": "like you prove this kind\nof generalization bound. So you prove something like\nL theta hat minus L hat theta",
    "start": "364110",
    "end": "371640"
  },
  {
    "text": "hat. We bound it by something\nlike some complexity",
    "start": "371640",
    "end": "381200"
  },
  {
    "text": "over square root of n. I guess typically, probably\nyou should write this. ",
    "start": "381200",
    "end": "387229"
  },
  {
    "text": "And the principle here is\nthat if your hypothesis class is of low complexity, then\nyou have better generalization",
    "start": "387230",
    "end": "395930"
  },
  {
    "text": "error, right? So simple hypothesis\ncan generalize better.",
    "start": "395930",
    "end": "402319"
  },
  {
    "text": "So I think sometimes also\npeople call this Occam's razor.",
    "start": "402320",
    "end": "407440"
  },
  {
    "text": "This is, I think, is kind of\nlike philosophical principle which dates back to something\nlike 1100 or around that time,",
    "start": "407440",
    "end": "417400"
  },
  {
    "text": "and the principle is something\nlike simple or parsimonious explanation can generalize\nbetter to other situations.",
    "start": "417400",
    "end": "426477"
  },
  {
    "text": "And you can see even from\nthese two things, right, you can see that there is some\nkind of conflict or trade-off",
    "start": "426477",
    "end": "433300"
  },
  {
    "text": "between the approximation theory\nand the generalization theory. Because if you use a very,\nvery simple hypothesis class,",
    "start": "433300",
    "end": "441460"
  },
  {
    "text": "then your L theta star\nmay not be good enough. For example, for the beta I drew\nhere, if you use linear model,",
    "start": "441460",
    "end": "448270"
  },
  {
    "text": "then your L theta\nstar is not great. But your generalization\nerror could be very good because your model\nis linear and simple.",
    "start": "448270",
    "end": "455880"
  },
  {
    "text": "So there is some\ntrade-off between-- and I think people\nalso sometimes called this bias and variance. So the variance\nmostly corresponds",
    "start": "455880",
    "end": "463592"
  },
  {
    "text": "to the generalization theory. It corresponds to statistical\nerror introduced from learning",
    "start": "463592",
    "end": "469720"
  },
  {
    "text": "because you have finite data. That's why you have to\npay something that depends on how many examples you have. That's the variance,\nand the bias mostly",
    "start": "469720",
    "end": "477760"
  },
  {
    "text": "is a quantity that\nonly depends-- bias, all the\nexpressivity, is a quantity that depends on the fundamental\npower of the hypothesis class.",
    "start": "477760",
    "end": "486010"
  },
  {
    "text": "It's not something that depends\non how many examples you have, right?",
    "start": "486010",
    "end": "491439"
  },
  {
    "text": "But the variance bias trade-off\nis essentially the same thing here, but the exact definition\nof bias and variance",
    "start": "491440",
    "end": "498820"
  },
  {
    "text": "can only apply to basically\nsquare loss and linear model. That's why we don't\nuse the explicit here.",
    "start": "498820",
    "end": "505990"
  },
  {
    "text": "But the principles\nare somewhat related.",
    "start": "505990",
    "end": "511060"
  },
  {
    "text": "And you can also kind of extend\nthis generalization theory a little bit by\nsaying that you can",
    "start": "511060",
    "end": "519849"
  },
  {
    "text": "consider the regularized loss. In some sense, you\ncan consider this as application, implication of\nthe transition theory, which",
    "start": "519850",
    "end": "526750"
  },
  {
    "text": "says that if you use\nregularized loss, right, something like L hat reg is\nsomething like L hat theta",
    "start": "526750",
    "end": "540820"
  },
  {
    "text": "plus lambda R theta, where\nthis is a regularizer that",
    "start": "540820",
    "end": "546650"
  },
  {
    "text": "captures the complexity\nof the hypothesis. So then, you can hope to\nhave a claim like this",
    "start": "546650",
    "end": "555260"
  },
  {
    "text": "so you can have a statistical\nclaim of the following form. Of course, this\ndepends on exactly which regularizer you use, what\nmodels, so on and so forth,",
    "start": "555260",
    "end": "563040"
  },
  {
    "text": "but the form of the claim is\nsomething like if theta lambda hat is the global\nminimizer of L hat reg,",
    "start": "563040",
    "end": "576840"
  },
  {
    "text": "then you have a\ngeneralization bound. ",
    "start": "576840",
    "end": "583230"
  },
  {
    "text": "You can bound excess risk,\nor you can bound, I guess, either the excess risk or\nthe generalization error.",
    "start": "583230",
    "end": "589560"
  },
  {
    "text": "I guess they are\npretty much related, as we have discussed, right? So all the generalization\nerror, so they",
    "start": "589560",
    "end": "597750"
  },
  {
    "text": "are bounded by something.",
    "start": "597750",
    "end": "604890"
  },
  {
    "text": "So this is the type of\nresults you probably get from this kind of\nstatistical generalization",
    "start": "604890",
    "end": "612970"
  },
  {
    "text": "theory. Because you know that if you-- the reason is that if you\noptimize this regularized loss,",
    "start": "612970",
    "end": "619140"
  },
  {
    "text": "and you, indeed, find a\nvery small regularized loss, that means that your\nregularizer-- the R theta, the complexity--",
    "start": "619140",
    "end": "626010"
  },
  {
    "text": "is small, and also it means that\nyour training area is small. And then if both\nof these are small, then you can show that\nyour excess risk is small",
    "start": "626010",
    "end": "634440"
  },
  {
    "text": "because this model\nwill generalize to the population\nof the test case.",
    "start": "634440",
    "end": "639840"
  },
  {
    "text": " And then there's a third aspect,\nwhich is called optimization.",
    "start": "639840",
    "end": "648280"
  },
  {
    "text": " Any questions so far?",
    "start": "648280",
    "end": "654899"
  },
  {
    "text": "Right, so there's a third aspect\nwhich is called optimization. So the question is\nabout numerically",
    "start": "654900",
    "end": "665590"
  },
  {
    "text": "how to find theta hat.",
    "start": "665590",
    "end": "672570"
  },
  {
    "text": "Theta hat could be the arg\nmin of the training loss, or maybe you can\ntalk about theta hat",
    "start": "672570",
    "end": "680100"
  },
  {
    "text": "lambda, the regularized\nloss from [INAUDIBLE] right? ",
    "start": "680100",
    "end": "697420"
  },
  {
    "text": "And this is a purely-- at least in a classical\nway of thinking about this, you can basically view this\nas a separate question about--",
    "start": "697420",
    "end": "706970"
  },
  {
    "text": "you can forget about\nwhere your data come from. You can forget about why\nyou care about minimizing",
    "start": "706970",
    "end": "713545"
  },
  {
    "text": "this training loss. You just say that I'm\ngetting this training loss. That's my job, right?",
    "start": "713545",
    "end": "718960"
  },
  {
    "text": "And typically, the\napproach is something like if the loss\nfunction is convex, you use convex optimization.",
    "start": "718960",
    "end": "727740"
  },
  {
    "text": "And in all, maybe you\ncan use gradient descent for non-convex functions,\nso on, so forth. Or maybe stochastic\ngradient descent.",
    "start": "727740",
    "end": "734950"
  },
  {
    "text": "There are many\ndifferent approaches. And when you\nmeasure the success, or you measure the\ninterface is that you",
    "start": "734950",
    "end": "740940"
  },
  {
    "text": "care about how well you can\napproximate the minimizer.",
    "start": "740940",
    "end": "746090"
  },
  {
    "text": "Or you can never find\nexact minimizer using a numerical approach, right? So you always have some\nsmall error compared",
    "start": "746090",
    "end": "754250"
  },
  {
    "text": "to the minimizer of\nthe empirical loss, and you can measure the\nerror in different ways,",
    "start": "754250",
    "end": "759650"
  },
  {
    "text": "maybe match the error in\nterms of the sub-optimality in terms of how different\nyour minimizer is",
    "start": "759650",
    "end": "766370"
  },
  {
    "text": "in terms of the loss function\ncompared to the best minimizer.",
    "start": "766370",
    "end": "771770"
  },
  {
    "text": "Or you can compare\nother quantities. So in some sense, I think from\nthis kind of summary here,",
    "start": "771770",
    "end": "782560"
  },
  {
    "text": "you can think of\nthe statistical part is kind of pretty\nmuch independent",
    "start": "782560",
    "end": "787960"
  },
  {
    "text": "from the optimization part. Of course, there are also\ninteresting interface. For example, you can\nalso ask about what",
    "start": "787960",
    "end": "795190"
  },
  {
    "text": "regularizer-- so when you\nadd regularizer, right, so you can ask the question,\nwhat regularizer can",
    "start": "795190",
    "end": "802270"
  },
  {
    "text": "simultaneously have good\nstatistical performance, but also can be easy\nto optimize, right?",
    "start": "802270",
    "end": "807970"
  },
  {
    "text": "And by easy to\noptimize, it means that you can optimize it\nfast, or maybe optimize it in a certain time,\nmaybe d time or d",
    "start": "807970",
    "end": "814240"
  },
  {
    "text": "squared time, so on so forth. So there are still interactions\nbetween different parts, but if you just need\na kind of high level",
    "start": "814240",
    "end": "822790"
  },
  {
    "text": "kind of understanding,\nyou can think of them as separate parts, right? The interaction are more\non the lower level details",
    "start": "822790",
    "end": "829630"
  },
  {
    "text": "about how do you achieve the\nbest statistical efficiency, or how do you achieve the best\ncomputational and statistical",
    "start": "829630",
    "end": "836230"
  },
  {
    "text": "efficiency? Then you have to talk\nabout the interactions. But at a high level,\nyou don't have to think about them\nsimultaneously.",
    "start": "836230",
    "end": "843766"
  },
  {
    "text": "You can think of them\nroughly separately. Is there a question? Yeah, [INAUDIBLE] ",
    "start": "843766",
    "end": "852870"
  },
  {
    "text": "Sorry-- no, no. This is another visual, sorry. My bad. This is just two things.",
    "start": "852870",
    "end": "858980"
  },
  {
    "text": "My writing is bad. And these two qualities are\nbasically similar, right? So like you care about\nthe excess risk, which",
    "start": "858980",
    "end": "865700"
  },
  {
    "text": "is the most important\nthing, but which is almost the same as\nthe generalization error.",
    "start": "865700",
    "end": "873800"
  },
  {
    "text": "And actually, you bound\nthe generalization error. Then you bound excess risk. Sorry, my writing is not clear.",
    "start": "873800",
    "end": "879680"
  },
  {
    "text": " So any questions so far?",
    "start": "879680",
    "end": "885470"
  },
  {
    "text": "So these are the standard way of\nthinking about these questions,",
    "start": "885470",
    "end": "891079"
  },
  {
    "text": "but what happens\nin deep learning? What happens in deep\nlearning is that, as you'll see, things\nbecomes more complicated,",
    "start": "891080",
    "end": "896420"
  },
  {
    "text": "and for a fundamental reason. And I think the first thing\nis that for deep learning,",
    "start": "896420",
    "end": "903020"
  },
  {
    "text": "there are probably two\nthings that change, at least on the surface. So one thing that\nchanges is that you",
    "start": "903020",
    "end": "908480"
  },
  {
    "text": "have from linear model, it\nbecomes nonlinear model, right? ",
    "start": "908480",
    "end": "915250"
  },
  {
    "text": "And this directly\naffects the optimization because when you\nhave nonlinear model, it becomes non-convex loss.",
    "start": "915250",
    "end": "920410"
  },
  {
    "text": " But this wouldn't change the\nstructure view fundamentally",
    "start": "920410",
    "end": "931700"
  },
  {
    "text": "because it makes the\noptimization question harder, right? So at least at the\nbeginning, this is what I thought\nfive years ago,",
    "start": "931700",
    "end": "938490"
  },
  {
    "text": "maybe more than five years ago. When I started to do deep\nlearning theory right",
    "start": "938490",
    "end": "944990"
  },
  {
    "text": "after deep learning took off,\nright, at very, very first I thought that the\nonly difference",
    "start": "944990",
    "end": "950180"
  },
  {
    "text": "is that now the optimization\nquestion becomes harder. And then the question is just\nhow do you optimize better?",
    "start": "950180",
    "end": "957750"
  },
  {
    "text": "But then, I think in\nprobably like about three or four years ago,\npeople realized that there is also another\nfundamental difference",
    "start": "957750",
    "end": "964282"
  },
  {
    "text": "from the statistical\nperspective, which is that empirically,\nyou always use",
    "start": "964282",
    "end": "969950"
  },
  {
    "text": "this so-called\noverparameterized model. Maybe it's not precisely\nto say that you always",
    "start": "969950",
    "end": "976360"
  },
  {
    "text": "use overparameterized\nmodel, but generally, overparameterized\nmodels are better than--",
    "start": "976360",
    "end": "983470"
  },
  {
    "text": "more parameters are\nalways generally better, or almost always better. So more parameters\ngenerally helps.",
    "start": "983470",
    "end": "994620"
  },
  {
    "text": "And it can help\neven to the extent that when your parameters are\nmore than the number of data",
    "start": "994620",
    "end": "1000227"
  },
  {
    "text": "points, right? So it even helps when\nd is larger than n, this still helps. ",
    "start": "1000227",
    "end": "1006339"
  },
  {
    "text": "And it even helps when you have\nalready zero training error. So even after you already\nhave zero training error.",
    "start": "1006340",
    "end": "1020330"
  },
  {
    "text": "So this is a plot that\nI got from some paper. This is from a paper by\nNeyshabur, Tomioka, and Srebro",
    "start": "1020330",
    "end": "1029209"
  },
  {
    "text": "in 2015. So this is what they've found. Of course, this is only a very\nsmall data set, but roughly",
    "start": "1029210",
    "end": "1037459"
  },
  {
    "text": "speaking, the same\nphenomenon also holds for a larger data set. And you can see here that the\nblack curve is the training",
    "start": "1037460",
    "end": "1045280"
  },
  {
    "text": "error, and the x-axis is\nhow many hidden units, or how large network is.",
    "start": "1045280",
    "end": "1050600"
  },
  {
    "text": "Hidden units means the number\nof neurons in your network. Which if you have\nmore hidden neurons,",
    "start": "1050600",
    "end": "1055820"
  },
  {
    "text": "you have more parameters. And actually, the\nnumber of parameters",
    "start": "1055820",
    "end": "1061730"
  },
  {
    "text": "is quadratic in\nthe hidden neurons in this fully connected case. This is a very simple fully\nconnected network, MNIST,",
    "start": "1061730",
    "end": "1070430"
  },
  {
    "text": "and you can see that after\nyou have more than 64 hidden neurons, you can\nfit MNIST perfectly.",
    "start": "1070430",
    "end": "1078020"
  },
  {
    "text": "0% error-- I think\nliterally zero. Maybe not exactly\nliterally, maybe 0.01%",
    "start": "1078020",
    "end": "1083240"
  },
  {
    "text": "error or something like that. And if you look at\na typical textbook,",
    "start": "1083240",
    "end": "1090132"
  },
  {
    "text": "right, so what you\nwould do is that you would predict that the\ntest error will go up",
    "start": "1090132",
    "end": "1097687"
  },
  {
    "text": "after a certain point\nbecause you are overfitting. You are using too\ncomplex of a model, and you are overfitting\nto the data.",
    "start": "1097687",
    "end": "1104400"
  },
  {
    "text": "That's the purple thing\nwhich you would probably read from some of the\nclassical textbooks. And actually, it does happen\nin some classical settings,",
    "start": "1104400",
    "end": "1112070"
  },
  {
    "text": "but does not happen\noften in neural networks, or probably never happens\nin neural networks.",
    "start": "1112070",
    "end": "1117440"
  },
  {
    "text": " And what really happens\nis the right one.",
    "start": "1117440",
    "end": "1124825"
  },
  {
    "text": "The generalization\nerror actually continue to improve as you\nhave more and more neurons even though you already\nmemorized everything, right?",
    "start": "1124825",
    "end": "1133010"
  },
  {
    "text": "So if you compare\n64 with this 4k, basically these are\njust two networks.",
    "start": "1133010",
    "end": "1142010"
  },
  {
    "text": "Both of them fit the training\ndata with 100% accuracy,",
    "start": "1142010",
    "end": "1147260"
  },
  {
    "text": "but one of them has better\ntest accuracy than the other. ",
    "start": "1147260",
    "end": "1153429"
  },
  {
    "text": "So this is kind of a big\nmystery from a theoretical point of view, especially if you\nbelieve in the classical",
    "start": "1153430",
    "end": "1160460"
  },
  {
    "text": "trade-off between bias and\nvariance or the trade-off between expressivity and\ngeneralization theory--",
    "start": "1160460",
    "end": "1166040"
  },
  {
    "text": "generalization power.  So this is the big\nopen question, right?",
    "start": "1166040",
    "end": "1172610"
  },
  {
    "text": "And briefly, let\nme discuss again",
    "start": "1172610",
    "end": "1178520"
  },
  {
    "text": "what's the impact on\neach of these concepts? And actually, you even have\nto really think about some",
    "start": "1178520",
    "end": "1184610"
  },
  {
    "text": "of these concepts. Like some of these concepts\nbecome entangled or intertwined",
    "start": "1184610",
    "end": "1192800"
  },
  {
    "text": "now in deep learning. So first of all, for\napproximation theory, I think things don't\nchange that much at least",
    "start": "1192800",
    "end": "1203980"
  },
  {
    "text": "compared to other parts. So for approximation\ntheory, I think generally,",
    "start": "1203980",
    "end": "1211090"
  },
  {
    "text": "I guess, you know that\nlarge models are expressive. ",
    "start": "1211090",
    "end": "1220730"
  },
  {
    "text": "And there's actually something\ncalled universal approximation theorem. ",
    "start": "1220730",
    "end": "1227870"
  },
  {
    "text": "I'm not sure if you\nheard of it or not. In some sense, this\nis saying that if you have a network that\nis wide enough, then",
    "start": "1227870",
    "end": "1234350"
  },
  {
    "text": "you can approximate\nany functions. Of course, that's,\nin some sense, a misleading way to say\nit because what does it",
    "start": "1234350",
    "end": "1241923"
  },
  {
    "text": "mean by large enough, right? So if you need exponential\nnumber of neurons, that's, indeed, very large.",
    "start": "1241923",
    "end": "1246980"
  },
  {
    "text": "That's large enough, but that's\nnot really implementable. So empirically, you don't\neven need that many neurons",
    "start": "1246980",
    "end": "1254390"
  },
  {
    "text": "to be expressive. I think you just need\npolynomial number of neurons. But anyway, so the\ngist is we do believe,",
    "start": "1254390",
    "end": "1262302"
  },
  {
    "text": "regardless whether this\nuniversal approximation theory is exactly answering\nthe question, at least",
    "start": "1262302",
    "end": "1268230"
  },
  {
    "text": "we believe that the neural\nnetworks are very powerful. So we generally believe that\nthe best model in this family,",
    "start": "1268230",
    "end": "1278430"
  },
  {
    "text": "especially if you use\na wide enough network, this is generally small. This is what we\ngenerally believe.",
    "start": "1278430",
    "end": "1284250"
  },
  {
    "text": " And at least what\nyou can show is",
    "start": "1284250",
    "end": "1290100"
  },
  {
    "text": "that you can say this is\nreally small, the minimizer",
    "start": "1290100",
    "end": "1296275"
  },
  {
    "text": "of the training loss. Because if you have a\nneural network with more than n neurons, so\nthis is just because",
    "start": "1296275",
    "end": "1303600"
  },
  {
    "text": "with more than n neurons, n\nis the number of examples,",
    "start": "1303600",
    "end": "1311299"
  },
  {
    "text": "you can provably memorize\nall the training examples. ",
    "start": "1311300",
    "end": "1317950"
  },
  {
    "text": "At least you can find\none network that memorize all the training examples. That network may not\ngeneralize, but this already",
    "start": "1317950",
    "end": "1325570"
  },
  {
    "text": "means that your minimal\ntraining loss is very small. It's probably zero. ",
    "start": "1325570",
    "end": "1336010"
  },
  {
    "text": "OK, so basically, for\napproximation theory, I think we generally\nbelieve that the models are",
    "start": "1336010",
    "end": "1341950"
  },
  {
    "text": "very expressive. And then that becomes the\ngeneralization part, which",
    "start": "1341950",
    "end": "1348400"
  },
  {
    "text": "becomes quite complicated. So there's another information\nabout what practical network",
    "start": "1348400",
    "end": "1359130"
  },
  {
    "text": "does is that in practice,\nalso people don't use very strong regularization.",
    "start": "1359130",
    "end": "1366540"
  },
  {
    "text": "Only weak\nregularizations are used.",
    "start": "1366540",
    "end": "1371940"
  },
  {
    "text": "And this is kind of like\na somewhat important thing to say just because recall that\neven in a classical setting,",
    "start": "1371940",
    "end": "1379185"
  },
  {
    "text": "right, it's not always\nthat you can show-- ",
    "start": "1379185",
    "end": "1385860"
  },
  {
    "text": "sometimes you can have a-- so even in a\nclassical setting, you can have the setting there where\nyou have a lot of parameters,",
    "start": "1385860",
    "end": "1392940"
  },
  {
    "text": "but you have a strong\nregularization to compensate. So that's allowed in a\nclassical setting, right?",
    "start": "1392940",
    "end": "1398500"
  },
  {
    "text": "For example, if you use a\nsparse linear regression where you have a\nlot of features,",
    "start": "1398500",
    "end": "1404460"
  },
  {
    "text": "the dimensionality is very high,\nbut you regularize the sparsity of your linear model. Then you-- wait,\nI think speaking",
    "start": "1404460",
    "end": "1416455"
  },
  {
    "text": "of sparse linear\nmodel, I think I forgot to do\nsomething that we left last time about the comparison\nbetween linear models.",
    "start": "1416455",
    "end": "1423550"
  },
  {
    "text": "But anyway, my bad. I think I should have it. But anyway, let's\ncontinue with this.",
    "start": "1423550",
    "end": "1429660"
  },
  {
    "text": "But anyway, so what I\nwas saying is that even in the classical case, you do\nallow to have d bigger than n.",
    "start": "1429660",
    "end": "1438060"
  },
  {
    "text": "The dimension can be\nbigger than n as long as you use the\nregularization, right? Because if you use\nthe regularization,",
    "start": "1438060",
    "end": "1443930"
  },
  {
    "text": "you implicitly restrict\nthe complexity. For example, if you say that\nthe sparsity of your model is s,",
    "start": "1443930",
    "end": "1450565"
  },
  {
    "text": "and s is less than\nn, then that's OK. So however, in deep\nlearning, in practice",
    "start": "1450565",
    "end": "1458680"
  },
  {
    "text": "we only use very weak\nregularization, right? So typically just some L2. ",
    "start": "1458680",
    "end": "1466000"
  },
  {
    "text": "At least even with\nL2 you can work. Sometimes even without L2,\nyou can work pretty well.",
    "start": "1466000",
    "end": "1472820"
  },
  {
    "text": "And also, the\nregularization strength is also relatively small. The strength is small\nenough so that you can still",
    "start": "1472820",
    "end": "1479470"
  },
  {
    "text": "fit your training data with\nbasically 100% accuracy.",
    "start": "1479470",
    "end": "1485940"
  },
  {
    "text": "And another way to see the\nweakness of the regularization is that you can consider\nthe following fact.",
    "start": "1485940",
    "end": "1492420"
  },
  {
    "text": "So this regularized loss,\nif you, for example, just",
    "start": "1492420",
    "end": "1498540"
  },
  {
    "text": "regularize with something\nlike L2 with some lambda,",
    "start": "1498540",
    "end": "1504050"
  },
  {
    "text": "the regularized loss doesn't\nhave unique global minimizer.",
    "start": "1504050",
    "end": "1512008"
  },
  {
    "text": " Or at least, it\nhas very different",
    "start": "1512008",
    "end": "1519289"
  },
  {
    "text": "approximate global\nminimizer, right? Maybe if you really care\nabout the numerical precision,",
    "start": "1519290",
    "end": "1525380"
  },
  {
    "text": "if you say like you care about\nvery, very small precision,",
    "start": "1525380",
    "end": "1533195"
  },
  {
    "text": "then maybe there's a\nunique global minimizer. But for practical\npurposes, there",
    "start": "1533195",
    "end": "1539600"
  },
  {
    "text": "are many different\nglobal minimizers that are very similar in\nterms of the training accuracy",
    "start": "1539600",
    "end": "1545059"
  },
  {
    "text": "and in terms of the\nregularized loss. And they all have very\nsmall regularized loss.",
    "start": "1545060",
    "end": "1550130"
  },
  {
    "text": "They have very small loss\nfrom the regularizer part.",
    "start": "1550130",
    "end": "1556160"
  },
  {
    "text": "They also have very small loss\nfrom the training error part, and they are different\nglobal minimizers.",
    "start": "1556160",
    "end": "1563320"
  },
  {
    "text": "And another thing is\nthat it's also not true that all of these global\nminimizers perform the same.",
    "start": "1563320",
    "end": "1569100"
  },
  {
    "text": "So these global minimizers\nperform the same on the test.",
    "start": "1569100",
    "end": "1587500"
  },
  {
    "text": "I guess probably it's easier\nto just have a figure here. I think I did prepare a figure.",
    "start": "1587500",
    "end": "1592615"
  },
  {
    "text": " So let's see.",
    "start": "1592615",
    "end": "1597640"
  },
  {
    "text": "I think this is experiment\nI've done a few years back.",
    "start": "1597640",
    "end": "1602740"
  },
  {
    "text": "There are many\ndifferent kind of plots you can find online on\ndifferent papers like this. This is just one of them.",
    "start": "1602740",
    "end": "1608380"
  },
  {
    "text": "I actually take a\nlittle bit to exacerbate the differences a\nlittle bit, but the gist",
    "start": "1608380",
    "end": "1614320"
  },
  {
    "text": "is always is the same. So this is what? This is CFAR-10, and you have\ntwo algorithm, the red one",
    "start": "1614320",
    "end": "1623290"
  },
  {
    "text": "or the blue one. And I'm plotting the\ntraining and the test. And these two algorithms only\ndiffer by the learning rate.",
    "start": "1623290",
    "end": "1632200"
  },
  {
    "text": "They have the same\ntraining objective. They have the same\nregularization strengths.",
    "start": "1632200",
    "end": "1639550"
  },
  {
    "text": "It's just that the\noptimizer are different. So at the end of day, you\nsee that both of these two",
    "start": "1639550",
    "end": "1645130"
  },
  {
    "text": "algorithms found some\nglobal minimizer, or approximate global minimizer.",
    "start": "1645130",
    "end": "1650440"
  },
  {
    "text": "You can see the training\nerror is close to 0 in both of the two cases, right? ",
    "start": "1650440",
    "end": "1657370"
  },
  {
    "text": "So both of these are global\nmin in some sense, or at least an approximate global min up\nto a very good approximation.",
    "start": "1657370",
    "end": "1665580"
  },
  {
    "text": "But you can see that their\ntest error are very different. So that means that these are two\ndifferent global min for sure,",
    "start": "1665580",
    "end": "1670830"
  },
  {
    "text": "right, in the parameter space. And also, they perform\nvery differently on a test. ",
    "start": "1670830",
    "end": "1677940"
  },
  {
    "text": "So that's kind of\nthe mystery, right, because this kind of\nrefutes the possibility",
    "start": "1677940",
    "end": "1683040"
  },
  {
    "text": "to have a theorem like in\nthe classical case, right? So recall that in\na classical case,",
    "start": "1683040",
    "end": "1689160"
  },
  {
    "text": "typically you have theorem like\nthis where I'm seeing-- yeah, so you have theorems like\nthis, saying something like,",
    "start": "1689160",
    "end": "1697960"
  },
  {
    "text": "if you find the\nglobal minimizer, or a global minimizer,\nor any global minimizer of the regularized loss,\nthen you can generalize.",
    "start": "1697960",
    "end": "1704860"
  },
  {
    "text": "You can bound the\ngeneralization error. And this is no longer\nthe case because not all the global minimizers\nare the same.",
    "start": "1704860",
    "end": "1711309"
  },
  {
    "text": "Some of them are better. Some of them are\nworse, and you probably shouldn't have the same\nbound for all of them. And some of them probably just\ndon't generalize at all, right?",
    "start": "1711310",
    "end": "1719090"
  },
  {
    "text": "So this is saying that you\ncannot just say any global minimizer generalized. You have to somehow distinguish\ndifferent global minimizers",
    "start": "1719090",
    "end": "1725842"
  },
  {
    "text": "found by different algorithm. ",
    "start": "1725842",
    "end": "1733660"
  },
  {
    "text": "But what happens here, right? So what happens is\nthat the optimizations start to come into play.",
    "start": "1733660",
    "end": "1739730"
  },
  {
    "text": "And this is the reason. So basically, as I\nalluded to in some sense,",
    "start": "1739730",
    "end": "1745410"
  },
  {
    "text": "different optimizers found\ndifferent global minima. And some of them are better,\nand some of them are worse.",
    "start": "1745410",
    "end": "1750520"
  },
  {
    "text": "So that is saying that\noptimization is not only-- so optimization is\nnot only about finding",
    "start": "1750520",
    "end": "1766200"
  },
  {
    "text": "any minimizers, any global min. ",
    "start": "1766200",
    "end": "1771310"
  },
  {
    "text": "If you just say you find a\nglobal min, that's not enough. You have to use optimization\nto find the right global min.",
    "start": "1771310",
    "end": "1777940"
  },
  {
    "text": "So in some sense,\nthe optimizations",
    "start": "1777940",
    "end": "1783070"
  },
  {
    "text": "have two jobs, uh-huh. One thing is they\nhave to find something that has smaller\nerror, or a small error",
    "start": "1783070",
    "end": "1790220"
  },
  {
    "text": "or small regularized\nloss, and the other job is that it also has to find\nsomething to generalize.",
    "start": "1790220",
    "end": "1797710"
  },
  {
    "text": "It has to find a global\nminimum that can generalize. So in some sense, the\nkind of the picture",
    "start": "1797710",
    "end": "1804070"
  },
  {
    "text": "is like this in my mind. So you have this--",
    "start": "1804070",
    "end": "1809890"
  },
  {
    "text": "I'm using one\ndimensional thing, right? This dimension is the parameter.",
    "start": "1809890",
    "end": "1815600"
  },
  {
    "text": "And basically, I'm envisioning\nthis kind of toy case where you have the landscape of\nthe training loss and test loss",
    "start": "1815600",
    "end": "1822273"
  },
  {
    "text": "look like this, right? So the training loss\nhas two global minimum. And one of them is a\ngood global minimum,",
    "start": "1822273",
    "end": "1827960"
  },
  {
    "text": "and the other one is a bad one. And a bad one in the sense\nthat the corresponding test error is bad.",
    "start": "1827960",
    "end": "1833929"
  },
  {
    "text": "And the optimization\nalgorithm is not only responsible for finding\nan arbitrary global min,",
    "start": "1833930",
    "end": "1840559"
  },
  {
    "text": "it's also-- it actually has to find the\nright global min instead of the bad global min.",
    "start": "1840560",
    "end": "1846290"
  },
  {
    "text": "So somehow, the\noptimization algorithm is doing something beyond what\nit's supposed to do, right?",
    "start": "1846290",
    "end": "1852440"
  },
  {
    "text": "So I guess in some sense, this\nis a one dimensional case. If you think about the\nhigh dimensional case,",
    "start": "1852440",
    "end": "1858170"
  },
  {
    "text": "this is something I\noften use in my slides, it's kind of like you are\ngoing to a ski resort.",
    "start": "1858170",
    "end": "1865400"
  },
  {
    "text": "And the first time\nI came to America, I didn't realize that you\ncan have multiple valets,",
    "start": "1865400",
    "end": "1872930"
  },
  {
    "text": "or multiple parking lots\nin the same ski resort. So when I go back home, I\ndo gradient descent, right?",
    "start": "1872930",
    "end": "1883910"
  },
  {
    "text": "I just go to an arbitrary\nvalet and I found that my car was not there. And then it's actually a trouble\nbecause the resort is closed,",
    "start": "1883910",
    "end": "1892277"
  },
  {
    "text": "and it cannot lift you up. And so it's actually\npretty annoying. And then I realized\nthat actually there",
    "start": "1892277",
    "end": "1898520"
  },
  {
    "text": "are much more global\nminimum, and one of them is better than the others. And you have to find it,\nso it's not like arbitrary",
    "start": "1898520",
    "end": "1907410"
  },
  {
    "text": "within a set. Or maybe the grid\nis doing something more than just the arbitrary\ndownhill skiing, right?",
    "start": "1907410",
    "end": "1916730"
  },
  {
    "text": "[INAUDIBLE] is it the\nfact that [INAUDIBLE] ",
    "start": "1916730",
    "end": "1926929"
  },
  {
    "text": "Right. So why the generalization? So the question is\nexactly mathematically",
    "start": "1926930",
    "end": "1932809"
  },
  {
    "text": "where the generalization\ntheory breaks down. I think the bounds becomes\nbackwards-- basically,",
    "start": "1932810",
    "end": "1944830"
  },
  {
    "text": "the bounds you can\nprove becomes backwards. The bound you can prove\nunder the existing language",
    "start": "1944830",
    "end": "1951880"
  },
  {
    "text": "becomes backwards. So basically, if\nyou say that you want to prove a bound\nthat works for all,",
    "start": "1951880",
    "end": "1958440"
  },
  {
    "text": "you might work of size 10\nmillion, of size 100 million",
    "start": "1958440",
    "end": "1964080"
  },
  {
    "text": "for only 1 million examples. So if that's the\nlanguage you are using, then it wouldn't work anymore.",
    "start": "1964080",
    "end": "1970840"
  },
  {
    "text": "So you have to have a more\nprecise way to think about it.",
    "start": "1970840",
    "end": "1977020"
  },
  {
    "text": "Does that answer the\nquestion to some extent? [INAUDIBLE] what\nif you incorporated the fact that [INAUDIBLE] ",
    "start": "1977020",
    "end": "2000184"
  },
  {
    "text": "Right. Roughly speaking, that's the\napproach we're going to take. But there's one\nproblem with this. If you just do it\nexactly what you said,",
    "start": "2000185",
    "end": "2006370"
  },
  {
    "text": "there's a problem,\nwhich is you're going to get the same bound\nfor any algorithm, right?",
    "start": "2006370",
    "end": "2013540"
  },
  {
    "text": "But empirically,\ndifferent algorithms have different performance.",
    "start": "2013540",
    "end": "2020289"
  },
  {
    "text": "And the way to fix it\nbecomes that you first say that different\nalgorithm find models",
    "start": "2020290",
    "end": "2026950"
  },
  {
    "text": "with different\ncomplexity, and then you can have different\nbounds for them.",
    "start": "2026950",
    "end": "2032530"
  },
  {
    "text": "So the algorithm has to come\ninto play in some way, right? ",
    "start": "2032530",
    "end": "2038140"
  },
  {
    "text": "So basically, that's kind\nof the conclusion here. So the algorithm has\nto come into play",
    "start": "2038140",
    "end": "2043960"
  },
  {
    "text": "in your statistical\nanalysis, right? Because if you don't\nhave the algorithm there,",
    "start": "2043960",
    "end": "2049960"
  },
  {
    "text": "you are not going to distinguish\nthese different algorithms. So in some sense, you\nentangle the statistics",
    "start": "2049960",
    "end": "2056319"
  },
  {
    "text": "with optimization\nto some extent. ",
    "start": "2056320",
    "end": "2064330"
  },
  {
    "text": "And so basically,\nthe way to fix it is, at least the current plan,\nthe general agenda I think",
    "start": "2064330",
    "end": "2071230"
  },
  {
    "text": "most of the researchers\nseems to agree on is that you analyze\nthe optimization",
    "start": "2071230",
    "end": "2078190"
  },
  {
    "text": "and analyze why the optimizer\nfinds a good local minimum. So basically, you need to have\na theory that says something",
    "start": "2078190",
    "end": "2086500"
  },
  {
    "text": "like, the optimizer find\na theta hat such that--",
    "start": "2086500",
    "end": "2094620"
  },
  {
    "text": "so one, this theta\nhat is a global min that's approximate global\nmin of the empirical risk,",
    "start": "2094620",
    "end": "2104870"
  },
  {
    "text": "and also two, theta hat\nhas some special property",
    "start": "2104870",
    "end": "2114160"
  },
  {
    "text": "that you didn't explicitly\nsay that they should do.",
    "start": "2114160",
    "end": "2119200"
  },
  {
    "text": "For example, the property\ncould be low complexity.  So maybe, for example, just\nto give you an extreme case,",
    "start": "2119200",
    "end": "2127349"
  },
  {
    "text": "for example, you\nrun an algorithm without any regularization. But then logically, you\nsay that even though I",
    "start": "2127350",
    "end": "2134130"
  },
  {
    "text": "didn't let it regularize, but\nactually, the theta hat I found has low L2 norm, or has\neven the minimum L2 norm.",
    "start": "2134130",
    "end": "2143153"
  },
  {
    "text": "Actually, you can\nprove these kind of theorems in certain cases. And then, because of the\nspecial property thing,",
    "start": "2143153",
    "end": "2149580"
  },
  {
    "text": "and this implies that it\ncan generalize, right? ",
    "start": "2149580",
    "end": "2157320"
  },
  {
    "text": "And the people have kind of\nproved theorems of this kind of form in many different case.",
    "start": "2157320",
    "end": "2163079"
  },
  {
    "start": "2163080",
    "end": "2174900"
  },
  {
    "text": "So for example, you can\ntalk about SUD, right? So SUD probably has\nsome special preferences",
    "start": "2174900",
    "end": "2182070"
  },
  {
    "text": "in terms of what models\nthey want to find, and maybe SUD with different\nkind of specifications, right?",
    "start": "2182070",
    "end": "2188849"
  },
  {
    "text": "So you can have large arrays,\nsmall batch, and so forth. I'll talk about\nthat in a moment.",
    "start": "2188850",
    "end": "2194319"
  },
  {
    "text": "But generally, we want to say\nthat the practical optimizer people are using can have some\npreferences on certain types",
    "start": "2194320",
    "end": "2202349"
  },
  {
    "text": "of global minimizer.  And then after you have this--",
    "start": "2202350",
    "end": "2208210"
  },
  {
    "text": "as I said, after you have\nthe special preference, then you can use the-- so this\npart from the special property,",
    "start": "2208210",
    "end": "2214520"
  },
  {
    "text": "the low complexity\nthrough generalization, this could be more classical. This could be classical\ntheory, or maybe",
    "start": "2214520",
    "end": "2223859"
  },
  {
    "text": "improvement of classical\ntheory depending on what complex measure\nyou are talking about, as you suggested.",
    "start": "2223860",
    "end": "2231450"
  },
  {
    "text": "So that's the current kind\nof statistical of extending",
    "start": "2231450",
    "end": "2238680"
  },
  {
    "text": "deep learning theory. Of course, there are\nother kind of approaches, but I think this is pretty\nmuch, I think, kind of",
    "start": "2238680",
    "end": "2245160"
  },
  {
    "text": "like the high level-- people have almost reached a\nconsensus on the high level approach here, I think.",
    "start": "2245160",
    "end": "2250470"
  },
  {
    "text": " And what are the best results?",
    "start": "2250470",
    "end": "2256900"
  },
  {
    "text": "Let me have a brief\nsummary of what are the best\nresults people know, roughly speaking, in\neach of this aspect.",
    "start": "2256900",
    "end": "2264970"
  },
  {
    "text": "So basically,\nfirst of all, for-- ",
    "start": "2264970",
    "end": "2271770"
  },
  {
    "text": "so let me just make\nthis a bit more formal. So I guess a little\nmore formally.",
    "start": "2271770",
    "end": "2276785"
  },
  {
    "text": " So basically, you have probably\nthree tasks in my language.",
    "start": "2276785",
    "end": "2284590"
  },
  {
    "text": "So first, you prove that--  I guess I'm repeating myself\na little bit in some sense.",
    "start": "2284590",
    "end": "2293110"
  },
  {
    "text": "So you prove that the\noptimizer converges to approximate local or\nglobal min of L hat theta.",
    "start": "2293110",
    "end": "2312020"
  },
  {
    "text": "And then in the\nsecond task, you also have to prove that\nin addition to one,",
    "start": "2312020",
    "end": "2324150"
  },
  {
    "text": "the theta hat also\nhas low complexity.",
    "start": "2324150",
    "end": "2330950"
  },
  {
    "text": " For example, something R\ntheta hat is less than C",
    "start": "2330950",
    "end": "2337600"
  },
  {
    "text": "for some complex dimension R.",
    "start": "2337600",
    "end": "2346790"
  },
  {
    "text": "And this R depends\non the algorithm, depends on even the details\nin the algorithm like learning rate, batch size,\nso and so forth.",
    "start": "2346790",
    "end": "2353850"
  },
  {
    "text": "And then task three, you\nsay that for every theta",
    "start": "2353850",
    "end": "2360050"
  },
  {
    "text": "such that R theta\nis less than C,",
    "start": "2360050",
    "end": "2367450"
  },
  {
    "text": "and maybe L hat\ntheta is close to 0-- so for every theta with low\ncomplexity and small training",
    "start": "2367450",
    "end": "2375190"
  },
  {
    "text": "error, we have the test\nerror L theta is also small.",
    "start": "2375190",
    "end": "2385875"
  },
  {
    "text": " So that's kind of\nlike the general idea.",
    "start": "2385875",
    "end": "2394340"
  },
  {
    "text": "And what people have done\nin this kind of area,",
    "start": "2394340",
    "end": "2400890"
  },
  {
    "text": "so regarding the task one, which\nis the optimization question, task one is optimization.",
    "start": "2400890",
    "end": "2407170"
  },
  {
    "text": "So I think maybe if you want to\nassociate some keyword to this, people would call the first\nquestion optimization,",
    "start": "2407170",
    "end": "2413710"
  },
  {
    "text": "and the second\nquestion people often call it as implicit\nregularization, in fact.",
    "start": "2413710",
    "end": "2421380"
  },
  {
    "text": "Yeah, probably I\nshould explain this because this is implicit because\nyou never told the algorithm",
    "start": "2421380",
    "end": "2429210"
  },
  {
    "text": "to minimize this complexity. It's implicit in the\noptimization procedure.",
    "start": "2429210",
    "end": "2435030"
  },
  {
    "text": "And it's a regularization\neffect because you get some low complexity solution.",
    "start": "2435030",
    "end": "2440130"
  },
  {
    "text": "And the third one, this\nis probably more or less the classical\noptimization bound.",
    "start": "2440130",
    "end": "2445230"
  },
  {
    "start": "2445230",
    "end": "2451040"
  },
  {
    "text": "And for task one, I\nthink what happens is that if you don't have\nregularization, so I guess--",
    "start": "2451040",
    "end": "2459740"
  },
  {
    "text": "sorry, so for task one, I think\nfor the optimization question, so one of research,\nconsider the case",
    "start": "2459740",
    "end": "2466580"
  },
  {
    "text": "where you don't have\noverparameterization.",
    "start": "2466580",
    "end": "2471680"
  },
  {
    "text": "This is overparameterization. ",
    "start": "2471680",
    "end": "2482170"
  },
  {
    "text": "Without overparameterization\nin some special case, you can still prove this\nin some special case.",
    "start": "2482170",
    "end": "2488740"
  },
  {
    "text": "For example, matrix\nfactorization problem,",
    "start": "2488740",
    "end": "2495630"
  },
  {
    "text": "maybe linearized network,\nor maybe something",
    "start": "2495630",
    "end": "2502230"
  },
  {
    "text": "like task optimization,\nyou can show that gradient descent or SGD\ncan converge to global min.",
    "start": "2502230",
    "end": "2515390"
  },
  {
    "text": " So here, linearized\nnetwork means that you don't have any activations.",
    "start": "2515390",
    "end": "2521010"
  },
  {
    "text": "Basically,\noptimization's linear, so you just stack a bunch\nof linear models, which doesn't really have any--",
    "start": "2521010",
    "end": "2526099"
  },
  {
    "text": "doesn't really do anything from\na statistical point of view. It's just purely for--",
    "start": "2526100",
    "end": "2532320"
  },
  {
    "text": "you only analyze that as an\nexercise for your technique in substance. But you can still\npublish papers in it",
    "start": "2532320",
    "end": "2538763"
  },
  {
    "text": "just because everything\nabout optimization is very complicated. Even analyzing linearized\nnetwork is difficult.",
    "start": "2538763",
    "end": "2546150"
  },
  {
    "text": "So one of the thing\nthat people have done. But you can see that\nthis doesn't really address all the issues, right? Because you don't allow\noverparameterization,",
    "start": "2546150",
    "end": "2553859"
  },
  {
    "text": "and it only works for linearized\nnetwork or matrix factorization",
    "start": "2553860",
    "end": "2560220"
  },
  {
    "text": "problem, which is\ncompletion, so and so forth. ",
    "start": "2560220",
    "end": "2566670"
  },
  {
    "text": "And recently, in the\nlast three or four years, I think, you can also do\nthis optimization question",
    "start": "2566670",
    "end": "2573600"
  },
  {
    "text": "for neural networks-- for any neural networks--",
    "start": "2573600",
    "end": "2580110"
  },
  {
    "text": "for almost any neural networks,\ndeep, shallow, so on, so forth, but with the caveat for\nspecial hyperparameters.",
    "start": "2580110",
    "end": "2593130"
  },
  {
    "text": " So special hyperparameters\nmeans something like maybe--",
    "start": "2593130",
    "end": "2601730"
  },
  {
    "text": "so first of all, you need\noverparameterization. That's actually probably\ngood because anyway,",
    "start": "2601730",
    "end": "2608490"
  },
  {
    "text": "empirically people use\noverparameterization. But the limitation\nis that you also need special learning rate\nor special initializations",
    "start": "2608490",
    "end": "2619790"
  },
  {
    "text": "and learning rate,\nso on, so forth.  And that becomes a problem.",
    "start": "2619790",
    "end": "2627890"
  },
  {
    "text": "By the way, this is typically\ncalled NTK approach, neural tangent kernel,\nwhich I'm going",
    "start": "2627890",
    "end": "2633350"
  },
  {
    "text": "to talk more in\nthe future lectures and explain why this is\ncalled neural tangent kernel.",
    "start": "2633350",
    "end": "2640440"
  },
  {
    "text": "So this is the\nso-called NTK approach. And the problem\nwith this approach is that this special\ninitialization",
    "start": "2640440",
    "end": "2647450"
  },
  {
    "text": "is a problem, and\nalso special learning rate or special algorithm. So you also have\nsomething maybe. So you need something\nabout batch size.",
    "start": "2647450",
    "end": "2655730"
  },
  {
    "text": "For example, in\nmost of the paper, the batch has to be very big. You can only analyze\ngradient design.",
    "start": "2655730",
    "end": "2661640"
  },
  {
    "text": "You cannot have stochastic\ngradient descent. So this is kind of\nlike the restriction",
    "start": "2661640",
    "end": "2669350"
  },
  {
    "text": "on the hyperparameters. At the beginning we thought,\nOK, that's not a big problem. We have these hyperparameters,\nand then next day we",
    "start": "2669350",
    "end": "2675710"
  },
  {
    "text": "probably extend them to\nother hyperparameters. But it turns out that there\nis some serious limitation",
    "start": "2675710",
    "end": "2682550"
  },
  {
    "text": "in the hyperparameters. Because as I motivate before,\neven you change the learning rate schedule-- in the figure\nwe found, right, so in this one,",
    "start": "2682550",
    "end": "2692272"
  },
  {
    "text": "this is a real experiment. Even if you change\nthe learning schedule, you change the\nperformance of your model.",
    "start": "2692272",
    "end": "2697515"
  },
  {
    "text": " So if you analyze the special\nlearning rate schedule",
    "start": "2697515",
    "end": "2703210"
  },
  {
    "text": "and you analyze especially\nin translation, then maybe you are not actually\nanalyzing anything impressive.",
    "start": "2703210",
    "end": "2711490"
  },
  {
    "text": "So for example,\nin this NTK case, I think what we can analyze,\nthe algorithm you can analyze",
    "start": "2711490",
    "end": "2718750"
  },
  {
    "text": "wouldn't give you the\nbest performance that deep learning offers. You probably get something\nlike 80% on CIFAR,",
    "start": "2718750",
    "end": "2724450"
  },
  {
    "text": "but the best algorithm\nprobably get like 95%. Of course, there are\nimprovements along this line,",
    "start": "2724450",
    "end": "2732040"
  },
  {
    "text": "but generally the issue is that\nyou make this hyperparameter so",
    "start": "2732040",
    "end": "2738580"
  },
  {
    "text": "special so that you lose the\ncorrect implicit regularization effect of the optimizers.",
    "start": "2738580",
    "end": "2746490"
  },
  {
    "text": "And you are analyzing\nan optimizer that doesn't have the correct\nimplicit regularization effect so that they\ndon't generalize",
    "start": "2746490",
    "end": "2753599"
  },
  {
    "text": "as well as the real deep\nlearning algorithms. But still, I'm going\nto talk about this because this is a very nice\nidea and in certain cases is",
    "start": "2753600",
    "end": "2762210"
  },
  {
    "text": "pretty useful. And then for the\nimplicit regularization",
    "start": "2762210",
    "end": "2770880"
  },
  {
    "text": "question, right, so the question\nabout why the optimizer prefers",
    "start": "2770880",
    "end": "2776339"
  },
  {
    "text": "certain kind of low\ncomplexity model, people have had a lot of\nresults on special cases.",
    "start": "2776340",
    "end": "2783069"
  },
  {
    "text": "So special models--\nand actually,",
    "start": "2783070",
    "end": "2788970"
  },
  {
    "text": "maybe I should call\nsimplified models-- ",
    "start": "2788970",
    "end": "2795200"
  },
  {
    "text": "I don't know why, somebody\ntook my yoga mat, yoga",
    "start": "2795200",
    "end": "2801140"
  },
  {
    "text": "brick for some reason, and\nI have to use the book. Anyway.",
    "start": "2801140",
    "end": "2806780"
  },
  {
    "text": "So special or simplified models,\nand also special optimizers.",
    "start": "2806780",
    "end": "2817270"
  },
  {
    "text": "But here, the special is\nespecially in the right way. So you're analyzing the\neffect of the optimizer.",
    "start": "2817270",
    "end": "2822820"
  },
  {
    "text": "So you focus on each aspect,\neach paper in some sense. So what are the models\nthat people have analyzed?",
    "start": "2822820",
    "end": "2829780"
  },
  {
    "text": "For example, linear regression,\nthis is something you can say--",
    "start": "2829780",
    "end": "2835060"
  },
  {
    "text": "and here you can say that\ncertain initialization prefers certain kind of models.",
    "start": "2835060",
    "end": "2842740"
  },
  {
    "text": "And you can also talk about\nlogistic linear regression,",
    "start": "2842740",
    "end": "2848300"
  },
  {
    "text": "and here, we will see that\nyou can prove something like even the model just\nwants to find the minimum--",
    "start": "2848300",
    "end": "2856490"
  },
  {
    "text": "even the model just tries to\nminimize the logistic loss actually tries to find\nthe max margin solution.",
    "start": "2856490",
    "end": "2863090"
  },
  {
    "text": "And also for matrix sensing or\nmatrix factorization problems",
    "start": "2863090",
    "end": "2872040"
  },
  {
    "text": "in a linear neural network. So you can talk about\nthis, and also, there",
    "start": "2872040",
    "end": "2877920"
  },
  {
    "text": "are special aspects\nof the optimizers. ",
    "start": "2877920",
    "end": "2885810"
  },
  {
    "text": "And sometimes there has to be\na combination of the problem and optimizer, because\ncertain optimizers wouldn't",
    "start": "2885810",
    "end": "2892250"
  },
  {
    "text": "have implicit regularization\nfor certain problems. So you can talk about the\nGD, you can talk about SGD,",
    "start": "2892250",
    "end": "2901369"
  },
  {
    "text": "and SGD, I think\nthere is actually also about the noise covariance.",
    "start": "2901370",
    "end": "2909359"
  },
  {
    "text": "Like what covariance\nwill give you the right implicit\nregularization, also the noise scale, which also matters.",
    "start": "2909360",
    "end": "2917270"
  },
  {
    "text": "And you can also talk\nabout approximate dropout. This is something you do\nin your optimizer which",
    "start": "2917270",
    "end": "2923720"
  },
  {
    "text": "will change the implicit\nbias, and you can also talk about learning rate, which\nis also actually important,",
    "start": "2923720",
    "end": "2934869"
  },
  {
    "text": "and batch size, so on so forth. ",
    "start": "2934870",
    "end": "2940369"
  },
  {
    "text": "And also there are\nunsolved open questions for tempo momentum you\nknow like factorization.",
    "start": "2940370",
    "end": "2945560"
  },
  {
    "text": "All of this has some implicit\nregularization effect. So that's why this becomes\ncomplicated, right?",
    "start": "2945560",
    "end": "2951029"
  },
  {
    "text": "So everything you do in\nyour optimizer, everything you change, would possibly have\nimplicit regularization effect.",
    "start": "2951030",
    "end": "2956830"
  },
  {
    "text": "Sometimes it's positive. Sometimes it's negative. Of course, most of the\ntricks that we have seen have positive effect\nbecause that's",
    "start": "2956830",
    "end": "2963410"
  },
  {
    "text": "why they survive and they\nare published, right? ",
    "start": "2963410",
    "end": "2969050"
  },
  {
    "text": "So that's the\nstatistical I guess. ",
    "start": "2969050",
    "end": "2975590"
  },
  {
    "text": "And I'm also going to try to\nmention a more general result that we have that me and\nsome collaborators have done.",
    "start": "2975590",
    "end": "2987609"
  },
  {
    "text": "So you can also try to have\na more general result which says something like\nSGD on L hat theta",
    "start": "2987610",
    "end": "2997099"
  },
  {
    "text": "is roughly equivalent to doing\ngradient descent on L hat theta",
    "start": "2997100",
    "end": "3002650"
  },
  {
    "text": "plus L lambda r theta for\nsome R, for some regularizer",
    "start": "3002650",
    "end": "3010500"
  },
  {
    "text": "R. This is a result\nthat we can show.",
    "start": "3010500",
    "end": "3018600"
  },
  {
    "text": "This is a much simplified--\nthe high level idea of a result that we can show.",
    "start": "3018600",
    "end": "3023610"
  },
  {
    "text": "But of course, there\nare limitations. So these kind of\nmore general results have weakness in other aspects.",
    "start": "3023610",
    "end": "3031280"
  },
  {
    "text": "For example, you may have\nadditional assumptions, or you can only deal with\ncertain stochasticity, so on, so forth.",
    "start": "3031280",
    "end": "3037980"
  },
  {
    "text": "But I think from\nthis result you can see that this is\nkind of the things that we are trying to do. So if you add stochasticity,\nthen you automatically,",
    "start": "3037980",
    "end": "3046070"
  },
  {
    "text": "implicitly you got a\nregularizer for free. Even though you are using\nstochastic gradient descent",
    "start": "3046070",
    "end": "3051589"
  },
  {
    "text": "on the original training\nloss, but somehow you get a optimizer\nfor free somewhere.",
    "start": "3051590",
    "end": "3057037"
  },
  {
    "text": " OK, so I think basically, we\nare going to talk about many",
    "start": "3057037",
    "end": "3062820"
  },
  {
    "text": "of this in the\nnext few lectures, in the future lectures. And for the task 3, for\nthe generalization bound,",
    "start": "3062820",
    "end": "3071940"
  },
  {
    "text": "this is also an interesting\nopen question for deep learning.",
    "start": "3071940",
    "end": "3078089"
  },
  {
    "text": "Because you also want to have\nprecise transition bonds that",
    "start": "3078090",
    "end": "3084000"
  },
  {
    "text": "can be compatible\nwith the regularizer you got from the\nprevious part, right? So we have said that the\noptimizer has a preference,",
    "start": "3084000",
    "end": "3091380"
  },
  {
    "text": "but does that preference leads\nto a better generalization? That's another open\nquestion, right?",
    "start": "3091380",
    "end": "3096390"
  },
  {
    "text": " So for example, you can have-- ",
    "start": "3096390",
    "end": "3103960"
  },
  {
    "text": "so one of the paper in\n2017 proved that for this,",
    "start": "3103960",
    "end": "3113890"
  },
  {
    "text": "if you use this as\nthe complexity measure where AI is the weight of ith\nlayer, so if you use this,",
    "start": "3113890",
    "end": "3129270"
  },
  {
    "text": "then you can guarantee\nyour generalization bound. That's one of the early\nresults along this line.",
    "start": "3129270",
    "end": "3135420"
  },
  {
    "text": "But the problem with\nthis is that this is not precise enough, right? This is still too big to\nbe modeling in some sense.",
    "start": "3135420",
    "end": "3142240"
  },
  {
    "text": "So you sometimes need\nmore precise optimizers.",
    "start": "3142240",
    "end": "3149730"
  },
  {
    "text": "For example, if you\ncan guarantee that-- I would talk about\nthe limitations probably when I really\ntalk about this,",
    "start": "3149730",
    "end": "3157110"
  },
  {
    "text": "but this is still\nnot precise enough. ",
    "start": "3157110",
    "end": "3163570"
  },
  {
    "text": "And you sometimes need more\nfine-grained complexity measure",
    "start": "3163570",
    "end": "3172150"
  },
  {
    "text": "that is more compatible,\nmore fine-grained. ",
    "start": "3172150",
    "end": "3179930"
  },
  {
    "text": "And also, ideally\nyou want something that is a result of\nthe optimizer, right?",
    "start": "3179930",
    "end": "3186500"
  },
  {
    "text": "So you want this\nregularizer here to be the same\nregularizer as what you had in the implicit\nrecognition effect part.",
    "start": "3186500",
    "end": "3193580"
  },
  {
    "text": " So that's the third part. ",
    "start": "3193580",
    "end": "3200980"
  },
  {
    "text": "Yeah, I think that's\nbasically a high level overview of some of the lectures\nwe're going to talk about--",
    "start": "3200980",
    "end": "3206890"
  },
  {
    "text": "some of the lectures\nin the next few weeks. And of course, there\nare other open questions in deep learning, as well.",
    "start": "3206890",
    "end": "3213200"
  },
  {
    "text": "For example, what's the role\nof the parameterization? So in these tasks, I didn't\nmention any of those,",
    "start": "3213200",
    "end": "3221619"
  },
  {
    "text": "so on, so forth. But for those kind\nof things, I don't think there's a\nsystematic study yet,",
    "start": "3221620",
    "end": "3227470"
  },
  {
    "text": "so that's why we don't talk\nabout them much for now. ",
    "start": "3227470",
    "end": "3234400"
  },
  {
    "text": "And I think for\nthe immediate plan, I'm going to talk\nabout task three here first because we are\nin this mode of proving",
    "start": "3234400",
    "end": "3241910"
  },
  {
    "text": "generalization bound. We have talked about Rademacher\ncomplexity, and all of this",
    "start": "3241910",
    "end": "3247558"
  },
  {
    "text": "depends on the\nRademacher complexity. And I'm going to talk\nabout that first, and then I'm going to move\non to the other parts.",
    "start": "3247558",
    "end": "3254950"
  },
  {
    "text": "Any questions so far?  [INAUDIBLE]",
    "start": "3254950",
    "end": "3260050"
  },
  {
    "start": "3260050",
    "end": "3265680"
  },
  {
    "text": "Sorry, I didn't\nhear the question. [INAUDIBLE] ",
    "start": "3265680",
    "end": "3278240"
  },
  {
    "text": "Yeah, I got the question. So the question is whether\nany of these results or tasks depends on the\ndata distribution?",
    "start": "3278240",
    "end": "3284819"
  },
  {
    "text": "Yes, they all depend on\ndata distribution, I think. So all of them assume some of\ndata distribution underlying.",
    "start": "3284820",
    "end": "3290520"
  },
  {
    "text": "So some of them require\nsomething stronger, some of them just require\nsome regularized connection,",
    "start": "3290520",
    "end": "3297330"
  },
  {
    "text": "but I don't think you can\ngo away without any data distribution assumption.",
    "start": "3297330",
    "end": "3303095"
  },
  {
    "text": "And some of them have very\nstrong data distribution assumptions, to be fair.",
    "start": "3303095",
    "end": "3308430"
  },
  {
    "text": "And that's actually,\nin some sense, in my opinion, that's one of\nthe technical challenge here.",
    "start": "3308430",
    "end": "3318290"
  },
  {
    "text": "It's kind of like\na subtle balance. If you assume too\nmuch about data,",
    "start": "3318290",
    "end": "3324380"
  },
  {
    "text": "then you lose the realisticness. But if you assume something\ntoo strong, then--",
    "start": "3324380",
    "end": "3331849"
  },
  {
    "text": "sorry, but if you assume\ntoo less about data, then you have some\nhardness results.",
    "start": "3331850",
    "end": "3340980"
  },
  {
    "text": "So certainly without\nany data assumption, you probably shouldn't be able\nto prove almost any results",
    "start": "3340980",
    "end": "3347420"
  },
  {
    "text": "here just because things become\nsimply hard, especially if you talk about\ncomputational procedure,",
    "start": "3347420",
    "end": "3353480"
  },
  {
    "text": "it's very easy to get\ninto NP hard instance. So we need some data\ndistribution assumption.",
    "start": "3353480",
    "end": "3360770"
  },
  {
    "text": "And another even\nmore complex question is that, how do you leverage\ndata distribution assumption?",
    "start": "3360770",
    "end": "3368180"
  },
  {
    "text": "Like we don't have\na lot of tools. So for example, if you\nassume it's Gaussian, then what you know?",
    "start": "3368180",
    "end": "3373400"
  },
  {
    "text": "You know something about what's\nthe moment, so on, so forth, right? You can do some certain\nkind of derivations.",
    "start": "3373400",
    "end": "3379010"
  },
  {
    "text": "But I don't feel like we used\neven the property of a Gaussian enough in some sense.",
    "start": "3379010",
    "end": "3385220"
  },
  {
    "text": "And let alone other kind of\ndata distribution assumption, we don't have a lot of\ngood tools to use them.",
    "start": "3385220",
    "end": "3391895"
  },
  {
    "text": "Cool.  So if there's no\nany other questions,",
    "start": "3391895",
    "end": "3403220"
  },
  {
    "text": "I'm going to move on\nto the generalization bond for neural networks. ",
    "start": "3403220",
    "end": "3412619"
  },
  {
    "text": "And you can see that\nthis is still roughly in the kind of mindset\nof the classical setting. The only difference\nis that we are looking",
    "start": "3412620",
    "end": "3419040"
  },
  {
    "text": "for proper complex measures,\nnot only a dimension dependency, but something sometimes\nmore complicated.",
    "start": "3419040",
    "end": "3426446"
  },
  {
    "start": "3426446",
    "end": "3434640"
  },
  {
    "text": "And you will see that\nthis part is really a direct extension of what we\nhave done in the last three weeks, because the\ntools are shared",
    "start": "3434640",
    "end": "3440190"
  },
  {
    "text": "and it's really just that\nyou need better tools. ",
    "start": "3440190",
    "end": "3448800"
  },
  {
    "text": "All right, so now let's talk\nabout the particular setup that we can do. So we're going to\nstart with two layers--",
    "start": "3448800",
    "end": "3455498"
  },
  {
    "text": " two neural networks. And then in the\nnext few lectures,",
    "start": "3455498",
    "end": "3462039"
  },
  {
    "text": "we're going to move\non to multiple layers. And for two layers let's\nuse the following notation.",
    "start": "3462040",
    "end": "3467386"
  },
  {
    "text": " So let's say your parameter\ntheta contains of two parts.",
    "start": "3467386",
    "end": "3473930"
  },
  {
    "text": "One part is w, and\nthe other part is u. So w is the second layer,\nand u is the first layer.",
    "start": "3473930",
    "end": "3484090"
  },
  {
    "text": "So basically, on\nthe network of theta x will be something like\nw transpose phi of ux",
    "start": "3484090",
    "end": "3491170"
  },
  {
    "text": "where u is a matrix that maps\ndimension d to dimension m",
    "start": "3491170",
    "end": "3496750"
  },
  {
    "text": "where m is the\nnumber of neurons. So basically, ux will\nbe m dimensional.",
    "start": "3496750",
    "end": "3501860"
  },
  {
    "text": "So this will be m dimensional,\nand you apply an element wise",
    "start": "3501860",
    "end": "3509080"
  },
  {
    "text": "ReLU function. So phi is element\nwise ReLU function.",
    "start": "3509080",
    "end": "3516480"
  },
  {
    "text": " So phi of a vector z1 up to\nzn is equal to, basically",
    "start": "3516480",
    "end": "3527040"
  },
  {
    "text": "you apply the elementwise. You get max z1 0 to max zm0.",
    "start": "3527040",
    "end": "3534770"
  },
  {
    "text": " So after you apply phi, you\nget an additional vector.",
    "start": "3534770",
    "end": "3539815"
  },
  {
    "text": "and you inner product with\nw, you get a single scalar. So we have a model that\noutputs a single scalar using",
    "start": "3539815",
    "end": "3547780"
  },
  {
    "text": "these two layers, u and w. And again, we still call xi\nyi, the training data set,",
    "start": "3547780",
    "end": "3560194"
  },
  {
    "text": "as usual. OK. ",
    "start": "3560195",
    "end": "3566580"
  },
  {
    "text": "So our goal is that first to\nshow a Rademacher complexity",
    "start": "3566580",
    "end": "3574210"
  },
  {
    "text": "bound, and then we also\ntalk about how this RC",
    "start": "3574210",
    "end": "3582030"
  },
  {
    "text": "bound is relevant to practice.",
    "start": "3582030",
    "end": "3587175"
  },
  {
    "text": " And I think for the day\nwe probably wouldn't even",
    "start": "3587175",
    "end": "3593990"
  },
  {
    "text": "be able to finish number\none because I'm going to have actually two bounds. One is better than the other.",
    "start": "3593990",
    "end": "3601480"
  },
  {
    "text": "So here is a theorem for a\nRademacher complexity bound. ",
    "start": "3601480",
    "end": "3609960"
  },
  {
    "text": "So the theorem is that--  so suppose you have a\nhypothesis class that",
    "start": "3609960",
    "end": "3618370"
  },
  {
    "text": "consists of models look like\nthis, parameterized by theta, where you require\nthat number of w",
    "start": "3618370",
    "end": "3625570"
  },
  {
    "text": "is less than Bw and norm\nof ui is bound by Bu.",
    "start": "3625570",
    "end": "3632500"
  },
  {
    "text": "I guess I didn't\ndefine ui, so this is-- let me say this. So u is this matrix\nof m by d matrix,",
    "start": "3632500",
    "end": "3641860"
  },
  {
    "text": "and let's say each of\nthe rows is u1 transpose",
    "start": "3641860",
    "end": "3648220"
  },
  {
    "text": "up to um transpose. ",
    "start": "3648220",
    "end": "3653869"
  },
  {
    "text": "So each ui is of dimension\nd, and so that's why u times",
    "start": "3653870",
    "end": "3660250"
  },
  {
    "text": "x is really in the\nproduct of this ui with x. ",
    "start": "3660250",
    "end": "3666858"
  },
  {
    "text": "That's the notation\nI'm going to use. OK? So basically, ui's are\nrows of the weight matrix.",
    "start": "3666858",
    "end": "3676329"
  },
  {
    "text": "So we restricted the w, and\nthe norm of w, and norm of ui to something like Bw\nand Bu, and then we",
    "start": "3676330",
    "end": "3683960"
  },
  {
    "text": "also assume something about-- ",
    "start": "3683960",
    "end": "3693789"
  },
  {
    "text": "the data has expected 2\nnorm square less than C. I guess actually this\nis probably C square.",
    "start": "3693790",
    "end": "3701099"
  },
  {
    "text": "Have a typo here. And then under all\nof these assumptions,",
    "start": "3701100",
    "end": "3709440"
  },
  {
    "text": "you can prove the Rademacher\ncomplexity bound Rn of H",
    "start": "3709440",
    "end": "3715500"
  },
  {
    "text": "is less than 2\ntimes Bw Bu times C",
    "start": "3715500",
    "end": "3722000"
  },
  {
    "text": "times square root m\nover square root n. ",
    "start": "3722000",
    "end": "3728190"
  },
  {
    "text": "So I guess just a remark\nis that this is not",
    "start": "3728190",
    "end": "3734000"
  },
  {
    "text": "ideal bound, not a good\nbound, because m shows up",
    "start": "3734000",
    "end": "3744083"
  },
  {
    "text": "in the bound.  And actually, it shows\nup in a wrong way because it says that if\nyou have more neurons",
    "start": "3744083",
    "end": "3752010"
  },
  {
    "text": "you have a worse bound. So the m shows up in the\nmore classical kind of sense",
    "start": "3752010",
    "end": "3757320"
  },
  {
    "text": "where you have more neurons,\nyou have more complex models, then it's not great.",
    "start": "3757320",
    "end": "3763313"
  },
  {
    "text": "So basically, you cannot\nuse this theorem to explain the size of deep learning or\nthe overparameterized model because this is saying\noverparameterized model",
    "start": "3763313",
    "end": "3769800"
  },
  {
    "text": "we'll have bigger\nRademacher complexity. But you want a\nbound that is better",
    "start": "3769800",
    "end": "3777880"
  },
  {
    "text": "when m goes to\ninfinity in some sense to explain the plot that\nI kind of showed here.",
    "start": "3777880",
    "end": "3784720"
  },
  {
    "text": "So as m goes to infinity, you\nwant a better and better bound, in some sense. But this one gives you\na worse and worse bound.",
    "start": "3784720",
    "end": "3792100"
  },
  {
    "text": "But it nevertheless\nlets you prove this because this is\nkind of like a warm up for what we'll show next.",
    "start": "3792100",
    "end": "3800100"
  },
  {
    "text": "[INAUDIBLE] ",
    "start": "3800100",
    "end": "3810799"
  },
  {
    "text": "I see. So maybe let me rephrase the\nquestion first, make sure. So the question, if my\nunderstanding is correct,",
    "start": "3810800",
    "end": "3822340"
  },
  {
    "text": "your question is that\nwhy you're expecting this right one is going to 0\nis decreasing forever, right,",
    "start": "3822340",
    "end": "3830450"
  },
  {
    "text": "instead of really going up\nafter a certain point, right? It's just we don't have\nenough data points, right?",
    "start": "3830450",
    "end": "3837650"
  },
  {
    "text": "Like we didn't run that very\nsuper large scale experiment. I think the answer is that\nwe do think this is already",
    "start": "3837650",
    "end": "3845960"
  },
  {
    "text": "large enough for us to kind\nof believe that it will never go up like this because\n4k neurons for this task",
    "start": "3845960",
    "end": "3851880"
  },
  {
    "text": "is really, really a lot. Like 64 already allowed\nyou to memorize. Typically, you wouldn't\neven run so many.",
    "start": "3851880",
    "end": "3858530"
  },
  {
    "text": "You probably just-- maybe it\nwould be easier to convince you if I show you 4 up to 108.",
    "start": "3858530",
    "end": "3867400"
  },
  {
    "text": "And you will see\nsomething like this, and then you ask\nme the question. I will show you 108 up to 4k.",
    "start": "3867400",
    "end": "3873790"
  },
  {
    "text": "You probably would\nbe more convinced. Yeah, but 4k is already\npretty large, I think.",
    "start": "3873790",
    "end": "3879790"
  },
  {
    "text": " But of course, you can never\nrule out the possibility",
    "start": "3879790",
    "end": "3885490"
  },
  {
    "text": "that after maybe a million\nneurons it goes up. It just sounds unlikely.",
    "start": "3885490",
    "end": "3893078"
  },
  {
    "text": "[INAUDIBLE] ",
    "start": "3893078",
    "end": "3920500"
  },
  {
    "text": "So I guess I think the\nintention of the question was that whether this\nbound really is growing",
    "start": "3920500",
    "end": "3928330"
  },
  {
    "text": "as m goes to infinity, right? So because both Bw and\nBu could depend on m,",
    "start": "3928330",
    "end": "3935380"
  },
  {
    "text": "and maybe they depend\non m in different ways. Maybe Bw increase as\nm goes to infinity, and Bu probably decrease\nas m goes to infinity.",
    "start": "3935380",
    "end": "3943490"
  },
  {
    "text": "So that's definitely\na possibility, right? So I think the thing here\nis I'm choosing the scaling",
    "start": "3943490",
    "end": "3949480"
  },
  {
    "text": "so that it's at least arguably\nfine to think of Bw and Bu",
    "start": "3949480",
    "end": "3957650"
  },
  {
    "text": "to be constant. So why? The reason is that this is\nprobably a little vague.",
    "start": "3957650",
    "end": "3965510"
  },
  {
    "text": "So the ui is the contribution\nof each component, right? ",
    "start": "3965510",
    "end": "3977040"
  },
  {
    "text": "But w is the contribution\nof all the components. So in some sense, you are\nsaying that the top layer, you control the contribution\nfrom all the components.",
    "start": "3977040",
    "end": "3984465"
  },
  {
    "text": "And you want to say that\nthat's the constant. You don't want that to grow as\nm goes to infinity because-- ",
    "start": "3984465",
    "end": "3992010"
  },
  {
    "text": "so basically, maybe one\nway to think about this is the following. So if you think about the\nscale, the scale over here",
    "start": "3992010",
    "end": "3997650"
  },
  {
    "text": "does make some sense. Because ui is on the\norder of, let's say, a constant, and ui transposed\nis at least a constant that",
    "start": "3997650",
    "end": "4005770"
  },
  {
    "text": "doesn't depend on m, right? So ui doesn't depend\non m, and ui transpose x doesn't depend on m.",
    "start": "4005770",
    "end": "4012340"
  },
  {
    "text": "Here, I'm writing\nthis a little bit--  so here, theta could probably\nhave some dependency on B.",
    "start": "4012340",
    "end": "4019420"
  },
  {
    "text": "We only have a dependency\non m, let's say So ui's are out of\nconstant, and then you have sum of wi phi of\nui transpose x, right?",
    "start": "4019420",
    "end": "4030349"
  },
  {
    "text": "So each of this term is\non order of constant, and your wi, the total\ncontribution is constant.",
    "start": "4030350",
    "end": "4036110"
  },
  {
    "text": "So that's why the total\nthing you can somewhat believe that this is on\nthe order of constant. Because it's not like each of\nthe wi's on order of constant.",
    "start": "4036110",
    "end": "4043720"
  },
  {
    "text": "It's that the sum of\nthe squares of them is on order of constant. So in some sense,\nyou can believe that this whole thing\nis on order of constant,",
    "start": "4043720",
    "end": "4053020"
  },
  {
    "text": "especially if you have-- I guess it depends on\nhow you think about this.",
    "start": "4053020",
    "end": "4058080"
  },
  {
    "text": "So if you replace wi by\n1 over square root m, this is actually--",
    "start": "4058080",
    "end": "4064076"
  },
  {
    "start": "4064076",
    "end": "4070190"
  },
  {
    "text": "I guess depending on how you\napproximate this, roughly speaking, if you\nuse Cauchy-Schwarz you're going to\napproximate by something",
    "start": "4070190",
    "end": "4076880"
  },
  {
    "text": "like sum of wi squared, or\n1/2 times sum of ui transpose x squared.",
    "start": "4076880",
    "end": "4084360"
  },
  {
    "text": "Uh-huh.  And this is something-- ",
    "start": "4084360",
    "end": "4091560"
  },
  {
    "text": "let me see. Why this is on\norder of constant? Maybe we're actually even\nmore generous than that.",
    "start": "4091560",
    "end": "4098120"
  },
  {
    "text": "So the L2 norm of\nw is a constant,",
    "start": "4098120",
    "end": "4117470"
  },
  {
    "text": "but I think you can still make\nthis bigger if all of them are correlated, right?",
    "start": "4117470",
    "end": "4122710"
  },
  {
    "start": "4122710",
    "end": "4139031"
  },
  {
    "text": "OK, so I think this is-- I'm pretty sure the answer-- I should have\nanswered, but I'm not--",
    "start": "4139032",
    "end": "4144729"
  },
  {
    "text": "I don't see I have a\nconvincing answer right now. So maybe we can discuss\noffline for a few minutes. Yeah.",
    "start": "4144729",
    "end": "4150339"
  },
  {
    "text": "But I think the\nscaling is chosen to be at least reasonably correct.",
    "start": "4150340",
    "end": "4155560"
  },
  {
    "text": "Of course, you can\nstill argue certain-- there's always a--\nfor example, depending on how w correlates\nwith the squares,",
    "start": "4155560",
    "end": "4162759"
  },
  {
    "text": "there's always some\nkind of flexibility. But I think the scaling\nis relatively OK.",
    "start": "4162760",
    "end": "4168520"
  },
  {
    "text": " Anyway, but it's is a very\ngood question because you",
    "start": "4168520",
    "end": "4173979"
  },
  {
    "text": "can have misleading\nresults if you are not very careful about the scale. ",
    "start": "4173979",
    "end": "4179564"
  },
  {
    "text": "OK. So let's see. OK. So maybe-- I have 15 minutes.",
    "start": "4179564",
    "end": "4186060"
  },
  {
    "text": "I think I can prove the\ntheorem in 15 minutes. ",
    "start": "4186060",
    "end": "4198889"
  },
  {
    "text": "So what we do is that we use\nthe definition of the Rademacher complexity and gradually\npeel of the sup,",
    "start": "4198890",
    "end": "4207050"
  },
  {
    "text": "so like we did before, right? We have a sup, and we have\nto somehow get rid of it.",
    "start": "4207050",
    "end": "4212438"
  },
  {
    "text": "[INAUDIBLE] ",
    "start": "4212438",
    "end": "4238360"
  },
  {
    "text": "But difference by square root\nof m, so that's why I got-- I was thinking to\nuse the argument,",
    "start": "4238360",
    "end": "4245120"
  },
  {
    "text": "but I think it's not\ngoing to be right. Because sum of wi phi\nof ui transpose x,",
    "start": "4245120",
    "end": "4250670"
  },
  {
    "text": "I think the most\npessimistic bound, right, would be something\nlike this is less",
    "start": "4250670",
    "end": "4256409"
  },
  {
    "text": "than if you replace each of the\nwi by 1 over square root of m, and each of these\nbear a constant, you're going to\nhave sum of this.",
    "start": "4256410",
    "end": "4262889"
  },
  {
    "text": "And this will be\nsquare root of m. So then in some\nsense, this is not",
    "start": "4262890",
    "end": "4268400"
  },
  {
    "text": "helping me to justify\nto use this scaling. Right. ",
    "start": "4268400",
    "end": "4274429"
  },
  {
    "text": "But if you believe that\nthere's some consolation, so suppose you believe that\nthere's a consolation here,",
    "start": "4274430",
    "end": "4280730"
  },
  {
    "text": "then this would be\nsomething on the order of 1.",
    "start": "4280730",
    "end": "4287250"
  },
  {
    "text": "So basically, if\nyou want to-- so if you believe in\nthere is a consolation, then it's a reasonable scaling.",
    "start": "4287250",
    "end": "4293580"
  },
  {
    "text": "Or in other words,\nsuppose you want to make the scaling\neven smaller, right? Suppose you want to say\nthat I'm going to believe",
    "start": "4293580",
    "end": "4299910"
  },
  {
    "text": "Bw is even smaller\nthan the scaling I give, or Bu is\neven smaller, then",
    "start": "4299910",
    "end": "4305670"
  },
  {
    "text": "you have to assume there's\na strong correlation in your model. Otherwise, your model wouldn't\neven [INAUDIBLE] with something",
    "start": "4305670",
    "end": "4312940"
  },
  {
    "text": "on order of 1 So whether you are\nwilling to do that, so for example, suppose I\ntell you that this actually",
    "start": "4312940",
    "end": "4321038"
  },
  {
    "text": "experience things, right? So then I have to\nconvince you that I can choose Bw to be on\nthe order of maybe 1,",
    "start": "4321038",
    "end": "4330310"
  },
  {
    "text": "and then ui to be on order\nof 1 over square root of m. And then this wouldn't go--",
    "start": "4330310",
    "end": "4336460"
  },
  {
    "text": "the bond, indeed, would not\ngrow as m goes to infinity,",
    "start": "4336460",
    "end": "4341570"
  },
  {
    "text": "but you will find that sum of\nBi and phi of ui transpose x, it's very difficult to be big.",
    "start": "4341570",
    "end": "4348055"
  },
  {
    "text": "You have to match up\neverything to make it big enough to fit that label.",
    "start": "4348055",
    "end": "4353350"
  },
  {
    "text": "So would you be\nwilling to do that? I think you can arguably say\nthat's not really realistic.",
    "start": "4353350",
    "end": "4359739"
  },
  {
    "text": " OK, cool.",
    "start": "4359740",
    "end": "4364772"
  },
  {
    "text": "So I guess let's prove this.",
    "start": "4364772",
    "end": "4370730"
  },
  {
    "text": "So the proof, as I\nsaid, we're going to try to remove the sup in our\ndefinition of the Rademacher",
    "start": "4370730",
    "end": "4375940"
  },
  {
    "text": "complexity step by step. So first of all,\nlet's define v to be",
    "start": "4375940",
    "end": "4384850"
  },
  {
    "text": "the post-activation\nintermediate layer. Let's define phi\nto be u times x,",
    "start": "4384850",
    "end": "4390530"
  },
  {
    "text": "which is a m-dimensional\nvector, and you can also correspondingly define vi to\nbe the corresponding activation",
    "start": "4390530",
    "end": "4402890"
  },
  {
    "text": "for the ith example. This is in m dimension. And then using this notation,\nthe Rademacher complexity",
    "start": "4402890",
    "end": "4410281"
  },
  {
    "text": "on the empirical sum of the\nempirical Rademacher complexity is you expectation. The randomness is from sigma.",
    "start": "4410282",
    "end": "4416940"
  },
  {
    "text": "And you take sup. You have sum of sigma\ni transposed times here",
    "start": "4416940",
    "end": "4425850"
  },
  {
    "text": "to write f theta of xi. But f theta of xi I'm going\nto rewrite it as w transpose",
    "start": "4425850",
    "end": "4433520"
  },
  {
    "text": "vi just because\nthat's the notation.",
    "start": "4433520",
    "end": "4441150"
  },
  {
    "text": "So let me just\nreplace this here. w transpose vi.",
    "start": "4441150",
    "end": "4447440"
  },
  {
    "text": "This is f theta of xi. And then here we take sup over\ntwo things, over both w and u.",
    "start": "4447440",
    "end": "4454250"
  },
  {
    "text": "And the dependency\non u is heading vi. And let's clean up this to\nput the 1 over n in front,",
    "start": "4454250",
    "end": "4461869"
  },
  {
    "text": "and you have sup over u and sup\nover w, w transposed times 1",
    "start": "4461870",
    "end": "4471370"
  },
  {
    "text": "over n times sum of sigma i vi.",
    "start": "4471370",
    "end": "4476470"
  },
  {
    "text": "I guess this probably\nlooks familiar for you because we did something\nlike this in the linear case,",
    "start": "4476470",
    "end": "4481840"
  },
  {
    "text": "as well. And then you can get rid of the\nw, but you still have the u.",
    "start": "4481840",
    "end": "4490024"
  },
  {
    "text": "So you sup over u, and\nyou get rid of the w. w has L2 norm bound,\nso it's less than Bw.",
    "start": "4490024",
    "end": "4498830"
  },
  {
    "text": "So you get the sup of this is\nequal to Bw times the L2 norm",
    "start": "4498830",
    "end": "4508120"
  },
  {
    "text": "of sigma i vi. ",
    "start": "4508120",
    "end": "4516390"
  },
  {
    "text": "So now we've got rid of the vw. We can put vw in front.",
    "start": "4516390",
    "end": "4521900"
  },
  {
    "text": "And now let's deal with the u,\nand the u is something like--",
    "start": "4521900",
    "end": "4529520"
  },
  {
    "text": " I think I shouldn't have\n1 over n here anymore. My bad.",
    "start": "4529520",
    "end": "4534880"
  },
  {
    "start": "4534880",
    "end": "4540750"
  },
  {
    "text": "So now this is i. This is a sum from 1 to n.",
    "start": "4540750",
    "end": "4548270"
  },
  {
    "text": "And as we write this, let's plug\nin back the definition of vi,",
    "start": "4548270",
    "end": "4554360"
  },
  {
    "text": "which is phi of u xi.  And what I'm going\nto do is that I'm",
    "start": "4554360",
    "end": "4562900"
  },
  {
    "text": "going to do kind of like a-- if you get familiar\nwith this, you can see that this is a\nvery loose way to do this.",
    "start": "4562900",
    "end": "4570430"
  },
  {
    "text": "I'm going to replace the\n2 norm by infinite norm. So I'm going to say that this\nis less than square root m",
    "start": "4570430",
    "end": "4579770"
  },
  {
    "text": "times infinite norm of this. ",
    "start": "4579770",
    "end": "4589680"
  },
  {
    "text": "This is just because-- and then vector v 2 norm is\nless than square root m times",
    "start": "4589680",
    "end": "4595969"
  },
  {
    "text": "the infinite norm.  And so if v is in\nm dimensional, OK?",
    "start": "4595970",
    "end": "4605260"
  },
  {
    "text": " And now the reason why I want\nto replace it by infinite norm",
    "start": "4605260",
    "end": "4612055"
  },
  {
    "text": "can be seen later,\ncan be seen now because somehow with infinite\nnorm I can simplify the sup.",
    "start": "4612055",
    "end": "4622350"
  },
  {
    "text": "So now I have a sup,\nand note that what this vector is, this vector--",
    "start": "4622350",
    "end": "4628949"
  },
  {
    "text": "maybe let's do something here. So this vector is the sum of\na bunch of vectors, right?",
    "start": "4628950",
    "end": "4637680"
  },
  {
    "text": "The infinity norm is\nabove the dimension, the coordinates of this vector. So basically, each\nof these dimensions",
    "start": "4637680",
    "end": "4646590"
  },
  {
    "text": "is E relay something like\nsum of sigma i phi uj xi.",
    "start": "4646590",
    "end": "4655820"
  },
  {
    "text": "And the sum is over i. This is the jth\ndimension of this vector.",
    "start": "4655820",
    "end": "4662520"
  },
  {
    "text": "So basically, I can take\nsup over j, and sup over u,",
    "start": "4662520",
    "end": "4671650"
  },
  {
    "text": "and sum of sigma i phi uj\ntranspose xi and for 1.",
    "start": "4671650",
    "end": "4680580"
  },
  {
    "text": " And I can actually\nalso write uj here because if I take\nthe sup over j,",
    "start": "4680580",
    "end": "4687699"
  },
  {
    "text": "the jth actually\nonly depends on uj. ",
    "start": "4687700",
    "end": "4695140"
  },
  {
    "text": "And that's actually\nkind of the main reason why we want to use this\ninfinity norm because once you write this, you found that all\nthe j's are equivalent, right?",
    "start": "4695140",
    "end": "4705960"
  },
  {
    "text": "Like anyway you are\ntaking sup, right, so it doesn't matter whether\nit's uj, u1, u2, right,",
    "start": "4705960",
    "end": "4711579"
  },
  {
    "text": "so the sup is the same. So this is equal to-- ",
    "start": "4711580",
    "end": "4717020"
  },
  {
    "text": "[INAUDIBLE] Is this an equality?",
    "start": "4717020",
    "end": "4723220"
  },
  {
    "text": "Oh sorry, this is an inequality. So sup over u, a\nsingle vector u.",
    "start": "4723220",
    "end": "4734770"
  },
  {
    "text": "So you replaced uj by\nu, and you say that this needs to less than Bu.",
    "start": "4734770",
    "end": "4741490"
  },
  {
    "text": "Because uj, you used\nto have a bump Bu. Let's just skip\nit for simplicity.",
    "start": "4741490",
    "end": "4747610"
  },
  {
    "text": "And then you can write this\nas i phi of u transpose xi.",
    "start": "4747610",
    "end": "4756060"
  },
  {
    "text": " In some sense, you\nremove the m dependency",
    "start": "4756060",
    "end": "4762159"
  },
  {
    "text": "because for infinity norm,\nhow many m's don't matter. ",
    "start": "4762160",
    "end": "4768652"
  },
  {
    "text": "And now there is one\nstep where I'm going to remove the absolute value.",
    "start": "4768652",
    "end": "4777195"
  },
  {
    "text": "Because if you don't\nhave the absolute value, it's kind of like-- let's first remove it. So by removing it,\nwill pay a factor 2,",
    "start": "4777195",
    "end": "4785920"
  },
  {
    "text": "and this requires something\nthat is not exactly trivial, but I will not prove it\nin the interest of time.",
    "start": "4785920",
    "end": "4795210"
  },
  {
    "text": "So you can remove\nthis absolute value. And the reason-- it's\nin the lecture notes.",
    "start": "4795210",
    "end": "4801700"
  },
  {
    "text": "The reason it's actually,\nfundamentally, it's pretty simple. It's just basically\nbecause the sup is actually",
    "start": "4801700",
    "end": "4808910"
  },
  {
    "text": "mostly positive, like\nalmost always positive. Because you can choose the\nu it's always positive.",
    "start": "4808910",
    "end": "4815898"
  },
  {
    "text": "With or without absolute\nvalue, it doesn't really matter, at least for this case. Because you are\ntaking the sup, right?",
    "start": "4815898",
    "end": "4823429"
  },
  {
    "text": "So anyway, it's\ngoing to be positive. Because you can choose u to\nmake this quantity positive.",
    "start": "4823430",
    "end": "4829380"
  },
  {
    "text": "So this is what I-- I will ask you to\nrefer to the lecture",
    "start": "4829380",
    "end": "4835310"
  },
  {
    "text": "notes for the formal proof. And then now, after\nremoving the absolute value,",
    "start": "4835310",
    "end": "4841580"
  },
  {
    "text": "you can see that this is\nsomething like a Rademacher complexity of something simple.",
    "start": "4841580",
    "end": "4848830"
  },
  {
    "text": "Because you can view this as\nyour function now, and this is the Rademacher complexity\nof this kind of function.",
    "start": "4848830",
    "end": "4856840"
  },
  {
    "text": "But still, you have\nphi and u, right? So that's why we are going to\nuse the Lipschitz composition.",
    "start": "4856840",
    "end": "4864340"
  },
  {
    "text": "So this will be less than 2.",
    "start": "4864340",
    "end": "4869440"
  },
  {
    "text": "You copy all the constants. ",
    "start": "4869440",
    "end": "4884660"
  },
  {
    "text": "So this is by the Lipschitz-- ",
    "start": "4884660",
    "end": "4891287"
  },
  {
    "text": "I think the Lipschitz\ncomposition or the Talagrand lemma. ",
    "start": "4891287",
    "end": "4896600"
  },
  {
    "text": "Talagrand lemma. So I think in some\nsense, you think of--",
    "start": "4896600",
    "end": "4903110"
  },
  {
    "text": "maybe you can define\nsomething like, I guess, maybe H prime to be\nthe family of u transpose x.",
    "start": "4903110",
    "end": "4911830"
  },
  {
    "start": "4911830",
    "end": "4918820"
  },
  {
    "text": "And then you can also look\nat phi of H constant composed with H prime.",
    "start": "4918820",
    "end": "4924250"
  },
  {
    "text": "So the Rademacher of phi\ncomposed with H prime is going to be equal to-- it's this quantity, right?",
    "start": "4924250",
    "end": "4930460"
  },
  {
    "text": "And this is less than\nthe Lipschitzness of phi, which is 1, ReLU\nand then you get H prime,",
    "start": "4930460",
    "end": "4936492"
  },
  {
    "text": "which is this quantity.  So that's how we do it.",
    "start": "4936492",
    "end": "4943070"
  },
  {
    "text": "And now it becomes linear. So u transpose xi is a\nlinear function class, and I think we have\ndone this before.",
    "start": "4943070",
    "end": "4952260"
  },
  {
    "text": "So for L2 norm\nconstant linear class, I think you can get something\nlike this is less than 2 square",
    "start": "4952260",
    "end": "4960600"
  },
  {
    "text": "root m Bu over n times--",
    "start": "4960600",
    "end": "4971320"
  },
  {
    "text": "so this is phi w times Bu times\nsquare root sum of xi 2 norm",
    "start": "4971320",
    "end": "4980300"
  },
  {
    "text": "squared. This is just by what we\nhad for the linear model.",
    "start": "4980300",
    "end": "4985885"
  },
  {
    "text": " For the loss of quality,\nyou didn't put [INAUDIBLE]..",
    "start": "4985885",
    "end": "4993880"
  },
  {
    "text": "Oh, sure. Yeah, sorry. My bad. ",
    "start": "4993880",
    "end": "5003070"
  },
  {
    "text": "Where the 2 come from? So the 2 come from\nhere, this line.",
    "start": "5003070",
    "end": "5009579"
  },
  {
    "text": "And this is something I\ndidn't explain, right, so when you remove the absolute value. So how you get they are exactly\nthe same without losing a 2,",
    "start": "5009580",
    "end": "5021195"
  },
  {
    "text": "is that the question? ",
    "start": "5021195",
    "end": "5031590"
  },
  {
    "text": "I suspect it's possible,\nbut I'm not 100% sure. So the proof in the\nlecture notes does lose 2,",
    "start": "5031590",
    "end": "5039510"
  },
  {
    "text": "but it sounds like\nit's possible, right, you can save that factor 2.",
    "start": "5039510",
    "end": "5045990"
  },
  {
    "text": "Because the intuition\nI had doesn't really tell you why you should\nlose anything, right? So my intuition is that\nthis quality is just",
    "start": "5045990",
    "end": "5051840"
  },
  {
    "text": "always positive. So the x value doesn't matter. So that intuition didn't tell\nyou why you should lose the 2.",
    "start": "5051840",
    "end": "5058424"
  },
  {
    "text": "Well, I will improve\nthe things, I think-- at least the proof I figure\nout, I read from the book.",
    "start": "5058424",
    "end": "5064719"
  },
  {
    "text": "Maybe I figured out\nmyself, I lose the 2, so maybe it's because I didn't\ndo exactly the right thing,",
    "start": "5064720",
    "end": "5070400"
  },
  {
    "text": "exactly the right thing. ",
    "start": "5070400",
    "end": "5075679"
  },
  {
    "text": "OK, so I may-- ",
    "start": "5075680",
    "end": "5080730"
  },
  {
    "text": "and then the very\nlast step, you can take the expectation of\nthe empirical Rademacher complexity, and this is\nthe expansion over s.",
    "start": "5080730",
    "end": "5090389"
  },
  {
    "text": "Then you just get like\nwhat we did before. So the expectancy of this is\nless than C times square root",
    "start": "5090390",
    "end": "5097423"
  },
  {
    "text": "of n, so you got this is\nbounded by 2 squared m phi w Bu",
    "start": "5097423",
    "end": "5102969"
  },
  {
    "text": "times C over square root of 2n. That's because you use the\nCauchy-Schwarz for this part.",
    "start": "5102970",
    "end": "5108650"
  },
  {
    "text": "This is exactly the\nsame as what we have done for the linear models. ",
    "start": "5108650",
    "end": "5114570"
  },
  {
    "text": "OK, so I guess this is a\nnatural stopping point, and next time we're going to\nhave a bound that somewhat",
    "start": "5114570",
    "end": "5122400"
  },
  {
    "text": "improved on this so\nthat you don't have the explicit dependency on m.",
    "start": "5122400",
    "end": "5127980"
  },
  {
    "text": "Any questions? ",
    "start": "5127980",
    "end": "5133680"
  },
  {
    "text": "OK, I guess I'll see\nyou on Wednesday. Sounds good. ",
    "start": "5133680",
    "end": "5143000"
  }
]