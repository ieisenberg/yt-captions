[
  {
    "start": "0",
    "end": "5500"
  },
  {
    "text": "All right Hi, everyone. Welcome to CS330.",
    "start": "5500",
    "end": "10750"
  },
  {
    "text": "And today we'll be talking\nabout hierarchical RL and skill discovery. These are quite exciting topics.",
    "start": "10750",
    "end": "18140"
  },
  {
    "text": "Some of them are a\nlittle bit out there. So please ask questions\nif something is unclear.",
    "start": "18140",
    "end": "24850"
  },
  {
    "text": "But first, a few reminders. So the homework 4, the\noptional one, is due on Monday.",
    "start": "24850",
    "end": "31820"
  },
  {
    "text": "And then on Wednesday, we\nhave the project milestone that is due as well. And then one more announcement.",
    "start": "31820",
    "end": "38500"
  },
  {
    "text": "Next week we'll have\ntwo guest lectures, and these should\nbe really exciting. So we'll have Colin\nRaffel here, who we'll",
    "start": "38500",
    "end": "45370"
  },
  {
    "text": "talk about big language models. This will be a virtual lecture,\nso you will attend on Zoom.",
    "start": "45370",
    "end": "50890"
  },
  {
    "text": "And it will start at\nnoon as opposed to 11:30. So he'll probably\ntalk a little bit",
    "start": "50890",
    "end": "57370"
  },
  {
    "text": "about how meta-learning\njust emerges when you train on\nbig enough data sets especially with language.",
    "start": "57370",
    "end": "63280"
  },
  {
    "text": "So it could be\nreally interesting. And then on Wednesday we'll\nhave Jascha Sohl-Dickstein,",
    "start": "63280",
    "end": "70240"
  },
  {
    "text": "who will talk about\nlearning optimizers, which is another way of\napproaching meta-learning.",
    "start": "70240",
    "end": "76570"
  },
  {
    "text": "And he's done a lot\nof work on the topic. So it will be-- I think it will be very\ninteresting to see--",
    "start": "76570",
    "end": "82690"
  },
  {
    "text": "to hear what he has to say. And this one will happen in\nperson, at the Standard Time.",
    "start": "82690",
    "end": "91200"
  },
  {
    "text": "All right. So let's do a little recall. So we've been talking\nabout reinforcement",
    "start": "91200",
    "end": "96690"
  },
  {
    "text": "learning for a little bit,\nfor actually quite some time right now. And we have these different\nkinds of reinforcement",
    "start": "96690",
    "end": "104095"
  },
  {
    "text": "learning that we discussed. There is the online\nreinforcement learning or on policy\nreinforcement learning.",
    "start": "104095",
    "end": "109500"
  },
  {
    "text": "Then we have off-policy\nreinforcement learning, where we gather\nsome experiences. We put them in the\nbuffer, and then we",
    "start": "109500",
    "end": "115620"
  },
  {
    "text": "have certain set\nof algorithms that can deal with data like this. And then in the last\nlecture on Monday,",
    "start": "115620",
    "end": "122280"
  },
  {
    "text": "we discussed offline\nreinforcement learning, where we don't get to\ninteract in the world. We can just access the data that\nwas already collected for us,",
    "start": "122280",
    "end": "131026"
  },
  {
    "text": "and then we have to learn\nthe best possible policy given the data set. ",
    "start": "131027",
    "end": "136799"
  },
  {
    "text": "We also talked about\na few different tasks throughout the lectures\non reinforcement learning. For instance, we\ntalked about grasping",
    "start": "136800",
    "end": "143580"
  },
  {
    "text": "in the single task case\nwhen we discussed QT-Opt. We talked about different\nsimple manipulation skills",
    "start": "143580",
    "end": "150750"
  },
  {
    "text": "when we talked about\nMT-Opt, such as pushing, grasping specific objects,\nplacing, rearranging, and so",
    "start": "150750",
    "end": "157170"
  },
  {
    "text": "on. And then we also talked\nabout goal conditioned reinforcement learning, where\na task is specified by a goal.",
    "start": "157170",
    "end": "164880"
  },
  {
    "text": "And this is where we talk\nabout actionable models, where we would present a\ngoal image that we want",
    "start": "164880",
    "end": "170879"
  },
  {
    "text": "the robot to achieve,\nand then the robot will try to get to that goal. As close to that goal\nimage as possible. ",
    "start": "170880",
    "end": "177820"
  },
  {
    "text": "So far, we knew what\nwe wanted, right? We had some kind\nof task in mind. It was specified\nin different ways,",
    "start": "177820",
    "end": "183980"
  },
  {
    "text": "sometimes as a one-hot\nvector, sometimes as a language instruction,\nsometimes as a goal image, but we knew what the task is.",
    "start": "183980",
    "end": "189505"
  },
  {
    "text": " If you look at all\nof these tasks here,",
    "start": "189505",
    "end": "194770"
  },
  {
    "text": "they are rather short\nhorizon behaviors, right? So maybe they span\ntens of time steps,",
    "start": "194770",
    "end": "200200"
  },
  {
    "text": "maybe hundreds of time steps,\nbut they're relatively short simple manipulation skills.",
    "start": "200200",
    "end": "206330"
  },
  {
    "text": "And then lastly, we\nhad well-defined tasks and well defined rewards. So this is kind of equivalent\nto knowing what we wanted.",
    "start": "206330",
    "end": "213710"
  },
  {
    "text": "We knew that we wanted to\noptimize for certain tasks, and then we could just go\nahead and optimize for them,",
    "start": "213710",
    "end": "219462"
  },
  {
    "text": "and then we would\ntest on those tasks.  So today we'll talk\nabout something",
    "start": "219462",
    "end": "226410"
  },
  {
    "text": "a little bit\ndifferent, something where we want to maybe\ndiscover interesting behaviors.",
    "start": "226410",
    "end": "232497"
  },
  {
    "text": "We want to have agents that can\ndiscover interesting behaviors on their own, right? So we want to specify the task.",
    "start": "232497",
    "end": "238560"
  },
  {
    "text": "We'll try to avoid\nspecifying tasks. And instead we would just\nwant to have an agent that we can just deploy\nin an environment",
    "start": "238560",
    "end": "244950"
  },
  {
    "text": "and have it find out\neverything that there is to find out about\nthat environment, and come up with its own tasks.",
    "start": "244950",
    "end": "252170"
  },
  {
    "text": "So one reason to do this is-- it's also part of how we learn.",
    "start": "252170",
    "end": "257810"
  },
  {
    "text": "So here is a little time lapse\nof a baby left unattended, and it does all kinds\nof different things",
    "start": "257810",
    "end": "263740"
  },
  {
    "text": "without being explicitly\ntold to do so. And these seem like some\ntargeted exploration behaviors.",
    "start": "263740",
    "end": "269139"
  },
  {
    "text": "So just it's curious\nabout the world and it explores, and\ntries a whole bunch of different things. And it comes up\nwith its own tasks.",
    "start": "269140",
    "end": "275229"
  },
  {
    "text": "We didn't have to\nsit down and specify what is the task that\nwe want the baby to do.",
    "start": "275230",
    "end": "281250"
  },
  {
    "text": "In addition to this, there\nhas been some research in biology, some\nresearch on frogs,",
    "start": "281250",
    "end": "287580"
  },
  {
    "text": "showing that there is actually\na small number of core skills that the spinal cord of the\nfrog uses to control all",
    "start": "287580",
    "end": "296062"
  },
  {
    "text": "of the movements of the frog. So there is just a\nhandful of skills, and then the spinal\ncord can modulate",
    "start": "296062",
    "end": "301950"
  },
  {
    "text": "to create a whole\nvariety of behaviors that we'd experience when\nwe watch frogs, right? So there's just a\nhandful of them.",
    "start": "301950",
    "end": "308550"
  },
  {
    "text": "And they span this whole\nvariety that we get to observe.",
    "start": "308550",
    "end": "313680"
  },
  {
    "text": "There is also a similar research\ndone on human subjects that drew a similar conclusion, where\nthey invited human subjects,",
    "start": "313680",
    "end": "320940"
  },
  {
    "text": "and they ask them to\ngrasp different objects. And then they record it using,\nI think, motion capture system.",
    "start": "320940",
    "end": "327810"
  },
  {
    "text": "They recorded all kinds\nof different grasps, how they were performing the tasks. And then they collected\nthat data set,",
    "start": "327810",
    "end": "334770"
  },
  {
    "text": "and then they run principal\ncomponent analysis on it. And they found out that most of\nthe behaviors can be understood",
    "start": "334770",
    "end": "342240"
  },
  {
    "text": "or can be described\nby just using a few top principal components.",
    "start": "342240",
    "end": "348480"
  },
  {
    "text": "So it seems like it's a similar\nconclusion where maybe there's just a few core skills\nthat we need to learn. And once we know\nabout those skills,",
    "start": "348480",
    "end": "354750"
  },
  {
    "text": "and they are well\nparameterized, then we can use those\nskills to emerge all the kind of behaviors that\nwe might want in the world.",
    "start": "354750",
    "end": "363000"
  },
  {
    "text": "Now, we could try to think of\nwhat these skills should be, right? We could try to prescribe\nthem and kind of through trial",
    "start": "363000",
    "end": "371430"
  },
  {
    "text": "and error, find out if these\nare the right ones or not. But a much more\nnatural way would be to have the agents\nleave them unattended,",
    "start": "371430",
    "end": "377550"
  },
  {
    "text": "and have them explore\nthe entire world, and try all kinds\nof different things that they might\nbe interested in. And then we can take that\nand then find the core skills",
    "start": "377550",
    "end": "386580"
  },
  {
    "text": "that span that space. ",
    "start": "386580",
    "end": "391703"
  },
  {
    "text": "All right, there is also a\nmore practical motivation for skill discovery algorithms. So we talked a\nlittle bit about this",
    "start": "391703",
    "end": "398430"
  },
  {
    "text": "during actionable models,\nthat actually coming up with tasks themselves can\nbe quite tricky, all right?",
    "start": "398430",
    "end": "406439"
  },
  {
    "text": "So actually, let's run\nthis little exercise. So let's think of a tabletop\nmanipulation scenario.",
    "start": "406440",
    "end": "413260"
  },
  {
    "text": "So we have a Single Arm robot\nthat is in front of a table. It has limited workspace. It's just a single arm.",
    "start": "413260",
    "end": "419340"
  },
  {
    "text": "It has a parallel jaw gripper. So it just has two fingers\nthat can just grasp like that.",
    "start": "419340",
    "end": "425879"
  },
  {
    "text": "And can you just shout,\nwhat are the different tasks that you can come up with? What can this robot do?",
    "start": "425880",
    "end": "432450"
  },
  {
    "text": "Any ideas? Just shout whatever\ncomes to mind. Yeah. It can stack. Stack, OK.",
    "start": "432450",
    "end": "439050"
  },
  {
    "text": "Taking place. Yep. Push and pull. Push and pull, OK. Slide.",
    "start": "439050",
    "end": "444240"
  },
  {
    "text": "Sorry, what is it? Slide. Like slide. Yeah, slide. Mhm. Kind of similar to push and\npull, maybe a little different.",
    "start": "444240",
    "end": "451650"
  },
  {
    "text": "Rotate things. Rotate things, OK. ",
    "start": "451650",
    "end": "457561"
  },
  {
    "text": "Drop them from an\nelevated position. Drop them. So you'll drop them\nfrom the table. But then you would need to pick\nthem up from the floor again.",
    "start": "457561",
    "end": "465543"
  },
  {
    "text": "Yeah, assuming that\nyou can reach there. Yep, that's a good one. Yeah. Manipulate some people who--",
    "start": "465543",
    "end": "470600"
  },
  {
    "text": "objects, like open or\nclose some ropes or-",
    "start": "470600",
    "end": "475700"
  },
  {
    "text": "Manipulation of\ndeformable objects. So pull a rope or put the rope\nin a certain configuration",
    "start": "475700",
    "end": "480980"
  },
  {
    "text": "or something like that. Mm-hmm. Or use a cloth or\nsomething like that. Any other ideas?",
    "start": "480980",
    "end": "487665"
  },
  {
    "text": "Yeah. Fold something. Fold something, yeah. Also using maybe\ndeformable objects.",
    "start": "487665",
    "end": "492810"
  },
  {
    "text": "Cool. Awesome. Yeah, these are some\nreally good ideas. So altogether maybe we came\nup with 20 tasks or so.",
    "start": "492810",
    "end": "501020"
  },
  {
    "text": "And what if we want to have\nrobots that can do anything that you can imagine, like\nthousands of thousands",
    "start": "501020",
    "end": "507500"
  },
  {
    "text": "of tasks. But if they are presented\nwith different objects, and with some scenarios that\nwe haven't thought of before.",
    "start": "507500",
    "end": "514849"
  },
  {
    "text": "And actually coming up with all\nthe different possible tasks, and we want to come up with\nas many tasks as possible",
    "start": "514850",
    "end": "520370"
  },
  {
    "text": "because the more tasks we have,\nthe better generalizations we will have to new tasks. This is quite tricky.",
    "start": "520370",
    "end": "527000"
  },
  {
    "text": "So we run for this exercise\nwhen designing this benchmark called meta-world, and\nwe are able to come up",
    "start": "527000",
    "end": "532160"
  },
  {
    "text": "with 50 different\ntasks, and some of them are very similar to the\nones that you mentioned. But there is also\nadditional considerations",
    "start": "532160",
    "end": "538670"
  },
  {
    "text": "that you should think\nabout when thinking of a benchmark like this. So for instance tasks need to be\nat similar level of difficulty",
    "start": "538670",
    "end": "545490"
  },
  {
    "text": "so that there is\nno one task that is way, way harder than\nsome of the other tasks. They should also be able--",
    "start": "545490",
    "end": "550790"
  },
  {
    "text": "they should also\nbe accomplishable within a similar time\nhorizon so that you don't have this imbalance\nduring training as well.",
    "start": "550790",
    "end": "557930"
  },
  {
    "text": "They also need to be-- every single task\nneeds to be solvable using standard single task\nreinforcement learning",
    "start": "557930",
    "end": "563180"
  },
  {
    "text": "algorithm, and sometimes\nit's quite tricky to get that to work. So there's a lot of\ndifferent constraints.",
    "start": "563180",
    "end": "569480"
  },
  {
    "text": "We are able to come\nup with 50 of them, but it is actually quite\npainful to do that. ",
    "start": "569480",
    "end": "576390"
  },
  {
    "text": "So one reason why we would want\nto do a skill discovery instead is, what if we could just drop\na robot in that environment",
    "start": "576390",
    "end": "582810"
  },
  {
    "text": "and spawn a bunch of\ndifferent objects. Maybe randomize them\nfrom time to time and have the robot\nfigure out on its own all the kinds of\ndifferent things",
    "start": "582810",
    "end": "588810"
  },
  {
    "text": "that it can do in\nthis environment, and then we can just\nname them afterwards. ",
    "start": "588810",
    "end": "595740"
  },
  {
    "text": "So we might need some\nkind of skill discovery to discover these behaviors. And today we'll also talk about\nhierarchical reinforcement",
    "start": "595740",
    "end": "602610"
  },
  {
    "text": "learning. So why hierarchies and\nwhy hierarchical RL? So we are pretty good\nat performing tests",
    "start": "602610",
    "end": "610450"
  },
  {
    "text": "at various levels of\nabstractions, we as humans. So for instance, I can\ngive you an instruction",
    "start": "610450",
    "end": "615700"
  },
  {
    "text": "to bake a cheesecake, and\nyou wouldn't necessarily think about how to contract your\nmuscles to bake a cheesecake.",
    "start": "615700",
    "end": "622720"
  },
  {
    "text": "You can think about on much\nhigher level of abstraction. So if you were to plan the\nfirst step, you can say, well,",
    "start": "622720",
    "end": "628630"
  },
  {
    "text": "first I need to buy\nthe ingredients. And then you can\ngo a level lower, and then you can say,\nwell, first step for this",
    "start": "628630",
    "end": "634339"
  },
  {
    "text": "is I need to go to the store. First step for this is I\nneed to walk to the door.",
    "start": "634340",
    "end": "640439"
  },
  {
    "text": "I need to take a\nstep to do that. And I need to contract a certain\nmuscle to take that step.",
    "start": "640440",
    "end": "645840"
  },
  {
    "text": "And we can plan on all\nthese different levels. And if we were to\nplan some long horizon",
    "start": "645840",
    "end": "651360"
  },
  {
    "text": "tasks like bake\ncheesecake, on the lowest level of abstraction, it would\nbe an extremely long plan that we wouldn't be able\nto even think about.",
    "start": "651360",
    "end": "658110"
  },
  {
    "text": " So we want to do\nsomething similar for our artificial agents.",
    "start": "658110",
    "end": "664080"
  },
  {
    "text": "We want them to\nbe able to reason about different levels\nof abstractions, and different time\nhorizons so that they",
    "start": "664080",
    "end": "669510"
  },
  {
    "text": "can plan for longer things. And then in addition\nto this, it seems",
    "start": "669510",
    "end": "674880"
  },
  {
    "text": "that these hierarchies\nand different levels of abstractions are really\nhelpful for exploration.",
    "start": "674880",
    "end": "680080"
  },
  {
    "text": "So here is a video of a baby\ntrying to accomplish a task. And it's not really\nclear what the task is.",
    "start": "680080",
    "end": "686857"
  },
  {
    "text": "But it's just trying\ndifferent things. And at this point it\nstacked this block, and then it looks for\napproval, whether this",
    "start": "686857",
    "end": "693270"
  },
  {
    "text": "was the right task. So in this case, it's\ndoing exploration in this very high level\nof well, I probably",
    "start": "693270",
    "end": "699717"
  },
  {
    "text": "need to pick some objects. I probably need to\nplace them somewhere. These are kind of high\nabstraction skills.",
    "start": "699717",
    "end": "705390"
  },
  {
    "text": "It's not really doing\nexploration on the motor babbling level where\nit's just like moving its hands in all kinds\nof different directions,",
    "start": "705390",
    "end": "711870"
  },
  {
    "text": "and looking for approval. So if we could have\nsomething similar, if we could have this much\nmore targeted exploration",
    "start": "711870",
    "end": "718380"
  },
  {
    "text": "that explores through\nhigher level concepts, we'll probably have better\nexploration algorithms.",
    "start": "718380",
    "end": "727139"
  },
  {
    "text": "All right, cool. So the plan for today\nis we'll talk first about information\ntheoretic concepts,",
    "start": "727140",
    "end": "733230"
  },
  {
    "text": "and we've talked about\na lot of them already. So this might be mostly\na review for most of you,",
    "start": "733230",
    "end": "738300"
  },
  {
    "text": "but we'll just make sure that\neveryone is on the same page there. Then we'll talk about\nskill discovery algorithms.",
    "start": "738300",
    "end": "746073"
  },
  {
    "text": "And then we'll talk about how we\ncan use the skills that we just discovered. And then at the\nvery end, we'll also",
    "start": "746073",
    "end": "751530"
  },
  {
    "text": "talk about hierarchical\nreinforcement learning. So let's start with\ninformation theoretic concepts.",
    "start": "751530",
    "end": "759110"
  },
  {
    "text": "So we'll start with an entropy. But before we talk\nabout entropy,",
    "start": "759110",
    "end": "764860"
  },
  {
    "text": "we have to talk about\nthe distribution. And hopefully, at this point,\nmost of you are familiar",
    "start": "764860",
    "end": "770560"
  },
  {
    "text": "or all of you are familiar\nwith what the distribution is. So we can plot it. Here on the x-axis is\none-dimensional random variable",
    "start": "770560",
    "end": "777910"
  },
  {
    "text": "x. And the y-axis is\nthe probability. And we can look at\nwhat kind of values",
    "start": "777910",
    "end": "784300"
  },
  {
    "text": "this random variable takes. Plot them on the\nx-axis and then try to fit the distribution to it.",
    "start": "784300",
    "end": "789830"
  },
  {
    "text": "For instance, a Gaussian that\nshows the probability density of that.",
    "start": "789830",
    "end": "794920"
  },
  {
    "text": "So now let's think about\nwhat an entropy is. It's an entropy\nof a distribution, and it's written with\nthis script H symbol.",
    "start": "794920",
    "end": "804390"
  },
  {
    "text": "And the formula, if you were\nto look it up, looks like this.",
    "start": "804390",
    "end": "810260"
  },
  {
    "text": "So let's think a little bit\nabout what it actually is. So we have a minus\nsign, and then we",
    "start": "810260",
    "end": "815959"
  },
  {
    "text": "have a log probability,\nlog p of x. Under the x-- with the\nexpectation under its own,",
    "start": "815960",
    "end": "824790"
  },
  {
    "text": "right? Under its own distribution. So what does that mean?",
    "start": "824790",
    "end": "829970"
  },
  {
    "text": "So let's think a little bit. Let's think about how can we\nmake entropy very, very small,",
    "start": "829970",
    "end": "837527"
  },
  {
    "text": "right? So how can we take this whole\nterm and make it very small. So for this term to be\nsmall, then this term",
    "start": "837527",
    "end": "844360"
  },
  {
    "text": "without the minus\nneeds to be big. So what does that mean?",
    "start": "844360",
    "end": "850310"
  },
  {
    "text": "That means that we\nwould need to sample-- it is not right.",
    "start": "850310",
    "end": "856600"
  },
  {
    "text": "Yeah, so we want the\nfull entropy to be small. So then this whole\nthing is to be small. So then this thing\nhas to be big.",
    "start": "856600",
    "end": "863080"
  },
  {
    "text": "Actually, let's switch that. Let's make the entropy big. OK. So let's try to see what would\nit take to make this term big.",
    "start": "863080",
    "end": "868840"
  },
  {
    "text": "So this whole thing\nneeds to be big. So then this part\nneeds to be small. So for this part to be small,\nwe would sample something",
    "start": "868840",
    "end": "875889"
  },
  {
    "text": "from our distribution. So we will sample\nfrom our distribution. And then the log\nprobability of that sample",
    "start": "875890",
    "end": "881612"
  },
  {
    "text": "needs to be small, all right?  So what that means is that\nI just sampled something,",
    "start": "881612",
    "end": "888565"
  },
  {
    "text": "but the probability of\nme sampling that thing is actually quite low. So I just had by chance\nsampled something",
    "start": "888565",
    "end": "894700"
  },
  {
    "text": "that I wasn't actually\nthat likely to sample. So what that means\nis that basically,",
    "start": "894700",
    "end": "900550"
  },
  {
    "text": "all of the samples that\nI'm going to sample are not going to be very likely. ",
    "start": "900550",
    "end": "907260"
  },
  {
    "text": "In other words, the\nentropy controls how broad our distribution is. If our distribution was\nuniform random, right?",
    "start": "907260",
    "end": "913770"
  },
  {
    "text": "Like if the probability of\nsampling any potential value was the same, then if I\nsampled that particular value,",
    "start": "913770",
    "end": "920475"
  },
  {
    "text": "the log probability of that\nvalue will be very small. So the more random\nthe distribution is,",
    "start": "920475",
    "end": "927570"
  },
  {
    "text": "the higher the entropy. ",
    "start": "927570",
    "end": "933550"
  },
  {
    "text": "So in this case, it\nwill tell us how wide this Gaussian right here is.",
    "start": "933550",
    "end": "939780"
  },
  {
    "text": "How broad it is. Yeah. Would the log probability\nbe true to Gaussian uniform?",
    "start": "939780",
    "end": "946560"
  },
  {
    "text": "Because there would\nbe one-- let's say, you have like a uniform-- it'd just be constant. Like, it'd be small, but it\nshould be by unit [INAUDIBLE]..",
    "start": "946560",
    "end": "956990"
  },
  {
    "text": "So, yeah. So the question was for\nthe uniform distribution, wouldn't the log\nprobability be very large? So if it's a uniform\ndistribution or let's",
    "start": "956990",
    "end": "963230"
  },
  {
    "text": "imagine a Gaussian that\nis really, really broad. Yeah. Right? Then the log-- then\nthe probability--",
    "start": "963230",
    "end": "968959"
  },
  {
    "text": "the log probability\nfor each sample would be relatively\nsmall, right? We will spread this peaky\nGaussian across the x-axis.",
    "start": "968960",
    "end": "977399"
  },
  {
    "text": "So it will be kind of small\nhere, small here, small here, small here. Well, wouldn't the\nlog [INAUDIBLE]??",
    "start": "977400",
    "end": "984350"
  },
  {
    "text": "No, logs does the same. Log doesn't change the-- its monotonic function.",
    "start": "984350",
    "end": "990954"
  },
  {
    "text": "Then if so, then do you have a\nsmall probability [INAUDIBLE].. But if we take the log with a\nsmall probability, [INAUDIBLE]??",
    "start": "990954",
    "end": "998125"
  },
  {
    "text": "So it would be something\nsuper negative [INAUDIBLE]",
    "start": "998125",
    "end": "1003820"
  },
  {
    "text": "So log doesn't change\nthe value of the--",
    "start": "1003820",
    "end": "1009790"
  },
  {
    "text": "so basically, the\ncorresponding-- the big values will stay big, though it\nwill just squish them.",
    "start": "1009790",
    "end": "1015262"
  },
  {
    "text": "The smaller values\nwill stay small. ",
    "start": "1015262",
    "end": "1021009"
  },
  {
    "text": "OK. So let's think of another\nexample of a Bernoulli",
    "start": "1021010",
    "end": "1026980"
  },
  {
    "text": "random variable. So let's think of\nit in the case where we are just flipping a coin. And we can set the probability\nthat the Bernoulli--",
    "start": "1026980",
    "end": "1034630"
  },
  {
    "text": "the parameter of the\nBernoulli distribution. So the probability\nof which we'll be getting heads or tails.",
    "start": "1034630",
    "end": "1042459"
  },
  {
    "text": "And that parameter is\nset here on the x-axis. So if it's 0.5, that means\nwe have an unbiased coin,",
    "start": "1042460",
    "end": "1050380"
  },
  {
    "text": "and it's as likely to get a\ntail as it is to get heads. Now, if we were to plot\nthe entropy of that,",
    "start": "1050380",
    "end": "1056560"
  },
  {
    "text": "of the distribution\ndependent on the parameter, it would look\nsomething like that.",
    "start": "1056560",
    "end": "1062210"
  },
  {
    "text": "So it basically says\nthat the entropy will be the highest when\nthe coin is very unbiased.",
    "start": "1062210",
    "end": "1070419"
  },
  {
    "text": "So when it's kind of the\nuniform probability where we don't really know\nwhat's going to happen.",
    "start": "1070420",
    "end": "1075820"
  },
  {
    "text": "The sample is basically-- the sample will carry a lot\nof information in other words.",
    "start": "1075820",
    "end": "1081799"
  },
  {
    "text": "So the other way\nyou can look at it is that if the entropy--\nif the parameter is really high, for\ninstance, 1 or 0.",
    "start": "1081800",
    "end": "1087820"
  },
  {
    "text": "And if you were to\ndo an experiment, if you were to take a coin\nthat always shows heads,",
    "start": "1087820",
    "end": "1093550"
  },
  {
    "text": "then the entropy\nof the distribution will be very, very small. What that means is that\nthis experiment is not going to reveal much more\nnew information to you.",
    "start": "1093550",
    "end": "1100940"
  },
  {
    "text": "You already know what's\ngoing to happen, right? If you have a coin that\nalways shows you heads, then you don't really\nhave to toss that coin.",
    "start": "1100940",
    "end": "1108309"
  },
  {
    "text": "Just you know that\nit's going to be heads. There is not much information\nin that experiment. But if you have\nan unbiased coin,",
    "start": "1108310",
    "end": "1115570"
  },
  {
    "text": "then throwing--\nthen tossing a coin will actually give you\na lot of information. So there's a lot of value in\nperforming that experiment,",
    "start": "1115570",
    "end": "1123768"
  },
  {
    "text": "and that corresponds\nto high entropy. ",
    "start": "1123768",
    "end": "1128970"
  },
  {
    "text": "Let's talk about\nanother concept, which is KL divergence. So KL divergence\nwe talked about it",
    "start": "1128970",
    "end": "1136710"
  },
  {
    "text": "a little bit during our--\nespecially our offline RL lecture. And we talked about it as\nthe distance between two",
    "start": "1136710",
    "end": "1143790"
  },
  {
    "text": "different distributions. So let's define that a\nlittle bit more strictly. Let's define it mathematically.",
    "start": "1143790",
    "end": "1149830"
  },
  {
    "text": "So the definition\nof the KL divergence between two distributions,\nq and p is defined as this.",
    "start": "1149830",
    "end": "1155440"
  },
  {
    "text": "It's defined as the expectation\nunder the first distribution q of the log of the ratio\nof the two distributions.",
    "start": "1155440",
    "end": "1162487"
  },
  {
    "text": "Then you can see that\nthis is not symmetric. The order matters and we'll\ntalk about that in a second.",
    "start": "1162487",
    "end": "1168289"
  },
  {
    "text": "So we can write it out,\nbecause the log of the ratio is the difference of the logs. So we can write\nit out like this.",
    "start": "1168290",
    "end": "1175450"
  },
  {
    "text": "And then given that we already\nknow what the entropy is, we can take this\nterm because this",
    "start": "1175450",
    "end": "1181000"
  },
  {
    "text": "is the log probability\nunder its own expectation, and it's just\nmissing a minus sign.",
    "start": "1181000",
    "end": "1188120"
  },
  {
    "text": "So we can say that this\nis a negative entropy. So I just switched it here. And this is the\nnegative entropy term.",
    "start": "1188120",
    "end": "1193250"
  },
  {
    "text": "And this term is the same\nterm that we have right here. So this is a distance metric.",
    "start": "1193250",
    "end": "1199669"
  },
  {
    "text": "So let's first see what happens\nif both of these distributions are exactly the same. So we're in q equals p.",
    "start": "1199670",
    "end": "1207120"
  },
  {
    "text": "So if q equals p, or p equals\nq, then we would have this term.",
    "start": "1207120",
    "end": "1214080"
  },
  {
    "text": "We just have q. q is in here. This term would have\njust q's in there, and there will be the\ntwo exact same terms.",
    "start": "1214080",
    "end": "1220110"
  },
  {
    "text": "We'll subtract 1 from the other. The distance will be 0. So KL divergence between\ntwo distributions",
    "start": "1220110",
    "end": "1226679"
  },
  {
    "text": "that are exactly the same is 0. So that seems to make sense. That seems like a\ndecent distance metric.",
    "start": "1226680",
    "end": "1232865"
  },
  {
    "text": "But let's think\na little bit more about what it actually\nmeans, all right? So let's do the\nfollowing experiment.",
    "start": "1232865",
    "end": "1240809"
  },
  {
    "text": "We will have some\ndistribution that is given to us, that\nis a little bit funky. This is the distribution\np of x, and I plotted it",
    "start": "1240810",
    "end": "1246809"
  },
  {
    "text": "right here, all right? So this is what it looks like. And now we'll try to\nfind another distribution",
    "start": "1246810",
    "end": "1253590"
  },
  {
    "text": "q, which is a Gaussian. So we are restricted\nto this Gaussian family",
    "start": "1253590",
    "end": "1259140"
  },
  {
    "text": "that minimizes the KL divergence\nbetween q and p, all right? So we are trying to find\nthe distribution q that",
    "start": "1259140",
    "end": "1266250"
  },
  {
    "text": "is a Gaussian that\nwill minimize the KL divergence to this\ndistribution, all right? ",
    "start": "1266250",
    "end": "1272690"
  },
  {
    "text": "So first, let's take a look\nat this term right here. So this says we are trying\nto minimize the whole thing.",
    "start": "1272690",
    "end": "1281640"
  },
  {
    "text": "So we are trying to\nmaximize this term. And this term says that\nunder the expectation of q--",
    "start": "1281640",
    "end": "1289520"
  },
  {
    "text": "so I'll be sampling from q-- the log probability of\np of x should be high. ",
    "start": "1289520",
    "end": "1296830"
  },
  {
    "text": "So that's how we\ncan minimize that. So that means that we can\nfind some kind of Gaussian.",
    "start": "1296830",
    "end": "1303820"
  },
  {
    "text": "So that if we sample\nfrom that Gaussian, the log probability of p of\nx, of this funky distribution,",
    "start": "1303820",
    "end": "1308920"
  },
  {
    "text": "will be as high as possible. So one Gaussian that\nfits that description",
    "start": "1308920",
    "end": "1314200"
  },
  {
    "text": "is a very peaky\nGaussian right here that covers the same mode as p of x.",
    "start": "1314200",
    "end": "1322075"
  },
  {
    "text": "Does that make sense? So because if we sample\nanything from that Gaussian, the log p of x--",
    "start": "1322075",
    "end": "1328190"
  },
  {
    "text": "log p of x. So the probability of this\ndistribution, of these samples",
    "start": "1328190",
    "end": "1333380"
  },
  {
    "text": "here will be very, very high.  So this is the first term.",
    "start": "1333380",
    "end": "1340419"
  },
  {
    "text": "And now we have the second\nterm, which is the entropy term. So we are minimizing\nthe whole thing.",
    "start": "1340420",
    "end": "1346110"
  },
  {
    "text": "So that means we are\nmaximizing the entropy. And to maximize the\nentropy of that Gaussian, we already know\nwhat we need to do.",
    "start": "1346110",
    "end": "1351510"
  },
  {
    "text": "We need to make it broader.  So in other words to minimize--\nto find the q distribution",
    "start": "1351510",
    "end": "1359420"
  },
  {
    "text": "that is a Gaussian. Find the parameters\nof the Gaussian that minimizes the KL divergence\nbetween q and p, which",
    "start": "1359420",
    "end": "1366140"
  },
  {
    "text": "is this funky distribution. We'll end up with a\nGaussian that covers the mode of this distribution.",
    "start": "1366140",
    "end": "1373920"
  },
  {
    "text": "So I have one question\nto you, which is, what would happen if\nwe change the order?",
    "start": "1373920",
    "end": "1379480"
  },
  {
    "text": "What would happen if\nwe were optimizing KL divergence between p\nand q, and not q and p?",
    "start": "1379480",
    "end": "1386592"
  },
  {
    "text": "Yeah. Question. When you're trying to minimize\nthat [INAUDIBLE] or maximize",
    "start": "1386592",
    "end": "1393129"
  },
  {
    "text": "the expected log probability\nof p of x under q of x,",
    "start": "1393130",
    "end": "1399240"
  },
  {
    "text": "why is the result a peaky\nGaussian because it wouldn't-- entropy or [INAUDIBLE] 0 is the\nexactly match the distribution.",
    "start": "1399240",
    "end": "1410886"
  },
  {
    "text": "If this- OK. Yeah, that's a great question. So why is this first Gaussian\nthat optimizes only this term,",
    "start": "1410886",
    "end": "1418649"
  },
  {
    "text": "tries to maximize this\nlittle term right here? Why is it so peaky? And wouldn't we have\nthe best KL divergence",
    "start": "1418650",
    "end": "1424860"
  },
  {
    "text": "if we just matched this\ndistribution exactly? Yeah, so these are two\nseparate questions. So let's first answer\nthe second one.",
    "start": "1424860",
    "end": "1431850"
  },
  {
    "text": "So why can't we just\nfind a distribution that is exactly the same? That's because we are\nrestricting ourselves to just a Gaussian family.",
    "start": "1431850",
    "end": "1438030"
  },
  {
    "text": "So the only thing we assume\nthat we can fit is a Gaussian. And so this is kind\nof the best Gaussian",
    "start": "1438030",
    "end": "1444179"
  },
  {
    "text": "we can find that minimizes\nthe KL divergence. Now, regarding the second\nquestion, why is it so narrow",
    "start": "1444180",
    "end": "1450900"
  },
  {
    "text": "and it's so peaky here? Is because if you put any\nprobability mass anywhere else, like if we make it a\nlittle bit broader,",
    "start": "1450900",
    "end": "1457590"
  },
  {
    "text": "then this term would be\na little bit smaller. Then if you sample\nfrom the Gaussian, the log probability of that\nwill be a little bit lower.",
    "start": "1457590",
    "end": "1463559"
  },
  {
    "text": "You'll be a little bit\nlower on that x-axis here. So the best thing you can do is\nfind the most narrow Gaussian",
    "start": "1463560",
    "end": "1470130"
  },
  {
    "text": "that finds the mode\nof the distribution. And then if you sample from that\nGaussian the log probability",
    "start": "1470130",
    "end": "1475559"
  },
  {
    "text": "your y value here\nwill be very high. So if people [INAUDIBLE]\nany distribution then",
    "start": "1475560",
    "end": "1484596"
  },
  {
    "text": "would the minimums\nbe the exact match? Like-- Right. So then the question is,\nif somehow we were not",
    "start": "1484596",
    "end": "1490768"
  },
  {
    "text": "restricted to a Gaussian, but\nwe could model any distribution somehow, would we\nminimize the KL divergence",
    "start": "1490768",
    "end": "1495865"
  },
  {
    "text": "with the exact\nsame distribution? Yes. So the KL divergence would be 0\nif its exact same distribution.",
    "start": "1495865",
    "end": "1501059"
  },
  {
    "text": "Yes. There's one more question. Is this linked to\ninformation gain at all?",
    "start": "1501060",
    "end": "1507050"
  },
  {
    "text": "I know that entropy-- when you're\nsubtracting entropies, we have this concept of\ninformation gain [INAUDIBLE]..",
    "start": "1507050",
    "end": "1515576"
  },
  {
    "text": "Is this trying to\nmaximize the [INAUDIBLE]?? Yeah, so all of the terms that\nwe'll be discussing today.",
    "start": "1515576",
    "end": "1522269"
  },
  {
    "text": "So entropy, KL divergence,\nmutual information, information gain, are closely related.",
    "start": "1522270",
    "end": "1528770"
  },
  {
    "text": "And there is close relationships\nbetween all of them. We'll talk about some\nof them in a second. So that will hopefully\nresolve some of that.",
    "start": "1528770",
    "end": "1537049"
  },
  {
    "text": "All right, cool. So we had this other\nquestion, which",
    "start": "1537050",
    "end": "1542830"
  },
  {
    "text": "is, what would happen\nif we change that order? So right now we have this\nmode seeking behavior, and that's actually\nhow people refer to it.",
    "start": "1542830",
    "end": "1549320"
  },
  {
    "text": "So we'll try to find a\nGaussian distribution that covers the mode of the p of x. So what would happen\nif we change the order?",
    "start": "1549320",
    "end": "1555190"
  },
  {
    "text": "Yes. So the [INAUDIBLE] I think we\ncan do p of x at the bottom when q of x [INAUDIBLE]\nit's not 0 when p of x is 0.",
    "start": "1555190",
    "end": "1565870"
  },
  {
    "text": "So in the other case q of x is\nnot allowed to be 0 when p of x",
    "start": "1565870",
    "end": "1571590"
  },
  {
    "text": "is not 0. So it's going to\nspread over them? That's right, yeah. So the q of x is not allowed\nto be 0 if p of x is 0.",
    "start": "1571590",
    "end": "1577450"
  },
  {
    "text": "So basically, it\nwill try to cover the entire support of p of x. So it will be a Gaussian\nthat goes like this.",
    "start": "1577450",
    "end": "1585460"
  },
  {
    "text": "That's right. And we refer to this\nKL divergence as a-- sorry, this one\nwas mode seeking.",
    "start": "1585460",
    "end": "1591970"
  },
  {
    "text": "And this one is mode covering. So it tries to\ncover all the modes. All right, cool.",
    "start": "1591970",
    "end": "1598330"
  },
  {
    "text": "So one more term. And that's mutual information. We haven't talked much about it. So let's do it now.",
    "start": "1598330",
    "end": "1604899"
  },
  {
    "text": "So mutual information is\ndenoted with the script I, and it's mutual information\nbetween two random variables.",
    "start": "1604900",
    "end": "1613090"
  },
  {
    "text": "And it's defined as following,\nas the KL divergence between the joint distribution\nof these two variables",
    "start": "1613090",
    "end": "1619059"
  },
  {
    "text": "and the product of\ntheir marginals.",
    "start": "1619060",
    "end": "1624720"
  },
  {
    "text": "So let's think a little\nbit about what this means. So this is how we can plug\nin our KL divergence formula",
    "start": "1624720",
    "end": "1632370"
  },
  {
    "text": "to this. But first, let's maybe\ndo a little example. So mutual information\nmeasures the dependence",
    "start": "1632370",
    "end": "1638039"
  },
  {
    "text": "between two variables, and\nthis is a symmetric measure. So if I told you something\nabout x, and that",
    "start": "1638040",
    "end": "1644450"
  },
  {
    "text": "would reveal a lot of\ninformation about y, that means that\nthey are dependent and there is high mutual\ninformation between them.",
    "start": "1644450",
    "end": "1650539"
  },
  {
    "text": "And if I could tell\nyou something about x, and it doesn't really help you\nwith knowing anything about y, that means that the\nmutual information is low.",
    "start": "1650540",
    "end": "1657220"
  },
  {
    "text": "So in these two cases, we\nhave a plot of x and y. So if I did tell\nyou the value of x,",
    "start": "1657220",
    "end": "1663690"
  },
  {
    "text": "if you knew what\nthe x event was, you could relatively--\nyou could know actually quite a bit about y, right?",
    "start": "1663690",
    "end": "1669630"
  },
  {
    "text": "You would have-- it would reveal\nquite a bit of information about what y could be.",
    "start": "1669630",
    "end": "1675780"
  },
  {
    "text": "Versus in this case, even though\nI could tell you what x is, it doesn't really tell you that\nmuch about what y would be.",
    "start": "1675780",
    "end": "1682710"
  },
  {
    "text": "These are independent variables. So in the first case, we have\nhigh mutual information's, x",
    "start": "1682710",
    "end": "1688020"
  },
  {
    "text": "and y are dependent\non each other. And in the second case, we'll\nhave low mutual information,",
    "start": "1688020",
    "end": "1693390"
  },
  {
    "text": "x and y are independent\nof each other. All right. ",
    "start": "1693390",
    "end": "1699060"
  },
  {
    "text": "So we can actually\nrewrite mutual information as an entropy. So you can do this by doing\na little bit of algebra",
    "start": "1699060",
    "end": "1704500"
  },
  {
    "text": "on this formula right here. And it actually results\nin something like this.",
    "start": "1704500",
    "end": "1709630"
  },
  {
    "text": "So it's an entropy\nof one variable, minus the conditional\nentropy or the entropy",
    "start": "1709630",
    "end": "1714930"
  },
  {
    "text": "of this conditional\ndistribution of y given x. And you can also flip the order.",
    "start": "1714930",
    "end": "1720610"
  },
  {
    "text": "So let's think a little\nbit about what it means. So that means that the entropy\nof-- so the mutual information",
    "start": "1720610",
    "end": "1729060"
  },
  {
    "text": "will be high if\nthis term is high, if the first term is high. So that means that the entropy\nof the distribution on its own",
    "start": "1729060",
    "end": "1737809"
  },
  {
    "text": "is high. So we don't know what\nx is going to be. And this term needs to be low.",
    "start": "1737810",
    "end": "1744299"
  },
  {
    "text": "But we don't know\nwhat x is going to be. It's almost a uniform\ndistribution, let's say. But if I told you\nsomething about y,",
    "start": "1744300",
    "end": "1751620"
  },
  {
    "text": "you would know exactly\nwhat x would be. So it's kind of, x on its\nown, it's very, very random,",
    "start": "1751620",
    "end": "1758720"
  },
  {
    "text": "but as soon as you know about\ny, x isn't random at all. You know exactly what x is. And that would mean that\nthere's high mutual information",
    "start": "1758720",
    "end": "1765130"
  },
  {
    "text": "between the two.  So let's go through\na little exercise.",
    "start": "1765130",
    "end": "1770440"
  },
  {
    "text": "So I'll give you\ntwo random events, and I'll ask whether these\ncorrespond to high mutual--",
    "start": "1770440",
    "end": "1775467"
  },
  {
    "text": "there's high mutual\ninformation between them or low mutual information\nbetween them, all right? So is there a high mutual\ninformation between x and y,",
    "start": "1775467",
    "end": "1782520"
  },
  {
    "text": "where x says that it's\ngoing to rain tomorrow or it rains tomorrow, and y\nsays streets are wet tomorrow.",
    "start": "1782520",
    "end": "1789720"
  },
  {
    "text": "Is it high mutual information? Yeah, it's high\nmutual information.",
    "start": "1789720",
    "end": "1794830"
  },
  {
    "text": "If you knew that streets\nare wet tomorrow, you would know that\nit rains tomorrow. If you knew that\nit rains tomorrow, then you would know that the\nstreets will be wet tomorrow.",
    "start": "1794830",
    "end": "1803380"
  },
  {
    "text": "But on its own, you\ncan't really tell if it's going to rain tomorrow,\nor if the streets are going",
    "start": "1803380",
    "end": "1808570"
  },
  {
    "text": "to be wet tomorrow, all right? Another example. Let's say x says\nit rains tomorrow",
    "start": "1808570",
    "end": "1813730"
  },
  {
    "text": "and y says we find\nlife on Mars tomorrow. High mutual information?",
    "start": "1813730",
    "end": "1818970"
  },
  {
    "text": "No. Yeah, two events are fairly-- there's a lot of uncertainty\nin both of them, right?",
    "start": "1818970",
    "end": "1825072"
  },
  {
    "text": "And even though if I told\nyou that it rains tomorrow, that wouldn't really\nreveal much information about whether we'll find\nlife on Mars tomorrow.",
    "start": "1825072",
    "end": "1830865"
  },
  {
    "start": "1830865",
    "end": "1836880"
  },
  {
    "text": "So there is one example\nof mutual information that is actually very\nuseful in robotics,",
    "start": "1836880",
    "end": "1842460"
  },
  {
    "text": "and this is called\nempowerment, and it was introduced by Polani et al.",
    "start": "1842460",
    "end": "1847950"
  },
  {
    "text": "And the form of that mutual\ninformation looks like this. It's the mutual information\nbetween the next state",
    "start": "1847950",
    "end": "1853740"
  },
  {
    "text": "and the action. So basically, the\nentropy of the next state minus the conditional\nentropy of the next state",
    "start": "1853740",
    "end": "1859380"
  },
  {
    "text": "given the action. So why do you think\nit's called empowerment? Any ideas?",
    "start": "1859380",
    "end": "1865110"
  },
  {
    "start": "1865110",
    "end": "1873210"
  },
  {
    "text": "Yes. Then it measures\nhow much [INAUDIBLE] the next state it goes through?",
    "start": "1873210",
    "end": "1879330"
  },
  {
    "text": "Yeah. It measures how much the robot\ncan influence the next state given its own actions.",
    "start": "1879330",
    "end": "1884429"
  },
  {
    "text": "That's right. So basically, it tells\nyou how empowered of an individual\nsomeone is, right? Like if your actions can\nreally influence the next state",
    "start": "1884430",
    "end": "1891930"
  },
  {
    "text": "that we're all going to see. If you're the President\nof the United States, then your actions\nare really powerful.",
    "start": "1891930",
    "end": "1898410"
  },
  {
    "text": "They will change what we\nare going to see next. ",
    "start": "1898410",
    "end": "1903700"
  },
  {
    "text": "And the same with robots. So if we want robots-- if we want to optimize\nfor empowerment, that means that we want\nrobots to take actions",
    "start": "1903700",
    "end": "1910390"
  },
  {
    "text": "that have high influence\non the next state they are going to see. So they'll will really\ninfluence the environment.",
    "start": "1910390",
    "end": "1918260"
  },
  {
    "text": "All right, cool. So we discussed a few\ninformation theoretic concepts. And now we'll use some of them\nto talk about skill discovery",
    "start": "1918260",
    "end": "1927679"
  },
  {
    "text": "algorithms. So first I'll talk about one\nalgorithm that isn't really",
    "start": "1927680",
    "end": "1933620"
  },
  {
    "text": "a skill discovery algorithm,\nbut kind of gets us in the right mindset. And this is the algorithm\ncalled soft Q-learning.",
    "start": "1933620",
    "end": "1941390"
  },
  {
    "text": "So far, when we talked\nabout Q-learning, we talked about always taking\nthe arg max of the q function.",
    "start": "1941390",
    "end": "1946799"
  },
  {
    "text": "So we had some kind of q\nfunction that is plotted here. And then our policy\nwould be always trying",
    "start": "1946800",
    "end": "1952250"
  },
  {
    "text": "to take the max of\nthat q function. And this would be-- so it would\nbe a very peaky Gaussian that",
    "start": "1952250",
    "end": "1957260"
  },
  {
    "text": "only cares about this very-- the peak of the q function.",
    "start": "1957260",
    "end": "1963240"
  },
  {
    "text": "But what that means sometimes\nis throughout training we'll want our policy\nto commit always",
    "start": "1963240",
    "end": "1969320"
  },
  {
    "text": "to the current q function. It will always try to find\nthe peak of the q function, and then go with that.",
    "start": "1969320",
    "end": "1974390"
  },
  {
    "text": " So sometimes it\nactually can have some catastrophic\ninfluence on how",
    "start": "1974390",
    "end": "1980690"
  },
  {
    "text": "the agent develops because it\ncan commit to a solution that is suboptimal. Just initially the q function\nisn't well fitted yet.",
    "start": "1980690",
    "end": "1986832"
  },
  {
    "text": "It doesn't represent the\nenvironment very well. And it will commit to it fully. So instead what we\nmight want to do,",
    "start": "1986832",
    "end": "1993260"
  },
  {
    "text": "is maybe have a\npolicy that doesn't put all of its probability mass\non the max of the q function,",
    "start": "1993260",
    "end": "1998750"
  },
  {
    "text": "but it also puts a little\nbit of probability mass on another peak that\nis a little bit lower.",
    "start": "1998750",
    "end": "2003860"
  },
  {
    "text": "That's why we don't fully commit\nto a solution straight away. Or we'll kind of like,\nhedge our bets a little bit.",
    "start": "2003860",
    "end": "2010270"
  },
  {
    "text": " There's actually\nanother motivation",
    "start": "2010270",
    "end": "2015610"
  },
  {
    "text": "for this, which comes\nfrom neuroscience and inverse optimal\ncontrol, which talks about the following.",
    "start": "2015610",
    "end": "2021730"
  },
  {
    "text": "If we had a dog and we were\nasking the dog to come to us-- if we only cared about--",
    "start": "2021730",
    "end": "2027279"
  },
  {
    "text": "if we wanted to describe\nthe behavior of the dog, and we're only talking about\nthis arg max of the q function,",
    "start": "2027280",
    "end": "2033860"
  },
  {
    "text": "then the only trajectory\nthat would make sense within that framework would\nbe the straight trajectory",
    "start": "2033860",
    "end": "2038920"
  },
  {
    "text": "that goes straight to us. The optimal trajectory. That's the only\ntrajectory that we can describe using that framework.",
    "start": "2038920",
    "end": "2045190"
  },
  {
    "text": "We always think that the dog\nis optimizing for the reward. The reward is you get a little\nsnack when you come to me.",
    "start": "2045190",
    "end": "2051730"
  },
  {
    "text": "So the only behavior\nthat makes sense for the dog within this\nframework is run towards me and don't deviate at all.",
    "start": "2051730",
    "end": "2058449"
  },
  {
    "text": "But we know that\nin reality, maybe there will be something\ninteresting in the way. Maybe the dog will wander\naround a little bit,",
    "start": "2058449",
    "end": "2063997"
  },
  {
    "text": "and the trajectory\nwon't be just straight. And we want to be\nable to describe all of those trajectories as well.",
    "start": "2063997",
    "end": "2071060"
  },
  {
    "text": "So to do this in inverse\nstochastic optimal control, people introduce this\nadditional-- people basically",
    "start": "2071060",
    "end": "2077169"
  },
  {
    "text": "started talking about a\nlittle bit of stochasticity in the policy as well. So it's a stochastic\noptimal policy.",
    "start": "2077170",
    "end": "2083930"
  },
  {
    "text": "It doesn't mean\nthat the dog always runs directly towards\nyou, but it can also deviate a little bit. And it's still an\noptimal policy,",
    "start": "2083931",
    "end": "2089408"
  },
  {
    "text": "it's just stochastic, all right?  So it turns out that the reward\nfunction for a Q function",
    "start": "2089409",
    "end": "2096940"
  },
  {
    "text": "like this includes the terms\nthat we just discussed.",
    "start": "2096940",
    "end": "2102410"
  },
  {
    "text": "So instead of just\noptimizing for the reward as we have done so\nfar, where we are",
    "start": "2102410",
    "end": "2107950"
  },
  {
    "text": "maximizing this whole objective\nover a number of time steps. We are also going to add\nan additional entropy",
    "start": "2107950",
    "end": "2114130"
  },
  {
    "text": "term, entropy of our\ncurrent policy, all right? So what that means\nis that, not only I",
    "start": "2114130",
    "end": "2120040"
  },
  {
    "text": "want you to optimize\nfor the rewards, but I also want you to have\na little bit of entropy. I don't want you to\ncommit too early.",
    "start": "2120040",
    "end": "2127360"
  },
  {
    "text": "So I don't want you\nto be very peaky. I want you to-- I want the distribution to\nbe a little bit broader, that",
    "start": "2127360",
    "end": "2133089"
  },
  {
    "text": "will result in higher entropy. So it turns out that if you do\nthat, and carry out the math--",
    "start": "2133090",
    "end": "2140539"
  },
  {
    "text": "this is our Q-learning\nalgorithm that we had before. It will look very different--",
    "start": "2140540",
    "end": "2146430"
  },
  {
    "text": "It will look very similar,\nexcept a few differences instead of taking\nthe max over the q, you'll be taking a soft\nmax here and the policy",
    "start": "2146430",
    "end": "2153155"
  },
  {
    "text": "won't be just the\narguments of the q function but it will be\nactually proportional to the exponent\nof the advantage.",
    "start": "2153155",
    "end": "2158690"
  },
  {
    "text": " All right, but\nthe more important",
    "start": "2158690",
    "end": "2163810"
  },
  {
    "text": "part is that when\nyou do this, it will result in policies\nthat look like this. So there are a little bit more\nrandom policies, all right?",
    "start": "2163810",
    "end": "2170262"
  },
  {
    "text": "We are still far from\ndiscovering skills, but now we have policies that\ncan do a little bit more. So let's see what that\nactually results in.",
    "start": "2170262",
    "end": "2177339"
  },
  {
    "text": "So this is a paper by\nTuomas Haarnoja et al called \"Deep Energy-Based Policies.\"",
    "start": "2177340",
    "end": "2184210"
  },
  {
    "text": "So first thing we can\nnotice is that exploration works a little bit\ndifferent in these policies. So here on the left you\ncan see a final policy",
    "start": "2184210",
    "end": "2193300"
  },
  {
    "text": "that is obtained by this ant\ntrying to get to this goal without using soft Q-learning. So we're using\nstandard Q-learning.",
    "start": "2193300",
    "end": "2201010"
  },
  {
    "text": "So you can see that the ant\njust decided to-- the reward is the distance to that goal. So the ant just\ndecided probably early",
    "start": "2201010",
    "end": "2207400"
  },
  {
    "text": "in its training to\ncommit to that path. And then it's really\nhard to uncommit. To just say, well, maybe I\nshould explore this other path.",
    "start": "2207400",
    "end": "2214060"
  },
  {
    "text": "This is very sub\noptimal behavior given that here you are\ngetting more, and more, and more rewards, but then you\nrealize that it doesn't really",
    "start": "2214060",
    "end": "2220030"
  },
  {
    "text": "get you fully to the goal. However, with soft Q-learning\nit's not as committed.",
    "start": "2220030",
    "end": "2225670"
  },
  {
    "text": "So it doesn't just pick a\nsolution at the very beginning. It actually keeps its\noption options open, and then that allows it to\nexplore a little bit further,",
    "start": "2225670",
    "end": "2232780"
  },
  {
    "text": "and then eventually\ndiscover the right solution.  Let's look at one\nother aspect of it.",
    "start": "2232780",
    "end": "2240440"
  },
  {
    "text": "So here we are\npre-training our ant. And the reward is\njust the speed.",
    "start": "2240440",
    "end": "2245650"
  },
  {
    "text": "So you can move\nin any direction. You will just get a\nreward if you're fast. So here on the left, you\nwill see the resulting policy",
    "start": "2245650",
    "end": "2254740"
  },
  {
    "text": "without this additional\nentropy term. And on the right, you'll see\nthe soft Q-learning policy.",
    "start": "2254740",
    "end": "2261440"
  },
  {
    "text": "So you can see that in\nthis case, all of the ants, so all of the\ndifferent random seeds basically are moving towards\nthe same direction versus--",
    "start": "2261440",
    "end": "2268630"
  },
  {
    "text": "let's play the second, maybe-- versus in this case, the ant\nis kind of exploring and going",
    "start": "2268630",
    "end": "2275560"
  },
  {
    "text": "in all kinds of\ndifferent directions because it doesn't want to\ncommit to a single solution. What that means is\nthat, if we were",
    "start": "2275560",
    "end": "2281170"
  },
  {
    "text": "to take this policy on the\nright that explores the state space a little bit better,\nit will be probably more",
    "start": "2281170",
    "end": "2288100"
  },
  {
    "text": "fine-tunable. Like if you wanted\nto just then go to a particular corner of the\nworkspace like right here.",
    "start": "2288100",
    "end": "2293770"
  },
  {
    "text": "Then this policy already\nvisited that corner. Maybe it already knows\nhow to get there, and we can fine-tune into this.",
    "start": "2293770",
    "end": "2298900"
  },
  {
    "text": "Versus with this policy,\nit would potentially need to unlearn something and\nlearn how to get there again.",
    "start": "2298900",
    "end": "2305960"
  },
  {
    "text": "And then one other\naspect that you get when adding this additional\nentropy term is the robustness.",
    "start": "2305960",
    "end": "2313290"
  },
  {
    "text": "So here is a robot trying to\nstack these two LEGO blocks. And you can see\nthat you can-- this",
    "start": "2313290",
    "end": "2318320"
  },
  {
    "text": "is the soft Q-learning policy. And you can see that you can\nperturb the robot quite a bit and then still figures\nout a way to get there.",
    "start": "2318320",
    "end": "2324960"
  },
  {
    "text": "So it explored quite a\nlot of its state space because it had this\nadditional entropy term that kind of allowed it\nto explore a little bit more.",
    "start": "2324960",
    "end": "2334349"
  },
  {
    "text": "And now it knows how\nto recover from all of these different\nstates, all right? If you try just\nstandard Q-learning",
    "start": "2334350",
    "end": "2340849"
  },
  {
    "text": "it's quite likely\nthat it will just go straight from the initial\nposition straight down, minimize that distance.",
    "start": "2340850",
    "end": "2347670"
  },
  {
    "text": "But then it won't be very\nrobust to any perturbations, all right? ",
    "start": "2347670",
    "end": "2354770"
  },
  {
    "text": "So this is-- yeah,\nthere is a question. Uh-huh. I have one for you. Sorry.",
    "start": "2354770",
    "end": "2360450"
  },
  {
    "text": "Yeah. So in the first ant\nexploration in the maze, how would this\ntypically be solved",
    "start": "2360450",
    "end": "2366589"
  },
  {
    "text": "in other algorithms like DDPG? Do you just add random\naction noise and hope that will take that other path? ",
    "start": "2366590",
    "end": "2374026"
  },
  {
    "text": "Yeah, so the question\nis, how would you solve this ant maze\nproblem in something",
    "start": "2374026",
    "end": "2379039"
  },
  {
    "text": "like DDPG when you don't have\nan additional entropy term? Yeah, so usually to encourage\nsome kind of exploration,",
    "start": "2379040",
    "end": "2386615"
  },
  {
    "text": "you will add some kind\nof noise, and then you would hope that it would\nfind the right path. But then it will\nprobably be very",
    "start": "2386615",
    "end": "2392779"
  },
  {
    "text": "dependent on the random\nseed that you use. So sometimes it will do it. Sometimes it won't. It will come to the wrong\nsolution at the beginning",
    "start": "2392780",
    "end": "2398895"
  },
  {
    "text": "and there is no really\ngood way to recover. But if you look at like an\nalgorithm like soft actor",
    "start": "2398895",
    "end": "2404210"
  },
  {
    "text": "critic, which is kind of\nanother version of DDPG that includes the\nentropy term, then it",
    "start": "2404210",
    "end": "2409549"
  },
  {
    "text": "should be able to discover the\nright solution all the time. So then if you're also\n[INAUDIBLE] entropy,",
    "start": "2409550",
    "end": "2414785"
  },
  {
    "text": "does that mean the variance\nacross different runs would be smaller typically? Oh, yeah.",
    "start": "2414785",
    "end": "2420672"
  },
  {
    "text": "That's a good question. So does that mean that if\nyou optimize additionally for the entropy, does that\nmean that the variance",
    "start": "2420672",
    "end": "2425940"
  },
  {
    "text": "between different\nruns will be smaller? Usually, it's the case, yeah. So with DDPG you\nare a little bit more reliant on the\nright random seed",
    "start": "2425940",
    "end": "2434100"
  },
  {
    "text": "that just happened to explore\nthat certain-- that part of the state space. With SAC you are always\nexploring all kinds",
    "start": "2434100",
    "end": "2439590"
  },
  {
    "text": "of part-- all the parts. So you should be able to\nfind the solution better or more reliably.",
    "start": "2439590",
    "end": "2445114"
  },
  {
    "text": "OK. Cool. ",
    "start": "2445114",
    "end": "2450819"
  },
  {
    "text": "So we want to learn\ndiverse skills. So far we only talked\nabout how we can just learn a skill that's a\nlittle bit more stochastic.",
    "start": "2450820",
    "end": "2457330"
  },
  {
    "text": "So when we learn diverse\nskills, what we want is some kind of way to\ncontrol the stochasticity.",
    "start": "2457330",
    "end": "2462880"
  },
  {
    "text": "So we want to be\nable to-- for the ant not just to go in all the\ndirections, but also to say, well, I want you to go up,\nor I want you to go left,",
    "start": "2462880",
    "end": "2469270"
  },
  {
    "text": "or I want you to go\nright, and so on. So we'll condition our policy\non this additional variable",
    "start": "2469270",
    "end": "2474700"
  },
  {
    "text": "that we'll use to control\nwhat kind of skill we want to ask for.",
    "start": "2474700",
    "end": "2480250"
  },
  {
    "text": "So let's call it just a task\nindex or a skill index, z.",
    "start": "2480250",
    "end": "2485590"
  },
  {
    "text": "And let's say that this is our\nant or just some kind of robot. And we want it on its own\nto explore this environment.",
    "start": "2485590",
    "end": "2493242"
  },
  {
    "text": "So to go in all the\ndifferent directions. But we want it to be\ncontrollable, right? So we want it such that if I\nset z to a particular parameter,",
    "start": "2493242",
    "end": "2501160"
  },
  {
    "text": "it will go in one direction. If I set it to some\nother parameter, it will go in another\ndirection, and so on.",
    "start": "2501160",
    "end": "2506330"
  },
  {
    "text": "So for instance, if I set z\nequals 0, it will go up here and it will result in\nthis green trajectory.",
    "start": "2506330",
    "end": "2512290"
  },
  {
    "text": "If I set z equal 1, it will\nresult in this trajectory. z2 in this one, and so on, and\nthey will be all different.",
    "start": "2512290",
    "end": "2518890"
  },
  {
    "text": "In that case, I'm\nkind of starting to discover the skills, and\nnow they're controllable. It's not just a\nstochastic policy.",
    "start": "2518890",
    "end": "2525130"
  },
  {
    "text": " So soft Q-learning\nis also referred",
    "start": "2525130",
    "end": "2531250"
  },
  {
    "text": "to as maxed entropy\nreinforcement learning or MaxEnt\nreinforcement learning, because we add this\nadditional entropy",
    "start": "2531250",
    "end": "2537070"
  },
  {
    "text": "term that we're maximizing. So why can't we just use\nmaxed entropy reinforcement learning instead?",
    "start": "2537070",
    "end": "2544280"
  },
  {
    "text": "So first, action entropy is\nnot the same as state entropy. So in the previous\ncase, we wanted",
    "start": "2544280",
    "end": "2550600"
  },
  {
    "text": "to make sure that\nthe agent doesn't commit to a single action. That keeps the distribution\nof our actions broad.",
    "start": "2550600",
    "end": "2557325"
  },
  {
    "text": "But that doesn't\nnecessarily mean that it will end up--\nthat it will end up in many different states. It sometimes happens,\nlike we saw in the ant,",
    "start": "2557325",
    "end": "2563770"
  },
  {
    "text": "but it doesn't need to happen. So agent can take\nvery different actions",
    "start": "2563770",
    "end": "2568940"
  },
  {
    "text": "but will end in\nvery similar states. In addition to this, these\nMaxEnt policies are stochastic",
    "start": "2568940",
    "end": "2575538"
  },
  {
    "text": "but they are not\nalways controllable. This is what we just\ndiscussed, right? So the ant can go in all\nthe different directions,",
    "start": "2575538",
    "end": "2581100"
  },
  {
    "text": "but we can't really\nsay, well, I want you to go in that particular one. ",
    "start": "2581100",
    "end": "2587050"
  },
  {
    "text": "So intuitively,\nwhat we want is we want low diversity\nfor a fixed z, so if I told you\nwhat skill I want,",
    "start": "2587050",
    "end": "2593110"
  },
  {
    "text": "I want you to know what to do-- but high diversity across these. So if I didn't tell\nyou what to do then",
    "start": "2593110",
    "end": "2599410"
  },
  {
    "text": "I want-- if I were to\nmarginalize over z, and just look at all\nof these trajectories, I want them to cover\nthe state space well.",
    "start": "2599410",
    "end": "2605230"
  },
  {
    "text": " That has some resemblance\nto one of the terms that we are discussing,\nwhere one of the term",
    "start": "2605230",
    "end": "2612480"
  },
  {
    "text": "was talking about diversity\nif you knew the z. And the other term was talking\nabout if you don't know z,",
    "start": "2612480",
    "end": "2617610"
  },
  {
    "text": "or if you don't know this\nother random variable there should be high entropy.",
    "start": "2617610",
    "end": "2623039"
  },
  {
    "text": "So keep that in mind. But let's see how we can design\nsome kind of reward function that will get us there.",
    "start": "2623040",
    "end": "2630109"
  },
  {
    "text": "So the intuition is that\ndifferent skills should visit different states space regions. So for different skills, we\nwant to explore different parts",
    "start": "2630110",
    "end": "2637257"
  },
  {
    "text": "of the state space.  So we'll try to think of\na reward function that",
    "start": "2637258",
    "end": "2644210"
  },
  {
    "text": "is not task dependent, right? So we don't really want to\nsay, I want you to go up or I want you to go to the side.",
    "start": "2644210",
    "end": "2650360"
  },
  {
    "text": "It should be task agnostic. And it should just\npromote diversity.",
    "start": "2650360",
    "end": "2655690"
  },
  {
    "text": "It should just promote going\nin all the different directions and be controllable\nat the same time. ",
    "start": "2655690",
    "end": "2662490"
  },
  {
    "text": "So that reward function\nshould probably be dependent on the\nskill variable z,",
    "start": "2662490",
    "end": "2667838"
  },
  {
    "text": "because for different\nskills it might look a little bit different. So let's just say that it's\ngoing to be dependent on that,",
    "start": "2667838",
    "end": "2674810"
  },
  {
    "text": "and then we'll sum\nover all the z's to get the reward for\nthe entire policy.",
    "start": "2674810",
    "end": "2680710"
  },
  {
    "text": "And what we would want is, we\nwant to reward the states that are unlikely for any other z's.",
    "start": "2680710",
    "end": "2686450"
  },
  {
    "text": "So for a particular\nskill, I want you to go to states that\nnone of the other skills would have gone to.",
    "start": "2686450",
    "end": "2693070"
  },
  {
    "text": "That would mean that this\nskill is really specific. It's different than\nall the other skills. And if all the skills\nwere to do that,",
    "start": "2693070",
    "end": "2699440"
  },
  {
    "text": "then we'll get every skill\nthat is very original, and altogether they will span\nthis diversity of behaviors.",
    "start": "2699440",
    "end": "2705910"
  },
  {
    "text": " All right, so here's one idea.",
    "start": "2705910",
    "end": "2711730"
  },
  {
    "text": "Let's train a classifier. This p of z given s that\ntells us given the state what",
    "start": "2711730",
    "end": "2720280"
  },
  {
    "text": "skill it came from, all right? We'll just do this using\nsupervised learning. We can roll out a trajectory.",
    "start": "2720280",
    "end": "2726940"
  },
  {
    "text": "We know what z we set\nwhen we rolled it out. We had our policy that\nwas conditioned on z.",
    "start": "2726940",
    "end": "2732760"
  },
  {
    "text": "And then we'll take this\ntrajectory, every state from this trajectory,\nand we'll try to fit a classifier\nthat tells us what",
    "start": "2732760",
    "end": "2738250"
  },
  {
    "text": "z did that state come from. And then in addition to\nlearning that classifier,",
    "start": "2738250",
    "end": "2743520"
  },
  {
    "text": "we'll also set that the\nreward function for the agent would be to maximize\nthe log probability of z given s, all right?",
    "start": "2743520",
    "end": "2751210"
  },
  {
    "text": "What does that mean? So both the classifier and the\nreward function, and the agent,",
    "start": "2751210",
    "end": "2756520"
  },
  {
    "text": "is trying to maximize this\nlog likelihood, all right? ",
    "start": "2756520",
    "end": "2761810"
  },
  {
    "text": "So what does it mean? So the agent itself is rewarded\nby helping the classifier.",
    "start": "2761810",
    "end": "2769530"
  },
  {
    "text": "So let's think\nabout it this way. We have a policy that produces\nsome actions in an environment,",
    "start": "2769530",
    "end": "2775730"
  },
  {
    "text": "and then we get\nthe states policies conditioned on the skill. And then in addition to\nthis, we have this classifier",
    "start": "2775730",
    "end": "2781320"
  },
  {
    "text": "and the discriminator\nthat takes a state and tells us what\nskill it came from. And this is the\nsame kind of skill",
    "start": "2781320",
    "end": "2787410"
  },
  {
    "text": "that we input to the policy. But why does it work? Why does it make sense?",
    "start": "2787410",
    "end": "2794740"
  },
  {
    "text": "So let's go through a little\nexample of how exactly this will work. So let's say we have two\ndifferent trajectories that",
    "start": "2794740",
    "end": "2801030"
  },
  {
    "text": "were produced by\ntwo different z's. So we are at the very\nbeginning of our training. The agent doesn't really\nknow that different",
    "start": "2801030",
    "end": "2806295"
  },
  {
    "text": "z's should lead to\ndifferent trajectories. So both of them\nare fairly similar.",
    "start": "2806295",
    "end": "2811360"
  },
  {
    "text": "So we produced--\nlet's say produce-- the green one is\nproduced for z equals 0. The blue one is\nproduced for z equal 1.",
    "start": "2811360",
    "end": "2818220"
  },
  {
    "text": "Now, we are trying to\nfit the classifier to it. So let's say that\nthe decision boundary that the classifier\nlearns, it tries",
    "start": "2818220",
    "end": "2823500"
  },
  {
    "text": "to classify these\ntwo trajectories. It tries to tell you, given the\nstate is it z 0, or z equal 1.",
    "start": "2823500",
    "end": "2829830"
  },
  {
    "text": "And this is the decision\nboundary it found. So it's not great but\nit's doing its best.",
    "start": "2829830",
    "end": "2835650"
  },
  {
    "text": "So now the agent is trying\nto optimize for the same loss as the classifier. So the agent now is trying to\nproduce new trajectories that",
    "start": "2835650",
    "end": "2841950"
  },
  {
    "text": "will help the class of--\nthe current classifier. The current decision boundary. So the next two\ntrajectories that the agent",
    "start": "2841950",
    "end": "2847770"
  },
  {
    "text": "will produce for z\nequals 0 and z equal 1 will be a little\nbit further apart so that the this log likelihood\nis a little bit better.",
    "start": "2847770",
    "end": "2855420"
  },
  {
    "text": "So it produces two\ntrajectories that are easily distinguishable.",
    "start": "2855420",
    "end": "2860520"
  },
  {
    "text": "And then the classifier\noptimizes its decision boundary again. So given these two trajectories\nit makes a small correction,",
    "start": "2860520",
    "end": "2867060"
  },
  {
    "text": "and now it's a\nlittle bit better. And now the agent does it again. So given the new decision\nboundary the agent",
    "start": "2867060",
    "end": "2872250"
  },
  {
    "text": "produces two trajectories\nthat help the classifier. And then we adjust the\ndecision boundary and so on.",
    "start": "2872250",
    "end": "2878100"
  },
  {
    "text": "So they play this game\nbut they play it-- they're on the same team, right? The agent is trying to\nproduce trajectories",
    "start": "2878100",
    "end": "2883590"
  },
  {
    "text": "that are easy to classify,\nand the classifier is then classifying--\nlearning how to classify these trajectories. All right. ",
    "start": "2883590",
    "end": "2891829"
  },
  {
    "text": "Are there any questions to this? ",
    "start": "2891830",
    "end": "2897124"
  },
  {
    "text": "Yes. This sort of\napproach trajectory, right, not just\nthe active state, like you do it for every state? ",
    "start": "2897124",
    "end": "2902967"
  },
  {
    "text": "Yeah, this was a great question. So this classifier here,\nwhat does it take as input? Is it just a single\nstate or a trajectory?",
    "start": "2902967",
    "end": "2908609"
  },
  {
    "text": "So in this particular\npaper called Diversity is All You Need\nby Ben Eysenbach and et al.",
    "start": "2908610",
    "end": "2914280"
  },
  {
    "text": "This is just on a single state. So it looks at\nevery single state. But you could condition\nit on something else,",
    "start": "2914280",
    "end": "2920450"
  },
  {
    "text": "on the whole trajectory\nor something like that. Now, I guess one question\nto all of you would be,",
    "start": "2920450",
    "end": "2926120"
  },
  {
    "text": "what would happen\nif we condition it not on just every single\nstate, but let's say only",
    "start": "2926120",
    "end": "2933830"
  },
  {
    "text": "on the last state\nof the trajectory? ",
    "start": "2933830",
    "end": "2945320"
  },
  {
    "text": "Just wondering other trajectory\nlooks like [INAUDIBLE] the exact same [INAUDIBLE]\ndivergent [INAUDIBLE]??",
    "start": "2945320",
    "end": "2952758"
  },
  {
    "text": "That's right. They could look exactly\nthe same way, all the way, except at the very\nend where they would need to diverge\nbecause that's",
    "start": "2952758",
    "end": "2958190"
  },
  {
    "text": "what we will be classifying by. Yeah, that's correct. So you can decide\nwhat kind of diversity",
    "start": "2958190",
    "end": "2963770"
  },
  {
    "text": "you want by deciding what your\nconditioning this classifier on.",
    "start": "2963770",
    "end": "2969050"
  },
  {
    "text": "If you're conditioning\nit on every single state, that means every\nsingle state has to be different for\nboth of these skills. And there's actually different\nchoices that you can take here.",
    "start": "2969050",
    "end": "2976230"
  },
  {
    "text": "You can condition only\non the last state, on the first state\nand the last state. On all of the states. On all kinds of\ndifferent things.",
    "start": "2976230",
    "end": "2981720"
  },
  {
    "text": "And there is\ndifferent works that actually discussed this choice.",
    "start": "2981720",
    "end": "2987010"
  },
  {
    "text": "Yes. I'm trying to understand\nthis reward function from [INAUDIBLE] perspective.",
    "start": "2987010",
    "end": "2993020"
  },
  {
    "text": "So maximize this reward function\ncorresponds to minimizing the negative reward\nfunction, which",
    "start": "2993020",
    "end": "3000809"
  },
  {
    "text": "is corresponding to, like,\nminimize the entropy of z",
    "start": "3000810",
    "end": "3006125"
  },
  {
    "text": "given the trajectory of space. So does it mean I want\nto reduce uncertainty",
    "start": "3006125",
    "end": "3012690"
  },
  {
    "text": "about [INAUDIBLE] given\nthe trajectory of space? Yes, that's a great intuition.",
    "start": "3012690",
    "end": "3018240"
  },
  {
    "text": "And we'll get to it in I\nthink one slide or two slides. Yes, that's exactly correct. All right, so first a few\nexamples of what kind of tasks",
    "start": "3018240",
    "end": "3026658"
  },
  {
    "text": "they could learn, what kind\nof skills they could learn. So again, they just dropped\nan agent in an environment",
    "start": "3026658",
    "end": "3031829"
  },
  {
    "text": "and then they have this very\ngeneric reward function that is not environment specific. It just rewards diversity\nand helps this classifier.",
    "start": "3031830",
    "end": "3040230"
  },
  {
    "text": "And then they can try different\nz's after this process and just see what\nthey correspond to. So in this half\ncheetah environment,",
    "start": "3040230",
    "end": "3046650"
  },
  {
    "text": "it turns out that one of the\nz's corresponds to the cheetah running forward. The other one corresponds\nto cheetah doing these back",
    "start": "3046650",
    "end": "3053550"
  },
  {
    "text": "flips or front flips. The other one corresponds to\nthe cheetah running backwards. So it can suddenly do all\nkinds of different things.",
    "start": "3053550",
    "end": "3059940"
  },
  {
    "text": " Here is another\nexample of an ant. So you can see that\nsometimes ants just",
    "start": "3059940",
    "end": "3066260"
  },
  {
    "text": "does like some random things. Jumps in place. Does kind of all kinds\nof different things. Sometimes it runs in one\nparticular direction.",
    "start": "3066260",
    "end": "3073463"
  },
  {
    "text": "And they also try it in\na simpler environment, the mountain car. And you can see that some of\nthe z's, some of the skills",
    "start": "3073463",
    "end": "3079640"
  },
  {
    "text": "that they learn actually\ncorrespond to solving the task. So sometimes you can\nkind of by accident stumble upon a skill that solves\na task that the original author",
    "start": "3079640",
    "end": "3087980"
  },
  {
    "text": "of the environment had\nin mind, but it just emerged from this\ndiversity objective. ",
    "start": "3087980",
    "end": "3095619"
  },
  {
    "text": "All right, so going back\nto your question-- yes. So in this case,\nthis corresponds",
    "start": "3095620",
    "end": "3102130"
  },
  {
    "text": "to sort of like an unsupervised\nRL, so we're designing a reward function, like in the\nmountain car example,",
    "start": "3102130",
    "end": "3109120"
  },
  {
    "text": "it doesn't have the concept\nof solving the task. That's correct, yeah. So just to repeat the\nquestion or the note",
    "start": "3109120",
    "end": "3118150"
  },
  {
    "text": "is that, this corresponds to\nunsupervised reinforcement learning because this is not\nreally specific to a task.",
    "start": "3118150",
    "end": "3124360"
  },
  {
    "text": "We designed a reward function\nthat is task agnostic. It doesn't know that\nthe goal of mountain car is to get to that top.",
    "start": "3124360",
    "end": "3130660"
  },
  {
    "text": "Yes, that's exactly right. It's completely\nunsupervised in this case.",
    "start": "3130660",
    "end": "3136530"
  },
  {
    "text": "Cool. So now a connection\nto mutual information that we talked about\njust a second ago. So so far we talked\nabout the reward function",
    "start": "3136530",
    "end": "3143000"
  },
  {
    "text": "optimizing this log probability. So remember that\nmutual information",
    "start": "3143000",
    "end": "3149150"
  },
  {
    "text": "is equal to this entropy\nminus the relative entropy. So let's think of the\nmutual information between our z variable,\nthe skill, and the state.",
    "start": "3149150",
    "end": "3157310"
  },
  {
    "text": " So that means your information\nis equal to the entropy of z,",
    "start": "3157310",
    "end": "3163020"
  },
  {
    "text": "minus the conditional\nentropy of z given s. ",
    "start": "3163020",
    "end": "3168580"
  },
  {
    "text": "So now this first\npart, we don't really have that much\ncontrol over this. We can just say that\nevery z is equally likely.",
    "start": "3168580",
    "end": "3175880"
  },
  {
    "text": "So we have a uniform\nprior that will lead us-- that will lead to\nthe highest entropy. What that means is basically\nevery skill has a chance",
    "start": "3175880",
    "end": "3182859"
  },
  {
    "text": "to go, right? Like you can sample\nthe z's uniformly, and you don't just focus\non one particular z.",
    "start": "3182860",
    "end": "3188359"
  },
  {
    "text": "So that's easy to do. And then the second part\nis the entropy of z given s",
    "start": "3188360",
    "end": "3193640"
  },
  {
    "text": "and this is exactly\nminimized by maximizing log of p of z given s. So this tells you that\nif I know what s it is--",
    "start": "3193640",
    "end": "3202060"
  },
  {
    "text": "then I should--\nif I told you what s you visited you should be\nable to very clearly tell me what z it came from.",
    "start": "3202060",
    "end": "3208917"
  },
  {
    "text": "So this is exactly the\nnote that you made earlier.  So this is the paper called\n\"Diversity is All You Need.\"",
    "start": "3208917",
    "end": "3215480"
  },
  {
    "text": "And then there is\na paper actually that came before this paper that\ndiscusses a different choice",
    "start": "3215480",
    "end": "3220750"
  },
  {
    "text": "of conditioning. And there's actually\na few more called-- this one is called \"Variational\nIntrinsic Control,\"",
    "start": "3220750",
    "end": "3227360"
  },
  {
    "text": "and I also encourage you\nto take a look at that.  So we talked a little bit about\nhow we can discover skills.",
    "start": "3227360",
    "end": "3235400"
  },
  {
    "text": "Let's talk about how we can\nactually use those skills. ",
    "start": "3235400",
    "end": "3240520"
  },
  {
    "text": "And how are we on time? OK. So let's talk about this\n\"Diversity is All You Need\"",
    "start": "3240520",
    "end": "3247460"
  },
  {
    "text": "paper. So we have this policy now\nthat is conditioned on z. And for different z's it can\ndo a bunch of different things.",
    "start": "3247460",
    "end": "3254270"
  },
  {
    "text": "So let's say we want to now\nuse those skills to accomplish some kind of tasks that we\ncare about in the world. So let's say we want\nto have an ant navigate",
    "start": "3254270",
    "end": "3260799"
  },
  {
    "text": "from this point, to that point,\nto this point, and so on. So how can we use the learned\nskills to accomplish a task?",
    "start": "3260800",
    "end": "3268830"
  },
  {
    "text": "Any ideas? Yes. ",
    "start": "3268830",
    "end": "3274566"
  },
  {
    "text": "You can just sample from\nyour skill conditioned on what your objective is.",
    "start": "3274566",
    "end": "3281810"
  },
  {
    "text": "You can just sample from\nyour skills conditioned on what your objective is. OK. Yes, so you can try\ndifferent skills",
    "start": "3281810",
    "end": "3287180"
  },
  {
    "text": "and kind of find\nwhich ones will work. Yep. Any other ideas? ",
    "start": "3287180",
    "end": "3294560"
  },
  {
    "text": "Yeah. Can we just fine-tune\non the new task? Can we just fine-tune\non the new task. Yeah, so how would\nyou exactly fine-tune?",
    "start": "3294560",
    "end": "3302010"
  },
  {
    "text": "[INAUDIBLE] OK, great. Yeah, so you could do it the\nway that you are describing.",
    "start": "3302010",
    "end": "3308750"
  },
  {
    "text": "So we can basically form it\nas an additional reinforcement learning problem\nwhere now you'll learn a policy that\noperates on z's rather",
    "start": "3308750",
    "end": "3316130"
  },
  {
    "text": "than on actions, right? So you'll just learn\nanother policy. Now the action\nspace of this policy is I can command that skill,\nthis skill, that skill, and so",
    "start": "3316130",
    "end": "3323650"
  },
  {
    "text": "on. And maybe you act for a\ncertain number of steps, and then after this, you get\nto choose the next skill,",
    "start": "3323650",
    "end": "3328830"
  },
  {
    "text": "all right?  So this is actually what\nthey show in Diversity",
    "start": "3328830",
    "end": "3335910"
  },
  {
    "text": "is All You Need paper. And they can get cheetah\nto run over these hurdles. They can also do a\nlittle ant navigation.",
    "start": "3335910",
    "end": "3342380"
  },
  {
    "text": "And they show some results\nwhere it actually works. But one question is,\ncan we do better?",
    "start": "3342380",
    "end": "3348463"
  },
  {
    "text": "And it turns out that we\nactually-- that we actually can. So what's the problem with this?",
    "start": "3348463",
    "end": "3355510"
  },
  {
    "text": "So the problem with the solution\nthat we discussed just now is that we'll be discovering\nall kinds of different skills",
    "start": "3355510",
    "end": "3361770"
  },
  {
    "text": "and some of those skills might\nnot be particularly useful. So sometimes we might\njust have these skills that correspond to the\nant just staying in place",
    "start": "3361770",
    "end": "3368849"
  },
  {
    "text": "or jumping a little\nbit, or motor babbling, doing different\nthings because as long as the skill is different\nit will be considered",
    "start": "3368850",
    "end": "3374550"
  },
  {
    "text": "as separate skill. So we maybe want to change\nthe objective a little",
    "start": "3374550",
    "end": "3379986"
  },
  {
    "text": "bit so that we can\nproduce skills that are a little bit more useful. ",
    "start": "3379987",
    "end": "3385162"
  },
  {
    "text": "And then the second\nproblem we have is that it's not actually\nvery easy to use the learning skills. So right now the solution\nthat we came up with",
    "start": "3385162",
    "end": "3391240"
  },
  {
    "text": "is that we would\nlearn another policy on top of those\nskills, which means we need to do another\nreinforcement learning problem.",
    "start": "3391240",
    "end": "3397040"
  },
  {
    "text": "And reinforcement\nlearning is hard, and now you have to explore\nin the z space, which would make it a little bit better.",
    "start": "3397040",
    "end": "3402427"
  },
  {
    "text": "But it's still quite a lot\nof work that you have to do. ",
    "start": "3402427",
    "end": "3408510"
  },
  {
    "text": "So we'll think\nabout one question, which will actually contribute\nto both of these problems.",
    "start": "3408510",
    "end": "3414839"
  },
  {
    "text": "And the question is, what\nmakes a useful skill? What makes a skill useful? ",
    "start": "3414840",
    "end": "3420860"
  },
  {
    "text": "So I'll give you two\nexamples of a skill that",
    "start": "3420860",
    "end": "3426050"
  },
  {
    "text": "is not particularly useful. So we can have this ant\nthat turns upside down,",
    "start": "3426050",
    "end": "3431870"
  },
  {
    "text": "and it's like-- it's kind of\ntricky to see what's going on. And then here is\nanother skill where",
    "start": "3431870",
    "end": "3438410"
  },
  {
    "text": "there's this baby\nplaying with Jenga, and it pulls the wrong thing,\nand results in a catastrophe.",
    "start": "3438410",
    "end": "3445340"
  },
  {
    "text": "And here's another\nset of skills that seem a little bit more useful. So here is an ant\nrunning forward.",
    "start": "3445340",
    "end": "3451010"
  },
  {
    "text": "And here's this cartoon\ncharacter juggling. So there could be a lot\nof opinions about what",
    "start": "3451010",
    "end": "3457310"
  },
  {
    "text": "makes a skill useful. But there is one approach\nthat was published recently",
    "start": "3457310",
    "end": "3462980"
  },
  {
    "text": "and that tries to\naddress this question and says the following.",
    "start": "3462980",
    "end": "3468680"
  },
  {
    "text": "\"Consequences are hard\nto predict for skills that are not very useful. And consequences\nare easy to predict",
    "start": "3468680",
    "end": "3475549"
  },
  {
    "text": "if the skill is useful.\" So if you're doing\nsomething useful that means it will be\nrelatively easy to predict",
    "start": "3475550",
    "end": "3480920"
  },
  {
    "text": "what's going to happen next. And that might be a\nlittle bit controversial, but let's run with it and\nlet's see what we get.",
    "start": "3480920",
    "end": "3486290"
  },
  {
    "text": "All right.  So this is introducing\nthis paper called DADS",
    "start": "3486290",
    "end": "3493020"
  },
  {
    "text": "by Archit Sharma et al. And it talks about a different\nkind of mutual information.",
    "start": "3493020",
    "end": "3500670"
  },
  {
    "text": "So it will try to change\nthe mutual information so that we can incorporate\nthis predictability objective.",
    "start": "3500670",
    "end": "3508910"
  },
  {
    "text": "So we want our skills\nthat are not only novel but are also predictable, right? And we'll think\nthat the assumption",
    "start": "3508910",
    "end": "3515869"
  },
  {
    "text": "is that predictable skills\nmeans more useful skills, right? So this is the\nmutual information",
    "start": "3515870",
    "end": "3521480"
  },
  {
    "text": "we had before between z and s. So now we'll have the slightly\ndifferent mutual information",
    "start": "3521480",
    "end": "3527000"
  },
  {
    "text": "that we'll try to\nmaximize, which is the mutual information\nbetween s prime, z given s, right?",
    "start": "3527000",
    "end": "3532220"
  },
  {
    "text": "We haven't really talked\nabout this conditional mutual information. So let's talk about\nit real quick. So this is the definition\nof the mutual information",
    "start": "3532220",
    "end": "3538940"
  },
  {
    "text": "we had before. So if we make it\na conditional, we can just condition everything\non this additional conditional",
    "start": "3538940",
    "end": "3545600"
  },
  {
    "text": "variable. So this mutual information\nwould be x, y given z.",
    "start": "3545600",
    "end": "3551329"
  },
  {
    "text": "And then this would be a joint\ndistribution conditioned on z, and the marginal is\nconditioned on z as well.",
    "start": "3551330",
    "end": "3556760"
  },
  {
    "text": " Al right, so we can now\nrewrite this objective given the definition\nthat's following.",
    "start": "3556760",
    "end": "3564820"
  },
  {
    "text": "It would be the max of these\ntwo conditional entropies. So one entropy\nsays that it should",
    "start": "3564820",
    "end": "3570810"
  },
  {
    "text": "be a high entropy of the next\nstate given the current state. And the other one\nsays it should be low entropy of the\nnext state given",
    "start": "3570810",
    "end": "3577030"
  },
  {
    "text": "the current state and the z. So what that means\nis that future--",
    "start": "3577030",
    "end": "3583410"
  },
  {
    "text": "so s prime-- what\nwe're going to see next-- is hard to predict\nfor different skills if I don't know\nwhat z is, right?",
    "start": "3583410",
    "end": "3589160"
  },
  {
    "text": "So it's difficult to say\nwhat's going to happen next. And the second one says\nthe future should be really",
    "start": "3589160",
    "end": "3595730"
  },
  {
    "text": "predictable for a given skill. So if I told you what\nz is, then you should be very easily able to\ntell me what s prime",
    "start": "3595730",
    "end": "3602450"
  },
  {
    "text": "is going to happen next. And if we optimize\nthis objective that means we'll end up with\nskills that are predictable,",
    "start": "3602450",
    "end": "3608510"
  },
  {
    "text": "right? So for every z this\nshould be very low. So we should be able to tell\nwhat's going to happen next.",
    "start": "3608510",
    "end": "3616700"
  },
  {
    "text": "So you can approximate it\nso that you can actually optimize it, and it results\nin something like this. And the two important terms are\nthe numerator and denominator.",
    "start": "3616700",
    "end": "3626560"
  },
  {
    "text": "And the numerator\nhere corresponds to having this additional neural\nnetwork that we are training,",
    "start": "3626560",
    "end": "3634070"
  },
  {
    "text": "which tells you\nwhat s prime, you're going to end up in given the\ncurrent s and the z, the skill",
    "start": "3634070",
    "end": "3639190"
  },
  {
    "text": "that you commanded. And we are trying to\nmaximize this whole thing. So that means we are trying\nto make this likelihood as",
    "start": "3639190",
    "end": "3646269"
  },
  {
    "text": "high as possible. So we want to be able to\ntell which state we're going to end up in.",
    "start": "3646270",
    "end": "3651730"
  },
  {
    "text": "And as the numerator then\nsays for every other skill, the future should\nbe hard to predict.",
    "start": "3651730",
    "end": "3657590"
  },
  {
    "text": "So if we sum over all the\nz's, it should be hard. They are marginalizing\nover z here. All right.",
    "start": "3657590",
    "end": "3663950"
  },
  {
    "text": "Cool. So what that means is now we\nhave a neural network that we have to fit to maximize\nthis objective, which",
    "start": "3663950",
    "end": "3673400"
  },
  {
    "text": "is this neural network that\ntells us what next stage we're going to see given the\ncurrent state and the skill.",
    "start": "3673400",
    "end": "3681772"
  },
  {
    "text": "This is fairly\nsimilar to something that you already learned about\nin model-based reinforcement learning, which is\nthe transition model",
    "start": "3681772",
    "end": "3686980"
  },
  {
    "text": "of the environment, all right? What's the next\nstate we're going to see given the current\nstate and the action.",
    "start": "3686980",
    "end": "3693090"
  },
  {
    "text": "So we are learning kind\nof a dynamics model, but it's a skill\ndynamics model compared to a conventional\nglobal dynamics",
    "start": "3693090",
    "end": "3699823"
  },
  {
    "text": "model that just tells us\nwhat's the next state we're going to see given an action. This one tells us\nwas the next state we're going to see given\na skill, all right?",
    "start": "3699823",
    "end": "3706553"
  },
  {
    "text": " And then skills\nwill be optimized",
    "start": "3706553",
    "end": "3713270"
  },
  {
    "text": "so that this skill dynamics will\nbe easier to model, all right? So the skills will try\nto produce such s primes",
    "start": "3713270",
    "end": "3721220"
  },
  {
    "text": "so that this is easy. So it's easy to fit that model. ",
    "start": "3721220",
    "end": "3728480"
  },
  {
    "text": "And hopefully, that should\nresult in a little bit more predictable skills. So let's look at the algorithm.",
    "start": "3728480",
    "end": "3734589"
  },
  {
    "text": "So we would initialize\nour policy and our-- the skill dynamics model q.",
    "start": "3734590",
    "end": "3742180"
  },
  {
    "text": "Then we will sample\na skill from a prior. This will be a uniform\nprior for every episode.",
    "start": "3742180",
    "end": "3747849"
  },
  {
    "text": "In this case it\nreally is a Gaussian but let's just\nimagine it's uniform. We sample different skills. ",
    "start": "3747850",
    "end": "3755240"
  },
  {
    "text": "And then we would collect new on\npolicy samples given those z's. So we'll have for the z will\ncollect this trajectory.",
    "start": "3755240",
    "end": "3761890"
  },
  {
    "text": "For that z that\ntrajectory, and so on. Then given that we\ncan compute the reward",
    "start": "3761890",
    "end": "3768690"
  },
  {
    "text": "for all of those\ntransitions and we can update our dynamics model.",
    "start": "3768690",
    "end": "3774410"
  },
  {
    "text": "So our reward is now dependent\non the skill dynamics model, and we can compute the\nreward based on this and we can also update\nthe dynamics model itself.",
    "start": "3774410",
    "end": "3782339"
  },
  {
    "text": "So we do that. And then we can update the\npolicy using any reinforcement learning algorithm in\nthis particular case.",
    "start": "3782340",
    "end": "3788670"
  },
  {
    "text": "I believe we used SAC. So relatively comparable to\nDiversity is All You Need.",
    "start": "3788670",
    "end": "3797780"
  },
  {
    "text": "Now it's just a\nslightly different model that we are fitting. Before we were fitting\nthis model that tells you given the\nstate what skill",
    "start": "3797780",
    "end": "3805700"
  },
  {
    "text": "you're going to end up in. Here we are fitting this\nmodel that tells you, given my current state\nand the skill what next state I'm going to see.",
    "start": "3805700",
    "end": "3812375"
  },
  {
    "text": "So relatively small change. All right. So this is the algorithm, and\nwe do this over and over again.",
    "start": "3812375",
    "end": "3819930"
  },
  {
    "text": "So let's look at\nsome of the results. So first, we see that\nthe things that we learn, if we just look across all the\ndifferent z's that we can set,",
    "start": "3819930",
    "end": "3828650"
  },
  {
    "text": "seem a little bit\nmore useful, right? So the ant actually,\nthey result to the ant anting in all kinds of\ndifferent directions.",
    "start": "3828650",
    "end": "3834607"
  },
  {
    "text": "And there is less of these--\nfewer of these skills where the ant is just\nlike doing random things.",
    "start": "3834607",
    "end": "3841089"
  },
  {
    "text": "We also see that\nif we were to plot the trajectories that the\nant takes in Diversity",
    "start": "3841090",
    "end": "3847263"
  },
  {
    "text": "is All You Need. In DIAYN algorithm, they will\nlook like this versus in DADS, they're much more--",
    "start": "3847263",
    "end": "3855160"
  },
  {
    "text": "they're much, much\neasier to predict, right? They're a little bit simpler. If the ant decide to go\nin a certain direction",
    "start": "3855160",
    "end": "3861130"
  },
  {
    "text": "it continues going\nin this direction, because that means that then\nthe skill dynamics model is easy to predict, right?",
    "start": "3861130",
    "end": "3866859"
  },
  {
    "text": "It doesn't change its\nmind all the time, right? So that seems useful.",
    "start": "3866860",
    "end": "3872619"
  },
  {
    "text": "But that also means that our\nhumanoid, when we train it on the humanoid, ends up\nwalking in different directions",
    "start": "3872620",
    "end": "3878570"
  },
  {
    "text": "as well.  So now we have skills that\nare may be a little bit more--",
    "start": "3878570",
    "end": "3883970"
  },
  {
    "text": "they are more\npredictable, and maybe they're a little\nbit more useful. But we also had\nthis other problem in DIAYN algorithm\nwhere we didn't really",
    "start": "3883970",
    "end": "3890420"
  },
  {
    "text": "know how to use those skills. So we had to learn another\npolicy on top of it.",
    "start": "3890420",
    "end": "3895790"
  },
  {
    "text": "So now because we have the\nskilled dynamics model, we can actually use that model\nfor model-based planning.",
    "start": "3895790",
    "end": "3901660"
  },
  {
    "text": "The same way as you did in\nthe model-based reinforcement learning lecture.",
    "start": "3901660",
    "end": "3907369"
  },
  {
    "text": "So we can do the following. We can consider different z's.",
    "start": "3907370",
    "end": "3913900"
  },
  {
    "text": "And for every z, we can\npredict what kind of states we are going to see as you did\nin model based-reinforcement",
    "start": "3913900",
    "end": "3920380"
  },
  {
    "text": "learning. Then the computer estimate\nthe cumulative reward for all of them for\na particular task, and then update the planner.",
    "start": "3920380",
    "end": "3927070"
  },
  {
    "text": "So now we can use a\nplanning procedure to tell us which\nskill to actually use because for every skill\nwe can kind of unfold it,",
    "start": "3927070",
    "end": "3934090"
  },
  {
    "text": "see what kind of\nfuture we are going to see with that skill using\nour skill dynamics model, and then we can see how that\ncorresponds to the reward",
    "start": "3934090",
    "end": "3941080"
  },
  {
    "text": "that we actually care about. And we don't need to do\nreinforcement learning here, we can just do planning.",
    "start": "3941080",
    "end": "3946740"
  },
  {
    "text": " So in this case,\nwe'll be planning",
    "start": "3946740",
    "end": "3951990"
  },
  {
    "text": "for skills, not actions. That's the difference\nto model-based overall. But otherwise, it's\nexactly the same. ",
    "start": "3951990",
    "end": "3959600"
  },
  {
    "text": "And in this case, we also don't\nneed any additional roll outs. We can just think of a\ntask that we want to learn, that we want to adjust\nour skills to and do it",
    "start": "3959600",
    "end": "3966400"
  },
  {
    "text": "in zero-shot because we can just\nimagine what the futures will be for different skills\nand then put them together.",
    "start": "3966400",
    "end": "3973700"
  },
  {
    "text": "And that actually results\nin this task where we can have the ant go to\ndifferent goal locations,",
    "start": "3973700",
    "end": "3981820"
  },
  {
    "text": "and we can do this without\nany additional roll outs. So we can just optimize offline\nusing the planning algorithm",
    "start": "3981820",
    "end": "3988809"
  },
  {
    "text": "to decide which skills we should\nbe using at any given point. And then we can find the\nright skills offline,",
    "start": "3988810",
    "end": "3996580"
  },
  {
    "text": "and then just\nexecute those skills, and the ant is able to navigate\nto all these different points.",
    "start": "3996580",
    "end": "4002310"
  },
  {
    "text": "We can also do the same of the\nhumanoid, where we can optimize for which skills to\nuse offline, and then use those online to tell\nthe humanoid where to go,",
    "start": "4002310",
    "end": "4010920"
  },
  {
    "text": "what skill to use. So we don't need\nto do reinforcement learning at this point,\nwe can do it all offline. ",
    "start": "4010920",
    "end": "4017520"
  },
  {
    "text": "So a quick summary. We talked about two skill\ndiscovery algorithms that both use\nmutual information, just slightly different\nmutual information.",
    "start": "4017520",
    "end": "4024230"
  },
  {
    "text": "And we saw how the choice\nof mutual information actually influences\nthe algorithm.",
    "start": "4024230",
    "end": "4029460"
  },
  {
    "text": " In particular, in DADS we\nuse predictability of a skill",
    "start": "4029460",
    "end": "4036240"
  },
  {
    "text": "as a proxy for a\nskill being useful. ",
    "start": "4036240",
    "end": "4041970"
  },
  {
    "text": "And then DADS\ndesigned this method that optimizes for both\npredictability and diversity. And then we showed that we\ncan use model-based planning",
    "start": "4041970",
    "end": "4049160"
  },
  {
    "text": "in the skill space after this. And then as one of\nyou already mentioned",
    "start": "4049160",
    "end": "4054710"
  },
  {
    "text": "this opens up new avenues\nsuch as unsupervised meta reinforcement learning or\nunsupervised multi-task",
    "start": "4054710",
    "end": "4060319"
  },
  {
    "text": "reinforcement\nlearning, and so on. Where we can just-- and this is\nactually describing this paper",
    "start": "4060320",
    "end": "4065390"
  },
  {
    "text": "here by Abhishek Gupta et\nall showing how you can just have an unsupervised reward\nfunction like you have",
    "start": "4065390",
    "end": "4071420"
  },
  {
    "text": "in the case of DIAYN or\nDADS and have the agent just explore unsupervisely\nand then do meta-learning in those tasks.",
    "start": "4071420",
    "end": "4077730"
  },
  {
    "text": "So now you can do\nunsupervised reinforcement learning where the agent is\ncoming up with its own tasks. ",
    "start": "4077730",
    "end": "4085240"
  },
  {
    "text": "Cool. Are there any questions\nat this point?",
    "start": "4085240",
    "end": "4090280"
  },
  {
    "text": "OK, great. So we have 10 minutes left. Let's talk about hierarchical\nreinforcement learning.",
    "start": "4090280",
    "end": "4096930"
  },
  {
    "text": "So we know the motivations. We can perform tasks at\nvarious levels of abstraction. It might potentially also\nhelp with exploration.",
    "start": "4096930",
    "end": "4104040"
  },
  {
    "text": "So it's actually\nquite tricky to talk about hierarchical\nreinforcement learning. There's a lot of\nalgorithms out there. So I thought that instead\nof presenting just one",
    "start": "4104040",
    "end": "4111990"
  },
  {
    "text": "or two of them, I'll just tell\nyou about the design choices that you can make when\nthinking about hierarchical",
    "start": "4111990",
    "end": "4117899"
  },
  {
    "text": "reinforcement\nlearning algorithms, and then we'll go\nthrough some examples. And we'll pick what\nchoices the ant just made.",
    "start": "4117899",
    "end": "4124089"
  },
  {
    "text": "So I'll be asking you to tell me\nwhat choices the ant just made here. All right.",
    "start": "4124090",
    "end": "4129540"
  },
  {
    "text": "So if we were to design just\na hierarchical reinforcement learning algorithm, it might\nlook like something like this.",
    "start": "4129540",
    "end": "4136200"
  },
  {
    "text": "We have an environment. Environment produces an action. And then we have a policy.",
    "start": "4136200",
    "end": "4141540"
  },
  {
    "text": "This has this superscript\nL, which means this is a low-level policy. So it takes the state\nfrom the environment",
    "start": "4141540",
    "end": "4147770"
  },
  {
    "text": "and then acts on\nthat environment. And then it takes the\nnext state from that and it acts on that, and so on.",
    "start": "4147770",
    "end": "4153290"
  },
  {
    "text": "Just like in normal\nreinforcement learning. But now in addition\nto this, we'll also have a high\nlevel policy, pi h.",
    "start": "4153290",
    "end": "4160609"
  },
  {
    "text": "And the high-level policy\nwill take the initial state. And then it will command\nwhat kind of skill",
    "start": "4160609",
    "end": "4166880"
  },
  {
    "text": "should the a low\nlevel policy be doing. And it can command it for a\ncertain number of time steps",
    "start": "4166880",
    "end": "4172189"
  },
  {
    "text": "or it can command it up until\nthe low-level policy says it's done. But it basically\nplans or produces",
    "start": "4172189",
    "end": "4180290"
  },
  {
    "text": "actions that are a little bit on\na higher level of abstraction.",
    "start": "4180290",
    "end": "4185649"
  },
  {
    "text": "And then we will-- let's say this policy only gets\nto act for three time steps. So then after this we'll\nget the next state.",
    "start": "4185649",
    "end": "4191799"
  },
  {
    "text": "And then this high\nlevel policy given that state will need to say,\nwell, what should the low level policy be doing now?",
    "start": "4191800",
    "end": "4197140"
  },
  {
    "text": "All right. So there are four\ndifferent design choices that I want to show you\nor four different axes.",
    "start": "4197140",
    "end": "4205370"
  },
  {
    "text": "So first, the one decision\nthat people often make is whether the low-level\npolicy is goal conditioned",
    "start": "4205370",
    "end": "4211590"
  },
  {
    "text": "or not goal conditioned. All right? So you can have this low-level\npolicy be conditioned",
    "start": "4211590",
    "end": "4216969"
  },
  {
    "text": "on particular goals. So then-- so like a state\nthat you want to get to. So then the high-level policy\nneeds to produce a state here.",
    "start": "4216970",
    "end": "4225400"
  },
  {
    "text": "So a particular state\nthat you need to get to. And then the\nlow-level policy just can do goal condition\nreinforcement learning.",
    "start": "4225400",
    "end": "4234100"
  },
  {
    "text": "But it doesn't have\nto command states. It can be some\nabstract z, like we had in the case of\nDIAYN, for instance,",
    "start": "4234100",
    "end": "4239990"
  },
  {
    "text": "that we don't really know\nexactly what it means, but it commands\nit to the policy, and then policy needs\nto act accordingly.",
    "start": "4239990",
    "end": "4245940"
  },
  {
    "text": "So that's one decision. The other decision\nis whether you may want to have a pre-trained\npolicy in the low level",
    "start": "4245940",
    "end": "4252900"
  },
  {
    "text": "or whether we want to train both\nof these policies end to end. So in the case of DIAYN, and\nin \"Diversity all You Need,\"",
    "start": "4252900",
    "end": "4260070"
  },
  {
    "text": "is we would first pre-train\nthe low-level policy to do all kinds of\ndifferent things given z. And then we'll in addition\nto this, afterwards,",
    "start": "4260070",
    "end": "4266850"
  },
  {
    "text": "train a high level policy\nthat just commands this z's. So we had a pre-trained\nlow-level policy",
    "start": "4266850",
    "end": "4272278"
  },
  {
    "text": "and the high level policy\nwas trained afterwards. We can also do it\nall end to end. ",
    "start": "4272278",
    "end": "4280160"
  },
  {
    "text": "Then the next choice is we\ncan have the low-level policy self terminate, or act for a\ncertain number of time steps.",
    "start": "4280160",
    "end": "4289500"
  },
  {
    "text": "So in this case, it's acting for\nthree time steps and that's it. So this is the\nbudget that it has. But it could be also that the\nlow-level policy has an action",
    "start": "4289500",
    "end": "4297470"
  },
  {
    "text": "that says, we'll terminate now. And if it terminates that's\nwhen the high-level policy gets to act again.",
    "start": "4297470",
    "end": "4302720"
  },
  {
    "text": " And then we have the choice\nthat we usually have, which is,",
    "start": "4302720",
    "end": "4310160"
  },
  {
    "text": "are they learning on\npolicy or off policy? That depends on the\nalgorithm you use.",
    "start": "4310160",
    "end": "4316007"
  },
  {
    "text": "So let's run through\nthis exercise. So I'll be asking you-- I'll be giving--\ndescribing work--",
    "start": "4316007",
    "end": "4321690"
  },
  {
    "text": "some work on hierarchical\nreinforcement learning briefly and then I'll ask you what kind\nof choices the ant just made.",
    "start": "4321690",
    "end": "4326980"
  },
  {
    "text": "So let me do the first one\nto explain how this works. So let's consider\nthe DIAYN approach.",
    "start": "4326980",
    "end": "4333610"
  },
  {
    "text": "So in this case,\nthe low-level policy is not a goal conditioned. It's conditioned on this\nabstract variable z.",
    "start": "4333610",
    "end": "4339900"
  },
  {
    "text": "The low level is pre-trained. It's not trained end to end. We didn't discuss this,\nbut actually in DIAYN, they",
    "start": "4339900",
    "end": "4348240"
  },
  {
    "text": "were running the\nlow-level policy at the fixed rate for a\nfixed number of time steps. It wasn't self-terminating.",
    "start": "4348240",
    "end": "4354570"
  },
  {
    "text": "And it was all trained\non policy as well. ",
    "start": "4354570",
    "end": "4359730"
  },
  {
    "text": "So let's go through a\nfew of these examples, and I'll be asking you what\ndesign choices did the ant just",
    "start": "4359730",
    "end": "4365269"
  },
  {
    "text": "make. So this is what are called\nLearning Locomotor Controllers by Nicolas Heess et al.",
    "start": "4365270",
    "end": "4371360"
  },
  {
    "text": "And then the works\nis as follows. So this is the little-- this is a little\nscreenshot from the paper.",
    "start": "4371360",
    "end": "4376940"
  },
  {
    "text": "We have a high-level controller\nor a high-level policy that is a recurrent controller. And then we have a low-level\ncontroller right here.",
    "start": "4376940",
    "end": "4386540"
  },
  {
    "text": "Now, this command for\nthe low-level controller is updated every K steps.",
    "start": "4386540",
    "end": "4392060"
  },
  {
    "text": "So it asks it to act for K\nsteps and then it asks it again.",
    "start": "4392060",
    "end": "4397680"
  },
  {
    "text": "And also uses\nproprioceptive information. And there is additional\ntask specific information",
    "start": "4397680",
    "end": "4402875"
  },
  {
    "text": "that is fed only to the\nhigh-level controller but not to the\nlow-level controller. So the high-level controller\nhas some privileged information.",
    "start": "4402875",
    "end": "4409220"
  },
  {
    "text": " In addition, both the\nhigh-level controller",
    "start": "4409220",
    "end": "4414540"
  },
  {
    "text": "and the low-level controllers\nare trained separately. So they first train the\nlow-level thing, and then the high-level thing\non top of that.",
    "start": "4414540",
    "end": "4421450"
  },
  {
    "text": "They are both trained\nwith policy gradients. And they add hierarchical\nnoise to this controller",
    "start": "4421450",
    "end": "4427980"
  },
  {
    "text": "for exploration purposes. So then they can show\nsomething like this,",
    "start": "4427980",
    "end": "4433440"
  },
  {
    "text": "where they have\nthis snake robot, and they can show\nthat by just inputting the noise to the high\nlevel controller,",
    "start": "4433440",
    "end": "4439980"
  },
  {
    "text": "it actually results in\nsome sensible behavior. So we can have some exploration\nbenefits from this hierarchy.",
    "start": "4439980",
    "end": "4447512"
  },
  {
    "text": "And then they show some\nresults showing that actually works fairly well. So the question to you\nis it goal conditioned",
    "start": "4447512",
    "end": "4455570"
  },
  {
    "text": "or not goal conditioned,\nthe low-level policy? Maybe just say one,\ngoal condition.",
    "start": "4455570",
    "end": "4461909"
  },
  {
    "text": "Who thinks it's\na goal condition? Raise your hand. OK, just a few people. Who think that it's\nnot a goal condition?",
    "start": "4461910",
    "end": "4469680"
  },
  {
    "text": "Any more people? OK, yeah. It's not a goal condition. We didn't explicitly\ntalk about it, but it just sends a z\ncommands to the low-level--",
    "start": "4469680",
    "end": "4477150"
  },
  {
    "text": "into the low-level policy. Is it pre-trained or\nend to end trained? Is it pre-trained? Who thinks it's pre-trained?",
    "start": "4477150",
    "end": "4484860"
  },
  {
    "text": "Right, quite a few. Who thinks it's end to end? Nobody. Yep, it's pre-trained.",
    "start": "4484860",
    "end": "4490908"
  },
  {
    "text": " That's a little too far. Well, yeah, it's fixed rate.",
    "start": "4490908",
    "end": "4497140"
  },
  {
    "text": "Is it on-policy or off-policy. Who thinks it's on policy? More or less everybody.",
    "start": "4497140",
    "end": "4502820"
  },
  {
    "text": "Who thinks it's off-policy? Nobody. Yeah, it's trained using policy\ngradient, so it's on-policy.",
    "start": "4502820",
    "end": "4509600"
  },
  {
    "text": "Another work. \"Option-Critic Architecture\"\nwork by Bacon et al.",
    "start": "4509600",
    "end": "4514610"
  },
  {
    "text": "So first an option is\ndescribed as following. It's actually used fairly often\nin hierarchical reinforcement",
    "start": "4514610",
    "end": "4521780"
  },
  {
    "text": "learning. It's written by this--\nas denoted by this omega. It's a triple in which this I\nomega is the initiation set.",
    "start": "4521780",
    "end": "4529820"
  },
  {
    "text": "So that's when the\noption can be activated. Then it also has this\npolicy, pi omega,",
    "start": "4529820",
    "end": "4535679"
  },
  {
    "text": "which is a policy, an\nintra-option policy. So this is the policy,\nthe low-level policy",
    "start": "4535680",
    "end": "4540740"
  },
  {
    "text": "that actually gets to act\non that low-level state. And then we have this\nbeta omega, which",
    "start": "4540740",
    "end": "4545960"
  },
  {
    "text": "is a termination function. So this is when the\noption knows when it can terminate, all right?",
    "start": "4545960",
    "end": "4552740"
  },
  {
    "text": "So here's a little plot\nfrom this paper showing a standard MDP where we\njust act at every time step",
    "start": "4552740",
    "end": "4558260"
  },
  {
    "text": "and we have options. The low-level option gets\nto act at every time step, but the option itself\ncan jump between them.",
    "start": "4558260",
    "end": "4566000"
  },
  {
    "text": "It's like a high-level policy. And they also--\nthis is a screenshot from the paper, where they have\nan environment that gives you",
    "start": "4566000",
    "end": "4573070"
  },
  {
    "text": "states. And then they have a\npolicy of option here-- over options that\ndecides which kind",
    "start": "4573070",
    "end": "4578920"
  },
  {
    "text": "of option, which kind of\npolicy the low-level option policy gets to go. So it decides whether it's\nthis policy, this policy,",
    "start": "4578920",
    "end": "4585219"
  },
  {
    "text": "or that policy. A few more facts.",
    "start": "4585220",
    "end": "4590750"
  },
  {
    "text": "So option is a\nself-terminating mini policy. So the option itself can\ndecide when to terminate.",
    "start": "4590750",
    "end": "4597100"
  },
  {
    "text": "Everything here was\ntrained together. Both the high-level\npolicy over options, as well as the\noptions themselves",
    "start": "4597100",
    "end": "4602740"
  },
  {
    "text": "with policy gradients. And here are some\nvisualizations showing that the actual termination\nconditions that it learned",
    "start": "4602740",
    "end": "4610180"
  },
  {
    "text": "in this four-room\ngrid world actually correspond very often to\nthe exit states, which",
    "start": "4610180",
    "end": "4616330"
  },
  {
    "text": "is kind of interesting. And then they show some\nadditional results. All right, so let's go\nover to design choices.",
    "start": "4616330",
    "end": "4621610"
  },
  {
    "text": "Who thinks it's\ngoal conditioned? Who thinks it's not\ngoal conditioned?",
    "start": "4621610",
    "end": "4627100"
  },
  {
    "text": "Yeah, pretty much everybody. Great. Who thinks it's pre-trained?",
    "start": "4627100",
    "end": "4632310"
  },
  {
    "text": "Who thinks it's end to end? Everyone. Great. It's all too easy. Who thinks it's\nself terminating?",
    "start": "4632310",
    "end": "4639144"
  },
  {
    "text": "Pretty much everyone. Yeah, that's correct. Who thinks it's on-policy?",
    "start": "4639145",
    "end": "4644250"
  },
  {
    "text": "Yep, pretty much\neveryone as well. Great. Cool. All right. This one was an easy one. All right. Another one.",
    "start": "4644250",
    "end": "4649650"
  },
  {
    "text": "Relay policy learning, work\nby Abhishek Gupta et all. So it works as following.",
    "start": "4649650",
    "end": "4655060"
  },
  {
    "text": "We have a low-level policy\nand a high-level policy. The high-level policy is\nsending goals, the actual state",
    "start": "4655060",
    "end": "4661440"
  },
  {
    "text": "goals to the\nlow-level policy that then gets to act for a\ncertain number of time steps.",
    "start": "4661440",
    "end": "4666780"
  },
  {
    "text": "Then it does goal\ncondition learning for the low-level\npolicy as well as goal condition learning for\nthe high-level policy using",
    "start": "4666780",
    "end": "4672929"
  },
  {
    "text": "hindsight relabeling actually.  So the way this is trained is\nfirst we collect demonstrations",
    "start": "4672930",
    "end": "4681370"
  },
  {
    "text": "in the environment doing all\nkinds of different things. So for instance in this\nkitchen environment,",
    "start": "4681370",
    "end": "4686860"
  },
  {
    "text": "it can be robot going to all\nkinds of different locations. And then we do data\nrelabeling, where",
    "start": "4686860",
    "end": "4693340"
  },
  {
    "text": "we can take a long trajectory\nand relabel the high-level as well as the low-level goals\nfor both the high-level policy",
    "start": "4693340",
    "end": "4699850"
  },
  {
    "text": "and the low-level policy. Then we start the\nimitation learning and then we continue\nwith policy gradients",
    "start": "4699850",
    "end": "4705820"
  },
  {
    "text": "to do fine tuning using a reward\nfunction for both low level and a high level policy. ",
    "start": "4705820",
    "end": "4714079"
  },
  {
    "text": "So a few more facts as\ngoal condition policies with relabeling. We use demonstrations\nto pre-train everything.",
    "start": "4714080",
    "end": "4722480"
  },
  {
    "text": "And well it's done on a\npolicy of policy gradients. And here are some results.",
    "start": "4722480",
    "end": "4728820"
  },
  {
    "text": "So here is a long horizon goal\nthat we're trying to achieve. And you can see that\nrelay policy learning",
    "start": "4728820",
    "end": "4734930"
  },
  {
    "text": "is able to do this fairly well. So it opens the microwave. Moves the kettle. Opens the cabinet and so on,\nand some other equivalents",
    "start": "4734930",
    "end": "4741559"
  },
  {
    "text": "are not able to do that. So as a goal\ncondition we're not. Who thinks it's\ngoal conditioned?",
    "start": "4741560",
    "end": "4749810"
  },
  {
    "text": "Yep, it's goal conditioned. Who thinks it's pre-trained? Most of the people. Who think it's end\nto end trained?",
    "start": "4749810",
    "end": "4757380"
  },
  {
    "text": "One person. Yeah, so it's actually\ntrained end to end. It's pre-trained in\nterms of both policies",
    "start": "4757380",
    "end": "4762510"
  },
  {
    "text": "are pre-trained\nwith demonstrations, but they are still learning\ntogether at all times. Is it self terminating?",
    "start": "4762510",
    "end": "4768255"
  },
  {
    "text": "Who thinks it's\nself terminating? Nobody. Who thinks it's fixed rate?",
    "start": "4768255",
    "end": "4773429"
  },
  {
    "text": "Yep. Who thinks it's on policy? Yep, pretty much everyone.",
    "start": "4773430",
    "end": "4779730"
  },
  {
    "text": "Cool. We have one more algorithm. But I think we're\nrunning out of time. So I'll skip this one.",
    "start": "4779730",
    "end": "4786570"
  },
  {
    "text": "And a quick summary. So we have multiple design\nchoices and frameworks for hierarchical\nreinforcement learning.",
    "start": "4786570",
    "end": "4794369"
  },
  {
    "text": "It usually helps\nwith exploration and with temporally\nextended tasks, with these long-horizon tasks,\nlike you've seen in the kitchen",
    "start": "4794370",
    "end": "4800253"
  },
  {
    "text": "environment. But it can be quite\ndifficult to get it to work. So you're adding an\nadditional policy.",
    "start": "4800253",
    "end": "4806643"
  },
  {
    "text": "You have to optimize for\nboth of the policies. Sometimes the low-level policy\nis kind of changing underneath your feet--",
    "start": "4806643",
    "end": "4811710"
  },
  {
    "text": "under your feet so you have\nto accommodate for this. If you use an off-policy\nalgorithm and so on. So it's actually quite\ntricky to get it to work.",
    "start": "4811710",
    "end": "4819750"
  },
  {
    "text": "And it also seems like a\nnatural extension for harder RL problems where we have\nto do longer horizon tasks that often it's quite\ntricky to get to work.",
    "start": "4819750",
    "end": "4828570"
  },
  {
    "text": "And there's actually some\ninteresting work talking about why hierarchy sometimes works\nand they do a decent analysis",
    "start": "4828570",
    "end": "4835530"
  },
  {
    "text": "showing in what cases it\nhelps, and where it doesn't. All right. ",
    "start": "4835530",
    "end": "4840810"
  },
  {
    "text": "So we talked about these\nfour different things. Thank you very much\nfor your attention. And just to remind\nyou, on Monday",
    "start": "4840810",
    "end": "4847679"
  },
  {
    "text": "we have the homework 4 due. On Wednesday project\nmilestone is due. And next week, please,\nattend the two guest lectures",
    "start": "4847680",
    "end": "4853469"
  },
  {
    "text": "that are going to be\nreally interesting I think. Thank you. ",
    "start": "4853470",
    "end": "4862000"
  }
]