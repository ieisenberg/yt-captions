[
  {
    "text": " Today, we're going to be talking\nabout transfer learning, which",
    "start": "0",
    "end": "6810"
  },
  {
    "text": "we didn't quite have\ntime to get to last time. And in particular, we'll talk\nabout the problem formulation",
    "start": "6810",
    "end": "13260"
  },
  {
    "text": "and fine-tuning, which is a very\nsuccessful and popular approach to transfer learning.",
    "start": "13260",
    "end": "19920"
  },
  {
    "text": "And then we'll start\ndiving into meta-learning. We'll talk about the\nproblem formulation--",
    "start": "19920",
    "end": "25000"
  },
  {
    "text": "a general recipe of\nmeta-learning algorithms. One class of approaches, which\nI call black box adaptation",
    "start": "25000",
    "end": "31260"
  },
  {
    "text": "approaches. In the time, we'll\nalso go through a case study of GPT-3, which can\nbe viewed as a black box",
    "start": "31260",
    "end": "38630"
  },
  {
    "text": "meta-learner. And then by the\nend of the lecture, hopefully, you'll be\nable to understand the differences between\nmulti-task learning,",
    "start": "38630",
    "end": "45000"
  },
  {
    "text": "transfer learning,\nand meta-learning, some of the basics\nof transfer learning, as well as the training set\nup for few-shot meta-learinig",
    "start": "45000",
    "end": "51840"
  },
  {
    "text": "algorithms, and how to implement\nblack box meta-learning techniques, which you'll\nbe doing in the first mark.",
    "start": "51840",
    "end": "58810"
  },
  {
    "text": "Great, so let's get started\nwith transfer learning. So I recall last\ntime when we talked,",
    "start": "58810",
    "end": "66490"
  },
  {
    "text": "we went over\nmulti-task learning. And the goal of the\nmulti-task learning problem is to solve multiple\ntasks at once.",
    "start": "66490",
    "end": "73850"
  },
  {
    "text": "So you have some number\nof tasks, maybe t tasks. You have a dataset\nfor each task. And you're trying to\nlearn a single model",
    "start": "73850",
    "end": "80380"
  },
  {
    "text": "with parameters theta that\ncan solve those tasks. And some of these\nparameters might be shared.",
    "start": "80380",
    "end": "86500"
  },
  {
    "text": "Some of them might be, actually,\nnot shared across tasks.",
    "start": "86500",
    "end": "91690"
  },
  {
    "text": "The goal is just to be able to\ndo well on the different tasks, on all the tasks that\nyou're given in the dataset.",
    "start": "91690",
    "end": "99430"
  },
  {
    "text": "Now, what does\ntransfer look like? What does transfer learning\nlook like in comparison to this?",
    "start": "99430",
    "end": "104810"
  },
  {
    "text": "So the goal of\ntransfer learning is to solve a target task,\nwhich I'll denote as task b,",
    "start": "104810",
    "end": "114399"
  },
  {
    "text": "after solving a\nsource task, task a. And in particular,\nthe goal here is",
    "start": "114400",
    "end": "119530"
  },
  {
    "text": "to kind of transfer\nsome knowledge that you learned, that you kind\nof acquired when learning task a when trying to solve task b.",
    "start": "119530",
    "end": "128710"
  },
  {
    "text": "And kind of one key assumption\ntransfer learning methods typically make is that\nyou can't actually",
    "start": "128710",
    "end": "133900"
  },
  {
    "text": "access the data for task a\nduring this transfer process.",
    "start": "133900",
    "end": "140280"
  },
  {
    "text": "So you basically have to\nlearn something about task a, then you can no longer\naccess that data. But you can access the\nknowledge that you've learned",
    "start": "140280",
    "end": "146430"
  },
  {
    "text": "when trying to solve task b. ",
    "start": "146430",
    "end": "151790"
  },
  {
    "text": "And so, transfer learning\nis actually a valid solution to multi-task learning. You could learn one task,\nand then learn the next task,",
    "start": "151790",
    "end": "158000"
  },
  {
    "text": "but not vice versa because\nmulti-task learning assumes that you\nhave all of the data",
    "start": "158000",
    "end": "163700"
  },
  {
    "text": "available when\nyou're solving them.  So a question for you to\nget started for today.",
    "start": "163700",
    "end": "171130"
  },
  {
    "text": "You can answer in chat\nor raise your hand. What are some problems\nor applications where transfer learning\nmight make sense,",
    "start": "171130",
    "end": "178690"
  },
  {
    "text": "but where multi-task learning\nmight not make sense? Any ideas or thoughts?",
    "start": "178690",
    "end": "183795"
  },
  {
    "start": "183795",
    "end": "191370"
  },
  {
    "text": "Sorry, I just had a question. So can you use the data for\nthe problem from task b?",
    "start": "191370",
    "end": "198046"
  },
  {
    "text": "Or is it just nothing at all? You can use the\ndata from task b. So kind of typically, you assume\nthat you've learned something",
    "start": "198046",
    "end": "205990"
  },
  {
    "text": "about task a. You have that\nknowledge, and then you also have a\ndataset from task b, and you want to leverage\nboth your previous knowledge",
    "start": "205990",
    "end": "212440"
  },
  {
    "text": "and the data from task b to\nlearn a model for task b. Great, thank you. ",
    "start": "212440",
    "end": "222180"
  },
  {
    "text": "Say we want to follow\ninstructions in one house, and I'm giving it to all\ninstructions in another house.",
    "start": "222180",
    "end": "229849"
  },
  {
    "text": "Yeah, that's a good example. So maybe the robot has learned\nto do one task in one house,",
    "start": "229850",
    "end": "234863"
  },
  {
    "text": "and now you want to\ntransfer what it's learned to that towards a new task. But you don't want to\nhave to kind of retrain it on that previous\nhouse, you just want",
    "start": "234863",
    "end": "240620"
  },
  {
    "text": "to do all in the next house.  Do you also have thoughts?",
    "start": "240620",
    "end": "247370"
  },
  {
    "text": "Yeah. I was going to say\nsomething similar transferring the robot\nin a simulated domain and then transferring it\nover to a real world domain.",
    "start": "247370",
    "end": "255690"
  },
  {
    "text": "Great. Yeah, so maybe you've trained\nsomething in simulation, you've learned\nsomething about physics,",
    "start": "255690",
    "end": "261049"
  },
  {
    "text": "or how to perform a\ntask, and then you want to kind of leverage\nthat knowledge when learning",
    "start": "261050",
    "end": "266300"
  },
  {
    "text": "something in the real world. But maybe you don't want to\nhave to retrain on simulation throughout the process\nbecause you really",
    "start": "266300",
    "end": "272922"
  },
  {
    "text": "care about learning\nsomething in the real world and not learning\nsomething in simulation. Looking at the\nchat, there's also",
    "start": "272922",
    "end": "281090"
  },
  {
    "text": "some thoughts about kind of\ntransferring from one hospital to another hospital\nwhen you're looking at kind of medical data.",
    "start": "281090",
    "end": "287178"
  },
  {
    "text": "You may not be able to access\nthe previous hospital's images, but you want to be able\nto kind of leverage what you learn there.",
    "start": "287178",
    "end": "294610"
  },
  {
    "text": "Also, things like transferring\nfrom learning a simpler task like picking up a\nblock to a harder task,",
    "start": "294610",
    "end": "301263"
  },
  {
    "text": "like picking up an\nirregular shaped object.  Personalization, well,\nI think, actually, it",
    "start": "301263",
    "end": "307810"
  },
  {
    "text": "falls well within the\nmeta-learning context. And we'll, essentially,\ntalk a bit about that later. ",
    "start": "307810",
    "end": "314950"
  },
  {
    "text": "Yeah, and then\nanother example is when you have a small\namount of data for task b, and task a and b are similar.",
    "start": "314950",
    "end": "322300"
  },
  {
    "text": "Also, image style transfer,\nand some other settings. And these are all good examples.",
    "start": "322300",
    "end": "329580"
  },
  {
    "text": "And then, yeah,\nanother example is like, maybe, you have kind\nof limited compute power, and so you don't want to have\nto keep on training on task a",
    "start": "329580",
    "end": "336870"
  },
  {
    "text": "when transferring to task b. So yeah, what I wrote\non the slide-- first",
    "start": "336870",
    "end": "343480"
  },
  {
    "text": "is something that you\nguys came up with, which is if this data\nset Da is very large,",
    "start": "343480",
    "end": "351820"
  },
  {
    "text": "and you only have kind of\na limited compute budget, then you don't want to have\nto retrain on that dataset",
    "start": "351820",
    "end": "358419"
  },
  {
    "text": "when transferring\nto the second task. Another example is we\ndon't care about solving",
    "start": "358420",
    "end": "365510"
  },
  {
    "text": "both task a and task\nb simultaneously. You only care about,\nreally, the thing that you're transferring to.",
    "start": "365510",
    "end": "371150"
  },
  {
    "text": "The sim to real example\nis like one good example of this, where you don't\nreally ultimately care about getting a policy\nthat works in simulation.",
    "start": "371150",
    "end": "376910"
  },
  {
    "text": "You care more about\nsolving the target task. And in this case, it's\nfine to kind of discard",
    "start": "376910",
    "end": "382039"
  },
  {
    "text": "what you have\nlearned from task a, and just try to use that\nknowledge when learning task b.",
    "start": "382040",
    "end": "387980"
  },
  {
    "text": "OK, and then, also, I guess,\none thing that isn't here is if there's some\nissue with kind of being able to access\nthe dataset from task a,",
    "start": "387980",
    "end": "394789"
  },
  {
    "text": "then this is also a scenario\nwhere that might make sense. OK.",
    "start": "394790",
    "end": "400597"
  },
  {
    "text": "Yeah, and then there's\nalso a question in the chat about is there any kind of\ndifferential privacy concerns when it comes to\ntransfer learning,",
    "start": "400597",
    "end": "406730"
  },
  {
    "text": "or is inferring\nthe data to train the neural network\ndifficult enough to render this irrelevant? This is actually a\nreally good question.",
    "start": "406730",
    "end": "412759"
  },
  {
    "text": "If one of the reasons why\nyou're using transfer learning is because you have to\nthrow away your data set,",
    "start": "412760",
    "end": "418430"
  },
  {
    "text": "Da, because you just-- like for privacy reasons\nor something else, then there are, potentially,\nconcerns about well, maybe,",
    "start": "418430",
    "end": "426770"
  },
  {
    "text": "if you're throwing\naway the dataset a, but you're still retaining the\nmodel that was trained on Da. You may actually be\nable to reconstruct",
    "start": "426770",
    "end": "433880"
  },
  {
    "text": "the data in a, and\nprivate information in a from that model. And so in those contexts\nwhen you do care",
    "start": "433880",
    "end": "440000"
  },
  {
    "text": "about privacy of things\nand the source task, then you do want to think\nabout whether it is actually",
    "start": "440000",
    "end": "446450"
  },
  {
    "text": "possible to reconstruct that\ndata from the model itself, and think carefully about that.",
    "start": "446450",
    "end": "452330"
  },
  {
    "text": " OK.",
    "start": "452330",
    "end": "457490"
  },
  {
    "text": "So this is the transfer\nlearning problem statement, and how it differs with\nmulti-task learning.",
    "start": "457490",
    "end": "465710"
  },
  {
    "text": "And really, the most prevalent\napproach to transfer learning",
    "start": "465710",
    "end": "471199"
  },
  {
    "text": "is something that's called\nfine-tuning, especially within neural networks\nand deep learning.",
    "start": "471200",
    "end": "477783"
  },
  {
    "text": "So the way that\nfine-tuning works is you assume that you have some set of\nparameters that are pre-trained on the source dataset Da.",
    "start": "477783",
    "end": "485840"
  },
  {
    "text": "And then you run gradient\ndescent using the training data",
    "start": "485840",
    "end": "491419"
  },
  {
    "text": "for the new task, task b. So this is kind of the data\ncorresponding to task b,",
    "start": "491420",
    "end": "498530"
  },
  {
    "text": "or, at least, the training\ndata according to task b. And just typically, you don't\njust run one gradient step.",
    "start": "498530",
    "end": "503900"
  },
  {
    "text": "You typically run\nmany gradient steps on this new task or\nthis new dataset.",
    "start": "503900",
    "end": "510949"
  },
  {
    "text": "That's it. It's quite simple. And it actually works\nquite well in a number",
    "start": "510950",
    "end": "516522"
  },
  {
    "text": "of different\ninstances, especially when there's a lot of kind of\nrich information and knowledge in your source data set Da.",
    "start": "516523",
    "end": "522380"
  },
  {
    "text": " So for example, if you\npre-trained on ImageNet,",
    "start": "522380",
    "end": "528139"
  },
  {
    "text": "that this is what this\nrow is right here. And then fine-tune on the Pascal\ndata set and the SUN data set.",
    "start": "528140",
    "end": "538400"
  },
  {
    "text": "These are kind of other image\nclassification benchmarks that are a bit different. You get 58% and 52%\naccuracy, respectively.",
    "start": "538400",
    "end": "547080"
  },
  {
    "text": "Whereas, if you instead\ninitialize your network weights randomly, you get 41% and 35%.",
    "start": "547080",
    "end": "553574"
  },
  {
    "text": "So essentially, all of the\nknowledge coming from ImageNet is quite helpful for\ngetting a higher accuracy on these downstream\ndataset in comparison",
    "start": "553575",
    "end": "561170"
  },
  {
    "text": "to a randomly initialized\nneural network. ",
    "start": "561170",
    "end": "566790"
  },
  {
    "text": "One question is\nwell, OK, say, I want to have some\ndownstream task that I want to be able to\ntransfer to, what",
    "start": "566790",
    "end": "572280"
  },
  {
    "text": "are some good datasets\nfor pre-training and where might I get\npre-training parameters?",
    "start": "572280",
    "end": "578760"
  },
  {
    "text": "ImageNet classification\nhas been one that's kind of been proven\ntime and time again. That's been quite\nhelpful, especially for downstream image-based\ntasks even when your target",
    "start": "578760",
    "end": "587910"
  },
  {
    "text": "task is not actually a natural\nimage classification task.",
    "start": "587910",
    "end": "593141"
  },
  {
    "text": "So people have shown that\nImageNet classification has been helpful for depth's images,\nfor example, and other kinds",
    "start": "593142",
    "end": "599907"
  },
  {
    "text": "of non-natural images.  Another kind of popular\nexample for pre-training",
    "start": "599908",
    "end": "606100"
  },
  {
    "text": "is using language\nmodels that are trained on very large language corpora.",
    "start": "606100",
    "end": "612250"
  },
  {
    "text": "Examples of this are\nBERT and language models. And we'll kind of\nshow some results on that on the next slide.",
    "start": "612250",
    "end": "619360"
  },
  {
    "text": "And then another kind of\njust generally popular, especially I would say\npopular maybe in the 90s",
    "start": "619360",
    "end": "626970"
  },
  {
    "text": "or so was actually\nthese unsupervised learning techniques to get\npre-trained parameters, and then fine-tune\nfrom that, unsupervised",
    "start": "626970",
    "end": "634080"
  },
  {
    "text": "learning techniques like\nautoencoders for example. And really, just whatever\nlarge and diverse ideas",
    "start": "634080",
    "end": "641450"
  },
  {
    "text": "that you might have\nlying around can be quite helpful\nfor pre-training when you want to\ntransfer information",
    "start": "641450",
    "end": "647900"
  },
  {
    "text": "in that large and diverse data\nset to your downstream task. Lastly, a lot of these\npre-trained models",
    "start": "647900",
    "end": "653490"
  },
  {
    "text": "are often available online. So you don't actually have to\neven download the data set.",
    "start": "653490",
    "end": "658530"
  },
  {
    "text": "And to train on that data set,\nyou typically just download a pre-trained model\nfrom the internet, and then fine-tune that model\non the task that you care about.",
    "start": "658530",
    "end": "665690"
  },
  {
    "text": " OK. And then fine-tuning, in\ngeneral, is a bit of an art.",
    "start": "665690",
    "end": "673910"
  },
  {
    "text": "There is a lot of\ndifferent design decisions that go into it with\nregard to which weights you choose to run\ngradient descent on,",
    "start": "673910",
    "end": "682070"
  },
  {
    "text": "how you set the learning\nrate, and so forth. And unfortunately,\nthere hasn't really been any kind of science behind\nwhat happens when you choose",
    "start": "682070",
    "end": "694850"
  },
  {
    "text": "these different design choices. But there are some\ncommon practices. So it's common to fine-tune\nwith a smaller learning rate,",
    "start": "694850",
    "end": "702080"
  },
  {
    "text": "so that you don't\nkind of destroy what is in the pre-trained\nnetwork with a larger learning",
    "start": "702080",
    "end": "707240"
  },
  {
    "text": "rate. It's also common to\nhave a smaller learning rate for the earlier layers. Because often, the earlier\nlayers in the network",
    "start": "707240",
    "end": "713899"
  },
  {
    "text": "have more general\npurpose features, like edge detectors in an\nimage classification network.",
    "start": "713900",
    "end": "719899"
  },
  {
    "text": "It's also common to even\nfreeze earlier layers, setting their\nlearning rate to 0, and then, potentially, gradually\nunfreeze them until you train--",
    "start": "719900",
    "end": "727057"
  },
  {
    "text": "once you train the later parts\nof the network effectively. Sometimes, you also\nreinitialize the last layer",
    "start": "727057",
    "end": "733760"
  },
  {
    "text": "if you want that to learn\nquickly and adapt quickly to your new task. And then in all of\nthese cases, you",
    "start": "733760",
    "end": "739120"
  },
  {
    "text": "can search over these\nvarious design choices and hyperparameters\nwith cross-validation on your downstream target task.",
    "start": "739120",
    "end": "747800"
  },
  {
    "text": "And then, finally, different\narchitectures matter a lot. So for example, a\nnetwork architecture called residual\nnetworks or ResNets",
    "start": "747800",
    "end": "754399"
  },
  {
    "text": "tend to fine-tune more\nquickly, simply because it's easier for gradients to pass\nthrough all of the network",
    "start": "754400",
    "end": "761240"
  },
  {
    "text": "compared to some\nother architectures. OK, so any kind of questions on\nfine-tuning and how it works?",
    "start": "761240",
    "end": "772339"
  },
  {
    "start": "772340",
    "end": "781398"
  },
  {
    "text": "There's a question\nfrom [INAUDIBLE].. Is pre-training in an\nunsupervised learning setting performing\nbetter than ImageNet?",
    "start": "781398",
    "end": "788589"
  },
  {
    "text": "This is a good question. I think that, in general--  as of a year ago, I know that\nImageNet was doing better.",
    "start": "788590",
    "end": "795980"
  },
  {
    "text": "I know that people have\nbeen really actually most recently pushing\non unsupervised learning techniques, and\nalso what's called semi-supervised learning.",
    "start": "795980",
    "end": "803055"
  },
  {
    "text": "In general, I\nthink that ImageNet has been more successful. But I think there have been\nsome recent results that have shown promise in\nunsupervised learning",
    "start": "803055",
    "end": "810460"
  },
  {
    "text": "approaches approaching\npre-training on ImageNet. ",
    "start": "810460",
    "end": "817850"
  },
  {
    "text": "Do you have a question? Yeah. Can you talk a little more about\nsearch over hyperparameters?",
    "start": "817850",
    "end": "825310"
  },
  {
    "text": "How would we do\nthat in practice? Yeah, so one kind of often--",
    "start": "825310",
    "end": "832720"
  },
  {
    "text": "one kind of reasonable\nway to do that in practice is to choose some\nhyperparameters that you think might be reasonable.",
    "start": "832720",
    "end": "838990"
  },
  {
    "text": "Try them out, run fine-tuning\nwith those hyperparameters. And then see what the\nvalidation performance",
    "start": "838990",
    "end": "844240"
  },
  {
    "text": "is on a held out validation\nset for some different choices. Then for the choice\nthat works the best,",
    "start": "844240",
    "end": "850780"
  },
  {
    "text": "you either choose that or\nmaybe refine them even further. There are also more\nautomated techniques for this, such as grid\nsearch, random search,",
    "start": "850780",
    "end": "858040"
  },
  {
    "text": "where you kind of sample values\nwithin some range for all the hyperparameters.",
    "start": "858040",
    "end": "865098"
  },
  {
    "text": "If you have a fair\namount of compute, then that could also\nbe an effective way to choose hyperparameters.",
    "start": "865098",
    "end": "871357"
  },
  {
    "text": "And then, lastly,\nthere's also techniques in Bayesian optimization\nthat try to optimize the hyperparameters for you.",
    "start": "871357",
    "end": "878319"
  },
  {
    "text": "I'm not sure if I\nput the answer right. Yeah, you got it right. So I have this basic question\nabout transfer learning.",
    "start": "878320",
    "end": "885850"
  },
  {
    "text": "So some of the\nexamples make sense, so like on the language\ncorpora where one language have",
    "start": "885850",
    "end": "893750"
  },
  {
    "text": "the [INAUDIBLE] to\ntransfer [INAUDIBLE] on the other language. But some of the examples,\npreviously stated,",
    "start": "893750",
    "end": "900080"
  },
  {
    "text": "like robot motion in one\nsetting versus the other.",
    "start": "900080",
    "end": "905450"
  },
  {
    "text": "How do I think about transfer\nlearning versus generalization? Because like in that\ncase, couldn't I also",
    "start": "905450",
    "end": "912439"
  },
  {
    "text": "think that if my model\nwas generalized enough, that it would be able to\nlearn in a different setting.",
    "start": "912440",
    "end": "920459"
  },
  {
    "text": "And I shouldn't be need to\nfine-tune my [INAUDIBLE] or--",
    "start": "920460",
    "end": "925670"
  },
  {
    "text": "It's a good question. So it's possible that\nyou gave the network the parameters theta\nthemselves, will just",
    "start": "925670",
    "end": "933110"
  },
  {
    "text": "generalize to a new\nsetting if your target task is close enough to the\ntask that you pre-trained on.",
    "start": "933110",
    "end": "940550"
  },
  {
    "text": "And if that is, then\nyou could just kind of test it and see if that works. And if it works, then\nyou don't actually need to do any fine-tuning.",
    "start": "940550",
    "end": "946440"
  },
  {
    "text": "And that's what's called\nzero-shot generalization, in the sense that you don't--\nyou're basically generalizing",
    "start": "946440",
    "end": "952250"
  },
  {
    "text": "to this new context without\nany new data points. And then if that doesn't\nwork, then fine-tuning",
    "start": "952250",
    "end": "957920"
  },
  {
    "text": "is usually kind of\nthe next step that you can use to try to be able to\ndo well in that new setting.",
    "start": "957920",
    "end": "965760"
  },
  {
    "text": "Thanks.  So this is a specific question\nfor large language models,",
    "start": "965760",
    "end": "975634"
  },
  {
    "text": "in that I've read a few papers\nthat kind of tried to fine tune BERT or GPT 2 for example\non some new data set. ",
    "start": "975634",
    "end": "982430"
  },
  {
    "text": "And rather than kind of\nadopting these common practices they started to learn at\na rate that was near 0",
    "start": "982430",
    "end": "988290"
  },
  {
    "text": "and to opt [INAUDIBLE]\nand bring it back down. Is there any intuition for why\nthat is a good thing to do,",
    "start": "988290",
    "end": "994412"
  },
  {
    "text": "why it's a reasonable\nthing to do, I just thought it seemed like\nsome weird kind of black magic. ",
    "start": "994413",
    "end": "1001570"
  },
  {
    "text": "Yeah, so I actually haven't\nread those papers myself. ",
    "start": "1001570",
    "end": "1008279"
  },
  {
    "text": "I guess I don't have any good\nintuition for that, at least",
    "start": "1008280",
    "end": "1014447"
  },
  {
    "text": "off of the top of my head for\nwhy that might make sense. I think that, in general,\nit makes sense for it to have a larger learning\nrate for the later layers",
    "start": "1014447",
    "end": "1020585"
  },
  {
    "text": "because those are\nprobably the things that need to change the most. So maybe when you\namp it up, it's really focusing on\nthose later layers.",
    "start": "1020585",
    "end": "1026589"
  },
  {
    "text": "And then after that\nand general decaying your learning rate\nmakes a lot of sense. But I'm not sure kind of\nramping up the learning rate",
    "start": "1026589",
    "end": "1034660"
  },
  {
    "text": "to be higher for the earlier\nlayers seems like it could also have a detrimental effect. So I'm not sure exactly what\nthe intuition for that is.",
    "start": "1034660",
    "end": "1043209"
  },
  {
    "text": "Thanks. OK.",
    "start": "1043210",
    "end": "1048245"
  },
  {
    "text": "Hi, yeah, my question\nwas about adimensionality and how it ties into transfer\nlearning, like for example,",
    "start": "1048245",
    "end": "1053490"
  },
  {
    "text": "if you do computer\nvision, right, you have your set of kernel sizes. But the images are going to be--",
    "start": "1053490",
    "end": "1059929"
  },
  {
    "text": "if you're adapting\nto a new data set, you could just have completely\ndifferent sized images, or fewer channels, or something.",
    "start": "1059930",
    "end": "1066510"
  },
  {
    "text": "So I guess I was wondering what\nyour intuition was, or let's say you're jumping to dataset\nB, which the images have",
    "start": "1066510",
    "end": "1073710"
  },
  {
    "text": "half the number of channels,\nbut twice the actual resolution. I guess how does\nthat tie into how",
    "start": "1073710",
    "end": "1080490"
  },
  {
    "text": "you decide to free\ncertain layers or reinitialize certain layers? Yeah, so one thing\nthat you could",
    "start": "1080490",
    "end": "1086340"
  },
  {
    "text": "do when handling images of\ndifferent dimensions, when you",
    "start": "1086340",
    "end": "1091900"
  },
  {
    "text": "want to kind of\nfine tune to images with a different dimension is\nto, well, one obvious thing you",
    "start": "1091900",
    "end": "1097725"
  },
  {
    "text": "can do is just to resize\nit to the dimension that's the same as what the pre-trained\nmodel was trained on.",
    "start": "1097725",
    "end": "1103799"
  },
  {
    "text": "But that may be discarding\ninformation or it may be, your images are smaller and\nyou don't have a good way",
    "start": "1103800",
    "end": "1110080"
  },
  {
    "text": "to upsample them. I think the most\nimportant thing to do is to make sure that especially\nif you're using smaller",
    "start": "1110080",
    "end": "1116590"
  },
  {
    "text": "images to make sure that\nwhen your network is kind of decreasing the\ndimensionality of the features",
    "start": "1116590",
    "end": "1122139"
  },
  {
    "text": "that they don't\nbecome too small. If you need to have\nhigher resolution, for example, then\nthe default way",
    "start": "1122140",
    "end": "1130300"
  },
  {
    "text": "to kind of pool\nover dimensionality may not actually work\nfor the smaller images",
    "start": "1130300",
    "end": "1136063"
  },
  {
    "text": "that you're transferring to. Beyond that, I think\nthat in some ways this is kind of a\nanother design choice",
    "start": "1136063",
    "end": "1143380"
  },
  {
    "text": "to see if it works for the\nhigher resolution images or not. I don't think that there's any\nwork that really studies this",
    "start": "1143380",
    "end": "1150550"
  },
  {
    "text": "systematically. Thank you. OK, then one more\nquestion from--",
    "start": "1150550",
    "end": "1156860"
  },
  {
    "text": "sorry. It's related. And so I wanted to\nask what size do",
    "start": "1156860",
    "end": "1161978"
  },
  {
    "text": "we expect for the new data set? So can we do transfer\nlearning for few shot learning and if we can, what are some\npractices you recommend? ",
    "start": "1161978",
    "end": "1171860"
  },
  {
    "text": "Yeah, that's a great\nquestion and actually kind of transitions nicely\ninto the next slide. So these plots are actually from\npre-training with a language",
    "start": "1171860",
    "end": "1181880"
  },
  {
    "text": "model, kind of\nULMFiT is the name of this paper, which is\nUnsupervised Language Model and then Fine Tuning.",
    "start": "1181880",
    "end": "1188870"
  },
  {
    "text": "And what these plots are\nshowing is that on the x-axis is the number of training\nexamples in your target task.",
    "start": "1188870",
    "end": "1194880"
  },
  {
    "text": "So on the right, you have kind\nof a larger target data set and on the left you have\na smaller target data set.",
    "start": "1194880",
    "end": "1201710"
  },
  {
    "text": "And what you see is that\nthe validation error rate on your target task goes\ndown as you have more data.",
    "start": "1201710",
    "end": "1207112"
  },
  {
    "text": "So if you have more\ndata from your target task than these methods,\nthen kind of fine tuning",
    "start": "1207112",
    "end": "1212120"
  },
  {
    "text": "works better. And specifically the\norange and the green curve are showing fine tuning and the\nblue curve is showing training",
    "start": "1212120",
    "end": "1218210"
  },
  {
    "text": "from scratch. But if the number\nof training examples is really small, like if\nyou only have 100 training",
    "start": "1218210",
    "end": "1223940"
  },
  {
    "text": "examples, then fine\ntuning isn't as successful with a substantially\nhigher validation error rate",
    "start": "1223940",
    "end": "1230299"
  },
  {
    "text": "than if you had\nmany more examples. So kind of the\noverall takeaway here",
    "start": "1230300",
    "end": "1236360"
  },
  {
    "text": "is that fine tuning\ndoesn't work quite as well when you have very\nsmall target task data sets.",
    "start": "1236360",
    "end": "1242900"
  },
  {
    "text": "It's most successful when\nyou have a more moderate size data set.",
    "start": "1242900",
    "end": "1248023"
  },
  {
    "text": "But you still don't\nneed something as large as ImageNet\nfor example, with millions of examples. ",
    "start": "1248023",
    "end": "1256920"
  },
  {
    "text": "And so this is also kind\nof where meta-learning can come in and\nkind of allow you",
    "start": "1256920",
    "end": "1263460"
  },
  {
    "text": "to be able to generalize\nbetter when you have a very small amount of data. So that's what we'll\ntalk about next.",
    "start": "1263460",
    "end": "1270260"
  },
  {
    "text": "Do you have a question? Yes, I think I still have eight. I keep bringing\n[INAUDIBLE] can you",
    "start": "1270260",
    "end": "1276582"
  },
  {
    "text": "explain why a small\nnumber of examples",
    "start": "1276582",
    "end": "1282470"
  },
  {
    "text": "means that the fine\ntuning doesn't perform well up on these graphs? Yeah, so what I mean by that\nis that when you have 100",
    "start": "1282470",
    "end": "1290169"
  },
  {
    "text": "examples, your validation\nerror rate for orange and green is around 30 to 40,\nwhereas if you have,",
    "start": "1290170",
    "end": "1297340"
  },
  {
    "text": "say, 1,000 examples, then\nyour validation error rate is closer to 10 on\nthis data set, which",
    "start": "1297340",
    "end": "1303760"
  },
  {
    "text": "is the track six data set. And you see kind\nof a similar trend. And ideally we'd like to\ntake this performance, when",
    "start": "1303760",
    "end": "1310659"
  },
  {
    "text": "you have only 100\ntraining examples, and get it closer to\nthe level that we could",
    "start": "1310660",
    "end": "1316210"
  },
  {
    "text": "perform if you have more data. That said, it is still\ndoing better than scratch",
    "start": "1316210",
    "end": "1322240"
  },
  {
    "text": "when you have small\namounts of data. So there's still a\nbig improvement there, which is good. ",
    "start": "1322240",
    "end": "1330010"
  },
  {
    "text": "OK, so now let's talk a\nbit about meta-learning. And we'll start by talking\nabout the problem formulation.",
    "start": "1330010",
    "end": "1338750"
  },
  {
    "text": "And I guess a\ndisclaimer is that I'll cover a meta-learning problem\nstatement that we'll consider",
    "start": "1338750",
    "end": "1344060"
  },
  {
    "text": "in this class, which is\nbasically the problem statement that the meta-learning\nalgorithms that we'll discuss",
    "start": "1344060",
    "end": "1350120"
  },
  {
    "text": "will solve. But there are other forms\nof meta-learning algorithms",
    "start": "1350120",
    "end": "1355220"
  },
  {
    "text": "that solve a different\nproblem statement. So that's kind of, I\nguess, a disclaimer.",
    "start": "1355220",
    "end": "1360680"
  },
  {
    "text": "There's a lot of\nthings that people have called meta-learning. This is what we'll be kind of-- These are the problem\nstatements that we'll",
    "start": "1360680",
    "end": "1366164"
  },
  {
    "text": "be considering in this course. And to start off, I think\nthere's, in many ways, two ways to view meta-learning\nalgorithms.",
    "start": "1366164",
    "end": "1372660"
  },
  {
    "text": "There is a more mechanistic\nview and there's more of a probablistic view. In the mechanistic\nview, you could",
    "start": "1372660",
    "end": "1379290"
  },
  {
    "text": "view meta-learning\nas, essentially, a deep neural network\nthat can read in datasets and learn from those datasets\nto make predictions for new data",
    "start": "1379290",
    "end": "1386580"
  },
  {
    "text": "points. And training the network\nitself uses a dataset of datasets or a metadata\nset, which in itself consists",
    "start": "1386580",
    "end": "1395399"
  },
  {
    "text": "of many datasets. And each dataset is\nfor a different task. This is sort of like\nthe mechanistic view",
    "start": "1395400",
    "end": "1401040"
  },
  {
    "text": "that we'll talk about. And then the\nprobabilistic view of this is you can view\nmeta-learning as trying",
    "start": "1401040",
    "end": "1406175"
  },
  {
    "text": "to extract prior knowledge\nfrom a set of tasks so that allows for efficient\nlearning of new tasks.",
    "start": "1406175",
    "end": "1412470"
  },
  {
    "text": "And specifically\nlearning a new task uses this prior and a\nsmall training dataset",
    "start": "1412470",
    "end": "1417840"
  },
  {
    "text": "to infer the most likely\nposterior parameters.",
    "start": "1417840",
    "end": "1422860"
  },
  {
    "text": "These two views I think are\nactually both quite interesting and useful.",
    "start": "1422860",
    "end": "1428080"
  },
  {
    "text": "Today we're going\nto focus primarily on the mechanistic\nview because I think it's a little bit easier\nto understand, especially",
    "start": "1428080",
    "end": "1434500"
  },
  {
    "text": "from the standpoint of\nimplementing these algorithms. But we will talk about\nthe probabilistic view",
    "start": "1434500",
    "end": "1439683"
  },
  {
    "text": "when we talk about Bayesian\nmeta-learning algorithms and explicitly connecting\nthese meta-learning algorithms to graphical models and\nlearning priors over parameters",
    "start": "1439683",
    "end": "1448447"
  },
  {
    "text": "in those graphical models.  OK, so when talking\nabout meta-learning",
    "start": "1448447",
    "end": "1459688"
  },
  {
    "text": "in a meta-learning\nproblem statement. I'd like to first\nstart with an example. And specifically, we'll look at\na few shot image classification",
    "start": "1459688",
    "end": "1466990"
  },
  {
    "text": "example. And the goal of this few shot\nimage classification problem will be the following.",
    "start": "1466990",
    "end": "1474400"
  },
  {
    "text": "Given one example of\nfive different classes, this is essentially a kind\nof little mini training data set with five examples.",
    "start": "1474400",
    "end": "1481600"
  },
  {
    "text": "Your goal is to\nclassify new examples among the five classes that\nare in the training data set.",
    "start": "1481600",
    "end": "1489549"
  },
  {
    "text": "So essentially, your goal is\nto learn from five data points to identify and\nclassify new examples.",
    "start": "1489550",
    "end": "1497560"
  },
  {
    "text": "This is analogous to the\nfew shot learning problem that you did in\nthe first lecture with the different\npaintings, where",
    "start": "1497560",
    "end": "1502818"
  },
  {
    "text": "you were trying to classify\nfrom six examples which painter painted the test image.",
    "start": "1502818",
    "end": "1512552"
  },
  {
    "text": "So this is an example of a\nfew shot learning problem. And this is kind of the\nproblem that meta-learning",
    "start": "1512552",
    "end": "1517720"
  },
  {
    "text": "tries to solve, or at least few\nshot meta-learning algorithms try to solve. And the way that\nthey do this is they",
    "start": "1517720",
    "end": "1523840"
  },
  {
    "text": "aren't going to\nstart from scratch. If you are starting\nfrom scratch, you wouldn't be able to train\non this kind of tiny dataset",
    "start": "1523840",
    "end": "1530020"
  },
  {
    "text": "of five examples, of course. And so what\nmeta-learning algorithms try to do is, like\ntransfer learning,",
    "start": "1530020",
    "end": "1535420"
  },
  {
    "text": "they assume that they\nhave some previous data. And in particular,\nin this example,",
    "start": "1535420",
    "end": "1540597"
  },
  {
    "text": "they assume that they have\nprevious data of other image classes. And that data will\nbe structured just",
    "start": "1540598",
    "end": "1548679"
  },
  {
    "text": "like what we see at task time. It will be structured into\ntraining sets and test sets.",
    "start": "1548680",
    "end": "1554440"
  },
  {
    "text": "And we'll kind of construct\nthese training sets and these testing sets for many\ndifferent image classification",
    "start": "1554440",
    "end": "1560200"
  },
  {
    "text": "tasks. These will be constructed among\na large number of training",
    "start": "1560200",
    "end": "1566690"
  },
  {
    "text": "classes, which will be\ndifferent from the classes that are used at test time.",
    "start": "1566690",
    "end": "1573920"
  },
  {
    "text": "And the will be to\nessentially figure out to train a model such that after\nit sees this training data set",
    "start": "1573920",
    "end": "1580330"
  },
  {
    "text": "on the left, it can effectively\nclassify new examples in the test set on the right.",
    "start": "1580330",
    "end": "1586840"
  },
  {
    "text": "So it's essentially learning\nhow to learn each of these image classification tasks. ",
    "start": "1586840",
    "end": "1593970"
  },
  {
    "text": "And then once you\nkind of learn how to learn these different tasks\nin the meta-training set,",
    "start": "1593970",
    "end": "1600120"
  },
  {
    "text": "then at meta-testing when\nyou're given this new few shot learning problem,\nyou'll be prepared",
    "start": "1600120",
    "end": "1605705"
  },
  {
    "text": "to solve it because\nyou've solved many other few-shot\nlearning problems. ",
    "start": "1605705",
    "end": "1611422"
  },
  {
    "text": "So this is kind of a\nnew task and it also has a training set\nand a test set. ",
    "start": "1611422",
    "end": "1617340"
  },
  {
    "text": "So this is one example of a\nmeta-learning problem where you have these different tasks,\nand each task corresponds",
    "start": "1617340",
    "end": "1622380"
  },
  {
    "text": "to an image classification task. But you can replace\nimage classification with different regression tasks.",
    "start": "1622380",
    "end": "1628260"
  },
  {
    "text": "You can replace it with\nlanguage generation tasks. You can replace it with\nrobotic skill learning tasks.",
    "start": "1628260",
    "end": "1633690"
  },
  {
    "text": "Really any kind of\nmachine-learning problem can be filled, you can\nreplace these tasks with.",
    "start": "1633690",
    "end": "1638760"
  },
  {
    "text": " So that's kind of an example.",
    "start": "1638760",
    "end": "1643870"
  },
  {
    "text": "And more concretely, the\nmeta-learning problem is the following. Given data from n\ndifferent tasks,",
    "start": "1643870",
    "end": "1650230"
  },
  {
    "text": "your role is to quickly solve a\nnew task given a small data set for that new task.",
    "start": "1650230",
    "end": "1656390"
  },
  {
    "text": "So maybe the data set is D test. ",
    "start": "1656390",
    "end": "1662679"
  },
  {
    "text": "And one of the key assumptions\nhere that we're going to use is that the meta-training\ntasks and the meta-test tasks",
    "start": "1662680",
    "end": "1668950"
  },
  {
    "text": "are drawn IID from the\nsame task distribution. So for example, there'll be\nsome distribution over tasks P.",
    "start": "1668950",
    "end": "1677320"
  },
  {
    "text": "All of your meta-training\ntasks, task 1 through task n are drawn from that distribution\nas well as your test task",
    "start": "1677320",
    "end": "1684640"
  },
  {
    "text": "is also drawn from\nthat distribution. And this assumption\nis really important, because that means that\nafter we've kind of trained",
    "start": "1684640",
    "end": "1692560"
  },
  {
    "text": "on this set of tasks when\nwe're given a new task, we can expect that\nwe'll generalize under the same principles\nof kind of IID training",
    "start": "1692560",
    "end": "1700480"
  },
  {
    "text": "in supervised learning.  OK, and then like before,\nlike multi-task learning",
    "start": "1700480",
    "end": "1710370"
  },
  {
    "text": "and transfer learning, tasks\nmust share some structure. So if this distribution\nP is the distribution",
    "start": "1710370",
    "end": "1716260"
  },
  {
    "text": "over all possible\ntasks and you're just drawing random tasks\nfrom it, there's no structure that you can latch\nonto to learn the new test",
    "start": "1716260",
    "end": "1723610"
  },
  {
    "text": "task more quickly. And instead what\nmeta-learning aims to do is to kind of find the\ncommon structure among all",
    "start": "1723610",
    "end": "1731200"
  },
  {
    "text": "of the training tasks and\nleverage that structure when learning the\nnew tasks so that it",
    "start": "1731200",
    "end": "1736690"
  },
  {
    "text": "can learn it with\nless data than if it were training from scratch. ",
    "start": "1736690",
    "end": "1742673"
  },
  {
    "text": "So there are some\nquestions on how this relates to multi-task\nlearning and meta-learning. And the kind of main difference\nis that in multi-task learning,",
    "start": "1742673",
    "end": "1750420"
  },
  {
    "text": "you would be given\ndata from n tasks, and you wanted to\nsolve those n tasks. You don't really care\nabout solving a new task",
    "start": "1750420",
    "end": "1758190"
  },
  {
    "text": "leveraging that experience. Whereas in\nmeta-learning, your goal is to potentially\nsolve those n tasks,",
    "start": "1758190",
    "end": "1763710"
  },
  {
    "text": "but most importantly, be able\nto quickly solve a new task with a small amount of data. ",
    "start": "1763710",
    "end": "1771420"
  },
  {
    "text": "OK, what do the different\ntasks correspond to?",
    "start": "1771420",
    "end": "1776600"
  },
  {
    "text": "They can correspond\nto a lot of the things that we were talking\nabout before. So they can correspond to\nrecognizing handwritten digits",
    "start": "1776600",
    "end": "1782280"
  },
  {
    "text": "from different languages. They'll be looking at\nhandwritten digits that look like this and trying to\nclassify handwritten digits",
    "start": "1782280",
    "end": "1791490"
  },
  {
    "text": "from a new language\nafter seeing data from a number of\ndifferent languages.",
    "start": "1791490",
    "end": "1797760"
  },
  {
    "text": "You can also have\nkind of a spam filter for different\nusers and your goal is to, kind of given a new\nuser and a little bit of data",
    "start": "1797760",
    "end": "1803880"
  },
  {
    "text": "from that new user, can you\nlearn a good spam filter for that user?",
    "start": "1803880",
    "end": "1809120"
  },
  {
    "text": "You may interested in looking\nat classifying species from different\nregions of the world. Maybe you have a lot\nof data from a range",
    "start": "1809120",
    "end": "1815830"
  },
  {
    "text": "of different regions. And then you're given a little\nbit of data for a new region. You want to be able to\nlearn a good classifier",
    "start": "1815830",
    "end": "1824139"
  },
  {
    "text": "for that new region from\na small amount of data.  And then lastly, it could\nalso correspond to your robot",
    "start": "1824140",
    "end": "1831300"
  },
  {
    "text": "performing different tasks. Maybe the robot has learned\n50 different tasks before and it wants to\nlearn, kind of add",
    "start": "1831300",
    "end": "1837720"
  },
  {
    "text": "a new task to its repertoire\nfrom a small amount of data. This is another way to do that.",
    "start": "1837720",
    "end": "1842930"
  },
  {
    "text": " OK, now one other\nnatural question",
    "start": "1842930",
    "end": "1849690"
  },
  {
    "text": "is, how many tasks do you need? What should the value of n be? ",
    "start": "1849690",
    "end": "1855350"
  },
  {
    "text": "The more the better. In many ways, kind of this is\nanalogous to having more data in machine learning.",
    "start": "1855350",
    "end": "1861240"
  },
  {
    "text": "If you have more\ntasks, then you'll be able to potentially do\nbetter on a new held out task.",
    "start": "1861240",
    "end": "1866462"
  },
  {
    "text": "One thing that's different,\nthough, is that per task, you don't necessarily\nneed a lot of data. You just want to\nhave a lot of tasks",
    "start": "1866462",
    "end": "1872770"
  },
  {
    "text": "to cover a broader distribution.  OK, any questions on this before\nwe go over some terminology?",
    "start": "1872770",
    "end": "1883581"
  },
  {
    "text": " Oh yeah, my question is a few\nslides back, it was a record.",
    "start": "1883581",
    "end": "1893060"
  },
  {
    "text": " OK, so when it's classifying\nthe new examples,",
    "start": "1893060",
    "end": "1900169"
  },
  {
    "text": "is it just going to be one--  so it can't tell us whether\nit's, like, a lion or a bowl.",
    "start": "1900170",
    "end": "1907870"
  },
  {
    "text": "It will just give us like\na 1 or a 0 that corresponds to the training data? ",
    "start": "1907870",
    "end": "1915048"
  },
  {
    "text": "So yeah, so at\nmeta-test time, you'll be given these five\nimages in green. And you'll also be given kind\nof labels associated with each",
    "start": "1915048",
    "end": "1923520"
  },
  {
    "text": "of these images, 0, 1, 2, 3, 4. And then your goal will be\ngiven these kind of test images,",
    "start": "1923520",
    "end": "1930620"
  },
  {
    "text": "you want a classifier\nthat basically tells you that this line corresponds\nto a label of 2.",
    "start": "1930620",
    "end": "1936660"
  },
  {
    "text": "And this bowl corresponds\nto a label of 4.",
    "start": "1936660",
    "end": "1941670"
  },
  {
    "text": "OK, makes sense, thanks. ",
    "start": "1941670",
    "end": "1947360"
  },
  {
    "text": "My quick question is\nabout, say like the number of tasks [INAUDIBLE]\nmeta-learning, right?",
    "start": "1947360",
    "end": "1952470"
  },
  {
    "text": "So what is the minimum number\nwe should at least have to be considered a meta\nlearning problem [INAUDIBLE]",
    "start": "1952470",
    "end": "1957643"
  },
  {
    "text": "Would you think that five\nwould potentially be sufficient or like you need 25 or 30? The minimum number of tasks.",
    "start": "1957643",
    "end": "1964860"
  },
  {
    "text": "Yeah, that's a good question. I would say more than one.",
    "start": "1964860",
    "end": "1970769"
  },
  {
    "text": "And I think that basically\nthe more tasks you have, the more effective it will be\na generalizing to a new task.",
    "start": "1970770",
    "end": "1976380"
  },
  {
    "text": "If you only have a\nfew tasks, then it may not be able to\nkind of generalize",
    "start": "1976380",
    "end": "1981640"
  },
  {
    "text": "and kind of find a common\nstructure from those tasks in a way that allows\nyou to do well.",
    "start": "1981640",
    "end": "1987390"
  },
  {
    "text": "But really, more than one is\nkind of what it looks like. And if you only\nhave one task, that",
    "start": "1987390",
    "end": "1993150"
  },
  {
    "text": "looks a lot like\ntransfer learning. Yeah, so like 10 tasks would\nbe like the minimum number to say okay, this might\nbe a meta learning problem",
    "start": "1993150",
    "end": "1999789"
  },
  {
    "text": "to start with? Like, 10 or 15 I would say?  Yeah, yeah, and then\nin the homework,",
    "start": "1999789",
    "end": "2006087"
  },
  {
    "text": "you'll be looking\nat the Omniglot data set which actually\nallows you to construct on the order of 1,000 tasks.",
    "start": "2006087",
    "end": "2012293"
  },
  {
    "text": "But I've also seen\nmeta-learning be successful when you only have maybe\n50 tasks or maybe even",
    "start": "2012293",
    "end": "2018590"
  },
  {
    "text": "a little bit less than that. So for a multi-task\nlearning what",
    "start": "2018590",
    "end": "2027294"
  },
  {
    "text": "is the maximum number\nof tasks you can see [INAUDIBLE] how maximum you can\ngo in multi task learning, like [INAUDIBLE] not work?",
    "start": "2027294",
    "end": "2032395"
  },
  {
    "text": " Yeah, it really depends\non the problem setting,",
    "start": "2032395",
    "end": "2039610"
  },
  {
    "text": "how many tasks you start\nseeing benefits or not.",
    "start": "2039610",
    "end": "2044630"
  },
  {
    "text": "Yeah, it's really\nproblem dependent. It looks like there is also\na clarification question",
    "start": "2044630",
    "end": "2050739"
  },
  {
    "text": "in the chat, which is what's the\ndifference between X test and D test? I'm kind of calling\nthese examples X test",
    "start": "2050739",
    "end": "2058330"
  },
  {
    "text": "and then these are within\nsome data set D test.",
    "start": "2058330",
    "end": "2064239"
  },
  {
    "text": "So yeah, so this is basically-- well, yeah, so for\nthis task for example,",
    "start": "2064239",
    "end": "2071169"
  },
  {
    "text": "I would call this kind\nof D train for task 1. And D test for task 1.",
    "start": "2071170",
    "end": "2078550"
  },
  {
    "text": "And yeah, I think that\nanswers the question. ",
    "start": "2078550",
    "end": "2089669"
  },
  {
    "text": "I just assume some of the\nideal meta in this [INAUDIBLE],,",
    "start": "2089670",
    "end": "2095085"
  },
  {
    "text": "is it like, we have\none task and oh, like, the multiple task which\nis like T1 [INAUDIBLE]..",
    "start": "2095085",
    "end": "2101609"
  },
  {
    "text": "And then we give it the meta,\nwhich is these five images. And then we have\nthe input, which is",
    "start": "2101610",
    "end": "2107490"
  },
  {
    "text": "those two images to classify. And then it needs\nto use the meta, or it prefers meta\nto make a prediction.",
    "start": "2107490",
    "end": "2114119"
  },
  {
    "text": "Is that kind of where\nthe meta term is--",
    "start": "2114120",
    "end": "2119350"
  },
  {
    "text": "So we'll get a\nbetter understanding of why it necessarily\ncorresponds with meta-learning",
    "start": "2119350",
    "end": "2124990"
  },
  {
    "text": "when we talk about algorithms. But one perspective\nthat you could take here is that for each task,\nfor example, you're",
    "start": "2124990",
    "end": "2132840"
  },
  {
    "text": "trying to learn from a datasets\nand be able to generalize to the test dataset.",
    "start": "2132840",
    "end": "2139230"
  },
  {
    "text": "So each of these\ntasks corresponds to a learning problem. And then when you're\nconsidering all of these tasks",
    "start": "2139230",
    "end": "2146280"
  },
  {
    "text": "and trying to be\nable to quickly learn from any of these\ntraining data sets,",
    "start": "2146280",
    "end": "2152369"
  },
  {
    "text": "then you're essentially\nlearning how to learn each of these tasks. And so that's where\nmeta-learning comes from.",
    "start": "2152370",
    "end": "2160613"
  },
  {
    "text": "OK, so when you\nsay, we want to be able to learn a\nnew task quickly, is that like-- so\na new task would be if we did an image from\neach of those classes.",
    "start": "2160613",
    "end": "2169740"
  },
  {
    "text": "And then we sample some random\ntwo images from a new class, that would be like a new task.",
    "start": "2169740",
    "end": "2174930"
  },
  {
    "text": "Yeah, exactly. So this is-- This corresponds to a new task. And your goal at test time\nis to learn this new task",
    "start": "2174930",
    "end": "2182790"
  },
  {
    "text": "with this small data set. Oh, OK, yeah, good. Thank you.",
    "start": "2182790",
    "end": "2187820"
  },
  {
    "start": "2187820",
    "end": "2193730"
  },
  {
    "text": "OK, so in NLP the learning\nhasn't converged and come to [INAUDIBLE] for\nnatural questions.",
    "start": "2193730",
    "end": "2200260"
  },
  {
    "text": "So is it true for\nvision as well, where the learning because\n[INAUDIBLE] as many tasks",
    "start": "2200260",
    "end": "2208550"
  },
  {
    "text": "as [INAUDIBLE] tasks. So are you asking if\nyou include more tasks?",
    "start": "2208550",
    "end": "2215780"
  },
  {
    "text": "I guess I don't\nunderstand the question. The question is\njust, yeah, I think this is a riff off of\nthe previous question",
    "start": "2215780",
    "end": "2221940"
  },
  {
    "text": "where the maximum\nnumber, so to speak, has the learning converged,\nhave people found that OK, adding more tasks or\nmore data to any task",
    "start": "2221940",
    "end": "2227665"
  },
  {
    "text": "for this particular task is\nmore beneficial or better?",
    "start": "2227665",
    "end": "2234484"
  },
  {
    "text": " Yeah, so I think\nthe question is kind",
    "start": "2234484",
    "end": "2240580"
  },
  {
    "text": "of like, what is the\nmaximum number of tasks until you really start\nseeing it converge?",
    "start": "2240580",
    "end": "2246520"
  },
  {
    "text": "In the Omniglot data\nset, which we'll talk a bit more\nabout later, where you have around 1,000 tasks,\nI think in that setting,",
    "start": "2246520",
    "end": "2253180"
  },
  {
    "text": "the validation\nperformance starts to converge towards the\ntraining performance. And that's a setting\nwhere it does",
    "start": "2253180",
    "end": "2259990"
  },
  {
    "text": "seem like if you have enough\ntasks in that setting, then again it is also still\nvery probability dependent.",
    "start": "2259990",
    "end": "2265370"
  },
  {
    "text": "If you have a very\nbroad data distribution, then you'll need more\ntasks to cover that. Whereas if you have a very\nnarrow task distribution,",
    "start": "2265370",
    "end": "2271990"
  },
  {
    "text": "then you may not actually need\na very large number of tasks to adequately cover that. Good, thank you.",
    "start": "2271990",
    "end": "2279785"
  },
  {
    "text": "I want to kind of\nquickly also go through a bit of terminology,\nsince this may also answer some of the questions. And then I'll take\nsome more questions.",
    "start": "2279785",
    "end": "2287570"
  },
  {
    "text": "So as I mentioned before, kind\nof each of these green boxes",
    "start": "2287570",
    "end": "2293770"
  },
  {
    "text": "correspond to trainining data\nsets for individual tasks, so maybe this is task i.",
    "start": "2293770",
    "end": "2301000"
  },
  {
    "text": "Then this box corresponds to the\ntraining set for that task i. And this red box\ncorresponds to the test data",
    "start": "2301000",
    "end": "2309520"
  },
  {
    "text": "set for that task.  This is kind of the\ntraining and test datasets",
    "start": "2309520",
    "end": "2316550"
  },
  {
    "text": "are distinct from what's called\nmeta-training and meta-testing. So in many ways,\nyou're actually going",
    "start": "2316550",
    "end": "2321890"
  },
  {
    "text": "to be using these individual\nmeta-training test task data sets. Or, sorry, the test data sets\nfor different meta-training",
    "start": "2321890",
    "end": "2329437"
  },
  {
    "text": "tasks, you'll be using\nthese and actually training with these during the\nmeta-training process.",
    "start": "2329437",
    "end": "2334670"
  },
  {
    "text": "And sometimes, just to kind\nof make a better distinction between meta-training,\nmeta-testing, and training and testing.",
    "start": "2334670",
    "end": "2340220"
  },
  {
    "text": "People instead call each of\nthese green boxes the support set and the red\nboxes the query set.",
    "start": "2340220",
    "end": "2347270"
  },
  {
    "text": "And we'll probably\nuse both of them interchangeably,\nalthough in practice I think we'll typically\nuse the training set",
    "start": "2347270",
    "end": "2352490"
  },
  {
    "text": "and the test set a\nbit more frequently. And then when we talk about few\nshot learning, or k learning,",
    "start": "2352490",
    "end": "2362390"
  },
  {
    "text": "one shot learning,\nfive shot learning, what this means is that\nwe're going to be learning with k examples per class.",
    "start": "2362390",
    "end": "2369035"
  },
  {
    "text": " Or if we're in a\nregression scenario, we're going to be using k total\ntraining examples for learning.",
    "start": "2369035",
    "end": "2379370"
  },
  {
    "text": "So kind of k shot means\nyou'll be using k examples. And then if you're doing\nn-ways classification,",
    "start": "2379370",
    "end": "2386680"
  },
  {
    "text": "then you're choosing\nbetween n classes. So you're basically\nclassifying among n things.",
    "start": "2386680",
    "end": "2394710"
  },
  {
    "text": "So one kind of question for\nyou based on this terminology is in this above example, what\nwould the values of k and n be?",
    "start": "2394710",
    "end": "2404570"
  },
  {
    "text": "And you could answer\nthis in the chat. ",
    "start": "2404570",
    "end": "2412600"
  },
  {
    "text": "So k is the number of\nexamples per class and n is the number of classes.",
    "start": "2412600",
    "end": "2420059"
  },
  {
    "text": "Great, so it looks like\npeople are entering the correct answer,\nwhich is k equals 1 and n equals 5 for\nthis above example.",
    "start": "2420060",
    "end": "2429390"
  },
  {
    "text": "Because we have one example\nfor five different classes.",
    "start": "2429390",
    "end": "2435180"
  },
  {
    "text": "Typically, the value of 1\nand the value of k and n, you often keep it consistent\namong meta-training",
    "start": "2435180",
    "end": "2441120"
  },
  {
    "text": "and meta-testing. But you could also train on a\nlarger number of n or variable",
    "start": "2441120",
    "end": "2446670"
  },
  {
    "text": "k. And then test it on\nvariable k and variable n.",
    "start": "2446670",
    "end": "2453830"
  },
  {
    "text": "So there is few shot learning. Does that mean, basically,\nthat when k is small",
    "start": "2453830",
    "end": "2460279"
  },
  {
    "text": "it does better or something? I don't know, I'm just\ncurious if you could talk about that a little bit.",
    "start": "2460280",
    "end": "2465420"
  },
  {
    "text": "Yeah, so the problem\nof few shot learning is basically when\nyou want to learn with a small amount of data.",
    "start": "2465420",
    "end": "2471800"
  },
  {
    "text": "So basically, when k is small,\nthen you have a small data set, you want to learn\nfrom a few examples. ",
    "start": "2471800",
    "end": "2481230"
  },
  {
    "text": "OK, do you have a question? Yes, is there some\nmore formal definition",
    "start": "2481230",
    "end": "2487480"
  },
  {
    "text": "of what a task distribution is? Do we have to have\ncommon classes? Or what's the definition\nof task distribution?",
    "start": "2487480",
    "end": "2496210"
  },
  {
    "text": "Yeah, I think that\nthere isn't really a formal definition of the\ndistribution over tasks.",
    "start": "2496210",
    "end": "2504150"
  },
  {
    "text": "I guess before I gave a formal\ndistribution of what a task is, which corresponds to kind of the\ndata-generating distributions",
    "start": "2504150",
    "end": "2510839"
  },
  {
    "text": "of basically p of x\nand p of y given x.",
    "start": "2510840",
    "end": "2515970"
  },
  {
    "text": "And you could\npotentially-- these are kind of the true\ndistributions underlying the data. And you could view maybe\nthese true distributions are",
    "start": "2515970",
    "end": "2525930"
  },
  {
    "text": "paramatized by phi in some way. And then you could formally\nview a distribution over tasks",
    "start": "2525930",
    "end": "2532140"
  },
  {
    "text": "as being equivalent\nto a distribution over the parameters of those\ndata generating distributions.",
    "start": "2532140",
    "end": "2539190"
  },
  {
    "text": "In general, though,\nit's a bit tricky to think about sometimes because\nsometimes these data generated",
    "start": "2539190",
    "end": "2545820"
  },
  {
    "text": "distributions aren't\neven necessarily-- don't necessarily have\na parametric form.",
    "start": "2545820",
    "end": "2551340"
  },
  {
    "text": "And so it's a bit tricky. And it's a little bit dependent\non the particular domain that you're in.",
    "start": "2551340",
    "end": "2557610"
  },
  {
    "text": "One example of where you\ncan parameterize a task distribution is\nmaybe different tasks",
    "start": "2557610",
    "end": "2563609"
  },
  {
    "text": "correspond to different\nsinusoids, where maybe they're all different-- a family\nof sinusoid functions",
    "start": "2563610",
    "end": "2569552"
  },
  {
    "text": "where you're trying to regress\nto the value of the sinusoid given the x value. And then maybe the\nparameters would",
    "start": "2569552",
    "end": "2575790"
  },
  {
    "text": "correspond to the amplitude\nof different sinusoids or the phase of\ndifferent sinusoids. And there you might actually\nbe able to explicitly represent",
    "start": "2575790",
    "end": "2582960"
  },
  {
    "text": "the distribution over the\ndata generating parameters. But in practice\nit's a bit tricky to do that because\nwe don't really",
    "start": "2582960",
    "end": "2588990"
  },
  {
    "text": "know the parameters underlying\nthe image formation process, for example. Right, so kind of the definition\nof of a task distribution",
    "start": "2588990",
    "end": "2595845"
  },
  {
    "text": "can be as broad as possible. Like, if there is a task it\nshoud be cased to all the tasks",
    "start": "2595845",
    "end": "2600895"
  },
  {
    "text": "if possible? Is that it? ",
    "start": "2600895",
    "end": "2607200"
  },
  {
    "text": "I guess it's not clear. You could have something\nthat conceivably covers all the tasks possible.",
    "start": "2607200",
    "end": "2612720"
  },
  {
    "text": "But then that may, if\np of x is truly random, then that may not actually be\nthat useful or interesting.",
    "start": "2612720",
    "end": "2620010"
  },
  {
    "text": "Because typically for\nimage classification, for example, if we're\ninterested in natural images. And so p of x, we want\nto cover the distribution",
    "start": "2620010",
    "end": "2630450"
  },
  {
    "text": "of our natural images and\nnot include random noise or white noise, for example. ",
    "start": "2630450",
    "end": "2636089"
  },
  {
    "text": "Thanks. ",
    "start": "2636090",
    "end": "2641600"
  },
  {
    "text": "Hi, I have a general question. I wonder how the\nmeta-learning is",
    "start": "2641600",
    "end": "2649220"
  },
  {
    "text": "robust in some extent compared\nto other conventional, supervised learning.",
    "start": "2649220",
    "end": "2655820"
  },
  {
    "text": "For example, if there\nare some wrongly labeled images in the image\nclassification for example.",
    "start": "2655820",
    "end": "2665020"
  },
  {
    "text": "Yeah, so I think it's going\nto depend on the algorithm that you use.",
    "start": "2665020",
    "end": "2670190"
  },
  {
    "text": "I think the different algorithms\nmay have different robustness properties.",
    "start": "2670190",
    "end": "2675340"
  },
  {
    "text": "I'm also not aware of any works\nthat explicitly study that. Although there was actually\na course project last year",
    "start": "2675340",
    "end": "2682010"
  },
  {
    "text": "that looked at, if you could\nessentially meta-train a model to be robust to label noise.",
    "start": "2682010",
    "end": "2687726"
  },
  {
    "text": "So you could look at the\ncourse projects from last year, if you're interested in that. ",
    "start": "2687727",
    "end": "2697705"
  },
  {
    "text": "Hi, yeah, I'm still\ntrying to tease out the difference\nbetween meta-learning and transfer learning\nin this case.",
    "start": "2697705",
    "end": "2704010"
  },
  {
    "text": "So it kind of looks\nlike if I were to cut off, in my head,\nthe meta-learning step,",
    "start": "2704010",
    "end": "2710370"
  },
  {
    "text": "the meta-training\nstep, it kind of looks like you're using an\nImagenet pre-trained model,",
    "start": "2710370",
    "end": "2715690"
  },
  {
    "text": "which is kind of written up\nat different tasks instead. Yeah, yeah, so here's a summary\nslide that kind of goes over it",
    "start": "2715690",
    "end": "2723610"
  },
  {
    "text": "to some degree. So in transfer learning,\nthe previous data that you're going to\nhave will be typically in the form of a single task.",
    "start": "2723610",
    "end": "2729820"
  },
  {
    "text": "Whereas in meta-learning\nyou're going to assume-- actually you'll have\na stronger assumption and assume that you have\nthese different tasks",
    "start": "2729820",
    "end": "2735490"
  },
  {
    "text": "and that the\ndifferent tasks were drawn from the same distribution\nas your downstream task. One way that you\ncould view this is",
    "start": "2735490",
    "end": "2740890"
  },
  {
    "text": "that meta-learning\nalgorithms are actually going to be explicitly\noptimizing for the ability to learn a task within\nthat distribution.",
    "start": "2740890",
    "end": "2748330"
  },
  {
    "text": "Whereas transfer\nlearning approaches tend to be a bit more ad hoc. And they, for example,\nin fine tuning will just kind of\ntrain on one task",
    "start": "2748330",
    "end": "2754570"
  },
  {
    "text": "and then fine tune the\nweights on another task. Does this answer\nto your question?",
    "start": "2754570",
    "end": "2760090"
  },
  {
    "text": " I guess I'm thinking what\nqualifies as a task being",
    "start": "2760090",
    "end": "2769320"
  },
  {
    "text": "distinct from another task. Yeah, I guess. So, yeah, in the image\nrecognition example,",
    "start": "2769320",
    "end": "2777190"
  },
  {
    "text": "it is a good point\nthat basically you could construct these tasks\nfrom a single dataset,",
    "start": "2777190",
    "end": "2782380"
  },
  {
    "text": "from the ImageNet\ndataset for example. And this is actually quite\ncommon in the few shot image classification literature.",
    "start": "2782380",
    "end": "2790805"
  },
  {
    "text": "One thing that is\nimportant is that you have to actually construct\nthose tasks, rather than just having a\nsingle monolithic task",
    "start": "2790805",
    "end": "2798490"
  },
  {
    "text": "in meta-learning. And you have to\nconstruct them in a way that it is from the same\ndistribution as your test task.",
    "start": "2798490",
    "end": "2806050"
  },
  {
    "text": " But it is true that\nbasically these tasks could be constructed\nfrom a data set that's",
    "start": "2806050",
    "end": "2814720"
  },
  {
    "text": "quite similar to the dataset\nthat you use for the source task in transfer learning. ",
    "start": "2814720",
    "end": "2821730"
  },
  {
    "text": "Thanks. ",
    "start": "2821730",
    "end": "2838083"
  },
  {
    "text": "All right, maybe\nwe'll move onto--  Hi, I had a question\nabout some of the slide",
    "start": "2838083",
    "end": "2845315"
  },
  {
    "text": "with the terminology on it. I'm still, I guess, a\nlittle bit confused still.",
    "start": "2845315",
    "end": "2851440"
  },
  {
    "text": "Is that first gray\nbox, is that equal to b sub i of the training set?",
    "start": "2851440",
    "end": "2858170"
  },
  {
    "text": "Or is that just one example from\nb sub 1 of the training set.",
    "start": "2858170",
    "end": "2863799"
  },
  {
    "text": "Yeah, so this first green box,\nthis kind of green box right",
    "start": "2863800",
    "end": "2868810"
  },
  {
    "text": "here, this would be like,\nd sub i for, like, task 1, so maybe i equals 1.",
    "start": "2868810",
    "end": "2874630"
  },
  {
    "text": "And then this whole box would\nbe d sub i for i equals 2.",
    "start": "2874630",
    "end": "2882480"
  },
  {
    "text": "That answer your question? Yep, thank you. Great. ",
    "start": "2882480",
    "end": "2888160"
  },
  {
    "text": "I think [AUDIO OUT] will type\nout her question in the chat. In the meantime. ",
    "start": "2888160",
    "end": "2895750"
  },
  {
    "text": "Hi, yeah, so I\nwas just wondering why you need the tasks\nto have the same number of [INAUDIBLE] and the fixed n.",
    "start": "2895750",
    "end": "2901843"
  },
  {
    "text": "Does it help? Because [INAUDIBLE]\nthe classes all look similar across the tasks.",
    "start": "2901843",
    "end": "2908200"
  },
  {
    "text": "So you're asking why have a\nfixed n for different tasks? Yes.",
    "start": "2908200",
    "end": "2914550"
  },
  {
    "text": "Yeah, so in practice\nyou could have kind of a varying n\nfor different tasks.",
    "start": "2914550",
    "end": "2921920"
  },
  {
    "text": "You do want to have a scenario\nwhere the test task that you're-- kind of meta-testing\ntask has a value of n",
    "start": "2921920",
    "end": "2930130"
  },
  {
    "text": "that's at least as kind\nof-- it's no larger than any",
    "start": "2930130",
    "end": "2935299"
  },
  {
    "text": "of the n that you\nsaw during training. But you can have variable n.",
    "start": "2935300",
    "end": "2940706"
  },
  {
    "text": "OK, thank you. And then, asked in the\nchat, is task grouping",
    "start": "2940706",
    "end": "2947930"
  },
  {
    "text": "considered a challenge? We see task grouping\nas a challenge",
    "start": "2947930",
    "end": "2953300"
  },
  {
    "text": "in multi-task learning. And I was reading\na recent paper that comes up with a task grouping\nframework to deal with it.",
    "start": "2953300",
    "end": "2958850"
  },
  {
    "text": "Yeah, so different tasks may\nbe more similar to one another. Like, you could construct\na task distribution",
    "start": "2958850",
    "end": "2965869"
  },
  {
    "text": "that has medical images and\nnatural images, for example. And it may actually\nbe helpful to learn a grouping of\nthose tasks, if you",
    "start": "2965870",
    "end": "2972260"
  },
  {
    "text": "find that your\nalgorithm isn't finding common ground between\nthose two sets of tasks.",
    "start": "2972260",
    "end": "2977420"
  },
  {
    "text": "And yeah, that's\ndefinitely a challenge that people have looked\nat, especially when you observe negative transfer\nbetween different sets",
    "start": "2977420",
    "end": "2983415"
  },
  {
    "text": "of tasks. ",
    "start": "2983415",
    "end": "2988620"
  },
  {
    "text": "Cool, and then one more\nquestion from [AUDIO OUT].. What do we have,\nwhat to do if we",
    "start": "2988620",
    "end": "2995250"
  },
  {
    "text": "have varying k for each task? Yeah, so you can\nalso have varying k.",
    "start": "2995250",
    "end": "3000800"
  },
  {
    "text": "And we'll talk about this a bit\nwhen we talk about algorithms. So feel free to bring\nthat up again when",
    "start": "3000800",
    "end": "3006397"
  },
  {
    "text": "we talk about algorithms. I will try to cover it when we\ntalk about black box problems. ",
    "start": "3006397",
    "end": "3014089"
  },
  {
    "text": "OK, so let's move\non to algorithms for the sake of time. So to recap the\nproblem setting, I",
    "start": "3014090",
    "end": "3020120"
  },
  {
    "text": "guess we went over\nthis slide a bit. But in multitask\nlearning, you're trying to solve\nmultiple task at once. In transfer learning,\nyou're trying",
    "start": "3020120",
    "end": "3026228"
  },
  {
    "text": "to solve a new task after\nsolving a source task. And in meta-learning,\nyou're trying to quickly solve a new task,\ngiven data from a number",
    "start": "3026228",
    "end": "3033140"
  },
  {
    "text": "of previous tasks. In both transfer learning\nand meta-learning, it's generally impractical\nto access the source task",
    "start": "3033140",
    "end": "3040140"
  },
  {
    "text": "for the previous\nmeta-training tasks, just because you often don't\nwant to retrain on that data",
    "start": "3040140",
    "end": "3045410"
  },
  {
    "text": "or maybe you don't have\nthe space to retain all that or just it's really slow.",
    "start": "3045410",
    "end": "3051630"
  },
  {
    "text": "And in all of these\nthree settings, the tasks must share\nsome common structure in order to get benefit\ncompared to just training",
    "start": "3051630",
    "end": "3057390"
  },
  {
    "text": "these tasks from scratch.  OK, so now let's actually\ntalk about a general recipe",
    "start": "3057390",
    "end": "3063910"
  },
  {
    "text": "of meta-learning algorithms and\nthen one instantiation of that.",
    "start": "3063910",
    "end": "3069299"
  },
  {
    "text": "And kind of I guess first let's\ntalk about a general recipe for evaluating a\nmeta-learning algorithm.",
    "start": "3069300",
    "end": "3075632"
  },
  {
    "text": "And in particular, I want to\nkind of highlight the Omniglot data set that we've talked\nabout a little bit up until now.",
    "start": "3075632",
    "end": "3083000"
  },
  {
    "text": "So the Omniglot data set\nhas these 1623 characters",
    "start": "3083000",
    "end": "3088660"
  },
  {
    "text": "from 50 different alphabets. And here's kind of an example\nof what the data looks like. So it has alphabets like Hebrew,\nBengali, Greek, Futurama.",
    "start": "3088660",
    "end": "3098589"
  },
  {
    "text": "And for each of the characters\nin each of these alphabets, there is 20 instances that are\nhandwritten of each character.",
    "start": "3098590",
    "end": "3108710"
  },
  {
    "text": "And so one thing that's\ninteresting about this data set is that it has many\ndifferent classes.",
    "start": "3108710",
    "end": "3114510"
  },
  {
    "text": "So it has more\nthan 1,000 classes. But it only has a few\nexamples per class,",
    "start": "3114510",
    "end": "3119870"
  },
  {
    "text": "so only 20 instances per class. And in some ways, this is kind\nof the transpose of MNIST,",
    "start": "3119870",
    "end": "3127440"
  },
  {
    "text": "because the MNIST data set,\nfor example, has 10 classes. And it has a very large,\nit's like 1,600 or something",
    "start": "3127440",
    "end": "3134670"
  },
  {
    "text": "like that, a very large\nnumber of instances per class. And I also think that\nthis sort of dataset",
    "start": "3134670",
    "end": "3141680"
  },
  {
    "text": "is actually more reflective of\nwhat we see in the real world. We don't see 1,000 different\ninstances of forks, and spoons,",
    "start": "3141680",
    "end": "3148400"
  },
  {
    "text": "and 1,000 different water\nbottles, and so forth. We see much more\ndiversity in the objects",
    "start": "3148400",
    "end": "3153440"
  },
  {
    "text": "that we see, and in the words\nthat we see, and so forth. But we only see each each\ninstance a few times often.",
    "start": "3153440",
    "end": "3160940"
  },
  {
    "text": " That, I think, makes these sort\nof datasets quite appealing.",
    "start": "3160940",
    "end": "3167710"
  },
  {
    "text": "And the Omniglot paper, proposed\nby Brendan Lake in 2015, actually proposes\ntwo different kinds",
    "start": "3167710",
    "end": "3174819"
  },
  {
    "text": "of problems, both\ndiscriminative problems, where you want to classify\nthe character from the image.",
    "start": "3174820",
    "end": "3180220"
  },
  {
    "text": "As well as a generative\nproblem, where given maybe one or a few\nexamples of a character,",
    "start": "3180220",
    "end": "3185320"
  },
  {
    "text": "you want to generate more\ninstances of that character. Although to date, the\ndiscriminative version has been much more popular\nand used much more widely",
    "start": "3185320",
    "end": "3193150"
  },
  {
    "text": "than the generative version.  And then kind of the initial\nfew shot learning techniques,",
    "start": "3193150",
    "end": "3200610"
  },
  {
    "text": "before this paper and also\naround the time of people were modelling\ncharacters, were actually",
    "start": "3200610",
    "end": "3205890"
  },
  {
    "text": "based off of Bayesian models\nand non-parametric approaches. And Brendan Lake showed that\nthese approaches can actually",
    "start": "3205890",
    "end": "3212400"
  },
  {
    "text": "be, these kinds of\napproaches can actually be quite good at modeling\nthese simple characters.",
    "start": "3212400",
    "end": "3220289"
  },
  {
    "text": "But they are harder to scale\ntowards natural images, and kind of larger data sets,\nand more complex data sets.",
    "start": "3220290",
    "end": "3227832"
  },
  {
    "text": "And so that's why\nwe'll be focusing on more approaches based on\ndeep learning in this course. ",
    "start": "3227832",
    "end": "3234920"
  },
  {
    "text": "And then beyond the Omniglot\ndata set, which in many ways is fairly saturated\nperformance, people",
    "start": "3234920",
    "end": "3243050"
  },
  {
    "text": "have used other image\nrecognition data sets like ImageNet and different\nvariants of that, CIFAR, CUB,",
    "start": "3243050",
    "end": "3250010"
  },
  {
    "text": "and CelebA. And then beyond\nimage classification,",
    "start": "3250010",
    "end": "3256060"
  },
  {
    "text": "there also are a\ncouple other benchmarks that I'll mention in case\nyou're interested in using them for your course project,\nsuch as predicting",
    "start": "3256060",
    "end": "3262750"
  },
  {
    "text": "the properties of\ndifferent molecules and predicting the pose\nof different objects. ",
    "start": "3262750",
    "end": "3270550"
  },
  {
    "text": "OK, so this is kind\nof some data sets that people have\nused for evaluating",
    "start": "3270550",
    "end": "3275859"
  },
  {
    "text": "meta-learning algorithms. And then now to start to talk\nabout different approaches,",
    "start": "3275860",
    "end": "3281410"
  },
  {
    "text": "I actually want to\npresent a second view on the meta-learning problem.",
    "start": "3281410",
    "end": "3286640"
  },
  {
    "text": "And in particular, I'll\ndo this in relation to supervised learning. So one way to view the\nsupervised learning problem",
    "start": "3286640",
    "end": "3292930"
  },
  {
    "text": "is that we're given an input\nx, a given image, an output y. We model a model that\npredicts y from x.",
    "start": "3292930",
    "end": "3300310"
  },
  {
    "text": "We do this with a dataset. Now, similar and\nanalogously to this,",
    "start": "3300310",
    "end": "3305619"
  },
  {
    "text": "we can view the meta-supervised\nlearning problem as something where we're given a dataset\nthat has k examples.",
    "start": "3305620",
    "end": "3314050"
  },
  {
    "text": "And a new data point, x test. And given this we want to\npredict the corresponding label",
    "start": "3314050",
    "end": "3321280"
  },
  {
    "text": "for x test. And so you can view the\nmeta-learning problem as something that\nessentially takes",
    "start": "3321280",
    "end": "3328000"
  },
  {
    "text": "in a data set and\na new data point, learn something from\nthat training dataset,",
    "start": "3328000",
    "end": "3334760"
  },
  {
    "text": "and makes a prediction\nthat hopefully generalizes to this new test\ndata point to predict y test.",
    "start": "3334760",
    "end": "3342400"
  },
  {
    "text": "And the way that we\ndo this is analogous to supervised learning. We have data. But in this case,\nour data corresponds",
    "start": "3342400",
    "end": "3349210"
  },
  {
    "text": "to a dataset of datasets,\nwhere each dataset has",
    "start": "3349210",
    "end": "3355480"
  },
  {
    "text": "more than k examples so\nthat you can sample k to be used in the training dataset. And at least one to be used to\nmeasure generalization here.",
    "start": "3355480",
    "end": "3364998"
  },
  {
    "text": "Now, you might ask,\nwell, you already talked about the\nmeta-learning problem. Why is this useful? Why are you telling me this?",
    "start": "3364998",
    "end": "3370618"
  },
  {
    "text": "This view is actually\nuseful because in many ways it reduces the meta-learning\nproblem to figuring out",
    "start": "3370618",
    "end": "3376140"
  },
  {
    "text": "how we might want to\ndesign this function h and how we want to\noptimize this function h. ",
    "start": "3376140",
    "end": "3383590"
  },
  {
    "text": "So essentially kind\nof a general recipe for meta-learning\nalgorithms is to choose a form of this function that\nbasically takes in a dataset,",
    "start": "3383590",
    "end": "3392512"
  },
  {
    "text": "learns something\nfrom that dataset, and makes predictions\non new data points. And then second,\nchoose how to optimize",
    "start": "3392512",
    "end": "3398980"
  },
  {
    "text": "the parameters of this\nlearner with respect to some maximum\nlikelihood objective using",
    "start": "3398980",
    "end": "3406360"
  },
  {
    "text": "the meta-training data. So this is basically kind\nof the general recipe",
    "start": "3406360",
    "end": "3411900"
  },
  {
    "text": "for meta-learning algorithms. We'll refer to theta as the\nmeta-parameters in the sense that they're the kind of\nparameters of this learner",
    "start": "3411900",
    "end": "3421380"
  },
  {
    "text": "here. And this function will\noften kind of refer to as the learner, in the sense\nthat it's learning something",
    "start": "3421380",
    "end": "3429300"
  },
  {
    "text": "from a dataset and applying\nwhat it's learned to predict the label for x test.",
    "start": "3429300",
    "end": "3434640"
  },
  {
    "text": " OK, so this is the\ngeneral recipe.",
    "start": "3434640",
    "end": "3441990"
  },
  {
    "text": "And really the simplest\nway to instantiate this is what I'll call\nblack box adaptation,",
    "start": "3441990",
    "end": "3450660"
  },
  {
    "text": "where the key idea is to kind\nof train a neural network",
    "start": "3450660",
    "end": "3455880"
  },
  {
    "text": "to represent this learner\nthat takes as input, a data set, and outputs\na set of parameters.",
    "start": "3455880",
    "end": "3465960"
  },
  {
    "text": "And then predicts\ntest data points with a model with\nthose parameters.",
    "start": "3465960",
    "end": "3471690"
  },
  {
    "text": "So in some ways, I guess\nbefore it called h a learner, you could also call\nthis a learner, where",
    "start": "3471690",
    "end": "3477740"
  },
  {
    "text": "it's taking a dataset\nand basically outputting a new set of parameters,\nanalogous to how",
    "start": "3477740",
    "end": "3483260"
  },
  {
    "text": "things like SGD take a\nset of initial parameters and then produce\na learned model.",
    "start": "3483260",
    "end": "3491020"
  },
  {
    "text": "But in this case, f isn't\ngoing to represent SGD, it's going to represent just\na black box neural network.",
    "start": "3491020",
    "end": "3499609"
  },
  {
    "text": "So we're basically going to\nbe training a neural network to take as input data and\noutput a set of parameters,",
    "start": "3499610",
    "end": "3505910"
  },
  {
    "text": "such that that set of\nparameters generalizes as well to new data points. ",
    "start": "3505910",
    "end": "3515480"
  },
  {
    "text": "So in this case,\nhere is D train. Here is the data\npoint in D test.",
    "start": "3515480",
    "end": "3520600"
  },
  {
    "text": "And we can train this with\nstandard supervised learning. So we can take our meta-training\ndata set and feed in the data",
    "start": "3520600",
    "end": "3528160"
  },
  {
    "text": "into this neural network, have\nit produce a set of parameters. And train it such that with\nthose set of parameters,",
    "start": "3528160",
    "end": "3536200"
  },
  {
    "text": "it makes accurate predictions. So what it looks like, for\nstandard supervised learning",
    "start": "3536200",
    "end": "3542280"
  },
  {
    "text": "is we sample a task. ",
    "start": "3542280",
    "end": "3547440"
  },
  {
    "text": "We have the train dataset for\nthat task, which produces phi. And then we train it\nsuch that this network",
    "start": "3547440",
    "end": "3554970"
  },
  {
    "text": "G with parameters phi can\naccurately predict labels based on that for that task.",
    "start": "3554970",
    "end": "3560850"
  },
  {
    "start": "3560850",
    "end": "3566080"
  },
  {
    "text": "Yeah, so you can also\nview kind of this loss as how well this set\nof parameters phi",
    "start": "3566080",
    "end": "3573010"
  },
  {
    "text": "does on the test set. And then from that\nstandpoint, you can view this\nmeta-training objective",
    "start": "3573010",
    "end": "3579370"
  },
  {
    "text": "as something that\nhas your model that takes as input in\nthe training dataset, produces a set of parameters\nbased on the training dataset.",
    "start": "3579370",
    "end": "3585520"
  },
  {
    "text": "And then it's trained such that\nset of parameters generalizes to a task data point.",
    "start": "3585520",
    "end": "3592040"
  },
  {
    "text": "And this optimization is\ndone across all of the tasks. So any clarification\nquestions on how this works?",
    "start": "3592040",
    "end": "3599770"
  },
  {
    "text": " Yeah, a couple of questions. On this slide, I'm just\nconfused [INAUDIBLE] to di,",
    "start": "3599770",
    "end": "3605727"
  },
  {
    "text": "if di is a bottleneck there. ",
    "start": "3605727",
    "end": "3614220"
  },
  {
    "text": "Yeah, so you do\nthis, this is kind of what happens during\ntraining, is you likely have,",
    "start": "3614220",
    "end": "3620817"
  },
  {
    "text": "we talked about before,\nyou have a train data set and a test data\nset for each task. And you sample different tasks.",
    "start": "3620818",
    "end": "3626820"
  },
  {
    "text": "You pass those into your model. That produces a\nset of parameters, and then you evaluate the\ngeneralization performance",
    "start": "3626820",
    "end": "3633960"
  },
  {
    "text": "of that set of parameters. And then at test\ntime, you no longer have any of the\nmeta-training data set.",
    "start": "3633960",
    "end": "3640490"
  },
  {
    "text": "And you're given a data\nset for a test task. ",
    "start": "3640490",
    "end": "3646810"
  },
  {
    "text": "This is kind of the training\ndata set for this new task. You pass that into the\nmodel and that gives you",
    "start": "3646810",
    "end": "3651880"
  },
  {
    "text": "a model that generalizes\nfor that test task. But you don't need any\nof the meta-training data",
    "start": "3651880",
    "end": "3659230"
  },
  {
    "text": "to do this at test time. Well actually, my\nquestion is about the TI in the bottom\nright of the slide.",
    "start": "3659230",
    "end": "3665390"
  },
  {
    "text": "Sorry, can you go\nover that again? Yeah, so this TI,\nthis is a sum over all",
    "start": "3665390",
    "end": "3671830"
  },
  {
    "text": "of the tasks in your\nmeta-training data set. OK, fine.",
    "start": "3671830",
    "end": "3678799"
  },
  {
    "text": "I have one more question\nif you have time. If you go two slides\nprior, or one slide prior,",
    "start": "3678800",
    "end": "3688420"
  },
  {
    "text": "maybe one more.  It's not about the k and\nof the [INAUDIBLE] just",
    "start": "3688420",
    "end": "3700036"
  },
  {
    "text": "for [INAUDIBLE]. Maybe I'll just follow up on\nPiazza, because probably it's mostly about the slides. Sounds good, yeah. ",
    "start": "3700037",
    "end": "3707920"
  },
  {
    "text": "I think it's on the last\nslide that you presented. ",
    "start": "3707920",
    "end": "3713557"
  },
  {
    "text": "Basically, I'm\nwondering conceptually why we introduce phi i as\nopposed to treating this",
    "start": "3713558",
    "end": "3719030"
  },
  {
    "text": "as a traditional learning\nproblem or in addition to your x test, you're\nalso priming the network",
    "start": "3719030",
    "end": "3727190"
  },
  {
    "text": "with the support set for\nthe test that corresponds",
    "start": "3727190",
    "end": "3734240"
  },
  {
    "text": "to the training example and the\ntest example that you've given [INAUDIBLE].",
    "start": "3734240",
    "end": "3739680"
  },
  {
    "text": "Yeah, so you could,\nin some ways, you don't necessarily need to\nrepresent phi separately.",
    "start": "3739680",
    "end": "3748772"
  },
  {
    "text": "And we'll talk about a situation\nwhere phi doesn't actually correspond to a huge parameter\nvector in a few slides.",
    "start": "3748772",
    "end": "3754575"
  },
  {
    "text": " Yeah, so you don't need to\nnecessarily represent that.",
    "start": "3754575",
    "end": "3761010"
  },
  {
    "text": "In this case, in\neither case though, you are taking in the\nsupport set right here.",
    "start": "3761010",
    "end": "3766530"
  },
  {
    "text": "These are kind of your\nsupport data points. And then passing in a\nnew data point here. Does that answer your question?",
    "start": "3766530",
    "end": "3772460"
  },
  {
    "text": " Sort of. ",
    "start": "3772460",
    "end": "3781084"
  },
  {
    "text": "The question was\nmore why i, I guess",
    "start": "3781084",
    "end": "3786880"
  },
  {
    "text": "that mechanically\nyou just said it doesn't make a difference, so. We'll also talk about,\nbasically, maybe",
    "start": "3786880",
    "end": "3794470"
  },
  {
    "text": "ask a question in a few\nslides if a few slides ahead don't kind of clarify it.",
    "start": "3794470",
    "end": "3801075"
  },
  {
    "text": "One important\nthing I will say is that it's important\nthat this phi i depends on i, because this is\ngoing to be different",
    "start": "3801075",
    "end": "3807868"
  },
  {
    "text": "for different tasks, depending\non the training data set that you [INAUDIBLE]. ",
    "start": "3807868",
    "end": "3816540"
  },
  {
    "text": "Yes, just wanted to clarify\nsome technical things. So f theta is I know\nthat that predicts",
    "start": "3816540",
    "end": "3822713"
  },
  {
    "text": "another neural\nnetwork's probably, like a task [INAUDIBLE]. And then when we have\nthose parameters, do we set that as the initial\nparameters of the network of G.",
    "start": "3822713",
    "end": "3831270"
  },
  {
    "text": "And then we train G to make\nthose parameters optimal. So x y predicts y\nalmost peripherally.",
    "start": "3831270",
    "end": "3837670"
  },
  {
    "text": "And then do we just [INAUDIBLE]\nto get the predicted phi i to be the optimal phi i.",
    "start": "3837670",
    "end": "3843494"
  },
  {
    "text": "Kind of like that, and\nthen backpropgate that in.  That's close, although it's\nnot quite that complicated.",
    "start": "3843495",
    "end": "3851467"
  },
  {
    "text": "You basically have a neural\nnetwork that takes as input, the training data set,\nand outputs the parameters for this other network.",
    "start": "3851467",
    "end": "3859079"
  },
  {
    "text": "You don't need to do any\nadditional training on phi. ",
    "start": "3859080",
    "end": "3865170"
  },
  {
    "text": "What you'll be doing\nhere is basically it opens the parameters of\nanother neural network that",
    "start": "3865170",
    "end": "3871500"
  },
  {
    "text": "corresponds to this\nneural network right here. And then when you do this kind\nof meta-training optimization,",
    "start": "3871500",
    "end": "3877859"
  },
  {
    "text": "you're optimizing over the\nparameters theta of this model right here.",
    "start": "3877860",
    "end": "3883220"
  },
  {
    "text": "And you're basically\noptimizing this model so that it can effectively\nproduce parameters for these different tasks.",
    "start": "3883220",
    "end": "3891580"
  },
  {
    "text": "OK, so I guess\nlike the gradients would kind of be\n[INAUDIBLE] straight, like the data [INAUDIBLE].",
    "start": "3891580",
    "end": "3898212"
  },
  {
    "text": "The gradients will pass\nstraight directly through phi into theta.",
    "start": "3898212",
    "end": "3903270"
  },
  {
    "text": "OK, and then for\nphi i, does that have to be the\nsame for all tasks?",
    "start": "3903270",
    "end": "3908290"
  },
  {
    "text": "Or I'm guessing there has to\nbe whatever task you have, it has to be the same\narchitectural neural network",
    "start": "3908290",
    "end": "3914930"
  },
  {
    "text": "that G would be. So in this case, you\nwould want it to probably have the same neural network\nG for the same architecture",
    "start": "3914930",
    "end": "3920833"
  },
  {
    "text": "for different tasks. And yeah, but you'll\nhave different parameters",
    "start": "3920833",
    "end": "3925910"
  },
  {
    "text": "of that architecture\nfor different tasks. I see, OK. ",
    "start": "3925910",
    "end": "3934560"
  },
  {
    "text": "Hey, so judging\nby what you said, it seems like you're\ndistinguishing between the meta-parameters\nare just theta",
    "start": "3934560",
    "end": "3942000"
  },
  {
    "text": "and the task-specific\nparameters that are output by theta,\nwhich is phi i, right?",
    "start": "3942000",
    "end": "3947319"
  },
  {
    "text": "Yeah. I was wondering, so the whole\nthing with [INAUDIBLE] theta.",
    "start": "3947320",
    "end": "3955559"
  },
  {
    "text": "Is it like a two step\nkind of [INAUDIBLE] like you can compute\nthe loss of phi i,",
    "start": "3955560",
    "end": "3963680"
  },
  {
    "text": "and then I guess\nthat itself just depends on theta, [INAUDIBLE]? ",
    "start": "3963680",
    "end": "3972300"
  },
  {
    "text": "Yeah, so when you compute\nthe loss with regard to theta of this objective,\nfor example, what you'll get",
    "start": "3972300",
    "end": "3978570"
  },
  {
    "text": "is you'll get something\nthat corresponds to basically DL/D phi i. And then you'll get\nD phi i D theta, just",
    "start": "3978570",
    "end": "3989190"
  },
  {
    "text": "with the chain rule. That will allow you to\nget gradients for theta for each of the tasks.",
    "start": "3989190",
    "end": "3995160"
  },
  {
    "text": "And you'll average those\nacross all of the tasks. Cool, thanks. ",
    "start": "3995160",
    "end": "4007277"
  },
  {
    "text": "Yeah, so while\ntraining this model, were you writing\nto all the tasks",
    "start": "4007278",
    "end": "4012840"
  },
  {
    "text": "and then update the gradients? Or would you wait for one\ntask to converge first? How would you do that?",
    "start": "4012840",
    "end": "4020670"
  },
  {
    "text": "Yeah, so in practice, you'll\nsample a batch of tasks,",
    "start": "4020670",
    "end": "4027200"
  },
  {
    "text": "and kind of a mini\nbatch of tasks, and you'll average across that. So actually I'll just go\nthrough the algorithm right now.",
    "start": "4027200",
    "end": "4032630"
  },
  {
    "text": "So what this looks\nlike is you'll sample one task or a\nmini batch of tasks.",
    "start": "4032630",
    "end": "4040130"
  },
  {
    "text": "You will sample\ndisjoint data sets. So you train in D\ntest from the data",
    "start": "4040130",
    "end": "4046160"
  },
  {
    "text": "that you have for that task. So maybe the data that\nyou have for this task looks like something like this.",
    "start": "4046160",
    "end": "4052850"
  },
  {
    "text": "Then you'll basically put this\ninto a D train and a D test.",
    "start": "4052850",
    "end": "4058850"
  },
  {
    "text": "Then you'll compute phi i\nfrom the training data set.",
    "start": "4058850",
    "end": "4066910"
  },
  {
    "text": "Then you'll update the\nmeta-parameters using basically the gradient with respect to\ntheta, evaluated with respect",
    "start": "4066910",
    "end": "4076090"
  },
  {
    "text": "to how well phi i does\nat predicting new data",
    "start": "4076090",
    "end": "4081250"
  },
  {
    "text": "points in D test. If you have just a single\ntask that you sampled, then",
    "start": "4081250",
    "end": "4087519"
  },
  {
    "text": "this gradient will just\nbe with that one task. If you use a mini\nbatch of tasks, then you'll basically\ncompute the gradient",
    "start": "4087520",
    "end": "4093440"
  },
  {
    "text": "for each of these tasks. Compute the meta-gradient\nwith respect to the meta-parameter's\ntheta for each of the tasks",
    "start": "4093440",
    "end": "4098649"
  },
  {
    "text": "and use that to update theta.  And then you'll\niterate this process,",
    "start": "4098649",
    "end": "4104490"
  },
  {
    "text": "iteratively updating theta based\non the tasks that you sample. ",
    "start": "4104490",
    "end": "4112970"
  },
  {
    "text": "Yeah, so my question is,\nin this particular setting, the model is basically trying\nto learn a kind of pattern",
    "start": "4112970",
    "end": "4120972"
  },
  {
    "text": "matching, right? It's not really understanding\nwhat really that image is. All its trying to\ndo is, in the test",
    "start": "4120973",
    "end": "4126580"
  },
  {
    "text": "set, what is this\nparticular image, what does this do in the given\ntask training, right?",
    "start": "4126580",
    "end": "4132528"
  },
  {
    "text": "It's not really trying to\nunderstand what that image is, or what are the people\nor features of that image",
    "start": "4132529",
    "end": "4137591"
  },
  {
    "text": "is, right? It's just trying to match what\nare the clues to its given training set.",
    "start": "4137592",
    "end": "4142770"
  },
  {
    "text": "Yeah, so when you have, in this\ncase, a four-way classification",
    "start": "4142770",
    "end": "4148220"
  },
  {
    "text": "problem, then a kind\nof reasonable strategy is just to find something\nthat kind of matches,",
    "start": "4148220",
    "end": "4154920"
  },
  {
    "text": "well, how does this image\nlook like in comparison to each of the examples\nin the train set?",
    "start": "4154920",
    "end": "4160460"
  },
  {
    "text": "Once we talk about\napproaches that specifically try to do this kind\nof matching process in a few lectures when we\ntalk about non-parametric",
    "start": "4160460",
    "end": "4167809"
  },
  {
    "text": "meta-learners, one thing\nthat I'll emphasize is that if you're\ndoing classification,",
    "start": "4167810",
    "end": "4172818"
  },
  {
    "text": "this strategy might work. But if you have regression\ntasks or motor control tasks or something, then kind\nof the general principle",
    "start": "4172819",
    "end": "4180439"
  },
  {
    "text": "of trying to take in a data\nset and learn something from that data set\nthat generalizes may be a little bit different\nthan the matching process",
    "start": "4180439",
    "end": "4187609"
  },
  {
    "text": "that you just described. Right, so just for\nmy understanding, that's what we are\ntrying to do, right?",
    "start": "4187609",
    "end": "4193100"
  },
  {
    "text": "We are not really trying to get\na deeper understanding of what these images are, right? We are just going to see--",
    "start": "4193100",
    "end": "4199242"
  },
  {
    "text": "Yeah, so we're just\nbasically trying-- Be able to generalize-- you'll produce an\nimage classifier",
    "start": "4199242",
    "end": "4205850"
  },
  {
    "text": "that can classify among these\nfour classes, in this example.",
    "start": "4205850",
    "end": "4211050"
  },
  {
    "text": "We're not trying to generalize\nbeyond these four classes or beyond what's in\nthe training data set. ",
    "start": "4211050",
    "end": "4220700"
  },
  {
    "text": "And then one more question\nbefore we [AUDIO OUT].. Are there any trade\noffs besides complexity",
    "start": "4220700",
    "end": "4226800"
  },
  {
    "text": "if we wanted to make phi\ni robust permutations in the support set?",
    "start": "4226800",
    "end": "4232170"
  },
  {
    "start": "4232170",
    "end": "4237600"
  },
  {
    "text": "So basically you're saying\nthat if you basically change the order of the examples\nin the training dataset,",
    "start": "4237600",
    "end": "4245025"
  },
  {
    "text": "you're asking about\nhow you can make it robust to those permutations? Yes.",
    "start": "4245025",
    "end": "4250460"
  },
  {
    "text": "You could build a more\ncomplex model, I suppose. But is there anything\nelse that you should think about before\njumping ahead and doing that?",
    "start": "4250460",
    "end": "4259950"
  },
  {
    "text": "Yeah, so we'll talk about\nthat in one or two slides. ",
    "start": "4259950",
    "end": "4267017"
  },
  {
    "text": "Cool, so one challenge that\ncomes up with this approach is that outputting a lot of\nneural network parameters",
    "start": "4267017",
    "end": "4274949"
  },
  {
    "text": "doesn't necessarily seem\nvery scalable because you have to have a neural network\noutput all of those parameters.",
    "start": "4274950",
    "end": "4281873"
  },
  {
    "text": "And so one idea for\napproaching this, and this has actually been one\nof the more common ways to approach black box adaptation\napproaches is to not output all",
    "start": "4281873",
    "end": "4290600"
  },
  {
    "text": "the parameters, but\nonly output some of the sufficient\nstatistics that you need.",
    "start": "4290600",
    "end": "4296000"
  },
  {
    "text": "And what this looks like is\ninstead of outputting a phi i, you instead output this\nkind of hidden vector h.",
    "start": "4296000",
    "end": "4305699"
  },
  {
    "text": "And I'll use h because it's\nanalogous to the hidden state of a recurrent neural network.",
    "start": "4305700",
    "end": "4313090"
  },
  {
    "text": "And this represents kind of\ncontextual task information. And then this\nneural network G is",
    "start": "4313090",
    "end": "4319830"
  },
  {
    "text": "going to be basically\na combination of this contextual\ninformation h.",
    "start": "4319830",
    "end": "4327449"
  },
  {
    "text": "And then some fixed\nparameters theta tree that are the same for all\nof the different tasks.",
    "start": "4327450",
    "end": "4333059"
  },
  {
    "text": "And so you may just have\nanother neural network that has these\nparameters theta G and then also takes as input\ntask specific information",
    "start": "4333060",
    "end": "4340950"
  },
  {
    "text": "and uses that to classify\nthe images in D test. ",
    "start": "4340950",
    "end": "4348530"
  },
  {
    "text": "If you recall, when we talked\nabout multi-task learning, where we had this vector zi\nthat kind of encapsulates",
    "start": "4348530",
    "end": "4355180"
  },
  {
    "text": "task-specific information, this\nis also kind of pretty similar, where this hi can capture\ninformation about that task",
    "start": "4355180",
    "end": "4362410"
  },
  {
    "text": "and tell you what task you're\ntrying to be performing, where theta G would then\ncorrespond to the parameters",
    "start": "4362410",
    "end": "4367989"
  },
  {
    "text": "of this network. ",
    "start": "4367990",
    "end": "4373099"
  },
  {
    "text": "Yeah, and then in\nthis case, when you optimize over\nthe meta-parameters, you'll optimize\nboth over theta G",
    "start": "4373100",
    "end": "4379700"
  },
  {
    "text": "as well as the parameters of\nthis network that produces hi.",
    "start": "4379700",
    "end": "4385640"
  },
  {
    "text": "You may also have some amount\nof parameter sharing, where theta G may also be a part\nof this network on the left.",
    "start": "4385640",
    "end": "4392810"
  },
  {
    "text": "Maybe these encoder\nparameters are also kind of the same as how\nyou encode the input here.",
    "start": "4392810",
    "end": "4401108"
  },
  {
    "text": "And then you get\nsomething that looks like a recurrent neural\nnetwork, where it's just taking in data points, and then\nalso taking the test data point",
    "start": "4401108",
    "end": "4407600"
  },
  {
    "text": "and making a prediction. ",
    "start": "4407600",
    "end": "4413719"
  },
  {
    "text": "OK, and then the most\ngeneral form of this is something just a model that\ntakes as input, the training",
    "start": "4413720",
    "end": "4419060"
  },
  {
    "text": "dataset, and the test\ninput and produces the corresponding label. ",
    "start": "4419060",
    "end": "4429070"
  },
  {
    "text": "OK, now, one natural question\nis, what architecture should we use for this model f theta?",
    "start": "4429070",
    "end": "4437659"
  },
  {
    "text": "There's a few different\nkind of approaches. One is to use an LSTM or\na neural turing machine. These are basically\ndifferent architectures",
    "start": "4437660",
    "end": "4444290"
  },
  {
    "text": "that incorporate some amount of\nmemory into the neural network. You could also use\nan architecture",
    "start": "4444290",
    "end": "4450230"
  },
  {
    "text": "that is feed forward. Basically takes as input, each\nof the examples in the feed forward network.",
    "start": "4450230",
    "end": "4456199"
  },
  {
    "text": "Produces some embedding R. And\nthen averages that embedding R.",
    "start": "4456200",
    "end": "4463558"
  },
  {
    "text": "And that R is going to\nprovide to another network another feed forward\nnetwork that tells you information about the tasks.",
    "start": "4463558",
    "end": "4469730"
  },
  {
    "text": "I'm curious if\nanyone has thoughts on why a feed forward and\naveraging architecture",
    "start": "4469730",
    "end": "4474920"
  },
  {
    "text": "might be better than a\nrecurrent neural network? ",
    "start": "4474920",
    "end": "4487280"
  },
  {
    "text": "I think that the memory of\nan LSTM or a recurrent model is basically so you have a bunch\nof temporally displaced data.",
    "start": "4487280",
    "end": "4494314"
  },
  {
    "text": "And here you're not\nnecessarily naturally capturing any sort of version\nof temporalities, it's just a bunch\nof data points. ",
    "start": "4494314",
    "end": "4504690"
  },
  {
    "text": "Yeah, so the data\nthat you're receiving in a supervised learning\nproblem may not have,",
    "start": "4504690",
    "end": "4510900"
  },
  {
    "text": "you don't have any\ntemporal structure. And then also, as\nmentioned in the chat",
    "start": "4510900",
    "end": "4517860"
  },
  {
    "text": "it also what was\nmentioned earlier, it doesn't matter what order\nthese data points are in.",
    "start": "4517860",
    "end": "4525260"
  },
  {
    "text": "And if you kind of\npermute the ordering, then you should get\nthe same output, right?",
    "start": "4525260",
    "end": "4532070"
  },
  {
    "text": "And so this feed\nforward architecture is actually permutation\ninvariant for the data.",
    "start": "4532070",
    "end": "4537290"
  },
  {
    "text": "You'll get the same\nembedding as a function of the different\npermutations of the data. And so that encodes this kind\nof nature about most datasets",
    "start": "4537290",
    "end": "4547190"
  },
  {
    "text": "that they are\npermutation invariant. Of course there is also\ndata that sometimes",
    "start": "4547190",
    "end": "4552200"
  },
  {
    "text": "has temporal structure. And in those cases, things\nlike RNNs might be better. ",
    "start": "4552200",
    "end": "4558657"
  },
  {
    "text": "And then there's also other\nexternal memory mechanisms, people have used\ntemporal convolutions.",
    "start": "4558658",
    "end": "4564370"
  },
  {
    "text": "And these kinds of\narchitectures tend to do-- have shown strong performance\non different problems.",
    "start": "4564370",
    "end": "4571360"
  },
  {
    "text": "So for example, this model that\nuses attention and convolutions is able to get around 99%\naccuracy on 5-way Omniglot",
    "start": "4571360",
    "end": "4579940"
  },
  {
    "text": "and around 97% accuracy on\n20-way one-shot Omniglot.",
    "start": "4579940",
    "end": "4586690"
  },
  {
    "text": "OK, so pros and cons, one\nbenefit of this approach is it's very expressive. It's also easy to combine with\na variety of learning problems.",
    "start": "4586690",
    "end": "4594490"
  },
  {
    "text": "So we'll see later\nin the course how you can combine this with a\nreinforcement learning problem.",
    "start": "4594490",
    "end": "4600940"
  },
  {
    "text": "One downside is that RNNs\nand these architectures can be fairly complex.",
    "start": "4600940",
    "end": "4606340"
  },
  {
    "text": "And they also have to learn\nhow to process this data and learn from this data\nfrom scratch, basically.",
    "start": "4606340",
    "end": "4613059"
  },
  {
    "text": "And so it could be a challenging\noptimization problem. And you'll find in\nyour homework that it",
    "start": "4613060",
    "end": "4618597"
  },
  {
    "text": "can be a bit finicky based\non different hyperparameters and different choices. Although we'll hopefully\nprovide good hyperparameters so",
    "start": "4618598",
    "end": "4624520"
  },
  {
    "text": "you don't need to do\ntoo much searching. And as a result, it tends to\nbe fairly data inefficient in terms of the amount\nof meta-training data",
    "start": "4624520",
    "end": "4631180"
  },
  {
    "text": "that it needs. And then on\nWednesday, we'll talk about what happens if we\ntreat this as an optimization",
    "start": "4631180",
    "end": "4639470"
  },
  {
    "text": "problem instead of kind of\na black box neural network problem.",
    "start": "4639470",
    "end": "4644570"
  },
  {
    "text": "If people want to stick\naround and ask more questions, you're welcome to\nand I'll answer them.",
    "start": "4644570",
    "end": "4649650"
  },
  {
    "text": "If not, I'll see you\nall on Wednesday. ",
    "start": "4649650",
    "end": "4657000"
  }
]