[
  {
    "text": "Hello, everyone. Welcome to the final EE259\nlecture for spring 2023.",
    "start": "5640",
    "end": "13020"
  },
  {
    "text": "So last time, we started\ntalking about imaging and vision",
    "start": "13020",
    "end": "19560"
  },
  {
    "text": "systems for cameras,\nand we briefly talked about the\nhistory of imaging. And now we are ready\nto start talking",
    "start": "19560",
    "end": "26040"
  },
  {
    "text": "about the physics of imaging. So as we did for other\ntypes of sensors, we're going to start with\nthe principle of image",
    "start": "26040",
    "end": "34500"
  },
  {
    "text": "formation, which is\npretty simple, as you see in this picture. So typically, you have some\nobject out in the field,",
    "start": "34500",
    "end": "43380"
  },
  {
    "text": "and then your imaging\nsystem, or camera, consists of some imaging optics,\nwhich is an assembly of lenses",
    "start": "43380",
    "end": "53880"
  },
  {
    "text": "typically, and\nthen rays of light basically pass\nthrough your optics",
    "start": "53880",
    "end": "61320"
  },
  {
    "text": "and come to focus\non an image sensor. So this, for instance, used to\nbe a film in traditional film",
    "start": "61320",
    "end": "68700"
  },
  {
    "text": "cameras, but in\nthe modern cameras, these are digital image sensors. That we'll talk about in detail.",
    "start": "68700",
    "end": "73799"
  },
  {
    "text": "And the sensor basically--\nor the digital image sensors",
    "start": "73800",
    "end": "78960"
  },
  {
    "text": "consists of many pixels that\nmeasure light intensity.",
    "start": "78960",
    "end": "83580"
  },
  {
    "text": "And they can do that at\ndifferent wavelengths. So color cameras, for instance,\nhave pixels that measure light",
    "start": "84660",
    "end": "91979"
  },
  {
    "text": "intensity at red, green, and\nblue wavelengths, or RGB, but you can also have\nother types of cameras.",
    "start": "91980",
    "end": "97320"
  },
  {
    "text": "For instance, thermal cameras\nmeasure light intensity at long wave IR, which\nis in 6 to 12 micron",
    "start": "97320",
    "end": "108060"
  },
  {
    "text": "range in wavelengths. And then what happens\nafter that, basically there",
    "start": "108060",
    "end": "113160"
  },
  {
    "text": "is some signal\nprocessing engine, or image signal\nprocessing engine, on the camera called\nISP 4 short that",
    "start": "113160",
    "end": "121799"
  },
  {
    "text": "collects the signals from all\nthe pixels and combines them and processes them to form an\nimage, which is what we get out",
    "start": "121800",
    "end": "130140"
  },
  {
    "text": "of the camera. So the first thing\nyou might ask is,",
    "start": "130140",
    "end": "135480"
  },
  {
    "text": "why do we need the lenses\nor the imaging optics? Why can't we just put the\nsensor in front of the object",
    "start": "136080",
    "end": "144420"
  },
  {
    "text": "and have an image formed on it? And the reason is basically\nexplained in this picture here.",
    "start": "144420",
    "end": "152099"
  },
  {
    "text": "So without optics,\nwhat happens is that every sensor pixel would\nbasically receive rays of light",
    "start": "152100",
    "end": "160620"
  },
  {
    "text": "from every point on\nthe object and, it would record basically the\nintegral of light arriving",
    "start": "160620",
    "end": "167340"
  },
  {
    "text": "from every point on this object. Then essentially what happens\nis that all sensor pixels would",
    "start": "168000",
    "end": "176520"
  },
  {
    "text": "record a similar color\nand similar intensity, so you get a complete blur. And that's, for\ninstance, why we don't",
    "start": "176520",
    "end": "184140"
  },
  {
    "text": "get images formed on\nwalls or other diffusely reflective surfaces,\nbecause there is no optics.",
    "start": "184140",
    "end": "192180"
  },
  {
    "text": "Every point receives light\nfrom everything out there, and there is no image\nto be seen on that.",
    "start": "192180",
    "end": "199800"
  },
  {
    "text": "So that is why we need\nthe imaging optics.",
    "start": "199800",
    "end": "205260"
  },
  {
    "text": "And basically, the job of a\nproperly designed lens system",
    "start": "205260",
    "end": "210420"
  },
  {
    "text": "is to make sure that every point\nin this object, or the rays",
    "start": "210420",
    "end": "215880"
  },
  {
    "text": "coming from every\npoint in the subject get focused properly on a point\nin the image sensor, the image",
    "start": "215880",
    "end": "222720"
  },
  {
    "text": "plane, and then we get a\nnice, sharp image for it. Now, at the core of the\nimage formation principle",
    "start": "222720",
    "end": "231240"
  },
  {
    "text": "is this idea of exposure. So what exposure is,\nmathematically denoted by H,",
    "start": "231240",
    "end": "238800"
  },
  {
    "text": "it is the amount of light\nenergy per unit area",
    "start": "238800",
    "end": "244020"
  },
  {
    "text": "that reaches the surface\nof the image sensor.",
    "start": "244020",
    "end": "249060"
  },
  {
    "text": "So there's two factors\ncontributing to the exposure. So our exposure is\nthe multiplication",
    "start": "249060",
    "end": "254220"
  },
  {
    "text": "of basically the power density,\nor the optical flux density,",
    "start": "254220",
    "end": "259799"
  },
  {
    "text": "that is hitting\nthe sensor surface. That is denoted by E, and\nit's called irradiance",
    "start": "260520",
    "end": "267900"
  },
  {
    "text": "in the optics field.",
    "start": "267900",
    "end": "272460"
  },
  {
    "text": "For us, it is because\nwe know about the theory of electromagnetic waves.",
    "start": "275220",
    "end": "280140"
  },
  {
    "text": "The irradiance is\nbasically the magnitude of the pointing vector,\nwhich, as we know, is basically the\npower flux density",
    "start": "280800",
    "end": "288780"
  },
  {
    "text": "of an electromagnetic wave. So it has a unit of\nwatts 4 meters squared. So that's the first factor that\ncontributes to the exposure,",
    "start": "288780",
    "end": "295560"
  },
  {
    "text": "and then the second one\nis the time duration that every pixel in the\nsensor is exposed to light.",
    "start": "295560",
    "end": "303600"
  },
  {
    "text": "So that's T, which is\nthe exposure duration. So we multiply irradiance\nby the exposure duration,",
    "start": "303600",
    "end": "310800"
  },
  {
    "text": "and that gives the\ntotal exposure. That is, two factors,\nE and T, they",
    "start": "310800",
    "end": "317099"
  },
  {
    "text": "can be independently controlled\nin an imaging system. So irradiance is\ntypically controlled",
    "start": "317100",
    "end": "323160"
  },
  {
    "text": "by the aperture size of the\noptics, or the lens system. So the larger the aperture\nopening, it is more light",
    "start": "323160",
    "end": "331140"
  },
  {
    "text": "is going to be\nled through, which increases the irradiance on\nthe image sensor surfaces.",
    "start": "331140",
    "end": "338700"
  },
  {
    "text": "And then T is\nbasically controlled by the shutter speed. So basically, every system,\nyou can set the time duration",
    "start": "338700",
    "end": "348840"
  },
  {
    "text": "that the sensor is exposed\nto light per frame of image that you are capturing.",
    "start": "348840",
    "end": "354180"
  },
  {
    "text": "So then because you can\ncontrol these two factors independently, then for\nthe set amount of exposure,",
    "start": "355740",
    "end": "363300"
  },
  {
    "text": "you can have\ndifferent combinations of aperture size and\nshutter speed that",
    "start": "363300",
    "end": "369360"
  },
  {
    "text": "give you the same exposure. So you can increase\nyour aperture size and reduce the shutter\nspeed or the other way",
    "start": "369360",
    "end": "374820"
  },
  {
    "text": "around, reduce aperture size\nand increase shutter speed and keep exposure the same.",
    "start": "374820",
    "end": "379380"
  },
  {
    "text": "But it turns out that\nchanging the aperture size",
    "start": "380340",
    "end": "386280"
  },
  {
    "text": "and the shutter speed while\nkeeping the total exposure",
    "start": "386280",
    "end": "391320"
  },
  {
    "text": "to same would have\nvery different effects on the final image that you get.",
    "start": "391320",
    "end": "396780"
  },
  {
    "text": "Specifically, what\nincreasing the aperture size does to the image is that it\nreduces the depth of field.",
    "start": "397740",
    "end": "406560"
  },
  {
    "text": "So here, if we look at the\ntwo images on the top row-- so these are two images\ncaptured of the same scene.",
    "start": "407100",
    "end": "413580"
  },
  {
    "text": "And between the two images,\nthe total exposure is the same. As you can see, the\ncolors look the same,",
    "start": "413580",
    "end": "419220"
  },
  {
    "text": "the intensity looks the same,\nso it's the same exposure. But what has changed\nbetween the two",
    "start": "419220",
    "end": "425220"
  },
  {
    "text": "is that the left one has\na large aperture opening, and the right one has a\nsmall aperture opening,",
    "start": "425220",
    "end": "431760"
  },
  {
    "text": "which means to keep\nthe exposure the same, the left one had a faster\nshutter speed or smaller",
    "start": "431760",
    "end": "438480"
  },
  {
    "text": "exposure duration,\nand the right one had a longer exposure duration. But because the\nscene was stationary,",
    "start": "438480",
    "end": "444420"
  },
  {
    "text": "at the time didn't have an\neffect on the final image, but the effect of\nthe aperture opening,",
    "start": "444960",
    "end": "450120"
  },
  {
    "text": "as you see in the left one,\nthe background is completely blurred, versus\nin the right one,",
    "start": "450120",
    "end": "455160"
  },
  {
    "text": "we get a nicely sharp\nfocused background. And that's the idea\nof depth of field. So we'll talk more about\nit, but the general trade",
    "start": "455760",
    "end": "463260"
  },
  {
    "text": "is large aperture openings lets\nmore light through but reduces the depth of field.",
    "start": "463260",
    "end": "468419"
  },
  {
    "text": "And the depth of field,\nroughly speaking, is basically the\nrange of distances",
    "start": "468420",
    "end": "477300"
  },
  {
    "text": "for which the subjects will be\nin focus in the final image. Now, let's talk about\nreducing shutter speed.",
    "start": "477300",
    "end": "484500"
  },
  {
    "text": "So reducing shutter speed,\nthe effect that it will have is that it will increase\nmotion blur in the final image.",
    "start": "484500",
    "end": "491100"
  },
  {
    "text": "So if we look at\nthe bottom row here, again, there's two\npictures of the same scene",
    "start": "491100",
    "end": "496319"
  },
  {
    "text": "with the same amount of\nexposure, but between the two, the left one has a\nslow shutter speed",
    "start": "496320",
    "end": "501720"
  },
  {
    "text": "and the right one has\na fast shutter speed. And here the scene, unlike the\ntop one, was not stationary.",
    "start": "501720",
    "end": "506879"
  },
  {
    "text": "So the hand and the hammer were\nremoving as the shot was taken, and the slow shutter speed\ncauses, as you can see,",
    "start": "506880",
    "end": "514800"
  },
  {
    "text": "significant motion blur in the\nscene, versus the fast shutter speed like freezes the\nmoving objects in place",
    "start": "514800",
    "end": "524279"
  },
  {
    "text": "and basically makes a\nsharper final image.",
    "start": "524280",
    "end": "530820"
  },
  {
    "text": "The depth of field\neffect is also there. If you look very closely,\nthe one with the slow shutter",
    "start": "531780",
    "end": "537000"
  },
  {
    "text": "speed, to keep the\nexposure the same, it means that the aperture\nsize was a smaller. And if you look\nat the background,",
    "start": "537000",
    "end": "542160"
  },
  {
    "text": "the wood grains are more\nin focus in this one compared to the right\none, which is kind of--",
    "start": "542160",
    "end": "547200"
  },
  {
    "text": "it looks a little softer\nor more out of focus, and that's the depth\nof field effect.",
    "start": "547200",
    "end": "552420"
  },
  {
    "text": "So these effects always\nexist and go hand in hand.",
    "start": "552420",
    "end": "555720"
  },
  {
    "text": "So what's the right choice? Well, it depends\non the application. There's really no\nright or wrong here.",
    "start": "558360",
    "end": "564120"
  },
  {
    "text": "It really depends on, do you\ncare more about depth of field or do you care more about\nthe sharpness and the motion",
    "start": "564120",
    "end": "570060"
  },
  {
    "text": "blur for your application? And what is the right\ntrade off to make there?",
    "start": "570060",
    "end": "576960"
  },
  {
    "text": "Now, when it comes\nto exposure itself, it turns out that\nthere is always-- for every given scene for\nthe given dynamic range",
    "start": "578520",
    "end": "586620"
  },
  {
    "text": "and the lighting\nconditions, there's always a correct amount of exposure. So that's the total\nenergy density",
    "start": "586620",
    "end": "592620"
  },
  {
    "text": "of the light that hits the\nsensor within one frame. And that's the correct\nexposure is such that,",
    "start": "592620",
    "end": "601140"
  },
  {
    "text": "basically, the highlights\non the shadows in that scene",
    "start": "601800",
    "end": "606899"
  },
  {
    "text": "show nicely in the final image. So if we do less than the\ncorrect amount of exposure,",
    "start": "608280",
    "end": "615120"
  },
  {
    "text": "so if we underexposed the\nimage, that's the case, we're not enough light\nreaches the sensor. And in that case,\nespecially the shadows",
    "start": "615120",
    "end": "622560"
  },
  {
    "text": "would look too dark or\nmaybe completely black, and we lose all\nthe details there, which is the case in this image.",
    "start": "622560",
    "end": "628800"
  },
  {
    "text": "You see the darker\nareas, there's not a lot of detail\nbecause of under exposure.",
    "start": "628800",
    "end": "634140"
  },
  {
    "text": "And then the opposite side of\nit is the case of overexposure,",
    "start": "634140",
    "end": "641040"
  },
  {
    "text": "and that's where there's too\nmuch light reaching the sensor. And in that case, the\nhighlights or the bright regions",
    "start": "641040",
    "end": "647820"
  },
  {
    "text": "of the image can get washed\nout or completely saturated",
    "start": "647820",
    "end": "653400"
  },
  {
    "text": "if you hit the dynamic\nrange of the sensor, and then we lose\nall detail, as is",
    "start": "653400",
    "end": "658620"
  },
  {
    "text": "the case in this right image. As you can see, the sky and\nthe bright areas of the scene",
    "start": "658620",
    "end": "665940"
  },
  {
    "text": "are completely washed out,\nand we have lost all detail. And then this middle one\nhas the correct amount",
    "start": "665940",
    "end": "672180"
  },
  {
    "text": "of exposure, where\neverything is nicely exposed. Now, it's not like there's\njust one value of exposure",
    "start": "672180",
    "end": "681240"
  },
  {
    "text": "that is correct. If you look, this\none to the right",
    "start": "681240",
    "end": "687900"
  },
  {
    "text": "and to the left\nof the middle one, one is slightly\noverexposed, the other one",
    "start": "687900",
    "end": "694560"
  },
  {
    "text": "is slightly underexposed,\nbut still, you can make out all the details in this scene. So there is a bit\nof a margin there.",
    "start": "694560",
    "end": "701100"
  },
  {
    "text": "And especially when it comes\nto artistic photography, different people have\ndifferent tastes.",
    "start": "701100",
    "end": "706980"
  },
  {
    "text": "Like I myself, I'm not a big\nartist, but when I take photos, I tend to underexpose\na little bit",
    "start": "706980",
    "end": "712680"
  },
  {
    "text": "because I like to exaggerate\nthe shadows a little bit. Now, when it comes to autonomy,\nmodern cameras almost all",
    "start": "712680",
    "end": "722520"
  },
  {
    "text": "of them have algorithms\nthat try to-- by some sensing from the scene\nfrom like the total intensity",
    "start": "722520",
    "end": "733140"
  },
  {
    "text": "that they're getting\nand things like that, with some heuristic\nalgorithms, they try to automatically set\nthe correct exposure level.",
    "start": "733140",
    "end": "741000"
  },
  {
    "text": "And in most cases, it\nworks reasonably well, but they're not perfect\nin certain cases.",
    "start": "741000",
    "end": "746160"
  },
  {
    "text": "Especially scenes with\nvery high dynamic range, they need to make a\ntrade off or sometimes,",
    "start": "746160",
    "end": "752340"
  },
  {
    "text": "they just flat out\nmake a wrong decision. So just note that\nauto exposure exists,",
    "start": "752340",
    "end": "757920"
  },
  {
    "text": "and then all modern cameras. But it's never perfect, but\nit works reasonably well",
    "start": "757920",
    "end": "763740"
  },
  {
    "text": "in most cases. So that's about the high\nlevel physics of imaging.",
    "start": "763740",
    "end": "771779"
  },
  {
    "text": "As we said, a big part\nof very important part of any imaging system is the\nimaging optics or the lenses.",
    "start": "771780",
    "end": "778860"
  },
  {
    "text": "So now we're going to talk\na little more about lenses. If you remember, we did\ntalk about the lenses",
    "start": "778860",
    "end": "786240"
  },
  {
    "text": "in the context of LiDAR\nand Gaussian beams and the importance of lenses for\nproper confirmation of beams.",
    "start": "786240",
    "end": "792840"
  },
  {
    "text": "Here, we are going\nto look at lenses for different applications,\nwhich is image formation.",
    "start": "794100",
    "end": "799920"
  },
  {
    "text": "And we are going to develop\na different framework which is more useful for understanding\nthe physics of imaging.",
    "start": "799920",
    "end": "809160"
  },
  {
    "text": "And the framework is\ncalled geometric optics, as we will see. But at a high level, again, what\na lens system or imaging optics",
    "start": "809160",
    "end": "820019"
  },
  {
    "text": "does for us, as is shown\nin this bottom example, is that it makes sure that\nevery point on your subject",
    "start": "820020",
    "end": "827940"
  },
  {
    "text": "or the rays of light that\nare coming from that point,",
    "start": "827940",
    "end": "834300"
  },
  {
    "text": "they nicely get\nconverged or focused on a point in the image plane.",
    "start": "834300",
    "end": "840240"
  },
  {
    "text": "And that means we get a\nnicely focused sharp image",
    "start": "840240",
    "end": "845399"
  },
  {
    "text": "form versus anything\nelse that you try to do. We talked about\nhaving no lens at all,",
    "start": "845400",
    "end": "851279"
  },
  {
    "text": "and then you get just\na complete glare. You don't even have an\nimage, but other things like if you put a tiny pinhole\nand try to just restrict",
    "start": "851280",
    "end": "866160"
  },
  {
    "text": "the small rays of light\nto go through that, even a small pinhole, it's\nnot going to be perfect. There's going to be--",
    "start": "866160",
    "end": "872280"
  },
  {
    "text": "every point is going to\nform a small spot instead of a nicely focused point in\nthe image, and then you get--",
    "start": "872280",
    "end": "879780"
  },
  {
    "text": "with a pinhole, you\nget a blurred image. Just to note, pinhole\nphotography is a thing.",
    "start": "879780",
    "end": "888360"
  },
  {
    "text": "And there is techniques that\ncan make very nice images with pinholes, but they come\nwith very, very big trade off.",
    "start": "888360",
    "end": "895140"
  },
  {
    "text": "So for artistic\nphotography, they're used, but in practice, for\nsensing application,",
    "start": "895140",
    "end": "901800"
  },
  {
    "text": "nobody uses pinhole cameras. Basically, everybody\nuses cameras with optics.",
    "start": "901800",
    "end": "907020"
  },
  {
    "text": "So with that, when it\ncomes to understanding",
    "start": "908880",
    "end": "915060"
  },
  {
    "text": "the properties of lenses\nand their importance, there are two main\nphysical parameters",
    "start": "916380",
    "end": "922260"
  },
  {
    "text": "of lenses that basically\naffect the quality and the characteristics of\nthe images that are formed.",
    "start": "922260",
    "end": "928620"
  },
  {
    "text": "First is the focal length. And focal length, as we'll\nsee, determines the field",
    "start": "928620",
    "end": "934860"
  },
  {
    "text": "of view of the\ncamera, what azimuth",
    "start": "934860",
    "end": "940380"
  },
  {
    "text": "and elevation field of\nview you get in an image, and then there is\nthe second parameter, which is called the F\nnumber, or N, for the lens.",
    "start": "940380",
    "end": "949140"
  },
  {
    "text": "And the F number,\nas we'll see, it determines the\neffective aperture size. And as we briefly\ntalked, aperture size",
    "start": "949140",
    "end": "957899"
  },
  {
    "text": "affects the depth of field. And we'll talk in more\ndetail about this,",
    "start": "957900",
    "end": "966120"
  },
  {
    "text": "but at a high level, the\nfocal length and the F number,",
    "start": "966120",
    "end": "970980"
  },
  {
    "text": "together determine\nthe field of view and the depth of field of the\nimage that you're going to get. But there's other\nlens parameters",
    "start": "971820",
    "end": "978300"
  },
  {
    "text": "that some lenses have these. Some don't, but\ngenerally speaking, you can have fixed versus\nvariable focal length lenses.",
    "start": "978300",
    "end": "986100"
  },
  {
    "text": "So fixed focal length lenses\nare called prime lenses, and then zoom lenses\nare the ones where",
    "start": "987120",
    "end": "993480"
  },
  {
    "text": "you can vary the focal length. Every lens has a\nminimum focus distance. So if a subject is\ncloser to the lens,",
    "start": "994200",
    "end": "1002840"
  },
  {
    "text": "it just cannot focus on it. And you would always,\nno matter what you do,",
    "start": "1002840",
    "end": "1007940"
  },
  {
    "text": "get a blurred image. Some modern lenses have\nactive image stabilization.",
    "start": "1007940",
    "end": "1013340"
  },
  {
    "text": "So this is like an\nactive mechanical system",
    "start": "1013340",
    "end": "1019040"
  },
  {
    "text": "that moves around some\nof the optical elements inside the lens to\ncancel out vibrations.",
    "start": "1019040",
    "end": "1025820"
  },
  {
    "text": "It's kind of a\nrelatively fancy thing. So many lenses that\nare used in robotics,",
    "start": "1027200",
    "end": "1034220"
  },
  {
    "text": "they just don't have it, but\nsome more complex ones do.",
    "start": "1034220",
    "end": "1039559"
  },
  {
    "text": "And then there are lenses\nthat have perspective control. You might have heard of,\nlike tilt shift lenses.",
    "start": "1039560",
    "end": "1047060"
  },
  {
    "text": "And these are mostly used for\nartistic photography and not really for autonomy\napplications.",
    "start": "1047780",
    "end": "1054560"
  },
  {
    "text": "So we are not going\nto cover this. We will mostly talk about\nthese two characteristics",
    "start": "1054560",
    "end": "1060800"
  },
  {
    "text": "of the lenses. We want to understand the\nfocal length and the F number and how exactly did\nthey determine field",
    "start": "1060800",
    "end": "1066920"
  },
  {
    "text": "of view and the aperture size. As I said, the framework we\nwill use to analyze lenses",
    "start": "1066920",
    "end": "1075020"
  },
  {
    "text": "is called geometric optics. So what geometric\noptics is is basically, it's an extension of this idea\nof paraxial wave approximation,",
    "start": "1075020",
    "end": "1084139"
  },
  {
    "text": "which we developed as we were\ntalking about Gaussian beams and LiDARs.",
    "start": "1084140",
    "end": "1089600"
  },
  {
    "text": "And now, it turns out that\nyou can extend that idea",
    "start": "1089600",
    "end": "1093080"
  },
  {
    "text": "in the cases where the\nwavelength lambda is small compared to the size of\nthe optical components.",
    "start": "1094820",
    "end": "1100460"
  },
  {
    "text": "You can extend the\nparaxial wave approximation to what is called\nthe geometric optics. And what happens in the\ngeometric optics regime",
    "start": "1101600",
    "end": "1111200"
  },
  {
    "text": "is that we now, instead of\nthinking of light as a wave,",
    "start": "1111200",
    "end": "1117799"
  },
  {
    "text": "we just think model light\nas straight rays that",
    "start": "1117800",
    "end": "1123560"
  },
  {
    "text": "just travel on straight\npaths in homogeneous media. And these rays, they do\nbend at the interface",
    "start": "1123560",
    "end": "1131660"
  },
  {
    "text": "between two dissimilar media. And by the similar,\nwe mean like media that are different materials\nor have different refractive",
    "start": "1131660",
    "end": "1138980"
  },
  {
    "text": "indices. For instance, they\ncan bend these rays of light, which is exactly\nwhat, for instance, a lens does.",
    "start": "1138980",
    "end": "1144500"
  },
  {
    "text": "But just note that the geometric\noptics is very useful to, for instance, understand\nimage formation physics,",
    "start": "1144500",
    "end": "1152120"
  },
  {
    "text": "but it's an\napproximate framework. So for instance, some\neffects such as diffraction",
    "start": "1152120",
    "end": "1158779"
  },
  {
    "text": "or interference are completely\nignored in the geometric optics framework.",
    "start": "1158780",
    "end": "1163040"
  },
  {
    "text": "And again, it also\ncompletely-- and that's because it completely\nignores that wave nature of light,\nwhich is-- as we know,",
    "start": "1164180",
    "end": "1171260"
  },
  {
    "text": "that's the underlying physics. But nevertheless,\nthe framework is",
    "start": "1171980",
    "end": "1177380"
  },
  {
    "text": "extremely useful for the\npurpose of understanding imaging systems. And that's what we\nare going to use.",
    "start": "1177380",
    "end": "1183800"
  },
  {
    "text": "So when it comes to lenses\nin this geometric optics",
    "start": "1185000",
    "end": "1191180"
  },
  {
    "text": "framework, the\noperation of lenses, or at least I should say\nideal lenses for now,",
    "start": "1191840",
    "end": "1197840"
  },
  {
    "text": "you can add nonidealities to\nthe behavior, ideal lenses, also called thin lenses,\nthey behave very simply.",
    "start": "1199160",
    "end": "1205759"
  },
  {
    "text": "So the way they're\nmodeled is that, if you have some parallel\nrays of light,",
    "start": "1205760",
    "end": "1211520"
  },
  {
    "text": "as shown here, that basically\nare incident on the lens,",
    "start": "1211520",
    "end": "1216680"
  },
  {
    "text": "they get converged to a point,\nthat is a focal length F,",
    "start": "1216680",
    "end": "1222440"
  },
  {
    "text": "behind the lens. So you basically take\nthe center of the lens",
    "start": "1222440",
    "end": "1229460"
  },
  {
    "text": "and then a focal\nlength F away from it, all the parallel rays are\nconverged to that one point.",
    "start": "1229460",
    "end": "1234860"
  },
  {
    "text": "So that's one basically\nmain result that comes out",
    "start": "1234860",
    "end": "1240380"
  },
  {
    "text": "of the geometric optics. And then the other thing related\nto geometric optics and lenses",
    "start": "1240380",
    "end": "1248720"
  },
  {
    "text": "is that if you\nhave rays that goes through the so-called\noptical center of the lens,",
    "start": "1248720",
    "end": "1254600"
  },
  {
    "text": "those rays would\nnot be deviated. So they just go\nstraight through. And for ideal lenses\nor thin lenses,",
    "start": "1254600",
    "end": "1262220"
  },
  {
    "text": "the optical center is basically\njust the geometric center of the lens.",
    "start": "1265460",
    "end": "1269539"
  },
  {
    "text": "And again, as shown\nhere, the rays that are pointed towards\nthe optical center",
    "start": "1270800",
    "end": "1278120"
  },
  {
    "text": "just go straight through with no\nbending or anything like that. Now, with just these\ntwo principles,",
    "start": "1278120",
    "end": "1285680"
  },
  {
    "text": "it turns out that\nwe can fully analyze how image formation works,\nat least with ideal lenses.",
    "start": "1286580",
    "end": "1292820"
  },
  {
    "text": "But before that,\nwe need to discuss what the focal length\nis and how it's",
    "start": "1292820",
    "end": "1298640"
  },
  {
    "text": "related to the geometry of\nthe ideal or the thinness.",
    "start": "1298640",
    "end": "1303680"
  },
  {
    "text": "So the ideal lens, the\ngeometry is very simple.",
    "start": "1304280",
    "end": "1309380"
  },
  {
    "text": "So for instance, if you\nhave a double convex lens, the geometry is just described\nby the radii of curvature",
    "start": "1310280",
    "end": "1318980"
  },
  {
    "text": "of the two surfaces. So you have a radius of\ncurvature RL1 for the convex",
    "start": "1318980",
    "end": "1329540"
  },
  {
    "text": "surface, and you have RL2\nfor the concave surface.",
    "start": "1330380",
    "end": "1335540"
  },
  {
    "text": "So those are the two\nradii of curvature. Note that the convention here\nis that the radius of curvature",
    "start": "1336260",
    "end": "1344840"
  },
  {
    "text": "for convex surfaces, it's\ntaken to be positive, and for concave surfaces,\nit's taken to be negative.",
    "start": "1344840",
    "end": "1351680"
  },
  {
    "text": "So in this case, this surface\nis the convex surface. So RL1 is positive,\nand RL2 is negative",
    "start": "1351680",
    "end": "1361700"
  },
  {
    "text": "because this second surface\nis the concave surface, as the rays go in\nthis direction.",
    "start": "1362420",
    "end": "1368300"
  },
  {
    "text": "Now, with this definition\nand this geometry, the focal length is a function\nof the two radii of curvature",
    "start": "1369440",
    "end": "1376640"
  },
  {
    "text": "and also the refractive\nindex of the lens material, which could be glass or\nplastic or other materials.",
    "start": "1376640",
    "end": "1384440"
  },
  {
    "text": "And this is the equation. So 1 over f is n minus 1, times",
    "start": "1384440",
    "end": "1387860"
  },
  {
    "text": "And remember that\nthe refractive index n is the square root of\nthe relative of mu R times",
    "start": "1391760",
    "end": "1398720"
  },
  {
    "text": "epsilon R, where mu R is\nthe relative permeability",
    "start": "1398720",
    "end": "1404179"
  },
  {
    "text": "and epsilon R is the\nrelative permeability, which is equal to\nC over V, where",
    "start": "1404180",
    "end": "1411560"
  },
  {
    "text": "C is the speed of\nlight in vacuum and V is the speed of light\ninside the lens material.",
    "start": "1411560",
    "end": "1420200"
  },
  {
    "text": "So that's the speed of flight\ninside the lens, whatever",
    "start": "1420200",
    "end": "1431120"
  },
  {
    "text": "that material is. So basically, light gets\nslowed down by a factor of n",
    "start": "1431120",
    "end": "1439520"
  },
  {
    "text": "when it's traveling\ninside the lens. So that's how for a\nsimple ideal lens,",
    "start": "1439520",
    "end": "1449120"
  },
  {
    "text": "like a double convex lens, how\nyou can find the focal length.",
    "start": "1449120",
    "end": "1453380"
  },
  {
    "text": "Now with that and with\nthe two principles that we talked about, parallel\nrates hitting the lens,",
    "start": "1454340",
    "end": "1462500"
  },
  {
    "text": "getting converged to a\npoint, a focal lengths behind and rays hitting the optical\ncenter, going straight through,",
    "start": "1462500",
    "end": "1471020"
  },
  {
    "text": "now we can have\na very simplified model using geometric optics\nof how image formation works.",
    "start": "1471920",
    "end": "1479420"
  },
  {
    "text": "So here, we assume we\nhave this ideal lens. And then, it's a 2D picture,\nbut it works the same in 3D.",
    "start": "1479420",
    "end": "1487040"
  },
  {
    "text": "So we have some object,\nor a point on an object, shown by the tip of\nthis arrow basically,",
    "start": "1487040",
    "end": "1492980"
  },
  {
    "text": "and the rays of light that\nare coming from that object",
    "start": "1492980",
    "end": "1501620"
  },
  {
    "text": "basically, all of them\nget converged to a point",
    "start": "1501620",
    "end": "1506780"
  },
  {
    "text": "on the image plane. So they all basically\nget focused.",
    "start": "1506780",
    "end": "1513620"
  },
  {
    "text": "And again, the parallel\nones that hit the lens, they go through the\nfocal point of the lens.",
    "start": "1513620",
    "end": "1522740"
  },
  {
    "text": "So that's the point which\nis a distance f away, and then the ray that is going\nthrough the optical center",
    "start": "1522740",
    "end": "1530180"
  },
  {
    "text": "goes right through. And then the point of\nintersection of those rays",
    "start": "1530180",
    "end": "1536060"
  },
  {
    "text": "is basically the focus\npoint on the image plane. And any other ray\nthat we basically",
    "start": "1536060",
    "end": "1542360"
  },
  {
    "text": "launch from that target is\ngoing to go, in the ideal case,",
    "start": "1542360",
    "end": "1549440"
  },
  {
    "text": "through the same focus point. So all the rays basically\nget converged in one point.",
    "start": "1549440",
    "end": "1555500"
  },
  {
    "text": "And this basically method\nof analyzing image formation",
    "start": "1555500",
    "end": "1561260"
  },
  {
    "text": "is also called Gauss' ray\ntracing image construction. Now, in this geometric view,\nwe need to basically understand",
    "start": "1561260",
    "end": "1571340"
  },
  {
    "text": "the main thing is like the\ndistance between the object",
    "start": "1571340",
    "end": "1578120"
  },
  {
    "text": "to the lens and the distance\nbetween the image to the lens. So we need to be able\nto find those distances",
    "start": "1578120",
    "end": "1585919"
  },
  {
    "text": "as well as basically, the\nheight of the image point",
    "start": "1585920",
    "end": "1594500"
  },
  {
    "text": "relative to the height\nof the object, which is like a magnification\nfactor, if you will.",
    "start": "1594500",
    "end": "1599900"
  },
  {
    "text": "Now, by pure geometric\nanalysis, and by this I mean just looking at\nsimilarities of triangles",
    "start": "1602180",
    "end": "1610340"
  },
  {
    "text": "and things like that, which\nI won't go through it,",
    "start": "1610340",
    "end": "1615799"
  },
  {
    "text": "but it's a very simple\ngeometric analysis, we can come up with the\nGaussian lens formula, which",
    "start": "1615800",
    "end": "1622400"
  },
  {
    "text": "relates the distance between\nthe lens plane and the object",
    "start": "1622400",
    "end": "1628040"
  },
  {
    "text": "and the distance between\nthe lens plane and the image and the focal\nlength of the lens. And it's given by this\nvery famous simple formula,",
    "start": "1628040",
    "end": "1634880"
  },
  {
    "text": "which is 1 over SI. SI is the object distance\nto the lens plus 1 over SO,",
    "start": "1634880",
    "end": "1641180"
  },
  {
    "text": "where SO is the image. Distance to the lens equals One thing to note is that these\nquantities, SI and SO, these",
    "start": "1641180",
    "end": "1650480"
  },
  {
    "text": "are signed distances. And for SI, the\npositive is right word.",
    "start": "1650480",
    "end": "1656419"
  },
  {
    "text": "So in this specific\npicture, SI is positive. SO is also positive, but for\nSO, positive is left word.",
    "start": "1656420",
    "end": "1665240"
  },
  {
    "text": "Now, for very specific cases\nlike special types of lenses,",
    "start": "1665240",
    "end": "1670760"
  },
  {
    "text": "SI, for instance,\ncan become negative, and that's where we have\na virtual image that is formed in front of the lens\ninstead of behind the lens.",
    "start": "1671720",
    "end": "1678920"
  },
  {
    "text": "So just note that those\nare signed quantities with different\npositive directions,",
    "start": "1678920",
    "end": "1685700"
  },
  {
    "text": "and also, we can find by,\nagain, a purely geometric type",
    "start": "1685700",
    "end": "1691820"
  },
  {
    "text": "of analysis the\nratio between YI, which is the height\nof the image point,",
    "start": "1691820",
    "end": "1697700"
  },
  {
    "text": "to the YO, which is the\nheight of the object point. And we call that MT, which\nis the lateral magnification",
    "start": "1697700",
    "end": "1706160"
  },
  {
    "text": "factor. So that's YI over YO. And YI and YO are also\nsigned quantities,",
    "start": "1706160",
    "end": "1712280"
  },
  {
    "text": "and here positive is\nupward for both of them. So with that, convention\nYO is positive,",
    "start": "1712280",
    "end": "1721280"
  },
  {
    "text": "YI is negative because\nthe image always comes flipped relative to the object.",
    "start": "1721280",
    "end": "1727580"
  },
  {
    "text": "And because of that,\nYI is negative. So the lateral\nmagnification YI over YO",
    "start": "1727580",
    "end": "1735679"
  },
  {
    "text": "becomes equal to\nnegative SI over SO.",
    "start": "1736220",
    "end": "1740900"
  },
  {
    "text": "And again, these equations\nare very simple to derive.",
    "start": "1741680",
    "end": "1749060"
  },
  {
    "text": "Maybe just to convince you\nthese are from pure geometry,",
    "start": "1749060",
    "end": "1754700"
  },
  {
    "text": "like this one, you get it just\nby the similarity of these two triangles that I'm going\nto highlight in blue,",
    "start": "1754700",
    "end": "1761540"
  },
  {
    "text": "so this one and this one.",
    "start": "1761540",
    "end": "1765140"
  },
  {
    "text": "These are two similar\ntriangles, and the sides",
    "start": "1766580",
    "end": "1771740"
  },
  {
    "text": "are of length YI and YO. And then this is SO and SI.",
    "start": "1771740",
    "end": "1776900"
  },
  {
    "text": "So just by similarity, that\ngives you this equation here.",
    "start": "1776900",
    "end": "1780920"
  },
  {
    "text": "Very good. So that's basically with the\ngeometric optics framework,",
    "start": "1783320",
    "end": "1790520"
  },
  {
    "text": "how image formation works,\nbut the question you might ask",
    "start": "1790520",
    "end": "1798140"
  },
  {
    "text": "is, what if we are not working\nwith a simple thin lens? What if we are working\nwith a optical system that",
    "start": "1798140",
    "end": "1806060"
  },
  {
    "text": "contains many lenses? And that's the case for\nmany modern optical systems like even small tiny\ndigital cameras.",
    "start": "1806060",
    "end": "1813139"
  },
  {
    "text": "Like the ones in the\nphones, these days have optics that consist\nof multiple lenses,",
    "start": "1813920",
    "end": "1821360"
  },
  {
    "text": "lenses go into it. And those are not the simple\npicture that we analyzed here.",
    "start": "1823340",
    "end": "1829820"
  },
  {
    "text": "And they are called-- or they can be modeled as-- the whole optical assembly\ncontaining all the lenses",
    "start": "1829820",
    "end": "1836840"
  },
  {
    "text": "can be modeled as\none thick lens. And thick lenses\ncan be characterized",
    "start": "1836840",
    "end": "1842420"
  },
  {
    "text": "by few parameters. We are not going to go\nthrough the full details. So instead of just the\nfocal length and the radius",
    "start": "1842420",
    "end": "1849140"
  },
  {
    "text": "of curvature, there is\nmultiple parameters that go into the description of--",
    "start": "1849140",
    "end": "1854600"
  },
  {
    "text": "or the model of a thick lens. So there is the secondary\nprinciple plane, the primary principle plane.",
    "start": "1855140",
    "end": "1861440"
  },
  {
    "text": "So in a way, you have\na different focal point f prime behind the\nlens, then another one",
    "start": "1865220",
    "end": "1872780"
  },
  {
    "text": "in front of the lens,\nwith different distance, so we have f prime and F.\nBut at the end of the day,",
    "start": "1873560",
    "end": "1879260"
  },
  {
    "text": "again, there's a link\nhere that describes the full characterization and\nthe model of the thick lens,",
    "start": "1880280",
    "end": "1886040"
  },
  {
    "text": "but the same ray\ntracing principles apply to the thick lenses.",
    "start": "1886040",
    "end": "1891200"
  },
  {
    "text": "So if you have an\nobject and then you basically trace the\nrays of light that come out",
    "start": "1891200",
    "end": "1897500"
  },
  {
    "text": "of a point and the object,\nthey do get converged to a point on an image plane.",
    "start": "1897500",
    "end": "1903020"
  },
  {
    "text": "So essentially, at\nthe end of the day, it's the same principle\nof image formation,",
    "start": "1903980",
    "end": "1909080"
  },
  {
    "text": "but the geometry and\nthe model becomes",
    "start": "1909080",
    "end": "1914299"
  },
  {
    "text": "slightly more complicated,\nnot even too much complicated.",
    "start": "1915020",
    "end": "1918740"
  },
  {
    "text": "So everything we\ntalk about, we did it for like a simple thin\nlens, but bear in mind",
    "start": "1920180",
    "end": "1925220"
  },
  {
    "text": "that it applies to more\ncomplex optical systems also.",
    "start": "1925220",
    "end": "1929960"
  },
  {
    "text": "So one thing, I'm going to\njust go back to this picture here and clean this\nup a little bit.",
    "start": "1932720",
    "end": "1939500"
  },
  {
    "text": "So if you look at this\nGaussian lens formula here, this one, this is telling\nus something very important.",
    "start": "1939500",
    "end": "1947720"
  },
  {
    "text": "Again, if we go beyond\nthe math, and just",
    "start": "1947720",
    "end": "1952880"
  },
  {
    "text": "think about what\nit means, what does this imply about the\nphysics of imaging,",
    "start": "1953480",
    "end": "1959720"
  },
  {
    "text": "one thing we can see\nhere is that, if we fix the focal\nlength F and we also",
    "start": "1960920",
    "end": "1968420"
  },
  {
    "text": "fix the position of our\nimage sensor, meaning, S of I",
    "start": "1968420",
    "end": "1973940"
  },
  {
    "text": "is fixed, we just have\na sensor at a fixed position relative to\nthe lens, and that's SI,",
    "start": "1973940",
    "end": "1980900"
  },
  {
    "text": "and that is fixed,\nand we are not going to change that, with\nthose two parameter fixed,",
    "start": "1981740",
    "end": "1987799"
  },
  {
    "text": "that means SO would\nalso become fixed. The SO that solves this\nequation becomes fixed.",
    "start": "1987800",
    "end": "1994820"
  },
  {
    "text": "And what that means is that for\na lens of fixed focal length",
    "start": "1994820",
    "end": "2000820"
  },
  {
    "text": "and fixed position\nof the image sensor, only objects in one specific\nplane out in the field",
    "start": "2000820",
    "end": "2010480"
  },
  {
    "text": "would become perfectly\nfocused in the image plane.",
    "start": "2010480",
    "end": "2014980"
  },
  {
    "text": "Any object with\nan SO that is not the one that solves this\nequation would be out of focus.",
    "start": "2015880",
    "end": "2023260"
  },
  {
    "text": "So if you have another object\nslightly in front or slightly behind that main object,\nthose two still form something",
    "start": "2023260",
    "end": "2031600"
  },
  {
    "text": "on the image plane, but it's not\ngoing to be in perfect focus. And that's a very, very\nimportant principle to note.",
    "start": "2031600",
    "end": "2041740"
  },
  {
    "text": "So then what can we do to\nchange that focus distance?",
    "start": "2044800",
    "end": "2050500"
  },
  {
    "text": "So if you want to bring\nobjects at different distances from the lens into\nfocus, what should we do?",
    "start": "2050500",
    "end": "2059620"
  },
  {
    "text": "If the lens has a fixed focal\nlength f, again, for that,",
    "start": "2059620",
    "end": "2066460"
  },
  {
    "text": "only objects at a specific\ndistance, or are in focus, so in order to focus on\nobjects at different distances,",
    "start": "2066460",
    "end": "2073360"
  },
  {
    "text": "we need to move the sensor\nrelative to the lens.",
    "start": "2073360",
    "end": "2078460"
  },
  {
    "text": "So we have to move the\nsensor relative to the lens. And that's the picture\nthat's shown here.",
    "start": "2079360",
    "end": "2086020"
  },
  {
    "text": "So when the sensor is exactly\nat the focal plane of the lens,",
    "start": "2086020",
    "end": "2092560"
  },
  {
    "text": "so that's where SI equals\nto F, if we go back",
    "start": "2092560",
    "end": "2098080"
  },
  {
    "text": "to our Gaussian lens\nformula, which was 1 over SI plus 1 over SO equals 1\nover F, that means 1 over SO",
    "start": "2098080",
    "end": "2107920"
  },
  {
    "text": "should be 0, which means\nSO equals infinity. So when the sensor is at\nthe focal point of the lens,",
    "start": "2107920",
    "end": "2114400"
  },
  {
    "text": "we are focusing at infinity. That's one case.",
    "start": "2114400",
    "end": "2120160"
  },
  {
    "text": "Now, as you start\nmoving your sensor back, the focus plane starts getting\ncloser and closer to the lens.",
    "start": "2120160",
    "end": "2128440"
  },
  {
    "text": "You move your sensor\neven further back. Now your focus plane is closer. And then at some\npoint, you basically--",
    "start": "2128440",
    "end": "2137020"
  },
  {
    "text": "if the focus plane is\nat the focal length, so if SO equals F, that means\nSI should equal infinity.",
    "start": "2137980",
    "end": "2147040"
  },
  {
    "text": "So that means you have to put\nyour sensor infinitely away",
    "start": "2147040",
    "end": "2152320"
  },
  {
    "text": "from your lens to be able\nto focus at the focal length",
    "start": "2152320",
    "end": "2158980"
  },
  {
    "text": "in front of the lens, which\nbasically means you cannot do that. So you cannot focus on objects\ncloser to the lens than its",
    "start": "2158980",
    "end": "2166060"
  },
  {
    "text": "focal length. And that's like that\nidea of the minimum focus distance for a given lens.",
    "start": "2166060",
    "end": "2174100"
  },
  {
    "text": "So this is important\nto note, that in order to focus at different\nplanes out in the field,",
    "start": "2174100",
    "end": "2181300"
  },
  {
    "text": "you need to move the sensor\nrelative to the lens. Another way of thinking\nabout it, which I",
    "start": "2181300",
    "end": "2187600"
  },
  {
    "text": "really, really like this\nway of thinking about it, which is given in this box\nhere, is that lenses actually",
    "start": "2187600",
    "end": "2195460"
  },
  {
    "text": "transform a 3D object, or a",
    "start": "2195460",
    "end": "2198339"
  },
  {
    "text": "So the actual image that\nis formed behind the lens is a 3D image, but because\nwe have a 2D sensor, just",
    "start": "2200980",
    "end": "2210820"
  },
  {
    "text": "a sensor in a plane, we can\nonly extract the 2D slice from that image.",
    "start": "2210820",
    "end": "2216580"
  },
  {
    "text": "And that slice is where\nyou pit your image. So we just extract the",
    "start": "2216580",
    "end": "2221619"
  },
  {
    "text": "And in that slice,\nonly objects that correspond to a\nspecific distance",
    "start": "2224200",
    "end": "2231760"
  },
  {
    "text": "from the lens in the\nphysical world are in focus, and everything else that\nis closer or further away",
    "start": "2231760",
    "end": "2237940"
  },
  {
    "text": "is going to be out of focus. That's another way\nof thinking about it.",
    "start": "2237940",
    "end": "2243160"
  },
  {
    "text": "So that's about changing\nthe focus distance. Another thing we\nneed to discuss is",
    "start": "2243160",
    "end": "2249039"
  },
  {
    "text": "what about changing\nthe focal length? And you can change\nthe focal length",
    "start": "2249040",
    "end": "2254800"
  },
  {
    "text": "by changing the lens\nentirely or, as we said, some zoom lenses have moving\noptical elements inside them.",
    "start": "2254800",
    "end": "2262600"
  },
  {
    "text": "And for instance, with a ring\nor some other control mechanism,",
    "start": "2262600",
    "end": "2268060"
  },
  {
    "text": "you can vary the focal length\nof the lens and some range.",
    "start": "2268600",
    "end": "2273640"
  },
  {
    "text": "So lenses that have larger\nor longer focal lengths, those are called weak\nlenses because they",
    "start": "2274240",
    "end": "2281260"
  },
  {
    "text": "bend to light less. So weak lenses have\nlonger focal lengths.",
    "start": "2281260",
    "end": "2286780"
  },
  {
    "text": "And as you see in this picture,\nso here, we have a system. And first, we were using\na lens with a focal length",
    "start": "2288880",
    "end": "2296320"
  },
  {
    "text": "of, say, 3 inches. And then to bring\nthis point in focus, which is where the\ntree is, we had",
    "start": "2296320",
    "end": "2302800"
  },
  {
    "text": "to place our sensor at\ncertain distance away. Now if we use weaker lens,\nand keeping everything",
    "start": "2302800",
    "end": "2311800"
  },
  {
    "text": "else the same, so now we go\nto a focal length of 6 inches, for instance, in order to\nfocus on the same image plane,",
    "start": "2311800",
    "end": "2320440"
  },
  {
    "text": "we need to move the\nsensor further back. Again, because this\nis a weaker lens and it's bending the\nrays of light less,",
    "start": "2320440",
    "end": "2327400"
  },
  {
    "text": "so the point that those\nrays converge are now going to be further\naway from the lens.",
    "start": "2327400",
    "end": "2332620"
  },
  {
    "text": "But another thing that happens,\nwhich is very important here,",
    "start": "2332620",
    "end": "2337600"
  },
  {
    "text": "is that-- OK, so we need to\nmove the sensor back with the longer focal length\nto focus on the same subject",
    "start": "2338380",
    "end": "2345760"
  },
  {
    "text": "basically, but if the\nsize of the sensor remains the same, which\nis almost always the case,",
    "start": "2345760",
    "end": "2351940"
  },
  {
    "text": "now we're going to have\na smaller field of view, as you saw from-- see, from these\nray tracing here.",
    "start": "2352720",
    "end": "2359380"
  },
  {
    "text": "So at the top one,\nthe field of view is like the topmost\npixel on the image.",
    "start": "2359380",
    "end": "2366880"
  },
  {
    "text": "We just pass it through the\noptical center of the lens, and that's hitting\nthe base of the tree.",
    "start": "2366880",
    "end": "2372520"
  },
  {
    "text": "And the bottom pixel,\nif you trace the ray, that's hitting that. It's getting the top of the\ntree, so our field of view",
    "start": "2372520",
    "end": "2380200"
  },
  {
    "text": "cover the entire tree. Now, with the weaker lens and\nthe same sensor placed further",
    "start": "2380200",
    "end": "2386140"
  },
  {
    "text": "back, if we trace the ray from\nthe top and bottom most pixels, we'll see that it only covers\nthe mid section of the tree.",
    "start": "2386140",
    "end": "2393820"
  },
  {
    "text": "So that means our field\nof view got reduced. So that's the effect of\nchanging the focal length",
    "start": "2393820",
    "end": "2399940"
  },
  {
    "text": "as you increase. The focal length,\nthe FOV goes down. Or another way to think about\nit, it's like a zooming effect.",
    "start": "2399940",
    "end": "2407320"
  },
  {
    "text": "But the longer focal\nlength, just the centerpiece of the tree is going to\ncover your entire sensor.",
    "start": "2408220",
    "end": "2415359"
  },
  {
    "text": "So it's like you're\nzooming in, and that's why these lenses with\nvariable focal lengths",
    "start": "2415360",
    "end": "2421480"
  },
  {
    "text": "are called zoom lenses. Here's a picture I'm\nshowing, for instance,",
    "start": "2421480",
    "end": "2427360"
  },
  {
    "text": "for how this effect works. So here, we have a fixed scene.",
    "start": "2427360",
    "end": "2433060"
  },
  {
    "text": "So that's the scene\nthat's like a harbor,",
    "start": "2433060",
    "end": "2436600"
  },
  {
    "text": "with some buildings and\npeople and ships around it.",
    "start": "2438160",
    "end": "2443740"
  },
  {
    "text": "And the camera was placed at a\nfixed position in the harbor,",
    "start": "2443740",
    "end": "2449680"
  },
  {
    "text": "and the subject or the\nscene is also constant, and all we are changing\nis the focal length.",
    "start": "2449680",
    "end": "2454900"
  },
  {
    "text": "So a zoom lens was used, and\nthe focal length was changed. So first, we start with\na short focal length",
    "start": "2454900",
    "end": "2461260"
  },
  {
    "text": "of 17 millimeters, which gave\nus a very wide field of view. So as you see here,",
    "start": "2461260",
    "end": "2466300"
  },
  {
    "text": "basically, that gave us and then we start zooming in\nor increasing the focal length.",
    "start": "2468340",
    "end": "2478720"
  },
  {
    "text": "So as you go from 17\nmillimeters to 50 millimeters, now your field of\nview gets reduced,",
    "start": "2478720",
    "end": "2484420"
  },
  {
    "text": "and it only gets roughly-- if I can get it\nhere-- it only gets like this part of the scene.",
    "start": "2484420",
    "end": "2493359"
  },
  {
    "text": "So that's what you get from a Now if you go from 50\nmillimeters to 135 millimeters,",
    "start": "2493960",
    "end": "2502180"
  },
  {
    "text": "you further zoom\nin, and now you only get maybe something like\nthis part of the scene.",
    "start": "2502180",
    "end": "2508840"
  },
  {
    "text": "And that's what this picture is. So you're reducing\nthe field of view.",
    "start": "2509680",
    "end": "2515080"
  },
  {
    "text": "Sensor size is constant, so it\nfeels like you're zooming in.",
    "start": "2515080",
    "end": "2519040"
  },
  {
    "text": "And then if you go from so now we are at 18 degrees.",
    "start": "2520120",
    "end": "2526360"
  },
  {
    "text": "Now, If we go to 500 millimeters,\nwe further zoom in to something like--",
    "start": "2526900",
    "end": "2531880"
  },
  {
    "text": "just this, and\nthat's what we get. And that's just a 5\ndegree field of view.",
    "start": "2533800",
    "end": "2541000"
  },
  {
    "text": "So you see, as we went from 17\nmillimeters to 500 millimeters, we reduced our field of view\nby almost a factor of 20,",
    "start": "2541540",
    "end": "2552760"
  },
  {
    "text": "right It went 20x down. So it's a nonlinear effect\nbetween focal length and field",
    "start": "2552760",
    "end": "2560920"
  },
  {
    "text": "of view. And it also depends\non your sensor size.",
    "start": "2560920",
    "end": "2563560"
  },
  {
    "text": "This ratio, it can be\ndifferent for different lenses and different\nsensors, but that's",
    "start": "2566860",
    "end": "2573040"
  },
  {
    "text": "the high level effect of how\nchanging the focal length affects the final image.",
    "start": "2573040",
    "end": "2578800"
  },
  {
    "text": "It basically controls your\nfield of view or zoom factor.",
    "start": "2578800",
    "end": "2582400"
  },
  {
    "text": "So that's about the focal\nlength and the effect it has on the pictures.",
    "start": "2584800",
    "end": "2590920"
  },
  {
    "text": "The second important last\nparameter, as we said, is the F number, which\ncontrols the aperture size.",
    "start": "2592240",
    "end": "2598779"
  },
  {
    "text": "So the F number n of the\nlens with aperture diameter A",
    "start": "2598780",
    "end": "2604540"
  },
  {
    "text": "is given by this. So n is the focal length divided\nby the aperture diameter.",
    "start": "2604540",
    "end": "2610240"
  },
  {
    "text": "So the aperture\ndiameter, remember, it's typically\ncircular for lenses, and then A is the diameter.",
    "start": "2610240",
    "end": "2616780"
  },
  {
    "text": "So F over A is the F\nnumber of the lens. So for instance, F number that\nis F over 2 on a 50 millimeter",
    "start": "2618160",
    "end": "2628120"
  },
  {
    "text": "focal length lens means\nthat the aperture diameter is 25 millimeters.",
    "start": "2628120",
    "end": "2633280"
  },
  {
    "text": "Remember, one of\nthe factors that affects the total exposure\nof pixels on the image",
    "start": "2638320",
    "end": "2646600"
  },
  {
    "text": "is irradiance, or\nE. And irradiance is proportional to two factors.",
    "start": "2646600",
    "end": "2653260"
  },
  {
    "text": "One is, basically the total\narea of the aperture opening, so the total area of this circle.",
    "start": "2653260",
    "end": "2659260"
  },
  {
    "text": "That's one thing. So that's basically how much\nlight is coming through. And the area is proportional\nto the square of the diameter.",
    "start": "2660760",
    "end": "2668800"
  },
  {
    "text": "So again, this is aperture\narea, or proportional",
    "start": "2670420",
    "end": "2676359"
  },
  {
    "text": "to aperture area. And that's fixed irradiance. And the second thing\nthat affects irradiance",
    "start": "2676360",
    "end": "2682180"
  },
  {
    "text": "is it's proportional\nto the inverse square of the distance between the\nsensor and underlines aperture.",
    "start": "2682180",
    "end": "2691420"
  },
  {
    "text": "And the distance, as\nwe know, it varies. It depends on where\nwe are focusing at, but roughly speaking,\nit's about a focal length.",
    "start": "2692560",
    "end": "2699820"
  },
  {
    "text": "It's usually close to\nF. It's not exactly F, but it's close to F. So our total irradiance\nthen from these two",
    "start": "2699820",
    "end": "2709120"
  },
  {
    "text": "is proportional to A squared\ntimes 1 over F squared.",
    "start": "2709120",
    "end": "2714520"
  },
  {
    "text": "And then from this formula, it\nmeans it's proportional to 1 over n squared, or\nthe F number squared.",
    "start": "2715960",
    "end": "2722560"
  },
  {
    "text": "So that's how our F number\naffects the total irradiance, or the total light\nenergy per unit area",
    "start": "2722560",
    "end": "2729640"
  },
  {
    "text": "that hits it's the\nsensor within one frame. So increasing our F\nnumber by a factor of K",
    "start": "2729640",
    "end": "2737680"
  },
  {
    "text": "would reduce the\nirradiance by k squared. And that's an important\ntrade off to note.",
    "start": "2737680",
    "end": "2745599"
  },
  {
    "text": "Now, as we said, the aperture\ndiameter or aperture size A",
    "start": "2746500",
    "end": "2752020"
  },
  {
    "text": "affects the depth of\nfield in the image. And that's one thing we\nneed to study and quantify.",
    "start": "2752020",
    "end": "2758980"
  },
  {
    "text": "And exactly first define what we\nmean by depth of field and then how we quantify it and how it's\naffected by the aperture size.",
    "start": "2758980",
    "end": "2765220"
  },
  {
    "text": "So the way of the\nfield as is defined is this idea of using this\nidea of permissible circle",
    "start": "2766060",
    "end": "2775599"
  },
  {
    "text": "of confusion. So remember that we just argued\nthat for any given imaging",
    "start": "2775600",
    "end": "2781120"
  },
  {
    "text": "system, only points\nin just one object plane, which is at a\nvery specific distance",
    "start": "2781120",
    "end": "2787780"
  },
  {
    "text": "from the lens, will be\nperfectly focused in image. So for a given\nfocal length, given",
    "start": "2787780",
    "end": "2795640"
  },
  {
    "text": "distance between the\nsensor and the lens, only one object plane, one SO\nwould come to perfect focus.",
    "start": "2795640",
    "end": "2804220"
  },
  {
    "text": "And points in any other\nplane or any other distance from the lens will\nform spots and image.",
    "start": "2804820",
    "end": "2810340"
  },
  {
    "text": "Instead of getting\nfocused to points, they form spots on the image. But it turns out that, for\nall practical purposes,",
    "start": "2810340",
    "end": "2818440"
  },
  {
    "text": "if those spots are small enough,\nwe can consider them almost",
    "start": "2819460",
    "end": "2824619"
  },
  {
    "text": "in focus. And that's this idea\nof permissible circle of confusion, or C,\nwhich is the size",
    "start": "2824620",
    "end": "2830619"
  },
  {
    "text": "of the spot on the image\nthat is considered acceptably sharp or almost in focus.",
    "start": "2830620",
    "end": "2837880"
  },
  {
    "text": "And this C, there is no formula\nthat we can give and say,",
    "start": "2838960",
    "end": "2844180"
  },
  {
    "text": "oh, OK, so this is\nhow you calculate C. And if any spot is smaller\nthan this, it's in focus,",
    "start": "2844180",
    "end": "2850840"
  },
  {
    "text": "and larger than\nthis, it's blurred. It depends on a lot of factors. So it depends on\nthe sensing medium.",
    "start": "2850840",
    "end": "2857920"
  },
  {
    "text": "It depends on the reproduction\nmedium, like is it printed? Is it on a display? Is it on a projector?",
    "start": "2857920",
    "end": "2863020"
  },
  {
    "text": "Is it on a TV? It really matters. The viewing distance\nalso matters,",
    "start": "2863020",
    "end": "2868480"
  },
  {
    "text": "like how close are\nV depends on even the vision of the\nhuman who is looking",
    "start": "2868480",
    "end": "2878140"
  },
  {
    "text": "at it, how good their\nvision is and other factors. So it's quite\nsubjective at the end",
    "start": "2878140",
    "end": "2886960"
  },
  {
    "text": "of the day, what the permissible\ncircle of confusion is. But typically for\ndigital sensors,",
    "start": "2886960",
    "end": "2895540"
  },
  {
    "text": "C is taken to be the\nsize of one pixel, which, for different\nsensors is different,",
    "start": "2895540",
    "end": "2902260"
  },
  {
    "text": "but, for instance, for a DSLR\ncamera, it's about 6 microns. That's what is taken.",
    "start": "2902260",
    "end": "2907539"
  },
  {
    "text": "So if the spot is less\nthan its pixel size, again, because that's\nour unit of measurement,",
    "start": "2907540",
    "end": "2913720"
  },
  {
    "text": "we can't even measure anything. That's less than the pixel size. So we just call it OK. That's perfect focus,\nwhich makes sense.",
    "start": "2913720",
    "end": "2920260"
  },
  {
    "text": "Now, in other cases,\nit can be that C is taken to be more than a\npixel, but that's typical.",
    "start": "2920980",
    "end": "2929380"
  },
  {
    "text": "And this C can then be used to\ndetermine the depth of field called D. And the depth\nof field is basically",
    "start": "2929380",
    "end": "2937119"
  },
  {
    "text": "a range of distances\nfrom the lens that all subjects that fall\nwithin that range of distances",
    "start": "2937120",
    "end": "2945160"
  },
  {
    "text": "are reasonably in focus,\nor acceptably sharp. And that's what we call\nthe depth of field.",
    "start": "2945160",
    "end": "2951940"
  },
  {
    "text": "So here, this picture\nillustrates this concept",
    "start": "2951940",
    "end": "2958780"
  },
  {
    "text": "of permissible\ncircle of confusion. So if we started to look at the\nmiddle of the picture first,",
    "start": "2958780",
    "end": "2965080"
  },
  {
    "text": "so here is the\nobject plane at which the points come to a\nperfect focus on the sensor.",
    "start": "2965080",
    "end": "2971560"
  },
  {
    "text": "So this is our sensor plane,\nand that's our object plane that is perfectly image.",
    "start": "2971560",
    "end": "2978100"
  },
  {
    "text": "So then, this is-- if that's our--\n[CLEARS THROAT] excuse me-- lens plane.",
    "start": "2978100",
    "end": "2983680"
  },
  {
    "text": "This is our SO and\nthis is our SI. And that's perfect.",
    "start": "2983680",
    "end": "2989020"
  },
  {
    "text": "And you get a perfect\npoint, perfect focus on the image plane.",
    "start": "2989020",
    "end": "2994119"
  },
  {
    "text": "Now for objects that\nare closer or further away, as we argued, what\nthey form on the image plane",
    "start": "2994120",
    "end": "3003000"
  },
  {
    "text": "is a spot. And if these spots are smaller\nthan or equal in diameter",
    "start": "3003000",
    "end": "3009839"
  },
  {
    "text": "to C, which is the permissible\ncircle of confusion, we call them acceptably sharp.",
    "start": "3009840",
    "end": "3015119"
  },
  {
    "text": "Same for objects that\nare further away,",
    "start": "3016320",
    "end": "3020580"
  },
  {
    "text": "they're going to form spots. And if the spot is smaller\nthan or equal to C in size, we call it acceptably sharp.",
    "start": "3021480",
    "end": "3028200"
  },
  {
    "text": "So with that, that\nmeans we can then have a range of distances, which\nwe call D, or depth of field.",
    "start": "3028200",
    "end": "3035340"
  },
  {
    "text": "And of course, the\nobject plane is within that depth of field, the\nperfectly focused object plane,",
    "start": "3036600",
    "end": "3046080"
  },
  {
    "text": "but everything that's\nwithin D out in the field",
    "start": "3046080",
    "end": "3050760"
  },
  {
    "text": "is considered reasonably\nsharp and in focus in the final image. So how do we calculate D?",
    "start": "3052620",
    "end": "3059100"
  },
  {
    "text": "Well, it turns out that-- again,\njust like we did the Gaussian",
    "start": "3060180",
    "end": "3065940"
  },
  {
    "text": "imaging formula, with\npure geometric arguments, we can also quantify\nwhat the depth of field",
    "start": "3066780",
    "end": "3076020"
  },
  {
    "text": "is for a given permissible\ncircle of confusion. So here's the picture here.",
    "start": "3076020",
    "end": "3082320"
  },
  {
    "text": "And again, D is the\ndistance between the nearest and the furthest objects that\nare acceptably sharp or focused",
    "start": "3083460",
    "end": "3093599"
  },
  {
    "text": "in the image or\nthey form spots that are no bigger than C in size.",
    "start": "3093600",
    "end": "3099540"
  },
  {
    "text": "We are not going to go through\nthe full geometric analysis, but note that it's pure geometry\ndone here to derive what D is.",
    "start": "3101880",
    "end": "3110520"
  },
  {
    "text": "So this picture shows\nthat the-- so on the image",
    "start": "3110520",
    "end": "3115920"
  },
  {
    "text": "plane, this is the permissible\ncircle of confusion. That's the green is\nthe diameter of it.",
    "start": "3115920",
    "end": "3121680"
  },
  {
    "text": "And then you can\ntranslate that to what",
    "start": "3121680",
    "end": "3127020"
  },
  {
    "text": "is called the conjugate ",
    "start": "3127020",
    "end": "3128280"
  },
  {
    "text": "circle of confusion,",
    "start": "3134880",
    "end": "3136019"
  },
  {
    "text": "which will have a diameter\nthat is C divided by MT. And remember, MT is the\nlateral magnification factor.",
    "start": "3142920",
    "end": "3150000"
  },
  {
    "text": "And the lateral\nmagnification factor can be roughly\napproximated by U over f,",
    "start": "3150000",
    "end": "3157200"
  },
  {
    "text": "where U here, as you\nsee, is the distance from the lens to the perfectly\nfocused object plane.",
    "start": "3157200",
    "end": "3163680"
  },
  {
    "text": "That's U and f is\nthe focal length. So the conjugate\ncircle of confusion",
    "start": "3164340",
    "end": "3171359"
  },
  {
    "text": "has diameter C over f. And then this diamond shape\nbasically area, that is--",
    "start": "3171360",
    "end": "3179460"
  },
  {
    "text": "the distances of\nthe two ends of it from the lens, that's what\nour depth of field is.",
    "start": "3181440",
    "end": "3187080"
  },
  {
    "text": "So from our image plane, you\ncan come forward a distance D1,",
    "start": "3187080",
    "end": "3192240"
  },
  {
    "text": "and you can go\nback a distance D2, and you would remain in focus.",
    "start": "3192240",
    "end": "3198720"
  },
  {
    "text": "That's the idea. And notice that it's asymmetric. So the perfect\nobject plane is not",
    "start": "3198720",
    "end": "3208200"
  },
  {
    "text": "in the center of\nthe depth of field. That's very important. So it's like this diamond\nhas asymmetry in it.",
    "start": "3208200",
    "end": "3216299"
  },
  {
    "text": "Specifically after solving\nthe geometry, EI, we can show, is equal to this, which is\nN times C times U squared,",
    "start": "3216300",
    "end": "3223920"
  },
  {
    "text": "where N is the F number, C\nis the permissible circle of confusion, U,\nremember, is the distance",
    "start": "3223920",
    "end": "3230339"
  },
  {
    "text": "between the in-focus\nobject plane and the lens divided by\nf squared plus NCU over f",
    "start": "3230340",
    "end": "3235860"
  },
  {
    "text": "is the focal length. So that's DI. And D2 is given by\nthis other expression,",
    "start": "3235860",
    "end": "3242040"
  },
  {
    "text": "which is NCU squared divided\nby f squared minus NCU. So again, note the asymmetry\nin the depth of field.",
    "start": "3242040",
    "end": "3251160"
  },
  {
    "text": "So what is the total\ndepth of field? That's just D1 plus D2.",
    "start": "3251700",
    "end": "3256440"
  },
  {
    "text": "We add the two, and this\nis our final formula for the total depth of\nfield that we get for--",
    "start": "3257100",
    "end": "3264120"
  },
  {
    "text": "and it's a function\nof many parameters. You see the F\nnumber n goes in it.",
    "start": "3264120",
    "end": "3270420"
  },
  {
    "text": "Of course, the circle\nof confusion goes in it. The distance from the lens\nto the in-focus object plane",
    "start": "3271020",
    "end": "3278040"
  },
  {
    "text": "goes in it, and the\nfocal length goes in it. So all of these parameters\nat the end of the day can affect our depth of field.",
    "start": "3278040",
    "end": "3284819"
  },
  {
    "text": "Now, this formula this is the\nmost accurate version of it, but in many cases, it turns\nout that the conjugate",
    "start": "3284820",
    "end": "3293640"
  },
  {
    "text": "of the circle of confusion,\nwhich has the size C over MT, is a small relative to the lens\naperture A, which is f over n.",
    "start": "3293640",
    "end": "3301740"
  },
  {
    "text": "And in those cases,\nthis n squared C squared U squared term in\nthe denominator can be ignored.",
    "start": "3301740",
    "end": "3307920"
  },
  {
    "text": "And then we get a simpler\nexpression, which is this. So the depth of field is\napproximately equal to 2NCU",
    "start": "3307920",
    "end": "3317040"
  },
  {
    "text": "squared over f squared. So let's remember that\nthis one is approximate, but it's a very\ngood approximation",
    "start": "3317040",
    "end": "3322260"
  },
  {
    "text": "in many practical applications. But you should always check\nthat this condition is C over MT",
    "start": "3322800",
    "end": "3332100"
  },
  {
    "text": "smaller than A or not. If not, you should use\nthe more accurate version.",
    "start": "3333840",
    "end": "3339240"
  },
  {
    "text": "Now, with this, we\ncan see, as we said,",
    "start": "3340740",
    "end": "3346380"
  },
  {
    "text": "there's multiple factors\naffecting depth of field. Number one is n or the F number.",
    "start": "3346380",
    "end": "3353160"
  },
  {
    "text": "As we saw before, it does\naffect the depth of field, and it affects it linearly.",
    "start": "3354240",
    "end": "3359880"
  },
  {
    "text": "So it's linear and F number. It's quadratic in\nU, which is distance",
    "start": "3359880",
    "end": "3367200"
  },
  {
    "text": "from the lens to the\nin-focus object plane, and it's inverse quadratic\nin the focal length.",
    "start": "3367200",
    "end": "3375839"
  },
  {
    "text": "So if any of these parameters\nchanged in our imaging setup, it's going to affect\nthe depth of field.",
    "start": "3375840",
    "end": "3382620"
  },
  {
    "text": "Let's look at some examples. We start with F number. So as we said, depth of\nfield varies linearly",
    "start": "3382620",
    "end": "3390300"
  },
  {
    "text": "with aperture size, which is\ncontrolled by the F number. So here is two pictures\nof the same scene taken",
    "start": "3390300",
    "end": "3399060"
  },
  {
    "text": "with different aperture sizes. So first one, we\nwere doing n is f over 2, which is\na large aperture,",
    "start": "3399060",
    "end": "3404820"
  },
  {
    "text": "and then the second picture\nis taken with n is f over 16. So we reduce the aperture\nsize by a factor of 8.",
    "start": "3404820",
    "end": "3414900"
  },
  {
    "text": "So it went 8x down-- or 1 should really say 1 over 8.",
    "start": "3415440",
    "end": "3421560"
  },
  {
    "text": "That's how much we reduce the\naperture size by 1 over 8.",
    "start": "3422100",
    "end": "3425520"
  },
  {
    "text": "And we note that the\nfield is linear in n, so increasing n\nby a factor of 8,",
    "start": "3428400",
    "end": "3435300"
  },
  {
    "text": "so increased n by a\nfactor of 8, which means aperture went\ndown by a factor of 8, should increase our depth of\nfield by a factor of 8 roughly.",
    "start": "3436080",
    "end": "3444420"
  },
  {
    "text": "And that's exactly the case. So we kept everything\nelse the same. So U, which is the distance to\nthe plane of in-focus plane,",
    "start": "3444420",
    "end": "3454800"
  },
  {
    "text": "that was kept the same. So in both cases,\nwe were focusing on the face of this middle\nperson in the scene,",
    "start": "3455520",
    "end": "3462780"
  },
  {
    "text": "so to keep the U same,\nfocal length the same, everything was the same. And with the small\naperture, the depth of field",
    "start": "3462780",
    "end": "3470099"
  },
  {
    "text": "was just about 1 foot. So as you see, just the main\ntarget that we focused on is in focus, and then object in\nthe front and the third person",
    "start": "3470100",
    "end": "3478740"
  },
  {
    "text": "in the back, they're both\ncompletely out of focus. And then we increased\nn by a factor of 8,",
    "start": "3478740",
    "end": "3484860"
  },
  {
    "text": "or reduce their aperture\nsize by a factor of 8. And now we have a depth\nof field of 8 feet.",
    "start": "3484860",
    "end": "3491280"
  },
  {
    "text": "Again, U is the same, so\nit's important to just change one parameter here. So that's U, but now with\nthe depth of field of 8 feet,",
    "start": "3491940",
    "end": "3500340"
  },
  {
    "text": "all three subjects\nare reasonably sharp and in focus, the middle\nperson, the front person,",
    "start": "3500340",
    "end": "3507300"
  },
  {
    "text": "and the back person. The back person, maybe\nit's not that perfect, but it's still\nreasonably sharp compared",
    "start": "3507300",
    "end": "3513960"
  },
  {
    "text": "to what we had originally. So that's the effect of aperture\nsize and on depth of field.",
    "start": "3513960",
    "end": "3520140"
  },
  {
    "text": "The other two parameter\nthat affect, as you know, is the focal length\nand U, which is",
    "start": "3520140",
    "end": "3527820"
  },
  {
    "text": "distance from the in-focus\nplane to the optic. Let's look how those affected.",
    "start": "3527820",
    "end": "3532860"
  },
  {
    "text": "So depth of field varies\ninverse quadratically with focal length.",
    "start": "3532860",
    "end": "3537900"
  },
  {
    "text": "And that's illustrated in this\ntwo pictures in the top row. So here, again, it's the\nsame scene, same lighting",
    "start": "3537900",
    "end": "3544920"
  },
  {
    "text": "and everything. And we have only\nchanged the focal length",
    "start": "3544920",
    "end": "3551640"
  },
  {
    "text": "of the lens that was used. So this one is a long focal\nlength, 135 millimeters.",
    "start": "3551640",
    "end": "3557640"
  },
  {
    "text": "And this picture is taken\nwith a shorter focal length, And depth of field is inversely\nproportional to F squared.",
    "start": "3557640",
    "end": "3566460"
  },
  {
    "text": "So it's proportional to means the short\nfocal length should",
    "start": "3566460",
    "end": "3571500"
  },
  {
    "text": "give a much higher depth of\nfield, which is the case here. As you see, the front\nsubject and the back subject",
    "start": "3571500",
    "end": "3577500"
  },
  {
    "text": "are both in focus, versus\nwith the longer focal length, the main subject that we\nactually focused on, of course,",
    "start": "3577500",
    "end": "3584580"
  },
  {
    "text": "is in focus but\nthe back one, not in focus because the depth\nof field got reduced.",
    "start": "3584580",
    "end": "3590280"
  },
  {
    "text": "Then the second thing\nto look at is the effect of focusing distance. So depth of field\nvaries quadratically,",
    "start": "3591120",
    "end": "3600060"
  },
  {
    "text": "with focusing\ndistance U. And that's what is illustrated here. So again, everything\nis kept the same,",
    "start": "3600060",
    "end": "3606540"
  },
  {
    "text": "except U for this experiment. And first, we have a small U. So\nthe camera was just 3 feet away",
    "start": "3606540",
    "end": "3614940"
  },
  {
    "text": "from the front subject\nthat we focused on, and then we moved back.",
    "start": "3615780",
    "end": "3621480"
  },
  {
    "text": "Basically, the camera\nmoved back to 10 feet. And with that, we now get\na longer depth of field.",
    "start": "3621480",
    "end": "3630240"
  },
  {
    "text": "So both subjects are now in\nfocus versus in the first one,",
    "start": "3630240",
    "end": "3634680"
  },
  {
    "text": "the back subject\nwas out of focus. So hopefully with\nthis, it's clear",
    "start": "3635460",
    "end": "3642839"
  },
  {
    "text": "how all these trade-offs work\nwhen different parameters affect the depth of field.",
    "start": "3642840",
    "end": "3648540"
  },
  {
    "text": "So this concludes the\ndiscussion about lenses. So we covered the geometric\noptics, the image formation",
    "start": "3648540",
    "end": "3655860"
  },
  {
    "text": "through lenses. And also we specifically\ntalked about the focal length and the f-number of the lenses\nand how the f-number controls",
    "start": "3655860",
    "end": "3665400"
  },
  {
    "text": "the field of view of\nthe image, and the N along with other\nparameters mainly",
    "start": "3665400",
    "end": "3671760"
  },
  {
    "text": "controls the depth of field.",
    "start": "3671760",
    "end": "3673140"
  },
  {
    "text": "Well, as we saw, f also\ncontributes to depth of field.",
    "start": "3678480",
    "end": "3682740"
  },
  {
    "text": "So that's about lenses. Next topic we want\nto talk about is",
    "start": "3684180",
    "end": "3691860"
  },
  {
    "text": "digital image sensors\nand the physics of it and the electronics of\nit and how these image",
    "start": "3691860",
    "end": "3697500"
  },
  {
    "text": "sensors are built and operated. So there's different types\nof digital image sensors",
    "start": "3697500",
    "end": "3704579"
  },
  {
    "text": "and different architectures\nand different constructions. The two main ones are called\ncomplementary metal-oxide",
    "start": "3705840",
    "end": "3713220"
  },
  {
    "text": "semiconductor or CMOS sensors. So that's one\nsemiconductor process",
    "start": "3713220",
    "end": "3719400"
  },
  {
    "text": "and architecture that is used\nto build digital image sensors. And the second type is called\ncharge-coupled devices or CCDs.",
    "start": "3719400",
    "end": "3727920"
  },
  {
    "text": "These are both used. They're both deployed. And there is a number of\ntrade-offs between these two",
    "start": "3727920",
    "end": "3735120"
  },
  {
    "text": "architectures. So they have different\nnoise levels. So for instance, CCDs are less\nnoisy than CMOS, typically.",
    "start": "3735120",
    "end": "3741420"
  },
  {
    "text": "The speeds are different. CMOS are typically\nfaster than CCDs. The sensitivity\nlevels are different,",
    "start": "3741420",
    "end": "3748440"
  },
  {
    "text": "the cost is different,\nand other factors. We will only study\nCMOS sensors in detail",
    "start": "3748440",
    "end": "3754920"
  },
  {
    "text": "because those are the ones\nthat are mostly used and more cheaper and more robust\nimaging sensors that",
    "start": "3754920",
    "end": "3762960"
  },
  {
    "text": "are used in robotics. CMOS sensors are more\nflexible, they're cheaper,",
    "start": "3762960",
    "end": "3768000"
  },
  {
    "text": "and the performance\nis pretty good. It's maybe not as good as\nthe best CCDs out there, but they're very good sensors.",
    "start": "3768000",
    "end": "3774480"
  },
  {
    "text": "And here's one image\nof 120 megapixel CMOS sensor from Canon.",
    "start": "3774480",
    "end": "3780300"
  },
  {
    "text": "And that's what the\nthing looks like. It's pretty small. I mean, this is\na few centimeters",
    "start": "3780300",
    "end": "3786540"
  },
  {
    "text": "in diameter, basically,\nand diagonally, basically.",
    "start": "3786540",
    "end": "3791160"
  },
  {
    "text": "OK. So let's talk\nabout CMOS sensors.",
    "start": "3792840",
    "end": "3797099"
  },
  {
    "text": "The type of CMOS sensor\nwe'll talk about, it's called the active\npixel sensor or CMOS APS.",
    "start": "3799080",
    "end": "3807180"
  },
  {
    "text": "And a CMOS APS consists of an\narray of active photosensitive",
    "start": "3807180",
    "end": "3813300"
  },
  {
    "text": "pixels. So each of those\npixels and the sensor becomes one pixel in the\nfinal image, basically.",
    "start": "3813300",
    "end": "3819000"
  },
  {
    "text": "And each pixel is consists\nof three main components.",
    "start": "3819540",
    "end": "3824940"
  },
  {
    "text": "First is the\nphotosensitive component in the pixel, which is a\nreverse-biased PIN photodiode.",
    "start": "3825600",
    "end": "3834000"
  },
  {
    "text": "So that's the part\nof the pixel which converts incident\nlight or photons",
    "start": "3834000",
    "end": "3839340"
  },
  {
    "text": "to an electrical signal. And this is exactly\nthe same as LiDAR.",
    "start": "3839340",
    "end": "3844319"
  },
  {
    "text": "So in great detail, we talked\nabout PIN and photodiodes and why they are reverse-biased,\nhow they operate,",
    "start": "3844920",
    "end": "3852540"
  },
  {
    "text": "what's the responsivity,\nand all that. So this is exactly the same.",
    "start": "3852540",
    "end": "3856980"
  },
  {
    "text": "Different materials\nbecause, for instance, if you are making a\nvisible light camera,",
    "start": "3857700",
    "end": "3864660"
  },
  {
    "text": "we are not going to use\nan InGaAs PIN diode which is most responsive\nat 1,550 nanometers,",
    "start": "3864660",
    "end": "3870240"
  },
  {
    "text": "we will use silicon,\nbut same physics. So we're not going to talk more\nabout the principles of PIN",
    "start": "3870240",
    "end": "3877200"
  },
  {
    "text": "because it's exactly the same\nas what was covered in LiDAR. Then the other two components\nin every pixel of a CMOS APS",
    "start": "3877200",
    "end": "3886380"
  },
  {
    "text": "is the readout circuitry. And the readout\ncircuitry what it does is that it sends the signal,\nthe electrical signal that you",
    "start": "3886380",
    "end": "3893940"
  },
  {
    "text": "get out of the PD\nor the photodiode. And it sends it to the\nADCs to be digitized.",
    "start": "3893940",
    "end": "3899100"
  },
  {
    "text": "So that's the readout circuitry. And it needs to be\narchitected and designed such that you can read out every\npixel in this APS array.",
    "start": "3899100",
    "end": "3908640"
  },
  {
    "text": "And remember that modern\nsensors have many, many pixels. You can have millions of pixels. So it's important to have an\narchitecture that can nicely",
    "start": "3908640",
    "end": "3918060"
  },
  {
    "text": "read out all the pixels without\ncross-talk or congestion or anything like that.",
    "start": "3918060",
    "end": "3922740"
  },
  {
    "text": "And then there is\na reset circuitry. And the reset circuitry,\nessentially, it controls exposure\nduration per image frame.",
    "start": "3923280",
    "end": "3930480"
  },
  {
    "text": "Remember that especially\nif you're taking videos, you want to take\nconsecutive frames.",
    "start": "3930480",
    "end": "3935160"
  },
  {
    "text": "And each frame you\nwant to start clean. So you want to\nreset all the pixels",
    "start": "3936300",
    "end": "3941339"
  },
  {
    "text": "and start exposure, and then\nat the end of the exposure duration, read out\nthe final signal",
    "start": "3941340",
    "end": "3948180"
  },
  {
    "text": "and then reset them\nagain, and repeat. So the reset\ncircuitry makes sure that at the beginning of every\nexposure for every frame,",
    "start": "3948180",
    "end": "3956760"
  },
  {
    "text": "all the signals are reset\nto the same baseline and there is no effect from the\nprevious frame to the next one.",
    "start": "3956760",
    "end": "3966240"
  },
  {
    "text": "So that's the job of\nthe reset circuitry. Though there's different\nelectrical designs",
    "start": "3966240",
    "end": "3972180"
  },
  {
    "text": "to achieve this\narchitecture, basically, the simplest possible APS\nsensor uses just three MOSFET",
    "start": "3973320",
    "end": "3982680"
  },
  {
    "text": "transistors to build the\nreset and readout circuitry. And this is, as of\ntoday, this design,",
    "start": "3982680",
    "end": "3990720"
  },
  {
    "text": "which is called the 3T CMOS\nAPS, 3T43 transistor, and then a CMOS APS.",
    "start": "3990720",
    "end": "3996240"
  },
  {
    "text": "It's the most\ndeployed architecture",
    "start": "3996240",
    "end": "4004880"
  },
  {
    "text": "for these small chip cameras\nthat are used in cell phones and in robotics and\nother applications.",
    "start": "4004880",
    "end": "4011000"
  },
  {
    "text": "So it's simple, but it's\nextremely well-thought-out and practical and\ngood performing",
    "start": "4012020",
    "end": "4017960"
  },
  {
    "text": "and it's used a lot today. So it's not like, we'll\ntalk about something simple, but it's so simple\nthat it's not useful.",
    "start": "4017960",
    "end": "4025760"
  },
  {
    "text": "No. What we'll show you is\nactually you open up any camera, that's\nactually most of them",
    "start": "4025760",
    "end": "4032000"
  },
  {
    "text": "are using this architecture. OK. So we're going to talk in detail\nabout how each pixel works",
    "start": "4032000",
    "end": "4039980"
  },
  {
    "text": "in the actual circuit. But first, let's do\na quick refresher",
    "start": "4039980",
    "end": "4046460"
  },
  {
    "text": "about MOSFET transistors\nand how they operate. So MOSFET transistor, you're\ngoing to at super high level",
    "start": "4046460",
    "end": "4055760"
  },
  {
    "text": "just discover the\npart of it that is required for understanding\nthe CMOS APS 3T pixel.",
    "start": "4055760",
    "end": "4062720"
  },
  {
    "text": "But remember that\na MOSFET transistor is a three-terminal\ndevice, as shown here.",
    "start": "4063620",
    "end": "4070460"
  },
  {
    "text": "So it has a gate, shown\nG, that's one terminal, and then it has a drain, shown\nas D, and a source, shown as S.",
    "start": "4070460",
    "end": "4078320"
  },
  {
    "text": "And there's lots\nof different things you can do with\nthese transistors.",
    "start": "4078320",
    "end": "4085220"
  },
  {
    "text": "You can operate them\nin different regimes. There's linear regimes,\nthere's saturation regimes, or cell thresholds,\nand so on and so forth.",
    "start": "4085220",
    "end": "4092060"
  },
  {
    "text": "And you can build lots\nof different circuits for different\napplications with this.",
    "start": "4092060",
    "end": "4098299"
  },
  {
    "text": "But specifically for imaging\nin the 3T APS design,",
    "start": "4098300",
    "end": "4103940"
  },
  {
    "text": "these transistors are\nmainly used as switches. So they're either on\nor off, basically.",
    "start": "4105500",
    "end": "4112759"
  },
  {
    "text": "And the way the switch works,\nso the digital switch works, is that as shown in this picture\nwith the three terminals,",
    "start": "4113960",
    "end": "4123259"
  },
  {
    "text": "you can define\ntwo voltages-- one is the voltage between the\ngate and the source, VGS, and the other one is the\nvoltage between the drain",
    "start": "4123260",
    "end": "4130040"
  },
  {
    "text": "and the source, VDS. And typically there's\nalmost no current following through the gate.",
    "start": "4130040",
    "end": "4135319"
  },
  {
    "text": "So the Ig is almost 0 because\nthis is a very high impedance",
    "start": "4135320",
    "end": "4141980"
  },
  {
    "text": "terminal and that's why it's\nshown as a capacitor here. So there's almost no current or\nno DC current basically going",
    "start": "4141980",
    "end": "4150259"
  },
  {
    "text": "through. That's their high\nterminal input. And then the main current path\nis from drain to the source,",
    "start": "4150260",
    "end": "4157640"
  },
  {
    "text": "and that's this IDS. So when we say these can\nbe operated as switches,",
    "start": "4157640",
    "end": "4163699"
  },
  {
    "text": "there's switches for that IDS. So you can either\nhave IDS go through or you can turn off\nthe switch and then",
    "start": "4163700",
    "end": "4169819"
  },
  {
    "text": "there's no IDS, no\ndrain-source current. And the way it works,\nit's controlled mainly",
    "start": "4169820",
    "end": "4176060"
  },
  {
    "text": "by this VGS voltage. So if this VGS is less\nthan the threshold voltage of the device\nand the threshold voltage",
    "start": "4176720",
    "end": "4183620"
  },
  {
    "text": "is a voltage that is determined\nby the actual geometry and material construction\nof these transistors,",
    "start": "4183620",
    "end": "4190339"
  },
  {
    "text": "it's typically sub 1 volt. It\ncan be around mostly 0.7 volts. Some devices have\nlower V thresholds,",
    "start": "4190340",
    "end": "4197180"
  },
  {
    "text": "some have slightly higher, but\nthat's the ballpark number. So if the gate-source\nvoltage applied",
    "start": "4197180",
    "end": "4203119"
  },
  {
    "text": "is less than the\nthreshold, there's going to be no current\nflowing through the device and that's where\nthe switch is off.",
    "start": "4203120",
    "end": "4208700"
  },
  {
    "text": "And then if you have a VGS\nbigger than V threshold, then also the drain-source\nvoltage is bigger",
    "start": "4208700",
    "end": "4214760"
  },
  {
    "text": "than VGS minus V threshold,\nthat's where the switch is on. And then you get\nthe current flowing",
    "start": "4214760",
    "end": "4220940"
  },
  {
    "text": "through the\ndrain-source path, which is proportional to VGS\nminus V threshold squared.",
    "start": "4220940",
    "end": "4226940"
  },
  {
    "text": "And that's where\nthe switch is on. So that's how in\nthe 3T APS pixel,",
    "start": "4226940",
    "end": "4234980"
  },
  {
    "text": "that's mostly how the\ntransistors are operated. So we are going\nto control the VGS",
    "start": "4234980",
    "end": "4240140"
  },
  {
    "text": "and then turn the switches\nbasically on and off. And this plot here\nshows the IDS which",
    "start": "4240140",
    "end": "4246080"
  },
  {
    "text": "is the current as a function of\nVDS and VGS minus V threshold. So each curve is for a different\nvalue of VGS minus V threshold.",
    "start": "4246080",
    "end": "4255020"
  },
  {
    "text": "As you see, we have 1 volt\nhere, 1, 2, 4, and so on. And then as we said, we're going\nto operate these transistors",
    "start": "4255020",
    "end": "4262699"
  },
  {
    "text": "in the saturation\nregion, excuse me. So the saturation\nregion is the region to the right of this red curve.",
    "start": "4263240",
    "end": "4269660"
  },
  {
    "text": "So that's where basically\nthis condition is satisfied. That's for saturation.",
    "start": "4271280",
    "end": "4276500"
  },
  {
    "text": "And then as you see as\nwe increase the VGS minus",
    "start": "4276500",
    "end": "4281900"
  },
  {
    "text": "VTH for a fixed VDS,\nthe current value is going up quadratically.",
    "start": "4282500",
    "end": "4289760"
  },
  {
    "text": "If you just follow this,\nit goes up quadratically. This quadratic current\nthing doesn't really",
    "start": "4290720",
    "end": "4297080"
  },
  {
    "text": "matter for the\noperation of the 3T APS.",
    "start": "4297080",
    "end": "4302300"
  },
  {
    "text": "What matters is that we can\njust turn the switch on and off. You're going to just\napply either VGS",
    "start": "4302300",
    "end": "4308420"
  },
  {
    "text": "equals 0, which turns the switch\noff, or VGS some constant value",
    "start": "4308420",
    "end": "4316040"
  },
  {
    "text": "of VGS that is bigger\nthan V threshold to turn on the switch. So we operated in\na digital mode.",
    "start": "4316040",
    "end": "4321920"
  },
  {
    "text": "So that's why this\nexpression doesn't really matter much because it's going\nto be either no current going",
    "start": "4322640",
    "end": "4328880"
  },
  {
    "text": "through it or some\nconstant current basically going through it. OK. So that's the\nprinciple of operation",
    "start": "4328880",
    "end": "4334460"
  },
  {
    "text": "of the MOSFET transistor. Not the full\nprinciple, but the part that is relevant to\nthe 3T APS pixel.",
    "start": "4334460",
    "end": "4341360"
  },
  {
    "text": "And now let's look at the\nactual pixel architecture. So this is what is\nreally, really important.",
    "start": "4341360",
    "end": "4346580"
  },
  {
    "text": "So let's make sure every detail\nthat we talk about here it makes sense and it's well\nunderstood because this",
    "start": "4346580",
    "end": "4354440"
  },
  {
    "text": "is at the core of essentially\nhow these digital cameras work. So what we are looking at\nhere is just one pixel,",
    "start": "4354440",
    "end": "4363740"
  },
  {
    "text": "one image pixel in a\nCMOS APS sensor 3T pixel.",
    "start": "4363740",
    "end": "4369440"
  },
  {
    "text": "And as mentioned, so we\nhave our PIN photodiode.",
    "start": "4369440",
    "end": "4374540"
  },
  {
    "text": "So that's this guy here. And in parallel\nto it, in blue, we",
    "start": "4374540",
    "end": "4379700"
  },
  {
    "text": "are showing the parasitic\ncapacitance of the diode. So that's not an external\ncapacitor component.",
    "start": "4379700",
    "end": "4386060"
  },
  {
    "text": "That's why it's shown in blue. That's just a\nparasitic capacitance that every PIN photodiode has.",
    "start": "4386060",
    "end": "4391820"
  },
  {
    "text": "Here, we're showing there. And then there is\nthree transistors here,",
    "start": "4393560",
    "end": "4400040"
  },
  {
    "text": "one is for resetting. That's this M reset. The other one is for readout,\nit's called M select.",
    "start": "4400040",
    "end": "4407179"
  },
  {
    "text": "And this third one,\nMSF, it basically makes a buffer for\nreadout circuitry",
    "start": "4407180",
    "end": "4414200"
  },
  {
    "text": "that we'll talk more about. And then there's\ntwo control signals that you can apply\nto every pixel.",
    "start": "4414200",
    "end": "4419599"
  },
  {
    "text": "There's a reset control\nsignal and there is what is called a row select,\nbasically, a control signal.",
    "start": "4419600",
    "end": "4428120"
  },
  {
    "text": "And these are digital signals. So it's either 0 or 1. And that's how we\ncontrol the switches.",
    "start": "4428120",
    "end": "4434300"
  },
  {
    "text": "So when the control signal is is off, and then you apply 1\nand it turns on the switch,",
    "start": "4434300",
    "end": "4443780"
  },
  {
    "text": "basically. So digital control. And let's us start with\nat the start of a frame.",
    "start": "4443780",
    "end": "4451460"
  },
  {
    "text": "So let's say that's the time 0. And here on the right, we are\nplotting the control signals,",
    "start": "4451460",
    "end": "4456559"
  },
  {
    "text": "and we are looking\nat the V out, which is the voltage at the\noutput of the the pixel.",
    "start": "4456560",
    "end": "4465620"
  },
  {
    "text": "So that's the output\nof the PIN photodiode. So at the beginning\nof the frame,",
    "start": "4466460",
    "end": "4472520"
  },
  {
    "text": "we apply a reset\nsignal, a short pulse. So we briefly turn\non the switch.",
    "start": "4472520",
    "end": "4477560"
  },
  {
    "text": "And what happens when\nthe switch turns on, basically, this V reset\ngets applied here.",
    "start": "4477560",
    "end": "4484520"
  },
  {
    "text": "So current flows through\nand we basically charge up this parasitic capacitance\nto the voltage that",
    "start": "4485060",
    "end": "4493520"
  },
  {
    "text": "is set by this V reset voltage. So you close your reset switch,\nand you basically set your V",
    "start": "4493520",
    "end": "4500840"
  },
  {
    "text": "out to be equal to V reset. Then exposure starts.",
    "start": "4500840",
    "end": "4506239"
  },
  {
    "text": "And by exposure, we mean light\nis going to hit the photodiode and it's going to generate\nsome photocurrent.",
    "start": "4506240",
    "end": "4511880"
  },
  {
    "text": "And if you follow the\npath of this current, remember now the only way the\ncurrent can flow in the circuit",
    "start": "4512660",
    "end": "4520820"
  },
  {
    "text": "is by discharging\nthis capacitor going through the diode to the ground\nbecause this switch is closed,",
    "start": "4520820",
    "end": "4528020"
  },
  {
    "text": "so there is no path here. And also if you look at this\nbuffer MOSFET here, as we said,",
    "start": "4528020",
    "end": "4535040"
  },
  {
    "text": "the gate has a very\nhigh impedance input. So there's also no current\ngoing this way into that MOSFET,",
    "start": "4535040",
    "end": "4542660"
  },
  {
    "text": "so the current needs\nto basically discharge the capacitor.",
    "start": "4542660",
    "end": "4545900"
  },
  {
    "text": "And so what happens\nif you follow V out during the\nexposure, it's going",
    "start": "4549620",
    "end": "4555260"
  },
  {
    "text": "to start basically\ndischarging the capacitor until the exposure time is over.",
    "start": "4555260",
    "end": "4561500"
  },
  {
    "text": "And then at the end\nof the exposure time, we are going to apply a short\npulse to the row select.",
    "start": "4562280",
    "end": "4568820"
  },
  {
    "text": "So what row select\ndoes is note that it's going to the gate of\nour select transistors,",
    "start": "4570140",
    "end": "4576920"
  },
  {
    "text": "so it's going to turn on\nor close this switch here.",
    "start": "4576920",
    "end": "4580880"
  },
  {
    "text": "And I'm going to just\nclean up a little bit to explain what's\nhappening here.",
    "start": "4582140",
    "end": "4588260"
  },
  {
    "text": "So this transistor, this\nis forming a buffer for us.",
    "start": "4588260",
    "end": "4593119"
  },
  {
    "text": "And the specific\nconfiguration here is called a source follower,\nthat's why it's called SF.",
    "start": "4594080",
    "end": "4601880"
  },
  {
    "text": "And it's a voltage buffer. So what this does if I draw it\nagain here the source follower,",
    "start": "4602540",
    "end": "4610220"
  },
  {
    "text": "so that's VDD applied here,\nand then there's this V out. And here, it's going to\nthe other transistor.",
    "start": "4610220",
    "end": "4619040"
  },
  {
    "text": "But the way the\nsource follower works is that it's like\nan ideal buffer.",
    "start": "4619040",
    "end": "4625640"
  },
  {
    "text": "So it's an amplifier\nwith a voltage gain of 1. So the voltage at this\noutput at the source",
    "start": "4625640",
    "end": "4634040"
  },
  {
    "text": "is this VS is almost equal to\nthe voltage of the gate, which",
    "start": "4634040",
    "end": "4639620"
  },
  {
    "text": "in our case is V out. So basically it\nbuffers the voltage. But because this\ninput is very high",
    "start": "4639620",
    "end": "4646460"
  },
  {
    "text": "impedance it doesn't\ndraw current. And the reason we need\nthis is that without this if we directly connect our diode\nto some low impedance path,",
    "start": "4646460",
    "end": "4655159"
  },
  {
    "text": "we are going to just very\nquickly drain this capacitor. And we don't want to disturb it. We want the voltage\nof the capacitor",
    "start": "4655940",
    "end": "4663440"
  },
  {
    "text": "to be only affected by the\nphotocurrent and nothing else. And that's why we\nadd this buffer.",
    "start": "4663440",
    "end": "4668660"
  },
  {
    "text": "So the voltage here, at this\nnode, is also equal to V out, that's the effect of our buffer.",
    "start": "4668660",
    "end": "4675739"
  },
  {
    "text": "And that means when\nwe apply the pulse,",
    "start": "4675740",
    "end": "4680900"
  },
  {
    "text": "the control signal to\nthe row select and we close the select\nswitch on this wire",
    "start": "4683660",
    "end": "4691580"
  },
  {
    "text": "basically, which\nwe call V column, we are going to read the output\nvoltage of our photodiode.",
    "start": "4691580",
    "end": "4698420"
  },
  {
    "text": "So again, our V out is going\nto drop during the exposure time because of the\nphotocurrent discharging",
    "start": "4698420",
    "end": "4705560"
  },
  {
    "text": "the parasitic capacitor,\nand the V output as soon as we apply\nthe row select, this output voltage, which\nwe call the V column,",
    "start": "4705560",
    "end": "4713660"
  },
  {
    "text": "it's going to get\nset equal to the V out at the end of the\nexposure duration.",
    "start": "4715340",
    "end": "4721460"
  },
  {
    "text": "So V column is V\nout at time t, which is the end of the\nexposure, which",
    "start": "4721460",
    "end": "4727040"
  },
  {
    "text": "is equal to the initial value,\nthe reset value minus delta V. And delta V is how much\nwe discharged our junction",
    "start": "4727040",
    "end": "4735140"
  },
  {
    "text": "capacitance within\nthe exposure duration. After we read out, so we first\napply our row select and read",
    "start": "4735140",
    "end": "4742760"
  },
  {
    "text": "out the junction\nvoltage, and then we apply another reset\npulse to the reset input and set it back\nto reset and then",
    "start": "4742760",
    "end": "4750440"
  },
  {
    "text": "we expose again\nfor our next frame. So this is frame",
    "start": "4750440",
    "end": "4756200"
  },
  {
    "text": "is the next frame,\nwhich is frame 2. Now, specifically,\nwhat delta V is? Delta V again is the voltage\ndrop of the capacitor, the C,",
    "start": "4759620",
    "end": "4769639"
  },
  {
    "text": "because of the photocurrent. So it's equal to 1/C times the\nintegral of the photocurrent IP",
    "start": "4770660",
    "end": "4777320"
  },
  {
    "text": "within the exposure duration. So t going from 0 to big T. And\nassuming because these exposure",
    "start": "4777320",
    "end": "4784580"
  },
  {
    "text": "durations are very short,\nthese are like milliseconds, IP is not going to change.",
    "start": "4784580",
    "end": "4789980"
  },
  {
    "text": "It's like we can\nassume a fixed power is hitting your photodiode\ncausing or generating",
    "start": "4789980",
    "end": "4797780"
  },
  {
    "text": "a fixed photocurrent. So we just get IP. The integral becomes just t,\nso we get IP times t over C.",
    "start": "4797780",
    "end": "4804500"
  },
  {
    "text": "And remember from our\ndiscussion in LiDAR that the photocurrent is\nequal to the responsivity",
    "start": "4804500",
    "end": "4810500"
  },
  {
    "text": "of the diode times the\nincident optical power. So delta V basically\nbecomes responsivity",
    "start": "4810500",
    "end": "4816020"
  },
  {
    "text": "times the optical\npower that's hitting that pixel times the\nexposure duration divided",
    "start": "4816020",
    "end": "4821780"
  },
  {
    "text": "by the junction capacitance. The point is at the end of\nthe day if you plug this in, the readout voltage\nfrom this column line",
    "start": "4821780",
    "end": "4830060"
  },
  {
    "text": "is a linear function, is linear\nin the incident optical power.",
    "start": "4831560",
    "end": "4836900"
  },
  {
    "text": "So our image sensor each\npixel would actually measure something that\nis a linear function",
    "start": "4837560",
    "end": "4843440"
  },
  {
    "text": "of the optical power\nthat was incident on it. And that's a proper optical\nsensor, an image sensor,",
    "start": "4843440",
    "end": "4850940"
  },
  {
    "text": "every pixel measures how\nmuch light was hitting it.",
    "start": "4850940",
    "end": "4855440"
  },
  {
    "text": "OK. So here's a picture-- now you can scale this.",
    "start": "4856460",
    "end": "4862760"
  },
  {
    "text": "So we looked at just one pixel. This is the image we looked at. And we know how that works,\nthat's the 3T APS pixel.",
    "start": "4862760",
    "end": "4869420"
  },
  {
    "text": "And now that's the magic of\nsemiconductor manufacturing process that you can\nhave millions of these",
    "start": "4869420",
    "end": "4877340"
  },
  {
    "text": "and build a big, big array\nwith millions of pixels and cover a large area and\nthen use that to form images.",
    "start": "4877340",
    "end": "4886040"
  },
  {
    "text": "Here's a picture of\nan 8 by 8 pixel array. And the way the full\nimage sensor works",
    "start": "4886040",
    "end": "4892160"
  },
  {
    "text": "is that you see you have\nrows and columns of pixels,",
    "start": "4892160",
    "end": "4897320"
  },
  {
    "text": "so it's going to apply a row\nselect signal to rows one",
    "start": "4897320",
    "end": "4902599"
  },
  {
    "text": "by one. So that's basically this row\nselect signal that we had here.",
    "start": "4902600",
    "end": "4909260"
  },
  {
    "text": "So when a row select\nis applied, basically, the readout transistors\nor the select transistors",
    "start": "4909260",
    "end": "4917360"
  },
  {
    "text": "on all of the pixels in that\nrow are going to get turned on, and the corresponding\nvoltages for those pixels",
    "start": "4917360",
    "end": "4925640"
  },
  {
    "text": "are going to be available\non these column traces.",
    "start": "4925640",
    "end": "4931400"
  },
  {
    "text": "And all of that go\nthrough some multiplexer and demultiplexer and\nget that to the ADC.",
    "start": "4932720",
    "end": "4941179"
  },
  {
    "text": "So that's how you\ndigitize the voltages that are read from every pixel.",
    "start": "4941180",
    "end": "4947600"
  },
  {
    "text": "So you do one row and then\nit selects the next row and then it selects\nthe next row. So each row is selected\nand all the columns are--",
    "start": "4947600",
    "end": "4954920"
  },
  {
    "text": "basically, voltages are\nbeing fed to the ADC and you basically digitize\none row at a time.",
    "start": "4954920",
    "end": "4962840"
  },
  {
    "text": "And there's also of\ncourse the reset signals that go to all the pixels. So after you read all your\nrows and columns for one frame,",
    "start": "4962840",
    "end": "4971120"
  },
  {
    "text": "you reset everything\nand you do that again. So that's the basic operating\nprinciple of the CMOS APS.",
    "start": "4971120",
    "end": "4980000"
  },
  {
    "text": "Now, if you look at this\npixel structure here, you see there's of course the\nphotosensitive component, which",
    "start": "4980000",
    "end": "4987980"
  },
  {
    "text": "is the PIN photodiode. But you also have the three\ntransistors and the wires",
    "start": "4987980",
    "end": "4993079"
  },
  {
    "text": "and these take up some of\nyour semiconductor area. So not the entire area\nof your semiconductor",
    "start": "4993080",
    "end": "5001360"
  },
  {
    "text": "is going to be photosensitive.",
    "start": "5001360",
    "end": "5003520"
  },
  {
    "text": "So this is for\ninstance a picture showing if you look at just one\npixel you have your photodiode",
    "start": "5006940",
    "end": "5013300"
  },
  {
    "text": "there. But a lot of surface\narea is taken up by the transistors and the bus\ntraces for row select and reset",
    "start": "5013300",
    "end": "5023560"
  },
  {
    "text": "and other things. So because of that, if you\njust shine a light on that, a lot of the light\nis going to hit",
    "start": "5023560",
    "end": "5030220"
  },
  {
    "text": "these non-photosensitive\nparts of the device",
    "start": "5030220",
    "end": "5035380"
  },
  {
    "text": "and basically get wasted. A way to improve that\nis to add microlenses",
    "start": "5035380",
    "end": "5042700"
  },
  {
    "text": "on top of all the pixels. And what the microlens does is\nthat it focuses rays of light,",
    "start": "5042700",
    "end": "5048219"
  },
  {
    "text": "bends them, and focuses them\ninto the photosensitive area. So here's an electron\nmicroscope cross-section",
    "start": "5048220",
    "end": "5056920"
  },
  {
    "text": "image of one pixel. So you have the microlens\nat the top, that's",
    "start": "5056920",
    "end": "5062620"
  },
  {
    "text": "just a curved structure,\nand you see rays of light that come through. They all get bent\nand they get focused",
    "start": "5062620",
    "end": "5069580"
  },
  {
    "text": "into the photodiode area. Without the\nmicrolens, these rays would have just hit all the\ntransistors and other things",
    "start": "5069580",
    "end": "5076960"
  },
  {
    "text": "and that light would not\nbe sensed, basically. So the microlens array and\nhere you see an array of them,",
    "start": "5077560",
    "end": "5083980"
  },
  {
    "text": "there's these little\nbubbles, they're typically added on\ntop of the CMOS sensor",
    "start": "5083980",
    "end": "5091239"
  },
  {
    "text": "to improve the so-called feel\nfactor or the light collection efficiency of the sensor.",
    "start": "5091240",
    "end": "5098980"
  },
  {
    "text": "So that's one. The other thing we\nneed to talk about is that how do we\nactually sense color?",
    "start": "5099820",
    "end": "5105280"
  },
  {
    "text": "Because basically,\nthe architecture we have so far\ndescribed every pixel",
    "start": "5105280",
    "end": "5112960"
  },
  {
    "text": "would just measure the total\npower that's incident on it. And if the semiconductor PIN\nphotodiode like a silicon",
    "start": "5112960",
    "end": "5122560"
  },
  {
    "text": "PIN photodiode is\nsensitive to all",
    "start": "5122560",
    "end": "5127960"
  },
  {
    "text": "the visible light\nwavelengths that each pixel would measure essentially\nat an average of all colors.",
    "start": "5127960",
    "end": "5134320"
  },
  {
    "text": "So we cannot form a\ncolor image out of it, we can only form an intensity\nimage or like a black-and-white",
    "start": "5134320",
    "end": "5140800"
  },
  {
    "text": "type image out of it. So there's different ways\nthat you can sense color.",
    "start": "5140800",
    "end": "5145660"
  },
  {
    "text": "The most common one is by\nadding another layer, which",
    "start": "5146680",
    "end": "5152620"
  },
  {
    "text": "goes between the semiconductor\npixels and the microlenses.",
    "start": "5152620",
    "end": "5162880"
  },
  {
    "text": "So there's microlenses\non top and then there is this color\nfilter array that's added underneath the microlens.",
    "start": "5162880",
    "end": "5170200"
  },
  {
    "text": "So in this picture, you see\nyou have the microlens on top and you have the\ncolor filter and then",
    "start": "5170200",
    "end": "5175300"
  },
  {
    "text": "you have the actual\npixel at the bottom. And the color filter it has\nthe same pixel structure",
    "start": "5175300",
    "end": "5182680"
  },
  {
    "text": "as the sensor itself, but\neach cell in the color array",
    "start": "5182680",
    "end": "5187960"
  },
  {
    "text": "mostly just lets one\ncolor of light through. So you can have\nan RGB type color",
    "start": "5187960",
    "end": "5196300"
  },
  {
    "text": "filter or CFA,\ncolor filter array, and this is what happens.",
    "start": "5196300",
    "end": "5201460"
  },
  {
    "text": "So you get a full spectrum light\nhitting the color filter array.",
    "start": "5201460",
    "end": "5207520"
  },
  {
    "text": "But if it's the\nred pixel, it only lets red light through or the\ngreen only lets green through",
    "start": "5207520",
    "end": "5212560"
  },
  {
    "text": "or the blue only lets\nblue color through. So here is one example\nof the spectral response",
    "start": "5212560",
    "end": "5221560"
  },
  {
    "text": "of the three different colors\nin the color filter array. So as you see the blue is\ncentered around 450 nanometers,",
    "start": "5221560",
    "end": "5229719"
  },
  {
    "text": "the green is centered\naround 540 nanometers, and the red is centered\naround 650 nanometers.",
    "start": "5229720",
    "end": "5236560"
  },
  {
    "text": "So each of them predominantly\nlets one color through,",
    "start": "5236560",
    "end": "5242440"
  },
  {
    "text": "which means the pixel that is\nunderneath each of these colors",
    "start": "5242440",
    "end": "5247480"
  },
  {
    "text": "mostly senses one color. So then the pixels under the\nred CFA cells only sense red,",
    "start": "5247480",
    "end": "5257140"
  },
  {
    "text": "the pixels under the green\nCFA cells only sense green, and the pixels under blue\nCFA rays only sense blue.",
    "start": "5257140",
    "end": "5265480"
  },
  {
    "text": "This is one way of making your\nimage sensor color sensitive.",
    "start": "5265480",
    "end": "5271300"
  },
  {
    "text": "There's other less common\ntechnologies just so you know, but most of the cameras\ntoday use color filters.",
    "start": "5271300",
    "end": "5279940"
  },
  {
    "text": "Now there's also\ndifferent designs for the color\nfilters like what's the pattern of these RGB\ncells that you want to use",
    "start": "5279940",
    "end": "5287679"
  },
  {
    "text": "in the color filter array. And the most common one is\nthis called the Bayer filter.",
    "start": "5287680",
    "end": "5293200"
  },
  {
    "text": "And the way the Bayer\nfilter is designed, this is the pattern of the\nBayer filter shown here,",
    "start": "5293920",
    "end": "5299860"
  },
  {
    "text": "the immediate thing\nyou would notice is that it has twice as many--",
    "start": "5299860",
    "end": "5304840"
  },
  {
    "text": "if you look at this\nlittle 2 by 2 square here, it has two greens and\none red and one blue.",
    "start": "5306160",
    "end": "5311680"
  },
  {
    "text": "So it has twice as many\ngreen cells as red and blue. And there was a\ndesign choice that",
    "start": "5311680",
    "end": "5318580"
  },
  {
    "text": "was made as such that\nthis color filter array mimics the physiology\nof the human eye.",
    "start": "5318580",
    "end": "5325600"
  },
  {
    "text": "So the human eye,\nas it turns out, they'll be more sensitive to\ngreen color than red and blue.",
    "start": "5325600",
    "end": "5331119"
  },
  {
    "text": "And then that's why this\nspecific CFA was designed to have a response very\nsimilar to human eyes",
    "start": "5331120",
    "end": "5337960"
  },
  {
    "text": "so that the color\npictures you get out of it look natural to the human eye.",
    "start": "5337960",
    "end": "5343300"
  },
  {
    "text": "So with this, what we\nget is that let's say this is the original scene\nthat you're trying to image,",
    "start": "5343300",
    "end": "5349000"
  },
  {
    "text": "and then let's say it was\na 120 by 80 pixel sensor,",
    "start": "5349000",
    "end": "5353980"
  },
  {
    "text": "and the actual output\nwith the Bayer filter it looks like a mosaic.",
    "start": "5354700",
    "end": "5360520"
  },
  {
    "text": "So if I zoom in here, before\nany processing, every pixel",
    "start": "5361060",
    "end": "5367540"
  },
  {
    "text": "at the output if\nyou color-code them, every pixel would either be just\nsensing red or green or blue.",
    "start": "5368680",
    "end": "5375280"
  },
  {
    "text": "So it looks like-- you see, again, if you\nzoom in even further, every pixel is just\nsensing the light intensity",
    "start": "5375280",
    "end": "5383020"
  },
  {
    "text": "at one specific color. So it looks like\nthis weird mosaic instead of a natural image\nyou would expect like that.",
    "start": "5383020",
    "end": "5391420"
  },
  {
    "text": "And that's something that\nis taken care of in DSP.",
    "start": "5391420",
    "end": "5398320"
  },
  {
    "text": "So there is some interpolation\nthat we need to do. And basically for every pixel\nby looking at the neighboring",
    "start": "5398320",
    "end": "5406420"
  },
  {
    "text": "ones, for every pixel, we need\nan R and a G and a V value",
    "start": "5406420",
    "end": "5411760"
  },
  {
    "text": "to get the full\ncolor of that pixel. But out of the\nsensor, every pixel",
    "start": "5411760",
    "end": "5418960"
  },
  {
    "text": "only gives you one of the\nthree naturally measures. So the other two colors\nneed to be interpolated",
    "start": "5418960",
    "end": "5424360"
  },
  {
    "text": "from neighboring pixels. And that's the job of the\ndemosaicing algorithms that run in DSP.",
    "start": "5424360",
    "end": "5430120"
  },
  {
    "text": "So the images that we get\nout of the cameras never look like this mosaic here. There's always some\ninterpolation applied,",
    "start": "5431140",
    "end": "5437080"
  },
  {
    "text": "and then we get a more\nnaturally looking image where every pixel\nis interpolated to have R, G, and B instead\nof just one of the three.",
    "start": "5437080",
    "end": "5445660"
  },
  {
    "text": "We'll talk more about\ndemosaicing just so you know. OK. So this kind of completes\nthe discussion of how",
    "start": "5445660",
    "end": "5453280"
  },
  {
    "text": "the digital image sensor\nworks or the specific one we talked about, which\nwas the 3T CMOS APS,",
    "start": "5453280",
    "end": "5460719"
  },
  {
    "text": "which is the most common type\nof image sensor used today.",
    "start": "5460720",
    "end": "5464980"
  },
  {
    "text": "And now we are ready-- so we talked about the physics\nand the electronics, now we can look at the full system\narchitecture of a vision",
    "start": "5466240",
    "end": "5475420"
  },
  {
    "text": "sensor. So this is the full\narchitecture diagram. So as we discussed there's\nalways some imaging optics",
    "start": "5475420",
    "end": "5484480"
  },
  {
    "text": "or lenses that we talked\nabout, and then the image comes to focus on the\nimage sensor plane.",
    "start": "5484480",
    "end": "5491920"
  },
  {
    "text": "And we talked about how\nthe image sensor works and the different\ncomponents that go into it.",
    "start": "5491920",
    "end": "5499420"
  },
  {
    "text": "And then the readouts\nof all the pixels get digitized through the ADC.",
    "start": "5499420",
    "end": "5505180"
  },
  {
    "text": "And then there is a number\nof signal processing steps that needs to happen\nbefore a final image is",
    "start": "5505180",
    "end": "5515380"
  },
  {
    "text": "produced that looks\nnatural and expected. And that's the job of this\nimage signal processor",
    "start": "5515380",
    "end": "5522880"
  },
  {
    "text": "or ISP module inside cameras. And usually, the cameras have\na hardware module, a chip,",
    "start": "5522880",
    "end": "5530560"
  },
  {
    "text": "that applies different\nsignal processing steps to the digitized\nreadouts of the sensor.",
    "start": "5530560",
    "end": "5538659"
  },
  {
    "text": "And the job of that is to\nbasically convert this RGB mosaic that you get\nout of the sensor",
    "start": "5538660",
    "end": "5545559"
  },
  {
    "text": "to a natural-looking image. Specifically, there's\nmultiple steps that happen inside the ISP.",
    "start": "5545560",
    "end": "5552219"
  },
  {
    "text": "There is of course\nthe demosaicing or the color interpolation\nthat we call and talked about. And there is white\nbalancing and tone mapping.",
    "start": "5552220",
    "end": "5559300"
  },
  {
    "text": "There's some denoising\nand image sharpening that typically\nhappens, and also it",
    "start": "5560380",
    "end": "5565840"
  },
  {
    "text": "can compress the image\nto require less storage. And after all of\nthat, the final image",
    "start": "5565840",
    "end": "5571840"
  },
  {
    "text": "is produced that can be\nstored on some storage medium. Now, note that when\nit comes to the ISP",
    "start": "5571840",
    "end": "5579099"
  },
  {
    "text": "every camera uses different\nalgorithms in their ISP. And most of these are\nproprietary to the camera",
    "start": "5579100",
    "end": "5585400"
  },
  {
    "text": "manufacturer. And we don't know\nexactly what is running",
    "start": "5585400",
    "end": "5590440"
  },
  {
    "text": "on your iPhone or your\nAndroid phone or your Canon or your Nikon phone or your\nSony phone or whatever.",
    "start": "5590440",
    "end": "5597940"
  },
  {
    "text": "They are all different. Generally, they do most of these\nfunctions that are shown here,",
    "start": "5597940",
    "end": "5606100"
  },
  {
    "text": "but they can do them\nin different orders. And the specific\nalgorithms they use",
    "start": "5606100",
    "end": "5612160"
  },
  {
    "text": "to achieve each of these\nfunctions shown here are proprietary\nand they vary a lot",
    "start": "5612160",
    "end": "5618219"
  },
  {
    "text": "from different manufacturers. So we're going to high\nlevel and describe at the functional level\nwhat each of these blocks",
    "start": "5618220",
    "end": "5626860"
  },
  {
    "text": "try to achieve. And we also talk about some\nvery simple algorithms that can achieve those functions.",
    "start": "5627580",
    "end": "5633639"
  },
  {
    "text": "But note again that we are not\ntalking about state-of-the-art",
    "start": "5633640",
    "end": "5638800"
  },
  {
    "text": "best kind of algorithms\nthat one could do here, we're just barely\nscratching the surface.",
    "start": "5638800",
    "end": "5645159"
  },
  {
    "text": "But nevertheless, I think it's\nvaluable to understand what each of these terms mean,\nwhat's the goal of them,",
    "start": "5646960",
    "end": "5654880"
  },
  {
    "text": "and know about at least the\nsimplest type of algorithm that can achieve that.",
    "start": "5655720",
    "end": "5659740"
  },
  {
    "text": "But if you want\nto work on an ISP, I do not recommend implementing\nthe algorithms that",
    "start": "5660760",
    "end": "5669160"
  },
  {
    "text": "are discussed here because\nthese are very simple and they typically don't\nwork super well compared",
    "start": "5669160",
    "end": "5676780"
  },
  {
    "text": "to the state of the art you\nshould come up with or learn",
    "start": "5676780",
    "end": "5681219"
  },
  {
    "text": "much, much better-sophisticated\nalgorithms to achieve these. So we are going to start\nwith color interpolation",
    "start": "5681880",
    "end": "5689080"
  },
  {
    "text": "or demosaicing. And again as we said,\nthe natural signal",
    "start": "5689080",
    "end": "5697720"
  },
  {
    "text": "that you get out of\nyour image sensor, the ones that have\nthese color filters",
    "start": "5697720",
    "end": "5702760"
  },
  {
    "text": "is this RGB mosaic where every\npixel has just one color. And then the job of demosaicing\nis that interpolates the color",
    "start": "5702760",
    "end": "5710920"
  },
  {
    "text": "so that we get R and G and\nB value for every pixel and we get an image that\nlooks much more natural.",
    "start": "5710920",
    "end": "5717520"
  },
  {
    "text": "This image looks\npixelated that's just because the\nresolution was low.",
    "start": "5717520",
    "end": "5723460"
  },
  {
    "text": "It's just 120 by 80 pixels. So it's not because of\ndemosaicing that it looks",
    "start": "5723460",
    "end": "5728980"
  },
  {
    "text": "pixelated, it's just not that. It was a low-resolution image. Now the simplest algorithm\nthat you can do for demosaicing",
    "start": "5728980",
    "end": "5736780"
  },
  {
    "text": "is just a linear interpolation. And the way it works is\nthat for every pixel,",
    "start": "5736780",
    "end": "5741159"
  },
  {
    "text": "you get one of the three\ncolors directly measured, depending on what the color\nfilter on top of it was.",
    "start": "5742000",
    "end": "5749320"
  },
  {
    "text": "So you get either R or\nG or B already measured. So that's fine. And then for the\nother two colors,",
    "start": "5749320",
    "end": "5755140"
  },
  {
    "text": "you just linearly\ninterpolate the values of the three closest or four\nor two closest neighbors",
    "start": "5755140",
    "end": "5762220"
  },
  {
    "text": "to do that. So for this one highlighted\nhere to get the R, you average the four pixels\nat the corners of it,",
    "start": "5762220",
    "end": "5769420"
  },
  {
    "text": "the R values of those,\nand that gives you the R for the middle pixel. The G is again, that's directly\nmeasured for that same pixel,",
    "start": "5769420",
    "end": "5777520"
  },
  {
    "text": "so no interpolation needed. And for blue, you get two\nnearest neighbor pixels",
    "start": "5777520",
    "end": "5783340"
  },
  {
    "text": "so it can average\nthese two pixels, and that gives you\nthe blue for that.",
    "start": "5783340",
    "end": "5788140"
  },
  {
    "text": "And that's how you can do your\nlinear color interpolation.",
    "start": "5788920",
    "end": "5794080"
  },
  {
    "text": "Note that with this algorithm\nor with any interpolation algorithm actually,\nbecause every pixel only",
    "start": "5795460",
    "end": "5801580"
  },
  {
    "text": "measures one color\nphysically, 2/3 of the information\nin the final image",
    "start": "5801580",
    "end": "5808000"
  },
  {
    "text": "is actually made\nup-- and by made up, I mean, it's interpolated. So it's not physically accurate.",
    "start": "5808000",
    "end": "5813219"
  },
  {
    "text": "But nevertheless, because\ncolor across pixels usually doesn't change\nthat rapidly usually,",
    "start": "5813220",
    "end": "5820840"
  },
  {
    "text": "in some cases it\ndoes, your final image can look very\nrealistic and good.",
    "start": "5820840",
    "end": "5826840"
  },
  {
    "text": "Now with this simple\nlinear color interpolation, in certain cases, it\ncan introduce artifacts",
    "start": "5827500",
    "end": "5835780"
  },
  {
    "text": "in the image,\nspecifically if there are some structures\nin the image that",
    "start": "5835780",
    "end": "5844240"
  },
  {
    "text": "can cause color fringes or\ncolor moire in the final image. Notorious structures\nare for instance",
    "start": "5844240",
    "end": "5852100"
  },
  {
    "text": "if you have black and\nwhite detail in your image. And here is one. If I zoom in, here's an\nimage like a fabric structure",
    "start": "5852100",
    "end": "5860739"
  },
  {
    "text": "where it had this fine black\nand white fabric lines woven",
    "start": "5860740",
    "end": "5865780"
  },
  {
    "text": "together in the image. And then the linear\ninterpolation causes these color fringes that\nyou can see across the image.",
    "start": "5865780",
    "end": "5874420"
  },
  {
    "text": "And that's a failure of the\ndemosaicing algorithm basically",
    "start": "5874420",
    "end": "5880179"
  },
  {
    "text": "that has happened in this case. What can you do? Well, there is more\nsophisticated algorithms",
    "start": "5880180",
    "end": "5886360"
  },
  {
    "text": "of course you can use. One way is you can\nfix that by applying a median filter on the colors.",
    "start": "5886360",
    "end": "5892600"
  },
  {
    "text": "So after interpolation, you\nlook at your color histograms and apply that median filter. And with that, you can\nremove outlier color changes.",
    "start": "5892600",
    "end": "5902500"
  },
  {
    "text": "And here's one example, same\npicture, two different ways of doing demosaicing.",
    "start": "5902500",
    "end": "5907960"
  },
  {
    "text": "The left one I'm going to zoom\nin a little more on it, that's what linear interpolation. And if you look\nat where there is",
    "start": "5907960",
    "end": "5914620"
  },
  {
    "text": "a fine black and white structure\nlike this electrical wireline here or the branches\nof the tree,",
    "start": "5914620",
    "end": "5919720"
  },
  {
    "text": "you would see significant\ncolor fringes. Also, on the windows and other\nplaces, you would notice that.",
    "start": "5919720",
    "end": "5926980"
  },
  {
    "text": "And then if you apply\nthe median filter, so if you do the\nmedian-filtered interpolation,",
    "start": "5926980",
    "end": "5932380"
  },
  {
    "text": "see how much it improves. It's still not perfect,\nperfect, but it's significantly improved with\nrespect to fringes and more.",
    "start": "5932380",
    "end": "5940900"
  },
  {
    "text": "So that's about demosaicing. The other thing that\nhappens in the ISP",
    "start": "5942340",
    "end": "5948280"
  },
  {
    "text": "typically is white balancing. And white balancing basically\nwhat it tries to do, it tries to ensure that the\nwhite color is represented",
    "start": "5948280",
    "end": "5956619"
  },
  {
    "text": "faithfully in the image. And why is that necessary? It's because different\nlighting conditions can change",
    "start": "5956620",
    "end": "5965320"
  },
  {
    "text": "the appearance of colors. And this is something\nthat our brains",
    "start": "5965320",
    "end": "5970599"
  },
  {
    "text": "have been evolved to\njust take care of so well that we really don't\nnotice it in our vision.",
    "start": "5970600",
    "end": "5977320"
  },
  {
    "text": "So for instance, if you\nhave a white piece of paper and you take it under a very\nwarm light source like sunlight",
    "start": "5977320",
    "end": "5985180"
  },
  {
    "text": "or you take it under\na very cold light source like fluorescent\nlight, the actual color that's",
    "start": "5985180",
    "end": "5991179"
  },
  {
    "text": "coming out of that paper or\nreflected off of that paper, it's going to be affected by\nthe color of the light that",
    "start": "5991180",
    "end": "5998980"
  },
  {
    "text": "was incident on\nit and it's going to look very different under\nthe two lighting conditions. But our brains just\nadjust for that",
    "start": "5998980",
    "end": "6006660"
  },
  {
    "text": "and balance it out such that\nwhite always looks white to us. Almost always, except if there's\nsome very, very weird lighting",
    "start": "6006660",
    "end": "6014699"
  },
  {
    "text": "condition like if you're in a\nclub with very unnatural color",
    "start": "6014700",
    "end": "6022020"
  },
  {
    "text": "lights, then yes,\ncolors look different. But otherwise, in normal\nlighting conditions,",
    "start": "6022020",
    "end": "6029340"
  },
  {
    "text": "white always looks white to us. With cameras, this\nneeds to be handled in DSP, which means the camera\nneeds to somehow guess or find",
    "start": "6029340",
    "end": "6041699"
  },
  {
    "text": "out what part of the scene\nor what object in that image was white in real world.",
    "start": "6041700",
    "end": "6047820"
  },
  {
    "text": "And it needs to find\nthe RGB, the sense RGB value for that object. And then it needs to scale\nall the colors, all the RGBs",
    "start": "6049080",
    "end": "6058500"
  },
  {
    "text": "in the image proportionally,\neither up or down, such that the chosen\nRGB of the white object",
    "start": "6058500",
    "end": "6067020"
  },
  {
    "text": "becomes 111 because RGB equals",
    "start": "6067020",
    "end": "6069360"
  },
  {
    "text": "So that's the job of\nthe white balancing. It's like find the\nwhite thing in the image and find the RGB value of\nthat and scale that RGB to 111",
    "start": "6072900",
    "end": "6082199"
  },
  {
    "text": "and scale all the other\npixels accordingly so all the colors get adjusted.",
    "start": "6082200",
    "end": "6088740"
  },
  {
    "text": "Here's one example. So it's a picture of a\nflower that was taken. And for this flower, the\npetals were actually white",
    "start": "6088740",
    "end": "6097980"
  },
  {
    "text": "or part of them was white. And if you just-- this is the raw image. If you look at the raw image\nor a pre-processed image",
    "start": "6097980",
    "end": "6105180"
  },
  {
    "text": "and you look at the actual\ncolor that the sensor sensed on the flower, the RGB value\nwas actually 0.7, 2.8, 0.85.",
    "start": "6105180",
    "end": "6114600"
  },
  {
    "text": "And this is the\nactual color here. It's actually gray. So white looked grayish in\nthat image, in the raw image.",
    "start": "6114600",
    "end": "6121440"
  },
  {
    "text": "And then after applying\nwhite balancing we basically scaled\nthat to 111 and scale",
    "start": "6121440",
    "end": "6126659"
  },
  {
    "text": "all the rest of the RGB values\nby the same scale factors. And this is the\nfinal image that you",
    "start": "6126660",
    "end": "6132840"
  },
  {
    "text": "get where the flower\nactually looks white as it would be in the real\nworld and all the colors look",
    "start": "6132840",
    "end": "6138060"
  },
  {
    "text": "a little warmer as a result of\nthe white balancing applied. Now modern cameras try\nto do this automatically.",
    "start": "6138060",
    "end": "6146760"
  },
  {
    "text": "So there's some automatic\nwhite balancing or AWB that is applied in cameras.",
    "start": "6146760",
    "end": "6152940"
  },
  {
    "text": "And it's a very non-trivial\ntask as you can imagine, because the camera needs\nto somehow find or guess",
    "start": "6152940",
    "end": "6159780"
  },
  {
    "text": "what that scale factor\nis, which means either it needs to find\nsomething that's white",
    "start": "6160440",
    "end": "6165540"
  },
  {
    "text": "and that's a very\nnon-trivial task in DSP, or otherwise, have\nsome heuristics.",
    "start": "6165540",
    "end": "6172320"
  },
  {
    "text": "There's different algorithms. There's many of\nthem, but here are",
    "start": "6173280",
    "end": "6180060"
  },
  {
    "text": "two that are\nsometimes used, one is the gray world of assumption. And this algorithm\nbasically assumes",
    "start": "6180060",
    "end": "6187260"
  },
  {
    "text": "that if you average the color\nof all the pixels in an image,",
    "start": "6187260",
    "end": "6191699"
  },
  {
    "text": "it becomes gray. So that's the gray world,\nbasically, approximation. So the algorithm forces\nthe average color",
    "start": "6193980",
    "end": "6200820"
  },
  {
    "text": "of the scene to become gray. So it finds the scaling\nfactor that after that is applied the average of\nall the pixels becomes gray",
    "start": "6200820",
    "end": "6208080"
  },
  {
    "text": "and that is used\nfor white balancing. The other algorithm looks\nat the brightest pixel.",
    "start": "6208080",
    "end": "6215640"
  },
  {
    "text": "So it assumes that\nthe brightest pixel you have in the image the\ncolor of that is white.",
    "start": "6216360",
    "end": "6221520"
  },
  {
    "text": "And then it forces that\nto basically become white if it's not already and then\napplies the same scaling",
    "start": "6222420",
    "end": "6229440"
  },
  {
    "text": "to all the pixels, but\nneither of these algorithms are perfect.",
    "start": "6229440",
    "end": "6233760"
  },
  {
    "text": "They can fail in\ndifferent scenarios. So for instance,\nthe gray world fails",
    "start": "6235080",
    "end": "6240900"
  },
  {
    "text": "when the lighting is on the\nextreme side of warm or cold.",
    "start": "6240900",
    "end": "6250199"
  },
  {
    "text": "So if it's very warm\nlike a sunset lighting or if it's very cold\nlike fluorescent,",
    "start": "6250200",
    "end": "6255060"
  },
  {
    "text": "gray world approximation\ncan really fail. And then the brightest\npixel approximation,",
    "start": "6255780",
    "end": "6261719"
  },
  {
    "text": "it can fail when your brightest\npixel is actually saturated. It was so bright\nthat it completely saturated the sensor.",
    "start": "6261720",
    "end": "6268560"
  },
  {
    "text": "So the value you're\nreading is clipped. Basically, it's\nnot the real value. Or if it's coming from\nsome metallic surface that",
    "start": "6268560",
    "end": "6275760"
  },
  {
    "text": "wasn't actually white, then\nthat assumption breaks down. Here's an example where the\ngray world approximation failed.",
    "start": "6275760",
    "end": "6282659"
  },
  {
    "text": "So this is a picture that\nwas taken during sunset with very warm lighting,\nas you can see,",
    "start": "6282660",
    "end": "6289620"
  },
  {
    "text": "so this is the raw image. And then with the gray\nworld out of white balancing",
    "start": "6289620",
    "end": "6295320"
  },
  {
    "text": "after that that is applied,\nyou get this picture which completely washed out colors.",
    "start": "6295320",
    "end": "6300360"
  },
  {
    "text": "You can see this is not the\nright white balancing applied. So just so you know these\nalgorithms they are deployed,",
    "start": "6300360",
    "end": "6307980"
  },
  {
    "text": "especially the gray\nworld still is deployed, but it's not perfect.",
    "start": "6307980",
    "end": "6314100"
  },
  {
    "text": "And this is one of those\ncases because the task is such a nontrivial subjective task.",
    "start": "6314100",
    "end": "6319920"
  },
  {
    "text": "Actually, modern machine\nlearning approaches can do really, really\nwell with white balancing",
    "start": "6321120",
    "end": "6326400"
  },
  {
    "text": "way better than what these\nclassical algorithms can do. And that's what\nmost of modern cell",
    "start": "6327120",
    "end": "6332400"
  },
  {
    "text": "phones for white balancing,\nthey use machine learning because you just show them\nenough data with ground truth",
    "start": "6332400",
    "end": "6341340"
  },
  {
    "text": "white balance. And then the network learns how\nto do it like the human brain has learned to do it and they\ncan do really, really well.",
    "start": "6341340",
    "end": "6349260"
  },
  {
    "text": "Especially in challenging\nlighting conditions, machine learning can do really\nwell with white balancing.",
    "start": "6349260",
    "end": "6354960"
  },
  {
    "text": "OK. Next thing that happens in the\nISP typically is tone mapping. And tone mapping is\ngenerally speaking,",
    "start": "6355620",
    "end": "6364140"
  },
  {
    "text": "it does some\nadjustments to colors. And that's on top\nof white balance",
    "start": "6364140",
    "end": "6369780"
  },
  {
    "text": "because after white\nbalancing is done, there could be some additional\nadjustments to colors. And tone mapping can be\ndone for various purposes.",
    "start": "6369780",
    "end": "6377579"
  },
  {
    "text": "Sometimes it's just done for\npurely aesthetic purposes just for the image\nto look better.",
    "start": "6378300",
    "end": "6383640"
  },
  {
    "text": "And that's for instance\nif you've used Instagram, there's always\nInstagram filters.",
    "start": "6383640",
    "end": "6388920"
  },
  {
    "text": "And many of those\nfilters are just different tone\nmapping filters that make your picture look a little\nmore vibrant or a little less",
    "start": "6388920",
    "end": "6397079"
  },
  {
    "text": "vibrant or black and\nwhite and this and that. So this is not a useful\ngoal for autonomy,",
    "start": "6397080",
    "end": "6402540"
  },
  {
    "text": "but there's other\npurposes for tone mapping that could be very\nuseful for autonomy. So for instance, you can use\nit to improve the dynamic range",
    "start": "6402540",
    "end": "6409260"
  },
  {
    "text": "of your image, or you\ncan use it to improve the contrast of the image\nand things of that nature.",
    "start": "6409260",
    "end": "6414360"
  },
  {
    "text": "It could be very useful\nfor sensing applications. Now, dynamic range,\nnote that it's",
    "start": "6414360",
    "end": "6420120"
  },
  {
    "text": "the ratio of the minimum\nto maximum intensity that the sensor can\nrecord in your image.",
    "start": "6420120",
    "end": "6425220"
  },
  {
    "text": "And the image\ncontrast is typically taken as the difference in the\nintensity of the pixels divided",
    "start": "6425220",
    "end": "6434280"
  },
  {
    "text": "by the average intensity. So that's the\ntechnical definition of dynamic range in contrast.",
    "start": "6434280",
    "end": "6439560"
  },
  {
    "text": "And when it comes\nto tone mapping there's two classes or types\nof algorithms for tone mapping,",
    "start": "6439560",
    "end": "6445320"
  },
  {
    "text": "one is called\nglobal tone mapping, and that's basically\nthe same function,",
    "start": "6445320",
    "end": "6450540"
  },
  {
    "text": "the same correction is\napplied to every image pixel. Same function, every\nimage, every pixel",
    "start": "6450540",
    "end": "6457739"
  },
  {
    "text": "is affected the same way. Or you can have\nalgorithms which do local tone mapping\nand that's where",
    "start": "6457740",
    "end": "6463199"
  },
  {
    "text": "the parameters of the\ntone mapping function is different for every pixel. And it uses some information\nfrom the neighboring pixels",
    "start": "6463200",
    "end": "6472020"
  },
  {
    "text": "to decide like is this pixel for\ninstance in a highlight area, in a shadow area,\nand things like that",
    "start": "6472020",
    "end": "6481080"
  },
  {
    "text": "to be smarter about how tone\nmapping should be applied. So generally speaking,\nlocal tone mapping",
    "start": "6481080",
    "end": "6487680"
  },
  {
    "text": "can result in much\nbetter images. But of course, it's a\nmore sophisticated type",
    "start": "6487680",
    "end": "6493500"
  },
  {
    "text": "of algorithm. And tone mapping again\nis one of those areas where many, many\ndifferent algorithms",
    "start": "6493500",
    "end": "6499920"
  },
  {
    "text": "exist from super, super\nsimple ones, one of which we'll talk about, to really\nadvanced models using",
    "start": "6499920",
    "end": "6506460"
  },
  {
    "text": "big machine learning models\nand things like that. The simplest global\ntone mapping algorithm",
    "start": "6506460",
    "end": "6515280"
  },
  {
    "text": "that actually is applied quite a\nlot is called gamma correction.",
    "start": "6515280",
    "end": "6521579"
  },
  {
    "text": "And gamma correction, it's\na very simple function. So it just basically\ntakes the value",
    "start": "6521580",
    "end": "6526920"
  },
  {
    "text": "of every pixel in the\nimage and raises that to some power gamma. So you apply a value for gamma.",
    "start": "6526920",
    "end": "6533280"
  },
  {
    "text": "It can be bigger than And you apply that\nsame function, I/O equals I to the power\nof gamma to every pixel.",
    "start": "6533280",
    "end": "6540900"
  },
  {
    "text": "Now, what is the input? What is I's of I? It can be the individual\nR and G and B color",
    "start": "6541500",
    "end": "6549960"
  },
  {
    "text": "values of every pixel. That's one way of\ndoing gamma correction, or you can apply it just to the\nintensity value of every pixel.",
    "start": "6549960",
    "end": "6558600"
  },
  {
    "text": "So what actually goes into\nthat gamma correction function you can decide what it could be. But here's one example\nto show you how",
    "start": "6558600",
    "end": "6566700"
  },
  {
    "text": "gamma correction can be useful. So in this case, we have an\nHDR, high dynamic range image,",
    "start": "6566700",
    "end": "6571980"
  },
  {
    "text": "which is this input image. And this image was captured with\na high dynamic range camera,",
    "start": "6571980",
    "end": "6577560"
  },
  {
    "text": "but now we are\ndisplaying it on a medium like my iPad display doesn't\nhave as much dynamic range",
    "start": "6577560",
    "end": "6584820"
  },
  {
    "text": "as the image sensor. So the image looks saturated. Specifically, if you look at\nthe highlights of the sky,",
    "start": "6584820",
    "end": "6591480"
  },
  {
    "text": "this all looks completely\nsaturated and white. So we need to compress\nthe dynamic range,",
    "start": "6591480",
    "end": "6597900"
  },
  {
    "text": "but the information is there. The actual sensor was able\nto capture those highlights but I can't display it.",
    "start": "6597900",
    "end": "6603780"
  },
  {
    "text": "I don't have enough\ndynamic range in my display to show that. So I need to compress the\ndynamic range of the image",
    "start": "6603780",
    "end": "6610860"
  },
  {
    "text": "such that I can\nshow all the details on my iPad for\ninstance, IPad display.",
    "start": "6610860",
    "end": "6617700"
  },
  {
    "text": "And this is a case where gamma\ncorrection can work and can be very, very effective.",
    "start": "6617700",
    "end": "6622980"
  },
  {
    "text": "So when it comes to\nthe image, as we said, you can apply gamma\ncorrection to the intensity. So this top image,\nyou can think of it",
    "start": "6622980",
    "end": "6630360"
  },
  {
    "text": "as another way of instead of\njust thinking about as RGB values, you can think of it as--",
    "start": "6630360",
    "end": "6635820"
  },
  {
    "text": "it has some intensity\ninformation, also called luminosity, plus some\ncolor information,",
    "start": "6636360",
    "end": "6642360"
  },
  {
    "text": "also called chrominance\nof the image.",
    "start": "6642360",
    "end": "6648060"
  },
  {
    "text": "So you can have luminance\nand chrominance. It's another way of\ndescribing what the image is.",
    "start": "6648060",
    "end": "6653880"
  },
  {
    "text": "So we are going to-- because we want\nto do compression",
    "start": "6654480",
    "end": "6659700"
  },
  {
    "text": "we're going to choose a gamma\nvalue that's less than 1. So let's say here we arbitrarily\nchose gamma equals 0.5,",
    "start": "6659700",
    "end": "6665280"
  },
  {
    "text": "and we're going\nto apply two ways, one is we apply it to\nevery color individually.",
    "start": "6665280",
    "end": "6670920"
  },
  {
    "text": "So we raise R to the\ngamma, B to the gamma, and G to the gamma in\nthat way-- in one way.",
    "start": "6670920",
    "end": "6676440"
  },
  {
    "text": "And this is the\nimage that we get. Note that it got the job done. So now we see all the\ndetails in the highlights",
    "start": "6676440",
    "end": "6684600"
  },
  {
    "text": "which were missing in\nthe original picture. But the artifact\nor the side effect",
    "start": "6684600",
    "end": "6690900"
  },
  {
    "text": "is that it kind of\nlooks washed out. That's the trade-off\nthat was made. Here's another way of applying.",
    "start": "6690900",
    "end": "6697680"
  },
  {
    "text": "So now we're applying\nthe gamma correction just to the intensity. So basically, we just\ncalculate the intensity,",
    "start": "6697680",
    "end": "6703560"
  },
  {
    "text": "apply gamma correction\nto that, and don't touch the chrominance. And then combine them again\nand then form the final image.",
    "start": "6703560",
    "end": "6711000"
  },
  {
    "text": "Again, it gets the dynamic\nrange compression job done. All those details,\nhighlights are recovered,",
    "start": "6711000",
    "end": "6718320"
  },
  {
    "text": "but the colors look\noverexaggerated in this one. Now, it's a matter of\ntaste which one of these",
    "start": "6719340",
    "end": "6727260"
  },
  {
    "text": "you like better or would like\nto use for artistic reasons.",
    "start": "6727260",
    "end": "6732960"
  },
  {
    "text": "But for sensing\npurposes, both of these are acceptable\nbecause the goal was",
    "start": "6732960",
    "end": "6739560"
  },
  {
    "text": "to recover the dynamic range or\ncompress the dynamic range such that on this display,\nI can show it",
    "start": "6739560",
    "end": "6745260"
  },
  {
    "text": "and both of these gamma\ncorrections achieve that.",
    "start": "6745260",
    "end": "6749519"
  },
  {
    "text": "OK. So the last thing we\nwant to talk about is the last step,\nwhich is typically",
    "start": "6750420",
    "end": "6758485"
  },
  {
    "text": "some denoising and sharpening\nas is applied to the images. And these are typically\nfiltering operations,",
    "start": "6758485",
    "end": "6764820"
  },
  {
    "text": "although more modern techniques\nagain like machine learning can also be used.",
    "start": "6764820",
    "end": "6769260"
  },
  {
    "text": "You can apply neural\nnets to achieve denoising and\nsharpening and those would do really, really well,\nespecially the modern ones.",
    "start": "6772200",
    "end": "6778679"
  },
  {
    "text": "But classically,\nthese were implemented as filtering operations. And in their simplest forms,\nthey use linear filtering.",
    "start": "6778680",
    "end": "6786780"
  },
  {
    "text": "So just basically 2D\nconvolutions with some filter kernels could be used for\ndenoising and sharpening.",
    "start": "6786780",
    "end": "6793620"
  },
  {
    "text": "Here are two examples. Here is a classical\ndenoising filter which",
    "start": "6793620",
    "end": "6799740"
  },
  {
    "text": "is called the Gaussian blur. So you take your noisy image\nlike a zoomed-in portion",
    "start": "6799740",
    "end": "6805020"
  },
  {
    "text": "of a bigger image\nand you convolved that with this filter kernel\nwhich, as you see, I mean,",
    "start": "6805020",
    "end": "6810480"
  },
  {
    "text": "it has a high value in\nthe center and it drops",
    "start": "6810480",
    "end": "6814440"
  },
  {
    "text": "according to your Gaussian\nprofile as you go away from the center pixel. And this is just to normalize\nthe sum of all the pixel values",
    "start": "6815580",
    "end": "6823440"
  },
  {
    "text": "to 1, so we don't saturate\nthe image after filtering or dim it too much.",
    "start": "6823440",
    "end": "6830099"
  },
  {
    "text": "And after applying this,\nit blends the value of each pixel with\nthe neighboring ones",
    "start": "6830100",
    "end": "6835680"
  },
  {
    "text": "with some Gaussian-weighted sum. And this is what you get. So it gets the job done. You see it's much less\nnoisy in all areas,",
    "start": "6835680",
    "end": "6845160"
  },
  {
    "text": "but also you lose some contrast. If you look at this portion,\nit's less sharp here.",
    "start": "6846180",
    "end": "6853079"
  },
  {
    "text": "You get some blurriness. And that's also the\nname of the filter. I mean, because you're blending\nthe value of neighboring pixels",
    "start": "6853080",
    "end": "6860160"
  },
  {
    "text": "we get a little bit of\nblurriness applied to it. And here is one very\nsimple sharpening filter.",
    "start": "6860160",
    "end": "6866699"
  },
  {
    "text": "This is our filter kernel. It has a positive\nvalue in the center, and then the side pixels\nhave a negative value.",
    "start": "6866700",
    "end": "6873300"
  },
  {
    "text": "So it takes the difference\nof the center pixel with the neighbors and\nenhances edge contrast.",
    "start": "6873300",
    "end": "6880920"
  },
  {
    "text": "If those pixels are at different\nvalues, it even enhances that. And that's the effect.",
    "start": "6880920",
    "end": "6886200"
  },
  {
    "text": "The original picture\nit was very soft, and then after applying that\nfilter, we get much sharper",
    "start": "6886200",
    "end": "6893520"
  },
  {
    "text": "features. Again, this is just to\nillustrate the possibility of what you can do\nwith linear filtering",
    "start": "6893520",
    "end": "6899040"
  },
  {
    "text": "when it comes to\ndenoising and sharpening. These are by no means\nthe best possible filters",
    "start": "6899040",
    "end": "6906900"
  },
  {
    "text": "you could use, but\ncould be useful for certain applications. Now, if you use any\nmodern-day computer vision,",
    "start": "6906900",
    "end": "6916380"
  },
  {
    "text": "programming library\nlike OpenCV and Python, all of the functions\nwe talked about--",
    "start": "6916380",
    "end": "6922380"
  },
  {
    "text": "demosaicing, white balancing,\ntone mapping, denoising, sharpening, all of\nthese are already--",
    "start": "6922920",
    "end": "6931380"
  },
  {
    "text": "all of these are included\nin these modern libraries. And for each\nfunction, you usually",
    "start": "6934080",
    "end": "6941520"
  },
  {
    "text": "get a variety of algorithms\nthat are already implemented. And you can try them and you can\nvary the parameters very easily",
    "start": "6941520",
    "end": "6950219"
  },
  {
    "text": "with a few lines of code and\nyou see what works better for your application.",
    "start": "6950220",
    "end": "6955440"
  },
  {
    "text": "So that could really be the way\nto go if you're not quite sure. Oh, I want to do some denoising.",
    "start": "6955440",
    "end": "6961260"
  },
  {
    "text": "I'm not quite sure\nwhere to start. You can always go-- OpenCV is usually\nmy go-to package.",
    "start": "6961260",
    "end": "6967380"
  },
  {
    "text": "I go and I read\nthe documentation, look what denoising options do\nI get, and I try a few of them.",
    "start": "6967380",
    "end": "6974160"
  },
  {
    "text": "And these are so well\nimplemented these days that literally in two lines\nof code you can do that.",
    "start": "6974160",
    "end": "6979560"
  },
  {
    "text": "Also, well, you could\nuse ChatGPT of course. It's actually\npretty good when it",
    "start": "6979560",
    "end": "6985320"
  },
  {
    "text": "comes to these kinds\nof experimentations. You can just say,\nhey, ChatGPT, write",
    "start": "6985320",
    "end": "6990360"
  },
  {
    "text": "me a OpenCV script that\nimports an image in this format and it applies a Gaussian\nblur denoising filter",
    "start": "6990360",
    "end": "6998580"
  },
  {
    "text": "with width of 1 and And it will do it. And then that can be a skeleton\nscript that you can start.",
    "start": "6998580",
    "end": "7007400"
  },
  {
    "text": "I'm not saying you should\ndefinitely use ChatGPT, but if you're very unfamiliar\nwith how these packages work,",
    "start": "7008300",
    "end": "7016820"
  },
  {
    "text": "it could be a start. It could give you\na place to start and then you can experiment\nwith different specifications.",
    "start": "7016820",
    "end": "7021620"
  },
  {
    "text": "OK. So this concludes the discussion\nabout the architecture of the vision system and the\nISP or image signal processing",
    "start": "7022640",
    "end": "7031100"
  },
  {
    "text": "steps and some\nsimple algorithms. I'm going to finish this\nchapter with just a very brief",
    "start": "7031100",
    "end": "7036980"
  },
  {
    "text": "discussion about\nfuture directions in vision systems for autonomy.",
    "start": "7036980",
    "end": "7043220"
  },
  {
    "text": "So I want to start by saying\nthat vision sensors as of today are the most important\nsensors in robotics today.",
    "start": "7043220",
    "end": "7051020"
  },
  {
    "text": "When it comes to\nsensing for robotics, vision or camera is\nreally king, basically.",
    "start": "7051020",
    "end": "7055640"
  },
  {
    "text": "And that's for a\ncouple of reasons. One is in terms of the\nmaturity of the hardware.",
    "start": "7057560",
    "end": "7065420"
  },
  {
    "text": "And by maturity, I mean,\nthe cost, the size, the performance, the power,\nall these key metrics.",
    "start": "7066380",
    "end": "7074000"
  },
  {
    "text": "Cameras today are way\nahead of other sensors. Their orders of magnitude is\ncheaper than radars and LiDARs.",
    "start": "7074000",
    "end": "7080840"
  },
  {
    "text": "They're much more robust. You can get much higher-quality\nsignals and images out of them.",
    "start": "7080840",
    "end": "7087260"
  },
  {
    "text": "So that's one reason. But this is not a\nfundamental reason. I mean, we know cameras\nbecause it started way",
    "start": "7088460",
    "end": "7096500"
  },
  {
    "text": "before robotics was a thing and\nthere's years and years of R&D",
    "start": "7096500",
    "end": "7101840"
  },
  {
    "text": "and academia and\nindustry behind it, so the technology is\nnaturally more advanced. But eventually, I think other\nsensors are going to catch up.",
    "start": "7101840",
    "end": "7110480"
  },
  {
    "text": "It's not exactly at the\nsame point as cameras. They're going to close the gap\nquite a bit in terms of cost",
    "start": "7110480",
    "end": "7116120"
  },
  {
    "text": "and power and performance. But also for cameras\nwe are benefiting",
    "start": "7116120",
    "end": "7121760"
  },
  {
    "text": "from many, many years of\nresearch and development and advancements in computer\nvision and machine learning.",
    "start": "7121760",
    "end": "7129800"
  },
  {
    "text": "So most of perception\npipelines today are just designed\nto work with images",
    "start": "7129800",
    "end": "7135260"
  },
  {
    "text": "and that really helps when it\ncomes to autonomy because we need to make sense\nof the signals",
    "start": "7135920",
    "end": "7142280"
  },
  {
    "text": "that we get out of our sensors. And already a lot of\ntools and algorithms",
    "start": "7142280",
    "end": "7147920"
  },
  {
    "text": "exist that helps us make\nsense of camera images. So for instance, if you\nwant to do classification",
    "start": "7147920",
    "end": "7153620"
  },
  {
    "text": "on a sensor image, a\nsensor output for cameras,",
    "start": "7153620",
    "end": "7160460"
  },
  {
    "text": "you can in 10 minutes\ngo download YOLOv8, for instance, which is a\nbeautiful really, really",
    "start": "7160460",
    "end": "7168500"
  },
  {
    "text": "high-quality implementation\nof the classification network. And it works real time very\nreliably, very good performing.",
    "start": "7168500",
    "end": "7176420"
  },
  {
    "text": "But if you want to do that\nsame task on a 3D point cloud from a LiDAR or a radar, your\noptions are much more limited.",
    "start": "7176420",
    "end": "7183200"
  },
  {
    "text": "They're not as\nhigh-performance and you need to come up maybe with your\nown ideas and implementations.",
    "start": "7183200",
    "end": "7190160"
  },
  {
    "text": "Again, this is not a\nfundamental reason. It's just the fact\nthat with cameras we're",
    "start": "7190160",
    "end": "7196460"
  },
  {
    "text": "benefiting from years and\nyears of machine and computer vision and machine vision R&D.",
    "start": "7196460",
    "end": "7201140"
  },
  {
    "text": "Now, also, I want to\nnote that we talked about RGB cameras, which is\nthe most common type of camera, but there are also other more\nspecialized types of imaging",
    "start": "7202220",
    "end": "7211220"
  },
  {
    "text": "sensors and some\nof them could be more suitable for specific\ntasks in autonomy.",
    "start": "7211220",
    "end": "7217400"
  },
  {
    "text": "So for instance, you\ncan get thermal cameras. They're very close\nto RGB cameras. They just sense at\nlonger wave IRs, which",
    "start": "7217400",
    "end": "7225320"
  },
  {
    "text": "is proportional\nto the temperature of different subjects,\nbut principle of operation",
    "start": "7225320",
    "end": "7231679"
  },
  {
    "text": "is very similar. These days you can get\nstereo camera systems which use basically two\ncameras with some distance",
    "start": "7231680",
    "end": "7239179"
  },
  {
    "text": "away, which is\ncalled the baseline. And then in addition to\ngiving the RGB image, they can also apply\nsome geometric methods",
    "start": "7239180",
    "end": "7248600"
  },
  {
    "text": "or some machine learning methods\nto estimate depth in the scene. So they give you point\nclouds which are RGB color.",
    "start": "7248600",
    "end": "7256040"
  },
  {
    "text": "They can be very useful\nbut note that typically the maximum depth that\nthey can sense is limited.",
    "start": "7256040",
    "end": "7263360"
  },
  {
    "text": "And it's a function\nof the baseline, so to be able to get\nmore depth you need to space your cameras\nmore further apart,",
    "start": "7263360",
    "end": "7268940"
  },
  {
    "text": "also as a function of the\nresolution of the camera sensors. But stereo cameras\nfor applications",
    "start": "7268940",
    "end": "7274880"
  },
  {
    "text": "in robotics where you\ndon't need to see that far, you only need to see It can be a very\ninteresting option.",
    "start": "7274880",
    "end": "7281660"
  },
  {
    "text": "And then you get even\nmore specialized types of cameras like\ndynamic vision sensors",
    "start": "7281660",
    "end": "7287420"
  },
  {
    "text": "where instead of just\nmeasuring intensity they measure time\nderivatives of intensity.",
    "start": "7287420",
    "end": "7294679"
  },
  {
    "text": "And they give you\nthese events per pixel, which could be useful for very\nspecific tasks in computer",
    "start": "7295280",
    "end": "7303079"
  },
  {
    "text": "vision and autonomy. And also, I want to note\nthat well, yes, camera",
    "start": "7303080",
    "end": "7310040"
  },
  {
    "text": "is the most important\ntype of sensor today, but I really believe-- and\nin certain applications,",
    "start": "7310040",
    "end": "7316340"
  },
  {
    "text": "a camera-only approach for\nsensing could be sufficient. But in general, that's not true.",
    "start": "7316340",
    "end": "7322460"
  },
  {
    "text": "I don't believe that is true. And that's again\nbecause by now, I think at the end\nof the quarter you",
    "start": "7322460",
    "end": "7328520"
  },
  {
    "text": "know a lot about the\nphysics of sensing. And with that, we know\nthat for instance cameras",
    "start": "7328520",
    "end": "7336440"
  },
  {
    "text": "do have fundamental issues\nwith use cases like low light",
    "start": "7336440",
    "end": "7342260"
  },
  {
    "text": "or ultra-high dynamic range\nscenes, or low visibility scenes where there's\nbad weather like rain",
    "start": "7342260",
    "end": "7348199"
  },
  {
    "text": "and fog and other things. And if you want a solution\nthat can cover these use cases",
    "start": "7348200",
    "end": "7355639"
  },
  {
    "text": "or a solution that has\nrequirements that a camera",
    "start": "7355640",
    "end": "7361460"
  },
  {
    "text": "approach fundamentally\ncannot do, so for instance, if you need to see\nvery long ranges,",
    "start": "7361460",
    "end": "7368600"
  },
  {
    "text": "if you need to see hundreds\nof meters with good accuracy, even a good stereo camera\nsystem would really,",
    "start": "7368600",
    "end": "7377660"
  },
  {
    "text": "really struggle to achieve\nthat kind of depth mapping. And that's where you're forced\nto use other sensing modalities",
    "start": "7377660",
    "end": "7386420"
  },
  {
    "text": "like LiDAR, radar,\nsonar, and others. And also another reason to\nnot just rely on cameras",
    "start": "7386420",
    "end": "7395300"
  },
  {
    "text": "is in cases where\nsafety is a concern. You need redundancy\nin your sensors such",
    "start": "7395300",
    "end": "7401000"
  },
  {
    "text": "that if one modality fails, the\nother one can still cover you. So for that, for\nthose reasons, I",
    "start": "7401000",
    "end": "7408200"
  },
  {
    "text": "think a vision-only\napproach to autonomy wouldn't basically\ncover all use cases",
    "start": "7408200",
    "end": "7414260"
  },
  {
    "text": "but it can cover a subset\nof autonomy applications.",
    "start": "7414260",
    "end": "7418639"
  },
  {
    "text": "And I want to close with talking\nabout some future directions in vision sensors. So vision sensors, we have years\nof development for RGB cameras",
    "start": "7419720",
    "end": "7432080"
  },
  {
    "text": "and they were mostly\nfor photography and for artistic reasons. But now that we're entering\nthis era of robotic autonomy,",
    "start": "7432080",
    "end": "7440600"
  },
  {
    "text": "there is a lot of\nrethinking going into what a proper camera or\na vision system for autonomy",
    "start": "7440600",
    "end": "7449240"
  },
  {
    "text": "could be or should be and\nredesigning even the hardware to be more suitable for\nthose types of applications.",
    "start": "7449240",
    "end": "7456680"
  },
  {
    "text": "The general trend is that\nthe future vision sensors",
    "start": "7456680",
    "end": "7461720"
  },
  {
    "text": "become more intelligent. And by more intelligent,\nI mean, if you",
    "start": "7461720",
    "end": "7468560"
  },
  {
    "text": "zoom into just one pixel\nin your vision sensor, what it does today\nis it basically",
    "start": "7468560",
    "end": "7474500"
  },
  {
    "text": "measures light intensity\nat a given wavelength. Basically, in red,\ngreen, or blue.",
    "start": "7474500",
    "end": "7480380"
  },
  {
    "text": "But now we're moving towards\nthese more sophisticated cameras that support\nin-pixel signal processing.",
    "start": "7480380",
    "end": "7486920"
  },
  {
    "text": "So every pixel has like a\nlittle brain of its own. Well, brain is of\ncourse overexaggerating,",
    "start": "7486920",
    "end": "7492619"
  },
  {
    "text": "but it has processing\ncapabilities of its own. And it can be\nindividually programmed.",
    "start": "7492620",
    "end": "7499040"
  },
  {
    "text": "And then by being clever about\nhow you program these things,",
    "start": "7499040",
    "end": "7503300"
  },
  {
    "text": "you can achieve like really,\nreally sophisticated sensing tasks. So for instance, you can do\nvery complex feature extractions",
    "start": "7504440",
    "end": "7513800"
  },
  {
    "text": "right by the camera. We're not talking about ISP. At pixel level, these\ncameras can be programmed",
    "start": "7513800",
    "end": "7519980"
  },
  {
    "text": "to do sophisticated things. Or for instance, if you\nhave individual control",
    "start": "7519980",
    "end": "7525199"
  },
  {
    "text": "over the exposure\nduration for every pixel versus the current cameras\nwhere you have just one fixed",
    "start": "7525200",
    "end": "7531740"
  },
  {
    "text": "exposure duration\nfor all the pixels, you can have your cameras\ndo ultra-high dynamic range",
    "start": "7531740",
    "end": "7537560"
  },
  {
    "text": "imaging things like has been demonstrated,\nwhich is unheard",
    "start": "7537560",
    "end": "7543380"
  },
  {
    "text": "of in classical cameras. You can do much better\ndepth estimation, much better tracking,\nand other tasks",
    "start": "7543380",
    "end": "7550040"
  },
  {
    "text": "that are very\nrelevant to autonomy. And the way this is achieved\nat the hardware level is--",
    "start": "7550040",
    "end": "7555200"
  },
  {
    "text": "this is one very interesting\nway of thinking about it. So we have this axis here\nshown by an arrow, where",
    "start": "7555200",
    "end": "7560780"
  },
  {
    "text": "as you go from left\nto right, your cameras or every pixel in\nthe camera hardware is becoming more intelligent.",
    "start": "7560780",
    "end": "7566660"
  },
  {
    "text": "And the way that is done is\nby having more transistors per pixel, which means more\nlogic or more signal processing",
    "start": "7566660",
    "end": "7574940"
  },
  {
    "text": "capabilities per pixel. So at the far left, we have\nour pretty unintelligent--",
    "start": "7574940",
    "end": "7582020"
  },
  {
    "text": "just the 3T APS pixel. And this is the numbers or\nnumber of transistors order",
    "start": "7582020",
    "end": "7587780"
  },
  {
    "text": "of magnitude per pixel. So this is 2 to 1, 2 the 2. Slightly more sophisticated\nthan the 3T APS",
    "start": "7587780",
    "end": "7594199"
  },
  {
    "text": "where you can have the log\npixel, which is basically an APS with improved dynamic\nrange but with some trade-offs.",
    "start": "7594200",
    "end": "7600380"
  },
  {
    "text": "Then you get to something\nlike 2 to the 4, transistors per pixels.",
    "start": "7600380",
    "end": "7606560"
  },
  {
    "text": "You can have the dynamic\nvision sensor pixel here, which we can break it\ndown into three pieces.",
    "start": "7606560",
    "end": "7613040"
  },
  {
    "text": "There's the\nphotoreceptor piece, then there is this\ndifference in circuit",
    "start": "7613040",
    "end": "7618140"
  },
  {
    "text": "which takes a time derivative\nof intensity per pixel, and then there's\nsome competitors.",
    "start": "7618140",
    "end": "7623360"
  },
  {
    "text": "So this already can do more\nsophisticated things per pixel. And then you go even\nfurther to 2 to the 8,",
    "start": "7623360",
    "end": "7630500"
  },
  {
    "text": "transistors per pixel, and then that's\nwhere you get things like SCAMP, which is a very\ninteresting type of camera,",
    "start": "7631220",
    "end": "7637520"
  },
  {
    "text": "which has almost general\npurpose programmable pixel. I'm not going to go\ninto the details of it,",
    "start": "7637520",
    "end": "7644239"
  },
  {
    "text": "but there's a lot that you can\ndo with these types of sensors now. Today they're not widely\ncommercially deployed.",
    "start": "7644240",
    "end": "7650720"
  },
  {
    "text": "And they come with\na lot of trade-offs, main one being as you increase\nthe number of transistors, as you know if you want to\nkeep the high resolution,",
    "start": "7650720",
    "end": "7657739"
  },
  {
    "text": "that means that photo-- well, these transistors take\narea in the silicon wafer.",
    "start": "7657740",
    "end": "7663140"
  },
  {
    "text": "So the photosensitive\narea becomes a smaller and\nsmaller and smaller, which means you're going to be--",
    "start": "7663140",
    "end": "7668660"
  },
  {
    "text": "your light collection\nefficiency is going to become lower and\nlower, which is a big trade-off.",
    "start": "7668660",
    "end": "7675680"
  },
  {
    "text": "So because of that,\nthese ideas, they have been deployed in\nresearch communities.",
    "start": "7675680",
    "end": "7683960"
  },
  {
    "text": "Very interesting things\nare being done with them, but I think they are still years\naway from becoming mainstream.",
    "start": "7683960",
    "end": "7691520"
  },
  {
    "text": "And of course, sky is the limit. I'm very curious to see as we\ngo to even more transistors",
    "start": "7691520",
    "end": "7700340"
  },
  {
    "text": "per pixel, which means we can\ndo more sophisticated logic, where do we land?",
    "start": "7700340",
    "end": "7705440"
  },
  {
    "text": "Could we eventually\nhave cameras where you have a full neural net per\npixel that's doing something",
    "start": "7705440",
    "end": "7711800"
  },
  {
    "text": "very, very sophisticated? I don't know. Maybe. OK. So that's my take on future\ndirections in vision sensors.",
    "start": "7711800",
    "end": "7719900"
  },
  {
    "text": "And I think with this, we\ncan conclude the chapter on cameras and vision\nsystems and the course.",
    "start": "7719900",
    "end": "7728360"
  },
  {
    "text": "Again, I hope you found the\ncourse useful and interesting. Thank you, and\nhave a nice summer.",
    "start": "7728360",
    "end": "7734840"
  }
]