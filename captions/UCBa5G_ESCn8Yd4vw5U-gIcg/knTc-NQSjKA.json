[
  {
    "text": "Okay. So I'm gonna talk about, um, BERT and also some, uh, kind of precursor work and then some",
    "start": "4340",
    "end": "11264"
  },
  {
    "text": "follow-up work that's happened in- in the last year and- well, not follow up, but, uh, more- more recent advancements, uh,",
    "start": "11265",
    "end": "16785"
  },
  {
    "text": "that's happened, uh, since then. So, uh, first, we're gonna talk about history and background.",
    "start": "16785",
    "end": "22425"
  },
  {
    "text": "So, um, everyone knows and loves word embeddings in NLP, right? They're kind of the basis for, uh,",
    "start": "22425",
    "end": "28785"
  },
  {
    "text": "why neural networks, um, work for NLP. Uh, because neural networks work in a continuous space, uh, vectors,",
    "start": "28785",
    "end": "37550"
  },
  {
    "text": "and- and matrices, and obviously, text is a discrete space and so there needs to be something to bridge the gap.",
    "start": "37550",
    "end": "44450"
  },
  {
    "text": "And, uh, it turns out that the thing to bridge the gap, it's actually pretty simple. It's just a lookup table from, um, each,",
    "start": "44450",
    "end": "52545"
  },
  {
    "text": "from a set of discrete vocabulary to, uh, a vector that's learned discriminatively end to end, right? So originally these were just learned,",
    "start": "52545",
    "end": "58730"
  },
  {
    "text": "uh, like in- in, uh, the original Bengio 2003 neural language model paper, these were just trained discriminatively end to end, um,",
    "start": "58730",
    "end": "65865"
  },
  {
    "text": "and these were actually, and- and so then, people would train language models and then use these pre-trained usi- using the embedding layer as pre-trained representations,",
    "start": "65865",
    "end": "74450"
  },
  {
    "text": "um, for- for other tasks. But they wouldn't use the rest of the language model, they would just use the embedding layer. And then, uh, word2vec and GloVe and stuff came",
    "start": "74450",
    "end": "81670"
  },
  {
    "text": "along where then people found a much cheaper, much more scalable way to train it. Where you can just use the, uh,",
    "start": "81670",
    "end": "89130"
  },
  {
    "text": "statistics of a- a corpus where it's just a linear model so you don't have to- to compute these expensive feedforward layers",
    "start": "89130",
    "end": "94600"
  },
  {
    "text": "that you're gonna throw out anyways, uh, and so you can scale up to billions of tokens on a single, uh, CPU, right?",
    "start": "94600",
    "end": "100580"
  },
  {
    "text": "So the problem though is that these word embeddings are,",
    "start": "100580",
    "end": "105920"
  },
  {
    "text": "uh, applied in the context free manner, right? So- so for like a kind of a simple toy example, the word bank,",
    "start": "105920",
    "end": "111550"
  },
  {
    "text": "if you say, open a bank account and on a river bank, it's gonna be the same embedding. So people have tried to do stuff like, uh,",
    "start": "111550",
    "end": "118230"
  },
  {
    "text": "word sense embeddings where it's not just a single word, it's a- it's a full word sentence. But this kind of bank example,",
    "start": "118230",
    "end": "124329"
  },
  {
    "text": "it's a little bit of a toy example, right? Most- almost any word has a different meaning depending on the context.",
    "start": "124330",
    "end": "130039"
  },
  {
    "text": "Um, it's very- so- so even- even like open the bank account and I went to the bank. Tho- those are still semi-different senses of the word bank,",
    "start": "130040",
    "end": "139210"
  },
  {
    "text": "one of them is a- I mean they have different part of speech tags kind of, uh, well, I guess, not really. But like they're kind of- they're kind of using different senses.",
    "start": "139210",
    "end": "145710"
  },
  {
    "text": "Right? And so, um, yes. So- so- so we really need a contextual representation. Right? So we want something where it's a representation of a word",
    "start": "145710",
    "end": "153340"
  },
  {
    "text": "after it's been put into the- the context of the sentence that we've seen it in, right? Which would be like at the bottom here.",
    "start": "153340",
    "end": "159100"
  },
  {
    "text": "So kind of for a history of contextual representations, the first big paper for this type of contextual representations was a paper, uh,",
    "start": "159100",
    "end": "166955"
  },
  {
    "text": "from Google, uh, in 2015 called Semi-supervised Sequence Learning from Andrew Dai and Quoc V. Le.",
    "start": "166955",
    "end": "172130"
  },
  {
    "text": "And so in this one, uh, it was actually very similar to- to- to papers that came after it.",
    "start": "172130",
    "end": "177950"
  },
  {
    "text": "It didn't get as much attention for- for various reasons. So, but basically, they- they had some classification task like,",
    "start": "177950",
    "end": "183510"
  },
  {
    "text": "uh, sentiment classification on movie reviews. And they had a big corpus of movie reviews. And so then they said, what happens if we just take our existing LSTM model,",
    "start": "183510",
    "end": "190780"
  },
  {
    "text": "and instead of just using pre-trained embeddings, which everyone has already been doing since, like at least, uh, for the like- actually probably since 2003,",
    "start": "190780",
    "end": "197629"
  },
  {
    "text": "um, people had been using pre-trained embeddings. But they- they said, let's actually pretrain the entire model as a language model,",
    "start": "197629",
    "end": "203239"
  },
  {
    "text": "and then let's fine tune it for our classification test. And they got pretty good results,:",
    "start": "203240",
    "end": "209450"
  },
  {
    "text": "but not like stellar results. And so now we know that the reason why they didn't get stellar results is they didn't train it on enough data.",
    "start": "209450",
    "end": "214700"
  },
  {
    "text": "And they- because they basically train on the same corpus that they were training on and they trained the same size model that they were training on,",
    "start": "214700",
    "end": "219845"
  },
  {
    "text": "which we now know it needs to be bigger. But that's kind of- uh, this was- this was already kind of a little bit ahead of its time,",
    "start": "219845",
    "end": "225845"
  },
  {
    "text": "um, partially because like, you know, stuff wasn't, like, we didn't have as much compute back then, even though it was like five years ago,",
    "start": "225845",
    "end": "231444"
  },
  {
    "text": "uh, and it would have been, you know, more expensive. So, uh, and then in, uh, 2017,",
    "start": "231445",
    "end": "237690"
  },
  {
    "text": "ELMo came out which was, uh, from the University of Washington and AI2. And so this one,",
    "start": "237690",
    "end": "244345"
  },
  {
    "text": "they- they did something pretty clever where they took, um, you train a language on a big corpus,",
    "start": "244345",
    "end": "249920"
  },
  {
    "text": "so they trained it on a billion word corpus. And they trained a- a big model, an LSTM with 4,000 hidden dimensions,",
    "start": "249920",
    "end": "255800"
  },
  {
    "text": "which is quite expensive. And they trained a bidirectional model. So, but- but it was",
    "start": "255800",
    "end": "260959"
  },
  {
    "text": "kind of weakly bidirectional where they trained a left-right model and then, a left to right model.",
    "start": "260960",
    "end": "266180"
  },
  {
    "text": "And then they concatenated the two. And they called these contextual pretrained embeddings. And so the idea behind ELMo is that this",
    "start": "266180",
    "end": "273139"
  },
  {
    "text": "doesn't actually change your existing model architecture. You kind of take whatever task-specific model architecture that you have,",
    "start": "273140",
    "end": "280160"
  },
  {
    "text": "which could be, you know, for, uh, question answering. It might be some sort of fancy model where you do a LSTM over",
    "start": "280160",
    "end": "285349"
  },
  {
    "text": "the source and over- over the question and over the answer. Then you- then you attend to one another and in whatever kind of architecture you have.",
    "start": "285350",
    "end": "291950"
  },
  {
    "text": "And just wherever you would have put in GloVe embeddings before, now you put in ELMo embeddings. Um, and so this got state of the art on,",
    "start": "291950",
    "end": "300455"
  },
  {
    "text": "you know, everything at the time, question-answering, semantic parsing, syntactic parsing because it was- and- and so if you just took any existing kinda state-of-the-art model.",
    "start": "300455",
    "end": "308845"
  },
  {
    "text": "You could fit in- put in ELMo and banks and get state of the art, right. Uh, but- but they weren't, uh- but these were kinda- the models were kinda fixed.",
    "start": "308845",
    "end": "316415"
  },
  {
    "text": "Um, and so then after that, uh, OpenAI published Improving Language Understanding With Generative Pre-training,",
    "start": "316415",
    "end": "325115"
  },
  {
    "text": "which is, uh, simply called GPT1. And so in this, they took a- just- just- just like a similarly large corpus,",
    "start": "325115",
    "end": "334345"
  },
  {
    "text": "about a billion words and they trained a very large language model. So a 12 layer language model,",
    "start": "334345",
    "end": "339470"
  },
  {
    "text": "which at the time was maybe I- I don't know whether it was actually a large language model they'd been training at the time. It certainly was the largest thing they all had been training on that much data",
    "start": "339470",
    "end": "345950"
  },
  {
    "text": "for a- a kind of open-source, um, model. And when I first read it, I actually thought that it was like too big.",
    "start": "345950",
    "end": "352380"
  },
  {
    "text": "Like not- not that it was worse, but that they were kinda just showing up and showing how big of a model they could train. But now we know that actually this- this- this depth that",
    "start": "352380",
    "end": "359120"
  },
  {
    "text": "they had was actually kinda the crucial- the crucial element. So they- they did something that was like fairly simple, right? They just trained a language model, a very large one,",
    "start": "359120",
    "end": "365920"
  },
  {
    "text": "and then they just fine tuned it by taking the last token and then fine tuning it for a classification task, right?",
    "start": "365920",
    "end": "371120"
  },
  {
    "text": "So is this positive or negative? And they got basically state-of-the-art on lots of different classification tasks.",
    "start": "371120",
    "end": "377629"
  },
  {
    "text": "Um, but [NOISE] I'm going to- I'm going to actually take a, uh, kind of an aside here before I go into BERT, um,",
    "start": "377630",
    "end": "384680"
  },
  {
    "text": "which is about Transformer, because that was the other, uh, kind of big thing like the- the big precursor that,",
    "start": "384680",
    "end": "390210"
  },
  {
    "text": "uh, allowed BERT and GPT to work well, right? So, um, [BACKGROUND] BERT and GPT both use a transformer,",
    "start": "390210",
    "end": "397280"
  },
  {
    "text": "which I'm sure you guys have learned about. And so, uh, I- I don't need to really go into all the details about it. But, um, so it has, you know,",
    "start": "397280",
    "end": "406550"
  },
  {
    "text": "multi-headed attention feedforward layers layering on- I- I won't go into all the details because- because I think you've already learned about it. But- so the- the big thing about why this kinda took over is,",
    "start": "406550",
    "end": "416180"
  },
  {
    "text": "uh, there is really two advantages versus the LSTM. One is that there's no locality bias. And so, um, long distance context has",
    "start": "416180",
    "end": "424610"
  },
  {
    "text": "a equal opportunity to short distance context, which is important. So for like normal language understanding,",
    "start": "424610",
    "end": "430865"
  },
  {
    "text": "that the- the locality bias of LSTM is generally considered to be a good thing,",
    "start": "430865",
    "end": "436185"
  },
  {
    "text": "um, because local context is more relevant than long-distance contexts.",
    "start": "436185",
    "end": "442235"
  },
  {
    "text": "But the way that GPT and BERT and other models work is that they actually concatenate, uh, context.",
    "start": "442235",
    "end": "448305"
  },
  {
    "text": "And so if you have a, uh, like a model that says, does this sentence- does sentence one entail sentence two?",
    "start": "448305",
    "end": "454315"
  },
  {
    "text": "The way that it was done historically, meaning like before GPT was that you would like encode them both,",
    "start": "454315",
    "end": "459620"
  },
  {
    "text": "let's say with an LSTM, then you would do attention from one to the other. With a transformer, you can just put them into the same sequence and,",
    "start": "459620",
    "end": "468320"
  },
  {
    "text": "uh, give them separate sequence embeddings or add a separator token. And it will learn how to, uh,",
    "start": "468320",
    "end": "473855"
  },
  {
    "text": "and then it can, it can attend to the- to its own sentence locally. But it can also attend all the way to the other sentence, uh,",
    "start": "473855",
    "end": "479550"
  },
  {
    "text": "for almost- for- for no- it's just as easy for it to attend all the way to the other sentence.",
    "start": "479550",
    "end": "484615"
  },
  {
    "text": "And so when you do this, kind of you can just pack everything to a single sequence and then, uh, everything will be learned rather than- rather than",
    "start": "484615",
    "end": "490310"
  },
  {
    "text": "having to do with this part of the model- model architecture, which is- ends up being a pretty important, uh, thing about simplifying these models.",
    "start": "490310",
    "end": "497044"
  },
  {
    "text": "And so the other thing is, uh, that having a, with transformers- with LSTMs,",
    "start": "497045",
    "end": "505065"
  },
  {
    "text": "let's say this- this is a batch and these are the- the words in the batch. So we have- we have two sentences and four words per sentence. Every step has to be computed,",
    "start": "505065",
    "end": "511170"
  },
  {
    "text": "um, one at a time. So you only get a batch size of- of two effectively. And so on modern hardware which is TPUs and GPUs,",
    "start": "511170",
    "end": "519334"
  },
  {
    "text": "the bigger the matrix multiplication, the better it is. And you- you want all three dimensions to be big. So even if you have big hidden layers, um,",
    "start": "519335",
    "end": "525490"
  },
  {
    "text": "your batch size dimension will still be small unless you have a huge batch, but then that's too expensive for long sequences.",
    "start": "525490",
    "end": "531154"
  },
  {
    "text": "But with transformers, uh, it's the tot- because it's- it's layer wise attention. Um, the total number,",
    "start": "531155",
    "end": "537635"
  },
  {
    "text": "the batch size is the total number of words. So if you have 500 words and then 32 sentences, it's actually 32 times 512 is the total batch size.",
    "start": "537635",
    "end": "545060"
  },
  {
    "text": "So you get these huge matrix multiplications and you can take advantage of modern hardware. Uh, and so that's kind of why the transformer has taken over because of these two things.",
    "start": "545060",
    "end": "553834"
  },
  {
    "text": "And that's why it was used in GPT and why it's used in BERT. So, uh, now I'm gonna talk about BERT.",
    "start": "553835",
    "end": "559339"
  },
  {
    "text": "So the problem with the previous models,",
    "start": "559340",
    "end": "565380"
  },
  {
    "text": "being ELMo and GPT and, um, those before it, is that the language models only used left context or right context,",
    "start": "565380",
    "end": "573175"
  },
  {
    "text": "or- or a concatenation of both. But really, um, the language understanding is bidirectional, right?",
    "start": "573175",
    "end": "579149"
  },
  {
    "text": "So there's this clear, uh, kind of mismatch between why is- why did everyone train on unidirectional models where you can only",
    "start": "579150",
    "end": "586520"
  },
  {
    "text": "see to the left or only see to the right when really we care- when we know that in order to understand language, you need to look in both directions, right?",
    "start": "586520",
    "end": "592820"
  },
  {
    "text": "So there's two reasons. So one is that, language models historically had been used for,",
    "start": "592820",
    "end": "599000"
  },
  {
    "text": "uh, typically as features in other systems. So like the most direct application of language modeling would be like predictive text, right?",
    "start": "599000",
    "end": "605210"
  },
  {
    "text": "Which is directly just saying predict the next word. The other- the other applications that are actually more common are to use them in a machine translation system or a speech recognition system,",
    "start": "605210",
    "end": "612695"
  },
  {
    "text": "where you have these features like translation features or- or acoustic features. And then you add a language model that says,",
    "start": "612695",
    "end": "619130"
  },
  {
    "text": "what's the probability of the sentence? And so for this, you want it to be a well-formed distribution. For these pre-trained models,",
    "start": "619130",
    "end": "624140"
  },
  {
    "text": "we actually don't care about this. But this was kind of something that was, uh, kinda, people had just been like, uh, kind of,",
    "start": "624140",
    "end": "630214"
  },
  {
    "text": "I guess, fixed on this idea that language models have to- have to have a distribution or probability distribution even though we actually don't care about that.",
    "start": "630215",
    "end": "635975"
  },
  {
    "text": "But the other kind of bigger reason is that words can see themselves in a bidirectional encoder. Um, and so what this means is when you build a representation incrementally. [NOISE]",
    "start": "635975",
    "end": "648185"
  },
  {
    "text": "So you have your input and then you have your output, and it's always offset by 1. So we can- we have the- the- the startup sentence token,",
    "start": "648185",
    "end": "656680"
  },
  {
    "text": "we predict the first word, then we feed in the second word- we feed in the first word and predict the second word. And so we can encode the sentence once",
    "start": "656680",
    "end": "663010"
  },
  {
    "text": "and predict all the words in the sentence with the unidirectional model. And so this gives us good sample efficiency, right? Because if we have a 512 dimension,",
    "start": "663010",
    "end": "669355"
  },
  {
    "text": "like- like a sequence of 500 words, we don't wanna have to, uh, only predict one word because it's gonna be 500 times,",
    "start": "669355",
    "end": "675685"
  },
  {
    "text": "uh, as much, uh, compute to get the same amount of predictions. But if we were to just trivially do a bidirectional LSTM or transformer,",
    "start": "675685",
    "end": "683829"
  },
  {
    "text": "we would have a situation where you- you encode your sentence, every- everything is bidirectional. And so after the first layer,",
    "start": "683830",
    "end": "690385"
  },
  {
    "text": "everything can see itself. So- so- so this word open, there's a- there's a path back down to open. And so it's trivial to predict a word that can- that can,",
    "start": "690385",
    "end": "697705"
  },
  {
    "text": "where it's in the input also, right? There's no- there's no actual prediction going on there. So the simple solution,",
    "start": "697705",
    "end": "704560"
  },
  {
    "text": "which is basically the whole crux of BERT is that let's, instead of, um, training a normal language model,",
    "start": "704560",
    "end": "712045"
  },
  {
    "text": "let's just predict, mask out k percent of the words. So, uh, the man went to the mask to buy a mask of milk.",
    "start": "712045",
    "end": "719425"
  },
  {
    "text": "And so- and so now you can run a bidirectional model on that, and because the words aren't in the input,",
    "start": "719425",
    "end": "724590"
  },
  {
    "text": "you can't cheat, right? The downs- the- the- the- the- and so th- the downside of this",
    "start": "724590",
    "end": "729675"
  },
  {
    "text": "is that you're not getting as many predictions per sentence, right? You're only getting- predicting 15% of words instead of 100% of words.",
    "start": "729675",
    "end": "736855"
  },
  {
    "text": "But the upside is that you're- you're getting a much more rich model because you're seeing in both directions, right? So this value of k is a hyperparameter that we have to,",
    "start": "736855",
    "end": "743950"
  },
  {
    "text": "uh, just decide on empirically, so we use 15%. It turns out that that's actually kind of an optimal value people have,",
    "start": "743950",
    "end": "749875"
  },
  {
    "text": "so we and also people since then have done, um, more thorough ablation experiments, um, and found that this 15% is good.",
    "start": "749875",
    "end": "756400"
  },
  {
    "text": "So the reason- the reason for doing a certain percent over another is that if yo- if you were to do, let's say 50% masking,",
    "start": "756400",
    "end": "761620"
  },
  {
    "text": "you would get way more predictions but you would also mask out like all of your context. And so, um, you can,",
    "start": "761620",
    "end": "769465"
  },
  {
    "text": "uh, if- if you mask out all of your context, and you're not getting any- you can't learn contextual models. And if you only do, like,",
    "start": "769465",
    "end": "774730"
  },
  {
    "text": "let's say you can mask out one word that would pro- that might be optimal, maybe, um, but you would have to do way more data processing.",
    "start": "774730",
    "end": "780790"
  },
  {
    "text": "So it'd be way more expensive to train, and we know that these models are basically just compute bounded. Um, so if you just have enough data, you can just kinda train",
    "start": "780790",
    "end": "787150"
  },
  {
    "text": "them infinitely and it'll always do better. So it's really just a trade-off between these two- these two things.",
    "start": "787150",
    "end": "792970"
  },
  {
    "text": "Um, so one other little detail in part, which turned out to be super important, [NOISE] is that because the mask token is never seen at fine-tuning time, uh,",
    "start": "792970",
    "end": "802450"
  },
  {
    "text": "instead of always, um, replacing a word with the mask token as in this case,",
    "start": "802450",
    "end": "807714"
  },
  {
    "text": "uh, we would randomly sometimes predict it with a random word and sometimes keep the same word. So like, uh, so 10% of the time,",
    "start": "807715",
    "end": "813880"
  },
  {
    "text": "we'd say, we went to the store and went to the running, right? And so we- we wouldn't tell the model which- which case was which.",
    "start": "813880",
    "end": "820930"
  },
  {
    "text": "Uh, we would- we would just have it- we would just have to- we wo- we wo- we would just say what- what should this word be, right?",
    "start": "820930",
    "end": "828760"
  },
  {
    "text": "And didn't know whether it's right or not. So it could be the same word. So because 10% of time it's the same word and it could be a random word. And so it has to, uh,",
    "start": "828760",
    "end": "834925"
  },
  {
    "text": "basically be able to maintain a good representation of every word because it is- it doesn't know whether it's really the right word.",
    "start": "834925",
    "end": "840730"
  },
  {
    "text": "So it has to actually look at every word and figure out whether this is the right word. So we could potentially even just get away with not using a  mask token at all and just",
    "start": "840730",
    "end": "845830"
  },
  {
    "text": "doing like this 50% of the time and this 50% of the time. But the reason for not doing that is that, uh, you know,",
    "start": "845830",
    "end": "851394"
  },
  {
    "text": "then we'd be corrupting a lot of our data and we don't want it to necessarily corrupt the data because the fact that this is the wrong word,",
    "start": "851395",
    "end": "857005"
  },
  {
    "text": "we might mess up our predictions for some other word over here, right? So whereas the mask token at least it knows that it's not the right word,",
    "start": "857005",
    "end": "862480"
  },
  {
    "text": "so it doesn't- it doesn't use that as part of its context. Um, so the other- the other kind of, uh, detail of BERT,",
    "start": "862480",
    "end": "870670"
  },
  {
    "text": "which also now in subsequently may not be- been that important, is that a lot of these tasks that we're doing,",
    "start": "870670",
    "end": "877690"
  },
  {
    "text": "we're not just learning words, we're lo- we- we're- want to predict the relationship between sentences.",
    "start": "877690",
    "end": "882700"
  },
  {
    "text": "So for question answering in particular, we have a- a query which is a- yeah, generally a sentence, and then we have a, uh,",
    "start": "882700",
    "end": "888745"
  },
  {
    "text": "answer which is a paragraph or- or a sentence or a document, and we want to- um,",
    "start": "888745",
    "end": "893920"
  },
  {
    "text": "you know, say, does this answer the question? So by doing that, we, um,",
    "start": "893920",
    "end": "899365"
  },
  {
    "text": "so we want to have some pertaining task that actually does a sentence level,",
    "start": "899365",
    "end": "904570"
  },
  {
    "text": "uh, prediction rather than just a word level prediction. So- so the way that we did this, which wa- and we needed this to have like an infinite amount of data, right?",
    "start": "904570",
    "end": "909759"
  },
  {
    "text": "Or we can just generate an infinite amount of data. So we do- we don't want this to be an annotated task. Um, so the way that we did this is we, uh,",
    "start": "909760",
    "end": "917910"
  },
  {
    "text": "just did a next sense prediction task where we just took two sentences from the same corpus,",
    "start": "917910",
    "end": "924175"
  },
  {
    "text": "uh, from the same document, and- and 50% of the time they're from the same document, 50% of the time, they're from a random document. And then we just said, was this the real next sentence, uh, or not?",
    "start": "924175",
    "end": "933264"
  },
  {
    "text": "And so if you just have, like, \"The man went to the store. He bought a gallon of milk.\" That is the next sentence. If you said, \"The man went to the store.",
    "start": "933265",
    "end": "938800"
  },
  {
    "text": "Penguins are flightless.\" That's not the next sentence. So basically, now we're forcing the model at pre-training time to actually make- to look",
    "start": "938800",
    "end": "945070"
  },
  {
    "text": "at the full sentences and then make some sort of sentence level prediction and we hope that this, uh, is kind of generalizedw.",
    "start": "945070",
    "end": "952950"
  },
  {
    "text": "So it's just something like question-answering, where you have a question and answer as Sentence A, Sentence B.",
    "start": "952950",
    "end": "958180"
  },
  {
    "text": "Uh, so in terms of our input representation, it looks pretty similar to a normal transformer.",
    "start": "958280",
    "end": "966700"
  },
  {
    "text": "But we have these additional embeddings, which are called segment embeddings. So a normal transformer,",
    "start": "966700",
    "end": "973209"
  },
  {
    "text": "you would have your input and then you would do WordPiece segmentation, right? Where you split up.",
    "start": "973209",
    "end": "978279"
  },
  {
    "text": "Where you apply this unsupervised, uh, splitting of words into, um, kind of, um,",
    "start": "978280",
    "end": "985300"
  },
  {
    "text": "morphological splits but they're usually often not morphological, sort of unsupervised. But you end up with something tha- that's roughly morphological, right?",
    "start": "985300",
    "end": "991780"
  },
  {
    "text": "And so now you have like no- no added vocabulary tokens. Everything is represented, a- at least at the- at the very least,",
    "start": "991780",
    "end": "997420"
  },
  {
    "text": "you always split into characters, uh, so we use a 30,000 word vocabulary. And then we have our token embeddings.",
    "start": "997420",
    "end": "1004545"
  },
  {
    "text": "Then we have our- our normal position embeddings, which is the bottom and so these are part of the transformer where- because transformers,",
    "start": "1004545",
    "end": "1010290"
  },
  {
    "text": "unlike LSTMs, don't have any sort of, um, locational, uh, awareness.",
    "start": "1010290",
    "end": "1015540"
  },
  {
    "text": "So the only- the way to encode that is that you encode a actual embedding for every position.",
    "start": "1015540",
    "end": "1020910"
  },
  {
    "text": "So this is called absolute position embeddings. There's other kind of techniques nowadays. Uh, and so- and then- then you have the segment embedding,",
    "start": "1020910",
    "end": "1028079"
  },
  {
    "text": "which is, this is Sentence A or Sentence B. And so this kind of generalizes in more, uh, general context.",
    "start": "1028080",
    "end": "1034290"
  },
  {
    "text": "So you can imagine if you're saying, we're trying to say, like, you're trying to do like web search. You might say, here's my query, here's the- the title,",
    "start": "1034290",
    "end": "1040530"
  },
  {
    "text": "here's the URL, here's the document content. Um, and so you can kind of just",
    "start": "1040530",
    "end": "1045540"
  },
  {
    "text": "pack these all into a single sequence and then just give them different, uh, segment embeddings or type embeddings.",
    "start": "1045540",
    "end": "1050655"
  },
  {
    "text": "So that now you get, uh, are able to have, um, a much stronger- um,",
    "start": "1050655",
    "end": "1056985"
  },
  {
    "text": "you're- you're able to kind of just represent everything, er, in this kind of same single sequence where you kind of just differentiated by just this single embedding that's different.",
    "start": "1056985",
    "end": "1064455"
  },
  {
    "text": "And this is all of course learned. Um, and so this is in contrast to kinda the older style where you would typically have a different encoder for every part.",
    "start": "1064455",
    "end": "1071610"
  },
  {
    "text": "So like you would have a different coder for the query and then maybe for the title and maybe for the URL, um, but this case it's all just a single sequence.",
    "start": "1071610",
    "end": "1077340"
  },
  {
    "text": "So we trained on about a 3 billion word corpus, which was at the time large.",
    "start": "1077340",
    "end": "1082440"
  },
  {
    "text": "Now it's not actually that big compared to what people are training on. We used a- a batch size,",
    "start": "1082440",
    "end": "1089190"
  },
  {
    "text": "uh, which was also large. Um, we trained for about 40 epochs of the data.",
    "start": "1089190",
    "end": "1094545"
  },
  {
    "text": "And we trained these two models, which are still relatively large. Um, so one of them is a 12 layer 768,",
    "start": "1094545",
    "end": "1100950"
  },
  {
    "text": "and then the other one, 24 layer 1024. So at the time this was basically like, one of the largest models that had been trained.",
    "start": "1100950",
    "end": "1106304"
  },
  {
    "text": "Although now people are training models that I think are, uh, 30 times or more bigger than this,",
    "start": "1106305",
    "end": "1111540"
  },
  {
    "text": "um in- in the more recent papers. So things have kind of exploded in terms of, uh, compu- com- computing in the last, I don't know, three years.",
    "start": "1111540",
    "end": "1118215"
  },
  {
    "text": "Um, but, uh, yeah, so the fine-tuning procedure is,",
    "start": "1118215",
    "end": "1124920"
  },
  {
    "text": "uh, it- it's pretty straightforward, right? So we- we pre-trained this model for these two tasks.",
    "start": "1124920",
    "end": "1130710"
  },
  {
    "text": "And so now we have an input sequence which has  multiple sentences with different type embeddings. We feed them through the- uh, our transformer model.",
    "start": "1130710",
    "end": "1139740"
  },
  {
    "text": "And now we have the special embedding, which I- I think I've been- didn't mention. So this- this special embedding.",
    "start": "1139740",
    "end": "1144885"
  },
  {
    "text": "This is- this is basically, it's learned to predict the- the next sentence prediction task and then this is used,",
    "start": "1144885",
    "end": "1152370"
  },
  {
    "text": "uh, also for a classification task. But this- it's not just that we're using the embedding, right? We are- we're fine tuning the entire model, right?",
    "start": "1152370",
    "end": "1158970"
  },
  {
    "text": "So- so this- so it's- it's really not that this embedding is intrinsically useful or that the word embeddings are intrinsically useful. It's that the- the- the- weights inside the entire 12 or 24 layer model are useful.",
    "start": "1158970",
    "end": "1169350"
  },
  {
    "text": "And so by fine-tuning the entire model, you can kind of pick out the salient parts that are- that are important for some downstream task.",
    "start": "1169350",
    "end": "1176325"
  },
  {
    "text": "Um, and so this is- this is the- kind of the- the- the class-specific fine-tuning, right?",
    "start": "1176325",
    "end": "1182450"
  },
  {
    "text": "So if we had a single classification task, uh, like, let's say sentence analysis,",
    "start": "1182450",
    "end": "1188594"
  },
  {
    "text": "um, where he says say, is it a positive or negative review? We- we- we encode our sentence with the- the BERT model.",
    "start": "1188594",
    "end": "1194924"
  },
  {
    "text": "Then the only parameters that we add are this final output matrix, right? So maybe if we have 3, let's say, positive,",
    "start": "1194925",
    "end": "1200850"
  },
  {
    "text": "negative or neutral, this might be 1,000 times 3, right? So it's just 3,000 parameters, and 300 million.",
    "start": "1200850",
    "end": "1207150"
  },
  {
    "text": "So a- a- 3,000 new parameters and 300 million old parameters and we- and we jointly train all 300 million plus 3,000,",
    "start": "1207150",
    "end": "1215680"
  },
  {
    "text": "uh, fo- fo- for this downstream task. But because the vast majority of them are pre-trained, we can kind of adapt to it in only like a few thousand, labeled examples.",
    "start": "1215680",
    "end": "1224150"
  },
  {
    "text": "And similarly for a sentence pair class, we do, um, we just concatenate the two sentences with different type embeddings.",
    "start": "1224150",
    "end": "1230025"
  },
  {
    "text": "So we have, if you want to say does this sentence entail this other sentence, you say sentence A, uh, you put it, concatenate sentence B,",
    "start": "1230025",
    "end": "1237029"
  },
  {
    "text": "and then also predict from- from this token and fine tune the entire thing. So similarly, very few additional parameters.",
    "start": "1237030",
    "end": "1242684"
  },
  {
    "text": "Uh, for span prediction tasks, you just have kinda a start of- a start of span, end of span,",
    "start": "1242685",
    "end": "1249169"
  },
  {
    "text": "so you're only adding a few thousand new parameters. And then for tagging tasks like part-of-speech tagging, you just,",
    "start": "1249170",
    "end": "1255674"
  },
  {
    "text": "um, have a single, uh, sentence. You- you have a- you have- you add ev- every single",
    "start": "1255675",
    "end": "1262740"
  },
  {
    "text": "token or maybe every token except for the- the- the WordPieces. But like, [NOISE] you- you- the it's kind of preprocessing.",
    "start": "1262740",
    "end": "1267779"
  },
  {
    "text": "You- you predict, um, what's the part of speech of this. And so this is really why, uh.",
    "start": "1267779",
    "end": "1275070"
  },
  {
    "text": "So like BERT itself is really a- a kind of, I would say an incremental improvement over what already just,",
    "start": "1275070",
    "end": "1280590"
  },
  {
    "text": "so it kind of took, uh, transformers, uh, ELMo, GPT, really these three ideas and kinda made a- a pretty simple,",
    "start": "1280590",
    "end": "1290445"
  },
  {
    "text": "uh, change on top of them. And but the reason why it had such big impact is not just the numbers that I'll show in a few slides.",
    "start": "1290445",
    "end": "1298500"
  },
  {
    "text": "It's really this- this thing, because, um, with Elmo,",
    "start": "1298500",
    "end": "1305595"
  },
  {
    "text": "there was really no fundamental difference between, it was just contextual embedding, right? You still had, because like a lot of deep learning historically",
    "start": "1305595",
    "end": "1312390"
  },
  {
    "text": "has been fun and building new models, right? So you have all of these components that are kinda like Lego blocks, right?",
    "start": "1312390",
    "end": "1318250"
  },
  {
    "text": "You have attention layers, feed forward layers, layered normalization, uh, LSTMs, uh, et cetera.",
    "start": "1318250",
    "end": "1324750"
  },
  {
    "text": "And you can just, kind of figure out to say okay, for this new task, how do I glue these together in a way that's best, right?",
    "start": "1324750",
    "end": "1332580"
  },
  {
    "text": "And so, um, and so with ELMo, it wasn't really- it didn't really change anything fundamentally.",
    "start": "1332580",
    "end": "1339225"
  },
  {
    "text": "It was just- because this would have- if you just fed it into your existing model and you got state of the art, for GPT1, it wasn't most- like these things didn't really work, right?",
    "start": "1339225",
    "end": "1349769"
  },
  {
    "text": "Because it was a left to right language model. And so you can just kind of take the last token, and then predict classification task,",
    "start": "1349770",
    "end": "1355515"
  },
  {
    "text": "but it didn't really make any sense to predict like part-of-speech tags because for the first word there's no context,",
    "start": "1355515",
    "end": "1361200"
  },
  {
    "text": "so it makes no sense to predict where there is no context. Um, with BERT, the reason why it had such high impact,",
    "start": "1361200",
    "end": "1368909"
  },
  {
    "text": "was because it kind of simplified things. And so that's not- I'm not saying that's necessarily a good thing",
    "start": "1368910",
    "end": "1374070"
  },
  {
    "text": "because as a researcher or a bad thing, I'm- so as a researcher kind of ironically,",
    "start": "1374070",
    "end": "1380355"
  },
  {
    "text": "the ultimate goal of research is often like research yourself out of a job, right? Like, it's like, you know, if a- if a physicist,",
    "start": "1380355",
    "end": "1385470"
  },
  {
    "text": "I'm not saying BERT had anyone near this- this impact, like a physicist that came up with a grand theory of physics. That would kinda like- they would be like the- the greatest moment in physics,",
    "start": "1385470",
    "end": "1393210"
  },
  {
    "text": "but also that would kind of, uh, eliminate a lot of research, right? And so that's kind of like the- the end goal of research is kind of solve the problem, right?",
    "start": "1393210",
    "end": "1400320"
  },
  {
    "text": "So- so BERT, um, kind of has- is- is a step where now,",
    "start": "1400320",
    "end": "1408315"
  },
  {
    "text": "like all of these different concepts and problems, there's really- it kind of killed like a lot of the need to do,",
    "start": "1408315",
    "end": "1413715"
  },
  {
    "text": "um, model architecture design, which is kind of unfortunate because that's like really fun. Um, and so that's kind of the- the impact.",
    "start": "1413715",
    "end": "1422220"
  },
  {
    "text": "I'm so- I'm not gonna say whether that's a good or bad impact. It's kinda like the objective impact. And that's why it's had so much impact is because it has kind of, uh,",
    "start": "1422220",
    "end": "1429360"
  },
  {
    "text": "had this effect on now, so many things that used to be like designing fun, you know, models.",
    "start": "1429360",
    "end": "1435225"
  },
  {
    "text": "It's just fit it in and use one of these four recipes. And it kinda just works for all of these different tasks.",
    "start": "1435225",
    "end": "1440835"
  },
  {
    "text": "Um so in terms of actual empirical results, so these- these are at the time the paper was published. Of course, things have gotten better since then.",
    "start": "1440835",
    "end": "1449100"
  },
  {
    "text": "So these- these- this GLUE task is a set of, um, they're all kind of similar in that they're",
    "start": "1449100",
    "end": "1454320"
  },
  {
    "text": "all sentences pair or sentence classification tasks. So like MultiNLI would be something like hills and mountains are especially sanctified in Jainism.",
    "start": "1454320",
    "end": "1462450"
  },
  {
    "text": "And then hypothesis is Jainism hates nature. That's a contradiction, right? So in order for an NLP model to be able to understand this,",
    "start": "1462450",
    "end": "1469275"
  },
  {
    "text": "or to be able to answer this correctly and give us the label of contradiction, it needs to know that hills and mountains are part of nature, sanctified is a good thing.",
    "start": "1469275",
    "end": "1479910"
  },
  {
    "text": "And that hating something is a bad thing and be able to do all of this reasoning, right? So it's pretty complicated reasoning. Similarly for CoLa, you have to be able to say like",
    "start": "1479910",
    "end": "1486645"
  },
  {
    "text": "the wagon rumbled down the road versus the car honked down the road. And so, um, these things are,",
    "start": "1486645",
    "end": "1494505"
  },
  {
    "text": "ah, you know, one of them to a- to a native English speaker sounds totally fine, the other one sounds weird. And so that, that similarly, and neither of these  have very much data,  right, so you have to be able to",
    "start": "1494505",
    "end": "1502110"
  },
  {
    "text": "generalize on- on only, ah, like a few thousand examples. Um, so BERT base,",
    "start": "1502110",
    "end": "1509220"
  },
  {
    "text": "which is the same size as Open AI, uh, it- it significantly beat OpenAI, uh, which was the previous state of the art.",
    "start": "1509220",
    "end": "1515790"
  },
  {
    "text": "And then, uh, the BERT large which was bigger,",
    "start": "1515790",
    "end": "1520860"
  },
  {
    "text": "of course got better results. Which is only surprising that it got better results across the board, including on the very,",
    "start": "1520860",
    "end": "1526440"
  },
  {
    "text": "very tiny datasets, it's only a few thousand examples. That was kind of- that was kind of more interesting result rather than just the fact that, uh, it got better results.",
    "start": "1526440",
    "end": "1532679"
  },
  {
    "text": "Because historically when, uh, there was, you know, rules of thumb about if you have some number of examples,",
    "start": "1532680",
    "end": "1539475"
  },
  {
    "text": "how do you design the model size that's optimal for that? And so if you don't do pre-training, like if- if you keep making the model bigger without pre-training,",
    "start": "1539475",
    "end": "1546720"
  },
  {
    "text": "eventually you'll get worse results because your model will overfit your training data. With pre-training you basically only ever do like one pass over the data anyways.",
    "start": "1546720",
    "end": "1554205"
  },
  {
    "text": "So there- there seems to be almost no limit to how big you can make it and still get good results even with a tiny amount of fine tuning data.",
    "start": "1554205",
    "end": "1560460"
  },
  {
    "text": "And that's really like one of the- one of the big takeaways. So I'm not going- yeah so, uh,",
    "start": "1560460",
    "end": "1568860"
  },
  {
    "text": "SQuAD 2.0- so the reason why these- these numbers are- these ranges are lower because I took the screenshot,",
    "start": "1568860",
    "end": "1574590"
  },
  {
    "text": "uh, after, like, these- these, you know, significantly after, ah, when a bunch of other people had submitted systems.",
    "start": "1574590",
    "end": "1581580"
  },
  {
    "text": "But, um, so this is a question answering data set, so it would be like, what action did the US take that started the second oil shock?",
    "start": "1581580",
    "end": "1588720"
  },
  {
    "text": "So in this case, there is no answer, right? So something you have to be able to predict is that there's no answer in this phase. So you have to be able to either predict the answer or say there's no answer.",
    "start": "1588720",
    "end": "1595980"
  },
  {
    "text": "So BERT beat the- BERT beat the previous state of the art, but at the time that it was submitted by about six points,",
    "start": "1595980",
    "end": "1603029"
  },
  {
    "text": "which was, you know, a pretty big gain. Now it's kind of gone past human level. But at the time, yeah it was- it was large.",
    "start": "1603030",
    "end": "1609960"
  },
  {
    "text": "So I'll kind of do some ablation experiments or go through some ablation experiments. So this one, there's four things that I'm comparing here.",
    "start": "1609960",
    "end": "1617550"
  },
  {
    "text": "So this is all- all BERT based sized models. So those are- the blue line is kinda the- the BERT based model.",
    "start": "1617550",
    "end": "1624585"
  },
  {
    "text": "Um, the red line is if we take up the next sentence prediction. So in our case, even though people have",
    "start": "1624585",
    "end": "1630450"
  },
  {
    "text": "subsequently said that they don't think it's important, in our case, we actually did measure it, and it turns out it seemed like it was important to have this next sentence prediction task,",
    "start": "1630450",
    "end": "1638414"
  },
  {
    "text": "especially for, um, kind of question answering task, which is this one.",
    "start": "1638415",
    "end": "1645000"
  },
  {
    "text": "But it- it seems to help a little bit at least in all four of them to- to have it. So this kinda doesn't- there's some thinking, learning some, uh,",
    "start": "1645000",
    "end": "1651810"
  },
  {
    "text": "model that learns, uh, joint- that learns the relationships between sentences.",
    "start": "1651810",
    "end": "1657165"
  },
  {
    "text": "So then this one is the one that makes an apples to apples comparison between OpenAI,",
    "start": "1657165",
    "end": "1664410"
  },
  {
    "text": "and OpenAI is GPT 1, and Bert, right? Because I also- I made the model bigger but not- not BERT based.",
    "start": "1664410",
    "end": "1670890"
  },
  {
    "text": "So BERT base was the exact same size, but it was trained on more data. So to make it a fair comparison, I basically retrained my implementation of OpenAI's GPT 1 and I,",
    "start": "1670890",
    "end": "1681195"
  },
  {
    "text": "ah, which is- which is this yellow line. And we can see that on- on some of the tasks it's not that far,",
    "start": "1681195",
    "end": "1687270"
  },
  {
    "text": "although this is actually a pretty big gap, like this- this drop is, you know, like four points which is- which is a lot.",
    "start": "1687270",
    "end": "1692400"
  },
  {
    "text": "But on- on some tests like SQuAD, and- and MRPC, it was- it was way worse. Um, and so, uh,",
    "start": "1692400",
    "end": "1699315"
  },
  {
    "text": "for SQuAD to make sense because SQuAD is a labeling- is a span labeling task. And so if you only have left",
    "start": "1699315",
    "end": "1706290"
  },
  {
    "text": "context then words at the beginning have basically no context. And so you're- you're asking it to do span labeling on words with- with almost no context.",
    "start": "1706290",
    "end": "1713115"
  },
  {
    "text": "So it really doesn't make any sense. And so of course it's gonna do much worse. So then we also added a- to make it fair,",
    "start": "1713115",
    "end": "1718440"
  },
  {
    "text": "we also added an LSTM on top of it which was trained from scratch. And this does help a little bit on some of the tasks,",
    "start": "1718440",
    "end": "1724215"
  },
  {
    "text": "but on other ones, it doesn't help. So like on SQuAD it helps because now you have bidirectional context, but on MRPC, because it's a very small task,",
    "start": "1724215",
    "end": "1730500"
  },
  {
    "text": "it's only got 3,000 labeled examples. It doesn't help at all. So it- this just show that the kind of",
    "start": "1730500",
    "end": "1736680"
  },
  {
    "text": "the masked language model and the- and the next sentence prediction are both important, especially the masked language model.",
    "start": "1736680",
    "end": "1742539"
  },
  {
    "text": "So the other thing, uh, one of the other ablations is that when we- when we apply the, the,",
    "start": "1742970",
    "end": "1750195"
  },
  {
    "text": "the math language model, we're only predicting 15% of words in the sentence. But when you do a left to right language model,",
    "start": "1750195",
    "end": "1756495"
  },
  {
    "text": "you're predicting every single word conditioning on all the words to the left. So one question might be,",
    "start": "1756495",
    "end": "1761700"
  },
  {
    "text": "how much, does this make it take longer to converge, even though eventually we know that it converges at a much better point. If you have a limited training budget,",
    "start": "1761700",
    "end": "1768150"
  },
  {
    "text": "is it better to do a left to right model? And so, uh, we see that [NOISE] when you do this",
    "start": "1768150",
    "end": "1774360"
  },
  {
    "text": "masked language model, the bi-directionality starts to improve- like at the very, very beginning because you're doing so many more predictions,",
    "start": "1774360",
    "end": "1779445"
  },
  {
    "text": "it's true that, uh, the left-right model does do better at the very- at the- like for, at  epoch one.",
    "start": "1779445",
    "end": "1785775"
  },
  {
    "text": "But then very soon after because the bi-directionality is so important, uh, it starts to take over. And so it's- it's basically better from almost the start to do bi-directionality,",
    "start": "1785775",
    "end": "1793890"
  },
  {
    "text": "and then it- it takes longer to converge but the- but the overall convergence is of course much higher.",
    "start": "1793890",
    "end": "1799000"
  },
  {
    "text": "Um, and then finally, for- for, uh, this, uh, ablations, um,",
    "start": "1799550",
    "end": "1806880"
  },
  {
    "text": "we can see that going from a smaller model, which was 100 million to 300 million parameters helps a lot, which isn't surprising.",
    "start": "1806880",
    "end": "1814289"
  },
  {
    "text": "But the more surprising thing is that one of these curves, these aren't- these aren't comparable- uh, um.",
    "start": "1814290",
    "end": "1820830"
  },
  {
    "text": "You shouldn't compare the curves to each other. The point is to look at the curves as a function of, uh, the- the number of parameters.",
    "start": "1820830",
    "end": "1827220"
  },
  {
    "text": "And see that this one is- um, this one only has 3,000 labeled examples,",
    "start": "1827220",
    "end": "1833190"
  },
  {
    "text": "and this one has, uh, 400,000 labeled examples. So in both cases, the curves look very similar,",
    "start": "1833190",
    "end": "1838500"
  },
  {
    "text": "which is surprising because, you know, the rule of thumb that you're gonna overfit your data, if you only have a few num- a few la- labeled examples",
    "start": "1838500",
    "end": "1846510"
  },
  {
    "text": "turns out not to really be true anymore. And there's- um, you know, and these- and these curves keep going up, right?",
    "start": "1846510",
    "end": "1851820"
  },
  {
    "text": "So now with subsequent papers which we'll talk about, the- like, this- this big one was 300 million parameters. People have gone up to 11 billion parameters and still seeing similar behaviors.",
    "start": "1851820",
    "end": "1859919"
  },
  {
    "text": "Still seeing the curves go way up and gotten state of the art results, which is kind of crazy because now we know that, um, you know, basically that- there's al- almost no limit.",
    "start": "1859920",
    "end": "1868275"
  },
  {
    "text": "So another thing I wanna talk about before I talk about, uh, stuff that's happened since BERT,",
    "start": "1868275",
    "end": "1873795"
  },
  {
    "text": "um, is the kind of is-. Even though BERT, um, itself was in some ways very simple,",
    "start": "1873795",
    "end": "1881670"
  },
  {
    "text": "which is, you know, not a bad thing. Um, it wa- it was very successful immediately. And, you know, part- part of that is, like,",
    "start": "1881670",
    "end": "1887309"
  },
  {
    "text": "the Google brand, and like, you know, got a cute name, and stuff like that. But I think that I- I- I spent a lot of thought and time with the open source release, and in particular,",
    "start": "1887310",
    "end": "1892560"
  },
  {
    "text": "looking at other open source releases, and figuring out what people didn't like about those, um.",
    "start": "1892560",
    "end": "1898034"
  },
  {
    "text": "And so I think this is important, like, when you're- uh, [NOISE] whe- when your [inaudible] , even- even working industry as a- and trying to release something.",
    "start": "1898035",
    "end": "1904980"
  },
  {
    "text": "So, um, I kind of just listed the things here that I thought were, uh, important for, like, why- why it was,",
    "start": "1904980",
    "end": "1911985"
  },
  {
    "text": "uh, successful compared to other things. Um, so like, I'm not- I'm not trying to call them out just to be mean,",
    "start": "1911985",
    "end": "1917250"
  },
  {
    "text": "because the- the- but like the OpenAI GPT-1 release was really- uh, was not very good. And- and the- then, like, they're aware of this,",
    "start": "1917250",
    "end": "1922800"
  },
  {
    "text": "because the- the OpenAI GPT-2 release was very good. Um, and so, uh, yeah,",
    "start": "1922800",
    "end": "1928140"
  },
  {
    "text": "because it was- it was very hard to run in the- in the- it was not commented, the TensorFlow code was very- um, like, it worked fine, like, I replicated it.",
    "start": "1928140",
    "end": "1934274"
  },
  {
    "text": "But, like, the- the TensorFlow code was very non-idiomatic. It used all sorts of weird stuff. The Python code was weird, there was no comments,",
    "start": "1934275",
    "end": "1940440"
  },
  {
    "text": "there was basically no instructions. So like- um, and then other codebases also are- uh, are kind of too big.",
    "start": "1940440",
    "end": "1946230"
  },
  {
    "text": "It's like people just want it- like, say like, we wanna have one unified codebase for our entire, you know, language team?",
    "start": "1946230",
    "end": "1952095"
  },
  {
    "text": "And so, they just put this stuff up as part of that, and people don't really like that either. So I was very insistent that we do a minimal release.",
    "start": "1952095",
    "end": "1957240"
  },
  {
    "text": "So, like, this, we're just gonna release BERT, it's not gonna be part of anything. There's not going to be any external dependencies, um, and it's gonna be, like, very well commented.",
    "start": "1957240",
    "end": "1964035"
  },
  {
    "text": "I think that people- and it was kind of also easy to drop in, just the modeling part, and just the tokenization part, and- and just the- uh,",
    "start": "1964035",
    "end": "1970020"
  },
  {
    "text": "the front end which runs like the training loop, and kind of separate all these out, because, um, that way- and so-.",
    "start": "1970020",
    "end": "1975945"
  },
  {
    "text": "I think because of that, um, people kind of started using it much quicker, um. And of course, like, all the publicity helped.",
    "start": "1975945",
    "end": "1982155"
  },
  {
    "text": "But, uh, I- I think that, uh, you know, it could have easily been not as successful if- if it had been,",
    "start": "1982155",
    "end": "1988005"
  },
  {
    "text": "you know, done in a different way, so, uh. It's just kind of advice. So, um, yeah.",
    "start": "1988005",
    "end": "1994695"
  },
  {
    "text": "So now I'm gonna talk about five models that have come out since BERT that have all improved on top of BERT in various ways.",
    "start": "1994695",
    "end": "2001925"
  },
  {
    "text": "There's been more than five, but I'm going to highlight these five. I think that they're, um, interesting. Uh, a lot of them didn't come from Google,",
    "start": "2001925",
    "end": "2007970"
  },
  {
    "text": "but it's not because, necessa- well, a lot of them involved Google. I would say many of them actually were not- they- they were, uh,",
    "start": "2007970",
    "end": "2015935"
  },
  {
    "text": "interns at Google from various universities, who were supervised by Google researchers and also used Google Compute.",
    "start": "2015935",
    "end": "2021785"
  },
  {
    "text": "I mean, the reason why a lot of them came from Google is because, like, frankly, like, other than Facebook, Google, and Microsoft,",
    "start": "2021785",
    "end": "2027034"
  },
  {
    "text": "there's not really many, uh, like, people that can- companies that have the- the resources to train these huge state of the art models.",
    "start": "2027035",
    "end": "2034070"
  },
  {
    "text": "And so almost by necessity it's gonna- it's gonna come from one of these- um, one of these labs.",
    "start": "2034070",
    "end": "2040985"
  },
  {
    "text": "So the first one was RoBERTa. And so this is probably the one that- that had like the least, uh, uh,",
    "start": "2040985",
    "end": "2047000"
  },
  {
    "text": "kind of new stuff, it was- it's really just- and it's just-. And so this was University of Washington and Facebook. It came out not that long after BERT.",
    "start": "2047000",
    "end": "2053659"
  },
  {
    "text": "And so, what they showed was that BERT was really under trained. And so, basically they took- even on",
    "start": "2053660",
    "end": "2060500"
  },
  {
    "text": "the same amount of data which- which was- even though I did 40 epochs on the data, if you do it for, like, 200 epochs,",
    "start": "2060500",
    "end": "2066080"
  },
  {
    "text": "you get even better results, like, significantly. Um, so they- uh, so basically, they trained more- more epochs on the same da- data.",
    "start": "2066080",
    "end": "2073610"
  },
  {
    "text": "And they also showed that more data helps, which is also not super surprising. Uh, and they did improved masking and pre-training using a couple of- a couple of tweaks to that,",
    "start": "2073610",
    "end": "2080284"
  },
  {
    "text": "and they- and they were able to get, uh, state of the art results, which is cool. And so, yeah, but that was- that was a pretty straightforward paper, um.",
    "start": "2080285",
    "end": "2088950"
  },
  {
    "text": "So the next one is XLNet, which was done by, uh, some interns at CMU when they were at Google Brain.",
    "start": "2088950",
    "end": "2095589"
  },
  {
    "text": "And so this actually had some- some really cool, uh, uh, changes. So one of them was- they used this Transformer-XL,",
    "start": "2095590",
    "end": "2103430"
  },
  {
    "text": "which was actually the precursor done by the same people that, uh, were- they were just doing language modeling tasks instead of whpre-training.",
    "start": "2103430",
    "end": "2110220"
  },
  {
    "text": "But the big- one of the big, uh, innovations of Transformer-XL is this idea of relative position embeddings.",
    "start": "2110220",
    "end": "2116380"
  },
  {
    "text": "And so with absolute position embeddings, the problem is that every word gets,",
    "start": "2116380",
    "end": "2123290"
  },
  {
    "text": "like, this is word four, this is word five, this is word six. And so they are embedding, so they do generalize, but in- in practice there's a quadratic number of relationships,",
    "start": "2123290",
    "end": "2130700"
  },
  {
    "text": "like, how does word 83 relate to word 76, right? That's- that's- that's like- and then once you get bigger,",
    "start": "2130700",
    "end": "2136160"
  },
  {
    "text": "like, 500 or 1,000, now you have 1,000 squared total relationships, like, you have to say, how does word 997 relate to whatever, right?",
    "start": "2136160",
    "end": "2142099"
  },
  {
    "text": "And so that's obviously not optimal once you get to a large size. Um, and so with- with,",
    "start": "2142100",
    "end": "2149555"
  },
  {
    "text": "uh, relative position embeddings, you basically can say, uh, how much does dog attend to hot?",
    "start": "2149555",
    "end": "2157580"
  },
  {
    "text": "And how much shou- should the word dog attend to the previous word? And then you get- and these are nonlinear at firs-. So these are linear at first,",
    "start": "2157580",
    "end": "2163835"
  },
  {
    "text": "but then you combine them and then you get a nonlinear, uh, contextual representation. Then you do this in many- many layers,",
    "start": "2163835",
    "end": "2169235"
  },
  {
    "text": "and this ends up being- so then you say, how much does this contextual representation of dog? How much should that attend to the previous word? Uh, and then you kind of get- can build up.",
    "start": "2169235",
    "end": "2176150"
  },
  {
    "text": "And so this generalizes much better for long sequences. So it's a cool, uh, innovation. And then the other one, which is specific",
    "start": "2176150",
    "end": "2182780"
  },
  {
    "text": "to pre-training and not just the- the model itself, is the idea of permutation language modeling. So this is a little bit hard to explain.",
    "start": "2182780",
    "end": "2189899"
  },
  {
    "text": "I think the paper, uh, explained it very formally, I guess. And so- um, but basically,",
    "start": "2190060",
    "end": "2197315"
  },
  {
    "text": "there's a trick where- so in the left to right language model, every words done predicting is- is based on the word to the left, right?",
    "start": "2197315",
    "end": "2204275"
  },
  {
    "text": "But imagine that instead of- instead of predicting all- every word of the- you can basically take any permutations. So it's like I'm gonna predict the first word,",
    "start": "2204275",
    "end": "2210890"
  },
  {
    "text": "then I'm gonna pick the- the third word, then the second word, then the fourth word. And so, um, that's a totally valid way",
    "start": "2210890",
    "end": "2218000"
  },
  {
    "text": "and you still get a well-formed probability distribution because it's still predicting one word at a time, given some permutation of the input.",
    "start": "2218000",
    "end": "2223625"
  },
  {
    "text": "And with transformers and with attention, you can actually do this very efficiently, just by masking out your attention probabilities.",
    "start": "2223625",
    "end": "2229115"
  },
  {
    "text": "And so, every single sentence you have, you can kind of sample a single permutation of this.",
    "start": "2229115",
    "end": "2235730"
  },
  {
    "text": "And you can- uh, now- now you can effectively train a bidirectional model, because of this word.",
    "start": "2235730",
    "end": "2241730"
  },
  {
    "text": "It won't be conditioned on every- still on average, every word will only be conditioned on half the words. But this word will be conditioned on- you know,",
    "start": "2241730",
    "end": "2248390"
  },
  {
    "text": "all these words to the left and all these words to the right, and maybe it'll be missing these words, but that's fine. And so you get much better sample efficiency.",
    "start": "2248390",
    "end": "2255319"
  },
  {
    "text": "So I thought this was- this was a really clever idea, uh. And so- and this was kinda like the- the main innovation of XLNet.",
    "start": "2255320",
    "end": "2260855"
  },
  {
    "text": "And so, um, yeah, they basically get better sample efficiency because they're able to- uh, to do this random permutation and- and kind of take advantage of this.",
    "start": "2260855",
    "end": "2270020"
  },
  {
    "text": "So this won't work with LSTMs because- uh, because of this ordering, but because the way that masking is done in transformers,",
    "start": "2270020",
    "end": "2276950"
  },
  {
    "text": "it's just- um, it's just a- it's just a mask on the attention. So it actually ends up working very well.",
    "start": "2276950",
    "end": "2283460"
  },
  {
    "text": "Uh, and so they also got- so yeah, the- the numbers compared to RoBERTa,",
    "start": "2283460",
    "end": "2289295"
  },
  {
    "text": "they actually ended up being pretty similar, um, but a lot of",
    "start": "2289295",
    "end": "2294530"
  },
  {
    "text": "these things are hard to compare because people change the dataset and change the- um, change the size of the model. So it's hard to compare apples to apples.",
    "start": "2294530",
    "end": "2300380"
  },
  {
    "text": "But these two techniques ended up being pretty similar, but I think, you know, XLNet had more innovations in terms of, uh, technique.",
    "start": "2300380",
    "end": "2306785"
  },
  {
    "text": "Um, so ALBERT, it's called a Lite BERT for, uh, self-supervised learning.",
    "start": "2306785",
    "end": "2313280"
  },
  {
    "text": "And so this also had a couple of cool innovations. And so the idea here is really massive parameter sharing,",
    "start": "2313280",
    "end": "2320545"
  },
  {
    "text": "with the idea being that, if you share parameters, you're not gonna get a better language model, but you're gonna get better sample efficiency.",
    "start": "2320545",
    "end": "2326244"
  },
  {
    "text": "You're gonna get less over-fitting when you fine tune, right? Because if you have a billion parameters, and you fine tune them on a 300- on a- dataset that was like 1,000 label examples,",
    "start": "2326245",
    "end": "2334045"
  },
  {
    "text": "you're still gonna over-fit very quickly, right? But if you- if you have a much smaller number of parameters, you're gonna get less over-fitting.",
    "start": "2334045",
    "end": "2339340"
  },
  {
    "text": "So if we get a similarly powerful model with fewer parameters, you're gonna get less over-fitting. And so they- uh,",
    "start": "2339340",
    "end": "2345140"
  },
  {
    "text": "so the- so the two major innovations where- so- instead of using a word em- because the word embedding table is big, right? Because it's the size of your- of your vocabulary,",
    "start": "2345140",
    "end": "2352039"
  },
  {
    "text": "the number of, uh, word pieces times the hidden size. And so it's gonna be much bigger than hidden- the hidden layer.",
    "start": "2352040",
    "end": "2358310"
  },
  {
    "text": "So first thing is that they used a factorized embedding table. So if they- if they had a hidden size of 1,000, they only, um,",
    "start": "2358310",
    "end": "2365630"
  },
  {
    "text": "[NOISE] used like 128 dimensional input embedding, and then they projected that to 1,000, using a- using a matrix.",
    "start": "2365630",
    "end": "2373235"
  },
  {
    "text": "And so, instead of having 1024 by 100,000, they would have 128 by 100,00 plus 1024 times 128.",
    "start": "2373235",
    "end": "2380810"
  },
  {
    "text": "And you multiply these together- and multiply the two matrices together, and then effectively you have a 1024 by 100,000 embedding matrix,",
    "start": "2380810",
    "end": "2388105"
  },
  {
    "text": "um, but you have much fewer parameters. That's- you're doing parameter tying. But not- well, this isn't parameter tying, but you're doing parameter reduction in- in a clever way.",
    "start": "2388105",
    "end": "2395910"
  },
  {
    "text": "The other one is cross-layer parameter sharing. So this is similar- uh, this is simple, and it was also- it's- it's been done in previous papers,",
    "start": "2395910",
    "end": "2403460"
  },
  {
    "text": "es- uh, especially, um, universal transformer. And the idea is that you- you have a bunch of transformer layers,",
    "start": "2403460",
    "end": "2409610"
  },
  {
    "text": "but all- let's say if you have 12 layers, all 12 layers should share the same parameters, right? And so, uh, that ends up- so- so now you can have, uh,",
    "start": "2409610",
    "end": "2417305"
  },
  {
    "text": "a mu- a much bigger model that has fewer parameters than- than BERT has,",
    "start": "2417305",
    "end": "2422329"
  },
  {
    "text": "and so you get less over-fitting. And so, um, they got state of the art compared to XLNet and RoBERTa.",
    "start": "2422330",
    "end": "2429305"
  },
  {
    "text": "But one important thing to keep in mind is that ALBERT is light in terms of parameters, not in terms of speed.",
    "start": "2429305",
    "end": "2434974"
  },
  {
    "text": "So for a- um, for the mixed- for the model that's- uh,",
    "start": "2434975",
    "end": "2441455"
  },
  {
    "text": "that's actually comparable to- to BERT, um,",
    "start": "2441455",
    "end": "2446615"
  },
  {
    "text": "they- they actually did slightly, like this- like this model and this model were,",
    "start": "2446615",
    "end": "2452825"
  },
  {
    "text": "uh, about the same, but this one was actually slower. So it's only when they started making models",
    "start": "2452825",
    "end": "2458000"
  },
  {
    "text": "that were much bigger in terms of compute than BERT, but doing more parameter tying,",
    "start": "2458000",
    "end": "2463700"
  },
  {
    "text": "then they started getting good results. And so the- the implication of this is that,",
    "start": "2463700",
    "end": "2468859"
  },
  {
    "text": "like, uh, you can- you can reduce the number of parameters, but still, um, [NOISE]",
    "start": "2468860",
    "end": "2474170"
  },
  {
    "text": "nobody has figured out how to reduce the amount of pre-training compute that- which required, which is, you know, kind of unfortunate. So, uh, the next one is T5,",
    "start": "2474170",
    "end": "2482270"
  },
  {
    "text": "which is, uh, exploring the limits of transfer learning with unified text-to-text transformer. So this was a paper by Google Brain and- and other groups in Google,",
    "start": "2482270",
    "end": "2491720"
  },
  {
    "text": "uh, where they used just- they used a lot of compute. And they- and they did tons of ablation on, uh, pre-training.",
    "start": "2491720",
    "end": "2498095"
  },
  {
    "text": "They didn't like- they didn't- their- their goal wasn't to come up some- with some super clever new pre-training technique, right?",
    "start": "2498095",
    "end": "2503120"
  },
  {
    "text": "It's really just that they carefully ablate every aspect. How much does model size matter? How much does training data matter? How much does cleanness of data matter?",
    "start": "2503120",
    "end": "2509195"
  },
  {
    "text": "Like, how much does the exact way that you do the pre-training objective matter? Like, how- doing the masking? Like, how many spans do you mask?",
    "start": "2509195",
    "end": "2514445"
  },
  {
    "text": "And so they wanted to kinda very clearly, um. Uh, do the- and they also wanted to like,",
    "start": "2514445",
    "end": "2519680"
  },
  {
    "text": "push the limits of size and say, what happens if we have 300 million, a billion, 10 billion parameters, right?",
    "start": "2519680",
    "end": "2525605"
  },
  {
    "text": "And- and then- so- so they did tons and tons of ablation and they got",
    "start": "2525605",
    "end": "2531380"
  },
  {
    "text": "state of the art and everything and they're still state of the art in everything. And the results, they were a little bit bleak, uh,",
    "start": "2531380",
    "end": "2538010"
  },
  {
    "text": "in the sense that [LAUGHTER] nothing really mattered except making the data- like- like all of the ablations- it wasn't like,",
    "start": "2538010",
    "end": "2545825"
  },
  {
    "text": "oh, you know, BERT did everything perfectly. It was that- it doesn't matter. Like, you could do 20%, 25%.",
    "start": "2545825",
    "end": "2552305"
  },
  {
    "text": "You can do this fine tuning recipe, this fine tuning recipe. It's like all that really matters is making them the model bigger and training it more data and clean data.",
    "start": "2552305",
    "end": "2558964"
  },
  {
    "text": "Uh, and so, um, yeah, it's a- it's a little bit of a- of a- of oblique paper.",
    "start": "2558965",
    "end": "2564920"
  },
  {
    "text": "If- if you ha- are hoping that there is- exists some pre-training technique which is super computationally efficient and all,",
    "start": "2564920",
    "end": "2572359"
  },
  {
    "text": "so you can get, you know, very impressive results, which I'm not saying there isn't, but like most of this evidence points to not.",
    "start": "2572360",
    "end": "2578734"
  },
  {
    "text": "So the one kind of, um, newest paper that is maybe the most positive in this direction is this, uh, paper called ELECTRA.",
    "start": "2578735",
    "end": "2585410"
  },
  {
    "text": "Uh, and so, uh, and so this was done by, uh, Kevin Clark,",
    "start": "2585410",
    "end": "2591650"
  },
  {
    "text": "from here, and, uh, and Google Brain. And so, yeah. And this one, it's- it's a pretty clever idea.",
    "start": "2591650",
    "end": "2596990"
  },
  {
    "text": "So basically their idea is, instead of training- instead of training to generate the output,",
    "start": "2596990",
    "end": "2603440"
  },
  {
    "text": "you just train it as a- as a discriminator. And so you have a local language model,",
    "start": "2603440",
    "end": "2608855"
  },
  {
    "text": "you have- you do some masking, you have a local language model which replaces it and then you train it to discriminate whether it's the original one or not.",
    "start": "2608855",
    "end": "2614555"
  },
  {
    "text": "And so the idea here is that you are, um, doing a much- you- you're- you're doing- you're getting",
    "start": "2614555",
    "end": "2620360"
  },
  {
    "text": "a better sample efficiency for pre-training because you're predicting every, uh, every word which is actually, I mean,",
    "start": "2620360",
    "end": "2627500"
  },
  {
    "text": "I- I don't know definitely why it would be that different from- from- from- from- BERT still, uh, in terms of- because you don't replace it with- with the mask with whatever,",
    "start": "2627500",
    "end": "2635300"
  },
  {
    "text": "you also randomly encrypt it. But the- but the- the bigger- the biggest difference is that, um, is that these are kind of contextually replaced.",
    "start": "2635300",
    "end": "2642140"
  },
  {
    "text": "So it's like, when BERT, when I did- done the masking and replace with the random word, it was truly a random word. So most of the time it was completely trivial to tell that this was not the right word.",
    "start": "2642140",
    "end": "2649430"
  },
  {
    "text": "You don't necessarily know which word should be replaced, but in this case, they actually used a intentionally weak,",
    "start": "2649430",
    "end": "2654650"
  },
  {
    "text": "but still non-trivial language model to predict which word. So like this locally makes sense, the chef ate the meal, but it doesn't make any sense,",
    "start": "2654650",
    "end": "2660800"
  },
  {
    "text": "like a very strong model will not predict this, right? So- so that's the idea that you use a weak model to- to- to- to do the substitution, then you train a strong model to- to, um, do this.",
    "start": "2660800",
    "end": "2669550"
  },
  {
    "text": "So these results are, and this is a big table, but these results are- they are certainly positive with regard to, uh,",
    "start": "2669550",
    "end": "2679295"
  },
  {
    "text": "previous results in terms of compute versus, um- so like for if we- if we compared this row,",
    "start": "2679295",
    "end": "2685970"
  },
  {
    "text": "so uh, which is one-tenth the compute of BERT-Large to BERT-Base,",
    "start": "2685970",
    "end": "2691880"
  },
  {
    "text": "which is also one-tenth the compute of BERT-Large. It really does a lot better than BERT-Base. But when they, uh- if you- but in terms of state of the art models,",
    "start": "2691880",
    "end": "2703910"
  },
  {
    "text": "um, when they do, you know, the same amount of, uh, compute as their BERT-Large, which is this one.",
    "start": "2703910",
    "end": "2711005"
  },
  {
    "text": "Or compared to other state of the art models, they're not- in order to get state of the art, or to get similar to state the art, they basically need to do as much compute as state of the art.",
    "start": "2711005",
    "end": "2717410"
  },
  {
    "text": "So like 44x, 5.4x. So I mean, at scaled down values they were able to do better,",
    "start": "2717410",
    "end": "2722660"
  },
  {
    "text": "but this is a- still a pretty big gap, like four points. Um, so it's- it's positive, but it's not- it's certainly not like the silver bullet in terms of, uh,",
    "start": "2722660",
    "end": "2731390"
  },
  {
    "text": "showing that, uh, we can, you know, pre-trained models much better for cheaper.",
    "start": "2731390",
    "end": "2739430"
  },
  {
    "text": "So- but- so the last thing I wanna talk about is how we actually serve these models, right? Because, you know, I've said that like,",
    "start": "2739430",
    "end": "2746465"
  },
  {
    "text": "they're incredibly expensive to train, and nobody has been able to figure how to make that faster,",
    "start": "2746465",
    "end": "2752240"
  },
  {
    "text": "but, you know, they're being used all over the place, right? So like, uh, you know, there's new stories.",
    "start": "2752240",
    "end": "2757580"
  },
  {
    "text": "Google has improved 10% of searches by language understanding, ''Say hello to BERT,'' and then Bing says it has been applying BERT since April.",
    "start": "2757580",
    "end": "2763655"
  },
  {
    "text": "So- and so this is live in Google Search and Bing search, and so these are like really low latency services, right? That have like, a few milliseconds of- of latency,",
    "start": "2763655",
    "end": "2771289"
  },
  {
    "text": "and they serve, you know, billions of- of queries a day. So how are- how are they doing this?",
    "start": "2771290",
    "end": "2777950"
  },
  {
    "text": "Is it just like, uh, that you know, Google and Microsoft are sp- are spending billions of dollars on hardware,",
    "start": "2777950",
    "end": "2783530"
  },
  {
    "text": "which they are, but not- not just for this, right? And so like, uh, like it would- it would cost",
    "start": "2783530",
    "end": "2789580"
  },
  {
    "text": "billions of dollars just to serve this if youwould actually be serving BERT, but we- we're serving, uh, not. Instead of reusing- reusing model distillation, right?",
    "start": "2789580",
    "end": "2796120"
  },
  {
    "text": "So this has been around for awhile. Um, so it's cal- you call it distillation or model compression.",
    "start": "2796120",
    "end": "2802220"
  },
  {
    "text": "One of the first papers was this model compression paper, um, that was- that was done for, uh,",
    "start": "2802220",
    "end": "2809950"
  },
  {
    "text": "I forgot exactly what task, but then- and then Hinton's paper Distilling Knowledge in a Neutral Network is a more more well-known version- not that version,",
    "start": "2809950",
    "end": "2815349"
  },
  {
    "text": "but a more well-known, uh, paper on- on distillation. But in- in reality, the one- the version that- that we use at Google,",
    "start": "2815350",
    "end": "2822470"
  },
  {
    "text": "and the version that most people use when they say model distillation for, uh, pre-trained language models, it's a, um,",
    "start": "2822470",
    "end": "2828515"
  },
  {
    "text": "it's a very simple technique, but it's easy to misinterpret wh- wha- what we mean. So what we do is we pre-train- we train the state of art model,",
    "start": "2828515",
    "end": "2836890"
  },
  {
    "text": "whichever the ones the mo- we can most afford to train, right? Because of course we can just make it bigger, but we- we set some budget of,",
    "start": "2836890",
    "end": "2842380"
  },
  {
    "text": "you know we want to train it for a day on some number of GPUs. And then we fine tune it, right? So we get a model that's the maximum accuracy,",
    "start": "2842380",
    "end": "2848810"
  },
  {
    "text": "and that's our teacher model, and this is expensive. Then we have a la- a large amount of unlabeled input, which is typically for- for most industry- industry applications,",
    "start": "2848810",
    "end": "2855230"
  },
  {
    "text": "you have unlabeled input because you have you know, in search, you have, this is what they use the search for, this is what they clicked on, that's how search engines are trained.",
    "start": "2855230",
    "end": "2862025"
  },
  {
    "text": "And so you can then just take these and you, um, and then you just label your examples with them.",
    "start": "2862025",
    "end": "2869015"
  },
  {
    "text": "So you can get billions of these, uh, If you actually want a real service, and then you sort of- then you- then you run these, you know,",
    "start": "2869015",
    "end": "2876140"
  },
  {
    "text": "query answer pairs through your teacher, and you get a pseudo label,",
    "start": "2876140",
    "end": "2881210"
  },
  {
    "text": "and you just train a much smaller model, much meaning like 50 times, 100 times smaller to,",
    "start": "2881210",
    "end": "2886295"
  },
  {
    "text": "uh, predict your student- your- your teacher outputs. And so- and you can generally do this for most techniques.",
    "start": "2886295",
    "end": "2891694"
  },
  {
    "text": "I mean, for most tasks you can do this, uh, pretty easily and get a huge 50-100x, uh,",
    "start": "2891695",
    "end": "2896960"
  },
  {
    "text": "compression with no degradation, but the important thing to realize is that we're not compressing the pre-trained model itself.",
    "start": "2896960",
    "end": "2903289"
  },
  {
    "text": "We haven't really had any luck doing that. So like you can't actually just take BERT, and then compress it to a smaller model,",
    "start": "2903290",
    "end": "2908900"
  },
  {
    "text": "which you can then fine tune for all these other tasks. It's only after you've chosen the task and after you fine tune it for this task,",
    "start": "2908900",
    "end": "2914195"
  },
  {
    "text": "that you- we're able to do it. Um, so to show some specific results.",
    "start": "2914195",
    "end": "2921215"
  },
  {
    "text": "So let's say we have- let's say we have a BERT-Large teacher. This is an Amazon book reviews score. So this is a paper that- that I forgot to cite it,",
    "start": "2921215",
    "end": "2926810"
  },
  {
    "text": "but this was a paper that my group, uh, published [inaudible] wrote. And so, um, this has 50,000 labeled examples and 8 million, uh, unlabeled examples.",
    "start": "2926810",
    "end": "2939349"
  },
  {
    "text": "So you- you- you fine tune on, you pre-train BERT-Large, normal, or you you take a pre-trained BERT-Large, you, uh, you fine tune it on these 50,000 examples,",
    "start": "2939350",
    "end": "2947330"
  },
  {
    "text": "you get this 88% accuracy right? Then, uh, and so- and,",
    "start": "2947330",
    "end": "2953059"
  },
  {
    "text": "but then now, let's- let's say instead of using BERT-Large, you used a much smaller version. So this one is a quarter of the size, this one is, you know, uh, 16th of the size,",
    "start": "2953060",
    "end": "2960530"
  },
  {
    "text": "whatever, this one is 100th the size, right? So this, this row that's 100th the size. If you were to just train it,",
    "start": "2960530",
    "end": "2965780"
  },
  {
    "text": "if you would have pre-trained this on the same Wikipedia book corpus just like BERT, and then fine tune it, you would get 82% accuracy,",
    "start": "2965780",
    "end": "2974270"
  },
  {
    "text": "which is, you know, a lot worse, 6%, 6%, like 66 absolute worse, which is quite a big drop, right? But then if you were to take this 88% teacher, label it with eight million examples,",
    "start": "2974270",
    "end": "2985400"
  },
  {
    "text": "which are of course held out,  this is test- this is testaccuracy, um, and then- uh, and then train",
    "start": "2985400",
    "end": "2992705"
  },
  {
    "text": "this classification model which says this is a good or bad review on these 8 million examples, you can take this model, it's 100 times smaller,",
    "start": "2992705",
    "end": "2999020"
  },
  {
    "text": "and get the same accuracy as the teacher, right? You get the same 88% accuracy. So that's really the, uh, the- the cool thing with distillation,",
    "start": "2999020",
    "end": "3005515"
  },
  {
    "text": "is that you can get models that are much smaller, but you still need to train the big model in the first place. So it doesn't help the training costs. It just helped- it actually works because then you can use this big model",
    "start": "3005515",
    "end": "3012430"
  },
  {
    "text": "to train- to label millions or billions of examples. So it ends up being more expensive than just training BERT, but- but you can actually serve this model at- at inference time for- for a tiny cost.",
    "start": "3012430",
    "end": "3021339"
  },
  {
    "text": "So the question is like why does distillation work so well? Um, so the big hypothesis is that,",
    "start": "3021340",
    "end": "3029260"
  },
  {
    "text": "language modeling is kind of the ultimate NLP task, right? A perfect language model is also a perfect question answering system,",
    "start": "3029260",
    "end": "3035380"
  },
  {
    "text": "a perfect entailment system, sentiment analysis, co-reference, etc. Because in order to be able to- to do these things,",
    "start": "3035380",
    "end": "3041140"
  },
  {
    "text": "you kind of have to be able- you could construct it as a language model. So when you're training a massive language model,",
    "start": "3041140",
    "end": "3047220"
  },
  {
    "text": "you are learning many millions of latent features which can, which are effectively the same features that you need for any other task.",
    "start": "3047220",
    "end": "3053339"
  },
  {
    "text": "And so when you're doing a simpler- a fine tuning of a more specific task, what's- the fine tuning is basically taking",
    "start": "3053340",
    "end": "3058780"
  },
  {
    "text": "these- these latent features which your system happened to learn, and some- encode it somewhere in your weights. And you are- it is kind of just tweaking these,",
    "start": "3058780",
    "end": "3064840"
  },
  {
    "text": "which is why I could do it with a single pass over the fine-tuning data. Um, and so- but once you've figured out which parts are important,",
    "start": "3064840",
    "end": "3071380"
  },
  {
    "text": "then there exists a hypothetically much smaller model size which can still get the same representation and same generalization, right?",
    "start": "3071380",
    "end": "3077799"
  },
  {
    "text": "So then you label a bunch of examples with this fine tuned model, and now you can learn a model that can really hone in on just these features that are important.",
    "start": "3077800",
    "end": "3083799"
  },
  {
    "text": "And so it can- it can- you know, it can train a model that is 100th the size",
    "start": "3083800",
    "end": "3089260"
  },
  {
    "text": "and just hone in on these features if you have a lot of- of pseudo labeled data,",
    "start": "3089260",
    "end": "3095020"
  },
  {
    "text": "uh, and- and that's why it works. And so the evidence really is that it just doesn't work to do self distillation, right?",
    "start": "3095020",
    "end": "3101620"
  },
  {
    "text": "And so it must be that it's really just learning a subset of the features for- for most of these tasks.",
    "start": "3101620",
    "end": "3107350"
  },
  {
    "text": "Um, and so basically every task about language modeling we've been able to- to get distillation to work for.",
    "start": "3107350",
    "end": "3113755"
  },
  {
    "text": "So this includes tasks that seem like really hard, like question answering and search, um, so that does imply that- that- that language modeling itself and predicting which",
    "start": "3113755",
    "end": "3122190"
  },
  {
    "text": "is basically language generation also because that's just a form of language modeling, uh, is fundamentally harder than language understanding,",
    "start": "3122190",
    "end": "3129255"
  },
  {
    "text": "which is not, uh, super hard to buy, or at least maybe it's not fundamentally harder,",
    "start": "3129255",
    "end": "3134290"
  },
  {
    "text": "but given the state of the art, state of the art models for language understanding are fundamentally simpler in what they do, right?",
    "start": "3134290",
    "end": "3140965"
  },
  {
    "text": "So presumably they're just doing this kind of pattern recognition than models that are generating language.",
    "start": "3140965",
    "end": "3146700"
  },
  {
    "text": "Um, and so that's kind of why all of these classification models can- can kind of be distilled so well. Um, so basically in conclusion, uh,",
    "start": "3146700",
    "end": "3154360"
  },
  {
    "text": "[NOISE] the pre-trained models work really well. They're very expensive. We know how to, um,",
    "start": "3154360",
    "end": "3161335"
  },
  {
    "text": "kind of solve this for inference time, and we can do fast inference, but it- it is still unsolved.",
    "start": "3161335",
    "end": "3167120"
  },
  {
    "text": "How to make these fast, ah, at training time. And moreover, uh, it seems",
    "start": "3167610",
    "end": "3175750"
  },
  {
    "text": "like a lot of the details about algorithmic improvements for- for making the training more efficient,",
    "start": "3175750",
    "end": "3180954"
  },
  {
    "text": "um, don't seem to have, er, a ton of, of benefit in terms of at least getting to see their results. Um, and seems like a lot of choices don't really matter that much.",
    "start": "3180955",
    "end": "3188950"
  },
  {
    "text": "And it's really just about, you know, a- a- a couple of, like- like, uh, competitors to the kind of the simple, masked language baseline.",
    "start": "3188950",
    "end": "3195010"
  },
  {
    "text": "It's- it's pretty hard to beat that in- in an apples to apples comparison. Um, so yeah, it's a little bit,",
    "start": "3195010",
    "end": "3201490"
  },
  {
    "text": "uh, I mean, it's a little bit unfortunate for- for from a research perspective. It's definitely good from, uh, from people who- who",
    "start": "3201490",
    "end": "3206890"
  },
  {
    "text": "want to build NLP systems and who want to, officially domain specific NLP systems, like people who wanna, you know,",
    "start": "3206890",
    "end": "3212605"
  },
  {
    "text": "adapt to a medical domain, or people- where you only have a tiny amount of data, or people who wanna do startups so they wanna,",
    "start": "3212605",
    "end": "3217990"
  },
  {
    "text": "you know, build an actual product and they only have a tiny amount of data. So it's definitely good from that perspective, but it's certainly, I think for- ah,",
    "start": "3217990",
    "end": "3224350"
  },
  {
    "text": "from the perspective of sometimes, you know, uh, research, like- as I was saying, the goal of research is to kind of like research yourself out of a job,",
    "start": "3224350",
    "end": "3231145"
  },
  {
    "text": "then it is kind of, uh, you know, it's- it's a little unfortunate from that perspective, um, but, you know,",
    "start": "3231145",
    "end": "3237220"
  },
  {
    "text": "I still think that there's- there's- there's a possibility that there's gonna be a breakthrough that kind of shows how to do computational efficiency, um,",
    "start": "3237220",
    "end": "3243714"
  },
  {
    "text": "without a kind of show compelling results that you don't need, you know, such an absurdly large model or actually, the size of model doesn't matter.",
    "start": "3243715",
    "end": "3251740"
  },
  {
    "text": "You don't need such an absurdly expensive model, uh, to- to- to do well. Um, maybe it'll come from sparsity, right?",
    "start": "3251740",
    "end": "3257035"
  },
  {
    "text": "Or something like that where you actually do have a really large model. It's just- [NOISE] it's just sparsely activated in some- using some efficiency tricks or whatever.",
    "start": "3257035",
    "end": "3263275"
  }
]