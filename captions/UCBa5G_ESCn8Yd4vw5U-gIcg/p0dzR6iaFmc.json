[
  {
    "start": "0",
    "end": "4840"
  },
  {
    "text": "SPEAKER: Welcome back, everyone.",
    "start": "4840",
    "end": "6260"
  },
  {
    "text": "This is part three in our series\non analysis methods for NLP.",
    "start": "6260",
    "end": "9790"
  },
  {
    "text": "We're going to be focused on\nfeature attribution methods.",
    "start": "9790",
    "end": "13480"
  },
  {
    "text": "I should say at the start that\nto keep things manageable,",
    "start": "13480",
    "end": "16430"
  },
  {
    "text": "we're going to mainly focus\non integrated gradients",
    "start": "16430",
    "end": "19150"
  },
  {
    "text": "from Sundararajan et al, 2017.",
    "start": "19150",
    "end": "21520"
  },
  {
    "text": "This is a shining,\npowerful, inspiring example",
    "start": "21520",
    "end": "25120"
  },
  {
    "text": "of an attribution method\nfor reasons I will discuss,",
    "start": "25120",
    "end": "28270"
  },
  {
    "text": "but it's by no means the\nonly method in this space.",
    "start": "28270",
    "end": "31600"
  },
  {
    "text": "For one stop shopping\non these methods,",
    "start": "31600",
    "end": "34060"
  },
  {
    "text": "I recommend the\ncaptum.ai library.",
    "start": "34060",
    "end": "36610"
  },
  {
    "text": "It will give you access to\nlots of gradient-based methods",
    "start": "36610",
    "end": "40000"
  },
  {
    "text": "like IG, as well as\nmany others, including",
    "start": "40000",
    "end": "43180"
  },
  {
    "text": "more traditional methods like\nfeature ablation and feature",
    "start": "43180",
    "end": "46510"
  },
  {
    "text": "permutation.",
    "start": "46510",
    "end": "47739"
  },
  {
    "text": "So check out captum.ai.",
    "start": "47740",
    "end": "49570"
  },
  {
    "text": "In addition, if you would like a\ndeeper dive on the calculations",
    "start": "49570",
    "end": "53380"
  },
  {
    "text": "and examples that I\nuse in this screencast,",
    "start": "53380",
    "end": "56000"
  },
  {
    "text": "I recommend the notebook\nfeature attribution,",
    "start": "56000",
    "end": "58370"
  },
  {
    "text": "which is part of the\ncourse code repository.",
    "start": "58370",
    "end": "62760"
  },
  {
    "text": "Now, I love the integrated\ngradients paper Sundararajan et",
    "start": "62760",
    "end": "67159"
  },
  {
    "text": "al, 2017 because of\nits method, but also",
    "start": "67160",
    "end": "70460"
  },
  {
    "text": "because it offers a really\nnice framework for thinking",
    "start": "70460",
    "end": "74090"
  },
  {
    "text": "about attribution in general.",
    "start": "74090",
    "end": "75740"
  },
  {
    "text": "And they do that in the\nform of three axioms.",
    "start": "75740",
    "end": "78470"
  },
  {
    "text": "I'm going to talk\nabout two of them.",
    "start": "78470",
    "end": "80210"
  },
  {
    "text": "And of the two, the most\nimportant one is sensitivity.",
    "start": "80210",
    "end": "83690"
  },
  {
    "text": "This is very intuitive.",
    "start": "83690",
    "end": "85810"
  },
  {
    "text": "The axiom of sensitivity\nfor attribution methods",
    "start": "85810",
    "end": "88200"
  },
  {
    "text": "says, if two inputs\nx and x prime",
    "start": "88200",
    "end": "90689"
  },
  {
    "text": "differ only at dimension i and\nlead to different predictions,",
    "start": "90690",
    "end": "95460"
  },
  {
    "text": "then the feature\nassociated with dimension i",
    "start": "95460",
    "end": "98370"
  },
  {
    "text": "has non-zero attribution.",
    "start": "98370",
    "end": "101430"
  },
  {
    "text": "So here's a quick example.",
    "start": "101430",
    "end": "102810"
  },
  {
    "text": "Our model is M and\nit takes inputs",
    "start": "102810",
    "end": "105899"
  },
  {
    "text": "that are three-dimensional.",
    "start": "105900",
    "end": "107130"
  },
  {
    "text": "And for input 1, 0, 1, this\nmodel outputs positive.",
    "start": "107130",
    "end": "111090"
  },
  {
    "text": "And for 1, 1, 1, it\noutputs negative.",
    "start": "111090",
    "end": "114659"
  },
  {
    "text": "That's a difference\nin the predictions,",
    "start": "114660",
    "end": "116790"
  },
  {
    "text": "and that means that the\nfeature in position 2",
    "start": "116790",
    "end": "119700"
  },
  {
    "text": "here must have\nnon-zero attribution.",
    "start": "119700",
    "end": "123149"
  },
  {
    "text": "Seems very intuitive\nbecause obviously,",
    "start": "123150",
    "end": "125100"
  },
  {
    "text": "this feature is important to\nthe behavior of this model.",
    "start": "125100",
    "end": "128940"
  },
  {
    "text": "Just quickly I'll\nmention a second axiom,",
    "start": "128940",
    "end": "131160"
  },
  {
    "text": "implementation invariance.",
    "start": "131160",
    "end": "132750"
  },
  {
    "text": "If two models, M and M prime\nhave identical input/output",
    "start": "132750",
    "end": "136320"
  },
  {
    "text": "behavior, then the\nattributions for M and M prime",
    "start": "136320",
    "end": "139530"
  },
  {
    "text": "are identical.",
    "start": "139530",
    "end": "140700"
  },
  {
    "text": "That's very intuitive.",
    "start": "140700",
    "end": "141810"
  },
  {
    "text": "If the models can't be\ndistinguished behaviorally,",
    "start": "141810",
    "end": "144480"
  },
  {
    "text": "then we should give them\nidentical attributions.",
    "start": "144480",
    "end": "147069"
  },
  {
    "text": "We should not be\nsensitive to kind",
    "start": "147070",
    "end": "148840"
  },
  {
    "text": "of incidental details of\nhow they were structured",
    "start": "148840",
    "end": "151870"
  },
  {
    "text": "or how they were implemented.",
    "start": "151870",
    "end": "155400"
  },
  {
    "text": "Let's begin with a baseline\nmethod gradients by inputs.",
    "start": "155400",
    "end": "159180"
  },
  {
    "text": "This is very intuitive\nand makes some kind",
    "start": "159180",
    "end": "161700"
  },
  {
    "text": "of sense from the\nperspective of doing feature",
    "start": "161700",
    "end": "163709"
  },
  {
    "text": "attribution in deep networks.",
    "start": "163710",
    "end": "166200"
  },
  {
    "text": "What we're going to do is\ncalculate the gradients",
    "start": "166200",
    "end": "168420"
  },
  {
    "text": "for our model with respect\nto the chosen feature",
    "start": "168420",
    "end": "171090"
  },
  {
    "text": "that we want to\ntarget and multiply",
    "start": "171090",
    "end": "173550"
  },
  {
    "text": "that value by the actual value\nof the feature, so gradients",
    "start": "173550",
    "end": "177270"
  },
  {
    "text": "by inputs.",
    "start": "177270",
    "end": "178230"
  },
  {
    "text": "It's called gradients by\ninputs, but obviously, we",
    "start": "178230",
    "end": "180900"
  },
  {
    "text": "could do this kind of\ngradient taking with respect",
    "start": "180900",
    "end": "183390"
  },
  {
    "text": "to any neuron in one of\nthese deep learning models",
    "start": "183390",
    "end": "185940"
  },
  {
    "text": "and multiply it by the\nactual value of that neuron",
    "start": "185940",
    "end": "188640"
  },
  {
    "text": "for some example.",
    "start": "188640",
    "end": "189880"
  },
  {
    "text": "And so actually this method\ngeneralizes really nicely",
    "start": "189880",
    "end": "192810"
  },
  {
    "text": "to any state in a\ndeep learning model.",
    "start": "192810",
    "end": "195959"
  },
  {
    "text": "It's really straightforward\nto implement that.",
    "start": "195960",
    "end": "198000"
  },
  {
    "text": "I've depicted that\non the slide here.",
    "start": "198000",
    "end": "199810"
  },
  {
    "text": "The first implementation\nuses Raw PyTorch.",
    "start": "199810",
    "end": "203340"
  },
  {
    "text": "And the second one is\njust a lightweight wrapper",
    "start": "203340",
    "end": "206069"
  },
  {
    "text": "around the captum implementation\nof input by gradient.",
    "start": "206070",
    "end": "210430"
  },
  {
    "text": "So it shows you how\nstraightforward this can be.",
    "start": "210430",
    "end": "213549"
  },
  {
    "text": "One issue that I want\nto linger over here",
    "start": "213550",
    "end": "216400"
  },
  {
    "text": "that I find\nconceptually difficult",
    "start": "216400",
    "end": "218590"
  },
  {
    "text": "is this question of how we\nshould do the attributions.",
    "start": "218590",
    "end": "222099"
  },
  {
    "text": "For classifier models,\nwe have a choice.",
    "start": "222100",
    "end": "224540"
  },
  {
    "text": "We can take attributions with\nrespect to the predicted labels",
    "start": "224540",
    "end": "227829"
  },
  {
    "text": "or with respect to the\nactual labels, which",
    "start": "227830",
    "end": "230170"
  },
  {
    "text": "are two different\ndimensions in the output",
    "start": "230170",
    "end": "232900"
  },
  {
    "text": "vector for these models.",
    "start": "232900",
    "end": "235480"
  },
  {
    "text": "Now, if the model\nyou're studying",
    "start": "235480",
    "end": "237430"
  },
  {
    "text": "is very high performing, then\nthe predicted and actual labels",
    "start": "237430",
    "end": "241209"
  },
  {
    "text": "will be almost identical, and\nthis is unlikely to matter.",
    "start": "241210",
    "end": "244150"
  },
  {
    "text": "But you might be trying to\nstudy a really poor performing",
    "start": "244150",
    "end": "247659"
  },
  {
    "text": "model to try to understand\nwhere its deficiencies lie,",
    "start": "247660",
    "end": "251140"
  },
  {
    "text": "and that's precisely\nthe case where",
    "start": "251140",
    "end": "252940"
  },
  {
    "text": "these two will come apart.",
    "start": "252940",
    "end": "254380"
  },
  {
    "text": "As an illustration\non this slide here,",
    "start": "254380",
    "end": "256299"
  },
  {
    "text": "I've defined a simple\nmake classification",
    "start": "256300",
    "end": "258639"
  },
  {
    "text": "synthetic problem\nusing scikit-learn.",
    "start": "258640",
    "end": "260920"
  },
  {
    "text": "It has four features.",
    "start": "260920",
    "end": "262660"
  },
  {
    "text": "And then I set up a\nshallow neural classifier",
    "start": "262660",
    "end": "265300"
  },
  {
    "text": "and I deliberately\nundertrained it.",
    "start": "265300",
    "end": "267190"
  },
  {
    "text": "It has just one\ntraining iteration.",
    "start": "267190",
    "end": "269680"
  },
  {
    "text": "This is a very bad model.",
    "start": "269680",
    "end": "272229"
  },
  {
    "text": "If I do attributions with\nrespect to the true labels,",
    "start": "272230",
    "end": "275980"
  },
  {
    "text": "I get one vector of\nattribution scores.",
    "start": "275980",
    "end": "279460"
  },
  {
    "text": "If I do attributions\nwith respect",
    "start": "279460",
    "end": "281470"
  },
  {
    "text": "to the predicted labels, I\nget a totally different set",
    "start": "281470",
    "end": "284950"
  },
  {
    "text": "of attribution scores.",
    "start": "284950",
    "end": "287170"
  },
  {
    "text": "And that confronts you with a\ndifficult conceptual question",
    "start": "287170",
    "end": "291100"
  },
  {
    "text": "of which ones you want to\nuse to guide your analysis.",
    "start": "291100",
    "end": "294310"
  },
  {
    "text": "They are giving us different\npictures of this model.",
    "start": "294310",
    "end": "297650"
  },
  {
    "text": "And I think that there\nis no a priori reason",
    "start": "297650",
    "end": "300310"
  },
  {
    "text": "to favor one over the other.",
    "start": "300310",
    "end": "302030"
  },
  {
    "text": "I think it really comes\ndown to what you're",
    "start": "302030",
    "end": "304389"
  },
  {
    "text": "trying to accomplish\nwith the analysis",
    "start": "304390",
    "end": "306370"
  },
  {
    "text": "that you are constructing.",
    "start": "306370",
    "end": "307600"
  },
  {
    "text": "And so the best\nanswer I can give",
    "start": "307600",
    "end": "309670"
  },
  {
    "text": "is to be explicit\nabout your assumptions",
    "start": "309670",
    "end": "312700"
  },
  {
    "text": "and about the methods\nthat you used.",
    "start": "312700",
    "end": "315040"
  },
  {
    "text": "This issue, by the\nway, will carry forward",
    "start": "315040",
    "end": "317140"
  },
  {
    "text": "through all of these\ngradient-based methods,",
    "start": "317140",
    "end": "319280"
  },
  {
    "text": "not just inputs by gradients.",
    "start": "319280",
    "end": "323730"
  },
  {
    "text": "Here's the fundamental sticking\npoint for gradients by inputs.",
    "start": "323730",
    "end": "327180"
  },
  {
    "text": "It fails that sensitivity axiom.",
    "start": "327180",
    "end": "329520"
  },
  {
    "text": "This is an example, a\ncounterexample to sensitivity",
    "start": "329520",
    "end": "332400"
  },
  {
    "text": "that comes directly from\nSundararajan et al, 2017.",
    "start": "332400",
    "end": "336060"
  },
  {
    "text": "We have a very\nsimple model here,",
    "start": "336060",
    "end": "338010"
  },
  {
    "text": "M. It takes\none-dimensional inputs.",
    "start": "338010",
    "end": "341010"
  },
  {
    "text": "And what it does is 1\nminus the ReLu activation",
    "start": "341010",
    "end": "345000"
  },
  {
    "text": "applied to 1 minus the input.",
    "start": "345000",
    "end": "347100"
  },
  {
    "text": "Very simple model.",
    "start": "347100",
    "end": "349050"
  },
  {
    "text": "When we use the model with\ninput 0, we get 0 as the output.",
    "start": "349050",
    "end": "353759"
  },
  {
    "text": "When we use the model with\ninput 2, we get 1 as the output.",
    "start": "353760",
    "end": "357760"
  },
  {
    "text": "So we have a difference\nin output predictions.",
    "start": "357760",
    "end": "360900"
  },
  {
    "text": "These are\none-dimensional inputs.",
    "start": "360900",
    "end": "362729"
  },
  {
    "text": "So we are now required\nby sensitivity",
    "start": "362730",
    "end": "364950"
  },
  {
    "text": "to give non-zero\nattribution to this feature.",
    "start": "364950",
    "end": "369240"
  },
  {
    "text": "But sadly, we do not.",
    "start": "369240",
    "end": "370680"
  },
  {
    "text": "When you run input by gradients\non this input, you get 0.",
    "start": "370680",
    "end": "374250"
  },
  {
    "text": "And when you run input\nby gradients on input 2,",
    "start": "374250",
    "end": "377250"
  },
  {
    "text": "you also get 0.",
    "start": "377250",
    "end": "378540"
  },
  {
    "text": "And that is just\na direct failure",
    "start": "378540",
    "end": "380520"
  },
  {
    "text": "to meet the sensitivity\nrequirement.",
    "start": "380520",
    "end": "382990"
  },
  {
    "text": "So that's a worrisome thing\nabout this baseline method.",
    "start": "382990",
    "end": "386590"
  },
  {
    "text": "It queues us up nicely to think\nabout how integrated gradients",
    "start": "386590",
    "end": "390100"
  },
  {
    "text": "will do better.",
    "start": "390100",
    "end": "392410"
  },
  {
    "text": "The intuition behind\nintegrated gradients",
    "start": "392410",
    "end": "395650"
  },
  {
    "text": "is that we are going to\nexplore counterfactual versions",
    "start": "395650",
    "end": "399160"
  },
  {
    "text": "of our input.",
    "start": "399160",
    "end": "399920"
  },
  {
    "text": "And I think that is\nan important insight.",
    "start": "399920",
    "end": "402130"
  },
  {
    "text": "As we try to get causal\ninsights into model behavior,",
    "start": "402130",
    "end": "405790"
  },
  {
    "text": "it becomes ever more essential\nto think about counterfactuals.",
    "start": "405790",
    "end": "409840"
  },
  {
    "text": "Here's the way IG does this.",
    "start": "409840",
    "end": "412240"
  },
  {
    "text": "We have two features in\nour space, x1 and x2.",
    "start": "412240",
    "end": "415419"
  },
  {
    "text": "And this blue dot\nrepresents the example",
    "start": "415420",
    "end": "418210"
  },
  {
    "text": "that we would like to\ndo attributions for.",
    "start": "418210",
    "end": "420880"
  },
  {
    "text": "With integrated gradients,\nthe first thing we do",
    "start": "420880",
    "end": "423010"
  },
  {
    "text": "is set up a baseline.",
    "start": "423010",
    "end": "424210"
  },
  {
    "text": "And a standard baseline for\nthis would be the 0 vector.",
    "start": "424210",
    "end": "428310"
  },
  {
    "text": "And then we are going to create\nsynthetic examples interpolated",
    "start": "428310",
    "end": "432000"
  },
  {
    "text": "between the baseline\nand our actual example.",
    "start": "432000",
    "end": "436060"
  },
  {
    "text": "And we are going to study\nthe gradients with respect",
    "start": "436060",
    "end": "438300"
  },
  {
    "text": "to every single one of\nthese interpolated examples,",
    "start": "438300",
    "end": "441900"
  },
  {
    "text": "aggregate them together, and\nuse all of that information",
    "start": "441900",
    "end": "445080"
  },
  {
    "text": "to do our attributions.",
    "start": "445080",
    "end": "447990"
  },
  {
    "text": "Here's a look at the IG\ncalculation in some detail",
    "start": "447990",
    "end": "451169"
  },
  {
    "text": "as you might implement it for\nan actual piece of software.",
    "start": "451170",
    "end": "455590"
  },
  {
    "text": "Let's break this down\ninto some pieces.",
    "start": "455590",
    "end": "457380"
  },
  {
    "text": "Step 1, we have\nthis vector alpha",
    "start": "457380",
    "end": "460080"
  },
  {
    "text": "and this is going to\ndetermine the number of steps",
    "start": "460080",
    "end": "462210"
  },
  {
    "text": "that we use to get\ndifferent synthetic inputs",
    "start": "462210",
    "end": "465000"
  },
  {
    "text": "between baseline and\nthe actual example.",
    "start": "465000",
    "end": "468850"
  },
  {
    "text": "We're going to interpolate these\ninputs between the baseline",
    "start": "468850",
    "end": "471930"
  },
  {
    "text": "and the actual example.",
    "start": "471930",
    "end": "473280"
  },
  {
    "text": "That's what happens\nin purple here,",
    "start": "473280",
    "end": "475590"
  },
  {
    "text": "according to these alpha steps.",
    "start": "475590",
    "end": "477690"
  },
  {
    "text": "And then we compute\nthe gradients",
    "start": "477690",
    "end": "479430"
  },
  {
    "text": "for each interpolated input.",
    "start": "479430",
    "end": "481199"
  },
  {
    "text": "And that part of\nthe calculation,",
    "start": "481200",
    "end": "482730"
  },
  {
    "text": "the individual\nones, looks exactly",
    "start": "482730",
    "end": "484800"
  },
  {
    "text": "like inputs by\ngradients, of course.",
    "start": "484800",
    "end": "487979"
  },
  {
    "text": "Next step, we do an\nintegral approximation",
    "start": "487980",
    "end": "490620"
  },
  {
    "text": "through the averaging.",
    "start": "490620",
    "end": "491669"
  },
  {
    "text": "That's the summation\nthat you see.",
    "start": "491670",
    "end": "493410"
  },
  {
    "text": "So we're going to sum\nover all of these examples",
    "start": "493410",
    "end": "496170"
  },
  {
    "text": "that we've taken and\ncreated the gradients for.",
    "start": "496170",
    "end": "499090"
  },
  {
    "text": "And then finally, we do some\nscaling to remain in the space",
    "start": "499090",
    "end": "502169"
  },
  {
    "text": "region of the original example.",
    "start": "502170",
    "end": "504990"
  },
  {
    "text": "And that is the\ncomplete IG calculation.",
    "start": "504990",
    "end": "509300"
  },
  {
    "text": "Let's return to sensitivity.",
    "start": "509300",
    "end": "511159"
  },
  {
    "text": "We have our model M with\nthese one-dimensional inputs 1",
    "start": "511160",
    "end": "516080"
  },
  {
    "text": "minus ReLu applied to 1 minus x.",
    "start": "516080",
    "end": "518840"
  },
  {
    "text": "This is the example\nfrom Sundararajan et al.",
    "start": "518840",
    "end": "521570"
  },
  {
    "text": "I showed you before\nthat inputs by gradients",
    "start": "521570",
    "end": "524330"
  },
  {
    "text": "fail sensitivity for this\nexample in this model.",
    "start": "524330",
    "end": "528320"
  },
  {
    "text": "Integrated gradients\ndoes better.",
    "start": "528320",
    "end": "531240"
  },
  {
    "text": "And the reason it does\nbetter, you can see this here",
    "start": "531240",
    "end": "533540"
  },
  {
    "text": "we are summing over all of those\ngradient calculations and kind",
    "start": "533540",
    "end": "537980"
  },
  {
    "text": "of averaging through them.",
    "start": "537980",
    "end": "539550"
  },
  {
    "text": "And the result of all of\nthat summing and averaging",
    "start": "539550",
    "end": "542959"
  },
  {
    "text": "is an attribution\nof approximately 1,",
    "start": "542960",
    "end": "545510"
  },
  {
    "text": "depending on exactly which\nsteps that you decide to look",
    "start": "545510",
    "end": "548210"
  },
  {
    "text": "at for the IG calculation.",
    "start": "548210",
    "end": "550890"
  },
  {
    "text": "So this example is no longer\na sensitive-- a counter",
    "start": "550890",
    "end": "554600"
  },
  {
    "text": "example to sensitivity.",
    "start": "554600",
    "end": "555949"
  },
  {
    "text": "And in fact, it's provable that\nIG satisfies the sensitivity",
    "start": "555950",
    "end": "559880"
  },
  {
    "text": "axiom.",
    "start": "559880",
    "end": "562210"
  },
  {
    "text": "Let's think in\npractical terms now.",
    "start": "562210",
    "end": "564280"
  },
  {
    "text": "We're likely to be thinking\nabout BERT style models.",
    "start": "564280",
    "end": "567280"
  },
  {
    "text": "This is a kind of\ncartoon version of BERT,",
    "start": "567280",
    "end": "569260"
  },
  {
    "text": "where I have some output\nlabels up here at the top.",
    "start": "569260",
    "end": "572110"
  },
  {
    "text": "I have a lot of hidden states.",
    "start": "572110",
    "end": "573850"
  },
  {
    "text": "And I have a lot\nof things happening",
    "start": "573850",
    "end": "575500"
  },
  {
    "text": "all the way down to maybe\nmultiple and fixed embedding",
    "start": "575500",
    "end": "578830"
  },
  {
    "text": "layers.",
    "start": "578830",
    "end": "579520"
  },
  {
    "text": "And the fundamental thing about\nIG that makes it so freeing",
    "start": "579520",
    "end": "582880"
  },
  {
    "text": "is that we can do\nattributions with respect",
    "start": "582880",
    "end": "585460"
  },
  {
    "text": "to any neuron in any state\nin this entire model.",
    "start": "585460",
    "end": "589700"
  },
  {
    "text": "And so we have some of the\nflexibility of probing,",
    "start": "589700",
    "end": "592300"
  },
  {
    "text": "but now we will get\ncausal guarantees that",
    "start": "592300",
    "end": "594790"
  },
  {
    "text": "are attributions relate to\nthe causal efficacy of neurons",
    "start": "594790",
    "end": "598930"
  },
  {
    "text": "on input/output behavior.",
    "start": "598930",
    "end": "601360"
  },
  {
    "text": "Here's a complete\nworked example that you",
    "start": "601360",
    "end": "604000"
  },
  {
    "text": "might want to work\nwith yourselves, modify",
    "start": "604000",
    "end": "606400"
  },
  {
    "text": "study, and so forth.",
    "start": "606400",
    "end": "607460"
  },
  {
    "text": "Let me walk through it\nat a high level for now.",
    "start": "607460",
    "end": "610420"
  },
  {
    "text": "The first thing I do is load\nin a Twitter-based sentiment",
    "start": "610420",
    "end": "615370"
  },
  {
    "text": "classifier based on Roberta\nthat I got from Hugging Face.",
    "start": "615370",
    "end": "619360"
  },
  {
    "text": "For the sake of captum,\nyou need to define",
    "start": "619360",
    "end": "621670"
  },
  {
    "text": "your own probabilistic\nprediction function,",
    "start": "621670",
    "end": "624529"
  },
  {
    "text": "and you need to\ndefine a function that",
    "start": "624530",
    "end": "626300"
  },
  {
    "text": "will create for you\nrepresentations for your base,",
    "start": "626300",
    "end": "629089"
  },
  {
    "text": "as well as for your\nactual example.",
    "start": "629090",
    "end": "631160"
  },
  {
    "text": "Those are the things that we'll\ninterpolate between with IG.",
    "start": "631160",
    "end": "635420"
  },
  {
    "text": "You need to do one\nmore function to find",
    "start": "635420",
    "end": "637700"
  },
  {
    "text": "the forward part of your model.",
    "start": "637700",
    "end": "639860"
  },
  {
    "text": "Here I just needed to grab the\nlogits, and then ig.forward.",
    "start": "639860",
    "end": "643790"
  },
  {
    "text": "And whatever layer I pick are\nthe core arguments to layer",
    "start": "643790",
    "end": "647600"
  },
  {
    "text": "integrated gradients for captum.",
    "start": "647600",
    "end": "650120"
  },
  {
    "text": "Here's my example.",
    "start": "650120",
    "end": "651500"
  },
  {
    "text": "This is illuminating.",
    "start": "651500",
    "end": "652490"
  },
  {
    "text": "It has true class positive.",
    "start": "652490",
    "end": "654709"
  },
  {
    "text": "I'll take attributions\nwith respect",
    "start": "654710",
    "end": "656480"
  },
  {
    "text": "to the positive\nclass, the true class.",
    "start": "656480",
    "end": "659120"
  },
  {
    "text": "Here are my base\nand actual inputs.",
    "start": "659120",
    "end": "662910"
  },
  {
    "text": "And here is the actual\nattribution step, inputs,",
    "start": "662910",
    "end": "666680"
  },
  {
    "text": "base IDs.",
    "start": "666680",
    "end": "668029"
  },
  {
    "text": "The target is the true class.",
    "start": "668030",
    "end": "669980"
  },
  {
    "text": "And this is some other\nkeyword argument.",
    "start": "669980",
    "end": "672260"
  },
  {
    "text": "The result of that is\nsome scores, which I use.",
    "start": "672260",
    "end": "675230"
  },
  {
    "text": "And then I kind of\nz-score normalize them",
    "start": "675230",
    "end": "677420"
  },
  {
    "text": "across individual\nrepresentations",
    "start": "677420",
    "end": "679790"
  },
  {
    "text": "in the BERT model.",
    "start": "679790",
    "end": "680703"
  },
  {
    "text": "That's an additional\nassumption that I'm",
    "start": "680703",
    "end": "682370"
  },
  {
    "text": "bringing in that will\ndo, kind of averaging",
    "start": "682370",
    "end": "684620"
  },
  {
    "text": "of attributions across these\nhidden representations.",
    "start": "684620",
    "end": "689089"
  },
  {
    "text": "A little more calculating.",
    "start": "689090",
    "end": "690410"
  },
  {
    "text": "And then captum\nprovides a nice function",
    "start": "690410",
    "end": "692600"
  },
  {
    "text": "for visualizing these things.",
    "start": "692600",
    "end": "694410"
  },
  {
    "text": "And so what you get out\nare little snippets of HTML",
    "start": "694410",
    "end": "697310"
  },
  {
    "text": "that summarize the attributions\nand associated metadata.",
    "start": "697310",
    "end": "700700"
  },
  {
    "text": "I've got the true label,\nthe predicted label.",
    "start": "700700",
    "end": "703160"
  },
  {
    "text": "Those align.",
    "start": "703160",
    "end": "705410"
  },
  {
    "text": "There's some scores.",
    "start": "705410",
    "end": "706250"
  },
  {
    "text": "And I'm not sure actually\nwhat attribution label",
    "start": "706250",
    "end": "708470"
  },
  {
    "text": "is supposed to do.",
    "start": "708470",
    "end": "709680"
  },
  {
    "text": "But the nice thing is that\nwe have color coding here",
    "start": "709680",
    "end": "712160"
  },
  {
    "text": "with color proportional\nto the attribution score.",
    "start": "712160",
    "end": "716029"
  },
  {
    "text": "You have to be a little\nbit careful here.",
    "start": "716030",
    "end": "718220"
  },
  {
    "text": "Green means evidence toward\nthe positive label, which",
    "start": "718220",
    "end": "722600"
  },
  {
    "text": "in sentiment might be negative.",
    "start": "722600",
    "end": "724079"
  },
  {
    "text": "This is meant to\nbe the true label.",
    "start": "724080",
    "end": "725690"
  },
  {
    "text": "And negative is evidence\naway from the true label.",
    "start": "725690",
    "end": "728570"
  },
  {
    "text": "And the White there is neutral.",
    "start": "728570",
    "end": "731240"
  },
  {
    "text": "So here's a fuller example.",
    "start": "731240",
    "end": "732810"
  },
  {
    "text": "And for this one to\navoid confusing myself,",
    "start": "732810",
    "end": "734779"
  },
  {
    "text": "I did relabel the legend\naway from the true label,",
    "start": "734780",
    "end": "737960"
  },
  {
    "text": "neutral with respect\nto the true label",
    "start": "737960",
    "end": "739820"
  },
  {
    "text": "and toward the true label.",
    "start": "739820",
    "end": "741450"
  },
  {
    "text": "So this is kind\nof very intuitive.",
    "start": "741450",
    "end": "743090"
  },
  {
    "text": "Where the true\nlabel is positive,",
    "start": "743090",
    "end": "744800"
  },
  {
    "text": "we get strong\nattributions for great.",
    "start": "744800",
    "end": "747410"
  },
  {
    "text": "Where the true\nlabel is negative,",
    "start": "747410",
    "end": "749490"
  },
  {
    "text": "we get strong\nattributions for words",
    "start": "749490",
    "end": "751339"
  },
  {
    "text": "like wrong and less activation\nfor things like great.",
    "start": "751340",
    "end": "754790"
  },
  {
    "text": "This is kind of\nintuitive here that we're",
    "start": "754790",
    "end": "756589"
  },
  {
    "text": "getting more activation for\nsaid and for great suggesting",
    "start": "756590",
    "end": "760850"
  },
  {
    "text": "the model has learned that\nthe reporting verb there",
    "start": "760850",
    "end": "764000"
  },
  {
    "text": "kind of modulates the positive\nsentiment that is in its scope.",
    "start": "764000",
    "end": "768350"
  },
  {
    "text": "And then down at\nthe bottom here,",
    "start": "768350",
    "end": "770149"
  },
  {
    "text": "we have one of these kind\nof tricky situations.",
    "start": "770150",
    "end": "772640"
  },
  {
    "text": "The true label is 0.",
    "start": "772640",
    "end": "774290"
  },
  {
    "text": "The predicted label is 1.",
    "start": "774290",
    "end": "776209"
  },
  {
    "text": "These are attributions with\nrespect to the true label.",
    "start": "776210",
    "end": "779460"
  },
  {
    "text": "So we're seeing, for\nexample, that incorrect",
    "start": "779460",
    "end": "781820"
  },
  {
    "text": "is biased toward negative.",
    "start": "781820",
    "end": "784550"
  },
  {
    "text": "My guess about\nthese attributions",
    "start": "784550",
    "end": "786410"
  },
  {
    "text": "is that the model is\nactually doing pretty well",
    "start": "786410",
    "end": "788540"
  },
  {
    "text": "with this example\nand maybe missed",
    "start": "788540",
    "end": "790730"
  },
  {
    "text": "for some incidental reason.",
    "start": "790730",
    "end": "793199"
  },
  {
    "text": "But overall, I would\nsay qualitatively,",
    "start": "793200",
    "end": "795950"
  },
  {
    "text": "this slide is a kind\nof reassuring picture",
    "start": "795950",
    "end": "798410"
  },
  {
    "text": "that the model is\ndoing something",
    "start": "798410",
    "end": "800149"
  },
  {
    "text": "systematic with its features\nin making these sentiment",
    "start": "800150",
    "end": "803630"
  },
  {
    "text": "predictions.",
    "start": "803630",
    "end": "806170"
  },
  {
    "text": "So to summarize,\nfeature attribution",
    "start": "806170",
    "end": "809070"
  },
  {
    "text": "can give OK characterizations\nof the representations.",
    "start": "809070",
    "end": "812850"
  },
  {
    "text": "You really just\nget a scalar value",
    "start": "812850",
    "end": "814889"
  },
  {
    "text": "about the degree of\nimportance and you",
    "start": "814890",
    "end": "817260"
  },
  {
    "text": "have to make further\nguesses about why examples",
    "start": "817260",
    "end": "820980"
  },
  {
    "text": "or, sorry, representations\nare important,",
    "start": "820980",
    "end": "822930"
  },
  {
    "text": "but it's still useful guidance.",
    "start": "822930",
    "end": "825720"
  },
  {
    "text": "We can get causal guarantees\nwhen we use models like IG,",
    "start": "825720",
    "end": "830220"
  },
  {
    "text": "but I'm afraid that there's\nno clear, direct path",
    "start": "830220",
    "end": "833399"
  },
  {
    "text": "to using methods like IG\nto directly improve models.",
    "start": "833400",
    "end": "837210"
  },
  {
    "text": "It's kind of like you've just\ngot some ambient information",
    "start": "837210",
    "end": "840720"
  },
  {
    "text": "that might guide you in\na subsequent and separate",
    "start": "840720",
    "end": "843779"
  },
  {
    "text": "modeling step that would\nimprove your model.",
    "start": "843780",
    "end": "846780"
  },
  {
    "text": "So that's a summary of feature\nattribution, a powerful, pretty",
    "start": "846780",
    "end": "850290"
  },
  {
    "text": "flexible, heuristic\nmethod that can",
    "start": "850290",
    "end": "852509"
  },
  {
    "text": "offer useful insights about\nhow models are solving tasks.",
    "start": "852510",
    "end": "857660"
  },
  {
    "start": "857660",
    "end": "862000"
  }
]