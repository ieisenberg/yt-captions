[
  {
    "start": "0",
    "end": "23000"
  },
  {
    "start": "0",
    "end": "4041"
  },
  {
    "text": "CHRISTOPHER POTTS:\nHello, everyone,",
    "start": "4042",
    "end": "5500"
  },
  {
    "text": "welcome to our final\nscreencast in our unit",
    "start": "5500",
    "end": "7450"
  },
  {
    "text": "on distributed word\nrepresentations.",
    "start": "7450",
    "end": "9250"
  },
  {
    "text": "Our topic is going to be\nderiving static representations",
    "start": "9250",
    "end": "12070"
  },
  {
    "text": "from contextual models.",
    "start": "12070",
    "end": "13599"
  },
  {
    "text": "That might sound awfully\nspecific, but as you'll see,",
    "start": "13600",
    "end": "15940"
  },
  {
    "text": "I think this could be\nreally empowering for you",
    "start": "15940",
    "end": "17949"
  },
  {
    "text": "as you work on your original\nsystem for your assignment",
    "start": "17950",
    "end": "20950"
  },
  {
    "text": "and the associated bake-off.",
    "start": "20950",
    "end": "23940"
  },
  {
    "start": "23000",
    "end": "117000"
  },
  {
    "text": "So let's dive in.",
    "start": "23940",
    "end": "24700"
  },
  {
    "text": "A question on your\nminds might be,",
    "start": "24700",
    "end": "26370"
  },
  {
    "text": "how can I use BERT or related\nmodels like RoBERTa or XLNet",
    "start": "26370",
    "end": "29970"
  },
  {
    "text": "or ELECTRA in the\ncontext of deriving",
    "start": "29970",
    "end": "32369"
  },
  {
    "text": "good static\nrepresentations of words?",
    "start": "32369",
    "end": "34450"
  },
  {
    "text": "You probably have heard\nabout these models",
    "start": "34450",
    "end": "36210"
  },
  {
    "text": "and heard that they lift\nall boats and the question",
    "start": "36210",
    "end": "38520"
  },
  {
    "text": "is, how can you take\nadvantage of those benefits?",
    "start": "38520",
    "end": "42075"
  },
  {
    "text": "But there's a tension here.",
    "start": "42075",
    "end": "43200"
  },
  {
    "text": "We've been developing\nstatic representations,",
    "start": "43200",
    "end": "45660"
  },
  {
    "text": "but these models like\nBERT are designed",
    "start": "45660",
    "end": "48390"
  },
  {
    "text": "to deliver contextual\nrepresentations of words.",
    "start": "48390",
    "end": "51242"
  },
  {
    "text": "And I'll return to what\nthat means in a second,",
    "start": "51242",
    "end": "53200"
  },
  {
    "text": "but that is the central tension\nbetween static and contextual.",
    "start": "53200",
    "end": "57100"
  },
  {
    "text": "So the question is,\nare there good methods",
    "start": "57100",
    "end": "58920"
  },
  {
    "text": "for deriving static\nrepresentations",
    "start": "58920",
    "end": "61079"
  },
  {
    "text": "from the contextual ones\nthat these models offer?",
    "start": "61080",
    "end": "64589"
  },
  {
    "text": "And the answer from\nBommasani et al is yes.",
    "start": "64590",
    "end": "67350"
  },
  {
    "text": "They are effective\nmethods for doing this",
    "start": "67350",
    "end": "69072"
  },
  {
    "text": "and it's those\nmethods that will be",
    "start": "69072",
    "end": "70530"
  },
  {
    "text": "the focus of this screencast.",
    "start": "70530",
    "end": "72327"
  },
  {
    "text": "I really want to do two things\nthough for this lecture.",
    "start": "72328",
    "end": "74620"
  },
  {
    "text": "I would like to get\nhands on a little bit",
    "start": "74620",
    "end": "77400"
  },
  {
    "text": "with a high-level overview\nof models like BERT.",
    "start": "77400",
    "end": "80370"
  },
  {
    "text": "We're going to look later in\nthe quarter in much more detail",
    "start": "80370",
    "end": "82980"
  },
  {
    "text": "at how these models work.",
    "start": "82980",
    "end": "84260"
  },
  {
    "text": "So for now, we're just\ngoing to treat them",
    "start": "84260",
    "end": "86010"
  },
  {
    "text": "as kind of black box.",
    "start": "86010",
    "end": "87300"
  },
  {
    "text": "It's just like you might look\nup a GloVe representation",
    "start": "87300",
    "end": "89760"
  },
  {
    "text": "of a word and just get back\nthat representation and use it.",
    "start": "89760",
    "end": "92870"
  },
  {
    "text": "So too here, we can think\nof these models as devices",
    "start": "92870",
    "end": "95520"
  },
  {
    "text": "for feeding in sequences\nand getting back",
    "start": "95520",
    "end": "97950"
  },
  {
    "text": "lots and lots of representations\nthat we might use.",
    "start": "97950",
    "end": "100649"
  },
  {
    "text": "And later in the\nquarter, we'll come",
    "start": "100650",
    "end": "102300"
  },
  {
    "text": "to a deeper understanding\nof precisely",
    "start": "102300",
    "end": "104040"
  },
  {
    "text": "where those\nrepresentations come from.",
    "start": "104040",
    "end": "107273"
  },
  {
    "text": "And in addition,\nof course, I want",
    "start": "107273",
    "end": "108690"
  },
  {
    "text": "to give you an overview of these\nexciting methods from Bommasani",
    "start": "108690",
    "end": "111510"
  },
  {
    "text": "et al in the hopes that\nthey are useful to you",
    "start": "111510",
    "end": "114390"
  },
  {
    "text": "in developing your\noriginal system.",
    "start": "114390",
    "end": "118140"
  },
  {
    "start": "117000",
    "end": "262000"
  },
  {
    "text": "So let's start with\nthe structure of BERT.",
    "start": "118140",
    "end": "120470"
  },
  {
    "text": "BERT processes\nsequences, here I've",
    "start": "120470",
    "end": "122305"
  },
  {
    "text": "got a sequence, the class\ntoken, the day broke, SEP, class",
    "start": "122305",
    "end": "125900"
  },
  {
    "text": "and separate designated tokens,\nthe class token typically",
    "start": "125900",
    "end": "128600"
  },
  {
    "text": "starts the sequence and\nthen SEP ends the sequence.",
    "start": "128600",
    "end": "131150"
  },
  {
    "text": "It can be also used\ninternally in sequences",
    "start": "131150",
    "end": "133370"
  },
  {
    "text": "to mark boundaries\nwithin the sequence",
    "start": "133370",
    "end": "135260"
  },
  {
    "text": "that you're processing.",
    "start": "135260",
    "end": "136939"
  },
  {
    "text": "But the fundamental\nthing is that we",
    "start": "136940",
    "end": "138440"
  },
  {
    "text": "have the short sentence,\n\"the day broke.\"",
    "start": "138440",
    "end": "141020"
  },
  {
    "text": "BERT processes those into an\nembedding layer and then a lot",
    "start": "141020",
    "end": "144650"
  },
  {
    "text": "of additional layers.",
    "start": "144650",
    "end": "145954"
  },
  {
    "text": "And here I depicted 4, but it\ncould be 12 or even 24 layers.",
    "start": "145955",
    "end": "150530"
  },
  {
    "text": "What we're seeing here, the\nrectangles represent vectors.",
    "start": "150530",
    "end": "154430"
  },
  {
    "text": "They are the outputs of\neach layer in the network.",
    "start": "154430",
    "end": "158269"
  },
  {
    "text": "A lot of computation goes into\ncomputing those output vector",
    "start": "158270",
    "end": "162200"
  },
  {
    "text": "representations at each layer.",
    "start": "162200",
    "end": "163940"
  },
  {
    "text": "We're going to set that\ncomputation aside for now",
    "start": "163940",
    "end": "166160"
  },
  {
    "text": "so that we can just think\nof this as a grid of vector",
    "start": "166160",
    "end": "169340"
  },
  {
    "text": "representations.",
    "start": "169340",
    "end": "171250"
  },
  {
    "text": "Here is the crucial thing\nthat makes BERT contextual.",
    "start": "171250",
    "end": "174140"
  },
  {
    "text": "For different sequences\nthat we process,",
    "start": "174140",
    "end": "176400"
  },
  {
    "text": "we will get very\ndifferent representations.",
    "start": "176400",
    "end": "179090"
  },
  {
    "text": "In fact, individual\ntokens occurring",
    "start": "179090",
    "end": "182660"
  },
  {
    "text": "in different sequences will get\nvery different representations.",
    "start": "182660",
    "end": "185600"
  },
  {
    "text": "I've tried to signal\nthat with the colors",
    "start": "185600",
    "end": "187340"
  },
  {
    "text": "here so like the two sequences\nboth contain the word \"the\"",
    "start": "187340",
    "end": "190970"
  },
  {
    "text": "and the word \"broke.\"",
    "start": "190970",
    "end": "192137"
  },
  {
    "text": "But in virtue of the fact that\nthey have different surrounding",
    "start": "192137",
    "end": "194719"
  },
  {
    "text": "material and different\npositions in the sequence,",
    "start": "194720",
    "end": "197180"
  },
  {
    "text": "almost all of the\nrepresentations",
    "start": "197180",
    "end": "199069"
  },
  {
    "text": "will be different.",
    "start": "199070",
    "end": "201000"
  },
  {
    "text": "The class and SEP tokens\nmight have the same embedding,",
    "start": "201000",
    "end": "203630"
  },
  {
    "text": "but through all of these\nlayers because of the way",
    "start": "203630",
    "end": "205970"
  },
  {
    "text": "all these tokens are going\nto interact with each other",
    "start": "205970",
    "end": "208220"
  },
  {
    "text": "when we derive the\nrepresentations,",
    "start": "208220",
    "end": "209900"
  },
  {
    "text": "everything will be different.",
    "start": "209900",
    "end": "211230"
  },
  {
    "text": "We do not get a static\nrepresentation out",
    "start": "211230",
    "end": "213200"
  },
  {
    "text": "of these models.",
    "start": "213200",
    "end": "214220"
  },
  {
    "text": "And I've specified that\neven in the embedding",
    "start": "214220",
    "end": "216770"
  },
  {
    "text": "layer, if the positions\nof the words vary,",
    "start": "216770",
    "end": "219200"
  },
  {
    "text": "one and the same token will\nget different representations.",
    "start": "219200",
    "end": "221900"
  },
  {
    "text": "The reason for that is that\nthis embedding layer is actually",
    "start": "221900",
    "end": "225079"
  },
  {
    "text": "hiding two components.",
    "start": "225080",
    "end": "227040"
  },
  {
    "text": "We do at the very\ncenter of this model",
    "start": "227040",
    "end": "229730"
  },
  {
    "text": "have a fixed static embedding,\nwhere we can look up",
    "start": "229730",
    "end": "232340"
  },
  {
    "text": "individual word sequences.",
    "start": "232340",
    "end": "234290"
  },
  {
    "text": "But for this thing that I've\ncalled the embedding layer,",
    "start": "234290",
    "end": "236689"
  },
  {
    "text": "that static\nrepresentation is combined",
    "start": "236690",
    "end": "238760"
  },
  {
    "text": "with a separate\npositional encoding",
    "start": "238760",
    "end": "240530"
  },
  {
    "text": "from a separate\nembedding space, and that",
    "start": "240530",
    "end": "242810"
  },
  {
    "text": "delivers what I've called\nthe embedding layer here.",
    "start": "242810",
    "end": "245090"
  },
  {
    "text": "And that means that\neven at this first layer",
    "start": "245090",
    "end": "247610"
  },
  {
    "text": "because, for\nexample, \"the\" occurs",
    "start": "247610",
    "end": "249590"
  },
  {
    "text": "in different points\nin the sequence,",
    "start": "249590",
    "end": "251580"
  },
  {
    "text": "it will get different\nrepresentations even",
    "start": "251580",
    "end": "253520"
  },
  {
    "text": "in the embedding space.",
    "start": "253520",
    "end": "255150"
  },
  {
    "text": "And from there, of course, as\nwe travel through these layers,",
    "start": "255150",
    "end": "257778"
  },
  {
    "text": "we expect even more\nthings to change",
    "start": "257779",
    "end": "259759"
  },
  {
    "text": "about the representations.",
    "start": "259760",
    "end": "263490"
  },
  {
    "start": "262000",
    "end": "359000"
  },
  {
    "text": "A second important\npreliminary is",
    "start": "263490",
    "end": "265919"
  },
  {
    "text": "to give some attention to\nhow BERT and models like it",
    "start": "265920",
    "end": "268860"
  },
  {
    "text": "tokenize sequences.",
    "start": "268860",
    "end": "270676"
  },
  {
    "text": "And here I'm giving you a\nbit of code in the hopes",
    "start": "270677",
    "end": "272760"
  },
  {
    "text": "that you can get hands-on\nand get a feel for how",
    "start": "272760",
    "end": "274800"
  },
  {
    "text": "these tokenizers behave.",
    "start": "274800",
    "end": "276349"
  },
  {
    "text": "I'm taking advantage of\nthe Hugging Face library.",
    "start": "276350",
    "end": "278700"
  },
  {
    "text": "I have loaded a BERT\ntokenizer and I load that",
    "start": "278700",
    "end": "281460"
  },
  {
    "text": "from a pre-trained model.",
    "start": "281460",
    "end": "283538"
  },
  {
    "text": "In cell 3, you can see that I've\ncalled the tokenize function",
    "start": "283538",
    "end": "286080"
  },
  {
    "text": "on the sentence.",
    "start": "286080",
    "end": "286770"
  },
  {
    "text": "This isn't too surprising and\nthe result is a pretty normal",
    "start": "286770",
    "end": "290069"
  },
  {
    "text": "looking sequence of tokens.",
    "start": "290070",
    "end": "291760"
  },
  {
    "text": "You see some punctuation\nhas been separated off,",
    "start": "291760",
    "end": "294100"
  },
  {
    "text": "but you also see a lot of words.",
    "start": "294100",
    "end": "296220"
  },
  {
    "text": "When you get down\nto cell 4, though,",
    "start": "296220",
    "end": "297720"
  },
  {
    "text": "for the sequence \"encode me\"\nthis is a bit surprising.",
    "start": "297720",
    "end": "300760"
  },
  {
    "text": "The word encode in\nthe input that's",
    "start": "300760",
    "end": "302370"
  },
  {
    "text": "been broken apart into\ntwo subword tokens, \"en\"",
    "start": "302370",
    "end": "306180"
  },
  {
    "text": "and then \"code\" with these\nboundary markers on it.",
    "start": "306180",
    "end": "308955"
  },
  {
    "text": "BERT has broken that apart\ninto two subword sequences.",
    "start": "308955",
    "end": "312689"
  },
  {
    "text": "And if I feed in a sequence\nthat has a really unfamiliar set",
    "start": "312690",
    "end": "316620"
  },
  {
    "text": "of tokens in it, it will\ndo a lot of breaking apart",
    "start": "316620",
    "end": "320370"
  },
  {
    "text": "of that sequence, as you can\nsee in cell 5 for the input",
    "start": "320370",
    "end": "323010"
  },
  {
    "text": "\"snuffleupagus\" where a lot\nof these pieces have come out.",
    "start": "323010",
    "end": "326700"
  },
  {
    "text": "This is the essential\npiece for why",
    "start": "326700",
    "end": "328620"
  },
  {
    "text": "BERT is able to have such a\nsmall vocabulary, only about",
    "start": "328620",
    "end": "331470"
  },
  {
    "text": "30,000 words, compare that\nwith the 400,000 words",
    "start": "331470",
    "end": "334890"
  },
  {
    "text": "that are in the GloVe space.",
    "start": "334890",
    "end": "336360"
  },
  {
    "text": "The reason it can\nget away with that",
    "start": "336360",
    "end": "337919"
  },
  {
    "text": "is that it does a\nlot of breaking apart",
    "start": "337920",
    "end": "339690"
  },
  {
    "text": "of words into subword tokens.",
    "start": "339690",
    "end": "342390"
  },
  {
    "text": "And of course, because\nthe model is contextual,",
    "start": "342390",
    "end": "344550"
  },
  {
    "text": "we have an expectation that,\nfor example, when it encounters",
    "start": "344550",
    "end": "347789"
  },
  {
    "text": "code here in the context of\n\"en\" at some conceptual level,",
    "start": "347790",
    "end": "351570"
  },
  {
    "text": "the model will recognize that it\nhas processed the word \"encode\"",
    "start": "351570",
    "end": "354510"
  },
  {
    "text": "even though there was two\ntokens underlining it.",
    "start": "354510",
    "end": "359120"
  },
  {
    "start": "359000",
    "end": "447000"
  },
  {
    "text": "Let's flesh this out a bit by\nlooking at the full interface",
    "start": "359120",
    "end": "361669"
  },
  {
    "text": "for dealing with these\nmodels and, again,",
    "start": "361670",
    "end": "363270"
  },
  {
    "text": "taking advantage\nof Hugging Face.",
    "start": "363270",
    "end": "364645"
  },
  {
    "text": "I'm going to load a BERT\nmodel and a BERT tokenizer.",
    "start": "364645",
    "end": "367849"
  },
  {
    "text": "It's important that they use the\nsame pre-trained weights, which",
    "start": "367850",
    "end": "370820"
  },
  {
    "text": "Hugging Face will download\nfor you from the web.",
    "start": "370820",
    "end": "373358"
  },
  {
    "text": "And so those are\ntied in and I set up",
    "start": "373358",
    "end": "374900"
  },
  {
    "text": "the tokenizer and the model.",
    "start": "374900",
    "end": "376130"
  },
  {
    "text": "If I call tokenizer.encode\non a sequence,",
    "start": "376130",
    "end": "378335"
  },
  {
    "text": "it will give me back\na list of indices.",
    "start": "378335",
    "end": "381110"
  },
  {
    "text": "And those indices will\nbe used as a lookup",
    "start": "381110",
    "end": "383000"
  },
  {
    "text": "to start the process of\ncomputing this entire sequence.",
    "start": "383000",
    "end": "386090"
  },
  {
    "text": "In cell 6, I actually\nuse the model",
    "start": "386090",
    "end": "387980"
  },
  {
    "text": "to derive that grid\nof representations.",
    "start": "387980",
    "end": "390530"
  },
  {
    "text": "Hugging Face is\ngiving us an object",
    "start": "390530",
    "end": "392210"
  },
  {
    "text": "that has a lot of attributes.",
    "start": "392210",
    "end": "393500"
  },
  {
    "text": "If I call output_hidden_states\nequals true when I use",
    "start": "393500",
    "end": "397220"
  },
  {
    "text": "the model here, but I can call\n.hidden_states and get that",
    "start": "397220",
    "end": "400520"
  },
  {
    "text": "full grid of representations\nthat I showed you before.",
    "start": "400520",
    "end": "403729"
  },
  {
    "text": "This is a sequence\nwith 13 layers.",
    "start": "403730",
    "end": "405470"
  },
  {
    "text": "That's 1 embedding layer plus\n12 of the additional layers.",
    "start": "405470",
    "end": "409790"
  },
  {
    "text": "And if I key into one\nof the first layer,",
    "start": "409790",
    "end": "411860"
  },
  {
    "text": "that will be the embedding.",
    "start": "411860",
    "end": "413150"
  },
  {
    "text": "You can see that its\nshape is 1 by 5 by 768.",
    "start": "413150",
    "end": "416970"
  },
  {
    "text": "This is the batch\nof one example.",
    "start": "416970",
    "end": "419060"
  },
  {
    "text": "It has five tokens.",
    "start": "419060",
    "end": "420770"
  },
  {
    "text": "The three that we can see here\nplus the class and SEP tokens.",
    "start": "420770",
    "end": "424280"
  },
  {
    "text": "And each one of those tokens\nin the embedding layer",
    "start": "424280",
    "end": "426755"
  },
  {
    "text": "is represented by a\nvector of dimension 768.",
    "start": "426755",
    "end": "430837"
  },
  {
    "text": "And that remains consistent\nthrough all the layers",
    "start": "430837",
    "end": "432920"
  },
  {
    "text": "in the model.",
    "start": "432920",
    "end": "433540"
  },
  {
    "text": "So I went to the\nfinal output states,",
    "start": "433540",
    "end": "435320"
  },
  {
    "text": "I again just index into\n.hidden_states here.",
    "start": "435320",
    "end": "438260"
  },
  {
    "text": "The shape is the\nsame and that will be",
    "start": "438260",
    "end": "440120"
  },
  {
    "text": "consistent for all the layers.",
    "start": "440120",
    "end": "442962"
  },
  {
    "text": "Those are the preliminaries.",
    "start": "442963",
    "end": "444130"
  },
  {
    "text": "And let's think about\nhow we could derive",
    "start": "444130",
    "end": "445838"
  },
  {
    "text": "some static representations.",
    "start": "445838",
    "end": "447430"
  },
  {
    "start": "447000",
    "end": "541000"
  },
  {
    "text": "The first approach that\nBommasani et al considered",
    "start": "447430",
    "end": "450280"
  },
  {
    "text": "is what they call the\ndecontextualized approach",
    "start": "450280",
    "end": "452770"
  },
  {
    "text": "and this is like the\nsimplest thing possible.",
    "start": "452770",
    "end": "455139"
  },
  {
    "text": "We are just going to process\nindividual words as though they",
    "start": "455140",
    "end": "458290"
  },
  {
    "text": "were sequences and see if we\ncan make any sense of them.",
    "start": "458290",
    "end": "461660"
  },
  {
    "text": "So we would start by feeding\nin a word like \"kitten\"",
    "start": "461660",
    "end": "464380"
  },
  {
    "text": "and we would allow the\nmodel to break it apart",
    "start": "464380",
    "end": "466420"
  },
  {
    "text": "into its subword pieces.",
    "start": "466420",
    "end": "468250"
  },
  {
    "text": "And then we simply process\nthat with the model,",
    "start": "468250",
    "end": "470290"
  },
  {
    "text": "we get a full grid\nof representations.",
    "start": "470290",
    "end": "473110"
  },
  {
    "text": "Now because we potentially\nhave subword tokens here,",
    "start": "473110",
    "end": "475870"
  },
  {
    "text": "we need some pooling function.",
    "start": "475870",
    "end": "477680"
  },
  {
    "text": "So what we can do is\njust pool using something",
    "start": "477680",
    "end": "479770"
  },
  {
    "text": "like mean to get a fixed static\nrepresentation of dimension 768",
    "start": "479770",
    "end": "485349"
  },
  {
    "text": "for this individual word.",
    "start": "485350",
    "end": "487233"
  },
  {
    "text": "And of course, we don't\nhave to use the final layer,",
    "start": "487233",
    "end": "489400"
  },
  {
    "text": "we can use lower down layers.",
    "start": "489400",
    "end": "491080"
  },
  {
    "text": "And we don't have to use\nmean as the pooling function.",
    "start": "491080",
    "end": "493330"
  },
  {
    "text": "You could consider\nsomething like max or min",
    "start": "493330",
    "end": "495759"
  },
  {
    "text": "or even last, which\nwould just disregard",
    "start": "495760",
    "end": "498010"
  },
  {
    "text": "all of the representations\nexcept for the one",
    "start": "498010",
    "end": "500830"
  },
  {
    "text": "corresponding to the\nfinal subword token.",
    "start": "500830",
    "end": "505689"
  },
  {
    "text": "This is really simple.",
    "start": "505690",
    "end": "507530"
  },
  {
    "text": "It's potentially\nunnatural, though.",
    "start": "507530",
    "end": "509560"
  },
  {
    "text": "BERT is a contextual model.",
    "start": "509560",
    "end": "510850"
  },
  {
    "text": "It was trained on\nfull sequences.",
    "start": "510850",
    "end": "512559"
  },
  {
    "text": "And especially if we leave\noff the class and SEP tokens,",
    "start": "512559",
    "end": "515140"
  },
  {
    "text": "we might be feeding in\nsequences that BERT has really",
    "start": "515140",
    "end": "518440"
  },
  {
    "text": "never seen before.",
    "start": "518440",
    "end": "519840"
  },
  {
    "text": "And so it might be\nunknown how it's",
    "start": "519840",
    "end": "521380"
  },
  {
    "text": "going to behave with\nthese unusual inputs.",
    "start": "521380",
    "end": "523900"
  },
  {
    "text": "Nonetheless, though, we\ncould repeat this process",
    "start": "523900",
    "end": "526120"
  },
  {
    "text": "for all the words\nin our vocabulary",
    "start": "526120",
    "end": "528010"
  },
  {
    "text": "and derive a static\nembedding space",
    "start": "528010",
    "end": "530050"
  },
  {
    "text": "and maybe it has some promise.",
    "start": "530050",
    "end": "532240"
  },
  {
    "text": "However, to address this\npotential unnaturalness",
    "start": "532240",
    "end": "535450"
  },
  {
    "text": "and potentially take more\nadvantage of the virtues",
    "start": "535450",
    "end": "538720"
  },
  {
    "text": "that BERT and\nrelated models have,",
    "start": "538720",
    "end": "540819"
  },
  {
    "text": "Bommasani et al consider\nalso the aggregated approach.",
    "start": "540820",
    "end": "544400"
  },
  {
    "start": "541000",
    "end": "603000"
  },
  {
    "text": "So in this approach, you\nprocess lots of corpus examples",
    "start": "544400",
    "end": "547420"
  },
  {
    "text": "that contain your target word.",
    "start": "547420",
    "end": "549130"
  },
  {
    "text": "You've got that sort\nof glimpse of a corpus.",
    "start": "549130",
    "end": "551800"
  },
  {
    "text": "Our target word is\nkitten, of course,",
    "start": "551800",
    "end": "553810"
  },
  {
    "text": "we allow it to be broken\napart into subword tokens.",
    "start": "553810",
    "end": "556480"
  },
  {
    "text": "The full sequences\nin these examples",
    "start": "556480",
    "end": "558490"
  },
  {
    "text": "would also be broken\napart into subword tokens.",
    "start": "558490",
    "end": "561152"
  },
  {
    "text": "But the important thing\nis that our target word",
    "start": "561152",
    "end": "563110"
  },
  {
    "text": "might have subword tokens.",
    "start": "563110",
    "end": "564579"
  },
  {
    "text": "We pool those as we did before\nfor the decontextualized",
    "start": "564580",
    "end": "567760"
  },
  {
    "text": "approach and we're\nalso going to pool",
    "start": "567760",
    "end": "569770"
  },
  {
    "text": "across all of the\ndifferent context examples",
    "start": "569770",
    "end": "572080"
  },
  {
    "text": "that we processed.",
    "start": "572080",
    "end": "573970"
  },
  {
    "text": "And the result of that should\nbe a bunch of natural inputs",
    "start": "573970",
    "end": "577209"
  },
  {
    "text": "to the model.",
    "start": "577210",
    "end": "578180"
  },
  {
    "text": "But in the end, we derive\na static representation",
    "start": "578180",
    "end": "580630"
  },
  {
    "text": "that is some kind of average\nacross all of the examples",
    "start": "580630",
    "end": "583720"
  },
  {
    "text": "that we processed.",
    "start": "583720",
    "end": "585220"
  },
  {
    "text": "This seems very natural.",
    "start": "585220",
    "end": "586490"
  },
  {
    "text": "It's taking advantage\nof what BERT is best at.",
    "start": "586490",
    "end": "589000"
  },
  {
    "text": "I will warn you, though, that\nthis is very computationally",
    "start": "589000",
    "end": "591640"
  },
  {
    "text": "demanding.",
    "start": "591640",
    "end": "592150"
  },
  {
    "text": "We're going to want to\nprocess lots of examples,",
    "start": "592150",
    "end": "594340"
  },
  {
    "text": "and BERT requires\nlots of resources",
    "start": "594340",
    "end": "596290"
  },
  {
    "text": "because it develops really large\nrepresentations as we've seen,",
    "start": "596290",
    "end": "600370"
  },
  {
    "text": "but it might be worth it.",
    "start": "600370",
    "end": "602300"
  },
  {
    "text": "Now, Bommasani et al\noffer lots of results",
    "start": "602300",
    "end": "604670"
  },
  {
    "start": "603000",
    "end": "736000"
  },
  {
    "text": "that help us understand\nthese approaches",
    "start": "604670",
    "end": "606339"
  },
  {
    "text": "and how they perform.",
    "start": "606340",
    "end": "608038"
  },
  {
    "text": "Let me give you a glimpse of\nthem as a kind of summary.",
    "start": "608038",
    "end": "610330"
  },
  {
    "text": "So what we've got here is\nresults for the SimVerb 3,500",
    "start": "610330",
    "end": "613840"
  },
  {
    "text": "dataset, a word\nsimilarity data set",
    "start": "613840",
    "end": "615978"
  },
  {
    "text": "that's very similar to\nthe ones that you'll",
    "start": "615978",
    "end": "617770"
  },
  {
    "text": "be working with on the\nhomework and bake-off.",
    "start": "617770",
    "end": "620440"
  },
  {
    "text": "Our metric is Spearman\ncorrelation and higher",
    "start": "620440",
    "end": "622780"
  },
  {
    "text": "is better.",
    "start": "622780",
    "end": "623440"
  },
  {
    "text": "That's along the y-axis.",
    "start": "623440",
    "end": "625000"
  },
  {
    "text": "And along the x-axis, I\nhave the layer in the model",
    "start": "625000",
    "end": "627580"
  },
  {
    "text": "that we're keying into.",
    "start": "627580",
    "end": "628842"
  },
  {
    "text": "And then, of course,\nwhat we should watch",
    "start": "628842",
    "end": "630550"
  },
  {
    "text": "is that we have two\npooling functions f and g.",
    "start": "630550",
    "end": "633160"
  },
  {
    "text": "f is subword pooling\nand g is context pooling",
    "start": "633160",
    "end": "636399"
  },
  {
    "text": "for models that\nhave it, and it's",
    "start": "636400",
    "end": "637840"
  },
  {
    "text": "decont for the\ndecontextualized approach.",
    "start": "637840",
    "end": "641380"
  },
  {
    "text": "Now we have a very clear\nresult across these results,",
    "start": "641380",
    "end": "644050"
  },
  {
    "text": "and I think across all\nthe results in the paper.",
    "start": "644050",
    "end": "646180"
  },
  {
    "text": "Lower layers are better.",
    "start": "646180",
    "end": "648370"
  },
  {
    "text": "Lower layers are giving us good\nhigh fidelity representations",
    "start": "648370",
    "end": "651339"
  },
  {
    "text": "of individual words.",
    "start": "651340",
    "end": "652300"
  },
  {
    "text": "As we travel higher\nin the model we",
    "start": "652300",
    "end": "654040"
  },
  {
    "text": "seem to lose a lot of that\nword level discrimination.",
    "start": "654040",
    "end": "658630"
  },
  {
    "text": "In addition, your\nbest choice is to do",
    "start": "658630",
    "end": "661270"
  },
  {
    "text": "mean pooling for the\ncontext and subword pooling",
    "start": "661270",
    "end": "664840"
  },
  {
    "text": "seems to matter less, right?",
    "start": "664840",
    "end": "666070"
  },
  {
    "text": "All of these lines here\nare all for the context",
    "start": "666070",
    "end": "668770"
  },
  {
    "text": "pooling model with mean as\nyour context pooling function.",
    "start": "668770",
    "end": "672640"
  },
  {
    "text": "The very best choice,\nthough, I think consistently",
    "start": "672640",
    "end": "674980"
  },
  {
    "text": "is mean for both of these\npooling functions here.",
    "start": "674980",
    "end": "678677"
  },
  {
    "text": "You can see that in\nthis result, and I",
    "start": "678677",
    "end": "680260"
  },
  {
    "text": "think that's consistent across\nall the results in the paper.",
    "start": "680260",
    "end": "683020"
  },
  {
    "text": "But the overall takeaway\nhere is that, as expected,",
    "start": "683020",
    "end": "686830"
  },
  {
    "text": "the aggregated\napproach is better",
    "start": "686830",
    "end": "688570"
  },
  {
    "text": "than the decontextualized\napproach.",
    "start": "688570",
    "end": "690770"
  },
  {
    "text": "However, if you don't have the\ncomputational budget for that,",
    "start": "690770",
    "end": "693820"
  },
  {
    "text": "then mean pooling and the\ndecontextualized approach",
    "start": "693820",
    "end": "696520"
  },
  {
    "text": "looks really competitive.",
    "start": "696520",
    "end": "698260"
  },
  {
    "text": "That's not so\nevident in this spot,",
    "start": "698260",
    "end": "700030"
  },
  {
    "text": "but if you look across all\nthe results in the paper,",
    "start": "700030",
    "end": "702460"
  },
  {
    "text": "I think that's a\npretty clear finding.",
    "start": "702460",
    "end": "704450"
  },
  {
    "text": "So that would be a good choice.",
    "start": "704450",
    "end": "706010"
  },
  {
    "text": "And one thing is clear,\nthat simple approach",
    "start": "706010",
    "end": "708580"
  },
  {
    "text": "is better than some\nkinds of context",
    "start": "708580",
    "end": "710950"
  },
  {
    "text": "pooling where you choose the\nwrong context pooling function",
    "start": "710950",
    "end": "713800"
  },
  {
    "text": "like min or max.",
    "start": "713800",
    "end": "715180"
  },
  {
    "text": "Despite all of the effort that\nwent into this set of results",
    "start": "715180",
    "end": "718240"
  },
  {
    "text": "and also these,\nthey're all kind of",
    "start": "718240",
    "end": "720250"
  },
  {
    "text": "down here entangled with the\ndecontextualized approach.",
    "start": "720250",
    "end": "723790"
  },
  {
    "text": "But mean as the pooling\nfunction there is really",
    "start": "723790",
    "end": "727180"
  },
  {
    "text": "an outstanding choice, as you\ncan see from these results.",
    "start": "727180",
    "end": "730950"
  },
  {
    "start": "730950",
    "end": "735000"
  }
]