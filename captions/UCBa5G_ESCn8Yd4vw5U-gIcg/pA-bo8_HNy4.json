[
  {
    "text": "All right. Welcome back everyone [NOISE]. So this is Lecture 17 of CS229.",
    "start": "7880",
    "end": "13965"
  },
  {
    "text": "The topics for today will be, uh, to finish, uh, the expectation maximization algorithm.",
    "start": "13965",
    "end": "19244"
  },
  {
    "text": "We'll prove the convergence of expectation maximization algorithm, and then we'll apply it, uh;",
    "start": "19245",
    "end": "24869"
  },
  {
    "text": "the expectation maximum- uh, maximization algorithm to the Gaussian mixture model that we saw last, um, in the last class.",
    "start": "24870",
    "end": "31755"
  },
  {
    "text": "And then we'll move on to a new model, uh, called the factor analysis [NOISE] and solve factor analysis,",
    "start": "31755",
    "end": "38685"
  },
  {
    "text": "again, using expectation maximization. So that's the plan for today. And before we jump into today's topics,",
    "start": "38685",
    "end": "44420"
  },
  {
    "text": "a quick recap of what we saw in the last class. So in the last class, we started off with unsupervised learning, right?",
    "start": "44420",
    "end": "51140"
  },
  {
    "text": "And in unsupervised learning, we are given just, um, data points, a set of xs,",
    "start": "51140",
    "end": "56465"
  },
  {
    "text": "and there is no supervision that comes along with it. What I mean is there's no y-values that come along with the, uh, corresponding x-values.",
    "start": "56465",
    "end": "63350"
  },
  {
    "text": "We're just given the x-values. And we started off with clustering problems, right? Clustering problems are those where which we want to, uh,",
    "start": "63350",
    "end": "71270"
  },
  {
    "text": "identify some kind of a hidden structure where the data kind of clusters into different groups.",
    "start": "71270",
    "end": "76700"
  },
  {
    "text": "The first algorithm that we saw was k-means. In k-means, we are given a collection of, uh, examples; xs.",
    "start": "76700",
    "end": "85235"
  },
  {
    "text": "And also we are- let's assume we are also given, uh, the nu- the number k of how many clusters we want to identify.",
    "start": "85235",
    "end": "93035"
  },
  {
    "text": "And the way we go about, um, executing k-means is to come up- uh,",
    "start": "93035",
    "end": "98884"
  },
  {
    "text": "is to randomly initialize k different cluster means where Mu_j refers to the j_th cluster.",
    "start": "98885",
    "end": "106125"
  },
  {
    "text": "There are k such Mu_js. Each of those, uh, cluster centroids; they are called centroids, they live in R_d.",
    "start": "106125",
    "end": "112829"
  },
  {
    "text": "The same space where our x lives in. And then we loop until convergence, where in step 1,",
    "start": "112830",
    "end": "119620"
  },
  {
    "text": "which you can think of as the E-step, uh, we set the cluster identity c_i of the i_th element to be the identity of the j_th,",
    "start": "119620",
    "end": "130840"
  },
  {
    "text": "um, uh, cluster centroid to which it is closest, to which it's nearest. Right? So there are k different Mus.",
    "start": "130840",
    "end": "138315"
  },
  {
    "text": "And for each i, we check which of the k Mus is nearest to it.",
    "start": "138315",
    "end": "145165"
  },
  {
    "text": "We- where it's commonly, we use the l_2 distance. And depending on which one is the nearest, we use the arg min.",
    "start": "145165",
    "end": "151230"
  },
  {
    "text": "So the- the identity of the cluster to which x_i is nearest, we set that to be c_i. And then what you can think of as the M-step is for each of the cluster centroids,",
    "start": "151230",
    "end": "163220"
  },
  {
    "text": "we only look at the examples which got assigned to that centroid in the previous E-step,",
    "start": "163220",
    "end": "168995"
  },
  {
    "text": "and take the average of those xs to update our estimate of Mu_j. Right? It's this iterative algorithm where in",
    "start": "168995",
    "end": "176510"
  },
  {
    "text": "one step we perform a cluster center assignment. So this is assignment where we assign each point to a centroid,",
    "start": "176510",
    "end": "183184"
  },
  {
    "text": "and then update the centroids using the mean of the assigned points. And we also, uh,",
    "start": "183184",
    "end": "190260"
  },
  {
    "text": "briefly discussed of how this overall algorithm can be thought of as coordinate descent on this loss function,",
    "start": "190260",
    "end": "197209"
  },
  {
    "text": "which is also called the distortion function. So the distortion function takes two parameters: the,",
    "start": "197210",
    "end": "202550"
  },
  {
    "text": "uh- the cs and the Mus; the set of all cs and the set of all Mus.",
    "start": "202550",
    "end": "207605"
  },
  {
    "text": "And this loss function can be minimized using coordinate descent.",
    "start": "207605",
    "end": "212885"
  },
  {
    "text": "By coordinate descent what we mean is, we hold some subset of the parameters fixed and optimize over the remaining, uh,",
    "start": "212885",
    "end": "220010"
  },
  {
    "text": "uh, subset of the parameters, and then we hold the remaining parameters fixed and optimize over the first,",
    "start": "220010",
    "end": "225305"
  },
  {
    "text": "uh, set of parameters, and we go on and on. All right. So, uh, you can think of this step as",
    "start": "225305",
    "end": "231394"
  },
  {
    "text": "optimizing over the c parameter's while holding the Mu's fixed, and this step as optimizing the Mu parameters while holding the c is fixed.",
    "start": "231395",
    "end": "240310"
  },
  {
    "text": "Right? So that was k-means. And then we saw this other model called [NOISE] Gaussian mixture model.",
    "start": "240310",
    "end": "247834"
  },
  {
    "text": "So Gaussian mixture model, we can, uh, think of it as soft k-means. [NOISE] In Gaussian mixture model, we are again,",
    "start": "247835",
    "end": "259745"
  },
  {
    "text": "given k, the number of clusters or the number of, uh, ga- mixtures- the Gaussian mixtures,",
    "start": "259745",
    "end": "265130"
  },
  {
    "text": "uh- um, that we wanna, um, uh, fit. We are given the set of xs.",
    "start": "265130",
    "end": "270170"
  },
  {
    "text": "We don't know the cluster identity. If this was supervised learning, then the cluster identities would be the y_is.",
    "start": "270170",
    "end": "276890"
  },
  {
    "text": "And again here if supervise- if this was supervised learning, we would be given the c_is, which would be like the y_is.",
    "start": "276890",
    "end": "283570"
  },
  {
    "text": "And we assume that the gen- the data generative model is something like this.",
    "start": "283570",
    "end": "289910"
  },
  {
    "text": "There is, uh, uh- so we have a multinomial distribution, which you can think of it as like the class prior.",
    "start": "289910",
    "end": "296525"
  },
  {
    "text": "You know, what- what's the total number of examples in each given cluster? That- you- you can think of that as a multinomial?",
    "start": "296525",
    "end": "303710"
  },
  {
    "text": "And the z_i is the cluster centroids- uh, I mean, the cluster identities of, uh,",
    "start": "303710",
    "end": "310335"
  },
  {
    "text": "every corresponding x_is, are sampled from this multinomial. Right? And then once we know the z_is,",
    "start": "310335",
    "end": "317229"
  },
  {
    "text": "each- for each identity of z_i there is a corresponding Mu and z.",
    "start": "317229",
    "end": "323170"
  },
  {
    "text": "Right? And the xs are sampled from the corresponding Gaussian based on the value that z was sampled from.",
    "start": "323170",
    "end": "331130"
  },
  {
    "text": "So it's important to note that z in this case is discrete. Okay? Because it is sampled from a multinomial, we get- we- uh,",
    "start": "331130",
    "end": "339530"
  },
  {
    "text": "z will happen to be one of the k different, um, uh, uh- one of the k different, uh, Gaussian mixtures.",
    "start": "339530",
    "end": "345620"
  },
  {
    "text": "And by the choice of z, there is an implicit Mu and Sigma associated with that Gaussian,",
    "start": "345620",
    "end": "354449"
  },
  {
    "text": "and the x will be sampled from that Gaussian. Right? We think of this as the data generating distribution.",
    "start": "354450",
    "end": "360665"
  },
  {
    "text": "And so the parameters that are in this model are the Phi, Mu, and- and- and- and Sigmas.",
    "start": "360665",
    "end": "366650"
  },
  {
    "text": "In this case, the parameters was- was both the c. C_i and the Mu_i were the parameters.",
    "start": "366650",
    "end": "375240"
  },
  {
    "text": "So the parameters are in the green boxes. Right? And then we came up with this- with this,",
    "start": "375240",
    "end": "380600"
  },
  {
    "text": "uh, iterative algorithm again. When we came up with this, we were just using heuristics to make it something that is similar to k-means.",
    "start": "380600",
    "end": "387095"
  },
  {
    "text": "We had not yet derived the EM algorithm in its- in its, uh, uh, most general form,",
    "start": "387095",
    "end": "392135"
  },
  {
    "text": "and with the algorithm that we came up was this. So set- so randomly initialize the parameters to some values just as in the case of k-means,",
    "start": "392135",
    "end": "400885"
  },
  {
    "text": "and then we set, uh, w_i for each example. Okay? So this loop is over the number of examples.",
    "start": "400885",
    "end": "407420"
  },
  {
    "text": "For each examples, we set w_i to be pro- the probability that, uh,",
    "start": "407420",
    "end": "413240"
  },
  {
    "text": "z_i- the- the probability that z_i is the- is the, uh, um- z_i equals j given x_i,",
    "start": "413240",
    "end": "423225"
  },
  {
    "text": "and- and the, uh, current- uh, uh, current values of the parameters. And then using these- these, uh, weights, where,",
    "start": "423225",
    "end": "432135"
  },
  {
    "text": "uh- the weight is basically telling us what's the probability that, um, um, x_i corresponds to the j_th, um, uh, cluster.",
    "start": "432135",
    "end": "440720"
  },
  {
    "text": "So the sum of all j of these w_is will be equal to 1 for every i.",
    "start": "440720",
    "end": "445820"
  },
  {
    "text": "And using these weights, we perform what- what we call as the m-step, which was to recalculate or update the values of the parameters using the,",
    "start": "445820",
    "end": "455610"
  },
  {
    "text": "uh, current estimates of the weights. Right? And this again, was very similar to what we did with k-means.",
    "start": "455610",
    "end": "461699"
  },
  {
    "text": "Right? So the- the red box think of it as the E-step, and the green boxes think of them as the M-step.",
    "start": "461700",
    "end": "467670"
  },
  {
    "text": "Right? And the green boxes, we are updating, the, uh, the parameter based on what we, uh, uh, thought off,",
    "start": "467670",
    "end": "474410"
  },
  {
    "text": "uh- based on what we- uh, what we derived in the E-step. Right? In the- in the- ah,",
    "start": "474410",
    "end": "480625"
  },
  {
    "text": "in the case of Gaussians- a mixture of Gaussians, each cluster centroid will be the weighted average of the x_is.",
    "start": "480625",
    "end": "488754"
  },
  {
    "text": "In k-means, it was just the average of the x_is which got assigned to that cluster. But over here, every- uh,",
    "start": "488755",
    "end": "495155"
  },
  {
    "text": "every example belongs to every cluster with different weights. So this is just a weighted means of all the xs.",
    "start": "495155",
    "end": "501800"
  },
  {
    "text": "And- you know, we have, um, um, corresponding, uh, uh, parameter update rules for the Phis and- and- and Sigma_js.",
    "start": "501800",
    "end": "508880"
  },
  {
    "text": "And basically, all of these, uh, derivations was very similar to the case of GDA;",
    "start": "508880",
    "end": "514378"
  },
  {
    "text": "Gaussian discriminant analysis, except, you know, we have a small- a few small, uh, uh, changes.",
    "start": "514379",
    "end": "520065"
  },
  {
    "text": "The changes are that each cluster centroid can have its own covariance rather than having a shared covariance,",
    "start": "520065",
    "end": "526190"
  },
  {
    "text": "and also that- that there are k such, uh, um- there are clusters rather than just two.",
    "start": "526190",
    "end": "531515"
  },
  {
    "text": "Right? Otherwise, this is pretty much exactly the same as GDM. And then we moved on to deriving the- the most general form of the EM algorithm.",
    "start": "531515",
    "end": "542135"
  },
  {
    "text": "Right? In the EM algorithm, what we, uh, wish to maximize is this marginal likelihood.",
    "start": "542135",
    "end": "549630"
  },
  {
    "text": "That's log p of x, uh, um, for a given value of Theta. Right? This is our likelihood objective.",
    "start": "549630",
    "end": "557485"
  },
  {
    "text": "Had we observed the full data, had we observed all the zs like in the case of, uh, GDA,",
    "start": "557485",
    "end": "564480"
  },
  {
    "text": "we would- we would instead maximize [NOISE] p of x, z Theta if z was observed.",
    "start": "564480",
    "end": "574779"
  },
  {
    "text": "But z is not observed, so the- the- the- the most rational thing to do is just to maximize the,",
    "start": "576780",
    "end": "583390"
  },
  {
    "text": "uh- uh, likelihood given the evidence that we have, right? So P of x is also called the evidence. And we do some basi- some-",
    "start": "583390",
    "end": "591700"
  },
  {
    "text": "some- some pretty straightforward, uh, algebraic manipulations. So log P of x is the same as log of the,",
    "start": "591700",
    "end": "599965"
  },
  {
    "text": "um, log of P of x, z with z getting marginalized, right? That's by definition log P of x.",
    "start": "599965",
    "end": "606834"
  },
  {
    "text": "And then we kind of cook up an expectation out of- out of nothing by multiplying and dividing it by some arbitrary,",
    "start": "606835",
    "end": "614140"
  },
  {
    "text": "uh, uh, distribution, q over z. And once we multiply it and divide it,",
    "start": "614140",
    "end": "619935"
  },
  {
    "text": "we can now think of this as an expectation, right? So we had a log and we cooked up an expectation out of nothing, right?",
    "start": "619935",
    "end": "628515"
  },
  {
    "text": "And the- the- to think of the way we- we come from this step to this step is described here.",
    "start": "628515",
    "end": "635395"
  },
  {
    "text": "So the expectation of some function g of z, where z is, um, uh, is distributed according to q,",
    "start": "635395",
    "end": "641755"
  },
  {
    "text": "by definition, is the sum over z, Q of z times g of z. That's just the definition of expectation.",
    "start": "641755",
    "end": "647980"
  },
  {
    "text": "And in this case, if we set g of z to be P of x, z divided by Q of z,",
    "start": "647980",
    "end": "653440"
  },
  {
    "text": "this becomes the g function, right? And therefore, this is just the expectation by",
    "start": "653440",
    "end": "658870"
  },
  {
    "text": "just the definition of, uh, expectations, right? So we started with the log-likelihood,",
    "start": "658870",
    "end": "665215"
  },
  {
    "text": "that's the- that's the one that we always want to maximize, and the log in the log-likelihood is a concave function, right?",
    "start": "665215",
    "end": "672640"
  },
  {
    "text": "And then we cooked up this expectation out of nothing by multiplying and dividing it by z. So we got a concave function and an expectation.",
    "start": "672640",
    "end": "680438"
  },
  {
    "text": "Using the two, we apply Jensen's inequality and Jensen's inequality allows us to swap the order",
    "start": "680439",
    "end": "685839"
  },
  {
    "text": "of the log in the expectation by using this inequality, right?",
    "start": "685840",
    "end": "691030"
  },
  {
    "text": "So this is the- the log-likelihood. The log in the log-likelihood is the concave function, the expectation is something that we cooked up out of thin air,",
    "start": "691030",
    "end": "698620"
  },
  {
    "text": "and Jensen's inequality now allows us to swap this login expectation and get this inequality, right?",
    "start": "698620",
    "end": "705340"
  },
  {
    "text": "And this inequality we just gave it a name, we called it ELBO. It's called the evidence lower bound because this is the evidence, right?",
    "start": "705340",
    "end": "713050"
  },
  {
    "text": "And this is a lower bound of the evidence, and the lower bound was obtained through Jensen's inequality, right?",
    "start": "713050",
    "end": "719580"
  },
  {
    "text": "And as a corollary, we also saw that Jensen's inequality tells us that this lower bound is exactly equal to,",
    "start": "719580",
    "end": "727590"
  },
  {
    "text": "if and only if Q of z, that is, uh, the- the- the probability distribution with which we were, uh,",
    "start": "727590",
    "end": "734770"
  },
  {
    "text": "taking, uh, the expectation happens to be P of z given x. Well, Jensen's inequality did not tell us this,",
    "start": "734770",
    "end": "741279"
  },
  {
    "text": "but we derived it and showed that as a consequence, Q of z must be equal to, uh, um,",
    "start": "741280",
    "end": "747145"
  },
  {
    "text": "P of z given x in order for this term to be a constant, right?",
    "start": "747145",
    "end": "752710"
  },
  {
    "text": "Now, we- we have this Jensen's inequality and we have this corollary,",
    "start": "752710",
    "end": "757974"
  },
  {
    "text": "and using these two, we then der- uh, came up with the- the more general form of the EM algorithm, right?",
    "start": "757974",
    "end": "767590"
  },
  {
    "text": "In the most general form of the EM algorithm, we assume there is some parameter Theta, right?",
    "start": "767590",
    "end": "773560"
  },
  {
    "text": "In case of Gaussian mixture model, Theta will be a collection of phis, Mus, and Sigmas, right? We're just calling it Theta.",
    "start": "773560",
    "end": "778990"
  },
  {
    "text": "It could be any collection of para- uh, it could be any parameter depending on the model that we have, right?",
    "start": "778990",
    "end": "784269"
  },
  {
    "text": "And the most general form of the EM algorithm is again iterative, and in each iteration what we do is we perform the E-step,",
    "start": "784270",
    "end": "792535"
  },
  {
    "text": "where the E-step is basically calculating these optimum Q distributions, right?",
    "start": "792535",
    "end": "799360"
  },
  {
    "text": "The optimum Q distribution, what we saw here is the- is the, uh- uh, posterior distribution of, uh, p, z- uh,",
    "start": "799360",
    "end": "805464"
  },
  {
    "text": "P of z given x, right? So in the E-step, we calculate the Q distributions, right?",
    "start": "805465",
    "end": "811990"
  },
  {
    "text": "And in the M-step, we maximize the ELBO, right? And in the ELBO, by writing it out in- in- in a more, um,",
    "start": "811990",
    "end": "820570"
  },
  {
    "text": "um, verbose form, the ELBO is basically, uh, what- what we have over there. It is the sum from, uh,",
    "start": "820570",
    "end": "827755"
  },
  {
    "text": "i equals 1 to n across all examples, and this again is just the definition of the expectation, right?",
    "start": "827755",
    "end": "833950"
  },
  {
    "text": "Q of, uh- of, uh, z times log of P of x, z, Theta over Q of z.",
    "start": "833950",
    "end": "839140"
  },
  {
    "text": "A few things- important things to note here is that, in the E-step, for each example,",
    "start": "839140",
    "end": "845895"
  },
  {
    "text": "we calculate the posterior distribution, right? And once we calculate the posterior distribution,",
    "start": "845895",
    "end": "851415"
  },
  {
    "text": "the Qs are held fixed during the M-step, right? That's a- that's a crucial detail, right?",
    "start": "851415",
    "end": "857740"
  },
  {
    "text": "The Qs over here are constant. We assume these Qs to be constant. Even though the Qs had Theta's in them,",
    "start": "857740",
    "end": "866350"
  },
  {
    "text": "we assume that these Thetas are from the previous iteration and we held- we- we hold them fixed during the M-step, right?",
    "start": "866350",
    "end": "874405"
  },
  {
    "text": "And when we are performing this arg max over Theta, the only Theta that is getting optimized is the Thetas that shows up over here, right?",
    "start": "874405",
    "end": "883570"
  },
  {
    "text": "So this is the, you know, P of x, z divide by- by Q of z is what we [NOISE] had over here that went into the ELBO,",
    "start": "883570",
    "end": "892389"
  },
  {
    "text": "and it is only this Theta that we are optimizing over in the M-step, right?",
    "start": "892390",
    "end": "899080"
  },
  {
    "text": "The Q distribution, remember it came from the- from the E-step, and in the E-step, in order to calculate Q,",
    "start": "899080",
    "end": "904689"
  },
  {
    "text": "there was a Q over here, but we don't write this as P of z given x parameterized by Theta,",
    "start": "904690",
    "end": "909790"
  },
  {
    "text": "because we are not optimizing that Theta, we're holding it fixed, right?",
    "start": "909790",
    "end": "915040"
  },
  {
    "text": "And we're only optimizing the- the- the Thetas that show up in this- in the- in the- in the numerator, right?",
    "start": "915040",
    "end": "923200"
  },
  {
    "text": "And this gives us, uh, a visual understanding of the EM algorithm, where the dotted black line is log P of x,",
    "start": "923200",
    "end": "933100"
  },
  {
    "text": "which we assume is- is hard- it could be hard to evaluate or it could be hard to- to optimize directly,",
    "start": "933100",
    "end": "939640"
  },
  {
    "text": "and we're gonna, um- we're gonna- we're gonna, uh, assume that this is not directly accessible for optimization, right?",
    "start": "939640",
    "end": "948205"
  },
  {
    "text": "Instead, what we do is we start with some random initialization Theta, some Theta naught, right?",
    "start": "948205",
    "end": "953515"
  },
  {
    "text": "And for this particular value of Theta naught, we construct the corresponding q distribution",
    "start": "953515",
    "end": "961105"
  },
  {
    "text": "as the posterior z given x at that value of Theta naught, right? And that gives us the corresponding ELBO where the q corresponds to,",
    "start": "961105",
    "end": "970600"
  },
  {
    "text": "uh- Q corresponds to the posterior at- at, um- um, Theta naught, right?",
    "start": "970600",
    "end": "976645"
  },
  {
    "text": "And this Theta naught by definition or by the corollary will",
    "start": "976645",
    "end": "983140"
  },
  {
    "text": "be exactly equal to log P of x over here, right?",
    "start": "983140",
    "end": "989890"
  },
  {
    "text": "So we are at Theta naught, and by constructing the ELBO and by evaluating the ELBO,",
    "start": "989890",
    "end": "996339"
  },
  {
    "text": "we have effectively evaluate- we- we know what value log P of x is at this,",
    "start": "996340",
    "end": "1001740"
  },
  {
    "text": "um- um, at this point, right? But now we want to keep improving our Theta values or keep adjusting",
    "start": "1001740",
    "end": "1008400"
  },
  {
    "text": "our Theta values such that we- the- the corresponding log P of x is increasing, right?",
    "start": "1008400",
    "end": "1013965"
  },
  {
    "text": "That's- that's our goal. Our objective is to find value of Theta that maximizes",
    "start": "1013965",
    "end": "1019020"
  },
  {
    "text": "log P of x. And- and we're doing this EM algorithm to do that indirectly, right?",
    "start": "1019020",
    "end": "1024435"
  },
  {
    "text": "If he had observed z, we could have directly performed gradient ascent or gradient descent over this, uh, objective.",
    "start": "1024435",
    "end": "1031424"
  },
  {
    "text": "However, we are doing this iterative algorithm wherein each step we construct a lower bound using",
    "start": "1031425",
    "end": "1038385"
  },
  {
    "text": "a specially chosen value of q and that value of q is the posterior distribution of z given x at Theta naught, right?",
    "start": "1038385",
    "end": "1046260"
  },
  {
    "text": "And then we maximize the ELBO instead of maximizing log P of x.",
    "start": "1046260",
    "end": "1052695"
  },
  {
    "text": "And when- when we maximize the ELBO, we get an updated value,",
    "start": "1052695",
    "end": "1057885"
  },
  {
    "text": "Theta 1, and this Theta 1 will correspond to, um, the ELBO being maximized, right?",
    "start": "1057885",
    "end": "1064425"
  },
  {
    "text": "And it is- using this Theta 1, we now construct a new ELBO,",
    "start": "1064425",
    "end": "1070605"
  },
  {
    "text": "which is tight at- at, uh, um, at Theta 1, and this ELBO has, you know, q1,",
    "start": "1070605",
    "end": "1077220"
  },
  {
    "text": "and q1 will be the posterior evaluated at Theta 1, right?",
    "start": "1077220",
    "end": "1083340"
  },
  {
    "text": "And that's the- that's the ELBO of the second iteration, and then we locally ma- and then we maximize ELBO or",
    "start": "1083340",
    "end": "1089280"
  },
  {
    "text": "locally maximize the ELBO of the second iteration to get, um, you know, Theta 2. And using this Theta 2 we can, you know,",
    "start": "1089280",
    "end": "1096330"
  },
  {
    "text": "construct ELBO 3 and so on and so on, right? So it is this- this, uh, um,",
    "start": "1096330",
    "end": "1102315"
  },
  {
    "text": "iterative algorithm, you know, which we call as the EM algorithm. And what we're gonna do today will be to uh, give- give, uh,",
    "start": "1102315",
    "end": "1112155"
  },
  {
    "text": "a quick and easy proof that proves that the EM algorithm always converges to a local optimum.",
    "start": "1112155",
    "end": "1118335"
  },
  {
    "text": "And then we're gonna apply it to, UH, the Gaussian mixture model and end up with the same update rules using this more principled approach of maximizing,",
    "start": "1118335",
    "end": "1127785"
  },
  {
    "text": "um- um- maximizing the ELBO iteratively. And then we'll move on to factor analysis, which is a different model.",
    "start": "1127785",
    "end": "1133515"
  },
  {
    "text": "Uh, a- another different, uh, latent variable model where there is a hidden z and apply",
    "start": "1133515",
    "end": "1139065"
  },
  {
    "text": "EM to the factor analysis model and solve that using EM as well. So that's the plan for today. Any- any questions so far? Yes?",
    "start": "1139065",
    "end": "1145590"
  },
  {
    "text": "[inaudible] You used Gaussian mixture model and",
    "start": "1145590",
    "end": "1153870"
  },
  {
    "text": "you used [inaudible] and",
    "start": "1153870",
    "end": "1159830"
  },
  {
    "text": "every time new customers [inaudible] a new initialization mission or will you have your, given [inaudible] shift over to,",
    "start": "1159830",
    "end": "1165810"
  },
  {
    "text": "um, Bayesian methods and then start classification. So the question is, what if you have like, uh, you're in, uh- in, uh,",
    "start": "1165810",
    "end": "1172000"
  },
  {
    "text": "if I- if I summarize it correctly, what if you're in a- in a streaming setting where you're getting new data, uh, um, you know,",
    "start": "1172000",
    "end": "1178145"
  },
  {
    "text": "and you already have a model that you fit with the old data, and yo- you obtain some- some more new data if you're using, uh- if you're performing market segmentation and you have some customer data,",
    "start": "1178145",
    "end": "1186550"
  },
  {
    "text": "you fit a model, and you got data from new customers, right? Uh, so the- the- um,",
    "start": "1186550",
    "end": "1191805"
  },
  {
    "text": "now, so what you wanna do over there is first, you want to ask the question, do we want to hold onto the same number,",
    "start": "1191805",
    "end": "1198750"
  },
  {
    "text": "you know, of clusters k, or do we want to update k? That's the first question you want to ask when you get the new data.",
    "start": "1198750",
    "end": "1204475"
  },
  {
    "text": "If the answer, uh, you know, if you want to hold onto the same cluster k, then you can perform EM algorithm where you can use",
    "start": "1204475",
    "end": "1212049"
  },
  {
    "text": "the- the previously optimized parameter values as the starting random initialization and then just refit over the whole data and,",
    "start": "1212050",
    "end": "1220230"
  },
  {
    "text": "we know, that hopefully will just converge faster. However, if you, um, decide to have,",
    "start": "1220230",
    "end": "1226335"
  },
  {
    "text": "you know, a larger k, uh, where you want to have more number of clusters, you know, with the- with the- with the updated data,",
    "start": "1226335",
    "end": "1231550"
  },
  {
    "text": "then you basically have to just start randomly effective business. [inaudible]",
    "start": "1231550",
    "end": "1246465"
  },
  {
    "text": "Yeah. Oh, yeah. Yeah- yeah- yeah- yeah. So- so the question is, uh, we fit the new models and- and we get new customer data and how do we, uh- uh,",
    "start": "1246465",
    "end": "1254790"
  },
  {
    "text": "place them into, um- um, place them into one of the existing um- um- um, clusters.",
    "start": "1254790",
    "end": "1260070"
  },
  {
    "text": "In case of K-means, that would be, you know, uh, pretty straightforward. In place of x_ i,",
    "start": "1260070",
    "end": "1265575"
  },
  {
    "text": "you place the new x star, to the, uh- uh- uh, um, you know, the new customer, uh, data that you get and find the minimum- the closest centroid.",
    "start": "1265575",
    "end": "1272445"
  },
  {
    "text": "In case of the, uh, Gaussian mixture model, um, you will calculate this posterior distribution using the Bayes rule to get,",
    "start": "1272445",
    "end": "1280784"
  },
  {
    "text": "uh- uh- uh- uh, you know, a probability distribution over your mixtures, right?",
    "start": "1280785",
    "end": "1286510"
  },
  {
    "text": "And- and that's- that's, uh, essentially something very similar to what you did with GDA. In GDA, it was only two classes,",
    "start": "1286510",
    "end": "1293765"
  },
  {
    "text": "and therefore it was- it took uh, a logistic regression form. In- in case of Gaussian mixture model,",
    "start": "1293765",
    "end": "1299955"
  },
  {
    "text": "this will end up taking the form of a Softmax, right? And because the co-variances are different,",
    "start": "1299955",
    "end": "1305250"
  },
  {
    "text": "it will effectively be Softmax using quadratic features. [NOISE] Right? Good question.",
    "start": "1305250",
    "end": "1313170"
  },
  {
    "text": "Was there another question? Yes, question. Is there a general setting for the co-variance? So it that for- for each of the i- x,",
    "start": "1313170",
    "end": "1321645"
  },
  {
    "text": "so Q_i will basically put them in the [inaudible] so is it like for discrete classification?",
    "start": "1321645",
    "end": "1332070"
  },
  {
    "text": "So the question is, in- in- in the E-step, are we calculating a discrete distribution over in- on the set of z,",
    "start": "1332070",
    "end": "1341370"
  },
  {
    "text": "the- the set of k classes? Exactly. That's exactly what happens here. For each x- um- uh,",
    "start": "1341370",
    "end": "1348360"
  },
  {
    "text": "so what- what you wanna do is equals j, equals j. That's what- that's how we wanna think of this, right?",
    "start": "1348360",
    "end": "1354270"
  },
  {
    "text": "So Q is- is a multinomial distribution over the clusters. So Q in this case, we'll, you know,",
    "start": "1354270",
    "end": "1361080"
  },
  {
    "text": "Q of z, you know, 3 for the third example. Once you calculate this,",
    "start": "1361080",
    "end": "1367125"
  },
  {
    "text": "will give you a multinomial distribution or the K clusters. So this is cluster 1 through cluster k and you will get some value,",
    "start": "1367125",
    "end": "1377610"
  },
  {
    "text": "0.1, 0.9, 0.01, and so on [NOISE]. So what that z_3 mean?",
    "start": "1377610",
    "end": "1383490"
  },
  {
    "text": "So z_3 means like the third example. So- so- Like, you know, x_ i, you know,",
    "start": "1383490",
    "end": "1389775"
  },
  {
    "text": "will have a z_ i corresponding here. Yeah, okay? Was their another question?",
    "start": "1389775",
    "end": "1395534"
  },
  {
    "text": "Yeah, sir. I didn't get what you mean by quadratic features because we used a covariance.",
    "start": "1395535",
    "end": "1401070"
  },
  {
    "text": "Oh yeah. So er, the question is, what did I mean by quadratic features because we used a covariance?",
    "start": "1401070",
    "end": "1406485"
  },
  {
    "text": "We discussed that briefly in- in GDA. It's- it's not super critical that you understand that.",
    "start": "1406485",
    "end": "1411705"
  },
  {
    "text": "What- uh, what- what- if you remember in GDA, what we- uh, what we saw was, um, you know,",
    "start": "1411705",
    "end": "1418500"
  },
  {
    "text": "if- if- if your data resides in two clusters and we assume they have equal co-variances,",
    "start": "1418500",
    "end": "1427170"
  },
  {
    "text": "then the two co-variances might look like this. And the separating hyper boundary will therefore be a straight line, right?",
    "start": "1427170",
    "end": "1434985"
  },
  {
    "text": "But if we assume different co-variances, then it could be, you know, a- a curved line.",
    "start": "1434985",
    "end": "1440595"
  },
  {
    "text": "And the posterior of this is effectively represented by uh- uh, logistic regression that uses quadratic features of x_1 and x_2, right?",
    "start": "1440595",
    "end": "1449610"
  },
  {
    "text": "So it's- it's essentially the same and stuff too. You will have, you know, k different centroids and because they're all having different co-variances, right?",
    "start": "1449610",
    "end": "1458115"
  },
  {
    "text": "We assume different co-variances for each class. That means uh, it's like uh, you're performing a Softmax with quadratic features, right?",
    "start": "1458115",
    "end": "1465660"
  },
  {
    "text": "So logistics regre- the generalization of-of the logistic function to k clusters is the Softmax, right?",
    "start": "1465660",
    "end": "1471765"
  },
  {
    "text": "And if you assume um- um, equal- equal covariance of the Gaussians,",
    "start": "1471765",
    "end": "1477105"
  },
  {
    "text": "then you will get- effectively get a Softmax uh, using linear features and if you include um- um- um,",
    "start": "1477105",
    "end": "1483960"
  },
  {
    "text": "different co-variances, it'll effectively be um- um- um, Softmax using quadratic features.",
    "start": "1483960",
    "end": "1489254"
  },
  {
    "text": "But it's not super critical that you understand the- the connection to the uh, uh, Softmax and the quadratic features. Yes, question.",
    "start": "1489255",
    "end": "1496320"
  },
  {
    "text": "So I have a question about why [inaudible] log p of x given only that particular theta, is that because that's how we are constricting the-",
    "start": "1496320",
    "end": "1512669"
  },
  {
    "text": "Exactly. So- so the- the fact that- uh, so the question is, um,",
    "start": "1512670",
    "end": "1517980"
  },
  {
    "text": "the ELBO touches log P of x over here. Does it touch because we know we intentionally",
    "start": "1517980",
    "end": "1525240"
  },
  {
    "text": "carefully constructed the ELBO to be that way and that's exactly right. We construct the ELBO over here such that the Q in the ELBO is",
    "start": "1525240",
    "end": "1533070"
  },
  {
    "text": "the posterior that makes it tight at- at log P of x, right?",
    "start": "1533070",
    "end": "1538575"
  },
  {
    "text": "So let's move on. Uh, so- so the proof of- of why EM algorithm converges.",
    "start": "1538575",
    "end": "1548889"
  },
  {
    "text": "Proof of EM convergence, right?",
    "start": "1551030",
    "end": "1559980"
  },
  {
    "text": "So the- so the proof of EM convergence is something that we'll almost theme uh, over here, right?",
    "start": "1559980",
    "end": "1567045"
  },
  {
    "text": "So in order to prove that uh, EM converges, our strategy will be to show that uh,",
    "start": "1567045",
    "end": "1573690"
  },
  {
    "text": "for every t, our time step or every iteration,",
    "start": "1573690",
    "end": "1582850"
  },
  {
    "text": "L theta of t plus 1 is greater than or equal to L theta of t, right?",
    "start": "1582980",
    "end": "1594165"
  },
  {
    "text": "So l over here is our objective that we want to uh- uh, optimize.",
    "start": "1594165",
    "end": "1601770"
  },
  {
    "text": "So L theta is a- is log P of x theta.",
    "start": "1601770",
    "end": "1609555"
  },
  {
    "text": "And what we wanna show is by- by performing the EM algorithm by iteration after iteration,",
    "start": "1609555",
    "end": "1616679"
  },
  {
    "text": "the log-likelihood of- uh, of- of the marginal log-likelihood of P of",
    "start": "1616680",
    "end": "1622980"
  },
  {
    "text": "x is always increasing with each iteration of EM algorithm. That's what we wanna show, right?",
    "start": "1622980",
    "end": "1628275"
  },
  {
    "text": "And by showing this uh, and also by recognizing the fact that the log-likelihood is- is bounded above.",
    "start": "1628275",
    "end": "1634815"
  },
  {
    "text": "So log-likelihood cannot go off to, you know, infinity. Using those two uh- uh,",
    "start": "1634815",
    "end": "1640230"
  },
  {
    "text": "assumptions, we will- um, we will make the case that because L is",
    "start": "1640230",
    "end": "1645240"
  },
  {
    "text": "bounded above and because with every iteration we are uh, increasing um, the value of L,",
    "start": "1645240",
    "end": "1651495"
  },
  {
    "text": "eventually we will converge. And so what remains is to- is to show,",
    "start": "1651495",
    "end": "1656535"
  },
  {
    "text": "um, this- this condition. And to show this condition, you will basically use the same reasoning that we saw with the uh,",
    "start": "1656535",
    "end": "1663705"
  },
  {
    "text": "diagrams over here, right? So um, L theta t plus 1, right,",
    "start": "1663705",
    "end": "1673904"
  },
  {
    "text": "is greater than or equal to ELBO of x,",
    "start": "1673905",
    "end": "1689700"
  },
  {
    "text": "Q_t, theta t plus 1. Why is this the case? So L theta- this is- uh- uh,",
    "start": "1690620",
    "end": "1700950"
  },
  {
    "text": "this is basically just the uh- uh, definition of- of- of what Jensen's inequality gave- gave us, that the uh- uh,",
    "start": "1700950",
    "end": "1710220"
  },
  {
    "text": "log-likelihood for- for the same values of theta, the likelihood P of x will always be greater than or equal to the ELBO",
    "start": "1710220",
    "end": "1720150"
  },
  {
    "text": "for any values of-of- of Q, right?",
    "start": "1720150",
    "end": "1727650"
  },
  {
    "text": "And this is greater than or equal to",
    "start": "1727650",
    "end": "1733210"
  },
  {
    "text": "ELBO of x Q_t",
    "start": "1735260",
    "end": "1743865"
  },
  {
    "text": "and theta t, okay? So this was Jensen's inequality.",
    "start": "1743865",
    "end": "1751799"
  },
  {
    "text": "[NOISE] Why is this the case?",
    "start": "1751800",
    "end": "1758850"
  },
  {
    "text": "[inaudible]",
    "start": "1758850",
    "end": "1766830"
  },
  {
    "text": "Exa- Exactly. This - this is because- because of the M-step, right? So the M-step guarantees that, um,",
    "start": "1766830",
    "end": "1775009"
  },
  {
    "text": "that Theta_t plus 1 was chosen to be the maximizer of - of this. So - so Theta_t t plus 1 over here will give us the- the highest value for- for ELBO.",
    "start": "1775010",
    "end": "1784924"
  },
  {
    "text": "And therefore this is due to M-step, [NOISE] because we have the,",
    "start": "1784925",
    "end": "1793865"
  },
  {
    "text": "uh, Theta_t plus 1 was chosen to be the argmax of the - of- of this, right?",
    "start": "1793865",
    "end": "1800660"
  },
  {
    "text": "And then, um, [NOISE] and this is",
    "start": "1800660",
    "end": "1808429"
  },
  {
    "text": "equal to l Theta_t.",
    "start": "1808430",
    "end": "1813120"
  },
  {
    "text": "And why is this the case? This was the last step. Right. This is the corollary of Jensen's inequality,",
    "start": "1815590",
    "end": "1822620"
  },
  {
    "text": "[NOISE]",
    "start": "1822620",
    "end": "1832940"
  },
  {
    "text": "okay? So what does this basically telling us? So to kind of visualize, uh, to visually understand this proof,",
    "start": "1832940",
    "end": "1839445"
  },
  {
    "text": "l Theta_t plus 1- [NOISE] so this black dot over here, right?",
    "start": "1839445",
    "end": "1845289"
  },
  {
    "text": "This is l Theta_1, right? Assume t equals 0, right? L Theta_1, [NOISE] it says is greater than,",
    "start": "1845290",
    "end": "1853610"
  },
  {
    "text": "equal to the ELBO, right? It's always greater than the ELBO. So the ELBO evaluates [NOISE] to a point here, right?",
    "start": "1853610",
    "end": "1864270"
  },
  {
    "text": "So this is greater than 0 is step 1, right?",
    "start": "1864520",
    "end": "1871835"
  },
  {
    "text": "And step 2, basically, told us ELBO evaluated at Theta_t plus 1, is greater than ELBO evaluated at Theta_t.",
    "start": "1871835",
    "end": "1878960"
  },
  {
    "text": "So that tells us that, [NOISE] this is greater than 0.",
    "start": "1878960",
    "end": "1884720"
  },
  {
    "text": "That's step 2 of the inequality, [NOISE] right? Because Theta - Theta_1 was chosen to maximize the blue curve, right?",
    "start": "1884720",
    "end": "1895505"
  },
  {
    "text": "And then step 3 tells us that, [NOISE] that L Theta equals the ELBO.",
    "start": "1895505",
    "end": "1902120"
  },
  {
    "text": "And - and that's because we - we constructed - we intentionally constructed the blue ELBO to be equal to log P of x at,",
    "start": "1902120",
    "end": "1911645"
  },
  {
    "text": "uh, at the current value of - at - at - at Theta t, right? So that basically tells us the gap between [NOISE] the - the - the gap between the",
    "start": "1911645",
    "end": "1919940"
  },
  {
    "text": "- the [NOISE]- the blue line and the black line at this is- is basically zero, right?",
    "start": "1919940",
    "end": "1926990"
  },
  {
    "text": "So that tells us that l of Theta_0 is less than L of Theta_1, right?",
    "start": "1926990",
    "end": "1932615"
  },
  {
    "text": "It- it so - so at each step we are [NOISE] guaranteed not to get worse in terms of the log likelihood. Yes, question?",
    "start": "1932615",
    "end": "1943680"
  },
  {
    "text": "Log l Theta_t [NOISE]. So the same thing except it should be Q t minus 1 because",
    "start": "1945250",
    "end": "1951590"
  },
  {
    "text": "when we got l Theta_t we didn't have to have Q t minus 1 from the Theta_t?",
    "start": "1951590",
    "end": "1957424"
  },
  {
    "text": "So once we - so - so, um, so the question is should this be Q t minus 1?",
    "start": "1957425",
    "end": "1963750"
  },
  {
    "text": "So I'm saying, uh, l Theta of t, uh, would be ELBO of x semicolon x, qt minus 1, theta d.",
    "start": "1964030",
    "end": "1972799"
  },
  {
    "text": "So l Theta of t [NOISE] will be equal to ELBO, where Q is evaluated at the posterior of Theta, right?",
    "start": "1972800",
    "end": "1980030"
  },
  {
    "text": "Yes - yes that is- [OVERLAPPING]. So that is - that is Theta t. So the right hand side is in the M-step, right? So this is the E-step.",
    "start": "1980030",
    "end": "1986780"
  },
  {
    "text": "You get this from the E-step, yeah. So that - and the next step is getting the theta for - for t.",
    "start": "1986780",
    "end": "1992600"
  },
  {
    "text": "So the next step, yeah - for - so the E-step. So at the current value of Theta, we use the E-step to calculate Q of t. And using Q of t, reconstruct the ELBO.",
    "start": "1992600",
    "end": "2003355"
  },
  {
    "text": "And that ELBO will be equal, right? And then from this ELBO, so this is basically like starting at",
    "start": "2003355",
    "end": "2009580"
  },
  {
    "text": "theta naught in the picture and constructing the blue ELBO. So the blue ELBO will be equal to this.",
    "start": "2009580",
    "end": "2017305"
  },
  {
    "text": "And the next step is to go from theta t to theta t plus 1 by optimizing the blue ELBO.",
    "start": "2017305",
    "end": "2023725"
  },
  {
    "text": "But L - L theta of t takes in the previous Q, not the current Q, right? So l theta of t is - has no Q, right?",
    "start": "2023725",
    "end": "2031720"
  },
  {
    "text": "L theta of t is just L Theta of t, L Theta of t is just [NOISE] log P of x. There's no Q in it, [NOISE] right?",
    "start": "2031720",
    "end": "2040635"
  },
  {
    "text": "Is - is - is this clear, Any questions? Cool. So its -so the proof here comes out to be pretty intuitive,",
    "start": "2040635",
    "end": "2050685"
  },
  {
    "text": "um, you know, It can be easily visualized. What we are saying is we start at any value of - of - of Theta,",
    "start": "2050685",
    "end": "2057345"
  },
  {
    "text": "some random value of Theta, right? The log-likelihood has, you know, evaluates to some value which we may or may not be able to calculate, right?",
    "start": "2057345",
    "end": "2066325"
  },
  {
    "text": "But we can always construct an ELBO such that log P of x equals to that ELBO at that point, right?",
    "start": "2066325",
    "end": "2073389"
  },
  {
    "text": "So we construct the ELBO, it has guaranteed to be lower than - than log P of x.",
    "start": "2073390",
    "end": "2078565"
  },
  {
    "text": "And then we optimize the ELBO, right? So once we optimize the ELBO, we reach a higher point of the ELBO.",
    "start": "2078565",
    "end": "2084685"
  },
  {
    "text": "Now it is important that this point over here was tight. What would happen if this point was not tight?",
    "start": "2084685",
    "end": "2091669"
  },
  {
    "text": "What would happen if the ELBO was not tight over here? Yes, question.",
    "start": "2092340",
    "end": "2102010"
  },
  {
    "text": "[inaudible]?",
    "start": "2102010",
    "end": "2109390"
  },
  {
    "text": "Right. So what would happen if, um, if - if this was not guaranteed to be tight is that - it may well have been the case.",
    "start": "2109390",
    "end": "2122710"
  },
  {
    "text": "Then the likelihood function goes like this. And if this was not tight,",
    "start": "2122710",
    "end": "2128335"
  },
  {
    "text": "we may have gone down in our likelihood rather than guaranteed to be going up, right?",
    "start": "2128335",
    "end": "2134470"
  },
  {
    "text": "Because this was tight at - at - at Q naught- because it was tight. We, you know, the - the other two pieces of the proof allow us to",
    "start": "2134470",
    "end": "2143410"
  },
  {
    "text": "connect L Theta of L of Theta 1 to be greater than L of theta of - of 0.",
    "start": "2143410",
    "end": "2149095"
  },
  {
    "text": "If this were not to be tight, then the likelihood at, at Theta naught could have been much higher. And we may have actually gone worse in terms of our optimizing our objective.",
    "start": "2149095",
    "end": "2159589"
  },
  {
    "text": "Is that clear? Cool. All right,",
    "start": "2160080",
    "end": "2166960"
  },
  {
    "text": "so now we have derived the - the most general form of the EM algorithm. Here, Z's and x's could be anything.",
    "start": "2166960",
    "end": "2175615"
  },
  {
    "text": "And the joint probability of the model [NOISE] , we made no assumption of",
    "start": "2175615",
    "end": "2181240"
  },
  {
    "text": "the joint probability of P of x and Z to take any specific form, right?",
    "start": "2181240",
    "end": "2187900"
  },
  {
    "text": "And now we will, taking this more generic algorithm,",
    "start": "2187900",
    "end": "2193045"
  },
  {
    "text": "you will see how we can apply it to the Gaussian mixture model and - and basically see that we will end up with",
    "start": "2193045",
    "end": "2199330"
  },
  {
    "text": "the same update rules. [NOISE].",
    "start": "2199330",
    "end": "2226220"
  },
  {
    "text": "So this part will be super quick.",
    "start": "2226220",
    "end": "2229200"
  },
  {
    "text": "I'll just give some- some- some hints on how to go about it. And in your homework you'll be basically doing",
    "start": "2232470",
    "end": "2239510"
  },
  {
    "text": "a more detailed version of, uh, what, we'll be doing now. So to derive. Uh, so G- Gaussian Mixture Model,",
    "start": "2239510",
    "end": "2249085"
  },
  {
    "text": "we have EM, right? So for the, uh, Gaussian mixture model by EM,",
    "start": "2249085",
    "end": "2256225"
  },
  {
    "text": "first we do the E-step, right?",
    "start": "2256225",
    "end": "2261280"
  },
  {
    "text": "So in the E-step for each height, right?",
    "start": "2261280",
    "end": "2268580"
  },
  {
    "text": "So bef- before we- we, uh, before we go into how we go about doing it, uh, a general kind of, uh,",
    "start": "2268580",
    "end": "2276065"
  },
  {
    "text": "template or thumbs of rules to follow when you wanna apply EM is to first always,",
    "start": "2276065",
    "end": "2281869"
  },
  {
    "text": "uh, get clarity on what the model is, right? First- step 1, right?",
    "start": "2281870",
    "end": "2290630"
  },
  {
    "text": "Write out p of x,z.",
    "start": "2290630",
    "end": "2297710"
  },
  {
    "text": "And what I mean by write out is write out your assumptions, right? You may observe only x's and z's. That is fine.",
    "start": "2297710",
    "end": "2305135"
  },
  {
    "text": "We may only observe x's and not z's, independent of what your- what data you observe and what you don't observe,",
    "start": "2305135",
    "end": "2311180"
  },
  {
    "text": "first, write out your model. And by writing out your model, it may be something like this. Z comes from multinomial, right?",
    "start": "2311180",
    "end": "2322160"
  },
  {
    "text": "And then x given z comes from normal of Mu z and Sigma z.",
    "start": "2322160",
    "end": "2331174"
  },
  {
    "text": "Why do we call this the model? Because this is effectively p of z times p of x given z,",
    "start": "2331175",
    "end": "2339245"
  },
  {
    "text": "and this is equal to p of x, z right? So first write out your model and writing it in this form is- is basically called",
    "start": "2339245",
    "end": "2347510"
  },
  {
    "text": "as writing the data-generative process and writing the data generative process. You're implicitly defining what the model is, right?",
    "start": "2347510",
    "end": "2354410"
  },
  {
    "text": "So the first thing you wanna always do is write out the model and be clear what the data-generating process is and what your parameters are, right?",
    "start": "2354410",
    "end": "2361705"
  },
  {
    "text": "And in this case the parameters basically, uh, are p of x comma z,",
    "start": "2361705",
    "end": "2367225"
  },
  {
    "text": "the parameters will be Phi, Mus and Sigmas. All right? Be clear on what the parameters",
    "start": "2367225",
    "end": "2375110"
  },
  {
    "text": "are and what the- what's the form of the full model. And then the- Step 2 is clearly identify what are",
    "start": "2375110",
    "end": "2386000"
  },
  {
    "text": "latent and what are observed.",
    "start": "2386000",
    "end": "2396750"
  },
  {
    "text": "Are, you know, what's the evidence? And- and in most of the, uh,",
    "start": "2396820",
    "end": "2404600"
  },
  {
    "text": "models that we will see x will generally be the evidence. That is the part of the data that is observed,",
    "start": "2404600",
    "end": "2412264"
  },
  {
    "text": "and z will be latent. That's how we generally name the things.",
    "start": "2412264",
    "end": "2417575"
  },
  {
    "text": "So when you- when you, uh, when you look at the models, uh, in- in the notes, uh, or in general, you know,",
    "start": "2417575",
    "end": "2423200"
  },
  {
    "text": "x will generally refer to something that is observed that will be given to us. And z will generally refer to,",
    "start": "2423200",
    "end": "2428795"
  },
  {
    "text": "you know, some latent variable that is not observed. Right? And uh, okay?",
    "start": "2428795",
    "end": "2435200"
  },
  {
    "text": "And then- once- once you are clear about what the full model is,",
    "start": "2435200",
    "end": "2441214"
  },
  {
    "text": "what the parameters are, what is observed, and what is latent. Only then attempt to apply EM on it, right?",
    "start": "2441215",
    "end": "2450244"
  },
  {
    "text": "If you're not clear of- of, you know, what- what, you know, if you're not clear about these first two steps,",
    "start": "2450245",
    "end": "2457160"
  },
  {
    "text": "It's gonna be very hard to apply EM and you're just gonna get confused really badly, right? Always be clear on what the full model is.",
    "start": "2457160",
    "end": "2464480"
  },
  {
    "text": "The data-generating process, which is the same as the model. What the parameters are, what's observed, and what's latent, right?",
    "start": "2464480",
    "end": "2471750"
  },
  {
    "text": "In case of the Gaussian mixture model, the- the z's, the latent variables are the cluster identities, right?",
    "start": "2475030",
    "end": "2484055"
  },
  {
    "text": "The parameters are the, uh, uh, the, uh, the multinomial parameters,",
    "start": "2484055",
    "end": "2490145"
  },
  {
    "text": "the different sets of mus and sigmas, right? Those are the parameters and Gaussian mixture model and what's",
    "start": "2490145",
    "end": "2495560"
  },
  {
    "text": "observed are just the x's without the cluster identity.",
    "start": "2495560",
    "end": "2499650"
  },
  {
    "text": "Okay? There's also a difference between um, ano- another thing to be clear about is there are two kinds of unknowns here, right?",
    "start": "2500800",
    "end": "2509930"
  },
  {
    "text": "So we have the unknown latent variables and the unknown parameters. What is the difference between the two?",
    "start": "2509930",
    "end": "2516575"
  },
  {
    "text": "The unknown latent variables are examples specific. Right? For a different choice of x,",
    "start": "2516575",
    "end": "2522860"
  },
  {
    "text": "you will have a different latent variable. Whereas the unknown parameters are global.",
    "start": "2522860",
    "end": "2528589"
  },
  {
    "text": "They- they're not specific to any example, the means and the co-variances and the- and the,",
    "start": "2528590",
    "end": "2533750"
  },
  {
    "text": "uh, parameters of the multinomial. They are kind of global in the sense that they don't belong to any specific example.",
    "start": "2533750",
    "end": "2539315"
  },
  {
    "text": "Whereas the latent variables are paired with a given example, right? And, uh, and- and this kind of, uh, a model,",
    "start": "2539315",
    "end": "2549079"
  },
  {
    "text": "you can think of it as a frequentist model where we- we considered the parameters to be unknown constants.",
    "start": "2549080",
    "end": "2555605"
  },
  {
    "text": "Whereas, but you can also perform a Bayesian treatment of this where you consider everything to be a random variable,",
    "start": "2555605",
    "end": "2562220"
  },
  {
    "text": "but, you know, that's beyond the scope. For the EM algorithm. It is- it is clear that- it's important that you're clear what",
    "start": "2562220",
    "end": "2569119"
  },
  {
    "text": "the- what the parameters are and the parameters are global, they are not specific to any example.",
    "start": "2569120",
    "end": "2575000"
  },
  {
    "text": "And there's this other kind of unknown, which is the latent variable, which is paired with an example.",
    "start": "2575000",
    "end": "2581370"
  },
  {
    "text": "And what we do in the E-step is basically estimate this latent variable that's pair with each example, right?",
    "start": "2582160",
    "end": "2591515"
  },
  {
    "text": "Which is why in the E-step, we loop over each i. Okay? You're calculating what",
    "start": "2591515",
    "end": "2597680"
  },
  {
    "text": "the corresponding hidden latent variable is for this photo given example, by holding the parameters fixed.",
    "start": "2597680",
    "end": "2605045"
  },
  {
    "text": "Right? And in the M-step, we go the other way. We assume we've, you know- you know,",
    "start": "2605045",
    "end": "2610175"
  },
  {
    "text": "the z values are, you know, the z values, um, um what they are.",
    "start": "2610175",
    "end": "2615350"
  },
  {
    "text": "And then we go about updating the parameters in the M-step, right? So in the E-step, we're- we're kind of attacking one kind of unobserved,",
    "start": "2615350",
    "end": "2622970"
  },
  {
    "text": "which is specific to each example, and in the M-step we are attacking the other kind of unknown, which is global. And this is gonna be common in all kinds of models where you're applying EM,",
    "start": "2622970",
    "end": "2632465"
  },
  {
    "text": "right in the E-step, we're attacking the latent variable, in the M-step we are attacking the parameters.",
    "start": "2632465",
    "end": "2637505"
  },
  {
    "text": "Right? So- so the E-step in Gaussian mixture model will- is basically for each i, right?",
    "start": "2637505",
    "end": "2648259"
  },
  {
    "text": "For each i, Qi of",
    "start": "2648260",
    "end": "2658170"
  },
  {
    "text": "z_i equals j is equal to P of",
    "start": "2658170",
    "end": "2666035"
  },
  {
    "text": "z_i given x_i at the current values of Phi, Mu Sigma.",
    "start": "2666035",
    "end": "2674974"
  },
  {
    "text": "Now how do we write- how do we obtain a posterior distribution of z given x,  using? Bayes' rule, exactly.",
    "start": "2674975",
    "end": "2681710"
  },
  {
    "text": "And this is P of x",
    "start": "2681710",
    "end": "2686869"
  },
  {
    "text": "given z times P of z over P of x.",
    "start": "2686870",
    "end": "2692645"
  },
  {
    "text": "All right? And again, this, because z is discrete, this will be P of x given z times P of z over summation over all z,",
    "start": "2692645",
    "end": "2703865"
  },
  {
    "text": "p of x given z times p of z.",
    "start": "2703865",
    "end": "2709160"
  },
  {
    "text": "Right? And this turns out to be P of x given Z. What's P of x given z here?",
    "start": "2709160",
    "end": "2714904"
  },
  {
    "text": "P of x given z is a- is a normal distribution, right? And- and p of x given z will therefore be uh,",
    "start": "2714905",
    "end": "2728329"
  },
  {
    "text": "1 over square root 2 Pi to the,",
    "start": "2728330",
    "end": "2734880"
  },
  {
    "text": "2 Pi to the d by 2. This is just- I'm just writing out the,",
    "start": "2735250",
    "end": "2742984"
  },
  {
    "text": "uh, the multivariate Gaussian. Now, sigma j to the 1 by 2.",
    "start": "2742985",
    "end": "2749569"
  },
  {
    "text": "That's the law- the determinant times the exponent we have, uh,",
    "start": "2749570",
    "end": "2759950"
  },
  {
    "text": "x_i minus Mu j transpose Sigma j inverse xi minus Mu j.",
    "start": "2759950",
    "end": "2771360"
  },
  {
    "text": "And then times-",
    "start": "2771730",
    "end": "2776100"
  },
  {
    "text": "times P of z. P of z is just Phi because it's no Phi j and- and,",
    "start": "2780130",
    "end": "2791480"
  },
  {
    "text": "uh, because it's- it's uh everywhere it's implicit that we're trying to do is equal to j.",
    "start": "2791480",
    "end": "2798410"
  },
  {
    "text": "So this is just Phi j. Now here, sum over all j,",
    "start": "2798410",
    "end": "2803839"
  },
  {
    "text": "you know, the same thing. Yes question. Should there be a minus?",
    "start": "2803840",
    "end": "2810095"
  },
  {
    "text": "Should there be. Yeah, yeah, there's a minus sign, yeah. Thank you. Minus 1 over 2.",
    "start": "2810095",
    "end": "2816060"
  },
  {
    "text": "Right? So we've done such calculations before.",
    "start": "2816060",
    "end": "2821735"
  },
  {
    "text": "Right? We've- we've calculated such posterior in GDA. And we also saw that if the, uh,",
    "start": "2821735",
    "end": "2829070"
  },
  {
    "text": "if p of x given c belongs to the exponential family, there's gonna be an exponent here. Right? And then there are all these constants,",
    "start": "2829070",
    "end": "2835910"
  },
  {
    "text": "they can get absorbed into the exponent. You can take the log of this, take it in, and you get, you know, an exponent of something divided by exponent of something plus exponent of something,",
    "start": "2835910",
    "end": "2844430"
  },
  {
    "text": "plus exponent of something that works out to be the softmax, uh, comes out to be in the softmax form, right?",
    "start": "2844430",
    "end": "2851855"
  },
  {
    "text": "And yes, so this is basically, you know- you know, all the values here. Plug it in and you get, uh,",
    "start": "2851855",
    "end": "2858890"
  },
  {
    "text": "uh, you get the estimates for- for- for the E-step. So that's the E-step and I'm gonna erase this up.",
    "start": "2858890",
    "end": "2872720"
  },
  {
    "text": "Hopefully you have taken note of this, right? And in the M-step. [NOISE]",
    "start": "2872720",
    "end": "2887465"
  },
  {
    "text": "Now, in the M-step. M-step what we wanna do is Mu Sigma Phi is equal",
    "start": "2887465",
    "end": "2898570"
  },
  {
    "text": "to argmax of Mu Sigma Phi, right?",
    "start": "2898570",
    "end": "2908300"
  },
  {
    "text": "And in the M-step, we do the argmax over the? The ELBO, exactly. And the ELBO can be written as what we saw over here.",
    "start": "2908300",
    "end": "2919160"
  },
  {
    "text": "So the ELBO we've written out the- the, uh, uh, expanded version there.",
    "start": "2919160",
    "end": "2924244"
  },
  {
    "text": "And for simplicity, let's just call this w^i_j.",
    "start": "2924245",
    "end": "2929670"
  },
  {
    "text": "We call it w^i_j, because once we calculated, this will be held constant in the M-step, right?",
    "start": "2930190",
    "end": "2937204"
  },
  {
    "text": "Even though w^i_j, dependent on the values of Theta- of the parameters to get calculated,",
    "start": "2937205",
    "end": "2944810"
  },
  {
    "text": "in the M-step, [NOISE] we will not be optimizing over them. We're gonna hold them fixed, right?",
    "start": "2944810",
    "end": "2950015"
  },
  {
    "text": "And because that's - that's what the- the- the, uh, uh, um, the ELBO should be held fixed.",
    "start": "2950015",
    "end": "2956405"
  },
  {
    "text": "And, uh, we are gonna argmax over the ELBO expression here.",
    "start": "2956405",
    "end": "2962195"
  },
  {
    "text": "And that will be sum of i equals 1 to n, sum of z,",
    "start": "2962195",
    "end": "2972600"
  },
  {
    "text": "w^i_j, or j equals 1 to k,",
    "start": "2972600",
    "end": "2979715"
  },
  {
    "text": "w^i_j of- so the ELBO is- so the ELBO is basically the expectation of the log of this term, right?",
    "start": "2979715",
    "end": "2991640"
  },
  {
    "text": "This would be log p of x^i,",
    "start": "2991640",
    "end": "3001345"
  },
  {
    "text": "z^i given Theta Mu Sigma and",
    "start": "3001345",
    "end": "3010090"
  },
  {
    "text": "Phi divided by w^i_j, right?",
    "start": "3010090",
    "end": "3019960"
  },
  {
    "text": "This is just writing out the ELBO by expanding out the expectations, right?",
    "start": "3019960",
    "end": "3025135"
  },
  {
    "text": "And this argmax Mu Sigma Phi,",
    "start": "3025135",
    "end": "3034730"
  },
  {
    "text": "i equals 1 to n, j equals 1 to k w^i_j.",
    "start": "3035250",
    "end": "3043900"
  },
  {
    "text": "Now, this we can again breakdown the joint according to our data-generating process.",
    "start": "3043900",
    "end": "3051010"
  },
  {
    "text": "So this will be p of x given z,",
    "start": "3051010",
    "end": "3057470"
  },
  {
    "text": "or z^i equals k, or z^i equals j times p of z^i equals j over w^i_j.",
    "start": "3058740",
    "end": "3071210"
  },
  {
    "text": "Now again, the same process that we did over here, p of x given z, will be a multivariate Gaussian PDF.",
    "start": "3071790",
    "end": "3080305"
  },
  {
    "text": "P of z given j is- is the multinomial, which will be just Phi of j, right?",
    "start": "3080305",
    "end": "3086545"
  },
  {
    "text": "And w's, the two w's are constants. For this- for the M-step, the two w's are constant.",
    "start": "3086545",
    "end": "3092620"
  },
  {
    "text": "In the E-step, we derive them using the previous values of Theta or the values of- of the parameters.",
    "start": "3092620",
    "end": "3098364"
  },
  {
    "text": "But once we calculate them in the E-step, we will hold- we will hold them frozen in the M-step for performing the maximization, right?",
    "start": "3098364",
    "end": "3106390"
  },
  {
    "text": "So plugging the values of- of the Gaussian PDF here, plug-in Phi j here. And to perform argmax,",
    "start": "3106390",
    "end": "3112390"
  },
  {
    "text": "how do we perform argmax? Take the gradient, set it equal to 0, solve for the parameters, right?",
    "start": "3112390",
    "end": "3119230"
  },
  {
    "text": "This looks pretty, you know, big and nasty, but, you know, it's pretty straightforward. There- there's no- there's no,",
    "start": "3119230",
    "end": "3125424"
  },
  {
    "text": "um, uh, uh, special tricks going on here. It's just tedious, right? So write out the, uh, uh,",
    "start": "3125425",
    "end": "3132130"
  },
  {
    "text": "write out the, uh, Gaussian PDF here, Phi j here. Take the derivatives, set it equal to 0.",
    "start": "3132130",
    "end": "3138430"
  },
  {
    "text": "And once you do that for separately for Phi Mu and j, we will end up getting Phi is equal to,",
    "start": "3138430",
    "end": "3144954"
  },
  {
    "text": "um, 1 over n, i equals 1 to n w^i_j.",
    "start": "3144955",
    "end": "3154180"
  },
  {
    "text": "And Mu hat will be basically the same thing.",
    "start": "3154180",
    "end": "3160069"
  },
  {
    "text": "So basically this will be Mu hat and Sigma hat will be,",
    "start": "3161250",
    "end": "3169825"
  },
  {
    "text": "you know, this thing, right? So write it out like this.",
    "start": "3169825",
    "end": "3176019"
  },
  {
    "text": "Take the gradient, set it equal to 0, and we get this, right? And this is the way we start from the general EM algorithm.",
    "start": "3176020",
    "end": "3184704"
  },
  {
    "text": "You know, work out the E-step and M-step and in the M-step, work out the maximization.",
    "start": "3184705",
    "end": "3189805"
  },
  {
    "text": "And by following this principled approach, we will see that we will end up with the same update rules that we get by fo-,",
    "start": "3189805",
    "end": "3197320"
  },
  {
    "text": "you know, that we obtained using the, uh, the heuristic soft K-means kind of an argument. Next question?",
    "start": "3197320",
    "end": "3203500"
  },
  {
    "text": "So do we have K different Mu's and K different Sigma's? Exactly. So the question is,",
    "start": "3203500",
    "end": "3208539"
  },
  {
    "text": "do we have k different Mu's and k different Sigma's? Yes, so over here, when I write Mu, I mean the collection of k different Mu vectors and K different,",
    "start": "3208540",
    "end": "3217360"
  },
  {
    "text": "you know, covariance matrices. So in the M-step values, we are simultaneously updating all of those?",
    "start": "3217360",
    "end": "3222955"
  },
  {
    "text": "Yes. So in the M-step, we simultaneously update all of those. Exactly. Yeah. You can do it for any Mu j or Sigma j and,",
    "start": "3222955",
    "end": "3230875"
  },
  {
    "text": "you know, it's symmetric. So once you do it for 1, it's the same pro- procedure for doing, uh, updating all the others. Good question.",
    "start": "3230875",
    "end": "3237250"
  },
  {
    "text": "[NOISE] So that's- that's,",
    "start": "3237250",
    "end": "3242695"
  },
  {
    "text": "uh, EM and EM applied to, um, Gaussian mixture models, right?",
    "start": "3242695",
    "end": "3249145"
  },
  {
    "text": "So what we did basically was to- was to come up with a general EM algorithm and using the general EM algorithm,",
    "start": "3249145",
    "end": "3256525"
  },
  {
    "text": "first, in order to apply to Gaussian mixture model, the first thing we had to do was get clarity on what the model was.",
    "start": "3256525",
    "end": "3263200"
  },
  {
    "text": "What's the full joint probability distribution of- of all the- of all the variables, x's and z's.",
    "start": "3263200",
    "end": "3268645"
  },
  {
    "text": "Have clarity on what the model parameters are. In case of, uh, Gaussian mixture model, it was Phi,",
    "start": "3268645",
    "end": "3274315"
  },
  {
    "text": "collection of Mu's, collection of covariance matrices Sigma's, right? And then get clarity on what's observed and what's not observed.",
    "start": "3274315",
    "end": "3281950"
  },
  {
    "text": "Once you know what's observed and what's not observed, it becomes clear what you need to do in the E-step.",
    "start": "3281950",
    "end": "3287140"
  },
  {
    "text": "In the E-step, we need to calculate the posterior, the probability of unobserved given the observed,",
    "start": "3287140",
    "end": "3292795"
  },
  {
    "text": "whatever it is for your specific model, construct the- construct the, uh, uh, E-step of probability of unobserved",
    "start": "3292795",
    "end": "3300819"
  },
  {
    "text": "given the observed and we hold the parameters fixed, right? And the- the steps of evaluating this will be different for different models.",
    "start": "3300820",
    "end": "3309714"
  },
  {
    "text": "For Gaussian mixture models, this happened to be normal and multinomial. For a different model in factor analysis that we're gonna see next,",
    "start": "3309715",
    "end": "3316690"
  },
  {
    "text": "these might be different, right? But the- but the general recipe is the same. Construct the, [NOISE] construct the, uh,",
    "start": "3316690",
    "end": "3323470"
  },
  {
    "text": "posterior distribution, uh, uh, in the E-step. And then in the M-step, write out the ELBO, right?",
    "start": "3323470",
    "end": "3330610"
  },
  {
    "text": "Write out the ELBO by plugging in the full model in the numerator and, uh,",
    "start": "3330610",
    "end": "3336250"
  },
  {
    "text": "uh, and the, uh, the- the posterior of the Q distributions in the- in the denominator and in the expectation.",
    "start": "3336250",
    "end": "3343525"
  },
  {
    "text": "The denominator and the- the probability with which we are taking the expectation are constant in the M-step, right?",
    "start": "3343525",
    "end": "3351385"
  },
  {
    "text": "We're gonna hold them constant. And by holding them constant, you're going to optimize over these set of parameters, right?",
    "start": "3351385",
    "end": "3359980"
  },
  {
    "text": "When we do the argmax, it is only these parameters that we are updating. There are no kind of hidden parameters inside these that were used in the- in the E-step.",
    "start": "3359980",
    "end": "3369835"
  },
  {
    "text": "But we're not gonna optimize them. For- for- for the purpose of the M-step, the Q distributions are fixed, right?",
    "start": "3369835",
    "end": "3376690"
  },
  {
    "text": "We're gonna only optimize over these. And then we take the gradient, everything except these will be assumed to be constant, right?",
    "start": "3376690",
    "end": "3384369"
  },
  {
    "text": "And basically, you know, this is just calculus. You know, take the gradient, set them equal to 0,",
    "start": "3384370",
    "end": "3389485"
  },
  {
    "text": "and we end up with the update rules that is specific to your model and what was observed and what was not observed,",
    "start": "3389485",
    "end": "3395839"
  },
  {
    "text": "right? Any questions? Yes.",
    "start": "3396570",
    "end": "3403510"
  },
  {
    "text": "So, uh, why, uh, sorry, uh, so [NOISE] why is that- why we take the sum over j.",
    "start": "3403510",
    "end": "3411400"
  },
  {
    "text": "Because let's say for the ith example if it is associated with the jth cluster,",
    "start": "3411400",
    "end": "3416645"
  },
  {
    "text": "why would you want to update Mu, all the other Mu's except [OVERLAPPING] Well, because- so- so the question is why are we summing over j?",
    "start": "3416645",
    "end": "3423400"
  },
  {
    "text": "Because in- only in k-means we do- we assign it to only one point. In- in- in, um,",
    "start": "3423400",
    "end": "3428845"
  },
  {
    "text": "in case of the Gaussian mixture model, we- we- every point belong to every cluster with different weights.",
    "start": "3428845",
    "end": "3434440"
  },
  {
    "text": "Yes. But why- why- why we do that? Like why not have it belong to either of the Gaussian's?",
    "start": "3434440",
    "end": "3441100"
  },
  {
    "text": "So this, what we got here was purely a consequence of applying the EM algorithm and the EM algorithm we- we proved that it will,",
    "start": "3441100",
    "end": "3448660"
  },
  {
    "text": "you know, converge and it'll maximize. So the- the choice of summing over here was not an arbitrary choice.",
    "start": "3448660",
    "end": "3454720"
  },
  {
    "text": "It's just a consequence of the EM algorithm. [NOISE] Yes, question? Like what you said,",
    "start": "3454720",
    "end": "3461110"
  },
  {
    "text": "we- we make a point estimate, every time we get estimation of a state, the maximum, [inaudible]",
    "start": "3461110",
    "end": "3474490"
  },
  {
    "text": "So the question is instead of taking the expectation, what if in- instead we took the mode, you know, take the- the- the highest, uh, highest probability.",
    "start": "3474490",
    "end": "3482260"
  },
  {
    "text": "In that case, we will- Gaussian mixture models will- will- will be kind of modified into becoming k-means in that case.",
    "start": "3482260",
    "end": "3490240"
  },
  {
    "text": "[NOISE] All right, so factor analysis. [NOISE]",
    "start": "3490240",
    "end": "3503980"
  },
  {
    "text": "So factor analysis probably has the most tedious calculus in all this course.",
    "start": "3503980",
    "end": "3513010"
  },
  {
    "text": "So there's gonna be quite a lot of symbols that s- that are gonna come up on the board.",
    "start": "3513010",
    "end": "3519890"
  },
  {
    "text": "But even though the- the, uh, the expressions might look complex and hard,",
    "start": "3520530",
    "end": "3528300"
  },
  {
    "text": "the idea is pretty simple. [NOISE]",
    "start": "3528300",
    "end": "3545600"
  },
  {
    "text": "Factor analysis. [NOISE]",
    "start": "3545600",
    "end": "3559955"
  },
  {
    "text": "So, in factor analysis, we consider an interesting and challenging scenario.",
    "start": "3559955",
    "end": "3568100"
  },
  {
    "text": "So in factor analysis, right, you have x_i belonging to some R_d, right?",
    "start": "3568100",
    "end": "3576350"
  },
  {
    "text": "And you are given a collection x_i where i equals 1_n.",
    "start": "3576350",
    "end": "3584850"
  },
  {
    "text": "And now, we are interested in this- in this kind of, um- um,",
    "start": "3585220",
    "end": "3591785"
  },
  {
    "text": "situation where d is much bigger than n, right?",
    "start": "3591785",
    "end": "3598190"
  },
  {
    "text": "In most of the- most of the, er- er, common scenarios that we encounter, the number of examples that we- that we have,",
    "start": "3598190",
    "end": "3604235"
  },
  {
    "text": "will generally be- be much bigger than the dimension of the data, but there can be situations where the dimension of the data is",
    "start": "3604235",
    "end": "3612950"
  },
  {
    "text": "much bigger than the number of examples that we have, right? And it is- it is challenging because when we,",
    "start": "3612950",
    "end": "3621815"
  },
  {
    "text": "uh, using this, if we- if we, uh, assume this came from some kind of a Gaussian distribution,",
    "start": "3621815",
    "end": "3627650"
  },
  {
    "text": "and if we estimate the covariance matrix sigma, sigma will now be 1 over n,",
    "start": "3627650",
    "end": "3633530"
  },
  {
    "text": "sum over i equals 1 to n x_i minus mu, x_i minus u_T.",
    "start": "3633530",
    "end": "3643440"
  },
  {
    "text": "So each of these, each- each term inside the sum is- is one rank,",
    "start": "3644110",
    "end": "3650119"
  },
  {
    "text": "one matrix and you're summing over n rank one matrices. So sigma will be rank n. But n is much smaller than d, right?",
    "start": "3650120",
    "end": "3662750"
  },
  {
    "text": "Sigma is a d by d matrix, but it's only rank n, right? And now if you want to er,",
    "start": "3662750",
    "end": "3667760"
  },
  {
    "text": "use this covariance matrix to calculate your Gaussian PDF, right? Your Gaussian PDF is p of x given mu",
    "start": "3667760",
    "end": "3676100"
  },
  {
    "text": "sigma 1 over 2pi to the d by 2.",
    "start": "3676100",
    "end": "3683880"
  },
  {
    "text": "So the determinant over here will be 0 because it is- it- it is not full rank, right?",
    "start": "3685090",
    "end": "3691850"
  },
  {
    "text": "And- and because you get- you get er, a 0 in the denominator,",
    "start": "3691850",
    "end": "3698345"
  },
  {
    "text": "you know you can't even evaluate the- the er, - evaluate the PDF, right? And- and er, the question is now what",
    "start": "3698345",
    "end": "3705110"
  },
  {
    "text": "do we do in such scenarios? Yes, was there a question? [BACKGROUND] I'm sorry? [BACKGROUND]",
    "start": "3705110",
    "end": "3710360"
  },
  {
    "text": "This will be rank n because we are summing over n rank one matrices. [BACKGROUND]",
    "start": "3710360",
    "end": "3716569"
  },
  {
    "text": "Matrix is d by d, Sigma is d by d, right?",
    "start": "3716570",
    "end": "3724910"
  },
  {
    "text": "But it will have only rank n. And most of the times when n is bigger than d, it will be rank d, right?",
    "start": "3724910",
    "end": "3733055"
  },
  {
    "text": "But in- in cases where n is much smaller than d, the rank will be- so- so this will be a singular matrix",
    "start": "3733055",
    "end": "3740705"
  },
  {
    "text": "and we- we can't do much with it. Yes question.",
    "start": "3740705",
    "end": "3745760"
  },
  {
    "text": "[BACKGROUND] I wouldn't say so. I- I- I'll come to that.",
    "start": "3745760",
    "end": "3752630"
  },
  {
    "text": "So we- we- we are now just considering a scenario where Xs are coming from a high-dimensional space.",
    "start": "3752630",
    "end": "3759950"
  },
  {
    "text": "And um- the- and- and the number of dimensions is much greater",
    "start": "3759950",
    "end": "3766730"
  },
  {
    "text": "than the number of um- um- the number of dimension is much greater than the number of examples we have, right?",
    "start": "3766730",
    "end": "3773450"
  },
  {
    "text": "[NOISE] We can think of- kind of, you know, a few ways in which we can kind of address this.",
    "start": "3773450",
    "end": "3780650"
  },
  {
    "text": "So one way is, you know, instead of thinking- instead of considering sigma to be, you know,",
    "start": "3780650",
    "end": "3786575"
  },
  {
    "text": "a full-blown covariance matrix, maybe we can restrict sigma to be just the diagonal matrix so that the number of parameters reduce.",
    "start": "3786575",
    "end": "3794525"
  },
  {
    "text": "But even if you limited to a diagonal matrix, the number of- of diagonal values is still d that's greater",
    "start": "3794525",
    "end": "3802010"
  },
  {
    "text": "than n. We could also think of um- um, sigma- thinking of um,",
    "start": "3802010",
    "end": "3807830"
  },
  {
    "text": "restricting sigma to be some kind of a scalar times the identity matrix.",
    "start": "3807830",
    "end": "3812855"
  },
  {
    "text": "But if we do this, then we are effectively limiting ourselves to covariances that are spherical, right?",
    "start": "3812855",
    "end": "3818720"
  },
  {
    "text": "And that may not capture all the interesting um- um- um, structure in your data. So this wouldn't be very interesting either.",
    "start": "3818720",
    "end": "3826175"
  },
  {
    "text": "So instead, what we want to know um- now do in- in um,",
    "start": "3826175",
    "end": "3832430"
  },
  {
    "text": "- with- with um, factor analysis is to think of a latent variable z.",
    "start": "3832430",
    "end": "3838089"
  },
  {
    "text": "So assume that your data, the true data that- so- so we are observing x's but we're going to assume that these",
    "start": "3838090",
    "end": "3845585"
  },
  {
    "text": "x's live on some kind of a lower dimensional subspace, right? So we're going to assume that there is a z,",
    "start": "3845585",
    "end": "3854460"
  },
  {
    "text": "right, that's normally distributed, right?",
    "start": "3854680",
    "end": "3860825"
  },
  {
    "text": "And then we're going to make this second assumption um, that- right?",
    "start": "3860825",
    "end": "3867650"
  },
  {
    "text": "So over here, i is- i is basically",
    "start": "3867650",
    "end": "3874355"
  },
  {
    "text": "k-dimensional and zi is therefore Rk.",
    "start": "3874355",
    "end": "3884285"
  },
  {
    "text": "So we're going to assume that there is this lower k-dimensional subspace in which these recite, right?",
    "start": "3884285",
    "end": "3891800"
  },
  {
    "text": "And from these- from this k-dimensional subspace, [NOISE] we are now- so x given",
    "start": "3891800",
    "end": "3903320"
  },
  {
    "text": "z is now normal distribution",
    "start": "3903320",
    "end": "3910430"
  },
  {
    "text": "with mean mu plus- so in the- in the notes,",
    "start": "3910430",
    "end": "3917134"
  },
  {
    "text": "they use capital lambda, right, to denote the um- um the um- up mapping matrix.",
    "start": "3917134",
    "end": "3925895"
  },
  {
    "text": "I personally don't like Greek letters. There's nothing against Greeks, but I'm just going to call it L because you know,",
    "start": "3925895",
    "end": "3931775"
  },
  {
    "text": "it looks less scary, right? So there is a- a- a matrix L times Z and a covariance Psi, right?",
    "start": "3931775",
    "end": "3944000"
  },
  {
    "text": "Let's leave one Greek- one more Greek in there, no problem, right? So here L is a matrix that maps us from k to d,",
    "start": "3944000",
    "end": "3953839"
  },
  {
    "text": "d by k, right? And you're going to assume Psi is- Are there any Greeks here?",
    "start": "3953840",
    "end": "3966215"
  },
  {
    "text": "What's the right way to pronounce this? Is it Psi or Psi? Anybody knows?",
    "start": "3966215",
    "end": "3972035"
  },
  {
    "text": "Psi? Psi. All right, So I'll call it Psi. So Psi is d by d and we're going to assume it is diagonal, all right?",
    "start": "3972035",
    "end": "3984060"
  },
  {
    "text": "So what's happening here? We're going to assume that there is this low",
    "start": "3987490",
    "end": "3994190"
  },
  {
    "text": "dimensional or k dimensional subspace where k, we will assume is less than n,",
    "start": "3994190",
    "end": "4001089"
  },
  {
    "text": "the number of examples that we have. All right? And there are",
    "start": "4001090",
    "end": "4006099"
  },
  {
    "text": "these latent variables z's that reside in this k-dimensional subspace,",
    "start": "4006100",
    "end": "4011425"
  },
  {
    "text": "which get mapped on to a higher-dimensional space, which is x's, and the way it gets- it get's mapped on is through this,",
    "start": "4011425",
    "end": "4021819"
  },
  {
    "text": "you know, what- what you can call as an uplifting matrix where z's when you multiply it with L,",
    "start": "4021819",
    "end": "4028930"
  },
  {
    "text": "it takes you from k- dimensional to, so Lz will be in d dimensions, and there is, you know,",
    "start": "4028930",
    "end": "4035020"
  },
  {
    "text": "it gets shifted by some- some offset Mu and some- there is some random noise that that gets- a diagonal noise that- that gets added, right?",
    "start": "4035020",
    "end": "4045460"
  },
  {
    "text": "So, previously in Gaussian mixture models, in order to compare it to Gaussian mixture models,",
    "start": "4045460",
    "end": "4050830"
  },
  {
    "text": "z is a multinomial in Gaussian mixture models, but now z is continuous here. All right? x given z was- in the Gaussian mixture model,",
    "start": "4050830",
    "end": "4060835"
  },
  {
    "text": "was nor- normally distributed. Here also it's normally distributed except in the Gaussian mixture model for each z,",
    "start": "4060835",
    "end": "4069490"
  },
  {
    "text": "because z was discrete, we had a separate Mu and- and a Sigma. But over here, you're not gonna have a separate Mu and- and a Sigma,",
    "start": "4069490",
    "end": "4079015"
  },
  {
    "text": "but the- the mean for x given z will be this term,",
    "start": "4079015",
    "end": "4084700"
  },
  {
    "text": "which is some parameter Mu that's common across all examples, plus the mapping of z from the load-",
    "start": "4084700",
    "end": "4093265"
  },
  {
    "text": "from the k-dimensional lower subspace to d-dimensional, that's the higher-dimensional subspace.",
    "start": "4093265",
    "end": "4098935"
  },
  {
    "text": "So L is this mapping that takes you from a lower dimension to a higher dimension. Yes, question?",
    "start": "4098935",
    "end": "4105025"
  },
  {
    "text": "What is z? Is z a- you're not doing discrete classification?",
    "start": "4105025",
    "end": "4111460"
  },
  {
    "text": "So this- so in factor analysis, we have moved on from classification, and here we are essentially trying to find a subspace of",
    "start": "4111460",
    "end": "4119620"
  },
  {
    "text": "our- of our- of the x's that we have, right? x's live in a high dimensional subspace.",
    "start": "4119620",
    "end": "4124884"
  },
  {
    "text": "What we're trying to do is to find a low dimensional subspace in which they reside.",
    "start": "4124885",
    "end": "4130644"
  },
  {
    "text": "In which z's reside? In which z's reside and the corresponding- and we assume",
    "start": "4130645",
    "end": "4136080"
  },
  {
    "text": "that the- the- the- the x's that we observe have a corresponding latent variable z",
    "start": "4136080",
    "end": "4142940"
  },
  {
    "text": "and the way x is generated from its latent variable is through this relation.",
    "start": "4142940",
    "end": "4148315"
  },
  {
    "text": "What is- what is your goal exactly? The goal here is to fit, um, as with all, you know,",
    "start": "4148315",
    "end": "4155154"
  },
  {
    "text": "probabilistic unsupervised models, the goal is to-P of x Theta.",
    "start": "4155155",
    "end": "4161170"
  },
  {
    "text": "In this case, Theta is you know Mu, Sigma and- and",
    "start": "4161170",
    "end": "4165739"
  },
  {
    "text": "And? [inaudible]. Yeah, Mu, so what do we have here?",
    "start": "4166560",
    "end": "4172839"
  },
  {
    "text": "Mu, Psi and L. All right? And we want to find these Mu, Psi,",
    "start": "4172840",
    "end": "4179589"
  },
  {
    "text": "L such that we have- we have a probabilistic model or a density estimator over x's,",
    "start": "4179590",
    "end": "4185395"
  },
  {
    "text": "and it would- there are many interesting um, um, scenarios where this- this- this can happen.",
    "start": "4185395",
    "end": "4192400"
  },
  {
    "text": "For example, uh, you can, you know, uh,",
    "start": "4192400",
    "end": "4197650"
  },
  {
    "text": "the reasons- the- the- the kind of scenarios where this can be useful is supposing you have some kind of a temperature sensor, right?",
    "start": "4197650",
    "end": "4205239"
  },
  {
    "text": "So you can have temperature sensors that are, you know, spread across your building. All right? And let's say there are d such sensors.",
    "start": "4205240",
    "end": "4214600"
  },
  {
    "text": "Right? And if you measure all your sensors at a particular time, that collection of observations can be your x^i's. All right?",
    "start": "4214600",
    "end": "4221950"
  },
  {
    "text": "So x^i belongs to R^d. So instead of i, you can think of it as t. At",
    "start": "4221950",
    "end": "4228400"
  },
  {
    "text": "one time mea- you- you take the measurement of all your sensors, of temperature sensors in your entire building,",
    "start": "4228400",
    "end": "4234145"
  },
  {
    "text": "and you get the d-dimensional, high dimensional subspace. But, you know, all these- these different temperature values may not be independent.",
    "start": "4234145",
    "end": "4243625"
  },
  {
    "text": "Which means, you know, two temp- temperature sensors inside this room are probably gonna give you,",
    "start": "4243625",
    "end": "4249040"
  },
  {
    "text": "you know, very similar values. Right? And similarly, if you have 10 different sensors here,",
    "start": "4249040",
    "end": "4254365"
  },
  {
    "text": "maybe you might see some slight changes where, you know, it might be closer to where the light bulbs are because of, you know,",
    "start": "4254365",
    "end": "4261985"
  },
  {
    "text": "it's warmer and maybe colder in a few places, but more or less they're not independent. And there are these um,",
    "start": "4261985",
    "end": "4269304"
  },
  {
    "text": "this similarly if you- you know, collect the set of all temperature sensors in the entire building,",
    "start": "4269305",
    "end": "4275770"
  },
  {
    "text": "there are probably a few factors that- that affect the different sensor readings across all the- you know,",
    "start": "4275770",
    "end": "4283030"
  },
  {
    "text": "across all the sensors. All right? And basically, what we are trying to find out- our goal here is to come up with a model, you know,",
    "start": "4283030",
    "end": "4293310"
  },
  {
    "text": "for P of x, and- and- and - and for example,",
    "start": "4293310",
    "end": "4299055"
  },
  {
    "text": "come up with a model for P of x to get a sense of what are normal sensor readings.",
    "start": "4299055",
    "end": "4304760"
  },
  {
    "text": "Right? And using this, you can probably build some kind of an anomaly detector to see if there's,",
    "start": "4304760",
    "end": "4310885"
  },
  {
    "text": "you know, a fire going on somewhere. Right? And the idea here is that even though x's reside in a much higher dimensional subspace,",
    "start": "4310885",
    "end": "4320980"
  },
  {
    "text": "their values, the actual temperatures that they end up measuring are based on a few- much fewer factors.",
    "start": "4320980",
    "end": "4327309"
  },
  {
    "text": "Right? And z's are, you know, that live in a k-dimensional subspace. The assumption here is that there are probably just k factors",
    "start": "4327310",
    "end": "4334540"
  },
  {
    "text": "that decide the temperatures that you observe in all the d sensors, and- and- and given these d-dimensional observations, we are trying to uh,",
    "start": "4334540",
    "end": "4343345"
  },
  {
    "text": "build a model from which we can not only fit P of x well, but also hopefully make inference about z units.",
    "start": "4343345",
    "end": "4352130"
  },
  {
    "text": "It's kind of a dual purpose. Uh, in- in case of GM Gaussian mixture models,",
    "start": "4353310",
    "end": "4358660"
  },
  {
    "text": "these were discrete where we were trying to cluster data. But here it's- it's- it's a fundamentally different problem that we're trying to find subspaces in our data.",
    "start": "4358660",
    "end": "4366730"
  },
  {
    "text": "So this is no longer- we're no longer doing the EM? We will do EM on this, right.",
    "start": "4366730",
    "end": "4372145"
  },
  {
    "text": "EM made no assumption about z being discrete or- or- or- or um- discrete or continuous.",
    "start": "4372145",
    "end": "4380545"
  },
  {
    "text": "When we wrote out the- the uh, ELBO. Right? So we intentionally used expectation here and if z was continuous,",
    "start": "4380545",
    "end": "4389709"
  },
  {
    "text": "then this summation will be an integral. Like we make no assumptions about z being continuous or discrete in the EM.",
    "start": "4389709",
    "end": "4397105"
  },
  {
    "text": "Right? So uh- so back to where we were,",
    "start": "4397105",
    "end": "4403480"
  },
  {
    "text": "so P of x, huh, We want to learn a model P of x so that we can do things like anomaly detection. Yes question?",
    "start": "4403480",
    "end": "4411400"
  },
  {
    "text": "[inaudible]",
    "start": "4411400",
    "end": "4426280"
  },
  {
    "text": "Are we going to learn [inaudible] Yeah, so the question is, is- is K going to",
    "start": "4426280",
    "end": "4432580"
  },
  {
    "text": "be learned automatically or is it going to be hyper-parameter? For the purpose of this lecture, assume it's a hyper-parameter,",
    "start": "4432580",
    "end": "4438130"
  },
  {
    "text": "and your tuned different values of K. Yes, question. [inaudible]",
    "start": "4438130",
    "end": "4449320"
  },
  {
    "text": "Yes. So- so this is- this is, ah, the, ah- the ma- the assumptions that we're ma- making over here are what is called as,",
    "start": "4449320",
    "end": "4457555"
  },
  {
    "text": "you know, factor analysis, right?",
    "start": "4457555",
    "end": "4464515"
  },
  {
    "text": "And there are- there are, ah, you know, few variants of factor analysis in- in fact. So here, we are assuming,",
    "start": "4464515",
    "end": "4470275"
  },
  {
    "text": "you know, the- the- the, ah, Psi matrix is diagonal and has, you know, different values on the diagonal.",
    "start": "4470275",
    "end": "4476545"
  },
  {
    "text": "You can tweak that assumption to say that is, you know, equal, uh, that is- that is, uh,",
    "start": "4476545",
    "end": "4482485"
  },
  {
    "text": "equal noise along all the diagonals, um, and- and, you know, that gives you a different model.",
    "start": "4482485",
    "end": "4488485"
  },
  {
    "text": "You can make few other small little changes here. And that gives you something called as probabilistic PCA.",
    "start": "4488485",
    "end": "4494005"
  },
  {
    "text": "This specific set of assumptions is called factor analysis. [inaudible]",
    "start": "4494005",
    "end": "4501430"
  },
  {
    "text": "Yeah, so that's just, you know, an assumption that we're making. [inaudible] Yeah, that's- so the assumption that Z has mean",
    "start": "4501430",
    "end": "4507130"
  },
  {
    "text": "0 and identity 1 is just an arbitrary assumption. And it turns out to be that, you know, it- it is,",
    "start": "4507130",
    "end": "4512560"
  },
  {
    "text": "most of the times that's- that's good enough, right? So making that assumption that our X has mean 0 and identity 1 would be,",
    "start": "4512560",
    "end": "4522204"
  },
  {
    "text": "you know, absurd, right? But then we have this, you know, offset Mu and, you know,",
    "start": "4522205",
    "end": "4527875"
  },
  {
    "text": "scaling L that we applied to Z. So you know, usually that- that this- this,",
    "start": "4527875",
    "end": "4535090"
  },
  {
    "text": "um- the L and Mu make up for the fact that we don't have any degrees of freedom here, right?",
    "start": "4535090",
    "end": "4545070"
  },
  {
    "text": "So um, and now our goal is to, using- using this set of assumptions,",
    "start": "4545070",
    "end": "4551799"
  },
  {
    "text": "we want to maximize, we want to learn log P of X given Mu,",
    "start": "4551799",
    "end": "4559495"
  },
  {
    "text": "L and Psi, right? How do we go about doing it?",
    "start": "4559495",
    "end": "4565745"
  },
  {
    "text": "The answer is EM, right? And as I- as I- as I, ah, mentioned already,",
    "start": "4565745",
    "end": "4572889"
  },
  {
    "text": "the first thing to do in EM is to be clear on what the model is.",
    "start": "4572890",
    "end": "4579620"
  },
  {
    "text": "So the model that we have was described like this initially.",
    "start": "4586040",
    "end": "4591360"
  },
  {
    "text": "So Z comes from normal zero identity and X",
    "start": "4591360",
    "end": "4601179"
  },
  {
    "text": "given Z comes from normal Mu plus Lz and Psi.",
    "start": "4601180",
    "end": "4613010"
  },
  {
    "text": "We are now gonna make the small change and rewrite this as",
    "start": "4615000",
    "end": "4621310"
  },
  {
    "text": "an equivalent model Z identity Epsilon comes from",
    "start": "4621310",
    "end": "4632630"
  },
  {
    "text": "0, Psi and X is equal to Mu",
    "start": "4634320",
    "end": "4641380"
  },
  {
    "text": "plus Lz plus Epsilon. Yes, question.",
    "start": "4641380",
    "end": "4649929"
  },
  {
    "text": "[inaudible]",
    "start": "4649930",
    "end": "4676840"
  },
  {
    "text": "why is Psi not dispersing? We'll- we'll see why spi will- Psi will not disperse. We're gonna work it out.",
    "start": "4676840",
    "end": "4682300"
  },
  {
    "text": "And so, um, so these two are basically equivalent.",
    "start": "4682300",
    "end": "4692425"
  },
  {
    "text": "Are there any questions on why these two are equivalent or not?",
    "start": "4692425",
    "end": "4697159"
  },
  {
    "text": "Because we will- we will be using the trick again in a couple of lectures?",
    "start": "4697890",
    "end": "4703015"
  },
  {
    "text": "Are you clear on why these two are equivalent? So this is basically called the- the, ah,",
    "start": "4703015",
    "end": "4711415"
  },
  {
    "text": "scale and location property of Gaussians, where if you take,",
    "start": "4711415",
    "end": "4716875"
  },
  {
    "text": "you know, some Gaussian, right? And, um, so this basically follows from the fact that, um,",
    "start": "4716875",
    "end": "4725730"
  },
  {
    "text": "over here, um, you can write x as, um, you can decompose this into- into two parts.",
    "start": "4725730",
    "end": "4733260"
  },
  {
    "text": "So you can- x given c has this Mu plus Lz and- and, um, Sigma.",
    "start": "4733260",
    "end": "4743019"
  },
  {
    "text": "So this basically tells you that for this given mean.",
    "start": "4743020",
    "end": "4749050"
  },
  {
    "text": "So think of this as the mean, and think of this as the covariance, right?",
    "start": "4749050",
    "end": "4754360"
  },
  {
    "text": "So a normal distribution, if it has some mean and some, ah, ah, covariance,",
    "start": "4754360",
    "end": "4763900"
  },
  {
    "text": "you can write this as m plus normal distribution with some mean",
    "start": "4763900",
    "end": "4769510"
  },
  {
    "text": "and some covariance, right?",
    "start": "4769510",
    "end": "4776260"
  },
  {
    "text": "[inaudible] All right. So, yeah, so this is bad notation.",
    "start": "4776260",
    "end": "4781690"
  },
  {
    "text": "Think of them as you are adding one- one, ah, number to a random variable that's distributed according to them.",
    "start": "4781690",
    "end": "4787120"
  },
  {
    "text": "[inaudible]",
    "start": "4787120",
    "end": "4794650"
  },
  {
    "text": "So, ah, so assume, ah, you know, there is a random, random variable which has mean m and, ah,",
    "start": "4794650",
    "end": "4802465"
  },
  {
    "text": "which is distributed normally according to mean m and stan- covariance s, right?",
    "start": "4802465",
    "end": "4807730"
  },
  {
    "text": "This random variable can be written as the sum of a constant plus another random variable, right?",
    "start": "4807730",
    "end": "4814630"
  },
  {
    "text": "So this- this other random variable has mean zero and covariance s, right? So you're basically, you know,",
    "start": "4814630",
    "end": "4820555"
  },
  {
    "text": "I'm doing essentially the same. So here the- this is Epsilon over here. Yes, question.",
    "start": "4820555",
    "end": "4829810"
  },
  {
    "text": "[inaudible]",
    "start": "4829810",
    "end": "4841330"
  },
  {
    "text": "So for- x had, you know, mean, I mean, you can see here, right?",
    "start": "4841330",
    "end": "4846820"
  },
  {
    "text": "So this is just Mu plus Lz, and we just repeat. Mu plus Lz plus Epsilon.",
    "start": "4846820",
    "end": "4852940"
  },
  {
    "text": "[inaudible] any contribution from Mu,",
    "start": "4852940",
    "end": "4860290"
  },
  {
    "text": "because X is Mu plus [OVERLAPPING] It will, but this is just x equals, we haven't written the distribution of x here.",
    "start": "4860290",
    "end": "4867640"
  },
  {
    "text": "You can just rewrite x as- as this thing. So, ah, this gives us, you know,",
    "start": "4867640",
    "end": "4875740"
  },
  {
    "text": "making this observation, we can now write out model, no,",
    "start": "4875740",
    "end": "4882265"
  },
  {
    "text": "z comma x will have some joint,",
    "start": "4882265",
    "end": "4893199"
  },
  {
    "text": "Mu z, x, and some covariance sigma, right?",
    "start": "4893200",
    "end": "4899290"
  },
  {
    "text": "So what we're gonna do for the next few minutes is to write out this model, right?",
    "start": "4899290",
    "end": "4904450"
  },
  {
    "text": "And once we are clear about the model and clear about the different components, then we're gonna attack it using EM. So first, as we did it with the Gaussian mixture model,",
    "start": "4904450",
    "end": "4912520"
  },
  {
    "text": "first we want to be very clear about the model. What are the latent variables, what are the observed variables? What are our parameters, right?",
    "start": "4912520",
    "end": "4919329"
  },
  {
    "text": "And it turns out that we can- so this Mu z,",
    "start": "4919330",
    "end": "4929260"
  },
  {
    "text": "x, will have two parts,",
    "start": "4929260",
    "end": "4934929"
  },
  {
    "text": "a mean corresponding to z and a mean corresponding to x, right? This is just the standard,",
    "start": "4934930",
    "end": "4940405"
  },
  {
    "text": "ah, Multivariate Gaussian properties. We- we saw such- we saw these properties using- when we were talking about Gaussian processes, right?",
    "start": "4940405",
    "end": "4948400"
  },
  {
    "text": "So this will correspond to the mean of z. So the mean of z is just 0, right?",
    "start": "4948400",
    "end": "4955690"
  },
  {
    "text": "And the mean of x, so the mean of x will be just,",
    "start": "4955690",
    "end": "4962985"
  },
  {
    "text": "um, so what will be the mean of x?",
    "start": "4962985",
    "end": "4967580"
  },
  {
    "text": "Mean of x is equal to the mean of Mu plus Lz plus Epsilon.",
    "start": "4969510",
    "end": "4978105"
  },
  {
    "text": "This is equal to Mu plus 0 plus 0, which is Mu, Mu.",
    "start": "4978105",
    "end": "4986020"
  },
  {
    "text": "So for the- for the joint probability distribution, p of x comma z is distributed according to a normal distribution whose mean is 0 Mu,",
    "start": "4986020",
    "end": "4995815"
  },
  {
    "text": "right? And the covariance-",
    "start": "4995815",
    "end": "4999770"
  },
  {
    "text": "So again, the covariance will have four parts, right? So here we will have covariance of z,",
    "start": "5008090",
    "end": "5016390"
  },
  {
    "text": "covariance of- covariance of x z,",
    "start": "5016390",
    "end": "5027850"
  },
  {
    "text": "x z transpose and covariance of x, all right?",
    "start": "5028460",
    "end": "5035340"
  },
  {
    "text": "And [NOISE] it can",
    "start": "5035340",
    "end": "5042420"
  },
  {
    "text": "be shown pretty straightforwardly that covariance of z is just identity, right?",
    "start": "5042420",
    "end": "5049050"
  },
  {
    "text": "And this will- the covariance of - of, uh, [NOISE]",
    "start": "5049050",
    "end": "5064680"
  },
  {
    "text": "the covariance of this- this is- so the covariance of x and e,",
    "start": "5064680",
    "end": "5073560"
  },
  {
    "text": "uh, uh, covariance of, uh, x and z is basically by definition, expectation of x minus expectation of x,",
    "start": "5073560",
    "end": "5083835"
  },
  {
    "text": "z minus expectation of z transpose.",
    "start": "5083835",
    "end": "5089535"
  },
  {
    "text": "And if you expand this out, this will just turn out to be L, [NOISE] and here you get L transpose.",
    "start": "5089535",
    "end": "5098805"
  },
  {
    "text": "I know, so this will be L transpose, this will be L, and here we get LL transpose plus Psi, right?",
    "start": "5098805",
    "end": "5110100"
  },
  {
    "text": "So the- the- the model or the- or the joint probability of x and z is gonna be our normal distribution,",
    "start": "5110100",
    "end": "5117675"
  },
  {
    "text": "a multivariate Gaussian distribution whose mean is given by, uh, um, you know,",
    "start": "5117675",
    "end": "5123614"
  },
  {
    "text": "zero and Mu and whose covariance is given by identity,",
    "start": "5123615",
    "end": "5128685"
  },
  {
    "text": "uh, LL transpose- LL transpose plus, uh, Psi. And the way you, uh,",
    "start": "5128685",
    "end": "5133740"
  },
  {
    "text": "arrive at this is by basically going with the definition of covariance of x and z to be expectation of x minus expectation of x times z minus expectation of z transpose, right?",
    "start": "5133740",
    "end": "5143890"
  },
  {
    "text": "And- and uh, similarly for - for the expectation of x, uh, you do the same x minus expectation of x,",
    "start": "5143890",
    "end": "5149699"
  },
  {
    "text": "x minus expectation tra, uh, transpose and you'll get these terms and, you know, the detailed steps are in the notes.",
    "start": "5149700",
    "end": "5155925"
  },
  {
    "text": "But the, uh, but the- the- the, uh, the higher-level message that we wanna take, uh,",
    "start": "5155925",
    "end": "5161750"
  },
  {
    "text": "take from this is given some- some data-generating process.",
    "start": "5161750",
    "end": "5167335"
  },
  {
    "text": "The first step that we did was to come up with, uh, the definition of the model [NOISE] so the model in this case,",
    "start": "5167335",
    "end": "5175980"
  },
  {
    "text": "[NOISE] turned out to be x and z",
    "start": "5175980",
    "end": "5182160"
  },
  {
    "text": "are jointly multivariate Gaussian with a given,",
    "start": "5182160",
    "end": "5189675"
  },
  {
    "text": "you know, with some particular mean and some particular covariance, right?",
    "start": "5189675",
    "end": "5196770"
  },
  {
    "text": "So the model, right, that was our- our- our- th - the first step that we wanna do is P of",
    "start": "5196770",
    "end": "5204840"
  },
  {
    "text": "x comma z is such that x z,",
    "start": "5204840",
    "end": "5210388"
  },
  {
    "text": "[NOISE] is jointly normal with mean zero Mu or z x rather,",
    "start": "5210389",
    "end": "5223270"
  },
  {
    "text": "z x Mu and covariance I,",
    "start": "5226400",
    "end": "5234480"
  },
  {
    "text": "L transpose L and LL transpose plus Psi all right?",
    "start": "5234480",
    "end": "5243255"
  },
  {
    "text": "So this is the, you know, the full model, and the parameters- parameters are basically Mu,",
    "start": "5243255",
    "end": "5255795"
  },
  {
    "text": "L, and Psi, [NOISE] all right?",
    "start": "5255795",
    "end": "5261960"
  },
  {
    "text": "And in this, what have you- what did we observe? We observe? We observe,",
    "start": "5261960",
    "end": "5268605"
  },
  {
    "text": "what's observed in this? x, and the latent variable is z okay?",
    "start": "5268605",
    "end": "5277260"
  },
  {
    "text": "So parameters, evidence, latent variable, right?",
    "start": "5277260",
    "end": "5283380"
  },
  {
    "text": "And now to apply EM on this,",
    "start": "5283380",
    "end": "5288340"
  },
  {
    "text": "the E-step [NOISE] you wanna calculate P of z given x, right?",
    "start": "5288440",
    "end": "5295199"
  },
  {
    "text": "So in the E-step, we attack the latent variables, and in the M-step,",
    "start": "5295200",
    "end": "5300840"
  },
  {
    "text": "you wanna do Mu L Sigma equals average max of the ELBO.",
    "start": "5300840",
    "end": "5309119"
  },
  {
    "text": "[NOISE] All right so this is- this is gonna be the- the high level flow of what we're gonna do next, okay?",
    "start": "5309120",
    "end": "5317415"
  },
  {
    "text": "And this- this- this, uh, recipe is the same for any kind of model on",
    "start": "5317415",
    "end": "5323220"
  },
  {
    "text": "which or any kind of latent variable model that we wanna solve using EM, all right? First, get clarity on the full model of- of the joint distribution,",
    "start": "5323220",
    "end": "5331080"
  },
  {
    "text": "identify what are the parameters? What's the evidence? What's the latent variable?",
    "start": "5331080",
    "end": "5336225"
  },
  {
    "text": "And then figure out the E-step and M-step where in the E-step, cal, you know,",
    "start": "5336225",
    "end": "5343200"
  },
  {
    "text": "re-estimate the latent variables, and in the M-step, re-estimate the parameters, all right?",
    "start": "5343200",
    "end": "5351690"
  },
  {
    "text": "[NOISE] It so happens",
    "start": "5351690",
    "end": "5361080"
  },
  {
    "text": "that in this- in the- in the- in the factor analysis model,",
    "start": "5361080",
    "end": "5366300"
  },
  {
    "text": "[NOISE] it so happens that because they are jointly Gaussian.",
    "start": "5366300",
    "end": "5375735"
  },
  {
    "text": "You know this is, you know a side note, [NOISE] right?",
    "start": "5375735",
    "end": "5382179"
  },
  {
    "text": "We- we can actually write L of Mu Sigma,",
    "start": "5382940",
    "end": "5389415"
  },
  {
    "text": "oops not Sigma write somewhere there no, yeah Mu L and Psi [NOISE] log P of x,",
    "start": "5389415",
    "end": "5400395"
  },
  {
    "text": "we are now using those parameters. And using the modularization property of- of, uh,",
    "start": "5400395",
    "end": "5407205"
  },
  {
    "text": "of Gaussians this actually has, you know, you can- you can write it out as,",
    "start": "5407205",
    "end": "5414789"
  },
  {
    "text": "um, um, you know, um, x is actually distributed according to normal, you know, uh,",
    "start": "5414800",
    "end": "5423825"
  },
  {
    "text": "we can just read off this, it has normal mu and variance LL transpose sigma, right?",
    "start": "5423825",
    "end": "5429690"
  },
  {
    "text": "Mu LL transpose plus Sigma, right?",
    "start": "5429690",
    "end": "5436785"
  },
  {
    "text": "But now, if you tried to, uh, perform maximum likelihood estimation and obtain expressions for these,",
    "start": "5436785",
    "end": "5446220"
  },
  {
    "text": "you cannot get a closed-form expression, all right?",
    "start": "5446220",
    "end": "5451485"
  },
  {
    "text": "Which is why, uh, EM kind of comes in- comes to our help here because, uh, in- in this case it so happens that log P of x,",
    "start": "5451485",
    "end": "5459780"
  },
  {
    "text": "you can actually write out, you know, it as 1 over, you know, 2 Pi to the d by 2.",
    "start": "5459780",
    "end": "5466560"
  },
  {
    "text": "And here LL transpose plus Psi to the half exponent minus 1.5 [NOISE] x minus",
    "start": "5466560",
    "end": "5476040"
  },
  {
    "text": "mu transpose LL transpose plus Psi inverse x minus Mu, right?",
    "start": "5476040",
    "end": "5485400"
  },
  {
    "text": "[NOISE] You can write it out like this, but there is no closed-form solution that you can- that you can,",
    "start": "5485400",
    "end": "5493034"
  },
  {
    "text": "uh, um, come up with for L's and L's and Psi. You can try it out but, you know, you- this cannot be solved, uh, uh, using closed form.",
    "start": "5493035",
    "end": "5500429"
  },
  {
    "text": "[NOISE] So instead what we do is, we- we use factor analysis,",
    "start": "5500430",
    "end": "5505500"
  },
  {
    "text": "uh, I'm sorry, yeah, expectation-maximization. And with expectation maximization,",
    "start": "5505500",
    "end": "5510855"
  },
  {
    "text": "we actually do get closed-form update steps for both the E-step and M-step.",
    "start": "5510855",
    "end": "5517155"
  },
  {
    "text": "Though they are a bit tedious in terms of how complex the expressions look,",
    "start": "5517155",
    "end": "5525659"
  },
  {
    "text": "but it is, you know, pretty straightforward it's just tedious, it's not tricky. [NOISE]",
    "start": "5525660",
    "end": "5537960"
  },
  {
    "text": "Right? So for EM, for factor analysis, the first thing we want to, you know,",
    "start": "5537960",
    "end": "5543390"
  },
  {
    "text": "attack for the E step is calculate p of z given x.",
    "start": "5543390",
    "end": "5552780"
  },
  {
    "text": "Right? p of z given x. In this case, both z and x are-are-are jointly Gaussian,",
    "start": "5552780",
    "end": "5562064"
  },
  {
    "text": "which is the best you can hope for, right? When- um, I'm gonna just write out the-the full model here.",
    "start": "5562064",
    "end": "5568290"
  },
  {
    "text": "So z- zx normal",
    "start": "5568290",
    "end": "5578020"
  },
  {
    "text": "0 miu and covariance identity",
    "start": "5578150",
    "end": "5584219"
  },
  {
    "text": "l transpose l- ll transpose plus Psi.",
    "start": "5584220",
    "end": "5590790"
  },
  {
    "text": "All right, so what is p of z given x?",
    "start": "5590790",
    "end": "5597225"
  },
  {
    "text": "You know, looking at this, we can just read it off, right? If you- if you, um,",
    "start": "5597225",
    "end": "5602715"
  },
  {
    "text": "we did the same thing for- in- in- in Gaussian processes where we calculated the-the conditionals of Gaussian.",
    "start": "5602715",
    "end": "5609929"
  },
  {
    "text": "Right? In this case, um, in this case,",
    "start": "5609930",
    "end": "5615300"
  },
  {
    "text": "p of z given x will be- z given x we have",
    "start": "5615300",
    "end": "5622244"
  },
  {
    "text": "a normal distribution where the mean is given by l transpose,",
    "start": "5622245",
    "end": "5631210"
  },
  {
    "text": "LL transpose plus Psi inverse x minus Mu, right?",
    "start": "5631640",
    "end": "5642060"
  },
  {
    "text": "So this is basically, you know, if- if you- if you remember a,b if they are jointly distributed as",
    "start": "5642060",
    "end": "5651405"
  },
  {
    "text": "Mu- Mu a Mu b",
    "start": "5651405",
    "end": "5656445"
  },
  {
    "text": "and some kind of a covariance Sigma a squared Rho Sigma a,",
    "start": "5656445",
    "end": "5663315"
  },
  {
    "text": "Sigma b Rho Sigma a Sigma b,",
    "start": "5663315",
    "end": "5668550"
  },
  {
    "text": "and Rho Sigma b square. Now, uh, a given b will be normal.",
    "start": "5668550",
    "end": "5678719"
  },
  {
    "text": "This basically, you know, I'm just- I'm just recapping what we have seen in the past.",
    "start": "5678720",
    "end": "5684135"
  },
  {
    "text": "a given b will be Mu b or b minus Mu b divided by Sigma b.",
    "start": "5684135",
    "end": "5691829"
  },
  {
    "text": "You know, get like the-the- obtain the- the z value of- of- of b times Rho times Sigma a.",
    "start": "5691830",
    "end": "5701580"
  },
  {
    "text": "That'll be the mean. And the covariance is- is like the Schur complement, right? So it was, um,",
    "start": "5701580",
    "end": "5709110"
  },
  {
    "text": "Sigma a squared minus, um, in this case, uh, uh.",
    "start": "5709110",
    "end": "5714720"
  },
  {
    "text": "So this minus this times the inverse times this. So it was Rho squared si-Sigma a squared,",
    "start": "5714720",
    "end": "5727575"
  },
  {
    "text": "right, in case of a and b. And this is basically, um, being done in the- in the multivariate case.",
    "start": "5727575",
    "end": "5734760"
  },
  {
    "text": "So it's-it's basically the same thing, but-but with matrices instead of scalars, right?",
    "start": "5734760",
    "end": "5739875"
  },
  {
    "text": "And this will give us the, um, Schur complement that- so that is, um,",
    "start": "5739875",
    "end": "5745260"
  },
  {
    "text": "covariance will be i minus, um, l transpose inverse of this ll transpose plus",
    "start": "5745260",
    "end": "5754830"
  },
  {
    "text": "xi inverse times l, right?",
    "start": "5754830",
    "end": "5760905"
  },
  {
    "text": "So this is the Schur complement.",
    "start": "5760905",
    "end": "5766000"
  },
  {
    "text": "And this is-is like calculate the- the,",
    "start": "5767450",
    "end": "5772740"
  },
  {
    "text": "um, um, um, z value, so x minus Mu divided by the- the- the covariance times, um, um,",
    "start": "5772740",
    "end": "5781225"
  },
  {
    "text": "l transpose, which is- is basically, um, map it back to the-the variance of- of c. Right?",
    "start": "5781225",
    "end": "5788850"
  },
  {
    "text": "So this is z given x. Any questions on this? And now, once we have z given x,",
    "start": "5788850",
    "end": "5796530"
  },
  {
    "text": "we basically are able to calculate set Q_i of z_i",
    "start": "5796530",
    "end": "5802155"
  },
  {
    "text": "to be normal with this mean and this variance, okay?",
    "start": "5802155",
    "end": "5810074"
  },
  {
    "text": "So that gives us our E-step. And the E-step we're gonna- we gonna hold the current estimates of the parameters",
    "start": "5810075",
    "end": "5815969"
  },
  {
    "text": "fixed and calculate the posterior of z given x, right? In the M-step- so in the M-step,",
    "start": "5815970",
    "end": "5829170"
  },
  {
    "text": "you want to do argmax of",
    "start": "5829170",
    "end": "5834765"
  },
  {
    "text": "Mu l psi of the ELBO, right?",
    "start": "5834765",
    "end": "5840840"
  },
  {
    "text": "So sum over i equals 1 to n. And now we will",
    "start": "5840840",
    "end": "5846000"
  },
  {
    "text": "have an integral of over z_i in place of the summation over k different clusters,",
    "start": "5846000",
    "end": "5852930"
  },
  {
    "text": "we instead have an integral over- over the z-space of",
    "start": "5852930",
    "end": "5859410"
  },
  {
    "text": "log p of x_i z_i parameterized by Mu,",
    "start": "5859410",
    "end": "5867735"
  },
  {
    "text": "l, psi divided by Q_i of z_i divided by Q_i of z_i.",
    "start": "5867735",
    "end": "5884020"
  },
  {
    "text": "Mu, l, psi equals this, right? So this is the M-step.",
    "start": "5887180",
    "end": "5896460"
  },
  {
    "text": "Now there are a few observations we can make here. So the first thing is that you're gonna",
    "start": "5896460",
    "end": "5903210"
  },
  {
    "text": "first think- write- write it out as an expectation, right? So the expectation makes it, makes it easier to understand.",
    "start": "5903210",
    "end": "5913980"
  },
  {
    "text": "So this is argmax Mu, l, Psi,",
    "start": "5913980",
    "end": "5922470"
  },
  {
    "text": "i equals 1 to n expectation z_i comes from",
    "start": "5922470",
    "end": "5932670"
  },
  {
    "text": "Q_i of- so we're",
    "start": "5932670",
    "end": "5941400"
  },
  {
    "text": "gonna factor this into x given z times p of z. And in the denominator we have q and there's a log.",
    "start": "5941400",
    "end": "5949574"
  },
  {
    "text": "So this will be log p of x given z plus log of z minus log of Q_i, right?",
    "start": "5949575",
    "end": "5956445"
  },
  {
    "text": "So factor this into two and these three apply to log, so you get log a plus log b minus log c, right.",
    "start": "5956445",
    "end": "5963600"
  },
  {
    "text": "So this will be addition of log p of xi given z_i",
    "start": "5963600",
    "end": "5972420"
  },
  {
    "text": "plus log p of z_i minus log Q_i of z_i.",
    "start": "5972420",
    "end": "5985600"
  },
  {
    "text": "Right? So this is the M-step.",
    "start": "5987050",
    "end": "5990429"
  },
  {
    "text": "Any questions on this, yes question? [BACKGROUND]",
    "start": "5992870",
    "end": "6004489"
  },
  {
    "text": "Yes. So this integral and Q_i became the expectation. Good question.",
    "start": "6004490",
    "end": "6011915"
  },
  {
    "text": "Right? So one thing that we can observe here is that in all- in any kind of an EM,",
    "start": "6011915",
    "end": "6022730"
  },
  {
    "text": "uh, um, whenever you apply EM. This part over here was- is log p of z.",
    "start": "6022730",
    "end": "6028909"
  },
  {
    "text": "So this was the numerator and this was the denominator.",
    "start": "6028910",
    "end": "6036545"
  },
  {
    "text": "Right? So this is the denominator, that's log Q- log Q and this is the numerator, right?",
    "start": "6036545",
    "end": "6042920"
  },
  {
    "text": "And numerator we factor out into two parts, right? In all- whenever you apply EM,",
    "start": "6042920",
    "end": "6049400"
  },
  {
    "text": "no matter what the model is, you always have this denominator Q,",
    "start": "6049400",
    "end": "6054590"
  },
  {
    "text": "that does not have any of the parameters that you want to optimize, right? So the parameters that we want to optimize are always in",
    "start": "6054590",
    "end": "6061190"
  },
  {
    "text": "the numerator, right? And there's a log. So which means in any given EM model,",
    "start": "6061190",
    "end": "6068435"
  },
  {
    "text": "when we're performing argmax, we can just strike out this part.",
    "start": "6068435",
    "end": "6073680"
  },
  {
    "text": "Not just in factor analysis or in Gaussian mixture model. Whenever you apply EM,",
    "start": "6074880",
    "end": "6081670"
  },
  {
    "text": "right, in the ELBO, the denominator, because there's a log- log, um, um,",
    "start": "6081670",
    "end": "6088340"
  },
  {
    "text": "log of this- this ratio, the log of the denominator will never have the parameters that we're optimizing over.",
    "start": "6088340",
    "end": "6094460"
  },
  {
    "text": "So you can always just strike out the- the, um, um, log Q. Over here, specifically in the case of factor analysis,",
    "start": "6094460",
    "end": "6103555"
  },
  {
    "text": "we make yet another observation that log p of z also has no parameters because we",
    "start": "6103555",
    "end": "6109750"
  },
  {
    "text": "assumed z comes from- from a normal distribution of 0, I, right?",
    "start": "6109750",
    "end": "6117650"
  },
  {
    "text": "That also had no parameters, right? So in case of- of, ah, factor analysis,",
    "start": "6117650",
    "end": "6123664"
  },
  {
    "text": "we gain this additional cancellation of this p of z as well, right?",
    "start": "6123665",
    "end": "6131255"
  },
  {
    "text": "So now all we're left with is- so x given z has- so what does x given z?",
    "start": "6131255",
    "end": "6142045"
  },
  {
    "text": "x given z had a normal of, yeah, so this does have the- the- the,",
    "start": "6142045",
    "end": "6148060"
  },
  {
    "text": "this does have Mu, l and psi. It has all three of them, right?",
    "start": "6148060",
    "end": "6153185"
  },
  {
    "text": "It's parametized on Mu, l and z. And so we're- we basically need to perform argmax of- of this term with respect to new LMC. Question?",
    "start": "6153185",
    "end": "6163310"
  },
  {
    "text": "[BACKGROUND]. We have not yet- We have not yet?",
    "start": "6163310",
    "end": "6169369"
  },
  {
    "text": "And I'm not sure if we will have the time to come to that yet, but, uh, let's wrap up as much as we can.",
    "start": "6169370",
    "end": "6176820"
  },
  {
    "text": "So- and now, once- once we have, er, written out in this form,",
    "start": "6187620",
    "end": "6193000"
  },
  {
    "text": "it is basically just calculus, right? Uh, it's- it's gonna be, um, so log p of, er,",
    "start": "6193000",
    "end": "6199135"
  },
  {
    "text": "x given z can be- can be written out as- this can be written",
    "start": "6199135",
    "end": "6205179"
  },
  {
    "text": "out as [NOISE] arg max of Mu L - Mu L Psi,",
    "start": "6205180",
    "end": "6215090"
  },
  {
    "text": "i equals 1-n expectation of z i from Q_i of log 1 over 2 Pi ^ d by 2,",
    "start": "6215880",
    "end": "6230800"
  },
  {
    "text": "Psi one-half exponent minus",
    "start": "6230800",
    "end": "6240699"
  },
  {
    "text": "one-half xi minus Mu minus Lzi,",
    "start": "6240700",
    "end": "6250820"
  },
  {
    "text": "times Psi inverse xi minus",
    "start": "6250920",
    "end": "6259000"
  },
  {
    "text": "Mu L - Mu minus Lzi.",
    "start": "6259000",
    "end": "6273260"
  },
  {
    "text": "All right? And here if - if you - if you, um, if you basically,",
    "start": "6274140",
    "end": "6281875"
  },
  {
    "text": "you know, just take the derivatives, set it equal to 0. Um, in order to do that,",
    "start": "6281875",
    "end": "6287245"
  },
  {
    "text": "uh, however, there is this expectation over here. Right? So we have this expectation over this whole thing.",
    "start": "6287245",
    "end": "6293230"
  },
  {
    "text": "So in order to take the expectation, we will now have to break this into smaller chunks.",
    "start": "6293230",
    "end": "6298900"
  },
  {
    "text": "So this will be log of - log of 1 over 2 Pi ^ d by 2.",
    "start": "6298900",
    "end": "6309145"
  },
  {
    "text": "And this will be minus half log determinant of Psi,",
    "start": "6309145",
    "end": "6315250"
  },
  {
    "text": "log and the exponent will cancel, minus one-half x minus Mu minus",
    "start": "6315250",
    "end": "6323050"
  },
  {
    "text": "Lzi Psi inverse x minus",
    "start": "6323050",
    "end": "6329184"
  },
  {
    "text": "Mu minus Lzi, right?",
    "start": "6329185",
    "end": "6339745"
  },
  {
    "text": "You still have the i equals 1 to n expectation z i from Q_i, right?",
    "start": "6339745",
    "end": "6347095"
  },
  {
    "text": "And now we can - we can, um, er, we can see what terms depend on z and what do not.",
    "start": "6347095",
    "end": "6354635"
  },
  {
    "text": "We are taking an arg max of an expectation of something, right? And so this term does not have z,",
    "start": "6354635",
    "end": "6365320"
  },
  {
    "text": "nor does it have any of the parameters. So this can just cancel out, right? This term does not have z,",
    "start": "6365320",
    "end": "6372190"
  },
  {
    "text": "so it can come out of the expectation. This term does have z and in - in order to perform the expectation,",
    "start": "6372190",
    "end": "6380650"
  },
  {
    "text": "you'll have to expand out, you know, distribute this multiplication across all terms.",
    "start": "6380650",
    "end": "6387070"
  },
  {
    "text": "And once we - once we, uh - uh, once we do all that and you,",
    "start": "6387070",
    "end": "6393160"
  },
  {
    "text": "uh, take the gradients and set it equal to 0, we will get L is equal to sum over i equals 1- n,",
    "start": "6393160",
    "end": "6403400"
  },
  {
    "text": "xi minus Mu times",
    "start": "6404130",
    "end": "6411505"
  },
  {
    "text": "Mu_z given x transpose,",
    "start": "6411505",
    "end": "6417368"
  },
  {
    "text": "where Mu_z given x transpose was from the posterior, right? It's - it's that big expression over here,",
    "start": "6417369",
    "end": "6425060"
  },
  {
    "text": "times summation i equals 1-n Mu_z given",
    "start": "6429330",
    "end": "6441175"
  },
  {
    "text": "x Mu_z given x transpose,",
    "start": "6441175",
    "end": "6447204"
  },
  {
    "text": "again, this - this Mu_z given x is the full posterior that we derived earlier, plus Sigma_z given x, right?",
    "start": "6447205",
    "end": "6458940"
  },
  {
    "text": "So this is the M-step update for L. And similarly,",
    "start": "6458940",
    "end": "6464279"
  },
  {
    "text": "the M-step update for Mu will be 1/n,",
    "start": "6464279",
    "end": "6470190"
  },
  {
    "text": "i equals 1-n xi, right?",
    "start": "6470190",
    "end": "6475780"
  },
  {
    "text": "This should be pretty straightforward because we assumed - we gave z a zero mean and x,",
    "start": "6475780",
    "end": "6481405"
  },
  {
    "text": "you know, x, therefore, had to be, uh - uh- so Mu, therefore had to have [NOISE], er, this - this mean.",
    "start": "6481405",
    "end": "6488140"
  },
  {
    "text": "And then we calculate this Phi matrix,",
    "start": "6488140",
    "end": "6493900"
  },
  {
    "text": "1/n, i equals 1-n, oh boy, this is painful, xi,",
    "start": "6493900",
    "end": "6501845"
  },
  {
    "text": "xi transpose minus xi Mu_z given",
    "start": "6501845",
    "end": "6508800"
  },
  {
    "text": "x transpose L transpose minus L Mu_z given x,",
    "start": "6508800",
    "end": "6518590"
  },
  {
    "text": "xi transpose plus L- I'm getting tired writing this- Mu_z given x,",
    "start": "6519260",
    "end": "6531309"
  },
  {
    "text": "Mu_z given x transpose plus Sigma_z given x times L transpose.",
    "start": "6531310",
    "end": "6544010"
  },
  {
    "text": "And Psi_ii equal to Phi_ii, right?",
    "start": "6544890",
    "end": "6551980"
  },
  {
    "text": "So this is an intermediate computation and you take the diagonal values of this and set them equal to be,",
    "start": "6551980",
    "end": "6557170"
  },
  {
    "text": "uh - uh, Psi_ii, right? So this- this, um,",
    "start": "6557170",
    "end": "6562720"
  },
  {
    "text": "a lot of symbols, a lot of monstrous-looking expressions, Um, I would not expect you to memorize this, right?",
    "start": "6562720",
    "end": "6571135"
  },
  {
    "text": "Um, however, the - the, uh, larger story from this is the way we went about applying EM,",
    "start": "6571135",
    "end": "6579250"
  },
  {
    "text": "the - the recipe of applying EM. We, uh, we obtained the, you know,",
    "start": "6579250",
    "end": "6584575"
  },
  {
    "text": "we derived the - the E-step, um, as - as, uh, the simple posterior,",
    "start": "6584575",
    "end": "6589690"
  },
  {
    "text": "and in the M-step, we had to take this expectation with respect to z, um,",
    "start": "6589690",
    "end": "6595195"
  },
  {
    "text": "of - of the - the, uh, of the - of the ELBO, uh, so the ELBO,",
    "start": "6595195",
    "end": "6600565"
  },
  {
    "text": "which is the expectation, uh, with respect to z. A crucial point, uh,",
    "start": "6600565",
    "end": "6605695"
  },
  {
    "text": "to note here is that in many places if you - if you, you know, search for EM algorithm on - on the - on Google and try to learn about EM algorithm,",
    "start": "6605695",
    "end": "6615085"
  },
  {
    "text": "in many places, you might see an algorithm that's described like this.",
    "start": "6615085",
    "end": "6620290"
  },
  {
    "text": "But in the E-step, you know, calculate expectation of z,",
    "start": "6620290",
    "end": "6627760"
  },
  {
    "text": "and in the M-step, you know,",
    "start": "6627760",
    "end": "6634150"
  },
  {
    "text": "do arg max of p of z,",
    "start": "6634150",
    "end": "6639265"
  },
  {
    "text": "expectation of z, right? You might see many, many, uh - uh,",
    "start": "6639265",
    "end": "6644945"
  },
  {
    "text": "articles or - or documents that describe E-step this way, where E-step corresponds to the expectation,",
    "start": "6644945",
    "end": "6650785"
  },
  {
    "text": "M-step correspondence to maximization. This recipe, this - this - this, uh - uh,",
    "start": "6650785",
    "end": "6658150"
  },
  {
    "text": "sequence of steps is true only sometimes, right? In simple models, you can write it like this.",
    "start": "6658150",
    "end": "6664705"
  },
  {
    "text": "In Gaussian mixture models, you can write it like this, right? But the - the correct way to do expectation maximization is to have in",
    "start": "6664705",
    "end": "6673929"
  },
  {
    "text": "the E-step perform only calculation of the posterior and construction of the ELBO and in the M-step, maximize the ELBO.",
    "start": "6673930",
    "end": "6681835"
  },
  {
    "text": "In simple models, those two steps are equivalent to this. But if you were to do this for more complex models, it will be wrong.",
    "start": "6681835",
    "end": "6691030"
  },
  {
    "text": "If you follow the step for factor analysis, you will get it wrong.",
    "start": "6691030",
    "end": "6696099"
  },
  {
    "text": "It works only for simple steps, but the - the - the process that works for any model is in E-step,",
    "start": "6696100",
    "end": "6703125"
  },
  {
    "text": "construct the posterior Q- the - the Q distribution, and in the M-step, maximize the ELBO.",
    "start": "6703125",
    "end": "6708840"
  },
  {
    "text": "That works all the time, no matter what. For simple models, that is equivalent to this. But, you know, that's something to - to, uh - uh, keep in your mind.",
    "start": "6708840",
    "end": "6717344"
  },
  {
    "text": "All right, with that, we'll, uh, we'll break. If you have any questions, you know, come up, uh, to the stage, and we can - we can, uh.",
    "start": "6717345",
    "end": "6724449"
  }
]