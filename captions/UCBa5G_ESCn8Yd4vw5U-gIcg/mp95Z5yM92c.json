[
  {
    "start": "0",
    "end": "84000"
  },
  {
    "text": "For today, I'm really delighted, um, to introduce our third guest speaker, who's Marc'Aurelio Ranzato.",
    "start": "4070",
    "end": "12635"
  },
  {
    "text": "So he's originally from Italy and then worked at NYU with Yann LeCun,",
    "start": "12635",
    "end": "18610"
  },
  {
    "text": "and then has a post-doc with Geoffrey Hinton. So he's a very dyed in the wool deep learning researcher.",
    "start": "18610",
    "end": "24835"
  },
  {
    "text": "A lot of his original work was in the areas like feature learning and vision,",
    "start": "24835",
    "end": "30808"
  },
  {
    "text": "but over the last few years he's really turned his interests to natural language processing.",
    "start": "30809",
    "end": "36985"
  },
  {
    "text": "And in particular, um, in the last few years, he's worked a huge amount in looking at machine translation in general and in",
    "start": "36985",
    "end": "45140"
  },
  {
    "text": "particular machine translation for languages for which less resources are available.",
    "start": "45140",
    "end": "50870"
  },
  {
    "text": "So I saw a talk of his about six months ago on this topic. And, um, through him and his team at Facebook,",
    "start": "50870",
    "end": "58670"
  },
  {
    "text": "they've really got a lot of exciting new work in ways to bring neural machine translation up to the next level.",
    "start": "58670",
    "end": "65390"
  },
  {
    "text": "And so I hope that this would be a really great opportunity for everyone to see some of the latest and most exciting techniques in neural machine translation.",
    "start": "65390",
    "end": "74345"
  },
  {
    "text": "That's sort of - of a next level beyond what we talked about, and you guys all did on assignments 4 and 5 of the class.",
    "start": "74345",
    "end": "81935"
  },
  {
    "text": "Um, so take it away, Marc'Aurelio. Okay. Thank you so much, Chris, for inviting me. Um, let me just put my face.",
    "start": "81935",
    "end": "89165"
  },
  {
    "start": "84000",
    "end": "253000"
  },
  {
    "text": "I'm here. [LAUGHTER] Hi everybody. I'm gonna disable it now so you can focus on the presentation.",
    "start": "89165",
    "end": "95155"
  },
  {
    "text": "Um, so share. I hope you should be able to see my presentation now.",
    "start": "95155",
    "end": "103670"
  },
  {
    "text": "Okay. So I'm very excited to tell you a little bit about low resource machine translation.",
    "start": "103670",
    "end": "110510"
  },
  {
    "text": "And let's start by, uh, revisiting, uh, the machine translation problems.",
    "start": "110510",
    "end": "116300"
  },
  {
    "text": "So let's say that we want to translate between English and French. And, uh, we started with a big, uh,",
    "start": "116300",
    "end": "124940"
  },
  {
    "text": "training set where we have a collection of sentences, uh, in English with their corresponding translation in French.",
    "start": "124940",
    "end": "132250"
  },
  {
    "text": "And this is what we call a parallel data set. And in particular the sentences in English,",
    "start": "132250",
    "end": "138205"
  },
  {
    "text": "we call them source sentences, right? And the corresponding sentences in French are,",
    "start": "138205",
    "end": "143470"
  },
  {
    "text": "uh, what we call the target sentences. And now, uh, the learning problem is about for a given,",
    "start": "143470",
    "end": "152590"
  },
  {
    "text": "uh, sentence in English, you want to predict the corresponding, uh, sentence in French.",
    "start": "152590",
    "end": "158140"
  },
  {
    "text": "And, uh, the way that we do that is by minimizing the cross-entropy loss,",
    "start": "158140",
    "end": "163525"
  },
  {
    "text": "which maximizes the low probability of the reference human translation given the input source sentence.",
    "start": "163525",
    "end": "171635"
  },
  {
    "text": "And we do this by stochastic gradient descent, using, uh,",
    "start": "171635",
    "end": "177049"
  },
  {
    "text": "as architecture a sequence to sequence with attention that as far as I know, you started and- and- and you had a homework on a few weeks ago.",
    "start": "177050",
    "end": "187174"
  },
  {
    "text": "And then after you train this, at this time, you are given a normal English sentence",
    "start": "187175",
    "end": "194285"
  },
  {
    "text": "and you want to produce the corresponding translation. And in order to do that, um,",
    "start": "194285",
    "end": "199295"
  },
  {
    "text": "we usually, uh, employ a heuristic, uh, search method like beam that tries to find, uh,",
    "start": "199295",
    "end": "207530"
  },
  {
    "text": "approximately the target sentence that maximizes the low probability given the- uh,",
    "start": "207530",
    "end": "212940"
  },
  {
    "text": "the given, uh, source sentence. So this is at the high level how machine translation works.",
    "start": "212940",
    "end": "221420"
  },
  {
    "text": "And let's think about the assumptions that, uh, we have been making, uh, through this discussion.",
    "start": "221420",
    "end": "226910"
  },
  {
    "text": "So the first assumption is that we are working with two, uh, fairly related languages like English and French.",
    "start": "226910",
    "end": "233300"
  },
  {
    "text": "And the second, uh, assumptions is that we have at our disposal, a large data set of parallel sentences.",
    "start": "233300",
    "end": "239959"
  },
  {
    "text": "Because here we are essentially doing, uh, supervised learning, right? And it is a beautiful example of end-to-end supervised learning,",
    "start": "239960",
    "end": "248195"
  },
  {
    "text": "that relies on the availability of a large parallel dataset. And so in the world there are more than 6,000 languages.",
    "start": "248195",
    "end": "259100"
  },
  {
    "start": "253000",
    "end": "370000"
  },
  {
    "text": "Um, and needless to say,",
    "start": "259100",
    "end": "264530"
  },
  {
    "text": "most of these languages don't belong to the European family for which much of the recent research on machine translation has been focusing on.",
    "start": "264530",
    "end": "273875"
  },
  {
    "text": "And even if you look at English, English is spoken by less than 5% of- as",
    "start": "273875",
    "end": "280129"
  },
  {
    "text": "native languages speak- is spoken by less than 5% of the world population. And so, uh, if you were",
    "start": "280130",
    "end": "288860"
  },
  {
    "text": "to count how many people speak a certain language and you look at that histogram, it's a very heavy tailed distribution.",
    "start": "288860",
    "end": "296015"
  },
  {
    "text": "So even if you take the top 10 spoken languages,",
    "start": "296015",
    "end": "301685"
  },
  {
    "text": "you find that this accounts for less than 50%of the people in the world. And, um, now if you look at the very far right of the tail,",
    "start": "301685",
    "end": "312860"
  },
  {
    "text": "those are languages for which there are very few speakers and essentially there is no digitized data material for you to train anything.",
    "start": "312860",
    "end": "320755"
  },
  {
    "text": "So for- for those I think it's almost hopeless, I would say. But in the middle of this tail,",
    "start": "320755",
    "end": "327845"
  },
  {
    "text": "we have a lot of languages for which there is some, uh, digital data and for which, uh,",
    "start": "327845",
    "end": "334685"
  },
  {
    "text": "we don't have good ways to translate nowadays, if you think about major providers like Google, Yandex,",
    "start": "334685",
    "end": "342500"
  },
  {
    "text": "Baidu, Facebook, and so on, and so forth, they provide translation for the top 100 languages.",
    "start": "342500",
    "end": "348110"
  },
  {
    "text": "So we are still very much at the far right of this normal distribution.",
    "start": "348110",
    "end": "353985"
  },
  {
    "text": "And so, uh, if- if we're able to, uh, um,. Improve machine translation in the middle,",
    "start": "353985",
    "end": "362015"
  },
  {
    "text": "I think we could do- uh, it would be very impactful, right? But so what happens as we walk down these tails?",
    "start": "362015",
    "end": "369949"
  },
  {
    "text": "So what happens is that the amount of data or parallel data decreases and,",
    "start": "369950",
    "end": "377390"
  },
  {
    "text": "uh, that correlates very much with the quality of the automatic machine translation systems that we have.",
    "start": "377390",
    "end": "383690"
  },
  {
    "text": "And particularly as you can see here, at some point there is actually a drastic drop",
    "start": "383690",
    "end": "389660"
  },
  {
    "text": "in accuracy of your machine translation system. So, um, so perhaps the initial,",
    "start": "389660",
    "end": "400365"
  },
  {
    "start": "398000",
    "end": "426000"
  },
  {
    "text": "um, picture that we had in mind is a little different. So now if we take a fairly low resource language like Nepali,",
    "start": "400365",
    "end": "409220"
  },
  {
    "text": "which is the language spoken in Nepal, a lovely country northeast of India,",
    "start": "409220",
    "end": "414630"
  },
  {
    "text": "um, with more than 25 million people. So it's not as handful of people.",
    "start": "414630",
    "end": "422210"
  },
  {
    "text": "Uh, first of all, the amount of training data is not as much as English. French is much, much less than that.",
    "start": "422210",
    "end": "428495"
  },
  {
    "start": "426000",
    "end": "454000"
  },
  {
    "text": "And here, uh, let's use a different visual representation. So let's use, um, uh,",
    "start": "428495",
    "end": "435080"
  },
  {
    "text": "few rectangles with a color that corresponds to the language. So the blue rectangle is English data and the red rectangle is Nepali data.",
    "start": "435080",
    "end": "445150"
  },
  {
    "text": "Now, in practice, the parallel dataset is not",
    "start": "445150",
    "end": "452180"
  },
  {
    "text": "just such a monolithic thing because some part originates in English and some parts originates in Nepali.",
    "start": "452180",
    "end": "461720"
  },
  {
    "text": "And now let's represent the Nepali translations of English data",
    "start": "461720",
    "end": "470240"
  },
  {
    "text": "with an empty rectangle where the color corresponds to the language and whether you fill",
    "start": "470240",
    "end": "476690"
  },
  {
    "text": "it or not depends whether this is translation is, so whether this is a human translation or whether it is,",
    "start": "476690",
    "end": "482915"
  },
  {
    "text": "uh, um, data originating in- in the language. So in this case, we take, uh,",
    "start": "482915",
    "end": "488870"
  },
  {
    "text": "data that originates in English and we translate it into Nepali. And so this is the empty red rectangle",
    "start": "488870",
    "end": "497870"
  },
  {
    "text": "and the same for when you go from Nepali to English. Now, in general, the data that originates in English and",
    "start": "497870",
    "end": "506000"
  },
  {
    "text": "the data that originates in Nepali come from different- may come from different domains. So here on the y-axis you have the domain.",
    "start": "506000",
    "end": "513815"
  },
  {
    "text": "And so in this example that I totally made up, but it's pretty, um,",
    "start": "513815",
    "end": "519210"
  },
  {
    "text": "uh, indicative of what happens in practice. You may have that English sentences may come from, let say Bible.",
    "start": "519210",
    "end": "526445"
  },
  {
    "text": "And so the Nepali here are translations from, uh, the Bible. And, uh, the Nepali sentences may come from parliamentary data, okay?",
    "start": "526445",
    "end": "537210"
  },
  {
    "text": "So you may agree with me that translating a normal sentence from",
    "start": "537350",
    "end": "543769"
  },
  {
    "text": "the Bible is not a super interesting task because the Bible is a pretty static dataset, right?",
    "start": "543770",
    "end": "550190"
  },
  {
    "text": "And so maybe we want to translate news data. And, uh, but so, uh,",
    "start": "550190",
    "end": "557595"
  },
  {
    "text": "in practice we don't have any, uh, parallel data in the news domain,",
    "start": "557595",
    "end": "562940"
  },
  {
    "text": "perhaps what we have, so what we really want to do at the end is translate sentences from this test set,",
    "start": "562940",
    "end": "570590"
  },
  {
    "text": "that is English news into Nepali. But all we have in the news domain is at",
    "start": "570590",
    "end": "576700"
  },
  {
    "text": "most monolingual data both in English and in Nepali. So these are English sentences that are not aligned at all with the Nepali,",
    "start": "576700",
    "end": "586170"
  },
  {
    "text": "uh, sentences over here. Here it just happened to be just data that you got from new sources.",
    "start": "586170",
    "end": "592899"
  },
  {
    "text": "Okay? And so this is a pretty complicated learning setting because you have a little bit of parallel sentences and,",
    "start": "592900",
    "end": "602770"
  },
  {
    "text": "uh, that are in a different domain from the test set, and, uh, all you have in the domain of interest is monolingual data.",
    "start": "602770",
    "end": "612905"
  },
  {
    "text": "And in fact, you may have also some other parallel data but in another language,",
    "start": "612905",
    "end": "621830"
  },
  {
    "text": "let's say Hindi that is in the same family as Nepali but maybe these parallel data is in a different domain, let say books.",
    "start": "621830",
    "end": "630380"
  },
  {
    "text": "And perhaps you have also monolingual data in Hindi that is also in the book domain.",
    "start": "630380",
    "end": "635525"
  },
  {
    "text": "So in fact, what you really- [LAUGHTER] in- in practice,",
    "start": "635525",
    "end": "642140"
  },
  {
    "text": "what you'll find is that you may have a lot of languages here from which you could learn and- and a lot of domains.",
    "start": "642140",
    "end": "651185"
  },
  {
    "text": "And all you want to do at the end is to be able to translate news data in English into Nepali.",
    "start": "651185",
    "end": "658580"
  },
  {
    "text": "But you don't have any supervision for that. You don't have any labeled data, any parallel data for that.",
    "start": "658580",
    "end": "663709"
  },
  {
    "text": "All you have is a bunch of data in different domains and in different languages. And so the question is,",
    "start": "663710",
    "end": "670279"
  },
  {
    "text": "how can you leverage all these data in order to perform your original translation task?",
    "start": "670280",
    "end": "677345"
  },
  {
    "text": "And so this is a- a Mondrian like learning setting, which is pretty tricky.",
    "start": "677345",
    "end": "684334"
  },
  {
    "text": "And this is going to be the topic of this lecture. And so, um, there is not a very, um,",
    "start": "684335",
    "end": "694770"
  },
  {
    "start": "688000",
    "end": "782000"
  },
  {
    "text": "clear definition of what Low Resource Machine Translation is but, loosely speaking, uh,",
    "start": "694770",
    "end": "700800"
  },
  {
    "text": "a language pair can be consider low-resource when the number of parallel sentences in domain is less than 10,000. Okay?",
    "start": "700800",
    "end": "709200"
  },
  {
    "text": "And as, uh, order of magnitude. And, uh, and this is very little,",
    "start": "709200",
    "end": "717430"
  },
  {
    "text": "particularly if you think that modern neural machine translation systems have easily hundreds of millions of parameters.",
    "start": "717430",
    "end": "726055"
  },
  {
    "text": "And so there are several challenges. There are challenges that pertain to data and challenges that pertain to the model design.",
    "start": "726055",
    "end": "732910"
  },
  {
    "text": "So in terms of the data, it is very hard to get data to train, right?",
    "start": "732910",
    "end": "739839"
  },
  {
    "text": "It is very hard to figure out where to get the data to train, data that is in- in a domain similar to",
    "start": "739840",
    "end": "746740"
  },
  {
    "text": "the domain that you are interested in eventually translating. Uh, if that doesn't exist,",
    "start": "746740",
    "end": "752635"
  },
  {
    "text": "how to get data in similar languages on other domains and even how to get data to evaluate your system on.",
    "start": "752635",
    "end": "761485"
  },
  {
    "text": "And on the modeling side, uh, there is the question of, of course,",
    "start": "761485",
    "end": "766510"
  },
  {
    "text": "how to learn with, uh, so little supervision, so little direct supervision at the very least,",
    "start": "766510",
    "end": "772420"
  },
  {
    "text": "and how to operate in this, um, framework for which you have so many languages and so many domains.",
    "start": "772420",
    "end": "780430"
  },
  {
    "text": "So, uh, so as,",
    "start": "780430",
    "end": "787390"
  },
  {
    "start": "782000",
    "end": "858000"
  },
  {
    "text": "uh, Chris mentioned at the very beginning, my background is not really NMT. My, um, I've always been interested in learning with less supervision.",
    "start": "787390",
    "end": "798009"
  },
  {
    "text": "And I think working low resource machine translation is, at least personally, a very unique opportunity.",
    "start": "798010",
    "end": "806155"
  },
  {
    "text": "It's a very rare case in which my research agenda is aligned with an application [LAUGHTER] because, um,",
    "start": "806155",
    "end": "813910"
  },
  {
    "text": "in low resource machine translation you don't have much level data and you need to make",
    "start": "813910",
    "end": "819069"
  },
  {
    "text": "the best use of auxiliary tasks and au- auxiliary data in order to, uh, perform well.",
    "start": "819070",
    "end": "825295"
  },
  {
    "text": "And this is a general problem. And at the same time, machine translation is a real application,",
    "start": "825295",
    "end": "831925"
  },
  {
    "text": "it's something that if we improve we can really, uh, have a chance to improve, uh,",
    "start": "831925",
    "end": "838660"
  },
  {
    "text": "a lot of applications and- and- and the life of a lot of people. So, uh, this concludes my introduction about low resource machine translation and,",
    "start": "838660",
    "end": "851289"
  },
  {
    "text": "um, the issues that we face when working on these languages. Before, uh, and let me just,",
    "start": "851289",
    "end": "860725"
  },
  {
    "start": "858000",
    "end": "1118000"
  },
  {
    "text": "uh, pause for a second, seeing the- the outline of this talk goes around three pillars that in a way define,",
    "start": "860725",
    "end": "869345"
  },
  {
    "text": "uh, the cycle of research. So the first pillar is data. So, um, I'm going to review,",
    "start": "869345",
    "end": "878680"
  },
  {
    "text": "uh, how we can get data in particular for evaluation. So, uh, data is the prerequisite to do",
    "start": "878680",
    "end": "884725"
  },
  {
    "text": "anything in our life as machine learner practitioners, right? And then, uh, afterwards I'm gonna move to the modeling, so, uh,",
    "start": "884725",
    "end": "893845"
  },
  {
    "text": "describing some, uh, algorithms to, uh, learn, um, on low resource languages.",
    "start": "893845",
    "end": "901374"
  },
  {
    "text": "And finally, I will conclude with some work on analyzing, uh, what a model does when we train on low resource languages.",
    "start": "901374",
    "end": "909264"
  },
  {
    "text": "And in practice, like, uh, throughout my, um, work here, I keep going around the circle because as I figured out,",
    "start": "909264",
    "end": "919225"
  },
  {
    "text": "uh, the issues that we have with the data, with the model, then I make a mark with a dataset",
    "start": "919225",
    "end": "924430"
  },
  {
    "text": "that better fits the kinds of problems that I'm interested in, and then I may, uh, go back to the modeling side to improve the models and- and so on and so forth, okay?",
    "start": "924430",
    "end": "934000"
  },
  {
    "text": "And here I'm giving some references of the works that I'm presenting, not all of them.",
    "start": "934000",
    "end": "939580"
  },
  {
    "text": "And just so- uh, and just to be clear, these are- this is not meant to be a chronological survey,",
    "start": "939580",
    "end": "946569"
  },
  {
    "text": "so these are not necessarily the works that introduce a certain- a- a certain idea,",
    "start": "946570",
    "end": "953320"
  },
  {
    "text": "but it's just, I would say, the most accessible entry points on the topic,",
    "start": "953320",
    "end": "958495"
  },
  {
    "text": "and then you can go on the related work sections to figure out, uh, if there was, eh,",
    "start": "958495",
    "end": "965080"
  },
  {
    "text": "some seminal paper that led to- to- to that line of research. And of course, there is quite a bit of presenter bias because most of these, uh,",
    "start": "965080",
    "end": "973209"
  },
  {
    "text": "works are- are being co-authored by me, so, uh, be mindful of that.",
    "start": "973210",
    "end": "978560"
  },
  {
    "text": "That said, do you have any questions so far? Uh, I have a quick question about, uh,",
    "start": "978560",
    "end": "983910"
  },
  {
    "text": "I- I see people posting the model about Phrase-based and Neural Unsupervised MT. I was wondering if you could talk about",
    "start": "983910",
    "end": "989790"
  },
  {
    "text": "the different approaches about unsupervised learning, and also, uh, a- algorithms like GLoVe and Word2vec are possible in low resource languages.",
    "start": "989790",
    "end": "999910"
  },
  {
    "text": "Yeah, yeah, so, um, Yuri, I- I- in this lecture I'm gonna focus on neural more.",
    "start": "999910",
    "end": "1006795"
  },
  {
    "text": "Actually I'm not even going into details of the architecture, I'm more talking about algorithms actually.",
    "start": "1006795",
    "end": "1013750"
  },
  {
    "text": "And so these algorithms are applicable both to neural machine translation system as",
    "start": "1013750",
    "end": "1019430"
  },
  {
    "text": "well as through statistical machine translation systems. Uh, when I go over this part I can,",
    "start": "1019430",
    "end": "1027770"
  },
  {
    "text": "um, uh, address a little bit your question and tell you a little bit about the differences between the- these two.",
    "start": "1027770",
    "end": "1034280"
  },
  {
    "text": "Um, uh, and then in terms of, um, uh,",
    "start": "1034280",
    "end": "1040125"
  },
  {
    "text": "methods to learn word embeddings and- and- and sentence embeddings,",
    "start": "1040125",
    "end": "1045464"
  },
  {
    "text": "I'm gonna touch very briefly on that. So at the end of the, er, lecture, I'm gonna, uh,",
    "start": "1045465",
    "end": "1051539"
  },
  {
    "text": "refer to some recent work on filtering where people use, um, sentence embedding methods.",
    "start": "1051540",
    "end": "1059055"
  },
  {
    "text": "It's not GloVe but it's, uh, something, uh, similar in the way.",
    "start": "1059055",
    "end": "1064275"
  },
  {
    "text": "Um, in practice, uh, for word embeddings, um, it's kind of,",
    "start": "1064275",
    "end": "1072240"
  },
  {
    "text": "I would say, um, a prerequisite for machine translation because if you can",
    "start": "1072240",
    "end": "1078060"
  },
  {
    "text": "align word embeddings, you learn a dictionary, and that's, uh, a primitive way to do machine translation.",
    "start": "1078060",
    "end": "1086159"
  },
  {
    "text": "So oftentimes we look at those things as a, um, good sanity check or as a simplified machine translation task.",
    "start": "1086160",
    "end": "1095955"
  },
  {
    "text": "Whenever you have a reference dictionary for which you can then, um, check the accuracy of your alignment.",
    "start": "1095955",
    "end": "1103270"
  },
  {
    "text": "But- so if you- let me get back to you when,",
    "start": "1105680",
    "end": "1112050"
  },
  {
    "text": "uh, we talk about, uh, uh, this paper, okay? So let's talk about data then.",
    "start": "1112050",
    "end": "1117300"
  },
  {
    "text": "So, uh, let's go back to, uh, our English, Nepali, um, um, translation task.",
    "start": "1117300",
    "end": "1127125"
  },
  {
    "start": "1118000",
    "end": "1186000"
  },
  {
    "text": "So there is, um, er, um, a resource called OPUS,",
    "start": "1127125",
    "end": "1133725"
  },
  {
    "text": "which is a very nice, uh, which hosts a very nice collection of datasets, all publically available in- in lots of languages.",
    "start": "1133725",
    "end": "1142650"
  },
  {
    "text": "And, uh, when you go to this website, the OPUS website, you find that for English,",
    "start": "1142650",
    "end": "1147870"
  },
  {
    "text": "Nepali, actually there are 1 million parallel sentences.",
    "start": "1147870",
    "end": "1153450"
  },
  {
    "text": "So maybe I lied to you, telling you that this is a low resource language. But if actually- if you look at what this corpus are,",
    "start": "1153450",
    "end": "1163755"
  },
  {
    "text": "you realize that pretty much half a million of these sentences come from,",
    "start": "1163755",
    "end": "1169200"
  },
  {
    "text": "uh, JW 300, uh, and, uh, which is a religious magazine, and then you have 60,000 sentences from the Bible,",
    "start": "1169200",
    "end": "1177585"
  },
  {
    "text": "and the rest come from GNOME, KDE, Ubuntu, so these are computer related, um, materials, right?",
    "start": "1177585",
    "end": "1185029"
  },
  {
    "text": "And so again, unless you're interesting in translating novel sentences from the Bible,",
    "start": "1185030",
    "end": "1190860"
  },
  {
    "text": "uh, this is not, um, super useful, I would say.",
    "start": "1190860",
    "end": "1197175"
  },
  {
    "text": "Um, and, uh, so one thing to notice that all this data originates from English,",
    "start": "1197175",
    "end": "1203520"
  },
  {
    "text": "we have nothing that originates from Nepali, first of all. And second of all, if you are interested in,",
    "start": "1203520",
    "end": "1208920"
  },
  {
    "text": "let's say translating Wikipedia, all you have is Wikipedia monolingual data both in English and Nepali,",
    "start": "1208920",
    "end": "1216660"
  },
  {
    "text": "and Nepali is not even very much. And then of course you can add some monolingual data in another domain like Common Crawl,",
    "start": "1216660",
    "end": "1224100"
  },
  {
    "text": "which is just a dump of the Internet. Uh, but again, uh,",
    "start": "1224100",
    "end": "1229860"
  },
  {
    "text": "translating between English and Nepali using publicly available data is going to be",
    "start": "1229860",
    "end": "1234900"
  },
  {
    "text": "a challenge because you don't have any in domain parallel dataset, okay?",
    "start": "1234900",
    "end": "1240795"
  },
  {
    "text": "All you have is at most some in domain monolingual data. But there is an even bigger problem,",
    "start": "1240795",
    "end": "1247500"
  },
  {
    "start": "1245000",
    "end": "1277000"
  },
  {
    "text": "which is that there is no test data, right? So here we don't have reference translations in",
    "start": "1247500",
    "end": "1256140"
  },
  {
    "text": "Nepali to measure the quality of our machine translation system. And this is a big problem because if you don't have",
    "start": "1256140",
    "end": "1263465"
  },
  {
    "text": "a high-quality or you don't have at all, er, the set, it's very hard to compare models and it's very hard",
    "start": "1263465",
    "end": "1271529"
  },
  {
    "text": "to do model selection to compare algorithms and- and our, uh, field is crippled.",
    "start": "1271530",
    "end": "1277380"
  },
  {
    "start": "1277000",
    "end": "1499000"
  },
  {
    "text": "We need, uh, strong evaluation, uh, benchmarks. And so this motivator,",
    "start": "1277380",
    "end": "1284070"
  },
  {
    "text": "a project that's called FloRes, that stands for Facebook low resource- low resource, um,",
    "start": "1284070",
    "end": "1290340"
  },
  {
    "text": "evaluation benchmark for machine translation, uh, where we took, uh,",
    "start": "1290340",
    "end": "1295794"
  },
  {
    "text": "Wikipedia sentences in English and translated them into Nepali and Sinhala,",
    "start": "1295795",
    "end": "1301650"
  },
  {
    "text": "and then we took, uh, Wikipedia. We - we took sentences from Nepali Wikipedia and translating them",
    "start": "1301650",
    "end": "1309300"
  },
  {
    "text": "into English as well as from Sinhala Wikipedia and translated them into English. Okay. So you may say this is a little bit boring because what's hard about it?",
    "start": "1309300",
    "end": "1320580"
  },
  {
    "text": "And [LAUGHTER] and tell me about, um, you know, tricks to do better modeling.",
    "start": "1320580",
    "end": "1326160"
  },
  {
    "text": "But actually, you'd be surprised that this data collection process was harder and more interesting also than we thought.",
    "start": "1326160",
    "end": "1334875"
  },
  {
    "text": "So it is hard because there are very few, ah,",
    "start": "1334875",
    "end": "1340790"
  },
  {
    "text": "fluent professional translators in these languages and this is not even super low resource, right?",
    "start": "1340790",
    "end": "1347185"
  },
  {
    "text": "And so, since, ah, so, ah, we dealt with a translator agency and typically there are not enough people,",
    "start": "1347185",
    "end": "1356700"
  },
  {
    "text": "ah, for which you can do kind of AB testing to test the translation of one person with another one.",
    "start": "1356700",
    "end": "1363345"
  },
  {
    "text": "That's number one. Number two in general, it's very hard to assess automatically the quality of the translation",
    "start": "1363345",
    "end": "1371010"
  },
  {
    "text": "because we don't have enough parallel data to train machine translation system, right?",
    "start": "1371010",
    "end": "1376125"
  },
  {
    "text": "And so we need to rely on other methods than a well-trained machine translation system to assess the quality.",
    "start": "1376125",
    "end": "1383490"
  },
  {
    "text": "And so, ah, we build a pipeline where we would have,",
    "start": "1383490",
    "end": "1389940"
  },
  {
    "text": "ah, we would send the sentences to the translators. Once the translations are back, we would, um,",
    "start": "1389940",
    "end": "1397695"
  },
  {
    "text": "do several checks like fluency checks, ah, using, ah, a language model.",
    "start": "1397695",
    "end": "1403725"
  },
  {
    "text": "We would check for transliteration to make sure that a sentence is not translated by simply transliterating.",
    "start": "1403725",
    "end": "1409664"
  },
  {
    "text": "We would check that, ah, the language is the desired one, right?",
    "start": "1409665",
    "end": "1415365"
  },
  {
    "text": "And so we will have a lot of checks like that. And then, if and of course here there are thresholds that you need to set somehow.",
    "start": "1415365",
    "end": "1422565"
  },
  {
    "text": "And then, ah, for those sentences that would fail, ah, this,",
    "start": "1422565",
    "end": "1428789"
  },
  {
    "text": "ah, this step we would send them back to, ah, re-translation.",
    "start": "1428790",
    "end": "1435135"
  },
  {
    "text": "And so after a few iterations of this, then eventually we do also a human evaluation. And then, ah, the sentences in this evaluation benchmark are those that are,",
    "start": "1435135",
    "end": "1446195"
  },
  {
    "text": "that have passed, that I've, um, passed all the automatic and human assessment checks.",
    "start": "1446195",
    "end": "1454585"
  },
  {
    "text": "Now, um, it turns out that there is not even very good literature that tells you how to collect data.",
    "start": "1454585",
    "end": "1463500"
  },
  {
    "text": "And in particular, for low-resource languages, there are a lot of issues related to the quality of the translations.",
    "start": "1463500",
    "end": "1470640"
  },
  {
    "text": "And so this was a process that we thought would take us a couple of months, but instead it took us more than six months.",
    "start": "1470640",
    "end": "1478890"
  },
  {
    "text": "Um, and, but that said, eventually we got a validation set, a test set,",
    "start": "1478890",
    "end": "1484935"
  },
  {
    "text": "and also a hidden test set because we used this data for a WMT competition and, um, for that,",
    "start": "1484935",
    "end": "1492000"
  },
  {
    "text": "they needed to have a test set that was not available to people to make sure that people were not,",
    "start": "1492000",
    "end": "1497265"
  },
  {
    "text": "ah, cross-validating on the test set. And, um, here are some examples of sentences: So",
    "start": "1497265",
    "end": "1504210"
  },
  {
    "start": "1499000",
    "end": "1551000"
  },
  {
    "text": "this is from a sentence from the Sinhala Wikipedia, translating into English a couple of sentences here,",
    "start": "1504210",
    "end": "1511335"
  },
  {
    "text": "and this from English Wikipedia translating into Sinhala. I don't know how many people in the audience come from Sri Lanka,",
    "start": "1511335",
    "end": "1517905"
  },
  {
    "text": "ah, that could appreciate [LAUGHTER] this set. But, um, one interesting thing that you can already see is that,",
    "start": "1517905",
    "end": "1526500"
  },
  {
    "text": "if you, although this is totally anecdotal, because it's just a couple of sentences from, for Sinhala and English,",
    "start": "1526500",
    "end": "1532470"
  },
  {
    "text": "you can see that - the topic, kind of the topic distribution is different. And - and here you have things that,",
    "start": "1532470",
    "end": "1539565"
  },
  {
    "text": "ah, would be a little unlikely in English Wikipedia.",
    "start": "1539565",
    "end": "1545294"
  },
  {
    "text": "And the same is for, ah, Nepali English and - and English Nepali.",
    "start": "1545295",
    "end": "1550845"
  },
  {
    "text": "So, ah, we have a GitHub repository where we host the data and also baseline models that we train on,",
    "start": "1550845",
    "end": "1559980"
  },
  {
    "start": "1551000",
    "end": "1578000"
  },
  {
    "text": "ah, publicly available data and then tested on this FloRes benchmark. And, uh, last week we released",
    "start": "1559980",
    "end": "1566730"
  },
  {
    "text": "another couple of languages: English Pashto and English Timor, and we are adding more and more languages,",
    "start": "1566730",
    "end": "1573320"
  },
  {
    "text": "uh, in the coming months. So, um, the point of this, ah,",
    "start": "1573320",
    "end": "1579785"
  },
  {
    "start": "1578000",
    "end": "1645000"
  },
  {
    "text": "section is just to say that data is oftentime more important than designing a model because without data in particular,",
    "start": "1579785",
    "end": "1589050"
  },
  {
    "text": "without a good evaluation benchmark it's essentially impossible to do research in - in this area.",
    "start": "1589050",
    "end": "1596880"
  },
  {
    "text": "And collecting data is not trivial. It's not trivial. The process the - that you use is - is not, ah,",
    "start": "1596880",
    "end": "1604890"
  },
  {
    "text": "well-established and, ah, and in practice, it - it is hard to do.",
    "start": "1604890",
    "end": "1611775"
  },
  {
    "text": "And another, ah, thing to consider, sorry, is to, ah, look at the data,",
    "start": "1611775",
    "end": "1617895"
  },
  {
    "text": "look at the data when you collect it, and also before you start training your model because you may realize",
    "start": "1617895",
    "end": "1624059"
  },
  {
    "text": "some issues with the quality of the translations if you speak the language, oftentimes, English is on one side.",
    "start": "1624060",
    "end": "1631970"
  },
  {
    "text": "And, or may, you may discover biases or you may discover, ah, interesting things.",
    "start": "1631970",
    "end": "1637220"
  },
  {
    "text": "So always look at the data, ah, as opposed to just apply your matter in a black box way.",
    "start": "1637220",
    "end": "1643860"
  },
  {
    "text": "Um, that concludes my, ah, little, um, discussion of the data part.",
    "start": "1643860",
    "end": "1650985"
  },
  {
    "start": "1645000",
    "end": "1658000"
  },
  {
    "text": "Are there any questions on this? Why don't people talk about building a language model for low resource languages?",
    "start": "1650985",
    "end": "1656370"
  },
  {
    "text": "Yeah, yeah, yeah. So in this case what we did, ah, actually we took, ah,",
    "start": "1656370",
    "end": "1663180"
  },
  {
    "start": "1658000",
    "end": "1780000"
  },
  {
    "text": "the Common Crawl data. And I think I actually don't remember exactly.",
    "start": "1663180",
    "end": "1669735"
  },
  {
    "text": "So for Nepali I think we had to, ah, concatenate the Wikipedia data and the Common Crawl data because the Wikipedia data was just too small.",
    "start": "1669735",
    "end": "1677670"
  },
  {
    "text": "And we simply train a count-based n-gram. And then, the count-based n-gram gives you,",
    "start": "1677670",
    "end": "1684645"
  },
  {
    "text": "I don't know if you study this, but it gives you the probability of one word given some fixed window of context.",
    "start": "1684645",
    "end": "1691709"
  },
  {
    "text": "And then, ah, for a given sentence, we put like, let's say, what is it?",
    "start": "1691709",
    "end": "1697635"
  },
  {
    "text": "For a given sentence, you would, um, compute a score for every board,",
    "start": "1697635",
    "end": "1703485"
  },
  {
    "text": "and then the score of a sentence is simply the average, ah, low-probability score, ah,",
    "start": "1703485",
    "end": "1710820"
  },
  {
    "text": "across all the words in the sentence and that will give you a score and then we will simply have a threshold on that.",
    "start": "1710820",
    "end": "1718184"
  },
  {
    "text": "And so all the sentences that would score too low, that would be deemed not fluent enough,",
    "start": "1718185",
    "end": "1724845"
  },
  {
    "text": "would be sent for rework. But, of course, um,",
    "start": "1724845",
    "end": "1730665"
  },
  {
    "text": "whenever you have an entity, whenever you have, ah, you know,",
    "start": "1730665",
    "end": "1736455"
  },
  {
    "text": "it's not super reliable and if you go on languages that are even lower resource than Sinhala than you have,",
    "start": "1736455",
    "end": "1744465"
  },
  {
    "text": "you don't even have really in domain data like Wikipedia is not in all the languages,",
    "start": "1744465",
    "end": "1750450"
  },
  {
    "text": "and then it becomes even harder. And so, oftentimes,",
    "start": "1750450",
    "end": "1756480"
  },
  {
    "text": "so now that we are scaling this up, we are looking at, um, language models,",
    "start": "1756480",
    "end": "1761610"
  },
  {
    "text": "neural language models that are trained in a multilingual way and that are fine tuned on a small, ah, in domain, ah,",
    "start": "1761610",
    "end": "1770010"
  },
  {
    "text": "limiting what they decide if available. But yeah. Also this type is not particularly,",
    "start": "1770010",
    "end": "1776595"
  },
  {
    "text": "um, obvious how to do it. Yeah, sure. So thank you for like this amazing result.",
    "start": "1776595",
    "end": "1785085"
  },
  {
    "start": "1780000",
    "end": "1941000"
  },
  {
    "text": "But, I just wanna comment because like, I've noticed that like Wikipedia actually will have like different content with different language you choose.",
    "start": "1785085",
    "end": "1793065"
  },
  {
    "text": "So for example, they'll have like very detailed, like description of some like basically topic.",
    "start": "1793065",
    "end": "1800279"
  },
  {
    "text": "And then in other languages, even if with like really commonly used language like Chinese,",
    "start": "1800280",
    "end": "1806445"
  },
  {
    "text": "they'll actually just have completely different content or basically simplified content.",
    "start": "1806445",
    "end": "1811860"
  },
  {
    "text": "So I'm like pretty sure this also gonna happen with like rarely used languages.",
    "start": "1811860",
    "end": "1818910"
  },
  {
    "text": "So yeah, I - I just, I just generally think that like Wikipedia might not be like basically.",
    "start": "1818910",
    "end": "1827010"
  },
  {
    "text": "[OVERLAPPING] You might not be like very direct reference to the tran - translation. Yeah.",
    "start": "1827010",
    "end": "1833380"
  },
  {
    "text": "Yeah, yeah, so it - it's an excellent point and this is something that I'm going to discuss more in the third part of the lecture.",
    "start": "1833380",
    "end": "1840155"
  },
  {
    "text": "And, um, in a way, ah, this is the translation problem, right?",
    "start": "1840155",
    "end": "1846735"
  },
  {
    "text": "So we need to accept the fact that content that is originated in",
    "start": "1846735",
    "end": "1852510"
  },
  {
    "text": "a certain language may have a different topic distribution than content that originates in another language.",
    "start": "1852510",
    "end": "1859395"
  },
  {
    "text": "And what you want to translate is really a content that originates in - in the source language.",
    "start": "1859395",
    "end": "1865585"
  },
  {
    "text": "Right? And so you need to - to live with it, that that's, ah, that's what it is.",
    "start": "1865585",
    "end": "1870620"
  },
  {
    "text": "[LAUGHTER] So oftentimes in, um, in the public benchmarks in - in the literature,",
    "start": "1870620",
    "end": "1878420"
  },
  {
    "text": "you find that people assume that corpora are comparable. So everything that originates in English,",
    "start": "1878420",
    "end": "1885069"
  },
  {
    "text": "and - and let's see, Nepali essentially comes from the same kind of sources. So it's news and it's all news,",
    "start": "1885069",
    "end": "1892155"
  },
  {
    "text": "talking about similar things. But in practice, this is not true, it's not true for Wikipedia,",
    "start": "1892155",
    "end": "1897419"
  },
  {
    "text": "as you correctly said, but it's also true for news, right? Because if - if local news in, ah,",
    "start": "1897419",
    "end": "1904125"
  },
  {
    "text": "Nepal and local news over here, ah, it's quite different, right?",
    "start": "1904125",
    "end": "1909360"
  },
  {
    "text": "So this is a general problem, yeah, and this has implications in terms of the matters that we are going to use as we will discuss later.",
    "start": "1909360",
    "end": "1919750"
  },
  {
    "text": "Other questions? Or- I'm not sure if I was clear.",
    "start": "1924080",
    "end": "1930015"
  },
  {
    "text": "It's really hard to [LAUGHTER] speak without seeing [LAUGHTER] without feedback.",
    "start": "1930015",
    "end": "1935760"
  },
  {
    "text": "[LAUGHTER] Uh, please- please let me know if- if- if anything is- is not clear.",
    "start": "1935760",
    "end": "1941040"
  },
  {
    "start": "1941000",
    "end": "1969000"
  },
  {
    "text": "Okay. Let's talk about modeling. And this is going to be, um, most of,",
    "start": "1941040",
    "end": "1946710"
  },
  {
    "text": "uh, our- uh, where we are going to spend most of our time. So remember that we have this, uh,",
    "start": "1946710",
    "end": "1954210"
  },
  {
    "text": "funky chart where we have domain and languages and it's a pretty complicated learning setting.",
    "start": "1954210",
    "end": "1960420"
  },
  {
    "text": "And here for simplicity, we are going to focus just on English and Nepali languages.",
    "start": "1960420",
    "end": "1968280"
  },
  {
    "text": "Um, and we start with the simplest setting ever, which is Supervised Learning. Assuming that all data is in the same domain.",
    "start": "1968280",
    "end": "1976575"
  },
  {
    "start": "1969000",
    "end": "2157000"
  },
  {
    "text": "So perhaps you have a small training set and the test set is in the same domain",
    "start": "1976575",
    "end": "1984120"
  },
  {
    "text": "as the training set- as the parallel training set, okay? So we denote as x,",
    "start": "1984120",
    "end": "1990360"
  },
  {
    "text": "the source sentence as y, the target sentence d. So d is, uh,",
    "start": "1990360",
    "end": "1995610"
  },
  {
    "text": "the parallel dataset that collects all these sentence pairs, right? And so this is the typical empirical risk minimization framework",
    "start": "1995610",
    "end": "2005210"
  },
  {
    "text": "whereby you, uh, you know, you do supervised learning, in this case,p you minimize the cross-entropy loss and you want to maximize",
    "start": "2005210",
    "end": "2011990"
  },
  {
    "text": "the probability of the target sentence given the source sentence. And so a way to visualize this is to say that,",
    "start": "2011990",
    "end": "2018410"
  },
  {
    "text": "uh, x, uh, is my English sentence. It goes to my encoder, decoder NMT system that produces a prediction,",
    "start": "2018410",
    "end": "2027215"
  },
  {
    "text": "and then we have a loss that measure the discrepancy between the human reference that,",
    "start": "2027215",
    "end": "2032390"
  },
  {
    "text": "you know, you took the sentence x, you asked, uh, your translator that gave you the human reference.",
    "start": "2032390",
    "end": "2037804"
  },
  {
    "text": "And so the cross-entropy loss measures the discrepancy between the model prediction and the human reference, right?",
    "start": "2037805",
    "end": "2044750"
  },
  {
    "text": "Um, now, uh, notice that here I'm denoting with boxes.",
    "start": "2044750",
    "end": "2051725"
  },
  {
    "text": "Uh, [LAUGHTER] now, uh, model components, in this case, the blue box is the encoder that processes English, er, sentences,",
    "start": "2051725",
    "end": "2060139"
  },
  {
    "text": "and the red box is the decoder that, uh, operates in Nepali.",
    "start": "2060140",
    "end": "2066004"
  },
  {
    "text": "And, uh, I just wanted to add one more thing, which is that if you don't have a lot of parallel data,",
    "start": "2066005",
    "end": "2072994"
  },
  {
    "text": "you need to regularize. And so you can do a word k, uh, which is pretty standard.",
    "start": "2072995",
    "end": "2078500"
  },
  {
    "text": "So you kind of minimize the L2 norm or the parameters, but there are also other methods that I think, uh,",
    "start": "2078500",
    "end": "2085040"
  },
  {
    "text": "in the machine learning class, you may have seen like dropout where you set to zero at random,",
    "start": "2085040",
    "end": "2090379"
  },
  {
    "text": "uh, hidden units in your encoder decoder. Or you can do label smoothing whereby you, um,",
    "start": "2090380",
    "end": "2097310"
  },
  {
    "text": "uh, in your cross-entropy loss instead of- actually it should be more more over here.",
    "start": "2097310",
    "end": "2102530"
  },
  {
    "text": "Uh, instead of setting, uh, um, as a target,",
    "start": "2102530",
    "end": "2107690"
  },
  {
    "text": "uh, for the correct word. So this is, uh, the probability over the whole sequence which you can factorize over,",
    "start": "2107690",
    "end": "2113450"
  },
  {
    "text": "uh, each individual word by the product, uh, rule. Uh, so for every word,",
    "start": "2113450",
    "end": "2119855"
  },
  {
    "text": "you- you- you have the correct, uh, word that you want- sorry.",
    "start": "2119855",
    "end": "2124970"
  },
  {
    "text": "At every timestamp, you want to predict the next word. And now instead of assigning 100% of probability on the, uh, next word,",
    "start": "2124970",
    "end": "2133940"
  },
  {
    "text": "you- let's say you assign 90% of the probability and the remaining 10% you evenly distribute across",
    "start": "2133940",
    "end": "2140420"
  },
  {
    "text": "all the remaining words so that the model is not too overly confident. So the combinations of these two things are usually good ways to,",
    "start": "2140420",
    "end": "2148490"
  },
  {
    "text": "um, regularize the system, okay? So that's the simplest setting. Now let's see what happens when we have also some source side, uh, monolingual data.",
    "start": "2148490",
    "end": "2158285"
  },
  {
    "start": "2157000",
    "end": "2446000"
  },
  {
    "text": "So here now we have a- an additional dataset that has only, uh, sentences in the source- in the source language, English.",
    "start": "2158285",
    "end": "2168619"
  },
  {
    "text": "So in addition to d, now we have also, uh, M_s, which is the monolingual data on the source, uh, side.",
    "start": "2168620",
    "end": "2175760"
  },
  {
    "text": "And so we have a bunch of X's. So typically, M is much greater than N, right?",
    "start": "2175760",
    "end": "2182570"
  },
  {
    "text": "And now, a typical way to use, uh, uh, this data is to model the marginal distribution of the data of x, right?",
    "start": "2182570",
    "end": "2191954"
  },
  {
    "text": "And so there are many ways to do that. One way that has proven to be pretty effective",
    "start": "2191955",
    "end": "2198415"
  },
  {
    "text": "in machine translation is to do denoising autoencoding. And so here the idea is that, um,",
    "start": "2198415",
    "end": "2205515"
  },
  {
    "text": "you have something similar to what we had before, except that now the input is taken from this monolingual dataset, okay?",
    "start": "2205515",
    "end": "2215550"
  },
  {
    "text": "And you add noise to it, and I'm going to describe the noise in a- in a second.",
    "start": "2215550",
    "end": "2221380"
  },
  {
    "text": "And then the, er, job, uh, of the encoder decoder is simply to denoise the noisy input.",
    "start": "2221380",
    "end": "2230255"
  },
  {
    "text": "And the cross-entropy loss measure the discrepancy between the cle- the prediction and the actual clean input.",
    "start": "2230255",
    "end": "2237890"
  },
  {
    "text": "But now notice that the decoder is not this red decoder because the decoder now is a decoder that operates in English,",
    "start": "2237890",
    "end": "2247519"
  },
  {
    "text": "but the encoder does not. The encoder is- uh, is, uh, the same that you have seen here.",
    "start": "2247520",
    "end": "2254270"
  },
  {
    "text": "So again, the- uh, the loss function here is,",
    "start": "2254270",
    "end": "2260030"
  },
  {
    "text": "er, very similar to before, except that, uh, the target is the clean input x,",
    "start": "2260030",
    "end": "2266270"
  },
  {
    "text": "and the input is, uh, a noisifed version of x.",
    "start": "2266270",
    "end": "2271880"
  },
  {
    "text": "So in this case, we are not predicting something in Nepali but something in English.",
    "start": "2271880",
    "end": "2278464"
  },
  {
    "text": "So this is a, uh, if you want the limitation of- of this work, but this is useful because you are anyway,",
    "start": "2278465",
    "end": "2286115"
  },
  {
    "text": "doing some good modeling of, uh, the input sentences,",
    "start": "2286115",
    "end": "2291650"
  },
  {
    "text": "and you're gonna train the encoder parameters that are going to be shared with your supervised system.",
    "start": "2291650",
    "end": "2298445"
  },
  {
    "text": "So the encoder is shared between, um, uh, the translation task on parallel data, right?",
    "start": "2298445",
    "end": "2305840"
  },
  {
    "text": "And the denoising auto-encoder task. So essentially you have an encoder and two decoders.",
    "start": "2305840",
    "end": "2311270"
  },
  {
    "text": "One that operates in Nepali, one that operates in English. And, um, so in terms of noise,",
    "start": "2311270",
    "end": "2320059"
  },
  {
    "text": "there are essentially two types of noise that we have been using in our work. Others are possible, but, uh,",
    "start": "2320060",
    "end": "2326795"
  },
  {
    "text": "in the simplest case, you can drop words or swap words. So assume that the input sentence is,",
    "start": "2326795",
    "end": "2333395"
  },
  {
    "text": "\"The cat sat on the mat.\" Then if you swap words, you may, uh, provide at the input \"The cat the on sat mat.\"",
    "start": "2333395",
    "end": "2341090"
  },
  {
    "text": "And so here the encoder decoder needs to understand a little bit of the- the syntax,",
    "start": "2341090",
    "end": "2346624"
  },
  {
    "text": "the grammatical rules in order to reorder. If you drop- let's say you drop the last word,",
    "start": "2346624",
    "end": "2352730"
  },
  {
    "text": "\"The cat sat on the,\" then, uh, the model needs to understand a little bit of the semantics because it needs to assign,",
    "start": "2352730",
    "end": "2359210"
  },
  {
    "text": "a higher probability to mat now, right? And so you can see that there is a little bit of,",
    "start": "2359210",
    "end": "2366800"
  },
  {
    "text": "uh, so there are two hyperparameters here. So one- actually, there are several ways to use denoising auto-encoding.",
    "start": "2366800",
    "end": "2373910"
  },
  {
    "text": "So you can use denoising auto-encoding as a way to pre-train the encoder. Or you can use it, uh,",
    "start": "2373910",
    "end": "2380270"
  },
  {
    "text": "as auxiliary loss when you do supervised learning. So you can have this term plus Lambda, this term, okay?",
    "start": "2380270",
    "end": "2389540"
  },
  {
    "text": "So both ways are fine. And, uh, so there is a very critical hyperparameter here,",
    "start": "2389540",
    "end": "2398420"
  },
  {
    "text": "which is the level of noise. If you don't have any noise or if the noise level is too low,",
    "start": "2398420",
    "end": "2403565"
  },
  {
    "text": "then this task is trivial because of the attention, you can simply copy the input.",
    "start": "2403565",
    "end": "2409099"
  },
  {
    "text": "And so the encoder and the decoder don't need to learn anything. If the noise level is too high,",
    "start": "2409100",
    "end": "2415744"
  },
  {
    "text": "then you destroy the input here. So the encoder is not useful and you just do language modeling using the decoder.",
    "start": "2415745",
    "end": "2423755"
  },
  {
    "text": "But remember that this decoder is then, uh, not use- used for translation because, uh,",
    "start": "2423755",
    "end": "2431195"
  },
  {
    "text": "what you use for- in machine translation system is- is the encoder box, right? The encoder module. Okay, so,",
    "start": "2431195",
    "end": "2440464"
  },
  {
    "text": "uh, there are other ways to use source-side monolingual data. In addition to denoising of encoding,",
    "start": "2440465",
    "end": "2447004"
  },
  {
    "start": "2446000",
    "end": "2609000"
  },
  {
    "text": "you could also do, uh, self-training, which is a method that comes from the '90s, if not earlier.",
    "start": "2447004",
    "end": "2454400"
  },
  {
    "text": "And the idea is very simple. So again, you take a sentence from your source-side monolingual dataset and you add noise to it,",
    "start": "2454400",
    "end": "2463265"
  },
  {
    "text": "and then you have an encoder decoder that tries to this time translate from this noisy input, okay?",
    "start": "2463265",
    "end": "2470870"
  },
  {
    "text": "And now what's the reference? The reference is given by a stale version of your machine translation system, okay?",
    "start": "2470870",
    "end": "2480109"
  },
  {
    "text": "Where the reference is produced by, let's say beam. And so, uh, the cross-entropy loss is then going to measure the discrepancy between",
    "start": "2480110",
    "end": "2489320"
  },
  {
    "text": "your prediction and what the prediction from- from a stale version of your system gave.",
    "start": "2489320",
    "end": "2495350"
  },
  {
    "text": "And the reason why this works is that when you do beam, you actually typically, um,",
    "start": "2495350",
    "end": "2502670"
  },
  {
    "text": "produce better quality, uh, outputs. And so when you train,",
    "start": "2502670",
    "end": "2509440"
  },
  {
    "text": "now this encoder-decoder by cross-entropy loss, you're going to learn the decoding process, okay?",
    "start": "2509440",
    "end": "2516490"
  },
  {
    "text": "And so this is something good for you. In addition, when you train,",
    "start": "2516490",
    "end": "2521920"
  },
  {
    "text": "you inject noise and, and the noise is regularizing, it's kind of smoothing out your prediction space.",
    "start": "2521920",
    "end": "2528700"
  },
  {
    "text": "And so if you're predicting correctly one sentence now also nearby sentences,",
    "start": "2528700",
    "end": "2533919"
  },
  {
    "text": "and by nearby I mean sentences that are similar phrases so they have a good overlap with the current sentence,",
    "start": "2533919",
    "end": "2540819"
  },
  {
    "text": "are gonna be more likely predicted correctly. And so we have this paper where we analyze a little bit, uh, these aspects.",
    "start": "2540820",
    "end": "2549055"
  },
  {
    "text": "And so the algorithm is very simple. And so first you train your machine translation system on the parallel data,",
    "start": "2549055",
    "end": "2555984"
  },
  {
    "text": "and then you repeat the following process. So first you decode your monolingual dataset using your current,",
    "start": "2555985",
    "end": "2563110"
  },
  {
    "text": "uh, machine translation system. And you make a new, uh, parallel data set of sentences from your monolingual dataset with,",
    "start": "2563110",
    "end": "2572109"
  },
  {
    "text": "sorry, with the, ah, translations from your current system.",
    "start": "2572110",
    "end": "2577345"
  },
  {
    "text": "And then you retrain the model, this p of y- p of y given x on the union of",
    "start": "2577345",
    "end": "2582910"
  },
  {
    "text": "your original parallel data plus this, uh, auxiliary dataset. And so here you have two hyperparameters.",
    "start": "2582910",
    "end": "2591880"
  },
  {
    "text": "One is the noise level and the other is the hyperparameters that weight this, ah, auxiliary dataset.",
    "start": "2591880",
    "end": "2598750"
  },
  {
    "text": "So this is set training loss, okay? Now let's- so that's- that concludes how we can use a source-side monolingual data.",
    "start": "2598750",
    "end": "2609385"
  },
  {
    "start": "2609000",
    "end": "2765000"
  },
  {
    "text": "Let me say a word about how we can use target-side monolingual data. So you could use the target-side monolingual data, uh,",
    "start": "2609385",
    "end": "2617155"
  },
  {
    "text": "to train a language model and then train the machine translation system in the residual space. So this language model.",
    "start": "2617155",
    "end": "2622945"
  },
  {
    "text": "But it turns out that there is a much more effective way to leverage this data, and that's called back translation.",
    "start": "2622945",
    "end": "2629410"
  },
  {
    "text": "So at the high level, it works as follows. So you take a sentence from your target-side monolingual data set, y_t here.",
    "start": "2629410",
    "end": "2639280"
  },
  {
    "text": "And on the parallel dataset, you train also backward machine translation system",
    "start": "2639280",
    "end": "2645370"
  },
  {
    "text": "that goes from Nepali to English, okay? So that's- so now you have a red encoder that takes Nepali and,",
    "start": "2645370",
    "end": "2651940"
  },
  {
    "text": "and the blue decoder that was in the English space and so you map the sentence into English,",
    "start": "2651940",
    "end": "2659500"
  },
  {
    "text": "this Nepali sentence into English. And now this may not be a correct translation but it's",
    "start": "2659500",
    "end": "2666010"
  },
  {
    "text": "a noisy input that you feed to your encoder-decoder that you want to train, right?",
    "start": "2666010",
    "end": "2672270"
  },
  {
    "text": "And so now the input is noisy, but the target here is clean because it",
    "start": "2672270",
    "end": "2678869"
  },
  {
    "text": "comes from the original target-side monolingual dataset. And so this is a very powerful algorithm because, um,",
    "start": "2678870",
    "end": "2692079"
  },
  {
    "text": "unlike cell training, here the targets are clean but the input is a little noisy,",
    "start": "2692080",
    "end": "2699100"
  },
  {
    "text": "and that's usually much better than having clean inputs but noisy targets, right? Because the targets affect",
    "start": "2699100",
    "end": "2704830"
  },
  {
    "text": "essentially all the other signals that you backpropagate through the NMT systems.",
    "start": "2704830",
    "end": "2710539"
  },
  {
    "text": "And this is- and you can see back translation is a way to do data augmentation because you",
    "start": "2710550",
    "end": "2715750"
  },
  {
    "text": "produce noisy version of inputs, ah, for a given target,",
    "start": "2715750",
    "end": "2721900"
  },
  {
    "text": "a little bit like in vision where they do, uh, well, I guess this is not the right audience to,",
    "start": "2721900",
    "end": "2728140"
  },
  {
    "text": "to do this analogy, but it should work on vision you- you will do scaling, rotation, different cropping, and that's a little bit similar to what we are doing here.",
    "start": "2728140",
    "end": "2736944"
  },
  {
    "text": "And so the algorithm again is, you train, er, backward and then forward machine translation system on the parallel data.",
    "start": "2736945",
    "end": "2745270"
  },
  {
    "text": "And then you use your backward model to decode the target-side monolingual dataset to produce an auxiliary parallel dataset.",
    "start": "2745270",
    "end": "2753430"
  },
  {
    "text": "And then you concatenate the, the, the two datasets, the original parallel data set and the auxiliary one",
    "start": "2753430",
    "end": "2760840"
  },
  {
    "text": "to train the new foreword model, okay? Of course you can combine set training and back translation.",
    "start": "2760840",
    "end": "2769150"
  },
  {
    "start": "2765000",
    "end": "2846000"
  },
  {
    "text": "So if you have both source monolingual dataset and target monolingual dataset,",
    "start": "2769150",
    "end": "2774220"
  },
  {
    "text": "you can do the following. So, uh, you can use the parallel data to train the forward and",
    "start": "2774220",
    "end": "2780490"
  },
  {
    "text": "the backward machine translation system and then at step two you can use the forward model to decode- to",
    "start": "2780490",
    "end": "2789190"
  },
  {
    "text": "translate the source-side monolingual dataset into, uh, this data. And you can use the backward machine translation system to",
    "start": "2789190",
    "end": "2799000"
  },
  {
    "text": "translate the target-side monolingual dataset into these, uh, translations, okay?",
    "start": "2799000",
    "end": "2805465"
  },
  {
    "text": "And then you treat these parallel sentences as real data and you concatenate them to the parallel dataset.",
    "start": "2805465",
    "end": "2812935"
  },
  {
    "text": "And now you retrain both the forward and the backward machine translation systems. And now as long as these two improve,",
    "start": "2812935",
    "end": "2820435"
  },
  {
    "text": "then you can go and do another iteration whereby you, again, you read the code, retranslate the source and the target-side monolingual dataset and then you go,",
    "start": "2820435",
    "end": "2829990"
  },
  {
    "text": "ah, and you retrain them. And this is as far as I know,",
    "start": "2829990",
    "end": "2836559"
  },
  {
    "text": "the most effective way to leverage monolingual data in low-resource languages nowadays.",
    "start": "2836560",
    "end": "2844820"
  },
  {
    "text": "Lemme talk a little bit about how we can do multilingual training.",
    "start": "2845970",
    "end": "2852055"
  },
  {
    "start": "2846000",
    "end": "2975000"
  },
  {
    "text": "So in this case we have parallel datasets on different language pairs.",
    "start": "2852055",
    "end": "2859059"
  },
  {
    "text": "And, uh, so you have a parallel dataset for English-Nepali, one for English-Hindi, one for Hindi-English,",
    "start": "2859060",
    "end": "2865750"
  },
  {
    "text": "or Nepali-Hindi, or any subset of these. And this is super simple.",
    "start": "2865750",
    "end": "2871720"
  },
  {
    "text": "So the way that it works is that, you have a single encoder and a single decoder, okay?",
    "start": "2871720",
    "end": "2878200"
  },
  {
    "text": "And you train by supervised learning. The only change that needs to be made is that,",
    "start": "2878200",
    "end": "2886075"
  },
  {
    "text": "at the input of the encoder you concatenate also a token that specifies the language in which you want to translate.",
    "start": "2886075",
    "end": "2894595"
  },
  {
    "text": "And so the encoder will learn to process multiple languages.",
    "start": "2894595",
    "end": "2900160"
  },
  {
    "text": "The decoder will learn to the- to produce",
    "start": "2900160",
    "end": "2905515"
  },
  {
    "text": "multiple languages as one and it will pick the language based on the token specified by the encoder input.",
    "start": "2905515",
    "end": "2913464"
  },
  {
    "text": "And so training is just minimizing the cross-entropy loss for all the parallel datasets that you have where you simply add an extra token,",
    "start": "2913465",
    "end": "2923830"
  },
  {
    "text": "ah, in the source sentence that specifies the target, uh, language that you want to translate.",
    "start": "2923830",
    "end": "2930680"
  },
  {
    "text": "And the only thing that I wanted to add on this is that often times it helps if you pre-process the data",
    "start": "2931890",
    "end": "2940990"
  },
  {
    "text": "by- I'm not sure if you learn about bipolar encoding sentence pieces,  essentially ways",
    "start": "2940990",
    "end": "2947710"
  },
  {
    "text": "to segment words into syllables or frequent, um, character n-grams.",
    "start": "2947710",
    "end": "2954895"
  },
  {
    "text": "And so if you concatenate this data in order to learn these ways to segment the data, then it's also possible that for",
    "start": "2954895",
    "end": "2961630"
  },
  {
    "text": "many languages there is a good fraction of the dictionary that is shared. And so this also helps,",
    "start": "2961630",
    "end": "2967150"
  },
  {
    "text": "making sure that you can do a good job translating multiple languages at one- at once.",
    "start": "2967150",
    "end": "2974599"
  },
  {
    "text": "And so my- the conclusion so far is that even without domain effect,",
    "start": "2974880",
    "end": "2982269"
  },
  {
    "start": "2975000",
    "end": "3053000"
  },
  {
    "text": "there are a lot of training paradigms depending on the available data that you have.",
    "start": "2982270",
    "end": "2987970"
  },
  {
    "text": "A priori, it's very hard to tell which method works",
    "start": "2987970",
    "end": "2993040"
  },
  {
    "text": "best nowadays because it really depends on how much data you have,",
    "start": "2993040",
    "end": "2998245"
  },
  {
    "text": "how different are the domains and- and what is the language pair that you are working with.",
    "start": "2998245",
    "end": "3004484"
  },
  {
    "text": "For instance, the domains may be very different, but if you have a lot of data, maybe, um, it doesn't matter much.",
    "start": "3004485",
    "end": "3013300"
  },
  {
    "text": "In practice, we have keen regions denoising with encoding by translation,",
    "start": "3013640",
    "end": "3019950"
  },
  {
    "text": "multilingual training that we're pretty wide. And nowadays, the field is",
    "start": "3019950",
    "end": "3027180"
  },
  {
    "text": "at the stage in which we are trying to figure out the best way to combine them. And right now there is a lot of what I would say,",
    "start": "3027180",
    "end": "3033840"
  },
  {
    "text": "craftsmanship to figure out how to best combine them. And, um, but hopefully we can",
    "start": "3033840",
    "end": "3041070"
  },
  {
    "text": "find- and I think there is a lot of effort in trying to automate this process because right now,",
    "start": "3041070",
    "end": "3046185"
  },
  {
    "text": "um, there is a lot of cross-validation I would say to figure out all these hyperparameters.",
    "start": "3046185",
    "end": "3052840"
  },
  {
    "start": "3053000",
    "end": "3109000"
  },
  {
    "text": "So the open challenges here is- are, dealing with a diversity of domain or domains,",
    "start": "3053510",
    "end": "3061799"
  },
  {
    "text": "dealing with datasets that have very wildly different translation quality,",
    "start": "3061800",
    "end": "3069345"
  },
  {
    "text": "some are very noisy, some are very clean, dealing with different- with datasets of",
    "start": "3069345",
    "end": "3074400"
  },
  {
    "text": "different size and very different language pairs. And yeah.",
    "start": "3074400",
    "end": "3083580"
  },
  {
    "text": "And so I would say that in general, it may be counter-intuitive, but working low-resource machine translation doesn't",
    "start": "3083580",
    "end": "3089940"
  },
  {
    "text": "mean training small models on small data. But actually means training even bigger models on",
    "start": "3089940",
    "end": "3096974"
  },
  {
    "text": "even more data because you need to compensate for the lack of supervision that you have, of direct supervision that you have.",
    "start": "3096975",
    "end": "3103715"
  },
  {
    "text": "Very good. Before I go on, are there any questions?",
    "start": "3103715",
    "end": "3108880"
  },
  {
    "text": "Um, yeah. I just had a quick question regarding, um- in the- in a few of the previous algorithms that you described,",
    "start": "3108880",
    "end": "3116144"
  },
  {
    "start": "3109000",
    "end": "3197000"
  },
  {
    "text": "is it necessary to retrain entirely, um, like retrain the model entirely,",
    "start": "3116145",
    "end": "3122010"
  },
  {
    "text": "or is there some way to augment the model or fine-tune it on the, um, on the generated-",
    "start": "3122010",
    "end": "3129000"
  },
  {
    "text": "So actually what usually happens is that as you iterate, you can make the model bigger.",
    "start": "3129000",
    "end": "3136305"
  },
  {
    "text": "So when you train on the parallel data set, usually this is not much data and so you need to train in something small.",
    "start": "3136305",
    "end": "3143369"
  },
  {
    "text": "Otherwise, you over- overfit too much. But once you add, uh, the monolingual data that you- this,",
    "start": "3143370",
    "end": "3150930"
  },
  {
    "text": "uh, A_t data set, then this model can be much bigger than the original model.",
    "start": "3150930",
    "end": "3157005"
  },
  {
    "text": "Now, it's not super obvious how to,",
    "start": "3157005",
    "end": "3163018"
  },
  {
    "text": "uh, you know, initialize a bigger model from a smaller model. [LAUGHTER] And so that's why people usually initialize from random.",
    "start": "3163019",
    "end": "3172335"
  },
  {
    "text": "At the next iterations, you can, um, initialize from the model at the previous iteration.",
    "start": "3172335",
    "end": "3181485"
  },
  {
    "text": "Uh, what we usually find is that initializing at random usually works as well.",
    "start": "3181485",
    "end": "3189510"
  },
  {
    "text": "Gotcha. Thank you. Thank you. Okay. Any other question?",
    "start": "3189510",
    "end": "3197804"
  },
  {
    "start": "3197000",
    "end": "3229000"
  },
  {
    "text": "When you say, uh, usually the model launch, ah, I was wondering do you mean that you add, ah,",
    "start": "3197804",
    "end": "3204615"
  },
  {
    "text": "more layers with more parameters, ah, as the model keeps training? Uh, usually you just make it bigger.",
    "start": "3204615",
    "end": "3212400"
  },
  {
    "text": "Yeah. Um, yeah, the more layers, more parameter.",
    "start": "3212400",
    "end": "3219450"
  },
  {
    "text": "So whether it is wider or- or deeper, um, I think usually, um- yeah.",
    "start": "3219450",
    "end": "3230630"
  },
  {
    "start": "3229000",
    "end": "3265000"
  },
  {
    "text": "I'm not entirely sure there is a definite answer on that. Um, usually making the encoder deeper is a good thing.",
    "start": "3230630",
    "end": "3239610"
  },
  {
    "text": "[LAUGHTER] Making the decoder deeper doesn't buy you much. Um, so usually we play with the encoder, I would say.",
    "start": "3239610",
    "end": "3247950"
  },
  {
    "text": "And, um- but yeah, uh, there is not so much difference in practice,",
    "start": "3247950",
    "end": "3255210"
  },
  {
    "text": "uh, I would say. Um, yeah. So you can imagine just double the size of your hidden state, that would work.",
    "start": "3255210",
    "end": "3264580"
  },
  {
    "start": "3265000",
    "end": "3294000"
  },
  {
    "text": "Okay? Okay. So let's see how",
    "start": "3265670",
    "end": "3271140"
  },
  {
    "text": "this- so I- I didn't speak about models but I spoke about algorithms. So you can turn these algorithms into models a little",
    "start": "3271140",
    "end": "3278400"
  },
  {
    "text": "bit and talk about joint distribution, marginal distribution. But in my view, it's just simpler to think in terms of algorithms because also the way that,",
    "start": "3278400",
    "end": "3288480"
  },
  {
    "text": "uh, we implement them. And so let's see how these algorithms can be put",
    "start": "3288480",
    "end": "3294120"
  },
  {
    "start": "3294000",
    "end": "3599000"
  },
  {
    "text": "together in some interesting case studies. So actually I realize that I'm really going slow.",
    "start": "3294120",
    "end": "3300675"
  },
  {
    "text": "So let's see the case where you only have monolingual data and no parallel data.",
    "start": "3300675",
    "end": "3306135"
  },
  {
    "text": "So this is what we call unsupervised machine translation. So let's say that you have an English and a French data set, uh,",
    "start": "3306135",
    "end": "3314445"
  },
  {
    "text": "this is not a typical use-case of unsupervised machine translation but this is where it works really well.",
    "start": "3314445",
    "end": "3320925"
  },
  {
    "text": "So let's focus for this for now. And so you take a sentence from the target monolingual data set,",
    "start": "3320925",
    "end": "3327735"
  },
  {
    "text": "you go through your encoder-decoder, and you produce an English translation. Obviously, you don't have the reference here.",
    "start": "3327735",
    "end": "3333780"
  },
  {
    "text": "So what you could do is to feed this to a machine translation system",
    "start": "3333780",
    "end": "3339300"
  },
  {
    "text": "that goes from English to French so that you kind of reconstruct the original French sentence.",
    "start": "3339300",
    "end": "3344640"
  },
  {
    "text": "And now you have- you can have another signal to back-propagate through your machine translation system.",
    "start": "3344640",
    "end": "3351255"
  },
  {
    "text": "And you can do the same going from English to French to English. This is very much what people have done in vision.",
    "start": "3351255",
    "end": "3357765"
  },
  {
    "text": "They call it, uh, cycle consistency. You can see this as an auto-encoder where the intermediate representation is uh,",
    "start": "3357765",
    "end": "3365430"
  },
  {
    "text": "er, you know, er, is a language in- in English. The problem is that, the- as it is,",
    "start": "3365430",
    "end": "3371550"
  },
  {
    "text": "the model is not constrained to produce something that is, uh, a fluent English sentence.",
    "start": "3371550",
    "end": "3377654"
  },
  {
    "text": "So in the vision domain, people use adversarial training, but in NLP it's kinda tricky because this is a discrete sequence.",
    "start": "3377654",
    "end": "3385125"
  },
  {
    "text": "And so in order to make sure that this decoder produces English- fluent English sentences,",
    "start": "3385125",
    "end": "3392085"
  },
  {
    "text": "you could imagine to do denoising auto-encoding, all right? So you could take a no- you could take an English sentence,",
    "start": "3392085",
    "end": "3398880"
  },
  {
    "text": "noisify it, go through your denoising auto-encoding. Now, this decoder is the same block that you have here,",
    "start": "3398880",
    "end": "3404835"
  },
  {
    "text": "it's gonna be forced to learn the statistics and the regularities of the English language.",
    "start": "3404835",
    "end": "3411580"
  },
  {
    "text": "The problem is that if you look at this decoder, this decoder is, in the denoising auto-encoding game,",
    "start": "3411920",
    "end": "3421155"
  },
  {
    "text": "it is operating on the output of this encoder that takes English as input,",
    "start": "3421155",
    "end": "3427320"
  },
  {
    "text": "while here, encoder takes French as input. It could be very well be the case that",
    "start": "3427320",
    "end": "3432795"
  },
  {
    "text": "the representation produced by these two encoders is different. So this decoder may work very well in this setting,",
    "start": "3432795",
    "end": "3439530"
  },
  {
    "text": "but not in this setting. And so in other ways, how can we make sure that these red and blue blocks are interchangeable?",
    "start": "3439530",
    "end": "3448244"
  },
  {
    "text": "How can we make sure that there is good modularity? And so- so one way to do this is to use the trick that we use for multilingual training,",
    "start": "3448245",
    "end": "3458115"
  },
  {
    "text": "whereby we have a single encoder and a single decoder. So the decoder is shared across French and English,",
    "start": "3458115",
    "end": "3465975"
  },
  {
    "text": "and the encoder is shared across English and French. And we specify the target language by an extra token, ah, at the input.",
    "start": "3465975",
    "end": "3475005"
  },
  {
    "text": "And so in particular, if you learn, ah, common VPs and if you share parameters,",
    "start": "3475005",
    "end": "3480630"
  },
  {
    "text": "then this process- sorry, this process really works well and- and you have an- a decoder that operates well with an- ah,",
    "start": "3480630",
    "end": "3489255"
  },
  {
    "text": "whenever it is fed with a hidden state that comes from an encoder operating on English or French.",
    "start": "3489255",
    "end": "3496410"
  },
  {
    "text": "And so again, the ke- key ingredients are iterative backtranslation,",
    "start": "3496410",
    "end": "3501525"
  },
  {
    "text": "denoising auto-encoding and multi-lingual training. And for unsupervised machine translation,",
    "start": "3501525",
    "end": "3506670"
  },
  {
    "text": "we do, uh, back translation in an online manner whereby for a given mini-batch,",
    "start": "3506670",
    "end": "3513270"
  },
  {
    "text": "we do, uh, back translation. We don't do it, uh, with the stale version on the model but you could do that, well, that works less well.",
    "start": "3513270",
    "end": "3523335"
  },
  {
    "text": "And so when you do this on English-French, you find, uh- actually you can get pretty good performance.",
    "start": "3523335",
    "end": "3531120"
  },
  {
    "text": "A BLEU of 30 usually gives you pretty fluent translations that are, uh, also adequate.",
    "start": "3531120",
    "end": "3536910"
  },
  {
    "text": "And, uh, if you compare that to what you got with the, er, supervised baseline that is trained on the parallel data set,",
    "start": "3536910",
    "end": "3544799"
  },
  {
    "text": "you find that training on 10 million monolingual sentences",
    "start": "3544800",
    "end": "3549914"
  },
  {
    "text": "in English and 10 million French gives you the same, uh, translation accuracy than training a supervised baseline,",
    "start": "3549915",
    "end": "3557610"
  },
  {
    "text": "that is this, um, uh, red curve- actually this red curve- this blue curve and this red curve.",
    "start": "3557610",
    "end": "3565619"
  },
  {
    "text": "This is the neural version, this is the phrase-based version with 100,000 parallel sentences.",
    "start": "3565620",
    "end": "3571364"
  },
  {
    "text": "So in other words, each parallel sentence pair is equivalent to 100 monolinguals sentences.",
    "start": "3571364",
    "end": "3581984"
  },
  {
    "text": "Equivalent in the sense that they give you a machine translation system of similar, uh, um, accuracy.",
    "start": "3581985",
    "end": "3591060"
  },
  {
    "text": "And so now the more the domains are different, and the more the languages are different from each other, the worse it gets.",
    "start": "3591060",
    "end": "3602550"
  },
  {
    "text": "And so that's why when you do low-resource machine translation, this is the extreme case of unsupervised machine translation,",
    "start": "3602550",
    "end": "3608549"
  },
  {
    "text": "you need to learn from lots of data in order to compensate for the lack of direct supervision. Um-",
    "start": "3608550",
    "end": "3616210"
  },
  {
    "text": "I'm gonna, um, maybe give you, uh,",
    "start": "3617540",
    "end": "3624030"
  },
  {
    "text": "an example on FloRes where for the FloRes, as we have seen there was no in-domain parallel data.",
    "start": "3624030",
    "end": "3631530"
  },
  {
    "text": "There was some monolingual data that was in domain. But not very much, and there was quite a bit of auto- auto domain parallel data,",
    "start": "3631530",
    "end": "3640770"
  },
  {
    "text": "you remember the one we used sentences from, um, Bible and Ubuntu.",
    "start": "3640770",
    "end": "3646380"
  },
  {
    "text": "And then we have quite a bit of monolingual data that is auto domain. And so this is the supervised baseline.",
    "start": "3646380",
    "end": "3655115"
  },
  {
    "text": "Unsupervised machine translation here didn't work at all because very much like was",
    "start": "3655115",
    "end": "3660530"
  },
  {
    "text": "mentioning the- the Wikipedia domains are not quite, uh, aligned.",
    "start": "3660530",
    "end": "3665610"
  },
  {
    "text": "And so this doesn't have unsupervised machine translation. If you do back translation,",
    "start": "3665610",
    "end": "3671460"
  },
  {
    "text": "if you do iterative back-translation, you do quite a bit better than the supervised baseline, which is quite good.",
    "start": "3671460",
    "end": "3678270"
  },
  {
    "text": "But now if you add those, so English-Hindi, parallel data, you do quite a bit better.",
    "start": "3678270",
    "end": "3683940"
  },
  {
    "text": "And now also the unsupervised machine translation works. It's unsupervised for English- English-Nepali,",
    "start": "3683940",
    "end": "3689220"
  },
  {
    "text": "but you do have supervision for English-Hindi. And so the combination of backtranslation and,",
    "start": "3689220",
    "end": "3695835"
  },
  {
    "text": "um, multilingual training is here, the winning combination. And this is something that we see, uh, through in general.",
    "start": "3695835",
    "end": "3704595"
  },
  {
    "text": "Okay? So I'm gonna skip the results on English-Burmese. Actually I had a nice demo.",
    "start": "3704595",
    "end": "3712020"
  },
  {
    "text": "But I'm going to show it to you later if there is time. Um, and so, as I said,",
    "start": "3712020",
    "end": "3719145"
  },
  {
    "text": "we have quite a few good components which we can combine pretty easily.",
    "start": "3719145",
    "end": "3725475"
  },
  {
    "text": "Right now the research is about how to best combine them, how to best weigh datasets,",
    "start": "3725475",
    "end": "3731460"
  },
  {
    "text": "how to best weigh the examples in order to automate the current cross-validation based process, I would say.",
    "start": "3731460",
    "end": "3739440"
  },
  {
    "text": "And the other message here is that it's- low-resource machine translation is a big data problem.",
    "start": "3739440",
    "end": "3746700"
  },
  {
    "text": "It- it requires big compute, it's a pretty big engineering feat. Uh, in order to, uh,",
    "start": "3746700",
    "end": "3753720"
  },
  {
    "text": "compensate for the lack of parallel data. Are there any questions on this?",
    "start": "3753720",
    "end": "3760410"
  },
  {
    "text": "I- I was just wondering when you mentioned that the parallel to, um, vision in cycle consistency, um,",
    "start": "3760410",
    "end": "3767369"
  },
  {
    "text": "you mentioned that we can't do adversarial training- Yeah. -and I was just wondering if you could flesh that out and wh we couldn't just",
    "start": "3767370",
    "end": "3774600"
  },
  {
    "text": "use say like an LSTM that performs adversarial training better. Yeah. Yes. So there are actually a bunch of papers trying to do, um,",
    "start": "3774600",
    "end": "3785100"
  },
  {
    "text": "adversarial training or, uh, dense style training for, uh, text generation.",
    "start": "3785100",
    "end": "3792345"
  },
  {
    "text": "I must say that it's a pretty active research area. I haven't seen a very compelling demonstration",
    "start": "3792345",
    "end": "3800220"
  },
  {
    "text": "that these methods work very well with, we've tried. And it's a little difficult to backpropagate.",
    "start": "3800220",
    "end": "3806355"
  },
  {
    "text": "So when this produces a sentence, you need to produce a, you know, a sentence and that's discrete.",
    "start": "3806355",
    "end": "3813480"
  },
  {
    "text": "And so you could backpropagate using reinforce kind of methods. You could do a lot of these things but essentially-",
    "start": "3813480",
    "end": "3822045"
  },
  {
    "text": "it's just a little hard to make it work and it's very finicky.",
    "start": "3822045",
    "end": "3827444"
  },
  {
    "text": "So it may work on simple datasets, but at scale, it's very hard to.",
    "start": "3827445",
    "end": "3833475"
  },
  {
    "text": "So another thing- another consideration is that anything that you do has to work at scale. Because again, the value- the amount of",
    "start": "3833475",
    "end": "3841680"
  },
  {
    "text": "information that you get from a monolingual sentence is not very much. And now if you do a lot of compute,",
    "start": "3841680",
    "end": "3847380"
  },
  {
    "text": "if you- or if your gradients have a lot of noise, uh, like when you train with reinforce,",
    "start": "3847380",
    "end": "3852810"
  },
  {
    "text": "then, uh, it's not going to work. But it's possible that people may come up with ways to make it work.",
    "start": "3852810",
    "end": "3861240"
  },
  {
    "text": "I don't think this is true at present, but it could be in the future.",
    "start": "3861240",
    "end": "3867390"
  },
  {
    "text": "So let me spend five minutes on the analysis, um, and then you will have the slides so you can,",
    "start": "3867390",
    "end": "3873555"
  },
  {
    "text": "uh, go over the remaining details. So here, uh, we- so the- the starting point is to say, well,",
    "start": "3873555",
    "end": "3880545"
  },
  {
    "text": "if I want to simulate low-resource machine translation with a high resource language like French to English.",
    "start": "3880545",
    "end": "3886530"
  },
  {
    "text": "Let's say you take EuroParl data, you have, let's say 20,000 parallel sentences and 100,000 monolingual target sentences,",
    "start": "3886530",
    "end": "3893744"
  },
  {
    "text": "and you apply backtranslation, you get a very nice improvement. Now, if you come here to Facebook,",
    "start": "3893744",
    "end": "3899670"
  },
  {
    "text": "[LAUGHTER] and- and you try this on Facebook data, you find that the improvement is actually very, very minimal.",
    "start": "3899670",
    "end": "3907275"
  },
  {
    "text": "And that relates to the discussion that we had at the very beginning, that what people talk about in different parts of the world is very different.",
    "start": "3907275",
    "end": "3915450"
  },
  {
    "text": "And so now you- you need- it's like you need to align two, uh,",
    "start": "3915450",
    "end": "3920820"
  },
  {
    "text": "point clouds, but the distribution in these two point clouds is very, very different from each other, and so it's very difficult to align them.",
    "start": "3920820",
    "end": "3927825"
  },
  {
    "text": "And so here I was making the example that even for English speaking countries, if you look at",
    "start": "3927825",
    "end": "3933150"
  },
  {
    "text": "topics on- on sports, you have that- in America people may",
    "start": "3933150",
    "end": "3938310"
  },
  {
    "text": "talk more about Football and Baseball while in the UK, more about Cricket and Soccer, right? And so for the same topic,",
    "start": "3938310",
    "end": "3945990"
  },
  {
    "text": "you have different distribution words, but you also have a different distribution of topics.",
    "start": "3945990",
    "end": "3951135"
  },
  {
    "text": "And so this is what we call the source target domain mismatch. So you may have several kinds of domain mismatch.",
    "start": "3951135",
    "end": "3958545"
  },
  {
    "text": "Typically, you have a mismatch between the training distribution and the test distribution.",
    "start": "3958545",
    "end": "3964200"
  },
  {
    "text": "Here I'm talking also about the mismatch between the source domain, the source language, the source domain,",
    "start": "3964200",
    "end": "3970109"
  },
  {
    "text": "and the target domain. Okay? And so there is a hypothesis that this may make backtranslation",
    "start": "3970110",
    "end": "3976499"
  },
  {
    "text": "less effective because even if you were to perfectly translate target side monolingual data,",
    "start": "3976499",
    "end": "3981945"
  },
  {
    "text": "once you translate it, it's going to be out of domain, uh,",
    "start": "3981945",
    "end": "3986985"
  },
  {
    "text": "with respect to the data that you really want to translate, which originates in the source domain. And so we had a very,",
    "start": "3986985",
    "end": "3995070"
  },
  {
    "text": "uh, nice controlled setting, to study this problem. Um, where, uh, we create",
    "start": "3995070",
    "end": "4003379"
  },
  {
    "text": "a syntactic dataset where the source domain comes from EuroParl data,",
    "start": "4003379",
    "end": "4009140"
  },
  {
    "text": "and the target domain counts from OpenSubtitles, which are movie captions. And now, by creating the target domain as a mixture of the two,",
    "start": "4009140",
    "end": "4017465"
  },
  {
    "text": "you can precisely control the amount of, uh, in-domainess between the source and the target domain.",
    "start": "4017465",
    "end": "4024365"
  },
  {
    "text": "And by varying Alpha, you can vary that. And so the major result is this figure,",
    "start": "4024365",
    "end": "4030860"
  },
  {
    "text": "where Alpha measures how much is the target domain,",
    "start": "4030860",
    "end": "4036185"
  },
  {
    "text": "uh, similar to the source domain. So if Alpha is equal to 1, they are all, uh, in the same domain.",
    "start": "4036185",
    "end": "4042350"
  },
  {
    "text": "If Alpha is equal to 0, they are very different. One is EuroParl and the other is OpenSubtitles. And so it turns out that in these extreme regime,",
    "start": "4042350",
    "end": "4051155"
  },
  {
    "text": "actually set training, which is this red line, works better than backtranslation.",
    "start": "4051155",
    "end": "4057035"
  },
  {
    "text": "But as you make the domains more and more similar, backtranslation is much better than set training.",
    "start": "4057035",
    "end": "4063320"
  },
  {
    "text": "And both of them are much better than, um, just if you were to use the parallel data.",
    "start": "4063320",
    "end": "4069630"
  },
  {
    "text": "Um, so I'm going to skip all of this. You can look at the paper,",
    "start": "4070240",
    "end": "4075410"
  },
  {
    "text": "uh, and the slides. I want to conclude that there are other things that I didn't talk about, like filtering.",
    "start": "4075410",
    "end": "4082430"
  },
  {
    "text": "This is one of the most exciting things nowadays, and the idea is to, uh,",
    "start": "4082430",
    "end": "4089260"
  },
  {
    "text": "essentially learn a joint embedding space for sentences by simply training a multilingual system on lots of public available data,",
    "start": "4089260",
    "end": "4098005"
  },
  {
    "text": "and then you use this in order to do nearest neighbor retrieval of a sentence for what the corresponding translation would be in other languages.",
    "start": "4098005",
    "end": "4106880"
  },
  {
    "text": "And they found that- they collected a large data set and they were able to beat the performance of",
    "start": "4106880",
    "end": "4113659"
  },
  {
    "text": "state of the art machine translation system on high-resource languages like English-German, English-Russian.",
    "start": "4113660",
    "end": "4120964"
  },
  {
    "text": "And the idea is that by using much more data, although noisy,",
    "start": "4120965",
    "end": "4126170"
  },
  {
    "text": "you can do better than using a curated, high-quality dataset, and this is something that we see over and over.",
    "start": "4126170",
    "end": "4133700"
  },
  {
    "text": "And again, the idea here is that we need to figure out how to best combine backtranslation, this filtering, multilingual,",
    "start": "4133700",
    "end": "4139970"
  },
  {
    "text": "and pretraining in order to, uh, uh, get, uh, the best combination ever for solving or for improving low-resource machine translation.",
    "start": "4139970",
    "end": "4149975"
  },
  {
    "text": "And so I just want to- maybe I should conclude here, uh, by thanking my collaborators and by, uh,",
    "start": "4149975",
    "end": "4157204"
  },
  {
    "text": "telling you that, um, uh, if you have any questions about this lecture,",
    "start": "4157205",
    "end": "4163130"
  },
  {
    "text": "you can always e-mail me, drop me a line, I'd be happy to follow up. And also, in my lab,",
    "start": "4163130",
    "end": "4168680"
  },
  {
    "text": "we have a lot of opportunities, from internships to full-time positions as a research scientist, research engineer.",
    "start": "4168680",
    "end": "4175759"
  },
  {
    "text": "So if you're interested or are curious, just also, uh, drop me an e-mail. Okay. Thank you.",
    "start": "4175760",
    "end": "4182609"
  },
  {
    "text": "Thanks a lot, Marc'Aurelio. Um, so- so maybe there's still a few people that might have questions.",
    "start": "4185440",
    "end": "4193730"
  },
  {
    "text": "And we are happy to stay a few more minutes for questions.",
    "start": "4193730",
    "end": "4197970"
  },
  {
    "text": "Happy to answer questions. Yes. Uh, I'd love to learn more about the models that you used.",
    "start": "4200830",
    "end": "4209540"
  },
  {
    "text": "Uh, actually, should we- should we go back to the model that you first talked about- spoke about, right, before back-translation?",
    "start": "4209540",
    "end": "4215495"
  },
  {
    "text": "Uh, in order to understand, uh, you have a pipeline from English to English, right? In this one, uh, you want something like, uh,",
    "start": "4215495",
    "end": "4223190"
  },
  {
    "text": "you- the data augmentation techniques like in vision such as dropping the word or switching, uh, switching words to be able to make an augmented dataset, is that right?",
    "start": "4223190",
    "end": "4231864"
  },
  {
    "text": "That's right. So the analogy that I made is, for back-translation were, yes,",
    "start": "4231865",
    "end": "4238390"
  },
  {
    "text": "all these methods, essentially, you don't have x and y- golden x and y pairs,",
    "start": "4238390",
    "end": "4245150"
  },
  {
    "text": "and so for set training, what you do, you, uh, fantasize the target.",
    "start": "4245150",
    "end": "4252485"
  },
  {
    "text": "For back-translation, you fantasize the input. And so you can see all these methods as a way in particular, back-translation,",
    "start": "4252485",
    "end": "4259310"
  },
  {
    "text": "is very similar to the data augmentation that people do in vision in the sense that here,",
    "start": "4259310",
    "end": "4264875"
  },
  {
    "text": "the transformation is not, uh, rule-based, it's produced by a backward machine translation system,",
    "start": "4264875",
    "end": "4271280"
  },
  {
    "text": "but it does the same objective of regularizing by adding a lot of noisy,",
    "start": "4271280",
    "end": "4277414"
  },
  {
    "text": "uh, uh, labeled data. So if you go back to the previous slide, when you say you fantasize the- the target.",
    "start": "4277415",
    "end": "4282905"
  },
  {
    "text": "So in this case, you have, uh, one where you- where you predict the goal target and one where you- where you change the input and then predict the target,",
    "start": "4282905",
    "end": "4290045"
  },
  {
    "text": "is that how it is? Yeah. So in- for set training, the way that it works is that you take the clean input,",
    "start": "4290045",
    "end": "4297380"
  },
  {
    "text": "you pass it through your machine translation system at the previous iteration. And, uh, you decode with beam or with other methods,",
    "start": "4297380",
    "end": "4305150"
  },
  {
    "text": "and you got a prediction for what the label should be. And that- that's now your reference.",
    "start": "4305150",
    "end": "4311150"
  },
  {
    "text": "But the way that you train your machine translation system is by noisifying the input. So you add noise to your input and the noise is you drop words,",
    "start": "4311150",
    "end": "4319010"
  },
  {
    "text": "you swap words, and then you try to predict the, uh, target that you fantasize.",
    "start": "4319010",
    "end": "4325085"
  },
  {
    "text": "And the idea is that the- that the two targets should be the same? Yeah. So the prediction and- and these targets,",
    "start": "4325085",
    "end": "4331460"
  },
  {
    "text": "when you train with cross-entropy loss, you- you try to tie them together as much as possible. Okay. Thanks so much. I have a followup question, later.",
    "start": "4331460",
    "end": "4341320"
  },
  {
    "text": "Yeah. This is a very- this is one of the first semi-supervised learning methods that,",
    "start": "4341320",
    "end": "4346630"
  },
  {
    "text": "uh, you find in the machine learning community. There are a lot of variants of this where they have perhaps, er, uh, uh,",
    "start": "4346630",
    "end": "4352330"
  },
  {
    "text": "a community of experts that produces the, uh- the label.",
    "start": "4352330",
    "end": "4359315"
  },
  {
    "text": "Um, there are a lot of variants of this and, um, it's something that makes a lot of sense,",
    "start": "4359315",
    "end": "4365510"
  },
  {
    "text": "particularly for asymmetric tasks. Like if you do, image speci- if you do text classification,",
    "start": "4365510",
    "end": "4372200"
  },
  {
    "text": "if you do summarization, then back-translation is not really applicable because, uh, you know, if you go from a label category- from a categorical,",
    "start": "4372200",
    "end": "4382790"
  },
  {
    "text": "uh, uh, input to a whole sentence, that's a very difficult task, right?",
    "start": "4382790",
    "end": "4387830"
  },
  {
    "text": "So back-translation works really well for symmetric tasks like, uh, machine translation.",
    "start": "4387830",
    "end": "4393110"
  },
  {
    "text": "But for, uh, things that are, uh- for many-to-one mapping, self-training is definitely- definitely works better.",
    "start": "4393110",
    "end": "4400219"
  },
  {
    "text": "Uh, self-training works well also in machine translation when there is a lot of domain mismatch between the source and the target as we're seeing.",
    "start": "4400220",
    "end": "4407765"
  },
  {
    "text": "Yeah. So unfortunately, these algorithms- so it's hard to",
    "start": "4407765",
    "end": "4412850"
  },
  {
    "text": "say in general what works best because it really depends on the application, it really depends on the kind of data that you have.",
    "start": "4412850",
    "end": "4420540"
  },
  {
    "text": "Dose anyone else have a question they'd like to ask? Well, it seems like we're maybe not getting another immediate question.",
    "start": "4421660",
    "end": "4430355"
  },
  {
    "text": "And I guess we have gone through the end of the time, that we're, uh, meant to do.",
    "start": "4430355",
    "end": "4436175"
  },
  {
    "text": "So maybe we should call it and bring it to a close, but thank you so much, Marc'Aurelio.",
    "start": "4436175",
    "end": "4441575"
  },
  {
    "text": "I mean, I hope everyone really enjoyed that. And I, you know, speaking as someone who did work in machine translation for a decade,",
    "start": "4441575",
    "end": "4449270"
  },
  {
    "text": "though I haven't so much for the last few years, I mean, you know, it actually still seems to me just amazing how successfully you can build things,",
    "start": "4449270",
    "end": "4460250"
  },
  {
    "text": "um, with these, um, building with monolingual data and using ideas like the back-translation.",
    "start": "4460250",
    "end": "4467960"
  },
  {
    "text": "I mean, it's just actually incredible that that's providing such competitive, um, machine translation systems now.",
    "start": "4467960",
    "end": "4476015"
  },
  {
    "text": "And, you know, obviously, this is something that isn't just of academic interest,",
    "start": "4476015",
    "end": "4481099"
  },
  {
    "text": "as you might have realized if you've thought about it, right? If you're at a company like Facebook, right? Being able- actually able to translate well data on domains that are very",
    "start": "4481100",
    "end": "4492290"
  },
  {
    "text": "far from news data or the Bible [LAUGHTER] and in languages of smaller communities of speakers,",
    "start": "4492290",
    "end": "4498695"
  },
  {
    "text": "it's just actually super-duper important, um, to people being happy users of and members of communities.",
    "start": "4498695",
    "end": "4506135"
  },
  {
    "text": "Yeah. And- and I just want to add, uh, the kind of- these methods are pretty general,",
    "start": "4506135",
    "end": "4511400"
  },
  {
    "text": "so we apply them to summarization, Q&A, uh, style transfer.",
    "start": "4511400",
    "end": "4516980"
  },
  {
    "text": "So, you know, it's really beautiful that- it's a set of tools that you can use them in many places and it's all about,",
    "start": "4516980",
    "end": "4526429"
  },
  {
    "text": "you know, in a way, um, aligning domains with little, uh,",
    "start": "4526430",
    "end": "4531620"
  },
  {
    "text": "supervision or correspondences, right? So, yeah. Okay. Thank you very much.",
    "start": "4531620",
    "end": "4537605"
  },
  {
    "text": "Thank you. Thank you. Bye. Bye-bye. Bye.",
    "start": "4537605",
    "end": "4541199"
  }
]