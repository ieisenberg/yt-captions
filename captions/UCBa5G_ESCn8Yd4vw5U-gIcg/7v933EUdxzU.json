[
  {
    "text": "Okay, so let's begin. Um, first of all, um, I want to say congratulations,",
    "start": "4850",
    "end": "13289"
  },
  {
    "text": "you all survived the exam. Uh, well you don't have your grades back but you, you completed it. Um, so yeah, I just want to say,",
    "start": "13290",
    "end": "21510"
  },
  {
    "text": "so we're gonna have the grades back as soon as we can. Um, the CAs are all busy grading. And we're actually gonna cancel office hours today so we can",
    "start": "21510",
    "end": "27570"
  },
  {
    "text": "focus on getting those grades back to you quickly. Um, after this, the course definitely goes downhill.",
    "start": "27570",
    "end": "32985"
  },
  {
    "text": "So you guys can [LAUGHTER] kind of like take a breath. Um, so after the exam there's pretty much just two things left.",
    "start": "32985",
    "end": "38550"
  },
  {
    "text": "So there's the project, um, so the final presentation, uh,",
    "start": "38550",
    "end": "44290"
  },
  {
    "text": "the poster session for the project is going to be I believe the Tuesday after vacation.",
    "start": "44290",
    "end": "49475"
  },
  {
    "text": "Um, it's like a big auditorium hall, there's gonna be a lot of people from industry and academia.",
    "start": "49475",
    "end": "55075"
  },
  {
    "text": "Um, it's really exciting to have like, you know, so many smart people showing off their hard work. Um, and then you have the last p-set which is logic. Yeah?",
    "start": "55075",
    "end": "64670"
  },
  {
    "text": "It's on Monday. Oh Monday, okay, yeah. Um, so, so right after you're back from vacation is that poster session.",
    "start": "64670",
    "end": "71405"
  },
  {
    "text": "Um, and then on Thursday is the last piece of it is due, logic.",
    "start": "71405",
    "end": "78080"
  },
  {
    "text": "Um, so logic is, um, uh,",
    "start": "78080",
    "end": "83170"
  },
  {
    "text": "so this is not like my official opinion but lo- I think logic when I took the class was easier than the others,",
    "start": "83170",
    "end": "88850"
  },
  {
    "text": "uh, it doesn't take as much time so you guys are definitely past the hardest point in this class.",
    "start": "88850",
    "end": "94220"
  },
  {
    "text": "Yeah, I think, I think that's the general opinion, yeah. Um, but that being said,",
    "start": "94220",
    "end": "99840"
  },
  {
    "text": "I wouldn't wait until the last minute, so I'd still start early and- Personally, I didn't get the [inaudible].",
    "start": "99840",
    "end": "107340"
  },
  {
    "text": "[LAUGHTER] Then, um, yeah so Piazza and the office hours will be your best friend, yeah.",
    "start": "107340",
    "end": "114384"
  },
  {
    "text": "Um, okay. So but today though we're talking about this fun advanced-ish topic which is deep learning.",
    "start": "114385",
    "end": "121510"
  },
  {
    "text": "Um, I say ish because I think a lot of you are probably working, um, on deep learning or have heard of it already.",
    "start": "121510",
    "end": "128194"
  },
  {
    "text": "A lot of you have it in your projects, um, and today, um, we just kinda do a very,",
    "start": "128195",
    "end": "136379"
  },
  {
    "text": "very high level broad passive, a lot of different subjects within deep learning. Hopefully get you excited, um,",
    "start": "136380",
    "end": "141834"
  },
  {
    "text": "give you kind of like a shallow understanding of a lot of different topics so that if you wanna take, um, follow-up classes like 224N or, um,",
    "start": "141835",
    "end": "150015"
  },
  {
    "text": "229 even, uh, then you'll be armed with some kind of background knowledge.",
    "start": "150015",
    "end": "156090"
  },
  {
    "text": "Um, okay, so first we're gonna talk about the history. So deep learning, you've probably heard of it,",
    "start": "156130",
    "end": "162095"
  },
  {
    "text": "it's really big especially in the last five, ten years, but it's actually been around for a long time.",
    "start": "162095",
    "end": "168910"
  },
  {
    "text": "Um, so even back to the '40s, there's this era where people are trying to build more computational neuroscience models.",
    "start": "168910",
    "end": "176900"
  },
  {
    "text": "They noticed that they knew back then that, you know, there's neurons in the brain and they're arranged in these networks,",
    "start": "176900",
    "end": "183175"
  },
  {
    "text": "and they know that intelligence arises from these small parts. And so they really wanted to model that.",
    "start": "183175",
    "end": "190204"
  },
  {
    "text": "Uh, the first people to really do this were McCulloch and Pitts. Uh, so Pitts was actually a logician,",
    "start": "190205",
    "end": "195680"
  },
  {
    "text": "and it was, um, they were concerned with making these kind of like logical circuits out of a network like topology.",
    "start": "195680",
    "end": "203720"
  },
  {
    "text": "Um, like what kind of logical expressions can we implement with the network? Um, back then this was,",
    "start": "203720",
    "end": "210525"
  },
  {
    "text": "this was all just a mathematical model. Like there was no backpropagation, there were no parameters,",
    "start": "210525",
    "end": "216290"
  },
  {
    "text": "uh, there was no inference. It was just trying to, uh, write about fru, I guess like theorems and proofs about what kind of problems these structures can solve.",
    "start": "216290",
    "end": "226610"
  },
  {
    "text": "Um, and then Hebb came along about 10 years later and started moving things in the direction of I guess like training these networks.",
    "start": "226610",
    "end": "235850"
  },
  {
    "text": "Uh, he noticed that if two cells are firing a lot together, then they should have some kind of connection, um, that is strong.",
    "start": "235850",
    "end": "242209"
  },
  {
    "text": "Uh, this is- was inspired by observation.",
    "start": "242210",
    "end": "247290"
  },
  {
    "text": "So there's actually no formal math theory backing this. There was, a lot of it was just, uh, very smart people making, um, conjecture.",
    "start": "247290",
    "end": "254815"
  },
  {
    "text": "And then it wasn't until the '60s, um, that,",
    "start": "254815",
    "end": "261195"
  },
  {
    "text": "so neural networks was I guess you could say maybe in the mainstream like a lot of people were thinking about it and excited about it,",
    "start": "261195",
    "end": "267430"
  },
  {
    "text": "until 1969 when Minsky and Papert they released this, uh, very famous book called Perceptrons, uh,",
    "start": "267430",
    "end": "274159"
  },
  {
    "text": "which was this like big fat book of proofs. And they were basically talking about the,",
    "start": "274160",
    "end": "282010"
  },
  {
    "text": "they approved a bunch of theorems that were about the limits of, uh, very shallow neural networks.",
    "start": "282010",
    "end": "287195"
  },
  {
    "text": "Um, so for example, [NOISE] um, early I think very, very early in this class we talked about the XOR example where if you have, um,",
    "start": "287195",
    "end": "296630"
  },
  {
    "text": "two classes and they're arranged in this, um, configuration then there's no,",
    "start": "296630",
    "end": "302044"
  },
  {
    "text": "there's no linear classification boundary that you can use to separate them and classify them correctly.",
    "start": "302045",
    "end": "307099"
  },
  {
    "text": "And so th- Minsky and Papert in their book Perceptrons they came up with a lot of these, um,",
    "start": "307100",
    "end": "312510"
  },
  {
    "text": "I guess you could say like counterexamples, um, like that a lot of theorems that really proved",
    "start": "312510",
    "end": "318710"
  },
  {
    "text": "that these thin neural networks couldn't really do a lot. Um, and at the time it,",
    "start": "318710",
    "end": "325935"
  },
  {
    "text": "it was, it was a little bit of a killing blow to neural network research. Uh, so mainstream AI became much more",
    "start": "325935",
    "end": "333310"
  },
  {
    "text": "logical and neural networks were pushed very much into I guess a minority group.",
    "start": "333310",
    "end": "339130"
  },
  {
    "text": "Uh, so there's all these people thinking about and working on it. But the mainstream AI went definitely towards kind of the symbolic logic based,",
    "start": "339130",
    "end": "347600"
  },
  {
    "text": "um, methods that Percy has been talking about the last couple of weeks. Um, but like I said,",
    "start": "347600",
    "end": "354550"
  },
  {
    "text": "there's still these people in the background working on it. So, um, for example in 1974, um,",
    "start": "354550",
    "end": "361830"
  },
  {
    "text": "Werbos came up with this idea that back-propagation that we learned about using the chain rule to automatically update weights in order to improve predictions,",
    "start": "361830",
    "end": "371830"
  },
  {
    "text": "um, and then later on, um, so Hinton, and Rumelhart, and Williams,",
    "start": "371830",
    "end": "377264"
  },
  {
    "text": "they kind of I guess you could say they, um, popularized this, so they, they definitely I guess you could say rediscovered, um,",
    "start": "377265",
    "end": "385125"
  },
  {
    "text": "Werbos's findings and they really said, \"Oh, hey everybody, you can use backpropagation.\" Um, and it's a",
    "start": "385125",
    "end": "391800"
  },
  {
    "text": "mathematically, well kinda like well-founded way of training these like deep neural networks. Um, and then in the '80s, uh,",
    "start": "391800",
    "end": "401850"
  },
  {
    "text": "so today we're gonna talk about two types of neural networks; convolutional neural networks and recurrent neural networks.",
    "start": "401850",
    "end": "408310"
  },
  {
    "text": "And the convolutional networks trace back to the '80s. So there's this neocognitron that was invented by a Japanese, uh, Fukushima,",
    "start": "408310",
    "end": "416660"
  },
  {
    "text": "[NOISE] and it kind of laid out the architecture for a CNN,",
    "start": "416660",
    "end": "423750"
  },
  {
    "text": "but there was no way of training it. And in the actual paper, they used hand-tuned weights. They're like oh, hey,",
    "start": "423750",
    "end": "429350"
  },
  {
    "text": "there's this architecture you can use and basically we just like by trial and error came up with these numbers to plug in and,",
    "start": "429350",
    "end": "435199"
  },
  {
    "text": "and look at how it works. Uh, now it just seems like insane, but back then that was this, you know,",
    "start": "435200",
    "end": "441195"
  },
  {
    "text": "there were no ways of training these things. Um, until LeCun came about 10 years later, and so,",
    "start": "441195",
    "end": "447525"
  },
  {
    "text": "um, he applied those ideas of backpropagation to CNN's.",
    "start": "447525",
    "end": "452560"
  },
  {
    "text": "And LeCun actually came up with a, so there's the LeCun Net which was a very famous check reading system,",
    "start": "452560",
    "end": "459845"
  },
  {
    "text": "um, and it was one of the first like industrial large-scale applications of deep learning.",
    "start": "459845",
    "end": "464905"
  },
  {
    "text": "Uh, so whenever you write a check in and have your bank read it, um, almost all the time there's a machine-learning model that",
    "start": "464905",
    "end": "472280"
  },
  {
    "text": "reads that check for you and, um, those check reading systems are some of the oldest machine-learning models that have been like used at scale.",
    "start": "472280",
    "end": "481450"
  },
  {
    "text": "And then later, so recurrent neural networks came in the '90s, so Elman kinda proposed it and",
    "start": "482240",
    "end": "488320"
  },
  {
    "text": "then there's this problem with training it that we'll talk about later, um, called expect- exploding or vanishing gradients.",
    "start": "488320",
    "end": "494175"
  },
  {
    "text": "And then, um, Hochreiter and Schmidhuber, about 10 years later came out with I guess you could say maybe a,",
    "start": "494175",
    "end": "502710"
  },
  {
    "text": "it solved to some extent those issues with a long short-term memory network, an LSTM. And we'll talk about that later.",
    "start": "502710",
    "end": "508490"
  },
  {
    "text": "Um. And then- but I guess you- you could still say that,",
    "start": "508490",
    "end": "515034"
  },
  {
    "text": "um, neural networks were kind of in the minority. So in the '80s,",
    "start": "515035",
    "end": "520044"
  },
  {
    "text": "you used a lot of rule-based AI, um, in the '90s, people were all about, uh,",
    "start": "520045",
    "end": "526270"
  },
  {
    "text": "support vector machines and inventing new kernels. Um, if you remember support vector machine is",
    "start": "526270",
    "end": "533410"
  },
  {
    "text": "basically just like a it's- it's a linear classifier with the hinge loss, and a kernel is a way of projecting, um,",
    "start": "533410",
    "end": "539950"
  },
  {
    "text": "data into kinda like a non-linear subspace. Um, but it was- the 2000s,",
    "start": "539950",
    "end": "546520"
  },
  {
    "text": "people finally started making progress. Um, so Hinton had this cool idea of hey,",
    "start": "546520",
    "end": "552370"
  },
  {
    "text": "we can train these deep networks one layer at a time. So we'll pre-train one layer, and then we'll pre-train a second layer and stack that on,",
    "start": "552370",
    "end": "558565"
  },
  {
    "text": "third layer stack that on, and you can build up these successive representations, um.",
    "start": "558565",
    "end": "563950"
  },
  {
    "text": "And then deep learning kinda became a thing. Er, so this looks like maybe,",
    "start": "563950",
    "end": "570520"
  },
  {
    "text": "uh, three-four years ago where they started taking off. And ever since then, it's really been in the mainstream,",
    "start": "570520",
    "end": "577315"
  },
  {
    "text": "and you can as kind of proof evidence towards its mainstreamness. Uh, you can look at all of these applications.",
    "start": "577315",
    "end": "583949"
  },
  {
    "text": "So speech recognition. Um, for about almost a decade, this performance in speech recognition, um,",
    "start": "583950",
    "end": "589920"
  },
  {
    "text": "state-of-the-art recognizers were using a hidden Markov model based um, like that was- that was the heart of these algorithms.",
    "start": "589920",
    "end": "597399"
  },
  {
    "text": "And for 10 years, performance just stagnated and then all a sudden neural networks came around and dropped that performance.",
    "start": "597400",
    "end": "603610"
  },
  {
    "text": "And what's new and surprising is that all of the big comps so IBM, Google, Microsoft,",
    "start": "603610",
    "end": "610120"
  },
  {
    "text": "they all switched over from these classical speech recognizers into fully end-to-end neural network-based recognizers, er,",
    "start": "610120",
    "end": "618279"
  },
  {
    "text": "very quickly in a matter of years, and when these- these large companies are operating at scale and they've, you know,",
    "start": "618280",
    "end": "625495"
  },
  {
    "text": "dozens maybe hundreds of people have tuned these systems very intricately and for them to so",
    "start": "625495",
    "end": "631150"
  },
  {
    "text": "quickly and so radically shift the core technology behind this product really speaks to its power.",
    "start": "631150",
    "end": "637300"
  },
  {
    "text": "Um, same thing with object recognition. So there's this er ImageNet competition er which",
    "start": "637300",
    "end": "644350"
  },
  {
    "text": "goes on every year that says basically like how well can you say what's in a picture and the first and so for years people use these handcrafted features,",
    "start": "644350",
    "end": "657199"
  },
  {
    "text": "um, and all of a sudden AlexNet was proposed and it almost got half the error of the next best submission for this competition,",
    "start": "657199",
    "end": "666490"
  },
  {
    "text": "and then ever since then people have been using neural networks. And now if you want to do computer vision, um,",
    "start": "666490",
    "end": "672355"
  },
  {
    "text": "you kind of have to use these CNN's, it's just the default, if you walk into a conference, every single poster is going to have a CNN in it.",
    "start": "672355",
    "end": "679460"
  },
  {
    "text": "Um, same thing with Go. So, um, um Google DeepMind had a,",
    "start": "679470",
    "end": "686125"
  },
  {
    "text": "had a CNN based um algorithm, they trained with reinforcement learning and it beat the world champion in this very difficult game,",
    "start": "686125",
    "end": "693610"
  },
  {
    "text": "and then in 2017 it did even better, it didn't even need a like real data just did self play um, and machine translation.",
    "start": "693610",
    "end": "701230"
  },
  {
    "text": "So Google Translate for almost a decade had been working on building a very,",
    "start": "701230",
    "end": "707410"
  },
  {
    "text": "very advanced and a very well performing classical machine translation system",
    "start": "707410",
    "end": "713095"
  },
  {
    "text": "and then all of a sudden, um, the first machine translation system was proposed in 2014-2015,",
    "start": "713095",
    "end": "719380"
  },
  {
    "text": "and then about a year later they threw away 10, you know, almost a decade of work on this system and",
    "start": "719380",
    "end": "727060"
  },
  {
    "text": "transferred entirely to a completely new algorithm, um, which again speaks to its power.",
    "start": "727060",
    "end": "733480"
  },
  {
    "text": "Er, so but what is I guess deep learning like what why",
    "start": "733480",
    "end": "741009"
  },
  {
    "text": "is this thing so powerful and why is it so good and, um, I think, um,",
    "start": "741010",
    "end": "747535"
  },
  {
    "text": "so broadly speaking it's a way of learning, um, of taking data you can slurp up any kind of data you want like I sequence, a picture, um,",
    "start": "747535",
    "end": "756640"
  },
  {
    "text": "even vectors um, or even like a game like Go and you can turn it into a vector,",
    "start": "756640",
    "end": "762970"
  },
  {
    "text": "and this vector is going to be a dense representation of whatever information is captured by that data.",
    "start": "762970",
    "end": "769945"
  },
  {
    "text": "And this is very powerful because these vectors are compositional and you can use these components,",
    "start": "769945",
    "end": "777130"
  },
  {
    "text": "these modules of your deep learning system kind of like Lego blocks, you can, you know, concatenate vectors and add them together and use this to",
    "start": "777130",
    "end": "785500"
  },
  {
    "text": "modify you and just the compositionality makes it very flexible.",
    "start": "785500",
    "end": "790730"
  },
  {
    "text": "Um, okay. So today we're going to talk about feedforward neural networks,",
    "start": "790920",
    "end": "796180"
  },
  {
    "text": "convolutional networks, which work on images, or I guess just anything with repeated kind of structural information in it,",
    "start": "796180",
    "end": "804220"
  },
  {
    "text": "recurrent neural networks which operate over sequences, and then if we have time we'll get to some,",
    "start": "804220",
    "end": "809455"
  },
  {
    "text": "um, unsupervised learning topics. Okay, so first for feedforward networks,",
    "start": "809455",
    "end": "817330"
  },
  {
    "text": "um, so in the very beginning of this class we talked about linear predictors.",
    "start": "817330",
    "end": "822970"
  },
  {
    "text": "Linear predictor, um, if you remember is basically you define like a vector w that's your weights and then you hit it with some input,",
    "start": "822970",
    "end": "831505"
  },
  {
    "text": "and you dot them together and that just gives you output. Um, and neural networks we defined very similarly.",
    "start": "831505",
    "end": "838410"
  },
  {
    "text": "So you can think of each of these hidden units as the result of a linear predictor in a way. Um, so working backwards you- so you have the,",
    "start": "838410",
    "end": "848220"
  },
  {
    "text": "you define a vector w and you hit it with some activation function- with some activation, um, like inputs, some hidden inputs,",
    "start": "848220",
    "end": "856735"
  },
  {
    "text": "and you dot that with your hidden and you get your output, and then you arrive with what you are hitting",
    "start": "856735",
    "end": "862330"
  },
  {
    "text": "by defining a vector and hitting it with inputs. Er, so you use your inputs to compute",
    "start": "862330",
    "end": "868540"
  },
  {
    "text": "hidden numbers and then you use your hidden numbers to compute your final output. Um, so in a way,",
    "start": "868540",
    "end": "875005"
  },
  {
    "text": "you're kind of like I guess stacking linear predictors, like each, each number.",
    "start": "875005",
    "end": "880240"
  },
  {
    "text": "So h1, h2, and f of beta are all the product,",
    "start": "880240",
    "end": "885310"
  },
  {
    "text": "I guess you could say they're all the result of like a little mini linear predictor and they're all kind of like roped together.",
    "start": "885310",
    "end": "891890"
  },
  {
    "text": "Um, so just to visualize this,",
    "start": "893880",
    "end": "899365"
  },
  {
    "text": "if we want to go deeper you just rinse and repeat. So this is- you can say this is a one layer neural network,",
    "start": "899365",
    "end": "904435"
  },
  {
    "text": "it's what we were talking about before with linear predictor. You just- you have your vector weights and you apply it to your inputs.",
    "start": "904435",
    "end": "911740"
  },
  {
    "text": "For a two layer, you apply, instead of a vector, to your inputs,",
    "start": "911740",
    "end": "917379"
  },
  {
    "text": "you apply a matrix to your inputs which gives you a new vector, and then you dot this intermediate vector,",
    "start": "917380",
    "end": "925600"
  },
  {
    "text": "this hidden vector with another set of weights and that gives you your final output,",
    "start": "925600",
    "end": "931040"
  },
  {
    "text": "and then you can just rinse and repeat. So you pass through a vector,",
    "start": "931080",
    "end": "936760"
  },
  {
    "text": "you pass through a matrix to get a new vector. You pass that through another matrix to get a new vector,",
    "start": "936760",
    "end": "942490"
  },
  {
    "text": "and then you finally at the very end dot it with a vector to get a single number.",
    "start": "942490",
    "end": "948140"
  },
  {
    "text": "Um, so just a word about depth, that's one of the reasons why these things are really powerful.",
    "start": "951120",
    "end": "958645"
  },
  {
    "text": "Um, so there's a lot of interpretations for why depth is helpful and why kind of like stacking these matrices works well.",
    "start": "958645",
    "end": "968170"
  },
  {
    "text": "One way to think about it is that it learns representations of the input which are hierarchical.",
    "start": "968170",
    "end": "973555"
  },
  {
    "text": "So h is going to be some kind of representation of x. H prime is going to be a slightly higher-level representation of x.",
    "start": "973555",
    "end": "983334"
  },
  {
    "text": "So for example in a lot of image processing systems, h maybe represents, um,",
    "start": "983335",
    "end": "990070"
  },
  {
    "text": "h could represent like the edges in a picture. H prime would represent, um, like corners.",
    "start": "990070",
    "end": "997750"
  },
  {
    "text": "H double prime could represent er like small like fingers or something.",
    "start": "997750",
    "end": "1003375"
  },
  {
    "text": "H triple prime would be the whole hand. Er, so it's successfully, I guess you could say like",
    "start": "1003375",
    "end": "1009000"
  },
  {
    "text": "higher-level representations of what's in the data you're giving it.",
    "start": "1009000",
    "end": "1014235"
  },
  {
    "text": "Um, another way to think about it is each layer is kind of like a step in processing and, um,",
    "start": "1014235",
    "end": "1023024"
  },
  {
    "text": "you can think of it maybe like a for-loop where it's- it's like the more, the more, er,",
    "start": "1023025",
    "end": "1029400"
  },
  {
    "text": "the more iterations you have, the more steps you, have the more depth you have, um, the more processing you're able to perform on the input.",
    "start": "1029400",
    "end": "1037694"
  },
  {
    "text": "And then last, um, the deeper the network is, um, the more kinds of functions it can represent,",
    "start": "1037695",
    "end": "1046295"
  },
  {
    "text": "and so the, um- yeah so there's flexibility in that as well.",
    "start": "1046295",
    "end": "1054330"
  },
  {
    "text": "Um, but in general, there isn't really a good formal understanding of why",
    "start": "1054330",
    "end": "1059490"
  },
  {
    "text": "depth is helpful and I think a lot of deep learning is- there's definitely a gap between the theory and the practice, um.",
    "start": "1059490",
    "end": "1068409"
  },
  {
    "text": "So yeah, so this I guess just goes to show why depth is helpful,",
    "start": "1069550",
    "end": "1075610"
  },
  {
    "text": "so if you input pixels, maybe your first layer is giving you edge detection and your second layer is giving you",
    "start": "1075610",
    "end": "1081760"
  },
  {
    "text": "little eyes or noses or ears and then your third layer and above is giving you whole objects.",
    "start": "1081760",
    "end": "1087595"
  },
  {
    "text": "Um, yeah. So just a summarize; so we have these deep neural networks and",
    "start": "1087595",
    "end": "1093760"
  },
  {
    "text": "they learn hierarchical representations of the data successfully, um, I guess you could say it's like gaining altitude in its perspective, um.",
    "start": "1093760",
    "end": "1104320"
  },
  {
    "text": "You can train them the same way that we learned, er- you can train them the same way that we learned how",
    "start": "1104320",
    "end": "1112600"
  },
  {
    "text": "to train our linear classifiers just with gradient descent, um, so you have your loss function,",
    "start": "1112600",
    "end": "1118460"
  },
  {
    "text": "you take the derivative with respect to your loss and then you propagate the gradients to step in a direction that you think would be helpful.",
    "start": "1118460",
    "end": "1124930"
  },
  {
    "text": "Um, and this optimization problem is difficult, um, so it's non-linear, and non-convex, um,",
    "start": "1124930",
    "end": "1134220"
  },
  {
    "text": "but in general if- we found that if you throw like a lot of data at it, a lot of compute at it then somehow you manage, um.",
    "start": "1134220",
    "end": "1147120"
  },
  {
    "text": "Okay. So it seems like the slides are a little out of order, but basically just to review how you train these things.",
    "start": "1147120",
    "end": "1153149"
  },
  {
    "text": "Um, in general, it's the same as a linear predictor. You define a loss function.",
    "start": "1153150",
    "end": "1158510"
  },
  {
    "text": "So for example, this is squared loss, where you'd say, I'm going to take the difference between my true output and my predicted output,",
    "start": "1158510",
    "end": "1165049"
  },
  {
    "text": "and square that, and then the idea is to minimize this. Um, and the way you do that,",
    "start": "1165050",
    "end": "1170475"
  },
  {
    "text": "is you sample data points from your training data, and you take the derivative of your parameters with respect to this,",
    "start": "1170475",
    "end": "1178110"
  },
  {
    "text": "um, with respect to your loss function, and then you move in the opposite direction of that gradient,",
    "start": "1178110",
    "end": "1184725"
  },
  {
    "text": "which would hopefully move you down on the area surface.",
    "start": "1184725",
    "end": "1189100"
  },
  {
    "text": "Um, so the problem is a non-convex optimization problem.",
    "start": "1190490",
    "end": "1196304"
  },
  {
    "text": "So, er, for example, linear classifier, because it's linear, will have co- it'll- it'll just look like a bowl,",
    "start": "1196305",
    "end": "1202559"
  },
  {
    "text": "um, whereas, these things, you have these non-linear activation functions, and you end up with a very messy looking area surface.",
    "start": "1202560",
    "end": "1210750"
  },
  {
    "text": "Um, and before the 2000s, that was the big- that was the number one thing that was holding back neural networks.",
    "start": "1210750",
    "end": "1219600"
  },
  {
    "text": "Is that they are difficult to get working or hard to train. Um, and so basically the thing that's changed is,",
    "start": "1219600",
    "end": "1228750"
  },
  {
    "text": "one, way faster computers. We have GPUs which can parallelize operations, especially those big matrix multiplications.",
    "start": "1228750",
    "end": "1235784"
  },
  {
    "text": "And then there's a lot more data. Um, that's not entirely true.",
    "start": "1235785",
    "end": "1241620"
  },
  {
    "text": "So there's also a lot of other tricks that we found out recently. So for example, if you have lots of hidden units,",
    "start": "1241620",
    "end": "1249900"
  },
  {
    "text": "then that can be helpful because it gives more- it gives more flexibility,",
    "start": "1249900",
    "end": "1256980"
  },
  {
    "text": "you could say, in the optimization. Like if you have- if you over-provision, if your model has more capacity than it needs,",
    "start": "1256980",
    "end": "1264360"
  },
  {
    "text": "then you can be more flexible with the kind of functions that you can learn. Um, so we have better optimizers.",
    "start": "1264360",
    "end": "1271320"
  },
  {
    "text": "So whereas SGD will make- it'll take- It'll step in the same direction by the same amount every time.",
    "start": "1271320",
    "end": "1276915"
  },
  {
    "text": "We have these newer optimizers like AdaGrad and ADAM, that decide how far to move in a direction,",
    "start": "1276915",
    "end": "1283170"
  },
  {
    "text": "once you've decided the direction. We have dropout, which is where you noise the outputs of each hidden unit,",
    "start": "1283170",
    "end": "1291955"
  },
  {
    "text": "and that makes the model more robust to its own errors, and it guards against overfitting.",
    "start": "1291955",
    "end": "1298400"
  },
  {
    "text": "Um, there's better initialization strategies. So there's things like Xavier initialization,",
    "start": "1298400",
    "end": "1304345"
  },
  {
    "text": "and there's things like pre-training the model on a related data set before moving on to the data you actually care about,",
    "start": "1304345",
    "end": "1311415"
  },
  {
    "text": "and then there's tricks like batch norm, uh, which is where you ensure that the inputs to your neural network units have,",
    "start": "1311415",
    "end": "1320610"
  },
  {
    "text": "uh- are normally distributed. They have mean zero, standard deviation one, and what that does is it allows you to basically take bigger step sizes.",
    "start": "1320610",
    "end": "1331360"
  },
  {
    "text": "Um, yeah, and the takeaway here is that- but i- in general",
    "start": "1333740",
    "end": "1338940"
  },
  {
    "text": "the optimization problem and the model architecture you define are very tightly coupled, and um, it's kind of a black magic to get that right balance that you need,",
    "start": "1338940",
    "end": "1351240"
  },
  {
    "text": "and we're still not very good at it. Um, okay. So we're gonna talk about convolutional neural networks now,",
    "start": "1351240",
    "end": "1358080"
  },
  {
    "text": "and so these operate over images. Um, the motivation is that- okay, so we have a picture here right?",
    "start": "1358080",
    "end": "1363525"
  },
  {
    "text": "And we want to do some kind of machine learning processing on it. Um, we have all the tools that we need to do that.",
    "start": "1363525",
    "end": "1370965"
  },
  {
    "text": "You could say, Okay, each picture, each pixel, is an element in a big long vector,",
    "start": "1370965",
    "end": "1376785"
  },
  {
    "text": "and then I'm just gonna throw that out of matrix. Um, but the thing is- is that- that",
    "start": "1376785",
    "end": "1382184"
  },
  {
    "text": "doesn't really take advantage of the fact that there's spatial structure in this picture. So this pixel, is going to be more similar to this pixel,",
    "start": "1382185",
    "end": "1390585"
  },
  {
    "text": "than this pixel down here. But if you pass this entire thing through a matrix,",
    "start": "1390585",
    "end": "1396809"
  },
  {
    "text": "then every pixel is gonna be treated uniquely and differently, and so we wanna leverage that spatial structure.",
    "start": "1396809",
    "end": "1404145"
  },
  {
    "text": "And the idea to- the core idea is with um, convolutions- so convolutions you have this thing called a filter,",
    "start": "1404145",
    "end": "1413205"
  },
  {
    "text": "which is some collection of parameters, and what you do is you run your filter over the input,",
    "start": "1413205",
    "end": "1421215"
  },
  {
    "text": "um, in order to produce each output element. So for example, this filter when applied to this upper left corner, um,",
    "start": "1421215",
    "end": "1432375"
  },
  {
    "text": "produces this upper left corner of the output, and an application of a filter works kind of like a dot-product.",
    "start": "1432375",
    "end": "1439820"
  },
  {
    "text": "Where you multiply- you multiply all the numbers, and then you add them all up,",
    "start": "1439820",
    "end": "1446555"
  },
  {
    "text": "and so how you produce these outputs, is you take your filter, and you basically just slide it around in the input,",
    "start": "1446555",
    "end": "1453150"
  },
  {
    "text": "um, in order to get your output at the next layer.",
    "start": "1453150",
    "end": "1457840"
  },
  {
    "text": "Um, so- yeah, so this- this example is a little more concrete.",
    "start": "1458990",
    "end": "1465090"
  },
  {
    "text": "So here- so whereas this was a two dimensional convolution, because we had a two-dimensional filter, and we were sliding around in both dimensions.",
    "start": "1465090",
    "end": "1471615"
  },
  {
    "text": "This is one-dimensional. We have a one-dimensional filter, and we slide it horizontally across.",
    "start": "1471615",
    "end": "1476805"
  },
  {
    "text": "So for example, at- at the very left, we apply it. So 1 times 0 is 0,",
    "start": "1476805",
    "end": "1483435"
  },
  {
    "text": "0 times 1 is 1, and negative 1 times 2 is negative 2. So negative 2 goes in the output,",
    "start": "1483435",
    "end": "1490169"
  },
  {
    "text": "and then we do the similar thing here. So we would dot-product this filter with um,",
    "start": "1490170",
    "end": "1495990"
  },
  {
    "text": "these three numbers in order to arrive at two. Um, one of the advantages of this, is that,",
    "start": "1495990",
    "end": "1504300"
  },
  {
    "text": "whereas a- so if you had- so if you had- let's say you had um,",
    "start": "1504300",
    "end": "1515894"
  },
  {
    "text": "four inputs, you- so this- this is your hidden layer. So what h_1, h_2, h_3, and h_4,",
    "start": "1515895",
    "end": "1523995"
  },
  {
    "text": "and then you had four inputs: X_1, X_2, X_3, and X_4.",
    "start": "1523995",
    "end": "1530174"
  },
  {
    "text": "If you did a regular fully-connected matrix layer, then every one of these is going to be connected to every one of these,",
    "start": "1530175",
    "end": "1540250"
  },
  {
    "text": "and your parameters, you're gonna- you're gonna end up with a four by four matrix,",
    "start": "1541040",
    "end": "1551730"
  },
  {
    "text": "W_11, W_12, W_13, W_14,",
    "start": "1551730",
    "end": "1561150"
  },
  {
    "text": "and W, W, W, W. This is what your matrix is gonna look like if this is your",
    "start": "1561150",
    "end": "1567270"
  },
  {
    "text": "W. Cause you need a new- you need a way for every one of those connections.",
    "start": "1567270",
    "end": "1573735"
  },
  {
    "text": "Whereas if you're doing convolutions, it's much more efficient.",
    "start": "1573735",
    "end": "1580245"
  },
  {
    "text": "Because there's this idea of local connectivity. So you have your h_1, h_2, h_3,",
    "start": "1580245",
    "end": "1587294"
  },
  {
    "text": "h_4, X_1, X_2, X_3, X_4.",
    "start": "1587295",
    "end": "1593820"
  },
  {
    "text": "Each hidden layer is only connected to, um, what's called its receptive field.",
    "start": "1593820",
    "end": "1601350"
  },
  {
    "text": "Which is the inputs that the filter would be applied to, and in this case,",
    "start": "1601350",
    "end": "1606675"
  },
  {
    "text": "we will only have three weights.",
    "start": "1606675",
    "end": "1612540"
  },
  {
    "text": "Because we just have this sliding window, and you apply it at each step. Um, so A, gives you local connectivity.",
    "start": "1612540",
    "end": "1626715"
  },
  {
    "text": "Um, B, it's much more efficient in terms of parameters. You- you're sharing the same parameters at different places in the input.",
    "start": "1626715",
    "end": "1635595"
  },
  {
    "text": "Um, and it gives you this cool intuition of sliding around in the input.",
    "start": "1635595",
    "end": "1641475"
  },
  {
    "text": "So it's like, I have my filter of three things, and it gives you this good intuition of- if-",
    "start": "1641475",
    "end": "1647955"
  },
  {
    "text": "if a- let's say this is- let say this is negative 1, this is 100, this is 1, and this is 3.",
    "start": "1647955",
    "end": "1653940"
  },
  {
    "text": "Then with this, you can- you can interpret this as my filter really likes whatever pattern is going on in these three inputs.",
    "start": "1653940",
    "end": "1663810"
  },
  {
    "text": "Um, and it doesn't like so much all the other patterns that it's picking up on.",
    "start": "1663810",
    "end": "1669525"
  },
  {
    "text": "Um, and so you- yeah, you have this nice interpretation for the filters.",
    "start": "1669525",
    "end": "1675615"
  },
  {
    "text": "Um, in general what this looks like, is- so in practice,",
    "start": "1675615",
    "end": "1681075"
  },
  {
    "text": "instead of one-dimensional two-dimensional, they're very high-dimensional volumes. Um, and so your filter is going to be a cube in the input space,",
    "start": "1681075",
    "end": "1692910"
  },
  {
    "text": "and you're sliding it around, and applying it at every place it can fit in this input, and then,",
    "start": "1692910",
    "end": "1700935"
  },
  {
    "text": "the reason why the output is also a volume, is because, um, you have multiple filters.",
    "start": "1700935",
    "end": "1707730"
  },
  {
    "text": "So over here for example, this blue filter is- when you slide it around an input,",
    "start": "1707730",
    "end": "1713985"
  },
  {
    "text": "it's gonna give you this, uh, like plane of outputs. But then you have a second filter, this green filter,",
    "start": "1713985",
    "end": "1721275"
  },
  {
    "text": "that you can also slide around the input, and that's gonna give you a second dimension to your hidden states.",
    "start": "1721275",
    "end": "1727840"
  },
  {
    "text": "Um, so Andrej Karpathy has this nice demo, where basically we have- so we have a three-dimensional input,",
    "start": "1728210",
    "end": "1737315"
  },
  {
    "text": "and we have two filters which are like, you can think of as little cubes, and it's sliding these cubes around the input,",
    "start": "1737315",
    "end": "1743960"
  },
  {
    "text": "and every application gives you one output in this,",
    "start": "1743960",
    "end": "1749125"
  },
  {
    "text": "um, like three-dimensional output volume. So this is, uh,",
    "start": "1749125",
    "end": "1755894"
  },
  {
    "text": "the same picture as before where you're sliding around cubes in order to fill in um,",
    "start": "1755895",
    "end": "1761040"
  },
  {
    "text": "you could say like layers of the output. Sliding around cubes to fill in these layers of the output.",
    "start": "1761040",
    "end": "1768670"
  },
  {
    "text": "Another thing people do is max pooling. So remember that interpretation of a filter as a,",
    "start": "1772880",
    "end": "1778740"
  },
  {
    "text": "as like a pattern detector. Um, what this is saying is you take a region in your input.",
    "start": "1778740",
    "end": "1785684"
  },
  {
    "text": "So you, you run your filters of the input. Get your, uh, like preliminary output,",
    "start": "1785685",
    "end": "1791295"
  },
  {
    "text": "and then you look at regions in the output and take the maximum activation and carry that on to successive layers.",
    "start": "1791295",
    "end": "1799890"
  },
  {
    "text": "An intuition there is that instead of, um, that you're looking- you're searching for a pattern in a region of the input.",
    "start": "1799890",
    "end": "1808680"
  },
  {
    "text": "And it's also helpful because remember at the end of the day we want to do classification or regression or something.",
    "start": "1808680",
    "end": "1814710"
  },
  {
    "text": "We wanna get this thing down to like a very small number of, um, numbers, and if we have this huge high dimensional volume,",
    "start": "1814710",
    "end": "1822120"
  },
  {
    "text": "then any way we can reduce its size is good.",
    "start": "1822120",
    "end": "1825820"
  },
  {
    "text": "Um, so this is an example of how these things work is,",
    "start": "1827210",
    "end": "1832770"
  },
  {
    "text": "is- it's they're pretty straightforward basically so you, you just have your convolutional layers, you stack them all up,",
    "start": "1832770",
    "end": "1838815"
  },
  {
    "text": "every once in awhile you have some pooling and you go down and down in dimensionality until you eventually get down to,",
    "start": "1838815",
    "end": "1846615"
  },
  {
    "text": "um, er, like a distribution over possible labels.",
    "start": "1846615",
    "end": "1852300"
  },
  {
    "text": "And this ties into what I was saying before about that Lego block analogy because this,",
    "start": "1852760",
    "end": "1858875"
  },
  {
    "text": "this entire network is built up of one, two, three, four different Lego blocks in a way,",
    "start": "1858875",
    "end": "1864870"
  },
  {
    "text": "and it's basically just stacking them on top of each other and composing them up in order to get a image classifier.",
    "start": "1864870",
    "end": "1872230"
  },
  {
    "text": "So I'm going to talk about three case studies of CNN architectures. So the first one, um, is AlexNet.",
    "start": "1873170",
    "end": "1879825"
  },
  {
    "text": "So this was that one that did really well in the ImageNet competition and really brought CNN's to the mainstream for computer vision.",
    "start": "1879825",
    "end": "1886845"
  },
  {
    "text": "Um, basically it was just a really big neural network.",
    "start": "1886845",
    "end": "1892260"
  },
  {
    "text": "Um, um, one trick they did was they used ReLU's instead of, um.",
    "start": "1892260",
    "end": "1899400"
  },
  {
    "text": "So the sigmoid that we've learned about I think we've. The sigmoid that we've learned about, um,",
    "start": "1899400",
    "end": "1907875"
  },
  {
    "text": "this is an activation function and it's gonna look something like this. And what they did was instead they use the ReLU,",
    "start": "1907875",
    "end": "1917110"
  },
  {
    "text": "um, which looks a little more like that,",
    "start": "1918200",
    "end": "1924570"
  },
  {
    "text": "and in practice it turns out to be a little easier to train and use.",
    "start": "1924570",
    "end": "1930429"
  },
  {
    "text": "Um, the next one is VGGNet, um, which did on ImageNet a couple years later.",
    "start": "1931820",
    "end": "1938040"
  },
  {
    "text": "Basically, it's, um, it's very similar, it's just a CNN. I think the thing to note about this one is that it's very uniform, um,",
    "start": "1938040",
    "end": "1946020"
  },
  {
    "text": "so it was 16 layers and there's nothing fancy in it. It was just a bunch of these Lego blocks stacked up.",
    "start": "1946020",
    "end": "1953790"
  },
  {
    "text": "Um, the entire network is pretty much, uh, like you- just by looking at this picture",
    "start": "1953790",
    "end": "1961800"
  },
  {
    "text": "you could probably re-implement their network. Um, something else to note about it is it started this trend of deeper,",
    "start": "1961800",
    "end": "1971399"
  },
  {
    "text": "of, of kind of like tall and skinny networks. So you'll notice that there's a lot of layers, but each layer is, uh, very thin.",
    "start": "1971400",
    "end": "1979299"
  },
  {
    "text": "And residual networks or ResNets, um, kinda takes that to the nth degree.",
    "start": "1979430",
    "end": "1985110"
  },
  {
    "text": "So the idea with a ResNet is, um, so most of the time you take your input,",
    "start": "1985110",
    "end": "1990450"
  },
  {
    "text": "you pass it through a matrix state and output. Um, if you add in your input again,",
    "start": "1990450",
    "end": "1997125"
  },
  {
    "text": "then that is very helpful because it makes it easy for the model to learn the identity function,",
    "start": "1997125",
    "end": "2004340"
  },
  {
    "text": "and so you can give the model the capacity for like 100 layers.",
    "start": "2004340",
    "end": "2009965"
  },
  {
    "text": "But if you add in these residual connections which is what you call it when you basically just like add in that,",
    "start": "2009965",
    "end": "2015980"
  },
  {
    "text": "add in x, um, then allows the model to skip a layer if it decides that that's what's best for itself.",
    "start": "2015980",
    "end": "2024180"
  },
  {
    "text": "You just set W to 0, that's what you would do. Um, so it also helps with training.",
    "start": "2025450",
    "end": "2038150"
  },
  {
    "text": "Um, back-propagation, if you take the derivative of the loss with respect to your input,",
    "start": "2038150",
    "end": "2043280"
  },
  {
    "text": "uh, that derivative is, is just going to be 1, um, for this part of the sum.",
    "start": "2043280",
    "end": "2048470"
  },
  {
    "text": "And so it gives- in a way you could think of it as it, as it gives the- it gives the error signal kinda like a highway through the network,",
    "start": "2048470",
    "end": "2056270"
  },
  {
    "text": "um, and it allows the gradients to propagate much deeper into these large neural networks.",
    "start": "2056270",
    "end": "2062569"
  },
  {
    "text": "Um, and so ResNet got a 3.6 % error on ImageNet. If you remember the AlexNet would-",
    "start": "2062570",
    "end": "2069514"
  },
  {
    "text": "it blew everyone off the water and it got like 15%. Uh, I think this is much better than human performance and, um,",
    "start": "2069515",
    "end": "2076294"
  },
  {
    "text": "it will come up later when we talk about- this idea of like residual connections will come up later when we talk about recurrent neural networks.",
    "start": "2076295",
    "end": "2084300"
  },
  {
    "text": "So just to summarize, uh, convolutional neural networks are often applied in image classification.",
    "start": "2086640",
    "end": "2096020"
  },
  {
    "text": "Um, the key idea is that there is- you have these filters which you are sliding around the input and that lets you one,",
    "start": "2096020",
    "end": "2104090"
  },
  {
    "text": "um, have- it does, uh, kind of this like idea of local connectivity.",
    "start": "2104090",
    "end": "2109535"
  },
  {
    "text": "So as a space in the ou- in the output only depends on a small patch of the input instead of the entire input.",
    "start": "2109535",
    "end": "2118010"
  },
  {
    "text": "And then second, um, it's- the parameters are shared.",
    "start": "2118010",
    "end": "2122880"
  },
  {
    "text": "Um, depth has turned out to really matter for these networks and I think to this day it's like people,",
    "start": "2124480",
    "end": "2131869"
  },
  {
    "text": "it's like every day there's just a deeper network that's coming out and people haven't really found a bound to depth I guess, yeah.",
    "start": "2131870",
    "end": "2139010"
  },
  {
    "text": "What's the best way to design one of these networks? Is it trial and error, um, but effectively you're trying to get out of some results or",
    "start": "2139010",
    "end": "2147290"
  },
  {
    "text": "is there any intuition as to how many layers, what the layer should be and so forth?",
    "start": "2147290",
    "end": "2152720"
  },
  {
    "text": "Yeah. So the question was how to design these things since there's- they seem so arbitrary. [LAUGHTER]. And yeah it is,",
    "start": "2152720",
    "end": "2158885"
  },
  {
    "text": "it is really arbitrary. I think, um, I think- so there's a few different ways.",
    "start": "2158885",
    "end": "2166250"
  },
  {
    "text": "So first, um, you start with something. Okay. So first, you would start with something that sounds reasonable,",
    "start": "2166250",
    "end": "2176465"
  },
  {
    "text": "um, and then you would do some kind of like a grid search or you would do, um,",
    "start": "2176465",
    "end": "2181625"
  },
  {
    "text": "now there's a literature on meta-learning which is where like you have a model, decide what your model looks like or- but in most cases you just kind of like hand-tune it,",
    "start": "2181625",
    "end": "2191705"
  },
  {
    "text": "you're like oh if I add a layer does it go up or down? Um, second, you look at the literature and say okay,",
    "start": "2191705",
    "end": "2198184"
  },
  {
    "text": "someone else solved a similar problem to me and they used network X, Y and Z and so I'm gonna start with that and then start fiddling from there.",
    "start": "2198184",
    "end": "2208055"
  },
  {
    "text": "Um, and then third is to literally take that network that's been pre-trained on an- a task and then apply it to yours.",
    "start": "2208055",
    "end": "2215060"
  },
  {
    "text": "Uh, we'll talk about it later but pre-training networks and applying that to your task has shown to be very helpful.",
    "start": "2215060",
    "end": "2223500"
  },
  {
    "text": "Okay. So now we're gonna talk about recurrent neural networks. The idea here is that you're modeling sequences of input.",
    "start": "2227410",
    "end": "2236540"
  },
  {
    "text": "Um, this could be things like text or sentences. It could also be things like time series data or financial data.",
    "start": "2236540",
    "end": "2244799"
  },
  {
    "text": "Um, and the recurrent neural network is something where the input,",
    "start": "2245470",
    "end": "2250940"
  },
  {
    "text": "um, it feeds its past inputs into itself, so it has time dependencies.",
    "start": "2250940",
    "end": "2256490"
  },
  {
    "text": "So for example, we have this very simple recurrent neural network here. Um, it is a function with one matrix and it",
    "start": "2256490",
    "end": "2265250"
  },
  {
    "text": "takes as arguments a past hidden state and a current input,",
    "start": "2265250",
    "end": "2271355"
  },
  {
    "text": "and then it predicts the next hidden state. So this, this is what it looks like if you were to write it in code or something.",
    "start": "2271355",
    "end": "2279875"
  },
  {
    "text": "This is what the actual network looks like. So there's an input and you feed that into your function as well as your current state,",
    "start": "2279875",
    "end": "2289790"
  },
  {
    "text": "and it just kind of loops on itself. Most of the time, people talk about this third perspective which is, uh,",
    "start": "2289790",
    "end": "2299660"
  },
  {
    "text": "taking kind of this network and I- and kind of like unraveling it across time.",
    "start": "2299660",
    "end": "2305180"
  },
  {
    "text": "I guess you can say unfolding it across time, um, where every time-step you have an input and you have a state,",
    "start": "2305180",
    "end": "2312925"
  },
  {
    "text": "and then you have your function which carries you to the next state. Yeah.",
    "start": "2312925",
    "end": "2319780"
  },
  {
    "text": "I'm just curious, how does it differ from having your original weight and",
    "start": "2319780",
    "end": "2327275"
  },
  {
    "text": "updating that weight because it sounds like that- that's a similar analogy. [inaudible] we have the previous state.",
    "start": "2327275",
    "end": "2334765"
  },
  {
    "text": "Yeah. But how is it compared to our machine [inaudible] classifier,",
    "start": "2334765",
    "end": "2340480"
  },
  {
    "text": "so we had the previous weights and we're just updating the previous weights. Oh I see. So the question was,",
    "start": "2340480",
    "end": "2346180"
  },
  {
    "text": "what's the difference between this and the setting before when we had- when we were, like, stochastic gradient descent where we were updating our weights sequentially.",
    "start": "2346180",
    "end": "2355930"
  },
  {
    "text": "Yeah. So, um, that is- that is an interesting question.",
    "start": "2355930",
    "end": "2361240"
  },
  {
    "text": "So the inf- difference is that before, for SGD, that was for,",
    "start": "2361240",
    "end": "2367030"
  },
  {
    "text": "um, it was sequential in the training whereas this is sequential in the inference.",
    "start": "2367030",
    "end": "2373660"
  },
  {
    "text": "So each- so you do- you feed in ten inputs,",
    "start": "2373660",
    "end": "2380230"
  },
  {
    "text": "let's say, ten timesteps as inputs. And then after all that time, then you back-propagate once for all those time steps.",
    "start": "2380230",
    "end": "2390325"
  },
  {
    "text": "So to make that more clear.",
    "start": "2390325",
    "end": "2394220"
  },
  {
    "text": "So for SGD, it's like you have x_1, y_1, x_2, y_2, and you use this to update w, right?",
    "start": "2397830",
    "end": "2408685"
  },
  {
    "text": "So you update w, and then you update w. And, yeah, that's- that's an interesting observation",
    "start": "2408685",
    "end": "2414880"
  },
  {
    "text": "that there's this kinda like time-dependency. Um, but there's no time within the data itself.",
    "start": "2414880",
    "end": "2420565"
  },
  {
    "text": "For, for the recurrent setting, it's, it's more like this. It's like, um, if,",
    "start": "2420565",
    "end": "2429250"
  },
  {
    "text": "uh, which marker is better? So it's more like this. It's more like you have x_1_1, x_1_2, x_1_3,",
    "start": "2429250",
    "end": "2438415"
  },
  {
    "text": "and y, and then x_2_1, x_2_2, and x_2_3 and y.",
    "start": "2438415",
    "end": "2447445"
  },
  {
    "text": "And then you use this to update w. And in this setting,",
    "start": "2447445",
    "end": "2454030"
  },
  {
    "text": "when we talk about time or temporal, like, when we talk about a sequence,",
    "start": "2454030",
    "end": "2459070"
  },
  {
    "text": "we're talking about a sequence here in the data, not necessarily in the learning.",
    "start": "2459070",
    "end": "2467800"
  },
  {
    "text": "Yeah. Okay. So to make- this a more concrete example.",
    "start": "2470300",
    "end": "2480445"
  },
  {
    "text": "We're gonna talk about a neural network language model. So this is a model that is in charge of sucking",
    "start": "2480445",
    "end": "2487960"
  },
  {
    "text": "in a sentence and predicting what is the most likely word that would come next in the sentence.",
    "start": "2487960",
    "end": "2494420"
  },
  {
    "text": "So we're- so each input,",
    "start": "2496140",
    "end": "2502480"
  },
  {
    "text": "we call x, and our hidden states, we call hs. And the way this works is we have some function that",
    "start": "2502480",
    "end": "2510010"
  },
  {
    "text": "takes x_1 and it encodes it into our hidden state. And then we have a second function that takes",
    "start": "2510010",
    "end": "2516040"
  },
  {
    "text": "the hidden state and decodes it into the next input. We continue by taking both the next both x_2, our next input,",
    "start": "2516040",
    "end": "2526675"
  },
  {
    "text": "and h_1, our previous hidden state, and then we use that to create a new hidden encoding.",
    "start": "2526675",
    "end": "2534265"
  },
  {
    "text": "And then, we take that new hidden encoding and decode it into our next input.",
    "start": "2534265",
    "end": "2539545"
  },
  {
    "text": "And we just rinse and repeat, each time we take the current input and",
    "start": "2539545",
    "end": "2545770"
  },
  {
    "text": "the previous hidden state to first create an encoding and then second predict our next input.",
    "start": "2545770",
    "end": "2553819"
  },
  {
    "text": "So there's those two steps.",
    "start": "2555480",
    "end": "2559340"
  },
  {
    "text": "The cool thing about this to note is that this now, we're building up vectors,",
    "start": "2564480",
    "end": "2570070"
  },
  {
    "text": "these h_is, and that's exactly what we're looking for. It's a vector that, in some way,",
    "start": "2570070",
    "end": "2576295"
  },
  {
    "text": "captures the meaning or a summary of all the x- all the inputs that we fed up until that time step.",
    "start": "2576295",
    "end": "2584200"
  },
  {
    "text": "So now, we have a vector which compresses all those inputs into one vector.",
    "start": "2584200",
    "end": "2591170"
  },
  {
    "text": "So to make this very concrete, one way you could build this thing is by basically,",
    "start": "2594140",
    "end": "2601845"
  },
  {
    "text": "each of these arrows, you stick a matrix in there. So our encode function would take the input, the x_t,",
    "start": "2601845",
    "end": "2610119"
  },
  {
    "text": "and multiply it by a matrix in order to get a vector and then it would take",
    "start": "2610120",
    "end": "2615205"
  },
  {
    "text": "the previous hidden state h_t minus 1 and multiply that by different a matrix to get a new vector,",
    "start": "2615205",
    "end": "2622435"
  },
  {
    "text": "and you add those vectors, and that gives you a vector which is your new hidden state.",
    "start": "2622435",
    "end": "2629420"
  },
  {
    "text": "And decode is the same thing, you take your hidden state, pass it through a matrix to a vector,",
    "start": "2629730",
    "end": "2635530"
  },
  {
    "text": "and then, um, send that through a Softmax to turn the vector of logits into a distribution of probabilities.",
    "start": "2635530",
    "end": "2643490"
  },
  {
    "text": "In general though, there's this problem with recurrent neural networks. So if there is a short dependency which mean the input and the output,",
    "start": "2646380",
    "end": "2658160"
  },
  {
    "text": "if an output depends on a recent input, then the path through this network is very short, right?",
    "start": "2658200",
    "end": "2666610"
  },
  {
    "text": "So it's easy for the gradients to reach where it needs to reach in order to train the network properly.",
    "start": "2666610",
    "end": "2671800"
  },
  {
    "text": "But if there's a very long dependency, then the gradients have a- have- they have difficulty getting all the way through.",
    "start": "2671800",
    "end": "2679520"
  },
  {
    "text": "You- so, uh, if you remember, we talked about, um, gradient descent as a credit-assignment problem where the gradient is in some ways,",
    "start": "2679650",
    "end": "2690010"
  },
  {
    "text": "like saying, how much, um, okay so if I change the in- if I change",
    "start": "2690010",
    "end": "2696100"
  },
  {
    "text": "this input by a small- if I perturbed by a small amount, how much will the output change? That's- in what's sense,",
    "start": "2696100",
    "end": "2702625"
  },
  {
    "text": "that what the gradient is saying. But if the input and output are super far away from each other, then it's very difficult to",
    "start": "2702625",
    "end": "2709494"
  },
  {
    "text": "compute how small perturbations the input would affect your output.",
    "start": "2709495",
    "end": "2714200"
  },
  {
    "text": "And the reason for that, we won't get into it so much,",
    "start": "2714690",
    "end": "2720505"
  },
  {
    "text": "but basically if you want to compute the gradient, then, what you have to do is you have to trace",
    "start": "2720505",
    "end": "2727330"
  },
  {
    "text": "the entire path of that dependency and you'd look at all the partial derivatives along that path,",
    "start": "2727330",
    "end": "2732445"
  },
  {
    "text": "and you multiply them all up. And so the problem is is that if the path is very long,",
    "start": "2732445",
    "end": "2737515"
  },
  {
    "text": "then you're multiplying a lot of numbers, and so if your numbers are less than 1,",
    "start": "2737515",
    "end": "2743350"
  },
  {
    "text": "then the- then the product, the overall product is gonna get really small and really fast, right?",
    "start": "2743350",
    "end": "2749050"
  },
  {
    "text": "And if the numbers are bigger than 1, then that product is going to blow up really quickly.",
    "start": "2749050",
    "end": "2754390"
  },
  {
    "text": "And so that is a problem because it means your gradients are going to be tiny and no learning signal,",
    "start": "2754390",
    "end": "2761215"
  },
  {
    "text": "or they're going to be way too big and you're going to just like shoot into some crazy direction that,",
    "start": "2761215",
    "end": "2768520"
  },
  {
    "text": "in practice, will like blow up your experiments and nothing will work. [LAUGHTER]. So it's a problem.",
    "start": "2768520",
    "end": "2773890"
  },
  {
    "text": "Um, So for the, the good thing is that for the exploding gradient problem that's not so bad, there's a quick fix.",
    "start": "2773890",
    "end": "2782020"
  },
  {
    "text": "What people do is, is, what they do is what's called clipping gradients. So you'll specify some norm.",
    "start": "2782020",
    "end": "2790645"
  },
  {
    "text": "You'll be like, any gradient with a norm bigger than 2, I'm going to clamp off at 2.",
    "start": "2790645",
    "end": "2798295"
  },
  {
    "text": "So if your gradients explode and they go to 10 million, you're gonna say, \"Okay, that's bigger than two.",
    "start": "2798295",
    "end": "2803890"
  },
  {
    "text": "So it was- it wasn't a million, it was actually two,\" and you go from there [LAUGHTER].",
    "start": "2803890",
    "end": "2809500"
  },
  {
    "text": "But for the vanishing gradient problem, here's this cool idea. The long short-term memory cell, uh,",
    "start": "2809500",
    "end": "2815680"
  },
  {
    "text": "which is similar to a recurrent neural network, um, but it has two hidden states.",
    "start": "2815680",
    "end": "2821050"
  },
  {
    "text": "And, um, so this is kind of a wall of equations,",
    "start": "2821050",
    "end": "2826195"
  },
  {
    "text": "but I think the important thing to note is that you're doing this,",
    "start": "2826195",
    "end": "2831745"
  },
  {
    "text": "this is basically like your input in a way, and this is kinda like your previous hidden state.",
    "start": "2831745",
    "end": "2837460"
  },
  {
    "text": "And so what's going on here is you're doing an additive combination, you're taking your input and you're adding in your previous hidden state,",
    "start": "2837460",
    "end": "2846550"
  },
  {
    "text": "very similarly to those residual connections in the ResNet. And so this- because you're adding in your previous state,",
    "start": "2846550",
    "end": "2855415"
  },
  {
    "text": "it's kinda like adding in your previous input, I guess you could say, and it allow- it gives the gradients, uh,",
    "start": "2855415",
    "end": "2862570"
  },
  {
    "text": "kind of like of a highway to very easily go back in time. Um, there's another perspective on this.",
    "start": "2862570",
    "end": "2870360"
  },
  {
    "text": "So this picture that in- the, uh, notation is different but, um,",
    "start": "2870360",
    "end": "2876155"
  },
  {
    "text": "I think the thing to note here is that this- so those, those are, are you could say,",
    "start": "2876155",
    "end": "2882310"
  },
  {
    "text": "are hidden states in this network. Or I guess- so in LSTM- so you co- I guess,",
    "start": "2882310",
    "end": "2888010"
  },
  {
    "text": "you could say that there's two hidden states. That's what people say. Um, so you have one hidden state",
    "start": "2888010",
    "end": "2894445"
  },
  {
    "text": "which is your HT and that's the state that you expose to the world. If you say, \"My LSTM is gonna have the same API as my RNN,",
    "start": "2894445",
    "end": "2903550"
  },
  {
    "text": "then this would be like the equivalent of that hidden state that we have for RNN. But then, you also have this internal hidden state- the C state,",
    "start": "2903550",
    "end": "2913150"
  },
  {
    "text": "that you never expose to the world. And so in this picture, um, the- sorry the notation is a little confusing but",
    "start": "2913150",
    "end": "2922525"
  },
  {
    "text": "the o in this picture corresponds to the h in the previous picture. So this is the hidden state that you are exposing to the world.",
    "start": "2922525",
    "end": "2930310"
  },
  {
    "text": "And then the s corresponds to c, so this is your internal hidden state.",
    "start": "2930310",
    "end": "2935710"
  },
  {
    "text": "And the thing to note about this picture is that s is just zipping around on what's called the constant error carousel.",
    "start": "2935710",
    "end": "2943810"
  },
  {
    "text": "And it's always internal and it's zipping around this thing in a loop. And so, um, what it ends up doing in practice is it ends up lear- like that- it's",
    "start": "2943810",
    "end": "2955210"
  },
  {
    "text": "a vector and it contains very long-term information that's useful for the network over many many time steps.",
    "start": "2955210",
    "end": "2963440"
  },
  {
    "text": "So if you, if you poke around at individual dimensions of that state,",
    "start": "2970170",
    "end": "2976599"
  },
  {
    "text": "then you an- then you can find these long term things being learned. So for example, Andrej Karpathy has a great blog post.",
    "start": "2976600",
    "end": "2983320"
  },
  {
    "text": "Um, you find networks that- you find units that track the length of the sentence,",
    "start": "2983320",
    "end": "2989395"
  },
  {
    "text": "you find units that track syntactic cues like quotes or brackets.",
    "start": "2989395",
    "end": "2995965"
  },
  {
    "text": "But in general, you find a lot of things that are just not easily interpretable. Uh.",
    "start": "2995965",
    "end": "3008155"
  },
  {
    "text": "So one last cool. I guess, idea that people have used with these recurrent neural networks",
    "start": "3008155",
    "end": "3014675"
  },
  {
    "text": "is sequence to sequence models, like machine translation. Um, which is where you have two sequences.",
    "start": "3014675",
    "end": "3020555"
  },
  {
    "text": "You have an input sequence, and an output sequence. And you want to suck in your input sequence and then spit out your output sequence.",
    "start": "3020555",
    "end": "3029375"
  },
  {
    "text": "And you do this with what's called the encoder decoder paradigm. You encode your sequence,",
    "start": "3029375",
    "end": "3036950"
  },
  {
    "text": "um, by giving it to your RNN, and that gives you a one vector which is encoding or compression of that input.",
    "start": "3036950",
    "end": "3043984"
  },
  {
    "text": "And then you decode your sequence by spitting out, um, your outputs just like we were talking about before with the language model.",
    "start": "3043985",
    "end": "3052590"
  },
  {
    "text": "And more recently, there's these attention-based models which are very",
    "start": "3057340",
    "end": "3063289"
  },
  {
    "text": "helpful in the case where there's long sequences. So if you look back here, x_1, x_1, x_2,",
    "start": "3063290",
    "end": "3069305"
  },
  {
    "text": "x_3 are all getting compressed into a single vector. Um, well if you have a really long paragraph,",
    "start": "3069305",
    "end": "3076490"
  },
  {
    "text": "maybe it's hard to shove that into your, you know, 200 dimensional vector.",
    "start": "3076490",
    "end": "3082535"
  },
  {
    "text": "It's hard to capture the depth of all that language with just a bunch of numbers.",
    "start": "3082535",
    "end": "3087859"
  },
  {
    "text": "Um, and so the idea behind attention is to, is to look back.",
    "start": "3087860",
    "end": "3093275"
  },
  {
    "text": "Um, so th- the way attention works is,",
    "start": "3093275",
    "end": "3100440"
  },
  {
    "text": "um, at a very high level is. So if you have your inputs,",
    "start": "3104350",
    "end": "3110225"
  },
  {
    "text": "we have x_1, x_2, and x_3. And we've run our RNN over these inputs.",
    "start": "3110225",
    "end": "3117019"
  },
  {
    "text": "And so we have three hidden state vectors, h_1, h_2, and h_3.",
    "start": "3117020",
    "end": "3126230"
  },
  {
    "text": "And now we're decoding. And so we have, we have our RNN decoder that has some hidden state.",
    "start": "3126230",
    "end": "3134974"
  },
  {
    "text": "We'll call it s_1. Um, what happens is you compare",
    "start": "3134975",
    "end": "3141110"
  },
  {
    "text": "your current hidden state with all of the states in your encoder,",
    "start": "3141110",
    "end": "3147920"
  },
  {
    "text": "and you compute a number that says how much do I like this state. So maybe, maybe it like really,",
    "start": "3147920",
    "end": "3154480"
  },
  {
    "text": "really likes this number, and it's not too happy about this num- this vector and it doesn't like this vector.",
    "start": "3154480",
    "end": "3163280"
  },
  {
    "text": "What it does is it, is it uses these scores to turn them into a distribution.",
    "start": "3164130",
    "end": "3171299"
  },
  {
    "text": "A probability distribution, that again says how much,",
    "start": "3172420",
    "end": "3177589"
  },
  {
    "text": "how much do I as s_1 like each of these vectors.",
    "start": "3177590",
    "end": "3183065"
  },
  {
    "text": "And then what you do, is you compute a weighted average of these hidden vectors [NOISE] where the weights come from this distribution.",
    "start": "3183065",
    "end": "3194400"
  },
  {
    "text": "And what this does is it serves two purposes. So first, um, and there,",
    "start": "3195100",
    "end": "3201515"
  },
  {
    "text": "there is another way of kind of writing this down on the slides, um. But I think this serves two purposes.",
    "start": "3201515",
    "end": "3208775"
  },
  {
    "text": "So first of all, it gives you some interpretation. So every time step you can see what parts of the input is it focusing on,",
    "start": "3208775",
    "end": "3217234"
  },
  {
    "text": "what parts of the inputs have a lot of probability mass on them. And then second, what it lets you do,",
    "start": "3217235",
    "end": "3222710"
  },
  {
    "text": "is it lets you, ah, it kind of releases the model from the pressure of having to put the entire input sequence into a single vector.",
    "start": "3222710",
    "end": "3230599"
  },
  {
    "text": "Now, what it can do is it can go- dynamically go back and retrieve the information it needs.",
    "start": "3230600",
    "end": "3236640"
  },
  {
    "text": "Um, and then more recently, uh, there's what's called transformer models,",
    "start": "3238390",
    "end": "3245494"
  },
  {
    "text": "um, which are, which do away entirely with the RNN aspect, it's just attention.",
    "start": "3245495",
    "end": "3254075"
  },
  {
    "text": "And with a transformer, you have your hidden states, and instead of, instead of having some kind of",
    "start": "3254075",
    "end": "3260990"
  },
  {
    "text": "decoder hidden state that you're comparing to the others. What you're doing is you just- you select each hidden state and you",
    "start": "3260990",
    "end": "3269420"
  },
  {
    "text": "compare h_1 to all the other h's including itself to get a number of how much,",
    "start": "3269420",
    "end": "3276125"
  },
  {
    "text": "um, h_1 likes those other h's. And then you compute your weighted average of all of these hidden states,",
    "start": "3276125",
    "end": "3284489"
  },
  {
    "text": "and that becomes [NOISE] your next layer.",
    "start": "3284489",
    "end": "3290900"
  },
  {
    "text": "Um, so I, I would recommend taking 224, and if you're interested in this topic, um,",
    "start": "3293640",
    "end": "3298685"
  },
  {
    "text": "transformers are very cool and they've recently become, I guess you could say like the new LSTM.",
    "start": "3298685",
    "end": "3304820"
  },
  {
    "text": "Um, so from these attention distributions you get cool interpretable maps, like in translation.",
    "start": "3304820",
    "end": "3311780"
  },
  {
    "text": "So this is an attention distribution and it points at words that are correspondent. So uh, like economic, um,",
    "start": "3311780",
    "end": "3318605"
  },
  {
    "text": "corresponds to economic and, and you can see that in the distribution.",
    "start": "3318605",
    "end": "3323609"
  },
  {
    "text": "Um, they also do this in computer vision. Um, you can highlight areas of a picture.",
    "start": "3327160",
    "end": "3333539"
  },
  {
    "text": "Yeah, so just to summarize. So recurrent neural networks, um, you can throw a sequence at them,",
    "start": "3334630",
    "end": "3339740"
  },
  {
    "text": "and they'll give you a vector. Um, there's this intuition, uh, that they are processing inputs sequentially kind of like a for loop.",
    "start": "3339740",
    "end": "3347165"
  },
  {
    "text": "But they have a problem with training where the gradients either blow up or they shrink very small.",
    "start": "3347165",
    "end": "3352490"
  },
  {
    "text": "So LSTMs are one way of mitigating this problem. Um, but they're not perfect,",
    "start": "3352490",
    "end": "3358955"
  },
  {
    "text": "they still have to shove information into one vector. And so the way people get around this is with attention based models,",
    "start": "3358955",
    "end": "3365480"
  },
  {
    "text": "where you dynamically go back into your input and retrieve the information you need as you need it.",
    "start": "3365480",
    "end": "3371090"
  },
  {
    "text": "[NOISE] Um,",
    "start": "3371090",
    "end": "3375930"
  },
  {
    "text": "so now we're gonna talk about, um, unsupervised learning.",
    "start": "3376300",
    "end": "3383850"
  },
  {
    "text": "Um, so like I said before,",
    "start": "3384790",
    "end": "3390110"
  },
  {
    "text": "neural networks, we got them to work well recently, and a lot of that is just because they need a lot of data.",
    "start": "3390110",
    "end": "3397055"
  },
  {
    "text": "Um, but if you're a smaller lab or if you don't have",
    "start": "3397055",
    "end": "3402575"
  },
  {
    "text": "enough money to basically pay for a data set or even if it's a hard problem that just there isn't a lot of data for it.",
    "start": "3402575",
    "end": "3408980"
  },
  {
    "text": "Um, there's a lot of cases where there isn't enough data to train these very, very large models with millions, billions of parameters.",
    "start": "3408980",
    "end": "3418835"
  },
  {
    "text": "Um, but on the other hand, there's tons of unlabeled data laying around. And you can download the whole Internet if you want.",
    "start": "3418835",
    "end": "3425135"
  },
  {
    "text": "Um, and there's kind of this real inspiration from us, as human beings.",
    "start": "3425135",
    "end": "3431525"
  },
  {
    "text": "Like, we are never given labeled data sets of, of what foods are edible and what foods are not edible.",
    "start": "3431525",
    "end": "3438800"
  },
  {
    "text": "Right. You just kind of, you absorb experiences from the world and then you use that to inform your future experiences,",
    "start": "3438800",
    "end": "3445925"
  },
  {
    "text": "and you're able to kind of like reason about it and make decisions. Um, so, uh, I'm gonna turn off my, okay.",
    "start": "3445925",
    "end": "3457985"
  },
  {
    "text": "So uh, yeah, so the first, I guess thing that we're gonna get into is auto-encoders. So the idea behind auto-encoders is that if you",
    "start": "3457985",
    "end": "3464780"
  },
  {
    "text": "have some information and you try to learn a compressed representation of that information that allows you to reconstruct it,",
    "start": "3464780",
    "end": "3471860"
  },
  {
    "text": "then presumably, um, you've done something useful.",
    "start": "3471860",
    "end": "3477155"
  },
  {
    "text": "So in neural network speak the way that works is you, ah, give it some kind of a vector,",
    "start": "3477155",
    "end": "3483064"
  },
  {
    "text": "and use pass that through an encoder, um, which gives you a hidden vector.",
    "start": "3483064",
    "end": "3489740"
  },
  {
    "text": "And then you pass that hidden vector through a decoder which you use to reconstruct your input.",
    "start": "3489740",
    "end": "3496619"
  },
  {
    "text": "And the implicit loss in most cases is you wanna take the difference.",
    "start": "3498730",
    "end": "3504755"
  },
  {
    "text": "Basically, you want your reconstructed input and the original input to be very similar.",
    "start": "3504755",
    "end": "3510089"
  },
  {
    "text": "Um, so just to motivate this, um, this isn't deep learning but principal component analysis is,",
    "start": "3511300",
    "end": "3518405"
  },
  {
    "text": "could be viewed as one of these encoder, decoders. And the idea behind principal component analysis is you",
    "start": "3518405",
    "end": "3525230"
  },
  {
    "text": "wanna come up with a matrix U, um, which can be used to both encode and decode a vector.",
    "start": "3525230",
    "end": "3535685"
  },
  {
    "text": "So you multiply x by U to give you a hidden vector or a new representation of your data.",
    "start": "3535685",
    "end": "3542615"
  },
  {
    "text": "But then if you transpose U and multiply it by your hidden vector then that should give you something as close as possible to your original data.",
    "start": "3542615",
    "end": "3551490"
  },
  {
    "text": "Um, so but there's a problem.",
    "start": "3554020",
    "end": "3559730"
  },
  {
    "text": "So as we- if we have a hidden vector with a bunch of units, then it's not gonna learn anything, right.",
    "start": "3559730",
    "end": "3567500"
  },
  {
    "text": "It's just gonna learn how to copy inputs into outputs. Um, and so a lot of research on auto-encoding",
    "start": "3567500",
    "end": "3576110"
  },
  {
    "text": "and this kind of unsupervised learning is about how to control the complexity,",
    "start": "3576110",
    "end": "3581240"
  },
  {
    "text": "and, and make the model robust enough to generate useful representations instead of just copying.",
    "start": "3581240",
    "end": "3589320"
  },
  {
    "text": "Um, so the first pass at that, can be using lawnmower transformations.",
    "start": "3589810",
    "end": "3596705"
  },
  {
    "text": "Um, so you would do something like the lo- logistic or the Sigmoid loss.",
    "start": "3596705",
    "end": "3602060"
  },
  {
    "text": "And that means that the problem can't be solved anymore by just copying into the output.",
    "start": "3602060",
    "end": "3608000"
  },
  {
    "text": "And so you're gonna have to actually learn something useful. [NOISE] Um, another way of doing it is by corrupting the input.",
    "start": "3608000",
    "end": "3619550"
  },
  {
    "text": "So you have your input and you noise it. Maybe you drop out some numbers from it. Maybe you perturb some numbers of it.",
    "start": "3619550",
    "end": "3626525"
  },
  {
    "text": "Maybe you add in, maybe draw from like a Gaussian and add that to your input, and then you pass that through.",
    "start": "3626525",
    "end": "3633569"
  },
  {
    "text": "Um, so yeah, so you could drop. So if your vector is 1, 2, 3, 4,",
    "start": "3634300",
    "end": "3641480"
  },
  {
    "text": "you could drop out 1 and 4, and just set them to 0, or you could slightly perturb these numbers so that they're close to their original but,",
    "start": "3641480",
    "end": "3651470"
  },
  {
    "text": "um, different, not the exact same. And then the idea is that after you pass this encoded input through",
    "start": "3651470",
    "end": "3659510"
  },
  {
    "text": "both your- after you pass this corrupted input through both your encoder and decoder,",
    "start": "3659510",
    "end": "3666619"
  },
  {
    "text": "then the output, the eventual x-hat should be very close to your original uncorrupted input.",
    "start": "3666619",
    "end": "3674150"
  },
  {
    "text": "[NOISE] Um. Yeah.",
    "start": "3674150",
    "end": "3684270"
  },
  {
    "text": "So another is a variational encoder which has a, um, cool comp probabilistic interpretation.",
    "start": "3684270",
    "end": "3691635"
  },
  {
    "text": "Um, so you could think of it as kind of a Bayesian network. Um, I- I think maybe this is more useful to look at.",
    "start": "3691635",
    "end": "3701820"
  },
  {
    "text": "So you have an encoder and a decoder and they are both modeling,",
    "start": "3701820",
    "end": "3707985"
  },
  {
    "text": "um, I guess probability distributions. So what this is saying is I want to encode x into a distribution over h's.",
    "start": "3707985",
    "end": "3719310"
  },
  {
    "text": "And you learn a function which is in charge of doing that. And then you want to specify some conditions.",
    "start": "3719310",
    "end": "3726615"
  },
  {
    "text": "First you say, \"Okay, I wanna make x recoverable from my h distribution.\"",
    "start": "3726615",
    "end": "3733619"
  },
  {
    "text": "And then second this is a term that kind of, um, it prevents h from being degenerate.",
    "start": "3733620",
    "end": "3742095"
  },
  {
    "text": "So maybe a good way of thinking about this",
    "start": "3742095",
    "end": "3749355"
  },
  {
    "text": "is instead of- so our traditional autoencoder would take my input and it would map it.",
    "start": "3749355",
    "end": "3759555"
  },
  {
    "text": "It would send it through some kind of encoder and map it into a hidden vector.",
    "start": "3759555",
    "end": "3766260"
  },
  {
    "text": "Send that through a decoder and that would reconstruct my input.",
    "start": "3766260",
    "end": "3771900"
  },
  {
    "text": "Whereas a variational autoencoder is gonna take my input and it's gonna map that into a distribution over possible h's.",
    "start": "3771900",
    "end": "3785505"
  },
  {
    "text": "And then what I'm gonna do is I'm going to sample from this distribution, pass that through my decoder and produce my reconstructed input.",
    "start": "3785505",
    "end": "3797400"
  },
  {
    "text": "Um, and the nice thing about this is that since this is a distribution instead of a vector,",
    "start": "3797400",
    "end": "3804165"
  },
  {
    "text": "you've imposed some structure on the space. So points that are close together in this space should map to similar x-hats.",
    "start": "3804165",
    "end": "3816270"
  },
  {
    "text": "And then similarly as you move through this space you shou- you should be able to gradually",
    "start": "3816270",
    "end": "3822089"
  },
  {
    "text": "transition from one I guess reconstructed input to a second.",
    "start": "3822090",
    "end": "3828230"
  },
  {
    "text": "So for example, there's these cool experiments in computer vision where they'll say, up here, this is gonna give me like a chair or something.",
    "start": "3828230",
    "end": "3837610"
  },
  {
    "text": "Um, and then down here it's gonna give me like a table.",
    "start": "3837610",
    "end": "3843390"
  },
  {
    "text": "And then if- if I move from one to the other, then- and you constantly decode then it'll like gradually",
    "start": "3843390",
    "end": "3849735"
  },
  {
    "text": "morph into the table. Um, it's really cool. Um, okay, and then- so then the last method of,",
    "start": "3849735",
    "end": "3863490"
  },
  {
    "text": "uh, of unsupervised learning that we're going to talk about is motivated by this task.",
    "start": "3863490",
    "end": "3869490"
  },
  {
    "text": "So there's this dataset called SQuAD, um, which is about 100,000 examples where each example consists of a paragraph,",
    "start": "3869490",
    "end": "3877439"
  },
  {
    "text": "and then a bunch of questions, uh, like multiple choice questions based on that paragraph.",
    "start": "3877439",
    "end": "3884770"
  },
  {
    "text": "Um, so the problem here is that there's only 100,000 examples. And really the intelligence that",
    "start": "3886730",
    "end": "3895589"
  },
  {
    "text": "this task is trying to get at is just can you read a text and understand it,",
    "start": "3895590",
    "end": "3901050"
  },
  {
    "text": "um, which is more general and is- is captured by more data than just these 100,000.",
    "start": "3901050",
    "end": "3906135"
  },
  {
    "text": "In particular, it's captured by just all the texts that you could possibly read.",
    "start": "3906135",
    "end": "3912750"
  },
  {
    "text": "So there's billions of words on Wikipedia, on Google. You can just crawl the web and download it.",
    "start": "3912750",
    "end": "3919155"
  },
  {
    "text": "And if somehow you could leverage that maybe that would be helpful for your reading comprehension.",
    "start": "3919155",
    "end": "3924585"
  },
  {
    "text": "Um, and that is just a perfect case of this- of this setting where we have tons of unlabeled data,",
    "start": "3924585",
    "end": "3932640"
  },
  {
    "text": "very small amount of data. Um, so recently the NLP community has come up with this idea called BERT.",
    "start": "3932640",
    "end": "3941940"
  },
  {
    "text": "Um, well, there's actually not just BERT, but there's a lot of people who are doing",
    "start": "3941940",
    "end": "3947760"
  },
  {
    "text": "similar things but BERT is the example we're talking about. With BERT what you do is you take a sentence and then",
    "start": "3947760",
    "end": "3954330"
  },
  {
    "text": "you mask out some of the tokens in the input. And then you train a model to fill in those tokens.",
    "start": "3954330",
    "end": "3962835"
  },
  {
    "text": "And they actually train the model on a bunch of things. So they trained it on token filling.",
    "start": "3962835",
    "end": "3968010"
  },
  {
    "text": "And then they also would glue two sentences together and- and ask the model are these sentences,",
    "start": "3968010",
    "end": "3974520"
  },
  {
    "text": "um, like would they be adjacent in the texts or not? Like do they make sense together or not?",
    "start": "3974520",
    "end": "3979950"
  },
  {
    "text": "Um, but the idea is basically like have- give a bunch of unlabeled text to",
    "start": "3979950",
    "end": "3986670"
  },
  {
    "text": "a model which is just going to kinda like manipulate that data in order to learn structure from it,",
    "start": "3986670",
    "end": "3993660"
  },
  {
    "text": "um, without any explicit purpose other than just learning the structure.",
    "start": "3993660",
    "end": "3999450"
  },
  {
    "text": "And they trained it on a bunch of data for a long time.",
    "start": "3999450",
    "end": "4005609"
  },
  {
    "text": "And then what you can do is once- so BERT is actually a big- so we talked about transformers before and BERT is like a big transformer.",
    "start": "4006280",
    "end": "4014404"
  },
  {
    "text": "So they trained this thing on a ton of unstructured texts, on just this word filling task for a long time.",
    "start": "4014405",
    "end": "4020450"
  },
  {
    "text": "And then what they did was they took their pre-trained BERT and they took the- and they started feeding it questions from SQuAD.",
    "start": "4020450",
    "end": "4028490"
  },
  {
    "text": "So they took questions and then they would glue on to it the paragraph,",
    "start": "4028490",
    "end": "4033830"
  },
  {
    "text": "the context that they used to answer the question and then they would take whatever vectors are coming out of BERT,",
    "start": "4033830",
    "end": "4040385"
  },
  {
    "text": "and they would pass that through a single matrix which basically predicts the answer for that SQuAD question.",
    "start": "4040385",
    "end": "4048545"
  },
  {
    "text": "Um, and it did really well. So this picture is right when BERT was released.",
    "start": "4048545",
    "end": "4055039"
  },
  {
    "text": "And these are all of the state of the art, um, models for SQuAD and BERT,",
    "start": "4055040",
    "end": "4063875"
  },
  {
    "text": "not only beat all the other models by a large margin, but also beat human performance.",
    "start": "4063875",
    "end": "4069420"
  },
  {
    "text": "And, um, I guess the- the intuition- the intuition behind BERT was",
    "start": "4069730",
    "end": "4077030"
  },
  {
    "text": "that by doing these seemingly trivial tasks like word filling and next sentence prediction,",
    "start": "4077030",
    "end": "4084350"
  },
  {
    "text": "what you end up learning is you end up learning the vectors that are coming out of this are vectors that say like,",
    "start": "4084350",
    "end": "4092555"
  },
  {
    "text": "what is the meaning of a word? Um, what is the meaning of a word in this context?",
    "start": "4092555",
    "end": "4098330"
  },
  {
    "text": "What is the meaning of this sentence? And that meaning isn't like operationalized towards solving any task in particular,",
    "start": "4098330",
    "end": "4107375"
  },
  {
    "text": "um, it's just like, in a very general sense, like what is- I'm going to imbue this model with",
    "start": "4107375",
    "end": "4112730"
  },
  {
    "text": "an understanding of language and then once I have an understanding of language, I'm going to then apply it to my very targeted downstream task.",
    "start": "4112730",
    "end": "4120810"
  },
  {
    "text": "Um, and that is- that is kind of the principle behind unsupervised learning.",
    "start": "4124540",
    "end": "4130775"
  },
  {
    "text": "So you kinda like make up these almost trivial prediction tasks, uh,",
    "start": "4130775",
    "end": "4135904"
  },
  {
    "text": "just to manipulate data and learn structure from it, understand language, understand what a picture is.",
    "start": "4135905",
    "end": "4142609"
  },
  {
    "text": "And then what you do is then you fine tune it on the very small amount of label data that you have.",
    "start": "4142610",
    "end": "4149600"
  },
  {
    "text": "Um, and that's kinda what the current state of the art",
    "start": "4149600",
    "end": "4154850"
  },
  {
    "text": "is in a lot of fields is basically just doing more and more unsupervised pre-training with bigger,",
    "start": "4154850",
    "end": "4160730"
  },
  {
    "text": "bigger models and bigger, bigger data. Um, and the field really hasn't found like a limit to this yet.",
    "start": "4160730",
    "end": "4167270"
  },
  {
    "text": "And it'll be interesting to see how far it goes. Um, so I'm going to skip those slides but just to kind of wrap things up, um,",
    "start": "4167270",
    "end": "4179435"
  },
  {
    "text": "recently I guess the biggest things that people have gotten to get neural networks working is;",
    "start": "4179435",
    "end": "4185600"
  },
  {
    "text": "one, better optimization algorithms. So we have these adaptive algorithms that are not as,",
    "start": "4185600",
    "end": "4191165"
  },
  {
    "text": "um, I would say like obtuse as SGD. It doesn't have to move in this- by the same amount every time.",
    "start": "4191165",
    "end": "4197735"
  },
  {
    "text": "You have a lot of tricks, like, uh, you know, fine tuning, unsupervised learning, clipping the gradients, batch norm.",
    "start": "4197735",
    "end": "4206030"
  },
  {
    "text": "Um, we have better hardware. We have better data and that allows us to experiment more and- and,",
    "start": "4206030",
    "end": "4215140"
  },
  {
    "text": "um, train larger models faster. Yeah. We're waiting a long time.",
    "start": "4215140",
    "end": "4222450"
  },
  {
    "text": "But I think- I think one of- maybe one of the problems with the field is that the theory is- is in a lot of ways lacking.",
    "start": "4222450",
    "end": "4229415"
  },
  {
    "text": "Um, and we don't know exactly why neural networks work well and why they're able to learn good functions despite having a very difficult,",
    "start": "4229415",
    "end": "4238909"
  },
  {
    "text": "um, like optimization surface.",
    "start": "4238910",
    "end": "4243360"
  },
  {
    "text": "Um, yeah. So just to summarize. We- we talked about a lot of different building blocks.",
    "start": "4244440",
    "end": "4251390"
  },
  {
    "text": "We talked about how to leverage spatial structure with convolutional neural networks. We talked about how to feed sequences into",
    "start": "4251390",
    "end": "4258800"
  },
  {
    "text": "recurrent neural networks and transformers, and LSTMs. We talked about, um, you know,",
    "start": "4258800",
    "end": "4263840"
  },
  {
    "text": "the sequence to sequence paradigm for machine translation and unsupervised learning methods that help you kinda",
    "start": "4263840",
    "end": "4270260"
  },
  {
    "text": "like jumpstart your downstream applications. Um, and I think the big takeaway here is that in-",
    "start": "4270260",
    "end": "4277565"
  },
  {
    "text": "in some ways the big advantage of a neural network is that they are compositional. So you- it's like a- it's like Legos.",
    "start": "4277565",
    "end": "4283730"
  },
  {
    "text": "You take- they take an input and they turn it into a vector. And then once you have a vector,",
    "start": "4283730",
    "end": "4289190"
  },
  {
    "text": "you can start combining these things in very flexible ways. And so in a lot of ways designing these things is a lot like putting together a Lego set.",
    "start": "4289190",
    "end": "4298445"
  },
  {
    "text": "You have your building blocks in LSTM, attention and coding, and you can decide how to- it's like,",
    "start": "4298445",
    "end": "4305329"
  },
  {
    "text": "\"Oh, I wanna run this LSTM here and this LSTM here. And then I want this one to attend over this one.",
    "start": "4305330",
    "end": "4310460"
  },
  {
    "text": "And then I'm going to concatenate the result with the output of this CNN.\" Um, and because of- because of I guess you could say like the magic about propagation,",
    "start": "4310460",
    "end": "4320315"
  },
  {
    "text": "you can combine these things. Um, and I think even more generally,",
    "start": "4320315",
    "end": "4326315"
  },
  {
    "text": "it allows you as a programmer to instead of make a program for solving a problem,",
    "start": "4326315",
    "end": "4332930"
  },
  {
    "text": "it allows you to make this scaffolding, um, that allows a computer to teach itself how to solve the problem.",
    "start": "4332930",
    "end": "4341120"
  },
  {
    "text": "So, um, instead of defining the function, you want the software to learn.",
    "start": "4341120",
    "end": "4347960"
  },
  {
    "text": "You define a very broad family of functions that the software is allowed to learn.",
    "start": "4347960",
    "end": "4353855"
  },
  {
    "text": "And then you let it go and run off and find the best match within that.",
    "start": "4353855",
    "end": "4359120"
  },
  {
    "text": "Uh, yeah. So those are all the things we're talking about today.",
    "start": "4359120",
    "end": "4365750"
  },
  {
    "text": "Uh, but, um, I hope you all have a good Thanksgiving break.",
    "start": "4365750",
    "end": "4371010"
  }
]