[
  {
    "start": "0",
    "end": "17000"
  },
  {
    "text": "Okay. Hi, everyone. Let's get started again. Okay, so first of all let me just say a bit about Assignment 5.",
    "start": "5480",
    "end": "17100"
  },
  {
    "start": "17000",
    "end": "184000"
  },
  {
    "text": "So Assignment 5 is coming out today. Um, it's a brand new assignment,",
    "start": "17100",
    "end": "22580"
  },
  {
    "text": "so you guys are the guinea pigs for that. Um, and so what it's going to be, it essentially builds on Assignment 4.",
    "start": "22580",
    "end": "31365"
  },
  {
    "text": "Um, so it's okay if you didn't do perfectly on Assignment 4, but I think actually most people did.",
    "start": "31365",
    "end": "36405"
  },
  {
    "text": "Um, and what we're gonna be doing is adding, um, convolutional neural networks and subword",
    "start": "36405",
    "end": "42290"
  },
  {
    "text": "modeling to the neural machine translation system, seeking it to make it better.",
    "start": "42290",
    "end": "47305"
  },
  {
    "text": "Um, so this assignment is coding heavy, written questions light.",
    "start": "47305",
    "end": "52850"
  },
  {
    "text": "Um, so I mean the coding that you have to do sort of isn't actually really more difficult than Assignment 4,",
    "start": "52850",
    "end": "60410"
  },
  {
    "text": "it's kind of like Assignment 4. But what we're hoping is that this time you will be able to do it on your own.",
    "start": "60410",
    "end": "68710"
  },
  {
    "text": "Now what I mean by that, um, is for Assignment 4. Well, there was tons of scaffolding telling you what everything should be,",
    "start": "68710",
    "end": "76640"
  },
  {
    "text": "and there were all of these auto-grader checks and you could keep on working on your code until they passed all the autograder checks, and everybody did.",
    "start": "76640",
    "end": "84979"
  },
  {
    "text": "Um, and so it was very kind of coddled, shall we say. Um, but I mean,",
    "start": "84980",
    "end": "91625"
  },
  {
    "text": "I guess what we're really wanting to achieve is to have a more- sorry, question.",
    "start": "91625",
    "end": "97160"
  },
  {
    "text": "[inaudible]? Yes. So what we're hoping is that this can be, uh, useful.",
    "start": "97160",
    "end": "103510"
  },
  {
    "text": "Um, it'll be short-term pain but useful as being a more effective ramp to doing",
    "start": "103510",
    "end": "111350"
  },
  {
    "text": "the final project and indeed for the rest of your life, right. And the- the reality is that in the rest of your life,",
    "start": "111350",
    "end": "118220"
  },
  {
    "text": "you sort of if you're going to be doing things with deep learning, you kind of have to work out what kind of",
    "start": "118220",
    "end": "123920"
  },
  {
    "text": "model to build and which pieces to stitch together, and how to write some tests to see if it's doing something sensible.",
    "start": "123920",
    "end": "131090"
  },
  {
    "text": "And if it's not doing something sensible, um, to figure out how you could change things and try different things,",
    "start": "131090",
    "end": "137508"
  },
  {
    "text": "and get it to work sensibly. And so that's what we're hoping, um, that people, um,",
    "start": "137509",
    "end": "142730"
  },
  {
    "text": "can do in Assignment 5, so you've got to, um, figure things out.",
    "start": "142730",
    "end": "148090"
  },
  {
    "text": "Um, should write your own testing code. Um, we don't have a public autograder,",
    "start": "148090",
    "end": "154489"
  },
  {
    "text": "so you should- that's part of working out your own sanity checks, trying to do things like what I talked about last week of sort of getting",
    "start": "154490",
    "end": "164150"
  },
  {
    "text": "simple bits working, confirming that they work on minute amounts of test data and so on, and doing things more sensibly.",
    "start": "164150",
    "end": "171620"
  },
  {
    "text": "I mean in particular, um, the one particular part of that,",
    "start": "171620",
    "end": "179100"
  },
  {
    "text": "that we were planning to do, um, for, um, this assignment, I was looking for it,",
    "start": "179100",
    "end": "186015"
  },
  {
    "start": "184000",
    "end": "232000"
  },
  {
    "text": "um, but it's on the next slide. Um, so, um, for this assignment and beyond, um,",
    "start": "186015",
    "end": "191620"
  },
  {
    "text": "we're going to enforce rules like more like they are in CS107, for those of you who are undergrads,",
    "start": "191620",
    "end": "198129"
  },
  {
    "text": "meaning that the TAs don't look at and debug your code for you. Um, and so, you know,",
    "start": "198130",
    "end": "204860"
  },
  {
    "text": "of course we still want TAs to be helpful, come to them with your problems, um,",
    "start": "204860",
    "end": "209930"
  },
  {
    "text": "talk about how you're meant to use different things, um, in the PyTorch library,",
    "start": "209930",
    "end": "215420"
  },
  {
    "text": "um, but you shouldn't be regarding it as the TA's job of, here's a big Python file.",
    "start": "215420",
    "end": "221300"
  },
  {
    "text": "Um, can you tell me what's wrong with it, and fix it up for you. Okay. Um, the precise policy for that's,",
    "start": "221300",
    "end": "229485"
  },
  {
    "text": "um, written up on Piazza. Okay. So after- any questions about that or do I go straight on in?",
    "start": "229485",
    "end": "238129"
  },
  {
    "start": "232000",
    "end": "370000"
  },
  {
    "text": "Okay. Um, yes so today's lecture,",
    "start": "239580",
    "end": "245145"
  },
  {
    "text": "um, in some sense today's lecture is an easy lecture.",
    "start": "245145",
    "end": "250650"
  },
  {
    "text": "Um, so last time's lecture, there was really sort of a ton of new stuff",
    "start": "250650",
    "end": "256420"
  },
  {
    "text": "of other stuff on neural networks that you haven't seen before, and we did Convnets and we did pooling layers,",
    "start": "256420",
    "end": "263410"
  },
  {
    "text": "and we did highway and residual connections, and batch norms, and I don't know, whatever else we did.",
    "start": "263410",
    "end": "269970"
  },
  {
    "text": "Um, size one convolutions I guess. So there are tons of new stuff really",
    "start": "269970",
    "end": "276110"
  },
  {
    "text": "in this lecture in terms of sort of neural network machinery, there isn't any new stuff at all.",
    "start": "276110",
    "end": "281300"
  },
  {
    "text": "So this is really easy. Um, and this is also really a new lecture but it was sort of put in for a reason.",
    "start": "281300",
    "end": "288875"
  },
  {
    "text": "And the reason for this relates to a kinda remark I made last time about how lots of stuff keeps changing in neural network land.",
    "start": "288875",
    "end": "298700"
  },
  {
    "text": "So at the time we first designed this class and the way as- that a lot of the structure of it still is.",
    "start": "298700",
    "end": "305090"
  },
  {
    "text": "Um, that sort of around 2014-2015 when we designed this class,",
    "start": "305090",
    "end": "310830"
  },
  {
    "text": "it was basically axiomatic that all deep learning models for natural language processing worked off words.",
    "start": "310830",
    "end": "318185"
  },
  {
    "text": "And therefore it completely made sense that we start with word vectors, and then we start looking at things like recurrent models over words.",
    "start": "318185",
    "end": "326169"
  },
  {
    "text": "Whereas the fact of the matter is in the last approximately three years,",
    "start": "326170",
    "end": "331580"
  },
  {
    "text": "there's been a ton of new work including some of the most influential new work. There's building language models that aren't- isn't -isn't aren't, um,",
    "start": "331580",
    "end": "340960"
  },
  {
    "text": "being built over words that they're building, built over pieces of words or characters.",
    "start": "340960",
    "end": "346895"
  },
  {
    "text": "And so this lecture is sort of meant to give you some sense of these other ways of doing things and,",
    "start": "346895",
    "end": "352985"
  },
  {
    "text": "um, some orientation to some of the things that's going on. But the actual kind of models that we're looking at, uh,",
    "start": "352985",
    "end": "359930"
  },
  {
    "text": "sort of using all of the building blocks that we've already looked at, things like, um, RNNs and ConvNets and things like that.",
    "start": "359930",
    "end": "368030"
  },
  {
    "text": "So let's get into this. Um, so I'm going to start off with a teeny bit",
    "start": "368030",
    "end": "373130"
  },
  {
    "start": "370000",
    "end": "659000"
  },
  {
    "text": "of linguistics of learning about the structure of language, um, at first sort of lower level units of language and then we'll see how that pans out,",
    "start": "373130",
    "end": "382520"
  },
  {
    "text": "um, for things like character level models. So in linguistics, um, if you start at the bottom of the totem pole,",
    "start": "382520",
    "end": "390980"
  },
  {
    "text": "the first-level of linguistics is phonetics, which is sort of understanding the sounds and the physiology of human speech.",
    "start": "390980",
    "end": "398420"
  },
  {
    "text": "So that's sort of like physics, or physiology, or something, right, there are mouth parts that move,",
    "start": "398420",
    "end": "404354"
  },
  {
    "text": "there are ear parts that act as filters, and there's, um, audio waves in between the two of them.",
    "start": "404355",
    "end": "410030"
  },
  {
    "text": "So that's kind of uncontroversial in some sense. Um, but above that level,",
    "start": "410030",
    "end": "415419"
  },
  {
    "text": "the standard thing that people do for the analysis of human languages is to say,",
    "start": "415420",
    "end": "420460"
  },
  {
    "text": "well human languages may seem to make use of a relatively small set of",
    "start": "420460",
    "end": "426229"
  },
  {
    "text": "distinctive units which are then commonly called phonemes which are actually categorical.",
    "start": "426230",
    "end": "432320"
  },
  {
    "text": "And the idea here is that well, uh, our mouths are continuous spaces, right.",
    "start": "432320",
    "end": "439890"
  },
  {
    "text": "That they've got these various bits of their mouths like, you know, tongues and pharynges and so on, but it's a continuous space.",
    "start": "439890",
    "end": "447020"
  },
  {
    "text": "So actually, um, we can make an infinite variety of sounds, right. So if I open my mouth and apply voicing and just wiggle my tongue around, I can go [NOISE].",
    "start": "447020",
    "end": "457830"
  },
  {
    "text": "And I can make an infinite variety of different sounds. But the reality is that human languages aren't like that,",
    "start": "457830",
    "end": "466990"
  },
  {
    "text": "that out of that infinite variety of sounds, we distinguish a small space of sounds.",
    "start": "466990",
    "end": "473230"
  },
  {
    "text": "Um, and something that happens when languages change is, um,",
    "start": "473230",
    "end": "478570"
  },
  {
    "text": "that the space of sounds that are seen as important and distinguished in a language change.",
    "start": "478570",
    "end": "485365"
  },
  {
    "text": "And that happens even with inside one language as- as English.",
    "start": "485365",
    "end": "490569"
  },
  {
    "text": "And I'm about to give an example of that. Um, so people in cog psych talk about the phenomenon of categorical perception.",
    "start": "490570",
    "end": "500630"
  },
  {
    "text": "And what that means is that really there's something continuous, but that humans perceive it as belonging to fairly sharp categories.",
    "start": "500630",
    "end": "510410"
  },
  {
    "text": "Um, and, you know, you can use that for sort of, you know, styles of clothing or whether someone counts as fat or not.",
    "start": "510410",
    "end": "518349"
  },
  {
    "text": "Um, but the most famous examples of categorical perception are in language,",
    "start": "518350",
    "end": "523745"
  },
  {
    "text": "where we can make an infinite variety of sounds but people per- perceive them as categories.",
    "start": "523745",
    "end": "530435"
  },
  {
    "text": "And so effectively what that means is when you have categorical perception,",
    "start": "530435",
    "end": "535775"
  },
  {
    "text": "the differences within a category are sort of perceived to have shrunk. You barely notice them at all where differences",
    "start": "535775",
    "end": "543700"
  },
  {
    "text": "across categories are expanded and very clear. And so one of the cases that sort of studied",
    "start": "543700",
    "end": "549850"
  },
  {
    "text": "a lot is what's called- referred to as sort of, um, voice onset time. So lots of languages including English have pairs of sounds like p and b, uh,",
    "start": "549850",
    "end": "559930"
  },
  {
    "text": "pah and bah, and they differ based on when voicing starts.",
    "start": "559930",
    "end": "565149"
  },
  {
    "text": "So buh, it has a voice sound like a vowel with an r in it. And well that's a continuous parameter,",
    "start": "565150",
    "end": "571509"
  },
  {
    "text": "you can sort of make any point along a spectrum between a p and a b but, um,",
    "start": "571510",
    "end": "576835"
  },
  {
    "text": "human beings, um, who speak English, um, perceive just two points on that spectrum.",
    "start": "576835",
    "end": "583010"
  },
  {
    "text": "And you don't sort of really notice the fine differences between them. Um, some languages distinguish more points on the spectrum.",
    "start": "583010",
    "end": "590380"
  },
  {
    "text": "So Thai distinguishes three different consonant sounds, um, in depending on the voice onset time.",
    "start": "590380",
    "end": "596810"
  },
  {
    "text": "Um, something that might be, um, more accessible to you is, um, this is an example of language change.",
    "start": "596810",
    "end": "603920"
  },
  {
    "text": "So for a speaker like me, um, there was caught and cot and those are different vowels,",
    "start": "603920",
    "end": "610480"
  },
  {
    "text": "and I hear them as different vowels. But if you are someone who grew up in the southwest of the United States,",
    "start": "610480",
    "end": "617455"
  },
  {
    "text": "um, then these are exactly the same vowel and you don't distinguish them. Then- you thought I said the same thing twice even though I'm saying two different vowels.",
    "start": "617455",
    "end": "626390"
  },
  {
    "text": "And so that's where in even at a dialectal issue- level that people develop",
    "start": "626390",
    "end": "634120"
  },
  {
    "text": "categorical perception as to which- which distinctions and sounds they're sensitive to or not sensitive to.",
    "start": "634120",
    "end": "643399"
  },
  {
    "text": "Okay. And summing- and, I mean why I'm mentioning this is in some senses these sound distinctions of",
    "start": "643950",
    "end": "651370"
  },
  {
    "text": "categorical sound distinctions that are what a lot of our language writing systems that we'll come to in a minute record.",
    "start": "651370",
    "end": "658975"
  },
  {
    "text": "Okay. Um, so in traditional linguistics, um, you have sounds, but sounds don't have any meanings in language.",
    "start": "658975",
    "end": "668195"
  },
  {
    "start": "659000",
    "end": "909000"
  },
  {
    "text": "So pah and bah don't have meanings, and a and e don't have meanings. And so people then normally distinguish as",
    "start": "668195",
    "end": "675875"
  },
  {
    "text": "the next level up morphology is parts of words. And this is seen as the minimal level that has meaning.",
    "start": "675875",
    "end": "683540"
  },
  {
    "text": "And so the idea is lots of words are complex and can be made u- made up of pieces but these pieces do have meanings.",
    "start": "683540",
    "end": "691660"
  },
  {
    "text": "So fortune has a meaning, um, fortunate, you end in this ate ending,",
    "start": "691660",
    "end": "698805"
  },
  {
    "text": "um, which sort of gives the- gives fortune to somebody. So that means you know, having fortune, um,",
    "start": "698805",
    "end": "705405"
  },
  {
    "text": "that has a meaning, un has a meaning which means to reverse that. So unfortunate means that you don't have fortune, and ly, um,",
    "start": "705405",
    "end": "714195"
  },
  {
    "text": "then has a meaning of turning this all into an adverb, and you can say unfortunately not having,",
    "start": "714195",
    "end": "719825"
  },
  {
    "text": "um, gotten fortune, something happened. And so these sort of pieces of, um, words are then the minimal things that have meaning.",
    "start": "719825",
    "end": "727910"
  },
  {
    "text": "Um, almost no work in deep learning has tried to make use of this sort of morpheme level of structure.",
    "start": "727910",
    "end": "733480"
  },
  {
    "text": "Actually me and a couple of students six years ago did actually try and build a system where it built these tree structured neural networks,",
    "start": "733480",
    "end": "742514"
  },
  {
    "text": "that put together meanings of words out of their pieces. Um, but that really isn't an idea that's taken on widely.",
    "start": "742515",
    "end": "749875"
  },
  {
    "text": "There's sort of a reason why it hasn't taken on widely, which is doing this and working out the semantically meaningful pieces of words,",
    "start": "749875",
    "end": "758490"
  },
  {
    "text": "is kind of hard and a lot of the time in NLP what people have found",
    "start": "758490",
    "end": "764925"
  },
  {
    "text": "is you can just about get the same kind of results if you just work with character n-grams.",
    "start": "764925",
    "end": "772090"
  },
  {
    "text": "The kind of units that you put into the convolutional neural net. Because if you just have a model that uses, um,",
    "start": "772090",
    "end": "779105"
  },
  {
    "text": "character trigrams and you have sort of start of word, un, and nfo, and so on.",
    "start": "779105",
    "end": "785045"
  },
  {
    "text": "For going through the ly end of word, that those different units.",
    "start": "785045",
    "end": "790840"
  },
  {
    "text": "There's different character trigrams, in a distributed way will pick up all the important meaning components of the word pretty well,",
    "start": "790840",
    "end": "799765"
  },
  {
    "text": "and that that's just good enough. And that's actually a very classic idea that's sort of been revived.",
    "start": "799765",
    "end": "807035"
  },
  {
    "text": "Um, so back in the second coming of neural networks in the mid-80s into the early 90s, um,",
    "start": "807035",
    "end": "815045"
  },
  {
    "text": "there was qu- um, there was quite a bit of sort of controversial work on",
    "start": "815045",
    "end": "821440"
  },
  {
    "text": "the structure of language and in particular Dave Rumelhart and Jay McClelland. So Jay McClelland's still in the psych department here,",
    "start": "821440",
    "end": "828500"
  },
  {
    "text": "if you want to look him up in your spare time. Um, they proposed a model of how to model generating past tense forms in English.",
    "start": "828500",
    "end": "837459"
  },
  {
    "text": "So this was sort of a cog-psych experiment of can we build a system that can learn past tenses of English verbs?",
    "start": "837460",
    "end": "844045"
  },
  {
    "text": "And the difficult part there is some, many verbs are regular, you add the kinda -ed ending,",
    "start": "844045",
    "end": "849740"
  },
  {
    "text": "but some words are irregular and you had to sort of learn about the irregular patterning. Um, but the way they did that.",
    "start": "849740",
    "end": "857015"
  },
  {
    "text": "I mean partly because this was sort of early days with respect to, um, sequence models,",
    "start": "857015",
    "end": "863530"
  },
  {
    "text": "is that they used a representation where they represented words precisely with these sort of character trigrams.",
    "start": "863530",
    "end": "871255"
  },
  {
    "text": "And that was the representation of words that they used and fed forward in their model. And that, um, idea was met with a lot of controversy by",
    "start": "871255",
    "end": "880220"
  },
  {
    "text": "linguists, philosophers, and other people with their ideas of language and so there was a lot of debate in those days about that.",
    "start": "880220",
    "end": "887074"
  },
  {
    "text": "But from a- as a purely engineering solution that sort of proved to be a pretty good way to do things.",
    "start": "887075",
    "end": "894070"
  },
  {
    "text": "And so this decade there's been other work which includes the model, um,",
    "start": "894070",
    "end": "899245"
  },
  {
    "text": "developed at Microsoft of a sort of a deep, um, semantics model where what they're using as these kind of",
    "start": "899245",
    "end": "905860"
  },
  {
    "text": "character n-grams to put meaning over words. Okay so, um, so now we might be interested in building models that aren't over words.",
    "start": "905860",
    "end": "917195"
  },
  {
    "text": "So we're going to have a word written as characters and we're going to do something with it such as build character n-grams.",
    "start": "917195",
    "end": "925355"
  },
  {
    "text": "And so something that is just useful, um, to know is there's actually",
    "start": "925355",
    "end": "931010"
  },
  {
    "text": "a fair amount of variation between languages when you do this. So it's not all the same stuff, right?",
    "start": "931010",
    "end": "936199"
  },
  {
    "text": "So the first problem is, um, there are some languages that don't put spaces between words.",
    "start": "936200",
    "end": "942455"
  },
  {
    "text": "The most famous example is Chinese. Um, but an interesting fact for those people of European ancestry, um,",
    "start": "942455",
    "end": "951545"
  },
  {
    "text": "is that you know if- for- when the ancient Greeks wrote ancient Greek,",
    "start": "951545",
    "end": "956630"
  },
  {
    "text": "um, they didn't put spaces between words either. It was actually a later invention of",
    "start": "956630",
    "end": "962390"
  },
  {
    "text": "medieval scholars who are recopying their manuscripts, who they decided [NOISE] maybe that'll be easier to read if we",
    "start": "962390",
    "end": "968690"
  },
  {
    "text": "put spaces in and then they started doing it. Um, [NOISE] most languages these days do put spaces in between words but even,",
    "start": "968690",
    "end": "980000"
  },
  {
    "text": "then there's sort of a lot of fine cases. So in particular, a lot of languages have some sort of little bits",
    "start": "980000",
    "end": "986930"
  },
  {
    "text": "of stuff which might be pronouns, or prepositions, or various kind of joining words like and, and so,",
    "start": "986930",
    "end": "996380"
  },
  {
    "text": "and which sometimes they write together and sometimes separately.",
    "start": "996380",
    "end": "1002050"
  },
  {
    "text": "So in French, um, you get these kind of prepositional, I'm sorry,",
    "start": "1002050",
    "end": "1009865"
  },
  {
    "text": "pronominal, um, markers for you, I, you, have brought.",
    "start": "1009865",
    "end": "1015865"
  },
  {
    "text": "Um, and you know, these kind of little words and pronunciation just sort of run together as je vous ai,",
    "start": "1015865",
    "end": "1021940"
  },
  {
    "text": "and arguably it's almost one word, but it's written as separate words.",
    "start": "1021940",
    "end": "1027324"
  },
  {
    "text": "Um, where there are other languages which sort of stick things together, where arguably they're separate words.",
    "start": "1027325",
    "end": "1033579"
  },
  {
    "text": "So in Arabic, you get pronominal clitics and some of these sort of joining words like so,",
    "start": "1033580",
    "end": "1039549"
  },
  {
    "text": "and and, and they are sort of written together as one word, where arguably that they should really be four words.",
    "start": "1039550",
    "end": "1047485"
  },
  {
    "text": "Another famous case of that is with compound nouns. Um, so in English,",
    "start": "1047485",
    "end": "1054025"
  },
  {
    "text": "we write compound nouns with spaces between them, so you can see each noun. Um, even though in many respects compound nouns",
    "start": "1054025",
    "end": "1062289"
  },
  {
    "text": "something like whiteboard behaves like it's one word, or high-school. Um, whereas other languages,",
    "start": "1062290",
    "end": "1068770"
  },
  {
    "text": "German is the most famous case, but also other Germanic languages, just write them all as one word and you get very long words like that.",
    "start": "1068770",
    "end": "1077470"
  },
  {
    "text": "So we can get different words if we just use spaces and don't do much else. Um, good.",
    "start": "1077470",
    "end": "1083679"
  },
  {
    "start": "1082000",
    "end": "1282000"
  },
  {
    "text": "Okay. Yes. So for dealing with words, there are these practical problems.",
    "start": "1083680",
    "end": "1090175"
  },
  {
    "text": "Um, and we sort of already started to touch on them that if you're trying to build word-based models,",
    "start": "1090175",
    "end": "1096339"
  },
  {
    "text": "there's this huge space of words, and well, strictly there's an infinite space of words",
    "start": "1096339",
    "end": "1101740"
  },
  {
    "text": "because once you allow in things like numbers, let alone FedEx routing numbers, or, um,",
    "start": "1101740",
    "end": "1108985"
  },
  {
    "text": "or if you allow just morphology, when you can make those ones like unfortunately.",
    "start": "1108985",
    "end": "1114175"
  },
  {
    "text": "Yeah, sort of, you can just expand the space of words, so you get this large open vocabulary.",
    "start": "1114175",
    "end": "1120264"
  },
  {
    "text": "Um, English, you know, a bit problematic. It gets way more problematic than a lot of other languages.",
    "start": "1120265",
    "end": "1126400"
  },
  {
    "text": "So here's a lovely Czech word, um, to the worst farmable one, um,",
    "start": "1126400",
    "end": "1131635"
  },
  {
    "text": "where you can make sort of much more complex words in lots of other languages. Um, many Native American languages,",
    "start": "1131635",
    "end": "1139150"
  },
  {
    "text": "other European languages like Finnish have these sort of very complex words, Turkish has very complex words.",
    "start": "1139150",
    "end": "1145570"
  },
  {
    "text": "Um, so that's bad news. Um, there are other more reasons we'd like to be able to look at words below the word level,",
    "start": "1145570",
    "end": "1152409"
  },
  {
    "text": "to know things about them. So when you're translating, there's a wide space of things,",
    "start": "1152410",
    "end": "1158110"
  },
  {
    "text": "especially names, where translation is essentially transliteration,",
    "start": "1158110",
    "end": "1163405"
  },
  {
    "text": "that you're going to rewrite the sound of somebody's name as just roughly, you know,",
    "start": "1163405",
    "end": "1169015"
  },
  {
    "text": "perhaps not perfectly correctly but roughly correctly according to the sound systems of the different language.",
    "start": "1169015",
    "end": "1174730"
  },
  {
    "text": "And well, if we want to do that, we essentially want to work- operate at the letter level, not the word level.",
    "start": "1174730",
    "end": "1180310"
  },
  {
    "text": "But another huge modern reason why we'd like to start modeling below the word level is,",
    "start": "1180310",
    "end": "1186340"
  },
  {
    "text": "we live in this age of social media and if you're in the social media land,",
    "start": "1186340",
    "end": "1191350"
  },
  {
    "text": "there's a lot of stuff that's written not using the canonical words that you find in the dictionary,",
    "start": "1191350",
    "end": "1196975"
  },
  {
    "text": "and somehow we'd wanna start, um, to model that. So in some sense this is the, um,",
    "start": "1196975",
    "end": "1202180"
  },
  {
    "text": "easy case. Um, good vibes. Um, but nevertheless this is spelled with one,",
    "start": "1202180",
    "end": "1208720"
  },
  {
    "text": "two, three, four, five, six, seven O's, and one, two, three, four, five, oh and also seven S's, they match.",
    "start": "1208720",
    "end": "1215875"
  },
  {
    "text": "I don't know if that's deliberate or not [LAUGHTER]. Um, okay. So this sty le of writing is very common, um, and well,",
    "start": "1215875",
    "end": "1224905"
  },
  {
    "text": "you know, we kind of sunk if we're treating things at the word level and we're trying to model this right.",
    "start": "1224905",
    "end": "1231100"
  },
  {
    "text": "That's clearly not what human beings are doing, we're sort of looking at the characters and recognizing what goes on.",
    "start": "1231100",
    "end": "1238674"
  },
  {
    "text": "Um, in some sense that's kind of the easy case that you could imagine preprocessing out.",
    "start": "1238675",
    "end": "1245290"
  },
  {
    "text": "Um, there's a lot of harder stuff that then turns up. Um, I guess there's sort of the abbreviation speak, like I don't care.",
    "start": "1245290",
    "end": "1253645"
  },
  {
    "text": "Um, but then you sort of get a lot of creative spellings, um, that come off of kind of reduced pronunciations like I'mma go, sumn.",
    "start": "1253645",
    "end": "1264565"
  },
  {
    "text": "Um, and it seems like somehow we need something other than canonical words if we're going to start to deal better with a lot of this text.",
    "start": "1264565",
    "end": "1273264"
  },
  {
    "text": "Oops. Okay. So that suggests we sort of want to start doing that with our models.",
    "start": "1273265",
    "end": "1282070"
  },
  {
    "start": "1282000",
    "end": "1462000"
  },
  {
    "text": "And so, that's led to a lot of interest in using character level models.",
    "start": "1282070",
    "end": "1288054"
  },
  {
    "text": "Um, and I mean there are sort of two extents to which you can do this,",
    "start": "1288055",
    "end": "1295300"
  },
  {
    "text": "and we'll look at them both a bit. Um, one level is to say,",
    "start": "1295300",
    "end": "1300940"
  },
  {
    "text": "look we're still gonna have words in our system. Basically, we're going to build a system that works over words,",
    "start": "1300940",
    "end": "1307345"
  },
  {
    "text": "but we want to be able to create word representations for any character sequence and we'd like to",
    "start": "1307345",
    "end": "1314950"
  },
  {
    "text": "do it in a way that takes advantage of being able to recognize parts of the character sequence that look familiar,",
    "start": "1314950",
    "end": "1323050"
  },
  {
    "text": "so that we can probably guess what vibes means. Um, and so that sort of then solves the problems",
    "start": "1323050",
    "end": "1330670"
  },
  {
    "text": "with unknown words and we get similar words, similar embeddings for words with similar terms, spellings, et cetera.",
    "start": "1330670",
    "end": "1338125"
  },
  {
    "text": "But the other alternative is to say, oh no, just forget about these words altogether, who needs- um,",
    "start": "1338125",
    "end": "1343820"
  },
  {
    "text": "why don't we just do all of our language processing on sequence of characters,",
    "start": "1343820",
    "end": "1348880"
  },
  {
    "text": "it'll work out fine. Um, both of these methods have been proven to work very successfully.",
    "start": "1348880",
    "end": "1355090"
  },
  {
    "text": "Um, and I just wanted to dwell on that for one moment, and that sort of goes back to my,",
    "start": "1355090",
    "end": "1361480"
  },
  {
    "text": "um, morphology slide here. When people first started proposing that they are going",
    "start": "1361480",
    "end": "1367840"
  },
  {
    "text": "to build deep learning models over characters. I mean, my first feeling was oh, that is never going to",
    "start": "1367840",
    "end": "1374650"
  },
  {
    "text": "work because it sort of seemed like, okay, words have a meaning it makes sense,",
    "start": "1374650",
    "end": "1381309"
  },
  {
    "text": "um, that you can do something like build a word2vec model and that's going to really be able to",
    "start": "1381310",
    "end": "1387160"
  },
  {
    "text": "sort of see words and their distribution and learn the meanings of the words because words have a meaning.",
    "start": "1387160",
    "end": "1392905"
  },
  {
    "text": "The idea that you're going to be able to say, well, I'm going to come up with a vector representation of h,",
    "start": "1392905",
    "end": "1399310"
  },
  {
    "text": "and a different vector representation of a, and a different vec- vector representation of t,",
    "start": "1399310",
    "end": "1406404"
  },
  {
    "text": "and somehow that'll be useful for representing what a hat means once I put it through enough neural network layers,",
    "start": "1406404",
    "end": "1412495"
  },
  {
    "text": "um, frankly it sounded pretty unconvincing to me. Um, but, um, I guess, you know-",
    "start": "1412495",
    "end": "1420315"
  },
  {
    "text": "But it, it, totally works so I'm convinced now, empirical proof. And I think what we so essentially need to realize,",
    "start": "1420315",
    "end": "1428885"
  },
  {
    "text": "is that with going- that yes, at some level we just have these characters that don't mean much.",
    "start": "1428885",
    "end": "1435755"
  },
  {
    "text": "But we then have these very powerful combinatory models with a lot of parameters in them,",
    "start": "1435755",
    "end": "1442940"
  },
  {
    "text": "things like recurrent neural networks and convolutional neural networks and that they are respectively able to, sort of, build,",
    "start": "1442940",
    "end": "1450304"
  },
  {
    "text": "store and build representations of meaning from multi-letter groups,",
    "start": "1450305",
    "end": "1455330"
  },
  {
    "text": "in such a way that they can model the meanings of morphemes and larger units and therefore put together word meanings.",
    "start": "1455330",
    "end": "1462590"
  },
  {
    "start": "1462000",
    "end": "1687000"
  },
  {
    "text": "Um, yeah. So, um, one more detail on using characters,",
    "start": "1462590",
    "end": "1467930"
  },
  {
    "text": "um, from writing systems. So if you're a linguist you tend to think of sounds as primary.",
    "start": "1467930",
    "end": "1473659"
  },
  {
    "text": "Those were the phonemes that we- I mentioned beforehand. You know, um, essentially,",
    "start": "1473660",
    "end": "1480170"
  },
  {
    "text": "um, deep learning hasn't tried to use phonemes at all. Traditional speech recognizers often did use phonemes,",
    "start": "1480170",
    "end": "1486425"
  },
  {
    "text": "but in the deep learning land, you want to have a lot of data and the way you get a lot of data is you just use, um,",
    "start": "1486425",
    "end": "1493580"
  },
  {
    "text": "written stuff because, you know, it's the easily found data where you can get millions and billions of words of stuff.",
    "start": "1493580",
    "end": "1500919"
  },
  {
    "text": "Um, so that sort of makes sense from a data point of view. But the thing that ends up as a little weird about that,",
    "start": "1500920",
    "end": "1507804"
  },
  {
    "text": "is that when you're then building a character level model, what your character level model is,",
    "start": "1507805",
    "end": "1513714"
  },
  {
    "text": "actually varies depending on the writing system of the language. And so you, kind of,",
    "start": "1513714",
    "end": "1518875"
  },
  {
    "text": "have these quite different writing systems. So you have some writing systems which are just completely phonemic,",
    "start": "1518875",
    "end": "1527405"
  },
  {
    "text": "that there are letters that have a particular sound and you say that sound. Something like Spanish is pretty much phonemic.",
    "start": "1527405",
    "end": "1534575"
  },
  {
    "text": "Sometimes it's a teeny bit complicated. So you might have a digraph. So this digraph, ngabulu, is, kind of, like,",
    "start": "1534575",
    "end": "1541700"
  },
  {
    "text": "the N-G of English that is used for \"ng\" sound like at the end of seeing, but, you know, basically this is just 'jiyawu',",
    "start": "1541700",
    "end": "1549080"
  },
  {
    "text": "each letter is a sound, you can read it, um, and it's just, um, phonemic.",
    "start": "1549080",
    "end": "1554960"
  },
  {
    "text": "Um, that then contrasts from something like English where all the non-native speakers know the spelling is terrible.",
    "start": "1554960",
    "end": "1561890"
  },
  {
    "text": "It's got this, sort of, highly fossilized, once upon a time, phonemic system in the tenth century or something.",
    "start": "1561890",
    "end": "1568885"
  },
  {
    "text": "Um, but now we have this system that words have fairly arbitrary spelling that doesn't actually represent the sounds, um, very clearly.",
    "start": "1568885",
    "end": "1577700"
  },
  {
    "text": "But it's sort of a phonemic system. But then there are languages that use larger units.",
    "start": "1577700",
    "end": "1583429"
  },
  {
    "text": "Um, this is, um, Canadian and Inuktitut which I just put in there because it's such a pretty writing system.",
    "start": "1583430",
    "end": "1589715"
  },
  {
    "text": "Um, but there are a lot of languages that represent syllables by their characters.",
    "start": "1589715",
    "end": "1595820"
  },
  {
    "text": "Um, so you'd have something like this in Korean for example, with Korean Hangul, that each, um,",
    "start": "1595820",
    "end": "1601190"
  },
  {
    "text": "letter is then being a syllable of a sort of consonant vowel combination like bar.",
    "start": "1601190",
    "end": "1606860"
  },
  {
    "text": "Um, if you can then go up a level from that and if we get back to Chinese again,",
    "start": "1606860",
    "end": "1612455"
  },
  {
    "text": "well, um, this is sort of also a syllabic system, you could say.",
    "start": "1612455",
    "end": "1617465"
  },
  {
    "text": "But really, the Chinese characters are much more than just the sound. They also have a meaning.",
    "start": "1617465",
    "end": "1623515"
  },
  {
    "text": "That this is really then an ideographic system where there are characters with particular meanings attached to them.",
    "start": "1623515",
    "end": "1629570"
  },
  {
    "text": "So they're, sort of, uh, whole morphemes in- written as one letter. And, you know, another example of such language,",
    "start": "1629570",
    "end": "1636964"
  },
  {
    "text": "um, was Egyptian hieroglyphs, if you've seen those. That they're, sort of, ideographic systems where you have letters with meanings.",
    "start": "1636964",
    "end": "1643700"
  },
  {
    "text": "Um, and then you have language systems that sort of mix several of those. So Japanese is sort of a mixture of partly moraic,",
    "start": "1643700",
    "end": "1651875"
  },
  {
    "text": "partly ideographic systems mixed together. So if you just, sort of, start off and say,",
    "start": "1651875",
    "end": "1657695"
  },
  {
    "text": "\"Okay, I'm gonna build a character-based system.\" That's fine. But effectively, your character units like",
    "start": "1657695",
    "end": "1665495"
  },
  {
    "text": "letter trigrams are just very different in a language like Chinese,",
    "start": "1665495",
    "end": "1670610"
  },
  {
    "text": "where commonly a letter trigram will be, sort of, a word and a half, three morphemes with meaning.",
    "start": "1670610",
    "end": "1677915"
  },
  {
    "text": "Whereas if you're in something like English, your character trigram will be something like T-H-O",
    "start": "1677915",
    "end": "1683105"
  },
  {
    "text": "which is still sort of much too small a unit to have any meaning. So moving right ahead.",
    "start": "1683105",
    "end": "1688809"
  },
  {
    "start": "1687000",
    "end": "2089000"
  },
  {
    "text": "So these two kind of approaches, um, one was just do a completely character level model and then the other one was,",
    "start": "1688810",
    "end": "1695875"
  },
  {
    "text": "sort of, you make use of characters to build bigger things that you're then gonna put something, like, into a more word level model.",
    "start": "1695875",
    "end": "1702860"
  },
  {
    "text": "So I'll do this one first and the other one. So for purely character level models, I actually showed an example of that last time. Do you remember?",
    "start": "1702860",
    "end": "1710785"
  },
  {
    "text": "So there was that very deep convolutional network from the Conneau et-al word for text classification at the end, um,",
    "start": "1710785",
    "end": "1717710"
  },
  {
    "text": "and that just started with a big line of characters and built these convolutional layers on top of that,",
    "start": "1717710",
    "end": "1723620"
  },
  {
    "text": "in the vision like network and classified the documents. Um, so that was, sort of, a completely character-level model.",
    "start": "1723620",
    "end": "1731450"
  },
  {
    "text": "Um, but here's a bit more work on this. So people for machine translation have built, um,",
    "start": "1731450",
    "end": "1739325"
  },
  {
    "text": "machine translation systems that just read characters and write characters.",
    "start": "1739325",
    "end": "1745250"
  },
  {
    "text": "And when people first tried to do that, um, it, sort of, didn't work, right?",
    "start": "1745250",
    "end": "1751940"
  },
  {
    "text": "The people thought it might help to build character-level models especially for languages like Chinese.",
    "start": "1751940",
    "end": "1757775"
  },
  {
    "text": "But people just weren't able to build models that worked as well as word-based models and either the pre-neural,",
    "start": "1757775",
    "end": "1765365"
  },
  {
    "text": "the non-neural or the neural world. But gradually that started to change.",
    "start": "1765365",
    "end": "1770929"
  },
  {
    "text": "So people started to have successful character-level decoders and then,",
    "start": "1770930",
    "end": "1776480"
  },
  {
    "text": "sort of, around 2015 and '16, um, people started to show,",
    "start": "1776480",
    "end": "1781789"
  },
  {
    "text": "look you could- can actually do machine translation very well at just a character level with a few asterisks.",
    "start": "1781790",
    "end": "1790054"
  },
  {
    "text": "And so, um, here's a bit of work, um, that we did. Um, the Luong and Manning one, from, um,",
    "start": "1790055",
    "end": "1796400"
  },
  {
    "text": "2015 on the last slide. So this is looking at English to Czech translation and Czech's",
    "start": "1796400",
    "end": "1803570"
  },
  {
    "text": "a good language to use if you want to motivate doing things at the character level, because it had those big horrible words with lots of",
    "start": "1803570",
    "end": "1810650"
  },
  {
    "text": "morphology like the example I showed you before and I'll show you some more later. So people had built word-level models for Czech.",
    "start": "1810650",
    "end": "1820145"
  },
  {
    "text": "Um, and, you know, they didn't work great, partly because of some of these vocab problems.",
    "start": "1820145",
    "end": "1825320"
  },
  {
    "text": "So, um, the, sort of, word-level state of the art was at this time was 15.7 BLEU,",
    "start": "1825320",
    "end": "1831905"
  },
  {
    "text": "which as you know is much less than we will accept for full grades in your homework.",
    "start": "1831905",
    "end": "1837680"
  },
  {
    "text": "[LAUGHTER] Um, but, you know, what counts as a good BLEU score depends on how difficult the language pair is.",
    "start": "1837680",
    "end": "1843740"
  },
  {
    "text": "Uh, um, and so you're not doing Czech. Um, but, um, so this was, sort of, the,",
    "start": "1843740",
    "end": "1848875"
  },
  {
    "text": "kind of, neural MT model that we've talked about. So it was a Seq2Seq model, with attention and then it had extra stuff for substituting UNKs with either,",
    "start": "1848875",
    "end": "1860375"
  },
  {
    "text": "uh, single word translation or by copying stuff from the source.",
    "start": "1860375",
    "end": "1865790"
  },
  {
    "text": "So it was, sort of, basically, state of the art neural MT of 2015, got 15.7 BLEU.",
    "start": "1865790",
    "end": "1872825"
  },
  {
    "text": "And the difference isn't big, um, but we were able to show, look we can build this completely, um,",
    "start": "1872825",
    "end": "1878875"
  },
  {
    "text": "character-level model and then actually, it did fractionally better. Um, so this, sort of,",
    "start": "1878875",
    "end": "1884300"
  },
  {
    "text": "showed that in terms of translation quality, um, character, purely character-based models were completely viable at",
    "start": "1884300",
    "end": "1892760"
  },
  {
    "text": "capturing the meaning of text as well as word-based models. Um, was this a great result?",
    "start": "1892760",
    "end": "1900395"
  },
  {
    "text": "Um, in many, in some ways, yes, in another way, no. I mean, this model was truly terrible to train, right?",
    "start": "1900395",
    "end": "1908809"
  },
  {
    "text": "So it took about three weeks for us to train this model and at run-time, it also worked very slowly.",
    "start": "1908810",
    "end": "1914855"
  },
  {
    "text": "And so the problem with character-level models, if you're putting them into something like an LSTM,",
    "start": "1914855",
    "end": "1920135"
  },
  {
    "text": "is your sequences get way longer, right. So you've got about seven times as long sequences as you used to have.",
    "start": "1920135",
    "end": "1927110"
  },
  {
    "text": "And since there's not much information, the characters, you have to do back propagation through time much further back.",
    "start": "1927110",
    "end": "1935840"
  },
  {
    "text": "And so we were running back propagation through time for 600 steps before we were trun- truncating it.",
    "start": "1935840",
    "end": "1942590"
  },
  {
    "text": "And so this, sort of, made, maybe that was excessive, but it made the models, um, very slow.",
    "start": "1942590",
    "end": "1947770"
  },
  {
    "text": "But we were able to show that it was able to get some of these good effects, right. So here's a Czech,",
    "start": "1947770",
    "end": "1953705"
  },
  {
    "text": "um, translating to Czech, her 11 year-old daughter, Shani Bart, said it felt a little bit weird.",
    "start": "1953705",
    "end": "1959645"
  },
  {
    "text": "And, um, I don't know, probably. Does anyone speak Czech, any Czech speakers?",
    "start": "1959645",
    "end": "1965480"
  },
  {
    "text": "Um, no Czech speakers? Okay, um, I don't speak Czech either, um,",
    "start": "1965480",
    "end": "1971195"
  },
  {
    "text": "but we can see that the [LAUGHTER] we can see that this does interesting things, right.",
    "start": "1971195",
    "end": "1977389"
  },
  {
    "text": "So the second line is the human translation into Czech which we can use for some guidance.",
    "start": "1977390",
    "end": "1983010"
  },
  {
    "text": "And so in particular, um, in Czech there's a word for 11 years old, um, which you can see is that blue word on the second line.",
    "start": "1983010",
    "end": "1991294"
  },
  {
    "text": "And you can see that despite 11-year-old was, um, that for 11-year-old it's just able to perfectly, um,",
    "start": "1991295",
    "end": "1999425"
  },
  {
    "text": "produce letter by letter, um, the Czech word for 11 years old and that works beautifully.",
    "start": "1999425",
    "end": "2005830"
  },
  {
    "text": "In contrast, for the word-level model, um, 11 year-old was an unknown word because that wasn't in the vocabulary.",
    "start": "2005830",
    "end": "2013585"
  },
  {
    "text": "And so then it had two mechanisms to try and deal with, um, unknown words. It could either do, uh,",
    "start": "2013585",
    "end": "2019435"
  },
  {
    "text": "unigram translation of them or it could just copy them. And for whatever reason, it decided here that the best strategy",
    "start": "2019435",
    "end": "2026230"
  },
  {
    "text": "was to copy and so that was a complete fail. Um, and if we go along for the character-level model,",
    "start": "2026230",
    "end": "2033115"
  },
  {
    "text": "another thing that it gets right that's really cool, um, is the name Shani Bart. It's able to do this transliteration tasks that I mentioned just perfectly.",
    "start": "2033115",
    "end": "2042370"
  },
  {
    "text": "And it turns that to Shani Bartova which is exactly what the human translator did as well.",
    "start": "2042370",
    "end": "2047760"
  },
  {
    "text": "And so, you know, it's actually doing some really kind of nice, um, human translator, um, like things.",
    "start": "2047760",
    "end": "2055635"
  },
  {
    "text": "I mean, in fact, as best I can tell from spending a bit of time on Google Translate, it actually does a pretty good job in this sentence, period.",
    "start": "2055635",
    "end": "2063195"
  },
  {
    "text": "All right, this part here starts to be different, um, from the human translator. But it's not actually bad.",
    "start": "2063195",
    "end": "2069480"
  },
  {
    "text": "It's sort of a more literal translation. So this citi um, actually translates feel like in the English texts.",
    "start": "2069480",
    "end": "2077339"
  },
  {
    "text": "Whereas the human, sort of, didn't actually use the word feel in the Czech version that they just went,",
    "start": "2077340",
    "end": "2083230"
  },
  {
    "text": "um, was a little bit, um, weird or strange. So that's cool.",
    "start": "2083230",
    "end": "2089360"
  },
  {
    "text": "Okay. So here are a couple more results from this.",
    "start": "2089360",
    "end": "2094965"
  },
  {
    "text": "So here's another system that was built the next year. By these people Jason Lee,",
    "start": "2094965",
    "end": "2100829"
  },
  {
    "text": "Kyunghyun Cho and Thomas Hoffman. So they wanted to do something that was, I don't know,",
    "start": "2100830",
    "end": "2109515"
  },
  {
    "text": "much more complex and neural and understanding the meaning of the text on the source side.",
    "start": "2109515",
    "end": "2116385"
  },
  {
    "text": "And so they were more using the kind of technologies we saw last time. So on the encoder side you started off with a letter sequence of character embeddings.",
    "start": "2116385",
    "end": "2129960"
  },
  {
    "text": "And then you're sort of using convolutions of four, three and five of characters to get representations up here.",
    "start": "2129960",
    "end": "2139710"
  },
  {
    "text": "You're then doing a max pooling with a stride of five. So you're getting a max pooled representation of pieces of the text for each of the three,",
    "start": "2139710",
    "end": "2149370"
  },
  {
    "text": "four and five convolutions. You're then feeding that through multiple layers of",
    "start": "2149370",
    "end": "2154589"
  },
  {
    "text": "highway network and feeding that through a bidirectional gated recurrent unit and that's giving you your source representation.",
    "start": "2154590",
    "end": "2165569"
  },
  {
    "text": "On the decoder side, it was sort of the same as our decoder, it was just running a character level sequence model.",
    "start": "2165570",
    "end": "2173680"
  },
  {
    "text": "So overall, so they were doing the opposite task.",
    "start": "2174110",
    "end": "2181590"
  },
  {
    "text": "This is Czech to English. But, so they are starting to get better scores.",
    "start": "2181590",
    "end": "2188325"
  },
  {
    "text": "But I mean actually if you're sort of looking at these different numbers, where I'll explain this system more in a minute,",
    "start": "2188325",
    "end": "2194475"
  },
  {
    "text": "I mean it sort of seems like the place where they get a lot of value is that using",
    "start": "2194475",
    "end": "2201630"
  },
  {
    "text": "the character level decoder gives them a lot of value by",
    "start": "2201630",
    "end": "2206700"
  },
  {
    "text": "this very complex model on the source side is giving them almost no value at all.",
    "start": "2206700",
    "end": "2213550"
  },
  {
    "text": "One even more recent paper, so this is Colin Cherry and fellow researchers at Google.",
    "start": "2214760",
    "end": "2223425"
  },
  {
    "start": "2215000",
    "end": "2405000"
  },
  {
    "text": "So they last year did one more exploration of",
    "start": "2223425",
    "end": "2229560"
  },
  {
    "text": "doing LSTM sequence to sequence style models of comparing word and character-based models.",
    "start": "2229560",
    "end": "2236505"
  },
  {
    "text": "And this is English to French and this is um, Czech to English which is just what we were doing.",
    "start": "2236505",
    "end": "2243090"
  },
  {
    "text": "And so in both cases when you have a big model, the character model wins for them.",
    "start": "2243090",
    "end": "2250005"
  },
  {
    "text": "The blue model comes out on top but the sort of interesting thing as you sort of see these different effects depending on",
    "start": "2250005",
    "end": "2256829"
  },
  {
    "text": "the morphological complexity of the language. So for a language like Czech, it's a really good idea,",
    "start": "2256830",
    "end": "2263849"
  },
  {
    "text": "if you want to build a good model, to use character level that they're getting about a BLEU point of difference there,",
    "start": "2263850",
    "end": "2269760"
  },
  {
    "text": "whereas for a model without putting French or English there's actually a tiny but very little gain from using a character level model.",
    "start": "2269760",
    "end": "2281295"
  },
  {
    "text": "Okay so let me just explain these models, so these models are models of different sizes.",
    "start": "2281295",
    "end": "2288405"
  },
  {
    "text": "So these models are using bidirectional LSTM encoders and one-directional LSTM decoders.",
    "start": "2288405",
    "end": "2297720"
  },
  {
    "text": "So the simplest model just has a shallow bidirectional LSTM encoder and a two layer LSTM decoder.",
    "start": "2297720",
    "end": "2308430"
  },
  {
    "text": "The middle model has a three deep stack of bidirectional LSTM encoders and a four deep stack of LSTM decoders.",
    "start": "2308430",
    "end": "2319935"
  },
  {
    "text": "And the most complex model has a six deep stack of bidirectional LSTM encoders and an eight deep stack of LSTM decoders.",
    "start": "2319935",
    "end": "2331530"
  },
  {
    "text": "This is where it helps to work at Google. Probably for your projects, you don't want to go beyond three or four. Stay over here.",
    "start": "2331530",
    "end": "2339750"
  },
  {
    "text": "Okay yeah so, so these are the results. So basically what you're finding is if you're making",
    "start": "2339750",
    "end": "2347010"
  },
  {
    "text": "sort of smaller models you're better off with words, but as you go to big models especially if you're in a morphologically rich language,",
    "start": "2347010",
    "end": "2355680"
  },
  {
    "text": "you clearly start to win from the characters. But there is still a loss which is essentially",
    "start": "2355680",
    "end": "2361410"
  },
  {
    "text": "exactly the same loss that we were suffering from in 2015, right?",
    "start": "2361410",
    "end": "2367635"
  },
  {
    "text": "This is the time graph and so these are the same three models as over here,",
    "start": "2367635",
    "end": "2375510"
  },
  {
    "text": "it's just the axis is changed to sort of sum the total number of LSTM layers.",
    "start": "2375510",
    "end": "2380610"
  },
  {
    "text": "And so essentially, if you're at the word level, you can run any of these three models and they are fast that you can be translating in",
    "start": "2380610",
    "end": "2391800"
  },
  {
    "text": "sort of not much time but for the character level models your slope is much higher.",
    "start": "2391800",
    "end": "2399000"
  },
  {
    "text": "So it starts to get quite expensive to run the deep character level models.",
    "start": "2399000",
    "end": "2404770"
  },
  {
    "start": "2405000",
    "end": "2486000"
  },
  {
    "text": "Okay, so that's that section. So then chugging along.",
    "start": "2405860",
    "end": "2412809"
  },
  {
    "text": "I then wanted to look at other ways of doing things.",
    "start": "2412910",
    "end": "2418395"
  },
  {
    "text": "And so these are models that in some sense still do have words but where",
    "start": "2418395",
    "end": "2423930"
  },
  {
    "text": "we're going to want to sort of build word representations out of pieces.",
    "start": "2423930",
    "end": "2429675"
  },
  {
    "text": "And there are essentially two families of ways that people have explored doing this.",
    "start": "2429675",
    "end": "2435465"
  },
  {
    "text": "One way of doing it is to say look we just want to use exactly the same architecture as we use for a word model",
    "start": "2435465",
    "end": "2444495"
  },
  {
    "text": "except our words aren't really going to be words at least sometimes they're going to be pieces of words.",
    "start": "2444495",
    "end": "2453390"
  },
  {
    "text": "And so those are often called word piece models. And in particular, there's one commonest way of doing it.",
    "start": "2453390",
    "end": "2459450"
  },
  {
    "text": "It's called BPE, which I'll go through in some detail. The other alternative is to say,",
    "start": "2459450",
    "end": "2466695"
  },
  {
    "text": "well, we're gonna kind of make a mixture or a hybrid. So our main model is going to work in terms of words but we're",
    "start": "2466695",
    "end": "2474420"
  },
  {
    "text": "going to have some kind of facility where we can construct a representation, for otherwise unknown words,",
    "start": "2474420",
    "end": "2481530"
  },
  {
    "text": "by doing things that at a character or a lower level. And I'll show you a bit of that as well.",
    "start": "2481530",
    "end": "2486750"
  },
  {
    "text": "Okay, so this is BPE. BPE is actually a pretty simple idea which has nothing to",
    "start": "2486750",
    "end": "2496080"
  },
  {
    "text": "do with deep learning but the use of BPE has sort of become",
    "start": "2496080",
    "end": "2501150"
  },
  {
    "text": "pretty standard and successful for representing pieces of words to allow you to",
    "start": "2501150",
    "end": "2509339"
  },
  {
    "text": "have an infinite vocabulary while an infinite effective vocabulary while actually working with a finite vocabulary.",
    "start": "2509340",
    "end": "2518490"
  },
  {
    "text": "So the origins of Byte Pair Encoding and the name byte pair has nothing to do with natural language processing or neural nets,",
    "start": "2518490",
    "end": "2527760"
  },
  {
    "text": "we're just writing a compression algorithm. So this is something like compressing your documents with gzip.",
    "start": "2527760",
    "end": "2534375"
  },
  {
    "text": "So what basic Byte Pair Encoding is, that you've got a collection of stuff with bytes and you are",
    "start": "2534375",
    "end": "2543090"
  },
  {
    "text": "looking for the most frequent sequence of two bytes and you say,",
    "start": "2543090",
    "end": "2548820"
  },
  {
    "text": "okay, I'm going to add that sequence of two bytes as a new element to my dictionary of possible values.",
    "start": "2548820",
    "end": "2557700"
  },
  {
    "text": "And that means I can have 257 different values for bytes so to",
    "start": "2557700",
    "end": "2562829"
  },
  {
    "text": "speak that I can shrink the length of my sequence and I can repeat over and do that again.",
    "start": "2562830",
    "end": "2569370"
  },
  {
    "text": "And so essentially, this work suggested, well we can apply this kind of compression algorithm and use it as",
    "start": "2569370",
    "end": "2578700"
  },
  {
    "text": "a way of coming up with pieces of words that were",
    "start": "2578700",
    "end": "2583890"
  },
  {
    "text": "useful, doing it not strictly with bytes despite the name but instead with characters and character n-grams.",
    "start": "2583890",
    "end": "2593400"
  },
  {
    "text": "And so the most common way to do this with characters and character n-grams and if you're up with modern times,",
    "start": "2593400",
    "end": "2599849"
  },
  {
    "text": "you know that means there's unicode and you can represent all of these lovely letters like Canadian Inuktitut's",
    "start": "2599850",
    "end": "2606060"
  },
  {
    "text": "syllabics and stuff like that. But there's actually a problem with Unicode,",
    "start": "2606060",
    "end": "2611340"
  },
  {
    "text": "which is there actually a lot of Unicode characters. I forget the number theoretically.",
    "start": "2611340",
    "end": "2616815"
  },
  {
    "text": "I think there's about 200,000 possible Unicode characters. But at any rate, if you want to handle a bunch of languages which include East Asian languages,",
    "start": "2616815",
    "end": "2624900"
  },
  {
    "text": "maybe you need something like 20,000 characters and that's sort of a lot. So there are actually some people who've literally gone back to bytes and said,",
    "start": "2624900",
    "end": "2634920"
  },
  {
    "text": "\"You know 200,000, that's a really big vocabulary. I don't want to deal with anything.\" Sorry, 200,000 is a really big vocabulary.",
    "start": "2634920",
    "end": "2641700"
  },
  {
    "text": "I don't even want to deal with anything that large. So why don't I actually just do these kind of algorithms over bytes?",
    "start": "2641700",
    "end": "2650010"
  },
  {
    "text": "And so that means that in UTF-8 encoding, Chinese characters take three bytes each.",
    "start": "2650010",
    "end": "2657870"
  },
  {
    "text": "And so you're actually have to- you only get whole characters if you actually merge together several bytes that are common sequencers.",
    "start": "2657870",
    "end": "2667099"
  },
  {
    "start": "2667000",
    "end": "2835000"
  },
  {
    "text": "Okay. So more concretely, um, how does this work? So we're sort of doing this bottom-up clustering of short sequences.",
    "start": "2667100",
    "end": "2677110"
  },
  {
    "text": "So we start with a unigram vocabulary, which is all of the Unicode characters and some data.",
    "start": "2677110",
    "end": "2684120"
  },
  {
    "text": "We then sort of ask, what's the most frequent ngram here?",
    "start": "2684120",
    "end": "2689310"
  },
  {
    "text": "Um, initially it will be a bigram pair and we add that to our vocabulary. So if we start off, you know,",
    "start": "2689310",
    "end": "2696610"
  },
  {
    "text": "we can take our text that's- um, I'll come back to this in a minute. Let's assume we have a text that has been divided into words so we do have word tokens.",
    "start": "2696610",
    "end": "2705895"
  },
  {
    "text": "And so we can represent as a dictionary and say here are some words with their frequency.",
    "start": "2705895",
    "end": "2711670"
  },
  {
    "text": "Um, and so now we look for a common letter sequence and we say, \"Oh, es.\"",
    "start": "2711670",
    "end": "2717984"
  },
  {
    "text": "That occurs nine times, um, in this data because we have the counts for the words on the left side.",
    "start": "2717985",
    "end": "2725250"
  },
  {
    "text": "So, um, we start with our vocabulary being all the individual letters.",
    "start": "2725250",
    "end": "2730600"
  },
  {
    "text": "We find a commonest letter sequence like es, and so we say, \"Let's clump that together and make that a new thing in our vocabulary.\"",
    "start": "2730600",
    "end": "2740244"
  },
  {
    "text": "So now we've got an extra thing in our vocabulary. And now what's the commonest ngram sequence that clumped something?",
    "start": "2740245",
    "end": "2747340"
  },
  {
    "text": "Well, actually all of these es's are followed by t, so we also have es,",
    "start": "2747340",
    "end": "2752650"
  },
  {
    "text": "t with frequency nine, and so we can add that to our vocabulary.",
    "start": "2752650",
    "end": "2758050"
  },
  {
    "text": "And then we ask again, well, what's another common letter sequence? Let's see, there are seven cases of o double- well,",
    "start": "2758050",
    "end": "2766825"
  },
  {
    "text": "I guess there are seven cases of either l o or o w, so we can lump those and then we can lump",
    "start": "2766825",
    "end": "2773380"
  },
  {
    "text": "again and make an lo w. So if we sort of run this, we start to build these clumps of common letter sequences,",
    "start": "2773380",
    "end": "2782410"
  },
  {
    "text": "and so common bits like est, but also just common words,",
    "start": "2782410",
    "end": "2788450"
  },
  {
    "text": "something like that in English will very quickly be clumped together and be a unit of our vocabulary.",
    "start": "2788450",
    "end": "2795130"
  },
  {
    "text": "Um, and so we do that for a while. So normally what we do is we decide",
    "start": "2795130",
    "end": "2800650"
  },
  {
    "text": "a vocabulary size that we want to work with. We say, \"Okay. I want to work with a vocabulary size of 8,000 words.\"",
    "start": "2800650",
    "end": "2807400"
  },
  {
    "text": "That'll mean my model will be fast, and we just sort of keep doing this until we have 8,000 things in our vocabulary.",
    "start": "2807400",
    "end": "2814255"
  },
  {
    "text": "And that means that our vocabulary will have in it all single letters because we started with them and it'll",
    "start": "2814255",
    "end": "2820650"
  },
  {
    "text": "have common subsequences of words like the es and the est that are now in our vocabulary,",
    "start": "2820650",
    "end": "2827285"
  },
  {
    "text": "but also have whole words whenever there're common words, like, you know, the, and too, and with,",
    "start": "2827285",
    "end": "2833280"
  },
  {
    "text": "and so on, will become parts of our vocabulary. Um, and so then when we have a piece of text we can do",
    "start": "2833280",
    "end": "2841000"
  },
  {
    "start": "2835000",
    "end": "3120000"
  },
  {
    "text": "a deterministic longest piece segmentation of words, and we will say that is our eeset of word pieces.",
    "start": "2841000",
    "end": "2848310"
  },
  {
    "text": "And so for an input piece of text, we turn into word pieces, and then we just run it through our MT system as if we were using words,",
    "start": "2848310",
    "end": "2857695"
  },
  {
    "text": "but really it's pieces of words, and then on the output side, we just concatenate them back together as needed.",
    "start": "2857695",
    "end": "2865845"
  },
  {
    "text": "Okay. So we get this sort of automatic word-based system. And that's proved to be a very successful system.",
    "start": "2865845",
    "end": "2873525"
  },
  {
    "text": "So this idea of using byte pair encoding sort of really emerged in 2015 and then in 2016, uh,",
    "start": "2873525",
    "end": "2882115"
  },
  {
    "text": "workshop on machine translation which has been the main sort of annual competition for MT systems that the several top systems were built using byte pair encoding.",
    "start": "2882115",
    "end": "2892300"
  },
  {
    "text": "If you look at last year's competition, there's a bit more variety, but really a number of the top systems are still using byte pair encoding,",
    "start": "2892300",
    "end": "2901044"
  },
  {
    "text": "that's just been a good way to do things. So for Google's Neural Machine Translation,",
    "start": "2901044",
    "end": "2909130"
  },
  {
    "text": "they effectively use of- a variant of byte pair encoding. So they don't use exactly the same algorithm, um,",
    "start": "2909130",
    "end": "2917160"
  },
  {
    "text": "they use a slightly different algorithm where they're using a language model and they're saying,",
    "start": "2917160",
    "end": "2924010"
  },
  {
    "text": "what- what- rather than just using pure counts, they're saying, \"What clumping together would maximally reduce",
    "start": "2924010",
    "end": "2932575"
  },
  {
    "text": "the perplexity of my language model and clump those things and repeat over?\"",
    "start": "2932575",
    "end": "2938410"
  },
  {
    "text": "And so they did- they've done two versions of this model. So the first version, the wordpiece model kind of like, um,",
    "start": "2938410",
    "end": "2947170"
  },
  {
    "text": "byte pair encoding assumed that you have an initial tokenization to words and then you're just sort of having pieces of words,",
    "start": "2947170",
    "end": "2956035"
  },
  {
    "text": "um, using this algorithm. And then they did a second version, um, the sentencepiece model which you can find at this GitHub site which said, \"Well,",
    "start": "2956035",
    "end": "2965050"
  },
  {
    "text": "it's problematic if we need to tokenize into words first because then we need to have a tokenizer for",
    "start": "2965050",
    "end": "2970208"
  },
  {
    "text": "every language and that's a lot of work.\" Um, so maybe instead of that, we could just sort of treat,",
    "start": "2970208",
    "end": "2976825"
  },
  {
    "text": "go from a character sequence, retain whitespaces and regard that as something that's part of the clumping process,",
    "start": "2976825",
    "end": "2984855"
  },
  {
    "text": "and so that, um, you just build your word pieces which",
    "start": "2984855",
    "end": "2990015"
  },
  {
    "text": "commonly will have spaces on one side or the other of them, um, because often things inside a word are the",
    "start": "2990015",
    "end": "2997710"
  },
  {
    "text": "commoner- more common clumps and you build those up, and that's proven to be quite successful.",
    "start": "2997710",
    "end": "3004755"
  },
  {
    "text": "Um, in particular, one place where some of you might see this,",
    "start": "3004755",
    "end": "3010529"
  },
  {
    "text": "um, is, um, we've yet to get to describing it in the class really, but there's been this recent work which we actually talk about next week in class, are",
    "start": "3010530",
    "end": "3019950"
  },
  {
    "text": "building these transformer models, in particular, Google has released this BERT model which gives you",
    "start": "3019950",
    "end": "3026160"
  },
  {
    "text": "very good, um, word representations. And if you download BERT and try and use it,",
    "start": "3026160",
    "end": "3032835"
  },
  {
    "text": "what you will find out is it doesn't operate over words, it operates over word pieces.",
    "start": "3032835",
    "end": "3040050"
  },
  {
    "text": "Um, and so it has a large vocabulary. It's not a vocabulary of like 8,000 words.",
    "start": "3040050",
    "end": "3046470"
  },
  {
    "text": "I forget the number, but the models have a large vocabulary, but they're still not a huge vocabulary and it's using word pieces.",
    "start": "3046470",
    "end": "3055380"
  },
  {
    "text": "So lots of words are in the vocabulary. So if you look at the English model, it not only has words like f in it,",
    "start": "3055380",
    "end": "3061140"
  },
  {
    "text": "but it even has words like Fairfax and 1910s, which aren't that common. Um, but it's nevertheless to cover all words,",
    "start": "3061140",
    "end": "3070244"
  },
  {
    "text": "it's again using this wordpiece idea. So if I want a representation for the word hypatia, um,",
    "start": "3070245",
    "end": "3076075"
  },
  {
    "text": "that's not in the vocabulary, and so I'm making it up of pieces. There's an h representation,",
    "start": "3076075",
    "end": "3081795"
  },
  {
    "text": "and then in the BERT version, which is different to the Google NMT version,",
    "start": "3081795",
    "end": "3086880"
  },
  {
    "text": "the non- the non-initial word pieces are represented with two hashes at the start,",
    "start": "3086880",
    "end": "3092625"
  },
  {
    "text": "so I can put that together with h##yp etc., and this would be my representation of hypatia.",
    "start": "3092625",
    "end": "3100200"
  },
  {
    "text": "So effectively, I have word vectors, um, for four word pieces,",
    "start": "3100200",
    "end": "3105435"
  },
  {
    "text": "and then I have to work out what to do with them. The simplest and quite common way is I just average the four of them.",
    "start": "3105435",
    "end": "3111599"
  },
  {
    "text": "And there are obviously other things you could do. You could ConvNet and maxpool or you could run a little LSTM or something to put together a representation.",
    "start": "3111600",
    "end": "3120640"
  },
  {
    "start": "3120000",
    "end": "3392000"
  },
  {
    "text": "Okay. Yeah. So- so those were the models that, um,",
    "start": "3120840",
    "end": "3127220"
  },
  {
    "text": "sort of, worked with pieces of words to give you infinite vocabulary and ran them through a normal system.",
    "start": "3127220",
    "end": "3134835"
  },
  {
    "text": "The other possibility is to say, \"Well, we wanna work with characters so we can deal with an infinite vocabulary,",
    "start": "3134835",
    "end": "3142530"
  },
  {
    "text": "but we're gonna sort of incorporate those into a bigger system.\"",
    "start": "3142530",
    "end": "3147945"
  },
  {
    "text": "And a whole bunch of work has done this and in some sense it's a fairly obvious thing to do.",
    "start": "3147945",
    "end": "3153990"
  },
  {
    "text": "Um, so this work in 2014 was one of the early ones. So they said, \"Well,",
    "start": "3153990",
    "end": "3160005"
  },
  {
    "text": "we could start with characters. We can do a convolution over the characters to generate word embeddings,",
    "start": "3160005",
    "end": "3167640"
  },
  {
    "text": "and then we can use those word embeddings for something in a higher level model.\"",
    "start": "3167640",
    "end": "3173250"
  },
  {
    "text": "Um, this was actually sort of a fixed window model for doing part of speech tagging. Um, that makes sense.",
    "start": "3173250",
    "end": "3180240"
  },
  {
    "text": "Instead of a convolution, you could use LSTM. So this was work from a year later,",
    "start": "3180240",
    "end": "3186720"
  },
  {
    "text": "and they said, \"Well, we're also gonna build up, um, word representations from characters.",
    "start": "3186720",
    "end": "3192244"
  },
  {
    "text": "And the way we're gonna do it is we're gonna run character level Bi-LSTMs,",
    "start": "3192245",
    "end": "3197520"
  },
  {
    "text": "concatenate the two final states, and we're gonna call that outward representation,",
    "start": "3197520",
    "end": "3203430"
  },
  {
    "text": "and then we're gonna put that word representation into a language model which is then a higher level LSTM that works along a sequence of words.\"",
    "start": "3203430",
    "end": "3214890"
  },
  {
    "text": "And I thought I'd just- Oh, yeah. Words, are they training uh, like character-",
    "start": "3214890",
    "end": "3221400"
  },
  {
    "text": "Yeah. Oh yeah, that's very important to realize. Yes so yeah so if you're learning- you'll learn- I mean this is the hidden layer.",
    "start": "3221400",
    "end": "3230549"
  },
  {
    "text": "I guess I'm not actually showing the input layer but for the input layer you're learning a vector for each character.",
    "start": "3230550",
    "end": "3238260"
  },
  {
    "text": "So effectively you're doing the same kind of thing we saw before that you're starting with random representations for each character.",
    "start": "3238260",
    "end": "3247545"
  },
  {
    "text": "You've got this embedded inside a word sequence LSTM,",
    "start": "3247545",
    "end": "3253589"
  },
  {
    "text": "your goal is to minimize the perplexity of the higher level LSTM as,",
    "start": "3253590",
    "end": "3260160"
  },
  {
    "text": "um, as a language model and so it filters back its gradients.",
    "start": "3260160",
    "end": "3266099"
  },
  {
    "text": "So it's wanting to come up with character vectors such that if it produces good word vectors which produces low, um, perplexities.",
    "start": "3266100",
    "end": "3275859"
  },
  {
    "text": "Good question. Um, so here's, um,",
    "start": "3275900",
    "end": "3281220"
  },
  {
    "text": "a slightly more complex version of trying to do this that's a bit more recent where again the idea is can we build",
    "start": "3281220",
    "end": "3288720"
  },
  {
    "text": "a good language model by starting out from characters and wanting to exploit sort of related sub words and rare words.",
    "start": "3288720",
    "end": "3299009"
  },
  {
    "text": "And so they built sort of this kind of this more stacked complex model that we'll go through the stages of wherefore",
    "start": "3299010",
    "end": "3307815"
  },
  {
    "text": "we start with a word represented as characters. We have character embeddings which we build",
    "start": "3307815",
    "end": "3314700"
  },
  {
    "text": "into a convolutional network and then we head upwards. So if we take that one piece at a time,",
    "start": "3314700",
    "end": "3320354"
  },
  {
    "text": "um, so you have a character embedding for each character. Um, you'll then have a convolutional layer which",
    "start": "3320354",
    "end": "3328680"
  },
  {
    "text": "then sort of rep, has various filters that work over those, um, character sequence of two, three and four grams of characters.",
    "start": "3328680",
    "end": "3337800"
  },
  {
    "text": "So you're getting representations of parts of words. Um, then from those convolutional networks you're then doing max pooling over time which",
    "start": "3337800",
    "end": "3348600"
  },
  {
    "text": "is effectively sort of like choosing which of these n-grams best represents the meaning of a word.",
    "start": "3348600",
    "end": "3356155"
  },
  {
    "text": "Um, then what they do after that is so at that point they've got an output representation for character n-grams,",
    "start": "3356155",
    "end": "3365030"
  },
  {
    "text": "and so then they feed that into a highway network like we talked about a bit last time.",
    "start": "3365030",
    "end": "3372095"
  },
  {
    "text": "And then the output of that then at the word level, um, goes into an LSTM network,",
    "start": "3372095",
    "end": "3380405"
  },
  {
    "text": "and this LSTM network is now word-level LSTM network, and you're trying to sort of maxim- minimize",
    "start": "3380405",
    "end": "3387710"
  },
  {
    "text": "perplexity like for the neural language models we saw earlier. Um, so what could they show with this?",
    "start": "3387710",
    "end": "3395385"
  },
  {
    "text": "Well, the first thing they could show with it is that it actually again just works well as a language model despite that skepticism",
    "start": "3395385",
    "end": "3403950"
  },
  {
    "text": "that I hadn't told you of about the fact of the matter is you could build these kind of character level models and",
    "start": "3403950",
    "end": "3410040"
  },
  {
    "text": "train them and they work to a first approximation as well as word-level language models.",
    "start": "3410040",
    "end": "3416535"
  },
  {
    "text": "But one of the observations that they make is that you can be getting as good results but with much smaller models.",
    "start": "3416535",
    "end": "3423840"
  },
  {
    "text": "So up at the top here are the character level LSTM models and word ones that the models they built.",
    "start": "3423840",
    "end": "3430695"
  },
  {
    "text": "And here are a whole bunch of models over this data-set. Um, and so as time went by perplexities have been going down,",
    "start": "3430695",
    "end": "3439410"
  },
  {
    "text": "gone to 78.4 and their point was well we can build pretty much as good a character model with 78.9 perplexity but",
    "start": "3439410",
    "end": "3449100"
  },
  {
    "text": "our model is actually much smaller, this model here has 52 million parameters whereas our model that",
    "start": "3449100",
    "end": "3455760"
  },
  {
    "text": "works on a character level has only 19 million parameters. So it's about 40% of the size.",
    "start": "3455760",
    "end": "3461685"
  },
  {
    "text": "And that seems,um, kind of interesting. But perhaps what's more interesting is to sort of peek inside it and see what",
    "start": "3461685",
    "end": "3471510"
  },
  {
    "text": "happens with the representation of words when built out of characters and this part is sort of actually a bit cool.",
    "start": "3471510",
    "end": "3479745"
  },
  {
    "text": "Um, so what this is showing is for words that are up to the top while,",
    "start": "3479745",
    "end": "3486064"
  },
  {
    "text": "his, you, Richard, trading. It's asking what other words are most",
    "start": "3486064",
    "end": "3491520"
  },
  {
    "text": "similar to it according to the word representations that's computed. And the top part is the output of a word level LSTM model and that's sort of okay.",
    "start": "3491520",
    "end": "3501480"
  },
  {
    "text": "Richard comes out as similar to Jonathan, Robert, Neil and Nancy et cetera. While although letting though minute mainly okay.",
    "start": "3501480",
    "end": "3510809"
  },
  {
    "text": "But it's sort of interesting what happens with their character level models,um, and so in particular, um,",
    "start": "3510810",
    "end": "3517605"
  },
  {
    "text": "what's kind of interesting is like first of all you remember they sort of had the character embeddings that went through the convolutional layer and the max pooling.",
    "start": "3517605",
    "end": "3526845"
  },
  {
    "text": "And if at that point you ask what things are most similar that basically it's still remembering things about characters.",
    "start": "3526845",
    "end": "3535829"
  },
  {
    "text": "So the most similar words to while, chile, whole, meanwhile and white.",
    "start": "3535830",
    "end": "3541740"
  },
  {
    "text": "So at least for the sort of first ones they all end in LE. And you see that pattern elsewhere right close to Richard,",
    "start": "3541740",
    "end": "3550560"
  },
  {
    "text": "hard, rich, richer, richter that hard ends in ARD, rich.",
    "start": "3550560",
    "end": "3555630"
  },
  {
    "text": "So you're just sort of getting this character sequence similarity, it's not really doing meaning at all. But interestingly when they're then putting it through the highway layers,",
    "start": "3555630",
    "end": "3565500"
  },
  {
    "text": "that the highway layers are suc- successfully learning how to transform those character sequence representations",
    "start": "3565500",
    "end": "3573285"
  },
  {
    "text": "into something that does capture meaning. So if you then say at the output of",
    "start": "3573285",
    "end": "3578850"
  },
  {
    "text": "the highway layers what words are most similar then it seems to be working pretty well,",
    "start": "3578850",
    "end": "3584295"
  },
  {
    "text": "While was similar to meanwhile, Richard is similar to Edward, Gerard, Edward with Carl. They're sort of now working much more like",
    "start": "3584295",
    "end": "3591750"
  },
  {
    "text": "a word level model in capturing semantic similarity. So that seems kind of cool.",
    "start": "3591750",
    "end": "3597900"
  },
  {
    "text": "Um, so then they say well what about if we ask about words that aren't in the vocabulary of the model.",
    "start": "3597900",
    "end": "3606555"
  },
  {
    "text": "Well, if they're not in the vocabulary of the model, the word level model can't do anything and so that's why you get those dashes there.",
    "start": "3606555",
    "end": "3613890"
  },
  {
    "text": "And what they're wanting to show is that the character level model still works pretty well.",
    "start": "3613890",
    "end": "3619470"
  },
  {
    "text": "So if you give it look with seven O's in the middle of it that it's correctly deciding that look,",
    "start": "3619470",
    "end": "3627300"
  },
  {
    "text": "look, look, looking are actually the most similar words to that which is actually working very nicely.",
    "start": "3627300",
    "end": "3633150"
  },
  {
    "text": "And some of the other examples are similar, computer-aided, is seen as most similar to computer-guided,",
    "start": "3633150",
    "end": "3638865"
  },
  {
    "text": "computer-driven, computerized, computer, you're getting pretty similar sensible results.",
    "start": "3638865",
    "end": "3644685"
  },
  {
    "text": "And then the little picture on the, um, right is sort of, um,",
    "start": "3644685",
    "end": "3649755"
  },
  {
    "text": "showing, um, one of these 2D visualizations of the units that have been learned.",
    "start": "3649755",
    "end": "3656234"
  },
  {
    "text": "And so the red, the red things are word character prefixes,",
    "start": "3656235",
    "end": "3662760"
  },
  {
    "text": "the blue things are character suffixes, the orange things are hyphenated things",
    "start": "3662760",
    "end": "3669180"
  },
  {
    "text": "like in the middle of computer-guided and gray is everything else. And so there's some sort of sense,",
    "start": "3669180",
    "end": "3674790"
  },
  {
    "text": "with which it's picking out different important parts of words. Okay. Um, and that's why also I guess just another good example of how you can sort of",
    "start": "3674790",
    "end": "3686880"
  },
  {
    "text": "compose together different kinds of building blocks to make more powerful models that you might",
    "start": "3686880",
    "end": "3692670"
  },
  {
    "text": "also want to think about for your final projects. Okay.",
    "start": "3692670",
    "end": "3697450"
  },
  {
    "text": "Um, so here's back to one other example from a neural machine translation system",
    "start": "3705200",
    "end": "3711135"
  },
  {
    "text": "of doing this hybrid architecture that has word-level and character level. I showed you earlier a purely character level model.",
    "start": "3711135",
    "end": "3719400"
  },
  {
    "text": "I mean we built that out of interest to see how well it did but we were sort of really wanting to build",
    "start": "3719400",
    "end": "3725940"
  },
  {
    "text": "a hybrid model because that seemed like it would be much more practical to build something that translated relatively quickly and well.",
    "start": "3725940",
    "end": "3734265"
  },
  {
    "text": "Um, so the idea was we'd mainly build a word-level neural machine translation system but we'd",
    "start": "3734265",
    "end": "3740550"
  },
  {
    "text": "be able to work with character level stuff when we had rare or unseen words.",
    "start": "3740550",
    "end": "3746970"
  },
  {
    "text": "Um, and that turned out to work pretty, um, successfully at improving performance.",
    "start": "3746970",
    "end": "3753000"
  },
  {
    "text": "So the idea of that model is this. Um, that we're going to run a pretty standard, um,",
    "start": "3753000",
    "end": "3760575"
  },
  {
    "text": "sequence to sequence with attention LSTM neural machine translation system.",
    "start": "3760575",
    "end": "3767099"
  },
  {
    "text": "In my pic- I mean, it's actually a four-level deep system but in my picture I showed less than four levels stacked to make it easier to see things.",
    "start": "3767100",
    "end": "3776490"
  },
  {
    "text": "And we're going to run this with a reasonable vocabulary of 16,000 words.",
    "start": "3776490",
    "end": "3781920"
  },
  {
    "text": "So for common words we just have word representations that we're feeding into",
    "start": "3781920",
    "end": "3787994"
  },
  {
    "text": "our neural machine translation model but for words that aren't in the vocabulary we're",
    "start": "3787995",
    "end": "3793320"
  },
  {
    "text": "going to work out a word representation for them by using a character level LSTM,",
    "start": "3793320",
    "end": "3799530"
  },
  {
    "text": "and conversely, when we start to generate words on the other side we have a soft max with a vocabulary of 16,000.",
    "start": "3799530",
    "end": "3808710"
  },
  {
    "text": "It could just generate words like [NOISE] but one of those words is the UNK symbol.",
    "start": "3808710",
    "end": "3814275"
  },
  {
    "text": "And if it generates the UNK symbol we then run a- we take this hidden representation and feed it in as",
    "start": "3814275",
    "end": "3822060"
  },
  {
    "text": "the initial input into a character level LSTM and then we have the character level",
    "start": "3822060",
    "end": "3827370"
  },
  {
    "text": "LSTM generate a character sequence until it generates a stop symbol and we use that to generate words. Um-",
    "start": "3827370",
    "end": "3836650"
  },
  {
    "text": "Okay. So we end up sort of with this sort of hybrid composed stack of eight LSTM layers. Uh, yeah.",
    "start": "3837240",
    "end": "3848170"
  },
  {
    "text": "[inaudible] and you always get some probability for the UNK symbol. So if you wanted to get the- the proper gradient,",
    "start": "3848170",
    "end": "3855180"
  },
  {
    "text": "you- you'll always have to run it for every word but what- what do you-? I would often say, you only run during training,",
    "start": "3855180",
    "end": "3862869"
  },
  {
    "text": "you only run the character level LSTM when the UNK symbol receives the highest likelihood.",
    "start": "3862870",
    "end": "3868990"
  },
  {
    "text": "So we- What is that? So at training, at training time,",
    "start": "3868990",
    "end": "3874335"
  },
  {
    "text": "there's a determinant piece of tech, right? You know the source and you know the target,",
    "start": "3874335",
    "end": "3879705"
  },
  {
    "text": "and so we're, and at training time, we've already decided our vocabulary, right?",
    "start": "3879705",
    "end": "3886230"
  },
  {
    "text": "That we've just decided what are the 15,999 most common words,",
    "start": "3886230",
    "end": "3891400"
  },
  {
    "text": "those and UNK are our vocabulary. So for both the input and the output side,",
    "start": "3891400",
    "end": "3897115"
  },
  {
    "text": "we know which words aren't in our vocabulary. And so if it's not in our vocabulary,",
    "start": "3897115",
    "end": "3903400"
  },
  {
    "text": "we're running this one. If if what was the output is not in our vocabulary, we're running that one, and otherwise we're just not running it at all, yeah.",
    "start": "3903400",
    "end": "3911950"
  },
  {
    "text": "So and and the bit that I didn't explain but is actually important perhaps",
    "start": "3911950",
    "end": "3918130"
  },
  {
    "text": "related like when we're calculating a loss that we can back",
    "start": "3918130",
    "end": "3923920"
  },
  {
    "text": "propagate, that sort of up here, there are sort of two losses. There's a loss at the word level that you know you'd like to in this position,",
    "start": "3923920",
    "end": "3932920"
  },
  {
    "text": "give probability 1 to generating UNK but really, this model we'll softmax, we'll say UNK is you know probability 0.2 or whatever.",
    "start": "3932920",
    "end": "3941470"
  },
  {
    "text": "So there's a loss there and then secondarily, there's a particular sequence of characters you wanna generate and you've also got",
    "start": "3941470",
    "end": "3948400"
  },
  {
    "text": "a loss because you've met the probabilities you put over the characters.",
    "start": "3948400",
    "end": "3953410"
  },
  {
    "text": "Um, So then, um, I think we- I think Abby sort of briefly mentioned this.",
    "start": "3953410",
    "end": "3958570"
  },
  {
    "text": "Commonly, the decoders do some kind of beam search to consider different possibilities before deciding,",
    "start": "3958570",
    "end": "3966310"
  },
  {
    "text": "um, the highest probability one over a sequence of words. And so this was doing a slightly more complex version of that.",
    "start": "3966310",
    "end": "3973930"
  },
  {
    "text": "So there's a word-level beam search when running it and then also doing a character level beam search to consider different possibilities.",
    "start": "3973930",
    "end": "3983335"
  },
  {
    "text": "And so if you wanna integrate the the two of those together. Um, but essentially, um,",
    "start": "3983335",
    "end": "3989785"
  },
  {
    "text": "this worked pretty well. Um, so, um, this was the winning system at WMT 2015 which",
    "start": "3989785",
    "end": "4000510"
  },
  {
    "text": "used 30 times as much data and ensembled together three other systems compared to the data that was provided for the task.",
    "start": "4000510",
    "end": "4008115"
  },
  {
    "text": "This was the system I showed before, they got 18.3. Um, and if you remember our character,",
    "start": "4008115",
    "end": "4014940"
  },
  {
    "text": "purely character level system got 18.5. Um, then by building this hybrid system,",
    "start": "4014940",
    "end": "4022500"
  },
  {
    "text": "that we were able to build a much better system that was about 2.5 BLEU points better,",
    "start": "4022500",
    "end": "4027600"
  },
  {
    "text": "um, than after- than either this word level or the character level system.",
    "start": "4027600",
    "end": "4032805"
  },
  {
    "text": "So that was kind of nice, um, and in particular that was the state of the art at the time. Now of course, if you were paying very close attention,",
    "start": "4032805",
    "end": "4041355"
  },
  {
    "text": "that's now nowhere near the state of the art. Because when I showed you that slide way earlier of the Google system,",
    "start": "4041355",
    "end": "4050369"
  },
  {
    "text": "you will have noticed that they have much higher numbers in the 20s, but that's what happens as the years go by.",
    "start": "4050370",
    "end": "4057600"
  },
  {
    "text": "Um, okay. But here's an example that shows these different systems working and some of the mistakes they make.",
    "start": "4057600",
    "end": "4066300"
  },
  {
    "text": "Um, here's a cherry picked example, um, where our system, the hybrid system,",
    "start": "4066300",
    "end": "4071940"
  },
  {
    "text": "works perfectly because what- that's what you expect to see. Um, and so, you know,",
    "start": "4071940",
    "end": "4077685"
  },
  {
    "text": "you can see some of the defects of things that can go wrong. Um, so in this case,",
    "start": "4077685",
    "end": "4085155"
  },
  {
    "text": "you know the character level system didn't work here because it just sort of starting with the Steph, it sort of seemed to free associate,",
    "start": "4085155",
    "end": "4095250"
  },
  {
    "text": "um, a completely made up name that doesn't really have anything to do with the source. So that one isn't very good.",
    "start": "4095250",
    "end": "4102569"
  },
  {
    "text": "Um, the word level system went bang here,",
    "start": "4102570",
    "end": "4107594"
  },
  {
    "text": "so you remember when it generates an UNK, the word level system would have when it generates, it's using attention.",
    "start": "4107595",
    "end": "4116400"
  },
  {
    "text": "So when it wants to generate, um, it has attention back to words and the source.",
    "start": "4116400",
    "end": "4122009"
  },
  {
    "text": "And when it generates UNK has two strategies. It can either do unigram translation of the word that it's maximally",
    "start": "4122010",
    "end": "4130409"
  },
  {
    "text": "putting attention on or it could copy the word that it's maximally putting attention on.",
    "start": "4130410",
    "end": "4136140"
  },
  {
    "text": "Um, so in this case, it chose to translate the word that it was maximally putting attention on but the word it",
    "start": "4136140",
    "end": "4142890"
  },
  {
    "text": "was maximally putting attention on was after rather than diagnosis.",
    "start": "4142890",
    "end": "4148080"
  },
  {
    "text": "And so you just get this po po coming out of after, after and we've completely lost the word.",
    "start": "4148080",
    "end": "4154140"
  },
  {
    "text": "Um, and in this example, in this example, how a hybrid system, um,",
    "start": "4154140",
    "end": "4160065"
  },
  {
    "text": "just ends up working beautifully and gives you exactly the right translation. Yeah. Um, of course,",
    "start": "4160065",
    "end": "4166740"
  },
  {
    "text": "it's not always that good in the real world. Um, so here's a different example. So this is the example I showed before with the 11-year-old daughter.",
    "start": "4166740",
    "end": "4175455"
  },
  {
    "text": "Um, and in this example, the hybrid model has the same strength of the character model.",
    "start": "4175455",
    "end": "4184350"
  },
  {
    "text": "It correctly generates 11 years old at a character level in its translation,",
    "start": "4184350",
    "end": "4190065"
  },
  {
    "text": "but you know this time, for whatever reason, it's the hybrid model that goes bang in",
    "start": "4190065",
    "end": "4196170"
  },
  {
    "text": "generating the names and it translates Shani Bart as Graham Bart. Um, whereas the character level model gets it right.",
    "start": "4196170",
    "end": "4204015"
  },
  {
    "text": "Um, actually, I think this is one of the weaknesses of this hybrid model compared to the character level model.",
    "start": "4204015",
    "end": "4210120"
  },
  {
    "text": "That because of the character level generator is kind of this sort of second level.",
    "start": "4210120",
    "end": "4216670"
  },
  {
    "text": "For the purely character level model, it's able to use the character sequence as conditioning context very effectively.",
    "start": "4217190",
    "end": "4227010"
  },
  {
    "text": "Whereas our hybrid model, although we feed the hidden representation of",
    "start": "4227010",
    "end": "4232410"
  },
  {
    "text": "the word level model in as the starting hidden representation of the character level model,",
    "start": "4232410",
    "end": "4237855"
  },
  {
    "text": "it doesn't have any representation further back than that of what's in the word level model.",
    "start": "4237855",
    "end": "4243614"
  },
  {
    "text": "And so it tends to not always do as good a job at representing, of capturing the context that allows it to do translation of things like names.",
    "start": "4243615",
    "end": "4253030"
  },
  {
    "text": "Okay. Um, very- almost finished but there's",
    "start": "4253100",
    "end": "4258510"
  },
  {
    "text": "just sort of one thing I wanted to mention before the end which is almost a practical thing.",
    "start": "4258510",
    "end": "4263985"
  },
  {
    "text": "Um, So we started off with word embeddings, but now we've been talking a lot of character level models.",
    "start": "4263985",
    "end": "4270510"
  },
  {
    "text": "So surely, just for word embedding, you should be able to do useful things with them,",
    "start": "4270510",
    "end": "4275760"
  },
  {
    "text": "with characters or pieces of words. And that's something that people start to play with.",
    "start": "4275760",
    "end": "4281340"
  },
  {
    "text": "So in this Cao and Rei paper they said well let's train a Word2vec model using exactly the same, um,",
    "start": "4281340",
    "end": "4290280"
  },
  {
    "text": "loss as Word2vec uses but let's, um, rather than having word representations,",
    "start": "4290280",
    "end": "4297795"
  },
  {
    "text": "let's start with character sequences and run a bidirectional LSTM to work out word representations,",
    "start": "4297795",
    "end": "4307425"
  },
  {
    "text": "and we'll then sort of be effectively training this more complex model where we're learning",
    "start": "4307425",
    "end": "4312780"
  },
  {
    "text": "character embeddings and LSTM parameters and that will give us our word representations.",
    "start": "4312780",
    "end": "4320550"
  },
  {
    "text": "And that's an idea that people have continued to play with, and so in particular I just wanted to mention these FastText embeddings.",
    "start": "4320550",
    "end": "4328845"
  },
  {
    "text": "Um, so a couple of years ago, um, people now at Facebook, the same Tomas Mikolov who did the original Word2vec,",
    "start": "4328845",
    "end": "4336014"
  },
  {
    "text": "brought out a new set of embeddings, the FastText embeddings and their goal was to sort of",
    "start": "4336015",
    "end": "4341340"
  },
  {
    "text": "have a next-generation Word2vec, um, which is sort of an efficient fast, um,",
    "start": "4341340",
    "end": "4347940"
  },
  {
    "text": "word vector learning library, um, but it was better for rare words and languages with lots of morphology.",
    "start": "4347940",
    "end": "4355215"
  },
  {
    "text": "And the way they did it was that they sort of essentially took the Word2vec skip gram model but they augmented it to put in character n-grams.",
    "start": "4355215",
    "end": "4363585"
  },
  {
    "text": "So more precisely, this is what they did. So, um, when you had a word,",
    "start": "4363585",
    "end": "4369720"
  },
  {
    "text": "my example word is where, for some n-gram size you represent it as a set of n-gram.",
    "start": "4369720",
    "end": "4377610"
  },
  {
    "text": "So this is kind of just about like those, we called phonemes I mentioned right at the beginning where you have a kind of a boundary symbol,",
    "start": "4377610",
    "end": "4385080"
  },
  {
    "text": "so you know the beginning of the word. So if the length is three you have beginning of word WH, WHE, HER,",
    "start": "4385080",
    "end": "4391965"
  },
  {
    "text": "ERE, RE end of word, as pieces of representation.",
    "start": "4391965",
    "end": "4397695"
  },
  {
    "text": "And then you have an additional one for just the whole word. So you do still have whole word representations in this model.",
    "start": "4397695",
    "end": "4404355"
  },
  {
    "text": "So where is represented by six things and so then you're going to use all six of those things in your computation.",
    "start": "4404355",
    "end": "4414350"
  },
  {
    "text": "Um, so if you sort of remember the guts of Word2vec that what you were doing was you were doing",
    "start": "4414350",
    "end": "4420590"
  },
  {
    "text": "these vector dot products between your context representation and your center word representation.",
    "start": "4420590",
    "end": "4428239"
  },
  {
    "text": "So they're going to do exactly the same thing but for the center word they're gonna use all six of these vectors.",
    "start": "4428240",
    "end": "4436870"
  },
  {
    "text": "All the vectors corresponding to all six of these representations and they're going to sum them.",
    "start": "4436870",
    "end": "4443685"
  },
  {
    "text": "And so you're just doing a simple summing operation, and that's sort of then giving you your representation of similarity.",
    "start": "4443685",
    "end": "4450765"
  },
  {
    "text": "Um, very precisely, they don't quite do that because there's a hashing trick but I'll leave that out. But what they're able to show is that that model actually works pretty successfully.",
    "start": "4450765",
    "end": "4461369"
  },
  {
    "text": "So these are words similarity scores, skip gram, they're all CBOW,",
    "start": "4461370",
    "end": "4468480"
  },
  {
    "text": "and then this is the sort of new model, um, that, um, uses these kind of n-grams.",
    "start": "4468480",
    "end": "4476925"
  },
  {
    "text": "And in this, um, you know at least for one of the English data sets, it doesn't get any better.",
    "start": "4476925",
    "end": "4482235"
  },
  {
    "text": "Um, but what they especially notice this is for languages that have more,",
    "start": "4482235",
    "end": "4487830"
  },
  {
    "text": "morp- more morphology that you're sort of getting some fairly clear gains. 70, 69 onto 75,",
    "start": "4487830",
    "end": "4495375"
  },
  {
    "text": "59, 60 on to 66 in the right column, so then these wordpiece models do give them a better model of",
    "start": "4495375",
    "end": "4502440"
  },
  {
    "text": "words and just practically FastText, um, library now has sort of word embeddings for about 60 or 70 different languages,",
    "start": "4502440",
    "end": "4512940"
  },
  {
    "text": "so it's sort of a good source of word embeddings for multilingual applications. Okay, I think I am done.",
    "start": "4512940",
    "end": "4519735"
  },
  {
    "text": "So thanks a lot and see you again next week.",
    "start": "4519735",
    "end": "4523000"
  }
]