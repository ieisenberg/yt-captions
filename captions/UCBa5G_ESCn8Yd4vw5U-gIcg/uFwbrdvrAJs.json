[
  {
    "start": "0",
    "end": "650"
  },
  {
    "text": "Here, we're going\nto see situations",
    "start": "650",
    "end": "2570"
  },
  {
    "text": "where our nearest neighbor\naveraging doesn't work so well,",
    "start": "2570",
    "end": "5390"
  },
  {
    "text": "and we're going to have to\nfind ways to deal with that.",
    "start": "5390",
    "end": "9715"
  },
  {
    "text": "Nearest neighbor averaging,\nwhich is the one we just saw,",
    "start": "9715",
    "end": "12090"
  },
  {
    "text": "can be pretty good for small p.",
    "start": "12090",
    "end": "13980"
  },
  {
    "text": "Small numbers of variables.",
    "start": "13980",
    "end": "15330"
  },
  {
    "text": "Here, we had just one variable.",
    "start": "15330",
    "end": "17369"
  },
  {
    "text": "But small maybe p less than\nor equal to 4 and large-ish N.",
    "start": "17370",
    "end": "22380"
  },
  {
    "text": "Not large N so that we have\nenough points in each neighbor",
    "start": "22380",
    "end": "25800"
  },
  {
    "text": "to average to give\nus our estimate.",
    "start": "25800",
    "end": "28500"
  },
  {
    "text": "Now, this is just one\nversion of a whole class",
    "start": "28500",
    "end": "31619"
  },
  {
    "text": "of techniques called smoothers.",
    "start": "31620",
    "end": "33910"
  },
  {
    "text": "And we're going to discuss later\non in this course, much cleverer",
    "start": "33910",
    "end": "38910"
  },
  {
    "text": "ways of doing this averaging\nsuch as kernel and spline",
    "start": "38910",
    "end": "41760"
  },
  {
    "text": "smoothing.",
    "start": "41760",
    "end": "42519"
  },
  {
    "start": "42520",
    "end": "46010"
  },
  {
    "text": "Now, there's a problem though.",
    "start": "46010",
    "end": "47879"
  },
  {
    "text": "Nearest neighbor methods can be\nreally lousy when p is large.",
    "start": "47880",
    "end": "51680"
  },
  {
    "text": "And the reason has got the name\nthe curse of dimensionality.",
    "start": "51680",
    "end": "56270"
  },
  {
    "text": "What it boils down to is\nthat nearest neighbors",
    "start": "56270",
    "end": "58430"
  },
  {
    "text": "tend to be far away\nin high dimensions.",
    "start": "58430",
    "end": "62750"
  },
  {
    "text": "And that creates a problem.",
    "start": "62750",
    "end": "66290"
  },
  {
    "text": "We need to get a reasonable\nfraction of the end values of yi",
    "start": "66290",
    "end": "70220"
  },
  {
    "text": "to average to bring\nthe variance down.",
    "start": "70220",
    "end": "72570"
  },
  {
    "text": "So we need to average a number\nof points in each neighborhood",
    "start": "72570",
    "end": "76640"
  },
  {
    "text": "so that our estimate has got a\nnice, reasonably small variance.",
    "start": "76640",
    "end": "83390"
  },
  {
    "text": "And let's suppose we\nwant 10% of the data",
    "start": "83390",
    "end": "85729"
  },
  {
    "text": "points to be in each interval.",
    "start": "85730",
    "end": "88020"
  },
  {
    "text": "The problem is that 10%\nneighborhood in high dimensions",
    "start": "88020",
    "end": "90930"
  },
  {
    "text": "need no longer be local.",
    "start": "90930",
    "end": "92560"
  },
  {
    "text": "So we lose the\nspirit of estimating",
    "start": "92560",
    "end": "94380"
  },
  {
    "text": "the conditional expectation\nby local averaging.",
    "start": "94380",
    "end": "97170"
  },
  {
    "text": "So let's look at a\nlittle example of that.",
    "start": "97170",
    "end": "100020"
  },
  {
    "text": "In the left panel,\nwe've got values",
    "start": "100020",
    "end": "104100"
  },
  {
    "text": "of two variables, x1 and x2.",
    "start": "104100",
    "end": "106350"
  },
  {
    "text": "And they actually uniformly\ndistributed in this little cube",
    "start": "106350",
    "end": "110549"
  },
  {
    "text": "with edges minus 1 to\nplus 1, minus 1 to plus 1.",
    "start": "110550",
    "end": "114960"
  },
  {
    "text": "And we form two 10%\nneighborhoods in this case.",
    "start": "114960",
    "end": "119550"
  },
  {
    "text": "The first neighborhood is just\ninvolving the variable x1,",
    "start": "119550",
    "end": "124620"
  },
  {
    "text": "ignoring x2.",
    "start": "124620",
    "end": "126330"
  },
  {
    "text": "And so that's indicated by\nthe vertical dotted lines.",
    "start": "126330",
    "end": "130169"
  },
  {
    "text": "Our target point is at 0.",
    "start": "130169",
    "end": "133170"
  },
  {
    "text": "And so we spread\nout a neighborhood",
    "start": "133170",
    "end": "136110"
  },
  {
    "text": "to the left and right\nuntil we capture",
    "start": "136110",
    "end": "138120"
  },
  {
    "text": "10% of the data points with\nrespect to the variable x1.",
    "start": "138120",
    "end": "142390"
  },
  {
    "text": "And the dotted line indicates\nthe width of the neighborhood.",
    "start": "142390",
    "end": "146430"
  },
  {
    "text": "Alternatively, if we want to\nfind a neighborhood in two",
    "start": "146430",
    "end": "149519"
  },
  {
    "text": "dimensions, we\nspread out a circle",
    "start": "149520",
    "end": "152310"
  },
  {
    "text": "centered at the target point,\nwhich is the red dot there",
    "start": "152310",
    "end": "155040"
  },
  {
    "text": "until we've captured\n10% of the points.",
    "start": "155040",
    "end": "157709"
  },
  {
    "text": "Now, notice the radius of\nthe circle in two dimensions",
    "start": "157710",
    "end": "161010"
  },
  {
    "text": "is much bigger than the\nradius of the circle in one",
    "start": "161010",
    "end": "165299"
  },
  {
    "text": "dimensions, which is just\nthe width between these two",
    "start": "165300",
    "end": "167970"
  },
  {
    "text": "dotted lines.",
    "start": "167970",
    "end": "169170"
  },
  {
    "text": "And so to capture 10% of the\npoints in two dimensions,",
    "start": "169170",
    "end": "172290"
  },
  {
    "text": "we have to go out\nfurther and less--",
    "start": "172290",
    "end": "174760"
  },
  {
    "text": "so we're less local than\nwe are in one dimension.",
    "start": "174760",
    "end": "178230"
  },
  {
    "text": "And so we can take\nthis example further.",
    "start": "178230",
    "end": "180940"
  },
  {
    "text": "And on the right hand\nplot, I've shown you",
    "start": "180940",
    "end": "184470"
  },
  {
    "text": "how far you have to go out in\n1, 2, 3, 5, and 10 dimensions.",
    "start": "184470",
    "end": "192190"
  },
  {
    "text": "These are different\nversions of this problem",
    "start": "192190",
    "end": "194250"
  },
  {
    "text": "as we go as the\ndimensions get higher",
    "start": "194250",
    "end": "196410"
  },
  {
    "text": "in order to capture a certain\nfraction of the volume.",
    "start": "196410",
    "end": "202080"
  },
  {
    "text": "And so take, for example, 10%\nor 0.1 fraction of the volume.",
    "start": "202080",
    "end": "208570"
  },
  {
    "text": "So for p equals 1, if\nthe data is uniform,",
    "start": "208570",
    "end": "211690"
  },
  {
    "text": "you roughly go out\n10% of the distance.",
    "start": "211690",
    "end": "217240"
  },
  {
    "text": "In two dimensions we\nsaw you went more.",
    "start": "217240",
    "end": "219920"
  },
  {
    "text": "Look what happens\nin 5 dimensions.",
    "start": "219920",
    "end": "222950"
  },
  {
    "text": "In 5 dimensions you\nhave to go out about 0.9",
    "start": "222950",
    "end": "226900"
  },
  {
    "text": "on each coordinate axis\nto get 10% of the data.",
    "start": "226900",
    "end": "229819"
  },
  {
    "text": "That's just about the\nwhole radius of the sphere.",
    "start": "229820",
    "end": "232600"
  },
  {
    "text": "And in 10 dimensions,\nyou actually",
    "start": "232600",
    "end": "234310"
  },
  {
    "text": "have to go break\nout of the sphere",
    "start": "234310",
    "end": "236680"
  },
  {
    "text": "in order to get points in\nthe corner to capture the 10%",
    "start": "236680",
    "end": "240040"
  },
  {
    "text": "So the bottom line here\nis it's really hard",
    "start": "240040",
    "end": "243730"
  },
  {
    "text": "to find near neighborhoods in\nhigh dimensions and stay local.",
    "start": "243730",
    "end": "249370"
  },
  {
    "text": "If this problem\ndidn't exist, we would",
    "start": "249370",
    "end": "251650"
  },
  {
    "text": "use near neighbor\naveraging as the sole basis",
    "start": "251650",
    "end": "255819"
  },
  {
    "text": "for doing estimation.",
    "start": "255820",
    "end": "257239"
  },
  {
    "start": "257240",
    "end": "260148"
  },
  {
    "text": "So how do we deal with this?",
    "start": "260149",
    "end": "262109"
  },
  {
    "text": "Well, we introduce\nstructure to our models.",
    "start": "262110",
    "end": "265050"
  },
  {
    "text": "And so the simplest structural\nmodel is a linear model.",
    "start": "265050",
    "end": "269830"
  },
  {
    "text": "And so here we have an\nexample of a linear model.",
    "start": "269830",
    "end": "273970"
  },
  {
    "text": "We've got p features.",
    "start": "273970",
    "end": "275680"
  },
  {
    "text": "It's just got p\nplus 1 parameters",
    "start": "275680",
    "end": "277780"
  },
  {
    "text": "and it says the\nfunction of X, we're",
    "start": "277780",
    "end": "281230"
  },
  {
    "text": "going to approximate\nby a linear function.",
    "start": "281230",
    "end": "283630"
  },
  {
    "text": "So there's a coefficient on each\nof the X's and an intercept.",
    "start": "283630",
    "end": "286640"
  },
  {
    "text": "So that's one of the\nsimplest structural models.",
    "start": "286640",
    "end": "292190"
  },
  {
    "text": "We can estimate the\nparameters of the model",
    "start": "292190",
    "end": "294950"
  },
  {
    "text": "by fitting the model\nto training data,",
    "start": "294950",
    "end": "297140"
  },
  {
    "text": "and we'll be talking more\nabout how you do that.",
    "start": "297140",
    "end": "300990"
  },
  {
    "text": "And when we use such a\nstructural model and of course,",
    "start": "300990",
    "end": "304465"
  },
  {
    "text": "this structural model\nis going to avoid",
    "start": "304465",
    "end": "306090"
  },
  {
    "text": "the curse of dimensionality\nbecause it's not",
    "start": "306090",
    "end": "307949"
  },
  {
    "text": "relying on any local properties\nand nearest neighbor averaging.",
    "start": "307950",
    "end": "312670"
  },
  {
    "text": "It's just fitting quite a\nrigid model to all the data.",
    "start": "312670",
    "end": "316410"
  },
  {
    "text": "Now, a linear model is\nalmost never correct.",
    "start": "316410",
    "end": "321300"
  },
  {
    "text": "But it often serves as\na good approximation,",
    "start": "321300",
    "end": "323729"
  },
  {
    "text": "an interpretable approximation,\nto the unknown true function",
    "start": "323730",
    "end": "326790"
  },
  {
    "text": "f of X.",
    "start": "326790",
    "end": "330050"
  },
  {
    "text": "So here's our same\nlittle example data set.",
    "start": "330050",
    "end": "334550"
  },
  {
    "text": "And we can see in the\ntop plot, a linear model",
    "start": "334550",
    "end": "338180"
  },
  {
    "text": "gives a reasonable\nfit to the data.",
    "start": "338180",
    "end": "340160"
  },
  {
    "text": "Not perfect.",
    "start": "340160",
    "end": "341630"
  },
  {
    "text": "In the bottom plot, we've\naugmented our linear model",
    "start": "341630",
    "end": "344930"
  },
  {
    "text": "and we've included\na quadratic term.",
    "start": "344930",
    "end": "347550"
  },
  {
    "text": "So we put in our X and we\nput in an X squared as well.",
    "start": "347550",
    "end": "352470"
  },
  {
    "text": "And we see that fits\nthe data much better.",
    "start": "352470",
    "end": "355460"
  },
  {
    "text": "It's also a linear model,\nbut it's linear in,",
    "start": "355460",
    "end": "357830"
  },
  {
    "text": "in some transformed values\nof X. And notice now",
    "start": "357830",
    "end": "361669"
  },
  {
    "text": "we've put hats on\neach of the parameters",
    "start": "361670",
    "end": "364820"
  },
  {
    "text": "suggesting they've\nbeen estimated",
    "start": "364820",
    "end": "366650"
  },
  {
    "text": "in this case from\nthese training data.",
    "start": "366650",
    "end": "370009"
  },
  {
    "text": "These little hats indicate\nthat they've been estimated",
    "start": "370010",
    "end": "372410"
  },
  {
    "text": "from the training data.",
    "start": "372410",
    "end": "374800"
  },
  {
    "text": "So those are two\nparametric models,",
    "start": "374800",
    "end": "376930"
  },
  {
    "text": "structured models that seem\nto do a good job in this case.",
    "start": "376930",
    "end": "381880"
  },
  {
    "text": "Here is a\ntwo-dimensional example.",
    "start": "381880",
    "end": "387070"
  },
  {
    "text": "Again, seniority, years\nof education, and income.",
    "start": "387070",
    "end": "390910"
  },
  {
    "text": "And this is simulated data, and\nso the blue surface is actually",
    "start": "390910",
    "end": "395560"
  },
  {
    "text": "showing you the true function\nfrom which the data was",
    "start": "395560",
    "end": "398470"
  },
  {
    "text": "simulated with errors.",
    "start": "398470",
    "end": "400100"
  },
  {
    "text": "We can see the errors aren't\nbig, each of those data points",
    "start": "400100",
    "end": "404980"
  },
  {
    "text": "comes from a particular\npair of years of education",
    "start": "404980",
    "end": "408160"
  },
  {
    "text": "and seniority, with some error.",
    "start": "408160",
    "end": "410320"
  },
  {
    "text": "And the little line\nsegments in the data points",
    "start": "410320",
    "end": "413380"
  },
  {
    "text": "show you the error.",
    "start": "413380",
    "end": "415210"
  },
  {
    "text": "So we can write that as income\nis a function of education",
    "start": "415210",
    "end": "418780"
  },
  {
    "text": "and seniority plus some error.",
    "start": "418780",
    "end": "421330"
  },
  {
    "text": "So this is the\ntruth, we actually",
    "start": "421330",
    "end": "422889"
  },
  {
    "text": "know these in this case.",
    "start": "422890",
    "end": "424690"
  },
  {
    "text": "And here's a linear\nregression model",
    "start": "424690",
    "end": "427240"
  },
  {
    "text": "fit to those simulation data.",
    "start": "427240",
    "end": "429220"
  },
  {
    "text": "So it's an approximation.",
    "start": "429220",
    "end": "432080"
  },
  {
    "text": "It captures the important\nelements of the relationship,",
    "start": "432080",
    "end": "437080"
  },
  {
    "text": "but it doesn't\ncapture everything.",
    "start": "437080",
    "end": "439220"
  },
  {
    "text": "It's got three parameters.",
    "start": "439220",
    "end": "443080"
  },
  {
    "text": "Here's a more flexible\nregression model.",
    "start": "443080",
    "end": "446470"
  },
  {
    "text": "We've actually fit this\nusing a technique called",
    "start": "446470",
    "end": "448870"
  },
  {
    "text": "thin-plate splines, and\nthat's a nice, smooth version",
    "start": "448870",
    "end": "453910"
  },
  {
    "text": "of a two-dimensional, smoother.",
    "start": "453910",
    "end": "457310"
  },
  {
    "text": "It's different from\nnearest neighbor averaging,",
    "start": "457310",
    "end": "460270"
  },
  {
    "text": "and it's got some\nnicer properties.",
    "start": "460270",
    "end": "462340"
  },
  {
    "text": "And you can see, this\ndoes a pretty good job.",
    "start": "462340",
    "end": "464630"
  },
  {
    "text": "If we go back to the generating\ndata and the generating surface,",
    "start": "464630",
    "end": "470740"
  },
  {
    "text": "this thin-plate spline\nactually captures",
    "start": "470740",
    "end": "473949"
  },
  {
    "text": "more of the essence of\nwhat's going on there.",
    "start": "473950",
    "end": "476800"
  },
  {
    "text": "And for thin-plate\nsplines, we're",
    "start": "476800",
    "end": "479409"
  },
  {
    "text": "going to talk about\nthem later in chapter 7.",
    "start": "479410",
    "end": "481670"
  },
  {
    "text": "There's a tuning\nparameter that controls",
    "start": "481670",
    "end": "484600"
  },
  {
    "text": "how smooth the surface is.",
    "start": "484600",
    "end": "488120"
  },
  {
    "text": "Here's an example of\na thin-plate spline",
    "start": "488120",
    "end": "490340"
  },
  {
    "text": "where we basically tune the\nparameter all the way down to 0.",
    "start": "490340",
    "end": "495110"
  },
  {
    "text": "And this surface actually goes\nthrough every single data point.",
    "start": "495110",
    "end": "500319"
  },
  {
    "text": "In this case, that's\noverfitting the data,",
    "start": "500320",
    "end": "503750"
  },
  {
    "text": "we expect to have some errors\nbecause the true function",
    "start": "503750",
    "end": "507640"
  },
  {
    "text": "generate data\npoints with errors.",
    "start": "507640",
    "end": "509590"
  },
  {
    "text": "So this is known\nas overfitting--",
    "start": "509590",
    "end": "511810"
  },
  {
    "text": "overfitting the training data.",
    "start": "511810",
    "end": "514860"
  },
  {
    "text": "So this is an example where\nwe've got a family of functions",
    "start": "514860",
    "end": "518279"
  },
  {
    "text": "and we've got a way of\ncontrolling the complexity.",
    "start": "518280",
    "end": "523469"
  },
  {
    "text": "So there are some trade-offs\nwhen building models.",
    "start": "523470",
    "end": "528060"
  },
  {
    "text": "One trade-off is\nprediction accuracy",
    "start": "528060",
    "end": "530310"
  },
  {
    "text": "versus interpretability.",
    "start": "530310",
    "end": "532120"
  },
  {
    "text": "So linear models are\neasy to interpret.",
    "start": "532120",
    "end": "534610"
  },
  {
    "text": "We've just got a few parameters.",
    "start": "534610",
    "end": "536200"
  },
  {
    "text": "Thin-plate splines are not.",
    "start": "536200",
    "end": "538030"
  },
  {
    "text": "They give you a\nwhole surface back.",
    "start": "538030",
    "end": "540340"
  },
  {
    "text": "And if you get given a\nsurface back in 10 dimensions,",
    "start": "540340",
    "end": "544470"
  },
  {
    "text": "it's hard to understand what\nit's actually telling you.",
    "start": "544470",
    "end": "550449"
  },
  {
    "text": "We can have a good fit\nversus over-fit or under-fit.",
    "start": "550450",
    "end": "555220"
  },
  {
    "text": "So in this previous example,\nthe middle one was a good fit,",
    "start": "555220",
    "end": "558439"
  },
  {
    "text": "the linear was\nslightly under-fit,",
    "start": "558440",
    "end": "560200"
  },
  {
    "text": "and the last one was over-fit.",
    "start": "560200",
    "end": "562750"
  },
  {
    "text": "So how do we know when\nthe fit is just right?",
    "start": "562750",
    "end": "565540"
  },
  {
    "text": "We need to be able to\nselect amongst those.",
    "start": "565540",
    "end": "570759"
  },
  {
    "text": "Parsimony versus black-box.",
    "start": "570760",
    "end": "573280"
  },
  {
    "text": "Parsimony means having\na model that's simpler",
    "start": "573280",
    "end": "577300"
  },
  {
    "text": "and that can be transmitted with\na small number of parameters",
    "start": "577300",
    "end": "583870"
  },
  {
    "text": "and explain in a simple\nfashion and involve maybe",
    "start": "583870",
    "end": "586330"
  },
  {
    "text": "a subset of the predictors.",
    "start": "586330",
    "end": "590770"
  },
  {
    "text": "And so those models,\nif they do as well as,",
    "start": "590770",
    "end": "594760"
  },
  {
    "text": "say, a black-box predictor,\nlike the thin-plate spline was",
    "start": "594760",
    "end": "597730"
  },
  {
    "text": "somewhat of a\nblack-box predictor,",
    "start": "597730",
    "end": "599449"
  },
  {
    "text": "we'd prefer the simpler model.",
    "start": "599450",
    "end": "602980"
  },
  {
    "text": "Here's a little schematic which\nshows a variety of the methods",
    "start": "602980",
    "end": "608260"
  },
  {
    "text": "that we're going to be\ndiscussing in this course.",
    "start": "608260",
    "end": "611530"
  },
  {
    "text": "And they ordered\nby interpretability",
    "start": "611530",
    "end": "614170"
  },
  {
    "text": "and flexibility.",
    "start": "614170",
    "end": "616120"
  },
  {
    "text": "And at the top, there are two\nversions of linear regression,",
    "start": "616120",
    "end": "622140"
  },
  {
    "text": "subset selection, and lasso,\nwhich we'll talk about.",
    "start": "622140",
    "end": "625320"
  },
  {
    "text": "That actually even think\nthe linear regression models",
    "start": "625320",
    "end": "628320"
  },
  {
    "text": "too complex and\ntry and reduce it",
    "start": "628320",
    "end": "631350"
  },
  {
    "text": "by throwing out some\nof the variables.",
    "start": "631350",
    "end": "634470"
  },
  {
    "text": "Linear models and least\nsquares, slightly more flexible,",
    "start": "634470",
    "end": "641490"
  },
  {
    "text": "but you lose some\ninterpretability",
    "start": "641490",
    "end": "643890"
  },
  {
    "text": "because now all the\nvariables are thrown in.",
    "start": "643890",
    "end": "646260"
  },
  {
    "text": "Then we have generalized\nadditive models,",
    "start": "646260",
    "end": "649020"
  },
  {
    "text": "which allow for transformations\nin an automatic way of each",
    "start": "649020",
    "end": "652470"
  },
  {
    "text": "of the variables and then\nat the high flexibility,",
    "start": "652470",
    "end": "656610"
  },
  {
    "text": "low interpretability,\nand bagging, boosting,",
    "start": "656610",
    "end": "659550"
  },
  {
    "text": "and support vector machines.",
    "start": "659550",
    "end": "661220"
  },
  {
    "text": "We'll discuss all\nthese techniques",
    "start": "661220",
    "end": "663300"
  },
  {
    "text": "but later on in the course.",
    "start": "663300",
    "end": "666800"
  },
  {
    "text": "So we covered linear\nregression and we",
    "start": "666800",
    "end": "668839"
  },
  {
    "text": "covered nearest neighbor\naveraging and we",
    "start": "668840",
    "end": "671480"
  },
  {
    "text": "talked about ways, places\nwhere that's not going to work.",
    "start": "671480",
    "end": "675420"
  },
  {
    "text": "And so we've briefly\nintroduced a number",
    "start": "675420",
    "end": "678589"
  },
  {
    "text": "of other different\nmethods and they're all",
    "start": "678590",
    "end": "680510"
  },
  {
    "text": "listed on the screen,\ndifferent methods",
    "start": "680510",
    "end": "683930"
  },
  {
    "text": "that we can use to solve the\nproblem when the dimension is",
    "start": "683930",
    "end": "687290"
  },
  {
    "text": "high and when\nlinearity doesn't work.",
    "start": "687290",
    "end": "691320"
  },
  {
    "text": "But we have to choose\namongst these methods.",
    "start": "691320",
    "end": "693360"
  },
  {
    "text": "And so we need to develop\nways of making those choices.",
    "start": "693360",
    "end": "696959"
  },
  {
    "text": "And that's what we're going\nto cover in the next segment.",
    "start": "696960",
    "end": "700480"
  },
  {
    "start": "700480",
    "end": "701000"
  }
]