[
  {
    "start": "0",
    "end": "5550"
  },
  {
    "text": "Hello, everyone, and\nwelcome back into week four. So for week four, it's\ngoing to come in two halves.",
    "start": "5550",
    "end": "14620"
  },
  {
    "text": "So today, I'm going to talk\nabout machine translation related topics. And then in the second\nhalf of the week,",
    "start": "14620",
    "end": "21810"
  },
  {
    "text": "we take a little bit of a break\nfrom learning more and more on neural network\ntopics, and talk",
    "start": "21810",
    "end": "27840"
  },
  {
    "text": "about final projects, but\nalso some practical tips for building neural\nnetwork systems.",
    "start": "27840",
    "end": "35020"
  },
  {
    "text": "So for today's lecture,\nthis is an important content full lecture.",
    "start": "35020",
    "end": "40480"
  },
  {
    "text": "So first of all, I'm going to\nintroduce a new task, machine translation.",
    "start": "40480",
    "end": "45989"
  },
  {
    "text": "And it turns out that\nour task is a major use case of a new\narchitectural technique",
    "start": "45990",
    "end": "53820"
  },
  {
    "text": "to teach you about\ndeep learning, which is sequence to sequence models. And so we'll spend a\nlot of time on those.",
    "start": "53820",
    "end": "60420"
  },
  {
    "text": "And then there's a\ncrucial way that's been developed to\nimprove sequence",
    "start": "60420",
    "end": "65580"
  },
  {
    "text": "to sequence models, which\nis the idea of attention. And so that's what I'll\ntalk about in the final part",
    "start": "65580",
    "end": "72090"
  },
  {
    "text": "of the class. I'm just checking\neveryone's keeping up with what's happening.",
    "start": "72090",
    "end": "78370"
  },
  {
    "text": "So first of all,\nassignment 3 is due today.",
    "start": "78370",
    "end": "84090"
  },
  {
    "text": "So hopefully you've all got in\nyour neural dependency parses parsing text well.",
    "start": "84090",
    "end": "89580"
  },
  {
    "text": "At the same time,\nassignment 4 is out today. And really today's lecture\nis the primary content",
    "start": "89580",
    "end": "96510"
  },
  {
    "text": "for what you'll be using for\nbuilding your assignment 4 systems. Switching it up for a little.",
    "start": "96510",
    "end": "103140"
  },
  {
    "text": "For assignment 4, we give\nyou a mighty two extra days. So you get nine days for it.",
    "start": "103140",
    "end": "109140"
  },
  {
    "text": "And it's due on Thursday. On the other hand,\ndo please be aware",
    "start": "109140",
    "end": "115380"
  },
  {
    "text": "that assignment 4\nis bigger and harder than the previous assignments.",
    "start": "115380",
    "end": "121570"
  },
  {
    "text": "So do make sure you get\nstarted on it early. And then as I mentioned Thursday\nI'll turn to final projects.",
    "start": "121570",
    "end": "127470"
  },
  {
    "text": " OK. So let's get straight into\nthis with machine translation.",
    "start": "127470",
    "end": "136590"
  },
  {
    "text": "So very quickly, I\nwanted to tell you a little bit about where we were\nand what we did before we get",
    "start": "136590",
    "end": "145159"
  },
  {
    "text": "to neural machine translation. And so let's do the prehistory\nof machine translation.",
    "start": "145160",
    "end": "151650"
  },
  {
    "text": "So machine translation\nis the task of translating a sentence\nx from one language",
    "start": "151650",
    "end": "157730"
  },
  {
    "text": "which is called\nthe source language to another language, the target\nlanguage forming a sentence y.",
    "start": "157730",
    "end": "164510"
  },
  {
    "text": "So we start off with a\nsource language sentence x. L'homme, and then we\ntranslate it and we get out",
    "start": "164510",
    "end": "174379"
  },
  {
    "text": "the translation\nman is born free, but everywhere he is in chains. OK.",
    "start": "174380",
    "end": "179690"
  },
  {
    "text": "So there's our\nmachine translation. OK. So in the early\n1950s, there started",
    "start": "179690",
    "end": "185630"
  },
  {
    "text": "to be work on\nmachine translation. And so it's actually a\nthing about computer science",
    "start": "185630",
    "end": "191570"
  },
  {
    "text": "if you find things that\nhave machine in the name, most of them are old things. ",
    "start": "191570",
    "end": "198410"
  },
  {
    "text": "And this really kind of\ncame about in the US context in the context of the Cold War.",
    "start": "198410",
    "end": "204920"
  },
  {
    "text": "So there was this\ndesire to keep tabs on what the Russians were doing. And people had the idea that\nbecause some of the earliest",
    "start": "204920",
    "end": "212660"
  },
  {
    "text": "computers had been so successful\nat doing code breaking during the Second World War.",
    "start": "212660",
    "end": "219200"
  },
  {
    "text": "Then maybe we could\nset early computers to work during the Cold\nWar to do translation.",
    "start": "219200",
    "end": "227270"
  },
  {
    "text": "And hopefully this will play\nand you'll be able to hear it. Here's a little\nvideo clip showing some of the earliest work in\nmachine translation from 1954.",
    "start": "227270",
    "end": "242340"
  },
  {
    "text": "They hadn't reckoned\nwith ambiguity when they set out\nto use computers to translate languages.",
    "start": "242340",
    "end": "248040"
  },
  {
    "text": "$500,000 simple calculator,\nmost versatile electronic brain known, translates\nRussian into English.",
    "start": "248040",
    "end": "255390"
  },
  {
    "text": "Instead of mathematical\nwizardry, a sentence in Russian, it could be-- One of the first non-numerical\napplications of computers,",
    "start": "255390",
    "end": "262410"
  },
  {
    "text": "it was hyped as the\nsolution to the Cold War obsession of keeping tabs on\nwhat the Russians were doing.",
    "start": "262410",
    "end": "268003"
  },
  {
    "text": "Claims were made that\nthe computer would replace most human translators. Then of course, you're just\nin the experimental stage.",
    "start": "268003",
    "end": "274800"
  },
  {
    "text": "When you go in for\nfull scale production what will the capacity be? We should be able\nto do about with",
    "start": "274800",
    "end": "279965"
  },
  {
    "text": "a modern commercial\ncomputer about one to two million words an hour.",
    "start": "279965",
    "end": "285000"
  },
  {
    "text": "And this will be quite\nan adequate speed to cope with the whole\noutput of the Soviet Union in just a few hours of\ncomputer time a week.",
    "start": "285000",
    "end": "292336"
  },
  {
    "text": "When do you hope to be\nable to achieve this speed? I our experiments go well,\nthen perhaps within five years",
    "start": "292337",
    "end": "297876"
  },
  {
    "text": "or so. And finally, Mr.\nMcDaniel, does this mean the end of human translators?",
    "start": "297876",
    "end": "303300"
  },
  {
    "text": "I say yes for translators\nof scientific and technical material. But as regards to\npoetry and novels,",
    "start": "303300",
    "end": "309090"
  },
  {
    "text": "no, I don't think we'll\never replace the translators of that type of material. Mr. McDaniel, thank\nyou very much.",
    "start": "309090",
    "end": "315510"
  },
  {
    "text": "But despite the hype it\nran into deep trouble. ",
    "start": "315510",
    "end": "320670"
  },
  {
    "text": "Yeah. So the experiments\ndid not go well. ",
    "start": "320670",
    "end": "327280"
  },
  {
    "text": "And so in retrospect,\nit's not very surprising that the early work did\nnot work out very well.",
    "start": "327280",
    "end": "336039"
  },
  {
    "text": "I mean, this was in the\nsort of really beginning of the computer\nage in the 1950s. That it was also the\nbeginning of people starting",
    "start": "336040",
    "end": "344910"
  },
  {
    "text": "to understand the science\nof human languages, the field of linguistics. So really people had\nnot much understanding",
    "start": "344910",
    "end": "351570"
  },
  {
    "text": "of either side of\nwhat was happening. So what you had was\npeople were trying",
    "start": "351570",
    "end": "356970"
  },
  {
    "text": "to write systems on really\nincredibly primitive computers, right?",
    "start": "356970",
    "end": "362040"
  },
  {
    "text": "It's probably the case that\nnow if you have a USB C power brick, that it has more\ncomputational capacity",
    "start": "362040",
    "end": "369780"
  },
  {
    "text": "inside it, than the\ncomputers that they were using to translate. And so effectively,\nwhat you were getting",
    "start": "369780",
    "end": "376800"
  },
  {
    "text": "were very simple rule based\nsystems and word lookup. So it was sort like,\ndictionary look up a word",
    "start": "376800",
    "end": "383370"
  },
  {
    "text": "and get its translation. But that just didn't work well. Because human languages are\nmuch more complex than that.",
    "start": "383370",
    "end": "390150"
  },
  {
    "text": "Often words have many\nmeanings and different senses as we've sort of\ndiscussed about a bit.",
    "start": "390150",
    "end": "396690"
  },
  {
    "text": "Often there are idioms. You need to\nunderstand the grammar to rewrite the sentences. So for all sorts of reasons,\nit didn't work well.",
    "start": "396690",
    "end": "405120"
  },
  {
    "text": "And this idea was\nlargely canned. In particular, there was a\nfamous US government report",
    "start": "405120",
    "end": "410700"
  },
  {
    "text": "in the mid 1960s, the ALPAC\nreport, which basically concluded this wasn't working.",
    "start": "410700",
    "end": "417650"
  },
  {
    "text": "Oops. OK. Work then did revive\nin AI at doing",
    "start": "417650",
    "end": "425270"
  },
  {
    "text": "rule based methods of machine\ntranslation in the 90s.",
    "start": "425270",
    "end": "430770"
  },
  {
    "text": "But when things\nreally became alive was once you got\ninto the mid 90s,",
    "start": "430770",
    "end": "437720"
  },
  {
    "text": "and when they were in the\nperiod of statistical NLP that we've seen in other\nplaces in the course.",
    "start": "437720",
    "end": "444060"
  },
  {
    "text": "And then the idea began,\ncan we start with just data",
    "start": "444060",
    "end": "449480"
  },
  {
    "text": "about translation i.e. sentences and\ntheir translations,",
    "start": "449480",
    "end": "454610"
  },
  {
    "text": "and learn a\nprobabilistic model that can predict the translations\nof fresh sentences?",
    "start": "454610",
    "end": "460490"
  },
  {
    "text": "So suppose we're translating\nFrench into English. So what we want to do is\nbuild a probabilistic model",
    "start": "460490",
    "end": "467810"
  },
  {
    "text": "that given a French sentence. We can say, what's\nthe probability of different English\ntranslations?",
    "start": "467810",
    "end": "474110"
  },
  {
    "text": "And then we'll choose the\nmost likely translation. We can then found--",
    "start": "474110",
    "end": "479950"
  },
  {
    "text": "it was found felicitous to break\nthis down into two components",
    "start": "479950",
    "end": "485860"
  },
  {
    "text": "by just reversing\nthis with Bayes' rule. So if instead we\nhad a probability",
    "start": "485860",
    "end": "493540"
  },
  {
    "text": "over English sentences\np of y, and then",
    "start": "493540",
    "end": "498760"
  },
  {
    "text": "a probability of\na French sentence given an English\nsentence, that people",
    "start": "498760",
    "end": "504580"
  },
  {
    "text": "were able to make more progress. And it's not\nimmediately obvious as to why this should\nbe because this",
    "start": "504580",
    "end": "509979"
  },
  {
    "text": "is sort of just a trivial\nrewrite with Bayes' rule. That allowed the problem to be\nseparated into two parts which",
    "start": "509980",
    "end": "516969"
  },
  {
    "text": "proved to be more tractable. So on the left hand\nside, you effectively",
    "start": "516970",
    "end": "522549"
  },
  {
    "text": "had a translation model\nwhere you could just give a probability\nof words or phrases",
    "start": "522549",
    "end": "529390"
  },
  {
    "text": "being translated between\nthe two languages without having to bother\nabout the structural word",
    "start": "529390",
    "end": "535780"
  },
  {
    "text": "order of the languages. And then on the right hand,\nyou saw precisely what we spent a long time with last\nweek, which is this is just",
    "start": "535780",
    "end": "544750"
  },
  {
    "text": "a probabilistic language model. So if we have a very\ngood model of what",
    "start": "544750",
    "end": "550149"
  },
  {
    "text": "good fluent English\nsentences sound like, which we can build just\nfrom monolingual data,",
    "start": "550150",
    "end": "556450"
  },
  {
    "text": "we can then get it to make sure\nwe're producing sentences that sound good while the\ntranslation model hopefully",
    "start": "556450",
    "end": "564130"
  },
  {
    "text": "puts the right words into them. So how do we learn\nthe translation model",
    "start": "564130",
    "end": "571230"
  },
  {
    "text": "since we haven't covered that? So the starting point\nwas to get a large amount of parallel data, which is\nhuman translated sentences.",
    "start": "571230",
    "end": "580380"
  },
  {
    "text": "At this point, it's\nmandatory that I show a picture of\nthe Rosetta Stone",
    "start": "580380",
    "end": "585839"
  },
  {
    "text": "which is the famous original\npiece of parallel data that allowed the decoding\nof Egyptian hieroglyphs",
    "start": "585840",
    "end": "594899"
  },
  {
    "text": "because it had the same piece\nof text in different languages. In the modern world, there\nare fortunately for people",
    "start": "594900",
    "end": "602699"
  },
  {
    "text": "who build natural language\nprocessing systems quite a few places, where\nparallel data is",
    "start": "602700",
    "end": "609150"
  },
  {
    "text": "produced in large quantities. So the European Union produces\na huge amount of parallel text",
    "start": "609150",
    "end": "616410"
  },
  {
    "text": "across European languages. The French. Sorry. Not the French.",
    "start": "616410",
    "end": "621930"
  },
  {
    "text": "The Canadian\nParliament conveniently produces parallel text\nbetween French and English,",
    "start": "621930",
    "end": "629610"
  },
  {
    "text": "and even a limited amount in\nInuktitut, Canadian Eskimo.",
    "start": "629610",
    "end": "636510"
  },
  {
    "text": "And then the Hong\nKong parliament produces English and Chinese.",
    "start": "636510",
    "end": "641639"
  },
  {
    "text": "So there's a fair availability\nfrom different sources. And we can use that\nto build models.",
    "start": "641640",
    "end": "648730"
  },
  {
    "text": "So how do we do it though? All we have is these sentences. And it's not quite obvious how\nto build a probabilistic model",
    "start": "648730",
    "end": "656460"
  },
  {
    "text": "out of those. Well, as before, what we want to\ndo is break this problem down.",
    "start": "656460",
    "end": "662680"
  },
  {
    "text": "So in this case, what\nwe're going to do is introduce an extra variable,\nwhich is an alignment variable.",
    "start": "662680",
    "end": "670430"
  },
  {
    "text": "So a is the alignment\nvariable, which is going to give a\nword level or sometimes",
    "start": "670430",
    "end": "676560"
  },
  {
    "text": "phrase level correspondence\nbetween parts of the source sentence\nand the target sentence.",
    "start": "676560",
    "end": "683520"
  },
  {
    "text": "So this is an example\nof an alignment. And so if we could induce\nthis alignment between the two",
    "start": "683520",
    "end": "692460"
  },
  {
    "text": "sentences, then we\ncan have probabilities of pieces of how likely\na word or a short phrase",
    "start": "692460",
    "end": "700680"
  },
  {
    "text": "is translated in\na particular way. And in general, alignment is\nworking out the correspondence",
    "start": "700680",
    "end": "711410"
  },
  {
    "text": "between words that is capturing\nthe grammatical differences",
    "start": "711410",
    "end": "717050"
  },
  {
    "text": "between languages. So words will occur in different\norders in different languages",
    "start": "717050",
    "end": "722630"
  },
  {
    "text": "depending on whether\nit's a language that puts on the subject\nbefore the verb,",
    "start": "722630",
    "end": "727730"
  },
  {
    "text": "or the subject after the\nverb, or the verb before both the subject and the object.",
    "start": "727730",
    "end": "732860"
  },
  {
    "text": "And the alignments will\nalso capture something about differences about the ways\nthat work languages do things.",
    "start": "732860",
    "end": "740399"
  },
  {
    "text": "So what we find is that we get\nevery possibility of how words can align between languages.",
    "start": "740400",
    "end": "747470"
  },
  {
    "text": "So you can have words\nthat don't get translated at all in the other language.",
    "start": "747470",
    "end": "754709"
  },
  {
    "text": "So in French, you put a definite\narticle \"the\" before country names like Japon.",
    "start": "754710",
    "end": "761540"
  },
  {
    "text": "So when that gets translated\nto English, you just get Japan. So there's no\ntranslation of the \"the\".",
    "start": "761540",
    "end": "767460"
  },
  {
    "text": "So it just goes away. On the other hand, you can get\nmany to one translations, where",
    "start": "767460",
    "end": "775220"
  },
  {
    "text": "one French word gets translated\nas several English words.",
    "start": "775220",
    "end": "781189"
  },
  {
    "text": "So for the last\nFrench word, it's been translated as Aboriginal\npeople as multiple words.",
    "start": "781190",
    "end": "789590"
  },
  {
    "text": "You can get the\nreverse, where you can have several\nFrench words that get",
    "start": "789590",
    "end": "794870"
  },
  {
    "text": "translated as one English word. So mis en application is getting\ntranslated as implemented.",
    "start": "794870",
    "end": "802820"
  },
  {
    "text": "And you can get even\nmore complicated ones. So here we sort of have\nfour English words being",
    "start": "802820",
    "end": "811190"
  },
  {
    "text": "translated as two French words. But they don't really break down\nand translate each other well.",
    "start": "811190",
    "end": "817700"
  },
  {
    "text": "I mean, these things don't\nonly happen across languages. They also happen\nwithin the language",
    "start": "817700",
    "end": "822949"
  },
  {
    "text": "when you have different ways\nof saying the same thing. So another way you\nmight have expressed",
    "start": "822950",
    "end": "829640"
  },
  {
    "text": "the poor don't have any money is\nto say the poor are moneyless. And that's much more\nsimilar to how the French is",
    "start": "829640",
    "end": "838010"
  },
  {
    "text": "being rendered here. And so even English\nto English, you have the same kind\nof alignment problem.",
    "start": "838010",
    "end": "845390"
  },
  {
    "text": "So probabilistic or\nstatistical machine translation is more commonly known.",
    "start": "845390",
    "end": "851350"
  },
  {
    "text": "What we wanted to do is\nlearn these alignments. And there's a bunch of sources\nof information you could use.",
    "start": "851350",
    "end": "858970"
  },
  {
    "text": "If you start with\nparallel sentences, you can see how often\nwords and phrases",
    "start": "858970",
    "end": "864580"
  },
  {
    "text": "co-occur in parallel sentences. You can look at their\npositions in the sentence.",
    "start": "864580",
    "end": "870880"
  },
  {
    "text": "And figure out what\nare good alignments. But alignments are\na categorical thing.",
    "start": "870880",
    "end": "877870"
  },
  {
    "text": "They're not probabilistic and\nso they are latent variables.",
    "start": "877870",
    "end": "883370"
  },
  {
    "text": "And so you need to use\nspecial learning algorithms like the expectation\nmaximization algorithm",
    "start": "883370",
    "end": "888610"
  },
  {
    "text": "for learning about\nlatent variables. In the olden days of CS224N\nbefore we start doing it",
    "start": "888610",
    "end": "894430"
  },
  {
    "text": "all with deep\nlearning, we spent tons of CS224N dealing with\nlatent variable algorithms.",
    "start": "894430",
    "end": "902350"
  },
  {
    "text": "But these days, we\ndon't cover that at all. And you're going to have to\ngo off and see CS228 if you",
    "start": "902350",
    "end": "908020"
  },
  {
    "text": "want to know more about that. And we're not\nreally expecting you to understand the details here.",
    "start": "908020",
    "end": "913450"
  },
  {
    "text": "But I did then want to\nsay a bit more about how decoding was done in a\nstatistical machine translation",
    "start": "913450",
    "end": "923600"
  },
  {
    "text": "system. And so what we wanted to do\nis to say we had a translation",
    "start": "923600",
    "end": "929000"
  },
  {
    "text": "model and a language model. And we want to pick\nout the most likely",
    "start": "929000",
    "end": "934790"
  },
  {
    "text": "why there's the translation\nof the sentence. And what kind of process\ncould we use to do that?",
    "start": "934790",
    "end": "943520"
  },
  {
    "text": "Well, the naive thing\nis to say, well, let's just enumerate every\npossible y and calculate",
    "start": "943520",
    "end": "951260"
  },
  {
    "text": "its probability. But we can't possibly\ndo that because there's a number of translation\nsentences in the target",
    "start": "951260",
    "end": "959690"
  },
  {
    "text": "language. That's exponential in the\nlength of the sentence. So that's way too expensive.",
    "start": "959690",
    "end": "965090"
  },
  {
    "text": "So we need to have some\nway to break it down more. And while we had a simple\nway for language models,",
    "start": "965090",
    "end": "973579"
  },
  {
    "text": "we just generated words one at a\ntime and laid out the sentence.",
    "start": "973580",
    "end": "978830"
  },
  {
    "text": "And so that seems a\nreasonable thing to do. But here we need to\ndeal with the fact",
    "start": "978830",
    "end": "984410"
  },
  {
    "text": "that things occur in different\norders in source languages",
    "start": "984410",
    "end": "990110"
  },
  {
    "text": "and in translations.  And so we do want to\nbreak it into pieces",
    "start": "990110",
    "end": "996290"
  },
  {
    "text": "with an independence assumption\nlike the language model. But then we want a\nway of breaking things",
    "start": "996290",
    "end": "1002020"
  },
  {
    "text": "apart and exploring it in what's\ncalled a decoding process. So this is the way it was done.",
    "start": "1002020",
    "end": "1008410"
  },
  {
    "text": "So we start with\na source sentence. So this is a German sentence.",
    "start": "1008410",
    "end": "1013990"
  },
  {
    "text": "And as is standard in German. You're getting this\nsecond position verb.",
    "start": "1013990",
    "end": "1022570"
  },
  {
    "text": "So that's probably not\nin the right position for where the English\ntranslation is going to be.",
    "start": "1022570",
    "end": "1028180"
  },
  {
    "text": "So we might need to\nrearrange the words. So what we have is based\non the translation model.",
    "start": "1028180",
    "end": "1036069"
  },
  {
    "text": "We have words or\nphrases that are reasonably likely translations\nof each German word,",
    "start": "1036069",
    "end": "1045550"
  },
  {
    "text": "or sometimes a German phrase. So these are effectively\nthe LEGO pieces out",
    "start": "1045550",
    "end": "1051310"
  },
  {
    "text": "of which we're going to want\nto create the translation. And so then inside that,\nmaking use of this data,",
    "start": "1051310",
    "end": "1061570"
  },
  {
    "text": "we're going to generate\nthe translation piece by piece kind of like we\ndid with our neural language",
    "start": "1061570",
    "end": "1068350"
  },
  {
    "text": "models. So we going to start with\nan empty translation. And then we're\ngoing to say, well,",
    "start": "1068350",
    "end": "1074559"
  },
  {
    "text": "we want to use one\nof these LEGO pieces. And so we could explore\ndifferent possible ones.",
    "start": "1074560",
    "end": "1082070"
  },
  {
    "text": "So there's a search process. But one of the\npossible pieces is we could translate\n\"er\" with \"he\",",
    "start": "1082070",
    "end": "1088390"
  },
  {
    "text": "or we could start the\nsentence with \"are\" translating the second word. So we could explore various\nlikely possibilities.",
    "start": "1088390",
    "end": "1097510"
  },
  {
    "text": "And if we're guided\nby our language model, it's probably much more likely\nto start the sentence with he",
    "start": "1097510",
    "end": "1103427"
  },
  {
    "text": "than it is to start the\nsentence with \"are\" though \"are\" is not impossible.. OK. And then the other\nthing we're doing",
    "start": "1103427",
    "end": "1109690"
  },
  {
    "text": "with these little blotches\nof black up at the top, we're sort of recording which\nGerman words we've translated.",
    "start": "1109690",
    "end": "1116080"
  },
  {
    "text": "And so we explore forward\nin the translation process.",
    "start": "1116080",
    "end": "1123070"
  },
  {
    "text": "And we could decide\nthat we could translate next the second word\ngoes, or we could",
    "start": "1123070",
    "end": "1129420"
  },
  {
    "text": "translate the negation here,\nand translate that as does not.",
    "start": "1129420",
    "end": "1134580"
  },
  {
    "text": "When we explore\nvarious continuations. And in the process, I'll\ngo through in more detail",
    "start": "1134580",
    "end": "1140130"
  },
  {
    "text": "later when we do the\nneural equivalent. We sort of do this search where\nwe explore likely translations",
    "start": "1140130",
    "end": "1147360"
  },
  {
    "text": "and prune. And eventually, we've\ntranslated the whole of the input sentence. And I've worked out a\nfairly likely translation.",
    "start": "1147360",
    "end": "1155100"
  },
  {
    "text": "He does not go home. And that's what we use\nas the translation.",
    "start": "1155100",
    "end": "1160980"
  },
  {
    "text": "OK. So in the period from\nabout 1997 to around 2013,",
    "start": "1160980",
    "end": "1173400"
  },
  {
    "text": "statistical machine translation\nwas a huge research field.",
    "start": "1173400",
    "end": "1178500"
  },
  {
    "text": "The best systems were\nextremely complex. And they had hundreds of\ndetails that I certainly",
    "start": "1178500",
    "end": "1184560"
  },
  {
    "text": "haven't mentioned here. The systems have lots of\nseparately designed and built components.",
    "start": "1184560",
    "end": "1190390"
  },
  {
    "text": "So I mentioned language model\nand the translation model. But they had lots of other\ncomponents for reordering",
    "start": "1190390",
    "end": "1197250"
  },
  {
    "text": "models, and inflection\nmodels, and other things. There was lots of\nfeature engineering.",
    "start": "1197250",
    "end": "1203520"
  },
  {
    "text": "Typically, the models also made\nuse of lots of extra resources.",
    "start": "1203520",
    "end": "1209460"
  },
  {
    "text": "And they were lots of\nhuman effort to maintain. But nevertheless, they were\nalready fairly successful.",
    "start": "1209460",
    "end": "1216580"
  },
  {
    "text": "So Google Translate\nlaunched in the mid 2000s. And people thought\nwow, this is amazing.",
    "start": "1216580",
    "end": "1223800"
  },
  {
    "text": "You could start to get sort\nof semi-decent automatic translations for\ndifferent web pages.",
    "start": "1223800",
    "end": "1232690"
  },
  {
    "text": "But that was chugging\nalong well enough. And then we got to 2014.",
    "start": "1232690",
    "end": "1239520"
  },
  {
    "text": "And really with\nenormous suddenness, people then worked out ways\nof doing machine translation",
    "start": "1239520",
    "end": "1248820"
  },
  {
    "text": "using a large neural network. And these large\nneural networks proved",
    "start": "1248820",
    "end": "1254310"
  },
  {
    "text": "to be just extremely successful,\nand largely blew away everything that preceded it.",
    "start": "1254310",
    "end": "1260430"
  },
  {
    "text": "So for the next big part of the\nlecture, what I'd like to do is tell you something about\nneural machine translation.",
    "start": "1260430",
    "end": "1269669"
  },
  {
    "text": "Neural machine\ntranslation, well, it means you're using\na neural network to do machine translation.",
    "start": "1269670",
    "end": "1276120"
  },
  {
    "text": "But in practice, it's meant\nslightly more than that. It has meant that\nwe're going to build",
    "start": "1276120",
    "end": "1283110"
  },
  {
    "text": "one very large neural\nnetwork, which completely does translation end to end.",
    "start": "1283110",
    "end": "1290080"
  },
  {
    "text": "So we're going to have\na large neural network, we're going to feed in the\nsource sentence into the input.",
    "start": "1290080",
    "end": "1295440"
  },
  {
    "text": "And what's going to\ncome out of the output of the neural network is the\ntranslation of the sentence.",
    "start": "1295440",
    "end": "1302580"
  },
  {
    "text": "We're going to\ntrain that model end to end on parallel sentences. And it's the entire\nsystem rather",
    "start": "1302580",
    "end": "1310230"
  },
  {
    "text": "than being lots of\nseparate components as in an old fashioned\nmachine translation system.",
    "start": "1310230",
    "end": "1316299"
  },
  {
    "text": "And we'll see that in a bit. So these neural\nnetwork architectures are called sequence\nto sequence models",
    "start": "1316300",
    "end": "1323700"
  },
  {
    "text": "or commonly abbreviated seq2seq. And they involve\ntwo neural networks.",
    "start": "1323700",
    "end": "1331530"
  },
  {
    "text": "Here it says two RNNs. The version I'm presenting\nnow has two RNNs. But more generally, they\ninvolve two neural networks.",
    "start": "1331530",
    "end": "1338909"
  },
  {
    "text": "There's one neural\nnetwork that is going to encode the source center.",
    "start": "1338910",
    "end": "1344429"
  },
  {
    "text": "So if we have a\nsource sentence here, we are going to\nencode that sentence.",
    "start": "1344430",
    "end": "1350160"
  },
  {
    "text": "And what we know about a\nway that we can do that. So using the kind of LSTMs\nthat we saw last class,",
    "start": "1350160",
    "end": "1357870"
  },
  {
    "text": "we can start at the beginning\nand go through a sentence and update the hidden\nstate each time.",
    "start": "1357870",
    "end": "1365040"
  },
  {
    "text": "And that will give us a\nrepresentation of the content of the source sentence.",
    "start": "1365040",
    "end": "1370990"
  },
  {
    "text": "So that's the first\nsequence model, which encodes the source sentence.",
    "start": "1370990",
    "end": "1377850"
  },
  {
    "text": "And we'll use the idea\nthat the final hidden state of the encode RNN is\ngoing to for instance,",
    "start": "1377850",
    "end": "1388330"
  },
  {
    "text": "represent the source sentence. And we're going to\nfeed it in directly",
    "start": "1388330",
    "end": "1393630"
  },
  {
    "text": "as the initial hidden state\nfor the decoder, or RNN. So then on the other\nside of the picture,",
    "start": "1393630",
    "end": "1399250"
  },
  {
    "text": "we have our decoder RNN. And it's a language\nmodel that's going to generate a target\nsentence conditioned",
    "start": "1399250",
    "end": "1407160"
  },
  {
    "text": "on the final hidden\nstate of the encoder RNN.",
    "start": "1407160",
    "end": "1414090"
  },
  {
    "text": "So we're going to start with\nthe input of start symbol. We're going to feed in the\nhidden state from the encoder",
    "start": "1414090",
    "end": "1421470"
  },
  {
    "text": "RNN. And now this second green\nRNN has completely separate",
    "start": "1421470",
    "end": "1426480"
  },
  {
    "text": "parameters. I might just emphasize. But we do the same kind\nof LSTM computations",
    "start": "1426480",
    "end": "1432360"
  },
  {
    "text": "and generate a first word\nof the sentence, \"he.\" And so then doing\nLSTM generation just",
    "start": "1432360",
    "end": "1440550"
  },
  {
    "text": "like last class, we copy\nthat down as the next input. We run the next\nstep of the LSTM,",
    "start": "1440550",
    "end": "1447419"
  },
  {
    "text": "generate another word here,\ncopy it down, and chug along. And we've translated\nthe sentence, right?",
    "start": "1447420",
    "end": "1456960"
  },
  {
    "text": "So this is showing\nthe test time behavior when we're generating\nthe next sentence.",
    "start": "1456960",
    "end": "1465150"
  },
  {
    "text": "For the training\ntime behavior, when we have parallel\nsentences, we're",
    "start": "1465150",
    "end": "1470700"
  },
  {
    "text": "still using the same kind of\nsequence to sequence model. But we're doing it\nwith the decoder part",
    "start": "1470700",
    "end": "1477450"
  },
  {
    "text": "just like training a\nlanguage model, where we're wanting to do teacher forcing\nand predict each word that's",
    "start": "1477450",
    "end": "1485460"
  },
  {
    "text": "actually found in the\nsource language sentence. ",
    "start": "1485460",
    "end": "1490700"
  },
  {
    "text": "Sequence to sequence\nmodels have been an incredibly powerful,\nwidely used work force",
    "start": "1490700",
    "end": "1498770"
  },
  {
    "text": "in neural networks for NLP. So although historically,\nmachine translation",
    "start": "1498770",
    "end": "1507170"
  },
  {
    "text": "was the first big\nuse of them, and it's sort of the canonical\nuse, they're",
    "start": "1507170",
    "end": "1512600"
  },
  {
    "text": "used everywhere else as well. So you can do many other\nNLP tasks for them.",
    "start": "1512600",
    "end": "1519000"
  },
  {
    "text": "So you can do summarization. You can think of\ntext summarization as translating a long\ntext into a short text.",
    "start": "1519000",
    "end": "1527960"
  },
  {
    "text": "But you can use them\nfor other things that are in no way a\ntranslation whatsoever.",
    "start": "1527960",
    "end": "1533010"
  },
  {
    "text": "So they're commonly used\nfor neural dialogue systems. So the encoder will encode the\nprevious two utterances, say.",
    "start": "1533010",
    "end": "1545000"
  },
  {
    "text": "And then you will\nuse the decoder to generate a next utterance.",
    "start": "1545000",
    "end": "1550430"
  },
  {
    "text": "Some other uses\nare even freakier but have proven to\nbe quite successful.",
    "start": "1550430",
    "end": "1558269"
  },
  {
    "text": "So if you have any\nway of representing the parse of a\nsentence as a string,",
    "start": "1558270",
    "end": "1565950"
  },
  {
    "text": "and if you sort\nof think a little it's fairly obvious how you can\nturn the parse of a sentence",
    "start": "1565950",
    "end": "1572299"
  },
  {
    "text": "into a string by just\nmaking use of extra syntax like parentheses, or putting in\nexplicit words that are saying",
    "start": "1572300",
    "end": "1582169"
  },
  {
    "text": "left arc, right arc, shifts\nlike the transition system",
    "start": "1582170",
    "end": "1588000"
  },
  {
    "text": "that you used for assignment 3. Well, then we could say\nlet's use the encoder.",
    "start": "1588000",
    "end": "1594920"
  },
  {
    "text": "Feed the input\nsentence to the encoder and let it output the\ntransition sequence",
    "start": "1594920",
    "end": "1600679"
  },
  {
    "text": "of our dependency parser. And somewhat surprisingly\nthat actually works well as another way\nto build a dependency parser",
    "start": "1600680",
    "end": "1609380"
  },
  {
    "text": "or other kinds of parser. These models have\nalso been applied not just to natural\nlanguages, but to other kinds",
    "start": "1609380",
    "end": "1617720"
  },
  {
    "text": "of languages, including\nmusic, and also programming language code.",
    "start": "1617720",
    "end": "1623360"
  },
  {
    "text": "So you can train\na seq2seq system, where it reads in pseudocode\nin natural language,",
    "start": "1623360",
    "end": "1632360"
  },
  {
    "text": "and it generates\nout Python code. And if you have a\ngood enough one, it can do the\nassignment for you.",
    "start": "1632360",
    "end": "1637789"
  },
  {
    "text": " So this central new idea\nhere with our sequence",
    "start": "1637790",
    "end": "1645280"
  },
  {
    "text": "to sequence models is we have an\nexample of conditional language models.",
    "start": "1645280",
    "end": "1650559"
  },
  {
    "text": "So previously, the main\nthing we were doing was just to start at the\nbeginning of the sentence",
    "start": "1650560",
    "end": "1656980"
  },
  {
    "text": "and generate a sentence\nbased on nothing. But here we have\nsomething that is",
    "start": "1656980",
    "end": "1664780"
  },
  {
    "text": "going to determine or\npartially determine that is going to condition\nwhat we should produce.",
    "start": "1664780",
    "end": "1670990"
  },
  {
    "text": "So we have a source sentence. And that's going to\nstrongly determine what is a good translation.",
    "start": "1670990",
    "end": "1677929"
  },
  {
    "text": "And so to achieve that,\nwhat we're going to do is have some way of transferring\ninformation about the source",
    "start": "1677930",
    "end": "1688660"
  },
  {
    "text": "sentence from the\nencoder to trigger what the decoder should do.",
    "start": "1688660",
    "end": "1695110"
  },
  {
    "text": "And the two standard ways\nof doing that are you either feed in a hidden state\nas the initial hidden state",
    "start": "1695110",
    "end": "1702039"
  },
  {
    "text": "to the decoder, or sometimes\nyou will feed something in as the initial\ninput to the decoder.",
    "start": "1702040",
    "end": "1709210"
  },
  {
    "text": "And so in neural\nmachine translation",
    "start": "1709210",
    "end": "1714220"
  },
  {
    "text": "we are directly calculating this\nconditional model probability of target language sentence\ngiven source language sentence.",
    "start": "1714220",
    "end": "1723460"
  },
  {
    "text": "And so at each step,\nas we break down the word by word generation,\nthat we're conditioning",
    "start": "1723460",
    "end": "1730330"
  },
  {
    "text": "not only on previous words\nof the target language, but also each time on our\nsource language sentence x.",
    "start": "1730330",
    "end": "1739570"
  },
  {
    "text": "Because of this,\nwe actually know a ton more about what our\nsentence that we generate",
    "start": "1739570",
    "end": "1744970"
  },
  {
    "text": "should be. So if you look at\nthe perplexities of these kind of\nconditional language models,",
    "start": "1744970",
    "end": "1752830"
  },
  {
    "text": "you will find them like the\nnumbers I showed last time. They usually have almost\nfreakily low perplexities,",
    "start": "1752830",
    "end": "1760510"
  },
  {
    "text": "that you will have\nmodels with perplexities that are something like 4\nor even less, sometimes 2.5",
    "start": "1760510",
    "end": "1768070"
  },
  {
    "text": "because you get a lot of\ninformation about what words you should be generating.",
    "start": "1768070",
    "end": "1773440"
  },
  {
    "text": "OK. So then we have\nthe same questions as we had for language\nmodels in general.",
    "start": "1773440",
    "end": "1779470"
  },
  {
    "text": "How to train a neural machine\ntranslation system and then how to use it at runtime?",
    "start": "1779470",
    "end": "1785840"
  },
  {
    "text": "So let's go through both of\nthose in a bit more detail.",
    "start": "1785840",
    "end": "1791059"
  },
  {
    "text": "So the first step is we get\na large parallel corpus. So we run off to the\nEuropean Union, for example.",
    "start": "1791060",
    "end": "1800470"
  },
  {
    "text": "And we grab a lot of\nparallel English French data from the European\nparliament proceedings.",
    "start": "1800470",
    "end": "1807790"
  },
  {
    "text": "So then once we have\nour parallel sentences, what we're going to do is take\nbatches of source sentences",
    "start": "1807790",
    "end": "1818770"
  },
  {
    "text": "and target sentences. We'll encode the source\nsentence with our encoder LSTM.",
    "start": "1818770",
    "end": "1826645"
  },
  {
    "text": "We'll feed its final hidden\nstate into a target LSTM.",
    "start": "1826645",
    "end": "1834820"
  },
  {
    "text": "And this one, we\nare now then going to train word by word by\ncomparing what it predicts",
    "start": "1834820",
    "end": "1842980"
  },
  {
    "text": "is the most likely\nword to be produced, versus what the actual\nfirst word, and then",
    "start": "1842980",
    "end": "1849760"
  },
  {
    "text": "the actual second word is. And to the extent\nthat we get it wrong, we're going to suffer some loss.",
    "start": "1849760",
    "end": "1857315"
  },
  {
    "text": "So this is going to\nbe the negative log probability of generating\nthe correct next word \"he\"",
    "start": "1857315",
    "end": "1864460"
  },
  {
    "text": "and so on along the sentence. And so in the same way that\nwe saw last time for language",
    "start": "1864460",
    "end": "1870160"
  },
  {
    "text": "models, we can work\nout our overall loss for the sentence doing\nthis teacher forcing style,",
    "start": "1870160",
    "end": "1877510"
  },
  {
    "text": "generate one word at a\ntime, calculate a loss relative to the word that\nyou should have produced.",
    "start": "1877510",
    "end": "1885990"
  },
  {
    "text": "And so that loss then\ngives us information that we can backpropagate\nthrough the entire network.",
    "start": "1885990",
    "end": "1893940"
  },
  {
    "text": "And the crucial thing\nabout these sequence to sequence models that\nhas made them extremely",
    "start": "1893940",
    "end": "1901020"
  },
  {
    "text": "successful in practice is that\nthe entire thing is optimized",
    "start": "1901020",
    "end": "1906330"
  },
  {
    "text": "as a single system end to end. So starting with\nour final loss, we",
    "start": "1906330",
    "end": "1913500"
  },
  {
    "text": "backpropagate it right\nthrough the system. So we not only update all\nthe parameters of the decoder",
    "start": "1913500",
    "end": "1921070"
  },
  {
    "text": "model, but we also update all\nof the parameters of the encoder",
    "start": "1921070",
    "end": "1926340"
  },
  {
    "text": "model, which in turn will\ninfluence what conditioning gets passed over from the\nencoder to the decoder.",
    "start": "1926340",
    "end": "1934005"
  },
  {
    "text": " So this moment is a\ngood moment for me",
    "start": "1934005",
    "end": "1939460"
  },
  {
    "text": "to return to the three\nslides that I skipped. I'm running out of time at\nthe end of last time, which",
    "start": "1939460",
    "end": "1946960"
  },
  {
    "text": "is to mention multilayer RNNs. So the RNNs that\nwe've looked at so far",
    "start": "1946960",
    "end": "1954856"
  },
  {
    "text": "are already deep on one\ndimension then unroll",
    "start": "1954857",
    "end": "1959950"
  },
  {
    "text": "horizontally over\nmany time steps. But they've been shallow\nin that there's just",
    "start": "1959950",
    "end": "1965800"
  },
  {
    "text": "been a single layer of recurrent\nstructure about our sentences.",
    "start": "1965800",
    "end": "1971080"
  },
  {
    "text": "We can also make them deep\nin the other dimension by applying multiple RNNs\non top of each other.",
    "start": "1971080",
    "end": "1978190"
  },
  {
    "text": "And this gives us\nsome multilayer RNN. Often also called a step RNN.",
    "start": "1978190",
    "end": "1984900"
  },
  {
    "text": "And having a\nmultilayer RNN allows",
    "start": "1984900",
    "end": "1990130"
  },
  {
    "text": "us the network to compute\nmore complex representations. So simply put the lower RNNs\ntend to compute lower level",
    "start": "1990130",
    "end": "1999400"
  },
  {
    "text": "features, and the higher RNNs\nshould compute higher level features.",
    "start": "1999400",
    "end": "2004860"
  },
  {
    "text": "And just like in\nother neural networks, whether it's feed\nforward networks,",
    "start": "2004860",
    "end": "2010230"
  },
  {
    "text": "or the kind of networks\nyou see in vision systems, you get much greater\npower and success",
    "start": "2010230",
    "end": "2017400"
  },
  {
    "text": "by having a stack\non multiple layers of recurrent neural\nnetworks, right?",
    "start": "2017400",
    "end": "2023850"
  },
  {
    "text": "That you might think that\noh, there are two things I could do. I could have a single LSTM with\na hidden state of dimension",
    "start": "2023850",
    "end": "2031110"
  },
  {
    "text": "2000, or I could have\nfour layers of LSTMs with a hidden state of 500 each.",
    "start": "2031110",
    "end": "2039840"
  },
  {
    "text": "And it shouldn't\nmake any difference because I've got the same\nnumber of parameters roughly. But that's not true.",
    "start": "2039840",
    "end": "2045600"
  },
  {
    "text": "In practice, it does\nmake a big difference. And multilayer or stacked\nRNNs are more powerful.",
    "start": "2045600",
    "end": "2052259"
  },
  {
    "text": " Can I ask you, there's a\ngood student question here?",
    "start": "2052259",
    "end": "2058879"
  },
  {
    "text": "What would lower level\nversus higher level features mean in this context? Sure.",
    "start": "2058880",
    "end": "2064270"
  },
  {
    "text": "Yeah. So I mean, in some sense, these\nare somewhat flimsy terms.",
    "start": "2064270",
    "end": "2073339"
  },
  {
    "text": " The meaning isn't precise.",
    "start": "2073339",
    "end": "2078469"
  },
  {
    "text": "But typically,\nwhat that's meaning is that lower level\nfeatures and knowing",
    "start": "2078469",
    "end": "2084439"
  },
  {
    "text": "sort of more basic things\nabout words and phrases.",
    "start": "2084439",
    "end": "2090030"
  },
  {
    "text": "So that commonly might be\nthings like what part of speech is this word, or are these\nwords the name of a person,",
    "start": "2090030",
    "end": "2099200"
  },
  {
    "text": "or the name of a company? Whereas higher level\nfeatures refer to things",
    "start": "2099200",
    "end": "2106010"
  },
  {
    "text": "that are at a higher\nsemantic level. So knowing more about\nthe overall structure",
    "start": "2106010",
    "end": "2111500"
  },
  {
    "text": "of a sentence, knowing\nsomething about what it means, whether a phrase has positive\nor negative connotations.",
    "start": "2111500",
    "end": "2120150"
  },
  {
    "text": "What its semantics are when\nyou put together several words into an idiomatic phrase,\nroughly the higher level",
    "start": "2120150",
    "end": "2128420"
  },
  {
    "text": "kinds of things. ",
    "start": "2128420",
    "end": "2135205"
  },
  {
    "text": "OK.  Jump ahead.",
    "start": "2135205",
    "end": "2141410"
  },
  {
    "text": "OK. So when we build\none of these end",
    "start": "2141410",
    "end": "2146470"
  },
  {
    "text": "to end neural machine\ntranslation systems, if we want them to work\nwell, single layer LSTM",
    "start": "2146470",
    "end": "2158339"
  },
  {
    "text": "encoder-decoder in neurals\nmachine translation systems just don't work well.",
    "start": "2158340",
    "end": "2164109"
  },
  {
    "text": "But you can build\nsomething that is no more complex than the model\nthat I've just explained now.",
    "start": "2164110",
    "end": "2170760"
  },
  {
    "text": "That does work pretty well by\nmaking it a multi-layer stacked",
    "start": "2170760",
    "end": "2177010"
  },
  {
    "text": "LSTM neural machine\ntranslation system. So therefore, the\npicture looks like this.",
    "start": "2177010",
    "end": "2183560"
  },
  {
    "text": "So we've got this\nmultilayer LSTM that's going through\nthe source sentence.",
    "start": "2183560",
    "end": "2189150"
  },
  {
    "text": "And so now, at\neach point in time, we calculate a new\nhidden representation",
    "start": "2189150",
    "end": "2195540"
  },
  {
    "text": "that rather than stopping there,\nwe sort of feed it as the input into another layer\nof LSTM, and we",
    "start": "2195540",
    "end": "2203609"
  },
  {
    "text": "calculate in the standard way\nits new hidden representation. And the output of it, we feed\ninto a third layer of LSTM.",
    "start": "2203610",
    "end": "2210780"
  },
  {
    "text": "And so we run that right along. And so our representation\nof the source sentence",
    "start": "2210780",
    "end": "2218280"
  },
  {
    "text": "from our encoder is then this\nstack of three hidden layers, whoops.",
    "start": "2218280",
    "end": "2224430"
  },
  {
    "text": "And then that we\nuse to then feed",
    "start": "2224430",
    "end": "2231510"
  },
  {
    "text": "in as the initial, as\nthe initial hidden layer",
    "start": "2231510",
    "end": "2236610"
  },
  {
    "text": "into then sort of\ngenerating translations, or for training the model\nof comparing the losses.",
    "start": "2236610",
    "end": "2244130"
  },
  {
    "text": "So this is what the\npicture of a LSTM encoder-decoder neural\nmachine translation system",
    "start": "2244130",
    "end": "2251339"
  },
  {
    "text": "really looks like. So in particular, to give\nyou some idea of that.",
    "start": "2251340",
    "end": "2260480"
  },
  {
    "text": "So a 2017 paper by\nDenny Britz and others, that what they found was\nthat for the encoder RNN,",
    "start": "2260480",
    "end": "2269560"
  },
  {
    "text": "it worked best if it\nhad two to four layers. And four layers was best\nfor the decoder RNN.",
    "start": "2269560",
    "end": "2279050"
  },
  {
    "text": "And the details here like\nfor a lot of neural nets depend so much on what you're\ndoing, and how much data",
    "start": "2279050",
    "end": "2285440"
  },
  {
    "text": "you have, and things like that. But as rules of thumb\nto have in your head,",
    "start": "2285440",
    "end": "2291180"
  },
  {
    "text": "it's almost invariably\nthe case that having a two layer LSTM works\na lot better than having a one",
    "start": "2291180",
    "end": "2299240"
  },
  {
    "text": "layer LSTM. After that, things\nbecome much less clear.",
    "start": "2299240",
    "end": "2305270"
  },
  {
    "text": "It's not so infrequent that\nif you try three layers, it's a fraction better than two. But not really.",
    "start": "2305270",
    "end": "2311060"
  },
  {
    "text": "And if you try four layers, it's\nactually getting worse again. It depends on how much\ndata, et cetera you have.",
    "start": "2311060",
    "end": "2317690"
  },
  {
    "text": "At any rate, it's normally\nvery hard with the model",
    "start": "2317690",
    "end": "2322970"
  },
  {
    "text": "architecture that I\njust showed back here to get better results with\nmore than four layers of LSTM.",
    "start": "2322970",
    "end": "2330950"
  },
  {
    "text": "Normally to do\ndeeper LSTM models and get even better results.",
    "start": "2330950",
    "end": "2338270"
  },
  {
    "text": "You have to be adding extra\nskip connections of the kind that I talked about at the\nvery end of the last class.",
    "start": "2338270",
    "end": "2347920"
  },
  {
    "text": "Next week, John is going\nto talk about transformer based networks. In contrast, for fairly\nfundamental reasons.",
    "start": "2347920",
    "end": "2356020"
  },
  {
    "text": "They're typically much deeper. But we'll leave discussing\nthem until we get on further.",
    "start": "2356020",
    "end": "2362890"
  },
  {
    "start": "2362890",
    "end": "2371450"
  },
  {
    "text": "So that was how we\ntrain the model. So let's just go a bit more\nthrough what the possibilities",
    "start": "2371450",
    "end": "2379060"
  },
  {
    "text": "are for decoding and explore a\nmore complex form of decoding",
    "start": "2379060",
    "end": "2384280"
  },
  {
    "text": "than we've looked at. But the simplest way\nto decode is the one that we presented so far.",
    "start": "2384280",
    "end": "2390440"
  },
  {
    "text": "So that we have our LSTM, we\nstart, generate a hidden state.",
    "start": "2390440",
    "end": "2396819"
  },
  {
    "text": "It has a probability\ndistribution over words. And you choose the most\nprobable one the argmax.",
    "start": "2396820",
    "end": "2403780"
  },
  {
    "text": "And you say \"he\", and you copy\nit down and you repeat over. So doing this is referred\nto as greedy decoding.",
    "start": "2403780",
    "end": "2411520"
  },
  {
    "text": "Taking the most probable\nword on each step. And it's sort of the\nobvious thing to do,",
    "start": "2411520",
    "end": "2418569"
  },
  {
    "text": "and doesn't seem like it\ncould be a bad thing to do. But it turns out\nthat it actually",
    "start": "2418570",
    "end": "2424360"
  },
  {
    "text": "can be a fairly\nproblematic thing to do. And the idea of that is\nthat with greedy decoding,",
    "start": "2424360",
    "end": "2432640"
  },
  {
    "text": "you're taking locally what\nseems the best choice. And then you're stuck with it.",
    "start": "2432640",
    "end": "2437920"
  },
  {
    "text": "And you have no way\nto undo decisions. So if these examples have been\nusing this sentence about,",
    "start": "2437920",
    "end": "2446140"
  },
  {
    "text": "he hit me with a pie\ngoing from translating from French to English. So if you start\noff, then you say,",
    "start": "2446140",
    "end": "2453460"
  },
  {
    "text": "OK, \"il\" the first word in\nthe translation, should be he. That looks good.",
    "start": "2453460",
    "end": "2459980"
  },
  {
    "text": "But then you-- and then you say,\nwell, hit, I'll generate hit.",
    "start": "2459980",
    "end": "2465430"
  },
  {
    "text": "Then somehow the model thinks\nthat the most likely next word after hit is \"a\".",
    "start": "2465430",
    "end": "2471430"
  },
  {
    "text": "And there are lots of\nreasons it could think so. Because after hit most commonly,\nthere's a direct object now",
    "start": "2471430",
    "end": "2479010"
  },
  {
    "text": "and then he hit a car, he\nhit a roadblock, right?",
    "start": "2479010",
    "end": "2484990"
  },
  {
    "text": "So that sounds pretty likely. But once you've generated it,\nthere's no way to go backwards.",
    "start": "2484990",
    "end": "2492280"
  },
  {
    "text": "And so you just have to\nkeep on going from there and you may not be able to\ngenerate the translation",
    "start": "2492280",
    "end": "2497950"
  },
  {
    "text": "you want. At best you can generate,\nhe hit a pie, or something.",
    "start": "2497950",
    "end": "2508040"
  },
  {
    "text": "So we'd like to be able\nto explore a bit more in generating our translations.",
    "start": "2508040",
    "end": "2514850"
  },
  {
    "text": "And well, what could we do? Well, I sort of mentioned\nthis before looking",
    "start": "2514850",
    "end": "2521420"
  },
  {
    "text": "at the statistical empty models. Overall, what we'd like\nto do is find translations",
    "start": "2521420",
    "end": "2529910"
  },
  {
    "text": "that maximize the\nprobability of y given x, and at least if we know what\nthe length of that translation",
    "start": "2529910",
    "end": "2539150"
  },
  {
    "text": "is. We can do that as a product of\ngenerating a word at a time. And so to have a full model.",
    "start": "2539150",
    "end": "2545220"
  },
  {
    "text": "We also have to\nhave a probability distribution over how long the\ntranslation length would be.",
    "start": "2545220",
    "end": "2551750"
  },
  {
    "text": "So we could say\nthis is the model. And let's generate and score\nall possible sequences y",
    "start": "2551750",
    "end": "2561530"
  },
  {
    "text": "using this model. And that's where\nthat then requires generating an exponential\nnumber of translations.",
    "start": "2561530",
    "end": "2569780"
  },
  {
    "text": "And it's far, far too expensive. So beyond greedy decoding,\nthe most important method",
    "start": "2569780",
    "end": "2578030"
  },
  {
    "text": "that is used. And you'll see lots\nof places is something called beam search decoding.",
    "start": "2578030",
    "end": "2583730"
  },
  {
    "text": "And so this isn't\nwhat neural, well, any kind of machine\ntranslation has one place",
    "start": "2583730",
    "end": "2590270"
  },
  {
    "text": "where it's commonly used. That this isn't\na method specific to machine translation.",
    "start": "2590270",
    "end": "2595490"
  },
  {
    "text": "You find lots of other places,\nincluding all other kinds of sequence to sequence models.",
    "start": "2595490",
    "end": "2601280"
  },
  {
    "text": "It's not the only\nother decoding method. Once when we got on to the\nlanguage generation class,",
    "start": "2601280",
    "end": "2607789"
  },
  {
    "text": "we'll see a couple more. But this is sort of the next\none that you should know about. So beam search's\nidea is that you're",
    "start": "2607790",
    "end": "2615799"
  },
  {
    "text": "going to keep some hypotheses to\nmake it more likely that you'll",
    "start": "2615800",
    "end": "2621350"
  },
  {
    "text": "find a good generation while\nkeeping the search tractable.",
    "start": "2621350",
    "end": "2627320"
  },
  {
    "text": "So what we do is\nchoose a beam size. And for neural MT, the beam\nsize is normally fairly small,",
    "start": "2627320",
    "end": "2634369"
  },
  {
    "text": "something like 5 to 10. And at each step of\nthe decoder, we're going to keep\ntrack of the k most",
    "start": "2634370",
    "end": "2641450"
  },
  {
    "text": "probable partial translation. So initial sub- sequences\nof what we're generating,",
    "start": "2641450",
    "end": "2648030"
  },
  {
    "text": "which we call hypotheses. So a hypothesis,\nwhich is then sort of the prefix of a\ntranslation has a score",
    "start": "2648030",
    "end": "2657410"
  },
  {
    "text": "which is this log\nprobability up to what's been generated so far. So we can generate that\nin the typical way using",
    "start": "2657410",
    "end": "2665270"
  },
  {
    "text": "our conditional language model. So as written all of\nthe scores are negative.",
    "start": "2665270",
    "end": "2671670"
  },
  {
    "text": "And so the least negative one,\ni.e., the highest probability one is the best one.",
    "start": "2671670",
    "end": "2676940"
  },
  {
    "text": "So what we want to do is\nsearch for high probability hypotheses.",
    "start": "2676940",
    "end": "2683790"
  },
  {
    "text": "So this is a heuristic method. It's not guaranteed to find the\nhighest probability decoding.",
    "start": "2683790",
    "end": "2690560"
  },
  {
    "text": "But at least, it gives\nyou more of a shot than simply doing\ngreedy decoding.",
    "start": "2690560",
    "end": "2695780"
  },
  {
    "text": "So let's go through an\nexample to see how it works.",
    "start": "2695780",
    "end": "2701430"
  },
  {
    "text": "So in this case, so I\ncan fit it on a slide. The size of our beam is just 2.",
    "start": "2701430",
    "end": "2708350"
  },
  {
    "text": "Though normally,\nit would actually be a bit bigger than that. And the blue numbers are\nthe scores of the prefixes.",
    "start": "2708350",
    "end": "2716480"
  },
  {
    "text": "So these are these log\nprobabilities of a prefix. So we start off with\nour start symbol.",
    "start": "2716480",
    "end": "2723540"
  },
  {
    "text": "And we're going to say, OK. What are the two\nmost likely words,",
    "start": "2723540",
    "end": "2729000"
  },
  {
    "text": "to generate first according\nto our language model? And so maybe the first two\nmost likely words are he and I.",
    "start": "2729000",
    "end": "2737090"
  },
  {
    "text": "And there are the\nlog probabilities. Then what we do next is for\neach of these k hypotheses,",
    "start": "2737090",
    "end": "2747260"
  },
  {
    "text": "we find what are likely\nwords to follow them? In particular, we find what\nare the k most likely words",
    "start": "2747260",
    "end": "2755119"
  },
  {
    "text": "to follow each of those. So we might generate he hit,\nhe struck, I was, I got.",
    "start": "2755120",
    "end": "2763190"
  },
  {
    "text": "OK. So at this point,\nit sort of looks like we're heading\ndown what will",
    "start": "2763190",
    "end": "2768530"
  },
  {
    "text": "turn into an exponential\nsize tree structure again. But what we do\nnow is we work out",
    "start": "2768530",
    "end": "2776599"
  },
  {
    "text": "the scores of each of\nthese partial hypotheses. So we have four\npartial hypotheses.",
    "start": "2776600",
    "end": "2783350"
  },
  {
    "text": "He hit, he struck. I was, I got. And we can do that by taking\nthe previous score that we have",
    "start": "2783350",
    "end": "2791300"
  },
  {
    "text": "the partial\nhypothesis and adding on the log probability of\ngenerating the next word here,",
    "start": "2791300",
    "end": "2798380"
  },
  {
    "text": "he, hit. So this gives the scores\nfor each hypothesis. And then we can say, which of\nthose two partial hypotheses?",
    "start": "2798380",
    "end": "2807470"
  },
  {
    "text": "Because our beam size, k equals\n2, have the highest score? And so they are,\nI was, and he hit.",
    "start": "2807470",
    "end": "2815660"
  },
  {
    "text": "So we keep those two\nand ignore the rest. And so then for\nthose two, we are",
    "start": "2815660",
    "end": "2822710"
  },
  {
    "text": "going to generate k\nhypotheses for the most likely following word.",
    "start": "2822710",
    "end": "2828380"
  },
  {
    "text": "He hit a, he hit me, I\nwas hit, I was struck.",
    "start": "2828380",
    "end": "2833509"
  },
  {
    "text": "And again, now, we want to find\nthe k most likely hypotheses",
    "start": "2833510",
    "end": "2840230"
  },
  {
    "text": "out of this full set. And so that's going\nto be he struck me and I was, I don't\nknow, he struck me.",
    "start": "2840230",
    "end": "2847700"
  },
  {
    "text": "And he hit a. So we keep just those ones.",
    "start": "2847700",
    "end": "2852800"
  },
  {
    "text": "And then for each of\nthose, we generate the k most likely next\nwords tart, pie, with, on.",
    "start": "2852800",
    "end": "2862549"
  },
  {
    "text": "And then again, we filter\nback down to size k by saying,",
    "start": "2862550",
    "end": "2867780"
  },
  {
    "text": "OK, the two most likely\nthings here are pie or with. So we continue working on\nthose, generate things,",
    "start": "2867780",
    "end": "2877460"
  },
  {
    "text": "find the two most\nlikely, generate things, find the two most likely.",
    "start": "2877460",
    "end": "2884510"
  },
  {
    "text": "And at this point, we would\ngenerate end of string.",
    "start": "2884510",
    "end": "2889700"
  },
  {
    "text": "And say, OK, we've got\na complete hypothesis. He struck me with a pie.",
    "start": "2889700",
    "end": "2897500"
  },
  {
    "text": "And we could then trace\nback through the tree",
    "start": "2897500",
    "end": "2902990"
  },
  {
    "text": "to obtain the full\nhypothesis for this sentence. So that's most of the algorithm.",
    "start": "2902990",
    "end": "2910329"
  },
  {
    "text": "There's one more detail, which\nis the stopping criterion.",
    "start": "2910330",
    "end": "2915410"
  },
  {
    "text": "So in greedy\ndecoding, we usually decode until the model\nproduces an end token.",
    "start": "2915410",
    "end": "2922960"
  },
  {
    "text": "And when it produces the end\ntoken, we say we are done.",
    "start": "2922960",
    "end": "2927970"
  },
  {
    "text": "In beam search decoding,\ndifferent hypotheses may produce end tokens\non different time steps.",
    "start": "2927970",
    "end": "2936050"
  },
  {
    "text": "And so we don't\nwant to stop as soon as one path through the\nsearch tree has generated end.",
    "start": "2936050",
    "end": "2944349"
  },
  {
    "text": "Because it could\nturn out there's a different path through\nthe search tree, which will still prove to be better.",
    "start": "2944350",
    "end": "2951170"
  },
  {
    "text": "So what we do is sort of put it\naside as a complete hypothesis",
    "start": "2951170",
    "end": "2956650"
  },
  {
    "text": "and continue exploring other\nhypotheses via our beam search.",
    "start": "2956650",
    "end": "2961660"
  },
  {
    "text": "And so usually, we\nwill then either stop when we've hit a cut off\nlength, or when we've completed",
    "start": "2961660",
    "end": "2972550"
  },
  {
    "text": "n complete hypotheses. And then we'll look through the\nhypotheses that we've completed",
    "start": "2972550",
    "end": "2981460"
  },
  {
    "text": "and say which is the\nbest one of those. And that's the one we'll use.",
    "start": "2981460",
    "end": "2987680"
  },
  {
    "text": "OK. So at that point, we have our\nlist of completed hypotheses.",
    "start": "2987680",
    "end": "2993190"
  },
  {
    "text": "And we want to select the top\none with the highest score. Well, that's exactly what\nwe've been computing.",
    "start": "2993190",
    "end": "2999680"
  },
  {
    "text": "Each one has a probability\nthat we've worked out.",
    "start": "2999680",
    "end": "3006780"
  },
  {
    "text": "But it turns out that\nwe might not want to use that just so naively.",
    "start": "3006780",
    "end": "3011820"
  },
  {
    "text": "Because that turns out to be a\nkind of a systematic problem, which is not as a theorem.",
    "start": "3011820",
    "end": "3018540"
  },
  {
    "text": "But in general, longer\nhypotheses have lower scores. So if you think about this as\nprobabilities of successively",
    "start": "3018540",
    "end": "3026850"
  },
  {
    "text": "generating each word, that\nbasically at each step, you're multiplying by\nanother chance of generating",
    "start": "3026850",
    "end": "3034290"
  },
  {
    "text": "the next word probability,\nand commonly those might be 10 to the minus\n3, 10 to the minus 2.",
    "start": "3034290",
    "end": "3040830"
  },
  {
    "text": "So just from the\nlength of the sentence, your probabilities are\ngetting much lower the longer",
    "start": "3040830",
    "end": "3046150"
  },
  {
    "text": "that they go on. In a way that appears to\nbe unfair since although",
    "start": "3046150",
    "end": "3051569"
  },
  {
    "text": "in some sense extremely\nlong sentences aren't as likely as short ones. They're not less\nlikely by that much.",
    "start": "3051570",
    "end": "3058170"
  },
  {
    "text": "A lot of the time we\nproduce long sentences. So for example, in a newspaper,\nthe median length of sentences",
    "start": "3058170",
    "end": "3068670"
  },
  {
    "text": "is over 20. So you wouldn't want to\nbe having a decoding model when translating news\narticles that says,",
    "start": "3068670",
    "end": "3076192"
  },
  {
    "text": "huh, just generate\ntwo word sentences. They're just way\nhigh probability according to my language model.",
    "start": "3076192",
    "end": "3082810"
  },
  {
    "text": "So the commonest way\nof dealing with that is that we normalize by length.",
    "start": "3082810",
    "end": "3088750"
  },
  {
    "text": "So if we're working\nin log probabilities, that means taking\ndividing through",
    "start": "3088750",
    "end": "3094200"
  },
  {
    "text": "by the length of the sentence. And then you have a per\nword log probability score.",
    "start": "3094200",
    "end": "3100890"
  },
  {
    "text": "And you can argue that\nthis isn't quite right. In some theoretical\nsense, but in practice",
    "start": "3100890",
    "end": "3107340"
  },
  {
    "text": "it works pretty well and\nit's very commonly used. Neural translation has proven\nto be much, much better.",
    "start": "3107340",
    "end": "3118180"
  },
  {
    "text": "I'll show you a\ncouple of statistics and about that in a moment. It has many advantages.",
    "start": "3118180",
    "end": "3125099"
  },
  {
    "text": "It gives better performance. The translations are better. In particular,\nthey're more fluent",
    "start": "3125100",
    "end": "3132029"
  },
  {
    "text": "because neural language\nmodels produce much more fluent sentences.",
    "start": "3132030",
    "end": "3137099"
  },
  {
    "text": "But also, they much\nbetter use context because neural language\nmodels, including",
    "start": "3137100",
    "end": "3144600"
  },
  {
    "text": "conditional neural\nlanguage models give us a very good\nway of conditioning",
    "start": "3144600",
    "end": "3149910"
  },
  {
    "text": "on a lot of contexts. In particular, we can just run\na long encoder and condition",
    "start": "3149910",
    "end": "3155970"
  },
  {
    "text": "on the previous sentence,\nor we can translate words well in context by making\nuse of neural context.",
    "start": "3155970",
    "end": "3164730"
  },
  {
    "text": "Neural models better understand\nphrase similarities and phrases",
    "start": "3164730",
    "end": "3170010"
  },
  {
    "text": "that mean approximately\nthe same thing. And then the technique of\noptimizing all parameters",
    "start": "3170010",
    "end": "3179050"
  },
  {
    "text": "of the model end to end in a\nsingle large neural network has just proved to be\na really powerful idea.",
    "start": "3179050",
    "end": "3186980"
  },
  {
    "text": "So previously, a\nlot of the time, people were building\nseparate components",
    "start": "3186980",
    "end": "3192940"
  },
  {
    "text": "and tuning them\nindividually, which just meant that they weren't\nactually optimal when put into a much bigger system.",
    "start": "3192940",
    "end": "3200450"
  },
  {
    "text": "So really a hugely\npowerful guiding idea in neural network land",
    "start": "3200450",
    "end": "3206859"
  },
  {
    "text": "is if you can sort of\nbuild one huge network, and just optimize the\nentire thing end to end,",
    "start": "3206860",
    "end": "3212170"
  },
  {
    "text": "that will give you\nmuch better performance than component-wise systems. We'll come back to the costs\nof that later in the course.",
    "start": "3212170",
    "end": "3222430"
  },
  {
    "text": "The models are also actually\ngreat in other ways. They actually require much\nless human effort to build.",
    "start": "3222430",
    "end": "3228400"
  },
  {
    "text": "There's no feature engineering. There's in general, no\nlanguage specific components.",
    "start": "3228400",
    "end": "3235090"
  },
  {
    "text": "You're using the same method\nfor all language pairs. Of course, it's rare for things\nto be perfect in every way.",
    "start": "3235090",
    "end": "3243370"
  },
  {
    "text": "So neural machine\ntranslation systems also have some\ndisadvantages compared",
    "start": "3243370",
    "end": "3248740"
  },
  {
    "text": "to the older statistical\nmachine translation systems. They're less interpretable.",
    "start": "3248740",
    "end": "3254569"
  },
  {
    "text": "It's harder to see why they're\ndoing what they're doing, where before you could\nactually look at phrase tables",
    "start": "3254570",
    "end": "3260320"
  },
  {
    "text": "and they were useful. So they're hard to debug. They also tend to be sort\nof difficult to control.",
    "start": "3260320",
    "end": "3268070"
  },
  {
    "text": "So compared to anything\nlike writing rules, you can't really give\nmuch specification",
    "start": "3268070",
    "end": "3274734"
  },
  {
    "text": "as if you like to say I'd like\nmy translations to be more casual or something like that.",
    "start": "3274735",
    "end": "3281990"
  },
  {
    "text": "It's hard to know\nwhat they'll generate. So there are various\nsafety concerns. ",
    "start": "3281990",
    "end": "3289000"
  },
  {
    "text": "I'll show a few examples\nof that in just a minute. But first, before doing\nthat, quickly how do we",
    "start": "3289000",
    "end": "3296770"
  },
  {
    "text": "evaluate machine translation? The best way to evaluate\nmachine translation",
    "start": "3296770",
    "end": "3303010"
  },
  {
    "text": "is to show a human being who's\nfluent in the source and target",
    "start": "3303010",
    "end": "3308320"
  },
  {
    "text": "languages the sentences, and\nget them to give judgment on how",
    "start": "3308320",
    "end": "3313390"
  },
  {
    "text": "good a translation it is. But that's expensive to\ndo, and might not even",
    "start": "3313390",
    "end": "3320470"
  },
  {
    "text": "be possible if you don't have\nthe right human beings around. So a lot of work\nwas put into finding",
    "start": "3320470",
    "end": "3326020"
  },
  {
    "text": "automatic methods of\nscoring translations that were good enough. And the most famous\nmethod of doing that",
    "start": "3326020",
    "end": "3333280"
  },
  {
    "text": "is what's called BLEU. And the way you do\nBLEU is you have",
    "start": "3333280",
    "end": "3339700"
  },
  {
    "text": "a human translation or\nseveral human translations of the source sentence, and\nyou're comparing a machine",
    "start": "3339700",
    "end": "3347980"
  },
  {
    "text": "generated translation to\nthose pre-given human written translations.",
    "start": "3347980",
    "end": "3353680"
  },
  {
    "text": "And you score them\nfor similarity by calculating n-gram\nprecisions, i.e., words",
    "start": "3353680",
    "end": "3360880"
  },
  {
    "text": "that overlap between the\ncomputer and human reason translation, bi-grams,\ntri-grams, and 4-grams.",
    "start": "3360880",
    "end": "3368710"
  },
  {
    "text": "And then working out\na geometric average between overlaps of\nn-grams, plus there's",
    "start": "3368710",
    "end": "3376600"
  },
  {
    "text": "a penalty for too short\nsystem translations. So BLEU has proven to be\na really useful measure.",
    "start": "3376600",
    "end": "3384370"
  },
  {
    "text": "But it's an imperfect measure. That commonly there\nare many valid ways to translate a sentence.",
    "start": "3384370",
    "end": "3389800"
  },
  {
    "text": "And so there's some\nluck as to whether the human written\ntranslations you have happened",
    "start": "3389800",
    "end": "3397510"
  },
  {
    "text": "to correspond to\nwhich what might be a good translation\nfrom the system.",
    "start": "3397510",
    "end": "3403450"
  },
  {
    "text": "There's more to say\nabout the details of BLEU and how it's implemented.",
    "start": "3403450",
    "end": "3409150"
  },
  {
    "text": "That you're going to see all\nof that during assignment 4, because you will be building\nneural machine translation",
    "start": "3409150",
    "end": "3417040"
  },
  {
    "text": "systems, and evaluating\nthem with a BLEU algorithm. And there are full details about\nBLEU in the assignment handout.",
    "start": "3417040",
    "end": "3427000"
  },
  {
    "text": "But at the end of the day, BLEU\ngives a score between 0 and 100",
    "start": "3427000",
    "end": "3432520"
  },
  {
    "text": "where your score is 100. If you are exactly producing\none of the human written",
    "start": "3432520",
    "end": "3437859"
  },
  {
    "text": "translations, and 0 if there's\nnot even a single unigram that",
    "start": "3437860",
    "end": "3442930"
  },
  {
    "text": "overlaps between the two, With that rather\nbrief intro, I wanted",
    "start": "3442930",
    "end": "3449200"
  },
  {
    "text": "to show you sort\nof what happened in machine translation.",
    "start": "3449200",
    "end": "3455360"
  },
  {
    "text": "So machine translation\nwith statistical models, phrase-based statistical\nmachine translation",
    "start": "3455360",
    "end": "3461470"
  },
  {
    "text": "that I showed at the\nbeginning of the class had been going on since\nthe mid 2000s decade.",
    "start": "3461470",
    "end": "3468369"
  },
  {
    "text": "And it had produced sort\nof semi-good results of the kind that are in Google\nTranslate in those days.",
    "start": "3468370",
    "end": "3475060"
  },
  {
    "text": "But by the time you entered\nthe 2010s, basically progress",
    "start": "3475060",
    "end": "3480887"
  },
  {
    "text": "in statistical machine\ntranslation had stalled.",
    "start": "3480887",
    "end": "3486940"
  },
  {
    "text": "And you were getting barely\nany increase over time. And most of the increase\nyou were getting over time",
    "start": "3486940",
    "end": "3494710"
  },
  {
    "text": "was simply because you're\ntraining your models on more data. In those years, around\nthe early 2010s,",
    "start": "3494710",
    "end": "3505900"
  },
  {
    "text": "the big hope that\nmost people had. Someone asked what\nis the y-axis here,",
    "start": "3505900",
    "end": "3511870"
  },
  {
    "text": "this y-axis is this BLEU\nscore that I told you about on the previous slide. In the early 2010s, the big hope\nthat most people in the machine",
    "start": "3511870",
    "end": "3521470"
  },
  {
    "text": "translation field had was, well,\nif we built a more complex kind of machine\ntranslation model that",
    "start": "3521470",
    "end": "3528099"
  },
  {
    "text": "knows about the syntactic\nstructure of languages, that makes use of tools like\ndependency parsers,",
    "start": "3528100",
    "end": "3535000"
  },
  {
    "text": "we'll be able to build\nmuch better translations. And so those are the\npurple systems here,",
    "start": "3535000",
    "end": "3540980"
  },
  {
    "text": "which I haven't\ndescribed at all. But it's as the years went\nby it was pretty obvious that",
    "start": "3540980",
    "end": "3549490"
  },
  {
    "text": "barely seemed to help. And so then in the\nmid 2010s, so in 2014",
    "start": "3549490",
    "end": "3559360"
  },
  {
    "text": "was the first modern attempt\nto build a neural network from machine translations\nand encoded-decoder model.",
    "start": "3559360",
    "end": "3567400"
  },
  {
    "text": "And by the time it was sort of\nevaluated in bake offs in 2015, it wasn't as good as\nwhat had been built up",
    "start": "3567400",
    "end": "3574240"
  },
  {
    "text": "over the preceding decade. But it was already\ngetting pretty good. But what was found was\nthat these new models just",
    "start": "3574240",
    "end": "3582460"
  },
  {
    "text": "really opened up a\nwhole new pathway to start building much, much\nbetter machine translation",
    "start": "3582460",
    "end": "3588820"
  },
  {
    "text": "systems. And since then, things have\njust sort of taken off. And year by year, neural\nmachine translation systems",
    "start": "3588820",
    "end": "3596980"
  },
  {
    "text": "are getting much\nbetter and far better than anything we\nhad preceding that.",
    "start": "3596980",
    "end": "3602650"
  },
  {
    "text": "So for at least the early\npart of the application",
    "start": "3602650",
    "end": "3608910"
  },
  {
    "text": "of deep learning and\nnatural language processing, neural machine translation was\nthe huge big success story.",
    "start": "3608910",
    "end": "3618029"
  },
  {
    "text": "In the last few\nyears, when we've had models like GPT2 and GPT3,\nand other huge neural models",
    "start": "3618030",
    "end": "3626310"
  },
  {
    "text": "like BERT improving web search. It's a bit more complex.",
    "start": "3626310",
    "end": "3632490"
  },
  {
    "text": "But this was the\nfirst area where there was a neural\nnetwork, which",
    "start": "3632490",
    "end": "3637530"
  },
  {
    "text": "was hugely better than what\nhad preceded, and was actually",
    "start": "3637530",
    "end": "3642600"
  },
  {
    "text": "solving a practical problem\nthat lots of people in the world need.",
    "start": "3642600",
    "end": "3648300"
  },
  {
    "text": "And it was stunning\nwith the speed at which success was achieved.",
    "start": "3648300",
    "end": "3654640"
  },
  {
    "text": "So 2014 were the\nfirst what I call",
    "start": "3654640",
    "end": "3660000"
  },
  {
    "text": "here fringe research attempts\nto build a neural machine translation system.",
    "start": "3660000",
    "end": "3665440"
  },
  {
    "text": "Meaning that three\nor four people who are working on\nneural network models",
    "start": "3665440",
    "end": "3671640"
  },
  {
    "text": "thought, oh, why don't we see\nif we can use one of these to translate, learn to translate\nsentences, where there weren't",
    "start": "3671640",
    "end": "3677910"
  },
  {
    "text": "really people with a background\nin machine translation at all? But a success was\nachieved so quickly",
    "start": "3677910",
    "end": "3686100"
  },
  {
    "text": "that within two\nyears' time, Google had switched to using neural\nmachine translation for most",
    "start": "3686100",
    "end": "3695069"
  },
  {
    "text": "languages. And by a couple of\nyears later, after that, essentially anybody who\ndoes machine translation",
    "start": "3695070",
    "end": "3702390"
  },
  {
    "text": "is now deploying live neural\nmachine translation systems",
    "start": "3702390",
    "end": "3707400"
  },
  {
    "text": "and getting much,\nmuch better results. So that was sort\nof just an amazing",
    "start": "3707400",
    "end": "3712650"
  },
  {
    "text": "technological transition that\nfor the preceding decade, the big statistical\nmachine translation",
    "start": "3712650",
    "end": "3719609"
  },
  {
    "text": "systems like the previous\ngeneration of Google Translate had literally been\nbuilt up by hundreds",
    "start": "3719610",
    "end": "3725160"
  },
  {
    "text": "of engineers over the years. But a comparatively small\ngroup of deep learning people",
    "start": "3725160",
    "end": "3735150"
  },
  {
    "text": "in a few months with a\nsmall amount of code. And hopefully, you'll\neven get a sense",
    "start": "3735150",
    "end": "3740580"
  },
  {
    "text": "of this doing assignment 4, were\nable to build neural machine translation systems that\nproved to work much better.",
    "start": "3740580",
    "end": "3750360"
  },
  {
    "text": "Does that mean that machine\ntranslation is solved? No. There are still\nlots of difficulties",
    "start": "3750360",
    "end": "3756359"
  },
  {
    "text": "which people continue to\nwork on very actively. And you can see more about it in\nthe Skynet Today article that's",
    "start": "3756360",
    "end": "3763440"
  },
  {
    "text": "linked at the bottom. But there are lots of problems\nwith out of vocabulary words.",
    "start": "3763440",
    "end": "3769010"
  },
  {
    "text": "There are domain mismatches\nbetween the training and test data. So it might be trained\nmainly on news wire data",
    "start": "3769010",
    "end": "3775680"
  },
  {
    "text": "but you want to translate\npeople's Facebook messages.",
    "start": "3775680",
    "end": "3780720"
  },
  {
    "text": "There are still problems\nof maintaining context over longer text. We'd like to translate\nlanguages for which we",
    "start": "3780720",
    "end": "3788280"
  },
  {
    "text": "don't have much data. And so these methods work\nby far the best when we have",
    "start": "3788280",
    "end": "3793920"
  },
  {
    "text": "huge amounts of parallel data. Even our best\nmultilayer LSTMs aren't",
    "start": "3793920",
    "end": "3801450"
  },
  {
    "text": "that great of capturing\nsentence meaning. There are particular\nproblems such as interpreting",
    "start": "3801450",
    "end": "3808110"
  },
  {
    "text": "what pronouns refer\nto, or in languages like Chinese or Japanese,\nwhere there's often no pronoun",
    "start": "3808110",
    "end": "3816270"
  },
  {
    "text": "present. But there is an\nimplied reference to some person working\nout how to translate that.",
    "start": "3816270",
    "end": "3821850"
  },
  {
    "text": "For languages that have lots\nof inflectional forms of nouns, verbs, and adjectives.",
    "start": "3821850",
    "end": "3827820"
  },
  {
    "text": "These systems often\nget them wrong. So there's still\ntons of stuff to do.",
    "start": "3827820",
    "end": "3833010"
  },
  {
    "text": "So here's just sort of quick\nfunny examples of the kind of things that go wrong, right?",
    "start": "3833010",
    "end": "3838970"
  },
  {
    "text": "So if you asked to\ntranslate paper jam.",
    "start": "3838970",
    "end": "3844170"
  },
  {
    "text": "Google Translate is\ndeciding that this is a kind of jam just like this.",
    "start": "3844170",
    "end": "3850170"
  },
  {
    "text": "Raspberry jam and\nstrawberry jam. And so this becomes\na jam of paper.",
    "start": "3850170",
    "end": "3858570"
  },
  {
    "text": "There are problems of\nagreement and choice.",
    "start": "3858570",
    "end": "3864100"
  },
  {
    "text": "So if you have many languages\ndon't distinguish gender. And so the sentences are\nneutral between things masculine",
    "start": "3864100",
    "end": "3874770"
  },
  {
    "text": "or feminine, so Malay,\nor Turkish are two well known languages of that sort.",
    "start": "3874770",
    "end": "3880200"
  },
  {
    "text": "But what happens when that\ngets translated into English by Google Translate is\nthat the English language",
    "start": "3880200",
    "end": "3888210"
  },
  {
    "text": "model just kicks in and\napplies stereotypical biases. And so these gender neutral\nsentences get translated into,",
    "start": "3888210",
    "end": "3897060"
  },
  {
    "text": "she works as a nurse. He works as a programmer. So if you want to help solve\nthis problem, all of you",
    "start": "3897060",
    "end": "3904260"
  },
  {
    "text": "can help by using singular\nthey in all contexts when you're putting material online.",
    "start": "3904260",
    "end": "3911400"
  },
  {
    "text": "And that could then\nchange the distribution of what's generated. And people also work on\nmodeling improvements",
    "start": "3911400",
    "end": "3918569"
  },
  {
    "text": "to try and avoid this. Here's one more example\nthat's kind of funny.",
    "start": "3918570",
    "end": "3925770"
  },
  {
    "text": "People noticed a\ncouple of years ago. That if you choose one\nof the rarer languages",
    "start": "3925770",
    "end": "3934290"
  },
  {
    "text": "that Google will\ntranslate such as Somali,",
    "start": "3934290",
    "end": "3940140"
  },
  {
    "text": "and you just write in some\nrubbish like ag ag ag ag.",
    "start": "3940140",
    "end": "3945900"
  },
  {
    "text": "Freakily, it had produced out of\nnowhere prophetic and biblical",
    "start": "3945900",
    "end": "3950910"
  },
  {
    "text": "texts, as the name\nof the Lord was written in the Hebrew language. It was written in the\nlanguage of the Hebrew nation,",
    "start": "3950910",
    "end": "3957960"
  },
  {
    "text": "which makes no sense at all. Well, we're about to see a bit\nmore about why this happens.",
    "start": "3957960",
    "end": "3966450"
  },
  {
    "text": "But that was a bit worrying. As far as I can see, this\nproblem is now fixed in 2021.",
    "start": "3966450",
    "end": "3974369"
  },
  {
    "text": "I couldn't actually get\nGoogle Translate to generate examples like this anymore.",
    "start": "3974370",
    "end": "3981830"
  },
  {
    "text": "So there are lots of ways\nto keep on doing research.",
    "start": "3981830",
    "end": "3987120"
  },
  {
    "text": "NMT certainly is a flagship\ntask for NLP and deep learning.",
    "start": "3987120",
    "end": "3993110"
  },
  {
    "text": "And it was a place where\nmany of the innovations of deep learning\nNLP were pioneered,",
    "start": "3993110",
    "end": "4001330"
  },
  {
    "text": "and people continue\nto work hard on it, people have found many,\nmany improvements.",
    "start": "4001330",
    "end": "4007060"
  },
  {
    "text": "And actually for the last bit\nof the class and the minute I'm going to present one\nhuge improvement, which",
    "start": "4007060",
    "end": "4014470"
  },
  {
    "text": "is so important that\nit's really come to dominate the whole of the\nrecent field of neural networks",
    "start": "4014470",
    "end": "4022800"
  },
  {
    "text": "for NLP. And that's the\nidea of attention. But before I get\nonto attention, I",
    "start": "4022800",
    "end": "4028900"
  },
  {
    "text": "want to spend three minutes\non our assignment 4.",
    "start": "4028900",
    "end": "4034000"
  },
  {
    "text": "So for assignment\n4 this year, we've got a new version of\nthe assignment, which",
    "start": "4034000",
    "end": "4041020"
  },
  {
    "text": "we hope will be interesting. But it's also a real challenge. So for assignment\n4 this year, we've",
    "start": "4041020",
    "end": "4047980"
  },
  {
    "text": "decided to do Cherokee\nEnglish machine translation. So Cherokee is an endangered\nNative American language that",
    "start": "4047980",
    "end": "4055600"
  },
  {
    "text": "has about 2000 fluent speakers. It's an extremely low\nresource language.",
    "start": "4055600",
    "end": "4061630"
  },
  {
    "text": "So it's just there\nisn't much written Cherokee data available period.",
    "start": "4061630",
    "end": "4067329"
  },
  {
    "text": "And particularly, there's not\na lot of parallel sentences between Cherokee and English.",
    "start": "4067330",
    "end": "4074080"
  },
  {
    "text": "And here's the answer to\nGoogle's freaky prophetic translations.",
    "start": "4074080",
    "end": "4080380"
  },
  {
    "text": "For languages for which there\nisn't much parallel data",
    "start": "4080380",
    "end": "4085480"
  },
  {
    "text": "available, commonly\nthe biggest place where you can get parallel data\nis from Bible translations.",
    "start": "4085480",
    "end": "4094640"
  },
  {
    "text": "So you can have your\nown personal choice wherever it is over\nthe map as to where you",
    "start": "4094640",
    "end": "4101770"
  },
  {
    "text": "stand with respect to religion. But the fact of the\nmatter is if you work on indigenous languages,\nwhat you very, very quickly",
    "start": "4101770",
    "end": "4110830"
  },
  {
    "text": "find is that a lot of the work\nthat's done on collecting data on indigenous languages, and\na lot of the material that",
    "start": "4110830",
    "end": "4119290"
  },
  {
    "text": "is available in written form\nfor many indigenous languages is Bible translations.",
    "start": "4119290",
    "end": "4126859"
  },
  {
    "text": "Yeah. OK. So this is what\nCherokee looks like.",
    "start": "4126859",
    "end": "4133040"
  },
  {
    "text": "And so you can see that the\nwriting system has a mixture of things that look like English\nletters and then all sorts",
    "start": "4133040",
    "end": "4142060"
  },
  {
    "text": "of letters that don't. And so here's the\ninitial bit of a story long ago where\nseven boys who used",
    "start": "4142060",
    "end": "4149439"
  },
  {
    "text": "to spend all their time\ndown by the townhouse. So this is a piece of parallel\ndata that we can learn from.",
    "start": "4149439",
    "end": "4156470"
  },
  {
    "text": "So the Cherokee writing\nsystem has 85 letters. And the reason why it\nhas so many letters",
    "start": "4156470",
    "end": "4163870"
  },
  {
    "text": "is that each of these letters\nactually represents a syllable. So many languages\nof the world have",
    "start": "4163870",
    "end": "4171790"
  },
  {
    "text": "strict consonant vowel\nsyllable structure. So you have words\nlike right up here",
    "start": "4171790",
    "end": "4177490"
  },
  {
    "text": "or something like that\nfor Cherokee, right?",
    "start": "4177490",
    "end": "4182859"
  },
  {
    "text": "And another language\nlike that's Hawaiian. And so each of the letters\nrepresents a combination",
    "start": "4182859",
    "end": "4190270"
  },
  {
    "text": "of a consonant and a vowel. And that's the set of those.",
    "start": "4190270",
    "end": "4196239"
  },
  {
    "text": "You then get 17 by 5\ngives you 85 letters.",
    "start": "4196240",
    "end": "4201710"
  },
  {
    "text": "Yeah, so being able\nto do this assignment. Big thanks to people\nfrom University of North Carolina, Chapel Hill\nwho've provided the resources",
    "start": "4201710",
    "end": "4211810"
  },
  {
    "text": "were using for this assignment. Although you can do\nquite a lot of languages",
    "start": "4211810",
    "end": "4217330"
  },
  {
    "text": "on Google Translate. Cherokee is not a\nlanguage that Google offers on Google Translate.",
    "start": "4217330",
    "end": "4223270"
  },
  {
    "text": "So we can see how\nfar we can get. But we have to be modest\nin our expectations",
    "start": "4223270",
    "end": "4229000"
  },
  {
    "text": "because it's hard to\nbuild a very good MT system with only a fairly\nlimited amount of data.",
    "start": "4229000",
    "end": "4235730"
  },
  {
    "text": "So we'll see how far we can get. There is a flipside, which\nis for you students doing",
    "start": "4235730",
    "end": "4241030"
  },
  {
    "text": "the assignment. The advantage of having\nnot too much data is that your models will\ntrain relatively quickly.",
    "start": "4241030",
    "end": "4248330"
  },
  {
    "text": "So we'll actually\nhave less trouble than we did last year with\npeople's models taking hours",
    "start": "4248330",
    "end": "4253960"
  },
  {
    "text": "to train as the assignment\ndeadline closed in.",
    "start": "4253960",
    "end": "4259210"
  },
  {
    "text": "There's a couple more\nwords about Cherokee. So we have some idea\nwhat we're talking about.",
    "start": "4259210",
    "end": "4264860"
  },
  {
    "text": "So the Cherokee originally\nlived in Western North Carolina and Eastern Tennessee.",
    "start": "4264860",
    "end": "4273520"
  },
  {
    "text": "They then sort of got\nshunted Southwest from that. And then in particular,\nfor those of you who",
    "start": "4273520",
    "end": "4282260"
  },
  {
    "text": "went to American high\nschools and paid attention, you might remember discussion\nof the Trail of Tears",
    "start": "4282260",
    "end": "4290179"
  },
  {
    "text": "when a lot of the\nNative Americans from the Southeast of\nthe US got forcibly",
    "start": "4290180",
    "end": "4295429"
  },
  {
    "text": "shoved a long way further West. And so most Cherokee\nnow live in Oklahoma.",
    "start": "4295430",
    "end": "4303640"
  },
  {
    "text": "There are some that\nare in North Carolina. The writing system that I\nshowed on this previous slide,",
    "start": "4303640",
    "end": "4311540"
  },
  {
    "text": "it was invented by a\nCherokee man, Sequoyah. That's a drawing of him there.",
    "start": "4311540",
    "end": "4319430"
  },
  {
    "text": "And that was actually kind\nof an incredible thing. So he started off\nilliterate and worked",
    "start": "4319430",
    "end": "4327830"
  },
  {
    "text": "out how to produce a\nwriting system that",
    "start": "4327830",
    "end": "4332990"
  },
  {
    "text": "would be good for Cherokee. And given that it has this\nconsonant-vowel structure,",
    "start": "4332990",
    "end": "4339260"
  },
  {
    "text": "he chose a syllabary which\nturned out to be a good choice.",
    "start": "4339260",
    "end": "4344570"
  },
  {
    "text": "So here's a neat\nhistorical fact. So in the 1830s and\n1840s, the percentage",
    "start": "4344570",
    "end": "4352580"
  },
  {
    "text": "of Cherokee that were literate\nin Cherokee written like this",
    "start": "4352580",
    "end": "4358790"
  },
  {
    "text": "was actually higher than the\npercentage of white people in the southeastern United\nStates at that point in time.",
    "start": "4358790",
    "end": "4366949"
  },
  {
    "text": "OK. Before time disappears, oops,\ntime has almost disappeared.",
    "start": "4366950",
    "end": "4373220"
  },
  {
    "text": "I was just starting\nto say, oh no, I'll have to do a bit more of this. We'll have to do a bit\nmore of this next time.",
    "start": "4373220",
    "end": "4380030"
  },
  {
    "text": "That'll be OK, right? So the final idea that's\nreally important for sequence",
    "start": "4380030",
    "end": "4385820"
  },
  {
    "text": "to sequence models is\nthe idea of attention. And so we had this\nmodel of doing sequence",
    "start": "4385820",
    "end": "4395750"
  },
  {
    "text": "to sequence models such as for\nneural machine translation. And the problem with\nthis architecture",
    "start": "4395750",
    "end": "4403280"
  },
  {
    "text": "is that we have this\none hidden state, which has to encode all the\ninformation about the source",
    "start": "4403280",
    "end": "4411980"
  },
  {
    "text": "sentence. So it acts as a kind of\ninformation bottleneck. And that's all the information\nthat the generation",
    "start": "4411980",
    "end": "4419719"
  },
  {
    "text": "is conditioned on. Well, I did already\nmention one idea last time",
    "start": "4419720",
    "end": "4426300"
  },
  {
    "text": "of how to get more information\nwhere I said look maybe you could kind of average all\nof the vectors of the source",
    "start": "4426300",
    "end": "4432679"
  },
  {
    "text": "to get a sentence\nrepresentation. But that method turns\nout to be better",
    "start": "4432680",
    "end": "4437900"
  },
  {
    "text": "for things like\nsentiment analysis. And not so good for\nmachine translation,",
    "start": "4437900",
    "end": "4443090"
  },
  {
    "text": "where the order of words is\nvery important to preserve. So it seems like we would\ndo better, if somehow, we",
    "start": "4443090",
    "end": "4452900"
  },
  {
    "text": "could get more information\nfrom the source sentence while we're generating\nthe translation.",
    "start": "4452900",
    "end": "4460070"
  },
  {
    "text": "And in some sense, this\njust corresponds to what a human translator does, right? If you're a human translator,\nyou read the sentence",
    "start": "4460070",
    "end": "4467900"
  },
  {
    "text": "that you're meant to translate. And you maybe start\ntranslating a few words. But then you look back\nat the source sentence",
    "start": "4467900",
    "end": "4474470"
  },
  {
    "text": "to see what else was in it\nand translate some more words. So very quickly after the first\nneural machine translation",
    "start": "4474470",
    "end": "4482300"
  },
  {
    "text": "systems, people came up\nwith the idea of maybe we could build a better\nneural empty MT that did that.",
    "start": "4482300",
    "end": "4490730"
  },
  {
    "text": "And that's the\nidea of attention. So the core idea is on\neach step of the decoder,",
    "start": "4490730",
    "end": "4499489"
  },
  {
    "text": "we're going to use a direct\nlink between the encoder",
    "start": "4499490",
    "end": "4504920"
  },
  {
    "text": "and the decoder\nthat will allow us to focus on a particular word\nor words in the source sequence",
    "start": "4504920",
    "end": "4513440"
  },
  {
    "text": "and use it to help us\ngenerate what words come next.",
    "start": "4513440",
    "end": "4519530"
  },
  {
    "text": "I'll just go through\nnow showing you the pictures of\nwhat attention does",
    "start": "4519530",
    "end": "4525050"
  },
  {
    "text": "and then at the\nstart of next time we'll go through the\nequations in more detail.",
    "start": "4525050",
    "end": "4531830"
  },
  {
    "text": "So we use our encoder\njust as before and generate our\nrepresentations,",
    "start": "4531830",
    "end": "4539469"
  },
  {
    "text": "feed in our\nconditioning as before, and say we're starting\nour translation.",
    "start": "4539470",
    "end": "4545290"
  },
  {
    "text": "But at this point, we take this\nhidden representation, and say,",
    "start": "4545290",
    "end": "4550510"
  },
  {
    "text": "I'm going to use this hidden\nrepresentation to look back at the source to get\ninformation directly from it.",
    "start": "4550510",
    "end": "4558290"
  },
  {
    "text": "So what I will do\nis I will compare",
    "start": "4558290",
    "end": "4563320"
  },
  {
    "text": "the hidden state of the\ndecoder with the hidden state of the encoder\nat each position",
    "start": "4563320",
    "end": "4570880"
  },
  {
    "text": "and generate an attention score,\nwhich is a kind of similarity",
    "start": "4570880",
    "end": "4575920"
  },
  {
    "text": "score like a product. And then based on\nthose attention scores,",
    "start": "4575920",
    "end": "4581980"
  },
  {
    "text": "I'm going to calculate a\nprobability distribution as to by using a\nsoftmax as usual to say",
    "start": "4581980",
    "end": "4591820"
  },
  {
    "text": "which of these encoder states\nis most like my decoder state.",
    "start": "4591820",
    "end": "4600070"
  },
  {
    "text": "And so we'll be training\nthe model here to be saying, well, probably you should\ntranslate the first word",
    "start": "4600070",
    "end": "4605680"
  },
  {
    "text": "of the sentence\nfirst, so that's where the attention should be placed. So then based on this\nattention distribution, which",
    "start": "4605680",
    "end": "4613480"
  },
  {
    "text": "is a probability distribution\ncoming out of the softmax, we're going to generate\na new attention output.",
    "start": "4613480",
    "end": "4625210"
  },
  {
    "text": "And so this attention\noutput is going to be an average of the hidden\nstates of the encoder model.",
    "start": "4625210",
    "end": "4631840"
  },
  {
    "text": "That is going to be a weighted\naverage based on our attention",
    "start": "4631840",
    "end": "4636880"
  },
  {
    "text": "distribution. And so we then kind of\ntake that attention output,",
    "start": "4636880",
    "end": "4641980"
  },
  {
    "text": "combine it with the hidden\nstate of the decoder RNN",
    "start": "4641980",
    "end": "4647500"
  },
  {
    "text": "and together, the\ntwo of them are then going to be used to predict via\na softmax what word to generate",
    "start": "4647500",
    "end": "4658870"
  },
  {
    "text": "first, and we hope\nto generate he. And then at that\npoint, we sort of",
    "start": "4658870",
    "end": "4664840"
  },
  {
    "text": "chug along and keep doing\nthe same kind of computations",
    "start": "4664840",
    "end": "4670780"
  },
  {
    "text": "at each position. There's a little side note\nhere that says sometimes",
    "start": "4670780",
    "end": "4676510"
  },
  {
    "text": "we take the attention output\nfrom the previous step, and also feed into the decoder\nalong with the usual decoder",
    "start": "4676510",
    "end": "4683679"
  },
  {
    "text": "input. So we're taking this\nattention output and actually feeding it\nback in to the hidden state",
    "start": "4683680",
    "end": "4690460"
  },
  {
    "text": "calculation. And that can sometimes\nimprove performance. And we actually have that trick\nin the assignment 4 system.",
    "start": "4690460",
    "end": "4697869"
  },
  {
    "text": "And you can try it out. OK. So we generate along and\ngenerate our whole sentence",
    "start": "4697870",
    "end": "4705670"
  },
  {
    "text": "in this manner. And that's proven to be a\nvery effective way of getting",
    "start": "4705670",
    "end": "4712000"
  },
  {
    "text": "more information from\nthe source sentence more flexibly to allow us to\ngenerate a good translation.",
    "start": "4712000",
    "end": "4720730"
  },
  {
    "text": "I'll stop here for now and\nat the start of next time. I'll finish this off by going\nthrough the actual equations",
    "start": "4720730",
    "end": "4727600"
  },
  {
    "text": "for how attention works. ",
    "start": "4727600",
    "end": "4734000"
  }
]