[
  {
    "start": "0",
    "end": "5230"
  },
  {
    "text": "I'm going to be revisiting some\nunsupervised pre-training stuff today. This is\nreconstruction-based methods",
    "start": "5230",
    "end": "11110"
  },
  {
    "text": "as opposed to the\ncontrastive methods that we looked at on Monday. All right, so the\nplan for today,",
    "start": "11110",
    "end": "17770"
  },
  {
    "text": "we're going to do a little recap\non what Chelsea talked about on Monday. So we're going to talk about the\nunsupervised learning problem",
    "start": "17770",
    "end": "23290"
  },
  {
    "text": "setup and just go over\ncontrastive learning briefly. And then the meat\nof the lecture is",
    "start": "23290",
    "end": "28660"
  },
  {
    "text": "going to be about\nreconstruction-based unsupervised pre-training. So the main difference here\nis that we're not learning",
    "start": "28660",
    "end": "34960"
  },
  {
    "text": "by comparing multiple examples. We're sort of learning\none example at a time by just trying to\ncompute a representation",
    "start": "34960",
    "end": "42520"
  },
  {
    "text": "and then reconstruct\nthe example. And we're going to\ntalk about a couple of different instantiations\nof this idea.",
    "start": "42520",
    "end": "47833"
  },
  {
    "text": "Some of them, you've\nprobably heard of before, some of these big, fancy,\nhopefully not scary models.",
    "start": "47833",
    "end": "53733"
  },
  {
    "text": "And a lot of this\ncontent is going to be stuff that\nhomework three is on, so it behooves you\nto pay attention.",
    "start": "53733",
    "end": "62199"
  },
  {
    "text": "And you're the people who\ncame, so you're already ahead of the curve. And if you pay attention,\nyou'll be even farther ahead",
    "start": "62200",
    "end": "68170"
  },
  {
    "text": "of the curve. So hopefully, by the\nend of the lecture, you'll have some familiarity\nwith some of these now pretty widely used methods\nfor unsupervised pre-training",
    "start": "68170",
    "end": "76225"
  },
  {
    "text": "and some of the ways that we\ncan then fine-tune these models once we've pre-trained them. And ideally, be ready to\nbreeze through homework three.",
    "start": "76225",
    "end": "85905"
  },
  {
    "text": "All right, so just\nto start this recap, so as Chelsea said\non Monday, we're doing unsupervised learning. We have some large\nunlabeled data set.",
    "start": "85905",
    "end": "92450"
  },
  {
    "text": "Ideally, a diverse\nunlabeled data set. We do this first\ngray arrow which is unsupervised pre-training,\nand this gives us",
    "start": "92450",
    "end": "98840"
  },
  {
    "text": "our pre-trained model. And then once we have\nour pre-trained model, we usually use some\noften small data",
    "start": "98840",
    "end": "105590"
  },
  {
    "text": "set for fine-tuning to\nspecialize this model to a particular task. And the goal is to get sort\nof a good predictive model",
    "start": "105590",
    "end": "111170"
  },
  {
    "text": "for this particular task.  And after that, we\ntalked quite a bit",
    "start": "111170",
    "end": "117808"
  },
  {
    "text": "about contrastive\nlearning on Monday. So the basic idea\nhere is that examples that are similar in\nsome sense should",
    "start": "117808",
    "end": "123370"
  },
  {
    "text": "have similar representations. And so this could be examples\nthat have the same class label. This could be examples\nthat just come",
    "start": "123370",
    "end": "130570"
  },
  {
    "text": "from nearby regions of an image\nif we don't have class labels. It could be augmented\nversions of the same example by flipping or cropping\nor adding noise.",
    "start": "130570",
    "end": "137800"
  },
  {
    "text": "Or if we have sort of temporally\nstructured data like videos, we can take video frames that\ncome near each other in time",
    "start": "137800",
    "end": "144730"
  },
  {
    "text": "to get sort of positive pairs. Another thing that\nwe talked about is it's not good enough to just\ntake things that are similar",
    "start": "144730",
    "end": "151610"
  },
  {
    "text": "and push their representations\ntogether because then we end up with this degeneracy\nwhere everything gets the same representation.",
    "start": "151610",
    "end": "157140"
  },
  {
    "text": "So we actually need to\ncontrast things, too, and that's how we get this\nwonderful name, contrastive learning.",
    "start": "157140",
    "end": "162380"
  },
  {
    "text": "And sort of the most\nstraightforward implementation of this is this triplet loss. So here we have a\ntriplet because we",
    "start": "162380",
    "end": "167960"
  },
  {
    "text": "have kind of our anchor\nexample, and then we have a positive example which\nwe say is sort of a similar one, and then the negative\nexample which is different.",
    "start": "167960",
    "end": "174920"
  },
  {
    "text": "We push together\nthe similar ones, and we pull apart\nthe negative ones. And then you can generalize this\nto have more than one negative.",
    "start": "174920",
    "end": "182060"
  },
  {
    "text": "So this is the sort of N-way\nclassification loss which is used in the SimCLR paper\nthat you might remember,",
    "start": "182060",
    "end": "189440"
  },
  {
    "text": "which is also known as\nthe NT-Xent or Normalized Temperature-scaled\nCross Entropy Loss, which is really an awesome name.",
    "start": "189440",
    "end": "198230"
  },
  {
    "text": "One minor correction\nfrom Monday is there were some questions\nabout whether or not the similarity score\nfor the positive pair",
    "start": "198230",
    "end": "204709"
  },
  {
    "text": "shows up in the numerator\nand the denominator. And it does, so\nin the SimCLR loss we have the similarity\nscore for the positive pair",
    "start": "204710",
    "end": "210910"
  },
  {
    "text": "in both the numerator\nand the denominator. And the reason that's\nuseful is since we have this positive score on the\ndenominator and the numerator,",
    "start": "210910",
    "end": "219230"
  },
  {
    "text": "we know that this fraction is\ngoing to be between 0 and 1. And so we can sort of interpret\nthis as a classification",
    "start": "219230",
    "end": "224870"
  },
  {
    "text": "probability. And so really, we\ncan kind of just think about this\ncontrastive objective as like training a classifier\nwith our representation.",
    "start": "224870",
    "end": "231410"
  },
  {
    "text": "So it makes it a little\neasier to think about the loss and what it's doing. ",
    "start": "231410",
    "end": "236616"
  },
  {
    "text": "All right, how do we feel?  Yeah, I love the enthusiasm.",
    "start": "236617",
    "end": "243340"
  },
  {
    "text": "Keep bringing the energy, folks. OK, so now let's talk about\nsome reconstruction-based",
    "start": "243340",
    "end": "249159"
  },
  {
    "text": "unsupervised objectives. So again, the main difference\nbetween these methods and the contrastive\nmethods are we're not",
    "start": "249160",
    "end": "255549"
  },
  {
    "text": "comparing and contrasting\nmultiple examples. We're sort of doing this\none example at a time, which makes things a little easier.",
    "start": "255550",
    "end": "260739"
  },
  {
    "text": "And the simple intuition\nfor these methods is that a good\nrepresentation of our data is one that lets\nus reconstruct it.",
    "start": "260740",
    "end": "266479"
  },
  {
    "text": "So if we have some image\nof this lovely little dog, we might pass it\nthrough our encoder to get a representation.",
    "start": "266480",
    "end": "272410"
  },
  {
    "text": "And ideally, if we have\na good representation, we should be able to decode\nthis representation back",
    "start": "272410",
    "end": "279760"
  },
  {
    "text": "into our original input. And ideally, with a decoder,\nthat's not absolutely massive.",
    "start": "279760",
    "end": "285460"
  },
  {
    "text": "It should be sort of a\nreasonable size decoder.",
    "start": "285460",
    "end": "291270"
  },
  {
    "text": "And sort of the bonus\nof this type of setup is that we don't have to worry\nabout things like sampling negatives or using\nlarge batch sizes",
    "start": "291270",
    "end": "297330"
  },
  {
    "text": "because again, this objective\nis purely defined on just a single training example. And so we don't have this\nsort of more complicated",
    "start": "297330",
    "end": "304470"
  },
  {
    "text": "relationship between\nmultiple examples and a batch to worry about. ",
    "start": "304470",
    "end": "310703"
  },
  {
    "text": "OK, so that gives\nyou a little bit of a flavor of what\nwe're talking about. We can make it a\nlittle more concrete.",
    "start": "310703",
    "end": "315970"
  },
  {
    "text": "So we have this intuition that\na good representation is what lets us reconstruct the input.",
    "start": "315970",
    "end": "321009"
  },
  {
    "text": "So how do we actually\ntrain the thing? Well, we really just\ndefine some loss function that tells us how close\nour reconstruction is",
    "start": "321010",
    "end": "327690"
  },
  {
    "text": "to the original input. So for images, we might use\njust L2 or squared L2 distance.",
    "start": "327690",
    "end": "333933"
  },
  {
    "text": "And then we train this\nwhole thing end to end. So we train the encoder\nand the decoder end to end. And hopefully, by\nthe end, our encoder",
    "start": "333933",
    "end": "340110"
  },
  {
    "text": "gives us representations\nthat are useful. So that's kind of a\nneat story, but I'm curious if people have any\ncreeping suspicions on what",
    "start": "340110",
    "end": "348530"
  },
  {
    "text": "can go wrong in this situation. If you've ever trained a VA,\nyou might know the answer.",
    "start": "348530",
    "end": "357660"
  },
  {
    "text": "Everything-- it is\nincluded under Everything. Yes? So [INAUDIBLE]\nmemorize the input?",
    "start": "357660",
    "end": "368980"
  },
  {
    "text": "Boom. You saw my slides, didn't you? Very impressive. So yeah.",
    "start": "368980",
    "end": "374130"
  },
  {
    "text": "So I mean, one way to think\nabout this is according to this loss function\nwe've written down, would the identity function\nbe a good encoder or decoder?",
    "start": "374130",
    "end": "379650"
  },
  {
    "text": " That's not actually\ntotally rhetorical. I'm curious if it would be.",
    "start": "379650",
    "end": "386160"
  },
  {
    "text": "So the answer is\nyes, it would be because if you have the\nidentity function as an encoder, r is just x. And if the decoder is the\nidentity function, x hat is x,",
    "start": "386160",
    "end": "393360"
  },
  {
    "text": "or our loss is 0. And that's a great loss\nfunction or a great encoder",
    "start": "393360",
    "end": "398530"
  },
  {
    "text": "and decoder for this loss,\nbut it's not really useful. So the question is,\nhow do we fix this? And there are a lot of\ndifferent ways to fix this,",
    "start": "398530",
    "end": "404910"
  },
  {
    "text": "but we're going to focus on\none common one that people use. So if r is basically\nunconstrained,",
    "start": "404910",
    "end": "412169"
  },
  {
    "text": "if it's just as high\ndimensional as our input, then there's no reason\nthe encoder can't just",
    "start": "412170",
    "end": "417540"
  },
  {
    "text": "copy the input into\nthe representation, and then the decoder just\ncopies that representation to be its prediction. And this is not really useful.",
    "start": "417540",
    "end": "423940"
  },
  {
    "text": "We're not learning\nanything meaningful. And so the key idea that people\nuse to kind of get around this issue is adding\nsome sort of a bottleneck",
    "start": "423940",
    "end": "430530"
  },
  {
    "text": "on our representation. So the most common, maybe\nmost intuitive type is just make it lower dimensional. So we might have an image\nthat's like 100 by 100 pixels,",
    "start": "430530",
    "end": "439710"
  },
  {
    "text": "and our representation\nmight only be 64 dimensions. So intuitively, it's much harder\nto just copy the whole image",
    "start": "439710",
    "end": "444930"
  },
  {
    "text": "into this low-dimensional space. And so the hope is that\nthese latent dimensions in our representation\nwill actually",
    "start": "444930",
    "end": "451133"
  },
  {
    "text": "be forced to represent some\nhigh-level concepts that are more efficient in\nstoring information than just memorizing one pixel at a time.",
    "start": "451133",
    "end": "457500"
  },
  {
    "text": " OK, so let's say we\ntrain this model now. So we have this encoder\nthat compresses our input",
    "start": "457500",
    "end": "464430"
  },
  {
    "text": "into some more compact, in\nsome sense, representation. How do we actually do the\nfew-shot learning part?",
    "start": "464430",
    "end": "471360"
  },
  {
    "text": "Well, we'll throw\naway our decoder, and we can just initialize\na prediction head",
    "start": "471360",
    "end": "477060"
  },
  {
    "text": "on top of this representation. So really, what\nwe've just learned is a feature\nextractor of our data. And now we'll just treat this\nas a normal machine learning",
    "start": "477060",
    "end": "484139"
  },
  {
    "text": "problem. We can fit some sort\nof prediction head to these representations. Instead of the raw images,\nthis could be an MLP,",
    "start": "484140",
    "end": "489900"
  },
  {
    "text": "or it could just be\nlike an SVM or whatever. And we'll train it to make\na classification score.",
    "start": "489900",
    "end": "496643"
  },
  {
    "text": "So here if we're doing like\nfive-way classification, we just have a classifier now.",
    "start": "496643",
    "end": "501650"
  },
  {
    "text": "And the simplest\nrecipe here is we'll freeze the encoder and just\nfine-tune this prediction head or just fit the prediction head.",
    "start": "501650",
    "end": "507590"
  },
  {
    "text": " OK, so this is the basic\nautoencoder or bottleneck",
    "start": "507590",
    "end": "514559"
  },
  {
    "text": "autoencoder setup,\nwhich is kind of the-- you can maybe think of\nlike the simplest thing that kind of works.",
    "start": "514559",
    "end": "521070"
  },
  {
    "text": "And the pros here are\nthat it is very simple. It's very general. You can kind of do this for\npretty much any sort of data because all you need to do is\npick this loss, this distance",
    "start": "521070",
    "end": "528510"
  },
  {
    "text": "function, which you\ncan usually come up with some sort of distance\nfunction for most data modalities.",
    "start": "528510",
    "end": "535118"
  },
  {
    "text": "You also have this\nadvantage compared to contrastive\nlearning that we don't need to select these\npositive and negative pairs. And this is often the sort\nof most difficult part",
    "start": "535118",
    "end": "543282"
  },
  {
    "text": "of the contrastive\nlearning set up. And so it's really nice\nto get rid of that. But the major downside\nhere, in particular,",
    "start": "543282",
    "end": "551329"
  },
  {
    "text": "is that we have to design\nthis bottlenecking mechanism. And it turns out this is\nreally hard to do well.",
    "start": "551330",
    "end": "556890"
  },
  {
    "text": "So you can use this sort of\nlow-dimensional bottleneck. In practice, this\ndoesn't really give you that good of representations\neven if you make",
    "start": "556890",
    "end": "564350"
  },
  {
    "text": "it relatively low-dimensional. And you might wonder,\nwell, why is that?",
    "start": "564350",
    "end": "570100"
  },
  {
    "text": "And a lot of this is known\nmostly just empirically by training them. But what you end\nup with is still",
    "start": "570100",
    "end": "577040"
  },
  {
    "text": "sort of representations that\nmostly memorize the images and don't actually\nencode any sort",
    "start": "577040",
    "end": "582889"
  },
  {
    "text": "of high-level or\ninterpretable concepts that are likely to generalize\nwell beyond just reconstructing",
    "start": "582890",
    "end": "588620"
  },
  {
    "text": "this exact image. And you really do kind of\nend up with representations that just encode the\nspecific details needed",
    "start": "588620",
    "end": "594890"
  },
  {
    "text": "to minimize the\nreconstruction loss but not really anything that is\neasily adaptable to new tasks.",
    "start": "594890",
    "end": "600282"
  },
  {
    "text": "And so what ends up\nhappening a lot of the time is r is more like a\nhash of your input than a sort of\nconceptual summary.",
    "start": "600282",
    "end": "607610"
  },
  {
    "text": "And if you think about-- for example, using the gzip\nrepresentation of an image as its representation\nfor fine-tuning,",
    "start": "607610",
    "end": "614120"
  },
  {
    "text": "that's not going to be a very\ninformative representation even though it's\ngoing to be smaller than the original image.",
    "start": "614120",
    "end": "619560"
  },
  {
    "text": "And so just compressing is not-- at least in this sense\nof dimensionality is not good enough\nto actually force",
    "start": "619560",
    "end": "625670"
  },
  {
    "text": "our encoder to squeeze out sort\nof useful reusable features. So how do we\nencourage the encoder",
    "start": "625670",
    "end": "631940"
  },
  {
    "text": "to extract these high-level\nreusable features. And one strategy that\npeople use is just other types of bottlenecks.",
    "start": "631940",
    "end": "637670"
  },
  {
    "text": "So we can bottleneck our\nrepresentation in many ways.",
    "start": "637670",
    "end": "643190"
  },
  {
    "text": "We can use information\nbottlenecks that essentially add noise\nto the representation. We can use sparsity\nbottlenecks that",
    "start": "643190",
    "end": "649460"
  },
  {
    "text": "constrain the number of\ndimensions that can be nonzero. We can also use\ncapacity bottlenecks to force the decoder to\nbe relatively weak so",
    "start": "649460",
    "end": "657170"
  },
  {
    "text": "that the representation needs\nto be sort of easy to decode. But the strategy that actually\nis most common in practice",
    "start": "657170",
    "end": "665810"
  },
  {
    "text": "is to worry less about\ndesigning a bottleneck and just make the\ntask a little bit more difficult to force the\nencoder to learn something",
    "start": "665810",
    "end": "673010"
  },
  {
    "text": "a little less trivial. And that's going to take\nus to our next section on masked autoencoders, which is\nsort of a class of models that",
    "start": "673010",
    "end": "681490"
  },
  {
    "text": "encompasses a lot of the\nlarge pre-trained models that we have these days\nthat we use in practice.",
    "start": "681490",
    "end": "687755"
  },
  {
    "text": "How are we doing so far? Any questions? Is everyone having fun, though?",
    "start": "687755",
    "end": "695280"
  },
  {
    "text": "Yes, very much. ",
    "start": "695280",
    "end": "700510"
  },
  {
    "text": "[INAUDIBLE] Does it\nmean you can somehow extract the information\nabout the samples that",
    "start": "700510",
    "end": "706823"
  },
  {
    "text": "run into the autoencoder? Just by taking a look\nat the encoding--  Yeah.",
    "start": "706823",
    "end": "711960"
  },
  {
    "text": "So in a sense, you can. So it depends on the exact\nmodel and the capacity and the training data. But in the same way\nthat you can-- hash",
    "start": "711960",
    "end": "720270"
  },
  {
    "text": "functions are generally\ndifficult to invert. But if they're imperfect,\nyou can invert them.",
    "start": "720270",
    "end": "726120"
  },
  {
    "text": "A lot of times, yeah,\nyou can not only just decode a particular\nrepresentation back",
    "start": "726120",
    "end": "731130"
  },
  {
    "text": "into its image, but\nyou can kind of fiddle with these models in\na way that lets you extract multiple examples from\nthe training distribution,",
    "start": "731130",
    "end": "737620"
  },
  {
    "text": "which can pose sort\nof a privacy risk.  Does that-- great.",
    "start": "737620",
    "end": "744730"
  },
  {
    "text": "Yes. So from your perspective\nfor few shot performance of these models,\nsince the model is",
    "start": "744730",
    "end": "752889"
  },
  {
    "text": "kind of just memorizing details\nfrom the original image. If you just [INAUDIBLE]",
    "start": "752890",
    "end": "760042"
  },
  {
    "start": "760042",
    "end": "765200"
  },
  {
    "text": "Yeah. So I mean, if you have-- I guess the question is, if\nthe representations aren't that great, can we get around\nthat by just making",
    "start": "765200",
    "end": "772839"
  },
  {
    "text": "our predictor higher capacity? And we definitely can\nif we have a lot of data that we're able to fine-tune on.",
    "start": "772840",
    "end": "778040"
  },
  {
    "text": "But the idea is in\nthe best-case setting, we're going to learn\nrepresentations",
    "start": "778040",
    "end": "783700"
  },
  {
    "text": "that let us perform well\non our downstream tasks with really simple predictors\nbecause the simple predictors are the ones that we can fit\nwith only a little bit of data",
    "start": "783700",
    "end": "791050"
  },
  {
    "text": "that are very difficult\nto sort of overfit with. And so if we need to\ndo this sort of thing,",
    "start": "791050",
    "end": "796210"
  },
  {
    "text": "that sort of means\nlike our objectives are not working very\nwell because ultimately, it just means we're\nnot going to be able to do few-shot learning\nas well because we'll need",
    "start": "796210",
    "end": "803332"
  },
  {
    "text": "a lot of data to fine-tune. Yeah? [INAUDIBLE] go back to\n[INAUDIBLE] image [INAUDIBLE]",
    "start": "803332",
    "end": "822940"
  },
  {
    "text": "that kind of thing. Yeah. So I mean, you could imagine\nsort of like a cross-encoder, where instead of reconstructing\nx, you reconstruct like X-PAWS.",
    "start": "822940",
    "end": "831820"
  },
  {
    "text": "Yeah, so I don't-- there's not\na specific paper I'm thinking of that's extensively\nevaluated this sort of thing.",
    "start": "831820",
    "end": "838280"
  },
  {
    "text": "But sort of colloquially,\nyou can do that. I'm not sure how it\ndirectly compares to stuff",
    "start": "838280",
    "end": "844510"
  },
  {
    "text": "like BERT or yeah,\nany of these models here but in a way that sort of\nis what masked autoencoders do.",
    "start": "844510",
    "end": "855910"
  },
  {
    "text": "In a sense, they do come up with\nthese positive pairs and then-- just hang on, all right?",
    "start": "855910",
    "end": "863270"
  },
  {
    "text": "But if you're still wondering\nthat at the end, come grab me. All right. So we want to go beyond\nthis bottlenecked approach",
    "start": "863270",
    "end": "869000"
  },
  {
    "text": "to the autoencoders\nbecause designing a good bottleneck that\nactually forces our encoder",
    "start": "869000",
    "end": "875090"
  },
  {
    "text": "to give us good features\nis really quite tricky. And so the common\nsolution people use now",
    "start": "875090",
    "end": "880880"
  },
  {
    "text": "is called masked autoencoders. And the issue that\nthis sort of gets around is like our regular\nautoencoders are really trying",
    "start": "880880",
    "end": "886490"
  },
  {
    "text": "to predict x from like x. And this is sort of\na degenerate task.",
    "start": "886490",
    "end": "892370"
  },
  {
    "text": "And so we add this\nbottleneck to try to avoid our\ndegenerate solutions, but maybe the task\nis just too easy. And the easiest way\nto solve the issue",
    "start": "892370",
    "end": "899560"
  },
  {
    "text": "is not to design a really fancy\nor really perfect bottleneck but just make the task\na little bit harder. And so that's sort\nof the approach",
    "start": "899560",
    "end": "905810"
  },
  {
    "text": "that we're taking with\nmasked autoencoders, where we no longer predict x from x. Instead, we just mask out\na little bit of the input,",
    "start": "905810",
    "end": "913580"
  },
  {
    "text": "and we try to reconstruct\nthe other part of the input. And so in this sense, going\nback to your question earlier,",
    "start": "913580",
    "end": "919160"
  },
  {
    "text": "we sort of are doing this\nin that now x and y are not the same. They're sort of like\nthis positive pair",
    "start": "919160",
    "end": "924740"
  },
  {
    "text": "that's generated by literally\njust like partitioning the input into pieces.",
    "start": "924740",
    "end": "929750"
  },
  {
    "text": "And this does work really well,\nso you're just like five years too late. But otherwise, that would have\nbeen like a brilliant research",
    "start": "929750",
    "end": "937160"
  },
  {
    "text": "idea. So that's sort of the gist\nof masked autoencoders. And even though this is a\nrelatively small departure",
    "start": "937160",
    "end": "944750"
  },
  {
    "text": "from the autoencoders\nwe already saw, it's a really, really\npowerful framework for representation learning that\nis pretty widely applicable.",
    "start": "944750",
    "end": "955010"
  },
  {
    "text": "OK, so let's go over kind of\nthe general recipe for how masked autoencoders get\ntrained in practice.",
    "start": "955010",
    "end": "960830"
  },
  {
    "text": "So again, just like when we're\ntraining a regular autoencoder, we need to pick some\nsort of distance function that tells us how good\nour reconstructions are,",
    "start": "960830",
    "end": "968000"
  },
  {
    "text": "given the original input. So this part is the same. And then for training\nbatch of examples,",
    "start": "968000",
    "end": "973973"
  },
  {
    "text": "for each example, what\nwe're going to do first is we're going to sample\nwhat we're going to define, which is a masking function.",
    "start": "973973",
    "end": "979170"
  },
  {
    "text": "So we're going to\ntake that example, and we're going to\ngenerate x tilde, which is sort of the masked out\nversion of that example",
    "start": "979170",
    "end": "985580"
  },
  {
    "text": "and then y, which is\nlike usually just going to be everything else. So typically, x\ntilde and y are just",
    "start": "985580",
    "end": "991190"
  },
  {
    "text": "going to be disjoint\nsubregions of the input. So in the image\nexample, it's literally like partitioning the image. And one part is x tilde.",
    "start": "991190",
    "end": "997340"
  },
  {
    "text": "The other part is y. And then we're going to make\na prediction with our model, f theta, that gets not\nx but x tilde's input.",
    "start": "997340",
    "end": "1005477"
  },
  {
    "text": "And then we'll compute our loss. So our loss is going\nto be between y, which is the sort of held\nout part of the input,",
    "start": "1005477",
    "end": "1010790"
  },
  {
    "text": "and y hat, which is what\nour model thought the held out part of our input is. In some cases, the\ntarget might just",
    "start": "1010790",
    "end": "1015980"
  },
  {
    "text": "be all of x, not just\nthe held out part. But this is a\nrelatively minor detail that doesn't make a huge\ndifference in practice.",
    "start": "1015980",
    "end": "1023440"
  },
  {
    "text": "And so these highlighted\nparts are basically our design choices or our control knobs. And two of them are\npretty much the same",
    "start": "1023440",
    "end": "1029490"
  },
  {
    "text": "as regular autoencoders, the\nmodel and the loss function. It's really just\nthis masking function that we are now able to define,\nwhich gives us sort of one",
    "start": "1029490",
    "end": "1037439"
  },
  {
    "text": "more knob to turn. And so again, as an example,\nin the case of an image, we'll split it up\ninto two subregions.",
    "start": "1037440",
    "end": "1044470"
  },
  {
    "text": "And our model might be a\nconvnet or a transformer, which we'll talk\nabout a little later.",
    "start": "1044470",
    "end": "1049820"
  },
  {
    "text": "And our distance function\nmight just be L2 distance. In the case of language or\none-dimensional sequences,",
    "start": "1049820",
    "end": "1057340"
  },
  {
    "text": "we might just mask out\na couple of the words or the tokens in our sequence. And that's sort of\nour noisy input.",
    "start": "1057340",
    "end": "1063880"
  },
  {
    "text": "And our targets are just\nthe words that were there or the original data there.",
    "start": "1063880",
    "end": "1070010"
  },
  {
    "text": "Here, often, and pretty\nmuch all the time now, the model that we\nuse is a transformer, such as BERT, which, again,\nwe'll talk about soon.",
    "start": "1070010",
    "end": "1078440"
  },
  {
    "text": "And the loss function\nor the distance function here is usually KL\nDivergence or cross entropy",
    "start": "1078440",
    "end": "1083690"
  },
  {
    "text": "between the model's\npredicted distribution of what it thinks the\nmissing words were and what they actually were.",
    "start": "1083690",
    "end": "1088910"
  },
  {
    "text": " OK, so probably to\ntalk about a couple of specific instantiations\nof this framework,",
    "start": "1088910",
    "end": "1097070"
  },
  {
    "text": "probably the most famous one\nis BERT, which maybe you've heard about, which is this\nlarge transformer model that's",
    "start": "1097070",
    "end": "1102410"
  },
  {
    "text": "pre-trained on a huge\namount of text that's really good for\nfine-tuning on new usually",
    "start": "1102410",
    "end": "1107600"
  },
  {
    "text": "classification tasks. And basically, BERT gets\nas input two sentences,",
    "start": "1107600",
    "end": "1113570"
  },
  {
    "text": "but that's sort of a\ndetail of BERT training. It computes representations\nof the sentences, and then it predicts\nbasically the missing words",
    "start": "1113570",
    "end": "1120515"
  },
  {
    "text": "in both of those sentences. And it just does this task\nfor many, many TPU hours, and then you get out a\ngood model at the end.",
    "start": "1120515",
    "end": "1128860"
  },
  {
    "text": "So let's look at this\nin a little more detail. So again, we might\nhave this text input.",
    "start": "1128860",
    "end": "1134900"
  },
  {
    "text": "And here, again, as sort\nof just a quirk of BERT, we use two sentences instead\nof just one for training,",
    "start": "1134900",
    "end": "1140570"
  },
  {
    "text": "but it's not a huge-- it's not hugely important. And so we apply our\nmasking function,",
    "start": "1140570",
    "end": "1146580"
  },
  {
    "text": "which gives us our masked\nout version of that input. And I've just annotated the\ntime step in the sequence here that hopefully will make\nthings a little easier",
    "start": "1146580",
    "end": "1153058"
  },
  {
    "text": "to see what's going on. And so I've randomly sampled\nthree of the time steps to mask out.",
    "start": "1153058",
    "end": "1158660"
  },
  {
    "text": "And I'm going to define\nmy targets which are just the original words there.",
    "start": "1158660",
    "end": "1164430"
  },
  {
    "text": "And then we stick\nthis entire sequence with the mask\ntokens through BERT. And that gives us a\nprediction for every time step",
    "start": "1164430",
    "end": "1171330"
  },
  {
    "text": "in the sequence of what\nthe word is likely to be, a probability distribution\nover all words of what",
    "start": "1171330",
    "end": "1176460"
  },
  {
    "text": "it thinks the word is there. That probability distribution\nisn't very interesting, except for the time steps\nwhere we masked out the word.",
    "start": "1176460",
    "end": "1182370"
  },
  {
    "text": "Because for all the\nother time steps, we know what the word is. So we just compute our loss on\nthose masked out time steps. And this gives us basically\nthree distributions",
    "start": "1182370",
    "end": "1190019"
  },
  {
    "text": "that are actually needed\nfor training, which is the model's distribution\nfor time step two, for time step six,\nand time step nine.",
    "start": "1190020",
    "end": "1195810"
  },
  {
    "text": " And our loss function here again\nis going to be KL Divergence.",
    "start": "1195810",
    "end": "1201860"
  },
  {
    "text": "And this is going\nto work out to be the cross entropy between the\nsort of one hot distribution",
    "start": "1201860",
    "end": "1207170"
  },
  {
    "text": "that is our actual labels here. So basically, a 1\non Biden and a 0 for everything else\nand the distribution",
    "start": "1207170",
    "end": "1213980"
  },
  {
    "text": "that our model predicted for\neach of those time steps. So what that works out\nto be is the negative log likelihood of Biden on this\ndistribution for time step two,",
    "start": "1213980",
    "end": "1222710"
  },
  {
    "text": "the negative log\nlikelihood of president for this distribution\nat time step six, and the negative log likelihood\nof was at this time step nine.",
    "start": "1222710",
    "end": "1230780"
  },
  {
    "text": "And we just sum those or really\naverage often in practice. And this is our\nloss for training.",
    "start": "1230780",
    "end": "1237919"
  },
  {
    "text": "And then we just optimize this\nend to end, and we are happy.",
    "start": "1237920",
    "end": "1243190"
  },
  {
    "text": "Just a couple of details\nabout the way that BERT does masking just for\ncompleteness, BERT masks",
    "start": "1243190",
    "end": "1250750"
  },
  {
    "text": "out 15% of the time steps\nat random just uniformly. And of these 15%,\n80% of them get",
    "start": "1250750",
    "end": "1257830"
  },
  {
    "text": "replaced by literally the mask\ntoken that I've used here. And the other 20%\nactually get replaced with just a random word\ninstead of a mask token.",
    "start": "1257830",
    "end": "1266730"
  },
  {
    "text": "I'm curious if you\njust think for a moment why we might want to\nuse a mixture of mask",
    "start": "1266730",
    "end": "1272080"
  },
  {
    "text": "tokens and random tokens\ninstead of just replacing it with a mask token all the time\nif you think about how we're",
    "start": "1272080",
    "end": "1277750"
  },
  {
    "text": "ultimately going to use this\nmodel once we've pre-trained it. ",
    "start": "1277750",
    "end": "1284559"
  },
  {
    "text": "Yeah? You don't want to\nconstrain your model to only a sentence,\nis that you've seen [INAUDIBLE] new sequences?",
    "start": "1284560",
    "end": "1290136"
  },
  {
    "start": "1290136",
    "end": "1299809"
  },
  {
    "text": "Yeah. So you definitely don't\nwant to sort of overfit to the types of sentences\nthat you see during training,",
    "start": "1299810",
    "end": "1305830"
  },
  {
    "text": "but it's sort of a\nspecific type of sentence we're only seeing during--\nthat we would only be seeing during training\nthat we want to avoid.",
    "start": "1305830",
    "end": "1311070"
  },
  {
    "text": "And that's what I'm\ntrying to get at. So we would be able to\ncorrect [INAUDIBLE]??",
    "start": "1311070",
    "end": "1316750"
  },
  {
    "text": "Right. So in other words,\nwe also want to have good representations\nof words that are not mask tokens, right?",
    "start": "1316750",
    "end": "1323560"
  },
  {
    "text": "So in a sense, during\ntraining, if we replace a word\nwith a random word and we ask our model to tell\nus what the original word was",
    "start": "1323560",
    "end": "1330280"
  },
  {
    "text": "there, now our model, in\naddition to just filling in mask tokens, is\nalso basically solving",
    "start": "1330280",
    "end": "1336100"
  },
  {
    "text": "this task of for every\nword in the sequence, assume the word that you see\nhere is not what was actually",
    "start": "1336100",
    "end": "1342190"
  },
  {
    "text": "there in the original text. What do you think the\nword would have been? And so now we have a model\nthat not only gives us",
    "start": "1342190",
    "end": "1347242"
  },
  {
    "text": "good representations of\nwords that are masked out but every word in the sentence. And that's important\nbecause at fine-tuning time",
    "start": "1347242",
    "end": "1353259"
  },
  {
    "text": "we're going to be getting\nsentences that have no mask tokens, and then\nwe're just going to be shoving like real\ndata into the model. And we don't want to be overfit.",
    "start": "1353260",
    "end": "1359259"
  },
  {
    "text": "Like you were saying,\nto only the data we see in training, which\nalways has mask tokens in it. So that's sort of the motivation\nfor why we sometimes actually",
    "start": "1359260",
    "end": "1365503"
  },
  {
    "text": "just mix in a random word. Yeah? So when predicting the\nidentity of one masked token,",
    "start": "1365503",
    "end": "1371720"
  },
  {
    "text": "it cannot see the identity\nof the other mask tokens? No. Yeah, and that's for\ncomputational reasons.",
    "start": "1371720",
    "end": "1378260"
  },
  {
    "text": "I mean, you could. You could only mask\none token at a time, but then you're sort of using\nonly a very small amount",
    "start": "1378260",
    "end": "1383600"
  },
  {
    "text": "of your sequence for\ntraining at every time step. And you have to make\nyour batch very big. And the thing that makes\nthat difficult is now you",
    "start": "1383600",
    "end": "1390770"
  },
  {
    "text": "basically need to tune this\npercentage of the input that you're masking, right? If you don't mask\nenough, then you're not really like learning very\nquickly because you're only",
    "start": "1390770",
    "end": "1397640"
  },
  {
    "text": "predicting one word at a time. But if you mask too\nmuch, now the task is too hard because you can\nonly see a couple of words. And there's a lot\nof blood has been",
    "start": "1397640",
    "end": "1403760"
  },
  {
    "text": "spilled trying to\nfigure out the right way to sort of do that masking. ",
    "start": "1403760",
    "end": "1409919"
  },
  {
    "text": "And as a perfect segue\nto this final point is it is possible that we\ncan do better than just picking random time steps.",
    "start": "1409920",
    "end": "1416737"
  },
  {
    "text": "And two common ways\npeople do this are they mask out longer spans of\ntext at a time, so not just like a single word\nhere and a single word",
    "start": "1416738",
    "end": "1422790"
  },
  {
    "text": "there, and also do what we\ncall Salient Span Masking. So looking for kind of\ninformation-dense parts",
    "start": "1422790",
    "end": "1428160"
  },
  {
    "text": "of the sequence or the\nimage and masking those out instead of just like masking\nout random words like was or on",
    "start": "1428160",
    "end": "1433560"
  },
  {
    "text": "or something. And in certain\nsettings, these can make a huge difference in how\ngood your pre-trained model is.",
    "start": "1433560",
    "end": "1439039"
  },
  {
    "text": "We have three questions. I think I saw you and\nthen you and then you. Will there be work on\nlooking at how similar",
    "start": "1439040",
    "end": "1446220"
  },
  {
    "text": "the random tokens are versus-- how similar the random tokens\nare to the ground truth tokens",
    "start": "1446220",
    "end": "1452610"
  },
  {
    "text": "and how that might affect\nBERT training outcomes? You want it to be,\nyou can make it",
    "start": "1452610",
    "end": "1459720"
  },
  {
    "text": "to be more difficult by\nmaking it [INAUDIBLE] synaptically similar but\nnot exactly the same.",
    "start": "1459720",
    "end": "1465212"
  },
  {
    "text": "Yeah, that's a good question. I'm not aware of actually\nablating the criterion for how",
    "start": "1465212",
    "end": "1470370"
  },
  {
    "text": "you pick a random token. As far as I know, BERT\njust samples uniformly from the entire vocabulary.",
    "start": "1470370",
    "end": "1476525"
  },
  {
    "text": "But it'll be\ninteresting to see-- if you pick synonyms,\nbasically, is that harder? Yeah, it's a good question.",
    "start": "1476525",
    "end": "1483750"
  },
  {
    "text": "Someone back here. I'm sorry, [INAUDIBLE] to figure\nout what the random word was. ",
    "start": "1483750",
    "end": "1494370"
  },
  {
    "text": "What the original word was. Yeah. Yes.",
    "start": "1494370",
    "end": "1499825"
  },
  {
    "text": "If you could repeat the\nquestion for the recording. Oh, yeah, yeah, yeah, sure. Sure. So the last question\nwas just that",
    "start": "1499825",
    "end": "1506230"
  },
  {
    "text": "what do you actually do when\nyou put in a random word. What are you predicting? And the idea is you replace--",
    "start": "1506230",
    "end": "1511299"
  },
  {
    "text": "like, for position nine, you\nreplace was with like apple. And then the model has to\npredict what word originally",
    "start": "1511300",
    "end": "1517210"
  },
  {
    "text": "was there. When you say we're selecting\nfor information dense spans, is it like, what\nyou're saying is we're looking for something\nlike high entropy, or how do you define that? ",
    "start": "1517210",
    "end": "1527912"
  },
  {
    "text": "Yeah, that's a good question. So the question was, when I say\ninformation-dense spans, what does that actually mean?",
    "start": "1527912",
    "end": "1532951"
  },
  {
    "text": "There are a lot of\nheuristics people use, but oftentimes, people will\njust take another model that",
    "start": "1532952",
    "end": "1539160"
  },
  {
    "text": "has been trained to\nrecognize essentially like dates and names and\njust pick out all the dates",
    "start": "1539160",
    "end": "1545100"
  },
  {
    "text": "and names, which often end up\nbeing high-entropy portions of the text, right? If you say my name\nis blank, it's",
    "start": "1545100",
    "end": "1550707"
  },
  {
    "text": "going to extremely\nhigh-entropy distribution because it could\njust be anything. So essentially that. But it's difficult to actually\ncompute the entropy itself,",
    "start": "1550707",
    "end": "1557980"
  },
  {
    "text": "so we use this proxy\nwhich is classifying the type of word it is. Yeah.",
    "start": "1557980",
    "end": "1563120"
  },
  {
    "text": "Why don't we use a random\nmask for all of this, instead of the masking, just\nuse a random token, maybe?",
    "start": "1563120",
    "end": "1570080"
  },
  {
    "text": "Mm-hmm. That's a good question. I guess I would suspect\nthat using random tokens",
    "start": "1570080",
    "end": "1577550"
  },
  {
    "text": "are a little bit more\ndamaging than mask tokens because for a mask token,\nthe model sort of knows",
    "start": "1577550",
    "end": "1583100"
  },
  {
    "text": "it can ignore that time step. There's no information there. Whereas if you replace\neverything with a random token, the model sort of now\nhas to solve two tasks.",
    "start": "1583100",
    "end": "1590000"
  },
  {
    "text": "First, it has to figure out\nwhich are the time steps that have real tokens and\nwhich are the ones that",
    "start": "1590000",
    "end": "1595730"
  },
  {
    "text": "don't have real tokens. And then once it has\nfigured that out, then it has to try\nto figure out what was the original token\nfor those time steps",
    "start": "1595730",
    "end": "1602480"
  },
  {
    "text": "where they are sort of\ndetermined to be false. So I don't know. That's just my one\npotential guess.",
    "start": "1602480",
    "end": "1607610"
  },
  {
    "text": "But it seems like it\ncould just be harder. But it's a good question. Yeah. [INAUDIBLE] the tokens?",
    "start": "1607610",
    "end": "1615655"
  },
  {
    "text": "So for BERT, we just compute\nthe loss on the masked out tokens because-- So then, what happens\nto the random tokens?",
    "start": "1615655",
    "end": "1621688"
  },
  {
    "text": "Oh, no, yeah. I'm sorry, including\nthe random tokens. So yeah, the loss\nis just computed on tokens that are perturbed,\nso masked or replaced",
    "start": "1621688",
    "end": "1629280"
  },
  {
    "text": "with a random token. But the tokens that\nare not changed, we don't compute a loss for. So what if BERT tries to\nchange the tokens which",
    "start": "1629280",
    "end": "1636580"
  },
  {
    "text": "weren't supposed to be changed? We don't compute a loss\nfor those tokens at all,",
    "start": "1636580",
    "end": "1642000"
  },
  {
    "text": "so it doesn't matter. But intuitively,\nwhat should happen is, since we are replacing\nsome of the tokens with random words,\nBERT sort of should",
    "start": "1642000",
    "end": "1649110"
  },
  {
    "text": "be acting as if every\nword might have been replaced with a random word. And so it should be giving you\na representation at every time",
    "start": "1649110",
    "end": "1655919"
  },
  {
    "text": "step that is useful\nfor predicting kind of the likely words\nthat would have gone there. So would that be like\nadversarial tokens",
    "start": "1655920",
    "end": "1663190"
  },
  {
    "text": "which could like completely\nmess up the whole thing? Absolutely. They're pretty\nrare, but there are",
    "start": "1663190",
    "end": "1668527"
  },
  {
    "text": "people who basically use BERT to\ncome up with adversarial things to mask out to train\nanother model like BERT.",
    "start": "1668527",
    "end": "1675865"
  },
  {
    "text": "I'm going to move on just\nbecause we have lot of content, but I love the questions. I really do.",
    "start": "1675865",
    "end": "1683660"
  },
  {
    "text": "OK, so zooming\nback out from BERT, I hope that was useful to see\nthat in a little more detail.",
    "start": "1683660",
    "end": "1688970"
  },
  {
    "text": "Although BERT is\nthe most famous, this sort of masked\npre-training kind of approach is not a language-specific\nphenomenon.",
    "start": "1688970",
    "end": "1694580"
  },
  {
    "text": "And especially recently, people\nfound that this works really, really well for learning\nrepresentations of images, too,",
    "start": "1694580",
    "end": "1701375"
  },
  {
    "text": "and also with transformers. So it's a very, very similar\nkind of learning setup. So here for an image, what\nwe do is we chop the image",
    "start": "1701375",
    "end": "1708860"
  },
  {
    "text": "up into a sequence essentially. So we turn the image into a grid\nof like subregions or patches.",
    "start": "1708860",
    "end": "1716510"
  },
  {
    "text": "We mask out-- in this\ncase, instead of 15%, we mask out 75%, which\ngives you some sense",
    "start": "1716510",
    "end": "1722000"
  },
  {
    "text": "of the way that\ninformation is spread out or how information-dense\nlike the signals",
    "start": "1722000",
    "end": "1728120"
  },
  {
    "text": "are in these two cases,\nwhich is sort of interesting. But first, we just\nmask out tokens the exact same way we\ndo in language, just",
    "start": "1728120",
    "end": "1734870"
  },
  {
    "text": "a different percent. And then we compute the\nrepresentations, in this case, of only the unmasked patches.",
    "start": "1734870",
    "end": "1740289"
  },
  {
    "text": "So we flatten out that\nsequence of all the patches that didn't get masked out.",
    "start": "1740290",
    "end": "1746293"
  },
  {
    "text": "We stick them\nthrough our encoder. We get a representation\nof all of those. And then we insert placeholder\npatches into all the locations",
    "start": "1746293",
    "end": "1753140"
  },
  {
    "text": "that were masked out. So now we have sort of the same\nshape as our original image. And in the places\nthat weren't masked,",
    "start": "1753140",
    "end": "1759380"
  },
  {
    "text": "we have our model's\nrepresentation. And in the places\nthat were masked, we just have placeholder tokens. And then we run this\nthrough a decoder",
    "start": "1759380",
    "end": "1765710"
  },
  {
    "text": "to get a prediction of what the\nrest of the image look like. And so at fine-tuning time, we\njust throw away the decoder.",
    "start": "1765710",
    "end": "1771630"
  },
  {
    "text": "We just use the\noutput of the encoder but with the whole image. And this is really, really\na powerful learning setup",
    "start": "1771630",
    "end": "1777710"
  },
  {
    "text": "as well. And actually, this sort of\nmasked autoencoder pre-training can give state-of-the-art\nfew-shot image classification",
    "start": "1777710",
    "end": "1784610"
  },
  {
    "text": "performance and actually can\ndo better than supervised pre-training. So if you do this\nprocedure on ImageNet",
    "start": "1784610",
    "end": "1791090"
  },
  {
    "text": "and then fine-tune on\nImageNet with labels, you can actually do better than\nif you just trained on ImageNet",
    "start": "1791090",
    "end": "1798830"
  },
  {
    "text": "with labels from the get-go. So in a sense, you're\nsort of extracting even more information from\nyour training data set",
    "start": "1798830",
    "end": "1804230"
  },
  {
    "text": "by doing this procedure and\nthen fine-tuning with labels than if you just train with\nall the labels to start.",
    "start": "1804230",
    "end": "1809480"
  },
  {
    "text": "So that's pretty cool. But in addition to that,\none thing, I think, that's interesting is to compare\nwith contrastive learning.",
    "start": "1809480",
    "end": "1816520"
  },
  {
    "text": "So this is a really, I think,\ninformative figure to get a sense of maybe\nsome of the strengths",
    "start": "1816520",
    "end": "1822750"
  },
  {
    "text": "and weaknesses of\nmasked autoencoders versus contrastive learning. And the interesting regime,\nI think, is right here.",
    "start": "1822750",
    "end": "1828460"
  },
  {
    "text": "So here we have MAE, which\nis this masked autoencoder model for images. And then MoCo is this momentum\ncontrastive learning method,",
    "start": "1828460",
    "end": "1835860"
  },
  {
    "text": "sort of V3 but whatever. And what's really\ninteresting is that if we",
    "start": "1835860",
    "end": "1840900"
  },
  {
    "text": "don't do any\nfine-tuning and we just fit-- we freeze the\npre-trained model and we just fit a linear head\non top of the frozen model's",
    "start": "1840900",
    "end": "1848550"
  },
  {
    "text": "representations,\ncontrastive learning works significantly better\nthan the masked autoencoder. So it's a difference of\na little more than 4%.",
    "start": "1848550",
    "end": "1854580"
  },
  {
    "text": "But if you actually fine-tune\nsome of the model itself, it seems that the masked\nautoencoder pre-trained models",
    "start": "1854580",
    "end": "1861210"
  },
  {
    "text": "fine-tuned better than the\ncontrastive pre-trained models. And so you have-- it's\nalmost like a little bit of a trade-off of\nrepresentation quality",
    "start": "1861210",
    "end": "1868500"
  },
  {
    "text": "of the frozen pre-trained model\nversus like fine-tunability. So it's not clear that\nit's actually a trade-off,",
    "start": "1868500",
    "end": "1874440"
  },
  {
    "text": "but it seems like for these\ntwo classes of methods at least right now, the\ncontrastive models give you slightly better representations\nbut the masked models maybe",
    "start": "1874440",
    "end": "1882720"
  },
  {
    "text": "fine-tune a little better. So if you have only a tiny\nbit of fine-tuning data, maybe the contrastive\nmodels are better",
    "start": "1882720",
    "end": "1888570"
  },
  {
    "text": "because you don't have\nenough data to fine-tune a lot of the model. But if you have a decent amount\nof fine-tuning data, which",
    "start": "1888570",
    "end": "1893790"
  },
  {
    "text": "often we have more than like\n30 examples and the hundreds of examples, the masked\nmethods can work really well.",
    "start": "1893790",
    "end": "1901659"
  },
  {
    "text": "So now we've talked a\nlot about transformers. And I should say that masked\nautoencoders work really well,",
    "start": "1901660",
    "end": "1909540"
  },
  {
    "text": "and they have gotten\na lot of attention. But one of the\nthings, I think, that has been the engine of\na lot of that progress",
    "start": "1909540",
    "end": "1915480"
  },
  {
    "text": "is the transformer architecture\nbecause to do this sort of mask completion task\nrequires a high capacity",
    "start": "1915480",
    "end": "1921570"
  },
  {
    "text": "model that you can scale. And in order to apply this\nrecipe across data modalities, it's nice if you have\nsome architecture that's",
    "start": "1921570",
    "end": "1927900"
  },
  {
    "text": "not really modality-specific. And transformers give you that. So transformers are this sort\nof general purpose architecture",
    "start": "1927900",
    "end": "1933262"
  },
  {
    "text": "that you can pretty\nmuch just directly point at language or\nimages or molecules",
    "start": "1933262",
    "end": "1938550"
  },
  {
    "text": "or like sequences of states\nand actions and reinforcement learning, and you\ndon't really need to change the architecture.",
    "start": "1938550",
    "end": "1943935"
  },
  {
    "text": "And so it makes it really\neasy to kind of transport this masked representation\nlearning approach",
    "start": "1943935",
    "end": "1949530"
  },
  {
    "text": "to new modalities. So I think it's important that\nwe spend a little time just looking at transformers. Some of you might be\nfamiliar with it already,",
    "start": "1949530",
    "end": "1955677"
  },
  {
    "text": "but hopefully, you'll\nlearn something. So OK, what is a transformer? So we got a little bit of a\npeek in the previous slides.",
    "start": "1955677",
    "end": "1964170"
  },
  {
    "text": "Yeah. Can you go back? Is there any intuition\nwhy constrastive learning",
    "start": "1964170",
    "end": "1969420"
  },
  {
    "text": "doesn't perform as well as most\nautoencoders for fine-tuning?",
    "start": "1969420",
    "end": "1975090"
  },
  {
    "text": "Let me think about that. No.",
    "start": "1975090",
    "end": "1980169"
  },
  {
    "text": "I don't have a concise\nanswer for why I would expect this to be the case. ",
    "start": "1980170",
    "end": "1988785"
  },
  {
    "text": "I guess one way-- and so that-- I\nmean, my short answer is I don't have an answer\nI'm very confident in.",
    "start": "1988785",
    "end": "1994110"
  },
  {
    "text": "If I had to totally\nhazard a guess, I guess you could say that\nthe pre-training objective",
    "start": "1994110",
    "end": "2001100"
  },
  {
    "text": "for contrastive learning\nis a little bit more like just doing\nclassification, like we saw in that early slide, right?",
    "start": "2001100",
    "end": "2006980"
  },
  {
    "text": "Like, we're basically coming\nup with a classification score for the positive pair\ncompared to other pairs.",
    "start": "2006980",
    "end": "2014120"
  },
  {
    "text": "And so it's almost\nlike your pre-training like in a pseudosupervised way.",
    "start": "2014120",
    "end": "2019575"
  },
  {
    "text": "So it kind of makes sense that\nif your downstream task is to do image\nclassification, maybe pre-training this\nway is going to work",
    "start": "2019575",
    "end": "2025430"
  },
  {
    "text": "pretty well without\nchanging the model at all. Whereas like the masked\npre-training approaches is a little more different\nthan classification.",
    "start": "2025430",
    "end": "2032540"
  },
  {
    "text": "But maybe you're extracting\nmore signal from the data. And so if you're able\nto fine-tune the model, it'll work better.",
    "start": "2032540",
    "end": "2038419"
  },
  {
    "text": "But because the\npre-training objective is a little different\nthan your downstream task, it's not necessarily\ngoing to work as well",
    "start": "2038420",
    "end": "2043460"
  },
  {
    "text": "with no fine-tuning at all. That's the best I can give you. [INAUDIBLE]",
    "start": "2043460",
    "end": "2049708"
  },
  {
    "text": "Oh, gosh. Sorry. Yeah, the question was, do\nwe have any intuition for why",
    "start": "2049708",
    "end": "2055270"
  },
  {
    "text": "contrastive methods might\ngive us better representations but not fine-tune as well? And the TLDR was that maybe\nthe contrastive objective",
    "start": "2055270",
    "end": "2062289"
  },
  {
    "text": "is a little closer to\ndoing classification based on the derivation\nwe saw before.",
    "start": "2062290",
    "end": "2067532"
  },
  {
    "text": "And so maybe it\nkind of make sense that without any\nfine-tuning, they do better at classification. ",
    "start": "2067532",
    "end": "2076030"
  },
  {
    "text": "OK, so transformers. This is a nice figure from the\nvision transformer paper that I",
    "start": "2076030",
    "end": "2082329"
  },
  {
    "text": "thought is a good at least\n20,000-foot view of the inputs and outputs of a transformer.",
    "start": "2082330",
    "end": "2088284"
  },
  {
    "text": "So we haven't cracked into\nthe main meat of it yet. But to see sort of what's going\nin and coming out of the model,",
    "start": "2088285",
    "end": "2096399"
  },
  {
    "text": "transformers operate on\nsequences of data and sequence in the very general sense. So this can be a\nsequence of words.",
    "start": "2096400",
    "end": "2102602"
  },
  {
    "text": "It can be a sequence\nof here image patches. It could be a sequence\nof bonds in a molecule.",
    "start": "2102602",
    "end": "2107770"
  },
  {
    "text": "It can be kind of\nwhatever you want. And so the way a\ntransformer works is we somehow\nconvert our data into",
    "start": "2107770",
    "end": "2114309"
  },
  {
    "text": "a one-dimensional sequence. We convert these into a\nsequence of embeddings. So in the case of\nan image patch,",
    "start": "2114310",
    "end": "2120820"
  },
  {
    "text": "we just have a\nlinear transformation that takes like our whatever\n16 by 16 grid of pixels",
    "start": "2120820",
    "end": "2126700"
  },
  {
    "text": "and projects it\ninto some usually like something like 768 or 1,024\ndimensional embedding space.",
    "start": "2126700",
    "end": "2133880"
  },
  {
    "text": "So now we have this sequence\nof image patch embeddings. And then before we stick this\ninto our actual transformer",
    "start": "2133880",
    "end": "2140720"
  },
  {
    "text": "encoder, we do one extra thing\nwhich is really important, sort of to. First, we concatenate\nwhat we call",
    "start": "2140720",
    "end": "2146300"
  },
  {
    "text": "a CLS token in the\nbeginning of the sequence. And this is just because at\nthe end, when we actually want to make a\nclassification score,",
    "start": "2146300",
    "end": "2152450"
  },
  {
    "text": "we need to-- we need to pick\none of these representations to actually stick in\nour final predictor MLP.",
    "start": "2152450",
    "end": "2157892"
  },
  {
    "text": "And so we just have\na special token in the beginning for that. But the really\nimportant thing is we have this thing that we\ncall the position embedding.",
    "start": "2157892",
    "end": "2164012"
  },
  {
    "text": "So we get our embedding\nfor each image patch here, but we concatenate this number,\nwhich really, in fact, isn't",
    "start": "2164012",
    "end": "2170048"
  },
  {
    "text": "literally a number. But it's an embedding\nthat is specific to each literal position\nin the sequence.",
    "start": "2170048",
    "end": "2175833"
  },
  {
    "text": "And this actually turns out\nto be really, really important because the transformer\narchitecture itself doesn't discriminate\nbetween locations",
    "start": "2175833",
    "end": "2182390"
  },
  {
    "text": "in the sequence. So it's just giving\nyou an output that's a function of the set\nof things you put into it.",
    "start": "2182390",
    "end": "2187980"
  },
  {
    "text": "And so if you\ndon't add something to tell the model that this is\nthe embedding at position one",
    "start": "2187980",
    "end": "2193450"
  },
  {
    "text": "and this is the embedding\nat position two, you'll end up with\na model that's sort of totally permutation\ninvariant of your input.",
    "start": "2193450",
    "end": "2199322"
  },
  {
    "text": "So if you totally scrambled\nthe patches in the image, you'd get the same output. And that is not-- that's not good.",
    "start": "2199322",
    "end": "2204990"
  },
  {
    "text": "We want our model\nto be able to reason about the relative\nposition of things. ",
    "start": "2204990",
    "end": "2210100"
  },
  {
    "text": "OK, so what's inside\nthis transformer encoder? The transformer encoder\nhas a few main pieces that we'll dig into in more\ndetail in the next slide.",
    "start": "2210100",
    "end": "2216640"
  },
  {
    "text": "But at a high\nlevel, we basically have this fundamental block\nthat we just apply over and over and over.",
    "start": "2216640",
    "end": "2222370"
  },
  {
    "text": "So we'll have\nsomething like 5 or 10 or up to like 100 of these\nblocks that we apply in a row.",
    "start": "2222370",
    "end": "2228243"
  },
  {
    "text": "And then we just take the\noutput of the final block. And inside each one of\nthose, what do we do?",
    "start": "2228243",
    "end": "2233260"
  },
  {
    "text": "Well, first, we do-- we have\na normalization step where we just take every\nembedding in our sequence, and we normalize it separately.",
    "start": "2233260",
    "end": "2238910"
  },
  {
    "text": "So every embedding should have\n0 mean and standard deviation 1. Then we do what we call\nMulti-Head Attention, which",
    "start": "2238910",
    "end": "2245200"
  },
  {
    "text": "is basically the only\nstep in this whole process where the embeddings at time\nstep one and another time step",
    "start": "2245200",
    "end": "2251580"
  },
  {
    "text": "will interact with each other. So this is where all of the\ninter time step interaction occurs. And we'll go into the\nmath of what exactly",
    "start": "2251580",
    "end": "2258400"
  },
  {
    "text": "is happening later. We have a residual\nconnection that wraps around both of these.",
    "start": "2258400",
    "end": "2263630"
  },
  {
    "text": "And then we have another\nnormalization step. And then we apply an MLP just\nseparately to every time step.",
    "start": "2263630",
    "end": "2270430"
  },
  {
    "text": "So again, there's\nno communication between the embeddings\nat different time steps. It's just a sort\nof weight-tied MLP.",
    "start": "2270430",
    "end": "2276460"
  },
  {
    "text": "And then we have another\nresidual connection that skips both of these. And then we have 12\nor 24 of these blocks",
    "start": "2276460",
    "end": "2283569"
  },
  {
    "text": "that we train end to end. So that's sort of the high-level\npicture of the mechanism",
    "start": "2283570",
    "end": "2290309"
  },
  {
    "text": "in the transformer. And again, this\nMulti-Head Attention is the only time where\nreasoning across time steps.",
    "start": "2290310",
    "end": "2295530"
  },
  {
    "text": "And it does so in a totally\nsort of order independent way. ",
    "start": "2295530",
    "end": "2301278"
  },
  {
    "text": "And another thing\nthat's worth noting is that really the only\ndifference between transformers for different modalities again\nis this initial embedding step.",
    "start": "2301278",
    "end": "2308019"
  },
  {
    "text": "So if you want to\napply a transformer to your new type of\ndata, whether it's",
    "start": "2308020",
    "end": "2313259"
  },
  {
    "text": "language or code or molecules\nor images or 3D point clouds or whatever, all you\nneed to do is figure out",
    "start": "2313260",
    "end": "2318930"
  },
  {
    "text": "how to take my original\ndata, which is the raw data, and turn it into some sequence.",
    "start": "2318930",
    "end": "2324720"
  },
  {
    "text": "And then you stuffed\nin the transformer. And usually, you'll get\nsomething reasonable. There are definitely better\nand worse ways to do that,",
    "start": "2324720",
    "end": "2331119"
  },
  {
    "text": "so I don't want to make it\nseem like a trivial point. But it is pretty much the only\ndecision you need to make. ",
    "start": "2331120",
    "end": "2338770"
  },
  {
    "text": "OK, so let's look at these\nin a little bit more detail. And this is one of the\ndenser parts of the lecture.",
    "start": "2338770",
    "end": "2345150"
  },
  {
    "text": "So feel free to ask questions\nif I've not explained anything adequately.",
    "start": "2345150",
    "end": "2351950"
  },
  {
    "text": "But here let's look\nat a language example just to not get too focused\non one modality or another.",
    "start": "2351950",
    "end": "2358050"
  },
  {
    "text": "So we get some input sequence. In this case, it's\na sequence of words. And we have T elements\nin this input sequence,",
    "start": "2358050",
    "end": "2364400"
  },
  {
    "text": "where here T is 6. And the first step that\nwe're going to do is-- for language, we're going\nto do a tokenization step.",
    "start": "2364400",
    "end": "2370890"
  },
  {
    "text": "And this is needed to basically\nconvert our words into indices",
    "start": "2370890",
    "end": "2375932"
  },
  {
    "text": "because that's how\nwe're ultimately going to convert them into vectors. So we have literally a big\nmapping that we store--",
    "start": "2375932",
    "end": "2383160"
  },
  {
    "text": "that we fit using a\nlarge corpus of text. And every word or really in\npractice, like subword sequence",
    "start": "2383160",
    "end": "2389100"
  },
  {
    "text": "of letters just gets\nassigned some number. That's sort of the\nfirst step here.",
    "start": "2389100",
    "end": "2394470"
  },
  {
    "text": "And then the second step is\nto do this embedding lookup where every token index\nhas its own learned vector.",
    "start": "2394470",
    "end": "2402770"
  },
  {
    "text": "And this is how we're going\nto get our initial input sequence to our transformer. So every word gets\na number, and then",
    "start": "2402770",
    "end": "2408170"
  },
  {
    "text": "we just look at the row in\nour big embedding matrix corresponding to each\nof these token indices.",
    "start": "2408170",
    "end": "2414350"
  },
  {
    "text": "And that's sort of\nour initial embedding. The only asterisk there is we\nadd a positional embedding.",
    "start": "2414350",
    "end": "2419690"
  },
  {
    "text": "Like we said before, to\nbe able to know that, the word Joe came\nfirst in the sequence,",
    "start": "2419690",
    "end": "2424730"
  },
  {
    "text": "not tenth in the sequence. So this is sort of that first\nstep outside of the gray box.",
    "start": "2424730",
    "end": "2430480"
  },
  {
    "text": "On this last slide, this\nis sort of preparing our initial embeddings to then\nput into our many transformer",
    "start": "2430480",
    "end": "2436060"
  },
  {
    "text": "blocks. And this is how we do this\nin the language setting. All right. So then let's get\nto the gray box.",
    "start": "2436060",
    "end": "2442360"
  },
  {
    "text": "That's what we're all here for. So again, this is just\none transformer block. And in practice, we're going to\nhave many of these in sequence",
    "start": "2442360",
    "end": "2449290"
  },
  {
    "text": "all with different\nparameters that we learn. And the first\nthing that we do is we're going to normalize all of\nthese vectors in our sequence",
    "start": "2449290",
    "end": "2457570"
  },
  {
    "text": "independently. So each of these\nvectors is going to have a mean 0 and a\nunit standard deviation.",
    "start": "2457570",
    "end": "2464860"
  },
  {
    "text": "And so again, we have\nthis chunk now of T by d, where our\nsequence length is T,",
    "start": "2464860",
    "end": "2470319"
  },
  {
    "text": "and d is the actual embedding\ndimension or the hidden state dimension of our model.",
    "start": "2470320",
    "end": "2476980"
  },
  {
    "text": "Then we have this\nself-attention step, which is sort of where the\nmagic happens, so to speak,",
    "start": "2476980",
    "end": "2482750"
  },
  {
    "text": "where we will compute this\nself-attention matrix, which essentially tells us how\nrelated or how similar",
    "start": "2482750",
    "end": "2488410"
  },
  {
    "text": "the token at one location is\nto a token at another location. And this is kind\nof what tells us like how we should be\npropagating information",
    "start": "2488410",
    "end": "2494725"
  },
  {
    "text": "between time steps. And this is sort of\nhypothesized to be what makes transformers\nreally powerful is that it can do\nthis explicit sort",
    "start": "2494725",
    "end": "2501280"
  },
  {
    "text": "of point-to-point reasoning\ninstead of for an RNN, where if you have information here\nthat's relevant to something",
    "start": "2501280",
    "end": "2507460"
  },
  {
    "text": "much later in the sequence, you\nhave to kind of recognize that here and then remember\nit as you process all of the tokens in between.",
    "start": "2507460",
    "end": "2513640"
  },
  {
    "text": "The difference is\nwith the transformer, I'm directly comparing each\ntoken to every other token. So I don't have to do that.",
    "start": "2513640",
    "end": "2520470"
  },
  {
    "text": "OK, so how does that work? Well, we compute a new\nset of representations, A, which is the same shape as\nthe input representations,",
    "start": "2520470",
    "end": "2528150"
  },
  {
    "text": "but it has this particular form,\nwhich is maybe a little bit like, what the hell, at first.",
    "start": "2528150",
    "end": "2533490"
  },
  {
    "text": "But we'll go through\nit one step at a time. So there are really\ntwo main pieces.",
    "start": "2533490",
    "end": "2539205"
  },
  {
    "text": "So this A again is the output\nof the self-attention step. And the two main\npieces are what we",
    "start": "2539205",
    "end": "2545100"
  },
  {
    "text": "call the self-attention\nmatrix and the value matrix. So the self-attention\nmatrix is this thing",
    "start": "2545100",
    "end": "2550125"
  },
  {
    "text": "that I've drawn right here. This is the T by T\nmatrix that tells us how similar is the\nthing at one location",
    "start": "2550125",
    "end": "2556440"
  },
  {
    "text": "to a thing at another location. And the value\nmatrix is basically like if I say that my\nrepresentation at this time",
    "start": "2556440",
    "end": "2562800"
  },
  {
    "text": "step really only depends on the\nthing at this other time step, the value matrix tells you what\nis the actual representation",
    "start": "2562800",
    "end": "2570563"
  },
  {
    "text": "that you're going to be\npulling from that other time step that's going to replace\nyour current representation.",
    "start": "2570563",
    "end": "2576150"
  },
  {
    "text": "To be more specific about this,\nso Xq is just your input here, x times some d by d or\neven a down projection",
    "start": "2576150",
    "end": "2584515"
  },
  {
    "text": "to a lower dimension. But some transformation matrix,\nwhich is a linear transform. And then the same thing\nwith Xk, it's just another",
    "start": "2584515",
    "end": "2590100"
  },
  {
    "text": "learned linear transform. And again, with Xv, it's just\nanother linear transform. So you just have three\ndifferent projections",
    "start": "2590100",
    "end": "2595470"
  },
  {
    "text": "of your input vectors. And to get your\nattention matrix,",
    "start": "2595470",
    "end": "2601530"
  },
  {
    "text": "you take the inner product\nof all pairs of what we can call the queries and the keys.",
    "start": "2601530",
    "end": "2607320"
  },
  {
    "text": "And this gives us some matrix. And then we do a softmax\nto sort of normalize each row to sum to 1.",
    "start": "2607320",
    "end": "2612510"
  },
  {
    "text": "So every time step\nbasically gets assigned a distribution over\nall time steps in the sequence.",
    "start": "2612510",
    "end": "2617763"
  },
  {
    "text": "And then when we do this\nmatrix multiplication with the value matrix,\nwhat we're doing is we're doing a weighted\nsum over the value",
    "start": "2617763",
    "end": "2623849"
  },
  {
    "text": "vector for every time step\nin the sequence weighted by the softmax attention score\nfor each of those time steps.",
    "start": "2623850",
    "end": "2631390"
  },
  {
    "text": "And-- [INAUDIBLE] Yeah. So when we do this-- when we do\nthe final matrix multiplication",
    "start": "2631390",
    "end": "2636480"
  },
  {
    "text": "to get A-- so the question was, can\nyou say that sentence again? The answer is no, but I'll\ntry to be similar because I",
    "start": "2636480",
    "end": "2643845"
  },
  {
    "text": "don't remember what I said. Essentially, what A is doing\nis for every time step-- so the value of a1\nis roughly just going",
    "start": "2643845",
    "end": "2651359"
  },
  {
    "text": "to be a weighted\nsum over each Xv so that the projected version\nof each one of these vectors",
    "start": "2651360",
    "end": "2659250"
  },
  {
    "text": "weighted by the attention score\nfor each one of these time steps. So this self-attention\nmatrix is this T by T thing,",
    "start": "2659250",
    "end": "2666599"
  },
  {
    "text": "meaning for each time step\nin our length-T sequence, we have a length-T vector. And that vector is a\nprobability distribution",
    "start": "2666600",
    "end": "2672900"
  },
  {
    "text": "over all of the time steps. And we just do a weighted\nsum over their value vectors.",
    "start": "2672900",
    "end": "2680600"
  },
  {
    "text": "And so just as an\nexample, if this is our self-attention\nmatrix, literally, the number right here in the\nmatrix is going",
    "start": "2680600",
    "end": "2686900"
  },
  {
    "text": "to be like x of\nthe inner product of the third row\nof the query matrix",
    "start": "2686900",
    "end": "2692210"
  },
  {
    "text": "and the second row\nof the key matrix over the sum of the x of the\nthird row of the query matrix",
    "start": "2692210",
    "end": "2697730"
  },
  {
    "text": "and all of our keys. This is just the softmax,\nbut I've just written it out",
    "start": "2697730",
    "end": "2703680"
  },
  {
    "text": "for a particular cell.  Yes.",
    "start": "2703680",
    "end": "2709430"
  },
  {
    "text": "[INAUDIBLE] ",
    "start": "2709430",
    "end": "2716309"
  },
  {
    "text": "And the MLPs. [INAUDIBLE] Yeah. So after self-attention, we\ndo another residual connection",
    "start": "2716310",
    "end": "2723160"
  },
  {
    "text": "and normalization. And then we have an\nMLP that we apply, it's the same MLP\nfor every time step.",
    "start": "2723160",
    "end": "2728829"
  },
  {
    "text": "The weights are shared. But it's just sort of like a\npost-processing on the output of self-attention. But yeah.",
    "start": "2728830",
    "end": "2734289"
  },
  {
    "text": "So now it's all the parameters. So it's the XQ, the\nWQ, the WK, the WV.",
    "start": "2734290",
    "end": "2739660"
  },
  {
    "text": "And then this is a\none hidden layer MLP, so we have two\nweight matrices here. And that's all the parameters.",
    "start": "2739660",
    "end": "2744790"
  },
  {
    "text": "Other than like your input\nembedding matrix, which is relatively few parameters. Yes.",
    "start": "2744790",
    "end": "2750160"
  },
  {
    "text": "Is there a benefit of\nlearning WQ and WK separately since we just want to\nfind them together?",
    "start": "2750160",
    "end": "2755829"
  },
  {
    "text": "Yeah, it's not obvious, but yes. Mathematically, it's equivalent. So actually, in an early\nversion of this slide,",
    "start": "2755830",
    "end": "2762010"
  },
  {
    "text": "I wrote it out with just\nthe collapsed single matrix because when you do\nthe transpose of XK, we get X times WQ times WK\ntranspose times X transpose.",
    "start": "2762010",
    "end": "2771010"
  },
  {
    "text": "So you can just define a new\nmatrix that's WQ, WK transpose, and it's like, it's\nthe same, right? But it's not the same. Because when you\ntrain it, it turns out",
    "start": "2771010",
    "end": "2777135"
  },
  {
    "text": "it's easier to train if you\nhave both of the matrices because when you do one\nstep of gradient descent",
    "start": "2777135",
    "end": "2782770"
  },
  {
    "text": "like you're going to update-- whoa, we're getting\nin the weeds. But like each matrix is going\nto get like a rank-one update.",
    "start": "2782770",
    "end": "2787970"
  },
  {
    "text": "And if you have a\nproduct of two matrices, they both get a rank-one update. So the update you\nmake to each of them",
    "start": "2787970",
    "end": "2793510"
  },
  {
    "text": "to the thing in\naggregate is richer. So in some sense, it's\neasier to optimize. It's not totally\nwell-understood, but yeah, people still do it\nwith this double matrix way",
    "start": "2793510",
    "end": "2801970"
  },
  {
    "text": "because it just\nseems to work better. It's a good question, though. Another question? [INAUDIBLE]",
    "start": "2801970",
    "end": "2808540"
  },
  {
    "text": " I don't know the answer to that. ",
    "start": "2808540",
    "end": "2814952"
  },
  {
    "text": "I think he was first,\nbut I'll get to you next. What do you mean\nby rank-one update? Now we're officially in the\nweeds, but I'm happy to talk.",
    "start": "2814952",
    "end": "2823450"
  },
  {
    "text": "Offline, then.  When we say, we got\nMulti-Head Attention,",
    "start": "2823450",
    "end": "2831674"
  },
  {
    "text": "where does that\nshow up in this one? Yeah. Good. So someone's paying\nattention at least.",
    "start": "2831674",
    "end": "2836869"
  },
  {
    "text": "So this is not\nMulti-Head Attention. This is Single-Headed Attention. And so really, what we'll\ndo with Multi-Head Attention",
    "start": "2836870",
    "end": "2842240"
  },
  {
    "text": "is we won't do this once. We'll actually repeat this. Basically, these WQ,\nWK and WV matrices",
    "start": "2842240",
    "end": "2850250"
  },
  {
    "text": "will all have an extra index,\nwhich will be the heads. And so you'll repeat this\neight times in parallel.",
    "start": "2850250",
    "end": "2856579"
  },
  {
    "text": "And each one of them will take\nlike a-- will project usually like a different\nsubspace of x because--",
    "start": "2856580",
    "end": "2862250"
  },
  {
    "text": "Maybe [INAUDIBLE] Right. So if you have-- yeah, basically, one of\nthe words in the sentence",
    "start": "2862250",
    "end": "2869290"
  },
  {
    "text": "might be related to like-- it might have multiple\nsenses, for example. And so if you have\nthe word bank,",
    "start": "2869290",
    "end": "2875470"
  },
  {
    "text": "maybe it's related to river\nbecause it's like a riverbank. But maybe it's also\nrelated to Federal Reserve,",
    "start": "2875470",
    "end": "2882340"
  },
  {
    "text": "and you haven't disambiguated\nwhat kind of bank it is yet. So you want to tend to both, but\nyou want to do them separately. So you do them in the\ntwo different heads,",
    "start": "2882340",
    "end": "2888630"
  },
  {
    "text": "and then later on,\nyou combine them. [INAUDIBLE] that's sort\nof a detail of like real transformers that--",
    "start": "2888630",
    "end": "2893960"
  },
  {
    "text": "we'll omit it for now. OK. And so-- oh, yeah, yeah.",
    "start": "2893960",
    "end": "2899470"
  },
  {
    "text": "So is this normalization before\nor after residual connection? Usually after. Sorry--",
    "start": "2899470",
    "end": "2904970"
  },
  {
    "text": "[INAUDIBLE] That's a great question. It's pretty much purely\nempirically decided as far",
    "start": "2904970",
    "end": "2913600"
  },
  {
    "text": "as I know. There are some papers\nthat try to analyze",
    "start": "2913600",
    "end": "2918670"
  },
  {
    "text": "some of these decisions and\ngive rationales behind them. ",
    "start": "2918670",
    "end": "2924340"
  },
  {
    "text": "There's a paper, I think,\ncalled like transformer circuits or \"A Mathematical Framework\nfor Transformer Architectures\" or something like\nthis from Anthropic",
    "start": "2924340",
    "end": "2931240"
  },
  {
    "text": "that gets into this\nin more detail. But my understanding is largely\nbecause it's what works best.",
    "start": "2931240",
    "end": "2938359"
  },
  {
    "text": "But I'm not sure actually the\nlocation of the normalization makes a huge difference\nin the final performance.",
    "start": "2938360",
    "end": "2943540"
  },
  {
    "text": "It's kind of arbitrary. OK. Yes.",
    "start": "2943540",
    "end": "2949098"
  },
  {
    "text": "[INAUDIBLE] but how exactly\ndo positional embeddings work?",
    "start": "2949098",
    "end": "2954390"
  },
  {
    "text": "How do you make them so that\nthe transformers [INAUDIBLE]?? Mm-hmm.",
    "start": "2954390",
    "end": "2959430"
  },
  {
    "text": "So you're embedding lookup\ngives you a T by d sequence.",
    "start": "2959430",
    "end": "2965082"
  },
  {
    "text": "So you have a\nsequence of length T, and you do your embedding\nlookup and you get a d length vector for each time step.",
    "start": "2965082",
    "end": "2970755"
  },
  {
    "text": "Your positional embeddings\ncan literally just be like a stack of\nlearned vectors.",
    "start": "2970755",
    "end": "2976200"
  },
  {
    "text": "And you have one for the\nthing at time step zero and one for the thing at time\nstep one and so on and so",
    "start": "2976200",
    "end": "2981720"
  },
  {
    "text": "forth. And you actually just\nliterally add them together. And that's what forms\nyour input sequence. And you just learn\nthem end to end.",
    "start": "2981720",
    "end": "2988698"
  },
  {
    "text": "There are other ways\nof doing it that aren't learned that\nalso work, and there are more complicated\nones that try",
    "start": "2988698",
    "end": "2994505"
  },
  {
    "text": "to make it easier to\ngeneralize to longer sequences than you saw during training. But the basic idea is\nit's just like a vector",
    "start": "2994505",
    "end": "2999750"
  },
  {
    "text": "you add to the raw inputs.  All right.",
    "start": "2999750",
    "end": "3005990"
  },
  {
    "text": "I'm going to move on. So the last question was\nyour question which was--",
    "start": "3005990",
    "end": "3015547"
  },
  {
    "text": "remind me of your\nquestion once again. How do you make the\npositional embeddings? Right, right. Yeah. So the question was about\nthe positional embeddings",
    "start": "3015548",
    "end": "3023119"
  },
  {
    "text": "and where do they\nactually come from. And basically,\nit's just you have a learned vector for every\ntime step in your sequence.",
    "start": "3023120",
    "end": "3030410"
  },
  {
    "text": "And for each\ncorresponding time step, you just look up the\none at that index. There are more complicated\nways of doing it but yeah.",
    "start": "3030410",
    "end": "3038144"
  },
  {
    "text": "OK, so now we have\nthe gray box, which is great, because we love it. And we're pretty\nmuch done now, so we",
    "start": "3038144",
    "end": "3045180"
  },
  {
    "text": "have lots of these blocks. And we train them\nall end to end. And we get some\nrepresentation for every time step in our sequence.",
    "start": "3045180",
    "end": "3050670"
  },
  {
    "text": "And then finally,\nif, for example, we're doing like\nmasked pre-training, we just have a final\nlinear transform",
    "start": "3050670",
    "end": "3056250"
  },
  {
    "text": "that projects the representation\nat every time step into the dimensionality\nof our vocabulary.",
    "start": "3056250",
    "end": "3061380"
  },
  {
    "text": "And then we do a softmax. This gives us a distribution\nover all the words in our vocabulary. And now we can just do like\nmaximum likelihood training",
    "start": "3061380",
    "end": "3068310"
  },
  {
    "text": "or whatever. And this is pretty\nmuch the whole thing.",
    "start": "3068310",
    "end": "3074100"
  },
  {
    "text": " Cool.",
    "start": "3074100",
    "end": "3080560"
  },
  {
    "text": "All right, so we've\ntalked about pre-training. We've talked about transformers. How do we actually\nfine-tune these guys",
    "start": "3080560",
    "end": "3086380"
  },
  {
    "text": "once we have them pre-trained? So let's look at\none more example. So here I've prepended\nthis CLS token",
    "start": "3086380",
    "end": "3093940"
  },
  {
    "text": "that we talked about before\nbecause it's not really needed to understand pre-training. But for fine-tuning,\nwe're going to use it.",
    "start": "3093940",
    "end": "3099410"
  },
  {
    "text": "So we're going to just push\nthis sequence through our model.",
    "start": "3099410",
    "end": "3104680"
  },
  {
    "text": "And when we do fine-tuning,\noftentimes, we'll just take the representation\nof the CLS token, and we'll fine-tune a new\nprediction head, which is often",
    "start": "3104680",
    "end": "3111915"
  },
  {
    "text": "just a linear transform or a\nsmall MLP on top of this CLS token representation.",
    "start": "3111915",
    "end": "3118627"
  },
  {
    "text": "I guess that's pretty\nstraightforward. That's pretty much\nwhat we said before. We were talking about\nregular autoencoders. But one of the-- the\nbig questions hopefully",
    "start": "3118627",
    "end": "3125540"
  },
  {
    "text": "on your mind is, what\ndo we do with this guy during fine-tuning?",
    "start": "3125540",
    "end": "3131220"
  },
  {
    "text": "So I mean, we could\nfreeze these parameters. We could fine-tune all of them.",
    "start": "3131220",
    "end": "3136700"
  },
  {
    "text": " There are other options. Maybe we fine-tune some of\nthem, or we could freeze them",
    "start": "3136700",
    "end": "3144288"
  },
  {
    "text": "but inject some new parameters\nthat we're going to fine-tune. And it turns out in\npractice people usually do sort of option three here.",
    "start": "3144288",
    "end": "3150410"
  },
  {
    "text": "So in some sense, they kind\nof freeze and fine-tune them in lots and lots\nof different ways.",
    "start": "3150410",
    "end": "3156260"
  },
  {
    "text": "And we're going to look at one\nof these methods in a little more detail so that you have\nsome intuition about what",
    "start": "3156260",
    "end": "3162040"
  },
  {
    "text": "that actually means. And the method that we're going\nto look at-- which again, there are many.",
    "start": "3162040",
    "end": "3167390"
  },
  {
    "text": "And I don't mean to pick\nthis one as like the end-all, be-all, but I think\nit's reasonably concise to get your arms around.",
    "start": "3167390",
    "end": "3173880"
  },
  {
    "text": "So I think it can be\nuseful to work through. It's called Low-Rank\nAdaptation of Language Models.",
    "start": "3173880",
    "end": "3180150"
  },
  {
    "text": "And again, the\nintuition is like we want to fine-tune our model\na little bit in some sense.",
    "start": "3180150",
    "end": "3185520"
  },
  {
    "text": "So we don't want to\nnecessarily fine-tune all the parameters of\nour model because we don't want to destroy all of\nthe knowledge in the model.",
    "start": "3185520",
    "end": "3191953"
  },
  {
    "text": " Of course, there's a side\nof the question of like,",
    "start": "3191953",
    "end": "3197810"
  },
  {
    "text": "what is a little bit even mean? So I'm curious if people\nhave just like a sense of,",
    "start": "3197810",
    "end": "3203300"
  },
  {
    "text": "if we want to fine-tune\nour really big model, we want to fine-tune it\nkind of a little bit, just enough to learn the\ntask but not too much.",
    "start": "3203300",
    "end": "3209480"
  },
  {
    "text": "Like, what would that mean\nto you in terms of maybe what a method might\nlook like or what",
    "start": "3209480",
    "end": "3215720"
  },
  {
    "text": "we might want to avoid what\nfailure modes we might observe? Yeah. ",
    "start": "3215720",
    "end": "3222500"
  },
  {
    "text": "Perhaps maybe you don't\nwant to lose the ability to do the original\ntask, or you don't",
    "start": "3222500",
    "end": "3229748"
  },
  {
    "text": "want the whole probability\ndistribution [INAUDIBLE]",
    "start": "3229748",
    "end": "3237945"
  },
  {
    "text": "how we used this\nprobability distribution. Mm-hmm. Right. And in some sense, we want to\npreserve the original model.",
    "start": "3237945",
    "end": "3244460"
  },
  {
    "text": "Because if we totally throw\naway the original model, what's the point of starting\nfrom the pre-trained model?",
    "start": "3244460",
    "end": "3249530"
  },
  {
    "text": "Yeah. I have a question. ",
    "start": "3249530",
    "end": "3255510"
  },
  {
    "text": "Want me to go back or-- I'll just ask, so [INAUDIBLE] ",
    "start": "3255510",
    "end": "3269110"
  },
  {
    "text": "Yeah, so the CLS token\nactually is in the vocabulary. It's just another--\nusually, it's like token 0",
    "start": "3269110",
    "end": "3274590"
  },
  {
    "text": "in the vocabulary. And it does show up\nduring pre-training. For BERT, there's actually--",
    "start": "3274590",
    "end": "3280230"
  },
  {
    "text": "I omitted one thing\nabout BERT training, which is that we don't just do\nthe masked language modeling.",
    "start": "3280230",
    "end": "3288420"
  },
  {
    "text": "There's actually this extra\ntask, this next sentence prediction task where we take\nthe representation of the CLS",
    "start": "3288420",
    "end": "3293850"
  },
  {
    "text": "token and we just do\nbinary classification which is whether these\ntwo sentences are",
    "start": "3293850",
    "end": "3299160"
  },
  {
    "text": "consecutive sentences or not. So like half the time, you'll\nsample two random sentences from your data set. Half the time, they'll\nbe one after another.",
    "start": "3299160",
    "end": "3304980"
  },
  {
    "text": "And you just have to\ndo this classification. And that's how we sort of\ntrain that representation. Yeah.",
    "start": "3304980",
    "end": "3310537"
  },
  {
    "text": "But later work has\nshowed that you actually don't need to do that. It doesn't really\nmake a difference. [INAUDIBLE]",
    "start": "3310538",
    "end": "3316640"
  },
  {
    "text": " Yes. So as far as I know,\nthe CLS token is just-- it's in the input during\ntraining, but it's not-- yeah,",
    "start": "3316640",
    "end": "3324080"
  },
  {
    "text": "it's not used. I don't think you'll mask\nit because it will always be the CLS token. So it'd be sort of degenerate. But you still get decent\nrepresentations out of it.",
    "start": "3324080",
    "end": "3330545"
  },
  {
    "text": "So it's kind of mysterious. But there's a\nfollow paper called RoBERTa, which is\nbasically ablating some",
    "start": "3330545",
    "end": "3337799"
  },
  {
    "text": "of the things in BERT\nand turns out you don't need to do the next\nsentence prediction thing. And you can still\nfine-tune it fine.",
    "start": "3337800",
    "end": "3343650"
  },
  {
    "text": "So if you're curious,\nyou can look into that. OK, this is going\nto take forever. ",
    "start": "3343650",
    "end": "3350619"
  },
  {
    "text": "OK, we are here. OK, so we talked about what\na little bit might mean.",
    "start": "3350620",
    "end": "3357310"
  },
  {
    "text": "And basically, we've gotten\nto it, which in my mind is we want to\npreserve the knowledge and the pre-trained model.",
    "start": "3357310",
    "end": "3362589"
  },
  {
    "text": "We don't want to totally\nobliterate everything that's there. And that's sort of like a\nlearning-based objective.",
    "start": "3362590",
    "end": "3368680"
  },
  {
    "text": "Practically speaking,\nwe also want to avoid having\nto actually store a new copy of every\nsingle parameter of our model for\nevery new task that we",
    "start": "3368680",
    "end": "3374920"
  },
  {
    "text": "want to fine-tune on, right? Like, a lot of these\npre-trained models are very big. If hundreds of\nmillions, billions, or even hundreds of\nbillions of parameters,",
    "start": "3374920",
    "end": "3381430"
  },
  {
    "text": "it'd be really\nannoying if we need like a whole new copy of\nthe model for every new task that we want to fine-tune on. So the other motivation\nhere is like we just",
    "start": "3381430",
    "end": "3387950"
  },
  {
    "text": "don't want to have to add\nthat much every single time from a storage perspective.",
    "start": "3387950",
    "end": "3393310"
  },
  {
    "text": "And so to get into\nthis one method that addresses these issues, I\nwant to take a brief walk back",
    "start": "3393310",
    "end": "3398670"
  },
  {
    "text": "in time to 1972 to describe this\nsort of particular view on what",
    "start": "3398670",
    "end": "3407016"
  },
  {
    "text": "a linear transformation\nactually does, which ultimately gets\nat this question of what is a little bit.",
    "start": "3407017",
    "end": "3412619"
  },
  {
    "text": "So consider the fine\nlinear transform, which is the building block of\nneural networks, in general,",
    "start": "3412620",
    "end": "3418380"
  },
  {
    "text": "but also transformers\nspecifically. And we know that for\nsome rank-r matrix, which",
    "start": "3418380",
    "end": "3424710"
  },
  {
    "text": "is a linear transform, we have\na decomposition of this form.",
    "start": "3424710",
    "end": "3429990"
  },
  {
    "text": "And using this\ndecomposition, we know that if we evaluate the matrix\nvector product with some input,",
    "start": "3429990",
    "end": "3438480"
  },
  {
    "text": "we can just rewrite it. So we can push the\nx inside the sum.",
    "start": "3438480",
    "end": "3443610"
  },
  {
    "text": "And we have this essentially\nweighted sum of each vector, v, weighted by this inner product\nof x with this particular u sub",
    "start": "3443610",
    "end": "3451650"
  },
  {
    "text": "r. So this is just like algebra,\nbarely algebra, at this point.",
    "start": "3451650",
    "end": "3459550"
  },
  {
    "text": "And one way we\ncan interpret this is that this matrix vector\nproduct, Wx, Is really",
    "start": "3459550",
    "end": "3465460"
  },
  {
    "text": "outputting a sum over what\nwe can think of as memories. So each vr is sort\nof like a memory,",
    "start": "3465460",
    "end": "3471550"
  },
  {
    "text": "and each ur is kind\nof like a key that determines how relevant\na particular input is",
    "start": "3471550",
    "end": "3476740"
  },
  {
    "text": "to that memory, right? So if x is totally irrelevant\nto a particular memory vr,",
    "start": "3476740",
    "end": "3482590"
  },
  {
    "text": "then that means x will\nbe orthogonal to ur. And so this inner\nproduct will be 0, so we won't include that\nmemory at all in our output.",
    "start": "3482590",
    "end": "3490000"
  },
  {
    "text": "If x is very aligned\nwith the particular ur, that means that memory\nis going to show up very strongly in our output.",
    "start": "3490000",
    "end": "3495080"
  },
  {
    "text": "So this is not\nreally even new math. This is just kind of\nanother way of thinking about the linear transformation,\nwhich is useful for motivating",
    "start": "3495080",
    "end": "3501820"
  },
  {
    "text": "all sorts of things in like\ncomputational neuroscience and ultimately machine learning. But it's useful\nhere because now we",
    "start": "3501820",
    "end": "3507192"
  },
  {
    "text": "can go back to what does it\nmean to fine-tune a little bit. And we can say, well, a little\nbit means we just only want to add a couple of memories.",
    "start": "3507192",
    "end": "3512980"
  },
  {
    "text": "And what that means is\nwe ultimately just want to make a low-rank change\nto W. And that's convenient because a low-rank matrix can\nbe stored much more effectively",
    "start": "3512980",
    "end": "3520119"
  },
  {
    "text": "than the full-rank matrix. So we've kind of addressed\nboth things here. We're going to preserve\nmost of the knowledge because the new\nmatrix is only going",
    "start": "3520120",
    "end": "3526630"
  },
  {
    "text": "to differ from the original\nmatrix by some low-rank change, and we can store this\nchange very efficiently because a low-rank matrix\nis not a lot of parameters.",
    "start": "3526630",
    "end": "3535100"
  },
  {
    "text": "And it's exactly what LoRA does. So when we do fine-tuning,\nultimately, we're going to start with some\npre-trained parameters, W0.",
    "start": "3535100",
    "end": "3542660"
  },
  {
    "text": "These are frozen. And our fine-tuned\nparameters are just going to differ from our\npre-trained parameters",
    "start": "3542660",
    "end": "3547760"
  },
  {
    "text": "by some low-rank\nmatrix AB transpose. So both A and B are\nthese d by p matrices.",
    "start": "3547760",
    "end": "3554090"
  },
  {
    "text": "Sorry, I should have\nwritten W0 and Wft R d by d. So we're just assuming\nthe matrix is square,",
    "start": "3554090",
    "end": "3559340"
  },
  {
    "text": "but there's nothing specific\nto square matrices here. This works perfectly well\nfor non-square matrices.",
    "start": "3559340",
    "end": "3565520"
  },
  {
    "text": "And so when we compute\nthis matrix product, we get some low-rank d by\nd matrix that we add to W0.",
    "start": "3565520",
    "end": "3574160"
  },
  {
    "text": "And we fine-tune\njust A and B here. So we don't fine-tune tune W0. And so we only end\nup needing to store kind of 2 times d times p new\nparameters instead of 2 times",
    "start": "3574160",
    "end": "3584029"
  },
  {
    "text": "d squared new parameters\nfor every layer. ",
    "start": "3584030",
    "end": "3589960"
  },
  {
    "text": "One minor note is that we do-- it's usually much easier\nto fine-tune these models if at the initialization,\nwe're starting out",
    "start": "3589960",
    "end": "3597300"
  },
  {
    "text": "with the same function\nas the pre-trained model. So that means we want AB to\nbe initialized with zeros.",
    "start": "3597300",
    "end": "3602723"
  },
  {
    "text": "But the way we do that\nis a little bit tricky. So if we initialize both\nA and B as all zeros, we're actually not going\nto get any gradient",
    "start": "3602723",
    "end": "3609972"
  },
  {
    "text": "for either of them, so\nwe won't learn anything. And so if we want the product\nAB transpose to be zeros",
    "start": "3609972",
    "end": "3615220"
  },
  {
    "text": "but we want to be\nable to learn things, we actually need to\ninitialize one of them as all zeros and the other\nwith basically nonzero,",
    "start": "3615220",
    "end": "3621520"
  },
  {
    "text": "like normal random\ninitialization. So that's kind of a detail. You'll see that\nin homework three, but it's something\nto keep in mind.",
    "start": "3621520",
    "end": "3626998"
  },
  {
    "text": " So like I said, that's LoRA.",
    "start": "3626998",
    "end": "3632569"
  },
  {
    "text": "I hope that was somewhat\ninteresting to think through. But there are lots and\nlots of different methods",
    "start": "3632570",
    "end": "3639103"
  },
  {
    "text": "for lightweight fine-tuning\nthat people have come up with in the last few years. This is one sort of\nsurvey of a lot of them",
    "start": "3639103",
    "end": "3645590"
  },
  {
    "text": "which plots the percentage\nof the parameters of the original model that\nactually need to get updated",
    "start": "3645590",
    "end": "3650720"
  },
  {
    "text": "with the actual accuracy. And we can see at least for this\nevaluation, which is just one benchmark, LoRA is in the\nrelatively like heavyweight",
    "start": "3650720",
    "end": "3659648"
  },
  {
    "text": "in terms of these\nlightweight methods but also the\nhigh-performing methods. But keep in mind, even though\nthis is relatively heavyweight,",
    "start": "3659648",
    "end": "3666869"
  },
  {
    "text": "it's still only\nfine-tuning less than 1% of the parameters of the model.",
    "start": "3666870",
    "end": "3672230"
  },
  {
    "text": "And what's really interesting\nis that in the same paper-- the paper has a nice, direct,\nand audacious title like,",
    "start": "3672230",
    "end": "3680150"
  },
  {
    "text": "lightweight fine-tuning is\nbetter than in-context learning or something. But their main result\nhere is that actually this",
    "start": "3680150",
    "end": "3687589"
  },
  {
    "text": "is the case at least\nin some settings where if you use some of these\nlightweight fine-tuning methods in the right\nway, you can get better",
    "start": "3687590",
    "end": "3693380"
  },
  {
    "text": "few-shot performance\nthan a GPT-3 model that's like 10 or 100 times bigger.",
    "start": "3693380",
    "end": "3700320"
  },
  {
    "text": "And so in a sense, if you have\nsort of more than a couple of examples--\nin-context learning",
    "start": "3700320",
    "end": "3706190"
  },
  {
    "text": "is still very\npowerful if you only have one example to adapt on. But if you have a\nfew more, if you have in like 20 to\n70 regime, which",
    "start": "3706190",
    "end": "3712280"
  },
  {
    "text": "is I think what they\nstudied in this paper, fine-tuning can be more\nparameter-efficient",
    "start": "3712280",
    "end": "3718460"
  },
  {
    "text": "or scale better than\nin-context learning. But again, this is very, very\nhot off the press, so to speak,",
    "start": "3718460",
    "end": "3724140"
  },
  {
    "text": "so I don't want to\nmake any claims that are too categorical. ",
    "start": "3724140",
    "end": "3730700"
  },
  {
    "text": "And you'll make a slightly\nsimpler but similar comparison in homework three actually. ",
    "start": "3730700",
    "end": "3738210"
  },
  {
    "text": "OK. How are we doing? We're doing great. Good for us.",
    "start": "3738210",
    "end": "3744680"
  },
  {
    "text": "Last section. So we've talked\nabout reconstruction as a heuristic for\nrepresentation learning.",
    "start": "3744680",
    "end": "3751070"
  },
  {
    "text": "We've talked about\nautoencoders which are sort of the first\nstab at using this. And now we've talked\nabout masked autoencoders",
    "start": "3751070",
    "end": "3756237"
  },
  {
    "text": "which sort of try to address\nsome of the issues with them. And I want to talk about\nanother related class of models,",
    "start": "3756237",
    "end": "3762472"
  },
  {
    "text": "another sort of related kind\nof heuristic for pre-training, which is autoregressive\nmodels, which I believe you saw in\na previous lecture",
    "start": "3762472",
    "end": "3769220"
  },
  {
    "text": "on Black-box meta-learning. But I want to revisit\nit in this context because I think\nit's interesting.",
    "start": "3769220",
    "end": "3775640"
  },
  {
    "text": "So in a sense,\nautoregressive models are very closely related\nto masked autoencoders.",
    "start": "3775640",
    "end": "3781220"
  },
  {
    "text": "But they sort of simplify\na couple of things. So first, we can\nthink about what",
    "start": "3781220",
    "end": "3786523"
  },
  {
    "text": "some of the downsides of masked\nautoencoders actually are. So I mean, maybe\nthe most obvious one is like we have this extra\nthing we have to pick.",
    "start": "3786523",
    "end": "3792770"
  },
  {
    "text": "Like, we have to decide how\nwe're going to do the masking, and maybe there are better\nand worse ways to do that. And it's not always\nobvious what they are.",
    "start": "3792770",
    "end": "3799260"
  },
  {
    "text": "And so that just like\ncauses us stress, and we lose sleep and\nall sorts of things. There are other things like--",
    "start": "3799260",
    "end": "3805650"
  },
  {
    "text": "as someone asked before,\nwe're only masking out",
    "start": "3805650",
    "end": "3811099"
  },
  {
    "text": "some of the examples. Sometimes it's 15%. Sometimes it's 75%. Can we mask out more?",
    "start": "3811100",
    "end": "3816260"
  },
  {
    "text": "Why not? Well, we're only using\nbasically part of each example to train on. We're only using the\nparts that we masked out,",
    "start": "3816260",
    "end": "3822710"
  },
  {
    "text": "and we're not computing loss\nfor all the other time steps. So in a sense, we're not\nfully utilizing each training",
    "start": "3822710",
    "end": "3828349"
  },
  {
    "text": "example. And then finally, the\ndownside is that once we pre-train this thing, we\ncan actually sample from it.",
    "start": "3828350",
    "end": "3834595"
  },
  {
    "text": "We don't have a\ngenerative model. We can't sample\nfrom the original-- generate new samples from\nthe new data distribution,",
    "start": "3834595",
    "end": "3840210"
  },
  {
    "text": "which sometimes is\ninteresting and fun.  OK, so basically, the idea\nbehind autoregressive models",
    "start": "3840210",
    "end": "3847380"
  },
  {
    "text": "is we're going to simply ask,\nwell, instead of masking out a random subset\nof the input, what if instead our sort of\nlearning objective is just",
    "start": "3847380",
    "end": "3855037"
  },
  {
    "text": "take a prefix of the\ninput and predict the next word or the next\npixel or the next token or the next patch or whatever?",
    "start": "3855037",
    "end": "3861488"
  },
  {
    "text": "So now we don't have to pick\na masking strategy at all. In a sense, we're\nmasking every token. ",
    "start": "3861488",
    "end": "3867825"
  },
  {
    "text": "And so what we're going\nto learn is simply this autoregressive\nmodel which parameterizes a distribution over the\nnext token or the next word",
    "start": "3867825",
    "end": "3874450"
  },
  {
    "text": "or whatever, given\nthe preceding. So if we have, again, a\ntraining sequence like Joe Biden",
    "start": "3874450",
    "end": "3880570"
  },
  {
    "text": "is the US president, we're\ngoing to actually convert this into six different\ntraining examples by masking out every word in\nthe sequence and conditioning",
    "start": "3880570",
    "end": "3888670"
  },
  {
    "text": "on the ones before it. So we don't really\nhave this constraint of only using 15% of the\nexample anymore because we're",
    "start": "3888670",
    "end": "3896620"
  },
  {
    "text": "computing a loss on\nevery single time step.  And here I've added a\nbeginning-of-sequence token",
    "start": "3896620",
    "end": "3903420"
  },
  {
    "text": "because we need something\nto put in the model to get our first prediction for\nwhat the first word is going to be.",
    "start": "3903420",
    "end": "3908700"
  },
  {
    "text": "So one difference here is\nfor masked language models, sort of the output at\na particular time step is what we think that\nword is if it's masked.",
    "start": "3908700",
    "end": "3916770"
  },
  {
    "text": "Here the output at a\nparticular time step is what we think the\nnext word is going to be. So we have this\nshift by 1 going on.",
    "start": "3916770",
    "end": "3924283"
  },
  {
    "text": "So what this looks\nlike when we actually stick it into our\nmodel is we have x0, which is just the BOS token.",
    "start": "3924283",
    "end": "3930698"
  },
  {
    "text": "So we put that in our\nautoregressive model, and this gives us some\ndistribution over next tokens, given just BOS.",
    "start": "3930698",
    "end": "3937369"
  },
  {
    "text": "And then we look at the\nnext token in the sequence. And this is used as both\nthe input at the next stage, as well as the target\nfor the previous stage.",
    "start": "3937370",
    "end": "3944450"
  },
  {
    "text": "So x1 is actually the target y0. And then we also put\nthis combined sequence",
    "start": "3944450",
    "end": "3951803"
  },
  {
    "text": "through our model, so\nnow we have BOS and Joe. And we get a prediction\nover what we think the next token is going to be.",
    "start": "3951803",
    "end": "3958280"
  },
  {
    "text": "And then we roll this out. So we see the next token. We see Biden. That's the target for\nthe previous step. And it's also the sort\nof incremental input",
    "start": "3958280",
    "end": "3964910"
  },
  {
    "text": "for this step. And we do this forever and ever. ",
    "start": "3964910",
    "end": "3971370"
  },
  {
    "text": "So this is great. We can do this, and\nyou can write papers that people will talk about\na lot, which is awesome.",
    "start": "3971370",
    "end": "3978970"
  },
  {
    "text": "But I think I've sort of\ncheated a little bit here in terms of comparing\nautoregressive models to masked autoencoders.",
    "start": "3978970",
    "end": "3984670"
  },
  {
    "text": "So I've said how with\nmasked autoencoders,",
    "start": "3984670",
    "end": "3989680"
  },
  {
    "text": "we only mask out\n15% of the example. We're not using\nthe whole example. Here we're masking\nout everything. That's great.",
    "start": "3989680",
    "end": "3996180"
  },
  {
    "text": "But I left something out. And you might be\nwondering, well,",
    "start": "3996180",
    "end": "4001800"
  },
  {
    "text": "why can't we just do\nmasked autoencoding but use the same example\nmultiple times until we've",
    "start": "4001800",
    "end": "4007250"
  },
  {
    "text": "masked out everything, right? If we're masking\nout 20%, well, we'll just mask it out\nfive different times.",
    "start": "4007250",
    "end": "4012410"
  },
  {
    "text": "We'll use a different\n20% every time. And now we've used\nthe whole example. And that's totally true.",
    "start": "4012410",
    "end": "4017610"
  },
  {
    "text": "We can do exactly that. But the difference is for an\nautoregressive transformer, the representation\nthat we compute",
    "start": "4017610",
    "end": "4024390"
  },
  {
    "text": "for each prefix is independent\nof all the stuff that comes later. And the significance\nof that is we",
    "start": "4024390",
    "end": "4030900"
  },
  {
    "text": "can actually do this sort\nof efficient training where we mask out everything\nreally efficiently.",
    "start": "4030900",
    "end": "4036120"
  },
  {
    "text": "And what I mean\nby that is we only have to compute a\nnew representation for sort of the marginal new\ntoken at every time step.",
    "start": "4036120",
    "end": "4043208"
  },
  {
    "text": "So when we're predicting what\nthe first token is going to be, we have to compute our\nrepresentation of BOS. But then when we're\npredicting the second token,",
    "start": "4043208",
    "end": "4049830"
  },
  {
    "text": "we can actually just\nreuse the representation we computed of BOS. And we just have to compute\nthe representation of the one",
    "start": "4049830",
    "end": "4056130"
  },
  {
    "text": "new word that we're seeing\nand so on and so forth. And so if we were doing this\nwith a masked autoencoder,",
    "start": "4056130",
    "end": "4061830"
  },
  {
    "text": "every time we change a single\nmask, since the attention is going both ways, the\nrepresentation for every token",
    "start": "4061830",
    "end": "4068130"
  },
  {
    "text": "depends on every other\ntoken in the sequence. If we change just\none mask, we have to do a completely new\nforward pass of our model",
    "start": "4068130",
    "end": "4074040"
  },
  {
    "text": "on the entire sequence. Whereas here, every\ntime we add a new token, we only have to actually\ncompute one new representation.",
    "start": "4074040",
    "end": "4080170"
  },
  {
    "text": "And so this makes-- the fact that our\nsort of attention is only looking backwards\nmakes it much more efficient",
    "start": "4080170",
    "end": "4086340"
  },
  {
    "text": "to do this. So that's why we really\nare gaining something over just doing\nmasked autoencoding with different masks.",
    "start": "4086340",
    "end": "4092050"
  },
  {
    "text": "Does that property\nmake it hard to use these models for\nspell-checking or something",
    "start": "4092050",
    "end": "4098058"
  },
  {
    "text": "like that where you\nreally would want to look in both directions? Yeah. So one of the\ntrade-offs that I think",
    "start": "4098058",
    "end": "4103350"
  },
  {
    "text": "I'll mention in the summary is\nthat the representation quality",
    "start": "4103350",
    "end": "4108870"
  },
  {
    "text": "is a little bit worse for\nautoregressive models, in general, because you have\na more constrained model.",
    "start": "4108870",
    "end": "4115199"
  },
  {
    "text": "It can only look one direction. So you have some benefits, but\nyou also have some drawbacks.",
    "start": "4115200",
    "end": "4120278"
  },
  {
    "text": "Yeah. [INAUDIBLE] ",
    "start": "4120279",
    "end": "4129359"
  },
  {
    "text": "Great question. Yeah. So no. So the way we get around\nthat actually is maybe easiest to visualize if we\njust look at this attention",
    "start": "4129359",
    "end": "4135960"
  },
  {
    "text": "map I drew here. So basically, what's\ngoing to happen-- for the autoregressive\ntransformer,",
    "start": "4135960",
    "end": "4142170"
  },
  {
    "text": "everything is the same,\nexcept what you end up doing is you literally zero\nout all of the attention scores",
    "start": "4142170",
    "end": "4148859"
  },
  {
    "text": "that are on the upper\nhalf of the attention map. And so literally, when\nyou do this weighted sum,",
    "start": "4148859",
    "end": "4155160"
  },
  {
    "text": "all of the terms in\nthis softmax matrix, which would be the\nattention of a token",
    "start": "4155160",
    "end": "4161159"
  },
  {
    "text": "to a token in the future,\nare manually set to 0. So they can't affect the output.",
    "start": "4161160",
    "end": "4166490"
  },
  {
    "text": "[INAUDIBLE] ",
    "start": "4166490",
    "end": "4185439"
  },
  {
    "text": "No. So the representation\nof a token at time, t, only depends on\nthe tokens before t.",
    "start": "4185439",
    "end": "4193770"
  },
  {
    "text": "So if I have-- in the autoregressive case-- [VOCALIZING]",
    "start": "4193770",
    "end": "4201738"
  },
  {
    "text": " in the autoregressive case, when\nI compute the representation",
    "start": "4201738",
    "end": "4208020"
  },
  {
    "text": "for Biden, this is only a\nfunction of the representations of Joe and BOS. So when I add is from my\nnext training example,",
    "start": "4208020",
    "end": "4217199"
  },
  {
    "text": "the representation\nof Biden is going to be the same\nbecause again, it's only going to be a function of\nthe words that come before it. So if I add words\nafter it, they don't",
    "start": "4217200",
    "end": "4223710"
  },
  {
    "text": "factor into the representation\nof Biden at all. [INAUDIBLE] tried to change it\nbecause the meaning [INAUDIBLE]",
    "start": "4223710",
    "end": "4230540"
  },
  {
    "text": "someone tried to\nchange it [INAUDIBLE] ",
    "start": "4230540",
    "end": "4242630"
  },
  {
    "text": "So is the question that\nwhen I add a new word, is that going to change\nthe representation",
    "start": "4242630",
    "end": "4247650"
  },
  {
    "text": "of previous tokens? [INAUDIBLE] prediction for\nthe new word [INAUDIBLE]",
    "start": "4247650",
    "end": "4253000"
  },
  {
    "text": "Yeah, so in a sense,\nthis is a limitation. So the question is-- yeah-- when we add new words, don't\nwe want the representation",
    "start": "4253000",
    "end": "4258420"
  },
  {
    "text": "of prior words to change\nbecause it's extra context? And in a sense, yes, we-- in a perfect model, we\nwould be able to do that.",
    "start": "4258420",
    "end": "4264910"
  },
  {
    "text": "But by making this\nsimplification, we have some other\nbenefits, which is that we can do\nthis form of masking and reuse the representations.",
    "start": "4264910",
    "end": "4270900"
  },
  {
    "text": "If we didn't have\nthis constraint that the map was\nmasked in this way, we would have to do a\ncompletely new forward pass",
    "start": "4270900",
    "end": "4277530"
  },
  {
    "text": "for every example. It would be much less efficient. And we also need to be able\nto sample from the model because it wouldn't give\nus an actual autoregressive",
    "start": "4277530",
    "end": "4283540"
  },
  {
    "text": "distribution that we can sample\none token at a time and just condition on the past stuff.",
    "start": "4283540",
    "end": "4288900"
  },
  {
    "text": "Yeah. One more. And then we'll move forward. I have a couple more slides. So if the words that\nthe model predicts next",
    "start": "4288900",
    "end": "4294770"
  },
  {
    "text": "is completely wrong. Does that wrong\nrepresentation still cache and use to\npredict future words?  Yeah, that's a good question.",
    "start": "4294770",
    "end": "4300690"
  },
  {
    "text": "So during training, we do\nwhat we call teacher forcing. So we-- oh, sorry. Yeah, yeah, yeah. So if the model samples\nlike a bad word basically,",
    "start": "4300690",
    "end": "4309080"
  },
  {
    "text": "like a random word,\nlike Joe Biden dog, you know what's going to happen?",
    "start": "4309080",
    "end": "4314330"
  },
  {
    "text": "During training, we don't\nuse samples from the model. We sort of only use as input\na prefix of our training data",
    "start": "4314330",
    "end": "4320900"
  },
  {
    "text": "and just predict the next word. So we never actually\nuse like a prediction of our model as part of\nthe training process.",
    "start": "4320900",
    "end": "4327398"
  },
  {
    "text": "But at sampling\ntime, this can happen where basically like your\nmodel will buy some bad fortune",
    "start": "4327398",
    "end": "4333199"
  },
  {
    "text": "sample kind of a random\nor low probability word. And it'll get totally derailed. And it'll just start saying\nlike, dog, dog, dog, dog, dog,",
    "start": "4333200",
    "end": "4338500"
  },
  {
    "text": "dog, dog, dog, dog. And then you just have to\nstop generating and try again. [INAUDIBLE] representations\nif we already",
    "start": "4338500",
    "end": "4345170"
  },
  {
    "text": "have embeddings of\nall the vocabulary?",
    "start": "4345170",
    "end": "4350491"
  },
  {
    "text": "So the representation\nhere of Biden is not just going to\nbe of the word Biden. It's going to be Biden\nin the context of Joe.",
    "start": "4350492",
    "end": "4356270"
  },
  {
    "text": "Yeah, so it's instance-specific. So yeah, that's important.",
    "start": "4356270",
    "end": "4362119"
  },
  {
    "text": "We're not just caching\nthe representation of individual words. We're caching the representation\nof this prefix of the sequence",
    "start": "4362120",
    "end": "4367450"
  },
  {
    "text": "in this specific context. So it's sort of caching\nwithin a single batch. ",
    "start": "4367450",
    "end": "4374460"
  },
  {
    "text": "And so just to hammer this home,\nautoregressive transformers are all over the\nplace these days, so you have the\nwhole GPT family.",
    "start": "4374460",
    "end": "4380739"
  },
  {
    "text": "But you also have Megatron,\nwhich is a model from NVIDIA. You have OPT, which is sort\nof an open reproduction of GPT",
    "start": "4380740",
    "end": "4387429"
  },
  {
    "text": "from some researchers at Meta. ",
    "start": "4387430",
    "end": "4392880"
  },
  {
    "text": "Just because they\nwere later doesn't mean we can laugh at them, OK? [INAUDIBLE]",
    "start": "4392880",
    "end": "4398610"
  },
  {
    "text": " We love being Meta.",
    "start": "4398610",
    "end": "4404740"
  },
  {
    "text": "And then there's other\nopen efforts from-- not from companies, from open-source\ncommunities like GPT-Neo.",
    "start": "4404740",
    "end": "4412480"
  },
  {
    "text": "And there are models\nfor vision, too, that are autoregressive\ntransformers, for RL and for decision-making\nfor navigating the web.",
    "start": "4412480",
    "end": "4418390"
  },
  {
    "text": "And then also for\nmultimodal settings for vision and language. So one case study\nof this is a model",
    "start": "4418390",
    "end": "4423580"
  },
  {
    "text": "from DeepMind called Flamingo. And they're sort of\ngetting at this question of how would you build a\nmultimodal autoregressive",
    "start": "4423580",
    "end": "4428920"
  },
  {
    "text": "model. And hopefully, the answer\nis not from scratch. And that's what they sort\nof show in this paper. So for the most part,\nwhat we've been showing",
    "start": "4428920",
    "end": "4435820"
  },
  {
    "text": "is fine-tuning as a\nform of specialization. So we take a general\npurpose model. We use few-shot data to make\nit a task-specific model.",
    "start": "4435820",
    "end": "4442750"
  },
  {
    "text": "But in Flamingo,\nwhat they do is they use fine-tuning as a way\nto actually just combine two pre-trained models. So they have a pre-trained\nautoregressive language",
    "start": "4442750",
    "end": "4449199"
  },
  {
    "text": "model and a pre-trained\nvision feature extractor, and they fine-tuned with a\nlittle bit of multimodal data to get this autoregressive\nimage language model.",
    "start": "4449200",
    "end": "4455690"
  },
  {
    "text": "And I'm not going to go\nthrough the architecture because we're running\na little low on time, but it looks kind of weird.",
    "start": "4455690",
    "end": "4461300"
  },
  {
    "text": "But it's actually\npretty straightforward. But what's really\ninteresting about it is you do this autoregressive\npre-training basically",
    "start": "4461300",
    "end": "4466978"
  },
  {
    "text": "with unsupervised data scraped\nfrom the web of websites that have both images and text. And now you get a model that\nyou can do a few-shot prompting",
    "start": "4466978",
    "end": "4472900"
  },
  {
    "text": "the way you can with GPT-3. But you can have images\nin the input too. So you can do sort of\nfew-shot image captioning",
    "start": "4472900",
    "end": "4478480"
  },
  {
    "text": "or visual question-answering. And what's kind of cool is\nthat actually the few-shot performance of the largest\nversion of their model actually",
    "start": "4478480",
    "end": "4485097"
  },
  {
    "text": "approaches state-of-the-art for\nmodels that are fine-tuned like on the whole training set. So that's pretty cool.",
    "start": "4485097",
    "end": "4492755"
  },
  {
    "text": "One little note is just that\nour autoregressive model is actually different\nfrom masked autoencoders. And to skip right to the\npunch line, the answer is no.",
    "start": "4492755",
    "end": "4499820"
  },
  {
    "text": "In a sense, an\nautoregressive model is really just a\nmasked autoencoder with a specific form\nof the mask function,",
    "start": "4499820",
    "end": "4506480"
  },
  {
    "text": "where x is just a prefix of--\nx tilde is just a prefix of x. And y is just the next token.",
    "start": "4506480",
    "end": "4514699"
  },
  {
    "text": "OK, so just to\nsummarize today, we talked about sort of\nthe main intuition for autoencoders, which is\nthat a good representation is",
    "start": "4514700",
    "end": "4521415"
  },
  {
    "text": "one that lets us\nreconstruct the input. And we talked about\nmasked autoencoders, which are a modification of\nthis basic autoencoder that",
    "start": "4521415",
    "end": "4527420"
  },
  {
    "text": "restores sort of a\npartially deleted input. And this helps avoid\nsome of the degeneracies of unmasked autoencoders.",
    "start": "4527420",
    "end": "4533873"
  },
  {
    "text": "These masked autoencoders\nare state-of-the-art in pre-training and few-shot\nlearning for both vision and language.",
    "start": "4533873",
    "end": "4539810"
  },
  {
    "text": "And we saw autoregressive\nmodels which are really a special case\nof masked autoencoders.",
    "start": "4539810",
    "end": "4546180"
  },
  {
    "text": "A couple of pros and cons\nof contrastive models and autoencoders and\nmasked autoencoders-- I don't know if I need to really\nenumerate these because I guess",
    "start": "4546180",
    "end": "4553710"
  },
  {
    "text": "we're right about out of time,\nbut I think the main trade-off that you can think\nabout is that--",
    "start": "4553710",
    "end": "4560298"
  },
  {
    "text": "especially using some of\nthese new contrastive learning methods that don't require\nsampling negatives anymore, really, one of the\nbiggest trade-offs",
    "start": "4560298",
    "end": "4566768"
  },
  {
    "text": "is that the contrastive\nmodels learn really high-quality representations. But the masked\nautoencoders might be a little better\nif you're actually",
    "start": "4566768",
    "end": "4573210"
  },
  {
    "text": "going to fine-tune the model. So if you just want to compute\nrepresentations and pre-cache all the representations and\nthen throw away the model,",
    "start": "4573210",
    "end": "4580290"
  },
  {
    "text": "maybe the contrastive model\nwill work a little better. But if you want to keep your\nactual pre-trained model around and fine-tune it a bit,\nthe masked autoencoders",
    "start": "4580290",
    "end": "4586889"
  },
  {
    "text": "might work better. ",
    "start": "4586890",
    "end": "4593000"
  }
]