[
  {
    "start": "0",
    "end": "26000"
  },
  {
    "text": "Hello and welcome to the Empirical Risk Minimization section.",
    "start": "4460",
    "end": "9570"
  },
  {
    "text": "This section is, in many ways, the heart of the class. Empirical risk minimization is the process by which predictors are learned from data.",
    "start": "9570",
    "end": "20560"
  },
  {
    "start": "26000",
    "end": "189000"
  },
  {
    "text": "So many of the predictors that we've seen, in fact, many predictors overall, have a parameterized form.",
    "start": "26050",
    "end": "33680"
  },
  {
    "text": "We think about y hat is g of x and Theta, where g is a function that determines the structural form or the predictor.",
    "start": "33680",
    "end": "44510"
  },
  {
    "text": "It might be a neural network, or it might be a linear predictor,",
    "start": "44510",
    "end": "49880"
  },
  {
    "text": "or it might be a tree. And Theta is a set of parameters.",
    "start": "49880",
    "end": "55560"
  },
  {
    "text": "It might be a vector, or a matrix, or some other data structure and that set of parameters is going",
    "start": "55560",
    "end": "62750"
  },
  {
    "text": "to be one of the determinants that produces the output y hat.",
    "start": "62750",
    "end": "69575"
  },
  {
    "text": "And when we learn, we're going to learn by choosing the parameters Theta and we're going to leave g fixed.",
    "start": "69575",
    "end": "78575"
  },
  {
    "text": "So for example, we might consider linear regression. And y is scalar and we'd have y hat will be g Theta of x.",
    "start": "78575",
    "end": "89985"
  },
  {
    "text": "And here, g Theta of x would take the form Theta 1 times x 1 plus Theta two times x2,",
    "start": "89985",
    "end": "96060"
  },
  {
    "text": "all the way up to Theta d times x d. And so Theta here is a parameter,",
    "start": "96060",
    "end": "102485"
  },
  {
    "text": "it's a d-dimensional vector. And we might write y hat is Theta transpose multiplied by x.",
    "start": "102485",
    "end": "111539"
  },
  {
    "text": "We also might have a predictor for a vector y in r_m.",
    "start": "111670",
    "end": "118070"
  },
  {
    "text": "Um, if that's a linear regression model,",
    "start": "118070",
    "end": "123080"
  },
  {
    "text": "then we would have y hat is g Theta of x. It's also Theta 1 times x_1 plus Theta 2 times x_2,",
    "start": "123080",
    "end": "130490"
  },
  {
    "text": "all the way up to Theta to d times x_d. But here, each of the Theta i's is an m-dimensional vector and the x's, x_1 through x_d,",
    "start": "130490",
    "end": "140625"
  },
  {
    "text": "are the coefficients which determine a linear combination of the vectors Theta 1 through Theta d. Often,",
    "start": "140625",
    "end": "149720"
  },
  {
    "text": "we would write that in terms of a matrix. So we'd write our matrix Theta,",
    "start": "149720",
    "end": "154730"
  },
  {
    "text": "which is a d by m matrix. And the ith row of that matrix is Theta i transpose.",
    "start": "154730",
    "end": "162349"
  },
  {
    "text": "And then we can express the relationship y hat is g Theta of x,",
    "start": "162350",
    "end": "168005"
  },
  {
    "text": "as y hat is Theta transpose times x, just as before.",
    "start": "168005",
    "end": "173580"
  },
  {
    "text": "We might have other types of predictors as well. We might have a tree prediction model, in which case, Theta would encode the tree.",
    "start": "174680",
    "end": "181939"
  },
  {
    "text": "It would tell us the thresholds at each of the vertices of the tree and the leaf values.",
    "start": "181940",
    "end": "188425"
  },
  {
    "text": "Now, we're going to choose which particular Theta to use based on some training data.",
    "start": "188425",
    "end": "194360"
  },
  {
    "start": "189000",
    "end": "314000"
  },
  {
    "text": "We're going to have n data pairs, xi, yi,",
    "start": "194360",
    "end": "199970"
  },
  {
    "text": "i is 1 up to n. And that's going to be the training data and we will use it to fit the model.",
    "start": "199970",
    "end": "208220"
  },
  {
    "text": "And this is called the training process. And there's many different training processes",
    "start": "208220",
    "end": "214903"
  },
  {
    "text": "that vary based on what kind of predictor, we're choosing to fit and what our performance metric is.",
    "start": "214904",
    "end": "223795"
  },
  {
    "text": "So for example, if we're training a linear regression model and if y is scalar,",
    "start": "223795",
    "end": "230629"
  },
  {
    "text": "and then we might use something like least squares. We will choose Theta to minimize the sum of all the data points i is 1 to",
    "start": "230629",
    "end": "240379"
  },
  {
    "text": "n of yi minus the predicted value y hat i squared.",
    "start": "240380",
    "end": "248320"
  },
  {
    "text": "And this notation, y hat i is- what it means is g evaluated at x i.",
    "start": "248320",
    "end": "257739"
  },
  {
    "text": "So y hat i is the prediction when the predictor is fed with the ith value of x.",
    "start": "257740",
    "end": "267949"
  },
  {
    "text": "And as a result, we say, well, we're going to choose Theta to minimize the sum of the squares of the prediction error,",
    "start": "267950",
    "end": "278645"
  },
  {
    "text": "which is the sum from i is 1 to n of g Theta of x i minus y i squared.",
    "start": "278645",
    "end": "284849"
  },
  {
    "text": "Now, that's a very reasonable way of learning a predictor. Um, uh, in- in this lecture,",
    "start": "285640",
    "end": "292310"
  },
  {
    "text": "we'll actually cover a- a more general method, which is very widely used and it's very effective.",
    "start": "292310",
    "end": "300620"
  },
  {
    "text": "And, uh, it's called empirical risk minimization. And what it is at its heart is a generalization of the least squares idea.",
    "start": "300620",
    "end": "311400"
  },
  {
    "start": "314000",
    "end": "695000"
  },
  {
    "text": "And the way this works is we have a loss function. A loss function takes two vectors as its input,",
    "start": "315610",
    "end": "323615"
  },
  {
    "text": "a y hat and a y, and it gives you back a real number. And it quantifies how close y hat is to y.",
    "start": "323615",
    "end": "333985"
  },
  {
    "text": "Really what it does is it quantifies how badly y hat approximates.",
    "start": "333985",
    "end": "340649"
  },
  {
    "text": "Why? Because normally, the loss function of y hat and y is",
    "start": "340649",
    "end": "346729"
  },
  {
    "text": "small when y hat is close to y and large when y hat is different from y. Um,",
    "start": "346730",
    "end": "354775"
  },
  {
    "text": "so if the loss function is small, we will say that y hat is really a good approximation of y.",
    "start": "354775",
    "end": "360410"
  },
  {
    "text": "And if the loss function is large, we'll say it's about approximation of y. And it's very common that we actually arrange things so",
    "start": "360410",
    "end": "369785"
  },
  {
    "text": "the loss function evaluated when y hat is equal to y is zero.",
    "start": "369785",
    "end": "376340"
  },
  {
    "text": "And the loss function when y hat is not equal to y is non-negative.",
    "start": "376340",
    "end": "382889"
  },
  {
    "text": "So the- some very common examples. The first is, uh, the quadratic loss function.",
    "start": "383390",
    "end": "391014"
  },
  {
    "text": "I've got a scalar y and therefore, a scalar y hat and the quadratic loss is just y hat minus y squared.",
    "start": "391015",
    "end": "399430"
  },
  {
    "text": "If I've got vectors for y and y hat, then the quadratic loss is the 2o-norm or the Euclidean norm of y hat minus y squared,",
    "start": "399430",
    "end": "410840"
  },
  {
    "text": "which is just the sum of the squares of the differences between y hat i and y i.",
    "start": "410840",
    "end": "420030"
  },
  {
    "text": "Uh, another common loss isthe absolute loss, the- if I've got a scalar y and a scalar y hat again,",
    "start": "420350",
    "end": "429260"
  },
  {
    "text": "then the absolute loss is just the absolute value of y hat minus y.",
    "start": "429260",
    "end": "434695"
  },
  {
    "text": "All right. Here's another loss. This is the maximum of y hat divided by y minus 1 and y divided by y hat minus 1.",
    "start": "434695",
    "end": "448380"
  },
  {
    "text": "So this is the fractional loss or the relative loss.",
    "start": "449180",
    "end": "455580"
  },
  {
    "text": "So if y hat is 20% more than y,",
    "start": "455580",
    "end": "462064"
  },
  {
    "text": "then y hat over y minus 1 will be 0.2. And, uh, if y hat is, uh,",
    "start": "462065",
    "end": "470430"
  },
  {
    "text": "20% less than y, then y over y hat minus 1 will be 0.2.",
    "start": "470430",
    "end": "477345"
  },
  {
    "text": "Uh, another convenient way of expressing this is as",
    "start": "477345",
    "end": "482750"
  },
  {
    "text": "the exponential of the absolute value of the log of y hat minus the log of y minus 1.",
    "start": "482750",
    "end": "490465"
  },
  {
    "text": "And often, we might scale it by 100 and then it really is percentage error.",
    "start": "490465",
    "end": "495900"
  },
  {
    "text": "Uh, we often use fractional loss for, uh,",
    "start": "496370",
    "end": "502210"
  },
  {
    "text": "quantities which either range over a very wide range of magnitudes.",
    "start": "502210",
    "end": "509895"
  },
  {
    "text": "We saw last time the example of website visits,",
    "start": "509895",
    "end": "515224"
  },
  {
    "text": "which can range over many orders of magnitude. Uh, another case where we might use fractional loss is where we're dealing with prices,",
    "start": "515225",
    "end": "525800"
  },
  {
    "text": "where very often, we're more concerned with the percentage difference in two prices,",
    "start": "525800",
    "end": "531035"
  },
  {
    "text": "then the absolute difference in two prices. And we'll see some particular interpretations of",
    "start": "531035",
    "end": "538385"
  },
  {
    "text": "these losses and cases where some of them are better, more naturally suited than others.",
    "start": "538385",
    "end": "544630"
  },
  {
    "text": "And we would also see many other possible loss functions in this class.",
    "start": "544630",
    "end": "550410"
  },
  {
    "text": "Now we start off with a loss function and when we're using it we can construct something called the empirical risk.",
    "start": "555200",
    "end": "561435"
  },
  {
    "text": "And the empirical risk is just the average loss over the data points. So in order to compute the empirical risk,",
    "start": "561435",
    "end": "569130"
  },
  {
    "text": "we need to have a bunch of data. We need to have a predictor and we need to have a loss function.",
    "start": "569130",
    "end": "576180"
  },
  {
    "text": "And then for each data point, we simply compute the loss between g Theta of x_i and y_i.",
    "start": "576180",
    "end": "585450"
  },
  {
    "text": "And then in order to compute the empirical risk, we average that loss over all the data points.",
    "start": "585450",
    "end": "592780"
  },
  {
    "text": "And then if the empirical risk is small, the predictor does a good job on average over all the data.",
    "start": "593210",
    "end": "603300"
  },
  {
    "text": "At least according to the particular loss function that we've chosen. We usually write the empirical risk as",
    "start": "603300",
    "end": "611760"
  },
  {
    "text": "a function of the Theta that parametizes the predictor. Of course, it's also a function of the dataset.",
    "start": "611760",
    "end": "620520"
  },
  {
    "text": "But we suppress that in the notation. And you might say, well, this is quite similar to what we talked about before,",
    "start": "620520",
    "end": "628709"
  },
  {
    "text": "when we talked about performance metrics. Uh, and that's absolutely true.",
    "start": "628710",
    "end": "633850"
  },
  {
    "text": "Um, I mean, in both cases, we're measuring how well a predictor does. Um, the difference is- is that performance metric is something that we",
    "start": "633850",
    "end": "642290"
  },
  {
    "text": "use to judge how well our predictor is doing.",
    "start": "642290",
    "end": "647320"
  },
  {
    "text": "Whereas empirical risk is something that we're going to use to train the model.",
    "start": "647320",
    "end": "654915"
  },
  {
    "text": "We're going to train the predictor. We're going to choose which predictor we're going to use according to",
    "start": "654915",
    "end": "660300"
  },
  {
    "text": "a procedure which is trying to make the empirical risk small.",
    "start": "660300",
    "end": "666134"
  },
  {
    "text": "And very often empirical risk and performance metric are the same.",
    "start": "666134",
    "end": "671250"
  },
  {
    "text": "And you might say, well, why not just choose the empirical risk equal to the performance metric? And we're going to have some,",
    "start": "671250",
    "end": "677970"
  },
  {
    "text": "quite some details to s- say about that in the- in the rest of this class.",
    "start": "677970",
    "end": "683985"
  },
  {
    "text": "Uh, and usually we t- what we try and do is- we try and pick empirical risk to correspond to a performance metric.",
    "start": "683985",
    "end": "690464"
  },
  {
    "text": "Um, sometimes training works better when you don't do that.",
    "start": "690465",
    "end": "698500"
  },
  {
    "start": "695000",
    "end": "928000"
  },
  {
    "text": "Here's an idea of some more examples of, uh, empirical risk. For example, if you got quadratic loss and scalar y.",
    "start": "701300",
    "end": "710579"
  },
  {
    "text": "Well then the empirical risk is the mean square error. If you've got scalar y and you're look- using an absolute loss function,",
    "start": "710580",
    "end": "721890"
  },
  {
    "text": "then the empirical risk is the mean absolute error.",
    "start": "721890",
    "end": "725410"
  },
  {
    "text": "So let's talk about empirical risk minimization.",
    "start": "727940",
    "end": "741690"
  },
  {
    "text": "It's the method according to which we choose Theta. And the idea is very simple.",
    "start": "741690",
    "end": "748185"
  },
  {
    "text": "Choose Theta to minimize the empirical risk. Um, and uh, one way to say that,",
    "start": "748185",
    "end": "758865"
  },
  {
    "text": "is to say what we're doing with Theta is we're trying to make the average loss small over the entire dataset.",
    "start": "758865",
    "end": "767654"
  },
  {
    "text": "It's a way of getting our predictor to match up with the dataset well.",
    "start": "767655",
    "end": "774340"
  },
  {
    "text": "Sometimes you can actually solve empirical risk minimization exactly analytically.",
    "start": "776480",
    "end": "784574"
  },
  {
    "text": "And so in particular, if g of Theta is a linear predictor and,",
    "start": "784575",
    "end": "790019"
  },
  {
    "text": "uh, we're using a square of loss function. Then the empirical risk minimization problem is the least squares problem.",
    "start": "790019",
    "end": "799050"
  },
  {
    "text": "And that's something that we can solve analytically. This is an explicit formula for the optimal Theta.",
    "start": "799050",
    "end": "807610"
  },
  {
    "text": "In most cases, it doesn't work like that. In most cases, there is no analytic solution to the minimization problem.",
    "start": "807860",
    "end": "818070"
  },
  {
    "text": "There's no formula. And instead, we have to use numerical optimization to find a Theta that minimizes the empirical risk.",
    "start": "818070",
    "end": "829805"
  },
  {
    "text": "And usually it's actually slightly worse than that in that the numerical procedures that we use cannot",
    "start": "829805",
    "end": "836345"
  },
  {
    "text": "guarantee to find the Theta that actually minimizes the empirical risk.",
    "start": "836345",
    "end": "842759"
  },
  {
    "text": "But instead can only guarantee to approximately minimize the empirical risk.",
    "start": "842760",
    "end": "848950"
  },
  {
    "text": "Um, but there are also reasons why that's okay. We typically don't want or need a Theta that is a perfect minimizer.",
    "start": "849890",
    "end": "859680"
  },
  {
    "text": "But an approximate minimizer is fine. And we'll have uh, more to say on that as well.",
    "start": "859680",
    "end": "866500"
  },
  {
    "text": "Now the particular value of Theta that you had- the particular predictor that you get depends on the particular loss that you chose.",
    "start": "867350",
    "end": "876834"
  },
  {
    "text": "And how are we going to determine which loss we should choose? Given that we've seen mean,",
    "start": "876835",
    "end": "884185"
  },
  {
    "text": "square error mean, absolute error mean, fractional error and many other potential loss functions.",
    "start": "884185",
    "end": "892680"
  },
  {
    "text": "And the answer to these kinds of questions is always the same. We validate- we validate against some external test set.",
    "start": "892680",
    "end": "901575"
  },
  {
    "text": "Um, and when we do the validation, we don't always validate with the same error measure that we've chosen to train with.",
    "start": "901575",
    "end": "912945"
  },
  {
    "text": "We don't validate with the risk. We validate with the performance metric.",
    "start": "912945",
    "end": "919930"
  },
  {
    "text": "Now, there is one more wrinkle",
    "start": "921800",
    "end": "929910"
  },
  {
    "start": "928000",
    "end": "1573000"
  },
  {
    "text": "that we add to empirical risk minimization. Which it turns out can make it work a lot better.",
    "start": "929910",
    "end": "936940"
  },
  {
    "text": "And that is this thing called regularization. And regularization works as follows.",
    "start": "937580",
    "end": "945540"
  },
  {
    "text": "We're very concerned when we're training a predictor that we don't overfit the predictor.",
    "start": "945540",
    "end": "955514"
  },
  {
    "text": "We don't have a predictor that is tuned very well to features of",
    "start": "955515",
    "end": "961200"
  },
  {
    "text": "the training data that aren't features that are generically in the data.",
    "start": "961200",
    "end": "970995"
  },
  {
    "text": "In other words, if we take some other dataset from the same phenomena, is that,",
    "start": "970995",
    "end": "977745"
  },
  {
    "text": "we want to make sure that our predictor is tuned well to those features that occur in that other dataset as well.",
    "start": "977745",
    "end": "985290"
  },
  {
    "text": "And not to particular wiggles that only showed up in our training set. The way we do that is we look at the sensitivity of the predictor.",
    "start": "985290",
    "end": "998235"
  },
  {
    "text": "We look at it's- how much it responds to small changes in x.",
    "start": "998235",
    "end": "1008945"
  },
  {
    "text": "So we would call a predictor d Theta insensitive. If for some x near x tilde,",
    "start": "1008945",
    "end": "1016714"
  },
  {
    "text": "g Theta of x is also near g Theta of x tilde.",
    "start": "1016715",
    "end": "1022770"
  },
  {
    "text": "Another way to say it is that, if the features are close, then the predictions will also be close.",
    "start": "1022930",
    "end": "1028685"
  },
  {
    "text": "Um, there are many ways that you can make this more mathematically precise.",
    "start": "1028685",
    "end": "1037535"
  },
  {
    "text": "Um, one is the notion of continuity. Um, and there are other more quantitative notions than continuity as well.",
    "start": "1037535",
    "end": "1046925"
  },
  {
    "text": "Um, but the key point is not so much how you measure the sensitivity of the predictor.",
    "start": "1046925",
    "end": "1055685"
  },
  {
    "text": "It's the benefit you get. By making a predictor which is insensitive to the features x,",
    "start": "1055685",
    "end": "1067700"
  },
  {
    "text": "you make a predictor that generalizes well to your new dataset.",
    "start": "1067700",
    "end": "1073054"
  },
  {
    "text": "And that's particularly important when you don't have a lot of training data.",
    "start": "1073055",
    "end": "1077760"
  },
  {
    "text": "So insensitivity is a good attribute for a predictor to have.",
    "start": "1078820",
    "end": "1083975"
  },
  {
    "text": "Despite sounding like a bad one.",
    "start": "1083975",
    "end": "1087210"
  },
  {
    "text": "And a regularizer, that's a function of",
    "start": "1093100",
    "end": "1098150"
  },
  {
    "text": "Theta that measure- measures how sensitive the predictor g Theta is.",
    "start": "1098150",
    "end": "1104710"
  },
  {
    "text": "So the regularizer is a function r. It takes as input Theta and it returns a real number.",
    "start": "1104710",
    "end": "1114975"
  },
  {
    "text": "So here we've used R_p to indicate the space in which Theta lives.",
    "start": "1114975",
    "end": "1123905"
  },
  {
    "text": "It is some p dimensional space. And if y is scalar and we're using a linear predictor,",
    "start": "1123905",
    "end": "1132580"
  },
  {
    "text": "then p is equal to d. If y is m dimensional and x is d dimensional,",
    "start": "1132580",
    "end": "1139909"
  },
  {
    "text": "then p is equal to d times m. And if we're using a neural network,",
    "start": "1139910",
    "end": "1145105"
  },
  {
    "text": "then p might be a much larger number. So r Theta is chosen such that it's small when",
    "start": "1145105",
    "end": "1154910"
  },
  {
    "text": "g Theta is insensitive and it's large when g Theta is sensitive.",
    "start": "1154910",
    "end": "1160305"
  },
  {
    "text": "Um, there are some cases where we can,",
    "start": "1160305",
    "end": "1165885"
  },
  {
    "text": "in a straightforward way, quantify how sensitive a predictor is.",
    "start": "1165885",
    "end": "1172550"
  },
  {
    "text": "Um, so if for example, in a linear regression model. Where g of Theta of x is Theta transpose x.",
    "start": "1172550",
    "end": "1180900"
  },
  {
    "text": "Then the small sensitivity corresponds to small Theta.",
    "start": "1180900",
    "end": "1189075"
  },
  {
    "text": "And the way to see then, is to look at what's called the Cauchy-Schwarz inequality.",
    "start": "1189075",
    "end": "1195230"
  },
  {
    "text": "Remember the Cauchy-Schwarz inequality? Let me write it down. It says that if I've got two vectors,",
    "start": "1195640",
    "end": "1203815"
  },
  {
    "text": "p and q, and I look at P transpose q. The absolute value of p transpose q is less than or equal",
    "start": "1203815",
    "end": "1213259"
  },
  {
    "text": "to the 2-norm of p multiplied by the 2-norm of q.",
    "start": "1213260",
    "end": "1219780"
  },
  {
    "text": "We're going to need a slight generalization of that to- to say the following result.",
    "start": "1231140",
    "end": "1236550"
  },
  {
    "text": "So I suppose g Theta of x is Theta transpose x and I look at",
    "start": "1236550",
    "end": "1242010"
  },
  {
    "text": "the sensitivity by measuring the norm of the difference between g Theta of x and g Theta of x tilde.",
    "start": "1242010",
    "end": "1250215"
  },
  {
    "text": "Well, that's equal to the norm of Theta transpose multiplied by x minus x tilde.",
    "start": "1250215",
    "end": "1258010"
  },
  {
    "text": "The norm of Theta transpose x minus",
    "start": "1259160",
    "end": "1265185"
  },
  {
    "text": "x tilde is less than or equal to the norm of Theta multiplied by the norm of x minus x tilde.",
    "start": "1265185",
    "end": "1274575"
  },
  {
    "text": "Now the tricky thing here is the choice of norm that we use for that inequality.",
    "start": "1274575",
    "end": "1280840"
  },
  {
    "text": "And first of all, let me, uh, remind you of the result we're using.",
    "start": "1280850",
    "end": "1286530"
  },
  {
    "text": "First of all, Theta transpose times a vector x,",
    "start": "1286530",
    "end": "1291855"
  },
  {
    "text": "the absolute value, less than or equal to the norm of Theta 2-norm, lives in norm of x in the 2-norm.",
    "start": "1291855",
    "end": "1301305"
  },
  {
    "text": "Now, this is true when both Theta and x are vectors.",
    "start": "1301305",
    "end": "1307740"
  },
  {
    "text": "So here we've got Theta is in R_d and x is in R_d.",
    "start": "1307740",
    "end": "1315550"
  },
  {
    "text": "Now I want to also consider the case where y is not a scalar,",
    "start": "1315740",
    "end": "1320880"
  },
  {
    "text": "but y is a vector. And then we must look at what happens when we multiply a matrix times a vector and ask",
    "start": "1320880",
    "end": "1328590"
  },
  {
    "text": "if there's a simple way of constructing a bound on that quantity.",
    "start": "1328590",
    "end": "1334184"
  },
  {
    "text": "So here, let's consider a matrix a and look at the two norm",
    "start": "1334185",
    "end": "1340785"
  },
  {
    "text": "of the matrix a times the ma- the vector x.",
    "start": "1340785",
    "end": "1348330"
  },
  {
    "text": "And we can- we know what that is, that's the sum over i is 1 up to",
    "start": "1348330",
    "end": "1357389"
  },
  {
    "text": "m of a_i transpose x squared.",
    "start": "1357390",
    "end": "1364725"
  },
  {
    "text": "I'm going to make that squared. Well, here what I've done is I've written A in terms",
    "start": "1364725",
    "end": "1372630"
  },
  {
    "text": "of its rows, a_1 transpose up to a_m transpose.",
    "start": "1372630",
    "end": "1380475"
  },
  {
    "text": "So each little a_i here is a vector in R_d.",
    "start": "1380475",
    "end": "1387299"
  },
  {
    "text": "And A here is a matrix in R_m",
    "start": "1387300",
    "end": "1394880"
  },
  {
    "text": "by d. Now this quantity here, a_i transpose x.",
    "start": "1394880",
    "end": "1402420"
  },
  {
    "text": "Well, we know how to bound that because that comes from the straightforward Cauchy-Schwarz inequality.",
    "start": "1402420",
    "end": "1409170"
  },
  {
    "text": "So this is less than or equal to, the sum over i is 1 up to m,",
    "start": "1409170",
    "end": "1414735"
  },
  {
    "text": "of the norm of a_i 2 norm squared,",
    "start": "1414735",
    "end": "1420420"
  },
  {
    "text": "the norm of x 2 norm squared, using the square of the Cauchy-Schwarz inequality.",
    "start": "1420420",
    "end": "1428169"
  },
  {
    "text": "Now of course, we can put parentheses around that because x doesn't depend on i.",
    "start": "1428480",
    "end": "1435840"
  },
  {
    "text": "And this, this quantity here",
    "start": "1435840",
    "end": "1443370"
  },
  {
    "text": "is the sum of the norm squared of each of the comp- col- of each of the rows of the matrix A.",
    "start": "1443370",
    "end": "1451695"
  },
  {
    "text": "And each row of a matrix A, we can calculate its norm squared by taking the sum of the squares of its entries.",
    "start": "1451695",
    "end": "1458445"
  },
  {
    "text": "And then if we sum over all the rows, what we've effectively done is taken the sum of the squares of all of the entries of A.",
    "start": "1458445",
    "end": "1467685"
  },
  {
    "text": "So that's equal to the sum over i,",
    "start": "1467685",
    "end": "1472830"
  },
  {
    "text": "1 up to m, the sum of a j is 1 up to d of the norm of a_ij.",
    "start": "1472830",
    "end": "1481605"
  },
  {
    "text": "And it's not the norm of a_ij- of a_ij squared multiplied",
    "start": "1481605",
    "end": "1491790"
  },
  {
    "text": "by the norm of x squared, and this quantity is known as the Frobenius norm squared of a matrix.",
    "start": "1491790",
    "end": "1504285"
  },
  {
    "text": "And there's the analog of the Euclidean norm of a vector. It just takes the sum of the squares and all of",
    "start": "1504285",
    "end": "1510260"
  },
  {
    "text": "the entries of the matrix A and square roots them. So we can apply that result to our predictor, our vector predictor,",
    "start": "1510260",
    "end": "1521819"
  },
  {
    "text": "and we find that the norm of Theta transpose x minus",
    "start": "1521819",
    "end": "1527610"
  },
  {
    "text": "x tilde is less than or equal to the Frobenius norm of f multiplied by the norm of x minus x tilde.",
    "start": "1527610",
    "end": "1535150"
  },
  {
    "text": "And this suggests that, well, maybe we should use as a regularizer,",
    "start": "1536690",
    "end": "1541750"
  },
  {
    "text": "the norm of Theta squared and the Frobenius norm.",
    "start": "1542690",
    "end": "1548370"
  },
  {
    "text": "And that is one possible choice and it's a very common choice for a regularizer.",
    "start": "1548370",
    "end": "1553970"
  },
  {
    "text": "If we keep the regularizer small, we choose Theta that makes R of Theta small,",
    "start": "1553970",
    "end": "1560480"
  },
  {
    "text": "then we will make our predictor less sensitive to x.",
    "start": "1560480",
    "end": "1568330"
  },
  {
    "start": "1573000",
    "end": "1634000"
  },
  {
    "text": "Now, when y is scalar, the most common regularizer to use",
    "start": "1573740",
    "end": "1579315"
  },
  {
    "text": "is simply the sum of the squares of the components of Theta. The 2-norm of Theta squared.",
    "start": "1579315",
    "end": "1585190"
  },
  {
    "text": "That has a name, that's called ridge regularization.",
    "start": "1585230",
    "end": "1590235"
  },
  {
    "text": "For vector y, we take the sum of the squares of all of the entries of the matrix Theta,",
    "start": "1590235",
    "end": "1597420"
  },
  {
    "text": "the Frobenius norm of Theta squared. And that's also called a ridge regularization.",
    "start": "1597420",
    "end": "1603639"
  },
  {
    "text": "Another very popular regularizer is to take the 1-norm of Theta,",
    "start": "1604550",
    "end": "1612765"
  },
  {
    "text": "when Theta is a vector, that's the sum of the absolute values of the entries of Theta.",
    "start": "1612765",
    "end": "1619560"
  },
  {
    "text": "And when Theta is a matrix instead of a vector, we take the sum over all of the entries of Theta.",
    "start": "1619560",
    "end": "1629565"
  },
  {
    "text": "We take the sum of the absolute values of all of the entries of Theta. Now, when there is a constant feature.",
    "start": "1629565",
    "end": "1638145"
  },
  {
    "start": "1634000",
    "end": "2077000"
  },
  {
    "text": "For example, when x_1 is 1,",
    "start": "1638145",
    "end": "1643154"
  },
  {
    "text": "um, then we do something slightly different when we're doing regularization.",
    "start": "1643154",
    "end": "1648345"
  },
  {
    "text": "This often happens because we very often would like to have a predictor with a constant term in it.",
    "start": "1648345",
    "end": "1656774"
  },
  {
    "text": "So for example, if g Theta is a linear predictor, then g Theta of x is Theta transpose x.",
    "start": "1656775",
    "end": "1664980"
  },
  {
    "text": "And if, uh, x_1 is chosen to be 1,",
    "start": "1664980",
    "end": "1670515"
  },
  {
    "text": "then g Theta of x will be Theta 1 plus Theta 2 transpose times the remaining components of x.",
    "start": "1670515",
    "end": "1678640"
  },
  {
    "text": "And we also do this when we have neural network predictors.",
    "start": "1678710",
    "end": "1683850"
  },
  {
    "text": "In the general matrix case, g Theta of x will look like Theta transpose x.",
    "start": "1683850",
    "end": "1691544"
  },
  {
    "text": "But now as the first component which corresponds to x_1 F_1 will be the first row of Theta.",
    "start": "1691545",
    "end": "1700410"
  },
  {
    "text": "Here we've written that as Theta 1 comma colon. And so the resulting predictor will be g Theta of x is",
    "start": "1700410",
    "end": "1711419"
  },
  {
    "text": "Theta 1 comma colon transpose plus Theta 2 colon d comma colon transpose x_2 colon d. Here,",
    "start": "1711420",
    "end": "1722820"
  },
  {
    "text": "let's be explicit about what the notation means. The notation here means the following.",
    "start": "1722820",
    "end": "1728910"
  },
  {
    "text": "If I have A_23 comma 4, 7,",
    "start": "1728910",
    "end": "1735285"
  },
  {
    "text": "that means take a slice out of their matrix, which consists of the second and third rows.",
    "start": "1735285",
    "end": "1743625"
  },
  {
    "text": "And the columns 4, 5, 6, and 7. I can also write things like this.",
    "start": "1743625",
    "end": "1751480"
  },
  {
    "text": "A_2 colon 3 comma colon, which means take the entire second and third rows in every column.",
    "start": "1751610",
    "end": "1762090"
  },
  {
    "text": "Now- and this is notation that's used by MATLAB and Julia and has also",
    "start": "1762090",
    "end": "1767690"
  },
  {
    "text": "now spread its way back from programming into mathematics.",
    "start": "1767690",
    "end": "1773490"
  },
  {
    "text": "Um, so g Theta of x is constant,",
    "start": "1773490",
    "end": "1779490"
  },
  {
    "text": "plus a linear term in x. Now the constant terms don't affect the sensitivity.",
    "start": "1779490",
    "end": "1786210"
  },
  {
    "text": "We can see this if we evaluate g Theta of x minus g Theta of x tilde,",
    "start": "1786210",
    "end": "1792345"
  },
  {
    "text": "then we get- the constant term simply canceling out, then we're left with the Theta 2 colon d transpose multiplied by x minus x tilde.",
    "start": "1792345",
    "end": "1804555"
  },
  {
    "text": "And as a result, there's no need to regularize the first row of Theta if x_1 is constant.",
    "start": "1804555",
    "end": "1812230"
  },
  {
    "text": "And we use a regularizer which is simply a norm or some other function of the remaining entries of Theta.",
    "start": "1812230",
    "end": "1820809"
  },
  {
    "text": "And we might use the Frobenius norm squared of the last d minus 1 rows of Theta.",
    "start": "1820810",
    "end": "1828580"
  },
  {
    "text": "And that brings us to a regularized ERM. That's a method where we instead of choosing",
    "start": "1833380",
    "end": "1841220"
  },
  {
    "text": "a Theta that simply minimizes the empirical risk, L of Theta,",
    "start": "1841220",
    "end": "1847400"
  },
  {
    "text": "we also try to find a Theta that trades off the insensitivity of the predictor.",
    "start": "1847400",
    "end": "1857810"
  },
  {
    "text": "And the way it does that is we find- try to find",
    "start": "1857810",
    "end": "1862880"
  },
  {
    "text": "a Theta which makes both L of Theta small and r of Theta small.",
    "start": "1862880",
    "end": "1868830"
  },
  {
    "text": "The way you do that is via technical regularized ERM, where we choose Theta to minimize",
    "start": "1869260",
    "end": "1875750"
  },
  {
    "text": "the weighted sum L of Theta plus Lambda times r of Theta.",
    "start": "1875750",
    "end": "1880860"
  },
  {
    "text": "Lambda here is a non-negative number, it's a parameter, it's called a regularization hyper-parameter.",
    "start": "1881350",
    "end": "1889895"
  },
  {
    "text": "And hyper-parameter here means that instead of it being a parameter that's learned from the training data directly,",
    "start": "1889895",
    "end": "1898264"
  },
  {
    "text": "it's a parameter we're going to choose via a different process, which I will tell you about in a second.",
    "start": "1898264",
    "end": "1905105"
  },
  {
    "text": "Now, when Lambda is 0, regularized ERM just reduces to ERM.",
    "start": "1905105",
    "end": "1910400"
  },
  {
    "text": "When Lambda is very large, then Theta that minimizes L of Theta plus Lambda r of",
    "start": "1910400",
    "end": "1917570"
  },
  {
    "text": "Theta is going to be very close to Theta that just minimizes r of Theta.",
    "start": "1917570",
    "end": "1923210"
  },
  {
    "text": "And so we're going to end up with a predictor which is very",
    "start": "1923210",
    "end": "1928640"
  },
  {
    "text": "insensitive but may not do very well at fitting the predicted data.",
    "start": "1928640",
    "end": "1935330"
  },
  {
    "text": "And so when Lambda takes intermediate values between 0 and very large,",
    "start": "1935330",
    "end": "1940445"
  },
  {
    "text": "well then we're gonna get some balance between minimizing L of Theta and minimizing r of Theta.",
    "start": "1940445",
    "end": "1947730"
  },
  {
    "text": "And in most cases, you cannot solve regularized ERM exactly",
    "start": "1947980",
    "end": "1953240"
  },
  {
    "text": "just as you can't solve- maybe solve ERM exactly. And so we use numerical optimization.",
    "start": "1953240",
    "end": "1959820"
  },
  {
    "text": "So with ERM, you're just minimizing the empirical risk L of Theta and you're choosing the Theta that does that.",
    "start": "1964180",
    "end": "1972320"
  },
  {
    "text": "With RERM because you are adding on this term to the objective function Lambda times r of Theta,",
    "start": "1972320",
    "end": "1980585"
  },
  {
    "text": "a Theta that you get does not minimize the empirical risk. In other words, it's not producing the predictor that best fits the training data,",
    "start": "1980585",
    "end": "1992495"
  },
  {
    "text": "it's producing a predictor that fits the training data worse than the ERM predictor.",
    "start": "1992495",
    "end": "1998195"
  },
  {
    "text": "But it is less sensitive than the ERM predictor because the- what you gain is you get a predictor that's making the regularizer small.",
    "start": "1998195",
    "end": "2009010"
  },
  {
    "text": "Now the benefit here is that a predictor that is less sensitive often generalizes better.",
    "start": "2009010",
    "end": "2015565"
  },
  {
    "text": "It makes better predictions on unseen data than the predictor that fits the training data as well as possible.",
    "start": "2015565",
    "end": "2024715"
  },
  {
    "text": "And this is what you see in practice, is that even though there's- there's a predictor,",
    "start": "2024715",
    "end": "2032320"
  },
  {
    "text": "which is the ERM predictor that does the best on the training data.",
    "start": "2032320",
    "end": "2038695"
  },
  {
    "text": "That predictor when you take it and try it on unseen data it's not the best predictor.",
    "start": "2038695",
    "end": "2044350"
  },
  {
    "text": "And you get a better predictor on unseen data by finding a predictor which solves the RERM minimization problem.",
    "start": "2044350",
    "end": "2057264"
  },
  {
    "text": "Which backs off a little bit from fitting the data as well as possible,",
    "start": "2057265",
    "end": "2062950"
  },
  {
    "text": "and instead compromises and chooses a Theta which is less sensitive.",
    "start": "2062950",
    "end": "2073369"
  },
  {
    "start": "2077000",
    "end": "2201000"
  },
  {
    "text": "We still have to say how do we choose these things? How do we choose the regularizer and how do we choose the regularization parameter, Lambda?",
    "start": "2078330",
    "end": "2086080"
  },
  {
    "text": "And ultimately the answer is the same for all of these questions. Use validation, you have a performance metric,",
    "start": "2086080",
    "end": "2093250"
  },
  {
    "text": "look at the predictor that you got and try it out on some unseen data and see how well it does.",
    "start": "2093250",
    "end": "2099835"
  },
  {
    "text": "Pick the one that does the best. For Lambda in particular,",
    "start": "2099835",
    "end": "2105040"
  },
  {
    "text": "there's a very specific technique called regularization hyper-parameter search.",
    "start": "2105040",
    "end": "2110065"
  },
  {
    "text": "And that's for choosing Lambda. And the way this works is that you choose a set of values of Lambda,",
    "start": "2110065",
    "end": "2117235"
  },
  {
    "text": "say 50 between say 10 to the minus 5 and 10 to the 5, and usually logarithmically spaced.",
    "start": "2117235",
    "end": "2124400"
  },
  {
    "text": "And for each one of those Lambda values,",
    "start": "2125420",
    "end": "2131594"
  },
  {
    "text": "you solve the RERM problem. You minimize L of Theta plus Lambda times r of Theta.",
    "start": "2131594",
    "end": "2138500"
  },
  {
    "text": "Now for each Lambda value, you're gonna get a different Theta, you're gonna get a different predictor.",
    "start": "2138500",
    "end": "2143515"
  },
  {
    "text": "And so Theta, you get a bunch of different Theta values, 50 different Theta values, 50 different Theta vectors,",
    "start": "2143515",
    "end": "2150265"
  },
  {
    "text": "and that's called the regularization path. Now with each of those 50 different Theta values,",
    "start": "2150265",
    "end": "2157300"
  },
  {
    "text": "we have a corresponding predictor and we can evaluate the performance of those predictors on the test set.",
    "start": "2157300",
    "end": "2165800"
  },
  {
    "text": "And what we do is we choose the value of Lambda that gives the best test performance.",
    "start": "2166440",
    "end": "2172525"
  },
  {
    "text": "And the corresponding predictor is the one we use.",
    "start": "2172525",
    "end": "2176779"
  },
  {
    "text": "Now there's a particular case for which we can solve ERM and RERM exactly,",
    "start": "2183150",
    "end": "2190630"
  },
  {
    "text": "and those are called least squares and ridge regression. And I'm going to review least squares and then tell you about ridge regression.",
    "start": "2190630",
    "end": "2199490"
  },
  {
    "text": "So when we have a square loss and linear predictor,",
    "start": "2200640",
    "end": "2207085"
  },
  {
    "start": "2201000",
    "end": "3122000"
  },
  {
    "text": "we can solve the ERM problem explicitly, exactly, analytically. We have a predictor GC till x is Theta transpose X.",
    "start": "2207085",
    "end": "2216655"
  },
  {
    "text": "We have data consisting of n pairs x_i, y_i.",
    "start": "2216655",
    "end": "2222230"
  },
  {
    "text": "Now the empirical risk is the average of the loss of",
    "start": "2222300",
    "end": "2230110"
  },
  {
    "text": "the predictor evaluated at each of the data points compared with,",
    "start": "2230110",
    "end": "2237805"
  },
  {
    "text": "so the loss evaluate to get a prediction value Theta transpose x_i and the true y_i at that point.",
    "start": "2237805",
    "end": "2246370"
  },
  {
    "text": "Now in our case the loss is just quadratic, and so this is Theta transpose x_i minus y_i squared.",
    "start": "2246370",
    "end": "2254980"
  },
  {
    "text": "And if y is a vector, then this should really be the norm.",
    "start": "2254980",
    "end": "2260770"
  },
  {
    "text": "L of Theta should be 1 on n sum from i",
    "start": "2260770",
    "end": "2266320"
  },
  {
    "text": "is 1 up to n over the norm of Theta transpose x_i minus y_i squared.",
    "start": "2266320",
    "end": "2274450"
  },
  {
    "text": "[NOISE]",
    "start": "2274450",
    "end": "2293010"
  },
  {
    "text": "Now we're going to express this in a convenient matrix form as the Frobenius norm",
    "start": "2293010",
    "end": "2298770"
  },
  {
    "text": "of X Theta minus Y squared divided by n. To do this,",
    "start": "2298770",
    "end": "2304170"
  },
  {
    "text": "we will construct two matrices, X and Y. X is an n by d matrix whose ith row is the ith feature vector x_i",
    "start": "2304170",
    "end": "2315120"
  },
  {
    "text": "transposed and y is an n by m matrix whose ith row is the ith target vector y_i transposed.",
    "start": "2315120",
    "end": "2326560"
  },
  {
    "text": "Now, when we look at this expression,",
    "start": "2327530",
    "end": "2333210"
  },
  {
    "text": "X Theta minus Y, that's a matrix. Its first row is simply x_1 transpose Theta minus y_1 transpose.",
    "start": "2333210",
    "end": "2344775"
  },
  {
    "text": "Second row is x_2 transpose Theta minus y_2 transpose and so on.",
    "start": "2344775",
    "end": "2351460"
  },
  {
    "text": "Now the norm squared of the prediction error,",
    "start": "2351650",
    "end": "2359049"
  },
  {
    "text": "so the loss at the first data point is the norm squared of that row,",
    "start": "2359090",
    "end": "2367770"
  },
  {
    "text": "and that's just the sum of the squares of the entries in the row. I'd like to compute the empirical risk.",
    "start": "2367770",
    "end": "2375690"
  },
  {
    "text": "In order to do that, I've got to sum up all of the different norm squareds, all of the different losses,",
    "start": "2375690",
    "end": "2382155"
  },
  {
    "text": "and divide them by n. So that's just the sum of the squares of all of the entries in this matrix divided by n,",
    "start": "2382155",
    "end": "2392655"
  },
  {
    "text": "and that's 1 on n times the Frobenius norm of that matrix squared.",
    "start": "2392655",
    "end": "2399550"
  },
  {
    "text": "And now I've got to choose the Theta that minimizes that quantity.",
    "start": "2399740",
    "end": "2407325"
  },
  {
    "text": "Now, let's do a quick review of least squares.",
    "start": "2407325",
    "end": "2412869"
  },
  {
    "text": "Suppose I had this problem, minimize the norm of Xw minus v squared.",
    "start": "2416240",
    "end": "2427560"
  },
  {
    "text": "That's a least squares problem, and here w and v are vectors.",
    "start": "2427560",
    "end": "2434760"
  },
  {
    "text": "What's the solution to this? Well, we've seen this before in our linear algebra class.",
    "start": "2434760",
    "end": "2440550"
  },
  {
    "text": "This- the optimal solution is X transpose X inverse X transpose v. We're minimizing over w,",
    "start": "2440550",
    "end": "2453655"
  },
  {
    "text": "and we can get this by expanding this norm and differentiating, for example.",
    "start": "2453655",
    "end": "2461710"
  },
  {
    "text": "Now, suppose I want to solve our more complicated problem,",
    "start": "2461840",
    "end": "2469244"
  },
  {
    "text": "X Theta minus Y Frobenius norm squared,",
    "start": "2469245",
    "end": "2475335"
  },
  {
    "text": "where Theta is now a matrix. I can do that by writing Theta in terms of its columns, w_1,",
    "start": "2475335",
    "end": "2487410"
  },
  {
    "text": "w_2 up to w_m, and y in terms of its columns,",
    "start": "2487410",
    "end": "2494714"
  },
  {
    "text": "y_1, y_2 up to y_m.",
    "start": "2494715",
    "end": "2499330"
  },
  {
    "text": "And then the ith column of X Theta minus Y is simply X w_i minus y_i.",
    "start": "2499730",
    "end": "2510330"
  },
  {
    "text": "Now, the Frobenius norm squared of a matrix is the sum of the squares of the Euclidean norm of the columns.",
    "start": "2510330",
    "end": "2519570"
  },
  {
    "text": "So this quantity is the sum",
    "start": "2519570",
    "end": "2527760"
  },
  {
    "text": "over i is 1 up to m of the norm of X w_i minus y_i squared.",
    "start": "2527760",
    "end": "2537190"
  },
  {
    "text": "Now I'm able to minimize this quantity and I'm minimizing it by choosing the w_is.",
    "start": "2537560",
    "end": "2546300"
  },
  {
    "text": "Now, this is a sum with m terms and the ith term in this sum only depends on w_i.",
    "start": "2546300",
    "end": "2556020"
  },
  {
    "text": "And so in order to minimize this sum, I choose the w_1 to minimize the first term,",
    "start": "2556020",
    "end": "2562905"
  },
  {
    "text": "the w_2 to minimize the second term and so on. Each one of those minimizers looks like this.",
    "start": "2562905",
    "end": "2570840"
  },
  {
    "text": "It's a least squares problem and so the answer to that individual least squares problem looks like that.",
    "start": "2570840",
    "end": "2581650"
  },
  {
    "text": "And that means that when I stack all these columns next to each other, I find that W is X transpose X inverse X transpose Y,",
    "start": "2582440",
    "end": "2596055"
  },
  {
    "text": "and so we can solve a matrix least squares problem in",
    "start": "2596055",
    "end": "2601410"
  },
  {
    "text": "the Frobenius norm by solving m separately squares problems,",
    "start": "2601410",
    "end": "2608670"
  },
  {
    "text": "and it turns out that the matrix inside each of those least squares problems is the same.",
    "start": "2608670",
    "end": "2615734"
  },
  {
    "text": "It's simply X transpose X inverse X transpose. So I just need to solve that once, multiply it by Y,",
    "start": "2615735",
    "end": "2624585"
  },
  {
    "text": "and that will give me all of the Ws at once, which is the matrix W that I wanted.",
    "start": "2624585",
    "end": "2629865"
  },
  {
    "text": "I guess I called that matrix Theta rather than W, let's call it Theta.",
    "start": "2629865",
    "end": "2636160"
  },
  {
    "text": "So that gives us the minimizing Theta, X dagger times Y,",
    "start": "2642980",
    "end": "2649380"
  },
  {
    "text": "which is just X transpose X inverse X transpose Y. This is called least squares regression.",
    "start": "2649380",
    "end": "2657310"
  },
  {
    "text": "Now, if we're doing regularized ERM and we've got a regularizer,",
    "start": "2662600",
    "end": "2670035"
  },
  {
    "text": "which is the norm squared of Theta, and we've got square losses in our loss function,",
    "start": "2670035",
    "end": "2677865"
  },
  {
    "text": "and we've got a linear predictor, then we can solve our ERM exactly as well,",
    "start": "2677865",
    "end": "2683984"
  },
  {
    "text": "and this is called ridge regression. The RERM objective function is just like our ERM objective function with one extra term.",
    "start": "2683985",
    "end": "2695820"
  },
  {
    "text": "The extra term being Lambda times the norm of Theta squared with the norm as the Frobenius norm.",
    "start": "2695820",
    "end": "2705220"
  },
  {
    "text": "And so the overall objective has two terms,",
    "start": "2705410",
    "end": "2712095"
  },
  {
    "text": "both of which have matrix norms in Theta.",
    "start": "2712095",
    "end": "2717540"
  },
  {
    "text": "Now, I can express this in a convenient way.",
    "start": "2717540",
    "end": "2724960"
  },
  {
    "text": "I can stack up my two expressions- my two different norm expressions to make one norm expression,",
    "start": "2725060",
    "end": "2733690"
  },
  {
    "text": "and in order to make sense of this expression here, we will notice the following things.",
    "start": "2733850",
    "end": "2739710"
  },
  {
    "text": "First of all, if I've got two matrices, A, B multiplied by Theta,",
    "start": "2739710",
    "end": "2746130"
  },
  {
    "text": "that just works out to be A Theta B Theta.",
    "start": "2746130",
    "end": "2752529"
  },
  {
    "text": "The second is that if I'm looking at",
    "start": "2752870",
    "end": "2758370"
  },
  {
    "text": "the norm of A Theta B Theta,",
    "start": "2758370",
    "end": "2766030"
  },
  {
    "text": "and that's the Frobenius norm squared. Well, that's just the sum of the squares of the entries of the matrix.",
    "start": "2767540",
    "end": "2774240"
  },
  {
    "text": "So I can break that up very conveniently as the norm of A Theta F squared plus the norm of B Theta F squared.",
    "start": "2774240",
    "end": "2787540"
  },
  {
    "text": "And if I use those two facts, I can see that this is going to have- this product here is going to be X Theta minus",
    "start": "2787670",
    "end": "2798480"
  },
  {
    "text": "Y in the top block and the bottom block is going to be root n Lambda Theta,",
    "start": "2798480",
    "end": "2805965"
  },
  {
    "text": "and then I'm going to take the norm squared of both of those blocks separately and end up with",
    "start": "2805965",
    "end": "2811590"
  },
  {
    "text": "my expression for the objective function of regularized ERM.",
    "start": "2811590",
    "end": "2819460"
  },
  {
    "text": "Now, this problem is",
    "start": "2821900",
    "end": "2828660"
  },
  {
    "text": "precisely of the form that we had for ERM. It's just got a larger matrix here and a larger matrix here.",
    "start": "2828660",
    "end": "2838570"
  },
  {
    "text": "And remember that in the ERM problem when I was trying to solve the minimum over Theta",
    "start": "2838690",
    "end": "2845494"
  },
  {
    "text": "of the norm of A Theta minus B Frobenius norm squared,",
    "start": "2845495",
    "end": "2852700"
  },
  {
    "text": "that was an explicit answer, which was Theta is equal to A transpose A inverse A transpose B.",
    "start": "2852700",
    "end": "2862780"
  },
  {
    "text": "So here I simply need to compute this matrix,",
    "start": "2863360",
    "end": "2868510"
  },
  {
    "text": "X on top of root n Lambda identity transposed times itself inversed times the same matrix again,",
    "start": "2868550",
    "end": "2879015"
  },
  {
    "text": "times the B matrix, which is Y, 0, and that works out to be this expression here as the optimal Theta.",
    "start": "2879015",
    "end": "2889090"
  },
  {
    "text": "And just to see that, if I work out X root n",
    "start": "2893810",
    "end": "2902160"
  },
  {
    "text": "Lambda identity transpose X root n Lambda identity,",
    "start": "2902160",
    "end": "2911895"
  },
  {
    "text": "I get X transpose X plus n Lambda identity.",
    "start": "2911895",
    "end": "2919690"
  },
  {
    "text": "Now, when Lambda is greater than zero, it so happens that this inverse always exists, so I don't need the usual assumption that the columns of x are linearly independent.",
    "start": "2923840",
    "end": "2934780"
  },
  {
    "text": "And so we can explicitly solve Ridge regression in the case where- which is RERM in the case where we've got a quadratic loss,",
    "start": "2935540",
    "end": "2946020"
  },
  {
    "text": "quadratic regularizer, and a linear predictor. [BACKGROUND] Uh, we can- here's a- uh, uh, Julia implementation.",
    "start": "2946020",
    "end": "2959625"
  },
  {
    "text": "Um, you give it a matrix X and a matrix Y and it will return for you the corresponding Theta matrix.",
    "start": "2959625",
    "end": "2965760"
  },
  {
    "text": "[NOISE] Here's the case where we don't regularize the first row of Theta.",
    "start": "2965760",
    "end": "2976125"
  },
  {
    "text": "The only thing that changes is that we multiply Theta inside the regularization term by E,",
    "start": "2976125",
    "end": "2986025"
  },
  {
    "text": "where E is a matrix [NOISE] which has zero for it's first column",
    "start": "2986025",
    "end": "2991770"
  },
  {
    "text": "and has a d minus 1 by d minus 1 identity for its remaining columns.",
    "start": "2991770",
    "end": "2997770"
  },
  {
    "text": "And therefore, E times Theta simply picks out the last d minus 1 rows of Theta.",
    "start": "2997770",
    "end": "3005510"
  },
  {
    "text": "As a result, the norm of E Theta squared is precisely the norm that we wanted,",
    "start": "3005510",
    "end": "3013760"
  },
  {
    "text": "the norm of the last d minus 1 rows of Theta.",
    "start": "3013760",
    "end": "3018450"
  },
  {
    "text": "That translates just as before [NOISE] into,",
    "start": "3018910",
    "end": "3024950"
  },
  {
    "text": "uh, uh, this larger matrix expression.",
    "start": "3024950",
    "end": "3030875"
  },
  {
    "text": "And when I take the product of this matrix transpose times itself,",
    "start": "3030875",
    "end": "3037190"
  },
  {
    "text": "I get the extra term back there, E transpose E. E transpose E,",
    "start": "3037190",
    "end": "3046520"
  },
  {
    "text": "it's a matrix which is square and has an- a d minus 1 by d minus 1identity in the lower right block,",
    "start": "3046520",
    "end": "3056825"
  },
  {
    "text": "and in the top left block, it just has a 0 instead of a 1. So it's an identity matrix which is just missing one of its 1s.",
    "start": "3056825",
    "end": "3065640"
  },
  {
    "text": "And here's a Julia implementation of that case as well.",
    "start": "3069280",
    "end": "3074690"
  },
  {
    "text": "Notice that when we write in Julia, the code looks very like the mathematics,",
    "start": "3074690",
    "end": "3083990"
  },
  {
    "text": "and that's a- a very convenient feature. The only perhaps slight difference here is that there's",
    "start": "3083990",
    "end": "3089240"
  },
  {
    "text": "this nice backslash function in Julia which is a- a convenient shorthand for the explicitly square solution that we've seen before,",
    "start": "3089240",
    "end": "3100565"
  },
  {
    "text": "so A backslash B means A transpose A inverse A transpose B.",
    "start": "3100565",
    "end": "3112620"
  },
  {
    "text": "At least in the case where A is skinny and full rank, that's exactly what it means.",
    "start": "3113050",
    "end": "3118560"
  },
  {
    "start": "3122000",
    "end": "3298000"
  },
  {
    "text": "Now, let's look at a, uh- an example of how this works in practice.",
    "start": "3122230",
    "end": "3127955"
  },
  {
    "text": "Now, this is a- a dataset for 442 individuals of",
    "start": "3127955",
    "end": "3134990"
  },
  {
    "text": "which has been measured 10 different indicators for diabetes,",
    "start": "3134990",
    "end": "3143015"
  },
  {
    "text": "and those are our 10 components of u.",
    "start": "3143015",
    "end": "3148654"
  },
  {
    "text": "And x will have 11 components because we will have a constant feature added on.",
    "start": "3148655",
    "end": "3155070"
  },
  {
    "text": "Now, the target variable, the y, is simply a scalar, and that's some measure of diabetes progression",
    "start": "3155530",
    "end": "3163130"
  },
  {
    "text": "over one year in some biological indicator of that, and that's on the vertical axis on these plots.",
    "start": "3163130",
    "end": "3170940"
  },
  {
    "text": "And on the horizontal axis in each one of these plots is a different component of u.",
    "start": "3171400",
    "end": "3178640"
  },
  {
    "text": "So the first one here is age, we can see individuals between age 20 and age 80,",
    "start": "3178640",
    "end": "3185045"
  },
  {
    "text": "and some spread over the resulting diabetes indicators. The second one is sex,",
    "start": "3185045",
    "end": "3193339"
  },
  {
    "text": "and those have been labeled here as one or two which is a particular choice of embedding for the two possible values in this dataset.",
    "start": "3193340",
    "end": "3202280"
  },
  {
    "text": "Uh, we have BMI, body mass indicator, BP is blood pressure,",
    "start": "3202280",
    "end": "3208895"
  },
  {
    "text": "and then we have s1 to s6 which are particular blood serum levels.",
    "start": "3208895",
    "end": "3214680"
  },
  {
    "text": "So we've got this data and we're going to try and fit it using ERM and RERM.",
    "start": "3215410",
    "end": "3225920"
  },
  {
    "text": "We will take the data and we'll split it 80, 20, so we'll use 80% of it for training,",
    "start": "3225920",
    "end": "3231740"
  },
  {
    "text": "and 20% of it, we'll use as validation data which- using which we're going- using them-",
    "start": "3231740",
    "end": "3237710"
  },
  {
    "text": "and we will use the validation data to choose the hyper-parameter Lambda.",
    "start": "3237710",
    "end": "3244830"
  },
  {
    "text": "We'll use the Lambda values between 10 to the minus 5 and 10 to the 4.",
    "start": "3245950",
    "end": "3251540"
  },
  {
    "text": "[NOISE] Here, we look at two plots.",
    "start": "3251540",
    "end": "3257315"
  },
  {
    "text": "The first plot is a plot of the empirical risk over the optimal Theta as we vary Lambda.",
    "start": "3257315",
    "end": "3265895"
  },
  {
    "text": "So remember what the approach is here, we've got lambda values varying between 10 to the minus 5 and 10 to the four,",
    "start": "3265895",
    "end": "3275300"
  },
  {
    "text": "and we've got, say, 50 different values. For each one of those Lambda values,",
    "start": "3275300",
    "end": "3280595"
  },
  {
    "text": "we minimize L of Theta plus Lambda times r of Theta.",
    "start": "3280595",
    "end": "3286310"
  },
  {
    "text": "We get a Theta for each one of those Lambda values,",
    "start": "3286310",
    "end": "3292115"
  },
  {
    "text": "and for each one of those Thetas, we can compute the empirical risk on the training dataset.",
    "start": "3292115",
    "end": "3299359"
  },
  {
    "start": "3298000",
    "end": "3515000"
  },
  {
    "text": "That's this plot right here. [NOISE] Uh, something to",
    "start": "3299360",
    "end": "3310130"
  },
  {
    "text": "notice is when Lambda is very small, [NOISE] down here, we've got",
    "start": "3310130",
    "end": "3317870"
  },
  {
    "text": "a Theta that is effectively minimizing empirical risk.",
    "start": "3317870",
    "end": "3325110"
  },
  {
    "text": "And as we increase lambda, well, then we're starting to trade off and instead of minimizing just empirical risk,",
    "start": "3326500",
    "end": "3335660"
  },
  {
    "text": "we're starting to balance out sensitivity. We can also look at the corresponding r values,",
    "start": "3335660",
    "end": "3344690"
  },
  {
    "text": "the regularizer values, and you can see that it goes the opposite way. When Lambda is very small,",
    "start": "3344690",
    "end": "3352339"
  },
  {
    "text": "the minimizer makes no effort to make r of Theta small,",
    "start": "3352340",
    "end": "3357890"
  },
  {
    "text": "and r of Theta is just whatever it happens to be. As we increase Lambda, well, suddenly,",
    "start": "3357890",
    "end": "3365299"
  },
  {
    "text": "the minimizer is compelled to minimize r of Theta instead of minimizing L of theta.",
    "start": "3365300",
    "end": "3373040"
  },
  {
    "text": "When we get over here, we've got r of Theta is 0.",
    "start": "3373040",
    "end": "3378395"
  },
  {
    "text": "Remember, that's the sum of the squares of the entries of the matrix of- of Theta, which means we've got a predictor which is completely 0,",
    "start": "3378395",
    "end": "3386670"
  },
  {
    "text": "'cause our predictor is not doing anything, it's completely insensitive to the data.",
    "start": "3387490",
    "end": "3392820"
  },
  {
    "text": "So at this end, when Lambda is very large, we end up with zero predictors,",
    "start": "3393700",
    "end": "3398900"
  },
  {
    "text": "predictors which are constant. They still have the constant term in there,",
    "start": "3398900",
    "end": "3404135"
  },
  {
    "text": "but the rest of the- the last d minus 1 columns of Theta are 0.",
    "start": "3404135",
    "end": "3409260"
  },
  {
    "text": "And at this end- at the other end of the- of the Lambdas, we end up with predictors which minimize the empirical risk.",
    "start": "3409330",
    "end": "3418730"
  },
  {
    "text": "As we increase Lambda, the empirical risk increases so the fit gets worse,",
    "start": "3418730",
    "end": "3425855"
  },
  {
    "text": "but the sensitivity decreases so the regularizer gets smaller.",
    "start": "3425855",
    "end": "3431000"
  },
  {
    "text": "[NOISE]",
    "start": "3431000",
    "end": "3438310"
  },
  {
    "text": "Now, for each value of Lambda, we have a Theta. Theta here is predicting a scalar Y from 11 components of X.",
    "start": "3438310",
    "end": "3448795"
  },
  {
    "text": "Remember, we've got 10 measured variables and the constant feature. And, uh, so C J is just 11 numbers.",
    "start": "3448795",
    "end": "3457855"
  },
  {
    "text": "It's, uh, a 11 dimensional vector. So we can plot those 11 numbers as we vary Lambda.",
    "start": "3457855",
    "end": "3466420"
  },
  {
    "text": "And what you can see is that when Lambda is very small, they have some particular values.",
    "start": "3466420",
    "end": "3472180"
  },
  {
    "text": "And then as we move to the right and increase Lambda, the components of Theta stopped to get smaller.",
    "start": "3472180",
    "end": "3480380"
  },
  {
    "text": "And eventually we end up with all of Theta 0 as we know.",
    "start": "3480390",
    "end": "3487160"
  },
  {
    "text": "Model parameters generally gets smaller because the- the regularized empirical risk minimization is starting to focus on minimizing our Theta,",
    "start": "3487530",
    "end": "3498565"
  },
  {
    "text": "which is the same as minimizing the norm squared of Theta. People call regularization shrinkage because of this phenomena.",
    "start": "3498565",
    "end": "3508250"
  },
  {
    "text": "Now this is the important plot. Here we have two performance blocks,",
    "start": "3513630",
    "end": "3523840"
  },
  {
    "start": "3515000",
    "end": "3874000"
  },
  {
    "text": "we're looking at mean square error on the training data,",
    "start": "3523840",
    "end": "3529750"
  },
  {
    "text": "that's the blue plot, and on the test data, that's the red plot. As we vary Lambda,",
    "start": "3529750",
    "end": "3536350"
  },
  {
    "text": "remember we've got 50 different values of Lambda. Each one- for each one of those, we have a predictor.",
    "start": "3536350",
    "end": "3541809"
  },
  {
    "text": "That predictor, we can measure its performance in two ways. One is we can measure it on the training set.",
    "start": "3541810",
    "end": "3549970"
  },
  {
    "text": "And the other time we can measure it on the validation set- the test set.",
    "start": "3549970",
    "end": "3555760"
  },
  {
    "text": "Now we already know, what happens on the training set as we increase Lambda.",
    "start": "3555760",
    "end": "3563540"
  },
  {
    "text": "The training error, the empirical risk measured on the training set increases.",
    "start": "3564930",
    "end": "3574450"
  },
  {
    "text": "But look what happens on the test set.",
    "start": "3574610",
    "end": "3579850"
  },
  {
    "text": "As we increase Lambda. Well, yes, we are naturally increasing the training error.",
    "start": "3581300",
    "end": "3591255"
  },
  {
    "text": "But because the predictor generalize better, because the predictor is less sensitive,",
    "start": "3591255",
    "end": "3596910"
  },
  {
    "text": "it does better on the test set.",
    "start": "3596910",
    "end": "3601660"
  },
  {
    "text": "At some point, we get past the point where we've traded off too much performance.",
    "start": "3602730",
    "end": "3611020"
  },
  {
    "text": "And we're making a predictor that's very insensitive, but so insensitive that it doesn't",
    "start": "3611020",
    "end": "3616450"
  },
  {
    "text": "bother looking at the data and doesn't bother making a good prediction.",
    "start": "3616450",
    "end": "3620119"
  },
  {
    "text": "This is the benefit, this is why we do regularization right here in this plot,",
    "start": "3626190",
    "end": "3632440"
  },
  {
    "text": "is for this dip there, which is where we see the regularized ERM",
    "start": "3632440",
    "end": "3640959"
  },
  {
    "text": "doing better than the un-regularized ERM over here.",
    "start": "3640959",
    "end": "3646580"
  },
  {
    "text": "On the validation data, which is unseen data which wasn't used to generate the predictor.",
    "start": "3647880",
    "end": "3655670"
  },
  {
    "text": "And as a result, we might say, pick a value for Theta.",
    "start": "3655710",
    "end": "3664315"
  },
  {
    "text": "Now we could say, let's pick a value for Theta at exactly this minimum.",
    "start": "3664315",
    "end": "3669320"
  },
  {
    "text": "Some people will do that, although actually you're often better off by predicting a value of Theta,",
    "start": "3669330",
    "end": "3677515"
  },
  {
    "text": "picking a value of Theta, which is a little bit to the right. And a little bit more insensitive than the minimizer.",
    "start": "3677515",
    "end": "3687170"
  },
  {
    "text": "As a result, we get a little bit extra insensitivity. Here we're only just testing on one validation set data set.",
    "start": "3689430",
    "end": "3697390"
  },
  {
    "text": "We might like to be a little bit sure that it's going to generalize when we see multiple unseen data sets.",
    "start": "3697390",
    "end": "3705055"
  },
  {
    "text": "And so we put a little bit of extra on sensitivity in it.",
    "start": "3705055",
    "end": "3709250"
  },
  {
    "text": "So we might choose Lambda to be 0.3 or even 1 for this data set.",
    "start": "3710520",
    "end": "3717275"
  },
  {
    "text": "And here we see regularization. It's improved the performance not by a great deal in this example,",
    "start": "3717275",
    "end": "3724735"
  },
  {
    "text": "it went from 0.63 down to 0.58 or something like that. Sometimes the benefits are much more dramatic.",
    "start": "3724735",
    "end": "3734034"
  },
  {
    "text": "And that depends on the characteristics of the data and how much data you",
    "start": "3734034",
    "end": "3741579"
  },
  {
    "text": "have and whether your particular structure of your predictor is prone to over fitting that data.",
    "start": "3741580",
    "end": "3749809"
  },
  {
    "text": "And some say sometimes regularization is massively important and an unregularized predictor and ERM predictor will not work at all,",
    "start": "3749880",
    "end": "3762535"
  },
  {
    "text": "whereas a regularized predictor can work very well.",
    "start": "3762535",
    "end": "3766309"
  },
  {
    "text": "Let's summarize, empirical risk is a function of the parameter Theta that measures the fit on the training data set.",
    "start": "3773870",
    "end": "3782515"
  },
  {
    "text": "It is often but not always the same as the performance metric. ERM chooses theta to minimize empirical risk on the training data set.",
    "start": "3782515",
    "end": "3794779"
  },
  {
    "text": "Regularized ERM doesn't. Regularized ERM chooses theta as a trade off between two different objectives.",
    "start": "3795030",
    "end": "3805435"
  },
  {
    "text": "The first being small empirical risk. Are you a good fit on the training data?",
    "start": "3805435",
    "end": "3810670"
  },
  {
    "text": "And the second being predictor insensitivity. We choose the loss function and the regularizer function",
    "start": "3810670",
    "end": "3821904"
  },
  {
    "text": "by validation, using our performance metric. We choose Lambda by validation.",
    "start": "3821905",
    "end": "3830890"
  },
  {
    "text": "Now when we have quadratic loss- quadratic regularization and the linear predictor,",
    "start": "3830890",
    "end": "3838014"
  },
  {
    "text": "we can find the optimal parameters using least squares. And that's either called least-squares regression when it's unregularized,",
    "start": "3838014",
    "end": "3846820"
  },
  {
    "text": "or it's called ridge regression when it's regularized.",
    "start": "3846820",
    "end": "3852385"
  },
  {
    "text": "In general for ERM, when we don't have quadratic loss,",
    "start": "3852385",
    "end": "3857890"
  },
  {
    "text": "we don't have quadratic regularizers or we've got complicated predictors, we have to use numerical optimization.",
    "start": "3857890",
    "end": "3864085"
  },
  {
    "text": "We're gonna cover this in detail later in the course.",
    "start": "3864085",
    "end": "3868640"
  }
]