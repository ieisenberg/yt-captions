[
  {
    "start": "0",
    "end": "5580"
  },
  {
    "text": "OK, hi everyone. Welcome back, we're now\npast the halfway point.",
    "start": "5580",
    "end": "10910"
  },
  {
    "text": "Week 6 of CS224N. ",
    "start": "10910",
    "end": "16039"
  },
  {
    "text": "And so let me just give a couple\nof quick announcements first. So today is the day\nthat you have to have",
    "start": "16040",
    "end": "23750"
  },
  {
    "text": "done the mid-quarter survey by. Hundreds of people have,\nbut if you haven't, this",
    "start": "23750",
    "end": "30320"
  },
  {
    "text": "is your last chance to get\nthe half-point for that. Today is also the day that\nfinal project proposals are due.",
    "start": "30320",
    "end": "39320"
  },
  {
    "text": "We really encourage you\nto try and hand them in on-time or nearly on-time. That's really just to help you.",
    "start": "39320",
    "end": "46020"
  },
  {
    "text": "So we can more quickly\ngive you feedback on final project proposals. And in the background then,\nthere's also assignment 5.",
    "start": "46020",
    "end": "53960"
  },
  {
    "text": "You'll all have seen the message\nthat we're giving you one extra day for that,\nbut we do certainly",
    "start": "53960",
    "end": "59360"
  },
  {
    "text": "encourage you to be hard at work\non assignment 5 at this point. Hopefully it's a great,\nexciting opportunity",
    "start": "59360",
    "end": "65840"
  },
  {
    "text": "to be learning all the latest\nstuff about transformers. And then today, delighted to\nhave our first invited speaker.",
    "start": "65840",
    "end": "75920"
  },
  {
    "text": "Let me just mention\nthat going along within [INAUDIBLE] half-point\nof participation credit",
    "start": "75920",
    "end": "83840"
  },
  {
    "text": "is you guys writing a\nreaction paragraph talking about something that\nthe speaker talks about.",
    "start": "83840",
    "end": "91190"
  },
  {
    "text": "And there are instructions\nfor that on Ed. But without further ado,\nlet me introduce Danqi Chen.",
    "start": "91190",
    "end": "100370"
  },
  {
    "text": "So Danqi is one of the foremost\nresearchers in question answering, and\nshe's particularly",
    "start": "100370",
    "end": "105590"
  },
  {
    "text": "well known in recent\nwork for being one of the co-authors of the\nRoberta paper, the SpanBERT",
    "start": "105590",
    "end": "113000"
  },
  {
    "text": "paper, and on using\ndense passage retrieval methods for open domain\nquestion answering,",
    "start": "113000",
    "end": "119869"
  },
  {
    "text": "and is the professor at\nthe Princeton University. But as one other comment,\nDanqi once upon a time",
    "start": "119870",
    "end": "128869"
  },
  {
    "text": "was the head TA of CS224N. So she's quite familiar with\nthe context of this class.",
    "start": "128870",
    "end": "137130"
  },
  {
    "text": "So I'm really delighted\nto have Danqi here to give this lecture\non question answering. Thanks.",
    "start": "137130",
    "end": "144129"
  },
  {
    "text": "Thank you Chris, for\nthe introduction. For me it's a great\nopportunity for me",
    "start": "144130",
    "end": "149290"
  },
  {
    "text": "to come back to CS224N today\nand to give this lecture. albeit virtually.",
    "start": "149290",
    "end": "155200"
  },
  {
    "text": "So question\nanswering is the area that I have been working quite\na bit in the last few years. So today I'm very happy\nto introduce to you some",
    "start": "155200",
    "end": "161630"
  },
  {
    "text": "of the fundamentals\nin this field, as well as on cutting edge\nand state of the art topics.",
    "start": "161630",
    "end": "168280"
  },
  {
    "text": "So here's my plan\nfor this lecture. So first, I would give\na brief introduction",
    "start": "168280",
    "end": "174159"
  },
  {
    "text": "of what is question answering,\nand what kind of problems that people are studying today. Then I'm going to spend the\nmost of this lecture focused",
    "start": "174160",
    "end": "182269"
  },
  {
    "text": "on one type of question\nanswering problems called reading comprehension. So it's just the\nbase of the problems",
    "start": "182270",
    "end": "187400"
  },
  {
    "text": "of how we build systems\nto answer questions over a single passage of text.",
    "start": "187400",
    "end": "193090"
  },
  {
    "text": "So I know that many\nof you are going to do a different project on\nthe Stanford Question Answering Dataset.",
    "start": "193090",
    "end": "198489"
  },
  {
    "text": "So understanding this\npart will be very crucial for your final project. So at the end of\nthis lecture, I'm",
    "start": "198490",
    "end": "205120"
  },
  {
    "text": "hoping to spend hopefully\nlike 10-ish minutes to talk about a more\npractical, and in my opinion,",
    "start": "205120",
    "end": "211120"
  },
  {
    "text": "more exciting problem called\nopen domain question answering. So basically try\nto answer questions",
    "start": "211120",
    "end": "216129"
  },
  {
    "text": "over a very large\ncollection of the documents. And so my point is to\ntry to quickly go over",
    "start": "216130",
    "end": "222069"
  },
  {
    "text": "some of those state of the\nart measures in this area. ",
    "start": "222070",
    "end": "227099"
  },
  {
    "text": "OK so let's just get started. So first, what is the\nquestion answering?",
    "start": "227100",
    "end": "233130"
  },
  {
    "text": "So it's the goal of\nquestion answering is to build systems\nthat can automatically answer questions posed by\nhumans in our natural language.",
    "start": "233130",
    "end": "241955"
  },
  {
    "text": "Question answering, or,\nlet's say QA in short, is one of the\nearliest NLP tasks,",
    "start": "241955",
    "end": "247780"
  },
  {
    "text": "and the early systems can\neven date back to the 1960s. So here is one example\nof one of the early QA",
    "start": "247780",
    "end": "257560"
  },
  {
    "text": "systems back to 1964. So as you can see,\nthis system is trying to answer a question\nlike, \"What do worms eat?\"",
    "start": "257560",
    "end": "265735"
  },
  {
    "text": "And they'll finally retrieve\nthe answer, that's the grass. So this system is built to\ntry to find some kind of text",
    "start": "265735",
    "end": "273130"
  },
  {
    "text": "matching between the question,\nand some kind of text segments, and then by using some kind\nof dependency analysis.",
    "start": "273130",
    "end": "279520"
  },
  {
    "text": "I'll assume that\nyou have already learned the dependence\nparsing in this class.",
    "start": "279520",
    "end": "285449"
  },
  {
    "text": "And there are many\ndifferent types of question answering problems. And we also have categories of\nthese question answer problems,",
    "start": "285450",
    "end": "292760"
  },
  {
    "text": "based on the information source,\nor the type of the questions, or the type of answers.",
    "start": "292760",
    "end": "299069"
  },
  {
    "text": "So for the\ninformation source, we have built a system that\ncan put like conditions",
    "start": "299070",
    "end": "304360"
  },
  {
    "text": "of short passage of text,\nor very large collection of documents, or\nlike a structure,",
    "start": "304360",
    "end": "310880"
  },
  {
    "text": "they have structured knowledge\nbases, or even tables or images.",
    "start": "310880",
    "end": "316710"
  },
  {
    "text": "So for the question\npart, we can also build systems that can\nanswer factoid questions or non-factoid questions.",
    "start": "316710",
    "end": "322820"
  },
  {
    "text": "Our open-domain questions\nor closed-domain questions are simple questions versus\nmore complex or compositional",
    "start": "322820",
    "end": "329450"
  },
  {
    "text": "questions. And for the answer\ntype, it can also be a short segment in the text,\nor a paragraph, or a document,",
    "start": "329450",
    "end": "337430"
  },
  {
    "text": "or a list or even the\nyes or no questions. So just have in mind\nthere's many different types",
    "start": "337430",
    "end": "342860"
  },
  {
    "text": "of questions in problems\nand all these problems may require very different\ntechniques or different data,",
    "start": "342860",
    "end": "348919"
  },
  {
    "text": "or even different\nevaluation metrics to evaluate all these\ndifferent problems. ",
    "start": "348920",
    "end": "356020"
  },
  {
    "text": "And, the question and\nanswer has enabled a lot of really useful\nreal world applications.",
    "start": "356020",
    "end": "361750"
  },
  {
    "text": "For example, today if you\njust put your question in a search engine like Google.",
    "start": "361750",
    "end": "367120"
  },
  {
    "text": "So for example, you can\nput your question like, where is the deepest\nlake in the world. So we can see that\nit's the kind of system",
    "start": "367120",
    "end": "373670"
  },
  {
    "text": "that it can find a short\nsnippet of text, including",
    "start": "373670",
    "end": "379690"
  },
  {
    "text": "Lake Baikal in Siberia holds\nthe distinction of being both",
    "start": "379690",
    "end": "386200"
  },
  {
    "text": "the deepest lake in the world\nand the largest fresh water lake, blah blah. And then you can actually\nclick on the correct answer,",
    "start": "386200",
    "end": "394370"
  },
  {
    "text": "which is actually our\nconcise answer, which should be Siberia.",
    "start": "394370",
    "end": "399740"
  },
  {
    "text": "And those kind of\nsystems are also able to handle more\ncomplex questions like how-to questions.",
    "start": "399740",
    "end": "405770"
  },
  {
    "text": "I guess this is the probable\nquestion that everyone currently cares about. So the question is, how can I\nprotect myself from COVID-19?",
    "start": "405770",
    "end": "414320"
  },
  {
    "text": "So there isn't really a\nsimple and short answer to this question. So you can see that\nthe system actually",
    "start": "414320",
    "end": "419920"
  },
  {
    "text": "returns a very long\nparagraph, including the best way to prevent\nillness is to avoid",
    "start": "419920",
    "end": "425000"
  },
  {
    "text": "being exposed to this virus. And to help prevent\nthe spread of COVID-19, you can do the following.",
    "start": "425000",
    "end": "432956"
  },
  {
    "text": "So actually this\nparagraph is actually a summary from CDC article.",
    "start": "432957",
    "end": "438240"
  },
  {
    "text": "If you just click this link\nand read through the article. So this is also one type of the\nquestion answering problems.",
    "start": "438240",
    "end": "446169"
  },
  {
    "text": "Now, this is our\nsurvey of the use cases for the current digital\nassistants such as Alexa or Google Home.",
    "start": "446170",
    "end": "452840"
  },
  {
    "text": "So according to the survey\nresults in January 2020, which is one year ago, so\nyou can see that as of--",
    "start": "452840",
    "end": "460310"
  },
  {
    "text": "people actually really\nlike to ask questions on these digital assistants.",
    "start": "460310",
    "end": "465500"
  },
  {
    "text": "So you can see that ask\na question is actually the second most used case. Only ranks after listening\nto music and before the check",
    "start": "465500",
    "end": "472675"
  },
  {
    "text": "the weather and set up timer. So question answering\nhas been really useful in digital assistants.",
    "start": "472675",
    "end": "480770"
  },
  {
    "text": "And another very famous example\nof the question answering system is this IBM Watson\nquestion answering system.",
    "start": "480770",
    "end": "486940"
  },
  {
    "text": "So in 19-- in 2011, so\nthis IBM Watson QA system",
    "start": "486940",
    "end": "493030"
  },
  {
    "text": "has been shown to beat\ntwo national Jeopardy champions in answering\nJeopardy questions.",
    "start": "493030",
    "end": "499990"
  },
  {
    "text": "So this is a part of a\nhistorical event, at least in the NLP history.",
    "start": "499990",
    "end": "507230"
  },
  {
    "text": "So if you look at the inner\nworking of this kind of system more closely, so you can\nsee that it is actually",
    "start": "507230",
    "end": "512990"
  },
  {
    "text": "a very complicated and\nhighly modularized system. So the system is built on\nboth the unstructured text",
    "start": "512990",
    "end": "520399"
  },
  {
    "text": "and also the structured data. So if I look at the system both\nfrom the left to the right,",
    "start": "520400",
    "end": "527220"
  },
  {
    "text": "you can see that\nthis system consists of the four stages,\nincluding the question processing, the candidate answer\ngeneration and the candidate",
    "start": "527220",
    "end": "535280"
  },
  {
    "text": "answer scoring, and the\nconfidence merging and ranking. And then if you\nlook at each stage, you can see that there are many\ndifferent NLP techniques that",
    "start": "535280",
    "end": "542910"
  },
  {
    "text": "have been actually\nincluded in this complex QA system, including the question\nclassification, parsing,",
    "start": "542910",
    "end": "549350"
  },
  {
    "text": "relation extraction,\nco-reference. So it's actually, there are\nreally lots of NLP modules",
    "start": "549350",
    "end": "554570"
  },
  {
    "text": "that have included. And now this system has been\nover 10 years older, actually, exactly 10 years now.",
    "start": "554570",
    "end": "560720"
  },
  {
    "text": "And this is actually\nrepresentative of state of the art 10\nyears ago at that time. ",
    "start": "560720",
    "end": "568280"
  },
  {
    "text": "So we know that this class\nis about deep learning. So today different\nhas completely",
    "start": "568280",
    "end": "573550"
  },
  {
    "text": "really transformed the landscape\nof the question answering systems. So there's no doubt\nthat we can say",
    "start": "573550",
    "end": "579670"
  },
  {
    "text": "that almost all of the\nstates are question answering systems today built\non top of the end",
    "start": "579670",
    "end": "585154"
  },
  {
    "text": "to end training of the\ndeep neural networks and the pre-trained language\nmodels such as BERT.",
    "start": "585155",
    "end": "590510"
  },
  {
    "text": "So today in this\nlecture, we're also going to know a lot of the\ndeep learning models used for questions answering.",
    "start": "590510",
    "end": "596230"
  },
  {
    "text": "And this statement is probably\nalso true for almost all of the NLP problems\nthat we can see today.",
    "start": "596230",
    "end": "601900"
  },
  {
    "text": "But we also argue that\nquestion answering is probably one of those fields that we\nhave seen the most remarkable",
    "start": "601900",
    "end": "608680"
  },
  {
    "text": "progress in the last couple of\nyears driven by deep learning. ",
    "start": "608680",
    "end": "617180"
  },
  {
    "text": "So in this lecture, I\nwill be mostly focusing",
    "start": "617180",
    "end": "623110"
  },
  {
    "text": "on the text based, or textual\nquestion answering problems. So basically, we are\ntrying to answer questions based on the unstructured text.",
    "start": "623110",
    "end": "629950"
  },
  {
    "text": "So before I jump to\nthat part, I also want to quickly point out that\nthere are many other really big",
    "start": "629950",
    "end": "636880"
  },
  {
    "text": "question answering problems. And each of them can be\nreally like a subfield in NLP",
    "start": "636880",
    "end": "642970"
  },
  {
    "text": "and they actually have\nvery different challenges and also model designs. So one bigger class of\nquestion answering problem",
    "start": "642970",
    "end": "649620"
  },
  {
    "text": "is this knowledge-based\nquestion answering. So basically we wanted\nyou to question build answering systems\nto answer questions",
    "start": "649620",
    "end": "655840"
  },
  {
    "text": "that can answer questions\nover a very large database.",
    "start": "655840",
    "end": "661000"
  },
  {
    "text": "So to solve this\nproblem, some approaches need to take this question\nand convert this question",
    "start": "661000",
    "end": "666760"
  },
  {
    "text": "into some kind of logic forms. And these kind of\nlogic forms can be executed against\nthis database",
    "start": "666760",
    "end": "673000"
  },
  {
    "text": "to give you the final answer.  And another class, bigger\nclass of the question answering",
    "start": "673000",
    "end": "679980"
  },
  {
    "text": "problems is called visual\nquestion answering. So basically you need to answer\nquestions based on the images.",
    "start": "679980",
    "end": "685579"
  },
  {
    "text": "So this problem\nbasically requires both understanding of the\nquestions and also images, and is actually a very active\nfield between the computer",
    "start": "685580",
    "end": "693550"
  },
  {
    "text": "vision and NLP. So if you have interest\nin these type of problems, I encourage you to check\nout those problems,",
    "start": "693550",
    "end": "699550"
  },
  {
    "text": "but I'm not going to dig\ninto these problems today. ",
    "start": "699550",
    "end": "705460"
  },
  {
    "text": "OK. So next, I'm going to\nstart with a part 2, reading comprehension. I just want to\nquickly check if there",
    "start": "705460",
    "end": "711940"
  },
  {
    "text": "are any quick\nquestions I can answer before I start us on part 2.",
    "start": "711940",
    "end": "718001"
  },
  {
    "text": "No, I think we're good for now. OK. So let's talk about the\nreading comprehension then.",
    "start": "718002",
    "end": "725360"
  },
  {
    "text": "So reading comprehension\nis a basic problem that we want to comprehend\na passage of text and answer",
    "start": "725360",
    "end": "731420"
  },
  {
    "text": "questions about the content. So it's an input of\nthis, probably basically a passage of text or\nquestion, and the goal",
    "start": "731420",
    "end": "737455"
  },
  {
    "text": "is to return the\nanswers that actually can answer this question. So here is one example.",
    "start": "737455",
    "end": "744550"
  },
  {
    "text": "So here is a passage of text. And we want to\nanswer a question. The question is what\nlanguage did Tesla",
    "start": "744550",
    "end": "750620"
  },
  {
    "text": "study while in school? OK. So I'm going to pause\nlike 5 or 10 seconds",
    "start": "750620",
    "end": "756250"
  },
  {
    "text": "and see if people can find the\nanswer to this question based on this passage. ",
    "start": "756250",
    "end": "774759"
  },
  {
    "text": "Any guess?  German.",
    "start": "774760",
    "end": "779890"
  },
  {
    "text": "People say it's German. Yeah, German is correct. So the answer should be German.",
    "start": "779890",
    "end": "785430"
  },
  {
    "text": "So basically to\nanswer this question, so you need to find this\nsentence, like, in 1861, Tesla attended this school where\nhe studied German, arithmetic,",
    "start": "785430",
    "end": "793950"
  },
  {
    "text": "and religion, and only\nGerman is a language. So the answer to this\nquestion should be German.",
    "start": "793950",
    "end": "800220"
  },
  {
    "text": "OK, here is another example, OK? Another passage of text. And the question is, which\nlinguistic minority is larger,",
    "start": "800220",
    "end": "807720"
  },
  {
    "text": "Hindi or Malayalam I think",
    "start": "807720",
    "end": "816879"
  },
  {
    "text": "5 seconds. ",
    "start": "816880",
    "end": "829640"
  },
  {
    "text": "OK. So the answer to this\nquestion should be Hindi. So this probably is not a\nvery hard question for humans,",
    "start": "829640",
    "end": "836080"
  },
  {
    "text": "it's actually a pretty\nhard question for machines because to get this\nquestion correctly. So the machines basically need\nto understand for the Hindi.",
    "start": "836080",
    "end": "843700"
  },
  {
    "text": "Like 3.3% of the population\nspeaks Hindi and only like 1.27% speaks Malayalam,\nthis language.",
    "start": "843700",
    "end": "852430"
  },
  {
    "text": "And then also compared these\ntwo numbers in the final case. So 3.3% is a bigger number,\nso the answer should",
    "start": "852430",
    "end": "860190"
  },
  {
    "text": "be Hindi to this question. OK. So next I'm going to\ntalk a little bit so why",
    "start": "860190",
    "end": "866350"
  },
  {
    "text": "do we care about this problem? So why do we care about the\nreading comprehension problem? So besides that it has\nactually many useful real-world",
    "start": "866350",
    "end": "874750"
  },
  {
    "text": "practical applications-- so as I've already showed us\nsome examples at the beginning,",
    "start": "874750",
    "end": "880240"
  },
  {
    "text": "I think there are also\ntwo other reasons. ",
    "start": "880240",
    "end": "885472"
  },
  {
    "text": "So the first reason, besides\napplications, the first reason is-- so reading comprehension\nhas been also viewed",
    "start": "885472",
    "end": "891139"
  },
  {
    "text": "as a very important\ntest bed for evaluating how well computer systems\nunderstand human language.",
    "start": "891140",
    "end": "898110"
  },
  {
    "text": "So this is really just similar\nto how we humans actually test the reading\ncomprehension test to evaluate",
    "start": "898110",
    "end": "905160"
  },
  {
    "text": "how well we actually\nunderstand one language. So this is also the\nway that we actually post questions to test\nthe machine's language",
    "start": "905160",
    "end": "913370"
  },
  {
    "text": "understanding ability. So it actually has been\nformally stated back in 1977",
    "start": "913370",
    "end": "920660"
  },
  {
    "text": "by Wendy Lehnert in\nher dissertation. She says that,\nsince questions can",
    "start": "920660",
    "end": "927800"
  },
  {
    "text": "be devised to query any\naspect of text comprehension, the ability to\nanswer questions is",
    "start": "927800",
    "end": "932990"
  },
  {
    "text": "the strongest possible\ndemonstration of understanding. So that's why\nreading comprehension can be a very important test\nbed because we can design very",
    "start": "932990",
    "end": "941410"
  },
  {
    "text": "complex questions to testbed. And also I think there is\nanother interesting and",
    "start": "941410",
    "end": "948330"
  },
  {
    "text": "important reason that reading\ncomprehension is important. So in recent few\nyears, some researchers",
    "start": "948330",
    "end": "955160"
  },
  {
    "text": "actually found that, OK, well,\nthere are many other NLP tasks. So we also reduce them to a\nreading comprehension problem.",
    "start": "955160",
    "end": "962290"
  },
  {
    "text": "So I'm going to give\nyou two examples. So one example is the\ninformation extraction.",
    "start": "962290",
    "end": "968220"
  },
  {
    "text": "So basically if we want to-- so to answer about a personal\nsubject, Barack Obama.",
    "start": "968220",
    "end": "974130"
  },
  {
    "text": "key to our relation,\neducated at. So we want to fill in what is-- fill in this question\nmark and figure out, OK,",
    "start": "974130",
    "end": "980640"
  },
  {
    "text": "where Barack Obama\nwas educated at. So one way to solve this\nproblem is basically",
    "start": "980640",
    "end": "985890"
  },
  {
    "text": "trying to convert this\nrelation into a question. So where did Barack\nObama graduate from?",
    "start": "985890",
    "end": "992340"
  },
  {
    "text": "And taking all of that\nrelevant piece of text and then by applying a\nreading comprehension problem.",
    "start": "992340",
    "end": "998070"
  },
  {
    "text": "Then basically,\nwe can find out-- the correct answer should\nbe Columbia University.",
    "start": "998070",
    "end": "1003290"
  },
  {
    "text": "That is also the output of this\ninformation extraction system.",
    "start": "1003290",
    "end": "1008420"
  },
  {
    "text": "And another example\nis actually called a semantic role labeling. I'm not sure you have learned\nthis in the past year,",
    "start": "1008420",
    "end": "1013610"
  },
  {
    "text": "probably not. But this is a class of\nthose-- semantic role labeling is trying\nto take one sentence",
    "start": "1013610",
    "end": "1020810"
  },
  {
    "text": "and then trying to identify\nthe roles for different words, at least for words in this\ncase, in one sentence.",
    "start": "1020810",
    "end": "1027240"
  },
  {
    "text": "So basically try to-- given one sentence,\ngiven one word, \"finish\" trying to figure\nout who did what to whom",
    "start": "1027240",
    "end": "1033869"
  },
  {
    "text": "and when and where. So by trying to-- so the goal is to try to\nfigure out all these roles",
    "start": "1033869",
    "end": "1042550"
  },
  {
    "text": "with respect to the words. So one way to solve\nthis problem is also by converting all these\ndifferent roles into questions,",
    "start": "1042550",
    "end": "1050080"
  },
  {
    "text": "such as who finished something,\nwhat did someone finish, and what did someone\nfinish something as.",
    "start": "1050080",
    "end": "1056710"
  },
  {
    "text": "So by converting all these kind\nof semantic role relations,",
    "start": "1056710",
    "end": "1062020"
  },
  {
    "text": "we can also just apply the\nreading comprehension problem and give you the correct answer. So this is actually a very\ninteresting perspective,",
    "start": "1062020",
    "end": "1069110"
  },
  {
    "text": "that reading comprehension can\nbe actually very universally useful to many other tasks.",
    "start": "1069110",
    "end": "1076600"
  },
  {
    "text": "So next, I'm going to introduce\nthis Stanford Question Answering Dataset called SQuAD.",
    "start": "1076600",
    "end": "1082260"
  },
  {
    "text": "So if you are going to develop\nfor the final projects, you will need to\nuse this dataset.",
    "start": "1082260",
    "end": "1088049"
  },
  {
    "text": "So Stanford Question\nAnswering Dataset is actually a supervised\nreading comprehension dataset,",
    "start": "1088050",
    "end": "1093509"
  },
  {
    "text": "which consists of 100K annotated\npassage question and answer",
    "start": "1093510",
    "end": "1099000"
  },
  {
    "text": "triples. So here is one example\nfrom this data set. ",
    "start": "1099000",
    "end": "1105230"
  },
  {
    "text": "And I just want to say that\nalso one important thing to have in mind is that-- so this data set consists\nof 100K annotated examples.",
    "start": "1105230",
    "end": "1114830"
  },
  {
    "text": "And this kind of a\nlarge scale supervised dataset are also\nvery key ingredient for training the effective\nneural models for reading",
    "start": "1114830",
    "end": "1122360"
  },
  {
    "text": "comprehension. So after the SQuAD dataset,\nmany other later data sets have been also\ncollected, basically",
    "start": "1122360",
    "end": "1129230"
  },
  {
    "text": "runs this size around 100K. So 100K is actually\nvery important",
    "start": "1129230",
    "end": "1134450"
  },
  {
    "text": "to train these neural models. So for these datasets-- so the passages is like a\nsingle paragraph selected",
    "start": "1134450",
    "end": "1144139"
  },
  {
    "text": "from the English\nWikipedia, which usually consists of 100 to 150 words. And the questions are\ncrowd-sourced, basically",
    "start": "1144140",
    "end": "1151060"
  },
  {
    "text": "like from Mechanical Turking. And this is a very important\nproperty of the dataset,",
    "start": "1151060",
    "end": "1156710"
  },
  {
    "text": "is that each answer is\na short segment of text, or we called it\nspan in the passage.",
    "start": "1156710",
    "end": "1163070"
  },
  {
    "text": "So as you can see\nfrom this example-- so here are three\ndifferent questions. And each of these answers\ncan be actually found off",
    "start": "1163070",
    "end": "1170990"
  },
  {
    "text": "on a short segment\ntext in the passage. So this is actually an\ninteresting property",
    "start": "1170990",
    "end": "1176586"
  },
  {
    "text": "and it's also an important\nproperty of this data set. But also just to caveat, that\nthis is also a limitation.",
    "start": "1176586",
    "end": "1185030"
  },
  {
    "text": "Because not all the questions\nhave the answers in this way. So only the questions that\nyou have found the answers",
    "start": "1185030",
    "end": "1192170"
  },
  {
    "text": "as a span in the\npassage can actually be included in this\ndataset basically. ",
    "start": "1192170",
    "end": "1200138"
  },
  {
    "text": "But today, so this dataset,\nI forgot to say that. This dataset was collected in\n2016 by several researchers",
    "start": "1200138",
    "end": "1207293"
  },
  {
    "text": "at Stanford, so it's called\nStanford Question Answering Dataset. Today, after four\nor five years now,",
    "start": "1207293",
    "end": "1213180"
  },
  {
    "text": "so SQuAD still remains the most\npopular reading comprehension data set. So it's actually very clearly\na high quality dataset,",
    "start": "1213180",
    "end": "1220610"
  },
  {
    "text": "but is also not a very\ndifficult dataset. So today, basically the SQuAD\ndataset itself has been almost",
    "start": "1220610",
    "end": "1226400"
  },
  {
    "text": "solved, and the state-of-the-art\nalready exceeds estimated human performance. ",
    "start": "1226400",
    "end": "1234360"
  },
  {
    "text": "And I also want\nto quickly mention the evaluation for this Stanford\nQuestion Answering Dataset.",
    "start": "1234360",
    "end": "1240650"
  },
  {
    "text": "So there are basically\ntwo evaluation metrics to evaluate how well the\nsystem can do on this dataset.",
    "start": "1240650",
    "end": "1246140"
  },
  {
    "text": "The two metrics are the\nexact match and the F1 score. So exact match is basically\njust a binary indicator, 0 or 1.",
    "start": "1246140",
    "end": "1253909"
  },
  {
    "text": "Basic measures can\nactually be exactly matched to the gold answer. And the F1 score\nbasically can measure",
    "start": "1253910",
    "end": "1260260"
  },
  {
    "text": "kind of some partial credit,\nenough to do the evaluation. So basically for the\ndevelopment and testing set,",
    "start": "1260260",
    "end": "1267289"
  },
  {
    "text": "there will be three\ngold answers collected because for some\nquestions, there might be",
    "start": "1267290",
    "end": "1272840"
  },
  {
    "text": "not just one unique\nanswer, there could be multiple\nplausible answers. And the evaluation\nmatrix basically takes",
    "start": "1272840",
    "end": "1279110"
  },
  {
    "text": "the predicted answer and then\ncompares the predicted answer to each gold answer\nwith some articles",
    "start": "1279110",
    "end": "1286910"
  },
  {
    "text": "and also the\npunctuation excluded. And basically you can\ncompare the exact match score",
    "start": "1286910",
    "end": "1292340"
  },
  {
    "text": "and also F1 score by\ncomparing the predicted answer to the gold answer.",
    "start": "1292340",
    "end": "1298080"
  },
  {
    "text": "And then finally you\ntake the max scores. And there are many different\nexamples in the dev or test",
    "start": "1298080",
    "end": "1303690"
  },
  {
    "text": "set. And then finally, we\njust take the average of all the examples for both the\nexact match and the reference",
    "start": "1303690",
    "end": "1309070"
  },
  {
    "text": "score. So by using this\nevaluation metric, the estimated human performance\nestimated by the researchers",
    "start": "1309070",
    "end": "1320480"
  },
  {
    "text": "at the time is-- the exact match score is 82.3%\nand the F1 score is 91.2.",
    "start": "1320480",
    "end": "1328580"
  },
  {
    "text": "So here is just a quick example. So here's the question what\ndid Tesla do in December 1878?",
    "start": "1328580",
    "end": "1336740"
  },
  {
    "text": "And there's three\npossible answers. So you can see that\nthe first two answers are the same, left Graz.",
    "start": "1336740",
    "end": "1343550"
  },
  {
    "text": "And then the second answer\nis left Graz and severed, I think there's a typo here,\nall relations with his family.",
    "start": "1343550",
    "end": "1353090"
  },
  {
    "text": "And then you find that\nprediction is span, which is left Graz and severed.",
    "start": "1353090",
    "end": "1358760"
  },
  {
    "text": "So we can see that\nthere is an exact match score between the\npredicted answer",
    "start": "1358760",
    "end": "1364040"
  },
  {
    "text": "and any of the gold answers. So the exact match would be 0. And the F1 score would\nbe taking the max.",
    "start": "1364040",
    "end": "1370340"
  },
  {
    "text": "I'm not going to talk about\nhow these are computed, so I suggest you check\nout the original paper.",
    "start": "1370340",
    "end": "1376110"
  },
  {
    "text": "So by computing these\nscores and taking the max, the final F1 score\nwill be 0.67, which",
    "start": "1376110",
    "end": "1383179"
  },
  {
    "text": "is F1 score for this predicted\nanswer on this dataset.",
    "start": "1383180",
    "end": "1389170"
  },
  {
    "text": "So Danqi, one question\nyou might answer is, so if you can do other tasks\nlike named entity recognition",
    "start": "1389170",
    "end": "1396190"
  },
  {
    "text": "or relation extraction\nby sticking something on top of BERT and\nfine tuning for it",
    "start": "1396190",
    "end": "1402790"
  },
  {
    "text": "or do it as a\nquestion answering, does one or the other method\nwork better and by how much?",
    "start": "1402790",
    "end": "1408550"
  },
  {
    "text": " That's an interesting question. ",
    "start": "1408550",
    "end": "1415320"
  },
  {
    "text": "So I haven't really seen the-- OK. So there has been\nsome claim, OK, that all the tasks\ncan be converted",
    "start": "1415320",
    "end": "1421130"
  },
  {
    "text": "into question answering task. But I'm not sure if there is\nreally a very fair comparison, let's say a yarn and\nentity recognition",
    "start": "1421130",
    "end": "1428100"
  },
  {
    "text": "by really converting that\ninto question answering task. So I don't have\nthe answer to that. So the kind of state-of-the-art\nAER systems still have time",
    "start": "1428100",
    "end": "1436160"
  },
  {
    "text": "to just train or sequence\ntagger on top of the word. So yeah, I don't really have\na precise answer to that.",
    "start": "1436160",
    "end": "1443030"
  },
  {
    "text": " Shall I continue?",
    "start": "1443030",
    "end": "1450160"
  },
  {
    "text": "OK. Yeah.  OK. So next, I'm going\nto talk about how",
    "start": "1450160",
    "end": "1455950"
  },
  {
    "text": "to build neural models\nfor reading comprehension, and in particular, how\nwe can build a model",
    "start": "1455950",
    "end": "1461409"
  },
  {
    "text": "to solve the Stanford\nQuestion Answering Dataset. I also to just quickly\nmention that because there",
    "start": "1461410",
    "end": "1467705"
  },
  {
    "text": "are many different\npapers that actually use different notions to\nrefer to the same thing,",
    "start": "1467705",
    "end": "1473539"
  },
  {
    "text": "so starting from-- so I'm going to use\nthe passage, paragraph, and context, and\nalso question and",
    "start": "1473540",
    "end": "1478900"
  },
  {
    "text": "query basic interchangeably. So they basically\nrefer to the same thing because different papers\nuse also different notions.",
    "start": "1478900",
    "end": "1486200"
  },
  {
    "text": "So I just want to\nquickly mention that. OK. So how can we build a model\nto solve this problem?",
    "start": "1486200",
    "end": "1492350"
  },
  {
    "text": "So let's first\nformulate this problem. So the input of this\nproblem is let's",
    "start": "1492350",
    "end": "1497830"
  },
  {
    "text": "take a context or paragraph. So C, which consists of\nthe N tokens C1 to CN.",
    "start": "1497830",
    "end": "1504250"
  },
  {
    "text": "And also we take\nour question, Q. And the question consists\nof n tokens q1 to qN.",
    "start": "1504250",
    "end": "1510790"
  },
  {
    "text": "So N could be something like\nbetween 100 and 200 for SQuAD.",
    "start": "1510790",
    "end": "1516460"
  },
  {
    "text": "And then N would\nbe much shorter, it would be something\nlike 10 or 15. And because the answer\nhas these constraints,",
    "start": "1516460",
    "end": "1523165"
  },
  {
    "text": "the answer must be a section\nof text in the passage. So the output can be\njust written this way.",
    "start": "1523165",
    "end": "1529960"
  },
  {
    "text": "We are going to predict\na start and end. So start and then end\nwould be within the range",
    "start": "1529960",
    "end": "1536730"
  },
  {
    "text": "between the 1 and the\nN. So it's basically just two checkpoints-- sorry,\ntwo end points of the answer.",
    "start": "1536730",
    "end": "1542110"
  },
  {
    "text": " So SQuAD has been collected\nbeginning late 2016.",
    "start": "1542110",
    "end": "1551200"
  },
  {
    "text": "So after 2016, there\nhave been-- there are two families of neural role\nmodels to solving this SQuAD",
    "start": "1551200",
    "end": "1561440"
  },
  {
    "text": "data set. So the first family\nbasically had done a lot of\nmodels that came out",
    "start": "1561440",
    "end": "1567340"
  },
  {
    "text": "during that period\nbetween 2016 and 2018. So this was a family of models,\nbasically LSTM-based models",
    "start": "1567340",
    "end": "1574230"
  },
  {
    "text": "with attention. So these are just a list\nof the representing models",
    "start": "1574230",
    "end": "1579730"
  },
  {
    "text": "that came out during\nthat period and including some work that I did when I\nwas a PhD student at Stanford.",
    "start": "1579730",
    "end": "1585455"
  },
  {
    "text": " And the second class of\nmodels that I put here is really divided here before\nthe BERT and after BERT.",
    "start": "1585455",
    "end": "1593920"
  },
  {
    "text": "So after BERT came out--\nso almost all these reading comprehension models\nwere built on how",
    "start": "1593920",
    "end": "1600130"
  },
  {
    "text": "to fine tune the BERT models. Not just BERT models,\nfor the BERT-like models. So pre-trained language models\nfor these kind of reading",
    "start": "1600130",
    "end": "1607210"
  },
  {
    "text": "comprehension problems. So here are the two-- ",
    "start": "1607210",
    "end": "1614162"
  },
  {
    "text": "the illustration of these\ntwo families of the models. So on the left is an LSTM-based\nmodels with attention,",
    "start": "1614162",
    "end": "1621799"
  },
  {
    "text": "and on the right\nis the BERT model. And then we need to fine-tunel\nthis model for the reading",
    "start": "1621800",
    "end": "1629690"
  },
  {
    "text": "comprehension task. So I know that for the-- so my plan today is\nfirst try to talk",
    "start": "1629690",
    "end": "1636139"
  },
  {
    "text": "about these LSTM-based models. So I'm going to spend a little\nbit more time on this part",
    "start": "1636140",
    "end": "1641390"
  },
  {
    "text": "because I know that for\nthe default final project, you will need to implement\nthis model from the scratch.",
    "start": "1641390",
    "end": "1647180"
  },
  {
    "text": "So I'm going to walk us through\nhard to build this model step by step, and\nhopefully with that, you'll have a good understanding\nof how this model works.",
    "start": "1647180",
    "end": "1654190"
  },
  {
    "text": "And so I'm just\ngoing to briefly tell about how to build this\nBER-- use the BERT models for the reading comprehension.",
    "start": "1654190",
    "end": "1662090"
  },
  {
    "text": "OK, so before I start talking\nabout these LSTM models, I know that you have\nalready learned the sequence to sequence models\nwith attention",
    "start": "1662090",
    "end": "1668990"
  },
  {
    "text": "for machine translation. So I want to draw some\nconnections between the machine",
    "start": "1668990",
    "end": "1675380"
  },
  {
    "text": "translation problem and the\nreading comprehension problem because they really share\na lot of similarities.",
    "start": "1675380",
    "end": "1681740"
  },
  {
    "text": "So first, in the machine\ntranslation model or this sequence\nto sequence model,",
    "start": "1681740",
    "end": "1687050"
  },
  {
    "text": "there are like a source\nand target sentence. So basically there\nare two sequences.",
    "start": "1687050",
    "end": "1692990"
  },
  {
    "text": "But in our case, in this\nreading comprehension case, we also have two sequences. One is a passage and\nanother is our question.",
    "start": "1692990",
    "end": "1700438"
  },
  {
    "text": "But the length could\nbe slightly imbalanced because a passage could be\nmuch longer than the question. But essentially, you\nalso choose sequences.",
    "start": "1700438",
    "end": "1707216"
  },
  {
    "text": " And so in the reading\ncomprehension, we need to model which\nwords in the passage",
    "start": "1707216",
    "end": "1715149"
  },
  {
    "text": "are most relevant\nto the question. And if they are relevant\nto the question, so these are also\nrelevant to which",
    "start": "1715150",
    "end": "1721880"
  },
  {
    "text": "set of the question words. So this is a very\nkey important thing",
    "start": "1721880",
    "end": "1728669"
  },
  {
    "text": "that we actually need to model. And that is actually very\nsimilar to the machine translation model\nthat we need to model",
    "start": "1728670",
    "end": "1735630"
  },
  {
    "text": "which words in the\nsource sentence that actually are most relevant\nto the current target words.",
    "start": "1735630",
    "end": "1741670"
  },
  {
    "text": "So you can imagine that\nattention would be also really the key ingredient here. Just like some sequence\nto sequence model,",
    "start": "1741670",
    "end": "1748320"
  },
  {
    "text": "we need to model the attention\nbetween the source sentence and the target sentence. We also need to model the\nattention between the passage",
    "start": "1748320",
    "end": "1755850"
  },
  {
    "text": "and the question. So this is actually\nvery similar. So something that's\nactually not very similar is for the sequence\nto sequence model,",
    "start": "1755850",
    "end": "1763620"
  },
  {
    "text": "we need to build a decoder,\nautoregressive decoder, to generate the target\nsentence word by word.",
    "start": "1763620",
    "end": "1769860"
  },
  {
    "text": "But in this reading\ncomprehension problem, we don't need to really\ngenerate anything. So we just take the\npath into question.",
    "start": "1769860",
    "end": "1777180"
  },
  {
    "text": "So at least for\nthe SQuAD data set, we just need to\ntrain two classifiers",
    "start": "1777180",
    "end": "1782340"
  },
  {
    "text": "to predict the start and\nend positions of answers. So that part is\nactually simplified.",
    "start": "1782340",
    "end": "1787980"
  },
  {
    "text": "So we only need to train the\ndecoder to generate the target sentence.",
    "start": "1787980",
    "end": "1793540"
  },
  {
    "text": "OK. So next, I'm going to talk\nabout this model called BiDAF.",
    "start": "1793540",
    "end": "1799410"
  },
  {
    "text": "So it stands for\nBidirectional Attention Flow for Machine Comprehension. So it was proposed by Minjoon\nSeo and other folks in 2017.",
    "start": "1799410",
    "end": "1810100"
  },
  {
    "text": "Before the BERT\ncame out, It remains one of the most popular\nreading comprehension models",
    "start": "1810100",
    "end": "1815580"
  },
  {
    "text": "and achieved a very\ngood performance at that time, at least\non the SQuAD data set. So you can see that this model\nseems to be pretty complicated.",
    "start": "1815580",
    "end": "1824460"
  },
  {
    "text": "But if you look at this model\nfrom the bottom to the top, it actually can be decomposed\ninto many different layers.",
    "start": "1824460",
    "end": "1834460"
  },
  {
    "text": "So next, I'm going to just\ndissect this model layer by layer and talk about what\nthis layer is actually doing",
    "start": "1834460",
    "end": "1841230"
  },
  {
    "text": "and how we can really build\nthis model from the bottom layer to the top layer,\nand the finally we train this model in\nan end to end way.",
    "start": "1841230",
    "end": "1847970"
  },
  {
    "text": " OK. So the first part is actually\nthe bottom three layers",
    "start": "1847970",
    "end": "1856570"
  },
  {
    "text": "called the character embedding\nlayer, word embedding layer, and the phrase embedding layer. So I just put them\ntogether and called this",
    "start": "1856570",
    "end": "1862820"
  },
  {
    "text": "as an encoding function. So the idea here\nis that OK, let's take the context part of\nthe passage in question.",
    "start": "1862820",
    "end": "1869330"
  },
  {
    "text": "We need to encode\nthem separately.  So to do this, so\nthis model basically",
    "start": "1869330",
    "end": "1875850"
  },
  {
    "text": "proposed to use a concatenation\nof the word embedding as well as the\ncharacter embedding",
    "start": "1875850",
    "end": "1882320"
  },
  {
    "text": "for each word in the\ncontext and the query. So the word embedding\nis straightforward.",
    "start": "1882320",
    "end": "1887820"
  },
  {
    "text": "So if you have the\nword embedding, you can just look at\nthe word for the-- just a word like \"Seattle,\"\njust use the GloVe embedding",
    "start": "1887820",
    "end": "1895730"
  },
  {
    "text": "as a representation\nfor this word. And for the character\nembedding part, so you basically\nneed to represent",
    "start": "1895730",
    "end": "1902540"
  },
  {
    "text": "each character in this\nword, like Seattle, and bypass this to a\nconvolutional neural network",
    "start": "1902540",
    "end": "1909260"
  },
  {
    "text": "with some kind of max\npooling operations. And then finally,\nyou can just get one representation at the top. And then you just concatenate\nthe word embedding",
    "start": "1909260",
    "end": "1917370"
  },
  {
    "text": "and the character embedding. So these character\nembeddings have been shown to improve the\nrepresentation for the unseen",
    "start": "1917370",
    "end": "1923720"
  },
  {
    "text": "or the real words. So mathematically, we can\nsee that for each word",
    "start": "1923720",
    "end": "1930080"
  },
  {
    "text": "in the context of the\nquery, we can just represent as GloVes with the\nembedding and the character",
    "start": "1930080",
    "end": "1937050"
  },
  {
    "text": "embedding. And then we just concatenate\nthem and pass this to a highway network. So I didn't write\nthe function here.",
    "start": "1937050",
    "end": "1944480"
  },
  {
    "text": "So you can just look\nat the original paper. And the second part. So after we encode the\nindividual word, so next,",
    "start": "1944480",
    "end": "1953660"
  },
  {
    "text": "we are going to pass\nthis word embedding into two separate\nbidirectional LSTMs",
    "start": "1953660",
    "end": "1960170"
  },
  {
    "text": "separately to produce these\ncontextualized embeddings for both the context\nand the query.",
    "start": "1960170",
    "end": "1965880"
  },
  {
    "text": "So let's look at this equation. So we take the\nrepresentation of this word.",
    "start": "1965880",
    "end": "1971630"
  },
  {
    "text": "And then we just--\nbasically, this is one LSTM model\nfrom one direction",
    "start": "1971630",
    "end": "1976700"
  },
  {
    "text": "and this is a LSTM model\nfrom another direction. So we just need to concatenate\nthe two hidden representation",
    "start": "1976700",
    "end": "1984070"
  },
  {
    "text": "in two directions. And finally, we can get a\ncontextualized representation",
    "start": "1984070",
    "end": "1989460"
  },
  {
    "text": "for each single\nword in the context. And we can do a similar\nthing for the question",
    "start": "1989460",
    "end": "1995060"
  },
  {
    "text": "representation. I also want to quickly\nmention, because I mentioned the sequence to sequence model.",
    "start": "1995060",
    "end": "2000340"
  },
  {
    "text": "So sequence to sequence model,\nwe cannot really do these bidirectional LSTMs for the two\nsequences because the decoder",
    "start": "2000340",
    "end": "2007390"
  },
  {
    "text": "is an autoregressive model. So that's why the decoder\nis usually just implemented",
    "start": "2007390",
    "end": "2012740"
  },
  {
    "text": "as a unidirectional LSTM. But because here we don't really\ncare about the generation,",
    "start": "2012740",
    "end": "2018700"
  },
  {
    "text": "so we can just use two\nbidirectional LSTMs to represent the\nrepresentations. This is actually very important.",
    "start": "2018700",
    "end": "2025725"
  },
  {
    "text": "This bidirectional [INAUDIBLE]\nis actually very important to capture the context from\nboth the left and right side.",
    "start": "2025725",
    "end": "2030950"
  },
  {
    "text": " OK. So the next component\nis our next layer.",
    "start": "2030950",
    "end": "2037160"
  },
  {
    "text": "It's called the\nattention flow layer. So I just called\nit attention here. So the idea of\nattention is trying",
    "start": "2037160",
    "end": "2043640"
  },
  {
    "text": "to capture the interactions\nbetween the context and the query. And I think in this\npaper, the BiDAF paper,",
    "start": "2043640",
    "end": "2050510"
  },
  {
    "text": "they proposed two\nkinds of attention. So the first type\nof attention we called context-to-query\nattention.",
    "start": "2050510",
    "end": "2058069"
  },
  {
    "text": "So the idea is for\neach context word, can we find the\nmost relevant words",
    "start": "2058070",
    "end": "2067530"
  },
  {
    "text": "from the question\nfor the query words. So is one example. So here the context\nis Barack Obama",
    "start": "2067530",
    "end": "2075110"
  },
  {
    "text": "is the president of the USA. So for each context\nword, we need to find our alignment\nbecause we find",
    "start": "2075110",
    "end": "2081690"
  },
  {
    "text": "which words in the question\ncan be actually aligned with this context word. So we can see that both\nBarack and Obama can",
    "start": "2081690",
    "end": "2088156"
  },
  {
    "text": "be aligned with who,\nand the president can be aligned to leads, and the\nUSA is aligned to United States.",
    "start": "2088156",
    "end": "2094080"
  },
  {
    "text": "So basically for\neach context, we're trying to find the most\nrelevant query words.",
    "start": "2094080",
    "end": "2099980"
  },
  {
    "text": "And then the second\ntype of attention is called the query\nto context attention. So it's basically in\nthe other direction.",
    "start": "2099980",
    "end": "2106380"
  },
  {
    "text": "So here the idea is\nto choose some context words that are most relevant\nto one of the query words.",
    "start": "2106380",
    "end": "2114290"
  },
  {
    "text": "Because the context\ncan be very long. So a lot of the context\ncould be just not relevant",
    "start": "2114290",
    "end": "2120020"
  },
  {
    "text": "to this question. So we just ran over several\nexamples you can see there.",
    "start": "2120020",
    "end": "2126079"
  },
  {
    "text": "The first thing we need to do\nto try to locate which part of the sentences in this\ncontext can be actually",
    "start": "2126080",
    "end": "2132170"
  },
  {
    "text": "relevant to this question. So this type of the query\nto context attention is trying to capture which\ncontext words actually",
    "start": "2132170",
    "end": "2142640"
  },
  {
    "text": "can be most relevant to\none of the query words. So for this example,\nthe question",
    "start": "2142640",
    "end": "2149180"
  },
  {
    "text": "is which city is\ngloomy in winter. So because the question\nasked about gloomy,",
    "start": "2149180",
    "end": "2154670"
  },
  {
    "text": "so you guys can\ntry to figure out, OK, gloomy cities is actually\nvery relevant to this question.",
    "start": "2154670",
    "end": "2163119"
  },
  {
    "text": "Now we also find this in winter\nbecause \"in winter\" is also mentioned in the question. So this part of\nthe context words",
    "start": "2163120",
    "end": "2170125"
  },
  {
    "text": "can be also relevant\nto this question. So these context\nwords probably need",
    "start": "2170125",
    "end": "2176049"
  },
  {
    "text": "to be captured in this attention\nthat OK, this is actually relevant to this question.",
    "start": "2176050",
    "end": "2182300"
  },
  {
    "text": "OK. So this is actually just\nan intuition of these two types of attention.",
    "start": "2182300",
    "end": "2187420"
  },
  {
    "text": "And this is also\nwhy this model is called a bidirectional\nattention flow because there's a context-to-query\nattention and there is also",
    "start": "2187420",
    "end": "2194800"
  },
  {
    "text": "a query-to-context attention. So let me just talk about,\ngo through how to actually do",
    "start": "2194800",
    "end": "2203405"
  },
  {
    "text": "this query-to-context attention\nand the context-to-query attention in this model. So the way they do this is\nfirst to compute a similarity",
    "start": "2203405",
    "end": "2212270"
  },
  {
    "text": "score for every pair of the\ncontextualized vector ci",
    "start": "2212270",
    "end": "2217880"
  },
  {
    "text": "and then for every pair\nof the question with qj. So this is actually the\noutput from an encoding layer.",
    "start": "2217880",
    "end": "2223500"
  },
  {
    "text": "So this already the output\nfrom the LSTM layers. And the way they basically\njust compute our similarity",
    "start": "2223500",
    "end": "2229289"
  },
  {
    "text": "score by taking the\nci, qj, and also",
    "start": "2229290",
    "end": "2234890"
  },
  {
    "text": "the element-wise multiplication\nof the ci and qj. So they just concatenate\nthese three vectors.",
    "start": "2234890",
    "end": "2242270"
  },
  {
    "text": "So the output would be\na 6H dimensional vector. And they just multiply this\nto compute the dot product",
    "start": "2242270",
    "end": "2250070"
  },
  {
    "text": "of another learnable vector. And they finally just\nbasically give you",
    "start": "2250070",
    "end": "2255650"
  },
  {
    "text": "one scalar or one\nnumber, the sij, which measures the similarity\nbetween this context word ci",
    "start": "2255650",
    "end": "2263330"
  },
  {
    "text": "and also this question word qj. So if I learned the\nattention before,",
    "start": "2263330",
    "end": "2269750"
  },
  {
    "text": "so this is actually just\none choice of this model. So there could have\nbeen many different ways to define this similarity\nand similarities scores.",
    "start": "2269750",
    "end": "2278530"
  },
  {
    "text": "So this basically just one\ndesign choice of this model. OK. So after you find the\nsimilarities for sij--",
    "start": "2278530",
    "end": "2285430"
  },
  {
    "text": " so the context-to-query\nattention again, like which question words\nare most relevant to ci.",
    "start": "2285430",
    "end": "2293760"
  },
  {
    "text": "So the way they do\nthis is basically just taking these metrics,\nthe similarities for sij,",
    "start": "2293760",
    "end": "2300040"
  },
  {
    "text": "for each row. Each row basically correspond\nto one context word. ",
    "start": "2300040",
    "end": "2306246"
  },
  {
    "text": "For each row, let's\nsay they are going to compute a softmax\nfor each row, and this can give us\nnormalization scores alpha ij,",
    "start": "2306247",
    "end": "2314730"
  },
  {
    "text": "which is our probability\ndistribution over all the question words alpha ij.",
    "start": "2314730",
    "end": "2320450"
  },
  {
    "text": "So this is just really similar\nto all the attention mechanisms that you probably have\nseen in this class.",
    "start": "2320450",
    "end": "2326240"
  },
  {
    "text": "So basically for\neach context words taking the softmax over\nall the question words and",
    "start": "2326240",
    "end": "2334410"
  },
  {
    "text": "get us a probability\ndistribution. And finally, you just take\nthe weighted combination",
    "start": "2334410",
    "end": "2342400"
  },
  {
    "text": "of this attention score alpha\nij and also the question vector, the qj, and then\nfinally, you can",
    "start": "2342400",
    "end": "2347870"
  },
  {
    "text": "get a vector ai, which is\nactually a 2H dimensional vector. So this context-to-query\nattention basically",
    "start": "2347870",
    "end": "2355130"
  },
  {
    "text": "just tries to capture\nwhich questions are most relevant to each context word.",
    "start": "2355130",
    "end": "2360839"
  },
  {
    "text": "So the next part is a query to-- sorry, there's a\ntypo here, sorry.",
    "start": "2360840",
    "end": "2365865"
  },
  {
    "text": "This is actually the\nquery-to-context attention. Which means that\nwhich context words",
    "start": "2365865",
    "end": "2371670"
  },
  {
    "text": "are relevant to\nsome question words. So a lot of context\nwords will be not relevant to this question.",
    "start": "2371670",
    "end": "2378100"
  },
  {
    "text": "So the idea is to do this. They do this, is for\neach row of this sij, it basically just takes the max\nscores over all the question",
    "start": "2378100",
    "end": "2387245"
  },
  {
    "text": "words. And after taking this max score\nto compute the softmax over all",
    "start": "2387245",
    "end": "2392730"
  },
  {
    "text": "the context words here. So here, i actually enumerates\nover all the context words.",
    "start": "2392730",
    "end": "2398069"
  },
  {
    "text": "And this can give us another\nattention score of beta i, which captures how important\nthis context word is",
    "start": "2398070",
    "end": "2405660"
  },
  {
    "text": "relevant to this question. So after computing\nthis beta i, so we",
    "start": "2405660",
    "end": "2411520"
  },
  {
    "text": "can again compute this\nlike a weighted combination",
    "start": "2411520",
    "end": "2418500"
  },
  {
    "text": "by summing up the beta i and\nalso the context vector ci, and then finally, you get\nour vector bi, which is also",
    "start": "2418500",
    "end": "2426150"
  },
  {
    "text": "another 2H dimensional vector. The final output of\nthis attention function",
    "start": "2426150",
    "end": "2431400"
  },
  {
    "text": "is actually very\ncomplicated here. It's also the design\nchoice of this model.",
    "start": "2431400",
    "end": "2436869"
  },
  {
    "text": "So it takes a context\nvector ci and it takes ai from this\ncontext-to-query attention",
    "start": "2436870",
    "end": "2444630"
  },
  {
    "text": "and takes the element of\nmultiplication between the ci and ai and also ci and\nbi, and finally, it",
    "start": "2444630",
    "end": "2450910"
  },
  {
    "text": "takes the concatenation and can\nproduce 8H dimensional vector.",
    "start": "2450910",
    "end": "2457670"
  },
  {
    "text": "OK. Maybe I want to\npause a little bit and check if there are any\nquestions because this part is a little bit complicated.",
    "start": "2457670",
    "end": "2465460"
  },
  {
    "text": "One question is why is\nquery-to-context and context-to-query\nattention not symmetrical?",
    "start": "2465460",
    "end": "2471680"
  },
  {
    "start": "2471680",
    "end": "2477069"
  },
  {
    "text": "That's a good question, yes. So here because essentially\nthe goal you're trying-- because final\ngoal, you're trying",
    "start": "2477070",
    "end": "2483220"
  },
  {
    "text": "to find a span in the passage. So the point of this\nattention function",
    "start": "2483220",
    "end": "2489369"
  },
  {
    "text": "is trying to produce\na representation for each single context\nword in this context.",
    "start": "2489370",
    "end": "2494934"
  },
  {
    "text": " So we are not trying to generate\nquestion representations.",
    "start": "2494935",
    "end": "2500320"
  },
  {
    "text": "Here, the goal is\ntrying to generate context representations. So the difference\nbetween these two,",
    "start": "2500320",
    "end": "2507250"
  },
  {
    "text": "first it's trying to see\nwhich question words are relevant to this context word. Another kind is\ntrying to figure out",
    "start": "2507250",
    "end": "2512890"
  },
  {
    "text": "which context word\ncan be relevant and which context word\ncan be not relevant.",
    "start": "2512890",
    "end": "2518030"
  },
  {
    "text": "I hope this answers\nyour question, yeah. Here's an easier question sort\nof on the same topic which",
    "start": "2518030",
    "end": "2523760"
  },
  {
    "text": "might help. Is there a reason why you\nuse both query-to-context and",
    "start": "2523760",
    "end": "2528800"
  },
  {
    "text": "context-to-query attention? Is it sometimes advantageous\nor OK to use just one?",
    "start": "2528800",
    "end": "2536510"
  },
  {
    "text": "Difficult question. Yeah. So I'm going to show us some\noperations from this figure,",
    "start": "2536510",
    "end": "2543150"
  },
  {
    "text": "so way we just find both\ndirections can really help by joining the\ncontext-to-query",
    "start": "2543150",
    "end": "2549200"
  },
  {
    "text": "and query-to-context. So there'll be some\noperation studies. So by using one set is\nuseful, but just not",
    "start": "2549200",
    "end": "2555619"
  },
  {
    "text": "with us using the\nboth directions. Yeah. ",
    "start": "2555620",
    "end": "2561369"
  },
  {
    "text": "Right. Let's see. In the bottom right,\nwe sum over i,",
    "start": "2561370",
    "end": "2568180"
  },
  {
    "text": "so why does the i remain in bi? Is that correct or\nis that a typo there?",
    "start": "2568180",
    "end": "2574550"
  },
  {
    "text": "This is another typo. So again, I'm sorry. I know it's all\ngetting confusing.",
    "start": "2574550",
    "end": "2579721"
  },
  {
    "text": "So the output of\nthis module is to get",
    "start": "2579721",
    "end": "2584950"
  },
  {
    "text": "a representation for each\ncontext word at the end. So both the output\nfor ai and bi-",
    "start": "2584950",
    "end": "2591630"
  },
  {
    "text": "i is actually enumerated from-- actually enumerated, iterates\nover all the context words.",
    "start": "2591630",
    "end": "2599819"
  },
  {
    "text": "So bi will be still the-- just to try to aggregate\nover all the context words.",
    "start": "2599820",
    "end": "2607500"
  },
  {
    "text": "But the beta i\nmeasures the importance of these context words compared\nto all the context words.",
    "start": "2607500",
    "end": "2614730"
  },
  {
    "text": "So both ai and bi are\nactually with respect to the context words, yes. So you can see that\nhere it's basically",
    "start": "2614730",
    "end": "2621000"
  },
  {
    "text": "doing some kind of a\nelement-wise multiplication. So the output of gi will\nactually range from the 1",
    "start": "2621000",
    "end": "2628440"
  },
  {
    "text": "to N, which is the number\nof the context words. There are lots of\nquestions about this.",
    "start": "2628440",
    "end": "2634860"
  },
  {
    "text": "What is the rationale for\nthe expression of the gi? How does one come up\nwith such an expression?",
    "start": "2634860",
    "end": "2642780"
  },
  {
    "text": "OK, I don't know. I guess the authors try\nout a lot of things. So, OK keep on\nhearing and trying",
    "start": "2642780",
    "end": "2649140"
  },
  {
    "text": "to understand the roles of\nthe context-to-query attention and query-to-context attention.",
    "start": "2649140",
    "end": "2654660"
  },
  {
    "text": "So I bet that there could be\nmany different combinations to do this. I also think the authors have\ntrialed many different variants",
    "start": "2654660",
    "end": "2661620"
  },
  {
    "text": "and those are the ones\nthat come up at the end. I think there could\nbe smarter ways",
    "start": "2661620",
    "end": "2669300"
  },
  {
    "text": "to incorporate both\nattentions, but it doesn't have to deal with in this way. ",
    "start": "2669300",
    "end": "2677539"
  },
  {
    "text": "I mean, one other\nquestion would be in the query-to-context\nattention.",
    "start": "2677540",
    "end": "2683060"
  },
  {
    "text": "Why do you do a max\ninside the softmax? ",
    "start": "2683060",
    "end": "2688980"
  },
  {
    "text": "Yeah. Sorry. I should have explained\nthis more clearly. So here again,\nquery-to-context attention",
    "start": "2688980",
    "end": "2694670"
  },
  {
    "text": "is trying to measure the\nimportance of these context words with respect to\nsome question words.",
    "start": "2694670",
    "end": "2705579"
  },
  {
    "text": "So by taking the max for each\nrow in this matrix, so it's basically trying to see, OK,\nwhich question word is actually",
    "start": "2705580",
    "end": "2714380"
  },
  {
    "text": "most relevant to\nthis context word? If this number is\nstill very low, that means there\nisn't any question",
    "start": "2714380",
    "end": "2721430"
  },
  {
    "text": "words that could be aligned\nwith this context world. ",
    "start": "2721430",
    "end": "2726740"
  },
  {
    "text": "So after taking the max, if\nthis number is still very low, that means this context\nword is not very relevant.",
    "start": "2726740",
    "end": "2732730"
  },
  {
    "text": "So basically, that's why\nwe take the softmax-- take softmax over on\ntop of the max, yeah. ",
    "start": "2732730",
    "end": "2742400"
  },
  {
    "text": "I know. Do you want even more\nor do you want to go on? I probably should go on.",
    "start": "2742400",
    "end": "2747890"
  },
  {
    "text": "We have a lot of slides, but\nI'm happy to answer questions after those ones. Maybe you should go on.",
    "start": "2747890",
    "end": "2754440"
  },
  {
    "text": "Yeah. OK. So the last part of this\nmodel is actually the easiest,",
    "start": "2754440",
    "end": "2761270"
  },
  {
    "text": "the most simplest one. Lastly, these are two\nlayers, modeling layer",
    "start": "2761270",
    "end": "2766290"
  },
  {
    "text": "and output layers. So for the modeling layer-- so again, after the\nattention layer,",
    "start": "2766290",
    "end": "2771440"
  },
  {
    "text": "they take the key\nrepresentation, so basically the gi, which captures the\nattention between the context",
    "start": "2771440",
    "end": "2780140"
  },
  {
    "text": "and the query, and\nthen basically just pass this gi to another two\nlayers of bidirectional LSTMs.",
    "start": "2780140",
    "end": "2786740"
  },
  {
    "text": "And there's many\nreasons they do this, is the attention layer\nis basically modeling the interactions between\nthe query and context.",
    "start": "2786740",
    "end": "2794060"
  },
  {
    "text": "And by passing\nthis to another two layers of bidirectional\nLSTMs, the modeling layer, the basic modeling\nlayer, can also",
    "start": "2794060",
    "end": "2800420"
  },
  {
    "text": "first model the interactions\nwithin the context words. So this is the formulation here.",
    "start": "2800420",
    "end": "2808050"
  },
  {
    "text": "So this is a two layer\nbidirectional LSTM. By taking the gi's\ninput and the output will be on the mi, which\nis another 2H dimensional",
    "start": "2808050",
    "end": "2817559"
  },
  {
    "text": "vector for each context\nword in the passage. ",
    "start": "2817560",
    "end": "2823119"
  },
  {
    "text": "OK. So the final is\nthe output layers. So final output\nlayers are basically just two classifiers just trying\nto predict the start and end",
    "start": "2823120",
    "end": "2831119"
  },
  {
    "text": "positions. So they first concatenate\nthe gi and mi.",
    "start": "2831120",
    "end": "2836760"
  },
  {
    "text": "So this will be actually\na tanh dimensional vector. And then by computing the\ndot product of another vector",
    "start": "2836760",
    "end": "2843300"
  },
  {
    "text": "called W start and this\nis resulting vector. And they basically get a\nscore for each position",
    "start": "2843300",
    "end": "2850559"
  },
  {
    "text": "in the context. And then you can just\napply your softmax. And then this will\ngive you a probability",
    "start": "2850560",
    "end": "2857310"
  },
  {
    "text": "that OK, what is the\nprobability of this condition i, would be actually based\non the start position",
    "start": "2857310",
    "end": "2866579"
  },
  {
    "text": "of the final answer string. And they also have\nanother classifier",
    "start": "2866580",
    "end": "2872069"
  },
  {
    "text": "to predict the end\nposition of the answer. But they also give something\na little bit more complicated.",
    "start": "2872070",
    "end": "2877240"
  },
  {
    "text": "So they actually pass the mi\nto another bidirectional LSTM here. So they call it m prime i.",
    "start": "2877240",
    "end": "2882870"
  },
  {
    "text": "And they concatenate\ngi and m prime i-- sorry, this is a typo. So this should be w end.",
    "start": "2882870",
    "end": "2890069"
  },
  {
    "text": "So they compute the dot product\nbetween w end and this vector, and this can produce\nall the probability",
    "start": "2890070",
    "end": "2899280"
  },
  {
    "text": "over all the conditions\nwhich predict how likely this position\nwill be the end the position",
    "start": "2899280",
    "end": "2905819"
  },
  {
    "text": "of the answer. So by passing the mi to\nanother bedirectional LSTM, their reasoning is\nthat they're trying",
    "start": "2905820",
    "end": "2912870"
  },
  {
    "text": "to capture some\nkind of dependence between the choice\nof the start and end. So you can imagine that\nstart and end shouldn't",
    "start": "2912870",
    "end": "2919350"
  },
  {
    "text": "be too separate. So it could be\nindependent predicted by the-- if it's a\nclaim that if you--",
    "start": "2919350",
    "end": "2927450"
  },
  {
    "text": "adding some kind of dependence\nbetween the mi and the Pstart and Pend, this can\nactually perform better.",
    "start": "2927450",
    "end": "2936140"
  },
  {
    "text": "OK. I'm done with this part,\ndescribing the BiDAF model.",
    "start": "2936140",
    "end": "2941599"
  },
  {
    "text": "Any quick questions\nI can answer? I think you can actually go on.",
    "start": "2941600",
    "end": "2947930"
  },
  {
    "text": "OK. Sorry, I forgot to mention this. OK, the final\ntraining loss will be",
    "start": "2947930",
    "end": "2954700"
  },
  {
    "text": "by taking these two\nprobability distributions. And it is basically just\nthe negative log likelihood",
    "start": "2954700",
    "end": "2962000"
  },
  {
    "text": "of the start position\nof the gold answer and the end position\nof the answer.",
    "start": "2962000",
    "end": "2971640"
  },
  {
    "text": "It's basically taking\nthe product of these two probabilities, but\nyou apply it on log",
    "start": "2971640",
    "end": "2978530"
  },
  {
    "text": "so it is the sum of the\ntwo negative log terms of this final training loss.",
    "start": "2978530",
    "end": "2984160"
  },
  {
    "text": "And the whole model can\nbe just trained in an end to end way from\nthe encoding layer to attention layer to modeling\nlayer and to output layer.",
    "start": "2984160",
    "end": "2992200"
  },
  {
    "text": "So this just completes the\nwhole model of the BiDAF model.",
    "start": "2992200",
    "end": "2997420"
  },
  {
    "text": " OK, so this model is\nactually achieved--",
    "start": "2997420",
    "end": "3002940"
  },
  {
    "text": "like on SQuAD data set, it\nachieved a 77.3 F1 score. So as I mentioned earlier, these\nare sum of operation study.",
    "start": "3002940",
    "end": "3011220"
  },
  {
    "text": "They found that both\nattentions in two directions are actually important.",
    "start": "3011220",
    "end": "3016290"
  },
  {
    "text": "If you remove the one\ndirection, the performance will actually drop quite a bit. If you remove the\ncontext-to-query attention,",
    "start": "3016290",
    "end": "3022809"
  },
  {
    "text": "the performance will\ndrop to 67.7 F1 score. And then if you\nremove this part,",
    "start": "3022810",
    "end": "3028110"
  },
  {
    "text": "it will drop a 4-point F1 score. And then also the character\nembeddings also help.",
    "start": "3028110",
    "end": "3033790"
  },
  {
    "text": "So if you remove the\ncharacter embeddings, you'll get like a\nbasically 1.9 point drop.",
    "start": "3033790",
    "end": "3040869"
  },
  {
    "text": "And on the right of this slide,\nyou can see a very big table. So it's basically\nall the models that",
    "start": "3040870",
    "end": "3047970"
  },
  {
    "text": "account at that time\nbetween 2016 and 2018. So you can see\nthat BiDAF is here.",
    "start": "3047970",
    "end": "3055240"
  },
  {
    "text": "So you can check\nthe 77.3 F1 score. And basically all of\nthe models are actually",
    "start": "3055240",
    "end": "3060300"
  },
  {
    "text": "a very similar ballpark. So numbers range from\nthe highest number",
    "start": "3060300",
    "end": "3065320"
  },
  {
    "text": "here, 79.8 until after\nthe ELMo was introduced, the numbers have actually\nimproved quite a bit.",
    "start": "3065320",
    "end": "3072840"
  },
  {
    "text": "So before the ELMo,\nbasically all the numbers are actually kind of similar. So each model actually\nimproved the previous model",
    "start": "3072840",
    "end": "3080250"
  },
  {
    "text": "by one point or two points. And now here is our\nattention visualization",
    "start": "3080250",
    "end": "3086650"
  },
  {
    "text": "to show that how this\nsmorgasbord of attention actually can capture the\nsimilarity between the question",
    "start": "3086650",
    "end": "3093100"
  },
  {
    "text": "words and the context words. So a clear example\nof a question, where did the Super\nBowl 50 take place?",
    "start": "3093100",
    "end": "3099530"
  },
  {
    "text": "So it show the actual\nquestion word here. And each column's\nmatrix basically",
    "start": "3099530",
    "end": "3105100"
  },
  {
    "text": "indicates the attention score,\nthe similarity score that has been learned by this model.",
    "start": "3105100",
    "end": "3111140"
  },
  {
    "text": "So you can see that on\nthe right it's basically trying to print out or\ndisplay the context words that",
    "start": "3111140",
    "end": "3120700"
  },
  {
    "text": "had the highest scores. So you can see that\nthe word has to be",
    "start": "3120700",
    "end": "3126060"
  },
  {
    "text": "aligned very well with the\nat, the, Stadium, Levi. And also the Super Bowl 50\nbasically aligns very well",
    "start": "3126060",
    "end": "3133270"
  },
  {
    "text": "with Super Bowl 50. So basically this\ntheory tells us that these kind of\nattention scores can actually capture those\nnegative scores pretty well,",
    "start": "3133270",
    "end": "3140470"
  },
  {
    "text": "yeah.  OK, so next, I'm going\nto talk about BERT,",
    "start": "3140470",
    "end": "3147119"
  },
  {
    "text": "how to use the BERT model\nto solve this problem. So I know that you have learned\nBERT in the last lecture,",
    "start": "3147120",
    "end": "3154020"
  },
  {
    "text": "so I'm not going to repeat this. So very quick. So BERT is basically a deep\nbidirectional transformer",
    "start": "3154020",
    "end": "3159840"
  },
  {
    "text": "encoder pre-trained on\nlarge amounts of text. And it is trained on the\ntwo training objectives,",
    "start": "3159840",
    "end": "3165420"
  },
  {
    "text": "including masked language\nmodeling and the next sentence prediction. And this model has\na lot of parameters.",
    "start": "3165420",
    "end": "3172049"
  },
  {
    "text": "So the BERTbase has\n110 million parameters and the BERT-large model\nhas 330 million parameters.",
    "start": "3172050",
    "end": "3180630"
  },
  {
    "text": "So, OK, we can actually use BERT\nfor our reading comprehension. So it's actually very easy\nand very straightforward.",
    "start": "3180630",
    "end": "3187359"
  },
  {
    "text": "The idea is to take the\nquestion as our segment A. So you got BERT\n[INAUDIBLE] these",
    "start": "3187360",
    "end": "3192960"
  },
  {
    "text": "are our two segments for the\nnext sentence prediction task. So then you apply the BERT over\nthe reading comprehension task.",
    "start": "3192960",
    "end": "3200430"
  },
  {
    "text": "You basically just take\nthe question as a segment A and take the passage\nas a segment B.",
    "start": "3200430",
    "end": "3205680"
  },
  {
    "text": "And then finally,\nyou go with trying to predict two endpoints in\nsegment B. So here is one more",
    "start": "3205680",
    "end": "3212539"
  },
  {
    "text": "complex example. So question is how many\nparameters does BERT-large have? So you can see\nthat they basically",
    "start": "3212540",
    "end": "3219170"
  },
  {
    "text": "just take the questions here\nand then take the passage here. And then by putting the CLS\ntoken and the SEP token,",
    "start": "3219170",
    "end": "3227880"
  },
  {
    "text": "and then by just concatenating\nthe questions past the tokens. And also for the\nquestions that we just need to pass the A to\na segment embeddings.",
    "start": "3227880",
    "end": "3235185"
  },
  {
    "text": "And the full passage, you\njust need to cut upward in the segment B embeddings.",
    "start": "3235185",
    "end": "3240200"
  },
  {
    "text": "And then finally, the training\nloss is also the same. So you basically\njust try to maximize",
    "start": "3240200",
    "end": "3246710"
  },
  {
    "text": "the sum of the negative\nlog likelihood of both the start and end positions.",
    "start": "3246710",
    "end": "3251839"
  },
  {
    "text": "Well here, the way that they\ncompute the start and end probability is\nslightly different.",
    "start": "3251840",
    "end": "3257790"
  },
  {
    "text": "So it's actually\nvery straightforward. So you just have this\ninput attention into BERT.",
    "start": "3257790",
    "end": "3264059"
  },
  {
    "text": "And BERT can give you the hidden\nvector hi that can actually represent the\nhidden vector that's",
    "start": "3264060",
    "end": "3271370"
  },
  {
    "text": "corresponding to\nthe context word ci. So you can just introduce\nanother two vectors Wstart",
    "start": "3271370",
    "end": "3279177"
  },
  {
    "text": "and Wend by computing\nthe dot product and then apply the softmax. Then it can just give you a very\nsimilar to what we had before.",
    "start": "3279177",
    "end": "3287550"
  },
  {
    "text": "But here the hi is the\noutput from the BERT encoder, and then we are training\nthis two Wstart and Wend",
    "start": "3287550",
    "end": "3296150"
  },
  {
    "text": "for these two probability\ndistribution Pstart and Pend. ",
    "start": "3296150",
    "end": "3303210"
  },
  {
    "text": "OK, so for this model, so\nall the BERT parameters, that is actually a\nvery large number.",
    "start": "3303210",
    "end": "3309180"
  },
  {
    "text": "If you use the BERTbase,\nit's 110 million parameters. As well the newly\nintroduced parameters,",
    "start": "3309180",
    "end": "3315210"
  },
  {
    "text": "Hstart and the Hend, which\nis if you take the BERTbase, the hidden side will be 768.",
    "start": "3315210",
    "end": "3321579"
  },
  {
    "text": "So it's only like\n1,500 new parameters. So we just have to\noptimize together jointly",
    "start": "3321580",
    "end": "3327790"
  },
  {
    "text": "for this training objective L.\nAnd it actually works really, really well with this model.",
    "start": "3327790",
    "end": "3333690"
  },
  {
    "text": "So if you just take\nthis BERT model, and by just optimizing all\nthe parameters together,",
    "start": "3333690",
    "end": "3339230"
  },
  {
    "text": "it can give you a\nvery high performance. I will show you\nBERT in a minute. And even if you use a stronger\npre-training models or modern,",
    "start": "3339230",
    "end": "3348110"
  },
  {
    "text": "like a-- stronger models than\nthe BERT models, they can even lead to\nbetter performance on SQuAD.",
    "start": "3348110",
    "end": "3355130"
  },
  {
    "text": "And SQuAD has also\nbecome a standard dataset for testing these kind\nof pre-trained models.",
    "start": "3355130",
    "end": "3360150"
  },
  {
    "text": "Let me show you some numbers. So again, human performance\nis 91 and BiDAF is 77.3.",
    "start": "3360150",
    "end": "3367430"
  },
  {
    "text": "And if we just do this\nfine-tuning model, so BERTbase can\ngive you of 88.5,",
    "start": "3367430",
    "end": "3373430"
  },
  {
    "text": "BERTlarge can give you 90.9. So you can see that this is a\nhuge jump from the BiDAF model",
    "start": "3373430",
    "end": "3379460"
  },
  {
    "text": "to the BERT models. And then finally,\nif you see even the latest pre-trained\nlanguage models, including",
    "start": "3379460",
    "end": "3387390"
  },
  {
    "text": "the XLNet or RoBERTa or Albert. So these models\nare either bigger or these models are\ntrained on bigger corpus,",
    "start": "3387390",
    "end": "3393980"
  },
  {
    "text": "or the model size are bigger. So basically these\nmodels can give you",
    "start": "3393980",
    "end": "3399150"
  },
  {
    "text": "another like 3, 4 F1 score\ncompared to the BERTlarge model.",
    "start": "3399150",
    "end": "3404329"
  },
  {
    "text": "So this is already way higher\nthan our estimated F1 score. So this just works really well.",
    "start": "3404330",
    "end": "3410619"
  },
  {
    "text": " Any quick questions? ",
    "start": "3410620",
    "end": "3419470"
  },
  {
    "text": "I think this may be OK. ",
    "start": "3419470",
    "end": "3424890"
  },
  {
    "text": "OK, so yeah, I guess I\nwent a little bit fast for these BERT models.",
    "start": "3424890",
    "end": "3430640"
  },
  {
    "text": "Next, so I want to also do\na bit of the comparisons between the BiDAF models\nand the BERT models.",
    "start": "3430640",
    "end": "3436380"
  },
  {
    "text": "So BERT model has many,\nmany more parameters. So has it easily\n110 more million",
    "start": "3436380",
    "end": "3441570"
  },
  {
    "text": "or 330 million parameters. But BiDAF has only like\n2.5 million parameters.",
    "start": "3441570",
    "end": "3448140"
  },
  {
    "text": "And the BiDAF is built on top\nof several bidirectional LSTMs while BERT is built on\ntop of their transformers.",
    "start": "3448140",
    "end": "3454619"
  },
  {
    "text": "So transformer means that there\nisn't any recurrence structure architecture. So their transformers are\nmuch easier to parallelize.",
    "start": "3454620",
    "end": "3462670"
  },
  {
    "text": "Another very key difference\nbetween the BERT models and the BiDAF models\nis BERT models",
    "start": "3462670",
    "end": "3467970"
  },
  {
    "text": "are pre-trained while\nBiDAF models only builtd on top of the GloVe\nvectors, which is pre-trained",
    "start": "3467970",
    "end": "3473730"
  },
  {
    "text": "and all the remaining\nparameters need to learn from this SQuAD\ndataset or the other supervision",
    "start": "3473730",
    "end": "3480089"
  },
  {
    "text": "datasets. So here it is very clear that\npre-training is a game changer",
    "start": "3480090",
    "end": "3486750"
  },
  {
    "text": "here. Pre-training basically\ncan just change everything and it also gives you a\nvery, very large boost",
    "start": "3486750",
    "end": "3492720"
  },
  {
    "text": "in terms of the performance. But I also want to\nraise another question.",
    "start": "3492720",
    "end": "3498620"
  },
  {
    "text": "So if we first think\nof this pre-training, these BiDAF models\nand BERT models",
    "start": "3498620",
    "end": "3504407"
  },
  {
    "text": "are really\nfundamentally different? I don't think so, so this\nis actually my argument.",
    "start": "3504407",
    "end": "3510520"
  },
  {
    "text": "So let's try to see how\nthese two models are actually connected, especially in\nterms of model design.",
    "start": "3510520",
    "end": "3517310"
  },
  {
    "text": "So BiDAF models\nessentially are trying to model the interactions\nbetween the question and the passage, right?",
    "start": "3517310",
    "end": "3523089"
  },
  {
    "text": "So both the question to passage\nand passage to question. And the BERT model\nessentially, they're",
    "start": "3523090",
    "end": "3528619"
  },
  {
    "text": "trying to use a self-attention\non top of the concatenation",
    "start": "3528620",
    "end": "3535250"
  },
  {
    "text": "of the question and passage. So this is a transformer model. So you could take the\nquestion and the passage,",
    "start": "3535250",
    "end": "3540350"
  },
  {
    "text": "so these are questions\nin the passage, and then you apply many,\nmany different layers of the self-attention.",
    "start": "3540350",
    "end": "3545539"
  },
  {
    "text": "Essentially, this\nself-attention is able to capture the attention\nbetween the context words,",
    "start": "3545540",
    "end": "3552410"
  },
  {
    "text": "the attention between the\npassage words and the question words, and the attention\nfrom the questions to the passage side, and also of\nthe attention from the question",
    "start": "3552410",
    "end": "3561650"
  },
  {
    "text": "words to other question words. So compared to BiDAF, BiDAF\nis trying to model this part.",
    "start": "3561650",
    "end": "3568280"
  },
  {
    "text": "But the BERT model essentially\ncan capture the attention between all these four parts.",
    "start": "3568280",
    "end": "3573860"
  },
  {
    "text": "And actually after\nthe BiDAF cannot. This is also before\nthe BERT can add.",
    "start": "3573860",
    "end": "3580530"
  },
  {
    "text": "So we were happy also\nshowing that if you just add a little self-attention\nlayer for the passage side,",
    "start": "3580530",
    "end": "3586790"
  },
  {
    "text": "so basically, you are\ntrying to exclusively model this attention between the\npassage words and passage words to the BiDAF.",
    "start": "3586790",
    "end": "3593120"
  },
  {
    "text": "This can also improve\nthe performance. So we can see that\nthese two models, you can really just\ntrying to model",
    "start": "3593120",
    "end": "3599589"
  },
  {
    "text": "the attention between\nthe passage and question and also the attention\nbetween the passage words and the passage words.",
    "start": "3599590",
    "end": "3605270"
  },
  {
    "text": "And this is actually was\nexactly the BERT model is doing. ",
    "start": "3605270",
    "end": "3613010"
  },
  {
    "text": "So if there's no\nfurther questions. So at this point,\nI'll talk about-- BERT models can do really\nwell on this kind of reading",
    "start": "3613010",
    "end": "3620690"
  },
  {
    "text": "comprehension data set. And we just talk\nabout pre-training can really change\nthe performance,",
    "start": "3620690",
    "end": "3625940"
  },
  {
    "text": "can be a game-changer in\nreading comprehension. Can I put in one-- Danqi, can I add\none question first?",
    "start": "3625940",
    "end": "3633230"
  },
  {
    "text": "People wonder whether\nyou can do well with a transformer that\nisn't pre-trained, right,",
    "start": "3633230",
    "end": "3639620"
  },
  {
    "text": "if you tried to build a\nquestion answering system using a transformer rather than\nLSTMs but no pre-training.",
    "start": "3639620",
    "end": "3645530"
  },
  {
    "text": "Does that work? That's a good question. Yeah, it works. But you probably cannot really\nbuild a model as big as 110",
    "start": "3645530",
    "end": "3653839"
  },
  {
    "text": "million parameters or 330\nmillion parameters models.",
    "start": "3653840",
    "end": "3659490"
  },
  {
    "text": "So as we train the model\nbetween these family",
    "start": "3659490",
    "end": "3666740"
  },
  {
    "text": "of LSTM models and BERT models. They're called\nQANet from Google. So QANet is actually built\non top of transformers",
    "start": "3666740",
    "end": "3673350"
  },
  {
    "text": "to be solved around\npre-training. So that the model actually can\nperform better than the BiDAF models and other models.",
    "start": "3673350",
    "end": "3678970"
  },
  {
    "text": "But it actually under-performs\nBERT models a little bit. So just check it out on QANet.",
    "start": "3678970",
    "end": "3685599"
  },
  {
    "text": " OK. I will just continue.",
    "start": "3685600",
    "end": "3692800"
  },
  {
    "text": "OK, so here, pre-training\nhas been so important. So next I will\nquickly talk about--",
    "start": "3692800",
    "end": "3699460"
  },
  {
    "text": "OK, a question here is\nthat can we actually even design better\npre-training objectives for reading comprehension\nor question answering?",
    "start": "3699460",
    "end": "3705880"
  },
  {
    "text": "And the answer is actually yes. So this is actually a work\nI did with Mandar Joshi and other folks one year\nago called SpanBERT.",
    "start": "3705880",
    "end": "3713617"
  },
  {
    "text": "So think about this. So for SQuAD and other a\nlot of extractable reading",
    "start": "3713617",
    "end": "3719210"
  },
  {
    "text": "comprehension\ndatasets, the goal is trying to predict\nthe answer span from the passage of the\nquestion, the answer",
    "start": "3719210",
    "end": "3727370"
  },
  {
    "text": "to this question. So there are two key ideas\nbeing proposed in SpanBERT.",
    "start": "3727370",
    "end": "3732869"
  },
  {
    "text": "So first idea is\nthat instead of using only the masking of\nindividual words,",
    "start": "3732870",
    "end": "3738440"
  },
  {
    "text": "we propose that we want\nto mask a contiguous spans of words in the passage.",
    "start": "3738440",
    "end": "3744200"
  },
  {
    "text": "Because the final\nanswer will be just a section of text\nin the passage. So we are trying to mask out\nall these possible answer",
    "start": "3744200",
    "end": "3752255"
  },
  {
    "text": "spans from the passage as\nour training objective. And the second idea\nproposed in SpanBERT is--",
    "start": "3752255",
    "end": "3759500"
  },
  {
    "text": "because at the end\nof it, it goes around to predict an answer span. So we're actually essentially\ntry to predict two endpoints as",
    "start": "3759500",
    "end": "3767140"
  },
  {
    "text": "well as the answer. So the idea here is that can\nyou try to compress the two end",
    "start": "3767140",
    "end": "3772600"
  },
  {
    "text": "points of answer span to-- sorry, can we try to compress\nall the information in the span",
    "start": "3772600",
    "end": "3780260"
  },
  {
    "text": "into the two end points? So here the idea is that-- here, let's think about this.",
    "start": "3780260",
    "end": "3785550"
  },
  {
    "text": "If we mask by the fours here. And can we try to use the two\nendpoints here in this figure,",
    "start": "3785550",
    "end": "3792470"
  },
  {
    "text": "like x4 and x9, to predict\nall the words in the middle? So essentially, we are\ntrying to take the two end",
    "start": "3792470",
    "end": "3799549"
  },
  {
    "text": "points and also some kind\nof position encoding. And then finally we are\ngoing to try to predict",
    "start": "3799550",
    "end": "3805250"
  },
  {
    "text": "all the words in this span. So this is why it\nis called SpanBERT. So I encourage you to\ncheck out our paper.",
    "start": "3805250",
    "end": "3812599"
  },
  {
    "text": "And this actually\nreally helps a lot, at least for the question\nanswering data set. So as you can see\nfrom this figure--",
    "start": "3812600",
    "end": "3819570"
  },
  {
    "text": "so this is SQuAD 1.1\nand this is SQuAD 2.0. And these are many\nother question answering",
    "start": "3819570",
    "end": "3825140"
  },
  {
    "text": "data sets as you can see here. So the blue bars here,\nwe call it Google BERT,",
    "start": "3825140",
    "end": "3831320"
  },
  {
    "text": "are actually the\noriginal checkpoints that are released by\nGoogle for researchers.",
    "start": "3831320",
    "end": "3836690"
  },
  {
    "text": "And our BERT is\nactually just the-- is actually our\nre-implementation of the BERT model using\nthe same data where",
    "start": "3836690",
    "end": "3844250"
  },
  {
    "text": "we have been trying to train\nthis model for slightly longer. So it's actually achieved\na better performance",
    "start": "3844250",
    "end": "3850160"
  },
  {
    "text": "than the original Google BERT. So as you can see,\nthe yellow bars here is actually the SpanBERT.",
    "start": "3850160",
    "end": "3855560"
  },
  {
    "text": "So SpanBERT actually\ngreatly outperformed Google BERT and\nother BERT basically",
    "start": "3855560",
    "end": "3861400"
  },
  {
    "text": "across all of the datasets. So that really tells us\nthat OK, even if we are not going to increase\nthe model size,",
    "start": "3861400",
    "end": "3867660"
  },
  {
    "text": "that we are not going\nto increase the data, by designing better\npre-training objectives can also go a long way and\ndo a much better job at least",
    "start": "3867660",
    "end": "3875468"
  },
  {
    "text": "in the question answering\nand the reading comprehension datasets.",
    "start": "3875468",
    "end": "3880940"
  },
  {
    "text": "OK. So I have several few\nslides left in this part.",
    "start": "3880940",
    "end": "3887160"
  },
  {
    "text": "So so far, we've demonstrated\nthat by using BiDAF model and by using BERT models, we\ncan get a very good performance",
    "start": "3887160",
    "end": "3895040"
  },
  {
    "text": "on the SQuAD dataset. This number has already exceeded\neven the human performance on SQuAD.",
    "start": "3895040",
    "end": "3900737"
  },
  {
    "text": "Does this means that reading\ncomprehension is already solved? The answer is of course not.",
    "start": "3900737",
    "end": "3908650"
  },
  {
    "text": "So in the recent\nlast couple of years, there's been a lot\nof evidence showing that the current systems\nstill perform poorly",
    "start": "3908650",
    "end": "3916030"
  },
  {
    "text": "on adversarial examples\nor the examples from out of domain\ndistributions.",
    "start": "3916030",
    "end": "3921660"
  },
  {
    "text": "So here is a very\nclassical example proposed by Robin Jia\nand Percy Liang in 2017.",
    "start": "3921660",
    "end": "3928859"
  },
  {
    "text": "So the idea is to take a passage\nand then take a question.",
    "start": "3928860",
    "end": "3934440"
  },
  {
    "text": "And they're trying to just\ninsert a random sentence to the end of the paragraph.",
    "start": "3934440",
    "end": "3940150"
  },
  {
    "text": "So you can see\nthat this sentence has even a little sense entity,\nin this context, Jeff Dean",
    "start": "3940150",
    "end": "3945599"
  },
  {
    "text": "here. But this sentence\nactually has some overlap",
    "start": "3945600",
    "end": "3950880"
  },
  {
    "text": "between the question. It's actually very\nsimilar to this question, but actually the numbers\nhave been changed.",
    "start": "3950880",
    "end": "3956009"
  },
  {
    "text": "The entity length\nhas been changed. And they found that these\nkind of adversarial examples",
    "start": "3956010",
    "end": "3961200"
  },
  {
    "text": "are actually very easy to\nfool the current systems and makes the system to predict\nthe answer to be Jeff Dean.",
    "start": "3961200",
    "end": "3971120"
  },
  {
    "text": "So here the table\nshows that by adding a lot of these\nadversarial examples, they found the\nperformance actually drops",
    "start": "3971120",
    "end": "3977730"
  },
  {
    "text": "a lot for this BiDAF model. So drops from 75.5\nto even like 30%.",
    "start": "3977730",
    "end": "3983640"
  },
  {
    "text": "So for you, like\nthis kind of attack, the performance level just\ndropped to very low, like 4.8%. ",
    "start": "3983640",
    "end": "3991090"
  },
  {
    "text": "So here is another paper that\nactually just came out in 2020. So there has to be\na lot of evidence",
    "start": "3991090",
    "end": "3996430"
  },
  {
    "text": "showing the similar things. So today we compute a very good\nreading comprehension data set",
    "start": "3996430",
    "end": "4002520"
  },
  {
    "text": "on the individual data sets. But these systems trained on one\ndataset basically cannot really generalize to other datasets.",
    "start": "4002520",
    "end": "4009180"
  },
  {
    "text": "So the diagonal of\nthis table is basically trying to model one\ndataset and evaluate it",
    "start": "4009180",
    "end": "4016170"
  },
  {
    "text": "on the same dataset. And all the other\nnumbers in this table basically shows that if you\ntrain one system on one dataset",
    "start": "4016170",
    "end": "4024299"
  },
  {
    "text": "and then evaluate\non another dataset, the performance will\ndrop quite a lot. So it basically really cannot\ngeneralize from one dataset",
    "start": "4024300",
    "end": "4031140"
  },
  {
    "text": "to another dataset. So finally, this is actually\na very interesting result.",
    "start": "4031140",
    "end": "4037480"
  },
  {
    "text": "So this table is actually\nthe best paper from ACL 2020, it's called a checklist paper.",
    "start": "4037480",
    "end": "4043960"
  },
  {
    "text": "So the idea is that\nthese authors basically try to propose some\nkind of the test cases",
    "start": "4043960",
    "end": "4050970"
  },
  {
    "text": "to checker whether these models\ncan actually really answer some simple questions when there\nis some specific or particular",
    "start": "4050970",
    "end": "4058350"
  },
  {
    "text": "phenomenon. They found that by just-- come up with a really\nsimple question.",
    "start": "4058350",
    "end": "4064020"
  },
  {
    "text": "For example here, Jeremy is\nmore optimistic than Taylor. And who is more pessimistic?",
    "start": "4064020",
    "end": "4070830"
  },
  {
    "text": "And they found that a\nBERTlarge model cannot solve and basically failed these type\nof test cases 100% of the time.",
    "start": "4070830",
    "end": "4081730"
  },
  {
    "text": "And now here is another table. So here is another\ncoref example.",
    "start": "4081730",
    "end": "4087309"
  },
  {
    "text": "Victoria and Alex are friends. Her mom is an agent. Whose mom is an agent?",
    "start": "4087310",
    "end": "4093130"
  },
  {
    "text": "And so to get this\nkind of question correctly, so it\nhas to understand the Victoria actually\nrefers to a female person",
    "start": "4093130",
    "end": "4100870"
  },
  {
    "text": "and Alex refers\nto a male person. So these kind of questions\nalso make the current models,",
    "start": "4100870",
    "end": "4107500"
  },
  {
    "text": "BERTlarge models\ntrained on SQuAD totally fail on these\nkind of test cases. ",
    "start": "4107500",
    "end": "4115318"
  },
  {
    "text": "OK. So I have 10 minutes left. Chris, is there any question\nI should answer at this point",
    "start": "4115319",
    "end": "4121220"
  },
  {
    "text": "here? I think you can go on. OK. So in the last 10\nminutes, I'm going",
    "start": "4121220",
    "end": "4127049"
  },
  {
    "text": "to give you a very,\nvery brief introduction of what is open-domain\nquestion answering and what we have been trying to\ndo in the last couple of years.",
    "start": "4127050",
    "end": "4136299"
  },
  {
    "text": "So open-domain question\nanswering is a problem that-- so it's different from\nreading comprehension that we don't assume\na given passage.",
    "start": "4136300",
    "end": "4143729"
  },
  {
    "text": "So here, we use the\nassumptions that we only have access to a large\ncollection of documents.",
    "start": "4143729",
    "end": "4149430"
  },
  {
    "text": "So one example is just taking\nthe whole Wikipedia, which has five million articles.",
    "start": "4149430",
    "end": "4154769"
  },
  {
    "text": "So we don't really know\nwhere the answer is located. And we're going to\nreturn the answer for any open-domain questions.",
    "start": "4154770",
    "end": "4162049"
  },
  {
    "text": "So this problem, there\nisn't any single passage, so we have to answer questions\nagainst a very large collection of documents or even\nthe whole web documents.",
    "start": "4162050",
    "end": "4169830"
  },
  {
    "text": "So this is actually a much\nmore challenging and also more practical problem.",
    "start": "4169830",
    "end": "4175560"
  },
  {
    "text": "So if you look at the\nexample of Google example I showed at the beginning,\nso these techniques",
    "start": "4175560",
    "end": "4181979"
  },
  {
    "text": "will be very useful in the\npractical applications.",
    "start": "4181979",
    "end": "4187720"
  },
  {
    "text": "So the term here open\ndomains is just in contrast to closed domains that\ndeal with questions",
    "start": "4187720",
    "end": "4194434"
  },
  {
    "text": "under a specific domain, yeah.  OK, so how can we solve\nthis type of problem?",
    "start": "4194434",
    "end": "4203113"
  },
  {
    "text": "Because for the reading\ncomprehension problem, we just make sure\nto answer questions based on a simple passage. So this is a paper that I\nwrote in 2017, four years now.",
    "start": "4203113",
    "end": "4211409"
  },
  {
    "text": "So the paper is called\nReading Wikipedia to Answer Open-domain Questions\non a system we call DrQA.",
    "start": "4211410",
    "end": "4217770"
  },
  {
    "text": "So this paper basically\nproposed the idea that we can actually\nsolve this problem",
    "start": "4217770",
    "end": "4222870"
  },
  {
    "text": "by using a retrieval and\nalso a reader framework. So the idea is that\nlet's take a question--",
    "start": "4222870",
    "end": "4230190"
  },
  {
    "text": "OK, so here, the\narticle is trying to answer questions using a very\nlarge collection of documents",
    "start": "4230190",
    "end": "4235660"
  },
  {
    "text": "such as the Wikipedia. So the idea is that there\nis a retrieval and also reader component.",
    "start": "4235660",
    "end": "4241020"
  },
  {
    "text": "So the retrieval\ntext in the question and try to find out a small\nnumber of documents that to be",
    "start": "4241020",
    "end": "4246990"
  },
  {
    "text": "relevant to this question. And this reader\nmodel is basically trying to read through\nall the documents",
    "start": "4246990",
    "end": "4252450"
  },
  {
    "text": "that this retrieval\nreturn and try to find out the correct answer.",
    "start": "4252450",
    "end": "4259559"
  },
  {
    "text": "So formally defined here is that\nthe input is a large collection of documents D and\nthe question Q.",
    "start": "4259560",
    "end": "4266239"
  },
  {
    "text": "And the output could\nbe our answer string A. So we can just decompose\nthis problem into,",
    "start": "4266240",
    "end": "4272083"
  },
  {
    "text": "as I just mentioned, in our\nretrieval and the reader component. So the retrieval\nis basically trying to take the large collection\nof document D and Q,",
    "start": "4272083",
    "end": "4279680"
  },
  {
    "text": "and is trying to return a set\nof documents or set of passages. So here, this number K could\nbe very small, just like 100.",
    "start": "4279680",
    "end": "4294199"
  },
  {
    "text": "So it's basically trying to find\nout 100 passages of documents",
    "start": "4294200",
    "end": "4299540"
  },
  {
    "text": "from let's say 5\nmillion documents. And finally the reader\nbasically takes the question",
    "start": "4299540",
    "end": "4305270"
  },
  {
    "text": "and takes this type\nof the passages and finally returns the answer. So the second\nproblem is actually",
    "start": "4305270",
    "end": "4311400"
  },
  {
    "text": "the reading comprehension\nmodel that we just learned. So these are just from\n2017 paper we wrote.",
    "start": "4311400",
    "end": "4318610"
  },
  {
    "text": "So it's actually doing\na very simple thing. So the retrieval is just\nstandard information retrieval",
    "start": "4318610",
    "end": "4324730"
  },
  {
    "text": "model with a TF-IDF information\nretrieval sparse model. And the real model\nessentially is just",
    "start": "4324730",
    "end": "4330700"
  },
  {
    "text": "the neural reading comprehension\nmodel I just talked about. So it's mainly trained on SQuAD\nand some other question answer",
    "start": "4330700",
    "end": "4337390"
  },
  {
    "text": "datasets. So the idea is very\nsimple, but it's trying to bridge\nwith two things,",
    "start": "4337390",
    "end": "4344239"
  },
  {
    "text": "how to have a bridge as the\nretrieval and also the reader to do this kind of open-domain\nquestion answering.",
    "start": "4344240",
    "end": "4349442"
  },
  {
    "text": " So I'm just going\nto quickly go over",
    "start": "4349442",
    "end": "4355310"
  },
  {
    "text": "some really exciting\nideas that has been happening in the\nlast two years basically.",
    "start": "4355310",
    "end": "4362449"
  },
  {
    "text": "So the first idea is that\nthis is retrieval part can be also trained. So we can actually\neven do this kind",
    "start": "4362450",
    "end": "4368950"
  },
  {
    "text": "of training of the\nretriever and the reader. So here is actually-- so this idea has been first\nproposed in Kenton Lee's paper",
    "start": "4368950",
    "end": "4377000"
  },
  {
    "text": "in 2019 called Latent Retrieval\nfor Weakly Supervised Open Domain Question Answering.",
    "start": "4377000",
    "end": "4382940"
  },
  {
    "text": "So this part is\nbasically the first model for reading comprehension,\nand this part is basically",
    "start": "4382940",
    "end": "4389780"
  },
  {
    "text": "the retrieval model. So to get this\nretrieval model working, they also tried to use the\nBERT to encode the passage",
    "start": "4389780",
    "end": "4396500"
  },
  {
    "text": "and also encode the\nquestion, and they tried to use a dot product\nbetween the question representation and the\npassage representation",
    "start": "4396500",
    "end": "4402055"
  },
  {
    "text": "to model the relevance, the\nsimilarity between the question",
    "start": "4402055",
    "end": "4407210"
  },
  {
    "text": "and the passage. But this is actually a very\ndifficult problem because of scalability of this problem.",
    "start": "4407210",
    "end": "4413929"
  },
  {
    "text": "Because there are 20 million\npassages in Wikipedia, so it's actually very\nhard to model this part.",
    "start": "4413930",
    "end": "4420080"
  },
  {
    "text": "So I encourage you to\ncheck out this paper.  And also second paper I\nwant to quickly mention",
    "start": "4420080",
    "end": "4428460"
  },
  {
    "text": "is also work I did last year. It's called the dense\npassage retrieval.",
    "start": "4428460",
    "end": "4434280"
  },
  {
    "text": "So the idea is actually very\nsimilar to the previous paper.",
    "start": "4434280",
    "end": "4439440"
  },
  {
    "text": "But the idea is an actually\nmuch more simplified model and a very easy, a very simple\nand straightforward approach.",
    "start": "4439440",
    "end": "4445800"
  },
  {
    "text": "The idea is that we can also\nreally just train the retrieval part by using two BERT models\nusing only the question",
    "start": "4445800",
    "end": "4451469"
  },
  {
    "text": "and answer pairs. And this model kind\nof works really well, and it can largely outperform\nthe traditional IR retrieval",
    "start": "4451470",
    "end": "4458880"
  },
  {
    "text": "models. If you see this figure,\nso the blue curve here is a traditional IR\napproach, like a BM25 approach.",
    "start": "4458880",
    "end": "4467040"
  },
  {
    "text": "And so the other\ncurve, the orange curve is basically training this kind\nof retrieval using only one",
    "start": "4467040",
    "end": "4472349"
  },
  {
    "text": "solved and question\nanswer pairs. So by looking at all these\ndifferent curves basically using a different number\nof training examples,",
    "start": "4472350",
    "end": "4479430"
  },
  {
    "text": "so it actually largely greatly\noutperforms the traditional IR models. ",
    "start": "4479430",
    "end": "4489640"
  },
  {
    "text": "OK. Again, really I don't\nhave time to talk about the details of\nall this projects,",
    "start": "4489640",
    "end": "4494800"
  },
  {
    "text": "so I just encourage you\nto check out these papers, and I think these results\nare really exciting.",
    "start": "4494800",
    "end": "4502400"
  },
  {
    "text": "So here is actually\na really nice demo. So demo is actually\nhosted on this website",
    "start": "4502400",
    "end": "4508425"
  },
  {
    "text": "if you want to check out. So again, the database\nferries the whole Wikipedia.",
    "start": "4508425",
    "end": "4514058"
  },
  {
    "text": "And you can see that if\nyou ask a question of who tells Harry Potter that he is\na wizard in the Harry Potter",
    "start": "4514058",
    "end": "4520270"
  },
  {
    "text": "series. And the system had\nreally found out the correct article should\nbe Harry Potter films",
    "start": "4520270",
    "end": "4525369"
  },
  {
    "text": "series, and then finally\ngave you the correct answer, which is exactly what\nyou have seen here",
    "start": "4525370",
    "end": "4530650"
  },
  {
    "text": "from the Google example here. So the answer would\nbe the Rubeus Hagrid,",
    "start": "4530650",
    "end": "4535659"
  },
  {
    "text": "which is actually the\nperson who told tells Harry Potter that he's a wizard. So this is actually the perfect\nanswer to this question.",
    "start": "4535660",
    "end": "4542400"
  },
  {
    "text": " OK, I'm going to\nskip this slide.",
    "start": "4542400",
    "end": "4548070"
  },
  {
    "text": "And then finally, very quick. So this is something that\ncame out very recently",
    "start": "4548070",
    "end": "4554940"
  },
  {
    "text": "that some researchers have\ndemonstrated that maybe you don't even need this\nretrieval stage.",
    "start": "4554940",
    "end": "4561700"
  },
  {
    "text": "If you just use a very\nlarge language model, you can also just use\nall open-domain questions answering.",
    "start": "4561700",
    "end": "4567580"
  },
  {
    "text": "So they way they\ndid this is that-- I hope that you have learned the\nT5 model in this class already. So they just take a\npre-trained language model T5.",
    "start": "4567580",
    "end": "4575250"
  },
  {
    "text": "And they are trying to\nfine tune this model by taking the question as the\ninput and the answer as output",
    "start": "4575250",
    "end": "4582960"
  },
  {
    "text": "without any explicit retrieval. And they just fine-tune\nthis data set, and they find this model can do\npretty well at the testing time",
    "start": "4582960",
    "end": "4591090"
  },
  {
    "text": "by just taking the question\nand then directly generate the answer without resorting\nto any documents or a retrieval",
    "start": "4591090",
    "end": "4598530"
  },
  {
    "text": "system. So this is actually\nvery amazing. So this kind of QA model is\nalso called Coastal QA systems.",
    "start": "4598530",
    "end": "4604485"
  },
  {
    "text": " OK. The last slide.",
    "start": "4604485",
    "end": "4611270"
  },
  {
    "text": "So this is one direction\nthat personally, I'm very excited about. So this is actually\na new direction",
    "start": "4611270",
    "end": "4618030"
  },
  {
    "text": "that basically shows that maybe\nfor the open-domain question answering, maybe this\nreader model is also not necessary anymore.",
    "start": "4618030",
    "end": "4626020"
  },
  {
    "text": "So this idea was first\nproposed by Minjoon Seo in 2019 and recently wrote a paper\ncalled dense phrases that",
    "start": "4626020",
    "end": "4633410"
  },
  {
    "text": "tried to demonstrate\nthat maybe you don't even need this reader model.",
    "start": "4633410",
    "end": "4638450"
  },
  {
    "text": "So instead we can just\nrecord all the phrases in Wikipedia using some\nkind of dense vectors.",
    "start": "4638450",
    "end": "4645470"
  },
  {
    "text": "So what you just\nneed to do is just to do this kind of nearest\nneighbor search in the answer space.",
    "start": "4645470",
    "end": "4650690"
  },
  {
    "text": "You just encode all the\nphrases in Wikipedia, encode them using vectors.",
    "start": "4650690",
    "end": "4655730"
  },
  {
    "text": "And by taking a\nquestion, you can just encode this question\nin a single vector and then we can just do the\nnearest neighbor search.",
    "start": "4655730",
    "end": "4663170"
  },
  {
    "text": "And then it can directly\ngive you the answer. So this particular\nnew paradigm is just kind of the question\nanswering model.",
    "start": "4663170",
    "end": "4671090"
  },
  {
    "text": "You just need the retriever,\nyou don't need a reader. So a great advantage\nfor doing this is that for the\nBERT reader model,",
    "start": "4671090",
    "end": "4678630"
  },
  {
    "text": "you essentially have\nto run the BERT model at the inference time. This is actually very expensive. Well, you just get a read\noff the reader model.",
    "start": "4678630",
    "end": "4685909"
  },
  {
    "text": "You can just do the\nsimilarities search, you can just do the\nnearest neighbor search without running a BERT model.",
    "start": "4685910",
    "end": "4691100"
  },
  {
    "text": "So this could be very fast and\nthey can even run on the CPUs without needing to run a very\nexpensive deep neural network",
    "start": "4691100",
    "end": "4699920"
  },
  {
    "text": "and then it can still run very\nwell, perform very, very well.",
    "start": "4699920",
    "end": "4705440"
  },
  {
    "text": "OK, finally, I hope this works. So I actually prepared a\ndemo for the DensePhrases.",
    "start": "4705440",
    "end": "4711540"
  },
  {
    "text": "So I want to show you\nhow this actually works. ",
    "start": "4711540",
    "end": "4723270"
  },
  {
    "text": "So you can see\nthat when you type this question of who won the\nNobel Prize in peace in 2014.",
    "start": "4723270",
    "end": "4729830"
  },
  {
    "text": "So everything just types a\nlittle piece of the input question, and the system\ncan basically just find out",
    "start": "4729830",
    "end": "4736520"
  },
  {
    "text": "the relevant answer and\nthe relevant text passages. And then finally, the\nanswer actually shows up.",
    "start": "4736520",
    "end": "4742040"
  },
  {
    "text": "It's actually very fast because\nit has bigger real time. We don't run the BERT\nmodel, so it's just",
    "start": "4742040",
    "end": "4747680"
  },
  {
    "text": "our retrieval model here. OK. I'm actually done\nwith this lecture.",
    "start": "4747680",
    "end": "4753480"
  },
  {
    "text": "So it is already 5:15 now, yeah. Thanks for joining me today.",
    "start": "4753480",
    "end": "4758630"
  },
  {
    "text": "Thank you very much Danqi for\nthat awesome survey of question answering.",
    "start": "4758630",
    "end": "4763730"
  },
  {
    "text": "I guess given that\ndemo at the end, people want to know\nwhether you're launching your own search engine soon.",
    "start": "4763730",
    "end": "4769850"
  },
  {
    "text": "But at any rate, Danqi can stay\nfor a bit to answer questions,",
    "start": "4769850",
    "end": "4776570"
  },
  {
    "text": "but not forever. but today, because she\ndoesn't have a Stanford login,",
    "start": "4776570",
    "end": "4782510"
  },
  {
    "text": "we're going to do\nquestions inside Zoom. So if you'd like\nto ask a question,",
    "start": "4782510",
    "end": "4788659"
  },
  {
    "text": "if you use the\nraise hand button, we can promote you so that\nyou appear in the regular Zoom",
    "start": "4788660",
    "end": "4794570"
  },
  {
    "text": "window and can just ask\nquestions and see each other.",
    "start": "4794570",
    "end": "4799739"
  },
  {
    "text": "And if you hang\naround and don't leave the Zoom for more\nthan a few minutes,",
    "start": "4799740",
    "end": "4805130"
  },
  {
    "text": "maybe we'll just promote\neverybody who's still there into people in the regular Zoom\nfor some bits of discussion.",
    "start": "4805130",
    "end": "4813830"
  },
  {
    "text": "But we'd welcome\nanyone who'd like to ask a question by asking\nit themselves at this point.",
    "start": "4813830",
    "end": "4820690"
  },
  {
    "text": " OK.",
    "start": "4820690",
    "end": "4826450"
  },
  {
    "text": "I've got one volunteer.  I've got more volunteers.",
    "start": "4826450",
    "end": "4835429"
  },
  {
    "text": "Should I read those questions? Should I look at the chat or? No. So there are now four\npeople who've been promoted.",
    "start": "4835430",
    "end": "4844070"
  },
  {
    "text": "There are four people. [AUDIO OUT] was the\nfirst, so maybe he could start by\nasking a question,",
    "start": "4844070",
    "end": "4850699"
  },
  {
    "text": "and then the other people\nthat we've promoted. OK.",
    "start": "4850700",
    "end": "4856760"
  },
  {
    "text": "So thank you so much\nfor the lecture today. My question is maybe if you\nuse a model like for example",
    "start": "4856760",
    "end": "4863090"
  },
  {
    "text": "BERT, how small can your\ntraining dataset be for you",
    "start": "4863090",
    "end": "4869820"
  },
  {
    "text": "to get reasonable results? So the question is how we can\ntrain the reading comprehension",
    "start": "4869820",
    "end": "4876800"
  },
  {
    "text": "model using only a small\nnumber of training examples.  Yeah, I think this is\na really cool question.",
    "start": "4876800",
    "end": "4883640"
  },
  {
    "text": " It's actually like\nthe, you probably have heard of GPT\nstream model that you",
    "start": "4883640",
    "end": "4890790"
  },
  {
    "text": "showed that if you only\nuse a very few examples, you can also do open-domain\nquestion answering pretty well.",
    "start": "4890790",
    "end": "4897790"
  },
  {
    "text": "But this kind of model is\nhuge, like what number?",
    "start": "4897790",
    "end": "4903850"
  },
  {
    "text": "How many parameters I forgot\nin the GPT Stream model, yeah. So it's a very large\nvariety of of model but--",
    "start": "4903850",
    "end": "4909950"
  },
  {
    "text": "OK. So this is my answer, is\nthat if we can leverage a very large and very powerful\npre-trained language model,",
    "start": "4909950",
    "end": "4918520"
  },
  {
    "text": "there is a possibility that we\ncan actually do the question answering well with only a\nsmall number of examples.",
    "start": "4918520",
    "end": "4925090"
  },
  {
    "text": "And also there are some\nother promising directions, including unsupervised\nquestion answering so",
    "start": "4925090",
    "end": "4931380"
  },
  {
    "text": "by using some kind of\napproach like some form of unsupervised machine\ntranslation, this kind of idea.",
    "start": "4931380",
    "end": "4938800"
  },
  {
    "text": "That can be borrowed-- can borrow the idea\nfrom that and can also",
    "start": "4938800",
    "end": "4945190"
  },
  {
    "text": "work pretty well, reasonably\nwell in unsupervised question answering. ",
    "start": "4945190",
    "end": "4953150"
  },
  {
    "text": "Yeah, also I have\nseen a lot of works like [INAUDIBLE] showing that\nsynthetic Pure-DSS can also",
    "start": "4953150",
    "end": "4959600"
  },
  {
    "text": "help a lot in boosting\nthe performance if you don't have enough\nsupervised examples, yeah.",
    "start": "4959600",
    "end": "4968400"
  },
  {
    "text": "So my question is I guess\nit's kind of interesting that there's not really\nthat strong of a transfer",
    "start": "4968400",
    "end": "4974465"
  },
  {
    "text": "effect between data sets that\nare kind of ostensibly similar.",
    "start": "4974465",
    "end": "4979980"
  },
  {
    "text": "So my question is has there\nbeen any research done on how close I\nguess the formatting",
    "start": "4979980",
    "end": "4988280"
  },
  {
    "text": "and the semantic content\nof these question answering datasets actually\nadheres to the data",
    "start": "4988280",
    "end": "4996260"
  },
  {
    "text": "that BERT is pre trained on? And if so, has there been\nsort of any effect found",
    "start": "4996260",
    "end": "5002440"
  },
  {
    "text": "between those similarities\nor differences?",
    "start": "5002440",
    "end": "5007969"
  },
  {
    "text": "Is the question asking whether\nthere has been like some kind of-- OK, maybe I can just try\nto clarify a little bit why",
    "start": "5007970",
    "end": "5015580"
  },
  {
    "text": "the current models cannot really\ngeneralize well from one data set to another data set, yeah.",
    "start": "5015580",
    "end": "5021400"
  },
  {
    "text": "So I actually truly believe\nthat most existing question answering datasets or\nreading comprehension",
    "start": "5021400",
    "end": "5027950"
  },
  {
    "text": "datasets have been collected\nfrom Mechanical Turk. So it is very difficult to\navoid some kind of artifact",
    "start": "5027950",
    "end": "5036275"
  },
  {
    "text": "though, like a simple clues\nor superficial clues-- let's say not superficial\nbut some simple clues",
    "start": "5036275",
    "end": "5043280"
  },
  {
    "text": "that are for the\nmachines to pick up. So let's take those\nexamples that--",
    "start": "5043280",
    "end": "5048848"
  },
  {
    "text": "so you can see actually if\nyou look at the dataset some more closely, there\nhas been a lot of examples of the\nquestion having",
    "start": "5048848",
    "end": "5055000"
  },
  {
    "text": "more like a large\noverlap in each of the words between the\nquestion and the passage. So the model is actually\nvery good at picking up",
    "start": "5055000",
    "end": "5062480"
  },
  {
    "text": "these kind of clues to\nget very high performance on this dataset.",
    "start": "5062480",
    "end": "5068720"
  },
  {
    "text": "And a lot of dataset drop. So it's basically about\ncomparisons with the two",
    "start": "5068720",
    "end": "5074270"
  },
  {
    "text": "numbers, something like that. So that's the reason that\nmore specialized models that have been trained very\nwell in one data set,",
    "start": "5074270",
    "end": "5081660"
  },
  {
    "text": "it's very easy to pick\nup these kind of clues, and is very hard to\ngeneralize this kind of thing",
    "start": "5081660",
    "end": "5087330"
  },
  {
    "text": "to another data set. What about the natural\nquestions data set? Doesn't that avoid\nthat objection?",
    "start": "5087330",
    "end": "5094949"
  },
  {
    "text": "Yeah. Natural questions\nwill be much better, but there are some other issues.",
    "start": "5094950",
    "end": "5100070"
  },
  {
    "text": "I'm not sure if\nyou have seen that. There is a recent paper\ncalled a question or train",
    "start": "5100070",
    "end": "5105520"
  },
  {
    "text": "test overlap paper. So that basically demonstrate-- Can I just interrupt? So Natural Questions\nwas a dataset",
    "start": "5105520",
    "end": "5112340"
  },
  {
    "text": "that Google put out\nabout a year and a half ago maybe where\nthey were actually taking real questions\nfrom Google search logs",
    "start": "5112340",
    "end": "5121520"
  },
  {
    "text": "and then trying to find answers\nfor them in web documents.",
    "start": "5121520",
    "end": "5127200"
  },
  {
    "text": "Sorry. Go on Danqi.  Yeah, I think that\ndefinitely natural questions",
    "start": "5127200",
    "end": "5133730"
  },
  {
    "text": "is a much better dataset because\nthe questions are natural, like you're collecting\nreal questions that",
    "start": "5133730",
    "end": "5139600"
  },
  {
    "text": "are asked by users. So it kind of avoids this\nkind of superficial artifact",
    "start": "5139600",
    "end": "5145679"
  },
  {
    "text": "between the question\nand the passage. But there's some other\nissues that people like to ask some common questions.",
    "start": "5145680",
    "end": "5152920"
  },
  {
    "text": "So if you just do the remnant\ndebate of the questions in your train, dev, and test.",
    "start": "5152920",
    "end": "5158340"
  },
  {
    "text": "And there's a\nrecent paper showing that there is actually a-- it's inevitable that there is a\nhigh overlap between the train",
    "start": "5158340",
    "end": "5167870"
  },
  {
    "text": "and dev. So if you have\nquestions that you are trying to test in the\ndev set or the test set that",
    "start": "5167870",
    "end": "5175960"
  },
  {
    "text": "has already appeared in the\ntraining set that is not really generalization, right? Yeah, but this is more\non the open-domain",
    "start": "5175960",
    "end": "5183599"
  },
  {
    "text": "setting not in the\nreally [INAUDIBLE] here. Yeah. Do you want to ask a question?",
    "start": "5183600",
    "end": "5191080"
  },
  {
    "text": "Yes. So you mentioned in the last\npart of the presentation that the reader model\nmay not be necessary",
    "start": "5191080",
    "end": "5198340"
  },
  {
    "text": "and you presented the\nDensePhrases which also work well on CPUs. So do we know how well it\nperforms on the question",
    "start": "5198340",
    "end": "5209375"
  },
  {
    "text": "and answering datasets compared\nto other models including",
    "start": "5209375",
    "end": "5214457"
  },
  {
    "text": "BERT and those [INAUDIBLE]\non computer of course. Yeah. I just encourage you to\ncheck out this paper.",
    "start": "5214457",
    "end": "5220185"
  },
  {
    "text": "So this model basically performs\non par with the dense passage",
    "start": "5220185",
    "end": "5226080"
  },
  {
    "text": "retrieval model. So its performance on par\nwith all the retrieval",
    "start": "5226080",
    "end": "5231204"
  },
  {
    "text": "reader models. But it is actually-- so I skipped one slide.",
    "start": "5231205",
    "end": "5236403"
  },
  {
    "text": "So right now, the\nstate of the art is actually dominated by this\nkind of dense passage retrieval",
    "start": "5236403",
    "end": "5241540"
  },
  {
    "text": "plus the generative model. So using a T5 model has\na better data retrieval.",
    "start": "5241540",
    "end": "5248320"
  },
  {
    "text": "This actually\nperformed really well. So I will just say dense\nphrases work basically similar in this block.",
    "start": "5248320",
    "end": "5255040"
  },
  {
    "text": "But compared to this\ngenerative model, we're still a few\npoints behind, yeah. OK.",
    "start": "5255040",
    "end": "5260480"
  },
  {
    "text": "And what is kind\nof the intuition behind the dense phrases\napart from the answers",
    "start": "5260480",
    "end": "5268610"
  },
  {
    "text": "will probably be\nin close proximity? And what if the datasets has\nanswers to a specific question",
    "start": "5268610",
    "end": "5277070"
  },
  {
    "text": "like very far from the\nactual information? ",
    "start": "5277070",
    "end": "5284700"
  },
  {
    "text": "Say, the answers\nto your question may not resided\nin close proximity",
    "start": "5284700",
    "end": "5291030"
  },
  {
    "text": "to the words in the question. So let me just clarify this.",
    "start": "5291030",
    "end": "5298187"
  },
  {
    "text": "OK, the goal of this\nproject is trying to ingest all the\nphrases in the Wikipedia.",
    "start": "5298187",
    "end": "5304920"
  },
  {
    "text": "So these conversations\nare built using the training set of the\nquestion answering datasets.",
    "start": "5304920",
    "end": "5311490"
  },
  {
    "text": "So the assumption is still\nthat the distribution of the examples in\nthe dev and test set",
    "start": "5311490",
    "end": "5316950"
  },
  {
    "text": "would be similar to the\ntraining set for sure. Does this answer your question?",
    "start": "5316950",
    "end": "5322270"
  },
  {
    "text": "So basically, we're\nstill trying to consider all the phrases in Wikipedia. And our test now, we\njust take the question",
    "start": "5322270",
    "end": "5327810"
  },
  {
    "text": "and try to compute\nthe dot products. So if we use say a different\ndata set that does not",
    "start": "5327810",
    "end": "5334840"
  },
  {
    "text": "present the information\nusing the structure presented in Wikipedias, this model\nmay not work as well as.",
    "start": "5334840",
    "end": "5342670"
  },
  {
    "text": " What do you mean by\nstructure represent?",
    "start": "5342670",
    "end": "5348080"
  },
  {
    "text": "So say if we lean more towards\nstructures like the passages",
    "start": "5348080",
    "end": "5355520"
  },
  {
    "text": "we see in standardized\ntests where the answers to the\nquestion may not",
    "start": "5355520",
    "end": "5364430"
  },
  {
    "text": "be in close proximity to\nwhere the information was first introduced.",
    "start": "5364430",
    "end": "5371639"
  },
  {
    "text": "Oh, so the answer doesn't have\nto be seen in the training set. So basically, the goal is\nto take the training set,",
    "start": "5371640",
    "end": "5377250"
  },
  {
    "text": "train and encode\nthem for the phrases. And by using end to end,\nwe apply this encoder",
    "start": "5377250",
    "end": "5382500"
  },
  {
    "text": "to all the phrases, like 60\nbillion phrases in this region.",
    "start": "5382500",
    "end": "5388850"
  },
  {
    "text": "So the model is definitely\nable to generalize from the training set to all\nthe phrases in Wikipedia.",
    "start": "5388850",
    "end": "5394960"
  },
  {
    "text": "So it doesn't have to\nhave seen the phrase then. Is this what you were asking? ",
    "start": "5394960",
    "end": "5404580"
  },
  {
    "text": "I see, OK. ",
    "start": "5404580",
    "end": "5410210"
  },
  {
    "text": "So it's actually similar to the\nretrieval or the dense passage retrieval. So you still try to train\non passage representation,",
    "start": "5410210",
    "end": "5417945"
  },
  {
    "text": "here is the phrase\nrepresentation. But the representation\nis only using the training set of the\nquestion answering datasets.",
    "start": "5417945",
    "end": "5426900"
  },
  {
    "text": "But by taking the\nencoder and then we are going to encode\nall the representation,",
    "start": "5426900",
    "end": "5432438"
  },
  {
    "text": "all the passages of\nphrases in Wikipedia. And then we can use text\nas the representation,",
    "start": "5432438",
    "end": "5437760"
  },
  {
    "text": "can actually generalize well\nfor the unseen questions. Yeah. ",
    "start": "5437760",
    "end": "5444070"
  },
  {
    "text": "So the question is what if\nthe nearest neighbor search doesn't return the answer?",
    "start": "5444070",
    "end": "5449230"
  },
  {
    "text": " So why do you think\nthe nearest neighbor--",
    "start": "5449230",
    "end": "5454450"
  },
  {
    "text": "I mean, you always can\nfind something, right? The question is that whether\nit's close enough or not.",
    "start": "5454450",
    "end": "5460070"
  },
  {
    "text": "Yes. So the question is\nwhat if in the datasets that the answer is\nnot close enough?",
    "start": "5460070",
    "end": "5465340"
  },
  {
    "text": " Yeah, that's a good question. I don't know.",
    "start": "5465340",
    "end": "5470929"
  },
  {
    "text": "If you really come up with\nsomething that is really very far away from\nall the questions that we have been seeing\nin the training set,",
    "start": "5470930",
    "end": "5477952"
  },
  {
    "text": "that could be possible. I don't know. Basically it depend on how\nthe text are formatted.",
    "start": "5477952",
    "end": "5488010"
  },
  {
    "text": "The nearest neighbor\nsearch may not work as well as other models.",
    "start": "5488010",
    "end": "5493740"
  },
  {
    "text": "So again, the question\nthe representation also returns the final\nquestion encoder.",
    "start": "5493740",
    "end": "5500540"
  },
  {
    "text": "So the question is whether\nthis question encoder can give you something\nreasonable in that space or not.",
    "start": "5500540",
    "end": "5507079"
  },
  {
    "text": "But I don't know. So we have been testing\na lot with a random-- even the input\nsentences or even--",
    "start": "5507080",
    "end": "5513650"
  },
  {
    "text": "the question doesn't have\nto be a real question. It could be a sentence. It doesn't seem to be\na problem so far, yeah.",
    "start": "5513650",
    "end": "5520630"
  },
  {
    "text": "Maybe we should give a\ncouple of other people a go. And you're allowed to\nturn your camera on while asking a question if you want.",
    "start": "5520630",
    "end": "5527469"
  },
  {
    "text": "So next person is [AUDIO OUT]. ",
    "start": "5527470",
    "end": "5533090"
  },
  {
    "text": "All right. Hi. Thank you for taking\nthe time to teach this. My question is kind of quick.",
    "start": "5533090",
    "end": "5538310"
  },
  {
    "text": "So you mentioned\nwork, they brought up a set of relatively\nsimple questions that show how brittle or poor\nthe current models can be,",
    "start": "5538310",
    "end": "5546210"
  },
  {
    "text": "right? I'm curious if that--",
    "start": "5546210",
    "end": "5551300"
  },
  {
    "text": "yeah, exactly. Did that kind of\nchange the community to improve how to\nevaluate the models?",
    "start": "5551300",
    "end": "5559310"
  },
  {
    "text": "Because they're actually doing\npretty poorly on some of those, right? Yes.",
    "start": "5559310",
    "end": "5564630"
  },
  {
    "text": "So first these questions\nare simple in terms of the--",
    "start": "5564630",
    "end": "5569820"
  },
  {
    "text": "the wording is very simple,\nthe template is very simple. But they're still trying\nto test a mutation",
    "start": "5569820",
    "end": "5575120"
  },
  {
    "text": "or temporal or\nrelational reference. So the questions are not-- I mean, in terms of the\nreasoning of the capacity,",
    "start": "5575120",
    "end": "5580835"
  },
  {
    "text": "it's not that simple, it's just\nthe wording that's very simple. ",
    "start": "5580835",
    "end": "5586300"
  },
  {
    "text": "OK. So this paper definitely\nreceived a lot of attention, it was the best paper last year\nat the [INAUDIBLE] conference.",
    "start": "5586300",
    "end": "5593550"
  },
  {
    "text": "So I think a lot of people are\ntrying to solve the problem. I cannot tell you that whether\nwe really have a solution",
    "start": "5593550",
    "end": "5600139"
  },
  {
    "text": "for this yet or not, yeah. Cool, yeah, thank you\nfor bringing this one up.",
    "start": "5600140",
    "end": "5606410"
  },
  {
    "text": "It's really interesting. OK. Next is-- ",
    "start": "5606410",
    "end": "5612070"
  },
  {
    "text": "Hi, thanks for taking the time. So my question is\nkind of non-relevant, but like computer robust\nsystem of question answering.",
    "start": "5612070",
    "end": "5620560"
  },
  {
    "text": "In what extent can in\ncontext learning help models to be more robust with\nrespect to different domains?",
    "start": "5620560",
    "end": "5628948"
  },
  {
    "text": "Can you tell me what\nyou mean by in context?",
    "start": "5628948",
    "end": "5634300"
  },
  {
    "text": "So like basically you provide\nthe template generated by BERT.",
    "start": "5634300",
    "end": "5639880"
  },
  {
    "text": "And then instead of\ndirectly predicting the classes of text\nclassifications,",
    "start": "5639880",
    "end": "5646090"
  },
  {
    "text": "you just use some word\nto represent that class or predict the wordings there.",
    "start": "5646090",
    "end": "5652885"
  },
  {
    "text": " OK.",
    "start": "5652886",
    "end": "5657990"
  },
  {
    "text": "So I assume that\nyou are actually referring to the in context\nlearning in logic history",
    "start": "5657990",
    "end": "5663150"
  },
  {
    "text": "or something like that. OK, actually, I have been\nthrough this something related",
    "start": "5663150",
    "end": "5669150"
  },
  {
    "text": "to that recently. But I'm not sure of\nhow we can actually use in context learning\nin everything for SQuAD",
    "start": "5669150",
    "end": "5676679"
  },
  {
    "text": "type of model problems.  Yeah, so I don't know if that is\nstill done, robustness or not.",
    "start": "5676680",
    "end": "5684570"
  },
  {
    "text": "I even [INAUDIBLE] used that\ntechnique for all the question answering use. I see, I see.",
    "start": "5684570",
    "end": "5689755"
  },
  {
    "text": "Thanks. And the you also mentioned\nthat we can train a retriever without a reader.",
    "start": "5689755",
    "end": "5694875"
  },
  {
    "text": "So is there a paper about that\ncurrent attempt to do that?",
    "start": "5694875",
    "end": "5700700"
  },
  {
    "text": "Yeah. The paper is already\nout so, yeah. OK. ",
    "start": "5700700",
    "end": "5708150"
  },
  {
    "text": "Thanks a lot. OK. Next is--",
    "start": "5708150",
    "end": "5713470"
  },
  {
    "text": "Hey, how is it going? Thanks so much for the lecture. A bit of a broader question\nsort of about the future of NLP.",
    "start": "5713470",
    "end": "5724560"
  },
  {
    "text": "Do you think that in order\nto solve NLP in the sense that you can perform on par\nwith humans on all NLP tasks",
    "start": "5724560",
    "end": "5733890"
  },
  {
    "text": "it's sufficient to only\ninteract with text data? Or do you think we'll eventually\nneed the sort of experiences",
    "start": "5733890",
    "end": "5742140"
  },
  {
    "text": "and common sense that you get\nonly from seeing and viewing the world and having\na set of interactions",
    "start": "5742140",
    "end": "5748500"
  },
  {
    "text": "that we as humans have?  Yeah. I mean, common sense is a very\ndifficult-- even in the context",
    "start": "5748500",
    "end": "5755800"
  },
  {
    "text": "question answering, common\nsense is a very important topic that I think still\nremains unresolved.",
    "start": "5755800",
    "end": "5765739"
  },
  {
    "text": "For that part, the\nanswer is definitely yes. And I also want to mention that\nOK, so for a lot of the reading",
    "start": "5765740",
    "end": "5772906"
  },
  {
    "text": "comprehension data sets or\nquestion answering data sets, you have seen that all\nwe QR start to achieve",
    "start": "5772907",
    "end": "5778795"
  },
  {
    "text": "the human performance. But we also see how critical\nthese systems are because they",
    "start": "5778795",
    "end": "5785230"
  },
  {
    "text": "cannot really generalize\nor solve the easy problems. So all these things\nneed to be resolved. ",
    "start": "5785230",
    "end": "5792514"
  },
  {
    "text": "Do you think that the current\nsort of benchmark data sets are maybe a little bit too easy for-",
    "start": "5792514",
    "end": "5799269"
  },
  {
    "text": "[INTERPOSING VOICES] Or just like that. Yeah. One final thought is that having\na lot of transversely trying",
    "start": "5799269",
    "end": "5807960"
  },
  {
    "text": "to have a lot of humans in\nthe loop of the frameworks to evaluate these\nkind of systems.",
    "start": "5807960",
    "end": "5814020"
  },
  {
    "text": "Just try to break\nthe current system, come up with some\nharder questions.",
    "start": "5814020",
    "end": "5820260"
  },
  {
    "text": "So yeah, that means maybe\nthe current static data sets are not good enough to\nmeasure the progress.",
    "start": "5820260",
    "end": "5825420"
  },
  {
    "text": "So we actually really need\nsome kind of dynamic evaluation and also introduce more just\nkind of adverse examples,",
    "start": "5825420",
    "end": "5833130"
  },
  {
    "text": "harder questions also,\nthings like that, yeah. Are you still game for\na couple more questions?",
    "start": "5833130",
    "end": "5840160"
  },
  {
    "text": "Sure. I did want to mention it is\n9:10 PM on the East Coast.",
    "start": "5840160",
    "end": "5846210"
  },
  {
    "text": "OK. So next is-- Hi.",
    "start": "5846210",
    "end": "5851270"
  },
  {
    "text": "Thanks so much for the lecture. So in 2020, there was this\nefficient open-domain question",
    "start": "5851270",
    "end": "5858800"
  },
  {
    "text": "answering challenge. And from performance\nit seemed like there",
    "start": "5858800",
    "end": "5865040"
  },
  {
    "text": "was quite substantial decrease\nversus human accuracy.",
    "start": "5865040",
    "end": "5870230"
  },
  {
    "text": "Probably primarily due to\nthe quantization and drift",
    "start": "5870230",
    "end": "5876053"
  },
  {
    "text": "that occurred when\nthey were quantizing. So I recently\nencountered this paper",
    "start": "5876053",
    "end": "5884090"
  },
  {
    "text": "called Learnable Quantizers,\nwhich essentially learns",
    "start": "5884090",
    "end": "5890270"
  },
  {
    "text": "baseless representation\nfor the quantizers jointly with the\nleads of the network.",
    "start": "5890270",
    "end": "5897230"
  },
  {
    "text": "And while this would\nbe extremely effective if you were to just like,\nsay, train from scratch,",
    "start": "5897230",
    "end": "5904363"
  },
  {
    "text": "I was just sort\nof curious, do you think there is some way to do\nthis say a pre-trained BERT",
    "start": "5904363",
    "end": "5911860"
  },
  {
    "text": "model or something like that? I had a few ideas with like\nbeam search for instance, but I don't see a very\nclear way of doing that.",
    "start": "5911860",
    "end": "5921210"
  },
  {
    "text": " Yeah. I don't think I'm really an\nexpert to answer that question.",
    "start": "5921210",
    "end": "5928105"
  },
  {
    "text": "Yeah, I'm not sure if I\nreally have the answer, but I also want\nto quickly mention",
    "start": "5928105",
    "end": "5933180"
  },
  {
    "text": "that quantization\nhas been definitely a very useful technique\nto make the model smaller. So we have been also\nexploring the quantization",
    "start": "5933180",
    "end": "5940920"
  },
  {
    "text": "in the DensePhrases project\nrecently because the storage",
    "start": "5940920",
    "end": "5947636"
  },
  {
    "text": "has been still very large. So we actually have been\ntrying to reduce that storage.",
    "start": "5947637",
    "end": "5953190"
  },
  {
    "text": "Yeah, I'm not sure\nabout the question about the connection\nbetween quantization and also pre-training.",
    "start": "5953190",
    "end": "5959220"
  },
  {
    "text": "I'm not sure. Yeah, I'm not sure I have\nan answer to that for you. Sorry. Thank you. Thanks.",
    "start": "5959220",
    "end": "5965190"
  },
  {
    "text": "Danqi is too modest\nto mention that she was one of the co-organizers\nof the EfficientQA task.",
    "start": "5965190",
    "end": "5973390"
  },
  {
    "text": "OK. Next question is-- ",
    "start": "5973390",
    "end": "5980200"
  },
  {
    "text": "Hi Danqi. Thanks so much for\nbeing here today. So my question is\na bit different.",
    "start": "5980200",
    "end": "5986570"
  },
  {
    "text": "So one example you gave\nthat caught my attention with this Alex Victoria\nexample in the checklist.",
    "start": "5986570",
    "end": "5992397"
  },
  {
    "text": "And I was thinking technically\nAlex wasn't the wrong answer, right? It's gender neutral\nand there wasn't enough context in the\nquestion to determine",
    "start": "5992397",
    "end": "6000960"
  },
  {
    "text": "who it's referring to. So my question is how\nconcerned should we be about potential encoding\nsort of biases into these record",
    "start": "6000960",
    "end": "6010500"
  },
  {
    "text": "labels or how we\nevaluate them, or is that just more of a concern for\nmore open ended questions?",
    "start": "6010500",
    "end": "6016730"
  },
  {
    "text": " Yeah, this is definitely\nvery important. Again, a lot of\npeople are trying",
    "start": "6016730",
    "end": "6023660"
  },
  {
    "text": "to study, OK, how much bias has\nbeen encoded in these models and how we can--\nyeah, I'm not sure I",
    "start": "6023660",
    "end": "6031340"
  },
  {
    "text": "have a good answer to that. Again, I just want\nto say some people do",
    "start": "6031340",
    "end": "6036630"
  },
  {
    "text": "a de-biasing of the\npre-trained language models, all these things\nare very important. ",
    "start": "6036630",
    "end": "6043260"
  },
  {
    "text": "And this is just one-- so you're\ntalking about this example, right? So this is just one test case. ",
    "start": "6043260",
    "end": "6051809"
  },
  {
    "text": "Yeah, I don't know, yeah. Right, so yeah, I guess I'm\njust a little worried about who comes up with the test cases?",
    "start": "6051810",
    "end": "6057635"
  },
  {
    "text": "Who determines what the\nright answer is, thank you.  I mean, we will have more\ndiscussion of toxicity and bias",
    "start": "6057635",
    "end": "6066690"
  },
  {
    "text": "coming up very soon, including\nactually Thursday's lecture as well as a later lecture, not\nspecifically about QA though.",
    "start": "6066690",
    "end": "6074890"
  },
  {
    "text": "OK. Next person is-- ",
    "start": "6074890",
    "end": "6079940"
  },
  {
    "text": "Thank you for the lecture. Yeah, my question is also\nrelated to the open domain",
    "start": "6079940",
    "end": "6085548"
  },
  {
    "text": "question answering. So I was just wondering how much\nof the learning side of domain",
    "start": "6085548",
    "end": "6096030"
  },
  {
    "text": "sort of generalization or\ndomain alignment techniques",
    "start": "6096030",
    "end": "6101280"
  },
  {
    "text": "can be combined with language\nlevel, like question answering.",
    "start": "6101280",
    "end": "6108969"
  },
  {
    "text": "Like to what extent\nwould they work, and what kind of\nlanguage specific design",
    "start": "6108970",
    "end": "6115030"
  },
  {
    "text": "should you leverage to combine\nwith those to sort of like--",
    "start": "6115030",
    "end": "6120280"
  },
  {
    "text": "if we want like\nhigher performance and stuff like that?",
    "start": "6120280",
    "end": "6125344"
  },
  {
    "text": "Is the question about\nhow to generalize between different domains about\nhow to design open domain QA",
    "start": "6125345",
    "end": "6131260"
  },
  {
    "text": "system for different languages? I'm not sure I get that. Sorry. Just wondering.",
    "start": "6131260",
    "end": "6137250"
  },
  {
    "text": "So there is some very specific\ndesigns like domain server",
    "start": "6137250",
    "end": "6144820"
  },
  {
    "text": "alignments and efficient level\ndisentanglement techniques",
    "start": "6144820",
    "end": "6150579"
  },
  {
    "text": "that has shown some interesting\nperformance on other tasks.",
    "start": "6150580",
    "end": "6157600"
  },
  {
    "text": "And I saw recently some people\nalso leveraged similar things",
    "start": "6157600",
    "end": "6163210"
  },
  {
    "text": "for question answering. So I was just wondering to what\nextent these kind of techniques",
    "start": "6163210",
    "end": "6170260"
  },
  {
    "text": "can work on one\ngroup of tasks, not just limited to\nquestion answering,",
    "start": "6170260",
    "end": "6176140"
  },
  {
    "text": "but mainly question answering. Sorry.",
    "start": "6176140",
    "end": "6181370"
  },
  {
    "text": "Which work are\nyou talking about? I'm still not sure. What do you mean by disentangled\nsolutions or question",
    "start": "6181370",
    "end": "6187070"
  },
  {
    "text": "answering.  This is a little bit\nmore specific for--",
    "start": "6187070",
    "end": "6194670"
  },
  {
    "text": "so there's a paper\ncalled domain--",
    "start": "6194670",
    "end": "6200570"
  },
  {
    "text": "I forgot the exact name. It's sort of adversarially\naligned to domains",
    "start": "6200570",
    "end": "6207261"
  },
  {
    "text": "by disentangling-- [INAUDIBLE] Got it. ",
    "start": "6207262",
    "end": "6215210"
  },
  {
    "text": "OK. I just want to be sure that\nwe are all on the same page. So I have seem some\nwork sort of trying",
    "start": "6215210",
    "end": "6220497"
  },
  {
    "text": "to learn some kind of\ndisentangled representations that can better generalize\nto the different domains",
    "start": "6220497",
    "end": "6226160"
  },
  {
    "text": "on adversarial examples. Is this what you're saying? Yeah, yeah. And the question is\nwhether this technique",
    "start": "6226160",
    "end": "6232670"
  },
  {
    "text": "can be generally applied\nto question answering or?",
    "start": "6232670",
    "end": "6237780"
  },
  {
    "text": "Yeah, I'm just wondering to\nwhat extent will they work. Because I think language\nhas a lot of specific things",
    "start": "6237780",
    "end": "6246620"
  },
  {
    "text": "like dependency and other\nsorts like these techniques does not actually take care of.",
    "start": "6246620",
    "end": "6254270"
  },
  {
    "start": "6254270",
    "end": "6260530"
  },
  {
    "text": "Yeah, I'm not sure. I think we have to try that. But it's definitely\nan interesting point.",
    "start": "6260530",
    "end": "6266110"
  },
  {
    "text": "Yeah, I don't know. At least for the work\nthat I have seen so far, it all applied or operated\nat a very simple sentence",
    "start": "6266110",
    "end": "6274240"
  },
  {
    "text": "classification task. Maybe that's not correct. So [INAUDIBLE] is a\nbasic type of a encoder",
    "start": "6274240",
    "end": "6282810"
  },
  {
    "text": "applied to a simple\nclassification task and take a hidden\nrepresentation through some kind of a transformation\nand make sure",
    "start": "6282810",
    "end": "6289880"
  },
  {
    "text": "that it can learn some kind\nof envrironment features that the hidden representation\nis talking about.",
    "start": "6289880",
    "end": "6296040"
  },
  {
    "text": "Right, right, yeah, cool. Cool. Fair input. Yeah, I'm not sure. I feel like QA is a\nmore structured task",
    "start": "6296040",
    "end": "6303770"
  },
  {
    "text": "and also handles\nlonger bar sequences. ",
    "start": "6303770",
    "end": "6309810"
  },
  {
    "text": "Yeah, so I don't\nknow if it works unless people have tried that. Thank you, thank you.",
    "start": "6309810",
    "end": "6316190"
  },
  {
    "text": "OK then we've got-- and maybe we should call\nthis the last question. ",
    "start": "6316190",
    "end": "6322099"
  },
  {
    "text": "Hi, I'm just wondering what\nis the intrinsic difference between solving\nquestion answering",
    "start": "6322100",
    "end": "6327665"
  },
  {
    "text": "with generative models like\nT5 versus encoders like BERT.",
    "start": "6327665",
    "end": "6335730"
  },
  {
    "text": "OK. That's a good point. OK, so I skipped\nthis slide still.",
    "start": "6335730",
    "end": "6341489"
  },
  {
    "text": "Why this model works so well. The reason is, actually,\nit's not really about the extractive model\nversus generative model.",
    "start": "6341490",
    "end": "6348000"
  },
  {
    "text": "The reason is that\nthey [INAUDIBLE] for the extractive model.",
    "start": "6348000",
    "end": "6353570"
  },
  {
    "text": "So if the retrieval returns\nlike, say, 100 passages, so they have to\nextract the answer",
    "start": "6353570",
    "end": "6358980"
  },
  {
    "text": "from each of the\npassages and then finally figure out which one\nhas the highest score.",
    "start": "6358980",
    "end": "6364320"
  },
  {
    "text": "But for the generative\nmodel, essentially they are trying to\naggregate all the 100 passages and the\ndense representations",
    "start": "6364320",
    "end": "6370469"
  },
  {
    "text": "together and do the\ngeneration jointly. You understand?",
    "start": "6370470",
    "end": "6375522"
  },
  {
    "text": "So essentially, you're taking\nthe 100 representations together through\nthe joint generation instead of only\ndo the extraction",
    "start": "6375522",
    "end": "6381960"
  },
  {
    "text": "from each of the passages. So I think that is actually\nthe key difference. So that's why this\ngenerative model",
    "start": "6381960",
    "end": "6387930"
  },
  {
    "text": "can do really well compared\nto the extractive models. So I also want to mention that--",
    "start": "6387930",
    "end": "6393532"
  },
  {
    "text": "OK, so if you look at this\nRAG model, it's actually--",
    "start": "6393532",
    "end": "6399240"
  },
  {
    "text": "compare this DPR and RAG model. So RAG model is always\ndoing the generative model. But they are not doing\nthis kind of aggregation.",
    "start": "6399240",
    "end": "6406380"
  },
  {
    "text": "They're just trying to\ntake on the single passage and then doing the generation. So the RAG model\nactually doesn't",
    "start": "6406380",
    "end": "6412710"
  },
  {
    "text": "perform as well as this model. By the way, I also want to\nmention RAG model is actually not doing better than DPR\nbecause it's base model,",
    "start": "6412710",
    "end": "6419070"
  },
  {
    "text": "this is large model. So these numbers are a\nlittle bit confusing. So it's actually\nbasically really on par.",
    "start": "6419070",
    "end": "6425430"
  },
  {
    "text": "Their base perform similarity. So the key difference\nbetween the generative model",
    "start": "6425430",
    "end": "6430500"
  },
  {
    "text": "and the extractive model is\nthat for generative models, you can actually leverage more\ninput passage together and do",
    "start": "6430500",
    "end": "6436530"
  },
  {
    "text": "the generation. Is that clear or not?",
    "start": "6436530",
    "end": "6443019"
  },
  {
    "text": "Yes. Thanks. Yeah, otherwise, you should\njust check out this paper here.",
    "start": "6443020",
    "end": "6449650"
  },
  {
    "text": "So this paper\nactually explains it pretty well why this\nmodel works better than the previous\ngenerative model.",
    "start": "6449650",
    "end": "6454730"
  },
  {
    "text": " So are these generative\nmodels do a simple question",
    "start": "6454730",
    "end": "6463270"
  },
  {
    "text": "answering over documents and\nwhere there is span prediction, can you use generative\nmodels to generate the span?",
    "start": "6463270",
    "end": "6470627"
  },
  {
    "text": " You can. Yeah. You can definitely do that.",
    "start": "6470627",
    "end": "6478360"
  },
  {
    "text": "So you are talking\nabout this T5 model. Yeah. So I'm just wondering\nabout if you use encoders,",
    "start": "6478360",
    "end": "6485950"
  },
  {
    "text": "it's like you're\nfinding similarities between the encodings. And then generative\nmodels are you're",
    "start": "6485950",
    "end": "6492750"
  },
  {
    "text": "remembering the whole\nquestion and you try to retrieve the memory\nwhen you answer the question.",
    "start": "6492750",
    "end": "6505750"
  },
  {
    "text": "OK. So for this model, there\nisn't any retrieval. So you can always find the\nanswer from the question,",
    "start": "6505750",
    "end": "6511980"
  },
  {
    "text": "right? So this model really\nhas you relying on all the parameters you\nmemorized, all the information.",
    "start": "6511980",
    "end": "6518610"
  },
  {
    "text": "So by just taking\nthis input, it has to just rely on the parameters\nto infer this answer.",
    "start": "6518610",
    "end": "6524429"
  },
  {
    "text": "So it's actually very hard to-- yeah, it's a definite\nbalance between memory",
    "start": "6524430",
    "end": "6530070"
  },
  {
    "text": "and the generalization\nfrom it, yeah. I see. So I'm just going\nto say what I--",
    "start": "6530070",
    "end": "6538110"
  },
  {
    "text": "like when you bar this question,\nit's embedding in some space. And using that\nembedding, the generator",
    "start": "6538110",
    "end": "6546210"
  },
  {
    "text": "matches that with [INAUDIBLE]. Is that what is\ngoing on in there?",
    "start": "6546210",
    "end": "6553262"
  },
  {
    "text": "Exactly, yes. The model is very large,\nlike 11 billion parameters.",
    "start": "6553262",
    "end": "6559837"
  },
  {
    "text": "So the parameters\nare basically trying to memorize a lot of\ninformation that has been.",
    "start": "6559837",
    "end": "6565739"
  },
  {
    "text": "Because the model has been\npre-trained on the text and also has been fine-tuned. So the model has been trying to\nmemorize a lot of information",
    "start": "6565740",
    "end": "6572760"
  },
  {
    "text": "from the text, yeah. All right, thanks. Do you want to call\nit a night or do",
    "start": "6572760",
    "end": "6578184"
  },
  {
    "text": "you want one more question?  Either way, yeah.",
    "start": "6578185",
    "end": "6584430"
  },
  {
    "text": "It's up to you. Sure, I'll take\none more question. OK. Let me just do--",
    "start": "6584430",
    "end": "6590349"
  },
  {
    "text": "OK. One more question from-- OK. The first question is about\nhow easily are these techniques",
    "start": "6590350",
    "end": "6598750"
  },
  {
    "text": "generalized to other\nlanguages or say languages that follow quite\ndifferent grammatical rules",
    "start": "6598750",
    "end": "6606550"
  },
  {
    "text": "like Chinese,\nJapanese, or Arabic, or some other languages?",
    "start": "6606550",
    "end": "6611740"
  },
  {
    "text": "Another followup question\nmaybe not exactly in your domain\nexpertise is there's",
    "start": "6611740",
    "end": "6617860"
  },
  {
    "text": "a lot of interest in\nmodelling user behavior, so online searching behavior\nor browsing behavior",
    "start": "6617860",
    "end": "6624070"
  },
  {
    "text": "as a sequence using, say,\ntransformers of attention. And then you can use that\nto predict how users--",
    "start": "6624070",
    "end": "6631660"
  },
  {
    "text": "like embed users, so that you\ncan can predict user's actions. How promising do you\nthink that would be?",
    "start": "6631660",
    "end": "6638560"
  },
  {
    "text": "I know this may not be\nyour domain expertise, but there is a lot of interest\nin extending these question",
    "start": "6638560",
    "end": "6644710"
  },
  {
    "text": "answering techniques or just\nencoding techniques, embedding techniques to\nrecommender systems.",
    "start": "6644710",
    "end": "6652070"
  },
  {
    "text": "I just want get your\nthoughts on that. OK.",
    "start": "6652070",
    "end": "6657830"
  },
  {
    "text": "The first question is whether\nthese techniques can be generalized to other languages.",
    "start": "6657830",
    "end": "6663020"
  },
  {
    "text": "I think the answer is yes. There has been a lot of actively\nsearching is this section.",
    "start": "6663020",
    "end": "6668300"
  },
  {
    "text": "But there has been\nsome constraints that a lot of models or\nsystems that I described here",
    "start": "6668300",
    "end": "6673940"
  },
  {
    "text": "actually require a\nlot of them, require very strong pre-trained\nlanguage model",
    "start": "6673940",
    "end": "6679220"
  },
  {
    "text": "and also requires\nlots of training examples for the Pure-DSS.",
    "start": "6679220",
    "end": "6684290"
  },
  {
    "text": "So that'll be\nactually I would say a bottleneck for many low\nresource languages, right?",
    "start": "6684290",
    "end": "6691410"
  },
  {
    "text": "So it's very hard to\ncollect so many examples for other languages. If we have, actually, I\nthink that the techniques",
    "start": "6691410",
    "end": "6698660"
  },
  {
    "text": "can be generally applied\nto other languages. And there has been\nalso a lot of work",
    "start": "6698660",
    "end": "6704100"
  },
  {
    "text": "trying to do cross-lingual\nquestion answering and stuff like that. ",
    "start": "6704100",
    "end": "6712235"
  }
]