[
  {
    "start": "0",
    "end": "5600"
  },
  {
    "text": "OK, hi, everyone. I'll be getting started. OK, so it's Tuesday of week two.",
    "start": "5600",
    "end": "11800"
  },
  {
    "text": "So hopefully, that\nmeans everyone has done the assignment one. Everyone done assignment one?",
    "start": "11800",
    "end": "17935"
  },
  {
    "text": " If I'm saying this, I'm probably\nsaying it to the wrong people,",
    "start": "17935",
    "end": "24360"
  },
  {
    "text": "but it seems like every\nyear, some people blow some of their late\ndays on assignment one,",
    "start": "24360",
    "end": "32840"
  },
  {
    "text": "and it's really just the\nwrong place to use them. So yeah, hopefully, you've all\ndone assignment one and did",
    "start": "32840",
    "end": "40700"
  },
  {
    "text": "not-- this is meant to\nbe the easy on-ramp, and then we go straight on\nfrom that, so that out today,",
    "start": "40700",
    "end": "48140"
  },
  {
    "text": "we have assignment two. So assignment two\nhas two purposes.",
    "start": "48140",
    "end": "54859"
  },
  {
    "text": "Purpose one is to make you do\nsome math, some understanding",
    "start": "54860",
    "end": "62120"
  },
  {
    "text": "of what neural networks really\ncompute and how they compute it. And that's what I'm going\nto talk about today,",
    "start": "62120",
    "end": "68290"
  },
  {
    "text": "is also going through that math. But then\nsimultaneously-- maybe it does three things\nin assignment two.",
    "start": "68290",
    "end": "75570"
  },
  {
    "text": "So we're going to be\nlearning something about dependency\nparsing, which will be, actually, something\nabout language structure",
    "start": "75570",
    "end": "82650"
  },
  {
    "text": "and linguistics. But then thirdly,\nfor assignment two, we're going to\nstart using PyTorch.",
    "start": "82650",
    "end": "89170"
  },
  {
    "text": "So PyTorch is one of the\nleading software frameworks for deep learning and\nthe one that we're",
    "start": "89170",
    "end": "94530"
  },
  {
    "text": "going to use for this class. So for-- I mean, the\nassignment three PyTorch",
    "start": "94530",
    "end": "102570"
  },
  {
    "text": "is exceedingly scaffolded, so\nit's sort of, here's this thing,",
    "start": "102570",
    "end": "110310"
  },
  {
    "text": "and you have to write these two\nlines, use these two functions. But nevertheless, for help\npeople get up to speed and get",
    "start": "110310",
    "end": "119460"
  },
  {
    "text": "started using PyTorch on\nFriday at 3:30 in gates B01",
    "start": "119460",
    "end": "124800"
  },
  {
    "text": "or it'll also,\nagain, be recorded. We have a tutorial\non PyTorch, and so that's a great way to get\nmore of a sense of PyTorch",
    "start": "124800",
    "end": "133050"
  },
  {
    "text": "and how it works before\ndoing assignment two. Yeah, the other things--",
    "start": "133050",
    "end": "141090"
  },
  {
    "text": "yeah, so for nearly\nall the lectures, we've got further reading\nof places that you can look.",
    "start": "141090",
    "end": "147480"
  },
  {
    "text": "Of all the classes in the entire\nquarter, this, for many people,",
    "start": "147480",
    "end": "154440"
  },
  {
    "text": "might be a really good one to\nlook at the suggested readings. We have several readings, which\nare sort of shorter tutorials",
    "start": "154440",
    "end": "162180"
  },
  {
    "text": "and reviews of\nthe kind of matrix calculus and linear algebra\nthat we need for this class.",
    "start": "162180",
    "end": "170440"
  },
  {
    "text": "So really encourage\nyou to look at those. If you decide that\none is your favorite,",
    "start": "170440",
    "end": "176440"
  },
  {
    "text": "you can tell us on\nEd which one you think is the best one\nto choose between them. I kind of like the one\nthat's first on the list,",
    "start": "176440",
    "end": "182890"
  },
  {
    "text": "but maybe you'll\nfeel differently. Yeah. Conversely, yeah, so\ntoday will be all math",
    "start": "182890",
    "end": "190910"
  },
  {
    "text": "and then Thursday will be\nall language and linguistics.",
    "start": "190910",
    "end": "196650"
  },
  {
    "text": "Some people find the language\nand linguistics hard, as well, so I guess different\nkinds of people.",
    "start": "196650",
    "end": "203570"
  },
  {
    "text": "OK, so getting straight into it. So where we started\nlast time, I'd",
    "start": "203570",
    "end": "211330"
  },
  {
    "text": "shown these baby neural\nnetworks and sort of said, well, we can think of each of\nthose orange things as basically",
    "start": "211330",
    "end": "218330"
  },
  {
    "text": "a little logistic\nregression unit. And the crucial difference\nfrom then the kind",
    "start": "218330",
    "end": "223879"
  },
  {
    "text": "of statistics machine learning\nyou see in the stats class, 109",
    "start": "223880",
    "end": "229340"
  },
  {
    "text": "or wherever, is\nthat in those, you have one logistic regression,\nand you're defining the input",
    "start": "229340",
    "end": "236060"
  },
  {
    "text": "features to it, and you've got\nsome decision variable that you want to have at the output.",
    "start": "236060",
    "end": "243390"
  },
  {
    "text": "Here, you're building\nthese cascades of little logistic regressions. And so the idea is\nright at the end,",
    "start": "243390",
    "end": "250550"
  },
  {
    "text": "we're going to\ndefine what we want. We're going to capture that by\nour objective function or loss",
    "start": "250550",
    "end": "255650"
  },
  {
    "text": "function. But for the stuff in the\nmiddle, that stuff in the middle",
    "start": "255650",
    "end": "260838"
  },
  {
    "text": "is going to be a chance\nfor the neural network to learn by itself what\nwould be useful inputs",
    "start": "260839",
    "end": "269030"
  },
  {
    "text": "to further downstream neurons. What kind of functions\nshould I come up",
    "start": "269030",
    "end": "275599"
  },
  {
    "text": "with in terms of\nmy inputs that will help me provide useful outputs\nto help the final computation",
    "start": "275600",
    "end": "284630"
  },
  {
    "text": "down the track? And if you haven't seen and\nthought about this much before,",
    "start": "284630",
    "end": "292140"
  },
  {
    "text": "I mean, I think\nit's worth sitting with that idea for a moment\nbecause this is really",
    "start": "292140",
    "end": "298370"
  },
  {
    "text": "a super powerful idea, which is\nwhat's made neural networks more",
    "start": "298370",
    "end": "303590"
  },
  {
    "text": "powerful in most circumstances\nthan other forms of machine learning. The fact that you have\nthis self-organization",
    "start": "303590",
    "end": "312030"
  },
  {
    "text": "of intermediate levels\nof representation that you use to compute\nthings that will be useful",
    "start": "312030",
    "end": "318030"
  },
  {
    "text": "downstream for what you\neventually want to do. The other reason I bring\nback up this picture is I've",
    "start": "318030",
    "end": "325470"
  },
  {
    "text": "wanted to go straight\nfrom here to matrices.",
    "start": "325470",
    "end": "330720"
  },
  {
    "text": "So while you could wire together\nneurons however you wanted to,",
    "start": "330720",
    "end": "336900"
  },
  {
    "text": "and arguably, if you\nlook at human brains, they look more like neurons\nwired together however",
    "start": "336900",
    "end": "343020"
  },
  {
    "text": "you wanted to, but for what's\ndone with neural networks, basically, there's\nalways this kind",
    "start": "343020",
    "end": "348330"
  },
  {
    "text": "of regular structure of layers. So once we have this\nregular structure of layers,",
    "start": "348330",
    "end": "354220"
  },
  {
    "text": "we are taking the output of one\nof our neurons at one layer,",
    "start": "354220",
    "end": "361890"
  },
  {
    "text": "and we're feeding them\ntogether with weights to produce the inputs\nto the next layer.",
    "start": "361890",
    "end": "370129"
  },
  {
    "text": "So we're taking the\nx1, x2, x3 outputs. We're multiplying\nthem all by weights.",
    "start": "370130",
    "end": "375650"
  },
  {
    "text": "We're adding a bias term. And then we're going to put\nit through a nonlinearity,",
    "start": "375650",
    "end": "382090"
  },
  {
    "text": "and that will give us the\nvalue at the next layer. So if we then collapse that to\na vector and this to a vector,",
    "start": "382090",
    "end": "389889"
  },
  {
    "text": "that then collapses into a\ncomputation that, first of all, we're doing a matrix\nmultiplication.",
    "start": "389890",
    "end": "397430"
  },
  {
    "text": "We're calculating\nWx of the inputs, and then we're\nadding on the biases",
    "start": "397430",
    "end": "403150"
  },
  {
    "text": "as a vector of\nbiases, which gives us this intermediate value, z.",
    "start": "403150",
    "end": "408430"
  },
  {
    "text": "And then we have this\nnonlinearity or activation function, which is\napplied to that,",
    "start": "408430",
    "end": "416659"
  },
  {
    "text": "which gives us the\nvalues in the next layer of the neural network. And the activation function\nis applied to a vector",
    "start": "416660",
    "end": "423669"
  },
  {
    "text": "and produces a vector,\nbut it's operating on each of the individual\ncomponents of that vector one",
    "start": "423670",
    "end": "429910"
  },
  {
    "text": "at a time. So we've got some\nscalar function that we're just applying to\neach element of the vector.",
    "start": "429910",
    "end": "437710"
  },
  {
    "text": "And so that's the\nkind of picture we saw when I did\nthis example, and I'm",
    "start": "437710",
    "end": "444070"
  },
  {
    "text": "going to continue to use this\nexample in today's class. Remember, we were\ngoing to decide whether the word in the\nmiddle of the input window",
    "start": "444070",
    "end": "451990"
  },
  {
    "text": "was a location or not. And so we were doing the matrix\nmultiplication, putting it",
    "start": "451990",
    "end": "458290"
  },
  {
    "text": "through the nonlinearity. We then are just doing\na dot product here, and then we're going-- that\ngot stuck into a sigmoid",
    "start": "458290",
    "end": "468280"
  },
  {
    "text": "to predict yes or no. And the final thing I wanted\nto say a little bit about",
    "start": "468280",
    "end": "474970"
  },
  {
    "text": "is these f's, the nonlinearity\nor the activation function. Where do they come in?",
    "start": "474970",
    "end": "481610"
  },
  {
    "text": "Well, the starting\npoint of where they came in the history\nof neural networks",
    "start": "481610",
    "end": "487030"
  },
  {
    "text": "is when people came\nup with this idea that, well, you could represent\nthe operation of a basic neuron",
    "start": "487030",
    "end": "495460"
  },
  {
    "text": "by doing a matrix\nmultiplication of the inputs and then having a bias term, or\nhere, it represents a threshold",
    "start": "495460",
    "end": "505030"
  },
  {
    "text": "term, to see whether the\nneurons should fire or not. That was actually in the\nvery first implementation,",
    "start": "505030",
    "end": "512570"
  },
  {
    "text": "which dates back to the 1940s. It was done as a threshold\nso that if the activation was",
    "start": "512570",
    "end": "521620"
  },
  {
    "text": "greater than theta, you output\n1, otherwise, you output 0.",
    "start": "521620",
    "end": "527029"
  },
  {
    "text": "And well, if you\nhave a threshold, the two lines are flat, right?",
    "start": "527030",
    "end": "533130"
  },
  {
    "text": "So there is no slope,\nthere is no gradient. So that actually makes\nlearning much harder.",
    "start": "533130",
    "end": "541110"
  },
  {
    "text": "So the whole secret of what\nwe build with neural networks, and in particular, an\nalternative name that's",
    "start": "541110",
    "end": "548270"
  },
  {
    "text": "popular in some\ncircles these days, is gradient-based learning. And the entire idea of\ngradient-based learning",
    "start": "548270",
    "end": "556610"
  },
  {
    "text": "is if we actually\nhave some slopes, then it's like going\nskiing during spring break.",
    "start": "556610",
    "end": "562560"
  },
  {
    "text": "You can work out\nwhere it's steeper, and you can head down\nwhere it's steeper, and that will allow us\nto optimize our function",
    "start": "562560",
    "end": "570829"
  },
  {
    "text": "to learn much more quickly. And so that's one reason\nthat we don't just",
    "start": "570830",
    "end": "576820"
  },
  {
    "text": "want to have threshold units. We want to have things with\nslopes, so we have gradients.",
    "start": "576820",
    "end": "583370"
  },
  {
    "text": "So in subsequent\nwork, people started using activation\nfunctions with slopes.",
    "start": "583370",
    "end": "590529"
  },
  {
    "text": "And so the first popular one\nwas this sigmoid or logistic that we've seen from\nmapping to probabilities.",
    "start": "590530",
    "end": "598400"
  },
  {
    "text": "But it's sort of\nimperfect, it seems, because the output was\nalways non-negative.",
    "start": "598400",
    "end": "605389"
  },
  {
    "text": "So that sort of tends to push\nthings towards bigger numbers. So there was quite a bit of use,\nthen, of this tanh function,",
    "start": "605390",
    "end": "616389"
  },
  {
    "text": "and you'll actually see tanh. When we do assignment\nthree, we'll be using tanh as in our\ncurrent neural networks.",
    "start": "616390",
    "end": "623920"
  },
  {
    "text": "And so I've written\nthere the formula",
    "start": "623920",
    "end": "629560"
  },
  {
    "text": "I usually give for tanh\nin terms of exponentials. Yeah, if your math\nis rusty, it's",
    "start": "629560",
    "end": "635750"
  },
  {
    "text": "not obvious that tanh\nand logistic have much to do with each\nother, but if you",
    "start": "635750",
    "end": "640790"
  },
  {
    "text": "want to treat this\nas a math problem, that a tanh is literally\njust a rescaled logistic.",
    "start": "640790",
    "end": "648000"
  },
  {
    "text": "You're stretching it by 2\nand moving it down by 1. It's the same function.",
    "start": "648000",
    "end": "654759"
  },
  {
    "text": "OK, so that's nice, but if\nyou're calculating tanh's, you",
    "start": "654760",
    "end": "660280"
  },
  {
    "text": "have to do all of\nthese exponentials, and exponentials are kind of\nslow on your computer and things",
    "start": "660280",
    "end": "666910"
  },
  {
    "text": "like that. You might wonder whether\nyou couldn't get away with something much cheaper.",
    "start": "666910",
    "end": "672230"
  },
  {
    "text": "And so people thought\nabout that and thought, maybe we could just use\na so-called hard tanh,",
    "start": "672230",
    "end": "678310"
  },
  {
    "text": "where it has a slope\nof 1 in the middle and is then just flat\noutside that area.",
    "start": "678310",
    "end": "684200"
  },
  {
    "text": "And that seemed to\nwork in many cases. And so that then led\nto the popularity",
    "start": "684200",
    "end": "690490"
  },
  {
    "text": "of the rectified linear unit. So the rectified linear unit is\nsimply 0 in the negative region",
    "start": "690490",
    "end": "698810"
  },
  {
    "text": "and then is y equals x\nin the positive region. Now, this seems kind\nof wonky and goes",
    "start": "698810",
    "end": "706149"
  },
  {
    "text": "against what I was saying\nabout gradient-based learning because once you're in\nthe negative region, there's no gradient.",
    "start": "706150",
    "end": "712450"
  },
  {
    "text": "You're just dead. But in the positive\nregion, there is gradient,",
    "start": "712450",
    "end": "718070"
  },
  {
    "text": "and the gradient is\nparticularly simple, right? The slope is always 1.",
    "start": "718070",
    "end": "723589"
  },
  {
    "text": "And so this still feels\nslightly perverse to me,",
    "start": "723590",
    "end": "729090"
  },
  {
    "text": "but this really became\nthe norm of what people use for a number of\nyears because people found",
    "start": "729090",
    "end": "736790"
  },
  {
    "text": "that although for an\nindividual neuron, it was dead half the time. Any time it went\nnegative, that, overall,",
    "start": "736790",
    "end": "742790"
  },
  {
    "text": "for your neural networks,\nsome things would be alive. So it kind of gave a\nform of specialization.",
    "start": "742790",
    "end": "750120"
  },
  {
    "text": "And the fact that the\nslope was always 1 meant that you got\nreally easy, productive,",
    "start": "750120",
    "end": "757260"
  },
  {
    "text": "backward flow of gradients in\na way we'll talk about later. And so learning with ReLU\nturned out to be very effective,",
    "start": "757260",
    "end": "767070"
  },
  {
    "text": "and people started using the\nReLU nonlinearity everywhere, and it sort of became\nthe default in the norm.",
    "start": "767070",
    "end": "774329"
  },
  {
    "text": "And you'll see us using\nit in the assignments. In particular, we use\nit in assignment two, and so you'll get to\nsee that it works.",
    "start": "774330",
    "end": "780930"
  },
  {
    "text": "But nevertheless, at\nsome point, people sort of had second\nthoughts and decided,",
    "start": "780930",
    "end": "787600"
  },
  {
    "text": "having it dead over\nhalf of its range maybe isn't such a good idea\nafter all, even though it seemed",
    "start": "787600",
    "end": "792800"
  },
  {
    "text": "to work great for a few years. And so a lot of what's\nhappened since then is then to come up with\nother functions which",
    "start": "792800",
    "end": "800690"
  },
  {
    "text": "are, in some sense, ReLU-like,\nbut not actually dead. ",
    "start": "800690",
    "end": "809500"
  },
  {
    "text": "OK, I don't really-- Yeah, here we go. So one version of that is\nthe so-called leaky ReLU.",
    "start": "809500",
    "end": "818209"
  },
  {
    "text": "So for the leaky ReLU,\nyou make the negative half a straight line, as well,\nwith a very minor slope.",
    "start": "818210",
    "end": "825200"
  },
  {
    "text": "But still, it's got a\nlittle bit of slope. There is then a variant of that\ncalled the parametric ReLU,",
    "start": "825200",
    "end": "830630"
  },
  {
    "text": "where you have one extra\nparameter, which is actually what the slope of\nthe negative part is,",
    "start": "830630",
    "end": "836720"
  },
  {
    "text": "and people showed some\npositive results with that. More recently, again, and\nthis is what you often",
    "start": "836720",
    "end": "843610"
  },
  {
    "text": "see in recent\ntransformer models, is you see nonlinearities\nlike swish and jello.",
    "start": "843610",
    "end": "853310"
  },
  {
    "text": "So both of these\nare fancy functions, but kind of what they both look\nlike is basically, this is y",
    "start": "853310",
    "end": "861670"
  },
  {
    "text": "equals x to all\nintents and purposes, not quite, but approximately. And then you've got\nsome funky bit of curve",
    "start": "861670",
    "end": "867670"
  },
  {
    "text": "down here, which, again,\ngives you a bit of slope. The curve is going\nthe opposite way.",
    "start": "867670",
    "end": "873270"
  },
  {
    "text": "That's a bit funny,\nbut they seem to work well, commonly used\nin recent transformer models.",
    "start": "873270",
    "end": "880490"
  },
  {
    "text": "So that's a bit of a dump of all\nthe nonlinearities people use.",
    "start": "880490",
    "end": "886529"
  },
  {
    "text": "I mean, the details of that\naren't super important right now. But the important thing\nto have in your head",
    "start": "886530",
    "end": "894350"
  },
  {
    "text": "is, why do we need\nnonlinearities? And the way to\nthink about that is",
    "start": "894350",
    "end": "900620"
  },
  {
    "text": "that what we're doing\nwith neural networks is function approximation. There's some very complex\nfunction that we want to learn,",
    "start": "900620",
    "end": "909230"
  },
  {
    "text": "like maybe we want to go from\na piece of text to its meaning, or we want to be\ninterpreting visual scenes",
    "start": "909230",
    "end": "915170"
  },
  {
    "text": "or something like that. And so we want to build really\ngood function approximators.",
    "start": "915170",
    "end": "921530"
  },
  {
    "text": "And well, if you're just\ndoing matrix multiplies, a matrix multiply of a\nvector is a linear transform.",
    "start": "921530",
    "end": "928860"
  },
  {
    "text": "So that doesn't let you\nmultiply complex functions. I guess strictly, if you\nput a bias on the end,",
    "start": "928860",
    "end": "934830"
  },
  {
    "text": "it's then an affine transform,\nbut let's keep it simple. Linear transforms, right? So if you're doing\nmultiple matrix multiplies,",
    "start": "934830",
    "end": "944130"
  },
  {
    "text": "you're doing multiple\nlinear transforms. But they compose, so you could\nhave just multiplied these two",
    "start": "944130",
    "end": "949760"
  },
  {
    "text": "matrices together, and you would\nhave a single linear transform. So you get no power in\nterms of representation",
    "start": "949760",
    "end": "956930"
  },
  {
    "text": "by having multilayer networks\nthat are just matrix multiplies.",
    "start": "956930",
    "end": "962945"
  },
  {
    "text": "As a little aside, in terms\nof representational power, having multilayer matrix\nmultiplies gives you no power.",
    "start": "962945",
    "end": "972270"
  },
  {
    "text": "But if you think about\nit in terms of learning, actually, it does\ngive you some power. So in the theoretical community,\nlooking at neural networks,",
    "start": "972270",
    "end": "979808"
  },
  {
    "text": "there are actually\nquite a few papers that look at linear\nneural networks, meaning that they're\njust sequences",
    "start": "979808",
    "end": "985670"
  },
  {
    "text": "of the multiplies\nwith no nonlinearities because they have interesting\nlearning properties,",
    "start": "985670",
    "end": "991310"
  },
  {
    "text": "even though they give you\nno representational power. OK, but we'd like to be able\nto learn functions like this,",
    "start": "991310",
    "end": "998330"
  },
  {
    "text": "not only functions like this. And to be able to learn\nfunctions like this, we need more than\nlinear transforms.",
    "start": "998330",
    "end": "1004980"
  },
  {
    "text": "And we achieve those\nby having something that makes us be calculating\na nonlinear function.",
    "start": "1004980",
    "end": "1012130"
  },
  {
    "text": "And it's these\nactivation functions that give us nonlinear functions.",
    "start": "1012130",
    "end": "1017709"
  },
  {
    "text": "OK, cool. OK, so then getting on to today.",
    "start": "1017710",
    "end": "1024730"
  },
  {
    "text": "So the whole thing we want to do\nnow is gradient-based learning. This is our stochastic\ngradient descent equation,",
    "start": "1024730",
    "end": "1031410"
  },
  {
    "text": "where here, that\nupside down triangle symbol, that's our gradient.",
    "start": "1031410",
    "end": "1037630"
  },
  {
    "text": "We're wanting to work out the\nslope of our objective function. And so this is how\nwe're going to learn,",
    "start": "1037630",
    "end": "1044819"
  },
  {
    "text": "by calculating gradients. So what we want to know is, how\ndo we calculate the gradients",
    "start": "1044819",
    "end": "1050220"
  },
  {
    "text": "for an arbitrary function? And so what I want to do\ntoday is, first of all,",
    "start": "1050220",
    "end": "1055419"
  },
  {
    "text": "do this by hand for math\nand then discuss, how do we",
    "start": "1055420",
    "end": "1062220"
  },
  {
    "text": "do it computationally? Which is effectively\nthe famous thing that's taken as\npowering under powering",
    "start": "1062220",
    "end": "1070080"
  },
  {
    "text": "all of neural nets, which is\nthe back propagation algorithm. But the back\npropagation algorithm",
    "start": "1070080",
    "end": "1075120"
  },
  {
    "text": "is just automating the math. OK. And so for the math,\nit's matrix calculus.",
    "start": "1075120",
    "end": "1082340"
  },
  {
    "text": "And at this point, then there's\na huge spectrum between people who know much more math than\nme and people who barely ever",
    "start": "1082340",
    "end": "1089919"
  },
  {
    "text": "learnt this. But I hope to explain\nthe essentials",
    "start": "1089920",
    "end": "1095980"
  },
  {
    "text": "or remind people of\nthem enough that you're at least at a starting point\nfor reading some other stuff",
    "start": "1095980",
    "end": "1103630"
  },
  {
    "text": "and doing homework two. So let's get into that. And so I'm going\nto spend about half",
    "start": "1103630",
    "end": "1108970"
  },
  {
    "text": "the time on those two halves. And the hope is that\nafter this, you'll",
    "start": "1108970",
    "end": "1114279"
  },
  {
    "text": "feel like, I actually understand\nhow neural networks work under the hood, fingers crossed.",
    "start": "1114280",
    "end": "1119330"
  },
  {
    "text": "OK, so here we go. So if you're a Stanford\nstudent, you maybe did math 51,",
    "start": "1119330",
    "end": "1128560"
  },
  {
    "text": "or else you could have\ndone math 51, which teaches linear algebra,\nmultivariate calculus,",
    "start": "1128560",
    "end": "1134830"
  },
  {
    "text": "and modern applications. Math 51 covers everything I'm\ngoing to talk about and way more",
    "start": "1134830",
    "end": "1141100"
  },
  {
    "text": "stuff. So if you actually know\nthat and remember it, you can look at Instagram\nfor the next 35 minutes.",
    "start": "1141100",
    "end": "1148179"
  },
  {
    "text": "But I think the problem is\nthat quite apart from the fact",
    "start": "1148180",
    "end": "1153560"
  },
  {
    "text": "a lot of people do\nit as frosh, this is a lot to get\nthrough in 10 weeks.",
    "start": "1153560",
    "end": "1158960"
  },
  {
    "text": "And I think that a\nlot of the people who do this class, by 2 years later,\ndon't really have much ability",
    "start": "1158960",
    "end": "1164770"
  },
  {
    "text": "to use any of it. But if you actually looked\nat this book really hard and for a very,\nreally long time,",
    "start": "1164770",
    "end": "1171770"
  },
  {
    "text": "you would have\ndiscovered that actually, right towards the end of\nthe book in appendix G,",
    "start": "1171770",
    "end": "1177730"
  },
  {
    "text": "there's actually an\nappendix on neural networks and the multivariable\nchain rule,",
    "start": "1177730",
    "end": "1183000"
  },
  {
    "text": "which is precisely what\nwe're going to be using for doing our neural networks.",
    "start": "1183000",
    "end": "1189390"
  },
  {
    "text": "But there are only two problems. One problem is that this\nis page 697 of the book,",
    "start": "1189390",
    "end": "1195149"
  },
  {
    "text": "and I'm not sure anyone\never gets that far. And the other problem is,\neven if you do get that far,",
    "start": "1195150",
    "end": "1202759"
  },
  {
    "text": "I don't know. I find these pages that they're\nreally dense, texty pages. It's not even easy to understand\nthem if you have gone there.",
    "start": "1202760",
    "end": "1210540"
  },
  {
    "text": "So here's my attempt on that. So the mantra to have\nin your head is, gee,",
    "start": "1210540",
    "end": "1217070"
  },
  {
    "text": "if I can remember basic\nsingle variable calculus, that I've got 3x squared and\nthe derivative of that is 6x,",
    "start": "1217070",
    "end": "1225780"
  },
  {
    "text": "that's all you need to know. The mantra is multivariable\ncalculus is just",
    "start": "1225780",
    "end": "1232520"
  },
  {
    "text": "like single variable calculus,\nexcept you're using matrices.",
    "start": "1232520",
    "end": "1237620"
  },
  {
    "text": "OK, so that's our\narticle of faith, and we're going to do that. And so what we're\nwanting to do is",
    "start": "1237620",
    "end": "1243049"
  },
  {
    "text": "to do matrix calculus on the\ngeneralization of that tensor calculus, using vectors,\nmatrices, and higher order",
    "start": "1243050",
    "end": "1251270"
  },
  {
    "text": "tensors, because\nif we can do things and what's referred to\nas vectorized gradients",
    "start": "1251270",
    "end": "1256610"
  },
  {
    "text": "in the neural network\nworld, that that will be the fast, efficient\nway to do our operations.",
    "start": "1256610",
    "end": "1264740"
  },
  {
    "text": "So if you want to\nthink it all through, you can do it single\nvariable at a time and check that you're\ndoing the right thing.",
    "start": "1264740",
    "end": "1271980"
  },
  {
    "text": "And I sort of tried to indicate\nthat in the first lecture. But if we want to have\nour networks go vroom,",
    "start": "1271980",
    "end": "1278040"
  },
  {
    "text": "we want to be doing\nmatrix calculus. OK, so let's work\nup to doing that.",
    "start": "1278040",
    "end": "1284610"
  },
  {
    "text": "OK, so this is the part that\nI trust everyone can remember,",
    "start": "1284610",
    "end": "1289750"
  },
  {
    "text": "right? So we have f of\nx equals x cubed, and we can do single\nvariable derivative,",
    "start": "1289750",
    "end": "1298429"
  },
  {
    "text": "and the derivative\nis 3x squared.",
    "start": "1298430",
    "end": "1303450"
  },
  {
    "text": "Everyone remember that one? OK. That's something we\ncan all start from. And remember, this derivative\nis saying the slope of things,",
    "start": "1303450",
    "end": "1311909"
  },
  {
    "text": "right? So the slope of things\nlets us work out, where is something steep?",
    "start": "1311910",
    "end": "1317520"
  },
  {
    "text": "So we'll be able to go skiing. That's our goal, right? And so you can think\nof the slope of things",
    "start": "1317520",
    "end": "1325460"
  },
  {
    "text": "as how much the output\nwill change if we change the input a bit, right? That's our measure of steepness.",
    "start": "1325460",
    "end": "1334710"
  },
  {
    "text": "So since the derivative is 3x\nsquared, if we're at x equals 1, that means the slope is about\nthree times 1 squared 3.",
    "start": "1334710",
    "end": "1344160"
  },
  {
    "text": "So if I work out the value\nof the function for 1.01, it's gone up by\nabout three times.",
    "start": "1344160",
    "end": "1351290"
  },
  {
    "text": "I move the x by 0.01 and\nthe output move by 0.03, where if I go to x\nequals 4, the derivative",
    "start": "1351290",
    "end": "1359630"
  },
  {
    "text": "is 3 times 4 squared is 48. And so if I work out the\nvalue of the function at 4.01,",
    "start": "1359630",
    "end": "1365760"
  },
  {
    "text": "I get approximately\n64.48 versus 64. That small difference\nfrom 4 to 4.01",
    "start": "1365760",
    "end": "1374179"
  },
  {
    "text": "has been magnified 48\ntimes in the output. OK, so now, we just\nremember the mantra.",
    "start": "1374180",
    "end": "1382880"
  },
  {
    "text": "It's going to be exactly the\nsame as single-value calculus, but with more stuff.",
    "start": "1382880",
    "end": "1388800"
  },
  {
    "text": "So if we have a\nfunction with n inputs, we're then going to work\nout its gradient, which",
    "start": "1388800",
    "end": "1396200"
  },
  {
    "text": "is its partial derivative\nwith respect to each input. So its gradient will now be\na vector of the same size",
    "start": "1396200",
    "end": "1405140"
  },
  {
    "text": "as the number of inputs. And there's this funky\nsymbol, which people pronounce various ways.",
    "start": "1405140",
    "end": "1411990"
  },
  {
    "text": "I mean, this kind of\noriginated as some kind of someone's weird way of\ndrawing a calligraphic D, right?",
    "start": "1411990",
    "end": "1418250"
  },
  {
    "text": "So it is really a D. So I think\ncall mainly just call it D, but sometimes, people\ncall it partial",
    "start": "1418250",
    "end": "1424080"
  },
  {
    "text": "or funky D or some\nother name, right? So you have df/dx1, df/dx2\nfor each of the variables.",
    "start": "1424080",
    "end": "1432540"
  },
  {
    "text": "OK, so if we go beyond that\nand then have a function",
    "start": "1432540",
    "end": "1437550"
  },
  {
    "text": "with n inputs and\nm outputs, what we",
    "start": "1437550",
    "end": "1442830"
  },
  {
    "text": "then get for the\ngradient is what's referred to as the Jacobian.",
    "start": "1442830",
    "end": "1449470"
  },
  {
    "text": "Now, actually, the dude this is\nnamed after was a German Jew,",
    "start": "1449470",
    "end": "1454809"
  },
  {
    "text": "so it should really be Jacobi. But no one says that in\nthis country, Jacobian.",
    "start": "1454810",
    "end": "1463410"
  },
  {
    "text": "OK, so the Jacobian is then a\nmatrix of partial derivatives,",
    "start": "1463410",
    "end": "1470160"
  },
  {
    "text": "where you're working out, for\neach output and each input, the partial derivative\nbetween the component",
    "start": "1470160",
    "end": "1477870"
  },
  {
    "text": "of the input and the output. So this looks like\nthe kind of thing that we're going to have\nwhen we have a neural network",
    "start": "1477870",
    "end": "1485620"
  },
  {
    "text": "layer because we're going\nto have n inputs and m outputs for the two layers\nof our neural network.",
    "start": "1485620",
    "end": "1492800"
  },
  {
    "text": "So we'll be using these\nkind of Jacobians. OK, so then the whole\nidea of neural networks",
    "start": "1492800",
    "end": "1501370"
  },
  {
    "text": "is we've got these\nmultilevel computations,",
    "start": "1501370",
    "end": "1506500"
  },
  {
    "text": "and they're going to correspond\nto composition of functions. So we need to know\nhow to compose things,",
    "start": "1506500",
    "end": "1514730"
  },
  {
    "text": "both for calculating\nfunctions and for calculating their gradients. So if we have a one\nvariable function,",
    "start": "1514730",
    "end": "1522280"
  },
  {
    "text": "and we want to work out\nits derivative in terms",
    "start": "1522280",
    "end": "1528040"
  },
  {
    "text": "of a composition of two\nfunctions, what we're doing is multiplying our computations.",
    "start": "1528040",
    "end": "1536180"
  },
  {
    "text": "OK, so if you compose\ntogether z of y,",
    "start": "1536180",
    "end": "1542240"
  },
  {
    "text": "that's the function that we\ndid at the beginning, that gives you-- was it?",
    "start": "1542240",
    "end": "1547460"
  },
  {
    "text": "No, it's not. Sorry, it's different. OK, z of y gives you 3x squared.",
    "start": "1547460",
    "end": "1552640"
  },
  {
    "text": "And so we know that the\nderivative of that is 6x.",
    "start": "1552640",
    "end": "1557720"
  },
  {
    "text": "OK, but if we do it in\nterms of the pieces, we can work out dz/dy,\nwhich is just going to be 3,",
    "start": "1557720",
    "end": "1568039"
  },
  {
    "text": "and dy/dx is 2x. And we can work out the total\nderivative by multiplying these",
    "start": "1568040",
    "end": "1575600"
  },
  {
    "text": "two pieces, and we get 6x,\nthe same answer, right? So matrix calculus is exactly\nlike single-variable calculus,",
    "start": "1575600",
    "end": "1586440"
  },
  {
    "text": "except we're using\ntensors of different-- so the word tensor\nis used to mean,",
    "start": "1586440",
    "end": "1593310"
  },
  {
    "text": "as you go up that spectrum\nin its size, so from scalar to vector to matrix to then\nwhat, in computer science,",
    "start": "1593310",
    "end": "1602090"
  },
  {
    "text": "is normally still is\nmultidimensional arrays, that spectrum is then tensors\nof different dimensions.",
    "start": "1602090",
    "end": "1611730"
  },
  {
    "text": "OK, so when we have\nmultiple variable functions, we're going to\nmultiply Jacobians.",
    "start": "1611730",
    "end": "1618300"
  },
  {
    "text": "So here, we have a function,\nWx plus b, and then we compose the\nnonlinearity f to get h.",
    "start": "1618300",
    "end": "1629070"
  },
  {
    "text": "And so we're going to be able\nto compute that in the same way as a product of these\npartial derivatives, which",
    "start": "1629070",
    "end": "1636500"
  },
  {
    "text": "are Jacobians. OK, so let's start looking at\na few examples of what we get.",
    "start": "1636500",
    "end": "1643490"
  },
  {
    "text": "So let's count. We start with an elementwise\nactivation function. So when we have a vector\nthat's being calculated",
    "start": "1643490",
    "end": "1653140"
  },
  {
    "text": "as the activation function of\na previously computed quantity, well, we're computing\nthat componentwise,",
    "start": "1653140",
    "end": "1661160"
  },
  {
    "text": "as I explained before. So hi equals f of\nzi, where the f",
    "start": "1661160",
    "end": "1667000"
  },
  {
    "text": "is our activation function that\nactually applies to a scalar. But overall, this layer is a\nfunction with n outputs and n",
    "start": "1667000",
    "end": "1675640"
  },
  {
    "text": "inputs, and so it's going\nto have an n by n Jacobian. And well, what that's\ngoing to-- so this is",
    "start": "1675640",
    "end": "1681910"
  },
  {
    "text": "our definition of the Jacobian. But in this case, this\nis sort of a special case",
    "start": "1681910",
    "end": "1689380"
  },
  {
    "text": "because if i equals\nj, then we're going to have the output,\nthe hj, depending on zi.",
    "start": "1689380",
    "end": "1703919"
  },
  {
    "text": "And otherwise,\nit's going to be 0, because for the\noff-diagonal entries, it doesn't matter how\nyou change the value.",
    "start": "1703920",
    "end": "1710470"
  },
  {
    "text": "It's not changing the output\nbecause the output only depends on the corresponding index.",
    "start": "1710470",
    "end": "1715960"
  },
  {
    "text": "And so what we're going to get\nfor this Jacobian of activation functions is a matrix\nwhere everything is 0,",
    "start": "1715960",
    "end": "1723100"
  },
  {
    "text": "apart from the\ndiagonal terms that correspond to where we're\ncalculating the activation",
    "start": "1723100",
    "end": "1730230"
  },
  {
    "text": "function. And for those ones,\nwe're going to have to work out how to compute the\nderivative of our activation",
    "start": "1730230",
    "end": "1738000"
  },
  {
    "text": "function. That was on assignment one, one\nof the questions on assignment one, I do believe.",
    "start": "1738000",
    "end": "1744030"
  },
  {
    "text": "Or was it on assignment two? No, no, it's the assignment two. One of the questions\non assignment two.",
    "start": "1744030",
    "end": "1749350"
  },
  {
    "text": "I got that wrong. One of the ones on the new\nassignment is say, hey, can you work out the derivative\nof a logistic function?",
    "start": "1749350",
    "end": "1757480"
  },
  {
    "text": "Well, then we'd be able to plug\nthat straight into f prime. So I'm not going to give\nthat answer away today.",
    "start": "1757480",
    "end": "1763920"
  },
  {
    "text": "OK, so other things that we\nwant to do with Jacobian is,",
    "start": "1763920",
    "end": "1770920"
  },
  {
    "text": "well, we have this layer\nof our neural network, where we're\ncalculating Wx plus b,",
    "start": "1770920",
    "end": "1778710"
  },
  {
    "text": "and we can want to work out\nthe partial derivative of that with respect to x.",
    "start": "1778710",
    "end": "1784200"
  },
  {
    "text": "This is the kind of\nplace where it actually works to remember\nthe mantra and say,",
    "start": "1784200",
    "end": "1790270"
  },
  {
    "text": "matrix calculus is just like\nsingle-variable calculus, but with matrices.",
    "start": "1790270",
    "end": "1796540"
  },
  {
    "text": "So if you just don't use your\nbrain too hard and think, it's just like single-variable\ncalculus, so what should the answer be?",
    "start": "1796540",
    "end": "1802660"
  },
  {
    "text": "It's obviously going\nto be W, right? And indeed, it is. Similarly, if we want to\ndo the same thing for Wx, b",
    "start": "1802660",
    "end": "1809790"
  },
  {
    "text": "and work out the partial\nderivative with respect to b, well that\nwould be 1 in terms",
    "start": "1809790",
    "end": "1816270"
  },
  {
    "text": "of single-variable calculus,\nand so in matrix calculus, that becomes an identity matrix.",
    "start": "1816270",
    "end": "1822530"
  },
  {
    "text": "OK, it's slightly\ndifferent, but same idea. But that's reflecting the\nfact that b is actually",
    "start": "1822530",
    "end": "1828130"
  },
  {
    "text": "a vector, so we need\nit to be coming out as an identity matrix.",
    "start": "1828130",
    "end": "1834970"
  },
  {
    "text": "OK, so higher up in\nmy example picture,",
    "start": "1834970",
    "end": "1840169"
  },
  {
    "text": "I did this vector\ndot product of uTh.",
    "start": "1840170",
    "end": "1845950"
  },
  {
    "text": "And well, what happens if we\nwork out the Jacobian of that?",
    "start": "1845950",
    "end": "1855899"
  },
  {
    "text": "What we end up with strictly\nis we come out with hT.",
    "start": "1855900",
    "end": "1862340"
  },
  {
    "text": "And this is sort of like\nwhen you're working out-- well, we did this\nin the first class",
    "start": "1862340",
    "end": "1868730"
  },
  {
    "text": "when we did a dot\nproduct calculation that you kind of get for\neach individual element,",
    "start": "1868730",
    "end": "1875460"
  },
  {
    "text": "you get the opposite\nterm, and so you get the other\nvector coming out.",
    "start": "1875460",
    "end": "1881360"
  },
  {
    "text": "These are good ones, so\ncompute at home for practice to make sure you really\ndo know the answers",
    "start": "1881360",
    "end": "1887900"
  },
  {
    "text": "and why they work\nout the way they do. OK, so let's go back to\nour little neural net.",
    "start": "1887900",
    "end": "1896030"
  },
  {
    "text": "This was most of our neural net. Above our neural net,\nthere was the nonlinearity.",
    "start": "1896030",
    "end": "1903110"
  },
  {
    "text": "Now, I'm going to leave\nthat out of this time. See, I got it wrong. It's on assignment two.",
    "start": "1903110",
    "end": "1909760"
  },
  {
    "text": "But normally, you'd\nbe calculating the partials of the\noutput, the loss function,",
    "start": "1909760",
    "end": "1916330"
  },
  {
    "text": "with respect to the inputs. But since the loss function\nis on assignment two,",
    "start": "1916330",
    "end": "1921500"
  },
  {
    "text": "I'm going to leave\nthat out, and I'm just going to calculate derivatives\nwith respect to this score that",
    "start": "1921500",
    "end": "1927820"
  },
  {
    "text": "feeds into the loss function. So we've, first of all, got\nthe neural network layer,",
    "start": "1927820",
    "end": "1933650"
  },
  {
    "text": "the nonlinearity, and then we're\ndoing this dot product to work out a score for each\nposition, which feeds",
    "start": "1933650",
    "end": "1940270"
  },
  {
    "text": "into the logistic function. So if we want to work\nout ds/db, so that's",
    "start": "1940270",
    "end": "1946330"
  },
  {
    "text": "with respect to the bias first,\nso the way we do it is we break up our equations\ninto our individual pieces",
    "start": "1946330",
    "end": "1955159"
  },
  {
    "text": "that are composed together. And so that means\nwe break this up. So we first calculate\nthe z equals Wx plus b,",
    "start": "1955160",
    "end": "1962549"
  },
  {
    "text": "then we apply the\nactivation function to the different components. OK, then after that,\nwhat we remember to do",
    "start": "1962550",
    "end": "1973309"
  },
  {
    "text": "is to work out our\npartial derivatives of s",
    "start": "1973310",
    "end": "1982190"
  },
  {
    "text": "with respect to b, that\nwhat we're going to be doing is doing the product of\nthe partial derivatives",
    "start": "1982190",
    "end": "1990380"
  },
  {
    "text": "of the component pieces. So we're applying\nthe matrix calculus version of the chain rule.",
    "start": "1990380",
    "end": "1997140"
  },
  {
    "text": "So ds/db equals ds/dh\ntimes dh/dz times dz/db,",
    "start": "1997140",
    "end": "2005140"
  },
  {
    "text": "which corresponds to\nthese three layers that are composed together. And so at that point, we\nremember our useful Jacobians",
    "start": "2005140",
    "end": "2014220"
  },
  {
    "text": "from the previous slide,\nand we can just apply them. So the top one, ds/dh,\nis the u transpose.",
    "start": "2014220",
    "end": "2024789"
  },
  {
    "text": " Well, maybe it's u. Let's come back to that.",
    "start": "2024790",
    "end": "2030540"
  },
  {
    "text": "There's a fine\npoint on that that I will explain more about later. ",
    "start": "2030540",
    "end": "2036600"
  },
  {
    "text": "OK, then for the dh/dz,\nthat was the activation function where we\ngot the diagonal",
    "start": "2036600",
    "end": "2043559"
  },
  {
    "text": "of the derivative of f of z. And then for dz/db, that's\nwhere we got the identity",
    "start": "2043560",
    "end": "2051419"
  },
  {
    "text": "function, OK? So we can simplify that down.",
    "start": "2051420",
    "end": "2057190"
  },
  {
    "text": "And so what that's\ngoing to end up as is the u transpose,\nthat funny symbol there,",
    "start": "2057190",
    "end": "2065070"
  },
  {
    "text": "times the vector\nelementwise derivative of f.",
    "start": "2065070",
    "end": "2072195"
  },
  {
    "text": "This symbol, which\ndoesn't normally turn up in your\nregular math course,",
    "start": "2072195",
    "end": "2077800"
  },
  {
    "text": "but turns up all the\ntime in neural networks, is referred to as\nthe Hadamard product.",
    "start": "2077800",
    "end": "2083429"
  },
  {
    "text": "And the Hadamard\nproduct is meaning elementwise multiplication. So it's not like a\ncross product where",
    "start": "2083429",
    "end": "2089739"
  },
  {
    "text": "you put two vectors\ntogether, and you get out one number, a scalar. You put two vectors together.",
    "start": "2089739",
    "end": "2095419"
  },
  {
    "text": "You elementwise\nmultiply them all, and you're left with another\nvector of the same type.",
    "start": "2095420",
    "end": "2102339"
  },
  {
    "text": "OK, so now, this\ngave us a working out of the partials of ds/db.",
    "start": "2102340",
    "end": "2108160"
  },
  {
    "text": "And for a neural\nnetwork, we want to work out all the\nother partials, as well.",
    "start": "2108160",
    "end": "2113870"
  },
  {
    "text": "So overall here in the picture,\nwe had the x, the W, the b,",
    "start": "2113870",
    "end": "2123730"
  },
  {
    "text": "and the u. And we'd like to work\nout partials with respect",
    "start": "2123730",
    "end": "2129099"
  },
  {
    "text": "to all of those variables,\nso we can change their values and learn so that our\nmodel predicts better.",
    "start": "2129100",
    "end": "2137500"
  },
  {
    "text": "So suppose we now want\nto calculate ds/dW.",
    "start": "2137500",
    "end": "2143770"
  },
  {
    "text": "So again, we can split it\nup with the same chain rule and say ds/dW equals the\nproduct of these three things.",
    "start": "2143770",
    "end": "2152270"
  },
  {
    "text": "And the important\nthing to notice is that two of\nthose three things were exactly the same ones\nthat we calculated before.",
    "start": "2152270",
    "end": "2159680"
  },
  {
    "text": "The only bit that's\ndifferent is that at the end, we're now doing dz/dW\nrather than dz/db.",
    "start": "2159680",
    "end": "2167270"
  },
  {
    "text": "And so the first central\nidea that we'll come back to when we do\ncomputation graphs is,",
    "start": "2167270",
    "end": "2174110"
  },
  {
    "text": "we really want to avoid\ndoing repeated work. So we want to realize that\nthose two parts of things",
    "start": "2174110",
    "end": "2180859"
  },
  {
    "text": "are the same. And since we're just multiplying\nthese partial derivatives",
    "start": "2180860",
    "end": "2185960"
  },
  {
    "text": "together, we can just compute\nwhat that part is and reuse it. And so if we want to--",
    "start": "2185960",
    "end": "2194360"
  },
  {
    "text": "wait, yeah. OK, so if we're wanting\nto calculate ds/dW,",
    "start": "2194360",
    "end": "2200130"
  },
  {
    "text": "the part that's the\nsame, this part here, we can refer to as delta.",
    "start": "2200130",
    "end": "2206280"
  },
  {
    "text": "So delta is the upstream\ngradient or the error signal, the part that you\ngot from starting",
    "start": "2206280",
    "end": "2212630"
  },
  {
    "text": "at the beginning, ds/dh,\ndh/dz, this sort of shared upstream part, we can\ncalculate that once",
    "start": "2212630",
    "end": "2220580"
  },
  {
    "text": "and then we can\nuse it to calculate both of these two things.",
    "start": "2220580",
    "end": "2226430"
  },
  {
    "text": "And for ds/db,\nbecause the dz/db just comes out as the\nidentity matrix,",
    "start": "2226430",
    "end": "2233020"
  },
  {
    "text": "the answer is just delta. But for ds/dW, we need\nto work out the dz/dw",
    "start": "2233020",
    "end": "2242109"
  },
  {
    "text": "before we're finished. OK, so what do we get\nfor that last piece?",
    "start": "2242110",
    "end": "2251550"
  },
  {
    "text": "So one question you\nmight start off with and is normally a good\nthing to think about when",
    "start": "2251550",
    "end": "2256880"
  },
  {
    "text": "you're doing assignment\nproblems on this and other things is, the\nfirst thing to think about",
    "start": "2256880",
    "end": "2262220"
  },
  {
    "text": "is, what do things look like? Should the answer be\na vector, a matrix?",
    "start": "2262220",
    "end": "2269400"
  },
  {
    "text": "What size should it be? And things like that. So for ds/dW, W is an n by\nm matrix and s is a scalar.",
    "start": "2269400",
    "end": "2282030"
  },
  {
    "text": "So therefore, since we have\none output and n times m inputs, the answer,\naccording to math,",
    "start": "2282030",
    "end": "2291200"
  },
  {
    "text": "should be that we've\ngot a 1 by n times m Jacobian, a big\nlong row vector.",
    "start": "2291200",
    "end": "2300050"
  },
  {
    "text": "But here's where things\nget a teeny bit tricky,",
    "start": "2300050",
    "end": "2305420"
  },
  {
    "text": "and there's sort of-- we end\nup with this weird mess of math",
    "start": "2305420",
    "end": "2312890"
  },
  {
    "text": "and engineering convenience\nbecause immediately, what we're wanting to do is\nwe're wanting to take",
    "start": "2312890",
    "end": "2320180"
  },
  {
    "text": "our old parameters,\nwhich will be stored in the form of matrices,\nvectors, and so on, that we're",
    "start": "2320180",
    "end": "2327590"
  },
  {
    "text": "using as coefficients,\nand we're going to want to subtract\nfrom them a fraction",
    "start": "2327590",
    "end": "2334400"
  },
  {
    "text": "of our calculated gradient. So what we'd like to do is\nhave our calculated gradients",
    "start": "2334400",
    "end": "2342589"
  },
  {
    "text": "in the same shapes\nas our parameters, because then, we can\njust do subtraction.",
    "start": "2342590",
    "end": "2348060"
  },
  {
    "text": "Whereas if they've turned into\na god almighty row vector, that's not quite so convenient.",
    "start": "2348060",
    "end": "2353869"
  },
  {
    "text": "So it turns out that\nwhat we end up doing is using something that gets\nreferred to as the shape",
    "start": "2353870",
    "end": "2361640"
  },
  {
    "text": "convention, that we reshape\nour Jacobians so they fit",
    "start": "2361640",
    "end": "2373039"
  },
  {
    "text": "into things that are of the\nsame shape as the parameters",
    "start": "2373040",
    "end": "2378440"
  },
  {
    "text": "that we are using. So we're going to\nrepresent ds/dW as an n",
    "start": "2378440",
    "end": "2384080"
  },
  {
    "text": "by m matrix laid out as follows. And that's a place that\npeople can get confused.",
    "start": "2384080",
    "end": "2391829"
  },
  {
    "text": "OK, so that's what we want to\ncalculate, that kind of matrix. And so that matrix is going\nto be delta times dz/dW.",
    "start": "2391830",
    "end": "2401520"
  },
  {
    "text": "So delta is going to\nbe part of the answer, and then we want to\nknow what dz/dW is.",
    "start": "2401520",
    "end": "2409069"
  },
  {
    "text": "And the answer is\ngoing to be-- it's going to come out like this. So the ds/dW is going\nto be delta T times xT.",
    "start": "2409070",
    "end": "2417080"
  },
  {
    "text": "So it's going to be the product\nof the upstream gradient, which was the same thing we calculated\nbefore for the other two",
    "start": "2417080",
    "end": "2424250"
  },
  {
    "text": "quantities, and then a local\ninput symbol, which is--",
    "start": "2424250",
    "end": "2430800"
  },
  {
    "text": "input signal, which is\nhere coming out to xT. OK.",
    "start": "2430800",
    "end": "2436800"
  },
  {
    "text": "And so we're taking the\ntransposes of those two vectors,",
    "start": "2436800",
    "end": "2441820"
  },
  {
    "text": "which it means that\nwe end up calculating an outer product of those\ntwo vectors, which gives us",
    "start": "2441820",
    "end": "2448800"
  },
  {
    "text": "our gradient. And so why is that\nthe right answer?",
    "start": "2448800",
    "end": "2454000"
  },
  {
    "text": "Well, it kind of\nlooks convenient because that's\ngiving us something of the right shape for what I\nwas arguing we want to find out.",
    "start": "2454000",
    "end": "2461290"
  },
  {
    "text": "And we have the right\nnumber of terms. Now, I'm going to\nrush through this,",
    "start": "2461290",
    "end": "2467050"
  },
  {
    "text": "so I encourage you to\nread the lecture notes and do this more carefully.",
    "start": "2467050",
    "end": "2472510"
  },
  {
    "text": "But let me, at least a little\nbit, explain why it makes sense. So if you think of one weight--",
    "start": "2472510",
    "end": "2482160"
  },
  {
    "text": "so all of these connections\nare our matrix, right? The matrix is being\nrepresented by all these lines",
    "start": "2482160",
    "end": "2487590"
  },
  {
    "text": "in the neural network. So if you think of one number\nin the matrix, so here is W23,",
    "start": "2487590",
    "end": "2493450"
  },
  {
    "text": "so it's connecting\nfrom input three or it's multiplying input 3 to\ngive part of the answer of h2.",
    "start": "2493450",
    "end": "2502380"
  },
  {
    "text": "So it's this line here. So for this line\nhere, this weight",
    "start": "2502380",
    "end": "2509880"
  },
  {
    "text": "is being used only in\nthe calculation of h2. And the only thing it's\ndependent on is x3.",
    "start": "2509880",
    "end": "2517650"
  },
  {
    "text": "So if you're then wanting to\nwork out the partial of h2--",
    "start": "2517650",
    "end": "2525275"
  },
  {
    "text": " or z2, sorry.",
    "start": "2525275",
    "end": "2531510"
  },
  {
    "text": "Yeah, sorry. Yeah, sorry, z2. The partial of z2\nwith respect to x3,",
    "start": "2531510",
    "end": "2539170"
  },
  {
    "text": "it's depending on\nthese two pieces only, and that's what you're\nachieving by working out",
    "start": "2539170",
    "end": "2547610"
  },
  {
    "text": "the outer product like that. OK.",
    "start": "2547610",
    "end": "2553500"
  },
  {
    "text": "Yeah, so let me just\ncome back one more time to this question of the\nshape of derivatives.",
    "start": "2553500",
    "end": "2563046"
  },
  {
    "text": " So I already fudged it\nwhen I was talking about,",
    "start": "2563046",
    "end": "2572220"
  },
  {
    "text": "should I put the transpose\nthere or should I not and get a row vector\nversus a column vector?",
    "start": "2572220",
    "end": "2579810"
  },
  {
    "text": "So there's this disagreement\nbetween whether you",
    "start": "2579810",
    "end": "2585480"
  },
  {
    "text": "have the Jacobian form, which\nis what actually makes the chain rule work right in terms\nof doing multiplication,",
    "start": "2585480",
    "end": "2594359"
  },
  {
    "text": "versus the shape convention,\nwhich is how we store everything for our computations and makes\ndoing stochastic gradient",
    "start": "2594360",
    "end": "2603000"
  },
  {
    "text": "descent, where you're\nsubtracting whatever kind of tensor you have, easy.",
    "start": "2603000",
    "end": "2610140"
  },
  {
    "text": "So this can be a\nsource of confusion.",
    "start": "2610140",
    "end": "2616049"
  },
  {
    "text": "Since we're doing a\ncomputer science course, for the answers\nin the assignment, we expect you to follow\nthe shape convention.",
    "start": "2616050",
    "end": "2623200"
  },
  {
    "text": "So if you're working out\nthe derivatives with respect to some matrix, it should\nbe shaped like a matrix",
    "start": "2623200",
    "end": "2631020"
  },
  {
    "text": "with the same parameters. But you may well want to\nthink about Jacobian forms",
    "start": "2631020",
    "end": "2636930"
  },
  {
    "text": "and computing your answers. I mean, there are two ways\nto go about doing this.",
    "start": "2636930",
    "end": "2642339"
  },
  {
    "text": "One way of doing it is\nto work out all the math using Jacobians, all our\nmath 51, and at the end,",
    "start": "2642340",
    "end": "2649740"
  },
  {
    "text": "just to reshape it so it\nfits into the same shape as the parameters according\nto our shape convention.",
    "start": "2649740",
    "end": "2657880"
  },
  {
    "text": "I mean, the other way is to do\neach stage following the shape convention, but then\nyou have to be game",
    "start": "2657880",
    "end": "2664680"
  },
  {
    "text": "to reshape things\nas needed by doing transposing to have things work\nout at the different stages.",
    "start": "2664680",
    "end": "2673790"
  },
  {
    "text": "OK, that was my attempt to\nquickly review the math. ",
    "start": "2673790",
    "end": "2680630"
  },
  {
    "text": "Most people are still here. I will now go on\nto the second half, and I'll go on to how\nwe do the computation.",
    "start": "2680630",
    "end": "2692385"
  },
  {
    "text": "So most of-- yeah,\nso the famous thing that powers neural networks is\nthe back propagation algorithm.",
    "start": "2692385",
    "end": "2700080"
  },
  {
    "text": "So the back\npropagation algorithm is really only two things.",
    "start": "2700080",
    "end": "2706420"
  },
  {
    "text": "Its invention made\npeople famous because it gave an effective\nlearning algorithm.",
    "start": "2706420",
    "end": "2711870"
  },
  {
    "text": "But at a fundamental level,\nthe back propagation algorithm is only two things.",
    "start": "2711870",
    "end": "2718070"
  },
  {
    "text": "Thing one is you\nuse the chain rule. You do calculus at\ncomplex functions.",
    "start": "2718070",
    "end": "2724260"
  },
  {
    "text": "And thing two is you store\nintermediate results,",
    "start": "2724260",
    "end": "2730290"
  },
  {
    "text": "so you never recompute\nthe same stuff again. That's all there is to the\nback propagation algorithm.",
    "start": "2730290",
    "end": "2738170"
  },
  {
    "text": "And so let's just\ngo through that. So if we're computationally\nwanting to deal with functions",
    "start": "2738170",
    "end": "2747540"
  },
  {
    "text": "and doing back propagation,\nwe can think of them as being represented as a graph.",
    "start": "2747540",
    "end": "2753640"
  },
  {
    "text": "And in some way or\nanother, this kind of graph is being used inside your\nneural network framework.",
    "start": "2753640",
    "end": "2761890"
  },
  {
    "text": "So here is a re-representation\nof my little neural network for finding whether the word\nat the center is a location.",
    "start": "2761890",
    "end": "2769900"
  },
  {
    "text": "So I'm taking the\nx vector input. I'm multiplying it by\nW. I'm adding b to it.",
    "start": "2769900",
    "end": "2776920"
  },
  {
    "text": "I'm putting it through\nthe nonlinearity, and then I'm doing the dot\nproduct with my vector u.",
    "start": "2776920",
    "end": "2783250"
  },
  {
    "text": "So that was my computation. And so the source\nnodes, the inputs",
    "start": "2783250",
    "end": "2789660"
  },
  {
    "text": "in this graph, the interior\nnodes, then the operations I do.",
    "start": "2789660",
    "end": "2795839"
  },
  {
    "text": "And so then the edges that\nconnect those together then",
    "start": "2795840",
    "end": "2801300"
  },
  {
    "text": "pass along the result\nof each operation. So I pass along Wx to\nthe addition function",
    "start": "2801300",
    "end": "2808860"
  },
  {
    "text": "with b, then that\ngives me z that I pass through the\nnonlinearity, which",
    "start": "2808860",
    "end": "2813900"
  },
  {
    "text": "gives me h, which I then dot\nproduct with the u to get s.",
    "start": "2813900",
    "end": "2818970"
  },
  {
    "text": "OK, so I do precisely\nthis computation, and this is referred\nto as forward",
    "start": "2818970",
    "end": "2825030"
  },
  {
    "text": "propagation or the forward\npass of a neural network. So the forward pass just\ncalculates functions.",
    "start": "2825030",
    "end": "2833550"
  },
  {
    "text": "OK, but then once we've done\nthat, what we want to do",
    "start": "2833550",
    "end": "2838890"
  },
  {
    "text": "is then work out gradients. So we can do\ngradient-based learning.",
    "start": "2838890",
    "end": "2844810"
  },
  {
    "text": "And so that part\nis then referred to as back propagation\nor the backward pass,",
    "start": "2844810",
    "end": "2852130"
  },
  {
    "text": "and then we run things backward. So if we're running\nthings backward, we're going to use\nthe same graph,",
    "start": "2852130",
    "end": "2858690"
  },
  {
    "text": "and we're going to backwards\npass along at gradients. And so we start at\nthe right-hand side,",
    "start": "2858690",
    "end": "2865500"
  },
  {
    "text": "and we have Ds/ds So ds/ds is\njust 1 because if you change s",
    "start": "2865500",
    "end": "2872280"
  },
  {
    "text": "you've changed. And then what we want to do\nis then work further back",
    "start": "2872280",
    "end": "2879030"
  },
  {
    "text": "so we can work out ds/dh,\nds/dz, dz/db, ds/dW, ds/dx",
    "start": "2879030",
    "end": "2885450"
  },
  {
    "text": "as we work back. And so this is what we want\nto work out with gradients.",
    "start": "2885450",
    "end": "2894030"
  },
  {
    "text": "And so how are we\ngoing to do that? Well, if we look at a\nsingle node, so for example,",
    "start": "2894030",
    "end": "2900360"
  },
  {
    "text": "our nonlinearity\nnode, but any node where h equals f of x,\nwhat we're going to have",
    "start": "2900360",
    "end": "2907500"
  },
  {
    "text": "is an upstream gradient, ds/dh. And what we want\nto do is calculate",
    "start": "2907500",
    "end": "2915330"
  },
  {
    "text": "the downstream gradient of the\nnext variable down, the ds/dz.",
    "start": "2915330",
    "end": "2920850"
  },
  {
    "text": "And the way that\nwe're going to do that is we're going to\nsay, well, let's look at f.",
    "start": "2920850",
    "end": "2927370"
  },
  {
    "text": "What is f's gradient? And that's going to\nbe our local gradient.",
    "start": "2927370",
    "end": "2933250"
  },
  {
    "text": "And then this is\nimmediately what gives us the chain\nrule, that ds/dz",
    "start": "2933250",
    "end": "2938760"
  },
  {
    "text": "is going to be the product\nof our upstream gradient, ds/dh, times the dh/dz,\nthe local gradient",
    "start": "2938760",
    "end": "2948570"
  },
  {
    "text": "that we calculate at that node. So downstream gradient\nequals upstream gradient",
    "start": "2948570",
    "end": "2955560"
  },
  {
    "text": "times local gradient. ",
    "start": "2955560",
    "end": "2962240"
  },
  {
    "text": "Well, yeah, that's what it\nsays when I press that again. OK, so this is the single\ninput, single output case,",
    "start": "2962240",
    "end": "2974060"
  },
  {
    "text": "though those inputs might be\nvectors or matrices or something like that. We then have more\ncomplex graph cases.",
    "start": "2974060",
    "end": "2983180"
  },
  {
    "text": "So I think I should\nhave retitled this. Yeah, so it's still--",
    "start": "2983180",
    "end": "2988490"
  },
  {
    "text": "sorry. So the next case\nis for our node, it might have multiple inputs.",
    "start": "2988490",
    "end": "2994590"
  },
  {
    "text": "So this is where\nwe're calculating Wx. So in that case, we\nstill have an-- we have",
    "start": "2994590",
    "end": "3001510"
  },
  {
    "text": "a single upstream gradient. And then what we're\ngoing to do is",
    "start": "3001510",
    "end": "3006580"
  },
  {
    "text": "we want to calculate the\ndownstream gradient with respect to each input.",
    "start": "3006580",
    "end": "3012319"
  },
  {
    "text": "And the way we're going\nto do that is we're going to work out the\nlocal gradient with respect",
    "start": "3012320",
    "end": "3018099"
  },
  {
    "text": "to each input, and\nthen we're going to do the same kind\nof multiplication of upstream gradient\ntimes local gradient",
    "start": "3018100",
    "end": "3026710"
  },
  {
    "text": "with respect to each input. Again, chain rule.",
    "start": "3026710",
    "end": "3032790"
  },
  {
    "text": "OK, so here's a little\nexample of this. So this isn't really\nthe kind of thing",
    "start": "3032790",
    "end": "3040650"
  },
  {
    "text": "you normally see in\na neural network, but it's an easy example. So f of xyz is going to be x\nplus y times the max of yz.",
    "start": "3040650",
    "end": "3051010"
  },
  {
    "text": "And we've got current\nvalues of x, y, and z of 1, 2, and 0, respectively.",
    "start": "3051010",
    "end": "3057850"
  },
  {
    "text": "So here's our little\ncomputation graph. And so for forward propagation,\nwe're going to do this addition.",
    "start": "3057850",
    "end": "3065860"
  },
  {
    "text": "We're going to do\nthis max function. And then we're going\nto multiply the two, and that gives us\nthe value of f.",
    "start": "3065860",
    "end": "3072570"
  },
  {
    "text": "So we can run that with the\ncurrent values of x, y, and z, and this is what we get.",
    "start": "3072570",
    "end": "3078850"
  },
  {
    "text": "So the max of 2 and 0\nis 2, addition is 3. The answer is 6.",
    "start": "3078850",
    "end": "3085440"
  },
  {
    "text": "OK, so then after\nhaving done that, we run the backward propagation.",
    "start": "3085440",
    "end": "3090510"
  },
  {
    "text": "And yeah, so this\nprocedure, it's not actually special to\nneural networks, right?",
    "start": "3090510",
    "end": "3096200"
  },
  {
    "text": "You can use it for\nany piece of math if you want to just run\nyour math on PyTorch rather",
    "start": "3096200",
    "end": "3101290"
  },
  {
    "text": "than working it out in your\nhead or with Mathematica. OK, so now we work\nout backwards.",
    "start": "3101290",
    "end": "3110270"
  },
  {
    "text": "So we want to know\nthe local gradient. So da/dz is going to be 1--",
    "start": "3110270",
    "end": "3118320"
  },
  {
    "text": "sorry, I said that wrong. Da/dx is going to be 1. So a equals x plus y. Da/dy equals 1.",
    "start": "3118320",
    "end": "3125130"
  },
  {
    "text": "For the max function,\nthat's going to depend on which\nof the two is larger",
    "start": "3125130",
    "end": "3130290"
  },
  {
    "text": "because it's going to have a\nslope of 1 for the one that's the biggest and 0 for the\none that's the smallest.",
    "start": "3130290",
    "end": "3138480"
  },
  {
    "text": "And then for the\nproduct, that's like what we saw with vectors, that df/da\nis going to be b and df/db",
    "start": "3138480",
    "end": "3147240"
  },
  {
    "text": "is going to be a. So those are all\nour local gradients. And so then we can use those to\ncalculate out the derivatives.",
    "start": "3147240",
    "end": "3156190"
  },
  {
    "text": "So df/df is 1. We then multiply that by\nthe two local gradients",
    "start": "3156190",
    "end": "3165570"
  },
  {
    "text": "that are calculated.  We want a and b,\nso that gives us",
    "start": "3165570",
    "end": "3173940"
  },
  {
    "text": "2 and 3, where you're\nswapping over the numbers. Then for the max, that we're\nhaving the one that is biggest,",
    "start": "3173940",
    "end": "3185910"
  },
  {
    "text": "we're taking the upstream\ntimes 1, so it gets 3.",
    "start": "3185910",
    "end": "3190970"
  },
  {
    "text": "The other one gets 0. And then for the plus, we're\njust sending the gradient down",
    "start": "3190970",
    "end": "3196380"
  },
  {
    "text": "in both directions, and so\nboth of them come out as 2.",
    "start": "3196380",
    "end": "3202269"
  },
  {
    "text": "And so that gives us df/dx. So the final\nfunction value is 2.",
    "start": "3202270",
    "end": "3208390"
  },
  {
    "text": "Df/dy, we're taking\nthe 3 and adding the 2,",
    "start": "3208390",
    "end": "3213529"
  },
  {
    "text": "I'll mention that again in\na minute, which gives us 5. And then df/dz is 0.",
    "start": "3213530",
    "end": "3221380"
  },
  {
    "text": "And we should be able\nto, again, quickly check that we've got this right. So if we consider\nthe slope around z",
    "start": "3221380",
    "end": "3234130"
  },
  {
    "text": "as you change z a\nlittle-- so z is 0. If we make z 0.1, that makes\nabsolutely no difference",
    "start": "3234130",
    "end": "3242470"
  },
  {
    "text": "to what the computed\nfunction value is. So the gradient there is 0.",
    "start": "3242470",
    "end": "3247880"
  },
  {
    "text": "That's correct. So if I-- up the top, if\nI change x a little bit,",
    "start": "3247880",
    "end": "3255070"
  },
  {
    "text": "if I change x to 1.1, then\nI'll be calculating 1.1 plus 2",
    "start": "3255070",
    "end": "3262610"
  },
  {
    "text": "is 3.1. And then I'll be taking\nthe max, which is 2,",
    "start": "3262610",
    "end": "3270750"
  },
  {
    "text": "and I'll be calculating 5.1. And so-- right,\nI did that wrong.",
    "start": "3270750",
    "end": "3280020"
  },
  {
    "text": "Oh, times 2. I didn't do the\nmultiplication right.",
    "start": "3280020",
    "end": "3286225"
  },
  {
    "text": "I'm sorry. Yeah, so we get the\n3.1 that's multiplied by 2, that gives us 6.2.",
    "start": "3286225",
    "end": "3294190"
  },
  {
    "text": "So a change of 0.1 in the x\nhas moved things up by 0.2.",
    "start": "3294190",
    "end": "3299579"
  },
  {
    "text": "So that corresponds to\nthe gradient being 2. And so then the final case is,\nwell, what if we change y to--",
    "start": "3299580",
    "end": "3309240"
  },
  {
    "text": "so y started off as\n2 and made it 2.1.",
    "start": "3309240",
    "end": "3314470"
  },
  {
    "text": "Then we're going to get\n2.1 multiplied by 1 is 2.1.",
    "start": "3314470",
    "end": "3322880"
  },
  {
    "text": "6.1-- or 6.5 Right. And then we've got the 2.1 here.",
    "start": "3322880",
    "end": "3330050"
  },
  {
    "text": "Sorry, I keep doing this wrong. 2.1 plus 1 equals 3.1. And then we've got\n2.1 is the max.",
    "start": "3330050",
    "end": "3336839"
  },
  {
    "text": "So we've got 2.1 times 3.1,\nand that comes out to be 6.51.",
    "start": "3336840",
    "end": "3345960"
  },
  {
    "text": "So it's approximately\ngone up by-- so our 0.1 difference has\ngone up to approximately 0.5.",
    "start": "3345960",
    "end": "3353220"
  },
  {
    "text": "That's just an estimate. And so that corresponds to\nthe gradient being 5, right?",
    "start": "3353220",
    "end": "3359069"
  },
  {
    "text": "We get this five times\nmultiplication of our changes. OK, and so that illustrates the\nfact that the right thing to do",
    "start": "3359070",
    "end": "3369230"
  },
  {
    "text": "is when you have outward\nbranches in your computation graph, and you're running\nthe back propagation,",
    "start": "3369230",
    "end": "3379050"
  },
  {
    "text": "that what you do is you\nsum the gradients, right?",
    "start": "3379050",
    "end": "3384320"
  },
  {
    "text": "So that for this case,\nwe had y being the--",
    "start": "3384320",
    "end": "3390170"
  },
  {
    "text": "y is sort of going into\nthese two different things in our previous chart.",
    "start": "3390170",
    "end": "3395340"
  },
  {
    "text": "So once we've worked out\nthe upstream gradients, we sum them to get\nthe total gradient.",
    "start": "3395340",
    "end": "3401010"
  },
  {
    "text": "And so that's what\nwe did back here. We had two outward\nthings, and we took these calculated upstream\ngradients and 2 and 3,",
    "start": "3401010",
    "end": "3409190"
  },
  {
    "text": "and we just summed\nthem to get 5, and that gave the right answer. ",
    "start": "3409190",
    "end": "3419620"
  },
  {
    "text": "OK. And so you can think about that\nfor the just generally how--",
    "start": "3419620",
    "end": "3427330"
  },
  {
    "text": "the things to think\nabout as gradients move around in these\npictures so that when",
    "start": "3427330",
    "end": "3433930"
  },
  {
    "text": "we have a plus operation, that\nplus just sort of distributes",
    "start": "3433930",
    "end": "3440740"
  },
  {
    "text": "gradients. So the same gradient, that's\nthe upstream gradient, goes to each input.",
    "start": "3440740",
    "end": "3446890"
  },
  {
    "text": "When you have a max, it's kind\nof like a router of gradients. So the max is going to send the\ngradient to one of the inputs",
    "start": "3446890",
    "end": "3456670"
  },
  {
    "text": "and send nothing at all\nto the other inputs. And when you have\na multiplication,",
    "start": "3456670",
    "end": "3462200"
  },
  {
    "text": "it's a little bit\nfunky because you're doing this sort of switching\nof the forward coefficients.",
    "start": "3462200",
    "end": "3469519"
  },
  {
    "text": "So you're taking the\nupstream gradient multiplied by the opposite forward\ncoefficient gives you",
    "start": "3469520",
    "end": "3477490"
  },
  {
    "text": "your downstream gradient. OK. So we have this systematic way\nof being able to forward pass",
    "start": "3477490",
    "end": "3488440"
  },
  {
    "text": "calculate the\nvalues of functions, then run this backward to work\nout the gradients heading down",
    "start": "3488440",
    "end": "3497460"
  },
  {
    "text": "the network. And so the main other thing of\nthe back propagation algorithm",
    "start": "3497460",
    "end": "3504210"
  },
  {
    "text": "is just that we want\nto do this efficiently. So the wrong way\nto do it would be",
    "start": "3504210",
    "end": "3509700"
  },
  {
    "text": "to say, well, gee, I want\nto calculate the ds/db, ds/dW, ds/dx, ds/du, so let me\nstart doing those one at a time,",
    "start": "3509700",
    "end": "3519910"
  },
  {
    "text": "and when I've done\nthem all, I will stop, because that means if you\nfirst calculate ds/db,",
    "start": "3519910",
    "end": "3526109"
  },
  {
    "text": "you do all of the\npart that's in blue. But then if you\nwent on to ds/dw,",
    "start": "3526110",
    "end": "3534510"
  },
  {
    "text": "you'd be calculating\nall the part in red. And, well, just as we\nsaw in the math part,",
    "start": "3534510",
    "end": "3540730"
  },
  {
    "text": "when we were doing it\nas math, these parts are exactly the same.",
    "start": "3540730",
    "end": "3546230"
  },
  {
    "text": "You're doing exactly\nthe same computations, so you only want to do\nthat part once and work out",
    "start": "3546230",
    "end": "3553810"
  },
  {
    "text": "this upstream gradient\nor error signal that is being then calculated\nand is then being shared.",
    "start": "3553810",
    "end": "3561170"
  },
  {
    "text": "So the picture that\nwe want to have is you're doing\ntogether the shared part",
    "start": "3561170",
    "end": "3567010"
  },
  {
    "text": "and then you're only doing\nseparately the little bits that you need to do.",
    "start": "3567010",
    "end": "3573789"
  },
  {
    "text": "OK. Boy, I seem to have been\nrushing through today, and I'm going to actually\nend early, unless anyone",
    "start": "3573790",
    "end": "3580420"
  },
  {
    "text": "is going to slow me down. But I do have just a few\nmore slides to go through.",
    "start": "3580420",
    "end": "3586690"
  },
  {
    "text": "Yeah, so the generalization\nof this as an algorithm",
    "start": "3586690",
    "end": "3592210"
  },
  {
    "text": "is in the general case-- so we normally have these sort\nof neural network layers, which",
    "start": "3592210",
    "end": "3601780"
  },
  {
    "text": "you can represent as\nvectors and matrices. And it's sort of nice\nand clean, and it",
    "start": "3601780",
    "end": "3608650"
  },
  {
    "text": "looks like doing that\nin calculus class. I mean, strictly speaking,\nthat isn't necessary.",
    "start": "3608650",
    "end": "3616130"
  },
  {
    "text": "So the algorithm for\nforward propagation and backward\npropagation that I've outlined that you can have\nit work in a completely",
    "start": "3616130",
    "end": "3624940"
  },
  {
    "text": "arbitrary computation\ngraph, providing it's a DAG that doesn't\nhave cycles in it.",
    "start": "3624940",
    "end": "3630609"
  },
  {
    "text": "So the general algorithm\nis, well, you've got a whole bunch of variables\nthat depend on other variables.",
    "start": "3630610",
    "end": "3638820"
  },
  {
    "text": "There's some way in\nwhich we can sort them so that each\nvariable only depends",
    "start": "3638820",
    "end": "3644869"
  },
  {
    "text": "on variables to the left of it. So that's referred to as a\ntopological sort of the outputs.",
    "start": "3644870",
    "end": "3651000"
  },
  {
    "text": "And so that means there's a way\nwe can do a forward pass, where we're calculating variables in\nterms of 1's that have already",
    "start": "3651000",
    "end": "3660980"
  },
  {
    "text": "been calculated. But if we want to have some\nextra wonky arcs, so it's not nice matrix multiplies\nor anything,",
    "start": "3660980",
    "end": "3668460"
  },
  {
    "text": "we're totally\nallowed to do that. Or we can have things not\nfully connected, right? So there's no\nconnections across here.",
    "start": "3668460",
    "end": "3676190"
  },
  {
    "text": "We can have an arbitrary\ncomputation graph. And so that gives us\nour forward propagation.",
    "start": "3676190",
    "end": "3682770"
  },
  {
    "text": "And then once we've done\nthe forward propagation, we can initialize the\noutput gradient as 1,",
    "start": "3682770",
    "end": "3691410"
  },
  {
    "text": "and then we're going to visit\nthe nodes in reverse order.",
    "start": "3691410",
    "end": "3696839"
  },
  {
    "text": "And for each node, we're\ngoing to compute a gradient by using the upstream gradient\nand the local gradient",
    "start": "3696840",
    "end": "3704750"
  },
  {
    "text": "to compute the\ndownstream gradient. And so then we can head back\ndown the computation graph",
    "start": "3704750",
    "end": "3710299"
  },
  {
    "text": "and work out all of the\ndownstream gradients. And so the crucial\nthing to notice",
    "start": "3710300",
    "end": "3717109"
  },
  {
    "text": "is that if you do it correctly,\nthat working out the gradients",
    "start": "3717110",
    "end": "3726980"
  },
  {
    "text": "has the same Big O\ncomplexity as working out the forward calculation.",
    "start": "3726980",
    "end": "3733470"
  },
  {
    "text": "So that if you're doing more-- in terms of Big O terms, you\nmight have different functions,",
    "start": "3733470",
    "end": "3740240"
  },
  {
    "text": "depending on what\nthe derivatives are. But in big O terms, if\nyou're doing more work in the backward pass than you're\ndoing in the forward pass,",
    "start": "3740240",
    "end": "3748230"
  },
  {
    "text": "that means that\nyou're somehow failing to do this efficient\ncomputation and that you're",
    "start": "3748230",
    "end": "3755340"
  },
  {
    "text": "recomputing some of your work. OK, so because we have\nsuch a good algorithm here,",
    "start": "3755340",
    "end": "3763880"
  },
  {
    "text": "you should be able to just\nwork out the backward pass automatically, and that gets\nreferred to as automatic",
    "start": "3763880",
    "end": "3771760"
  },
  {
    "text": "differentiation. So if you had the symbolic\nform of what you're calculating",
    "start": "3771760",
    "end": "3780100"
  },
  {
    "text": "with your forward\npass, you should just be able to say, yo,\ncomputer, can you work out",
    "start": "3780100",
    "end": "3787690"
  },
  {
    "text": "the backward pass for me? And kind of\nmathematical like, it",
    "start": "3787690",
    "end": "3792760"
  },
  {
    "text": "could look at the symbolic\nform of all of your functions, work out their derivatives, and\ndo the entire thing for you.",
    "start": "3792760",
    "end": "3803079"
  },
  {
    "text": "So early on, there was a\npioneering deep learning",
    "start": "3803080",
    "end": "3808420"
  },
  {
    "text": "framework, Theano, principally\nfrom the University of Montreal, which attempted\nto do precisely that.",
    "start": "3808420",
    "end": "3816835"
  },
  {
    "text": "That you had the entire forward\npass computation started in symbolic form, and it just\ndid the entire thing for you",
    "start": "3816835",
    "end": "3825320"
  },
  {
    "text": "and worked out the backward\npass automatically.",
    "start": "3825320",
    "end": "3830570"
  },
  {
    "text": "But somehow, that proved\nto be too heavyweight,",
    "start": "3830570",
    "end": "3837050"
  },
  {
    "text": "or hard to deal with\ndifferent things, or people just like to\nwrite their own Python",
    "start": "3837050",
    "end": "3843200"
  },
  {
    "text": "or whatever it is. So that idea did\nnot fully succeed.",
    "start": "3843200",
    "end": "3848730"
  },
  {
    "text": "And so what, in practice, all\nof the current main frameworks have fallen back on is\nsomething that's actually",
    "start": "3848730",
    "end": "3857450"
  },
  {
    "text": "less automated than that. So it's sort of like we've\ngone backwards in time, but the software has gotten\na lot better, really.",
    "start": "3857450",
    "end": "3864119"
  },
  {
    "text": "It's a lot stabler and faster. So all of the modern\ndeep learning frameworks",
    "start": "3864120",
    "end": "3871083"
  },
  {
    "text": "say, look, I will manage the\ncomputation graph for you, and I can run the\nforward propagation",
    "start": "3871083",
    "end": "3877789"
  },
  {
    "text": "pass and the backward\npropagation pass, but you're going to have to\nwork out the local derivatives",
    "start": "3877790",
    "end": "3884180"
  },
  {
    "text": "yourself. So if you're putting in a\nlayer or putting in a function,",
    "start": "3884180",
    "end": "3893420"
  },
  {
    "text": "like an activation function, in\na neural network, your class,",
    "start": "3893420",
    "end": "3899369"
  },
  {
    "text": "your Python class\nthat represents that, you're going to have to tell me\nwhat the forward computation is",
    "start": "3899370",
    "end": "3906920"
  },
  {
    "text": "and what the local gradient is. And I'm just going to\ncall your local gradient",
    "start": "3906920",
    "end": "3912350"
  },
  {
    "text": "and assume it's correct. So there's a bit more that\nhas to be done manually.",
    "start": "3912350",
    "end": "3917790"
  },
  {
    "text": "So the part that's automated\nthen is that when--",
    "start": "3917790",
    "end": "3925390"
  },
  {
    "text": "not precisely this\ncode, obviously, but roughly inside the\ndeep learning software,",
    "start": "3925390",
    "end": "3932480"
  },
  {
    "text": "it's computing with\na computation graph, and it's got a forward\nand a backward,",
    "start": "3932480",
    "end": "3937980"
  },
  {
    "text": "and it's doing what I presented\non the pictures before. So for the forward\npass, it's topologically",
    "start": "3937980",
    "end": "3945830"
  },
  {
    "text": "sorting all the\nnodes of the graph and then it's\ngoing through them.",
    "start": "3945830",
    "end": "3950849"
  },
  {
    "text": "And for each node\nin the graph, it's calling its forward\nfunction, which",
    "start": "3950850",
    "end": "3955880"
  },
  {
    "text": "will be able to compute\nits local value in terms of its inputs,\nwhich have already",
    "start": "3955880",
    "end": "3961820"
  },
  {
    "text": "been calculated because\nit's topologically sorted. And then it's running\nthe backward pass.",
    "start": "3961820",
    "end": "3968180"
  },
  {
    "text": "And in the backward pass, you're\nreversing your topological sort and then you're working\nout the gradient, which",
    "start": "3968180",
    "end": "3976970"
  },
  {
    "text": "is going to be\nthe multiplication of the upstream error signal\ntimes your local gradient.",
    "start": "3976970",
    "end": "3982410"
  },
  {
    "text": "And so what a human\nbeing has to implement is that for anything,\nwhether it's a single gate,",
    "start": "3982410",
    "end": "3990510"
  },
  {
    "text": "here's a multiply gate,\nor a neural network layer, you have to implement a forward\npass and a backward pass.",
    "start": "3990510",
    "end": "3998610"
  },
  {
    "text": "So here, for my baby\nexample, since we're just doing multiplication,\nmy forward pass",
    "start": "3998610",
    "end": "4004570"
  },
  {
    "text": "is that I just multiply the\ntwo numbers and return it. So I'm specifying that\nfor the local node.",
    "start": "4004570",
    "end": "4012290"
  },
  {
    "text": "And then the other\npart is that I have to work out those gradients.",
    "start": "4012290",
    "end": "4018250"
  },
  {
    "text": "And well, we sort\nof know how to do that because that's the examples\nthat we've been doing here.",
    "start": "4018250",
    "end": "4023980"
  },
  {
    "text": "But notice that there's\nsort of a trick, right? For what I've got\nnow, you kind of",
    "start": "4023980",
    "end": "4029140"
  },
  {
    "text": "can't write down\nwhat the gradients are because what these--",
    "start": "4029140",
    "end": "4036790"
  },
  {
    "text": "because backward is just\ntaking, as an input, the upstream gradient. And you can't work out what the\ndownstream gradients are going",
    "start": "4036790",
    "end": "4046030"
  },
  {
    "text": "to be, unless you know\nwhat function values you're calculating it at.",
    "start": "4046030",
    "end": "4051790"
  },
  {
    "text": "So the standard trick that all-- which is how everyone\nwrites this code, is you're relying on the fact\nthat the forward is being",
    "start": "4051790",
    "end": "4060050"
  },
  {
    "text": "calculated before the backward,\nand so your forward method",
    "start": "4060050",
    "end": "4065120"
  },
  {
    "text": "shoves into some local\nvariables of the class what the values of the\ninputs are, and then you",
    "start": "4065120",
    "end": "4072350"
  },
  {
    "text": "have them available. So when you get to\nthe backward pass, you can do what we did\nbefore, that the dx is going",
    "start": "4072350",
    "end": "4082460"
  },
  {
    "text": "to be the upstream error signal\ntimes the opposite input,",
    "start": "4082460",
    "end": "4088490"
  },
  {
    "text": "and similarly for dy, and that's\ngoing to give us the answer. OK, just two last\nthings, then, to mention.",
    "start": "4088490",
    "end": "4098589"
  },
  {
    "text": "Yeah, so doing this,\nyou need to write--",
    "start": "4098590",
    "end": "4105134"
  },
  {
    "text": "you need to get the\nmath right for what's the derivative of your\nfunction so you get",
    "start": "4105135",
    "end": "4111210"
  },
  {
    "text": "the right backward calculation. So the standard way\nto check that you've",
    "start": "4111210",
    "end": "4116790"
  },
  {
    "text": "got the right\nbackward calculation is to do manual gradient\nchecking with numeric gradients.",
    "start": "4116790",
    "end": "4125318"
  },
  {
    "text": "So the way you do that is you--",
    "start": "4125319",
    "end": "4130500"
  },
  {
    "text": "like for the couple\nof examples I did when I said, let's check\nit by going from 1 to 1.1,",
    "start": "4130500",
    "end": "4137830"
  },
  {
    "text": "what should the slope\nbe approximately? We're going to do that\nin an automated way. And so we're going\nto say at the value",
    "start": "4137830",
    "end": "4144568"
  },
  {
    "text": "x, let's estimate what\nthe gradient should be. And the way to do that\nis to pick a small h.",
    "start": "4144569",
    "end": "4152620"
  },
  {
    "text": "There isn't a magical\nnumber because it depends on the function. But typically, for neural\nnetworks, around 10",
    "start": "4152620",
    "end": "4158109"
  },
  {
    "text": "to the minus 4 is good. A small h, and work\nout the function value,",
    "start": "4158109",
    "end": "4164380"
  },
  {
    "text": "either forward pass at x\nplus h and x minus h divided",
    "start": "4164380",
    "end": "4169960"
  },
  {
    "text": "by the run, which is 2h, and\nthat should give you an estimate",
    "start": "4169960",
    "end": "4175450"
  },
  {
    "text": "of the slope, what the\nbackward pass is calculating. And you want those two numbers\nto be approximately equal,",
    "start": "4175450",
    "end": "4183988"
  },
  {
    "text": "within some 10 to the\nminus 2 of each other, and then probably, you're\ncalculating the gradient right.",
    "start": "4183989",
    "end": "4189890"
  },
  {
    "text": "And if they aren't\nequal, that you probably",
    "start": "4189890",
    "end": "4196570"
  },
  {
    "text": "have made a mistake. Yeah, so note that\nthis formula-- for the version I\ndid for my examples,",
    "start": "4196570",
    "end": "4204290"
  },
  {
    "text": "I just compared to\nx with x plus h.",
    "start": "4204290",
    "end": "4209300"
  },
  {
    "text": "I did a one-sided estimate,\nwhich is normally what you get taught in a math class.",
    "start": "4209300",
    "end": "4214350"
  },
  {
    "text": "If you're doing this to check\nyour gradients numerically, you're far, far better off\ndoing this two-sided estimate",
    "start": "4214350",
    "end": "4222280"
  },
  {
    "text": "because it's much more\naccurate and stable when you're doing it equally\naround both sides of your h.",
    "start": "4222280",
    "end": "4229090"
  },
  {
    "text": "Yeah, so this looks easy to do. If this was just so good,\nwhy doesn't everyone",
    "start": "4229090",
    "end": "4236920"
  },
  {
    "text": "do this all the time and\nforget about calculus? The reason you don't want to\ndo this is that doing this",
    "start": "4236920",
    "end": "4244870"
  },
  {
    "text": "is incredibly slow, right? Because you have to repeat this\ncomputation for every parameter",
    "start": "4244870",
    "end": "4251139"
  },
  {
    "text": "of your model, that you're\nnot getting the speed-ups you're getting from the\nback propagation algorithm.",
    "start": "4251140",
    "end": "4259340"
  },
  {
    "text": "But it's useful for checking\nyour implementation is correct. In the old days before\nframeworks like PyTorch,",
    "start": "4259340",
    "end": "4267070"
  },
  {
    "text": "we used to write\neverything by hand, and people often\ngot things wrong. But nowadays, it's\nless needed, but it's",
    "start": "4267070",
    "end": "4274390"
  },
  {
    "text": "good to check that\nif you've implemented your own new layer that\nit's doing the right thing.",
    "start": "4274390",
    "end": "4280150"
  },
  {
    "text": "OK. Yeah, so that's\neverything that we need to know about neural nets.",
    "start": "4280150",
    "end": "4285260"
  },
  {
    "text": "Back propagation is the chain\nrule applied efficiently. Forward pass is just\nfunction application.",
    "start": "4285260",
    "end": "4292390"
  },
  {
    "text": "Backward pass is chain\nrule applied inefficiently. So we're going to inflict\npain on our students",
    "start": "4292390",
    "end": "4303190"
  },
  {
    "text": "by making them do some math and\ncalculate some of these things and do the homework.",
    "start": "4303190",
    "end": "4309560"
  },
  {
    "text": "And I know that'll be harder\nfor some of you than others. ",
    "start": "4309560",
    "end": "4316060"
  },
  {
    "text": "In some sense,\nyou don't actually need to know how to do this. The beauty of these modern\ndeep learning frameworks",
    "start": "4316060",
    "end": "4321580"
  },
  {
    "text": "is they'll do it all for you. They predefine\ncommon layer types, and you can just plug them\ntogether like pieces of LEGO,",
    "start": "4321580",
    "end": "4328420"
  },
  {
    "text": "and they'll be computed right. And this is precisely the\nreason that high school students",
    "start": "4328420",
    "end": "4333640"
  },
  {
    "text": "across the country and the\nworld can now do deep learning projects for their science fairs\nbecause you don't actually have",
    "start": "4333640",
    "end": "4340389"
  },
  {
    "text": "to understand any of this math. You can just use\nwhat's given to you. But we want to hope that you\nactually do understand something",
    "start": "4340390",
    "end": "4351490"
  },
  {
    "text": "about what's going\non under the hood and how neural networks\nwork, so, therefore, we",
    "start": "4351490",
    "end": "4356829"
  },
  {
    "text": "make you suffer a little bit. And, of course,\nif you're wanting",
    "start": "4356830",
    "end": "4362080"
  },
  {
    "text": "to look at and understand\nmore complex things, you need to have some\nsense of what's going on.",
    "start": "4362080",
    "end": "4368570"
  },
  {
    "text": "So later on, when we get on\nto recurrent neural networks, we'll talk a bit about things\nlike exploding and vanishing",
    "start": "4368570",
    "end": "4375040"
  },
  {
    "text": "gradients. And if you want to have\nsome understanding about, well, why things aren't working\nand things are going wrong,",
    "start": "4375040",
    "end": "4381430"
  },
  {
    "text": "then you want to know\nwhat it's actually calculating rather\nthan just thinking it's all a black box magic.",
    "start": "4381430",
    "end": "4387860"
  },
  {
    "text": "And so that's why we hope\nto have taught something about that. OK, I think I'm done if the\naudience is sufficiently",
    "start": "4387860",
    "end": "4395860"
  },
  {
    "text": "stunned. And we can stop for today. OK, thank you.",
    "start": "4395860",
    "end": "4401550"
  },
  {
    "start": "4401550",
    "end": "4407000"
  }
]