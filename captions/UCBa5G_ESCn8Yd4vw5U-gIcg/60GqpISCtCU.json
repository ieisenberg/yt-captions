[
  {
    "start": "0",
    "end": "6340"
  },
  {
    "text": "OK, cool. Let's get started. So I guess today\nwe're going to talk",
    "start": "6340",
    "end": "11740"
  },
  {
    "text": "about implicit\nregularization of noise. ",
    "start": "11740",
    "end": "27480"
  },
  {
    "text": "And the plan today\nis that because this is a pretty\nchallenging topic and I",
    "start": "27480",
    "end": "34560"
  },
  {
    "text": "think the research community\nis still, in some sense, doing research on this--",
    "start": "34560",
    "end": "41110"
  },
  {
    "text": "so we have some results. It's pretty complicated. So what I'm going to do\nis I'm going to somewhat--",
    "start": "41110",
    "end": "48629"
  },
  {
    "text": "using a relatively\nheuristic approach. So I'm going to try to convey\nthe main idea without doing",
    "start": "48630",
    "end": "55020"
  },
  {
    "text": "the actual rigorous statement. So in this lecture, I don't even\nthink I have a formal statement",
    "start": "55020",
    "end": "63240"
  },
  {
    "text": "to state because it's just\na little bit too complicated and unnecessary, right? So if I really proved the\nformal version of the theorem,",
    "start": "63240",
    "end": "70859"
  },
  {
    "text": "that probably would take two\nlectures or three lectures. So that's why instead I'm\ntrying to kind of at least",
    "start": "70860",
    "end": "77670"
  },
  {
    "text": "convey the main intuition\nwhy the noise is useful still with some math, because\nwithout the math,",
    "start": "77670",
    "end": "83940"
  },
  {
    "text": "you don't even see the\nintuition sometimes. But the math wouldn't\nbe always rigorous, and I would not know where it\nis kind of like not rigorous.",
    "start": "83940",
    "end": "92310"
  },
  {
    "text": "And also, some part cannot\nbe made rigorous without additional assumption. And I'll be-- I am\nclarifying that.",
    "start": "92310",
    "end": "99120"
  },
  {
    "text": "And so some part is really\njust for convenience. I ignore some kind\nof the jargons, but they can be fixed by\njust doing more careful math.",
    "start": "99120",
    "end": "107040"
  },
  {
    "text": "And some part is actually\nfundamental challenges, and you have to really\nuse additional assumptions or maybe even change the\nproblem setting to go",
    "start": "107040",
    "end": "113397"
  },
  {
    "text": "through those steps rigorously. So I guess the main portion\nof the talk, the lecture",
    "start": "113397",
    "end": "124049"
  },
  {
    "text": "is actually not about any\nparticular loss function. It's about generic\nclass function. We're going to make some\nspecial simplification for them,",
    "start": "124050",
    "end": "132295"
  },
  {
    "text": "but you don't even\nneed to really think about parameterization\nin most of this lecture.",
    "start": "132295",
    "end": "138000"
  },
  {
    "text": "So the setup is that we\nhave a loss function.  Let's call this\nfunction g theta.",
    "start": "138000",
    "end": "144750"
  },
  {
    "text": "And I'm also going to use x as\nthe variable at certain cases. So the stochastic gradient\ndescent algorithm-- by noise,",
    "start": "144750",
    "end": "153150"
  },
  {
    "text": "I really mean the noise in SGD.  The stochastic gradient descent\nalgorithm that'll we'll analyze",
    "start": "153150",
    "end": "159560"
  },
  {
    "text": "is something like this. So theta t is equal\nto theta t min. Theta plus 1 is equal to t\nminus some noisy gradient.",
    "start": "159560",
    "end": "168690"
  },
  {
    "text": "So we have the full gradient\nplus some stochastic noise,",
    "start": "168690",
    "end": "175580"
  },
  {
    "text": "where the expectation\nof this ksi t is 0.",
    "start": "175580",
    "end": "182010"
  },
  {
    "text": "So this is really\na mean 0 noise. But the distribution, ksi\nt, in the most general case,",
    "start": "182010",
    "end": "200464"
  },
  {
    "text": "the distribution of ksi\nt can depend on theta t.",
    "start": "200464",
    "end": "209210"
  },
  {
    "text": "Right. So the noise distribution\ndepends on which point",
    "start": "209210",
    "end": "214570"
  },
  {
    "text": "you are evaluating at, right? So you can see this\nformulation at least so far,",
    "start": "214570",
    "end": "219760"
  },
  {
    "text": "at a very general level,\ndoes capture, for example, stochastic gradient descent,\nas you usually know,",
    "start": "219760",
    "end": "225040"
  },
  {
    "text": "like the mini-batch\nstochastic gradient descent. Because suppose you take\na mini-batch gradient",
    "start": "225040",
    "end": "230830"
  },
  {
    "text": "with a few samples,\nthen it is indeed can be written as something\nlike the full gradient plus",
    "start": "230830",
    "end": "236440"
  },
  {
    "text": "a stochastic variable,\nwhich means 0. But we are not going to\nanalyze that particular version",
    "start": "236440",
    "end": "242769"
  },
  {
    "text": "because then noise becomes\ntoo complicated in some sense, right? So we're going to analyze\nmuch simpler noise",
    "start": "242770",
    "end": "248961"
  },
  {
    "text": "in most of the\ncases like something like a Gaussian noise. So this is-- so\nstrictly speaking,",
    "start": "248962",
    "end": "254710"
  },
  {
    "text": "this is more about\nnoisy gradient descent than stochastic\nmini-batch stochastic gradient",
    "start": "254710",
    "end": "260319"
  },
  {
    "text": "descent. But they do share a\nlot of similarities. OK? ",
    "start": "260320",
    "end": "266410"
  },
  {
    "text": "And what we are\ntrying to do is we're going to gradually build\nup our intuition about how does this noise affect our\noptimization algorithm.",
    "start": "266410",
    "end": "274990"
  },
  {
    "text": "So we're going to\nstart with various-- we're going to have\nseveral levels warmup.",
    "start": "274990",
    "end": "280159"
  },
  {
    "text": "So the first warmup is that what\nif you have a quadratic loss function?",
    "start": "280160",
    "end": "285620"
  },
  {
    "text": "So quadratic loss\npretty much means that you have a linear\nmodel under the hood. But here I don't even have\nmodel parameterization.",
    "start": "285620",
    "end": "291100"
  },
  {
    "text": "I only have a loss\nfunction, g theta. And say we have a\nquadratic loss function and have Gaussian noise.",
    "start": "291100",
    "end": "296425"
  },
  {
    "text": " And also, we have 1D,\none-dimensional functions,",
    "start": "296425",
    "end": "306060"
  },
  {
    "text": "like theta is one-dimensional. So I guess here\nI'm going to use-- from now on, we're going to\nuse x as my variable just",
    "start": "306060",
    "end": "313330"
  },
  {
    "text": "to make it more consistent with\nthe optimization literature. And g(x) let's just assume\nis 1 over 2 times x squared.",
    "start": "313330",
    "end": "320033"
  },
  {
    "text": "And this 1 over 2\ndoesn't really matter. It's just to make\nthe gradient cleaner.",
    "start": "320033",
    "end": "325400"
  },
  {
    "text": "So then what's the update\nrule for this case? You are optimizing a\nquadratic function basically,",
    "start": "325400",
    "end": "330729"
  },
  {
    "text": "which we are at global\nminimum is at 0. But you are using stochastic\ngradient descent or gradient",
    "start": "330730",
    "end": "336415"
  },
  {
    "text": "descent with noise. So xt plus 1 is\nequal to xt minus eta",
    "start": "336415",
    "end": "343900"
  },
  {
    "text": "within g of xt plus some noise. Let's say the noise has some\nscale sigma and multiply ksi t.",
    "start": "343900",
    "end": "354280"
  },
  {
    "text": "And ksi t is not going to\nhave a scale, so it means 0 on standard deviation 1.",
    "start": "354280",
    "end": "360460"
  },
  {
    "text": "So basically, a noise has\nstandard deviation sigma and Gaussian distribution.",
    "start": "360460",
    "end": "365870"
  },
  {
    "text": "And now, let's\ncompute the gradient. The gradient is really\njust a xt, right?",
    "start": "365870",
    "end": "371820"
  },
  {
    "text": "The gradient of 1 over 2\ntimes x squared is the x. So it would be plus\nsigma times ksi t.",
    "start": "371820",
    "end": "379169"
  },
  {
    "text": "This 1 over eta xt minus\neta times sigma times ksi t.",
    "start": "379170",
    "end": "385410"
  },
  {
    "text": " This one is not working.",
    "start": "385410",
    "end": "390630"
  },
  {
    "text": "Maybe that might have-- ",
    "start": "390630",
    "end": "395980"
  },
  {
    "text": "so what's happening\nhere is really that this is a contraction,\nmeaning that if you have xt,",
    "start": "395980",
    "end": "404400"
  },
  {
    "text": "then it contracts a minimum\nof make the xt smaller by a factor of 1 minus eta.",
    "start": "404400",
    "end": "410850"
  },
  {
    "text": "And this is the\nstochastic term that may make x get\nbigger or smaller,",
    "start": "410850",
    "end": "416360"
  },
  {
    "text": "depending on whether you\nare lucky or not, right? So in some sense,\nwhat happens is that--",
    "start": "416360",
    "end": "422940"
  },
  {
    "text": "so the interesting\nthing is that if-- ",
    "start": "422940",
    "end": "430550"
  },
  {
    "text": "so when xt is large,\nthe contraction",
    "start": "430550",
    "end": "438759"
  },
  {
    "text": "is dominating The\ncontraction, or the shrinking, is dominating, right? Because for example,\nsuppose your xt is here,",
    "start": "438760",
    "end": "446260"
  },
  {
    "text": "then what happens\nis that you first contract it to somewhere here\nby multiplying 1 minus eta.",
    "start": "446260",
    "end": "453160"
  },
  {
    "text": "And then you add some\nstochastic noise. So then maybe at\nthe end of that, you may end up somewhere\nnear the [INAUDIBLE],, right?",
    "start": "453160",
    "end": "462250"
  },
  {
    "text": "But still, largely\nspeaking, you are moving towards 0 because of the\ncontraction, or the shrinking",
    "start": "462250",
    "end": "468699"
  },
  {
    "text": "just because the shrinking\nis doing most of the work. However, so let me finish--\nso the contraction dominates.",
    "start": "468700",
    "end": "480990"
  },
  {
    "text": "However, when xt is small,\nor maybe xt is very small, or maybe xt is 0 for\nsimplicity, then the noise",
    "start": "480990",
    "end": "491330"
  },
  {
    "text": "is dominating the process. ",
    "start": "491330",
    "end": "497380"
  },
  {
    "text": "I guess this-- suppose you start\nsomewhere very, very close to 0",
    "start": "497380",
    "end": "502990"
  },
  {
    "text": "then maybe shrink it. It doesn't really change much\nbecause 1 minus eta times a smaller number is\nstill a small number.",
    "start": "502990",
    "end": "511120"
  },
  {
    "text": "And the noise probably\nmake it somewhere either on the left-hand\nside or the right-hand side.",
    "start": "511120",
    "end": "517979"
  },
  {
    "text": "So the noise becomes\nthe dominating part when xt is small.",
    "start": "517980",
    "end": "523579"
  },
  {
    "text": "And eventually,\nbasically, you are going to basically converge\nto this second case",
    "start": "523580",
    "end": "529730"
  },
  {
    "text": "to some extent. Because if xt is large, then\nyou are moving towards 0.",
    "start": "529730",
    "end": "535970"
  },
  {
    "text": "And what happens is eventually\nxt becomes somewhat small, and noise is kind of like\ngoverning all the process.",
    "start": "535970",
    "end": "542430"
  },
  {
    "text": "So eventually, you\nare just bouncing-- you're bouncing just around\nglobally on a certain level,",
    "start": "542430",
    "end": "554019"
  },
  {
    "text": "right? So you cannot bounce\naround on some very, very high-level values because\nthen your contraction is too",
    "start": "554020",
    "end": "559839"
  },
  {
    "text": "large. You wouldn't be able to bounce\naround that level very much. So eventually, you\nwill be bouncing around",
    "start": "559840",
    "end": "566310"
  },
  {
    "text": "on certain level, depending\non the noise level. Right. So it's kind of like what\nhappens if you have, I guess,",
    "start": "566310",
    "end": "576293"
  },
  {
    "text": "if you think about\nyou drop a ball in a kind of a concave kind\nof thing without any friction.",
    "start": "576293",
    "end": "583050"
  },
  {
    "text": "It's not exactly the\nsame because there you don't have really\nadditive noise, but you still see this\nbouncing around eventually",
    "start": "583050",
    "end": "589740"
  },
  {
    "text": "just because you can\novershoot a little bit. Yeah, maybe that's not exactly\nthe right analogy but anyway,",
    "start": "589740",
    "end": "596040"
  },
  {
    "text": "So eventually, you will\nbounce around some-- the valley of a certain level.",
    "start": "596040",
    "end": "602070"
  },
  {
    "text": "And how do we kind of\nprecisely-- by the way, this sounds like nothing\nreally to do with implicit regularization because\neventually, whatever you do,",
    "start": "602070",
    "end": "609330"
  },
  {
    "text": "you always stay close\nto a global minimum because there's no\neven two other-- there is no two\nglobal minimum, right?",
    "start": "609330",
    "end": "615930"
  },
  {
    "text": "But the intuition is very\nuseful for future things when you kind of\nmove away from this.",
    "start": "615930",
    "end": "622000"
  },
  {
    "text": "So this is indeed important. And let's try to\nbe more precise.",
    "start": "622000",
    "end": "628480"
  },
  {
    "text": "And this is actually our\ncase that we can be precise. So we can solve the recurrence.",
    "start": "628480",
    "end": "634140"
  },
  {
    "text": " So when we solve the\nrecurrence, what happens",
    "start": "634140",
    "end": "641000"
  },
  {
    "text": "is xt is equal to\n1 minus eta xt-- xt plus 1 is equal to 1 eta\nxt minus eta sigma ksi t.",
    "start": "641000",
    "end": "649190"
  },
  {
    "text": "And then you plug in the\ndefinition of xt again. 1 minus eta 1 minus eta xt minus\n1 minus eta sigma ksi t minus 1",
    "start": "649190",
    "end": "659330"
  },
  {
    "text": "minus eta sigma ksi t. And if you rearrange this,\nyou get 1 minus eta squared xt",
    "start": "659330",
    "end": "665880"
  },
  {
    "text": "minus 1 minus 1 minus eta\ntimes eta sigma ksi t minus 1",
    "start": "665880",
    "end": "671130"
  },
  {
    "text": "minus eta sigma ksi t. And if you do this\nfor another level, you get 1 minus eta cube xt\nminus 2 minus 1 minus eta",
    "start": "671130",
    "end": "680400"
  },
  {
    "text": "squared eta sigma ksi t minus 2.",
    "start": "680400",
    "end": "686635"
  },
  {
    "start": "686635",
    "end": "693910"
  },
  {
    "text": "And if you do this\nmore and more, eventually, what you're\ngoing to get is 1 minus eta",
    "start": "693910",
    "end": "700120"
  },
  {
    "text": "to the power t plus 1\ntimes x0 minus eta sigma",
    "start": "700120",
    "end": "705700"
  },
  {
    "text": "times the summation. Summation looks like this. ",
    "start": "705700",
    "end": "712709"
  },
  {
    "text": "It's a linear combination\nbecause ksi t-- ksi k, but the coefficient\nin front of it",
    "start": "712710",
    "end": "719050"
  },
  {
    "text": "is some power of 1 minus eta. ",
    "start": "719050",
    "end": "724480"
  },
  {
    "text": "So from this, you can see\nthat structurally there are several interesting things\nabout this formula, which can give you some intuition.",
    "start": "724480",
    "end": "732025"
  },
  {
    "text": "So one thing is that this thing\nis a very strong contraction,",
    "start": "732025",
    "end": "737930"
  },
  {
    "text": "right? So this is the\ncontraction part, right? ",
    "start": "737930",
    "end": "744060"
  },
  {
    "text": "And in some sense,\nthis term comes from you construct that initial\nvalue by a lot of 1 minus eta",
    "start": "744060",
    "end": "749910"
  },
  {
    "text": "so that basically this\nbecomes negligible. This becomes negligible when\neta times t is much, much bigger",
    "start": "749910",
    "end": "764880"
  },
  {
    "text": "than 1, because 1 minus\neta to the power of t is something like e\nto the minus eta t.",
    "start": "764880",
    "end": "770910"
  },
  {
    "text": "And when eta times t\nis much bigger than 1, then this term\nbecomes super small.",
    "start": "770910",
    "end": "778440"
  },
  {
    "text": "And you can also see from\nthe other term this is, in some sense--",
    "start": "778440",
    "end": "785370"
  },
  {
    "text": "you can view this\nas accumulation, accumulation of the noise.",
    "start": "785370",
    "end": "794070"
  },
  {
    "text": "The noise are not\njust adding up. The noise are accumulated\nin a certain way.",
    "start": "794070",
    "end": "799649"
  },
  {
    "text": "And maybe it is easier\neven to look at this. So the noise that you\nadded at last step",
    "start": "799650",
    "end": "807630"
  },
  {
    "text": "is scaled by eta times sigma. But the noise that you added\nat the second to last time step",
    "start": "807630",
    "end": "815430"
  },
  {
    "text": "is scaled by 1\nminus et-- you have additional factor, 1 minus eta. And how does this 1\nminus eta come from?",
    "start": "815430",
    "end": "821940"
  },
  {
    "text": "This come from the contraction\nof the second last step.",
    "start": "821940",
    "end": "827530"
  },
  {
    "text": "So basically, ksi t\nminus 1 is what you added in the second last step. And then because you do\nanother gradient step--",
    "start": "827530",
    "end": "834779"
  },
  {
    "text": "gradient descent step\non top of that, we still contract the noise\na little bit, right?",
    "start": "834780",
    "end": "840050"
  },
  {
    "text": "So maybe you can see this\nfrom here as well, right? So this is what\nyou got from here.",
    "start": "840050",
    "end": "846100"
  },
  {
    "text": "But this minus eta comes\nfrom the contraction in the very last step. And the same thing\nhappens, right?",
    "start": "846100",
    "end": "851950"
  },
  {
    "text": "So this comes from the\ncontraction in the last two steps. So basically, every\ntime you add noise",
    "start": "851950",
    "end": "858240"
  },
  {
    "text": "in some intermediate\nstep, eventually, this noise will die\neventually at some point",
    "start": "858240",
    "end": "863250"
  },
  {
    "text": "if you run for a long,\nand long enough time just because there is always\na contraction that is applied",
    "start": "863250",
    "end": "868440"
  },
  {
    "text": "after this noisy step, right? So that's why it\nwas-- multiplied",
    "start": "868440",
    "end": "874590"
  },
  {
    "text": "in front of the noise\nis a geometric series. And it depends on when\nyou add in this noise,",
    "start": "874590",
    "end": "881550"
  },
  {
    "text": "the coefficient in\nfront of the noise becomes smaller and smaller.",
    "start": "881550",
    "end": "886660"
  },
  {
    "text": "So you forget about the very,\nvery long history, right? So if you add a noise at\nthe very, very first step,",
    "start": "886660",
    "end": "891840"
  },
  {
    "text": "it doesn't really matter\nbecause you multiply that-- so when k is equal to,\nfor example, t minus 1.",
    "start": "891840",
    "end": "897180"
  },
  {
    "text": "When that's the\nnoise for c1 then you add it in the\nvery first step. Then that noise\nbecomes much less",
    "start": "897180",
    "end": "902790"
  },
  {
    "text": "because you multiply 1 minus eta\nto the power k in front of it.",
    "start": "902790",
    "end": "908256"
  },
  {
    "text": "Because of the\ncontraction, that noise becomes less or less important.",
    "start": "908257",
    "end": "913820"
  },
  {
    "text": "So that's one thing. The accumulation of the\nnoise is kind of like--",
    "start": "913820",
    "end": "919630"
  },
  {
    "text": "prefers the closed history and\nignore the long-term history.",
    "start": "919630",
    "end": "925390"
  },
  {
    "text": "And another thing is that\nthis is a sum of Gaussian. ",
    "start": "925390",
    "end": "931460"
  },
  {
    "text": "Right? Because each of these term is a\nGaussian under our assumption, because ksi is a\nGaussian, and ksi times",
    "start": "931460",
    "end": "936880"
  },
  {
    "text": "something is still a Gaussian. And you can also compute\nthe variance of this.",
    "start": "936880",
    "end": "941990"
  },
  {
    "text": "So the variance of this is equal\nto eta squared sigma squared",
    "start": "941990",
    "end": "948050"
  },
  {
    "text": "times the variance of each of\nthe term, which is something like 1 minus eta\nto the power of 2k.",
    "start": "948050",
    "end": "955050"
  },
  {
    "text": " And the point is that if\nyou take k go to infinity,",
    "start": "955050",
    "end": "962230"
  },
  {
    "text": "then you can know what\nthe limiting variance, what's the variance at the end. So if k goes to t\ngoes to infinity,",
    "start": "962230",
    "end": "972510"
  },
  {
    "text": "you can compute\nthe variance of xt, is roughly something\nlike eta squared",
    "start": "972510",
    "end": "980220"
  },
  {
    "text": "sigma squared if you\nreplace t to be infinity,",
    "start": "980220",
    "end": "986139"
  },
  {
    "text": "1 over 2 eta 2k. And this is eta squared\nsigma squared 1 minus 1--",
    "start": "986140",
    "end": "992365"
  },
  {
    "text": " sorry, my writing\nis not very clean.",
    "start": "992365",
    "end": "999280"
  },
  {
    "text": "1 minus eta squared is how\nyou compute geometric series.",
    "start": "999280",
    "end": "1004410"
  },
  {
    "text": "And this is eta squared\nover sigma squared 2 eta minus eta squared.",
    "start": "1004410",
    "end": "1009840"
  },
  {
    "text": "But this term can be\ndropped because that's going to play out the term. So this is approximately just on\nthe order of eta sigma squared.",
    "start": "1009840",
    "end": "1021880"
  },
  {
    "text": "OK? So in other words, the xt-- eventually as t\ngoes to infinity,",
    "start": "1021880",
    "end": "1029599"
  },
  {
    "text": "eventually has this Gaussian\ndistribution with mean 0. And the variance is something on\nthe order of eta sigma squared.",
    "start": "1029599",
    "end": "1035900"
  },
  {
    "text": " So I think-- so here so far,\nagain, we haven't really",
    "start": "1035900",
    "end": "1042859"
  },
  {
    "text": "talked about implicit bias\nyet, but I think we still already got some intuition\nabout what's happening here",
    "start": "1042859",
    "end": "1050150"
  },
  {
    "text": "with the convex case. So small iterate,\nsmall eta, means",
    "start": "1050150",
    "end": "1058120"
  },
  {
    "text": "that your iterate will have\nsmall bouncing around, right? So small stochasticity\nof the final iterate--",
    "start": "1058120",
    "end": "1069830"
  },
  {
    "text": "in the final-- in the\niterate, because your variance",
    "start": "1069830",
    "end": "1075860"
  },
  {
    "text": "in the iterate xt is smaller. And you have small\nnoise, the same thing.",
    "start": "1075860",
    "end": "1083370"
  },
  {
    "text": "Also, it implies the same thing. ",
    "start": "1083370",
    "end": "1090522"
  },
  {
    "text": "And so basically,\nwhat happens here is that the noise\nonly makes it harder",
    "start": "1090522",
    "end": "1101100"
  },
  {
    "text": "to converge to a global min. ",
    "start": "1101100",
    "end": "1108830"
  },
  {
    "text": "So in some sense,\nif you only care about the quality of the final\nsolution you converge to,",
    "start": "1108830",
    "end": "1114919"
  },
  {
    "text": "the noise is always\npertinent especially if you are willing to\ntake t to infinity, right?",
    "start": "1114920",
    "end": "1122088"
  },
  {
    "text": "So here you can see that\nwhen t goes to infinity, you see it never converge to\nexactly the global min, right? So you should always have some\nvariance around a global min.",
    "start": "1122088",
    "end": "1130513"
  },
  {
    "text": "And you want the variance\nto be as small as possible because you want to be as close\nto the global min as possible.",
    "start": "1130513",
    "end": "1136500"
  },
  {
    "text": "And a noise is only\na hurdle instead of [INAUDIBLE] anything.",
    "start": "1136500",
    "end": "1142250"
  },
  {
    "text": "So this is why in the\nclassical convex case--",
    "start": "1142250",
    "end": "1147725"
  },
  {
    "text": " so this is why the classical\nconvex case composition,",
    "start": "1147725",
    "end": "1164520"
  },
  {
    "text": "typically, when you\nthink about noise, it's only about two things. It's only about A, the\nnoisy gradient descent",
    "start": "1164520",
    "end": "1176260"
  },
  {
    "text": "that leads to less\naccurate solutions.",
    "start": "1176260",
    "end": "1187657"
  },
  {
    "text": "So this is the best thing. That's what we discussed. And second, noisy gradient\ndescent is faster to compute.",
    "start": "1187657",
    "end": "1197835"
  },
  {
    "start": "1197835",
    "end": "1203850"
  },
  {
    "text": "Why the noise come into play? This is because maybe you\nonly sample a few examples to do the sampling,\nwhere you do empirical",
    "start": "1203850",
    "end": "1210750"
  },
  {
    "text": "into a minibatch with\ngradient descent. So noisy gradient descent\nis faster to compute.",
    "start": "1210750",
    "end": "1218330"
  },
  {
    "text": "And the only thing is\nthat you are feeding off these two factors of--",
    "start": "1218330",
    "end": "1228770"
  },
  {
    "text": "that's, I would\nsay, the typical way of thinking about stochastic\ngradient descent when",
    "start": "1228770",
    "end": "1234500"
  },
  {
    "text": "you really think about\na convex case, right? So noise is bad because it\nhurts your final accuracy.",
    "start": "1234500",
    "end": "1241309"
  },
  {
    "text": "But you want to allow some\nnoise in certain cases because you can\ncompute faster, right? So you can trade off in a--",
    "start": "1241310",
    "end": "1247370"
  },
  {
    "text": "trade off in the right way. You can get the fastest\nalgorithm eventually.",
    "start": "1247370",
    "end": "1252440"
  },
  {
    "text": "And you can kind of imagine\nhow you trade off this, right? So at the beginning\nof the optimization,",
    "start": "1252440",
    "end": "1258133"
  },
  {
    "text": "you don't care that\nmuch about accuracy. You don't care about that\nmuch about converting to exactly the global min.",
    "start": "1258133",
    "end": "1263990"
  },
  {
    "text": "You want to go to the global\nmin as fast as possible. So that's why at the beginning,\nyou don't care that much about noise.",
    "start": "1263990",
    "end": "1269808"
  },
  {
    "text": "So that's why in the beginning,\nyou use a large number. And then when you are already\nclose, where your goal changes, because now your goal is\nto really literally go",
    "start": "1269808",
    "end": "1277399"
  },
  {
    "text": "to the global min, period\nSo then you cannot allow any noise. So that's why you have\nthe decay order rate.",
    "start": "1277400",
    "end": "1282800"
  },
  {
    "text": "So that's why there\nis always this kind of decay on linear algorithms. ",
    "start": "1282800",
    "end": "1289320"
  },
  {
    "text": "So, so far, this is about-- ",
    "start": "1289320",
    "end": "1294929"
  },
  {
    "text": "yeah. And also another thing is-- just a side remark,\nwhich is useful for us, which is a useful\ncomparison for us and later.",
    "start": "1294930",
    "end": "1302360"
  },
  {
    "text": "So for any fixed-- suppose you fixed eta and sigma.",
    "start": "1302360",
    "end": "1309020"
  },
  {
    "text": "So the expectation of\nxt is always going to-- convert into 0 as",
    "start": "1309020",
    "end": "1314630"
  },
  {
    "text": "t goes to the infinity, right? So even though there's\na stochasticity, there's a bouncing around,\nyour average is always at 0.",
    "start": "1314630",
    "end": "1322580"
  },
  {
    "text": "So this is saying that\nthere's no bias introduced",
    "start": "1322580",
    "end": "1329990"
  },
  {
    "text": "for the stochasticity. You only introduce, in some\nsense, some fluctuation.",
    "start": "1329990",
    "end": "1337152"
  },
  {
    "text": "Of course, fluctuation\nis also bad, but at least you didn't need to\nuse any bias or systematic bias",
    "start": "1337152",
    "end": "1342540"
  },
  {
    "text": "against any other directions. ",
    "start": "1342540",
    "end": "1350220"
  },
  {
    "text": "So that's another\nstriking remark which we will kind of like\ncompare with in a bit.",
    "start": "1350220",
    "end": "1356370"
  },
  {
    "text": "And also another small\nremark is that this is also called this process.",
    "start": "1356370",
    "end": "1363440"
  },
  {
    "text": "It actually has a name. It's called\nOrnstein-Uhlenbeck process.",
    "start": "1363440",
    "end": "1371780"
  },
  {
    "text": "If you are familiar with this\nprocess in some other context, you can see this is actually\ndoing the same thing.",
    "start": "1371780",
    "end": "1377720"
  },
  {
    "text": "And we are going to call it OU\nprocess just for simplicity. This is going to be kind\nof a basic building block",
    "start": "1377720",
    "end": "1384500"
  },
  {
    "text": "for us to analyze SGD\nand more complex case. OK. So we have kind of like\nunderstood the quadratic.",
    "start": "1384500",
    "end": "1393170"
  },
  {
    "text": "And now let's do the\nmultidimensional quadratic, which is not really\nmuch different. But I think I needed,\nin some sense,",
    "start": "1393170",
    "end": "1400440"
  },
  {
    "text": "just to evolve for the sake of\nfuture, like the future steps. So suppose you have a\nmultidimensional quadratic--",
    "start": "1400440",
    "end": "1407330"
  },
  {
    "text": " suppose you have some\nlike g(x) is equal to one",
    "start": "1407330",
    "end": "1413330"
  },
  {
    "text": "half times x transpose\nAx, where A is a matrix. Dimension d by d. x is the\nvariable in dimension d.",
    "start": "1413330",
    "end": "1421460"
  },
  {
    "text": "And a is PSD. So and then suppose\nthe noise ksi t--",
    "start": "1421460",
    "end": "1428950"
  },
  {
    "text": "now let's not assume. It's just a square\nroot of Gaussian. Let's assume it has\na covariance sigma.",
    "start": "1428950",
    "end": "1435450"
  },
  {
    "text": "And then your update\nrule, let's say, suppose we care\nabout this process",
    "start": "1435450",
    "end": "1440610"
  },
  {
    "text": "where you do gradient descent\nwith this stochasticity ksi t.",
    "start": "1440610",
    "end": "1449760"
  },
  {
    "text": "And then this\nbecomes xt minus eta. The gradient will\njust be a times xt.",
    "start": "1449760",
    "end": "1456044"
  },
  {
    "text": "Then you add ksi t. And this rearranging,\nyou get I minus eta a times xt minus eta ksi t.",
    "start": "1456045",
    "end": "1464760"
  },
  {
    "text": " And you can do the same\nrecursion as we did before,",
    "start": "1464760",
    "end": "1470490"
  },
  {
    "text": "but we replace the definition of\nxt as a function of xt minus 1.",
    "start": "1470490",
    "end": "1475800"
  },
  {
    "text": "And you do this recursively. Eventually, if you\ndo all else, you get i minus eta a\nto the power of t",
    "start": "1475800",
    "end": "1483070"
  },
  {
    "text": "plus 1 times x0 minus eta times\ni minus eta a to the power of k",
    "start": "1483070",
    "end": "1495360"
  },
  {
    "text": "times ksi t minus k. And you can see, this is still\nthe same kind of intuition.",
    "start": "1495360",
    "end": "1500940"
  },
  {
    "text": "This is the contraction. Of course, this is a matrix. We are multiplying\nsomething less than 1, less",
    "start": "1500940",
    "end": "1506549"
  },
  {
    "text": "than any of these. So we are contracting\nmatrix signs. And here, this is how\nthe noise accumulates",
    "start": "1506550",
    "end": "1514170"
  },
  {
    "text": "and also the noise\nin the history, in the very far history.",
    "start": "1514170",
    "end": "1519860"
  },
  {
    "text": "Suppose you take, for example,\nk to be close to t, right? ksi t minus k, this is something\nvery, very far of our history.",
    "start": "1519860",
    "end": "1529370"
  },
  {
    "text": "In a remote history, the\nnoise becomes less important because there's a\ncontraction term applied",
    "start": "1529370",
    "end": "1535370"
  },
  {
    "text": "after noise is added.  And this is-- right.",
    "start": "1535370",
    "end": "1541350"
  },
  {
    "text": "And you can-- in some sense-- so this becomes a more\ncomplicated formula,",
    "start": "1541350",
    "end": "1546570"
  },
  {
    "text": "but you can still somewhat\ndo the same thing. ",
    "start": "1546570",
    "end": "1553379"
  },
  {
    "text": "Suppose if you--\nso you can still do a similar calculation if A\nand sigma are simultaneously",
    "start": "1553380",
    "end": "1569840"
  },
  {
    "text": "diagonalizable. ",
    "start": "1569840",
    "end": "1578090"
  },
  {
    "text": "So if they are not\nsimultaneously diagonalizable,",
    "start": "1578090",
    "end": "1583250"
  },
  {
    "text": "you can still do something to\nsolve this to compute the sum, to simplify the sum.",
    "start": "1583250",
    "end": "1589028"
  },
  {
    "text": "But it's going to be\neven more complicated. So let's only think\nabout case of a in sigma. in the same case, they\nare diagonalizable.",
    "start": "1589028",
    "end": "1595160"
  },
  {
    "text": "Then in some sense, you\ncan just view this as--",
    "start": "1595160",
    "end": "1600740"
  },
  {
    "text": "view as d, different is separate\nOU process in the eigen space.",
    "start": "1600740",
    "end": "1609665"
  },
  {
    "text": " OU process just means\none-dimensional problem",
    "start": "1609665",
    "end": "1617200"
  },
  {
    "text": "in the eigen coordinate system. ",
    "start": "1617200",
    "end": "1624020"
  },
  {
    "text": "Because when you use the\neigen coordinate system, then A and sigma are just\nboth diagonal matrices.",
    "start": "1624020",
    "end": "1629540"
  },
  {
    "text": "And then you are\nbasically just updating as if you are\none-dimensional case.",
    "start": "1629540",
    "end": "1635600"
  },
  {
    "text": "And in some sense, more\nformally, what happens is that's supposed to take A--",
    "start": "1635600",
    "end": "1641420"
  },
  {
    "text": "suppose A is UD U\nin transpose, where D is this diagonal matrix,\nwhich has the eigenvalue of A.",
    "start": "1641420",
    "end": "1652549"
  },
  {
    "text": "And just suppose sigma is\nU diag sigma i U transpose.",
    "start": "1652550",
    "end": "1659510"
  },
  {
    "text": "Then what you can do is that,\nas t goes to the infinity, the xt roughly comes\nfrom this Gaussian,",
    "start": "1659510",
    "end": "1669139"
  },
  {
    "text": "which means 0, because\nthis part got contracted. And you can look at\nthe variance, which",
    "start": "1669140",
    "end": "1674598"
  },
  {
    "text": "looks like something like this. ",
    "start": "1674598",
    "end": "1680490"
  },
  {
    "text": "The power of k times\nsigma epsilon 1 minus eta A to the power of k.",
    "start": "1680490",
    "end": "1685620"
  },
  {
    "start": "1685620",
    "end": "1691254"
  },
  {
    "text": "This is just because we\ncomputed the variance of the [INAUDIBLE].",
    "start": "1691254",
    "end": "1697300"
  },
  {
    "text": "Expectation some matrix W ksi\nksi times ksi W. Transpose",
    "start": "1697300",
    "end": "1705857"
  },
  {
    "text": "the covariance of this linear\ntransformation of Gaussian is equals to W.",
    "start": "1705857",
    "end": "1711860"
  },
  {
    "text": "See? Transpose W transpose, which\nis equal to W times sigma. Both transposed, right? Sorry.",
    "start": "1711860",
    "end": "1717395"
  },
  {
    "text": "That's how you compute\nthe covariance of each of this term, and then\nyou take the sum of them. And A is a symmetric matrix.",
    "start": "1717395",
    "end": "1723690"
  },
  {
    "text": "So A and A transpose\nare the same. And you can do\nthis, and then you can simplify this when you\nhave the eigendecomposition.",
    "start": "1723690",
    "end": "1731550"
  },
  {
    "text": "So you have the\neigendecomposition. Then i minus eta A\nis U times diagonal 1",
    "start": "1731550",
    "end": "1740390"
  },
  {
    "text": "minus eta di U transpose. And sigma is U diag\nsigma i U transpose.",
    "start": "1740390",
    "end": "1749630"
  },
  {
    "text": "And then you can compute this. So you can compute this sum.",
    "start": "1749630",
    "end": "1756040"
  },
  {
    "text": "It could be something\nlike eta squared times--",
    "start": "1756040",
    "end": "1761575"
  },
  {
    "text": "I guess maybe that's also\nthrough the-- let me just do this. If you look at a kth power, you\njust multiply the k in front",
    "start": "1761575",
    "end": "1769370"
  },
  {
    "text": "because the U and U\ntranspose would cancel if you put the sequence [INAUDIBLE].",
    "start": "1769370",
    "end": "1775130"
  },
  {
    "text": "So then this becomes eta squared\ntimes sum of U diagonal--",
    "start": "1775130",
    "end": "1784520"
  },
  {
    "text": "I guess I should\nassume this is sigma. Let's assume this is\nsigma squared just to make it nicer looking\nsigma squared times 1 minutes",
    "start": "1784520",
    "end": "1792080"
  },
  {
    "text": "eta di to the power of\n2k U transpose, right?",
    "start": "1792080",
    "end": "1802510"
  },
  {
    "text": "That's this matrix. And then this is the beauty\nof eigendecomposition because everything\nbecomes on a diagonal,",
    "start": "1802510",
    "end": "1808720"
  },
  {
    "text": "and they have u squared\neta squared u times. Then this becomes\nyou take the sum.",
    "start": "1808720",
    "end": "1814345"
  },
  {
    "text": " So the infinity-- because i\nfrom 1 to infinity would--",
    "start": "1814345",
    "end": "1823230"
  },
  {
    "text": "this is k.  The i is for the coordinate.",
    "start": "1823230",
    "end": "1828950"
  },
  {
    "text": "The k is for the-- ",
    "start": "1828950",
    "end": "1834880"
  },
  {
    "text": "sum over k sigma i squared 1\nminus eta di 2k U transpose.",
    "start": "1834880",
    "end": "1843600"
  },
  {
    "text": "And this becomes eta squared\ntimes U times something like sigma i squared\nover di U transpose.",
    "start": "1843600",
    "end": "1853630"
  },
  {
    "text": "And it is over eta here, so\nlet's remove the eta here. So you can see that basically\nyou have some noise,",
    "start": "1853630",
    "end": "1861279"
  },
  {
    "text": "and noise is something\non this level. So this is the noise level in\nthe i-th eigenvector direction.",
    "start": "1861280",
    "end": "1875340"
  },
  {
    "text": " And noise level depends on-- ",
    "start": "1875340",
    "end": "1883622"
  },
  {
    "text": "this is the lab-- maybe\nlet's just be precise.",
    "start": "1883622",
    "end": "1890049"
  },
  {
    "text": "This is the iterate noise here--",
    "start": "1890050",
    "end": "1898070"
  },
  {
    "text": "iterate stochasticity\nfluctuation that I will-- because we are competing with\nthe fluctuation of the iterate.",
    "start": "1898070",
    "end": "1905779"
  },
  {
    "text": "And the fluctuation level\nin the eigenvector direction depends on the noise\nlevel in that direction,",
    "start": "1905780",
    "end": "1910970"
  },
  {
    "text": "and also depends on how\nstrong the contraction is. If the contraction\nis big then you",
    "start": "1910970",
    "end": "1916399"
  },
  {
    "text": "are going to have smaller\nnoise, smaller iterate stochasticity because you\nhave so strong contraction.",
    "start": "1916400",
    "end": "1923720"
  },
  {
    "text": "And it doesn't withstand a\nlot of noise to build up. And if the noise\nis big, of course,",
    "start": "1923720",
    "end": "1929663"
  },
  {
    "text": "eventually, you're going\nto have like a larger fluctuation of iterate, right?",
    "start": "1929663",
    "end": "1936809"
  },
  {
    "text": "And another thing that\nis useful to realize is that another small\nremark that is useful",
    "start": "1936810",
    "end": "1943070"
  },
  {
    "text": "is that this matrix U diag\nsigma i squared di U transpose.",
    "start": "1943070",
    "end": "1955120"
  },
  {
    "text": "This is always in the\nspan of sigma, right?",
    "start": "1955120",
    "end": "1960880"
  },
  {
    "text": "So if sigma has some direction\nwhere suppose sigma is lower rank-- capital\nsigma is lower rank.",
    "start": "1960880",
    "end": "1967059"
  },
  {
    "text": "So in some direction,\nthere's no noise. And then in those direction,\nxt doesn't have any fluctuation",
    "start": "1967060",
    "end": "1972700"
  },
  {
    "text": "either. So that will be something\nuseful for us in the future.",
    "start": "1972700",
    "end": "1977940"
  },
  {
    "text": "And another thing is that xt-- if you think about what's\nthe rough side of xt,",
    "start": "1977940",
    "end": "1984460"
  },
  {
    "text": "just the norm of xt. This is on order\nof square root eta",
    "start": "1984460",
    "end": "1990970"
  },
  {
    "text": "because this\nquantity is something",
    "start": "1990970",
    "end": "1996260"
  },
  {
    "text": "that doesn't depend on eta. So if you want to look\nat the interdependency, then the norm of\nthe stochasticity",
    "start": "1996260",
    "end": "2005200"
  },
  {
    "text": "or the fluctuation\nin the iterate will be on all of\nsquare root eta.",
    "start": "2005200",
    "end": "2011058"
  },
  {
    "text": "And this is something\nthat's probably good to remember for the moment. It'll be useful for us\nin the future as well.",
    "start": "2011058",
    "end": "2017820"
  },
  {
    "text": " Any questions so far? ",
    "start": "2017820",
    "end": "2028810"
  },
  {
    "text": "[INAUDIBLE] should it\nbe also summed to i?",
    "start": "2028810",
    "end": "2035066"
  },
  {
    "text": "Right. Right. Yeah. But all of those depends\non dimension, for example. It depends on how large\nour sigma i's and how large",
    "start": "2035066",
    "end": "2042220"
  },
  {
    "text": "those di's are. But in terms of\ndependency on eta, this is on order\nof square root eta.",
    "start": "2042220",
    "end": "2048138"
  },
  {
    "text": "That's what I mean. [INAUDIBLE] Yeah, yeah.",
    "start": "2048139",
    "end": "2053645"
  },
  {
    "text": "Sure, sure. Yeah. I guess I'm only talking\nabout depends on eta so far. ",
    "start": "2053645",
    "end": "2060949"
  },
  {
    "text": "That's like the standard\ndeviation of xt essentially? Sure.",
    "start": "2060949",
    "end": "2066350"
  },
  {
    "text": "Yeah. [INAUDIBLE] square root n. Yes. Well, it's the size of x\nalso takes into account",
    "start": "2066350",
    "end": "2073129"
  },
  {
    "text": "a contraction term. So is this for large t so\nthat the contraction for it",
    "start": "2073130",
    "end": "2078649"
  },
  {
    "text": "turns sufficiently small? Yes. I'm talking about the\ncase where t is infinity.",
    "start": "2078650",
    "end": "2085179"
  },
  {
    "text": "t is going to infinity. So maybe one way to\nthink about this-- I think I kind of sense\nwhat your question is.",
    "start": "2085179",
    "end": "2092210"
  },
  {
    "text": "So this is the fluctuation\nin the [INAUDIBLE] iterate. So in the iterate when\nt is kind of infinity,",
    "start": "2092210",
    "end": "2099080"
  },
  {
    "text": "it's different from the\nnoise you added at each time. So again, that's actually\na very good question.",
    "start": "2099080",
    "end": "2104920"
  },
  {
    "text": "So if you look at the noise\nthat you added at each time-- so this is how large is this.",
    "start": "2104920",
    "end": "2110920"
  },
  {
    "text": "This is on order of\neta if you ignore-- of course, you can ignore\nother dependencies except eta.",
    "start": "2110920",
    "end": "2116230"
  },
  {
    "text": "So each time you add some\nnoise on the order of eta, eventually, all of\nthis noise build up.",
    "start": "2116230",
    "end": "2122605"
  },
  {
    "text": "They got add up together. And they add up to something\non order of square root eta.",
    "start": "2122605",
    "end": "2128970"
  },
  {
    "text": "So that's how the noise\nkind of like accumulate. But it wouldn't accumulate\nto infinity just",
    "start": "2128970",
    "end": "2134105"
  },
  {
    "text": "because of the\ncontraction, because of this, this term that\nalso contracts the noise to some extent, a little bit.",
    "start": "2134105",
    "end": "2140060"
  },
  {
    "text": "But still the noise\nbuild up with one kind of like half order higher\nin terms of eta, right?",
    "start": "2140060",
    "end": "2147200"
  },
  {
    "text": "So it accumulates from eta\nto square root eta over time.",
    "start": "2147200",
    "end": "2153060"
  },
  {
    "text": "So, yeah. ",
    "start": "2153060",
    "end": "2161498"
  },
  {
    "text": "Order of eta.  OK. So we have a pretty kind\nof good understanding",
    "start": "2161498",
    "end": "2168410"
  },
  {
    "text": "of what's happening basically. Basically, eventually,\nit's bouncing around",
    "start": "2168410",
    "end": "2173994"
  },
  {
    "text": "with the radius, something\nlike a square root eta in the value of this quadratic.",
    "start": "2173995",
    "end": "2179953"
  },
  {
    "text": "And also, you don't bounce\naround in those direction where you need to add noise. So that's the [INAUDIBLE].",
    "start": "2179953",
    "end": "2187700"
  },
  {
    "text": "And now let's look at-- Is there a reason-- can we talk about the idea\nof noise in this direction",
    "start": "2187700",
    "end": "2194450"
  },
  {
    "text": "back onto minibatch\nor stochastic gradient descent in a natural way, or\nis that not the [INAUDIBLE]??",
    "start": "2194450",
    "end": "2203640"
  },
  {
    "text": "So is there any way for\nus to map back how-- so you want to connect\nback to the world",
    "start": "2203640",
    "end": "2209100"
  },
  {
    "text": "where we have the minibatch\nor gradient descent? So for convex case,\nit's not that difficult.",
    "start": "2209100",
    "end": "2217930"
  },
  {
    "text": "So what do you do--\nbasically, what you say is that what is sigma. Sigma is your-- so in our\ncalculations of sigma,",
    "start": "2217930",
    "end": "2225180"
  },
  {
    "text": "in our definition, a sigma is\nthe covariance of the noise",
    "start": "2225180",
    "end": "2230744"
  },
  {
    "text": "in a gradient, right? You can compute what's the\ncovariance of the noise when you use mini-batch gradients.",
    "start": "2230745",
    "end": "2237870"
  },
  {
    "text": "So you can compute that. And that is something that\nmight change over time.",
    "start": "2237870",
    "end": "2244770"
  },
  {
    "text": "But I think you\ncan pretty much say that when you are kind of\nclose to the global minimum,",
    "start": "2244770",
    "end": "2250380"
  },
  {
    "text": "the changes of the covariance\nof the gradient-- the changes",
    "start": "2250380",
    "end": "2255539"
  },
  {
    "text": "in the covariance\nof the gradient, of the mini-batch\ngradient, is negligible.",
    "start": "2255540",
    "end": "2262230"
  },
  {
    "text": "It's even higher long term. You can basically ignore it. So basically, if you\nwant to map this back to the mini-batch\ngradient, this sigma",
    "start": "2262230",
    "end": "2268859"
  },
  {
    "text": "will just map to the covariance\nof the mini-batch gradient",
    "start": "2268860",
    "end": "2279560"
  },
  {
    "text": "at the global min theta star. So then you can kind\nof face everything.",
    "start": "2279560",
    "end": "2286640"
  },
  {
    "text": "But I don't think you get\nanything super interpretable anyways, so that's why\nI didn't get into it.",
    "start": "2286640",
    "end": "2291997"
  },
  {
    "text": "[INAUDIBLE] it just seems\nlike, if the global minimum is",
    "start": "2291998",
    "end": "2298370"
  },
  {
    "text": "very flat on some\ndimension, the variance would have a very large effect.",
    "start": "2298370",
    "end": "2305980"
  },
  {
    "text": "Yes, exactly. Exactly, exactly. That's exactly correct. So this is-- so suppose\nyou have two dimensions.",
    "start": "2305980",
    "end": "2314180"
  },
  {
    "text": "I think this is actually\na very good question. So if you have one direction\nwhich is like this and suppose",
    "start": "2314180",
    "end": "2320350"
  },
  {
    "text": "you have another direction\nwhich is like this.",
    "start": "2320350",
    "end": "2325820"
  },
  {
    "text": "So the question is,\nhow does the noise affect these two dimensions?",
    "start": "2325820",
    "end": "2331930"
  },
  {
    "text": "And also, there's a question\nabout how do you evaluate the impact of the noise? What's the metric you\nare thinking about?",
    "start": "2331930",
    "end": "2337910"
  },
  {
    "text": "So, so far, I'm\nthinking about how does the noise change the\nfluctuation in the iterate,",
    "start": "2337910",
    "end": "2344180"
  },
  {
    "text": "right? So suppose I'm adding noise\nto the same amount of noise,",
    "start": "2344180",
    "end": "2351280"
  },
  {
    "text": "one unit of noise in\nboth of these cases. I think it's indeed true that\nstochastic gradient descent itself will kind of\nfluctuate more in this case.",
    "start": "2351280",
    "end": "2359863"
  },
  {
    "text": "Actually, it probably\nwouldn't look like this. It'd probably look\nlike something like-- maybe you do some kind of like\nstochastic things like this.",
    "start": "2359863",
    "end": "2367510"
  },
  {
    "text": "But this is-- you're\ngoing to have a larger radius for bouncing around.",
    "start": "2367510",
    "end": "2374665"
  },
  {
    "text": "And here you're going to\nhave a smaller radius. You are going to be more\ncloser to the value.",
    "start": "2374665",
    "end": "2380109"
  },
  {
    "text": "However, even if you have\na larger radius here, it doesn't necessarily\nmean that you",
    "start": "2380110",
    "end": "2385240"
  },
  {
    "text": "have a larger effect\non a function value because you fluctuate a lot, but\nthe function is flat, as well.",
    "start": "2385240",
    "end": "2392839"
  },
  {
    "text": "So it's OK to fluctuate\nmore in some cases. So I think let's see whether we\ncan compute the fluctuations.",
    "start": "2392840",
    "end": "2401150"
  },
  {
    "text": "So suppose you have sigma\nsquared over i di squared. This is the radius\nof the fluctuation.",
    "start": "2401150",
    "end": "2408160"
  },
  {
    "text": "And you multiply--\nwhat do you multiply? You multiply di because\ndi is the curvature",
    "start": "2408160",
    "end": "2415869"
  },
  {
    "text": "of your open function. So this is sigma i squared. This is something that doesn't\ndepend on the curvature.",
    "start": "2415870",
    "end": "2420960"
  },
  {
    "text": "di is the curvature,\nso this is kind of like x squared, the\nfluctuation you have.",
    "start": "2420960",
    "end": "2427119"
  },
  {
    "text": "So if you look at the effects\nto the function value-- and it may not depend on the\ncurvature that much, at least",
    "start": "2427120",
    "end": "2434490"
  },
  {
    "text": "not for the quadratic. ",
    "start": "2434490",
    "end": "2441210"
  },
  {
    "text": "Yeah. Right. So that make sense?",
    "start": "2441210",
    "end": "2447100"
  },
  {
    "text": "OK, cool. All right. So now let's talk about\nnonquadratic function.",
    "start": "2447100",
    "end": "2452710"
  },
  {
    "text": "And then this is kind of where\nthe things become interesting, but it's interesting only on\ntop of what we have discussed.",
    "start": "2452710",
    "end": "2459090"
  },
  {
    "text": "That's why we need\nto have the warmup. So nonquadratic--\nand so far, I'm",
    "start": "2459090",
    "end": "2468099"
  },
  {
    "text": "still doing kind of\nlike-- you can still think of this as a\nconvex function, even",
    "start": "2468100",
    "end": "2473140"
  },
  {
    "text": "one-dimensional\nconvex form so far. I'm going to change\nthat a little bit.",
    "start": "2473140",
    "end": "2478990"
  },
  {
    "text": "And again, for\nsimplicity, let's say, with the loss of\ngenerality, let's assume the global minimizer\nof this g(x)",
    "start": "2478990",
    "end": "2490720"
  },
  {
    "text": "is just the 0 back here right? So we still have 0 as\nthe global minimizer. We are still doing\nsomething around 0.",
    "start": "2490720",
    "end": "2497065"
  },
  {
    "start": "2497065",
    "end": "2504030"
  },
  {
    "text": "And I think I'm using a matrix\nnotation right now here, but I think I realized that\nin the matrix notation--",
    "start": "2504030",
    "end": "2512829"
  },
  {
    "text": "oh, I remember. OK. So the reason why I'm\nusing matrix rotation's because I don't have to do the\ntwo things there, the scalar",
    "start": "2512830",
    "end": "2518980"
  },
  {
    "text": "case in the matrix case. But for simplicity, you can-- in your mind, you\ncan pretty much interpret all of\nthese as scalars.",
    "start": "2518980",
    "end": "2527074"
  },
  {
    "text": "OK, so I'm also seeing\nthat-- because 0 is the global min, then that\nmeans that the gradient at 0",
    "start": "2527074",
    "end": "2533950"
  },
  {
    "text": "is 0, right? That's a necessary condition. And also, that means that\nthe Hessian g(0) is PSD.",
    "start": "2533950",
    "end": "2544930"
  },
  {
    "text": "OK. And let's also assume-- this is the part\nwhere they kind of",
    "start": "2544930",
    "end": "2551230"
  },
  {
    "text": "like become not super\nrigorous, but we can make this part rigorous. It's just that I\nwouldn't have time",
    "start": "2551230",
    "end": "2560160"
  },
  {
    "text": "to do all the rigorous stuff. But this part is doable. So suppose we'll assume the\niterate are close to 0--",
    "start": "2560160",
    "end": "2568400"
  },
  {
    "text": "so start from somewhere\nthat's close to 0. And then you can do\nTaylor expansion around 0.",
    "start": "2568400",
    "end": "2578000"
  },
  {
    "text": " So what you do is\nyou do xt plus 1",
    "start": "2578000",
    "end": "2583480"
  },
  {
    "text": "is equal to xt minus eta\ntimes gradient xt plus ksi t.",
    "start": "2583480",
    "end": "2590760"
  },
  {
    "text": "And you do Taylor expansion\nto approximate the gradient",
    "start": "2590760",
    "end": "2595780"
  },
  {
    "text": "at ksi t-- at xt. So how do you do a\nTaylor expansion?",
    "start": "2595780",
    "end": "2601250"
  },
  {
    "text": "So if you take expand\nat 0, what have you got? You're going to get the\nnabla g(0) plus nabla",
    "start": "2601250",
    "end": "2607349"
  },
  {
    "text": "squared g(0) times\nxt minus 0 and plus",
    "start": "2607350",
    "end": "2615606"
  },
  {
    "text": "nabla cube g(0) at xt xt.",
    "start": "2615606",
    "end": "2621970"
  },
  {
    "text": "And maybe let's have also\nhigh order terms, which we are going to\nignore heuristically,",
    "start": "2621970",
    "end": "2628569"
  },
  {
    "text": "and then we're going to\nget epsilon t [INAUDIBLE]..",
    "start": "2628570",
    "end": "2636290"
  },
  {
    "text": "So I guess if you're not\nfamiliar with the matrixing, then I guess this is really just\nsaying that g prime xt roughly",
    "start": "2636290",
    "end": "2645079"
  },
  {
    "text": "goes to g prime 0 times\nxt plus g prime 0 times xt",
    "start": "2645080",
    "end": "2651230"
  },
  {
    "text": "squared plus g cubed third\norder of theta times--",
    "start": "2651230",
    "end": "2656415"
  },
  {
    "text": " wait, what I'm doing here. So there is no xt here.",
    "start": "2656415",
    "end": "2663490"
  },
  {
    "text": "There's xt. And xt squared plus\nin high order terms,",
    "start": "2663490",
    "end": "2670950"
  },
  {
    "text": "something like this, right? But I want to use\nnotation that this-- if you do the matrixing that\nthis is the matrix vector",
    "start": "2670950",
    "end": "2680069"
  },
  {
    "text": "product. And this is a tensor\nvector product. Let me explain\nthat a little bit.",
    "start": "2680070",
    "end": "2687780"
  },
  {
    "text": "So if you do the\nmultidimensional case, this is a third-order tensor\nof dimensions d by d by d.",
    "start": "2687780",
    "end": "2697930"
  },
  {
    "text": "And suppose you have a t\nthat is a third-order tensor. Then I'm using this t x y,\nwhere x and y are all vectors,",
    "start": "2697930",
    "end": "2713550"
  },
  {
    "text": "is defined to be a vector. So this is the multiplication\nof this tensor with two vectors.",
    "start": "2713550",
    "end": "2721710"
  },
  {
    "text": "First of all, it's a vector. And second, the\ndefinition of this is that the i-th part of this\nis the sum over jk ti jk, xi yj.",
    "start": "2721710",
    "end": "2736160"
  },
  {
    "text": "So xk y-- xj yk.",
    "start": "2736160",
    "end": "2743591"
  },
  {
    "text": "So basically, sum over the\nremaining part of j and k, and you leave the i alone,\nleft, and that's the outcome.",
    "start": "2743592",
    "end": "2754330"
  },
  {
    "text": "So this is basically\nthe Taylor expansion in multiple dimensions.",
    "start": "2754330",
    "end": "2760210"
  },
  {
    "text": "OK. By the way, just a reminder\nfor the scribe note takers, I think, for this kind of small\nthings I write on the side,",
    "start": "2760210",
    "end": "2768880"
  },
  {
    "text": "please also take notes\nfor those because they are useful for readers, as well.",
    "start": "2768880",
    "end": "2775089"
  },
  {
    "text": "If someone doesn't have\ntime to take the lectures, they read the lecture notes. I think these small\nexplanations are also useful.",
    "start": "2775090",
    "end": "2781720"
  },
  {
    "text": "You can just have a small\nkind of remark of some writing in the paragraph left bound. ",
    "start": "2781720",
    "end": "2789567"
  },
  {
    "text": "So, all right. So we have to do then\nTaylor expansion. And then we can see that's why--",
    "start": "2789567",
    "end": "2795700"
  },
  {
    "text": "so we're expecting something\nsomewhat similar to what we had done before, right? And indeed, you will\nsee that, because, A,",
    "start": "2795700",
    "end": "2803690"
  },
  {
    "text": "this is going to be 0 because\nthis is the gradient at 0.",
    "start": "2803690",
    "end": "2809200"
  },
  {
    "text": "And this is 0. So basically, what you can get\nis that xt minus eta times--",
    "start": "2809200",
    "end": "2820480"
  },
  {
    "text": "OK, so I guess let me\ndefine this for simplicity H",
    "start": "2820480",
    "end": "2826960"
  },
  {
    "text": "to be this like H to\nbe the Hessian at 0.",
    "start": "2826960",
    "end": "2838270"
  },
  {
    "text": "Then you can rewrite this as\nxt minus eta H xt minus eta",
    "start": "2838270",
    "end": "2851260"
  },
  {
    "text": "ksi t and minus the third term.",
    "start": "2851260",
    "end": "2857570"
  },
  {
    "text": "I guess I'm also\ngoing to define-- let me see what's\nmy location here.",
    "start": "2857570",
    "end": "2863366"
  },
  {
    "text": "I define T to be the\nthird other derivative,",
    "start": "2863366",
    "end": "2870369"
  },
  {
    "text": "and then this is T xt xt. ",
    "start": "2870370",
    "end": "2877910"
  },
  {
    "text": "And high-order terms, let's\nignore that formula from now on. I know we read up on-- we had\na formula dealing with eta,",
    "start": "2877910",
    "end": "2883873"
  },
  {
    "text": "but we just have an\napproximation here. And then this is i minus eta H\nxt minus eta ksi t minus eta T",
    "start": "2883873",
    "end": "2897396"
  },
  {
    "text": "xt xt. And I think you can see. I guess what I-- I was hoping for you to see is\nthat the third-order term is",
    "start": "2897396",
    "end": "2908460"
  },
  {
    "text": "something new. But this first-order term and\nsecond-order term are not new. The noise term and\nthe second-order term",
    "start": "2908460",
    "end": "2916079"
  },
  {
    "text": "are exactly what we\nhad before, right? So if you look at here--",
    "start": "2916080",
    "end": "2921585"
  },
  {
    "text": " so for the quadratic\ncase, you have extraction,",
    "start": "2921585",
    "end": "2930970"
  },
  {
    "text": "and you have a noise. The extraction is linear,\nand you have the noise. And now the only difference is\nthat you have additional term",
    "start": "2930970",
    "end": "2937707"
  },
  {
    "text": "from the third-order derivative. And that's expected because if\nyou don't have the third-order, you ignore the third-order term.",
    "start": "2937707",
    "end": "2943090"
  },
  {
    "text": "And it becomes just quadratic. So that's why we wanted to\nexpand out to the third order because we want to really\nuse the fact that this is not",
    "start": "2943090",
    "end": "2950470"
  },
  {
    "text": "a quadratic function. So basically, you can think of\nthis as two process going on, right? So one process is this\nOU process kind of like--",
    "start": "2950470",
    "end": "2959650"
  },
  {
    "text": "this kind of like basic\none about quadratic. And you have additional\nterm that in fact make",
    "start": "2959650",
    "end": "2965320"
  },
  {
    "text": "a little bit more complicated. Right. And how do we proceed here?",
    "start": "2965320",
    "end": "2973380"
  },
  {
    "text": "So if you really think about-- ",
    "start": "2973380",
    "end": "2984470"
  },
  {
    "text": "so in some sense-- so there is one thing-- This is a heuristic derivation.",
    "start": "2984470",
    "end": "2995290"
  },
  {
    "text": "So when in certain\ncases, you're kind of attempting to even drop the\nthird-order term because maybe",
    "start": "2995290",
    "end": "3000720"
  },
  {
    "text": "you have a small term. And let's try to do that. So just drop the\nthird-order term.",
    "start": "3000720",
    "end": "3013150"
  },
  {
    "text": "Just drop it, all right? So suppose you drop it. Then you have this process, x is\nupdated by something like this.",
    "start": "3013150",
    "end": "3024280"
  },
  {
    "text": "Right? So this is the process. This is something\nwe have analyzed. And we know that,\nwith convergence, xt",
    "start": "3024280",
    "end": "3035240"
  },
  {
    "text": "will be something on\nthe order third eta. So here I'm ignoring\nall the dependencies,",
    "start": "3035240",
    "end": "3041240"
  },
  {
    "text": "except the dependency on eta.  And now, if you\nlook back on what",
    "start": "3041240",
    "end": "3049520"
  },
  {
    "text": "happened with this\nthird-order term, so when xt is on this order. So eta T xt xt.",
    "start": "3049520",
    "end": "3058488"
  },
  {
    "text": "What is this? This is on order of\neta squared because xt contributes square root eta. This actually contributes square\neta, and there's an eta here.",
    "start": "3058488",
    "end": "3066839"
  },
  {
    "text": "So basically, we have\nan eta square term which sounds very small.",
    "start": "3066840",
    "end": "3072140"
  },
  {
    "text": "Why this is very small? This is much smaller. So eta squared is much,\nmuch smaller than,",
    "start": "3072140",
    "end": "3079440"
  },
  {
    "text": "for example, eta ksi t, right? But that probably is\nnot unfair because ksi t",
    "start": "3079440",
    "end": "3084680"
  },
  {
    "text": "is doing some random stuff. But eta squared is also\nmuch, much smaller than even",
    "start": "3084680",
    "end": "3091190"
  },
  {
    "text": "just eta H xt, which is on\nthe order of eta to the 1.5.",
    "start": "3091190",
    "end": "3100849"
  },
  {
    "text": "So basically, the\nchanges of your process where the two other\nchanges of your process--",
    "start": "3100850",
    "end": "3106190"
  },
  {
    "text": "is these two terms. Right? And this term you\ncan say comparing",
    "start": "3106190",
    "end": "3111420"
  },
  {
    "text": "with that is a little bit like\nunfair because that term is",
    "start": "3111420",
    "end": "3117210"
  },
  {
    "text": "doing some random stuff, right? So maybe you\nshouldn't compare it with the absolute value of it\njust because eventually there",
    "start": "3117210",
    "end": "3124137"
  },
  {
    "text": "will be some cancellation. But at least you can pair it\nwith the other deterministic term eta H xt.",
    "start": "3124137",
    "end": "3130470"
  },
  {
    "text": "You are still-- this eta\nsquared term is still much smaller than the\ndeterministic term.",
    "start": "3130470",
    "end": "3135960"
  },
  {
    "text": "So in some sense,\nit's very tempting to say that OK, this third\norder term is tx xt xt thing",
    "start": "3135960",
    "end": "3142680"
  },
  {
    "text": "A is very small. Right? So the conclusion\nwould be that--",
    "start": "3142680",
    "end": "3149370"
  },
  {
    "text": " the conclusion is that\nthis is kind of negligible.",
    "start": "3149370",
    "end": "3155160"
  },
  {
    "start": "3155160",
    "end": "3161210"
  },
  {
    "text": "And indeed, it's true when-- and this is indeed true. This is negligible.",
    "start": "3161210",
    "end": "3167385"
  },
  {
    "text": "And indeed, it is true\nunder one condition. When the H, the Hessian,\nis strictly PSD.",
    "start": "3167385",
    "end": "3177090"
  },
  {
    "text": "So that's when you\nhave contraction in all different directions. ",
    "start": "3177090",
    "end": "3184650"
  },
  {
    "text": "So however, when H\nis not strictly PSD-- so for example, in\nsome direction--",
    "start": "3184650",
    "end": "3190598"
  },
  {
    "text": "so basically, in other words,\nif you think about this-- so this term is only on order\nof eta to 1.5, where H is not 0,",
    "start": "3190598",
    "end": "3199529"
  },
  {
    "text": "right? So if H is 0 in some direction,\nthen this eta H t term is just literally 0.",
    "start": "3199530",
    "end": "3206060"
  },
  {
    "text": "So then the eta squared\nterm is winning, right? So basically in some\ndirection where H is 0",
    "start": "3206060",
    "end": "3221490"
  },
  {
    "text": "then eta H xt is just\na 0 in that direction. ",
    "start": "3221490",
    "end": "3229030"
  },
  {
    "text": "And then eta squared\nbecomes the largest update.",
    "start": "3229030",
    "end": "3239051"
  },
  {
    "text": "[INAUDIBLE] ",
    "start": "3239051",
    "end": "3246745"
  },
  {
    "text": "Eta ksi t is always the largest\nif you really kind of look at the absolute value, right? So eta ksi t is on\nthe order of what?",
    "start": "3246745",
    "end": "3252829"
  },
  {
    "text": "This is on the order\nof just the eta. It's always the\nlargest, but I think I-- I kind of-- I try to--",
    "start": "3252830",
    "end": "3258859"
  },
  {
    "text": "I'm trying to argue that eta\nksi t if you compare with that, it's a little bit kind of\nlike misleading in the sense",
    "start": "3258860",
    "end": "3267090"
  },
  {
    "text": "that eta ksi t is doing\nrandom stuff, right? So in one step, it's\ngoing positive direction.",
    "start": "3267090",
    "end": "3272108"
  },
  {
    "text": "The other step, it's going\nto negative direction.  So eventually\nbasically, what happens",
    "start": "3272108",
    "end": "3277980"
  },
  {
    "text": "is that if you have a\nrandom stochastic term-- suppose you have a stochastic\nterm or min 0 term, such that,",
    "start": "3277980",
    "end": "3287300"
  },
  {
    "text": "if you have one step, it's\non the order of eta implies",
    "start": "3287300",
    "end": "3294200"
  },
  {
    "text": "eventually like [INAUDIBLE]--",
    "start": "3294200",
    "end": "3299971"
  },
  {
    "text": "this will be something like\non the square root of eta. That's kind of like what we have\ndiscussed in the quadratic case",
    "start": "3299971",
    "end": "3307300"
  },
  {
    "text": "right? If every step is\nstochastic term, it's going to give you a\neta noise perturbation.",
    "start": "3307300",
    "end": "3313190"
  },
  {
    "text": "And then, eventually, they will\nbuild up to square root eta. However, when you have\na deterministic term--",
    "start": "3313190",
    "end": "3321980"
  },
  {
    "text": " so if one step is\nsomething like eta,",
    "start": "3321980",
    "end": "3328420"
  },
  {
    "text": "then eventually it's\nunclear what will build up. It probably would build up\nto eta T because eta times",
    "start": "3328420",
    "end": "3335020"
  },
  {
    "text": "little t is a-- It won't kind of like--\nthey won't cancel. Maybe this is a little bit--",
    "start": "3335020",
    "end": "3342000"
  },
  {
    "text": "I'm not sure whether this is\nthe best way to explain it,",
    "start": "3342000",
    "end": "3347337"
  },
  {
    "text": "although I say this. So another way,\nthis is a heuristic because it requires\na little bit more--",
    "start": "3347337",
    "end": "3352618"
  },
  {
    "text": "if you want to\nformalize all of this, it requires a little more work. But I guess what I'm-- maybe what I'm\nsaying is basically",
    "start": "3352618",
    "end": "3358720"
  },
  {
    "text": "like the low equality. The largest update.",
    "start": "3358720",
    "end": "3364140"
  },
  {
    "text": "But in all cases, by locally\nthe oldest update of course, is eta ksi t. But this one will have\ncancellation over time,",
    "start": "3364140",
    "end": "3371079"
  },
  {
    "text": "because in future,\nyou're going to have like a-- you're going to\nmove in different directions. So that's why it's probably\ngood to also compare",
    "start": "3371080",
    "end": "3378010"
  },
  {
    "text": "with the deterministic\nchanges, which is the eta H xt. And then when you\ncompare it with that,",
    "start": "3378010",
    "end": "3383110"
  },
  {
    "text": "typically the\ndeterministic changes, is bigger than the\neta squared term from the third-order derivative.",
    "start": "3383110",
    "end": "3388900"
  },
  {
    "text": "But when H is 0\nin some direction, then it's no longer achievable.",
    "start": "3388900",
    "end": "3394040"
  },
  {
    "text": " But you can-- sometimes you\ncan prove that if H is--",
    "start": "3394040",
    "end": "3401079"
  },
  {
    "start": "3401080",
    "end": "3406340"
  },
  {
    "text": "so when H is strictly\npositive, it's nonzero, then it's negligible. And otherwise, it\nbecomes trickier.",
    "start": "3406340",
    "end": "3413010"
  },
  {
    "text": "So if H has a completely flat\ndirection, it becomes tricky.",
    "start": "3413010",
    "end": "3420000"
  },
  {
    "text": "So I think here is a good point. Maybe let's just\ncontinue with this. So when this is the case, then--",
    "start": "3420000",
    "end": "3427190"
  },
  {
    "text": "so the third-- so in this\ncase, the third-order term",
    "start": "3427190",
    "end": "3433880"
  },
  {
    "text": "will introduce some\nbiases but very small,",
    "start": "3433880",
    "end": "3439039"
  },
  {
    "text": "some bias but very small. And small in the sense\nthat, as eta goes to 0,",
    "start": "3439040",
    "end": "3445930"
  },
  {
    "text": "this becomes negligible. And I think I have some\nkind of like figures here.",
    "start": "3445930",
    "end": "3458190"
  },
  {
    "start": "3458190",
    "end": "3464560"
  },
  {
    "text": "And so I have this figure.  Let's see whether\nyou can see it.",
    "start": "3464560",
    "end": "3470420"
  },
  {
    "text": "Yes. So this is a little\nbit small here. Maybe this way.",
    "start": "3470420",
    "end": "3476300"
  },
  {
    "text": "So the function is a\none-dimensional function, is a complex one.",
    "start": "3476300",
    "end": "3481380"
  },
  {
    "text": "So I'm in the case where the\nH is strictly bigger than 0, so H. Because it's\none-dimensional,",
    "start": "3481380",
    "end": "3487280"
  },
  {
    "text": "this is strictly\nconvex function. So this is the function, but\nit's not like a quadratic.",
    "start": "3487280",
    "end": "3493310"
  },
  {
    "text": "It's something like-- I think it's quadratic\non both sides, but it is not the same\nkind of curvature.",
    "start": "3493310",
    "end": "3498687"
  },
  {
    "text": "The left-hand side is more\nflat, and the right-hand side is more sharp. ",
    "start": "3498687",
    "end": "3506910"
  },
  {
    "text": "And if you do gradient\ndescent, so I guess probably the only thing\nimportant is this.",
    "start": "3506910",
    "end": "3512790"
  },
  {
    "text": "So if you look at-- this is after you take\n100 to 1,000 steps",
    "start": "3512790",
    "end": "3519360"
  },
  {
    "text": "of stochastic gradient descent. And you can see that\nthe mean, the iterate",
    "start": "3519360",
    "end": "3526940"
  },
  {
    "text": "is bouncing around between. This is the distribution of\nthe iterate, distribution",
    "start": "3526940",
    "end": "3533750"
  },
  {
    "text": "of the xt when t is 1024.",
    "start": "3533750",
    "end": "3538900"
  },
  {
    "text": "So 1024 is pretty\nbig, considered to be infinity, right? And you can see that\nit's bouncing around 0.",
    "start": "3538900",
    "end": "3545680"
  },
  {
    "text": "0 is the global minimum. But the mean is no\nlonger 0 anymore",
    "start": "3545680",
    "end": "3551410"
  },
  {
    "text": "because you have the third-order\nkind of like derivative. And the mean is\nsomething left to 0.",
    "start": "3551410",
    "end": "3559390"
  },
  {
    "text": "In some sense, you\nprefer the left-hand side a little bit more than\nthe right-hand side because it's easier to\nstay on the left-hand side.",
    "start": "3559390",
    "end": "3566170"
  },
  {
    "text": "The left-hand side is\nlighter, so it's easier to stay there because the\ncontraction is weaker. And the right-hand\nside is sharper",
    "start": "3566170",
    "end": "3572589"
  },
  {
    "text": "and is-- you add some noise,\nand you kind of contract it. And you go back to the\n0 quickly, more quickly.",
    "start": "3572590",
    "end": "3579559"
  },
  {
    "text": "So that's why the bias is\ntowards that left-hand side, where you have\nlighter curvature.",
    "start": "3579560",
    "end": "3586390"
  },
  {
    "text": "But the bias is\nrelatively small. You can see like-- you can even say this is\nnegligible because at least you",
    "start": "3586390",
    "end": "3592941"
  },
  {
    "text": "know, if you do\ntake a random point, you're going to take something\nbetween minus 0.05 to 0.05",
    "start": "3592941",
    "end": "3598180"
  },
  {
    "text": "maybe. And the bias is only\na very small number. Anyway, you are-- your\nstep-- your fluctuation",
    "start": "3598180",
    "end": "3603519"
  },
  {
    "text": "is bigger than the bias, also. So that's why in\nthe classical kind",
    "start": "3603520",
    "end": "3609010"
  },
  {
    "text": "of like in the classical kind\nof like optimization settings",
    "start": "3609010",
    "end": "3617320"
  },
  {
    "text": "people didn't really pay\ntoo much attention to this. I think they are--",
    "start": "3617320",
    "end": "3622329"
  },
  {
    "text": "some papers I guess--  so I guess there is\nthis paper by Bach 17",
    "start": "3622330",
    "end": "3633250"
  },
  {
    "text": "bridging the gap I guess\nbetween constant step size SGD.",
    "start": "3633250",
    "end": "3645990"
  },
  {
    "text": " Any more questions?",
    "start": "3645990",
    "end": "3651770"
  },
  {
    "text": "So this paper characterized\nthis effect for convex case.",
    "start": "3651770",
    "end": "3656840"
  },
  {
    "text": "And you can see from\nthe title of this paper, it's talking about\nconstant step size.",
    "start": "3656840",
    "end": "3662480"
  },
  {
    "text": "And why you have to talk\nabout constant step size just because this will go-- if you decay those steps\nsize, then this bias effect",
    "start": "3662480",
    "end": "3670069"
  },
  {
    "text": "will be even smaller. And it will be negligible, just\ncompletely gone eventually.",
    "start": "3670070",
    "end": "3675090"
  },
  {
    "text": "So that's why you have to make\nthis even somewhat useful, somewhat can make\na difference you",
    "start": "3675090",
    "end": "3681380"
  },
  {
    "text": "have to make the stuff\nthat's not going to 0. That's why in the combat\nphase, people typically don't care about this that much.",
    "start": "3681380",
    "end": "3687079"
  },
  {
    "text": "In some other cases, you\ncare about this a little bit. This figure is from\none of my recent papers",
    "start": "3687080",
    "end": "3693349"
  },
  {
    "text": "with some students at Stanford. Hi, guys. So and here the reason\nwhy we talked about this",
    "start": "3693350",
    "end": "3699650"
  },
  {
    "text": "is because you have\nmultiple machines. And for some other reasons,\nyou have to care about it.",
    "start": "3699650",
    "end": "3705390"
  },
  {
    "text": "But typically, you wouldn't\nreally care that much about it. It's just because\nthe bias is small.",
    "start": "3705390",
    "end": "3711473"
  },
  {
    "text": "OK. So now let's go back to-- now let's move on to--",
    "start": "3711473",
    "end": "3716545"
  },
  {
    "text": " OK.",
    "start": "3716545",
    "end": "3721600"
  },
  {
    "text": "Finally, we are moving to the\nplace of regularization impact. ",
    "start": "3721600",
    "end": "3727090"
  },
  {
    "text": "So the more complex case. ",
    "start": "3727090",
    "end": "3734890"
  },
  {
    "text": "I'm writing too fast. Too cursory, I guess. ",
    "start": "3734890",
    "end": "3744214"
  },
  {
    "text": "With stronger implicit reg. ",
    "start": "3744215",
    "end": "3755200"
  },
  {
    "text": "And these are cases where both\nH and sigma are not full rank.",
    "start": "3755200",
    "end": "3766599"
  },
  {
    "text": " So your Hessian and your\nnoise are both somewhat not",
    "start": "3766600",
    "end": "3775280"
  },
  {
    "text": "four-dimensional. And this is not something\nto be super surprised. This is called part that comes\nfrom overparameterization.",
    "start": "3775280",
    "end": "3787860"
  },
  {
    "text": "Especially, I think, it's easier\nto think about the Hessian. If you have a manifold\nof global minimum,",
    "start": "3787860",
    "end": "3795960"
  },
  {
    "text": "then along that-- the\ndirection of the manifold your Hessian will be 0.",
    "start": "3795960",
    "end": "3801560"
  },
  {
    "text": "So I thought you have a lot\nof different global minimum. Then your Hessian will be flat. It will be 0 in\ncertain directions.",
    "start": "3801560",
    "end": "3809020"
  },
  {
    "text": " And let's suppose--\nso for simplicity, I",
    "start": "3809020",
    "end": "3815760"
  },
  {
    "text": "may not discuss when\nthis can happen exactly because you need some\ncalculations so and so forth.",
    "start": "3815760",
    "end": "3821610"
  },
  {
    "text": "But suppose when I say H and\nsigma are both in a subspace K",
    "start": "3821610",
    "end": "3831540"
  },
  {
    "text": "and the subspace K is\nlow-dimensional or is not full-dimensional and if\nthe loss is quadratic--",
    "start": "3831540",
    "end": "3840900"
  },
  {
    "text": " for the moment,\nlet's still think about the loss as quadratic.",
    "start": "3840900",
    "end": "3846700"
  },
  {
    "text": "I guess we have\nto conclude this. We said that the iterate will\nhave 0, something like this.",
    "start": "3846700",
    "end": "3857789"
  },
  {
    "text": "Recall that this\nis our calculation, sigma squared di U transpose.",
    "start": "3857790",
    "end": "3865109"
  },
  {
    "text": "And so kind of the picture, I\nthink, is that there is no--",
    "start": "3865110",
    "end": "3879610"
  },
  {
    "text": "so basically, you have no\nnoise and no contraction,",
    "start": "3879610",
    "end": "3886300"
  },
  {
    "text": "nothing in the\nperpendicular space of k.",
    "start": "3886300",
    "end": "3892570"
  },
  {
    "text": "So in some sense, I think\nthe function look like this. So suppose you have\nsome direction of K.",
    "start": "3892570",
    "end": "3903070"
  },
  {
    "text": "This is the direction\nof K, and this is the direction of K perp.",
    "start": "3903070",
    "end": "3909100"
  },
  {
    "text": "And suppose your function is\nquadratic in dimension of K, something like this.",
    "start": "3909100",
    "end": "3914800"
  },
  {
    "text": "I'm not sure whether you can-- I think my drawing is too bad.",
    "start": "3914800",
    "end": "3920540"
  },
  {
    "text": "So imagine a valley. This is a-- I'm drawing\na valley like this.",
    "start": "3920540",
    "end": "3926630"
  },
  {
    "text": "But this valley is\ncompletely kind of like oblivious to\ndimension of k per. ",
    "start": "3926630",
    "end": "3934085"
  },
  {
    "text": "So this thing is the\nmiddle of the valley. This Is the middle\nof the valley.",
    "start": "3934085",
    "end": "3940930"
  },
  {
    "text": "So basically then\nwhat happens is that if you start\nsomewhere here,",
    "start": "3940930",
    "end": "3946090"
  },
  {
    "text": "everything happens in\nthe direction of K, and nothing happens in\nthe direction of k per. So you're basically bouncing\naround the direction of k.",
    "start": "3946090",
    "end": "3952750"
  },
  {
    "text": "So basically, you\nmaybe go here, here, and go about to do some bouncing\nlike something like this.",
    "start": "3952750",
    "end": "3958300"
  },
  {
    "text": "But you never move in\nthe direction of k per. So in k per, it's kind of\nlike you just know nothing.",
    "start": "3958300",
    "end": "3965470"
  },
  {
    "text": "You know nothing, or\nyou don't move at all. So let's do not kind of like--",
    "start": "3965470",
    "end": "3970525"
  },
  {
    "text": "that's a little\nbit implicit bias in place of requisition because\nthe implicit requisition comes",
    "start": "3970525",
    "end": "3975710"
  },
  {
    "text": "from what? Comes from the transition. If you start with this\npoint, then you're going to stay in this part.",
    "start": "3975710",
    "end": "3982623"
  },
  {
    "text": "But if you start\nhere, then you're going to bounce\naround here, right? And this is exactly what happens\nwhen you have overparameterized",
    "start": "3982623",
    "end": "3990160"
  },
  {
    "text": "linear model, because when\nyou have overparameterized linear model, you never\nleave the subspace.",
    "start": "3990160",
    "end": "3995565"
  },
  {
    "text": "It may never leave\nsome subspace. And in other subspace,\nyou'll never move. So this is not the most\nimportant thing about noise",
    "start": "3995565",
    "end": "4002538"
  },
  {
    "text": "because noise doesn't\nreally do much. It's really just that you\ncannot leave a certain subspace.",
    "start": "4002538",
    "end": "4008370"
  },
  {
    "text": "However, when your\nloss is quadratic, when your loss is not quadratic,\nthen the third-order term",
    "start": "4008370",
    "end": "4016150"
  },
  {
    "text": "is going to matter. So this is the main\nthing that I want",
    "start": "4016150",
    "end": "4021510"
  },
  {
    "text": "to kind of like commit today,\nbut, unfortunately, just because this is complicated.",
    "start": "4021510",
    "end": "4026580"
  },
  {
    "text": "So I probably wouldn't be able\nto do everything rigorously. So I just really can't\ndo everything rigorously.",
    "start": "4026580",
    "end": "4033990"
  },
  {
    "text": "So what happens is that if\nthe loss is not quadratic, then-- recall that\nwhat happens is",
    "start": "4033990",
    "end": "4039540"
  },
  {
    "text": "that you have xt plus 1 is equal\nto 1 minus eta H xt minus eta",
    "start": "4039540",
    "end": "4046060"
  },
  {
    "text": "ksi t minus eta T xt xt\nplus high-order term.",
    "start": "4046060",
    "end": "4054500"
  },
  {
    "text": " And this is happening.",
    "start": "4054500",
    "end": "4060220"
  },
  {
    "text": " So this is working\nin K because I",
    "start": "4060220",
    "end": "4070680"
  },
  {
    "text": "assume that H is working in K,\nand the noise is always in K in a subspace. So this left part\nis we're always",
    "start": "4070680",
    "end": "4078090"
  },
  {
    "text": "working in K. You are\nbouncing around in K. And this is working in K perp.",
    "start": "4078090",
    "end": "4086186"
  },
  {
    "text": "And that makes them kind\nof complete separate, so there's nothing you can\ncontrol the third-order term.",
    "start": "4086186",
    "end": "4091840"
  },
  {
    "text": "The third-order\nterm can build up for a long and long--\nvery long time. ",
    "start": "4091840",
    "end": "4098439"
  },
  {
    "text": "So maybe this is the one. Let me see. ",
    "start": "4098439",
    "end": "4106439"
  },
  {
    "text": "So basically, let's see.  I probably will go to this\nfigure multiple times.",
    "start": "4106439",
    "end": "4113605"
  },
  {
    "text": " Right. So this is what's\nhappening here.",
    "start": "4113605",
    "end": "4118790"
  },
  {
    "text": "So I don't think I can--",
    "start": "4118790",
    "end": "4124165"
  },
  {
    "text": "I don't think I can\ndraw anything here. But maybe first watch it.",
    "start": "4124165",
    "end": "4131620"
  },
  {
    "text": "And then I'm going to go to a\nstatic figure so that you can--",
    "start": "4131620",
    "end": "4137242"
  },
  {
    "text": "I can annotate. So this is a stochastic\ngradient descent in this valley.",
    "start": "4137242",
    "end": "4142290"
  },
  {
    "text": " And you can see that it's\nmoving in this valley.",
    "start": "4142290",
    "end": "4147528"
  },
  {
    "text": "So now let's look at\nthe static figure. I have one somewhere.",
    "start": "4147529",
    "end": "4154549"
  },
  {
    "text": "So in our mathematical\nlanguage-- so this direction--",
    "start": "4154550",
    "end": "4159979"
  },
  {
    "text": " let's see a different color.",
    "start": "4159979",
    "end": "4168068"
  },
  {
    "text": "So this is the\ndirection of the K perp, and this is the direction of K.",
    "start": "4168069",
    "end": "4176339"
  },
  {
    "text": "So this direction is K. OK?",
    "start": "4176340",
    "end": "4181359"
  },
  {
    "text": "But here this is not\na quadratic because-- at least this is-- it is not a quadratic because\nyour-- at least the K perp",
    "start": "4181359",
    "end": "4188229"
  },
  {
    "text": "direction doesn't\nmatter to some extent. Because the K perp--\nyou can see that if you go from here to\nhere, you're going",
    "start": "4188229",
    "end": "4194200"
  },
  {
    "text": "to go to flatter and\nflatter region, right? So what happens is\nthat most of your work",
    "start": "4194200",
    "end": "4201610"
  },
  {
    "text": "is in the K direction. You are just bouncing\naround in the K direction. But there is some certain\nterm that drives you",
    "start": "4201610",
    "end": "4207730"
  },
  {
    "text": "in the K per direction. And that can build up\neventually for a long time. Recall that you start from here. You do a lot of bouncing around.",
    "start": "4207730",
    "end": "4214150"
  },
  {
    "text": "But eventually, after you\nbounce for a so long time, you move in the K per direction. And this is because\nthe third-order term",
    "start": "4214150",
    "end": "4221780"
  },
  {
    "text": "is accumulating for\na long time until you go to the flatter region.",
    "start": "4221780",
    "end": "4226870"
  },
  {
    "text": " So the min term is\ndoing the bouncing,",
    "start": "4226870",
    "end": "4233469"
  },
  {
    "text": "and the lower term\nis accumulating in the direction of the valley.",
    "start": "4233470",
    "end": "4239163"
  },
  {
    "text": " Any questions so far?",
    "start": "4239163",
    "end": "4244349"
  },
  {
    "text": "I'll go back to\nthis bigger problem once again just once I do\na little bit more math.",
    "start": "4244350",
    "end": "4249420"
  },
  {
    "text": "If we sort of know\nthat this is happening, do we want to do this\nfirst [INAUDIBLE] direction",
    "start": "4249420",
    "end": "4255520"
  },
  {
    "text": "and then-- is that a feasible thing\nyou can do with it? Yeah, that's a good question.",
    "start": "4255520",
    "end": "4261300"
  },
  {
    "text": "So if you know this\nis what's happening. Why not just do something more\nexplicit to make it faster, right? ",
    "start": "4261300",
    "end": "4269950"
  },
  {
    "text": "I think there are several-- let me see. So there are several\nthings that--",
    "start": "4269950",
    "end": "4276060"
  },
  {
    "text": "it's a good question, but this\nis not something super new. People have thought about it,\nand I have thought about it.",
    "start": "4276060",
    "end": "4281860"
  },
  {
    "text": "I think there are\nmultiple constraints we have to kind of respect.",
    "start": "4281860",
    "end": "4289390"
  },
  {
    "text": "I still think this is a\nfeasible thing, direction to go, but it's not easy. And I don't think there's an\nexisting paper that can really",
    "start": "4289390",
    "end": "4297070"
  },
  {
    "text": "achieve this very well. So one of the thing is that--\nso how do you go to the valley?",
    "start": "4297070",
    "end": "4303445"
  },
  {
    "text": "So what you're going to do? You want to go to the valley. You want to compute the\ndirection of K perp, and you go there, right?",
    "start": "4303445",
    "end": "4310380"
  },
  {
    "text": "So how do you go to the valley? I think that's not too hard\nbecause you have to use but not trivial.",
    "start": "4310380",
    "end": "4315849"
  },
  {
    "text": "Because to go to\nthe valley, you have to either decay on your rate\nor make your batch size bigger",
    "start": "4315850",
    "end": "4320890"
  },
  {
    "text": "so that you have smaller norms. But that requires more\ncompute because you",
    "start": "4320890",
    "end": "4327160"
  },
  {
    "text": "want to be more accurate. Sometimes you want to be more\naccurate in a K direction, so it requires more compute.",
    "start": "4327160",
    "end": "4333885"
  },
  {
    "text": "So that's one\nsmall thing, right? So whether you really\ncan afford to compute to really go to the\nvalley in the first place.",
    "start": "4333885",
    "end": "4340160"
  },
  {
    "text": "I think you can probably. In most cases, you can. But there's not like a--",
    "start": "4340160",
    "end": "4345340"
  },
  {
    "text": "for free, so you do have\nto consider the cost. And then you go to the\nvalley, and then you",
    "start": "4345340",
    "end": "4350710"
  },
  {
    "text": "do the-- you do this, right? You move in a K perp direction. But the problem becomes\nthat the real picture is not",
    "start": "4350710",
    "end": "4358750"
  },
  {
    "text": "just one single-- this is only a local view.",
    "start": "4358750",
    "end": "4364090"
  },
  {
    "text": "So once you go to here maybe-- if locally, it sounds great. I'm going to a better place.",
    "start": "4364090",
    "end": "4369687"
  },
  {
    "text": "But maybe there is--\nactually, this function has a lot of other parts. So actually, I have to\ntravel really far, far,",
    "start": "4369687",
    "end": "4374820"
  },
  {
    "text": "far away, somewhere else. So then you have to do this\nagain, this local view again, and then try to do it and\nso on and so forth, right?",
    "start": "4374820",
    "end": "4381570"
  },
  {
    "text": "So then it becomes a-- then you have to also\nfind a new valley and then probably find the\ndirection of the K perp.",
    "start": "4381570",
    "end": "4388500"
  },
  {
    "text": "And also finding the\ndirection of K perp is also not mentioned because\nit requires completing a third-order derivative.",
    "start": "4388500",
    "end": "4395250"
  },
  {
    "text": "Continuous third-order\nderivative on one example is still OK.",
    "start": "4395250",
    "end": "4400880"
  },
  {
    "text": "It cost you-- computing\nhigh-order derivative on one example takes\nyou a constant factor",
    "start": "4400880",
    "end": "4406820"
  },
  {
    "text": "more than computing the\nfirst-order derivative. This is a very interesting\nthing about deep learning. So computing any derivative\ngive you almost--",
    "start": "4406820",
    "end": "4416389"
  },
  {
    "text": "requires almost the same time as\ncomputing the first derivative as long as your\noutput is a vector.",
    "start": "4416390",
    "end": "4421953"
  },
  {
    "text": "But I do-- you do have to pay\na constant factor, something like two or three\ntimes more compute.",
    "start": "4421953",
    "end": "4427550"
  },
  {
    "text": "And also, you have\nto do this for-- and this T, this K per\ndirection, to get it exactly,",
    "start": "4427550",
    "end": "4437909"
  },
  {
    "text": "you also have to do a full\nbatch, full [INAUDIBLE]",
    "start": "4437910",
    "end": "4443670"
  },
  {
    "text": "so that the third-order term\nis the third-order derivative of the full function,\nof the population,",
    "start": "4443670",
    "end": "4451380"
  },
  {
    "text": "of the population function, of\nthe full empirical function. So if you use this\nminimax thing,",
    "start": "4451380",
    "end": "4457300"
  },
  {
    "text": "then maybe you wouldn't\nget the K perp direction very accurately. So there's a bunch of decisions\nwhich makes it complicated.",
    "start": "4457300",
    "end": "4464580"
  },
  {
    "text": "We don't even know exactly\nwhich one is the bottleneck, so it's a little bit tricky.",
    "start": "4464580",
    "end": "4469920"
  },
  {
    "text": "So whoo, but that's\na great question. Yeah. So we tried very\nhard to somehow do this for quite a while already.",
    "start": "4469920",
    "end": "4476900"
  },
  {
    "text": "Yeah. So OK, all right. So now let's see. So I think I would do\na little more math just",
    "start": "4476900",
    "end": "4491940"
  },
  {
    "text": "to kind of give you a\nsmall feeling about how we perceive to analyze this.",
    "start": "4491940",
    "end": "4498120"
  },
  {
    "text": "And the way to analyze this\nis that you somehow view this, as I said, two things.",
    "start": "4498120",
    "end": "4503640"
  },
  {
    "text": "So you first define a\ncompetitive process, which is easier to define,\nUt plus 1 to be 1",
    "start": "4503640",
    "end": "4511739"
  },
  {
    "text": "minus eta H Ut minus eta ksi t. And this is where it's\nunderstood because basically",
    "start": "4511740",
    "end": "4520980"
  },
  {
    "text": "you are doing optimization on\nthe quadratic approximation.",
    "start": "4520980",
    "end": "4526370"
  },
  {
    "text": "And we have done this already. And then you characterize\nthe difference between them.",
    "start": "4526370",
    "end": "4531720"
  },
  {
    "text": "So xt minus Ut. We define this to be rt.",
    "start": "4531720",
    "end": "4536869"
  },
  {
    "text": "So basically, the main\nquestion is what rt is doing. And we get to take a--",
    "start": "4536870",
    "end": "4544400"
  },
  {
    "text": "we can compute the\nrecursion of rt. ",
    "start": "4544400",
    "end": "4550050"
  },
  {
    "text": "Right. This is equals to you plug in\na definition of xt plus and ut plus 1 you get 1 minus eta H\nxt minus Ut minus eta T xt xt.",
    "start": "4550050",
    "end": "4562185"
  },
  {
    "text": " That's high-order term.",
    "start": "4562185",
    "end": "4568180"
  },
  {
    "text": "And then still-- and then it's\n1 minus eta H rt minus eta T xt",
    "start": "4568180",
    "end": "4575645"
  },
  {
    "text": "xt.  And the interesting\ntheory is that you still",
    "start": "4575645",
    "end": "4582390"
  },
  {
    "text": "have the contraction. And this is the bias or\nthe regularization effect,",
    "start": "4582390",
    "end": "4588560"
  },
  {
    "text": "but there's no noise anymore. There's no stochasticity,\nno stochasticity.",
    "start": "4588560",
    "end": "4595280"
  },
  {
    "text": "There are still a little\nbit of stochasticity in xt, but at least you don't have\nthe ksi t term that you",
    "start": "4595280",
    "end": "4600320"
  },
  {
    "text": "have added intentionally. Just because you\nare taking a diff",
    "start": "4600320",
    "end": "4605516"
  },
  {
    "text": "with the stochastic trajectory. And you can actually\nmove the xt as well",
    "start": "4605516",
    "end": "4610880"
  },
  {
    "text": "because you can\nbasically claim that this is close to the version\nwhere you plug in.",
    "start": "4610880",
    "end": "4618701"
  },
  {
    "text": "You're not plugging xt. You're plugging Ut. ",
    "start": "4618701",
    "end": "4624010"
  },
  {
    "text": "So this is just because xt\nand Ut are somewhat similar.",
    "start": "4624010",
    "end": "4632764"
  },
  {
    "text": "Of course, you\nwant to understand the exact difference. But for this level,\nespecially because you are operating on eta here--",
    "start": "4632765",
    "end": "4639180"
  },
  {
    "text": "so the further their\ndifferences become so you have a higher\norder term you can jump. ",
    "start": "4639180",
    "end": "4647250"
  },
  {
    "text": "So for now, what happens is\nthat if you look at the diff,",
    "start": "4647250",
    "end": "4655380"
  },
  {
    "text": "if you look at the inner\nsubspace, the subspace of K,",
    "start": "4655380",
    "end": "4662610"
  },
  {
    "text": "which is the span of H,\nthis is still contraction",
    "start": "4662610",
    "end": "4669350"
  },
  {
    "text": "because you have some\nadditional biases. But the biases will be corrected\nby the contraction eventually.",
    "start": "4669350",
    "end": "4678100"
  },
  {
    "text": "However, for the\ninner K perp subspace,",
    "start": "4678100",
    "end": "4687280"
  },
  {
    "text": "the contraction is gone. You project everything\nto the K per subspace.",
    "start": "4687280",
    "end": "4693520"
  },
  {
    "text": "Then you got-- all right.",
    "start": "4693520",
    "end": "4704950"
  },
  {
    "text": "So the H doesn't have any effect\nanymore because H has nothing to do with the K per direction.",
    "start": "4704950",
    "end": "4710980"
  },
  {
    "text": "It's just the projected outcome. So now, the thing\nis really simple. So in the K per\nsubspace, you are just",
    "start": "4710980",
    "end": "4717580"
  },
  {
    "text": "basically taking\nthe previous rt, projecting the current\npair plus something new.",
    "start": "4717580",
    "end": "4723890"
  },
  {
    "text": "So basically, you are\njust the [INAUDIBLE].. You don't have any\ncontraction even. So if you do this\nrecursively, you",
    "start": "4723890",
    "end": "4730150"
  },
  {
    "text": "are going to get the K per\nof r0 minus the sum of--",
    "start": "4730150",
    "end": "4737860"
  },
  {
    "start": "4737860",
    "end": "4750159"
  },
  {
    "text": "but now it becomes-- the question becomes, how\ndo you understand the sum of the third-order term. And by the way,\nI've never told you",
    "start": "4750160",
    "end": "4756268"
  },
  {
    "text": "where the third-order\nterm is going. I only claim that there\nis a third-order term. I didn't really say\nwhere it's going.",
    "start": "4756268",
    "end": "4762079"
  },
  {
    "text": "So now the question is,\nwhat is third-order terms are going in average, right?",
    "start": "4762080",
    "end": "4768080"
  },
  {
    "text": "So in the long run, over time. So we can kind of ignore this.",
    "start": "4768080",
    "end": "4773330"
  },
  {
    "text": "This is just a restriction\nto the subspace. So if you look at sum of those--",
    "start": "4773330",
    "end": "4778960"
  },
  {
    "text": "where the third-order\nterm is going. ",
    "start": "4778960",
    "end": "4784280"
  },
  {
    "text": "So from this, you can-- so first of all, let's assume\nmaybe this is a heuristic.",
    "start": "4784280",
    "end": "4792872"
  },
  {
    "text": "Let's say this is a Ut. ",
    "start": "4792872",
    "end": "4797900"
  },
  {
    "text": "Let's say s is\nsomething like UK.",
    "start": "4797900",
    "end": "4806219"
  },
  {
    "text": "UK transpose is the\ncovariance of UK.  This is as K goes to\ninfinity and also assume",
    "start": "4806220",
    "end": "4818674"
  },
  {
    "text": "this UK mixes as a Markov chain. I'm not sure whether\nyou are familiar with this maximum chain\nmixing, but you just",
    "start": "4818674",
    "end": "4825463"
  },
  {
    "text": "assume that UK is kind\nof like doing the-- UK is doing the bouncing around. You assume that it's\nreally just doing that.",
    "start": "4825463",
    "end": "4832540"
  },
  {
    "text": "It's kind of like a Gaussian. And S is the covariance\nof the Gaussian. And then this one, you\ncan rewrite this as T of--",
    "start": "4832540",
    "end": "4845360"
  },
  {
    "text": " in some sense, this is like--",
    "start": "4845360",
    "end": "4850520"
  },
  {
    "text": "it goes to little t times-- roughly equals\nlittle t times the T",
    "start": "4850520",
    "end": "4856230"
  },
  {
    "text": "with the expectation of u and\nUK transpose-- or maybe with s.",
    "start": "4856230",
    "end": "4864760"
  },
  {
    "text": "So I guess maybe-- so what I'm doing\nhere is that, suppose",
    "start": "4864760",
    "end": "4870620"
  },
  {
    "text": "you have some variable U that\nis drawn from s, from Gaussian with covariance S, then\nthe expectation of T u u",
    "start": "4870620",
    "end": "4883809"
  },
  {
    "text": "is equal to T of-- this is expectation--",
    "start": "4883810",
    "end": "4890690"
  },
  {
    "text": "I guess let's look\nat i-th coordinate. And this is sum of the Tijk,\nUj, Uk and you look at i-th",
    "start": "4890690",
    "end": "4899220"
  },
  {
    "text": "coordinate-- the i-th for this.",
    "start": "4899220",
    "end": "4904770"
  },
  {
    "text": "And then you switch the\nsum with the expectation. You have j k Tijk\nexpectation Uj Uk.",
    "start": "4904770",
    "end": "4918810"
  },
  {
    "text": "And this is sum over Jk Tijk\nexpectation u, u transpose.",
    "start": "4918810",
    "end": "4926250"
  },
  {
    "text": "You take the jk coordinate. And this is-- If you know this by T of\nexpectation u, u transpose.",
    "start": "4926250",
    "end": "4938460"
  },
  {
    "text": "So you can also apply\nthe tensor on a matrix, and the definition\nis really just this.",
    "start": "4938460",
    "end": "4943920"
  },
  {
    "text": "So the definition\nof the matrix is that you have some of Tijk Sjk.",
    "start": "4943920",
    "end": "4954935"
  },
  {
    "text": "Anyway, this just might\nbe a little bit too much for this course.",
    "start": "4954936",
    "end": "4960000"
  },
  {
    "text": "But anyway, you can basically\nidentify what you have is that you use--",
    "start": "4960000",
    "end": "4965633"
  },
  {
    "text": "I guess there's an eta here. My bad. So this t comes from you\nhave multiple times where you have t steps.",
    "start": "4965633",
    "end": "4971660"
  },
  {
    "text": "That's where you got t. And eta is what you\ngot from this eta. And this is something\nlike you apply the tensor",
    "start": "4971660",
    "end": "4978680"
  },
  {
    "text": "to the average covariance,\nthe mixed covariance of t.",
    "start": "4978680",
    "end": "4984360"
  },
  {
    "text": "So basically, the question\nbecomes, what is x? If you know x, then you know\nwhich direction you're going.",
    "start": "4984360",
    "end": "4989659"
  },
  {
    "text": "And you know how\nfar you are going. You are going by t\ntimes this direction because you take t steps.",
    "start": "4989660",
    "end": "4997895"
  },
  {
    "text": "So the next sort of-- so the final question\ni what this TS is. And this is very informal\nand not even exactly correct.",
    "start": "4997895",
    "end": "5006250"
  },
  {
    "text": "And to fix it, you need\nsomething a little bit more",
    "start": "5006250",
    "end": "5011320"
  },
  {
    "text": "[INAUDIBLE]. So this biased direction\nT of S minus T of S is the biased direction.",
    "start": "5011320",
    "end": "5017668"
  },
  {
    "text": " So this batch direction\nis equal to minus--",
    "start": "5017668",
    "end": "5024730"
  },
  {
    "text": "I guess remember what's the t t\nis the third-order derivative. And S is the iterate of the--",
    "start": "5024730",
    "end": "5031660"
  },
  {
    "text": "no, S is just some\nmatrix for the moment. And you can rewrite this. So if you think\nabout what is this,",
    "start": "5031660",
    "end": "5039156"
  },
  {
    "text": "this is the gradient of the\ninner product of the Hessian",
    "start": "5039156",
    "end": "5044719"
  },
  {
    "text": "and S. So this is an equation.",
    "start": "5044720",
    "end": "5050110"
  },
  {
    "text": "And in some sense, you\ncan argue that this is a--",
    "start": "5050110",
    "end": "5055170"
  },
  {
    "text": "heuristic argument. And actually, it's\nnot even correct, not even a 100%\ncorrect argument.",
    "start": "5055170",
    "end": "5061600"
  },
  {
    "text": "So the T(S) is trying to make\nnabla squared g(x) times S,",
    "start": "5061600",
    "end": "5072030"
  },
  {
    "text": "smaller, because you are moving\nthe gradient of that function, right? So let's define this to be R(x).",
    "start": "5072030",
    "end": "5078929"
  },
  {
    "text": "Right. So you are moving in that\nnegative R(x), negative nabla R(x) direction.",
    "start": "5078930",
    "end": "5084767"
  },
  {
    "text": "That's why you can argue\nthat you are trying to make that function smaller.",
    "start": "5084767",
    "end": "5090110"
  },
  {
    "text": "So the additional bias\nis trying to make-- this minus T(S) term is trying\nto make the R(x) smaller",
    "start": "5090110",
    "end": "5099020"
  },
  {
    "text": "by moving in a gradient\nof this R(x) direction. And eventually, I\nthink if you work out",
    "start": "5099020",
    "end": "5106880"
  },
  {
    "text": "all of this kind of\nlike subtle details with a lot of other like stance,\nand fixes, and assumptions.",
    "start": "5106880",
    "end": "5119680"
  },
  {
    "text": " And I think I've not time\nto go through all of this,",
    "start": "5119680",
    "end": "5125400"
  },
  {
    "text": "as we already are running late. But you can somewhat\nprove in certain cases--",
    "start": "5125400",
    "end": "5132940"
  },
  {
    "text": "let me just write down\nwhat formula you can prove. So you can prove\nsomething like SGD",
    "start": "5132940",
    "end": "5145950"
  },
  {
    "text": "with the so-called label noise. ",
    "start": "5145950",
    "end": "5151840"
  },
  {
    "text": "I didn't tell you\nwhat label noise mean. It doesn't matter. It's one kind of noise, and\nit's not exactly the min.",
    "start": "5151840",
    "end": "5158240"
  },
  {
    "text": "it's just some additional noise. And convert this to a stationary\npoint of the regularized loss.",
    "start": "5158240",
    "end": "5177815"
  },
  {
    "start": "5177815",
    "end": "5183440"
  },
  {
    "text": "I plus lambda R-- l hat plus lambda R, where\nR theta is equal to--",
    "start": "5183440",
    "end": "5192190"
  },
  {
    "text": "roughly equal to trace of\nthe Hessian of the loss.",
    "start": "5192190",
    "end": "5198550"
  },
  {
    "text": " Yeah, so I guess there's no need\nto understand any details here.",
    "start": "5198550",
    "end": "5207090"
  },
  {
    "text": "There are some other subtleties. There are other assumptions,\nso and so forth. I just want to give you a\ntaste on what kind of theorems",
    "start": "5207090",
    "end": "5212463"
  },
  {
    "text": "you may hope to prove. So basically, you are saying\nthat, if you run SGD, OK,",
    "start": "5212463",
    "end": "5218580"
  },
  {
    "text": "this is on the\noriginal loss, L hat. So if you wear a certain kind of\nSGD on the unregularized loss.",
    "start": "5218580",
    "end": "5225810"
  },
  {
    "text": "It converges to the stationary\npoint of a regularized loss. So that's why you get\nthis regularizer for free.",
    "start": "5225810",
    "end": "5233099"
  },
  {
    "text": "And what regularizer it is-- here, the regularizer is\nthe trace of the Hessian is something about the flatness\nof the loss L hat, right?",
    "start": "5233100",
    "end": "5242880"
  },
  {
    "text": "The Hessian is the curvature. The trace of the Hessian\nis about the flatness at that point. So you are implicitly\nencouraging the flatness",
    "start": "5242880",
    "end": "5253844"
  },
  {
    "text": "of the loss function.  But this has a lot of\nthings hidden here.",
    "start": "5253844",
    "end": "5261670"
  },
  {
    "text": "And actually, I\nthink I'm missing a few kind of\nimportant question, important assumptions. I'm not writing down some\nof the important assumptions",
    "start": "5261670",
    "end": "5269340"
  },
  {
    "text": "just because they are not-- it takes too much\ntime to write It down. But this is kind\nof like something",
    "start": "5269340",
    "end": "5275670"
  },
  {
    "text": "we may hope to prove\nin some other cases. OK, any questions?",
    "start": "5275670",
    "end": "5281949"
  },
  {
    "text": "[INAUDIBLE] ",
    "start": "5281949",
    "end": "5300980"
  },
  {
    "text": "That's a great point. So the question-- just\nto rephrase the question. The question is that whether\nthe even high-order derivative,",
    "start": "5300980",
    "end": "5309560"
  },
  {
    "text": "the fourth-order gradient\nwould increase the bounds.",
    "start": "5309560",
    "end": "5315200"
  },
  {
    "text": "I think on the conceptual\nlevel, if your third-order thing",
    "start": "5315200",
    "end": "5320390"
  },
  {
    "text": "is not 0, then I think the\nfourth-order one wouldn't matter that much. And if the third-order\nterm is 0, I think, indeed,",
    "start": "5320390",
    "end": "5328970"
  },
  {
    "text": "there should be a fourth-order-- the fourth-order term\nwould have an effect.",
    "start": "5328970",
    "end": "5335390"
  },
  {
    "text": "But so far we're not\nthinking about that. We are assuming the\nthird-order term is doing something non-trivial.",
    "start": "5335390",
    "end": "5342170"
  },
  {
    "text": "so that the fourth-order\nterm will be dominated by the third-order term. I see. It seems like [INAUDIBLE].",
    "start": "5342170",
    "end": "5349424"
  },
  {
    "start": "5349424",
    "end": "5358369"
  },
  {
    "text": "In the last theorem, the\nstationary point is stochastic.",
    "start": "5358370",
    "end": "5364168"
  },
  {
    "text": " So regular [INAUDIBLE] Hessian\ninstead of the [INAUDIBLE]..",
    "start": "5364168",
    "end": "5375440"
  },
  {
    "text": "Oh, I see. Yeah. So the question is,\nwhy the regularization is the trace of Hessian. This is because when a regulator\nis over the second-order term,",
    "start": "5375440",
    "end": "5384730"
  },
  {
    "text": "the second derivative, then the\ndirection you want to move to is the gradient of\nthe regularizer.",
    "start": "5384730",
    "end": "5391290"
  },
  {
    "text": " So when you have a regularizer,\nwhat do they really mean? It means that you should\nmove to the direction",
    "start": "5391290",
    "end": "5398067"
  },
  {
    "text": "of the grid of the regularizer. That's how they match up.",
    "start": "5398067",
    "end": "5403338"
  },
  {
    "text": "So actually, the\ndirection you really move to is the\nthird-order derivative, depends on the\nthird-order derivative of the loss function.",
    "start": "5403338",
    "end": "5409453"
  },
  {
    "text": "And then the second--\nso the Hessian becomes the derivation and\nthen the corresponding term",
    "start": "5409454",
    "end": "5419790"
  },
  {
    "text": "[INAUDIBLE]. ",
    "start": "5419790",
    "end": "5431820"
  },
  {
    "text": "So I guess there are two views. One view is that you look at\nit on the regularizer level.",
    "start": "5431820",
    "end": "5439320"
  },
  {
    "text": "Then currently it's\nthe second-order term, the second-order\nderivative of the loss.",
    "start": "5439320",
    "end": "5445715"
  },
  {
    "text": "And another view\nis that you look at the actual,\nthe iterate space. The current is the\nthird-order derivative,",
    "start": "5445715",
    "end": "5452639"
  },
  {
    "text": "it's about third-order\nderivative of the loss. And supposing that you're\nin the iterate space, the third-order\nderivative manages.",
    "start": "5452640",
    "end": "5459889"
  },
  {
    "text": "Then you have to talk about\nthe fourth-order derivative of the loss. And in that case,\nthe regularizer",
    "start": "5459889",
    "end": "5465250"
  },
  {
    "text": "probably will be above\na third-order derivative of the loss. It's because your regularizer\nis always one order up compared",
    "start": "5465250",
    "end": "5472720"
  },
  {
    "text": "to the direction you move to. This makes sense now?",
    "start": "5472720",
    "end": "5477782"
  },
  {
    "text": "[INAUDIBLE] the SGD [INAUDIBLE].",
    "start": "5477783",
    "end": "5482800"
  },
  {
    "text": " So what's special about that?",
    "start": "5482800",
    "end": "5489160"
  },
  {
    "text": "[INAUDIBLE] ",
    "start": "5489160",
    "end": "5495940"
  },
  {
    "text": "Yeah. So why a flat stationary\npoint is better? Right. So I think-- [INAUDIBLE]",
    "start": "5495940",
    "end": "5502659"
  },
  {
    "text": "Why do we spend or not? So I think I'm going to\ntalk about that immediately",
    "start": "5502660",
    "end": "5509200"
  },
  {
    "text": "next in the beginning\nof the next lecture. And the answer is\nthat we do believe-- is generally [INAUDIBLE].",
    "start": "5509200",
    "end": "5517420"
  },
  {
    "text": "It depends on some-- it kind\nof relates to the Lipschitzness of the models-- I'll discuss more next week--",
    "start": "5517420",
    "end": "5524290"
  },
  {
    "text": "on Wednesday. OK, bye.",
    "start": "5524290",
    "end": "5529980"
  },
  {
    "start": "5529980",
    "end": "5534000"
  }
]