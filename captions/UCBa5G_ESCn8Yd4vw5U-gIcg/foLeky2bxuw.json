[
  {
    "text": "Everyone, welcome\nback to CS 330. And today we'll be talking about\nA Graphical Model Perspective",
    "start": "5230",
    "end": "11663"
  },
  {
    "text": "on Multi-Task and Meta\nReinforcement Learning.",
    "start": "11663",
    "end": "13580"
  },
  {
    "text": "One second, does that work? All right, so the plan for\nthe lecture is as following.",
    "start": "17480",
    "end": "24830"
  },
  {
    "text": "First, we'll go over\nvariational inference. And I think Chelsea went\nover it a little bit already.",
    "start": "24830",
    "end": "29993"
  },
  {
    "text": "So this will be--\nwe'll go over it again to make sure that you understand\nthe intuition behind it.",
    "start": "29993",
    "end": "36190"
  },
  {
    "text": "Then we will present\na framework that allows us to frame\na control problem as an inference problem in\na specific probabilistic",
    "start": "36190",
    "end": "43420"
  },
  {
    "text": "graphical model. Then we'll apply variational\ninference to that model.",
    "start": "43420",
    "end": "49700"
  },
  {
    "text": "And then finally\nwe'll get to the point where we can derive a new\nmeta reinforcement learning algorithm using all\nof that knowledge",
    "start": "49700",
    "end": "55690"
  },
  {
    "text": "and applying variational\ninference to, slightly different variational inference\nto the graphical model.",
    "start": "55690",
    "end": "62070"
  },
  {
    "text": "All right, so a\nquick disclaimer. There will be quite a bit\nof math in this lecture",
    "start": "62070",
    "end": "68229"
  },
  {
    "text": "and quite a bit of derivations. I want to go through all\nthe derivations in detail. I'll skim through them and try\nto give you the intuitions,",
    "start": "68230",
    "end": "75549"
  },
  {
    "text": "although the derivations\nare there in the slides. So if you'd like to derive\nit yourself or just check",
    "start": "75550",
    "end": "81730"
  },
  {
    "text": "how it's derived, you\ncan take a look at that. I will try to make sure that\nthere are some anchor points.",
    "start": "81730",
    "end": "89210"
  },
  {
    "text": "So even if you get\nlost, you still can kind of get back to\nit and get the intuition",
    "start": "89210",
    "end": "94240"
  },
  {
    "text": "behind the ideas. And if there are\nany questions, I",
    "start": "94240",
    "end": "100540"
  },
  {
    "text": "think the best option is\nto raise your hand rather than using the chat. The chat can be a\nlittle distracting.",
    "start": "100540",
    "end": "105710"
  },
  {
    "text": "So I would encourage\nyou to do that. And I'll also ask every now\nand then after every section",
    "start": "105710",
    "end": "111490"
  },
  {
    "text": "if there are any questions. Another thing is that this\nis an active research topic.",
    "start": "111490",
    "end": "117920"
  },
  {
    "text": "There's a lot of development\ngoing on in that subfield.",
    "start": "117920",
    "end": "123250"
  },
  {
    "text": "So there might be more\nquestions than answers. But that's one of the reasons\nwhy this is quite exciting.",
    "start": "123250",
    "end": "129369"
  },
  {
    "text": "All right, so let's start with,\nwhy variational inference? Why should we care about this?",
    "start": "133190",
    "end": "139580"
  },
  {
    "text": "So variational inference\nis the main tool that helps us cope with\nlatent variable models.",
    "start": "139580",
    "end": "144818"
  },
  {
    "text": "So then the natural\nquestion is, what are latent variable models? And to introduce this, I will\ngo through a quick example.",
    "start": "144818",
    "end": "153930"
  },
  {
    "text": "So here we plotted some kind\nof data distribution, right?",
    "start": "153930",
    "end": "159329"
  },
  {
    "text": "So this is two-dimensional data. And here I color-coded\nit in a certain way.",
    "start": "159330",
    "end": "164930"
  },
  {
    "text": "This is just for\nyour convenience. But this is the\ndata we get to see. And looking at\nthis data, it's not",
    "start": "164930",
    "end": "173120"
  },
  {
    "text": "very conveniently distributed. It's not that you can\neasily fit a Gaussian to it and it will just work. So there may be some\nlatent structure",
    "start": "173120",
    "end": "180170"
  },
  {
    "text": "that might help us to\nmodel the data better. So in particular, we can\nlook at that color encoding,",
    "start": "180170",
    "end": "187319"
  },
  {
    "text": "which suggests that-- maybe there are three Gaussians. Maybe this is a mixture\nof three Gaussians.",
    "start": "187320",
    "end": "193670"
  },
  {
    "text": "So if we discover\nthat fact, that there is some underlying structure,\nthat there are three Gaussians,",
    "start": "193670",
    "end": "200660"
  },
  {
    "text": "that will not only help us\nto model the data little bit better, but it would also\ntell us a little bit more about the structure\nof the data we see.",
    "start": "200660",
    "end": "208780"
  },
  {
    "text": "So now if we use the fact that\nthere are three Gaussians here, we can write the\nprobability of a data point",
    "start": "208780",
    "end": "215909"
  },
  {
    "text": "as the sum over additional\nlatent variable z. So that latent variable here\nis a discrete random variable.",
    "start": "215910",
    "end": "223740"
  },
  {
    "text": "It's a categorical\nrandom variable that tells us which cluster does\nthe data belong to, all right?",
    "start": "223740",
    "end": "229290"
  },
  {
    "text": "So it can only take\nvalues 1, 2, or 3. It gives us the\nnumber of the cluster. So now in order to compute\nthe probability of A data",
    "start": "229290",
    "end": "236070"
  },
  {
    "text": "point, what we can\ndo is we can sum over all the clusters, sum over z.",
    "start": "236070",
    "end": "241590"
  },
  {
    "text": "And maybe I'll use the\nlaser pointer right here. Sum over z. And we can take the probability\nof the data belonging",
    "start": "241590",
    "end": "250950"
  },
  {
    "text": "to a particular Gaussian,\nto a particular cluster. So what's the probability\nof the data point given that you are in\na particular Gaussian?",
    "start": "250950",
    "end": "256989"
  },
  {
    "text": "So that will be, for\nexample, that one times the prior probability of\nwhich cluster the data is in.",
    "start": "256990",
    "end": "263850"
  },
  {
    "text": "All right. So to say a little bit\nmore what that p of z is, if we were to\nplot the distribution,",
    "start": "263850",
    "end": "270460"
  },
  {
    "text": "it might look\nsomething like that. So it's a discrete\ncategorical random variable",
    "start": "270460",
    "end": "276400"
  },
  {
    "text": "that can be distributed\nlike this, for instance, and this just tells us what's\nthe probability of the data point belonging to\na specific Gaussian.",
    "start": "276400",
    "end": "282460"
  },
  {
    "text": "This is the prior on that. All right. So I hope this is\nrelatively clear.",
    "start": "282460",
    "end": "289060"
  },
  {
    "text": "So how can we extend\nthis concept such that we don't just operate on\ndiscrete number of clusters",
    "start": "289060",
    "end": "295689"
  },
  {
    "text": "or a latent variable that\nis a discrete variable, but instead we expand it\nto a continuous variable?",
    "start": "295690",
    "end": "302289"
  },
  {
    "text": "All right. So let's try to do this. So I'll change the\ndistribution a little bit.",
    "start": "302290",
    "end": "307460"
  },
  {
    "text": "So now our distribution\nis over a variable that is just one dimensional. So we only have x here.",
    "start": "307460",
    "end": "313120"
  },
  {
    "text": "And this is our probability\ndistribution p of x. So it's a little\nfunky-looking distribution,",
    "start": "313120",
    "end": "318610"
  },
  {
    "text": "and it's not very\nconveniently distributed. It's not just a Gaussian\nor something like this. So we need to\nrepresent it somehow.",
    "start": "318610",
    "end": "326420"
  },
  {
    "text": "So now p of z, instead\nof being a distribution over a discrete\nrandom variable, would be a distribution over a\ncontinuous random variable.",
    "start": "326420",
    "end": "334260"
  },
  {
    "text": "For example, a\nnormal distribution. So this would be a Gaussian. So that p of z over here,\nthis is the equivalent just",
    "start": "334260",
    "end": "341569"
  },
  {
    "text": "in the continuous world of that\np of z that we had over here.",
    "start": "341570",
    "end": "346720"
  },
  {
    "text": "So now if we were to\nrewrite that formula, it would look very similar. But instead of summing over\nthe discrete number of clusters",
    "start": "346720",
    "end": "354150"
  },
  {
    "text": "or discrete number of values\nthat the latent variable can take, we would be taking the\nintegral over all the values",
    "start": "354150",
    "end": "362580"
  },
  {
    "text": "of z, all right? That's what makes\nit a little tricky. So now what's the\nintuition behind it?",
    "start": "362580",
    "end": "368949"
  },
  {
    "text": "What does it mean in practice? So if we were to sample a\nparticular z, for example,",
    "start": "368950",
    "end": "375130"
  },
  {
    "text": "let's say that one right here. That's what corresponds\nto some kind of Gaussian,",
    "start": "375130",
    "end": "381340"
  },
  {
    "text": "the same as that\nz correspondent, let's say to Gaussian\nnumber 1 over here.",
    "start": "381340",
    "end": "387884"
  },
  {
    "text": "So that's what we\ngot transformed to some kind of Gaussian\np of x given z, right? Then the Gaussian would\nlook maybe like this.",
    "start": "387885",
    "end": "396100"
  },
  {
    "text": "Now what the integral\nis doing is it's taking",
    "start": "396100",
    "end": "401184"
  },
  {
    "text": "the-- it's integrating at z. So it's taking the integral\nover all possible values of z.",
    "start": "401184",
    "end": "406750"
  },
  {
    "text": "For example, these\nare a few samples. And each one of them can be\nmapped to a slightly different",
    "start": "406750",
    "end": "412180"
  },
  {
    "text": "Gaussian. For example, something like\nthat, or something like that.",
    "start": "412180",
    "end": "418438"
  },
  {
    "text": "And they could be right\nnext to each other. Sorry, this is a\nterrible picture but I hope you get the idea.",
    "start": "418438",
    "end": "424900"
  },
  {
    "text": "And if we integrate\nout z entirely, we should be able\nthen to represent",
    "start": "424900",
    "end": "430199"
  },
  {
    "text": "any continuous\ndistribution, right? We are just integrating z\nx, so we are integrating over every single possible z.",
    "start": "430200",
    "end": "435883"
  },
  {
    "text": "So it's kind of like creating\na mixture of Gaussians where you have infinitely many\ncomponents, as opposed",
    "start": "435883",
    "end": "443430"
  },
  {
    "text": "to just three\ncomponents on the left. All right, so going back\nto why this is useful.",
    "start": "443430",
    "end": "451450"
  },
  {
    "text": "Well, so I think\nyou've heard about the variational autoencoder. So variational autoencoder\nis one use case for us",
    "start": "451450",
    "end": "458890"
  },
  {
    "text": "where we can have our\ndata, for example, images.",
    "start": "458890",
    "end": "463585"
  },
  {
    "text": "And we want to\nrepresent some kind-- we want to discover some kind\nof latent structure in the data. So what the variational\nautoencoder,",
    "start": "465222",
    "end": "471580"
  },
  {
    "text": "which is a latent variable\nmodel, would allow us to do is to map our images\ninto some latent space.",
    "start": "471580",
    "end": "476689"
  },
  {
    "text": "So two points in\nsome latent space. That could be two points\nunder the Gaussian. And then we can interpolate\nthrough that latent space,",
    "start": "476690",
    "end": "483940"
  },
  {
    "text": "and that interpolation\nwould result in a smooth interpolation\nin the image space as well. All right, so we discovered\nsome kind of latent structure",
    "start": "483940",
    "end": "490930"
  },
  {
    "text": "in the data and we can\nthen generate the data using the latent structure.",
    "start": "490930",
    "end": "497473"
  },
  {
    "text": "All right, so this\nis a little bit about latent variable models. So why are we talking about\nthis in the context of control?",
    "start": "497473",
    "end": "506030"
  },
  {
    "text": "So they are actually\nvery common in control. And one particular use\ncase is a Kalman filter.",
    "start": "506030",
    "end": "512559"
  },
  {
    "text": "And Kalman filter\nis something that is used very often in\nrobotics to this day",
    "start": "512559",
    "end": "518860"
  },
  {
    "text": "to do things such as state\nestimation or localization. And it's a very\npopular approach.",
    "start": "518860",
    "end": "526029"
  },
  {
    "text": "Actually quite all of it\nwas discovered in the '60s. And Kalman filter just\non a graphical model that",
    "start": "526030",
    "end": "535510"
  },
  {
    "text": "looks more or less like this. It's a hidden Markov model\nwhere we observe some variables.",
    "start": "535510",
    "end": "540949"
  },
  {
    "text": "So in this case,\nwe are observing, for example, the measurements. So for example, your robot\nhas some kind of LiDAR on it",
    "start": "540950",
    "end": "549040"
  },
  {
    "text": "and based on this, it\nobserves the world around it. And then a Kalman\nfilter tells us",
    "start": "549040",
    "end": "554050"
  },
  {
    "text": "how we can estimate the state\nor the position of your robots. So these are the\nlatent factors z.",
    "start": "554050",
    "end": "561680"
  },
  {
    "text": "So this is one possible\nuse case of latent variable models in control. And we'll learn\nabout a few more.",
    "start": "561680",
    "end": "567209"
  },
  {
    "text": "All right, and then we'll\nalso talk about control as inference. And this is a slightly\ndifferent topic.",
    "start": "570090",
    "end": "575589"
  },
  {
    "text": "So let me try to introduce it\nas well, why this is important.",
    "start": "575590",
    "end": "582070"
  },
  {
    "text": "And this is a framework\nthat allows us to describe a stochastic optimal behavior.",
    "start": "582070",
    "end": "587710"
  },
  {
    "text": "So what do we mean by this? So far, our algorithms,\nour reinforcement",
    "start": "587710",
    "end": "593160"
  },
  {
    "text": "learning algorithms, the only\nthing they were focusing on was to optimize the\nsum of future rewards.",
    "start": "593160",
    "end": "599320"
  },
  {
    "text": "And so they're just\ntrying to be optimal. So what does that mean? So, for example, if we\nhave an owner and a dog,",
    "start": "599320",
    "end": "608210"
  },
  {
    "text": "the dog is the agent. If the owner is asking\nthe dog to come over,",
    "start": "608210",
    "end": "614420"
  },
  {
    "text": "and we observe the data how\nthe dog is coming to the owner.",
    "start": "614420",
    "end": "619620"
  },
  {
    "text": "It would look something\nprobably like that. So we will have a dog here. We will have the owner here.",
    "start": "619620",
    "end": "624680"
  },
  {
    "text": "And then the owner is\nasking the dog to come over. And let's say the reward is the\nnegative distance to the owner.",
    "start": "624680",
    "end": "630920"
  },
  {
    "text": "And then the only thing that\nour reinforcement learning framework can explain right\nnow is the straight path,",
    "start": "630920",
    "end": "637940"
  },
  {
    "text": "the perfect optimal behavior\nbecause the agent is only optimizing for the\nsum of the rewards.",
    "start": "637940",
    "end": "644940"
  },
  {
    "text": "However, we know that\nin the real life, a dog can actually take\na few different paths.",
    "start": "644940",
    "end": "650389"
  },
  {
    "text": "Maybe it will look something\nlike that where maybe it will sniff something\non the side, maybe it will get distracted,\nand it won't be always perfect.",
    "start": "650390",
    "end": "658340"
  },
  {
    "text": "So it's optimal, but\nit's a little stochastic. And so that's also how\nwe humans do things.",
    "start": "658340",
    "end": "663620"
  },
  {
    "text": "We are not always\nexactly perfect and we are not always exactly\noptimizing some kind of reward.",
    "start": "663620",
    "end": "668870"
  },
  {
    "text": "But you can get\nfrom our movements what we are trying to\ndo, but our behavior is a little bit stochastic.",
    "start": "668870",
    "end": "676360"
  },
  {
    "text": "So the idea in\ncontrol as inference is that if we can describe that\nstochastic optimal behavior through some model, and then\ntransfer this to our agents,",
    "start": "676360",
    "end": "685330"
  },
  {
    "text": "maybe our agents would behave\na little bit more like we do. And maybe it will improve our\nreinforcement learning agents.",
    "start": "685330",
    "end": "693260"
  },
  {
    "text": "And that turns out to\nbe true to some extent. So we'll learn in\nthe future lectures on how the probabilistic\ngraphical model that we'll",
    "start": "693260",
    "end": "700610"
  },
  {
    "text": "use to describe control\ncan help with better exploration with hierarchical\nreinforcement learning and skill discovery.",
    "start": "700610",
    "end": "706178"
  },
  {
    "text": "All right. So this is a short\nintro to the topic. We'll jump into\nvariational inference. Now are there any questions?",
    "start": "709000",
    "end": "717560"
  },
  {
    "text": "All right, looks like\nno questions so far. OK, so let's talk about\nvariational inference.",
    "start": "717560",
    "end": "725430"
  },
  {
    "text": "So variational\ninference is a tool that allows us to deal with\nthese latent variable models. And just to repeat, what's the\nproblem with the formulation",
    "start": "725430",
    "end": "733620"
  },
  {
    "text": "that we have so far? Well, the problem is\nthat-- so this p of z,",
    "start": "733620",
    "end": "738870"
  },
  {
    "text": "this is just a simple\nnormal distribution. That's a Gaussian. Gaussians are nice. We know how to deal with them.",
    "start": "738870",
    "end": "744029"
  },
  {
    "text": "This p of x given z, it's\nalso a Gaussian, for example, a Gaussian right here. That's relatively easy as well.",
    "start": "744030",
    "end": "750735"
  },
  {
    "text": "But what's difficult, what makes\nthat entire equation difficult is that integral over\na continuous variable. That makes it intractable.",
    "start": "750735",
    "end": "756600"
  },
  {
    "text": "So we need to\nsomehow get our zs, but do it without the integral. And this is where\nvariational inference comes",
    "start": "756600",
    "end": "763470"
  },
  {
    "text": "in where it can be useful. All right. So I'll go through this\nrelatively quickly.",
    "start": "763470",
    "end": "770915"
  },
  {
    "text": "Let's see how we\ncan go about it. So usually we will try to-- what we are trying to do\nin supervised learning,",
    "start": "770915",
    "end": "777260"
  },
  {
    "text": "we're trying to maximize\nlog-likelihood of our data. All right, so here we\nhave the i-th data point,",
    "start": "777260",
    "end": "783740"
  },
  {
    "text": "i depicts the index\nof the data point. We are having the IID\nassumption of our data",
    "start": "783740",
    "end": "789560"
  },
  {
    "text": "and we are multiplying the\nprobabilities of our data belonging to the model. And we are trying\nto maximize that.",
    "start": "789560",
    "end": "794889"
  },
  {
    "text": "And then we can do this\nwith log-likelihood instead, since logarithm\nis a monotonic function.",
    "start": "794890",
    "end": "801170"
  },
  {
    "text": "All right, so far so good. So let's now take the log\nof p theta of xi, so the log",
    "start": "801170",
    "end": "807860"
  },
  {
    "text": "probability of a\nparticular data point xi, and let's write it out\naccording to our model, so according to our little\nintegral right here.",
    "start": "807860",
    "end": "817470"
  },
  {
    "text": "All right, so let's\nmove a little further. Now we can multiply that\nentire equation by 1.",
    "start": "817470",
    "end": "824790"
  },
  {
    "text": "We can always do that. And here we take a\nparticular version of 1, which is some kind\nof distribution of z",
    "start": "824790",
    "end": "830970"
  },
  {
    "text": "of our latent variable. We'll call it qi of z. And we will divide it by the\nsame thing so that it's just 1.",
    "start": "830970",
    "end": "839010"
  },
  {
    "text": "All right. Now an important\nthing to note here is that this could be\nany distribution of z. We just picked some qi of\nz. it could be anything.",
    "start": "839010",
    "end": "848050"
  },
  {
    "text": "It could be q of\nz given something. It could be just q of z. The options are endless here.",
    "start": "848050",
    "end": "856040"
  },
  {
    "text": "All right, so now what we can\ndo is we can take that qz, and we have an\nintegral of qi of dz,",
    "start": "856040",
    "end": "864170"
  },
  {
    "text": "which is the definition of\nthe expectation under qi(z). So this is what\nwe did right here. We still have the log.",
    "start": "864170",
    "end": "870170"
  },
  {
    "text": "And instead of the integral and\nthe qi of z in the numerator, we have the expectation.",
    "start": "870170",
    "end": "878220"
  },
  {
    "text": "Now we can apply\nJensen's inequality, which is a simple\ninequality that tells us",
    "start": "878220",
    "end": "883529"
  },
  {
    "text": "that logarithm of a log-- logarithm of an\nexpectation is always greater or equal than the\nexpectation of the log.",
    "start": "883530",
    "end": "890460"
  },
  {
    "text": "All right, so we are\nnot going to prove it, but it basically\nlooks like this.",
    "start": "890460",
    "end": "897190"
  },
  {
    "text": "And we can actually\nderive for that bound s and that's not that complicated.",
    "start": "897190",
    "end": "903630"
  },
  {
    "text": "I can include that\nin the lecture notes. But let's just keep going and\nsee where we get with this.",
    "start": "903630",
    "end": "909070"
  },
  {
    "text": "So right now we are lower\nbounding our objective by something that looks\nlike the expectation",
    "start": "909070",
    "end": "914640"
  },
  {
    "text": "of the log of that ratio. Now we can split that ratio\ninto two different terms,",
    "start": "914640",
    "end": "921230"
  },
  {
    "text": "which we did down here. And we can split it\neven further and say that our log p\ntheta of xi is lower",
    "start": "921230",
    "end": "930730"
  },
  {
    "text": "bounded by something that\nlooks like this, by log of p of xi given z under\nthe expectation of qi of z",
    "start": "930730",
    "end": "938140"
  },
  {
    "text": "minus the KL divergence\nbetween qi of z, so some kind of distribution\nthat we just made up,",
    "start": "938140",
    "end": "943630"
  },
  {
    "text": "and the prior p of z. All right. So this was a very\nquick derivation.",
    "start": "943630",
    "end": "949800"
  },
  {
    "text": "But let's just focus\non that formula and try to understand\nwhat that means.",
    "start": "949800",
    "end": "955050"
  },
  {
    "text": "All right, so here I just copy\npasted the final equation. And just one quick note,\nso what we did here,",
    "start": "955050",
    "end": "964820"
  },
  {
    "text": "if we look at this,\nwe said that i-th is the index of the data point.",
    "start": "964820",
    "end": "969950"
  },
  {
    "text": "So xi is a particular\ndata point in your data. So far, we introduce\nqi of z, which",
    "start": "969950",
    "end": "976970"
  },
  {
    "text": "is a distribution over z that\nis specific to that particular data point, all right?",
    "start": "976970",
    "end": "982520"
  },
  {
    "text": "So we have qi of z\nhere and qi of z here. So what that means, we will have\nas many q of z distributions",
    "start": "982520",
    "end": "989750"
  },
  {
    "text": "as we have data points, which\nis in the era of deep learning, this is a very,\nvery large number.",
    "start": "989750",
    "end": "996269"
  },
  {
    "text": "So we don't\nnecessarily want that. So instead, what\nwe will do is we will use so-called amortized\nvariational inference which",
    "start": "996270",
    "end": "1004630"
  },
  {
    "text": "changes the distribution queue\nrather than it being qi of z,",
    "start": "1004630",
    "end": "1010570"
  },
  {
    "text": "it becomes q of z given xi. So we are free to choose\nwhatever distribution over z we like.",
    "start": "1010570",
    "end": "1016360"
  },
  {
    "text": "We chose that distribution\nso that instead of having many,\nmany distributions, we just have one distribution\nthat tells us the z given",
    "start": "1016360",
    "end": "1023920"
  },
  {
    "text": "the current data point. And so this is a\nsmall modification. All right, so let's\ntake the formula",
    "start": "1023920",
    "end": "1029920"
  },
  {
    "text": "and let's try to understand\nthe intuition behind it. And here one last\nthing to note is",
    "start": "1029920",
    "end": "1037178"
  },
  {
    "text": "that we would have two\ndifferent parameters here. We will have parameters\nfor our p distribution.",
    "start": "1037178",
    "end": "1043329"
  },
  {
    "text": "And these we'll call\nparameters theta. These are the\nparameters that given",
    "start": "1043329",
    "end": "1048760"
  },
  {
    "text": "our z, what tells us\nwould produce data-- will tell us the\nprobability of the image.",
    "start": "1048760",
    "end": "1055100"
  },
  {
    "text": "And we have parameters\nphi which are the parameters of our\nvariational distribution q.",
    "start": "1055100",
    "end": "1060990"
  },
  {
    "text": "All right, and we can\nuse that equation. We can maximize that variational\nlower bound so-called,",
    "start": "1060990",
    "end": "1066510"
  },
  {
    "text": "to optimize for both of\nthese, for phi and theta.",
    "start": "1066510",
    "end": "1071790"
  },
  {
    "text": "All right, so intuition. All right, so what\nwe built actually",
    "start": "1071790",
    "end": "1077179"
  },
  {
    "text": "looks something like that. We have our input\ndata point, xi.",
    "start": "1077180",
    "end": "1083970"
  },
  {
    "text": "Then this gets transformed\ninto some kind of latent code using our variational\ndistribution. So we'll have some\nkind of neural network",
    "start": "1083970",
    "end": "1090420"
  },
  {
    "text": "that takes that data point\nand then transforms it into some lower\ndimensional latent space z.",
    "start": "1090420",
    "end": "1097370"
  },
  {
    "text": "All right, and this is\nour q-phi of z given xi. So in other words, this\nis encoding the image",
    "start": "1097370",
    "end": "1103880"
  },
  {
    "text": "into some kind of\nlatent code z, right? We can also call it the encoder.",
    "start": "1103880",
    "end": "1110340"
  },
  {
    "text": "And then we have the second\npart of that equation, which now takes that latent code,\nsamples from that distribution,",
    "start": "1110340",
    "end": "1117450"
  },
  {
    "text": "takes that latent code and\nproduces the image, right? So this is now\np-theta of xi given z.",
    "start": "1117450",
    "end": "1122970"
  },
  {
    "text": "So this is trying to decode\nthat latent code back into the image.",
    "start": "1122970",
    "end": "1128140"
  },
  {
    "text": "And so this is where-- this is a variational\nautoencoder where we are first encoding\nthe image and then decoding it.",
    "start": "1128140",
    "end": "1136240"
  },
  {
    "text": "All right. So this is the equation. This is the\nvariational lower bound",
    "start": "1136240",
    "end": "1142010"
  },
  {
    "text": "that we just\nderived that we have to maximize with respect to\nboth theta and phi in order",
    "start": "1142010",
    "end": "1147350"
  },
  {
    "text": "to learn that. And usually, we have\nthat little term p",
    "start": "1147350",
    "end": "1153430"
  },
  {
    "text": "of z here that we haven't\ntalked about, usually what we pick for this is just a\nstandard normal distribution so",
    "start": "1153430",
    "end": "1161620"
  },
  {
    "text": "that the KL divergence is\nfairly easy between the two and in general that\ndoesn't lose generality",
    "start": "1161620",
    "end": "1167380"
  },
  {
    "text": "for the entire\nvariational autoencoder.",
    "start": "1167380",
    "end": "1172770"
  },
  {
    "text": "And that would allow us\nto get to the use case that we talked about where\nwe can have two images mapped",
    "start": "1172770",
    "end": "1178500"
  },
  {
    "text": "to different parts\nof the latent space, and then we can interpolate\nthrough that latent space z. And different samples\nfrom that space",
    "start": "1178500",
    "end": "1184535"
  },
  {
    "text": "could result in different\nimages from our data set.",
    "start": "1184535",
    "end": "1186660"
  },
  {
    "text": "All right, so what does that\nmean exactly to the equation? It looks a little complicated. So what's going on here?",
    "start": "1189700",
    "end": "1196820"
  },
  {
    "text": "So there are two terms here. There was the first\nterm right here, which is that expectation\nunder q-phi z given xi.",
    "start": "1196820",
    "end": "1204970"
  },
  {
    "text": "So this is expectation\nunder our encoder. So it just tells\nus, given the image,",
    "start": "1204970",
    "end": "1210550"
  },
  {
    "text": "what is the most probable z that\nwould have produced an image. We're trying to reconstruct\nthat image right? We're trying to maximize\nthe log-likelihood",
    "start": "1210550",
    "end": "1217030"
  },
  {
    "text": "of that image given z. So that means that\nwe are trying to do--",
    "start": "1217030",
    "end": "1224020"
  },
  {
    "text": "we can call it the\nreconstruction loss. And this is just trying to\nencourage the distribution",
    "start": "1224020",
    "end": "1229930"
  },
  {
    "text": "to describe the input. So it will be pushing both q of\nz given xi, and p of xi given z",
    "start": "1229930",
    "end": "1238390"
  },
  {
    "text": "to just describe the\ninput as well as possible. So if we just look at our q,\nwhat our q will be able to do",
    "start": "1238390",
    "end": "1244450"
  },
  {
    "text": "is it will just try\nto find such a code z",
    "start": "1244450",
    "end": "1249519"
  },
  {
    "text": "so that the reconstruction\nis as easy as possible. So q is trying to help\np as much as it can.",
    "start": "1249520",
    "end": "1255340"
  },
  {
    "text": "Now what can happen if we\njust use that first term only is that our q can\ncheat a little bit",
    "start": "1258160",
    "end": "1264790"
  },
  {
    "text": "and it can just have a\nvery tiny distribution. Basically it derives delta.",
    "start": "1264790",
    "end": "1270160"
  },
  {
    "text": "And say that for that particular\ncode, this is the image, right? So it's a perfect encoding\nbut it's not necessarily",
    "start": "1270160",
    "end": "1276159"
  },
  {
    "text": "what we are looking for. We are trying to look\nfor a distribution that is a little bit wider so that we\ncan sample from it, and so on.",
    "start": "1276160",
    "end": "1283570"
  },
  {
    "text": "So purely that encoding\nobjective, that reconstruction loss, can result in something\nthat we don't necessarily",
    "start": "1286210",
    "end": "1292480"
  },
  {
    "text": "want, in a way that-- in something where our\nvariational distribution can cheat a little bit.",
    "start": "1292480",
    "end": "1300110"
  },
  {
    "text": "So in addition, then we\nintroduce this additional term, the DKL term, the KL divergence\nbetween our variation",
    "start": "1300110",
    "end": "1306820"
  },
  {
    "text": "distribution and the prior. And this is penalizing the\nKL divergence, which kind of",
    "start": "1306820",
    "end": "1312010"
  },
  {
    "text": "acts as a regularizer. So because our p of z\nis a normal distribution of a certain variance,\nwe are trying",
    "start": "1312010",
    "end": "1318532"
  },
  {
    "text": "to make sure that\nthe KL divergence between our q and that\nnormal distribution is as small as possible.",
    "start": "1318532",
    "end": "1324860"
  },
  {
    "text": "So basically that means-- let's keep that variance of\nour variational distribution a little bit larger so that\nwe can describe more input,",
    "start": "1324860",
    "end": "1332539"
  },
  {
    "text": "so it's a better regularizer. It prevents our q\ndistribution from cheating.",
    "start": "1332540",
    "end": "1339760"
  },
  {
    "text": "All right. So that was a little\nbit of intuition behind variational inference. Are there any questions?",
    "start": "1339760",
    "end": "1347538"
  },
  {
    "text": "One question that\ncame up in the chat is, like, how do you\nrepresent distributions over z and for x given z?",
    "start": "1347538",
    "end": "1356100"
  },
  {
    "text": "So one choice is Gaussian, which\nis typically the normal choice. Are there other choices?",
    "start": "1356100",
    "end": "1362000"
  },
  {
    "text": "Yeah, in variational\nautoencoder, as far as I know, people use Gaussians.",
    "start": "1362000",
    "end": "1368540"
  },
  {
    "text": "I think there's also-- there\nare other distributions that you can use. There's one little caveat\nthat we haven't talked about,",
    "start": "1368540",
    "end": "1376140"
  },
  {
    "text": "which is how do\nyou backpropagate through all of this. And this is by using so-called\nreparameterization trick.",
    "start": "1376140",
    "end": "1381152"
  },
  {
    "text": "And there are\ncertain distributions that we know how we can do\nreparameterization trick with, such as Gaussians.",
    "start": "1381152",
    "end": "1386389"
  },
  {
    "text": "It's fairly easy. And such as some\ndiscrete distributions. So some people use discrete\ndistributions there too,",
    "start": "1386390",
    "end": "1393200"
  },
  {
    "text": "but I think Gaussian is by\nfar the most popular choice. Hi, there's another question.",
    "start": "1393200",
    "end": "1398710"
  },
  {
    "text": "Why do we always choose the\nstandard normal distribution for p of z?",
    "start": "1398710",
    "end": "1403720"
  },
  {
    "text": "Right. So that's a little bit of a\ndesign choice, but our p of z",
    "start": "1403720",
    "end": "1410050"
  },
  {
    "text": "can be-- it kind of doesn't matter\nwhat it is because our--",
    "start": "1410050",
    "end": "1415090"
  },
  {
    "text": "if we go back a little bit here,\nlet me just show the pointer.",
    "start": "1415090",
    "end": "1423635"
  },
  {
    "text": "Let me switch to this, OK. Our p of z can be\nany distribution. And then if we have a\nneural network that can",
    "start": "1423635",
    "end": "1429090"
  },
  {
    "text": "map from z to p of x given z. Then our neural network\nis expressive enough",
    "start": "1429090",
    "end": "1435750"
  },
  {
    "text": "to take a sample\nfrom any distribution and transform it to any other\ndistribution we would like. So it kind of doesn't matter\nwhat p of z we'll choose.",
    "start": "1435750",
    "end": "1443190"
  },
  {
    "text": "Neural network is\npowerful enough to represent anything\nelse after that. So usually we try\nto pick something",
    "start": "1443190",
    "end": "1449160"
  },
  {
    "text": "that is really\neasy to deal with, such as a normal distribution,\nthe KL divergence is given,",
    "start": "1449160",
    "end": "1454710"
  },
  {
    "text": "things that are very well\nunderstood because we don't really lose much. The expressiveness\nis still there",
    "start": "1454710",
    "end": "1459870"
  },
  {
    "text": "because our neural network\nwill be able to transform it into p of x given z.",
    "start": "1459870",
    "end": "1463380"
  },
  {
    "text": "All right.",
    "start": "1470190",
    "end": "1470690"
  },
  {
    "text": "So let's go into -- let's\nkind of put a pin in this, in variational inference. We'll get back to that method.",
    "start": "1475385",
    "end": "1481392"
  },
  {
    "text": "But now I would like\nto introduce control as inference framework. And this is the framework\nthat we briefly introduced.",
    "start": "1481392",
    "end": "1489007"
  },
  {
    "text": "And this is a framework\nthat allows us to describe a stochastic optimal behavior. So we had the dog.",
    "start": "1489007",
    "end": "1494059"
  },
  {
    "text": "The dog is trying to\ncome to the owner. And we just don't want to\nhave the only behavior being",
    "start": "1494060",
    "end": "1499382"
  },
  {
    "text": "the straight line,\nbut we also want to be able to represent\nbehavior that is a little bit stochastic, right?",
    "start": "1499382",
    "end": "1505370"
  },
  {
    "text": "And when we deal\nwith stochasticity, we usually want to introduce\nsome random variables",
    "start": "1505370",
    "end": "1510679"
  },
  {
    "text": "and potentially a\nprobabilistic graphical model. So let's do that.",
    "start": "1510680",
    "end": "1515929"
  },
  {
    "text": "So the things we\nknow are that there are some states and actions\nthat the dog can do, right?",
    "start": "1515930",
    "end": "1522169"
  },
  {
    "text": "So we can write a very simple\nprobabilistic graphical model where we basically represent\nthe dynamics of the world when",
    "start": "1522170",
    "end": "1529789"
  },
  {
    "text": "we say that there are some\nstates, there are some actions, and the dog can take\nan action in a state.",
    "start": "1529790",
    "end": "1536090"
  },
  {
    "text": "And that will lead to a\ndifferent state, right? So that just represents\nkind of all the behaviors",
    "start": "1536090",
    "end": "1541730"
  },
  {
    "text": "that the dog could do. And we will now represent the\njoint probability distribution",
    "start": "1541730",
    "end": "1547500"
  },
  {
    "text": "of all the states and\nactions as p of tau. So tau represents a trajectory.",
    "start": "1547500",
    "end": "1552640"
  },
  {
    "text": "So this is a probability\ndistribution over trajectories. And this is not really\nvery expressive because--",
    "start": "1552640",
    "end": "1562299"
  },
  {
    "text": "actually before we go there-- So what we assume here is that\nwe have the transition model, p",
    "start": "1562300",
    "end": "1567985"
  },
  {
    "text": "of s-prime given (s, a). So we now have the other\ndynamics propagate.",
    "start": "1567985",
    "end": "1575220"
  },
  {
    "text": "Now this is not\nextremely expressive because this just describes\nkind of any behavior that the dog can do, right?",
    "start": "1575220",
    "end": "1581070"
  },
  {
    "text": "The dog can take\nany actions that will result in next states. But it doesn't really\ndescribe the situation",
    "start": "1581070",
    "end": "1586380"
  },
  {
    "text": "that we are in right here\nwhere the dog is doing a very specific behavior, right? The dog is very intentional.",
    "start": "1586380",
    "end": "1592769"
  },
  {
    "text": "It has a task that\nhas a mind and it's trying to reach the owner. So we introduce an additional\nrandom variable that",
    "start": "1592770",
    "end": "1600509"
  },
  {
    "text": "tries to describe this, right? And this would look\na little funny. This is a random variable.",
    "start": "1600510",
    "end": "1607740"
  },
  {
    "text": "That is a binary random\nvariable depicted by a script O, which stands\nfor optimality, which",
    "start": "1607740",
    "end": "1615480"
  },
  {
    "text": "is a variable that\nbasically represents the intentionality of the dog.",
    "start": "1615480",
    "end": "1622019"
  },
  {
    "text": "So the dog is trying to\naccomplish a certain task. And it's not just\ndoing random things.",
    "start": "1622020",
    "end": "1630390"
  },
  {
    "text": "All right, so we'll introduce\nit as a binary random variable. So it can take only true\nor false as its values.",
    "start": "1630390",
    "end": "1638375"
  },
  {
    "text": "And if it's true, that\nmeans that the dog is trying to be as optimal as possible. And if it's false, then the dog\ndoesn't care about the task.",
    "start": "1638375",
    "end": "1645420"
  },
  {
    "text": "The optimality\ndoesn't matter to it. All right. So now we can represent\nthe probability",
    "start": "1645420",
    "end": "1653230"
  },
  {
    "text": "of the trajectory given\nthe fact that the dog is trying to be optimal. So given that our optimality\nvariable 1 through capital",
    "start": "1653230",
    "end": "1661540"
  },
  {
    "text": "T is equal to true. There was one point\nthat is missing in this.",
    "start": "1661540",
    "end": "1666970"
  },
  {
    "text": "We introduced an\nadditional variable that we assume to be known. We assume that the\ndog is always optimal,",
    "start": "1666970",
    "end": "1672478"
  },
  {
    "text": "so always-- if we say\nthat we have probability of the trajectory given O,\nwe'll assume that the O is",
    "start": "1672478",
    "end": "1678400"
  },
  {
    "text": "equal to true, right? So optimality, dog is\ntrying to be optimal. But we introduce this\nadditional random variable",
    "start": "1678400",
    "end": "1684879"
  },
  {
    "text": "and we didn't really describe\nwhat those connections mean, right? It was the dependence\nof that variable",
    "start": "1684880",
    "end": "1691240"
  },
  {
    "text": "on the action in the state. So we'll also introduce this. And we'll introduce\nthis as following.",
    "start": "1691240",
    "end": "1697760"
  },
  {
    "text": "We'll say that the probability\nof the dog being optimal at timestep t, given a state\nat that timestep and the action",
    "start": "1697760",
    "end": "1704960"
  },
  {
    "text": "that the dog took is equal to\nthe exponential of the reward of the state and the\naction-- at that state",
    "start": "1704960",
    "end": "1712429"
  },
  {
    "text": "with that action, all right? So the dog is basically\nexponentially--",
    "start": "1712430",
    "end": "1718597"
  },
  {
    "text": "what's the best way to describe? So basically if the\nhigher the reward is, we will exponentiate\nthis and our optimality",
    "start": "1718597",
    "end": "1725600"
  },
  {
    "text": "will be proportional to that. So the dog is trying\nto be more optimal as it's getting more reward,\nand that scales exponentially.",
    "start": "1725600",
    "end": "1733228"
  },
  {
    "text": "So this might seem a\nlittle funny right now why there is an exponent here,\nbut this is a particular design",
    "start": "1733228",
    "end": "1739040"
  },
  {
    "text": "choice that we choose so that we\nmay arrive at the algorithm we want at the end.",
    "start": "1739040",
    "end": "1743840"
  },
  {
    "text": "All right, so this is\nthe graphical model that we introduced to describe\na stochastic optimal behavior of the dog that is intentional.",
    "start": "1746630",
    "end": "1755390"
  },
  {
    "text": "I would like to do\na little sidestep. And we are dealing now with a\nprobabilistic graphical model",
    "start": "1755390",
    "end": "1761210"
  },
  {
    "text": "that has some variables\nthat are observable, such as optimality and\nsome that are not known.",
    "start": "1761210",
    "end": "1766220"
  },
  {
    "text": "And this looks like\na hidden Markov model that we briefly mentioned. So I just wanted to do a little\nrecap of hidden Markov models",
    "start": "1766220",
    "end": "1773389"
  },
  {
    "text": "and briefly describe\nhow they work. So hidden Markov models are\nmodels that look like that.",
    "start": "1773390",
    "end": "1779100"
  },
  {
    "text": "So we have some kind of\nevidence that is given to us. And there are some\nlatent factors.",
    "start": "1779100",
    "end": "1784348"
  },
  {
    "text": "We don't get to see that. We would want to estimate. We would want to\ninfer what they are. And one instance for us is,\nfor example, in a Kalman filter",
    "start": "1784348",
    "end": "1791300"
  },
  {
    "text": "where we are getting\nsome kind of measurements and we're trying to estimate\nthe positions of the robot.",
    "start": "1791300",
    "end": "1796800"
  },
  {
    "text": "And we know that given that\nwe are in a certain position, we can be in a different\nposition at the next timestep. But we also know that given that\nwe are in a certain position,",
    "start": "1796800",
    "end": "1804270"
  },
  {
    "text": "we should get a\ncertain measurement. All right, so what's usually\nassumed to be known here,",
    "start": "1804270",
    "end": "1810870"
  },
  {
    "text": "first, we assume that we\nknow so-called emission probabilities. So what is the probability\nof our measurement",
    "start": "1810870",
    "end": "1817230"
  },
  {
    "text": "given the latent state? So this is also often\nknown as the observation",
    "start": "1817230",
    "end": "1823140"
  },
  {
    "text": "model in the Kalman filter. We also assume that we know\nthe transmission dynamics. So we know the probability\nof zk, given zk minus 1.",
    "start": "1823140",
    "end": "1832830"
  },
  {
    "text": "And then we also assume that\nwe know the initial probability distribution z1, p of z1.",
    "start": "1832830",
    "end": "1840240"
  },
  {
    "text": "All right, so\nusually what we are trying to do in\nhidden Markov models is we are trying to\nestimate this quantity.",
    "start": "1840240",
    "end": "1846550"
  },
  {
    "text": "So what is the\nprobability of our latent variable at timestep k, given\neverything that we get to see?",
    "start": "1846550",
    "end": "1852000"
  },
  {
    "text": "So given all the measurements\n1 through n, right? So if we're able\nto estimate this,",
    "start": "1852000",
    "end": "1858350"
  },
  {
    "text": "then we can do\ninference about any z. So this is the goal\nfor our inference,",
    "start": "1858350",
    "end": "1863790"
  },
  {
    "text": "what we are trying to do. And there's one\nparticular algorithm that is trying to do this.",
    "start": "1863790",
    "end": "1870250"
  },
  {
    "text": "And this is the so-called\nforward-backward algorithm. Now just very briefly\ntell you what it's doing,",
    "start": "1870250",
    "end": "1877030"
  },
  {
    "text": "and then we'll use it for our\nprobabilistic graphical model. All right, so\nforward-backward algorithm",
    "start": "1877030",
    "end": "1883520"
  },
  {
    "text": "consists of two steps. There is the forward\nstep, which is trying to compute\nthis quantity, which",
    "start": "1883520",
    "end": "1888860"
  },
  {
    "text": "is the probability of our\nlatent, at timestep k, given all the measurements\nup until that point.",
    "start": "1888860",
    "end": "1896430"
  },
  {
    "text": "So given all the measurements\nof x1 through k minus 1. And then there is\na forward-- there's",
    "start": "1896430",
    "end": "1902010"
  },
  {
    "text": "the backward algorithm\nthat is trying to compute kind of the opposite. So it's trying to compute the\nprobability of our measurements",
    "start": "1902010",
    "end": "1908310"
  },
  {
    "text": "from k through n. So from now till the end,\ngiven our latent variables zk.",
    "start": "1908310",
    "end": "1915929"
  },
  {
    "text": "All right. So you might ask why\ndoes it look like this?",
    "start": "1915930",
    "end": "1921590"
  },
  {
    "text": "Why do we compute these\nparticular quantities? And the reason is that if we\nare interested in this quantity",
    "start": "1921590",
    "end": "1928050"
  },
  {
    "text": "right here, p of zk\ngiven x1 through n, we can show there is a little\nbit of derivation here.",
    "start": "1928050",
    "end": "1933570"
  },
  {
    "text": "I will skip that, but-- skip right to the answer. And we can show that\nthat's quantity that we're",
    "start": "1933570",
    "end": "1938759"
  },
  {
    "text": "interested in is equal-- product of both of these\nprobabilities, right?",
    "start": "1938760",
    "end": "1945010"
  },
  {
    "text": "So if we are able to\ndo the forward pass and the backward\npass, then we'll be able to find the\nanswer to our question",
    "start": "1945010",
    "end": "1950460"
  },
  {
    "text": "that we're interested in. All right, so this is a little\nsidestep into the hidden Markov",
    "start": "1950460",
    "end": "1956370"
  },
  {
    "text": "models. There's a question\nin the chat asking about the arrow between--",
    "start": "1956370",
    "end": "1962470"
  },
  {
    "text": "from z to x. Whether it should be from\nz to x, or whether it should be from x to z?",
    "start": "1962470",
    "end": "1967305"
  },
  {
    "text": "Shouldn't there\nbe arrow xn to zn.",
    "start": "1970290",
    "end": "1973710"
  },
  {
    "text": "I'm not sure I\nunderstand the question. Could you repeat that?",
    "start": "1977020",
    "end": "1980020"
  },
  {
    "text": "So I think that there-- you could also speak\nup if you like. I think that they're\nasking in the [INAUDIBLE] Yeah, I can skip it.",
    "start": "1983010",
    "end": "1990760"
  },
  {
    "text": "So I want to ask if the\narrow should be x1 to z1,",
    "start": "1990760",
    "end": "1996270"
  },
  {
    "text": "because x1 is\nobservation here, right? And from the observation,\nyou're up to latent space, z1.",
    "start": "1996270",
    "end": "2002830"
  },
  {
    "text": "Right. Yeah, so the-- I see. So it's about the direction\nof this arrow right here.",
    "start": "2002830",
    "end": "2008299"
  },
  {
    "text": "Yes. Yeah, and so in that model,\nthe reasoning is as following.",
    "start": "2008300",
    "end": "2014018"
  },
  {
    "text": "There are some\nunderlying state, that is the true state that gives\nrise to some measurement.",
    "start": "2014018",
    "end": "2019370"
  },
  {
    "text": "That's not that the measurement\ngives rise to where you are at. It's rather that you are\nat a certain position.",
    "start": "2019370",
    "end": "2025039"
  },
  {
    "text": "And that would give rise\nto a certain measurement. So kind of causality works this\nway rather than the other way.",
    "start": "2025040",
    "end": "2032320"
  },
  {
    "text": "Does that make sense? Yes, it does. Thanks. OK, great.",
    "start": "2032320",
    "end": "2038910"
  },
  {
    "text": "All right, so we have our\nhidden Markov model side note. So let's try to\nlook in particular",
    "start": "2038910",
    "end": "2044909"
  },
  {
    "text": "at the backward\nalgorithm and let's try to apply it to our case. So in this case, we are trying\nto estimate the probability",
    "start": "2044910",
    "end": "2052109"
  },
  {
    "text": "of our evidence of\nthe things we know, xk through n, given our latent\non the current timestep.",
    "start": "2052110",
    "end": "2059539"
  },
  {
    "text": "Right? So we go back to our model. Here in the top right\ncorner, I described everything we know about it.",
    "start": "2059540",
    "end": "2066690"
  },
  {
    "text": "And now we will\ndepict the-- we'll denote the backward message\nas beta, beta of st, at.",
    "start": "2066690",
    "end": "2073679"
  },
  {
    "text": "And this would be\nequal what do we know from the current\ntimestep till the end. So from a small t\nthrough a capital",
    "start": "2073679",
    "end": "2082050"
  },
  {
    "text": "T, given the latent at the\ncurrent timestep st, at, right?",
    "start": "2082050",
    "end": "2087850"
  },
  {
    "text": "So what that depicts\nis the probability that we can be optimal at\ntimesteps t through capital T,",
    "start": "2087850",
    "end": "2094510"
  },
  {
    "text": "given that we take action\nat in state st. All right, so this is our backward message.",
    "start": "2094510",
    "end": "2099870"
  },
  {
    "text": "And we can go through\nthe backward algorithm and compute this. And I am not going to go\nthrough the derivation,",
    "start": "2099870",
    "end": "2107700"
  },
  {
    "text": "but I will show you\nthe final result. And the final result\nlooks like this.",
    "start": "2107700",
    "end": "2113049"
  },
  {
    "text": "As we will arrive at the\nrecursive algorithm that starts from the\nend, that's actually",
    "start": "2113050",
    "end": "2118599"
  },
  {
    "text": "where the backward\nname comes from. It's a dynamic\nprogramming algorithm. And it computes the\nbackward message like this.",
    "start": "2118600",
    "end": "2126800"
  },
  {
    "text": "It uses the probability\nof optimality given the current state\nand action, and then the expectation under\nthe current dynamics",
    "start": "2126800",
    "end": "2133490"
  },
  {
    "text": "of the backward message. At the next time step, that is\nonly independent of st plus 1.",
    "start": "2133490",
    "end": "2141590"
  },
  {
    "text": "And here we define what that\nbackward message that is only dependent on st is equal to.",
    "start": "2141590",
    "end": "2148190"
  },
  {
    "text": "So basically if you\ndo dynamic programming and you go backwards,\nat this point you would already\nknow what that is.",
    "start": "2148190",
    "end": "2153680"
  },
  {
    "text": "And we know what that\nis, this is given to us. So we should be able to do\nthe entire backward pass using",
    "start": "2153680",
    "end": "2160359"
  },
  {
    "text": "these formulas. All right, so I wanted to\nlook a little bit closer at",
    "start": "2160360",
    "end": "2166670"
  },
  {
    "text": "what this actually means. So we went through\nthis backward algorithm",
    "start": "2166670",
    "end": "2172430"
  },
  {
    "text": "for the backward pass. And we arrived at\nthese two quantities. And they're maybe-- that's not\nentirely clear what they mean.",
    "start": "2172430",
    "end": "2180630"
  },
  {
    "text": "So let's do a little assignment. And I'll introduce a\nnew letter Vt of st.",
    "start": "2180630",
    "end": "2187130"
  },
  {
    "text": "And I will say that this\nis equal to the log of beta of our backward message\nthat is dependent on the end state of st, all right?",
    "start": "2187130",
    "end": "2194240"
  },
  {
    "text": "We know what the beta of st is. We just defined it right here. We just derived it right here.",
    "start": "2194240",
    "end": "2199730"
  },
  {
    "text": "And then I'll introduce\nanother letter, Qt of st, at. And I'll say that this\nis equal to the log",
    "start": "2199730",
    "end": "2205190"
  },
  {
    "text": "of the backward message that\nis dependent on the st and at. So this is what we\nderived right here.",
    "start": "2205190",
    "end": "2210109"
  },
  {
    "text": "So this is kind of a random\nassignment that I just made up. Let's try to run\nwith it, then let's",
    "start": "2212900",
    "end": "2218870"
  },
  {
    "text": "try to replace that equation\nwith that assignment and see what we get--",
    "start": "2218870",
    "end": "2224143"
  },
  {
    "text": "actually, sorry. Let's try to start with\nthis equation right there. All right, so if we\ndo that replacement,",
    "start": "2224143",
    "end": "2230270"
  },
  {
    "text": "we'll end up with something\nthat looks like this. Our Vt of St is equal to\nthe log of the integral",
    "start": "2230270",
    "end": "2236510"
  },
  {
    "text": "of the exponential of Qt of\nst, at over all the actions at,",
    "start": "2236510",
    "end": "2241536"
  },
  {
    "text": "all right? So that looks a little weird\nbut somewhat close to Q",
    "start": "2241536",
    "end": "2247590"
  },
  {
    "text": "learning where the value\nfunction is very closely related to the Q function. So if we assume that our V is\nkind of like a value function",
    "start": "2247590",
    "end": "2254220"
  },
  {
    "text": "and the Q is kind of\nlike a Q function, these two seem kind\nof related here.",
    "start": "2254220",
    "end": "2259530"
  },
  {
    "text": "But we have that\nlittle term here that is a little\ncomplicated, this log of integral of exponential.",
    "start": "2259530",
    "end": "2265200"
  },
  {
    "text": "All right. So what is that term? Let's try to think a\nlittle bit about this.",
    "start": "2265200",
    "end": "2271470"
  },
  {
    "text": "So let's try to\nconsider a situation where our Q values are\nreally big, all right?",
    "start": "2271470",
    "end": "2276960"
  },
  {
    "text": "So we have really big Q values. So for any different action that\nthe value there is really big.",
    "start": "2276960",
    "end": "2282720"
  },
  {
    "text": "Now we will exponentiate\nall of them. So if we exponentiate\nevery single one of them,",
    "start": "2282720",
    "end": "2288323"
  },
  {
    "text": "and these are really big\nvalues, then the difference between them will\nget much, much bigger because of the\nexponential function.",
    "start": "2288323",
    "end": "2295510"
  },
  {
    "text": "So if we did that, then\nour biggest action will be really, really big, even\ncompared to the second biggest",
    "start": "2295510",
    "end": "2302730"
  },
  {
    "text": "action, right? So then we'll take the integral\nacross all of these actions. So we'll kind of sum over\nall the possible actions.",
    "start": "2302730",
    "end": "2309930"
  },
  {
    "text": "And that sum will be now\ndominated by the top action, right, by the action that\nhad the biggest Q value.",
    "start": "2309930",
    "end": "2319250"
  },
  {
    "text": "And then we'll take\nthe log of all of this. So basically, if\nthat sum is dominated by the exponent of the\nhighest for the Q value",
    "start": "2319250",
    "end": "2328338"
  },
  {
    "text": "for the highest action,\nif we take a log of this, we're going back to the\nQ for the highest action.",
    "start": "2328338",
    "end": "2335529"
  },
  {
    "text": "So basically we can\nsay the following. This log of integral\nof exponential",
    "start": "2335530",
    "end": "2341510"
  },
  {
    "text": "is basically a max over\nactions of our Q function",
    "start": "2341510",
    "end": "2347330"
  },
  {
    "text": "as Q function gets\nbigger and bigger. So the bigger the Q is, the\nmore it looks like this.",
    "start": "2347330",
    "end": "2354690"
  },
  {
    "text": "All right, that looks\nsomewhat familiar now. So let's take a look\nat the second equation.",
    "start": "2354690",
    "end": "2360350"
  },
  {
    "text": "All right, so again\nwe have this equation that we got from\nthe backward pass. Now we have our assignments\nthat I wrote it down here.",
    "start": "2360350",
    "end": "2367500"
  },
  {
    "text": "Let's try to transform this. And if we do this,\nwe will end up with something that\nlooks like this. We would have our\nQ function of st,",
    "start": "2367500",
    "end": "2373882"
  },
  {
    "text": "at is equal to the\nreward of the time step plus the log\nof the expectation.",
    "start": "2373882",
    "end": "2379275"
  },
  {
    "text": "This is our expectation over\nthe dynamics of the exponential of Vt plus 1 of st plus 1.",
    "start": "2379275",
    "end": "2385900"
  },
  {
    "text": "All right, so this\nkind of almost looks like the Bellman equation. So let's try to make\nit a little bit more",
    "start": "2385900",
    "end": "2392100"
  },
  {
    "text": "into the Bellman equation. So this is the expectation\nunder our current dynamics.",
    "start": "2392100",
    "end": "2397230"
  },
  {
    "text": "So let's pretend that the\ndynamics are deterministic. So there's no expectation,\nthe expectation of a constant.",
    "start": "2397230",
    "end": "2402940"
  },
  {
    "text": "We can remove that. So then we have the logarithm\nof the exponent, right?",
    "start": "2402940",
    "end": "2408730"
  },
  {
    "text": "So these two cancel out. And we'll end up-- for deterministic\ntransitions, we'll end up in an equation\nthat looks like this.",
    "start": "2408730",
    "end": "2416410"
  },
  {
    "text": "Our Q is equal to the reward\nplus the value of the next time step.",
    "start": "2416410",
    "end": "2419640"
  },
  {
    "text": "All right, so now this\nlooks very familiar. Now there is a little\nbit of a problem because we just got rid\nof that expectation here",
    "start": "2422400",
    "end": "2429755"
  },
  {
    "text": "and there is something a\nlittle bit off about this, but we'll come back to that\nstochastic case a little later.",
    "start": "2429755",
    "end": "2435900"
  },
  {
    "text": "All right, but for\nnow, we saw that with that specific\nassignment, we can kind of retrieve\nsomething that",
    "start": "2435900",
    "end": "2442060"
  },
  {
    "text": "looks like a\nQ-learning algorithm where our Q is equal to\nthe reward plus the value",
    "start": "2442060",
    "end": "2447070"
  },
  {
    "text": "of the next time\nstep, and our value is the max over actions\nof the Q. All right,",
    "start": "2447070",
    "end": "2452690"
  },
  {
    "text": "so let's go one step further. And so far, we just\ncompleted the backward step.",
    "start": "2452690",
    "end": "2458150"
  },
  {
    "text": "And we kind of\narrived at something that looks like Q-learning. Let's try to compute the policy.",
    "start": "2458150",
    "end": "2463730"
  },
  {
    "text": "All right, so the\npolicy will try to compute the\nprobability of action at the given state assuming\nthat we are optimal, right?",
    "start": "2463730",
    "end": "2470525"
  },
  {
    "text": "We'll depict that this\npi of at, given st. And I'll skip the derivation.",
    "start": "2473780",
    "end": "2479610"
  },
  {
    "text": "We apply Bayes' rule\nhere, but at the end, we arrive at something\nthat looks like this. Our policy is equal to\nthe backward message that",
    "start": "2479610",
    "end": "2486720"
  },
  {
    "text": "is dependent on the\nst given and at, divided by the backward message\nthat is only dependent on st.",
    "start": "2486720",
    "end": "2494170"
  },
  {
    "text": "All right, so let's try to\nput all of this together. So first we have our\nbackward pass formulas",
    "start": "2494170",
    "end": "2501550"
  },
  {
    "text": "that look like Q-learning. And now we have our policy\nof this equal to the ratio",
    "start": "2501550",
    "end": "2506920"
  },
  {
    "text": "between these two\nbackward messages. Now if we use our assignment\nthat we had before,",
    "start": "2506920",
    "end": "2513440"
  },
  {
    "text": "we will arrive at something that\nlooks like this, our policy, so our probability of\naction of at a given state",
    "start": "2513440",
    "end": "2520130"
  },
  {
    "text": "is equal to the\nexponential of the Q at the state for that action\nminus V at that state.",
    "start": "2520130",
    "end": "2526609"
  },
  {
    "text": "So this is the difference\nbetween our Q function and the value function, which\nis also known as the advantage function, right?",
    "start": "2526610",
    "end": "2532910"
  },
  {
    "text": "So now we said\nthat our policy is equal to the exponential\nof our advantage for that specific action\nin that specific state.",
    "start": "2532910",
    "end": "2541450"
  },
  {
    "text": "And that seems to make sense. So basically what it says\nis that the policy assuming that you are trying to be\noptimal in that probabilistic",
    "start": "2541450",
    "end": "2548710"
  },
  {
    "text": "graphical model will\ntry to pick actions that are exponentially\nmore likely",
    "start": "2548710",
    "end": "2554920"
  },
  {
    "text": "or exponentially give\nhigher advantage. All right, so like the\nhigher the advantage is, they'll exponentiallly\nmore likely",
    "start": "2554920",
    "end": "2561130"
  },
  {
    "text": "will be to pick those actions. And importantly, we won't\nbe picking the actions that are always optimal.",
    "start": "2561130",
    "end": "2567115"
  },
  {
    "text": "We are not just taking\nthe actions that are doing the max\nover the Q. But we are doing something that\nis almost like this.",
    "start": "2567115",
    "end": "2573250"
  },
  {
    "text": "It's a little bit softer. They're distributed according\nto the distribution. And that distribution is very\ntightly tied to the reward,",
    "start": "2573250",
    "end": "2580960"
  },
  {
    "text": "but it's not just the max. All right, so this is quite\na bit of math in control",
    "start": "2580960",
    "end": "2586795"
  },
  {
    "text": "as inference. I was trying also to give you\na little bit of intuition, but are there any\nquestions at this point?",
    "start": "2586795",
    "end": "2593440"
  },
  {
    "text": "I do have a question\nabout the backward pass, and in particular the\nbig T. I didn't really",
    "start": "2596440",
    "end": "2602410"
  },
  {
    "text": "understand how this\nalgorithm gets off the ground running because maybe\nyou could just explain first",
    "start": "2602410",
    "end": "2609130"
  },
  {
    "text": "what big T is. Yeah, if you look at the first-- when we first started\nthe dynamic algorithm,",
    "start": "2609130",
    "end": "2615579"
  },
  {
    "text": "we need some value\nat t plus 1, right?",
    "start": "2615580",
    "end": "2622270"
  },
  {
    "text": "Right. Move on the next slide. Next slide. Yeah, the first\nline under for loop.",
    "start": "2622270",
    "end": "2628540"
  },
  {
    "text": "When we first got\nthe algorithm, we have to have the expectation\nthere of beta t plus 1.",
    "start": "2628540",
    "end": "2636250"
  },
  {
    "text": "How do we get that\nto begin with? Right, right. Yeah. This is a great question.",
    "start": "2636250",
    "end": "2642500"
  },
  {
    "text": "All right, so actually\nto explain this, we need to go back here. So here what we would\nhave is we would have a note at the end, that\nwould be s capital T, right?",
    "start": "2642500",
    "end": "2656829"
  },
  {
    "text": "And I believe we would also\nhave the optimality variable",
    "start": "2659720",
    "end": "2665400"
  },
  {
    "text": "at that time step. O at timestep T. I\nbelieve that's the case.",
    "start": "2665400",
    "end": "2675385"
  },
  {
    "text": "Let me think about this.",
    "start": "2675385",
    "end": "2676385"
  },
  {
    "text": "Yeah, I think so. So I think at the very-- I'm not entirely\nsure about this.",
    "start": "2683150",
    "end": "2689330"
  },
  {
    "text": "I can get back to this for you. I'm sorry. But I believe that at the\nvery final timestep at t",
    "start": "2689330",
    "end": "2697220"
  },
  {
    "text": "minus 1, this quantity\nhere, that is in question, will be equal to--",
    "start": "2697220",
    "end": "2703444"
  },
  {
    "text": "so we will have the\nbackward message of Bt plus 1 of st plus 1.",
    "start": "2706280",
    "end": "2712070"
  },
  {
    "text": "That Bt plus 1 over\nst plus 1 would be equal to something like that. So the question\nis basically what would that be equal to for\nthe very last timestep?",
    "start": "2712070",
    "end": "2720560"
  },
  {
    "text": "And I believe this would be the\nprobability of the optimality",
    "start": "2720560",
    "end": "2727900"
  },
  {
    "text": "at the very final timestep,\ngiven the state and the action. And that will be just equal to\nthis exponential of the reward",
    "start": "2727900",
    "end": "2733869"
  },
  {
    "text": "at the very final timestep. So for the very final timestep,\nit would just equal to this.",
    "start": "2733870",
    "end": "2740650"
  },
  {
    "text": "And then we were to go\nbackward from there, I believe. But I can double\ncheck that for you.",
    "start": "2740650",
    "end": "2747230"
  },
  {
    "text": "And t is in the future\nand it's defined a priori.",
    "start": "2747230",
    "end": "2753140"
  },
  {
    "text": "Big T is in the future\nsite, and it's-- Right, so T is kind of\nthe horizon for how long",
    "start": "2753140",
    "end": "2759470"
  },
  {
    "text": "the dog will be acting. OK. So the T will be the\nvery final timestep where the dog gets to the\nowner and it stops there.",
    "start": "2759470",
    "end": "2768090"
  },
  {
    "text": "OK, cool. I get it. Thank you. Thank you. All right, cool.",
    "start": "2768090",
    "end": "2774700"
  },
  {
    "text": "So we talked a little bit\nabout control as inference. Now we'll take the\nconcept that we",
    "start": "2774700",
    "end": "2779827"
  },
  {
    "text": "introduced in the first\npart of the lecture, the variational inference. And we'll try to apply\nit to our problem and see where we were we end up.",
    "start": "2779827",
    "end": "2788470"
  },
  {
    "text": "All right, so we started\nwith a graphical model that looks like this.",
    "start": "2788470",
    "end": "2793700"
  },
  {
    "text": "And we can write a\njoint probability of that entire model. And what we know is we have\nthe initial probability",
    "start": "2793700",
    "end": "2801170"
  },
  {
    "text": "distribution of\nthe initial state. We have the transition\nprobability. And we also have the\nprobability of optimality",
    "start": "2801170",
    "end": "2807410"
  },
  {
    "text": "for the given state and action. This is the exponent\nof the reward. So we know that our joint\ndistribution given that",
    "start": "2807410",
    "end": "2815339"
  },
  {
    "text": "the graphical\nmodel, what factor-- like this, factorizes\nlike that, all right?",
    "start": "2815340",
    "end": "2821849"
  },
  {
    "text": "So this is what we know so far. Now we will try to introduce\na variational distribution.",
    "start": "2824500",
    "end": "2831080"
  },
  {
    "text": "So variational distribution\nis a distribution over things that we've done now that\nare not visible to us. So this is not\nover the evidence.",
    "start": "2831080",
    "end": "2837430"
  },
  {
    "text": "The evidence in this case\nare optimality variables. So the variational\ndistribution, or q of z",
    "start": "2837430",
    "end": "2843430"
  },
  {
    "text": "before has to be over the\nthings that we don't see. All right, so this would be\nq of s1 through capital T,",
    "start": "2843430",
    "end": "2851319"
  },
  {
    "text": "a1 through capital\nT. And we'll just say that our variational\ndistribution is of this form.",
    "start": "2851320",
    "end": "2858850"
  },
  {
    "text": "And we picked this particular\ngraph with a certain goal in mind.",
    "start": "2858850",
    "end": "2864280"
  },
  {
    "text": "And you will see\nthat in a second. All right, so this\nis a graph that looks very similar to\nthe graph we had before.",
    "start": "2864280",
    "end": "2870859"
  },
  {
    "text": "This is very, very\nclose, but it has this one additional\nconnection between s and a. And usually we try to pick\nvariational distributions that",
    "start": "2870860",
    "end": "2878110"
  },
  {
    "text": "are very simple so that all\nthe equations get simpler.",
    "start": "2878110",
    "end": "2884230"
  },
  {
    "text": "Here we made a little\nbit of an odd choice, but we will see that\nit will actually",
    "start": "2884230",
    "end": "2890950"
  },
  {
    "text": "allow us to make things\nsimple at the end. All right, so let's talk a\nlittle bit about this graph.",
    "start": "2890950",
    "end": "2896840"
  },
  {
    "text": "So here we have the probability\nof the initial state. It was the same as\nwhat we had before.",
    "start": "2896840",
    "end": "2903849"
  },
  {
    "text": "We have transition\ndynamics that was also described at the exact\nsame probability as before.",
    "start": "2903850",
    "end": "2908860"
  },
  {
    "text": "But here we introduce an\nadditional connection, and this is the\nvariational connection that we didn't have\nin the previous graph.",
    "start": "2908860",
    "end": "2914330"
  },
  {
    "text": "And we'll call it\nq of at given st.",
    "start": "2914330",
    "end": "2920060"
  },
  {
    "text": "So now we can write the\nentire joint distribution for that graphical model.",
    "start": "2920060",
    "end": "2925800"
  },
  {
    "text": "And this would be\nequal to probability of the initial state times the\nproduct of all the transition",
    "start": "2925800",
    "end": "2932630"
  },
  {
    "text": "probabilities and\nour q of at given st. So now we have our\nvariational distribution.",
    "start": "2932630",
    "end": "2939800"
  },
  {
    "text": "And we have our\noriginal distribution. And one thing to\nnote here is that",
    "start": "2939800",
    "end": "2945580"
  },
  {
    "text": "the variational distribution\nhas the exact same dynamics and the initial state as p,\nso our original distribution.",
    "start": "2945580",
    "end": "2952210"
  },
  {
    "text": "And the only new thing is this\nvariational distribution q of at given st. So this is\nthe choice we made for our q.",
    "start": "2952210",
    "end": "2961119"
  },
  {
    "text": "All right. So now if you remember, we\nwere operating on xs and zs before when we are talking\nabout variational inference",
    "start": "2963700",
    "end": "2969980"
  },
  {
    "text": "where x is for the\nthings that we know and z is for the latent factors. So here we'll do assignment\nthat looks like this,",
    "start": "2969980",
    "end": "2975560"
  },
  {
    "text": "but we'll say that the things we\nknow, our optimality variables, we assume them to be true,\nand the latent factors",
    "start": "2975560",
    "end": "2983030"
  },
  {
    "text": "are all the states and actions.",
    "start": "2983030",
    "end": "2984440"
  },
  {
    "text": "So in that case, the joint\nprobability distribution down here is q of z.",
    "start": "2988320",
    "end": "2994650"
  },
  {
    "text": "And that joint probably\ndistribution right here is the probability\nof z, x, right?",
    "start": "2994650",
    "end": "2999854"
  },
  {
    "text": "Since z is everything\nhere, and x is that part. All right, so let's try to apply\nour variational lower bounds.",
    "start": "2999854",
    "end": "3007920"
  },
  {
    "text": "So this is a quick reminder\nof variation lower bound that we had at the\nbeginning of the lecture.",
    "start": "3007920",
    "end": "3012960"
  },
  {
    "text": "Let's try to apply it here. So the variational lower\nbound looked like this. We can transform it a little\nbit to look like that.",
    "start": "3012960",
    "end": "3019440"
  },
  {
    "text": "All right. So now we have that log\nof p, that's lower bounded by this formula right here.",
    "start": "3019440",
    "end": "3026550"
  },
  {
    "text": "So let's try to apply it, given\nour assignment that we set. Right, so if we do that, when\nwe have our joint probability",
    "start": "3026550",
    "end": "3034710"
  },
  {
    "text": "distribution of p of x, z. And we have our variational\ndistribution of q of z",
    "start": "3034710",
    "end": "3039900"
  },
  {
    "text": "or q of s1 through capital\nT, a1 through capital T.",
    "start": "3039900",
    "end": "3045020"
  },
  {
    "text": "We will now do the assignment. And so we will say that log\nprobability of our optimality 1",
    "start": "3045020",
    "end": "3050510"
  },
  {
    "text": "through capital T is lower\nbounded by the expectation now. The expectation is\nunder the q, and our z",
    "start": "3050510",
    "end": "3057710"
  },
  {
    "text": "is our joint probability of\nall the states and actions. And that will take\nthat log of p of x, z.",
    "start": "3057710",
    "end": "3066740"
  },
  {
    "text": "And we'll apply the\nrule that if you take the logarithm\nof the product,",
    "start": "3066740",
    "end": "3071750"
  },
  {
    "text": "you will get the sum\nof the logarithms. And that would look\nsomething like that.",
    "start": "3071750",
    "end": "3078120"
  },
  {
    "text": "So basically what\nwe'll have here, we are describing\nthe log of p of x, z just using our assignment.",
    "start": "3078120",
    "end": "3085000"
  },
  {
    "text": "We're just basically splitting\nthe product into the sum. And now we have to\nsubtract our log of q of z.",
    "start": "3085000",
    "end": "3093530"
  },
  {
    "text": "So let's do that. So this is our q of z. We are applying the log rule and\nwe are splitting it into a sum.",
    "start": "3093530",
    "end": "3102290"
  },
  {
    "text": "So now you can see that this\nlog of p of s1 is equal to this. This part is equal to our\nproduct of transition dynamics.",
    "start": "3102290",
    "end": "3110020"
  },
  {
    "text": "And then we have one additional\nterm, log of q, at given st.",
    "start": "3110020",
    "end": "3115850"
  },
  {
    "text": "Now you can see why we made that\nodd choice at the beginning. What that allows us\nto do is that allows",
    "start": "3115850",
    "end": "3121200"
  },
  {
    "text": "us to cancel out the initial\nstate distribution as well as the dynamics. So what do we end up with is\nsomething that looks like this.",
    "start": "3121200",
    "end": "3128807"
  },
  {
    "text": "And in particular, something\nthat looks like that. So our optimality, the log\nprobability of optimality",
    "start": "3128807",
    "end": "3136760"
  },
  {
    "text": "is lower bounded\nby the subjective. So if we're trying to optimize\nour optimality, since this",
    "start": "3136760",
    "end": "3144319"
  },
  {
    "text": "is lower bounded by\nthis, we can instead maximize that lower bound. And that will result in\nkind of the same thing.",
    "start": "3144320",
    "end": "3151319"
  },
  {
    "text": "So what is that\nlower bound here? The lower bound says\nthat will be taking and--",
    "start": "3151320",
    "end": "3158030"
  },
  {
    "text": "will be summing over timesteps\nunder the expectation of our current q.",
    "start": "3158030",
    "end": "3164869"
  },
  {
    "text": "All the rewards\nthat we're getting at every timestep plus\nthe entropy of the policy",
    "start": "3164870",
    "end": "3169970"
  },
  {
    "text": "q of at given st, right? So what that means is that if\nwe are trying to be optimal,",
    "start": "3169970",
    "end": "3177589"
  },
  {
    "text": "in order to do this, given that\nprobabilistic graphical model and that variational\ninference, we will be optimizing the\nrewards, but not only",
    "start": "3177590",
    "end": "3184250"
  },
  {
    "text": "just the rewards, which\nis what we were doing, the standard\nreinforcement learning, as we will also add\nto that objective",
    "start": "3184250",
    "end": "3190280"
  },
  {
    "text": "the entropy of the policy. So what that means is that\nthe policy is trying to stay as wide as possible, right?",
    "start": "3190280",
    "end": "3196262"
  },
  {
    "text": "So it's trying to not be\ncommitted to any solution, it's trying to be wide. So what that translates\nto is that we're",
    "start": "3196262",
    "end": "3202250"
  },
  {
    "text": "trying to maximize the\nreward and maximize the action entropy.",
    "start": "3202250",
    "end": "3208000"
  },
  {
    "text": "All right, so to\nquickly summarize this, our objective from that\nvariational inference",
    "start": "3208000",
    "end": "3213010"
  },
  {
    "text": "in this graphical\nmodel looks like this. It's the sum of the\nrewards plus the entropy.",
    "start": "3213010",
    "end": "3218349"
  },
  {
    "text": "Then we have the\nvalue and Q functions that are also derived\nusing that same framework.",
    "start": "3218350",
    "end": "3223845"
  },
  {
    "text": "And actually if we apply\nthat variational inference that we just described\nto our Q function, it will result in\nsomething like this,",
    "start": "3223845",
    "end": "3230500"
  },
  {
    "text": "which resolves our problem\nthat we had before, that expectation right\nin the middle here.",
    "start": "3230500",
    "end": "3235820"
  },
  {
    "text": "So this problem is gone\nif we did this correctly. And our policy itself would be\nexponential of the advantage,",
    "start": "3235820",
    "end": "3242700"
  },
  {
    "text": "right? So we have kind of\nall the components that we would need to\nconstruct an algorithm now.",
    "start": "3242700",
    "end": "3250240"
  },
  {
    "text": "So if you remember the\nQ-learning algorithm, it looked like this.",
    "start": "3250240",
    "end": "3255799"
  },
  {
    "text": "We were collecting\na dataset, and we're calculating the target\nvalues for the Q function.",
    "start": "3255800",
    "end": "3260980"
  },
  {
    "text": "And this was equal to\nthe Bellman equation, so it's the reward\nplus the maximum of the Q at the next timestep.",
    "start": "3260980",
    "end": "3266349"
  },
  {
    "text": "And then we're trying to fit\nour Q function to these targets, right? This is a standard\nQ-learning algorithm.",
    "start": "3266350",
    "end": "3273180"
  },
  {
    "text": "And we saw an\napplication of this, for example, for\nQTL for robotics. And our policy was\npicking the max--",
    "start": "3273180",
    "end": "3281190"
  },
  {
    "text": "the argmax of the Q, so\nit was picking the action that maximizes our Q value.",
    "start": "3281190",
    "end": "3286700"
  },
  {
    "text": "All right, so here\nwe derived something that's what we'll call a\nsoft Q-learning algorithm. Since the max is\nnot just a pure max",
    "start": "3286700",
    "end": "3293203"
  },
  {
    "text": "but it's a little bit\nsofter, we kind of incorporate this\nstochasticity here.",
    "start": "3293203",
    "end": "3298350"
  },
  {
    "text": "So far I just copied everything\nthat we had in the left side. So here are the notifications. So the Bellman equation\nhas changed as following.",
    "start": "3298350",
    "end": "3305700"
  },
  {
    "text": "Instead of having the\nhardmax over the actions, this has changed into a softmax. And softmax describes a log\nof integral of the exponent,",
    "start": "3305700",
    "end": "3313470"
  },
  {
    "text": "all right? So we are not\ntaking the fullmax, but we are a little\nbit softer here.",
    "start": "3313470",
    "end": "3320849"
  },
  {
    "text": "And instead of taking\nthe policy that is doing the argmax\nover the Q function, our policy is now the\nexponential of the advantage,",
    "start": "3320850",
    "end": "3327809"
  },
  {
    "text": "right? So it's not as hard\nas before either.",
    "start": "3327810",
    "end": "3330630"
  },
  {
    "text": "All right. So we started with some\nprobabilistic graphical model that describes stochastic\noptimal behavior.",
    "start": "3334060",
    "end": "3339750"
  },
  {
    "text": "And then using backward messages\nand a little bit of algebra, we derived\nvariational inference.",
    "start": "3339750",
    "end": "3346410"
  },
  {
    "text": "We derived an algorithm\nthat resembles Q-learning, but it's a little bit softer.",
    "start": "3346410",
    "end": "3351630"
  },
  {
    "text": "That takes into account\nthe fact that we are trying to be a little\nbit more stochastic.",
    "start": "3351630",
    "end": "3358250"
  },
  {
    "text": "All right, so let's quickly\nsee how this algorithm works in practice. So this algorithm was\nimplemented in this paper",
    "start": "3358250",
    "end": "3364110"
  },
  {
    "text": "from Tuomas Haarnoja et al. And just to, again, give a\nlittle bit more intuition,",
    "start": "3364110",
    "end": "3370140"
  },
  {
    "text": "if this is the landscape\nof our Q function in gray here, so on the y-axis,\nwe have the Q value.",
    "start": "3370140",
    "end": "3376109"
  },
  {
    "text": "On the x-axis is the action. And the Q function has\nsome hills and valleys.",
    "start": "3376110",
    "end": "3382500"
  },
  {
    "text": "Usually if we're to apply a\nstandard Q-learning algorithm, our policy will basically focus\non the peak of that Q function.",
    "start": "3382500",
    "end": "3390290"
  },
  {
    "text": "That will always try\nto pick the max here. The soft Q-learning,\nour policy is now",
    "start": "3390290",
    "end": "3395990"
  },
  {
    "text": "proportional to the\nexponent of the Q, or the exponent\nof the advantage. So it's not only trying\nto put all the probability",
    "start": "3395990",
    "end": "3401599"
  },
  {
    "text": "mass at the peak,\nbut it will save a little bit of probability\nmass for other hills as well.",
    "start": "3401600",
    "end": "3408180"
  },
  {
    "text": "So it's not as committed as the\nstandard Q-learning algorithm, all right?",
    "start": "3408180",
    "end": "3413220"
  },
  {
    "text": "So this is kind of\nthe motivation for it. All right, so why is this\nuseful for the policy",
    "start": "3413220",
    "end": "3419930"
  },
  {
    "text": "to not be as committed? So here are a few examples. So first, this is\nuseful for exploration.",
    "start": "3419930",
    "end": "3425570"
  },
  {
    "text": "So here's a task where you\nhave this ant-like robot, and the robot starts right here.",
    "start": "3425570",
    "end": "3430940"
  },
  {
    "text": "And it's trying to minimize\nthe distance to the goal, which is that blue square right here.",
    "start": "3430940",
    "end": "3437150"
  },
  {
    "text": "Now if the Q-learning--\non the left, we see the behavior of the\nconverged Q-learning algorithm",
    "start": "3437150",
    "end": "3444079"
  },
  {
    "text": "where the ant just happened to\nexplore the path up here first. And since this is\nalso initially getting",
    "start": "3444080",
    "end": "3450320"
  },
  {
    "text": "smaller and smaller distance,\nit gets a little bit stuck here. And the policy is\nbasically, like always,",
    "start": "3450320",
    "end": "3456619"
  },
  {
    "text": "looking for those hills, for\nthe hill in the Q function landscape. So it's really\ndifficult. The policy",
    "start": "3456620",
    "end": "3463430"
  },
  {
    "text": "has already committed\nto that solution. And it's really difficult\nfor it to unlearn it and try something else.",
    "start": "3463430",
    "end": "3469859"
  },
  {
    "text": "On the right here,\nwe see a solution coming from a soft\nQ-learning algorithm where you're not as committed. You still have some\nprobability mass",
    "start": "3469860",
    "end": "3478190"
  },
  {
    "text": "on actions that might have\na little bit lower outcome,",
    "start": "3478190",
    "end": "3484380"
  },
  {
    "text": "but that allows you to keep\nexploring different options and eventually find the better\nsolution to the problem.",
    "start": "3484380",
    "end": "3492079"
  },
  {
    "text": "This is also shown here\nwhere the reward is just",
    "start": "3492080",
    "end": "3498440"
  },
  {
    "text": "the speed of the ant and the\nspeed can be in any direction. So basically just the speed.",
    "start": "3498440",
    "end": "3504950"
  },
  {
    "text": "And on the left\nhere, we will see what's the distribution\nof behaviors that we get if we do\na standard Q-learning",
    "start": "3504950",
    "end": "3512000"
  },
  {
    "text": "algorithm, such\nas DTPG, which is a specific\nactor-critic algorithm. And what is the\ndistribution of behaviors",
    "start": "3512000",
    "end": "3519050"
  },
  {
    "text": "we get if we apply soft\nQ-learning to that problem? So let's see.",
    "start": "3519050",
    "end": "3524390"
  },
  {
    "text": "So you can see that DTPG\nseems to be committed to one direction and\njust goes with that,",
    "start": "3524390",
    "end": "3529440"
  },
  {
    "text": "versus soft Q-learning allows\nthe ant to kind of explore all kinds of\ndifferent directions and cover the state space a\nlittle bit better because it's",
    "start": "3529440",
    "end": "3536810"
  },
  {
    "text": "also trying to optimize for\nthe entropy of the policy. So what that also\nmeans is that if we",
    "start": "3536810",
    "end": "3542779"
  },
  {
    "text": "were to pre-train our\npolicy with soft Q-learning, it will be much\neasier to find him",
    "start": "3542780",
    "end": "3548270"
  },
  {
    "text": "because now our ant is a\nlittle bit more competent. It knows how to behave in\nmany different conditions.",
    "start": "3548270",
    "end": "3554210"
  },
  {
    "text": "And lastly, that also leads to\nmore robust policies, right? So here is an example of\nreal robot experiments",
    "start": "3557580",
    "end": "3563039"
  },
  {
    "text": "where the others are trying to\nstack these two LEGO blocks. And because this is trained\nwith soft Q-learning,",
    "start": "3563040",
    "end": "3571119"
  },
  {
    "text": "now if you perturb\nthe robot, the policy has visited these states\nbecause it was not trying to be very committed\nand just train optimal.",
    "start": "3571120",
    "end": "3578470"
  },
  {
    "text": "And it knows what\nto do with them. So it's a little\nbit more robust. If you perturb\nit, it can still--",
    "start": "3578470",
    "end": "3583670"
  },
  {
    "text": "it might have\nexperienced that state during training because\nit wasn't as committed. And it knows how\nto get to the goal.",
    "start": "3583670",
    "end": "3588940"
  },
  {
    "text": "All right, so this\nsummarizes control as variational\ninference framework. And we introduced a soft\nQ-learning algorithm.",
    "start": "3592020",
    "end": "3599520"
  },
  {
    "text": "Are there any questions\nat this point? I think, raising hand.",
    "start": "3599520",
    "end": "3605820"
  },
  {
    "text": "Yeah, would you mind\ngo up by two slides. Yeah.",
    "start": "3605820",
    "end": "3609292"
  },
  {
    "text": "So just curious, what\nis the softmax term that you add to the second\nline of the soft Q-learning",
    "start": "3613140",
    "end": "3619859"
  },
  {
    "text": "algorithm. Because you don't have access\nto that, i-prime that action.",
    "start": "3619860",
    "end": "3625050"
  },
  {
    "text": "Sorry, so this is\nover all the actions. All right, so the softmax\nterm is exactly this.",
    "start": "3627740",
    "end": "3634950"
  },
  {
    "text": "Right? So you're taking\nthe log of integral across all the actions. OK. It should be da.",
    "start": "3634950",
    "end": "3640030"
  },
  {
    "text": "So before we were taking the\nmax over all the actions, now we are taking softmax. And softmax is a term\nfor this little equation",
    "start": "3642780",
    "end": "3651519"
  },
  {
    "text": "here, log of integral of da. OK, thanks.",
    "start": "3651520",
    "end": "3655340"
  },
  {
    "text": "Here's another question. Yeah, I just was wondering\nwhen we made the assumption",
    "start": "3657972",
    "end": "3663190"
  },
  {
    "text": "about the probabilistical\ngraphical model, that enabled us to cancel out the dynamics.",
    "start": "3663190",
    "end": "3670630"
  },
  {
    "text": "Are there any practical\nlimitations to that? Like that would limit the\nlearning of the algorithm?",
    "start": "3670630",
    "end": "3678140"
  },
  {
    "text": "That's an interesting question. So to rephrase this\nquestion, is it possible to come up with\na different variational",
    "start": "3678140",
    "end": "3685849"
  },
  {
    "text": "distribution that will\nmake the algorithm better? Yeah, that's really interesting.",
    "start": "3685850",
    "end": "3691800"
  },
  {
    "text": "I'm not sure, potentially. I haven't thought about other\nvariational distributions",
    "start": "3691800",
    "end": "3699380"
  },
  {
    "text": "that we could do there. There is many, many\ndifferent options. It just has to be any\ndistribution over the variables",
    "start": "3699380",
    "end": "3706460"
  },
  {
    "text": "that we don't see,\nover latent variables. This particular one gives\nrise to this algorithm.",
    "start": "3706460",
    "end": "3711930"
  },
  {
    "text": "But I think there are probably\nother options that could give",
    "start": "3711930",
    "end": "3718040"
  },
  {
    "text": "rise to different algorithms. I'm not sure, but this is a\nvery interesting question.",
    "start": "3718040",
    "end": "3723500"
  },
  {
    "text": "Thanks. All right.",
    "start": "3728140",
    "end": "3734360"
  },
  {
    "text": "There are no other questions. Then let's take all of\nthis that we derived so far",
    "start": "3734360",
    "end": "3741140"
  },
  {
    "text": "and let's try to introduce a\nnew meta-reinforcement learning algorithm using\nall of these tools.",
    "start": "3741140",
    "end": "3747500"
  },
  {
    "text": "All right, so far\nwe've been considering the dog and the owner. And the task was very\nclear to the dog.",
    "start": "3747500",
    "end": "3754590"
  },
  {
    "text": "The dog was trying\nto get to the owner. And it was getting there\nin a stochastic way. And we're trying to\ndescribe that process.",
    "start": "3754590",
    "end": "3762100"
  },
  {
    "text": "So now we've meta-reinforcement\nlearning, the problem set up is a little bit different. Now the dog doesn't necessarily\nknow what the task is,",
    "start": "3762100",
    "end": "3769640"
  },
  {
    "text": "but the owner does, right? So the owner has\nsome kind of trick in mind that they\nwant the dog to do.",
    "start": "3769640",
    "end": "3775730"
  },
  {
    "text": "But the dog doesn't\nknow what the trick is. Now the dog knows that if\nit does the correct trick,",
    "start": "3775730",
    "end": "3782450"
  },
  {
    "text": "if it guesses it right,\nthen it would get a treat.",
    "start": "3782450",
    "end": "3787460"
  },
  {
    "text": "So it's a slightly\ndifferent situation. And my question to you is,\ngiven that setting, what's",
    "start": "3787460",
    "end": "3795290"
  },
  {
    "text": "a good strategy for the dog?",
    "start": "3795290",
    "end": "3796610"
  },
  {
    "text": "And we can either write in\nchat or we can raise your hand. But what would you do\nif you were the dog?",
    "start": "3800710",
    "end": "3806540"
  },
  {
    "text": "I think you have\nyour hand raised. I'm not sure if this is\nfrom the previous question. But feel free to\nanswer if you have.",
    "start": "3812020",
    "end": "3819310"
  },
  {
    "text": "Do you have any ideas? She does in the previous one. I think I missed the\nquestion actually, but I think if the dog\nis being ordered to sit,",
    "start": "3819310",
    "end": "3827289"
  },
  {
    "text": "right, he thinks that\nthe other person wants him to sit that you should sit. All right.",
    "start": "3827290",
    "end": "3832690"
  },
  {
    "text": "Yeah, so the dog\ndoesn't necessarily know what the owner\nwants from him, right? It's trying to guess.",
    "start": "3832690",
    "end": "3840460"
  },
  {
    "text": "I guess we can assume\nthat maybe the dog can think that maybe the owner is-- maybe the dog can try to\nguess what the owner wants.",
    "start": "3840460",
    "end": "3847420"
  },
  {
    "text": "For example, maybe the\nowner wants him to sit and it should then just sit. So he should just\ntry something out.",
    "start": "3847420",
    "end": "3852980"
  },
  {
    "text": "I think that's a\nfairly good strategy. Learning to follow different\ndirections of the owner.",
    "start": "3852980",
    "end": "3858720"
  },
  {
    "text": "Right. So if it could understand\nthe directions of the owner, then it should just\ntry to follow those.",
    "start": "3858720",
    "end": "3867450"
  },
  {
    "text": "But let's assume that the owner\ncan't communicate with the dog, right? The owner has just\nsomething in mind.",
    "start": "3867450",
    "end": "3875470"
  },
  {
    "text": "Try everything in\nequal probability. Yeah. I think that's a really\ngood strategy too. Yeah, so just guess what\nthe owner has in mind",
    "start": "3875470",
    "end": "3881530"
  },
  {
    "text": "and just go for it, and try it. And if you get the\ntreat, good for you. If you don't, it'll just-- take a step back and try\nsomething else, and go for it",
    "start": "3881530",
    "end": "3888970"
  },
  {
    "text": "and try that other idea that you\nhave until you get the treat.",
    "start": "3888970",
    "end": "3893990"
  },
  {
    "text": "All right. And believe it or\nnot, but this is connected to our variational\ninference framework.",
    "start": "3893990",
    "end": "3900430"
  },
  {
    "text": "So we'll talk about that. All right, so before we had the\nprobabilistic graphical model that looked like this, and\nthat described the situation",
    "start": "3900430",
    "end": "3908400"
  },
  {
    "text": "where the dog knows what to do. The dog knows what the task is. So now we'll introduce an\nadditional random variable",
    "start": "3908400",
    "end": "3916440"
  },
  {
    "text": "that describes a situation\nwhere the dog doesn't know what the task is. So we'll introduce an additional\nlatent variable z, which",
    "start": "3916440",
    "end": "3923450"
  },
  {
    "text": "describes the task, right? And this is not observed. This is not part\nof the evidence.",
    "start": "3923450",
    "end": "3929390"
  },
  {
    "text": "Now our z-- our optimality\nwill depend on this. Whether you get the treat\nor not depends on what task",
    "start": "3929390",
    "end": "3934880"
  },
  {
    "text": "you have in mind. And because we introduced the\nz in the original graphical model, we also have\nto introduce a thing",
    "start": "3934880",
    "end": "3941180"
  },
  {
    "text": "in our variational distribution. So now our policy will\nbe also dependent on z. So depending on what you\nthink the owner wants,",
    "start": "3941180",
    "end": "3948950"
  },
  {
    "text": "you'll be trying very\ndifferent actions, right? For example, if you think that\nthe owner wants you to sit,",
    "start": "3948950",
    "end": "3954320"
  },
  {
    "text": "you will try to sit. All right, so z in this case\ncorresponds to the latent task",
    "start": "3954320",
    "end": "3960100"
  },
  {
    "text": "that the owner has in mind. So we change our probabilistic\ngraphical model a little bit",
    "start": "3960100",
    "end": "3965460"
  },
  {
    "text": "to describe that situation. All right, so we have that\nadditional z, what will we do?",
    "start": "3965460",
    "end": "3972430"
  },
  {
    "text": "We will apply variational\ninference again, right? This is the tool that\nwe know how to use. Let's try to use it.",
    "start": "3972430",
    "end": "3979240"
  },
  {
    "text": "In this case, we'll\nchange it a little bit. So before we had\nx's as the evidence,",
    "start": "3979240",
    "end": "3984280"
  },
  {
    "text": "so as our optimality variables,\nand z-squared the trajectory, so all the states and actions.",
    "start": "3984280",
    "end": "3990532"
  },
  {
    "text": "I'll change it a\nlittle bit and say that x's are still the\noptimality variables, but z is the latent task.",
    "start": "3990532",
    "end": "3995320"
  },
  {
    "text": "And then we had our\nvariational lower bound that we direct\nat the beginning. So now let's use the assignment\nand see how this will change.",
    "start": "3997830",
    "end": "4006390"
  },
  {
    "text": "So what will happen\nis, here instead of x, I will have the log\nprobability of the optimality.",
    "start": "4006390",
    "end": "4012920"
  },
  {
    "text": "And for now, I left\nthe q untouched. But I just changed log\nof p of xi given z, the",
    "start": "4012920",
    "end": "4019220"
  },
  {
    "text": "log of probability of the\noptimality 1 through capital T given z. So z in this case describes\nthe latent task, OK?",
    "start": "4019220",
    "end": "4026164"
  },
  {
    "text": "So those are the\nonly thing I did. Let's look a little bit further. What is this term?",
    "start": "4028913",
    "end": "4034020"
  },
  {
    "text": "So this is the probability\nof optimality given that during a specific task.",
    "start": "4034020",
    "end": "4037770"
  },
  {
    "text": "So we know from the\nprevious section that the log probability\nof optimality",
    "start": "4040360",
    "end": "4045780"
  },
  {
    "text": "is lower bounded by this,\nby this objective where we are maximizing the reward\nplus the entropy of our policy.",
    "start": "4045780",
    "end": "4054220"
  },
  {
    "text": "So now this is now\nconditioned on z. So we'll change that\nformula a little bit, and we could go through\nthe entire derivation.",
    "start": "4054220",
    "end": "4059667"
  },
  {
    "text": "But I will just show\nyou what the result is. So now the log of p of 1\nthrough capital T given z",
    "start": "4059667",
    "end": "4066240"
  },
  {
    "text": "will be equal to still the\nsum over all the timesteps.",
    "start": "4066240",
    "end": "4071360"
  },
  {
    "text": "Now our variational distribution\nis not only over st and at, but also over z.",
    "start": "4071360",
    "end": "4076750"
  },
  {
    "text": "So we'll change that. The reward will depend\non the task z as well.",
    "start": "4076750",
    "end": "4082680"
  },
  {
    "text": "And our entropy will be on the-- of the policy, that is\nalso dependent on z. So the policy works differently\ndepending on the different z's.",
    "start": "4082680",
    "end": "4090610"
  },
  {
    "text": "So it's a very similar\nformula to what we had before. We didn't go for the\nwhole derivation, but it works basically\nalmost exactly the same.",
    "start": "4090610",
    "end": "4099370"
  },
  {
    "text": "All right. So we have the other\nterm left, which is this q of z of\nour task given xi.",
    "start": "4099370",
    "end": "4106960"
  },
  {
    "text": "And so in this case,\nour xi technically should be the\noptimality variables, but they are not really telling\nbecause these optimality",
    "start": "4106960",
    "end": "4112778"
  },
  {
    "text": "variables are always true. And if you remember, we can pick\nthe variational distribution to be conditioned\non anything we like.",
    "start": "4112779",
    "end": "4120210"
  },
  {
    "text": "This is our design choice. And we would want\nto pick the thing",
    "start": "4120210",
    "end": "4126399"
  },
  {
    "text": "that our variational\ndistribution should be conditioned on such\nthat it's very easy to guess what task it is.",
    "start": "4126399",
    "end": "4133200"
  },
  {
    "text": "So the question is, what's\na good xi in this case? What should we condition\nour variational distribution on so that we can easily\nguess what the task is?",
    "start": "4133200",
    "end": "4142057"
  },
  {
    "text": "So in this case,\none thing we can try is just to transition\non all the transitions that we have seen so far.",
    "start": "4142058",
    "end": "4147240"
  },
  {
    "text": "So basically the dog is\ntrying different tricks. And it will try to guess what\ntask it is given everything",
    "start": "4147240",
    "end": "4152930"
  },
  {
    "text": "that the dog has done so far. All right, so if\nit's tried sitting and it didn't get\nany rewards, it",
    "start": "4152930",
    "end": "4158115"
  },
  {
    "text": "will incorporate that into\nits thinking if it tried, I don't know, jumping\nor shaking or something,",
    "start": "4158115",
    "end": "4165073"
  },
  {
    "text": "it will also\nincorporate this, it will incorporate\neverything it's ever done and the rewards associated\nwith those behaviors.",
    "start": "4165074",
    "end": "4170569"
  },
  {
    "text": "All right. So this is the main idea\nbehind the PEARL algorithm.",
    "start": "4173240",
    "end": "4179130"
  },
  {
    "text": "And this is a way\nto derive it using a probabilistic graphical model. And I think if you've heard\nabout this algorithm before,",
    "start": "4179130",
    "end": "4184700"
  },
  {
    "text": "so I'll just quickly give you\nthe intuition of how this works with this graphical model.",
    "start": "4184700",
    "end": "4189949"
  },
  {
    "text": "So this is work by\nKate Rakelly et al. So PEARL is an off-policy\nmeta-reinforcement algorithm",
    "start": "4189950",
    "end": "4199010"
  },
  {
    "text": "that works like this. We would have some kind\nof adaptation data. So this is a dog\ntrying different things",
    "start": "4199010",
    "end": "4206088"
  },
  {
    "text": "and collecting all the\ntransitions-- so that they are trying to sit,\ntrying to jump, trying all kinds of\ndifferent things.",
    "start": "4206088",
    "end": "4212100"
  },
  {
    "text": "And in this paper, they\nrefer to this as context c. So c in this case are\nall the transitions",
    "start": "4212100",
    "end": "4217670"
  },
  {
    "text": "that we've seen so far. And then we will have our\nvariational distribution.",
    "start": "4217670",
    "end": "4222949"
  },
  {
    "text": "We're trying to guess\nwhat task it is. So this is the probability\nof the task of z given all the history so\nfar, or given our context.",
    "start": "4222950",
    "end": "4231079"
  },
  {
    "text": "So this is basically this\ndistribution right here, our Q of z given xi,\nwhere xi is the context z.",
    "start": "4231080",
    "end": "4238937"
  },
  {
    "text": "So we'll take the adaptation\nthat I will take everything we've seen so far and\nwe'll try to guess what's the most likely z\nthat would explain that data.",
    "start": "4238937",
    "end": "4250078"
  },
  {
    "text": "And then we will\npretend that this is the task that the owner wants. So we will guess\nour best z, we'll",
    "start": "4250078",
    "end": "4255220"
  },
  {
    "text": "pretend that this is the task\nthat the owner has in mind. And then we'll try to\noptimize our Q function with that task in mind.",
    "start": "4255220",
    "end": "4262370"
  },
  {
    "text": "So we'll try to optimize\nas if this was the task. So it's basically\nequivalent to the dog",
    "start": "4262370",
    "end": "4267520"
  },
  {
    "text": "trying to guess what is the\ntask that the owner has in mind and just going for it.",
    "start": "4267520",
    "end": "4274720"
  },
  {
    "text": "So if you look at the objective\nthat the authors write in this paper, it looks exactly\nthe same as the objective",
    "start": "4274720",
    "end": "4280770"
  },
  {
    "text": "that we wrote in\nthe previous slide with variational inference. So this is the\nobjective where you take the expectation\nunder our q phi of z",
    "start": "4280770",
    "end": "4288960"
  },
  {
    "text": "given the context, so\ngiven all the transitions. Here we have the reward, and\nthis is actually a soft reward.",
    "start": "4288960",
    "end": "4295540"
  },
  {
    "text": "So this is using the\nreward plus the entropy. And we have the\nKL divergence term",
    "start": "4295540",
    "end": "4301590"
  },
  {
    "text": "that tries to keep\nthis distribution, our variational distribution,\nclose to a normal Gaussian",
    "start": "4301590",
    "end": "4307500"
  },
  {
    "text": "so that we have the\nregularization term comes from variational inference.",
    "start": "4307500",
    "end": "4312685"
  },
  {
    "text": "And then because\nthis is-- we can do all of this using off-policy\nreinforcement learning, we can optimize for this\nreward using our soft q",
    "start": "4315655",
    "end": "4322610"
  },
  {
    "text": "functions and so on. This is an algorithm that\ncan be run off-policy.",
    "start": "4322610",
    "end": "4327680"
  },
  {
    "text": "And because of this,\nit's much more efficient than many other\non-policy equivalents,",
    "start": "4327680",
    "end": "4332840"
  },
  {
    "text": "such as RL-squared,\nMAML, or ProMP. And this is what you see\nin these graphs here.",
    "start": "4332840",
    "end": "4339380"
  },
  {
    "text": "So it works really, really well. All right. So I just wanted\nto reiterate this,",
    "start": "4339380",
    "end": "4346800"
  },
  {
    "text": "the intuition behind this. So if you remember,\nwe were talking about the dog that knows that\nif it does the right thing,",
    "start": "4346800",
    "end": "4354180"
  },
  {
    "text": "it will get the\ntreat but it doesn't know what it is that\nthe owner has in mind. So it will just commit and try. And based on the\nthings it sees, it",
    "start": "4354180",
    "end": "4361530"
  },
  {
    "text": "will eventually figure out\nwhat the task is and do that. So it turns out that the\nbehavior that PEARL exhibits",
    "start": "4361530",
    "end": "4368329"
  },
  {
    "text": "is very similar to this. So here's a task\nthat was given to it. So we start at that\ncircle down here.",
    "start": "4368330",
    "end": "4377390"
  },
  {
    "text": "And then the only\nthing we know is that there is a reward\nsomewhere on that circle. But the algorithm doesn't\nknow where it is, right?",
    "start": "4377390",
    "end": "4385160"
  },
  {
    "text": "And what you see\nhere, over these lines is the trajectories\nthat the PEARL tries,",
    "start": "4385160",
    "end": "4390199"
  },
  {
    "text": "that the PEARL\nalgorithm tries to do. So at first, just\nrandomly tries and tries to commit to a specific\ngoal that it thinks maybe",
    "start": "4390200",
    "end": "4399170"
  },
  {
    "text": "the designer has in mind. And then it tries it a few\ntimes until the blue circle",
    "start": "4399170",
    "end": "4404540"
  },
  {
    "text": "depicts the actual\nreward that is not visible to the PEARL algorithm. And then finally, it samples\nthe execution of the task that",
    "start": "4404540",
    "end": "4410743"
  },
  {
    "text": "corresponds to it, and\nthen it narrows it down and knows what task it is. And that procedure is\nalso called posterior",
    "start": "4410743",
    "end": "4416870"
  },
  {
    "text": "sampling since we are using\nposterior belief to sample what the task is.",
    "start": "4416870",
    "end": "4423470"
  },
  {
    "text": "So it behaves very similarly\nto what we described as a good strategy for the dog.",
    "start": "4423470",
    "end": "4427540"
  },
  {
    "text": "All right, so that\nsummarizes meta-RL as variational inference. Are there any questions?",
    "start": "4430140",
    "end": "4436889"
  },
  {
    "text": "Right. Do you think there's better\nstrategies for the dog or for PEARL algorithm\nfor this specific task,",
    "start": "4436890",
    "end": "4444400"
  },
  {
    "text": "other than sampling a specific\ntask and going for it? Can you do something\nbetter knowing that there",
    "start": "4444400",
    "end": "4450350"
  },
  {
    "text": "is a reward on the circle? Anywhere on the circle.",
    "start": "4450350",
    "end": "4453770"
  },
  {
    "text": "I think we saw an\nexample where you could like go out to the\narc and then track it",
    "start": "4459340",
    "end": "4464622"
  },
  {
    "text": "so that you're guaranteed\nyou can find it in like one trajectory. Yeah. Yeah, exactly. So the series sampling might\nnot be the best solution here.",
    "start": "4464622",
    "end": "4472720"
  },
  {
    "text": "And there are other papers\nthat do it differently. Yeah, so it's important\nto keep that in mind.",
    "start": "4472720",
    "end": "4478570"
  },
  {
    "text": "All right, cool. So that summarizes\nthe lecture where",
    "start": "4481980",
    "end": "4487800"
  },
  {
    "text": "we learned a lot about\nvariational inference and we presented\nthe framework that allows us to phrase a controller\nreinforcement learning problem",
    "start": "4487800",
    "end": "4494940"
  },
  {
    "text": "as an inference problem. And that has many\nother applications that we haven't talked\nabout, that a lot of papers",
    "start": "4494940",
    "end": "4500340"
  },
  {
    "text": "use that correspondence. Then we apply variational\ninference to that problem",
    "start": "4500340",
    "end": "4506820"
  },
  {
    "text": "and saw how we can derive\nan algorithm by using that. And then using that framework,\nby a simple addition",
    "start": "4506820",
    "end": "4513630"
  },
  {
    "text": "of additional\nlatent variable that is not visible to\nus, the task, we arrived at the\nmeta-reinforcement learning",
    "start": "4513630",
    "end": "4520050"
  },
  {
    "text": "algorithm that can use\noff-policy learning and then produce state\nof the art results.",
    "start": "4520050",
    "end": "4527650"
  },
  {
    "text": "All right, so as\nfor next week, we'll try to answer additional\nquestions that",
    "start": "4527650",
    "end": "4535119"
  },
  {
    "text": "come to mind when you\nthink about these problems, such as what should we do if\nwe want the agent to come up",
    "start": "4535120",
    "end": "4542110"
  },
  {
    "text": "with their own tasks? What if we don't want\nto specify the task and we just want to leave\nthe agent unattended",
    "start": "4542110",
    "end": "4547840"
  },
  {
    "text": "and have them figure out\nall the useful tasks? How should we think about\nhierarchies of tasks?",
    "start": "4547840",
    "end": "4555900"
  },
  {
    "text": "And then, can the agent\nlearn continuously over their lifetime? So how can we produce an\nagent that not only comes up",
    "start": "4555900",
    "end": "4561929"
  },
  {
    "text": "with its own task, it can\nbuild its own hierarchies, but also can do it over the\ncourse of its entire life?",
    "start": "4561930",
    "end": "4568580"
  },
  {
    "text": "And next week we'll\ntalk about about-- on Monday, we'll talk\nabout the first two topics. And then Wednesday, we'll\ntalk about the last one.",
    "start": "4568580",
    "end": "4576970"
  },
  {
    "text": "There was a few\nadditional resources that I encourage you\nto take a look at. First, there is this\nreally good tutorial",
    "start": "4576970",
    "end": "4583210"
  },
  {
    "text": "written by Sergey Levine\non control as inference. If you'd like to learn a\nlittle bit more about PEARL,",
    "start": "4583210",
    "end": "4590350"
  },
  {
    "text": "there is this quite\nnicely and simply written BAIR blog post about it.",
    "start": "4590350",
    "end": "4597622"
  },
  {
    "text": "And then there's also the\nBerkeley Deep Reinforcement Learning course,\nthat also has classes on control as inference.",
    "start": "4597622",
    "end": "4604960"
  },
  {
    "text": "All right. And this is it. Thank you so much. And if there are any other\nquestions, I'll stick around.",
    "start": "4604960",
    "end": "4610559"
  },
  {
    "text": "So please just ask.",
    "start": "4610560",
    "end": "4613690"
  },
  {
    "text": "One quick question. So the additional\ntarget of optimizing",
    "start": "4617270",
    "end": "4625190"
  },
  {
    "text": "for the entropy of the policy\nreminded me of Epsilon,",
    "start": "4625190",
    "end": "4632030"
  },
  {
    "text": "like adding a little bit of\nprobability for your model, like the policy.",
    "start": "4632030",
    "end": "4637130"
  },
  {
    "text": "I was wondering like how those\ncompare in terms of performance or in terms of theory. Yeah, yeah.",
    "start": "4637130",
    "end": "4643340"
  },
  {
    "text": "That's a really good question. So Epsilon-Greedy is kind\nof like a little heuristic",
    "start": "4643340",
    "end": "4649160"
  },
  {
    "text": "that people came up with\nto encourage a little bit more exploration. Here we kind of derive\na more principled way",
    "start": "4649160",
    "end": "4656060"
  },
  {
    "text": "to do this in a\nway that it's not just that we'll try random\nthings from time to time,",
    "start": "4656060",
    "end": "4661140"
  },
  {
    "text": "but we'll try to keep the\ndistribution very wide. So we'll allow the agent\nto kind of incorporate that",
    "start": "4661140",
    "end": "4667370"
  },
  {
    "text": "into its own objective. It's not that we are altering\nthe algorithm by incorporating that little heuristic, but\nwe tell the agent upfront",
    "start": "4667370",
    "end": "4675500"
  },
  {
    "text": "that we want you to be\nnot as committed as you would have been. I think in terms of\nhow they compare,",
    "start": "4675500",
    "end": "4683660"
  },
  {
    "text": "I believe that the state of\nthe art Deep RL algorithms use these soft objectives.",
    "start": "4683660",
    "end": "4689040"
  },
  {
    "text": "So things such as\nsoft-actor critic, I think, are still one of the best\nRL algorithms out there.",
    "start": "4689040",
    "end": "4695150"
  },
  {
    "text": "And they use the soft objective\nthat incorporates the entropy. And it's also--\nsometimes what people",
    "start": "4695150",
    "end": "4703099"
  },
  {
    "text": "do is they will\nstill incorporate the Epsilon-Greedy even to\nthe soft objective, right? So that's how the\nheuristic works regardless",
    "start": "4703100",
    "end": "4709730"
  },
  {
    "text": "where you apply it. So you can do both as well.",
    "start": "4709730",
    "end": "4712489"
  },
  {
    "text": "Got it. Thanks. Great.",
    "start": "4715466",
    "end": "4720320"
  }
]