[
  {
    "text": "Okay, let's get started. Uh, welcome back, everyone. Today, we will continue into lecture eight.",
    "start": "4610",
    "end": "11594"
  },
  {
    "text": "Is that right? Yes, it's lecture eight. Um, so the topics for today will be kernel methods and support vector machines.",
    "start": "11595",
    "end": "19800"
  },
  {
    "text": "[NOISE] And before we jump into, uh, today's topics, let's do a quick recap of what we covered in the previous lecture.",
    "start": "19800",
    "end": "29580"
  },
  {
    "text": "We covered generative algorithms. Um, in generative algorithms, for training,",
    "start": "29580",
    "end": "37545"
  },
  {
    "text": "we maximize the joint probability of p of, um, at the joint likelihood, p of x,",
    "start": "37545",
    "end": "44385"
  },
  {
    "text": "y, where x is your input and y is the output. And we, uh, we train the parameters using MLE.",
    "start": "44385",
    "end": "51385"
  },
  {
    "text": "And this is generally decomposed into two parts, the class prior and p of x given y,",
    "start": "51385",
    "end": "57640"
  },
  {
    "text": "and at prediction time use Bayes' rule to construct the posterior distribution of p of y given x, okay?",
    "start": "57640",
    "end": "63350"
  },
  {
    "text": "And the posterior will- will be different for different models, depending on what assumptions we make of whether p of x given",
    "start": "63350",
    "end": "70790"
  },
  {
    "text": "y is Gaussian or Bernoulli or- or whatever you find, we find p of y given x [NOISE] accordingly, right?",
    "start": "70790",
    "end": "77165"
  },
  {
    "text": "And we saw two examples of, uh, two different types of generative models,",
    "start": "77165",
    "end": "83030"
  },
  {
    "text": "the Gaussian discriminant analysis and naive Bayes. So in Gaussian discriminant analysis,",
    "start": "83030",
    "end": "88925"
  },
  {
    "text": "x is continuous, the inputs are continuous. And we make the assumption that, uh,",
    "start": "88925",
    "end": "94700"
  },
  {
    "text": "p of x given y follows a normal distribution with a mean that is specific to y,",
    "start": "94700",
    "end": "100310"
  },
  {
    "text": "and some shared co-variance Sigma, right? And the, uh, the corresponding posterior for prediction time follows,",
    "start": "100310",
    "end": "111035"
  },
  {
    "text": "uh, logistic, um, um, um, logistic, uh, regression, um, [NOISE] functional form, right?",
    "start": "111035",
    "end": "117660"
  },
  {
    "text": "And, um, [NOISE] you have more details of this in your first homework and,",
    "start": "117660",
    "end": "122945"
  },
  {
    "text": "um, hopefully, that's- that's all clear. Next, we saw naive Bayes.",
    "start": "122945",
    "end": "128160"
  },
  {
    "text": "Uh, here x was discrete like words in a text message, um, and we- we, um,",
    "start": "128160",
    "end": "136030"
  },
  {
    "text": "made this crucial conditional independence assumption. So to, uh, remind you,",
    "start": "136030",
    "end": "141095"
  },
  {
    "text": "conditional independence means p of x_j given y and x_k is equal to p of x_j given y, right?",
    "start": "141095",
    "end": "148920"
  },
  {
    "text": "[NOISE] And conditional independence is very different from general independence, right?",
    "start": "148920",
    "end": "154025"
  },
  {
    "text": "if two variables are conditionally independent, it says nothing about whether they're independent or not, and similarly if two variables- or two, uh,",
    "start": "154025",
    "end": "161209"
  },
  {
    "text": "variables are independent it says nothing about whether they are conditionally independent or not. [NOISE] Um, and then with the- with using this conditional independence assumption,",
    "start": "161210",
    "end": "172190"
  },
  {
    "text": "we constructed two different event models. In one of the- the first event model that we saw was called",
    "start": "172190",
    "end": "178849"
  },
  {
    "text": "the Bernoulli event model where p of x_j given y follows a Bernoulli distribution. And here x_j refers to the jth word in our vocabulary or our dictionary.",
    "start": "178850",
    "end": "188800"
  },
  {
    "text": "And we saw another model called the multinomial event model, where p of x_j given Phi follows a multinomial distribution.",
    "start": "188800",
    "end": "195844"
  },
  {
    "text": "But crucially, x_j over here refers to the jth word in a message. Here, x_j refers to the jth word in the vocabulary, right?",
    "start": "195844",
    "end": "203815"
  },
  {
    "text": "And the general intuition behind these two is that, in the Bernoulli event model,",
    "start": "203815",
    "end": "209390"
  },
  {
    "text": "suppose we apply it for, say, spam classification, the spamminess of a word is determined by how many messages the word appears in,",
    "start": "209390",
    "end": "220685"
  },
  {
    "text": "and how many message or how- in what fraction of the spam messages the word occurs in, in what fraction of the non-spam messages the word occurs in.",
    "start": "220685",
    "end": "228485"
  },
  {
    "text": "Whereas in the multinomial model, the spamminess of a word is determined by",
    "start": "228485",
    "end": "233780"
  },
  {
    "text": "what fraction of all the words in the spammy messages is this word? And what fraction of all the words in the non-spammy messages",
    "start": "233780",
    "end": "241849"
  },
  {
    "text": "is this word by not caring about the message boundaries, right? We- we consider just the set of all,",
    "start": "241850",
    "end": "248085"
  },
  {
    "text": "the- the collection of all words across all messages and just construct a multinomial distribution of all the words, right?",
    "start": "248085",
    "end": "256475"
  },
  {
    "text": "And then we also covered Laplace smoothing, where the idea of Laplace smoothing is we don't want to be,",
    "start": "256475",
    "end": "262069"
  },
  {
    "text": "uh, severely swayed by rare words. So words that probably never occurred in our training,",
    "start": "262070",
    "end": "268460"
  },
  {
    "text": "training, um, training dataset. We still wanna do something meaningful in at- at prediction time,",
    "start": "268460",
    "end": "274820"
  },
  {
    "text": "and the idea there is, um, you perform something called Laplace smoothing, where we, um,",
    "start": "274820",
    "end": "280849"
  },
  {
    "text": "assume we've seen, say every word, um, or- or we assume that you know,",
    "start": "280850",
    "end": "286700"
  },
  {
    "text": "we see one example of each class, uh, once in- in our training set,",
    "start": "286700",
    "end": "292410"
  },
  {
    "text": "and then we start to look at the- uh, at the training data and do our, you know, counting of messages or counting words or so on, right?",
    "start": "292410",
    "end": "300860"
  },
  {
    "text": "That's, uh, that's a quick recap of what we did. Any questions on this before we move on to the next topic? Yes.",
    "start": "300860",
    "end": "307595"
  },
  {
    "text": "Should we have any maximum likelihood estimation on the priors?",
    "start": "307595",
    "end": "313190"
  },
  {
    "text": "So the question is, are there any conditions on the prior that, uh, that will make the posterior take,",
    "start": "313190",
    "end": "319070"
  },
  {
    "text": "uh, a logistic form? So that's a good question. So in general, uh, if- if, um, if we are having,",
    "start": "319070",
    "end": "327395"
  },
  {
    "text": "um, a binary class where y is 0 or 1, right?",
    "start": "327395",
    "end": "334354"
  },
  {
    "text": "And x given y takes a form, uh, any form in the exponential family,",
    "start": "334355",
    "end": "341215"
  },
  {
    "text": "the posterior will always be a logistic [NOISE] form, right? Similarly, if your y is a categorical,",
    "start": "341215",
    "end": "349955"
  },
  {
    "text": "so one among many, and x given y belongs to any distribution in the exponential family,",
    "start": "349955",
    "end": "356255"
  },
  {
    "text": "then your y given x will take the softmax form. That's- that's- that's a common, um,",
    "start": "356255",
    "end": "362789"
  },
  {
    "text": "um, that's a property that holds for all exponential family distributions, er, uh, with either, uh,",
    "start": "362790",
    "end": "369830"
  },
  {
    "text": "a Bernoulli prior or a categorical prior. Good question. Any other distribution [inaudible]",
    "start": "369830",
    "end": "377280"
  },
  {
    "text": "Yeah, for, uh, for the logistic, uh, x, given y needs to be in the exponential family, yeah, good question.",
    "start": "377280",
    "end": "383770"
  },
  {
    "text": "Any other questions before we move on to kernel methods? Okay, so let's move on to kernel methods.",
    "start": "383770",
    "end": "390889"
  },
  {
    "text": "[NOISE]",
    "start": "390890",
    "end": "399540"
  },
  {
    "text": "So I'm gonna start off- yes question. [inaudible]",
    "start": "399540",
    "end": "411600"
  },
  {
    "text": "So the question is, you know, why do we wanna use, um, um, the- a generative model in- in place of say, logistic regression for,",
    "start": "411600",
    "end": "419335"
  },
  {
    "text": "um, if- if you know the- at prediction time, if it takes a logistic regression from why not just take,",
    "start": "419335",
    "end": "424884"
  },
  {
    "text": "you know, uh, um, a dual logistic regression itself? And I think we touched upon this,",
    "start": "424885",
    "end": "429940"
  },
  {
    "text": "um, in- in the last class. So in situations where you don't have a lot of data,",
    "start": "429940",
    "end": "435355"
  },
  {
    "text": "and you know that your x is distributed according to GDA, then using GDA, you're gonna have you,",
    "start": "435355",
    "end": "443025"
  },
  {
    "text": "um, is- it's going to be most efficient. By efficient, I mean sample efficient, you can- you can achieve higher accuracy or- or, you know,",
    "start": "443025",
    "end": "449824"
  },
  {
    "text": "your model works better with fewer amount of data if the modeling assumptions hold true,",
    "start": "449825",
    "end": "456240"
  },
  {
    "text": "if your x given y actually, uh, follows a Gaussian, right? Uh, whereas if a logistic regression,",
    "start": "456240",
    "end": "461880"
  },
  {
    "text": "in general, is more robust, even if, um, these assumptions are violated, logistic regression tends to just work well.",
    "start": "461880",
    "end": "469085"
  },
  {
    "text": "But in those cases where, um, [NOISE] the assumptions hold true, then GDA is more sample efficient with fewer number of examples, you get a better model.",
    "start": "469085",
    "end": "477980"
  },
  {
    "text": "[NOISE] Right, so Kernel methods.",
    "start": "477980",
    "end": "486120"
  },
  {
    "text": "So, um, [NOISE]",
    "start": "486120",
    "end": "500889"
  },
  {
    "text": "so far in the class, all the models that we've seen have been linear models, right? They are, um, they are linear in x.",
    "start": "500890",
    "end": "508000"
  },
  {
    "text": "And, um, the- for- for example, linear regression gives you a linear hypothesis in x and",
    "start": "508000",
    "end": "515709"
  },
  {
    "text": "logistic regression gives you a linear classifying boundary in your, um, data space.",
    "start": "515710",
    "end": "521005"
  },
  {
    "text": "However, as you've probably seen in your Homework 1, uh, question five I think,",
    "start": "521005",
    "end": "526569"
  },
  {
    "text": "you can actually start constructing nonlinear hypotheses using linear regression, right?",
    "start": "526570",
    "end": "531685"
  },
  {
    "text": "And similarly, um, you- we can also construct nonlinear classifiers using logistic regression by including higher-order, uh, features,",
    "start": "531685",
    "end": "541765"
  },
  {
    "text": "uh, for example, if you, if, um, your x, um, if you map it to some Phi of x,",
    "start": "541765",
    "end": "551204"
  },
  {
    "text": "and Phi of x is, um, 1, x,",
    "start": "551205",
    "end": "556620"
  },
  {
    "text": "x squared, norm, x^4, right?",
    "start": "556620",
    "end": "563130"
  },
  {
    "text": "And now if you perform linear regression on this feature vector, rather than performing linear regression directly on x,",
    "start": "563130",
    "end": "570815"
  },
  {
    "text": "the, um, the linear regression will give you a nonlinear hypothesis.",
    "start": "570815",
    "end": "576560"
  },
  {
    "text": "[NOISE] This is x. Your, uh, the linear regression will give you a hypothesis that might look like this.",
    "start": "576560",
    "end": "585000"
  },
  {
    "text": "And- and this is, uh- this is because we've included higher-order polynomial terms.",
    "start": "585000",
    "end": "590605"
  },
  {
    "text": "Now, to, um, extend this into the classification setting similarly, right?",
    "start": "590605",
    "end": "598889"
  },
  {
    "text": "Uh, suppose this is x_1, x_d. In this case, it was,",
    "start": "598890",
    "end": "604515"
  },
  {
    "text": "um, x and y. Uh, so supposing you may have two classes.",
    "start": "604515",
    "end": "610350"
  },
  {
    "text": "[NOISE]",
    "start": "610350",
    "end": "616000"
  },
  {
    "text": "Okay. Now, so, um, if you fit a, uh, a linear regression, for example, you might get, uh, a separating, uh,",
    "start": "616000",
    "end": "623920"
  },
  {
    "text": "hyperplane like this, but what if, um, um,",
    "start": "623920",
    "end": "629065"
  },
  {
    "text": "in - in - in case of, um, logistic regression, our data was something like this.",
    "start": "629065",
    "end": "635140"
  },
  {
    "text": "[NOISE] In such cases we could have,",
    "start": "635140",
    "end": "642985"
  },
  {
    "text": "uh, uh, just as in the case of linear regression, we could have included a few, uh, say, quadratic features and we would have gotten a separating boundary that looks like this.",
    "start": "642985",
    "end": "651945"
  },
  {
    "text": "You know it was - we're - we're performing logistic regression because we have mapped our data into a higher dimensional feature space, right?",
    "start": "651945",
    "end": "660500"
  },
  {
    "text": "Now, this seemed a little, um, arbitrary.",
    "start": "660500",
    "end": "666055"
  },
  {
    "text": "Uh, for example, how many features do we map it to? In this case, we map, uh,",
    "start": "666055",
    "end": "671320"
  },
  {
    "text": "a one-dimensional feature to 5 dimensions. Why not map it to 10 dimensions,",
    "start": "671320",
    "end": "676765"
  },
  {
    "text": "right? What's the right answer? Why not map it to 1,000 dimensions, right? Or even why not map it to an infinite number of dimensions, right?",
    "start": "676765",
    "end": "684315"
  },
  {
    "text": "Um, it's hard to think of, uh, infinite-dimensional feature vectors. You cannot even represent it explicitly on a computer,",
    "start": "684315",
    "end": "691385"
  },
  {
    "text": "um, but we will see that with kernel methods, we can actually do such things of taking an example and mapping it to [NOISE]",
    "start": "691385",
    "end": "698860"
  },
  {
    "text": "an - potentially infinite-dimensional feature vector [NOISE] and perform our learning algorithm in that infinite-dimensional space, right?",
    "start": "698860",
    "end": "706900"
  },
  {
    "text": "[NOISE] So the, um,",
    "start": "706900",
    "end": "713480"
  },
  {
    "text": "in terms of terminology, um, given x, [NOISE] we will call it attributes, right?",
    "start": "714030",
    "end": "723770"
  },
  {
    "text": "And Phi of x, which is the set of features, right? We will call this features.",
    "start": "724950",
    "end": "731300"
  },
  {
    "text": "And in cases when there are no feature maps, it's common to call Xs features itself.",
    "start": "732900",
    "end": "738295"
  },
  {
    "text": "You know, just - just think your feature map as an identity function, right?",
    "start": "738295",
    "end": "743410"
  },
  {
    "text": "And let's start with, uh, uh,",
    "start": "743410",
    "end": "748660"
  },
  {
    "text": "a motivating example of linear regression of how we solve linear regression using gradient descent, all right?",
    "start": "748660",
    "end": "757185"
  },
  {
    "text": "In linear regression for gradient descent, we - the update rule we had was Theta t plus",
    "start": "757185",
    "end": "766100"
  },
  {
    "text": "1 equals Theta t plus Alpha,",
    "start": "766100",
    "end": "774459"
  },
  {
    "text": "some learning rate, times i equals 1 to n,",
    "start": "774460",
    "end": "780230"
  },
  {
    "text": "y^i minus h Theta of x^i. In linear regression, this was Theta transpose x^i times x^i.",
    "start": "781890",
    "end": "795660"
  },
  {
    "text": "All right. This was the update rule for linear regression using gradient descent.",
    "start": "795660",
    "end": "801339"
  },
  {
    "text": "And we run this over and over until the model converges, which means our Theta vector stops - stops changing a lot, right?",
    "start": "801340",
    "end": "809515"
  },
  {
    "text": "Now, if we were to use a feature map like this, so with a feature map,",
    "start": "809515",
    "end": "820220"
  },
  {
    "text": "we get the rule, Theta t plus 1 equals",
    "start": "822210",
    "end": "827695"
  },
  {
    "text": "Theta t plus Alpha times i equals 1 to n,",
    "start": "827695",
    "end": "836120"
  },
  {
    "text": "y^i minus Theta transpose Phi x^i times Phi of x^i, right?",
    "start": "837240",
    "end": "851620"
  },
  {
    "text": "So we just replaced x with Phi of x. And we are performing gradient descent on this - on this, uh, uh, feature map.",
    "start": "851620",
    "end": "858190"
  },
  {
    "text": "The difference between these two equations is over here, Theta is in R^d, and over here,",
    "start": "858190",
    "end": "867279"
  },
  {
    "text": "Theta is in R^p, assuming our Phi takes you from R^d to R^p.",
    "start": "867280",
    "end": "878579"
  },
  {
    "text": "The idea here is d is the dimension of your original data and p is some high-dimensional space, potentially infinite, right?",
    "start": "878580",
    "end": "888245"
  },
  {
    "text": "So, uh, so this is the, um, um, update rule that we get.",
    "start": "888245",
    "end": "894970"
  },
  {
    "text": "And now, uh, imagine our Phi to be,",
    "start": "894970",
    "end": "900439"
  },
  {
    "text": "um, a feature map like - like this, like Phi of x equals, there's just one example, right?",
    "start": "901770",
    "end": "909200"
  },
  {
    "text": "1, x_1, x_2, and all the - then x_1 square,",
    "start": "909630",
    "end": "919270"
  },
  {
    "text": "x_1 x_2, x_1 x_3, and so on.",
    "start": "919270",
    "end": "924310"
  },
  {
    "text": "So I write x_1 cubed, x_1 square x2,",
    "start": "924310",
    "end": "931255"
  },
  {
    "text": "and so on, right? So basically, a set of all monomial terms of",
    "start": "931255",
    "end": "944125"
  },
  {
    "text": "order less than equal to 3, right? Now, what we see is, uh,",
    "start": "944125",
    "end": "950875"
  },
  {
    "text": "the number of - the dimension of the feature vector, in this case, will be - p will be approximately already cubed, right?",
    "start": "950875",
    "end": "962590"
  },
  {
    "text": "It's going to be, um, um, cubic times for all the three order terms, and some two order terms,",
    "start": "962590",
    "end": "967630"
  },
  {
    "text": "one order term, but overall it's going to be, you know, - the - the cubic term is gonna dominate and it's gonna be,",
    "start": "967630",
    "end": "972820"
  },
  {
    "text": "uh, approximately d cubed number of, uh, features, which means, um,",
    "start": "972820",
    "end": "978805"
  },
  {
    "text": "to perform each gradient, uh, uh, descent update, we now move from calculating dot products in d dimension.",
    "start": "978805",
    "end": "990460"
  },
  {
    "text": "For example, if d was 1,000 - d was 1,000, right? This dot product would take about, uh,",
    "start": "990460",
    "end": "997495"
  },
  {
    "text": "order d, uh, order d, right? Whereas, this dot product is gonna take about order d cubed,",
    "start": "997495",
    "end": "1005940"
  },
  {
    "text": "so which means if - if you had d equals to 1,000, this will take about, say, a - a - a - a 1,000 time-steps, whereas,",
    "start": "1005940",
    "end": "1012839"
  },
  {
    "text": "this would take about 1,000 cubed, would be like a billion time-steps, right?",
    "start": "1012839",
    "end": "1018690"
  },
  {
    "text": "So potentially, each - performing each update rule can be a million times slower.",
    "start": "1018690",
    "end": "1024240"
  },
  {
    "text": "And that expense is mostly because we chose - we just happened to choose a higher dimensional feature space, right?",
    "start": "1024240",
    "end": "1033990"
  },
  {
    "text": "Now, let's make a few observations.",
    "start": "1033990",
    "end": "1039550"
  },
  {
    "text": "So the claim here is that if we - in linear regression,",
    "start": "1043040",
    "end": "1049920"
  },
  {
    "text": "if we were to start with Theta naught equal to the 0 vector, if we start with the 0 vector as our initial starting point,",
    "start": "1049920",
    "end": "1058410"
  },
  {
    "text": "if gradient descent has started, uh, from there, then the claim is that any Theta vector,",
    "start": "1058410",
    "end": "1064830"
  },
  {
    "text": "Theta t can be represented as 1 to",
    "start": "1064830",
    "end": "1073120"
  },
  {
    "text": "n Beta i Phi of x^i, right?",
    "start": "1073120",
    "end": "1083580"
  },
  {
    "text": "So the claim is the Theta t that we encounter",
    "start": "1083580",
    "end": "1092054"
  },
  {
    "text": "during any state - any state of our gradient descent can always be represented as a linear combination of the features, right?",
    "start": "1092055",
    "end": "1101280"
  },
  {
    "text": "And this should be obvious, because, um, assume Theta 0 is - is just 0,",
    "start": "1101280",
    "end": "1108870"
  },
  {
    "text": "then Theta 1 will be - each of this is a scalar,",
    "start": "1108870",
    "end": "1115514"
  },
  {
    "text": "a scalar times Phi of x^i. And it's - it's the sum over them. And you can - you can absorb the Alpha - Alpha inside,",
    "start": "1115514",
    "end": "1122760"
  },
  {
    "text": "and Alpha times y^i minus Theta t of, uh, Phi of x^i will be,",
    "start": "1122760",
    "end": "1128385"
  },
  {
    "text": "uh, and Theta at - at 0 will be, uh, uh, just 0. So the - Theta of 1,",
    "start": "1128385",
    "end": "1137775"
  },
  {
    "text": "will be i equals 1 to n Alpha",
    "start": "1137775",
    "end": "1146205"
  },
  {
    "text": "times y^i times Phi of x^i, right?",
    "start": "1146205",
    "end": "1156120"
  },
  {
    "text": "So the very first, uh, uh, Theta vector that we get after the first step is going to be a linear combination",
    "start": "1156120",
    "end": "1163395"
  },
  {
    "text": "of the feature vectors, right?",
    "start": "1163395",
    "end": "1169500"
  },
  {
    "text": "And look, - and this will hold on - this will hold at every state - at every state of gradient descent, right?",
    "start": "1169500",
    "end": "1176625"
  },
  {
    "text": "The next, uh, for example, you know, we can show by induction",
    "start": "1176625",
    "end": "1182140"
  },
  {
    "text": "that - let me start on a fresh board. [NOISE]",
    "start": "1187130",
    "end": "1200370"
  },
  {
    "text": "Theta T plus 1 equals",
    "start": "1200370",
    "end": "1205845"
  },
  {
    "text": "theta plus t plus alpha 1 to n,",
    "start": "1205845",
    "end": "1215820"
  },
  {
    "text": "y_i minus theta t",
    "start": "1216880",
    "end": "1222815"
  },
  {
    "text": "transpose phi of x_i",
    "start": "1222815",
    "end": "1229955"
  },
  {
    "text": "times phi of x_i,",
    "start": "1229955",
    "end": "1236029"
  },
  {
    "text": "and we can just expand this. So theta t, by induction, we can, um,",
    "start": "1236030",
    "end": "1245795"
  },
  {
    "text": "write it as a linear combination of beta_i of t times phi of x_i.",
    "start": "1245795",
    "end": "1259130"
  },
  {
    "text": "So this is this plus, we'll write here,",
    "start": "1259130",
    "end": "1266600"
  },
  {
    "text": "alpha times i equals 1 to n y_i minus, again,",
    "start": "1266600",
    "end": "1275750"
  },
  {
    "text": "we're gonna expand this with this, i equals 1 to n beta_i of",
    "start": "1275750",
    "end": "1286370"
  },
  {
    "text": "t times phi of x_i.",
    "start": "1286370",
    "end": "1293165"
  },
  {
    "text": "So this is",
    "start": "1293165",
    "end": "1299090"
  },
  {
    "text": "the theta transpose phi of x_i",
    "start": "1299090",
    "end": "1311130"
  },
  {
    "text": "times phi of x_i, right?",
    "start": "1315970",
    "end": "1324440"
  },
  {
    "text": "Maybe I can use a few different colors to, um, highlight this.",
    "start": "1324440",
    "end": "1329779"
  },
  {
    "text": "So, um, what we did",
    "start": "1329780",
    "end": "1336035"
  },
  {
    "text": "is this is theta- theta t,",
    "start": "1336035",
    "end": "1345465"
  },
  {
    "text": "and this is the other theta t, right?",
    "start": "1345465",
    "end": "1353580"
  },
  {
    "text": "Did I miss something? Yep. And this thing over here is now essentially",
    "start": "1373980",
    "end": "1384109"
  },
  {
    "text": "our- before we do this,",
    "start": "1384109",
    "end": "1391190"
  },
  {
    "text": "- and this is, I'm gonna rewrite this as i equals 1 to",
    "start": "1391190",
    "end": "1400429"
  },
  {
    "text": "n beta_i of t plus.",
    "start": "1400430",
    "end": "1408245"
  },
  {
    "text": "So basically I'm taking the, uh, common sum across- across, um, both the terms.",
    "start": "1408245",
    "end": "1413960"
  },
  {
    "text": "Beta_i plus alpha times",
    "start": "1413960",
    "end": "1419730"
  },
  {
    "text": "y_i minus equals 1,",
    "start": "1420430",
    "end": "1428615"
  },
  {
    "text": "j equals 1 to n beta_j phi of",
    "start": "1428615",
    "end": "1436265"
  },
  {
    "text": "x_j transpose phi of x_i,",
    "start": "1436265",
    "end": "1447540"
  },
  {
    "text": "phi of x_j transpose.",
    "start": "1447670",
    "end": "1454850"
  },
  {
    "text": "We're gonna use a different color,",
    "start": "1454850",
    "end": "1461970"
  },
  {
    "text": "times phi of x_i, okay?",
    "start": "1472780",
    "end": "1480020"
  },
  {
    "text": "So, uh, what we see is that now theta of t plus 1",
    "start": "1480020",
    "end": "1485045"
  },
  {
    "text": "can be represented as again,",
    "start": "1485045",
    "end": "1492260"
  },
  {
    "text": "a linear combination of phi i's where each of these is beta t plus 1 of i, right?",
    "start": "1492260",
    "end": "1500120"
  },
  {
    "text": "What- what- what happened just here just now? So we started with the- with",
    "start": "1500120",
    "end": "1506510"
  },
  {
    "text": "the usual gradient descent update rule of theta t plus 1 is theta t plus, you know, um, plus the gradient.",
    "start": "1506510",
    "end": "1515509"
  },
  {
    "text": "And then to show by induction that theta t plus 1 can be represented as a linear combination of our feature vectors,",
    "start": "1515510",
    "end": "1524569"
  },
  {
    "text": "we, uh, we use the, uh, uh, the inductive assumption that theta t can now be",
    "start": "1524569",
    "end": "1529880"
  },
  {
    "text": "written as a linear combination of our feature vectors. So we plug- plugged in,",
    "start": "1529880",
    "end": "1535160"
  },
  {
    "text": "um- so we plugged in that expansion here and similarly we plugged in that expansion here and basically just reorganized the sums, right?",
    "start": "1535160",
    "end": "1544205"
  },
  {
    "text": "And we get that theta t plus 1 can now also be written as a linear combination of our feature vectors",
    "start": "1544205",
    "end": "1550820"
  },
  {
    "text": "where the new coefficients for performing the linear combination are these terms. Yes, question.",
    "start": "1550820",
    "end": "1557900"
  },
  {
    "text": "[inaudible]",
    "start": "1557900",
    "end": "1578240"
  },
  {
    "text": "So- so the question is in the higher dimensional feature space, the features are not independent.",
    "start": "1578240",
    "end": "1583355"
  },
  {
    "text": "Um, but that's fine because we made no independence assumptions about, um, about the features being independent for gradient descent.",
    "start": "1583355",
    "end": "1591560"
  },
  {
    "text": "So the update is the same. Yeah, the update is the same, exactly. So we don't care about what the features are. We just perform.",
    "start": "1591560",
    "end": "1596809"
  },
  {
    "text": "So examples [inaudible]. Well, so the feature- so for linear regression to work, um,",
    "start": "1596810",
    "end": "1604100"
  },
  {
    "text": "the features should ideally not be linearly dependent. In this case, they are not linearly dependent, right?",
    "start": "1604100",
    "end": "1611600"
  },
  {
    "text": "They- they are somewhat dependent, but it's not that one of them is a linear combination of the other, right?",
    "start": "1611600",
    "end": "1617460"
  },
  {
    "text": "So this is, um, so with this observation, we get the update rule in",
    "start": "1618430",
    "end": "1625130"
  },
  {
    "text": "terms of betas. [NOISE]",
    "start": "1625130",
    "end": "1650840"
  },
  {
    "text": "Let me go over this one more time. Just because this is- this is uh, probably the most crucial part for- to- to understanding kernel methods, right?",
    "start": "1650840",
    "end": "1659150"
  },
  {
    "text": "So we started with the theta vectors, right? We're performing gradient descent over theta vectors o- o- on the parameter space.",
    "start": "1659150",
    "end": "1667850"
  },
  {
    "text": "And this is the update rule that we have, right? And we saw that theta 1 over here.",
    "start": "1667850",
    "end": "1675800"
  },
  {
    "text": "[NOISE] Theta 1 is",
    "start": "1675800",
    "end": "1683420"
  },
  {
    "text": "trivially a linear combination of your features based on this rule, where ah, theta 0 is 0.",
    "start": "1683420",
    "end": "1690050"
  },
  {
    "text": "So this term cancels out and you get theta 1 equals 0 plus alpha, you know sum over i from 1 to n alpha times y^i times feature of x^i.",
    "start": "1690050",
    "end": "1699740"
  },
  {
    "text": "So theta 1 is trivially a linear combination of our features. Right, all good?",
    "start": "1699740",
    "end": "1705035"
  },
  {
    "text": "So that kind of bootstraps our inductive argument, and for the inductive step,",
    "start": "1705035",
    "end": "1711380"
  },
  {
    "text": "we assume theta t is- [NOISE] is already a linear combination of our features,",
    "start": "1711380",
    "end": "1718834"
  },
  {
    "text": "and the- the- the beta i's are the coefficients which make up the linear co- uh,",
    "start": "1718834",
    "end": "1724070"
  },
  {
    "text": "combination for theta uh, uh, for theta t. And now theta t plus 1 is,",
    "start": "1724070",
    "end": "1730905"
  },
  {
    "text": "um into- into this expression, you know, expand our theta t in terms of the linear weights of uh,",
    "start": "1730905",
    "end": "1737320"
  },
  {
    "text": "uh, of phi of x^i, right? So in this place we plug in the expanded, um, linear combination of phi of x^i,",
    "start": "1737320",
    "end": "1745325"
  },
  {
    "text": "and in this place we uh, uh, plug in the linear combination of feature of x^i and then just do some algebra,",
    "start": "1745325",
    "end": "1751760"
  },
  {
    "text": "move things around and we're going to get it in the form where theta t plus 1 is still a linear combination of features of x^i,",
    "start": "1751760",
    "end": "1759365"
  },
  {
    "text": "where the next set of coefficients are calculated this way.",
    "start": "1759365",
    "end": "1763530"
  },
  {
    "text": "Right? So that completes our inductive proof that in case of linear regression,",
    "start": "1765670",
    "end": "1772700"
  },
  {
    "text": "your- the theta vectors at any stage during gradient descent can always be represented as a linear combination of our features, right?",
    "start": "1772700",
    "end": "1781720"
  },
  {
    "text": "And- any questions in this?",
    "start": "1781720",
    "end": "1784100"
  },
  {
    "text": "Yes. How about these [inaudible]",
    "start": "1788530",
    "end": "1794019"
  },
  {
    "text": "This-this- this is, yeah, you're right. So this should be j. Yeah.",
    "start": "1794020",
    "end": "1802720"
  },
  {
    "text": "Thank you. Yes, question?",
    "start": "1802720",
    "end": "1808914"
  },
  {
    "text": "Why aren't those- Why isn't the summation over there- [inaudible]",
    "start": "1808915",
    "end": "1817900"
  },
  {
    "text": "Over here? The equation over n is included, I mean, over all the examples. [OVERLAPPING]",
    "start": "1817900",
    "end": "1831170"
  },
  {
    "text": "So this one was- was sum- I took this summation and this summation,",
    "start": "1831170",
    "end": "1836270"
  },
  {
    "text": "uh, this summation and this summation common outside. So this appears just once here.",
    "start": "1836270",
    "end": "1841230"
  },
  {
    "text": "Right? So the- this summation is- is- is common for this term and this term.",
    "start": "1842140",
    "end": "1847490"
  },
  {
    "text": "So I've taken that out. Does that make sense? Or did I misunderstand your question?",
    "start": "1847490",
    "end": "1855780"
  },
  {
    "text": "Um, sorry [inaudible] So uh, [OVERLAPPING] right.",
    "start": "1856300",
    "end": "1861590"
  },
  {
    "text": "So um, uh, for each- so there is one summation over here,",
    "start": "1861590",
    "end": "1868924"
  },
  {
    "text": "and beta i appears once per summation, and over here it appears once per summation.",
    "start": "1868925",
    "end": "1874309"
  },
  {
    "text": "Right? And over here there are, you know, two nested summations. So there- here's the outer summation and here's the inner summation for this beta.",
    "start": "1874310",
    "end": "1881840"
  },
  {
    "text": "So that's the inner summation? Yeah, that's the inner summation. All right? So now we um,",
    "start": "1881840",
    "end": "1887750"
  },
  {
    "text": "[NOISE] we can write this as, you know, um, we can write the same, same thing, right, on this.",
    "start": "1887750",
    "end": "1897870"
  },
  {
    "text": "Beta t plus 1 i is equal to",
    "start": "1898150",
    "end": "1906780"
  },
  {
    "text": "beta t of i plus alpha times y^i",
    "start": "1907270",
    "end": "1917510"
  },
  {
    "text": "minus j equals 1 through n beta j of",
    "start": "1917510",
    "end": "1925835"
  },
  {
    "text": "t times phi of x^j.",
    "start": "1925835",
    "end": "1936380"
  },
  {
    "text": "Transpose phi of x^i.",
    "start": "1936380",
    "end": "1941710"
  },
  {
    "text": "All right, so basically just put this here where now the next- the beta co-efficient,",
    "start": "1941710",
    "end": "1949370"
  },
  {
    "text": "the ith beta co-efficient at time t plus 1 is equal to this over here, right?",
    "start": "1949370",
    "end": "1957815"
  },
  {
    "text": "And this, we do it for i equals 1 [NOISE] through n,",
    "start": "1957815",
    "end": "1964355"
  },
  {
    "text": "and we repeat this over and over until we converge in our beta values, right?",
    "start": "1964355",
    "end": "1972419"
  },
  {
    "text": "So this is basically gradient descent written in terms of theta space,",
    "start": "1975190",
    "end": "1981980"
  },
  {
    "text": "[NOISE] and this is the same gradient descent written as coefficients of the feature vector, right?",
    "start": "1981980",
    "end": "1991250"
  },
  {
    "text": "So um, a few things we can notice um, right away is that at first,",
    "start": "1991250",
    "end": "1998510"
  },
  {
    "text": "it might appear for each iteration, for each of the example we are summing over.",
    "start": "1998510",
    "end": "2005260"
  },
  {
    "text": "We are um, we are doing a dot product",
    "start": "2005260",
    "end": "2012085"
  },
  {
    "text": "between two high-dimensional feature vectors at each iteration for each example, right?",
    "start": "2012085",
    "end": "2019299"
  },
  {
    "text": "But however uh, the dot products between all the high-level features can be",
    "start": "2019300",
    "end": "2025120"
  },
  {
    "text": "pre-computed because the examples are not changing iteration to iteration, right? So we can pre-compute the dot product",
    "start": "2025120",
    "end": "2031720"
  },
  {
    "text": "between every pair of examples in that high dimensional feature space, and just use the pre-computed values in each iteration.",
    "start": "2031720",
    "end": "2039385"
  },
  {
    "text": "Right? So this is not changing example to example, the only thing that's changing is- are the betas, okay? All right?",
    "start": "2039385",
    "end": "2047559"
  },
  {
    "text": "And so the bBetas are changing, um, iteration to iteration.",
    "start": "2047560",
    "end": "2053215"
  },
  {
    "text": "The feature maps are constant so we can pre-compute them and use them.",
    "start": "2053215",
    "end": "2059215"
  },
  {
    "text": "So this thing, evaluates to some scalar, right? So you can pre-compute a big matrix of scalars. Yes, question?",
    "start": "2059215",
    "end": "2067149"
  },
  {
    "text": "What is the last phi, um for us [inaudible] So this is x^j and this is x^i.",
    "start": "2067150",
    "end": "2075710"
  },
  {
    "text": "Right? However, now the question is now,",
    "start": "2079470",
    "end": "2085060"
  },
  {
    "text": "what if this is infinite-dimensional? All right? We got- we got, er, a way in which we can now represent our parameters,",
    "start": "2085060",
    "end": "2094929"
  },
  {
    "text": "um, using by replacing our parameters with coefficients. Because theta in a high-dimensional feature space,",
    "start": "2094930",
    "end": "2102220"
  },
  {
    "text": "theta would also have been high-dimensional, right? So we got to way around representing",
    "start": "2102220",
    "end": "2107680"
  },
  {
    "text": "a very high dimensional parameter vector with a set of coefficients where the coefficients have",
    "start": "2107680",
    "end": "2114714"
  },
  {
    "text": "are-are-you have asked many coefficients as the number of examples which is always finite.",
    "start": "2114715",
    "end": "2119995"
  },
  {
    "text": "However, we still have a dot-product between potentially two infinite-dimensional feature vectors, right?",
    "start": "2119995",
    "end": "2127540"
  },
  {
    "text": "And that's where the concept of, a kernel comes into picture. Right? So kernel is defined as- so a kernel is some function k,",
    "start": "2127540",
    "end": "2142310"
  },
  {
    "text": "that maps- all right, two.",
    "start": "2142650",
    "end": "2149589"
  },
  {
    "text": "So script x represents the space where uh, the space where x's reside.",
    "start": "2149590",
    "end": "2156430"
  },
  {
    "text": "In this case it is, you know um, in our case, x is in RD,",
    "start": "2156430",
    "end": "2162340"
  },
  {
    "text": "which means script x equals RD. So it takes two examples and maps it to a real number.",
    "start": "2162340",
    "end": "2174010"
  },
  {
    "text": "Right? So this is the definition of a kernel, and it does it in such a way such that it satisfies k of x comma z is equal",
    "start": "2174010",
    "end": "2187540"
  },
  {
    "text": "to the inner product",
    "start": "2187540",
    "end": "2193060"
  },
  {
    "text": "between phi of x comma phi of z,",
    "start": "2193060",
    "end": "2198505"
  },
  {
    "text": "where phi is some feature vector. Right? So um, the kernel corresponding to a feature vector phi is defined as",
    "start": "2198505",
    "end": "2209830"
  },
  {
    "text": "a function where- the value of the function is equal",
    "start": "2209830",
    "end": "2216640"
  },
  {
    "text": "to the inner product between the feature map of x dot product with the feature map of z.",
    "start": "2216640",
    "end": "2223450"
  },
  {
    "text": "So this is the same as phi of x transpose phi of z,",
    "start": "2223450",
    "end": "2229385"
  },
  {
    "text": "right? Right? Yes. Question? [inaudible]",
    "start": "2229385",
    "end": "2241235"
  },
  {
    "text": "Can you- can you please repeat the question? This line? The line above it?",
    "start": "2241235",
    "end": "2250680"
  },
  {
    "text": "RD. So, so. This is defining the space. So x is in script x.",
    "start": "2251500",
    "end": "2260099"
  },
  {
    "text": "Right? In this case, the script x happens to be RD, but in general, this need not be in RD.",
    "start": "2260650",
    "end": "2268100"
  },
  {
    "text": "It could even be like strings, the set of all strings or set of all graphs, right? It's an abstract space.",
    "start": "2268100",
    "end": "2274955"
  },
  {
    "text": "And the kernel is a function, which takes two elements of that space and returns a real valued scalar.",
    "start": "2274955",
    "end": "2285215"
  },
  {
    "text": "And it also satisfies the property that the evaluated value can be expressed as the inner product",
    "start": "2285215",
    "end": "2295355"
  },
  {
    "text": "between two feature maps or between",
    "start": "2295355",
    "end": "2300060"
  },
  {
    "text": "the feature map of input one dot-product with the feature map of input two, right?",
    "start": "2301180",
    "end": "2310535"
  },
  {
    "text": "And for the example that we saw here, okay?",
    "start": "2310535",
    "end": "2316789"
  },
  {
    "text": "So for the example where phi of x is equal to the set of all monomials up to order 3x,",
    "start": "2316790",
    "end": "2324245"
  },
  {
    "text": "x1, x2, et cetera, x1 cube, right?",
    "start": "2324245",
    "end": "2331760"
  },
  {
    "text": "Where, so this feature vector has a corresponding kernel.",
    "start": "2331760",
    "end": "2340535"
  },
  {
    "text": "So k of x comma z is equal to 1 plus inner product of x comma z",
    "start": "2340535",
    "end": "2350809"
  },
  {
    "text": "[NOISE] plus inner product of x comma",
    "start": "2350810",
    "end": "2356840"
  },
  {
    "text": "z squared plus inner product of x comma zq.",
    "start": "2356840",
    "end": "2365130"
  },
  {
    "text": "This is exactly the same as phi transpose phi of z. So this is one example of a kernel, right?",
    "start": "2367390",
    "end": "2377780"
  },
  {
    "text": "Where for this particular feature map which had, you know, if x was in rd,",
    "start": "2377780",
    "end": "2386859"
  },
  {
    "text": "then p is approximately order dq, right?",
    "start": "2386860",
    "end": "2394205"
  },
  {
    "text": "And in order to perform an inner product or the outer product in this high dimensional feature space would require harder dq number of operations, right?",
    "start": "2394205",
    "end": "2405935"
  },
  {
    "text": "Whereas over here, in the kernel form, we see that this takes order d,",
    "start": "2405935",
    "end": "2412970"
  },
  {
    "text": "this takes order d, and this takes order d, right?",
    "start": "2412970",
    "end": "2418205"
  },
  {
    "text": "And you sum over three such order d operations and you're still in order d. Right?",
    "start": "2418205",
    "end": "2424970"
  },
  {
    "text": "So this is a computational trick where a high dimensional feature map can be,",
    "start": "2424970",
    "end": "2434345"
  },
  {
    "text": "or the dot-product between two high-dimensional feature maps can be compactly represented by a simple,",
    "start": "2434345",
    "end": "2443315"
  },
  {
    "text": "most straightforward function like this. Yes, question? [inaudible]",
    "start": "2443315",
    "end": "2452870"
  },
  {
    "text": "Why does this hold? Yeah. [inaudible] This one? Yeah. So, there is details of that in the notes.",
    "start": "2452870",
    "end": "2458990"
  },
  {
    "text": "It's algebra, you just work through it and you can see that these two are actually exactly the same, right?",
    "start": "2458990",
    "end": "2465440"
  },
  {
    "text": "I could give you some intuitions, for example, right? So one x1, x2, til xd.",
    "start": "2465440",
    "end": "2474995"
  },
  {
    "text": "And similarly, here you have say, z1, z1, z2, til zd.",
    "start": "2474995",
    "end": "2483800"
  },
  {
    "text": "You have x1 squared, z1 squared, and so on. So if you take the dot product between these two,",
    "start": "2483800",
    "end": "2492920"
  },
  {
    "text": "you get 1 plus x1z1 plus",
    "start": "2492920",
    "end": "2501200"
  },
  {
    "text": "x2z2 plus so on until xdzd plus x1 squared,",
    "start": "2501200",
    "end": "2510155"
  },
  {
    "text": "z1 squared, and so on. And then you can group the terms and each of those will correspond to one of them.",
    "start": "2510155",
    "end": "2516140"
  },
  {
    "text": "[inaudible]",
    "start": "2516140",
    "end": "2527869"
  },
  {
    "text": "Yeah, so the kernel function is defined such that,",
    "start": "2527870",
    "end": "2534800"
  },
  {
    "text": "the functional form of the kernel can",
    "start": "2534800",
    "end": "2541685"
  },
  {
    "text": "always be rewritten as and in a dot product between some feature map of x and some feature map of z,",
    "start": "2541685",
    "end": "2548915"
  },
  {
    "text": "where you apply the same feature map for both the inputs, and then take a dot product in that higher dimensional feature space, right?",
    "start": "2548915",
    "end": "2556130"
  },
  {
    "text": "And this way an order d cube operation was reduced to an order d operation. Yes question?",
    "start": "2556130",
    "end": "2568720"
  },
  {
    "text": "[inaudible]",
    "start": "2568720",
    "end": "2582760"
  },
  {
    "text": "So the script x is. First, it's important to know that the script x is",
    "start": "2582760",
    "end": "2589550"
  },
  {
    "text": "the input to the kernel or the input of the feature map, right? So that is not the high dimensional space.",
    "start": "2589550",
    "end": "2595460"
  },
  {
    "text": "Script x is not the high dimensional space where we want to do the dot product. Script x is the space over which we want,",
    "start": "2595460",
    "end": "2602915"
  },
  {
    "text": "over which we are trying to learn, so that our examples reside in script x, right?",
    "start": "2602915",
    "end": "2608795"
  },
  {
    "text": "And the feature map takes something in script.",
    "start": "2608795",
    "end": "2615510"
  },
  {
    "text": "So, the feature map maps script x to rp,",
    "start": "2615640",
    "end": "2622190"
  },
  {
    "text": "where p is potentially an infinite dimensional space. Right? And the claim here is",
    "start": "2622190",
    "end": "2630530"
  },
  {
    "text": "that instead of performing inner products in high dimensional spaces.",
    "start": "2630530",
    "end": "2638270"
  },
  {
    "text": "There would, for a feature phi can have a corresponding kernel k,",
    "start": "2638270",
    "end": "2647330"
  },
  {
    "text": "where the kernel function has a more compact representation, which is equivalent to performing a dot product in the high dimensional space.",
    "start": "2647330",
    "end": "2659190"
  },
  {
    "text": "Okay. So- so the question is, how does- how does the kernel reduce the number of operations?",
    "start": "2666360",
    "end": "2672100"
  },
  {
    "text": "Uh, the- the way it reduces the operation is so mathematically we",
    "start": "2672100",
    "end": "2677110"
  },
  {
    "text": "can see that this representation and this representation are the same, right? But computationally in this rep- in- in this representation,",
    "start": "2677110",
    "end": "2685750"
  },
  {
    "text": "we require order d cube operations. Whereas to do this,",
    "start": "2685750",
    "end": "2691075"
  },
  {
    "text": "this requires order d operations, order d operations, order d operations. You get a scalar and then you do square cube and sum them up,",
    "start": "2691075",
    "end": "2697210"
  },
  {
    "text": "which are uh, negligible, right? So computationally this requires order d,",
    "start": "2697210",
    "end": "2702535"
  },
  {
    "text": "whereas this requires order d cube. Yes, question? So that kernel that you wrote is always specific to that?",
    "start": "2702535",
    "end": "2710650"
  },
  {
    "text": "Feature map. Yes. So the kernel that uh- that uh, we wrote here is specific to this feature map, exactly.",
    "start": "2710650",
    "end": "2717579"
  },
  {
    "text": "And we are gonna discuss about, you know, kernels in general next. So this is just a motivating example where, uh,",
    "start": "2717580",
    "end": "2724734"
  },
  {
    "text": "instead of performing operations in, you know, if d is 1000, instead of doing a dot product with, you know,",
    "start": "2724735",
    "end": "2731230"
  },
  {
    "text": "2 billion dimensional vectors, instead, we can do something with just, you know, 1,000 dimensional vectors.",
    "start": "2731230",
    "end": "2736690"
  },
  {
    "text": "It's like a million times faster, all right?",
    "start": "2736690",
    "end": "2739310"
  },
  {
    "text": "So now if we replace the, uh,",
    "start": "2744540",
    "end": "2749665"
  },
  {
    "text": "the inner product between these two feature, uh, uh vectors with the kernel,",
    "start": "2749665",
    "end": "2755275"
  },
  {
    "text": "we end up with an algorithm for linear regression like this, right?",
    "start": "2755275",
    "end": "2762520"
  },
  {
    "text": "So this is linear regression kernelized.",
    "start": "2762520",
    "end": "2773210"
  },
  {
    "text": "Now the first thing is pre-compute a matrix k,",
    "start": "2777480",
    "end": "2788140"
  },
  {
    "text": "where K_ij is equal to K of x_i, x_j, right?",
    "start": "2788140",
    "end": "2799750"
  },
  {
    "text": "And it is common in literature to abuse notation and use the same letter K for",
    "start": "2799750",
    "end": "2806620"
  },
  {
    "text": "both the matrix of- for- for the kernel function and also what is called as the kernel matrix,",
    "start": "2806620",
    "end": "2813970"
  },
  {
    "text": "which is a square symmetric matrix of all the kernel- the kernel is evaluated between every pair of your examples, right?",
    "start": "2813970",
    "end": "2825325"
  },
  {
    "text": "And this is equal to phi of",
    "start": "2825325",
    "end": "2831070"
  },
  {
    "text": "x_i, phi of x_j, right?",
    "start": "2831070",
    "end": "2839800"
  },
  {
    "text": "So pre-compute that, and then do a loop, er,",
    "start": "2839800",
    "end": "2846760"
  },
  {
    "text": "for all i in 1 through n. Beta i of",
    "start": "2846760",
    "end": "2856660"
  },
  {
    "text": "t plus 1 equals beta i of t plus Alpha",
    "start": "2856660",
    "end": "2864609"
  },
  {
    "text": "times y_i minus loop over j equals 1 through n,",
    "start": "2864610",
    "end": "2874480"
  },
  {
    "text": "beta j of t times K_",
    "start": "2874480",
    "end": "2880975"
  },
  {
    "text": "ij Beta j of t times K_ ij, yeah.",
    "start": "2880975",
    "end": "2890900"
  },
  {
    "text": "All right? So pre-compute all the- all the- all the,",
    "start": "2891900",
    "end": "2900700"
  },
  {
    "text": "uh, n squared possible inner products and construct what is called as a kernel matrix.",
    "start": "2900700",
    "end": "2906714"
  },
  {
    "text": "And once we pre-compute it. Even though phis are- are- in order to pre-compute it.",
    "start": "2906715",
    "end": "2915085"
  },
  {
    "text": "Even though phis might live in like a billion dimensional vectors by using the kernel form,",
    "start": "2915085",
    "end": "2920425"
  },
  {
    "text": "the corresponding kernel form for that feature vector, we will, uh, uh,",
    "start": "2920425",
    "end": "2926020"
  },
  {
    "text": "we can compute it- compute each element effectively in just order, uh, uh, order d instead of order d cube for this kind of a feature map.",
    "start": "2926020",
    "end": "2934210"
  },
  {
    "text": "And then we iterate over this step where we are updating the corresponding coefficients,",
    "start": "2934210",
    "end": "2942714"
  },
  {
    "text": "where beta coefficients from step to step in- in the following way. And this is taken directly from here,",
    "start": "2942715",
    "end": "2949495"
  },
  {
    "text": "where we replace this inner product with K_ij of the kernel matrix, right?",
    "start": "2949495",
    "end": "2954110"
  },
  {
    "text": "And we can also compactly write this as beta t",
    "start": "2957450",
    "end": "2968140"
  },
  {
    "text": "plus 1 equals beta t plus Alpha",
    "start": "2968140",
    "end": "2975970"
  },
  {
    "text": "times the vector y minus K beta t. So this is,",
    "start": "2975970",
    "end": "2984415"
  },
  {
    "text": "uh, these two are equivalent, but instead of doing it for all i, you can perform it in- in, uh, in one iteration in a vectorized format, there is a question?",
    "start": "2984415",
    "end": "2993040"
  },
  {
    "text": "[inaudible]",
    "start": "2993040",
    "end": "3001800"
  },
  {
    "text": "So I'm- I'm gonna come to that. So the question is, what if we don't have a few terms,",
    "start": "3001800",
    "end": "3006975"
  },
  {
    "text": "uh, um, in this feature map, then this is not, uh, uh, then this will not be a- the kernel corresponding to this feature map, that is true.",
    "start": "3006975",
    "end": "3014790"
  },
  {
    "text": "And we'll go into feature maps, uh, in a moment right after we wrap up this example, right?",
    "start": "3014790",
    "end": "3021045"
  },
  {
    "text": "So this is for learning [NOISE] and for prediction time,",
    "start": "3021045",
    "end": "3030700"
  },
  {
    "text": "um, for prediction, so the, uh,",
    "start": "3030980",
    "end": "3038475"
  },
  {
    "text": "we have h theta of x is equal to theta transpose- theta transpose phi of x.",
    "start": "3038475",
    "end": "3049770"
  },
  {
    "text": "And again, we expand out theta as i equals 1 to n",
    "start": "3049770",
    "end": "3059025"
  },
  {
    "text": "beta times phi of",
    "start": "3059025",
    "end": "3064829"
  },
  {
    "text": "X transpose phi of X,",
    "start": "3064830",
    "end": "3073900"
  },
  {
    "text": "i equals 1 to n beta i, K of x_i,",
    "start": "3074090",
    "end": "3083790"
  },
  {
    "text": "x where x is the test example, right?",
    "start": "3083790",
    "end": "3095020"
  },
  {
    "text": "So the, um, in- in- in case of the linear regression,",
    "start": "3095780",
    "end": "3102749"
  },
  {
    "text": "without feature maps or without, uh, uh, kernels, we would have ended up with some theta vector,",
    "start": "3102749",
    "end": "3108825"
  },
  {
    "text": "some high-dimensional theta vector. And this could be potentially infinite dimensional.",
    "start": "3108825",
    "end": "3114450"
  },
  {
    "text": "That's why we use our- our, um, coefficient format to rewrite theta.",
    "start": "3114450",
    "end": "3120450"
  },
  {
    "text": "So this is theta. Theta is- we rewrite theta as a linear combination of",
    "start": "3120450",
    "end": "3131940"
  },
  {
    "text": "our feature vectors where the corresponding coefficients were learned in- in through this algorithm, right?",
    "start": "3131940",
    "end": "3139049"
  },
  {
    "text": "And at prediction time, we evaluate the kernel between",
    "start": "3139050",
    "end": "3144194"
  },
  {
    "text": "our test example and every example in our training set, right?",
    "start": "3144195",
    "end": "3149880"
  },
  {
    "text": "Multiplied by the corresponding, um, um beta coefficients.",
    "start": "3149880",
    "end": "3155669"
  },
  {
    "text": "And this is gonna be our prediction, right? So with this, let's make a few observations.",
    "start": "3155669",
    "end": "3165900"
  },
  {
    "text": "[NOISE] So any questions on this?",
    "start": "3165900",
    "end": "3173950"
  },
  {
    "text": "So let's make a few observations.",
    "start": "3175850",
    "end": "3179260"
  },
  {
    "text": "So in our observations um, first we see that in our training rule, right, um,",
    "start": "3214110",
    "end": "3224210"
  },
  {
    "text": "so the training rule was beta equals beta plus alpha,",
    "start": "3225390",
    "end": "3233769"
  },
  {
    "text": "times y minus k. That's the kernel matrix of,",
    "start": "3233770",
    "end": "3240625"
  },
  {
    "text": "of beta and at test time, where y hat equals",
    "start": "3240625",
    "end": "3254080"
  },
  {
    "text": "nk of x^i, x, right?",
    "start": "3254080",
    "end": "3262360"
  },
  {
    "text": "In both of these, phi of x does not appear, right?",
    "start": "3262360",
    "end": "3273880"
  },
  {
    "text": "So that's the most important thing, right? We have rewritten our algorithms in such a way that the high-dimensional feature map,",
    "start": "3273880",
    "end": "3282370"
  },
  {
    "text": "phi of x, does not appear either during training or during testing, and that's the most important thing, because if we have phi of x",
    "start": "3282370",
    "end": "3288760"
  },
  {
    "text": "explicitly occurring in any of our update rules, then we would have to compute that, and that could be infinite-dimensional, right?",
    "start": "3288760",
    "end": "3295540"
  },
  {
    "text": "So we got rid of phi of x completely, right? That's the most important observation.",
    "start": "3295540",
    "end": "3302330"
  },
  {
    "text": "The second observation is that for test-time. Yes, question?",
    "start": "3302490",
    "end": "3309280"
  },
  {
    "text": "[inaudible]",
    "start": "3309280",
    "end": "3318430"
  },
  {
    "text": "So the question is, um, if phi was infinite dimensional,",
    "start": "3318430",
    "end": "3325345"
  },
  {
    "text": "then will the kernel also have an infinite sum? And the answer is, not always,",
    "start": "3325345",
    "end": "3333130"
  },
  {
    "text": "there are many kernels that we're gonna see next, where even though you have an infinite-dimensionally long feature vector,",
    "start": "3333130",
    "end": "3343135"
  },
  {
    "text": "your kernel can always be evaluated at- sometimes even constant time, right?",
    "start": "3343135",
    "end": "3348670"
  },
  {
    "text": "And we're gonna see a few examples. The whole idea is that the explicit form representation of",
    "start": "3348670",
    "end": "3355480"
  },
  {
    "text": "the feature map can be computationally very expensive and there will",
    "start": "3355480",
    "end": "3360970"
  },
  {
    "text": "sometimes be this more compact kernelized form where this explicit form can be computed in a more efficient way, okay? Yes, question?",
    "start": "3360970",
    "end": "3371260"
  },
  {
    "text": "[inaudible] Uh, of- of this equation? Update rule?",
    "start": "3371260",
    "end": "3381390"
  },
  {
    "text": "Yeah. So in the update rule- um, so this is just, um, so kij represents the- the, um, um,",
    "start": "3381390",
    "end": "3389280"
  },
  {
    "text": "ij element of the matrix, and that should be a scalar. And the kernel always computes into a scalar.",
    "start": "3389280",
    "end": "3396278"
  },
  {
    "text": "The kernel always computes into a scalar, right? And for beta- so beta is a vector,",
    "start": "3396279",
    "end": "3404349"
  },
  {
    "text": "so beta of t is an Rn, right?",
    "start": "3404350",
    "end": "3411880"
  },
  {
    "text": "So beta t plus 1 are the- the jth beta at step t plus 1 is equal to the old beta plus alpha, again scalar.",
    "start": "3411880",
    "end": "3422065"
  },
  {
    "text": "So scalar, scalar times something, yi is a scalar and kij are scalars,",
    "start": "3422065",
    "end": "3430030"
  },
  {
    "text": "and you're summing over some scalar times some scalar, so this whole thing is a scalar, right? So-.",
    "start": "3430030",
    "end": "3438430"
  },
  {
    "text": "[inaudible] I'm sorry? [inaudible] So j is just the index you, you are summing over.",
    "start": "3438430",
    "end": "3445000"
  },
  {
    "text": "[inaudible] Yeah, so beta- beta j is a scalar,",
    "start": "3445000",
    "end": "3451000"
  },
  {
    "text": "so beta t is an Rn. So beta t subscript b is, is a scalar, all right?",
    "start": "3451000",
    "end": "3457060"
  },
  {
    "text": "So this whole thing is a scalar, scalar minus a scalar times a scalar, plus a scalar equals a scalar, right?",
    "start": "3457060",
    "end": "3463480"
  },
  {
    "text": "And similarly over here, this is a vector in Rn, Rn. Y is in Rn.",
    "start": "3463480",
    "end": "3469885"
  },
  {
    "text": "K is n by n, beta t is Rn, so n by n times Rn will give you n. So there's an n vector,",
    "start": "3469885",
    "end": "3478225"
  },
  {
    "text": "n vector, difference is an n vector, times a scalar is an n vector plus an n vector equals n vector.",
    "start": "3478225",
    "end": "3484670"
  },
  {
    "text": "So back to our observations. So the first thing we observed is that we have eliminated phi of x completely, right?",
    "start": "3491150",
    "end": "3499049"
  },
  {
    "text": "That's, that's the, uh, most important, um, most important step. The, uh, second observation that we make is to make a prediction,",
    "start": "3499050",
    "end": "3509674"
  },
  {
    "text": "we need to remember all our test examples, right?",
    "start": "3509674",
    "end": "3516145"
  },
  {
    "text": "So for prediction, we need",
    "start": "3516145",
    "end": "3525879"
  },
  {
    "text": "training examples to be",
    "start": "3525879",
    "end": "3534680"
  },
  {
    "text": "stored in memory, okay?",
    "start": "3534680",
    "end": "3541990"
  },
  {
    "text": "And this is probably the most distinctive feature from linear regression that we've seen.",
    "start": "3542130",
    "end": "3548394"
  },
  {
    "text": "In linear regression, we started with a training, training set, with x's and y's.",
    "start": "3548395",
    "end": "3554455"
  },
  {
    "text": "We learned the theta vector by performing, say, the normal equations or gradient descent,",
    "start": "3554455",
    "end": "3560125"
  },
  {
    "text": "and once we obtain the theta vector, we could discard our entire training set and only",
    "start": "3560125",
    "end": "3565300"
  },
  {
    "text": "carry forward theta- the theta vector from that point on. And to perform a prediction on a new example,",
    "start": "3565300",
    "end": "3570970"
  },
  {
    "text": "all you needed was the theta vector. You didn't need the training sets anymore, right? But with kernel methods, that's not true anymore, right?",
    "start": "3570970",
    "end": "3578140"
  },
  {
    "text": "By giving up the, uh, phi of x representation- explicit phi of x representation,",
    "start": "3578140",
    "end": "3584440"
  },
  {
    "text": "we've also given up the possibility of having a theta vector that we could- that alone we could carry forward, right?",
    "start": "3584440",
    "end": "3591055"
  },
  {
    "text": "In place of that, we need to remember all our training examples.",
    "start": "3591055",
    "end": "3596839"
  },
  {
    "text": "I missed a beta i here. We -um, we need to remember all our training examples and when we get a new test example,",
    "start": "3597210",
    "end": "3607630"
  },
  {
    "text": "we evaluate the kernel with every training example and take a linear combination of those,",
    "start": "3607630",
    "end": "3615550"
  },
  {
    "text": "uh, uh, kernel outputs weighted by the- uh, the beta vector that we learned, right?",
    "start": "3615550",
    "end": "3623380"
  },
  {
    "text": "So training examples must be carried forward as is in to test-time with kernel methods.",
    "start": "3623380",
    "end": "3630829"
  },
  {
    "text": "Any, any questions on this? And to- to kind of better understand,",
    "start": "3631830",
    "end": "3644664"
  },
  {
    "text": "uh, what was happening, so let's assume that in linear regression we were given a matrix.",
    "start": "3644664",
    "end": "3651010"
  },
  {
    "text": "Let's go- so this was the design matrix x, where each row was an example.",
    "start": "3651010",
    "end": "3658420"
  },
  {
    "text": "Let's call this x, uh, x1 to xn,",
    "start": "3658420",
    "end": "3669370"
  },
  {
    "text": "and we had y, right?",
    "start": "3669370",
    "end": "3676195"
  },
  {
    "text": "And in linear regression, we wanted to learn a theta vector, right?",
    "start": "3676195",
    "end": "3682130"
  },
  {
    "text": "So if x was in rd, theta was also in rd, right?",
    "start": "3684570",
    "end": "3694075"
  },
  {
    "text": "And when we started gradient descent, we start with some theta 0 and then perform a gradient update and get a theta 1, right?",
    "start": "3694075",
    "end": "3704980"
  },
  {
    "text": "And then perform an update and get theta 2 and so on,",
    "start": "3704980",
    "end": "3711070"
  },
  {
    "text": "until the theta vectors converge, right? But with kernel methods,",
    "start": "3711070",
    "end": "3716590"
  },
  {
    "text": "we observe the fact that the thetas were always a linear combination of our x vectors, right?",
    "start": "3716590",
    "end": "3725799"
  },
  {
    "text": "That's, that's the observation that we made, that theta, um, that theta at any, any step in gradient descent could always be",
    "start": "3725800",
    "end": "3733420"
  },
  {
    "text": "represented as some linear combination of our vectors, right? And we instead stored beta 0,",
    "start": "3733420",
    "end": "3745990"
  },
  {
    "text": "where we had one beta for example, right? And each theta, so say this was beta 0, and using our,",
    "start": "3745990",
    "end": "3758970"
  },
  {
    "text": "our algorithm over here, we would get beta 1, right?",
    "start": "3758970",
    "end": "3770820"
  },
  {
    "text": "Then we get beta 2, and so on,",
    "start": "3770820",
    "end": "3778855"
  },
  {
    "text": "where each of the betas were in Rn,",
    "start": "3778855",
    "end": "3786070"
  },
  {
    "text": "whereas theta was in Rd, right? Thetas had one component per feature.",
    "start": "3786070",
    "end": "3796195"
  },
  {
    "text": "Betas have one component per example, right? And every beta vector has a corresponding theta vector,",
    "start": "3796195",
    "end": "3805240"
  },
  {
    "text": "where theta vector is- so theta of t is equal to sum over i,",
    "start": "3805240",
    "end": "3815770"
  },
  {
    "text": "equals 1 to n beta of ti x of i, right?",
    "start": "3815770",
    "end": "3825760"
  },
  {
    "text": "And this holds true even if we- so we use this fact to,",
    "start": "3825760",
    "end": "3835225"
  },
  {
    "text": "to kind of extend our x into a, a high dimension.",
    "start": "3835225",
    "end": "3843625"
  },
  {
    "text": "So supposing, you know, we map this into phi of x, which is in Rp, right?",
    "start": "3843625",
    "end": "3851965"
  },
  {
    "text": "And now what used to be a d-dimensional vector is now a p-dimensional vector, right?",
    "start": "3851965",
    "end": "3860160"
  },
  {
    "text": "And this, we could- we could- we could have map x to- we map x into some phi of x, and this phi of x could have been infinitely- infinite dimensionally wide, right?",
    "start": "3860160",
    "end": "3870055"
  },
  {
    "text": "And no matter how big our feature vectors are, the beta vectors are, are gonna be of fixed length, 1, for example, right?",
    "start": "3870055",
    "end": "3878575"
  },
  {
    "text": "And this is what allows us to scale in terms of features and we just",
    "start": "3878575",
    "end": "3886510"
  },
  {
    "text": "remember the corresponding beta vectors by which we weight our examples.",
    "start": "3886510",
    "end": "3891110"
  },
  {
    "text": "Any, any questions on this? Yes, question?",
    "start": "3892830",
    "end": "3898300"
  },
  {
    "text": "[inaudible]",
    "start": "3898300",
    "end": "3921470"
  },
  {
    "text": "Yeah, so the question is-is it really practical to remember all-all um,",
    "start": "3921470",
    "end": "3926795"
  },
  {
    "text": "our training examples into test-time? And the answer is, in general, yes,",
    "start": "3926795",
    "end": "3933770"
  },
  {
    "text": "you need to do that, and um, that makes kernel methods not very scalable as you get lots and lots of data.",
    "start": "3933770",
    "end": "3940279"
  },
  {
    "text": "Which is why, you know, in-in practice, you see when you have lots and lots of data, you see methods like neural networks and deep learning take over",
    "start": "3940280",
    "end": "3947329"
  },
  {
    "text": "because they don't have this limitation that you need to remember all your examples, all your training examples. But at the same time, there are algorithms that we're going to see,",
    "start": "3947330",
    "end": "3955550"
  },
  {
    "text": "in fact later today uh, called the Support Vector Machine, where the support vector machine will result in beta vectors that are very sparse,",
    "start": "3955550",
    "end": "3963950"
  },
  {
    "text": "which means most of them are zeros. So you just need to remember a few of your examples,",
    "start": "3963950",
    "end": "3969349"
  },
  {
    "text": "and both are called support vectors. We will-will be going to support vectors,",
    "start": "3969350",
    "end": "3975349"
  },
  {
    "text": "but in general, your beta vectors can be dense, which means you need to remember all your training examples all the way to test time, and in fact,",
    "start": "3975350",
    "end": "3983119"
  },
  {
    "text": "that is a big obstacle for scaling your algorithms to big data sets.",
    "start": "3983120",
    "end": "3988280"
  },
  {
    "text": "And that does come into-come, come into, come into play uh, uh, if you want to use kernel methods.",
    "start": "3988280",
    "end": "3994345"
  },
  {
    "text": "That's right, yeah, good question.",
    "start": "3994345",
    "end": "3997040"
  },
  {
    "text": "Any other questions? So let's look at some properties of kernels, okay?",
    "start": "3999510",
    "end": "4005215"
  },
  {
    "text": "So what we saw so far was a way um, was-was an example of an algorithm which was linear regression that we kernelized right?",
    "start": "4005215",
    "end": "4014455"
  },
  {
    "text": "And Linear regression is just one algorithm that you can kernelize. You can kernelize a lot of algorithms and-and in fact,",
    "start": "4014455",
    "end": "4025135"
  },
  {
    "text": "you can follow the same steps and you can even can kernelize generalized linear models,",
    "start": "4025135",
    "end": "4031195"
  },
  {
    "text": "which means in a logistic regression or Poisson regression, they can also be kernelized where you,",
    "start": "4031195",
    "end": "4037340"
  },
  {
    "text": "you replace this with some G function. Okay? And um, you know by-by having a G function, for example,",
    "start": "4038520",
    "end": "4048550"
  },
  {
    "text": "you know the sigmoid function or the logistic function um, and following the same recipe, you, we can kernelize any generalized linear model algorithm as well.",
    "start": "4048550",
    "end": "4058270"
  },
  {
    "text": "And there are lots of other algorithms that can also be kernelized. And in fact, the question of",
    "start": "4058270",
    "end": "4066055"
  },
  {
    "text": "our kernel methods for classification or regression, no, wrong question. You know, you can-you can um,",
    "start": "4066055",
    "end": "4071950"
  },
  {
    "text": "kernelize lots of different kinds of algorithms are kernel-kernel methods for discriminative or generative models?",
    "start": "4071950",
    "end": "4078609"
  },
  {
    "text": "The answer is they can work for both you can kernelize generative models, you can kernelize discriminative models, are",
    "start": "4078610",
    "end": "4085000"
  },
  {
    "text": "kernel methods for supervised learning or unsupervised learning? They can work for both. You know, you can kernelize supervised learning algorithms,",
    "start": "4085000",
    "end": "4091090"
  },
  {
    "text": "you can ah, kernelize unsupervised learning algorithms. So kernel-this kernelization or what's called as the Kernel trick,",
    "start": "4091090",
    "end": "4099279"
  },
  {
    "text": "is a very general technique that can be applied in lots of different places. And let's look at",
    "start": "4099280",
    "end": "4106045"
  },
  {
    "text": "a few properties of kernels [BACKGROUND].",
    "start": "4106045",
    "end": "4134799"
  },
  {
    "text": "Kernel examples, so example.",
    "start": "4134800",
    "end": "4140005"
  },
  {
    "text": "Example A, if k of x comma z",
    "start": "4140005",
    "end": "4147250"
  },
  {
    "text": "equals x transpose z squared.",
    "start": "4147250",
    "end": "4155720"
  },
  {
    "text": "And this has the corresponding phi of x equals x1,",
    "start": "4160260",
    "end": "4171460"
  },
  {
    "text": "x2, x1, x1, x1, x2,",
    "start": "4171460",
    "end": "4176588"
  },
  {
    "text": "x D x D",
    "start": "4176589",
    "end": "4181670"
  },
  {
    "text": "if x is in R D. Okay,",
    "start": "4181670",
    "end": "4187239"
  },
  {
    "text": "so this is one example. In fact, we saw a more general example um, uh, previously.",
    "start": "4187240",
    "end": "4192685"
  },
  {
    "text": "Similarly you can have Example B. Uh, I'm just-I'm just writing out a few examples that are in your notes.",
    "start": "4192685",
    "end": "4201320"
  },
  {
    "text": "Um, if k of x comma z equals x transpose z plus c square.",
    "start": "4201390",
    "end": "4210850"
  },
  {
    "text": "So I take the dot product, add a constant and then square it. That can also be uh, that's also a kernel um,",
    "start": "4210850",
    "end": "4217330"
  },
  {
    "text": "which corresponds to a feature vector that looks like this. phi of x equals x1 square, x1, x2,",
    "start": "4217330",
    "end": "4228684"
  },
  {
    "text": "and then you have square root of 2C times x1,",
    "start": "4228684",
    "end": "4235555"
  },
  {
    "text": "square root of 2C times x2, and so on, all right? And it might not be obvious uh, at first,",
    "start": "4235555",
    "end": "4249690"
  },
  {
    "text": "you know, that looking at this form, it's not directly obvious whether you can represent this",
    "start": "4249690",
    "end": "4257190"
  },
  {
    "text": "as a dot product between two feature vectors. But if you work through it, you can come up with a feature representation.",
    "start": "4257190",
    "end": "4264400"
  },
  {
    "text": "And similarly, uh, when you uh, when you're given a feature representation, it's not always obvious whether you can actually, you know,",
    "start": "4264400",
    "end": "4271449"
  },
  {
    "text": "save some computation by coming up with a more compact representation. And in fact, for many years,",
    "start": "4271450",
    "end": "4277435"
  },
  {
    "text": "this was like an active area of research where, you know, researchers would come up with new kernels, which are, you know,",
    "start": "4277435",
    "end": "4283060"
  },
  {
    "text": "which had nice properties and so forth. And uh, also one would imagine",
    "start": "4283060",
    "end": "4290770"
  },
  {
    "text": "that the-the order in which we presented kernels was that first uh,",
    "start": "4290770",
    "end": "4296665"
  },
  {
    "text": "we-first we came up with a feature vector and",
    "start": "4296665",
    "end": "4302035"
  },
  {
    "text": "then we saw that the feature vector could have been represented as a more compact kernel.",
    "start": "4302035",
    "end": "4307945"
  },
  {
    "text": "But in practice, um, the way things go-go about in research,",
    "start": "4307945",
    "end": "4313300"
  },
  {
    "text": "in reality is that people come up with a function k equals, you know, some form.",
    "start": "4313300",
    "end": "4319885"
  },
  {
    "text": "You know k of x comma z equals something that involves x and z and whatnot.",
    "start": "4319885",
    "end": "4326199"
  },
  {
    "text": "And then tried to, you know, convince themselves that you can come up with some feature vector for it.",
    "start": "4326200",
    "end": "4331599"
  },
  {
    "text": "In-in-in um, reality people come up with functions and try to prove that it's a kernel rather than starting with",
    "start": "4331600",
    "end": "4338320"
  },
  {
    "text": "a feature map and see if there's a kernel corresponding to it. Right? And kernels can also be seen as similarity metrics.",
    "start": "4338320",
    "end": "4348160"
  },
  {
    "text": "Was there a question? Yes question. So the x1, x1,",
    "start": "4348160",
    "end": "4354730"
  },
  {
    "text": "x1, x2, x1, x3, and so on, then x2 x1, x2, x2, x2, x3.",
    "start": "4354730",
    "end": "4359630"
  },
  {
    "text": "So this is a feature map, a feature map can work only on one input, right?",
    "start": "4365910",
    "end": "4372985"
  },
  {
    "text": "So it's-it's important to note that when we write k of x comma",
    "start": "4372985",
    "end": "4378340"
  },
  {
    "text": "z equals phi x transpose phi of z.",
    "start": "4378340",
    "end": "4384159"
  },
  {
    "text": "The phi, the feature map must only involve terms from one input.",
    "start": "4384160",
    "end": "4391150"
  },
  {
    "text": "So [inaudible]",
    "start": "4391150",
    "end": "4410395"
  },
  {
    "text": "So, um, so that would be, you know,",
    "start": "4410395",
    "end": "4415825"
  },
  {
    "text": "x_1, z_1 plus x_2, z_2, and so on, whole squared, right?",
    "start": "4415825",
    "end": "4423010"
  },
  {
    "text": "And this would be x_1 squared z_1 square plus x_2 squared,",
    "start": "4423010",
    "end": "4431349"
  },
  {
    "text": "z_2 squared plus cross terms, right? And this is x_1, x_1.",
    "start": "4431350",
    "end": "4436929"
  },
  {
    "text": "And for Phi of z is going to be z1, z1. And when you do the dot product, you're going to get this term.",
    "start": "4436930",
    "end": "4442610"
  },
  {
    "text": "Right. So kernels can be seen as similarity metrics, which means if you can, um, so, ah,",
    "start": "4445500",
    "end": "4453430"
  },
  {
    "text": "kernels are generally constructed such that similar examples evaluate to a higher value in the kernel",
    "start": "4453430",
    "end": "4460000"
  },
  {
    "text": "and dissimilar examples evaluate to a small value in the kernel, right? And this idea of using kernels as similarity metrics will- will actually show up,",
    "start": "4460000",
    "end": "4470620"
  },
  {
    "text": "in- in, uh, one of the future topics that we're going to cover called Gaussian processes, right? Gaussian processes is- is a kernel method, uh, algorithm, and,",
    "start": "4470620",
    "end": "4478225"
  },
  {
    "text": "over there, um, kernel acting as a similarity metric is kind of highlighted. [NOISE] In your example B, I- I can't read what's written there.",
    "start": "4478225",
    "end": "4486440"
  },
  {
    "text": "I'm sorry. In the second set of parenthesis squared. This one? Yeah.",
    "start": "4488010",
    "end": "4493344"
  },
  {
    "text": "So this is x transpose z plus c. Oh, c. Some constant c.",
    "start": "4493345",
    "end": "4499660"
  },
  {
    "text": "Cool. Whole squared, and that C shows up over here. Thanks. Sorry. Um, so, kernels can be seen as similarity metrics.",
    "start": "4499660",
    "end": "4512650"
  },
  {
    "text": "And we want k of x,",
    "start": "4512650",
    "end": "4517780"
  },
  {
    "text": "z to be high for similar x, z.",
    "start": "4517780",
    "end": "4528264"
  },
  {
    "text": "And we want it to be low for not similar x, z, right?",
    "start": "4528265",
    "end": "4540400"
  },
  {
    "text": "And- and this should be kind of - this, this might be obvious.",
    "start": "4540400",
    "end": "4546860"
  },
  {
    "text": "Uh, the reason why we- we, uh, we say this is because k of [NOISE] x,",
    "start": "4546930",
    "end": "4559015"
  },
  {
    "text": "z is equal to Phi of x transpose Phi of z.",
    "start": "4559015",
    "end": "4564900"
  },
  {
    "text": "And we've seen that for similarly oriented vectors, Phi x and Phi z their dot- their dot product will be",
    "start": "4564900",
    "end": "4571020"
  },
  {
    "text": "high if they are oriented somewhat similarly, which means they are, you know,  similar examples, and similar, you know,",
    "start": "4571020",
    "end": "4577375"
  },
  {
    "text": "the dot-product between you know,  opposite vectors, pointing opposite is going to be negative,",
    "start": "4577375",
    "end": "4583060"
  },
  {
    "text": "uh, which means if they're not similar, the- the kernel will, will, will evaluate them to,",
    "start": "4583060",
    "end": "4588100"
  },
  {
    "text": "uh, be a smaller value. And in fact, there is also this kernel which is very, uh,",
    "start": "4588100",
    "end": "4594055"
  },
  {
    "text": "popular, k of x, z equal to exponent of",
    "start": "4594055",
    "end": "4604315"
  },
  {
    "text": "minus x minus z squared over 2 Sigma squared.",
    "start": "4604315",
    "end": "4612940"
  },
  {
    "text": "So this kernel is also called the Gaussian kernel, because it- it looks,",
    "start": "4612940",
    "end": "4618280"
  },
  {
    "text": "uh, it looks like, you know, the- the, um, um, Gaussian PDF, um, subject to, you know, um- if you ignored the normalizing constant,",
    "start": "4618280",
    "end": "4626290"
  },
  {
    "text": "this will be- this is also called the, um, uh, squared exponential kernel because you square it and you exponentiate it.",
    "start": "4626290",
    "end": "4632815"
  },
  {
    "text": "Um, and the idea here is that if x and z are very close or similar,",
    "start": "4632815",
    "end": "4638770"
  },
  {
    "text": "x minus z will be 0 and exponent of 0 is just 1. But if x and z are far apart,",
    "start": "4638770",
    "end": "4646570"
  },
  {
    "text": "then x minus z will be a big value. And the square root of that will be also be a big value.",
    "start": "4646570",
    "end": "4652330"
  },
  {
    "text": "And exponent of a negative big value is as- is close to 0, right? So for similar examples,",
    "start": "4652330",
    "end": "4658420"
  },
  {
    "text": "the Gaussian Kernel will evaluate to closer to 1. And for dissimilar examples, it'll- it'll evaluate close to 0, right?",
    "start": "4658420",
    "end": "4666445"
  },
  {
    "text": "And this kernel in particular has- if you expand it,",
    "start": "4666445",
    "end": "4672040"
  },
  {
    "text": "which is, I would say beyond the scope of our course, if you were to expand this kernel into a feature vector,",
    "start": "4672040",
    "end": "4679060"
  },
  {
    "text": "then you will see that there are, uh, the Gaussian kernel has an infinite dimensional feature vector.",
    "start": "4679060",
    "end": "4686530"
  },
  {
    "text": "But that's that's beyond the scope of this course. A question. Yes, question. [inaudible]",
    "start": "4686530",
    "end": "4698530"
  },
  {
    "text": "Yeah. The- the, uh, question is, uh, is there- is there any relation between this and locally weighted, um, um, regression?",
    "start": "4698530",
    "end": "4706435"
  },
  {
    "text": "Yes, there are, uh, connections but we did not cover locally weighted regression. So I will not go into it in the lecture.",
    "start": "4706435",
    "end": "4711925"
  },
  {
    "text": "But yes, there are- there are some connections. Yes. [NOISE]",
    "start": "4711925",
    "end": "4737640"
  },
  {
    "text": "So now the question is, given a ker - uh, a kernel function k,",
    "start": "4737640",
    "end": "4743409"
  },
  {
    "text": "how do we know - or given a function k, what makes it a kernel?",
    "start": "4744540",
    "end": "4752110"
  },
  {
    "text": "How do we know that a function k is a kernel? [NOISE] Right?",
    "start": "4752110",
    "end": "4762400"
  },
  {
    "text": "So necessary conditions for",
    "start": "4762400",
    "end": "4774310"
  },
  {
    "text": "k to be a kernel, right?",
    "start": "4774310",
    "end": "4782410"
  },
  {
    "text": "So first of all, k should be symmetric, [NOISE] which means k of x,",
    "start": "4782410",
    "end": "4796480"
  },
  {
    "text": "z should be equal to k of z, x, right?",
    "start": "4796480",
    "end": "4804110"
  },
  {
    "text": "And then for a given collection of examples,",
    "start": "4808050",
    "end": "4818184"
  },
  {
    "text": "say x_1 through x, uh,",
    "start": "4818185",
    "end": "4824725"
  },
  {
    "text": "m. Now, some collection of examples that this may not be training set.",
    "start": "4824725",
    "end": "4830950"
  },
  {
    "text": "It could be any collection of m examples. The kernel matrix k_i,",
    "start": "4830950",
    "end": "4837130"
  },
  {
    "text": "j equals k of x_i, x_j.",
    "start": "4837130",
    "end": "4847150"
  },
  {
    "text": "Where the collection of- of, um, examples could be anything, you know,",
    "start": "4847150",
    "end": "4853045"
  },
  {
    "text": "completely unrelated to your training set even. This kernel matrix k is",
    "start": "4853045",
    "end": "4859614"
  },
  {
    "text": "symmetric and positive semidefinite, right?",
    "start": "4859615",
    "end": "4870490"
  },
  {
    "text": "And the reason why it's- it should be positive semidefinite is it's pretty easy to see.",
    "start": "4870490",
    "end": "4875800"
  },
  {
    "text": "So consider some other vector z and z transpose kz can be written as sum over i,",
    "start": "4875800",
    "end": "4884980"
  },
  {
    "text": "sum over j, z_i, k_i, j, z_j, all right?",
    "start": "4884980",
    "end": "4892870"
  },
  {
    "text": "And this, you know, if you, er, do some steps, you will see that this is equal to sum over k,",
    "start": "4892870",
    "end": "4900534"
  },
  {
    "text": "where k is the- is the, uh, uh, um, is the number of features in this, uh,",
    "start": "4900535",
    "end": "4905545"
  },
  {
    "text": "feature vector, um. [NOISE]",
    "start": "4905545",
    "end": "4922114"
  },
  {
    "text": "So we show that z transpose kz is greater than equal to 0 for any z, right?",
    "start": "4922115",
    "end": "4928220"
  },
  {
    "text": "[NOISE] So if a kernel- so the definition of a kernel as a reminder,",
    "start": "4928220",
    "end": "4938700"
  },
  {
    "text": "definition of a kernel corresponding to some feature vector Phi is Phi of x transpose Phi of z.",
    "start": "4940450",
    "end": "4951020"
  },
  {
    "text": "This is the mathematical definition, right? But in general, um, this could be any function k,",
    "start": "4951020",
    "end": "4958175"
  },
  {
    "text": "that evaluates to the same value as the explicit dot product.",
    "start": "4958175",
    "end": "4964474"
  },
  {
    "text": "And, uh, that's the definition of a kernel, that it has a- a- a feature representation like this.",
    "start": "4964475",
    "end": "4970925"
  },
  {
    "text": "And in order for a kernel- in order for any function k to be a kernel, these are the necessary conditions for it.",
    "start": "4970925",
    "end": "4978065"
  },
  {
    "text": "And the second necessary condition of a- a so this is the symmetric and this is on,",
    "start": "4978065",
    "end": "4989975"
  },
  {
    "text": "you know, uh, the PSD condition on a set of examples. There is a theorem called Mercer's theorem,",
    "start": "4989975",
    "end": "4999239"
  },
  {
    "text": "which says this necessary condition is also a sufficient condition for k to be a kernel, right?",
    "start": "5003060",
    "end": "5012099"
  },
  {
    "text": "So the Mercer's theorem says, uh, let k,",
    "start": "5012100",
    "end": "5019480"
  },
  {
    "text": "in this case, we're going to just limit ourselves to R^d to R be given.",
    "start": "5019480",
    "end": "5028390"
  },
  {
    "text": "So this is sum function- sum function. Then for k to be a kernel,",
    "start": "5028390",
    "end": "5039050"
  },
  {
    "text": "it is necessary and sufficient [NOISE]",
    "start": "5041190",
    "end": "5049900"
  },
  {
    "text": "for x^1",
    "start": "5049900",
    "end": "5055610"
  },
  {
    "text": "through x^m.",
    "start": "5055620",
    "end": "5062020"
  },
  {
    "text": "The corresponding",
    "start": "5063390",
    "end": "5071450"
  },
  {
    "text": "kernel matrix k",
    "start": "5074310",
    "end": "5081025"
  },
  {
    "text": "equals kij equals of k of x^i x^j is positive",
    "start": "5081025",
    "end": "5093010"
  },
  {
    "text": "semi-definite and symmetric, right? So, um, Mercer's theorem tells us that a function k for it to be a kernel,",
    "start": "5093010",
    "end": "5106030"
  },
  {
    "text": "and for any collection of input examples, the corresponding kernel matrix will be symmetric and positive semi-definite,",
    "start": "5106030",
    "end": "5116454"
  },
  {
    "text": "and- and if this property holds for, uh- uh,",
    "start": "5116455",
    "end": "5122230"
  },
  {
    "text": "for- for any set of, uh- uh, examples, if the corresponding kernel matrix is positive and semi-definite,",
    "start": "5122230",
    "end": "5127779"
  },
  {
    "text": "then k must be a kernel, so it goes both ways. And the proof of this theorem is- is beyond the scope of our course,",
    "start": "5127779",
    "end": "5135940"
  },
  {
    "text": "but we can still get some intuitions on this, right? So as- as- as a reminder, um,",
    "start": "5135940",
    "end": "5145255"
  },
  {
    "text": "a few lectures ago, we- we saw this kind of relation between linear algebra and functional analysis, right?",
    "start": "5145255",
    "end": "5151930"
  },
  {
    "text": "In that, um, vectors are like functions, right?",
    "start": "5151930",
    "end": "5161035"
  },
  {
    "text": "And matrices- matrix is like",
    "start": "5161035",
    "end": "5166645"
  },
  {
    "text": "operators or- or functions with two inputs also.",
    "start": "5166645",
    "end": "5177620"
  },
  {
    "text": "Functions with say, x and y, right? So the kernel function k, right?",
    "start": "5178920",
    "end": "5188304"
  },
  {
    "text": "Which- which- for which we solve, you know, many compact- compact representations. Imagine this kernel to be an infinite dimensional matrix, right?",
    "start": "5188305",
    "end": "5197860"
  },
  {
    "text": "It extends to infinity in- in- in all directions, right? And k of s, uh,",
    "start": "5197860",
    "end": "5205690"
  },
  {
    "text": "let's call it x comma z is- you know, evaluates to some value, right?",
    "start": "5205690",
    "end": "5212469"
  },
  {
    "text": "Where on one- one axis consider the x-th row and on the other axis consider the z-th column,",
    "start": "5212470",
    "end": "5219385"
  },
  {
    "text": "x-th row, z-th column, right?",
    "start": "5219385",
    "end": "5224514"
  },
  {
    "text": "Imagine this to be a- a two input function. And in this case, um,",
    "start": "5224515",
    "end": "5230800"
  },
  {
    "text": "I'm drawing the horizontal and vertical, uh, axes as- as linear, but, you know, think of this as some abstract axis.",
    "start": "5230800",
    "end": "5238344"
  },
  {
    "text": "So each- each column corresponds to z, which could actually be a vector itself, right?",
    "start": "5238345",
    "end": "5244540"
  },
  {
    "text": "Here, it- it appears as though, um, z-axis is- is real value, but, you know, um, z is- is a think of each of the axes as- as,",
    "start": "5244540",
    "end": "5252925"
  },
  {
    "text": "um, some abstract axes, right? So k of x comma z is gonna give you, evaluate to some value.",
    "start": "5252925",
    "end": "5260425"
  },
  {
    "text": "And if you have a collection, um, x^1 through x^m,",
    "start": "5260425",
    "end": "5268420"
  },
  {
    "text": "then evaluating, uh, say, k of x1 x2 would be here, k of x1 x3 would be here- k of x1 x3 would be here.",
    "start": "5268420",
    "end": "5277315"
  },
  {
    "text": "So- and similarly k of x2 x1 and so on, right?",
    "start": "5277315",
    "end": "5287320"
  },
  {
    "text": "And you're gonna extract these evaluations, right, into a kernel matrix where each row belongs to x^1, x^2,",
    "start": "5287320",
    "end": "5298989"
  },
  {
    "text": "and so on, x^m and similarly x^1, x^m, right?",
    "start": "5298990",
    "end": "5307195"
  },
  {
    "text": "And we also know that k can be represented as the,",
    "start": "5307195",
    "end": "5313465"
  },
  {
    "text": "um- um, the feature as- as the, um, as the inner product between two features,",
    "start": "5313465",
    "end": "5320065"
  },
  {
    "text": "which means k is in some way a positive definite kernel or a positive definite function.",
    "start": "5320065",
    "end": "5327505"
  },
  {
    "text": "And what it means is that if you, uh, take some bunch of examples, which means you choose a few axes and extract the values into a matrix.",
    "start": "5327505",
    "end": "5337760"
  },
  {
    "text": "This matrix is always positive semi-definite. And it says if and only if, which means, um,",
    "start": "5337980",
    "end": "5345730"
  },
  {
    "text": "if any such matrix for any such, you know, collection of examples, if you extract the evaluations,",
    "start": "5345730",
    "end": "5351460"
  },
  {
    "text": "you get a positive semi-definite matrix. And if that holds true for any positive semi definite matrix,",
    "start": "5351460",
    "end": "5358420"
  },
  {
    "text": "then k is a valid kernel, which means it's a positive definite function. And that's basically a way of saying, um,",
    "start": "5358420",
    "end": "5365395"
  },
  {
    "text": "any sub matrix of a positive semi-definite matrix is also positive semi-definite, right?",
    "start": "5365395",
    "end": "5372130"
  },
  {
    "text": "So Mercer's kernel- Mercer's theorem is basically a way of saying, um, a sub-matrix of any positive definite matrix is also positive definite.",
    "start": "5372130",
    "end": "5381400"
  },
  {
    "text": "[NOISE] And that's just some intuition for- for, uh, what, uh, Mercer's kernel is, uh, telling us.",
    "start": "5381400",
    "end": "5388250"
  },
  {
    "text": "Okay, so, uh, that's about it in terms of properties of kernels.",
    "start": "5389010",
    "end": "5395559"
  },
  {
    "text": "To, uh, as I said, in- in order to prove that a function is a kernel, you have two options.",
    "start": "5395560",
    "end": "5402530"
  },
  {
    "text": "To prove k is kernel. Just to summarize this one, you know,",
    "start": "5403230",
    "end": "5411340"
  },
  {
    "text": "construct Phi such that",
    "start": "5411340",
    "end": "5416935"
  },
  {
    "text": "k is equal to Phi transpose Phi, right?",
    "start": "5416935",
    "end": "5423595"
  },
  {
    "text": "Or two, for, use Mercer's theorem,",
    "start": "5423595",
    "end": "5431060"
  },
  {
    "text": "Mercer's theorem, which means for any collection x^1 through x^m,",
    "start": "5432300",
    "end": "5440844"
  },
  {
    "text": "kij equals k of x^i,",
    "start": "5440845",
    "end": "5452070"
  },
  {
    "text": "x^j is positive semi-definite, right? In fact, there is also a three,",
    "start": "5452070",
    "end": "5461219"
  },
  {
    "text": "which generally it's- it's very hard to- to prove that the double integral of- for any f or all f the double integral of f of x,",
    "start": "5461220",
    "end": "5473485"
  },
  {
    "text": "k of x, x prime, f of x prime,",
    "start": "5473485",
    "end": "5480279"
  },
  {
    "text": "dxdx prime, is greater than or equal to 0.",
    "start": "5480279",
    "end": "5485755"
  },
  {
    "text": "So this is basically saying this infinite dimensional matrix is positive semi-definite.",
    "start": "5485755",
    "end": "5490915"
  },
  {
    "text": "And this is saying, um, you know, a- a sub matrix is positive semi-definite.",
    "start": "5490915",
    "end": "5498280"
  },
  {
    "text": "And Mercer's theorem basically tells us that this will hold true for any sub matrix.",
    "start": "5498280",
    "end": "5503425"
  },
  {
    "text": "You know, it's- it's basically a- a statement, uh, saying any sub-matrix of positive definite is also positive definite. Yes.",
    "start": "5503425",
    "end": "5508750"
  },
  {
    "text": "[inaudible] Yeah. This is just the quadratic form written in",
    "start": "5508750",
    "end": "5513850"
  },
  {
    "text": "functional analysis- in function notation. Yeah. [NOISE] All right.",
    "start": "5513850",
    "end": "5519640"
  },
  {
    "text": "So that's it about kernels. So we saw kernels for, um-um,",
    "start": "5519640",
    "end": "5525085"
  },
  {
    "text": "linear regression and now we're gonna spend a little- little bit of time to,",
    "start": "5525085",
    "end": "5531670"
  },
  {
    "text": "uh, see how they get used in support vector machines. So support vector machines, we're not gonna,",
    "start": "5531670",
    "end": "5538750"
  },
  {
    "text": "uh, cover support vector machines in detail in this class. Mostly because, um, once upon a time support vector machines like, you know,",
    "start": "5538750",
    "end": "5547590"
  },
  {
    "text": "ten years ago was a super hot topic and there is shrinking amount of interest in support vector machines.",
    "start": "5547590",
    "end": "5556660"
  },
  {
    "text": "But it is still interesting to- to see how support vector machines are formulated and how kernels play a role there.",
    "start": "5556660",
    "end": "5564610"
  },
  {
    "text": "[NOISE]",
    "start": "5564610",
    "end": "5579460"
  },
  {
    "text": "Right? So support vector machines. [NOISE]",
    "start": "5579460",
    "end": "5593380"
  },
  {
    "text": "So support vector machines is a discriminative",
    "start": "5593380",
    "end": "5598880"
  },
  {
    "text": "classification algorithm.",
    "start": "5602040",
    "end": "5608260"
  },
  {
    "text": "And the idea of support vector machines is if you have some examples, x_1 and x_d.",
    "start": "5608260",
    "end": "5618360"
  },
  {
    "text": "And let's say you have a few examples here and a few [NOISE] examples here.",
    "start": "5618360",
    "end": "5627880"
  },
  {
    "text": "Right? Now, how do we come up with a separating hyperplane, right? By just looking at this, there- there are",
    "start": "5627880",
    "end": "5634074"
  },
  {
    "text": "so many possible hyperplanes that we can come up with. For example, this could be a separating hyperplane- this could be a separating hyperplane.",
    "start": "5634075",
    "end": "5642909"
  },
  {
    "text": "Right. There are so many and infinitely many possible separating hyperplanes that- that we could come up with.",
    "start": "5642910",
    "end": "5649090"
  },
  {
    "text": "And the question is, you know, wha- what should be the ideal separating hyperplane that,",
    "start": "5649090",
    "end": "5655614"
  },
  {
    "text": "um, that we- that we can come up with given a data set like this that's, uh, separable.",
    "start": "5655615",
    "end": "5662065"
  },
  {
    "text": "And for that, the answer uses this concept called a margin, right.",
    "start": "5662065",
    "end": "5669175"
  },
  {
    "text": "So the margin is [NOISE] - it",
    "start": "5669175",
    "end": "5674619"
  },
  {
    "text": "basically tells us [NOISE] right.",
    "start": "5674620",
    "end": "5680755"
  },
  {
    "text": "So the margin- um, you can think of the margin as y_i. All right.",
    "start": "5680755",
    "end": "5688960"
  },
  {
    "text": "Before- before we get into there, some, uh, notational changes from before, y_ i for this lecture alone,",
    "start": "5688960",
    "end": "5700780"
  },
  {
    "text": "think of it as being in plus 1 or minus 1 instead of 1 and 0, just because it's gonna make the notation a little simpler.",
    "start": "5700780",
    "end": "5708475"
  },
  {
    "text": "But this is just a- a superficial change that is, you know, nothing fundamentally changes.",
    "start": "5708475",
    "end": "5714580"
  },
  {
    "text": "We're just using different numbers to indicate different classes. And the parameters [NOISE] will be w and b,",
    "start": "5714580",
    "end": "5725804"
  },
  {
    "text": "where w is in R_ d and b is in R and,",
    "start": "5725805",
    "end": "5735595"
  },
  {
    "text": "uh, for examples, x are also in R_ d. And we make predictions with w transpose x plus b.",
    "start": "5735595",
    "end": "5745690"
  },
  {
    "text": "What this basically means is we assume we're not loading the intercept term x naught equal to 1 into our examples.",
    "start": "5745690",
    "end": "5753610"
  },
  {
    "text": "And we're gonna have an explicit separately written out parameter called b in place of having an interceptor.",
    "start": "5753610",
    "end": "5760344"
  },
  {
    "text": "Right? So the margin is defined as y_ i times",
    "start": "5760345",
    "end": "5766375"
  },
  {
    "text": "w transpose x_ i plus b, right?",
    "start": "5766375",
    "end": "5772990"
  },
  {
    "text": "Now the- the, um, the idea is again very similar to,",
    "start": "5772990",
    "end": "5782620"
  },
  {
    "text": "um, logistic regression, where our predicted value w transpose x plus b.",
    "start": "5782620",
    "end": "5788330"
  },
  {
    "text": "We want it to be greater than 0 for y equals plus 1.",
    "start": "5789060",
    "end": "5796075"
  },
  {
    "text": "And we want it to be less than 0 for y equals minus 1.",
    "start": "5796075",
    "end": "5801175"
  },
  {
    "text": "So let's suppose this is y equals plus 1 class, and this is y equals minus 1 class, right?",
    "start": "5801175",
    "end": "5808870"
  },
  {
    "text": "We want, y_ i times w,",
    "start": "5808870",
    "end": "5814000"
  },
  {
    "text": "uh - uh, w transpose x plus b to be greater than 0 for y equals 1.",
    "start": "5814000",
    "end": "5820660"
  },
  {
    "text": "No, we want w transpose x to be greater than 0 for y equals 1, and uh, w transpose x to be less than 0 for y equals minus 1,",
    "start": "5820660",
    "end": "5829570"
  },
  {
    "text": "which means y_ i times w transpose x plus b should always be",
    "start": "5829570",
    "end": "5835074"
  },
  {
    "text": "greater than 0 for both y equals 1 and y equals minus 1, right?",
    "start": "5835075",
    "end": "5841270"
  },
  {
    "text": "So what we desire is",
    "start": "5841270",
    "end": "5847870"
  },
  {
    "text": "margin to be large, right?",
    "start": "5847870",
    "end": "5854850"
  },
  {
    "text": "And the- the definition of the margin is taking into account that,  uh, for some of the examples, uh,",
    "start": "5854850",
    "end": "5861389"
  },
  {
    "text": "w transpose x plus b will be greater than 0, and for some it is less than 0. And for the cases when it is supposed to be less than 0,",
    "start": "5861390",
    "end": "5868710"
  },
  {
    "text": "we multiply it by minus 1. So the margin, you know, after multiplying by minus 1 has to be large no matter what the example is, right?",
    "start": "5868710",
    "end": "5877255"
  },
  {
    "text": "And the idea of support vector machine is to calculate",
    "start": "5877255",
    "end": "5882639"
  },
  {
    "text": "the margin for- from the given- given,",
    "start": "5882640",
    "end": "5888250"
  },
  {
    "text": "uh, uh, hyperplane, calculate the margin with respect to all the examples and maximize the smallest margin.",
    "start": "5888250",
    "end": "5895210"
  },
  {
    "text": "Okay, that's the- that's the idea of- of, uh, the support vector machine, which means in general,",
    "start": "5895210",
    "end": "5901255"
  },
  {
    "text": "now, for example, uh, for this separating hyperplane,",
    "start": "5901255",
    "end": "5907435"
  },
  {
    "text": "the smallest margin is here, right? And for this hyperplane,",
    "start": "5907435",
    "end": "5914304"
  },
  {
    "text": "the smallest margin is here, right? And for this hyperplane,",
    "start": "5914305",
    "end": "5921940"
  },
  {
    "text": "the smallest margin is probably here, right? And we want to choose the hyperplane that gives us the largest small margin, right?",
    "start": "5921940",
    "end": "5934045"
  },
  {
    "text": "So calculate the smallest margin and choose the hyperplane for which the smallest Margin is the largest, right?",
    "start": "5934045",
    "end": "5941440"
  },
  {
    "text": "And that's the intuition behind a support vector machine, right. Now, the, uh, one thing that you can- that you can, um,",
    "start": "5941440",
    "end": "5949780"
  },
  {
    "text": "observe is that if we are able to find some hyperplane for which we are able to correctly classify all the examples.",
    "start": "5949780",
    "end": "5959050"
  },
  {
    "text": "If you are able to find one such w and b that correctly classifies all the examples,",
    "start": "5959050",
    "end": "5965139"
  },
  {
    "text": "then with this definition of a margin, we can kind of fool the system by choosing scaled versions of w and b.",
    "start": "5965140",
    "end": "5977980"
  },
  {
    "text": "So for example, in place of w, if we have 2w and in place of- of b,",
    "start": "5977980",
    "end": "5983934"
  },
  {
    "text": "we have 2b, then the margin by this definition effectively doubles, right?",
    "start": "5983935",
    "end": "5989350"
  },
  {
    "text": "So if we are seeking a separating hyperplane where this- this geometric notion of",
    "start": "5989350",
    "end": "5995890"
  },
  {
    "text": "margin is largest in some sense where the smallest geometric margin is- is- is the largest,",
    "start": "5995890",
    "end": "6002925"
  },
  {
    "text": "then this definition of margin might- is- is vulnerable for being gamed.",
    "start": "6002925",
    "end": "6008940"
  },
  {
    "text": "You can always scale up your w's and b's and increase your margin even more. Though it does not change your separating hyperplane in any way, right?",
    "start": "6008940",
    "end": "6018225"
  },
  {
    "text": "And that is why- um, so this is also called as functional margin.",
    "start": "6018225",
    "end": "6025600"
  },
  {
    "text": "So the- a support vector machine is an algorithm that tries to maximize the geometric margin,",
    "start": "6027140",
    "end": "6035070"
  },
  {
    "text": "which means the actual geometric distance between the- the, uh, separating hyperplane and the examples, right?",
    "start": "6035070",
    "end": "6041940"
  },
  {
    "text": "This is not the geometric distance between x and the hyperplane, right?",
    "start": "6041940",
    "end": "6047969"
  },
  {
    "text": "Because this, you can- you can- um, you can use 2w and 2b.",
    "start": "6047970",
    "end": "6053145"
  },
  {
    "text": "And- [NOISE] so if w transpose x plus b equals 0,",
    "start": "6053145",
    "end": "6061530"
  },
  {
    "text": "this corresponds to the separating hyperplane. You can scale your w's and b's with any",
    "start": "6061530",
    "end": "6067440"
  },
  {
    "text": "scalar and still have the same separating hyperplane. But this definition of margin would result in a- in a larger value.",
    "start": "6067440",
    "end": "6076949"
  },
  {
    "text": "You would get a larger functional margin for the same separating hyperplane, right? And the support vector machine addresses this- this specific, uh, uh, problem.",
    "start": "6076950",
    "end": "6087690"
  },
  {
    "text": "And we're only gonna look at the- the, um, problem formulation and not actually solve it for this course.",
    "start": "6087690",
    "end": "6096945"
  },
  {
    "text": "But the support vector machine- [NOISE] so in support vector machine,",
    "start": "6096945",
    "end": "6104730"
  },
  {
    "text": "uh, we seek a value of w and b such that, uh, w and b gives us the ideal margin.",
    "start": "6104730",
    "end": "6111885"
  },
  {
    "text": "And this is the wrong- wrong objective to maximize because this objective is vulnerable to being scaled and being fooled, right?",
    "start": "6111885",
    "end": "6120344"
  },
  {
    "text": "So the objective instead that, uh, support vector machines uses is this. So minimize the spectrum w",
    "start": "6120345",
    "end": "6132690"
  },
  {
    "text": "and b [NOISE] i equals 1 to n,",
    "start": "6132690",
    "end": "6140100"
  },
  {
    "text": "max 0 comma 1",
    "start": "6140100",
    "end": "6147840"
  },
  {
    "text": "minus y_ i times",
    "start": "6147840",
    "end": "6154185"
  },
  {
    "text": "w transpose x_ i plus b",
    "start": "6154185",
    "end": "6159490"
  },
  {
    "text": "plus 1 over c [NOISE] w squared.",
    "start": "6167480",
    "end": "6174880"
  },
  {
    "text": "So what's- what's happening here? So we have a 1 minus y_ i times w transpose x plus b.",
    "start": "6175490",
    "end": "6185305"
  },
  {
    "text": "And we have a max between this times 0, right? This term over here is commonly called as the hinge loss.",
    "start": "6185305",
    "end": "6194740"
  },
  {
    "text": "You'll see why it's called that way, or it's also called sometimes the SVM loss.",
    "start": "6194740",
    "end": "6201090"
  },
  {
    "text": "Maybe you see this term in, um, many cases. So what this means is, you know,",
    "start": "6201090",
    "end": "6207945"
  },
  {
    "text": "if we want to think of this as your margin.",
    "start": "6207945",
    "end": "6213155"
  },
  {
    "text": "You know, this is still the function margin. All right. Let's call this equal to gamma of I. All right.",
    "start": "6213155",
    "end": "6221860"
  },
  {
    "text": "And if this is gamma, then 1,",
    "start": "6221860",
    "end": "6229420"
  },
  {
    "text": "the [NOISE] the hinge loss looks like this, right?",
    "start": "6229420",
    "end": "6238469"
  },
  {
    "text": "So this whole term is called the hinge loss.",
    "start": "6238470",
    "end": "6241750"
  },
  {
    "text": "Hinge loss. And it looks like this. Why- why does it look like this?",
    "start": "6245960",
    "end": "6252090"
  },
  {
    "text": "So if for all values where the margin is- margin does not include the- 1 minus,",
    "start": "6252090",
    "end": "6267150"
  },
  {
    "text": "so this is the margin. So whenever the margin is bigger than 1,",
    "start": "6267150",
    "end": "6274514"
  },
  {
    "text": "then the right-hand term after the comma is going to be less than 0.",
    "start": "6274515",
    "end": "6280605"
  },
  {
    "text": "So for all values of n, the margin is bigger than 1, the right time- right hand,",
    "start": "6280605",
    "end": "6285765"
  },
  {
    "text": "the second- the second term will be less than 0 and the max between 0 and a negative number will be 0.",
    "start": "6285765",
    "end": "6293040"
  },
  {
    "text": "And so the hinge loss for any value of- of- of Gamma or margin bigger than 1,",
    "start": "6293040",
    "end": "6298980"
  },
  {
    "text": "the hinge loss is always 0. And for all values where the hinge loss is- evaluates to",
    "start": "6298980",
    "end": "6306060"
  },
  {
    "text": "something less than 1, so for example, if the hinge loss evaluates to, let's say 0.1,",
    "start": "6306060",
    "end": "6316075"
  },
  {
    "text": "then this is going to be between max of 0 and 1 minus 0.1,",
    "start": "6316075",
    "end": "6322100"
  },
  {
    "text": "that's max of 0 and 0.9. So when ga- gamma is 0.1,",
    "start": "6322100",
    "end": "6328614"
  },
  {
    "text": "the loss is going to be 0.9, right? And as- as you keep decreasing gamma further,",
    "start": "6328615",
    "end": "6335235"
  },
  {
    "text": "the hinge loss is going to be more and more, okay? So what this- what this is basically telling us is learn w and",
    "start": "6335235",
    "end": "6344640"
  },
  {
    "text": "b such that w transpose x plus b is at least plus 1.",
    "start": "6344640",
    "end": "6352485"
  },
  {
    "text": "We want it to be just greater than 0 for y equals plus 1.",
    "start": "6352485",
    "end": "6357600"
  },
  {
    "text": "But the loss is instead telling- actually, we are not satisfied if it's just bigger than 0,",
    "start": "6357600",
    "end": "6364350"
  },
  {
    "text": "we'd actually make it bigger than 1, right? Go further. And because we know",
    "start": "6364350",
    "end": "6372420"
  },
  {
    "text": "that- because we know that this functional margin,",
    "start": "6372420",
    "end": "6379485"
  },
  {
    "text": "now, this functional margin is vulnerable to this kind of scaling where,",
    "start": "6379485",
    "end": "6386025"
  },
  {
    "text": "you know, the- the algorithm can, once- once the algorithm finds- finds a w and b such that w transpose x plus b is,",
    "start": "6386025",
    "end": "6394695"
  },
  {
    "text": "is 0.1. it could just scale up w and b further to just increase,",
    "start": "6394695",
    "end": "6401630"
  },
  {
    "text": "uh, your- the- the margin to go greater than 1. And in order to safeguard against that kind of a scaling, uh,",
    "start": "6401630",
    "end": "6412440"
  },
  {
    "text": "we also penalize the norm of w, which means you can't just increase the value of",
    "start": "6412440",
    "end": "6418034"
  },
  {
    "text": "w by- by scaling it larger and thereby increasing the functional margin. You actually need to, you know,",
    "start": "6418035",
    "end": "6424665"
  },
  {
    "text": "find a well-suited, um, well-suited R w and v such that, you know,",
    "start": "6424665",
    "end": "6431385"
  },
  {
    "text": "there is- there is- such that they are actually far away from the examples.",
    "start": "6431385",
    "end": "6437295"
  },
  {
    "text": "All right? And this formulation can be rewritten",
    "start": "6437295",
    "end": "6442559"
  },
  {
    "text": "as- so this optimization problem can be",
    "start": "6442559",
    "end": "6448650"
  },
  {
    "text": "rewritten as min xi,",
    "start": "6448650",
    "end": "6455159"
  },
  {
    "text": "w, b half w square",
    "start": "6455160",
    "end": "6462090"
  },
  {
    "text": "plus c times sum over I equals 1 to n.",
    "start": "6462090",
    "end": "6467235"
  },
  {
    "text": "C i such that y i times W transpose X",
    "start": "6467235",
    "end": "6483030"
  },
  {
    "text": "i plus b greater than equal to one minus c i.",
    "start": "6483030",
    "end": "6490559"
  },
  {
    "text": "for all i in 1 through n and because xi",
    "start": "6490560",
    "end": "6500040"
  },
  {
    "text": "i is greater than equal to 0, i equal to 1n.",
    "start": "6500040",
    "end": "6505710"
  },
  {
    "text": "Now this formulation, it looks different from this, but it, uh, but the two are- are,",
    "start": "6505710",
    "end": "6513930"
  },
  {
    "text": "are actually equivalent in the sense the optimal solution you get for W and b from this problem is the same as,",
    "start": "6513930",
    "end": "6521235"
  },
  {
    "text": "uh, the W and B you would get from this problem. And, uh, the reason why we don't",
    "start": "6521235",
    "end": "6528500"
  },
  {
    "text": "want this form to be sought directly is because this max operator is not differentiable.",
    "start": "6528500",
    "end": "6534595"
  },
  {
    "text": "Whereas this is what is also called as a convex problem, where the losses convex,",
    "start": "6534595",
    "end": "6541004"
  },
  {
    "text": "the thing that we want to minimize, and the constraints are convex. And once you have a problem, um,",
    "start": "6541004",
    "end": "6547155"
  },
  {
    "text": "written out in this form where the objective and the constraints are all convex, then there are lots of solvers that can just solve this for you, right?",
    "start": "6547155",
    "end": "6554010"
  },
  {
    "text": "And the way the, uh,",
    "start": "6554010",
    "end": "6560260"
  },
  {
    "text": "another- another topic in convex optimization is that for every such convex problem,",
    "start": "6560260",
    "end": "6567230"
  },
  {
    "text": "there is an equivalent dual problem, right? So this is also called, think of this as the primal convex problem.",
    "start": "6567230",
    "end": "6576190"
  },
  {
    "text": "Correspondingly there, that is a dual convex problem.",
    "start": "6582350",
    "end": "6588550"
  },
  {
    "text": "And the dual convex problem looks something like this. Max over Alpha.",
    "start": "6594290",
    "end": "6602170"
  },
  {
    "text": "So to be clear, you don't need to know convex optimization for this course and you don't need to know how to derive this.",
    "start": "6604370",
    "end": "6612150"
  },
  {
    "text": "Uh, you just need to have the intuitions of how the SVM is trying to optimize the geometric margin.",
    "start": "6612150",
    "end": "6619469"
  },
  {
    "text": "And whereas this is just the functional margin, the functional margin can be fooled by just scaling Ws and b.",
    "start": "6619470",
    "end": "6625575"
  },
  {
    "text": "So instead, SVM includes this penalty term to- to save itself from getting fooled.",
    "start": "6625575",
    "end": "6632910"
  },
  {
    "text": "And that's equivalent to this convex problem. And that is equivalent to this dual convex problem, yes, question.",
    "start": "6632910",
    "end": "6639330"
  },
  {
    "text": "[BACKGROUND] Good question. Why don't we penalize B?",
    "start": "6639330",
    "end": "6644895"
  },
  {
    "text": "There are good reasons why you don't want to penalize B. Because if you penalize B, you're constraining your separating hyperplane to be close to the origin, right?",
    "start": "6644895",
    "end": "6653534"
  },
  {
    "text": "But that there is no value in doing that. You want, you want to give your algorithm freedom took",
    "start": "6653535",
    "end": "6659159"
  },
  {
    "text": "place it however far from the origin as necessary, depending on the data. [BACKGROUND]",
    "start": "6659160",
    "end": "6667800"
  },
  {
    "text": "So- so, uh, I would say, I would encourage you to go through the notes and you can see",
    "start": "6667800",
    "end": "6673020"
  },
  {
    "text": "why penalizing Bs is the wrong idea if, you know, if you're still interested. But the intuition there is that by penalizing B,",
    "start": "6673020",
    "end": "6679995"
  },
  {
    "text": "you're restricting the algorithm to have a hyperplane that's close to 0. And that's, you know,",
    "start": "6679995",
    "end": "6685185"
  },
  {
    "text": "that- that's seems pretty arbitrary. [BACKGROUND]",
    "start": "6685185",
    "end": "6693630"
  },
  {
    "text": "So- so the thing is w and b are kind of work differently. So B tells you how far apart you can be from",
    "start": "6693630",
    "end": "6699300"
  },
  {
    "text": "the origin and you don't want any kind of constraint over there. Uh, so there is an equivalent dual problem,",
    "start": "6699300",
    "end": "6706020"
  },
  {
    "text": "of i minus half Y i.",
    "start": "6706020",
    "end": "6712090"
  },
  {
    "text": "This is a double sum over i j, y i, y j, Alpha i Alpha j and",
    "start": "6712730",
    "end": "6723450"
  },
  {
    "text": "inner product between x i x j such",
    "start": "6723450",
    "end": "6730710"
  },
  {
    "text": "that 0 less than equal to Alpha I less than equal to c. And sum over i equals 1 to n,",
    "start": "6730710",
    "end": "6740430"
  },
  {
    "text": "Alpha i, y i equals 0. Anyways, so if- if you know Convex analysis- if you know convex optimization,",
    "start": "6740430",
    "end": "6751605"
  },
  {
    "text": "this problem can be- can be rewritten in this form. But you don't need to know how- how- how",
    "start": "6751605",
    "end": "6759150"
  },
  {
    "text": "you go from here to here for this course, all right? But once you write it in this form,",
    "start": "6759150",
    "end": "6764370"
  },
  {
    "text": "you see that we end up with, uh, we end up with a term where",
    "start": "6764370",
    "end": "6771840"
  },
  {
    "text": "we are doing or taking a dot product between pairs of examples. And we're doing that across all pairs, right?",
    "start": "6771840",
    "end": "6777030"
  },
  {
    "text": "Over here we are- we're explicitly trying to learn W and b, right?",
    "start": "6777030",
    "end": "6782235"
  },
  {
    "text": "And to kind of draw the similarity between linear regression, this is like trying to find the theta vector.",
    "start": "6782235",
    "end": "6787785"
  },
  {
    "text": "But in the dual form we are trying to find Alpha, a collection of coefficients,",
    "start": "6787785",
    "end": "6794159"
  },
  {
    "text": "Alphas where each Alpha is the weight you apply to each example. And that was very similar to the per example betas that we saw in linear regression.",
    "start": "6794160",
    "end": "6803100"
  },
  {
    "text": "[NOISE] So in the dual problem, we are trying to find the coefficients with which we weight examples.",
    "start": "6803100",
    "end": "6809119"
  },
  {
    "text": "And we can represent the algorithm completely in terms of- in terms of a dot product between examples, right?.",
    "start": "6809120",
    "end": "6816700"
  },
  {
    "text": "And here you can replace this dot product with the feature map. And you can replace that dot-product between features with a kernel.",
    "start": "6816700",
    "end": "6823665"
  },
  {
    "text": "And that's how kernels come into picture in support vector machines using the dual formulation, all right?",
    "start": "6823665",
    "end": "6828795"
  },
  {
    "text": "And the one unique property. I wouldn't say unique property or one nice property",
    "start": "6828795",
    "end": "6835575"
  },
  {
    "text": "of support vector machines is that once you solve this problem, the set of Alphas that you,",
    "start": "6835575",
    "end": "6841715"
  },
  {
    "text": "that you obtain as- as- as a result of this optimization problem will be sparse. Which means only a small number of",
    "start": "6841715",
    "end": "6850655"
  },
  {
    "text": "Alpha i's will be non-zero but most of the alpha i's will be 0, right?",
    "start": "6850655",
    "end": "6856005"
  },
  {
    "text": "And that means that those set of examples for which the alpha i's were non-zeros are called the support vectors.",
    "start": "6856005",
    "end": "6864380"
  },
  {
    "text": "And intuitively, those will be the- those examples that are nearest to the separating hyperplane.",
    "start": "6864380",
    "end": "6873059"
  },
  {
    "text": "All right. Uh, I think we're over time and, uh, that's pretty much all we wanted to cover about support vector machines.",
    "start": "6873440",
    "end": "6881115"
  },
  {
    "text": "The main idea that you need to remember about support vector machines for this course is this idea of functional margin versus geometric margin.",
    "start": "6881115",
    "end": "6890355"
  },
  {
    "text": "And that it's possible to write the SVM in this dual convex problem as this dual convex problem.",
    "start": "6890355",
    "end": "6896370"
  },
  {
    "text": "And that's going to result in coefficients that are sparse, right? That's- that's the extent to which you need to know about SVMs for this course.",
    "start": "6896370",
    "end": "6903940"
  }
]