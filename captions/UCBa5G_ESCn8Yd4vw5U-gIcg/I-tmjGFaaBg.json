[
  {
    "start": "0",
    "end": "5670"
  },
  {
    "text": "OK, so let's get started. So the formulation--\nso most of this course will be about\nsupervised learning.",
    "start": "5670",
    "end": "12580"
  },
  {
    "text": "So in some part,\nwe're going to talk about unsupervised learning. But I think maybe like\n80 of the lectures",
    "start": "12580",
    "end": "21630"
  },
  {
    "text": "will be about\nsupervised learning. So this is about\nsupervised learning.",
    "start": "21630",
    "end": "26702"
  },
  {
    "text": "OK. ",
    "start": "26702",
    "end": "31970"
  },
  {
    "text": "So let me just-- so we have some definitions. So input space-- so this is the\ndata that you want to classify",
    "start": "31970",
    "end": "43585"
  },
  {
    "text": "or kind of like you want\nto do regression on. So under the label\nspace, that's called y.",
    "start": "43585",
    "end": "51309"
  },
  {
    "text": "And there's a joint probability\ndistribution [PAUSES]",
    "start": "51310",
    "end": "60720"
  },
  {
    "text": "p over the space of x times y.",
    "start": "60720",
    "end": "66870"
  },
  {
    "text": "And there is a-- let me see how do I-- I guess probably, I\nwould still try to do--",
    "start": "66870",
    "end": "74060"
  },
  {
    "text": "maybe I should do this. And this is better, right?",
    "start": "74060",
    "end": "80549"
  },
  {
    "text": "And we're going to have\nsome trending data points. ",
    "start": "80550",
    "end": "89850"
  },
  {
    "text": "So these are x1,\ny1 up to xn, yn.",
    "start": "89850",
    "end": "97830"
  },
  {
    "text": "Each data point is a\npair of input and output. And we will use n for the\nnumber of examples forever.",
    "start": "97830",
    "end": "107200"
  },
  {
    "text": "So n is reserved for the number\nof examples for this course. And each of these\ndata points xi,",
    "start": "107200",
    "end": "114475"
  },
  {
    "text": "yi is assumed to be joint\nIID from this distribution p.",
    "start": "114475",
    "end": "120043"
  },
  {
    "text": "So p is the distribution\nwe are interested in. And then we have some\nexamples from it.",
    "start": "120043",
    "end": "125530"
  },
  {
    "text": "And we have some loss function. ",
    "start": "125530",
    "end": "130638"
  },
  {
    "text": "And this loss function\ntakes in two labels. ",
    "start": "130639",
    "end": "135740"
  },
  {
    "text": "And it also is a number that\ncharacterizes how different these two labels are.",
    "start": "135740",
    "end": "141560"
  },
  {
    "text": "And I think typical convention\nis that the first one is the predicted label. ",
    "start": "141560",
    "end": "149060"
  },
  {
    "text": "And this is the true label. Oh, this is the observed label.",
    "start": "149060",
    "end": "154870"
  },
  {
    "text": "And you assume\nthat if you have--",
    "start": "154870",
    "end": "163010"
  },
  {
    "text": "the loss is always non-negative. I think in some cases\nthe loss can be negative. But in most of the cases,\nthe loss is non-negative.",
    "start": "163010",
    "end": "172880"
  },
  {
    "text": "And now suppose you have a--",
    "start": "172880",
    "end": "178475"
  },
  {
    "text": " you can also have a predictor.",
    "start": "178475",
    "end": "185508"
  },
  {
    "text": "Because this is what\nyou are interested in. You want to have-- sometimes it's called model.",
    "start": "185508",
    "end": "190720"
  },
  {
    "text": "Sometimes its the hypothesis. We're going to use all\nof this interchangeably.",
    "start": "190720",
    "end": "198079"
  },
  {
    "text": "All of these are used in some\ndifferent sets of contexts, but they all mean\nthe same thing.",
    "start": "198080",
    "end": "203480"
  },
  {
    "text": "It means the function\nyou want to look for to predict your label.",
    "start": "203480",
    "end": "208590"
  },
  {
    "text": "So this is a function\nthat's called h. It's a mapping\nfrom y to-- x to y.",
    "start": "208590",
    "end": "214250"
  },
  {
    "text": "And you can define the\nloss of the predictor.",
    "start": "214250",
    "end": "219850"
  },
  {
    "text": " Example, x, y and a loss\nwill be you first plug",
    "start": "219850",
    "end": "231360"
  },
  {
    "text": "in h of x, which\nis your prediction. And you have y. This is the loss. And then after we\ndivide all of this,",
    "start": "231360",
    "end": "238190"
  },
  {
    "text": "you can define the so-called\nexpected or population, risk",
    "start": "238190",
    "end": "248550"
  },
  {
    "text": "or loss. This is kind of the interesting\nthing about machine learning, like everything has\nlike two names at least.",
    "start": "248550",
    "end": "257187"
  },
  {
    "text": "I think two is the lower bound. Sometimes you need three. And also, my brain\nis kind of like--",
    "start": "257188",
    "end": "265070"
  },
  {
    "text": "for different kind\nof situations, I use different name for this. You prepare for that. Just because when I learn\nthis part of things and those",
    "start": "265070",
    "end": "272790"
  },
  {
    "text": "literature, I use that name,\nand then if you learn something else, then you'll--",
    "start": "272790",
    "end": "278250"
  },
  {
    "text": "those kind of papers\nuse a different name. But so my brain is\njust like-- these names",
    "start": "278250",
    "end": "283800"
  },
  {
    "text": "spreading into different\nparts of my brain. So I might use inconsistent\nterminologies a little bit.",
    "start": "283800",
    "end": "290880"
  },
  {
    "text": "But all of these are the same. They are the same. Expected just means population. And risk just means loss.",
    "start": "290880",
    "end": "296595"
  },
  {
    "text": " But of course, I will try to--",
    "start": "296595",
    "end": "301650"
  },
  {
    "text": "I will try to be consistent\nas much as possible. And this expected risk\nor population risk",
    "start": "301650",
    "end": "309000"
  },
  {
    "text": "is defined to be the\nexpectation of the loss. ",
    "start": "309000",
    "end": "316449"
  },
  {
    "text": "And here, the random\nvariables are x and y. And they are drawn from this\npopulation distribution p.",
    "start": "316450",
    "end": "324419"
  },
  {
    "text": "And that's why it's\ncalled population risk. And this is your final goal. ",
    "start": "324420",
    "end": "331620"
  },
  {
    "text": "Your final goal is\nbasically to minimize. So find H that minimize\nthe population risk.",
    "start": "331620",
    "end": "347229"
  },
  {
    "text": "At least, this is the goal for\nthe first at least 15 lectures. Right? So this is the goal for\nsupervised learning.",
    "start": "347230",
    "end": "354430"
  },
  {
    "text": "You just want to predict\nx value as possible. OK? ",
    "start": "354430",
    "end": "365740"
  },
  {
    "text": "OK, so to achieve\nthis goal, you also have to introduce\nmore concepts, right? So one concept is this\nso-called hypothesis class--",
    "start": "365740",
    "end": "374510"
  },
  {
    "text": " sometimes, hypothesis family.",
    "start": "374510",
    "end": "380669"
  },
  {
    "text": "And you can also call\nit predictor class, predictor family, model\nclass, model family.",
    "start": "380670",
    "end": "386070"
  },
  {
    "text": "So let's call it\ncapital H. And this is a set of functions\n[PAUSES] from x to y.",
    "start": "386070",
    "end": "397860"
  },
  {
    "text": "All right. And you can define the\nso-called excess risk.",
    "start": "397860",
    "end": "403522"
  },
  {
    "text": "Because at the end of it,\nyou're going to search over a set of functions. Right? And maybe the set of\nfunctions is very bad.",
    "start": "403522",
    "end": "408880"
  },
  {
    "text": "For example, the\nset of functions only contain one function. So that's why people define this\nas so-called excess risk, which",
    "start": "408880",
    "end": "415270"
  },
  {
    "text": "tries to kind of\ndefine your error relative to the power\nof this hypothesis class",
    "start": "415270",
    "end": "423100"
  },
  {
    "text": "or this set of functions. All right. So these excess risk is\nwith respect to capital",
    "start": "423100",
    "end": "428500"
  },
  {
    "text": "H, so this is defined to be your\npopulation loss or population",
    "start": "428500",
    "end": "434590"
  },
  {
    "text": "risk minus the best you\ncan find in this family.",
    "start": "434590",
    "end": "441160"
  },
  {
    "start": "441160",
    "end": "459448"
  },
  {
    "text": "OK. So basically, this\nterm is the best model. ",
    "start": "459448",
    "end": "467008"
  },
  {
    "text": "In H. Which H, sorry?",
    "start": "467008",
    "end": "472020"
  },
  {
    "text": "The inf. The inf. Yes. So this is a good question. So inf-- basically,\nyou can if you ask me.",
    "start": "472020",
    "end": "481030"
  },
  {
    "text": "I guess we just-- for this course, let's say\nthey are exactly the same. Of course, they are\nnot exactly the same",
    "start": "481030",
    "end": "487600"
  },
  {
    "text": "just because sometimes you don't\nhave a unique minimizer, right? So you have a sequence of--",
    "start": "487600",
    "end": "492970"
  },
  {
    "text": "like you can-- maybe\nI'll have a post",
    "start": "492970",
    "end": "498730"
  },
  {
    "text": "to explain the subtle\ndifferences between these two. But for almost-- I think for exactly\nfor this entire class,",
    "start": "498730",
    "end": "505000"
  },
  {
    "text": "you can just assume inf\nis the same as minimal. Yup. ",
    "start": "505000",
    "end": "511470"
  },
  {
    "text": "Cool. And this is a lot of 0,\nbecause this is the minimum.",
    "start": "511470",
    "end": "516690"
  },
  {
    "text": "And so that there's no\nway you can get better than the minimum. So that's why it's\nlarger than 0.",
    "start": "516690",
    "end": "523140"
  },
  {
    "text": "So in some sense, this\nis trying to kind of-- think about excess\nrisk as one way to kind of only think within\nthe family of H, right?",
    "start": "523140",
    "end": "532410"
  },
  {
    "text": "So now, if you\nget 0 excess risk, that means that you cannot do\nanything better within this",
    "start": "532410",
    "end": "538200"
  },
  {
    "text": "family. Right? Of course, if you\nchange your family, maybe you can get\nsomething else. But at least within\nthis family, there's",
    "start": "538200",
    "end": "543326"
  },
  {
    "text": "no way you can do better. OK. So this is the basic\nlanguage we have been",
    "start": "543327",
    "end": "549420"
  },
  {
    "text": "working for this entire course.  Any questions so far?",
    "start": "549420",
    "end": "554880"
  },
  {
    "text": "In any case, just feel free\nto interact me at any point. You don't have to wait until\nI pause in either the Zoom",
    "start": "554880",
    "end": "560580"
  },
  {
    "text": "meeting or here.  So some quick examples to make\nit less abstract, so this is--",
    "start": "560580",
    "end": "570650"
  },
  {
    "text": "I assume this is\nrelatively abstract. So one of the type of\nquestions in regression problem",
    "start": "570650",
    "end": "578080"
  },
  {
    "text": "where your y, the label\nset, is real number,",
    "start": "578080",
    "end": "583310"
  },
  {
    "text": "so they are continuous labels. And oftentimes, for\nregression problem",
    "start": "583310",
    "end": "589210"
  },
  {
    "text": "you have the\nso-called square loss. ",
    "start": "589210",
    "end": "595460"
  },
  {
    "text": "So this is the L y-hat\ny is equals to maybe something about half absolute\ny-hat minus y square.",
    "start": "595460",
    "end": "604698"
  },
  {
    "text": "For example, you want to\npredict the temperature, it makes sense to\nuse the square loss. Of course, there are other\ndifferent type of loss.",
    "start": "604698",
    "end": "610460"
  },
  {
    "text": "And another possibility\nis classification problem. ",
    "start": "610460",
    "end": "618380"
  },
  {
    "text": "So in this case, the\ny is a discrete site. You have a set of k labels.",
    "start": "618380",
    "end": "625490"
  },
  {
    "text": "Maybe it could be two\nlabels-- cat versus dog. Or it could be multiple labels. ",
    "start": "625490",
    "end": "632150"
  },
  {
    "text": "And then your loss-- the final loss you\ncare about often",
    "start": "632150",
    "end": "637850"
  },
  {
    "text": "is this so-called 0-1 loss. So basically, you say that if\nyou didn't get the right label,",
    "start": "637850",
    "end": "644930"
  },
  {
    "text": "then you are close to 1. Otherwise, you are close to 0. This 1 is the indicator.",
    "start": "644930",
    "end": "650899"
  },
  {
    "text": "So indicator E is\n1 if E happens. ",
    "start": "650900",
    "end": "657480"
  },
  {
    "text": "An indicator of\nE is 0 otherwise.",
    "start": "657480",
    "end": "662820"
  },
  {
    "text": "So you'll see that when you\nreally do the practical machine learning and\nalgorithm, you are not going to use this loss\nbecause of the other issues,",
    "start": "662820",
    "end": "669319"
  },
  {
    "text": "but this is the loss you\ncare about eventually. At least, this is\none of the losses you could care about eventually.",
    "start": "669320",
    "end": "674990"
  },
  {
    "text": "This is the so-called\naccuracy of the error, right? But when you tune it, you\nmaybe use cross entropy. That's a slightly\ndifferent question.",
    "start": "674990",
    "end": "682880"
  },
  {
    "text": "So OK. So now, this is the\nsetup and the goals.",
    "start": "682880",
    "end": "689080"
  },
  {
    "text": "And now, let's talk about\none important algorithm,",
    "start": "689080",
    "end": "695310"
  },
  {
    "text": "which is called empirical\nrisk minimization. This is the algorithm\nor type of algorithm",
    "start": "695310",
    "end": "701540"
  },
  {
    "text": "that we can analyze\nfor quite some time. So the algorithm is very simple.",
    "start": "701540",
    "end": "707160"
  },
  {
    "text": "So I guess this is what you\ndo in practice everyday. So you have some training loss.",
    "start": "707160",
    "end": "713292"
  },
  {
    "text": "Sometimes, it's\ncalled empirical loss. ",
    "start": "713292",
    "end": "719420"
  },
  {
    "text": "And sometimes, it's\ncalled empirical risk. So this loss, we use our l hat.\nl hat means it's empirical.",
    "start": "719420",
    "end": "726379"
  },
  {
    "text": "Every time we use\na hat here, it's kind of like-- it pretty\nmuch means empirical.",
    "start": "726380",
    "end": "731449"
  },
  {
    "text": "So you have the\nsum of the average of the loss and all the\nexamples, h i, y i from 1 to n.",
    "start": "731450",
    "end": "744490"
  },
  {
    "text": "And then, you do the so-called\nempirical risk minimization, ERM, empirical risk\nminimization, where h hat is--",
    "start": "744490",
    "end": "752800"
  },
  {
    "text": "you find the best\nmodel in this family.",
    "start": "752800",
    "end": "760050"
  },
  {
    "text": "I guess here I'm using argmin. So argmin are is just exactly\nthe same for this course.",
    "start": "760050",
    "end": "768180"
  },
  {
    "text": "So you find the best model\nwithin the family that minimizes your empirical risk.",
    "start": "768180",
    "end": "775519"
  },
  {
    "text": "And you can break\ntie arbitrarily. ",
    "start": "775520",
    "end": "780900"
  },
  {
    "text": "We don't care about\nbreaking ties in many cases. And so this is the algorithm.",
    "start": "780900",
    "end": "787840"
  },
  {
    "text": "So using this\nalgorithm, you may need to use some other optimizers\nto find the minimum, right?",
    "start": "787840",
    "end": "793423"
  },
  {
    "text": "But this is the abstract way\nof thinking of the algorithm. You'll find a minimum. ",
    "start": "793423",
    "end": "800550"
  },
  {
    "text": "And the key question\nis that, how do we-- why this is a good algorithm?",
    "start": "800550",
    "end": "805950"
  },
  {
    "text": "Why this is doing\nsomething sensible? And one of the key\nproperty of this is that--",
    "start": "805950",
    "end": "813390"
  },
  {
    "text": "one of the reason why this\nis somewhat meaningful is, I guess as you know\nalready from previous classes,",
    "start": "813390",
    "end": "819060"
  },
  {
    "text": "because it's x i,\ny i, i b from p.",
    "start": "819060",
    "end": "825860"
  },
  {
    "text": "So then, if you look\nat the expectation of the empirical loss over the\nrandomness of the examples--",
    "start": "825860",
    "end": "835760"
  },
  {
    "text": "so if you take expectation\nin particular of one example, let's say. And let's say the\nexamples are run,",
    "start": "835760",
    "end": "844038"
  },
  {
    "text": "then this is equal to\nthe population, right? This is exactly the same as\nyou draw x y from p, h x y.",
    "start": "844038",
    "end": "855029"
  },
  {
    "text": "To verify this is just\na change of notation. In some sense, this is average. So the expectation\nof the empirical loss",
    "start": "855030",
    "end": "861630"
  },
  {
    "text": "is the same as the-- so basically, it's saying\nthat if you take expectation",
    "start": "861630",
    "end": "868050"
  },
  {
    "text": "of the empirical\nloss, I will have h, which will be an average\nof all of this, 1 over n times",
    "start": "868050",
    "end": "877340"
  },
  {
    "text": "expectation l of h x y. ",
    "start": "877340",
    "end": "883430"
  },
  {
    "text": "All right, so this will\nbe equal to l of h. And here, the randomness comes\nfrom all the x i's and y i's.",
    "start": "883430",
    "end": "889330"
  },
  {
    "text": " So this is the\ntypical justification",
    "start": "889330",
    "end": "895803"
  },
  {
    "text": "we have for this\nkind of algorithm because the empirical\nloss is a good-- is an estimate for\nthe population loss.",
    "start": "895803",
    "end": "902680"
  },
  {
    "text": "That's why minimizing\nthe empirical loss probably would lead you to\nminimize the population loss.",
    "start": "902680",
    "end": "909500"
  },
  {
    "text": "So in some sense, at least\na good part of this course is to justify more\nformally why this is the right thing for us to do.",
    "start": "909500",
    "end": "915370"
  },
  {
    "text": "Intuitively, it sounds right. But formally,\nwhether this is-- we want to kind of prove that this\nis actually the right thing.",
    "start": "915370",
    "end": "922030"
  },
  {
    "text": "And it's actually not\nthat easy because it does depend on\nsome other things,",
    "start": "922030",
    "end": "927485"
  },
  {
    "text": "for example, how many\nexamples you have and how large your\nhypothesis class h is.",
    "start": "927485",
    "end": "932709"
  },
  {
    "text": "It's not that simple. This is just kind\nof an intuition.",
    "start": "932710",
    "end": "938710"
  },
  {
    "text": "So all right, any\nquestion so far?",
    "start": "938710",
    "end": "945960"
  },
  {
    "start": "945960",
    "end": "954310"
  },
  {
    "text": "And also, we're going to have-- I guess-- I assume that\nmost of you know this.",
    "start": "954310",
    "end": "961399"
  },
  {
    "text": "This is just a\nformal definition. So when you really do this,\nyou have a hypothesis class.",
    "start": "961400",
    "end": "967988"
  },
  {
    "text": "When you really\ndo it in computer, you have to have a\nparameterized family so that you can optimize the parameters.",
    "start": "967988",
    "end": "974050"
  },
  {
    "text": "So you can also have a\nparameterized family. So you call this H, for example,\nsomething like h is sub theta,",
    "start": "974050",
    "end": "981390"
  },
  {
    "text": "and where theta is in\nsome space of parameters. And maybe let's say theta\nis in RB or some kind",
    "start": "981390",
    "end": "990030"
  },
  {
    "text": "of like-- is the parameter.  And then, for example,\ntheta that could be--",
    "start": "990030",
    "end": "998810"
  },
  {
    "text": " so capital theta is the\nfamily of parameters.",
    "start": "998810",
    "end": "1005510"
  },
  {
    "text": "This is the-- sometimes you\nwant to say that you only do it",
    "start": "1005510",
    "end": "1016000"
  },
  {
    "text": "for sparse parameters\nor only do it for certain kind of parameters. ",
    "start": "1016000",
    "end": "1023140"
  },
  {
    "text": "And one of the example of this\nis that you can take h to be, for example, h theta x, which\nis equal to theta transpose x.",
    "start": "1023140",
    "end": "1030880"
  },
  {
    "text": "Then this is all\nthe linear models. OK, so this is easy. And then you can also do ERM\nfor parameterized family.",
    "start": "1030880",
    "end": "1044230"
  },
  {
    "text": " So I guess here, this\nis actually probably",
    "start": "1044230",
    "end": "1050510"
  },
  {
    "text": "the most important cases\nbecause in particular you do parameterized family.",
    "start": "1050510",
    "end": "1056875"
  },
  {
    "text": "And now your\ntraining loss, let's define these training loss\nstill as l hat theta, as l hat. But with a little\nabuse of notation,",
    "start": "1056875",
    "end": "1065240"
  },
  {
    "text": "you say that theta is your\ninput of the training loss. Theta is the parameter. Before we said,\nthe training loss is a function of the\nmodel, and now it's",
    "start": "1065240",
    "end": "1072274"
  },
  {
    "text": "a function of the\nparameter because the model and the parameter are just\na one to one correspondence,",
    "start": "1072275",
    "end": "1077300"
  },
  {
    "text": "in some sense. Maybe not one to one, but\nthey have a correspondence. So your representation\nfor the model",
    "start": "1077300",
    "end": "1084890"
  },
  {
    "text": "is really through\nthe parameters. So each parameter\ncorresponds to model. And this is just the--",
    "start": "1084890",
    "end": "1091730"
  },
  {
    "text": "I'm just writing what\nyou're expecting probably.",
    "start": "1091730",
    "end": "1098270"
  },
  {
    "text": "So this is the empirical loss. And here I'm overloading\nthe notation a little bit, and we are going to overload\nthis notation in this course",
    "start": "1098270",
    "end": "1105390"
  },
  {
    "text": "many times.  And sometimes you\nwrite this thing--",
    "start": "1105390",
    "end": "1113139"
  },
  {
    "text": "sometimes you write\nthis as alternatively, again with a little\nabuse of notation,",
    "start": "1113140",
    "end": "1120405"
  },
  {
    "text": "you sometimes\nwrite this as this. ",
    "start": "1120405",
    "end": "1125620"
  },
  {
    "text": "And just because\ntheta is what-- and x, and y, are what you care about,\nafter you know these three",
    "start": "1125620",
    "end": "1131350"
  },
  {
    "text": "things, you can compute a loss. These are just some notations.",
    "start": "1131350",
    "end": "1136450"
  },
  {
    "text": "Because we are sometimes\ngoing to use these notations. Sometimes we use these notations\na little bit exchangeably,",
    "start": "1136450",
    "end": "1142250"
  },
  {
    "text": "so it's good to\nbe aware of that. And you can define\nthe so-called ERM",
    "start": "1142250",
    "end": "1147970"
  },
  {
    "text": "solution, which is the\nargmin of the empirical loss. And where theta, is in this\nparameter, capital theta.",
    "start": "1147970",
    "end": "1157130"
  },
  {
    "text": "And sometimes you\njust write theta hat",
    "start": "1157130",
    "end": "1162890"
  },
  {
    "text": "as a shorthand for ERM,\nsometimes you can write this. But you don't have to remember\nsome of these kind of cases,",
    "start": "1162890",
    "end": "1171380"
  },
  {
    "text": "just we're going to\nremind you later. So in the goal, as\nyou can expect it, it's really, again, just to show\nthe excess risk of theta hat",
    "start": "1171380",
    "end": "1185030"
  },
  {
    "text": "ERM is small. Because that's the success\nkind of criterion, right?",
    "start": "1185030",
    "end": "1192020"
  },
  {
    "text": "You want to show that,\nyou find some theta hat,",
    "start": "1192020",
    "end": "1197390"
  },
  {
    "text": "and this theta hat is working\nand working in the sense that the excess risk is small.",
    "start": "1197390",
    "end": "1208460"
  },
  {
    "text": "And this is basically\nthe goal of the first probably few weeks.",
    "start": "1208460",
    "end": "1215179"
  },
  {
    "text": "And the core in some\nsense is really to, I guess kind of like a trailer.",
    "start": "1215180",
    "end": "1224429"
  },
  {
    "text": "In some sense, the core idea\nis to show that l theta is close to l hat theta, right?",
    "start": "1224430",
    "end": "1232240"
  },
  {
    "text": "Because you are minimizing\nthe l hat theta, but you care about l theta. So you have to show these two\nare similar in some sense,",
    "start": "1232240",
    "end": "1239120"
  },
  {
    "text": "but it's not that easy. Next question [INAUDIBLE]. ",
    "start": "1239120",
    "end": "1245519"
  },
  {
    "text": "Sorry this is-- I guess that's me. Sorry, this is me. My bad. ",
    "start": "1245520",
    "end": "1253049"
  },
  {
    "text": "Actually I have a typo here,\nyou might notice as well. Thanks. ",
    "start": "1253050",
    "end": "1260180"
  },
  {
    "text": "OK. [INAUDIBLE] goal again? So the goal is to show that\nyour algorithm works, right?",
    "start": "1260180",
    "end": "1267730"
  },
  {
    "text": "So this theta hat ERM is doing\nsomething, right, it's good.",
    "start": "1267730",
    "end": "1274780"
  },
  {
    "text": "And what does it mean\nby a model is good? A model is good\nin it means that,",
    "start": "1274780",
    "end": "1280269"
  },
  {
    "text": "at least in our\ndefinition, it really only means that the excess\nrisk is small, right? But if you can\nmake sure that you",
    "start": "1280270",
    "end": "1287050"
  },
  {
    "text": "are kind of close to getting\nthe best model in this family then that means\nyou are doing well.",
    "start": "1287050",
    "end": "1293200"
  },
  {
    "text": "So that's why the goal is\nto show that excess risk is small for this model. [INAUDIBLE]",
    "start": "1293200",
    "end": "1298830"
  },
  {
    "text": " Eventually you care about\nthe learning algorithm.",
    "start": "1298830",
    "end": "1305650"
  },
  {
    "text": "But to show this, it\ndoes depend on what the family of the hypothesis is.",
    "start": "1305650",
    "end": "1311679"
  },
  {
    "text": "But the final, final\ngoal is that you show a learning algorithm\nusing these family of models can work.",
    "start": "1311680",
    "end": "1317290"
  },
  {
    "text": " Do you ever actually\nevaluate l hat?",
    "start": "1317290",
    "end": "1323810"
  },
  {
    "text": "I assume it requires a sort of\ndistribution of l theta hat. So can you evaluate l? Yeah. Empirically, I guess, yeah.",
    "start": "1323810",
    "end": "1330299"
  },
  {
    "text": "So yes, you can evaluate\nl pretty well in the sense that you can have\na hold out of data. So that's why the\nvalidation data is used.",
    "start": "1330300",
    "end": "1339597"
  },
  {
    "text": "Of course, there are\nsome subtleties about-- OK, so how do you\nevaluate l if you want.",
    "start": "1339597",
    "end": "1345510"
  },
  {
    "text": "So the ideal scenario is that\nyou collect some new data,",
    "start": "1345510",
    "end": "1351750"
  },
  {
    "text": "and they are fresh data. And then you use the\nempirical estimator for it.",
    "start": "1351750",
    "end": "1358649"
  },
  {
    "text": "The subtlety would be\nthat whether you have seen this data before, right? If you haven't seen this data\nbefore, then you are all great.",
    "start": "1358650",
    "end": "1365130"
  },
  {
    "text": "But if you have seen this\ndata, then it becomes tricky. So that's actually exactly what\nwe are doing here because I hat",
    "start": "1365130",
    "end": "1371970"
  },
  {
    "text": "and l, right, so this is\nintuitively very much correct,",
    "start": "1371970",
    "end": "1377419"
  },
  {
    "text": "but the question is that--  we will talk more about this.",
    "start": "1377420",
    "end": "1383180"
  },
  {
    "text": "The subtlety is\nthat whether we have seen the data before or not. ",
    "start": "1383180",
    "end": "1391690"
  },
  {
    "text": "Any other questions? OK, cool. OK, sounds good.",
    "start": "1391690",
    "end": "1396940"
  },
  {
    "start": "1396940",
    "end": "1408039"
  },
  {
    "text": "And this is the kind of\nmain topic in this course. Although there are going\nto be more and more",
    "start": "1408040",
    "end": "1413830"
  },
  {
    "text": "subtleties about\nthis, for example, like in the first few weeks,\nwe're going to talk about this. And then other things\nin this course--",
    "start": "1413830",
    "end": "1420655"
  },
  {
    "text": " so we're going to talk\nabout how to for example,",
    "start": "1420655",
    "end": "1428340"
  },
  {
    "text": "one thing is how to\nminimize l hat theta, right? So suppose you know that\nall of this is great,",
    "start": "1428340",
    "end": "1437300"
  },
  {
    "text": "but you still want\nto know how do you do this in a computationally\nefficient way, right?",
    "start": "1437300",
    "end": "1442750"
  },
  {
    "text": "That's something we're going\nto touch on for a few lectures.",
    "start": "1442750",
    "end": "1448360"
  },
  {
    "text": "And also we're going to talk\nabout additional complications in some sense in deep learning.",
    "start": "1448360",
    "end": "1457420"
  },
  {
    "text": "In some sense, this framework\nbecomes questionable when you do deep learning. Of course, some part\nof it still survives,",
    "start": "1457420",
    "end": "1463539"
  },
  {
    "text": "actually most of the part\nsurvives, but some of the-- if you really go into the\nlow level technical stuff,",
    "start": "1463540",
    "end": "1468789"
  },
  {
    "text": "then some of the technical stuff\nstops kind of making sense,",
    "start": "1468790",
    "end": "1473875"
  },
  {
    "text": "and there are a lot of\nadditional complications, right? So far everything\nis still kind of OK,",
    "start": "1473875",
    "end": "1479980"
  },
  {
    "text": "but then once you go\none level lower then some of the classical techniques\ndon't apply to deep learning.",
    "start": "1479980",
    "end": "1486623"
  },
  {
    "text": "And also we're going to talk\na little bit about enterprise learning, which is\nsomewhat different,",
    "start": "1486623",
    "end": "1493420"
  },
  {
    "text": "but still some of these losses\nare involved, of course. And the transition,\nall of this-- ",
    "start": "1493420",
    "end": "1500440"
  },
  {
    "text": "like the notation\nstill mostly applies, but with a little\nbit of differences. OK, so that's the formulation.",
    "start": "1500440",
    "end": "1509320"
  },
  {
    "text": "And now let's move\non to asymptotics.",
    "start": "1509320",
    "end": "1515350"
  },
  {
    "text": "Before that, any questions? ",
    "start": "1515350",
    "end": "1527909"
  },
  {
    "text": "OK, cool.  So what does asymptotical\nanalysis mean, right?",
    "start": "1527910",
    "end": "1537090"
  },
  {
    "text": "So this is a type of\nanalysis where you assume that n goes to infinity.",
    "start": "1537090",
    "end": "1547980"
  },
  {
    "text": "So like n, the number of\nexamples, goes to infinity. And you show a bound\nlike this of the form.",
    "start": "1547980",
    "end": "1558405"
  },
  {
    "text": " Something like excess risk,\nthis is our goal, which",
    "start": "1558405",
    "end": "1564880"
  },
  {
    "text": "is l theta hat minus argmin-- sorry minus mean.",
    "start": "1564880",
    "end": "1570555"
  },
  {
    "start": "1570555",
    "end": "1577990"
  },
  {
    "text": "This is less than c over\nn, plus little [INAUDIBLE]..",
    "start": "1577990",
    "end": "1584220"
  },
  {
    "text": "And here this constant\nc, is a constant,",
    "start": "1584220",
    "end": "1589603"
  },
  {
    "text": "but it's not a\nuniversal constant. It's a constant that, of\ncourse, that depends on n,",
    "start": "1589603",
    "end": "1599990"
  },
  {
    "text": "but could depend on the problem.",
    "start": "1599990",
    "end": "1607410"
  },
  {
    "text": " For example, dimension, right?",
    "start": "1607410",
    "end": "1614259"
  },
  {
    "text": "So and a little on\nthis, kind of what you learn from calculus,\nthis is a lower level",
    "start": "1614260",
    "end": "1620140"
  },
  {
    "text": "term compared to 1 over n. So this is kind of the\ngeneral kind of approach.",
    "start": "1620140",
    "end": "1626170"
  },
  {
    "text": "And after we talk about\nthis, I will talk about-- then we're going to move on\nthe so-called non-asymptotic",
    "start": "1626170",
    "end": "1632320"
  },
  {
    "text": "approach, which, I will discuss\nthat after we talk about this. ",
    "start": "1632320",
    "end": "1638748"
  },
  {
    "text": "OK, so-- There's a question. [INAUDIBLE] ",
    "start": "1638748",
    "end": "1650889"
  },
  {
    "text": "Shall we close door? I don't know. ",
    "start": "1650890",
    "end": "1664390"
  },
  {
    "text": "Could you be a little quieter? ",
    "start": "1664390",
    "end": "1671070"
  },
  {
    "text": "Yeah, I think that\none probably is fine. Anyway, yeah, that's\na good question.",
    "start": "1671070",
    "end": "1679590"
  },
  {
    "text": "Yeah, so I think while\nwe care about this bound. So we want a bound that goes\nto 0 as n goes to infinity.",
    "start": "1679590",
    "end": "1686820"
  },
  {
    "text": "Because you want\nto say that, if you have more and more examples,\nyou can do better and better. But whether it's 1 over\nn, or 1 over squared n,",
    "start": "1686820",
    "end": "1693570"
  },
  {
    "text": "or 1 over over n\nsquare, that depends on what's the truth, right? So it just turns out that\nthe right bound is 1 over n",
    "start": "1693570",
    "end": "1701670"
  },
  {
    "text": "as we'll see. You cannot get better. You shouldn't get worse. Of course, it still depends\non the setting a little bit.",
    "start": "1701670",
    "end": "1708850"
  },
  {
    "text": "But for the setting, we're\ngoing to talk about well over a is indeed the right path. Yep, so cool.",
    "start": "1708850",
    "end": "1717732"
  },
  {
    "text": "OK, so now let's get into a\nlittle more concrete set up. ",
    "start": "1717732",
    "end": "1724670"
  },
  {
    "text": "So we are going to write\nthis theta is our p. This is our family\nof parameters.",
    "start": "1724670",
    "end": "1731149"
  },
  {
    "text": "And theta hat is-- I'm writing this again,\nso but just to rewrite it,",
    "start": "1731150",
    "end": "1738409"
  },
  {
    "text": "this is the ERM solution. And let's just for notational\nconvenience, let's define theta",
    "start": "1738410",
    "end": "1744890"
  },
  {
    "text": "star to be the best model\nin this family, right?",
    "start": "1744890",
    "end": "1753830"
  },
  {
    "text": "But this is the population\nrisk but not empirical risk. Theta star is the best in\nterms of the population,",
    "start": "1753830",
    "end": "1761160"
  },
  {
    "text": "and our goal is to bound the\nexcess risk, which will be just l theta minus l theta sub.",
    "start": "1761160",
    "end": "1767870"
  },
  {
    "text": "OK, excess risk. So our goal is to show\nl theta minus l-- sorry,",
    "start": "1767870",
    "end": "1774270"
  },
  {
    "text": "I theta hat minus l\ntheta star is small. ",
    "start": "1774270",
    "end": "1784020"
  },
  {
    "text": "OK. And a trivial\nconsequence of this",
    "start": "1784020",
    "end": "1789580"
  },
  {
    "text": "is that l theta star\nis the mean of l theta. Find this.",
    "start": "1789580",
    "end": "1795060"
  },
  {
    "start": "1795060",
    "end": "1802600"
  },
  {
    "text": "OK, so here's the theorem\nthat we are going to prove. So typically in this\ncourse, I'm going",
    "start": "1802600",
    "end": "1808210"
  },
  {
    "text": "to take this approach that\nwe state the theorem first, and then talk about why\nwe have to prove it, or how do we prove it.",
    "start": "1808210",
    "end": "1817330"
  },
  {
    "text": "So we assume the consistency. By the way, this is like--",
    "start": "1817330",
    "end": "1823840"
  },
  {
    "text": "as with what I said\nin the beginning, this part of the lecture\nis a little bit informal",
    "start": "1823840",
    "end": "1830308"
  },
  {
    "text": "just because I don't want to\nget into too much trouble. Too many [INAUDIBLE].",
    "start": "1830308",
    "end": "1835630"
  },
  {
    "text": "So what does the consistency\nof theta hat mean? So this means that\ntheta hat eventually converts to theta\nstar in probability,",
    "start": "1835630",
    "end": "1843190"
  },
  {
    "text": "as n goes to infinity. If you are not familiar with\nwhat convergence in probability",
    "start": "1843190",
    "end": "1848680"
  },
  {
    "text": "means, it doesn't\nreally matter that much. So the reason why\nyou have to have something slightly\ndifferent is because this",
    "start": "1848680",
    "end": "1854650"
  },
  {
    "text": "is a random variable. Theta hat is a random variable. If it's just some deterministic\nvariable as a function of n,",
    "start": "1854650",
    "end": "1862270"
  },
  {
    "text": "then you can define a\nconvergence in the trivial way. But here theta hat\nis a random variable. So technically this means\nconvergence in probability,",
    "start": "1862270",
    "end": "1875000"
  },
  {
    "text": "just in case you are\ninterested in this, but it's not that important. So convergence in\nprobability means",
    "start": "1875000",
    "end": "1880670"
  },
  {
    "text": "that if you take a limit,\nas n goes to infinity, we look at the probability\nof theta hat minus theta star",
    "start": "1880670",
    "end": "1890419"
  },
  {
    "text": "is larger than some epsilon. Its probability will go to\n0 as n goes to infinity,",
    "start": "1890420",
    "end": "1896149"
  },
  {
    "text": "for any epsilon\nthat's larger than 0. But it's not very\nimportant for this course.",
    "start": "1896150",
    "end": "1901669"
  },
  {
    "text": "For this course,\nit's perfectly fine that you just understand\nthis intuitively. Theta hat is a random variable\nbecause [INAUDIBLE] depends",
    "start": "1901670",
    "end": "1908780"
  },
  {
    "text": "on the probability\ndistribution-- Yeah. Exactly. Is it except for the\nfact that the math",
    "start": "1908780",
    "end": "1914867"
  },
  {
    "text": "from the set of theta hat\nis like measurable, correct?",
    "start": "1914867",
    "end": "1920710"
  },
  {
    "text": "Sorry, can you say that again? Something like a map from\nthe samples of theta hat is measurable. Yeah, we have all of those.",
    "start": "1920710",
    "end": "1926840"
  },
  {
    "text": "Yeah.  When you're writing\nin landscape,",
    "start": "1926840",
    "end": "1932132"
  },
  {
    "text": "the stuff was a bit\nbigger on the board. Would it be possible to- ",
    "start": "1932133",
    "end": "1938660"
  },
  {
    "text": "Yes, but I think the\nissue is that it's going to be smaller vertically.",
    "start": "1938660",
    "end": "1945530"
  },
  {
    "text": "So I felt that this is better\nI think because there is more things shown on the board.",
    "start": "1945530",
    "end": "1951280"
  },
  {
    "text": "Could you maybe\nwrite a bit bigger? Bigger? Yes, sure. That's fine. Maybe I should also\nrepeat the question",
    "start": "1951280",
    "end": "1957730"
  },
  {
    "text": "for the Zoom meeting as\nwell, but yeah, next time. OK, cool. ",
    "start": "1957730",
    "end": "1965190"
  },
  {
    "text": "And also this in formulae-- OK, sounds good. So and then, let's see.",
    "start": "1965190",
    "end": "1970320"
  },
  {
    "text": "So we also assume that the\nHessian of the loss h theta",
    "start": "1970320",
    "end": "1983529"
  },
  {
    "text": "star is full rank. And what does the Hessian mean?",
    "start": "1983530",
    "end": "1989750"
  },
  {
    "text": "The Hessian is--\nprobably most of you have seen Hessian if\nyou have taken CS29,",
    "start": "1989750",
    "end": "1996720"
  },
  {
    "text": "but Hessian is just a\nsecond of derivative, but you organize\nit in the matrix. So the Hessian of a\nfunction f is a matrix.",
    "start": "1996720",
    "end": "2003590"
  },
  {
    "text": "And its matrix, all the answers\nare the partial derivatives of this.",
    "start": "2003590",
    "end": "2009824"
  },
  {
    "text": "All right, this is a\nmatrix of dimension p by p if f is a function\nthat maps r p to r.",
    "start": "2009824",
    "end": "2021100"
  },
  {
    "text": "OK, so and there's also some\nother regularity conditions",
    "start": "2021100",
    "end": "2031190"
  },
  {
    "text": "which I'm not\ngoing to even state because it's probably not super\nimportant for this course.",
    "start": "2031190",
    "end": "2039889"
  },
  {
    "text": "And for example, this involves\nsomething like the gradient is finite, something like that.",
    "start": "2039890",
    "end": "2047960"
  },
  {
    "text": "And under these assumptions,\nthen you have a few things. You can know a lot of\nthings about the theta hat.",
    "start": "2047960",
    "end": "2055469"
  },
  {
    "text": "So first thing you know is that\nformula you have to write this.",
    "start": "2055469",
    "end": "2060590"
  },
  {
    "text": "So with square root n times\ntheta hat minus theta star. This is bounded, this op of 1.",
    "start": "2060590",
    "end": "2068658"
  },
  {
    "text": "So I'm going to define\nop of 1 in a moment. But this is really\nroughly speaking just the saying that theta\nhat minus theta star",
    "start": "2068659",
    "end": "2076250"
  },
  {
    "text": "is roughly on the order of\nlike 1 over square root of 2. Something like this.",
    "start": "2076250",
    "end": "2081840"
  },
  {
    "text": "So if you multiply theta\nhat minus theta star by square root n,\nthen it becomes on an order of a constant.",
    "start": "2081840",
    "end": "2087440"
  },
  {
    "text": "So what is this op of 1 here? Again, this is not super\nimportant for the course.",
    "start": "2087440",
    "end": "2092480"
  },
  {
    "text": "You can, if you don't\nthink of it as constant, you can just think of as o of\n1 as in most of the standard CS",
    "start": "2092480",
    "end": "2098340"
  },
  {
    "text": "courses. But the detail here is that\nbounded in probability,",
    "start": "2098340",
    "end": "2105380"
  },
  {
    "text": "xn is a random variable and\nindexed by n is op of 1. This means that for every\nepsilon that is not a 0,",
    "start": "2105380",
    "end": "2119150"
  },
  {
    "text": "there exists a bound n,\nsuch that if you look",
    "start": "2119150",
    "end": "2124220"
  },
  {
    "text": "at the probability that xn\nis bigger than the bound,",
    "start": "2124220",
    "end": "2130520"
  },
  {
    "text": "this would go to 0.  I guess for sup, you\ncan think of it as max.",
    "start": "2130520",
    "end": "2137240"
  },
  {
    "text": "If you are not\nfamiliar with the sup, this is going to be\nvery small eventually",
    "start": "2137240",
    "end": "2143420"
  },
  {
    "text": "as n goes to infinity. But if you are not familiar\nwith all of these jargons,",
    "start": "2143420",
    "end": "2150190"
  },
  {
    "text": "just think of this as o of 1. [INAUDIBLE] minimizer is unique?",
    "start": "2150190",
    "end": "2160210"
  },
  {
    "text": "Yes. Actually, the minimizer is\nunique is already assumed when I defined this, in some sense.",
    "start": "2160210",
    "end": "2166539"
  },
  {
    "text": "So again, I'm pretty\ninformal here, but I'm already assuming\nthat the minimizer is unique.",
    "start": "2166540",
    "end": "2171700"
  },
  {
    "text": "But indeed, if the\nminimizer is unique, I think you need\nHessian to be full rank.",
    "start": "2171700",
    "end": "2177622"
  },
  {
    "text": "But I think Hessian\nis full rank doesn't mean the minimizer is unique. ",
    "start": "2177623",
    "end": "2185180"
  },
  {
    "text": "OK, sounds good. So any other questions? But the most\nimportant thing here",
    "start": "2185180",
    "end": "2191809"
  },
  {
    "text": "is that you somehow know\nhow far theta hat is close to theta star. And how far it is,\nit's kind of something",
    "start": "2191810",
    "end": "2196880"
  },
  {
    "text": "like 1 over square root n\nand as n goes to infinity. And then also you know that\nhow different l theta hat,",
    "start": "2196880",
    "end": "2205490"
  },
  {
    "text": "the population risk of\nthe minimizer, theta hat, is close to the population risk\nof the best model, theta star.",
    "start": "2205490",
    "end": "2216780"
  },
  {
    "text": "And how different they are? So they are different,\nin this sense, where if you multiply\nthe difference by n, then",
    "start": "2216780",
    "end": "2223370"
  },
  {
    "text": "you get a constant, which\npretty much is saying that l theta hat\nminus l theta star",
    "start": "2223370",
    "end": "2228950"
  },
  {
    "text": "is something like 1 over n. OK, so and actually\nmore on this.",
    "start": "2228950",
    "end": "2238720"
  },
  {
    "text": "So you also know that what's\nthe distribution of this theta hat minus theta star.",
    "start": "2238720",
    "end": "2245380"
  },
  {
    "text": "So theta hat minus theta\nstar is a vector, right? And if it's multiplied\nby square root n,",
    "start": "2245380",
    "end": "2250829"
  },
  {
    "text": "it's going to be on\nthe order of constant. But also you know\nwhat the distribution of this random variable is.",
    "start": "2250830",
    "end": "2256920"
  },
  {
    "text": "As n goes to infinity, I\nthink this distribution is, in distribution, is\nconverging to a Gaussian",
    "start": "2256920",
    "end": "2265079"
  },
  {
    "text": "distribution, which means\n0 and some covariance. And this covariance\nis complicated,",
    "start": "2265080",
    "end": "2270640"
  },
  {
    "text": "let me write it down. ",
    "start": "2270640",
    "end": "2290250"
  },
  {
    "text": "Something like this. By the way, all of these\nare in the lecture notes. So you don't necessarily\nhave to take notes",
    "start": "2290250",
    "end": "2295830"
  },
  {
    "text": "if you don't want it. Anyway, right, so how to\ninterpret this covariance,",
    "start": "2295830",
    "end": "2307620"
  },
  {
    "text": "I think that's-- it's not interpretable\nfor the moment. But the point is that it's\na Gaussian distribution,",
    "start": "2307620",
    "end": "2313799"
  },
  {
    "text": "and after scaling\nby square root n. So if you don't scale\nby square root n,",
    "start": "2313800",
    "end": "2318840"
  },
  {
    "text": "it's going to be\nsmaller and smaller. But if you scale\nby square root n, then it's going to be a Gaussian\ndistribution with a fixed",
    "start": "2318840",
    "end": "2325500"
  },
  {
    "text": "covariance. And it means 0, so theta hat\nis centered around theta star, so that's very good news.",
    "start": "2325500",
    "end": "2332200"
  },
  {
    "text": "And also, the first\nthing you also know something\nabout how different was the distribution\nfor excess risk.",
    "start": "2332200",
    "end": "2338520"
  },
  {
    "text": "We have talked about\nthe excess risk as a random variable\nis on the order of 1 over square root, 1\nover n here, right?",
    "start": "2338520",
    "end": "2345300"
  },
  {
    "text": "So this is what we\nhave talked about. But then also you know exactly\nwhat's the distribution. So the distribution is, it's\nactually a complicated stage,",
    "start": "2345300",
    "end": "2352980"
  },
  {
    "text": "but let me do it. So first you define\na random variable.",
    "start": "2352980",
    "end": "2358030"
  },
  {
    "text": "Let's call it a Gaussian random\nvariable with some covariance.",
    "start": "2358030",
    "end": "2363450"
  },
  {
    "start": "2363450",
    "end": "2370585"
  },
  {
    "text": "The exact details\nhere also don't matter that much because it\ncomes from the derivation. You derived it, and you\nfound that this is exactly",
    "start": "2370585",
    "end": "2378083"
  },
  {
    "text": "the right thing. So the point here is that if\nyou define this random variable,",
    "start": "2378083",
    "end": "2383820"
  },
  {
    "text": "then you can know that\nexcess risk, which is",
    "start": "2383820",
    "end": "2389850"
  },
  {
    "text": "l theta hat minus l theta star. If you multiply that by 1\nover n, then in distribution,",
    "start": "2389850",
    "end": "2396390"
  },
  {
    "text": "it converges to this\nrandom variable, the norm of this\nrandom variable s.",
    "start": "2396390",
    "end": "2402780"
  },
  {
    "text": "s is the Gaussian distribution. And you also know what's\nthe expectation of this.",
    "start": "2402780",
    "end": "2409110"
  },
  {
    "text": "If you really want, which is\nsomething on the order of 1",
    "start": "2409110",
    "end": "2414810"
  },
  {
    "text": "over 2n. And you also know,\nwas the constant.",
    "start": "2414810",
    "end": "2420135"
  },
  {
    "start": "2420135",
    "end": "2428126"
  },
  {
    "text": "OK, so all of these\nformulas don't necessarily matter that much because you\nderive it, you got this, right?",
    "start": "2428126",
    "end": "2434507"
  },
  {
    "text": "But the point is that you\nalmost know everything. So you know everything\nabout l theta hat. You know the distribution\nof theta hat.",
    "start": "2434508",
    "end": "2439790"
  },
  {
    "text": "You know l theta hat. You know the distribution\nof l theta hat. It's very powerful.",
    "start": "2439790",
    "end": "2447020"
  },
  {
    "text": "And you can make all of\nthese formal if you want. ",
    "start": "2447020",
    "end": "2454080"
  },
  {
    "text": "Any questions so far? The first assumption [INAUDIBLE]\nproperty of [INAUDIBLE]..",
    "start": "2454080",
    "end": "2461282"
  },
  {
    "text": " Is that a property of what?",
    "start": "2461282",
    "end": "2466555"
  },
  {
    "text": "[INAUDIBLE] ",
    "start": "2466555",
    "end": "2475550"
  },
  {
    "text": "Yeah, so my understanding\nis the question is that, what the\nconsistency assumption was?",
    "start": "2475550",
    "end": "2481125"
  },
  {
    "text": "Is that a property\nof something, right? So what property like, is this\nthe property of the problem?",
    "start": "2481125",
    "end": "2487400"
  },
  {
    "text": "Yes, that's correct. So it's a property\nof the problem, meaning that it's a property\nof the model parameterization.",
    "start": "2487400",
    "end": "2498089"
  },
  {
    "text": "Yeah.  So this might\nanswer the question.",
    "start": "2498090",
    "end": "2503200"
  },
  {
    "text": "I have no idea how we would do\nthis equation from a Gaussian.",
    "start": "2503200",
    "end": "2508359"
  },
  {
    "text": "I'm not following [INAUDIBLE]. ",
    "start": "2508360",
    "end": "2515400"
  },
  {
    "text": "Sorry, you are not\nfollowing why this is true? So what are some other\nmaterials that can be-.",
    "start": "2515400",
    "end": "2521420"
  },
  {
    "text": " I guess maybe you can talk\nabout this offline, it's OK.",
    "start": "2521420",
    "end": "2527790"
  },
  {
    "text": "Yeah, just come to\nme after the course. Yeah. But you're not expected to,\njust one thing for anybody,",
    "start": "2527790",
    "end": "2534170"
  },
  {
    "text": "you are not expected to see\nwhy these are true, right? These are just some statements\nsaying that, OK, this can be done mathematically.",
    "start": "2534170",
    "end": "2539900"
  },
  {
    "text": "I will show you something\nabout how to draft this, at least somewhat informally.",
    "start": "2539900",
    "end": "2545390"
  },
  {
    "text": "And the proof of actual\ntechniques is pretty simple. The calculation is\na little bit tricky. It's a little bit complicated. You have to work through it.",
    "start": "2545390",
    "end": "2551688"
  },
  {
    "text": "But the fundamental\nidea is very simple.  Yeah, so far I'm only stating\nthat these are all correct.",
    "start": "2551688",
    "end": "2560267"
  },
  {
    "text": "You can prove all of this. That's the only thing\nI'm saying so far. Are these [INAUDIBLE]\nvery strong,",
    "start": "2560267",
    "end": "2566680"
  },
  {
    "text": "or are they easily\nverified by any problem? Yeah, so for example, the\nconsistency assumption, right?",
    "start": "2566680",
    "end": "2572950"
  },
  {
    "text": "Yes, so that's a very\ngood question, right? So far we see this very\nstrong statement, everything about theta hat, right?",
    "start": "2572950",
    "end": "2577958"
  },
  {
    "text": "So some things probably\nshould go wrong because otherwise\nwe would probably solve all the problems. There's no non-linear\nassumption.",
    "start": "2577958",
    "end": "2583990"
  },
  {
    "text": "It works for\nnonlinearity, right? So I think the problem is that\nthe consistency assumption is",
    "start": "2583990",
    "end": "2590260"
  },
  {
    "text": "a little bit tricky if you\ndon't have n goes to infinity. You really have to have n\nto be really, really big.",
    "start": "2590260",
    "end": "2597370"
  },
  {
    "text": "Then you can somewhat\nhave the consistency. And I think basically,\nthe whole thing--",
    "start": "2597370",
    "end": "2605500"
  },
  {
    "text": "the whole problem of this-- the limitation of\nthis theorem is that you need to let\nn goes to infinity,",
    "start": "2605500",
    "end": "2611200"
  },
  {
    "text": "and you really need\nvery, very big n to potentially see this effect. So we're going to\ndiscuss this a little bit",
    "start": "2611200",
    "end": "2617290"
  },
  {
    "text": "after we move on to\nthe non-asymptotics. But yeah, that's a trailer.",
    "start": "2617290",
    "end": "2623690"
  },
  {
    "text": "Yeah.  Right, so when n\ngoes to infinity,",
    "start": "2623690",
    "end": "2629560"
  },
  {
    "text": "you have super powerful\ntools, in some sense. But still these are actually\nreasonable characterizations",
    "start": "2629560",
    "end": "2636910"
  },
  {
    "text": "for minor cases. So it's not like they are\ncompletely off the reality.",
    "start": "2636910",
    "end": "2642160"
  },
  {
    "text": "I guess they are not\nnecessarily that applicable to the modern practice\njust because in these days we don't have n goes\nto infinity, right?",
    "start": "2642160",
    "end": "2648880"
  },
  {
    "text": "You have a million data\npoints in your ImageNet but your parameters\nare like 10 million. So n is not going to infinity\nas you fix the parameter.",
    "start": "2648880",
    "end": "2657172"
  },
  {
    "text": "So that's going to\nbe the next half of the lecture to some extent.",
    "start": "2657172",
    "end": "2662640"
  },
  {
    "text": "[INAUDIBLE] one or two or-- [INAUDIBLE]",
    "start": "2662640",
    "end": "2668110"
  },
  {
    "text": "Yeah. One and two are consequences\nof three and four, yeah. And actually when\nwe really prove it,",
    "start": "2668110",
    "end": "2674470"
  },
  {
    "text": "if we do a very\nformal proof, you're going to prove three\nand four first, and then do one and two, yeah.",
    "start": "2674470",
    "end": "2681108"
  },
  {
    "text": "OK, I think I have\n15 minutes, right? Yeah, 15 minutes.",
    "start": "2681108",
    "end": "2686210"
  },
  {
    "start": "2686210",
    "end": "2692137"
  },
  {
    "text": "Yeah, OK, so what I'm going\nto do in the next 15 minutes is to show kind of an informal\nproof for one and two.",
    "start": "2692137",
    "end": "2701447"
  },
  {
    "text": "And next time I'm going to\ndo a little more formal proof of three and four,\nand then we're going to get done\nwith this asymptotics.",
    "start": "2701447",
    "end": "2707620"
  },
  {
    "text": "And then we'll move on to the\nmore non-asymptotic stuff. ",
    "start": "2707620",
    "end": "2715660"
  },
  {
    "text": "So this is actually\nthe proof, right? So the key of the\nproof is two things. One of the things\nis that you're going",
    "start": "2715660",
    "end": "2722200"
  },
  {
    "text": "to do tail expansion\naround to the star.",
    "start": "2722200",
    "end": "2730440"
  },
  {
    "text": "And second thing\nis that you want to somehow use the fact\nthat I hat is close to l,",
    "start": "2730440",
    "end": "2735600"
  },
  {
    "text": "and nabla l hat is\nclose to nabla l. Nabla l hat is the gradient,\nthe empirical gradient,",
    "start": "2735600",
    "end": "2740730"
  },
  {
    "text": "and nabla l is the\npopulation gradient. And this is by law\nof large numbers. ",
    "start": "2740730",
    "end": "2748380"
  },
  {
    "text": "OK, I'll elaborate on this. But the most important thing\nis really the tail expansion, right? So once you can work in that\nneighborhood of something",
    "start": "2748380",
    "end": "2756120"
  },
  {
    "text": "then everything becomes\nsomewhat easy, OK? So now let's talk about\nhow to really do it.",
    "start": "2756120",
    "end": "2762900"
  },
  {
    "text": " So when you do tail expansion,\nso the starting point",
    "start": "2762900",
    "end": "2769789"
  },
  {
    "text": "is the following. So you care about\ntheta hat, right? And what you know\nabout theta hat is that 0 is equal\nto nabla hat l--",
    "start": "2769790",
    "end": "2777290"
  },
  {
    "text": " 0 is equals to-- the gradient of the\nempirical loss at theta hat",
    "start": "2777290",
    "end": "2786260"
  },
  {
    "text": "is equals to 0. This is because theta hat\nis the minimizer, right? And minimizer then, the\nstationary condition",
    "start": "2786260",
    "end": "2792950"
  },
  {
    "text": "tells you that\nthe gradient is 0. But you want to relate this\nto l because everything",
    "start": "2792950",
    "end": "2798890"
  },
  {
    "text": "is easier when you do it with\nl, because I is the population.",
    "start": "2798890",
    "end": "2804559"
  },
  {
    "text": "First relate this to theta star. So you want to relate\neverything-- basically,",
    "start": "2804560",
    "end": "2810680"
  },
  {
    "text": "the whole idea is that you\nwant to relate theta hat to theta star and l hat to l. So the first thing\nis that we try",
    "start": "2810680",
    "end": "2816200"
  },
  {
    "text": "to relate this to theta star. So you can write this as, theta\nexpands around theta star.",
    "start": "2816200",
    "end": "2824400"
  },
  {
    "text": "So theta star is\na reference point. And then this first-order term,\nthis is a zeroth-order term,",
    "start": "2824400",
    "end": "2831057"
  },
  {
    "text": "and the first-order\nterm will be the Hessian of the empirical loss times\ntheta hat minus theta star.",
    "start": "2831057",
    "end": "2837630"
  },
  {
    "text": "So this is the higher-order-- this is the tail expansion for\nmulti-dimensional function,",
    "start": "2837630",
    "end": "2846780"
  },
  {
    "text": "but it's exactly the same\nas one dimensional case.",
    "start": "2846780",
    "end": "2852990"
  },
  {
    "text": "It's just that you have to\ndeal with some of the matrices. So maybe just a\nsmall remark here.",
    "start": "2852990",
    "end": "2859180"
  },
  {
    "text": "So what I'm doing here is\nthat I'm expanding something like gradient of g, g plus\nepsilon, abstractly speaking.",
    "start": "2859180",
    "end": "2866279"
  },
  {
    "text": "I'm going to do a lot of these\nabstractions for small things,",
    "start": "2866280",
    "end": "2871610"
  },
  {
    "text": "right? So suppose you care about this,\nand epsilon is a small thing, and z is your\nreference point, you",
    "start": "2871610",
    "end": "2877410"
  },
  {
    "text": "can show that the tail\nexpansion for this is really something like nabla\nc plus nabla square root g",
    "start": "2877410",
    "end": "2886050"
  },
  {
    "text": "z times epsilon. And this is a matrix. And this is the vector.",
    "start": "2886050",
    "end": "2892010"
  },
  {
    "text": " And how do you verify this? You can do this for each\ndimension individually,",
    "start": "2892010",
    "end": "2900300"
  },
  {
    "text": "and you get this equation. This is intuitive\nas well, right,",
    "start": "2900300",
    "end": "2906880"
  },
  {
    "text": "because the Hessian is the\ngradient of the gradient.",
    "start": "2906880",
    "end": "2912549"
  },
  {
    "text": "So this is the first-order\ntail expansion, OK? Any questions?",
    "start": "2912550",
    "end": "2920470"
  },
  {
    "text": "OK, so now after I do\nthe tail expansion, then you know that, this\nis-- the left hand side is 0.",
    "start": "2920470",
    "end": "2926110"
  },
  {
    "text": "And then you can\nrearrange, right? So put this on the\nleft hand side.",
    "start": "2926110",
    "end": "2931880"
  },
  {
    "text": "So what you get is that\nnabla l theta star,",
    "start": "2931880",
    "end": "2937690"
  },
  {
    "text": "theta hat minus theta star. It is roughly equal to minus--",
    "start": "2937690",
    "end": "2946770"
  },
  {
    "text": "sorry, this is equal\nto minus [INAUDIBLE] l hat theta star, plus\nsome higher-order terms",
    "start": "2946770",
    "end": "2952980"
  },
  {
    "text": "And then, now you have\ntheta hat minus theta star. You can take the\ninverse of the Hessian. So you have theta\nhat minus theta star",
    "start": "2952980",
    "end": "2959970"
  },
  {
    "text": "is equal to the\ninverse of the Hessian times the empirical gradient\nat theta star plus higher order",
    "start": "2959970",
    "end": "2969070"
  },
  {
    "text": "terms. Total terms. ",
    "start": "2969070",
    "end": "2975040"
  },
  {
    "text": "OK? [INAUDIBLE] ",
    "start": "2975040",
    "end": "2981029"
  },
  {
    "text": "Sorry, that's my bad. It's still l hat so far. Thanks.",
    "start": "2981030",
    "end": "2986984"
  },
  {
    "text": "OK, cool. OK, so that's exactly\nthe right point.",
    "start": "2986984",
    "end": "2992740"
  },
  {
    "text": "So now I need to change\nall the l hats to l.",
    "start": "2992740",
    "end": "2997890"
  },
  {
    "text": "And what I know-- so I know that basically, I want\nto kind of change this to l.",
    "start": "2997890",
    "end": "3005570"
  },
  {
    "text": "I want to change this to\nl hat, to nabla l as well. And also I need to consider\nthe differences between them.",
    "start": "3005570",
    "end": "3015090"
  },
  {
    "text": "So how do I do that?  So at least I know\na few things, right?",
    "start": "3015090",
    "end": "3021050"
  },
  {
    "text": "I know that expectation\nof l hat theta star",
    "start": "3021050",
    "end": "3026520"
  },
  {
    "text": "is equal to l theta star. I know the expectation\nof nabla l hat theta star",
    "start": "3026520",
    "end": "3032550"
  },
  {
    "text": "is also equal to\nnabla l theta star. So you assume enough\nregularity conditions",
    "start": "3032550",
    "end": "3037870"
  },
  {
    "text": "so that you can switch to the\ngradient with the expectation and you also have\nsomething like this. ",
    "start": "3037870",
    "end": "3049410"
  },
  {
    "text": "And this is equal to\n0 because theta star is the minimizer of the l.",
    "start": "3049410",
    "end": "3055500"
  },
  {
    "text": "So that's why this is 0. And this is a p\nby p matrix, which",
    "start": "3055500",
    "end": "3060600"
  },
  {
    "text": "is full rank, as we assumed. And basically, this is\nsaying-- and also you can,",
    "start": "3060600",
    "end": "3069880"
  },
  {
    "text": "because this, this is average\nof n IID terms, right?",
    "start": "3069880",
    "end": "3081829"
  },
  {
    "text": "What is the-- because this is\n1 over n times sum of nabla square, l x i y i theta.",
    "start": "3081830",
    "end": "3092210"
  },
  {
    "text": "So it's a sum of IID terms. Then you can use\nlaw of large numbers",
    "start": "3092210",
    "end": "3102180"
  },
  {
    "text": "to say that this n\nconverges to this.",
    "start": "3102180",
    "end": "3113990"
  },
  {
    "text": "And similarly, you\nalso know that-- ",
    "start": "3113990",
    "end": "3123060"
  },
  {
    "text": "I'm sorry, what I'm doing here. My bad. Nabla l l, this is\nconverging to this.",
    "start": "3123060",
    "end": "3132590"
  },
  {
    "text": "OK, so if you want\nto just get the--",
    "start": "3132590",
    "end": "3140130"
  },
  {
    "start": "3140130",
    "end": "3145630"
  },
  {
    "text": "Moreover, by law\nof large numbers, you can also get\nsomething more accurate about this convergence.",
    "start": "3145630",
    "end": "3151947"
  },
  {
    "text": "So here you are only showing\nthat it's converging, but also you can know that\nhow much different they are.",
    "start": "3151947",
    "end": "3157360"
  },
  {
    "text": "You know that if you look at\nthe difference between this",
    "start": "3157360",
    "end": "3163990"
  },
  {
    "text": "minus this, this should\nbe on the order of one.",
    "start": "3163990",
    "end": "3170970"
  },
  {
    "text": "Or more accurately, this will be\na Gaussian distribution, which",
    "start": "3170970",
    "end": "3176090"
  },
  {
    "text": "means 0 and covariance\nnabla l i theta star.",
    "start": "3176090",
    "end": "3182465"
  },
  {
    "text": "Nabla x y theta star.",
    "start": "3182465",
    "end": "3189950"
  },
  {
    "text": "I guess this is because-- I'm using central\nlimit theorem here, maybe I should first review\nthe central limit theorem.",
    "start": "3189950",
    "end": "3196235"
  },
  {
    "start": "3196235",
    "end": "3204540"
  },
  {
    "text": "So when you have\ncentral limit theorem, you know that suppose\nx hat is equal to 1",
    "start": "3204540",
    "end": "3210109"
  },
  {
    "text": "over n times sum of xi. And xi or IID, from\nsome distribution, d.",
    "start": "3210110",
    "end": "3217280"
  },
  {
    "text": "And xi, let's say\nis in d dimensional. Then let's say sigma is\nthe covariance of xi.",
    "start": "3217280",
    "end": "3227390"
  },
  {
    "text": "Then you know that as n\ngoes to infinity, a x hat,",
    "start": "3227390",
    "end": "3238109"
  },
  {
    "text": "convert this in probability\nto the expectation of x, all right? That's the law of large numbers.",
    "start": "3238110",
    "end": "3246215"
  },
  {
    "text": "It's called law\nof large numbers. And then the more\naccurate thing is that you can look at the\ndifference between x hat",
    "start": "3246215",
    "end": "3252780"
  },
  {
    "text": "and expectation for x. And you know that if you scale a\ndifference by square root of n,",
    "start": "3252780",
    "end": "3257850"
  },
  {
    "text": "then this converts it to\na Gaussian distribution. First of all, it's on\nthe order of a constant. And secondly, the distribution\nis mean 0 and covariance x.",
    "start": "3257850",
    "end": "3270160"
  },
  {
    "text": "And in some sense this is\nsaying that x hat intuitively-- or informally, this is saying\nthat x hat minus e of x",
    "start": "3270160",
    "end": "3280050"
  },
  {
    "text": "is on the order of 1\nover square root 2. OK, so this is\ncentral limit theorem.",
    "start": "3280050",
    "end": "3287010"
  },
  {
    "text": "And what we are\ndoing here is that-- in this equation, what we\nare doing here is basically applying the central limit\ntheorem where you apply x i,",
    "start": "3287010",
    "end": "3299290"
  },
  {
    "text": "it corresponds to the\ngradient of l at xi by i. ",
    "start": "3299290",
    "end": "3307069"
  },
  {
    "text": "Yeah, this is the gradient\nof the loss at example r. OK, so basically we have done\nsome of these preparations",
    "start": "3307070",
    "end": "3315280"
  },
  {
    "text": "so we know how different\nnabla l hat is from l,",
    "start": "3315280",
    "end": "3321850"
  },
  {
    "text": "and also we know that,\nthe highest n converges. And now we can come back to\nthis important equation here.",
    "start": "3321850",
    "end": "3327460"
  },
  {
    "text": " And we are ready to get\nsomething real, so let me--",
    "start": "3327460",
    "end": "3336470"
  },
  {
    "text": "I mean rewrite that. So theta hat minus theta star\nthis was nabla square I hat",
    "start": "3336470",
    "end": "3342520"
  },
  {
    "text": "theta star inverse times nabla\nI hat theta nabla l theta star,",
    "start": "3342520",
    "end": "3350663"
  },
  {
    "text": "is that right? No.  Copy, this is not.",
    "start": "3350663",
    "end": "3357450"
  },
  {
    "text": " Nabla l theta star hat\nplus higher-order terms,",
    "start": "3357450",
    "end": "3367970"
  },
  {
    "text": "so this one is close to nabla\nsquare l theta star inverse.",
    "start": "3367970",
    "end": "3374119"
  },
  {
    "text": "That's the first thing we know. And also we know that\nthis one is roughly speaking nabla l theta star\nplus some [INAUDIBLE],, right?",
    "start": "3374120",
    "end": "3384670"
  },
  {
    "text": "So suppose you do\nmost of this, then you get something like\nthis is roughly equal to 0.",
    "start": "3384670",
    "end": "3402410"
  },
  {
    "text": "So then you get 1\nover square root. ",
    "start": "3402410",
    "end": "3408714"
  },
  {
    "text": "Maybe I'll ask\nthe question first because this takes a\nlittle bit of time. [INAUDIBLE] ",
    "start": "3408715",
    "end": "3417040"
  },
  {
    "text": "Can you say that again? Is there a difference\nbetween x hat and x when you're using\nthe central limit theorem? ",
    "start": "3417040",
    "end": "3422771"
  },
  {
    "text": "x hat and x? Oh, sorry, my bad. Wait, what? ",
    "start": "3422771",
    "end": "3430490"
  },
  {
    "text": "Oh, I guess so. Yes, so I'm thinking x\nis also drawn from b. So maybe I should either\nuse xi, or let's say",
    "start": "3430490",
    "end": "3440060"
  },
  {
    "text": "x is a generic\nvariable that is drawn from the same distribution d. ",
    "start": "3440060",
    "end": "3446060"
  },
  {
    "text": "But the expectation of x\nis the same as expectation of xi, that's right. Are we taking a bias term here?",
    "start": "3446060",
    "end": "3451338"
  },
  {
    "text": " Here? Right. Yes, I'm using that.",
    "start": "3451338",
    "end": "3457730"
  },
  {
    "text": " OK, so maybe I'll just do this\na little bit more carefully.",
    "start": "3457730",
    "end": "3465620"
  },
  {
    "text": "So I'm basically trying\nto replace the l hat with l, right? So the first thing is\nthat the gradient--",
    "start": "3465620",
    "end": "3472450"
  },
  {
    "start": "3472450",
    "end": "3479849"
  },
  {
    "text": "using this equation,\nmaybe x squared 1. So this is roughly equal\nto 1 over square root n,",
    "start": "3479850",
    "end": "3486839"
  },
  {
    "text": "plus nabla l theta\nstar, which is 0, so this is roughly equal\nto 1 over square root 2.",
    "start": "3486840",
    "end": "3493470"
  },
  {
    "text": "So if you don't care too\nmuch about the vector versus real number\nof the distribution",
    "start": "3493470",
    "end": "3498660"
  },
  {
    "text": "then you get 1 over\nsquare root of 2. And this one is kind of close\nto a constant, inverse converges",
    "start": "3498660",
    "end": "3505745"
  },
  {
    "text": "to--  which is a constant. So that these two things\ntogether will give that--",
    "start": "3505745",
    "end": "3513942"
  },
  {
    "text": "maybe I should use\nthe yellow color to continue-- minus something\nlike one over square root n,",
    "start": "3513942",
    "end": "3519526"
  },
  {
    "text": "on this other 1 over\nsquare root of 2. So something like maybe-- basically you get nabla square\nl theta star inverse times 1",
    "start": "3519527",
    "end": "3530973"
  },
  {
    "text": "over square root of 2. And this is on the order\nof 1 over square root of 2. So that's how you got that\ntheta hat minus theta star,",
    "start": "3530973",
    "end": "3536570"
  },
  {
    "text": "it's on the order of 1\nover square root of 2. Of course, just to clarify,\nthis is not exactly formal because I'm ignoring\na lot of things.",
    "start": "3536570",
    "end": "3544319"
  },
  {
    "text": "For example, this is a vector-- this 1 over square root n\nthing is really a vector, it's not a scalar, but\nit's on that order.",
    "start": "3544320",
    "end": "3552490"
  },
  {
    "text": "So that's how you get that\ntheta hat minus theta star is on the order of 1\nover square root of 2. And also, heuristically\nif you really",
    "start": "3552490",
    "end": "3559690"
  },
  {
    "text": "care about I theta minus l\ntheta star, then you can--",
    "start": "3559690",
    "end": "3567160"
  },
  {
    "text": "this is the excess risk. You can also do\nthe tail expansion. You say that this is-- it\nwill expand along theta star.",
    "start": "3567160",
    "end": "3572500"
  },
  {
    "text": "You get that this is\nl theta star times theta minus theta star.",
    "start": "3572500",
    "end": "3577970"
  },
  {
    "text": "Sorry, this is maybe-- why do I have so many\npapers in my notes? Sorry, my bad.",
    "start": "3577970",
    "end": "3583520"
  },
  {
    "text": " So this is theta hat. ",
    "start": "3583520",
    "end": "3593871"
  },
  {
    "text": "Here the interesting\nthing is that if you do a first-order tail expansion,\nyou're going to get 0. So you'll have to do\nsecond-order tail expansion.",
    "start": "3593872",
    "end": "3601050"
  },
  {
    "text": "So we're going to get\ntheta hat minus theta star times Hessian plus\nhigher-order terms.",
    "start": "3601050",
    "end": "3614140"
  },
  {
    "text": "OK, so the reason why I need\nto do the second-order tail expansion is because the\nfirst-order expansion is 0,",
    "start": "3614140",
    "end": "3620950"
  },
  {
    "text": "because this is 0. Theta star is the\nminimizer of l, right? So that's why we have to\nlook at the second-order tail",
    "start": "3620950",
    "end": "3626990"
  },
  {
    "text": "expansion. And if you want to roughly see\nhow large the second-order tail",
    "start": "3626990",
    "end": "3632110"
  },
  {
    "text": "expansion is, you can see that\neach of these terms-- this is 1 over square root n. This is 1 over square root n.",
    "start": "3632110",
    "end": "3639040"
  },
  {
    "text": "So basically, the second\nterm will be something like 1 over n plus higher-order terms.",
    "start": "3639040",
    "end": "3645609"
  },
  {
    "text": " OK, so this is some\nkind heuristical proof",
    "start": "3645610",
    "end": "3653870"
  },
  {
    "text": "to show why theta\nhat minus theta star is in the order one\nover square root n, and in terms of the loss,\nis on the order of 1 over n.",
    "start": "3653870",
    "end": "3661487"
  },
  {
    "text": " Any questions so far?",
    "start": "3661487",
    "end": "3668830"
  },
  {
    "text": "So the consistency is\nneeded [INAUDIBLE]??",
    "start": "3668830",
    "end": "3674550"
  },
  {
    "text": "So consistency is needed\nalmost every step. [INAUDIBLE]",
    "start": "3674550",
    "end": "3681172"
  },
  {
    "start": "3681172",
    "end": "3689210"
  },
  {
    "text": "I'm using the central\nlimit theorem only on random variable,\nnot the function of it. Because I'm not sure\nwhether that's--",
    "start": "3689210",
    "end": "3697130"
  },
  {
    "text": " Oh, by the way, I forgot\nto repeat the question, but anyway, I'll\nremember that next time.",
    "start": "3697130",
    "end": "3704349"
  },
  {
    "text": "So the question was that\nwhether the central limit theorem is applied to the\nrandom variable itself.",
    "start": "3704350",
    "end": "3710259"
  },
  {
    "text": "I think so because\nxi, this xi, right, so that corresponds\nto the gradient.",
    "start": "3710260",
    "end": "3718580"
  },
  {
    "text": "So gradient of l at that\nexample i is my random variable. ",
    "start": "3718580",
    "end": "3725410"
  },
  {
    "text": "So that's how I got-- and then the sum of\nxi, this corresponds",
    "start": "3725410",
    "end": "3730600"
  },
  {
    "text": "to the empirical\ngradient, right?",
    "start": "3730600",
    "end": "3736360"
  },
  {
    "text": "And the expectation corresponds\nto the population gradient. ",
    "start": "3736360",
    "end": "3743095"
  },
  {
    "text": "[INAUDIBLE] wouldn't you\nneed some [INAUDIBLE]?? ",
    "start": "3743095",
    "end": "3753290"
  },
  {
    "text": "Yeah. You need a lot of different\nregularity conditions to make all of this work\nbecause for example,",
    "start": "3753290",
    "end": "3758630"
  },
  {
    "text": "also there's implicit stuff\nthat I didn't go through which is the inverse, right?",
    "start": "3758630",
    "end": "3765319"
  },
  {
    "text": "I only show that the Hessian\nconverges to for example-- where I did that,\nso I only showed",
    "start": "3765320",
    "end": "3771680"
  },
  {
    "text": "that the empirical\nHessian converges to a population Hessian. You also need to\nshow that the inverse",
    "start": "3771680",
    "end": "3778160"
  },
  {
    "text": "of the empirical\nHessian converges to the inverse of the\npopulation Hessian. So that's another thing you\nwant to formally deal with.",
    "start": "3778160",
    "end": "3785630"
  },
  {
    "text": " So yeah, every\ntime I give this-- I've taught this\ntwo or three times.",
    "start": "3785630",
    "end": "3792790"
  },
  {
    "text": "And every time there\nare a lot of questions about this first lecture. I still haven't figured out\na better way to teach it,",
    "start": "3792790",
    "end": "3799810"
  },
  {
    "text": "but I think maybe the\nthing is just that I really",
    "start": "3799810",
    "end": "3805650"
  },
  {
    "text": "want to convey this--\nconvey this idea. The idea is that you\ncan do tail expansion. And you can pretty much do\na lot of heuristical stuff",
    "start": "3805650",
    "end": "3811560"
  },
  {
    "text": "and all of them\ncan be made formal. And how to exactly\nmake it formal, it's a little bit tricky as--\nthese are all great questions,",
    "start": "3811560",
    "end": "3819150"
  },
  {
    "text": "right? All the questions\nare welcome, but just to set up expectations,\nthis is not",
    "start": "3819150",
    "end": "3824250"
  },
  {
    "text": "meaning to have a very\nformal derivation here. ",
    "start": "3824250",
    "end": "3832238"
  },
  {
    "text": "OK, so I think\nthat's all for today. So next time we\nare going to make",
    "start": "3832238",
    "end": "3837700"
  },
  {
    "text": "this a little bit formal\nmaybe for 15 minutes, and then we can move\non to other things.",
    "start": "3837700",
    "end": "3842760"
  },
  {
    "start": "3842760",
    "end": "3847000"
  }
]