[
  {
    "start": "0",
    "end": "32000"
  },
  {
    "start": "0",
    "end": "5163"
  },
  {
    "text": "So far in the\ncourse, we've talked about multi-task learning\nand meta-learning for supervised learning. And now we're going to start\ntalking about reinforcement",
    "start": "5163",
    "end": "11950"
  },
  {
    "text": "learning, which introduces\na number of really interesting challenges and also\nopportunities to do better when",
    "start": "11950",
    "end": "18058"
  },
  {
    "text": "you're trying to\nlearn multiple tasks and meta-learn across tasks.",
    "start": "18058",
    "end": "23090"
  },
  {
    "text": "And the first lecture on\nreinforcement learning and multi-task reinforcement\nlearning will be by Karol.",
    "start": "23090",
    "end": "29080"
  },
  {
    "text": "So I'll let Karol take it away. ",
    "start": "29080",
    "end": "34510"
  },
  {
    "start": "32000",
    "end": "192000"
  },
  {
    "text": "All right, welcome, everybody. So thanks, Chelsea,\nfor the introduction.",
    "start": "34510",
    "end": "39820"
  },
  {
    "text": "And today, we'll be talking\nabout reinforcement learning. It will be mostly a\nprimer, but we'll also",
    "start": "39820",
    "end": "45610"
  },
  {
    "text": "talk a little bit about\nmulti-task and goal-conditioned RL. So this is the first lecture,\nbut I'm giving the first.",
    "start": "45610",
    "end": "55480"
  },
  {
    "text": "So I'd like to do a\nlittle introduction. My name is Karol Hausman.",
    "start": "55480",
    "end": "61149"
  },
  {
    "text": "I come from a country\nin Europe called Poland. That is right here. And I'm not a native\nEnglish speaker",
    "start": "61150",
    "end": "66550"
  },
  {
    "text": "as you can probably tell by now. So please, please let me\nknow if there is something that you don't understand.",
    "start": "66550",
    "end": "71830"
  },
  {
    "text": "I really care about making\nsure that everything I say and explain is\nunderstood by all of you.",
    "start": "71830",
    "end": "79280"
  },
  {
    "text": "So please raise\nyour hand in chat-- raise your hand or just\nwrite something in the chat, and I'll try to get back\nto you as soon as I see it.",
    "start": "79280",
    "end": "87039"
  },
  {
    "text": " If there is something that\nyou should know about me is that I really like robots.",
    "start": "87040",
    "end": "93620"
  },
  {
    "text": "And I've been working on\nrobots for quite some time now. So during the classes\nthat I'll be giving,",
    "start": "93620",
    "end": "98680"
  },
  {
    "text": "the material might be\nskewed a little bit towards that application. ",
    "start": "98680",
    "end": "104980"
  },
  {
    "text": "Also I studied\nclassical robotics first where I was working on a concept\ncalled interactive perception",
    "start": "104980",
    "end": "112810"
  },
  {
    "text": "where basically we're\ntrying to control the robots in a\nway that will make the perception for the\nrobots a little bit easier.",
    "start": "112810",
    "end": "119420"
  },
  {
    "text": "And while I was doing\nthis, I remember going to a talk that\nwas given by a student at that point, Sergey Levine,\nabout deep reinforcement",
    "start": "119420",
    "end": "127060"
  },
  {
    "text": "learning. And I got completely\nfascinated by it. I thought that this\nis the right way of doing things and the\nright way of solving things",
    "start": "127060",
    "end": "134770"
  },
  {
    "text": "that I was actually working on. And that eliminates these\nboundaries between the control and perception\nand considers them",
    "start": "134770",
    "end": "140140"
  },
  {
    "text": "as one interconnected problem. After that talk, I\ngot very fascinated and I completely\nswitched my PhD topic",
    "start": "140140",
    "end": "146637"
  },
  {
    "text": "and started working on\ndeep reinforcement learning and I've continued\nworking on it ever since. So if I have any\nsmall piece of advice",
    "start": "146637",
    "end": "153790"
  },
  {
    "text": "to you is if you find something\nlike this that you really like and you're really\nfascinated by it,",
    "start": "153790",
    "end": "159849"
  },
  {
    "text": "I would recommend to\nfollowing that path because it can be very rewarding.",
    "start": "159850",
    "end": "166000"
  },
  {
    "text": "And then lastly, I'm\nalso a research scientist at Robotics at Google, which\nis part of Google Brain.",
    "start": "166000",
    "end": "172440"
  },
  {
    "text": "And that's where we try to\napply some of the algorithms that we'll be\nlearning about today. And during other lectures,\nwe try to apply them",
    "start": "172440",
    "end": "180600"
  },
  {
    "text": "to real robots at scale. So I'll try to bring that\nperspective a little bit more throughout the\nlectures so that you",
    "start": "180600",
    "end": "187125"
  },
  {
    "text": "can see how some of these\nalgorithms work in practice. ",
    "start": "187125",
    "end": "192510"
  },
  {
    "start": "192000",
    "end": "488000"
  },
  {
    "text": "All right, so with\nthat, let's first start with why should we\ncare, why should we",
    "start": "192510",
    "end": "198030"
  },
  {
    "text": "focus on reinforcement\nlearning in general? And I'd like to pose a\nquestion to you to think about.",
    "start": "198030",
    "end": "204880"
  },
  {
    "text": "Can you think of an action,\na fully isolated action that doesn't affect the future? And what I mean by\nthat is an action",
    "start": "204880",
    "end": "210780"
  },
  {
    "text": "that doesn't have any\nlong-term consequences. So there is the\nimmediate consequence when you perform that\naction in the real world,",
    "start": "210780",
    "end": "217080"
  },
  {
    "text": "but are there any\nactions that don't have long-term\nconsequences and I",
    "start": "217080",
    "end": "222420"
  },
  {
    "text": "encourage you to think\na little bit about this. But in the meantime, I\nwould like the answer",
    "start": "222420",
    "end": "227550"
  },
  {
    "text": "that I think it's\nreally, really hard. So even an accent that seems\nvery meaningless, like,",
    "start": "227550",
    "end": "234690"
  },
  {
    "text": "for example, me tapping\nmy fingers right now. It seems that it doesn't have\nany influence in the future,",
    "start": "234690",
    "end": "241110"
  },
  {
    "text": "but it might be that because\nI tapped my fingers right now, I exerted so much energy or\nI served more energy that I",
    "start": "241110",
    "end": "248040"
  },
  {
    "text": "wouldn't if I didn't have them. And because of that,\nlater on during the day, I'll need maybe a nap\nto get my energy back.",
    "start": "248040",
    "end": "258690"
  },
  {
    "text": "And in particular, I\nwould like to show you one experiment\nthat kind of tries to exemplify this concept.",
    "start": "258690",
    "end": "264900"
  },
  {
    "text": "And this is an\nexperiment that is often shown in things such as chaos\ntheory or butterfly effect.",
    "start": "264900",
    "end": "271050"
  },
  {
    "text": "I encourage you to\nlook some of these up. And basically it shows\na double pendulum",
    "start": "271050",
    "end": "277199"
  },
  {
    "text": "that starts from more or less\nthe same initial conditions. So you can see six different\npendulums right here.",
    "start": "277200",
    "end": "282510"
  },
  {
    "text": "This is the same pendulum\njust repeated six times. And you can see that\nthere is a human hand that",
    "start": "282510",
    "end": "288600"
  },
  {
    "text": "starts the experiment. But from what we can\ntell, it basically looks exactly the same\nin all of these pictures.",
    "start": "288600",
    "end": "295650"
  },
  {
    "text": "And now we start the experiment. We let the pendulum go. And this is recorded\nin slow motion.",
    "start": "295650",
    "end": "300845"
  },
  {
    "text": "And you can see that\ninitially all of the pendulums behave exactly the same. And so they're indistinguishable\neven at this point.",
    "start": "300845",
    "end": "307835"
  },
  {
    "text": "However over time, you will be\nable to tell that all of them perform completely\ndifferent motions.",
    "start": "307835",
    "end": "314229"
  },
  {
    "text": "And they're in completely\ndifferent positions. And this is one of the\nstatements of chaos theory",
    "start": "314230",
    "end": "320230"
  },
  {
    "text": "that states that even in a\nfully deterministic world or in a world that is governed\nby deterministic equations,",
    "start": "320230",
    "end": "328990"
  },
  {
    "text": "because of the fact that we\ncan't really disambiguate the initial\nconditions, we always have to round it up\nto a certain number",
    "start": "328990",
    "end": "338949"
  },
  {
    "text": "that we can actually do\nthe real experiments on.",
    "start": "338950",
    "end": "344170"
  },
  {
    "text": "Because of that initial state\nthat is a little bit different and the initial action that is\nalso a little bit different,",
    "start": "344170",
    "end": "350110"
  },
  {
    "text": "the final effects will\nbe actually random, even though the entire\nworld is governed by-- even if we assume that the\nentire world is governed",
    "start": "350110",
    "end": "357070"
  },
  {
    "text": "by deterministic equations.  So there is actually one\nexample of an action that",
    "start": "357070",
    "end": "364190"
  },
  {
    "text": "does not affect the future. And this is how we model\nthings in supervised learning.",
    "start": "364190",
    "end": "369439"
  },
  {
    "text": "So in supervised learning,\nwe make certain decisions. And that decision\ncould be, for example, on what's in the image.",
    "start": "369440",
    "end": "375830"
  },
  {
    "text": "In the object\nrecognition case, we're given an image, we have to say,\nis there a cat in the image?",
    "start": "375830",
    "end": "381380"
  },
  {
    "text": "And we'll perform\na decision and we pretend that that decision has\nno consequences beyond that one action that we just did.",
    "start": "381380",
    "end": "388940"
  },
  {
    "text": "So this seems a little-- this looks like a very\nsmall isolated world that doesn't really adhere\nto how the real world works.",
    "start": "388940",
    "end": "400730"
  },
  {
    "text": "So reinforcement learning\ntries to deal with this problem and tries to model the fact\nthat every decision we make",
    "start": "400730",
    "end": "407210"
  },
  {
    "text": "can actually have\nconsequences long term. And it goes to the problem of\nsequential decision making.",
    "start": "407210",
    "end": "413678"
  },
  {
    "text": "So how the decisions\nthat we make now will influence the\nfuture, and how it will influence our future decisions?",
    "start": "413678",
    "end": "419680"
  },
  {
    "text": "And because of that, because\nit's such a real world kind of concept, it has\nmany common applications,",
    "start": "419680",
    "end": "425930"
  },
  {
    "text": "some of which we'll talk about,\nsuch as robotics, language and dialog systems, autonomous\ndriving where your actions will",
    "start": "425930",
    "end": "432127"
  },
  {
    "text": "influence other agents' actions,\nand because of that they will influence your actions again,\nas well as business operations",
    "start": "432127",
    "end": "437710"
  },
  {
    "text": "and finance.  So basically most deployed\nmachine learning systems",
    "start": "437710",
    "end": "443210"
  },
  {
    "text": "have to deal with that\nsequential decision making aspect. And also I think we can argue\nthat the fact that we can make",
    "start": "443210",
    "end": "450890"
  },
  {
    "text": "sequential decisions is\none of the key aspects of intelligence.",
    "start": "450890",
    "end": "456900"
  },
  {
    "text": "All right, so the plan for this\nlecture would be as following. First, we'll get familiar with\nthe multi-task reinforcement",
    "start": "456900",
    "end": "462660"
  },
  {
    "text": "learning problem. Then we'll discuss one way to\nsolve that problem by using the policy gradient method.",
    "start": "462660",
    "end": "469765"
  },
  {
    "text": "And we'll learn about their\nmulti-task counterparts as well. Then we'll talk a little\nbit about Q-learning, which should be mostly a\nreview based on the lectures",
    "start": "469765",
    "end": "477620"
  },
  {
    "text": "that you've taken previously. And then we'll talk about\nmulti-task Q-learning as well.",
    "start": "477620",
    "end": "484340"
  },
  {
    "text": "All right, so let's\nstart with multi-task RL.  So first, let's introduce some\nterminology so that we can all",
    "start": "484340",
    "end": "492509"
  },
  {
    "start": "488000",
    "end": "977000"
  },
  {
    "text": "operate on the same terms. This should be mostly I\nthink review for most of you. So we will go over this quickly.",
    "start": "492510",
    "end": "499410"
  },
  {
    "text": "So if we start from\nan object recognition, a supervised\nlearning pipeline, it might look something\nlike this where",
    "start": "499410",
    "end": "505680"
  },
  {
    "text": "we are given an\nimage as an input, then that image is propagated\nthrough some neural network,",
    "start": "505680",
    "end": "511020"
  },
  {
    "text": "a convolutional neural\nnetwork, for example. And at the end of\nthat network, we are trying to classify\nwhat's in that image,",
    "start": "511020",
    "end": "516429"
  },
  {
    "text": "whether it's a tiger\nor another animal. One change that we'll make\nin the reinforcement learning",
    "start": "516429",
    "end": "522090"
  },
  {
    "text": "scenario is that we will\nsubindex all of the things by timestep t, right?",
    "start": "522090",
    "end": "527520"
  },
  {
    "text": "So we'll observe an image\nat a certain timestep t. That image will\nthen be propagated",
    "start": "527520",
    "end": "533410"
  },
  {
    "text": "through a neural\nnetwork, and then we'll output some kind of action\nalso at the same timestep.",
    "start": "533410",
    "end": "539830"
  },
  {
    "text": "Now, that action will\ninfluence our next observation. So we'll then get our next\nobservation ot plus 1.",
    "start": "539830",
    "end": "545680"
  },
  {
    "text": "And then we'll need to\nperform another action based on that observation.",
    "start": "545680",
    "end": "550740"
  },
  {
    "text": "Also as we just\nsaid, our actions are real actions\nin the real world.",
    "start": "550740",
    "end": "555900"
  },
  {
    "text": "So we no longer have to\ndecide what's in the image. But we actually have to\ndo something about it.",
    "start": "555900",
    "end": "561180"
  },
  {
    "text": "So when we are given\nan image of a tiger, we, for example,\nhave three options. We can run away,\nignore, or pet it.",
    "start": "561180",
    "end": "566550"
  },
  {
    "text": "And all of these will\ninfluence how the world is going to propagate from now on. ",
    "start": "566550",
    "end": "574620"
  },
  {
    "text": "All right, so to describe\nthis notation, o of t would denote observation\nat timestep t.",
    "start": "574620",
    "end": "582690"
  },
  {
    "text": "at is an action that we\nperform at the timestep. And then we'll describe a\npolicy pi-theta, pi-theta",
    "start": "582690",
    "end": "594000"
  },
  {
    "text": "that gives us the\nprobability of action given the observation as the policy. So this is the\npolicy that we'll use",
    "start": "594000",
    "end": "599520"
  },
  {
    "text": "to know how to act\nin a given situation. In addition to this, we'll have\nst, which we'll call state,",
    "start": "599520",
    "end": "608700"
  },
  {
    "text": "and we'll get back\nto it in a second. And then we also have a\nfully observed version of a policy, which is\npi-theta that gives us actions",
    "start": "608700",
    "end": "617130"
  },
  {
    "text": "given the current state. So let's talk a little bit about\nthat state versus observation",
    "start": "617130",
    "end": "623600"
  },
  {
    "text": "problem. And I'd like to bring up\nan example from philosophy.",
    "start": "623600",
    "end": "629240"
  },
  {
    "text": "This is known as\nPlato's cave where Plato had the idea\nthat we as humans are",
    "start": "629240",
    "end": "636170"
  },
  {
    "text": "captured in a little cave. And it takes quite long for\nus to get out of that cave.",
    "start": "636170",
    "end": "643910"
  },
  {
    "text": "And there's like a whole\ncomplex philosophy behind it. But the basic idea is that\nwe are captured in that cave.",
    "start": "643910",
    "end": "650210"
  },
  {
    "text": "And we don't really\nget to observe the true nature of things. But instead, we just see the\nreflections of them, right?",
    "start": "650210",
    "end": "659459"
  },
  {
    "text": "So in particular-- whether\nyou can see right here, there is a God right here that\nis holding the true object we",
    "start": "659460",
    "end": "666149"
  },
  {
    "text": "don't get to see. This is a human. This is us sitting in the cave. And there's this\nbig fire behind us.",
    "start": "666150",
    "end": "672180"
  },
  {
    "text": "But we also don't get to see it. We only get to see the shadow\nof the true object that is projected onto the wall.",
    "start": "672180",
    "end": "680750"
  },
  {
    "text": "All right, so now using that\nmetaphor, the true nature of objects is the\nstate that we don't",
    "start": "680750",
    "end": "686480"
  },
  {
    "text": "get to observe very often, and\nthen the reflection that we get to see is the\nobservation, right?",
    "start": "686480",
    "end": "693230"
  },
  {
    "text": "The observations\nmight be incomplete. It might not have all the\nnecessary information for us to be able to perform\nthe right decision.",
    "start": "693230",
    "end": "700332"
  },
  {
    "text": "But the state has everything\nthat we would ever need to know how to act. ",
    "start": "700332",
    "end": "705957"
  },
  {
    "text": "All right, so maybe a\nmore practical example would look something like that. We are looking at an image.",
    "start": "705957",
    "end": "712540"
  },
  {
    "text": "And that image is\nan observation. And we have to make a\ndecision based on that. But in order to\nmake that decision,",
    "start": "712540",
    "end": "717880"
  },
  {
    "text": "we actually might want to\nknow a little bit more. That's what in the image. For example, the positions of\nthe objects as well as their",
    "start": "717880",
    "end": "724080"
  },
  {
    "text": "velocities. So the state should\nhave all the information that we would need to\nmake that decision.",
    "start": "724080",
    "end": "730769"
  },
  {
    "text": "And the observation can be\nactually quite incomplete. So for example, there\ncould be an occlusion. There could be a car in front\nof us, and because of that,",
    "start": "730770",
    "end": "737770"
  },
  {
    "text": "we don't even get\nto see the cheetah. And we'll talk about\nthe problem of not",
    "start": "737770",
    "end": "742980"
  },
  {
    "text": "being able to observe everything\na little bit later too. All right. So this is our terminology.",
    "start": "742980",
    "end": "751250"
  },
  {
    "text": "So let's try to think how we\ncould find a policy, right?",
    "start": "751250",
    "end": "756410"
  },
  {
    "text": "So one way we can do this\nis to take the knowledge that we have from a\nsupervised learning scenario",
    "start": "756410",
    "end": "762080"
  },
  {
    "text": "and try to apply\ndirectly to this problem. And this is what we'll be\nreferring to as imitation learning.",
    "start": "762080",
    "end": "767912"
  },
  {
    "text": "So in this case,\nwe have that image that goes through the\nnetwork, outputs actions. And the images that we want\nto focus on in particular",
    "start": "767912",
    "end": "774830"
  },
  {
    "text": "are images that are\ncoming from a camera that was installed in a car. And we'll be recording actions\nof humans that were actually",
    "start": "774830",
    "end": "781400"
  },
  {
    "text": "driving these cars. And this is research\ndone at NVIDIA by Bojarski et al in a\nsystem called ALVINN, where",
    "start": "781400",
    "end": "788510"
  },
  {
    "text": "they basically had human\ntruck drivers driving their cars as usual. And they would record all the\nobservations, all the images",
    "start": "788510",
    "end": "795410"
  },
  {
    "text": "that the car would see, and\nthe corresponding actions that the human\ndrivers would take. So now what we could do is we\ncould just store all of these",
    "start": "795410",
    "end": "803700"
  },
  {
    "text": "and compose a big\ntraining data set. And then use supervised learning\nto train our policy based",
    "start": "803700",
    "end": "810450"
  },
  {
    "text": "on that. So if we did that, at the end we\nwould get our policy pi-theta. ",
    "start": "810450",
    "end": "817890"
  },
  {
    "text": "So this is one way\nhow we could do this. ",
    "start": "817890",
    "end": "824170"
  },
  {
    "text": "Sorry, one second. So the question is, are\nthere any problems with that? So are there any problems with\nimitation learning with the way",
    "start": "824170",
    "end": "831880"
  },
  {
    "text": "how it was done, for example,\nby the folks at NVIDIA? ",
    "start": "831880",
    "end": "837710"
  },
  {
    "text": "Anybody has any ideas,\nplease write in chat or raise your hand. ",
    "start": "837710",
    "end": "850319"
  },
  {
    "text": "Right, OK. Problems might occur in a\nnew environment or decisions",
    "start": "850320",
    "end": "856210"
  },
  {
    "text": "that are independent. We're only capturing\nobservations, not a fully observed state.",
    "start": "856210",
    "end": "861610"
  },
  {
    "text": "It's not scalable. All right. No idea if a wrong\ndecision is made.",
    "start": "861610",
    "end": "866930"
  },
  {
    "text": "Yes, so one of the problems is-- I think these are all\nreally good answers. One of the problems is\nrelated to something",
    "start": "866930",
    "end": "873380"
  },
  {
    "text": "that we saw before with\nthe double pendulum. So if there was some error. And we don't fit our\nactions perfectly,",
    "start": "873380",
    "end": "880460"
  },
  {
    "text": "then the error will accumulate\nand we'll get into a state that we haven't seen any data.",
    "start": "880460",
    "end": "887070"
  },
  {
    "text": "For example, the cars\nwill start deviating and because we are only\ncollecting the data of drivers",
    "start": "887070",
    "end": "892652"
  },
  {
    "text": "driving correctly,\nwe wouldn't know how to behave if we are\ngetting off the road.",
    "start": "892652",
    "end": "898500"
  },
  {
    "text": "All right, great. So reinforcement\nlearning is trying to model this problem\nin a more holistic way.",
    "start": "898500",
    "end": "908850"
  },
  {
    "text": "So in this case, we have to\nhave some kind of way of--",
    "start": "908850",
    "end": "914490"
  },
  {
    "text": "we need to have some utility\nthat we want to optimize for. So before we could\njust say, let's just try to optimize\nour policy such",
    "start": "914490",
    "end": "921000"
  },
  {
    "text": "that we repeat the actions\nthat the humans would take. In this case, we have to\noptimize for something else.",
    "start": "921000",
    "end": "926639"
  },
  {
    "text": "And this is where reward\nfunctions come in handy. So a reward function\nwould be something that will allow\nus to disambiguate",
    "start": "926640",
    "end": "933450"
  },
  {
    "text": "which actions are better\nand which ones are worse. The reward function will\nbe dependent on the state",
    "start": "933450",
    "end": "939660"
  },
  {
    "text": "and the action. And it will have a scalar value\nthat we'll try to optimize.",
    "start": "939660",
    "end": "945300"
  },
  {
    "text": "So in the case of driving\nthat we just talked about, a high reward would correspond\nto us driving safely",
    "start": "945300",
    "end": "951300"
  },
  {
    "text": "and smoothly on the\nroad, and a low reward will correspond to an\naccident, for example.",
    "start": "951300",
    "end": "959129"
  },
  {
    "text": "And all of these components,\nso the state, the action, as well as the\nreward function that depends on the current\nstate and action,",
    "start": "959130",
    "end": "965317"
  },
  {
    "text": "as well as the\ntransition dynamics which tell us the probability\nof the next state, given my current state and\nthe action that we just took,",
    "start": "965317",
    "end": "971610"
  },
  {
    "text": "define so-called Markov\ndecision process. And this is the framework\nthat we'll be operating in.",
    "start": "971610",
    "end": "978520"
  },
  {
    "start": "977000",
    "end": "1178000"
  },
  {
    "text": "All right, so let's\ntalk a little bit more about that framework and the\ngoal of reinforcement learning.",
    "start": "978520",
    "end": "984460"
  },
  {
    "text": "So our goal is to find the\nparameters theta of the policy pi-theta.",
    "start": "984460",
    "end": "989640"
  },
  {
    "text": "And that policy\ntakes observations as inputs and outputs actions. ",
    "start": "989640",
    "end": "996340"
  },
  {
    "text": "These observations\nactually are a result of some underlying state\nthat we don't get to see.",
    "start": "996340",
    "end": "1002610"
  },
  {
    "text": "And then that state\ntogether with our action produce the next state\naccording to these transition",
    "start": "1002610",
    "end": "1007980"
  },
  {
    "text": "dynamics, the probability\nof s-prime given s, a. That results in the next\nstate, which then produces",
    "start": "1007980",
    "end": "1014260"
  },
  {
    "text": "the next observation, which\nis then fed into our policy and we get to\nproduce the action.",
    "start": "1014260",
    "end": "1020440"
  },
  {
    "text": "All right, so this\ncorresponds to a probabilistic\ngraphical model that looks more or less\nlike this where we",
    "start": "1020440",
    "end": "1026109"
  },
  {
    "text": "have various connections here. So for example, the connection\nbetween the observation and action is our\npolicy pi-theta.",
    "start": "1026109",
    "end": "1034880"
  },
  {
    "text": "And the connection between\nthe current state and action that connects these\ntwo to the next state is our transition dynamics p\nof st plus 1 given st and at.",
    "start": "1034880",
    "end": "1042880"
  },
  {
    "text": " So there was an\nimportant thing to note here, which is that if you look\nat this probabilistic graphical",
    "start": "1042880",
    "end": "1049970"
  },
  {
    "text": "model, the probability\nof the current state",
    "start": "1049970",
    "end": "1055220"
  },
  {
    "text": "only depends or\nthe next state only depends on the current state\nand the current action.",
    "start": "1055220",
    "end": "1060320"
  },
  {
    "text": "It's completely independent\nof the states before that and the actions before that. And this is known as\nthe Markov property.",
    "start": "1060320",
    "end": "1068310"
  },
  {
    "text": "So basically, what we mean by\nthat is something like that.",
    "start": "1068310",
    "end": "1077290"
  },
  {
    "text": "Let me just try to draw on this. ",
    "start": "1077290",
    "end": "1083790"
  },
  {
    "text": "So we see that given our\ns2 and a2, s3 is known.",
    "start": "1083790",
    "end": "1092280"
  },
  {
    "text": "And this is conditionally\nindependent of s1, for example. So this connection\nis not there, right?",
    "start": "1092280",
    "end": "1101200"
  },
  {
    "text": "In addition to this, which\nwe'll learn a little bit later in the course, there is one\nother random variable here,",
    "start": "1101200",
    "end": "1109100"
  },
  {
    "text": "which is dependent on the\ncurrent state and action. And this is the\noptimality variable.",
    "start": "1109100",
    "end": "1117299"
  },
  {
    "text": "Let's call it big O1. And this is related\nto the reward",
    "start": "1117300",
    "end": "1122779"
  },
  {
    "text": "function at the timestep. This is the reward of state\ngiven state 1 and action 1.",
    "start": "1122780",
    "end": "1128870"
  },
  {
    "text": " And that optimality\nvariable, or that reward,",
    "start": "1128870",
    "end": "1134309"
  },
  {
    "text": "is also conditionally\nindependent of the previous state. It's only dependent\non the current state",
    "start": "1134310",
    "end": "1139680"
  },
  {
    "text": "and the current action. So we can also draw it right\nhere, state and action.",
    "start": "1139680",
    "end": "1145020"
  },
  {
    "text": "This will be our O2. And now given s2 and\na2, that's everything we",
    "start": "1145020",
    "end": "1153100"
  },
  {
    "text": "need to know about the reward.",
    "start": "1153100",
    "end": "1159880"
  },
  {
    "text": "And this is conditionally\nindependent of s1, for instance. So this connection\nis not there either.",
    "start": "1159880",
    "end": "1164920"
  },
  {
    "text": " All right. So this is the basic idea\nof the Markov property.",
    "start": "1164920",
    "end": "1172000"
  },
  {
    "text": "It states that--\nthe next state is only dependent on the\ncurrent state and the action, and the same applies\nto the reward.",
    "start": "1172000",
    "end": "1177628"
  },
  {
    "text": " So if that's not\nthe case, we will",
    "start": "1177628",
    "end": "1183480"
  },
  {
    "start": "1178000",
    "end": "1539000"
  },
  {
    "text": "observe a system that is called\na partially observable system. So it's not enough. The Markov property\nis not satisfied.",
    "start": "1183480",
    "end": "1189670"
  },
  {
    "text": "And it's not enough\nfor us to determine what the next state\nis going to be or what the reward is going\nto be given the current state",
    "start": "1189670",
    "end": "1194765"
  },
  {
    "text": "and action. And it seems like a\nfairly obvious concept,",
    "start": "1194765",
    "end": "1199980"
  },
  {
    "text": "but very often it's a\nlittle bit more complicated than it appears. And I'll ask you a\nfew questions here.",
    "start": "1199980",
    "end": "1207270"
  },
  {
    "text": "I'll give you some examples. And I would ask you whether this\nis an observable system or not, fully observable or not.",
    "start": "1207270",
    "end": "1213559"
  },
  {
    "text": "So here's the first example. We have a simulated\nrobot, which is the robot that you see in\nthe picture here.",
    "start": "1213560",
    "end": "1219779"
  },
  {
    "text": "This is a robot that\nperforms the reaching task. So it tries to get\nthat end effector, that green dot over here,\nto the goal position, that",
    "start": "1219780",
    "end": "1226370"
  },
  {
    "text": "is the red dot over here. And it's trying to do this.",
    "start": "1226370",
    "end": "1231710"
  },
  {
    "text": "So it gets a reward\n1 when it gets there and 0 in any other location.",
    "start": "1231710",
    "end": "1236840"
  },
  {
    "text": "And our observation consists\nof the goal position as well as the positions and velocities\nof all of the joints.",
    "start": "1236840",
    "end": "1244580"
  },
  {
    "text": "So if you could please\nanswer in the chat, do you think this is\nfully observable or not. So just say yes if it's\nfully observable, or no",
    "start": "1244580",
    "end": "1251360"
  },
  {
    "text": "if you think otherwise.  OK, we see lots of yeses.",
    "start": "1251360",
    "end": "1257920"
  },
  {
    "text": "We see some nos.  All right, yeah.",
    "start": "1257920",
    "end": "1263320"
  },
  {
    "text": "So the answer is that it's\nprobably fully observable, right? Like we can tell everything\nabout the future given",
    "start": "1263320",
    "end": "1270533"
  },
  {
    "text": "that we know the\nposition and velocities and the commands that we\nare sending to the robot. And given the goal position\nand the current position",
    "start": "1270533",
    "end": "1277360"
  },
  {
    "text": "and velocities, we should be\nable to determine the reward. Right, let's look\nat another example.",
    "start": "1277360",
    "end": "1284000"
  },
  {
    "text": "Indiscriminate robotic\ngrasping from a bin given an overhead image. So the task as for this robot\nthat you see in the picture",
    "start": "1284000",
    "end": "1290350"
  },
  {
    "text": "to grasp any object. And it will get the reward\n1 if the object is grasped,",
    "start": "1290350",
    "end": "1295610"
  },
  {
    "text": "or reward 0 if it's not. And we only get to\nsee the overhead image from that monocular RGB camera.",
    "start": "1295610",
    "end": "1303340"
  },
  {
    "text": "Again, please answer in the\nchat, is it fully observable or not? No.",
    "start": "1303340",
    "end": "1308470"
  },
  {
    "text": "No, no, no. That was no, yes. That was a no. ",
    "start": "1308470",
    "end": "1315350"
  },
  {
    "text": "OK, everybody is answering no. Yes, great perfect. One of the reasons for\nit is, for instance,",
    "start": "1315350",
    "end": "1321440"
  },
  {
    "text": "we can't tell what's the\ndistribution of the weight or how heavy the objects are. So it's possible\nthat even though we",
    "start": "1321440",
    "end": "1327860"
  },
  {
    "text": "get to see exact same\nobservations from-- we get to see the two very--",
    "start": "1327860",
    "end": "1333590"
  },
  {
    "text": "basically the same\nimages, they might consist of very\ndifferent objects depending on the\nweight distribution. So then we don't get\nto see that we don't",
    "start": "1333590",
    "end": "1340610"
  },
  {
    "text": "get to observe that setup. All right, so one last example. And this is actually an\nexample from the real world",
    "start": "1340610",
    "end": "1347035"
  },
  {
    "text": "from a project that we\nworked on at Google, which is a robot that\nis trying to sort trash.",
    "start": "1347035",
    "end": "1352580"
  },
  {
    "text": "Right? So this is the robot, that\nyou can see in the image. You can see three\nbins in front of it.",
    "start": "1352580",
    "end": "1358549"
  },
  {
    "text": "There is recyclables,\ncompost, and landfill. You can assume that, in the\ncamera, wherever you stand,",
    "start": "1358550",
    "end": "1365720"
  },
  {
    "text": "you can see all of\nthe bins at once and everything is reachable. And so the robot starts\nwith some trash in the bins,",
    "start": "1365720",
    "end": "1372860"
  },
  {
    "text": "and then it has to sort\nthem correctly, right? It gets a reward 1, if at\nthe end of the episode,",
    "start": "1372860",
    "end": "1380840"
  },
  {
    "text": "the object that was\npreviously unsorted is now correctly sorted. Is this an observable\nenvironment or not?",
    "start": "1380840",
    "end": "1388413"
  },
  {
    "text": "So it it a fully\nobservable environment? Please answer in the chat. ",
    "start": "1388413",
    "end": "1395820"
  },
  {
    "text": "No. ",
    "start": "1395820",
    "end": "1402460"
  },
  {
    "text": "OK. We see quite a few nos. So this is a very\nsimilar environment that we have due to\nwhat we had before.",
    "start": "1402460",
    "end": "1409980"
  },
  {
    "text": "So let's just\neliminate the reason that we use previous links. So let's just say\nthat we don't really care about the\ndistribution of weight.",
    "start": "1409980",
    "end": "1416309"
  },
  {
    "text": "And let's assume that this\nis not that big of a problem. Is there something else\nabout this problem that makes it even less observable?",
    "start": "1416310",
    "end": "1421350"
  },
  {
    "text": " Anybody has any idea,\nplease raise your hand.",
    "start": "1421350",
    "end": "1427785"
  },
  {
    "text": " Occlusion?",
    "start": "1427785",
    "end": "1433420"
  },
  {
    "text": "Yes, occlusions are possible. Yes, so there is\none other reason.",
    "start": "1433420",
    "end": "1438730"
  },
  {
    "text": "I could pose an unknown, right? I guess this is a\npose of the object.",
    "start": "1438730",
    "end": "1444820"
  },
  {
    "text": "You can't necessarily\ndefine materials, OK. So it might not be clear whether\nit's a compostible object or not. Yeah, these are\nall great reasons.",
    "start": "1444820",
    "end": "1451540"
  },
  {
    "text": "So one reason that we actually\nrealized quite recently, which was quite\ntricky, is that given an image of a robot holding\nan object above the band",
    "start": "1451540",
    "end": "1460240"
  },
  {
    "text": "that it's supposed to\ngo to, it's not clear whether you're about\nto get the reward 1,",
    "start": "1460240",
    "end": "1465910"
  },
  {
    "text": "because you're about to\nsort that object correctly or whether you unsorted\nit in the first place. So it was already in the correct\nbin at the very beginning",
    "start": "1465910",
    "end": "1473290"
  },
  {
    "text": "and you're about to drop it\ninto the same bin, in which case you will get a reward 0. So from that state alone, it's\nnot really possible to tell.",
    "start": "1473290",
    "end": "1481357"
  },
  {
    "text": "And this is actually\none of the big problems that contributed to making\nthis problem quite challenging.",
    "start": "1481357",
    "end": "1488294"
  },
  {
    "text": "All right, so as we can\ntell, partial observability can be quite tricky\nsometimes to determine.",
    "start": "1488295",
    "end": "1494830"
  },
  {
    "text": "But let's go back to the goal\nof reinforcement learning. So we talked a little bit\nabout the transition dynamics and Markov property.",
    "start": "1494830",
    "end": "1501010"
  },
  {
    "text": "The goal that we are trying to\noptimize is in the stationary, in the infinite horizon case.",
    "start": "1501010",
    "end": "1506860"
  },
  {
    "text": "We have some stationary state\nand action distribution, and we are trying to\nmaximize the expected reward",
    "start": "1506860",
    "end": "1513309"
  },
  {
    "text": "in that environment. So we're trying to find the\narg max of the parameters",
    "start": "1513310",
    "end": "1518530"
  },
  {
    "text": "of our policy theta\nthat will maximize on the expected reward, or\nin the finite horizon case,",
    "start": "1518530",
    "end": "1527530"
  },
  {
    "text": "we are trying to find\nan arg max and optimize some of the rewards\nfrom time step 1",
    "start": "1527530",
    "end": "1532830"
  },
  {
    "text": "to time step t of all\nthe expected rewards that are coming in the future. ",
    "start": "1532830",
    "end": "1541000"
  },
  {
    "text": "All right. So we know a little bit about\nthe reinforcement learning setting. So we talk -- this course is a\nlot about multi-task learning.",
    "start": "1541000",
    "end": "1548940"
  },
  {
    "text": "So what's a reinforcement\nlearning task? So in supervised\nlearning, if you recall,",
    "start": "1548940",
    "end": "1555030"
  },
  {
    "text": "a task was defined by a data\ngenerating distribution. So p of x and p of y given\nx as well as the loss.",
    "start": "1555030",
    "end": "1561990"
  },
  {
    "text": "In reinforcement\nlearning, a task will be defined by\na few more terms. So first, a task will\nconsist of a state space.",
    "start": "1561990",
    "end": "1569770"
  },
  {
    "text": "So these are all the\nstates that we can enter. The action space, so all the\nactions that we could take.",
    "start": "1569770",
    "end": "1577140"
  },
  {
    "text": "The initial state\ndistribution, the dynamics, so probability of\ns-prime given s,",
    "start": "1577140",
    "end": "1583155"
  },
  {
    "text": "a, as well as the\nreward function. And it's important to note\nhere that all of these",
    "start": "1583155",
    "end": "1589679"
  },
  {
    "text": "define the Markov\ndecision process, and this is much more than just\na semantic meaning of a task. So you can have a task\nthat semantically means--",
    "start": "1589680",
    "end": "1597973"
  },
  {
    "text": "or you can have two\ndifferent tasks that are doing semantically\nthe same thing but they are still\nconsidered different tasks",
    "start": "1597973",
    "end": "1603060"
  },
  {
    "text": "in the reinforcement\nlearning sense. All right, so let's go\nthrough some examples of task distributions.",
    "start": "1603060",
    "end": "1612410"
  },
  {
    "text": "So for example, in\ncharacter animation, we can have different\ntasks where we try to learn different maneuvers.",
    "start": "1612410",
    "end": "1618342"
  },
  {
    "text": "So for example, we can have\nthis little stick figure that is trying to learn\nhow to spin the cake, how to cartwheel, how\nto run and backflip.",
    "start": "1618343",
    "end": "1625010"
  },
  {
    "text": "And in this case,\nthe reward function would vary across these tasks.",
    "start": "1625010",
    "end": "1630590"
  },
  {
    "text": "We can also do\ncharacter animation where the character that\nis shown in the picture is trying to put on a shirt.",
    "start": "1630590",
    "end": "1637220"
  },
  {
    "text": "And here the shirts may vary,\nas well as the initial states. So the initial state\ndistribution may vary.",
    "start": "1637220",
    "end": "1642240"
  },
  {
    "text": "We might start with\na different shirt or the shirt might\nbe partly on already. And the dynamics will\nvary quite a bit as well",
    "start": "1642240",
    "end": "1648259"
  },
  {
    "text": "depending on which shirt\nyou're trying to put on. So these are\ndifferent tasks even though the reward is the same. We only get the reward\nwhen the shirt is on.",
    "start": "1648260",
    "end": "1656615"
  },
  {
    "text": "But the dynamics and the initial\nstate distributions will vary.  Another example would be\nmulti-robot reinforcement",
    "start": "1656615",
    "end": "1663680"
  },
  {
    "text": "learning. Here we have three\ndifferent robots. So here the state space will\nbe different for all of them.",
    "start": "1663680",
    "end": "1670789"
  },
  {
    "text": "The action space will\nbe different as well. The initial state\ndistribution too, as well as the transmission dynamics\neven though all of them",
    "start": "1670790",
    "end": "1677602"
  },
  {
    "text": "can be trained to\nperform the same tasks. ",
    "start": "1677602",
    "end": "1683700"
  },
  {
    "text": "All right. Great. So we talked a\nlittle bit about what is a task in a\nreinforcement learning view, but we can actually have\nan alternative view.",
    "start": "1683700",
    "end": "1691590"
  },
  {
    "text": "We can augment our\nstate a little bit. And we can-- here we'll denote\ns bar as the original state",
    "start": "1691590",
    "end": "1698220"
  },
  {
    "text": "that we have. And we can add a task\nidentifier to that state.",
    "start": "1698220",
    "end": "1703289"
  },
  {
    "text": "So we'll just append\nthat to our state vector, the task identifier\ncalled zi which",
    "start": "1703290",
    "end": "1708840"
  },
  {
    "text": "tells us if we are in task i. So this is the original state\nand we augment it a little bit.",
    "start": "1708840",
    "end": "1715430"
  },
  {
    "text": "So in this case, the\ntask that was originally defined like this,\nthe multi-task setup",
    "start": "1715430",
    "end": "1721190"
  },
  {
    "text": "could be defined\nas a single task. So in this case,\nthe task would be defined by the union\nof all of the tasks",
    "start": "1721190",
    "end": "1728870"
  },
  {
    "text": "that we are considering. So in terms of the\nstate space, the status of that new multi-task\ntask will consist",
    "start": "1728870",
    "end": "1736010"
  },
  {
    "text": "of the union of all the states\nspaces of all the tasks, the union of all\nthe action spaces,",
    "start": "1736010",
    "end": "1742010"
  },
  {
    "text": "the mixture of the initial\nstate distribution for all of the tasks, the transmission\ndynamics, and the reward.",
    "start": "1742010",
    "end": "1749600"
  },
  {
    "text": "There's an\nalternative view where a multi-task reinforcement\nlearning problem is still a one Markov decision process.",
    "start": "1749600",
    "end": "1754955"
  },
  {
    "text": " Great. ",
    "start": "1754955",
    "end": "1762106"
  },
  {
    "text": "So in terms of the goal\nof multitask reinforcement learning, it would look\nexactly the same as before, except we will augment our state\nwith this task identifier, zi.",
    "start": "1762107",
    "end": "1771759"
  },
  {
    "text": "And that zi could be\nmany different things. So it could be a\none-hot task ID, tells you which task you are in.",
    "start": "1771760",
    "end": "1777370"
  },
  {
    "text": "It could be a\nlanguage description, or it could be a\ndesired goal state. So for example, an image\nof what you're trying,",
    "start": "1777370",
    "end": "1783840"
  },
  {
    "text": "what I want you to achieve. In this case, the\ndesired goal state",
    "start": "1783840",
    "end": "1789150"
  },
  {
    "text": "would be corresponding to a\nstate that we want to reach, sg.",
    "start": "1789150",
    "end": "1794352"
  },
  {
    "text": "And this will be referred to\nas goal-conditioned RL, which is a special case of a\nmulti-task reinforcement learning problem.",
    "start": "1794353",
    "end": "1799725"
  },
  {
    "text": " What could be a\nreward in that case?",
    "start": "1799725",
    "end": "1804980"
  },
  {
    "text": "So it could be exactly\nthe same as before. Depending on the task, we'll\nbe getting different rewards. Or for goal-conditioned\nRL, the reward",
    "start": "1804980",
    "end": "1811970"
  },
  {
    "text": "could be a distance\nfunction that tells us the distance between our current\nstate s bar and the goal state",
    "start": "1811970",
    "end": "1818390"
  },
  {
    "text": "sg. And we're trying to minimize\nthe distance so the reward will be the negative distance.",
    "start": "1818390",
    "end": "1826538"
  },
  {
    "text": "There could be different\ndistance functions that we can use for this,\nsuch as the Euclidean distance or a sparse reward so you\nonly get the reward when",
    "start": "1826538",
    "end": "1833790"
  },
  {
    "text": "you match the state exactly. Otherwise, you get 0. But we'll talk about this a\nlittle bit more later, too.",
    "start": "1833790",
    "end": "1840920"
  },
  {
    "text": "All right, so one\nquestion you can ask is, if it's still a standard\nMDP, a Markov decision process, then why can't we just apply\nstandard RL algorithms?",
    "start": "1840920",
    "end": "1850420"
  },
  {
    "text": "And the answer to\nthis is, yes, you can. You totally can. But very often, we can\ndo much better than that.",
    "start": "1850420",
    "end": "1857140"
  },
  {
    "text": "And that's what we'll\nlearn about in this course. All right, so are\nthere any questions",
    "start": "1857140",
    "end": "1863150"
  },
  {
    "text": "about the multi-task\nreinforcement learning problem? ",
    "start": "1863150",
    "end": "1870850"
  },
  {
    "text": "I've one quick question,\nshould be easy to answer. So is the main point you're\nmaking with adding z sub",
    "start": "1870850",
    "end": "1879300"
  },
  {
    "text": "i to the state that in a\nsingle-task RL problem, this would just be\ntrivially like part",
    "start": "1879300",
    "end": "1885667"
  },
  {
    "text": "of the context of\nthe overall problem, but this gives you\nadding the z sub i, you have a way to sort of embed\nmany tasks in some slightly",
    "start": "1885667",
    "end": "1894268"
  },
  {
    "text": "higher dimensional state space. And you already talked\nabout how to solve this problem\nnontrivially, by a method",
    "start": "1894268",
    "end": "1902409"
  },
  {
    "text": "other than just using RL. Right, so zi-- Yes, I think\nthat's a correct intuition. zi",
    "start": "1902410",
    "end": "1907630"
  },
  {
    "text": "just makes the entire\nMDP observable again. It basically tells you which\ntask you're in currently.",
    "start": "1907630",
    "end": "1914410"
  },
  {
    "text": "And regardless how\nit's represented, it's one-hot or goal\nimage or something else, it disambiguates fully which\ntask you're trying to do.",
    "start": "1914410",
    "end": "1923410"
  },
  {
    "text": "And that's what allows you\nto cast it as a single MDP as a single reinforcement\nlearning problem.",
    "start": "1923410",
    "end": "1931018"
  },
  {
    "text": "Cool, thanks. ",
    "start": "1931018",
    "end": "1938129"
  },
  {
    "text": "All right, there's\nanother question. If the tasks done belong to\nthe same environment, then can we still consider\nunder the same MDP?",
    "start": "1938130",
    "end": "1944790"
  },
  {
    "text": "Yeah, that's a\nreally good question.  So I think usually\nwe wouldn't do that.",
    "start": "1944790",
    "end": "1951610"
  },
  {
    "text": "I think we can still consider\nthem as separate MDPs. But it is possible to\nphrase them as a single MDP",
    "start": "1951610",
    "end": "1956820"
  },
  {
    "text": "as well where we just\nconsider the transmission dynamics of the union\nof all of the dynamics of both environments,\nand the same",
    "start": "1956820",
    "end": "1963840"
  },
  {
    "text": "applies to the state space\nand the action space. ",
    "start": "1963840",
    "end": "1969870"
  },
  {
    "text": "All right, great. Sorry, one more question\nif you don't mind. What's the significance\nof your distinction",
    "start": "1969870",
    "end": "1976659"
  },
  {
    "text": "between the optimality\nvariable and the reward? Is that easy to answer why\nthere's a difference there?",
    "start": "1976660",
    "end": "1981759"
  },
  {
    "text": "Yeah, so this is a\nlittle more tricky. And we'll talk about it\nin two weeks, I think. We'll have a full\nlecture on that.",
    "start": "1981760",
    "end": "1988330"
  },
  {
    "text": "Cool. All right. All right, so we\nspecified the problem.",
    "start": "1988330",
    "end": "1993730"
  },
  {
    "text": "Let's look a little bit\nat ways to solve it. So we'll talk about the\npolicy gradient method",
    "start": "1993730",
    "end": "1999730"
  },
  {
    "text": "and how we can change it\nto be a multitask method and how it works. ",
    "start": "1999730",
    "end": "2006830"
  },
  {
    "start": "2005000",
    "end": "2102000"
  },
  {
    "text": "So to start, I would\nlike to tell you a little bit about the anatomy\nof any reinforcement learning algorithm. And they usually\nconsist of three stages.",
    "start": "2006830",
    "end": "2013909"
  },
  {
    "text": "There is a stage where\nwe generate samples. We can do this by, for\nexample, running our policy. There is a stage where\nwe try to fit the model",
    "start": "2013910",
    "end": "2020540"
  },
  {
    "text": "to estimate the\nreturn and the stage where we improve the policy. And different algorithms\nwould implement these boxes",
    "start": "2020540",
    "end": "2026420"
  },
  {
    "text": "in different ways. So for example, to fit a\nmodel to estimate a return, what policy gradients\nwould do, it",
    "start": "2026420",
    "end": "2032330"
  },
  {
    "text": "would just sum\nall of the rewards that you got in the\ncurrent rollout. This is also referred to\nas Monte Carlo Estimation.",
    "start": "2032330",
    "end": "2040430"
  },
  {
    "text": " Different Q-learning\nmethods would allow you to try to fit a\nQ-function to your returns.",
    "start": "2040430",
    "end": "2048359"
  },
  {
    "text": "And this would be\nsomething that would represent the future returns.",
    "start": "2048360",
    "end": "2054135"
  },
  {
    "text": "And model based\nmethods would try to estimate the transition\nprobability of p of s-prime given s, a.",
    "start": "2054135",
    "end": "2062169"
  },
  {
    "text": "Then in terms of\nimproving the policies, the policy gradient\nmethod will try to optimize the objective\nthat we care about directly",
    "start": "2062170",
    "end": "2068672"
  },
  {
    "text": "and apply the gradient to it. And we'll learn\nabout it in a second. Q-learning methods would try to\nmaximize the Q-functions that",
    "start": "2068672",
    "end": "2076859"
  },
  {
    "text": "was already learned, so find\nthe action that maximizes it. And that's the action that\nyou would want to perform.",
    "start": "2076860",
    "end": "2081989"
  },
  {
    "text": "And for model-based methods,\none thing you can do is, for example, back\npropagate through the model and optimize directly\nfor the policy.",
    "start": "2081989",
    "end": "2090406"
  },
  {
    "text": "So in this lecture,\nwe'll focus mostly on model-free RL methods,\nsuch as policy gradient and Q-learning. And you'll learn more about\nmodel-based RL methods,",
    "start": "2090406",
    "end": "2098510"
  },
  {
    "text": "I think, in the next lecture\nor maybe the one after that. All right, so before we\ntalk about policy gradients,",
    "start": "2098510",
    "end": "2105810"
  },
  {
    "start": "2102000",
    "end": "2180000"
  },
  {
    "text": "I would like to bring up\none more concept, which is the on-policy versus\noff-policy reinforcement learning.",
    "start": "2105810",
    "end": "2112290"
  },
  {
    "text": "So in the on-policy\nreinforcement learning, the important distinction\nis that the data that we are training on comes\nfrom our current policy.",
    "start": "2112290",
    "end": "2119790"
  },
  {
    "text": "We can only use\nour current policy to do rollouts to actually\nsee how they work. And then we can use that\ndata to update our policy.",
    "start": "2119790",
    "end": "2127950"
  },
  {
    "text": "On-policy data is\ncompatible with pretty much all reinforcement\nlearning algorithms.",
    "start": "2127950",
    "end": "2133530"
  },
  {
    "text": "And it also means that\nwe can't reuse data from previous policies. You can only use\ndata from the policy",
    "start": "2133530",
    "end": "2139150"
  },
  {
    "text": "that you're currently learning. That's why it's on-policy,\nor learning on the policy that you're actually executing.",
    "start": "2139150",
    "end": "2146700"
  },
  {
    "text": "For off-policy algorithms, the\ndata can come from any policy. It actually doesn't even have\nto come from a policy at all.",
    "start": "2146700",
    "end": "2152519"
  },
  {
    "text": "It can be some data\nthat was collected by humans or by some engineer\nscript or something like that.",
    "start": "2152520",
    "end": "2162110"
  },
  {
    "text": "It only works with\nspecific RL algorithms. And we'll learn about that too. And it can be much\nmore sample efficient",
    "start": "2162110",
    "end": "2168650"
  },
  {
    "text": "because now we can reuse\nall of the old data as well. We are not restricted\nto the data that we currently see\nbut we can use data",
    "start": "2168650",
    "end": "2175490"
  },
  {
    "text": "from other tasks or older data.  All right, so in this\npart of the lecture,",
    "start": "2175490",
    "end": "2182790"
  },
  {
    "start": "2180000",
    "end": "2791000"
  },
  {
    "text": "we'll be talking about\npolicy gradients. And in policy gradients,\nwe'll be staying on-policy. So we'll be only\nusing the data that",
    "start": "2182790",
    "end": "2189020"
  },
  {
    "text": "comes from the current policy.  All right.",
    "start": "2189020",
    "end": "2196580"
  },
  {
    "text": "So policy gradients try to\ndirectly optimize the objective that we care about.",
    "start": "2196580",
    "end": "2201880"
  },
  {
    "text": "The objective that\nwe care about, which we talked about\njust a few minutes ago,",
    "start": "2201880",
    "end": "2207010"
  },
  {
    "text": "is the expected sum of rewards. This is depicted here. You are trying to find\nthe arg max of that",
    "start": "2207010",
    "end": "2213640"
  },
  {
    "text": "where we are trying to\noptimize the parameters theta of our policy. And the one term that might\nbe a little unfamiliar to you",
    "start": "2213640",
    "end": "2220809"
  },
  {
    "text": "is this term right\nhere, the tau. And that tau is\nnothing else in the--",
    "start": "2220810",
    "end": "2229329"
  },
  {
    "text": "let's just write it right\nhere. p of theta of tau.",
    "start": "2229330",
    "end": "2234580"
  },
  {
    "text": "There's the joint\nprobability distribution of all the states\nand actions which can be done factored\nas this probability",
    "start": "2234580",
    "end": "2243830"
  },
  {
    "text": "of the initial state times our\npolicy pi-theta of the action",
    "start": "2243830",
    "end": "2250460"
  },
  {
    "text": "a1 given state s1 times\nthe transition probability",
    "start": "2250460",
    "end": "2255540"
  },
  {
    "text": "p of s2 given s1, a1 times the\npolicy pi-theta of a2 given s2,",
    "start": "2255540",
    "end": "2272330"
  },
  {
    "text": "and so on. And that at the end results in\na joint probability distribution",
    "start": "2272330",
    "end": "2278330"
  },
  {
    "text": "of actions over all the\nactions and all the states. So you are trying to\ntake the expectation",
    "start": "2278330",
    "end": "2283370"
  },
  {
    "text": "under that trajectory\ndistribution that is dependent on our\nparameters theta. This is the parameters\nof our policy.",
    "start": "2283370",
    "end": "2289340"
  },
  {
    "text": "They influence what kind\nof trajectory is policy and we're trying to maximize\nover that expectation the sum of rewards.",
    "start": "2289340",
    "end": "2296360"
  },
  {
    "text": "All right, so we'll refer to\nthat objective as z of theta, right? So the objective is z of theta.",
    "start": "2296360",
    "end": "2302390"
  },
  {
    "text": " So we are just rewriting\nthis here again.",
    "start": "2302390",
    "end": "2308020"
  },
  {
    "text": "And because this is an\nexpectation under trajectories, what we can do is we can\napproximate it with samples.",
    "start": "2308020",
    "end": "2313640"
  },
  {
    "text": "So what that would look\nin the real world was we would take our current\npolicy, roll it out.",
    "start": "2313640",
    "end": "2319480"
  },
  {
    "text": "So just run it a bunch\nof times and generate a few trajectories. For example, three\ntrajectories like that.",
    "start": "2319480",
    "end": "2324880"
  },
  {
    "text": "And then we will get the\nrewards for these trajectories. So let's say the top trajectory\nwill get the top reward, the middle one will\nget the medium reward,",
    "start": "2324880",
    "end": "2331100"
  },
  {
    "text": "and that one at the\nbottom will get 0 reward. Now to evaluate our objective,\nwe can just sum over samples",
    "start": "2331100",
    "end": "2339030"
  },
  {
    "text": "from our policy. So we can sum all the\nrewards for each one of the trajectories and then\ndo an average across all",
    "start": "2339030",
    "end": "2344593"
  },
  {
    "text": "of these trajectories. This is just to\nevaluate the objective. So now given the policy,\nwe can fairly easily",
    "start": "2344593",
    "end": "2349830"
  },
  {
    "text": "say how good it is.  All right, so now that we\nknow how to evaluate it,",
    "start": "2349830",
    "end": "2359549"
  },
  {
    "text": "let's try to see if\nwe can improve it, what can we do to actually\ndirectly improve our policy.",
    "start": "2359550",
    "end": "2366150"
  },
  {
    "text": "So we have our\nobjective J-theta. And here, we'll do a\nlittle shortcut in notation",
    "start": "2366150",
    "end": "2372359"
  },
  {
    "text": "where we'll say that r of tau\nis equal to the sum of all the rewards that will\nget in the trajectory.",
    "start": "2372360",
    "end": "2378599"
  },
  {
    "text": "Because tau corresponds to\nthe trajectory, r of tau will be the full\nreturn that we get, the sum of all the rewards\nfor that trajectory.",
    "start": "2378600",
    "end": "2385138"
  },
  {
    "text": " All right. Since we are taking the\nexpectation under tau,",
    "start": "2385138",
    "end": "2391460"
  },
  {
    "text": "we can write that expectation\nas the integral of pi-theta tau, r of tau, d-tau,\nwhich would mean",
    "start": "2391460",
    "end": "2398539"
  },
  {
    "text": "that would mean to integrate\nover all the taus, all the potential trajectories. Now let's try to take a gradient\nof respect to that objective.",
    "start": "2398540",
    "end": "2407128"
  },
  {
    "text": "So the gradient, if we\njust apply directly, would look something like that. We have an integral of\nthe gradient of respect",
    "start": "2407128",
    "end": "2412790"
  },
  {
    "text": "to theta of our pi-theta r,\npi-theta of tau, r of tau,",
    "start": "2412790",
    "end": "2418520"
  },
  {
    "text": "d-tau. And that would mean that\nwe need to integrate over all the trajectories which\nis completely intractable.",
    "start": "2418520",
    "end": "2425220"
  },
  {
    "text": "So this is a very\ntricky objective, but there is a single\nidentity that is often",
    "start": "2425220",
    "end": "2431250"
  },
  {
    "text": "referred to as a log trick. That turns out to\nbe very useful here.",
    "start": "2431250",
    "end": "2437089"
  },
  {
    "text": "So let's try to derive\nthis real quick. So let's try the following.",
    "start": "2437090",
    "end": "2442680"
  },
  {
    "text": "Gradient with respect to\nx of logarithm of f of x.",
    "start": "2442680",
    "end": "2449998"
  },
  {
    "text": "Let's just try to estimate\nwhat that gradient is. I just kind of made it up. And let's see where\nwe get from there.",
    "start": "2449998",
    "end": "2456330"
  },
  {
    "text": "So this would be equal\nto 1 over f of x--",
    "start": "2456330",
    "end": "2462380"
  },
  {
    "text": "this is just the gradient\nof the logarithm-- times the inner-gradient which\nwould be the gradient respect",
    "start": "2462380",
    "end": "2468710"
  },
  {
    "text": "to x of f of x. I just threw that right in.",
    "start": "2468710",
    "end": "2475550"
  },
  {
    "text": "So now what we can do? We can multiply both\nsides by f of x.",
    "start": "2475550",
    "end": "2480760"
  },
  {
    "text": "We will get something like that. We would get-- so\nthe right side--",
    "start": "2480760",
    "end": "2487232"
  },
  {
    "text": "I'll write the left\nside right now. So we'll get grad x, f of x will\nbe equal to-- we just multiply",
    "start": "2487232",
    "end": "2495970"
  },
  {
    "text": "both sides by f of x,\nwould be equal to f of x times grad x log f of x.",
    "start": "2495970",
    "end": "2507140"
  },
  {
    "text": "All right, so you have that\nlittle identity right here. And you can see--",
    "start": "2507140",
    "end": "2514350"
  },
  {
    "text": "we didn't even talk about\nreinforcement learning here. Here we are just trying to\nshow a mathematical proof.",
    "start": "2514350",
    "end": "2521000"
  },
  {
    "text": "And this says that-- this has some familiar terms. So we have this--",
    "start": "2521000",
    "end": "2526533"
  },
  {
    "text": "We can just change\nthe color real quick. ",
    "start": "2526533",
    "end": "2531940"
  },
  {
    "text": "We have a term right here. It looks very similar\nwith a term right here.",
    "start": "2531940",
    "end": "2539119"
  },
  {
    "text": " So I also put this\nidentity here in the slide",
    "start": "2539120",
    "end": "2544560"
  },
  {
    "text": "so that you can see it in the\nPDF that is on the website. And this is the observation\nthat we just made.",
    "start": "2544560",
    "end": "2551310"
  },
  {
    "text": "So the gradient of f of x,\nor the gradient of pi-theta is the same gradient\nthat we see right here.",
    "start": "2551310",
    "end": "2557750"
  },
  {
    "text": "So let's just replace\nit with the identity that we just derived. So here, just our\nf of x right here",
    "start": "2557750",
    "end": "2564619"
  },
  {
    "text": "got transformed into\npi-theta of tau.",
    "start": "2564620",
    "end": "2572950"
  },
  {
    "text": "Instead of x as a tau and\ninstead of f as a pi-theta. ",
    "start": "2572950",
    "end": "2578643"
  },
  {
    "text": "All right, so we\njust replace that according to that identity. And now we have an equation\nthat can be actually rephrased",
    "start": "2578643",
    "end": "2585610"
  },
  {
    "text": "as an expectation again. So now because we have an\nintegral over d-theta--",
    "start": "2585610",
    "end": "2590770"
  },
  {
    "text": "over d-tau, sorry,\nover trajectories. So d-tau.",
    "start": "2590770",
    "end": "2596110"
  },
  {
    "text": "And inside of integral,\nwe have the term called pi-theta of tau. We can say that this is\nthe expectation of this",
    "start": "2596110",
    "end": "2603130"
  },
  {
    "text": "under the trajectory\ndistribution tau of grad log pi times r of tau.",
    "start": "2603130",
    "end": "2610240"
  },
  {
    "text": " All right, so far\nwe said that we",
    "start": "2610240",
    "end": "2618340"
  },
  {
    "text": "are trying to optimize\nthat objective J of theta. The objective is fairly\neasy to evaluate.",
    "start": "2618340",
    "end": "2623590"
  },
  {
    "text": "It's right here. And if we want to take the\ngradient with respect to it, we can do it by doing\nthat expectation",
    "start": "2623590",
    "end": "2629350"
  },
  {
    "text": "and evaluating that term\nunder that expectation. All right.",
    "start": "2629350",
    "end": "2634380"
  },
  {
    "text": "So you can see that there\nis a lot of taus in here. So let's bring\nback the definition of what tau is and see how\nit actually corresponds",
    "start": "2634380",
    "end": "2641280"
  },
  {
    "text": "to our actual policy. So we have our pi of\npi-tau of pi-theta of tau",
    "start": "2641280",
    "end": "2649810"
  },
  {
    "text": "is equal to this. And this is the joint\ndistribution of all our states",
    "start": "2649810",
    "end": "2656710"
  },
  {
    "text": "and actions and this factors as\nthe initial state distribution and times the policy plus times\nthe transmission dynamics.",
    "start": "2656710",
    "end": "2666870"
  },
  {
    "text": "Yes, well, there's a question. Can you explain how\ndoes this trick solve the tractability problem? Yes, we'll get to\nit in just a second.",
    "start": "2666870",
    "end": "2675110"
  },
  {
    "text": "All right, so we have our\npi-tau, pi-theta of tau. Now if we take the\nlogarithm on both sides,",
    "start": "2675110",
    "end": "2682650"
  },
  {
    "text": "we see that something\nlike this happens. We have our log pi-theta of tau.",
    "start": "2682650",
    "end": "2688040"
  },
  {
    "text": "And now we just replace all the\nmultiplications using the fact that the logarithm of a product\nis the sum of the logarithms,",
    "start": "2688040",
    "end": "2695690"
  },
  {
    "text": "and we end up with an\nequation like that. And now we can see\nthat we can replace",
    "start": "2695690",
    "end": "2702800"
  },
  {
    "text": "the log of pi-theta of\ntau with the equation",
    "start": "2702800",
    "end": "2708410"
  },
  {
    "text": "that we just derived right here. So let's do that. Let's swap that in.",
    "start": "2708410",
    "end": "2714670"
  },
  {
    "text": "So if we do this, we\ncan now see that it has all the familiar terms. We have the initial\nstate distribution.",
    "start": "2714670",
    "end": "2721240"
  },
  {
    "text": "We have the policy distribution. And we have the transition,\nthe model of the transition.",
    "start": "2721240",
    "end": "2726312"
  },
  {
    "text": "And we have to take\ngradient with respect to theta, the entire thing. This is the gradient\nthat comes from here.",
    "start": "2726312",
    "end": "2733843"
  },
  {
    "text": "All right, so now if we take the\ngradient with respect to theta, the initial state distribution\ndoes not depend on theta.",
    "start": "2733843",
    "end": "2738850"
  },
  {
    "text": "This is not something\nthat we can control. That's given to us\nas part of the task. So the gradient with\nrespect to theta would be 0.",
    "start": "2738850",
    "end": "2748030"
  },
  {
    "text": "And the same is true for\nthe transition dynamics. So we don't get to control this. This is not dependent\non our policy.",
    "start": "2748030",
    "end": "2753160"
  },
  {
    "text": "So the gradient with respect\nto the parameters of the policy theta would be 0. ",
    "start": "2753160",
    "end": "2759110"
  },
  {
    "text": "All right, so what\nwe end up with is then something\nthat looks like that. This is just basically\ncrossing out the terms",
    "start": "2759110",
    "end": "2764630"
  },
  {
    "text": "and putting everything together. The gradient with\nrespect to our objective",
    "start": "2764630",
    "end": "2770674"
  },
  {
    "text": "is equal to an expectation under\nour trajectory distribution tau of the sum over grad log\npi and the sum times the sum",
    "start": "2770675",
    "end": "2780738"
  },
  {
    "text": "of all the rewards.  All right, cool. So we derived a gradient\nthat we can actually",
    "start": "2780738",
    "end": "2787400"
  },
  {
    "text": "apply to try to make\nour policy better. So now there is a question.",
    "start": "2787400",
    "end": "2792870"
  },
  {
    "start": "2791000",
    "end": "2975000"
  },
  {
    "text": "How does this solve the\ntractability problem? So basically this\nquestion can be translated to how can\nwe evaluate that policy",
    "start": "2792870",
    "end": "2799650"
  },
  {
    "text": "gradient easily?  So you can recall that we can\nevaluate the objective fairly",
    "start": "2799650",
    "end": "2806150"
  },
  {
    "text": "easily by approximating the\nexpectation over trajectories by just samples over these\ntrajectories, all right?",
    "start": "2806150",
    "end": "2811676"
  },
  {
    "text": "So remember that, the picture\nthat we talked about where we had three\ndifferent trajectories",
    "start": "2811676",
    "end": "2818180"
  },
  {
    "text": "and we can just average\nacross the samples. So we can apply exact same\ntrick right here as well.",
    "start": "2818180",
    "end": "2824290"
  },
  {
    "text": "We have the exact\nsame expectation. This is the expectation under\nthe trajectory distribution tau.",
    "start": "2824290",
    "end": "2829750"
  },
  {
    "text": "And we can take the\naverage so we can basically sample our policy and get our\ntrajectories and an average",
    "start": "2829750",
    "end": "2838030"
  },
  {
    "text": "across the samples.  So if we do that, then\nnow we don't really",
    "start": "2838030",
    "end": "2843720"
  },
  {
    "text": "have to take that integral\nthat was so intractable. We can just approximate\nour expectation of samples, which is what we\ndo very, very often in machine",
    "start": "2843720",
    "end": "2850800"
  },
  {
    "text": "learning. And now all of the\nother terms are actually very easy to evaluate for us. We know what log of pi is.",
    "start": "2850800",
    "end": "2857460"
  },
  {
    "text": "This would be the logarithm\nof the output of our policy. And we can also sum\nall the rewards.",
    "start": "2857460",
    "end": "2862650"
  },
  {
    "text": " So now if we wanted to actually\nmake it into an algorithm,",
    "start": "2862650",
    "end": "2870119"
  },
  {
    "text": "we could just take that gradient\nand apply a gradient ascent algorithm to it and improve\nour policy this way.",
    "start": "2870120",
    "end": "2878190"
  },
  {
    "text": "So now if we go back to\nour little diagram of how",
    "start": "2878190",
    "end": "2884060"
  },
  {
    "text": "reinforcement learning\nalgorithms work, we generate samples right here.",
    "start": "2884060",
    "end": "2890700"
  },
  {
    "text": "Let me switch to that pointer. Generate samples right here. This is the approximation of\nthe expectation of sample.",
    "start": "2890700",
    "end": "2897480"
  },
  {
    "text": "So this is our orange box. We fit the model to estimate\nthe return right here.",
    "start": "2897480",
    "end": "2904400"
  },
  {
    "text": "So in this case, this is\nthe Monte Carlo Estimation of our returns, which is sum\nall the returns we actually got from the trajectories.",
    "start": "2904400",
    "end": "2911450"
  },
  {
    "text": "And to improve the policy, we\ndo the gradient ascent procedure where we are applying the\ngradient that we can evaluate,",
    "start": "2911450",
    "end": "2918270"
  },
  {
    "text": "we know how to\nevaluate it right here. And we apply it to\nour policy parameters.",
    "start": "2918270",
    "end": "2925845"
  },
  {
    "text": "And because of that, we can\nget better and better policy.  All right, so all\nof these things,",
    "start": "2925845",
    "end": "2931850"
  },
  {
    "text": "if we put them together,\nwe can implement a fairly straightforward\nalgorithm called reinforce.",
    "start": "2931850",
    "end": "2937130"
  },
  {
    "text": "And this is the basic\npolicy gradient algorithm. The way it works is as follows. We sample a bunch\nof trajectories",
    "start": "2937130",
    "end": "2943250"
  },
  {
    "text": "from our current policy\nby just running it. So we produce the trajectories. Then we evaluate our gradient.",
    "start": "2943250",
    "end": "2950536"
  },
  {
    "text": "So we basically do the\nequation that we just talked about right here.",
    "start": "2950537",
    "end": "2955830"
  },
  {
    "text": "And then we apply that\ngradient to our parameters using gradient ascent procedure.",
    "start": "2955830",
    "end": "2961538"
  },
  {
    "text": "And we keep doing that\nover and over again. So we apply that gradient,\nwe get a policy that is a little bit better.",
    "start": "2961538",
    "end": "2966890"
  },
  {
    "text": "Then we sample from that\npolicy and do this iteratively over and over until\nthe policy gets better.",
    "start": "2966890",
    "end": "2972019"
  },
  {
    "text": " All right, so there is\none interesting fact",
    "start": "2972020",
    "end": "2979470"
  },
  {
    "text": "about this entire\nprocedure, which is that it's actually fairly\nsimilar to a maximum likelihood",
    "start": "2979470",
    "end": "2984930"
  },
  {
    "text": "model that you would use\nfor imitation learning. So if you recall\nthat flow example",
    "start": "2984930",
    "end": "2990220"
  },
  {
    "text": "that we had with drivers where\nwe were collecting states and actions and collecting\ntraining data sets",
    "start": "2990220",
    "end": "2997059"
  },
  {
    "text": "and doing supervised learning\non it to get our policy, the objective that\nwe would actually try to optimize to achieve that\nwould be a maximum likelihood",
    "start": "2997060",
    "end": "3004920"
  },
  {
    "text": "objective that is well known\nfrom a standard supervised learning.",
    "start": "3004920",
    "end": "3010432"
  },
  {
    "text": "Now there's a question. Where represents the\nmulti-tasking reinforce? We'll talk a little bit\nabout the multi-tasking in one second. ",
    "start": "3010432",
    "end": "3018829"
  },
  {
    "text": "So you can see that the\nimitation learning objective",
    "start": "3018830",
    "end": "3024320"
  },
  {
    "text": "would look like a\nmaximum likelihood objective that is right here. This is just maximum\nlikelihood of our actions",
    "start": "3024320",
    "end": "3031460"
  },
  {
    "text": "that were taken by humans. And the policy\ngradient objective",
    "start": "3031460",
    "end": "3036470"
  },
  {
    "text": "is actually extremely\nsimilar, if you look at it. So the first term\nright here, it's",
    "start": "3036470",
    "end": "3042980"
  },
  {
    "text": "pretty much exactly the same. So these are the samples\nfrom our current policy. And these are the samples\nfrom the trajectories",
    "start": "3042980",
    "end": "3049130"
  },
  {
    "text": "that we got from the humans. But we have this multiplication,\nthis addition and--",
    "start": "3049130",
    "end": "3055940"
  },
  {
    "text": "multiplication that multiplies\nit by the sum of the rewards. And that means a few things.",
    "start": "3055940",
    "end": "3061760"
  },
  {
    "text": "So first, that means that\nwe can make these two algorithms completely\nequivalent if we set a reward",
    "start": "3061760",
    "end": "3067910"
  },
  {
    "text": "function to a very specific\nreward function that says that every time you\nsucceed at something, you will get a reward,\nthe sum of reward is 1.",
    "start": "3067910",
    "end": "3074690"
  },
  {
    "text": "And if you fail at something,\nthe sum of rewards would be 0. Then both of these algorithms\nlook exactly the same.",
    "start": "3074690",
    "end": "3080455"
  },
  {
    "text": "It would just mean\nthat you would have to do imitation learning\non your current policy on your current rollouts, update\nyour policy according to it,",
    "start": "3080455",
    "end": "3088520"
  },
  {
    "text": "and then do rollouts again,\nupdate the policy using only the positives, and so on.",
    "start": "3088520",
    "end": "3094740"
  },
  {
    "text": "The other thing to notice is\nthat because this objective is so similar to a supervised\nlearning objective, actually",
    "start": "3094740",
    "end": "3100693"
  },
  {
    "text": "all of the things that you\nlearn about multi-task learning algorithms can be\nreadily applied here too.",
    "start": "3100693",
    "end": "3106017"
  },
  {
    "text": "Everything that you learn about\nmulti-task supervised learning can be very, very easily\nused in this setting as well.",
    "start": "3106017",
    "end": "3111510"
  },
  {
    "text": " All right, so a little\nrecap of what just happened.",
    "start": "3111510",
    "end": "3117600"
  },
  {
    "start": "3114000",
    "end": "3213000"
  },
  {
    "text": "So we derived a way to\napproximate the gradient with respect to our objective.",
    "start": "3117600",
    "end": "3123330"
  },
  {
    "text": " And we show that it's very\nsimilar to maximum likelihood.",
    "start": "3123330",
    "end": "3128990"
  },
  {
    "text": "So what does that\nactually mean in practice? So if you didn't pay\nattention and you didn't follow all the math,\nwhat does this algorithm",
    "start": "3128990",
    "end": "3136240"
  },
  {
    "text": "actually do? So what it does\nis the following. We roll out a bunch\nof trajectories",
    "start": "3136240",
    "end": "3141250"
  },
  {
    "text": "according to our current policy. You can see them here, say\nthree different trajectories.",
    "start": "3141250",
    "end": "3147350"
  },
  {
    "text": "And they get different rewards. So we sum the rewards for each\none of these trajectories. And what policy\ngradient is trying to do",
    "start": "3147350",
    "end": "3153380"
  },
  {
    "text": "is trying to make-- trying to shift the\ndistribution of our policy",
    "start": "3153380",
    "end": "3159650"
  },
  {
    "text": "to make the good\nstuff, the stuff that got really high\nreward more likely and the bad stuff\nthe less likely.",
    "start": "3159650",
    "end": "3167150"
  },
  {
    "text": "So basically if you\ngot a high reward, we'll maximize the\nlikelihood of those actions",
    "start": "3167150",
    "end": "3173030"
  },
  {
    "text": "and the bad stuff will\nmake it less likely. We'll maximize the less. And if the reward was\n0, we won't do it.",
    "start": "3173030",
    "end": "3179370"
  },
  {
    "text": "We won't do anything with\nit because if you multiply-- if this is equal to 0,\nthat sum over the rewards",
    "start": "3179370",
    "end": "3184760"
  },
  {
    "text": "in the gradient will\nbe equal to 0 as well. So if it didn't succeed,\nif it didn't get any reward for the current task or\nfor the current rollout,",
    "start": "3184760",
    "end": "3193730"
  },
  {
    "text": "that won't contribute\nto the gradient. So basically that entire\npolicy gradient procedure",
    "start": "3193730",
    "end": "3199070"
  },
  {
    "text": "simply formalizes the\nnotion of trial and error. We'll try some\nstuff, and then we'll increase the probability\nof good stuff",
    "start": "3199070",
    "end": "3205220"
  },
  {
    "text": "and reduce the\nprobability of bad stuff. And this is what\nREINFORCE algorithm does.",
    "start": "3205220",
    "end": "3210330"
  },
  {
    "text": " All right, so to summarize\nthe policy gradient procedure,",
    "start": "3210330",
    "end": "3218415"
  },
  {
    "start": "3213000",
    "end": "3322000"
  },
  {
    "text": "we know how to take the gradient\nwith respect to the objective. And there's a few pros\nof policy gradients. First is very simple.",
    "start": "3218415",
    "end": "3224910"
  },
  {
    "text": "We can easily combine\nthem with existing multitasking meta-RL and\nmeta-learning algorithms.",
    "start": "3224910",
    "end": "3231420"
  },
  {
    "text": "There are also quite a few cons. So first, the gradient\nthat we produce right here",
    "start": "3231420",
    "end": "3237550"
  },
  {
    "text": "is a gradient of\nquite high variance. ",
    "start": "3237550",
    "end": "3243595"
  },
  {
    "text": "All right, so the\nreason for this is-- there's multiple reasons,\nbut one of the reasons is that reward here could vary\nfrom trajectory to trajectory",
    "start": "3243595",
    "end": "3252619"
  },
  {
    "text": "and you would need\na lot of rollouts to actually estimate the\nentire expectation here.",
    "start": "3252620",
    "end": "3262950"
  },
  {
    "text": "So there is a few ways\nto deal with this. For example,\nusually in practice, people subtract state\ndependent baseline from this.",
    "start": "3262950",
    "end": "3271600"
  },
  {
    "text": "This is a very common\nway to subtract from the magnitude\nof those rewards so that it's a little\nbit more normalized",
    "start": "3271600",
    "end": "3278180"
  },
  {
    "text": "and that variances is not\nas high for the gradient. And this is what is usually\nused by algorithms in practice.",
    "start": "3278180",
    "end": "3285755"
  },
  {
    "text": " The other con is that it\nrequires on-policy data.",
    "start": "3285755",
    "end": "3293170"
  },
  {
    "text": "So in particular, if you look\nunder this expectation here, you see that the\nexpectation is under",
    "start": "3293170",
    "end": "3298440"
  },
  {
    "text": "that trajectory\ndistribution that comes from our current\npolicy pi-theta of tau.",
    "start": "3298440",
    "end": "3306020"
  },
  {
    "text": "So we cannot really reuse\nexisting experience to estimate the gradient. This is a strict\non-policy algorithm.",
    "start": "3306020",
    "end": "3312530"
  },
  {
    "text": "There are some ways to deal\nwith this by using, for example, importance weighting\nbut this comes with its own set of problems\nand produces high variance",
    "start": "3312530",
    "end": "3319530"
  },
  {
    "text": "gradients as well. All right, so that\nwas quite a bit. ",
    "start": "3319530",
    "end": "3327270"
  },
  {
    "start": "3322000",
    "end": "3518000"
  },
  {
    "text": "Are there any questions\nregarding policy gradients",
    "start": "3327270",
    "end": "3332880"
  },
  {
    "text": "as a way to solve a\nreinforcement learning task? And we don't have\nthat much time left.",
    "start": "3332880",
    "end": "3339640"
  },
  {
    "text": "So I will maybe\ntake one question. You see that Chelsea\nreplied to some of them.",
    "start": "3339640",
    "end": "3346560"
  },
  {
    "text": "So that's really good. All right, so given\nthat, maybe let's just move on because we\ndon't have that much time.",
    "start": "3346560",
    "end": "3352730"
  },
  {
    "text": "So all right, so let's\ntalk about Q-learning. So Q-learning could be an\nexample of an off-policy",
    "start": "3352730",
    "end": "3358610"
  },
  {
    "text": "algorithm that would allow\nus to use all kinds of data, not only data that come--",
    "start": "3358610",
    "end": "3364640"
  },
  {
    "text": "There's one question. Is it true that REINFORCE\ndoesn't encourage exploration? Yes, that is one\nof the problems.",
    "start": "3364640",
    "end": "3371300"
  },
  {
    "text": "And there's ways that\npeople deal with that by, for example, adding an\nentropy term to the rewards",
    "start": "3371300",
    "end": "3377540"
  },
  {
    "text": "such that you not only try\nto get to the best solution but you also try to be diverse\nor sometimes people introduce",
    "start": "3377540",
    "end": "3385490"
  },
  {
    "text": "other tricks such as Epsilon\nand greedy and so on. All right, to Q-learning. So it's an off-policy algorithm.",
    "start": "3385490",
    "end": "3391569"
  },
  {
    "text": "So we can use data that comes\nfrom many different policies and many different tasks.",
    "start": "3391570",
    "end": "3397480"
  },
  {
    "text": "And we'll start with\nsome definitions. So first, we start with a\ndefinition of a value function.",
    "start": "3397480",
    "end": "3403730"
  },
  {
    "text": "I hope that this is a\nreview for most of you. So we won't go into\na lot of details. But the value\nfunction V-pi of st",
    "start": "3403730",
    "end": "3412600"
  },
  {
    "text": "describes the total rewards\nstarting from the current state and following the policy pi.",
    "start": "3412600",
    "end": "3418550"
  },
  {
    "text": "It tries to answer the question\nhow good is the current state. We also have a\nQ-function, q-pi of st,",
    "start": "3418550",
    "end": "3425810"
  },
  {
    "text": "at, which tries to describe\nthe total reward starting from s assuming that you take\nan action a of the current step",
    "start": "3425810",
    "end": "3434060"
  },
  {
    "text": "and then you follow the\npolicy pi from then on.  And that tries to answer the\nquestion how good is a state",
    "start": "3434060",
    "end": "3441370"
  },
  {
    "text": "action pair.  Both of these are related. And they're related\nquite closely.",
    "start": "3441370",
    "end": "3447940"
  },
  {
    "text": "So if you take the\nexpectation with respect to your current policy\npi of the Q-function,",
    "start": "3447940",
    "end": "3454410"
  },
  {
    "text": "it will result in the\nvalue function V-pi of s. ",
    "start": "3454410",
    "end": "3460180"
  },
  {
    "text": "And the other thing is that if\nyou know your Q-function, q-pi, you can also use it to\nimprove your current policy.",
    "start": "3460180",
    "end": "3467460"
  },
  {
    "text": "And you can improve\nit fairly easily. If you just set the policy\nto always pick the action, that is the arg max of\nyour current Q-function.",
    "start": "3467460",
    "end": "3474280"
  },
  {
    "text": "So the action that\nmaximizes the q will be the action that\nyou would want to pick. In this way, the policy\nwill be at least as good",
    "start": "3474280",
    "end": "3480119"
  },
  {
    "text": "as the old policy.  All right. One other really\nimportant equation",
    "start": "3480120",
    "end": "3486190"
  },
  {
    "text": "is Bellman equation,\nwhich describes how this works for the optimal policy.",
    "start": "3486190",
    "end": "3491599"
  },
  {
    "text": "So we have Q-star\nnow, where star means that this is for the\npolicy that is optimal.",
    "start": "3491600",
    "end": "3497500"
  },
  {
    "text": "And Q-star is equal\nto this expectation on the next state of the\ncurrent reward times the maximum",
    "start": "3497500",
    "end": "3504940"
  },
  {
    "text": "of the Q-star of the next\nstate and action, where we take the max over the actions.",
    "start": "3504940",
    "end": "3510962"
  },
  {
    "text": "All right, so this is\nthe Bellman equation that we can use it to\nperform Q-learning.",
    "start": "3510962",
    "end": "3518950"
  },
  {
    "start": "3518000",
    "end": "3599000"
  },
  {
    "text": "All right. So I think most of these are\nprobably familiar to you, but we will go\nthrough a quick list",
    "start": "3518950",
    "end": "3525820"
  },
  {
    "text": "to make sure that you\nunderstand these concepts. So let's say that we have\na little stick figure.",
    "start": "3525820",
    "end": "3531250"
  },
  {
    "text": "And that stick figure is\ndreaming of being a drummer. ",
    "start": "3531250",
    "end": "3538072"
  },
  {
    "text": "All right, so the reward\nthat it's trying to optimize is that there is a test\ncoming up in a month from now.",
    "start": "3538072",
    "end": "3544270"
  },
  {
    "text": "And in the test, it\nwould need to learn how to play these notes,\nthe sheet of notes.",
    "start": "3544270",
    "end": "3551830"
  },
  {
    "text": "And if it will get a reward\nequal 1 if it can play it in a month, or 0 otherwise.",
    "start": "3551830",
    "end": "3557540"
  },
  {
    "text": "So this is its reward functions. Its actions are as following. Action 1 is that it can just\nrelax and don't do anything.",
    "start": "3557540",
    "end": "3565510"
  },
  {
    "text": "Action 2 is that it can watch\nvideos of people playing drums and try to learn from that.",
    "start": "3565510",
    "end": "3571300"
  },
  {
    "text": "And action 3 is\nactually practice drums.",
    "start": "3571300",
    "end": "3576640"
  },
  {
    "text": "The current policy\nof our stick figure is to always take action 1.",
    "start": "3576640",
    "end": "3582492"
  },
  {
    "text": "All right, so the\nprobability of action 1 given the current state is always 1. All right, so here\nare a few questions,",
    "start": "3582492",
    "end": "3588350"
  },
  {
    "text": "so please answer in chat. What's the value\nfunction V-pi of st? ",
    "start": "3588350",
    "end": "3602010"
  },
  {
    "text": "All right, lots of 0s. All right, so a lot of\nthis will require some kind of estimation of\nhow likely it is",
    "start": "3602010",
    "end": "3610013"
  },
  {
    "text": "that you will be able to play\ndrums if you don't practice. But I think we can\nfairly confidently say that if you continue the policy\nwhere you don't do anything,",
    "start": "3610013",
    "end": "3617849"
  },
  {
    "text": "you won't be able\nto play the drums and get the reward equal 1. OK, yes, the answer is 0.",
    "start": "3617850",
    "end": "3625109"
  },
  {
    "text": "So what's the Q-function\nq-pi of st, at? Now given that at is equal\na1, so assuming that at",
    "start": "3625110",
    "end": "3634710"
  },
  {
    "text": "is equal a1, what would\nbe that Q-function? Please answer in the chat. ",
    "start": "3634710",
    "end": "3642260"
  },
  {
    "text": "0? Yep, lots of 0s. Yes, it's exactly the same\nas the value function. OK, great.",
    "start": "3642260",
    "end": "3647380"
  },
  {
    "text": "What if you do-- what if it's the Q-function\nq-pi st and at is equal a2?",
    "start": "3647380",
    "end": "3655088"
  },
  {
    "text": "What would that be? So that would require a\nlittle bit of an estimate. Please answer in the chat.",
    "start": "3655088",
    "end": "3660621"
  },
  {
    "text": "That would correspond\nto a behavior where you watch a video of\nsomeone playing drums once. And after that, you will just\ncontinue relaxing and not",
    "start": "3660622",
    "end": "3668410"
  },
  {
    "text": "doing anything. OK, it's probably a\nlittle bit higher than 0, but probably still\nvery, very low. There are some answers,\n0.1 or something like that.",
    "start": "3668410",
    "end": "3676540"
  },
  {
    "text": "What about if at is equal a3? So you practice drums once\nat the current timestep,",
    "start": "3676540",
    "end": "3681760"
  },
  {
    "text": "and then after\nthat, you continue with your current policy pi? It'll be probably-- OK, yes,\nit will be something like 0.2.",
    "start": "3681760",
    "end": "3688750"
  },
  {
    "text": "It'll be a little bit higher,\nbut still not that high. What about the Q-star?",
    "start": "3688750",
    "end": "3694650"
  },
  {
    "text": "So what about Q-star\nif at is equal a1?",
    "start": "3694650",
    "end": "3700437"
  },
  {
    "text": "Please answer in the chat. ",
    "start": "3700437",
    "end": "3708990"
  },
  {
    "text": "Right, we didn't\nreally see any answer. 0.9.",
    "start": "3708990",
    "end": "3715180"
  },
  {
    "text": "OK, it depends how\ntalented stick figure is. All right, yeah, it's\na really good answer. ",
    "start": "3715180",
    "end": "3721800"
  },
  {
    "text": "So it will be probably something\nreally close to 1, right? Because that means that just\nin the current timestep,",
    "start": "3721800",
    "end": "3727910"
  },
  {
    "text": "you will relax, but\nfrom the timestep all-- during the days after that,\nyou will always be practicing.",
    "start": "3727910",
    "end": "3736359"
  },
  {
    "text": "And then the value\nfunction will be equal to 1 because that means that you\nwill be just always practicing",
    "start": "3736360",
    "end": "3741390"
  },
  {
    "text": "and probably the-- or\nwe hope that the reward will be then equal to 1. All right, so this is\njust a little example",
    "start": "3741390",
    "end": "3748080"
  },
  {
    "text": "that show us how these value\nfunctions in Q functions would work in practice. ",
    "start": "3748080",
    "end": "3754800"
  },
  {
    "text": "All right. So let's look quickly\nat an algorithm that we could implement\nto actually do",
    "start": "3754800",
    "end": "3761180"
  },
  {
    "text": "Q-learning in practice. So this is called the fitted\nQ-iteration algorithm.",
    "start": "3761180",
    "end": "3767470"
  },
  {
    "text": "And it will work like this. We will collect the data set of\nstates and actions, next states and rewards using some policy.",
    "start": "3767470",
    "end": "3773290"
  },
  {
    "text": "It can be any policy. This is an off-policy algorithm. Then we'll set our\ntargets according",
    "start": "3773290",
    "end": "3780867"
  },
  {
    "text": "to the Bellman equation\nto the reward plus the max of the next Q function. And they will try to fit our\nQ-function to these targets,",
    "start": "3780867",
    "end": "3787940"
  },
  {
    "text": "right? Then we can use, for example,\nan MSE loss to try to do that.",
    "start": "3787940",
    "end": "3795120"
  },
  {
    "text": "All right, and we have\nour parameters phi here and our policy will be\nor-- our Q-function will",
    "start": "3795120",
    "end": "3802050"
  },
  {
    "text": "be parameterized by a neural\nnetwork where as an input it detects states and actions\nand outputs a Q value.",
    "start": "3802050",
    "end": "3808470"
  },
  {
    "text": "One way to do this in\na discrete action space is actually have\nmultiple heads that where each head corresponds\nto a different action.",
    "start": "3808470",
    "end": "3816609"
  },
  {
    "text": "All right, we'll take a\nnumber of gradient steps S and we'll perform it\nover K iterations.",
    "start": "3816610",
    "end": "3821720"
  },
  {
    "text": "So we'll just collect the data\nset, estimate the Q targets and try to fit our Q-functions\nto these Q targets,",
    "start": "3821720",
    "end": "3827559"
  },
  {
    "text": "and then keep doing that.  All right, then we can learn\na q function from that.",
    "start": "3827560",
    "end": "3835050"
  },
  {
    "text": "And as a result, once\nwe have the Q function, we can get a policy\nfrom it fairly easily.",
    "start": "3835050",
    "end": "3840522"
  },
  {
    "text": "And that would just\ncorrespond to taking an arg max over our current\nq function to find the action that maximizes it.",
    "start": "3840522",
    "end": "3846230"
  },
  {
    "text": "And that action will be the\naction that we should take. And this will be our policy. ",
    "start": "3846230",
    "end": "3852490"
  },
  {
    "text": "All right, a few\nimportant notes here. So first, we can reuse data\nfrom previous policies. So we can use any policy\nto collect the data,",
    "start": "3852490",
    "end": "3859370"
  },
  {
    "text": "and it should still\nimprove our q function. ",
    "start": "3859370",
    "end": "3864780"
  },
  {
    "text": "It's an off-policy algorithm. And people usually in\npractice use replay buffers to replay the data\nthat was stored",
    "start": "3864780",
    "end": "3871579"
  },
  {
    "text": "by many different policies. An important note here that\nthis is not a gradient descent algorithm, right?",
    "start": "3871580",
    "end": "3877070"
  },
  {
    "text": "This is a dynamic\nprogramming algorithm. And then we are doing a\ngradient descent in addition",
    "start": "3877070",
    "end": "3882380"
  },
  {
    "text": "to this to just fit our\nQ-function to Q targets. This can be easily extended to\nmulti-task and goal conditioned",
    "start": "3882380",
    "end": "3890500"
  },
  {
    "text": "RL. settings, but not as easily\nto meta-learning settings. And we'll talk about it\nin the next lectures too.",
    "start": "3890500",
    "end": "3898510"
  },
  {
    "text": "All right, so I want to\ngive you one more example of how this could\nwork in practice and how we actually\nuse it at Google.",
    "start": "3898510",
    "end": "3903680"
  },
  {
    "text": "But I have to go over\nthis really quickly, because we're\nrunning out of time. ",
    "start": "3903680",
    "end": "3910670"
  },
  {
    "text": "So let's just look at how we\ncan apply this to robotics. And we saw that these\ntargets required",
    "start": "3910670",
    "end": "3916040"
  },
  {
    "text": "a maximum of the\nQ-function and the maximum was taken over actions.",
    "start": "3916040",
    "end": "3921110"
  },
  {
    "text": "Now if the actions\nare continuous, which is the case in robotics,\nthis can be quite tricky.",
    "start": "3921110",
    "end": "3926492"
  },
  {
    "text": "So one way we can\ndo this is to use a simple stochastic\noptimization algorithm called cross entropy method. And it works as follows.",
    "start": "3926492",
    "end": "3933383"
  },
  {
    "text": "You would start with the current\nposition of where the robot is, and you'll start with a\nnormal distribution of mean 0",
    "start": "3933383",
    "end": "3940520"
  },
  {
    "text": "and some standard deviation. And then you will just sample\nfrom the distribution and you will evaluate--",
    "start": "3940520",
    "end": "3946730"
  },
  {
    "text": "you will ask the\nQ-function, how good are the actions from\nthat distribution? So you'll sample\na bunch of actions",
    "start": "3946730",
    "end": "3952520"
  },
  {
    "text": "and then you will query\nyour Q-function about it. And the Q-functions\nwill tell you",
    "start": "3952520",
    "end": "3958359"
  },
  {
    "text": "that some actions are a\nlittle bit better than others. And these are here\ndepicted in gray. So this one is a little\nbit better than this one,",
    "start": "3958360",
    "end": "3964162"
  },
  {
    "text": "and so on. And then you would compute\nthe new mean and standard deviation of the best actions. So right here.",
    "start": "3964162",
    "end": "3970329"
  },
  {
    "text": "And then you can do\nthis procedure again. And if you keep doing that, you\ncan sometimes even do it just",
    "start": "3970330",
    "end": "3975430"
  },
  {
    "text": "for one or two iterations,\nyou will get more or less the maximum of the Q-function.",
    "start": "3975430",
    "end": "3980940"
  },
  {
    "text": "All right, so let's do that. And this was applied\nto the algorithm called QT-Opt where Q-learning\nwas applied at scale.",
    "start": "3980940",
    "end": "3988500"
  },
  {
    "text": "And this was done\nat Google where this was done in the\nfully distributed setup",
    "start": "3988500",
    "end": "3994080"
  },
  {
    "text": "and applied to the problem\nof grasping in robotics. We will start with a\ndata that will start",
    "start": "3994080",
    "end": "4000500"
  },
  {
    "text": "from all the past experiments. We'll have some\nbuffers that we use",
    "start": "4000500",
    "end": "4005660"
  },
  {
    "text": "for the data that was\ncollected previously, as well as the data that\nis coming from the robots right now.",
    "start": "4005660",
    "end": "4012490"
  },
  {
    "text": "It will represent our\nQ-function by neural network. And we'll have\nseparate processes that will do the Bellman updates.",
    "start": "4012490",
    "end": "4018427"
  },
  {
    "text": "So they'll just\ncompute the target that we would want to\nfit our Q-function to.",
    "start": "4018427",
    "end": "4024070"
  },
  {
    "text": "This will be the\nCEM optimization that we just talked about. After it's done, after\nBellman updaters are done,",
    "start": "4024070",
    "end": "4030570"
  },
  {
    "text": "they will just set a set of\ntuples with the Q targets as well as the current\nstate and action.",
    "start": "4030570",
    "end": "4035880"
  },
  {
    "text": "And this will be\nthen used to actually to do the mean squared error\nand update our Q-function.",
    "start": "4035880",
    "end": "4044127"
  },
  {
    "text": "So we'll have a bunch\nof training jobs that are running on GPUs or\nTPUs or something like that. And they can be\nupdating the parameters.",
    "start": "4044127",
    "end": "4051410"
  },
  {
    "text": " All right, so we\nhave the algorithm.",
    "start": "4051410",
    "end": "4056770"
  },
  {
    "text": "Now the problem we're\ntrying to apply this to is something like this.",
    "start": "4056770",
    "end": "4061940"
  },
  {
    "text": "We have a state that\nis just a camera image over the shoulder, which\nis similar to the example we",
    "start": "4061940",
    "end": "4067660"
  },
  {
    "text": "discussed before. So this is not a fully\nobservable setting, but in this case,\nit was good enough.",
    "start": "4067660",
    "end": "4072779"
  },
  {
    "text": "The action is a 4\ndegree of freedom pose change in the Cartesian space\nplus the gripper control. So we can also close\nand open the gripper.",
    "start": "4072780",
    "end": "4081040"
  },
  {
    "text": "And the reward will\nbe binary at the end. So if the object was\nlifted, you get a reward 1. And if it wasn't,\nyou get a reward 0.",
    "start": "4081040",
    "end": "4089830"
  },
  {
    "text": "All right, so some\nresults from it. So first, there\nwere seven robots that were collecting data over\nthe course of a few months",
    "start": "4089830",
    "end": "4096250"
  },
  {
    "text": "and it collected 580,000 grasps. And we're evaluating things\non an unseen test object,",
    "start": "4096250",
    "end": "4103270"
  },
  {
    "text": "so these objects were\nnot used during training and was able to grasp them\nwith 96% test success rate.",
    "start": "4103270",
    "end": "4111679"
  },
  {
    "text": "So basically,\nreally, really good. And it was also able to learn\nsome behaviors that were not",
    "start": "4111680",
    "end": "4117270"
  },
  {
    "text": "really in the\ntraining set that were necessary to grasp new objects. So this is, for example,\na little construction",
    "start": "4117270",
    "end": "4122640"
  },
  {
    "text": "where you first need\nto simulate the objects and then you can grasp them. So it'd learn how\nto do that as well.",
    "start": "4122640",
    "end": "4128339"
  },
  {
    "text": " It is also able to grasp objects\nsuch as that little ball.",
    "start": "4128340",
    "end": "4138330"
  },
  {
    "text": "And you can mess\nwith it a little bit, and it's a closed loop behavior. So that's able to track\nthe ball and get back to it",
    "start": "4138330",
    "end": "4146068"
  },
  {
    "text": "and try to grasp it again. And it is also capable of doing\nan autonomous free trials where",
    "start": "4146069",
    "end": "4154000"
  },
  {
    "text": "it's not able to grasp it\nand it fails a few times. It will know that\nit failed and it will try to do it over and over\nagain until it gets it right.",
    "start": "4154000",
    "end": "4160689"
  },
  {
    "text": " All right, so to\nsummarize Q-learning,",
    "start": "4160689",
    "end": "4166859"
  },
  {
    "text": "the big process\nthat-- this is more sample efficient than\non-policy methods. We can incorporate off-policy\ndata even including",
    "start": "4166859",
    "end": "4174701"
  },
  {
    "text": "a fully offline\nsetting where there are no robots or no agents\nthat are collecting experience",
    "start": "4174702",
    "end": "4179729"
  },
  {
    "text": "as we were training the\npolicy over the Q-function. And another important\npoint is that we",
    "start": "4179729",
    "end": "4185520"
  },
  {
    "text": "can update the policy even\nif we don't see the reward. So if you recall in the\npolicy gradient case,",
    "start": "4185520",
    "end": "4191700"
  },
  {
    "text": "if our reward was equal\nto 0, the gradient would be equal to 0. Here it's not\nnecessarily the case.",
    "start": "4191700",
    "end": "4197070"
  },
  {
    "text": "So even if you don't\nsee the reward, that might inform\nyour Q-function. And it's relatively\neasy to parallelize",
    "start": "4197070",
    "end": "4202960"
  },
  {
    "text": "and this was used by the\nQT-Opt, in the QT-Opt algorithm.",
    "start": "4202960",
    "end": "4208420"
  },
  {
    "text": "In terms of cons, this\nis harder to apply standard meta-learning\nalgorithms because it's a dynamic\nprogramming algorithm, not",
    "start": "4208420",
    "end": "4214902"
  },
  {
    "text": "a gradient descent algorithm,\nand requires quite a few tricks to make it more stable\nand to get it to work.",
    "start": "4214902",
    "end": "4221940"
  },
  {
    "text": "Another argument that it\nmight be potentially harder to train a Q-function\nthan to train a policy, but that really\ndepends on the problem.",
    "start": "4221940",
    "end": "4230540"
  },
  {
    "text": "All right, so this is a quick\nreview of how Q-learning works. So now I would like\nto talk a little bit",
    "start": "4230540",
    "end": "4236360"
  },
  {
    "text": "about multi-task Q-learning.  And I'll stay a little\nbit after the lecture",
    "start": "4236360",
    "end": "4242595"
  },
  {
    "text": "to answer your\nquestions, because we're running out of time. All right, so in terms of\nmulti-task RL algorithms,",
    "start": "4242595",
    "end": "4250830"
  },
  {
    "text": "we learned so far\nthat we can easily transform our\npolicy or our state to include the task indicator\nto augment the state by it",
    "start": "4250830",
    "end": "4260190"
  },
  {
    "text": "and then we can apply it\nto multi-task RL problems. So we can do exactly the\nsame with Q-function,",
    "start": "4260190",
    "end": "4265260"
  },
  {
    "text": "augment the state with a z\ninformation that tells us about the task and keep going.",
    "start": "4265260",
    "end": "4270370"
  },
  {
    "text": "And it's analogous to\nmulti-task supervised learning. So we can use all\nthe tricks that we know from there, such stratified\nsampling, different weight",
    "start": "4270370",
    "end": "4277390"
  },
  {
    "text": "sharing schemes, and so on. However, there is something\ndifferent about RL here.",
    "start": "4277390",
    "end": "4282750"
  },
  {
    "text": "And the difference is that\nhere the data distribution is controlled by the agent. ",
    "start": "4282750",
    "end": "4290010"
  },
  {
    "text": "So the fact is that in\nsupervised learning, we had that data\nset that was curated",
    "start": "4290010",
    "end": "4295113"
  },
  {
    "text": "and we could sample from it. Here we get to decide which\ntask we're going to explore. ",
    "start": "4295113",
    "end": "4302040"
  },
  {
    "text": "So one question that comes\nimmediately to mind is, should we in addition to sharing\nweights, because we are trying",
    "start": "4302040",
    "end": "4307410"
  },
  {
    "text": "to learn multiple-- a policy\nthat can do multiple tasks, can we also share data\nbetween different tasks? ",
    "start": "4307410",
    "end": "4314330"
  },
  {
    "text": "And one little question\nI have to all of you is, why are we\nmentioning this now? Why are we talking about\ndata sharing right now?",
    "start": "4314330",
    "end": "4321590"
  },
  {
    "text": "And we'll get back\nto this question. So think about\nthis a little bit. All right, so first\na little example,",
    "start": "4321590",
    "end": "4327990"
  },
  {
    "text": "how we could reuse the data\nbetween different tasks. So let's pretend that we are\ntrying to learn a hockey task.",
    "start": "4327990",
    "end": "4333960"
  },
  {
    "text": "And there are two actual tasks. One is to do passing. We are trying to pass\nas well as possible.",
    "start": "4333960",
    "end": "4340960"
  },
  {
    "text": "And another task, task\nnumber 2, is to shoot goals. Now we can train\nall of these tasks.",
    "start": "4340960",
    "end": "4346849"
  },
  {
    "text": "But both of these tasks, we\ncan share weights between them. But now what happens\nif we accidentally",
    "start": "4346850",
    "end": "4352180"
  },
  {
    "text": "perform a good pass when\ntrying to shoot a goal? All right, so we're\ntrying to shoot a goal, but it just happened to be\nthat we did a really good pass.",
    "start": "4352180",
    "end": "4362380"
  },
  {
    "text": "So one thing we can do is we\ncan store that experience. This is a valid experience. But in addition to this,\nwe can create a copy of it",
    "start": "4362380",
    "end": "4368260"
  },
  {
    "text": "where we relabel that\nexperience with another task ID, so pretend that this was task\n2, and put the reward there",
    "start": "4368260",
    "end": "4374410"
  },
  {
    "text": "for task 2 and\nstore that as well. So basically it's equivalent\nto us saying that this",
    "start": "4374410",
    "end": "4379929"
  },
  {
    "text": "was a plan all along, right? So I was trying to\nshoot it, I passed it, I was actually\ntrying to pass it.",
    "start": "4379930",
    "end": "4385869"
  },
  {
    "text": "And we'll try to store\nthat experience as such. And this we'll refer to\nas hindsight relabeling",
    "start": "4385870",
    "end": "4391060"
  },
  {
    "text": "or hindsight\nexperience replay, HER. All right, so how would\nthat work in practice?",
    "start": "4391060",
    "end": "4398870"
  },
  {
    "text": "So we'll collect some\ndata using some policy. We'll start the data\nin the replay buffer.",
    "start": "4398870",
    "end": "4406590"
  },
  {
    "text": "And then on the data, we can\nperform hindsight relabeling. So we can relabel the experience\nthat we just received.",
    "start": "4406590",
    "end": "4414840"
  },
  {
    "text": "And we can pretend that the\nlast state was the actual goal that we wanted. ",
    "start": "4414840",
    "end": "4420810"
  },
  {
    "text": "All right, so here we are\nchanging the reward function. We have r-prime instead of\nthe original r that we had.",
    "start": "4420810",
    "end": "4429450"
  },
  {
    "text": "And we said that the r-prime\nis equal to the distance between the current\nstate and the final state",
    "start": "4429450",
    "end": "4435270"
  },
  {
    "text": "that we achieve. So we pretend that this\nwas the plan all along and this is what we did here.",
    "start": "4435270",
    "end": "4444310"
  },
  {
    "text": "Then we store the relabeled\ndata in the replay buffer, and then we update the policy\nusing the replay buffer.",
    "start": "4444310",
    "end": "4450360"
  },
  {
    "text": "So this is similar to a\nstandard Q-learning algorithm, except we can now\ngenerate much more data because we can do the\nhindsight relabeling procedure.",
    "start": "4450360",
    "end": "4459250"
  },
  {
    "text": "So are there any other\nrelabeling strategies that we can think of? For example, we don't\nhave to use necessarily",
    "start": "4459250",
    "end": "4466210"
  },
  {
    "text": "just the last state,\nbut we can pretend that any state during\nthat trajectory could have been our\ngoal state, and use",
    "start": "4466210",
    "end": "4472840"
  },
  {
    "text": "that as a potential\nrelabeling strategy as well. And if we do this,\nthen very often we",
    "start": "4472840",
    "end": "4479380"
  },
  {
    "text": "can see that exploration\nchallenges are getting a little bit easier\nbecause we are getting much more experience\nand we are getting",
    "start": "4479380",
    "end": "4485030"
  },
  {
    "text": "kind of a curriculum of tasks\nthat leads us towards the goal that we wanted to achieve. ",
    "start": "4485030",
    "end": "4491730"
  },
  {
    "text": "All right, so I asked you,\nI think one slide ago, why are we mentioning this\nnow, the data sharing concept?",
    "start": "4491730",
    "end": "4500310"
  },
  {
    "text": "And with the task of\npassing and shooting goals, we could use our experiences\nand relabel them.",
    "start": "4500310",
    "end": "4508830"
  },
  {
    "text": "And if we get to achieve another\ntask that we weren't trying to do, we can\nrelabel them as such",
    "start": "4508830",
    "end": "4515340"
  },
  {
    "text": "and just use the successes\nof these other tasks and learn from those as well.",
    "start": "4515340",
    "end": "4521639"
  },
  {
    "text": "So let's consider\ntwo other tasks. One task is to close\nthe drawer, so this is the final goal of the task.",
    "start": "4521640",
    "end": "4527929"
  },
  {
    "text": "And another task is\nto open the drawer. So the final goal\nof the task would be the drawer at the\nbottom that is fully open.",
    "start": "4527930",
    "end": "4534470"
  },
  {
    "text": " All right, so now the\nquestion is, can we use episodes from drawer opening\ntasks or drawer closing tasks",
    "start": "4534470",
    "end": "4542930"
  },
  {
    "text": "as well? And how does that answer\nchange for Q-learning versus policy gradient? ",
    "start": "4542930",
    "end": "4550429"
  },
  {
    "text": "So please keep in mind\nthat, for these two tasks, it's-- let's just assume that\nit's impossible to succeed",
    "start": "4550430",
    "end": "4555800"
  },
  {
    "text": "at opening a drawer when\nyou're trying to close it, and vice versa as well. It's not that you can\naccidentally achieve success",
    "start": "4555800",
    "end": "4563780"
  },
  {
    "text": "as we did before in\nthe hockey example, but here you would rather\nbe using experiences",
    "start": "4563780",
    "end": "4570410"
  },
  {
    "text": "from opening the drawer task as\nsomething that you should not do when you're trying\nto close the drawer.",
    "start": "4570410",
    "end": "4575900"
  },
  {
    "text": " All right, we are\nnot necessarily using just our\naccidental successes",
    "start": "4575900",
    "end": "4582590"
  },
  {
    "text": "that we can get to when\nwe perform a certain task. But we can also use them\nas potential failures.",
    "start": "4582590",
    "end": "4590140"
  },
  {
    "text": "And now how does that\nanswer change for Q-learning versus policy gradient?",
    "start": "4590140",
    "end": "4595662"
  },
  {
    "text": "In the policy gradient\ncase, because it's an on-policy\nalgorithm and because of that specific equation\nthat we wrote how we estimate",
    "start": "4595662",
    "end": "4603790"
  },
  {
    "text": "the gradient, if the reward\nfor a given task is 0, it does not contribute\nto the gradient.",
    "start": "4603790",
    "end": "4609250"
  },
  {
    "text": "The resulting gradient from\nthat will be 0 as well. In Q-learning, however, if we\nsee a negative, that is, even",
    "start": "4609250",
    "end": "4616100"
  },
  {
    "text": "coming from another task,\nyou can still use it and it can still inform our\nQ-function that action in that",
    "start": "4616100",
    "end": "4622100"
  },
  {
    "text": "state is not really good. So this Q-learning and\noff-policy learning can allow us to use\nnegatives as well",
    "start": "4622100",
    "end": "4627980"
  },
  {
    "text": "and inform us how\nto do things better by learning what not to do. ",
    "start": "4627980",
    "end": "4635870"
  },
  {
    "text": "All right, so how can we\ndo multi-task reinforcement learning with relabeling\neven if it's not a goal conditioned setting?",
    "start": "4635870",
    "end": "4641750"
  },
  {
    "text": "It looks very similar, except\nfor hindsight relabeling,",
    "start": "4641750",
    "end": "4647510"
  },
  {
    "text": "we'll just relabel our\nexperience according to some other task j.",
    "start": "4647510",
    "end": "4652770"
  },
  {
    "text": "All right, so initially\nwe were collecting data for a specific task.",
    "start": "4652770",
    "end": "4659300"
  },
  {
    "text": "Now we can pretend that\nthat task can be relabeled for any other task,\nfor example, task j",
    "start": "4659300",
    "end": "4665075"
  },
  {
    "text": "and we'll just overwrite a\nreward with r-prime instead of original r where the r-prime\ncorresponds to the reward",
    "start": "4665075",
    "end": "4673730"
  },
  {
    "text": "function of task j, so rj here. The question here is what kind\nof tasks should we choose?",
    "start": "4673730",
    "end": "4682600"
  },
  {
    "text": "And we don't really\nknow how to do this. So one way to do this is to\njust pick them randomly and free",
    "start": "4682600",
    "end": "4687780"
  },
  {
    "text": "label to random tasks. Another way is to pick\ntasks in which trajectory is",
    "start": "4687780",
    "end": "4694300"
  },
  {
    "text": "more likely to get high reward. And there's multiple\nmore recent papers that talk about it, that\nI put the link here.",
    "start": "4694300",
    "end": "4699820"
  },
  {
    "text": " One more question\nis, when can we actually apply this relabeling?",
    "start": "4699820",
    "end": "4706340"
  },
  {
    "text": "So first, we need to know-- there's a question. How do you get a reward\nfor the other task exactly?",
    "start": "4706340",
    "end": "4712219"
  },
  {
    "text": "So first, we need to assume that\nthe reward function is known and it's easy to evaluate. So we can just take\na look at the state",
    "start": "4712220",
    "end": "4718640"
  },
  {
    "text": "and evaluate it for\ndifferent tasks. We also need to make\nsure the dynamics are consistent across\ngoals and tasks.",
    "start": "4718640",
    "end": "4725410"
  },
  {
    "text": "And we need to be using\nan off-policy algorithm although there are\npeople that are trying. There is research that\nis trying to do this also",
    "start": "4725410",
    "end": "4732010"
  },
  {
    "text": "for on-policy algorithms, but\nit's a little more tricky. ",
    "start": "4732010",
    "end": "4737320"
  },
  {
    "text": "All right, so I think\nwe're running out of time. So I will skip the results\nand the other example.",
    "start": "4737320",
    "end": "4745800"
  },
  {
    "text": "But basically, today, we talked\nabout multi-task reinforcement learning problem, how we can\nsolve it with policy gradient",
    "start": "4745800",
    "end": "4751860"
  },
  {
    "text": "algorithms, how we can\nsolve it with another family of algorithms called Q-learning\nthat are off-policy algorithms,",
    "start": "4751860",
    "end": "4758580"
  },
  {
    "text": "and how we can use\nthose algorithms to multi-task Q-learning where\nin addition to sharing weight, we can also share data\nbetween the tasks,",
    "start": "4758580",
    "end": "4765195"
  },
  {
    "text": "and we talk a little\nbit how that data can be shared across different tasks. ",
    "start": "4765195",
    "end": "4770909"
  },
  {
    "text": "And so there are many\nremaining questions still. So first, can we use a model\nin multi-task reinforcement learning? What about meta-reinforcement\nlearning algorithms?",
    "start": "4770910",
    "end": "4777993"
  },
  {
    "text": "We didn't get to\ntalk about these. Can we learn exploration\nstrategies in addition to learning tasks?",
    "start": "4777993",
    "end": "4783770"
  },
  {
    "text": "And what about\nhierarchies of tasks? And we'll talk about this\nin the upcoming lectures.",
    "start": "4783770",
    "end": "4791119"
  },
  {
    "text": "There is also many\nadditional RL resources. I realized that we\nwent really quickly over a very large\nbody of material",
    "start": "4791120",
    "end": "4797870"
  },
  {
    "text": "and it might be\ndifficult to keep up and to understand everything\nabout policy gradient, and especially about\nQ-learning because we",
    "start": "4797870",
    "end": "4804000"
  },
  {
    "text": "didn't talk in detail about it. So I encourage you to take a\nlook at some of the resources",
    "start": "4804000",
    "end": "4809119"
  },
  {
    "text": "that are available online\nas well as the other class at Stanford, CS234.",
    "start": "4809120",
    "end": "4816370"
  },
  {
    "text": "All right, so thank you very\nmuch for your attention. And I'll stick around\nfor a little bit longer,",
    "start": "4816370",
    "end": "4822390"
  },
  {
    "text": "if there are any questions. I've a quick question on\nthe hindsight relabeling, if that's OK.",
    "start": "4822390",
    "end": "4827550"
  },
  {
    "text": "No, of course. So the first one you showed\nwhere you talked about the goal",
    "start": "4827550",
    "end": "4833160"
  },
  {
    "text": "state, can you only\nreplace that-- is that only if you, like, happened\nto end up in the goal",
    "start": "4833160",
    "end": "4838590"
  },
  {
    "text": "state for another task? I was kind of confused what was\ngoing on with the goal states and when you could relabel.",
    "start": "4838590",
    "end": "4843690"
  },
  {
    "text": "Right, right. I think we went a little\nbit quickly over this. Yes. So you can just take the\nlast state that you actually",
    "start": "4843690",
    "end": "4851730"
  },
  {
    "text": "accomplish in that trajectory. And so our trajectory\nconsists of all the states",
    "start": "4851730",
    "end": "4856860"
  },
  {
    "text": "that we encountered\nas well as the actions that we took in the very last\nstate that we ended up in.",
    "start": "4856860",
    "end": "4862350"
  },
  {
    "text": "And pretend that this\nwas your original goal. All right, so this is a\ngoal-conditioned setting.",
    "start": "4862350",
    "end": "4868370"
  },
  {
    "text": "So the goal is fully\ndefined by the state. And you can just\nsay that the reward",
    "start": "4868370",
    "end": "4874330"
  },
  {
    "text": "for that particular transition,\nfor that particular episode, assuming that you are trying to\naccomplish the state that you",
    "start": "4874330",
    "end": "4880900"
  },
  {
    "text": "actually ended up in is maximum. So in this case, because\nthis is a negative reward, it would be 0.",
    "start": "4880900",
    "end": "4887139"
  },
  {
    "text": "So we were just trying to\nrelabel to the very final state that you achieved. And we were saying\nthat this is the goal that you wanted to achieve.",
    "start": "4887140",
    "end": "4893329"
  },
  {
    "text": "Does that make sense? I think so, yeah. Thank you. ",
    "start": "4893330",
    "end": "4904000"
  }
]